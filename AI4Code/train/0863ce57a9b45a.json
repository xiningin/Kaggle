{"cell_type":{"f82b1d92":"code","fd34fe27":"code","b16c7613":"code","711b44c6":"code","f00b2b07":"code","6ecd73ce":"code","9da3f9c9":"code","52c67a3d":"code","ba3fe119":"code","a292c02d":"code","80ebe74e":"code","1ca53c5b":"code","ef3e9c8c":"code","594a613a":"code","186252bb":"code","7e5709b7":"code","89679a3e":"code","6acff99e":"code","3450ad19":"code","2a4a138d":"code","529584b2":"code","f8101b22":"code","2c6160fb":"code","7e8d68ca":"code","9fcfa478":"code","862dcf11":"code","99287421":"code","af134657":"code","bdd2900a":"code","be88170e":"code","2ed399c6":"code","4f5b609b":"code","de792175":"code","7dd505af":"code","367b9ebb":"code","9c267c46":"code","013773ce":"code","2f452785":"code","32e173a9":"code","aa009339":"code","a4fcfe62":"code","1f573ba2":"code","39850bbc":"code","7d7f1051":"code","1bfdf5d0":"code","49339d57":"code","138ac6ba":"code","43ff7ce6":"code","193b3dcc":"code","04e170a9":"code","d52bdf62":"code","933bdba7":"code","fcf3f63a":"code","2c63c1dc":"code","02c7130e":"code","f18e5f9d":"code","ee46460a":"code","c0468874":"code","110fe4fe":"code","2a583538":"code","98cdc6a4":"code","4defb1dc":"code","5103157b":"code","e531f8ea":"code","6634ced1":"code","e06e416f":"code","2fdbdf7d":"code","4c396584":"code","5f8114ad":"code","714e47a8":"code","7aa456ba":"code","7d76dee3":"code","7376f56d":"code","b032366e":"markdown","400bf68c":"markdown","c9f3253e":"markdown","815ea78f":"markdown","541b2410":"markdown","4ea020a8":"markdown","0f7e33dd":"markdown","504ddd47":"markdown","cb41c609":"markdown","ea6ba7da":"markdown","881d205b":"markdown","3b3b9196":"markdown","79a13d5c":"markdown","aeccfb7e":"markdown","327ea54c":"markdown","2d0d7aa3":"markdown","5e3d3664":"markdown","458007f7":"markdown","92a57ac1":"markdown","6b2e6e7a":"markdown","a71afa96":"markdown","f8977009":"markdown","a6e5bc57":"markdown","37f97532":"markdown"},"source":{"f82b1d92":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fd34fe27":"df0 = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","b16c7613":"df = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","711b44c6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder,StandardScaler,PowerTransformer, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_predict, train_test_split,GridSearchCV,cross_val_score, cross_validate\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge,ElasticNet\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.metrics import classification_report, confusion_matrix, log_loss, recall_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVR\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.feature_selection import SelectKBest,SelectPercentile,f_classif,f_regression,mutual_info_regression\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.impute import SimpleImputer\nfrom yellowbrick.classifier import ConfusionMatrix\nfrom yellowbrick.cluster import KElbowVisualizer\n\nimport missingno as msno\n\n#importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams[\"figure.figsize\"] = (10,6)\npd.set_option('max_colwidth',200)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.7f' % x)","f00b2b07":"def fill_most(df, group_col, col_name):\n    '''Fills the missing values with the most existing value (mode) in the relevant column according to single-stage grouping'''\n    for group in list(df[group_col].unique()):\n        cond = df[group_col]==group\n        mode = list(df[cond][col_name].mode())\n        if mode != []:\n            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[cond][col_name].mode()[0])\n        else:\n            df.loc[cond, col_name] = df.loc[cond, col_name].fillna(df[col_name].mode()[0])\n    print(\"Number of NaN : \",df[col_name].isnull().sum())\n    print(\"------------------\")\n    print(df[col_name].value_counts(dropna=False))","6ecd73ce":"df.info()","9da3f9c9":"df.describe().T","52c67a3d":"df.describe(include=object).T","ba3fe119":"print(f\"Data has {df.shape[0]} instances and {df.shape[1] - 1} attributes.\")","a292c02d":"df.duplicated().value_counts()","80ebe74e":"def missing (df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values","1ca53c5b":"missing(df)","ef3e9c8c":"df['stroke'].value_counts()","594a613a":"df.drop('id', axis=1, inplace=True)","186252bb":"df.columns = df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')","7e5709b7":"df.gender.value_counts()","89679a3e":"sns.heatmap(df.corr(), annot=True);","6acff99e":"numerical= df.drop(['stroke'], axis=1).select_dtypes('number').columns\n\ncategorical = df.select_dtypes('object').columns\n\nprint(\"\\033[1m\", \"Numerical Columns:\", \"\\033[0;0m\",  numerical)\nprint(\"------------------------------------------------------------------------------------------------------------------\")\nprint(\"\\033[1m\", \"Categorical Columns:\", \"\\033[0;0m\", categorical)","3450ad19":"df['stroke'].describe().T","2a4a138d":"df['stroke'].value_counts()","529584b2":"print( f\"Skewness: {df['stroke'].skew()}\")","f8101b22":"df[numerical].describe().T","2c6160fb":"df[numerical].iplot(kind='hist');","7e8d68ca":"index = 0\nplt.figure(figsize=(20,20))\nfor feature in df.drop(columns=['gender', 'hypertension', 'heart_disease', 'ever_married',\n       'work_type', 'residence_type', 'smoking_status']).columns:\n    if feature != \"stroke\":\n        index += 1\n        plt.subplot(4,3,index)\n        sns.boxplot(x='stroke', y=feature, data=df)","9fcfa478":"df[(df['age'] <= 14) & (df['stroke'] == 1)] ","862dcf11":"skew_limit = 0.75 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models. \nskew_vals = df[numerical].skew()\nskew_cols = skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols ","99287421":"df[skew_cols.index].iplot(kind='histogram',subplots=True,bins=50)","af134657":"skew_limit = 0.75 # This is our threshold-limit to evaluate skewness. Overall below abs(1) seems acceptable for the linear models.\nskew_vals = df[numerical].skew()\nskew_cols = skew_vals[abs(skew_vals)> skew_limit].sort_values(ascending=False)\nskew_cols = skew_cols.drop(['heart_disease', 'hypertension'])\nskew_cols","bdd2900a":"df_trans = df[skew_cols.index].copy()\npt = PowerTransformer(method='yeo-johnson')\ntrans= pt.fit_transform(df_trans)\ndf_trans = pd.DataFrame(trans, columns =skew_cols.index )\nprint(df_trans.skew())\nprint()\ndf_trans.iplot(kind='histogram',subplots=True,bins=50);","be88170e":"df.shape","2ed399c6":"df.drop([\"avg_glucose_level\", \"bmi\"], axis=1, inplace=True)","4f5b609b":"df = pd.concat([df, df_trans], axis=1)","de792175":"df.shape","7dd505af":"df.drop(df[df['gender'] == 'Other'].index, inplace = True)","367b9ebb":"missing(df)","9c267c46":"df[categorical].head().T","013773ce":"df[categorical].head().T","2f452785":"def first_looking(col):\n    print(\"column name    : \", col)\n    print(\"--------------------------------\")\n    print(\"per_of_nulls   : \", \"%\", round(df[col].isnull().sum()\/df.shape[0]*100, 2))\n    print(\"num_of_nulls   : \", df[col].isnull().sum())\n    print(\"num_of_uniques : \", df[col].nunique())\n    print(df[col].value_counts(dropna = False))","32e173a9":"first_looking(\"gender\")","aa009339":"first_looking(\"ever_married\")","a4fcfe62":"first_looking(\"work_type\")","1f573ba2":"first_looking(\"work_type\")","39850bbc":"first_looking(\"smoking_status\")","7d7f1051":"sns.pairplot(df, hue=\"stroke\");","1bfdf5d0":"df = pd.get_dummies(df, columns=['gender', 'ever_married',\n       'work_type', 'residence_type', 'smoking_status',], drop_first=True)","49339d57":"df.head()","138ac6ba":"plt.figure(figsize=(12, 8))\nsns.heatmap(df.corr(), annot=True);","43ff7ce6":"df.shape","193b3dcc":"X = df.drop(columns=[\"stroke\"])\ny = df.stroke","04e170a9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","d52bdf62":"df.bmi.isnull().sum()","933bdba7":"from sklearn.impute import SimpleImputer","fcf3f63a":"imputer = SimpleImputer(missing_values=np.nan, strategy=\"median\")\n\nX_train['bmi'] = imputer.fit_transform(X_train['bmi'].values.reshape(-1,1))[:,0]","2c63c1dc":"X_test['bmi'] = imputer.fit_transform(X_test['bmi'].values.reshape(-1,1))[:,0]","02c7130e":"X_test['bmi'].isna().sum()","f18e5f9d":"X_train['bmi'].isna().sum()","ee46460a":"from sklearn.preprocessing import MinMaxScaler","c0468874":"scaler = MinMaxScaler()","110fe4fe":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","2a583538":"from sklearn.linear_model import LogisticRegression","98cdc6a4":"log_model = LogisticRegression(class_weight=\"balanced\").fit(X_train_scaled, y_train)\nlog_model","4defb1dc":"y_test_pred = log_model.predict(X_test_scaled)","5103157b":"y_pred_proba = log_model.predict_proba(X_test_scaled)","e531f8ea":"test_data = pd.concat([X_test, y_test], axis=1)\ntest_data[\"pred\"] = y_test_pred\ntest_data[\"pred_proba\"] = y_pred_proba[:,1]\ntest_data.head()","6634ced1":"from sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix","e06e416f":"print(confusion_matrix(y_test,y_test_pred))\nprint(classification_report(y_test,y_test_pred))","2fdbdf7d":"from sklearn.model_selection import cross_val_score, cross_validate","4c396584":"LogModel_CV = LogisticRegression(class_weight = \"balanced\")","5f8114ad":"scores = cross_validate(LogModel_CV, X_train_scaled, y_train, scoring = ['accuracy', 'precision','recall','f1'], cv = 10)\ndf_scores = pd.DataFrame(scores, index = range(1, 11))\ndf_scores.mean()[2:]","714e47a8":"from sklearn.metrics import plot_roc_curve, plot_precision_recall_curve, roc_auc_score, auc, roc_curve","7aa456ba":"fp_rate, tp_rate, thresholds = roc_curve(y_test, y_pred_proba[:,1])\n\noptimal_idx = np.argmax(tp_rate - fp_rate)\noptimal_threshold = thresholds[optimal_idx]\noptimal_threshold","7d76dee3":"test_data = pd.concat([X_test, y_test], axis=1)\ntest_data[\"pred_proba\"] = y_pred_proba[:,1]\ntest_data[\"pred\"] = y_test_pred\ntest_data[\"pred2\"] = test_data[\"pred_proba\"].apply(lambda x : 1 if x >= optimal_threshold else 0)\n\ny_test_pred2 = test_data[\"pred2\"]","7376f56d":"print(confusion_matrix(y_test, y_test_pred))\nprint(classification_report(y_test, y_test_pred2))","b032366e":"# Crooss Validate","400bf68c":"Distribution of the target variable is one of the most important things in a classification problem. So let's a close look at how its values are distributed.","c9f3253e":"# Importing libraries","815ea78f":"Before deeping into the analysis it would be benefical to examine the correlation among variables using heatmap.","541b2410":"# Categorical Features","4ea020a8":"**A General Looking at the Data**","0f7e33dd":"Let's split our features into two part, numerical and categorical, for easing our further examination.","504ddd47":"## EDA","cb41c609":"According to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\nThis dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\nAttribute Information\n* 1- id: unique identifier\n* 2- gender: \"Male\", \"Female\" or \"Other\"\n* 3- age: age of the patient\n* 4- hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n* 5- heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n* 6- ever_married: \"No\" or \"Yes\"\n* 7- work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n* 8- Residence_type: \"Rural\" or \"Urban\"\n* 9- avg_glucose_level: average glucose level in blood\n* 10- bmi: body mass index\n* 11- smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n* 12- stroke: 1 if the patient had a stroke or 0 if not\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient\nAcknowledgements (Confidential Source) - Use only for educational purposes If you use this dataset in your research, please credit the author.","ea6ba7da":"# Model Performance on Classification Tasks","881d205b":"# The Examination of Target Variable","3b3b9196":"### Data Cleaning","79a13d5c":"In the \"gender\" column there has been an undefined classification which makes no contribution to understand stroke. So let's discard this row from the analysis.","aeccfb7e":"### Handling with Skewness","327ea54c":"# Reading file","2d0d7aa3":"### Examination of Skewness","5e3d3664":"# Modelling","458007f7":"# Scaling","92a57ac1":"## The Implementattion of Logistic Methods\n\n### Train | Test Split and Scaling","6b2e6e7a":"# Numerical Features","a71afa96":"# Context","f8977009":"The column of 'Residence_type' begins with uppercase while others are not. To make a standardize grammer to prevent mistake we will change all column names into lowercase","a6e5bc57":"* this is a classification issue\n* in the begining EDA was done\n* target (y), and other features (x) analyzed\n* aim to figured the metrics\n* worked on numeric and categoric variables for skewness problem\n* handled the missing values\n* logisitic regression model was established\n* model was cross validated, and evaluated the model\n* for avoid the data leakage, handlig missing values was filled after splittin the data\n* at the end, obtained 0.84 point recall values...","37f97532":"# At the end..."}}