{"cell_type":{"a92c4fce":"code","89a8d6bb":"code","7a213d68":"code","e30a6a4e":"code","14df711c":"code","0465e7cf":"code","7694a7c7":"code","9a49e99a":"code","1ce856f2":"code","99f0a591":"code","e0bd50a1":"code","94a9631d":"code","5cad652e":"code","a7a17401":"markdown","133ec613":"markdown","0b3f8890":"markdown","73842420":"markdown","78bf6ad7":"markdown","52f890bb":"markdown","0c9bae51":"markdown","b5826600":"markdown","517cc729":"markdown"},"source":{"a92c4fce":"import bq_helper\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nhacker_news = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\", dataset_name=\"hacker_news\")\nhacker_news.head(\"full\") # print the first rows to see if it works","89a8d6bb":"# Counts of submissions, submitters, distinct urls, etc per year.\nquery = \"\"\"\nSELECT\n    extract(YEAR from timestamp) as year,\n    count(*) as submissions, \n    count(distinct `by`) as submitters,\n    \/*count(distinct url) as urls,*\/\n    sum(score) as total_votes,\n    count(*)\/count(distinct `by`) subm_per_user,\n    avg(score) as votes_per_subm,\n    approx_quantiles(score,4) as votes_quartiles,\n    (max(time)-min(time))\/count(*) as subm_every_s\nFROM `bigquery-public-data.hacker_news.full`\nWHERE `type` = 'story' AND score IS NOT NULL AND score > 0\nGROUP BY year\nORDER BY year DESC\n\"\"\"\n\nresult = hacker_news.query_to_pandas_safe(query)\nresult.set_index('year', inplace=True)\n#print(result.to_markdown(floatfmt=(\".0f\", \".0f\", \".0f\", \".0f\", \".2f\", \".2f\")))\nresult","7a213d68":"# Scatterplot of all submissions with time on the x-axis and score on a logarithmic y-axis\n\nquery = \"\"\"\nSELECT\n    time\/(365*24*60*60)+1970 as year,\n    score + pow(2,rand())-1 as score, \/* exponential jitter to make the lower values easier to interpret *\/\nFROM `bigquery-public-data.hacker_news.full`\nWHERE `type` = 'story' AND score IS NOT NULL AND score > 0\nORDER BY time DESC\nLIMIT 10000000\n\"\"\"\n\nresult = hacker_news.query_to_pandas_safe(query)\nsplot = sns.jointplot(data=result, x=\"year\", y=\"score\", marginal_kws=dict(bins=3000, rug=False), s=2, alpha=0.05, linewidth=0, height=8)\nsplot.ax_joint.set_yscale('log')\nfig = splot.fig\nfig.savefig(\"scores_over_time.png\", dpi=300) \n#result","e30a6a4e":"# count the submissions for different score intervals. Consecutive intervals are twice as big.\nquery = \"\"\"\nWITH\n    stories AS (\n        SELECT *\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE `type` = 'story' AND url != '' AND score IS NOT NULL AND score > 0\n    ),\n    intervals AS (    \n        SELECT\n            min(score) as min_score,\n            max(score) as max_score,\n            COUNT(*) as submissions,\n            SUM(score) as total_votes,\n        FROM\n            stories\n        GROUP BY\n            ceil(log(score)\/log(2))\n    ),\n    totals AS (\n        SELECT\n            count(*) AS total,\n            sum(score) AS total_score,\n        FROM stories\n    )\nSELECT\n    max_score,\n    [min_score, max_score] as score_interval,\n    submissions,\n    submissions \/ totals.total as subm_fraction,\n    (SELECT COUNT(*) FROM stories where score <= max_score) \/ totals.total as cumulative_subm_fraction,\n    total_votes,\n    total_votes \/ totals.total_score as votes_fraction,\nFROM\n    intervals,\n    totals\nORDER BY\n    min_score ASC\n\"\"\"\n\nresult = hacker_news.query_to_pandas_safe(query)\n\nmelted = pd.melt(result, id_vars='max_score', value_vars=['subm_fraction', 'votes_fraction'], var_name=\"column\", value_name=\"fraction\")\nplt.figure(figsize=(8.0, 4.8))\nsplot = sns.barplot(data=melted, x='max_score', y='fraction', hue='column')\nfig = splot.get_figure()\nfig.savefig(\"submission_and_votes_distribution_over_score_intervals.png\", dpi=300)\n\nresult.set_index('score_interval', inplace=True)\ndel result['max_score']\n#print(result.to_markdown(floatfmt=(\".0f\", \".0f\", \".6f\", \".6f\", \".0f\", \".6f\")))\nresult","14df711c":"# count the number of submissions per url\/title combination. Then count how many urls have been submitted N times.\nquery = \"\"\"\nWITH\n    stories AS (\n        SELECT *\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE `type` = 'story' AND url != '' AND score IS NOT NULL AND score > 0\n    ),\n    counts AS (\n        SELECT\n            COUNT(*) as submission_count,\n            max(score) as max_score,\n        FROM stories\n        GROUP BY url,title\n    ),\n    totals AS (\n        SELECT\n            count(*) AS total\n        FROM counts\n    )\nSELECT\n    counts.submission_count,\n    COUNT(*) as urls,\n    COUNT(*) \/ ANY_VALUE(totals.total) as fraction,\n    pow(2,avg(log(counts.max_score)\/log(2))) as max_score_log2_avg,\n    approx_quantiles(counts.max_score, 2)[OFFSET(1)] as max_score_median,\nFROM\n     counts,\n     totals\nGROUP BY counts.submission_count\nHAVING urls > 50 \/* only show submission counts with enough samples *\/\nORDER BY counts.submission_count\n\"\"\"\n\nresult = hacker_news.query_to_pandas_safe(query)\n\nmelted = pd.melt(result, id_vars='submission_count', value_vars=['max_score_log2_avg', 'max_score_median'], var_name=\"column\", value_name=\"value\")\nplt.figure(figsize=(8.0, 4.8))\nsplot = sns.barplot(data=melted, x='submission_count', y='value', hue='column')\n\nfig = splot.get_figure()\nfig.savefig(\"mean_score_for_different_submission_counts.png\", dpi=300) \n\nresult.set_index('submission_count', inplace=True)\n#print(result.to_markdown(floatfmt=(\".0f\", \".0f\", \".5f\", \".2f\", \".2f\")))\nresult","0465e7cf":"# for every submission count, plot a kernel density function (KDF) of scores\nmax_submission_count= 10\n\nquery = f\"\"\"\nWITH\n    stories AS (\n        SELECT *\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE `type` = 'story' AND url != '' AND score IS NOT NULL AND score > 0\n    )\nSELECT\n    COUNT(*) as submission_count,\n    log(max(score) + (pow(2, rand())-1))\/log(10) as log10_max_score,   \/* exponential jitter to make the lower values easier to interpret for the kdf *\/\nFROM stories\nGROUP BY url, title\nHAVING submission_count <= {max_submission_count}\nORDER BY max(time) DESC   \/* ensure useful plots with limited data *\/\nLIMIT 10000000\n\"\"\"\n\nresult = hacker_news.query_to_pandas_safe(query)\n\n# overlays multiple distplots\n# from https:\/\/github.com\/mwaskom\/seaborn\/issues\/861#issuecomment-549072489\ndef distplot_fig(data, x, hue=None, row=None, col=None, legend=True, hist=False, **kwargs):\n    \"\"\"A figure-level distribution plot with support for hue, col, row arguments.\"\"\"\n    bins = kwargs.pop('bins', None)\n    if (bins is None) and hist: \n        # Make sure that the groups have equal-sized bins\n        bins = np.histogram_bin_edges(data[x].dropna())\n    g = sns.FacetGrid(data, hue=hue, row=row, col=col, height=8, palette='colorblind')\n    g.map(sns.distplot, x, bins=bins, hist=hist, **kwargs)\n    if legend and (hue is not None) and (hue not in [x, row, col]):\n        g.add_legend(title=hue) \n    return g  \n\nsplot = distplot_fig(data=result, x=\"log10_max_score\", hue=\"submission_count\")\nfig = splot.fig\nfig.savefig(\"score_histograms_for_different_submission_counts.png\", dpi=300) \n\n\nsplot","7694a7c7":"# for the best urls+title combinations, which have been submitted more than 5 times, show the scores for every submission.\nquery = \"\"\"\nSELECT\n    title,\n    url,\n    \/*FORMAT(\"[%s...](%s)\", SUBSTR(`title`, 0, 30), url) as url,*\/\n    COUNT(*) as submissions,\n    \/*COUNT(distinct `by`) as submitters,*\/\n    \/* FORMAT_DATE(\"%Y-%m-%d\", DATE(TIMESTAMP_SECONDS(MIN(time)))) as first, *\/\n    ARRAY_AGG(score order by time ASC) as scores_by_time,\n    \/*ARRAY_AGG(EXTRACT(HOUR FROM timestamp) order by time ASC) time_of_day,*\/\n    \/*FORMAT_DATE(\"%Y-%m-%d\", DATE(TIMESTAMP_SECONDS(MAX(time)))) as last, *\/\n    approx_quantiles(score, 4) as quartiles,\n    DATE_DIFF(DATE(TIMESTAMP_SECONDS(MAX(time))),DATE(TIMESTAMP_SECONDS(MIN(time))), DAY) as days,\n    \/*DATE_DIFF(DATE(TIMESTAMP_SECONDS(MAX(time))),DATE(TIMESTAMP_SECONDS(MIN(time))), DAY) \/ count(*) as avg_day,*\/\nFROM `bigquery-public-data.hacker_news.full`\nWHERE `type` = 'story' AND url != '' AND score IS NOT NULL AND score > 2\nGROUP BY url,title\nHAVING submissions >= 5 \/*AND submitters = 1*\/ AND days <= 14\nORDER BY max(score) DESC\nLIMIT 30\n\"\"\"\n\nresult = hacker_news.query_to_pandas_safe(query)\nresult.set_index('url', inplace=True)\n#print(result.to_markdown())\nresult","9a49e99a":"# for urls with different high scores, how different are the titles?\nquery = \"\"\"\nWITH\n    stories AS (\n        SELECT *\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE `type` = 'story' AND url != '' AND score IS NOT NULL AND score > 0\n    )\nSELECT\n    \/*url,*\/\n    min(score) as worst,\n    max(score) as best,\n    ARRAY_AGG(title ORDER BY score ASC LIMIT 1)[OFFSET(0)] as worst_title,\n    ARRAY_AGG(title ORDER BY score DESC LIMIT 1)[OFFSET(0)] as best_title,\nFROM stories\nGROUP BY url\nHAVING\n    COUNT(*) >= 2\n    AND best_title != worst_title\n    AND DATE_DIFF(DATE(TIMESTAMP_SECONDS(MAX(time))),DATE(TIMESTAMP_SECONDS(MIN(time))), DAY) < 14\n    AND worst\/best < 0.8\n    AND worst\/best > 0.2\nORDER BY worst DESC\nLIMIT 50\n\"\"\"\n\nresult = hacker_news.query_to_pandas_safe(query)\n# print(result.to_markdown())\nresult","1ce856f2":"\nquery = \"\"\"\nWITH\n    stories AS (\n        SELECT *\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE `type` = 'story' AND url != '' AND score IS NOT NULL AND score > 0\n        ORDER BY time DESC\n    )\nSELECT\n    \/*url,*\/\n    ARRAY_AGG(score ORDER BY time) scores,\n    ARRAY_AGG(EXTRACT(HOUR FROM timestamp) order by time ASC) time_of_day,\n    stddev(EXTRACT(HOUR FROM timestamp)+EXTRACT(MINUTE FROM timestamp)\/60) time_of_day_stddev,\n    DATE_DIFF(DATE(TIMESTAMP_SECONDS(MAX(time))),DATE(TIMESTAMP_SECONDS(MIN(time))), DAY) as days,\nFROM stories\nGROUP BY url,title\nHAVING\n    COUNT(*) >= 3 AND COUNT(*) <= 10\n    AND max(score) > 50\n    AND days < 90\n    AND time_of_day_stddev <= 2.5\n\nORDER BY count(score) DESC, time_of_day_stddev ASC\nLIMIT 20\n\"\"\"\n\nresult = hacker_news.query_to_pandas_safe(query,max_gb_scanned = 2)\n#print(result.to_markdown())\nresult","99f0a591":"# count the unique urls for different maximal score intervals. Consecutive intervals are twice as big.\nquery = \"\"\"\nWITH\n    urls AS (\n        SELECT\n            url,\n            max(score) as max_score,\n            min(score) as min_score,\n            ceil(log(max(score))\/log(2)) as score_range,\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE `type` = 'story' AND url != '' AND score IS NOT NULL AND score > 0\n        GROUP BY url,title\n        HAVING COUNT(*) > 5\n    )\nSELECT \n    [min(max_score), max(max_score)] as max_score_interval,\n    count(*) as urls,\n    approx_quantiles(score, 4) as score_quartiles,\nFROM urls\nJOIN `bigquery-public-data.hacker_news.full` as submissions\n    ON submissions.url = urls.url\nWHERE submissions.`type` = 'story' AND submissions.score IS NOT NULL AND submissions.score > 0\nGROUP BY score_range\nORDER BY score_range\n\"\"\"\n\nresult = hacker_news.query_to_pandas_safe(query)\nresult.set_index('max_score_interval', inplace=True)\n#print(result.to_markdown(floatfmt=(\".0f\", \".0f\", \".0f\")))\nresult","e0bd50a1":"# scatter plot scores of multiple submissions of the same url against its maximum score\nmin_submission_count = 1\nquery = f\"\"\"\nWITH\n    stories AS (\n        SELECT *\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE `type` = 'story' AND url != '' AND score IS NOT NULL AND score > 0\n    ),\n    best AS (\n        SELECT\n            url,\n            title,\n            max(score) as max_score,\n        FROM stories\n        GROUP BY url,title\n        HAVING count(*) > {min_submission_count}\n    )\nSELECT\n    \/*best.url,*\/\n    submission.score + pow(2, rand())-1 as score, \/* exponential jitter to make the lower values easier to interpret *\/\n    best.max_score + pow(2, rand())-1 as max_score,\nFROM best\nJOIN stories as submission\n    ON submission.url = best.url AND submission.title = best.title\nORDER BY best.max_score DESC\nLIMIT 10000000\n\"\"\"\n\nresult = hacker_news.query_to_pandas_safe(query)\nplt.figure(figsize=(8, 8))\n\nsplot = sns.scatterplot(data=result, x=\"max_score\", y=\"score\", s=4, alpha=0.1, linewidth=0)\nsplot.set(xscale='log')\nsplot.set(yscale='log')\n\nfig = splot.get_figure()\nfig.savefig(\"every_url_scores_against_maxscore.png\", dpi=300) \n\n\n#result","94a9631d":"# plot all submission scores against the time of day\nquery = \"\"\"\nSELECT\n    score + (pow(2,rand())-1) as score, \/* exponential jitter to make the lower values easier to interpret *\/\n    EXTRACT(HOUR FROM timestamp)+EXTRACT(MINUTE FROM timestamp)\/60 as hour_utc,\nFROM `bigquery-public-data.hacker_news.full`\nWHERE `type` = 'story' AND url != '' AND score IS NOT NULL AND score > 0\nORDER BY time DESC \/* a lower limit skips older submissions *\/\nLIMIT 10000000\n\"\"\"\n\nresult = hacker_news.query_to_pandas_safe(query)\nsplot = sns.jointplot(data=result, x='hour_utc', y='score', s=2, alpha=0.05, linewidth=0, marginal_kws=dict(bins=3000, rug=False), height=6.4)\n#splot = sns.jointplot(data=result, x='hour', y='score', kind=\"kde\", levels=30, height=20)\nsplot.ax_joint.set_yscale('log')\n\nfig = splot.fig\nfig.savefig(\"scores_over_hour.png\", dpi=300) \n\nsplot","5cad652e":"# for all urls which have been submitted 5 times, look up the hour of the submission with the highest score. Visualize these hours in a histogram.\nquery = \"\"\"\nWITH maxed AS (\n    SELECT\n        url,\n        max(score) max_score,\n    FROM `bigquery-public-data.hacker_news.full`\n    WHERE `type` = 'story' AND url != '' AND score IS NOT NULL AND score > 0\n    GROUP BY url,title\n    HAVING COUNT(*) >= 5 AND max(score)-min(score) >= 1\n    )\nSELECT\n    EXTRACT(HOUR FROM timestamp)+EXTRACT(MINUTE FROM timestamp)\/60 as hour,\nFROM\n    maxed\n\/* to access timestamp of submission with max_score: *\/            \nJOIN `bigquery-public-data.hacker_news.full` as raw\n    ON raw.url = maxed.url AND raw.score = max_score\nLIMIT 10000000\n        \"\"\"\n\nresult = hacker_news.query_to_pandas_safe(query)\nresult\nplt.figure(figsize=(6.4, 4.8))\nsplot = sns.distplot(result, bins=24*6, kde=False)\n\nfig = splot.get_figure()\nfig.savefig(\"histogram_of_best_submission_hours.png\", dpi=300) ","a7a17401":"# Dataset Overview","133ec613":"# How many resubmissions exist?","0b3f8890":"# Do the best submissions have consistent scores on re-submission?","73842420":"# Environment setup","78bf6ad7":"# Are scores in different scoring ranges consistent?","52f890bb":"# How does submission time affect scores?","0c9bae51":"# How many submissions received similar scores?","b5826600":"## submissions at the same time, with same url and title","517cc729":"## Clickbait titles"}}