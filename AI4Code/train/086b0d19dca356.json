{"cell_type":{"c633296c":"code","488b5c4f":"code","1a2530e1":"code","3bdb5cbb":"code","7fcd7644":"code","ff85d04b":"code","6ac5a984":"code","37d3f64d":"code","fea051e4":"code","50ce18b3":"code","42b15411":"code","c7171913":"code","fa26a0d3":"code","b345061d":"code","3622554e":"code","a4173967":"code","8185f56d":"code","f847c739":"code","44781d2c":"code","2a2fadab":"code","009766f0":"code","8bb10029":"code","8aad4c57":"code","3d7c29a4":"code","23407eb9":"code","d7903649":"code","ab21621e":"code","8a9e3b74":"code","186dd115":"code","3649d7d4":"code","aafb561a":"code","851205a4":"code","5a6c2705":"code","3d7e3efc":"code","7a685635":"markdown","b935c53e":"markdown","a2ec7642":"markdown","bdf82a14":"markdown","fb98e716":"markdown","26de2099":"markdown","50c9c875":"markdown","fea3d860":"markdown","f63f15e7":"markdown","53443755":"markdown","16e0795a":"markdown","1750c4ac":"markdown","f904b694":"markdown","6bb0a507":"markdown","7038656d":"markdown","aeadf584":"markdown","8a0b24b7":"markdown","3d6393d7":"markdown","a9ed1b05":"markdown","269925f7":"markdown","2d5251c9":"markdown","a1c8c95b":"markdown","74c644ed":"markdown","3ea98b4a":"markdown","81df2774":"markdown","4ce65028":"markdown","ab9d8e1e":"markdown","c5be9244":"markdown","d1fe610a":"markdown","004ab2de":"markdown","4351b35f":"markdown","ae03c242":"markdown","2d8393c8":"markdown","49d0350a":"markdown","0b9c653f":"markdown","67f5c67a":"markdown","fede0b07":"markdown","ec13b5e8":"markdown","bf083b51":"markdown","37c246a8":"markdown","3ab94121":"markdown","fbc351b7":"markdown","199f7bf0":"markdown","6b6544e2":"markdown","2cfd6aa5":"markdown","50e56fe9":"markdown","ac50c847":"markdown","c98e4f6c":"markdown","1a79b658":"markdown","8a9a3be3":"markdown","1e077fc3":"markdown"},"source":{"c633296c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport string\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom nltk.tokenize import word_tokenize","488b5c4f":"# Filter warnings out of outputs\nimport warnings\nwarnings.filterwarnings('ignore')","1a2530e1":"# Import Kaggle MBTI data from your local folder\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\ndf_train.info()\ndf_test.info()","3bdb5cbb":"f, ax = plt.subplots(figsize=(10, 10))\nsns.countplot(df_train['type'].sort_values(ascending=False))\nplt.title(\"Count of Personality Types\")\nplt.xlabel(\"Personality Type\")\nplt.ylabel(\"Count\")","7fcd7644":"# Save the 'Id' column for later use in model predictions\ndf_test_Id = df_test['id']\n\n# Now drop the 'Id' column from the base dataframe\ndf_test.drop(\"id\", axis=1, inplace=True)\n\n# Lambda expressions written to convert the personality type into the correct attribute encodings\ndf_train['E\/I'] = df_train['type'].apply(lambda x: x[0] == 'E').astype('int')\ndf_train['S\/N'] = df_train['type'].apply(lambda x: x[1] == 'N').astype('int')\ndf_train['T\/F'] = df_train['type'].apply(lambda x: x[2] == 'T').astype('int')\ndf_train['J\/P'] = df_train['type'].apply(lambda x: x[3] == 'J').astype('int')\n\n# Check encodings\ndf_train.head()","ff85d04b":"# Split off personality attributes from train data into y_train for later use in the modelling section\ny_train = df_train[['E\/I', 'S\/N', 'T\/F', 'J\/P']]\n\n# Create mask varaibles for test and train subsetting later on\nntrain = df_train.shape[0]\nntest = df_test.shape[0]\n\n# Concatenate train and test dataframes\nall_data = pd.concat((df_train, df_test)).reset_index(drop=True)\n\n# Check all data shape\nprint(\"all_data size is : {}\".format(all_data.shape))","6ac5a984":"# Split posts within the posts column on the triple pipe (|||)\nall_data['split_posts'] = all_data['posts'].str.split('\\|\\|\\|')\nall_data['split_posts'] = all_data['split_posts'].apply(', '.join)\n\n# Transform all text to lowercase\nall_data['split_posts'] = all_data['split_posts'].str.lower()\n\n# Detect and replace any urls with the string 'url-web'\npattern_url = r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\nsubs_url = r'url-web'\nall_data['split_posts'] = all_data['split_posts'].replace(to_replace=pattern_url, value=subs_url, regex=True)\n\n\n# Create and apply a function for removing punctuation from posts\ndef remove_punctuation(post):\n    '''\n    Strips all punctuation tokens present in the string packages punctuation object from the desired colunm.\n\n    Parameters\n    ----------\n\n    post: str\n        str object containing the text to be stripped of punctuation.\n\n    Returns\n    -------\n\n    method: remove_punctuation\n        method of removing punctuations from a given dataframe column\n   '''\n    return ''.join([l for l in post if l not in string.punctuation])\n\nall_data['posts_no_punct'] = all_data['split_posts'].apply(remove_punctuation)\n\n# Tokenise the posts text into individual words\nall_data['words'] = all_data['posts_no_punct'].apply(word_tokenize)\n\n# Check preprossesing steps were successful\nall_data.head()","37d3f64d":"# Subset all_data \ntrain_wordclouds = all_data[:ntrain]\n\n# Group data by personality type\ngrouped_wordclouds = train_wordclouds[['type','words']]\ngrouped_wordclouds = grouped_wordclouds.groupby('type').sum()\ngrouped_wordclouds = grouped_wordclouds.reset_index()\n\n# Check grouped personality type words\ngrouped_wordclouds.head(20)","fea051e4":"# Instatiate figure and axis and the number of subplots to use\nfig, ax = plt.subplots(nrows=4, ncols=4)\nfig.set_size_inches(22, 10)\n\n# Create a list containing all the words for all the personalities then loop through these creating a wordcloud for each one\nrandom = grouped_wordclouds['words']\nfor i, j in grouped_wordclouds.iterrows():\n    text = ', '.join(random[i])\n\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud().generate(text)\n\n    # Display the generated images:\n    plt.subplot(4, 4, (i+1))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(str(grouped_wordclouds['type'].iloc[i]))","50ce18b3":"# Create a list containing all the words for all the personalities then loop through these creating a wordcloud for the total dataset\ngrouped_wordclouds = grouped_wordclouds['words']\n\nvocab = []\nfor i in random:\n    vocab.append(i)\n\nflat_vocab = []\nfor sublist in vocab:\n    for item in sublist:\n        flat_vocab.append(item)\n\ntext = ', '.join(word for word in flat_vocab)\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)\n\n# Display the generated image:\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.title('Total vocab Wordcloud')\nplt.show()","42b15411":"from sklearn.feature_extraction.text import TfidfVectorizer","c7171913":"# Create a TfidfVectorizer and apply it to the data\nTFIDF_vect = TfidfVectorizer()\nall_data_TFIDF = TFIDF_vect.fit_transform(all_data['posts'])\n\n# Check the TfidfVectorizer shape\nall_data_TFIDF.shape","fa26a0d3":"# Create a TfidfVectorizer with better parameter usage and apply it to the data\nTFIDF_vect = TfidfVectorizer(lowercase=True, stop_words='english', max_df=0.5, min_df=0.01, max_features=10000)\nall_data_TFIDF = TFIDF_vect.fit_transform(all_data['posts'])\n\n# Check the new TfidfVectorizer shape\nall_data_TFIDF.shape","b345061d":"# Split into train and test and check that shapes match\ntrain = all_data_TFIDF[:ntrain]\ntest = all_data_TFIDF[ntrain:]\nprint(train.shape)\nprint(test.shape)\nprint(y_train.shape)","3622554e":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nimport optuna","a4173967":"# Define fuction to calculate the log loss over 5 cross validation sets\ndef log_loss_cv(model, category):\n    '''\n    Gets the average log loss score for a model across a given number of cross validation sets.\n\n    Parameters\n    ----------\n\n    model: model_object\n        model object containing a trained sklearn model on which the score can be calculated.\n\n    category: dataframe\n        dataframe object containing the specific response variable to use as the response.\n\n    Returns\n    -------\n\n    log_loss: int\n        average log loss score for a given model and response variable.\n\n    '''\n\n    log_loss = -cross_val_score(model, train, y_train[category], scoring=\"neg_log_loss\", cv=5)\n    return(log_loss)","8185f56d":"# Create base Logistic Regression models for personality attributes\nlogreg_EI = make_pipeline(LogisticRegression())\nlogreg_SN = make_pipeline(LogisticRegression())\nlogreg_TF = make_pipeline(LogisticRegression())\nlogreg_JP = make_pipeline(LogisticRegression())\n\n# Check the cross-validation scores of the Logistic Regression base models on the train data\nEI_score = log_loss_cv(logreg_EI, 'E\/I')\nSN_score = log_loss_cv(logreg_SN, 'S\/N')\nTF_score = log_loss_cv(logreg_TF, 'T\/F')\nJP_score = log_loss_cv(logreg_JP, 'J\/P')\n\n# Print out model score for each category\nprint('Extrovert\/Introvert Score: ', EI_score.mean())\nprint('Sensing\/Intuition Score: ', SN_score.mean())\nprint('Thinking\/Feeling Score: ', TF_score.mean())\nprint('Judging\/Percieving Score: ', JP_score.mean())","f847c739":"# Create base Multinomial Naive Bayes models for personality attributes\nMultiNB_EI = make_pipeline(MultinomialNB())\nMultiNB_SN = make_pipeline(MultinomialNB())\nMultiNB_TF = make_pipeline(MultinomialNB())\nMultiNB_JP = make_pipeline(MultinomialNB())\n\n# Check the cross-validation scores of the Multinomial Naive Bayes base models on the train data\nEI_score = log_loss_cv(MultiNB_EI, 'E\/I')\nSN_score = log_loss_cv(MultiNB_EI, 'S\/N')\nTF_score = log_loss_cv(MultiNB_EI, 'T\/F')\nJP_score = log_loss_cv(MultiNB_EI, 'J\/P')\n\n# Print out model score for each category\nprint('Extrovert\/Introvert Score: ', EI_score.mean())\nprint('Sensing\/Intuition Score: ', SN_score.mean())\nprint('Thinking\/Feeling Score: ', TF_score.mean())\nprint('Judging\/Percieving Score: ', JP_score.mean())","44781d2c":"# Create base AdaBoost models for personality attributes\nAdaB_EI = make_pipeline(AdaBoostClassifier())\nAdaB_SN = make_pipeline(AdaBoostClassifier())\nAdaB_TF = make_pipeline(AdaBoostClassifier())\nAdaB_JP = make_pipeline(AdaBoostClassifier())\n\n# Check the cross-validation scores of the AdaBoost base models on the train data\nEI_score = log_loss_cv(AdaB_EI, 'E\/I')\nSN_score = log_loss_cv(AdaB_EI, 'S\/N')\nTF_score = log_loss_cv(AdaB_EI, 'T\/F')\nJP_score = log_loss_cv(AdaB_EI, 'J\/P')\n\n# Print out model score for each category\nprint('Extrovert\/Introvert Score: ', EI_score.mean())\nprint('Sensing\/Intuition Score: ', SN_score.mean())\nprint('Thinking\/Feeling Score: ', TF_score.mean())\nprint('Judging\/Percieving Score: ', JP_score.mean())","2a2fadab":"# Define an objective function to be minimized.\ndef objective(trial):\n\n    # Invoke suggest methods of a Trial object to generate hyperparameters.\n    penalty = trial.suggest_categorical('penalty', ['l1', 'l2'])\n    tol = trial.suggest_loguniform('tol', 1e-10, 1)\n    C = trial.suggest_loguniform('C', 1e-10, 1)\n    random_state = trial.suggest_int('random_state', 1, 10)\n    max_iter = trial.suggest_int('max_iter', 1000, 10000)\n    warm_start = trial.suggest_categorical('warm_start', [True, False])\n\n    # Create a variable containing the model and a set of selected hyperparameter values\n    classifier_obj = LogisticRegression(penalty=penalty,\n                                        tol=tol,\n                                        C=C,\n                                        random_state=random_state,\n                                        max_iter=max_iter,\n                                        warm_start=warm_start)\n\n    # Define x and y variables\n    x = train\n    y = y_train['E\/I']\n\n    # Check cross validation score of the model based on x and y values\n    score = cross_val_score(classifier_obj, x, y, scoring=\"neg_log_loss\")\n    accuracy = score.mean()\n\n    # A objective value linked with the Trial object.\n    return 1.0 - accuracy\n\n# Create a new study and invoke optimization of the objective function\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=1000)","009766f0":"# Used to print the optimal hyperparameters found by the objective function\nstudy.best_params","8bb10029":"# Run Logistic Regression models using optimised hyperparameters\nlogreg_EI = make_pipeline(LogisticRegression(penalty='l1',\n                                             tol=0.003850701503405173,\n                                             C=0.9981811566847507,\n                                             random_state=1,\n                                             max_iter=1762,\n                                             warm_start=True))\nlogreg_SN = make_pipeline(LogisticRegression(penalty='l1',\n                                             tol=0.003850701503405173,\n                                             C=0.9981811566847507,\n                                             random_state=1,\n                                             max_iter=1762,\n                                             warm_start=True))\nlogreg_TF = make_pipeline(LogisticRegression(penalty='l1',\n                                             tol=0.003850701503405173,\n                                             C=0.9981811566847507,\n                                             random_state=1,\n                                             max_iter=1762,\n                                             warm_start=True))\nlogreg_JP = make_pipeline(LogisticRegression(penalty='l1',\n                                             tol=0.003850701503405173,\n                                             C=0.9981811566847507,\n                                             random_state=1,\n                                             max_iter=1762,\n                                             warm_start=True))","8aad4c57":"# Check the cross-validation score of the model from the train data\nEI_score = log_loss_cv(logreg_EI, 'E\/I')\nSN_score = log_loss_cv(logreg_SN, 'S\/N')\nTF_score = log_loss_cv(logreg_TF, 'T\/F')\nJP_score = log_loss_cv(logreg_JP, 'J\/P')\n\n# Print out model score for each category\nprint('Extrovert\/Introvert Score: ' + EI_score.mean())\nprint('Sensing\/Intuition Score: ' + SN_score.mean())\nprint('Thinking\/Feeling Score: ' + TF_score.mean())\nprint('Judging\/Percieving Score: ' + JP_score.mean())","3d7c29a4":"# Define an objective function to be minimized.\ndef objective(trial):\n\n    # Invoke suggest methods of a Trial object to generate hyperparameters.\n    alpha = trial.suggest_loguniform('alpha', 1e-10, 1)\n    fit_prior = trial.suggest_categorical('fit_prior', [True, False])\n\n    # Create a variable containing the model and a set of selected hyperparameter values\n    classifier_obj = MultinomialNB(alpha=alpha,\n                                   fit_prior=fit_prior)\n\n    # Define x and y variables\n    x = train\n    y = y_train['E\/I']\n\n    # Check cross validation score of the model based on x and y values\n    score = cross_val_score(classifier_obj, x, y, scoring=\"neg_log_loss\")\n    accuracy = score.mean()\n\n    # A objective value linked with the Trial object.\n    return 1.0 - accuracy\n\n# Create a new study and invoke optimization of the objective function\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=1000)","23407eb9":"# Used to print the optimal hyperparameters found by the objective function\nstudy.best_params","d7903649":"# Run Multinomial Naive Bayes models using optimised hyperparameters\nMultiNB_EI = make_pipeline(MultinomialNB(alpha=0.08874918773669986,\n                                         fit_prior=True))\nMultiNB_SN = make_pipeline(MultinomialNB(alpha=0.08874918773669986,\n                                         fit_prior=True))\nMultiNB_TF = make_pipeline(MultinomialNB(alpha=0.08874918773669986,\n                                         fit_prior=True))\nMultiNB_JP = make_pipeline(MultinomialNB(alpha=0.08874918773669986,\n                                         fit_prior=True))","ab21621e":"# Check the cross-validation score of the model from the train data\nEI_score = log_loss_cv(MultiNB_EI, 'E\/I')\nSN_score = log_loss_cv(MultiNB_EI, 'S\/N')\nTF_score = log_loss_cv(MultiNB_EI, 'T\/F')\nJP_score = log_loss_cv(MultiNB_EI, 'J\/P')\n\n# Print out model score for each category\nprint('Extrovert\/Introvert Score: ' + EI_score.mean())\nprint('Sensing\/Intuition Score: ' + SN_score.mean())\nprint('Thinking\/Feeling Score: ' + TF_score.mean())\nprint('Judging\/Percieving Score: ' + JP_score.mean())","8a9e3b74":"# Define an objective function to be minimized.\ndef objective(trial):\n\n    # Invoke suggest methods of a Trial object to generate hyperparameters.\n    n_estimators = trial.suggest_int('n_estimators', 1, 100)\n    learning_rate = trial.suggest_loguniform('learning_rate', 1e-10, 1)\n    algorithm = trial.suggest_categorical('algorithm', ['SAMME', 'SAMME.R'])\n    random_state = trial.suggest_int('random_state', 1, 10)\n\n    # Create a variable containing the model and a set of selected hyperparameter values\n    classifier_obj = AdaBoostClassifier(n_estimators=n_estimators,\n                                        learning_rate=learning_rate,\n                                        algorithm=algorithm,\n                                        random_state=random_state)\n\n    # Define x and y variables\n    x = train\n    y = y_train['E\/I']\n\n    # Check cross validation score of the model based on x and y values\n    score = cross_val_score(classifier_obj, x, y, scoring=\"neg_log_loss\")\n    accuracy = score.mean()\n\n    # A objective value linked with the Trial object.\n    return 1.0 - accuracy\n\n# Create a new study and invoke optimization of the objective function\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)","186dd115":"# Used to print the optimal hyperparameters found by the objective function\nstudy.best_params","3649d7d4":"# Run AdaBoost models using optimised hyperparameters\nAdaB_EI = make_pipeline(AdaBoostClassifier(n_estimators=84,\n                                           learning_rate=0.0025576981225485613,\n                                           algorithm='SAMME.R',\n                                           random_state=4))\nAdaB_SN = make_pipeline(AdaBoostClassifier(n_estimators=84,\n                                           learning_rate=0.0025576981225485613,\n                                           algorithm='SAMME.R',\n                                           random_state=4))\nAdaB_TF = make_pipeline(AdaBoostClassifier(n_estimators=84,\n                                           learning_rate=0.0025576981225485613,\n                                           algorithm='SAMME.R',\n                                           random_state=4))\nAdaB_JP = make_pipeline(AdaBoostClassifier(n_estimators=84,\n                                           learning_rate=0.0025576981225485613,\n                                           algorithm='SAMME.R',\n                                           random_state=4))","aafb561a":"# Check the cross-validation score of the model from the train data\nEI_score = log_loss_cv(AdaB_EI, 'E\/I')\nSN_score = log_loss_cv(AdaB_EI, 'S\/N')\nTF_score = log_loss_cv(AdaB_EI, 'T\/F')\nJP_score = log_loss_cv(AdaB_EI, 'J\/P')\n\n# Print out model score for each category\nprint('Extrovert\/Introvert Score: ' + EI_score.mean())\nprint('Sensing\/Intuition Score: ' + SN_score.mean())\nprint('Thinking\/Feeling Score: ' + TF_score.mean())\nprint('Judging\/Percieving Score: ' + JP_score.mean())","851205a4":"# Fit final models to training data\nlogreg_EI.fit(train, y_train['E\/I'])\nlogreg_SN.fit(train, y_train['S\/N'])\nlogreg_TF.fit(train, y_train['T\/F'])\nlogreg_JP.fit(train, y_train['J\/P'])","5a6c2705":"# Generate predictions\nEI_y_pred_test = logreg_EI.predict(test)\nSN_y_pred_test = logreg_SN.predict(test)\nTF_y_pred_test = logreg_TF.predict(test)\nJP_y_pred_test = logreg_JP.predict(test)","3d7e3efc":"# Create submission dataframe and add predictions to it\nsub = pd.DataFrame()\nsub['id'] = df_test_Id\nsub['mind'] = EI_y_pred_test\nsub['energy'] = SN_y_pred_test\nsub['nature'] = TF_y_pred_test\nsub['tactics'] = JP_y_pred_test\n\n# Write submission dataframe to a csv for submission\nsub.to_csv('submission.csv', index=False)","7a685635":"The aim of this project was to predict a person's MBTI personality type according to their online social media posts. To tackle this challenge we employed the use of Machine Learning Classification and NLP techniques. Classification of each MBTI type is based off four categories, each of which have two classes that an individual can be classified as. Each category was created as a target variable with binary labels assigned to each class. The data was preprocessed to remove stopwords and a cross validation scoring function was setup. Three classification models were tested for best output and hyperparameter tuning was done to optimize the models to yield the best results. Using the cross validation scoring function, we concluded that the Logistic Regression model performed the best in comparison to the Mulinomial Naive Bayes and AdaBoost models. We have concluded that Logistic Regression performed better due to us perfomring binary classification on each category rather than multiclass classification. In future, use of resampling methods may be employed to balance the data for the categories and potentially better prediction results.","b935c53e":"# Predicting a person's MBTI personality type based on their online presence and communications.","a2ec7642":"We chose to use [Optuna](https:\/\/optuna.org) to perform our hyperparameter tuning for the following reasons:\n1. Other frameworks separately define the search space and the objective function. In Optuna, the search spaces are defined inside the objective function, and all hyperparameters are defined on the run. This feature makes the code written in Optuna more modulated and easier to modify.\n2. Optuna can parallelize optimization. To initialise parallelization, you can simply execute multiple optimization processes, and Optuna will automatically share trials in background.\n3. Pruning feature automatically stops unpromising trials at the early stages of the training (a.k.a. automated early-stopping).\n4. Optuna is based on a Bayesian optimization algorithm, which accelerates your hyperparameter search. The pruning and parallelization features help try out large number of hyperparameter combinations in a short time.\n\nThis enabled us to perform more iterations of each hyperparameter tuning step that we would have performed with RandomisedSearchCV and thus create more accurate models. Furtuermore Optuna allowed us to save time and computing resources when performing the hyperparameter tuning","bdf82a14":"This dataframe now contained the individuals personality type, their posts on social media as well as the binarised version of their personailty attributes(**E\/I, S\/N, T\/F, and J\/P columns**)","fb98e716":"<a id=\"section17\"><\/a>\n### Importing the necessary librairies for modeling and scoring","26de2099":"This was done in order to get cleaned and formatted text for use in generating WordClouds","50c9c875":"<a id=\"section18\"><\/a>\n### Creating a cross-validation log loss scoring function","fea3d860":"<a id=\"section1\"><\/a>\n## 1. List of non-standard packages used in this Notebook","f63f15e7":"We created this fuction as it allows us to use cross validation scoring as opposed to tranditional train test spilt scoring. This enabled better judging of scores during the hyperparameter tuning stage. As an added benfit the metric that kaggle uses to score submissions (log loss) could be used during the model building process","53443755":"<a id=\"section10\"><\/a>\n### Wordclouds","16e0795a":"This dataframe now contained the binarised version of the individuals personality attributes(**E\/I, S\/N, T\/F, and J\/P columns**), their personality type, their posts with pipes(|||) removed and urls converted to url_web(**split_posts column**), their posts with no punctuation (**posts_no_punct column**) and finally a list of all the individual words present in their posts (**words column**)","1750c4ac":"WordClouds are often used to represent the frequency or importance of words from a given body of text. Here we made use of word clouds to represent the words that were most important or frequently occuring according to each MBTI personality type. \nThis was performed to aid in decision making for what cleaning processes need to be applied to the data so that it could be used in the modeling steps of this kernal","f904b694":"<a id=\"section24\"><\/a>\n#### Final modeling with hyperparameters","6bb0a507":"<a id=\"section8\"><\/a>\n### Concatenate train and test data into one dataframe","7038656d":"<a id=\"section26\"><\/a>\n#### Final modeling with hyperparameters","aeadf584":"We elected to use TfidfVectorizer as it enables us to remove stopwords and set boundaries for removal of words that over occur in the dataset. This particualr vectoriser supports frequency based removal of words and the ability to limit the feature size of our dataset to a resonable size","8a0b24b7":"<a id=\"section28\"><\/a>\n#### Final modeling with hyperparameters","3d6393d7":"<a id=\"section29\"><\/a>\n## 7. Final model fitting and submission creation","a9ed1b05":"<a id=\"section21\"><\/a>\n### AdaBoost Modeling\n\nAdaboost is an ensemble technique that combines multiple 'weak classifiers' into a single 'strong classifier'. Ensemble learning methods are meta-algorithms that combine several machine learning methods into a single predictive model to increase performance. Ensemble methods can decrease variance using bagging approach, bias using a boosting approach, or improve predictions using stacking approach.\n\n\nPros:\n- Reduces overfitting\n- Increases performance\n\nCons:\n- High bias\n- Long runtime","269925f7":"<a id=\"section7\"><\/a>\n### Create response variables for the different attributes of the personality traits","2d5251c9":"<a id=\"section25\"><\/a>\n### Multinomial Naive Bayes","a1c8c95b":"<a id=\"section16\"><\/a>\n## 5. Initial Modelling","74c644ed":"<a id=\"section15\"><\/a>\n### Split the processed data into train and test sets for use in the modeling section","3ea98b4a":"This was done in order to simplify the preprocessing of posts and to ensure that train and test vectors match in size later on during the modeling process.","81df2774":"The Myers\u2013Briggs Type Indicator (MBTI) is an introspective self-report questionnaire with the purpose of indicating differing psychological preferences in how people perceive the world around them and make decisions. \n\nThe MBTI was constructed by Katharine Cook Briggs and her daughter Isabel Briggs Myers. It is based on the conceptual theory proposed by Swiss psychiatrist Carl Jung, who had speculated that humans experience the world using four principal psychological functions \u2013 sensation, intuition, feeling, and thinking \u2013 and that one of these four functions is dominant for a person most of the time.\n\nAccording to the Myers & Briggs Foundation, there are 16 personality types based on four key dimensions that could be used to categorize people:\n- Introversion vs. Extraversion\n- Sensing vs. Intuition\n- Thinking vs. Feeling\n- Judging vs. Perceiving\n\nBelow, are the key dimensions that makeup each personality type:\n\n<img src=\"https:\/\/miro.medium.com\/max\/700\/0*afKj6ym-KtsVkJF8.png\" alt=\"Drawing\" style=\"width: 500px;\"\/>\n\nWith the help of machine learning, we are able to breakdown and classify these key dimensions into the 16 personality types. Machine learning (ML) is a category of algorithm that allows software applications to become more accurate in predicting outcomes without being explicitly programmed. The basic premise of machine learning is to build algorithms that can receive input data and use statistical analysis to predict an output while updating outputs as new data becomes available. In this section of our analysis, we relied on two specific techniques of ML, namely: Natural Language Processing (NLP) and Classification.\n\n<img src=\"https:\/\/www.kdnuggets.com\/wp-content\/uploads\/nlp-text-mining-venn.jpg\" alt=\"Drawing\" style=\"width: 500px;\"\/>\n\nNLP processing is a vitally important part of any data scientist's toolkit. Put simply, it is the art of processing unstructured text data into a format that can be interpreted and used by computers. This unfortunately is no simple task and often requires unique solutions to a particular datasets problems. In this kernel we will show you how we went about processing unstructured text data, in the form of posts from a particular website, into a usable form which was then used to predict the given positers MBTI personality type. This kind of text processing architecture and classification has many other applications such as in use for sentiment analysis or in use for recommendation engines. \n\nClassification on the other hand is branch of statistical learning and ML used for identifying to which of a set of categories a new observation belongs, on the basis of a training set of data containing observations whose category membership is known.  Classification is an example of pattern recognition.\n\nIn the terminology of machine learning, classification is considered an instance of supervised learning, i.e., learning where a training set of correctly identified observations is available, and involves grouping data into categories based on some measure of inherent similarity or distance.\n\nOften, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features. These features may variously be categorical (e.g. \"A\", \"B\", \"AB\" or \"O\", for blood type), ordinal (e.g. \"large\", \"medium\" or \"small\") and even integer-valued (e.g. the number of occurrences of a particular word in an email). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.\n\nThanks to these ML techniques, we are able to decipher the personality of an indivisual base on the above mentioned \"key dimensions\" that makeup a personality type in to the 16 MBT Indicators envisioned by Katharine and Isabel.\n\nBelow is a list showing the 16 different personility types in greater detail:\n\n<img src=\"https:\/\/www.kona.com.au\/wp-content\/uploads\/2017\/02\/MBTI-Matrix.jpg\" alt=\"Drawing\" style=\"width: 500px;\"\/>\n","4ce65028":"This was done in order to enable us to use binary classification techniques to save on computation time and resources and to make the whole classification process easier to work with","ab9d8e1e":"The above word cloud graphic is a visual representation of the most important or frequently occuring words over the entire dataset","c5be9244":"We chose to filter out the warnings that python prints to the console due to verbosity. You may want to comment this section out if you are experiencing any unexpected problems with the notebook","d1fe610a":"<a id=\"section4\"><\/a>\n### Importing the necessary librairies for EDA","004ab2de":"<a id=\"section2\"><\/a>\n## 2. Introduction","4351b35f":"We concluded several intersting things from our EDA analysis:\n1. The dataset is highly skewed in favour of introverted types and class imbalances are rife. Bootstrapping or resampling techniques could be used to balance this data but we chose not to do this due to word frequency measures being a main pillar of our analysis \n2. There are several words, such as 'one', 'think', 'thing', 'people' etc., that occur multiple times aross the body of text. (these words can be viewed in the Total vocab Wordcloud plot)\n3. Several stopwords such as 'im' and 'dont' etc., occur very frequently across all the personality types\n\nThese discoveries necesitated the removal of english stopwords and the use of TdidfVectoriser in our final data preparation step","ae03c242":"<a id=\"section30\"><\/a>\n## 8. Conclusion and final thoughts","2d8393c8":"<a id=\"section5\"><\/a>\n### Importing the train and test data","49d0350a":"1. [List of non-standard packages used in this Notebook](#section1)\n2. [Introduction](#section2)\n3. [Exploratory Data Analysis](#section3)\n    - [Importing the necessary librairies for EDA](#section4)\n    - [Importing the train and test data](#section5)\n    - [Check the distribution of the Response Variable (Personality Type)](#section6)\n    - [Create response variables for the different attributes of the personality traits](#section7)\n    - [Concatenate train and test data into one dataframe](#section8)\n    - [Preprocessing of text for use in visualisation](#section9)\n    - [Wordclouds](#section10)\n    - [EDA Conclusions](#section11)\n4. [Data Preprocessing and Final preparation for modeling](#section12)\n    - [Importing the necessary librairies for preprocessing and final data preparation](#section13)\n    - [Processing raw data using TfidfVectorizer](#section14)\n    - [Split the processed data into train and test sets for use in the modeling section](#section15)\n5. [Initial Modelling](#section16)\n    - [Importing the necessary librairies for modeling and scoring](#section17)\n    - [Creating a cross-validation log loss scoring function](#section18)\n    - [Logistic Regression Modeling](#section19)\n    - [Multinomial Naive Bayes Modeling](#section20)\n    - [AdaBoost Modeling](#section21)\n6. [Hyperparameter tuning](#section22)\n    - [Logistic Regression](#section23)\n        - [Final modeling with hyperparameters](#section24)\n    - [Multinomial Naive Bayes](#section25)\n        - [Final modeling with hyperparameters](#section26)\n    - [AdaBoost](#section27)\n        - [Final modeling with hyperparameters](#section28)\n7. [Final model fitting and submission creation](#section29)\n8. [Conclusion and final thoughts](#section30)","0b9c653f":"<a id=\"section6\"><\/a>\n### Check the distribution of the Response Variable (Personality Type)","67f5c67a":"The above word cloud graphic is a visual representation of the most important or frequently occuring words per MBTI peronality type","fede0b07":"<a id=\"section9\"><\/a>\n### Preprocessing of text for use in visualisation","ec13b5e8":"<a id=\"section23\"><\/a>\n### Logistic Regression","bf083b51":"<a id=\"section14\"><\/a>\n### Processing raw data using TfidfVectorizer","37c246a8":"This dataframe contained the 16 personality types as well as all words associated with that specific personality type(**words column**)","3ab94121":"<a id=\"section11\"><\/a>\n### EDA Conclusions","fbc351b7":"<a id=\"section19\"><\/a>\n### Logistic Regression Modeling\n\nLogistic Regression measures the relationship between the dependent variable (our label, what we want to predict) and the one or more independent variables (our features), by estimating probabilities using it\u2019s underlying logistic function. \n\nPros:\n- Low variance\n- Provides probabilities for outcomes\n- Works well with diagonal (feature) decision boundaries\n\nCons:\n- High bias","199f7bf0":"* [Pillow](https:\/\/pillow.readthedocs.io\/en\/stable\/) - friendly PIL (Python Image Library) fork\n* [WordCloud](https:\/\/github.com\/amueller\/word_cloud) - WordCloud, STOPWORDS, ImageColorGenerator\n* [Natural Language Toolkit](https:\/\/github.com\/amueller\/word_cloud) - imported the relevant portions of the package\n* [Optuna](https:\/\/optuna.org) - used in hyperparameter tuning","6b6544e2":"<a id=\"section22\"><\/a>\n## 6. Hyperparameter tuning","2cfd6aa5":"You generally want to check the distribution of your response variable to ascertain whether scaling or transformations need to be performed","50e56fe9":"<a id=\"section13\"><\/a>\n### Importing the necessary librairies for preprocessing and final data preparation","ac50c847":"<a id=\"section12\"><\/a>\n## 4. Data Preprocessing and Final preparation for modeling","c98e4f6c":"<a id=\"section27\"><\/a>\n### AdaBoost","1a79b658":"<a id=\"section20\"><\/a>\n### Multinomial Naive Bayes Modeling\n\nMultinomial Naive Bayes is a specialized version of Naive Bayes that is designed more for text documents. It estimates the conditional probability of a particular word given a class as the relative frequency of term *t* in documents belonging to class(c). The variation takes into account the number of occurrences of term *t* in training documents from class (*c*),including multiple occurrences.\n\n\nPros:\n- Computationally fast\n- Simple to implement\n- Works well with high dimensions\n\nCons:\n- Relies on independence assumption and will perform badly if this assumption is not met","8a9a3be3":"<a id=\"section3\"><\/a>\n## 3. Exploratory Data Analysis","1e077fc3":"Hyperparameters are preset parameter values of a machine learning model which directy control the behavior of the training algorithm and when optimized can have a significant impact on the model and the way it works. Hyperparameters directly affect how the model is structured and we do this to identify which parameters perform optimally for the given machine learning algorithm."}}