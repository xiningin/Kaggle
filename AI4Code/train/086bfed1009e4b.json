{"cell_type":{"8e008180":"code","b07b9dd6":"code","3eb45fe7":"code","7855aa19":"code","72e79a6a":"code","37d532d4":"code","b3572633":"code","d9c9196e":"code","efdf78ae":"code","0a5b07c0":"code","268f6643":"code","e6a31b0d":"code","71527437":"code","2e83ed85":"code","ad5f3b2d":"code","64864e4d":"code","9d61441e":"code","1db0496f":"code","e8f40ca1":"code","71b6b62b":"code","b818466e":"code","c80554a6":"code","a5332e16":"code","ed29fc9e":"code","ffcbf33a":"code","8956e1dc":"code","eb5dbe57":"code","9bcd9904":"code","0f046e40":"code","dd00c342":"code","b4aa7df2":"code","2cb71f2e":"code","9a46dc10":"code","af1167bc":"code","c66d75ae":"code","3a68bb87":"code","1c6d4767":"code","e94f7a92":"code","253a2b80":"code","bcae184a":"code","4b20fce4":"code","b6e0c444":"markdown","069c6c0b":"markdown","5449f5ea":"markdown","42b694fe":"markdown","ea9cda87":"markdown","5edc56de":"markdown","e7528194":"markdown","bc5ec3e5":"markdown","c0b0386d":"markdown","b92bb0dc":"markdown","812e978a":"markdown","467ef752":"markdown","98ac39c2":"markdown","a185cf21":"markdown","b8ad3389":"markdown","da620402":"markdown","532d5338":"markdown","b4507c0f":"markdown","d5ddc70f":"markdown"},"source":{"8e008180":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\ntqdm.pandas(desc=\"progress-bar\")\nfrom gensim.models import Doc2Vec\nfrom sklearn import utils\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.sequence import pad_sequences\nimport gensim\nfrom sklearn.linear_model import LogisticRegression\nfrom gensim.models.doc2vec import TaggedDocument\nimport re\nimport seaborn as sns\nimport matplotlib.pyplot as plt","b07b9dd6":"df = pd.read_csv('..\/input\/sentiment-analysis-for-financial-news\/all-data.csv',delimiter=',',encoding='latin-1')\ndf.head()","3eb45fe7":"df = df.rename(columns={'neutral':'sentiment','According to Gran , the company has no plans to move all production to Russia , although that is where the company is growing .':'Message'})","7855aa19":"df.shape","72e79a6a":"df.index = range(4845)\ndf['Message'].apply(lambda x: len(x.split(' '))).sum()","37d532d4":"cnt_pro = df['sentiment'].value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(cnt_pro.index, cnt_pro.values, alpha=0.8)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('sentiment', fontsize=12)\nplt.xticks(rotation=90)\nplt.show();","b3572633":"#Convert sting to numeric\nsentiment  = {'positive': 0,'neutral': 1,'negative':2} \n\ndf.sentiment = [sentiment[item] for item in df.sentiment] \nprint(df)","d9c9196e":"def print_message(index):\n    example = df[df.index == index][['Message', 'sentiment']].values[0]\n    if len(example) > 0:\n        print(example[0])\n        print('Message:', example[1])\nprint_message(12)","efdf78ae":"print_message(0)","0a5b07c0":"from bs4 import BeautifulSoup\ndef cleanText(text):\n    text = BeautifulSoup(text, \"lxml\").text\n    text = re.sub(r'\\|\\|\\|', r' ', text) \n    text = re.sub(r'http\\S+', r'<URL>', text)\n    text = text.lower()\n    text = text.replace('x', '')\n    return text\ndf['Message'] = df['Message'].apply(cleanText)","268f6643":"df['Message'] = df['Message'].apply(cleanText)\ntrain, test = train_test_split(df, test_size=0.000001 , random_state=42)\nimport nltk\nfrom nltk.corpus import stopwords\ndef tokenize_text(text):\n    tokens = []\n    for sent in nltk.sent_tokenize(text):\n        for word in nltk.word_tokenize(sent):\n            #if len(word) < 0:\n            if len(word) <= 0:\n                continue\n            tokens.append(word.lower())\n    return tokens\ntrain_tagged = train.apply(\n    lambda r: TaggedDocument(words=tokenize_text(r['Message']), tags=[r.sentiment]), axis=1)\ntest_tagged = test.apply(\n    lambda r: TaggedDocument(words=tokenize_text(r['Message']), tags=[r.sentiment]), axis=1)\n\n# The maximum number of words to be used. (most frequent)\nmax_fatures = 500000\n\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 50\n\n#tokenizer = Tokenizer(num_words=max_fatures, split=' ')\ntokenizer = Tokenizer(num_words=max_fatures, split=' ', filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(df['Message'].values)\nX = tokenizer.texts_to_sequences(df['Message'].values)\nX = pad_sequences(X)\nprint('Found %s unique tokens.' % len(X))","e6a31b0d":"X = tokenizer.texts_to_sequences(df['Message'].values)\nX = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)","71527437":"#train_tagged.values[2173]\ntrain_tagged.values","2e83ed85":"d2v_model = Doc2Vec(dm=1, dm_mean=1, size=20, window=8, min_count=1, workers=1, alpha=0.065, min_alpha=0.065)\nd2v_model.build_vocab([x for x in tqdm(train_tagged.values)])\n","ad5f3b2d":"%%time\nfor epoch in range(30):\n    d2v_model.train(utils.shuffle([x for x in tqdm(train_tagged.values)]), total_examples=len(train_tagged.values), epochs=1)\n    d2v_model.alpha -= 0.002\n    d2v_model.min_alpha = d2v_model.alpha","64864e4d":"print(d2v_model)\n","9d61441e":"len(d2v_model.wv.vocab)\n","1db0496f":"# save the vectors in a new matrix\nembedding_matrix = np.zeros((len(d2v_model.wv.vocab)+ 1, 20))\n\nfor i, vec in enumerate(d2v_model.docvecs.vectors_docs):\n    while i in vec <= 1000:\n    #print(i)\n    #print(model.docvecs)\n          embedding_matrix[i]=vec\n    #print(vec)\n    #print(vec[i])","e8f40ca1":"d2v_model.wv.most_similar(positive=['profit'], topn=10)\n","71b6b62b":"d2v_model.wv.most_similar(positive=['investment'], topn=10)\n","b818466e":"d2v_model.wv.most_similar(positive=['broke'], topn=10)","c80554a6":"from sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef tsne_plot(model):\n    \"Creates and TSNE model and plots it\"\n    labels = []\n    tokens = []\n\n    for word in d2v_model.wv.vocab:\n        tokens.append(model[word])\n        labels.append(word)\n    \n    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=250, random_state=23)\n    new_values = tsne_model.fit_transform(tokens)\n\n    x = []\n    y = []\n    for value in new_values:\n        x.append(value[0])\n        y.append(value[1])\n        \n    plt.figure(figsize=(16, 16)) \n    for i in range(len(x)):\n        plt.scatter(x[i],y[i])\n        plt.annotate(labels[i],\n                     xy=(x[i], y[i]),\n                     xytext=(5, 2),\n                     textcoords='offset points',\n                     ha='right',\n                     va='bottom')\n    plt.show()","a5332e16":"tsne_plot(d2v_model)","ed29fc9e":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense, Embedding\n\n\n# init layer\nmodel = Sequential()\n\n# emmbed word vectors\nmodel.add(Embedding(len(d2v_model.wv.vocab)+1,20,input_length=X.shape[1],weights=[embedding_matrix],trainable=True))\n\n# learn the correlations\ndef split_input(sequence):\n     return sequence[:-1], tf.reshape(sequence[1:], (-1,1))\nmodel.add(LSTM(50,return_sequences=False))\nmodel.add(Dense(3,activation=\"softmax\"))\n\n# output model skeleton\nmodel.summary()\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['acc'])","ffcbf33a":"from keras.utils import plot_model\nplot_model(model, to_file='model.png')","8956e1dc":"Y = pd.get_dummies(df['sentiment']).values\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.15, random_state = 42)\nprint(X_train.shape,Y_train.shape)\nprint(X_test.shape,Y_test.shape)","eb5dbe57":"batch_size = 32\nhistory=model.fit(X_train, Y_train, epochs =50, batch_size=batch_size, verbose = 2)","9bcd9904":"plt.plot(history.history['acc'])\nplt.title('model accuracy')\nplt.ylabel('acc')\nplt.xlabel('epochs')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.savefig('model_accuracy.png')\n\n# summarize history for loss\nplt.plot(history.history['loss'])\n#plt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epochs')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\nplt.savefig('model_loss.png')","0f046e40":"# evaluate the model\n_, train_acc = model.evaluate(X_train, Y_train, verbose=2)\n_, test_acc = model.evaluate(X_test, Y_test, verbose=2)\nprint('Train: %.3f, Test: %.4f' % (train_acc, test_acc))","dd00c342":"# predict probabilities for test set\nyhat_probs = model.predict(X_test, verbose=0)\nprint(yhat_probs)\n# predict crisp classes for test set\nyhat_classes = model.predict_classes(X_test, verbose=0)\nprint(yhat_classes)\n# reduce to 1d array\n#yhat_probs = yhat_probs[:, 0]\n#yhat_classes = yhat_classes[:, 1","b4aa7df2":"import numpy as np\nrounded_labels=np.argmax(Y_test, axis=1)\nrounded_labels","2cb71f2e":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(rounded_labels, yhat_classes)\ncm","9a46dc10":"# The confusion matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\n\nlstm_val = confusion_matrix(rounded_labels, yhat_classes)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(lstm_val, annot=True, linewidth=0.7, linecolor='cyan', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title('LSTM Classification Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","af1167bc":"validation_size = 610\n\nX_validate = X_test[-validation_size:]\nY_validate = Y_test[-validation_size:]\nX_test = X_test[:-validation_size]\nY_test = Y_test[:-validation_size]\nscore,acc = model.evaluate(X_test, Y_test, verbose = 1, batch_size = batch_size)\n\nprint(\"score: %.2f\" % (score))\nprint(\"acc: %.2f\" % (acc))","c66d75ae":"\nmodel.save('Mymodel.h5')","3a68bb87":"message = ['Congratulations! you have won a $1,000 Walmart gift card']\nseq = tokenizer.texts_to_sequences(message)\n\npadded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n\npred = model.predict(padded)\n\nlabels = ['0','1','2']\nprint(pred, labels[np.argmax(pred)])","1c6d4767":"message = ['such massive asteroid hit will certainly create new business opportunities']\nseq = tokenizer.texts_to_sequences(message)\n\npadded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n\npred = model.predict(padded)\n\nlabels = ['0','1','2']\nprint(pred, labels[np.argmax(pred)])","e94f7a92":"message = ['so does anyone else not open Snapchat anymore ? or is it just me ... ugh this so sad.']\nseq = tokenizer.texts_to_sequences(message)\n\npadded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n\npred = model.predict(padded)\n\nlabels = ['0','1','2']\nprint(pred, labels[np.argmax(pred)])","253a2b80":"message = ['@elonmusk had a terrible experience with a very pushy sales guy from tesla Stanford shop while shopping for model x']\nseq = tokenizer.texts_to_sequences(message)\n\npadded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n\npred = model.predict(padded)\n\nlabels = ['0','1','2']\nprint(pred, labels[np.argmax(pred)])","bcae184a":"message = ['The local electronics industry is expected to remain stable amid layoff concerns surrounding Japanese electronics giants operating in the country, an official says.']\nseq = tokenizer.texts_to_sequences(message)\n\npadded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n\npred = model.predict(padded)\n\nlabels = ['0','1','2']\nprint(pred, labels[np.argmax(pred)])","4b20fce4":"message = ['The local electronics industry is amid layoff concerns and last year has laid off tens of employees']\nseq = tokenizer.texts_to_sequences(message)\n\npadded = pad_sequences(seq, maxlen=X.shape[1], dtype='int32', value=0)\n\npred = model.predict(padded)\n\nlabels = ['0','1','2']\nprint(pred, labels[np.argmax(pred)])","b6e0c444":"# Practical Example","069c6c0b":"# NLP\n\nNatural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.\n\nNLP is currently the focus of significant interest in the machine learning community. Some of the use cases for NLP are listed here:\n* Chatbots\n* Search(text and Audio)\n* Text Classification\n* Sentiment Analysis\n* Recomendation System\n* Quesstion Answering\n* Speech recognation\n* NLU (Natural Language Understanding)\n* NLG ( Natural Language Generation)\n\nYou encounter many of these use cases in everyday life : when you visit web pages or perform an online search for books, or recommendation regarding movies.\n\n\n","5449f5ea":"Read Dataset","42b694fe":"# Example Cases : Sentiment Analysis\n\n# This work tries to classifying sentiment from financial news using Combination Doc2vec and LSTM","ea9cda87":"Plot Similarity word in Doc2vec","5edc56de":"# NLP Techniques\n\nThe earliest approach for solving NLP task involves rule-based approachers, which dominated the indusytry for decades. Examples of techniques using rule based approaches include Regular Expressions (RegExs) and Context Free Grammars (CFGs). RegExs are sometimes used in order to remove HTML tags from text that has been scraped from a web page or uwanted special characters from a document.\n\n\nThe second approach involved training a machine learning model with some data that is based on some user defined features. This technique requires a considerable amount of features engineering ( a nontrivial task), and includes analyzing the text to remove undersired and superfluous content(including stop words), as well as transforming the word (e.g., converting upercase to lowercase).\n\n\nThe most recent approach involves deep learning, whereby a neural network learns the features instead of relying on human to perform feature engineering. One of the key ideas involves mapping words to numbers, which enables us to map sentence to vector of number. After transforming documents to vector, we can perform a myriad of operations on those vector.  For example we use the notion of vector space to define vector space model, where the distance between two vector can be measured  by the angle between them (related to cosine similarity). If two vector are closed to each other, then it's likelier that the coresponding sentence are similar meaning. Their similarity is based on the distributional hypothesis, which asserts that words in the same contexts tent to have similar meaning.The NLP models that use deep learning can comprise CNNs, RNNs, LSTMs, and bidirectional LSTMs.\n","e7528194":"References\n* [1] Artificial Inteligence, Machine Learning and Deep Learning by Mercury Learning and Informarion (Book)\n* [2] Source Information of NLP :https:\/\/monkeylearn.com\/what-is-text-classification\/\n* [3] Parameter doc2vec and  LSTM inspired  by : https:\/\/dl.acm.org\/doi\/10.1145\/3406601.3406624\n* [4] (Bing Liu. Sentiment Analysis and Opinion Mining, Morgan & Claypool Publishers, May 2012.)","bc5ec3e5":"Text Preprocessing Below we define a function to convert text to lower-case and strip punctuation\/symbols from words and so on.","c0b0386d":"# Create the LSTM Model\n\nThe parameter used here inspired by [3].\n\n","b92bb0dc":"![https:\/\/miro.medium.com\/max\/9000\/1*h0mO4PdZaQKtbwWJW40FKQ.jpeg](https:\/\/miro.medium.com\/max\/9000\/1*h0mO4PdZaQKtbwWJW40FKQ.jpeg)","812e978a":"![https:\/\/miro.medium.com\/max\/6656\/1*99ToSc3FWZWlZ_BPcTe9zQ.png](https:\/\/miro.medium.com\/max\/6656\/1*99ToSc3FWZWlZ_BPcTe9zQ.png)","467ef752":"# Measuring distance between two vectors (related to cosine similarity)","98ac39c2":"VISUALIZING THE DATA","a185cf21":"Model DM = 1\n\nThis work use DM=1 (it preserve word order)","b8ad3389":"# Validation","da620402":"# What is Sentimen Analysis ?\n\nSentiment analysis refers to the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment Analysis [2]: the process of understanding if a given text is talking positively or negatively about a given subject, aiming for brand monitoring.\n\n\n![https:\/\/mk0ecommercefas531pc.kinstacdn.com\/wp-content\/uploads\/2019\/12\/sentiment-analysis.png](https:\/\/mk0ecommercefas531pc.kinstacdn.com\/wp-content\/uploads\/2019\/12\/sentiment-analysis.png)\n\n\n# Different Levels of Analysis \n\nAccording to [4] sentiment analysis has been investigated mainly at three levels:  \n\n*** Document level:**\n\nThe task at this level is to classify whether a whole opinion document expresses a positive or negative sentiment (Pang, Lee and Vaithyanathan, 2002; Turney, 2002). For example, given a product\nreview, the system determines whether the review expresses an overall positive or negative opinion about the product. This task is commonly known as document-level sentiment classification. This level of analysis\nassumes that each document expresses opinions on a single entity (e.g., a single product). Thus, it is not applicable to documents which evaluate or compare multiple entities. \n\n\n\n** Sentence level: **\n\n\nThe task at this level goes to the sentences and determines whether each sentence expressed a positive, negative, or neutral opinion. Neutral usually means no opinion. This level of analysis is closely related to subjectivity classification (Wiebe, Bruce and O'Hara, 1999), which distinguishes sentences (called objective sentences) that express factual information from sentences (called subjective sentences) that express\nsubjective views and opinions. However, we should note that subjectivity is not equivalent to sentiment as many objective sentences can imply opinions, e.g., \u201cWe bought the car last month and the windshield wiper\nhas fallen off.\u201d Researchers have also analyzed clauses (Wilson, Wiebe and Hwa, 2004), but the clause level is still not enough, e.g., \u201cApple is doing very well in this lousy economy.\u201d \n\n\n**Entity and Aspect level: **\n\n\nBoth the document level and the sentence level analyses do not discover what exactly people liked and did not like. Aspect level performs finer-grained analysis. Aspect level was earlier called feature level (feature-based opinion mining and summarization) (Hu and Liu, 2004). Instead of looking at language constructs (documents, paragraphs, sentences, clauses or phrases), aspect level directly looks at the opinion itself. It is based on the idea that an opinion consists of a sentiment (positive or negative) and a target (of opinion). An opinion without its target being identified is of limited use. Realizing the importance of opinion targets also helps us understand the sentiment\nanalysis problem better. \n\n","532d5338":"**Test with new and different data set aside from the data to build the model.**\n\nnote : sentiment  = {'positive': 0,'neutral': 1,'negative':2} ","b4507c0f":"\n# What is LSTM ?\n\nLong Short Term Memory (LSTM) is a special kind of Recurrent Neural Network (RNN), capable of learning long-term dependencies. These long-term dependencies have a great influence on the meaning and overall polarity of a document. Long short-term memory networks (LSTM) address this long-term dependency problem by introducing a memory into the network. It was first introduced by **Hochreiter & Schmidhuber**.\n\n![https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/5\/53\/Peephole_Long_Short-Term_Memory.svg\/2000px-Peephole_Long_Short-Term_Memory.svg.png](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/5\/53\/Peephole_Long_Short-Term_Memory.svg\/2000px-Peephole_Long_Short-Term_Memory.svg.png)\n\n\nThe LSTM architecture has a range of repeated modules for each time step as in a standard RNN. At each time step, the output of the module is controlled by a set of gates, as a function of the old hidden state \u210e\ud835\udc61\u22121 and the input at the current time step \ud835\udc65\ud835\udc61 : the forget gate \ud835\udc53\ud835\udc61, the input gate \ud835\udc56\ud835\udc61 , and the output gate \ud835\udc42\ud835\udc61 . These gates collectively decide how to update the current memory cell \ud835\udc36\ud835\udc61 and the current hidden state \u210e\ud835\udc61 . The LSTM transition functions are defined as follows:\n\n\ud835\udc56\ud835\udc61=(\ud835\udc4a\ud835\udc56[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61]+\ud835\udc4f\ud835\udc56) \n\n\ud835\udc36\u00b4\ud835\udc61=\ud835\udc61\ud835\udc4e\ud835\udc5b\u210e(\ud835\udc4a\ud835\udc50[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61]+\ud835\udc4f\ud835\udc36) \n\n\ud835\udc53\ud835\udc61=(\ud835\udc4a\ud835\udc53[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61]+\ud835\udc4f\ud835\udc53) \n\n\ud835\udc42\ud835\udc61=(\ud835\udc4a\ud835\udc5c[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61]+\ud835\udc4f\ud835\udc5c) \n\n\ud835\udc36\ud835\udc61= \ud835\udc53\ud835\udc61\u2217\ud835\udc36\ud835\udc61\u22121+\ud835\udc56\ud835\udc61\u2217\ud835\udc36\u00b4\ud835\udc61\n\nHere \ud835\udf0e is logistic sigmoid function that has an output in \ud835\udc5b [0,1] ,tanh denotes the hyperbolic tangent function that has an output \u210e \ud835\udc56\ud835\udc5b [\u22121,1], and \u2217 denotes the pointwise multiplication.\n\n\nAs I mentioned before to deal with Deep learning we have to map sentence to vector of number. In this work inspired by [3] Doc2vec as the embedding used for extracting information context. The Doc2vec is an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts such as sentences, paragraphs, and documents.\n","d5ddc70f":"**Save Model**\n"}}