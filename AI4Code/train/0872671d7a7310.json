{"cell_type":{"d9340af4":"code","2c39d425":"code","be1d2cb5":"code","f9838e6c":"code","6b561329":"code","f9a175b5":"code","ea0fceb7":"code","7a714321":"code","295fc7f9":"code","2198d99c":"code","7825cf2e":"code","8ffe3cb9":"code","745b60c4":"code","aceade34":"code","e8e49843":"code","694fc340":"code","f5078eaf":"code","c891328e":"code","84e0977e":"code","ec2af508":"code","67a551f7":"code","cedb653c":"code","c8274386":"code","554dc3f2":"code","5c385bdf":"code","b6fb490a":"code","1ce4a1a4":"code","e4fe3859":"code","cdd19f91":"code","e125edc1":"code","e27a0ead":"code","e16213b7":"code","dc0e206b":"code","43e791cd":"code","6971428d":"code","bb11e522":"code","8b52099b":"code","89e9b66f":"code","8295d994":"code","26e30b5f":"code","fd0d5ea8":"code","1fbe3cba":"code","fd9ea4d5":"code","35210a80":"code","4570fa7f":"code","17afd4f5":"code","528c8603":"code","ca59358c":"code","04b76984":"code","c6f17ce4":"code","d14fc49b":"code","53a94436":"code","336efe83":"code","d8b0685d":"code","43599022":"code","0582d247":"code","a7902dfb":"code","1ac55981":"code","c9f8711f":"code","b35e7d81":"code","f0e982f9":"code","71eeb517":"code","5d4177fb":"code","fb0ac4ac":"code","cdc51209":"code","1eef4648":"code","11bf5479":"code","68803591":"code","c1ae7012":"code","92c92f08":"code","7cec07c0":"code","9b14f50e":"code","8061440a":"code","3f8891f9":"code","97252c59":"code","745a3ab1":"code","8655a24d":"markdown","905836c0":"markdown","359f3c29":"markdown","519de779":"markdown","ff331106":"markdown","164dd8c5":"markdown","233edeae":"markdown","6fa42393":"markdown","7577339e":"markdown","ffdec177":"markdown","0299e1a3":"markdown","62e9e712":"markdown","52bd6e04":"markdown","5e6d3223":"markdown","5688509c":"markdown","1aa1537c":"markdown","e0243be9":"markdown","a0590bd7":"markdown","1df2c222":"markdown","8c130a42":"markdown","86f8941d":"markdown","dd71345d":"markdown","eb27a381":"markdown","d7f39fd2":"markdown","15388799":"markdown","e13c9f7d":"markdown","dcc83ff1":"markdown","726dfcd3":"markdown","3ce364e4":"markdown","fc62f6d1":"markdown","ffea191e":"markdown","fb15348f":"markdown","7144e5b3":"markdown","94387962":"markdown","6ff429d0":"markdown","a6a58ae8":"markdown","c131a38c":"markdown","b40d080b":"markdown","81ed0da6":"markdown","abc96369":"markdown","635c3a57":"markdown"},"source":{"d9340af4":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","2c39d425":"df_Train = pd.read_csv('..\/input\/d\/rahulsah06\/titanic\/train.csv')\ndf_test=pd.read_csv('..\/input\/d\/rahulsah06\/titanic\/test.csv')\ndf_survived=pd.read_csv('..\/input\/d\/rahulsah06\/titanic\/gender_submission.csv')","be1d2cb5":"df_test.head()","f9838e6c":"df_survived.head()","6b561329":"df_survived.drop(['PassengerId'],axis=1)","f9a175b5":"df_test['Survived']=df_survived['Survived']","ea0fceb7":"df_survived.head()","7a714321":"df_test.head()","295fc7f9":"df_Train.head()","2198d99c":"df=pd.concat([df_test,df_Train])","7825cf2e":"df.head()","8ffe3cb9":"df.info()","745b60c4":"df.corr()['Survived'].sort_values()","aceade34":"sns.scatterplot(data=df , x='Survived', y='Fare')","e8e49843":"sns.boxplot(data=df , x='Survived', y='Fare')","694fc340":"df[(df['Survived']==1)&(df['Fare']>400)][['Survived','Fare']]","f5078eaf":"index_drop=df[(df['Fare']>400) & (df['Survived']==1) ].index\ndf=df.drop(index_drop, axis=0)","c891328e":"sns.scatterplot(data=df_Train , x='Survived', y='Fare')","84e0977e":"sns.boxplot(data=df , x='Survived', y='Fare')","ec2af508":"df.isnull().any()","67a551f7":"df=df.drop(['Name'],axis=1)","cedb653c":"df.isnull().sum()","c8274386":"100*(df.isnull().sum()\/len(df))","554dc3f2":"def missing_percent(df):\n    nan_percent= 100*(df.isnull().sum()\/len(df))\n    nan_percent= nan_percent[nan_percent>0].sort_values()\n    return nan_percent","5c385bdf":"nan_percent= missing_percent(df)","b6fb490a":"sns.barplot(x=nan_percent.index, y=nan_percent)\nplt.xticks(rotation=90)","1ce4a1a4":"sns.barplot(x=nan_percent.index, y=nan_percent)\nplt.xticks(rotation=90)\nplt.ylim(0,1)","e4fe3859":"df[df['Fare'].isnull()]","cdd19f91":"df= df.dropna(axis=0, subset=['Fare'])","e125edc1":"df[df['Embarked'].isnull()]","e27a0ead":"df= df.dropna(axis=0, subset=['Embarked'])","e16213b7":"nan_percent= missing_percent(df)\nsns.barplot(x=nan_percent.index, y=nan_percent)\nplt.xticks(rotation=90)","dc0e206b":"df[df['Age'].isnull()]","43e791cd":"sns.kdeplot(data=df['Age'])","6971428d":"df['Age'] = df['Age'].fillna(df['Age'].mean())\n\nmissing_percent(df)","bb11e522":"df= df.drop(['Cabin'], axis=1)","8b52099b":"df.isnull().any()","89e9b66f":"df=df.drop(['Ticket'],axis=1)","8295d994":"df.head()","26e30b5f":"sns.barplot(data=df,x='Survived',y='Fare',color='c')","fd0d5ea8":"sns.barplot(data=df,x='Survived',y='Age',color='c')","1fbe3cba":"sns.barplot(data=df,y='Survived',x='Sex',color='c')","fd9ea4d5":"sns.heatmap(data=df.corr(),annot=True,cmap=\"Blues\")","35210a80":"sns.barplot(x=\"Survived\", y=\"Fare\", hue=\"Pclass\", data=df,palette=\"Blues_d\")","4570fa7f":"df.info()","17afd4f5":"df.head()","528c8603":"df['Survived'] = df['Survived'].apply(str)\ndf['Pclass'] = df['Pclass'].apply(str)","ca59358c":"df.info()","04b76984":"df_num= df.select_dtypes(exclude='object')\ndf_obj= df.select_dtypes(include='object')","c6f17ce4":"df_num.info()","d14fc49b":"df_obj.info()","53a94436":"# Converting objects to number by one-hot encoding(drop_first=True:removes multi-collinearity)\ndf_obj= pd.get_dummies(df_obj, drop_first=True)","336efe83":"Final_df= pd.concat([df_num, df_obj], axis=1)","d8b0685d":"Final_df.head()","43599022":"sns.countplot(data=Final_df, x='Survived_1')","0582d247":"X= Final_df.drop('Survived_1', axis=1)\ny= Final_df['Survived_1']","a7902dfb":"from sklearn.model_selection import train_test_split","1ac55981":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","c9f8711f":"from sklearn.tree import DecisionTreeClassifier","b35e7d81":"model = DecisionTreeClassifier()","f0e982f9":"model.fit(X_train,y_train)","71eeb517":"y_pred = model.predict(X_test)","5d4177fb":"from sklearn.metrics import confusion_matrix, classification_report","fb0ac4ac":"confusion_matrix(y_test,y_pred)","cdc51209":"print(classification_report(y_test,y_pred))","1eef4648":"#importance of the features: it shows that which feature was pure to help splitting the data\nmodel.feature_importances_","11bf5479":"pd.DataFrame(index=X.columns,data=model.feature_importances_,columns=['Feature Importance'])","68803591":"from sklearn.tree import plot_tree","c1ae7012":"plt.figure(figsize=(50,30), dpi=250)\nplot_tree(model,fontsize=20,filled=True,feature_names=X.columns);","92c92f08":"def report_model(model):\n    model_preds = model.predict(X_test)\n    print(classification_report(y_test,model_preds))\n    print('\\n')\n    plt.figure(figsize=(12,8),dpi=150)\n    plot_tree(model,filled=True,feature_names=X.columns);","7cec07c0":"pruned_tree = DecisionTreeClassifier(max_depth=2)\npruned_tree.fit(X_train,y_train)","9b14f50e":"report_model(pruned_tree)","8061440a":"pruned_tree = DecisionTreeClassifier(max_leaf_nodes=3)\npruned_tree.fit(X_train,y_train)","3f8891f9":"report_model(pruned_tree)","97252c59":"entropy_tree = DecisionTreeClassifier(criterion='entropy')\nentropy_tree.fit(X_train,y_train)","745a3ab1":"report_model(entropy_tree)","8655a24d":"**Predicting**","905836c0":"**here is another classification algorithm(categorical labels)**\n\n**this is statistical decision trees**\n\n**all decision trees based methods rely on the ability to split data based on information from features. this means we need a mathematical definition of information and the ability to measure it**\n\n**this tree structure works on features and split the input samples and the result is linear segmented boundaries.**\n\n**this algorithm chooses one feature and do a simple test on it and then it continues on the other features until we get to point in leaf with high purity and then we can decide the class.**\n\n**leafs= High purity --> classification in done**\n\n**In training(making Tree) phase each faeture has its own threshold and we go through the depth of the tree and then based maximun vote and threshold classifying is done in the leafs with high purity**\n\n**In test phase we compare the feature through the tree depth and choose the right class**","359f3c29":"# EDA","519de779":"## common impurity measures:\n\n### Entropy(in C4.5 Tree): it shows impurity\n\n**min entropy: all sample in one class(deterministic)**\n\n**max entropy: uniform distribution**\n\n### Gini impurity(CART Tree):\n\n**it measures the purity of the information in a data set.**\n\n**in classification, purity means class uniformity.**\n\n**Gini impurity for classification:**\n\n**we want as low gini impurity as we can. the less gini impurity, the high uniformity. so we can split the data sets.**\n\n**we want to minimize the gini impurity at leaf nodes.so we can saparate classes effectively.**\n\n**this formula also helps us to choose root node for our decision tree(we choose the less impure information for root)**\n\n![main-qimg-3800e86a4f0a8c548f29b025ce45d4d6.png](attachment:9cbd3bba-c86e-48e0-b6eb-07a2ada8645a.png)","ff331106":"## Training Phase\n\n### top-bottom algorithm:\n\n**find the best partition base on impurity**\n\n**Recurrent partitioning until the best purity**\n\n### what is impurity measure?\n\n**it shows the amount of separability. ideally we want to separate all of the two-classes(we have to becareful about overfitting)**\n\n**two condition of impurity measure:**\n\n* max impurity: uniform variance\n\n* min impurity: all data in one class","164dd8c5":"### Checking for missing data:","233edeae":"## Max Leaf Nodes","6fa42393":"**it seems age didn't affected on this issue**","7577339e":"**most of the survivors were female**","ffdec177":"### hyper parameters","0299e1a3":"**as you see gini!=0 so the precision is lower that before**","62e9e712":"**Gini Impurity for different kind of features:**\n\n* continous features\n\n* discrete features","52bd6e04":"**\"Survived\" and \"Pclass\" are categorical but now they are Int. we have to change them to object**","5e6d3223":"## Max Depth","5688509c":"**Split the Data to Train & Test**","1aa1537c":"# dealing with categorical data","e0243be9":"## Criterion","a0590bd7":"**as you see we have minimal gini in terminal nodes so we split well**\n\n**note that focusing on impurity measure leads to overfitting and concidering the outliers or noises so we have to change other hyperparameters.**\n\n**two way to overcome this issue:**\n\n1. Prune the tree that we made in training phase and using validation sets(validation error).\n\n1. stop building the tree while we see that the validation error are increasing.","1df2c222":"### terminology of Decision Tree:\n\n**splitting: don't get it wrong with spliting train and test in data set**\n\n![Screenshot-2020-07-20-at-6.02.10-PM.png](attachment:8e520b47-69b4-4818-8847-383a5f6c8265.png)\n\n**node: root node , leaf(Terminal) node , parent and children node**\n\n**sub tree**\n\n**pruning: cutting the trees. to overcome the overfitting.**","8c130a42":"**the more they paid for Fare the more they survived**","86f8941d":"# EDA & Data Cleaning:","dd71345d":"**Evaluate**","eb27a381":"# Visualization","d7f39fd2":"# some notes:\n\n**big trees leads to overfitting**\n\n**this algorithm is a Greedy algorithm that cannot calculate the global optimum**\n\n**in reality building tree in training phase make with help of thw regularization e.g validation errors and this is leads to have smaller and more informative trees**","15388799":"### checking for outliers:","e13c9f7d":"**since missing ages are not neglectable,we replace them with mean.**","dcc83ff1":"**Fare value had the most impact on surviving**","726dfcd3":"**we almost don't have any data about Cabin(missing=77%) so we drop it**","3ce364e4":"use grid search","fc62f6d1":"**we don't have any missing value**","ffea191e":"**we have passenger Id so we can drop the names because it can not help us for our model building**","fb15348f":"# Decision Tree","7144e5b3":"**as it shows people with more fare and the high Pclass had more chance to survive**","94387962":"# Decision Tree","6ff429d0":"**now the data set is ready**","a6a58ae8":"**node impurity: this formula helps to split the data in this algorithm.**\n\n\n**THAID-CHAID-CART-ID3-ID3 & C4.5-C5.0: all of these algorithms are helped in developement of the Decision Tree**\n\n**some other tree based algorithms are: Random Forests, Gradient boosted trees.**","c131a38c":"**SEX was the purest**","b40d080b":"**Dummy Variables:**","81ed0da6":"**ticket column doesn't help us in model buillding so we drop it**","abc96369":"the problem is balanced","635c3a57":"**To begin experimenting with hyperparameters, let's create a function that reports back classification results and plots out the tree.**"}}