{"cell_type":{"20dd36ba":"code","2702d22d":"code","8bc0d7ca":"code","041f1bbc":"code","4e937937":"code","92d921b9":"code","dd837536":"code","e1307916":"code","85bd0f49":"code","bd27917c":"code","c7f64187":"code","b1cf5cee":"code","0e77acc6":"code","61fd330f":"code","f9e91d21":"code","06536a96":"code","02265fba":"code","9f639de5":"code","34c5b678":"code","59436703":"code","f5fa527b":"code","1824347d":"code","7b4312d7":"code","a521d549":"code","ed4c211b":"code","4fd2cd16":"code","92bd2e8c":"code","8e6055b8":"code","9552e218":"code","9e75f099":"code","ffb6cc53":"code","2beeabf7":"code","440d6276":"code","63c76c59":"code","9d1a4b5d":"code","194be2cd":"code","d88a94b3":"code","a690f8ee":"markdown","ba14337a":"markdown","c1a9444e":"markdown","6825062b":"markdown","3c51acf7":"markdown","ad7318e4":"markdown","c1a9f5c3":"markdown","0ade16f5":"markdown","7904ec1d":"markdown"},"source":{"20dd36ba":"#IMPORTING ALL REQUIRED LIBRARIES\nimport numpy as np\nimport pandas as pd\n#from fancyimpute import KNN\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score,accuracy_score\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression","2702d22d":"df_train=pd.read_csv(\"..\/input\/kagglethon-iste\/train.csv\")\ndf_test=pd.read_csv(\"..\/input\/kagglethon-iste\/test.csv\")","8bc0d7ca":"df_train.head()","041f1bbc":"df_train.describe()","4e937937":"df_train.isnull().sum()","92d921b9":"df_test.isnull().sum()","dd837536":"#Separate target varibale\nY_train=df_train['y']\nY_train=Y_train.to_numpy()","e1307916":"#Separate features\nX_train=df_train.loc[:, df_train.columns != 'y']\nX_test=df_test.loc[:,df_test.columns!='y']","85bd0f49":"#Combine Test and train data by concatenation\ncombo=pd.concat(objs=[X_train,X_test])\ncombo.describe()","bd27917c":"#Detect categorical variables\ncombo.nunique()","c7f64187":"#One-Hot-Encoding\ncombo=pd.get_dummies(data=combo,columns=[\"x9\",\"x16\",\"x17\",\"x18\",\"x19\"],dummy_na=True,drop_first=True)","b1cf5cee":"combo.head()","0e77acc6":"combo.describe()","61fd330f":"#Split Train and Test by index\nX_train_dummy=pd.DataFrame(data=combo[0:Y_train.shape[0]])\nX_train_dummy.describe()","f9e91d21":"X_test_dummy=pd.DataFrame(data=combo[Y_train.shape[0]:])\nX_test_dummy.head()","06536a96":"#Normalise Data to improve performance of KNN\nX_train_normalized = scale(X_train_dummy)\nX_test_normalized=scale(X_test_dummy)\nX_train_try=pd.DataFrame(data=X_train_normalized,columns=X_test_dummy.columns)\nX_train_try.head()","02265fba":"#Imputing Misssing values with KNN\nX_train_filled=KNNImputer(n_neighbors=7).fit_transform(X_train_normalized)\nX_train_filled=pd.DataFrame(data=X_train_filled,columns=X_train_dummy.columns)","9f639de5":"#Rename columns to orignal convention \nX_train_filled.columns=X_train_dummy.columns\nX_train_filled.head()","34c5b678":"#Verify\nX_train_filled.isnull().sum()","59436703":"X_test_filled=KNNImputer(n_neighbors=7).fit_transform(X_test_normalized)\nX_test_filled=pd.DataFrame(data=X_test_filled,columns=X_test_dummy.columns)\nX_test_filled.columns=X_test_dummy.columns\nX_test_filled.head()","f5fa527b":"X_test_filled.isnull().sum()","1824347d":"#Final train and test after preprocessing is done\nX_test=X_test_filled","7b4312d7":"X_train=X_train_filled","a521d549":"#Logistic Regression\n\nlog=LogisticRegression(random_state=2)\nlog.fit(X_train,Y_train)","ed4c211b":"predlog=log.predict_proba(X_test)","4fd2cd16":"# I have commented this cell as it takes a long time to run\n# tuned_parameters_svc = {'kernel': ['rbf'], 'gamma': [1e-3, 1e-4,'scale','auto'],\n#                      'C': [1, 10, 100, 1000]},\n                   \n# md_svc=SVC()\n# svc_rscv=RandomizedSearchCV(md_svc,tuned_parameters_svc,n_iter=16,scoring='roc_auc',verbose=5,cv=3,n_jobs=-1)\n# svc_rscv.fit(X_train,Y_train)\n# svc_rscv.best_params_","92bd2e8c":"#SVM\n#Best Values\nsvmc=SVC(probability=True,random_state=2,gamma=0.001,C=1000)\nsvmc.fit(X_train,Y_train)","8e6055b8":"# predict_proba outputs a soft label ie probabilty like 0.99 rather than 1.\n# Due to the way auc metric is defined; this gives better score on the leaderboard\npredsvm=svmc.predict_proba(X_test)","9552e218":"#Commented for obvious reasons\n# tuned_parameters_rfc = {'n_estimators':[1000,2000,5000],'max_features':[\"log2\",\"sqrt\",1,0.5],\n#                      'max_depth': [2,3,4,5,6,10],'criterion':[\"gini\"]}\n# md_rfc=RandomForestClassifier()\n# rfc_rscv=RandomizedSearchCV(md_rfc,tuned_parameters_rfc,n_iter=30,scoring='roc_auc',verbose=5,cv=3,n_jobs=-1)\n# rfc_rscv.fit(X_train,Y_train)\n# rfc_rscv.best_params_    ","9e75f099":"#Random Forest Best Params\nrfc=RandomForestClassifier(n_estimators=2000,random_state=2,max_depth=10,max_features='sqrt')\nrfc.fit(X_train,Y_train)","ffb6cc53":"predrfc=rfc.predict_proba(X_test)\n\n","2beeabf7":"#Commented for obvious reasons\n# tuned_params_xgb={'n_estimators':[200],'subsample':[0.7],'colsample_bytree':[0.9],'max_depth':[7],'gamma':[1,5,10]}\n# md_xgb=XGBClassifier()\n# xgb_rscv=RandomizedSearchCV(md_xgb,tuned_params_xgb,n_iter=60,cv=3,scoring='roc_auc',verbose=5,n_jobs=-1)\n# xgb_rscv.fit(X_train,Y_train)\n# xgb_rscv.best_params_","440d6276":"#Best Params XGB\nxgb=XGBClassifier(n_estimators=5000,subsample=0.7,colsample_bytree=0.9,max_depth=7,gamma=1,random_state=2)\nxgb.fit(X_train,Y_train)","63c76c59":"predxgb=xgb.predict_proba(X_test)","9d1a4b5d":"#Stacking weak models\npredf=predxgb+predsvm\npredf=predf\/2\n","194be2cd":"out=pd.DataFrame()\nout[\"Id\"]=range(0,4000)\nout[\"Predicted\"]=pd.DataFrame(predf[:,1])\nout.head()","d88a94b3":"out.to_csv(\"TheOneAboveAll.csv\",index=False)","a690f8ee":"### Hyperparameter tuning\n* This is probably one of the most tedious and often confusing aspects of Data Science.\n* Most modern algorithms are extremely sensitive to these values and can cause a drastic degradation in performance if tuned poorly.\n* Unfortunately there is no sure shot way of tuning hyperparameters that I am aware of.\n* Hence we use GridSearch which is the next best thing\n\n### GridSearch\n- To understand GridSearch lets consider a model with 3 parameters that need to be optimised.\n- Since almost always hyperparameter are continous values; we can imagine each one to form an axis.\n- Then we can consider our hyperparameter search space to be a 3D space(each axis corresponding to each hyperparameter)\n ![graph123.jpg](attachment:4012807f-781c-4cd1-8b41-58af799227d6.jpg)\n- Now we need to find the perfect combination of hyperparameters ie a point in this space that will give us best results\n- As you can see this is an extremely difficult task especially if we have more than 3 parameters.\n- But we can be smart about how we search. Usually we can perform 2 step search\n- First step is **Coarse Search**. This is done by keeping our step size big so we can cover as much region of the hyperspace as possible. If I were optimising parameter A ; I would try out values [1,100,400,800,1200,2000,5000]\n- Once we see that a certain set of parameters are giving us better results; we can perform **Fine Search**. This is done by searching in the neighbourhood of space obtained in Coarse Search. For ex if the best value of A obtained was 400; for fine search I would try out values [300,325,350,375,400,425,450,475,500](notice step size is smaller and more consistent than coarse)\n-  The process of Fine Search can be extended infinitely till you are satisfied\n- This can still be a very tedious approach. But we can try a hack to make this faster\n#### _Enter : RandomisedGridSearch_\n- This is a random version of the previous method. Suppose we have 3 parameters and for each parameter we have 10 possible values. That gives us $10^{3}$ combinations to try.\n- Even if each combination took 2 minutes and we set cv as 3 thats an insanely long amount of time(1.5hrs+)\n- Here suppose we use RandomisedGridSearch and decide to sample 100 points out of these 1000 possibilites we would save a lot of time\n- But ofcourse this solution wouldnt be the best solution. But here we make the assumption that all things are equally chaotic in the sense that a totally random sample performing good suggests it is in the neighbourhood of the best solution.\n- However never use RandomisedGridSearch as substitute for Fine Search. Fine Search is the last step of hyperparameter tuning we do and it must be as good as we can make it.Random Search can help us isolate intresting neighbourhoods of hyperspace faster.\n\n#### Recently there have been some developments in this field by using Bayesian Optimisation and Genetic Algorithms that are essentially clever ways of exploring the hyperparameter space.Google them to find out more\n#### Another important concept is CrossValidation. Google it","ba14337a":"### Hope you all learnt something through this Kagglethon.\n### At ISTE we believe the best learning comes from getting your hands dirty and so we thought this would be a great event for you guys to enter into the fascinating world of Machine Learning.\n### I strongly recommend you guys to make your notebooks public as well. This way all of us can learn and grow together.\n### Theres always something to learn on kaggle so keep kaggling and have fun!","c1a9444e":"# Step 1 : Data Preprocessing","6825062b":"# Step0: Import Libraries\n* **Numpy** and **Pandas** need no introduction.They help us manipulate arrays and csv files easily\n* **sklearn** has a lot of modules like metrics,preprocessing,impute etc that help manipulate the data we get.Here I have used **KNNImputer** for missing values.\n* sklearn.model_selection has the ever so important **GridSearchCV** that lets you do hyperparameter optimisation.**RandomisedSearchCV** is even better as it lets you explore a larger region of hyperspace in about the same time if you choose parameters carefully.\n* **SMOTE** is a technique for oversampling the minority class.Usually it improves performance however in this case it didn't really help that much.\n* Scaling data is super important.Read about **normalisation** in Machine Learning to get a better intuition.","3c51acf7":"# This was my approach.\n#### If there is any term you dint understand ; Google is your best friend","ad7318e4":"#### Stacking v\/s Blending\n![Untitled Diagram(3).png](attachment:ddfe239b-f1b7-4776-8372-2f6bbb4d2c05.png)\n\nFor more info Google","c1a9f5c3":"# Models\n#### Here I tried 4 models namely:\n1. **Logistic Regression** : Always try this out the first. Its the simplest algorithm and gives you a good idea of baseline performance. Some people claim that a carefully tuned logistic regression can even beat Neural Networks. I think this is an exaggeration but nonetheless this is a fantastic algorithm.\n2. **Support Vector Classifier** : This algorithm tends to work very well for small datasets.Its mathematical basis is totally different from tree based algorithms so it is a good candidate for blended solutions\n3. **Random Forest Classifier** : This can be skipped but I used it for demo purposes. Almost always; gradient boosted variants will outperform vanilla forests.\n4. **XGBoost** : Often called the \"Queen of ML algorithms\" this is an insanely powerful model.Its essentially Random Forest injected with super soldier serum.There was a time when almost 90% of all kaggle contests winning solutions used to be an overfitted xgboost model.\n\n#### Other good candidates that can be tried are: **Naive Bayes**, **CatBoost**(works especially well with many categorical variables) and **LightGBM**","0ade16f5":"### Why did I not concatenate test and train while performing Normalisation and Imputation?\n* Normalisation and Imputation are statistical operations.Hence their results are heavily dependant on data.\n* If I concatenate them and normalise;statistics is no more isolated.Hence their is some \"leak\" of information from test to train.If our model is clever enough....it may pick up on these leaks\n* In a way we have provided a cheat sheet to our model and thus we cant depend on its quality since it may not have truly \"learnt\" the concept\n* This was okay with categorical since there;we are simply warning our model of the possible options it may encounter.There is no major leak of info as such.\n* The problem of data leak is widespread and dangerous.So be very careful before mixing train and test data for all future projects you may take up","7904ec1d":"## Why Have I combined test and train data here?\n* This is common practice when we have categorical data.Let us take an example to see why this is important\n\nSuppose our train data is as follows\n \n|**Breed**|**Height**|**Animal**| \n|-|-|-| \n| Lab     | 0.5    | DOG| \n| GerShep | 0.75   | DOG|\n| Persian | 0.2    | CAT    |\n\nLet us say we perform OHE on the above table.We get something like this\n\n| IsLab? | IsGerShep? | IsPersian? | Height | Animal |\n|--------|-----------|-----------|--------|--------|\n| 1      | 0         | 0         | 0.5    | DOG    |\n| 0      | 1         | 0         | 0.75    | DOG    |\n| 0      | 0         | 1         | 0.2    | CAT    |\n\nWe train our model so it expects these 5 columns.\n\n\nBut consider Test Data to be something like this\n\n\n| **Breed**   | **Height** | **Animal** |  \n|---------|--------|--------|  \n| Lab     | 0.4    | DOG    | \n| PitBull | 0.3   | DOG    |\n| Persian | 0.2    | CAT    |\n\nOHE on this would yield something like\n\n| IsLab? | IsPitBull? | IsPersian? | Height | Animal |\n|--------|-----------|-----------|--------|--------|\n| 1      | 0         | 0         | 0.4    | DOG    |\n| 0      | 1         | 0         | 0.3    | DOG    |\n| 0      | 0         | 1         | 0.2    | CAT    |\n\n\nAs you can see there is a column mismatch between the two.This will cause weird performance issues with model\n\nHence we first concatenate train and test;perform OHE and then split back so that we are able to capture all categories"}}