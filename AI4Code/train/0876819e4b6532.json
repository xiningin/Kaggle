{"cell_type":{"e661b9ea":"code","8d1b5cfe":"code","86a13430":"code","f29daa70":"code","7fcdd7de":"code","b10b4da0":"code","c8108038":"code","3fdae152":"code","298f1338":"code","a7899c38":"code","dbb13657":"code","ae52c530":"code","e404603c":"code","cc5132c6":"code","848079b5":"code","82bff69c":"code","2a2b2b60":"code","8e07470e":"markdown","145dd60b":"markdown","95727c9c":"markdown","16b79718":"markdown","65cca720":"markdown","25dd2eec":"markdown","71f60552":"markdown","ef1a460d":"markdown"},"source":{"e661b9ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8d1b5cfe":"dataset = pd.read_csv(\"\/kaggle\/input\/advtlr\/Advertising.csv\")\nprint(dataset.shape)\nprint(dataset.head(5))","86a13430":"# Selecting the Second, Third and Fouth Column\nX= dataset.iloc[:,1:4]\n# Selecting Fouth Columnn\ny=dataset.iloc[:,4]\nfrom sklearn.model_selection import train_test_split \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nX_train.shape","f29daa70":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error \nregressor = LinearRegression()\n\nnof= X_train.shape[1]\nprint(nof)\nmse=np.empty(2**nof-1)\nind=np.empty(nof)\nl=list()\nbl=list()\nbitem=list()\nk=0\n\n#remaining = set(X_train.columns)\nfrom itertools import combinations\nremaining = set(X_train.columns)\nfor j in range(1,len(remaining)+1):\n    comb = combinations(remaining, j)\n    tempbest=5000\n   \n    for i in list(comb): \n        lsti=list(i)\n        print(list(i))\n        l.append(lsti)\n        #X_train.iloc[:,i]\n        regressor.fit(X_train.loc[:,lsti], y_train)\n        y_exp=regressor.predict(X_train.loc[:,list(i)])\n        mse[k] = mean_squared_error(y_train,y_exp)*y_train.shape[0]\/(y_train.shape[0]-len(list(i)))\n        if mse[k]<tempbest:\n            bitem = lsti\n            tempbest=mse[k]\n        k = k + 1\n        \n    bl.append(bitem)\n\n#X_train[list(i)]\nmse\n                \n","7fcdd7de":"tmse=np.empty(len(bl))\nk1=0\nbfs=list()\ntempbest=5000\nfor m in bl:\n      regressor.fit(X_train.loc[:,m], y_train)\n      y_exp=regressor.predict(X_test.loc[:,m])\n      tmse[k1] = mean_squared_error(y_test,y_exp)*y_test.shape[0]\/(y_test.shape[0]-len(list(i)))\n      if tmse[k1]<tempbest:\n            bfs = m\n            tempbest=tmse[k1]\n      k1 = k1 + 1\nprint(bfs)","b10b4da0":"tmse","c8108038":"from sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n\nnp.random.seed(42)\nm = 20\nX = 3 * np.random.rand(m, 1)\ny = 1 + 0.5 * X + np.random.randn(m, 1) \/ 1.5\nX_new = np.linspace(0, 3, 100).reshape(100, 1)\n\ndef plot_model(model_class, polynomial, alphas, **model_kargs):\n    for alpha, style in zip(alphas, (\"b-\", \"g--\", \"r:\")):\n        model = model_class(alpha, **model_kargs) if alpha > 0 else LinearRegression()\n        if polynomial:\n            model = Pipeline([\n                    (\"poly_features\", PolynomialFeatures(degree=10, include_bias=False)),\n                    (\"std_scaler\", StandardScaler()),\n                    (\"regul_reg\", model),\n                ])\n        model.fit(X, y)\n        y_new_regul = model.predict(X_new)\n        lw = 2 if alpha > 0 else 1\n        plt.plot(X_new, y_new_regul, style, linewidth=lw, label=r\"$\\alpha = {}$\".format(alpha))\n    plt.plot(X, y, \"b.\", linewidth=3)\n    plt.legend(loc=\"upper left\", fontsize=15)\n    plt.xlabel(\"$x_1$\", fontsize=18)\n    plt.axis([0, 3, 0, 4])\n\nplt.figure(figsize=(8,4))\n#plt.subplot(121)\nplot_model(Ridge, polynomial=False, alphas=(0, 10, 100), random_state=42)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\n#plt.subplot(122)\n#plot_model(Ridge, polynomial=True, alphas=(0, 10**-5, 1), random_state=42)\nplt.show()","3fdae152":"from sklearn.linear_model import Lasso\n\nplt.figure(figsize=(8,4))\n#plt.subplot(121)\nplot_model(Lasso, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\n#plt.subplot(122)\n#plot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), tol=1, random_state=42)\nplt.show()","298f1338":"from sklearn.linear_model import ElasticNet\n#elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\nplt.figure(figsize=(8,4))\n#plt.subplot(121)\nplot_model(ElasticNet, polynomial=False, alphas=(0, 0.1, 1), random_state=42)\nplt.ylabel(\"$y$\", rotation=0, fontsize=18)\n#plt.subplot(122)\n#plot_model(Lasso, polynomial=True, alphas=(0, 10**-7, 1), tol=1, random_state=42)\nplt.show()","a7899c38":"auto=pd.read_csv('\/kaggle\/input\/autompg-dataset\/auto-mpg.csv')\nauto.head(5)","dbb13657":"X=auto.iloc[:,[1,2,4,5,6,7]]","ae52c530":"X.shape\n#X.dropna(inplace=True)\n#X.fillna(X.mean(),inplace=True)\ny=auto.iloc[:,0]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nX.describe()","e404603c":"lassoreg = Lasso(alpha=0.1,normalize=True, max_iter=1e5)\nlassoreg.fit(X_train,y_train)","cc5132c6":"print(lassoreg.coef_)","848079b5":"df=pd.DataFrame()\nalpha=[0.0001,0.001,0.01,0.1,1,10]\ntmse=np.empty(len(alpha))\ni=0\nfor k in alpha:\n    lassoreg = Lasso(k,normalize=True, max_iter=1e5)\n    lassoreg.fit(X_train,y_train)\n    #a_row = pd.DataFrame([X.columns, lassoreg.coef_])\n    #row_df = pd.DataFrame([a_row])\n    df[str(k)]=lassoreg.coef_.tolist()\n    y_exp=lassoreg.predict(X_test)\n    #tmse[k1] = mean_squared_error(y_test,y_exp)*y_test.shape[0]\/(y_test.shape[0]-len(list(i)))\n    tmse[i] = mean_squared_error(y_test,y_exp)*y_test.shape[0]\/(y_test.shape[0])\n    i = i + 1\n    #df = pd.concat([row_df, df], ignore_index=False)\n    #df.append(lassoreg.coef_, ignore_index=True)\ndf_transposed = df.T\ndf_transposed.columns=X.columns\n#df_transposed['alpha']=alpha\n","82bff69c":"df_transposed.plot.line()","2a2b2b60":"val=pd.Series(tmse,index=alpha)\nval.plot()\n# Add title and axis names\nplt.title('Test MSE and Alpha')\nplt.xlabel('Alpha')\nplt.ylabel('Accuracy')\nplt.show() ","8e07470e":"# Question 2: How to fit a Ridge Model <a id=\"1\"><\/a>","145dd60b":"# Question 5: What is the effect of lambda <a id=\"5\"><\/a>","95727c9c":"# Question 1: How to do best features subset search <a id=\"1\"><\/a>","16b79718":"* [<font size=4>Question 1:How to do best features subset search<\/font>](#1)\n* [<font size=4>Question 2: How can we fit a ridge model <\/font>](#2)   \n* [<font size=4>Question 3: How can we fit a lasso model <\/font>](#3)   \n* [<font size=4>Question 4: How to fit a Elastic Net model  <\/font>](#4)    \n* [<font size=4>Question 5: Effect of Lambda <\/font>](#6)   ","65cca720":"# ","25dd2eec":"# Question 4: How to fit a ElasticNet Model <a id=\"4\"><\/a>","71f60552":"# Question 3: How to fit a Lasso Model <a id=\"3\"><\/a>","ef1a460d":"# Finding the one with best test MSE"}}