{"cell_type":{"1755fd8b":"code","e96d177b":"code","8776755a":"code","800bea48":"code","424957ae":"code","35c77ca6":"code","e2169dd0":"markdown","69f1d13c":"markdown","8e56d495":"markdown","92da7617":"markdown","b588de50":"markdown","5edd57ef":"markdown","4e4ce817":"markdown","9e9e16ca":"markdown","0190bda9":"markdown","6ca1560d":"markdown","7c00c528":"markdown","1a9dd884":"markdown","bd19db8c":"markdown"},"source":{"1755fd8b":"import os\nimport pandas as pd\nimport numpy as np\nimport transformers\nimport tokenizers","e96d177b":"from transformers import BertTokenizer\nTOKENIZER = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nenc = TOKENIZER.encode(\"Hello there!\")\ndec = TOKENIZER.decode(enc)\nprint(\"Encode: \" + str(enc))\nprint(\"Decode: \" + str(dec))\nprint(\"[CLS]: \" + str(enc[0]))\nprint(\"[SEP]: \" + str(enc[4]))\nprint(\"[PAD]: \" + str(TOKENIZER.encode(\"[PAD]\")[1]))","8776755a":"from transformers import RobertaTokenizer\nTOKENIZER = RobertaTokenizer.from_pretrained(\"roberta-base\")\nenc = TOKENIZER.encode(\"Hello there!\")\ndec = TOKENIZER.decode(enc)\nprint(\"Encode: \" + str(enc))\nprint(\"Decode: \" + str(dec))\nprint(\"[CLS]: \" + str(enc[0]))\nprint(\"[SEP]: \" + str(enc[4]))\nprint(\"<pad>: \" + str(TOKENIZER.encode(\"<pad>\")[1]))","800bea48":"from transformers import AlbertTokenizer\nTOKENIZER = AlbertTokenizer.from_pretrained(\"albert-base-v1\")\nenc = TOKENIZER.encode(\"Hello there!\")\ndec = TOKENIZER.decode(enc)\nprint(\"Encode: \" + str(enc))\nprint(\"Decode: \" + str(dec))\nprint(\"[CLS]: \" + str(enc[0]))\nprint(\"[SEP]: \" + str(enc[4]))\nprint(\"<pad>: \" + str(TOKENIZER.encode(\"<pad>\")[1]))","424957ae":"from transformers import BartTokenizer\nTOKENIZER = BartTokenizer.from_pretrained('bart-large')\nenc = TOKENIZER.encode(\"Hello there!\")\ndec = TOKENIZER.decode(enc)\nprint(\"Encode: \" + str(enc))\nprint(\"Decode: \" + str(dec))\nprint(\"[CLS]: \" + str(enc[0]))\nprint(\"[SEP]: \" + str(enc[4]))\nprint(\"<pad>: \" + str(TOKENIZER.encode(\"<pad>\")[1]))","35c77ca6":"from transformers import BertTokenizer\n# class:`~transformers.ElectraTokenizer` is identical to :class:`~transformers.BertTokenizer` and runs end-to-end tokenization: punctuation splitting + wordpiece.\nTOKENIZER = BertTokenizer.from_pretrained('google\/electra-base-generator')\nenc = TOKENIZER.encode(\"Hello there!\")\ndec = TOKENIZER.decode(enc)\nprint(\"Encode: \" + str(enc))\nprint(\"Decode: \" + str(dec))\nprint(\"[CLS]: \" + str(enc[0]))\nprint(\"[SEP]: \" + str(enc[4]))\nprint(\"[PAD]: \" + str(TOKENIZER.encode(\"[PAD]\")[1]))","e2169dd0":"**Note: There are also tons of other pretrained models other than ```\"albert-base-v1\"```:**\n```\nPRETRAINED_VOCAB_FILES_MAP = {\n    \"vocab_file\": {\n        \"albert-base-v1\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/albert-base-v1-spiece.model\",\n        \"albert-large-v1\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/albert-large-v1-spiece.model\",\n        \"albert-xlarge-v1\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/albert-xlarge-v1-spiece.model\",\n        \"albert-xxlarge-v1\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/albert-xxlarge-v1-spiece.model\",\n        \"albert-base-v2\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/albert-base-v2-spiece.model\",\n        \"albert-large-v2\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/albert-large-v2-spiece.model\",\n        \"albert-xlarge-v2\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/albert-xlarge-v2-spiece.model\",\n        \"albert-xxlarge-v2\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/albert-xxlarge-v2-spiece.model\",\n    }\n}\n\n```","69f1d13c":"## Import Useful Packages","8e56d495":"# TOKENIZERS","92da7617":"## <span style=\"color:green\">Bart <\/span>\n\n### Offline loading in Kaggle Notebook\n\n```\n[TODO]\n```\n\n\n### Online loading in Kaggle Notebook (needs Internet) using ```RobertaTokenizer```\n\n```\nhttps:\/\/huggingface.co\/transformers\/_modules\/transformers\/modeling_bart.html\n```\n\n<span style=\"color:blue\">Example usage: <\/span>","b588de50":"### This notebook contains handy information to help building NLP models using Hugging Face Library (https:\/\/huggingface.co\/transformers). \n\n#### I will keep updating this notebook. \n\n\nPlease upvote if this is useful!\n","5edd57ef":"## <span style=\"color:green\">BERT Base Uncased <\/span>\n\n### Offline loading in Kaggle Notebook (Thanks to https:\/\/www.kaggle.com\/abhishek)\n\n```\nhttps:\/\/www.kaggle.com\/abhishek\/bert-base-uncased\n```\n\nThat contains:\n\n```\nconfig.json\nvocab.txt\npytorch_model.bin\n```\n\n<span style=\"color:blue\">Example usage: <\/span>\n```\nimport tokenizers\n\nTOKENIZER = tokenizers.BertWordPieceTokenizer(\n    f\"{BERT_PATH}\/vocab.txt\", \n    lowercase=True\n)\n```\n\n\n### Online loading in Kaggle Notebook (needs Internet) using ```BertTokenizer```\n\n```\nhttps:\/\/huggingface.co\/transformers\/_modules\/transformers\/tokenization_bert.html\n```\n\n<span style=\"color:blue\">Example usage: <\/span>","4e4ce817":"## <span style=\"color:green\">ALBERT<\/span>\n\n### Offline loading in Kaggle Notebook\n\n```\n[TODO] Planning to create a dataset in future!\n```\n\n\n\n### Online loading in Kaggle Notebook (needs Internet) using ```AlbertTokenizer```\n\n```\nhttps:\/\/huggingface.co\/transformers\/_modules\/transformers\/tokenization_albert.html\n```\n\n<span style=\"color:blue\">Example usage: <\/span>","9e9e16ca":"<centre> ![image.png](attachment:image.png) \n\n# <center>   HuggingFace Tokenizer Cheat-Sheet","0190bda9":"**Note: There are also tons of other pretrained models other than ```\"roberta-base\"```:**\n\n```\nPRETRAINED_VOCAB_FILES_MAP = {\n    \"vocab_file\": {\n        \"roberta-base\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/roberta-base-vocab.json\",\n        \"roberta-large\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/roberta-large-vocab.json\",\n        \"roberta-large-mnli\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/roberta-large-mnli-vocab.json\",\n        \"distilroberta-base\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/distilroberta-base-vocab.json\",\n        \"roberta-base-openai-detector\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/roberta-base-vocab.json\",\n        \"roberta-large-openai-detector\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/roberta-large-vocab.json\",\n    },\n    \"merges_file\": {\n        \"roberta-base\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/roberta-base-merges.txt\",\n        \"roberta-large\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/roberta-large-merges.txt\",\n        \"roberta-large-mnli\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/roberta-large-mnli-merges.txt\",\n        \"distilroberta-base\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/distilroberta-base-merges.txt\",\n        \"roberta-base-openai-detector\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/roberta-base-merges.txt\",\n        \"roberta-large-openai-detector\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/roberta-large-merges.txt\",\n    },\n}\n```","6ca1560d":"\n**Note: There are also tons of other pretrained models other than ```\"bart-large\"```:**\n\n```\nBART_PRETRAINED_MODEL_ARCHIVE_MAP = {\n    \"bart-large\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/facebook\/bart-large\/pytorch_model.bin\",\n    \"bart-large-mnli\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/facebook\/bart-large-mnli\/pytorch_model.bin\",\n    \"bart-large-cnn\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/facebook\/bart-large-cnn\/pytorch_model.bin\",\n    \"bart-large-xsum\": \"https:\/\/s3.amazonaws.com\/models.huggingface.co\/bert\/facebook\/bart-large-xsum\/pytorch_model.bin\",\n}\n```","7c00c528":"## <span style=\"color:green\">ELECTRA <\/span>\n\n### Offline loading in Kaggle Notebook (Thanks to https:\/\/www.kaggle.com\/ratan123)\n\n```\nhttps:\/\/www.kaggle.com\/ratan123\/electra-base\n```\n\n\n<span style=\"color:blue\">Example usage: <\/span>\n```\nimport tokenizers\n\nELECTRA_PATH = \"\/kaggle\/input\/electra-base\/\"\nTOKENIZER = tokenizers.BertWordPieceTokenizer(\n    f\"{ELECTRA_PATH}\/vocab.txt\", \n    lowercase=True\n)\n```\n\n### Online loading in Kaggle Notebook (needs Internet) using ```ElectraTokenizer```\n\n```\nhttps:\/\/huggingface.co\/transformers\/_modules\/transformers\/tokenization_electra.html\n```\n\n<span style=\"color:blue\">Example usage: <\/span>","1a9dd884":"**Note: There are also tons of other pretrained models other than ```\"bert-base-uncased\"```:**\n```\nPRETRAINED_INIT_CONFIGURATION = {\n    \"bert-base-uncased\": {\"do_lower_case\": True},\n    \"bert-large-uncased\": {\"do_lower_case\": True},\n    \"bert-base-cased\": {\"do_lower_case\": False},\n    \"bert-large-cased\": {\"do_lower_case\": False},\n    \"bert-base-multilingual-uncased\": {\"do_lower_case\": True},\n    \"bert-base-multilingual-cased\": {\"do_lower_case\": False},\n    \"bert-base-chinese\": {\"do_lower_case\": False},\n    \"bert-base-german-cased\": {\"do_lower_case\": False},\n    \"bert-large-uncased-whole-word-masking\": {\"do_lower_case\": True},\n    \"bert-large-cased-whole-word-masking\": {\"do_lower_case\": False},\n    \"bert-large-uncased-whole-word-masking-finetuned-squad\": {\"do_lower_case\": True},\n    \"bert-large-cased-whole-word-masking-finetuned-squad\": {\"do_lower_case\": False},\n    \"bert-base-cased-finetuned-mrpc\": {\"do_lower_case\": False},\n    \"bert-base-german-dbmdz-cased\": {\"do_lower_case\": False},\n    \"bert-base-german-dbmdz-uncased\": {\"do_lower_case\": True},\n    \"bert-base-finnish-cased-v1\": {\"do_lower_case\": False},\n    \"bert-base-finnish-uncased-v1\": {\"do_lower_case\": True},\n    \"bert-base-dutch-cased\": {\"do_lower_case\": False},\n}\n```","bd19db8c":"## <span style=\"color:green\">RoBERTa <\/span>\n\n### Offline loading in Kaggle Notebook (Thanks to https:\/\/www.kaggle.com\/abhishek)\n\n```\nhttps:\/\/www.kaggle.com\/abhishek\/roberta-base\n```\n\nThat contains:\n\n```\nconfig.json\nvocab.json\nmerges.txt\npytorch_model.bin\n```\n\n\n<span style=\"color:blue\">Example usage: <\/span>\n```\nimport tokenizers\n\nROBERTA_PATH = \"..\/input\/roberta-base\"\nTOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"{ROBERTA_PATH}\/vocab.json\", \n    merges_file=f\"{ROBERTA_PATH}\/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True\n)\n```\n\n\n### Online loading in Kaggle Notebook (needs Internet) using ```RobertaTokenizer```\n\n```\nhttps:\/\/huggingface.co\/transformers\/_modules\/transformers\/tokenization_roberta.html\n```\n\n<span style=\"color:blue\">Example usage: <\/span>"}}