{"cell_type":{"ea11347c":"code","2c0340d0":"code","15c0d1fb":"code","3440c651":"code","d6ef2e62":"code","993e4e0a":"code","b274890d":"code","888fbe1e":"code","5e6f374b":"code","50a0cc47":"code","7c5d8695":"code","b1031c2d":"code","b7398b34":"code","a935e009":"code","d522e041":"code","39ccd208":"code","0044c332":"code","992110a5":"code","51272528":"code","c3c8a1ca":"code","b6467b94":"code","fc2b6de2":"code","46c3e53a":"code","08affbd0":"code","05600fba":"code","7b402ee5":"code","d2be8917":"code","0e87c9dc":"code","c52983d9":"code","8fa270c9":"code","e7b3f58a":"code","bc616a79":"code","cdf183b0":"code","d84d997a":"code","8bf2772e":"code","a464ca93":"code","c02dbde6":"code","dc5d77f5":"code","767a44aa":"code","5422e2e5":"code","3fd1cf05":"code","c87ca6c6":"code","6281f8f3":"code","04d135a9":"code","20638a5d":"code","36dbb6d8":"markdown","0f1d6be7":"markdown","c6fa5bec":"markdown","c2f06ccf":"markdown","68e9ab91":"markdown","cb44fd13":"markdown","831c0c7b":"markdown","6fb25501":"markdown","49d04fd5":"markdown","0d4a1024":"markdown","90cd4488":"markdown"},"source":{"ea11347c":"import pandas as pd\n\n# Selecting a subset of data to be faster in demonstration\ntrain_df = pd.read_csv('..\/input\/imdb-dataset-sentiment-analysis-in-csv-format\/Train.csv').head(4000)\nvalid_df = pd.read_csv('..\/input\/imdb-dataset-sentiment-analysis-in-csv-format\/Valid.csv').head(500)\ntest_df = pd.read_csv('..\/input\/imdb-dataset-sentiment-analysis-in-csv-format\/Test.csv').head(500)\nprint('Train: '+ str(len(train_df)))\nprint('Valid: '+ str(len(valid_df)))\nprint('Test: '+ str(len(test_df)))\ntrain_df.head(10)","2c0340d0":"# Turnig all text to lowercase\ntrain_df['text'] = train_df['text'].str.lower()\nvalid_df['text'] = valid_df['text'].str.lower()\ntest_df['text'] = test_df['text'].str.lower()\ntrain_df.head()","15c0d1fb":"# Removing ponctuation\nimport string\n\nexclude = set(string.punctuation) \n\ndef remove_punctuation(x): \n    try: \n        x = ''.join(ch for ch in x if ch not in exclude) \n    except: \n        pass \n    return x \n\ntrain_df['text'] = train_df['text'].apply(remove_punctuation)\nvalid_df['text'] = valid_df['text'].apply(remove_punctuation)\ntest_df['text'] = test_df['text'].apply(remove_punctuation)\ntrain_df.head()","3440c651":"# Removing stopwords\nfrom nltk.corpus import stopwords\n\nstop = stopwords.words('english')\n\ntrain_df['text'] = train_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\nvalid_df['text'] = valid_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ntest_df['text'] = test_df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\ntrain_df.head()","d6ef2e62":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create feature vectors for every sentence\nvectorizer = TfidfVectorizer(#min_df = 5,\n                             #max_df = 0.8,\n                             max_features = 20000,\n                             sublinear_tf = True,\n                             use_idf = True)#, stop_words='english')#vocabulary = list(embeddings_index.keys()\n\ntrain_vectors = vectorizer.fit_transform(train_df['text'])\nvalid_vectors = vectorizer.transform(valid_df['text'])\ntest_vectors = vectorizer.transform(test_df['text'])","993e4e0a":"from sklearn import svm\n# SVM\nclassifier_linear = svm.SVC(kernel='linear')\n#Train\nclassifier_linear.fit(train_vectors, train_df['label'])","b274890d":"from sklearn.metrics import classification_report\n\npredictions = classifier_linear.predict(test_vectors)\n# results\nreport = classification_report(test_df['label'], predictions)\nprint(report)","888fbe1e":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\n\n# Defining the NN model\nmodel = Sequential()\nmodel.add(Dense(20, input_shape=(train_vectors.shape[1],), activation='relu'))\nmodel.add(Dropout(0.3))\n#model.add(Dense(5, activation='relu'))\n#model.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\n# compile network\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","5e6f374b":"# Train\n#Salvar o melhor modelo\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,verbose=1)\n]\n\nhistory = model.fit(\n    train_vectors.toarray(), train_df['label'], \n    epochs=20, \n    verbose=1,\n    callbacks = callbacks_list,\n    validation_data=(valid_vectors.toarray(), valid_df['label']))","50a0cc47":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","7c5d8695":"from tensorflow.keras.models import load_model\n# Load the best saved model\nmodel = load_model('model.h5')\n\ny_pred = model.predict_classes(valid_vectors.toarray())\nprint(classification_report(valid_df['label'], y_pred, target_names=['0','1']))","b1031c2d":"import tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nimport string\n\n# Model constants.\nmax_features = 20000\nembedding_dim = 100\nsequence_length = 500\n\n# function to stardardize texts\ndef custom_standardization(input_data):\n    lowercase = tf.strings.lower(input_data)\n    #stripped_html = tf.strings.regex_replace(lowercase, \"<br \/>\", \" \")\n    #return tf.strings.regex_replace(\n    #    stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n    #)\n    return lowercase\n\n# normalize, split, and map strings to integers\nvectorize_layer = TextVectorization(\n    #standardize=custom_standardization,\n    max_tokens=max_features,\n    output_mode=\"int\",\n    output_sequence_length=sequence_length,\n)\n\n# Creating the vocabulary\nvectorize_layer.adapt(train_df['text'].values)","b7398b34":"# Vetorizing all the texts\ndef vectorize_text(text):\n    text = tf.expand_dims(text, -1)\n    return vectorize_layer(text)\n\n\n# Vectorize the data.\ntrain_ds = vectorize_text(train_df['text'])\nvalid_ds = vectorize_text(valid_df['text'])\ntest_ds = vectorize_text(test_df[ 'text'])\n","a935e009":"print(train_df['text'][0])\nprint(train_ds[0])\nprint(vectorize_text(['beautiful pretty']))","d522e041":"from keras import layers\n\nmodel = Sequential()\n# A integer input for vocab indices.\nmodel.add(layers.Input(shape=(None,), dtype=\"int64\"))\n# Layer to map those vocab indices into a space of dimensionality 'embedding_dim'.\nmodel.add(layers.Embedding(max_features, embedding_dim))\n# Conv1D + global max pooling\nmodel.add(layers.Conv1D(50, 7, padding=\"valid\", activation=\"relu\", strides=3))#200\n#model.add(layers.Conv1D(100, 7, padding=\"valid\", activation=\"relu\", strides=3))\nmodel.add(layers.GlobalMaxPooling1D())\n\n# Common hidden layer for final classification\nmodel.add(layers.Dense(10, activation=\"relu\"))#100\nmodel.add(layers.Dropout(0.5))\n# Single unit output layer with sigmoid activation\nmodel.add(layers.Dense(1, activation=\"sigmoid\", name=\"predictions\"))\n\n# Compile the model\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","39ccd208":"# Train\n#Salvar o melhor modelo\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,verbose=1)\n]\n\nhistory = model.fit(\n    train_ds, train_df['label'], \n    epochs=20, \n    verbose=1,\n    callbacks = callbacks_list,\n    validation_data=(valid_ds, valid_df['label']))","0044c332":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","992110a5":"model = load_model('model.h5')\n\ny_pred = model.predict_classes(test_ds)\nprint(classification_report(test_df['label'], y_pred, target_names=['0','1']))","51272528":"model = Sequential()\n# Input for variable-length sequences of integers\nmodel.add(keras.Input(shape=(None,), dtype=\"int64\"))\n# Embed each integer in a embedding_dim vector\nmodel.add(layers.Embedding(max_features, embedding_dim))\n# Add 2 bidirectional LSTMs\n#model.add(layers.Bidirectional(layers.LSTM(32, return_sequences=True)))\nmodel.add(layers.Bidirectional(layers.LSTM(32)))\n# Add a classifier\nmodel.add(layers.Dense(1, activation=\"sigmoid\"))\n\n# Compile the model\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","c3c8a1ca":"# Train\n#Salvar o melhor modelo\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,verbose=1)\n]\n\nhistory = model.fit(\n    train_ds, train_df['label'], \n    epochs=20, \n    verbose=1,\n    callbacks = callbacks_list,\n    validation_data=(valid_ds, valid_df['label']))","b6467b94":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","fc2b6de2":"model = load_model('model.h5')\n\ny_pred = model.predict_classes(test_ds)\nprint(classification_report(test_df['label'], y_pred, target_names=['0','1']))","46c3e53a":"voc = vectorize_layer.get_vocabulary()\nword_index = dict(zip(voc, range(len(voc))))","08affbd0":"word_index","05600fba":"test = [\"pretty\", \"cat\", \"sat\", \"near\", \"yellow\", \"cat\"]\n[word_index[w] for w in test]","7b402ee5":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip -q glove.6B.zip","d2be8917":"!ls","0e87c9dc":"embeddings_index = {}\nwith open(\"glove.6B.100d.txt\") as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index))","c52983d9":"embeddings_index[\"cat\"]","8fa270c9":"num_tokens = len(voc) + 2\n#embedding_dim = 100\nhits = 0\nmisses = 0\n\n# Prepare embedding matrix to be used in a Embedding layer\n# matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","e7b3f58a":"embeddings_index[\"movie\"]","bc616a79":"word_index[\"movie\"]","cdf183b0":"embedding_matrix[3]","d84d997a":"# Visualizing with T-SNE\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nmax_w = 1000\n# Creates and TSNE model and plots it\nlabels = []\ntokens = []\nfor word, i in word_index.items():\n    if i == max_w:\n        break\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        tokens.append(embedding_vector)\n        labels.append(word)\ntsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\nnew_values = tsne_model.fit_transform(tokens)\nx = []\ny = []\nfor value in new_values:\n    x.append(value[0])\n    y.append(value[1])\nplt.figure(figsize=(16, 16))\nfor i in range(len(x)):\n    plt.scatter(x[i],y[i])\n    plt.annotate(labels[i],\n        xy=(x[i], y[i]),\n        xytext=(5, 2),\n        textcoords='offset points',\n        ha='right',\n        va='bottom')\nplt.show()\n","8bf2772e":"# load the pre-trained word embeddings matrix into an Embedding layer. trainable = False\nembedding_layer = layers.Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n    trainable=False,\n)","a464ca93":"model = Sequential()\n# Input for variable-length sequences of integers\nmodel.add(keras.Input(shape=(None,), dtype=\"int64\"))\n# load the pre-trained word embeddings matrix into an Embedding layer. trainable = False\nmodel.add(layers.Embedding(num_tokens,embedding_dim, embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=False))\n# Add 2 bidirectional LSTMs\nmodel.add(layers.Bidirectional(layers.LSTM(32, return_sequences=True)))\nmodel.add(layers.Bidirectional(layers.LSTM(32)))\n# Add a classifier\nmodel.add(layers.Dense(1, activation=\"sigmoid\"))\n\n# Compile the model\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","c02dbde6":"# Train\n#Salvar o melhor modelo\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,verbose=1)\n]\n\nhistory = model.fit(\n    train_ds, train_df['label'], \n    epochs=20, \n    verbose=1,\n    callbacks = callbacks_list,\n    validation_data=(valid_ds, valid_df['label']))","dc5d77f5":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","767a44aa":"model = load_model('model.h5')\n\ny_pred = model.predict(test_ds)\ny_pred = [1 if x >=0.5 else 0 for x in y_pred]\nprint(classification_report(test_df['label'], y_pred, target_names=['0','1']))","5422e2e5":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, name=None, **kwargs):\n        super(TransformerBlock, self).__init__(name=name)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.ff_dim = ff_dim\n        self.rate = rate\n        super(TransformerBlock, self).__init__(**kwargs)\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'embed_dim': self.embed_dim,\n            'num_heads': self.num_heads,\n            'ff_dim': self.ff_dim,\n            'rate': self.rate,\n        })\n        return config","3fd1cf05":"#Two seperate embedding layers, one for tokens, one for token index (positions).\nclass TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim, name=None, **kwargs):\n        super(TokenAndPositionEmbedding, self).__init__(name=name)\n        self.maxlen = maxlen\n        self.vocab_size = vocab_size\n        self.embed_dim = embed_dim\n        super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions\n    \n    def get_config(self):\n        config = super().get_config().copy()\n        config.update({\n            'maxlen': self.maxlen,\n            'vocab_size': self.vocab_size,\n            'embed_dim': self.embed_dim\n        })\n        return config","c87ca6c6":"#max_features = 20000\n#embedding_dim = 100\n#sequence_length = 500\n\n#embed_dim = 32  # Embedding size for each token\nnum_heads = 2  # Number of attention heads\nff_dim = 32  # Hidden layer size in feed forward network inside transformer\n\ninputs = layers.Input(shape=(sequence_length,))\nembedding_layer = TokenAndPositionEmbedding(sequence_length, max_features, embedding_dim)\nx = embedding_layer(inputs)\ntransformer_block = TransformerBlock(embedding_dim, num_heads, ff_dim)\nx = transformer_block(x)\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dropout(0.1)(x)\nx = layers.Dense(20, activation=\"relu\")(x)\nx = layers.Dropout(0.1)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\n#model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","6281f8f3":"# Train\n#Salvar o melhor modelo\ncallbacks_list = [\n    keras.callbacks.ModelCheckpoint(\n        filepath='model.h5',\n        monitor='val_loss', save_best_only=True, verbose=1),\n    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,verbose=1)\n]\n\nhistory = model.fit(\n    train_ds, train_df['label'], \n    epochs=10, \n    verbose=1,\n    callbacks = callbacks_list,\n    validation_data=(valid_ds, valid_df['label'])\n)","04d135a9":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","20638a5d":"model = load_model('model.h5', custom_objects={'TokenAndPositionEmbedding': TokenAndPositionEmbedding, 'TransformerBlock': TransformerBlock })\n\ny_pred = model.predict(test_ds)\ny_pred = [1 if x >=0.5 else 0 for x in y_pred]\nprint(classification_report(test_df['label'], y_pred, target_names=['0','1']))","36dbb6d8":"## A LSTM Recursive Model","0f1d6be7":"## Glove Embeddings","c6fa5bec":"# 4. Using Word Embeddings","c2f06ccf":"## A Convolutional Model","68e9ab91":"## Classical Model with TF-IDF and SVM","cb44fd13":"## Some text pre-processing","831c0c7b":"# 2. Sentences as Bag of Words","6fb25501":"# 1. Reading the Dataset","49d04fd5":"## Changing the classifier by a NN model","0d4a1024":"# 3. Sentences as stream of words","90cd4488":"## Using a Transformer"}}