{"cell_type":{"7bc69e2e":"code","1a7fedc4":"code","e645fa47":"code","f2b365a2":"code","cb070606":"code","75d14d40":"code","5070ed27":"code","d9ed6caa":"code","08088db4":"code","f714d126":"code","d86d9b30":"code","f730024b":"code","dedd554b":"code","c2f8209a":"code","11500230":"code","0e773f99":"code","c96200bc":"code","577a05f8":"code","6aaee1a0":"code","99379392":"code","a0c4742d":"code","50c1fbbb":"code","fe4d8984":"code","aac4c666":"code","a0de3718":"code","37e798a8":"code","781e6b8f":"code","9cfbbe1d":"code","0b76676b":"code","782224ec":"code","bb4440a9":"code","62bb813e":"code","ab9a024e":"code","d71ce4d7":"code","5c6d165a":"code","1647e250":"code","8e0ae504":"code","0ede0103":"code","12b61984":"code","ff79aa00":"code","e4e2aecd":"code","a3ab14e9":"code","449418d8":"code","4bce4353":"code","d454597e":"code","7c3028a7":"code","5a176e3a":"markdown","556593a3":"markdown","3ccabe6a":"markdown","b43b6bc1":"markdown","d87ea893":"markdown","f7a21b2a":"markdown","2014769d":"markdown","b24f9004":"markdown","b82e6279":"markdown","ee38557a":"markdown","92e892ca":"markdown","b8e36d96":"markdown","8eac7a97":"markdown"},"source":{"7bc69e2e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1a7fedc4":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nimport gc\nimport re\nimport folium\n#import textstat\nfrom scipy import stats\nfrom colorama import Fore, Back, Style, init\n\nimport math\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\n\nimport random\nimport networkx as nx\nfrom pandas import Timestamp\n\nfrom PIL import Image\nfrom IPython.display import SVG\nfrom keras.utils import model_to_dot\n\nimport requests\nfrom IPython.display import HTML\n\nimport seaborn as sns\nfrom tqdm import tqdm\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\n\ntqdm.pandas()\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\n#import transformers\nimport tensorflow as tf\n\nfrom tensorflow.keras.callbacks import Callback\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n\nfrom tensorflow.keras.models import Model\n#from kaggle_datasets import KaggleDatasets\n#from tensorflow.keras.optimizers import Adam\n#from tokenizers import BertWordPieceTokenizer\nfrom tensorflow.keras.layers import Dense, Input, Dropout, Embedding\nfrom tensorflow.keras.layers import LSTM, GRU, Conv1D, SpatialDropout1D\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import activations\nfrom tensorflow.keras import constraints\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\n\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.activations import *\nfrom tensorflow.keras.constraints import *\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.regularizers import *\n\nfrom sklearn import metrics\nfrom sklearn.utils import shuffle\nfrom gensim.models import Word2Vec\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer,\\\n                                            CountVectorizer,\\\n                                            HashingVectorizer\n\nfrom nltk.stem.wordnet import WordNetLemmatizer \nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import TweetTokenizer  \n\nimport nltk\nfrom textblob import TextBlob\n\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\n#from googletrans import Translator\nfrom nltk import WordNetLemmatizer\n#from polyglot.detect import Detector\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nstopword=set(STOPWORDS)\n\nlem = WordNetLemmatizer()\ntokenizer=TweetTokenizer()\n\nnp.random.seed(0)","e645fa47":"train_data = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv')\nval_data = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv')","f2b365a2":"train_data.head()","cb070606":"test_data.head()","75d14d40":"val_data.head()","5070ed27":"train_data.columns","d9ed6caa":"train_data.info()","08088db4":"train_data.shape","f714d126":"train_data.isnull().sum()","d86d9b30":"train_data[\"toxic\"].value_counts()","f730024b":"# making a donut chart to represent share of each.\n\nsize = [202165, 21384]\ncolors = ['pink', 'lightblue']\nlabels = \"0\",\"1\"\n\nmy_circle = plt.Circle((0, 0), 0.7, color = 'white')\n\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.pie(size, colors = colors, labels = labels, autopct = '%.2f%%')\nplt.axis('off')\nplt.title('A Pie Chart Representing share of Toxic Comments', fontsize = 30)\np = plt.gcf()\nplt.gca().add_artist(my_circle)\nplt.legend()\nplt.show()","dedd554b":"\nNegative_sentiments = \" \".join([text for text in train_data['comment_text'][train_data['toxic'] == 1]])\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'lightgreen', stopwords = stopwords, width = 1200, height = 800).generate(Negative_sentiments)\n\nplt.rcParams['figure.figsize'] = (15, 15)\nplt.title('Most Common Negative Toxic Words', fontsize = 30)\nprint(wordcloud)\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","c2f8209a":"# making Words cloud for the postive sentiments\n\nPositive_sentiments = \" \".join([text for text in train_data['comment_text'][train_data['toxic'] == 0]])\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'pink', stopwords = stopwords, width = 1200, height = 800).generate(Positive_sentiments)\n\nplt.rcParams['figure.figsize'] = (15, 15)\nplt.title('Most Common Positive Words', fontsize = 30)\nprint(wordcloud)\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","11500230":"train_data[\"severe_toxic\"].value_counts()","0e773f99":"# making a donut chart \n\nsize = [221587, 1962]\ncolors = ['yellow', 'orange']\nlabels = \"0\",\"1\"\n\nmy_circle = plt.Circle((0, 0), 0.7, color = 'white')\n\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.pie(size, colors = colors, labels = labels, autopct = '%.2f%%')\nplt.axis('off')\nplt.title('A Pie Chart Representing the Share of Toxic', fontsize = 30)\np = plt.gcf()\nplt.gca().add_artist(my_circle)\nplt.legend()\nplt.show()","c96200bc":"\n\nNegative_sentiments = \" \".join([text for text in train_data['comment_text'][train_data['severe_toxic'] == 1]])\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'cyan', stopwords = stopwords, width = 1200, height = 800).generate(Negative_sentiments)\n\nplt.rcParams['figure.figsize'] = (15, 15)\nplt.title('Most Common Negative severe toxic words', fontsize = 30)\nprint(wordcloud)\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","577a05f8":"\nPositive_sentiments = \" \".join([text for text in train_data['comment_text'][train_data['severe_toxic'] == 0]])\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'yellow', stopwords = stopwords, width = 1200, height = 800).generate(Positive_sentiments)\n\nplt.rcParams['figure.figsize'] = (15, 15)\nplt.title('Most Common Positive Words', fontsize = 30)\nprint(wordcloud)\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","6aaee1a0":"train_data[\"obscene\"].value_counts()","99379392":"# making a donut chart to represent share of each ratings\n\nsize = [211409, 12140]\ncolors = ['cyan', 'maroon']\nlabels = \"0\",\"1\"\n\nmy_circle = plt.Circle((0, 0), 0.7, color = 'white')\n\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.pie(size, colors = colors, labels = labels, autopct = '%.2f%%')\nplt.axis('off')\nplt.title('A Pie Chart Representing the Share of Toxic', fontsize = 30)\np = plt.gcf()\nplt.gca().add_artist(my_circle)\nplt.legend()\nplt.show()","a0c4742d":"\n\nNegative_sentiments = \" \".join([text for text in train_data['comment_text'][train_data[\"obscene\"] == 1]])\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'pink', stopwords = stopwords, width = 1200, height = 800).generate(Negative_sentiments)\n\nplt.rcParams['figure.figsize'] = (15, 15)\nplt.title('Most Common Negative Obscene words', fontsize = 30)\nprint(wordcloud)\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","50c1fbbb":"\n\nPositive_sentiments = \" \".join([text for text in train_data['comment_text'][train_data[\"obscene\"] == 0]])\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'grey', stopwords = stopwords, width = 1200, height = 800).generate(Positive_sentiments)\n\nplt.rcParams['figure.figsize'] = (15, 15)\nplt.title('Most Common Words n Positive', fontsize = 30)\nprint(wordcloud)\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","fe4d8984":"train_data[\"threat\"].value_counts()","aac4c666":"# making a donut chart to represent share of each ratings\n\nsize = [222860,  689]\ncolors = ['lightgreen', 'yellow']\nlabels = \"0\",\"1\"\n\nmy_circle = plt.Circle((0, 0), 0.7, color = 'white')\n\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.pie(size, colors = colors, labels = labels, autopct = '%.2f%%')\nplt.axis('off')\nplt.title('A Pie Chart Representing the Share of Toxic', fontsize = 30)\np = plt.gcf()\nplt.gca().add_artist(my_circle)\nplt.legend()\nplt.show()","a0de3718":"Negative_sentiments = \" \".join([text for text in train_data['comment_text'][train_data[\"threat\"] == 1]])\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'blue', stopwords = stopwords, width = 1200, height = 800).generate(Negative_sentiments)\n\nplt.rcParams['figure.figsize'] = (15, 15)\nplt.title('Most Common Negative threat Words', fontsize = 30)\nprint(wordcloud)\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","37e798a8":"Positive_sentiments = \" \".join([text for text in train_data['comment_text'][train_data[\"obscene\"] == 0]])\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'black', stopwords = stopwords, width = 1200, height = 800).generate(Positive_sentiments)\n\nplt.rcParams['figure.figsize'] = (15, 15)\nplt.title('Most Common Words n Positive', fontsize = 30)\nprint(wordcloud)\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","781e6b8f":"train_data[\"identity_hate\"].value_counts()","9cfbbe1d":"# making a donut chart to represent share of each ratings\n\nsize = [221432,   2117]\ncolors = ['yellow', 'magenta']\nlabels = \"0\",\"1\"\n\nmy_circle = plt.Circle((0, 0), 0.7, color = 'white')\n\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.pie(size, colors = colors, labels = labels, autopct = '%.2f%%')\nplt.axis('off')\nplt.title('A Pie Chart Representing the Share of Toxic', fontsize = 30)\np = plt.gcf()\nplt.gca().add_artist(my_circle)\nplt.legend()\nplt.show()","0b76676b":"\n\nNegative_sentiments = \" \".join([text for text in train_data['comment_text'][train_data['identity_hate'] == 1]])\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'purple', stopwords = stopwords, width = 1200, height = 800).generate(Negative_sentiments)\n\nplt.rcParams['figure.figsize'] = (15, 15)\nplt.title('Most Common  NEGATIVE HATE WORDS', fontsize = 30)\nprint(wordcloud)\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","782224ec":"# Postive sentiments\n\nPositive_sentiments = \" \".join([text for text in train_data['comment_text'][train_data['identity_hate'] == 0]])\n\nfrom wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'lightblue', stopwords = stopwords, width = 1200, height = 800).generate(Positive_sentiments)\n\nplt.rcParams['figure.figsize'] = (15, 15)\nplt.title('Most Common Words n Positive_sentiments', fontsize = 30)\nprint(wordcloud)\nplt.axis('off')\nplt.imshow(wordcloud)\nplt.show()","bb4440a9":"def new_len(x):\n    if type(x) is str:\n        return len(x.split())\n    else:\n        return 0\n\ntrain_data[\"comment_words\"] = train_data[\"comment_text\"].apply(new_len)\nnums = train_data.query(\"comment_words != 0 and comment_words < 200\").sample(frac=0.1)[\"comment_words\"]\nfig = ff.create_distplot(hist_data=[nums],\n                         group_labels=[\"All comments\"],\n                         colors=[\"black\"])\n\nfig.update_layout(title_text=\"Comment words\", xaxis_title=\"Comment words\", template=\"simple_white\", showlegend=False)\nfig.show()","62bb813e":"fig = go.Figure(data=[\n    go.Pie(labels=train_data.columns[2:7],\n           values=train_data.iloc[:, 2:7].sum().values, marker=dict(colors=px.colors.qualitative.Plotly))\n])\nfig.update_traces(textposition='outside', textfont=dict(color=\"black\"))\nfig.update_layout(title_text=\"Pie chart of labels\")\nfig.show()","ab9a024e":"fig = go.Figure(data=[\n    go.Bar(y=train_data.columns[2:7],\n           x=train_data.iloc[:, 2:7].sum().values, marker=dict(color=px.colors.qualitative.Plotly))\n])\n\nfig.data[0].marker.line.color = 'rgb(0, 0, 0)'\nfig.data[0].marker.line.width = 0.75\nfig.update_traces(orientation=\"h\")\nfig.update_layout(title_text=\"Bar chart of labels\", template=\"plotly_white\")\nfig.show()","d71ce4d7":"val = val_data\ntrain = train_data\n\ndef clean(text):\n    text = text.fillna(\"fillna\").str.lower()\n    text = text.map(lambda x: re.sub('\\\\n',' ',str(x)))\n    text = text.map(lambda x: re.sub(\"\\[\\[User.*\",'',str(x)))\n    text = text.map(lambda x: re.sub(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\",'',str(x)))\n    text = text.map(lambda x: re.sub(\"\\(http:\/\/.*?\\s\\(http:\/\/.*\\)\",'',str(x)))\n    return text\n\nval[\"comment_text\"] = clean(val[\"comment_text\"])\ntest_data[\"content\"] = clean(test_data[\"content\"])\ntrain[\"comment_text\"] = clean(train[\"comment_text\"])","5c6d165a":"class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))","1647e250":"def fast_encode(texts, tokenizer, chunk_size=240, maxlen=512):\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n    \n    for i in range(0, len(texts), chunk_size):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n    \n    return np.array(all_ids)","8e0ae504":"from tokenizers import Tokenizer, Encoding\nfrom tokenizers import decoders\nfrom tokenizers import models\nfrom tokenizers import normalizers\nfrom tokenizers import pre_tokenizers\nfrom tokenizers import processors\nfrom tokenizers import trainers\nfrom tokenizers import BertWordPieceTokenizer","0ede0103":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","12b61984":"train_data.drop(['severe_toxic','obscene','threat','insult','identity_hate'],axis=1,inplace=True)","ff79aa00":"train_data['comment_text'].apply(lambda x:len(str(x).split())).max()","e4e2aecd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU,SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping\n\nxtrain, xvalid, ytrain, yvalid = train_test_split(train_data.comment_text.values, train.toxic.values, \n                                                  stratify=train.toxic.values, \n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)","a3ab14e9":"# using keras tokenizer here\ntoken = text.Tokenizer(num_words=None)\nmax_len = 1500\n\ntoken.fit_on_texts(list(xtrain) + list(xvalid))\nxtrain_seq = token.texts_to_sequences(xtrain)\nxvalid_seq = token.texts_to_sequences(xvalid)\n\n#zero pad the sequences\nxtrain_pad = sequence.pad_sequences(xtrain_seq, maxlen=max_len)\nxvalid_pad = sequence.pad_sequences(xvalid_seq, maxlen=max_len)\n\nword_index = token.word_index","449418d8":"%%time\nwith strategy.scope():\n    # A simpleRNN without any pretrained embeddings and one dense layer\n    model = Sequential()\n    model.add(Embedding(len(word_index) + 1,\n                     300,\n                     input_length=max_len))\n    model.add(SimpleRNN(100))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \nmodel.summary()","4bce4353":"model.fit(xtrain_pad, ytrain, batch_size=64*strategy.num_replicas_in_sync) ","d454597e":"def roc_auc(predictions,target):\n    '''\n    This methods returns the AUC Score when given the Predictions\n    and Labels\n    '''\n    \n    fpr, tpr, thresholds = metrics.roc_curve(target, predictions)\n    roc_auc = metrics.auc(fpr, tpr)\n    return roc_auc","7c3028a7":"\nscores = model.predict(xvalid_pad)\nprint(\"Auc: %.2f%%\" % (roc_auc(scores,yvalid)))","5a176e3a":"It only takes one toxic comment to sour an online discussion. The Conversation AI team, a research initiative founded by Jigsaw and Google, builds technology to protect voices in conversation. A main area of focus is machine learning models that can identify toxicity in online conversations, where toxicity is defined as anything rude, disrespectful or otherwise likely to make someone leave a discussion. If these toxic contributions can be identified, we could have a safer, more collaborative internet.\n\nIn the previous 2018 Toxic Comment Classification Challenge, Kagglers built multi-headed models to recognize toxicity and several subtypes of toxicity. In 2019, in the Unintended Bias in Toxicity Classification Challenge, you worked to build toxicity models that operate fairly across a diverse range of conversations. This year, we're taking advantage of Kaggle's new TPU support and challenging you to build multilingual models with English-only training data.\n\nJigsaw's API, Perspective, serves toxicity models and others in a growing set of languages (see our documentation for the full list). Over the past year, the field has seen impressive multilingual capabilities from the latest model innovations, including few- and zero-shot learning. We're excited to learn whether these results \"translate\" (pun intended!) to toxicity classification. Your training data will be the English data provided for our previous two competitions and your test data will be Wikipedia talk page comments in several different languages.\n\nAs our computing resources and modeling capabilities grow, so does our potential to support healthy conversations across the globe. Develop strategies to build effective multilingual models and you'll help Conversation AI and the entire industry realize that potential.\n\nDisclaimer: The dataset for this competition contains text that may be considered profane, vulgar, or offensive.","556593a3":"**SEVERE TOXIC**","3ccabe6a":"**\nObscene\n**","b43b6bc1":"**Words cloud for the Negative Obscene words**","d87ea893":"**TOXIC**","f7a21b2a":" **Words cloud for Negative severe_toxic sentiments**","2014769d":"**Words cloud for the Negative THreat words**","b24f9004":"**THREAT**","b82e6279":"**Words cloud for NEGATIVE HATE WORDS**","ee38557a":"**Words cloud of Negative toxic sentiments**","92e892ca":"**Words cloud of Positive sentiments**","b8e36d96":"**IDENTITY HATE\n**","8eac7a97":"**Distribution of comment words**"}}