{"cell_type":{"c63a7988":"code","3fa234bc":"code","7d1730b8":"code","fe6339d0":"code","e5906a23":"code","aa403719":"code","c6afaa0b":"code","5307ec29":"code","554541fc":"code","3f60b0e3":"code","7dd5c762":"code","05b7f599":"code","76720687":"code","f26e9dd7":"code","4311b99e":"code","44e8c450":"code","a5a91705":"code","4c29e136":"code","995ebad5":"code","9cfa4486":"code","0e7a8022":"code","4979d21d":"code","aa7c01ce":"code","cede6260":"code","df389e38":"code","7ae6b5e6":"code","6b860a7a":"code","a75548f7":"code","75bf5515":"code","b9faf530":"markdown","b6ef79ff":"markdown","0e3dea7b":"markdown","dc78bed4":"markdown","d7bb4e54":"markdown","5dcd443e":"markdown","bb55623a":"markdown","56558290":"markdown","b672f815":"markdown","262e3f73":"markdown","07c42e6d":"markdown","53396e91":"markdown","37d46769":"markdown"},"source":{"c63a7988":"import pandas as pd\nimport numpy as np\nimport os\nimport datetime as dt\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.feature_selection import VarianceThreshold\nfrom tqdm import tqdm_notebook as tqdm\n\nimport lightgbm as lgbm","3fa234bc":"os.listdir('..\/input\/')","7d1730b8":"sns.set(style = 'darkgrid', palette = 'spring')\n\npd.set_option('display.max_columns', 400)\npd.set_option('max_rows', 1000)","fe6339d0":"DATA_FOLDER = '..\/input\/infopulsehackathon\/'\nSUBMISSIONS_FOLDER = ''\nos.listdir(DATA_FOLDER)","e5906a23":"df  = pd.read_csv(DATA_FOLDER+'train.csv', index_col='Id')\ndf.shape","aa403719":"df.head()","c6afaa0b":"target = 'Energy_consumption'","5307ec29":"df.isna().any().any()","554541fc":"fig, ax = plt.subplots(1,2,figsize=(10, 25))\ndf.nunique()[:(df.shape[1] \/\/ 2):].sort_index(ascending=False).plot(kind='barh', ax = ax[0])\ndf.nunique()[(df.shape[1] \/\/ 2):].sort_index(ascending=False).plot(kind='barh', ax = ax[1])\nax[0].set_xlim(0, 4000)\nax[1].set_xlim(0, 4000)\nfig.suptitle('Count unique values of each feature');\nfig.tight_layout(rect=[0, 0.05, 1, 0.98])","3f60b0e3":"print('Stats of unique values number:')\ndf.nunique().describe()","7dd5c762":"df.select_dtypes(exclude = np.number)","05b7f599":"print('Number of unique values:')\ndf.select_dtypes(exclude = np.number).nunique()","76720687":"var_cuter = VarianceThreshold(threshold=0.1)\ndf_var = df.drop(target, axis=1).select_dtypes(np.number)\nvar_mask = var_cuter.fit(df_var).get_support()\nvar_drop_cols = df_var.iloc[:,var_mask==False].columns.tolist()\n\nprint('low-variance features:')\nvar_drop_cols","f26e9dd7":"print('Features that have single value')\ndf.nunique()[df.nunique()<2]","4311b99e":"drop_cols = df.nunique()[df.nunique()<2].index.tolist()\ndrop_cols += var_drop_cols\nprint(f'{len(drop_cols)} features will be droped:')\ndrop_cols","44e8c450":"cat_cols = df.select_dtypes(exclude= np.number).columns.tolist() # feature_3, feature_4, feature_257, feature_258\ncat_cols += df.nunique()[df.nunique() < 5].index.tolist() # features that have less that n unique values\ncat_cols = [col for col in cat_cols if col not in drop_cols]\ncat_cols = list(set(cat_cols))\nlen(cat_cols)","a5a91705":"X = df.drop([target]+drop_cols, axis=1)\ny = df[target]","4c29e136":"n_splits=5\nn_repeats=10\n\nnum_iter = 300000\nearly_stopping_rounds = 10000\nverbose = 10000\n\nlgb_params = {\n    'learning_rate': 0.0005,\n    'num_leaves': 15,\n    'feature_fraction': 0.1,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'lambda_l1': 8,\n    'application': 'regression',\n    'metric': ['mse'],\n    'num_threads': -1,\n    'seed': 42}\n","995ebad5":"models = []\n\nkf = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n\nk=0\nfor train_ids, val_ids in tqdm(kf.split(X), desc='KFold', total=n_splits*n_repeats):\n    k+=1\n    print(f'\\nFOLD {k}')\n    print(dt.datetime.now().time())\n    X_train, y_train = X.iloc[train_ids].copy(), y.iloc[train_ids]\n    X_val, y_val = X.iloc[val_ids].copy(), y.iloc[val_ids]\n\n    X_train[cat_cols] = X_train[cat_cols].astype('category')\n    X_val[cat_cols] = X_val[cat_cols].astype('category')\n\n    lgb_train = lgbm.Dataset(X_train, y_train)\n    lgb_eval = lgbm.Dataset(X_val, y_val)\n\n    lgb = lgbm.train(lgb_params,\n                lgb_train,\n                num_boost_round=num_iter,\n                valid_sets=(lgb_train, lgb_eval),\n                valid_names=('train', 'val'),\n               early_stopping_rounds=early_stopping_rounds,\n               verbose_eval = verbose)\n    \n    models.append(lgb)\n","9cfa4486":"def get_score_lgb(model, name='val'):\n    return list(model.best_score[name].values())[0]\n\ntrain_scores = np.array([get_score_lgb(model, 'train') for model in models])\nval_scores = np.array([get_score_lgb(model, 'val') for model in models])\n\nprint('mean train score: {:.0f}+-{:.0f}'.format(train_scores.mean(), train_scores.std()))\nprint('min {:.0f}; max {:.0f}\\n'.format(train_scores.min(), train_scores.max()))\nprint('mean validation score: {:.0f}+-{:.0f}'.format(val_scores.mean(), val_scores.std()))\nprint('min {:.0f}; max {:.0f}\\n'.format(val_scores.min(), val_scores.max()))","0e7a8022":"# lgbm.plot_importance(models[0], importance_type='gain', figsize=(5,40), ignore_zero=False)","4979d21d":"df_test = pd.read_csv(DATA_FOLDER + 'test.csv', index_col='Id')\n\ndf_test.shape","aa7c01ce":"df_test.drop(drop_cols, axis=1, inplace=True)","cede6260":"df_test[cat_cols] = df_test[cat_cols].astype('category')","df389e38":"predictions = np.array([model.predict(df_test, num_iter=model.best_iteration) for model in models]).mean(axis=0)","7ae6b5e6":"sns.distplot(y)","6b860a7a":"sns.distplot(predictions)","a75548f7":"submission = pd.DataFrame({\n    'Id': df_test.index,\n    'Energy_consumption': predictions\n})","75bf5515":"submission.to_csv(SUBMISSIONS_FOLDER + 'submission.csv', index = False)","b9faf530":"### Submission","b6ef79ff":"## Feature selection","0e3dea7b":"#### RepeatedKFold. \nStarted from simple KFold with 5 folds (also tried 4 and 10 folds), than switched to RepeatedKFold with 5 folds and 10 repeats. So, 5x10=50 models are used for prediction which is complete overkill:) But I wanted to be sure that performance of the models is stable and does not depend on random state. Also, I decided to use this approarch instead of blending because it's easier. StratifiedKfold with stratification along target quartiles did not improve public score in my case (tried with a few different random states); even though standard deviation of validation set is smaller with StratifiedKfold it might be because train and validation sets have similar distributions and not because models performance gets better. \n#### LightGBM. \nI didn't expect that such an exhaustive LightGBM (300000 iterations and learning rate 0.0005) will give me the best performance on such a small dataset. However, I ended up with this model.","dc78bed4":"## EDA","d7bb4e54":"We have already seen that feature_3, feature_4, feature_257, feature_258 are string type thus they are categorical for sure.\nI also assume that there are more features that are categorical but represented as numeric features. Of course, I cannot prove it and I'm not sure that my assumption is right, but I tried to treat features with low number of unique values as categorical ones and that slightly improved performance.","5dcd443e":"Looks like most features have just a few unique values thus they might be categorical even if they are represented as numeric variables.","bb55623a":"## Categorical features","56558290":"## Load data","b672f815":"Lets look at the features that are not numeric for sure.","262e3f73":"## Model training","07c42e6d":"Versions of some packages installed on my computer are different from what they have on kaggle, that's why output of this notebook has slightly different scores from what you've seen on the leaderboard. However, that's close version of notebook that gave me submission with private LB score 564855.334.","53396e91":"We have large amount of features, but doubt all of them should be included modeling dataset. \nSeveral steps were followed to decrease number of features:\n- try to get rid of multicollinearity (high intercorrelations  between independent variables). Even though LightGBM is known to be robusted to such a thing, highly intercorrelated features make no good as well - they simply do not provide model with new information. sklearn.feature_selection.VarianceThreshold is used to remove similar variables\n- exclude variables that have only one unique value","37d46769":"Luckily, there are no NAs :)"}}