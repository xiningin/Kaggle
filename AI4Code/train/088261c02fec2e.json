{"cell_type":{"b716197d":"code","0035a0a3":"code","5c270159":"code","428aef06":"code","aedda741":"code","f516dbdf":"code","4b09574f":"code","1b61d85a":"code","3d93a133":"code","aa8cdc2f":"code","57e1c05f":"code","b7fb731d":"code","d22bedca":"code","d929dba9":"markdown","ac97a1da":"markdown","13d80539":"markdown","5f904b93":"markdown","cef69303":"markdown","09e0da01":"markdown"},"source":{"b716197d":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBRegressor\n\n%matplotlib inline\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0035a0a3":"train_df = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\n\ntrain_df.head()","5c270159":"train_df.info()","428aef06":"# train and validation data set \n\ny = train_df['SalePrice']\n\ntrain_df2 = train_df.drop(['SalePrice'], axis=1)\nX = train_df2.select_dtypes(exclude=['object'])\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, train_size=0.8, test_size=0.2, random_state=0)\n\ncol_wiht_missing_value = [col for col in X_train.columns \n                          if X_train[col].isnull().any()]\n\nprint(col_wiht_missing_value)\n\nX_train = X_train.drop(col_wiht_missing_value, axis=1)\nX_valid = X_valid.drop(col_wiht_missing_value, axis=1)\n\n# random forest model training\n\nmodel = RandomForestRegressor(random_state=0)\n\nmodel.fit(X_train,y_train)\n\n# predicting validation values\n\npred_values = model.predict(X_valid)\n\n# Calculating MAE\n\nMAE = mean_absolute_error(y_valid,pred_values)\n\nprint(\"mean absolute error of model:\", MAE)","aedda741":"test_df = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv', index_col=0)\n\ntest_df.head()","f516dbdf":"X = test_df.select_dtypes(exclude=['object'])\n\nX_test = X.drop(col_wiht_missing_value, axis=1)\n\nX_test.dropna(inplace=True)\n\ntest_pred = model.predict(X_test)\n\ntest_pred[:5]\n\nmy_submission = pd.DataFrame({'id': X_test.index,\n                              'SalePrice': test_pred})\n\nmy_submission.to_csv('submission.csv', index=False)","4b09574f":"train_df = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\n\ny = train_df['SalePrice']\n\nX = train_df.drop(['SalePrice'], axis=1)\n\n\n# dividing train and validation data\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8,\n                                                      test_size=0.2, random_state=0)\n\nX_train = X_train.drop(col_wiht_missing_value, axis=1)\nX_valid = X_valid.drop(col_wiht_missing_value, axis=1)\n\n\n# removing noisy categorical columns\n\nobject_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n\n\ngood_labels = [col for col in object_cols if set(X_valid[col]).issubset(set(X_train[col]))]\n\nbad_labels = list(set(object_cols)-set(good_labels))\n\nprint(bad_labels)\n\nX_train_label = X_train.drop(bad_labels, axis=1)\nX_valid_label = X_valid.drop(bad_labels, axis=1)\n\ncategorical_missing = [col for col in good_labels if X_train[col].isnull().any()]\n\nprint(categorical_missing)\n\nX_train_label = X_train_label.drop(categorical_missing, axis=1)\nX_valid_label = X_valid_label.drop(categorical_missing, axis=1)\n\ncat_objects = list(set(good_labels)-set(categorical_missing))\n\n# encoding categorical variables \n\nmyencoder = OrdinalEncoder()\n\nX_train_label[cat_objects] = myencoder.fit_transform(X_train_label[cat_objects])\nX_valid_label[cat_objects] = myencoder.transform(X_valid_label[cat_objects])\n\n# fitting model to data\n\nmymodel = RandomForestRegressor(n_estimators=100, random_state=0)\n\nmymodel.fit(X_train_label,y_train)\n\npredicted_val = mymodel.predict(X_valid_label)\n\n# calculating MAE of the model\n\nMAE = mean_absolute_error(y_valid,predicted_val)\n\nprint(\"mean absolute error of mymodel: \", MAE)","1b61d85a":"# importing data sets\n\ny = train_df['SalePrice']\n\nX = train_df.drop(['SalePrice'], axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, train_size=0.8, test_size=0.2,\n                                                     random_state=0)\n# categorical columns\n\ncategorical_cols = [col for col in X_train.columns if X_train[col].dtype == 'object' and \n                    X_train[col].nunique() < 9]\n\n# numerical columns\n\nnumerical_cols = [col for col in X_train.columns if X_train[col].dtype in ['int64','float64']]\n    \n# total number of columns to be learn from \n\nmy_cols = categorical_cols + numerical_cols\n\ntrain_X =X_train[my_cols].copy()\nvalid_X = X_valid[my_cols].copy()\ntest_X = test_df[my_cols].copy()\n\nprint(\"Total number of columns to preprocess: \", len(my_cols))","3d93a133":"# numerical columns transoformation\nnumerical_transformer = SimpleImputer(strategy='mean')\n\n# categorical columns transformation\n\ncategorical_transformer = Pipeline(steps=[('impute', SimpleImputer(strategy='constant')),\n                                          ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n                             ])\n\n# preprocess both num and cat columns\n\npreprocessor = ColumnTransformer(transformers=[('num', numerical_transformer, numerical_cols),\n                                               ('cat', categorical_transformer,categorical_cols)\n])\n\n# define model\n\nmodel_ = RandomForestRegressor(n_estimators=155, random_state=0)\n\n# combine both preprocessor and model\n\nfinal_model = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model',model_ )\n    ])\n\n# fitting model to data\n\nfinal_model.fit(train_X, y_train)\n\n# predcting outcomes\n\npredicted_price = final_model.predict(valid_X)\n\n# calculating MAE\n\nMAE = mean_absolute_error(y_valid,predicted_price)\n\nprint(\"MAE: \", MAE)","aa8cdc2f":"test_pred = final_model.predict(test_X)\n\n\nmysub = pd.DataFrame({'Id': test_X.index,\n                      'SalePrice': test_pred})\n\nmysub.to_csv('submission.csv', index=False)","57e1c05f":"# using cross validation on dataset\n\ny = train_df['SalePrice']\n\ntrain_df2 = train_df.drop(['SalePrice'], axis=1)\n\nnumerical_cols = [col for col in train_df2.columns if train_df2[col].dtype in ['int64','float64']]\n\nX = train_df2[numerical_cols]\n\n# building model and imputer pipeline\n\nmypipeline = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n                             ('model', RandomForestRegressor(n_estimators=200\n                                                            , random_state=0))])\n\n# cross validation scoring \n\nscores = -1*cross_val_score(mypipeline, X,y, cv=5, scoring='neg_mean_absolute_error')\n\nprint(\"average of mean absolute error is: \", scores.mean())","b7fb731d":"y = train_df['SalePrice']\n\ntrain_df2 = train_df.drop(['SalePrice'], axis=1)\nX = train_df2.select_dtypes(exclude=['object'])\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, train_size=0.8, test_size=0.2, random_state=0)\n\ncol_wiht_missing_value = [col for col in X_train.columns \n                          if X_train[col].isnull().any()]\n\nprint(col_wiht_missing_value)\n\nX_train = X_train.drop(col_wiht_missing_value, axis=1)\nX_valid = X_valid.drop(col_wiht_missing_value, axis=1)\n\n\nmy_model = XGBRegressor(n_estimators=1500, learning_rate=0.05, random_state=0)\n\nmy_model.fit(X_train,y_train)\n\n# predicting values for validation data\n\npreds = my_model.predict(X_valid)\n\n# calculating MAE\n\nMAE = mean_absolute_error(y_valid,preds)\n\nprint(\"mean absolute error of the model: \", MAE)","d22bedca":"X = test_df.select_dtypes(exclude=['object'])\n\nX_test = X.drop(col_wiht_missing_value, axis=1)\n\npreds_ = my_model.predict(X_test)\n\npreds_[:5]\n\nmysub = pd.DataFrame({'ID': test_df.index,\n                      'SalePrice': preds_})\n\nmysub.to_csv('mysubmission.csv', index=False)","d929dba9":"# Using sklearn pipeline","ac97a1da":"# Thank you!","13d80539":" # Model training with dropping object columns and columns with missing values","5f904b93":"# Ordinal encoding of categorical variables","cef69303":"# Cross validation having cv fold = 5","09e0da01":"> Considering above two model ordinal encoding has lower MAE than onehot encoding"}}