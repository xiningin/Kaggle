{"cell_type":{"ef41b8ef":"code","a360c1f3":"code","fabe9d70":"code","fe9307e9":"code","9714a294":"code","ce82df71":"code","16f0ba20":"code","cd58a41d":"code","329759ca":"code","b8d08478":"code","a3414954":"code","1bf5b623":"code","8a9bb7aa":"code","6f7cf239":"code","19d0f80f":"code","6d9ee06e":"code","754c1608":"code","1b9d8397":"code","458f90d7":"code","c75a539c":"code","ea625dde":"code","3891eebd":"code","775ee6fd":"code","cea8fde3":"code","9d0d95e3":"code","67fea851":"code","fa48774f":"markdown","e558adb1":"markdown","48c96e15":"markdown","479581ae":"markdown","53e7e3b7":"markdown","63af8624":"markdown","441634e1":"markdown","efed2175":"markdown","0239bf0c":"markdown","c5917790":"markdown","29c43b8c":"markdown","5bd6e0c4":"markdown","967dbf89":"markdown","0da9e1dc":"markdown","06fa2baf":"markdown","533d3d0d":"markdown","604ed443":"markdown","0b284aea":"markdown","0ea36a1f":"markdown","0475fdbb":"markdown","0ecf2d25":"markdown","e5a8c9c2":"markdown","af3ca943":"markdown","efc595f1":"markdown","afafd6ec":"markdown","125b310d":"markdown","6137c82e":"markdown","370ac0ac":"markdown","3e3d2046":"markdown","e68c49f6":"markdown","8543649d":"markdown","929f6460":"markdown"},"source":{"ef41b8ef":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # plot data\nimport matplotlib.pyplot as plt # plot data\nimport os\n\nfrom sklearn.tree import DecisionTreeClassifier # Algorithm","a360c1f3":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntrain_data['dataset'] = 'train'\ntest_data['dataset'] = 'test'\n\nall_data = pd.concat([train_data, test_data])","fabe9d70":"all_data.head()","fe9307e9":"all_data.describe()","9714a294":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","ce82df71":"all_data['title'] = all_data['Name'].str.extract(r'([A-z]+)\\.', expand=True)\nall_data['title'].unique()","16f0ba20":"plt.figure(figsize=(16, 8))\n\nfor title in all_data['title'].unique():\n    f1 = all_data['title'] == title\n    sns.distplot(all_data[f1]['Age'], label=title, kde=False)\n\nplt.legend()","cd58a41d":"f = all_data['Age'].isnull() == True\nall_data[f]['title'].unique()","329759ca":"plt.figure(figsize=(16, 8))\n\nfor title in all_data[f]['title'].unique():\n    f1 = all_data['title'] == title\n    sns.distplot(all_data[f1]['Age'], label=title, kde=False)\n\nplt.legend()","b8d08478":"for title, age in all_data.groupby('title')['Age'].median().iteritems():\n    \n    if title in all_data[f]['title'].unique():\n        print(title, age)\n        \n    all_data.loc[(all_data['title']==title) & (all_data['Age'].isnull()), 'Age'] = age","a3414954":"all_data = all_data.drop(['Cabin', 'Embarked', 'Fare'], axis=1)","1bf5b623":"all_data.head()","8a9bb7aa":"all_data.describe()","6f7cf239":"all_data_simple = all_data.drop(['Pclass', 'Name', 'SibSp', 'Parch', 'Ticket', 'title'], axis=1)","19d0f80f":"all_data_simple.head()","6d9ee06e":"all_data_simple.describe()","754c1608":"train_filter = all_data_simple['dataset'] == 'train'\ntest_filter = all_data_simple['dataset'] == 'test'\n\ntrain_set = all_data_simple[train_filter].drop(['dataset'], axis=1).reset_index(drop=True)\ntest_set = all_data_simple[test_filter].drop(['dataset'], axis=1).reset_index(drop=True)","1b9d8397":"train_set = pd.get_dummies(train_set)\ntest_set = pd.get_dummies(test_set)\n\ntrain_set = train_set.set_index('PassengerId')\ntest_set = test_set.set_index('PassengerId')","458f90d7":"train_set.head()","c75a539c":"train_set.describe()","ea625dde":"test_set.head()","3891eebd":"test_set.describe()","775ee6fd":"features = ['Age', 'Sex_female', 'Sex_male']\ntarget = ['Survived']","cea8fde3":"titanic_model = DecisionTreeClassifier(random_state=0)\n\nX = train_set[features]\ny = train_set[target]\n\n# Fit the model\ntitanic_model.fit(X, y)","9d0d95e3":"X = test_set[features]\n\npredictions = titanic_model.predict(X)\n\nd = {'PassengerId': X.index, 'Survived': predictions.astype(int)}\nresult = pd.DataFrame(data=d)\n\nresult","67fea851":"result.to_csv('results.csv', index=False)","fa48774f":"### 2.1.1 Age column preparation","e558adb1":"In the end, let's make our prediction based on the trained model and print them in the results.csv files","48c96e15":"## 1.1 Approach","479581ae":"# Titanic: \"Women and children first!\"","53e7e3b7":"### 2.1.2 Drop of unused columns with missing values\nSince there are also other columns with missing values we may have to decide how to handle these other missing values. However, as said at the beginning, this is a special case in which the features are known a priori. For this reason we can simply drop the columns which show missing values since we know for sure that those columns will not be used as features.  ","63af8624":"As we can see we have to estimate a median age only for 6 titles. So that, we can not consider titles such as *Capt*, *Col* and so on.<br><br>\n_**Note that**, we could also have mapped titles in such a way that, for example, Capt. title belongs to Mr. title. This would have increased the amount of samples on which estimate the median for a certain title. However, most of the not considered titles are very specific ones which belong to a very limited number of people. Obviously, an even more accurate estimate and analysis could be made, however for the moment no mapping will be done since these new points would result in outliners with the risk of ruining the estimate._\n<br><br>\nLet's now reduce the distribution study to the titles we are interested in.","441634e1":"# 2. Data preparation","efed2175":"For each title, let's now take a look at how age is distributed.","0239bf0c":"## 3.1 Create and fit the model","c5917790":"Load train and test data into two different data objects and concatenate them","29c43b8c":"After submission of the results.csv file, the model obtained a score of 0.72248, which means that about 72% of the records in the test set has been correctly classified. Since the model is very simple, it's not a bad result in the end. <br> \nObviously, we cannot say that the model has a very good accuracy but it can be seen as a good starting point for further model expansions, maybe with the add of other features with a more accurate feature selection phase.","5bd6e0c4":"As we can see the chart above is pretty messy, basically because there are a lot titles. But, the question is: **all those titles are useful?** In other words, which are the titles for which we have to estimate the median age?<br>\nIn order to answer this question, let's analyse the people for which the age is not available and let's look at their titles.","967dbf89":"As a final step we can also clean our dataset once more and make it very simple. In other words, we could remove all columns except for *PassengerId*, *Age*, *Sex* and *Survived*.","0da9e1dc":"The approach used in this notebook aims to find out how much accurate could be a very simple model, in this case a *DecisionTree* with only two features: *Sex* and *Age*.<br>\n<br>\nAs we can see the feature selection process has been skipped and the features have been chosen a priori. However, the choice of the features is not random.<br>\nThe idea comes from the homonymous film 'Titanic', in which, in the phase of abandonment of the ship, he relied on first saving \"women and children\". \nThe first thought was: **\"Great, they are telling me exactly what the decision-making process is by which a person or not gets into a lifeboat.\"**\nFurthermore, considering that there are not known to be lifeboats for everyone, it is logical to assume that the surviving people are mostly women and children.<br>\n<br>\nSo the question is: **relying only on age and gender, what performances can we achieve in order to classify who survived and who didn't?**","06fa2baf":"And let's transform the *Sex* feature into a 0\/1 field","533d3d0d":"## 1.3 Disclaimer","604ed443":"# 1. Intro","0b284aea":"So, we have this dataset","0ea36a1f":"For the algorithm part the idea is to use a very simple *DecisionTreeRegressor*. <br>\nSo, let's define the features and target variables.","0475fdbb":"Now, we have this dataset","0ecf2d25":"# 4. Predictions & results","e5a8c9c2":"#### How fill the missing values for the *Age* column?\nUsing the median or mean of the *Age* of the entire dataset, probably, is not the most accurate way to do it.<br>\nOne idea could be derived the *Age* from the other columns. Obviously, there might be several combinations that could make sense and effective in estimating a person's age, but one of the simplest I think is to use the person's title, contained in the *Name* column, estimate the median for that title, and use it to fill in the missing data, again based on the person's title.","af3ca943":"# 5. Conclusions","efc595f1":"As we can see from the above table we have some columns in which we have some missing values.\nThe *Survived* column missing rate is not relevant since the considering dataset includes also the *test* set. Moreover, since the features have been chosen a priori, we also don't care about the missing values in *Cabin*, *Embarked* and *Fare* columns.<br>\nBasically, **we only have to take care about the *Age* column in which there is a missing rate of about 20%**.","afafd6ec":"_**Note that**, since the model and the features have been already chosen, the data preparation process will be a very simple one, since we have only to deal with Age and Sex columns._","125b310d":"## 2.2 Train and test set division\nIn the end, before the model part, let's divide the two datasets","6137c82e":"## 1.2 Implementation","370ac0ac":"This notebook was created primarily for the purpose of curiosity and entertainment and it doesn't show a typical procedure used from a data scientist that aims to reach the best possible performance on test set. This is clear since the feature selection and data exploration phases have been skipped. There are, of course, numerous notebooks containing much more accurate analyzes and much more complex and refined models, but in this case we don't want to scale the leaderboard, but just understand how much an extremely simple model, based on a prior knowledge that everyone has of this event, can perform. We can say that we know from the outset that the model will not reach 100% accuracy. This is obvious since the model is too simple and there is an high probability that it suffers from a large underfitting problem.<br>\n<br>\nFinally, I want to emphasize that this notebook can be a good starting point for all those people who are entering the world of data science, as it is a very simple and easy to understand model and that can be easily extended and made more complex.","3e3d2046":"The aim is to verify how much a very simple model with a priori defined features performs. <br>\nAlso, since \"women and children first\" decision making can be modeled with a simple decision tree, we will just use a DecisionTreeClassifier as a model. In order to keep the procedure as simple as possible, the classifier hyperparameters will be kept standard.<br>\nRegarding data preparation, this will be very simple, as we have to deal with the data quality of only two variables, namely *Sex* and *Age*.","e68c49f6":"# 3. Model","8543649d":"So, let's now compute the median for these 6 titles","929f6460":"## 2.1 Missing values\nFirst of all, let's check if the dataset contains missing values."}}