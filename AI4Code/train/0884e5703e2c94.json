{"cell_type":{"a4f58ffc":"code","d533fdf9":"code","2b929e36":"code","5dd169c4":"code","7fdcee5d":"code","4b83f218":"code","aee91225":"code","37fdb92f":"code","0a8cadeb":"code","7f781b99":"code","613041c1":"code","c2c31839":"code","23bc80e9":"code","6ca5e789":"code","de2b0174":"code","33a32461":"code","c62d8e87":"code","feca35ec":"code","c13a825e":"code","2122c672":"code","a2628950":"code","4c020b04":"code","f9c0a508":"code","286afef0":"code","51cf9a60":"code","e1174b73":"code","fd74b618":"code","2b3ba330":"code","466bea98":"code","4aacda93":"code","5be8e6d2":"code","01eb002e":"code","91bab372":"code","be3f3d4c":"code","767a80ab":"code","12c72c9b":"code","0fa88a0e":"code","5c5d42b8":"code","8b36a690":"code","e1ca8387":"code","a1343030":"code","421fff81":"code","a0a78340":"code","7dd755e4":"code","e2c1cefc":"code","6df57418":"code","932c63a8":"code","787b5a08":"code","6e79e270":"code","ae76424b":"code","61f18da0":"code","9d0500f9":"code","c011c12d":"code","b26fc420":"code","c8061263":"code","09419f03":"code","6b20ba17":"code","2db419a3":"code","c622e932":"code","f5921c31":"code","40f52ca2":"code","573d0a3f":"code","45ecc6e6":"code","850d6277":"code","1a438a77":"code","ea9bf66a":"code","f5c37d99":"code","29f4f555":"code","417d9bf2":"code","2eb597c7":"code","bd23f1e8":"code","ec43a81a":"code","2072058e":"code","fcb43b5a":"code","7cd621c0":"code","80893e69":"code","3666aa34":"code","40b3a55d":"code","31809209":"code","ccb16399":"code","2158e3ea":"code","5087a6cd":"code","97121f37":"code","b6f69479":"code","71f0dc50":"code","cda00f5a":"code","9a8ccbd0":"code","863e97e1":"code","e50b32b1":"code","e0f8c5a9":"code","6e7c9cfc":"code","e83be746":"code","0e7c92b8":"code","0160f6fd":"code","e309324e":"code","1082003b":"code","132d4426":"code","a4e44be2":"code","069796ee":"code","38bc8bf0":"code","b78a11f9":"code","a0beee5d":"code","4d58b428":"code","474c44d1":"code","2f7bffb1":"code","e1fd1622":"code","4f66706b":"code","a82f8cd9":"code","6860d08d":"code","73e215ba":"code","c098b9bd":"code","4433eef4":"code","a2abd3c8":"code","7fbe3e5c":"code","6c726d2c":"code","692479b7":"code","f58fded6":"code","77703763":"code","41fdcbad":"code","a5b15f26":"code","5a0c5c72":"code","2d95d5a4":"code","4064563e":"code","24bb2cf9":"code","82f1d765":"code","58a1553c":"code","b8e16dc4":"code","c85137b2":"code","f51fb326":"code","8175b0a2":"code","a9aa4f0a":"code","a379944a":"markdown","7d339faa":"markdown","9726842b":"markdown","fc3053bc":"markdown","bdcea5b1":"markdown","5ecd635b":"markdown","a200e7af":"markdown","655a31db":"markdown","6fa67ed3":"markdown","e8e92965":"markdown","e183c47e":"markdown","eb1872e7":"markdown","5589e8d4":"markdown","9a2c2458":"markdown","0d338d42":"markdown","c8609585":"markdown","9394a991":"markdown","d468309f":"markdown","720b7108":"markdown","59189443":"markdown","fa4107d0":"markdown","bba06531":"markdown","902f4532":"markdown","f1ebe69b":"markdown","aff75a6d":"markdown","ba1845da":"markdown","a14ea985":"markdown","99954f74":"markdown","c0866f3b":"markdown","1b907183":"markdown","da14225b":"markdown","e3062506":"markdown","e467ee0c":"markdown","2dd0b741":"markdown","3a9a6d70":"markdown","3e1b97ae":"markdown","63d73ae7":"markdown","a61814f4":"markdown","c6eaf097":"markdown","30ba27f1":"markdown","3f28efac":"markdown","22baa11e":"markdown","0a7b4cae":"markdown","d108ab5c":"markdown","319d7bba":"markdown","81749884":"markdown","b9688694":"markdown","d500f909":"markdown","ccea050f":"markdown","54c9d7ce":"markdown","3d3d5b85":"markdown","de502af1":"markdown","4b2657ea":"markdown","dea2ac14":"markdown","eb4b1510":"markdown","d93c65ab":"markdown","661675d8":"markdown"},"source":{"a4f58ffc":"# Import Packages\nimport numpy as np\nimport scipy as sp\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV, KFold, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, RidgeCV, Ridge, Lasso\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom mlxtend.regressor import StackingCVRegressor\nimport plotly.express as px\nfrom statlearning import plot_coefficients\nfrom statlearning import plot_feature_importance\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_columns', None)","d533fdf9":"#install.packages(\"xgboost\")","2b929e36":"# Read in csv files and plot top of training data\ntrain = pd.read_csv(\"..\/input\/airbnb\/train.csv\")\ntest = pd.read_csv(\"..\/input\/airbnb\/test.csv\")\ntrain.drop(\"NA\", 1, inplace=True)\ntest.drop(\"NA\", 1, inplace=True)\ntrain.head()","5dd169c4":"#Examine the missingness of the training data\ntrain.isna().sum()","7fdcee5d":"pd.crosstab(index=train[\"host_response_time\"], columns=\"count\")","4b83f218":"train[\"host_response_time\"].fillna('a few days or more', inplace=True)\ntest[\"host_response_time\"].fillna('a few days or more', inplace=True)","aee91225":"print(train[\"host_response_time\"])\nprint(test[\"host_response_time\"])","37fdb92f":"pd.crosstab(index=train[\"host_response_rate\"], columns=\"count\")","0a8cadeb":"train[\"host_response_rate\"].fillna(np.mean(train[\"host_response_rate\"]), inplace=True)\ntest[\"host_response_rate\"].fillna(np.mean(test[\"host_response_rate\"]), inplace=True)","7f781b99":"print(train[\"host_response_rate\"])\nprint(test[\"host_response_rate\"])","613041c1":"pd.crosstab(index=train[\"host_acceptance_rate\"], columns=\"count\")","c2c31839":"train[\"host_acceptance_rate\"].fillna(np.mean(train[\"host_acceptance_rate\"]), inplace=True)\ntest[\"host_acceptance_rate\"].fillna(np.mean(test[\"host_acceptance_rate\"]), inplace=True)","23bc80e9":"print(train[\"host_acceptance_rate\"])\nprint(test[\"host_acceptance_rate\"])","6ca5e789":"train[train['bedrooms'].isna()]","de2b0174":"test[test['bedrooms'].isna()]","33a32461":"for index in train[train['bedrooms'].isna()].index:\n    if train.iloc[index,15] >= 0:\n        train.iloc[index,14] = train.iloc[index,15]\n    else:\n        train.iloc[index,14] = 0\n\nfor index in test[test['bedrooms'].isna()].index:\n    if test.iloc[index,14] >= 0:\n        test.iloc[index,13] = test.iloc[index,14]\n    else:\n        test.iloc[index,13] = 0","c62d8e87":"print(train[\"bedrooms\"])\nprint(test[\"bedrooms\"])","feca35ec":"train[train['beds'].isna()]","c13a825e":"test[test['beds'].isna()]","2122c672":"for index in train[train['beds'].isna()].index:\n    if train.iloc[index,14] >= 0:\n        train.iloc[index,15] = max(1,train.iloc[index,14])\n    else:\n        train.iloc[index,15] = 1\n        \nfor index in test[test['beds'].isna()].index:\n    if test.iloc[index,13] >= 0:\n        test.iloc[index,14] = max(1,test.iloc[index,13])\n    else:\n        test.iloc[index,14] = 1","a2628950":"print(train[\"beds\"])\nprint(test[\"beds\"])","4c020b04":"pd.crosstab(index=train[\"security_deposit\"], columns=\"count\")","f9c0a508":"train[\"security_deposit\"].fillna(np.mean(train[\"security_deposit\"]), inplace=True)\ntest[\"security_deposit\"].fillna(np.mean(test[\"security_deposit\"]), inplace=True)","286afef0":"print(train[\"security_deposit\"])\nprint(test[\"security_deposit\"])","51cf9a60":"pd.crosstab(index=train[\"cleaning_fee\"], columns=\"count\")","e1174b73":"train[\"cleaning_fee\"].fillna(np.mean(train[\"cleaning_fee\"]), inplace=True)\ntest[\"cleaning_fee\"].fillna(np.mean(test[\"cleaning_fee\"]), inplace=True)","fd74b618":"print(train[\"cleaning_fee\"])\nprint(test[\"cleaning_fee\"])","2b3ba330":"# List the 7 review_scores columns\ncol  = ['review_scores_rating','review_scores_accuracy','review_scores_cleanliness',\n        'review_scores_checkin','review_scores_communication','review_scores_location','review_scores_value']\n\nfor name in col:\n    train[name].fillna(np.mean(train[name]), inplace=True)\n    test[name].fillna(np.mean(test[name]), inplace=True)\n    \n    print(train[name])\n    print(test[name])","466bea98":"pd.crosstab(index=train[\"reviews_per_month\"], columns=\"count\")","4aacda93":"train[\"reviews_per_month\"].fillna(np.mean(train[\"reviews_per_month\"]), inplace=True)\ntest[\"reviews_per_month\"].fillna(np.mean(test[\"reviews_per_month\"]), inplace=True)","5be8e6d2":"print(train[\"reviews_per_month\"])\nprint(test[\"reviews_per_month\"])","01eb002e":"# List all columns contain boolean information\ncol2 = ['host_is_superhost','host_identity_verified','instant_bookable','require_guest_profile_picture',\n        'require_guest_phone_verification']\n\nfor name in col2:\n    train[name] = train[name].map({'f':0, 't':1})\n    test[name] = test[name].map({'f':0, 't':1})\n    \n    print(train[name])\n    print(test[name])","91bab372":"train.head()","be3f3d4c":"test.head()","767a80ab":"train.isna().sum()  # check whether all NaN values have been processed with replacement for both sets","12c72c9b":"test.isna().sum()","0fa88a0e":"descriptive = train.describe()\ndescriptive.loc['skew', :] = train.skew()\ndescriptive.loc['kurt', :] = train.kurt()\ndescriptive.round(3)","5c5d42b8":"train.cov()['price'].sort_values().round(3)","8b36a690":"train.corr()[\"price\"].sort_values().round(3)","e1ca8387":"# plot the histogram using initial response variable (price) to check its distribution\nfrom statlearning import plot_dist\n\ny_train = train.iloc[:,1]\n\nplot_dist(y_train)\nplt.title('Distribution of Price')\nplt.show()\n","a1343030":"#predictors= ['host_response_rate', 'host_acceptance_rate','host_listings_count','property_type',\n             #'accommodates','bathrooms','bedrooms','beds','security_deposit','cleaning_fee','guests_included',\n             #'extra_people','minimum_nights','maximum_nights','number_of_reviews','review_scores_rating',\n             #'review_scores_accuracy','review_scores_cleanliness','review_scores_checkin',\n             #'review_scores_communication','review_scores_location','review_scores_value']\n#train_X = train[predictors]","421fff81":"#from statlearning import plot_dists\n#plot_dists(train[predictors[:21]])\n#plt.show()","a0a78340":"# Split some of the clean training & testing data into \n#indep_variables = test.columns\n#dep_variable = [\"price\"]\n\n# Exploratory Data Analysis: scatter plots\n#for i in range(len(indep_variables)):\n    #plt.figure(figsize=(10,5))\n    #plt.scatter(x=train[indep_variables[i]], y=train[dep_variable[0]])\n    #plt.xlabel(str([indep_variables[i]]))\n    #plt.ylabel(str([dep_variable[0]]))\n    #plt.title(\"Training: Price vs \"+str([indep_variables[i]]))\n    #plt.savefig(\"price vs \"+indep_variables[i])\n    #plt.show()","7dd755e4":"f,ax = plt.subplots(figsize=(20, 15))\nsns.heatmap(train.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","e2c1cefc":"# a rough scatter plot using longitude and latitude\ntrain.plot(kind='scatter', x='longitude', y='latitude', alpha=0.1)","6df57418":"# the Sydney Heatmap of Rental Prices\nfig = px.density_mapbox(train, lat='latitude', lon='longitude', z=train['price'],\n                        center=dict(lat=-33.918732, lon=151.242035), zoom=9,\n                        mapbox_style=\"stamen-terrain\",\n                        radius=20,\n                        opacity=0.5)\nfig.update_layout(title_text='Sydney Heatmap of the Rental Prices', title_x=0.5, title_font=dict(size=32))\nfig.show()","932c63a8":"#1 property_type\ncount_property_type = train['property_type'].value_counts()\nprint(count_property_type.head(100))  # show the top 100 property types  \n# use these top 3 types","787b5a08":"train['property_type'] = np.where(train['property_type'].str.contains('House'), \"House\", \n                                  (np.where(train['property_type'].str.contains('Apartment'), \"Apartment\", \n                                  (np.where(train['property_type'].str.contains('Townhouse'), \"Townhouse\", \"Other\")))))\ntest['property_type'] = np.where(test['property_type'].str.contains('House'), \"House\", \n                                 (np.where(test['property_type'].str.contains('Apartment'), \"Apartment\", \n                                 (np.where(test['property_type'].str.contains('Townhouse'), \"Townhouse\", \"Other\")))))","6e79e270":"#2 bed_type\ncount_bed_type = train['bed_type'].value_counts()\nprint(count_bed_type.head(3))  # show the top 3 bed types  # the \"Real Bed\" is extremely dominant","ae76424b":"train['bed_type'] = np.where(train['bed_type'].str.contains('Real'), \"Real Bed\", \"Other\")\ntest['bed_type'] = np.where(test['bed_type'].str.contains('Real'), \"Real Bed\", \"Other\")","61f18da0":"#3 host_response_time\npd.crosstab(index=train['host_response_time'], columns=\"count\")","9d0500f9":"train['host_response_time'] = np.where(train['host_response_time'].str.contains('within an hour'), 'Quick', \n                                       (np.where(train['host_response_time'].str.contains('a few days or more'), 'Slow', \n                                        'Medium')))\ntest['host_response_time'] = np.where(test['host_response_time'].str.contains('within an hour'), 'Quick', \n                                       (np.where(test['host_response_time'].str.contains('a few days or more'), 'Slow', \n                                        'Medium')))","c011c12d":"#4 room_type\npd.crosstab(index=train['room_type'], columns=\"count\")","b26fc420":"train['room_type'] = np.where(train['room_type'].str.contains('Entire home\/apt'), 'Entire', \n                                       (np.where(train['room_type'].str.contains('Private room'), 'Private', 'Other')))\ntest['room_type'] = np.where(test['room_type'].str.contains('Entire home\/apt'), 'Entire', \n                                       (np.where(test['room_type'].str.contains('Private room'), 'Private', 'Other')))","c8061263":"#5 cancellation_policy\npd.crosstab(index=train['cancellation_policy'], columns=\"count\")","09419f03":"train['cancellation_policy'] = np.where(train['cancellation_policy'].str.contains('flexible'), 'flexible', \n                                       (np.where(train['cancellation_policy'].str.contains('moderate'), 'moderate', \n                                        'strict')))\ntest['cancellation_policy'] = np.where(test['cancellation_policy'].str.contains('flexible'), 'flexible', \n                                       (np.where(test['cancellation_policy'].str.contains('moderate'), 'moderate', \n                                        'strict')))","6b20ba17":"test","2db419a3":"#1 box plot for property_type\nsns.catplot(x=\"property_type\", y=\"price\", kind=\"box\", data=train)","c622e932":"#2 box plot for bed_type\nsns.catplot(x=\"bed_type\", y=\"price\", kind=\"box\", data=train)","f5921c31":"#3 box plot for host_response_time\nsns.catplot(x=\"host_response_time\", y=\"price\", kind=\"box\", data=train)","40f52ca2":"#4 box plot for room_type\nsns.catplot(x=\"room_type\", y=\"price\", kind=\"box\", data=train)","573d0a3f":"#5 box plot for cancellation_policy\nsns.catplot(x=\"cancellation_policy\", y=\"price\", kind=\"box\", data=train)","45ecc6e6":"surcharges_train = [ row.security_deposit + row.cleaning_fee + row.extra_people for index, row in train.iterrows() ]\nsurcharges_test = [ row.security_deposit + row.cleaning_fee + row.extra_people for index, row in test.iterrows() ]\ntrain['surcharges'] = surcharges_train\ntrain['surcharges'] = train['surcharges'].round(2)\ntest['surcharges'] = surcharges_test\ntest['surcharges'] = test['surcharges'].round(2)","850d6277":"train = train.drop(['security_deposit','cleaning_fee', 'extra_people'], axis = 1)\ntest = test.drop(['security_deposit','cleaning_fee', 'extra_people'], axis = 1)","1a438a77":"#1 getting dummies for property_type\ntrain = pd.get_dummies(train, columns=['property_type'])\ntest = pd.get_dummies(test, columns=['property_type'])","ea9bf66a":"#2 getting dummies for bed_type\ntrain = pd.get_dummies(train, columns=['bed_type'])\ntest = pd.get_dummies(test, columns=['bed_type'])","f5c37d99":"#3 getting dummies for host_response_time\ntrain = pd.get_dummies(train, columns=['host_response_time'])\ntest = pd.get_dummies(test, columns=['host_response_time'])","29f4f555":"#4 getting dummies for room_type\ntrain = pd.get_dummies(train, columns=['room_type'])\ntest = pd.get_dummies(test, columns=['room_type'])","417d9bf2":"#5 getting dummies for cancellation_policy\ntrain = pd.get_dummies(train, columns=['cancellation_policy'])\ntest = pd.get_dummies(test, columns=['cancellation_policy'])","2eb597c7":"#6 getting dummies for host_is_superhost\ntrain = pd.get_dummies(train, columns=['host_is_superhost'])\ntest = pd.get_dummies(test, columns=['host_is_superhost'])","bd23f1e8":"#7 getting dummies for host_identity_verified\ntrain = pd.get_dummies(train, columns=['host_identity_verified'])\ntest = pd.get_dummies(test, columns=['host_identity_verified'])","ec43a81a":"#7 getting dummies for instant_bookable\ntrain = pd.get_dummies(train, columns=['instant_bookable'])\ntest = pd.get_dummies(test, columns=['instant_bookable'])","2072058e":"#7 getting dummies for require_guest_profile_picture\ntrain = pd.get_dummies(train, columns=['require_guest_profile_picture'])\ntest = pd.get_dummies(test, columns=['require_guest_profile_picture'])","fcb43b5a":"#7 getting dummies for require_guest_phone_verification\ntrain = pd.get_dummies(train, columns=['require_guest_phone_verification'])\ntest = pd.get_dummies(test, columns=['require_guest_phone_verification'])","7cd621c0":"# plot the histogram using initial response variable (price) to check its distribution\nfrom statlearning import plot_dist\n\ny_train = train.iloc[:,1]\n\nplot_dist(y_train)\nplt.title('Distribution of Price')\nplt.show()","80893e69":"# log-transformation of response variable (price) in training set\nlog_y_train = np.log(train.iloc[:,1])","3666aa34":"# plot the histogram using log response variable (log_price) to check its distribution\nfrom statlearning import plot_dist\n\nplot_dist(log_y_train)\nplt.title('Distribution of log_Price')\nplt.show()","40b3a55d":"#(1) Scatter plots\n## Create a new column to store the log_price\ntrain['log_price'] = np.log(train.iloc[:,1])","31809209":"X_train = train.iloc[:,2:]\nX_train = X_train.drop(['log_price'], axis = 1)","ccb16399":"# Plot each variable against the log response variable (log_price) respectively\nfrom statlearning import plot_regressions\n\nlog_y_train = train['log_price']\n\nwith sns.color_palette('Paired'):\n    plot_regressions(X_train.iloc[:,:22], log_y_train)\n    plt.show()","2158e3ea":"#(2) Histograms\nfrom statlearning import plot_dists\n\nplot_dists(X_train.iloc[:,0:])\nplt.show()\n","5087a6cd":"# create the variable 'geom' to store the combination of \ntrain['geom'] = list(zip(train['latitude'], train['longitude']))\ntest['geom'] = list(zip(test['latitude'], test['longitude']))","97121f37":"geom_list1 = np.array(train['geom'])\ngeom_list2 = np.array(test['geom'])","b6f69479":"import geopy.distance","71f0dc50":"# the coordinates(latitude, longitude) of famous places in Sydney\n# assumption: the closer a property to a famous view, the higher price a property has\n#1 Sydney Opera House->-33.858611, 151.214167\nview_1 = (-33.858611, 151.214167)\n#2 Sydney Harbour Bridge->-33.852222, 151.210556\nview_2 = (-33.852222, 151.210556)\n#3 Darling Harbour->-33.8723, 151.19896\nview_3 = (-33.8723, 151.19896)\n#4 Bondi Beach->-33.891, 151.278\nview_4 = (-33.891, 151.278)\n#5 Martin Place->-33.8677, 151.20892\nview_5 = (-33.8677, 151.20892)\n\nviews = [view_1,view_2,view_3,view_4,view_5]","cda00f5a":"views = pd.DataFrame(views)\nprint(views)\nprint(views.values)","9a8ccbd0":"import math\n\nb = [0]\np = [0]","863e97e1":"type(b)","e50b32b1":"def nearestDistance(boundary, p):\n    minDistList = map(lambda b: (b[0] - p[0])**2 + (b[1] - p[1])**2, boundary)\n    minDist2 = min(minDistList)\n    return math.sqrt(float(minDist2))","e0f8c5a9":"# create a list to store the nearest distance calculated between each property and the view\nnearest_dis_train = []\nnearest_dis_test = []\nfor p in geom_list1:\n    nearest_dis_train.append(nearestDistance(views.values, p))\nfor p in geom_list2:\n    nearest_dis_test.append(nearestDistance(views.values, p))","6e7c9cfc":"# add this 'nearest_dis' list into the train dataset as a column\ntrain['nearest_dis'] = nearest_dis_train\ntest['nearest_dis'] = nearest_dis_test","e83be746":"# plot to see the distribution of this nearest_dis variable & define the \"very close\" where nearest_dis<0.025\nplot_dist(nearest_dis_train)\nplt.show()","0e7c92b8":"# create a categorical variable \"near_or_not\"\n# the distance between a property and a view is recognised as near if nearest_dis<0.025\ntrain['near_or_not'] = pd.cut(train['nearest_dis'], bins=[0, 0.025, float('Inf')], labels=['near', 'far'])\ntest['near_or_not'] = pd.cut(test['nearest_dis'], bins=[0, 0.025, float('Inf')], labels=['near', 'far'])","0160f6fd":"# get dummies for near_or_not\ntrain = pd.get_dummies(train, columns=['near_or_not'])\ntest = pd.get_dummies(test, columns=['near_or_not'])","e309324e":"#(1) according to the plots about each explanatory variable and the response variable obtained above\n# drop the explanatory variables that has no significant correlation with the response variable (price)\nnumerical_data = train.drop(['host_response_rate','host_acceptance_rate', 'maximum_nights'], axis = 1)\ntest_data = test.drop(['host_response_rate','host_acceptance_rate', 'maximum_nights'], axis = 1)","1082003b":"#(2) irrelevant info\nnumerical_data = numerical_data.drop(['Id',  \n                                      'require_guest_phone_verification_0', 'require_guest_phone_verification_1',\n                                      'require_guest_profile_picture_0', 'require_guest_profile_picture_1'], axis = 1) \ntest_data = test_data.drop(['Id', \n                            'require_guest_phone_verification_0', 'require_guest_phone_verification_1',\n                            'require_guest_profile_picture_0', 'require_guest_profile_picture_1'], axis = 1)","132d4426":"#(3) multicollinearity issue\nnumerical_data = numerical_data.drop(['bedrooms', 'beds', 'review_scores_accuracy'], axis = 1) \ntest_data = test_data.drop(['bedrooms', 'beds', 'review_scores_accuracy'], axis = 1) ","a4e44be2":"# view the numerical data after the explanatory variable deduction\nnumerical_data.head()","069796ee":"test_data.head()","38bc8bf0":"# make dummies for categorical variables\n#numerical_data = pd.get_dummies(numerical_data, drop_first = True)\n#numerical_data","b78a11f9":"train = numerical_data.drop(['price', 'log_price', 'nearest_dis' ,'geom', 'latitude', 'longitude'], axis = 1)\ntest = test_data.drop(['nearest_dis' ,'geom', 'latitude', 'longitude'], axis = 1)\ntrain = train.astype(float)\ntest = test.astype(float)","a0beee5d":"# Scaling\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaler.fit(train)\n\ntrain_scaled = scaler.transform(train)","4d58b428":"# Split train to train and validation datasets to develop the best model\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, log_y_train, random_state=8)","474c44d1":"import statsmodels.api as sm\nfrom scipy import stats\n\nols_train = sm.add_constant(X_train)\nest = sm.OLS(y_train, ols_train)\nols = est.fit() \n\nprint(ols.summary())","2f7bffb1":"ols = LinearRegression().fit(X_train, y_train)\n\n# Create grid\nols_param_grid = {'fit_intercept': ['False', 'True'],\n                 'normalize': ['False', True]}    \n\n# K-fold on training\nols_grid_search = GridSearchCV(ols, ols_param_grid, cv=KFold(n_splits=10, random_state=8,shuffle=True), return_train_score=True)\nols_grid_search.fit(X_train, y_train)\nols_predicted = ols_grid_search.predict(X_test)\n\n# Evaluation\nprint('Best fit_intercept: ' + ols_grid_search.best_params_['fit_intercept'])\nprint('Best normalize: ' + ols_grid_search.best_params_['normalize'])\nprint('Training set score: {:.04f}'.format(ols_grid_search.best_score_))\nprint('Test set score: {:.04f}'.format(ols_grid_search.score(X_test, y_test)))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, ols_predicted))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, ols_predicted))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ols_predicted)))","e1fd1622":"sns.distplot(ols_predicted, hist=False, rug=True, label=\"Prediction\")\nsns.distplot(y_test, hist=False, rug=True, label=\"Actual\")\nplt.title('OLS Prediction VS Actual')\nplt.legend()","4f66706b":"ols_residuals = y_test - ols_predicted\nplt.scatter(ols_predicted, ols_residuals)\nplt.title('OLS Residuals VS Prediction')","a82f8cd9":"ols_qq = stats.probplot(ols_residuals, plot=plt)\nplt.title('OLS Probability Plot')\nplt.show","6860d08d":"ridge = Ridge(random_state=8).fit(X_train, y_train)\n\nprint('Training set score: {:.4f}'.format(ridge.score(X_train, y_train)))\nprint('Test set score: {:.4f}'.format(ridge.score(X_test, y_test)))","73e215ba":"# Create grid\nridge_param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}    \n\n# K-fold on training\nridge_grid_search = GridSearchCV(ridge, ridge_param_grid, cv=KFold(n_splits=10, random_state=8,shuffle=True), return_train_score=True)\nridge_grid_search.fit(X_train, y_train)\nridge_predicted = ridge_grid_search.predict(X_test)\n\n# Evaluation\nprint('Best alpha: ' + str(ridge_grid_search.best_params_['alpha']))\nprint('Training set score: {:.04f}'.format(ridge_grid_search.best_score_))\nprint('Test set score: {:.04f}'.format(ridge_grid_search.score(X_test, y_test)))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, ridge_predicted))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, ridge_predicted))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, ridge_predicted)))","c098b9bd":"sns.distplot(ridge_predicted, hist=False, rug=True, label=\"Prediction\")\nsns.distplot(y_test, hist=False, rug=True, label=\"Actual\")\nplt.title('Ridge Prediction VS Actual')\nplt.legend()","4433eef4":"ridge_residuals = y_test - ridge_predicted\nplt.scatter(ridge_predicted, ridge_residuals)\nplt.title('Ridge Residuals VS Prediction')","a2abd3c8":"ridge_qq = stats.probplot(ridge_residuals, plot=plt)\nplt.title('Ridge Probability Plot')\nplt.show","7fbe3e5c":"tree = DecisionTreeRegressor(random_state=8) \nscores = cross_val_score(tree, X_train, y_train, cv = KFold(n_splits=10, shuffle=True, random_state=8))\nprint('{:.04f}'.format(scores.mean()))","6c726d2c":"%%time\n# Create grid\ntree_param_grid = {'criterion': ['mse', 'mae'],\n             'min_samples_split': [2, 5, 10],\n             'max_depth': ['None', 2, 6, 8],\n             'min_samples_leaf': [1, 5, 10],\n             'max_leaf_nodes': ['None', 5, 20]}    \n\n# K-fold on training\ntree_grid_search = GridSearchCV(tree, tree_param_grid, cv=KFold(n_splits=10, random_state=8,shuffle=True), return_train_score=True)\ntree_grid_search.fit(X_train, y_train)\ntree_predicted = tree_grid_search.predict(X_test)\n\n# Evaluation\nprint(tree_grid_search.best_params_)\nprint('Training set score: {:.04f}'.format(tree_grid_search.best_score_))\nprint('Test set score: {:.04f}'.format(tree_grid_search.score(X_test, y_test)))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, tree_predicted))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, tree_predicted))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, tree_predicted)))","692479b7":"sns.distplot(tree_predicted, hist=False, rug=True, label=\"Prediction\")\nsns.distplot(y_test, hist=False, rug=True, label=\"Actual\")\nplt.title('Decision Tree Prediction VS Actual')\nplt.legend()","f58fded6":"tree_residuals = y_test - tree_predicted\nplt.scatter(tree_predicted, tree_residuals)\nplt.title('Decision Tree Residuals VS Prediction')","77703763":"tree_qq = stats.probplot(tree_residuals, plot=plt)\nplt.title('Decision Tree Probability Plot')\nplt.show","41fdcbad":"plot_feature_importance(tree_grid_search.best_estimator_, X_train.columns)\nplt.show()","a5b15f26":"gb = GradientBoostingRegressor(random_state=8) \nscores = cross_val_score(gb, X_train, y_train, cv = KFold(n_splits=10, shuffle=True, random_state=8))\nprint('{:.04f}'.format(scores.mean()))","5a0c5c72":"%%time\n# Create grid\ngb_param_grid = {'learning_rate': [0.1, 0.5, 1],\n             'criterion': ['mae', 'friedman_mse'],\n             'n_estimators': [50, 100],\n             'max_depth': ['None', 3, 5],\n            'subsample' : [0.6, 0.8, 1.0]   }    \n\n# K-fold on training\ngb_grid_search = GridSearchCV(gb, gb_param_grid, cv=KFold(n_splits=10, random_state=8,shuffle=True), return_train_score=True)\ngb_grid_search.fit(X_train, y_train)\ngb_predicted = gb_grid_search.predict(X_test)\n\n# Evaluation\nprint(gb_grid_search.best_params_)\nprint('Training set score: {:.04f}'.format(gb_grid_search.best_score_))\nprint('Test set score: {:.04f}'.format(gb_grid_search.score(X_test, y_test)))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, gb_predicted))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, gb_predicted))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, gb_predicted)))","2d95d5a4":"sns.distplot(gb_predicted, hist=False, rug=True, label=\"Prediction\")\nsns.distplot(y_test, hist=False, rug=True, label=\"Actual\")\nplt.title('Gradient Boosting Prediction VS Actual')\nplt.legend()","4064563e":"gb_residuals = y_test - gb_predicted\nplt.scatter(gb_predicted, gb_residuals)\nplt.title('Gradient Boosting Residuals VS Prediction')","24bb2cf9":"gb_qq = stats.probplot(gb_residuals, plot=plt)\nplt.title('Gradient Boosting Probability Plot')\nplt.show","82f1d765":"plot_feature_importance(gb_grid_search.best_estimator_, X_train.columns)\nplt.show()","58a1553c":"ols_scores = [ols_grid_search.best_score_, ols_grid_search.score(X_test, y_test), \n                 metrics.mean_absolute_error(y_test, ols_predicted), \n                 metrics.mean_squared_error(y_test, ols_predicted), \n                 np.sqrt(metrics.mean_squared_error(y_test, ols_predicted))]\n\nridge_scores = [ridge_grid_search.best_score_, ridge_grid_search.score(X_test, y_test), \n                 metrics.mean_absolute_error(y_test, ridge_predicted), \n                 metrics.mean_squared_error(y_test, ridge_predicted), \n                 np.sqrt(metrics.mean_squared_error(y_test, ridge_predicted))]\n\ntree_scores = [tree_grid_search.best_score_, tree_grid_search.score(X_test, y_test), \n                 metrics.mean_absolute_error(y_test, tree_predicted), \n                 metrics.mean_squared_error(y_test, tree_predicted), \n                 np.sqrt(metrics.mean_squared_error(y_test, tree_predicted))]\n\ngb_scores = [gb_grid_search.best_score_, gb_grid_search.score(X_test, y_test), \n                 metrics.mean_absolute_error(y_test, gb_predicted), \n                 metrics.mean_squared_error(y_test, gb_predicted), \n                 np.sqrt(metrics.mean_squared_error(y_test, gb_predicted))]\n\nprint('   Training set score,         Test set score,          MAE,                MSE,              RMSE')\nprint('OLS: ' + str(ols_scores))\nprint('Ridge: ' + str(ridge_scores))\nprint('DT: ' + str(tree_scores))\nprint('GB: ' + str(gb_scores))","b8e16dc4":"ols_best = ols_grid_search.best_estimator_\nridge_best = ridge_grid_search.best_estimator_\ntree_best = tree_grid_search.best_estimator_\ngb_best = gb_grid_search.best_estimator_\n\nmodels = [ols_best, ridge_best, tree_best, gb_best]\n\nstack = StackingCVRegressor(models, meta_regressor = Ridge(), cv=10)\nstack.fit(X_train, y_train)\nplot_coefficients(stack.meta_regr_, labels = ['OLS', 'Ridge', 'DT', 'GB'])\nplt.show()","c85137b2":"stack_predicted = stack.predict(X_test)\nprint(stack.get_params)\n# print('Training set score: {:.04f}'.format(stack.best_score_))\nprint('Test set score: {:.04f}'.format(stack.score(X_test, y_test)))\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, stack_predicted))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, stack_predicted))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, stack_predicted)))","f51fb326":"sns.distplot(stack_predicted, hist=False, rug=True, label=\"Prediction\")\nsns.distplot(y_test, hist=False, rug=True, label=\"Actual\")\nplt.title('Model Stacking Prediction VS Actual')\nplt.legend()","8175b0a2":"stack_residuals = y_test - stack_predicted\nplt.scatter(stack_predicted, stack_residuals)\nplt.title('Model Stacking Residuals VS Prediction')","a9aa4f0a":"test_predicted = stack.predict(test)\nfinal_prediction = np.exp(test_predicted).round(0)\nsubmission = pd.DataFrame(np.c_[test.index, final_prediction], columns=['Id', 'price'])\nsubmission['Id'] = submission['Id'].astype(int)\nsubmission.to_csv('Stack.csv',  index=False)","a379944a":"As we can see below, all scores generated by Gradient Boosting model are better than any previous model, which meets our initial expectation. However, the improvement is still not significant, and the last solution might be model stacking.","7d339faa":"### Several observations between different features:\n1. __The correlation between beds and bedrooms are high(= 0.8)__: we may eliminate one of these two features since they contribute the similar information.\n  \n  \n2. __The correlation between review_scores_rating and review_scores_accuracy are high(= 0.8)__: we may eliminate one of these two features since they contribute the similar information.\n  \n  \n3. __The correlations of 7 features which related to review_scores are genrally higher__: we may combine these 7 features into one feature by applying the average review scores and observe the overall performance.  \n  \n  \nThe heat map shows the correlations between each feature. The small value represent these two intercept features tend to be uncorrelated.","9726842b":"### 3. Merge the surcharges together--security_deposit, cleaning_fee and extra_people (in AU dollars)","fc3053bc":"### 5. Visualisation of geographical data","bdcea5b1":"### 8. Explanatory variable exclusion for further analysis and modelling\n(1) due to insignificant correlation with the response variable\n\n(2) due to irrelevant information for modelling purpose\n\n(3) due to multicollinearity with other explanatory variable which contributes similar information","5ecd635b":"## Import Libraries","a200e7af":"## 4. Gradient Boosting\nGradient Boosting is an essemble learning method, which is more complex than previous models and usually outperforms them. However, it is extremely slow to train with our dataset compared with the rest. Boosting is a method such that each classifier contains additional information than the last classifier. For Gradient Boosting, which usually uses tree as base classifier, each classifier predicts the residual of the previous one classifier, which is a widely used machine learning method. It is expected to have the best performance among all previous models. In addition, as our model does not have overfitting issue according to the previous analysis, XGBoosting is not chosen although it could be faster than Gradient Boosting.","655a31db":"## Modelling","6fa67ed3":"Note that the prediction curve generated by Gradient Boosting model has exactly the same problem as all previous models.","e8e92965":"We should deal with the NaN values in each column based on the reasonable methods.","e183c47e":"Model stacking can combine predictions of models to improve the performance. It combines multiple regression models via meta regressor. Note that Ridge is the best meta regressor after testing. This method provides more stable performance than a single regressor, and it is expected to have the best performance as our final model. Note that we aquire our best models from previous GridSearch-CV step, and also obtain a 10 times cross validation to generate the result. As we can see from the coefficients importance plot, Gradient Boosting is the best contributor with almost 70%, two linear models contribute to the other 30%, and decision tree has almost no contribution at all.","eb1872e7":"## Feature Engineering","5589e8d4":"#### Reason for doing this transformation\nTo improve performance by reducing skewness and making the noise variance near constant.","9a2c2458":"Unlike in Decition Tree model, two of our location variables are the 13th and 15th important features in Gradient Boosting model. However, from the plot we can see there is not a significant importance difference between featrues under the top 3 features. Therefore, location is still an important feature to consider when predicting price.","0d338d42":"### Note\nR-squared scores we conclude for all of the models are the results of cross validation. For example, below the summary  of a single OLS model shows its R-squared is 0.668, but we got 0.648 after cross validation in the next block, which is a lower but more generalize score. Moreover, we would like to use grid-search cv to get the best performance.","c8609585":"#### Standardisation\nUsing standard scaler in this case, fit and transform the training data.","9394a991":"### 1. Covariance and correlation","d468309f":"#### 2. For the column: host_response_rate  \n  \nSince we cannot get the responses from hosts, we cannot obtain these response rates. We decide to use average rate to fill the corresponding missing data, so the mean of overall response rate will not change.","720b7108":"All of the scores are very slightly better than Gradient Boosting, model stacking does not boost the performance as we expected.","59189443":"### 7. location-related variable creation","fa4107d0":"## Data preprocessing","bba06531":"### 3. Scatter plots with fitted lines (each feature against the response)","902f4532":"However, with its best alpha=10, the model R-squared scores are only 0.649 and 0.617 for training and testing(validation) datasets, which has almost no improvement compared with OLS model. The reason might be the size of the dataset is too small, and the model does not have overfitting problem. Besides, the as its similar to OLS model, the location variables should also have positive relationships with price.","f1ebe69b":"### 6. The visualisation and EDA after the transformation\n(1) The scatter plots with fitted lines (each feature against the response), excluding dummies\n\n(2) The histograms which show the distributions and some patterns(e.g. outliers, modes, etc)","aff75a6d":"### 2. Distribution of the response variable (housing price in )","ba1845da":"### 4. Turning into dummies","a14ea985":"#### 4. For the column: bedrooms  \n  \nIn general, one bedroom contains one bed. If the selected properties have the information of number of beds, we would replace the NaN number of bedrooms to number of beds in selected properties. Otherwise, we assume the properties may not have specific bedroom(= 0).","99954f74":"### 5. Response Transformation\nThe histogram plotted below shows that the distribution of response varible (price) is right-skewed, so the response transformation is needed to deal with this pattern of skewness.","c0866f3b":"### Scaling\nIt is always important to do scaling when applying machine learning algorithms. It can prevent high scale feature dominating the other features when using certain models, which can improve the performance of the model significantly.","1b907183":"### 2. Boxplot visualisation before making dummies","da14225b":"#### 8. For the 7 columns: different types of review_scores \nFor the 7 columns, review_scores_rating, review_scores_accuracy, review_scores_cleanliness, review_scores_checkin, review_scores_communication, review_scores_location, review_scores_value, we decide to use average scores to fill the NaN inforamtion respectively.","e3062506":"### 4. Correlation heat map","e467ee0c":"As we can see from the variable importance plot, 'near_or_not_near' is the 4th important variable in Decision Tree model, which means the location has a significant contribution to the price level.","2dd0b741":"# 6810 Assignment","3a9a6d70":"## 1. OLS\nOLS is a quite simple model so it is not expected to have a great performance. The OLS R-squared scores for training and testing(validation) datasets are and 0.648, 0.615, it is not well as we expected. In addtion, the location variables 'near_or_not_near' and 'near_or_not_far' both have positive relationships with the price as their coefficients are positive.","3e1b97ae":"#### The log-transformation process and plotting\nThe histogram plotted below using the log_price indicates an approximately normal distribution.","63d73ae7":"### 1. Deduction in number of categories for categorical variable\n(Directly using the tutorial given code)\n\nEliminating the categorical data into several main categories with most votes and encoding other types as one category.","a61814f4":"#### 1. For the column: host_response_time  \n\nBecause several host's response times are not recorded in the data, our assumption is that the host's reponses are inactived or the response speed is very slow. Therefore, we use \"a few days or more\" to fill in such missing data.","c6eaf097":"### Model Comparison\nAs we can see below, Gradient Boosting is the best model so far.","30ba27f1":"## 3. Decision Tree","3f28efac":"#### 10. For the columns contain boolean information  \nWe decide to follow the assignment tutorial method to replace the t to 1 and f to 0 for all boolean features.","22baa11e":"As we can see from the plot, the shape of the prediction is different from the actual shape of data, which means the complexity of the function might be wrong.","0a7b4cae":"## 5. Model Stacking","d108ab5c":"# Conclusion","319d7bba":"### Note: Be careful to run this section as it takes 40mins on my i5-8th Gen CPU","81749884":"#### 5. For the column: beds  \n  \nIn general, every property should provide at least one bed to tenants(>= 1). Suppose every bedroom should have one bed, we decide to fill the NaN bed information by using the number of bedrooms in the selected property. If the properties do not have bedroom, we fill the NaN bed information by 1.","b9688694":"#### 6. For the column: security_deposit\n\nWe decide to use average amount of security deposit to fill the NaN inforamtion.","d500f909":"### Missing values imputation","ccea050f":"## 2. Ridge\nRegularization is essential to prevent overfitting. Personally, Ridge regression is preferred rather than Lasso, since L1 regularization may ignore some features and cause information loss. In Ridge Regression, ||y - Xw||^2_2 + alpha * ||w||^2_2 is the objective function that needs to be minimized. Compared with Linear Regression, we have to tune the hyperparameter alpha in order to reach its best performance. Alpha will determine the effect of our regularization, and it is crucial to find the sweet point by testing different alpha as what we did below. Since it is an optimized version of Linear Regression, i is expected to have a better performance.","54c9d7ce":"#### 3. For the column: host_acceptance_rate  \n  \nSince we cannot get these host acceptance rates. We decide to use average rate to fill the corresponding missing data, so the mean of overall acceptance rate will not change.","3d3d5b85":"#### 9. For the column: reviews_per_month  \nWe decide to use average value of reviews_per_month to fill the NaN inforamtion.","de502af1":"Note that it also has the same issue as previous models.","4b2657ea":"#### 7. For the column: cleaning_fee  \n\nWe decide to use average amount of cleaning fee to fill the NaN inforamtion.","dea2ac14":"Decition Tree is also a simple and straight forward model. However, it has way more hyperparameters to tune, which has a high computational cost. Its R-squared scores are 0.626 and 0.617 for training and testing(validation) datasets, the score for testing is almost the same as our previous two linear models, but the score for training is lower, which indicates Decision Tree is more generalize.","eb4b1510":"### Several observations between independent variable and dependent variable:\n1. __host response rate vs price__: When the host response rate close to 1, the price given in the train data generally shows an upward trend.  \n  \n  \n2. __room type vs price__: The properties of Entire home\/apt room type could set higher prices over 300 AUD, and the properties of shared room type cannot set high prices under 100 AUD.  \n  \n  \n3. __bed type vs price__: By observation, if the properties do not have real bed, the price would not exceed 300 AUD. Only the properties with real bed could set the price over 300 AUD.  \n  \n  \n4. __minimum nights vs price__: When the minimum nights become larger, the price range would be shrinked and price become lower.  \n  \n  \n5. __review scores vs price__: When the review scores increased, the price range would be extended and the maximum price become larger.  \n  \n  \n6. __encoded property type vs price__: By observation, the prices of apartment and house are generally higher than the prices of townhouse and other property types.  \n  \n---------------------------------------------------------------------------------------------------------------------------------","d93c65ab":"## Data after pre-processing","661675d8":"## Exploratory data analysis "}}