{"cell_type":{"cd5408be":"code","eb8fdda7":"code","132607cf":"code","da77fc50":"code","4f6f5b16":"code","aa3de69f":"code","e70faf5c":"code","e2283d45":"code","bf98b23d":"code","c67e6657":"code","ffe84de3":"code","0e379967":"code","840da68d":"code","bd2a8582":"code","64ad341e":"code","b61163b0":"code","734839df":"code","adde2765":"code","4a9e0349":"code","483906ce":"code","0e33372b":"code","06128fe5":"code","0eb4d07b":"code","170f445d":"code","8d39f4e5":"code","b44fea55":"code","eecb90e0":"code","7836ca5e":"code","4041ff65":"code","1ac5bd8e":"markdown","700a3a63":"markdown","eebadc28":"markdown","c5318f1c":"markdown","05d5a734":"markdown","82b4556c":"markdown","62e1361d":"markdown","41747f6b":"markdown","e78d7dee":"markdown","38b48187":"markdown","ae95a334":"markdown","bb731faf":"markdown","0713c1bb":"markdown","066aca5f":"markdown","330b41c8":"markdown","69e93583":"markdown","863b2002":"markdown","4bebeabf":"markdown","863a53f4":"markdown","79de043e":"markdown","351ddd1b":"markdown"},"source":{"cd5408be":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.svm import OneClassSVM \nfrom sklearn.model_selection import train_test_split\nimport datetime\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom scipy.stats import norm\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eb8fdda7":"data = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","132607cf":"print(\"Data columns:{}\".format(data.columns))\nprint(\"Data shape :{}\".format(data.shape))","da77fc50":"describe = data[[\"Time\",\"Amount\",\"V1\",\"V10\",\"V27\"]].describe().T\ndescribe.style.background_gradient(cmap=\"inferno\")","4f6f5b16":"data.head()","aa3de69f":"plt.figure(figsize=(12,6))\nsns.countplot(x=\"Class\", data=data, palette=\"bwr\")\nplt.xticks(np.arange(2),(\"Fraud(-1)\", \"Not Fraud(1)\"))\nplt.xlabel(\"Fraud Detection\" , color=\"red\", alpha=0.7, size=22)\nplt.show()","e70faf5c":"print(data[\"Class\"].value_counts())","e2283d45":"plt.figure(figsize=(12,6))\nsns.distplot(data[\"Time\"], color=\"red\", bins=50, kde=True,  \n             hist_kws={\"linewidth\": 3,\"alpha\": 0.5,\"edgecolor\":\"pink\"},\n             kde_kws={\"color\": \"k\", \"lw\": 3})\n\nplt.xlabel(\"Transaction Time\", size=20)\nplt.show()","bf98b23d":"time = data.groupby(\"Class\")[\"Time\"].mean()\ntime\n#-1 fraud ","c67e6657":"plt.figure(figsize=(12,6))\nsns.distplot(data[\"Amount\"], color=\"blue\", kde=True,  \n             hist_kws={\"linewidth\": 3,\"alpha\": 0.5},\n             kde_kws={\"color\": \"k\", \"lw\": 3})\nplt.xlabel(\"Transaction Amount\", size=20)\nplt.show()","ffe84de3":"amount = data.groupby(\"Class\")[\"Amount\"].mean()\namount\n#-1 fraud ","0e379967":"f, ax = plt.subplots(figsize=(14, 14))\ndata_corr = data.corr()\ncmap = sns.diverging_palette(230, 0, as_cmap=True)\nsns.heatmap(data_corr,cmap=cmap, vmax=.8, center=0, square=True, linewidths=.5, fmt=\":.2f\")\nplt.show()","840da68d":"data[\"Class\"].value_counts()","bd2a8582":"f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(24, 10))\n\nsns.scatterplot(x=data[\"V26\"], y=data[\"V27\"], hue=data[\"Class\"], ax=ax1)\nax1.set_title('V26--V27 Scatter plot', fontsize=14, color=\"red\")\nax1.grid()\nax1.legend()\n\nsns.scatterplot(x=data[\"V27\"], y=data[\"V28\"], hue=data[\"Class\"], ax=ax2)\nax2.set_title('V27--V28 Scatter plot', fontsize=14, color=\"red\")\nax2.grid()\nax2.legend()\n\nsns.scatterplot(x=data[\"V26\"], y=data[\"V28\"], hue=data[\"Class\"], ax=ax3)\nax3.set_title('V26--V28 Scatter plot', fontsize=14, color=\"red\")\nax3.legend()\nax3.grid()\nplt.show()","64ad341e":"data.loc[data['Class'] == 1, \"Class\"] = -1\ndata.loc[data['Class'] == 0, \"Class\"] = 1\ndata = data.drop([\"V27\"], axis=1)","b61163b0":"#getting random set of nonfraud data to train on\nnon_fraud = data[data['Class']==1]\nfraud = pd.DataFrame(data[data['Class']==-1])\n\n#ONEClassSVM yap\u0131s\u0131 gere\u011fi sadece nonfraud olan \u00f6rnekler ile e\u011fitiyoruz.\ndf_train_nonfraud, df_test_nonfraud = train_test_split(non_fraud, test_size=0.3, random_state=40)\nprint(df_train_nonfraud.columns)","734839df":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer","adde2765":"standard_scaler = StandardScaler()\n\n\ndf_train_nonfraud_label = df_train_nonfraud[\"Class\"].values\ndf_train_nonfraud = df_train_nonfraud.drop([\"Class\"], axis=1)\ndf_train_nonfraud = standard_scaler.fit_transform(df_train_nonfraud)\ndf_train_nonfraud = pd.DataFrame(df_train_nonfraud)\n\ndf_test_nonfraud_labels = df_test_nonfraud[\"Class\"]\ndf_test_nonfraud= pd.DataFrame(df_test_nonfraud.drop([\"Class\"], axis=1))\ndf_test_nonfraud = standard_scaler.fit_transform(df_test_nonfraud)\ndf_test_nonfraud = pd.DataFrame(df_test_nonfraud)\n\nfraud_labels =fraud[\"Class\"]\nfraud=  pd.DataFrame(fraud.drop([\"Class\"], axis=1))\nfraud = standard_scaler.fit_transform(fraud)\nfraud = pd.DataFrame(fraud)","4a9e0349":"MinMaxScaler = MinMaxScaler()\ndf_train_nonfraud_minm = MinMaxScaler.fit_transform(df_train_nonfraud)\nfraud_minm = MinMaxScaler.fit_transform(fraud)\nfraud_minm = pd.DataFrame(fraud_minm)\n\nrobust = RobustScaler()\ndf_train_nonfraud_rob = robust.fit_transform(df_train_nonfraud)\nfraud_rob = robust.fit_transform(fraud)\nfraud_rob = pd.DataFrame(fraud_rob)\n\nnormalizer = Normalizer()\ndf_train_nonfraud_nor = normalizer.fit_transform(df_train_nonfraud)\nfraud_nor = normalizer.fit_transform(fraud)\nfraud_nor = pd.DataFrame(fraud_nor)","483906ce":"from sklearn.decomposition import PCA\n\n#StandartScaler\npca_reduc_nonfraud = PCA(n_components=2, random_state=0).fit_transform(df_train_nonfraud)\npca_reduc_fraud = PCA(n_components=2, random_state=0).fit_transform(fraud)\n#MinMAxScaler\npca_reduc_nonfraud_minm = PCA(n_components=2, random_state=0).fit_transform(df_train_nonfraud_minm)\npca_reduc_fraud_minm = PCA(n_components=2, random_state=0).fit_transform(fraud_minm)\n#RobustScaler\npca_reduc_nonfraud_rob = PCA(n_components=2, random_state=0).fit_transform(df_train_nonfraud_rob)\npca_reduc_fraud_rob = PCA(n_components=2, random_state=0).fit_transform(fraud_rob)\n#Normalizer\npca_reduc_nonfraud_nor = PCA(n_components=2, random_state=0).fit_transform(df_train_nonfraud_nor)\npca_reduc_fraud_nor = PCA(n_components=2, random_state=0).fit_transform(fraud_nor)\n\nfig, axs = plt.subplots(2, 2, figsize=(32,16))\nfig.suptitle(\"Dimension Reduction with PCA\", size=20)\n\naxs[0,0].scatter(pca_reduc_nonfraud[:,0], pca_reduc_nonfraud[:,1], color=\"#45CDE5\", label=(\"clear\"), linewidth=0.5, alpha=0.7, edgecolors=\"#E0F8E0\")\naxs[0,0].scatter(pca_reduc_fraud[:,0], pca_reduc_fraud[:,1], color=\"#FE2EF7\",  label=(\"Fraud\"), linewidth=1, alpha=0.8, edgecolors=\"black\")\naxs[0,0].set_title(\"Standart Scaler Distribution\", color=\"#FF0040\", size=25)\naxs[0,0].legend()\n\naxs[0,1].scatter(pca_reduc_nonfraud_minm[:,0], pca_reduc_nonfraud_minm[:,1], color=\"#45CDE5\", label=(\"clear\"), linewidth=0.5, alpha=0.7, edgecolors=\"#E0F8E0\")\naxs[0,1].scatter(pca_reduc_fraud_minm[:,0], pca_reduc_fraud_minm[:,1], color=\"#FE2EF7\",  label=(\"Fraud\"), linewidth=1, alpha=0.8, edgecolors=\"black\")\naxs[0,1].set_title(\"MinMax Scaler Distribution\", color=\"#FF0040\", size=25)\naxs[0,1].legend()\n\naxs[1,0].scatter(pca_reduc_nonfraud_rob[:,0], pca_reduc_nonfraud_rob[:,1], color=\"#45CDE5\",  label=(\"clear\"), linewidth=0.5, alpha=0.7, edgecolors=\"#E0F8E0\")\naxs[1,0].scatter(pca_reduc_fraud_rob[:,0], pca_reduc_fraud_rob[:,1], color=\"#FE2EF7\", label=(\"Fraud\"), linewidth=1, alpha=0.8, edgecolors=\"black\")\naxs[1,0].set_title(\"Robust Scaler Distribution\", color=\"#FF0040\", size=25)\naxs[1,0].legend()\n\naxs[1,1].scatter(pca_reduc_nonfraud_nor[:,0], pca_reduc_nonfraud_nor[:,1], color=\"#45CDE5\",  label=(\"clear\"), linewidth=0.5, alpha=0.7, edgecolors=\"#E0F8E0\")\naxs[1,1].scatter(pca_reduc_fraud_nor[:,0], pca_reduc_fraud_nor[:,1], color=\"#FE2EF7\",  label=(\"Fraud\"), linewidth=1, alpha=0.8, edgecolors=\"black\")\naxs[1,1].set_title(\"Normalization Distribution\", color=\"#FF0040\", size=25)\naxs[1,1].legend()","0e33372b":"start = datetime.datetime.now()\n\nmodel = OneClassSVM(kernel='poly', nu=0.0005,gamma=0.07, degree=3, coef0=0.02)\nmodel.fit(df_train_nonfraud)\n\nfinish = datetime.datetime.now()\nprint(\"Time taken:\" ,(finish-start))","06128fe5":"X_test = pd.concat([df_test_nonfraud,fraud])\n\ny_test= pd.concat([df_test_nonfraud_labels,fraud_labels])","0eb4d07b":"pred = model.predict(X_test)\nprint(classification_report(y_test, pred))","170f445d":"plt.figure(figsize=(12,6))\ncm = confusion_matrix(y_test, pred)\nsns.heatmap(cm, annot = True, fmt=\"g\", cmap=\"Greens\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","8d39f4e5":"df_train_nonfraud_label = pd.DataFrame(df_train_nonfraud_label)\n\ndf_train_nonfraud_label.iloc[:, 0].value_counts()\n# we use only non-fraud data","b44fea55":"from sklearn.model_selection import KFold\n# df_train_nonfraud_label is df_Train labels\n\ncv = KFold(n_splits=5,shuffle=True, random_state=0)\n# this process takes 11 minute.\n\nfor train_index, test_index in cv.split(df_train_nonfraud):\n    \n    X_train, X_val = df_train_nonfraud.iloc[train_index], df_train_nonfraud.iloc[test_index]\n    y_train, y_val = df_train_nonfraud_label.iloc[train_index], df_train_nonfraud_label.iloc[test_index]\n    train_set = pd.concat([X_train,y_train],axis=1)\n    \n \n    # ********model start *********\n    start = datetime.datetime.now()\n    model = OneClassSVM(kernel='poly', nu=0.0005,gamma=0.07, degree=3, coef0=0.02)\n    model.fit(X_train)\n    finish = datetime.datetime.now()\n    print(\"Time taken:\" ,(finish-start))\n     \n    #*********model predict ************\n    pred = model.predict(X_test)\n    print(classification_report(y_test, pred))\n    print(\"*\"*50)\n\n    ","eecb90e0":"\"\"\"lets make some modification on model parameters\"\"\"\n\nstart = datetime.datetime.now()\n\nmodel = OneClassSVM(kernel='poly', nu=0.0005,gamma=0.007, degree=5)\nmodel.fit(df_train_nonfraud)\n\nfinish = datetime.datetime.now()\nprint(\"Time taken:\" ,(finish-start))\n\n\nX_test = pd.concat([df_test_nonfraud,fraud])\ny_test= pd.concat([df_test_nonfraud_labels,fraud_labels])","7836ca5e":"pred = model.predict(X_test)\nprint(classification_report(y_test, pred))","4041ff65":"plt.figure(figsize=(12,6))\ncm = confusion_matrix(y_test, pred)\nsns.heatmap(cm, annot = True, fmt=\"g\", cmap=\"BuPu\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()","1ac5bd8e":"# I have not any logical reason can explain for removing V27. But with this step models precision value increase 2%  at the same time it detect more 1 fraud process. But \u0131 think that V28 or V27 distrupt distribution when \u0131 investigate scatter plots.","700a3a63":" # 1.For this situation (fraud detection) we care recall metric firstly.\n # 2. I can make more the recall value but there is a \"Trade of\" with precision value.\n# 3. Both should be as high as possible (precision, recall)","eebadc28":"# 1. what happens if we care only recall value ?\n# 2. does it make sense ? (I think no !)","c5318f1c":"# Now lets prepare test datasets. Pay attention to sequence which must be same on X_test and y_test.\n# Fraud data join the game on this part.","05d5a734":"# we arrive the critical point eventually. Our models learn only non_fraud data. When model face to any fraud data it says that \"Wow \u0131 dont know this topic so \u0131 will classify with different label which -1\"\n# that is the story about OneClassSVM how to work.","82b4556c":"# 1. Our goal is protect outliers and original distribution while make scaling. That's show why we use Standard Scaler. \n# 2. On the other hand \u0131 was try these techniques after than \u0131 see that most successful scale is StandardS. So I explain this topic with that way. \n# 3.Finally robust scaler look pretty but pay attention to the interval.\n\nfor more information about that\n\n1. [StandardScaler Documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html?highlight=standardscale#sklearn.preprocessing.StandardScaler)\n\n2. [Compare the effect of different scalers ](https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html)","62e1361d":"# Scaler Types and selection\n# which one should we choose ? why?\n\n<a id=\"scaler\"><\/a>","41747f6b":"# OneClassSVM\n<a id=\"oneclasssvm\"><\/a>","e78d7dee":"1. nu     : \"slackness\"\n2. gamma  : a distance metric which affected by a sample\n3. degree : poly degree (default 3 for more complex boundaries you can increase that.)\n4. kernel : Specifies the kernel type to be used in the algorithm.","38b48187":"![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_anomaly_comparison_001.png)\n\n\n\n# [references scikit-learn](https:\/\/scikit-learn.org\/stable\/auto_examples\/miscellaneous\/plot_anomaly_comparison.html)","ae95a334":"#### introduction:\n# \u0130n this kernel we will try to detection of fraud with OneClassSvm. We will make some visualizations , models and discussion.\n# I suppose that you know the awesome \"Credit Card Fraud Detection\" Dataset so \u0131 will not any description about that. \n# if you liked my work, please upvote this kernel since it will keep me motivated for my other studies.Thank you for your suggestions and advice.\n\n# content: \n1.  [Visualizations](#visualizations)\n2.  [About Scaler Types](#scaler)\n3.  [OneClassSVM(first model)](#oneclasssvm)\n       3.  1       [model one results](#results)\n       \n4.  [OneClassSVM(second model)](#model2)\n       4. 1   [Second model results](#results2)","bb731faf":"# I hope this kernel is educational and useful for you.","0713c1bb":"# model 2 results: you can see that \"trade of\" between precision and recall values.\n<a id=\"result2\"><\/a>","066aca5f":"# description:\nA One-Class Support Vector Machine is an unsupervised learning algorithm that is trained on the \"normal\" or \"anormal\" data. Model learns the boundaries of given points and than make a classification with that boundaries.\n\nIn that model we can use only \"normal\" data for training and than we will mix fraud and normal data for test dataset.\n\n1. [Scklearn Documentation](https:\/\/scikit-learn.org\/stable\/modules\/outlier_detection.html#outlier-detection)\n2. [a helpful article about OneClassSvm](https:\/\/towardsdatascience.com\/outlier-detection-with-one-class-svms-5403a1a1878c)","330b41c8":"# Scaling is our second station. This topic can be basic and easly but we must be careful. our model's life depends on cotton thread.\n# Thread is scaling for us. Lets we draw some graph and be clear.","69e93583":"# Second model\n<a id=\"model2\"><\/a>","863b2002":"# Visualizations\n<a id=\"visualizations\"><\/a>","4bebeabf":"# small clue...  we have to keep moving\n","863a53f4":"# Over Fitting or Not ? ","79de043e":"# Results of first model:\n<a id=\"results\"><\/a>","351ddd1b":"# cross validation show us a little bit over-fitting for our first model. But we can handle that amounts.\n# I think recall is cool but can we increase precision score ?"}}