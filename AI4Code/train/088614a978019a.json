{"cell_type":{"150c60e7":"code","0502d0e2":"code","86e7f324":"code","6176be4c":"code","1bcac616":"code","66340fa9":"code","6a25a83b":"code","aa24f280":"code","7d48c621":"code","9576c71b":"code","558190b3":"code","59e925da":"code","b35a209f":"code","25aad4b4":"code","e3b5d79b":"code","def23478":"code","9a4525f5":"code","e2f0e6c6":"code","9e528b12":"code","68f4c24f":"code","400ba308":"code","9904103c":"code","105c48fe":"code","74403acf":"code","68311365":"code","d4cf6a62":"code","14569b80":"code","bc3082ec":"code","b8650018":"markdown","77a2d981":"markdown","d863bf04":"markdown","04e15d9f":"markdown","bd8da40d":"markdown","4131edbc":"markdown","f9ebab6e":"markdown","c378768b":"markdown","f7ae235c":"markdown","dacc0c2c":"markdown","eac1b051":"markdown","6fabb5ce":"markdown","380f47f6":"markdown","b26777ba":"markdown","05cc1142":"markdown","84b4388a":"markdown","4c1d3f20":"markdown","9f69d4f9":"markdown","192e9012":"markdown","e80fd5a8":"markdown","6cc0bc8d":"markdown","19540d3f":"markdown"},"source":{"150c60e7":"import numpy as np # linear algebra\nimport matplotlib.pyplot as plt\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.stats.stats import pearsonr\nfrom scipy.optimize import minimize\nfrom scipy.optimize import LinearConstraint\nfrom sklearn.model_selection import train_test_split\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain=pd.read_csv(dirname+'\/train.csv')\ntest= pd.read_csv(dirname+'\/test.csv')\n\ntrain.head()","0502d0e2":"def evalCorr(col): # correlation of col vs SalePrice (for training data, obviously)\n    corr,_=pearsonr(train.loc[train[col].notnull(), 'SalePrice'],train.loc[train[col].notnull(), col] )\n    return corr\n\ndef printcorrOverview():\n    print('How different numerical columns correlate directly with SalePrice and how many NaNs they contain:')\n    print('Correl. nullTrain, nullTest, nuniqueTrain, nuniqueTest, Feature\\tHigh correlation?')\n    for col in train.columns:\n        if col in train.select_dtypes([np.number]).columns: # numeric columns\n            corr=evalCorr(col)\n        else: \n            corr=np.nan\n        nnullTrain=np.sum(train[col].isnull())\n        nuniqueTrain=len(train[col].unique())\n        if not col=='SalePrice':\n            nnullTest =np.sum(test[col].isnull())\n            nuniqueTest=len(test[col].unique())\n        else:\n            nnullTest=len(test)\n            nuniqueTest=0\n        if np.abs(corr)>0.6: mark='!!!'\n        elif np.abs(corr)>0.5: mark='!'\n        else: mark=''\n        print('%+.2f  ' %corr + '\\t%i\\t%i\\t%i\\t%i\\t' %(nnullTrain,nnullTest,nuniqueTrain,nuniqueTest)+ col +'\\t'+mark)\nprintcorrOverview()","86e7f324":"# Simplify garage info\n\n# print(train['GarageFinish'].unique())\n# train.GarageArea.hist(); plt.show()\n\n# Encode how finished the garage is:\nnumGarageFinish = {\"Fin\": 4, \"RFn\": 3, \"Unf\": 2, np.nan: 0}\ntrain['GarageFinish'].replace(numGarageFinish, inplace=True)\ntest[ 'GarageFinish'].replace(numGarageFinish, inplace=True)\n\n# Encode quality and condition of the garage:\nnumGarageQual = {\"Ex\": 4,\"Gd\": 3,\"TA\": 1,\"Fa\": 1,\"Po\": -1, np.nan: 0}\ntrain['GarageQual'].replace(numGarageQual, inplace=True)\ntest[ 'GarageQual'].replace(numGarageQual, inplace=True)\ntrain['GarageCond'].replace(numGarageQual, inplace=True)\ntest[ 'GarageCond'].replace(numGarageQual, inplace=True)\n\n# Encode PavedDrive:\nnumPavedDrive = {\"Y\": 2,\"P\": 1,\"N\": 0}\ntrain['PavedDrive'].replace(numPavedDrive, inplace=True)\ntest[ 'PavedDrive'].replace(numPavedDrive, inplace=True)\n\n\ndef calcGarageScore(df):\n    gs=0.\n    \n    if df.GarageYrBlt>=2000:\n        gs+=3\n    elif df.GarageYrBlt<=1050:\n        gs-=1\n        \n    gs+=df.GarageFinish\n    \n    if not np.isnan(df.GarageCars):\n        gs+=df.GarageCars*3\n        \n    if df.GarageArea>600:\n        gs+=6\n        \n    gs+=df.GarageQual*0.5\n    gs+=df.GarageCond*0.5\n    gs+=df.PavedDrive\n    \n    return gs\n\ntrain['garageScore'] = train.apply(calcGarageScore,axis=1)\ntest['garageScore']  = test.apply( calcGarageScore,axis=1)\nprint('garageScore correlation with SalePrice = %.3f' %evalCorr('garageScore') )\ntrain.drop(columns=['GarageYrBlt','GarageFinish','GarageQual','GarageCond','PavedDrive'],inplace=True)\n\n# fill NAN of features I want to use later again\ntest['GarageArea'].fillna(test['GarageArea'].mean(), inplace=True) # affects only one value\ntest['GarageCars'].fillna(test['GarageCars'].mode()[0], inplace=True) # affects only one value","6176be4c":"# Simplify basement info\n\n# replace the one NAN of basement area (feature correlates well with SalesPrice)\nfor df in [train,test]:\n    df.loc[:,'BsmtUnfSF'].fillna(df['BsmtUnfSF'].mode()[0], inplace=True)\n    df.loc[:,'TotalBsmtSF'].fillna(df['TotalBsmtSF'].mode()[0], inplace=True)\n\n#BsmtQual is actually the height and NAN means doesnt have a basement\nfor df in [train,test]:\n    df['BsmtQual'].replace({\"Ex\": 100, \"Gd\": 90, \"TA\": 80, \"Fa\": 70, \"Po\": 60, np.nan: 0}, inplace=True)\n\n# BsmtCond\nfor df in [train,test]:\n    df['BsmtCond'].replace({\"Ex\": 100, \"Gd\": 90, \"TA\": 80, \"Fa\": 70, \"Po\": 60, np.nan: 0}, inplace=True)\n\n# BsmtExposure\nfor df in [train,test]:\n    df['BsmtExposure'].replace({\"Gd\": 30, \"Av\": 20, \"Mn\": 10, \"No\": 0, np.nan: 0}, inplace=True)\n\n# BsmtFinType1 how finished basement is\nfor df in [train,test]:\n    df['BsmtFinType1'].replace({\"GLQ\": 70, \"ALQ\": 60, \"BLQ\": 40, \"Rec\": 30, \"LwQ\": 10, \"Unf\": 0, np.nan: 0}, inplace=True)\n\nfor col in test.columns:\n    if \"Bsmt\" in col:\n        nnullTrain=np.sum(train[col].isnull())\n        nnullTest =np.sum(test[col].isnull())\n        print(col,nnullTrain,nnullTest)\n        \n\ndef calcBsmtScore(df):\n    b=0.\n    \n    b+=df.BsmtQual\/10.\n    b+=df.BsmtCond\/100. # slightly reduces correlation with SalePrice\n    b+=df.BsmtExposure\/10.\n    b+=df.BsmtFinType1\/10. # slightly reduces correlation with SalePrice\n    if not np.isnan(df.BsmtFinSF1):\n        b+=df.BsmtFinSF1\/1000.\n\n    b+=df.BsmtUnfSF\/1000.\n    b+=df.TotalBsmtSF\/100. # this is the important feature\n    \n#     if not np.isnan(df.BsmtFullBath):\n#         b+=df.BsmtFullBath*3 # reduces correlation with SalePrice\n        \n    return b\n\n\ntrain['bsmtScore'] = train.apply(calcBsmtScore,axis=1)\ntest['bsmtScore']  = test.apply( calcBsmtScore,axis=1)\nprint('bsmtScore correlation with SalePrice = %.3f' %evalCorr('bsmtScore') )\ntrain.drop(columns=['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinSF1','BsmtFinType2','BsmtFinSF2','BsmtUnfSF','BsmtFullBath','BsmtHalfBath'],inplace=True)\n","1bcac616":"print(train['GarageType'].unique())\nprint(train['HouseStyle'].unique())\n\nw=np.ones(8) # initial weights\nbounds=[[0,10],[0,10],[0,10],[0,10],[0,10],[0,10],[0,10],[0,10]]\ndef calcFancyness(df):\n    f=0.\n    f+=df.FullBath*w[0]\n    f+=df.BedroomAbvGr*w[1]\n    if '2' in df.HouseStyle:\n        f+=w[2]\n    f+=df.Fireplaces*w[3]\n    if df.WoodDeckSF>0.:\n        f+=w[4]\n    if df.PoolArea>0.:\n        f+=w[5]\n    if df.GarageType in ['Attchd', 'Detchd', 'BuiltIn', 'Basment', 'CarPort','2Types']: # i.e. has garage\n        f+=w[6]\n    else:\n        f-=w[7]\n    return f\n\ndef target(x):\n    global w\n    w=x\n    train['fancyness'] = train.apply(calcFancyness,axis=1)\n    corr=evalCorr('fancyness')\n    return 1-corr # minimize this\n\nif 0: # run optimization (takes a while)\n    res = minimize(target, w, bounds=bounds)\n    print(res)\nelse: # hardcode previously optimized values\n    w=np.array([2.25768391, 0.        , 0.1627718 , 1.39476413, 0.75533618,\n       1.2838466 , 0.72309263, 0.72310406])\n\ntrain['fancyness'] = train.apply(calcFancyness,axis=1)\ntest['fancyness']  = test.apply(calcFancyness,axis=1)\nprint('Fancyness correlation with SalePrice = %.3f' %evalCorr('fancyness') )\ntrain['fancyness'].describe()\ntrain.drop(columns=['GarageType'],inplace=True)","66340fa9":"# test.MSSubClass.hist() # many different classes\n\ntrain.drop(columns='Id',inplace=True) # dont wanna use Id for modelling and only need it in test\n\n# Encode street access:\nfor df in [train,test]:\n    df['Street'].replace({\"Grvl\": 0, \"Pave\": 1}, inplace=True)\n    \n# Encode Alley into either has access to an alley or not\nfor df in [train,test]:\n    df['Alley'].replace({\"Grvl\": 1, \"Pave\": 1, np.nan: 0}, inplace=True)\n    \n# Encode LotShape (no NANs): regular or not\n# train.LotShape.hist()\n# plt.plot(train.SalePrice,train.LotShape,'k.') # Reg and Irreg span all prices\nfor df in [train,test]:\n    df['LotShape'].replace({\"Reg\": 3, \"IR1\": 2, \"IR2\": 1, \"IR3\": 0}, inplace=True)\n\n# LandContour is mostly level, make it binary\nfor df in [train,test]:\n    df['LandContour'].replace({\"Lvl\": 1, \"Bnk\": 0, \"HLS\": 0, \"Low\": 0}, inplace=True)\n\n# test.Utilities.hist() # literally all are AllPub. Useless feature\nif 'Utilities' in train.columns: # in case we're re-running the code\n    train.drop(columns=['Utilities'],inplace=True)\n    test.drop( columns=['Utilities'],inplace=True)\n    \n# LotConfig inside or not\nfor df in [train,test]:\n    df['LotConfig'].replace({\"Inside\": 1, \"Corner\": 0, \"FR2\": 0, \"FR3\": 0, \"CulDSac\": 0}, inplace=True)\n\n# LandSlope\nfor df in [train,test]:\n    df['LandSlope'].replace({\"Gtl\": 0, \"Mod\": 1, \"Sev\": 1}, inplace=True)\n    \n# train.Electrical.hist() almost all are SBrkr\nfor df in [train,test]:\n    df['Electrical'].replace({\"SBrkr\": 1, \"FuseA\": 0, \"FuseF\": 0, \"FuseP\": 0, \"Mix\": 0, np.nan: 0}, inplace=True)\n    \n# KitchenQual\nfor df in [train,test]:\n    df['KitchenQual'].replace({\"Ex\": 100, \"Gd\": 90, \"TA\": 80, \"Fa\": 70, \"Po\": 60, np.nan: 0}, inplace=True)\n    \n# Functional\nfor df in [train,test]:\n    df['Functional'].replace({\"Typ\": 100, \"Min1\": 90, \"Min2\": 90, \"Mod\": 70, \"Maj1\": 50, \"Maj2\": 50, \"Sev\": 20, \"Sal\": 0, np.nan: 100}, inplace=True)\n    \n# Fireplaces:\nfor df in [train,test]:\n    df['FireplaceQu'].replace({\"Ex\": 100, \"Gd\": 90, \"TA\": 80, \"Fa\": 70, \"Po\": 60, np.nan: 0}, inplace=True)\n\n# PoolQC:\nfor df in [train,test]:\n    df['PoolQC'].replace({\"Ex\": 100, \"Gd\": 90, \"TA\": 80, \"Fa\": 70, \"Po\": 60, np.nan: 0}, inplace=True)\n    \n# Fence\nfor df in [train,test]:\n    df['Fence'].replace({\"GdPrv\": 100, \"MnPrv\": 80, \"GdWo\": 80, \"MnWw\": 60, np.nan: 0}, inplace=True)\n    \n# Misc\nfor df in [train,test]:\n    df['MiscFeature'].replace({\"Elev\": 1, \"Gar2\": 1, \"Othr\": 1, \"Shed\": 1, \"TenC\": 1, np.nan: 0}, inplace=True)\n    \n# Condition2 almost all are Norm\nfor df in [train,test]:\n    df.loc[:,'Condition2']=[1 if tmp=='Norm' else 0 for tmp in df.Condition2]\n    \n# ExterQual and ExterCond\nfor df in [train,test]:\n    df['ExterQual'].replace({\"Ex\": 100, \"Gd\": 90, \"TA\": 80, \"Fa\": 70, \"Po\": 60, np.nan: 0}, inplace=True)\n    df['ExterCond'].replace({\"Ex\": 100, \"Gd\": 90, \"TA\": 80, \"Fa\": 70, \"Po\": 60, np.nan: 0}, inplace=True)\n    \n# HeatingQC\nfor df in [train,test]:\n    df['HeatingQC'].replace({\"Ex\": 100, \"Gd\": 90, \"TA\": 80, \"Fa\": 70, \"Po\": 60, np.nan: 0}, inplace=True)\n    \n# CentralAir\nfor df in [train,test]:\n    df['CentralAir'].replace({\"Y\": 1, \"N\": 0}, inplace=True)","6a25a83b":"#test.MSZoning.hist() # most are RL. Assume so are the 4 nulls\ntest.loc[:,'MSZoning'].fillna('RL', inplace=True)\n\n# Lot frontage is not given in quite a few cases, but lot area is\n# The distribution of the aspect ratio is a bit skewed\n# Use the median aspact ratio to set the frontage\nfor df in [train,test]:\n    tmp=df.LotArea\/df.LotFrontage\n    AR=np.median(tmp[np.logical_not(np.isnan(tmp))])\n    df.loc[:,'LotFrontage'].fillna(df.loc[:,'LotArea']\/AR, inplace=True)\n\n# Exterior1st (1 NAN in test)\ntest.loc[:,'Exterior1st'].fillna('Other', inplace=True)\n\n# Exterior2nd (1 NAN in test)\ntest.loc[:,'Exterior2nd'].fillna('Other', inplace=True)\n\n# MasVnrType (8 and 16 NANs). \n# The same ones (one one more in test) dont have the Area:\nfor df in [train,test]:\n    df.loc[:,'MasVnrType'].fillna('None',inplace=True)\n    df.loc[:,'MasVnrArea'].fillna(0.,inplace=True)\n\n# SaleType has 1 NAN in test only.\n# i=np.where(test.SaleType.isnull())[0]\n# print(test.loc[i,:].YearBuilt) # 1958. So it's not New, assume its WD then\ntest.loc[:,'SaleType'].fillna('WD', inplace=True)\n    \nfor col in train.columns:\n    if not col==\"SalePrice\":\n        nnullTrain=np.sum(train[col].isnull())\n        nnullTest =np.sum(test[col].isnull())\n        if (nnullTrain+nnullTest)>0:\n            print(col,'\\t',nnullTrain,nnullTest,'\\t' ,test[col].dtype )","aa24f280":"for df in [train,test]:\n    df.loc[:,'TotalSF'] = df.loc[:,'1stFlrSF']+ df.loc[:,'2ndFlrSF']+ df.loc[:,'TotalBsmtSF']+ df.loc[:,'OpenPorchSF']+ df.loc[:,'WoodDeckSF']\n\nprintcorrOverview()","7d48c621":"print(train.shape, test.shape)\ndef oneHot(train,test):\n    ntrain=len(train)\n    for col in train.select_dtypes(['object']).columns: # non-numeric columns of train set\n        feature = pd.concat((train[col], test[col]))\n        dummy = pd.get_dummies(feature) # dummies based on that one feature in both data sets\n        nall,nnewcol=dummy.shape\n        print('Replacing '+col+'\\t with %i binary (numerical) features' %nnewcol)\n        \n        train=pd.merge(left=train,right=dummy[:ntrain], left_index=True, right_index=True,suffixes=(\"\", col)) # add derived features to train\n        test =pd.merge(left=test, right=dummy[ntrain:], left_index=True, right_index=True,suffixes=(\"\", col)) # add derived features to test\n        \n        train.drop(columns=col,inplace=True) # drop original feature in train\n        test.drop( columns=col,inplace=True) # drop original feature in test\n    for col in train.columns:\n        if len(train[col].unique())<2:\n            print('Dropping '+col)\n            train.drop(columns=col,inplace=True) # drop useless feature in train\n            test.drop( columns=col,inplace=True) # drop useless feature in test\n            \n    return train,test\n    \noriginalNumFeatures=train.drop(columns='SalePrice').select_dtypes([np.number]).columns # save for later\ntrain,test=oneHot(train,test)\nprint(train.shape, test.shape)","9576c71b":"printcorrOverview() # of all 181 features after one hot encoding\n# the new one-hot encoded features mostly dont correlate with SalePrice and could probably be dropped with little loss of information. Ah well.","558190b3":"plt.plot(train.TotalSF,train.SalePrice,'r.')\nplt.xlabel('Total SF'); plt.ylabel('Sale Price')\ntrain.drop(index=np.where(train.TotalSF>6500)[0],inplace=True)\nplt.plot(train.TotalSF,train.SalePrice,'k.'); plt.show()","59e925da":"train.SalePrice.hist(); plt.show()","b35a209f":"def boxcoxTransform(x):\n    lmbda=0.15\n    return ((1.+x)**lmbda - 1.) \/ lmbda\ndef boxcoxInvTransform(x):\n    lmbda=0.15\n    #x[x<0.]=0.\n    return (lmbda*(1.\/lmbda + x))**(1.\/lmbda) -1.\n\nprint('Mean, std and skew before transform: %.2f\\t%.2f\\t%.2f' %(train.SalePrice.mean(), train.SalePrice.std(), train.SalePrice.skew()))\ntrain['SalePrice'] = train.SalePrice.apply(boxcoxTransform)\ntrain.SalePrice.hist(); plt.show()\nprint('Mean, std and skew after transform: %.2f\\t%.2f\\t%.2f' %(train.SalePrice.mean(), train.SalePrice.std(), train.SalePrice.skew()))","25aad4b4":"from sklearn.model_selection import KFold\n\ndef testModelOnTrainData(model,modelIsNN=False):\n    x=train.drop(columns='SalePrice')\n    y=train['SalePrice']\n    \n    plt.figure()\n    crossval=True\n    if not crossval:\n        xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.2, random_state=42)\n        if modelIsNN:\n            model.fit(xtrain,ytrain, batch_size=5, epochs=30)\n            model.summary() # print\n            predictions=boxcoxInvTransform(np.squeeze(model.predict_on_batch(xtest)))\n        else:\n            model.fit(xtrain,ytrain)\n            predictions=boxcoxInvTransform(model.predict(xtest))\n        error=np.std(predictions-boxcoxInvTransform(ytest))\n        plt.plot(boxcoxInvTransform(ytest),predictions,'k.')\n    else:\n        kf=KFold(n_splits=5, random_state=40, shuffle=True)\n        predictions=np.zeros(len(y))\n        error      =np.zeros(len(y))\n        for train_index, test_index in kf.split(x):\n            xtrain, xtest = x.iloc[train_index], x.iloc[test_index]\n            ytrain, ytest = y.iloc[train_index], y.iloc[test_index]\n            if modelIsNN:\n                model.fit(xtrain,ytrain, batch_size=5, epochs=30)\n                predictions[test_index]=boxcoxInvTransform(np.squeeze(model.predict_on_batch(xtest)))\n            else:\n                model.fit(xtrain,ytrain) # this will generally start over, forgetting everything from previous fits\n                predictions[test_index]=boxcoxInvTransform(model.predict(xtest))\n            error[test_index]      =predictions[test_index]-boxcoxInvTransform(ytest)\n        error=np.std(error)\n        plt.plot(boxcoxInvTransform(y),predictions,'k.')\n\n    plt.xlabel('True value'); plt.ylabel('Prediction')\n    plt.xlim([0,8e5]); plt.ylim([0,8e5])\n    plt.plot([0,8e5],[0,8e5],'k--')\n    plt.title('RMS error = %e' %error)\n    plt.show()\n    return predictions\n    \n\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\ntrainPredictions_linReg = testModelOnTrainData(model) # save for later","e3b5d79b":"def modelTestData(model,modelstr):\n    xtrain=train.drop(columns='SalePrice')\n    ytrain=train['SalePrice']\n    xtest=test.loc[:,xtrain.columns]\n\n    model.fit(xtrain,ytrain)\n    predictions=boxcoxInvTransform(model.predict(xtest))\n    output = pd.DataFrame({'Id': test.Id, 'SalePrice': predictions})\n    output.to_csv('prediction_'+modelstr+'.csv', index=False)\n    return\n\nmodel = LinearRegression()\nmodelTestData(model,'linReg')","def23478":"from sklearn.linear_model import ElasticNet\nmodel=ElasticNet()\ntrainPredictions_elNet = testModelOnTrainData(model) # save for later","9a4525f5":"model = ElasticNet()\nmodelTestData(model,'elNet')\n# Print the features that the elastic net found to be most important:\ncoeffs = pd.DataFrame(model.coef_,train.columns[1:],columns=['Coefficient'])\ncoeffs.sort_values(by='Coefficient',ascending=False,inplace=True)\ncoeffs.head(25)","e2f0e6c6":"# de-skew data\nfor col in originalNumFeatures: # numeric columns other than SalePrice that are not from one-hot encoding\n    sk=train[col].skew()\n    kt=train[col].kurtosis()\n    print('%.2f\\t%.2f\\t' %(sk, kt) +col)\n    if np.abs(sk)>0.8:\n        # apply the same transform as for the price to skewed features\n        # separately for train and test data\n        for df in [train,test]:\n            df[col] = df[col].apply(boxcoxTransform)\n        sk=train[col].skew()\n        kt=train[col].kurtosis()\n        print('%.2f\\t%.2f\\t' %(sk, kt) +col)\n        \n# normalize all input data\nfor col in train.drop(columns=['SalePrice']).columns:\n    if len(train[col].unique())<0.5*len(train[col]): # not continuous\n        norm= 1.*np.max(np.abs(train[col]))\n        if norm==0: norm=1.\n        test.loc[:,col] \/= norm\n        train.loc[:,col]\/= norm\n    else:\n        test.loc[:,col] = (test[col]  -test[col].mean()) \/ test[col].std()\n        train.loc[:,col]= (train[col]-train[col].mean()) \/ train[col].std()","9e528b12":"import tensorflow as tf\ntf.random.set_seed(42)\nnp.random.seed(42)\nfrom tensorflow.keras import layers\n\n\ndef buildNN(nunits=None):\n    \n    #ntrain,nfeatures=xtrain.shape\n    if nunits==None:\n        #nunits=nfeatures\/4\n        nunits=76 # old result\n        #nunits=93\n\n    model = tf.keras.Sequential()\n\n    model.add(tf.keras.layers.Dense(\n            units=nunits,\n#             input_shape=(nfeatures,),\n#             activation='relu',\n            activation='sigmoid', # important!\n#             activation='tanh', \n            use_bias=True, \n            kernel_initializer='glorot_uniform',\n            #kernel_initializer='random_uniform',\n            bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n            activity_regularizer=None, kernel_constraint=None, bias_constraint=None,\n            ))\n\n#     model.add(tf.keras.layers.Dense(\n#             units=nfeatures,\n#     #         input_shape=(nfeatures,),\n# #             activation='relu',\n#             activation='sigmoid', \n# #             activation='tanh', \n#             use_bias=True, \n#             kernel_initializer='glorot_uniform',\n#             #kernel_initializer='random_uniform',\n#             bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n#             activity_regularizer=None, kernel_constraint=None, bias_constraint=None,\n#             ))\n\n    model.add(tf.keras.layers.Dense(\n            units=1,\n    #         activation='sigmoid',\n            activation='relu',\n            use_bias=True, kernel_initializer='glorot_uniform',\n            bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None,\n            activity_regularizer=None, kernel_constraint=None, bias_constraint=None,\n            ))\n\n\n\n    model.compile(\n                    loss=tf.keras.losses.MeanSquaredError(),\n                    #loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n                  optimizer='sgd', # stochastic gradient descent\n                  #optimizer='RMSprop',\n#                   metrics=['accuracy']\n                  metrics=['MeanSquaredError'] # this is just for the print I think\n                    )\n    return model\n\n\nif 0: # find ideal number of nodes in the NN layer (takes long)\n    # test model on part of training data:\n    x=train.drop(columns='SalePrice')\n    y=train['SalePrice']\n    xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.2, random_state=42)\n    n=180\n    error=np.zeros(n)\n    for i in range(n):\n        model = buildNN(nunits=i)\n        model.fit(xtrain,ytrain, batch_size=5, epochs=10)\n        predictions=boxcoxInvTransform(np.squeeze(model.predict_on_batch(xtest)))\n        error[i]=np.std(predictions-boxcoxInvTransform(ytest))\n    nunitsmin=np.argmin(error)\n    plt.figure()\n    plt.plot(range(n),error,'k-')\n    plt.xlabel('Number of nodes in NN layer'); plt.ylabel('error')\n    plt.title('Min at nunits=%.2f' %nunitsmin)\n    plt.show()\n    # for a lot of nunits all throughout the range, predictions are 0. and the error very high\n    # for about half of the nunits in the range, the error is low.\n    # the lowest error came up for nunits=93\n    \nmodel = buildNN()  \ntrainPredictions_NN = testModelOnTrainData(model,modelIsNN=True) # save for later","68f4c24f":"xtrain=train.drop(columns='SalePrice')\nytrain=train['SalePrice']\nxtest=test.loc[:,xtrain.columns]\n\n\nmodel = buildNN()\nmodel.fit(xtrain,ytrain, batch_size=5, epochs=30)\nmodel.summary() # print\npredictions=boxcoxInvTransform(np.squeeze(model.predict_on_batch(xtest)))\noutput = pd.DataFrame({'Id': test.Id, 'SalePrice': predictions})\noutput.to_csv('prediction_NN.csv', index=False)\n\nyy=pd.DataFrame({\"Predicted SalePrice\": predictions})\nyy.describe()","400ba308":"from sklearn.ensemble import GradientBoostingRegressor\nmodel=GradientBoostingRegressor(loss='ls',n_estimators=200,learning_rate=0.1)\ntrainPredictions_grBoo = testModelOnTrainData(model) # save for later","9904103c":"model=GradientBoostingRegressor(loss='ls',n_estimators=200,learning_rate=0.1)\nmodelTestData(model,'grBoo')","105c48fe":"from sklearn.kernel_ridge import KernelRidge\nmodel=KernelRidge(alpha=0.7)\ntrainPredictions_krr = testModelOnTrainData(model) # save for later","74403acf":"model=KernelRidge(alpha=0.7)\nmodelTestData(model,'krr')","68311365":"n=101\nf=np.linspace(0,1.,n)\nerror=np.zeros(n)\nfor i in range(n):\n    predictions= f[i]*trainPredictions_linReg + (1.-f[i])*trainPredictions_NN\n    error[i]=np.std(predictions-boxcoxInvTransform(train.SalePrice))\nfmin=f[np.argmin(error)]\n\nplt.figure\nplt.plot(f,error,'k-')\nplt.xlabel('Weight of linReg model'); plt.ylabel('error')\nplt.title('Min at f=%.2f' %fmin)\nplt.show()","d4cf6a62":"# optimize the blend of all models\n\nallTrainPredictions=np.vstack([\n#                 trainPredictions_linReg, # when included, weight becomes about 0. Better to exclude here manually to set the weight to exactly 0.\n                trainPredictions_elNet,\n                trainPredictions_NN,\n                trainPredictions_grBoo,\n#                 trainPredictions_krr,\n                              ])\nnmodels,ny=allTrainPredictions.shape\n\ndef target(x):\n    predictions=np.matmul(x,allTrainPredictions)\n    error=np.std(predictions-boxcoxInvTransform(train.SalePrice))\n    return error # to minimize\ndef con(x):\n    return np.sum(np.abs(x))-1. # constraint is that this ==0., i.e. that weights be positive and add to 1.\n\nconstraints = {'type':'eq', 'fun': con}\nresult=minimize(target,np.ones(nmodels)\/(1.*nmodels),constraints=constraints)\nprint('Train data suggests to blend models with these weights:', result.x)\nprint('The factors add up to %e' %np.sum(result.x))\nprint('The avg error then comes to %e' %result.fun)","14569b80":"# load just-saved results again and blend them\ntestpredictions_linReg=pd.read_csv('prediction_linReg.csv')\ntestpredictions_elNet =pd.read_csv('prediction_elNet.csv')\ntestpredictions_NN    =pd.read_csv('prediction_NN.csv')\ntestpredictions_grBoo =pd.read_csv('prediction_grBoo.csv')\ntestpredictions_krr   =pd.read_csv('prediction_krr.csv')\n\npredictions= (result.x[0]*testpredictions_elNet.SalePrice + \n              result.x[1]*testpredictions_NN.SalePrice +\n              result.x[2]*testpredictions_grBoo.SalePrice)\noutput = pd.DataFrame({'Id': test.Id, 'SalePrice': predictions})\noutput.to_csv('prediction_blend.csv', index=False)","bc3082ec":"def addPrediction(df,col,pred):\n    df.loc[:,col] = pred\n    df[col].apply(boxcoxTransform)\n    df.loc[:,col]= (df[col]-df[col].mean()) \/ df[col].std()\n    return \n\naddPrediction(train,'predLinReg',trainPredictions_linReg) # on its own, changes NN's RMS error to 1.94e4\naddPrediction(train,'predelNet',trainPredictions_elNet) # on its own, changes NN's RMS error to 1.892e4\naddPrediction(train,'predgrBoo',trainPredictions_grBoo) # on its own, changes NN's RMS error to 2.0e4\naddPrediction(train,'predkrr',trainPredictions_krr) # on its own, changes NN's RMS error to 1.987e4\n# adding all four columns: RMS error = 1.98e4, i.e. worse than without those predictions\n# note that the random seeding for the NN doesnt work and weights are still random so those \n# comparisons might not be fair. Also I had to change nunits when adding columns.\n\nmodel = buildNN(nunits=128)  \ntrainPredictions_NNetal = testModelOnTrainData(model,modelIsNN=True)","b8650018":"# Elastic Net\nLinear regressions with regularization","77a2d981":"# Kernel Ridge Regression","d863bf04":"Train on all training data and make a prediction for the truly unknown data","04e15d9f":"Get rid of remaining NANs","bd8da40d":"# Future work\n- Try other models. If they're somewhat successfull, throw them into the blend and find optimal weights based on split training data set. Ridge regression, lasso regression, support vector regression\n- Bear in mind when selecting models: some of the features are dependent (most likely correlated). I.e. fancyness or bsmtScore are computed from other features, some of which are still in the dataset. The linear regression might suffer from the correlation between features, giving undue value to some. A ridge regression might be more appropriate. Lasso regression might set the weight of the weaker of two similar features to zero.","4131edbc":"# Outliers\nDropping outliers significantly improves the predictions below","f9ebab6e":"Pre-model how fancy the house is, by optimizing weights of contributions to fancyness","c378768b":"Retrain on all training data and make a prediction for the unknown test data","f7ae235c":"# A first linear regression","dacc0c2c":"# Predict real estate prices\nFirst, load data","eac1b051":"Blending predictions of the linReg model with those from the NN helps most if using about only 0.05 linReg. ","6fabb5ce":"Create new feature \"total house area\"","380f47f6":"Results\n- The blend of the linear regression and the NN got a new best score of 0.13633 (top 40% of entrants).\n- The blend of linear regression, elastic net and NN got a new score of 0.13703. The blend based on the training data subset does not seem to be representative of the actual testing data.\n- After introducing cross-validation to find the optimal blend based on all training data and not just 20 per cent, the new score is 0.13594. The blended model generalizes better than before.\n- After de-skewing data (Box Cox, as suggested by [Serigne](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard#Modelling)) prior to the NN, the NN is weighted much more and the new score is is 0.13014. Interestingly, the de-skewing adversly affected the elastic net and it had very little effect on the linear regression. Hence I'm de-skewing only for the NN, where it significantly improves predictions.\n- Blending the predictions from the elastic net, neural net and gradient boosting model returns a new best score of 0.12846 (top 26% of entrants).","b26777ba":"# Combine models","05cc1142":"Some categorical data remain for one hot encoding because the categories cannot be ordered in a meaningful way","84b4388a":"# Stack a bunch of linear regressions by building a NN","4c1d3f20":"# Scaling\n[Serigne](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard#Modelling) and others suggest to Box-Cox transform the SalesPrice because it is otherwise very skewed, making it difficult to approximate the quantity with linear models","9f69d4f9":"# Gradient Boosting","192e9012":"# Use results from some models in NN\nLet's see what happens if I train the NN using also the predictions from other models. The NN performed best on its own (RMS error 1.884e4), can that be further improved by learning from other models?\n\n-> Adding those predictions as columns increases the error. Is there not a better way of combining models other than the linear combination above?","e80fd5a8":"The prediction from the simple linear regression got into the top 48% with a score of 0.14146, thanks to NAN cleaning, encoding and, importantly, transforming the target variable. On average, the predictions of house prices are only about $22,800 off.","6cc0bc8d":"Encode categorical data that has some structure","19540d3f":"# Feature engineering"}}