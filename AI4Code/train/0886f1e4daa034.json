{"cell_type":{"9ad58269":"code","97c9d0c0":"code","aa62401a":"code","7985f5f0":"code","817857c5":"code","8fc3b53f":"code","93bcc16f":"code","8c617683":"code","2e6a45ee":"code","36d42501":"code","adc7121e":"code","e9ef90e6":"code","66b395aa":"code","1af8e1f7":"code","1297a786":"code","89eda4c8":"code","5725813a":"code","c090a725":"code","8b0114d4":"code","c33ad2d9":"code","70bab365":"code","2975623d":"code","954cf229":"code","11b8a3d8":"code","21b4bd9d":"code","59e80a5e":"markdown","ed33c55a":"markdown","1541ce54":"markdown","967732ed":"markdown","15e67429":"markdown","d1311f3f":"markdown","a8b8c215":"markdown","73a89694":"markdown","a536a4d6":"markdown","5e3e62ff":"markdown","a5cb0ab9":"markdown","ec91cbfd":"markdown","3a7fc5b3":"markdown","1c7b52d9":"markdown","b215732b":"markdown","1268325c":"markdown","67f99fba":"markdown","e0e1416b":"markdown","c3ca63f0":"markdown","04066f96":"markdown","fd6e61e7":"markdown","a8136e7a":"markdown","c85181fb":"markdown","39fbb455":"markdown","a268308d":"markdown","94ca6eca":"markdown","8422eebb":"markdown","53b3b7e2":"markdown","d0246ae1":"markdown","ff28c14f":"markdown","1a745236":"markdown"},"source":{"9ad58269":"#The AutoML we are using here is pycaret, this is the step to install pycaret.\n\n!pip install pycaret\n","97c9d0c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # Data visualisation \nimport matplotlib.pyplot as plt # Data visualisation \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","aa62401a":"# Getting the dataset to \"dataset\" variable\n\ndataset = pd.read_csv(\"..\/input\/iris\/Iris.csv\") # the iris dataset is now a Pandas DataFrame","7985f5f0":"# Showing first 5 rows.\n\ndataset.head()","817857c5":"sns.FacetGrid(dataset,hue='Species',size=5).map(plt.scatter,'PetalLengthCm','PetalWidthCm').add_legend()","8fc3b53f":"sns.FacetGrid(dataset,hue='Species',size=5).map(plt.scatter,'SepalWidthCm','PetalWidthCm').add_legend()","93bcc16f":"sns.FacetGrid(dataset,hue='Species',size=5).map(plt.scatter,'SepalWidthCm','PetalLengthCm').add_legend()","8c617683":"sns.FacetGrid(dataset,hue='Species',size=5).map(plt.scatter,'SepalLengthCm','PetalWidthCm').add_legend()","2e6a45ee":"sns.FacetGrid(dataset,hue='Species',size=5).map(plt.scatter,'SepalLengthCm','PetalLengthCm').add_legend()","36d42501":"sns.FacetGrid(dataset,hue='Species',size=5).map(plt.scatter,'SepalLengthCm','SepalWidthCm').add_legend()","adc7121e":"dataset.info()","e9ef90e6":"dataset['Species'].value_counts().plot.pie(explode=[0.1,0.1,0.1],autopct='%1.1f%%',shadow=True,figsize=(10,8))\nplt.show()","66b395aa":"data = dataset.sample(frac=0.95, random_state=786).reset_index(drop=True)\ndata_unseen = dataset.drop(data.index).reset_index(drop=True)","1af8e1f7":"print('Data for Modeling: ' + str(data.shape))\nprint('Unseen Data For Predictions ' + str(data_unseen.shape))","1297a786":"data['Species'].value_counts()","89eda4c8":"# Imporing pycaret classification method\n\nfrom pycaret.classification import *","5725813a":"# This is the first step of model selection\n# Here the data is our dataset, target is the labeled column(dependent variable), section is is random number for future identification.\nexp = setup(data = data, target = 'Species', session_id=77 )\n\n# After this we will get a list of our columns and its type, just conferm they are the same. Then hit enter.","c090a725":"#This comand is used to compare different models with our dataset.\n#The acuuracy,F1 etc of each model is listed in a table.\n#Choose which model you want\ncompare_models()","8b0114d4":"# With this command we are creating a Naives Byes model\n# The code for Naives Byes is \" nb \"\n# fold is the number of fold you want\n\nnb_model = create_model('nb', fold = 10)","c33ad2d9":"\nnb_tuned = tune_model('nb')","70bab365":"plot_model(nb_tuned, plot = 'auc')","2975623d":"plot_model(nb_tuned, plot = 'confusion_matrix')","954cf229":"predict_model(nb_tuned);","11b8a3d8":"new_prediction = predict_model(nb_tuned, data=data_unseen)","21b4bd9d":"new_prediction","59e80a5e":"In the above table we can see 6 models is showing accuracy of 100%(i.e. 1). For this notebook we use Naive Bayes.\n\nCodes for different models are given below. ","ed33c55a":"![Iris-Classification-Data-Set.png](attachment:Iris-Classification-Data-Set.png)","1541ce54":"Its a balanced dataset","967732ed":"# **Iris Species Classification Using Auto ML (pycaret)**","15e67429":"# 7. Importing pycaret classification method\n\n\nThe Auto Ml we are using here is pycaret. It\u2019s working is very easy as we want to give the datasets as data and our target column as target. We can also set some other features as shown below.","d1311f3f":"# 15. Summary\n\nAs we see above, we got a high accuracy model with 100% accuracy, with no over fitting. Auto Ml is preferred more because it\u2019s less time consuming and gives a very good result. The hyper parameter tuning is not that easy for the less experienced people, but it makes a huge difference in the performance of the model.\n\nAs NO one is perfect, if anyone find any errors or suggestion please feel free to comment below.\n\nMail ID : jerryjohn1995@gmail.com\n\n**If this notebook finds interesting and useful please upvote. **\n","a8b8c215":"# 5. Checking the datasets whether balanced or unbalanced","73a89694":"### This table shows the accuracy and other readings, for all the 10 folds.\n\nNow tuning of the hyper parameters.\nTuning the hyper parameters will be useful to increase the accuracy and other features.","a536a4d6":"# 10. Tuning the hyper parameters ","5e3e62ff":"# 13. Predicting the accuracy using the test dataset.\n\n## We get a accuracy of 1 (i.e. 100% accuracy)","a5cb0ab9":"There is no null values, the data is in a perfect condition so no data pre-processing is need here.\nWe know that there are three different species of Iris flowers (i.e.: Iris versicolor, Iris setosa and Iris virginica). For every classification problems it is better if the datasets are balanced(i.e. : if there are two classes infected(1) and non-infected(0) with total 1000 rows of data, we can say the datasets is balanced if 50% of data set is infected(1) and the other 50% is not infected(0)). In our case if all the three species are having approximately same amount of data then it is the best case. (If in case the datasets are unbalanced then there are lot of chances for under fitting and over fitting)\nLet\u2019s check this condition below.","ec91cbfd":"# 11. Plotting the ROC Curves\n\nAs the curve moves towards the x and y axis, the performance is increased.","3a7fc5b3":"c. Sepal Width & Petal Length","1c7b52d9":"b. Sepal Width & Petal Width","b215732b":"d. Sepal Length & Petal Width\n","1268325c":"f. Sepal Length & Sepal Width","67f99fba":"d. Sepal Length & Petal Width\n","e0e1416b":"# 1.Introduction\nIn this article we are going to learn how to build a model through a method which is quite different from the normal traditional method or so called raw method used in other Kaggle Works. The drawback of traditional method is that a lot of time is wasted in data pre-processing, feature selection, model selection, hyper parameter tuning etc.. These days many Auto MI\u2019s are available which can be easily pip installed and used very effectively. A lot of time-consuming works can be simply done with a couple of lines. In the majority of cases model accuracy level is more in this than the model which is made using the traditional method.","c3ca63f0":"Initially we separated a part of the datasets as unseen data set for checking the final developed model. Below we are checking this. The result is a data frame with Label and the score(last two columns). Where the label is the predicted label and score is how much percentage does the machine think of having an accuracy.","04066f96":"### It is a balanced dataset","fd6e61e7":"\n\nLogistic Regression\t\u2018lr\u2019\n\nK Nearest Neighbour\t\u2018knn\u2019\n\n**Naives Bayes\t\u2018nb\u2019 **\n\nDecision Tree\t\u2018dt\u2019\n\nSVM (Linear)\t\u2018svm\u2019\n\nSVM (RBF)\t\u2018rbfsvm\u2019\n\nGaussian Process\t\u2018gpc\u2019\n\nMulti Level Perceptron\t\u2018mlp\u2019\n\nRidge Classifier\t\u2018ridge\u2019\n\nRandom Forest\t\u2018rf\u2019\n\nQuadratic Disc. Analysis\t\u2018qda\u2019\n\nAdaBoost\t\u2018ada\u2019\n\nGradient Boosting Classifier\t\u2018gbc\u2019\n\nLinear Disc. Analysis\t\u2018lda\u2019\n\nExtra Trees Classifier\t\u2018et\u2019\n\nExtreme Gradient Boosting\t\u2018xgboost\u2019\n\nLight Gradient Boosting\t\u2018lightgbm\u2019\n\nCat Boost Classifier\t\u2018catboost\u2019","a8136e7a":"# 6. Preparing the data for model selection\nIn this step we are splitting the datasets into two. The first part contains 95% of the data that is used for training and testing. The remaining 5% is stored and is used to try with the final model we developed (This data is named as unseen data).","c85181fb":"**There is no null values, the data is in a perfect condition so no data preprocessing is need here.**","39fbb455":"# 14. Checking with the unseen data","a268308d":"# 3. Installing necessary packages  ","94ca6eca":"# 2. About the datasets\n\n\nEach row of the table represents an iris flower, including its species and dimensions of its botanical parts, sepal and petal, in centimetre. Here we are looking into three different species of iris flowers. They are\n1. Iris Versicolor\n2. Iris Setosa\n3. Iris Virginica\n\nIt includes 50 samples of each iris species as well as some features of the flowers. One flower species is linearly separable from the other two, but the other two are not linearly separable from each other.\n\nThe different columns in this dataset are:\n\n1. Id\n\n2. SepalLengthCm\n\n3. SepalWidthCm\n\n4. PetalLengthCm\n\n5. PetalWidthCm\n\n6. Species","8422eebb":"# 9. Creating the model\n\nNow create the model using the codes.","53b3b7e2":"# For unbalanced datasets we mainly look F1 score, as our datasets our balanced we can use the accuracy.\n\nFor this dataset we are already getting 100% accuracy.So without tuning the hyper parameters, it will work.","d0246ae1":"# 4. Now we are plotting the graphs by comparing each of the columns.\n\na. Petal Length & Petal Width","ff28c14f":"# 12. Confusion Matrix\n\nHere we can see that every value is Predicted accurately. All are in true positive.","1a745236":"# 8. Comparing the models\n\n\nAfter confirming the column types, we can run our datasets with a couple of ML algorithms and compare the performance, as shown below."}}