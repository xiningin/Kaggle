{"cell_type":{"6699c67d":"code","602e7b7d":"code","04212221":"code","34fe886c":"code","9b523f26":"code","fbbcef8e":"code","633aa91c":"markdown","9134e27b":"markdown","d81520be":"markdown","e684ac16":"markdown","06f8c67d":"markdown","fe905f15":"markdown","8c3c08c9":"markdown"},"source":{"6699c67d":"def split_data(data, data_labels, n, classes):\n    n_classes = len(np.unique(data_labels))\n    #classes = np.unique(data_labels)\n    n_train = n*n_classes\n    n_test = len(data)-n_train\n    test_index = data.index\n    \n    i= 0\n    for x_class in classes:\n        index = data_labels.loc[data_labels == classes[i]].index\n        nclass = len(index)\n        nclass = np.min([n,nclass])\n        train_choices = np.random.choice(index, nclass, replace = False)\n        test_index = test_index.drop(train_choices)\n        if'train_index' not in locals():\n            train_index = train_choices\n        else:\n            train_index = np.concatenate([train_index, train_choices])\n            \n        i = i+1\n    \n    return train_index, test_index\n\n\ndef upper_bound_partials(signal1, signal2):\n    signal_list = [signal1, signal2]\n    coordinate_list = [[],[]]\n    signal_lengths = [len(x) for x in signal_list]\n    \n    if signal_lengths[0]!= signal_lengths[1]:\n        index_long = np.argmax(signal_lengths)\n        index_short = np.argmin(signal_lengths)\n        long_len = len(signal_list[index_long])\n        short_len = len(signal_list[index_short])\n        fraction = short_len\/long_len\n        long_index_list = [i for i in range(long_len)]\n        short_index_list = [int(np.floor(x*fraction)) for x in range(long_len)]\n        warped_short = np.take(signal_list[index_short], short_index_list)\n        ub_partials = np.abs(signal_list[index_long]-warped_short)[::-1]\n        coordinate_list[index_long] = long_index_list\n        coordinate_list[index_short] = short_index_list\n        \n    else:\n        ub_partials = np.abs(signal_list[0]-signal_list[1])[::-1]\n        fraction = 1\n        index_list1 = [i for i in range(signal_lengths[0])]\n        index_list2 = [i for i in range(signal_lengths[1])]\n        coordinate_list[0] = index_list1\n        coordinate_list[1] = index_list2\n        \n    ub_partials = np.cumsum(ub_partials)[::-1]\n    coordinate_list = zip(coordinate_list[0],coordinate_list[1])\n    return list(coordinate_list), fraction, ub_partials \n\ndef pruned_dtw(matched, warped, window_size):\n    #create distance matrix\n    N = len(matched)\n    M = len(warped)\n   \n    ub_coordinate_list, fraction, ub_partials = upper_bound_partials(matched, warped)\n    \n    #initialize auxiliary pruning variables\n    start_column = 1 \n    end_column = 1 \n    \n    #window must be greater than N-M\n    window_size = np.max([window_size, N-M])\n    \n    #create cost matrix\n    cost_matrix = np.ndarray((N+1, M+1))\n    \n    #initialize to infinity\n    cost_matrix[:] = np.inf\n   \n    cost_matrix[0,0] = 0\n    \n    UB = ub_partials[1]\n  \n    \n    #create traceback matrix\n    traceback_matrix = np.ones((N,M))*np.inf\n    \n    \n    #initialize window elements to zero\n    for i in range(1,N+1):\n        \n        if N>=M:\n            ub_col_index = int(np.floor(i*fraction))\n            \n        else:\n            ub_col_index = int(np.floor(i\/fraction))\n        \n        \n        beg = np.max([start_column,ub_col_index-window_size])\n        end = np.min([M+1, ub_col_index+window_size+1])\n        smaller_found = False\n        \n        \n        end_column_next = ub_col_index\n      \n        for j in range(beg, end):\n            #cost = np.abs(matched[i-1]-warped[j-1])\n            cost = (matched[i-1]-warped[j-1])**2\n            \n            penalty = [cost_matrix[i-1,j-1],  #match 0\n                       cost_matrix[i-1,j], # insertion 1\n                       cost_matrix[i, j-1]] # deletion 2\n            penalty_index = np.argmin(penalty)\n            traceback_matrix[i-1,j-1] = penalty_index\n            cost_matrix[i,j] = cost +penalty[penalty_index]\n            \n            ###\n            #if i == ub_col_index and i != N:\n            if (i-1,j-1) in ub_coordinate_list:\n                ub_index = ub_coordinate_list.index((i-1,j-1))\n                UB = cost_matrix[i,j] + ub_partials[ub_index]\n                   \n            \n            ##pruning algorithm\n            if cost_matrix[i,j] > UB:\n                if smaller_found == False:\n                    start_column = j+1\n                \n                if j >= end_column:\n                    break\n    \n            else:\n                smaller_found = True\n                end_column_next = j+1\n        end_column = end_column_next\n        \n    #traceback from bottom right corner\n    \n    i = N-1\n    j = M-1\n    \n    path = [(i, j)]\n    \n    while (i>0 or j>0):\n        \n        tb_type = traceback_matrix[i,j]\n        \n        if tb_type == 0: \n            #match\n            i = i-1\n            j= j-1\n        \n        elif tb_type == 1:\n            #insertion\n            i = i-1\n        else:\n            #deletion\n            j = j-1\n        \n        path.append((i,j))\n    \n    #strip infinite row and column from cost matrix\n    cost_matrix = cost_matrix[1:, 1:]\n    \n    distance = np.sqrt(cost_matrix[-1,-1])\n    #invert path\n    path= path[::-1]\n    \n    return cost_matrix, path, distance\n\ndef scale_data(data, new_length):\n    nsignals = len(data)\n    scaled_data = np.zeros([nsignals,new_length])\n    for i in range(nsignals):\n        scaled_data[i] = adaptive_scaling(data[i], new_length)\n    return scaled_data\n\n\ndef adaptive_scaling(signal1, new_length):\n    differences = np.diff(signal1)\n    absolute_diffs = np.array([abs(d) for d in differences])\n\n    while len(signal1)> new_length:\n        \n        index = np.argmin(absolute_diffs)\n        new_val = np.mean([signal1[index],signal1[index+1]])\n        signal1[index] = new_val\n        signal1 = np.delete(signal1, index+1)\n        differences = np.delete(differences, index)\n        absolute_diffs = np.delete(absolute_diffs, index)\n        last_index = len(signal1)-1\n        \n        if index != last_index:\n            diff = signal1[index+1]-signal1[index]\n            differences[index] = diff\n            absolute_diffs[index] = abs(diff)\n            \n        if index != 0:\n            diff = signal1[index]-signal1[index-1]\n            differences[index-1] = diff\n            absolute_diffs[index-1] = abs(diff)\n    \n    return signal1\n\ndef dtw_average(signals, average_signal, iterations, window_size):\n    nsignals = len(signals)\n    paths = []\n    match_indices = []\n    signal_indices =[]\n    \n    for j in range(iterations):\n        \n        for n in range(nsignals):\n            #perform dtw between signals and average \n            paths.append(pruned_dtw(average_signal, signals[n], window_size)[1])\n    \n            #extract paired template indexes\n            match_indices.append(np.array(paths[n])[:,0])\n        \n            #extract the paired signal indexes\n            signal_indices.append(np.array(paths[n])[:,1]) \n            \n        #length of template\n        n_average = len(average_signal)\n\n        #initialize array to store the new average signal\n        average_signal = []\n        time_index = []\n        \n        #iterate through indices of the template signal\n        for i in range(n_average):\n            \n            for n in range(nsignals):\n               \n                #check to see if the template index i was paired with indexes of signaln\n                if i in match_indices[n]:\n                    \n    \n                    #if so find indexes of pairs which contain the template index\n                    indices = np.where(match_indices[n] == i)\n\n                \n                    #check to see if signal_values exists\n                    if 'signal_values' in locals():\n\n                        #if so find the paired signal2 indices, extract signal values and concat to signal_values\n                        signal_values = np.concatenate((signal_values,signals[n][signal_indices[n][tuple(indices)]]))\n\n                    else:\n                        #if not find the paired signal2 indices, extract signal values \n                        signal_values = signals[n][signal_indices[n][tuple(indices)]]\n            \n            #if signal_values exists take the mean and update average_signal[i]\n            if 'signal_values' in locals():\n                average_signal.append(np.mean(signal_values))\n                del signal_values\n                #save index value for time vector\n                time_index.append(i)\n        #convert lists to numpy arrays\n        average_signal = np.array(average_signal)\n        time_index = np.array(time_index)\n        \n    return average_signal\n\ndef get_boundary_signals(window, signal1):\n\n    sig_length = len(signal1)\n    ub_signal = np.zeros([sig_length])\n    lb_signal = np.zeros([sig_length])\n    for i in range(sig_length): \n        low_index = np.max([0, i-window])\n        up_index = np.min([i+window+1, sig_length])\n        ub_signal[i] = np.max(signal1[low_index:up_index])\n        lb_signal[i] = np.min(signal1[low_index:up_index])\n    return ub_signal, lb_signal\n    \n\ndef lb_keogh(ub_signal, lb_signal, signal2):\n\n    sig_length = len(signal2)\n    max_dists = np.zeros([sig_length])\n\n    for i in range(sig_length):\n        if signal2[i]>ub_signal[i]:\n            max_dists[i] = (signal2[i]- ub_signal[i])**2\n\n        elif signal2[i]<lb_signal[i]:\n            max_dists[i] = (signal2[i]- lb_signal[i])**2\n\n        else:\n            max_dists[i] = 0\n\n    lb_keogh = np.sqrt(np.sum(max_dists))\n    return lb_keogh\n\n\ndef kmeans_dtw(data, n_clusters, km_iterations, av_iterations , dtw_window, data_labels):\n    n = len(data)\n\n    l_signal = len(data[0])\n\n    #randomly assign samples to clusters\n    clusters = np.random.choice(a= np.array(range(n_clusters)),size = n)\n    \n    #instantiate centroid array\n    centroids = np.zeros((n_clusters, data.shape[1]))\n\n    #instantiate distance array\n    distances = np.zeros((n))\n    \n    purities = []\n    \n    best_purity = 0\n\n    #number of rounds of optimization\n    for i in range(km_iterations):\n\n        ub_matrix = np.zeros([n_clusters, l_signal])\n        lb_matrix = np.zeros([n_clusters, l_signal])\n\n        #calculate cluster centroids\n        for k in range(n_clusters):\n            \n            indices = np.where(clusters == k)[0]\n            \n            if np.any(indices):\n###\n                if i ==0:\n                    centroids[k] = data[np.random.choice(a= indices,size = 1)]\n\n                else:\n                    cluster_dists = distances[indices]\n\n                    length_c = len(cluster_dists)\n\n###\n                    #index =  length_c\/\/2 - math.floor((length_c\/3)*(purity**2))\n                    #index =  length_c\/\/2 -math.floor((length_c\/4)*(purity))\n                    #centroid_index = indices[np.argsort(cluster_dists)[index]]\n###\n                    centroid_index = indices[np.argsort(cluster_dists)[length_c\/\/2]]\n\n                    \n\n                    centroids[k] = data[centroid_index]\n            \n###\n\n            \n\n                ##change to barycenter averaging\n                centroids[k] = dtw_average(data[indices], centroids[k], av_iterations, dtw_window)\n                ub_signal, lb_signal = get_boundary_signals(dtw_window, centroids[k])\n                ub_matrix[k] = ub_signal\n                lb_matrix[k] = lb_signal\n\n\n        for j in range(n):\n            best_so_far = np.inf\n            best_cluster_index = int\n            signal1 = data[j]\n            for k in range(n_clusters):\n                centroid = centroids[k]\n\n                ub_signal = ub_matrix[k]\n                lb_signal = lb_matrix[k]\n                lb_k = lb_keogh(ub_signal, lb_signal, signal1)\n\n                if lb_k< best_so_far:\n\n                    cost_matrix, path, distance = pruned_dtw(centroid, signal1, dtw_window)\n\n                    if distance < best_so_far:\n                        best_so_far = distance\n                        best_cluster_index = k   \n\n            clusters[j] = best_cluster_index\n            distances[j] = best_so_far\n        correspondence, purity = purity_score(data_labels, clusters)\n        if purity > best_purity:\n            best_purity = purity\n            best_clusters = deepcopy(clusters)\n            best_centroids = deepcopy(centroids)\n            best_correspondence = deepcopy(correspondence)\n            \n        purities.append(purity)\n        \n        if purity == 1:\n            break\n           \n    return best_clusters, best_centroids, purities, best_purity, best_correspondence\n\ndef purity_score(data_labels, clusters):\n    in_cluster_count= {}\n    correspondence = {}\n    cluster_labels = np.unique(clusters)\n    N = len(data_labels)\n    for k in range(len(cluster_labels)):\n        cluster_label = cluster_labels[k]\n        cluster = np.where(clusters ==cluster_label)\n        if np.any(cluster):\n            data_labels = np.array(data_labels, dtype='int')\n            \n            in_cluster_count[cluster_label] = np.bincount(data_labels[cluster]).max()\n            correspondence[cluster_label] = np.bincount(data_labels[cluster]).argmax()\n    purity = 0\n    for val in in_cluster_count.values():\n        purity = purity + val\/N\n        \n    return correspondence, purity    \n\ndef get_best_cluster_index(purities, ncluster_list, threshold = .005):\n    best_index = 0\n    best_number = ncluster_list[best_index]\n    best_so_far = purities[best_index]\n    max_score = np.max(purities)\n    max_score_index = np.argmax(purities)\n    \n    for i in range(1,len(purities)):\n\n        increase_from_best_so_far = (purities[i]-best_so_far)\/(ncluster_list[i]-best_number)\n\n        if increase_from_best_so_far> threshold:\n\n            best_index = i\n            best_number = ns[best_index]\n            best_so_far = purities[i]\n\n    return best_index\n\ndef predict_test_data_lb_k(test_filt_data,average_signal_list, window_size, correspondence):    \n    nsignals = len(test_filt_data)\n    nclasses = len(average_signal_list)\n    l_signal = len(test_filt_data[0])\n    \n    up_bound_matrix = np.zeros([nclasses, l_signal])\n    low_bound_matrix = np.zeros([nclasses, l_signal])\n    \n    for j in range(nclasses):\n        ub_signal, lb_signal = get_boundary_signals(window_size, average_signal_list[j])\n        up_bound_matrix[j] = ub_signal\n        low_bound_matrix[j] = lb_signal\n\n    predicted_labels = np.zeros([nsignals])\n    \n    for i in range(nsignals):\n       \n        best_so_far = np.inf\n        best_class_index = 0\n        \n        signal1 = test_filt_data[i]\n        \n        for j in range(nclasses):\n###\n            if j in correspondence.keys():\n###\n                ub_signal = up_bound_matrix[j]\n                lb_signal = low_bound_matrix[j]\n                lb_k = lb_keogh(ub_signal, lb_signal, test_filt_data[i])\n\n                if lb_k < best_so_far:\n\n                    signal2 = average_signal_list[j]\n                    cost_matrix, path, distance = pruned_dtw(signal1, signal2, window_size)\n                    dist = cost_matrix[-1,-1]\n\n                    if dist < best_so_far:\n                        best_so_far = dist\n                        best_class_index = j\n          \n        #map to original data_labels\n        predicted_label = correspondence[best_class_index]\n        predicted_labels[i] = predicted_label\n        \n    return predicted_labels","602e7b7d":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport time\nimport math\nfrom copy import deepcopy","04212221":"data = pd.read_csv('http:\/\/storage.googleapis.com\/download.tensorflow.org\/data\/ecg.csv', header = None)\ndata_labels = data[140].apply(lambda x: int(x))\ndel data[140]\ndata_labels.value_counts()\n\n\nscale_number = 80\ntrain_index, test_index = split_data(data, data_labels, 40, [0,1])\ntrain_data = data.reindex(train_index).to_numpy()\ntrain_labels = data_labels.reindex(train_index).to_numpy()\ntrain_data = scale_data(train_data, scale_number)\n\ntest_data = data.reindex(test_index).to_numpy()\ntest_data = scale_data(test_data, scale_number)\ntest_labels = data_labels.reindex(test_index).to_numpy()","34fe886c":"km_iterations =10\nav_iterations =1\ndtw_window = 10\n\nns = [2, 3, 4, 5]\npurities = []\ncorrespondences = []\nclusters_list = []\ncentroids_list = []\n\nstart_time = time.time()\nfor i in range(len(ns)):\n    \n    n = ns[i]\n    \n    #kmeans clustering\n   \n    clusters, centroids, _, purity, correspondence = kmeans_dtw(train_data, n, km_iterations, av_iterations , dtw_window, train_labels)\n    \n    clusters_list.append(clusters)\n    centroids_list.append(centroids)\n    correspondences.append(correspondence)\n    purities.append(purity)\n\nend_time = time.time()\nprint('runtime is {} s'.format(end_time-start_time))\n\nbest_index = get_best_cluster_index(purities, ns, threshold = .005)\n\ncorrespondence = correspondences[best_index]\nclusters = clusters_list[best_index]\ncentroids = centroids_list[best_index]\nn_clusters = ns[best_index]\n\nplt.plot(ns, purities)\nplt.scatter(ns[best_index], purities[best_index])\nplt.xlabel('number of clusters')\nplt.ylabel('clusters purity score')\nplt.title('purity score vs. number of clusters')\nplt.show()\n\nprint('best purity: {}'.format(purities[best_index]))\nprint('best number of clusters: {}'.format(ns[best_index]))","9b523f26":"average_signals = np.zeros(centroids.shape)\nn_clusters = ns[best_index]\nav_iterations = 10\n\n\nfor i in correspondence.keys():\n   \n    corr = correspondence[i]\n    average_signal = centroids[i]\n    index1 = set(np.where(clusters == i)[0])\n    index2 = set(np.where(train_labels == corr)[0])\n    intersection = list(index1.intersection(index2))\n    signals1 = train_data[intersection]\n    average_signal = dtw_average(signals1, average_signal, av_iterations, dtw_window)\n    average_signals[i] = average_signal\n\nk=0\ncolors = ['blue', 'red', 'purple', 'orange']\nfor i in range(0, n_clusters):\n    \n    if i in correspondence.keys():\n        label = correspondence[i]\n        if label == 1:\n            label = 'Normal'\n        else:\n            label = 'Abnormal'\n        c = colors[correspondence[i]-1]\n        \n        indices = np.where(clusters == i)[0]\n        for index in indices:\n            plt.plot(train_data[index]+5*k, alpha= .5)\n        \n        plt.plot(average_signals[i]+5*k, c= c, label = label)\n    k+=1\n    \n    \nplt.title('Cluster Centroid Sinals')\nhandles, labels = plt.gca().get_legend_handles_labels()\nby_label = dict(zip(labels, handles))\nplt.legend(by_label.values(), by_label.keys())\nplt.show()","fbbcef8e":"start_time = time.time()\npredicted_labels = predict_test_data_lb_k(test_data, average_signals, dtw_window, correspondence)\nend_time = time.time()\nprint('runtime is {} s'.format(end_time-start_time))\n\nscore = accuracy_score(test_labels, predicted_labels, normalize=True)\nc_mat = confusion_matrix(test_labels, predicted_labels)\n\nabnorm_pr = c_mat[0,0]\/(np.sum(c_mat[:,0]))\nabnorm_tot = np.sum(c_mat[0,:])\nabnorm_re = c_mat[0,0]\/abnorm_tot\n\nnorm_pr = c_mat[1,1]\/np.sum(c_mat[:,1])\nnorm_tot = np.sum(c_mat[1,:])\nnorm_re = c_mat[1,1]\/norm_tot\n\nprint('total accuracy_score: {}'.format(score))\nprint('predicton of normal ECG: \\n    recall: {}\\n    precision:{}\\n    total: {}'.format(norm_re, norm_pr,norm_tot))\nprint('predicton of abnormal ECG: \\n    recall: {}\\n    precision:{}\\n    total: {}'.format(abnorm_re, abnorm_pr, abnorm_tot))","633aa91c":"#### Nearest centroid classification and model evaluation\n\nThe test data is predicted using a nearest centroid classifier and evaluated with respect to runtime, accuracy, class recall and class precision.","9134e27b":"#### Function definitions for prunedDTW, barycenter averaging, k-means, nearest centroid classification, adaptive scaling, lb Keogh etc.","d81520be":"#### Imports","e684ac16":"### ECG Classification using PrunedDTW\n\n#### Goal: \n\nThe purpose of this notebook is to explore the use of dynamic time warping (DTW) in the clustering and classification of electrocardiogram (ECG) time series as either 'normal\u2019 or \u2018abnormal\u2019. \n\n#### Data:\n\nThe data is a simplified version of the ECG5000 data set. The dataset contains 5000 samples of 140 data points each representing an ECG waveform of a single heartbeat. Each sample is labeled either 0 or 1, where the label of 0 represents an abnormal heart rhythm and the label of 1 represents a normal heart rhythm. The ECG5000 data set was ultimately derived from record chf7 of the the BIDMC Congestive Heart Failure Database (CHFD). Each record in CHGD is a 20h long ECG recording of a patient having congestive heart failure which were recorded at Beth Israel Deaconess Medical Center in Boston. To create ECG5000, each heartbeat was extracted from record chf7 and interpolated to have a length of 140 data points. Then 5000 heartbeats were randomly selected. Heartbeats were annotated using an automated detector.\n\n* [original ECG5000 dataset](http:\/\/www.timeseriesclassification.com\/description.php?Dataset=ECG5000)\n* [simplified ECG5000dataset](https:\/\/www.tensorflow.org\/tutorials\/generative\/autoencoder)\n* [BIDMC Congestive Heart Failure Database](https:\/\/physionet.org\/content\/chfdb\/1.0.0\/)\n\n#### Background:\n\nDynamic Time Warping (DTW) is a technique used to measure the similarity\/distance between two time series. DTW first finds an optimal alignment between two time series allowing one time series to stretch or compress to best fit the other time series. The DTW distance is then calculated as the Euclidean distance between the first matched time series and the optimally warped second time series. \n\nDTW is useful when we are interested in morphological features between time series which do not exactly align with each other temporally and\/or which are occurring at different speeds relative to each other. For instance, imagine measuring the similarity between two waveforms representing the same spoken sentence. The first speaker might speak relatively slow and take long pauses between each word, whereas the second speaker might speak relatively quick with short pauses between words. A normal euclidean distance would be an inappropriate measure of similarity in this case since a) the two time series likely have different lengths for which the euclidean distance would be undefined and b) the similar morphological features between the two time series would likely not be aligned with each other. DTW solves this problem by allowing one waveform to be stretched in some places and compressed in other places before taking the Euclidean distance so that similar portions between the two time series can be better compared.\n\nThe exact implementation of the DTW algorithm is a little outside the scope of this notebook, but in a nutshell DTW is an example of dynamic programming in which solutions to smaller problems are iteratively solved and used to find solution to bigger problems until the solution to the whole problem is found. Practically speaking, DTW is implemented by calculating each cell of a cost matrix which is NXM where N is the length of a first time series and M is the length of a second time series. The fundamental limitation of DTW is that as the lengths of both time series increases linearly, the number of calculations which need to be performed increases quadratically.\n\nSeveral approaches can be employed to mitigate the time complexity of DTW alone or in combination with each other. The first approach is to limit the number of cells evaluated in the cost matrix. This can be achieved by implementing a fixed window. Using this technique, only matrix cells within the defined fixed-window are evaluated  which reduces the total number of calculations implemented by the algorithm. This approach constrains the degree to which the time series can be warped. A second technique within this approach is to implement an optimized version of the DTW algorithm called PrunedDTW. PrunedDTW reduces the number of cells that need to be evaluated by pruning sections of cells which are cleverly determined to not include the optimal warping path. PrunedDTW can be combined with the fixed window technique. A final technique which falls within this approach is to implement the FastDTW algorithm. FastDTW works by recursively calculating DTW between lower resolution representations of the time series and using the calculated lower resolution warping path to define a tight window for the next higher resolution representation of the signal. Unlike the naive DTW algorithm, FastDTW is shown to have a linear time-complexity. The downside of FastDTW is that it is not guaranteed to provide an optimal warping path.\n\nThe second approach is to reduce the total number of calculations by shortening the time series prior to performing DTW. This can be achieved by using a technique called adaptive scaling.\n\nThe third approach is to reduce the total number of calculations by reducing the number of times that DTW needs to be performed as part of a larger algorithm. For instance, if we are performing a nearest neighbor search as part of a KNN algorithm, if prior to performing DTW we can determine that the minimum value or lower bound of the DTW distance between a given time series and a candidate nearest neighbor is guaranteed to be greater than our best-so-far distance, then we can skip performing the DTW altogether and move on to the next candidate. A DTW lower bound can be calculated using the LB Keogh technique.\n\nAnother important piece of background information, is that DTW has an associated averaging technique, called barycenter averaging, which can be used to average multiple time series together into a single representative time series. The implication of this is that DTW distance and barycenter averaging can be used in the context of a K-Means clustering algorithm to cluster time-series directly. Further, if the found clusters are largely homogeneous with respect to class then their respective centroids can be considered good representatives of their respective classes and they can be used to implement a parsimonious nearest centroid classifier on unseen data. In other words, test signals can be classified based on which centroid they are closest to based on DTW distance.\n\n#### References:\n\n* [naive dtw algorithm along with sample python code](https:\/\/www.youtube.com\/playlist?list=PLmZlBIcArwhMJoGk5zpiRlkaHUqy5dLzL)\n* [FastDTW algorithm](https:\/\/cs.fit.edu\/~pkc\/papers\/tdm04.pdf)\n* [PrunedDTW algorithm](http:\/\/sites.labic.icmc.usp.br\/dfs\/pdf\/SDM_PrunedDTW.pdf)\n* [barycenter averaging and nearest centroid classification](http:\/\/www.francois-petitjean.com\/Research\/Petitjean2014-ICDM-DTW.pdf)\n* [barycenter averaging and adaptive scaling](http:\/\/www.francois-petitjean.com\/Research\/Petitjean2011-PR.pdf)\n* [lb Keogh](https:\/\/tslearn.readthedocs.io\/en\/stable\/auto_examples\/metrics\/plot_lb_keogh.html)\n\n\n#### Methodology:\n\nData is first split into training and test datasets and scaled down using the adaptive scaling. PrunedDTW and barycenter averaging are then used in the context of K-means clustering algorithm to cluster the training set into a parsimonious set of homogenous clusters where the different centroids are taken as representative of \u2018normal\u2019 and \u2018abnormal\u2019 ECG signals. These cluster centroids are then further refined by removing any extraneous non-class signals from the different clusters and performing barycenter averaging using the centroids as the initial template signals. Finally, these average signals are used to implement a nearest centroid classification algorithm using PrunedDTW as the distance measure. The K-means and nearest centroid algorithms also employ lb Keogh as a lower bounding technique to speed up their implementations.\n","06f8c67d":"#### Cluster centroids are further refined\n\nCluster centroids are further refined by removing any out-of-class signals from the clusters and then performing barycenter averaging. The centroid itself is used as the initial average template signal. ","fe905f15":"#### Preprocessing\n\nData is imported and split into test and training sets. Training and test sets are rescaled to a reduced representation.","8c3c08c9":"#### K-means to find best centroids\n\nK-means is performed for several numbers of clusters, the best model from each clustering is saved and evaluated with respect to cluster purity. The best model is chosen based on a tradeoff between cluster purity and number of clusters. "}}