{"cell_type":{"8190e4ba":"code","8f8a6d35":"code","52970683":"code","e5fcf591":"code","2d6ba6d7":"code","f32e370d":"code","dcd9bb55":"code","d5342ba4":"code","daeea833":"code","e52a8583":"code","fca46122":"code","10d6ad70":"code","2736593f":"code","a17de3b1":"code","6e8f74a8":"code","478e0c0e":"code","49627866":"code","5622a470":"code","0cc62f4b":"code","ab62f615":"code","602bbbf3":"code","fa10957c":"code","55d20a1d":"code","e5c3d931":"code","a8c5a83b":"code","d868f8d7":"code","1446d9c6":"code","b2916c21":"code","653155df":"markdown"},"source":{"8190e4ba":"# Parameters\nFUDGE_FACTOR = 1.1200  # Multiply forecasts by this\n\nXGB_WEIGHT = 0.6200\nBASELINE_WEIGHT = 0.0100\nOLS_WEIGHT = 0.0620\nNN_WEIGHT = 0.0800\n\nXGB1_WEIGHT = 0.8000  # Weight of first in combination of two XGB models\n\nBASELINE_PRED = 0.0115   # Baseline based on mean of training data, per Oleg","8f8a6d35":"import numpy as np\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport gc\nfrom sklearn.linear_model import LinearRegression\nimport random\nimport datetime as dt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout, BatchNormalization\nfrom keras.layers.advanced_activations import PReLU\nfrom keras.layers.noise import GaussianDropout\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer as Imputer\nimport warnings\nwarnings.filterwarnings('ignore')","52970683":"# read in raw data\nprint('reading data from disk...')\nprop = pd.read_csv('..\/input\/zillow-prize-1\/properties_2016.csv')\ntrain = pd.read_csv('..\/input\/zillow-prize-1\/train_2016_v2.csv')","e5fcf591":"# process data fro lightgbm\nprint('processing data for lightGBM...')\nfor c, dtype in zip(prop.columns, prop.dtypes):\n    if dtype == np.float64:\n        prop[c] = prop[c].astype(np.float32)\n\ndf_train = train.merge(prop, how = 'left', on = 'parcelid')\ndf_train.fillna(df_train.median(), inplace = True)\n\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', \n                         'propertycountylandusecode', 'fireplacecnt', 'fireplaceflag'], axis=1)\ny_train = df_train['logerror'].values\nprint(x_train.shape, y_train.shape)","2d6ba6d7":"train_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)\n    \ndel df_train; gc.collect()\n\nx_train = x_train.values.astype(np.float32, copy=False)\nd_train = lgb.Dataset(x_train, label = y_train)","f32e370d":"# run light gbm\n\nparams = {}\nparams['max_bin'] = 10\nparams['learning_rate'] = 0.0021 # shrinkage_rate\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = 'l1'          # or 'mae'\nparams['sub_feature'] = 0.345    # feature_fraction (small values => use very different submodels)\nparams['bagging_fraction'] = 0.85 # sub_row\nparams['bagging_freq'] = 40\nparams['num_leaves'] = 512        # num_leaf\nparams['min_data'] = 500         # min_data_in_leaf\nparams['min_hessian'] = 0.05     # min_sum_hessian_in_leaf\nparams['verbose'] = 0\nparams['feature_fraction_seed'] = 2\nparams['bagging_seed'] = 3\n\nnp.random.seed(0)\nrandom.seed(0)\n\nprint(\"\\nFitting LightGBM model ...\")\nclf = lgb.train(params, d_train, 10)","dcd9bb55":"del d_train; gc.collect()\ndel x_train; gc.collect()","d5342ba4":"print(\"\\nPrepare for LightGBM prediction ...\")\nprint(\"   Read sample file ...\")\nsample = pd.read_csv('..\/input\/zillow-prize-1\/sample_submission.csv')\nprint(\"   ...\")\n\nsample['parcelid'] = sample['ParcelId']\nprint(\"   Merge with property data ...\")\n\ndf_test = sample.merge(prop, on='parcelid', how='left')\nprint(\"   ...\")\n\ndel sample, prop; gc.collect()\nprint(\"   ...\")\n\n#df_test['Ratio_1'] = df_test['taxvaluedollarcnt']\/df_test['taxamount']\nx_test = df_test[train_columns]\nprint(\"   ...\")\ndel df_test; gc.collect()\n\nprint(\"   Preparing x_test...\")\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)\nprint(\"   ...\")\nx_test = x_test.values.astype(np.float32, copy=False)\n\nprint(\"\\nStart LightGBM prediction ...\")\np_test = clf.predict(x_test)\n\ndel x_test; gc.collect()\n\nprint( \"\\nUnadjusted LightGBM predictions:\" )\nprint( pd.DataFrame(p_test).head() )","daeea833":"##### RE-READ PROPERTIES FILE\n\nprint( \"\\nRe-reading properties file ...\")\nproperties = pd.read_csv('..\/input\/zillow-prize-1\/properties_2016.csv')","e52a8583":"##### Process data for xgboost\n\nprint('\\nProcessing data for XGBoost...')\nfor c in properties.columns:\n    properties[c] = properties[c].fillna(-1)\n    if properties[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(properties[c].values))\n        properties[c] = lbl.transform(list(properties[c].values))\n        \ntrain_df = train.merge(properties, how = 'left', on ='parcelid')\nx_train = train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\nx_test = properties.drop(['parcelid'], axis = 1)\n\nprint('shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))","fca46122":"# drop out outliers\ntrain_df = train_df[train_df.logerror > -0.4]\ntrain_df = train_df[train_df.logerror < 0.419]\nx_train=train_df.drop(['parcelid', 'logerror','transactiondate'], axis=1)\ny_train = train_df[\"logerror\"].values.astype(np.float32)\ny_mean = np.mean(y_train)\n\nprint('After removing outliers:')     \nprint('Shape train: {}\\nShape test: {}'.format(x_train.shape, x_test.shape))","10d6ad70":"##### RUN XGBOOST\n\nprint(\"\\nSetting up data for XGBoost ...\")\n# xgboost params\nxgb_params = {\n    'eta': 0.037,\n    'max_depth': 5,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'lambda': 0.8,   \n    'alpha': 0.4, \n    'base_score': y_mean,\n    'silent': 1\n}\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndtest = xgb.DMatrix(x_test)\n\nnum_boost_rounds = 50\nprint('num_boost_rounds = '+str(num_boost_rounds))","2736593f":"# train model\nprint('\\nTraining XGBoost')\nmodel = xgb.train(dict(xgb_params, verbosity=0),\n                  dtrain, num_boost_round=num_boost_rounds)\n\nprint('\\nPredicting with XGBoost....')\nxgb_pred1 = model.predict(dtest)\n\nprint('\\nFirst XGBoost predictinos:')\nprint(pd.DataFrame(xgb_pred1).head())","a17de3b1":"# RUN  xgboost again\n\nprint('\\nSetting up data for XGBoost ...')\n\n# xgboost params\nxgb_params = {\n    'eta': 0.033,\n    'max_depth': 6,\n    'subsample': 0.80,\n    'objective': 'reg:linear',\n    'eval_metric': 'mae',\n    'base_score': y_mean,\n    'silent': 1\n}\n\nnum_boost_rounds = 50\nprint(\"num_boost_rounds=\"+str(num_boost_rounds))\n\nprint( \"\\nTraining XGBoost again ...\")\nmodel = xgb.train(dict(xgb_params, silent=1), dtrain, num_boost_round=num_boost_rounds)\n\nprint( \"\\nPredicting with XGBoost again ...\")\nxgb_pred2 = model.predict(dtest)\n\nprint( \"\\nSecond XGBoost predictions:\" )\nprint( pd.DataFrame(xgb_pred2).head())","6e8f74a8":"# combine xgboost results\nxgb_pred = XGB1_WEIGHT * xgb_pred1 + (1 - XGB1_WEIGHT) * xgb_pred2\n\nprint('\\ncombined XGBoost predictions: ')\nprint(pd.DataFrame(xgb_pred).head())\n\ndel train_df\ndel x_train\ndel x_test\ndel properties\ndel dtest\ndel dtrain\ndel xgb_pred1\ndel xgb_pred2 \ngc.collect()","478e0c0e":"# Read in data for neural network\nprint( \"\\n\\nProcessing data for Neural Network ...\")\nprint('\\nLoading train, prop and sample data...')\ntrain = pd.read_csv(\"..\/input\/zillow-prize-1\/train_2016_v2.csv\",\n                    parse_dates=[\"transactiondate\"])\nprop = pd.read_csv('..\/input\/zillow-prize-1\/properties_2016.csv')\nsample = pd.read_csv('..\/input\/zillow-prize-1\/sample_submission.csv')","49627866":"print('Fitting Label Encoder on properties...')\nfor c in prop.columns:\n    prop[c] = prop[c].fillna(-1)\n    if prop[c].dtype == 'object':\n        lbl = LabelEncoder()\n        lbl.fit(list(prop[c].values))\n        prop[c] = lbl.transform(list(prop[c].values))\n        \nprint('Creating training set...')\ndf_train = train.merge(prop, how='left', on='parcelid')\n\ndf_train[\"transactiondate\"] = pd.to_datetime(df_train[\"transactiondate\"])\ndf_train[\"transactiondate_year\"] = df_train[\"transactiondate\"].dt.year\ndf_train[\"transactiondate_month\"] = df_train[\"transactiondate\"].dt.month\ndf_train['transactiondate_quarter'] = df_train['transactiondate'].dt.quarter\ndf_train[\"transactiondate\"] = df_train[\"transactiondate\"].dt.day","5622a470":"print('Filling NA\/NaN values...')\ndf_train.fillna(-1.0)\n\nprint('Creating x_train and y_train form df_train....')\n\nx_train = df_train.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'propertycountylandusecode','fireplacecnt', 'fireplaceflag'], axis=1)\ny_train = df_train[\"logerror\"]\n\ny_mean = np.mean(y_train)\nprint(x_train.shape, y_train.shape)\ntrain_columns = x_train.columns\n\nfor c in x_train.dtypes[x_train.dtypes == object].index.values:\n    x_train[c] = (x_train[c] == True)","0cc62f4b":"print('Creating df_test...')\nsample['parcelid'] = sample['ParcelId']\n\nprint(\"Merging Sample with property data...\")\ndf_test = sample.merge(prop, on='parcelid', how='left')\n\ndf_test[\"transactiondate\"] = pd.to_datetime('2016-11-15')  # placeholder value for preliminary version\ndf_test[\"transactiondate_year\"] = df_test[\"transactiondate\"].dt.year\ndf_test[\"transactiondate_month\"] = df_test[\"transactiondate\"].dt.month\ndf_test['transactiondate_quarter'] = df_test['transactiondate'].dt.quarter\ndf_test[\"transactiondate\"] = df_test[\"transactiondate\"].dt.day     \nx_test = df_test[train_columns]\n\nprint('Shape of x_test:', x_test.shape)\nprint(\"Preparing x_test...\")\nfor c in x_test.dtypes[x_test.dtypes == object].index.values:\n    x_test[c] = (x_test[c] == True)","ab62f615":"# preprocssing\nprint('\\nPreprocssing neural network data...')\nimputer = Imputer()\nimputer.fit(x_train.iloc[:, :])\nx_train = imputer.transform(x_train.iloc[:, :])\nimputer.fit(x_test.iloc[:, :])\nx_test = imputer.transform(x_test.iloc[:, :])\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\nlen_x = int(x_train.shape[1])\nprint('len_x is: ', len_x)","602bbbf3":"# Neural Network\nprint(\"\\nSetting up neural network model...\")\nnn = Sequential()\nnn.add(Dense(units = 400, kernel_initializer = 'normal', input_dim = len_x))\nnn.add(PReLU())\nnn.add(Dropout(.4))\nnn.add(Dense(units = 160 , kernel_initializer = 'normal'))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(.6))\nnn.add(Dense(units = 64 , kernel_initializer = 'normal'))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(.5))\nnn.add(Dense(units = 26, kernel_initializer = 'normal'))\nnn.add(PReLU())\nnn.add(BatchNormalization())\nnn.add(Dropout(.6))\nnn.add(Dense(1, kernel_initializer='normal'))\nnn.compile(loss='mae', optimizer=Adam(lr=4e-3, decay=1e-4))","fa10957c":"print(\"\\nFitting neural network model...\")\nnn.fit(np.array(x_train), np.array(y_train), batch_size = 32, epochs = 70, verbose=2)\n\nprint(\"\\nPredicting with neural network model...\")\n#print(\"x_test.shape:\",x_test.shape)\ny_pred_ann = nn.predict(x_test)\n\nprint( \"\\nPreparing results for write...\" )\nnn_pred = y_pred_ann.flatten()\nprint( \"Type of nn_pred is \", type(nn_pred) )\nprint( \"Shape of nn_pred is \", nn_pred.shape )\n\nprint( \"\\nNeural Network predictions:\" )\nprint( pd.DataFrame(nn_pred).head() )","55d20a1d":"# Cleanup\ndel train\ndel prop\ndel sample\ndel x_train\ndel x_test\ndel df_train\ndel df_test\ndel y_pred_ann\ngc.collect()","e5c3d931":"# OLS\nnp.random.seed(17)\nrandom.seed(17)\n\nprint( \"\\n\\nProcessing data for OLS ...\")\n\ntrain = pd.read_csv(\"..\/input\/zillow-prize-1\/train_2016_v2.csv\", \n                    parse_dates=[\"transactiondate\"])\nproperties = pd.read_csv(\"..\/input\/zillow-prize-1\/properties_2016.csv\")\nsubmission = pd.read_csv(\"..\/input\/zillow-prize-1\/sample_submission.csv\")\nprint(len(train),len(properties),len(submission))\n\ndef get_features(df):\n    df[\"transactiondate\"] = pd.to_datetime(df[\"transactiondate\"])\n    df[\"transactiondate_year\"] = df[\"transactiondate\"].dt.year\n    df[\"transactiondate_month\"] = df[\"transactiondate\"].dt.month\n    df['transactiondate'] = df['transactiondate'].dt.quarter\n    df = df.fillna(-1.0)\n    return df\n\ndef MAE(y, ypred):\n    #logerror=log(Zestimate)\u2212log(SalePrice)\n    return np.sum([abs(y[i]-ypred[i]) for i in range(len(y))]) \/ len(y)","a8c5a83b":"train = pd.merge(train, properties, how = 'left', on = 'parcelid')\ny = train['logerror'].values\ntest = pd.merge(submission, properties, how='left', left_on ='ParcelId',\n                 right_on='parcelid')\nproperties = [] #memory\n\nexc = [train.columns[c] for c in range(len(train.columns)) if train.dtypes[c] == 'O'] + ['logerror','parcelid']\ncol = [c for c in train.columns if c not in exc]\n\ntrain = get_features(train[col])\ntest['transactiondate'] = '2016-01-01'\ntest = get_features(test[col])\n\nprint(\"\\nFitting OLS...\")\nreg = LinearRegression(n_jobs=-1)\nreg.fit(train, y); print('fit...')\nprint(MAE(y, reg.predict(train)))\ntrain = [];  y = [] #memory\n\ntest_dates = ['2016-10-01','2016-11-01','2016-12-01','2017-10-01','2017-11-01','2017-12-01']\ntest_columns = ['201610','201611','201612','201710','201711','201712']","d868f8d7":"print( \"\\nCombining XGBoost, LightGBM, NN, and baseline predicitons ...\" )\nlgb_weight = 1 - XGB_WEIGHT - BASELINE_WEIGHT - NN_WEIGHT - OLS_WEIGHT \nlgb_weight0 = lgb_weight \/ (1 - OLS_WEIGHT)\nxgb_weight0 = XGB_WEIGHT \/ (1 - OLS_WEIGHT)\nbaseline_weight0 =  BASELINE_WEIGHT \/ (1 - OLS_WEIGHT)\nnn_weight0 = NN_WEIGHT \/ (1 - OLS_WEIGHT)\npred0 = 0\npred0 += xgb_weight0*xgb_pred\npred0 += baseline_weight0*BASELINE_PRED\npred0 += lgb_weight0*p_test\npred0 += nn_weight0*nn_pred","1446d9c6":"print( \"\\nCombined XGB\/LGB\/NN\/baseline predictions:\" )\nprint( pd.DataFrame(pred0).head() )\n\nprint( \"\\nPredicting with OLS and combining with XGB\/LGB\/NN\/baseline predicitons: ...\" )\nfor i in range(len(test_dates)):\n    test['transactiondate'] = test_dates[i]\n    pred = FUDGE_FACTOR * ( OLS_WEIGHT*reg.predict(get_features(test)) + (1-OLS_WEIGHT)*pred0 )\n    submission[test_columns[i]] = [float(format(x, '.4f')) for x in pred]\n    print('predict...', i)\n\nprint( \"\\nCombined XGB\/LGB\/NN\/baseline\/OLS predictions:\" )\nprint( submission.head() )","b2916c21":"##### WRITE THE RESULTS\n\nfrom datetime import datetime\n\nprint( \"\\nWriting results to disk ...\" )\nsubmission.to_csv('sub{}.csv'.format(datetime.now().strftime('%Y%m%d_%H%M%S')), index=False)\n\nprint( \"\\nFinished ...\")","653155df":"#### This kernel used dataset from the Zillow Prize: Zillow\u2019s Home Value Prediction and copied from the 'XGBoost, LightGBM, and OLS and NN' written by ANDY HARLESS.\n#### Introduction to 'XGBoost, LightGBM, and OLS and NN' : [URL](https:\/\/www.kaggle.com\/aharless\/xgboost-lightgbm-and-ols-and-nn)\n#### Thanks for sharing kernel, ANDY HARLESS."}}