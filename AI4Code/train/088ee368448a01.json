{"cell_type":{"87136386":"code","f7abc024":"code","902f9810":"code","bb050c5f":"code","7df9a28a":"code","b80a9829":"code","a9337be9":"code","5106fd3b":"code","b0c5c2a1":"code","7ddd8ae6":"code","a0e40df8":"code","b573139e":"code","40530651":"code","cc486425":"code","3e7f8d5e":"code","a8f74bf6":"code","b78e730e":"markdown","036d20fb":"markdown","b9ee9190":"markdown","e4e6be19":"markdown","1005d3b9":"markdown","07f38851":"markdown","6d7584be":"markdown","3ec6b442":"markdown","e1356c1d":"markdown","5f1d3564":"markdown","925a00a6":"markdown","91ad2ad0":"markdown","629f68e7":"markdown","5d5e0222":"markdown","0091ab48":"markdown","1f17691c":"markdown","dc7eba52":"markdown","04641e95":"markdown","83d278c9":"markdown","92b32641":"markdown","710638a5":"markdown","e2705909":"markdown","2a448eae":"markdown"},"source":{"87136386":"print(\"\\n... IMPORTS STARTING ...\\n\")\n\nprint(\"\\n... PIP\/APT INSTALLS AND DOWNLOADS\/ZIP STARTING ...\")\n!pip install -q tensorflow-model-optimization\n!pip install -q neural-structured-learning\n!pip install -q tensorflow_datasets\n!pip install -q segmentation-models\n!pip install -q xarray-spatial\n\n## Only to be used if we can figure out that TPU bug\n# !pip install -q tf-nightly\n\n# Try to skip and disable so we can submit w\/o internet\n# !pip install -q ..\/input\/tensorflow-model-optimization\/numpy-1.21.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n# !pip install -q ..\/input\/tensorflow-model-optimization\/dm_tree-0.1.6-cp37-cp37m-manylinux_2_24_x86_64.whl\n# !pip install -q ..\/input\/tensorflow-model-optimization\/six-1.16.0-py2.py3-none-any.whl\n# !pip install -q ..\/input\/tensorflow-model-optimization\/tensorflow_model_optimization-0.7.0-py2.py3-none-any.whl\n# !pip install ..\/input\/neural-structued-learning\/neural_structured_learning-1.3.1-py2.py3-none-any.whl\n\nprint(\"... PIP\/APT INSTALLS COMPLETE ...\\n\")\n\nprint(\"\\n\\tVERSION INFORMATION\")\n# Machine Learning and Data Science Imports\nimport tensorflow as tf; print(f\"\\t\\t\u2013 TENSORFLOW VERSION: {tf.__version__}\");\nimport tensorflow_addons as tfa; print(f\"\\t\\t\u2013 TENSORFLOW ADDONS VERSION: {tfa.__version__}\");\nimport pandas as pd; pd.options.mode.chained_assignment = None;\nimport numpy as np; print(f\"\\t\\t\u2013 NUMPY VERSION: {np.__version__}\");\nimport sklearn; print(f\"\\t\\t\u2013 SKLEARN VERSION: {sklearn.__version__}\");\nfrom sklearn.preprocessing import RobustScaler, PolynomialFeatures\nfrom pandarallel import pandarallel; pandarallel.initialize();\nfrom sklearn.model_selection import GroupKFold;\nimport segmentation_models as sm; sm.set_framework(\"tf.keras\");\nimport xrspatial.multispectral as ms\nimport xarray\n\n# # This is necessary to force the TPU client to have the same TF version as TF-Nightly\n# from cloud_tpu_client import Client\n# c = Client(tpu=''); c.configure_tpu_version(tf.__version__, restart_type='ifNeeded')\n\n# Built In Imports\nfrom kaggle_datasets import KaggleDatasets\nfrom collections import Counter\nfrom datetime import datetime\nfrom glob import glob\nimport warnings\nimport requests\nimport hashlib\nimport imageio\nimport IPython\nimport sklearn\nimport urllib\nimport zipfile\nimport pickle\nimport random\nimport shutil\nimport string\nimport json\nimport math\nimport time\nimport gzip\nimport ast\nimport sys\nimport io\nimport os\nimport gc\nimport re\n\n# Visualization Imports\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.patches as patches\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm; tqdm.pandas();\nimport plotly.express as px\nimport seaborn as sns\nfrom PIL import Image, ImageEnhance\nimport matplotlib; print(f\"\\t\\t\u2013 MATPLOTLIB VERSION: {matplotlib.__version__}\");\nfrom matplotlib import animation, rc; rc('animation', html='jshtml')\nimport plotly\nimport PIL\nimport cv2\n\ndef seed_it_all(seed=7):\n    \"\"\" Attempt to be Reproducible \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\n    \nprint(\"\\n\\n... IMPORTS COMPLETE ...\\n\")","f7abc024":"print(f\"\\n... ACCELERATOR SETUP STARTING ...\\n\")\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    TPU = tf.distribute.cluster_resolver.TPUClusterResolver()  \nexcept ValueError:\n    TPU = None\n\nif TPU:\n    print(f\"\\n... RUNNING ON TPU - {TPU.master()}...\")\n    tf.config.experimental_connect_to_cluster(TPU)\n    tf.tpu.experimental.initialize_tpu_system(TPU)\n    strategy = tf.distribute.experimental.TPUStrategy(TPU)\nelse:\n    print(f\"\\n... RUNNING ON CPU\/GPU ...\")\n    # Yield the default distribution strategy in Tensorflow\n    #   --> Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy() \n\n# What Is a Replica?\n#    --> A single Cloud TPU device consists of FOUR chips, each of which has TWO TPU cores. \n#    --> Therefore, for efficient utilization of Cloud TPU, a program should make use of each of the EIGHT (4x2) cores. \n#    --> Each replica is essentially a copy of the training graph that is run on each core and \n#        trains a mini-batch containing 1\/8th of the overall batch size\nN_REPLICAS = strategy.num_replicas_in_sync\n    \nprint(f\"... # OF REPLICAS: {N_REPLICAS} ...\\n\")\n\nprint(f\"\\n... ACCELERATOR SETUP COMPLTED ...\\n\")","902f9810":"print(\"\\n... DATA ACCESS SETUP STARTED ...\\n\")\n\nif TPU:\n    # Google Cloud Dataset path to training and validation images\n    TFRECORD_DIR = os.path.join(KaggleDatasets().get_gcs_path(\"cloud-cover-detection-tfrecords\"), \"tfrecords\")\n    save_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='\/job:localhost')\nelse:\n    # Local path to training and validation images\n    TFRECORD_DIR = os.path.join(\"\/kaggle\/input\/cloud-cover-detection-tfrecords\", \"tfrecords\")\n    save_locally = load_locally = None\nRAW_DATA_DIR = \"\/kaggle\/input\/anco-dataset\"\nNPY_DATA_DIR = \"\/kaggle\/input\/anco-npy-dataset\"    \nRAW_TRAIN_FEAT_DIR = os.path.join(RAW_DATA_DIR, \"train_features\")\nNPY_TRAIN_IMAGE_DIR = os.path.join(NPY_DATA_DIR, \"train_images\")\n\nRAW_TRAIN_LBL_DIR = os.path.join(RAW_DATA_DIR, \"train_labels\")\nNPY_TRAIN_MASK_DIR = os.path.join(NPY_DATA_DIR, \"train_masks\")\n\n# This CSV contains everything in the raw csv but with additional information\nTRAIN_CSV = os.path.join(NPY_DATA_DIR, \"train.csv\")\n\nprint(f\"\\n... RAW DATA DIRECTORY PATH IS:\\n\\t--> {RAW_DATA_DIR}\")\nprint(f\"\\n... NPY DATA DIRECTORY PATH IS:\\n\\t--> {NPY_DATA_DIR}\")\nprint(f\"\\n... TFRECORD DATA DIRECTORY PATH IS:\\n\\t--> {TFRECORD_DIR}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF RAW DATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(RAW_DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF NPY DATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(NPY_DATA_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(f\"\\n... IMMEDIATE CONTENTS OF TFRECORD DATA DIRECTORY IS:\")\nfor file in tf.io.gfile.glob(os.path.join(TFRECORD_DIR, \"*\")): print(f\"\\t--> {file}\")\n\nprint(\"\\n\\n... DATA ACCESS SETUP COMPLETED ...\\n\")","bb050c5f":"print(f\"\\n... XLA OPTIMIZATIONS STARTING ...\\n\")\n\nprint(f\"\\n... CONFIGURE JIT (JUST IN TIME) COMPILATION ...\\n\")\n# enable XLA optmizations (10% speedup when using @tf.function calls)\ntf.config.optimizer.set_jit(True)\n\nprint(f\"\\n... XLA OPTIMIZATIONS COMPLETED ...\\n\")","7df9a28a":"####################################################################\n# ####################     TEST API CODE      #################### #\n####################################################################\n# iter_test = env.iter_test()    # an iterator which loops over the test set and sample submission\n# for (pixel_array, sample_prediction_df) in iter_test:\n#     sample_prediction_df['annotations'] = 'abc,0.5 0 0 100 100'  # make your predictions here\n#     env.predict(sample_prediction_df)   # register your predictions\n####################################################################\n\nprint(\"\\n... BASIC DATA SETUP STARTING ...\\n\\n\")\n\n# Load the training dataframe\ntrain_df = pd.read_csv(TRAIN_CSV)\n\n# Helpful mappings\nCHANNEL_MAP = {0:\"Blue Visible Light\", 1:\"Green Visible Light\", 2:\"Red Visible Light\", 3:\"Near Infrared Light\"}\nLOC_TO_IDX = {loc:i for i, loc in enumerate(sorted(train_df.location.unique()))}\nIDX_TO_LOC = {v:k for k,v in LOC_TO_IDX.items()}\n\n# Location information\ntrain_df[\"loc_idx\"] = train_df[\"location\"].map(LOC_TO_IDX)\ntrain_df[\"loc_str\"] = train_df[\"location\"]\n\n# Local path information\ntrain_df[\"local_img_path\"] = train_df.chip_id.apply(lambda x: os.path.join(NPY_TRAIN_IMAGE_DIR, x+\".npy\"))\ntrain_df[\"local_msk_path\"] = train_df.chip_id.apply(lambda x: os.path.join(NPY_TRAIN_MASK_DIR, x+\".npy\"))\n\n# Date time information\ntrain_df[\"dt_full_datetime\"] = pd.to_datetime(train_df[\"datetime\"])\ntrain_df[\"dt_month_norm\"] = (train_df.dt_full_datetime.dt.month-1)\/(12-1) # divide by number of months in a year\ntrain_df[\"dt_week_norm\"] = (train_df.dt_full_datetime.dt.isocalendar().week-1)\/(52-1) # divide by number of weeks in a year\ntrain_df[\"dt_day_norm\"] = (train_df.dt_full_datetime.dt.dayofyear-1)\/(365-1) # divide by number of days in a year\ntrain_df[\"dt_second_norm\"] = (train_df.dt_full_datetime.dt.hour*3600+\n                              train_df.dt_full_datetime.dt.minute*60+\n                              train_df.dt_full_datetime.dt.second)\/(60*60*24)\n\n# Reorder columns\ntrain_df = train_df[[\"chip_id\", \"local_img_path\", \"local_msk_path\"]+\n                    [x for x in train_df.columns if \"dt\" in x]+\n                    [\"loc_idx\", \"loc_str\", \"pixel_count\",]+\n                    sorted([x for x in train_df.columns if \"B0\" in x])]\n\n# Define input shape and mask shape as well as identify the number of training examples\nINPUT_SHAPE = (512, 512, 4)\nMASK_SHAPE = (512,512)\nN_TRAIN = len(train_df)\n\nprint(\"\\n... TRAIN DATAFRAME ...\\n\")\ndisplay(train_df)\n\nprint(\"\\n\\n... IMAGE FEATURES ...\")\nprint(\"\\t--> 'local_img_path'\")\n\nprint(\"\\n... LABEL FEATURES ...\")\nprint(\"\\t--> 'local_msk_path'\")\n\nprint(\"\\n... STRUCTURED FEATURES ...\")\nprint(\"\\t--> 'location'\")\nprint(\"\\t--> 'pixel_count'\")\nprint(\"\\t--> 'B02_max', 'B03_max', 'B04_max', 'B08_max'\")\nprint(\"\\t--> 'B02_min', 'B03_min', 'B04_min', 'B08_min'\")\nprint(\"\\t--> 'dt_month_norm'\")\nprint(\"\\t--> 'dt_week_norm'\")\nprint(\"\\t--> 'dt_day_norm'\")\nprint(\"\\t--> 'dt_second_norm'\")\n\nprint(\"\\n\\n... BASIC DATA SETUP FINISHING ...\\n\")","b80a9829":"def flatten_l_o_l(nested_list):\n    \"\"\" Flatten a list of lists \"\"\"\n    return [item for sublist in nested_list for item in sublist]\n\ndef tf_load_img(img_path, _channels=4, _dtype=tf.uint8, reshape_to=None):\n    if reshape_to is None:\n        return tf.image.decode_image(tf.io.read_file(img_path), channels=_channels, dtype=_dtype)\n    else:\n        return tf.image.resize(tf.image.decode_image(tf.io.read_file(img_path), _channels=3), reshape_to)\n    \ndef plot_chip(chip_series, _cmap=\"magma\"):\n    \"\"\" Plot 5 images representing a chip from pd.Series\n    \n    Args:\n        chip_series (pd.Series): Pandas series with the required information\n        \n    Returns:\n        None; Will plot a strip including the 5 images\n    \"\"\"\n    tmp_img = np.load(chip_series.local_img_path)\n    tmp_msk = np.load(chip_series.local_msk_path)\n    \n    plt.figure(figsize=(20,10))\n    for i in range(tmp_img.shape[-1]):\n        plt.subplot(1,5,i+1)\n        plt.imshow(tmp_img[..., i], cmap=_cmap)\n        plt.axis(False)\n        plt.title(f\"{chip_series.chip_id.upper()} - {CHANNEL_MAP[i]}\\nLOCATION: {chip_series.loc_str}\", fontweight=\"bold\")\n    \n    plt.subplot(1,5,5)\n    plt.imshow(tmp_msk, cmap=_cmap, vmin=0, vmax=1)\n    plt.axis(False)\n    plt.title(f\"{chip_series.chip_id.upper()} - Cloud Mask\\nLOCATION: {chip_series.loc_str}\", fontweight=\"bold\")\n        \n    plt.tight_layout()\n    plt.show()","a9337be9":"print(\"\\n... TRAIN METADATA INVESTIGATION STARTING ...\\n\\n\")\n\nprint(\"\\n... TRAIN DATAFRAME HEAD(5) ...\\n\\n\")\ndisplay(train_df.head(5))\n\nprint(\"\\n\\n... TRAIN DATAFRAME TAIL(5) ...\\n\\n\")\ndisplay(train_df.tail(5))\n\nprint(\"\\n\\n... TRAIN DATAFRAME SAMPLE(5) ...\\n\\n\")\ndisplay(train_df.sample(5))\n\nprint(f\"\\n... NUMBER OF UNIQUE TRAINING IMAGES: {N_TRAIN} ...\")\nprint(f\"... INPUT SHAPE: {INPUT_SHAPE}\")\nprint(f\"... MASK SHAPE: {MASK_SHAPE}\")\n\nprint(\"\\n... TRAIN DATAFRAME PANDAS DESCRIPTION ...\\n\\n\")\ndisplay(train_df.describe().T)\n\nfig = px.histogram(train_df, x=\"loc_str\", title=\"Location Count Distribution\")\nfig.show()\n\nfig = px.histogram(train_df, x=\"pixel_count\", title=\"Pixel Count Distribution\")\nfig.show()\n\nfig = px.histogram(train_df, x=\"dt_month_norm\", title=\"Month Count Distribution\")\nfig.show()\n\nfig = px.histogram(train_df, x=\"dt_week_norm\", title=\"Week Count Distribution\")\nfig.show()\n\nfig = px.histogram(train_df, x=\"dt_day_norm\", title=\"Day Count Distribution\")\nfig.show()\n\n\nprint(\"\\n\\n... TRAIN METADATA INVESTIGATION FINISHING ...\\n\")","5106fd3b":"N_VIZ = 5\nprint(f\"\\n... VISUALIZE {N_VIZ} RANDOM CHIPS ...\\n\")\nfor j in range(N_VIZ):\n    train_sub_series = train_df.sample(1).reset_index(drop=True).iloc[0]\n    plot_chip(train_sub_series)","b0c5c2a1":"def _bytes_feature(value, is_list=False):\n    \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n    \n    if not is_list:\n        value = [value]\n    \n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\ndef _float_feature(value, is_list=False):\n    \"\"\"Returns a float_list from a float \/ double.\"\"\"\n        \n    if not is_list:\n        value = [value]\n        \n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\ndef _int64_feature(value, is_list=False):\n    \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n        \n    if not is_list:\n        value = [value]\n        \n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\ndef serialize_raw(example_data):\n    \"\"\"\n    Creates a tf.Example message ready to be written to a file features and mask.\n\n    Args:\n        example_data: Everything from pandas row\n            ... IMAGE FEATURES ...\n                --> 'local_img_path'\n            ... LABEL FEATURES ...\n                --> 'local_msk_path'\n            ... STRUCTURED FEATURES ...\n                --> 'location'\n                --> 'pixel_count'\n                --> 'B02_max', 'B03_max', 'B04_max', 'B08_max'\n                --> 'B02_min', 'B03_min', 'B04_min', 'B08_min'\n                --> 'dt_month_norm'\n                --> 'dt_week_norm'\n                --> 'dt_day_norm'\n                --> 'dt_second_norm'\n    \n    Returns:\n        A tf.Example Message ready to be written to file\n    \"\"\"\n    img_encoded = tf.io.encode_png(np.load(example_data[\"local_img_path\"]))\n    msk_encoded = tf.io.encode_png(tf.expand_dims(np.load(example_data[\"local_msk_path\"]), axis=-1))\n    \n    feat_chip_id        = example_data[\"chip_id\"].encode('utf8')\n    feat_loc_idx        = example_data[\"loc_idx\"]\n    feat_pixel_count    = example_data[\"pixel_count\"]\n    feat_b02_min        = example_data[\"B02_min\"]\n    feat_b02_max        = example_data[\"B02_max\"]\n    feat_b03_min        = example_data[\"B03_min\"]\n    feat_b03_max        = example_data[\"B03_max\"]\n    feat_b04_min        = example_data[\"B04_min\"]\n    feat_b04_max        = example_data[\"B04_max\"]\n    feat_b08_min        = example_data[\"B08_min\"]\n    feat_b08_max        = example_data[\"B08_max\"]\n    feat_dt_month_norm  = example_data[\"dt_month_norm\"]\n    feat_dt_week_norm   = example_data[\"dt_week_norm\"]\n    feat_dt_day_norm    = example_data[\"dt_day_norm\"]\n    feat_dt_second_norm = example_data[\"dt_second_norm\"]\n    \n    # Create a dictionary mapping the feature name to the \n    # tf.Example-compatible data type.\n    feature_dict = {\n        'img': _bytes_feature(img_encoded),\n        'msk': _bytes_feature(msk_encoded),\n        'feat_chip_id': _bytes_feature(feat_chip_id),\n        'feat_loc_idx': _int64_feature(feat_loc_idx),\n        'feat_pixel_count': _int64_feature(feat_pixel_count),\n        'feat_b02_min': _int64_feature(feat_b02_min),\n        'feat_b02_max': _int64_feature(feat_b02_max),\n        'feat_b03_min': _int64_feature(feat_b03_min),\n        'feat_b03_max': _int64_feature(feat_b03_max),\n        'feat_b04_min': _int64_feature(feat_b04_min),\n        'feat_b04_max': _int64_feature(feat_b04_max),\n        'feat_b08_min': _int64_feature(feat_b08_min),\n        'feat_b08_max': _int64_feature(feat_b08_max),\n        'feat_dt_month_norm': _float_feature(feat_dt_month_norm),\n        'feat_dt_week_norm': _float_feature(feat_dt_week_norm),\n        'feat_dt_day_norm': _float_feature(feat_dt_day_norm),\n        'feat_dt_second_norm': _float_feature(feat_dt_second_norm),\n    }\n       \n    # Create a Features message using tf.train.Example.\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n    return example_proto.SerializeToString()\n\n\ndef write_tfrecords(df, n_ex, n_ex_per_rec=80, serialize_fn=serialize_raw, out_dir=\"\/kaggle\/working\/tfrecords\", ds_type=\"train\"):\n    \"\"\" Function to create our tfrecords \n    \n    Args:\n        TBD\n    \n    Returns:\n        TBD\n    \"\"\"\n    \n    n_recs = int(np.ceil(n_ex\/n_ex_per_rec))\n    \n    # Make dataframe iterable\n    iter_df = df.iterrows()\n        \n    out_dir = os.path.join(out_dir, ds_type)\n    \n    # Create folder\n    if not os.path.isdir(out_dir):\n        os.makedirs(out_dir, exist_ok=True)\n        \n    # Create tfrecords\n    for i in tqdm(range(n_recs), total=n_recs):\n        print(f\"\\n... Writing {ds_type.title()} TFRecord {i+1} of {n_recs} ...\\n\")\n        tfrec_path = os.path.join(out_dir, f\"{ds_type}__{(i+1):02}_{n_recs:02}.tfrec\")\n        \n        # This makes the tfrecord\n        with tf.io.TFRecordWriter(tfrec_path) as writer:\n            for ex in tqdm(range(n_ex_per_rec), total=n_ex_per_rec):\n                try:\n                    example = serialize_fn(next(iter_df)[1])\n                    writer.write(example)\n                except:\n                    break\n                    \ndef decode_image(image_data, n_channels=4, decode_dtype=tf.uint16, reshape_to=(512,512)):\n    image = tf.image.decode_png(image_data, channels=n_channels, dtype=decode_dtype)\n    image = tf.reshape(image, reshape_to) \n    return image\n\ndef decode(serialized_example):\n    \"\"\" Parses a set of features and label from the given `serialized_example`.\n        \n        It is used as a map function for `dataset.map`\n\n    Args:\n        serialized_example (tf.Example): A serialized example\n        \n    Returns:\n        A decoded tf.data.Dataset object representing the tfrecord dataset\n    \"\"\"\n    \n    # Defaults are not specified since both keys are required.\n    feature_dict = {\n        'img': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n        'msk': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n        'feat_chip_id': tf.io.FixedLenFeature(shape=(), dtype=tf.string),\n        'feat_loc_idx': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n        'feat_pixel_count': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n        'feat_b02_min': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n        'feat_b02_max': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n        'feat_b03_min': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n        'feat_b03_max': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n        'feat_b04_min': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n        'feat_b04_max': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n        'feat_b08_min': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n        'feat_b08_max': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n        'feat_dt_month_norm': tf.io.FixedLenFeature(shape=(), dtype=tf.float32),\n        'feat_dt_week_norm': tf.io.FixedLenFeature(shape=(), dtype=tf.float32),\n        'feat_dt_day_norm': tf.io.FixedLenFeature(shape=(), dtype=tf.float32),\n        'feat_dt_second_norm': tf.io.FixedLenFeature(shape=(), dtype=tf.float32),\n    }\n    \n    # Define a parser\n    features = tf.io.parse_single_example(serialized_example, features=feature_dict)   \n    img = decode_image(features[\"img\"], n_channels=4, decode_dtype=tf.uint16, reshape_to=(512,512,4))\n    msk = decode_image(features[\"msk\"], n_channels=1, decode_dtype=tf.uint8, reshape_to=(512,512,1))\n    \n    return img, msk\n\ndef convert_to_float32(img_batch, msk_batch):\n    return img_batch\/65535, tf.cast(msk_batch, tf.float32)\n\ndef augment_fn(img_batch, msk_batch):\n    \"\"\" super simple augmentation fn (for now) \"\"\"\n    \n    combo_batch = tf.concat((img_batch, msk_batch), axis=-1)\n    combo_batch = tf.image.random_flip_left_right(combo_batch)\n    combo_batch = tf.image.random_flip_up_down(combo_batch)\n    \n    return combo_batch[..., :-1], combo_batch[..., -1:]","7ddd8ae6":"N_VAL = 1000\nN_EPOCHS=30\nN_TRAIN = N_TRAIN-N_VAL \nBATCH_SIZE= 8*N_REPLICAS\nSHUFFLE_BUFFER=5*BATCH_SIZE\nENCODER_BACKBONE = 'efficientnetb5'\nPREPROCESS_FN = sm.get_preprocessing(ENCODER_BACKBONE)\n\n# Create new tfrecords path\nif not tf.io.gfile.exists(TFRECORD_DIR):\n    # Create train\/val split\n    sub_train_df=train_df.sample(len(train_df)).reset_index(drop=True)\n    sub_val_df=sub_train_df[-N_VAL:].reset_index(drop=True)\n    sub_train_df=sub_train_df[:-N_VAL]\n\n    # Create train\/val tfrecords\n    write_tfrecords(df=sub_train_df,\n                    n_ex=N_TRAIN, ds_type=\"train\")\n    write_tfrecords(df=sub_val_df,\n                    n_ex=N_VAL, ds_type=\"val\")\n\n# Now that we have created tfrecords (or they already existed)\n# we can drill down to the train and validation tfrecords\nTRAIN_TFREC_PATHS = tf.io.gfile.glob(os.path.join(TFRECORD_DIR, \"train\/*.tfrec\"))\nVAL_TFREC_PATHS = tf.io.gfile.glob(os.path.join(TFRECORD_DIR, \"val\/*.tfrec\"))\n\n# Explore briefly\nprint(\"\\n... NUMBER OF TFRECORD FILES ...\\n\")\nprint(f\"\\t--> {len(TRAIN_TFREC_PATHS)} TRAIN TFRECORDS\")\nprint(f\"\\t--> {len(VAL_TFREC_PATHS)} VALIDATION TFRECORDS\")","a0e40df8":"train_ds = tf.data.TFRecordDataset(TRAIN_TFREC_PATHS, num_parallel_reads=tf.data.AUTOTUNE)\nval_ds = tf.data.TFRecordDataset(VAL_TFREC_PATHS, num_parallel_reads=tf.data.AUTOTUNE)\n\ntrain_ds = train_ds.map(decode)\ntrain_ds = train_ds.shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE, drop_remainder=True)\ntrain_ds = train_ds.map(convert_to_float32, num_parallel_calls=tf.data.AUTOTUNE)\ntrain_ds = train_ds.map(augment_fn, num_parallel_calls=tf.data.AUTOTUNE)\n# train_ds = train_ds.map(lambda x,y: (PREPROCESS_FN(x), y), num_parallel_calls=tf.data.AUTOTUNE)\ntrain_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n\nval_ds = val_ds.map(decode)\nval_ds = val_ds.shuffle(SHUFFLE_BUFFER).batch(BATCH_SIZE, drop_remainder=True)\nval_ds = val_ds.map(convert_to_float32, num_parallel_calls=tf.data.AUTOTUNE)\n# val_ds = val_ds.map(lambda x,y: (PREPROCESS_FN(x), y), num_parallel_calls=tf.data.AUTOTUNE)\nval_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n\nprint(\"\\nTRAIN DATASET :\\n\\t-->\", train_ds)\nprint(\"\\nVAL DATASET   :\\n\\t-->\", val_ds)","b573139e":"for tmp_img,tmp_msk in val_ds.unbatch().take(3):\n    plt.figure(figsize=(20,10))\n    for i in range(tmp_img.shape[-1]):\n        plt.subplot(1,5,i+1)\n        plt.imshow(tmp_img[..., i], cmap=\"magma\")\n        plt.axis(False)\n        plt.title(f\"{CHANNEL_MAP[i]}\", fontweight=\"bold\")\n\n    plt.subplot(1,5,5)\n    plt.imshow(tmp_msk[..., 0], cmap=\"magma\", vmin=0, vmax=1)\n    plt.axis(False)\n    plt.title(f\"Cloud Mask\", fontweight=\"bold\")\n\n    plt.tight_layout()\n    plt.show()\n    \n    # RGB Version of the image\n    rgb_tmp_img = tmp_img.numpy()[..., 2::-1]\n    rgb_tmp_img = (rgb_tmp_img - rgb_tmp_img.min())\/(rgb_tmp_img.max()-rgb_tmp_img.min())\n    plt.figure(figsize=(7,8))\n    plt.title(\"BGR-->RGB Channels, Rescaled and Displayed\", fontweight=\"bold\")\n    plt.imshow(rgb_tmp_img)\n    plt.axis(False)\n    plt.tight_layout()\n    plt.show()","40530651":"def true_color_img(bgr_img, return_as_npy=True):\n    \"\"\"Given the path to the directory of Sentinel-2 chip feature images,\n    plots the true color image\"\"\"\n    _red = xarray.DataArray(bgr_img[..., 2], dims=[\"y\", \"x\"])\n    _green = xarray.DataArray(bgr_img[..., 1], dims=[\"y\", \"x\"])\n    _blue = xarray.DataArray(bgr_img[..., 0], dims=[\"y\", \"x\"])\n    rgb_img = ms.true_color(r=_red, g=_green, b=_blue)[..., :-1]\n    \n    if return_as_npy:\n        rgb_img = np.asarray(rgb_img)\n        \n    return rgb_img\n    \n\ndef convert_3_to_4_channel(model):\n    _inputs = tf.keras.layers.Input(shape=INPUT_SHAPE)\n    x = tf.keras.layers.Conv2D(3, 1, 1, padding=\"same\")(_inputs)\n    _outputs = model(x)\n    _model = tf.keras.Model(_inputs, _outputs)\n    return _model\n\nLOAD_MODEL = True\nLOAD_CKPT_DIR = \"\/kaggle\/input\/on-cloud-n-cloud-cover-detection-challenge\/model_wts\/epoch_20__loss_0.407\"\nMODEL_CKPT_DIR = \"\/kaggle\/working\/model_wts\"\nif not os.path.isdir(MODEL_CKPT_DIR): os.makedirs(MODEL_CKPT_DIR, exist_ok=True)","cc486425":"if LOAD_MODEL:\n    with strategy.scope():\n        model = tf.keras.models.load_model(LOAD_CKPT_DIR, options=load_locally, compile=False)\n        model.compile(tf.keras.optimizers.Adam(0.0075), loss=sm.losses.bce_jaccard_loss, metrics=[sm.metrics.iou_score,])\n        print(model.summary())\n        _history=None\n        \nelse:\n    with strategy.scope():\n        # define model\n        model = sm.Unet(ENCODER_BACKBONE, encoder_weights='imagenet', input_shape=(*INPUT_SHAPE[:2],3))\n        model = convert_3_to_4_channel(model)\n        radam = tfa.optimizers.RectifiedAdam(lr=1e-3, total_steps=(N_EPOCHS*N_TRAIN)\/\/BATCH_SIZE,\n                                             warmup_proportion=0.075, min_lr=1e-5,\n        )\n        ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n        model.compile(tf.keras.optimizers.Adam(0.0075), loss=sm.losses.bce_jaccard_loss, metrics=[sm.metrics.iou_score,])\n        print(model.summary())\n\n        cb_list = [\n            tf.keras.callbacks.ModelCheckpoint(\n                filepath=os.path.join(MODEL_CKPT_DIR, \"epoch_{epoch:02d}__loss_{val_loss:.3f}\"),\n                monitor='val_loss', save_best_only=True,\n                options=save_locally),\n        ]\n\n    # fit model\n    _history = model.fit(train_ds, batch_size=BATCH_SIZE, epochs=N_EPOCHS, validation_data=val_ds, callbacks=cb_list)","3e7f8d5e":"MINIBATCH_SIZE = 16\nTO_TAKE = 1\nfor img_batch, msk_batch in val_ds.unbatch().batch(MINIBATCH_SIZE).take(TO_TAKE):\n    pred_batch = model.predict(img_batch)\n    metrics = model.evaluate(x=img_batch, y=msk_batch, verbose=0)\n    print(\"\\n... METRICS ...\")\n    print(f\"\\t--> bce_jaccard_loss : {metrics[0]:.4f}\")\n    print(f\"\\t--> iou_score        : {metrics[1]:.4f}\\n\")\n    \n    print(\"\\n... PLOTTING EACH IMAGE! ...\\n\")\n    for i, (img, msk, pred) in enumerate(zip(img_batch, msk_batch, pred_batch)):\n        \n        # Get true colour image and overlay for display purposes\n        tc_img = true_color_img(img.numpy())\n        merge_msk = np.zeros_like(tc_img)\n        merge_msk[..., 0] = (msk.numpy()*255).astype(np.uint8)[..., 0]\n        tc_overlay = cv2.addWeighted(tc_img, 0.7, merge_msk, 0.5, 0.0)\n        \n        pred_msk = np.zeros_like(tc_img)\n        pred_msk[..., 0] = ((pred**0.25)*255).astype(np.uint8)[..., 0]\n        pred_overlay = cv2.addWeighted(tc_img, 0.7, pred_msk, 0.5, 0.0)\n        \n        # For each image we want to display\n        #  1. True Colour Image\n        #  2. Ground Truth Overlay Mask\n        #  3. Ground Truth Segmentation Mask\n        #  4. Model Predicted Segmentation Mask\n        #  5. Model Predicted Segmentation Overlay Mask?\n        plt.figure(figsize=(20,10))    \n        \n        plt.subplot(1,5,1)\n        plt.imshow(tc_img)\n        plt.axis(False)\n        plt.title(\"True Colour Image\", fontweight=\"bold\")\n        \n        plt.subplot(1,5,2)\n        plt.imshow(tc_overlay)\n        plt.axis(False)\n        plt.title(\"Ground Truth Overlay Mask\", fontweight=\"bold\")\n        \n        plt.subplot(1,5,3)\n        plt.imshow(msk[..., -1], vmin=0, vmax=1)\n        plt.axis(False)\n        plt.title(\"Ground Truth Segmentation Mask\", fontweight=\"bold\")\n        \n        plt.subplot(1,5,4)\n        plt.imshow(pred[..., -1]**0.25, )\n        plt.axis(False)\n        plt.title(\"Predicted Segmentation Mask\", fontweight=\"bold\")\n        \n        plt.subplot(1,5,5)\n        plt.imshow(pred_overlay)\n        plt.axis(False)\n        plt.title(\"Prediction Overlay Mask\", fontweight=\"bold\")\n        \n        plt.tight_layout()\n        plt.show()\n        ","a8f74bf6":"!cp -r \/kaggle\/input\/on-cloud-n-cloud-cover-detection-challenge\/model_wts\/epoch_20* \/kaggle\/working\/model_wts","b78e730e":"<br>\n\n\n<a id=\"helper_functions\"><\/a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #40E0D0; background-color: #ffffff;\" id=\"helper_functions\">\n    3&nbsp;&nbsp;HELPER FUNCTION & CLASSES&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a>\n<\/h1>\n\n---","036d20fb":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">2.2 COMPETITION DATA ACCESS<\/h3>\n\n---\n\nTPUs read data must be read directly from **G**oogle **C**loud **S**torage **(GCS)**. Kaggle provides a utility library \u2013\u00a0**`KaggleDatasets`** \u2013 which has a utility function **`.get_gcs_path`** that will allow us to access the location of our input datasets within **GCS**.<br><br>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">\ud83d\udccc &nbsp; TIPS:<\/b><br><br>- If you have multiple datasets attached to the notebook, you should pass the name of a specific dataset to the <b><code>`get_gcs_path()`<\/code><\/b> function. <i>In our case, the name of the dataset is the name of the directory the dataset is mounted within.<\/i><br><br>\n<\/div>","b9ee9190":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">2.3 LEVERAGING XLA OPTIMIZATIONS<\/h3>\n\n---\n\n\n**XLA** (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes. **The results are improvements in speed and memory usage**.\n\n<br>\n\nWhen a TensorFlow program is run, all of the operations are executed individually by the TensorFlow executor. Each TensorFlow operation has a precompiled GPU\/TPU kernel implementation that the executor dispatches to.\n\nXLA provides us with an alternative mode of running models: it compiles the TensorFlow graph into a sequence of computation kernels generated specifically for the given model. Because these kernels are unique to the model, they can exploit model-specific information for optimization.<br><br>\n\n<div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">\ud83d\uded1 &nbsp; WARNING:<\/b><br><br>- XLA can not currently compile functions where dimensions are not inferrable: that is, if it's not possible to infer the dimensions of all tensors without running the entire computation<br>\n<\/div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">\ud83d\udccc &nbsp; NOTE:<\/b><br><br>- XLA compilation is only applied to code that is compiled into a graph (in <b>TF2<\/b> that's only a code inside <b><code>tf.function<\/code><\/b>).<br>- The <b><code>jit_compile<\/code><\/b> API has must-compile semantics, i.e. either the entire function is compiled with XLA, or an <b><code>errors.InvalidArgumentError<\/code><\/b> exception is thrown)\n<\/div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">\ud83d\udcd6 &nbsp; REFERENCE:<\/b><br><br>    - <a href=\"https:\/\/www.tensorflow.org\/xla\"><b>XLA: Optimizing Compiler for Machine Learning<\/b><\/a><br>\n<\/div>","e4e6be19":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">5.2 INSTANTIATE OUR TF.DATA.DATASETS<\/h3>\n\n---\n\nWe stick with simple augmentation for now.","1005d3b9":"<br>\n\n\n<a id=\"create_dataset\"><\/a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #40E0D0; background-color: #ffffff;\" id=\"create_dataset\">\n    4&nbsp;&nbsp;DATASET EXPLORATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a>\n<\/h1>\n\n---","07f38851":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">4.1 TRAIN METADATA<\/h3>\n\n---\n","6d7584be":"<br>\n\n<a id=\"background_information\"><\/a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #40E0D0; background-color: #ffffff;\" id=\"background_information\">1&nbsp;&nbsp;BACKGROUND INFORMATION&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a><\/h1>\n\n---\n","3ec6b442":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">2.1 ACCELERATOR DETECTION<\/h3>\n\n---\n\nIn order to use **`TPU`**, we use **`TPUClusterResolver`** for the initialization which is necessary to connect to the remote cluster and initialize cloud TPUs. Let's go over two important points\n\n1. When using TPU on Kaggle, you don't need to specify arguments for **`TPUClusterResolver`**\n2. However, on **G**oogle **C**ompute **E**ngine (**GCE**), you will need to do the following:\n\n<br>\n\n```python\n# The name you gave to the TPU to use\nTPU_WORKER = 'my-tpu-name'\n\n# or you can also specify the grpc path directly\n# TPU_WORKER = 'grpc:\/\/xxx.xxx.xxx.xxx:8470'\n\n# The zone you chose when you created the TPU to use on GCP.\nZONE = 'us-east1-b'\n\n# The name of the GCP project where you created the TPU to use on GCP.\nPROJECT = 'my-tpu-project'\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_WORKER, zone=ZONE, project=PROJECT)\n```\n\n<div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">\ud83d\uded1 &nbsp; WARNING:<\/b><br><br>- Although the Tensorflow documentation says it is the <b>project name<\/b> that should be provided for the argument <b><code>`project`<\/code><\/b>, it is actually the <b>Project ID<\/b>, that you should provide. This can be found on the GCP project dashboard page.<br>\n<\/div>\n\n<div class=\"alert alert-block alert-info\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 16px;\">\ud83d\udcd6 &nbsp; REFERENCES:<\/b><br><br>\n    - <a href=\"https:\/\/www.tensorflow.org\/guide\/tpu#tpu_initialization\"><b>Guide - Use TPUs<\/b><\/a><br>\n    - <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/distribute\/cluster_resolver\/TPUClusterResolver\"><b>Doc - TPUClusterResolver<\/b><\/a><br>\n\n<\/div>","e1356c1d":"<br>\n\n<a id=\"background_information\"><\/a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #40E0D0; background-color: #ffffff;\" id=\"setup\">2&nbsp;&nbsp;SETUP&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a><\/h1>\n\n---\n","5f1d3564":"<br>\n\n<a id=\"imports\"><\/a>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; background-color: #ffffff; color: #40E0D0;\" id=\"imports\">0&nbsp;&nbsp;IMPORTS&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a><\/h1>","925a00a6":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">4.2 TRAIN IMAGES<\/h3>\n\n---\n","91ad2ad0":"<br>\n\n\n<a id=\"modelling\"><\/a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #40E0D0; background-color: #ffffff;\" id=\"modelling\">\n    6&nbsp;&nbsp;MODELLING&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a>\n<\/h1>\n\n---","629f68e7":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">6.3 VISUALIZE THE MODEL PERFORMANCE<\/h3>\n\n---","5d5e0222":"<br>\n\n\n<a id=\"create_load_tfrecord_dataset\"><\/a>\n\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #40E0D0; background-color: #ffffff;\" id=\"create_dataset\">\n    5&nbsp;&nbsp;CREATE\/LOAD TFRECORD DATASET&nbsp;&nbsp;&nbsp;&nbsp;<a href=\"#toc\">&#10514;<\/a>\n<\/h1>\n\n---","0091ab48":"<br>\n\n<center><img src=\"https:\/\/static.eos.com\/wp-content\/uploads\/2021\/03\/satellite-view-with-clouds.jpg\" style=\"opacity: 0.85; filter: alpha(opacity=85); border-radius: 20%;\" width=110%><\/center>\n\n<h2 style=\"text-align: center; font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: underline; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">\u2601\ufe0f On Cloud N: Cloud Cover Detection Challenge \u2601\ufe0f<\/h2>\n<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">CREATED BY: DARIEN SCHETTLER<\/h5>\n\n<br>\n\n---\n\n<br>\n\n<center><div class=\"alert alert-block alert-danger\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">\ud83d\uded1 &nbsp; WARNING:<\/b><br><br><b>THIS IS A WORK IN PROGRESS<\/b><br>\n<\/div><\/center>\n\n\n<center><div class=\"alert alert-block alert-warning\" style=\"margin: 2em; line-height: 1.7em; font-family: Verdana;\">\n    <b style=\"font-size: 18px;\">\ud83d\udc4f &nbsp; IF YOU FORK THIS OR FIND THIS HELPFUL &nbsp; \ud83d\udc4f<\/b><br><br><b style=\"font-size: 22px; color: darkorange\">PLEASE UPVOTE!<\/b><br><br>This was a lot of work for me and while it may seem silly, it makes me feel appreciated when others like my work. \ud83d\ude05\n<\/div><\/center>\n\n\n","1f17691c":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">5.0 DEFINE USEFUL TFRECORD & TF.DATA FUNCTIONS<\/h3>\n\n---\n","dc7eba52":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">6.2 LOAD\/TRAIN THE MODEL<\/h3>\n\n---","04641e95":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">5.1 DEFINE CONSTANTS, HYPERPARAMETERS AND PATHS<\/h3>\n\n---\n\nThis step will either create new tfrecords or point to the existing tfrecords","83d278c9":"<p id=\"toc\"><\/p>\n\n<br><br>\n\n<h1 style=\"font-family: Verdana; font-size: 24px; font-style: normal; font-weight: bold; text-decoration: none; text-transform: none; letter-spacing: 3px; color: #40E0D0; background-color: #ffffff;\">TABLE OF CONTENTS<\/h1>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#imports\">0&nbsp;&nbsp;&nbsp;&nbsp;IMPORTS<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#background_information\">1&nbsp;&nbsp;&nbsp;&nbsp;BACKGROUND INFORMATION<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#setup\">2&nbsp;&nbsp;&nbsp;&nbsp;SETUP<\/a><\/h3>\n\n---\n\n<h3 style=\"text-indent: 10vw; font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: navy; background-color: #ffffff;\"><a href=\"#helper_functions\">3&nbsp;&nbsp;&nbsp;&nbsp;HELPER FUNCTIONS<\/a><\/h3>\n\n---","92b32641":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">6.1 CREATE USEFUL FUNCTIONS AND DEFINE CONSTANTS<\/h3>\n\n---","710638a5":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">5.3 VISUALIZE THE VAL DATASET<\/h3>\n\n---","e2705909":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">2.4 BASIC DATA DEFINITIONS & INITIALIZATIONS<\/h3>\n\n---\n","2a448eae":"<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">1.0 EXTRA BACKGROUND INFORMATION TOC<\/h3>\n\n---\n\n<strong>Features<\/strong>\n<ul>\n    <li><a href=\"#images\">Images<\/a><\/li>\n    <li><a href=\"#metadata\">Metadata<\/a><\/li>\n    <li><a href=\"#feature-data-example\">Example<\/a><\/li>\n    <li><a href=\"#additional-data\">Additional data<\/a><\/li>\n<\/ul>\n\n<strong>Labels<\/strong>\n<ul>\n    <li><a href=\"#labels\">Label format<\/a><\/li>\n    <li><a href=\"#label-example\">Example<\/a><\/li>\n<\/ul>\n\n<strong>Submission<\/strong>\n<ul>\n    <li><a href=\"#performance-metric\">Performance metric<\/a><\/li>\n    <li><a href=\"#submission-format\">Submission format<\/a><\/li>\n    <li><a href=\"#prediction-format-ex\">Example<\/a><\/li>\n<\/ul>\n\n---\n\n<br>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">1.1 BASIC COMPETITION INFORMATION<\/h3>\n\n---\n\n<br><b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">PRIMARY TASK DESCRIPTION<\/b>\n\nIn this challenge, your goal is to label clouds in satellite imagery. In many uses of satellite imagery, clouds obscure what we really care about - for example, tracking wildfires, mapping deforestation, or visualizing crop health. Being able to more accurately remove clouds from satellite images filters out interference, unlocking the potential of a vast range of use cases.\n\nThe challenge uses publicly available satellite data from the [**Sentinel-2**](https:\/\/sentinel.esa.int\/web\/sentinel\/missions\/sentinel-2) mission, which captures wide-swath, high-resolution, multi-spectral imaging. Data is publicly shared through [**Microsoft's Planetary Computer**](https:\/\/planetarycomputer.microsoft.com\/).\n>\n<br>\n\n<b style=\"text-decoration: underline; font-family: Verdana; text-transform: uppercase;\">Overview of the data provided for this competition<\/b>\n\n<pre><code><b>\/kaggle\/input\/[anco-dataset|anco-npy-dataset]<\/b>\n        \u2502\n        \u2502\n        \u251c\u2500\u2500 train_features\n        \u2502    \u2502      \n        \u2502    \u2514\u2500\u2500 ...\n        \u2502\n        \u251c\u2500\u2500 train_labels\n        \u2502    \u2502\n        \u2502    \u2514\u2500\u2500 ...\n        \u2502\n        \u2514\u2500\u2500 train_metadata.csv\n<\/code><\/pre>\n\n---\n\n<br><p><a id=\"feature-data\"><\/a><\/p>\n<h2 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">1.2 FEATURE DATA<\/h2>\n\n---\n\n<br><p>The dataset consists of Sentinel-2 satellite imagery stored as GeoTiffs. There are almost 12,000 chips in the training data, collected between 2018 and 2020. Each chip is imagery of a specific area captured at a specific point in time. <\/p>\n\n<p><b><a href=\"https:\/\/sentinel.esa.int\/web\/sentinel\/missions\/sentinel-2\/observation-scenario\">Sentinel-2<\/a><\/b> flies over the part of the Earth between 56\u00b0 South (Cape Horn, South America) and 82.8\u00b0 North (above Greenland), so our observations are all between these two latitudes. The chips are mostly from Africa, South America, and Australia. For more background about how data from Sentinel-2 is collected, see the <b><a href=\"https:\/\/www.drivendata.org\/competitions\/83\/cloud-cover\/page\/397\/\">About page<\/a><\/b>.<\/p>\n\n---\n\n<br><p><a id=\"images\"><\/a><\/p>\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">1.3 IMAGES<\/h3>\n\n---\n\n<br><p>The main features in this challenge are the satellite images themselves. There are four images associated with each chip. Each image within a chip captures light from a different range of wavelengths, or \"band\". For example, the B02 band for each chip shows the strengh of visible blue light, which has a wavelength around 492 nanometers (nm). The bands provided are:<\/p>\n\n<table border=\"1\" class=\"table\" style=\"width:90%; margin-left:auto; margin-right:auto\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Band<\/th>\n      <th>Description<\/th>\n      <th>Center wavelength<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <td>B02<\/td>\n      <td>Blue visible light<\/td>\n      <td>497 nm<\/td>\n    <\/tr>\n    <tr>\n      <td>B03<\/td>\n      <td>Green visible light<\/td>\n      <td>560 nm<\/td>\n    <\/tr>\n    <tr>\n      <td>B04<\/td>\n      <td>Red visible light<\/td>\n      <td>665 nm<\/td>\n    <\/tr>\n    <tr>\n      <td>B08<\/td>\n      <td>Near infrared light<\/td>\n      <td>835 nm<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n\n<p>Each band image is provided as a 512 x 512 GeoTIFF. The resolution, or real-world distance between pixels, is 10m. All four bands for a given chip cover the exact same area.<\/p>\n\n<p>The <a href=\"https:\/\/www.drivendata.org\/competitions\/83\/cloud-cover\/data\/\">data download page<\/a> includes instructions for how to download the training images. Within the folder <code>train_features<\/code>, there is a folder for every chip. <\/p>\n\n<pre><code><b>...\/train_features<\/b>\n        \u2502\n        \u2502\n        \u251c\u2500\u2500 <b>adwp<\/b> # chip with id adwp        \n        \u251c\u2500\u2500  \u251c\u2500\u2500B02.tif\n        \u251c\u2500\u2500  \u251c\u2500\u2500B03.tif\n        \u251c\u2500\u2500  \u251c\u2500\u2500B04.tif\n        \u251c\u2500\u2500  \u2514\u2500\u2500B08.tif\n        \u2502        \n        \u251c\u2500\u2500 <b>adwu<\/b>\n        \u251c\u2500\u2500  \u251c\u2500\u2500B02.tif\n        \u251c\u2500\u2500  \u251c\u2500\u2500B03.tif\n        \u251c\u2500\u2500  \u251c\u2500\u2500B04.tif\n        \u251c\u2500\u2500  \u2514\u2500\u2500B08.tif\n        \u2502\n        \u2514\u2500\u2500 ...\n<\/code><\/pre>\n\n<p>For example, the red visible light band of a chip with ID <code>abcd<\/code> in the training data would be saved as <code>train_features\/abcd\/B04.tif<\/code>.<\/p>\n\nEach GeoTIFF contains a set of metadata including:\n* bounding coordinates\n* an affine transform (and its coordinate reference system (CRS) projection). In Python, you can access geospatial raster data and extract this metadata using the <b><a href=\"https:\/\/rasterio.readthedocs.io\/en\/latest\/\">rasterio package<\/a><\/b>\n\n<pre><code class=\"python\">with rasterio.open(&quot;train_features\/cjge\/B04.tif&quot;) as f:\n    meta = f.meta\n    bounds = f.bounds\nprint(&quot;Meta:&quot;, meta, &quot;\\nBounds:&quot;, bounds)\n<\/code><\/pre>\n\n<pre><code>Meta: \n{'driver': 'GTiff',\n  'dtype': 'uint16',\n  'nodata': 0.0,\n  'width': 512,\n  'height': 512,\n  'count': 1,\n  'crs': CRS.from_epsg(32630),\n  'transform': Affine(10.0, 0.0, 579060.0,\n         0.0, -10.0, 3568130.0)}\n\nBoundingBox(left=579060.0, bottom=3563010.0, right=584180.0, top=3568130.0))\n<\/code><\/pre>\n\n<p>For an example of how to manipulate GeoTIFF metadata, see the benchmark <a href=\"https:\/\/www.drivendata.co\/blog\/cloud-cover-benchmark\/\">blog post<\/a>.<\/p>\n\n---\n\n<br><p><a id=\"metadata\"><\/a><\/p>\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">1.4 METADATA<\/h3>\n\n---\n\n<br><p>Metadata is also included as a CSV, <code>train_metadata.csv<\/code>. It contains the following columns:<\/p>\n<ul>\n<li><code>chip_id (string)<\/code>: A unique identifier for each chip. There is one row per chip in each of the metadata files<\/li>\n<li><code>location (string)<\/code>: General location of the chip, either country (eg. Eswatini), metropolitan region (eg. Lusaka), or country sub-region (eg. Australia - Central)<\/li>\n<li><code>datetime (datetime64[ns, UTC])<\/code>: Date and time that the images in the chip were captured. These will be loaded as strings with the format <code>%Y-%m-%dT%H:%M:%SZ<\/code> (using standard <a href=\"https:\/\/docs.python.org\/3\/library\/datetime.html#strftime-and-strptime-format-codes\">string format codes<\/a>). <code>Z<\/code> indicates that timestamps are in coordinated universal time (UTC)<\/li>\n<li><code>cloudpath (string)<\/code>: The path to download the folder of chip images from the Azure Blob Storage container. For a step-by-step guide, see <code>data_download_instructions.txt<\/code> on the <a href=\"https:\/\/www.drivendata.org\/competitions\/83\/cloud-cover\/data\/\">data download page<\/a>.<\/li>\n<\/ul>\n<p>The first few rows of <code>train_metadata.csv<\/code> are:<\/p>\n<table class=\"table\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>chip_id<\/th>\n      <th>location<\/th>\n      <th>datetime<\/th>\n      <th>cloudpath<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>adwp<\/td>\n      <td>Chifunfu<\/td>\n      <td>2020-04-29T08:20:47Z<\/td>\n      <td>az:\/\/.\/train_features\/adwp<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>adwu<\/td>\n      <td>Chifunfu<\/td>\n      <td>2020-04-29T08:20:47Z<\/td>\n      <td>az:\/\/.\/train_features\/adwu<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>adwz<\/td>\n      <td>Chifunfu<\/td>\n      <td>2020-04-29T08:20:47Z<\/td>\n      <td>az:\/\/.\/train_features\/adwz<\/td>\n    <\/tr>\n    <tr>\n      <th>3<\/th>\n      <td>adxp<\/td>\n      <td>Chifunfu<\/td>\n      <td>2020-04-29T08:20:47Z<\/td>\n      <td>az:\/\/.\/train_features\/adxp<\/td>\n    <\/tr>\n    <tr>\n      <th>4<\/th>\n      <td>aeaj<\/td>\n      <td>Chifunfu<\/td>\n      <td>2020-04-29T08:20:47Z<\/td>\n      <td>az:\/\/.\/train_features\/aeaj<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n\n<br><p><b>Chifunfu is an area of Tanzania!<\/b> \n* There are up to 400 chips for each location in the data. \n* Each location is either entirely in the train set or the test set, so all of the settings in the test set will be entirely new.<\/p>\n\n---\n\n<br><p><a id=\"feature-data-example\"><\/a><\/p>\n<div class=\"well\">\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">1.5 FEATURE DATA EXAMPLE<\/h3>\n\n---\n\n<br><b>Feature information for the chip in the training set with ID <code>cjge<\/code> (taken from Bechar, Algeria)<\/b><br><br>\n<b>Metadata<\/b>\n<table style=\"width:70%; margin-left:15%; margin-right:15%;\" class=\"table\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>chip_id<\/th>\n      <th>location<\/th>\n      <th>datetime<\/th>\n      <th>cloudpath<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>cjge<\/th>\n      <td>Bechar<\/td>\n      <td>2019-11-12T11:02:20Z<\/td>\n      <td>az:\/\/.\/train_features\/cjge<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n\n<b>Feature images<\/b>\n<table style=\"width:70%; margin-left:15%; margin-right:15%;\" class=\"table\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>band<\/th>\n      <th>description<\/th>\n      <th>filepath<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <td>B02<\/td>\n      <td>Blue visible light<\/td>\n      <td>train_features\/cjge\/B02.tif<\/td>\n    <\/tr>\n    <tr>\n      <td>B03<\/td>\n      <td>Green visible light<\/td>\n      <td>train_features\/cjge\/B03.tif<\/td>\n    <\/tr>\n    <tr>\n      <td>B04<\/td>\n      <td>Red visible light<\/td>\n      <td>train_features\/cjge\/B04.tif<\/td>\n    <\/tr>\n    <tr>\n      <td>B08<\/td>\n      <td>Near infrared light<\/td>\n      <td>train_features\/cjge\/B08.tif<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table><br>\n\n---\n    \n<br><code>B02.tif<\/code> (blue visible light band)<br>\n\n<img src=\"https:\/\/drivendata-public-assets.s3.amazonaws.com\/clouds-example-B02.png\" style=\"height: 250px\"><br><br>\n\n```python\narray([[1102, 1250, 1324, ..., 2928, 2902, 2802],\n       [1160, 1252, 1326, ..., 2854, 2844, 2754],\n       [1270, 1338, 1348, ..., 2732, 2800, 2814],\n       ...,\n       [1086, 1100, 1072, ..., 1200, 1188, 1076],\n       [1118, 1118, 1082, ..., 1168, 1190, 1140],\n       [1158, 1140, 1086, ..., 1222, 1208, 1148]], dtype=uint16)\n```\n\n<br>\n    \n---\n\n<br><code>B04.tif<\/code> (red visible light band)<br>\n<img alt=\"An example TIF image for the B04 band, which shows visible red light, that has clouds in the foreground\" src=\"https:\/\/drivendata-public-assets.s3.amazonaws.com\/clouds-example-B04.png\" style=\"height: 250px\"><br><br>\n    \n```python\narray([[3134, 3350, 3492, ..., 4074, 4036, 3978],\n       [3190, 3386, 3466, ..., 4064, 4040, 3996],\n       [3232, 3420, 3488, ..., 4016, 4016, 4016],\n       ...,\n       [2714, 2678, 2654, ..., 2942, 2968, 2822],\n       [2778, 2732, 2688, ..., 2930, 2996, 2822],\n       [2812, 2724, 2654, ..., 2924, 2958, 2886]], dtype=uint16)\n```\n\n<br><br>\n    \n* Each TIF is a single-band image. \n* The shape of each image array is (512, 512).\n<\/div>\n\n---\n\n<br><p><a id=\"additional-data\"><\/a><\/p>\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">1.6 ADDITIONAL DATA<\/h3>\n\n---\n\n<br>\n\n<p>While four bands per chip are included in the competition data, the publicly available Sentinal-2 dataset includes up to <a href=\"https:\/\/docs.sentinel-hub.com\/api\/latest\/data\/sentinel-2-l2a\/#available-bands-and-data\">10 bands<\/a> capturing different wavelengths. <strong>You may pull in any other information from the Planetary Computer to supplement the provided data.<\/strong> We recommend using the <a href=\"https:\/\/planetarycomputer.microsoft.com\/docs\/quickstarts\/reading-stac\/\">Planetary Computer STAC API<\/a> to access any additional data. Access to the Planetary Computer will be allowed during inference in the <a href=\"https:\/\/www.drivendata.org\/competitions\/83\/cloud-cover\/page\/412\/\">code execution environment<\/a>. To find additional bands for a given chip, search the Planetary Computer Hub based on both the geographic coordinates and the timestamp.<\/p>\n<p>The competition dataset includes the four bands with a resolution of 10m, and is exclusively from the L2A dataset. Some chips may not have every single band available. <\/p>\n<div class=\"alert alert-info\">\n<strong>Note on external data:<\/strong> External data that is not from the Planetary Computer is not allowed in this competition. Participants can use pre-trained computer vision models as long as they were available freely and openly in that form at the start of the competition.\n<\/div>\n\n---\n\n<br><p><a id=\"labels\"><\/a><\/p>\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">1.7 LABELS<\/h3>\n\n---\n\n<p>The labels for the competition are 512 x 512 GeoTIFFs with pixels indicating cloud (<code>1<\/code>) \/ no cloud (<code>0<\/code>) for each chip. Each label is saved as <code>&lt;chip_id&gt;.tif<\/code> in the <code>train_labels<\/code> folder. For example, listing the first few tiles in <code>train_labels<\/code> in your terminal would give:<\/p>\n<pre><code class=\"console\">\nls train_labels | head -n 5\nadwp.tif\nadwu.tif\nadwz.tif\nadxp.tif\naeaj.tif \n<\/code><\/pre>\n\n<p>Like the feature images, label GeoTIFFs contain additional metadata including bounding coordinates that can be accessed with <a href=\"https:\/\/rasterio.readthedocs.io\/en\/latest\/\">rasterio<\/a>. Any missing values in the labels have been converted to <code>0<\/code>, or no cloud, during validation. None of the cloud cover masks in the competition dataset are publicly available in the Planetary Computer.<\/p>\n\n---\n\n<br><p><a id=\"label-example\"><\/a><\/p>\n<div class=\"well\">\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">1.8 LABEL EXAMPLE<\/h3>\n\n---\n\n<br><b><code>cjge.tif<\/code><\/b>\n<br\/>\n<img alt=\"An example TIF image with the cloud cover ground truth label. About half of the image is covered in clouds.\" src=\"https:\/\/drivendata-public-assets.s3.amazonaws.com\/clouds-example-labels.png\" style=\"height: 250px\"><br>\n<pre><code>array([[0, 0, 0, ..., 1, 1, 1],\n       [0, 0, 0, ..., 1, 1, 1],\n       [0, 0, 0, ..., 1, 1, 1],\n       ...,\n       [1, 1, 1, ..., 0, 0, 0],\n       [1, 1, 1, ..., 0, 0, 0],\n       [0, 1, 1, ..., 0, 0, 0]], dtype=uint8)<\/code><\/pre>\nEach label TIF is a single-band image. The shape of each image array is (512, 512).\n<\/div>\n\n---\n\n<br><p><a id=\"performance-metric\"><\/a><\/p>\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">1.9 PERFORMANCE METRIC<\/h3>\n\n---\n\n<br><p>To measure your model\u2019s performance, we\u2019ll use a metric called Jaccard index, also known as Generalized Intersection over Union (IoU). Jaccard index is a similarity measure between two label sets. In this case, it is defined as the size of the intersection divided by the size of the union of non-missing pixels. In this competition there should be no missing data. Because it is an accuracy metric, <strong>a higher value is better<\/strong>. The Jaccard index can be calculated as follows:<\/p>\n\n<p>$$J(A, B) = \\frac{\\left|A\\cap B\\right|}{\\left|A\\cup B\\right|} = \\frac{\\left|A\\cap B\\right|}{\\left|A|+|B|-|A\\cap B\\right|}$$<\/p>\n\n<p>where |$A$| is the set of true pixels and |$B$| is the set of predicted pixels.<\/p>\n<p>In Python, you can easily calculate the Jaccard index using the scikit-learn function <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.jaccard_score.html\"><code>sklearn.metrics.jaccard_score(y_true, y_pred, average='binary')<\/code><\/a>. Below is some pseudocode that demonstrates how you might calculate the Jaccard index in Python without using <code>sklearn<\/code>:<\/p>\n\n```python\nimport numpy as np\n\n# get the sum of intersection and union over all chips\nintersection = 0\nunion = 0\n\nfor pred, actual in file_pairs:\n    intersection += np.logical_and(actual, pred).sum()\n    union += np.logical_or(actual, pred).sum()\n\n# calculate the score across all chips\niou = intersection \/ union \n```\n\n<p>In the above, <code>pred<\/code> and <code>actual<\/code> are each an array of 512x512 containing only <code>0<\/code>s and <code>1<\/code>s. <code>file_pairs<\/code> is a list of tuples where each tuple is <code>(pred, actual)<\/code>.<\/p>\n\n---\n\n<br><p><a id=\"submission-format\"><\/a><\/p>\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">1.10 SUBMISSION FORMAT<\/h3>\n\n---\n\n<br><p>This is a code execution challenge! \n* Rather than submitting your predicted labels, you\u2019ll <strong>package everything needed to do inference at the chip level and submit that for containerized execution<\/strong>. \n* Your code must be able to generate predictions in the form of single-band 512x512 pixel TIFs. \n* The lone band should consist of <code>1<\/code> (cloud) and <code>0<\/code> (no cloud) pixel values. \n\n<br>\n    \n<b><i>An example prediction for one chip could look like the following...<\/i><\/b><\/p>\n\n---\n\n<br><p><a id=\"prediction-format-ex\"><\/a><\/p>\n<div class=\"well\">\n\n<h3 style=\"font-family: Verdana; font-size: 20px; font-style: normal; font-weight: normal; text-decoration: none; text-transform: none; letter-spacing: 2px; color: #40E0D0; background-color: #ffffff;\">1.11 PREDICTION FORMAT EXAMPLE<\/h3>\n\n---\n\n<br><img alt=\"An example TIF image with the predicted cloud cover mask\" src=\"https:\/\/drivendata-public-assets.s3.amazonaws.com\/clouds-example-prediction-tif.png\" style=\"height: 250px\"><br\/>\n\n```python\narray([[0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       [0, 0, 0, ..., 0, 0, 0],\n       ...,\n       [0, 0, 0, ..., 1, 1, 1],\n       [0, 0, 0, ..., 1, 1, 1],\n       [0, 0, 0, ..., 1, 1, 1]], dtype=uint8)\n```\n\n<br>\n    \n* Each prediction TIF is a single-band image. \n* The shape of each image array is (512, 512).\n<\/div>"}}