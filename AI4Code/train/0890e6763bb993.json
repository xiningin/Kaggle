{"cell_type":{"f200d324":"code","5b1f3c0d":"code","201859d2":"code","1bf92444":"code","01b5ee69":"code","a009a7c6":"markdown","1ad01ff9":"markdown","0e5cf7aa":"markdown","cfab1012":"markdown","41cedd2e":"markdown","0deb9229":"markdown"},"source":{"f200d324":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nfrom sklearn.linear_model import SGDClassifier # Adjust the model\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5b1f3c0d":"def winner(p1, p2):\n    # This function will return (1, 0) if p1 beat p2,\n    # (0, 1) if p2 beat p1 and (0 ,0) for tie.\n    if (p1 == 0 and p2 == 1) or (p1 == 1 and p2 == 2) or (p1 == 2 and p2 == 0):\n        return (0, 1)\n    elif (p1 == 0 and p2 == 2) or (p1 == 1 and p2 == 0) or (p1 == 2 and p2 == 1):\n        return (1, 0)\n    else:\n        return (0, 0)","201859d2":"def build_features(opponent_moves):\n    # Builds a new set of features where rock (0) will be mapped to (1, 0, 0),\n    # paper (1) to (0, 1, 0) and scissors (2) to (0, 0, 1)\n    map_moves = {0: (1, 0, 0), 1: (0, 1, 0), 2: (0, 0, 1)}\n    features = []\n    for move in opponent_moves:\n        features.extend(map_moves.get(move))\n    return features","1bf92444":"class MyAgent(object):\n\n    def __init__(self):\n        self.cls = SGDClassifier(warm_start=True)\n\n    def train(self, opponent_moves):\n        x, y = opponent_moves[0:-1], [opponent_moves[-1]]\n        new_features = build_features(x)\n        x = np.array(new_features).reshape((1, -1))\n        self.cls.partial_fit(x, y, classes=[0, 1, 2])\n\n    def opponent_next_move(self, opponent_moves):\n        x = opponent_moves[1:]\n        new_features = build_features(x)\n        x = np.array(new_features).reshape((1, -1))\n        return self.cls.predict(x)[0]","01b5ee69":"def my_agent(observation, configuration):\n\n    if observation.step > 0:\n\n        # This if statement will ensure that the opponent's \n        # number of moves will not exceed the number of moves\n        # that can be stored, erasing the oldest move.\n        if len(my_agent.opponent_moves) >= my_agent.len_opponent_moves:\n            del my_agent.opponent_moves[0]\n\n        my_agent.opponent_moves.append(observation.lastOpponentAction) # Append the last opponent play\n        my_agent.ai.train(my_agent.opponent_moves) # Training the agent using the last turn\n        opponent_next_move = my_agent.ai.opponent_next_move(my_agent.opponent_moves) # Predicting the next move\n    \n        # Once we have the most likely opponet's play\n        # we can check which of the options (rock, paper, scissors)\n        # will ensure our victory\n        for i in (0, 1, 2):\n            ans = winner(i, opponent_next_move)\n            if ans == (1, 0):\n                return i\n        return observation.lastOpponentAction\n    else:\n        my_agent.len_opponent_moves = 51 # Number opponent moves storage\n        my_agent.opponent_moves = my_agent.len_opponent_moves * [0] # List of last nth opponent moves\n        my_agent.ai = MyAgent() # Agent object for training and preditions\n        return 0","a009a7c6":"The class below is designed to learn the last move online and predict your opponent's next move. The learning process is done using the last 50 opponent moves through logistic regression.","1ad01ff9":"A disadvantage here is that the agent is trained from scratch for each opponent. An idea here is to pre-train this agent, to achieve better results!","0e5cf7aa":"This is a simple agent, that scores more that 700. This agent will adapt to each opponent.\n\n**If you find this notebook helpful, please slap the vote button!**","cfab1012":"Finally, we have the function of the agent. On the first call to this function, three permanent function variables will be created (the first prediction for the opponent's next move will be rock). One indicating the number of movements of the opponent that will be stored, one with the last nth movements of the opponent (initially all zero) and the last containing the agent object.","41cedd2e":"The next piece of code will build binary features for the last nth plays. To make myself clear, let's say that the last opponent 3 plays was: rock, paper and scissors. So, we have a list of the opponent moves like [0, 1, 2]. When we pass this list to the function, it will build a new list as following [1, 0, 0, 0, 1, 0, 0, 0, 1]. Check out the dictionary map_moves for further comprehension.","0deb9229":"The following function is designed to get which player won the last turn."}}