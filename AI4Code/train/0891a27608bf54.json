{"cell_type":{"f4c30233":"code","06698e93":"code","3543c40e":"code","4ce88263":"code","faf2e46a":"code","e5128cd8":"code","1445c0d2":"code","a91bef86":"code","99ccbf28":"code","5434cbf6":"code","c2244a0b":"code","6278c28c":"code","764f7d2c":"code","9c253c75":"code","68237128":"code","c5b4319d":"code","c4a6c900":"code","2a38a5d5":"code","1ee37474":"code","b9e09dd8":"code","3cc45260":"code","8227cb72":"code","af7f4414":"code","df3f2f0f":"code","00d952d8":"code","013fdbd7":"code","c955b182":"code","8e9bcdb5":"code","35ec8cf4":"code","c65200a1":"code","7054e23c":"code","0bfc4b53":"markdown","6a308c20":"markdown","f2e1d103":"markdown","c83d3516":"markdown","6a412089":"markdown","ec6c0164":"markdown","9cb19375":"markdown","c09a6def":"markdown","17fea43d":"markdown","c6999207":"markdown","ef42934e":"markdown","a865ec2f":"markdown","b0cefe3f":"markdown","cb25fb57":"markdown","a4283910":"markdown","9cf3ce77":"markdown","4ec9ee86":"markdown","b95b4e4f":"markdown","0bdcd02c":"markdown"},"source":{"f4c30233":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport os\nfrom sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nprint(os.listdir(\"..\/input\"))","06698e93":"import seaborn as sns\nimport json\nimport pandas.io.json as pdjson\nimport ast\n\nfrom pandas.io.json import json_normalize\ndef load_df(csv_path='..\/input\/train_v2.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': 'str'}, # Important!!\n                     nrows=nrows)\n    \n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    for column in JSON_COLUMNS:\n        column_as_df = pdjson.json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df\n","3543c40e":"%%time\npath = \"..\/input\/parse-json-v2-without-hits-column\/\"\ntrain_df = pd.read_pickle(path + 'train_v2_clean.pkl')\ntest_df = pd.read_pickle(path + 'test_v2_clean.pkl')","4ce88263":"for df in [train_df,test_df]:\n    df['date'] = pd.to_datetime(df['visitStartTime'], unit='s')\n    df[\"day\"] = df['date'].dt.day\n    df['month'] = df['date'].dt.month\n    df['weekday'] = df['date'].dt.weekday\n    df['weekofyear'] = df['date'].dt.weekofyear","faf2e46a":"train_df.shape, test_df.shape","e5128cd8":"train_df['totals_pageviews']=train_df['totals_pageviews'].astype('float')\ntrain_df['totals_hits']=train_df['totals_hits'].astype('float')\ntest_df['totals_pageviews']=test_df['totals_pageviews'].astype('float')\ntest_df['totals_hits']=test_df['totals_hits'].astype('float')","1445c0d2":"train_df['totals_pageviews_mean']=train_df.groupby(['fullVisitorId'])['totals_pageviews'].transform('mean')\ntrain_df['totals_pageviews_max']=train_df.groupby(['fullVisitorId'])['totals_pageviews'].transform('max')\ntrain_df['totals_pageviews_min']=train_df.groupby(['fullVisitorId'])['totals_pageviews'].transform('min')\ntrain_df['totals_hits_mean']=train_df.groupby(['fullVisitorId'])['totals_hits'].transform('mean')\ntrain_df['totals_hits_max']=train_df.groupby(['fullVisitorId'])['totals_hits'].transform('max')\ntrain_df['totals_hits_min']=train_df.groupby(['fullVisitorId'])['totals_hits'].transform('min')\ntest_df['totals_pageviews_mean']=test_df.groupby(['fullVisitorId'])['totals_pageviews'].transform('mean')\ntest_df['totals_pageviews_max']=test_df.groupby(['fullVisitorId'])['totals_pageviews'].transform('max')\ntest_df['totals_pageviews_min']=test_df.groupby(['fullVisitorId'])['totals_pageviews'].transform('min')\ntest_df['totals_hits_mean']=test_df.groupby(['fullVisitorId'])['totals_hits'].transform('mean')\ntest_df['totals_hits_max']=test_df.groupby(['fullVisitorId'])['totals_hits'].transform('max')\ntest_df['totals_hits_min']=test_df.groupby(['fullVisitorId'])['totals_hits'].transform('min')","a91bef86":"\"\"\"\ndef process_totals(data_df):\n    print(\"process totals ...\")\n    #data_df['visitNumber'] = np.log1p(data_df['visitNumber'])\n    #data_df['totals_hits'] = np.log1p(data_df['totals_hits'])\n    #data_df['totals_pageviews'] = np.log1p(data_df['totals_pageviews'].fillna(0))\n    data_df['mean_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('mean')\n    data_df['sum_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('sum')\n    data_df['max_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('max')\n    data_df['min_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('min')\n    data_df['var_hits_per_day'] = data_df.groupby(['day'])['totals_hits'].transform('var')\n    data_df['mean_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('mean')\n    data_df['sum_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('sum')\n    data_df['max_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('max')\n    data_df['min_pageviews_per_day'] = data_df.groupby(['day'])['totals_pageviews'].transform('min')    \n    return data_df\ntrain_df = process_totals(train_df)\ntest_df = process_totals(test_df)\n\"\"\"","99ccbf28":"train_df.drop(['trafficSource_referralPath', 'trafficSource_source'], axis=1, inplace=True)\ntest_df.drop(['trafficSource_referralPath', 'trafficSource_source'], axis=1, inplace=True)","5434cbf6":"excluded_features = [\n    'date','fullVisitorId', 'sessionId','classfication_target','totals_totalTransactionRevenue','totals_transactionRevenue',\n    'visitId', 'visitStartTime', 'vis_date', 'nb_sessions', 'max_visits','next_session_1','next_session_2'\n]\ncategorical_features = [\n    _f for _f in train_df.columns\n    if (_f not in excluded_features) & (train_df[_f].dtype == 'object')\n]\none_hot_features = ['day','month','weekday']\n#'totals.totalTransactionRevenue','totals.TransactionRevenue','classfication_target'","c2244a0b":"\nfor i in one_hot_features:\n    print(\"Process feature =====>\"+str(i))\n    train_df[\"one_hot_feature\"] = train_df[i]\n    train_df[\"one_hot_feature\"] =  str(i) + \".\" + train_df[\"one_hot_feature\"].astype('str')\n    one_hot_combine = pd.get_dummies(train_df[\"one_hot_feature\"])\n    print(one_hot_combine.shape)\n    train_df = train_df.join(one_hot_combine)\n    del train_df[\"one_hot_feature\"]\n    del train_df[i]\n    del one_hot_combine\n    print(train_df.shape)\n","6278c28c":"\nfor f in categorical_features:\n    train_df[f], indexer = pd.factorize(train_df[f])\n    test_df[f] = indexer.get_indexer(test_df[f])\n","764f7d2c":"train_df.shape","9c253c75":"gc.collect()","68237128":"train_df['date'].max(),train_df['date'].min()","c5b4319d":"test_df['date'].max(),test_df['date'].min()","c4a6c900":"train_period_1 = train_df[(train_df['date']<=pd.datetime(2017,1,15)) & (train_df['date']>=pd.datetime(2016,8,1))]\ntrain_predict_preiod_1 = train_df[(train_df['date']<=pd.datetime(2017,4,30)) & (train_df['date']>=pd.datetime(2017,3,1))]\ntrain_period_2 = train_df[(train_df['date']<=pd.datetime(2017,11,15)) & (train_df['date']>=pd.datetime(2017,6,1))]\ntrain_predict_preiod_2 = train_df[(train_df['date']<=pd.datetime(2018,2,28)) & (train_df['date']>=pd.datetime(2018,1,1))]","2a38a5d5":"valid_period = train_df[(train_df['date']<=pd.datetime(2017,10,15)) & (train_df['date']>=pd.datetime(2017,5,1))]\nvalid_predict_preiod = train_df[(train_df['date']<=pd.datetime(2018,1,31)) & (train_df['date']>=pd.datetime(2017,12,1))]","1ee37474":"print('train_period1_shape',train_period_1.shape) \nprint('train_target1_period_shape',train_predict_preiod_1.shape)\nprint('train_period2_shape',train_period_2.shape) \nprint('train_target2_period_shape',train_predict_preiod_2.shape)\nprint('valid_period_shape',valid_period.shape) \nprint('valid_target_period_shape',valid_predict_preiod.shape)","b9e09dd8":"def add_target(train_period,target_period):\n    \n    train_period['totals_totalTransactionRevenue'] = train_period['totals_totalTransactionRevenue'].fillna(0).astype('float64')\n    target_period['totals_totalTransactionRevenue'] =target_period['totals_totalTransactionRevenue'].fillna(0).astype('float64')\n    train_period['totals_transactionRevenue'] = train_period['totals_transactionRevenue'].fillna(0).astype('float64')\n    target_period['totals_transactionRevenue'] = target_period['totals_transactionRevenue'].fillna(0).astype('float64')\n    #train_period['totals_transactions'] = train_period['totals_transactions'].fillna(0).astype('float64')\n    #target_period['totals_transactions'] = target_period['totals_transactions'].fillna(0).astype('float64')\n    \n    #train_pd=train_period\n    train_pd = train_period.groupby('fullVisitorId').mean().reset_index()\n    target_pd = target_period.groupby('fullVisitorId').mean().reset_index()\n    #target_pd=target_period\n    #Find the visitors those back puchased in future period\n    train_visitors = train_pd.fullVisitorId.unique()\n    train_predict_visitors = target_pd.fullVisitorId.unique()\n    same_visitors = np.intersect1d(train_visitors, train_predict_visitors)\n    \n    #Process data type\n    \n    \n    #Process back user df\n    back_user = target_pd[(target_pd.fullVisitorId.isin(same_visitors)) & (target_pd['totals_transactionRevenue'] > 0)]\n    back_user = back_user[['fullVisitorId','totals_transactionRevenue']]\n    print('we have',len(back_user['fullVisitorId'].value_counts()),'visitors back to purchase at target periods')\n    \n    #Add target\n    train_pd['classfication_target'] = train_pd['fullVisitorId'].map(lambda x: 1 if x in list(back_user['fullVisitorId']) else 0)\n    train_pd['totals_totalTransactionRevenue'] = np.log1p(train_pd['totals_totalTransactionRevenue'])\n    train_pd['totals_transactionRevenue'] = np.log1p(train_pd['totals_transactionRevenue'])\n    print (train_pd.shape)\n    return train_pd","3cc45260":"train_pd_1=add_target(train_period_1,train_predict_preiod_1)\ntrain_pd_2=add_target(train_period_2,train_predict_preiod_2)\nvalid_pd = add_target(valid_period,valid_predict_preiod)","8227cb72":"train_set = pd.concat([train_pd_1,train_pd_2], axis=0)","af7f4414":"train_set.shape","df3f2f0f":"excluded_features = [\n    'date','fullVisitorId', 'sessionId','classfication_target',\n    'visitId', 'visitStartTime', 'vis_date', 'nb_sessions', 'max_visits','next_session_1','next_session_2'\n]\ntrain_features = [_f for _f in train_set.columns if _f not in excluded_features ]","00d952d8":"from sklearn.model_selection import GroupKFold\ndef get_folds(df=None, n_splits=5):\n    \"\"\"Returns dataframe indices corresponding to Visitors Group KFold\"\"\"\n    # Get sorted unique visitors\n    unique_vis = np.array(sorted(df['fullVisitorId'].unique()))\n\n    # Get folds\n    folds = GroupKFold(n_splits=n_splits)\n    fold_ids = []\n    ids = np.arange(df.shape[0])\n    for trn_vis, val_vis in folds.split(X=unique_vis, y=unique_vis, groups=unique_vis):\n        fold_ids.append(\n            [\n                ids[df['fullVisitorId'].isin(unique_vis[trn_vis])],\n                ids[df['fullVisitorId'].isin(unique_vis[val_vis])]\n            ]\n        )\n\n    return fold_ids","013fdbd7":"y_target = train_set['classfication_target']\nvalid_target = valid_pd['classfication_target']","c955b182":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import TimeSeriesSplit, KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nfrom sklearn.model_selection import GroupKFold\nparams = {\n    \"max_bin\": 512,\n    \"learning_rate\": 0.02,\n    \"boosting_type\": \"gbdt\",\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"num_leaves\": 10,\n    \"min_data\": 100,\n    \"boost_from_average\": True\n}\nn_fold = 5\n#print(train_features)\nfolds = get_folds(df=train_set, n_splits=5)\n\nmodel = lgb.LGBMClassifier(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n\noof_reg_preds = np.zeros(train_set.shape[0])\nprediction = np.zeros(valid_pd.shape[0])\n\nfor fold_n, (trn_, val_) in enumerate(folds):\n    print('Fold:', fold_n)\n    #print(f'Train samples: {len(train_index)}. Valid samples: {len(test_index)}')\n    trn_x, trn_y = train_set[train_features].iloc[trn_], y_target.iloc[trn_]\n    val_x, val_y = train_set[train_features].iloc[val_], y_target.iloc[val_]\n    \n\n    model.fit(trn_x, trn_y, \n            eval_set=[(trn_x, trn_y), (val_x, val_y)], eval_metric='AUC',\n            verbose=500, early_stopping_rounds=100)\n    \n    oof_reg_preds[val_] = model.predict(val_x, num_iteration=model.best_iteration_)\n    \n    pred = model.predict(valid_pd[train_features], num_iteration=model.best_iteration_)\n    prediction += pred\n    \nprediction \/= n_fold\n#print(accuracy_score(y_target,np.float64(oof_reg_preds>=0.5)))\n#print(accuracy_score(valid_target,np.float64(prediction>=0.5)))\n","8e9bcdb5":"lgb.plot_importance(model, figsize=(15, 10))\nplt.show()","35ec8cf4":"prediction_ans = np.where(prediction >= 0.2, 1, 0)\n#valid_ans = np.where(prediction>=0.5,1,0)","c65200a1":"plt.figure(figsize=(16,6))\nfalse_positive_rate, recall, thresholds = roc_curve(y_target, oof_reg_preds)\nroc_auc = auc(false_positive_rate, recall)\nplt.subplot(121)\nplt.title('Receiver Operating Characteristic (ROC)_train')\nplt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1], [0,1], 'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.ylabel('Recall')\nplt.xlabel('Fall-out (1-Specificity)')\n\nfalse_positive_rate, recall, thresholds = roc_curve(valid_target, prediction_ans)\nroc_auc = auc(false_positive_rate, recall)\nplt.subplot(122)\nplt.title('Receiver Operating Characteristic (ROC)_Valid')\nplt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1], [0,1], 'r--')\nplt.xlim([0.0,1.0])\nplt.ylim([0.0,1.0])\nplt.ylabel('Recall')\nplt.xlabel('Fall-out (1-Specificity)')\nplt.show()","7054e23c":"import seaborn as sns\n#Print Confusion Matrix\nplt.figure(figsize=(16,6))\ncm1 = confusion_matrix(y_target, oof_reg_preds)\nlabels = ['0', '1']\nplt.subplot(121)\nsns.heatmap(cm1, xticklabels = labels, yticklabels = labels, annot = True, fmt='d', cmap=\"Blues\", vmin = 0.2);\nplt.title('Confusion Matrix_train')\nplt.ylabel('True Class')\nplt.xlabel('Predicted Class')\n\ncm2 = confusion_matrix(valid_target, prediction_ans)\nlabels = ['0', '1']\nplt.subplot(122)\nsns.heatmap(cm2, xticklabels = labels, yticklabels = labels, annot = True, fmt='d', cmap=\"Blues\", vmin = 0.2);\nplt.title('Confusion Matrix_valid')\nplt.ylabel('True Class')\nplt.xlabel('Predicted Class')\nplt.show()\n","0bfc4b53":"## Plot confusion matrix of prediction","6a308c20":"## Training Set\n* Training period set 1==> 2016\/08\/01 ~ 2017\/1\/15 (5.5 month)\n* Target period set 1  ==> 2017\/03\/1 ~ 2017\/04\/30 (2 month)\n* Training period set 2==> 2017\/06\/01 ~ 2017\/11\/15 (5.5 month)\n* Target period set 2  ==> 2018\/1\/1 ~ 2018\/02\/30 (2 month)","f2e1d103":"## Add time feature","c83d3516":"## Plot feature important","6a412089":"## Process one hot encoding on time ","ec6c0164":"## Valid Set (1 year ago of our test set and target )\n* Valid period set ==> 2017\/5\/1 ~ 2017\/10\/15\n* Valid target period set ==> 2017\/12\/1 ~ 2018\/1\/31","9cb19375":"## Valid period","c09a6def":"## Training period","17fea43d":"## Set K fold","c6999207":"## Conclusion \n* We only can see there are only 5 true positives labels....   \n* Try find the key feature, and do another feature enginnering for the future predict \n* Did anyone have better idea and improve AUC for classification?\n\n## Next Step\n* Doing regression for future revenue prediction...\n","ef42934e":"## Feature engineering \n* mean, max, min for \"totals_pagevies\" and \"totals_hits \"\n* Change to lable encoding for categorical feature\n* Drop 'trafficSource_referralPath','trafficSource_source'","a865ec2f":"## Split Validate and Train data by timeframe","b0cefe3f":"## Load Data\n* use Aguiar's dataset (Many thanks): https:\/\/www.kaggle.com\/jsaguiar\/parse-json-v2-without-hits-column","cb25fb57":"### Factoriza  categorical featuers","a4283910":"# This kernel is to predict the future of customer will come back purchase or not\n* Fot train_v2 data, we have 2016\/08\/01 ~ 2018\/04\/30 period data\n* For test_v2 data, we have 2018\/05\/1 ~ 2018\/10\/15 period data\n* The Public LB  score is base on timeframe 2018\/05\/1~ 2018\/10\/15\n* The Private LB score is base on timeframe of 2018\/12\/1 ~ 2019\/01\/31 with same visitor ID that in test_v2\n* So this competition become the future prediction question .....","9cf3ce77":"## Add the target on training data and validation data","4ec9ee86":"## Training Set\n* Training period set 1==> 2016\/08\/01 ~ 2017\/1\/15 (5.5 month)\n* Target period set 1  ==> 2017\/03\/1 ~ 2017\/04\/30 (2 month)\n* Training period set 2==> 2017\/06\/01 ~ 2017\/11\/15 (5.5 month)\n* Target period set 2  ==> 2018\/1\/1 ~ 2018\/02\/30 (2 month)\n* Concate set 1 and set 2 to be training data\n* Feature engineering on training period feature\n* Target set that those come back purchased user in target period","b95b4e4f":"## Start training (5 fold LightGBM)","0bdcd02c":"## Discussion topic about this idea from AmirH\nhttps:\/\/www.kaggle.com\/c\/ga-customer-revenue-prediction\/discussion\/71427\n* I use LGBM to predict the user will come back purchase or not (Classification)\n"}}