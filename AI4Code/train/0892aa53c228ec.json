{"cell_type":{"bd35efcb":"code","b4126073":"code","70ce5ba3":"code","2e29fa8a":"code","f8964cbb":"code","3fd3de6d":"code","e951e779":"code","8d2be656":"code","5d04a627":"code","518cd425":"code","8f172002":"code","cc8fb5b5":"code","322f2cd0":"code","53b0afc8":"code","7f93e275":"code","2e8fee95":"code","86b68e0c":"code","d0054a9a":"code","5c6e95b5":"code","e4dd6aa9":"code","1818f094":"code","a6831eff":"code","eec89c22":"code","6a78899d":"code","4e17bbca":"code","3a819f0b":"code","ec6d99a4":"code","ddcf82d8":"code","1eea2c83":"code","fc42a5f7":"code","01efe3a4":"code","85529823":"code","44290a8a":"code","aa79bd0c":"code","7f5ef2ec":"code","b5234c01":"code","f79b6ae7":"code","33e89fac":"code","e6682448":"code","a547ccb5":"code","8230fc30":"code","fd908b3d":"code","7d93c802":"code","40432707":"code","b5312919":"code","88e6914d":"code","1a209701":"code","f3983cf8":"code","7dc5746d":"code","2a99789b":"code","947b8604":"code","72918a2f":"code","7f798161":"code","248e9c36":"code","ca9e68d5":"code","d562e7a6":"code","464426e8":"code","82e181cf":"code","219e8c89":"code","ac8ef21b":"code","487d6cd8":"code","759d93cf":"code","a622bc95":"code","8d98b349":"code","500a3327":"code","5f4d3889":"code","db4640d2":"code","6a5825e8":"code","69aec144":"code","cbb6e9f1":"code","a1093ccf":"code","89332181":"code","0cea3594":"code","b7f704a0":"code","d970ab60":"code","a51f78a0":"code","1bda80a8":"code","745d6f9d":"code","19eeabfa":"code","886ff0a9":"code","85b4ef53":"code","f931376c":"code","5ccb46df":"code","62866b65":"code","20cb942f":"code","ec559f55":"code","0e563acd":"markdown","814f0e4d":"markdown","551323ae":"markdown","09eaea5d":"markdown","b9ca78c8":"markdown","5fae7f54":"markdown","4f7a0d34":"markdown","cbf12123":"markdown","9e3dfa1d":"markdown"},"source":{"bd35efcb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input\/house-prices-advanced-regression-techniques'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b4126073":"# Importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","70ce5ba3":"# Importing train and test data\ntrain_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","2e29fa8a":"# Lokking at the train data to observe X and y i.e. input and output\ntrain_df.head()","f8964cbb":"# Lokking at the test data to observe X and y i.e. input and output\ntest_df.head()","3fd3de6d":"# Observing the shape of train and test data\ntrain_df.shape, test_df.shape","e951e779":"# Observing the Column output GrLivArea against Output SalePrice\n# To check for outliers\nplt.scatter(train_df.GrLivArea, train_df.SalePrice)\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')","8d2be656":"# Removing the outliers\ntrain_df = train_df.drop(train_df[\n    (train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)\nplt.scatter(train_df.GrLivArea, train_df.SalePrice)\nplt.xlabel('GrLivArea')\nplt.ylabel('SalePrice')","5d04a627":"# Checking the shape of train data again\ntrain_df.shape","518cd425":"# Analysing the Saleprice variable.\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nsns.distplot(train_df['SalePrice'] , fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","8f172002":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train_df['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_df['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_df['SalePrice'], plot=plt)\nplt.show()","cc8fb5b5":"train_df.shape, test_df.shape","322f2cd0":"#Save the 'Id' column\ntrain_ID = train_df['Id']\ntest_ID = test_df['Id']\n\nprint(\"train shape before dropping id: {}\".format(train_df.shape))\nprint(\"test shape before dropping id: {}\".format(test_df.shape))\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain_df.drop(\"Id\", axis = 1, inplace = True)\ntest_df.drop(\"Id\", axis = 1, inplace = True)\n\nprint(\"train shape after dropping id: {}\".format(train_df.shape))\nprint(\"test shape after dropping id: {}\".format(test_df.shape))","53b0afc8":"# Analysing the LotArea Feature against SalePrice\nplt.scatter(train_df.LotArea, train_df.SalePrice)\nplt.xlabel('LotArea')\nplt.ylabel('SalePrice')\n# it shows outliers in it","7f93e275":"# Analysing Lotfrontage against SalePrice\nplt.scatter(train_df.LotFrontage, train_df.SalePrice)\nplt.xlabel('LotFrontage')\nplt.ylabel('SalePrice')","2e8fee95":"# Analysing the OverallQual feature to draw pie chart\ntrain_df['OverallQual'].value_counts()","86b68e0c":"labels = ['Average', 'Above Average', 'Good', 'Very Good', 'Below Average','Excellent', 'Fair', 'Very Excellent', 'Poor', 'Very Poor' ]\nexplode = (0, 0.0, 0.0, 0.1, 0.1, 0.1, 0.2, 0.3, 0.4, 0.6)\n\nfig, ax = plt.subplots()\nax.pie(train_df['OverallQual'].value_counts(), explode=explode,\n       labels=labels, autopct='%1.1f%%', shadow=True, startangle=30)\nax.axis('equal')\nplt.show()","d0054a9a":"# Analysing OverallQual against SalePrice in barplot\nimport seaborn as sns\nfig = sns.barplot(x='OverallQual', y='SalePrice', data=train_df)\nfig.set_xticklabels(labels=['Very Poor', 'Poor', 'Fair', 'Below Average', 'Average', 'Above Average', 'Good', 'Very Good', 'Excellent', 'Very Excellent'], rotation=90);","5c6e95b5":"# Analysing FOundation feature for pie chart\ntrain_df['Foundation'].value_counts()","e4dd6aa9":"labels = ['Concrete', 'Cinder block', 'Brick&tile', 'Slab', 'Stone', 'Wood']\nexplode = (0, 0.0, 0.0, 0.1, 0.3, 0.5)\n\nfig, ax = plt.subplots()\nax.pie(train_df['Foundation'].value_counts(), explode=explode,\n       labels=labels, autopct='%1.1f%%', shadow=True, startangle=30)\nax.axis('equal')\nplt.show()","1818f094":"# Analysing FOundation against SalePrice for barplot\nfig1=sns.barplot(x='Foundation', y='SalePrice', data=train_df)\nfig1.set_xticklabels(labels=['Contrete', 'Cinder Block', 'Brick&Tile', 'Wood', 'Slab', 'Stone'], rotation=90)\nplt.xlabel(\"Types of Foundation\")","a6831eff":"# Analysing GarageCars against SalePrice from dataframe to use it in barplot\ncheck = train_df[['GarageCars', 'SalePrice']]\ncheck","eec89c22":"# Creating pivot table to check GarageCars against SalePrice using aggregate function\npd.pivot_table(check, values='SalePrice', index=['GarageCars'], aggfunc=np.sum)","6a78899d":"fig = sns.barplot(x='GarageCars', y='SalePrice', data=train_df)\nfig.set_xticklabels(labels=['4 car', 'no car', '3 car', '2 car', '1 car'], rotation=90)\nplt.xlabel('No of cars in garage')","4e17bbca":"# Checking max value count for each value to use it in plot\ntrain_df['Fireplaces'].value_counts()","3a819f0b":"# Analysing Fireplaces against SalePrice\nfig2 = sns.barplot(x='Fireplaces', y='SalePrice', data=train_df)\nfig2.set_xticklabels(labels=['3 fireplace', '2fireplace', '1 fireplace', 'No fireplace'], rotation=90)\nplt.xlabel('No of fireplace')","ec6d99a4":"# Using displot instead of distplot which is deprecating in next version\nsns.displot(x='LotArea', data=train_df, kde=True)\nskew_ness = str(train_df['LotArea'].skew())\nkurt_ = str(train_df['LotArea'].kurt())\nplt.legend([skew_ness, kurt_], title='skewness&kurtosis')\nplt.title(\"before transform\")\nplt.show()","ddcf82d8":"sns.displot(x='GrLivArea', data=train_df, kde=True)\nskew_ness=str(train_df['GrLivArea'].skew())\nkurt_=str(train_df['GrLivArea'].kurt())\nplt.legend([skew_ness, kurt_], title='skewness&kurtosis')\nplt.title('before transform')\nplt.show()","1eea2c83":"sns.displot(x=train_df['LotFrontage'],kde=True)\nskew_ness = str(train_df['LotFrontage'].skew())\nkurt_ = str(train_df['LotFrontage'].kurt())\nplt.legend([skew_ness, kurt_], title='skewness&kurtosis')\nplt.title('before apply')\nplt.show()","fc42a5f7":"sns.displot(x='OverallQual', data=train_df, kde=True)\nskew_ness = str(train_df['OverallQual'].skew())\nkurt_ = str(train_df['OverallQual'].kurt())\nplt.legend([skew_ness, kurt_], title='skewness&kurtosis')\nplt.title(\"before transform\")\nplt.show()","01efe3a4":"sns.displot(x='LotFrontage', data=train_df, kde=True)\nskew_ness = str(train_df['LotFrontage'].skew())\nkurt_ = str(train_df['LotFrontage'].kurt())\nplt.legend([skew_ness, kurt_], title='skewness&kurtosis')\nplt.title('before transform')\nplt.show()","85529823":"sns.displot(x='GrLivArea', data=train_df, kde=True)\nskew_ness = str(train_df['GrLivArea'].skew())\nkurt_ = str(train_df['GrLivArea'].kurt())\nplt.legend([skew_ness, kurt_], title='skewness&kurtosis')\nplt.title('before transform')\nplt.show()\n","44290a8a":"sns.displot(x='LotArea', data=train_df, kde=True)\nskew_ness = str(train_df['LotArea'].skew())\nkurt_ = str(train_df['LotArea'].kurt()) \nplt.legend([skew_ness, kurt_], title='skewness&kurtosis')\nplt.title('before transform')\nplt.show()","aa79bd0c":"# Saving the length of train and test and concatenating to make a single df\ntrain_range = train_df.shape[0]  # 1458\ntest_range = test_df.shape[0]  # 1459\ny_train = train_df.SalePrice.values # saving target variable\ntotal_data = pd.concat((train_df, test_df)).reset_index(drop=True)","7f5ef2ec":"# shape of total_data\ntotal_data.shape","b5234c01":"# dropping target variable\ntotal_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"total_data size is : {}\".format(total_data.shape))\ntotal_data.head()","f79b6ae7":"# Counting unique value in each categorical column\ncat_col =total_data.select_dtypes(include=['object'])\n\nfor i in list(cat_col.columns):\n    print(\"We have {} unique value in {} column: {}.\".format(len(cat_col[i].unique()), \n                                                     i, cat_col[i].unique()))\n    print('*'*140)","33e89fac":"# divide data into categorical and numerical features\ncat, num = [], []\nfor i in total_data.columns:\n    d = total_data.dtypes[i]\n    if d == 'object':\n        cat.append(i)\n    else:\n        num.append(i)\n\nprint(\"Categorical: {}\".format(cat))\nprint(\"Numerical: {}\".format(num))","e6682448":"# Checking length of categorical and numerical\nprint(\"Length of categorical: {}\".format(len(cat)))\nprint(\"Length of numerical: {}\".format(len(num)))","a547ccb5":"# Checking unique in catgeorical column by using our categorical list\nfor i in cat:\n    print(i, \"-\"*(30-len(i)), len(total_data[i].unique()))","8230fc30":"# Checking %  of missing value in categorical features\ncat_feature = [feature for feature in cat if total_data[feature].isnull().sum()]\nfor feature in cat_feature:\n    print(\"{}: {} %\".format(feature, round((total_data[feature].isnull().sum()\n                                          \/len(total_data[feature]))*100, 3)))","fd908b3d":"# missing % in numerical features\nnum_feature = [feature for feature in num if total_data[feature].isnull().sum()]\nfor feature in num_feature:\n    print(\"{}: {} %\".format(feature, round((total_data[feature].isnull().sum()\n                                            \/len(total_data[feature]))*100, 3)))","7d93c802":"# missing value in overall whole data\ntotal_data_na = (total_data.isnull().sum() \/ len(total_data)) * 100\n\ntotal_data_na","40432707":"# Checking columns where there's no missing value\n(total_data_na[total_data_na == 0].index)","b5312919":"# Another way to check missing ratios\ntotal_data_na = total_data_na.drop(total_data_na[total_data_na == 0].index).sort_values(ascending=False)[:30]\n\ntotal_data_na","88e6914d":"# creating dataframe of missing data values\nmissing_data = pd.DataFrame({'Missing Ratio' :total_data_na})\nmissing_data.head(20)","1a209701":"# plotting missing values\nplt.subplots(figsize=(15,12))\nplt.xticks(rotation='90')\nsns.barplot(x=total_data_na.index, y=total_data_na)\n\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing value', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=25)","f3983cf8":"# Filling missing values with none values \ntotal_data[\"PoolQC\"] = total_data[\"PoolQC\"].fillna(\"None\")\ntotal_data[\"MiscFeature\"] = total_data[\"MiscFeature\"].fillna(\"None\")\ntotal_data[\"Alley\"] = total_data[\"Alley\"].fillna(\"None\")\ntotal_data[\"Fence\"] = total_data[\"Fence\"].fillna(\"None\")\ntotal_data[\"FireplaceQu\"] = total_data[\"FireplaceQu\"].fillna(\"None\")","7dc5746d":"# Area of each street is connected so replacing missing value with median\ntotal_data[\"LotFrontage\"]=total_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","2a99789b":"# replacing with most frequent value in the column which is None\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    total_data[col] =total_data[col].fillna('None')","947b8604":"# Replacing missing with 0 (No garage = no cars in such garage)\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    total_data[col] = total_data[col].fillna(0)","72918a2f":"# Missing value in these feature means no basement (replace with 0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    total_data[col] = total_data[col].fillna(0)","7f798161":"# NaN means in these column is no basement\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    total_data[col] = total_data[col].fillna('None')","248e9c36":"# NA means no masonry veneer for houses. (Fill zero for area and None for type)\ntotal_data[\"MasVnrType\"] = total_data[\"MasVnrType\"].fillna(\"None\")\ntotal_data[\"MasVnrArea\"] = total_data[\"MasVnrArea\"].fillna(0)\n\n# Filling MSZoning with most common value 'RL'\ntotal_data['MSZoning'] = total_data['MSZoning'].fillna(total_data['MSZoning'].mode()[0])\n\n# For Utilitites, most of the records are 'AllPub'. It won't help in prediction\ntotal_data = total_data.drop(['Utilities'], axis=1)\n\n# data description says- Assume typical(Typ) unless deductions are warranted\ntotal_data[\"Functional\"] = total_data[\"Functional\"].fillna(\"Typ\")\n\n# replacing with most repeated \"SBrkr\" value\ntotal_data['Electrical'] = total_data['Electrical'].fillna(total_data['Electrical'].mode()[0])\n\n# set \"TA\" most frequent in place of missing value\ntotal_data['KitchenQual'] = total_data['KitchenQual'].fillna(total_data['KitchenQual'].mode()[0])\n\n# replacing with most common value \ntotal_data['Exterior1st'] = total_data['Exterior1st'].fillna(total_data['Exterior1st'].mode()[0])\ntotal_data['Exterior2nd'] = total_data['Exterior2nd'].fillna(total_data['Exterior2nd'].mode()[0])\n\n# replacing with most common value again\ntotal_data['SaleType'] = total_data['SaleType'].fillna(total_data['SaleType'].mode()[0])\n\n# MSSubClass-type of dwelling, NA means no building class. (fill with None)\ntotal_data['MSSubClass'] = total_data['MSSubClass'].fillna(\"None\")","ca9e68d5":"#Check remaining missing values if any \ntotal_data_na = (total_data.isnull().sum()\/len(total_data))*100\ntotal_data_na = total_data_na.drop(total_data_na[total_data_na==0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({\"missing ratio\":total_data_na})\nmissing_data.head()","d562e7a6":"# Converting some column feature who are int but should be str\n\ntotal_data['MSSubClass'].dtype, total_data['YrSold'].dtype","464426e8":"#MSSubClass=The building class\ntotal_data['MSSubClass'] = total_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ntotal_data['OverallCond'] = total_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ntotal_data['YrSold'] = total_data['YrSold'].astype(str)\ntotal_data['MoSold'] = total_data['MoSold'].astype(str)","82e181cf":"# process columns, apply LabelEncoder to categorical features\nfrom sklearn.preprocessing import LabelEncoder\n\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\n\nfor c in cols:\n    lbl = LabelEncoder()\n    lbl.fit(list(total_data[c].values))\n    total_data[c] = lbl.transform(list(total_data[c].values))\n# shape        \nprint('Shape all_data: {}'.format(total_data.shape))","219e8c89":"# Adding one extra feature -> total sqfootage feature \ntotal_data['TotalSF'] = total_data['TotalBsmtSF'] + total_data['1stFlrSF'] + total_data['2ndFlrSF']","ac8ef21b":"# Checking numerical feature again\nnumeric_feats = total_data.dtypes[total_data.dtypes != 'object' ].index\nnumeric_feats","487d6cd8":"# Checking skewness level on numerical features to remove\nskewed_feats = total_data[numeric_feats].apply(lambda x: skew(x.dropna())).\\\nsort_values(ascending=False)\nskewness = pd.DataFrame({\"Skewness \": skewed_feats})\nskewness.head()","759d93cf":"# Applying boxcox on those features having skewness > 0.75\nskewness = skewness[abs(skewness)>0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\n\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    total_data[feat] = boxcox1p(total_data[feat], lam)","a622bc95":"total_data.shape","8d98b349":"# Coverting features into dummies for expanding\n# dimensions for convenient access\ntotal_data = pd.get_dummies(total_data)\nprint(total_data.shape)","500a3327":"# Converting total data back to train and test\ntrain = total_data[:train_range]\ntest = total_data[train_range:]\ntrain.shape","5f4d3889":"# Importing different models to try on our dataset\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb","db4640d2":"n_folds = 5\n# defining our own function to get root\n# mean square value using 5 fold split\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","6a5825e8":"# Passing RobustScaler() to make the lasso model more robust to outlier\nlasso = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))","69aec144":"# Passing RobustScaler() to make the Enet model more robust to outlier\nEnet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005,\n                                                l1_ratio=.9, random_state=3))","cbb6e9f1":"krr = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","a1093ccf":"# passing huber loss to make it robust to outliers\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","89332181":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","0cea3594":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","b7f704a0":"score = rmsle_cv(Enet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","d970ab60":"score = rmsle_cv(krr)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a51f78a0":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","1bda80a8":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","745d6f9d":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self,models):\n        self.models = models\n        \n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        for model in self.models_:\n            model.fit(X, y)\n            \n        return self\n    \n    def predict(self, X):\n        predictions = np.column_stack([model.predict(X)\n                                       for model in self.models_])\n        return np.mean(predictions, axis=1)","19eeabfa":"# Averaging the score\naveraged_models = AveragingModels(models = (Enet, GBoost, krr, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","886ff0a9":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","85b4ef53":"# Averaging the score again\nstacked_averaged_models = StackingAveragedModels(base_models = (Enet, GBoost, krr),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","f931376c":"# defined function to get rmse\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","5ccb46df":"# Fitting the stacked model and predicting\nstacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","62866b65":"# xgb model prediction\nmodel_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","20cb942f":"#ensembling the model\nensemble = stacked_pred*0.70 + xgb_pred*0.30","ec559f55":"# creating submission file\nsub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","0e563acd":"<h3>poolQC - NA means no pool, so replacing with None<\/h3>\n<h3>Miscfeature - NA means no miscfeature, so replacing with None<\/h3>\n<h3>Alley - NA means no Alley access, so replacing with None<\/h3>\n<h3>Fence - NA means no fence, so replacing with None<\/h3>\n<h3>FireplaceQu - NA means no fireplace, so replacing with None<\/h3>","814f0e4d":"<h4>Approach - we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model. <\/h4>","551323ae":"# Taking insights from the dataset(EDA)","09eaea5d":"<h3>This EDA+Regression notebook is a combination of two notebooks I was referring while housing regression competition. I loved the code of these notebooks so much that I thought of saving and sharing with all of you. I loved the insights of the creator of these 2 notebooks. I'm mentioning the link for further reference if you want.<\/h3>","b9ca78c8":"<h4><a href= \"https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\">Click here for first link- Top 4%<\/a><\/h4>\n<h4><a href= \"https:\/\/www.kaggle.com\/bakar31\/eda-house-price-prediction\">Click here for second link<\/a><\/h4>","5fae7f54":"<h2>Analysing the skewness and kurtosis of some features to understand the \nvariation of the data<\/h1>","4f7a0d34":"<h3>Stacking using a meta model<\/h3>","cbf12123":"<h3>Stacking of models<\/h3>","9e3dfa1d":"<h2>We will not remove outliers\nfrom every feature as it may affect the model\nsince test set will have outliers too and our model \nneeds to be robust against them <\/h2>"}}