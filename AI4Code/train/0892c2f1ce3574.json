{"cell_type":{"590401d6":"code","d681062e":"code","dc433e46":"code","6c063a05":"code","1c467967":"code","11cd41db":"code","9ed85618":"code","f2d93bad":"code","6fbb4d2f":"code","58962809":"code","6e6048dd":"code","6629b556":"code","9cc3ba54":"code","7e0ddf99":"code","ec18b8fc":"code","ac0e9a45":"code","41f5e65d":"code","2e641189":"code","e7d71f98":"code","d5f9f4ee":"code","c41f8846":"code","b79e0c08":"code","53ef667c":"code","1de3e34b":"code","b3cfe836":"code","9e54264d":"code","db34f72c":"code","a35a7583":"code","f35ade7f":"code","0d288b31":"code","5fab113c":"code","90d650fd":"code","4deec239":"code","1a8e61e7":"code","30054175":"code","1c977ad0":"code","1b1b087c":"code","365bb290":"code","220fd314":"code","db75924f":"code","4c9a576e":"code","ac67c63f":"code","7935cc24":"code","00546827":"code","7971f86a":"code","13bb7ae5":"code","81e89f07":"code","1adc3fde":"code","a6909076":"markdown","eeb136db":"markdown","510d779d":"markdown","f22766e9":"markdown","7c809176":"markdown","9051a664":"markdown","e8e36f07":"markdown","ff92bb69":"markdown","623a7839":"markdown"},"source":{"590401d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d681062e":"#Load Data\ntrain_data = pd.read_csv(\"..\/input\/train.csv\")","dc433e46":"#get info about data\ntrain_data.info()","6c063a05":"x = train_data.columns.values\nprint(x)","1c467967":"train_data.head(10)","11cd41db":"#how many missing values exist in the collection \ntrain_data.isnull().sum()","9ed85618":"train_data.fillna(train_data['Age'].dropna().median(), inplace = True)","f2d93bad":"train_data.shape","6fbb4d2f":"train_data.plot(kind = \"scatter\", x = \"Fare\", y = \"Survived\", color = \"r\", linewidth = 1)\nplt.show()","58962809":"sns.barplot(x = \"Pclass\", y = \"Survived\", data = train_data)","6e6048dd":"sns.barplot(x = \"Sex\", y = \"Survived\", data = train_data)","6629b556":"#correlation \nf, ax = plt.subplots(figsize = (18, 18))\nsns.heatmap(train_data.corr(), linewidth = 1, annot = True, fmt = '.1f', ax=ax)\nplt.show()","9cc3ba54":"#extract title from names - train_data\ntrain_data['Title'] = train_data['Name'].str.extract(\"([A-za-a]+)\\.\", expand = False) ","7e0ddf99":"train_data\n#now we have Title column as well","ec18b8fc":"#delete unnecessary features from dataset\ntrain_data.drop('Name', axis = 1, inplace = True)\n","ac0e9a45":"train_data.head(10)","41f5e65d":"train_data","2e641189":"title_mapping = {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \"Master\": 3,\n                \"Dr\": 3, \"Rev\": 3, \"Col\": 3, \"Major\": 3, \"Mlle\": 3, \"Countless\": 3, \n                \"Ms\": 3, \"Lady\": 3, \"Jonkheer\": 3, \"Don\": 3, \"Dona\": 3, \"Mme\": 3, \"Capt\": 3, \"Sir\": 3}\n\ntrain_data['Title'] = train_data['Title'].map(title_mapping)","e7d71f98":"#Sex mapping male: 0 female: 1\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain_data['Sex'] = train_data['Sex'].map(sex_mapping)","d5f9f4ee":"train_data.head(10)","c41f8846":"train_data.loc[train_data['Age'] <= 16, 'Age'] = 0\ntrain_data.loc[(train_data['Age'] > 16) & (train_data['Age'] <= 26), 'Age'] = 1\ntrain_data.loc[(train_data['Age'] > 26) & (train_data['Age'] <= 36), 'Age'] = 2\ntrain_data.loc[(train_data['Age'] > 36) & (train_data['Age'] <= 62), 'Age'] = 3\ntrain_data.loc[train_data['Age'] > 62, 'Age'] = 4","b79e0c08":"train_data.head(10)","53ef667c":"#embarked mapping\nembarked_mapping = {\"S\": 0, \"C\": 1, \"Q\": 2}\ntrain_data['Embarked'] = train_data[\"Embarked\"].map(embarked_mapping)","1de3e34b":"#fare categorising\ntrain_data.loc[train_data['Fare'] <= 17, 'Fare'] = 0\ntrain_data.loc[(train_data['Fare']> 17) & (train_data['Fare'] <= 29), 'Fare'] = 1\ntrain_data.loc[(train_data['Fare']> 29) & (train_data['Fare'] <= 100), 'Fare'] = 2\ntrain_data.loc[train_data['Fare']>100, 'Fare'] = 3","b3cfe836":"train_data.head(10)","9e54264d":"train_data = train_data.drop(['Cabin', 'Ticket'], axis = 1)\n","db34f72c":"train_data.head(10)","a35a7583":"#create new attributes called FamilySize\ntrain_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1\n","f35ade7f":"train_data.head(10)","0d288b31":"#family mapping\nfamily_mapping = {1: 0, 2: 0.4, 3: 0.8, 4: 1.2, 5: 1.6, 6: 2.0, 7: 2.4, 8: 2.8, 9: 3.2, 10: 3.6, 11: 4.0}\ntrain_data['FamilySize'] = train_data['FamilySize'].map(family_mapping)","5fab113c":"train_data.head(10)","90d650fd":"#we can remove unnecessary features from the dataset\nfeatures_drop = ['SibSp', 'Parch']\ntrain_data = train_data.drop(features_drop, axis = 1)\ntrain_data = train_data.drop(['PassengerId'], axis = 1)\n","4deec239":"target = train_data['Survived']\ntrain = train_data.drop(['Survived'], axis = 1)\n","1a8e61e7":"train_data.head(10)","30054175":"x_train = train_data.drop(\"Survived\", axis = 1)\ny_train = train_data['Survived']","1c977ad0":"x_train.fillna(x_train.dropna().median(), inplace = True)","1b1b087c":"y_train.fillna(y_train.dropna().median(), inplace = True)","365bb290":"#remove feature by using backward elimination\nimport statsmodels.formula.api as sm\nregressor_OLS = sm.OLS(y_train, x_train).fit()\n","220fd314":"regressor_OLS.summary()","db75924f":"x_train = x_train.loc[:, ['Pclass', 'Sex', 'Age', 'Fare', 'Title', 'FamilySize']]\nregressor_OLS = sm.OLS(y_train, x_train).fit()\nregressor_OLS.summary()","4c9a576e":"x_train = x_train.loc[:, ['Pclass', 'Sex', 'Fare', 'Title', 'FamilySize']]\nregressor_OLS = sm.OLS(y_train, x_train).fit()\nregressor_OLS.summary()","ac67c63f":"#I am gonna create my own test-set from a training set rather than using existing test-set\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2)","7935cc24":"#Scaling the data\nfrom sklearn.preprocessing import StandardScaler \nscaler = StandardScaler()\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)","00546827":"#Logistic Regression\nlogistic_regression = LogisticRegression()\nlogistic_regression.fit(x_train, y_train)\ny_predict = logistic_regression.predict(x_test)\n#score returns the mean accuracy on the given test data and labels\naccuracy_lr = round(logistic_regression.score(x_train, y_train) * 100, 2)\nprint(\"Logistic Regression Accuracy: \" +str(accuracy_lr))","7971f86a":"from sklearn.metrics import roc_curve, confusion_matrix, auc\ndef evalBinaryClassifier(model,x, y, labels=['Positives','Negatives']):\n    '''\n    Visualize the performance of  a Logistic Regression Binary Classifier.\n    \n    Displays a labelled Confusion Matrix, distributions of the predicted\n    probabilities for both classes, the ROC curve, and F1 score of a fitted\n    Binary Logistic Classifier. Author: gregcondit.com\/articles\/logr-charts\n    \n    Parameters\n    ----------\n    model : fitted scikit-learn model with predict_proba & predict methods\n        and classes_ attribute. Typically LogisticRegression or \n        LogisticRegressionCV\n    \n    x : {array-like, sparse matrix}, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples\n        in the data to be tested, and n_features is the number of features\n    \n    y : array-like, shape (n_samples,)\n        Target vector relative to x.\n    \n    labels: list, optional\n        list of text labels for the two classes, with the positive label first\n        \n    Displays\n    ----------\n    3 Subplots\n    \n    Returns\n    ----------\n    F1: float\n    '''\n    #model predicts probabilities of positive class\n    p = model.predict_proba(x)\n    if len(model.classes_)!=2:\n        raise ValueError('A binary class problem is required')\n    if model.classes_[1] == 1:\n        pos_p = p[:,1]\n    elif model.classes_[0] == 1:\n        pos_p = p[:,0]\n    \n    #FIGURE\n    plt.figure(figsize=[15,4])\n    \n    #1 -- Confusion matrix\n    cm = confusion_matrix(y,model.predict(x))\n    plt.subplot(131)\n    ax = sns.heatmap(cm, annot=True, cmap='Blues', cbar=False, \n                annot_kws={\"size\": 14}, fmt='g')\n    cmlabels = ['True Negatives', 'False Positives',\n              'False Negatives', 'True Positives']\n    for i,t in enumerate(ax.texts):\n        t.set_text(t.get_text() + \"\\n\" + cmlabels[i])\n    plt.title('Confusion Matrix', size=15)\n    plt.xlabel('Predicted Values', size=13)\n    plt.ylabel('True Values', size=13)\n      \n    #2 -- Distributions of Predicted Probabilities of both classes\n    df = pd.DataFrame({'probPos':pos_p, 'target': y})\n    plt.subplot(132)\n    plt.hist(df[df.target==1].probPos, density=True, \n             alpha=.5, color='green',  label=labels[0])\n    plt.hist(df[df.target==0].probPos, density=True, \n             alpha=.5, color='red', label=labels[1])\n    plt.axvline(.5, color='blue', linestyle='--', label='Boundary')\n    plt.xlim([0,1])\n    plt.title('Distributions of Predictions', size=15)\n    plt.xlabel('Positive Probability (predicted)', size=13)\n    plt.ylabel('Samples (normalized scale)', size=13)\n    plt.legend(loc=\"upper right\")\n    \n    #3 -- ROC curve with annotated decision point\n    fp_rates, tp_rates, _ = roc_curve(y,p[:,1])\n    roc_auc = auc(fp_rates, tp_rates)\n    plt.subplot(133)\n    plt.plot(fp_rates, tp_rates, color='green',\n             lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], lw=1, linestyle='--', color='grey')\n    #plot current decision point:\n    tn, fp, fn, tp = [i for i in cm.ravel()]\n    plt.plot(fp\/(fp+tn), tp\/(tp+fn), 'bo', markersize=8, label='Decision Point')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', size=13)\n    plt.ylabel('True Positive Rate', size=13)\n    plt.title('ROC Curve', size=15)\n    plt.legend(loc=\"lower right\")\n    plt.subplots_adjust(wspace=.3)\n    plt.show()\n    #Print and Return the F1 score\n    tn, fp, fn, tp = [i for i in cm.ravel()]\n    precision = tp \/ (tp + fp)\n    recall = tp \/ (tp + fn)\n    F1 = 2*(precision * recall) \/ (precision + recall)\n    printout = (\n        f'Precision: {round(precision,2)} | '\n        f'Recall: {round(recall,2)} | '\n        f'F1 Score: {round(F1,2)} | '\n    )\n    print(printout)\n    return F1","13bb7ae5":"F1 = evalBinaryClassifier(logistic_regression, x_test, y_predict)\n","81e89f07":"data_to_submit = pd.DataFrame({\n    'Survived':randomprediction\n})","1adc3fde":"data_to_submit.to_csv('csv_to_submit.csv', index = False)","a6909076":"Logistic regression is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variable (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. ","eeb136db":"Binning\/Converting Numerical Age to Categorical Variable\n\nfeature vector map ->\nchild: 0\nyoung: 1\nadult: 2\nmid-age: 3\nsenior: 4","510d779d":"Correlation is a way to understand the relationship between multiple variables and attributes in your dataset. Using correlation, you can get some insight such as:\n* One or multiple attributes depend on another attribute or a cause for another attribute.\n* One or multiple attributes are associated with other attributes.\n\n**Negative Correlation**\nNegative correlation means that if feature A increases then feature B decreases and vice versa. \n\n**No Correlation**\nNo relationship between those two attributes.\n\nhttps:\/\/towardsdatascience.com\/why-feature-correlation-matters-a-lot-847e8ba439c4\n","f22766e9":"Age values have some null values. We have to fill those values.","7c809176":"<p>We should remove Age columns from our dataset because P > SL (0.365> 0.05)<\/p>\n<p>After this step we have to fit the model<\/p>","9051a664":"We cleared our dataset. Now we can feed this dataset to a neural network","e8e36f07":"Cabin and Ticket values have some duplicate values. Also, those attributes can't help us to guess Survival rate. Therefore, we can remove Cabin and Ticket attributes from our dataset.","ff92bb69":"Title map - we have to present string as integer so that we can feed neural network\n\nMr: 0\nMiss: 1\nMrs: 2\nOthers: 3","623a7839":"<h2>Confusion Matrix<\/h2>\n<p>The confusion matrix descirbes the predictions that the model as either true (correct) or false (wrong). It compares these tp the real truth. A perfect model would have only True Positives and True Negatives. <\/p>\n<h2>Distributions of Predictions<\/h2>\n<p>The Center Graph is the distribution of predicted probabilities of a Positive Outcome. For example, If your model is 100% sure a sample is positive, if will be in the far right bin. The two different colors indicate the TRUE class, not the predicted class. A perfect model would show no overlap at all between the green and red distributions. A purely random model will see them overlap each other entirely.<\/p>\n<h2>The ROC Curve<\/h2>\n<p>The Receiver Operating Characteristic curve describes all possible decision boundaries. The green curve represents the possibilities, and the trade off between the True Positive Rate and the False Positive Rate at different decision points. The extremes are easy to understand: your model could lazily predict 1 for ALL samples and achieve a perfect True Positive Rate but it would also have a False Positive Rate of 1. Similarly, you could reduce your False Positive rate to zero by lazily predicting everything as Negative, but your True Positive Rate would also be zero. The value in your model is its ability to increase the True Positive Rate faster than it increases the False Positive Rate.<\/p>\n<h5>code and descriptions have taken from the below link:<\/h5>\n<p>https:\/\/medium.com\/@conditg\/how-to-interpret-a-binary-logistic-regressor-with-scikit-learn-6d56c5783b49<\/p>"}}