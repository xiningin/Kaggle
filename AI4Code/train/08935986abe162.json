{"cell_type":{"ee9f342c":"code","9abf4bbb":"code","6fe4f5db":"code","4a619818":"code","7a6ac4f4":"code","8a04b9db":"code","10636f19":"code","110b55bf":"code","a8d3f699":"code","ccfd9f87":"code","7c5d211a":"code","96800237":"code","303bdf62":"code","e06621e9":"code","a09173f7":"code","0bc3d5c5":"code","edbf89b4":"code","e6e29452":"code","32bf069b":"code","ba394d4e":"code","9aa8c9e2":"code","b246b2a4":"code","946f98a7":"code","a74dc550":"code","7caebc75":"code","4c8e6cf3":"code","12525db9":"code","28b25c96":"code","348c2f3f":"code","a95374ff":"code","f01be248":"code","0fc44d75":"code","bdf311db":"code","776d8b35":"code","42a120f4":"code","0999d70f":"code","42b4452f":"code","658f9755":"code","47a8eab7":"markdown","3d0b4d19":"markdown","2fd4466f":"markdown","9b9aa916":"markdown","4e0b8d81":"markdown","9f61daea":"markdown","240c9ef1":"markdown","94dd1737":"markdown","222cd441":"markdown","9e60cd91":"markdown","72410be1":"markdown","2e427ac9":"markdown","f7f31ef5":"markdown","ceeb9c93":"markdown","99e180e9":"markdown","d4c98750":"markdown","ad558a0e":"markdown","8dd00acf":"markdown"},"source":{"ee9f342c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9abf4bbb":"import os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\nwarnings.filterwarnings('ignore')","6fe4f5db":"# import datasets\ntrain_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","4a619818":"print(train_df.shape[0])\nprint(test_df.shape[0])","7a6ac4f4":"train_df.describe()","8a04b9db":"test_df.describe()","10636f19":"missing_train_df = pd.DataFrame(train_df.isna().sum(axis=0))\nmissing_train_df = missing_train_df.drop(['id', 'claim']).reset_index()\nmissing_train_df.columns = ['feature', 'count']\nmissing_train_df['count_percent'] = missing_train_df['count']\/train_df.shape[0]\n\n\nmissing_test_df = pd.DataFrame(test_df.isna().sum(axis=0))\nmissing_test_df = missing_test_df.drop(['id']).reset_index()\nmissing_test_df.columns = ['feature', 'count']\nmissing_test_df['count_percent'] = missing_test_df['count']\/test_df.shape[0]","110b55bf":"missing_train_df.head()","a8d3f699":"missing_test_df.head()","ccfd9f87":"missing_train_row = train_df.drop(['id', 'claim'], axis=1).isna().sum(axis=1)\nmissing_train_feature_numbers = pd.DataFrame(missing_train_row.value_counts()\/train_df.shape[0]).reset_index()\nmissing_train_feature_numbers.columns = ['no_of_feature', 'count_percent']\n\nmissing_test_row = test_df.drop(['id'], axis=1).isna().sum(axis=1)\nmissing_test_feature_numbers = pd.DataFrame(missing_test_row.value_counts()\/test_df.shape[0]).reset_index()\nmissing_test_feature_numbers.columns = ['no_of_feature', 'count_percent']","7c5d211a":"missing_train_row.max()","96800237":"missing_train_feature_numbers.head(missing_train_row.max())","303bdf62":"missing_test_feature_numbers.head(missing_test_row.max())","e06621e9":"fig = plt.figure(figsize=(16, 16))\nax0_sns = sns.barplot(y=missing_train_df['feature'], x=missing_train_df['count_percent'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"missing values\", weight='bold')\nax0_sns.set_ylabel(\"features\", weight='bold')\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE')","a09173f7":"fig = plt.figure(figsize=(16, 16))\nax0_sns = sns.barplot(y=missing_train_feature_numbers['no_of_feature'], x=missing_train_feature_numbers['count_percent'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"missing values\", weight='bold')\nax0_sns.set_ylabel(\"features\", weight='bold')\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)","0bc3d5c5":"fig = plt.figure(figsize=(16, 16))\nax0_sns = sns.barplot(y=missing_test_df['feature'], x=missing_train_df['count_percent'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"missing values\", weight='bold')\nax0_sns.set_ylabel(\"features\", weight='bold')\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE')","edbf89b4":"fig = plt.figure(figsize=(16, 16))\nax0_sns = sns.barplot(y=missing_test_feature_numbers['no_of_feature'], x=missing_test_feature_numbers['count_percent'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"missing values\", weight='bold')\nax0_sns.set_ylabel(\"features\", weight='bold')\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)","e6e29452":"#extract duplicates\ntrain_df.loc[train_df.duplicated(), :]","32bf069b":"train_df['num_nulls'] = train_df.drop(['id', 'claim'], axis = 1).isna().sum(axis = 1)\ntest_df['num_nulls'] = test_df.drop(['id'], axis = 1).isna().sum(axis = 1)","ba394d4e":"train_df['num_nulls'].corr(train_df['claim'])","9aa8c9e2":"train_df.claim.value_counts()","b246b2a4":"sns.countplot(train_df['claim'])","946f98a7":"ctrain_df= train_df.copy() \nctrain_df.drop(['id','num_nulls'], axis=1, inplace=True)\nctrain_df.head()\n","a74dc550":"ctest_df= test_df.copy()\nctest_df.drop(['id','num_nulls'], axis=1, inplace=True)\nctest_df.head()","7caebc75":"# Separate the target variable and rest of the variables \nX, y = ctrain_df.iloc[:,:-1],ctrain_df.iloc[:,-1]","4c8e6cf3":"X.head()","12525db9":"y.head()","28b25c96":"ctest_df.head()","348c2f3f":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import QuantileTransformer,  KBinsDiscretizer\nfrom sklearn.impute import SimpleImputer\n\nfeatures = [col for col in X.columns if col not in ['id']]\npipe = Pipeline([\n        ('imputer', SimpleImputer(strategy='median',missing_values=np.nan)),\n        (\"scaler\", QuantileTransformer(n_quantiles=64,output_distribution='uniform')),\n        ('bin', KBinsDiscretizer(n_bins=64, encode='ordinal',strategy='uniform'))\n        ])\nX[features] = pipe.fit_transform(X[features])\nctest_df[features] = pipe.transform(ctest_df[features])","a95374ff":"X.head()","f01be248":"ctest_df.head()","0fc44d75":"y.head()","bdf311db":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","776d8b35":"# Create the training and test sets\nX_train, X_val, y_train, y_val= train_test_split(X, y, test_size=0.2, random_state=42)\n# Instantiate the XGBClassifier: xg_cl\nxg_cl = xgb.XGBClassifier(objective='binary:logistic', \n                          n_estimators=100, \n                          eval_metric = 'logloss', \n                          learning_rate= 5e-3, \n                          seed=42,  \n                          tree_method ='gpu_hist',\n                          gpu_id =0)\n\n# Fit the classifier to the training set\nxg_cl.fit(X_train,y_train)\n# validate\ny_preds = xg_cl.predict(X_val)\n\n# Predict the labels of the test set: preds\npreds = xg_cl.predict(ctest_df)\n\n# Compute the accuracy: accuracy\naccuracy = float(np.sum(y_preds==y_val))\/y_val.shape[0]\nprint(\"accuracy: %f\" % (accuracy))","42a120f4":"#Visulaizing \nfrom xgboost import plot_importance\n\n# Create the DMatrix: claim_dmatrix\nclaim_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Create the parameter dictionary: params\nparams = {\"objective\":\"reg:logistic\", \"max_depth\":4}\n\n# Train the model: xg_reg\nxg_reg = xgb.train(params=params, dtrain=claim_dmatrix, num_boost_round=10)","0999d70f":"# Plot the feature importances\nxgb.plot_importance(xg_reg, max_num_features=30)\nplt.rcParams['figure.figsize'] = [15, 15]\nplt.show()","42b4452f":"#-------  Visualize Boosting Trees -----------\nxgb.plot_tree(xg_reg,num_trees=0)\nplt.rcParams['figure.figsize'] = [150, 150]\nplt.show()","658f9755":"submission['claim'] = preds.tolist()\nsubmission.to_csv('submission.csv', index=False)\ns=pd.read_csv('.\/submission.csv')\ns.tail(40)","47a8eab7":"Damn!! thats a large correlation . Need to keep this factor. ","3d0b4d19":"Lets check same for test","2fd4466f":"**Start XGBOOST**","9b9aa916":"**Check duplicated**","4e0b8d81":"# Let's see data","9f61daea":"**Preprocessing**","240c9ef1":"See all less than 2%","94dd1737":"**There are no duplicates**","222cd441":"**Maximum number of missing values\/row** ","9e60cd91":"# A lot of missing values. Let's see the correlation","72410be1":"# So now the work remaining is the removal of null values. (Also a bit of Preprocessing) \nBut we can't drop the rows owing to the large amount single null rows","2e427ac9":"# Lets see the visual tables quickly ","f7f31ef5":"# Lets also look if we have imbalance case","ceeb9c93":"Good to go. No Imbalance Class","99e180e9":"**Examine missing data **","d4c98750":"# Load the Data","ad558a0e":"Lets see why people are obsessed with the null counts","8dd00acf":"Well can't throw the null data more than 37% "}}