{"cell_type":{"e224be5a":"code","563f6a57":"code","f7dac6c9":"code","11af0335":"code","2ded2e81":"code","6219d925":"code","fd3821e8":"code","07e3c122":"code","0916fbcc":"code","af0942b5":"code","fc94f020":"code","503e4513":"code","9575a572":"code","3fb5d5f4":"code","36d0fe57":"code","bd0ab50a":"code","a7e43133":"code","de255ec9":"code","65f6bf6d":"code","921bf794":"code","3cf8d809":"code","a1d4fbd0":"code","76f26535":"code","040a3279":"code","91cabc73":"code","35954a97":"code","2d47e1c0":"code","852f8784":"code","e137baa2":"code","a730b967":"code","4e988568":"code","617557a9":"code","3ff77d49":"code","ec523983":"code","48e59925":"code","33a522d3":"code","c62ff04a":"code","38953e78":"code","6c489440":"code","9278c446":"code","d409b8a9":"code","c2418815":"code","650ee7eb":"markdown","f467c266":"markdown","6a4b71c5":"markdown","2e2b0886":"markdown","a8a1b368":"markdown","fdd9fb12":"markdown","99c972f9":"markdown","348d71fb":"markdown","5af6d4a8":"markdown","defcb1c3":"markdown","fb3854b8":"markdown","c0a5bac0":"markdown","3bb0b636":"markdown","a5abaae6":"markdown","358e7e47":"markdown","e0bb1cc8":"markdown","e676a1b6":"markdown","cef791e5":"markdown","df8ee408":"markdown","b6bb7094":"markdown","5a52a063":"markdown","6db9e41a":"markdown","fd0ca060":"markdown","a7bddc3e":"markdown","864aa08e":"markdown","9b2bb15e":"markdown","523c5fb5":"markdown","5a6b71f4":"markdown","5935de1d":"markdown","50c1bfc0":"markdown","c64d9756":"markdown","453198db":"markdown","acce2e74":"markdown","ec80949e":"markdown","fc4527bb":"markdown","afacd494":"markdown","3de149d2":"markdown","0ba9255a":"markdown","383bb5e1":"markdown","79240e29":"markdown"},"source":{"e224be5a":"import os\nimport json\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\n%matplotlib inline\nfrom tqdm.notebook import tqdm # for progress bar\n\nimport cv2\nimport albumentations as A","563f6a57":"train = pd.DataFrame(pd.read_csv(\"..\/input\/cassava-leaf-disease-classification\/train.csv\"))\n\nprint(\"Shape of dataframe = \", train.shape)","f7dac6c9":"train.head()","11af0335":"train.info()","2ded2e81":"PATH_TRAIN_IMAGES = \"..\/input\/cassava-leaf-disease-classification\/train_images\/\" ","6219d925":"def load_image(base_path, image_name) : \n    image_path = base_path + image_name\n    image = cv2.imread(image_path)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    return image","fd3821e8":"def resize(image, image_size) : \n    image = cv2.resize(image, (image_size[0], image_size[1]), interpolation = cv2.INTER_AREA)\n    return image","07e3c122":"random_images = [np.random.choice(os.listdir(PATH_TRAIN_IMAGES)) for i in range(4)]\nrandom_images","0916fbcc":"plt.figure(figsize = (12, 8))\nfor i in range(4) : \n    plt.subplot(2, 2, i+1)\n    plt.title(\"Sampled Image {}\".format(i + 1))\n    image = load_image(PATH_TRAIN_IMAGES, random_images[i])\n    plt.imshow(image, cmap = \"gray\")\n    plt.grid(True)\n    \n# Automatically adjust subplot parameters to give specified padding.\nplt.tight_layout()","af0942b5":"d = dict()\nfor label in tqdm(train[\"label\"].values) : \n    if label not in d : \n        d[label] = 1\n    else:\n        d[label] += 1\n\nprint(d)","fc94f020":"labels = [\"Class0\", \"Class1\", \"Class2\", \"Class3\"]\ncounts = [d[0], d[1], d[2], d[3]]\n\nexplode = (0.05, 0.05, 0.05, 0.05)\nfig, ax = plt.subplots(figsize = (15, 12))\nax.pie(counts, explode = explode, labels = labels, shadow = True, startangle = 90)\nax.axis(\"equal\")","503e4513":"image_ids = train[\"image_id\"].values\nlabels = train[\"label\"].values\n\nred_spread = dict()\ngreen_spread = dict()\nblue_spread = dict()\n\nfor image_id, label in tqdm(zip(image_ids, labels)) : \n    image = load_image(PATH_TRAIN_IMAGES, image_id)\n    mean_red = np.mean(image[:,:, 0])\n    mean_green = np.mean(image[:, :, 1])\n    mean_blue = np.mean(image[:, :, 2])\n        \n    if label not in red_spread : # since we are appending in all three dictionaries together, so if label is not in one of them, it won't be in the rest of them too.\n        red_spread[label] = [mean_red]\n        green_spread[label] = [mean_green]\n        blue_spread[label] = [mean_blue]\n    else:\n        red_spread[label].append(mean_red)\n        green_spread[label].append(mean_green)\n        blue_spread[label].append(mean_blue)","9575a572":"# red\n\nplt.figure(figsize = (20, 12))\n\nplt.subplot(2,2,1)\nplt.title(\"Red Channel Analysis = Class 0\", fontsize = 18)\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(red_spread[0], hist = True, kde = True, label = \"Class0 red channel intensities\", color = \"r\")\nfig.set(xlabel = \"Mean Red Channel Intensities Observed For Each Image\", ylabel = \"Probability Density\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(2,2,2)\nplt.title(\"Red Channel Analysis = Class 1\", fontsize = 18)\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(red_spread[1], hist = True, kde = True, label = \"Class1 red channel intensities\", color = \"r\")\nfig.set(xlabel = \"Mean Red Channel Intensities Observed For Each Image\", ylabel = \"Probability Density\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(2,2,3)\nplt.title(\"Red Channel Analysis = Class 2\", fontsize = 18)\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(red_spread[2], hist = True, kde = True, label = \"Class2 red channel intensities\", color = \"r\")\nfig.set(xlabel = \"Mean Red Channel Intensities Observed For Each Image\", ylabel = \"Probability Density\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(2,2,4)\nplt.title(\"Red Channel Analysis = Class 3\", fontsize = 18)\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(red_spread[3], hist = True, kde = True, label = \"Class3 red channel intensities\", color = \"r\")\nfig.set(xlabel = \"Mean Red Channel Intensities Observed For Each Image\", ylabel = \"Probability Density\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\n\nprint(\"Mean and Standard deviation for class 0 = {}, {}\".format(np.mean(red_spread[0]), np.std(red_spread[0])))\nprint(\"Mean and Standard deviation for class 1 = {}, {}\".format(np.mean(red_spread[1]), np.std(red_spread[1])))\nprint(\"Mean and Standard deviation for class 2 = {}, {}\".format(np.mean(red_spread[2]), np.std(red_spread[2])))\nprint(\"Mean and Standard deviation for class 3 = {}, {}\".format(np.mean(red_spread[3]), np.std(red_spread[3])))","3fb5d5f4":"# green\n\nplt.figure(figsize = (20, 12))\n\nplt.subplot(2,2,1)\nplt.title(\"Green Channel Analysis = Class 0\", fontsize = 18)\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(green_spread[0], hist = True, kde = True, label = \"Class0 green channel intensities\", color = \"g\")\nfig.set(xlabel = \"Mean Green Channel Intensities Observed For Each Image\", ylabel = \"Probability Density\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(2,2,2)\nplt.title(\"Green Channel Analysis = Class 1\", fontsize = 18)\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(green_spread[1], hist = True, kde = True, label = \"Class1 green channel intensities\", color = \"g\")\nfig.set(xlabel = \"Mean Green Channel Intensities Observed For Each Image\", ylabel = \"Probability Density\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(2,2,3)\nplt.title(\"Green Channel Analysis = Class 2\", fontsize = 18)\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(green_spread[2], hist = True, kde = True, label = \"Class2 green channel intensities\", color = \"g\")\nfig.set(xlabel = \"Mean Green Channel Intensities Observed For Each Image\", ylabel = \"Probability Density\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(2,2,4)\nplt.title(\"Green Channel Analysis = Class 3\", fontsize = 18)\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(green_spread[3], hist = True, kde = True, label = \"Class3 green channel intensities\", color = \"g\")\nfig.set(xlabel = \"Mean Green Channel Intensities Observed For Each Image\", ylabel = \"Probability Density\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\n\nprint(\"Mean and Standard deviation for class 0 = {}, {}\".format(np.mean(green_spread[0]), np.std(green_spread[0])))\nprint(\"Mean and Standard deviation for class 1 = {}, {}\".format(np.mean(green_spread[1]), np.std(green_spread[1])))\nprint(\"Mean and Standard deviation for class 2 = {}, {}\".format(np.mean(green_spread[2]), np.std(green_spread[2])))\nprint(\"Mean and Standard deviation for class 3 = {}, {}\".format(np.mean(green_spread[3]), np.std(green_spread[3])))","36d0fe57":"# blue\n\nplt.figure(figsize = (20, 12))\n\nplt.subplot(2,2,1)\nplt.title(\"Blue Channel Analysis = Class 0\", fontsize = 18)\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(blue_spread[0], hist = True, kde = True, label = \"Class0 blue channel intensities\", color = \"b\")\nfig.set(xlabel = \"Mean Blue Channel Intensities Observed For Each Image\", ylabel = \"Probability Density\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(2,2,2)\nplt.title(\"Blue Channel Analysis = Class 1\", fontsize = 18)\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(blue_spread[1], hist = True, kde = True, label = \"Class1 blue channel intensities\", color = \"b\")\nfig.set(xlabel = \"Mean Blue Channel Intensities Observed For Each Image\", ylabel = \"Probability Density\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(2,2,3)\nplt.title(\"Blue Channel Analysis = Class 2\", fontsize = 18)\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(blue_spread[2], hist = True, kde = True, label = \"Class2 blue channel intensities\", color = \"b\")\nfig.set(xlabel = \"Mean Blue Channel Intensities Observed For Each Image\", ylabel = \"Probability Density\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(2,2,4)\nplt.title(\"Blue Channel Analysis = Class 3\", fontsize = 18)\nplt.rc('font', weight='bold')\nsns.set_style(\"whitegrid\")\nfig = sns.distplot(blue_spread[3], hist = True, kde = True, label = \"Class3 blue channel intensities\", color = \"b\")\nfig.set(xlabel = \"Mean Blue Channel Intensities Observed For Each Image\", ylabel = \"Probability Density\")\nplt.grid(True)\nplt.legend()\n\nplt.tight_layout()\n\nprint(\"Mean and Standard deviation for class 0 = {}, {}\".format(np.mean(blue_spread[0]), np.std(blue_spread[0])))\nprint(\"Mean and Standard deviation for class 1 = {}, {}\".format(np.mean(blue_spread[1]), np.std(blue_spread[1])))\nprint(\"Mean and Standard deviation for class 2 = {}, {}\".format(np.mean(blue_spread[2]), np.std(blue_spread[2])))\nprint(\"Mean and Standard deviation for class 3 = {}, {}\".format(np.mean(blue_spread[3]), np.std(blue_spread[3])))","bd0ab50a":"train_image_statistics = pd.DataFrame(pd.read_csv(\"..\/input\/cassavaimagestatistics\/train_image_statistics.csv\"))\n\nprint(\"Shape of metadata file = \", train_image_statistics.shape)","a7e43133":"train_image_statistics.head()","de255ec9":"x = train_image_statistics[\"rows\"]\ny = train_image_statistics[\"columns\"]\n\nplt.figure(figsize = (12, 8))\nplt.subplot(1,1,1)\nplt.scatter(x, y, cmap = \"magma\")\nplt.title(\"Shape Analysis Of Training Images\", fontsize = 18)\nplt.xlabel(\"Number of Rows\", fontsize = 16)\nplt.ylabel(\"Numbe of Columns\", fontsize = 16)\nplt.grid(True)\nplt.minorticks_on()\nplt.grid(which = \"major\", linestyle = \"-\", linewidth = '1.0', color = \"grey\")\nplt.grid(which = \"minor\", linestyle = \":\", linewidth = '0.5', color = \"black\")\nplt.tight_layout()","65f6bf6d":"fig = go.Figure(data = [go.Scatter3d(x = train_image_statistics[\"image_mean\"], \n                                    y = train_image_statistics[\"image_sd\"],\n                                    z = train_image_statistics[\"image_skewness\"],\n                                    mode = \"markers\",\n                                    marker = dict(size = 4, color = train_image_statistics[\"rows\"],\n                                                 colorscale = \"jet\", opacity = 0.4))],\n               layout = go.Layout(margin = dict(l = 0, r = 0, b = 0, t = 0),\n                                 scene = dict(xaxis = dict(title = \"image_mean\"),\n                                             yaxis = dict(title = \"image_sd\"),\n                                             zaxis = dict(title = \"image_skewness\"),),))\nfig.show()","921bf794":"def non_local_means_denoising(image) : \n    denoised_image = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)\n    return denoised_image","3cf8d809":"plt.figure(figsize = (20, 12))\nsample_image = load_image(PATH_TRAIN_IMAGES, random_images[0])\nplt.imshow(sample_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Sample Image\", fontsize = 18)\nplt.axis(\"off\")","a1d4fbd0":"denoised_image = non_local_means_denoising(sample_image)\n\nplt.figure(figsize = (12, 8))\nplt.subplot(1,2,1)\nplt.imshow(sample_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Normal Image\")\n\nplt.subplot(1,2,2)  \nplt.imshow(denoised_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Denoised image\")    \n# Automatically adjust subplot parameters to give specified padding.\nplt.tight_layout()","76f26535":"def sobel_edge_detection(image):\n    sobel_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize = 5)\n    sobel_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize = 5)\n    return sobel_x, sobel_y","040a3279":"s_img_x, s_img_y = sobel_edge_detection(denoised_image)\n\nplt.figure(figsize = (12, 8))\nplt.subplot(2,2,1)\nplt.imshow(sample_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Sample Image\", fontsize = 18)\n\nplt.subplot(2,2,2)\nplt.imshow(denoised_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Denoised Image\", fontsize = 18)\n\nplt.subplot(2,2,3)\nplt.imshow(s_img_x, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Sobel X filtered Image\", fontsize = 18)\n\nplt.subplot(2,2,4)\nplt.imshow(s_img_y, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Sobel Y filtered Image\", fontsize = 18)\n\n# Automatically adjust subplot parameters to give specified padding.\nplt.tight_layout()","91cabc73":"def canny_edge_detection(image) : \n    edges = cv2.Canny(image, 170, 200)\n    return edges","35954a97":"plt.figure(figsize = (12, 8))\nplt.subplot(1,2,1)\nplt.imshow(sample_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Sample Image\", fontsize = 18)\n\nedge_image = canny_edge_detection(sample_image) \n\nplt.subplot(1,2,2)\nplt.imshow(edge_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Canny Edge Image\", fontsize = 18)\n# Automatically adjust subplot parameters to give specified padding.\nplt.tight_layout()","2d47e1c0":"def histogram_equalization(image) : \n    image_ycrcb = cv2.cvtColor(image, cv2.COLOR_RGB2YCR_CB)\n    y_channel = image_ycrcb[:, :, 0] # apply histogram equalization here\n    cr_channel = image_ycrcb[:, :, 1]\n    cb_channel = image_ycrcb[:, :, 2]\n    \n    # local histogram equalization\n    clahe = cv2.createCLAHE(clipLimit = 2.0, tileGridSize = (8, 8))\n    equalized = clahe.apply(y_channel)\n    equalized_image = cv2.merge([equalized, cr_channel, cb_channel])\n    equalized_image = cv2.cvtColor(equalized_image, cv2.COLOR_YCR_CB2RGB)\n    return equalized_image","852f8784":"equalized_denoised_image = histogram_equalization(denoised_image)\n\nplt.figure(figsize = (12, 8))\nplt.subplot(1,2,1)\nplt.imshow(denoised_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"de-Noised Image\", fontsize = 18)\n\nplt.subplot(1,2,2)\nplt.imshow(equalized_denoised_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Histogram Equalized de-Noised Image\", fontsize = 18)  \n# Automatically adjust subplot parameters to give specified padding.\nplt.tight_layout()","e137baa2":"otsu_threshold, otsu_image = cv2.threshold(cv2.cvtColor(equalized_denoised_image, cv2.COLOR_RGB2GRAY), 0, 255, cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n\nplt.figure(figsize = (12, 8))\nplt.subplot(1,2,1)\nplt.imshow(equalized_denoised_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"equalized de Noised Image\", fontsize = 18)\n\nplt.subplot(1,2,2)\nplt.imshow(otsu_image, cmap = \"gray\")\nplt.grid(False)\nplt.title(\"Otsu's Thresholded Image\", fontsize = 18)  \n# Automatically adjust subplot parameters to give specified padding.\n\nplt.tight_layout()","a730b967":"with open(\"..\/input\/cassava-leaf-disease-classification\/label_num_to_disease_map.json\") as file : \n    mapped_classes = json.loads(file.read())\n\nprint(json.dumps(mapped_classes, indent = 1))","4e988568":"d = {\n     0: \"Cassava Bacterial Blight (CBB)\",\n     1: \"Cassava Brown Streak Disease (CBSD)\",\n     2: \"Cassava Green Mottle (CGM)\",\n     3: \"Cassava Mosaic Disease (CMD)\",\n     4: \"Healthy\"\n}","617557a9":"def plot_class(class_label) : \n    image_names = train[train[\"label\"] == class_label]\n    image_ids = image_names[\"image_id\"].values\n    random_images = [np.random.choice(image_ids) for i in range(4)]\n    label = d[class_label]\n    \n    plt.figure(figsize = (12, 8))\n    for i in range(4) :\n        \n        plt.subplot(2, 2, i+1)\n        plt.title(\"{} {}\".format(label, i + 1))\n        image = load_image(PATH_TRAIN_IMAGES, random_images[i])\n        plt.imshow(image, cmap = \"gray\")\n        plt.grid(False)\n    # Automatically adjust subplot parameters to give specified padding.\n    plt.tight_layout()","3ff77d49":"# Cassava Bacterial Blight (CBB)\n\nplot_class(0)","ec523983":"# Cassava Brown Streak Disease (CBSD)\n\nplot_class(1)","48e59925":"# Cassava Green Mottle (CGM)\n\nplot_class(2)","33a522d3":"# Cassava Mosaic Disease (CMD)\n\nplot_class(3)","c62ff04a":"# Healthy\n\nplot_class(4)","38953e78":"def plot_augmentation(image, transform) : \n    plt.figure(figsize = (12, 8))\n    \n    plt.subplot(1, 2, 1)\n    plt.title(\"Sample Image\", fontsize = 18)\n    plt.imshow(image, cmap = \"gray\")\n    plt.grid(False)\n    \n    plt.subplot(1, 2, 2)\n    augmented_image = transform(image = image)[\"image\"] # transform will return a dictionary with a single key image. Value at that key will contain an augmented image.\n    plt.title(\"Augmented Image\", fontsize = 18)\n    plt.imshow(augmented_image, cmap = \"gray\")\n    plt.grid(False)\n    \n    \n    # Automatically adjust subplot parameters to give specified padding.\n    plt.tight_layout()    ","6c489440":"transform = A.Compose([\n    A.RandomCrop(width = 256, height = 256),\n    A.HorizontalFlip(p = 0.5),\n    A.RandomBrightnessContrast(p = 0.2)\n])","9278c446":"plot_augmentation(sample_image, transform)","d409b8a9":"coarse_dropout_transform = A.CoarseDropout(\n    p = 1.0,\n    max_holes = 100,\n    min_holes = 20,\n    max_height = 50,\n    min_height = 20,\n    max_width = 50,\n    min_width = 20\n)","c2418815":"plot_augmentation(sample_image, coarse_dropout_transform)","650ee7eb":"To pass an image to the augmentation pipeline we need to call the transform function. The argument to that function, we need to pass an image that we want to augment.","f467c266":"# Histogram Equalization : \n\nFirst of all, why can we not apply histogram equalization directly to an RGB image?\n\nHistogram equalization is a non-linear process. Channel splitting and equalizing each channel separately is **incorrect**. **Equalization involves intensity values of the image, not the color components**. \n\nSo for a simple RGB color image, histogram equalization cannot be applied directly on the channels. **It needs to be applied in such a way that the intensity values are equalized without disturbing the color balance of the image**. \n\n* So, the **first step** is to convert the color space of the image from RGB into one of the color spaces that separates intensity values from color components. Some of the possible options are HSV\/HLS, YUV, YCbCr, etc. YCbCr is preferred as it is designed for digital images. \n\n* **Perform histogram equalization on the intensity plane Y**. \n* Now **convert** the resultant YCbCr image back to RGB.\n\n(Excerpt taken from :\n\nhttps:\/\/prateekvjoshi.com\/2013\/11\/22\/histogram-equalization-of-rgb-images\/ )","6a4b71c5":"So, all images have the same shape - that's kinda great!! Shape = **600 X 800**.","2e2b0886":"# Analysing the color channel distribution in each Image\n\nWe know that upon diving deeper into an image, we find that **it is actually a 3-D tensor**, which is a fancy way of saying that it is multiple grids stacked one behind the other. Each grid correspond to one color channel. Hence, in a RGB image, we have three such grids(**a grid is also called a 2-D tensor**) corresponding to **Red, Green and Blue** color channels.\n\nAn illustration: \n\n![image.png](attachment:image.png)\n\nNote that it is actually the stacking of three channels, that collectively make up the final image we see.","a8a1b368":"# Image Segmentation(Half-Toned Images) : Otsu's Binarization\n\nIn global thresholding, we used an arbitrary chosen value as a threshold. In contrast, Otsu's method avoids having to choose a value and determines it automatically.\n\nWe will apply Otsu's binarization segmentation method on the histogram equalized image obtained in the previous stage.","fdd9fb12":"**Inference** : \n\n* Class3 is quite in abundance. The dataset is **heavily skewed** when analuzed from class distribution perspective. **This is the popular class imbalance problem**. We will address this problem when in preprocessing segment.\n* Class0 has the least presence in the overall distribution.\n* Classes 1 and 2 have comparable presence.","99c972f9":"**Red Channel Analysis for ALL Classes**","348d71fb":"Observe the change in gray values\/intensities in the second image. That's the result of **local histogram equalization**.","5af6d4a8":"# Cassava Leaf-Disease Identification - EDA cum Image Processing\n\n![image.png](attachment:image.png)\n\nThis book composes of initial **Exploratory Data Analysis(EDA) and Image Processing techniques** that could be employed in the analysis of images, in such competitions. Apart from the competiton dataset, I also used a metadata file from : \n\n**https:\/\/www.kaggle.com\/fireheart7\/cassavaimagestatistics** \n\nThis file encapsulates **pre-compiled statistics for every image in our training set**. If you wish to check on how this metadata is compiled, it's available at : \n\n**https:\/\/www.kaggle.com\/fireheart7\/cassava-metadata-preparation**\n\nThe notebook is still under work, but a significant portion is ready for sharing with the community. Hope you all find it useful here, and in the future competitons.\n\n\n*All the best and let's dive straight in!*","defcb1c3":"Observe carefully. You will nitice the differences at various regions.","fb3854b8":"So, we have images in **4 classes**, hence, we try to analyze the RGB spread of each class. \n\n**So, what's the point you may ask?**\n\n*It's simple. It might happen that their spreads may give us some info on how to segregate\/identify them. For example, it may happen that class0 red channel spread is highly negatively skewed, while that of class1 is high positive. So, this is a stong feature in order to distinguish between class0 and class1 image*. \n\nLet's see how the story unfolds :)","c0a5bac0":"We had loaded some random images previously. Let's select one as our *sample_image* and perform all preprocessing ops on it.","3bb0b636":"# Load Some Images : \n\nWe will randomly sample 4 images from our training data folder, using **numpy.random.choice( )** module.","a5abaae6":"## Augmentation II\n\nFunctions used : \n* **CoarseDropout** : Dropout of the rectangular regions in the image. **This is geat when we don't want our model to look too carefully into images**.\n\nLink : **https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/#albumentations.augmentations.transforms.CoarseDropout**","358e7e47":"# Shape Analysis","e0bb1cc8":"# Load Additional Data For Further EDA\n\nFor this, I prepared additional metadata containing compiled statistics of all training images. \n\nDataset : **https:\/\/www.kaggle.com\/fireheart7\/cassavaimagestatistics**","e676a1b6":"Now, we will define `load_image( )` function which will take the name of the image to be loaded as input from the base directory, and returns the image.\n\n*Note that **openCV** reads image in BGR format by default. Hence, post loading it ought be converted into RGB format*.","cef791e5":"# Class Count Distribution : \n\nLet's count the various classes we have in our dataset. ","df8ee408":"# Image Denoising\n\nMany image smoothing techniques like Gaussian Blurring, Median Blurring etc were **good to some extent in removing small quantities of noise**. In those techniques, *we took a small neighbourhood around a pixel and performed some operations like gaussian weighted average, median of the values etc to replace the central element. In short, noise removal at a pixel was local to its neighbourhood*.\n\n**There is a property of noise. Noise is generally considered to be a random variable with zero mean**.\n\nSuppose we hold a static camera to a certain location for a couple of seconds. This will give us plenty of frames, or a lot of images of the same scene. Then averaging all the frames, we compare the final result and first frame. **Reduction in noise would be easily observed**.\n\nSo idea is simple, we need a set of similar images to average out the noise. Considering a small window (say 5x5 window) in the image, chance is large that the same patch may be somewhere else in the image. Sometimes in a small neighbourhood around it. Hence, using these similar patches together averaging them can lead to an efficient denoised image.\n\n**This method is Non-Local Means Denoising**. It takes more time compared to blurring techniques, but the result are very satisfying.\n\nDenoising illustration :\n\n![image.png](attachment:image.png)","b6bb7094":"We will define a **resizing( )** function which will resize any image passed to it, according to our desired shape.","5a52a063":"Load important dependencies!","6db9e41a":"Thank you for reading this far!","fd0ca060":"**Blue Channel Analysis For ALL Classes**","a7bddc3e":"# Image Augmentation : \n\nWe have near about 20,000 images in our dataset, which is not exactly humongous by deep learning standards. We can apply augmentations to images, in order to expand our dataset for training, further down the line.\n\nWe will use **Albumentation** library, which is a **image augmentation** library.\n\n**Link** : **https:\/\/albumentations.ai\/**\n\nUnlike openCV, Albumentation reads images in **RGB format**.","864aa08e":"## Inference : \n\n**RED Spread** : \n* The red spread cannot be considered as a distinguishing property as for all classes, it is nearly normal.\n\n**Green Spread** :\n* In terms of skweness, green spread's skewness also isn't a powerful factor to distinguish amongst classes.\n\n**Blue Spread** : \n* In terms of skweness, blue spread's skewness also isn't a powerful factor to distinguish amongst classes.\n\n**Couple Of Other Insights** : \n* The **mean value for class0 images** (be it any color channel in RGB) is the lowest among all other classes.\n* The **standard deviation for class1 images** (be it any color channel in RGB) is the highest among all other classes.","9b2bb15e":"# A Quick Look At The Need Of EDA : \n\nWhen we\u2019re getting started with a machine learning (ML) project, one critical principle to keep in mind is that data is everything. It is often said that if ML is the rocket engine, then the fuel is the (high-quality) data fed to ML algorithms. However, deriving truth and insight from a pile of data can be a complicated and error-prone job. To have a solid start for our ML project, it always helps to analyze the data up front.\n\nDuring EDA, it\u2019s important that we get a deep understanding of:\n\n* The properties of the data, such as schema and statistical properties;\n* The quality of the data, like missing values and inconsistent data types;\n* The predictive power of the data, such as correlation of features against target.","523c5fb5":"**Using Sobel filter**\n\nSobel filter takes the following arguments : \n1. Original Image\n2. Depth of the destination image\n3. Order of derivative x\n4. Order of derivative y\n5. Kernel size for convolutions\n\n**f(Image, depth, order_dx, order_dy, kernel_size)** ","5a6b71f4":"# Canny Edge Detector :\n\nThe Canny filter is a multi-stage edge detector. It uses a filter based on the derivative of a Gaussian in order to compute the intensity of the gradients.The Gaussian reduces the effect of noise present in the image. Then, potential edges are thinned down to 1-pixel curves by removing non-maximum pixels of the gradient magnitude. Finally, edge pixels are kept or removed using hysteresis thresholding on the gradient magnitude.\n\nThe Canny has three adjustable parameters: the width of the Gaussian (the noisier the image, the greater the width), and the low and high threshold for the hysteresis thresholding.\n\nThe Canny edge detection algorithm is composed of 5 steps:\n\n* Noise reduction;\n* Gradient calculation;\n* Non-maximum suppression;\n* Double threshold;\n* Edge Tracking by Hysteresis.\n\n\n![image.png](attachment:image.png)\n\n## Noise Reduction :\n\nOne way to get rid of the noise on the image, is by applying Gaussian blur to smooth it. To do so, image convolution technique is applied with a Gaussian Kernel (3x3, 5x5, 7x7 etc\u2026). The kernel size depends on the expected blurring effect. Basically, the smallest the kernel, the less visible is the blur.\n\n## Gradient Calculation :\n\nThe Gradient calculation step detects the edge intensity and direction by calculating the gradient of the image using edge detection operators. **The result is almost the expected one, but we can see that some of the edges are thick and others are thin. Non-Max Suppression step will help us mitigate the thick ones**.\n\n## Non-Maximum Supression :\n\nIdeally, **the final image should have thin edges. Thus, we must perform non-maximum suppression to thin out the edges**.\n\n## Double Threshold :\n\nThe double threshold step aims at **identifying 3 kinds of pixels: strong, weak, and non-relevant**.\n\n* `Strong pixels` are pixels that have an intensity so high that we are sure they contribute to the final edge.\n\n* `Weak pixels` are pixels that have an intensity value that is not enough to be considered as strong ones, but yet not small enough to be considered as non-relevant for the edge detection.\n\n* Other pixels are considered as non-relevant for the edge.\n\nTherefore, the significance of having **two values in double threshold** :\n\n* **High threshold is used to identify the strong pixels (intensity higher than the high threshold)**.\n\n* **Low threshold is used to identify the non-relevant pixels (intensity lower than the low threshold)**.\n\n**All pixels having intensity between both thresholds are flagged as weak and the Hysteresis mechanism (next step) will help us identify the ones that could be considered as strong and the ones that are considered as non-relevant.\n\n## Hysteresis :\n\nBased on the threshold results, **the hysteresis consists of transforming weak pixels into strong ones, if and only if at least one of the pixels around the one being processed is a strong one**.\n\nWe will be using OpenCV's implementation of Canny edge detection. This was the theory involved behind the entire process.\n\nFurther information can be found on OpenCV's documentation : https:\/\/docs.opencv.org\/trunk\/da\/d22\/tutorial_py_canny.html","5935de1d":"# Edge detection Using Sobel filter :\n\nEdge detection is one of the fundamental operation in image processing. Using this, we can reduce the amount of pixels while maintaining the structural aspect of the images.\n\nThe basic operation involved behind edge detection is called Convolution and is illustrated below :\n\n![image.png](attachment:image.png)\n\nEdges can be detected using various kinds of filters.\n\n* First derivative based Sobel filter(for thicker edges).\n* Second derivative based Laplacian filter(for finer edges).\n\nHere, we want to consider the area containing only the leaf, while ignoring the background green. Hence, we use Sobel filter to identify the prominent edge of the leaf.","50c1bfc0":"## Skewness in EDA :\n\nSkewness is the measure of symmetry or asymmetry of a data distribution. A distribution or data set is said to be symmetric if it looks same to the left and right point of the center.\n\nTypes of Skewness :\n\nSkewness is generally classified into 2 broad categories-\n\n* Right skewness or Positive skewness\n* Left skewness or Negative skewness\n\nIt is very difficult to interpret and analyse the data which is skewed.","c64d9756":"While a healthy bunch is clustered together, however we do see potential outliers that **fall way outside the big green cluster**. \n\nIn data cleansing endeavour we might discard these paticular images.","453198db":"Before moving to augmentation pipeline, let's design a general function that prints original as well as augmented image, side-by-side for fair comparison.","acce2e74":"# A Look Into Disease Names :\n\nWe have been provided with a **.json** file mapping the numeric labels to disease names. Let's have a look at disease names.\n\n**Little Insight Into JSON(Javascript Object Notation)** :\nJSON is a script (executable) file which is made of text in a programming language, **is used to store and transfer the data**. Python supports JSON through a built-in package called **json**. The text in JSON is done through quoted-string which contains the value in **key-value mapping within { }**. \n\nIt is similar to the dictionary in Python. ","ec80949e":"**Green Channel Analysis For ALL Classes**","fc4527bb":"## Augmentation Pipeline : \n\nTo define an augmentation pipeline, we need to **create an instance of the `Compose` class**. As an argument to the **Compose class**, **we need to pass a list of augmentations you want to apply**. \n\nA call to Compose will return a transform function that will perform image augmentation.","afacd494":"## OpenCV implementation of the aforementioned approach :\n\n* **cv2.fastNlMeansDenoisingColored()** - Works on Colored images. \n* **cv2.fastNlMeansDenoising()** - Works on grayscale images.\n\nCommon arguments are:\n\n* **h** : parameter deciding filter strength. Higher h value removes noise better, but removes details of image also. (10 is ok). \n* **hForColorComponents** : same as h, but for color images only. (normally same as h). \n* **templateWindowSize** : should be odd. (recommended 7). \n* **searchWindowSize** : should be odd. (recommended 21).","3de149d2":"## Augmentation I\n\nFunctions used : \n\n* **A.RandomCrop** : Receives two parameters, `height` and `width`. A.RandomCrop(width=256, height=256) means that A.RandomCrop will take an input image, extract a random patch with size 256 by 256 pixels from it.\n* **A.HorizontalFlip** : It recieves **one parameter named p**, which controls the probability of applying the augmentation. p=0.5 means that with a probability of 50%, the transform will flip the image horizontally, and with a probability of 50%, the transform won't modify the input image.\n* **A.RandomBrighntessContrast** : It also has one parameter, p. This augmentation will change the brightness and contrast of the image. p has the same meaning as it had in **A.HorizontalFlip**.","0ba9255a":"So, yes no missing labels. We have a complete dataset.","383bb5e1":"Thank you for reading this far!! Have a great journey in this competiton.","79240e29":"Earlier we printed random images, to view them. Now, let's print a couple of images from each class."}}