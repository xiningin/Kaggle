{"cell_type":{"a4079ab4":"code","60d61bde":"code","262bcdd6":"code","5d81d935":"code","801da4a5":"code","0c1b277c":"code","30dc0c84":"code","a6c283f9":"code","50bd2c79":"code","9647471c":"code","52c1000d":"code","83fead8c":"code","e7ff5c2a":"code","ce7425ce":"code","4c134631":"code","287546dd":"code","3215cc12":"code","ca378780":"code","f0bc30cf":"code","b7117b91":"code","3af3dd7b":"code","a3653e59":"code","88983664":"code","f794ff5b":"code","b5f2d3eb":"code","83e4ca59":"code","96547b3e":"code","754b389b":"code","2d96f804":"code","e6bd797d":"code","368587ca":"code","93976516":"code","9fdbe0d1":"code","ded5d746":"code","9aa854d8":"code","52fff811":"code","826a11c2":"code","ed368d50":"code","6fc856c5":"code","a87ac3b8":"code","30cb9341":"code","6ca92b6b":"code","14714885":"code","0685a2c9":"code","18acb3fe":"code","ee7c1424":"code","0f802b37":"code","9e62400e":"code","1d9275d5":"code","747db385":"code","145da13c":"code","f478ba65":"code","365878d5":"code","d7c2d477":"code","36cbcf8a":"code","48ff0f7f":"code","6d4006af":"code","2f749554":"code","fd131f87":"code","3c18cf88":"code","e1de81a2":"code","f94c0ea6":"code","c816412d":"code","e658d512":"code","2b45ad0e":"code","5c2ce398":"code","8cca090c":"code","f762f150":"code","fcee886f":"code","c4f93160":"code","ba4b5970":"code","6b4d8db1":"code","8075dcfc":"code","78ce3001":"code","3007a957":"code","ccdf4417":"code","12555b47":"code","3ebcda13":"code","3d0901a4":"code","4d80af7c":"code","959d1401":"code","ec640cd4":"code","0e758cec":"code","2e238d68":"code","07df0a44":"code","a0373fd5":"code","4830084c":"code","861fcd1f":"code","dd811557":"code","8a05f24c":"code","4e4cf710":"code","099bc57a":"code","d57b0986":"code","71f06852":"code","02d535d4":"code","a9298e98":"code","4bd6a7ab":"code","d8b7235f":"code","6c1d4361":"code","d2c19dd5":"code","3a433daf":"code","07184b26":"code","d61e838f":"code","a1340f61":"code","bdd83f11":"code","a3c1061f":"code","286bbd81":"code","a021ea0c":"code","37767cd6":"code","b15d4973":"code","cb796c8f":"code","e25659c5":"code","e76beab7":"code","921b436b":"code","b4f8b38f":"code","ddb32fec":"code","8b52fd53":"code","32137ff3":"code","3f2190f4":"code","89399520":"code","d54859c4":"code","66f04cd2":"code","3ffb205b":"code","cfbbcc2b":"code","96cdf971":"code","0ad3961d":"code","005ce6a2":"code","bcddd603":"code","a745ea06":"code","6de2936c":"code","14e82839":"code","6c8cb9d0":"code","52f7ada7":"code","5b5a9da2":"code","e8462270":"code","90d18c77":"code","07ccc308":"code","02aa0812":"code","fca913cb":"code","c1da84ba":"code","7cc41f0a":"code","199529fd":"code","358c2277":"code","489fc2a3":"code","f4e6bfb7":"code","9f3b7a12":"code","9e61d15d":"code","33bf1bb5":"code","ee49cb05":"code","7ef39e84":"code","b23f26e4":"code","7edba8d3":"code","75f2c592":"code","6e8965d5":"code","bca5368b":"code","fc7886af":"code","e647e628":"code","6e96bfa6":"code","4dbd2791":"code","e456c4d6":"code","2fd621d9":"code","9cb85f24":"code","8451b8ca":"code","7a1a19f0":"code","0aaa7a43":"code","04cdc7bd":"code","9a92320d":"code","8e858142":"code","f2e843c9":"code","dd758832":"code","16d6b715":"code","78bb95d7":"code","f36bea7b":"code","ea715145":"code","a7a071a4":"code","631e0344":"code","84a51459":"code","e1a97228":"code","8b80026b":"code","5c6801a8":"code","98bf1da1":"code","7c78740a":"code","a6a62fff":"code","ad835d23":"code","b81087d5":"code","a472c966":"code","50c953de":"code","6e411636":"code","620f1032":"code","ada316e4":"code","e9a0089b":"code","cee18af8":"code","910fa44e":"code","2d4a8022":"code","eb51e498":"code","3792816a":"code","325b2d33":"code","bec28e4d":"code","8020caff":"code","6c8afbd7":"code","5c119e76":"code","d13b3e82":"code","0ea47aca":"code","d1518358":"code","fd52d44e":"code","65c3e287":"code","d4c38b0b":"code","c378a0f5":"code","22f109a7":"code","7ac1a207":"code","295bb619":"code","f87cd83a":"code","0d460f87":"code","3f03cde2":"code","de2cf0ab":"code","f878ae03":"code","4898151b":"code","4a9bc310":"code","67cd7905":"code","7f5e121a":"code","6716e99c":"code","3043e90b":"code","ad2e53ca":"code","ae43b17f":"code","fd60ac49":"code","ead2b1f9":"code","ef6eaae0":"code","da052930":"code","e116fb79":"code","893f81ad":"code","69aced14":"code","ba76a451":"code","f3836306":"code","906c335c":"code","4c8600e3":"code","aa6d4eb3":"code","cbff601d":"code","463af85a":"code","74d29b38":"code","586a1db4":"code","3c6b322f":"code","f2bb8cb5":"code","d874ce0e":"code","c9b84850":"code","fdddb2f6":"code","49a0b533":"code","f92458d4":"code","f82aa1e7":"code","29fb0f2c":"code","15599b10":"code","da30a63b":"code","62c70774":"code","975b35a8":"code","258e6af4":"code","0556a347":"code","c74426fd":"code","621721a3":"code","3a0526ac":"code","22366374":"code","73b473f3":"code","bb355cb7":"code","d9e0e889":"code","19a21c76":"code","0f139884":"code","6ee580d3":"code","94b785cf":"code","7612a597":"code","c70d32f2":"code","17d856b7":"code","e57142e5":"code","b929886e":"code","c5aa6620":"code","5199d851":"code","0489e757":"code","9b5ee8bb":"code","8c60b8a3":"code","acacd7ef":"code","83056206":"code","5b4142c8":"code","6d0b5c8b":"code","2a034ed9":"code","5b596f02":"code","56756c64":"code","dad29f99":"code","5c2db0ee":"code","871f6887":"code","8739af44":"code","278c228a":"code","34f55785":"code","25c30f20":"code","e3e53376":"code","7f25732a":"code","df77780a":"code","41d00cdd":"code","0b7c63bc":"code","f6c1ff40":"code","e891819f":"code","32e09656":"code","f056e690":"code","ca4422af":"code","204deaad":"code","dcea4acb":"code","088a062b":"code","a03237bc":"code","d1c0688d":"code","a19bc368":"code","e1d5f306":"markdown","4be3a61c":"markdown","31cd8950":"markdown","98d15a1f":"markdown","9e92e0fe":"markdown","439cfced":"markdown","2a26e504":"markdown","cb7b983c":"markdown","89d12804":"markdown","684c3865":"markdown","28c891bb":"markdown","92caee9f":"markdown","6264f349":"markdown","4e14cdcc":"markdown","d9846de7":"markdown","3fdccbcc":"markdown","26794185":"markdown","2676e8fe":"markdown","5a4d2eb7":"markdown","0757201e":"markdown","b4d16583":"markdown","c7424e5e":"markdown","099c218f":"markdown","970cbc1a":"markdown","f834cf92":"markdown","6b409ae2":"markdown","99e5118a":"markdown","d9deaf88":"markdown","7e601644":"markdown","7e5ea627":"markdown","9ea8c882":"markdown","588fb27d":"markdown","ce23091b":"markdown","ce74b4bc":"markdown","2a9eb731":"markdown","e5bf0862":"markdown","581039e3":"markdown","88cb6d91":"markdown","a3e87a5b":"markdown","2c6ccecc":"markdown","53772dc1":"markdown","ed9d3ea7":"markdown","517e1168":"markdown","d1461f25":"markdown","3593d714":"markdown","bb3dffbc":"markdown","571b287e":"markdown","53e402a8":"markdown","883dde54":"markdown","6e05886b":"markdown","19fb9e64":"markdown","a88bedcf":"markdown","9842158e":"markdown","0d0cef74":"markdown","71c2129e":"markdown","d89798e5":"markdown","16ee4b13":"markdown","7c2bf40f":"markdown","b347966d":"markdown","d26e3fe2":"markdown","44ceb553":"markdown","c214b867":"markdown","131ec7b4":"markdown","db500388":"markdown","db302ae3":"markdown","7f4fc054":"markdown","8e795e51":"markdown","178f7fab":"markdown","c0eea3b7":"markdown","cfa0f30b":"markdown","3e9bb749":"markdown","79887e88":"markdown","fe114726":"markdown","976edf3e":"markdown","c1ee8a2d":"markdown","78c6489d":"markdown","c13122d7":"markdown","89938974":"markdown","142ac060":"markdown","9dffa9de":"markdown","79efcb56":"markdown","8c97ab2c":"markdown","7a913116":"markdown","94ffb4eb":"markdown","55e6a4a0":"markdown","2873ad48":"markdown","1ecba2d6":"markdown","6371d91a":"markdown","9278d507":"markdown","5e033fc3":"markdown","f9dfa233":"markdown","753df590":"markdown","fbc40b78":"markdown","d9d50b63":"markdown","d4f54c92":"markdown","83f00d99":"markdown","41ebf7aa":"markdown","113e9c6f":"markdown","108a270d":"markdown","a7ceb058":"markdown","cd956f43":"markdown","ec73fc4b":"markdown","e4bc2d71":"markdown","a0d6e5a7":"markdown","2ab7e3fa":"markdown","3cadf372":"markdown","3a89e72c":"markdown","6c563260":"markdown","9a8cf267":"markdown","f9eaf4fd":"markdown","e0269214":"markdown","2a06873c":"markdown","dd8c16fa":"markdown","10bd840d":"markdown","0c7614a3":"markdown","5735e2d4":"markdown","7e4d7b86":"markdown","14d2a620":"markdown","424bf14c":"markdown","78ae7a8b":"markdown","5606d45b":"markdown","e0369e72":"markdown","fed9a1a8":"markdown","a61a61f3":"markdown","9413498a":"markdown","f542cc01":"markdown","f94cbb48":"markdown","383d310b":"markdown","866d8b40":"markdown","7b2ce251":"markdown","6d8e6923":"markdown","ca91dce7":"markdown","0a9be52c":"markdown","81ad4adf":"markdown","482dc13f":"markdown","b6227339":"markdown","bcfcce90":"markdown","1bc9dd77":"markdown"},"source":{"a4079ab4":"# system and performance\nimport gc\nimport time\nimport os\nimport pickle\n\n\n# date management\nimport datetime\nimport calendar\n\n\n# data management\nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_rows', 500)\n\nfrom itertools import product\n\n# visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\n# machine learning\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error","60d61bde":"def create_directory(path):\n    if not os.path.isdir(path):\n        os.mkdir(path)\n        print('directory '+path+' created succesfully !')\n    else:\n        print('directory '+path+' already exists')","262bcdd6":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df","5d81d935":"# path to data\nRAW_DATA_FOLDER = '\/kaggle\/input\/competitive-data-science-predict-future-sales\/'\nDATA_FOLDER = '\/kaggle\/working\/'","801da4a5":"loaded=%who_ls\nloaded.append('loaded')","0c1b277c":"all_vars=%who_ls\nall_vars.append('all_vars')\nfor var in list(set(all_vars)-set(loaded)):\n    exec('del '+var)\ndel var","30dc0c84":"%macro reset_variable_space 6\nloaded.append('reset_variable_space')","a6c283f9":"train_df        = pd.read_csv(os.path.join(RAW_DATA_FOLDER, 'sales_train.csv'))\nitems           = pd.read_csv(os.path.join(RAW_DATA_FOLDER, 'items.csv'))\nitem_categories = pd.read_csv(os.path.join(RAW_DATA_FOLDER, 'item_categories.csv'))\nshops           = pd.read_csv(os.path.join(RAW_DATA_FOLDER, 'shops.csv'))\n\ntest_df         = pd.read_csv(os.path.join(RAW_DATA_FOLDER, 'test.csv'))","50bd2c79":"print('items :' + str(items.shape))\nprint()\nitems.info(null_counts=True)\nprint('_'*40)\nprint()\nprint('item_categories :' + str(item_categories.shape))\nprint()\nitem_categories.info(null_counts=True)\nprint('_'*40)\nprint()\nprint('shops :' + str(shops.shape))\nprint()\nshops.info(null_counts=True)","9647471c":"print('train_df :' + str(train_df.shape))\nprint()\ntrain_df.info(null_counts=True)\nprint('_'*40)\nprint()\nprint('test set :' + str(test_df.shape))\nprint()\ntest_df.info(null_counts=True)\nprint('_'*40)\nprint('_'*40)\nprint()\nprint('number of different items in training set : '+str(train_df['item_id'].nunique()) + ' \/ ' + str(items.item_id.nunique()))\nprint('number of different items in test set : '+str(test_df['item_id'].nunique()) + ' \/ ' + str(items.item_id.nunique()))\nprint('number of different items ONLY in test set : '+str(test_df.loc[~test_df['item_id'].isin(train_df['item_id']),'item_id'].nunique()))\nprint()\nprint('number of different shops in training set : '+str(train_df['shop_id'].nunique()) + ' \/ ' + str(shops.shop_id.nunique()) )\nprint('number of different shops in test set : '+str(test_df['shop_id'].nunique()) + ' \/ ' + str(shops.shop_id.nunique()))","52c1000d":"# rename features\ntrain_df.rename({'date_block_num':'month_id', 'item_cnt_day':'item_quantity'},axis=1,inplace=True)","83fead8c":"# convert date feature to datetime type\ntrain_df['date']=pd.DataFrame(pd.to_datetime(train_df['date'],format=\"%d.%m.%Y\"))","e7ff5c2a":"train_df['day_id']=(train_df['date']-train_df['date'].min()).dt.days","ce7425ce":"# discard returned articles (items with quantity < 1)\n# we are only interested in predicting the sales, returned articles are irrelevant to the study\nprint('percentage of realisations that represent returned articles : ' +str(round((train_df['item_quantity']<0).sum()\/(train_df['item_quantity']>=0).sum()*100,2)) + ' %')\ntrain_df.drop(train_df[train_df['item_quantity']<0].index,axis=0,inplace=True)","4c134631":"print('shop  0 : ' + shops.shop_name[0])\nprint('shop 57 : ' + shops.shop_name[57])\nprint()\n\ntmp=train_df[(train_df['shop_id']==0) | (train_df['shop_id']==57)].drop('month_id',axis=1)\n\nprint('## Value counts ##')\nprint(tmp['shop_id'].value_counts())\nprint()\n\nprint('## Activity period ##')\nprint('shop  0 : ' + tmp[tmp.shop_id==0].date.min().strftime(\"%d.%m.%Y\") + ' --- ' + tmp[tmp.shop_id==0].date.max().strftime(\"%d.%m.%Y\"))\nprint('shop 57 : ' + tmp[tmp.shop_id==57].date.min().strftime(\"%d.%m.%Y\") + ' --- ' + tmp[tmp.shop_id==57].date.max().strftime(\"%d.%m.%Y\"))\nprint()\n\nprint('## The period of activities do not overlap and are consecutive !! ##')\nprint('## SHOP 0 = SHOP 57 ##')\n\ndel tmp","287546dd":"print('shop  1 : ' + shops.shop_name[1])\nprint('shop 58 : ' + shops.shop_name[58])\nprint()\n\ntmp=train_df[(train_df['shop_id']==1) | (train_df['shop_id']==58)].drop('month_id',axis=1)\n\nprint('## Value counts ##')\nprint(tmp['shop_id'].value_counts())\nprint()\n\nprint('## Activity period ##')\nprint('shop  1 : ' + tmp[tmp.shop_id==1].date.min().strftime(\"%d.%m.%Y\") + ' --- ' + tmp[tmp.shop_id==1].date.max().strftime(\"%d.%m.%Y\"))\nprint('shop 58 : ' + tmp[tmp.shop_id==58].date.min().strftime(\"%d.%m.%Y\") + ' --- ' + tmp[tmp.shop_id==58].date.max().strftime(\"%d.%m.%Y\"))\nprint()\n\nprint('## The period of activities do not overlap and are consecutive !! ##')\nprint('## SHOP 1 = SHOP 58 ##')\n\ndel tmp","3215cc12":"print('shop 10 : ' + shops.shop_name[10])\nprint('shop 11 : ' + shops.shop_name[11])\nprint()\n\ntmp=train_df[(train_df['shop_id']==10) | (train_df['shop_id']==11)].drop('month_id',axis=1)\n\nprint('## Value counts ##')\nprint(tmp['shop_id'].value_counts())\nprint()\n\nprint('## Activity period ##')\nprint('shop 10 : ' + tmp[tmp.shop_id==10].date.min().strftime(\"%d.%m.%Y\") + ' --- ' + tmp[tmp.shop_id==10].date.max().strftime(\"%d.%m.%Y\"))\nprint('shop 11 : ' + tmp[tmp.shop_id==11].date.min().strftime(\"%d.%m.%Y\") + ' --- ' + tmp[tmp.shop_id==11].date.max().strftime(\"%d.%m.%Y\"))\nprint()\n\nprint('### Count values per shop between 01.02.2015 --- 28.02.2015 ###')\nprint(tmp[(tmp['date'] <= tmp[tmp.shop_id==11].date.max()) & (tmp['date'] >= tmp[tmp.shop_id==11].date.min())]['shop_id'].value_counts())\nprint()\n\nprint('## The period of activities do not overlap !! ##')\nprint('## SHOP 10 = SHOP 11 ##')\n\ndel tmp","ca378780":"print('shop 39 : ' + shops.shop_name[39])\nprint('shop 40 : ' + shops.shop_name[40])\nprint()\n\ntmp=train_df[(train_df['shop_id']==39) | (train_df['shop_id']==40)].drop('month_id',axis=1)\n\nprint('## Value count ##')\nprint(tmp['shop_id'].value_counts())\nprint()\n\nprint('## Each shop individually has no duplicate sale...')\ntmpa=train_df[(train_df['shop_id']==39)].drop('month_id',axis=1)\ntmpb=train_df[(train_df['shop_id']==40)].drop('month_id',axis=1)\nprint('duplicates date\/item (shop 39)      : ' + str(tmpa.duplicated(subset=['date','item_id'],keep=False).sum()))\nprint('duplicates date\/item (shop 40)      : ' + str(tmpb.duplicated(subset=['date','item_id'],keep=False).sum()))\nprint('...but the reunion of both has many...')\nprint('duplicates date\/item                : ' + str(tmp.duplicated(subset=['date','item_id'],keep=False).sum()))\nprint('duplicates date\/item\/price          : ' + str(tmp.duplicated(subset=['date','item_id','item_price'],keep=False).sum()))\nprint('duplicates date\/item\/price\/quantity : ' + str(tmp.duplicated(subset=['date','item_id','item_price','item_quantity'],keep=False).sum()))\nprint('...so all duplicates are exactly paired between the two shops (39,40)! ##')\nprint()\n\nprint('## The period of activities overlap ##')\nprint('shop 39 : ' + tmp[tmp.shop_id==39].date.min().strftime(\"%d.%m.%Y\") + ' --- ' + tmp[tmp.shop_id==39].date.max().strftime(\"%d.%m.%Y\"))\nprint('shop 40 : ' + tmp[tmp.shop_id==40].date.min().strftime(\"%d.%m.%Y\") + ' --- ' + tmp[tmp.shop_id==40].date.max().strftime(\"%d.%m.%Y\"))\nprint()\n\nprint('####################################')\nprint('## Restrict dataframe to overlapping period ##')\ntmpr=tmp[tmp['date']<tmpb['date'].max()]\nprint(tmpr['shop_id'].value_counts())\nprint()\n\nprint('####################################')\npa=np.zeros(22170)\npa[tmpa['item_id'].unique()]=1\npa.astype(int)\npb=np.zeros(22170)\npb[tmpb['item_id'].unique()]=1\npb.astype(int)\n\nprint('number of different items sold in shop 39 : ' + str(sum(pa).astype(int)))\nprint('number of different items sold in shop 40 : ' + str(sum(pb).astype(int)))\nprint('number of different items sold in both shops : ' + str(sum(pa*pb).astype(int)))\nprint()\n\nprint(\"## It is likely that the shop 40 is an 'island' in the commercial center, related to the main shop 39 ##\")\nprint(\"## The island 40 appears to have closed at the end of January 2015 ##\")\nprint()\n\ndel tmp,tmpa,tmpb,tmpr,pa,pb","f0bc30cf":"# shops 0-57, 1-58, 10-11 are in fact the same but on different time periods: relabel similar shops with same id\n# shops 0,1,11 are not present in the test set, but shops 57,58,10 are present so we keep the latter ids instead of the former\n\n# shop 40 is likely to be an antenna of shop 39, so we aggregate their sales together (here we simply relabel shop 40 as shop 39, and we will aggregate the sales when grouping sales by (date,shop,item,price) in the next step)\n# shop 40 is not present in the test set, but shop 39 is present so we keep the latter instead of the former\n\nshops.drop(0,axis=0,inplace=True)\nshops.drop(1,axis=0,inplace=True)\nshops.drop(11,axis=0,inplace=True)\nshops.drop(40,axis=0,inplace=True)\n\ntrain_df.loc[train_df['shop_id']==0,'shop_id']=57\ntrain_df.loc[train_df['shop_id']==1,'shop_id']=58\ntrain_df.loc[train_df['shop_id']==11,'shop_id']=10\ntrain_df.loc[train_df['shop_id']==40,'shop_id']=39","b7117b91":"# group together sales by (date,shop_id,item_id,item_price)\ntrain_df=train_df.groupby(list(train_df.columns.drop('item_quantity')),as_index=False).sum()\ntrain_df.info(null_counts=True)","3af3dd7b":"print('Remaining duplicates in the training set : ' + str(train_df.duplicated(subset=['date','shop_id','item_id','item_price'],keep=False).sum()))","a3653e59":"# create feature city from shop name\nshops['city'] = shops['shop_name'].str.extract('(\\S+)\\s', expand=False)\nprint('number of cities in the dataset : ' + str(shops.city.nunique()))\nprint('number of null cities : ' + str(shops.city.isnull().sum()))\nshops.city.unique()","88983664":"# label encoding of city names\nshops['city_id']=pd.factorize(shops['city'])[0]","f794ff5b":"tmp=train_df[['month_id','shop_id']].groupby('shop_id').agg({'month_id':['min','max','nunique']})\ntmp[('month_id','laps')]=tmp[('month_id','max')]-tmp[('month_id','min')]+1\ntmp=tmp.join(shops[['shop_id','shop_name','city','city_id']].set_index('shop_id'))\ntmp.sort_values(by='city_id',inplace=True)\ntmp","b5f2d3eb":"# the shops are generally open every day of the month\n# on the opening and closing months, the shops are open less days\n# shop 12 is open on average 77% of the days\n\nfig,axes=plt.subplots(1,2,figsize=(15,7))\ntmp=train_df[['date','month_id','shop_id']].groupby(['month_id','shop_id'])['date'].nunique().unstack().T\nsns.heatmap(tmp,ax=axes[0])\nsns.heatmap(tmp<=3,ax=axes[1])","83e4ca59":"print('item 11030 is sold on months : '+str(train_df.loc[(train_df['item_id']==11030),'month_id'].unique()))\nprint('item 20949 is sold on months : '+str(train_df.loc[(train_df['item_id']==20949),'month_id'].unique()))\n\ntrain_df.loc[(train_df['shop_id']==34)&(train_df['month_id']==18)]\n\n# only 2 items are sold on the opening month (month 18) for shop 34, each in only one copy\n# these two items were sold in the past, and are sold later as well... we may just delete these two entries from the dataset!","96547b3e":"train_df.loc[(train_df['shop_id']==33)&(train_df['month_id']==19)]","754b389b":"# restriction to shops that are present in the test set\nfig,axes=plt.subplots(1,2,figsize=(15,7))\ntmp_test=tmp.loc[test_df['shop_id'].unique(),:]\nsns.heatmap(tmp_test,ax=axes[0])\nsns.heatmap(tmp_test<=3,ax=axes[1])","2d96f804":"del tmp, tmp_test\ngc.collect()","e6bd797d":"# shop 9 and 20 are only open on October months (and only a few days within these months)\nprint(shops.loc[shops['shop_id']==9,'shop_name'].values)\nprint('shop 9 is for \"out-bound trade\", and it is only open on months 9,21 and 33')\nprint()\nprint(shops.loc[shops['shop_id']==20,'shop_name'].values)\nprint('shop 20 is for \"Moscow sell-out\" and is only open on months 21 and 33')","368587ca":"# shop 12 and 55 are not physical shops: they appear in the test set so we do not discard them\nprint(shops.loc[shops['shop_id']==12,'shop_name'].values)\nprint('shop 12 is \"emergency online shop\" (appears in the test set as well)')\nprint()\nprint(shops.loc[shops['shop_id']==55,'shop_name'].values)\nprint('shop 55 is \"digital warehouse 1C online\" (appears in the test set as well)')","93976516":"train_df.loc[train_df['shop_id']==9].info(null_counts=True)\nprint()\ntrain_df.loc[train_df['shop_id']==20].info(null_counts=True)\nprint()\ntrain_df.loc[train_df['shop_id']==12].info(null_counts=True)\nprint()\ntrain_df.loc[train_df['shop_id']==55].info(null_counts=True)","9fdbe0d1":"# we remove shops 9 and 20 as they are not open on continuous time periods and they likely behave differently from other shops and are not in the test set\nshops.drop(9,axis=0,inplace=True)\nshops.drop(20,axis=0,inplace=True)\n\ntrain_df.drop(train_df.loc[train_df['shop_id']==9].index,axis=0,inplace=True)\ntrain_df.drop(train_df.loc[train_df['shop_id']==20].index,axis=0,inplace=True)","ded5d746":"# we remove shop 33 from the dataset: it is only open on a short period of time in the middle of the training period, it has 6 full months for 1 opening and 1 closing month...\nshops.drop(33,axis=0,inplace=True)\n\ntrain_df.drop(train_df.loc[train_df['shop_id']==33].index,axis=0,inplace=True)","9aa854d8":"# we remove the two entries for shop 34 on month 18, as only two items were sold in only one copy each\ntrain_df.drop(train_df.loc[(train_df['shop_id']==34)&(train_df['month_id']==18)].index,axis=0,inplace=True)","52fff811":"shops","826a11c2":"# opening days of shops after cleaning\nfig,axes=plt.subplots(1,2,figsize=(15,7))\ntmp=train_df[['date','month_id','shop_id']].groupby(['month_id','shop_id'])['date'].nunique().unstack().T\nsns.heatmap(tmp,ax=axes[0])\nsns.heatmap(tmp<=3,ax=axes[1])","ed368d50":"# collect garbage\ndel tmp\ngc.collect()","6fc856c5":"# Rename categories\nitem_categories.loc[0,'item_category_name']='\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b - PC (\u0413\u0430\u0440\u043d\u0438\u0442\u0443\u0440\u044b\/\u041d\u0430\u0443\u0448\u043d\u0438\u043a\u0438)'\nitem_categories.loc[8,'item_category_name']='\u0411\u0438\u043b\u0435\u0442\u044b - \u0411\u0438\u043b\u0435\u0442\u044b (\u0426\u0438\u0444\u0440\u0430)'\nitem_categories.loc[9,'item_category_name']='\u0414\u043e\u0441\u0442\u0430\u0432\u043a\u0430 \u0442\u043e\u0432\u0430\u0440\u0430 - \u0414\u043e\u0441\u0442\u0430\u0432\u043a\u0430 \u0442\u043e\u0432\u0430\u0440\u0430'\nitem_categories.loc[26,'item_category_name']='\u0418\u0433\u0440\u044b - Android (\u0426\u0438\u0444\u0440\u0430)'\nitem_categories.loc[27,'item_category_name']='\u0418\u0433\u0440\u044b - MAC (\u0426\u0438\u0444\u0440\u0430)'\nitem_categories.loc[28,'item_category_name']='\u0418\u0433\u0440\u044b - PC (\u0414\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0438\u0437\u0434\u0430\u043d\u0438\u044f)'\nitem_categories.loc[29,'item_category_name']='\u0418\u0433\u0440\u044b - PC (\u041a\u043e\u043b\u043b\u0435\u043a\u0446\u0438\u043e\u043d\u043d\u044b\u0435 \u0438\u0437\u0434\u0430\u043d\u0438\u044f)'\nitem_categories.loc[30,'item_category_name']='\u0418\u0433\u0440\u044b - PC (\u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u044b\u0435 \u0438\u0437\u0434\u0430\u043d\u0438\u044f)'\nitem_categories.loc[31,'item_category_name']='\u0418\u0433\u0440\u044b - PC (\u0426\u0438\u0444\u0440\u0430)'\nitem_categories.loc[32,'item_category_name']='\u041a\u0430\u0440\u0442\u044b \u043e\u043f\u043b\u0430\u0442\u044b - \u041a\u0438\u043d\u043e, \u041c\u0443\u0437\u044b\u043a\u0430, \u0418\u0433\u0440\u044b'\nitem_categories.loc[79,'item_category_name']='\u041f\u0440\u0438\u0435\u043c \u0434\u0435\u043d\u0435\u0436\u043d\u044b\u0445 \u0441\u0440\u0435\u0434\u0441\u0442\u0432 \u0434\u043b\u044f 1\u0421-\u041e\u043d\u043b\u0430\u0439\u043d - \u041f\u0440\u0438\u0435\u043c \u0434\u0435\u043d\u0435\u0436\u043d\u044b\u0445 \u0441\u0440\u0435\u0434\u0441\u0442\u0432 \u0434\u043b\u044f 1\u0421-\u041e\u043d\u043b\u0430\u0439\u043d'\nitem_categories.loc[80,'item_category_name']='\u0411\u0438\u043b\u0435\u0442\u044b - \u0411\u0438\u043b\u0435\u0442\u044b'\nitem_categories.loc[81,'item_category_name']='Misc - \u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u043f\u0438\u043b\u044c)'\nitem_categories.loc[82,'item_category_name']='Misc - \u0427\u0438\u0441\u0442\u044b\u0435 \u043d\u043e\u0441\u0438\u0442\u0435\u043b\u0438 (\u0448\u0442\u0443\u0447\u043d\u044b\u0435)'\nitem_categories.loc[83,'item_category_name']='Misc - \u042d\u043b\u0435\u043c\u0435\u043d\u0442\u044b \u043f\u0438\u0442\u0430\u043d\u0438\u044f'","a87ac3b8":"# create feature item_supercategory_name, item_category_console, item_category_is_digital from item_category_name\nitem_categories['item_supercategory_name'] = item_categories['item_category_name'].str.extract('([\\S\\s]+)\\s\\-', expand=False)\nitem_categories['item_category_is_digital']=(item_categories['item_category_name'].str.find('(\u0426\u0438\u0444\u0440\u0430)')>=0)\n\nconsoles=['PS2','PS3','PS4','PSP','PSVita','XBOX 360','XBOX ONE','PC','MAC','Android']\nitem_categories['item_category_console']=''\nfor console in consoles:\n    item_categories['item_category_console']+=item_categories['item_category_name'].str.extract('('+console+')',expand=False).fillna('')\n\nitem_categories.loc[item_categories['item_category_console']=='','item_category_console']='None'\n\nprint('cardinality of the supercategories : ' + str(item_categories['item_supercategory_name'].nunique()))\nprint('items in the null category : ' + str(item_categories['item_supercategory_name'].isnull().sum()))\nprint()\nprint('cardinality of the consoles : ' + str(item_categories['item_category_console'].nunique()))\nprint('items associated with no consoles : ' + str(item_categories['item_category_console'].isnull().sum()))\nprint()\nprint('cardinality of digital items : '+ str(item_categories['item_category_is_digital'].sum()))","30cb9341":"# categories in training and test set\ntrain_categories=sorted(train_df['item_id'].map(items['item_category_id']).unique())\nprint(str(len(train_categories)) + ' categories in the training set :')\nprint(train_categories)\nprint()\n\ntest_categories=sorted(test_df['item_id'].map(items['item_category_id']).unique())\nprint(str(len(test_categories)) + ' categories in the test set :')\nprint(test_categories)\nprint()\n\n# display supercategories\nprint('number of supercategories : '+str(item_categories['item_supercategory_name'].nunique()))\nprint()\nfor supercat in item_categories['item_supercategory_name'].unique():\n    print(supercat + ' : ' +str(item_categories.loc[item_categories['item_supercategory_name']==supercat,'item_category_id'].nunique()))\n    print(item_categories.loc[item_categories['item_supercategory_name']==supercat,['item_category_id','item_category_name']])\n    print()\n    \n# display categories and their features\nitem_categories\n\ndel train_categories, test_categories","6ca92b6b":"train_df['item_category_id']=train_df['item_id'].map(items['item_category_id'])","14714885":"def cat_analyse(cat_id):\n    tmp=train_df.loc[(train_df['item_category_id']==cat_id)]\n\n    print('category '+str(cat_id)+' : ' + str(item_categories.loc[cat_id,'item_category_name']))\n    print('total number of sales : ' + str(tmp.count()[0]))\n    print('dates : ' + tmp.date.min().strftime(\"%d.%m.%Y\") + ' --- ' + tmp.date.max().strftime(\"%d.%m.%Y\"))\n    print('total number of items in the category: ' + str(items.loc[(items['item_category_id']==cat_id)].count()[0]))\n    print('some of the items : ')\n    print(items.loc[items['item_category_id']==cat_id,'item_name'].head(10))\n    print()\n\n    sns.relplot(data=tmp,x='month_id',y='item_quantity',kind='line',estimator=sum,ci=None,marker='o')","0685a2c9":"def cat_compare(cat_id_a,cat_id_b):\n    tmp=train_df.loc[(train_df['item_category_id']==cat_id_a) | (train_df['item_category_id']==cat_id_b)]\n\n    print('category '+str(cat_id_a)+' : ' + str(item_categories.loc[cat_id_a,'item_category_name']))\n    print('total number of sales : ' + str(tmp.loc[(tmp['item_category_id']==cat_id_a)].count()[0]))\n    print('dates : ' + tmp.loc[(tmp['item_category_id']==cat_id_a)].date.min().strftime(\"%d.%m.%Y\") + ' --- ' + tmp.loc[(tmp['item_category_id']==cat_id_a)].date.max().strftime(\"%d.%m.%Y\"))\n    print('total number of items in the category: ' + str(items.loc[(items['item_category_id']==cat_id_a)].count()[0]))\n    print('list of items : ')\n    print(items.loc[items['item_category_id']==cat_id_a,'item_name'])\n    print()\n\n    print('category '+str(cat_id_b)+' : ' + str(item_categories.loc[cat_id_b,'item_category_name']))\n    print('total number of sales : ' + str(tmp.loc[(tmp['item_category_id']==cat_id_b)].count()[0]))\n    print('dates : ' + tmp.loc[(tmp['item_category_id']==cat_id_b)].date.min().strftime(\"%d.%m.%Y\") + ' --- ' + tmp.loc[(tmp['item_category_id']==cat_id_b)].date.max().strftime(\"%d.%m.%Y\"))\n    print('total number of items in the category: ' + str(items.loc[(items['item_category_id']==cat_id_b)].count()[0]))\n    print('list of items : ')\n    print(items.loc[items['item_category_id']==cat_id_b,'item_name'])\n    print()\n\n    sns.relplot(data=tmp,x='month_id',y='item_quantity',col='item_category_id',kind='line',estimator=sum,ci=None,marker='o')","18acb3fe":"# Categories 8 and 80 are only for tickets for the GameWorld conference in October 2014 and 2015\n# A significant part of the items between the two categories are actually duplicates!\ncat_compare(8,80)","ee7c1424":"# Categories 81 and 82 are specific to \"blank media\" (CD-R,CD-RW,DVD-R,DVD-RW)\ncat_compare(81,82)","0f802b37":"# Category 79 is specific to \"Acceptance of funds for 1C-online\"\ncat_analyse(79)","9e62400e":"# Category 9 is specific to \"Delivery\"\n# NB: this category is exclusively sold in shop 12 !\ncat_analyse(9)","1d9275d5":"# Category 0 is specific to \"Headphones\"\ncat_analyse(0)","747db385":"# Category 83 is specific to \"Batteries\"\ncat_analyse(83)","145da13c":"# A significant part of the items between the two categories 8 and 80 seem to actually be duplicates (even though the shops don't match)\n# Categories 8 and 80 are not in the test set so we might as well just ignore them\n\n# Categories 81 and 82 are specific to \"blank media\" (CD-R,CD-RW,DVD-R,DVD-RW)\n# Categories 81 and 82 are not in the test set so we might as well just ignore them\n\nprint('category  8 : ' + str(item_categories.loc[8,'item_category_name']))\nprint('category 80 : ' + str(item_categories.loc[80,'item_category_name']))\nprint('category 81 : ' + str(item_categories.loc[81,'item_category_name']))\nprint('category 82 : ' + str(item_categories.loc[82,'item_category_name']))\n\ntrain_df.drop(train_df.loc[(train_df['item_id'].map(items['item_category_id'])==8)].index.values,axis=0,inplace=True)\ntrain_df.drop(train_df.loc[(train_df['item_id'].map(items['item_category_id'])==80)].index.values,axis=0,inplace=True)\ntrain_df.drop(train_df.loc[(train_df['item_id'].map(items['item_category_id'])==81)].index.values,axis=0,inplace=True)\ntrain_df.drop(train_df.loc[(train_df['item_id'].map(items['item_category_id'])==82)].index.values,axis=0,inplace=True)\n\nitem_categories.drop(8,axis=0,inplace=True)\nitem_categories.drop(80,axis=0,inplace=True)\nitem_categories.drop(81,axis=0,inplace=True)\nitem_categories.drop(82,axis=0,inplace=True)\n\nitems.drop(items.loc[items['item_category_id']==8].index.values,axis=0,inplace=True)\nitems.drop(items.loc[items['item_category_id']==80].index.values,axis=0,inplace=True)\nitems.drop(items.loc[items['item_category_id']==81].index.values,axis=0,inplace=True)\nitems.drop(items.loc[items['item_category_id']==82].index.values,axis=0,inplace=True)","f478ba65":"# label encoding of item_category additional features\nitem_categories['item_supercategory_id']=item_categories['item_supercategory_name'].map({'\u0418\u0433\u0440\u043e\u0432\u044b\u0435 \u043a\u043e\u043d\u0441\u043e\u043b\u0438':0,'\u0418\u0433\u0440\u044b':1,'\u0410\u043a\u0441\u0435\u0441\u0441\u0443\u0430\u0440\u044b':2,'\u0414\u043e\u0441\u0442\u0430\u0432\u043a\u0430 \u0442\u043e\u0432\u0430\u0440\u0430':3,'\u041f\u0440\u0438\u0435\u043c \u0434\u0435\u043d\u0435\u0436\u043d\u044b\u0445 \u0441\u0440\u0435\u0434\u0441\u0442\u0432 \u0434\u043b\u044f 1\u0421-\u041e\u043d\u043b\u0430\u0439\u043d':4,'\u041a\u0430\u0440\u0442\u044b \u043e\u043f\u043b\u0430\u0442\u044b':5,'\u041a\u0438\u043d\u043e':6,'\u041a\u043d\u0438\u0433\u0438':7,'\u041c\u0443\u0437\u044b\u043a\u0430':8,'\u041f\u043e\u0434\u0430\u0440\u043a\u0438':9,'\u041f\u0440\u043e\u0433\u0440\u0430\u043c\u043c\u044b':10,'Misc':11})\nitem_categories['item_category_console_id']=item_categories['item_category_console'].map({console:i for i,console in enumerate(consoles+['None'])})","365878d5":"# reorder columns\noriginal_cols=['item_category_name','item_supercategory_name','item_category_console','item_category_is_digital']\nlabel_cols=['item_category_id','item_supercategory_id','item_category_console_id']\nitem_categories=item_categories[original_cols+label_cols]","d7c2d477":"# join columns to training set\nfor col in original_cols:\n    train_df[col]=train_df['item_category_id'].map(item_categories[col])\n    \ndel original_cols\ndel label_cols","36cbcf8a":"item_categories","48ff0f7f":"# categories in training and test set\ntrain_categories=sorted(train_df['item_id'].map(items['item_category_id']).unique())\nprint(str(len(train_categories)) + ' categories in the training set :')\nprint(train_categories)\nprint()\n\ntest_categories=sorted(test_df['item_id'].map(items['item_category_id']).unique())\nprint(str(len(test_categories)) + ' categories in the test set :')\nprint(test_categories)\nprint()\n\n# display supercategories\nprint('number of supercategories : '+str(item_categories['item_supercategory_name'].nunique()))\nprint()\nfor supercat in item_categories['item_supercategory_name'].unique():\n    print(supercat + ' : ' +str(item_categories.loc[item_categories['item_supercategory_name']==supercat,'item_category_id'].nunique()))\n    print(item_categories.loc[item_categories['item_supercategory_name']==supercat,['item_category_id','item_category_name']])\n    print()\n    \n# display consoles\nprint('number of consoles : '+str(item_categories['item_category_console'].nunique()))\nprint()\nfor console in item_categories['item_category_console'].unique():\n    print(console + ' : ' +str(item_categories.loc[item_categories['item_category_console']==console,'item_category_id'].nunique()))\n    print(item_categories.loc[item_categories['item_category_console']==console,['item_category_id','item_category_name']])\n    print()\n    \n# display categories and their features\nitem_categories\n\ndel train_categories, test_categories","6d4006af":"# collect garbage\ngc.collect()","2f749554":"max_price=train_df['item_price'].max()\nmost_expensive_item=train_df.loc[train_df['item_price']==max_price,'item_id'].values[0]\nprint('index of most expensive item : '+str(most_expensive_item))\nprint('price of most expensive item : '+str(max_price))\nprint('number of times where the most expensive item appears in training set : '+str(train_df.loc[train_df['item_id']==most_expensive_item].count()[0]))\nprint('most expensive item appears in test set : '+str(most_expensive_item in test_df['item_id'].values))\n\n# We drop the outlier\ntrain_df.drop(train_df['item_price'].idxmax(),axis=0,inplace=True)\n\ndel max_price, most_expensive_item","fd131f87":"# Drop the realisation with negative price (there is only one, probably missing value)\nprint('number of realisation with negative prices : '+str((train_df['item_price']<0).sum()))\ntrain_df.drop(train_df['item_price'].idxmin(),axis=0,inplace=True)\nprint('number of realisation with negative prices : '+str((train_df['item_price']<0).sum()))","3c18cf88":"# collect garbage\ngc.collect()","e1de81a2":"# reduce training set to original columns only\ntrain_df=train_df.iloc[:,0:7]\ntrain_df.head()","f94c0ea6":"# downcast dtypes for all dataframes\ndowncast_dtypes(train_df)\ntrain_df['month_id']=train_df['month_id'].astype(np.int8)\ntrain_df['shop_id']=train_df['shop_id'].astype(np.int8)\n\ndowncast_dtypes(shops)\nshops['shop_id']=shops['shop_id'].astype(np.int8)\n\ndowncast_dtypes(items)\nitems['item_category_id']=items['item_category_id'].astype(np.int8)\n\ndowncast_dtypes(item_categories)\nitem_categories['item_category_id']=item_categories['item_category_id'].astype(np.int8)\n\n# collect garbage\ngc.collect()","c816412d":"items.head()","e658d512":"item_categories.head()","2b45ad0e":"shops.head()","5c2ce398":"train_df.head()","8cca090c":"print('TRAINING SET')\nprint()\nprint(train_df.dtypes)\nprint()\nprint('-----------')\nprint('SHOPS')\nprint()\nprint(shops.dtypes)\nprint()\nprint('-----------')\nprint('ITEMS')\nprint()\nprint(items.dtypes)\nprint()\nprint('-----------')\nprint('ITEM_CATEGORIES')\nprint()\nprint(item_categories.dtypes)","f762f150":"# create directory\ncreate_directory(os.path.join(DATA_FOLDER, 'cleaned'))\n\n# export data\ntrain_df.to_pickle(os.path.join(DATA_FOLDER, 'cleaned\/train.pkl'))\nshops.to_pickle(os.path.join(DATA_FOLDER,'cleaned\/shops.pkl'))\nitems.to_pickle(os.path.join(DATA_FOLDER,'cleaned\/items.pkl'))\nitem_categories.to_pickle(os.path.join(DATA_FOLDER,'cleaned\/item_categories.pkl'))","fcee886f":"# clear memory\ndel train_df\ndel shops\ndel items\ndel item_categories\ndel test_df\n\ngc.collect()","c4f93160":"reset_variable_space","ba4b5970":"train_df=pd.read_pickle(os.path.join(DATA_FOLDER,'cleaned\/train.pkl'))\nshops=pd.read_pickle(os.path.join(DATA_FOLDER,'cleaned\/shops.pkl'))\nitems=pd.read_pickle(os.path.join(DATA_FOLDER,'cleaned\/items.pkl'))\nitem_categories=pd.read_pickle(os.path.join(DATA_FOLDER,'cleaned\/item_categories.pkl'))","6b4d8db1":"# drop day identifier\ntrain_df.drop('day_id',axis=1,inplace=True)","8075dcfc":"# aggregate sales data by (month,shop,item)\ncol_agg = ['month_id','shop_id','item_id']\ntrain_agg=train_df.groupby(col_agg).agg(item_quantity=pd.NamedAgg(column='item_quantity',aggfunc='sum'))\n\ntrain_agg.reset_index(inplace=True)\ntrain_agg['month_id'] = train_agg['month_id'].astype(np.int8)\ntrain_agg['shop_id'] = train_agg['shop_id'].astype(np.int8)\ntrain_agg['item_id'] = train_agg['item_id'].astype(np.int16)\n\n# true target values are clipped between 0 and 20, so do the same to training set\ntrain_agg['item_quantity'].clip(0,20,inplace=True)\n\ntrain_agg","78ce3001":"# import and format test set \ntest_df=pd.read_csv(os.path.join(RAW_DATA_FOLDER, 'test.csv'))\ntest_df['ID']=test_df['ID'].astype(np.int32)\ntest_df['shop_id']=test_df['shop_id'].astype(np.int8)\ntest_df['item_id']=test_df['item_id'].astype(np.int16)\ntest_df['month_id']=34\ntest_df=test_df[col_agg]","3007a957":"##################################################################################################################                             #\n# For each month, we keep all pairs (shop,item) such that either shop or item is present in the original dataset #\n##################################################################################################################\n\nts = time.time()\n\n# build full multiindex\ntrain_X = []\nfor i in range(0,34):\n    sales = train_df.loc[train_df['month_id']==i]\n    train_X.append(np.array(list(product([i], sales['shop_id'].unique(), sales['item_id'].unique())), dtype='int16'))\n    \n# build dataframe from multiindex, downcast dtypes, and sort array\ntrain_X = pd.DataFrame(np.vstack(train_X), columns=col_agg)\ntrain_X = pd.concat([train_X,test_df],ignore_index=True)\ntrain_X['month_id'] = train_X['month_id'].astype(np.int8)\ntrain_X['shop_id'] = train_X['shop_id'].astype(np.int8)\ntrain_X['item_id'] = train_X['item_id'].astype(np.int16)\ntrain_X.sort_values(by=col_agg,inplace=True)\n\nprint('time : ' +str(time.time() - ts))\nprint()\nprint(train_X.info(null_counts=True))\n\ndel sales\ngc.collect()","ccdf4417":"# ADD AGGREGATED SALES BY ( month, shop, item ) TO FULL DATAFRAME\nts = time.time()\n\n# join aggregated data\ntrain_X=train_X.join(train_agg.set_index(col_agg),on=col_agg)\n\n# fill missing values for the item_quantity\ntrain_X['item_quantity'].fillna(0,inplace=True)\n\n# NB: the item_quantities for month 34 are all filled with 0 but in reality this quantity is unknown\n\nprint('time : ' +str(time.time() - ts))","12555b47":"del col_agg\ngc.collect()","3ebcda13":"# month of the year\ntrain_X['month']=train_X['month_id']%12+1","3d0901a4":"# number of months since the shop has been opened\ntrain_X['shop_months_since_opening']=train_X['month_id']-train_X['shop_id'].map(train_X[['month_id','shop_id']].groupby('shop_id').min()['month_id'])\n\n# whether the shop is opening this month or not\ntrain_X['shop_opening']=(train_X['shop_months_since_opening']==0)","4d80af7c":"# month where the item has been released (items released at certain times of the year are consistently less popular than those released at other times)\ntrain_X['item_month_id_of_release']=train_X['item_id'].map(train_X[['month_id','item_id']].groupby('item_id').min()['month_id'])\ntrain_X['item_month_of_release']=train_X['item_month_id_of_release']%12+1\n\n# number of months since the item has been released\ntrain_X['item_months_since_release']=train_X['month_id']-train_X['item_month_id_of_release']\ntrain_X['item_months_since_release'].clip(0,12,inplace=True)  # group together items older than a year\n\n# whether the item is new in the catalogue or not\ntrain_X['item_new']=(train_X['item_months_since_release']==0)","959d1401":"# month where the item has first been sold in shop\ntrain_X=train_X.join(train_agg[['shop_id','month_id','item_id']].groupby(['shop_id','item_id']).min().rename({'month_id':'item_month_id_of_first_sale_in_shop'},axis=1),on=['shop_id','item_id'])\ntrain_X['item_month_of_first_sale_in_shop']=train_X['item_month_id_of_first_sale_in_shop']%12+1\n\n# number of months since the item has been released in this shop\ntrain_X['item_months_since_first_sale_in_shop']=(train_X['month_id']-train_X['item_month_id_of_first_sale_in_shop'])\ntrain_X['item_months_since_first_sale_in_shop'].clip(0,12,inplace=True)  # group together items sold for more than a year in the shop\n\n# whether the item has already been sold in this shop before or not\ntrain_X['item_never_sold_in_shop_before']=~(train_X['item_months_since_first_sale_in_shop']>0)\n\n\n\n# set month of release and number of months since the item has been released in this shop to -1 for all items never sold in shop before (remove info from future)\ntrain_X.loc[train_X['item_never_sold_in_shop_before'],'item_months_since_first_sale_in_shop']=-1\ntrain_X.loc[train_X['item_never_sold_in_shop_before'],'item_month_id_of_first_sale_in_shop']=-1\ntrain_X.loc[train_X['item_never_sold_in_shop_before'],'item_month_of_first_sale_in_shop']=-1\n\n# downcast dtype\ntrain_X['item_months_since_first_sale_in_shop']=train_X['item_months_since_first_sale_in_shop'].astype(np.int8)\ntrain_X['item_month_id_of_first_sale_in_shop']=train_X['item_month_id_of_first_sale_in_shop'].astype(np.int8)\ntrain_X['item_month_of_first_sale_in_shop']=train_X['item_month_of_first_sale_in_shop'].astype(np.int8)","ec640cd4":"# indicator for items never sold in this shop, but already sold in other shops in the past\ntrain_X['item_never_sold_in_shop_before_but_not_new']=((1-train_X['item_new'])*train_X['item_never_sold_in_shop_before']).astype(bool)","0e758cec":"# label encode the 3 categories: 0-'new item', 1-'item never sold in shop but not new (ie sold in some other shops in the past)', 2-'item sold in shop in the past'\n# <1  --> item never sold anywhere in the past  |   >=1 --> item sold in at least one shop in the past\n# <2   --> item never sold in this shop before  |   >=2 --> item sold in this shop in the past\n\ntrain_X['item_seniority']=(2-train_X['item_new'].astype(int)-train_X['item_never_sold_in_shop_before'].astype(int)).astype(np.int8)","2e238d68":"###\n# defined only for item_seniority==2\n\n# compute month of most recent sale of item in shop\ntmp_list=[]\nfor mid in train_X['month_id'].unique():\n    tmp=train_agg.loc[train_agg['month_id']<mid,['month_id','shop_id','item_id']].groupby(['shop_id','item_id']).last().rename({'month_id':'item_month_id_of_last_sale_in_shop'},axis=1).astype(np.int16)\n    tmp['month_id']=mid\n    tmp.reset_index(inplace=True)\n    tmp_list.append(tmp)\n    \ntmp=pd.concat(tmp_list)\ndel tmp_list\ntrain_X=train_X.join(tmp.set_index(['month_id','shop_id','item_id']),on=['month_id','shop_id','item_id'])\ndel tmp\n\n# downcast dtype (int not possible due to NaN values)\ntrain_X['item_month_id_of_last_sale_in_shop']=train_X['item_month_id_of_last_sale_in_shop'].astype(np.float16)\n\n# time since last sale in shop\ntrain_X['item_months_since_last_sale_in_shop']=train_X['month_id']-train_X['item_month_id_of_last_sale_in_shop']","07df0a44":"###\n# defined only for item_seniority>=1\n\n# compute month of most recent sale of item over all shops\ntmp_list=[]\nfor mid in train_X['month_id'].unique():\n    tmp=train_agg.loc[train_agg['month_id']<mid,['month_id','item_id']].groupby('item_id').last().rename({'month_id':'item_month_id_of_last_sale'},axis=1)\n    tmp['month_id']=mid\n    tmp.reset_index(inplace=True)\n    tmp_list.append(tmp)\n    \ntmp=pd.concat(tmp_list)\ndel tmp_list\ntrain_X=train_X.join(tmp.set_index(['month_id','item_id']),on=['month_id','item_id'])\ndel tmp\n\n# downcast dtype (int not possible due to NaN values)\ntrain_X['item_month_id_of_last_sale']=train_X['item_month_id_of_last_sale'].astype(np.float16)\n\n# time since last sale over all shops\ntrain_X['item_months_since_last_sale']=train_X['month_id']-train_X['item_month_id_of_last_sale']","a0373fd5":"# binary indicator for whether a pair has resulted in a sale or not (for feature generation later on)\ntrain_X['item_sold_in_shop']=(train_X['item_quantity']>0)","4830084c":"# ADD PERMANENT FEATURES TO FULL DATAFRAME\nts=time.time()\n\ntrain_X=train_X.join(shops.set_index('shop_id'),on='shop_id')\ntrain_X=train_X.join(items.set_index('item_id'),on='item_id')\ntrain_X=train_X.join(item_categories.set_index('item_category_id'),on='item_category_id')\n\nprint('time : ' +str(time.time() - ts))\n\ngc.collect()","861fcd1f":"# Frequency encodings (by month)\nts=time.time()\n\n# NB: items and shops are evenly distributed each month in the global dataset\n\n# monthly fraction of each category in the global catalogue\ntrain_X=train_X.join(train_X.loc[:,['month_id','item_category_id']].groupby('month_id')['item_category_id'].value_counts(normalize=True).rename('item_category_freq',axis=1).astype(np.float32),on=['month_id','item_category_id'])\ntrain_X=train_X.join(train_X.loc[:,['month_id','item_supercategory_id']].groupby('month_id')['item_supercategory_id'].value_counts(normalize=True).rename('item_supercategory_freq',axis=1).astype(np.float32),on=['month_id','item_supercategory_id'])\ntrain_X=train_X.join(train_X.loc[:,['month_id','item_category_console_id']].groupby('month_id')['item_category_console_id'].value_counts(normalize=True).rename('item_category_console_freq',axis=1).astype(np.float32),on=['month_id','item_category_console_id'])\ntrain_X=train_X.join(train_X.loc[:,['month_id','item_category_is_digital']].groupby('month_id')['item_category_is_digital'].value_counts(normalize=True).rename('item_category_digital_freq',axis=1).astype(np.float32),on=['month_id','item_category_is_digital'])\n\nprint('time : ' +str(time.time() - ts))\n\ngc.collect()","dd811557":"# Frequency encodings (by month \/ seniority)\nts=time.time()\n\n# monthly fraction of items and shops in each seniority level\ntrain_X=train_X.join(train_X.loc[:,['month_id','item_seniority','item_id']].groupby(['month_id','item_seniority'])['item_id'].value_counts(normalize=True).rename('item_freq_in_seniority',axis=1).astype(np.float32),on=['month_id','item_seniority','item_id'])\ntrain_X=train_X.join(train_X.loc[:,['month_id','item_seniority','shop_id']].groupby(['month_id','item_seniority'])['shop_id'].value_counts(normalize=True).rename('shop_freq_in_seniority',axis=1).astype(np.float32),on=['month_id','item_seniority','shop_id'])\n\n# monthly fraction of each category in each of the seniority level\ntrain_X=train_X.join(train_X.loc[:,['month_id','item_seniority','item_category_id']].groupby(['month_id','item_seniority'])['item_category_id'].value_counts(normalize=True).rename('item_category_freq_in_seniority',axis=1).astype(np.float32),on=['month_id','item_seniority','item_category_id'])\ntrain_X=train_X.join(train_X.loc[:,['month_id','item_seniority','item_supercategory_id']].groupby(['month_id','item_seniority'])['item_supercategory_id'].value_counts(normalize=True).rename('item_supercategory_freq_in_seniority',axis=1).astype(np.float32),on=['month_id','item_seniority','item_supercategory_id'])\ntrain_X=train_X.join(train_X.loc[:,['month_id','item_seniority','item_category_console_id']].groupby(['month_id','item_seniority'])['item_category_console_id'].value_counts(normalize=True).rename('item_category_console_freq_in_seniority',axis=1).astype(np.float32),on=['month_id','item_seniority','item_category_console_id'])\ntrain_X=train_X.join(train_X.loc[:,['month_id','item_seniority','item_category_is_digital']].groupby(['month_id','item_seniority'])['item_category_is_digital'].value_counts(normalize=True).rename('item_category_digital_freq_in_seniority',axis=1).astype(np.float32),on=['month_id','item_seniority','item_category_is_digital'])\n\nprint('time : ' +str(time.time() - ts))\n\ngc.collect()","8a05f24c":"print(train_X.info(null_counts=True))\ntrain_X.head()","4e4cf710":"# expand sales to multiple rows to make statistics on the prices\ntrain_df_expand=train_df.set_index(['month_id','shop_id','item_id'])['item_price']\ntrain_df_expand=pd.DataFrame(train_df_expand.repeat(train_df['item_quantity']))\ntrain_df_expand.reset_index(inplace=True)\n\n# add values for the month 34 (useful for lagging values later on)\ntrain_df_expand=pd.concat([train_df_expand,test_df],sort=False)\n\ntrain_df_expand['item_category_id']=train_df_expand['item_id'].map(items['item_category_id'])\ntrain_df_expand['item_supercategory_id']=train_df_expand['item_category_id'].map(item_categories['item_supercategory_id'])\ntrain_df_expand['item_category_console_id']=train_df_expand['item_category_id'].map(item_categories['item_category_console_id'])","099bc57a":"def encode_prices(groupby_labels,name):\n    df_prices=train_df_expand.groupby(groupby_labels).agg({'item_price':['min','max','median','mean']})\n    df_prices.columns=[name+'_price_'+col for col in df_prices.columns.get_level_values(1)]\n\n    df_prices.reset_index(inplace=True)\n\n    # downcast dtypes\n    df_prices=downcast_dtypes(df_prices)\n    \n    return df_prices\n\ndef compare_to_super(df,df_super,join_labels,label,label_super,name,name_super,df_mapping):\n    col_list=['price_min','price_max','price_median','price_mean']\n    df[label_super]=df[label].map(df_mapping[label_super])\n    df=df.join(df_super[join_labels+[label_super]+[name_super+'_'+col for col in col_list]].set_index(join_labels+[label_super]),on=join_labels+[label_super])\n    df.drop(label_super,axis=1,inplace=True)\n    for col in col_list:\n        df[name_super+'_'+col]=df[name+'_'+col]\/df[name_super+'_'+col]\n        df.rename({name_super+'_'+col:name+'_'+col+'_compared_to_'+name_super+'_'+col},axis=1,inplace=True)\n\n    # downcast dtypes\n    df=downcast_dtypes(df)\n    \n    return df\n    \ndef compare_to_lower(df,df_lower,label_lower,name,name_lower):\n    col_list=['price_min','price_max','price_median','price_mean']\n    df=df.join(df_lower[['month_id',label_lower]+[name_lower+'_'+col for col in col_list]].set_index(['month_id',label_lower]),on=['month_id',label_lower])\n    for col in col_list:\n        df[name_lower+'_'+col]=df[name+'_'+col]\/df[name_lower+'_'+col]\n        df.rename({name_lower+'_'+col:name+'_'+col+'_compared_to_'+name_lower+'_'+col},axis=1,inplace=True)\n\n    # downcast dtypes\n    df=downcast_dtypes(df)\n    \n    return df","d57b0986":"# price data aggregated by (month,item_supercategory) (over all shops and items)\nsupercategory_prices=encode_prices(['month_id','item_supercategory_id'],'supercategory')\n\nsupercategory_prices['month_id'] = supercategory_prices['month_id'].astype(np.int8)\n\nprint(supercategory_prices.loc[supercategory_prices['month_id']<34,:].info(null_counts=True))\nsupercategory_prices","71f06852":"# price data aggregated by (month,item_category) (over all shops and items)\ncategory_prices=encode_prices(['month_id','item_category_id'],'category')\ncategory_prices=compare_to_super(category_prices,supercategory_prices,['month_id'],'item_category_id','item_supercategory_id','category','supercategory',item_categories)\n\ncategory_prices['month_id'] = category_prices['month_id'].astype(np.int8)\n\nprint(category_prices.loc[category_prices['month_id']<34,:].info(null_counts=True))\ncategory_prices","02d535d4":"# price data aggregated by (month,item) (over all shops)\nitem_prices=encode_prices(['month_id','item_id'],'item')\nitem_prices=compare_to_super(item_prices,category_prices,['month_id'],'item_id','item_category_id','item','category',items)\n\nitem_prices['month_id'] = item_prices['month_id'].astype(np.int8)\n\nprint(item_prices.loc[item_prices['month_id']<34,:].info(null_counts=True))\nitem_prices","a9298e98":"# price data aggregated by (month,shop,item_supercategory) (over all items)\nshop_supercategory_prices=encode_prices(['month_id','shop_id','item_supercategory_id'],'shop_supercategory')\nshop_supercategory_prices=compare_to_lower(shop_supercategory_prices,supercategory_prices,'item_supercategory_id','shop_supercategory','supercategory')\n\nshop_supercategory_prices['month_id'] = shop_supercategory_prices['month_id'].astype(np.int8)\nshop_supercategory_prices['shop_id'] = shop_supercategory_prices['shop_id'].astype(np.int8)\n\nprint(shop_supercategory_prices.loc[shop_supercategory_prices['month_id']<34,:].info(null_counts=True))\nshop_supercategory_prices","4bd6a7ab":"# price data aggregated by (month,shop,item_category) (over all items)\nshop_category_prices=encode_prices(['month_id','shop_id','item_category_id'],'shop_category')\nshop_category_prices=compare_to_lower(shop_category_prices,category_prices,'item_category_id','shop_category','category')\nshop_category_prices=compare_to_super(shop_category_prices,shop_supercategory_prices,['month_id','shop_id'],'item_category_id','item_supercategory_id','shop_category','shop_supercategory',item_categories)\n\nshop_category_prices['month_id'] = shop_category_prices['month_id'].astype(np.int8)\nshop_category_prices['shop_id'] = shop_category_prices['shop_id'].astype(np.int8)\n\nprint(shop_category_prices.loc[shop_category_prices['month_id']<34,:].info(null_counts=True))\nshop_category_prices","d8b7235f":"shop_item_prices=encode_prices(['month_id','shop_id','item_id'],'shop_item')\nshop_item_prices=compare_to_lower(shop_item_prices,item_prices,'item_id','shop_item','item')\nshop_item_prices=compare_to_super(shop_item_prices,shop_category_prices,['month_id','shop_id'],'item_id','item_category_id','shop_item','shop_category',items)\n\n\nshop_item_prices['month_id'] = shop_item_prices['month_id'].astype(np.int8)\nshop_item_prices['shop_id'] = shop_item_prices['shop_id'].astype(np.int8)\n\nprint(shop_item_prices.loc[shop_item_prices['month_id']<34,:].info(null_counts=True))\nshop_item_prices","6c1d4361":"# create directories\ncreate_directory(os.path.join(DATA_FOLDER, 'processed'))\ncreate_directory(os.path.join(DATA_FOLDER, 'processed\/price_features'))\n\n# export data\ntrain_X.to_pickle(os.path.join(DATA_FOLDER,'processed\/train_X0.pkl'))\n\nshop_item_prices.to_pickle(os.path.join(DATA_FOLDER,'processed\/price_features\/shop_item_prices.pkl'))\nitem_prices.to_pickle(os.path.join(DATA_FOLDER,'processed\/price_features\/item_prices.pkl'))\nshop_category_prices.to_pickle(os.path.join(DATA_FOLDER,'processed\/price_features\/shop_category_prices.pkl'))\ncategory_prices.to_pickle(os.path.join(DATA_FOLDER,'processed\/price_features\/category_prices.pkl'))\nshop_supercategory_prices.to_pickle(os.path.join(DATA_FOLDER,'processed\/price_features\/shop_supercategory_prices.pkl'))\nsupercategory_prices.to_pickle(os.path.join(DATA_FOLDER,'processed\/price_features\/supercategory_prices.pkl'))","d2c19dd5":"# clear memory\ndel items\ndel item_categories\ndel shops\ndel train_df\ndel train_df_expand\ndel train_agg\ndel train_X\n\ndel shop_item_prices\ndel item_prices\ndel shop_category_prices\ndel category_prices\ndel shop_supercategory_prices\ndel supercategory_prices\n\n\ngc.collect()","3a433daf":"reset_variable_space","07184b26":"train_X=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/train_X0.pkl'))\n\n# restrict to training set\ntrain_X=train_X[train_X['month_id']<34]","d61e838f":"# Analyse total monthly sales\ntmp_sales=train_X.loc[:,['month_id','shop_id','item_id','item_quantity']]\ntmp_sales['item_quantity']=tmp_sales['item_quantity'].astype(np.float32)\n\ntmp=pd.DataFrame()\ntmp['total_sales_per_month']=tmp_sales[['month_id','item_quantity']].groupby('month_id').sum()['item_quantity']\ntmp['shop_count']=tmp_sales[['month_id','shop_id']].groupby('month_id').nunique()['shop_id']\ntmp['item_count']=tmp_sales[['month_id','item_id']].groupby('month_id').nunique()['item_id']\ntmp['avg_sales_per_shop']=tmp['total_sales_per_month']\/tmp['shop_count']\ntmp['avg_sales_per_item']=tmp['total_sales_per_month']\/tmp['item_count']\ntmp['pair_count']=tmp['shop_count']*tmp['item_count']\ntmp['avg_sales_per_pair']=tmp['total_sales_per_month']\/tmp['pair_count']\n\nfig,axes=plt.subplots(4,2,figsize=(15,20))\n\naxes[0,1].plot(tmp['total_sales_per_month'],'-o')\naxes[0,1].set_xlim(0,34)\naxes[0,1].set_ylim(0,170000)\naxes[0,1].grid(True)\naxes[0,1].set_xlabel('month_id')\naxes[0,1].set_ylabel('total_sales')\n\naxes[1,0].plot(tmp['shop_count'],'-o')\naxes[1,0].set_xlim(0,34)\naxes[1,0].set_ylim(0,60)\naxes[1,0].grid(True)\naxes[1,0].set_xlabel('month_id')\naxes[1,0].set_ylabel('shop_count')\n\naxes[1,1].plot(tmp['avg_sales_per_shop'],'-o')\naxes[1,1].set_xlim(0,34)\naxes[1,1].set_ylim(0,3700)\naxes[1,1].grid(True)\naxes[1,1].set_xlabel('month_id')\naxes[1,1].set_ylabel('avg_sales_per_shop')\n\naxes[2,0].plot(tmp['item_count'],'-o')\naxes[2,0].set_xlim(0,34)\naxes[2,0].set_ylim(0,9000)\naxes[2,0].grid(True)\naxes[2,0].set_xlabel('month_id')\naxes[2,0].set_ylabel('item_count')\n\naxes[2,1].plot(tmp['avg_sales_per_item'],'-o')\naxes[2,1].set_xlim(0,34)\naxes[2,1].set_ylim(0,25)\naxes[2,1].grid(True)\naxes[2,1].set_xlabel('month_id')\naxes[2,1].set_ylabel('avg_sales_per_item')\n\naxes[3,0].plot(tmp['pair_count'],'-o')\naxes[3,0].set_xlim(0,34)\naxes[3,0].set_ylim(0,400000)\naxes[3,0].grid(True)\naxes[3,0].set_xlabel('month_id')\naxes[3,0].set_ylabel('pair_count')\n\naxes[3,1].plot(tmp['avg_sales_per_pair'],'-o')\naxes[3,1].set_xlim(0,34)\naxes[3,1].set_ylim(0,0.5)\naxes[3,1].grid(True)\naxes[3,1].set_xlabel('month_id')\naxes[3,1].set_ylabel('avg_sales_per_pair')\n\ndel tmp_sales,tmp,fig,axes","a1340f61":"# Compare distributions of the target value 'item_quantity' in dataset between all months\ndf_count=train_X[['month_id','item_quantity']].groupby('month_id')['item_quantity'].value_counts()\ndf_count=df_count.unstack()\n\ndf_percentage=train_X[['month_id','item_quantity']].groupby('month_id')['item_quantity'].value_counts(normalize=True)\ndf_percentage=df_percentage.unstack()*100\n\na=df_count.values\nb=df_percentage.values\n\nprint('percentages')\nprint(df_percentage.iloc[:,0:5].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_percentage.iloc[:,5:10].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_percentage.iloc[:,10:15].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_percentage.iloc[:,15:].describe(percentiles=[]).drop('50%',axis=0))\nprint()\nprint('counts')\nprint(df_count.iloc[:,0:4].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,4:8].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,8:12].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,12:16].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,16:].describe(percentiles=[]).drop('50%',axis=0))\n\nfig,axes=plt.subplots(1,2,figsize=(15,5))\naxes[0].plot(b.T,'-o')\naxes[0].plot(b.mean(axis=0),'-ok',linewidth=3)\naxes[0].set_xlabel('target value : item_quantity')\naxes[0].set_ylabel('percentage of total realisations')\naxes[0].grid(True)\naxes[0].set_ylim(0,100)\n\naxes[1].plot(a.T,'-o')\naxes[1].plot(a.mean(axis=0),'-ok',linewidth=3)\naxes[1].set_xlabel('target value : item_quantity')\naxes[1].set_ylabel('number of realisations (log scale)')\naxes[1].grid(True)\naxes[1].set_yscale('log')\n\ndel axes,fig\n\n# The distribution of the target value is similar every month\n# Clipping the values above 20 together will result in about 100-900 realisations in the class item_quantity = 20\n# Then, the amount of realisations with value 19 (lowest populated category after clipping) varies around 10-100","bdd83f11":"# compare the distribution to its mean over all months\n# threshold the comparison at 1 to focus on under\/overshooting values (set all values below threshold to 'threshold')\nb_over=b\/b.mean(axis=0)\nb_under=b\/b.mean(axis=0)\nthres=1       # here threshold=1\nb_over[b_over<thres]=thres\nb_under[b_under>thres]=thres\n\nfig,axes=plt.subplots(1,2,figsize=(15,5))\nsns.heatmap(b_under,ax=axes[0])\naxes[0].set_title('undershooting values')\naxes[0].set_ylabel('month_id')\naxes[0].set_xlabel('target_value')\nsns.heatmap(b_over,ax=axes[1])\naxes[1].set_title('overshooting values')\naxes[1].set_ylabel('month_id')\naxes[1].set_xlabel('target_value')\n\ndel fig\n\n# the distribution of target variable is clearly moved towards the larger values in December\n# in December, the larger values may be up to 2.7 more frequent than on average !!\n# the frequency of the low values varies very little from month to month\n\n# November appears like a fairly average month!","a3c1061f":"del a,b,b_over,b_under,df_count,df_percentage\ngc.collect()","286bbd81":"# repartition of seniority levels in the dataset each month\ntmp=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/train_X0.pkl'))\n\ntmp=tmp[['month_id','item_seniority']].groupby('month_id')['item_seniority'].value_counts(normalize=True).unstack()\n\nprint(tmp)\n\ndel tmp","a021ea0c":"# Assess variations of sales between seniority levels\ntmp=train_X.loc[:,['month_id','item_seniority','item_sold_in_shop']]\n\ntmp['all']=1\n\ntmp['seniority_0']=(tmp['item_seniority']==0)\ntmp['seniority_1']=(tmp['item_seniority']==1)\ntmp['seniority_2']=(tmp['item_seniority']==2)\n\ntmp['sold_0']=tmp['item_sold_in_shop']&tmp['seniority_0']\ntmp['sold_1']=tmp['item_sold_in_shop']&tmp['seniority_1']\ntmp['sold_2']=tmp['item_sold_in_shop']&tmp['seniority_2']\n\n\n\ntmp_count=tmp[['month_id','seniority_0','seniority_1','seniority_2','sold_0','sold_1','sold_2','item_sold_in_shop','all']].groupby('month_id').sum()\n\nfig,axes=plt.subplots(2,2,figsize=(15,10))\naxes[0,0].plot(tmp_count['seniority_0']\/tmp_count['all'],'-o')\naxes[0,0].plot(tmp_count['seniority_1']\/tmp_count['all'],'-o')\naxes[0,0].plot(tmp_count['seniority_2']\/tmp_count['all'],'-o')\naxes[0,0].grid(True)\naxes[0,0].set_ylim(0,1)\naxes[0,0].set_title('proportion of items of each seniority in the dataset')\n\naxes[0,1].plot(tmp_count['item_sold_in_shop']\/tmp_count['all'],'-o')\naxes[0,1].grid(True)\naxes[0,1].set_ylim(0,1)\naxes[0,1].set_title('fraction of items sold overall')\n\naxes[1,0].plot(tmp_count['sold_0']\/tmp_count['item_sold_in_shop'],'-o')\naxes[1,0].plot(tmp_count['sold_1']\/tmp_count['item_sold_in_shop'],'-o')\naxes[1,0].plot(tmp_count['sold_2']\/tmp_count['item_sold_in_shop'],'-o')\naxes[1,0].grid(True)\naxes[1,0].set_ylim(0,1)\naxes[1,0].set_title('contribution of seniority to items sold in shop')\n\naxes[1,1].plot(tmp_count['sold_0']\/tmp_count['seniority_0'],'-o')\naxes[1,1].plot(tmp_count['sold_1']\/tmp_count['seniority_1'],'-o')\naxes[1,1].plot(tmp_count['sold_2']\/tmp_count['seniority_2'],'-o')\naxes[1,1].grid(True)\naxes[1,1].set_ylim(0,1)\naxes[1,1].set_title('fraction of items sold per seniority')\naxes[1,1].legend(['seniority 0','seniority 1','seniority 2'])\n\ndel fig,axes,tmp,tmp_count\n\n# among all items, the pairs (shop,item) are distributed approximately:\n    # ~ 5%  in seniority 0\n    # ~ 40% in seniority 1\n    # ~ 55% in seniority 2\n    \n# among the items sold:\n    # ~5% in seniority 0\n    # ~12% in seniority 1\n    # ~83% in seniority 2\n    \n# every month, the proportion of pairs (shop,item) that are sold is aproximately\n    # ~15% overall\n    # ~15-20% in seniority 0\n    # ~5% in seniority 1\n    # ~20-25% in seniority 2","37767cd6":"# drop first 12 months\ntrain_X.drop(train_X.loc[train_X['month_id']<12].index,axis=0,inplace=True)","b15d4973":"# Compare distributions of the target value 'item_quantity' in dataset between all months\n# RESTRICT TO NEW ITEMS (seniority = 0)\n\nidxbool=train_X['item_new']\ntarget_label='item_quantity'\n\ndf_count=train_X.loc[idxbool,['month_id',target_label]].groupby('month_id')[target_label].value_counts()\ndf_count=df_count.unstack()\n\ndf_percentage=train_X.loc[idxbool,['month_id',target_label]].groupby('month_id')[target_label].value_counts(normalize=True)\ndf_percentage=df_percentage.unstack()*100\n\na=df_count.values\nb=df_percentage.values\n\nprint('percentages')\nprint(df_percentage.iloc[:,0:5].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_percentage.iloc[:,5:10].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_percentage.iloc[:,10:15].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_percentage.iloc[:,15:].describe(percentiles=[]).drop('50%',axis=0))\nprint()\nprint('counts')\nprint(df_count.iloc[:,0:4].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,4:8].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,8:12].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,12:16].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,16:].describe(percentiles=[]).drop('50%',axis=0))\n\nfig,axes=plt.subplots(2,2,figsize=(15,10))\naxes[0,0].plot(b.T,'-o')\naxes[0,0].plot(b.mean(axis=0),'-ok',linewidth=3)\naxes[0,0].set_xlabel('target value : item_quantity')\naxes[0,0].set_ylabel('percentage of total realisations')\naxes[0,0].grid(True)\naxes[0,0].set_ylim(0,100)\n\naxes[0,1].plot(b.T,'-o')\naxes[0,1].plot(b.mean(axis=0),'-ok',linewidth=3)\naxes[0,1].set_xlabel('target value : item_quantity')\naxes[0,1].set_ylabel('percentage of total realisations (log scale)')\naxes[0,1].grid(True)\naxes[0,1].set_yscale('log')\n\naxes[1,0].plot(b,'-o')\naxes[1,0].set_xlabel('month_id')\naxes[1,0].set_ylabel('percentage of total realisations')\naxes[1,0].grid(True)\naxes[1,0].set_ylim(0,100)\n\naxes[1,1].plot(b,'-o')\naxes[1,1].set_xlabel('month_id')\naxes[1,1].set_ylabel('percentage of total realisations (log scale)')\naxes[1,1].grid(True)\naxes[1,1].set_yscale('log')\n\ndel idxbool,target_label,axes,fig,a,b,df_count,df_percentage\n\n# The distribution include significant contribution from quite large values\n# The probability of an item being sold in quantity 13 is still 0.1%","cb796c8f":"# Compare distributions of the target value 'item_quantity' in dataset between all months\n# RESTRICT TO ITEM_NEVER_SOLD_IN_SHOP_BEFORE_BUT_NOT_NEW (seniority = 1)\n\nidxbool=train_X['item_never_sold_in_shop_before_but_not_new']\ntarget_label='item_quantity'\n\ndf_count=train_X.loc[idxbool,['month_id',target_label]].groupby('month_id')[target_label].value_counts()\ndf_count=df_count.unstack()\n\ndf_percentage=train_X.loc[idxbool,['month_id',target_label]].groupby('month_id')[target_label].value_counts(normalize=True)\ndf_percentage=df_percentage.unstack()*100\n\na=df_count.values\nb=df_percentage.values\n\nprint('percentages')\nprint(df_percentage.iloc[:,0:5].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_percentage.iloc[:,5:10].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_percentage.iloc[:,10:15].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_percentage.iloc[:,15:].describe(percentiles=[]).drop('50%',axis=0))\nprint()\nprint('counts')\nprint(df_count.iloc[:,0:4].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,4:8].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,8:12].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,12:16].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,16:].describe(percentiles=[]).drop('50%',axis=0))\n\nfig,axes=plt.subplots(2,2,figsize=(15,10))\naxes[0,0].plot(b.T,'-o')\naxes[0,0].plot(b.mean(axis=0),'-ok',linewidth=3)\naxes[0,0].set_xlabel('target value : item_quantity')\naxes[0,0].set_ylabel('percentage of total realisations')\naxes[0,0].grid(True)\naxes[0,0].set_ylim(0,100)\n\naxes[0,1].plot(b.T,'-o')\naxes[0,1].plot(b.mean(axis=0),'-ok',linewidth=3)\naxes[0,1].set_xlabel('target value : item_quantity')\naxes[0,1].set_ylabel('percentage of total realisations (log scale)')\naxes[0,1].grid(True)\naxes[0,1].set_yscale('log')\n\naxes[1,0].plot(b,'-o')\naxes[1,0].set_xlabel('month_id')\naxes[1,0].set_ylabel('percentage of total realisations')\naxes[1,0].grid(True)\naxes[1,0].set_ylim(0,100)\n\naxes[1,1].plot(b,'-o')\naxes[1,1].set_xlabel('month_id')\naxes[1,1].set_ylabel('percentage of total realisations (log scale)')\naxes[1,1].grid(True)\naxes[1,1].set_yscale('log')\n\ndel idxbool,target_label,axes,fig,a,b,df_count,df_percentage\n\n# 95% of items are NOT sold, and the amount of realisations is rapidly decreasing when the number of items sold increases\n# The probability of an item being sold in amounts superior to 3 is less than 0.1% !","e25659c5":"# Compare distributions of the target value 'item_quantity' in dataset between all months\n# RESTRICT TO ITEM_SOLD_IN_SHOP_BEFORE (seniority = 2)\n\nidxbool=~train_X['item_never_sold_in_shop_before']\ntarget_label='item_quantity'\n\ndf_count=train_X.loc[idxbool,['month_id',target_label]].groupby('month_id')[target_label].value_counts()\ndf_count=df_count.unstack()\n\ndf_percentage=train_X.loc[idxbool,['month_id',target_label]].groupby('month_id')[target_label].value_counts(normalize=True)\ndf_percentage=df_percentage.unstack()*100\n\na=df_count.values\nb=df_percentage.values\n\nprint('percentages')\nprint(df_percentage.iloc[:,0:5].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_percentage.iloc[:,5:10].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_percentage.iloc[:,10:15].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_percentage.iloc[:,15:].describe(percentiles=[]).drop('50%',axis=0))\nprint()\nprint('counts')\nprint(df_count.iloc[:,0:4].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,4:8].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,8:12].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,12:16].describe(percentiles=[]).drop('50%',axis=0))\nprint(df_count.iloc[:,16:].describe(percentiles=[]).drop('50%',axis=0))\n\nfig,axes=plt.subplots(2,2,figsize=(15,10))\naxes[0,0].plot(b.T,'-o')\naxes[0,0].plot(b.mean(axis=0),'-ok',linewidth=3)\naxes[0,0].set_xlabel('target value : item_quantity')\naxes[0,0].set_ylabel('percentage of total realisations')\naxes[0,0].grid(True)\naxes[0,0].set_ylim(0,100)\n\naxes[0,1].plot(b.T,'-o')\naxes[0,1].plot(b.mean(axis=0),'-ok',linewidth=3)\naxes[0,1].set_xlabel('target value : item_quantity')\naxes[0,1].set_ylabel('percentage of total realisations (log scale)')\naxes[0,1].grid(True)\naxes[0,1].set_yscale('log')\n\naxes[1,0].plot(b,'-o')\naxes[1,0].set_xlabel('month_id')\naxes[1,0].set_ylabel('percentage of total realisations')\naxes[1,0].grid(True)\naxes[1,0].set_ylim(0,100)\n\naxes[1,1].plot(b,'-o')\naxes[1,1].set_xlabel('month_id')\naxes[1,1].set_ylabel('percentage of total realisations (log scale)')\naxes[1,1].grid(True)\naxes[1,1].set_yscale('log')\n\ndel idxbool,target_label,axes,fig,a,b,df_count,df_percentage\n\n# The distribution of sales is fairly even from month to month\n# The probability of an item being sold in a given quantity decreases quite smoothly and rapidly with the amount of quantities\n# The probability that an item is sold in quantity 9 is 0.1%","e76beab7":"# Analyse the mean of target value over all months\ntmp_avg=train_X[['month_id','item_quantity']].groupby('month_id').mean()\ntmp_avg.rename({'item_quantity':'overall'},axis=1,inplace=True)\n\nidxbool=(train_X['item_quantity']>0)\ntmp_avg['over_sold']=train_X.loc[idxbool,['month_id','item_quantity']].groupby('month_id').mean()\n\nidxbool=(train_X['item_new'])\ntmp_avg['over_new']=train_X.loc[idxbool,['month_id','item_quantity']].groupby('month_id').mean()\n\nfor i in range(1,13):\n    idxbool=(train_X['item_months_since_release']==i)\n    tmp_avg['over_released_since_'+str(i)+'_months']=train_X.loc[idxbool,['month_id','item_quantity']].groupby('month_id').mean()\n\nidxbool=(train_X['item_never_sold_in_shop_before'])\ntmp_avg['over_never_sold_in_shop_before']=train_X.loc[idxbool,['month_id','item_quantity']].groupby('month_id').mean()\n \nfor i in range(1,13):\n    idxbool=(train_X['item_months_since_first_sale_in_shop']==i)\n    tmp_avg['over_first_sold_in_shop_since_'+str(i)+'_months']=train_X.loc[idxbool,['month_id','item_quantity']].groupby('month_id').mean()\n    \n    \n    \n# display    \nplt.figure(figsize=(15,10))\nsns.heatmap(tmp_avg.T)\nplt.title('average sales')\n\ndel tmp_avg","921b436b":"fig,axes=plt.subplots(3,2,figsize=(15,15))\nsns.heatmap(train_X[['item_month_of_release','item_months_since_release','item_quantity']].groupby(['item_month_of_release','item_months_since_release']).mean()['item_quantity'].unstack(level=0),ax=axes[0,0])\nsns.heatmap(train_X.loc[~train_X['item_never_sold_in_shop_before'],['item_month_of_first_sale_in_shop','item_months_since_first_sale_in_shop','item_quantity']].groupby(['item_month_of_first_sale_in_shop','item_months_since_first_sale_in_shop']).mean()['item_quantity'].unstack(level=0),ax=axes[0,1])\nsns.heatmap(train_X[['month','item_months_since_release','item_quantity']].groupby(['month','item_months_since_release']).mean()['item_quantity'].unstack(level=0),ax=axes[1,0])\nsns.heatmap(train_X.loc[~train_X['item_never_sold_in_shop_before'],['month','item_months_since_first_sale_in_shop','item_quantity']].groupby(['month','item_months_since_first_sale_in_shop']).mean()['item_quantity'].unstack(level=0),ax=axes[1,1])\nsns.heatmap(train_X[['month','item_month_of_release','item_quantity']].groupby(['month','item_month_of_release']).mean()['item_quantity'].unstack(level=0),ax=axes[2,0])\nsns.heatmap(train_X.loc[~train_X['item_never_sold_in_shop_before'],['month','item_month_of_first_sale_in_shop','item_quantity']].groupby(['month','item_month_of_first_sale_in_shop']).mean()['item_quantity'].unstack(level=0),ax=axes[2,1])","b4f8b38f":"tmp=train_X.loc[:,['month_id','shop_id','item_quantity']]\ntmp['sold']=(tmp['item_quantity']>0)\ntmp['count']=(tmp['item_quantity']>=0)\ntmp.drop('item_quantity',axis=1,inplace=True)\ntmp_1=tmp.groupby(['month_id','shop_id'],as_index=False).sum()\ntmp_1['fraction_of_new_items_sold_in_shop']=tmp_1['sold']\/tmp_1['count']\n\n\ntmp=tmp.groupby(['month_id','shop_id']).sum()\ntmp['fraction_of_all_items_sold_in_shop']=tmp['sold']\/tmp['count']\ntmp=tmp['fraction_of_all_items_sold_in_shop'].unstack()\n\n\n\nfig=plt.figure(figsize=(15,5))\nsns.scatterplot(data=tmp_1,x='shop_id',y='fraction_of_new_items_sold_in_shop')\nplt.plot(tmp.columns,tmp.mean(),'ok')\nplt.grid(True)\nplt.title('fraction of all items that are sold each month in each shop (different points for different months)')\nplt.show()\n\ndel tmp, tmp_1, fig\n\n# SOME SHOPS SELL A LARGER PORTION OF THE GLOBAL CATALOGUE THAN OTHERS","ddb32fec":"# clear memory\ndel train_X\n\ngc.collect()","8b52fd53":"reset_variable_space","32137ff3":"train_X=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/train_X0.pkl'))\n\nitems=pd.read_pickle(os.path.join(DATA_FOLDER,'cleaned\/items.pkl'))\nitem_categories=pd.read_pickle(os.path.join(DATA_FOLDER,'cleaned\/item_categories.pkl'))","3f2190f4":"def encoder(agg_func,target_label,groupby_labels,idxbool=None,target_dtype=None):\n    if target_dtype is None:\n        target_dtype=train_X[target_label].dtype\n    if agg_func=='std':\n        if idxbool is None:\n            return train_X.loc[:,groupby_labels+[target_label]].groupby(groupby_labels,as_index=True).agg(agg_func,ddof=0)[target_label].astype(target_dtype)\n        return train_X.loc[idxbool,groupby_labels+[target_label]].groupby(groupby_labels,as_index=True).agg(agg_func,ddof=0)[target_label].astype(target_dtype)\n    else:\n        if idxbool is None:\n            return train_X.loc[:,groupby_labels+[target_label]].groupby(groupby_labels,as_index=True).agg(agg_func)[target_label].astype(target_dtype)\n        return train_X.loc[idxbool,groupby_labels+[target_label]].groupby(groupby_labels,as_index=True).agg(agg_func)[target_label].astype(target_dtype)","89399520":"def encode_comparison(df,df_super,join_labels,target_labels,comparison_labels,new_labels):\n    df=df.join(df_super[join_labels+comparison_labels].set_index(join_labels),on=join_labels)\n    for target_label,cp_label in zip(target_labels,comparison_labels):\n        df[cp_label]=df[target_label]\/df[cp_label]\n    return df.rename({cp_label:new_label for (cp_label,new_label) in zip(comparison_labels,new_labels)},axis=1)","d54859c4":"all_items=(train_X['item_quantity']>=0)\n\n#----------------------------------------------------------------\ndef encoding_0(idxbool,groupby_labels,prefix,suffix):\n    idxbool_sold=idxbool&train_X['item_sold_in_shop']\n    \n    df=pd.DataFrame(encoder('mean','item_quantity',groupby_labels,idxbool)).rename({'item_quantity':prefix+'_avg_sales'+suffix},axis=1)\n    df[prefix+'_fraction_of_items_sold'+suffix]=encoder('mean','item_sold_in_shop',groupby_labels,idxbool,target_dtype=np.float32)\n    df[prefix+'_avg_sales_over_sold'+suffix]=encoder('mean','item_quantity',groupby_labels,idxbool_sold)\n\n    # reset index\n    df.reset_index(inplace=True)\n\n    return df\n\n\n\n#----------------------------------------------------------------\ndef encoding_1(idxbool,groupby_labels,prefix,suffix):\n    idxbool_sold=idxbool&train_X['item_sold_in_shop']\n    \n    df=pd.DataFrame(encoder('mean','item_quantity',groupby_labels,idxbool)).rename({'item_quantity':prefix+'_avg_sales'+suffix},axis=1)\n    df[prefix+'_fraction_of_items_sold'+suffix]=encoder('mean','item_sold_in_shop',groupby_labels,idxbool,target_dtype=np.float32)\n    df[prefix+'_avg_sales_over_sold'+suffix]=encoder('mean','item_quantity',groupby_labels,idxbool_sold)\n\n    df[prefix+'_min_quantity_over_sold'+suffix]=encoder('min','item_quantity',groupby_labels,idxbool_sold)\n    df[prefix+'_max_quantity'+suffix]=encoder('max','item_quantity',groupby_labels,idxbool)\n\n    # reset index\n    df.reset_index(inplace=True)\n\n    return df\n\n\n\ndef add_comparison_super(df,idxbool,groupby_labels,prefix,suffix,df_super,label_super,prefix_super,df_mapping):\n\n    # comparison of features with respect to superfeatures\n    compared_features=['_avg_sales','_fraction_of_items_sold','_avg_sales_over_sold']\n    \n    df[label_super]=df[groupby_labels[1]].map(df_mapping[label_super])\n    df=encode_comparison(df,df_super,['month_id',label_super],[prefix+feature+suffix for feature in compared_features],[prefix_super+feature+suffix for feature in compared_features],[prefix+feature+'_compared_to_'+prefix_super+suffix for feature in compared_features])\n    df.drop(label_super,axis=1,inplace=True)\n    \n    return df\n\n\n\n#----------------------------------------------------------------\ndef add_comparison_lower(df,idxbool,groupby_labels,prefix_1,prefix_2,suffix,df_compare_1,df_compare_2):\n    prefix=prefix_1+'_'+prefix_2\n\n    # comparison to lower order encoding\n    compared_features=['_avg_sales','_fraction_of_items_sold','_avg_sales_over_sold']\n    \n    df=encode_comparison(df,df_compare_1,['month_id',groupby_labels[1]],[prefix+feature+suffix for feature in compared_features],[prefix_1+feature+suffix for feature in compared_features],[prefix+feature+'_compared_to_'+prefix_1+suffix for feature in compared_features])\n    df=encode_comparison(df,df_compare_2,['month_id',groupby_labels[2]],[prefix+feature+suffix for feature in compared_features],[prefix_2+feature+suffix for feature in compared_features],[prefix+feature+'_compared_to_'+prefix_2+suffix for feature in compared_features])\n\n    return df\n\n\ndef encoding_2(idxbool,groupby_labels,prefix_1,prefix_2,suffix,df_compare_1,df_compare_2):\n    prefix=prefix_1+'_'+prefix_2\n    df=encoding_1(idxbool,groupby_labels,prefix,suffix)\n    return add_comparison_lower(df,idxbool,groupby_labels,prefix_1,prefix_2,suffix,df_compare_1,df_compare_2)\n\n\n#----------------------------------------------------------------\ndef encoding_3(idxbool,groupby_labels,prefix_1,prefix_2,prefix_3,suffix):\n    prefix=prefix_1+'_'+prefix_2+'_'+prefix_3\n    df=encoding_1(idxbool,groupby_labels,prefix,suffix)\n\n    return df","66f04cd2":"# functions for display purposes only\n\ndef print_infos(df):\n    print(df[df['month_id']<34].info(null_counts=True))\n\ndef display_1(df,display_label):\n    groupby_labels=list(df.columns[0:2])\n    \n    fig,axes=plt.subplots(2,2,figsize=(15,18))\n    sns.lineplot(data=df[df['month_id']<34],x=groupby_labels[0],y=display_label,hue=groupby_labels[1],marker='o',ax=axes[0,0])\n    sns.lineplot(data=df[df['month_id']<34],x=groupby_labels[0],y=display_label,marker='o',ax=axes[0,1])\n    sns.lineplot(data=df[df['month_id']<34],x=groupby_labels[1],y=display_label,hue=groupby_labels[0],marker='o',ax=axes[1,0])\n    sns.lineplot(data=df[df['month_id']<34],x=groupby_labels[1],y=display_label,marker='o',ax=axes[1,1])\n    for i in range(0,2):\n        for j in range(0,2):\n            axes[i,j].grid(True)\n\ndef display_2(df,display_label):\n    groupby_labels=list(df.columns[0:2])\n    \n    fig,axes=plt.subplots(2,2,figsize=(15,18))\n    sns.lineplot(data=df[df['month_id']<34],x=groupby_labels[0],y=display_label,hue=groupby_labels[1],marker='o',ax=axes[0,0])\n    sns.lineplot(data=df[df['month_id']<34],x=groupby_labels[0],y=display_label,marker='o',ax=axes[0,1])\n    sns.scatterplot(data=df[df['month_id']<34],x=groupby_labels[1],y=display_label,hue=groupby_labels[0],marker='o',ax=axes[1,0])\n    sns.lineplot(data=df[df['month_id']<34],x=groupby_labels[1],y=display_label,marker='o',ax=axes[1,1])\n    for i in range(0,2):\n        for j in range(0,2):\n            axes[i,j].grid(True)\n\ndef display_3(df,display_label):\n    groupby_labels=list(df.columns[0:2])\n    \n    fig,axes=plt.subplots(2,2,figsize=(15,18))\n    sns.lineplot(data=df[df['month_id']<34],x=groupby_labels[0],y=display_label,hue=groupby_labels[1],marker='o',ax=axes[0,0])\n    sns.lineplot(data=df[df['month_id']<34],x=groupby_labels[0],y=display_label,marker='o',ax=axes[0,1])\n    sns.scatterplot(data=df[df['month_id']<34],x=groupby_labels[1],y=display_label,hue=groupby_labels[0],marker='o',ax=axes[1,0])\n    sns.boxplot(data=df[df['month_id']<34],x=groupby_labels[1],y=display_label,ax=axes[1,1])\n\n    axes[0,0].grid(True)\n    axes[0,1].grid(True)\n    axes[1,0].grid(True)\n\ndef display_4(df,display_label):\n    groupby_labels=list(df.columns[1:3])\n    \n    fig,axes=plt.subplots(1,2,figsize=(16,6))\n    tmp=df[groupby_labels+[display_label]].groupby(groupby_labels).mean()[display_label].unstack()\n    sns.heatmap(tmp\/tmp.max(axis=0),ax=axes[0])\n    axes[0].set_title('normalized over rows')\n    sns.heatmap(((tmp.T)\/tmp.max(axis=1)).T,ax=axes[1])\n    axes[1].set_title('normalized over columns')\n    \ndef display_5(df,display_label):\n    groupby_labels=list(df.columns[1:3])\n    \n    fig,axes=plt.subplots(1,2,figsize=(16,3))\n    tmp=df[groupby_labels+[display_label]].groupby(groupby_labels).mean()[display_label].unstack()\n    sns.heatmap(((tmp.T)\/tmp.max(axis=1)),ax=axes[0])\n    axes[0].set_title('normalized over columns')\n    sns.heatmap((tmp\/tmp.max(axis=0)).T,ax=axes[1])\n    axes[1].set_title('normalized over rows')\n    \ndef display_6(df,display_label):\n    groupby_labels=list(df.columns[1:3])\n    tmp=df.drop('month_id',axis=1).groupby(groupby_labels,as_index=False).mean()\n    \n    fig,axes=plt.subplots(2,2,figsize=(15,18))\n    sns.lineplot(data=tmp,x=groupby_labels[1],y=display_label,marker='o',hue=groupby_labels[0],ax=axes[0,0])\n    sns.lineplot(data=tmp,x=groupby_labels[1],y=display_label,marker='o',ax=axes[0,1])\n    sns.scatterplot(data=tmp,x=groupby_labels[0],y=display_label,hue=groupby_labels[1],marker='o',ax=axes[1,0])\n    sns.boxplot(data=tmp,x=groupby_labels[0],y=display_label,ax=axes[1,1])\n\n    axes[0,0].grid(True)\n    axes[0,1].grid(True)\n    axes[1,0].grid(True)","3ffb205b":"# MONTHS\n\n# build dataframes\nmonth_encoding=encoding_0(all_items,['month_id'],'month','')\nmonth_encoding_seniority_0=encoding_0((train_X['item_seniority']==0),['month_id'],'month','_seniority_0')\nmonth_encoding_seniority_1=encoding_0((train_X['item_seniority']==1),['month_id'],'month','_seniority_1')\nmonth_encoding_seniority_2=encoding_0((train_X['item_seniority']==2),['month_id'],'month','_seniority_2')\n\n#----------------------------------------------------------------\n# display\nprint_infos(month_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(month_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(month_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(month_encoding_seniority_2)\nprint()\n\nfig,axes=plt.subplots(1,3,figsize=(15,4))\naxes[0].plot(month_encoding['month_avg_sales'].values[:-1],'-o')\naxes[0].plot(month_encoding_seniority_0['month_avg_sales_seniority_0'].values[:-1],'-o')\naxes[0].plot(month_encoding_seniority_1['month_avg_sales_seniority_1'].values[:-1],'-o')\naxes[0].plot(month_encoding_seniority_2['month_avg_sales_seniority_2'].values[:-1],'-o')\naxes[0].grid(True)\naxes[0].set_ylim(0,1.5)\naxes[0].legend(['overall','seniority 0','seniority 1','seniority 2'])\naxes[0].set_title('average sales')\naxes[0].set_xlabel('month_id')\naxes[1].plot(month_encoding['month_fraction_of_items_sold'].values[:-1],'-o')\naxes[1].plot(month_encoding_seniority_0['month_fraction_of_items_sold_seniority_0'].values[:-1],'-o')\naxes[1].plot(month_encoding_seniority_1['month_fraction_of_items_sold_seniority_1'].values[:-1],'-o')\naxes[1].plot(month_encoding_seniority_2['month_fraction_of_items_sold_seniority_2'].values[:-1],'-o')\naxes[1].grid(True)\naxes[1].set_ylim(0,0.3)\naxes[1].set_title('fraction of items sold')\naxes[1].set_xlabel('month_id')\naxes[2].plot(month_encoding['month_avg_sales_over_sold'].values[:-1],'-o')\naxes[2].plot(month_encoding_seniority_0['month_avg_sales_over_sold_seniority_0'].values[:-1],'-o')\naxes[2].plot(month_encoding_seniority_1['month_avg_sales_over_sold_seniority_1'].values[:-1],'-o')\naxes[2].plot(month_encoding_seniority_2['month_avg_sales_over_sold_seniority_2'].values[:-1],'-o')\naxes[2].grid(True)\naxes[2].set_ylim(0,5)\naxes[2].set_title('average sales over sold')\naxes[2].set_xlabel('month_id')","cfbbcc2b":"# MONTHS SINCE RELEASE-TIME\n\n# build dataframes\nmonths_since_release_encoding=encoding_1(all_items,['month_id','item_months_since_release'],'months_since_release','')\nmonths_since_release_encoding_seniority_0=encoding_1((train_X['item_seniority']==0),['month_id','item_months_since_release'],'months_since_release','_seniority_0')\nmonths_since_release_encoding_seniority_1=encoding_1((train_X['item_seniority']==1),['month_id','item_months_since_release'],'months_since_release','_seniority_1')\nmonths_since_release_encoding_seniority_2=encoding_1((train_X['item_seniority']==2),['month_id','item_months_since_release'],'months_since_release','_seniority_2')\n\n\n\n#----------------------------------------------------------------\n# display\nprint_infos(months_since_release_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(months_since_release_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(months_since_release_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(months_since_release_encoding_seniority_2)\nprint()\n\ndisplay_1(months_since_release_encoding,'months_since_release_avg_sales')","96cdf971":"display_1(months_since_release_encoding_seniority_1,'months_since_release_avg_sales_seniority_1')","0ad3961d":"display_1(months_since_release_encoding_seniority_2,'months_since_release_avg_sales_seniority_2')","005ce6a2":"# MONTH OF RELEASE-TIME\n\n# build dataframes\nmonth_of_release_encoding=encoding_1(all_items,['month_id','item_month_of_release'],'month_of_release','')\nmonth_of_release_encoding_seniority_0=encoding_1((train_X['item_seniority']==0),['month_id','item_month_of_release'],'month_of_release','_seniority_0')\nmonth_of_release_encoding_seniority_1=encoding_1((train_X['item_seniority']==1),['month_id','item_month_of_release'],'month_of_release','_seniority_1')\nmonth_of_release_encoding_seniority_2=encoding_1((train_X['item_seniority']==2),['month_id','item_month_of_release'],'month_of_release','_seniority_2')\n\n\n\n#----------------------------------------------------------------\n# display\nprint_infos(month_of_release_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(month_of_release_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(month_of_release_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(month_of_release_encoding_seniority_2)\nprint()\n\ndisplay_1(month_of_release_encoding,'month_of_release_avg_sales')","bcddd603":"display_1(month_of_release_encoding_seniority_1,'month_of_release_avg_sales_seniority_1')","a745ea06":"display_1(month_of_release_encoding_seniority_2,'month_of_release_avg_sales_seniority_2')","6de2936c":"# MONTHS SINCE FIRST SALE IN SHOP-TIME\n# only seniority=2 has known values\n\n# build dataframes\nmonths_since_first_sale_in_shop_encoding=encoding_1(all_items,['month_id','item_months_since_first_sale_in_shop'],'months_since_first_sale_in_shop','')\nmonths_since_first_sale_in_shop_encoding_seniority_0=encoding_1((train_X['item_seniority']==0),['month_id','item_months_since_first_sale_in_shop'],'months_since_first_sale_in_shop','_seniority_0')\nmonths_since_first_sale_in_shop_encoding_seniority_1=encoding_1((train_X['item_seniority']==1),['month_id','item_months_since_first_sale_in_shop'],'months_since_first_sale_in_shop','_seniority_1')\nmonths_since_first_sale_in_shop_encoding_seniority_2=encoding_1((train_X['item_seniority']==2),['month_id','item_months_since_first_sale_in_shop'],'months_since_first_sale_in_shop','_seniority_2')\n\n# erase data from the future\nmonths_since_first_sale_in_shop_encoding.loc[months_since_first_sale_in_shop_encoding['item_months_since_first_sale_in_shop']==-1,months_since_first_sale_in_shop_encoding.columns[2:]]=-1\nmonths_since_first_sale_in_shop_encoding_seniority_0.loc[months_since_first_sale_in_shop_encoding_seniority_0['item_months_since_first_sale_in_shop']==-1,months_since_first_sale_in_shop_encoding_seniority_0.columns[2:]]=-1\nmonths_since_first_sale_in_shop_encoding_seniority_1.loc[months_since_first_sale_in_shop_encoding_seniority_1['item_months_since_first_sale_in_shop']==-1,months_since_first_sale_in_shop_encoding_seniority_1.columns[2:]]=-1\nmonths_since_first_sale_in_shop_encoding_seniority_2.loc[months_since_first_sale_in_shop_encoding_seniority_2['item_months_since_first_sale_in_shop']==-1,months_since_first_sale_in_shop_encoding_seniority_2.columns[2:]]=-1\n\n\n#----------------------------------------------------------------\n# display\nprint_infos(months_since_first_sale_in_shop_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(months_since_first_sale_in_shop_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(months_since_first_sale_in_shop_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(months_since_first_sale_in_shop_encoding_seniority_2)\nprint()\n\ndisplay_1(months_since_first_sale_in_shop_encoding_seniority_2,'months_since_first_sale_in_shop_avg_sales_seniority_2')","14e82839":"# MONTH OF FIRST SALE IN SHOP-TIME\n# only seniority=2 has known values\n\n# build dataframes\nmonth_of_first_sale_in_shop_encoding=encoding_1(all_items,['month_id','item_month_of_first_sale_in_shop'],'month_of_first_sale_in_shop','')\nmonth_of_first_sale_in_shop_encoding_seniority_0=encoding_1((train_X['item_seniority']==0),['month_id','item_month_of_first_sale_in_shop'],'month_of_first_sale_in_shop','_seniority_0')\nmonth_of_first_sale_in_shop_encoding_seniority_1=encoding_1((train_X['item_seniority']==1),['month_id','item_month_of_first_sale_in_shop'],'month_of_first_sale_in_shop','_seniority_1')\nmonth_of_first_sale_in_shop_encoding_seniority_2=encoding_1((train_X['item_seniority']==2),['month_id','item_month_of_first_sale_in_shop'],'month_of_first_sale_in_shop','_seniority_2')\n\n# erase data from the future\nmonth_of_first_sale_in_shop_encoding.loc[month_of_first_sale_in_shop_encoding['item_month_of_first_sale_in_shop']==-1,month_of_first_sale_in_shop_encoding.columns[2:]]=-1\nmonth_of_first_sale_in_shop_encoding_seniority_0.loc[month_of_first_sale_in_shop_encoding_seniority_0['item_month_of_first_sale_in_shop']==-1,month_of_first_sale_in_shop_encoding_seniority_0.columns[2:]]=-1\nmonth_of_first_sale_in_shop_encoding_seniority_1.loc[month_of_first_sale_in_shop_encoding_seniority_1['item_month_of_first_sale_in_shop']==-1,month_of_first_sale_in_shop_encoding_seniority_1.columns[2:]]=-1\nmonth_of_first_sale_in_shop_encoding_seniority_2.loc[month_of_first_sale_in_shop_encoding_seniority_2['item_month_of_first_sale_in_shop']==-1,month_of_first_sale_in_shop_encoding_seniority_2.columns[2:]]=-1\n\n\n#----------------------------------------------------------------\n# display\nprint_infos(month_of_first_sale_in_shop_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(month_of_first_sale_in_shop_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(month_of_first_sale_in_shop_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(month_of_first_sale_in_shop_encoding_seniority_2)\nprint()\n\ndisplay_1(month_of_first_sale_in_shop_encoding_seniority_2,'month_of_first_sale_in_shop_avg_sales_seniority_2')","6c8cb9d0":"# SHOP-TIME\n\n# build dataframes\nshop_encoding=encoding_1(all_items,['month_id','shop_id'],'shop','')\nshop_encoding_seniority_0=encoding_1((train_X['item_seniority']==0),['month_id','shop_id'],'shop','_seniority_0')\nshop_encoding_seniority_1=encoding_1((train_X['item_seniority']==1),['month_id','shop_id'],'shop','_seniority_1')\nshop_encoding_seniority_2=encoding_1((train_X['item_seniority']==2),['month_id','shop_id'],'shop','_seniority_2')\n\n#----------------------------------------------------------------\n# display\nprint_infos(shop_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_encoding_seniority_2)\nprint()\n\ndisplay_2(shop_encoding,'shop_avg_sales')","52f7ada7":"# SUPERCATEGORY-TIME\n\n# build dataframes\nsupercategory_encoding=encoding_1(all_items,['month_id','item_supercategory_id'],'supercategory','')\nsupercategory_encoding_seniority_0=encoding_1((train_X['item_seniority']==0),['month_id','item_supercategory_id'],'supercategory','_seniority_0')\nsupercategory_encoding_seniority_1=encoding_1((train_X['item_seniority']==1),['month_id','item_supercategory_id'],'supercategory','_seniority_1')\nsupercategory_encoding_seniority_2=encoding_1((train_X['item_seniority']==2),['month_id','item_supercategory_id'],'supercategory','_seniority_2')\n\n#----------------------------------------------------------------\n# display\nprint_infos(supercategory_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(supercategory_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(supercategory_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(supercategory_encoding_seniority_2)\nprint()\n\ndisplay_3(supercategory_encoding,'supercategory_avg_sales')","5b5a9da2":"# CATEGORY-TIME\n\n# build dataframes\ncategory_encoding=encoding_1(all_items,['month_id','item_category_id'],'category','')\ncategory_encoding_seniority_0=encoding_1((train_X['item_seniority']==0),['month_id','item_category_id'],'category','_seniority_0')\ncategory_encoding_seniority_1=encoding_1((train_X['item_seniority']==1),['month_id','item_category_id'],'category','_seniority_1')\ncategory_encoding_seniority_2=encoding_1((train_X['item_seniority']==2),['month_id','item_category_id'],'category','_seniority_2')\n\n# compare to supercategory\ncategory_encoding=add_comparison_super(category_encoding,all_items,['month_id','item_category_id'],'category','',supercategory_encoding,'item_supercategory_id','supercategory',item_categories)\ncategory_encoding_seniority_0=add_comparison_super(category_encoding_seniority_0,(train_X['item_seniority']==0),['month_id','item_category_id'],'category','_seniority_0',supercategory_encoding_seniority_0,'item_supercategory_id','supercategory',item_categories)\ncategory_encoding_seniority_1=add_comparison_super(category_encoding_seniority_1,(train_X['item_seniority']==1),['month_id','item_category_id'],'category','_seniority_1',supercategory_encoding_seniority_1,'item_supercategory_id','supercategory',item_categories)\ncategory_encoding_seniority_2=add_comparison_super(category_encoding_seniority_2,(train_X['item_seniority']==2),['month_id','item_category_id'],'category','_seniority_2',supercategory_encoding_seniority_2,'item_supercategory_id','supercategory',item_categories)\n\n#----------------------------------------------------------------\n# display\nprint_infos(category_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(category_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(category_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(category_encoding_seniority_2)\nprint()\n\ndisplay_3(category_encoding,'category_avg_sales')","e8462270":"# ITEM-TIME\n# NB: \n    # - 'fraction_of_items_sold' here actually stands for 'fraction of shops where this item is sold'\n    # - the dataframe 'seniority_0' (new items) should be identical to the corresponding rows of the full dataframe for most features, but not for the weights and comparison\n    # - on the other hand, the distinction seniority 1 or 2 should yield different statistics depending on whether the row correspond to a shop where the item has previously been sold or not\n\n# build dataframes\nitem_encoding=encoding_1(all_items,['month_id','item_id'],'item','')\nitem_encoding_seniority_0=encoding_1((train_X['item_seniority']==0),['month_id','item_id'],'item','_seniority_0')\nitem_encoding_seniority_1=encoding_1((train_X['item_seniority']==1),['month_id','item_id'],'item','_seniority_1')\nitem_encoding_seniority_2=encoding_1((train_X['item_seniority']==2),['month_id','item_id'],'item','_seniority_2')\n\n# compare to category\nitem_encoding=add_comparison_super(item_encoding,all_items,['month_id','item_id'],'item','',category_encoding,'item_category_id','category',items)\nitem_encoding_seniority_0=add_comparison_super(item_encoding_seniority_0,(train_X['item_seniority']==0),['month_id','item_id'],'item','_seniority_0',category_encoding_seniority_0,'item_category_id','category',items)\nitem_encoding_seniority_1=add_comparison_super(item_encoding_seniority_1,(train_X['item_seniority']==1),['month_id','item_id'],'item','_seniority_1',category_encoding_seniority_1,'item_category_id','category',items)\nitem_encoding_seniority_2=add_comparison_super(item_encoding_seniority_2,(train_X['item_seniority']==2),['month_id','item_id'],'item','_seniority_2',category_encoding_seniority_2,'item_category_id','category',items)\n\n#----------------------------------------------------------------\n# display\nprint_infos(item_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(item_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(item_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(item_encoding_seniority_2)\nprint()","90d18c77":"# SHOP-CATEGORY-TIME\n\n# build dataframes\nshop_category_encoding=encoding_2(all_items,['month_id','shop_id','item_category_id'],'shop','category','',shop_encoding,category_encoding)\nshop_category_encoding_seniority_0=encoding_2((train_X['item_seniority']==0),['month_id','shop_id','item_category_id'],'shop','category','_seniority_0',shop_encoding_seniority_0,category_encoding_seniority_0)\nshop_category_encoding_seniority_1=encoding_2((train_X['item_seniority']==1),['month_id','shop_id','item_category_id'],'shop','category','_seniority_1',shop_encoding_seniority_1,category_encoding_seniority_1)\nshop_category_encoding_seniority_2=encoding_2((train_X['item_seniority']==2),['month_id','shop_id','item_category_id'],'shop','category','_seniority_2',shop_encoding_seniority_2,category_encoding_seniority_2)\n\n#----------------------------------------------------------------\n# display\nprint_infos(shop_category_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_category_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_category_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_category_encoding_seniority_2)\nprint()\n\ndisplay_4(shop_category_encoding,'shop_category_avg_sales')","07ccc308":"# SHOP-SUPERCATEGORY-TIME\n\n# build dataframes\nshop_supercategory_encoding=encoding_2(all_items,['month_id','shop_id','item_supercategory_id'],'shop','supercategory','',shop_encoding,supercategory_encoding)\nshop_supercategory_encoding_seniority_0=encoding_2((train_X['item_seniority']==0),['month_id','shop_id','item_supercategory_id'],'shop','supercategory','_seniority_0',shop_encoding_seniority_0,supercategory_encoding_seniority_0)\nshop_supercategory_encoding_seniority_1=encoding_2((train_X['item_seniority']==1),['month_id','shop_id','item_supercategory_id'],'shop','supercategory','_seniority_1',shop_encoding_seniority_1,supercategory_encoding_seniority_1)\nshop_supercategory_encoding_seniority_2=encoding_2((train_X['item_seniority']==2),['month_id','shop_id','item_supercategory_id'],'shop','supercategory','_seniority_2',shop_encoding_seniority_2,supercategory_encoding_seniority_2)\n\n#----------------------------------------------------------------\n# display\nprint_infos(shop_supercategory_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_supercategory_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_supercategory_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_supercategory_encoding_seniority_2)\nprint()\n\ndisplay_5(shop_supercategory_encoding,'shop_supercategory_avg_sales')","02aa0812":"# SHOP-ITEM-TIME\n\n# MEAN ENCODING\n# Each month, encode each pair (item-shop) according to its monthly-average sales (='item_quantity')\nshop_item_encoding=train_X[['month_id','shop_id','item_id','item_quantity']]\n\n# COMPARISON TO LOWER ORDER ENCODING\nshop_item_encoding=encode_comparison(shop_item_encoding,shop_encoding,['month_id','shop_id'],['item_quantity','item_quantity'],['shop_avg_sales','shop_avg_sales_over_sold'],['item_quantity_compared_to_shop_avg_sales','item_quantity_compared_to_shop_avg_sales_over_sold'])\nshop_item_encoding=encode_comparison(shop_item_encoding,item_encoding,['month_id','item_id'],['item_quantity','item_quantity'],['item_avg_sales','item_avg_sales_over_sold'],['item_quantity_compared_to_item_avg_sales','item_quantity_compared_to_item_avg_sales_over_sold'])\n\n# COMPARISON TO SUPERFEATURE\nshop_item_encoding['item_category_id']=shop_item_encoding['item_id'].map(items['item_category_id'])\nshop_item_encoding=encode_comparison(shop_item_encoding,shop_category_encoding,['month_id','shop_id','item_category_id'],['item_quantity','item_quantity'],['shop_category_avg_sales','shop_category_avg_sales_over_sold'],['item_quantity_compared_to_shop_category_avg_sales','item_quantity_compared_to_shop_category_avg_sales_over_sold'])\nshop_item_encoding.drop('item_category_id',axis=1,inplace=True)\n\nprint_infos(shop_item_encoding)","fca913cb":"# CATEGORY-MONTHS SINCE RELEASE-TIME\n\n# build dataframes\ncategory_months_since_release_encoding=encoding_2(all_items,['month_id','item_category_id','item_months_since_release'],'category','months_since_release','',category_encoding,months_since_release_encoding)\ncategory_months_since_release_encoding_seniority_0=encoding_2((train_X['item_seniority']==0),['month_id','item_category_id','item_months_since_release'],'category','months_since_release','_seniority_0',category_encoding_seniority_0,months_since_release_encoding_seniority_0)\ncategory_months_since_release_encoding_seniority_1=encoding_2((train_X['item_seniority']==1),['month_id','item_category_id','item_months_since_release'],'category','months_since_release','_seniority_1',category_encoding_seniority_1,months_since_release_encoding_seniority_1)\ncategory_months_since_release_encoding_seniority_2=encoding_2((train_X['item_seniority']==2),['month_id','item_category_id','item_months_since_release'],'category','months_since_release','_seniority_2',category_encoding_seniority_2,months_since_release_encoding_seniority_2)\n\n#----------------------------------------------------------------\n# display\nprint_infos(category_months_since_release_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(category_months_since_release_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(category_months_since_release_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(category_months_since_release_encoding_seniority_2)\nprint()\n\ndisplay_6(category_months_since_release_encoding,'category_months_since_release_avg_sales')","c1da84ba":"# SUPERCATEGORY-MONTHS SINCE RELEASE-TIME\n\n# build dataframes\nsupercategory_months_since_release_encoding=encoding_2(all_items,['month_id','item_supercategory_id','item_months_since_release'],'supercategory','months_since_release','',supercategory_encoding,months_since_release_encoding)\nsupercategory_months_since_release_encoding_seniority_0=encoding_2((train_X['item_seniority']==0),['month_id','item_supercategory_id','item_months_since_release'],'supercategory','months_since_release','_seniority_0',supercategory_encoding_seniority_0,months_since_release_encoding_seniority_0)\nsupercategory_months_since_release_encoding_seniority_1=encoding_2((train_X['item_seniority']==1),['month_id','item_supercategory_id','item_months_since_release'],'supercategory','months_since_release','_seniority_1',supercategory_encoding_seniority_1,months_since_release_encoding_seniority_1)\nsupercategory_months_since_release_encoding_seniority_2=encoding_2((train_X['item_seniority']==2),['month_id','item_supercategory_id','item_months_since_release'],'supercategory','months_since_release','_seniority_2',supercategory_encoding_seniority_2,months_since_release_encoding_seniority_2)\n\n#----------------------------------------------------------------\n# display\nprint_infos(supercategory_months_since_release_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(supercategory_months_since_release_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(supercategory_months_since_release_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(supercategory_months_since_release_encoding_seniority_2)\nprint()\n\ndisplay_6(supercategory_months_since_release_encoding,'supercategory_months_since_release_avg_sales')","7cc41f0a":"# CATEGORY-MONTH OF RELEASE-TIME\n\n# build dataframes\ncategory_month_of_release_encoding=encoding_2(all_items,['month_id','item_category_id','item_month_of_release'],'category','month_of_release','',category_encoding,month_of_release_encoding)\ncategory_month_of_release_encoding_seniority_0=encoding_2((train_X['item_seniority']==0),['month_id','item_category_id','item_month_of_release'],'category','month_of_release','_seniority_0',category_encoding_seniority_0,month_of_release_encoding_seniority_0)\ncategory_month_of_release_encoding_seniority_1=encoding_2((train_X['item_seniority']==1),['month_id','item_category_id','item_month_of_release'],'category','month_of_release','_seniority_1',category_encoding_seniority_1,month_of_release_encoding_seniority_1)\ncategory_month_of_release_encoding_seniority_2=encoding_2((train_X['item_seniority']==2),['month_id','item_category_id','item_month_of_release'],'category','month_of_release','_seniority_2',category_encoding_seniority_2,month_of_release_encoding_seniority_2)\n\n#----------------------------------------------------------------\n# display\nprint_infos(category_month_of_release_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(category_month_of_release_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(category_month_of_release_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(category_month_of_release_encoding_seniority_2)\nprint()\n\ndisplay_6(category_month_of_release_encoding,'category_month_of_release_avg_sales')","199529fd":"# SUPERCATEGORY-MONTH OF RELEASE-TIME\n\n# build dataframes\nsupercategory_month_of_release_encoding=encoding_2(all_items,['month_id','item_supercategory_id','item_month_of_release'],'supercategory','month_of_release','',supercategory_encoding,month_of_release_encoding)\nsupercategory_month_of_release_encoding_seniority_0=encoding_2((train_X['item_seniority']==0),['month_id','item_supercategory_id','item_month_of_release'],'supercategory','month_of_release','_seniority_0',supercategory_encoding_seniority_0,month_of_release_encoding_seniority_0)\nsupercategory_month_of_release_encoding_seniority_1=encoding_2((train_X['item_seniority']==1),['month_id','item_supercategory_id','item_month_of_release'],'supercategory','month_of_release','_seniority_1',supercategory_encoding_seniority_1,month_of_release_encoding_seniority_1)\nsupercategory_month_of_release_encoding_seniority_2=encoding_2((train_X['item_seniority']==2),['month_id','item_supercategory_id','item_month_of_release'],'supercategory','month_of_release','_seniority_2',supercategory_encoding_seniority_2,month_of_release_encoding_seniority_2)\n\n#----------------------------------------------------------------\n# display\nprint_infos(supercategory_month_of_release_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(supercategory_month_of_release_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(supercategory_month_of_release_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(supercategory_month_of_release_encoding_seniority_2)\nprint()\n\ndisplay_6(supercategory_month_of_release_encoding,'supercategory_month_of_release_avg_sales')","358c2277":"# CATEGORY-MONTHS SINCE FIRST SALE IN SHOP-TIME\n\n# build dataframes\ncategory_months_since_first_sale_in_shop_encoding=encoding_2(all_items,['month_id','item_category_id','item_months_since_first_sale_in_shop'],'category','months_since_first_sale_in_shop','',category_encoding,months_since_first_sale_in_shop_encoding)\ncategory_months_since_first_sale_in_shop_encoding_seniority_0=encoding_2((train_X['item_seniority']==0),['month_id','item_category_id','item_months_since_first_sale_in_shop'],'category','months_since_first_sale_in_shop','_seniority_0',category_encoding_seniority_0,months_since_first_sale_in_shop_encoding_seniority_0)\ncategory_months_since_first_sale_in_shop_encoding_seniority_1=encoding_2((train_X['item_seniority']==1),['month_id','item_category_id','item_months_since_first_sale_in_shop'],'category','months_since_first_sale_in_shop','_seniority_1',category_encoding_seniority_1,months_since_first_sale_in_shop_encoding_seniority_1)\ncategory_months_since_first_sale_in_shop_encoding_seniority_2=encoding_2((train_X['item_seniority']==2),['month_id','item_category_id','item_months_since_first_sale_in_shop'],'category','months_since_first_sale_in_shop','_seniority_2',category_encoding_seniority_2,months_since_first_sale_in_shop_encoding_seniority_2)\n\n# erase data from the future\ncategory_months_since_first_sale_in_shop_encoding.loc[category_months_since_first_sale_in_shop_encoding['item_months_since_first_sale_in_shop']==-1,category_months_since_first_sale_in_shop_encoding.columns[3:]]=-1\ncategory_months_since_first_sale_in_shop_encoding_seniority_0.loc[category_months_since_first_sale_in_shop_encoding_seniority_0['item_months_since_first_sale_in_shop']==-1,category_months_since_first_sale_in_shop_encoding_seniority_0.columns[3:]]=-1\ncategory_months_since_first_sale_in_shop_encoding_seniority_1.loc[category_months_since_first_sale_in_shop_encoding_seniority_1['item_months_since_first_sale_in_shop']==-1,category_months_since_first_sale_in_shop_encoding_seniority_1.columns[3:]]=-1\ncategory_months_since_first_sale_in_shop_encoding_seniority_2.loc[category_months_since_first_sale_in_shop_encoding_seniority_2['item_months_since_first_sale_in_shop']==-1,category_months_since_first_sale_in_shop_encoding_seniority_2.columns[3:]]=-1\n\n\n#----------------------------------------------------------------\n# display\nprint_infos(category_months_since_first_sale_in_shop_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(category_months_since_first_sale_in_shop_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(category_months_since_first_sale_in_shop_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(category_months_since_first_sale_in_shop_encoding_seniority_2)\nprint()\n\ndisplay_6(category_months_since_first_sale_in_shop_encoding_seniority_2,'category_months_since_first_sale_in_shop_avg_sales_seniority_2')","489fc2a3":"# SUPERCATEGORY-MONTHS SINCE FIRST SALE IN SHOP-TIME\n\n# build dataframes\nsupercategory_months_since_first_sale_in_shop_encoding=encoding_2(all_items,['month_id','item_supercategory_id','item_months_since_first_sale_in_shop'],'supercategory','months_since_first_sale_in_shop','',supercategory_encoding,months_since_first_sale_in_shop_encoding)\nsupercategory_months_since_first_sale_in_shop_encoding_seniority_0=encoding_2((train_X['item_seniority']==0),['month_id','item_supercategory_id','item_months_since_first_sale_in_shop'],'supercategory','months_since_first_sale_in_shop','_seniority_0',supercategory_encoding_seniority_0,months_since_first_sale_in_shop_encoding_seniority_0)\nsupercategory_months_since_first_sale_in_shop_encoding_seniority_1=encoding_2((train_X['item_seniority']==1),['month_id','item_supercategory_id','item_months_since_first_sale_in_shop'],'supercategory','months_since_first_sale_in_shop','_seniority_1',supercategory_encoding_seniority_1,months_since_first_sale_in_shop_encoding_seniority_1)\nsupercategory_months_since_first_sale_in_shop_encoding_seniority_2=encoding_2((train_X['item_seniority']==2),['month_id','item_supercategory_id','item_months_since_first_sale_in_shop'],'supercategory','months_since_first_sale_in_shop','_seniority_2',supercategory_encoding_seniority_2,months_since_first_sale_in_shop_encoding_seniority_2)\n\n# erase data from the future\nsupercategory_months_since_first_sale_in_shop_encoding.loc[supercategory_months_since_first_sale_in_shop_encoding['item_months_since_first_sale_in_shop']==-1,supercategory_months_since_first_sale_in_shop_encoding.columns[3:]]=-1\nsupercategory_months_since_first_sale_in_shop_encoding_seniority_0.loc[supercategory_months_since_first_sale_in_shop_encoding_seniority_0['item_months_since_first_sale_in_shop']==-1,supercategory_months_since_first_sale_in_shop_encoding_seniority_0.columns[3:]]=-1\nsupercategory_months_since_first_sale_in_shop_encoding_seniority_1.loc[supercategory_months_since_first_sale_in_shop_encoding_seniority_1['item_months_since_first_sale_in_shop']==-1,supercategory_months_since_first_sale_in_shop_encoding_seniority_1.columns[3:]]=-1\nsupercategory_months_since_first_sale_in_shop_encoding_seniority_2.loc[supercategory_months_since_first_sale_in_shop_encoding_seniority_2['item_months_since_first_sale_in_shop']==-1,supercategory_months_since_first_sale_in_shop_encoding_seniority_2.columns[3:]]=-1\n\n\n#----------------------------------------------------------------\n# display\nprint_infos(supercategory_months_since_first_sale_in_shop_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(supercategory_months_since_first_sale_in_shop_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(supercategory_months_since_first_sale_in_shop_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(supercategory_months_since_first_sale_in_shop_encoding_seniority_2)\nprint()\n\ndisplay_6(supercategory_months_since_first_sale_in_shop_encoding_seniority_2,'supercategory_months_since_first_sale_in_shop_avg_sales_seniority_2')","f4e6bfb7":"# SHOP-CATEGORY-MONTHS SINCE RELEASE-TIME\n\n# build dataframes\nshop_category_months_since_release_encoding=encoding_3(all_items,['month_id','shop_id','item_category_id','item_months_since_release'],'shop','category','months_since_release','')\nshop_category_months_since_release_encoding_seniority_0=encoding_3((train_X['item_seniority']==0),['month_id','shop_id','item_category_id','item_months_since_release'],'shop','category','months_since_release','_seniority_0')\nshop_category_months_since_release_encoding_seniority_1=encoding_3((train_X['item_seniority']==1),['month_id','shop_id','item_category_id','item_months_since_release'],'shop','category','months_since_release','_seniority_1')\nshop_category_months_since_release_encoding_seniority_2=encoding_3((train_X['item_seniority']==2),['month_id','shop_id','item_category_id','item_months_since_release'],'shop','category','months_since_release','_seniority_2')\n\n#----------------------------------------------------------------\n# display\nprint_infos(shop_category_months_since_release_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_category_months_since_release_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_category_months_since_release_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_category_months_since_release_encoding_seniority_2)\nprint()","9f3b7a12":"# SHOP-CATEGORY-MONTHS SINCE FIRST SALE IN SHOP-TIME\n\n# build dataframes\nshop_category_months_since_first_sale_in_shop_encoding=encoding_3(all_items,['month_id','shop_id','item_category_id','item_months_since_first_sale_in_shop'],'shop','category','months_since_first_sale_in_shop','')\nshop_category_months_since_first_sale_in_shop_encoding_seniority_0=encoding_3((train_X['item_seniority']==0),['month_id','shop_id','item_category_id','item_months_since_first_sale_in_shop'],'shop','category','months_since_first_sale_in_shop','_seniority_0')\nshop_category_months_since_first_sale_in_shop_encoding_seniority_1=encoding_3((train_X['item_seniority']==1),['month_id','shop_id','item_category_id','item_months_since_first_sale_in_shop'],'shop','category','months_since_first_sale_in_shop','_seniority_1')\nshop_category_months_since_first_sale_in_shop_encoding_seniority_2=encoding_3((train_X['item_seniority']==2),['month_id','shop_id','item_category_id','item_months_since_first_sale_in_shop'],'shop','category','months_since_first_sale_in_shop','_seniority_2')\n\n#----------------------------------------------------------------\n# display\nprint_infos(shop_category_months_since_first_sale_in_shop_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_category_months_since_first_sale_in_shop_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_category_months_since_first_sale_in_shop_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_category_months_since_first_sale_in_shop_encoding_seniority_2)\nprint()","9e61d15d":"# SHOP-SUPERCATEGORY-MONTHS SINCE RELEASE-TIME\n\n# build dataframes\nshop_supercategory_months_since_release_encoding=encoding_3(all_items,['month_id','shop_id','item_supercategory_id','item_months_since_release'],'shop','supercategory','months_since_release','')\nshop_supercategory_months_since_release_encoding_seniority_0=encoding_3((train_X['item_seniority']==0),['month_id','shop_id','item_supercategory_id','item_months_since_release'],'shop','supercategory','months_since_release','_seniority_0')\nshop_supercategory_months_since_release_encoding_seniority_1=encoding_3((train_X['item_seniority']==1),['month_id','shop_id','item_supercategory_id','item_months_since_release'],'shop','supercategory','months_since_release','_seniority_1')\nshop_supercategory_months_since_release_encoding_seniority_2=encoding_3((train_X['item_seniority']==2),['month_id','shop_id','item_supercategory_id','item_months_since_release'],'shop','supercategory','months_since_release','_seniority_2')\n\n#----------------------------------------------------------------\n# display\nprint_infos(shop_supercategory_months_since_release_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_supercategory_months_since_release_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_supercategory_months_since_release_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_supercategory_months_since_release_encoding_seniority_2)\nprint()","33bf1bb5":"# SHOP-SUPERCATEGORY-MONTHS SINCE FIRST SALE IN SHOP-TIME\n\n# build dataframes\nshop_supercategory_months_since_first_sale_in_shop_encoding=encoding_3(all_items,['month_id','shop_id','item_supercategory_id','item_months_since_first_sale_in_shop'],'shop','supercategory','months_since_first_sale_in_shop','')\nshop_supercategory_months_since_first_sale_in_shop_encoding_seniority_0=encoding_3((train_X['item_seniority']==0),['month_id','shop_id','item_supercategory_id','item_months_since_first_sale_in_shop'],'shop','supercategory','months_since_first_sale_in_shop','_seniority_0')\nshop_supercategory_months_since_first_sale_in_shop_encoding_seniority_1=encoding_3((train_X['item_seniority']==1),['month_id','shop_id','item_supercategory_id','item_months_since_first_sale_in_shop'],'shop','supercategory','months_since_first_sale_in_shop','_seniority_1')\nshop_supercategory_months_since_first_sale_in_shop_encoding_seniority_2=encoding_3((train_X['item_seniority']==2),['month_id','shop_id','item_supercategory_id','item_months_since_first_sale_in_shop'],'shop','supercategory','months_since_first_sale_in_shop','_seniority_2')\n\n#----------------------------------------------------------------\n# display\nprint_infos(shop_supercategory_months_since_first_sale_in_shop_encoding)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_supercategory_months_since_first_sale_in_shop_encoding_seniority_0)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_supercategory_months_since_first_sale_in_shop_encoding_seniority_1)\nprint()\nprint('------------------------------------------------')\nprint()\nprint_infos(shop_supercategory_months_since_first_sale_in_shop_encoding_seniority_2)\nprint()","ee49cb05":"dfs=['month_encoding',\n     'months_since_release_encoding',\n     'month_of_release_encoding',\n     'months_since_first_sale_in_shop_encoding',\n     'month_of_first_sale_in_shop_encoding',\n     'shop_encoding',\n     'item_encoding',\n     'category_encoding',\n     'supercategory_encoding',\n     'shop_category_encoding',\n     'shop_supercategory_encoding',\n     'category_months_since_release_encoding',\n     'supercategory_months_since_release_encoding',\n     'category_month_of_release_encoding',\n     'supercategory_month_of_release_encoding',\n     'category_months_since_first_sale_in_shop_encoding',\n     'supercategory_months_since_first_sale_in_shop_encoding',\n     'shop_category_months_since_release_encoding',\n     'shop_supercategory_months_since_release_encoding',\n     'shop_category_months_since_first_sale_in_shop_encoding',\n     'shop_supercategory_months_since_first_sale_in_shop_encoding'\n    ]","7ef39e84":"for df in dfs+['shop_item_encoding']:\n    print('-----------')\n    print(df)\n    print()\n    exec(\"print(\"+df+\"[\"+df+\"['month_id']<34].info(null_counts=True))\")\n    print()","b23f26e4":"for df in dfs:\n    for seniority in range(0,3):\n        print('-----------')\n        print(df+\"_seniority_\"+str(seniority))\n        print()\n        exec(\"print(\"+df+\"_seniority_\"+str(seniority)+\"[\"+df+\"_seniority_\"+str(seniority)+\"['month_id']<34].info(null_counts=True))\")\n        print()","7edba8d3":"# create directory\ncreate_directory(os.path.join(DATA_FOLDER,'processed\/target_encodings'))\n\n# export encoded features\nfor df in dfs+['shop_item_encoding']:\n    exec(df+\".to_pickle(os.path.join(DATA_FOLDER,'processed\/target_encodings\/\"+df+\".pkl'))\")\n    exec(\"del \"+df)\n\nfor df in dfs:\n    for seniority in range(0,3):\n        exec(df+\"_seniority_\"+str(seniority)+\".to_pickle(os.path.join(DATA_FOLDER,'processed\/target_encodings\/\"+df+\"_seniority_\"+str(seniority)+\".pkl'))\")\n        exec(\"del \"+df+\"_seniority_\"+str(seniority))","75f2c592":"# clear memory\ndel train_X\ndel items, item_categories\n\ngc.collect()","6e8965d5":"reset_variable_space","bca5368b":"# LAG FEATURES\n\ndef lag_features(df,df_features,col_agg,col_lag,lagging_values,fill_value=None):\n    col_agg=list(col_agg)\n    col_lag=list(col_lag)\n    \n    def lag_dataframe(df_original,lag):\n        df_shift=df_original.copy()\n        df_shift['month_id']+=lag\n        for col in col_lag:\n            df_shift.rename({col:col+'_lag_'+str(lag)},axis=1,inplace=True)\n        return df_shift\n    \n    df_features_lag=df.loc[:,col_agg]\n    tmp=df_features.loc[:,col_agg+col_lag]\n    for lag in lagging_values:\n        df_features_lag=df_features_lag.join(lag_dataframe(tmp,lag).set_index(col_agg),on=col_agg)\n        \n    if fill_value is not None:\n        for col in col_lag:\n            for lag in lagging_values:\n                df_features_lag[col+'_lag_'+str(lag)].fillna(fill_value,inplace=True)\n            \n    return df_features_lag.drop(col_agg,axis=1)","fc7886af":"# TEMPORAL STATISTICS\n\ndef moving_statistics(df,df_features,col_agg,col_avg,windowsize,func,suffix='',fill_value=None):\n    col_agg=list(col_agg)\n    col_lag=list(col_avg)\n    \n    def lag_dataframe(df_original,lag):\n        df_shift=df_original.copy()\n        df_shift['month_id']+=lag\n        for col in col_lag:\n            df_shift.rename({col:col+'_lag_'+str(lag)},axis=1,inplace=True)\n        return df_shift\n    \n    df_features_lag=df.loc[:,col_agg]\n    tmp=df_features.loc[:,col_agg+col_lag]\n    for lag in range(1,windowsize+1):\n        df_features_lag=df_features_lag.join(lag_dataframe(tmp,lag).set_index(col_agg),on=col_agg)\n\n    df_features_avg=df.loc[:,col_agg]\n    for col in col_avg:\n        if func=='mean':\n            if suffix=='':\n                suffix='_movavg_'+str(windowsize)\n            df_features_avg[col+suffix]=df_features_lag.loc[:,[col+'_lag_'+str(lag) for lag in range(1,windowsize+1)]].mean(axis=1)\n        elif func=='max':\n            if suffix=='':\n                suffix='_movmax_'+str(windowsize)\n            df_features_avg[col+suffix]=df_features_lag.loc[:,[col+'_lag_'+str(lag) for lag in range(1,windowsize+1)]].max(axis=1)\n        elif func=='min':\n            if suffix=='':\n                suffix='_movmin_'+str(windowsize)\n            df_features_avg[col+suffix]=df_features_lag.loc[:,[col+'_lag_'+str(lag) for lag in range(1,windowsize+1)]].min(axis=1)\n        elif func=='std':\n            if suffix=='':\n                suffix='_movstd_'+str(windowsize)\n            df_features_avg[col+suffix]=df_features_lag.loc[:,[col+'_lag_'+str(lag) for lag in range(1,windowsize+1)]].std(ddof=0,axis=1)\n        elif func=='rsd':\n            if suffix=='':\n                suffix='_movrsd_'+str(windowsize)\n            denom_avg=df_features_lag.loc[:,[col+'_lag_'+str(lag) for lag in range(1,windowsize+1)]].mean(axis=1)\n            num_std=df_features_lag.loc[:,[col+'_lag_'+str(lag) for lag in range(1,windowsize+1)]].std(ddof=0,axis=1)\n            df_features_avg[col+suffix]=num_std\/denom_avg\n            df_features_avg.loc[num_std==0,col+suffix]=0\n            del denom_avg\n            del num_std\n\n\n    if fill_value is not None:\n        for col in col_lag:\n            df_features_avg[col+suffix].fillna(fill_value,inplace=True)\n    \n    return df_features_avg.drop(col_agg,axis=1)","e647e628":"# LINEAR COMBINATION OF PAST VALUES (differentiation, extrapolation, weighted average)\n\n# weight vectors for...\nfd1=[1,-1]          # first order backward finite difference scheme for derivative on previous month\nfd2=[3\/2,-2,1\/2]    # second order backward finite difference scheme for derivative on previous month\nextralin2=[2,-1]         # linear extrapolation from 2 previous months\nextralin3=[4\/3,1\/3,-2\/3] # linear extrapolation from 3 previous months\nextraquad3=[3,-3,1]      # quadratic extrapolation from 3 previous months\n\ndef linear_combination(df,df_features,col_agg,col_avg,weights,suffix,fill_value=None):\n    col_agg=list(col_agg)\n    col_lag=list(col_avg)\n    windowsize=len(weights)\n    \n    def lag_dataframe(df_original,lag):\n        df_shift=df_original.copy()\n        df_shift['month_id']+=lag\n        for col in col_lag:\n            df_shift.rename({col:col+'_lag_'+str(lag)},axis=1,inplace=True)\n        return df_shift\n    \n    df_features_lag=df.loc[:,col_agg]\n    tmp=df_features.loc[:,col_agg+col_lag]\n    for lag in range(1,windowsize+1):\n        df_features_lag=df_features_lag.join(lag_dataframe(tmp,lag).set_index(col_agg)*weights[lag-1],on=col_agg)\n        \n    df_features_avg=df.loc[:,col_agg]\n    for col in col_avg:\n        df_features_avg[col+suffix]=df_features_lag.loc[:,[col+'_lag_'+str(lag) for lag in range(1,windowsize+1)]].sum(axis=1,skipna=False)\n\n    if fill_value is not None:\n        for col in col_lag:\n            df_features_avg[col+suffix].fillna(fill_value,inplace=True)\n    \n    return df_features_avg.drop(col_agg,axis=1)","6e96bfa6":"# COMPARE PAST VALUES\n\ndef temporal_compare(df,df_features,col_agg,col_avg,lags_num,lags_denom,suffix,fill_value=None):\n    col_agg=list(col_agg)\n    col_lag=list(col_avg)\n    \n    def lag_dataframe(df_original,lag):\n        df_shift=df_original.copy()\n        df_shift['month_id']+=lag\n        for col in col_lag:\n            df_shift.rename({col:col+'_lag_'+str(lag)},axis=1,inplace=True)\n        return df_shift\n    \n    \n    \n    tmp=df_features.loc[:,col_agg+col_lag]\n    \n    df_features_lag_num=df.loc[:,col_agg]\n    for lag in lags_num:\n        df_features_lag_num=df_features_lag_num.join(lag_dataframe(tmp,lag).set_index(col_agg),on=col_agg)\n    \n    df_features_lag_denom=df.loc[:,col_agg]\n    for lag in lags_denom:\n        df_features_lag_denom=df_features_lag_denom.join(lag_dataframe(tmp,lag).set_index(col_agg),on=col_agg)\n        \n        \n    df_features_comp=df.loc[:,col_agg]\n    for col in col_avg:\n        df_features_lag_num[col+'_avg']=df_features_lag_num.loc[:,[col+'_lag_'+str(lag) for lag in lags_num]].mean(axis=1)\n        df_features_lag_denom[col+'_avg']=df_features_lag_denom.loc[:,[col+'_lag_'+str(lag) for lag in lags_denom]].mean(axis=1)\n\n        df_features_comp[col+suffix]=df_features_lag_num[col+'_avg']\/df_features_lag_denom[col+'_avg']\n\n    if fill_value is not None:\n        for col in col_lag:\n            df_features_comp[col+suffix].fillna(fill_value,inplace=True)\n    \n    return df_features_comp.drop(col_agg,axis=1)","4dbd2791":"# COMPARE PAST VALUES\n\ndef rational_fraction(df,df_features,col_agg,col_avg,weights_num,weights_denom,suffix,fill_value=None):\n    col_agg=list(col_agg)\n    col_lag=list(col_avg)\n    windowsize=len(weights_num)\n    \n    def lag_dataframe(df_original,lag):\n        df_shift=df_original.copy()\n        df_shift['month_id']+=lag\n        for col in col_lag:\n            df_shift.rename({col:col+'_lag_'+str(lag)},axis=1,inplace=True)\n        return df_shift\n    \n    \n    \n    tmp=df_features.loc[:,col_agg+col_lag]\n    \n    df_features_lag_num=df.loc[:,col_agg]\n    for lag in range(1,windowsize+1):\n        df_features_lag_num=df_features_lag_num.join(lag_dataframe(tmp,lag).set_index(col_agg)*weights_num[lag-1],on=col_agg)\n    \n    df_features_lag_denom=df.loc[:,col_agg]\n    for lag in range(1,windowsize+1):\n        df_features_lag_denom=df_features_lag_denom.join(lag_dataframe(tmp,lag).set_index(col_agg)*weights_denom[lag-1],on=col_agg)\n        \n        \n    df_features_comp=df.loc[:,col_agg]\n    for col in col_avg:\n        df_features_lag_num[col+'_avg']=df_features_lag_num.loc[:,[col+'_lag_'+str(lag) for lag in range(1,windowsize+1)]].sum(axis=1)\n        df_features_lag_denom[col+'_avg']=df_features_lag_denom.loc[:,[col+'_lag_'+str(lag) for lag in range(1,windowsize+1)]].sum(axis=1)\n\n        df_features_comp[col+suffix]=df_features_lag_num[col+'_avg']\/df_features_lag_denom[col+'_avg']\n        \n    if fill_value is not None:\n        for col in col_lag:\n            df_features_comp[col+suffix].fillna(fill_value,inplace=True)\n    \n    return df_features_comp.drop(col_agg,axis=1)","e456c4d6":"# TIME MAPPING\n# map columns 'col_labels' for the current month to the value at another point in time given by 'map_to_label'\n\ndef time_mapping(df,df_features,join_labels,col_labels,map_to_label,suffix):\n    df_mapped=df.loc[:,[map_to_label]+join_labels]\n    \n    shifted_df=df_features.loc[:,['month_id']+join_labels+col_labels].rename({'month_id':map_to_label},axis=1).set_index([map_to_label]+join_labels)\n    shifted_df.rename({col:col+suffix for col in col_labels},axis=1,inplace=True)\n    \n    return df_mapped.join(shifted_df,on=[map_to_label]+join_labels).drop([map_to_label]+join_labels,axis=1)   ","2fd621d9":"def feature_correlation_analysis(train_df,feature_df,feature_df_seniority,feature_name,groupby_labels,seniority):\n\n    tmp=feature_df_seniority.loc[(feature_df_seniority['month_id']>12)&(feature_df_seniority['month_id']<34),groupby_labels+[feature_name+'_avg_sales_seniority_'+str(seniority),feature_name+'_avg_sales_over_sold_seniority_'+str(seniority)]]\n    tmp2=feature_df.loc[(feature_df['month_id']>12)&(feature_df['month_id']<34),groupby_labels+[feature_name+'_avg_sales',feature_name+'_avg_sales_over_sold']]\n    tmp=tmp.join(tmp2.set_index(groupby_labels),on=groupby_labels)\n\n    tmp_full=train_df[groupby_labels+['item_quantity']].join(tmp.set_index(groupby_labels),on=groupby_labels)\n\n    fig=plt.figure(figsize=(5,5))\n    sns.heatmap(tmp_full.drop(groupby_labels,axis=1).corr(),annot=True)\n    \n    \n    \ndef compare_features(df,df_seniority,name,labels,seniority,avg_sales_max=20,avg_sales_seniority_max=20,avg_sales_over_sold_max=20):\n    n=len(labels)\n    \n    tmp=df.join(df_seniority.set_index(labels),on=labels)\n    labels.remove('month_id')\n    labels.append('month')\n    tmp['month']=tmp['month_id']%12+1\n    \n    if name=='item':\n        fig,axes=plt.subplots(1,3,figsize=(15,5))\n        sns.scatterplot(data=tmp,x=name+'_avg_sales',y=name+'_avg_sales_seniority_'+str(seniority),hue='month',ax=axes[0])\n        axes[0].plot([0,20],[0,20],'k')\n        axes[0].grid(True)\n        axes[0].set_xlim(0,avg_sales_max)\n        axes[0].set_ylim(0,avg_sales_seniority_max)\n        sns.scatterplot(data=tmp,x=name+'_avg_sales',y=name+'_avg_sales_over_sold',hue='month',ax=axes[1])\n        axes[1].plot([0,20],[0,20],'k')\n        axes[1].grid(True)\n        axes[1].set_xlim(0,avg_sales_max)\n        axes[1].set_ylim(0,avg_sales_over_sold_max)\n        sns.scatterplot(data=tmp,x=name+'_avg_sales_over_sold',y=name+'_avg_sales_seniority_'+str(seniority),hue='month',ax=axes[2])\n        axes[2].plot([0,20],[0,20],'k')\n        axes[2].grid(True)\n        axes[2].set_xlim(0,avg_sales_over_sold_max)\n        axes[2].set_ylim(0,avg_sales_seniority_max)\n    \n    else:    \n        fig,axes=plt.subplots(n,3,figsize=(15,n*5))\n        for i,label in enumerate(labels):\n            sns.scatterplot(data=tmp,x=name+'_avg_sales',y=name+'_avg_sales_seniority_'+str(seniority),hue=label,ax=axes[i,0])\n            axes[i,0].plot([0,20],[0,20],'k')\n            axes[i,0].grid(True)\n            axes[i,0].set_xlim(0,avg_sales_max)\n            axes[i,0].set_ylim(0,avg_sales_seniority_max)\n            sns.scatterplot(data=tmp,x=name+'_avg_sales',y=name+'_avg_sales_over_sold',hue=label,ax=axes[i,1])\n            axes[i,1].plot([0,20],[0,20],'k')\n            axes[i,1].grid(True)\n            axes[i,1].set_xlim(0,avg_sales_max)\n            axes[i,1].set_ylim(0,avg_sales_over_sold_max)\n            sns.scatterplot(data=tmp,x=name+'_avg_sales_over_sold',y=name+'_avg_sales_seniority_'+str(seniority),hue=label,ax=axes[i,2])\n            axes[i,2].plot([0,20],[0,20],'k')\n            axes[i,2].grid(True)\n            axes[i,2].set_xlim(0,avg_sales_over_sold_max)\n            axes[i,2].set_ylim(0,avg_sales_seniority_max)","9cb85f24":"loaded=%who_ls\nloaded.append('loaded')","8451b8ca":"# import dataset\ntrain_X=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/train_X0.pkl'))","7a1a19f0":"def moving_statistics_0(df,df_features,col_agg,col_avg,windowsize,func,suffix='',fill_value=None):\n    col_agg=list(col_agg)\n    col_lag=list(col_avg)\n    \n    def lag_dataframe(df_original,lag):\n        df_shift=df_original.copy()\n        df_shift['month_id']+=lag\n        for col in col_lag:\n            df_shift.rename({col:col+'_lag_'+str(lag)},axis=1,inplace=True)\n        return df_shift\n    \n    df_features_lag=df.loc[:,col_agg]\n    tmp=df_features.loc[:,col_agg+col_lag]\n    for lag in range(0,windowsize):\n        df_features_lag=df_features_lag.join(lag_dataframe(tmp,lag).set_index(col_agg),on=col_agg)\n\n    df_features_avg=df.loc[:,col_agg]\n    for col in col_avg:\n        if func=='mean':\n            if suffix=='':\n                suffix='_movavg_'+str(windowsize)\n            df_features_avg[col+suffix]=df_features_lag.loc[:,[col+'_lag_'+str(lag) for lag in range(0,windowsize)]].mean(axis=1)\n        elif func=='max':\n            if suffix=='':\n                suffix='_movmax_'+str(windowsize)\n            df_features_avg[col+suffix]=df_features_lag.loc[:,[col+'_lag_'+str(lag) for lag in range(0,windowsize)]].max(axis=1)\n        elif func=='min':\n            if suffix=='':\n                suffix='_movmin_'+str(windowsize)\n            df_features_avg[col+suffix]=df_features_lag.loc[:,[col+'_lag_'+str(lag) for lag in range(0,windowsize)]].min(axis=1)\n        elif func=='std':\n            if suffix=='':\n                suffix='_movstd_'+str(windowsize)\n            df_features_avg[col+suffix]=df_features_lag.loc[:,[col+'_lag_'+str(lag) for lag in range(0,windowsize)]].std(ddof=0,axis=1)\n        elif func=='rsd':\n            if suffix=='':\n                suffix='_movrsd_'+str(windowsize)\n            denom_avg=df_features_lag.loc[:,[col+'_lag_'+str(lag) for lag in range(0,windowsize)]].mean(axis=1)\n            num_std=df_features_lag.loc[:,[col+'_lag_'+str(lag) for lag in range(0,windowsize)]].std(ddof=0,axis=1)\n            df_features_avg[col+suffix]=num_std\/denom_avg\n            df_features_avg.loc[num_std==0,col+suffix]=0\n            del denom_avg\n            del num_std\n\n\n    if fill_value is not None:\n        for col in col_lag:\n            df_features_avg[col+suffix].fillna(fill_value,inplace=True)\n    \n    return df_features_avg.drop(col_agg,axis=1)\n\n\ndef categorical_variability_encoding(agg_label,name,windowsize_inner,suffix,fill_value=None,windowsize_outer=34):\n    df=train_X[['month_id','shop_id','item_id',agg_label,'item_quantity']]\n\n    # compute the temporal relative standard deviation of the item_quantity of item in shop\n    tmp=moving_statistics_0(df,df,['month_id','shop_id','item_id'],['item_quantity'],windowsize_inner,'rsd',suffix,fill_value=None)\n    tmp=pd.concat([df,tmp],axis=1,sort=False)\n    \n    # average within categories every month over all shop and items...\n    # ...then average over all past months\n    # remaining NaNs correspond only to categories that are new in the dataset (no past data for this category in the dataset)\n    tmp=tmp.groupby(['month_id',agg_label]).agg({'item_quantity'+suffix:'mean'}).rename({'item_quantity'+suffix:name+suffix},axis=1)\n    tmp=moving_statistics(df,tmp.reset_index(),['month_id',agg_label],[name+suffix],windowsize_outer,'mean','_',fill_value)\n    tmp.rename({name+suffix+'_':name+suffix},axis=1,inplace=True)\n\n    return tmp","0aaa7a43":"df=categorical_variability_encoding('item_category_id','category_semiannual_avg',3,'_recent_rsd',fill_value=None,windowsize_outer=6)\ndf2=pd.concat([train_X[['month_id','shop_id','item_id','item_category_id']],df],axis=1,sort=False)\nprint(df2.info(null_counts=True))\n\ntmp=df2[['month_id','item_category_id','category_semiannual_avg_recent_rsd']].groupby(['month_id','item_category_id']).mean().unstack()\ntmp.columns=tmp.columns.droplevel(0)\n\nfig,axes=plt.subplots(1,2,figsize=(15,5))\naxes[0].plot(tmp)\n\nsns.heatmap(tmp.T,ax=axes[1])\naxes[1].set_xlim(20,35)\n\ndel df, df2, tmp, fig, axes","04cdc7bd":"df=categorical_variability_encoding('item_category_id','category_semiannual_avg',12,'_annual_rsd',fill_value=None,windowsize_outer=6)\ndf2=pd.concat([train_X[['month_id','shop_id','item_id','item_category_id']],df],axis=1,sort=False)\nprint(df2.info(null_counts=True))\n\ntmp=df2[['month_id','item_category_id','category_semiannual_avg_annual_rsd']].groupby(['month_id','item_category_id']).mean().unstack()\ntmp.columns=tmp.columns.droplevel(0)\n\nfig,axes=plt.subplots(1,2,figsize=(15,5))\naxes[0].plot(tmp)\n\nsns.heatmap(tmp.T,ax=axes[1])\naxes[1].set_xlim(20,35)\n\ndel df, df2, tmp, fig, axes","9a92320d":"# generate features\ncategory_variability_rs=categorical_variability_encoding('item_category_id','category_semiannual_avg',3,'_recent_rsd',fill_value=None,windowsize_outer=6)\ncategory_variability_as=categorical_variability_encoding('item_category_id','category_semiannual_avg',12,'_annual_rsd',fill_value=None,windowsize_outer=6)\n\n# join features to dataframe\ntrain_X=pd.concat([train_X,category_variability_rs,category_variability_as],axis=1,sort=False)\n\ndel category_variability_rs, category_variability_as ","8e858142":"# restrict temporal extent of the dataframe (drop first 18 months)\ntrain_X.drop(train_X.loc[train_X['month_id']<18].index,axis=0,inplace=True)","f2e843c9":"# split dataframe depending on seniority\ntrain_0=train_X.loc[train_X['item_seniority']==0,:]\ntrain_1=train_X.loc[train_X['item_seniority']==1,:]\ntrain_2=train_X.loc[train_X['item_seniority']==2,:]\n\npd.DataFrame([train_X.count().rename('global'),train_0.count().rename('seniority_0'),train_1.count().rename('seniority_1'),train_2.count().rename('seniority_2')]).T","dd758832":"# clear disk space\nos.remove(os.path.join(DATA_FOLDER,'processed\/train_X0.pkl'))\n\n# export datasets\ntrain_0.to_pickle(os.path.join(DATA_FOLDER,'processed\/train_0.pkl'))\ntrain_1.to_pickle(os.path.join(DATA_FOLDER,'processed\/train_1.pkl'))\ntrain_2.to_pickle(os.path.join(DATA_FOLDER,'processed\/train_2.pkl'))","16d6b715":"# clear memory\ndel train_X\n\ndel train_0\ndel train_1\ndel train_2\n\ngc.collect()","78bb95d7":"reset_variable_space","f36bea7b":"# import dataset\ntrain_0=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/train_0.pkl'))\n\nprint(train_0.info(null_counts=True))","ea715145":"# RAW FEATURES SELECTION\nsplit_col='month_id'\ntarget_col='item_quantity'\nraw_features=[]\n\n# label encodings must be retained at this stage for aggregating features\n# most should be discarded later on\nlabel_encoding=['shop_id',\n                'item_id',\n                'item_category_id',\n                'item_supercategory_id',\n                'item_category_console_id',\n                'item_category_is_digital'\n               ]\n\n# mapping info for aggregation (to be discarded later)\nmapping_columns=[]\n\n# define already the columns to discard after aggregation of all features\nfeatures_to_discard=['item_id',\n                     'shop_id',\n                     'item_supercategory_id',\n                     'item_category_console_id',\n                     'item_category_is_digital'\n                    ]\nfeatures_to_discard+=mapping_columns\n\n\n#-----------------------------------\n# SHOPS\n\n# months since opening\nraw_features+=['shop_months_since_opening']\nraw_features+=['shop_opening']\n\n#-----------------------------------\n# ITEMS\n\n# month-specific frequency encoding\n#raw_features+=['item_freq_in_seniority']    # uniform in seniority 0 (when an item is new, it is new in all shops)\n\n#-----------------------------------\n# ITEM CATEGORIES\n\n# month-specific frequency encoding\nraw_features+=['item_category_freq']\nraw_features+=['item_supercategory_freq']\nraw_features+=['item_category_console_freq']\nraw_features+=['item_category_digital_freq']\n\nraw_features+=['item_category_freq_in_seniority']\nraw_features+=['item_supercategory_freq_in_seniority']\nraw_features+=['item_category_console_freq_in_seniority']\nraw_features+=['item_category_digital_freq_in_seniority']\n\n# encoding of category temporal variability\nraw_features+=['category_semiannual_avg_recent_rsd']\nraw_features+=['category_semiannual_avg_annual_rsd']\n\n\n#-----------------------------------\n# RELATIVE TIME FEATURES\n#raw_features+=['item_months_since_release']     # doesn't make sense for seniority 0 (items have all just been released)\n\n#raw_features+=['item_months_since_first_sale_in_shop']   # doesn't make sense for seniority 0 (items have never been sold in shop)\n#raw_features+=['item_months_since_last_sale_in_shop']    # doesn't make sense for seniority 0 (items have never been sold in shop)\n\n\ntrain_0=train_0[[split_col]+label_encoding+mapping_columns+[target_col]+raw_features]\n\nprint(train_0.info(null_counts=True))\n\ngc.collect()","a7a071a4":"# import price data\n\nshop_category_prices=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/price_features\/shop_category_prices.pkl'))\ncategory_prices=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/price_features\/category_prices.pkl'))\nsupercategory_prices=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/price_features\/supercategory_prices.pkl'))","631e0344":"ts = time.time()\n\nprice_features_df=[]\n\n# -------------------------------------------------\n# SHOP-CATEGORY\n\n# assess typical price range for this category in this shop: recently, and at the same period last year\nprice_features_df.append(moving_statistics(train_0,shop_category_prices,['month_id','shop_id','item_category_id'],['shop_category_price_median'],34,'mean',suffix='_absolute_mean'))\nprice_features_df.append(moving_statistics(train_0,shop_category_prices,['month_id','shop_id','item_category_id'],['shop_category_price_median'],3,'mean',suffix='_recent_mean'))\nprice_features_df.append(lag_features(train_0,shop_category_prices,['month_id','shop_id','item_category_id'],['shop_category_price_median'],[12]))\n\n# -------------------------------------------------\n# CATEGORY\n\n# infer range of possible price values from all values observed over time in the shops where it was sold\nprice_features_df.append(moving_statistics(train_0,category_prices,['month_id','item_category_id'],['category_price_min'],34,'min',suffix='_absolute_min'))\nprice_features_df.append(moving_statistics(train_0,category_prices,['month_id','item_category_id'],['category_price_max'],34,'max',suffix='_absolute_max'))\n\nprice_features_df.append(moving_statistics(train_0,category_prices,['month_id','item_category_id'],['category_price_min'],12,'min',suffix='_annual_min'))\nprice_features_df.append(moving_statistics(train_0,category_prices,['month_id','item_category_id'],['category_price_max'],12,'max',suffix='_annual_max'))\n\n# assess typical price range for this category of items recently, and at the same period last year\nprice_features_df.append(moving_statistics(train_0,category_prices,['month_id','item_category_id'],['category_price_median'],3,'mean',suffix='_recent_mean'))\nprice_features_df.append(lag_features(train_0,category_prices,['month_id','item_category_id'],['category_price_median'],[12]))\n\n# compare price of category to supercategory\nprice_features_df.append(moving_statistics(train_0,category_prices,['month_id','item_category_id'],['category_price_median_compared_to_supercategory_price_median'],34,'mean',suffix='_absolute_mean'))\n\n\n# for totally new categories, we need data from the supercategory\n# -------------------------------------------------\n# SUPERCATEGORY\n\n# assess typical price range for this supercategory of items recently, and at the same period last year\nprice_features_df.append(moving_statistics(train_0,supercategory_prices,['month_id','item_supercategory_id'],['supercategory_price_median'],3,'mean',suffix='_recent_mean'))\nprice_features_df.append(lag_features(train_0,supercategory_prices,['month_id','item_supercategory_id'],['supercategory_price_median'],[12]))\n\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n# -------------------------------------------------\n# CONCATENATE FEATURES\n\ntrain_0=pd.concat([train_0]+price_features_df,axis=1,sort=False)\ndel price_features_df\ngc.collect()\n\nprint('time : ' +str(time.time() - ts))\n\n\n# -------------------------------------------------\n# DOWNCAST DTYPES\ntrain_0=downcast_dtypes(train_0)\nprint('time : ' +str(time.time() - ts))\n\n\nprint(train_0.info(null_counts=True))\ntrain_0","84a51459":"# clear memory space\n\ndel shop_category_prices\ndel category_prices\ndel supercategory_prices\n\ngc.collect()","e1a97228":"dfs=['shop_encoding',\n     'category_encoding',\n     'supercategory_encoding',\n     'shop_category_encoding',\n     'shop_supercategory_encoding'\n    ]\n\n    \nfor df in dfs:\n    exec(df+\"=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/target_encodings\/\"+df+\".pkl'))\")\n    exec(df+\"_seniority_0=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/target_encodings\/\"+df+\"_seniority_0.pkl'))\")","8b80026b":"feature_correlation_analysis(train_0,shop_encoding,shop_encoding_seniority_0,'shop',['month_id','shop_id'],0)\ncompare_features(shop_encoding,shop_encoding_seniority_0,'shop',['month_id','shop_id'],0,4,4,4)","5c6801a8":"feature_correlation_analysis(train_0,category_encoding,category_encoding_seniority_0,'category',['month_id','item_category_id'],0)\ncompare_features(category_encoding,category_encoding_seniority_0,'category',['month_id','item_category_id'],0,20,20,20)","98bf1da1":"feature_correlation_analysis(train_0,supercategory_encoding,supercategory_encoding_seniority_0,'supercategory',['month_id','item_supercategory_id'],0)\ncompare_features(supercategory_encoding,supercategory_encoding_seniority_0,'supercategory',['month_id','item_supercategory_id'],0,20,6,20)","7c78740a":"feature_correlation_analysis(train_0,shop_category_encoding,shop_category_encoding_seniority_0,'shop_category',['month_id','shop_id','item_category_id'],0)\ncompare_features(shop_category_encoding,shop_category_encoding_seniority_0,'shop_category',['month_id','shop_id','item_category_id'],0,20,20,20)","a6a62fff":"feature_correlation_analysis(train_0,shop_supercategory_encoding,shop_supercategory_encoding_seniority_0,'shop_supercategory',['month_id','shop_id','item_supercategory_id'],0)\ncompare_features(shop_supercategory_encoding,shop_supercategory_encoding_seniority_0,'shop_supercategory',['month_id','shop_id','item_supercategory_id'],0,20,20,20)","ad835d23":"ts = time.time()\n\ntarget_features_df=[]\n\n#############################################################\n# SHOP\ncol_labels=['shop_avg_sales']\ncol_labels_seniority_0=[col+'_seniority_0' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_0,shop_encoding_seniority_0,['month_id','shop_id'],col_labels_seniority_0,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_encoding_seniority_0,['month_id','shop_id'],col_labels_seniority_0,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_encoding_seniority_0,['month_id','shop_id'],col_labels_seniority_0,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_encoding_seniority_0,['month_id','shop_id'],col_labels_seniority_0,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_0,shop_encoding_seniority_0,['month_id','shop_id'],col_labels_seniority_0,[1]))\ntarget_features_df.append(lag_features(train_0,shop_encoding_seniority_0,['month_id','shop_id'],col_labels_seniority_0,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n#############################################################\n# SUPERCATEGORY\ncol_labels=['supercategory_avg_sales']\ncol_labels_seniority_0=[col+'_seniority_0' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_0,supercategory_encoding_seniority_0,['month_id','item_supercategory_id'],col_labels_seniority_0,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_0,supercategory_encoding_seniority_0,['month_id','item_supercategory_id'],col_labels_seniority_0,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_0,supercategory_encoding_seniority_0,['month_id','item_supercategory_id'],col_labels_seniority_0,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_0,supercategory_encoding_seniority_0,['month_id','item_supercategory_id'],col_labels_seniority_0,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_0,supercategory_encoding_seniority_0,['month_id','item_supercategory_id'],col_labels_seniority_0,[1]))\ntarget_features_df.append(lag_features(train_0,supercategory_encoding_seniority_0,['month_id','item_supercategory_id'],col_labels_seniority_0,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n#############################################################\n# SHOP-SUPERCATEGORY\ncol_labels=['shop_supercategory_avg_sales']\ncol_labels_seniority_0=[col+'_seniority_0' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_0,shop_supercategory_encoding_seniority_0,['month_id','shop_id','item_supercategory_id'],col_labels_seniority_0,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_supercategory_encoding_seniority_0,['month_id','shop_id','item_supercategory_id'],col_labels_seniority_0,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_supercategory_encoding_seniority_0,['month_id','shop_id','item_supercategory_id'],col_labels_seniority_0,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_supercategory_encoding_seniority_0,['month_id','shop_id','item_supercategory_id'],col_labels_seniority_0,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_0,shop_supercategory_encoding_seniority_0,['month_id','shop_id','item_supercategory_id'],col_labels_seniority_0,[1]))\ntarget_features_df.append(lag_features(train_0,shop_supercategory_encoding_seniority_0,['month_id','shop_id','item_supercategory_id'],col_labels_seniority_0,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n#############################################################\n# CATEGORY\ncol_labels=['category_avg_sales']\ncol_labels_seniority_0=[col+'_seniority_0' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_0,category_encoding_seniority_0,['month_id','item_category_id'],col_labels_seniority_0,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_0,category_encoding_seniority_0,['month_id','item_category_id'],col_labels_seniority_0,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_0,category_encoding_seniority_0,['month_id','item_category_id'],col_labels_seniority_0,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_0,category_encoding_seniority_0,['month_id','item_category_id'],col_labels_seniority_0,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_0,category_encoding_seniority_0,['month_id','item_category_id'],col_labels_seniority_0,[1]))\ntarget_features_df.append(lag_features(train_0,category_encoding_seniority_0,['month_id','item_category_id'],col_labels_seniority_0,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n#############################################################\n# SHOP-CATEGORY\ncol_labels=['shop_category_avg_sales']\ncol_labels_seniority_0=[col+'_seniority_0' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_0,shop_category_encoding_seniority_0,['month_id','shop_id','item_category_id'],col_labels_seniority_0,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_category_encoding_seniority_0,['month_id','shop_id','item_category_id'],col_labels_seniority_0,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_category_encoding_seniority_0,['month_id','shop_id','item_category_id'],col_labels_seniority_0,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_category_encoding_seniority_0,['month_id','shop_id','item_category_id'],col_labels_seniority_0,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_0,shop_category_encoding_seniority_0,['month_id','shop_id','item_category_id'],col_labels_seniority_0,[1]))\ntarget_features_df.append(lag_features(train_0,shop_category_encoding_seniority_0,['month_id','shop_id','item_category_id'],col_labels_seniority_0,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n\n\n\n\n\n#############################################################\n# SPATIAL TRENDS\n# assess whether shop sells more of this category than other shops (to relate to category-specific data)\ncol_labels=['shop_category_avg_sales_compared_to_category']\n\ntarget_features_df.append(moving_statistics(train_0,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_0,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,[1]))\ntarget_features_df.append(lag_features(train_0,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,[12]))\n\ncol_labels=['shop_supercategory_avg_sales_compared_to_supercategory']\n\ntarget_features_df.append(moving_statistics(train_0,shop_supercategory_encoding,['month_id','shop_id','item_supercategory_id'],col_labels,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_supercategory_encoding,['month_id','shop_id','item_supercategory_id'],col_labels,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_supercategory_encoding,['month_id','shop_id','item_supercategory_id'],col_labels,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_0,shop_supercategory_encoding,['month_id','shop_id','item_supercategory_id'],col_labels,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_0,shop_supercategory_encoding,['month_id','shop_id','item_supercategory_id'],col_labels,[1]))\ntarget_features_df.append(lag_features(train_0,shop_supercategory_encoding,['month_id','shop_id','item_supercategory_id'],col_labels,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n#############################################################################\n# TEMPORAL TRENDS (encode month or recent period wrt to how it compares to other month of the year)\n\n# SHOPS\ncol_labels=['shop_avg_sales']\ncol_labels_seniority_0=[col+'_seniority_0' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_0,shop_encoding,['month_id','shop_id'],col_labels,weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_0,shop_encoding,['month_id','shop_id'],col_labels,weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_0,shop_encoding,['month_id','shop_id'],col_labels,weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\n# CATEGORY\ncol_labels=['category_avg_sales']\ncol_labels_seniority_0=[col+'_seniority_0' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_0,category_encoding,['month_id','item_category_id'],['category_avg_sales'],weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_0,category_encoding,['month_id','item_category_id'],['category_avg_sales'],weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_0,category_encoding,['month_id','item_category_id'],['category_avg_sales'],weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\n# decouple SUPERCATEGORY (long-term trend) \/ CATEGORY within SUPERCATEGORY (short-term trend)\ncol_labels=['supercategory_avg_sales']\ncol_labels_seniority_0=[col+'_seniority_0' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_0,supercategory_encoding,['month_id','item_supercategory_id'],col_labels,weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_0,supercategory_encoding,['month_id','item_supercategory_id'],col_labels,weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_0,supercategory_encoding,['month_id','item_supercategory_id'],col_labels,weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\ncol_labels=['category_avg_sales_compared_to_supercategory']\ncol_labels_seniority_0=[col+'_seniority_0' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_0,category_encoding,['month_id','item_category_id'],col_labels,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_0,category_encoding,['month_id','item_category_id'],col_labels,[1]))\ntarget_features_df.append(lag_features(train_0,category_encoding,['month_id','item_category_id'],col_labels,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n#############################################################################\n# TEMPORAL TRENDS (are items of seniority 0 sold in larger quantities at given time of the year?)\n\n# SHOPS\ncol_labels=['shop_avg_sales']\ncol_labels_seniority_0=[col+'_seniority_0' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_0,shop_encoding_seniority_0,['month_id','shop_id'],col_labels_seniority_0,weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_0,shop_encoding_seniority_0,['month_id','shop_id'],col_labels_seniority_0,weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_0,shop_encoding_seniority_0,['month_id','shop_id'],col_labels_seniority_0,weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\n# SUPERCATEGORY\ncol_labels=['supercategory_avg_sales']\ncol_labels_seniority_0=[col+'_seniority_0' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_0,supercategory_encoding_seniority_0,['month_id','item_supercategory_id'],col_labels_seniority_0,weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_0,supercategory_encoding_seniority_0,['month_id','item_supercategory_id'],col_labels_seniority_0,weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_0,supercategory_encoding_seniority_0,['month_id','item_supercategory_id'],col_labels_seniority_0,weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\n# CATEGORY\ncol_labels=['category_avg_sales']\ncol_labels_seniority_0=[col+'_seniority_0' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_0,category_encoding_seniority_0,['month_id','item_category_id'],col_labels_seniority_0,weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_0,category_encoding_seniority_0,['month_id','item_category_id'],col_labels_seniority_0,weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_0,category_encoding_seniority_0,['month_id','item_category_id'],col_labels_seniority_0,weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n\n\n\n\n\n# -------------------------------------------------\n# CONCATENATE FEATURES\n\ntrain_0=pd.concat([train_0]+target_features_df,axis=1,sort=False)\ndel target_features_df\ngc.collect()\n                         \nprint('time : ' +str(time.time() - ts))\n\n# -------------------------------------------------\nprint(train_0.info(null_counts=True,verbose=True))\ntrain_0","b81087d5":"# clear memory space\n\nfor df in dfs:\n    exec(\"del \"+df)\n    exec(\"del \"+df+\"_seniority_0\")\n\ngc.collect()","a472c966":"train_0.drop(features_to_discard,axis=1,inplace=True)\n\ntrain_0.info(null_counts=True,verbose=True)","50c953de":"# create directory\ncreate_directory(os.path.join(DATA_FOLDER, 'training'))\n\n# export dataset\ntrain_0.to_pickle(os.path.join(DATA_FOLDER,'training\/train_0_pred.pkl'))","6e411636":"# clear memory\ndel train_0\n\ngc.collect()","620f1032":"reset_variable_space","ada316e4":"# import dataset\ntrain_1=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/train_1.pkl'))\n\nprint(train_1.info(null_counts=True))","e9a0089b":"# RAW FEATURES SELECTION\nsplit_col='month_id'\ntarget_col='item_quantity'\nraw_features=[]\n\n# label encodings must be retained at this stage for aggregating features\n# most should be discarded later on\nlabel_encoding=['shop_id',\n                'item_id',\n                'item_category_id',\n                'item_supercategory_id',\n                'item_category_console_id',\n                'item_category_is_digital'\n               ]\n\n# mapping info for aggregation (to be discarded later)\nmapping_columns=['item_month_id_of_last_sale',\n                 'item_month_id_of_release',\n                 'item_month_of_release'\n                ]\n\n# define already the columns to discard after aggregation of all features\nfeatures_to_discard=['item_id',\n                     'shop_id',\n                     'item_supercategory_id',\n                     'item_category_console_id',\n                     'item_category_is_digital'\n                    ]\nfeatures_to_discard+=mapping_columns\n\n\n#-----------------------------------\n# SHOPS\n\n# months since opening\nraw_features+=['shop_months_since_opening']\nraw_features+=['shop_opening']\n\n#-----------------------------------\n# ITEMS\n\n# month-specific frequency encoding\nraw_features+=['item_freq_in_seniority']\n\n#-----------------------------------\n# ITEM CATEGORIES\n\n# month-specific frequency encoding\nraw_features+=['item_category_freq']\nraw_features+=['item_supercategory_freq']\nraw_features+=['item_category_console_freq']\nraw_features+=['item_category_digital_freq']\n\nraw_features+=['item_category_freq_in_seniority']\nraw_features+=['item_supercategory_freq_in_seniority']\nraw_features+=['item_category_console_freq_in_seniority']\nraw_features+=['item_category_digital_freq_in_seniority']\n\n# encoding of category temporal variability\nraw_features+=['category_semiannual_avg_recent_rsd']\nraw_features+=['category_semiannual_avg_annual_rsd']\n\n\n#-----------------------------------\n# RELATIVE TIME FEATURES\nraw_features+=['item_months_since_release']\n\n#raw_features+=['item_months_since_first_sale_in_shop']   # doesn't make sense for seniority 1 (items have never been sold in shop)\n#raw_features+=['item_months_since_last_sale_in_shop']    # doesn't make sense for seniority 1 (items have never been sold in shop)\n\n\ntrain_1=train_1[[split_col]+label_encoding+mapping_columns+[target_col]+raw_features]\n\nprint(train_1.info(null_counts=True))\n\ngc.collect()","cee18af8":"# import price data\n\nitem_prices=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/price_features\/item_prices.pkl'))\ncategory_prices=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/price_features\/category_prices.pkl'))","910fa44e":"ts = time.time()\n\nprice_features_df=[]\n\n# -------------------------------------------------\n# ITEM\n# infer item typical price from values observed in the shops where it was most recently sold\ncol_labels=['item_price_median']\ncol_labels+=['item_price_median_compared_to_category_price_median']\nprice_features_df.append(time_mapping(train_1,item_prices,['item_id'],col_labels,'item_month_id_of_last_sale',suffix='_last_sale'))\nprice_features_df.append(moving_statistics(train_1,item_prices,['month_id','item_id'],col_labels,34,'mean',suffix='_absolute_mean'))\n\n# infer range of possible price values from all values observed over time in the shops where it was sold\ntime_window=34\nprice_features_df.append(moving_statistics(train_1,item_prices,['month_id','item_id'],['item_price_min'],34,'min',suffix='_absolute_min'))\nprice_features_df.append(moving_statistics(train_1,item_prices,['month_id','item_id'],['item_price_max'],34,'max',suffix='_absolute_max'))\n\nprint('time : ' +str(time.time() - ts))\n\n\n# -------------------------------------------------\n# CATEGORY\n# assess typical price range for this category of items recently\nprice_features_df.append(moving_statistics(train_1,category_prices,['month_id','item_category_id'],['category_price_median'],3,'mean',suffix='_recent_mean'))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n# -------------------------------------------------\n# CONCATENATE FEATURES\n\ntrain_1=pd.concat([train_1]+price_features_df,axis=1,sort=False)\ndel price_features_df\ngc.collect()\n\nprint('time : ' +str(time.time() - ts))\n\n\n# -------------------------------------------------\n# DOWNCAST DTYPES\ntrain_1=downcast_dtypes(train_1)\nprint('time : ' +str(time.time() - ts))\n\n\nprint(train_1.info(null_counts=True))\ntrain_1","2d4a8022":"# clear memory space\n\ndel item_prices\ndel category_prices\n\ngc.collect()","eb51e498":"dfs=['shop_encoding',\n     'item_encoding',\n     'category_encoding',\n     'supercategory_encoding',\n     'shop_category_encoding',\n     'shop_supercategory_encoding',\n     'category_months_since_release_encoding',\n     'supercategory_months_since_release_encoding',\n     'category_month_of_release_encoding',\n     'supercategory_month_of_release_encoding',\n     'shop_category_months_since_release_encoding',\n     'shop_supercategory_months_since_release_encoding'\n    ]\n\n    \nfor df in dfs:\n    exec(df+\"=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/target_encodings\/\"+df+\".pkl'))\")\n    exec(df+\"_seniority_1=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/target_encodings\/\"+df+\"_seniority_1.pkl'))\")","3792816a":"feature_correlation_analysis(train_1,item_encoding,item_encoding_seniority_1,'item',['month_id','item_id'],1)\ncompare_features(item_encoding,item_encoding_seniority_1,'item',['month_id','item_id'],1,20,20,20)","325b2d33":"feature_correlation_analysis(train_1,shop_encoding,shop_encoding_seniority_1,'shop',['month_id','shop_id'],1)\ncompare_features(shop_encoding,shop_encoding_seniority_1,'shop',['month_id','shop_id'],1,2,2,4)","bec28e4d":"feature_correlation_analysis(train_1,category_encoding,category_encoding_seniority_1,'category',['month_id','item_category_id'],1)\ncompare_features(category_encoding,category_encoding_seniority_1,'category',['month_id','item_category_id'],1,20,13,20)","8020caff":"feature_correlation_analysis(train_1,supercategory_encoding,supercategory_encoding_seniority_1,'supercategory',['month_id','item_supercategory_id'],1)\ncompare_features(supercategory_encoding,supercategory_encoding_seniority_1,'supercategory',['month_id','item_supercategory_id'],1,20,3,20)","6c8afbd7":"feature_correlation_analysis(train_1,shop_category_encoding,shop_category_encoding_seniority_1,'shop_category',['month_id','shop_id','item_category_id'],1)\ncompare_features(shop_category_encoding,shop_category_encoding_seniority_1,'shop_category',['month_id','shop_id','item_category_id'],1,20,20,20)","5c119e76":"feature_correlation_analysis(train_1,shop_supercategory_encoding,shop_supercategory_encoding_seniority_1,'shop_supercategory',['month_id','shop_id','item_supercategory_id'],1)\ncompare_features(shop_supercategory_encoding,shop_supercategory_encoding_seniority_1,'shop_supercategory',['month_id','shop_id','item_supercategory_id'],1,20,20,20)","d13b3e82":"feature_correlation_analysis(train_1,category_months_since_release_encoding,category_months_since_release_encoding_seniority_1,'category_months_since_release',['month_id','item_category_id','item_months_since_release'],1)\ncompare_features(category_months_since_release_encoding,category_months_since_release_encoding_seniority_1,'category_months_since_release',['month_id','item_category_id','item_months_since_release'],1,20,20,20)","0ea47aca":"feature_correlation_analysis(train_1,shop_category_months_since_release_encoding,shop_category_months_since_release_encoding_seniority_1,'shop_category_months_since_release',['month_id','shop_id','item_category_id','item_months_since_release'],1)","d1518358":"ts = time.time()\n\ntarget_features_df=[]\n\n#############################################################\n# ITEM\ncol_labels=['item_avg_sales','item_avg_sales_over_sold']\n\ntarget_features_df.append(moving_statistics(train_1,item_encoding,['month_id','item_id'],col_labels,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_1,item_encoding,['month_id','item_id'],col_labels,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_1,item_encoding,['month_id','item_id'],col_labels,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_1,item_encoding,['month_id','item_id'],col_labels,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_1,item_encoding,['month_id','item_id'],col_labels,[1]))\ntarget_features_df.append(lag_features(train_1,item_encoding,['month_id','item_id'],col_labels,[12]))\n\ntarget_features_df.append(time_mapping(train_1,item_encoding,['item_id'],col_labels,'item_month_id_of_last_sale',suffix='_last_sale'))\n\n# maximum amount sold observed over time in the shops where it has been sold\ntarget_features_df.append(moving_statistics(train_1,item_encoding,['month_id','item_id'],['item_max_quantity'],34,'max',suffix='_absolute_max'))\n\n\n# Seniority 1\ncol_labels_seniority_1=['item_avg_sales_seniority_1']\n\ntarget_features_df.append(moving_statistics(train_1,item_encoding_seniority_1,['month_id','item_id'],col_labels_seniority_1,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_1,item_encoding_seniority_1,['month_id','item_id'],col_labels_seniority_1,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_1,item_encoding_seniority_1,['month_id','item_id'],col_labels_seniority_1,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_1,item_encoding_seniority_1,['month_id','item_id'],col_labels_seniority_1,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_1,item_encoding_seniority_1,['month_id','item_id'],col_labels_seniority_1,[1]))\ntarget_features_df.append(lag_features(train_1,item_encoding_seniority_1,['month_id','item_id'],col_labels_seniority_1,[12]))\n\ntarget_features_df.append(time_mapping(train_1,item_encoding_seniority_1,['item_id'],col_labels_seniority_1,'item_month_id_of_last_sale',suffix='_last_sale'))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n#############################################################\n# SHOP\ncol_labels=['shop_avg_sales']\ncol_labels_seniority_1=[col+'_seniority_1' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_1,shop_encoding,['month_id','shop_id'],col_labels,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_encoding,['month_id','shop_id'],col_labels,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_encoding,['month_id','shop_id'],col_labels,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_encoding,['month_id','shop_id'],col_labels,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_1,shop_encoding,['month_id','shop_id'],col_labels,[1]))\ntarget_features_df.append(lag_features(train_1,shop_encoding,['month_id','shop_id'],col_labels,[12]))\n\ntarget_features_df.append(moving_statistics(train_1,shop_encoding_seniority_1,['month_id','shop_id'],col_labels_seniority_1,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_encoding_seniority_1,['month_id','shop_id'],col_labels_seniority_1,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_encoding_seniority_1,['month_id','shop_id'],col_labels_seniority_1,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_encoding_seniority_1,['month_id','shop_id'],col_labels_seniority_1,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_1,shop_encoding_seniority_1,['month_id','shop_id'],col_labels_seniority_1,[1]))\ntarget_features_df.append(lag_features(train_1,shop_encoding_seniority_1,['month_id','shop_id'],col_labels_seniority_1,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n#############################################################\n# MONTHS SINCE RELEASE\ncol_labels=['category_months_since_release_avg_sales']\ncol_labels_seniority_1=[col+'_seniority_1' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_1,category_months_since_release_encoding_seniority_1,['month_id','item_category_id','item_months_since_release'],col_labels_seniority_1,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_1,category_months_since_release_encoding_seniority_1,['month_id','item_category_id','item_months_since_release'],col_labels_seniority_1,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_1,category_months_since_release_encoding_seniority_1,['month_id','item_category_id','item_months_since_release'],col_labels_seniority_1,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_1,category_months_since_release_encoding_seniority_1,['month_id','item_category_id','item_months_since_release'],col_labels_seniority_1,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_1,category_months_since_release_encoding_seniority_1,['month_id','item_category_id','item_months_since_release'],col_labels_seniority_1,[1]))\ntarget_features_df.append(lag_features(train_1,category_months_since_release_encoding_seniority_1,['month_id','item_category_id','item_months_since_release'],col_labels_seniority_1,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\ncol_labels=['shop_category_months_since_release_avg_sales']\ncol_labels_seniority_1=[col+'_seniority_1' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_1,shop_category_months_since_release_encoding_seniority_1,['month_id','shop_id','item_category_id','item_months_since_release'],col_labels_seniority_1,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_category_months_since_release_encoding_seniority_1,['month_id','shop_id','item_category_id','item_months_since_release'],col_labels_seniority_1,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_category_months_since_release_encoding_seniority_1,['month_id','shop_id','item_category_id','item_months_since_release'],col_labels_seniority_1,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_category_months_since_release_encoding_seniority_1,['month_id','shop_id','item_category_id','item_months_since_release'],col_labels_seniority_1,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_1,shop_category_months_since_release_encoding_seniority_1,['month_id','shop_id','item_category_id','item_months_since_release'],col_labels_seniority_1,[1]))\ntarget_features_df.append(lag_features(train_1,shop_category_months_since_release_encoding_seniority_1,['month_id','shop_id','item_category_id','item_months_since_release'],col_labels_seniority_1,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\ncol_labels=['supercategory_months_since_release_avg_sales']\ncol_labels_seniority_1=[col+'_seniority_1' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_1,supercategory_months_since_release_encoding_seniority_1,['month_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_1,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_1,supercategory_months_since_release_encoding_seniority_1,['month_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_1,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_1,supercategory_months_since_release_encoding_seniority_1,['month_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_1,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_1,supercategory_months_since_release_encoding_seniority_1,['month_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_1,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_1,supercategory_months_since_release_encoding_seniority_1,['month_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_1,[1]))\ntarget_features_df.append(lag_features(train_1,supercategory_months_since_release_encoding_seniority_1,['month_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_1,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\ncol_labels=['shop_supercategory_months_since_release_avg_sales']\ncol_labels_seniority_1=[col+'_seniority_1' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_1,shop_supercategory_months_since_release_encoding_seniority_1,['month_id','shop_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_1,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_supercategory_months_since_release_encoding_seniority_1,['month_id','shop_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_1,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_supercategory_months_since_release_encoding_seniority_1,['month_id','shop_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_1,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_supercategory_months_since_release_encoding_seniority_1,['month_id','shop_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_1,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_1,shop_supercategory_months_since_release_encoding_seniority_1,['month_id','shop_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_1,[1]))\ntarget_features_df.append(lag_features(train_1,shop_supercategory_months_since_release_encoding_seniority_1,['month_id','shop_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_1,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n\n\n\n\n\n#############################################################\n# SPATIAL TRENDS\n# assess whether shop sells more of this category than other shops (to relate to category-specific data)\ncol_labels=['shop_category_avg_sales_compared_to_category']\n\ntarget_features_df.append(moving_statistics(train_1,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_1,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_1,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,[1]))\ntarget_features_df.append(lag_features(train_1,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n#############################################################################\n# TEMPORAL TRENDS (encode month or recent period wrt to how it compares to other month of the year)\n\n# SHOPS\ncol_labels=['shop_avg_sales']\ncol_labels_seniority_1=[col+'_seniority_1' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_1,shop_encoding,['month_id','shop_id'],col_labels,weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_1,shop_encoding,['month_id','shop_id'],col_labels,weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_1,shop_encoding,['month_id','shop_id'],col_labels,weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\n# CATEGORY\ncol_labels=['category_avg_sales']\ncol_labels_seniority_1=[col+'_seniority_1' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_1,category_encoding,['month_id','item_category_id'],['category_avg_sales'],weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_1,category_encoding,['month_id','item_category_id'],['category_avg_sales'],weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_1,category_encoding,['month_id','item_category_id'],['category_avg_sales'],weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\n# decouple SUPERCATEGORY (long-term trend) \/ CATEGORY within SUPERCATEGORY (short-term trend)\ncol_labels=['supercategory_avg_sales']\ncol_labels_seniority_1=[col+'_seniority_1' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_1,supercategory_encoding,['month_id','item_supercategory_id'],col_labels,weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_1,supercategory_encoding,['month_id','item_supercategory_id'],col_labels,weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_1,supercategory_encoding,['month_id','item_supercategory_id'],col_labels,weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\ncol_labels=['category_avg_sales_compared_to_supercategory']\ncol_labels_seniority_1=[col+'_seniority_1' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_1,category_encoding,['month_id','item_category_id'],col_labels,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_1,category_encoding,['month_id','item_category_id'],col_labels,[1]))\ntarget_features_df.append(lag_features(train_1,category_encoding,['month_id','item_category_id'],col_labels,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n#############################################################################\n# MONTH OF RELEASE\ncol_labels=['category_month_of_release_avg_sales_compared_to_category']\ntarget_features_df.append(moving_statistics(train_1,category_month_of_release_encoding,['month_id','item_category_id','item_month_of_release'],col_labels,34,'mean',suffix='_absolute_mean'))\n\ncol_labels=['supercategory_month_of_release_avg_sales_compared_to_supercategory']\ntarget_features_df.append(moving_statistics(train_1,supercategory_month_of_release_encoding,['month_id','item_supercategory_id','item_month_of_release'],col_labels,34,'mean',suffix='_absolute_mean'))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n\n\n\n\n\n# -------------------------------------------------\n# CONCATENATE FEATURES\n\ntrain_1=pd.concat([train_1]+target_features_df,axis=1,sort=False)\ndel target_features_df\ngc.collect()\n                         \nprint('time : ' +str(time.time() - ts))\n\n# -------------------------------------------------\nprint(train_1.info(null_counts=True,verbose=True))\ntrain_1","fd52d44e":"# clear memory space\n\nfor df in dfs:\n    exec(\"del \"+df)\n    exec(\"del \"+df+\"_seniority_1\")\n\ngc.collect()","65c3e287":"train_1.drop(features_to_discard,axis=1,inplace=True)\n\ntrain_1.info(null_counts=True,verbose=True)","d4c38b0b":"# create directory\ncreate_directory(os.path.join(DATA_FOLDER, 'training'))\n\n# export dataset\ntrain_1.to_pickle(os.path.join(DATA_FOLDER,'training\/train_1_pred.pkl'))","c378a0f5":"# clear memory\ndel train_1\n\ngc.collect()","22f109a7":"reset_variable_space","7ac1a207":"# import dataset\ntrain_2=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/train_2.pkl'))\n\nprint(train_2.info(null_counts=True))","295bb619":"# RAW FEATURES SELECTION\nsplit_col='month_id'\ntarget_col='item_quantity'\nraw_features=[]\n\n# label encodings must be retained at this stage for aggregating features\n# most should be discarded later on\nlabel_encoding=['shop_id',\n                'item_id',\n                'item_category_id',\n                'item_supercategory_id',\n                'item_category_console_id',\n                'item_category_is_digital'\n               ]\n\n# mapping info for aggregation (to be discarded later)\nmapping_columns=['item_month_id_of_last_sale',\n                 'item_month_id_of_release',\n                 'item_month_of_release',\n                 'item_month_id_of_last_sale_in_shop',\n                 'item_month_id_of_first_sale_in_shop'\n                ]\n\n# define already the columns to discard after aggregation of all features\nfeatures_to_discard=['item_id',\n                     'shop_id',\n                     'item_supercategory_id',\n                     'item_category_console_id',\n                     'item_category_is_digital'\n                    ]\nfeatures_to_discard+=mapping_columns\n\n\n#-----------------------------------\n# SHOPS\n\n# months since opening\nraw_features+=['shop_months_since_opening']\n#raw_features+=['shop_opening']     # uniformly 0 in seniority 2 (no shop that just opened this month can possibly have already sold any item in the past)\n\n#-----------------------------------\n# ITEMS\n\n# month-specific frequency encoding\nraw_features+=['item_freq_in_seniority']\n\n#-----------------------------------\n# ITEM CATEGORIES\n\n# month-specific frequency encoding\nraw_features+=['item_category_freq']\nraw_features+=['item_supercategory_freq']\nraw_features+=['item_category_console_freq']\nraw_features+=['item_category_digital_freq']\n\nraw_features+=['item_category_freq_in_seniority']\nraw_features+=['item_supercategory_freq_in_seniority']\nraw_features+=['item_category_console_freq_in_seniority']\nraw_features+=['item_category_digital_freq_in_seniority']\n\n# encoding of category temporal variability\nraw_features+=['category_semiannual_avg_recent_rsd']\nraw_features+=['category_semiannual_avg_annual_rsd']\n\n\n#-----------------------------------\n# RELATIVE TIME FEATURES\nraw_features+=['item_months_since_release']\n\nraw_features+=['item_months_since_first_sale_in_shop']\nraw_features+=['item_months_since_last_sale_in_shop']\n\n\ntrain_2=train_2[[split_col]+label_encoding+mapping_columns+[target_col]+raw_features]\n\nprint(train_2.info(null_counts=True))\n\ngc.collect()","f87cd83a":"# import price data\n\nitem_prices=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/price_features\/item_prices.pkl'))\ncategory_prices=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/price_features\/category_prices.pkl'))","0d460f87":"ts = time.time()\n\nprice_features_df=[]\n\n# -------------------------------------------------\n# ITEM\n# infer item typical price from values observed in the shops where it was most recently sold\ncol_labels=['item_price_median']\ncol_labels+=['item_price_median_compared_to_category_price_median']\nprice_features_df.append(time_mapping(train_2,item_prices,['item_id'],col_labels,'item_month_id_of_last_sale',suffix='_last_sale'))\nprice_features_df.append(moving_statistics(train_2,item_prices,['month_id','item_id'],col_labels,34,'mean',suffix='_absolute_mean'))\n\n# infer range of possible price values from all values observed over time in the shops where it was sold\ntime_window=34\nprice_features_df.append(moving_statistics(train_2,item_prices,['month_id','item_id'],['item_price_min'],34,'min',suffix='_absolute_min'))\nprice_features_df.append(moving_statistics(train_2,item_prices,['month_id','item_id'],['item_price_max'],34,'max',suffix='_absolute_max'))\n\nprint('time : ' +str(time.time() - ts))\n\n\n# -------------------------------------------------\n# CATEGORY\n# assess typical price range for this category of items recently\nprice_features_df.append(moving_statistics(train_2,category_prices,['month_id','item_category_id'],['category_price_median'],3,'mean',suffix='_recent_mean'))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n# -------------------------------------------------\n# CONCATENATE FEATURES\n\ntrain_2=pd.concat([train_2]+price_features_df,axis=1,sort=False)\ndel price_features_df\ngc.collect()\n\nprint('time : ' +str(time.time() - ts))\n\n\n# -------------------------------------------------\n# DOWNCAST DTYPES\ntrain_2=downcast_dtypes(train_2)\nprint('time : ' +str(time.time() - ts))\n\n\nprint(train_2.info(null_counts=True))\ntrain_2","3f03cde2":"# clear memory space\n\ndel item_prices\ndel category_prices\n\ngc.collect()","de2cf0ab":"dfs=['shop_encoding',\n     'item_encoding',\n     'category_encoding',\n     'supercategory_encoding',\n     'shop_category_encoding',\n     'shop_supercategory_encoding',\n     'category_months_since_release_encoding',\n     'supercategory_months_since_release_encoding',\n     'category_month_of_release_encoding',\n     'supercategory_month_of_release_encoding',\n     'category_months_since_first_sale_in_shop_encoding',\n     'supercategory_months_since_first_sale_in_shop_encoding',\n     'shop_category_months_since_release_encoding',\n     'shop_supercategory_months_since_release_encoding',\n     'shop_category_months_since_first_sale_in_shop_encoding',\n     'shop_supercategory_months_since_first_sale_in_shop_encoding'\n    ]\n\n    \nfor df in dfs:\n    exec(df+\"=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/target_encodings\/\"+df+\".pkl'))\")\n    exec(df+\"_seniority_2=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/target_encodings\/\"+df+\"_seniority_2.pkl'))\")\n\nshop_item_encoding=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/target_encodings\/shop_item_encoding.pkl'))","f878ae03":"tmp=lag_features(train_2,shop_item_encoding,['month_id','shop_id','item_id'],['item_quantity'],[1,2,3,4,5,6],fill_value=None)\ntmp=pd.concat([train_2[['month_id','shop_id','item_id','item_quantity']],tmp],axis=1,sort=False)\ntmp.info(null_counts=True)\n    \ntmp=train_2[['month_id','shop_id','item_id']].join(shop_item_encoding.set_index(['month_id','shop_id','item_id']),on=['month_id','shop_id','item_id'])\nsns.heatmap(tmp.drop(['month_id','shop_id','item_id','item_quantity_compared_to_shop_avg_sales','item_quantity_compared_to_item_avg_sales','item_quantity_compared_to_item_avg_sales_over_sold','item_quantity_compared_to_shop_avg_sales_over_sold'],axis=1).corr(),annot=True)\n\ndel tmp","4898151b":"feature_correlation_analysis(train_2,item_encoding,item_encoding_seniority_2,'item',['month_id','item_id'],2)\ncompare_features(item_encoding,item_encoding_seniority_2,'item',['month_id','item_id'],2,20,20,20)","4a9bc310":"feature_correlation_analysis(train_2,shop_encoding,shop_encoding_seniority_2,'shop',['month_id','shop_id'],2)\ncompare_features(shop_encoding,shop_encoding_seniority_2,'shop',['month_id','shop_id'],2,2,4,4)","67cd7905":"feature_correlation_analysis(train_2,category_encoding,category_encoding_seniority_2,'category',['month_id','item_category_id'],2)\ncompare_features(category_encoding,category_encoding_seniority_2,'category',['month_id','item_category_id'],2,20,20,20)","7f5e121a":"feature_correlation_analysis(train_2,supercategory_encoding,supercategory_encoding_seniority_2,'supercategory',['month_id','item_supercategory_id'],2)\ncompare_features(supercategory_encoding,supercategory_encoding_seniority_2,'supercategory',['month_id','item_supercategory_id'],2,20,20,20)","6716e99c":"feature_correlation_analysis(train_2,shop_category_encoding,shop_category_encoding_seniority_2,'shop_category',['month_id','shop_id','item_category_id'],2)\ncompare_features(shop_category_encoding,shop_category_encoding_seniority_2,'shop_category',['month_id','shop_id','item_category_id'],2,20,20,20)","3043e90b":"feature_correlation_analysis(train_2,shop_supercategory_encoding,shop_supercategory_encoding_seniority_2,'shop_supercategory',['month_id','shop_id','item_supercategory_id'],2)\ncompare_features(shop_supercategory_encoding,shop_supercategory_encoding_seniority_2,'shop_supercategory',['month_id','shop_id','item_supercategory_id'],2,20,20,20)","ad2e53ca":"feature_correlation_analysis(train_2,category_months_since_release_encoding,category_months_since_release_encoding_seniority_2,'category_months_since_release',['month_id','item_category_id','item_months_since_release'],2)\ncompare_features(category_months_since_release_encoding,category_months_since_release_encoding_seniority_2,'category_months_since_release',['month_id','item_category_id','item_months_since_release'],2,20,20,20)","ae43b17f":"feature_correlation_analysis(train_2,category_months_since_first_sale_in_shop_encoding,category_months_since_first_sale_in_shop_encoding_seniority_2,'category_months_since_first_sale_in_shop',['month_id','item_category_id','item_months_since_first_sale_in_shop'],2)\ncompare_features(category_months_since_first_sale_in_shop_encoding,category_months_since_first_sale_in_shop_encoding_seniority_2,'category_months_since_first_sale_in_shop',['month_id','item_category_id','item_months_since_first_sale_in_shop'],2,20,20,20)","fd60ac49":"# correlation between 'month_since_release' and 'month_since_first_sale_in_shop'\ntmp=train_2[['month_id','shop_id','item_id','item_category_id','item_months_since_release','item_months_since_first_sale_in_shop']]\n\ntmp2=category_months_since_release_encoding_seniority_2.loc[(category_months_since_release_encoding_seniority_2['month_id']>12)&(category_months_since_release_encoding_seniority_2['month_id']<34),['month_id','item_category_id','item_months_since_release','category_months_since_release_avg_sales_seniority_2','category_months_since_release_avg_sales_over_sold_seniority_2']]\ntmp3=category_months_since_release_encoding.loc[(category_months_since_release_encoding['month_id']>12)&(category_months_since_release_encoding['month_id']<34),['month_id','item_category_id','item_months_since_release','category_months_since_release_avg_sales','category_months_since_release_avg_sales_over_sold']]\ntmp=tmp.join(tmp2.set_index(['month_id','item_category_id','item_months_since_release']),on=['month_id','item_category_id','item_months_since_release'])\ntmp=tmp.join(tmp3.set_index(['month_id','item_category_id','item_months_since_release']),on=['month_id','item_category_id','item_months_since_release'])\n\ntmp4=category_months_since_first_sale_in_shop_encoding.loc[(category_months_since_first_sale_in_shop_encoding['month_id']>12)&(category_months_since_first_sale_in_shop_encoding['month_id']<34),['month_id','item_category_id','item_months_since_first_sale_in_shop','category_months_since_first_sale_in_shop_avg_sales','category_months_since_first_sale_in_shop_avg_sales_over_sold']]\ntmp=tmp.join(tmp4.set_index(['month_id','item_category_id','item_months_since_first_sale_in_shop']),on=['month_id','item_category_id','item_months_since_first_sale_in_shop'])\n\nprint(tmp[['item_months_since_release','item_months_since_first_sale_in_shop']].corr().loc['item_months_since_release',:])\nsns.heatmap(tmp.drop(['month_id','shop_id','item_id','item_category_id','item_months_since_release','item_months_since_first_sale_in_shop'],axis=1).corr(),annot=True)             \n\ndel tmp, tmp2, tmp3, tmp4","ead2b1f9":"feature_correlation_analysis(train_2,shop_category_months_since_release_encoding,shop_category_months_since_release_encoding_seniority_2,'shop_category_months_since_release',['month_id','shop_id','item_category_id','item_months_since_release'],2)","ef6eaae0":"ts = time.time()\n\ntarget_features_df=[]\n    \n#############################################################\n# SHOP-ITEM\n\n# typical sale values\ntarget_features_df.append(moving_statistics(train_2,shop_item_encoding,['month_id','shop_id','item_id'],['item_quantity'],34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_item_encoding,['month_id','shop_id','item_id'],['item_quantity'],12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_item_encoding,['month_id','shop_id','item_id'],['item_quantity'],6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_item_encoding,['month_id','shop_id','item_id'],['item_quantity'],3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,shop_item_encoding,['month_id','shop_id','item_id'],['item_quantity'],[1]))\ntarget_features_df.append(lag_features(train_2,shop_item_encoding,['month_id','shop_id','item_id'],['item_quantity'],[12]))\n\n# extrapolation: linear predictions of sales in this shop based on sale quantities for the past 2 and 3 months\ntarget_features_df.append(linear_combination(train_2,shop_item_encoding,['month_id','shop_id','item_id'],['item_quantity'],weights=extralin2,suffix='_extralin2'))\ntarget_features_df.append(linear_combination(train_2,shop_item_encoding,['month_id','shop_id','item_id'],['item_quantity'],weights=extralin3,suffix='_extralin3'))\n\n# range of all possible sale quantities observed over time in this shop\ntarget_features_df.append(moving_statistics(train_2,shop_item_encoding,['month_id','shop_id','item_id'],['item_quantity'],34,'min',suffix='_absolute_min'))\ntarget_features_df.append(moving_statistics(train_2,shop_item_encoding,['month_id','shop_id','item_id'],['item_quantity'],34,'max',suffix='_absolute_max'))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n#############################################################\n# ITEM\ncol_labels=['item_avg_sales','item_avg_sales_over_sold']\n\ntarget_features_df.append(moving_statistics(train_2,item_encoding,['month_id','item_id'],col_labels,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_2,item_encoding,['month_id','item_id'],col_labels,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_2,item_encoding,['month_id','item_id'],col_labels,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_2,item_encoding,['month_id','item_id'],col_labels,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,item_encoding,['month_id','item_id'],col_labels,[1]))\ntarget_features_df.append(lag_features(train_2,item_encoding,['month_id','item_id'],col_labels,[12]))\n\ntarget_features_df.append(time_mapping(train_2,item_encoding,['item_id'],col_labels,'item_month_id_of_last_sale',suffix='_last_sale'))\n\n\n# Seniority 2\ncol_labels_seniority_2=['item_avg_sales_seniority_2']\n\ntarget_features_df.append(moving_statistics(train_2,item_encoding_seniority_2,['month_id','item_id'],col_labels_seniority_2,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_2,item_encoding_seniority_2,['month_id','item_id'],col_labels_seniority_2,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_2,item_encoding_seniority_2,['month_id','item_id'],col_labels_seniority_2,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_2,item_encoding_seniority_2,['month_id','item_id'],col_labels_seniority_2,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,item_encoding_seniority_2,['month_id','item_id'],col_labels_seniority_2,[1]))\ntarget_features_df.append(lag_features(train_2,item_encoding_seniority_2,['month_id','item_id'],col_labels_seniority_2,[12]))\n\ntarget_features_df.append(time_mapping(train_2,item_encoding_seniority_2,['item_id'],col_labels_seniority_2,'item_month_id_of_last_sale',suffix='_last_sale'))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n#############################################################\n# SHOP\ncol_labels=['shop_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_2,shop_encoding_seniority_2,['month_id','shop_id'],col_labels_seniority_2,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_encoding_seniority_2,['month_id','shop_id'],col_labels_seniority_2,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_encoding_seniority_2,['month_id','shop_id'],col_labels_seniority_2,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_encoding_seniority_2,['month_id','shop_id'],col_labels_seniority_2,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,shop_encoding_seniority_2,['month_id','shop_id'],col_labels_seniority_2,[1]))\ntarget_features_df.append(lag_features(train_2,shop_encoding_seniority_2,['month_id','shop_id'],col_labels_seniority_2,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n#############################################################\n# SUPERCATEGORY\ncol_labels=['supercategory_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_2,supercategory_encoding_seniority_2,['month_id','item_supercategory_id'],col_labels_seniority_2,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_2,supercategory_encoding_seniority_2,['month_id','item_supercategory_id'],col_labels_seniority_2,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_2,supercategory_encoding_seniority_2,['month_id','item_supercategory_id'],col_labels_seniority_2,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_2,supercategory_encoding_seniority_2,['month_id','item_supercategory_id'],col_labels_seniority_2,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,supercategory_encoding_seniority_2,['month_id','item_supercategory_id'],col_labels_seniority_2,[1]))\ntarget_features_df.append(lag_features(train_2,supercategory_encoding_seniority_2,['month_id','item_supercategory_id'],col_labels_seniority_2,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n#############################################################\n# SHOP-SUPERCATEGORY\ncol_labels=['shop_supercategory_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_2,shop_supercategory_encoding_seniority_2,['month_id','shop_id','item_supercategory_id'],col_labels_seniority_2,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_supercategory_encoding_seniority_2,['month_id','shop_id','item_supercategory_id'],col_labels_seniority_2,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_supercategory_encoding_seniority_2,['month_id','shop_id','item_supercategory_id'],col_labels_seniority_2,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_supercategory_encoding_seniority_2,['month_id','shop_id','item_supercategory_id'],col_labels_seniority_2,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,shop_supercategory_encoding_seniority_2,['month_id','shop_id','item_supercategory_id'],col_labels_seniority_2,[1]))\ntarget_features_df.append(lag_features(train_2,shop_supercategory_encoding_seniority_2,['month_id','shop_id','item_supercategory_id'],col_labels_seniority_2,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n#############################################################\n# CATEGORY\ncol_labels=['category_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_2,category_encoding_seniority_2,['month_id','item_category_id'],col_labels_seniority_2,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_2,category_encoding_seniority_2,['month_id','item_category_id'],col_labels_seniority_2,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_2,category_encoding_seniority_2,['month_id','item_category_id'],col_labels_seniority_2,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_2,category_encoding_seniority_2,['month_id','item_category_id'],col_labels_seniority_2,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,category_encoding_seniority_2,['month_id','item_category_id'],col_labels_seniority_2,[1]))\ntarget_features_df.append(lag_features(train_2,category_encoding_seniority_2,['month_id','item_category_id'],col_labels_seniority_2,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n#############################################################\n# SHOP-CATEGORY\ncol_labels=['shop_category_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_2,shop_category_encoding_seniority_2,['month_id','shop_id','item_category_id'],col_labels_seniority_2,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_category_encoding_seniority_2,['month_id','shop_id','item_category_id'],col_labels_seniority_2,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_category_encoding_seniority_2,['month_id','shop_id','item_category_id'],col_labels_seniority_2,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_category_encoding_seniority_2,['month_id','shop_id','item_category_id'],col_labels_seniority_2,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,shop_category_encoding_seniority_2,['month_id','shop_id','item_category_id'],col_labels_seniority_2,[1]))\ntarget_features_df.append(lag_features(train_2,shop_category_encoding_seniority_2,['month_id','shop_id','item_category_id'],col_labels_seniority_2,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n\n\n\n\n\n#############################################################\n# MONTHS SINCE RELEASE\ncol_labels=['category_months_since_release_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_2,category_months_since_release_encoding_seniority_2,['month_id','item_category_id','item_months_since_release'],col_labels_seniority_2,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_2,category_months_since_release_encoding_seniority_2,['month_id','item_category_id','item_months_since_release'],col_labels_seniority_2,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_2,category_months_since_release_encoding_seniority_2,['month_id','item_category_id','item_months_since_release'],col_labels_seniority_2,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_2,category_months_since_release_encoding_seniority_2,['month_id','item_category_id','item_months_since_release'],col_labels_seniority_2,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,category_months_since_release_encoding_seniority_2,['month_id','item_category_id','item_months_since_release'],col_labels_seniority_2,[1]))\ntarget_features_df.append(lag_features(train_2,category_months_since_release_encoding_seniority_2,['month_id','item_category_id','item_months_since_release'],col_labels_seniority_2,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\ncol_labels=['shop_category_months_since_release_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_2,shop_category_months_since_release_encoding_seniority_2,['month_id','shop_id','item_category_id','item_months_since_release'],col_labels_seniority_2,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_category_months_since_release_encoding_seniority_2,['month_id','shop_id','item_category_id','item_months_since_release'],col_labels_seniority_2,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_category_months_since_release_encoding_seniority_2,['month_id','shop_id','item_category_id','item_months_since_release'],col_labels_seniority_2,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_category_months_since_release_encoding_seniority_2,['month_id','shop_id','item_category_id','item_months_since_release'],col_labels_seniority_2,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,shop_category_months_since_release_encoding_seniority_2,['month_id','shop_id','item_category_id','item_months_since_release'],col_labels_seniority_2,[1]))\ntarget_features_df.append(lag_features(train_2,shop_category_months_since_release_encoding_seniority_2,['month_id','shop_id','item_category_id','item_months_since_release'],col_labels_seniority_2,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\ncol_labels=['supercategory_months_since_release_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_2,supercategory_months_since_release_encoding_seniority_2,['month_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_2,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_2,supercategory_months_since_release_encoding_seniority_2,['month_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_2,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_2,supercategory_months_since_release_encoding_seniority_2,['month_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_2,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_2,supercategory_months_since_release_encoding_seniority_2,['month_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_2,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,supercategory_months_since_release_encoding_seniority_2,['month_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_2,[1]))\ntarget_features_df.append(lag_features(train_2,supercategory_months_since_release_encoding_seniority_2,['month_id','item_supercategory_id','item_months_since_release'],col_labels_seniority_2,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n#############################################################\n# MONTHS SINCE FIRST SALE IN SHOP\ncol_labels=['category_months_since_first_sale_in_shop_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_2,category_months_since_first_sale_in_shop_encoding_seniority_2,['month_id','item_category_id','item_months_since_first_sale_in_shop'],col_labels_seniority_2,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_2,category_months_since_first_sale_in_shop_encoding_seniority_2,['month_id','item_category_id','item_months_since_first_sale_in_shop'],col_labels_seniority_2,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_2,category_months_since_first_sale_in_shop_encoding_seniority_2,['month_id','item_category_id','item_months_since_first_sale_in_shop'],col_labels_seniority_2,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_2,category_months_since_first_sale_in_shop_encoding_seniority_2,['month_id','item_category_id','item_months_since_first_sale_in_shop'],col_labels_seniority_2,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,category_months_since_first_sale_in_shop_encoding_seniority_2,['month_id','item_category_id','item_months_since_first_sale_in_shop'],col_labels_seniority_2,[1]))\ntarget_features_df.append(lag_features(train_2,category_months_since_first_sale_in_shop_encoding_seniority_2,['month_id','item_category_id','item_months_since_first_sale_in_shop'],col_labels_seniority_2,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\ncol_labels=['shop_category_months_since_first_sale_in_shop_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_2,shop_category_months_since_first_sale_in_shop_encoding_seniority_2,['month_id','shop_id','item_category_id','item_months_since_first_sale_in_shop'],col_labels_seniority_2,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_category_months_since_first_sale_in_shop_encoding_seniority_2,['month_id','shop_id','item_category_id','item_months_since_first_sale_in_shop'],col_labels_seniority_2,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_category_months_since_first_sale_in_shop_encoding_seniority_2,['month_id','shop_id','item_category_id','item_months_since_first_sale_in_shop'],col_labels_seniority_2,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_category_months_since_first_sale_in_shop_encoding_seniority_2,['month_id','shop_id','item_category_id','item_months_since_first_sale_in_shop'],col_labels_seniority_2,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,shop_category_months_since_first_sale_in_shop_encoding_seniority_2,['month_id','shop_id','item_category_id','item_months_since_first_sale_in_shop'],col_labels_seniority_2,[1]))\ntarget_features_df.append(lag_features(train_2,shop_category_months_since_first_sale_in_shop_encoding_seniority_2,['month_id','shop_id','item_category_id','item_months_since_first_sale_in_shop'],col_labels_seniority_2,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n\n\n\n\n\n#############################################################\n# SPATIAL TRENDS\n# assess whether shop sells more of this category than other shops (to relate to category-specific data)\ncol_labels=['shop_category_avg_sales_compared_to_category']\n\ntarget_features_df.append(moving_statistics(train_2,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,34,'mean',suffix='_absolute_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,12,'mean',suffix='_annual_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,6,'mean',suffix='_semiannual_mean'))\ntarget_features_df.append(moving_statistics(train_2,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,[1]))\ntarget_features_df.append(lag_features(train_2,shop_category_encoding,['month_id','shop_id','item_category_id'],col_labels,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n#############################################################################\n# TEMPORAL TRENDS (encode month or recent period wrt to how it compares to other month of the year)\n\n# SHOPS\ncol_labels=['shop_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_2,shop_encoding,['month_id','shop_id'],col_labels,weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_2,shop_encoding,['month_id','shop_id'],col_labels,weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_2,shop_encoding,['month_id','shop_id'],col_labels,weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\n# CATEGORY\ncol_labels=['category_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_2,category_encoding,['month_id','item_category_id'],['category_avg_sales'],weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_2,category_encoding,['month_id','item_category_id'],['category_avg_sales'],weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_2,category_encoding,['month_id','item_category_id'],['category_avg_sales'],weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\n# decouple SUPERCATEGORY (long-term trend) \/ CATEGORY within SUPERCATEGORY (short-term trend)\ncol_labels=['supercategory_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_2,supercategory_encoding,['month_id','item_supercategory_id'],col_labels,weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_2,supercategory_encoding,['month_id','item_supercategory_id'],col_labels,weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_2,supercategory_encoding,['month_id','item_supercategory_id'],col_labels,weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\ncol_labels=['category_avg_sales_compared_to_supercategory']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(moving_statistics(train_2,category_encoding,['month_id','item_category_id'],col_labels,3,'mean',suffix='_recent_mean'))\ntarget_features_df.append(lag_features(train_2,category_encoding,['month_id','item_category_id'],col_labels,[1]))\ntarget_features_df.append(lag_features(train_2,category_encoding,['month_id','item_category_id'],col_labels,[12]))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n#############################################################################\n# TEMPORAL TRENDS (are items of seniority 2 sold in larger quantities at given time of the year? for instance in january, when many items have been released for christmas? or on any month following a large release of new items)\n\n# SHOPS\ncol_labels=['shop_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_2,shop_encoding_seniority_2,['month_id','shop_id'],col_labels_seniority_2,weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_2,shop_encoding_seniority_2,['month_id','shop_id'],col_labels_seniority_2,weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_2,shop_encoding_seniority_2,['month_id','shop_id'],col_labels_seniority_2,weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\n# SUPERCATEGORY\ncol_labels=['supercategory_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_2,supercategory_encoding_seniority_2,['month_id','item_supercategory_id'],col_labels_seniority_2,weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_2,supercategory_encoding_seniority_2,['month_id','item_supercategory_id'],col_labels_seniority_2,weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_2,supercategory_encoding_seniority_2,['month_id','item_supercategory_id'],col_labels_seniority_2,weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\n# CATEGORY\ncol_labels=['category_avg_sales']\ncol_labels_seniority_2=[col+'_seniority_2' for col in col_labels]\n\ntarget_features_df.append(rational_fraction(train_2,category_encoding_seniority_2,['month_id','item_category_id'],col_labels_seniority_2,weights_num=[0,0,0,0,0,0,0,0,0,0,0,12],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag12_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_2,category_encoding_seniority_2,['month_id','item_category_id'],col_labels_seniority_2,weights_num=[12,0,0,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_lag1_to_annual_mean'))\ntarget_features_df.append(rational_fraction(train_2,category_encoding_seniority_2,['month_id','item_category_id'],col_labels_seniority_2,weights_num=[4,4,4,0,0,0,0,0,0,0,0,0],weights_denom=[1,1,1,1,1,1,1,1,1,1,1,1],suffix='_compare_recent_mean_to_annual_mean'))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n#############################################################################\n# MONTH OF RELEASE\ncol_labels=['category_month_of_release_avg_sales_compared_to_category']\ntarget_features_df.append(moving_statistics(train_2,category_month_of_release_encoding,['month_id','item_category_id','item_month_of_release'],col_labels,34,'mean',suffix='_absolute_mean'))\n\ncol_labels=['supercategory_month_of_release_avg_sales_compared_to_supercategory']\ntarget_features_df.append(moving_statistics(train_2,supercategory_month_of_release_encoding,['month_id','item_supercategory_id','item_month_of_release'],col_labels,34,'mean',suffix='_absolute_mean'))\n\nprint('time : ' +str(time.time() - ts))\n\n\n\n\n\n\n\n\n\n\n# -------------------------------------------------\n# CONCATENATE FEATURES\n\ntrain_2=pd.concat([train_2]+target_features_df,axis=1,sort=False)\ndel target_features_df\ngc.collect()\n                         \nprint('time : ' +str(time.time() - ts))\n\n# -------------------------------------------------\nprint(train_2.info(null_counts=True,verbose=True))\ntrain_2","da052930":"# clear memory space\n\nfor df in dfs:\n    exec(\"del \"+df)\n    exec(\"del \"+df+\"_seniority_2\")\n\ndel shop_item_encoding\n\ngc.collect()","e116fb79":"train_2.drop(features_to_discard,axis=1,inplace=True)\n\ntrain_2.info(null_counts=True,verbose=True)","893f81ad":"# create directory\ncreate_directory(os.path.join(DATA_FOLDER, 'training'))\n\n# export dataset\ntrain_2.to_pickle(os.path.join(DATA_FOLDER,'training\/train_2_pred.pkl'))","69aced14":"# clear memory\ndel train_2\n\ngc.collect()","ba76a451":"reset_variable_space","f3836306":"# SPLIT TRAIN-VALIDATION SET\ndef datasplit_train_val(train_df,n_months_val=1,month_id_first=0):\n    if month_id_first==0:\n        month_id_first=train_df['month_id'].min()\n        \n    month_id_split=train_df['month_id'].max()-(n_months_val-1)\n\n    # TRAINING SET\n    X_train=train_df.loc[(train_df['month_id']>=month_id_first)&(train_df['month_id']<month_id_split),:].astype(np.float32)\n    Y_train=X_train['item_quantity'].astype(np.float32)\n    X_train.drop(['month_id','item_quantity'],axis=1,inplace=True)\n\n    print(X_train.shape, Y_train.shape)\n\n    # VALIDATION SET\n    X_val=train_df.loc[train_df['month_id']>=month_id_split,:].astype(np.float32)\n    Y_val=X_val['item_quantity'].astype(np.float32)\n    X_val.drop(['month_id','item_quantity'],axis=1,inplace=True)\n\n    print(X_val.shape , Y_val.shape)\n\n    return (X_train,Y_train,X_val,Y_val)\n\n\n\n\n\n# SPLIT TRAIN-TEST SET\ndef datasplit_train_test(train_df,month_id_first=0):\n    if month_id_first==0:\n        month_id_first=train_df['month_id'].min()\n        \n    # TRAINING SET\n    X_train=train_df.loc[(train_df['month_id']>=month_id_first)&(train_df['month_id']<34),:].astype(np.float32)\n    Y_train=X_train['item_quantity'].astype(np.float32)\n    X_train.drop(['month_id','item_quantity'],axis=1,inplace=True)\n\n    print(X_train.shape, Y_train.shape)\n\n    # TEST SET\n    X_test=train_df.loc[train_df['month_id']==34,:].astype(np.float32)\n    X_test.drop(['month_id','item_quantity'],axis=1,inplace=True)\n\n    print(X_test.shape)\n\n    return (X_train,Y_train,X_test)\n\n\n\n\n\n# SPLIT TRAINING SET MONTH BY MONTH\ndef datasplit_evalset(train_df,month_id_first=0):\n    if month_id_first==0:\n        month_id_first=train_df['month_id'].min()\n        \n    eval_set=[]\n    for mid in train_df['month_id'].unique():\n        if (mid>=month_id_first)&(mid<34):\n            X_val=train_df.loc[(train_df['month_id']==mid),:].astype(np.float32)\n            Y_val=X_val['item_quantity'].astype(np.float32)\n            X_val.drop(['month_id','item_quantity'],axis=1,inplace=True)\n            eval_set.append((X_val,Y_val))\n        \n    return eval_set","906c335c":"# ANALYSIS OF CORRELATIONS OF FEATURES WITH TARGET\ndef show_feature_target_correlations(train_df,month_id_first=22,clip_value=20):\n    n_features=len(train_df.columns)-1\n    \n    train_clip=train_df.copy()\n    train_clip['item_quantity'].clip(0,clip_value,inplace=True)\n\n    # plot month by month correlations\n    fig,ax=plt.subplots(1,1,figsize=(15,np.floor(n_features\/3)))\n    for i in range(month_id_first,34):\n        tmp=train_clip.loc[train_clip['month_id']==i,:].corr()\n        tmp.drop('item_quantity',axis=0,inplace=True)\n        ax.plot(tmp['item_quantity'],tmp.index,'x')\n    ax.grid(True)\n\n    # plot global correlation over all months\n    tmp=train_clip.loc[train_clip['month_id']>=month_id_first,:].corr()\n    tmp.drop('item_quantity',axis=0,inplace=True)\n    ax.plot(tmp['item_quantity'],tmp.index,'ko',markersize=10)","4c8600e3":"# ANALYSIS OF TARGET VALUE AND NAIVE PREDICTIONS\ndef target_analysis_val(train_df,n_months_val=1,month_id_first=0):\n\n    # split dataset\n    (X_train,Y_train,X_val,Y_val) = datasplit_train_val(train_df,n_months_val,month_id_first)\n    evalset = datasplit_evalset(train_df,month_id_first)\n\n    # TRAINING SET\n    base_score=Y_train.mean()\n    \n    Y_guess_train=0*Y_train\n    rmse_guess_train=np.sqrt(mean_squared_error(Y_train, Y_guess_train))\n\n    Y_base_train=0*Y_train+base_score\n    rmse_base_train=np.sqrt(mean_squared_error(Y_train, Y_base_train))\n\n    print('#### TRAINING SET ###')\n    print('mean target value: '+str(base_score))\n    print('RMSE (guess,base): '+str(rmse_guess_train)+' , '+str(rmse_base_train))\n\n    \n    # VALIDATION SET\n    base_score_val=Y_val.mean()\n    \n    Y_guess_val=0*Y_val \n    rmse_guess_val=np.sqrt(mean_squared_error(Y_val, Y_guess_val))\n\n    Y_base_val=0*Y_val+base_score_val\n    rmse_base_val=np.sqrt(mean_squared_error(Y_val, Y_base_val))\n\n    print('#### VALIDATION SET ###')\n    print('mean target value: '+str(base_score_val))\n    print('RMSE (guess,base): '+str(rmse_guess_val)+' , '+str(rmse_base_val))\n    \n\n    # EVALUATION SET\n    base_score_evalset=[Y.mean() for (_,Y) in evalset]\n\n    rmse_guess_evalset=[]\n    rmse_base_evalset=[]\n    for (_,Y) in evalset:\n        Y_guess=0*Y\n        rmse_guess_evalset.append(np.sqrt(mean_squared_error(Y, Y_guess)))\n\n        Y_base=0*Y+Y.mean()\n        rmse_base_evalset.append(np.sqrt(mean_squared_error(Y, Y_base)))\n\n    print('#### EVAL SET ###')\n    print('mean target value: '+str([round(b*1e4)\/1e4 for b in base_score_evalset]))\n    print('RMSE (guess): '+str([round(b*1e2)\/1e2 for b in rmse_guess_evalset]))\n    print('RMSE (base): '+str([round(b*1e2)\/1e2 for b in rmse_base_evalset]))\n\n    \n    \n    # DISPLAY\n    fig,axes=plt.subplots(1,2,figsize=(15,5))\n    axes[0].plot(train_df.loc[(train_df['month_id']>=month_id_first)&(train_df['month_id']<34),'month_id'].unique(),base_score_evalset,'-o')\n    axes[0].set_xlabel('month_id')\n    axes[0].set_ylabel('mean of target value')\n    axes[0].grid(True)\n    axes[0].set_ylim(bottom=0)\n\n    axes[1].plot(train_df.loc[(train_df['month_id']>=month_id_first)&(train_df['month_id']<34),'month_id'].unique(),rmse_guess_evalset,'-o')\n    axes[1].plot(train_df.loc[(train_df['month_id']>=month_id_first)&(train_df['month_id']<34),'month_id'].unique(),rmse_base_evalset,'-o')\n    axes[1].set_xlabel('month_id')\n    axes[1].set_ylabel('rmse')\n    axes[1].legend(['guess = 0','mean value'])\n    axes[1].grid(True)\n    axes[1].set_ylim(bottom=0)\n                             \n    return rmse_guess_train, rmse_base_train, rmse_guess_val, rmse_base_val, rmse_guess_evalset, rmse_base_evalset\n\n\n\n\n\n\n\n\n# ANALYSIS OF TARGET VALUE AND NAIVE PREDICTIONS\ndef target_analysis_test(train_df,month_id_first=0):\n\n    # split dataset\n    (X_train,Y_train,_) = datasplit_train_test(train_df,month_id_first)\n    evalset = datasplit_evalset(train_df,month_id_first)\n\n    # BASE SCORE\n    base_score=Y_train.mean()\n    base_score_evalset=[Y.mean() for (_,Y) in evalset]\n\n    # BENCHMARK (guess with 0 everywhere)\n    Y_guess_train=0*Y_train\n    rmse_guess_train=np.sqrt(mean_squared_error(Y_train, Y_guess_train))\n\n    Y_base_train=0*Y_train+base_score\n    rmse_base_train=np.sqrt(mean_squared_error(Y_train, Y_base_train))\n\n    print('#### TRAINING SET ###')\n    print('mean target value: '+str(base_score))\n    print('RMSE (guess,base): '+str(rmse_guess_train)+' , '+str(rmse_base_train))\n\n    rmse_guess_evalset=[]\n    rmse_base_evalset=[]\n    for (_,Y) in evalset:\n        Y_guess=0*Y\n        rmse_guess_evalset.append(np.sqrt(mean_squared_error(Y, Y_guess)))\n\n        Y_base=0*Y+Y.mean()\n        rmse_base_evalset.append(np.sqrt(mean_squared_error(Y, Y_base)))\n\n    print('#### EVAL SET ###')\n    print('mean target value: '+str([round(b*1e4)\/1e4 for b in base_score_evalset]))\n    print('RMSE (guess): '+str([round(b*1e2)\/1e2 for b in rmse_guess_evalset]))\n    print('RMSE (base): '+str([round(b*1e2)\/1e2 for b in rmse_base_evalset]))\n\n    fig,axes=plt.subplots(1,2,figsize=(15,5))\n    axes[0].plot(train_df.loc[(train_df['month_id']>=month_id_first)&(train_df['month_id']<34),'month_id'].unique(),base_score_evalset,'-o')\n    axes[0].set_xlabel('month_id')\n    axes[0].set_ylabel('mean of target value')\n    axes[0].grid(True)\n    axes[0].set_ylim(bottom=0)\n\n    axes[1].plot(train_df.loc[(train_df['month_id']>=month_id_first)&(train_df['month_id']<34),'month_id'].unique(),rmse_guess_evalset,'-o')\n    axes[1].plot(train_df.loc[(train_df['month_id']>=month_id_first)&(train_df['month_id']<34),'month_id'].unique(),rmse_base_evalset,'-o')\n    axes[1].set_xlabel('month_id')\n    axes[1].set_ylabel('rmse')\n    axes[1].legend(['guess = 0','mean value'])\n    axes[1].grid(True)\n    axes[1].set_ylim(bottom=0)\n    \n    return rmse_guess_train, rmse_base_train, rmse_guess_evalset, rmse_base_evalset","aa6d4eb3":"# PERFORMANCES ON TRAINING SET\ndef performance_analysis_val(xgbreg,rmse_guess_train,rmse_base_train,rmse_guess_val,rmse_base_val,rmse_guess_evalset,rmse_base_evalset):\n\n    rmse_evalset_last=[]\n    rmse_evalset_min=[]\n    for evset in xgbreg.evals_result().values():\n        rmse_evalset_last.append(evset['rmse'][-1])\n        rmse_evalset_min.append(min(evset['rmse']))\n\n    # display\n    print('#### TRAINING SET ###')\n    print('RMSE (last, min, base, guess): '+str(rmse_evalset_last[0]) +' , '+str(rmse_evalset_min[0])+' , '+str(rmse_base_train)+' , '+str(rmse_guess_train))\n    print()\n    print('#### VALIDATION SET ###')\n    print('RMSE (last, min, base, guess): '+str(rmse_evalset_last[1]) +' , '+str(rmse_evalset_min[1])+' , '+str(rmse_base_val)+' , '+str(rmse_guess_val))\n    print()\n    print('#### EVAL SET ###')\n    print('RMSE (last): '+str([round(b*1e4)\/1e4 for b in rmse_evalset_last[2:]]))\n    print('RMSE (min): '+str([round(b*1e4)\/1e4 for b in rmse_evalset_min[2:]]))\n    print('RMSE (guess): '+str([round(b*1e4)\/1e4 for b in rmse_guess_evalset]))\n    print('RMSE (base): '+str([round(b*1e4)\/1e4 for b in rmse_base_evalset]))\n\n\n    fig,axes=plt.subplots(1,2,figsize=(15,5))\n    for i in range(2,len(xgbreg.evals_result())):\n        evset=xgbreg.evals_result()['validation_'+str(i)]['rmse']\n        axes[0].plot(evset,'-o')\n    axes[0].legend(range(0,len(eval_set)))\n    axes[0].plot(xgbreg.evals_result()['validation_0']['rmse'],'k',linewidth=5)\n    axes[0].plot(xgbreg.evals_result()['validation_1']['rmse'],'k--',linewidth=5)\n\n\n    axes[1].plot(rmse_evalset_last[2:],'-o')\n    axes[1].plot(0*np.array(rmse_evalset_last[2:])+rmse_evalset_last[0],'k')\n    axes[1].plot(0*np.array(rmse_evalset_last[2:])+rmse_evalset_last[1],'k--')\n    axes[1].plot(rmse_evalset_min[2:],'-o')\n    axes[1].plot(0*np.array(rmse_evalset_min[2:])+rmse_evalset_last[0],'r')\n    axes[1].plot(0*np.array(rmse_evalset_min[2:])+rmse_evalset_last[1],'r--')\n    axes[1].plot(rmse_guess_evalset,'k-s')\n    axes[1].set_xlabel('month_id')\n    axes[1].set_ylabel('rmse (last,min)')\n    axes[1].grid(True)\n    axes[1].set_ylim(bottom=0)\n    \n    \n    \n\n    \n# PERFORMANCES ON TRAINING SET\ndef performance_analysis_test(xgbreg,rmse_guess_train,rmse_base_train,rmse_guess_evalset,rmse_base_evalset):\n\n    rmse_evalset_last=[]\n    rmse_evalset_min=[]\n    for evset in xgbreg.evals_result().values():\n        rmse_evalset_last.append(evset['rmse'][-1])\n        rmse_evalset_min.append(min(evset['rmse']))\n\n    # display\n    print('#### TRAINING SET ###')\n    print('RMSE (last, min, base, guess): '+str(rmse_evalset_last[0]) +' , '+str(rmse_evalset_min[0])+' , '+str(rmse_base_train)+' , '+str(rmse_guess_train))\n    print()\n    print('#### EVAL SET ###')\n    print('RMSE (last): '+str([round(b*1e4)\/1e4 for b in rmse_evalset_last[1:]]))\n    print('RMSE (min): '+str([round(b*1e4)\/1e4 for b in rmse_evalset_min[1:]]))\n    print('RMSE (base): '+str([round(b*1e4)\/1e4 for b in rmse_base_evalset]))\n    print('RMSE (guess): '+str([round(b*1e4)\/1e4 for b in rmse_guess_evalset]))\n\n\n    fig,axes=plt.subplots(1,2,figsize=(15,5))\n    for i in range(1,len(xgbreg.evals_result())):\n        evset=xgbreg.evals_result()['validation_'+str(i)]['rmse']\n        axes[0].plot(evset,'-o')\n    axes[0].legend(range(0,len(eval_set)))\n    axes[0].plot(xgbreg.evals_result()['validation_0']['rmse'],'k',linewidth=5)\n\n\n    axes[1].plot(rmse_evalset_last[1:],'-o')\n    axes[1].plot(0*np.array(rmse_evalset_last[1:])+rmse_evalset_last[0],'k')\n    axes[1].plot(rmse_evalset_min[1:],'-o')\n    axes[1].plot(0*np.array(rmse_evalset_min[1:])+rmse_evalset_last[0],'r')\n    axes[1].plot(rmse_guess_evalset,'k-s')\n    axes[1].set_xlabel('month_id')\n    axes[1].set_ylabel('rmse (last,min)')\n    axes[1].grid(True)\n    axes[1].set_ylim(bottom=0)","cbff601d":"# PLOT LEARNING CURVES\ndef learning_curves(xgbreg,n_months_val,month_id_first):\n    n_sets=len(xgbreg.evals_result())-2\n    ntrees=xgbreg.get_num_boosting_rounds()\n    \n    fig,axes=plt.subplots(2,2,figsize=(15,10))\n    axes[0,0].plot(xgbreg.evals_result()['validation_1']['rmse'])\n    for i in range(0,n_months_val):\n        axes[0,0].plot(xgbreg.evals_result()['validation_'+str(n_sets+i)]['rmse'])\n    axes[0,0].set_xlim(0,ntrees)\n    axes[0,0].grid(True)\n    axes[0,0].set_title('validation')\n\n    axes[0,1].plot(xgbreg.evals_result()['validation_1']['rmse'],label='validation')\n    for i in range(0,n_months_val):\n        axes[0,1].plot(xgbreg.evals_result()['validation_'+str(n_sets+i)]['rmse'],label='month '+str(month_id_first+n_sets+i-2))\n    axes[0,1].set_xlim(0,ntrees)\n    axes[0,1].grid(True)\n    axes[0,1].legend()\n    axes[0,1].set_title('validation')\n\n    \n    axes[1,0].plot(xgbreg.evals_result()['validation_0']['rmse'],'k',linewidth=3)\n    for i in range(2,n_sets):\n        axes[1,0].plot(xgbreg.evals_result()['validation_'+str(i)]['rmse'])\n    axes[1,0].set_xlim(0,ntrees)\n    axes[1,0].grid(True)\n    axes[1,0].set_title('training')\n\n    \n    axes[1,1].plot(xgbreg.evals_result()['validation_0']['rmse'],'k',linewidth=3,label='training')\n    for i in range(2,n_sets):\n        axes[1,1].plot(xgbreg.evals_result()['validation_'+str(i)]['rmse'],label='month '+str(month_id_first+i-2))\n    axes[1,1].set_xlim(0,ntrees)\n    axes[1,1].grid(True)\n    axes[1,1].legend()\n    axes[1,1].set_title('training')\n    \n    return axes","463af85a":"# FEATURE IMPORTANCE\ndef plot_feature_importance(xgbreg,X_train):\n    # gain     := improvement in accuracy resulting from a split according to this feature (measured by a reduction of the optimization metric)\n    # weight   := number of times a feature is used to split the data across all trees\n    # coverage := number of observations that are classified according to this feature in the tree\n\n    n_features=len(X_train.columns)\n    \n    fi_types=[ 'total_gain' , 'gain', 'weight', 'cover' , 'total_cover']\n    xgb_fi=pd.DataFrame(index=X_train.columns)\n    for typ in fi_types:\n        xgb_fi[typ]=pd.Series(data=xgbreg.get_booster().get_score(importance_type=typ),index=X_train.columns)\n\n    xgb_fi.fillna(0,inplace=True)\n    xgb_fi=xgb_fi.sort_values(by='gain',ascending=True)\n\n    fig,ax=plt.subplots(1,5,figsize=(18,np.floor(n_features\/2.5)))\n    for i,typ in enumerate(fi_types[0:5]):\n        if i==0:\n            ax[i].barh(y=xgb_fi.index,width=xgb_fi[typ].values)\n        else:\n            ax[i].barh(y=xgb_fi.index,width=xgb_fi[typ].values,tick_label=[None])\n        ax[i].grid(True)\n        ax[i].set_title(typ)","74d29b38":"loaded=%who_ls\nloaded.append('loaded')","586a1db4":"dataset_name='train_0_pred'\nn_months_val=2\nmonth_id_first=18\n\ntrain_X=pd.read_pickle(os.path.join(DATA_FOLDER,'training\/'+dataset_name+'.pkl'))\n\n# drop test set for validation purposes\ntrain_X.drop(train_X.loc[train_X['month_id']==34].index,axis=0,inplace=True)\n\nprint(train_X.info(null_counts=True,verbose=True))","3c6b322f":"# analyse correlation of features with target variable\nshow_feature_target_correlations(train_X,month_id_first=22)","f2bb8cb5":"# analyse target value\nrmse_guess_train, rmse_base_train, rmse_guess_val, rmse_base_val, rmse_guess_evalset, rmse_base_evalset = target_analysis_val(train_X,n_months_val=2,month_id_first=18)","d874ce0e":"features_to_keep_0=[]\n\n#--------------\n# CATEGORY\nfeatures_to_keep_0+=['item_category_id']\n\nfeatures_to_keep_0+=['item_category_freq']\nfeatures_to_keep_0+=['item_supercategory_freq']\nfeatures_to_keep_0+=['item_category_console_freq']\n#features_to_keep_0+=['item_category_digital_freq']\n\nfeatures_to_keep_0+=['item_category_freq_in_seniority']\nfeatures_to_keep_0+=['item_supercategory_freq_in_seniority']\nfeatures_to_keep_0+=['item_category_console_freq_in_seniority']\n#features_to_keep_0+=['item_category_digital_freq_in_seniority']\n\n#features_to_keep_0+=['category_semiannual_avg_recent_rsd']\n#features_to_keep_0+=['category_semiannual_avg_annual_rsd']\n\n\n#--------------\n# SHOP\n#features_to_keep_0+=['shop_months_since_opening']\n#features_to_keep_0+=['shop_opening']\n\n\n\n\n\n\n\n\n\n\n#--------------\n# PRICES\n\n# typical prices\n\nfeatures_to_keep_0+=['shop_category_price_median_absolute_mean']         \n#features_to_keep_0+=['shop_category_price_median_recent_mean']           \nfeatures_to_keep_0+=['shop_category_price_median_lag_12']                \n\nfeatures_to_keep_0+=['category_price_median_recent_mean']                \nfeatures_to_keep_0+=['category_price_median_lag_12']                     \n\n#features_to_keep_0+=['supercategory_price_median_recent_mean']       \nfeatures_to_keep_0+=['supercategory_price_median_lag_12']                \n\nfeatures_to_keep_0+=['category_price_min_absolute_min']                  \nfeatures_to_keep_0+=['category_price_max_absolute_max']                  \n\nfeatures_to_keep_0+=['category_price_min_annual_min']                    \nfeatures_to_keep_0+=['category_price_max_annual_max']                    \n\nfeatures_to_keep_0+=['category_price_median_compared_to_supercategory_price_median_absolute_mean']\n\n\n\n\n\n\n\n\n\n\n#--------------\n# SHOP\n#features_to_keep_0+=['shop_avg_sales_seniority_0_absolute_mean']\n#features_to_keep_0+=['shop_avg_sales_seniority_0_annual_mean']           \nfeatures_to_keep_0+=['shop_avg_sales_seniority_0_semiannual_mean']       \n#features_to_keep_0+=['shop_avg_sales_seniority_0_recent_mean']           \n#features_to_keep_0+=['shop_avg_sales_seniority_0_lag_1']                 \n#features_to_keep_0+=['shop_avg_sales_seniority_0_lag_12']                \n\n\n#--------------\n# SUPERCATEGORY\n#features_to_keep_0+=['supercategory_avg_sales_seniority_0_absolute_mean']\n#features_to_keep_0+=['supercategory_avg_sales_seniority_0_annual_mean']          \nfeatures_to_keep_0+=['supercategory_avg_sales_seniority_0_semiannual_mean']   \n#features_to_keep_0+=['supercategory_avg_sales_seniority_0_recent_mean']          \n#features_to_keep_0+=['supercategory_avg_sales_seniority_0_lag_1']                \nfeatures_to_keep_0+=['supercategory_avg_sales_seniority_0_lag_12']            \n\n\n#--------------\n# SHOP-SUPERCATEGORY\n#features_to_keep_0+=['shop_supercategory_avg_sales_seniority_0_absolute_mean']\nfeatures_to_keep_0+=['shop_supercategory_avg_sales_seniority_0_annual_mean']      \n#features_to_keep_0+=['shop_supercategory_avg_sales_seniority_0_semiannual_mean']     \n#features_to_keep_0+=['shop_supercategory_avg_sales_seniority_0_recent_mean']         \n#features_to_keep_0+=['shop_supercategory_avg_sales_seniority_0_lag_1']               \nfeatures_to_keep_0+=['shop_supercategory_avg_sales_seniority_0_lag_12']              \n\n\n\n\n#--------------\n# CATEGORY\n#features_to_keep_0+=['category_avg_sales_seniority_0_absolute_mean']\nfeatures_to_keep_0+=['category_avg_sales_seniority_0_annual_mean']        \nfeatures_to_keep_0+=['category_avg_sales_seniority_0_semiannual_mean']    \n#features_to_keep_0+=['category_avg_sales_seniority_0_recent_mean']        \n#features_to_keep_0+=['category_avg_sales_seniority_0_lag_1']              \n#features_to_keep_0+=['category_avg_sales_seniority_0_lag_12']             \n\n\n\n#--------------\n# SHOP-CATEGORY\n#features_to_keep_0+=['shop_category_avg_sales_seniority_0_absolute_mean']\nfeatures_to_keep_0+=['shop_category_avg_sales_seniority_0_annual_mean']        \nfeatures_to_keep_0+=['shop_category_avg_sales_seniority_0_semiannual_mean']    \nfeatures_to_keep_0+=['shop_category_avg_sales_seniority_0_recent_mean']        \nfeatures_to_keep_0+=['shop_category_avg_sales_seniority_0_lag_1']              \nfeatures_to_keep_0+=['shop_category_avg_sales_seniority_0_lag_12']             \n\n\n\n\n\n#--------------\n# SPATIAL TRENDS\n#features_to_keep_0+=['shop_category_avg_sales_compared_to_category_absolute_mean']\nfeatures_to_keep_0+=['shop_category_avg_sales_compared_to_category_annual_mean']                  \n#features_to_keep_0+=['shop_category_avg_sales_compared_to_category_semiannual_mean']\nfeatures_to_keep_0+=['shop_category_avg_sales_compared_to_category_recent_mean']                  \n#features_to_keep_0+=['shop_category_avg_sales_compared_to_category_lag_1']\nfeatures_to_keep_0+=['shop_category_avg_sales_compared_to_category_lag_12']                       \n\n#features_to_keep_0+=['shop_supercategory_avg_sales_compared_to_supercategory_absolute_mean']\n#features_to_keep_0+=['shop_supercategory_avg_sales_compared_to_supercategory_annual_mean']       \n#features_to_keep_0+=['shop_supercategory_avg_sales_compared_to_supercategory_semiannual_mean']\nfeatures_to_keep_0+=['shop_supercategory_avg_sales_compared_to_supercategory_recent_mean']        \n#features_to_keep_0+=['shop_supercategory_avg_sales_compared_to_supercategory_lag_1']\n#features_to_keep_0+=['shop_supercategory_avg_sales_compared_to_supercategory_lag_12']            \n\n\n# TEMPORAL TRENDS\n#features_to_keep_0+=['shop_avg_sales_compare_recent_mean_to_annual_mean']          \n#features_to_keep_0+=['shop_avg_sales_compare_lag1_to_annual_mean']\n#features_to_keep_0+=['shop_avg_sales_compare_lag12_to_annual_mean']                \n\nfeatures_to_keep_0+=['supercategory_avg_sales_compare_recent_mean_to_annual_mean'] \n#features_to_keep_0+=['supercategory_avg_sales_compare_lag1_to_annual_mean']\nfeatures_to_keep_0+=['supercategory_avg_sales_compare_lag12_to_annual_mean']       \n\n#features_to_keep_0+=['category_avg_sales_compare_recent_mean_to_annual_mean']    \n#features_to_keep_0+=['category_avg_sales_compare_lag1_to_annual_mean']\nfeatures_to_keep_0+=['category_avg_sales_compare_lag12_to_annual_mean']           \n\nfeatures_to_keep_0+=['category_avg_sales_compared_to_supercategory_recent_mean']  \n#features_to_keep_0+=['category_avg_sales_compared_to_supercategory_lag_1']\n#features_to_keep_0+=['category_avg_sales_compared_to_supercategory_lag_12']        \n\n\n# TEMPORAL TRENDS FOR SENIORITY 0 (months of the year where seniority 0 is boosted)\n#features_to_keep_0+=['shop_avg_sales_seniority_0_compare_recent_mean_to_annual_mean']\n#features_to_keep_0+=['shop_avg_sales_seniority_0_compare_lag1_to_annual_mean']\n#features_to_keep_0+=['shop_avg_sales_seniority_0_compare_lag12_to_annual_mean']    \n\n#features_to_keep_0+=['supercategory_avg_sales_seniority_0_compare_recent_mean_to_annual_mean']\n#features_to_keep_0+=['supercategory_avg_sales_seniority_0_compare_lag1_to_annual_mean']\nfeatures_to_keep_0+=['supercategory_avg_sales_seniority_0_compare_lag12_to_annual_mean']  \n\n#features_to_keep_0+=['category_avg_sales_seniority_0_compare_recent_mean_to_annual_mean']\n#features_to_keep_0+=['category_avg_sales_seniority_0_compare_lag1_to_annual_mean']\n#features_to_keep_0+=['category_avg_sales_seniority_0_compare_lag12_to_annual_mean'] ","c9b84850":"# drop features\nfeatures_to_discard_0=list( set(list(train_X.columns)) - set(['month_id','item_quantity']+features_to_keep_0) )\ntrain_X.drop(features_to_discard_0,axis=1,inplace=True)\n\n# fill missing values\ntrain_X.fillna(-1,inplace=True)\n\n# split dataset\n(X_train,Y_train,X_val,Y_val) = datasplit_train_val(train_X,n_months_val,month_id_first)\neval_set = [(X_train,Y_train),(X_val,Y_val)]+datasplit_evalset(train_X,month_id_first)\n\n# show dataset info\nprint('number of features to keep : '+str(len(features_to_keep_0)))\nprint('number of features kept    : '+str(len(X_train.columns)))\n\nprint(X_train.info(null_counts=True,verbose=True))\n\n\n# clear memory space\ndel train_X\ngc.collect()","fdddb2f6":"# SET XGBOOST PARAMETERS\n\nxgb_params_0={'objective':        'reg:squarederror',\n# TREE SPECIFIC PARAMETERS\n              'max_depth':        4,\n              'min_child_weight': 1e3,\n              'subsample':        0.8,\n              'colsample_bytree': 0.8,\n# PARAMETERS RELATED TO THE LEARNING\/BOOSTING PROCESS\n              'learning_rate':    0.01,\n              'n_estimators':     300,  \n# MISCELLANEOUS PARAMETERS\n              'base_score':       0.6,\n              'n_jobs':           4,\n              'random_state':     2\n             }\n\n\n\n\n# FIT MODEL\nts=time.time()\n\nxgbreg=XGBRegressor(**xgb_params_0)\nxgbreg.fit(X_train,Y_train,eval_set=eval_set,eval_metric='rmse',verbose=False)\n\nprint(time.time()-ts)","49a0b533":"# PERFORMANCE ANALYSIS\nperformance_analysis_val(xgbreg,rmse_guess_train,rmse_base_train,rmse_guess_val,rmse_base_val,rmse_guess_evalset,rmse_base_evalset)\n\n# learning curves\naxes=learning_curves(xgbreg,n_months_val,month_id_first)\n\naxes[0,0].set_ylim(1.7,2.7)\naxes[0,1].set_ylim(1.7,2.3)\naxes[1,0].set_ylim(0.9,3)\naxes[1,1].set_ylim(0.9,3)\n\ndel axes","f92458d4":"# FEATURE IMPORTANCE\nplot_feature_importance(xgbreg,X_train)","f82aa1e7":"# clear memory\ndel X_train, Y_train, X_val, Y_val\ndel eval_set\ndel rmse_guess_train, rmse_base_train, rmse_guess_val, rmse_base_val, rmse_guess_evalset, rmse_base_evalset\ndel xgbreg\n\ngc.collect()","29fb0f2c":"# append variables to list of variables to keep\nloaded.extend(['features_to_keep_0','features_to_discard_0','xgb_params_0'])","15599b10":"reset_variable_space","da30a63b":"dataset_name='train_1_pred'\nn_months_val=2\nmonth_id_first=18\n\ntrain_X=pd.read_pickle(os.path.join(DATA_FOLDER,'training\/'+dataset_name+'.pkl'))\n\n# drop test set for validation purposes\ntrain_X.drop(train_X.loc[train_X['month_id']==34].index,axis=0,inplace=True)\n\nprint(train_X.info(null_counts=True,verbose=True))","62c70774":"# analyse correlation of features with target variable\nshow_feature_target_correlations(train_X,month_id_first=22,clip_value=6)","975b35a8":"# analyse target value\nrmse_guess_train, rmse_base_train, rmse_guess_val, rmse_base_val, rmse_guess_evalset, rmse_base_evalset = target_analysis_val(train_X,n_months_val=2,month_id_first=18)","258e6af4":"features_to_keep_1=[]\n\n#--------------\n# CATEGORY\nfeatures_to_keep_1+=['item_category_id']                                     \n\nfeatures_to_keep_1+=['item_category_freq']                                   \nfeatures_to_keep_1+=['item_supercategory_freq']                              \nfeatures_to_keep_1+=['item_category_console_freq']                            \nfeatures_to_keep_1+=['item_category_digital_freq']\n\n#features_to_keep_1+=['item_category_freq_in_seniority']\n#features_to_keep_1+=['item_supercategory_freq_in_seniority']\n#features_to_keep_1+=['item_category_console_freq_in_seniority']\n#features_to_keep_1+=['item_category_digital_freq_in_seniority']\n\n#features_to_keep_1+=['category_semiannual_avg_recent_rsd']\n#features_to_keep_1+=['category_semiannual_avg_annual_rsd']\n\n\n#--------------\n# SHOP\nfeatures_to_keep_1+=['shop_months_since_opening']\nfeatures_to_keep_1+=['shop_opening']\n\n#--------------\n# ITEM\nfeatures_to_keep_1+=['item_freq_in_seniority']\n\n#--------------\n# RELATIVE TIME FEATURES\nfeatures_to_keep_1+=['item_months_since_release']\n\n\n\n\n\n\n\n\n\n\n#--------------\n# PRICES\n\nfeatures_to_keep_1+=['item_price_median_absolute_mean']                \nfeatures_to_keep_1+=['item_price_median_last_sale']                    \nfeatures_to_keep_1+=['item_price_min_absolute_min']                    \nfeatures_to_keep_1+=['item_price_max_absolute_max']                    \n\n#features_to_keep_1+=['item_price_median_compared_to_category_price_median_absolute_mean']                \n#features_to_keep_1+=['item_price_median_compared_to_category_price_median_last_sale']                    \n\nfeatures_to_keep_1+=['category_price_median_recent_mean']                    \n\n\n\n\n\n\n\n\n\n\n#--------------\n# ITEM\nfeatures_to_keep_1+=['item_max_quantity_absolute_max'] \n\n#features_to_keep_1+=['item_avg_sales_absolute_mean']\n#features_to_keep_1+=['item_avg_sales_annual_mean']\n#features_to_keep_1+=['item_avg_sales_semiannual_mean']\nfeatures_to_keep_1+=['item_avg_sales_recent_mean']                \nfeatures_to_keep_1+=['item_avg_sales_lag_1']                        \n#features_to_keep_1+=['item_avg_sales_lag_12']\nfeatures_to_keep_1+=['item_avg_sales_last_sale']                   \n\n\n#features_to_keep_1+=['item_avg_sales_over_sold_absolute_mean']\n#features_to_keep_1+=['item_avg_sales_over_sold_annual_mean']\n#features_to_keep_1+=['item_avg_sales_over_sold_semiannual_mean']\nfeatures_to_keep_1+=['item_avg_sales_over_sold_recent_mean']                  \nfeatures_to_keep_1+=['item_avg_sales_over_sold_lag_1']                        \n#features_to_keep_1+=['item_avg_sales_over_sold_lag_12']                         \nfeatures_to_keep_1+=['item_avg_sales_over_sold_last_sale']                    \n\n\n#features_to_keep_1+=['item_avg_sales_seniority_1_absolute_mean']\n#features_to_keep_1+=['item_avg_sales_seniority_1_annual_mean']\n#features_to_keep_1+=['item_avg_sales_seniority_1_semiannual_mean']\nfeatures_to_keep_1+=['item_avg_sales_seniority_1_recent_mean']\nfeatures_to_keep_1+=['item_avg_sales_seniority_1_lag_1']\n#features_to_keep_1+=['item_avg_sales_seniority_1_lag_12']\nfeatures_to_keep_1+=['item_avg_sales_seniority_1_last_sale']\n\n\n\n#--------------\n# SHOP\n#features_to_keep_1+=['shop_avg_sales_seniority_1_absolute_mean']\n#features_to_keep_1+=['shop_avg_sales_seniority_1_annual_mean']\n#features_to_keep_1+=['shop_avg_sales_seniority_1_semiannual_mean']\n#features_to_keep_1+=['shop_avg_sales_seniority_1_recent_mean']\n#features_to_keep_1+=['shop_avg_sales_seniority_1_lag_1']\nfeatures_to_keep_1+=['shop_avg_sales_seniority_1_lag_12']                                                     \n\n#features_to_keep_1+=['shop_avg_sales_absolute_mean']\n#features_to_keep_1+=['shop_avg_sales_annual_mean']\n#features_to_keep_1+=['shop_avg_sales_semiannual_mean']\n#features_to_keep_1+=['shop_avg_sales_recent_mean']\n#features_to_keep_1+=['shop_avg_sales_lag_1']                                                                          \nfeatures_to_keep_1+=['shop_avg_sales_lag_12']                                                                         \n\n\n\n#--------------\n# MONTHS SINCE RELEASE\nfeatures_to_keep_1+=['category_months_since_release_avg_sales_seniority_1_absolute_mean']                       \nfeatures_to_keep_1+=['category_months_since_release_avg_sales_seniority_1_annual_mean']\n#features_to_keep_1+=['category_months_since_release_avg_sales_seniority_1_semiannual_mean']\nfeatures_to_keep_1+=['category_months_since_release_avg_sales_seniority_1_recent_mean']               \n#features_to_keep_1+=['category_months_since_release_avg_sales_seniority_1_lag_1']\nfeatures_to_keep_1+=['category_months_since_release_avg_sales_seniority_1_lag_12']                   \n\nfeatures_to_keep_1+=['shop_category_months_since_release_avg_sales_seniority_1_absolute_mean']                        \nfeatures_to_keep_1+=['shop_category_months_since_release_avg_sales_seniority_1_annual_mean'] \n#features_to_keep_1+=['shop_category_months_since_release_avg_sales_seniority_1_semiannual_mean']\nfeatures_to_keep_1+=['shop_category_months_since_release_avg_sales_seniority_1_recent_mean']                         \n#features_to_keep_1+=['shop_category_months_since_release_avg_sales_seniority_1_lag_1']               \nfeatures_to_keep_1+=['shop_category_months_since_release_avg_sales_seniority_1_lag_12']                                 \n\nfeatures_to_keep_1+=['supercategory_months_since_release_avg_sales_seniority_1_absolute_mean']\nfeatures_to_keep_1+=['supercategory_months_since_release_avg_sales_seniority_1_annual_mean']\n#features_to_keep_1+=['supercategory_months_since_release_avg_sales_seniority_1_semiannual_mean']\nfeatures_to_keep_1+=['supercategory_months_since_release_avg_sales_seniority_1_recent_mean']\n#features_to_keep_1+=['supercategory_months_since_release_avg_sales_seniority_1_lag_1']\nfeatures_to_keep_1+=['supercategory_months_since_release_avg_sales_seniority_1_lag_12']                                \n\nfeatures_to_keep_1+=['shop_supercategory_months_since_release_avg_sales_seniority_1_absolute_mean']\nfeatures_to_keep_1+=['shop_supercategory_months_since_release_avg_sales_seniority_1_annual_mean']\n#features_to_keep_1+=['shop_supercategory_months_since_release_avg_sales_seniority_1_semiannual_mean']\nfeatures_to_keep_1+=['shop_supercategory_months_since_release_avg_sales_seniority_1_recent_mean']\n#features_to_keep_1+=['shop_supercategory_months_since_release_avg_sales_seniority_1_lag_1']\nfeatures_to_keep_1+=['shop_supercategory_months_since_release_avg_sales_seniority_1_lag_12']                         \n\n\n\n#--------------\n# SPATIAL TRENDS\n#features_to_keep_1+=['shop_category_avg_sales_compared_to_category_absolute_mean']\nfeatures_to_keep_1+=['shop_category_avg_sales_compared_to_category_annual_mean']               \n#features_to_keep_1+=['shop_category_avg_sales_compared_to_category_semiannual_mean']\nfeatures_to_keep_1+=['shop_category_avg_sales_compared_to_category_recent_mean']              \nfeatures_to_keep_1+=['shop_category_avg_sales_compared_to_category_lag_1']                    \nfeatures_to_keep_1+=['shop_category_avg_sales_compared_to_category_lag_12']                    \n\n\n# TEMPORAL TRENDS\n#features_to_keep_1+=['shop_avg_sales_compare_recent_mean_to_annual_mean']\n#features_to_keep_1+=['shop_avg_sales_compare_lag1_to_annual_mean']                              \nfeatures_to_keep_1+=['shop_avg_sales_compare_lag12_to_annual_mean']                               \n\nfeatures_to_keep_1+=['supercategory_avg_sales_compare_recent_mean_to_annual_mean']        \n#features_to_keep_1+=['supercategory_avg_sales_compare_lag1_to_annual_mean']\nfeatures_to_keep_1+=['supercategory_avg_sales_compare_lag12_to_annual_mean']              \n\nfeatures_to_keep_1+=['category_avg_sales_compare_recent_mean_to_annual_mean']               \n#features_to_keep_1+=['category_avg_sales_compare_lag1_to_annual_mean']\nfeatures_to_keep_1+=['category_avg_sales_compare_lag12_to_annual_mean']                     \n\nfeatures_to_keep_1+=['category_avg_sales_compared_to_supercategory_recent_mean']          \n#features_to_keep_1+=['category_avg_sales_compared_to_supercategory_lag_1']                \nfeatures_to_keep_1+=['category_avg_sales_compared_to_supercategory_lag_12']                    \n\n\n# MONTH OF RELEASE\nfeatures_to_keep_1+=['supercategory_month_of_release_avg_sales_compared_to_supercategory_absolute_mean']         \nfeatures_to_keep_1+=['category_month_of_release_avg_sales_compared_to_category_absolute_mean']                  ","0556a347":"# drop features\nfeatures_to_discard_1=list( set(list(train_X.columns)) - set(['month_id','item_quantity']+features_to_keep_1) )\ntrain_X.drop(features_to_discard_1,axis=1,inplace=True)\n\n# fill missing values\ntrain_X.fillna(-1,inplace=True)\n\n# split dataset\n(X_train,Y_train,X_val,Y_val) = datasplit_train_val(train_X,n_months_val,month_id_first)\neval_set = [(X_train,Y_train),(X_val,Y_val)]+datasplit_evalset(train_X,month_id_first)\n\n# show dataset info\nprint('number of features to keep : '+str(len(features_to_keep_1)))\nprint('number of features kept    : '+str(len(X_train.columns)))\n\nprint(X_train.info(null_counts=True,verbose=True))\n\n\n# clear memory space\ndel train_X\ngc.collect()","c74426fd":"# SET XGBOOST PARAMETERS\n\nxgb_params_1={'objective':        'reg:squarederror',\n# TREE SPECIFIC PARAMETERS\n              'max_depth':        5,\n              'min_child_weight': 1e3,\n              'subsample':        0.8,\n              'colsample_bytree': 0.8,\n# PARAMETERS RELATED TO THE LEARNING\/BOOSTING PROCESS\n              'learning_rate':    0.05,\n              'n_estimators':     500,  \n# MISCELLANEOUS PARAMETERS\n              'base_score':       0.05,\n              'n_jobs':           4,\n              'random_state':     2\n             }\n\n\n# restrict clipping range for training data\n# for seniority 1, we observed that target values larger than 8 are so rare that they may be considered noise\n# clipping target values to 6 acts as a filter that improves the overall prediction on that class of samples\nY_train.clip(0,6,inplace=True)\n\n\n\n# FIT MODEL\nts=time.time()\n\nxgbreg=XGBRegressor(**xgb_params_1)\nxgbreg.fit(X_train,Y_train,eval_set=eval_set,eval_metric='rmse',verbose=False)\n\nprint(time.time()-ts)","621721a3":"# PERFORMANCE ANALYSIS\nperformance_analysis_val(xgbreg,rmse_guess_train,rmse_base_train,rmse_guess_val,rmse_base_val,rmse_guess_evalset,rmse_base_evalset)\n\n# learning curves\naxes=learning_curves(xgbreg,n_months_val,month_id_first)\n\naxes[0,0].set_ylim(0.25,0.35)\naxes[0,1].set_ylim(0.28,0.31)\naxes[1,0].set_ylim(0.2,0.6)\naxes[1,1].set_ylim(0.2,0.6)\n\ndel axes","3a0526ac":"# FEATURE IMPORTANCE\nplot_feature_importance(xgbreg,X_train)","22366374":"# clear memory\ndel X_train, Y_train, X_val, Y_val\ndel eval_set\ndel rmse_guess_train, rmse_base_train, rmse_guess_val, rmse_base_val, rmse_guess_evalset, rmse_base_evalset\ndel xgbreg\n\ngc.collect()","73b473f3":"# append variables to list of variables to keep\nloaded.extend(['features_to_keep_1','features_to_discard_1','xgb_params_1'])","bb355cb7":"reset_variable_space","d9e0e889":"dataset_name='train_2_pred'\nn_months_val=2\nmonth_id_first=18\n\ntrain_X=pd.read_pickle(os.path.join(DATA_FOLDER,'training\/'+dataset_name+'.pkl'))\n\n# drop test set for validation purposes\ntrain_X.drop(train_X.loc[train_X['month_id']==34].index,axis=0,inplace=True)\n\nprint(train_X.info(null_counts=True,verbose=True))","19a21c76":"# analyse correlation of features with target variable\nshow_feature_target_correlations(train_X,month_id_first=22)","0f139884":"# analyse target value\nrmse_guess_train, rmse_base_train, rmse_guess_val, rmse_base_val, rmse_guess_evalset, rmse_base_evalset = target_analysis_val(train_X,n_months_val=2,month_id_first=18)","6ee580d3":"features_to_keep_2=[]\n\n#--------------\n# CATEGORY\nfeatures_to_keep_2+=['item_category_id']                                      \n\nfeatures_to_keep_2+=['item_category_freq']                                    \nfeatures_to_keep_2+=['item_supercategory_freq']                               \nfeatures_to_keep_2+=['item_category_console_freq']                            \n#features_to_keep_2+=['item_category_digital_freq']\n\n#features_to_keep_2+=['item_category_freq_in_seniority']\n#features_to_keep_2+=['item_supercategory_freq_in_seniority']\n#features_to_keep_2+=['item_category_console_freq_in_seniority']\n#features_to_keep_2+=['item_category_digital_freq_in_seniority']\n\nfeatures_to_keep_2+=['category_semiannual_avg_recent_rsd']\nfeatures_to_keep_2+=['category_semiannual_avg_annual_rsd']\n\n\n#--------------\n# SHOP\nfeatures_to_keep_2+=['shop_months_since_opening']\n\n#--------------\n# ITEM\n#features_to_keep_2+=['item_freq_in_seniority']\n\n#--------------\n# RELATIVE TIME FEATURES\nfeatures_to_keep_2+=['item_months_since_release']\nfeatures_to_keep_2+=['item_months_since_first_sale_in_shop']\nfeatures_to_keep_2+=['item_months_since_last_sale_in_shop']\n\n\n\n\n\n\n\n\n\n\n#--------------\n# PRICES\n\nfeatures_to_keep_2+=['item_price_median_absolute_mean']                \nfeatures_to_keep_2+=['item_price_median_last_sale']                    \nfeatures_to_keep_2+=['item_price_min_absolute_min']                    \nfeatures_to_keep_2+=['item_price_max_absolute_max']                    \n\nfeatures_to_keep_2+=['item_price_median_compared_to_category_price_median_absolute_mean']                \nfeatures_to_keep_2+=['item_price_median_compared_to_category_price_median_last_sale']                    \n\nfeatures_to_keep_2+=['category_price_median_recent_mean']                    \n\n\n\n\n\n\n\n\n\n\n#--------------\n# SHOP-ITEM\n#features_to_keep_2+=['item_quantity_absolute_mean']\nfeatures_to_keep_2+=['item_quantity_annual_mean']                    \nfeatures_to_keep_2+=['item_quantity_semiannual_mean']                \nfeatures_to_keep_2+=['item_quantity_recent_mean']                    \nfeatures_to_keep_2+=['item_quantity_lag_1']                          \nfeatures_to_keep_2+=['item_quantity_lag_12']                         \n\nfeatures_to_keep_2+=['item_quantity_extralin2']                       \nfeatures_to_keep_2+=['item_quantity_extralin3']    \n\nfeatures_to_keep_2+=['item_quantity_absolute_min']                    \nfeatures_to_keep_2+=['item_quantity_absolute_max']                    \n\n#--------------\n# ITEM\n#features_to_keep_2+=['item_avg_sales_absolute_mean']\n#features_to_keep_2+=['item_avg_sales_annual_mean']\n#features_to_keep_2+=['item_avg_sales_semiannual_mean']\nfeatures_to_keep_2+=['item_avg_sales_recent_mean']                  \nfeatures_to_keep_2+=['item_avg_sales_lag_1']                        \n#features_to_keep_2+=['item_avg_sales_lag_12']\nfeatures_to_keep_2+=['item_avg_sales_last_sale']                    \n\n\n#features_to_keep_2+=['item_avg_sales_over_sold_absolute_mean']\n#features_to_keep_2+=['item_avg_sales_over_sold_annual_mean']\n#features_to_keep_2+=['item_avg_sales_over_sold_semiannual_mean']\nfeatures_to_keep_2+=['item_avg_sales_over_sold_recent_mean']                    \nfeatures_to_keep_2+=['item_avg_sales_over_sold_lag_1']                          \nfeatures_to_keep_2+=['item_avg_sales_over_sold_lag_12']                         \nfeatures_to_keep_2+=['item_avg_sales_over_sold_last_sale']                      \n\n\n#features_to_keep_2+=['item_avg_sales_seniority_2_absolute_mean']\n#features_to_keep_2+=['item_avg_sales_seniority_2_annual_mean']\n#features_to_keep_2+=['item_avg_sales_seniority_2_semiannual_mean']\n#features_to_keep_2+=['item_avg_sales_seniority_2_recent_mean']\n#features_to_keep_2+=['item_avg_sales_seniority_2_lag_1']\n#features_to_keep_2+=['item_avg_sales_seniority_2_lag_12']\n#features_to_keep_2+=['item_avg_sales_seniority_2_last_sale']\n\n\n\n#--------------\n# SHOP\n#features_to_keep_2+=['shop_avg_sales_seniority_2_absolute_mean']\n#features_to_keep_2+=['shop_avg_sales_seniority_2_annual_mean']\n#features_to_keep_2+=['shop_avg_sales_seniority_2_semiannual_mean']\n#features_to_keep_2+=['shop_avg_sales_seniority_2_recent_mean']\n#features_to_keep_2+=['shop_avg_sales_seniority_2_lag_1']\nfeatures_to_keep_2+=['shop_avg_sales_seniority_2_lag_12']                                             \n                                                                    \n#--------------\n# SUPERCATEGORY\n#features_to_keep_2+=['supercategory_avg_sales_seniority_2_absolute_mean']\n#features_to_keep_2+=['supercategory_avg_sales_seniority_2_annual_mean']                \n#features_to_keep_2+=['supercategory_avg_sales_seniority_2_semiannual_mean']\nfeatures_to_keep_2+=['supercategory_avg_sales_seniority_2_recent_mean']               \nfeatures_to_keep_2+=['supercategory_avg_sales_seniority_2_lag_1']                     \nfeatures_to_keep_2+=['supercategory_avg_sales_seniority_2_lag_12']                       \n\n#--------------\n# SHOP-SUPERCATEGORY\n#features_to_keep_2+=['shop_supercategory_avg_sales_seniority_2_absolute_mean']\n#features_to_keep_2+=['shop_supercategory_avg_sales_seniority_2_annual_mean']                            \n#features_to_keep_2+=['shop_supercategory_avg_sales_seniority_2_semiannual_mean']\n#features_to_keep_2+=['shop_supercategory_avg_sales_seniority_2_recent_mean']                              \n#features_to_keep_2+=['shop_supercategory_avg_sales_seniority_2_lag_1']                                    \nfeatures_to_keep_2+=['shop_supercategory_avg_sales_seniority_2_lag_12']                                   \n\n#--------------\n# CATEGORY\n#features_to_keep_2+=['category_avg_sales_seniority_2_absolute_mean']\nfeatures_to_keep_2+=['category_avg_sales_seniority_2_annual_mean']             \nfeatures_to_keep_2+=['category_avg_sales_seniority_2_semiannual_mean']             \nfeatures_to_keep_2+=['category_avg_sales_seniority_2_recent_mean']                   \nfeatures_to_keep_2+=['category_avg_sales_seniority_2_lag_1']                    \n#features_to_keep_2+=['category_avg_sales_seniority_2_lag_12']                      \n\n#--------------\n# SHOP-CATEGORY\n#features_to_keep_2+=['shop_category_avg_sales_seniority_2_absolute_mean']\nfeatures_to_keep_2+=['shop_category_avg_sales_seniority_2_annual_mean']                                              \nfeatures_to_keep_2+=['shop_category_avg_sales_seniority_2_semiannual_mean']                                            \nfeatures_to_keep_2+=['shop_category_avg_sales_seniority_2_recent_mean']                                                \nfeatures_to_keep_2+=['shop_category_avg_sales_seniority_2_lag_1']                                                      \nfeatures_to_keep_2+=['shop_category_avg_sales_seniority_2_lag_12']                                                     \n                  \n\n\n    \n    \n\n#--------------\n# MONTHS SINCE RELEASE\n#features_to_keep_2+=['category_months_since_release_avg_sales_seniority_2_absolute_mean']         \n#features_to_keep_2+=['category_months_since_release_avg_sales_seniority_2_annual_mean']\n#features_to_keep_2+=['category_months_since_release_avg_sales_seniority_2_semiannual_mean']\nfeatures_to_keep_2+=['category_months_since_release_avg_sales_seniority_2_recent_mean']              \n#features_to_keep_2+=['category_months_since_release_avg_sales_seniority_2_lag_1']\n#features_to_keep_2+=['category_months_since_release_avg_sales_seniority_2_lag_12']                  \n\nfeatures_to_keep_2+=['shop_category_months_since_release_avg_sales_seniority_2_absolute_mean']                       \n#features_to_keep_2+=['shop_category_months_since_release_avg_sales_seniority_2_annual_mean'] \n#features_to_keep_2+=['shop_category_months_since_release_avg_sales_seniority_2_semiannual_mean']\n#features_to_keep_2+=['shop_category_months_since_release_avg_sales_seniority_2_recent_mean']                         \n#features_to_keep_2+=['shop_category_months_since_release_avg_sales_seniority_2_lag_1']               \nfeatures_to_keep_2+=['shop_category_months_since_release_avg_sales_seniority_2_lag_12']                               \n\n\n#features_to_keep_2+=['supercategory_months_since_release_avg_sales_seniority_2_absolute_mean']\n#features_to_keep_2+=['supercategory_months_since_release_avg_sales_seniority_2_annual_mean']\n#features_to_keep_2+=['supercategory_months_since_release_avg_sales_seniority_2_semiannual_mean']\n#features_to_keep_2+=['supercategory_months_since_release_avg_sales_seniority_2_recent_mean']\n#features_to_keep_2+=['supercategory_months_since_release_avg_sales_seniority_2_lag_1']\nfeatures_to_keep_2+=['supercategory_months_since_release_avg_sales_seniority_2_lag_12']                             \n\n\n#--------------\n# MONTHS SINCE FIRST SALE IN SHOP\n#features_to_keep_2+=['category_months_since_first_sale_in_shop_avg_sales_seniority_2_absolute_mean']                \n#features_to_keep_2+=['category_months_since_first_sale_in_shop_avg_sales_seniority_2_annual_mean']           \n#features_to_keep_2+=['category_months_since_first_sale_in_shop_avg_sales_seniority_2_semiannual_mean']\nfeatures_to_keep_2+=['category_months_since_first_sale_in_shop_avg_sales_seniority_2_recent_mean']              \n#features_to_keep_2+=['category_months_since_first_sale_in_shop_avg_sales_seniority_2_lag_1']                  \n#features_to_keep_2+=['category_months_since_first_sale_in_shop_avg_sales_seniority_2_lag_12']                  \n\n#features_to_keep_2+=['shop_category_months_since_first_sale_in_shop_avg_sales_seniority_2_absolute_mean']            \n#features_to_keep_2+=['shop_category_months_since_first_sale_in_shop_avg_sales_seniority_2_annual_mean']\n#features_to_keep_2+=['shop_category_months_since_first_sale_in_shop_avg_sales_seniority_2_semiannual_mean']\n#features_to_keep_2+=['shop_category_months_since_first_sale_in_shop_avg_sales_seniority_2_recent_mean']               \n#features_to_keep_2+=['shop_category_months_since_first_sale_in_shop_avg_sales_seniority_2_lag_1']\nfeatures_to_keep_2+=['shop_category_months_since_first_sale_in_shop_avg_sales_seniority_2_lag_12']                \n\n\n\n#--------------\n# SPATIAL TRENDS\n#features_to_keep_2+=['shop_category_avg_sales_compared_to_category_absolute_mean']\nfeatures_to_keep_2+=['shop_category_avg_sales_compared_to_category_annual_mean']              \n#features_to_keep_2+=['shop_category_avg_sales_compared_to_category_semiannual_mean']\nfeatures_to_keep_2+=['shop_category_avg_sales_compared_to_category_recent_mean']            \n#features_to_keep_2+=['shop_category_avg_sales_compared_to_category_lag_1']                  \n#features_to_keep_2+=['shop_category_avg_sales_compared_to_category_lag_12']                \n\n\n# TEMPORAL TRENDS\n#features_to_keep_2+=['shop_avg_sales_compare_recent_mean_to_annual_mean']\n#features_to_keep_2+=['shop_avg_sales_compare_lag1_to_annual_mean']                               \nfeatures_to_keep_2+=['shop_avg_sales_compare_lag12_to_annual_mean']                              \n\nfeatures_to_keep_2+=['supercategory_avg_sales_compare_recent_mean_to_annual_mean']         \n#features_to_keep_2+=['supercategory_avg_sales_compare_lag1_to_annual_mean']\nfeatures_to_keep_2+=['supercategory_avg_sales_compare_lag12_to_annual_mean']               \n\nfeatures_to_keep_2+=['category_avg_sales_compare_recent_mean_to_annual_mean']            \n#features_to_keep_2+=['category_avg_sales_compare_lag1_to_annual_mean']\nfeatures_to_keep_2+=['category_avg_sales_compare_lag12_to_annual_mean']                  \n\nfeatures_to_keep_2+=['category_avg_sales_compared_to_supercategory_recent_mean']           \n#features_to_keep_2+=['category_avg_sales_compared_to_supercategory_lag_1']                \nfeatures_to_keep_2+=['category_avg_sales_compared_to_supercategory_lag_12']                      \n\n\n# TEMPORAL TRENDS FOR SENIORITY 2 (months of the year where seniority 2 is boosted due to previous release of new items)\n#features_to_keep_2+=['shop_avg_sales_seniority_2_compare_recent_mean_to_annual_mean']\n#features_to_keep_2+=['shop_avg_sales_seniority_2_compare_lag1_to_annual_mean']                   \nfeatures_to_keep_2+=['shop_avg_sales_seniority_2_compare_lag12_to_annual_mean']                 \n\n#features_to_keep_2+=['supercategory_avg_sales_seniority_2_compare_recent_mean_to_annual_mean']\n#features_to_keep_2+=['supercategory_avg_sales_seniority_2_compare_lag1_to_annual_mean']\nfeatures_to_keep_2+=['supercategory_avg_sales_seniority_2_compare_lag12_to_annual_mean']         \n\n#features_to_keep_2+=['category_avg_sales_seniority_2_compare_recent_mean_to_annual_mean']\n#features_to_keep_2+=['category_avg_sales_seniority_2_compare_lag1_to_annual_mean']\nfeatures_to_keep_2+=['category_avg_sales_seniority_2_compare_lag12_to_annual_mean']              \n\n\n# MONTH OF RELEASE\nfeatures_to_keep_2+=['supercategory_month_of_release_avg_sales_compared_to_supercategory_absolute_mean']          \nfeatures_to_keep_2+=['category_month_of_release_avg_sales_compared_to_category_absolute_mean']                    ","94b785cf":"# drop features\nfeatures_to_discard_2=list( set(list(train_X.columns)) - set(['month_id','item_quantity']+features_to_keep_2) )\ntrain_X.drop(features_to_discard_2,axis=1,inplace=True)\n\n# fill missing values\ntrain_X['item_quantity_extralin2'].fillna(train_X['item_quantity_lag_1'],inplace=True)\ntrain_X['item_quantity_extralin3'].fillna(train_X['item_quantity_lag_1'],inplace=True)\ntrain_X.fillna(-1,inplace=True)\n\n# split dataset\n(X_train,Y_train,X_val,Y_val) = datasplit_train_val(train_X,n_months_val,month_id_first)\neval_set = [(X_train,Y_train),(X_val,Y_val)]+datasplit_evalset(train_X,month_id_first)\n\n# show dataset info\nprint('number of features to keep : '+str(len(features_to_keep_2)))\nprint('number of features kept    : '+str(len(X_train.columns)))\n\nprint(X_train.info(null_counts=True,verbose=True))\n\n\n# clear memory space\ndel train_X\ngc.collect()","7612a597":"# SET XGBOOST PARAMETERS\n\nxgb_params_2={'objective':        'reg:squarederror',\n# TREE SPECIFIC PARAMETERS\n              'max_depth':        6,\n              'min_child_weight': 300,\n              'subsample':        0.8,\n              'colsample_bytree': 0.8,\n# PARAMETERS RELATED TO THE LEARNING\/BOOSTING PROCESS\n              'learning_rate':    0.02,\n              'n_estimators':     2000,  \n# MISCELLANEOUS PARAMETERS\n              'base_score':       0.4,\n              'n_jobs':           4,\n              'random_state':     2\n             }\n\n\n\n# FIT MODEL\nts=time.time()\n\nxgbreg=XGBRegressor(**xgb_params_2)\nxgbreg.fit(X_train,Y_train,eval_set=eval_set,eval_metric='rmse',verbose=False)\n\nprint(time.time()-ts)","c70d32f2":"# PERFORMANCE ANALYSIS\nperformance_analysis_val(xgbreg,rmse_guess_train,rmse_base_train,rmse_guess_val,rmse_base_val,rmse_guess_evalset,rmse_base_evalset)\n\n# learning curves\naxes=learning_curves(xgbreg,n_months_val,month_id_first)\n\naxes[0,0].set_ylim(0.8,0.95)\naxes[0,1].set_ylim(0.804,0.82)\naxes[1,0].set_ylim(0.7,0.9)\naxes[1,1].set_ylim(0.9,1.4)\n\ndel axes","17d856b7":"# FEATURE IMPORTANCE\nplot_feature_importance(xgbreg,X_train)","e57142e5":"# clear memory\ndel X_train, Y_train, X_val, Y_val\ndel eval_set\ndel rmse_guess_train, rmse_base_train, rmse_guess_val, rmse_base_val, rmse_guess_evalset, rmse_base_evalset\ndel xgbreg\n\ngc.collect()","b929886e":"# append variables to list of variables to keep\nloaded.extend(['features_to_keep_2','features_to_discard_2','xgb_params_2'])","c5aa6620":"reset_variable_space","5199d851":"# create directories\ncreate_directory(os.path.join(DATA_FOLDER, 'predictions'))\ncreate_directory(os.path.join(DATA_FOLDER, 'predictions\/models\/'))","0489e757":"dataset_name='train_0_pred'\nmonth_id_first=18\n\ntrain_X=pd.read_pickle(os.path.join(DATA_FOLDER,'training\/'+dataset_name+'.pkl'))\n\nprint(train_X.info(null_counts=True,verbose=True))","9b5ee8bb":"# analyse dataset\nrmse_guess_train, rmse_base_train, rmse_guess_evalset, rmse_base_evalset = target_analysis_test(train_X,month_id_first=18)","8c60b8a3":"# drop features\ntrain_X.drop(features_to_discard_0,axis=1,inplace=True)\n\n# fill missing values\ntrain_X.fillna(-1,inplace=True)\n\n# split dataset\n(X_train,Y_train,X_test) = datasplit_train_test(train_X,month_id_first)\neval_set = [(X_train,Y_train)]+datasplit_evalset(train_X,month_id_first)\n\n# show dataset info\nprint('number of features to keep : '+str(len(features_to_keep_0)))\nprint('number of features kept    : '+str(len(X_train.columns)))\n\nprint(X_train.info(null_counts=True,verbose=True))\n\n\n# clear memory space\ndel train_X\ngc.collect()","acacd7ef":"# FIT MODEL\nts=time.time()\n\nxgbreg=XGBRegressor(**xgb_params_0)\nxgbreg.fit(X_train,Y_train,eval_set=eval_set,eval_metric='rmse',verbose=False)\n\nprint(time.time()-ts)","83056206":"# PERFORMANCE ANALYSIS\nperformance_analysis_test(xgbreg,rmse_guess_train,rmse_base_train,rmse_guess_evalset,rmse_base_evalset)","5b4142c8":"# FEATURE IMPORTANCE\nplot_feature_importance(xgbreg,X_train)","6d0b5c8b":"# create directory\ncreate_directory(os.path.join(DATA_FOLDER, 'predictions\/models\/xgbreg_seniority0'))\n\n# export model\npickle.dump(xgbreg, open(os.path.join(DATA_FOLDER,'predictions\/models\/xgbreg_seniority0\/model.pickle'), 'wb'))\nX_test.to_pickle(os.path.join(DATA_FOLDER,'predictions\/models\/xgbreg_seniority0\/X_test.pkl'))","2a034ed9":"# clear memory\ndel X_train, Y_train, X_test\ndel eval_set\ndel rmse_guess_train, rmse_base_train, rmse_guess_evalset, rmse_base_evalset\ndel xgbreg\n\ndel features_to_keep_0, features_to_discard_0\ndel xgb_params_0\n\nloaded.remove('features_to_keep_0')\nloaded.remove('features_to_discard_0')\nloaded.remove('xgb_params_0')\n\ngc.collect()","5b596f02":"reset_variable_space","56756c64":"dataset_name='train_1_pred'\nmonth_id_first=18\n\ntrain_X=pd.read_pickle(os.path.join(DATA_FOLDER,'training\/'+dataset_name+'.pkl'))\n\nprint(train_X.info(null_counts=True,verbose=True))","dad29f99":"# analyse dataset\nrmse_guess_train, rmse_base_train, rmse_guess_evalset, rmse_base_evalset = target_analysis_test(train_X,month_id_first=18)","5c2db0ee":"# drop features\ntrain_X.drop(features_to_discard_1,axis=1,inplace=True)\n\n# fill missing values\ntrain_X.fillna(-1,inplace=True)\n\n# split dataset\n(X_train,Y_train,X_test) = datasplit_train_test(train_X,month_id_first)\neval_set = [(X_train,Y_train)]+datasplit_evalset(train_X,month_id_first)\n\n# show dataset info\nprint('number of features to keep : '+str(len(features_to_keep_1)))\nprint('number of features kept    : '+str(len(X_train.columns)))\n\nprint(X_train.info(null_counts=True,verbose=True))\n\n\n# clear memory space\ndel train_X\ngc.collect()","871f6887":"# restrict clipping range for training data\n# for seniority 1, we observed that target values larger than 8 are so rare that they may be considered noise\n# clipping target values to 6 acts as a filter that improves the overall prediction on that class of samples\nY_train.clip(0,6,inplace=True)","8739af44":"# FIT MODEL\nts=time.time()\n\nxgbreg=XGBRegressor(**xgb_params_1)\nxgbreg.fit(X_train,Y_train,eval_set=eval_set,eval_metric='rmse',verbose=False)\n\nprint(time.time()-ts)","278c228a":"# PERFORMANCE ANALYSIS\nperformance_analysis_test(xgbreg,rmse_guess_train,rmse_base_train,rmse_guess_evalset,rmse_base_evalset)","34f55785":"# FEATURE IMPORTANCE\nplot_feature_importance(xgbreg,X_train)","25c30f20":"# create directory\ncreate_directory(os.path.join(DATA_FOLDER, 'predictions\/models\/xgbreg_seniority1'))\n\n# export model\npickle.dump(xgbreg, open(os.path.join(DATA_FOLDER,'predictions\/models\/xgbreg_seniority1\/model.pickle'), 'wb'))\nX_test.to_pickle(os.path.join(DATA_FOLDER,'predictions\/models\/xgbreg_seniority1\/X_test.pkl'))","e3e53376":"# clear memory\ndel X_train, Y_train, X_test\ndel eval_set\ndel rmse_guess_train, rmse_base_train, rmse_guess_evalset, rmse_base_evalset\ndel xgbreg\n\ndel features_to_keep_1, features_to_discard_1\ndel xgb_params_1\n\nloaded.remove('features_to_keep_1')\nloaded.remove('features_to_discard_1')\nloaded.remove('xgb_params_1')\n\ngc.collect()","7f25732a":"reset_variable_space","df77780a":"dataset_name='train_2_pred'\nmonth_id_first=18\n\ntrain_X=pd.read_pickle(os.path.join(DATA_FOLDER,'training\/'+dataset_name+'.pkl'))\n\nprint(train_X.info(null_counts=True,verbose=True))","41d00cdd":"# analyse dataset\nrmse_guess_train, rmse_base_train, rmse_guess_evalset, rmse_base_evalset = target_analysis_test(train_X,month_id_first=18)","0b7c63bc":"# drop features\ntrain_X.drop(features_to_discard_2,axis=1,inplace=True)\n\n# fill missing values\ntrain_X['item_quantity_extralin2'].fillna(train_X['item_quantity_lag_1'],inplace=True)\ntrain_X['item_quantity_extralin3'].fillna(train_X['item_quantity_lag_1'],inplace=True)\ntrain_X.fillna(-1,inplace=True)\n\n# split dataset\n(X_train,Y_train,X_test) = datasplit_train_test(train_X,month_id_first)\neval_set = [(X_train,Y_train)]+datasplit_evalset(train_X,month_id_first)\n\n# show dataset info\nprint('number of features to keep : '+str(len(features_to_keep_2)))\nprint('number of features kept    : '+str(len(X_train.columns)))\n\nprint(X_train.info(null_counts=True,verbose=True))\n\n\n# clear memory space\ndel train_X\ngc.collect()","f6c1ff40":"# FIT MODEL\nts=time.time()\n\nxgbreg=XGBRegressor(**xgb_params_2)\nxgbreg.fit(X_train,Y_train,eval_set=eval_set,eval_metric='rmse',verbose=False)\n\nprint(time.time()-ts)","e891819f":"# PERFORMANCE ANALYSIS\nperformance_analysis_test(xgbreg,rmse_guess_train,rmse_base_train,rmse_guess_evalset,rmse_base_evalset)","32e09656":"# FEATURE IMPORTANCE\nplot_feature_importance(xgbreg,X_train)","f056e690":"# create directory\ncreate_directory(os.path.join(DATA_FOLDER, 'predictions\/models\/xgbreg_seniority2'))\n\n# export model\npickle.dump(xgbreg, open(os.path.join(DATA_FOLDER,'predictions\/models\/xgbreg_seniority2\/model.pickle'), 'wb'))\nX_test.to_pickle(os.path.join(DATA_FOLDER,'predictions\/models\/xgbreg_seniority2\/X_test.pkl'))","ca4422af":"# clear memory\ndel X_train, Y_train, X_test\ndel eval_set\ndel rmse_guess_train, rmse_base_train, rmse_guess_evalset, rmse_base_evalset\ndel xgbreg\n\ndel features_to_keep_2, features_to_discard_2\ndel xgb_params_2\n\nloaded.remove('features_to_keep_2')\nloaded.remove('features_to_discard_2')\nloaded.remove('xgb_params_2')\n\ngc.collect()","204deaad":"reset_variable_space","dcea4acb":"def predict_seniority(seniority,ntree=0):\n\n    # import model and test set\n    xgbreg = pickle.load(open(os.path.join(DATA_FOLDER,'predictions\/models\/xgbreg_seniority'+str(seniority)+'\/model.pickle'), 'rb'))\n    X_test = pd.read_pickle(os.path.join(DATA_FOLDER,'predictions\/models\/xgbreg_seniority'+str(seniority)+'\/X_test.pkl'))\n\n    # form prediction\n    Y_pred_test=xgbreg.predict(X_test,ntree_limit=ntree).clip(0,20)\n\n    Y_pred=pd.read_pickle(os.path.join(DATA_FOLDER,'processed\/train_'+str(seniority)+'.pkl'))\n    Y_pred=Y_pred.loc[Y_pred['month_id']==34,['shop_id','item_id']]\n    Y_pred['prediction']=Y_pred_test\n\n    return Y_pred","088a062b":"# build global prediction\n\n# number of trees used for predictions for pairs of seniority 0,1,2 respectively\nntrees=[200,400,2000]\n\nY_0=predict_seniority(seniority=0,ntree=ntrees[0])\nY_1=predict_seniority(seniority=1,ntree=ntrees[1])\nY_2=predict_seniority(seniority=2,ntree=ntrees[2])\nY_pred=pd.concat([Y_0,Y_1,Y_2],axis=0,sort=False)\n\ntest_df = pd.read_csv(os.path.join(RAW_DATA_FOLDER, 'test.csv'))\ntest_df=test_df.join(Y_pred.set_index(['shop_id','item_id']),on=['shop_id','item_id'])\n\nprint(test_df.info(null_counts=True))\ntest_df","a03237bc":"# build and export submission\nsubmission = pd.DataFrame({\n    \"ID\": test_df['ID'], \n    \"item_cnt_month\": test_df['prediction']\n})\n\nsubmission.to_csv('xgb_prediction.csv', index=False)","d1c0688d":"# clear memory\ndel test_df\ndel Y_0, Y_1, Y_2, Y_pred\ndel submission\n\ngc.collect()","a19bc368":"reset_variable_space","e1d5f306":"## Export data","4be3a61c":"3 situations may occur, and it is probably best to think of 3 different models for each of these categories!\n\n1) the item has previously been sold in this shop (seniority = 2)\n- This category covers about 50-60% of the dataset\n- We are able to use information about past sales for this pair (shop, item) to predict future sales\n\n2) the item has never been sold in this shop, but it was already in the global catalogue before  (seniority = 1)\n- This category covers about 40-50% of the dataset each month\n- From the above analysis, such a pair (shop,item) has around 95% chance not to result in any sales this month either, so estimating their sales to 0 should be correct for 95% of them, and wrong for about 5% of them\n- We have no past data for this pair (shop,item), but we have past data for this item in other shops where it was sold.\n\n3) the item is new in the global catalogue  (seniority = 0)\n- This category covers roughly between 2.5 and 10% of all realisations in the dataset\n- There is no past data for this item, so we must estimate the sales of this item based on the sales of similar items when they were released.\n\nNB: All items are assumed new on the first months because we do not have data from earlier months. This causes the repartition of items among the different categories to be misestimated for about the first 12 months.","31cd8950":"## FEATURE ENGINEERING","98d15a1f":"### Save model and test set","9e92e0fe":"## PREPROCESSING OF THE SHOPS","439cfced":"### Temporal transformation of features","2a26e504":"## Import data","cb7b983c":"## SECOND ORDER INTERACTIONS FEATURES","89d12804":"## DISTRIBUTION OF SALES AMONG SHOPS THROUGH TIME","684c3865":"## ----------------------------------------------","28c891bb":"### Remove outlier shops","92caee9f":"### Discard irrelevant features","6264f349":"## Format and export data","4e14cdcc":"## ----------------------------------------------","d9846de7":"### Total monthly sales","3fdccbcc":"## ----------------------------------------------","26794185":"## -------------------------------------------------------------","2676e8fe":"## ----------------------------------------------","5a4d2eb7":"## ABSOLUTE TIME","0757201e":"### Discard irrelevant features","b4d16583":"### 2nd order features - Temporal dynamics of sales for the items","c7424e5e":"### Visualization","099c218f":"### Discard irrelevant features","970cbc1a":"# 6 - TRAINING FOR PREDICTION","f834cf92":"## ANALYSIS OF TEMPORAL TRENDS","6b409ae2":"## -------------------------------------------------------------","99e5118a":"## Format dataframes","d9deaf88":"### Remove outliers in price data","7e601644":"### Import and process training set","7e5ea627":"# 4 - FEATURE AGGREGATION","9ea8c882":"### PRICE FEATURES","588fb27d":"#### Feature analysis","ce23091b":"### XGBRegressor - training for predictions","ce74b4bc":"### Visualization","2a9eb731":"## -------------------------------------------------------------","e5bf0862":"# 3 - TARGET ENCODING","581039e3":"## SENIORITY 0","88cb6d91":"### PRICE FEATURES","a3e87a5b":"## SENIORITY 1","2c6ccecc":"#### Feature analysis","53772dc1":"## -------------------------------------------------------------","ed9d3ea7":"## SENIORITY 0","517e1168":"## SENIORITY 2","d1461f25":"There seem to be a peak of average sales on December months.\n\nWe also notice that the sale quantities of items released in October, November, December appears to remain consistently higher than average the next months. Conversely, items released in February are sold in smaller quantities all year round.\n\n--> There is an influence of the month where the item is first released!","3593d714":"## SPLIT DATAFRAME DEPENDING ON SENIORITY","bb3dffbc":"## SENIORITY 0","571b287e":"## Import data","53e402a8":"CONCLUSION:\n   - pairs (shop,item) of seniority 0 are independent of all temporal features\n   - pairs (shop,item) of seniority 1 depends strongly on 'months since release', with a weak peak in December and no clear influence of the month of release of the item\n   - pairs (shop,item) of seniority 2 depends strongly on 'months since release', as well as on 'months since first sale in shop' in a slightly different fashion. There is also a peak of sales in December, and for items released in September and November. The dependency on the release time is likely to vary from one type of items to another.","883dde54":"## Export data","6e05886b":"# 1 - MONTHLY AGGREGATION","19fb9e64":"# 0 - DATA CLEANING","a88bedcf":"## THIRD ORDER INTERACTIONS FEATURES","9842158e":"## FIRST ORDER INTERACTIONS FEATURES","0d0cef74":"### XGBRegressor - training for predictions","71c2129e":"# 5 - VALIDATION","d89798e5":"### Import and process training set","16ee4b13":"# ----------------------------------------------","7c2bf40f":"### XGBRegressor - validation","b347966d":"### TARGET ENCODED FEATURES","d26e3fe2":"### Save model and test set","44ceb553":"## Functions for training and validation","c214b867":"### XGBRegressor - validation","131ec7b4":"### 1st order features - ITEM-RELATIVE DISTRIBUTION","db500388":"#### Feature aggregation","db302ae3":"## TEMPORAL VARIABILITY ENCODING","7f4fc054":"##### Define core variable space and macro to reset variable space","8e795e51":"# 7 - ASSEMBLE PREDICTION","178f7fab":"Samples (shop, item) of seniority 1 are made of items that have never been sold in this shop, but have been sold in other shops before. Such a sample (shop, item) is likely to be made of an item that is not actually available in this shop, and the likelihood that this item will be sold in this shop in the future is very low. In fact, data analysis proves that only 5% of these samples contribute to the sales (compared to 20-25% of samples of seniority 2. Those few samples of seniority 1 that will be sold are likely made of items that are always sold in low quantities, possibly less than one per month. Here again, data analysis shows that the average sales for items of seniority 1 is around 0.05 (against 0.4 in seniority 2) and the likelihood that an item of seniority 1 is sold in quantities superior to 3 is less than 0.1%. Samples of seniority 1 yielding sale quantities above 8 are so rare that they may be considered noise. For seniority 1, the critical features are the number of months since it has been released in the catalogue without being sold in this shop, and the typical sale quantities of this item in shops where it has been sold in the past. If an item is typically sold in large quantities but have never been sold in a given shop, it is unlikely that it will suddenly start being sold next month. Similarly, if the item has never been sold in a given shop in spite having been released for many months in the catalogue, it is likely that it will never be sold in this shop, or only in very small quantities. Conversely, if the item has only been released last month, it is possible that it just hasn't add the chance to be sold last month but it may be sold in low quantities next month. In particular, we find that the most relevant features are target encodings of the past typical sale quantities of samples of seniority 1 in this shop, in the same category of items, and with the same number of months since it has been released, as well as the quantities sold for this item in other shops where it has been sold in the past.\n\n\nADDITIONAL REMARKS:\n- For now, we disregard the influence of the shops that have just opened. Such shops are rare in the dataset and we expect they do not impact significantly the overall result.\n\n- The temporality of items in seniority 1 seem to be mostly related to 'months_since_release', but the sales of such items do not seem to be related to the absolute period of the year, apart from a peak in December. The effect of the month of release or the time of the year apart from the Christmas period are negligible.","c0eea3b7":"### 1st order features - RELATIVE TEMPORAL DYNAMICS","cfa0f30b":"### Export aggregated dataset","3e9bb749":"### Export aggregated dataset","79887e88":"## BASIC PREPROCESSING","fe114726":"### Import and process training set","976edf3e":"## PREPROCESSING OF ITEM_CATEGORIES","c1ee8a2d":"### Feature analysis functions","78c6489d":"- Cleaning of input dataframes\n- Preliminary data analysis","c13122d7":"### Import and process training set","89938974":"## ----------------------------------------------","142ac060":"Samples (shop, item) of seniority 2 are made of items that have previously been sold in this very shop, and so they are in the local catalogue of this shop for sure. For such samples, our predictions will be mostly based on the quantities sold for this item in this shop in the past.","9dffa9de":"## SENIORITY 2","79efcb56":"### Remove duplicate shops","8c97ab2c":"### Analysis of opening periods","7a913116":"### TARGET ENCODED FEATURES","94ffb4eb":"### Feature engineering","55e6a4a0":"# ----------------------------------------------","2873ad48":"## ----------------------------------------------","1ecba2d6":"## -------------------------------------------------------------","6371d91a":"### Analysis of duplicate shops","9278d507":"## Aggregation functions","5e033fc3":"## Import data","f9dfa233":"### Distribution of target value for each seniority level","753df590":"### 2nd order features - Spatial repartition of items \/ Topical specialization of the shops","fbc40b78":"## -------------------------------------------------------------","d9d50b63":"## ANALYSIS BY SENIORITY LEVEL","d4f54c92":"### RAW FEATURES","83f00d99":"## SENIORITY 1","41ebf7aa":"## SENIORITY 1","113e9c6f":"## Import raw data","108a270d":"### XGBRegressor - validation","a7ceb058":"## ----------------------------------------------","cd956f43":"Samples (shop, item) of seniority 0 are totally new in the catalogue. No past data is available for these pairs. The typical sale quantities of the new items vary a lot from month to month and their temporal evolution appears much noisier than older items. For these items, we base our predictions mostly on past typical sale quantities of similar items on the month where they were released. Similar items are defined as items of the same category, or same supercategory (like some other game, or some other book).","ec73fc4b":"## -------------------------------------------------------------","e4bc2d71":"### XGBRegressor - training for predictions","a0d6e5a7":"### Feature engineering: cities","2ab7e3fa":"### Analysis of item categories","3cadf372":"### Distribution of target value","3a89e72c":"### Export aggregated dataset","6c563260":"### TARGET ENCODED FEATURES","9a8cf267":"### Drop irrelevant categories","f9eaf4fd":"## ANALYSE TARGET DISTRIBUTIONS IN THE TRAINING SET","e0269214":"### Import and process training set","2a06873c":"- Aggregation of data at the monthly level\n- Feature engineering at the monthly level","dd8c16fa":"## Functions for target encoding","10bd840d":"## PROCESS PRICE DATA","0c7614a3":"# 2 - EXPLORATORY DATA ANALYSIS","5735e2d4":"* seniority 0: \n    - there is no relative time for new items because 'month of release' = 'current month', so we can only analyse the correlation of sales with the current month of the year for new items.\n\n* seniority 1: \n    - these items are highly correlated with the number of 'months since release', but very weakly with the absolute month of the year, and almost not at all with the 'month of release'\n\n* seniority 2: \n    - these items are highly correlated with the months since release and the month since first sale in shop. \n    - there is a slight correlation with 'month of release' (more items sold in September and November). This is likely to be specific to some items and not others.","7e4d7b86":"### RAW FEATURES","14d2a620":"### Save model and test set","424bf14c":"#### Feature aggregation","78ae7a8b":"We also notice a storng influence of the time spent wince the item has been released: on average, newer items are sold in larger quantities than older ones.\n\n--> There is an influence of the number of months since the item was first released (or first sold in shop)","5606d45b":"In general:\n\nshop_avg_sales_seniority_2 ~= shop_avg_sales\n\nshop_avg_sales_over_sold   ~= (1+shop_avg_sales)\n\n(seniority_2 is slightly superior due to a larger fraction of items sold)\n\nBUT: shop 55 (digital warehouse) has similar shop_avg_sales_seniority_2 = shop_avg_sales_over_sold --> this shop always sells the same article every month\n\nOtherwise, seniority_2 or global are fairly similar. Early months of the year have a premium on seniority 2 compared to global (product released during christmas are sold better in the early months).","e0369e72":"## EXTEND DATAFRAME TO PRODUCT SHOP x ITEMS EVERY MONTH","fed9a1a8":"## PREPROCESSING OF THE PRICES","a61a61f3":"### RAW FEATURES","9413498a":"### 1st order features - SPATIAL DISTRIBUTION","f542cc01":"- seniority 0: there is no clear correlation of the month of the year with the sales of new items\n- seniority 1: weak peak of sales in December due to a larger fraction of items sold, but no clear peak of the quantity of items sold when they are sold\n- seniority 2: peak of sales in December due to a larger fraction of items sold and larger quantities of items sold when they are sold\n\n- on average: seniority 1 < seniority 2 < seniority 0\n- seniority 2 represent most of the sales, and so the quantities over sold are almost identical on the global average and on the seniority 2 averages. However, the quantities over all items are much more precisely predicted when seniority 2 is isolated from seniority 0 and 1 (they are underestimated otherwise due to the large contribution of seniority 1 in the global average)","f94cbb48":"QUICK DESCRIPTION\n\nThis notebook provides a full solution to the Predict Future Sales challenge. It includes\n- dataset cleaning and monthly aggregation\n- exploratory data analysis\n- feature engineering (extraction of supercategories from the category names, features to account for time since release of an item, time since first\/last sale in shop, typical values and statistics on the prices,...)\n- target encoding, then temporal lagging and temporal averaging (or min\/max) over past months\n- splitting of dataset into three classes of realisations (month,shop,item): item new in the dataset this month \/ item not new but never sold in this shop before \/ item sold in this shop in the past\n- validation and fitting of xgbregressor for each of the three classes of realisations defined above, with different features and hyperparameters","383d310b":"### 0-th order feature: ABSOLUTE TIME","866d8b40":"DESCRIPTION OF THE APPROACH\n\nWe aim at predicting the quantities sold for each sample (shop, item) on the next month. For each item, the quantities sold vary through time (from month to month) and space (from shop to shop). Besides, similarity between items is embedded in the category they belong to.\n\n- Additional features are extracted from the item categories to enhance the understanding of item similarity. In particular, supercategories are defined that encompass several categories of items of the same type (such as consoles, video games, accessories, books, music, ...).\n\n- The spatial variability of the sales appear to be mostly orthogonal to the temporal variations (similar temporal variations across all shops). Spatial trends are accounted for through target encoding of the shops.\n\n- This solution is mostly based on the temporal separation of the samples (shop, item) depending on so-called 'seniority' levels. Each month, we analyse separately samples made of items that are new in the dataset this month (seniority = 0), samples (shop, item) made of items that are not new but have never been sold in this specific shop before (seniority = 1), and samples (shop, item) made of items that have already been sold in the past in this very shop (seniority = 2).\n\nThe main idea behind this separation is that the dataset on a given month is made of the cartesian product between the whole global catalogue of items and the whole set of shops open, whereas in practice the local catalogue of items available in a given shop only includes a fraction of the global catalogue. Being able to discriminate items that are in fact available in a given shop from items that are actually not should ease the predictions, as samples (shop, item) with items not even available in the shop in question cannot possibly be sold there in the future. \n\nSamples (shop, item) of seniority 2 are made of items that have previously been sold in this very shop, and so they are in the local catalogue of this shop for sure. For such samples, our predictions will be mostly based on the quantities sold for this item in this shop in the past.\n\nSamples (shop, item) of seniority 1 are made of items that have never been sold in this shop, but have been sold in other shops before. Such a sample (shop, item) is likely to be made of an item that is not actually available in this shop, and the likelihood that this item will be sold in this shop in the future is very low. In fact, data analysis proves that only 5% of these samples contribute to the sales (compared to 20-25% of samples of seniority 2. Those few samples of seniority 1 that will be sold are likely made of items that are always sold in low quantities, possibly less than one per month. Here again, data analysis shows that the average sales for items of seniority 1 is around 0.05 (against 0.4 in seniority 2) and the likelihood that an item of seniority 1 is sold in quantities superior to 3 is less than 0.1%. Samples of seniority 1 yielding sale quantities above 8 are so rare that they may be considered noise. For seniority 1, the critical features are the number of months since it has been released in the catalogue without being sold in this shop, and the typical sale quantities of this item in shops where it has been sold in the past. If an item is typically sold in large quantities but have never been sold in a given shop, it is unlikely that it will suddenly start being sold next month. Similarly, if the item has never been sold in a given shop in spite having been released for many months in the catalogue, it is likely that it will never be sold in this shop, or only in very small quantities. Conversely, if the item has only been released last month, it is possible that it just hasn't add the chance to be sold last month but it may be sold in low quantities next month. In particular, we find that the most relevant features are target encodings of the past typical sale quantities of samples of seniority 1 in this shop, in the same category of items, and with the same number of months since it has been released, as well as the quantities sold for this item in other shops where it has been sold in the past.\n\nFinally, samples (shop, item) of seniority 0 are totally new in the catalogue. No past data is available for these pairs. Besides, the typical sale quantities of the new items vary a lot from month to month and their temporal evolution appears much noisier than older items. For these items, we base our predictions mostly on past typical sale quantities of similar items on the month where they were released. Similar items are defined as items of the same category, or same supercategory (like some other game, or some other book).\n\n- Apart from the separation between seniority levels, the influence of temporality is threefold: the quantities sold for a given item in a given shop depend on the absolute period of the year (more sales in December due to Christmas), on how many months since the item has been released (older items are less popular than newer ones), and also on the month where it has been released (items released in December are sold more all year round than items released in February). The sensitivity of the items to each of these three temporality vary from category to category and depending on the seniority level of the sample: samples of seniority 1 are almost insensitive to the absolute period of the year but very sensitive to the number of months since they have been released, while deliveries are sold in similar quantities regardless of how long the service has been offered, etc... Thus, the effect of temporality is accounted for jointly with other features through target encoding.\n\n- Three separate models are built using the XGBRegressor algorithm from xgboost. Different hyperparameters are used to account for the different distributions of target variable, different problem complexities, and different tendencies to overfit","7b2ce251":"## SENIORITY 2","6d8e6923":"## -------------------------------------------------------------","ca91dce7":"## Import libraries","0a9be52c":"### PRICE FEATURES","81ad4adf":"#### Feature analysis","482dc13f":"### Import and process training set","b6227339":"The globally decreasing trend of the total sales over time is mostly due to the decreasing amount of items in the catalogue. The number of shops open remains roughly the same, and the average sales per item as well, except for the December peaks (months 11 and 23)\n\nThe December peaks should therefore imply a different distribution of the target distributions among the values 0,1,...,20. In December, the repartition should have more realisations in the classes with higher values compared to other months.\n\nApart from December, the average amount of sales per item in the catalogue remain roughly the same over all months.","bcfcce90":"- Encode the temporal variability of the sale quantities, category by category","1bc9dd77":"#### Feature aggregation"}}