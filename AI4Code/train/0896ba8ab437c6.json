{"cell_type":{"11f4e457":"code","99980af8":"code","ad85542c":"code","ec3d9caf":"code","f816f3a7":"code","270b9a1f":"code","cb511b85":"code","1aaf9432":"code","22a65aed":"code","76b6c57f":"code","076bf787":"code","2df614b0":"code","6910d948":"code","49d3ddad":"code","23c3aa64":"code","a57d8960":"code","19d99636":"code","f4f5b0c3":"code","8819386e":"code","cb77110d":"code","5fe5ef6d":"code","15dd248f":"code","0bc2749b":"code","159c68bc":"code","c33a6626":"code","cebd2f06":"code","213668d2":"code","08ae3f2f":"code","73aca97e":"code","57e4a7b3":"code","513e19ae":"markdown","841e9f41":"markdown","6b4e6859":"markdown","9bed22bf":"markdown","ff9b6c98":"markdown","130953f9":"markdown","fb379fce":"markdown"},"source":{"11f4e457":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport argparse, os, cv2, pickle\n\nfrom PIL import Image\nfrom IPython.display import SVG\nfrom tensorflow.keras.utils import plot_model, model_to_dot\nfrom sklearn.utils import class_weight\nfrom tqdm import tqdm\n\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import (Add, Input, Conv2D, Dense, MaxPooling2D, UpSampling2D, Activation)\nfrom tensorflow.keras.optimizers import Adam, SGD, Adadelta\nfrom tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint, Callback\nfrom tensorflow.keras.initializers import *\nfrom tensorflow.keras.regularizers import *\n# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"","99980af8":"def dense_autoencoder(input_shape):\n    \n    # Stage 1: Encoder Input\n    encoder_input = Input(input_shape,name=\"encoder_input\")\n    \n#     # Stage 2: Dense => ReLU\n#     encoded = Dense(1024,name=\"encoder_1\")(encoder_input)\n#     encoded = Activation(\"relu\",name=\"relu_encoder_1\")(encoded)\n    \n    # Stage 2: Dense => ReLU\n    encoded = Dense(512,name=\"encoder_2\")(encoder_input)\n    encoded = Activation(\"relu\",name=\"relu_encoder_2\")(encoded)\n    \n    # Stage 3: Dense => ReLU\n    encoded = Dense(256,name=\"encoder_3\")(encoded)\n    encoded = Activation(\"relu\",name=\"relu_encoder_3\")(encoded)\n    \n    # Stage 4: Dense => ReLU\n    encoded = Dense(64,name=\"encoder_4\")(encoded)\n    encoded = Activation(\"relu\",name=\"relu_encoder_4\")(encoded)\n    \n    # Encoder model\n    encoder = Model(inputs=encoder_input,outputs=encoded)\n    \n    # Stage 5: Decoder Input\n    decoder_input = Input(shape=(64,),name=\"decoder_input\")\n    \n    # Stage 6: Dense => ReLU\n    decoded = Dense(256,name=\"decoder_1\")(decoder_input)\n    decoded = Activation(\"relu\",name=\"relu_decoder_1\")(decoded)\n    \n    # Stage 7: Dense => ReLU\n    decoded = Dense(512,name=\"decoder_2\")(decoded)\n    decoded = Activation(\"relu\",name=\"relu_decoder_2\")(decoded)\n    \n#     # Stage 9: Dense => ReLU\n#     decoded = Dense(1024,name=\"decoder_3\")(decoded)\n#     decoded = Activation(\"relu\",name=\"relu_decoder_3\")(decoded)\n    \n    # Stage 8: Dense => ReLU\n    decoded = Dense(input_shape[0],name=\"decoder_4\")(decoded)\n    decoded = Activation(\"relu\",name=\"relu_decoder_4\")(decoded)\n    \n    # Decoder Model\n    decoder = Model(inputs=decoder_input,outputs=decoded)\n    \n    # Creating autoencoder model\n    # # Autoencoder input\n    autoencoder_input = Input(input_shape)\n    encoded_autoencoder = encoder(autoencoder_input)\n    decoded_autoencoder = decoder(encoded_autoencoder)\n    \n    autoencoder = Model(inputs=autoencoder_input,outputs=decoded_autoencoder)\n    \n    return autoencoder, encoder, decoder\n    pass","ad85542c":"autoencoder, encoder, decoder = dense_autoencoder(input_shape=(11520,))","ec3d9caf":"# autoencoder_plot = os.path.join(\"output\/autoencoder_plot.png\")\nplot_model(autoencoder,to_file=\"autoencoder_plot.png\",show_shapes=True,show_layer_names=True)\nSVG(model_to_dot(autoencoder).create(prog='dot',format='svg'))","f816f3a7":"autoencoder.summary()","270b9a1f":"# encoder_plot = os.path.join(config.BASE_CSV_PATH,\"encoder_plot.png\")\nplot_model(encoder,to_file = \"encoder_plot.png\",show_shapes = True,show_layer_names = True)\nSVG(model_to_dot(encoder).create(prog='dot',format='svg'))","cb511b85":"encoder.summary()","1aaf9432":"# decoder_plot = os.path.join(config.BASE_CSV_PATH,\"decoder_plot.png\")\nplot_model(encoder,to_file=\"decoder_plot.png\",show_shapes=True,show_layer_names=True)\nSVG(model_to_dot(decoder).create(prog='dot',format='svg'))","22a65aed":"decoder.summary()","76b6c57f":"opt = Adadelta(lr=1e-3)\nautoencoder.compile(optimizer=opt,loss=\"mse\")","076bf787":"logs = TensorBoard(\"logs\")\n# checkpoint = ModelCheckpoint(\"model_weights.h5\",monitor=\"val_accuracy\",\n#                             verbose=1,save_best_only=True,mode='max')","2df614b0":"def feature_generator(inputPath):\n    \n    df = pd.read_csv(inputPath,header=None)\n    \n    filenames = df.loc[:,0]\n    return np.array(filenames)\n    pass","6910d948":"def data_generator(inputPath, batchSize, mode=\"train\"):\n    \n    df = pd.read_csv(inputPath,header=None)\n    num_samples = df.shape[0]\n    \n    while True:\n        \n        for offset in range(0, num_samples, batchSize):\n            batchSamplesIdx = df.index[offset:offset+batchSize]\n            \n            X, y = [], []\n            \n            for i in batchSamplesIdx:\n            \n                feature = df.loc[i,2].split(\" \")\n                feature = np.array(feature, dtype=np.float32)\n                X.append(feature)\n                y.append(feature)\n                pass\n            \n            X = np.array(X, dtype=np.float32)\n            y = np.array(y, dtype=np.float32)\n            \n            yield X, y\n            pass\n        pass\n    pass","49d3ddad":"le = pickle.load(open(\"..\/input\/image-search-engine\/ImageSearch\/dataset\/label_map.pkl\",\"rb\"))\nle","23c3aa64":"numClasses = len(le.keys())\nnumClasses","a57d8960":"trainPath = \"..\/input\/testdata\/train.csv\"\nvalPath = \"..\/input\/testdata\/validation.csv\"","19d99636":"%%time\ntrain_filenames = feature_generator(trainPath)\nval_filenames = feature_generator(valPath)\n\ntrain_filenames.shape, val_filenames.shape","f4f5b0c3":"train_generator = data_generator(trainPath, 2)\nval_generator = data_generator(valPath, 2)","8819386e":"numEpochs = 100\nbatchSize = 19\n\ndense_history = autoencoder.fit(train_generator,\n                               steps_per_epoch = 1832\/\/batchSize,\n                               epochs = numEpochs,\n                               verbose = 1,\n                               validation_data = val_generator,\n                               validation_steps = 532\/\/batchSize,\n                               callbacks=[logs])","cb77110d":"def show_final_history(history):\n    \n    plt.style.use(\"ggplot\")\n\n    plt.plot(history.history['loss'],label='Train Loss')\n    plt.plot(history.history['val_loss'],label='Validation Loss')\n    \n#     ax[0].legend(loc='upper right')\n#     ax[1].legend(loc='lower right')\n    plt.show();\n    pass","5fe5ef6d":"show_final_history(dense_history)","15dd248f":"def feature_generator(inputPath):\n    \n    df = pd.read_csv(inputPath,header=None)\n    numSamples = df.shape[0]\n    \n    features = np.zeros(11520)\n    \n    for i in tqdm(range(0, numSamples, 1)):\n        \n        feature = np.array(df.loc[i,2].split(\" \"), dtype=np.float32)\n        features = np.vstack((features, feature))\n        pass\n    \n    return features\n    pass","0bc2749b":"X_train = feature_generator(trainPath)\nX_train.shape","159c68bc":"X_train = np.delete(X_train, 0, 0)\nX_train.shape","c33a6626":"val_train = feature_generator(valPath)\nval_train.shape","cebd2f06":"val_train = np.delete(val_train, 0, 0)\nval_train.shape","213668d2":"%%time\nX_train_encoded = encoder.predict(X_train)\nX_val_encoded = encoder.predict(val_train)\n\nX_train_encoded.shape, X_val_encoded.shape","08ae3f2f":"filenames = np.concatenate((train_filenames,val_filenames),axis=0)\nencoded_values = np.concatenate((X_train_encoded,X_val_encoded), axis=0)\n\nfilenames.shape, encoded_values.shape","73aca97e":"output_file = open(\"encoded_values_train_val.csv\",\"w\")\nfor (filename, vec) in zip(filenames, encoded_values):\n    vec = \",\".join([str(v) for v in vec])\n    output_file.write(\"{},{}\\n\".format(filename, vec))\n    pass\n\ndf_encoded = pd.read_csv(\".\/encoded_values_train_val.csv\",header=None)\ndf_encoded.head()","57e4a7b3":"encoder.save(\"encoder_model.h5\")\ndecoder.save(\"decoder_model.h5\")\nautoencoder.save(\"autoencoder_model.h5\")","513e19ae":"### Training the autoencoder model\n\nThe *autoencoder* model is trained using the above obtained datasets. It will currently be trained for 5 epochs with a batch_size of 16. This is done with computation cost in mind.","841e9f41":"### Compiling the model\n\nAdadelta with learning rate of 0.001 is used as an optimizer. *Binary crossentropy* is used for calculating the loss function while training.\n\nA learning rate of 0.001 is used to see if the model overfits or underfits. As the training progresses, the learning rate as well as the model structure will be finely tuned.\n\nTensorboard is used for stroring the accuracy and loss of the training and validation datasets. The files are stored in *output\/autoencoder\/dense\/logs*. ModelCheckpoint is also used for storing the best possible weights of the trained model. The file will be found at *output\/autoencoder\/dense*. ","6b4e6859":"## Fully Connected Autoencoder\n\nThis autoencoder consists of only dense layers. This is done as the features extracted from the CNN model were reshaped from *7\\*7\\*1024* feature vector to *1\\*50176* feature vector. \n\nThe fully connected layers are used when the data is present in 1-dimensional format. In the encoder model(compression) the number of neurons per layer decreases as the depth increases. With the decoder model(decompression) the number of neurons per layer increases as the depth increases.","9bed22bf":"batchSize = 19","ff9b6c98":"### Reading the data\n\nThe CSV files *train.csv* and *validation.csv* are read into the dataframes, *train_df* and *val_df* respectively.","130953f9":"# Autoencoders\n\n\"Autoencoding\" is a data compression algorithm where the compression and decrompression functions are,\n1. Data specific -> The autoencoder will only be able to compress data which it has been trained on. That is the compression and decompression accuracy will depend on the dataset.\n2. Lossy -> The decompressed outputs will be degraded with respect to the original ones. This happen as the model shooses the relevant features while compressing and then builds upon them during decompression.\n3. Learns automatically from the examples -> It will learn automatically from the dataset. Useful for specialised datasets. Learning depends upon the quality and quantity of the dataset.\n\nThe compression and decrompession functions can be implemented using either fully connected layers or fully convolutional layers.\n\nAutoecoders are capable of modelling complex non linear functions due to the presence of neurons within the model. These neurons are trained like any other neural network. PCA or ICA are not used as they look for liner relationships between features. For images, linear features may or may not exist such as edges.\n\nThe features obtained from the compression might have correlations since they are trained for accurate reconstruction.\n\nAutoencoders are prone to overfitting which can be litigated through regularization and careful design.\n\nAutoencoders consists of two parts, namely the encoder and the decoder. The input shape and output shape of the encoder and decoder respectively should match.","fb379fce":"### Creation of Model\n\nThe *autoencoder* consisting of the *encoder* and *decoder* models is defined in the function *dense_autoencoder*.\n\n**Encoder** model :-\n\nStage | Function\n----- | --------\nStage 1 | The input for the encoder model is defined with shape corresponding to length of one feature vector.\nStage 2 | Dense layer with 512 units and a ReLU activation layer. Number of dimensions of feature vector is reduced to 512 from 1024.\nStage 3 | Dense layer with 256 units and a ReLU activation layer. Number of dimensions of feature vector is reduced to 256 from 512.\nStage 4 | Dense layer with 64 units and a ReLU activation. Number of dimensions of feature vector is reduced to 64 from 256. The ouput of this stage is the output of the *encoder* model and input of *decoder* model.\n\nModel is defined as *encoder*.\n\n**Decoder** model :-\n\nStage | Function\n----- | --------\nStage 5 | Input for the decoder model. The input is the ouput of the encoder model with input shape of being the shape of the number of units in last layer of encoder model. Here it is 64.\nStage 6 | Dense layer with 256 units and a ReLU activation layer. Number of dimensions of feature vector increases to 256 from 64.\nStage 7 | Dense layer with 512 units and a ReLU activation layer. Number of dimensions of feature vector increases to 512 from 256.\nStage 8 | Dense layer with 1024 units and a ReLU activation layer. Number of dimensions of feature vector increases to 1024 from 512.\nStage 9 | Dense layer with original size of feature vector as number of units and a Sigmoid activation layer. Number of dimensions is back to the original value. The output of this layer is the output of both the *decoder* and *autoencoder* models."}}