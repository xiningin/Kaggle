{"cell_type":{"20196a52":"code","74ab0692":"code","d792da85":"code","507e2917":"code","37fbc25b":"code","fdd3ac47":"code","e651757a":"code","bcb2e1e9":"code","04befec9":"code","5c5a2492":"code","19fcd4df":"code","6905e7c7":"code","13d5f5fc":"code","dd908d95":"code","803d47cf":"code","a9fd9587":"code","f3a4f4cd":"code","a613f4d1":"code","7682806f":"code","00b4875f":"code","581c1c47":"code","160443c5":"code","cb5bf505":"code","c2e9c110":"code","f6231418":"code","52493c53":"code","1c42f70a":"code","d70082e9":"code","baa6d39e":"code","67d2b664":"code","078b0703":"code","4bb2376c":"code","12995e2c":"code","1f523805":"code","05093c47":"code","efefb899":"code","9cc9e7a4":"code","88174b44":"code","7e9b0f1c":"code","865bc9b9":"code","7ab74f4f":"code","34fac091":"code","13ae7d90":"code","aa1a893b":"code","35804705":"code","05337fd1":"code","13869b0e":"code","756649ed":"code","a8cb35bd":"code","f2762b80":"code","ee82e46f":"code","f86063a4":"code","0b24b474":"code","2377c8f7":"code","40174d51":"code","c409aa1d":"code","866ad902":"code","3ecf401c":"code","3329f01b":"code","ea5ed621":"markdown","28b73427":"markdown","3ba4dd7e":"markdown","ca975850":"markdown","da24815b":"markdown","6546cac7":"markdown","fc542873":"markdown","358ecceb":"markdown","d373fa42":"markdown","a3a48c6c":"markdown","507df144":"markdown","eb41dffd":"markdown","c5305a1c":"markdown","e8340180":"markdown","ad0080a9":"markdown","3dbb205f":"markdown","e8e86370":"markdown","14973d39":"markdown","660bbfd7":"markdown"},"source":{"20196a52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.gridspec as gridspec\nfrom scipy import stats\nimport matplotlib.style as style\nimport math\n\n%matplotlib inline\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import ensemble, tree, linear_model\nfrom scipy.stats import skew\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nimport missingno as msno\n\n#Model Train\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nfrom lightgbm import LGBMRegressor\n\n# Feature Selection Techniques\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","74ab0692":"os.listdir('\/kaggle\/input\/house-prices-advanced-regression-techniques')","d792da85":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","507e2917":"train.head()","37fbc25b":"train.shape","fdd3ac47":"test.shape","e651757a":"def column_unique(col_list):\n    for column_name in train.columns:\n        if train[column_name].nunique() < 35 and train[column_name].dtypes == 'int64':\n            unique_category = len(train[column_name].unique())\n            print(f'Feature {column_name} with dtype discrete has {unique_category} unique categories')\n        elif train[column_name].dtypes == 'object':\n            unique_category = len(train[column_name].unique())\n            print(f'Feature {column_name} with dtype object has {unique_category} unique categories')\n        else:\n            dtype = train[column_name].dtypes\n            print(f'Feature {column_name} is of dtype {dtype}')","bcb2e1e9":"cat_cols = list(train.select_dtypes('object').columns)\ndis_cols = [feature for feature in train.columns if train[feature].nunique() < 25 and train[feature].dtypes == 'int64' ]\nnum_cols = [feature for feature in train.columns if train[feature].nunique() > 25]","04befec9":"def missing_data(df):\n    total = df.isnull().sum()\n    percent = round(df.isnull().sum() \/ df.shape[0]* 100)\n    missing_info = pd.concat([total, percent], axis = 1, keys=['Total', 'Percent']).sort_values(by='Percent', ascending=False)\n    missing_info = missing_info[missing_info['Total'] > 0]\n    return missing_info","5c5a2492":"train_v1 = train.copy()","19fcd4df":"# train.drop(train[(train['OverallQual']<5) & (train['SalePrice']>200000)].index, inplace=True)\n# train.drop(train[(train['GrLivArea']>4500) & (train['SalePrice']<300000)].index, inplace=True)\n# train.reset_index(drop=True, inplace=True)","6905e7c7":"# #Uncomment when needed\n# #Normalize SalePrice\ntrain['SalePrice'] = np.log1p(train['SalePrice'])","13d5f5fc":"y = train['SalePrice'].reset_index(drop=True)\n## Remove Id and save target variable as y\ntrain = train.drop(['Id', 'SalePrice'], axis=1)\ntest = test.drop(['Id'], axis=1)\n\n","dd908d95":"train.shape","803d47cf":"test.shape","a9fd9587":"train_copy = train.copy()\ntest_copy = test.copy()","f3a4f4cd":"all_data_list  = [train_copy, test_copy]","a613f4d1":"all_data_list[0].shape","7682806f":"for dataset in all_data_list:\n    ## No Data Leakage ################################\n    for feature in ['MoSold', 'YrSold', 'MSSubClass']:\n        dataset[feature] = dataset[feature].apply(str)\n    # Assume typical unless deductions are warranted (from the data description)\n\n\n\n    ## Some missing values are intentionally left blank, for example: In the Alley feature \n    ## there are blank values meaning that there are no alley's in that specific house. \n    none_available = [ \"Alley\", \n                       \"PoolQC\", \n                       \"MiscFeature\",\n                       \"Fence\",\n                       \"FireplaceQu\",\n                       \"GarageType\",\n                       \"GarageFinish\",\n                       \"GarageQual\",\n                       \"GarageCond\",\n                       'BsmtQual',\n                       'BsmtCond',\n                       'BsmtExposure',\n                       'BsmtFinType1',\n                       'BsmtFinType2',\n                       'MasVnrType']\n\n    for feature in none_available:\n        dataset[feature] = dataset[feature].fillna('None')\n    none_available2 =  ['BsmtFinSF1',\n                        'BsmtFinSF2',\n                        'BsmtUnfSF',\n                        'TotalBsmtSF',\n                        'BsmtFullBath', \n                        'BsmtHalfBath', \n                        'GarageYrBlt',\n                        'GarageArea',\n                        'GarageCars',\n                        'MasVnrArea']\n\n    for feature in none_available2:\n        dataset[feature] = dataset[feature].fillna(0)\n\n    ### Possible Data Leakage ####################################\n\n    dataset['Functional'] = dataset['Functional'].fillna('Typ')\n\n    # Fillna with modes as these columns has very less missing data\n    mode_feats = list(missing_data(dataset[cat_cols])[missing_data(dataset[cat_cols])['Total'] <2].index)\n    for feature in mode_feats:\n        dataset[feature] = dataset[feature].fillna(dataset[feature].mode()[0])\n    dataset['MSZoning'] = dataset.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n    ## Replaced all missing values in LotFrontage by imputing the median value of each neighborhood. \n    dataset['LotFrontage'] = dataset.groupby('Neighborhood')['LotFrontage'].transform( lambda x: x.fillna(x.mean()))\n    dataset['Utilities'] = dataset['Utilities'].fillna(dataset['Utilities'].mode()[0])","00b4875f":"missing_data(train_copy)","581c1c47":"missing_data(test_copy)","160443c5":"train_copy.shape","cb5bf505":"test_copy.shape","c2e9c110":"for dataset in all_data_list:\n    ## Uncomment when needed\n    skew_cols = dataset.dtypes[dataset.dtypes != 'object'].index\n    skewness = dataset[skew_cols].apply(lambda x: skew(x)).sort_values(ascending =False)\n    skewness = skewness[skewness > 0.5]\n    high_skew = pd.DataFrame({'Skew' : skewness })\n    high_skew_cols = high_skew.index\n\n    # Normalize skewed features\n    for i in high_skew_cols:\n        dataset[i] = boxcox1p(dataset[i], boxcox_normmax(dataset[i] + 1))","f6231418":"#Creating More Features\nfor dataset in all_data_list:\n    dataset['BsmtFinType1_Unf'] = 1*(dataset['BsmtFinType1'] == 'Unf')\n    dataset['HasWoodDeck'] = (dataset['WoodDeckSF'] == 0) * 1\n    dataset['HasOpenPorch'] = (dataset['OpenPorchSF'] == 0) * 1\n    dataset['HasEnclosedPorch'] = (dataset['EnclosedPorch'] == 0) * 1\n    dataset['Has3SsnPorch'] = (dataset['3SsnPorch'] == 0) * 1\n    dataset['HasScreenPorch'] = (dataset['ScreenPorch'] == 0) * 1\n    dataset['YearsSinceRemodel'] = dataset['YrSold'].astype(int) - dataset['YearRemodAdd'].astype(int)\n    dataset['Total_Home_Quality'] = dataset['OverallQual'] + dataset['OverallCond']\n    dataset = dataset.drop(['Utilities', 'Street', 'PoolQC',], axis=1)\n    dataset['TotalSF'] = dataset['TotalBsmtSF'] + dataset['1stFlrSF'] + dataset['2ndFlrSF']\n    dataset['YrBltAndRemod'] = dataset['YearBuilt'] + dataset['YearRemodAdd']\n\n    dataset['Total_sqr_footage'] = (dataset['BsmtFinSF1'] + dataset['BsmtFinSF2'] +\n                                     dataset['1stFlrSF'] + dataset['2ndFlrSF'])\n    dataset['Total_Bathrooms'] = (dataset['FullBath'] + (0.5 * dataset['HalfBath']) +\n                                   dataset['BsmtFullBath'] + (0.5 * dataset['BsmtHalfBath']))\n    dataset['Total_porch_sf'] = (dataset['OpenPorchSF'] + dataset['3SsnPorch'] +\n                                  dataset['EnclosedPorch'] + dataset['ScreenPorch'] +\n                                  dataset['WoodDeckSF'])\n    dataset['TotalBsmtSF'] = dataset['TotalBsmtSF'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\n    dataset['2ndFlrSF'] = dataset['2ndFlrSF'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n    dataset['GarageArea'] = dataset['GarageArea'].apply(lambda x: np.exp(6) if x <= 0.0 else x)\n    dataset['GarageCars'] = dataset['GarageCars'].apply(lambda x: 0 if x <= 0.0 else x)\n    dataset['LotFrontage'] = dataset['LotFrontage'].apply(lambda x: np.exp(4.2) if x <= 0.0 else x)\n    dataset['MasVnrArea'] = dataset['MasVnrArea'].apply(lambda x: np.exp(4) if x <= 0.0 else x)\n    dataset['BsmtFinSF1'] = dataset['BsmtFinSF1'].apply(lambda x: np.exp(6.5) if x <= 0.0 else x)\n\n    dataset['haspool'] = dataset['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n    dataset['has2ndfloor'] = dataset['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n    dataset['hasgarage'] = dataset['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n    dataset['hasbsmt'] = dataset['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n    dataset['hasfireplace'] = dataset['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","52493c53":"train_copy.shape","1c42f70a":"test_copy.shape","d70082e9":"# def logs(res, ls):\n#     m = res.shape[1]\n#     for l in ls:\n#         res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n#         res.columns.values[m] = l + '_log'\n#         m += 1\n#     return res\n\n# log_features = ['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n#                  'TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea',\n#                  'BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr',\n#                  'TotRmsAbvGrd','Fireplaces','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF',\n#                  'EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','YearRemodAdd']\n\n# all_data = logs(all_data, log_features)","baa6d39e":"train_copy = pd.get_dummies(train_copy).reset_index(drop=True)\ntest_copy = pd.get_dummies(test_copy).reset_index(drop=True)","67d2b664":"train_copy.shape","078b0703":"test_copy.shape","4bb2376c":"\n# train_clean = all_data.iloc[:len(y), :]\n# test_clean = all_data.iloc[len(y):, :]\n# train_clean.shape, y.shape, test_clean.shape\n\n## Use train_copy and test_copy instead","12995e2c":"num_feats = 280","1f523805":"cor_cols = [x for x in num_cols+dis_cols if x not in ('Id', 'SalePrice')]","05093c47":"num_feats= 300\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nrfe_selector = RFE(estimator=LinearRegression(), n_features_to_select=num_feats, step=10, verbose=5)\nrfe_selector.fit(train_copy, y)\nrfe_support = rfe_selector.get_support()\nrfe_feature = train_copy.loc[:,rfe_support].columns.tolist()\nprint(str(len(rfe_feature)), 'selected features')","efefb899":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import Lasso, Ridge\n\nembeded_lr_selector = SelectFromModel(Ridge(), max_features=num_feats)\nembeded_lr_selector.fit(train_copy, y)\n\nembeded_lr_support = embeded_lr_selector.get_support()\nembeded_lr_feature = train_copy.loc[:,embeded_lr_support].columns.tolist()\nprint(str(len(embeded_lr_feature)), 'selected features')","9cc9e7a4":"embeded_lr_feature","88174b44":"X_train, X_test, y_train, y_test = train_test_split(train_copy[rfe_feature], y, train_size=0.75, shuffle=True, random_state=1)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","7e9b0f1c":"# Fit and Predict on X_test\nlr = LinearRegression()\nlr.fit(X_train, y_train)\ny_pred = lr.predict(X_test)\nprint (f' Train Score is {lr.score(X_train, y_train)}')\nprint (f' Test Score is {lr.score(X_test, y_test)}')\nmse = mean_squared_error(y_test, y_pred)\nprint (f' Mean squared error is {(mse)}')\nprint (format(mse, '.2f'))","865bc9b9":"# Train Score is 0.9061921419871923\n#  Test Score is 0.8117714739572311\n#  Mean squared error is 1263621018.643197","7ab74f4f":"coefficients = pd.concat([pd.DataFrame(X_train.columns),pd.DataFrame(np.transpose(lr.coef_))],keys=['feature', 'importance'],ignore_index=True, axis = 1)","34fac091":"coefficients.columns = ['feature', 'importance']","13ae7d90":"coefficients.sort_values(by='importance', ascending=False)","aa1a893b":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\nmodel = DecisionTreeRegressor()\nmodel.fit(X_train,y_train)\n# print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\ny_pred2 = model.predict(X_test)\nprint (f' Train Score is {model.score(X_train, y_train)}')\nprint (f' Test Score is {model.score(X_test, y_test)}')\nmse = mean_squared_error(y_test, y_pred2)\nprint (f' Mean squared error is {mse}')\n# feat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\n# feat_importances.nlargest(15).plot(kind='barh')\n# plt.show()","35804705":"from sklearn.ensemble import ExtraTreesRegressor\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesRegressor()\nmodel.fit(X_train,y_train)\n# print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(15).plot(kind='barh')\nplt.show()","05337fd1":"import time\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm","13869b0e":"start = time.process_time()\ntrainedforest = RandomForestRegressor(n_estimators=700).fit(X_train,y_train)\nprint(time.process_time() - start)\npredictionforest = trainedforest.predict(X_test)\nprint (f' Train Score is {trainedforest.score(X_train, y_train)}')\nprint (f' Test Score is {trainedforest.score(X_test, y_test)}')\nmse = mean_squared_error(y_test, predictionforest)\nprint (f' Mean squared error is {mse}')","756649ed":"alpha_ridge = [-3,-2,-1,1e-15, 1e-10, 1e-8,1e-5,1e-4, 1e-3,1e-2,0.5,1,1.5, 2,3,4, 5, 10, 20, 30, 40]","a8cb35bd":"from sklearn.linear_model import Lasso \ntemp_rss = {}\ntemp_mse = {}\nfor i in alpha_ridge:\n    ## Assigin each model. \n    lasso_reg = Lasso(alpha= i, normalize=True)\n    ## fit the model. \n    lasso_reg.fit(X_train, y_train)\n    ## Predicting the target value based on \"Test_x\"\n    y_pred = lasso_reg.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    rss = sum((y_pred-y_test)**2)\n    temp_mse[i] = mse\n    temp_rss[i] = rss","f2762b80":"for key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","ee82e46f":"lasso_reg = Lasso(alpha=0.0001 , normalize=True)\n## fit the model. \nlasso_reg.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\ny_pred = lasso_reg.predict(X_test)","f86063a4":"print (f' Train Score is {lasso_reg.score(X_train, y_train)}')\nprint (f' Test Score is {lasso_reg.score(X_test, y_test)}')\nmse = mean_squared_error(y_test, y_pred)\nprint (f' Mean squared error is {mse}')","0b24b474":"from sklearn.linear_model import Ridge \ntemp_rss = {}\ntemp_mse = {}\nfor i in alpha_ridge:\n    ## Assigin each model. \n    ridge_reg = Ridge(alpha= i, normalize=True)\n    ## fit the model. \n    ridge_reg.fit(X_train, y_train)\n    ## Predicting the target value based on \"Test_x\"\n    y_pred = ridge_reg.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    rss = sum((y_pred-y_test)**2)\n    temp_mse[i] = mse\n    temp_rss[i] = rss","2377c8f7":"for key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","40174d51":"ridge_reg = Ridge(alpha=0.5 , normalize=True)\n## fit the model. \nridge_reg.fit(X_train, y_train)\n## Predicting the target value based on \"Test_x\"\ny_pred = ridge_reg.predict(X_test)","c409aa1d":"print (f' Train Score is {ridge_reg.score(X_train, y_train)}')\nprint (f' Test Score is {ridge_reg.score(X_test, y_test)}')\nmse = mean_squared_error(y_test, y_pred)\nprint (f' Mean squared error is {mse}')","866ad902":"coefficients = pd.concat([pd.DataFrame(X_train.columns),pd.DataFrame(np.transpose(ridge_reg.coef_))],ignore_index=True, axis = 1)","3ecf401c":"coefficients.columns = ['feature', 'importance']","3329f01b":"coefficients.sort_values(by='importance', ascending = False)","ea5ed621":"## Ridge Regression\nL2 Regularization","28b73427":"Test Codes\n","3ba4dd7e":"## Create dtype lists","ca975850":"# \ud83d\udcd6 Read Files ","da24815b":"## Linear Regression Model","6546cac7":"## Creating Dummy Variables","fc542873":"# D. Modeling","358ecceb":"# C. Feature Engineering - Creating New Features","d373fa42":"## Fixing Skewness","a3a48c6c":"# \ud83d\udcd0 B. Feature Engineering","507df144":"## Ridge Regression\nL1 Regularization","eb41dffd":"## Value counts for discrete and categorical features","c5305a1c":"## Missing Data Assessment","e8340180":"* Train Score is 0.9539328929511139\n* Test Score is 0.891812312528051\n* Mean squared error is 0.01813198817893803","ad0080a9":"# **\ud83d\udcca A.Exploratory Data Analysis**","3dbb205f":"## Todos\n* Fix Data Leakage\n* Try blending models","e8e86370":"# Handling Missing Values\nWill be using all_data from here on","14973d39":"# Trying some nifty Feature Selection Techniques","660bbfd7":"There you go, SalePrice looking off the charts. (Pun Intended)"}}