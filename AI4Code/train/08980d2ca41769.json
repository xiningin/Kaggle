{"cell_type":{"7f1dc42b":"code","8d209eb2":"code","1d63c2f9":"code","339470f6":"code","baeda165":"code","cefd8bd6":"code","59b50922":"code","ae2ea539":"code","722d0574":"code","d5a7dfe6":"code","b282975c":"code","e4d323f1":"code","9f6feca8":"code","9dac9fdf":"code","e9c279eb":"code","fac37e53":"code","703bb4fa":"code","4bce9d34":"code","fa74257b":"code","02316212":"code","4d462d5e":"code","97475d33":"code","d28394a1":"code","824439db":"code","c789d5b9":"code","8dae0283":"code","a7157188":"code","137c9e71":"code","f88f2a11":"code","192d53f7":"code","3159b620":"code","ec88e7a9":"code","92602384":"code","66b52cd5":"code","6da02b2d":"code","e32afa89":"code","6cca6829":"code","6ff27cc8":"code","57db707f":"code","635f2766":"code","14defdde":"code","86694bf4":"code","c69c9ff8":"code","caf97c88":"code","f51b9380":"code","26fc76f3":"code","17ec7913":"code","c3d0a696":"code","6111f926":"code","680049c9":"code","fe9372d2":"code","71eeba10":"code","327dc45d":"code","984a3b51":"code","67b5e374":"code","6042a644":"code","56181eeb":"code","b824f548":"code","ff28d755":"code","e1dde72a":"code","ff26bf3f":"code","85972618":"code","01f533c6":"code","71c35293":"code","612ce326":"code","2d89fca0":"markdown","fe2634f2":"markdown","a7d2cdfc":"markdown","34d93c39":"markdown","639466ca":"markdown","bb4211a3":"markdown","a967c68e":"markdown","b7eccfa5":"markdown","498552ba":"markdown","06e5a138":"markdown","32d9c29e":"markdown","27eda48c":"markdown","0a2a12d9":"markdown","74abe2ec":"markdown","7e8d6576":"markdown","872ced88":"markdown","4eb37791":"markdown","70b66748":"markdown","6fad34db":"markdown","2f47ca87":"markdown","73241e6e":"markdown"},"source":{"7f1dc42b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport string","8d209eb2":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","1d63c2f9":"train","339470f6":"test","baeda165":"train.isnull().sum()","cefd8bd6":"test.isnull().sum()","59b50922":"train\ntrain['text'].str.startswith('@').value_counts()","ae2ea539":"train","722d0574":"train['keyword'].unique()","d5a7dfe6":"colors = sns.color_palette('viridis')\n\nplt.pie(train['target'].value_counts(), labels=['Non disaster', 'Disaster'], autopct='%.0f%%', colors = colors)\nplt.show()","b282975c":"plt.figure(figsize=(15,9))\nsns.set(font_scale = 1)\nplt.title('Number of occurancies of 30 First keywords')\nax = sns.barplot(x=train[\"keyword\"].value_counts()[:30].index, y=train[\"keyword\"].value_counts()[:30], data=train)\nax.tick_params(axis='x', rotation=90)","e4d323f1":"plt.figure(figsize=(15,9))\nbx = sns.barplot(x=train['location'].value_counts()[:30].index,y=train['location'].value_counts()[:30])\nbx.tick_params(axis='x', rotation=90)\n\nplt.title('30 Top locations')\nplt.show()","9f6feca8":"disaster = train.loc[train[\"target\"] == 1][\"keyword\"].value_counts()\nnondisaster = train.loc[train[\"target\"] == 0][\"keyword\"].value_counts()\n\nplt.figure(figsize=(15,9))\ncx = sns.barplot(x=disaster[:30].index, y=disaster[:30], palette = 'cubehelix')\ncx.tick_params(axis='x', rotation=90)\n\nplt.figure(figsize=(15,9))\ndx = sns.barplot(x=nondisaster[:30].index, y=nondisaster[:30], palette = 'magma')\ndx.tick_params(axis='x', rotation=90)\n","9dac9fdf":"train.fillna('', inplace=True)\ntest.fillna('', inplace=True)","e9c279eb":"train['text'] = train['text'].apply(lambda x: x.lower())\ntest['text'] = test['text'].apply(lambda x: x.lower())\n\ntrain['location'] = train['location'].apply(lambda x: x.lower())\ntest['location'] = test['location'].apply(lambda x: x.lower())","fac37e53":"train['text']=train['text'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\ntest['text']=test['text'].apply(lambda x: re.sub('\\w*\\d\\w*','', x))\n\ntrain['location']=train['location'].apply(lambda x: re.sub('\\w*\\d\\w*','', str(x)))\ntest['location']=test['location'].apply(lambda x: re.sub('\\w*\\d\\w*','', str(x)))","703bb4fa":"train['text']=train['text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\ntest['text']=test['text'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', x))\n\ntrain['location']=train['location'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', str(x)))\ntest['location']=test['location'].apply(lambda x: re.sub('[%s]' % re.escape(string.punctuation), '', str(x)))","4bce9d34":"train['text'] = train['text'].str.replace('http\\S+|www.\\S+', '', case=False)\ntest['text'] = test['text'].str.replace('http\\S+|www.\\S+', '', case=False)\n\ntrain['text'] = train['text'].str.replace('@\\S+', '', case=False)\ntest['text'] = test['text'].str.replace('@\\S+', '', case=False)","fa74257b":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","02316212":"def create_soup(x):\n    return x['location'] + ' ' + x['text']","4d462d5e":"df1 = train\nfeatures=['location','text']\ndf1 = df1[features]","97475d33":"df2 = test\ndf2 = df2[features]","d28394a1":"df1['soup'] = df1.apply(create_soup, axis=1)","824439db":"df2['soup'] = df2.apply(create_soup, axis=1)","c789d5b9":"y = train['target']\n\nX = df1['soup']\n#X = train['text']","8dae0283":"#X_te = test['text']\nX_te = df2['soup']","a7157188":"y","137c9e71":"X","f88f2a11":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","192d53f7":"print(len(X_train))","3159b620":"tfidf =  TfidfVectorizer(stop_words='english')\nX_train = tfidf.fit_transform(X_train)\nX_test = tfidf.transform(X_test)","ec88e7a9":"X_te = tfidf.transform(X_te)","92602384":"print(X_train[0])","66b52cd5":"\"\"\"\"countv = CountVectorizer(stop_words='english')\nX_train = countv.fit_transform(X_train)\nX_test = countv.transform(X_test)\"\"\"","6da02b2d":"\"\"\"\"X_te = countv.transform(X_te)\"\"\"","e32afa89":"print(X_train)","6cca6829":"from sklearn.naive_bayes import MultinomialNB\n\nnb = MultinomialNB()\nnb.fit(X_train, y_train)\n\n","6ff27cc8":"preds = nb.predict(X_test)","57db707f":"from sklearn.metrics import f1_score\n\nf1_score(y_test, preds, average='micro')\n#model.score(X_test, y_test)","635f2766":"final_preds = nb.predict(X_te)","14defdde":"submission_file1=pd.DataFrame({'id':test['id'],'target':final_preds})\nsubmission_file1","86694bf4":"submission_file1.to_csv('submission_1.csv', index = False)","c69c9ff8":"from sklearn import svm\n\nsvc = svm.SVC(kernel='linear') \nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)","caf97c88":"from sklearn import metrics\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n#f1_score(y_test, y_pred, average='micro')\n","f51b9380":"final_preds = svc.predict(X_te)","26fc76f3":"final_preds","17ec7913":"X_train","c3d0a696":"import keras\nfrom keras import optimizers\nimport tensorflow as tf \nfrom keras.models import Sequential\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Embedding, LSTM, Dropout","6111f926":"tokenizer = Tokenizer(num_words=3000, split=' ') \ntokenizer.fit_on_texts(train['text'].values)\nX = tokenizer.texts_to_sequences(train['text'].values)\n#X = pad_sequences(X)","680049c9":"tokenizer = Tokenizer(num_words=3000, split=' ') \ntokenizer.fit_on_texts(test['text'].values)\nX_t = tokenizer.texts_to_sequences(test['text'].values)\n#X_t = pad_sequences(X_t)","fe9372d2":"test","71eeba10":"from keras import preprocessing\nfrom keras.preprocessing import sequence\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.embeddings import Embedding\n\nmax_length = 600\ntop_words = 3000\nX_train = sequence.pad_sequences(X_train, maxlen=max_length)\nX_test = sequence.pad_sequences(X_test, maxlen=max_length)\n# create the model\nembedding_vecor_length = 64\nmodel = Sequential()\nmodel.add(Embedding(top_words, embedding_vecor_length, input_length=max_length))\nmodel.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(LSTM(64, activation='tanh', dropout=0.2, recurrent_dropout=0.4))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adamax', metrics=['accuracy'])\nprint(model.summary())\n","327dc45d":"history = model.fit(X_train, y_train, epochs=10, batch_size=128,validation_data=(X_test, y_test))\n\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: \" % (scores[1]*100))","984a3b51":"plt.plot(history.history['val_loss'], label='val')\nplt.plot(history.history['loss'], label='train')\nplt.xlabel('Epochs:')\nplt.ylabel('Loss:')\nplt.show()","67b5e374":"plt.plot(history.history['val_accuracy'], label='val')\nplt.plot(history.history['accuracy'], label='train')\nplt.xlabel('Epochs:')\nplt.ylabel('Accuracy:')\nplt.show()","6042a644":"X_t = sequence.pad_sequences(X_t, maxlen=max_length)\npred_test = model.predict(X_t)","56181eeb":"pred_test","b824f548":"pred_test = (pred_test>0.5)*1","ff28d755":"pred_test\n","e1dde72a":"pred_test.shape","ff26bf3f":"pred_test = pred_test.flatten()","85972618":"submission_file=pd.DataFrame({'id':test['id'],'target':pred_test})\nsubmission_file","01f533c6":"submission_file.to_csv('submission_1.csv', index = False)","71c35293":"p = pd.read_csv('submission_1.csv')","612ce326":"p","2d89fca0":"Split train dataframe in train and validation set with observations and labels (80-20%)","fe2634f2":"Tfidf vectorizer used on train and validation sets (used for ml models)","a7d2cdfc":"Check how many NaN values there are ","34d93c39":"Check tags (words starting with @)","639466ca":"Write predictions to submission file","bb4211a3":"To run LSTM model, some of the above preprocessing of the ml models should not run e.g the vectorizers.","a967c68e":"Just see unique keywords","b7eccfa5":"Linear Support Vector classifier","498552ba":"*In this notebook there is some basic text preprocessing and data visualization, followed by machine learning algortihms and deep learning (LSTM Network)*","06e5a138":"TEXT PREPROCESSING (lower, remove digits and specific chars, remove urls)","32d9c29e":"Alternatively, use count vectorizer to trasnform text ","27eda48c":"LSTM model with a layer of 1d convolution","0a2a12d9":"Fill NaN values with empty strings","74abe2ec":"fit train data into the LSTM","7e8d6576":"Multinomial Naive Bayes","872ced88":"Just checking the train and test dataframes","4eb37791":"Round the results to 0 or 1","70b66748":"Although LSTM seems to be doing ok during the training, on train and validation data, it never does well on the test data. Sometimes though it seems to overfit.","6fad34db":"Method to tokenize text for lstm ","2f47ca87":"Creating a new column 'soup', in which text and location is stored for more information in the final model","73241e6e":"Visualizations"}}