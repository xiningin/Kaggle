{"cell_type":{"4cbd3b0e":"code","66867cdc":"code","82fb6c0d":"code","b29a92e5":"code","daf86efe":"code","f53205af":"code","6ddb405c":"code","e6d7d1a0":"code","4ae44785":"code","f2de1a8c":"code","c1b8b89d":"code","8b80bdb0":"code","b25ff2b6":"markdown","9a0c5d0e":"markdown","d729e0d3":"markdown","c9d21d0c":"markdown","5c129777":"markdown","bc61c4c7":"markdown","44df5d5a":"markdown","3c29c7d6":"markdown"},"source":{"4cbd3b0e":"import os\nimport random\nimport numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import KFold","66867cdc":"SEED = 9\ndef all_seeds(s):\n    random.seed(s)\n    os.environ['PYTHONHASHSEED'] = str(s)\n    np.random.seed(s)\n    tf.random.set_seed(s)\n    \nall_seeds(SEED)","82fb6c0d":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b29a92e5":"test_df = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrain_df = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntr_target_df = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\nsample_df = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\ntarget_cols = tr_target_df.columns[1:]\nN_TARGETS = len(target_cols)","daf86efe":"def preprocess_df(df):\n    df['cp_type'] = (df['cp_type'] == 'trt_cp').astype(int)\n    df['cp_dose'] = (df['cp_dose'] == 'D2').astype(int)\n    return df\n\nx_train = preprocess_df(train_df.drop(columns=\"sig_id\"))\ny_train = tr_target_df.drop(columns=\"sig_id\")\nx_test = preprocess_df(test_df.drop(columns=\"sig_id\"))\n\nN_FEATURES = x_train.shape[1]","f53205af":"print(\"Number of Features:\",N_FEATURES)\nprint(\"Number of Targets:\",N_TARGETS)\nprint(\"x_train shape:\",x_train.shape)\nprint(\"y_train shape:\",y_train.shape)\nprint(\"x_test shape:\",x_test.shape)","6ddb405c":"EPOCHS = 64\nBATCH_SIZE = 128\nFOLDS = 5\nREPEATS = 5\nLR = 0.0008\nN_TARGETS = len(target_cols)\n\ndef multi_log_loss(y_true, y_pred):\n    losses = []\n    for col in y_true.columns:\n        losses.append(log_loss(y_true.loc[:, col], y_pred.loc[:, col]))\n    return np.mean(losses)","e6d7d1a0":"def my_model():\n    model = tf.keras.Sequential([\n        \n                tf.keras.layers.Input(N_FEATURES), \n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Dropout(0.1),\n        \n                tf.keras.layers.Dense(3500, activation=\"relu\"),\n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Dropout(0.4),\n        \n                tf.keras.layers.Dense(1750, activation=\"relu\"),\n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Dropout(0.2),\n                \n                tf.keras.layers.Dense(875, activation=\"relu\"),\n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Dropout(0.1),\n                \n                tf.keras.layers.Dense(412, activation=\"relu\"),\n                tf.keras.layers.BatchNormalization(),\n                tf.keras.layers.Dropout(0.2),\n        \n                tf.keras.layers.Dense(N_TARGETS, activation=\"sigmoid\") \n            ])\n    \n    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = LR),\n                  loss = 'binary_crossentropy', \n                  metrics = [\"accuracy\"])\n    return model","4ae44785":"def train(resume_models = None, repeat_number = 0, folds = 5, skip_folds = 0):\n    \n    models = []\n    oof_pred = y_train.copy()  \n\n    kfold = KFold(folds, shuffle = True, random_state = 9)\n    for fold, (i_tr, i_va) in enumerate(kfold.split(x_train)):\n\n        print('-'*85)\n        print(f'Repeat {repeat_number}, Fold {fold}')\n        \n        cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', \n                                                              factor = 0.4, \n                                                              patience = 1, \n                                                              verbose = 2, \n                                                              min_delta = 0.0001, \n                                                              mode = 'auto')\n        \n        cb_early_stop = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss', \n                                                         min_delta = 0, \n                                                         patience = 4, \n                                                         verbose = 1, \n                                                         mode = 'min')\n        \n        checkpt_path = f'repeat:{repeat_number}_Fold:{fold}.hdf5'\n        cb_checkpt = tf.keras.callbacks.ModelCheckpoint(checkpt_path, \n                                                        monitor = 'val_loss', \n                                                        verbose = 2, \n                                                        save_best_only = True, \n                                                        save_weights_only = True, \n                                                        mode = 'min')\n\n        model = my_model()\n        model.fit(x_train.values[i_tr], y_train.values[i_tr],\n                  validation_data = (x_train.values[i_va], y_train.values[i_va]),\n                  callbacks = [cb_lr_schedule, cb_early_stop, cb_checkpt],\n                  epochs = EPOCHS, \n                  batch_size = BATCH_SIZE, \n                  verbose = 2)\n        \n        model.load_weights(checkpt_path)\n        oof_pred.loc[i_va, :] = model.predict(x_train.values[i_va])\n        models.append(model)\n\n    return models, oof_pred","f2de1a8c":"models = []\noof_pred = []\n\nfor i in range(REPEATS):\n    m, oof = train(repeat_number = i, folds = FOLDS)\n    print('-'*85)\n    models = models + m\n    oof_pred.append(oof)","c1b8b89d":"mean_oof_pred = y_train.copy()\nmean_oof_pred.loc[:, target_cols] = 0\nfor i, p in enumerate(oof_pred):\n    print(f\"Repeat {i} OOF Log Loss: {multi_log_loss(y_train, p)}\")\n    mean_oof_pred.loc[:, target_cols] += p[target_cols]\n\nmean_oof_pred.loc[:, target_cols] \/= len(oof_pred)\nprint(f\"Mean OOF Log Loss: {multi_log_loss(y_train, mean_oof_pred)}\")\nmean_oof_pred.loc[x_train['cp_type'] == 0, target_cols] = 0\nprint(f\"Mean OOF Log Loss (ctl adjusted): {multi_log_loss(y_train, mean_oof_pred)}\")","8b80bdb0":"test_pred = sample_df.copy()\ntest_pred[target_cols] = 0\n\nfor model in models:\n    test_pred.loc[:,target_cols] += model.predict(x_test)\ntest_pred.loc[:,target_cols] \/= len(models)\ntest_pred.loc[x_test['cp_type'] == 0, target_cols] = 0\ntest_pred.to_csv('submission.csv', index=False)","b25ff2b6":"# 6. Mean OOF Log Loss","9a0c5d0e":"Our aim is to predict multiple targets of the Mechanism of Action (MoA) response(s) of different samples (`sig_id`), given various inputs such as gene expression data and cell viability data.","d729e0d3":"# 2. Load Data","c9d21d0c":"# 5. Training","5c129777":"# 4. Building Model","bc61c4c7":"# 1. Import Packages","44df5d5a":"# 3. Preprocess Data","3c29c7d6":"# 7. Predictions and Submission File"}}