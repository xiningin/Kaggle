{"cell_type":{"24616c0a":"code","d81046e3":"code","c49795c1":"code","5b377f74":"code","af9aa6f1":"code","74efc14a":"code","8e00d93b":"code","69b192f6":"code","3f134e4d":"code","003f5543":"code","5ac7c338":"code","430d1b29":"code","85e700ed":"code","061f13e1":"code","f3ccc107":"code","4a6e1c50":"code","06657704":"code","837426d2":"code","e37a5823":"code","e8d0c08b":"markdown","bfe64ffa":"markdown","ac1f2bcf":"markdown","86f0eab6":"markdown","4ff7c47b":"markdown","ef12fbfe":"markdown","9edb51b8":"markdown","68a00a15":"markdown","40603737":"markdown","ed297143":"markdown"},"source":{"24616c0a":"import torch.nn as nn\nfrom transformers import AdamW\nfrom torch.utils.data import Dataset\nimport pandas as pd\nimport torch\nfrom transformers import BertConfig, BertForSequenceClassification\nfrom transformers import BertTokenizer\nfrom torch.utils.data import DataLoader","d81046e3":"# \u8d85\u53c2\u6570\npara_list = {\n    'hidden_dropout_prob' : 0.3,\n    'num_labels' : 3,\n    'learning_rate' : 1e-5,\n    'weight_decay' : 1e-2,\n    'epochs' : 2,\n    'batch_size' : 16,\n    'max_len' : 100\n}\n\n# \u6587\u4ef6\u8def\u5f84\ndata_path = \"\/kaggle\/input\/\"\ntrain_path = data_path + \"train-balanced-data\/train_balanced_data.csv\"    # \u8bad\u7ec3\u6570\u636e\u96c6(\u5e73\u8861\u5904\u7406\u540e)\ntest_path  = data_path + \"testdataprocessed\/test_data_processed.csv\"      # \u6d4b\u8bd5\u6570\u636e\u96c6\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","c49795c1":"# \u5b9a\u4e49 Dataset\nclass SentimentDataset(Dataset):\n    def __init__(self, path_to_file, mode):\n        self.mode = mode\n        if (mode == \"train\"):\n            self.dataset = pd.read_csv(path_to_file).loc[:, ['text', 'class']]\n        else:\n            self.dataset = pd.read_csv(path_to_file).loc[:, ['text']]\n    def __len__(self):\n        return len(self.dataset)\n    def __getitem__(self, idx):\n        # \u6839\u636e idx \u5206\u522b\u627e\u5230 text \u548c label\n        if (self.mode == \"train\"):\n            texts = self.dataset.loc[idx, \"text\"]\n            classes = self.dataset.loc[idx, \"class\"]\n            sample = {\"text\": texts, \"class\": classes}\n        else:\n            texts = self.dataset.loc[idx, \"text\"]\n            sample = {\"text\": texts}\n        # \u8fd4\u56de\u4e00\u4e2a dict\n        return sample","5b377f74":"from torch.utils.data import TensorDataset, random_split\n\n# \u52a0\u8f7d\u3001\u5207\u5206 \u8bad\u7ec3\u96c6\u3001\u9a8c\u8bc1\u96c6\nsentiment_train_set = SentimentDataset(train_path, \"train\")    # {0: 2000,1: 2000,2: 9620}\n\n# 80% dataset \u4e3atrain_dataset, 20% dataset \u4e3aval_dataset.\ntrain_size = int(0.8 * len(sentiment_train_set))\nval_size = len(sentiment_train_set) - train_size\ntrain_dataset, val_dataset = random_split(sentiment_train_set, [train_size, val_size])\nsentiment_train_loader = DataLoader(train_dataset, batch_size=para_list['batch_size'], shuffle=True, num_workers=0)\n\n# \u52a0\u8f7d\u9a8c\u8bc1\u96c6\nsentiment_valid_loader = DataLoader(val_dataset, batch_size=para_list['batch_size'], shuffle=False, num_workers=0)\nlen(train_dataset), len(val_dataset)","af9aa6f1":"# \u5b9a\u4e49 tokenizer\uff0c\u7528'bert-base-chinese'\u9884\u8bad\u7ec3\uff0cdo_lower_case \u5168\u90e8\u5c0f\u5199\ntokenizer = BertTokenizer.from_pretrained('bert-base-chinese', do_lower_case=True)","74efc14a":"# \u52a0\u8f7d\u6a21\u578b\nfrom transformers import BertForSequenceClassification, AdamW, BertConfig\n\nmodel = BertForSequenceClassification.from_pretrained(\n    \"bert-base-chinese\",            # \u4f7f\u7528 12-layer \u7684 BERT \u6a21\u578b.\n    num_labels = para_list['num_labels'],                 # \u591a\u5206\u7c7b\u4efb\u52a1\u7684\u8f93\u51fa\u6807\u7b7e\u4e3a 3\u4e2a.                     \n    output_attentions = False,      # \u4e0d\u8fd4\u56de attentions weights.\n    output_hidden_states = False,   # \u4e0d\u8fd4\u56de all hidden-states.\n)\n# config = BertConfig.from_pretrained(\"bert-base-uncase\", num_labels=num_labels, hidden_dropout_prob=hidden_dropout_prob)\n# model = BertForSequenceClassification.from_pretrained(\"bert-base-uncase\", config=config)\nmodel.to(device)","8e00d93b":"# no_decay = ['bias', 'LayerNorm.weight']\n# optimizer_grouped_parameters = [\n#         {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': para_list['weight_decay']},\n#         {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n# ]\n\noptimizer = AdamW(model.parameters(), lr=para_list['learning_rate'])\n# optimizer = AdamW(optimizer_grouped_parameters, lr=para_list['learning_rate'])\ncriterion = nn.CrossEntropyLoss()","69b192f6":"from tqdm import tqdm\n# \u8bad\u7ec3\ndef train(model, dataloader, optimizer, criterion, device):\n    model.train()    \n    train_loss_alter_list = []\n    valid_loss_alter_list = []\n    epoch_loss = 0\n    epoch_acc = 0\n    for i, batch in tqdm(enumerate(dataloader)):\n        # \u6807\u7b7e\u5f62\u72b6\u4e3a (batch_size, 1)\n        label = batch[\"class\"].cuda()\n        text = batch[\"text\"]\n\n        # tokenized_text \u5305\u62ec input_ids\uff0c token_type_ids\uff0c attention_mask\n        tokenized_text = tokenizer(text, max_length=para_list['max_len'], add_special_tokens=True, truncation=True, padding=True, return_tensors=\"pt\")\n        tokenized_text = tokenized_text.to(device)\n        # \u68af\u5ea6\u6e05\u96f6\n        optimizer.zero_grad()\n\n        #output: (loss), logits, (hidden_states), (attentions)\n        output = model(**tokenized_text, labels=label)\n\n        # y_pred_prob = logits : [batch_size, num_labels]\n        y_pred_prob = output[1]\n        y_pred_label = y_pred_prob.argmax(dim=1)\n\n        # \u8ba1\u7b97loss\n        loss = criterion(y_pred_prob.view(-1, 3), label.view(-1))\n\n        # \u8ba1\u7b97acc\n        acc = ((y_pred_label == label.view(-1)).sum()).item()\n\n        # \u53cd\u5411\u4f20\u64ad\n        loss.backward()\n        optimizer.step()\n\n        # epoch \u4e2d\u7684 loss \u548c acc \u7d2f\u52a0\n        epoch_loss += loss.item()\n        epoch_acc += acc\n        \n        # \u6253\u5370\u4fe1\u606f\n        if i % 100 == 0:\n            valid_loss, valid_acc = evaluate(model, sentiment_valid_loader, criterion, device)\n            print(\"current loss: {:.5f}  current acc: {:.5f}  valid loss: {:.5f}  valid acc: {:.5f}\".format(epoch_loss\/(i+1),epoch_acc \/ ((i+1)*len(label)),valid_loss,valid_acc))  \n            \n        # \u8bb0\u5f55loss\u7684\u53d8\u5316\n        train_loss_alter_list.append(epoch_loss\/(i+1))\n        valid_loss_alter_list.append(valid_loss)\n\n    # len(dataloader) \u8868\u793a\u6709\u591a\u5c11\u4e2a batch\uff0clen(dataloader.dataset) \u8868\u793a\u6837\u672c\u6570\u91cf\n    return epoch_loss \/ len(dataloader), epoch_acc \/ len(dataloader.dataset), train_loss_alter_list, valid_loss_alter_list","3f134e4d":"# \u9a8c\u8bc1\ndef evaluate(model, iterator, criterion, device):\n    model.eval()\n    epoch_loss = 0\n    epoch_acc = 0\n    with torch.no_grad():\n        for _, batch in enumerate(iterator):\n            label = batch[\"class\"].cuda()\n            text = batch[\"text\"]\n            tokenized_text = tokenizer(text, max_length=para_list['max_len'], add_special_tokens=True, truncation=True, padding=True,\n                                       return_tensors=\"pt\")\n            tokenized_text = tokenized_text.to(device)\n\n            output = model(**tokenized_text, labels=label)\n            \n            y_pred_prob = output[1]\n            y_pred_label = y_pred_prob.argmax(dim=1)\n            \n            loss = criterion(y_pred_prob.view(-1, 3), label.view(-1))\n            acc = ((y_pred_label == label.view(-1)).sum()).item()\n            \n            epoch_loss += loss.item()\n            epoch_acc += acc\n\n    # len(dataloader) \u8868\u793a\u6709\u591a\u5c11\u4e2a batch\uff0clen(dataloader.dataset) \u8868\u793a\u6837\u672c\u6570\u91cf\n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator.dataset)","003f5543":"import random\nimport numpy as np\n# \u8bbe\u7f6e\u968f\u673a\u79cd\u5b50\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\nloss_train = []\nloss_valid = []\n\n# \u5f00\u59cb\u8bad\u7ec3\u548c\u9a8c\u8bc1\nfor i in range(para_list['epochs']):\n    train_loss, train_acc, train_loss_alter_list, valid_loss_alter_list = train(model, sentiment_train_loader, optimizer, criterion, device)\n    loss_train += train_loss_alter_list\n    loss_valid += valid_loss_alter_list\n    valid_loss, valid_acc = evaluate(model, sentiment_valid_loader, criterion, device)\n    print(\"train loss:   {:.5f}  train acc:   {:.5f}  valid loss: {:.5f}  valid acc: {:.5f}\".format(train_loss,train_acc,valid_loss,valid_acc))","5ac7c338":"# \u7ed8\u5236loss\u53d8\u5316\u66f2\u7ebf\nimport matplotlib.pyplot as plt\n\nplt.plot(loss_train, 'b-', loss_valid, 'r-')\nplt.ylabel('loss')\nplt.show()","430d1b29":"# \u4fdd\u5b58\u6a21\u578b\ntorch.save(model, '\/kaggle\/working\/bertforsequence_model.h5')","85e700ed":"# \u52a0\u8f7d\u6a21\u578b\n# model = torch.load('\/kaggle\/working\/bertforsequence_model.h5')\n# model.eval()","061f13e1":"# \u52a0\u8f7d\u6d4b\u8bd5\u96c6\ntest_dataset = SentimentDataset(test_path, \"test\")\nsentiment_test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\nlen(sentiment_test_loader)","f3ccc107":"# test\nmodel.eval()\nsentiment_result = []\nfor _, batch in tqdm(enumerate(sentiment_test_loader)):\n    text = batch[\"text\"]\n    tokenized_text = tokenizer(text, max_length=100, add_special_tokens=True, truncation=True, padding=True,\n                               return_tensors=\"pt\")\n    tokenized_text = tokenized_text.to(device)\n\n    output = model(**tokenized_text, labels=None)\n\n    y_pred_prob = output[0]\n    y_pred_label = y_pred_prob.argmax(dim=1)\n    sentiment_result.append(y_pred_label)","4a6e1c50":"# \u8c03\u6574\u683c\u5f0f\nSA_result = []\nfor sent in sentiment_result:\n    SA_result.append(int(sent[0]))\nSA_result","06657704":"# \u7edf\u8ba1\u5404\u5206\u7c7b\u4e2a\u6570\ndef count(classes_list):\n    neg = pos = neu = 0\n    for clas in classes_list:\n        if clas == 0:\n            neg = neg+1\n        if clas == 1:\n            pos = pos+1\n        if clas == 2:\n            neu = neu+1\n    print(\"negative: {}, positive: {}, neutral: {}\".format(neg, pos, neu))\ncount(SA_result)","837426d2":"highest_BIO = pd.read_csv('\/kaggle\/input\/submit\/highest_BIO.csv')\ncmp_sa = list(highest_BIO['class'])\ncount(cmp_sa)","e37a5823":"highest_BIO['class'] = SA_result\nhighest_BIO.to_csv('\/kaggle\/working\/BertforSequenceClassification2.csv', index = None)","e8d0c08b":"## \u4fdd\u5b58\u6a21\u578b","bfe64ffa":"## \u5b9a\u4e49\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u51fd\u6570","ac1f2bcf":"## \u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668","86f0eab6":"## \u53c2\u6570\u8bbe\u7f6e","4ff7c47b":"## \u5f00\u59cb\u6d4b\u8bd5","ef12fbfe":"## \u5b9a\u4e49 Tokenizer \u548c Model","9edb51b8":"## \u5b9a\u4e49 Dataset\uff0c\u52a0\u8f7d\u6570\u636e","68a00a15":"## \u8f93\u51fa\u5230csv\u6587\u4ef6","40603737":"## \u5bfc\u5165\u5e93","ed297143":"## \u5f00\u59cb\u8bad\u7ec3\u548c\u9a8c\u8bc1"}}