{"cell_type":{"82e34a4b":"code","3e5b6198":"code","8fe13e46":"code","e7fa3a56":"code","2db56f0e":"code","577d49a3":"code","658dd7d9":"code","d95bdf52":"code","25baa2b0":"code","33ebe87b":"code","e056e1ac":"code","4b263157":"code","4ca51d3c":"code","7b32f482":"code","45d2447e":"code","002b8cad":"code","0f01aefb":"code","b3c11098":"code","bf473f2e":"markdown","7ea0fa5f":"markdown","1b77e97e":"markdown","8bccfc53":"markdown","550a1236":"markdown","ff53c79e":"markdown","b69cbb61":"markdown","c411b8f3":"markdown","1092d9ed":"markdown","5590349d":"markdown","d3f64c00":"markdown","79849a5c":"markdown","9e3aea04":"markdown","eadce4d7":"markdown","0755914a":"markdown","908236dc":"markdown","dbd61abf":"markdown","baadd3d0":"markdown","d7b205e9":"markdown","c7f4ae8c":"markdown","7f398843":"markdown","ae8822f0":"markdown","e2b43561":"markdown","f956a9d7":"markdown","e9d289e5":"markdown","521e744c":"markdown","6b5275a0":"markdown","af79b414":"markdown","6c0078d2":"markdown","7d78dae4":"markdown","9694f57e":"markdown","fa689f93":"markdown","19d87a61":"markdown"},"source":{"82e34a4b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3e5b6198":"from sklearn import datasets\nimport matplotlib.pyplot as plt\nimport pandas as pd","8fe13e46":"iris = datasets.load_iris()\nX = iris.data[:, :2]\ny = iris.target","e7fa3a56":"plt.scatter(X[:,0], X[:,1], c=y, cmap='gist_rainbow')\nplt.xlabel('Spea1 Length', fontsize=18)\nplt.ylabel('Sepal Width', fontsize=18)","2db56f0e":"from sklearn.cluster import AffinityPropagation\naf = AffinityPropagation(damping=0.7)\naf.fit(X)","577d49a3":"#this will tell us to which cluster does the data observations belong.\nnew_labels = af.labels_\n# Plot the identified clusters and compare with the answers\nfig, axes = plt.subplots(1, 2, figsize=(16,8))\naxes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='gist_rainbow',\nedgecolor='k', s=150)\naxes[1].scatter(X[:, 0], X[:, 1], c=new_labels, cmap='jet',\nedgecolor='k', s=150)\naxes[0].set_xlabel('Sepal length', fontsize=12)\naxes[0].set_ylabel('Sepal width', fontsize=12)\naxes[1].set_xlabel('Sepal length', fontsize=12)\naxes[1].set_ylabel('Sepal width', fontsize=12)\naxes[0].tick_params(direction='in', length=8, width=4, colors='k', labelsize=10)\naxes[1].tick_params(direction='in', length=8, width=4, colors='k', labelsize=10)\naxes[0].set_title('Actual', fontsize=12)\naxes[1].set_title('Predicted', fontsize=12)","658dd7d9":"from sklearn.cluster import AgglomerativeClustering\nagl = AgglomerativeClustering(n_clusters=3) #as it is for learning purpose , we set clusters to 3 which we already know\nagl.fit(X)","d95bdf52":"#this will tell us to which cluster does the data observations belong.\nnew_labels = agl.labels_\n# Plot the identified clusters and compare with the answers\nfig, axes = plt.subplots(1, 2, figsize=(12,6))\naxes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='gist_rainbow',\nedgecolor='k', s=150)\naxes[1].scatter(X[:, 0], X[:, 1], c=new_labels, cmap='jet',\nedgecolor='k', s=150)\naxes[0].set_xlabel('Sepal length', fontsize=12)\naxes[0].set_ylabel('Sepal width', fontsize=12)\naxes[1].set_xlabel('Sepal length', fontsize=12)\naxes[1].set_ylabel('Sepal width', fontsize=12)\naxes[0].tick_params(direction='in', length=8, width=4, colors='k', labelsize=10)\naxes[1].tick_params(direction='in', length=8, width=4, colors='k', labelsize=10)\naxes[0].set_title('Actual', fontsize=12)\naxes[1].set_title('Predicted', fontsize=12)","25baa2b0":"from sklearn.cluster import Birch\nbir = Birch(n_clusters=3,threshold=0.01)\nbir.fit(X)","33ebe87b":"#this will tell us to which cluster does the data observations belong.\nnew_labels = bir.labels_\n# Plot the identified clusters and compare with the answers\nfig, axes = plt.subplots(1, 2, figsize=(12,6))\naxes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='gist_rainbow',\nedgecolor='k', s=150)\naxes[1].scatter(X[:, 0], X[:, 1], c=new_labels, cmap='jet',\nedgecolor='k', s=150)\naxes[0].set_xlabel('Sepal length', fontsize=12)\naxes[0].set_ylabel('Sepal width', fontsize=12)\naxes[1].set_xlabel('Sepal length', fontsize=12)\naxes[1].set_ylabel('Sepal width', fontsize=12)\naxes[0].tick_params(direction='in', length=8, width=4, colors='k', labelsize=10)\naxes[1].tick_params(direction='in', length=8, width=4, colors='k', labelsize=10)\naxes[0].set_title('Actual', fontsize=12)\naxes[1].set_title('Predicted', fontsize=12)","e056e1ac":"from sklearn.cluster import DBSCAN\ndbs = DBSCAN(eps=0.2)\ndbs.fit(X)","4b263157":"#this will tell us to which cluster does the data observations belong.\nnew_labels = dbs.labels_\n# Plot the identified clusters and compare with the answers\nfig, axes = plt.subplots(1, 2, figsize=(12,6))\naxes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='gist_rainbow',\nedgecolor='k', s=150)\naxes[1].scatter(X[:, 0], X[:, 1], c=new_labels, cmap='jet',\nedgecolor='k', s=150)\naxes[0].set_xlabel('Sepal length', fontsize=12)\naxes[0].set_ylabel('Sepal width', fontsize=12)\naxes[1].set_xlabel('Sepal length', fontsize=12)\naxes[1].set_ylabel('Sepal width', fontsize=12)\naxes[0].tick_params(direction='in', length=8, width=4, colors='k', labelsize=10)\naxes[1].tick_params(direction='in', length=8, width=4, colors='k', labelsize=10)\naxes[0].set_title('Actual', fontsize=12)\naxes[1].set_title('Predicted', fontsize=12)","4ca51d3c":"from sklearn.cluster import KMeans\nkme = KMeans(n_clusters=3)\nkme.fit(X)","7b32f482":"#this will tell us to which cluster does the data observations belong.\nnew_labels = kme.labels_\n# Plot the identified clusters and compare with the answers\nfig, axes = plt.subplots(1, 2, figsize=(12,6))\naxes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='gist_rainbow',\nedgecolor='k', s=150)\naxes[1].scatter(X[:, 0], X[:, 1], c=new_labels, cmap='jet',\nedgecolor='k', s=150)\naxes[0].set_xlabel('Sepal length', fontsize=12)\naxes[0].set_ylabel('Sepal width', fontsize=12)\naxes[1].set_xlabel('Sepal length', fontsize=12)\naxes[1].set_ylabel('Sepal width', fontsize=12)\naxes[0].tick_params(direction='in', length=8, width=4, colors='k', labelsize=10)\naxes[1].tick_params(direction='in', length=8, width=4, colors='k', labelsize=10)\naxes[0].set_title('Actual', fontsize=12)\naxes[1].set_title('Predicted', fontsize=12)","45d2447e":"from sklearn.cluster import MiniBatchKMeans\nmkm = MiniBatchKMeans(n_clusters=3)\nmkm.fit(X)","002b8cad":"#this will tell us to which cluster does the data observations belong.\nnew_labels = mkm.labels_\n# Plot the identified clusters and compare with the answers\nfig, axes = plt.subplots(1, 2, figsize=(12,6))\naxes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='gist_rainbow',\nedgecolor='k', s=150)\naxes[1].scatter(X[:, 0], X[:, 1], c=new_labels, cmap='jet',\nedgecolor='k', s=150)\naxes[0].set_xlabel('Sepal length', fontsize=12)\naxes[0].set_ylabel('Sepal width', fontsize=12)\naxes[1].set_xlabel('Sepal length', fontsize=12)\naxes[1].set_ylabel('Sepal width', fontsize=12)\naxes[0].tick_params(direction='in', length=8, width=4, colors='k', labelsize=10)\naxes[1].tick_params(direction='in', length=8, width=4, colors='k', labelsize=10)\naxes[0].set_title('Actual', fontsize=12)\naxes[1].set_title('Predicted', fontsize=12)","0f01aefb":"from sklearn.cluster import MeanShift\nmshf = MeanShift()\nmshf.fit(X)","b3c11098":"#this will tell us to which cluster does the data observations belong.\nnew_labels = mshf.labels_\n# Plot the identified clusters and compare with the answers\nfig, axes = plt.subplots(1, 2, figsize=(12,6))\naxes[0].scatter(X[:, 0], X[:, 1], c=y, cmap='gist_rainbow',\nedgecolor='k', s=150)\naxes[1].scatter(X[:, 0], X[:, 1], c=new_labels, cmap='jet',\nedgecolor='k', s=150)\naxes[0].set_xlabel('Sepal length', fontsize=12)\naxes[0].set_ylabel('Sepal width', fontsize=12)\naxes[1].set_xlabel('Sepal length', fontsize=12)\naxes[1].set_ylabel('Sepal width', fontsize=12)\naxes[0].tick_params(direction='in', length=8, width=4, colors='k', labelsize=10)\naxes[1].tick_params(direction='in', length=8, width=4, colors='k', labelsize=10)\naxes[0].set_title('Actual', fontsize=12)\naxes[1].set_title('Predicted', fontsize=12)","bf473f2e":"# What is Cluster","7ea0fa5f":"1. All those algortihm where we provided number of clusters before did very good in finding the actual clusters\n2. Kmeans , Mini Batch K means and Birch algorithm are actually doing good but as we had provided number of clusters before which might not be the case in the real world datasets\n3. if clusters are not known mean shift and DBSCAN are good , also DBSCAN 's accuracy is higly affected by the parameter eps , tuning the eps parameter can give better results\n\nHOPE YOU LEARNED SOMETHING FROM IT","1b77e97e":"Agglomerative clustering involves merging examples until the desired number of clusters is achieved.\n\nIt is a part of a broader class of hierarchical clustering methods","8bccfc53":"As it is evident that it worked terrible in making clusters","550a1236":"1. Affinity Propagation\n> We devised a method called \u201caffinity propagation,\u201d which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges\n\n\u2014 Clustering by Passing Messages Between Data Points, 2007.","ff53c79e":"# Mini-Batch K-Means\nMini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.","b69cbb61":"> These clusters presumably reflect some mechanism at work in the domain from which instances are drawn, a mechanism that causes some instances to bear a stronger resemblance to each other than they do to the remaining instances.\n\n\u2014 Pages 141-142, Data Mining: Practical Machine Learning Tools and Techniques, 2016.","c411b8f3":"A cluster is often an area of density in the feature space where examples from the domain (observations or rows of data) are closer to the cluster than other clusters. The cluster may have a center (the centroid) that is a sample or a point feature space and may have a boundary or extent.","1092d9ed":"Evaluation of identified clusters is subjective and may require a domain expert, although many clustering-specific quantitative measures do exist.","5590349d":"# Agglomerative Clustering","d3f64c00":"we will use famous iris data set for our cluster analysis","79849a5c":"# DATASET","9e3aea04":"# BIRCH\nBIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using\nHierarchies) involves constructing a tree structure from which cluster centroids are extracted.","eadce4d7":"# Conclusion","0755914a":"# K-Means\nK-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster","908236dc":"# Mean Shift\nMean shift clustering involves finding and adapting centroids based on the density of examples in the feature space.","dbd61abf":"Unsupervised learning is a class of machine learning (ML) techniques used to find patterns in data. The data given to unsupervised algorithms is not labelled, which means only the input variables (x) are given with no corresponding output variables. In unsupervised learning, the algorithms are left to discover interesting structures in the data on their own.","baadd3d0":"Clustering can be helpful as a data analysis activity in order to learn more about the problem domain, so-called pattern discovery or knowledge discovery.","d7b205e9":"This also created a good clustering like the previous","c7f4ae8c":"# Clustering in Unsupervised Learning","7f398843":"Three Existing Clusters","ae8822f0":"> The most prominent methods of unsupervised learning are cluster analysis and principal component analysis.","e2b43561":"There are many types of clustering algorithms.\n\nMany algorithms use similarity or distance measures between examples in the feature space in an effort to discover dense regions of observations. As such, it is often good practice to scale data prior to using clustering algorithms.","f956a9d7":"It is evident that this algorithm is not working fine as it created more clusters and more complicated data","e9d289e5":"# DBSCAN\nDBSCAN Clustering (where DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise) involves finding high-density areas in the domain and expanding those areas of the feature space around them as clusters.","521e744c":"# CLUSTERING ALGORITHMS","6b5275a0":"It did terrible in making clusters","af79b414":"> Clustering techniques apply when there is no class to be predicted but rather when the instances are to be divided into natural groups.\n\n\u2014 Page 141, Data Mining: Practical Machine Learning Tools and Techniques, 2016.","6c0078d2":"![](http:\/\/https:\/\/builtin.com\/sites\/default\/files\/styles\/ckeditor_optimize\/public\/inline-images\/supervised-vs-unsupervised-learning.png)","7d78dae4":"As such, cluster analysis is an iterative process where subjective evaluation of the identified clusters is fed back into changes to algorithm configuration until a desired or appropriate result is achieved.","9694f57e":"Cluster analysis, or clustering, is an unsupervised machine learning task.\n\nIt involves automatically discovering natural grouping in data. Unlike supervised learning (like predictive modeling), clustering algorithms only interpret the input data and find natural groups or clusters in feature space.","fa689f93":"This is working pretty good and we can see it has almost accurately created the clusters","19d87a61":"it is by far the best clustering algorithm that created highly accurate clusters"}}