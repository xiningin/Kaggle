{"cell_type":{"b4ce6b12":"code","94fce0a9":"code","39c6e946":"code","fcd8dcd8":"code","0cef9c38":"code","54ed1dd3":"code","f74c22b3":"code","c8aa7e3b":"code","b006dc22":"code","6948aab9":"code","3da685a5":"code","a68f31c0":"code","d01ad541":"code","e23fc4e1":"code","c21dbd6f":"code","2382377b":"code","ad8e3ada":"code","ceaf1ea9":"code","c7f41661":"code","d0bab170":"code","eada0f28":"code","113030db":"code","0fe451f8":"code","4fb1e56c":"code","9fa67f3a":"code","87f9c898":"code","6bd49065":"code","30439bd9":"code","f01a605a":"markdown","0027c01e":"markdown","7ec96824":"markdown","5f4fc404":"markdown","c15aa0e1":"markdown","2fc1a686":"markdown","eef35f36":"markdown","f5bec96f":"markdown","43566e23":"markdown","1562fd88":"markdown","c065fff4":"markdown","047c005c":"markdown","4d3e1d0d":"markdown"},"source":{"b4ce6b12":"import numpy as np\nimport os\nimport pandas as pd\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.np_utils import to_categorical\nimport matplotlib.pyplot as plt\nfrom PIL import Image, ImageDraw\nimport random\nimport urllib.request\nimport io\nimport random","94fce0a9":"!gdown --id 1xxbviC-btZjsVenkUazp9ytiOHeSdyN3","39c6e946":"!unzip \"\/content\/trainKeys.zip\"","fcd8dcd8":"path = \"\/content\/trainKeys\"\nfor folder in os.listdir(path):\n  print(folder,len( os.listdir(os.path.join(path,folder))  ))\n","0cef9c38":"#df\n!cd \/content\/&& gdown --id 1zb4vWj_YxMbKEEtBQ8zaNrVL5g9yC_e_ ","54ed1dd3":"!cd \/content\/&& unzip '\/content\/wazzadu.zip'","f74c22b3":"cat = pd.read_csv(\"\/content\/dim_cat_subcat_tag_key.csv\")\ntrain_df = pd.read_csv(\"\/content\/train.csv\")\ntest_df = pd.read_csv(\"\/content\/test-sample.csv\")","c8aa7e3b":"from tqdm.auto import tqdm\nfrom random import shuffle,sample\n\nbase_dir = \"\/content\/trainKeys\"\nfolder_list = [f for f in os.listdir(base_dir) if f[0]=='p']\nprint( len(folder_list), \"categories found in the dataset\")\n\n# Train test split \ntrain_test_split = 0.8\nno_of_files_in_each_class = 100\n\n# Declare training array\ncat_list = []\nx = []\ny = []\ny_label = ''\nindex = 0\n\n# Using just 100 images per category\nfor folder_name in tqdm(folder_list, desc = 'folder'):\n    file_list = os.listdir(os.path.join(base_dir, folder_name))\n    temp=[]\n\n    #random\n    file_list_pick = sample(file_list,no_of_files_in_each_class)\n    for file_name in file_list_pick:\n        temp.append(index)\n        x.append(np.asarray(Image.open(os.path.join(base_dir, folder_name, file_name)).convert('RGB').resize((100, 100))))\n        y.append(str(folder_name))\n        index += 1\n    cat_list.append(temp)\n\ncat_list = np.asarray(cat_list)\nx = np.asarray(x).astype(float)\/255.0\ny = np.asarray(y).astype(str)\nprint('x, y shape',x.shape, y.shape, cat_list.shape)      ","b006dc22":"# group x\nx_group = []\nfor i in range(0,len(folder_list)):\n  x_group.append(x[i*no_of_files_in_each_class:(i+1)*no_of_files_in_each_class])\nx_group = np.asarray(x_group)\n\n#group y\ny_group = np.unique(y)","6948aab9":"import numpy as np\n\ntrain_size = int(len(folder_list) *0.8)\ntest_size = len(folder_list) - train_size\nprint(train_size, 'classes for training and', test_size, ' classes for testing')\n\n# Training Split\nx_train = x_group[:train_size]\ny_train = y[:train_size*no_of_files_in_each_class]\ncat_train = cat_list[:train_size*no_of_files_in_each_class]\n\nx_train_full = x[:train_size*no_of_files_in_each_class]\nx_test_full = x[train_size*no_of_files_in_each_class:]\n\n# Validation Split\nx_test = x_group[train_size:]\ny_test = y[train_size*no_of_files_in_each_class:]\ncat_test = cat_list[train_size*no_of_files_in_each_class:]\n\nprint('X&Y shape of training data :',x_train.shape, 'and', y_train.shape, cat_train.shape)\nprint('X&Y shape of testing data :' , x_test.shape, 'and', y_test.shape, cat_test.shape)","3da685a5":"def gen_random_batch(in_groups, batch_halfsize = 8):\n    out_img_a, out_img_b, out_score = [], [], []\n    all_groups = list(range(len(in_groups)))\n    for match_group in [True, False]:\n        group_idx = np.random.choice(all_groups, size = batch_halfsize)\n        out_img_a += [in_groups[c_idx][np.random.choice(range(in_groups[c_idx].shape[0]))] for c_idx in group_idx]\n        if match_group:\n            b_group_idx = group_idx\n            out_score += [1]*batch_halfsize\n        else:\n            # anything but the same group\n            non_group_idx = [np.random.choice([i for i in all_groups if i!=c_idx]) for c_idx in group_idx] \n            b_group_idx = non_group_idx\n            out_score += [0]*batch_halfsize\n            \n        out_img_b += [in_groups[c_idx][np.random.choice(range(in_groups[c_idx].shape[0]))] for c_idx in b_group_idx]\n            \n    return np.stack(out_img_a,0), np.stack(out_img_b,0), np.stack(out_score,0)","a68f31c0":"pv_a, pv_b, pv_sim = gen_random_batch(x_train, 3)\nfig, m_axs = plt.subplots(2, pv_a.shape[0], figsize = (12, 6))\nfor c_a, c_b, c_d, (ax1, ax2) in zip(pv_a, pv_b, pv_sim, m_axs.T):\n    ax1.imshow(c_a)\n    ax1.set_title('Image A')\n    ax1.axis('off')\n    ax2.imshow(c_b)\n    ax2.set_title('Image B\\n Similarity: %3.0f%%' % (100*c_d))\n    ax2.axis('off')","d01ad541":"from keras.models import Model\nfrom keras.layers import Input, Conv2D, BatchNormalization, MaxPool2D, Activation, Flatten, Dense, Dropout\nimg_in = Input(shape = x_train.shape[2:], name = 'FeatureNet_ImageInput')\nn_layer = img_in\nfor i in range(2):\n    n_layer = Conv2D(8*2**i, kernel_size = (3,3), activation = 'linear')(n_layer)\n    n_layer = BatchNormalization()(n_layer)\n    n_layer = Activation('relu')(n_layer)\n    n_layer = Conv2D(16*2**i, kernel_size = (3,3), activation = 'linear')(n_layer)\n    n_layer = BatchNormalization()(n_layer)\n    n_layer = Activation('relu')(n_layer)\n    n_layer = MaxPool2D((2,2))(n_layer)\nn_layer = Flatten()(n_layer)\nn_layer = Dense(32, activation = 'linear')(n_layer)\nn_layer = Dropout(0.5)(n_layer)\nn_layer = BatchNormalization()(n_layer)\nn_layer = Activation('relu')(n_layer)\nfeature_model = Model(inputs = [img_in], outputs = [n_layer], name = 'FeatureGenerationModel')\nfeature_model.summary()","e23fc4e1":"from keras.layers import concatenate\nimg_a_in = Input(shape = x_train.shape[2:], name = 'ImageA_Input')\nimg_b_in = Input(shape = x_train.shape[2:], name = 'ImageB_Input')\nimg_a_feat = feature_model(img_a_in)\nimg_b_feat = feature_model(img_b_in)\ncombined_features = concatenate([img_a_feat, img_b_feat], name = 'merge_features')\ncombined_features = Dense(16, activation = 'linear')(combined_features)\ncombined_features = BatchNormalization()(combined_features)\ncombined_features = Activation('relu')(combined_features)\ncombined_features = Dense(4, activation = 'linear')(combined_features)\ncombined_features = BatchNormalization()(combined_features)\ncombined_features = Activation('relu')(combined_features)\ncombined_features = Dense(1, activation = 'sigmoid')(combined_features)\nsimilarity_model = Model(inputs = [img_a_in, img_b_in], outputs = [combined_features], name = 'Similarity_Model')\nsimilarity_model.summary()","c21dbd6f":"# setup the optimization process\nsimilarity_model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['mae'])","2382377b":"def show_model_output(nb_examples = 3):\n    pv_a, pv_b, pv_sim = gen_random_batch(x_test, nb_examples)\n    pred_sim = similarity_model.predict([pv_a, pv_b])\n    fig, m_axs = plt.subplots(2, pv_a.shape[0], figsize = (12, 6))\n    for c_a, c_b, c_d, p_d, (ax1, ax2) in zip(pv_a, pv_b, pv_sim, pred_sim, m_axs.T):\n        ax1.imshow(c_a[:,:,:])\n        ax1.set_title('Image A\\n Actual: %3.0f%%' % (100*c_d))\n        ax1.axis('off')\n        ax2.imshow(c_b[:,:,:])\n        ax2.set_title('Image B\\n Predicted: %3.0f%%' % (100*p_d))\n        ax2.axis('off')\n    return fig\n# a completely untrained model\n_ = show_model_output()","ad8e3ada":"# make a generator out of the data\ndef siam_gen(in_groups, batch_size = 64):\n    while True:\n        pv_a, pv_b, pv_sim = gen_random_batch(x_train, batch_size\/\/2)\n        yield [pv_a, pv_b], pv_sim\n        \n# we want a constant validation group to have a frame of reference for model performance\nvalid_a, valid_b, valid_sim = gen_random_batch(x_test, 1024)\nloss_history = similarity_model.fit_generator(siam_gen(x_train), \n                               steps_per_epoch = 500,\n                               validation_data=([valid_a, valid_b], valid_sim),\n                                              epochs = 50,\n                                             verbose = True)\n\nsimilarity_model.save('similarity_model_latest.h5')","ceaf1ea9":"_ = show_model_output()","c7f41661":"test_df = pd.read_csv(\"\/content\/test-sample.csv\")\ntest_df","d0bab170":"def read_im(i):\n  # Display image \n  URL = test_df.url[i]\n\n  with urllib.request.urlopen(URL) as url:\n    f = io.BytesIO(url.read())\n  img = Image.open(f)\n\n  # Dimension\n  w, h = np.array(img).shape[1], np.array(img).shape[0]\n  shape = [(int(test_df.tl_x[i]*w), int(test_df.tl_y[i]*h)), (int(test_df.br_x[i]*w), int(test_df.br_y[i]*h))] \n\n  ip = img.crop((int(test_df.tl_x[i]*w), int(test_df.tl_y[i]*h), int(test_df.br_x[i]*w), int(test_df.br_y[i]*h)))\n  # display(ip)\n  ip = ip.convert('RGB').resize((100, 100))\n  ip = np.asarray(ip)\/255.0\n  return ip","eada0f28":"#test\nip = read_im(0)\nplt.imshow(ip)","113030db":"# similarity_model.load_weights('\/content\/weight\/weights.3000.h5')","0fe451f8":"x_test_submit = []\nsubmit = []\nx_test_submit.append(np.zeros((24, 100, 100, 3)))\nx_test_submit.append(np.zeros((24, 100, 100, 3)))\nfor i in range(len(test_df)):\n  ip = read_im(i)\n  for j in range(24):\n    x_test_submit[0][j] = ip\n    index = random.randint(0,no_of_files_in_each_class-1)\n    x_test_submit[1][j] = x_group[j][index]\n\n  pred = similarity_model.predict(x_test_submit)\n  submit.append(y_group[np.argmax(pred)])\n  # print(pred)\n  # print(y_group[np.argmax(pred)])","4fb1e56c":"submit_df = pd.DataFrame()\nsubmit_df['key']=submit\nsubmit_df.to_csv('machima_3.csv')","9fa67f3a":"x_test_features = feature_model.predict(x_test_full, verbose = True, batch_size=128)","87f9c898":"%%time\nfrom sklearn.manifold import TSNE\ntsne_obj = TSNE(n_components=2,\n                         init='pca',\n                         random_state=101,\n                         method='barnes_hut',\n                         n_iter=500,\n                         verbose=2)\ntsne_features = tsne_obj.fit_transform(x_test_features)","6bd49065":"obj_categories = y_group\ncolors = plt.cm.rainbow(np.linspace(0, 1, 24))\nplt.figure(figsize=(10, 10))\n\nfor c_group, (c_color, c_label) in enumerate(zip(colors, obj_categories)):\n    plt.scatter(tsne_features[np.where(y_test == y_group[c_group]), 0],\n                tsne_features[np.where(y_test == y_group[c_group]), 1],\n                marker='o',\n                color=c_color,\n                linewidth='1',\n                alpha=0.8,\n                label=c_label)\nplt.xlabel('Dimension 1')\nplt.ylabel('Dimension 2')\nplt.title('t-SNE on Testing Samples')\nplt.legend(loc='best')\nplt.savefig('clothes-dist.png')\nplt.show(block=False)","30439bd9":"similarity_model.save('similarity_model_latest.h5')","f01a605a":"## Neighbor Visualization\nFor this we use the TSNE neighborhood embedding to visualize the features on a 2D plane and see if it roughly corresponds to the groups. We use the test data for this example as well since the training has been contaminated","0027c01e":"###train test split","7ec96824":"# Feature Generation\nHere we make the feature generation network to process images into features. The network starts off randomly initialized and will be trained to generate useful vector features from input images (_hopefully_)","5f4fc404":"###x_group  and y_group","c15aa0e1":"### Batch Generator\nHere the idea is to make usuable batches for training the network. We need to create parallel inputs for the $A$ and $B$ images where the output is the distance. Here we make the naive assumption that if images are in the same group the similarity is 1 otherwise it is 0.\n\nIf we randomly selected all of the images we would likely end up with most images in different groups.","2fc1a686":"## Before train\nHere we visualize what the model does by taking a small sample of randomly selected A and B images the first half from the same category and the second from different categories. We then show the actual distance (0 for the same category and 1 for different categories) as well as the model predicted distance. The first run here is with a completely untrained network so we do not expect meaningful results.","eef35f36":"#Prediction\n","f5bec96f":"# Load Data\n","43566e23":"###After train","1562fd88":"# Siamese Model\nWe apply the feature generating model to both images and then combine them together to predict if they are similar or not. The model is designed to very simple. The ultimate idea is when a new image is taken that a feature vector can be calculated for it using the _FeatureGenerationModel_. All existing images have been pre-calculated and stored in a database of feature vectors. The model can be applied using a few vector additions and multiplications to determine the most similar images. These operations can be implemented as a stored procedure or similar task inside the database itself since they do not require an entire deep learning framework to run.","c065fff4":"#Examining the Features\nHere we aim to answer the more general question: did we generate useful features with the Feature Generation model? And how can we visualize this.","047c005c":"# Import Library","4d3e1d0d":"## Validate Data\nHere we make sure the generator is doing something sensible, we show the images and their similarity percentage."}}