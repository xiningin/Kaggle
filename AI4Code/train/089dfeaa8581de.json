{"cell_type":{"fbde0b54":"code","8c6013ee":"code","e013f386":"code","ef465431":"code","687f3271":"code","dadd679f":"code","87be16af":"code","46143e25":"code","50c74ebc":"code","2682dc29":"code","0e512a89":"code","a40fe03b":"code","bc0cd1e2":"code","7671dd95":"code","c716f37a":"code","30099a3d":"code","00ac72d9":"code","4015f097":"code","586b5ef6":"code","c7220240":"code","6aef2c6e":"code","557bc976":"code","75cb71a5":"code","9690b27c":"code","5f17b1d5":"code","3ebe0910":"code","fb4c9cbc":"code","4acbd32d":"code","4fea4d00":"code","64ec232e":"markdown","8e3acca6":"markdown","d83e4af6":"markdown","6b9d866c":"markdown","413c3cc3":"markdown","a350aa9b":"markdown","cc7fd6fa":"markdown","1da14dcd":"markdown","b5c62bf1":"markdown","2cf09b64":"markdown","b3160419":"markdown","c8e4785f":"markdown","d343b549":"markdown","85138929":"markdown","bca146cb":"markdown","0978bdf6":"markdown","fd01f312":"markdown","3b68c5fe":"markdown","5d91fd7f":"markdown","d035b759":"markdown","c7cdb23f":"markdown","193bce77":"markdown","56255b24":"markdown","c5a18fb3":"markdown","1f5eb984":"markdown","3f9a33bb":"markdown","b3871585":"markdown","aa913052":"markdown","876f66bc":"markdown","61f1e923":"markdown","284e3101":"markdown"},"source":{"fbde0b54":"\n# supporting libraries -----------------------------------------\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport pandas as pd\nimport re\n\n# for data processing ------------------------------------------\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import*\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import*\n\n# for prediction (machine learning models) ---------------------\n\nfrom sklearn.linear_model import*\nfrom sklearn.preprocessing import*\nfrom sklearn.ensemble import*\nfrom sklearn.neighbors import*\nfrom sklearn import svm\nfrom sklearn.naive_bayes import*\nimport xgboost as xgb","8c6013ee":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e013f386":"df=pd.read_csv('\/kaggle\/input\/diamonds\/diamonds.csv')\ndf.head()","ef465431":"df.info()","687f3271":"cut=df['cut'].value_counts().index\nk=0\nfor i in cut:\n    df['cut'].replace(i,k,inplace=True)\n    k+=1\ndf.head()","dadd679f":"color=df['color'].value_counts().index\nk=0\nfor i in color:\n    df['color'].replace(i,k,inplace=True)\n    k+=1\ndf.head()","87be16af":"clarity=df['clarity'].value_counts().index\nk=0\nfor i in clarity:\n    df['clarity'].replace(i,k,inplace=True)\n    k+=1\ndf.head()","46143e25":"df.isnull().sum()","50c74ebc":"df.describe()","2682dc29":"depth_percentage=[]\nfor i in range(len(df)):\n    depth_percentage.append((2*df['z'][i])\/(df['x'][i]+df['y'][i]))","0e512a89":"len(depth_percentage)","a40fe03b":"df.drop(labels=['x','y','z'],axis=1,inplace=True)\ndepth_percentage=pd.DataFrame({'depth_percentage':depth_percentage})\ndf=pd.concat([df,depth_percentage],axis=1)\ndf.head()","bc0cd1e2":"plt.figure(figsize=(20,5))\nplt.title('depth vs depth_percentage')\nplt.xlabel('depth percentage')\nplt.ylabel('depth')\nplt.scatter(df['depth_percentage'],df['depth'],s=4,color='g')\nplt.show()","7671dd95":"df.drop('depth_percentage',1,inplace=True)\ndf.head()","c716f37a":"fig, axs = plt.subplots(3,1,figsize=(20,20))\nplt.subplot(3,1,1)\nplt.title('CARAT')\nplt.scatter(df['carat'],df['price'],s=4)\nplt.subplot(3,1,2)\nplt.title('DEPTH')\nplt.scatter(df['depth'],df['price'],s=4)\nplt.subplot(3,1,3)\nplt.title('TABLE')\nplt.scatter(df['table'],df['price'],s=4)\n# plt.show()","30099a3d":"\nfig, axs = plt.subplots(3,1,figsize=(10,10))\nfig.suptitle('Discrete Features Correlation with price',fontsize=20)\nplt.subplot(3,1,1)\nplt.title('CUT')\nsns.violinplot(x=\"cut\", y=\"price\", data=df)\nplt.subplot(3,1,2)\nplt.title('COLOUR')\nsns.violinplot(x=\"color\", y=\"price\", data=df)\nplt.subplot(3,1,3)\nplt.title('CLARITY')\nsns.violinplot(x=\"clarity\", y=\"price\", data=df)","00ac72d9":"sns.violinplot(x='color',y='clarity',data=df)","4015f097":"df=df.sample(frac=1)","586b5ef6":"X=df.drop('price',1)\ny=df['price']\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","c7220240":"model=RandomForestRegressor(random_state=0)\nmodel.fit(X_train,y_train)\ny_1=model.predict(X_train)\nprint('RMSE in train data :',np.sqrt(mean_squared_error(y_train,y_1)))\ny_pred=model.predict(X_test)\nprint('RMSE in test data :',np.sqrt(mean_squared_error(y_test,y_pred)))","6aef2c6e":"print('R-squared score of the model on train data is : ',model.score(X_train,y_train))","557bc976":"print('R-squared score of the model on test data is : ',model.score(X_test,y_test))","75cb71a5":"plt.figure(figsize=(15,5))\nplt.title('Model Evaluation')\nplt.scatter(range(0,len(X_test)),y_test,label='true',s=5)\nplt.scatter(range(0,len(X_test)),y_pred,label='predicted',s=5)\nplt.xlabel('index')\nplt.ylabel('Price')\nplt.legend()\nplt.show()","9690b27c":"difference=abs(y_test-y_pred)\nplt.figure(figsize=(15,5))\nplt.title('difference')\nplt.scatter(range(0,len(difference)),difference,s=3)\nplt.xlabel('index')\nplt.ylabel('Difference in Price')\nplt.show()","5f17b1d5":"X_train.drop('Unnamed: 0',1,inplace=True)\nX_test.drop('Unnamed: 0',1,inplace=True)","3ebe0910":"model=RandomForestRegressor(random_state=0)\nmodel.fit(X_train,y_train)\ny_1=model.predict(X_train)\nprint('RMSE in train data :',np.sqrt(mean_squared_error(y_train,y_1)))\ny_pred=model.predict(X_test)\nprint('RMSE in test data :',np.sqrt(mean_squared_error(y_test,y_pred)))","fb4c9cbc":"print('R-squared score of the model on train data is : ',model.score(X_train,y_train))\nprint('R-squared score of the model on test data is : ',model.score(X_test,y_test))","4acbd32d":"plt.figure(figsize=(15,5))\nplt.title('Model Evaluation')\nplt.scatter(range(0,len(X_test)),y_test,label='true',s=5)\nplt.scatter(range(0,len(X_test)),y_pred,label='predicted',s=5)\nplt.xlabel('index')\nplt.ylabel('Price')\nplt.legend()\nplt.show()","4fea4d00":"difference=abs(y_test-y_pred)\nplt.figure(figsize=(15,5))\nplt.title('difference')\nplt.scatter(range(0,len(difference)),difference,s=3)\nplt.xlabel('index')\nplt.ylabel('Difference in Price')\nplt.show()","64ec232e":"We can see that maximum of 5 lue points are visible , this indicates that the model really is well-tuned.","8e3acca6":"**Feature : CLARITY**","d83e4af6":"# Model Creating, Fitting and Evaluation :\nAt first we are going to chcek the model with the Unnamed :0 feature. \nAfter that we are going to check omitting that feature.","6b9d866c":"# EDA : Exploratory Data Analysis","413c3cc3":"This project is based on analysis and prediction of diamonds.\nIn these days diamonds are very costly , so the buyer can face difficulties or abrupt changes in prices.\nUsing this project they can find the best diamon for their utility.\nThis project is made with \u2764\ufe0f.","a350aa9b":"# Libraries :\nIn this project we are using the classical process rather than NN , so we are not importing **tensorflow** or **pytorch**.\n","cc7fd6fa":"Now we are going to see the correlation of the continuous features with the *price* feature using [*matplotlib.pyplot.scatter*](https:\/\/matplotlib.org\/3.3.3\/api\/_as_gen\/matplotlib.pyplot.scatter.html)","1da14dcd":"So ,we can see there is depth and depth percentage which looks similar. so we can check if they are similar or not.\nIf found similar then we can drop a single feature.","b5c62bf1":"So depth and depth percentage shows similar behaviour. So we are omiting depth_percentage for dimensionality reduction.","2cf09b64":"### Hurrah !  We've completed the project. \nIf you find any queries or want to give any feedback please contact me over my email.\n\n**Email** : *sagnik.jal00@gmail.com*\n\nOr you can contact me over **discord** -***'s_agnik1511#6085'***\n\n","b3160419":"As we have seen in the [data description](https:\/\/www.kaggle.com\/shivam2503\/diamonds) that \n              \n              Depth Percentage (D) = z\/mean(x,y)\n              \n                                   = z\/{(x+y)\/2}\n                                   \n                                   = (2*z)\/(x+y)","c8e4785f":"# Final Data Preparation :\n1. At first we have to create the X and Y .\n2. Then we have to split the X and y into train and validation.\n   In this project we are performing a 80%-20% train-test split.","d343b549":"So we can drop the features 'x','y','z' and then concatenate the depth percentage .","85138929":"Most of the points are bounded with ground line showing a very good prediction.","bca146cb":"# UPVOTE if you like my project :)\nYou can visit my other works at [kaggle](https:\/\/www.kaggle.com\/sagnik1511\/notebooks)  or [github](https:\/\/github.com\/sagnik1511?tab=repositories).","0978bdf6":"# Thank You :)","fd01f312":"**Feature : CUT**","3b68c5fe":"The model gives 99.998 % accuracy when 'Unnamed :0' is present.\nThe model gives 98.182 % accuracy when 'Unnamed :0' is present.","5d91fd7f":"As the featureset is in a pattern we have to shuffle the dataset and that will help us find a better prediction.","d035b759":"### Leakagae Processing :\nIf there is leakage in the data , we can follow these steps -\n\n     1. If there are no leakage then we should skip to next steps.\n     2. If there are less leakage then we should fill those with a very small number e.g. -99999.\n     3. If there are moderate number of leakagaes then we can fill those with th mean of the feature.\n     4. If there are only or too much leakage then it is best to drop or omit the feature.","c7cdb23f":"# Final Conclusion :\n\n As we have seen here that the model accuracy decrease when the ***Unnamed :0*** feature is omitted . So , we can say that the price is also dependable on that feature and can't be taken out if any other prediction happens in future.","193bce77":"# Diamond Price Prediction : Complete Project","56255b24":"# Data Gathering and Primary Visualization:\nWe are using the [*read_csv*](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html) function of [*pandas*](https:\/\/pandas.pydata.org\/) to make the dataframe and visualize it.","c5a18fb3":"We can see there are -\n1. 2 **Integer** type features\n2. 6 **Float** type features.\n3. 3 **Object** type features.\n\nSo, we have to encode those categorical features as we can feed only numerical features into the machine learning model.","1f5eb984":"#### Conclusion:\n* This graphs are showing that the number of features make the violin plots wider and smaller cause heavy number of points hold different values and thus the mean of the plot boils down.\n* The colour and cut are directly proportionate and the clarity is inversely proportionate.","3f9a33bb":"Total dataframe has been encoded. ","b3871585":"Here we can see the clarity is also inversely proportionate with the colour.","aa913052":"#### Conclusion :\n* The continuous features are showing a similar graphical manner as gaussian distribution.\n* The cotinuous features aren't complete continuous rather they have a wide range of values.\n* The Carat is found presenting a direct proportional behaviour with the price . It is also a reminder that the data isn't shuffled well.\n* The Depth feature is seen forming a pyramidal shape with the price feature showing a singular point of depth is more valuable.","876f66bc":"**Feature : COLOR**","61f1e923":"The dataset is quite good as there are no leakages. So, we can proceed further.","284e3101":"We are manually encoding those features."}}