{"cell_type":{"b4384b03":"code","a4bf53b9":"code","00b56cc5":"code","27e511ff":"code","0161c995":"code","57a3470b":"code","6e0e0226":"code","233bd512":"code","0f1dbb10":"code","1598e20b":"code","d63f2b93":"code","78d8343b":"code","fc9b351d":"code","7013af17":"code","82616312":"code","08ff2715":"code","6b930458":"code","253ae419":"code","4dacf467":"markdown","bd9337c7":"markdown","bdbfafdb":"markdown","cdbae314":"markdown","664bae82":"markdown","81b75bb5":"markdown","40a24d75":"markdown","343405b8":"markdown","2ece1a47":"markdown","417fd840":"markdown","c9ad4093":"markdown","e4d0adcc":"markdown","a6d6e2c8":"markdown","1e2e78fe":"markdown","2b1b15ba":"markdown","3d753522":"markdown","656d2249":"markdown","279ea470":"markdown","0b754c31":"markdown","7c78209d":"markdown"},"source":{"b4384b03":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets","a4bf53b9":"train_dataset = dsets.FashionMNIST(root='.\/data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.FashionMNIST(root='.\/data', \n                           train=False, \n                           transform=transforms.ToTensor())","00b56cc5":"print(train_dataset.train_data.size())","27e511ff":"print(train_dataset.train_labels.size())","0161c995":"print(test_dataset.test_data.size())","57a3470b":"print(test_dataset.test_labels.size())","6e0e0226":"batch_size = 100\nn_iters = 3000\nnum_epochs = n_iters \/ (len(train_dataset) \/ batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)","233bd512":"class LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(LSTMModel, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n        \n        # Number of hidden layers\n        self.layer_dim = layer_dim\n        \n        # Building your LSTM\n        # batch_first=True causes input\/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n        \n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n        \n        # Initialize cell state\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n        \n        # 28 time steps\n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don't, we'll backprop all the way to the start even after going through another batch\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n        \n        # Index hidden state of last time step\n        # out.size() --> 100, 28, 100\n        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --> 100, 10\n        return out","0f1dbb10":"input_dim = 28\nhidden_dim = 100\nlayer_dim = 1\noutput_dim = 10","1598e20b":"model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)","d63f2b93":"criterion = nn.CrossEntropyLoss()","78d8343b":"learning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  ","fc9b351d":"len(list(model.parameters()))","7013af17":"for i in range(len(list(model.parameters()))):\n    print(list(model.parameters())[i].size())","82616312":"# Number of steps to unroll\nseq_dim = 28  \n\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as a torch tensor with gradient accumulation abilities\n        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n        \n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n        \n        # Forward pass to get output\/logits\n        # outputs.size() --> 100, 10\n        outputs = model(images)\n        \n        # Calculate Loss: softmax --> cross entropy loss\n        loss = criterion(outputs, labels)\n        \n        # Getting gradients w.r.t. parameters\n        loss.backward()\n        \n        # Updating parameters\n        optimizer.step()\n        \n        iter += 1\n        \n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Resize images\n                images = images.view(-1, seq_dim, input_dim)\n                \n                # Forward pass only to get logits\/output\n                outputs = model(images)\n                \n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n                \n                # Total number of labels\n                total += labels.size(0)\n                \n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ total\n            \n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))","08ff2715":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\ntrain_dataset = dsets.FashionMNIST(root='.\/data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.FashionMNIST(root='.\/data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters \/ (len(train_dataset) \/ batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(LSTMModel, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n        \n        # Number of hidden layers\n        self.layer_dim = layer_dim\n        \n        # Building your LSTM\n        # batch_first=True causes input\/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n        \n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n        \n        # Initialize cell state\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n        \n        # One time step\n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don't, we'll backprop all the way to the start even after going through another batch\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n        \n        # Index hidden state of last time step\n        # out.size() --> 100, 28, 100\n        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --> 100, 10\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28\nhidden_dim = 100\nlayer_dim = 2  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\noutput_dim = 10\n\nmodel = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n# JUST PRINTING MODEL & PARAMETERS \nprint(model)\nprint(len(list(model.parameters())))\nfor i in range(len(list(model.parameters()))):\n    print(list(model.parameters())[i].size())\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\n\n# Number of steps to unroll\nseq_dim = 28  \n\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as torch tensor with gradient accumulation abilities\n        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n        \n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n        \n        # Forward pass to get output\/logits\n        # outputs.size() --> 100, 10\n        outputs = model(images)\n        \n        # Calculate Loss: softmax --> cross entropy loss\n        loss = criterion(outputs, labels)\n        \n        # Getting gradients w.r.t. parameters\n        loss.backward()\n        \n        # Updating parameters\n        optimizer.step()\n        \n        iter += 1\n        \n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Resize image\n                images = images.view(-1, seq_dim, input_dim)\n                \n                # Forward pass only to get logits\/output\n                outputs = model(images)\n                \n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n                \n                # Total number of labels\n                total += labels.size(0)\n                \n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ total\n            \n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))","6b930458":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\ntrain_dataset = dsets.FashionMNIST(root='.\/data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.FashionMNIST(root='.\/data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters \/ (len(train_dataset) \/ batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(LSTMModel, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n        \n        # Number of hidden layers\n        self.layer_dim = layer_dim\n        \n        # Building your LSTM\n        # batch_first=True causes input\/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n        \n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n        \n        # Initialize cell state\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n        \n        # One time step\n        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n        # If we don't, we'll backprop all the way to the start even after going through another batch\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n        \n        # Index hidden state of last time step\n        # out.size() --> 100, 28, 100\n        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --> 100, 10\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28\nhidden_dim = 100\nlayer_dim = 3  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\noutput_dim = 10\n\nmodel = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n# JUST PRINTING MODEL & PARAMETERS \nprint(model)\nprint(len(list(model.parameters())))\nfor i in range(len(list(model.parameters()))):\n    print(list(model.parameters())[i].size())\n\n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\n\n# Number of steps to unroll\nseq_dim = 28  \n\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        images = images.view(-1, seq_dim, input_dim).requires_grad_()\n        \n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n        \n        # Forward pass to get output\/logits\n        # outputs.size() --> 100, 10\n        outputs = model(images)\n        \n        # Calculate Loss: softmax --> cross entropy loss\n        loss = criterion(outputs, labels)\n        \n        # Getting gradients w.r.t. parameters\n        loss.backward()\n        \n        # Updating parameters\n        optimizer.step()\n        \n        iter += 1\n        \n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                # Load images to a Torch Variable\n                images = images.view(-1, seq_dim, input_dim).requires_grad_()\n                \n                # Forward pass only to get logits\/output\n                outputs = model(images)\n                \n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n                \n                # Total number of labels\n                total += labels.size(0)\n                \n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ total\n            \n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))","253ae419":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\n'''\nSTEP 1: LOADING DATASET\n'''\ntrain_dataset = dsets.FashionMNIST(root='.\/data', \n                            train=True, \n                            transform=transforms.ToTensor(),\n                            download=True)\n\ntest_dataset = dsets.FashionMNIST(root='.\/data', \n                           train=False, \n                           transform=transforms.ToTensor())\n\n'''\nSTEP 2: MAKING DATASET ITERABLE\n'''\n\nbatch_size = 100\nn_iters = 3000\nnum_epochs = n_iters \/ (len(train_dataset) \/ batch_size)\nnum_epochs = int(num_epochs)\n\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n                                           batch_size=batch_size, \n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n                                          batch_size=batch_size, \n                                          shuffle=False)\n\n'''\nSTEP 3: CREATE MODEL CLASS\n'''\n\nclass LSTMModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n        super(LSTMModel, self).__init__()\n        # Hidden dimensions\n        self.hidden_dim = hidden_dim\n        \n        # Number of hidden layers\n        self.layer_dim = layer_dim\n        \n        # Building your LSTM\n        # batch_first=True causes input\/output tensors to be of shape\n        # (batch_dim, seq_dim, feature_dim)\n        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n        \n        # Readout layer\n        self.fc = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x):\n        # Initialize hidden state with zeros\n        #######################\n        #  USE GPU FOR MODEL  #\n        #######################\n        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n        \n        # Initialize cell state\n        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n        \n        # One time step\n        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n        \n        # Index hidden state of last time step\n        # out.size() --> 100, 28, 100\n        # out[:, -1, :] --> 100, 100 --> just want last time step hidden states! \n        out = self.fc(out[:, -1, :]) \n        # out.size() --> 100, 10\n        return out\n\n'''\nSTEP 4: INSTANTIATE MODEL CLASS\n'''\ninput_dim = 28\nhidden_dim = 100\nlayer_dim = 3  # ONLY CHANGE IS HERE FROM ONE LAYER TO TWO LAYER\noutput_dim = 10\n\nmodel = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim)\n\n#######################\n#  USE GPU FOR MODEL  #\n#######################\n    \ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n    \n'''\nSTEP 5: INSTANTIATE LOSS CLASS\n'''\ncriterion = nn.CrossEntropyLoss()\n\n'''\nSTEP 6: INSTANTIATE OPTIMIZER CLASS\n'''\nlearning_rate = 0.1\n\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n\n'''\nSTEP 7: TRAIN THE MODEL\n'''\n\n# Number of steps to unroll\nseq_dim = 28  \n\niter = 0\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Load images as Variable\n        #######################\n        #  USE GPU FOR MODEL  #\n        #######################\n        images = images.view(-1, seq_dim, input_dim).requires_grad_().to(device)\n        labels = labels.to(device)\n            \n        # Clear gradients w.r.t. parameters\n        optimizer.zero_grad()\n        \n        # Forward pass to get output\/logits\n        # outputs.size() --> 100, 10\n        outputs = model(images)\n        \n        # Calculate Loss: softmax --> cross entropy loss\n        loss = criterion(outputs, labels)\n        \n        # Getting gradients w.r.t. parameters\n        loss.backward()\n        \n        # Updating parameters\n        optimizer.step()\n        \n        iter += 1\n        \n        if iter % 500 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Iterate through test dataset\n            for images, labels in test_loader:\n                #######################\n                #  USE GPU FOR MODEL  #\n                #######################\n                images = images.view(-1, seq_dim, input_dim).to(device)\n                labels = labels.to(device)\n                \n                # Forward pass only to get logits\/output\n                outputs = model(images)\n                \n                # Get predictions from the maximum value\n                _, predicted = torch.max(outputs.data, 1)\n                \n                # Total number of labels\n                total += labels.size(0)\n                \n                # Total correct predictions\n                #######################\n                #  USE GPU FOR MODEL  #\n                #######################\n                if torch.cuda.is_available():\n                    correct += (predicted.cpu() == labels.cpu()).sum()\n                else:\n                    correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ total\n            \n            # Print Loss\n            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))","4dacf467":"### Step 5: Instantiate Loss Class\n- Long Short-Term Memory Neural Network: **Cross Entropy Loss**\n    - _Recurrent Neural Network_: **Cross Entropy Loss**\n    - _Convolutional Neural Network_: **Cross Entropy Loss**\n    - _Feedforward Neural Network_: **Cross Entropy Loss**\n    - _Logistic Regression_: **Cross Entropy Loss**\n    - _Linear Regression_: **MSE**\n    ","bd9337c7":"#### Parameters (Layer 1)\n- **Input** $\\rightarrow$ **Gates**\n    - [400, 28]\n    - [400]\n- **Hidden State** $\\rightarrow$ **Gates**\n    - [400,100]\n    - [400]\n   \n#### Parameters (Layer 2)\n- **Input** $\\rightarrow$ **Gates** \n    - [400, 100]\n    - [400]\n- **Hidden State** $\\rightarrow$ **Gates** \n    - [400,100]\n    - [400]\n\n  \n\n#### Parameters (Layer 3)\n- **Input** $\\rightarrow$ **Gates** \n    - [400, 100]\n    - [400]\n- **Hidden State** $\\rightarrow$ **Gates** \n    - [400,100]\n    - [400]\n\n    \n#### Parameters (Readout Layer)\n- **Hidden State** $\\rightarrow$ **Output**\n    - [10, 100]\n    - [10]","bdbfafdb":"- RNN transition to LSTM\n- LSTM Models in PyTorch\n    - Model A: 1 Hidden Layer LSTM\n    - Model B: 2 Hidden Layer LSTM\n    - Model C: 3 Hidden Layer LSTM\n- Models Variation in **Code**\n    - Modifying only step 4\n- Ways to Expand Model\u2019s **Capacity**\n    - More **hidden units**\n    - More **hidden layers**\n- **Cons** of Expanding Capacity\n    - Need more **data**\n    - Does not necessarily mean higher **accuracy**\n- **GPU** Code\n    - 2 things on GPU\n        - **model**\n        - **tensors**\n    - Modifying only **Step 3, 4 and 7**\n- **7 Step** Model Building Recap\n    - Step 1: Load Dataset\n    - Step 2: Make Dataset Iterable\n    - **Step 3: Create Model Class**\n    - **Step 4: Instantiate Model Class**\n    - Step 5: Instantiate Loss Class\n    - Step 6: Instantiate Optimizer Class\n    - **Step 7: Train Model**\n\n","cdbae314":"### Model C: 3 Hidden Layer \n- Unroll 28 time steps\n    - Each step input size: 28 x 1\n    - Total per unroll: 28 x 28\n        - Feedforward Neural Network inpt size: 28 x 28 \n- **3 Hidden layer**\n\n### Steps\n- Step 1: Load Dataset\n- Step 2: Make Dataset Iterable\n- Step 3: Create Model Class\n- **Step 4: Instantiate Model Class**\n- Step 5: Instantiate Loss Class\n- Step 6: Instantiate Optimizer Class\n- Step 7: Train Model\n","664bae82":"### Step 1: Loading MNIST Train Dataset","81b75bb5":"### Parameters In-Depth","40a24d75":"## 3. Building a Recurrent Neural Network with PyTorch (GPU)\n\n### Model A: 3 Hidden Layers\n\nGPU: 2 things must be on GPU\n- `model`\n- `tensors`\n\n### Steps\n- Step 1: Load Dataset\n- Step 2: Make Dataset Iterable\n- **Step 3: Create Model Class**\n- **Step 4: Instantiate Model Class**\n- Step 5: Instantiate Loss Class\n- Step 6: Instantiate Optimizer Class\n- **Step 7: Train Model**","343405b8":"### Step 4: Instantiate Model Class\n- 28 time steps\n    - Each time step: input dimension = 28\n- 1 hidden layer\n- Fashion MNIST 0-9 categories $\\rightarrow$ output dimension = 10","2ece1a47":"# Summary","417fd840":"### Step 7: Train Model\n- Process \n    1. **Convert inputs\/labels to variables**\n        - LSTM Input: (1, 28)\n        - RNN Input: (1, 28)\n        - CNN Input: (1, 28, 28) \n        - FNN Input: (1, 28*28)\n    2. Clear gradient buffets\n    3. Get output given inputs \n    4. Get loss\n    5. Get gradients w.r.t. parameters\n    6. Update parameters using gradients\n        - `parameters = parameters - learning_rate * parameters_gradients`\n    7. REPEAT","c9ad4093":"\n# Long Short-Term Memory (LSTM) network with PyTorch\n## 1. About LSTMs: Special RNN\n- Capable of learning long-term dependencies\n- LSTM = RNN on super juice\n\n### 1.1 RNN Transition to LSTM","e4d0adcc":"#### Parameters\n- **Input** $\\rightarrow$ **Gates**\n    - $[400, 28] \\rightarrow w_1, w_3, w_5, w_7$\n    - $[400] \\rightarrow b_1, b_3, b_5, b_7$\n- **Hidden State** $\\rightarrow$ **Gates**\n    - $[400,100] \\rightarrow w_2, w_4, w_6, w_8$\n    - $[400] \\rightarrow b_2, b_4, b_6, b_8$\n- **Hidden State** $\\rightarrow$ **Output**\n    - $[10, 100] \\rightarrow w_9 $\n    - $[10] \\rightarrow b_9$","a6d6e2c8":"### Deep Learning\n- 2 ways to expand a recurrent neural network\n    - More hidden units\n         - `(o, i, f, g) gates`\n    - More hidden layers\n- Cons\n    - Need a larger dataset\n        - Curse of dimensionality\n    - Does not necessarily mean higher accuracy","1e2e78fe":"### Step 3: Create Model Class","2b1b15ba":"### Step 6: Instantiate Optimizer Class\n- Simplified equation\n    - $\\theta = \\theta - \\eta \\cdot \\nabla_\\theta $\n        - $\\theta$: parameters (our variables)\n        - $\\eta$: learning rate (how fast we want to learn)\n        - $\\nabla_\\theta$: parameters' gradients\n- Even simplier equation\n    - `parameters = parameters - learning_rate * parameters_gradients`\n    - **At every iteration, we update our model's parameters**","3d753522":"## 2. Building an LSTM with PyTorch\n\n### Model A: 1 Hidden Layer\n- Unroll 28 time steps\n    - Each step input size: 28 x 1\n    - Total per unroll: 28 x 28\n        - Feedforward Neural Network input size: 28 x 28 \n- 1 Hidden layer\n\n### Steps\n- Step 1: Load Dataset\n- Step 2: Make Dataset Iterable\n- Step 3: Create Model Class\n- Step 4: Instantiate Model Class\n- Step 5: Instantiate Loss Class\n- Step 6: Instantiate Optimizer Class\n- Step 7: Train Model","656d2249":"### Comparison with RNN\n| Model A RNN | Model B RNN   | Model C RNN | \n|------|------|\n|   ReLU | ReLU | Tanh |\n| 1 Hidden Layer | 2 Hidden Layers | 3 Hidden Layers | \n| 100 Hidden Units | 100 Hidden Units |100 Hidden Units |\n| 92.48% | 95.09% | 95.54% | \n\n| Model A LSTM | Model B LSTM   | Model C LSTM | \n|------|------|\n| 1 Hidden Layer | 2 Hidden Layers | 3 Hidden Layers |\n| 100 Hidden Units | 100 Hidden Units |100 Hidden Units |\n| 96.05% | 95.24% | 91.22% | \n\n\n","279ea470":"### Model B: 2 Hidden Layer \n- Unroll 28 time steps\n    - Each step input size: 28 x 1\n    - Total per unroll: 28 x 28\n        - Feedforward Neural Network inpt size: 28 x 28 \n- **2 Hidden layer**\n\n### Steps\n- Step 1: Load Dataset\n- Step 2: Make Dataset Iterable\n- Step 3: Create Model Class\n- **Step 4: Instantiate Model Class**\n- Step 5: Instantiate Loss Class\n- Step 6: Instantiate Optimizer Class\n- Step 7: Train Model","0b754c31":"### Step 2: Make Dataset Iterable","7c78209d":"#### Parameters (Layer 1)\n- **Input** $\\rightarrow$ **Gates**\n    - $[400, 28]$\n    - $[400]$\n- **Hidden State** $\\rightarrow$ **Gates**\n    - $[400,100]$\n    - $[400]$\n   \n#### Parameters (Layer 2)\n- **Input** $\\rightarrow$ **Gates** \n    - $[400, 100]$\n    - $[400]$\n- **Hidden State** $\\rightarrow$ **Gates** \n    - $[400,100]$\n    - $[400]$\n   \n#### Parameters (Readout Layer)\n- **Hidden State** $\\rightarrow$ **Output**\n    - $[10, 100]$\n    - $[10]$"}}