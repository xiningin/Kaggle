{"cell_type":{"8ea894f1":"code","31fdf17d":"code","614dace7":"code","6874d1f9":"code","b908d4e5":"code","1f694b61":"code","6d3ff221":"code","deda98ab":"code","8b211601":"code","c49b5e07":"code","291d1ade":"code","5ebaafd0":"code","bb440a0f":"code","9ca2ddc3":"code","aa694308":"code","28983b3e":"code","1e59db41":"code","c2c1dda3":"code","aef1ece4":"code","8d82a14e":"code","26931072":"code","6399ad9f":"code","82ab72db":"code","d7e16fe0":"code","3a2a8ce9":"code","3b474aa2":"code","6645fc59":"code","38b653c2":"markdown","ff286902":"markdown","543d3478":"markdown","24e52cac":"markdown","4a544af1":"markdown","64d31cb2":"markdown","21140a41":"markdown","02d69bd6":"markdown","e4e6162c":"markdown","fe749674":"markdown","fdc752ec":"markdown","68603638":"markdown","4fb3a3f9":"markdown","cc20d1e0":"markdown","6972c1cc":"markdown","f5ba80f0":"markdown"},"source":{"8ea894f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","31fdf17d":"# We are reading our data That we Uploaded \ndf = pd.read_csv(\"..\/input\/heart-dissease\/heart_Disease.csv\")","614dace7":"#Displaying Data Records\ndf","6874d1f9":"#Displaying First Five Records\ndf.head()","b908d4e5":"#Displaying First Five Records\ndf.tail()","1f694b61":"#Displaying Count Of Each Instance in Dataset\ndf.count()","6d3ff221":"#Displaying Count Of Null Values ( if any ):\ndf.isna().sum()","deda98ab":"#Summary Of Dataset \ndf.describe()","8b211601":"#Displaying Count Of Patients Have \/ Not Haven\u2019t Heart Diseases\ndf.target.value_counts()","c49b5e07":"#BarPlot For Count Of Patients Have \/ Not Haven\u2019t Heart Diseases\nsns.countplot(x=\"target\", data=df, palette=\"bwr\")\nplt.show()","291d1ade":"#Displaying Percentage Of Patients Having \/ Not Having Heart Diseases\ncountNoDisease = len(df[df.target == 0])\ncountHaveDisease = len(df[df.target == 1])\nprint(\"Percentage of Patients Haven't Heart Disease: {:.2f}%\".format((countNoDisease \/ (len(df.target))*100)))\nprint(\"Percentage of Patients Have Heart Disease: {:.2f}%\".format((countHaveDisease \/ (len(df.target))*100)))","5ebaafd0":"#Displaying Percentage Of Male\/Female Patients\ncountFemale = len(df[df.sex == 0])\ncountMale = len(df[df.sex == 1])\nprint(\"Percentage of Female Patients: {:.2f}%\".format((countFemale \/ (len(df.sex))*100)))\nprint(\"Percentage of Male Patients: {:.2f}%\".format((countMale \/ (len(df.sex))*100)))","bb440a0f":"#BarPlot For Count Based Of Sex( Male\/Female )\nsns.countplot(x='sex', data=df, palette=\"mako_r\")\nplt.xlabel(\"Sex (0 = female, 1= male)\")\nplt.show()","9ca2ddc3":"#Histogram for Dataset\ndf.plot.hist()","aa694308":"#BoxPlot  for Dataset\ndf.plot.box()","28983b3e":"#Histogram for Heart Disease Frequency According to  Chest Pain (CP) Type \npd.crosstab(df.cp,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#aa4711','#11aa93' ])\nplt.title('Heart Disease Frequency According To Chest Pain Type')\nplt.xlabel('Chest Pain Type')\nplt.xticks(rotation = 0)\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","1e59db41":"#Scatterplot for Maximum Heart Rate According Disease\/Not Disease \nplt.scatter(x=df.age[df.target==1], y=df.thalach[(df.target==1)], c=\"red\")\nplt.scatter(x=df.age[df.target==0], y=df.thalach[(df.target==0)])\nplt.legend([\"Disease\", \"Not Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Maximum Heart Rate\")\nplt.show()","c2c1dda3":"#Histogram for Heart Disease Frequency For Ages\npd.crosstab(df.age,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","aef1ece4":"y = df.target.values\nx_data = df.drop(['target'], axis = 1)","8d82a14e":"# Normalize\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data)).values","26931072":"#We will split our data. 80% of our data will be train data and 20% of it will be test data.\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)","6399ad9f":"#Transpose Matrices for Avoiding Confusion\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T","82ab72db":"#Implementing K-Nearest Neighbour (KNN) Model Classification :\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\nknn.fit(x_train.T, y_train.T)\nprediction = knn.predict(x_test.T)\nprint(\"{} NN Score: {:.2f}%\".format(2, knn.score(x_test.T, y_test.T)*100))","d7e16fe0":"# Trying to find Best Feasible K Value\naccuracies = {}\nscoreList = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(x_train.T, y_train.T)\n    scoreList.append(knn2.score(x_test.T, y_test.T))\n    \nplt.plot(range(1,20), scoreList)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\n#Accuracy Predictor\nacc = max(scoreList)*100\naccuracies['KNN'] = acc\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))","3a2a8ce9":"#Implementing Naive Bayes Classifier\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train.T, y_train.T)\n#Accuracy Predictor\nacc = nb.score(x_test.T,y_test.T)*100\naccuracies['Naive Bayes'] = acc\nprint(\"Accuracy of Naive Bayes: {:.2f}%\".format(acc))","3b474aa2":"#Implementing Decision Tree Model \nfrom sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train.T, y_train.T)\n#Accuracy Predictor\nacc = dtc.score(x_test.T, y_test.T)*100\naccuracies['Decision Tree'] = acc\nprint(\"Decision Tree Test Accuracy {:.2f}%\".format(acc))","6645fc59":"#Comparing The Proposed Model Classifications\ncolors = [\"red\",\"blue\",\"green\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","38b653c2":"**Decision Tree Model**\n\n**Description :**\n* Decision tree is used to resolve the complex troubles. It resembles tree-like shape. \n* The essential additives of choice tree are selection node, nodes, and root. \n* Algorithms particularly used by selection tree are ID3, CART, CY3, C5.Zero and J48.These algorithms used by selection tree are used to analyse the dataset\n\n**Strengths of Decision Tree** :\n\nDecision Tree Deals With :\n  * Categorical attributes with many distinct values\n  * Variables with nonlinear effect on outcome\n  \n-> Let's see the Implementation of Decision Tree.","ff286902":"**K-Nearest Neighbour (KNN) Model Classification**\n\n**Description** :\n\n* KNN is easy and supervised studying set of rules and it could keep all of the whole dataset consequently there may be no studying required.\n* KNN is used in statistical estimation and this classification is solely dependent on votes of the neighbours.\n* **Advantages** : No assumptions about data \u2014 useful, for example, for nonlinear data\n* **Applications** : Using KNN Algorithm Researchers are using data mining techniques in the medical diagnosis of several diseases such as diabetes ,stroke , cancer , and heart disease respectively.\n\n-> Let's see the Implementation of KNN Classification.","543d3478":"**Naive Bayes Model Classification**\n\n**Description :**\n* In machine learning, Naive Bayes is a classification technique is based on Bayes theorem and not only simple classifier but it is also a simple probabilistic classifier with strong and independent assumptions.\n* The most important aspect of naive Bayes is even the attributes are independent still we can predict the classification.\n* **Na\u00efve Bayes Classifier** : Applied to learn tasks and each instance x is described by the conjunction of attribute values and target function f(x) of any value in some finite set V. It is based on the simplifying the assumption that all the attribute values are conditionally independent given the target value.\n\n**Strengths of Naive Bayes Classifier** :\n* It handles both continuous and discrete data.\n* It is fast and can be used to make real-time predictions.\n\n**Real World Applications**\n* Face Recognition : As a classifier, it is used to identify the faces or its other features, like nose, mouth, eyes, etc.\n* Weather Prediction : It can be used to predict if the weather will be good or bad.\n\n-> Let's see the Implementation of Naive Bayes Classifier.","24e52cac":"**Comparison of Proposed Models Accuracy Rates**\n\nHere,we Visualized the Accuracies for Comparing the Proposed Models to check the outcome of Effectiveness respectively.","4a544af1":"> **DATA CONTAINS :**\n* age - age in years\n* sex - (1 = male; 0 = female)\n* cp - chest pain type\n* trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n* chol - serum cholestoral in mg\/dl\n* fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* restecg - resting electrocardiographic results\n* thalach - maximum heart rate achieved\n* exang - exercise induced angina (1 = yes; 0 = no)\n* oldpeak - ST depression induced by exercise relative to rest\n* slope - the slope of the peak exercise ST segment\n* ca - number of major vessels (0-3) colored by flourosopy\n* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect\n* target - have disease or not (1=yes, 0=no) \n\n**Note **: In this dataset we have were given 14 attributes with three hundred times has been used as the primary database for the training and trying out of the developed gadget .","64d31cb2":"**Building of Models**\n\nHere,we are intended to Build Models to Predict The Accuracies\n\nThe Proposed Models are as follows :\n* K-Nearest Neighbour (KNN) Classification.\n* Naive Bayes Classifier.\n* Decision Tree Algorithm.","21140a41":"**Visualization Techniques**\n\n**Description** :\n\nData visualization is the discipline of trying to understand data by placing it in a visual context so that patterns, trends and correlations that might not otherwise be detected can be exposed. \n\n-> Python offers multiple great graphing libraries that come packed with lots of different features.\n\n**Basic Techniques** :\n\n* **Histogram** :\nIn Pandas, we can create a Histogram with the plot.hist method\n* **Bar Chart** :\nTo plot a bar-chart we can use the plot.bar() method.\n* **Box Plot** :\nA Box Plot is a graphical method of displaying the five-number summary. We can create box plots using seaborns sns.boxplot method","02d69bd6":"**Conclusion**\n\nwe just performed a survey associated with prediction of coronary heart illnesses the use of records mining strategies and evaluation had been achieved in step with the accuracy and the records mining techniques are KNN,Naive Bayes,Decision Tree respectively.\n\nBy Taking The Predicted Accuracies into Considerations ,We Concluded that \n* KNN is model Which Shown 90.16% Accuracy rate respectively.\n* Naive Bayes model Which Shown 85.25% Accuracy rate respectively.\n* Decision Tree model  Shown 75.41% Accuracy rate respectively.\n\nHere we finished some statistical checks to find out viable output predictions with the assist of some in build dataset and we as compared with the strategies with the help of the facts set. While we examine the strategies different techniques may be better or poor.\n\n**Future Scope** : we will Perform the more Predictions by adopting new Enhanced Machine Learning Techniques\/Algorithms like \n* Logistic Regression,\n* K-Means Clustering, \n* Random Forest, and\n* Neural Networks etc.,","e4e6162c":"**Data PreProcessing Techniques**\n\n**Key Points** :\n* dataframe_name.head() : This function returns the first n rows for the object based on position. \n* dataframe_name.tail() : This function returns the last n rows for the object based on position. \n* dataframe_name.count() : This function returns the Count Of Each Instance in Dataset.\n* datafram_name.isna().sum() : This function returns the Count Of Null Values Presented in Dataset.\n* dataframe_name.describe() : This function returns the Summary of Dataset (Summary : count,mean,std,min,25%,50%,75%,max)","fe749674":"**Normalization Of Data**\n  \nNormalization Formula :  Xnorm = ((x-x.min)\/(x.max-x.min))","fdc752ec":"-> By Implementing Naive Bayes lassifier We got Accuracy Rate : **85.25**%","68603638":"**INTRODUCTION**\n\n* We have a data which classified if patients have heart disease or not according to features in it.\n* The task can be performed through the usage of device mastering algorithms to locate the accuracy of the dataset which incorporate the attributes like BP,sugar ,heartbeat and many others.\n* There are  distinct classi\ufb01ers, specifically K \u2013Nearest Neighbour (KNN) , Decision Tree (DT), Naive Bayes (NB) been used to evaluate the accuracy and check out.","4fb3a3f9":"**Data Exploration** : Understanding Relationships among Variables\n* **Variable Selection** : All the variables are Required in order to Predict Diseases\n* **Model Selection** : KNN  , Na\u00efve Bayes , Decision Tree\n","cc20d1e0":"-> By Implementing Decision Tree We got Accuracy Rate : **75.41**%","6972c1cc":"-> By Implementing KNN Classification We got Accuracy Rate : **90.16**%","f5ba80f0":"**Notes**:\n* read_csv is an important pandas function to read csv(comma-separated values)files and do operations on it."}}