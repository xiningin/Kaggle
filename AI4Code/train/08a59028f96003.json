{"cell_type":{"929564d4":"code","dd4010b0":"code","c23b6f1d":"code","5b6622f9":"code","9ce35f85":"code","b55de9f6":"code","29dc70f3":"code","de01a80c":"code","10da9591":"code","50502470":"code","bd6a4881":"code","b2ecf539":"code","e09e0de5":"code","677ba989":"code","93b99e0d":"code","8a097e42":"code","5959638d":"code","f7aa1832":"code","904b493d":"code","52464133":"code","ca040e81":"code","75b684b6":"code","2a607b1d":"code","0ebd5959":"code","d745736d":"code","e7f5f3cd":"code","c38d9ace":"code","43b303ba":"code","eb98d9da":"code","e9bfc82e":"code","f719f771":"markdown","97000507":"markdown","96fcdc35":"markdown","5e652a35":"markdown","a49d3402":"markdown","f5c6421c":"markdown","abfbc742":"markdown","d3000efe":"markdown","ed8dab80":"markdown","51801e1b":"markdown","9082a084":"markdown","d0b2a183":"markdown","5c970b53":"markdown","6328c5c3":"markdown","1b92a2ed":"markdown","da3351e4":"markdown","2d3c4114":"markdown","f46a708e":"markdown","a738f8a1":"markdown","5bafc624":"markdown","126f50fd":"markdown","39b5f1de":"markdown","9e80772d":"markdown","9ea00544":"markdown","79d7f0a3":"markdown","17213016":"markdown","06c57c77":"markdown","fe9c0812":"markdown","33cd5a8b":"markdown","dfaee30f":"markdown","243f8e49":"markdown","6466eba5":"markdown","1d52be7d":"markdown"},"source":{"929564d4":"# Importing Libraries\nimport string\nimport numpy as np\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom pickle import dump, load\nimport numpy as np\nfrom keras.applications.xception import Xception, preprocess_input\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.layers.merge import add\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense, LSTM, Embedding, Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler\n\n# small library for seeing the progress of loops.\nfrom tqdm.notebook import tqdm\ntqdm().pandas()","dd4010b0":"# loading the file and reading a content as a string\ndef load_document(filename):\n    file=open(filename,'r') #open file in read only mode\n    text=file.read()        #reading the file content\n    file.close()\n    return text           # return the file content as a string\n\n# mapping each image to their captions\ndef map_img_to_captions(filename):\n    file=load_document(filename)\n    captions=file.split('\\n')\n    img_to_its_caption={}    # dictionary that map the image with its caption\n    for caption in captions[:-1]:\n        img, caption = caption.split('\\t')\n        if img[:-2] not in img_to_its_caption:\n            img_to_its_caption[img[:-2]]=[caption]\n        else:\n            img_to_its_caption[img[:-2]].append(caption)\n    return img_to_its_caption\n    ","c23b6f1d":"#Data cleaning- function for cleaning text like lower casing, removing puntuations and words containing numbers\n  #string.punctuation gives all set of punctuation\ndef preprocessing_text(captions):\n    # punctuation is mapped to none\n    table=str.maketrans('','',string.punctuation)  #The string maketrans() method returns a mapping table for translation usable for translate() method\n    for img,caps in captions.items():\n        for i,img_caption in enumerate(caps):\n            \n            img_caption.replace(\"-\",\" \")\n            cleaned_text = img_caption.split()\n            \n            #convert each caption to lowercase\n            cleaned_text=[word.lower() for word in cleaned_text]\n            \n            #remove punctuation from each token\n            cleaned_text=[word.translate(table) for word in cleaned_text]\n            \n            #remove hanging 's and a \n            cleaned_text = [word for word in cleaned_text if(len(word)>1)]\n            \n            #remove tokens with numbers in them\n            cleaned_text = [word for word in cleaned_text if(word.isalpha())]\n            #convert back to string\n            img_caption = ' '.join(cleaned_text)\n            captions[img][i]= img_caption\n            \n    return captions","5b6622f9":"pwd","9ce35f85":"# function that will separate all the unique words and create the vocabulary from all the descriptions.\ndef text_vocabulary(descriptions):\n    # build vocabulary of all unique words\n    vocabulary=set()\n    for key in descriptions.keys():\n        [vocabulary.update(d.split()) for d in descriptions[key]]\n        \n    return vocabulary\n\n# Saving clean and processed descriptions in one file\ndef saving_descriptions(descriptions,filename):\n    captions=list()\n    for key,cap_list in descriptions.items():\n        for caption in cap_list:\n            captions.append(key+'\\t'+caption)\n            \n    data=\"\\n\".join(captions)\n    file=open(filename,'w')\n    file.write(data)\n    file.close()\n    ","b55de9f6":"# We will define all the paths to the files that we require and save the images id and their captions.\n\ndataset_text = '..\/input\/flickr8k-imageswithcaptions\/Flickr8k_text\/'\ndataset_images = '..\/input\/flickr8k-imageswithcaptions\/Flickr8k_Dataset\/Flicker8k_Dataset\/'\n\n#we prepare our text data\nfilename = dataset_text  + \"Flickr8k.token.txt\"\n#loading the file that contains all data\n#mapping them into descriptions dictionary img to 5 captions\ndescriptions =  map_img_to_captions(filename)\nprint(\"Length of descriptions =\" ,len(descriptions))\n\n#cleaning the descriptions\nclean_descriptions = preprocessing_text(descriptions)\n#building vocabulary \nvocabulary = text_vocabulary(clean_descriptions)\nprint(\"Length of vocabulary = \", len(vocabulary))\n#saving each description to file \nsaving_descriptions(clean_descriptions, \"descriptions.txt\")","29dc70f3":"pwd","de01a80c":"os.listdir()","10da9591":"### Reading about particular image and its captions\nx=plt.imread(\"..\/input\/flickr8k-imageswithcaptions\/Flickr8k_Dataset\/Flicker8k_Dataset\/10815824_2997e03d76.jpg\")\nplt.imshow(x)\nplt.show()\nprint(\"max_pixel=\",x.max(),\"min_pixel=\",x.min())","50502470":"def extract_features(directory):\n    XceptionModel=Xception(include_top=False,pooling='avg')\n    features={}\n    for img in tqdm(os.listdir(directory)):\n        filename=directory + \"\/\" +img\n        image=Image.open(filename)      #Opens and identifies the given image file.\n        image=image.resize((299,299))   # resizing the image to make as a input to Xception model\n        #Expand the shape of an array.\n        image=np.expand_dims(image,axis=0)  #Insert a new axis that will appear at the axis position in the expanded array shape.\n        #normalizing the image\n        image=image\/255\n        \n        feature=XceptionModel.predict(image)\n        features[img]=feature\n    return features\n\n","bd6a4881":"#2048 feature vector\nfeatures = extract_features(dataset_images)\ndump(features, open(\"features.p\",\"wb\"))      # write in binary mode","b2ecf539":"# load the text file in a string and will return the list of image names.\ndef load_photos(filename):\n    file=load_document(filename)\n    photos=file.split('\\n')[:-1]\n    return photos\n\ndef load_clean_caption(filename,photos):\n    file=load_document(filename)\n    map_img_to_caps={}           #dictionary that contains captions for each photo from the list of photos\n    lines=file.split('\\n')\n    for line in lines:\n        words=line.split()\n        \n        if len(words)<1:\n            continue\n            \n        img,img_caption=words[0],words[1:]\n        if img in photos:\n            if img not in map_img_to_caps:\n                map_img_to_caps[img] = []\n            desc = '<start> ' + \" \".join(img_caption) + ' <end>'\n            map_img_to_caps[img].append(desc)\n            \n    return map_img_to_caps\n    \ndef load_features(photos):\n    #loading all features\n    all_features=load(open(\"..\/input\/model-requirments\/features.p\",\"rb\"))   #load()-Decode the given Python file-like stream containing a JSON formatted value into Python object.\n    #selecting only needed features for training dataset(only)\n    features = {k:all_features[k] for k in photos}\n    #print(features)\n    return features\n    ","e09e0de5":"filename = dataset_text + \"\/\" + \"Flickr_8k.trainImages.txt\"\n\ntrain_imgs = load_photos(filename)\ntrain_descriptions = load_clean_caption(\"descriptions.txt\", train_imgs)\ntrain_features = load_features(train_imgs)","677ba989":"print(train_features)","93b99e0d":"#converting dictionary to clean list of descriptions\ndef dict_to_list(descriptions):\n    all_desc=[]\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc","8a097e42":"all_desc=dict_to_list(train_descriptions)\nprint(len(all_desc))\nprint(all_desc[0:10])     # sample","5959638d":"#creating tokenizer class \n#this will vectorise text corpus\n#each integer will represent token in dictionary\n\nfrom keras.preprocessing.text import Tokenizer\ndef create_tokenizer(descriptions):\n    desc_list=dict_to_list(descriptions)\n    tokenizer=Tokenizer(filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n',lower=True,split=\" \",char_level=False)  # default parameters\n    tokenizer.fit_on_texts(desc_list)\n    print(tokenizer)\n    return tokenizer\n\n# give each word an index, and store that into tokenizer.p pickle file\ntokenizer = create_tokenizer(train_descriptions)\ndump(tokenizer,open('tokenizer.p','wb'))\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"len of vvocab:\"+str(vocab_size))","f7aa1832":"def max_length(descriptions):\n    desc_list = dict_to_list(descriptions)\n    for d in descriptions:\n        return max(len(d.split()) for d in desc_list)    # calculating maximum of all list length\n    \nmax_length = max_length(descriptions)\nmax_length\n","904b493d":"tokenize_text=load(open(\"tokenizer.p\",\"rb\"))      # read in binary mode of tokenized text","52464133":"#print(tokenize_text.word_index)   # prinitng the tokenized dictionary","ca040e81":"#create input-output sequence pairs from the image description.\n\ndef create_sequences(tokenizer,max_length,desc_list,feature):\n    X1, X2, y = list(), list(), list()\n    # looping through each description of  an image\n    for desc in desc_list:\n        #encoding the sequence\n        seq=tokenizer.texts_to_sequences([desc])[0]\n        #print(seq,\"seq\")   # 5 list(5 captions) of 5 captions containing the words position in vocab.\n        for i in range(1,len(seq)):  \n            # split one Sequence into multiple X,y pairs\n            in_seq,out_seq=seq[:i],seq[i]\n#             print(in_seq,\"in_seq\")\n#             print(out_seq,\"out_seq\")\n            #pad input sequence\n            in_seq=pad_sequences([in_seq],maxlen=max_length)[0]\n            #encode output\n            out_seq=to_categorical([out_seq],num_classes=vocab_size)[0]\n            \n            X1.append(feature)\n            X2.append(in_seq)\n            y.append(out_seq)\n            \n    return np.array(X1),np.array(X2),np.array(y)","75b684b6":"#data generator, used by model.fit_generator()\n\ndef data_generator(descriptions,features,tokenizer,max_len):\n    while 1:\n        for key,description_list in descriptions.items():\n            #retreive photo features\n            feature=features[key][0]\n            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n            yield ([input_image, input_sequence], output_word)     # works when return tuple instead of list","2a607b1d":"#You can check the shape of the input and output for your model\n[a,b],c = next(data_generator(train_descriptions, features, tokenizer, max_length))\nprint(a.shape)\nprint(b.shape)\nprint(c.shape)\n#((47, 2048), (47, 32), (47, 7577))\nprint(a)\nprint(b)\nprint(c)","0ebd5959":"#Model\nfrom keras.utils import plot_model\n\n# defining the model\ndef caption_model(vocab_size,max_len):\n    \n    \n    # features from the CNN model squeezed from 2048 to 256 nodes\n    inputs1 = Input(shape=(2048,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(256, activation='relu')(fe1)\n    \n    # LSTM sequence model\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n    se3 = LSTM(256)(se2)\n    \n    # Merging both models\n    decoder1 = add([fe2, se3])\n    decoder2 = Dense(256, activation='relu')(decoder1)\n    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n    \n    # tie it together [image, seq] [word]\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    # summarize model\n    print(model.summary())\n    plot_model(model, to_file='model.png', show_shapes=True)\n    \n    return model","d745736d":"# train our model\nprint('Dataset: ', len(train_imgs))\nprint('Descriptions: train=', len(train_descriptions))\nprint('Photos: train=', len(train_features))\nprint('Vocabulary Size:', vocab_size)\nprint('Description Length: ', max_length)\n\nmodel = caption_model(vocab_size, max_length)\nepochs = 15\nsteps = len(train_descriptions)\n\nfilepath_loss=\"loss_on_val_set.hd5\"\ncheckpoint_loss = ModelCheckpoint(filepath_loss, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n\ngenerator = data_generator(train_descriptions, train_features, tokenizer, max_length)\nmodel.fit(generator, epochs=epochs, steps_per_epoch= steps, verbose=1,callbacks=[checkpoint_loss])\n\nmodel.save(\"caption_model.h5\")","e7f5f3cd":"model.save(\"caption_model.h5\")","c38d9ace":"pip install gTTS","43b303ba":"import IPython\nimport random","eb98d9da":"# Testing the model\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport argparse\n# ap = argparse.ArgumentParser()\n# ap.add_argument('-i', '--image', required=True, help=\"Image Path\")\n# args = vars(ap.parse_args())\n\npath = '..\/input\/flickr8k-imageswithcaptions\/Flickr8k_Dataset\/Flicker8k_Dataset'\n\n# img_path = '..\/input\/flickr8k-imageswithcaptions\/Flickr8k_Dataset\/Flicker8k_Dataset\/218342358_1755a9cce1.jpg'\n\nfiles=os.listdir(path)\nd=random.choice(files)     #randomly choosing a single image\nimg_path=path + '\/'+d\n\n\n\ndef extract_features(filename, model):\n        try:\n            image = Image.open(filename)\n        except:\n            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n        image = image.resize((299,299))\n        image = np.array(image)\n        # for images that has 4 channels, we convert them into 3 channels\n        if image.shape[2] == 4: \n            image = image[..., :3]\n        image = np.expand_dims(image, axis=0)\n        image = image\/127.5\n        image = image - 1.0\n        feature = model.predict(image)\n        return feature\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\ndef generate_desc(model, tokenizer, photo, max_length):\n    in_text = 'start'\n    for i in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        pred = model.predict([photo,sequence], verbose=0)\n        pred = np.argmax(pred)\n        word = word_for_id(pred, tokenizer)\n        if word is None:\n            break\n        in_text += ' ' + word\n        if word == 'end':\n            break\n    return in_text\n\n\n\nmax_length = 32\ntokenizer = load(open(\"tokenizer.p\",\"rb\"))\nmodel = load_model('.\/caption_model.h5')\nxception_model = Xception(include_top=False, pooling=\"avg\")\nphoto = extract_features(img_path, xception_model)\nimg = Image.open(img_path)\ndescription = generate_desc(model, tokenizer, photo, max_length)\nprint(\"\\n\\n\")\nprint(description)\nplt.imshow(img)\n# Import the required module for text  \n# to speech conversion \nfrom gtts import gTTS \n\n\ndescription=description[6:len(description)-4]\n\nlanguage = \"en\"\nspeech = gTTS(text = description, lang = language, slow = False)\nspeech.save(\"Audio.mp3\")\n\n# Playing the converted file \nIPython.display.Audio(\"Audio.mp3\")\n\n","e9bfc82e":"filename=''\n","f719f771":"1001773457_577c3a7d70.jpg#0 &emsp;\tA black dog and a spotted dog are fighting <br>\n1001773457_577c3a7d70.jpg#1 &emsp;\tA black dog and a tri-colored dog playing with each other on the road .<br>\n1001773457_577c3a7d70.jpg#2 &emsp;\tA black dog and a white dog with brown spots are staring at each other in the street .<br>\n1001773457_577c3a7d70.jpg#3 &emsp;\tTwo dogs of different breeds looking at each other on the road .<br>\n1001773457_577c3a7d70.jpg#4 &emsp;\tTwo dogs on pavement moving toward each other .<br>\n1002674143_1b742ab4b8.jpg#0 &emsp;\tA little girl covered in paint sits in front of a painted rainbow with her hands in a bowl .<br>\n1002674143_1b742ab4b8.jpg#1 &emsp;\tA little girl is sitting in front of a large painted rainbow .<br>\n1002674143_1b742ab4b8.jpg#2 &emsp;\tA small girl in the grass plays with fingerpaints in front of a white canvas with a rainbow on it .<br>\n1002674143_1b742ab4b8.jpg#3 &emsp;\tThere is a girl with pigtails sitting in front of a rainbow painting .<br>\n1002674143_1b742ab4b8.jpg#4 &emsp;\tYoung girl with pigtails painting outside in the grass .<br>\n1003163366_44323f5815.jpg#0 &emsp;\tA man lays on a bench while his dog sits by him .<br>\n1003163366_44323f5815.jpg#1 &emsp;\tA man lays on the bench to which a white dog is also tied .<br>\n1003163366_44323f5815.jpg#2 &emsp;\ta man sleeping on a bench outside with a white and black dog sitting next to him .<br>\n1003163366_44323f5815.jpg#3 &emsp;\tA shirtless man lies on a park bench with his dog .<br>\n1003163366_44323f5815.jpg#4 &emsp;\tman laying on bench holding leash of dog sitting on ground<br>\n1007129816_e794419615.jpg#0 &emsp;\tA man in an orange hat starring at something .<br>\n1007129816_e794419615.jpg#1 &emsp;\tA man wears an orange hat and glasses .<br>\n1007129816_e794419615.jpg#2 &emsp;\tA man with gauges and glasses is wearing a Blitz hat .<br>\n1007129816_e794419615.jpg#3 &emsp;\tA man with glasses is wearing a beer can crocheted hat .<br>\n1007129816_e794419615.jpg#4 &emsp;\tThe man with pierced ears is wearing glasses and an orange hat .<br>","97000507":"### sample  Flickr8k.token dataset","96fcdc35":"# Define the CNN_RNN Model architecture","5e652a35":"* As this is a supervised learning Task we have to provide input an output to the model.\n* We have to train our model on 6000 images and each image will contain 2048 length feature vector and caption is also represented as numbers\n* This large amount of data is not possilble to store in the memory.So ,we have to use a geneartor method that will yield batches.","a49d3402":"# Getting and cleaning the dataset","f5c6421c":"The input to our model is [x1, x2] and the output will be y, where x1 is the 2048 feature vector of that image, x2 is the input text sequence and y is the output text sequence that the model has to predict.","abfbc742":"# Create data generator","d3000efe":"We are using the Xception model which has been trained on imagenet dataset that had 1000 different classes to classify. `Since the Xception model was originally built for imagenet, we will do little changes for integrating with our model. One thing to notice is that the Xception model takes 299*299*3 image size as input. We will remove the last classification layer and get the 2048 feature vector.`","ed8dab80":"# AUDIO-DESCRIPTION-OF-IMAGE-FOR-VISUALLY-IMAPIRED","51801e1b":"# Testing on our Own Image","9082a084":"'''cleaning_text( descriptions)''' \u2013  function takes all descriptions and performs cleaning on caption. This is an important step when we work with textual data, according to our goal, we decide what type of cleaning we want to perform on the text. In our case, we will be removing punctuations, converting all text to lowercase and removing words that contain numbers.\nSo, a caption like \u201cA man riding on a three-wheeled wheelchair\u201d will be transformed into \u201cman riding on three wheeled wheelchair\u201d","d0b2a183":"Let's visulize an example image and its caption\n","5c970b53":"##### Xception Architecture","6328c5c3":"Now calculate the maximum length of the descriptions.","1b92a2ed":"***load_clean_caption( filename, photos )***\u2013 This function will create a dictionary that contains captions for each photo from the list of photos. Append the start and end identifier for each caption. We need this so that our LSTM model can identify the starting and ending of the caption.<br>\n***load_features(photos)*** \u2013 This function will give us the dictionary for image names and their feature vector which we have previously extracted from the Xception model.","da3351e4":"We are creating a Merge model where we combine the image vector and the partial caption. Therefore our model will have 3 major steps:\n\n1. Processing the sequence from the text \n2. Extracting the feature vector from the image\n3. Decoding the output using softmax by concatenating the above two layers","2d3c4114":"The Idea Behind the Project: <br>\n    Image ---> Caption  ---> Audio","f46a708e":"# Load the Data for Training the Model.","a738f8a1":"Now , we will use Transfer Learning for determining the feature Vector from Pre-trained Model","5bafc624":"Xception Model is proposed by Francois Chollet. Xception is an extension of the inception Architecture which replaces the standard Inception modules with depthwise Separable Convolutions.\n","126f50fd":"Let's start the Project","39b5f1de":"![image.png](attachment:image.png)","9e80772d":"Each image has 5 captions and we can see that #(0 to 5)number is assigned for each caption.","9ea00544":"`Yield` is a keyword in Python that is used to return from a function without destroying the states of its local variable and when the function is called, the execution starts from the last yield statement. Any function that contains a yield keyword is termed as generator. ","79d7f0a3":"![image.png](attachment:image.png)","17213016":"# Extracting the feature vector from all images","06c57c77":"# Testing the Model","fe9c0812":"For converting the image to Caption we will use MultiModal technique of CNN-RNN architecture where we first built CNN model to get feature of the image and finally put the faeture as input to RNN(LSTM) Modal to predict the caption of image and finally convert that text to Audio.","33cd5a8b":"# Training the model","dfaee30f":"For the image caption generator, we will be using the Flickr_8K dataset.","243f8e49":"To train the model, we will be using the 6000 training images by generating the input and output sequences in batches and fitting them to the model using model.fit_generator() method. We also save the model to our models folder. This will take some time depending on your system capability.","6466eba5":"# Tokenizing the Vocabulary","1d52be7d":"We will map each word of the vocabulary with unique index value .Keras library provides us with the tokenizer function that we will use to create tokens from our vocabulary and save them to a \u201ctokenizer.p\u201d pickle file."}}