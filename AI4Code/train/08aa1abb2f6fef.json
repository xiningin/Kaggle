{"cell_type":{"acbb4132":"code","50644d31":"code","dfb33ec2":"code","be73f61e":"code","d0d53493":"code","7111efb2":"code","7fc4d0b7":"code","60a093c7":"code","62ed3271":"code","8923e4a0":"code","bc2226b1":"code","bebf2871":"code","4b0af8f6":"code","3dcd6aca":"code","5811c7a1":"code","ac9beb05":"code","1b3a4ddc":"code","61bef5dc":"code","789fc63b":"code","7373e72a":"code","37556188":"code","b24bd1d8":"code","e3708189":"code","288f3ec7":"code","7474190f":"code","7866e765":"code","343a2512":"code","ffd2a48a":"code","bbce89c0":"code","3664a94c":"markdown","a066dffc":"markdown","c0e314da":"markdown","5412884a":"markdown","cf7c1181":"markdown","94ce978b":"markdown","bcaf2542":"markdown","59711e45":"markdown","c54ac2ad":"markdown","74ed5b43":"markdown","6db5b5f5":"markdown","9e736e46":"markdown","3c7e5b23":"markdown"},"source":{"acbb4132":"# libraries\nimport os\nimport re \nimport gc\nimport numpy as np \nimport pandas as pd\npd.set_option('display.max_colwidth',None)\n\n# scikit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import metrics\n\n# gensim\nimport gensim\nfrom gensim.models import KeyedVectors\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","50644d31":"# Process the data sets\n\ndef load_datasets():\n    pd.set_option('display.max_colwidth',None)\n    train = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')\n    test  = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv')\n    return train, test\n\ntrain, test = load_datasets()\ndisplay(train)","dfb33ec2":"import re\n\ndef clean_data(data):\n    tag=re.compile(r'[0-9]+')\n    data=tag.sub(r'',data)\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    tag=re.compile(r'\\s+')\n    data=tag.sub(r' ',data)\n    red_tag=re.compile(r'[?<=(  )\\\\]|[&&|\\|\\|-]')\n    data=red_tag.sub(r' ',data)\n    return \"\".join(data)\n    \ntrain['question_text'] = train['question_text'].apply(lambda x: clean_data(x))\ntest['question_text']  = test['question_text'].apply(lambda x: clean_data(x))","be73f61e":"%%time\n\n#stemmimng the text\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import *\n\ndef stem_corpus(data):\n    stemmer = PorterStemmer()\n    out_data=\"\"\n    for words in data:\n        out_data+= stemmer.stem(words)\n    return out_data\n\n\ntrain['question_text'] = train['question_text'].apply(lambda x: stem_corpus(x))\ntest['question_text']  = test['question_text'].apply(lambda x: stem_corpus(x))","d0d53493":"#Converting uppercase letters to lowercase\n\ndef convert_2lowercase(data):\n    data =[string.lower() for string in data if string.isupper]\n    return ''.join(data)\n\ntrain['question_text'] = train['question_text'].apply(lambda z: convert_2lowercase(z))\ntest['question_text']  = test['question_text'].apply(lambda z: convert_2lowercase(z))","7111efb2":"train.info()","7fc4d0b7":"test.info()","60a093c7":"#Let's have a look at the clean and preprocessed data sets \ntrain.to_csv(\"train_data.csv\",sep=\",\",index=False)\ntest.to_csv(\"test_data.csv\",sep=\",\",index=False)","62ed3271":"# keras-tf 2.0\nimport tensorflow as tf\nfrom tensorflow import keras","8923e4a0":"# keras-tf 2.0\nfrom tensorflow.keras import regularizers\nfrom keras import layers\nfrom tensorflow.keras.layers import Input, Embedding, Bidirectional,LSTM,Dense,Flatten,Conv2D,Conv1D,GlobalMaxPooling1D,Concatenate,TimeDistributed\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.utils import plot_model\nfrom keras.wrappers.scikit_learn import KerasClassifier","bc2226b1":"# Load the input features\n\nX = train['question_text'] # input\ny = train['target'].values # target \/label\n\nsentences_train,sentences_val,y_train,y_val = train_test_split(X,y,test_size=0.2,random_state=14)\n\n# tokenize the text corpus with keras tokenizer\ntokenizer = Tokenizer(num_words=100000)\ntokenizer.fit_on_texts(sentences_train)\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_val = tokenizer.texts_to_sequences(sentences_val)\n\n# Adding 1 because of  reserved 0 index\nvocab_size = len(tokenizer.word_index) + 2 # (in case of pre-trained embeddings it's +2)                         \nmaxlen = 128 # sentence length\n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_val = pad_sequences(X_val, padding='post', maxlen=maxlen)\n\nword_index = tokenizer.word_index\nnum_tokens = len(tokenizer.word_index)+2\n\nprint(\"Vocabulary Size \/ Unique Words in the Corpus:\",num_tokens)","bebf2871":"del train\ngc.collect()","4b0af8f6":"# generic function to plot the train Vs validation loss\/accuracy:\ndef plot_history(history):\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n    if len(loss_list) == 0:\n        print('Loss is missing in history')\n        return \n    ## As loss always exists\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n    plt.figure(figsize=(25,15))\n    ## Accuracy\n    plt.subplot(2,2,1)\n    for l in acc_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.4f'))+')')\n    for l in val_acc_list:    \n        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.4f'))+')')\n\n    plt.title('Training Accuracy Vs Validation Accuracy\\n')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    ## Loss\n    plt.subplot(2,2,2)\n    for l in loss_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.4f'))+')'))\n    for l in val_loss_list:\n        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.4f'))+')'))\n    \n    plt.title('Training Loss Vs Validation Loss\\n')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()","3dcd6aca":"def conf_matrix(actual, prediction, model_name):\n    cm_array=metrics.confusion_matrix(actual,prediction,labels=[0,1])\n    sns.set_context(\"notebook\", font_scale=1.1)\n    plt.figure(figsize=(5,5))\n    sns.heatmap(cm_array,annot=True, fmt='.0f',xticklabels=['Sincere','Insincere'],yticklabels=['Sincere','Insincere'])\n    plt.ylabel('True\\n')\n    plt.xlabel('Predicted\\n')\n    plt.title(model_name)\n    plt.show()","5811c7a1":"import io\nfrom tqdm import tqdm\nimport zipfile\n\n## make a dict mapping words (strings) to their NumPy vector representation:\nembeddings_index = {}\n\nwith zipfile.ZipFile(\"..\/input\/quora-insincere-questions-classification\/embeddings.zip\") as zf:\n    with io.TextIOWrapper(zf.open(\"glove.840B.300d\/glove.840B.300d.txt\"), encoding=\"utf-8\") as f:\n        for line in tqdm(f):\n            word, coefs = line.split(maxsplit=1)\n            coefs = np.fromstring(coefs, dtype=float, sep=\" \")\n            embeddings_index[word] = coefs\n            \n            \nprint(\"Found %s word vectors.\" % len(embeddings_index))","ac9beb05":"gc.collect()","1b3a4ddc":"## prepare a corresponding embedding matrix that we can use in a Keras Embedding layer. \n## It's a simple NumPy matrix where entry at index i is the pre-trained vector for the word of index i in our vectorizer's vocabulary.\n\nword_index=tokenizer.word_index\nnum_tokens = len(tokenizer.word_index)+ 2\nembedding_dim = 300\n\nhits = 0\nmisses = 0\n\n# Prepare embedding matrix\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\n\nfor word, i in word_index.items():\n    try:\n        embedding_vector = embeddings_index.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector\n            hits += 1\n        else:\n            misses += 1\n    except:\n        misses += 1\n        \nprint(\"Converted %d words (%d misses)\" % (hits, misses))\n\n\n#load the pre-trained word embeddings matrix into an Embedding layer.\nembedding_layer = Embedding(\n    num_tokens,\n    embedding_dim,\n    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n    trainable=False\n)","61bef5dc":"embedding_dim = 300\n\n\n# Input for variable-length sequences of integers\ninputs = keras.Input(shape=(None,), dtype=\"int32\")\n# Embed each integer in a 128-dimensional vector\nx = layers.Embedding(num_tokens,embedding_dim,weights=[embedding_matrix],input_length=maxlen,trainable=False)(inputs)\n# Add 2 bidirectional LSTMs\nx = layers.Bidirectional(layers.LSTM(64,return_sequences=True))(x)\nx = layers.Bidirectional(layers.LSTM(64,return_sequences=False))(x)\nx = layers.Dense(64, activation=\"relu\")(x)\nx = layers.Dropout(0.5)(x)\n# Add a classifier\noutputs = layers.Dense(1, activation=\"sigmoid\")(x)\nmodel = keras.Model(inputs, outputs)\nmodel.summary()\n\n\nopt = Adam(learning_rate=0.01)\n\n\n# defining the call backs\nearly_stopping=tf.keras.callbacks.EarlyStopping(\n                                                monitor=\"val_loss\",\n                                                patience=3,\n                                                mode=\"min\",\n                                                restore_best_weights=True\n                                              )\n### Now reducing the learning rate when the model is not improvinig \nreduce_lr=tf.keras.callbacks.ReduceLROnPlateau(\n                                                monitor=\"val_loss\",\n                                                factor=0.2,\n                                                patience=2,\n                                                verbose=1,\n                                                mode=\"auto\"\n                                            )\n\nmy_callbacks=[early_stopping,reduce_lr]\n\nmodel.compile(optimizer=opt,loss='binary_crossentropy',metrics=['accuracy'])","789fc63b":"%%time\n\nepochs=2\n\nfor e in range(epochs):\n    history = model.fit(X_train,y_train,batch_size=256,epochs=5,validation_split=0.2,callbacks=my_callbacks)\n    pred_val_y = model.predict([X_val], batch_size=512, verbose=1)\n    best_thresh = 0.5\n    best_score = 0.0\n    for thresh in np.arange(0.1, 0.501, 0.01):\n        thresh = np.round(thresh, 2)\n        score = metrics.f1_score(y_val, (pred_val_y>thresh).astype(int))\n        if score > best_score:\n            best_thresh = thresh\n            best_score = score\n    print(\"Val F1 Score: {:.6f}\".format(best_score))","7373e72a":"gc.collect()","37556188":"best_thresh","b24bd1d8":"best_score","e3708189":"plot_history(history)","288f3ec7":"pred_y_val = (pred_val_y>best_thresh).astype(int)\nprint(\"Metrics\\n\")\nprint(metrics.classification_report(y_val,pred_y_val))","7474190f":"conf_matrix(y_val,pred_y_val,'Bi-RNN Model Validation Scores\\n')","7866e765":"test_df    = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/test.csv\")\ntest_input = test_df['question_text']\nids        = test_df['qid']\ntest_input = np.array(test_input)\n\ntokenizer = Tokenizer(num_words=100000)\ntokenizer.fit_on_texts(test_input)\nsequences = tokenizer.texts_to_sequences(test_input)\ntest_sequences = pad_sequences(sequences, maxlen=128,padding='post')\nindices = np.arange(test_sequences.shape[0])\ntest_padded = test_sequences[indices]\ntest_padded.shape","343a2512":"gc.collect()","ffd2a48a":"test_predictions = model.predict(test_padded)\npredictions = (test_predictions>best_thresh).astype(int)\npredictions = predictions.flatten()\n\nout_df = pd.DataFrame({'qid':ids,'prediction':predictions})\npd.set_option('display.max_colwidth',None)\n# submission file\nout_df.to_csv(\"submission.csv\", index=False)","bbce89c0":"out_df.head()","3664a94c":"### Plot the history of the model","a066dffc":"## Validate the fit ","c0e314da":"## Final Submission - Glove - BiLSTM","5412884a":"## Confusion Matrix on Validation Data","cf7c1181":"## Split the train and validation data","94ce978b":"### Prediction - Glove","bcaf2542":"## Metrics on Validation Data","59711e45":"## Plot history","c54ac2ad":"### Using the Glove word embeddding","74ed5b43":"### Designing Embedding Matrix","6db5b5f5":"# Sentence Classification with GloVe embeddings on Bi-LSTM Networks\n\n\n**Data Set:** \"Quora insincere questions\" \n\n**Pre_trained Embeddings:** GloVe Embedding","9e736e46":"### Generic function to plot the confusion matrix","3c7e5b23":"# Build the model"}}