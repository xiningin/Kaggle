{"cell_type":{"2f4baddf":"code","0198590b":"code","b17e1702":"code","dc0ebe1f":"code","3a67d162":"code","8c97862a":"code","e227c43d":"code","971e6763":"code","520ae91d":"code","6b8fa236":"code","18690230":"code","e596e2a6":"code","9e2edfb9":"code","c68b1fd8":"code","f945870e":"code","d7cd2ac9":"code","6cae3906":"code","d221322a":"code","353c9452":"code","97111fab":"code","2e3bfacf":"code","a64c2416":"code","074a420b":"code","ef5bb4d0":"code","fae22acd":"code","c31252d8":"code","cc7104ee":"code","5a679a5a":"code","d9326cb4":"code","88c7e747":"code","cf97e8f0":"code","afe0c007":"code","7e2a0e0e":"code","09f1dd28":"code","22639f1c":"code","85c712e9":"code","a249ac6e":"code","c9f107e2":"code","8b45cac9":"code","c4b683dc":"code","83012ca0":"code","70a326df":"code","4626f5a4":"code","0c6835f6":"code","e1f29e52":"code","37469f18":"code","577e03d9":"code","e5860fe5":"code","6753cd3a":"code","5202d726":"code","a45ab04a":"code","c970269d":"code","fce26dc2":"code","37f74813":"code","9c650340":"code","096f576d":"code","31c7b67f":"code","793b3273":"code","c276380e":"code","feafac61":"code","3019baeb":"code","80fe0a51":"code","30344731":"code","2bd1f8a6":"code","6a65d620":"code","f2f524ff":"code","f20d5421":"code","f2737c1c":"code","5c0c65d9":"code","346ad301":"code","1d5b82c0":"code","5a83e336":"code","1b8cb949":"code","58a648f2":"code","caeab666":"code","d70fbe42":"code","8cab5f0a":"code","034b2ed5":"code","f9f0c285":"code","4cbf910f":"code","20d9581f":"code","b503e6a9":"code","e278c294":"code","d15142ac":"code","5ea50029":"code","e96f9768":"code","d355bf4e":"code","519bcd43":"code","f1543017":"code","ee21d9c2":"code","ba2d239c":"code","865a7caf":"code","218a8e51":"code","fdc0036c":"code","9b2124a3":"code","2dec53fb":"code","cc4fe906":"code","c7f250eb":"code","0926d8a5":"code","7d28d0af":"code","51792047":"code","e734c02a":"code","6709aed6":"code","e1f6511f":"code","5696fd0b":"code","75210d7f":"code","6b5a77ca":"code","2a5f6a30":"code","d63c9d21":"code","af4d745c":"code","7d270cf3":"code","ef54b2c9":"code","58f9dd27":"code","a2899dee":"code","5e935f1e":"code","2b68f00b":"code","5cb6b45c":"code","3ccb8540":"code","c88982f4":"code","ce98d2a7":"code","87914bdf":"code","19b24e0a":"code","65d46bca":"code","f3701f07":"code","4ace214b":"code","4b53141a":"code","9010d23a":"code","9afa12ca":"code","1ae7c13c":"code","fb069ee6":"code","8393286c":"code","c31cca1d":"code","9642161a":"code","d0bdeda6":"code","da4446fa":"code","56d5b14f":"code","cca00c72":"code","20e91da6":"code","91ae78d0":"code","22a8ee93":"code","d8d73000":"code","bf39764f":"code","ffcd024e":"code","6a540506":"code","648cfccb":"code","fe0ec1e7":"code","ebd3992a":"code","1f0be96a":"code","efe13499":"code","26055cd5":"code","dcb61701":"code","9ddd93ce":"code","a0453025":"code","974e661b":"code","e1952fd0":"code","87b6ba0d":"code","97064643":"code","064b9705":"code","9eae5438":"code","e25fec83":"code","12d03d41":"code","c58cb89c":"code","6599e733":"code","512ce23c":"code","170001e1":"code","ba4dd859":"code","81c9f674":"code","7111ee10":"code","2c536e05":"code","19cedc2a":"code","cc221f28":"code","971ff8a0":"code","9f9b14c9":"code","84e0e9fe":"code","38547021":"code","3ebd1229":"code","4c8c866c":"code","6f34d11e":"code","decc1950":"code","01790de6":"code","cb483009":"code","35ce1241":"code","0a36975a":"code","2dccb261":"code","a92ed943":"code","e0da894c":"code","5a3f3a00":"code","ce6bc633":"code","466bb946":"code","2a572f0f":"code","770e6627":"code","6faa3e26":"code","1339ce47":"code","11d0b63a":"code","4901a7e6":"code","b5b78e08":"code","61f3b67d":"code","9515b1de":"code","57f557b3":"code","c0e4be51":"code","d5403de9":"code","cd5f7c74":"code","4c37516f":"code","d8f97b11":"code","15a2060b":"code","74c8ce87":"code","2a9f06e5":"code","090acd48":"code","793de419":"code","95c7a7ce":"code","f04f981d":"code","039da291":"code","c8d62f01":"code","ce12558e":"code","6bbcb09a":"code","692adf50":"code","65a2f06e":"code","c88937e9":"code","d1971109":"code","c6991824":"code","a88edba5":"code","f3259779":"code","1bc504c0":"code","1e7639f9":"code","e39a4c93":"code","0d2d566e":"code","d4a79cdc":"code","5b2f223d":"code","5b8d6c27":"code","9aa87c9c":"code","c46a77af":"code","ad95e302":"code","d83566c5":"code","6be97557":"code","ed7091ed":"code","c170bf6a":"code","8b8787f7":"code","199f1434":"code","af5c766a":"code","0f53a54c":"code","73a6edf9":"code","1352ce15":"code","25d61f8c":"code","90a1e24b":"code","1c1636dd":"code","0a777982":"code","54f9a4fd":"code","78b497e3":"code","f71c12b2":"code","130b70f4":"code","e1e7d196":"code","14e279aa":"code","cce64c5e":"code","a050817b":"code","ea7e18a1":"code","24678ea0":"code","db774d3f":"code","c871f967":"code","8803cde9":"code","3c8d1be3":"code","84394718":"code","6b3ab7d5":"code","3ac49515":"code","d026032f":"code","abc1b66c":"code","0513e873":"code","8c71d694":"code","14c23cd5":"code","224bc722":"code","febc53e0":"code","a7afd80f":"code","81f87db6":"code","1a3c6616":"code","518c0ec2":"code","439f5079":"code","754a9a06":"code","701bc495":"code","010a9eb5":"code","d8360064":"code","b3ef5e63":"code","a945a220":"code","7263967e":"code","41199af7":"code","27623bfd":"code","58591255":"code","6dab934b":"code","f6562b8d":"code","6df63692":"code","447f7b45":"code","b7c65230":"code","238480fe":"code","4c660188":"code","ef7f434a":"code","7cf3a8f8":"code","7f58eb7f":"markdown","5ad57e1a":"markdown","6e1b4507":"markdown","f052c980":"markdown","10a55b55":"markdown","babc9735":"markdown","248ee2f1":"markdown","da7bf949":"markdown","91175240":"markdown","de29902d":"markdown","6fc07b0d":"markdown","223d1a2b":"markdown","3bbe7f5f":"markdown","65e19823":"markdown","b4ab35c2":"markdown","1ee0753c":"markdown","0d414f71":"markdown","88cffba2":"markdown","c0db34fd":"markdown","3d7265f8":"markdown","8c274814":"markdown","5880d891":"markdown","a34c22fa":"markdown","18af3a65":"markdown","8f9b86d4":"markdown","d8ab86da":"markdown","2e805e32":"markdown","482924b8":"markdown","9e03cc69":"markdown","ce6ae0ad":"markdown","54bdf180":"markdown","1fd188b4":"markdown","dd671af8":"markdown","eeff09a2":"markdown","90881267":"markdown","4479bae3":"markdown","a41b9197":"markdown","51ab9ce3":"markdown","9b106258":"markdown","6481c8f0":"markdown","a5aa15a4":"markdown","aa2be1bd":"markdown","d5e10c7a":"markdown","7a7ab860":"markdown","69f2cb06":"markdown","1943f6d2":"markdown","44fb7252":"markdown","46190b53":"markdown","45a0f492":"markdown","4330bbb1":"markdown","2198336e":"markdown","10ad5eef":"markdown","b868cc68":"markdown","dfe0cbf8":"markdown","72c3a92c":"markdown","995eb150":"markdown","5081c0f0":"markdown","44d88ed6":"markdown","555b5869":"markdown","6a772e6a":"markdown","1e97a365":"markdown","a9bc754f":"markdown","eae2849e":"markdown","fbbf1f89":"markdown","6033040b":"markdown","c3f432af":"markdown","d9505bf8":"markdown","ad88a9ad":"markdown","76251179":"markdown","cde313cc":"markdown","caab58ec":"markdown","3d44cbfd":"markdown","35889e60":"markdown","1a6b0d2b":"markdown","3904aabc":"markdown","0dfd7754":"markdown","b595bf8a":"markdown","de41d9fc":"markdown","5dac5962":"markdown","5004c551":"markdown","28005026":"markdown","f129de91":"markdown","29da0221":"markdown","e19b3f37":"markdown","e16f820a":"markdown","54fa9f47":"markdown","0f3f2551":"markdown","ebbcbc73":"markdown","0a3f999f":"markdown","c3d72830":"markdown","f7fbf161":"markdown","4c19208c":"markdown","a5160986":"markdown","6951e5a5":"markdown","ad1a0700":"markdown","665313db":"markdown","de8e88c9":"markdown","b5814d58":"markdown","b062fd9c":"markdown","03cb4352":"markdown","d6decd7a":"markdown","9f7f973f":"markdown","ab6c1667":"markdown","1b855da8":"markdown","3cbe4beb":"markdown","9399fdbb":"markdown","8059f4b4":"markdown"},"source":{"2f4baddf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0198590b":"import numpy as np\nimport pandas as pd\n\nimport math\n\n# Geographic\nfrom sklearn import preprocessing\nimport matplotlib.cm as cm\nfrom geopy.geocoders import Nominatim\nimport folium\nfrom tqdm import tqdm \n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom datetime import datetime, date\n\n# Visualization \nimport missingno as msno # missing values\nimport matplotlib.pyplot as plt\nimport seaborn as sns # Visualization\nfrom matplotlib.pyplot import figure # Visualization","b17e1702":"import tensorflow as tf\nimport random\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nimport gc\nimport lightgbm as lgb","dc0ebe1f":"def plot_corr_hitmap(df, title):    \n    fig, ax = plt.subplots(figsize=(20,10)) \n    ax.set_title(f'Correlation Matrix - {title}', fontsize=14)\n    ax = sns.heatmap(df.corr(method='pearson'), vmin=-1, vmax=1, center=0, cmap='Blues', annot=True, square=False)","3a67d162":"path = '\/kaggle\/input\/acea-water-prediction\/'\n\ndf_auser = pd.read_csv(f'{path}Aquifer_Auser.csv')\ndf_doganella = pd.read_csv(f'{path}Aquifer_Doganella.csv')\ndf_luco = pd.read_csv(f'{path}Aquifer_Luco.csv')\ndf_petrignano = pd.read_csv(f'{path}Aquifer_Petrignano.csv')\n\nauser = df_auser.copy()\ndoganella = df_doganella.copy()\nluco = df_luco.copy()\npetrignano = df_petrignano.copy()","8c97862a":"# 'Date' : Series -> datetime \n\nauser['Date']=pd.to_datetime(auser['Date'],format ='%d\/%m\/%Y')\ndoganella['Date']=pd.to_datetime(doganella['Date'],format ='%d\/%m\/%Y')\nluco['Date']=pd.to_datetime(luco['Date'],format ='%d\/%m\/%Y')\npetrignano['Date']=pd.to_datetime(petrignano['Date'],format ='%d\/%m\/%Y')","e227c43d":"path = '\/kaggle\/input\/acea-water-prediction\/'\n\ndf_amiata = pd.read_csv(f'{path}Water_Spring_Amiata.csv')\ndf_lupa = pd.read_csv(f'{path}Water_Spring_Lupa.csv')\ndf_madonna = pd.read_csv(f'{path}Water_Spring_Madonna_di_Canneto.csv')\n\namiata = df_amiata.copy()\nlupa = df_lupa.copy()\nmadonna = df_madonna.copy()","971e6763":"# 'Date' : Series -> datetime \n\namiata['Date']=pd.to_datetime(amiata['Date'],format ='%d\/%m\/%Y')\nlupa['Date']=pd.to_datetime(lupa['Date'],format ='%d\/%m\/%Y')\nmadonna['Date']=pd.to_datetime(madonna['Date'],format ='%d\/%m\/%Y')","520ae91d":"path = '\/kaggle\/input\/acea-water-prediction\/'\n\ndf_arno = pd.read_csv(f'{path}River_Arno.csv')\n\narno = df_arno.copy()","6b8fa236":"# 'Date' : Series -> datetime \n\narno['Date']=pd.to_datetime(arno['Date'],format ='%d\/%m\/%Y')","18690230":"path = '\/kaggle\/input\/acea-water-prediction\/'\n\ndf_bilancino = pd.read_csv(f'{path}Lake_Bilancino.csv')\n\nbilancino = df_bilancino.copy()","e596e2a6":"# 'Date' : Series -> datetime \n\nbilancino['Date']=pd.to_datetime(bilancino['Date'],format ='%d\/%m\/%Y')","9e2edfb9":"# List of Dataframes columns\n\ncolumns_auser = auser.columns.tolist()\ncolumns_doganella = doganella.columns.tolist()\ncolumns_luco = luco.columns.tolist()\ncolumns_petrignano = petrignano.columns.tolist()","c68b1fd8":"# Number of variables\nprint('Number of variables : ' + str(len(columns_auser)))\nprint()\n\n# Type of variables\nprint('type of variables : ')\nprint(auser.dtypes)","f945870e":"print('Auser')\ndf_auser.describe()","d7cd2ac9":"print('Number of variables :', str(len(columns_doganella)))\nprint()\nprint('type of variables : ')\nprint(doganella.dtypes)","6cae3906":"print('Doganella')\ndoganella.describe()","d221322a":"print('Number of variables :', str(len(columns_luco)))\nprint()\nprint('type of variables : ')\nprint(luco.dtypes)","353c9452":"print('Luco')\nluco.describe()","97111fab":"print('Number of variables :', str(len(columns_petrignano)))\nprint('type of variables : ')\nprint(petrignano.dtypes)","2e3bfacf":"print('Petrignano')\npetrignano.describe()","a64c2416":"## DIVISION DATAFRAME\n\n# Target variables\ntarget_auser = auser[[target for target in auser.columns if target.startswith('Depth')]]\ntarget_auser_list = target_auser.columns.tolist()\nprint('Number of Target : ' + str(len(target_auser_list)))\nprint(target_auser_list)\n\nprint()\n\n# Feature variables\nfeature_auser = auser[[feature for feature in auser.columns if feature not in target_auser]]\nfeature_auser_list = feature_auser.columns.tolist()\nprint('Number of Features : ' + str(len(feature_auser_list)))\nprint(feature_auser_list)","074a420b":"## HISTOGRAM (AUSER FEATURE VARIABLES)\n\n# Number of columns\nncols = 3\n# Number of rows\nimport math\nnrows = math.ceil((len(feature_auser.columns)-1) \/ ncols)\nf, ax = plt.subplots(ncols = ncols, nrows = nrows, figsize= (24, 5*nrows))\ngrid = []\n\nfor i in range(nrows):\n    for j in range(ncols):\n        grid.append([i,j])\n#[sns.histplot(feature_auser.columns[i+1], ax=ax[grid[i][0], grid[i][1]]) for i in range(0,len(auser_feature.columns)-1)]\n[sns.distplot(feature_auser[feature_auser.columns[i+1]], ax=ax[grid[i][0], grid[i][1]]) for i in range(0,len(feature_auser.columns)-1)]\n\nprint('### AUSER_FEATURES ###')","ef5bb4d0":"## HISTOGRAM (AUSER TARGET VARIABLES)\n\nf, ax = plt.subplots(ncols = 5, nrows = 1, figsize= (24, 5))\n\nfor i in range(len(target_auser.columns)):\n    sns.distplot(target_auser[target_auser.columns[i]], ax=ax[i])\n\n\nprint('### AUSER_TARGET ###')","fae22acd":"## DIVISION DATAFRAME\n\n# Target variables\ntarget_doganella = doganella[[target for target in doganella.columns if target.startswith('Depth')]]\ntarget_doganella_list = target_doganella.columns.tolist()\nprint('Number of Target : ' + str(len(target_doganella_list)))\nprint(target_doganella_list)\n\nprint()\n\n# Feature variables\nfeature_doganella = doganella[[feature for feature in doganella.columns if feature not in target_doganella]]\nfeature_doganella_list = feature_doganella.columns.tolist()\nprint('Number of Features : ' + str(len(feature_doganella_list)))\nprint(feature_doganella_list)","c31252d8":"# Number of columns\nncols = 3\n# Number of rows\nimport math\nnrows = math.ceil((len(feature_doganella.columns)-1) \/ ncols)\nf, ax = plt.subplots(ncols = ncols, nrows = nrows, figsize= (24, 5*nrows))\ngrid = []\n\nfor i in range(nrows):\n    for j in range(ncols):\n        grid.append([i,j])\n#[sns.histplot(auser_feature.columns[i+1], ax=axs[grid[i][0], grid[i][1]]) for i in range(0,len(auser_feature.columns)-1)]\n[sns.distplot(feature_doganella[feature_doganella.columns[i+1]], ax=ax[grid[i][0], grid[i][1]]) for i in range(0,len(feature_doganella.columns)-1)]\n\nprint('### DOGANELLA_FEATURES ###')","cc7104ee":"# Number of columns\nncols = 3\n# Number of rows\nimport math\nnrows = math.ceil((len(target_doganella.columns)-1) \/ ncols)\nf, ax = plt.subplots(ncols = ncols, nrows = nrows, figsize= (24, 5*nrows))\ngrid = []\n\nfor i in range(nrows):\n    for j in range(ncols):\n        grid.append([i,j])\n[sns.distplot(target_doganella[target_doganella.columns[i]], ax=ax[grid[i][0], grid[i][1]]) for i in range(0,len(target_doganella.columns)-1)]\n\nprint('### DOGANELLA_TARGET ###')","5a679a5a":"# Target\ntarget_luco = luco[[target for target in luco.columns if target.startswith('Depth')]]\ntarget_luco_list = target_luco.columns.tolist()\nprint('Number of Target : ' + str(len(target_luco_list)))\nprint(target_luco_list)\n\nprint()\n\n# Feature\nfeature_luco = luco[[feature for feature in luco.columns if feature not in target_luco]]\nfeature_luco_list = feature_luco.columns.tolist()\nprint('Number of Features : ' + str(len(feature_luco_list)))\nprint(feature_luco_list)","d9326cb4":"# Number of columns\nncols = 3\n# Number of rows\nimport math\nnrows = math.ceil((len(feature_luco.columns)-1) \/ ncols)\nf, ax = plt.subplots(ncols = ncols, nrows = nrows, figsize= (24, 5*nrows))\ngrid = []\n\nfor i in range(nrows):\n    for j in range(ncols):\n        grid.append([i,j])\n[sns.distplot(feature_luco[feature_luco.columns[i+1]], ax=ax[grid[i][0], grid[i][1]]) for i in range(0,len(feature_luco.columns)-1)]\n\nprint('### LUCO_FEATURE ###')","88c7e747":"f, ax = plt.subplots(ncols = 4, nrows = 1, figsize= (24, 5))\n\nfor i in range(len(target_luco.columns)):\n    sns.distplot(target_luco[target_luco.columns[i]], ax=ax[i])\n\n\nprint('###LUCO_TARGET ###')","cf97e8f0":"# Target\ntarget_petrignano = petrignano[[target for target in petrignano.columns if target.startswith('Depth')]]\ntarget_petrignano_list = target_petrignano.columns.tolist()\nprint('Number of Target : ' + str(len(target_petrignano_list)))\nprint(target_petrignano_list)\n\nprint()\n\n# Feature\nfeature_petrignano = petrignano[[feature for feature in petrignano.columns if feature not in target_luco]]\nfeature_petrignano_list = feature_petrignano.columns.tolist()\nprint('Number of Features :' + str(len(feature_petrignano_list)))\nprint(feature_petrignano_list)","afe0c007":"# Number of columns\nncols = 3\n# Number of rows\nimport math\nnrows = math.ceil((len(feature_petrignano.columns)-1) \/ ncols)\nf, ax = plt.subplots(ncols = ncols, nrows = nrows, figsize= (24, 5*nrows))\ngrid = []\n\nfor i in range(nrows):\n    for j in range(ncols):\n        grid.append([i,j])\n[sns.distplot(feature_petrignano[feature_petrignano.columns[i+1]], ax=ax[grid[i][0], grid[i][1]]) for i in range(0,len(feature_petrignano.columns)-1)]\n\nprint('### PETRIGNANO_FEATURE ###')","7e2a0e0e":"f, ax = plt.subplots(ncols = 2, nrows = 1, figsize= (24, 5))\n\nfor i in range(len(target_petrignano.columns)):\n    sns.distplot(target_petrignano[target_petrignano.columns[i]], ax=ax[i])\n\n\nprint('### PETRIGNANO_TARGET ###')","09f1dd28":"feature_auser_list","22639f1c":"columns = feature_auser.columns.to_list()\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns) -1, figsize= (10, 5 * len(columns)))\n\nfor var in range(1,len(columns)):\n    axs[var-1].plot(auser['Date'], auser[columns[var]])\n    \n    axs[var-1].set_xlabel('Date')\n    axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n    axs[var-1].grid()","85c712e9":"target_auser_list","a249ac6e":"columns = target_auser.columns.to_list()\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns), figsize= (10, 5 * len(columns)))\n\nfor var in range(0,len(columns)):\n    axs[var].plot(auser['Date'], auser[columns[var]])\n    \n    axs[var].set_xlabel('Date')\n    axs[var].set_ylabel(columns[var], fontsize = 15)\n    \n    axs[var].grid()","c9f107e2":"feature_doganella_list","8b45cac9":"columns = feature_doganella.columns.to_list()\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns) -1, figsize= (10, 5 * len(columns)))\n\nfor var in range(1,len(columns)):\n    axs[var-1].plot(doganella['Date'], doganella[columns[var]])\n    \n    axs[var-1].set_xlabel('Date')\n    axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n    axs[var-1].grid()","c4b683dc":"target_doganella_list","83012ca0":"columns = target_doganella.columns.to_list()\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns), figsize= (10, 5 * len(columns)))\n\nfor var in range(0,len(columns)):\n    axs[var].plot(doganella['Date'], doganella[columns[var]])\n    \n    axs[var].set_xlabel('Date')\n    axs[var].set_ylabel(columns[var], fontsize = 15)\n    \n    axs[var].grid()","70a326df":"feature_luco_list","4626f5a4":"columns = feature_luco.columns.to_list()\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns) -1, figsize= (10, 5 * len(columns)))\n\nfor var in range(1,len(columns)):\n    axs[var-1].plot(luco['Date'], luco[columns[var]])\n    \n    axs[var-1].set_xlabel('Date')\n    axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n    axs[var-1].grid()","0c6835f6":"columns = target_luco.columns.to_list()\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns) -1, figsize= (10, 5 * len(columns)))\n\nfor var in range(1,len(columns)):\n    axs[var-1].plot(luco['Date'], luco[columns[var]])\n    \n    axs[var-1].set_xlabel('Date')\n    axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n    axs[var-1].grid()","e1f29e52":"columns = feature_petrignano.columns.to_list()\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns) -1, figsize= (10, 5 * len(columns)))\n\nfor var in range(1,len(columns)):\n    axs[var-1].plot(petrignano['Date'], petrignano[columns[var]])\n    \n    axs[var-1].set_xlabel('Date')\n    axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n    axs[var-1].grid()","37469f18":"columns = target_petrignano.columns.to_list()\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns), figsize= (10, 5 * len(columns)))\n\nfor var in range(0,len(columns)):\n    axs[var].plot(petrignano['Date'], petrignano[columns[var]])\n    \n    axs[var].set_xlabel('Date')\n    axs[var].set_ylabel(columns[var], fontsize = 15)\n    \n    axs[var].grid()","577e03d9":"# List of Dataframes columns\n\ncolumns_amiata = amiata.columns.tolist()\ncolumns_lupa = lupa.columns.tolist()\ncolumns_madonna = madonna.columns.tolist()","e5860fe5":"print('Number of variables : ' + str(len(columns_amiata)))\n\nprint('type of variables : ')\nprint(amiata.dtypes)","6753cd3a":"print('Amiata')\namiata.describe()","5202d726":"print('Number of variables : ' + str(len(columns_lupa)))\n\nprint('type of variables : ')\nprint(lupa.dtypes)","a45ab04a":"print('Lupa')\nlupa.describe()","c970269d":"print('Number of variables : ' + str(len(columns_madonna)))\n\nprint('type of variables : ')\nprint(madonna.dtypes)","fce26dc2":"print('Madonna')\ndf_auser.describe()","37f74813":"## DIVISION DATAFRAME\n\n# Target variables\ntarget_amiata = amiata[[target for target in amiata.columns if target.startswith('Flow')]]\ntarget_amiata_list = target_amiata.columns.tolist()\nprint('Number of Target : ' + str(len(target_amiata_list)))\nprint(target_amiata_list)\n\nprint()\n\n# Feature variables\nfeature_amiata = amiata[[feature for feature in amiata.columns if feature not in target_amiata]]\nfeature_amiata_list = feature_amiata.columns.tolist()\nprint('Number of Features : ' + str(len(feature_amiata_list)))\nprint(feature_amiata_list)","9c650340":"## HISTOGRAM (AMIATA FEATURE VARIABLES)\n\n# Number of columns\nncols = 4\nnrows = math.ceil((len(feature_amiata.columns)-1) \/ ncols)\nf, ax = plt.subplots(ncols = ncols, nrows = nrows, figsize= (24, 5*nrows))\ngrid = []\n\nfor i in range(nrows):\n    for j in range(ncols):\n        grid.append([i,j])\n[sns.distplot(feature_amiata[feature_amiata.columns[i+1]], ax=ax[grid[i][0], grid[i][1]]) for i in range(0,len(feature_amiata.columns)-1)]\n\nprint('### AMIATA_FEATURES ###')","096f576d":"## HISTOGRAM (AMIATA TARGET VARIABLES)\n\nf, ax = plt.subplots(ncols = 4, nrows = 1, figsize= (24, 5))\n\nfor i in range(len(target_amiata.columns)):\n    sns.distplot(target_amiata[target_amiata.columns[i]], ax=ax[i])\n\n\nprint('### AMIATA_TARGET ###')","31c7b67f":"## DIVISION DATAFRAME\n\n# Target variables\ntarget_lupa = lupa[[target for target in lupa.columns if target.startswith('Flow')]]\ntarget_lupa_list = target_lupa.columns.tolist()\nprint('Number of Target : ' + str(len(target_lupa_list)))\nprint(target_lupa_list)\n\nprint()\n\n# Feature variables\nfeature_lupa = lupa[[feature for feature in lupa.columns if feature not in target_lupa]]\nfeature_lupa_list = feature_lupa.columns.tolist()\nprint('Number of Features : ' + str(len(feature_lupa_list)))\nprint(feature_lupa_list)","793b3273":"## HISTOGRAM (LUPA FEATURE VARIABLE)\n\nf, ax = plt.subplots(ncols = 1, nrows = 1, figsize= (12, 5))\nsns.distplot(feature_lupa[feature_lupa.columns[1]], ax=ax)\n\nprint('### LUPA_FEATURE ###')","c276380e":"## HISTOGRAM (LUPA TARGET VARIABLE)\n\nf, ax = plt.subplots(ncols = 1, nrows = 1, figsize= (12, 5))\nsns.distplot(target_lupa[target_lupa.columns[0]], ax=ax)\n\nprint('### LUPA_TARGET ###')","feafac61":"## DIVISION DATAFRAME\n\n# Target variables\ntarget_madonna = madonna[[target for target in madonna.columns if target.startswith('Flow')]]\ntarget_madonna_list = target_madonna.columns.tolist()\nprint('Number of Target : ' + str(len(target_lupa_list)))\nprint(target_madonna_list)\n\nprint()\n\n# Feature variables\nfeature_madonna = madonna[[feature for feature in madonna.columns if feature not in target_madonna]]\nfeature_madonna_list = feature_madonna.columns.tolist()\nprint('Number of Features : ' + str(len(feature_madonna_list)))\nprint(feature_madonna_list)","3019baeb":"## HISTOGRAM (MADONNA_DI_CANNETO FEATURE VARIABLES)\n\n# Number of columns\nncols = 2\nnrows = math.ceil((len(feature_madonna.columns)-1) \/ ncols)\nf, ax = plt.subplots(ncols = ncols, nrows = nrows, figsize= (24, 5*nrows))\ngrid = []\n\nfor i in range(nrows):\n    for j in range(ncols):\n        grid.append([i,j])\n[sns.distplot(feature_madonna[feature_madonna.columns[i+1]], ax=ax[i]) for i in range(0,len(feature_madonna.columns)-1)]\n\nprint('### MADONNA_DI_CANNETO FEATURES ###')","80fe0a51":"## HISTOGRAM (MADONNA_DI_CANNETO TARGET VARIABLE)\n\nf, ax = plt.subplots(ncols = 1, nrows = 1, figsize= (12, 5))\nsns.distplot(target_madonna[target_madonna.columns[0]], ax=ax)\n\nprint('### MADONNA_DI_CANNETO TARGET ###')","30344731":"columns = feature_amiata.columns.to_list()\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns) -1, figsize= (10, 5 * len(columns)))\n\nfor var in range(1,len(columns)):\n    axs[var-1].plot(amiata['Date'], amiata[columns[var]])\n    \n    axs[var-1].set_xlabel('Date')\n    axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n    axs[var-1].grid()","2bd1f8a6":"columns = target_amiata.columns.to_list()\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns) -1, figsize= (10, 5 * len(columns)))\n\nfor var in range(1,len(columns)):\n    axs[var-1].plot(amiata['Date'], amiata[columns[var]])\n    \n    axs[var-1].set_xlabel('Date')\n    axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n    axs[var-1].grid()","6a65d620":"fig, axs = plt.subplots(nrows = 1, figsize= (10, 5))\naxs.plot(lupa['Date'],lupa['Rainfall_Terni'])\naxs.set_xlabel('Date')\naxs.set_ylabel('Rainfall_Terni', fontsize = 15)\naxs.grid()","f2f524ff":"fig, axs = plt.subplots(nrows = 1, figsize= (10, 5))\naxs.plot(lupa['Date'],lupa['Flow_Rate_Lupa'])\naxs.set_xlabel('Date')\naxs.set_ylabel('Flow_Rate_Lupa', fontsize = 15)\naxs.grid()","f20d5421":"target_lupa_list","f2737c1c":"columns = feature_madonna.columns.to_list()\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns) -1, figsize= (10, 5 * len(columns)))\n\nfor var in range(1,len(columns)):\n    axs[var-1].plot(madonna['Date'], madonna[columns[var]])\n    \n    axs[var-1].set_xlabel('Date')\n    axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n    axs[var-1].grid()","5c0c65d9":"fig, axs = plt.subplots(nrows = 1, figsize= (10, 5))\naxs.plot(madonna['Date'],madonna['Flow_Rate_Madonna_di_Canneto'])\naxs.set_xlabel('Date')\naxs.set_ylabel('Flow_Rate_Madonna_di_Canneto', fontsize = 15)\naxs.grid()","346ad301":"# List of Dataframes columns\n\ncolumns_arno = arno.columns.tolist()\nprint(columns_arno)","1d5b82c0":"print('Number of variables : ' + str(len(columns_arno)))\n\nprint('type of variables : ')\nprint(arno.dtypes)","5a83e336":"arno.describe()","1b8cb949":"## DIVISION DATAFRAME\n\n# Target variables\ntarget_arno = arno[[target for target in arno.columns if target.startswith('Hydrometry')]]\ntarget_arno_list = target_arno.columns.tolist()\nprint('Number of Target : ' + str(len(target_arno_list)))\nprint(target_arno_list)\n\nprint()\n\n# Feature variables\nfeature_arno = arno[[feature for feature in arno.columns if feature not in target_arno]]\nfeature_arno_list = feature_arno.columns.tolist()\nprint('Number of Features : ' + str(len(feature_arno_list)))\nprint(feature_arno_list)","58a648f2":"## HISTOGRAM (ARNO FEATURE VARIABLES)\n\n# Number of columns\nncols = 3\nnrows = math.ceil((len(feature_arno.columns)-1) \/ ncols)\nf, ax = plt.subplots(ncols = ncols, nrows = nrows, figsize= (24, 5*nrows))\ngrid = []\n\nfor i in range(nrows):\n    for j in range(ncols):\n        grid.append([i,j])\n[sns.distplot(feature_arno[feature_arno.columns[i+1]], ax=ax[grid[i][0], grid[i][1]]) for i in range(0,len(feature_arno.columns)-1)]\n\nprint('### ARNO_FEATURES ###')","caeab666":"## HISTOGRAM (ARNO TARGET VARIABLE)\n\nf, ax = plt.subplots(ncols = 1, nrows = 1, figsize= (12, 5))\nsns.distplot(target_arno[target_arno.columns[0]], ax=ax)\n\nprint('### ARNO TARGET ###')","d70fbe42":"columns = feature_arno.columns.to_list()\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns) -1, figsize= (10, 5 * len(columns)))\n\nfor var in range(1,len(columns)):\n    axs[var-1].plot(arno['Date'], arno[columns[var]])\n    \n    axs[var-1].set_xlabel('Date')\n    axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n    axs[var-1].grid()","8cab5f0a":"fig, axs = plt.subplots(nrows = 1, figsize= (10, 5))\naxs.plot(arno['Date'],arno['Hydrometry_Nave_di_Rosano'])\naxs.set_xlabel('Date')\naxs.set_ylabel('Hydrometry_Nave_di_Rosano', fontsize = 15)\naxs.grid()","034b2ed5":"# List of Dataframes columns\n\ncolumns_bilancino = bilancino.columns.tolist()\nprint(columns_bilancino)","f9f0c285":"print('Number of variables : ' + str(len(columns_bilancino)))\n\nprint('type of variables : ')\nprint(bilancino.dtypes)","4cbf910f":"bilancino.describe()","20d9581f":"## DIVISION DATAFRAME\n\n# Target variables\ntarget_bilancino = bilancino[[target for target in bilancino.columns if target.startswith('Lake') or target.startswith('Flow')]]\ntarget_bilancino_list = target_bilancino.columns.tolist()\nprint('Number of Target : ' + str(len(target_bilancino_list)))\nprint(target_bilancino_list)\n\nprint()\n\n# Feature variables\nfeature_bilancino = bilancino[[feature for feature in bilancino.columns if feature not in target_bilancino]]\nfeature_bilancino_list = feature_bilancino.columns.tolist()\nprint('Number of Features : ' + str(len(feature_bilancino_list)))\nprint(feature_bilancino_list)","b503e6a9":"## HISTOGRAM (BILANCINO FEATURE VARIABLES)\n\n# Number of columns\nncols = 3\nnrows = math.ceil((len(feature_bilancino.columns)-1) \/ ncols)\nf, ax = plt.subplots(ncols = ncols, nrows = nrows, figsize= (24, 5*nrows))\ngrid = []\n\nfor i in range(nrows):\n    for j in range(ncols):\n        grid.append([i,j])\n[sns.distplot(feature_bilancino[feature_bilancino.columns[i+1]], ax=ax[grid[i][0], grid[i][1]]) for i in range(0,len(feature_bilancino.columns)-1)]\n\nprint('### BILANCINO_FEATURES ###')","e278c294":"## HISTOGRAM (BILANCINO TARGET VARIABLES)\n\nf, ax = plt.subplots(ncols = 2, nrows = 1, figsize= (25, 5))\ngrid = []\n\nfor i in range(len(target_bilancino.columns)):\n    sns.distplot(feature_bilancino[feature_bilancino.columns[i+1]], ax=ax[i])\n\nprint('### BILANCINO TARGET ###')","d15142ac":"columns = feature_bilancino.columns.to_list()\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns) -1, figsize= (10, 5 * len(columns)))\n\nfor var in range(1,len(columns)):\n    axs[var-1].plot(bilancino['Date'], bilancino[columns[var]])\n    \n    axs[var-1].set_xlabel('Date')\n    axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n    axs[var-1].grid()","5ea50029":"columns = target_bilancino.columns.to_list()\n\n# Plotting the time series for every variable\nfig, axs = plt.subplots(nrows = len(columns), figsize= (10, 5 * len(columns)))\n\nfor var in range(0,len(columns)):\n    axs[var].plot(bilancino['Date'], bilancino[columns[var]])\n    \n    axs[var].set_xlabel('Date')\n    axs[var].set_ylabel(columns[var], fontsize = 15)\n    \n    axs[var].grid()","e96f9768":"locations = {}\n\nlocations['Settefrati'] = {'lat' : 41.669624, 'lon' : 13.850011 }\nlocations['Velletri'] = {'lat' : 41.6867015, 'lon' : 12.7770433 }\nlocations['Petrignano'] = {'lat' : 43.1029282, 'lon' : 12.5237369 }\nlocations['Piaggione'] = {'lat' : 43.936794, 'lon' : 10.5040929 }\nlocations['S_Fiora'] = {'lat' : 42.854, 'lon' : 11.556 }\nlocations['Abbadia_S_Salvatore'] = {'lat' : 42.8809724, 'lon' : 11.6724203 }\nlocations['Vetta_Amiata'] = {'lat' : 42.8908958, 'lon' : 11.6264863 }\nlocations['Castel_del_Piano'] = {'lat' : 42.8932352, 'lon' : 11.5383804 }\nlocations['Terni'] = {'lat' : 42.6537515, 'lon' : 12.43981163 }\nlocations['Bastia_Umbra'] = {'lat' : 43.0677554, 'lon' : 12.5495816  }\nlocations['S_Savino'] = {'lat' : 43.339, 'lon' : 11.742 }\nlocations['Monteroni_Arbia_Biena'] = {'lat' : 43.228279, 'lon' : 11.4021433 }\nlocations['Monticiano_la_Pineta'] = {'lat' : 43.1335066 , 'lon' : 11.2408464 }\nlocations['Montalcinello'] = {'lat' : 43.1978783, 'lon' : 11.0787906 }\nlocations['Sovicille'] = {'lat' : 43.2806018, 'lon' : 11.2281756 }\nlocations['Simignano'] = {'lat' : 43.2921965, 'lon' : 11.1680079 }\nlocations['Mensano'] = {'lat' : 43.3009594 , 'lon' : 11.0548528 }\nlocations['Siena_Poggio_al_Vento'] = {'lat' : 43.1399762, 'lon' : 11.3832092 }\nlocations['Scorgiano'] = {'lat' : 43.3521445 , 'lon' : 11.15867 }\nlocations['Ponte_Orgia'] = {'lat' : 43.2074581 , 'lon' : 11.2504416 }\nlocations['Pentolina'] = {'lat' : 43.1968029, 'lon' : 11.1754672 }\nlocations['Montevarchi'] = {'lat' : 43.5234999, 'lon' : 11.5675911 }\nlocations['Incisa'] = {'lat' : 43.6558723, 'lon' : 11.4526838 }\nlocations['Camaldoli'] = {'lat' : 43.7943293, 'lon' : 11.8199481 }\nlocations['Bibbiena'] = {'lat' : 43.6955475, 'lon' : 11.817341 }\nlocations['Stia'] = {'lat' : 43.801537, 'lon' : 11.7067347 }\nlocations['Laterina'] = {'lat' : 43.5081823, 'lon' : 11.7102588 }\nlocations['Monteporzio'] = {'lat' : 41.817251, 'lon' : 12.7050839 }\nlocations['Pontetetto'] = {'lat' : 43.8226294, 'lon' : 10.4940843 }\nlocations['Ponte_a_Moriano'] = {'lat' : 43.9083609 , 'lon' : 10.5342488 }\nlocations['Calavorno'] = {'lat' : 44.0217216, 'lon' : 10.5297323 }\nlocations['Borgo_a_Mozzano'] = {'lat' : 43.978948, 'lon' : 10.545703  }\nlocations['Gallicano'] = {'lat' : 44.0606512, 'lon' : 10.435668  }\nlocations['Tereglio_Coreglia_Antelminelli'] = {'lat' : 44.0550548 , 'lon' : 10.5623594 }\nlocations['Lucca_Orto_Botanico'] = {'lat' : 43.84149865, 'lon' : 10.51169066 }\nlocations['Orentano'] = {'lat' : 43.7796506, 'lon' : 10.6583892 }\nlocations['Fabbriche_di_Vallico'] = {'lat' : 43.997647, 'lon' : 10.4279  }\nlocations['Monte_Serra'] = {'lat' : 43.750833, 'lon' : 10.555278 }\nlocations['Mangona'] = {'lat' : 44.0496863, 'lon' : 11.1958797 }\nlocations['Le_Croci'] = {'lat' : 44.0360503, 'lon' : 11.2675661 }\nlocations['Cavallina'] = {'lat' : 43.9833515, 'lon' : 11.2323312 }\nlocations['S_Agata'] = {'lat' : 43.9438247, 'lon' : 11.3089835 }\nlocations['Firenze'] = {'lat' : 43.7698712, 'lon' : 11.2555757 }\nlocations['S_Piero'] = {'lat' : 43.9637372, 'lon' : 11.3182991 }\nlocations['Vernio'] = {'lat' : 44.0440508 , 'lon' : 11.1498804  }\nlocations['Consuma'] = {'lat' : 43.784, 'lon' : 11.585 }\nlocations['Croce_Arcana']  = {'lat' : 44.1323056, 'lon' : 10.7689152 }\nlocations['Laghetto_Verde']  = {'lat' :   42.883, 'lon' : 11.662  }\n\nlocations_df = pd.DataFrame(columns=['city', 'lat', 'lon'] )\n\ndef get_location_coordinates(df, column_type, cluster, target_df):\n    for location in df.columns[df.columns.str.startswith(column_type)]:\n        location = location.split(column_type)[1]\n\n        loc_dict = {}\n        loc_dict['city'] = location\n        loc_dict['cluster'] = cluster\n        loc_dict['lat'] = locations[location]['lat']\n        loc_dict['lon'] = locations[location]['lon']\n\n        target_df = target_df.append(loc_dict, ignore_index=True)\n\n    return target_df\n\nlocations_df = get_location_coordinates(auser, 'Temperature_', 'auser', locations_df)\nlocations_df = get_location_coordinates(auser, 'Rainfall_', 'auser', locations_df)\n\nlocations_df = get_location_coordinates(doganella, 'Temperature_', 'doganella', locations_df)\nlocations_df = get_location_coordinates(doganella, 'Rainfall_', 'doganella', locations_df)\n\nlocations_df = get_location_coordinates(luco, 'Temperature_', 'luco', locations_df)\nlocations_df = get_location_coordinates(luco, 'Rainfall_', 'luco', locations_df)\n\nlocations_df = get_location_coordinates(petrignano, 'Temperature_', 'petrignano', locations_df)\nlocations_df = get_location_coordinates(petrignano, 'Rainfall_', 'petrignano', locations_df)\n\nlocations_df = get_location_coordinates(bilancino, 'Temperature_', 'bilancino', locations_df)\nlocations_df = get_location_coordinates(bilancino, 'Rainfall_', 'bilancino', locations_df)\n\nlocations_df = get_location_coordinates(arno, 'Temperature_', 'arno', locations_df)\nlocations_df = get_location_coordinates(arno, 'Rainfall_', 'arno', locations_df)\n\nlocations_df = get_location_coordinates(amiata, 'Temperature_', 'amiata', locations_df)\nlocations_df = get_location_coordinates(amiata, 'Rainfall_', 'amiata', locations_df)\n\nlocations_df = get_location_coordinates(lupa, 'Temperature_', 'lupa', locations_df)\nlocations_df = get_location_coordinates(lupa, 'Rainfall_', 'lupa', locations_df)\n\nlocations_df = get_location_coordinates(madonna, 'Temperature_', 'madonna', locations_df)\nlocations_df = get_location_coordinates(madonna, 'Rainfall_', 'madonna', locations_df)\n\n# Drop duplicates\nlocations_df = locations_df.sort_values(by='city').drop_duplicates().reset_index(drop=True)\n\n# Label Encode cluster feature for visualization puposes\nle = preprocessing.LabelEncoder()\nle.fit(locations_df.cluster)\nlocations_df['cluster_enc'] = le.transform(locations_df.cluster)\n\nm = folium.Map(location=[42.6, 12.4], tiles='cartodbpositron',zoom_start=7)\n\ncolors = ['purple','lightred','green', 'lightblue', 'red', 'blue', 'darkblue','lightgreen', 'orange',  'darkgreen', 'beige',  'pink', 'darkred', 'darkpurple', 'cadetblue',]\n\ngeolocator = Nominatim(user_agent='myapplication')\nfor i in locations_df.index:\n    folium.Marker([locations_df.iloc[i].lat, \n                  locations_df.iloc[i].lon],\n                  popup=locations_df.iloc[i].city, \n                  icon=folium.Icon(color=colors[locations_df.iloc[i].cluster_enc])).add_to(m)\n\nm","d355bf4e":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,10))\nsns.heatmap(auser.T.isna(), cmap='Blues')\nax.set_title('Fields with Missing Values', fontsize=16)\n\nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()","519bcd43":"date = '2014-01-01'\nbool_filter = pd.to_datetime(auser['Date'], errors='coerce').ge(date)\nauser = auser[bool_filter].reset_index(drop=True)\nauser.head()","f1543017":"auser['Temperature_Ponte_a_Moriano'] = np.where((auser['Temperature_Ponte_a_Moriano'] == 0),\n                                                auser['Temperature_Lucca_Orto_Botanico'], auser['Temperature_Ponte_a_Moriano'])","ee21d9c2":"tmp = auser[auser['Date'] >= '2019-06-22']\ntmp = tmp[tmp['Date'] <= '2019-06-26'].reset_index(drop=True).reset_index(drop=True)\ntmp['Temperature_Monte_Serra'] = np.where((tmp['Temperature_Monte_Serra'] == 0), np.nan, tmp['Temperature_Monte_Serra'])\n\nfor i in range(len(tmp)):\n  auser['Temperature_Monte_Serra'] = np.where((auser['Date'] == tmp['Date'][i]), tmp['Temperature_Monte_Serra'][i], auser['Temperature_Monte_Serra'])","ba2d239c":"col = ['Depth_to_Groundwater_LT2',\n       'Depth_to_Groundwater_SAL',\n       'Depth_to_Groundwater_CoS']\ncolumns = auser.loc[:, col[:]].columns.to_list()\nfor column in columns:\n  print(column)\n  auser[f'{column}'] = np.where((auser[f'{column}'] == 0), np.nan, auser[f'{column}'])","865a7caf":"\ncol = auser.columns.to_list()\ncolumns = auser.loc[:, col[1:]].columns.to_list()\nfor column in columns:\n  print(column)\n  auser[f'{column}'] = auser[f'{column}'].interpolate()","218a8e51":"auser.Depth_to_Groundwater_CoS.isnull().sum()","fdc0036c":"auser = auser[auser.Depth_to_Groundwater_CoS.notna()].reset_index(drop=True)\nauser.info()","9b2124a3":"auser.isnull().sum()","2dec53fb":"# columns = auser.columns.to_list()\n# # Plotting the time series for every variable\n# fig, axs = plt.subplots(nrows = len(columns)-1, figsize= (10, 5 * len(columns)))\n\n\n# for var in range(1,len(columns)):\n#     axs[var-1].plot(auser['Date'], auser[columns[var]])\n    \n#     axs[var-1].set_xlabel('Date')\n#     axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n#     axs[var-1].grid()","cc4fe906":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,5))\nsns.heatmap(doganella.T.isna(), cmap='Blues')\nax.set_title('Fields with Missing Values', fontsize=16)\n#for tick in ax.xaxis.get_major_ticks():\n#    tick.label.set_fontsize(14) \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()","c7f250eb":"temperature_cols=[]\nfor column in doganella.columns:\n    if 'Temperature' in column:\n        temperature_cols.append(column)\ntemperature_cols","0926d8a5":"'''\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nimport gc\nimport lightgbm as lgb\n'''\n\ndef get_time_features(df):\n    df['year'] = pd.DatetimeIndex(df['Date']).year\n    df['month'] = pd.DatetimeIndex(df['Date']).month\n    df['day_in_year'] = pd.DatetimeIndex(df['Date']).dayofyear\n    df['week_in_year'] = pd.DatetimeIndex(df['Date']).weekofyear\n    return df\n\noriginal = doganella.copy()\ntmp_original = get_time_features(original)\ntmp_original.month = tmp_original.month.astype(int)\ntmp_original.year = tmp_original.year.astype(int)\n\nparams = {'num_leaves': 24,\n          'objective': 'regression_l1',\n          'max_depth': 8,\n          'learning_rate': 0.01,\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'verbose': -1,\n          'seed' : 42\n         }\n\ndef predict_missing_temperature(target):\n    test_df = tmp_original.copy()\n    train_df = tmp_original[tmp_original[target].notna()]\n\n    features = [c for c in (list(temperature_cols) + list(['month', 'day_in_year', 'week_in_year'])) if c != target]\n\n    X = train_df[features]\n    y = train_df[[target]]\n    X_test = test_df[features]\n\n    y_preds = np.zeros(X_test.shape[0])\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    dtrain = lgb.Dataset(X_train, y_train, params= {'verbose': -1})\n    dvalid = lgb.Dataset(X_valid, y_valid, params= {'verbose': -1})\n\n    # For analysis set 'verbose_eval' to 200, false\n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=False,  early_stopping_rounds=100)\n\n    y_pred_valid = clf.predict(X_valid)\n    y_preds = clf.predict(X_test)\n    \n    f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 4))\n    ax.set_title(f'Rolling Mean Temperatures for {target} \\n(Rolling Window: 28 Days, MAE: {mean_absolute_error(y_valid.rolling(28).mean()[28:], pd.Series(y_pred_valid).rolling(28).mean()[28:])})', fontsize=16)\n    old = tmp_original[target].copy().replace({np.nan : np.inf})\n    tmp_original[target] = np.where(tmp_original[target].isna(), pd.Series(y_preds), tmp_original[target])\n    sns.lineplot(x=original.Date, y=tmp_original[target].rolling(28).mean(), label='Imputed', color='darkorange')\n    sns.lineplot(x=original.Date, y=old.rolling(28).mean(), label='Original', color='dodgerblue')\n    ax.set_xlabel('Date', fontsize=14)\n#     ax.set_xlim([date(2010, 1, 1), date(2020, 6, 30)])\n    plt.show()\n\n    # return pd.Series(y_preds)\n    return tmp_original[target]\n\n# Sort temperature columns according to amount of missing values.\n# Fill NaN values for features with least missing values first\nsorted_temperature_cols = tmp_original[temperature_cols].isna().sum(axis=0).sort_values().index\n\nexample_col = 'Temperature_Monteporzio'\ndoganella['Temperature_Monteporzio'] = predict_missing_temperature(example_col)\n\nexample_col2 = 'Temperature_Velletri'\ndoganella['Temperature_Velletri'] = predict_missing_temperature(example_col2)","7d28d0af":"doganella = doganella[doganella.Date > '2016-10-06'].reset_index(drop=True)\ndoganella.info()","51792047":"col = ['Volume_Pozzo_1', 'Volume_Pozzo_2',\n       'Volume_Pozzo_3', 'Volume_Pozzo_4', 'Volume_Pozzo_5+6',\n       'Volume_Pozzo_7', 'Volume_Pozzo_8', 'Volume_Pozzo_9']\ncolumns = doganella.loc[:, col[:]].columns.to_list()\nfor column in columns:\n  print(column)\n  doganella[f'{column}'] = np.where((doganella[f'{column}'] == 0), np.nan, doganella[f'{column}'])","e734c02a":"col = doganella.columns.to_list()\ncolumns = doganella.loc[:, col[1:]].columns.to_list()\nfor column in columns:\n  print(column)\n  doganella[f'{column}'] = doganella[f'{column}'].interpolate()","6709aed6":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,5))\nsns.heatmap(doganella.T.isna(), cmap='Blues')\nax.set_title('Fields with Missing Values', fontsize=16)\n#for tick in ax.xaxis.get_major_ticks():\n#    tick.label.set_fontsize(14) \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()","e1f6511f":"doganella = doganella[doganella.Depth_to_Groundwater_Pozzo_4.notna()].reset_index(drop=True)\ndoganella.info()","5696fd0b":"doganella.isnull().sum()","75210d7f":"# columns = doganella.columns.to_list()\n# # Plotting the time series for every variable\n# fig, axs = plt.subplots(nrows = len(columns)-1, figsize= (10, 5 * len(columns)))\n\n\n# for var in range(1,len(columns)):\n#     axs[var-1].plot(doganella['Date'], doganella[columns[var]])\n    \n#     axs[var-1].set_xlabel('Date')\n#     axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n#     axs[var-1].grid()","6b5a77ca":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))\nsns.heatmap(luco.T.isna(), cmap='Blues')\nax.set_title('Fields with Missing Values', fontsize=16)\n#for tick in ax.xaxis.get_major_ticks():\n#    tick.label.set_fontsize(14) \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()","2a5f6a30":"luco = luco[luco.Rainfall_Siena_Poggio_al_Vento.notna()].reset_index(drop=True)\nluco.info()","d63c9d21":"luco = luco.drop(['Depth_to_Groundwater_Podere_Casetta'], axis=1)\nluco.columns","af4d745c":"col = luco.columns.to_list()\ncolumns = luco.loc[:, col[1:]].columns.to_list()\nfor column in columns:\n  print(column)\n  luco[f'{column}'] = luco[f'{column}'].interpolate()","7d270cf3":"luco.isnull().sum()","ef54b2c9":"# columns = luco.columns.to_list()\n# # Plotting the time series for every variable\n# fig, axs = plt.subplots(nrows = len(columns)-1, figsize= (10, 5 * len(columns)))\n\n\n# for var in range(1,len(columns)):\n#     axs[var-1].plot(luco['Date'], luco[columns[var]])\n    \n#     axs[var-1].set_xlabel('Date')\n#     axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n#     axs[var-1].grid()","58f9dd27":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(16,5))\nsns.heatmap(petrignano.T.isna(), cmap='Blues')\nax.set_title('Fields with Missing Values', fontsize=16)\n#for tick in ax.xaxis.get_major_ticks():\n#    tick.label.set_fontsize(14) \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()","a2899dee":"petrignano = petrignano[petrignano.Rainfall_Bastia_Umbra.notna()].reset_index(drop=True)\npetrignano.info()","5e935f1e":"col = ['Volume_C10_Petrignano',\n       'Hydrometry_Fiume_Chiascio_Petrignano']\ncolumns = petrignano.loc[:, col[:]].columns.to_list()\nfor column in columns:\n  print(column)\n  petrignano[f'{column}'] = np.where((petrignano[f'{column}'] == 0), np.nan, petrignano[f'{column}'])","2b68f00b":"col = petrignano.columns.to_list()\ncolumns = petrignano.loc[:, col[1:]].columns.to_list()\nfor column in columns:\n  print(column)\n  petrignano[f'{column}'] = petrignano[f'{column}'].interpolate()","5cb6b45c":"petrignano.isnull().sum()","3ccb8540":"# columns = petrignano.columns.to_list()\n# # Plotting the time series for every variable\n# fig, axs = plt.subplots(nrows = len(columns)-1, figsize= (10, 5 * len(columns)))\n\n\n# for var in range(1,len(columns)):\n#     axs[var-1].plot(petrignano['Date'], petrignano[columns[var]])\n    \n#     axs[var-1].set_xlabel('Date')\n#     axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n#     axs[var-1].grid()","c88982f4":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,8))\nsns.heatmap(amiata.T.isna(), cmap='Blues')\nax.set_title('Fields with Missing Values', fontsize=16)\n\nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()","ce98d2a7":"amiata = amiata[amiata.Flow_Rate_Galleria_Alta.notna()].reset_index(drop=True)\namiata.info()","87914bdf":"amiata = amiata[amiata.Rainfall_S_Fiora.notna()].reset_index(drop=True)\namiata.info()","19b24e0a":"col = ['Flow_Rate_Bugnano', 'Flow_Rate_Arbure', 'Flow_Rate_Ermicciolo']\ncolumns = amiata.loc[:, col[:]].columns.to_list()\nfor column in columns:\n  print(column)\n  amiata[f'{column}'] = np.where((amiata[f'{column}'] == 0), np.nan, amiata[f'{column}'])","65d46bca":"col = amiata.columns.to_list()\ncolumns = amiata.loc[:, col[1:]].columns.to_list()\nfor column in columns:\n  print(column)\n  amiata[f'{column}'] = amiata[f'{column}'].interpolate()","f3701f07":"amiata = amiata[amiata.Rainfall_Abbadia_S_Salvatore.notna()].reset_index(drop=True)\namiata.info()","4ace214b":"amiata.isnull().sum()","4b53141a":"# columns = amiata.columns.to_list()\n# # Plotting the time series for every variable\n# fig, axs = plt.subplots(nrows = len(columns)-1, figsize= (10, 5 * len(columns)))\n\n\n# for var in range(1,len(columns)):\n#     axs[var-1].plot(amiata['Date'], amiata[columns[var]])\n    \n#     axs[var-1].set_xlabel('Date')\n#     axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n#     axs[var-1].grid()","9010d23a":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(15,8))\nsns.heatmap(madonna.T.isna(), cmap='Blues')\nax.set_title('Fields with Missing Values', fontsize=16)\n\nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()","9afa12ca":"madonna = madonna[madonna['Date'] < '2019-01-01'].reset_index(drop=True)\nmadonna.isnull().sum()","1ae7c13c":"madonna = madonna[madonna['Date'] >= '2015-03-13'].reset_index(drop=True)\nmadonna.head()","fb069ee6":"col = madonna.columns.to_list()\ncolumns = madonna.loc[:, col[1:]].columns.to_list()\nfor column in columns:\n  print(column)\n  madonna[f'{column}'] = madonna[f'{column}'].interpolate()","8393286c":"madonna.isnull().sum()","c31cca1d":"# columns = madonna.columns.to_list()\n# # Plotting the time series for every variable\n# fig, axs = plt.subplots(nrows = len(columns)-1, figsize= (10, 5 * len(columns)))\n\n\n# for var in range(1,len(columns)):\n#     axs[var-1].plot(madonna['Date'], madonna[columns[var]])\n    \n#     axs[var-1].set_xlabel('Date')\n#     axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n#     axs[var-1].grid()","9642161a":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,10))\nsns.heatmap(arno.T.isna(), cmap='Blues')\nax.set_title('Fields with Missing Values', fontsize=16)\n\nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()","d0bdeda6":"columns = ['Date', 'Rainfall_Le_Croci', 'Rainfall_Cavallina', 'Rainfall_S_Agata',\n       'Rainfall_Mangona', 'Rainfall_S_Piero', 'Temperature_Firenze',\n       'Hydrometry_Nave_di_Rosano']\narno = arno[columns]\narno.info()","da4446fa":"temperature_cols=[]\nfor column in arno.columns:\n    if 'Temperature' in column:\n        temperature_cols.append(column)\ntemperature_cols","56d5b14f":"def get_time_features(df):\n    df['year'] = pd.DatetimeIndex(df['Date']).year\n    df['month'] = pd.DatetimeIndex(df['Date']).month\n    df['day_in_year'] = pd.DatetimeIndex(df['Date']).dayofyear\n    df['week_in_year'] = pd.DatetimeIndex(df['Date']).weekofyear\n    return df\n\noriginal = arno.copy()\ntmp_original = get_time_features(original)\ntmp_original.month = tmp_original.month.astype(int)\ntmp_original.year = tmp_original.year.astype(int)\n\nparams = {'num_leaves': 24,\n          'objective': 'regression_l1',\n          'max_depth': 8,\n          'learning_rate': 0.01,\n          \"metric\": 'mae',\n          \"verbosity\": -1,\n          'verbose': -1,\n          'seed' : 42\n         }\n\ndef predict_missing_temperature(target):\n    test_df = tmp_original.copy()\n    train_df = tmp_original[tmp_original[target].notna()]\n\n    features = [c for c in (list(temperature_cols) + list(['month', 'day_in_year', 'week_in_year'])) if c != target]\n\n    X = train_df[features]\n    y = train_df[[target]]\n    X_test = test_df[features]\n\n    y_preds = np.zeros(X_test.shape[0])\n\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    dtrain = lgb.Dataset(X_train, y_train, params= {'verbose': -1})\n    dvalid = lgb.Dataset(X_valid, y_valid, params= {'verbose': -1})\n\n    # For analysis set 'verbose_eval' to 200, false\n    clf = lgb.train(params, dtrain, 10000, valid_sets = [dtrain, dvalid], verbose_eval=False,  early_stopping_rounds=100)\n\n    y_pred_valid = clf.predict(X_valid)\n    y_preds = clf.predict(X_test)\n    \n    f, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 4))\n    ax.set_title(f'Rolling Mean Temperatures for {target} \\n(Rolling Window: 28 Days, MAE: {mean_absolute_error(y_valid.rolling(28).mean()[28:], pd.Series(y_pred_valid).rolling(28).mean()[28:])})', fontsize=16)\n    old = tmp_original[target].copy().replace({np.nan : np.inf})\n    tmp_original[target] = np.where(tmp_original[target].isna(), pd.Series(y_preds), tmp_original[target])\n    sns.lineplot(x=original.Date, y=tmp_original[target].rolling(28).mean(), label='Imputed', color='darkorange')\n    sns.lineplot(x=original.Date, y=old.rolling(28).mean(), label='Original', color='dodgerblue')\n    ax.set_xlabel('Date', fontsize=14)\n    # ax.set_xlim([date(2014, 1, 1), date(2020, 6, 30)])\n    plt.show()\n\n    # return pd.Series(y_preds)\n    return tmp_original[target]\n\n# Sort temperature columns according to amount of missing values.\n# Fill NaN values for features with least missing values first\nsorted_temperature_cols = tmp_original[temperature_cols].isna().sum(axis=0).sort_values().index\n\nexample_col = 'Temperature_Firenze'\narno['Temperature_Firenze'] = predict_missing_temperature(example_col)","cca00c72":"col = ['Hydrometry_Nave_di_Rosano']\ncolumns = arno.loc[:, col[:]].columns.to_list()\nfor column in columns:\n  print(column)\n  arno[f'{column}'] = np.where((arno[f'{column}'] == 0), np.nan, arno[f'{column}'])","20e91da6":"col = arno.columns.to_list()\ncolumns = arno.loc[:, col[1:]].columns.to_list()\nfor column in columns:\n  print(column)\n  arno[f'{column}'] = arno[f'{column}'].interpolate()","91ae78d0":"arno = arno[arno.Rainfall_Le_Croci.notna()].reset_index(drop=True)\narno.info()","22a8ee93":"arno.isnull().sum()","d8d73000":"# columns = arno.columns.to_list()\n# # Plotting the time series for every variable\n# fig, axs = plt.subplots(nrows = len(columns)-1, figsize= (10, 5 * len(columns)))\n\n\n# for var in range(1,len(columns)):\n#     axs[var-1].plot(arno['Date'], arno[columns[var]])\n    \n#     axs[var-1].set_xlabel('Date')\n#     axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n#     axs[var-1].grid()","bf39764f":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(15, 8))\nsns.heatmap(bilancino.T.isna(), cmap='Blues')\nax.set_title('Fields with Missing Values', fontsize=16)\n\nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\nplt.show()","ffcd024e":"bilancino = bilancino[bilancino.Temperature_Le_Croci.notna()].reset_index(drop=True)\nbilancino.info()","6a540506":"bilancino.isnull().sum()","648cfccb":"# columns = bilancino.columns.to_list()\n# # Plotting the time series for every variable\n# fig, axs = plt.subplots(nrows = len(columns)-1, figsize= (10, 5 * len(columns)))\n\n\n# for var in range(1,len(columns)):\n#     axs[var-1].plot(bilancino['Date'], bilancino[columns[var]])\n    \n#     axs[var-1].set_xlabel('Date')\n#     axs[var-1].set_ylabel(columns[var], fontsize = 15)\n    \n#     axs[var-1].grid()","fe0ec1e7":"Auser_Rainfall = auser.iloc[:,0:11]\nAuser_Rainfall.head()","ebd3992a":"Auser_Rainfall.set_index(\"Date\",inplace=True)","1f0be96a":"Auser_Rainfall_Relation=Auser_Rainfall.corr()\nAuser_Rainfall_Relation.head()","efe13499":"sns.heatmap(Auser_Rainfall_Relation,annot=True)\nAuser_Rainfall.resample(rule='A').mean()['2015']\n\n","26055cd5":"Auser_ground = auser.iloc[:,11:16]\nAuser_ground_relation = Auser_ground.corr()\nAuser_ground_relation","dcb61701":"sns.heatmap(Auser_ground_relation,annot=True)\n","9ddd93ce":"Temperature=auser[['Date','Temperature_Orentano','Temperature_Monte_Serra','Temperature_Ponte_a_Moriano','Temperature_Lucca_Orto_Botanico']]\nTemperature\n","a0453025":"Temperature_relation = Temperature.corr()\nsns.heatmap(Temperature_relation,annot=True)\n\n","974e661b":"plot_corr_hitmap(arno, 'River Arno')","e1952fd0":"bilancino.info()","87b6ba0d":"arno.info()","97064643":"tmp = pd.merge(arno, bilancino, left_on='Date', right_on='Date', how='left')\ncolumns = ['Date', 'Rainfall_Le_Croci_x', 'Rainfall_Cavallina_x', 'Rainfall_S_Agata_x',\n       'Rainfall_Mangona_x', 'Rainfall_S_Piero_x', 'Temperature_Firenze',\n       'Hydrometry_Nave_di_Rosano', 'Lake_Level', 'Flow_Rate']\narno = tmp[columns]\n\nnew_columns = ['Date', 'Rainfall_Le_Croci', 'Rainfall_Cavallina', 'Rainfall_S_Agata',\n       'Rainfall_Mangona', 'Rainfall_S_Piero', 'Temperature_Firenze',\n       'Hydrometry_Nave_di_Rosano', 'Lake_Level', 'Flow_Rate']\narno.columns = new_columns\narno.columns","064b9705":"arno.info()","9eae5438":"arno = arno[arno.Lake_Level.notna()].reset_index(drop=True)\narno.info()","e25fec83":"plot_corr_hitmap(bilancino, 'Lake Bilancino')","12d03d41":"auser.info()","c58cb89c":"# As we can see, downsample to weekly could smooth the data and hgelp with analysis\ncolumns = auser.columns.to_list()\ndownsample = auser[columns].resample('7D', on='Date').mean().reset_index(drop=False)\nauser_ds = downsample.copy()","6599e733":"auser_ds.info()","512ce23c":"auser_ds = auser_ds.drop(['Depth_to_Groundwater_LT2', 'Depth_to_Groundwater_PAG', 'Depth_to_Groundwater_CoS', 'Depth_to_Groundwater_DIEC'], axis=1)\nauser_ds.columns","170001e1":"auser_ds.info()","ba4dd859":"target_column=[]\nfor column in auser_ds.columns:\n    if 'Depth' in column:\n        target_column.append(column)\ntarget_column","81c9f674":"feature_columns = [feature for feature in auser_ds.columns.to_list() if feature not in target_column ][1:]\n\ntrain_size = int(0.85 * len(auser_ds))\n\nmultivariate_df = auser_ds[['Date'] + target_column + feature_columns].copy()\nmultivariate_df.columns = ['ds', 'y'] + feature_columns\n\ntrain = multivariate_df.iloc[:train_size, :]\n\ncol_tmp = [i for i in range(2, len(feature_columns)+2)]\ncol_tmp.append(0)\ncol_tmp.sort()\nx_train, y_train = pd.DataFrame(multivariate_df.iloc[:train_size, col_tmp]), pd.DataFrame(multivariate_df.iloc[:train_size, 1])\nx_valid, y_valid = pd.DataFrame(multivariate_df.iloc[train_size:, col_tmp]), pd.DataFrame(multivariate_df.iloc[train_size:, 1])\n\ntrain.head()","7111ee10":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom fbprophet import Prophet\n\n\n# Train the model\nmodel = Prophet()\nfor f_col in feature_columns:\n  print(f_col)\n  model.add_regressor(f'{f_col}')\n\n# Fit the model with train set\nmodel.fit(train)\n\n# Predict on valid set\ny_pred = model.predict(x_valid)\n\n# Calcuate metrics\nscore_mae = mean_absolute_error(y_valid, y_pred['yhat'])\nscore_rmse = math.sqrt(mean_squared_error(y_valid, y_pred['yhat']))\n\nprint('RMSE: {}'.format(score_rmse))","2c536e05":"# Plot the forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nmodel.plot(y_pred, ax=ax)\nsns.lineplot(x=x_valid['ds'], y=y_valid['y'], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Depth to Groundwater', fontsize=14)\n\nplt.show()","19cedc2a":"target_mean = abs(y_valid['y'].mean())\ntarget_mean","cc221f28":"# Relative MAE\nscore_mae\/target_mean","971ff8a0":"# Relative RMSE\nscore_rmse\/target_mean","9f9b14c9":"amiata.info()","84e0e9fe":"# As we can see, downsample to weekly could smooth the data and hgelp with analysis\ncolumns = amiata.columns.to_list()\ndownsample = amiata[columns].resample('7D', on='Date').mean().reset_index(drop=False)\namiata_ds = downsample.copy()","38547021":"amiata_ds.info()","3ebd1229":"target_column=[]\nfor column in amiata_ds.columns:\n    if 'Flow_Rate' in column:\n        target_column.append(column)\ntarget_column","4c8c866c":"# CASE : 'Flow_Rate_Bugnano'\nfeature_columns = [feature for feature in amiata_ds.columns.to_list() if feature not in target_column ][1:]\n\ntrain_size = int(0.85 * len(amiata_ds))\n\nmultivariate_df = amiata_ds[['Date'] + [target_column[0]] + feature_columns].copy()\nmultivariate_df.columns = ['ds', 'y'] + feature_columns\n\ntrain = multivariate_df.iloc[:train_size, :]\n\ncol_tmp = [i for i in range(2, len(feature_columns)+2)]\ncol_tmp.append(0)\ncol_tmp.sort()\nx_train, y_train = pd.DataFrame(multivariate_df.iloc[:train_size, col_tmp]), pd.DataFrame(multivariate_df.iloc[:train_size, 1])\nx_valid, y_valid = pd.DataFrame(multivariate_df.iloc[train_size:, col_tmp]), pd.DataFrame(multivariate_df.iloc[train_size:, 1])\n\ntrain.head()","6f34d11e":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom fbprophet import Prophet\n\n\n# Train the model\nmodel = Prophet()\nfor f_col in feature_columns:\n  print(f_col)\n  model.add_regressor(f'{f_col}')\n\n# Fit the model with train set\nmodel.fit(train)\n\n# Predict on valid set\ny_pred = model.predict(x_valid)\n\n# Calcuate metrics\nscore_mae = mean_absolute_error(y_valid, y_pred['yhat'])\nscore_rmse = math.sqrt(mean_squared_error(y_valid, y_pred['yhat']))\n\nprint('RMSE: {}'.format(score_rmse))","decc1950":"# Plot the forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nmodel.plot(y_pred, ax=ax)\nsns.lineplot(x=x_valid['ds'], y=y_valid['y'], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Flow Rate', fontsize=14)\n\nplt.show()","01790de6":"Bugnano_target_mean = abs(y_valid['y'].mean())\nBugnano_target_mean","cb483009":"# Relative MAE\nscore_mae\/Bugnano_target_mean","35ce1241":"# Relative RMSE\nscore_rmse\/Bugnano_target_mean","0a36975a":"# CASE : 'Flow_Rate_Arbure'\nfeature_columns = [feature for feature in amiata_ds.columns.to_list() if feature not in target_column ][1:]\n\ntrain_size = int(0.85 * len(amiata_ds))\n\nmultivariate_df = amiata_ds[['Date'] + [target_column[1]] + feature_columns].copy()\nmultivariate_df.columns = ['ds', 'y'] + feature_columns\n\ntrain = multivariate_df.iloc[:train_size, :]\n\ncol_tmp = [i for i in range(2, len(feature_columns)+2)]\ncol_tmp.append(0)\ncol_tmp.sort()\nx_train, y_train = pd.DataFrame(multivariate_df.iloc[:train_size, col_tmp]), pd.DataFrame(multivariate_df.iloc[:train_size, 1])\nx_valid, y_valid = pd.DataFrame(multivariate_df.iloc[train_size:, col_tmp]), pd.DataFrame(multivariate_df.iloc[train_size:, 1])\n\ntrain.head()","2dccb261":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom fbprophet import Prophet\n\n\n# Train the model\nmodel = Prophet()\nfor f_col in feature_columns:\n  print(f_col)\n  model.add_regressor(f'{f_col}')\n\n# Fit the model with train set\nmodel.fit(train)\n\n# Predict on valid set\ny_pred = model.predict(x_valid)\n\n# Calcuate metrics\nscore_mae = mean_absolute_error(y_valid, y_pred['yhat'])\nscore_rmse = math.sqrt(mean_squared_error(y_valid, y_pred['yhat']))\n\nprint('RMSE: {}'.format(score_rmse))","a92ed943":"# Plot the forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nmodel.plot(y_pred, ax=ax)\nsns.lineplot(x=x_valid['ds'], y=y_valid['y'], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Flow Rate', fontsize=14)\n\nplt.show()","e0da894c":"Arbure_target_mean = abs(y_valid['y'].mean())\nArbure_target_mean","5a3f3a00":"# Relative MAE\nscore_mae\/Arbure_target_mean","ce6bc633":"# Relative RMSE\nscore_rmse\/Arbure_target_mean","466bb946":"# CASE : 'Flow_Rate_Ermicciolo'\nfeature_columns = [feature for feature in amiata_ds.columns.to_list() if feature not in target_column ][1:]\n\ntrain_size = int(0.85 * len(amiata_ds))\n\nmultivariate_df = amiata_ds[['Date'] + [target_column[2]] + feature_columns].copy()\nmultivariate_df.columns = ['ds', 'y'] + feature_columns\n\ntrain = multivariate_df.iloc[:train_size, :]\n\ncol_tmp = [i for i in range(2, len(feature_columns)+2)]\ncol_tmp.append(0)\ncol_tmp.sort()\nx_train, y_train = pd.DataFrame(multivariate_df.iloc[:train_size, col_tmp]), pd.DataFrame(multivariate_df.iloc[:train_size, 1])\nx_valid, y_valid = pd.DataFrame(multivariate_df.iloc[train_size:, col_tmp]), pd.DataFrame(multivariate_df.iloc[train_size:, 1])\n\ntrain.head()","2a572f0f":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom fbprophet import Prophet\n\n\n# Train the model\nmodel = Prophet()\nfor f_col in feature_columns:\n  print(f_col)\n  model.add_regressor(f'{f_col}')\n\n# Fit the model with train set\nmodel.fit(train)\n\n# Predict on valid set\ny_pred = model.predict(x_valid)\n\n# Calcuate metrics\nscore_mae = mean_absolute_error(y_valid, y_pred['yhat'])\nscore_rmse = math.sqrt(mean_squared_error(y_valid, y_pred['yhat']))\n\nprint('RMSE: {}'.format(score_rmse))","770e6627":"# Plot the forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nmodel.plot(y_pred, ax=ax)\nsns.lineplot(x=x_valid['ds'], y=y_valid['y'], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Flow Rate', fontsize=14)\n\nplt.show()","6faa3e26":"Ermicciolo_target_mean = abs(y_valid['y'].mean())\nErmicciolo_target_mean","1339ce47":"# Relative MAE\nscore_mae\/Ermicciolo_target_mean","11d0b63a":"# Relative RMSE\nscore_rmse\/Ermicciolo_target_mean","4901a7e6":"# CASE : 'Flow_Rate_Galleria_Alta'\nfeature_columns = [feature for feature in amiata_ds.columns.to_list() if feature not in target_column ][1:]\n\ntrain_size = int(0.85 * len(amiata_ds))\n\nmultivariate_df = amiata_ds[['Date'] + [target_column[3]] + feature_columns].copy()\nmultivariate_df.columns = ['ds', 'y'] + feature_columns\n\ntrain = multivariate_df.iloc[:train_size, :]\n\ncol_tmp = [i for i in range(2, len(feature_columns)+2)]\ncol_tmp.append(0)\ncol_tmp.sort()\nx_train, y_train = pd.DataFrame(multivariate_df.iloc[:train_size, col_tmp]), pd.DataFrame(multivariate_df.iloc[:train_size, 1])\nx_valid, y_valid = pd.DataFrame(multivariate_df.iloc[train_size:, col_tmp]), pd.DataFrame(multivariate_df.iloc[train_size:, 1])\n\ntrain.head()","b5b78e08":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom fbprophet import Prophet\n\n\n# Train the model\nmodel = Prophet()\nfor f_col in feature_columns:\n  print(f_col)\n  model.add_regressor(f'{f_col}')\n\n# Fit the model with train set\nmodel.fit(train)\n\n# Predict on valid set\ny_pred = model.predict(x_valid)\n\n# Calcuate metrics\nscore_mae = mean_absolute_error(y_valid, y_pred['yhat'])\nscore_rmse = math.sqrt(mean_squared_error(y_valid, y_pred['yhat']))\n\nprint('RMSE: {}'.format(score_rmse))","61f3b67d":"# Plot the forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nmodel.plot(y_pred, ax=ax)\nsns.lineplot(x=x_valid['ds'], y=y_valid['y'], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Flow Rate', fontsize=14)\n\nplt.show()","9515b1de":"Galleria_Alta_target_mean = abs(y_valid['y'].mean())\nGalleria_Alta_target_mean","57f557b3":"# Relative MAE\nscore_mae\/Galleria_Alta_target_mean","c0e4be51":"# Relative RMSE\nscore_rmse\/Galleria_Alta_target_mean","d5403de9":"arno.info()","cd5f7c74":"# As we can see, downsample to weekly could smooth the data and hgelp with analysis\ncolumns = arno.columns.to_list()\ndownsample = arno[columns].resample('7D', on='Date').mean().reset_index(drop=False)\narno_ds = downsample.copy()","4c37516f":"arno_ds.info()","d8f97b11":"target_column=[]\nfor column in arno_ds.columns:\n  if 'Hydrometry' in column:\n    target_column.append(column)\ntarget_column","15a2060b":"feature_columns = [feature for feature in arno_ds.columns.to_list() if feature not in target_column ][1:]\n\ntrain_size = int(0.85 * len(arno_ds))\n\nmultivariate_df = arno_ds[['Date'] + target_column + feature_columns].copy()\nmultivariate_df.columns = ['ds', 'y'] + feature_columns\n\ntrain = multivariate_df.iloc[:train_size, :]\n\ncol_tmp = [i for i in range(2, len(feature_columns)+2)]\ncol_tmp.append(0)\ncol_tmp.sort()\nx_train, y_train = pd.DataFrame(multivariate_df.iloc[:train_size, col_tmp]), pd.DataFrame(multivariate_df.iloc[:train_size, 1])\nx_valid, y_valid = pd.DataFrame(multivariate_df.iloc[train_size:, col_tmp]), pd.DataFrame(multivariate_df.iloc[train_size:, 1])\n\ntrain.head()","74c8ce87":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom fbprophet import Prophet\n\n\n# Train the model\nmodel = Prophet()\nfor f_col in feature_columns:\n  print(f_col)\n  model.add_regressor(f'{f_col}')\n\n# Fit the model with train set\nmodel.fit(train)\n\n# Predict on valid set\ny_pred = model.predict(x_valid)\n\n# Calcuate metrics\nscore_mae = mean_absolute_error(y_valid, y_pred['yhat'])\nscore_rmse = math.sqrt(mean_squared_error(y_valid, y_pred['yhat']))\n\nprint('RMSE: {}'.format(score_rmse))","2a9f06e5":"# Plot the forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nmodel.plot(y_pred, ax=ax)\nsns.lineplot(x=x_valid['ds'], y=y_valid['y'], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Lake Level', fontsize=14)\n\nplt.show()","090acd48":"target_mean = abs(y_valid['y'].mean())\ntarget_mean","793de419":"# Relative MAE\nscore_mae\/target_mean","95c7a7ce":"# Relative RMSE\nscore_rmse\/target_mean","f04f981d":"bilancino.info()","039da291":"# As we can see, downsample to weekly could smooth the data and hgelp with analysis\ncolumns = bilancino.columns.to_list()\ndownsample = bilancino[columns].resample('7D', on='Date').mean().reset_index(drop=False)\nbilancino_ds = downsample.copy()","c8d62f01":"bilancino_ds.info()","ce12558e":"target_column=[]\nfor column in bilancino_ds.columns:\n  if 'Lake_Level' in column:\n    target_column.append(column)\n  if 'Flow_Rate' in column:\n    target_column.append(column)\ntarget_column","6bbcb09a":"# CASE : Lake_Level\nfeature_columns = [feature for feature in bilancino_ds.columns.to_list() if feature not in target_column ][1:]\n\ntrain_size = int(0.85 * len(bilancino_ds))\n\nmultivariate_df = bilancino_ds[['Date'] + [target_column[0]] + feature_columns].copy()\nmultivariate_df.columns = ['ds', 'y'] + feature_columns\n\ntrain = multivariate_df.iloc[:train_size, :]\n\ncol_tmp = [i for i in range(2, len(feature_columns)+2)]\ncol_tmp.append(0)\ncol_tmp.sort()\nx_train, y_train = pd.DataFrame(multivariate_df.iloc[:train_size, col_tmp]), pd.DataFrame(multivariate_df.iloc[:train_size, 1])\nx_valid, y_valid = pd.DataFrame(multivariate_df.iloc[train_size:, col_tmp]), pd.DataFrame(multivariate_df.iloc[train_size:, 1])\n\ntrain.head()","692adf50":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom fbprophet import Prophet\n\n\n# Train the model\nmodel = Prophet()\nfor f_col in feature_columns:\n  print(f_col)\n  model.add_regressor(f'{f_col}')\n\n# Fit the model with train set\nmodel.fit(train)\n\n# Predict on valid set\ny_pred = model.predict(x_valid)\n\n# Calcuate metrics\nscore_mae = mean_absolute_error(y_valid, y_pred['yhat'])\nscore_rmse = math.sqrt(mean_squared_error(y_valid, y_pred['yhat']))\n\nprint('RMSE: {}'.format(score_rmse))","65a2f06e":"# Plot the forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nmodel.plot(y_pred, ax=ax)\nsns.lineplot(x=x_valid['ds'], y=y_valid['y'], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Lake Level', fontsize=14)\n\nplt.show()","c88937e9":"Level_target_mean = abs(y_valid['y'].mean())\nLevel_target_mean","d1971109":"# Relative MAE\nscore_mae\/Level_target_mean","c6991824":"# Relative RMSE\nscore_rmse\/Level_target_mean","a88edba5":"# CASE : Flow_Rate\nfeature_columns = [feature for feature in bilancino_ds.columns.to_list() if feature not in target_column ][1:]\n\ntrain_size = int(0.85 * len(bilancino_ds))\n\nmultivariate_df = bilancino_ds[['Date'] + [target_column[1]] + feature_columns].copy()\nmultivariate_df.columns = ['ds', 'y'] + feature_columns\n\ntrain = multivariate_df.iloc[:train_size, :]\ncol_tmp = [i for i in range(2, len(feature_columns)+2)]\ncol_tmp.append(0)\ncol_tmp.sort()\nx_train, y_train = pd.DataFrame(multivariate_df.iloc[:train_size, col_tmp]), pd.DataFrame(multivariate_df.iloc[:train_size, 1])\nx_valid, y_valid = pd.DataFrame(multivariate_df.iloc[train_size:, col_tmp]), pd.DataFrame(multivariate_df.iloc[train_size:, 1])\n\ntrain.head()","f3259779":"from sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom fbprophet import Prophet\n\n\n# Train the model\nmodel = Prophet()\nfor f_col in feature_columns:\n  print(f_col)\n  model.add_regressor(f'{f_col}')\n\n# Fit the model with train set\nmodel.fit(train)\n\n# Predict on valid set\ny_pred = model.predict(x_valid)\n\n# Calcuate metrics\nscore_mae = mean_absolute_error(y_valid, y_pred['yhat'])\nscore_rmse = math.sqrt(mean_squared_error(y_valid, y_pred['yhat']))\n\nprint('RMSE: {}'.format(score_rmse))","1bc504c0":"# Plot the forecast\nf, ax = plt.subplots(1)\nf.set_figheight(6)\nf.set_figwidth(15)\n\nmodel.plot(y_pred, ax=ax)\nsns.lineplot(x=x_valid['ds'], y=y_valid['y'], ax=ax, color='orange', label='Ground truth') #navajowhite\n\nax.set_title(f'Prediction \\n MAE: {score_mae:.2f}, RMSE: {score_rmse:.2f}', fontsize=14)\nax.set_xlabel(xlabel='Date', fontsize=14)\nax.set_ylabel(ylabel='Flow Rate', fontsize=14)\n\nplt.show()","1e7639f9":"Rate_target_mean = abs(y_valid['y'].mean())\nRate_target_mean","e39a4c93":"# Relative MAE\nscore_mae\/Rate_target_mean","0d2d566e":"# Relative RMSE\nscore_rmse\/Rate_target_mean","d4a79cdc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport random\npd.set_option('display.max_columns',100)\npd.set_option('display.max_row',1000)","5b2f223d":"data = auser.copy()\ndata['Date']=pd.to_datetime(data['Date'],format ='%d\/%m\/%Y')\ndata.set_index(\"Date\",inplace=True)\ndata=data.dropna()\ndata","5b8d6c27":"data = auser.copy()\ndata.set_index(\"Date\",inplace=True)\ndata","9aa87c9c":"DepthToGroundColumns=[]\nfor column in data.columns:\n    if 'Depth' in column:\n        DepthToGroundColumns.append(column)\nDepthToGroundColumns","c46a77af":"dayToUnderground = 3\ndataShift = data.copy()\ndataShift[DepthToGroundColumns] = dataShift[DepthToGroundColumns].shift(-dayToUnderground,freq='D')\ndataShift.tail()","ad95e302":"dataSet = dataShift.dropna()\ndataSet","d83566c5":"dataSetExtract = dataSet.drop(DepthToGroundColumns,axis=1)\ndataSetExtract.head()","6be97557":"dataGround = dataSet[DepthToGroundColumns]\ndataGround\ndataModified = pd.concat([dataGround,dataSetExtract],axis=1)\ndataModified.iloc[:,:-5]","ed7091ed":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndataSetScaler = scaler.fit_transform(dataModified)\nprint(dataSetScaler.shape)","c170bf6a":"dataSetScaled = dataSetScaler","8b8787f7":"from numpy import array","199f1434":"n_steps_in, n_steps_out = 120 , 20\ndef split_dataSet(data,n_steps_in,n_steps_out):\n    X,y = list(),list()\n    for i in range(len(dataSetScaled)):\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out-1\n        if out_end_ix > len(data):\n            break\n        data_x,data_y = data[i:end_ix, :],data[end_ix-1:out_end_ix,:]\n        X.append(data_x)\n        y.append(data_y)\n    return np.array(X), np.array(y)","af5c766a":"from sklearn.model_selection import train_test_split\nX,y = split_dataSet(dataSetScaled,n_steps_in,n_steps_out)\nprint(X.shape)\nprint(y.shape)\n\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n\nn_features = x_train.shape[2]\nx_train.shape","0f53a54c":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed","73a6edf9":"model = Sequential()\nmodel.add(LSTM(200, input_shape=(n_steps_in, n_features)))\nmodel.add(RepeatVector(n_steps_out))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(TimeDistributed(Dense(n_features)))\nmodel.compile(optimizer='adam', loss='mse')","1352ce15":"model.fit(x_train, y_train, epochs=100,validation_data=(x_test,y_test) ,verbose=1)","25d61f8c":"predicData = model.predict(x_test, verbose=1)\npredictlist20days=[]\nfor i in range(len(predicData[0])):\n    predictlist20days.append(predicData[0][i][0])\ndays120list=[]\nfor i in range(len(x_test[0])):\n    days120list.append(x_test[0][i][0])\n\ndays120list20days=[]\nfor i in range(len(y_test[0])):\n    days120list20days.append(y_test[0][i][0])\npredictlist20days","90a1e24b":"aAxis=np.arange(0,len(days120list))\nbAxis=np.arange(len(days120list)-1,len(days120list)+19)\nplt.plot(aAxis,days120list)\nplt.plot(bAxis,days120list20days,color='cyan')\nplt.plot(bAxis,predictlist20days,color='red')","1c1636dd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport random\npd.set_option('display.max_columns',100)\npd.set_option('display.max_row',1000)","0a777982":"data = amiata.copy()\ndata['Date']=pd.to_datetime(data['Date'],format ='%d\/%m\/%Y')\ndata.set_index(\"Date\",inplace=True)\ndata=data.dropna()\ndata","54f9a4fd":"DepthToGroundColumns=[]\nfor column in data.columns:\n    if 'Hydro' in column:\n        DepthToGroundColumns.append(column)\nDepthToGroundColumns","78b497e3":"dayToUnderground = 0\ndataShift = data.copy()\ndataShift[DepthToGroundColumns] = dataShift[DepthToGroundColumns].shift(-dayToUnderground,freq='D')\ndataShift.tail()","f71c12b2":"dataSet = dataShift.dropna()\ndataSet","130b70f4":"dataSetExtract = dataSet.drop(DepthToGroundColumns,axis=1)\ndataSetExtract.head()","e1e7d196":"dataGround = dataSet[DepthToGroundColumns]\ndataGround\ndataModified = pd.concat([dataGround,dataSetExtract],axis=1)\ndataModified.iloc[:,:-5]","14e279aa":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndataSetScaler = scaler.fit_transform(dataModified)\nprint(dataSetScaler.shape)","cce64c5e":"dataSetScaled = dataSetScaler","a050817b":"from numpy import array","ea7e18a1":"n_steps_in, n_steps_out = 120 , 20\ndef split_dataSet(data,n_steps_in,n_steps_out):\n    X,y = list(),list()\n    for i in range(len(dataSetScaled)):\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out-1\n        if out_end_ix > len(data):\n            break\n        data_x,data_y = data[i:end_ix, :],data[end_ix-1:out_end_ix,:]\n        X.append(data_x)\n        y.append(data_y)\n    return np.array(X), np.array(y)","24678ea0":"from sklearn.model_selection import train_test_split\nX,y = split_dataSet(dataSetScaled,n_steps_in,n_steps_out)\nprint(X.shape)\nprint(y.shape)\n\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n\nn_features = x_train.shape[2]\nx_train.shape","db774d3f":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed","c871f967":"model = Sequential()\nmodel.add(LSTM(200, input_shape=(n_steps_in, n_features)))\nmodel.add(RepeatVector(n_steps_out))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(TimeDistributed(Dense(n_features)))\nmodel.compile(optimizer='adam', loss='mse')","8803cde9":"model.fit(x_train, y_train, epochs=100,validation_data=(x_test,y_test) ,verbose=1)\n\n","3c8d1be3":"predicData = model.predict(x_test, verbose=1)\npredictlist20days=[]\nfor i in range(len(predicData[0])):\n    predictlist20days.append(predicData[0][i][-1])\ndays120list=[]\nfor i in range(len(x_test[0])):\n    days120list.append(x_test[0][i][-1])\n\ndays120list20days=[]\nfor i in range(len(y_test[0])):\n    days120list20days.append(y_test[0][i][-1])\npredictlist20days\n\n","84394718":"aAxis=np.arange(0,len(days120list))\nbAxis=np.arange(len(days120list)-1,len(days120list)+19)\nplt.plot(aAxis,days120list)\nplt.plot(bAxis,days120list20days,color='cyan')\nplt.plot(bAxis,predictlist20days,color='red')","6b3ab7d5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport random\npd.set_option('display.max_columns',100)\npd.set_option('display.max_row',1000)","3ac49515":"data = arno.copy()\ndata['Date']=pd.to_datetime(data['Date'],format ='%d\/%m\/%Y')\ndata.set_index(\"Date\",inplace=True)\ndata=data.dropna()\ndata","d026032f":"DepthToGroundColumns=[]\nfor column in data.columns:\n    if 'Hydro' in column:\n        DepthToGroundColumns.append(column)\nDepthToGroundColumns","abc1b66c":"dayToUnderground = 0\ndataShift = data.copy()\ndataShift[DepthToGroundColumns] = dataShift[DepthToGroundColumns].shift(-dayToUnderground,freq='D')\ndataShift.tail()","0513e873":"dataSet = dataShift.dropna()\ndataSet","8c71d694":"dataSetExtract = dataSet.drop(DepthToGroundColumns,axis=1)\ndataSetExtract.head()","14c23cd5":"dataGround = dataSet[DepthToGroundColumns]\ndataGround\ndataModified = pd.concat([dataGround,dataSetExtract],axis=1)\ndataModified.iloc[:,:-5]","224bc722":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndataSetScaler = scaler.fit_transform(dataModified)\nprint(dataSetScaler.shape)","febc53e0":"from numpy import array","a7afd80f":"n_steps_in, n_steps_out = 120 , 20\ndef split_dataSet(data,n_steps_in,n_steps_out):\n    X,y = list(),list()\n    for i in range(len(dataSetScaled)):\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out-1\n        if out_end_ix > len(data):\n            break\n        data_x,data_y = data[i:end_ix, :],data[end_ix-1:out_end_ix,:]\n        X.append(data_x)\n        y.append(data_y)\n    return np.array(X), np.array(y)","81f87db6":"from sklearn.model_selection import train_test_split\nX,y = split_dataSet(dataSetScaled,n_steps_in,n_steps_out)\nprint(X.shape)\nprint(y.shape)\n\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n\nn_features = x_train.shape[2]\nx_train.shape","1a3c6616":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed","518c0ec2":"model = Sequential()\nmodel.add(LSTM(200, input_shape=(n_steps_in, n_features)))\nmodel.add(RepeatVector(n_steps_out))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(TimeDistributed(Dense(n_features)))\nmodel.compile(optimizer='adam', loss='mse')","439f5079":"model.fit(x_train, y_train, epochs=5,batch_size=1,validation_data=(x_test,y_test) ,verbose=1)\n\n","754a9a06":"predicData = model.predict(x_test, verbose=1)\npredictlist20days=[]\nfor i in range(len(predicData[0])):\n    predictlist20days.append(predicData[0][i][0])\ndays120list=[]\nfor i in range(len(x_test[0])):\n    days120list.append(x_test[0][i][0])\n\ndays120list20days=[]\nfor i in range(len(y_test[0])):\n    days120list20days.append(y_test[0][i][0])\npredictlist20days","701bc495":"aAxis=np.arange(0,len(days120list))\nbAxis=np.arange(len(days120list)-1,len(days120list)+19)\nplt.plot(aAxis,days120list)\nplt.plot(bAxis,days120list20days,color='cyan')\nplt.plot(bAxis,predictlist20days,color='red')","010a9eb5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport random\npd.set_option('display.max_columns',100)\npd.set_option('display.max_row',1000)","d8360064":"data = bilancino.copy()\ndata['Date']=pd.to_datetime(data['Date'],format ='%d\/%m\/%Y')\ndata.set_index(\"Date\",inplace=True)\ndata=data.dropna()\ndata","b3ef5e63":"DepthToGroundColumns=[]\nfor column in data.columns:\n    if 'Depth' in column:\n        DepthToGroundColumns.append(column)\nDepthToGroundColumns","a945a220":"dayToUnderground = 0\ndataShift = data.copy()\ndataShift[DepthToGroundColumns] = dataShift[DepthToGroundColumns].shift(-dayToUnderground,freq='D')\ndataShift.tail()","7263967e":"dataSet = dataShift.dropna()\ndataSet","41199af7":"dataSetExtract = dataSet.drop(DepthToGroundColumns,axis=1)\ndataSetExtract.head()","27623bfd":"dataGround = dataSet[DepthToGroundColumns]\ndataGround\ndataModified = pd.concat([dataGround,dataSetExtract],axis=1)\ndataModified.iloc[:,:-5]","58591255":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndataSetScaler = scaler.fit_transform(dataModified)\nprint(dataSetScaler.shape)","6dab934b":"dataSetScaled = dataSetScaler","f6562b8d":"from numpy import array","6df63692":"n_steps_in, n_steps_out = 120 , 20\ndef split_dataSet(data,n_steps_in,n_steps_out):\n    X,y = list(),list()\n    for i in range(len(dataSetScaled)):\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out-1\n        if out_end_ix > len(data):\n            break\n        data_x,data_y = data[i:end_ix, :],data[end_ix-1:out_end_ix,:]\n        X.append(data_x)\n        y.append(data_y)\n    return np.array(X), np.array(y)","447f7b45":"from sklearn.model_selection import train_test_split\nX,y = split_dataSet(dataSetScaled,n_steps_in,n_steps_out)\nprint(X.shape)\nprint(y.shape)\n\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n\nn_features = x_train.shape[2]\nx_train.shape","b7c65230":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed","238480fe":"model = Sequential()\nmodel.add(LSTM(200, input_shape=(n_steps_in, n_features)))\nmodel.add(RepeatVector(n_steps_out))\nmodel.add(LSTM(100, return_sequences=True))\nmodel.add(TimeDistributed(Dense(n_features)))\nmodel.compile(optimizer='adam', loss='mse')","4c660188":"model.fit(x_train, y_train, epochs=50,validation_data=(x_test,y_test) ,verbose=1)\n\n","ef7f434a":"predicData = model.predict(x_test, verbose=1)\npredictlist20days=[]\nfor i in range(len(predicData[0])):\n    predictlist20days.append(predicData[0][i][-1])\ndays120list=[]\nfor i in range(len(x_test[0])):\n    days120list.append(x_test[0][i][-1])\n\ndays120list20days=[]\nfor i in range(len(y_test[0])):\n    days120list20days.append(y_test[0][i][-1])\npredictlist20days","7cf3a8f8":"aAxis=np.arange(0,len(days120list))\nbAxis=np.arange(len(days120list)-1,len(days120list)+19)\nplt.plot(aAxis,days120list)\nplt.plot(bAxis,days120list20days,color='cyan')\nplt.plot(bAxis,predictlist20days,color='red')","7f58eb7f":"### WATER SPRING","5ad57e1a":"### AQUIFER","6e1b4507":"### RIVER","f052c980":"### LAKE","10a55b55":"It can be estimated that there are missing values in the number of counts and skewness. **Range of Rainfall** data is very wide.","babc9735":"##### Doganella","248ee2f1":"## Data on geographical view\nWith work of [EDA:Quenching the Thirst for Insights](https:\/\/www.kaggle.com\/iamleonie\/eda-quenching-the-thirst-for-insights), we will gather the coordinates for each location and than we could have an insight about relation amongs the locations.\n\nAnd there is relation ship between the `lake Bilancino` and the `river Arno`(see [Wikipedia: Lago di Bilancino](https:\/\/en.wikipedia.org\/wiki\/Lago_di_Bilancino), [Wikipedia: Sieve (river)]( https:\/\/en.wikipedia.org\/wiki\/Sieve_(river))) Because  Lake Bilancino is an artificial lake made with a dam on the Sieve river. And the Sieve River is the most important tributary of the Arno river.\n\nWater spring Lupa and Madonna have one location but river Arno(15),aquifer Auser(12), aquifer Luco(10) and the rest has the former. \n\nWe will use this location information for missing data et feature engineering.\n\n* Need to install module `folium` and `geopy`","da7bf949":"###### - Extract Rainfall values","91175240":"# REFERENCES\n\n* Groundwater Prediction\n  * Changhuia.P,Il-Moon.C(2020). Evaluating the groundwater prediction using LSTM model. J. Korea Water Resour. Assoc. Vol. 53, No. 4, pp. 273-283\n\n\n\n* EDA (Exploratory Data Analysis)\n  - https:\/\/www.kaggle.com\/luca31394\/acea-eda-and-data-cleaning-deep-water\/output#0.-Geographic-overview-of-the-sites\n  - https:\/\/www.kaggle.com\/iamleonie\/eda-quenching-the-thirst-for-insights\n\n* Times Series Analysis\n  - https:\/\/www.kaggle.com\/andreshg\/timeseries-analysis-a-complete-guide\n  - \n* LSTM(Long-short Term Memory)\n  - https:\/\/www.kaggle.com\/iwasnothing\/river-level-detection-using-lstm#\n  - https:\/\/dgkim5360.tistory.com\/entry\/understanding-long-short-term-memory-lstm-kr\n\n* VAR(Vector Autoregression Model) - https:\/\/www.machinelearningplus.com\/time-series\/vector-autoregression-examples-python\/","de29902d":"#### View Variables","6fc07b0d":"##### Luco","223d1a2b":"#### Plotting","3bbe7f5f":"##### Doganella","65e19823":"### WATER SPRING","b4ab35c2":"#### Bilancino\n Bilancino lake is an artificial lake located in the municipality of Barberino di Mugello (about 50 km from Florence). It is used to refill the Arno river during the summer months. Indeed, during the winter months, the lake is filled up and then, during the summer months, the water of the lake is poured into the Arno river.","1ee0753c":"## Prophet","0d414f71":"### ACQUIFER","88cffba2":"### WATER SPRING","c0db34fd":"##### Amiata","3d7265f8":"#### Histogram\nIn the preceding work, we were able to predict **skweness of distribution**. Histogram need to understand the data distibution of variables.\n\n**Process**\n\n1. divide dataframes into feuture variables and target variables\n2. do histogram each variables","8c274814":"#### Plotting","5880d891":"##### Lupa","a34c22fa":"### LAKE","18af3a65":"### LAKE","8f9b86d4":"##### Arno","d8ab86da":"##### Data Standardization","2e805e32":"###### Check Rainfall correlation\nIn the case of Lake Bilancino, we select `the Rainfall_S_Piero` which has a strong correlation with the other precipitation variables.","482924b8":"##### Lupa","9e03cc69":"##### Auser","ce6ae0ad":"##### Bilancino","54bdf180":"##### Doganella","1fd188b4":"###### Check Rainfall correlation","dd671af8":"## LSTM","eeff09a2":"##### Arno\n Arno is the second largest river in peninsular Italy and the main waterway in Tuscany and it has a relatively torrential regime, due to the nature of the surrounding soils (marl and impermeable clays). \n Arno is the main source of water supply of Florence-Prato-Pistoia.","90881267":"### AQUIFER","4479bae3":"##### Madonna di Canneto","a41b9197":"### WATER SPRING","51ab9ce3":"##### Bilancino\n\nBilancino lake is an artificial lake located in the municipality of Barberino di Mugello (about 50 km from Florence). It is used to refill the Arno river during the summer months. Indeed, during the winter months, the lake is filled up and then, during the summer months, the water of the lake is poured into the Arno river.\n\n- Lake Level(m)\n\n- Flow Rate : cubic meters per seconds (mc\/s)","9b106258":"# INTRODUCTION","6481c8f0":"## Abstract\n\n**Keyword** : **Time series**, **LSTM**, **Prediction water level**\n\nTo achieve this of Acea, we have used various time series analysis and machine learning techniques. In this work, we developed a prediction model based on LSTM (Long short term memory),and prophet, one of the artificial neural network (ANN) algorithms, for predicting each waterbody.\n\nHaving approched this subject, we have concerned about each characteristic et geological dataset. Our model was developed by Keras on python-based work.","a5aa15a4":"##### Bilancino","aa2be1bd":"## Methodology\nFirst, let's take a overview about each waterbody dataset doing Exploratory data analysis(including data description, histogram, plotting). And we will an insight which give us a good approach to modeling.\nAnd handling missing data, dataset will be conserved maximum data to model.\nNext, we will develope predictive models and tune these models.","d5e10c7a":"##### Luco","7a7ab860":"### RIVER","69f2cb06":"# EXPLORATORY DATA ANALYSIS","1943f6d2":"## FEATURE ENGINEERING","44fb7252":"### WATER SPRING","46190b53":"#### Histogram","45a0f492":"##### Data Standardization","4330bbb1":"## Data import\nImport each waterbody data set( `AQUIFER`, `WATER SPRING`, `RIVER`, `LAKE`) ","2198336e":"### AQUIFER","10ad5eef":"#### Plotting","b868cc68":"##### Amiata","dfe0cbf8":"##### Petrignano","72c3a92c":"### LAKE","995eb150":"#### View Variables","5081c0f0":"###### check the corrolation by Temperature","44d88ed6":"##### Amiata","555b5869":"##### Madonna di Canneto","6a772e6a":"### LAKE","1e97a365":"## Data Description","a9bc754f":"###### Check Rainfall correlation\nThere is two correlated groups of rainfall. with geograpical analysis, we have found why there are two rainfall groups. Vernio, Mangona, Cavallina, Le_Croci, S_Agata and S_Piero are to the north of Firenze, near to Lake Bilancino, these precipitations reach the River Arno through the River Sieve. The other points are located to the southeast of Firenze and fall directly into the Arno riverbed.\n\nWe chose Rainfall_S_Piero to represent precipitation in the north and Rainfall_Incisa for precipitation in the south.","eae2849e":"# DATA PROCESSING","fbbf1f89":"##### Petrignano","6033040b":"##### Amiata","c3f432af":"##### Auser","d9505bf8":"### WATER SPRING","ad88a9ad":"#### Histogram","76251179":"##### Amiata\n\nAmiata is reached through the Ermicciolo, Arbure, Bugnano and Galleria Alta springs. The levels and volumes of the four springs are influenced by the parameters: rainfall, depth to groundwater, temperature.","cde313cc":"##### Auser","caab58ec":"##### Doganella","3d44cbfd":"##### Luco","35889e60":"##### Petrignano","1a6b0d2b":"### RIVER","3904aabc":"### AQUIFER\nAquifer have 5 types of data (Rainfall, Depth_to_groundwater, temperature, volume, hydrometry) \n\n- Rainfall : quantity of rain falling(mm)\n- Depth_to_groundwater: from ground floor to the groundwater level(m)\n- Temperature : the temperature(\u00b0C)\n- Volume : volume of water taken by the drinkin water treatment plant (m\u00b3)\n- Hydrometry : the groundwater level (m)","0dfd7754":"### ACQUIFER","b595bf8a":"It can be sure that the corrolation among region in Rainfall section.\n- If it rains in one area, it is likely to rain in other area even if the amount can be different.","de41d9fc":"##### Lupa","5dac5962":"##### Arno","5004c551":"##### Luco","28005026":"### LAKE","f129de91":"## Module import","29da0221":"# PREDICTIVE MODEL","e19b3f37":"##### Petrignano","e16f820a":"#### View Variables","54fa9f47":"##### Arno","0f3f2551":"##### Auser","ebbcbc73":"###### conclusion\n   - Each region's climate is correlated except one.\n   - The rainfall in each region shows a correlation except one.\n   - The value of each Depth_to_underground is correlated except one.\n   - Then climate is similar in all but one area which mean that we can assume 3 of them are in similar region.\n\n","0a3f999f":"Dataset have similar data type. ","c3d72830":"#### View Variables","f7fbf161":"##### Arno","4c19208c":"##### Madonna di Canneto","a5160986":"##### Data Standardization","6951e5a5":"##### DepthToGround\n    - It takes time for rainfall to seep into the ground and reach the groundwater.\n    - I set the day as 3days","ad1a0700":"##### Bilancino","665313db":"## HANDLING MISSING DATA","de8e88c9":"##### Auser\n\nAquifer Auser waterbody consists of two subsystems (NORH, SOUTH), where the former partly influences the behaviour of the latter. The levels of the NORTH sector are represented by the values of the SAL, PAG, CoS and DIEC wells, while the levels of the SOUTH sector by the LT2 well.","b5814d58":"###### check the corrolation by among the Depth_to_ground","b062fd9c":"### RIVER","03cb4352":"### RIVER","d6decd7a":"### RIVER","9f7f973f":"##### Madonna di Canneto","ab6c1667":"##### Lupa","1b855da8":"#### Plotting","3cbe4beb":"#### Data Standardization","9399fdbb":"It can be estimated that there are missing values in the number of counts and skewness. `Rainfall` data is very wide. `Temperature` data follows narmal distribution. ","8059f4b4":"#### Histogram"}}