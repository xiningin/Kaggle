{"cell_type":{"2f6d6e37":"code","176b0c34":"code","73037a6c":"code","f9d1136e":"code","3698290f":"code","4741dbcc":"code","26550395":"code","383f0b0f":"code","7db68641":"code","df6731cd":"code","b49cd554":"code","e1e54c12":"code","f3581706":"code","ce20cd1d":"code","ef7d975d":"code","3766838e":"code","a91d038a":"code","76cfd6a8":"code","e39a1512":"code","90bacd0b":"code","62d69ba9":"code","9b25ef0e":"code","c145f846":"code","91f8a72f":"code","a23164bb":"code","8949ddf6":"code","b76b61d3":"code","c88d5f87":"code","68aa1f90":"code","1459c383":"code","4f831dcb":"code","784b1279":"code","2ae19a98":"code","77b341d1":"code","3efc1a7c":"code","48989c42":"code","988dbbd7":"code","d274dfcf":"code","2f556d6f":"code","8c004270":"code","f9ad0cef":"code","483d40fc":"code","f4b18d75":"code","9b43d94f":"code","0bff8f06":"code","61fbf832":"code","9f4621c1":"code","2b4a7c2c":"code","78447fef":"code","1165ad5d":"code","4682d4d7":"code","5a4fbf4b":"code","c9e09407":"code","a3804f53":"code","db298903":"code","ada3fd39":"code","5685cec9":"code","c8bc51cc":"code","3df99c2f":"code","ec31af80":"code","6489d80b":"code","d0f86d6e":"code","9231d12b":"code","edaf8e2e":"code","7e362cf1":"code","c99b61b8":"code","4d3bb46d":"code","2eda4fb7":"code","a6b5cf77":"code","1502ec3a":"code","a9fbebb3":"code","4a7746a1":"code","f0a1331c":"code","e04e1100":"code","14a313db":"code","845b4e64":"code","6b80061a":"code","7e178330":"code","a7222ba8":"code","591e8c02":"code","27f67d63":"code","9d45564d":"code","23002533":"code","187b6c8e":"code","979b1f0f":"code","cb7cf07f":"code","979e4608":"code","dc4ccf5e":"code","815d4e5d":"code","9949b02f":"code","ef0cde4b":"code","612f762c":"markdown","563bc445":"markdown","d451c2b6":"markdown","af0291fb":"markdown","642eb477":"markdown","852e6ad3":"markdown","e68587bd":"markdown","3ee5f32a":"markdown","fe5f8055":"markdown","c72dbcc5":"markdown","20a606fd":"markdown","dc521211":"markdown","982d278d":"markdown","f2cd595f":"markdown","1c4cb61b":"markdown","1e77d209":"markdown","378978e5":"markdown","7ab312c0":"markdown","25fee63c":"markdown","1da48cbe":"markdown","ffc6b065":"markdown","22cf7825":"markdown","bcc4e3cf":"markdown","664eb8da":"markdown","baa3f096":"markdown","b7a851fc":"markdown","b1764c19":"markdown","a9bcb5f6":"markdown","614918fb":"markdown","8dae8dfb":"markdown","6ccb1628":"markdown","3518ae5d":"markdown","ac805580":"markdown","1d459cc1":"markdown","df5932f8":"markdown","b51dbbf1":"markdown","742a1ad1":"markdown","252a7084":"markdown","21bb3826":"markdown","1ea41691":"markdown","6baef13c":"markdown","1a69b908":"markdown","f4fafad7":"markdown","53ed3d36":"markdown","c9f082aa":"markdown","c64a8bbe":"markdown"},"source":{"2f6d6e37":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.gridspec as gridspec\nfrom datetime import datetime\nfrom scipy.stats import skew  # for some statistics\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport sklearn.linear_model as linear_model\nimport matplotlib.style as style\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport missingno as msno\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","176b0c34":"## Import Trainning data. \ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntrain.head()","73037a6c":"## Import test data.\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest.head()","f9d1136e":"print (f\"Train has {train.shape[0]} rows and {train.shape[1]} columns\")\nprint (f\"Test has {test.shape[0]} rows and {test.shape[1]} columns\")","3698290f":"# gives us statistical info about the numerical variables. \ntrain.describe().T","4741dbcc":"## Gives us information about the features. \ntrain.info()","26550395":"msno.matrix(train);","383f0b0f":"def missing_percentage(df):\n    \"\"\"This function takes a DataFrame(df) as input and returns two columns, total missing values and total missing values percentage\"\"\"\n    ## the two following line may seem complicated but its actually very simple. \n    total = df.isnull().sum().sort_values(ascending = False)[df.isnull().sum().sort_values(ascending = False) != 0]\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)[round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2) != 0]\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])\n\nmissing_percentage(train)","7db68641":"msno.matrix(test);","df6731cd":"missing_percentage(test)","b49cd554":"def plotting_3_chart(df, feature):\n    ## Importing seaborn, matplotlab and scipy modules. \n    import seaborn as sns\n    import matplotlib.pyplot as plt\n    import matplotlib.gridspec as gridspec\n    from scipy import stats\n    import matplotlib.style as style\n    style.use('fivethirtyeight')\n\n    ## Creating a customized chart. and giving in figsize and everything. \n    fig = plt.figure(constrained_layout=True, figsize=(12,8))\n    ## creating a grid of 3 cols and 3 rows. \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n    #gs = fig3.add_gridspec(3, 3)\n\n    ## Customizing the histogram grid. \n    ax1 = fig.add_subplot(grid[0, :2])\n    ## Set the title. \n    ax1.set_title('Histogram')\n    ## plot the histogram. \n    sns.distplot(df.loc[:,feature], norm_hist=True, ax = ax1)\n\n    # customizing the QQ_plot. \n    ax2 = fig.add_subplot(grid[1, :2])\n    ## Set the title. \n    ax2.set_title('QQ_plot')\n    ## Plotting the QQ_Plot. \n    stats.probplot(df.loc[:,feature], plot = ax2)\n\n    ## Customizing the Box Plot. \n    ax3 = fig.add_subplot(grid[:, 2])\n    ## Set title. \n    ax3.set_title('Box Plot')\n    ## Plotting the box plot. \n    sns.boxplot(df.loc[:,feature], orient='v', ax = ax3 );\n    \nplotting_3_chart(train, 'SalePrice')","e1e54c12":"#skewness and kurtosis\nprint(\"Skewness: \" + str(train['SalePrice'].skew()))\nprint(\"Kurtosis: \" + str(train['SalePrice'].kurt()))","f3581706":"## Getting the correlation of all the features with target variable. \n(train.corr()**2)[\"SalePrice\"].sort_values(ascending = False)[1:]","ce20cd1d":"def customized_scatterplot(y, x):\n        ## Sizing the plot. \n    style.use('fivethirtyeight')\n    plt.subplots(figsize = (12,8))\n    ## Plotting target variable with predictor variable(OverallQual)\n    sns.scatterplot(y = y, x = x);","ef7d975d":"customized_scatterplot(train.SalePrice, train.OverallQual)","3766838e":"customized_scatterplot(train.SalePrice, train.GrLivArea)","a91d038a":"customized_scatterplot(train.SalePrice, train.GarageArea);","76cfd6a8":"customized_scatterplot(train.SalePrice, train.TotalBsmtSF)","e39a1512":"customized_scatterplot(train.SalePrice, train['1stFlrSF']);","90bacd0b":"customized_scatterplot(train.SalePrice, train.MasVnrArea);","62d69ba9":"\n## Deleting those two values with outliers. \ntrain = train[train.GrLivArea < 4500]\ntrain.reset_index(drop = True, inplace = True)\n\n## save a copy of this dataset so that any changes later on can be compared side by side.\nprevious_train = train.copy()","9b25ef0e":"## Plot sizing. \nfig, (ax1, ax2) = plt.subplots(figsize = (12,8), ncols=2,sharey=False)\n## Scatter plotting for SalePrice and GrLivArea. \nsns.scatterplot( x = train.GrLivArea, y = train.SalePrice,  ax=ax1)\n## Putting a regression line. \nsns.regplot(x=train.GrLivArea, y=train.SalePrice, ax=ax1)\n\n## Scatter plotting for SalePrice and MasVnrArea. \nsns.scatterplot(x = train.MasVnrArea,y = train.SalePrice, ax=ax2)\n## regression line for MasVnrArea and SalePrice. \nsns.regplot(x=train.MasVnrArea, y=train.SalePrice, ax=ax2);","c145f846":"plt.subplots(figsize = (12,8))\nsns.residplot(train.GrLivArea, train.SalePrice);","91f8a72f":"plotting_3_chart(train, 'SalePrice')","a23164bb":"## trainsforming target variable using numpy.log1p, \ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n## Plotting the newly transformed response variable\nplotting_3_chart(train, 'SalePrice')","8949ddf6":"## Customizing grid for two plots. \nfig, (ax1, ax2) = plt.subplots(figsize = (15,6), \n                               ncols=2, \n                               sharey = False, \n                               sharex=False\n                              )\n## doing the first scatter plot. \nsns.residplot(x = previous_train.GrLivArea, y = previous_train.SalePrice, ax = ax1)\n## doing the scatter plot for GrLivArea and SalePrice. \nsns.residplot(x = train.GrLivArea, y = train.SalePrice, ax = ax2);","b76b61d3":"## Plot fig sizing. \nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (30,20))\n## Plotting heatmap. \n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(train.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(train.corr(), \n            cmap=sns.diverging_palette(20, 220, n=200), \n            mask = mask, \n            annot=True, \n            center = 0, \n           );\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 30);","c88d5f87":"## Dropping the \"Id\" from train and test set. \n# train.drop(columns=['Id'],axis=1, inplace=True)\n\ntrain.drop(columns=['Id'],axis=1, inplace=True)\ntest.drop(columns=['Id'],axis=1, inplace=True)\n\n## Saving the target values in \"y_train\". \ny = train['SalePrice'].reset_index(drop=True)\n\n\n\n# getting a copy of train\nprevious_train = train.copy()","68aa1f90":"## Combining train and test datasets together so that we can do all the work at once. \nall_data = pd.concat((train, test)).reset_index(drop = True)\n## Dropping the target variable. \nall_data.drop(['SalePrice'], axis = 1, inplace = True)","1459c383":"missing_percentage(all_data)","4f831dcb":"## Some missing values are intentionally left blank, for example: In the Alley feature \n## there are blank values meaning that there are no alley's in that specific house. \nmissing_val_col = [\"Alley\", \n                   \"PoolQC\", \n                   \"MiscFeature\",\n                   \"Fence\",\n                   \"FireplaceQu\",\n                   \"GarageType\",\n                   \"GarageFinish\",\n                   \"GarageQual\",\n                   \"GarageCond\",\n                   'BsmtQual',\n                   'BsmtCond',\n                   'BsmtExposure',\n                   'BsmtFinType1',\n                   'BsmtFinType2',\n                   'MasVnrType']\n\nfor i in missing_val_col:\n    all_data[i] = all_data[i].fillna('None')","784b1279":"## In the following features the null values are there for a purpose, so we replace them with \"0\"\nmissing_val_col2 = ['BsmtFinSF1',\n                    'BsmtFinSF2',\n                    'BsmtUnfSF',\n                    'TotalBsmtSF',\n                    'BsmtFullBath', \n                    'BsmtHalfBath', \n                    'GarageYrBlt',\n                    'GarageArea',\n                    'GarageCars',\n                    'MasVnrArea']\n\nfor i in missing_val_col2:\n    all_data[i] = all_data[i].fillna(0)\n    \n## Replaced all missing values in LotFrontage by imputing the median value of each neighborhood. \nall_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].transform( lambda x: x.fillna(x.mean()))","2ae19a98":"## the \"OverallCond\" and \"OverallQual\" of the house. \n# all_data['OverallCond'] = all_data['OverallCond'].astype(str) \n# all_data['OverallQual'] = all_data['OverallQual'].astype(str)\n\n## Zoning class are given in numerical; therefore converted to categorical variables. \nall_data['MSSubClass'] = all_data['MSSubClass'].astype(str)\nall_data['MSZoning'] = all_data.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\n## Important years and months that should be categorical variables not numerical. \n# all_data['YearBuilt'] = all_data['YearBuilt'].astype(str)\n# all_data['YearRemodAdd'] = all_data['YearRemodAdd'].astype(str)\n# all_data['GarageYrBlt'] = all_data['GarageYrBlt'].astype(str)\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str) ","77b341d1":"all_data['Functional'] = all_data['Functional'].fillna('Typ') \nall_data['Utilities'] = all_data['Utilities'].fillna('AllPub') \nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0]) \nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(\"TA\") \nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['Electrical'] = all_data['Electrical'].fillna(\"SBrkr\") \n","3efc1a7c":"missing_percentage(all_data)","48989c42":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nskewed_feats","988dbbd7":"sns.distplot(all_data['1stFlrSF']);","d274dfcf":"## Fixing Skewed features using boxcox transformation. \n\n\ndef fixing_skewness(df):\n    \"\"\"\n    This function takes in a dataframe and return fixed skewed dataframe\n    \"\"\"\n    ## Import necessary modules \n    from scipy.stats import skew\n    from scipy.special import boxcox1p\n    from scipy.stats import boxcox_normmax\n    \n    ## Getting all the data that are not of \"object\" type. \n    numeric_feats = df.dtypes[df.dtypes != \"object\"].index\n\n    # Check the skew of all numerical features\n    skewed_feats = df[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n    high_skew = skewed_feats[abs(skewed_feats) > 0.5]\n    skewed_features = high_skew.index\n\n    for feat in skewed_features:\n        df[feat] = boxcox1p(df[feat], boxcox_normmax(df[feat] + 1))\n\nfixing_skewness(all_data)","2f556d6f":"sns.distplot(all_data['1stFlrSF']);","8c004270":"# feture engineering a new feature \"TotalFS\"\nall_data['TotalSF'] = (all_data['TotalBsmtSF'] \n                       + all_data['1stFlrSF'] \n                       + all_data['2ndFlrSF'])\n\nall_data['YrBltAndRemod'] = all_data['YearBuilt'] + all_data['YearRemodAdd']\n\nall_data['Total_sqr_footage'] = (all_data['BsmtFinSF1'] \n                                 + all_data['BsmtFinSF2'] \n                                 + all_data['1stFlrSF'] \n                                 + all_data['2ndFlrSF']\n                                )\n                                 \n\nall_data['Total_Bathrooms'] = (all_data['FullBath'] \n                               + (0.5 * all_data['HalfBath']) \n                               + all_data['BsmtFullBath'] \n                               + (0.5 * all_data['BsmtHalfBath'])\n                              )\n                               \n\nall_data['Total_porch_sf'] = (all_data['OpenPorchSF'] \n                              + all_data['3SsnPorch'] \n                              + all_data['EnclosedPorch'] \n                              + all_data['ScreenPorch'] \n                              + all_data['WoodDeckSF']\n                             )\n                              \n                              \n","f9ad0cef":"all_data['haspool'] = all_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['has2ndfloor'] = all_data['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasgarage'] = all_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasbsmt'] = all_data['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\nall_data['hasfireplace'] = all_data['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","483d40fc":"all_data.shape","f4b18d75":"all_data = all_data.drop(['Utilities', 'Street', 'PoolQC',], axis=1)","9b43d94f":"## Creating dummy variable \nfinal_features = pd.get_dummies(all_data).reset_index(drop=True)\nfinal_features.shape","0bff8f06":"X = final_features.iloc[:len(y), :]\n\nX_sub = final_features.iloc[len(y):, :]","61fbf832":"outliers = [30, 88, 462, 631, 1322]\nX = X.drop(X.index[outliers])\ny = y.drop(y.index[outliers])","9f4621c1":"counts = X.BsmtUnfSF.value_counts()","2b4a7c2c":"counts.iloc[0]","78447fef":"for i in X.columns:\n    counts = X[i].value_counts()\n    print (counts)","1165ad5d":"def overfit_reducer(df):\n    \"\"\"\n    This function takes in a dataframe and returns a list of features that are overfitted.\n    \"\"\"\n    overfit = []\n    for i in df.columns:\n        counts = df[i].value_counts()\n        zeros = counts.iloc[0]\n        if zeros \/ len(df) * 100 > 99.94:\n            overfit.append(i)\n    overfit = list(overfit)\n    return overfit\n\n\noverfitted_features = overfit_reducer(X)\n\nX = X.drop(overfitted_features, axis=1)\nX_sub = X_sub.drop(overfitted_features, axis=1)","4682d4d7":"X.shape,y.shape, X_sub.shape","5a4fbf4b":"## Train test split\nfrom sklearn.model_selection import train_test_split\n## Train test split follows this distinguished code pattern and helps creating train and test set to build machine learning. \nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .33, random_state = 0)","c9e09407":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","a3804f53":"sample_train = previous_train.sample(300)\nimport seaborn as sns\nplt.subplots(figsize = (15,8))\nax = plt.gca()\nax.scatter(sample_train.GrLivArea.values, sample_train.SalePrice.values, color ='b');\nplt.title(\"Chart with Data Points\");\n#ax = sns.regplot(sample_train.GrLivArea.values, sample_train.SalePrice.values)\n#ax.plot((sample_train.GrLivArea.values.min(),sample_train.GrLivArea.values.max()), (sample_train.SalePrice.values.mean(),sample_train.SalePrice.values.mean()), color = 'r');","db298903":"plt.subplots(figsize = (15,8))\nax = plt.gca()\nax.scatter(sample_train.GrLivArea.values, sample_train.SalePrice.values, color ='b');\n#ax = sns.regplot(sample_train.GrLivArea.values, sample_train.SalePrice.values)\nax.plot((sample_train.GrLivArea.values.min(),sample_train.GrLivArea.values.max()), (sample_train.SalePrice.values.mean(),sample_train.SalePrice.values.mean()), color = 'r');\nplt.title(\"Chart with Average Line\");","ada3fd39":"## Calculating Mean Squared Error(MSE)\nsample_train['mean_sale_price'] = sample_train.SalePrice.mean()\nsample_train['mse'] = np.square(sample_train.mean_sale_price - sample_train.SalePrice)\nsample_train.mse.mean()\n## getting mse\nprint(\"Mean Squared Error(MSE) for average line is : {}\".format(sample_train.mse.mean()))","5685cec9":"## Calculating the beta coefficients by hand. \n## mean of y. \ny_bar = sample_train.SalePrice.mean()\n## mean of x. \nx_bar = sample_train.GrLivArea.mean()\n## Std of y\nstd_y = sample_train.SalePrice.std()\n## std of x\nstd_x = sample_train.GrLivArea.std()\n## correlation of x and y\nr_xy = sample_train.corr().loc['GrLivArea','SalePrice']\n## finding beta_1\nbeta_1 = r_xy*(std_y\/std_x)\n## finding beta_0\nbeta_0 = y_bar - beta_1*x_bar","c8bc51cc":"## getting y_hat, which is the predicted y values. \nsample_train['Linear_Yhat'] = beta_0 + beta_1*sample_train['GrLivArea']","3df99c2f":"# create a figure\nfig = plt.figure(figsize=(15,7))\n# get the axis of that figure\nax = plt.gca()\n\n# plot a scatter plot on it with our data\nax.scatter(sample_train.GrLivArea, sample_train.SalePrice, c='b')\nax.plot(sample_train['GrLivArea'], sample_train['Linear_Yhat'], color='r');","ec31af80":"## getting mse\nprint(\"Mean Squared Error(MSE) for regression line is : {}\".format(np.square(sample_train['SalePrice'] - sample_train['Linear_Yhat']).mean()))","6489d80b":"from sklearn.metrics import mean_squared_error\nmean_squared_error(sample_train['SalePrice'], sample_train.Linear_Yhat)","d0f86d6e":"## Creating a customized chart. and giving in figsize and everything. \nfig = plt.figure(constrained_layout=True, figsize=(15,5))\n## creating a grid of 3 cols and 3 rows. \ngrid = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)\n#gs = fig3.add_gridspec(3, 3)\n#ax1 = fig.add_subplot(grid[row, column])\nax1 = fig.add_subplot(grid[0, :1])\n\n# get the axis\nax1 = fig.gca()\n\n# plot it\nax1.scatter(x=sample_train['GrLivArea'], y=sample_train['SalePrice'], c='b')\nax1.plot(sample_train['GrLivArea'], sample_train['mean_sale_price'], color='k');\n\n# iterate over predictions\nfor _, row in sample_train.iterrows():\n    plt.plot((row['GrLivArea'], row['GrLivArea']), (row['SalePrice'], row['mean_sale_price']), 'r-')\n    \nax2 = fig.add_subplot(grid[0, 1:])\n\n# plot it\nax2.scatter(x=sample_train['GrLivArea'], y=sample_train['SalePrice'], c='b')\nax2.plot(sample_train['GrLivArea'], sample_train['Linear_Yhat'], color='k');\n# iterate over predictions\nfor _, row in sample_train.iterrows():\n    plt.plot((row['GrLivArea'], row['GrLivArea']), (row['SalePrice'], row['Linear_Yhat']), 'r-')","9231d12b":"## importing necessary models.\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n## Call in the LinearRegression object\nlin_reg = LinearRegression(normalize=True, n_jobs=-1)\n## fit train and test data. \nlin_reg.fit(X_train, y_train)\n## Predict test data. \ny_pred = lin_reg.predict(X_test)","edaf8e2e":"## get average squared error(MSE) by comparing predicted values with real values. \nprint ('%.2f'%mean_squared_error(y_test, y_pred))","7e362cf1":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score, KFold\nlin_reg = LinearRegression()\ncv = KFold(shuffle=True, random_state=2, n_splits=10)\nscores = cross_val_score(lin_reg, X,y,cv = cv, scoring = 'neg_mean_absolute_error')","c99b61b8":"print ('%.8f'%scores.mean())","4d3bb46d":"## Importing Ridge. \nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n## Assiging different sets of alpha values to explore which can be the best fit for the model. \nalpha_ridge = [-3,-2,-1,1e-15, 1e-10, 1e-8,1e-5,1e-4, 1e-3,1e-2,0.5,1,1.5, 2,3,4, 5, 10, 20, 30, 40]\ntemp_rss = {}\ntemp_mse = {}\nfor i in alpha_ridge:\n    ## Assigin each model. \n    ridge = Ridge(alpha= i, normalize=True)\n    ## fit the model. \n    ridge.fit(X_train, y_train)\n    ## Predicting the target value based on \"Test_x\"\n    y_pred = ridge.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    rss = sum((y_pred-y_test)**2)\n    temp_mse[i] = mse\n    temp_rss[i] = rss","2eda4fb7":"for key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","a6b5cf77":"for key, value in sorted(temp_rss.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","1502ec3a":"from sklearn.linear_model import Lasso \ntemp_rss = {}\ntemp_mse = {}\nfor i in alpha_ridge:\n    ## Assigin each model. \n    lasso_reg = Lasso(alpha= i, normalize=True)\n    ## fit the model. \n    lasso_reg.fit(X_train, y_train)\n    ## Predicting the target value based on \"Test_x\"\n    y_pred = lasso_reg.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    rss = sum((y_pred-y_test)**2)\n    temp_mse[i] = mse\n    temp_rss[i] = rss","a9fbebb3":"for key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","4a7746a1":"for key, value in sorted(temp_rss.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","f0a1331c":"from sklearn.linear_model import ElasticNet\ntemp_rss = {}\ntemp_mse = {}\nfor i in alpha_ridge:\n    ## Assigin each model. \n    lasso_reg = ElasticNet(alpha= i, normalize=True)\n    ## fit the model. \n    lasso_reg.fit(X_train, y_train)\n    ## Predicting the target value based on \"Test_x\"\n    y_pred = lasso_reg.predict(X_test)\n\n    mse = mean_squared_error(y_test, y_pred)\n    rss = sum((y_pred-y_test)**2)\n    temp_mse[i] = mse\n    temp_rss[i] = rss","e04e1100":"for key, value in sorted(temp_mse.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","14a313db":"for key, value in sorted(temp_rss.items(), key=lambda item: item[1]):\n    print(\"%s: %s\" % (key, value))","845b4e64":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","6b80061a":"alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","7e178330":"ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, \n                                              alphas=alphas2, \n                                              random_state=42, \n                                              cv=kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","a7222ba8":"gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)                             ","591e8c02":"lightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )","27f67d63":"xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","9d45564d":"stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","23002533":"# score = cv_rmse(stack_gen)\n# print(\"Stack: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )","187b6c8e":"score = cv_rmse(ridge)\nprint(\"Ridge: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lasso)\nprint(\"LASSO: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(elasticnet)\nprint(\"elastic net: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(svr)\nprint(\"SVR: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\n# score = cv_rmse(gbr)\n# print(\"gbr: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n\nscore = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )","979b1f0f":"print('START Fit')\n\nprint('stack_gen')\nstack_gen_model = stack_gen.fit(np.array(X), np.array(y))\n\nprint('elasticnet')\nelastic_model_full_data = elasticnet.fit(X, y)\n\nprint('Lasso')\nlasso_model_full_data = lasso.fit(X, y)\n\nprint('Ridge') \nridge_model_full_data = ridge.fit(X, y)\n\nprint('Svr')\nsvr_model_full_data = svr.fit(X, y)\n\n# print('GradientBoosting')\n# gbr_model_full_data = gbr.fit(X, y)\n\nprint('xgboost')\nxgb_model_full_data = xgboost.fit(X, y)\n\nprint('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)","cb7cf07f":"1.0 * elastic_model_full_data.predict(X)","979e4608":"def blend_models_predict(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) + \\\n            (0.05 * lasso_model_full_data.predict(X)) + \\\n            (0.2 * ridge_model_full_data.predict(X)) + \\\n            (0.1 * svr_model_full_data.predict(X)) + \\\n#             (0.1 * gbr_model_full_data.predict(X)) + \\\n            (0.15 * xgb_model_full_data.predict(X)) + \\\n            (0.1 * lgb_model_full_data.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))","dc4ccf5e":"print('RMSLE score on train data:')\nprint(rmsle(y, blend_models_predict(X)))","815d4e5d":"print('Predict submission')\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission.iloc[:,1] = np.floor(np.expm1(blend_models_predict(X_sub)))","9949b02f":"print('Blend with Top Kernels submissions\\n')\nsub_1 = pd.read_csv('..\/input\/top-house-price-kernel-predictions\/blending_high_scores_top_1_8th_place.csv')\nsub_2 = pd.read_csv('..\/input\/top-house-price-kernel-predictions\/house_prices_ensemble_7models.csv')\nsub_3 = pd.read_csv('..\/input\/top-house-price-kernel-predictions\/blend_and_stack_LR.csv')\nsubmission.iloc[:,1] = np.floor((0.25 * np.floor(np.expm1(blend_models_predict(X_sub)))) + \n                                (0.25 * sub_1.iloc[:,1]) + \n                                (0.25 * sub_2.iloc[:,1]) + \n                                (0.25 * sub_3.iloc[:,1]))","ef0cde4b":"q1 = submission['SalePrice'].quantile(0.005)\nq2 = submission['SalePrice'].quantile(0.995)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\nsubmission.to_csv(\"submission.csv\", index=False)","612f762c":"> **Sample Test Dataset**","563bc445":"Here, we see that the pre-transformed chart on the left has heteroscedasticity, and the post-transformed chart on the right has Homoscedasticity(almost an equal amount of variance across the zero lines). It looks like a blob of data points and doesn't seem to give away any relationships. That's the sort of relationship we would like to see to avoid some of these assumptions. \n\n**No or Little multicollinearity:** \nMulticollinearity is when there is a strong correlation between independent variables. Linear regression or multilinear regression requires independent variables to have little or no similar features. Multicollinearity can lead to a variety of problems, including:\n* The effect of predictor variables estimated by our regression will depend on what other variables are included in our model. \n* Predictors can have wildly different results depending on the observations in our sample, and small changes in samples can result in very different estimated effects. \n* With very high multicollinearity, the inverse matrix, the computer calculates may not be accurate. \n* We can no longer interpret a coefficient on a variable as the effect on the target of a one-unit increase in that variable holding the other variables constant. The reason behind that is, when predictors are strongly correlated, there is not a scenario in which one variable can change without a conditional change in another variable.\n\nHeatmap is an excellent way to identify whether there is multicollinearity or not. The best way to solve multicollinearity is to use regularization methods like Ridge or Lasso.","d451c2b6":"\n\nThe evaluation metrics often named in such a way that I find it confusing to remember. So, this is a guide for me and everyone else who is reading it. There are many evaluation metrics. Let's name a few of them. \n\nIt may seem confusing with multiple similar abbreviations, but once we focus on what they each do, things will become much more intuitive. For now, I am going to dive right into the $R^2$.\n\n# $R^2$(The \"Coefficient of determination\"): \n> $R^2$ describes the proportion of variance of the dependent variable explained by the regression model. Let's write the equation for $R^2$. \n\n# $$ \\operatorname{R^2} = \\frac{SSR}{SST} $$\n\nHere,\n\n* SST(Sum of the Total Squared Error) is the total residual. It is also known as TSS(Total Sum of the Squared Error)\n* SSR(Sum of the Squared Regression) is the residual explained by the regression line. SSR is also known as ESS(Explained Sum of the Squared Error)\n\nand\n\n* SSE(Sum of the Squared Error)\/RSS(Residual Sum of the Squared Error)\nLet's break these down. \n\n## SST\/TSS:\nSST is the sum of the squared distance from all points to average line ( $\\bar{y}$ ). We call this the **total variation** in the Y's of the **Total Sum of the Squares(SST).** Let's see it in the function. \n### $$ \\operatorname{SST} = \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2 $$\n\nHere\n* $y_i$ = Each observed data point. \n* $\\bar{y}$ = Mean of y value.\n* $\\hat{y_i}$ = Predicted data point for each $x_i$ depending on i. \n\nA visualization would make things much more clear.\n![](http:\/\/blog.hackerearth.com\/wp-content\/uploads\/2016\/12\/anat.png)\n \nIn this visualization above, the light green line is the <font color=\"green\"><b>average line<\/b><\/font> and the black dot is the observed value. So, SST describes the distance between the black dot and the <font color=\"green\"><b>average line<\/b><\/font>.\n\n\n## SSR\/ESS:\nSSR is the sum of the squared residual between each predicted value and the average line. In statistics language we say that, SSR is the squared residual explained by the regression line. In the visualization above SSR is the distance from <font color='green'><b>baseline model<\/b><\/font> to the <font color = 'blue'><b>regression line.<\/b><\/font> \n### $$ SSR = \\sum_{i=1}^n \\left(\\hat{y_i} - \\bar{y}\\right)^2 $$\n\n## SSE\/RSS: \nRSS is calculated by squaring each residual of the data points and then adding them together. This residual is the difference between the predicted line and the observed value. In statistics language, we say, SSE is the squared residual that was not explained by the regression line, and this is the quantity least-square minimizes. In the chart above SSE is the distance of the actual data point from the <font color = 'blue'><b>regression line<\/b><\/font>. \n\n### $$ SSE = \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2 $$\n\nAnd the relation between all three of these metrics is\n## $$SST = SSR + SSE$$\n\n\nFrom the equation above and the $R^2$ equation from the top we can modify the $R^2$ equation as the following\n# $$ R^2 = 1 - \\frac{SSE}{SST} $$\n\n## More on $R^2$: \n* $R^2$ is matric with a value between 0 and 1. \n* If the points are perfectly linear, then error sum of squares is 0, In that case, SSR = SST. Which means the variation in the Y's is completely explained by the regression line causing the value of $R^2$ to be close to 1. \n* In other extreme cases, when there is no relation between x and y, hence SSR = 0 and therefore SSE = SST, The regression line explains none of the variances in Y causing $R^2$ to be close to 0.\n* $R^2$ measures the explanatory power of the model; The more of the variance in the dependent variable(Y) the model can explain, the more powerful it is.\n* $R^2$ can be infinitely negative as well. Having a negative indicates that the predictive equation has a greater error than the baseline model.\n* The value of $R^2$ increases as more feature gets added despite the effectiveness of those features in the model.\n* This is a problem, since we may think that having a greater $R^2$ means a better model, even though the model didnot actually improved. In order to get around this we use Adjusted R-Squared($R^2_{adj}$)\n\n**Adjusted R-Squared($R^2_{adj}$)**: \n\n$R^2_{adj}$ is similar to $R^2$. However, the value of$R^2_{adj}$ decreases if we use a feature that doesn't improve the model significantly. Let's write the equation for $R^2_{adj}$. \n\n## $$ {R^2_{adj}} = 1 - [\\frac{(1 - R^2)(n-1)}{(n-k-1)}]$$\n\nhere, \n* n = # of datapoints. \n* k = # of feature used. \n\nAs you can see from the equation, the increase of k(feature) in the denumerator penilizes the adjusted $R^2$ value if there is not a significant improvement of $R^2$ in the numerator.  ","af0291fb":"## Creating New Features","642eb477":"Phew!! This looks like something we can work with!! Let's find out the MSE for the regression line as well.","852e6ad3":"You can tell this is not the most efficient way to estimate the price of houses. The average line clearly does not represent all the datapoint and fails to grasp the linear relationship between <b>GrLivArea & SalePrice. <\/b> Let use one of the evaluation regression metrics and find out the Mean Squared Error(more on this later) of this line.","e68587bd":"### SalePrice vs GrLivArea","3ee5f32a":"Ideally, if the assumptions are met, the residuals will be randomly scattered around the centerline of zero with no apparent pattern. The residual will look like an unstructured cloud of points centered around zero. However, our residual plot is anything but an unstructured cloud of points. Even though it seems like there is a linear relationship between the response variable and predictor variable, the residual plot looks more like a funnel. The error plot shows that as **GrLivArea** value increases, the variance also increases, which is the characteristics known as **Heteroscedasticity**. Let's break this down. \n\n**Homoscedasticity ( Constant Variance ):** \nThe assumption of Homoscedasticity is crucial to linear regression models. Homoscedasticity describes a situation in which the error term or variance or the \"noise\" or random disturbance in the relationship between the independent variables and the dependent variable is the same across all values of the independent variable. In other words, there is a constant variance present in the response variable as the predictor variable increases. If the \"noise\" is not the same across the values of an independent variable like the residual plot above, we call that **Heteroscedasticity**. As you can tell, it is the opposite of **Homoscedasticity.**\n\n<p><img src=\"https:\/\/www.dummies.com\/wp-content\/uploads\/415147.image1.jpg\" style=\"float:center\"><\/img><\/p>\n\nThis plot above is an excellent example of Homoscedasticity. As you can see, the residual variance is the same as the value of the predictor variable increases. One way to fix this Heteroscedasticity is by using a transformation method like log-transformation or box-cox transformation. We will do that later.\n\n**Multivariate Normality ( Normality of Errors):**\nThe linear regression analysis requires the dependent variable to be multivariate normally distributed. A histogram, box plot, or a Q-Q-Plot can check if the target variable is normally distributed. The goodness of fit test, e.g., the Kolmogorov-Smirnov test can check for normality in the dependent variable. We already know that our target variable does not follow a normal distribution. Let's bring back the three charts to show our target variable.","fe5f8055":"**OverallQual** is a categorical variable, and a scatter plot is not the best way to visualize categorical variables. However, there is an apparent relationship between the two features. The price of the houses increases with the overall quality. Let's check out some more features to determine the outliers. Let's focus on the numerical variables this time.","c72dbcc5":"As we discussed before, there is a linear relationship between SalePrice and GrLivArea. We want to know\/estimate\/predict the sale price of a house based on the given area, How do we do that? One naive way is to find the average of all the house prices. Let's find a line with the average of all houses and place it in the scatter plot. Simple enough.","20a606fd":"# Modeling the Data\n \nBefore modeling each algorithm, I would like to discuss them for a better understanding. This way I would review what I know and at the same time help out the community. If you already know enough about Linear Regression, you may skip this part and go straight to the part where I fit the model. However, if you take your time to read this and other model description sections and let me know how I am doing, I would genuinely appreciate it. Let's get started. \n\n**Linear Regression**\n<div>\n    We will start with one of the most basic but useful machine learning model, **Linear Regression**. However, do not let the simplicity of this model fool you, as Linear Regression is the base some of the most complex models out there. For the sake of understanding this model, we will use only two features, **SalePrice** and **GrLivArea**. Let's take a sample of the data and graph it.","dc521211":"## Creating Dummy Variables. \n","982d278d":"### Elastic Net: \nElastic Net is the combination of both Ridge and Lasso. It adds both the sum of squared coefficients and the absolute sum of the coefficients with the ordinary least square function. Let's look at the function. \n\n### $$ \\text{minimize:}\\; RSS + Ridge + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_1\\sum_{j=1}^p |\\beta_j| + \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n\nThis equation is pretty self-explanatory if you have been following this kernel so far.","f2cd595f":"### Missing Train values","1c4cb61b":"# Fitting model (Advanced approach)","1e77d209":"So, there are no missing value left. ","378978e5":"So, we have calculated the beta coefficients.  We can now plug them in the linear equation to get the predicted y value. Let's do that.","7ab312c0":"### Lasso:\nLasso adds penalty equivalent to the absolute value of the sum of coefficients. This penalty is added to the least square loss function and replaces the squared sum of coefficients from Ridge. \n\n## $$ \\text{minimize:}\\; RSS + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_1\\sum_{j=1}^p |\\beta_j|$$\n\nHere, \n* $\\lambda_2$ is a constant similar to the Ridge function. \n* $\\sum_{j=1}^p |\\beta_j|$ is the absolute sum of the coefficients.","25fee63c":" This way of model fitting above is probably the simplest way to construct a machine learning model. However, Let's dive deep into some more complex regression. \n\n### Regularization Models\nWhat makes regression model more effective is its ability of *regularizing*. The term \"regularizing\" stands for models ability **to structurally prevent overfitting by imposing a penalty on the coefficients.** \n\n\nThere are three types of regularizations. \n* **Ridge**\n* **Lasso**\n* **Elastic Net**\n\nThese regularization methods work by penalizing **the magnitude of the coefficients of features** and at the same time **minimizing the error between the predicted value and actual observed values**.  This minimization becomes a balance between the error (the difference between the predicted value and observed value) and the size of the coefficients. The only difference between Ridge and Lasso is **the way they penalize the coefficients.** Elastic Net is the combination of these two. **Elastic Net** adds both the sum of the squares errors and the absolute value of the squared error. To get more in-depth of it, let us review the least squared loss function. \n\n**Ordinary least squared** loss function minimizes the residual sum of the square(RSS) to fit the data:\n\n### $$ \\text{minimize:}\\; RSS = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 $$\n\nLet's review this equation once again, Here: \n* $y_i$ is the observed value. \n* $\\hat{y}_i$ is the predicted value. \n* The error = $y_i$ - $\\hat{y}_i$\n* The square of the error = $(y_i - \\hat{y}_i)^2$\n* The sum of the square of the error = $\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$, that's the equation on the left. \n* The only difference between left sides equation vs. the right sides one above is the replacement of $\\hat{y}_i$, it is replaced by $\\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)$, which simply follow's the slope equation, y = mx+b, where, \n* $\\beta_0$ is the intercept. \n* **$\\beta_j$ is the coefficient of the feature($x_j$).**\n\nLet's describe the effect of regularization and then we will learn how we can use loss function in Ridge.\n* One of the benefits of regularization is that it deals with **multicollinearity**(high correlation between predictor variables) well, especially Ridge method. Lasso deals with **multicollinearity** more brutally by penalizing related coefficients and force them to become zero, hence removing them. However, **Lasso** is well suited for redundant variables. \n \n***\n<div>\n    \n ### Ridge:\nRidge regression adds penalty equivalent to the square of the magnitude of the coefficients. This penalty is added to the least square loss function above and looks like this...\n\n### $$ \\text{minimize:}\\; RSS+Ridge = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\lambda_2\\sum_{j=1}^p \\beta_j^2$$\n\nHere, \n* $\\lambda_2$ is constant; a regularization parameter. It is also known as $\\alpha$. The higher the value of this constant the more the impact in the loss function. \n    * When $\\lambda_2$ is 0, the loss funciton becomes same as simple linear regression. \n    * When $\\lambda_2$ is $\\infty$, the coefficients become 0\n    * When $\\lambda_2$ is between  0 and $\\infty$(0<$\\lambda_2$<$\\infty$), The $\\lambda_2$ parameter will decide the miagnitude given to the coefficients. The coefficients will be somewhere between 0 and ones for simple linear regression. \n* $\\sum_{j=1}^p \\beta_j^2$, is the squared sum of all coefficients. \n\nNow that we know every nitty-gritty details about this equation, let's use it for science, but before that a couple of things to remember. \n* It is essential to standardize the predictor variables before constructing the models. \n* It is important to check for multicollinearity,","1da48cbe":"Here we are plotting our target variable with two independent variables **GrLivArea** and **MasVnrArea**. It's pretty apparent from the chart that there is a better linear relationship between **SalePrice** and **GrLivArea** than **SalePrice** and **MasVnrArea**. One thing to take note here, there are some outliers in the dataset. It is imperative to check for outliers since linear regression is sensitive to outlier effects. Sometimes we may be trying to fit a linear regression model when the data might not be so linear, or the function may need another degree of freedom to fit the data. In that case, we may need to change our function depending on the data to get the best possible fit. In addition to that, we can also check the residual plot, which tells us how is the error variance across the true line. Let's look at the residual plot for independent variable **GrLivArea** and our target variable **SalePrice **. ","ffc6b065":"### Missing Train values","22cf7825":"Now, let's make sure that the target variable follows a normal distribution. If you want to learn more about the probability plot(Q-Q plot), try [this](https:\/\/www.youtube.com\/watch?v=smJBsZ4YQZw) video. You can also check out [this](https:\/\/www.youtube.com\/watch?v=9IcaQwQkE9I) one if you have some extra time.","bcc4e3cf":"As you can see, there are two outliers in the plot above. We will get rid off them later. Let's look at another scatter plot with a different feature.\n\n### SalePrice vs GarageArea","664eb8da":"Okay, I think we have seen enough. Let's discuss what we have found so far. \n\n# Observations\n* Our target variable shows an unequal level of variance across most predictor(independent) variables. This is called **Heteroscedasticity(more explanation below)** and is a red flag for the multiple linear regression model.\n* There are many outliers in the scatter plots above that took my attention. \n\n* The two on the top-right edge of **SalePrice vs. GrLivArea** seem to follow a trend, which can be explained by saying that \"As the prices increased, so did the area. \n* However, The two on the bottom right of the same chart do not follow any trends. We will get rid of these two below.","baa3f096":"If you would like to improve this result further, you can think about the assumptions of the linear regressions and apply them as we have discussed earlier in this kernel. \n\n\nSimilar to **Simple Linear Regression**, there is an equation for multiple independent variables to predict a target variable. The equation is as follows.\n\n## $$ \\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n $$\n\nHere, We already know parts of the equation, and from there we can keep adding new features and their coefficients with the equations. Quite simple, isn't it. \n\nWe can have a target variable predicted by multiple independent variables using this equation. Therefore this equation is called **Multiple Linear Regression.** Let's try this regression in the housing dataset.\n","b7a851fc":"> If you are reading this in my github page, you may find it difficult to follow through as the following section includes mathematical equation. Please checkout [this](https:\/\/www.kaggle.com\/masumrumi\/a-stats-analysis-and-ml-workflow-of-house-pricing) kernel at Kaggle. \n\nWe will explain more on MSE later. For now, let's just say, the closer the value of MSE is to \"0\", the better. Of course, it makes sense since we are talking about an error(mean squared error). We want to minimize this error. How can we do that? \n\nIntroducing **Linear Regression**, one of the most basic and straightforward models. Many of us may have learned to show the relationship between two variable using something called \"y equals mX plus b.\" Let's refresh our memory and call upon on that equation.\n\n\n\n# $$ {y} = mX + b $$\n\n\n\nHere, \n* **m** = slope of the regression line. It represents the relationship between X and y. In another word, it gives weight as to for each x(horizontal space) how much y(vertical space) we have to cover. In machine learning, we call it **coefficient**. \n* **b** = y-intercept. \n* **x** and **y** are the data points located in x_axis and y_axis respectively. \n\n\n<br\/>\n\nIf you would like to know more about this equation, Please check out this [video](https:\/\/www.khanacademy.org\/math\/algebra\/two-var-linear-equations\/writing-slope-intercept-equations\/v\/graphs-using-slope-intercept-form). \n\nThis slope equation gives us an exact linear relationship between X and y. This relationship is \"exact\" because we are given X and y beforehand and based on the value of X and y, we come up with the slope and y-intercept, which in turns determine the relationship between X and y. However, in real life, data is not that simple. Often the relationship is unknown to us, and even if we know the relationship, it may not always be exact. To fit an exact slope equation in an inexact relationship of data we introduce the term error. Let's see how mathematicians express this error with the slope equation. \n\n## $$ y = \\beta_0 + \\beta_1 x + \\epsilon \\\\ $$\n\nAnd, this is the equation for a simple linear regression.\nHere,\n* y = Dependent variable. This is what we are trying to estimate\/solve\/understand. \n* $\\beta_0$ = the y-intercept, it is a constant and it represents the value of y when x is 0. \n* $\\beta_1$ = Slope, Weight, Coefficient of x. This metrics is the relationship between y and x. In simple terms, it shows 1 unit increase in y changes when 1 unit increases in x. \n* $x_1$ = Independent variable ( simple linear regression ) \/variables.\n* $ \\epsilon$ = error or residual. \n\n### $$ \\text{residual}_i = y_i - \\hat{y}_i$$\nThis error is the only part that's different\/addition from the slope equation. This error exists because in real life we will never have a dataset where the regression line crosses exactly every single data point. There will be at least a good amount of points where the regression line will not be able to go through for the sake of model specifications and ** bias-variance tradeoff **(more on this later). This error term accounts for the difference of those points. So, simply speaking, an error is the difference between an original value( $y_i$ ) and a predicted value( $\\hat{y}_i$ ). \n\nWe use this function to predict the values of one dependent(target) variable based on one independent(predictor) variable. Therefore this regression is called **Simple linear regression(SLR).** If we were to write the equation regarding the sample example above it would simply look like the following equation, \n## $$ Sale Price= \\beta_0 + \\beta_1 (Area) + \\epsilon \\\\ $$\n\nThis equation gives us a line that fits the data and often performs better than the average line above. But,\n* How do we know that Linear regression line is actually performing better than the average line? \n* What metrics can we use to answer that? \n* How do we know if this line is even the best line(best-fit line) for the dataset? \n* If we want to get even more clear on this we may start with answering, How do we find the $\\beta_0$(intercept) and  $\\beta_1$(coefficient) of the equation?\n\n<b>Finding $\\beta_0$(intercept) and  $\\beta_1$(coefficient):<\/b>\n\nWe can use the following equation to find the $\\beta_0$(intercept) and  $\\beta_1$(coefficient)\n\n\n### $$ \\hat{\\beta}_1 = r_{xy} \\frac{s_y}{s_x}$$\n### $$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$\n\nHere...\n- $\\bar{y}$ : the sample mean of observed values $Y$\n- $\\bar{x}$ : the sample mean of observed values $X$\n- $s_y$ : the sample standard deviation of observed values $Y$\n- $s_x$ : the sample standard deviation of observed values $X$\n\n    > There are two types of STD's. one is for sample population and one is for Total population.\n    > Check out [this](https:\/\/statistics.laerd.com\/statistical-guides\/measures-of-spread-standard-deviation.php) article for more. \n\n- $r_{xy}$ : the sample Pearson correlation coefficient between observed $X$ and $Y$\n\n\nI hope most of us know how to calculate all these components from the two equations above by hand. I am going to only mention the equation of the pearson correlation(r_xy) here as it may be unknown to some of the readers. \n\n### $$ r_{xy}= \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sqrt{\\sum(x_i - \\bar{x})^2{\\sum(y_i - \\bar{y})^2}}}$$\n\nLet's get on with calculating the rest by coding.","b1764c19":"> **Imputing Missing Values**","a9bcb5f6":"# Submission","614918fb":"Now that we have our predicted y values let's see how the predicted regression line looks in the graph.","8dae8dfb":"# Blending Models","6ccb1628":"A much-anticipated decrease in mean squared error(mse), therefore better-predicted model. The way we compare between the two predicted lines is by considering their errors. Let's put both of the model's side by side and compare the errors.","3518ae5d":"These are the predictor variables sorted in a descending order starting with the most correlated one **OverallQual**. Let's put this one in a scatter plot and see how it looks.","ac805580":"On the two charts above, the left one is the average line, and the right one is the regression line. <font color=\"blue\"><b>Blue<\/b><\/font> dots are observed data points and <font color=\"red\"><b>red<\/b><\/font> lines are error distance from each observed data points to model-predicted line. As you can see, the regression line reduces much of the errors; therefore, performs much better than average line. \n\nNow, we need to introduce a couple of evaluation metrics that will help us compare and contrast models. One of them is mean squared error(MSE) which we used while comparing two models. Some of the other metrics are...\n\n* RMSE (Root Mean Squared Error)\n### $$ \\operatorname{RMSE}= \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(\\hat{y_i} - y_i)^2} $$\n\nHere\n* $y_i$ = Each observed data point. \n* $\\bar{y}$ = Mean of y value.\n* $\\hat{y_i}$ = Predicted data point for each $x_i$ depending on i. \n\n\n* MSE(Mean Squared Error)\n### $$\\operatorname{MSE}= \\frac{1}{n}\\sum_{i=1}^n(\\hat{y_i} - y_i)^2$$\n\n* MAE (Mean Absolute Error)\n### $$\\operatorname{MAE} = \\frac{\\sum_{i=1}^n|{\\bar{y} - y_i}|}{n}$$\n\n* RSE (Relative Squared Error)\n### $$\\operatorname{RSE}= \\frac{\\sum_{i=1}^n(\\hat{y_i} - y_i)^2}{\\sum_{i=1}^n(\\bar{y} - y_i)^2}$$\n\n* RAE (Relative Absolute Error) \n### $$\\operatorname{RAE}= \\frac{\\sum_{i=1}^n |\\hat{y_i} - y_i|}{\\sum_{i=1}^n |\\bar{y} - y_i|}$$\n\n> and \n* $R^2$ (Coefficient of the determination)","1d459cc1":"# Describe the Datasets","df5932f8":"It looks like there are quite a bit Skewness and Kurtosis in the target variable. Let's talk about those a bit. \n\n<b>Skewness<\/b> \n* is the degree of distortion from the symmetrical bell curve or the normal curve. \n* So, a symmetrical distribution will have a skewness of \"0\". \n* There are two types of Skewness: <b>Positive and Negative.<\/b> \n* <b>Positive Skewness<\/b>(similar to our target variable distribution) means the tail on the right side of the distribution is longer and fatter. \n* In <b>positive Skewness <\/b> the mean and median will be greater than the mode similar to this dataset. Which means more houses were sold by less than the average price. \n* <b>Negative Skewness<\/b> means the tail on the left side of the distribution is longer and fatter.\n* In <b>negative Skewness <\/b> the mean and median will be less than the mode. \n* Skewness differentiates in extreme values in one versus the other tail. \n\nHere is a picture to make more sense.  \n![image](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*nj-Ch3AUFmkd0JUSOW_bTQ.jpeg)\n\n\n<b>Kurtosis<\/b>\nAccording to Wikipedia, \n\n*In probability theory and statistics, **Kurtosis** is the measure of the \"tailedness\" of the probability. distribution of a real-valued random variable.* So, In other words, **it is the measure of the extreme values(outliers) present in the distribution.** \n\n* There are three types of Kurtosis: <b>Mesokurtic, Leptokurtic, and Platykurtic<\/b>. \n* Mesokurtic is similar to the normal curve with the standard value of 3. This means that the extreme values of this distribution are similar to that of a normal distribution. \n* Leptokurtic Example of leptokurtic distributions are the T-distributions with small degrees of freedom.\n* Platykurtic: Platykurtic describes a particular statistical distribution with thinner tails than a normal distribution. Because this distribution has thin tails, it has fewer outliers (e.g., extreme values three or more standard deviations from the mean) than do mesokurtic and leptokurtic distributions. \n\n![image](https:\/\/i2.wp.com\/mvpprograms.com\/help\/images\/KurtosisPict.jpg?resize=375%2C234)\n\n\nYou can read more about this from [this](https:\/\/codeburst.io\/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa) article. \n\nWe can fix this by using different types of transformation(more on this later). However, before doing that, I want to find out the relationships among the target variable and other predictor variables. Let's find out.","b51dbbf1":"and the next ?\n### SalePrice vs 1stFlrSF","742a1ad1":"## Using cross validation.","252a7084":"## Deleting features","21bb3826":"## Fixing Skewness","1ea41691":"And the next one..?\n### SalePrice vs TotalBsmtSF","6baef13c":"As you can see, the log transformation removes the normality of errors, which solves most of the other errors we talked about above. Let's make a comparison of the pre-transformed and post-transformed state of residual plots. ","1a69b908":"## Dealing with Missing Values\n> **Missing data in train and test data(all_data)**","f4fafad7":"### The following part is a work in progress!!\n\nSo, from the Evaluation section above, we know that, \n### $$ RSS = \\sum_{i=1}^n \\left(y_i - \\hat{y}_i\\right)^2 $$\n\nAnd, we already know ...\n## $$ \\hat{y} = \\beta_0 + \\beta_1 x + \\epsilon \\\\ $$\n\nLet's plug in( $\\hat{Y}$  ) equation in the RSS equation and we get...\n$$RSS = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 $$\n\nThis equation is also known as the loss function. Here, **\"loss\"** is the sum of squared residuals(More on this later). \n\n### Mean Squared Error\nNow let's get back to our naive prediction and calculate the **Mean squared error**, which is also a metrics similar to RSS, helps us determine how well our model is performing. In **Mean squared error** we subtract the mean of y from each y datapoints and square them. \n","53ed3d36":"As we look through these scatter plots, I realized that it is time to explain the assumptions of Multiple Linear Regression. Before building a multiple linear regression model, we need to check that these assumptions below are valid.\n## Assumptions of Regression\n\n* **Linearity ( Correct functional form )** \n* **Homoscedasticity ( Constant Error Variance )( vs Heteroscedasticity ). **\n* **Independence of Errors ( vs Autocorrelation ) **\n* **Multivariate Normality ( Normality of Errors ) **\n* **No or little Multicollinearity. ** \n\nSince we fit a linear model, we assume that the relationship is **linear**, and the errors, or residuals, are pure random fluctuations around the true line. We expect that the variability in the response(dependent) variable doesn't increase as the value of the predictor(independent) increases, which is the assumptions of equal variance, also known as **Homoscedasticity**. We also assume that the observations are independent of one another(**No Multicollinearity**), and a correlation between sequential observations or auto-correlation is not there.\n\nNow, these assumptions are prone to happen altogether. In other words, if we see one of these assumptions in the dataset, it's more likely that we may come across with others mentioned above. Therefore, we can find and fix various assumptions with a few unique techniques.\n\nSo, **How do we check regression assumptions? We fit a regression line and look for the variability of the response data along the regression line.** Let's apply this to each one of them.\n\n**Linearity(Correct functional form):** \nLinear regression needs the relationship between each independent variable and the dependent variable to be linear. The linearity assumption can be tested with scatter plots. The following two examples depict two cases, where no or little linearity is present. ","c9f082aa":"How about one more...","c64a8bbe":"### SalePrice vs OverallQual"}}