{"cell_type":{"7cad2266":"code","0bf10852":"code","2e03c3ac":"code","30498863":"code","dc9daed9":"code","213d08da":"code","d101111e":"code","05dfef69":"code","91008fe1":"code","074116fa":"code","6c91ae45":"code","5185fec9":"code","27f05895":"code","9eff6d3f":"code","51d68904":"code","f5b5f0a8":"code","e9dbf8c0":"code","477c6188":"code","56477447":"code","4cf7be95":"code","864fe6ea":"code","58639eed":"code","7295ce90":"code","27c1b631":"code","5e2a989d":"code","ac84149a":"code","5a551149":"code","e9297381":"code","3d3a757c":"code","987fdb0e":"code","36ebdfb1":"code","1d72d814":"code","1cbd81bb":"code","04a0c783":"code","4c71d60b":"code","82fded41":"code","c0af59d4":"code","e300a245":"code","0232c4b1":"code","cd6af489":"code","a5d454b4":"code","8ead6728":"code","e0cf6eda":"code","640faf14":"code","6e2db2f8":"code","1e341334":"code","617fe0f7":"code","adbe1648":"code","e196ca1c":"code","8383af20":"code","182d6b4e":"code","756bf852":"code","fc81191c":"code","a514b13d":"code","36da7eec":"code","f7b866d0":"code","176eccd3":"code","dcad6c3c":"code","004bf50d":"code","a90a8e34":"code","a6ac3335":"code","4375222e":"code","a1cb94bb":"code","ed38da75":"code","833d2b49":"code","64007462":"code","782801e4":"code","83e47552":"code","80038df7":"code","b4496118":"code","50ce135d":"code","7903f217":"code","bf6d7910":"markdown","4e14a326":"markdown","cd7b3dad":"markdown","b4c7a80e":"markdown","ebc776e4":"markdown","9466057a":"markdown","1802a411":"markdown","14211a32":"markdown","8ca51050":"markdown","82e7cdf5":"markdown","7fbac148":"markdown","e3e0dd0b":"markdown","6de66fac":"markdown","7684d4b4":"markdown","1991f950":"markdown","183e97d2":"markdown","034bd04d":"markdown","b96d6675":"markdown","6074da0e":"markdown","6c123cbd":"markdown","1f4a50ce":"markdown","7ed251c5":"markdown","c45accb1":"markdown","d133f6cd":"markdown","b58a0f19":"markdown","7cd63317":"markdown","f6c6c84a":"markdown","7a06c0c1":"markdown","990d68e5":"markdown","69464578":"markdown","8478de81":"markdown","6a3ce33f":"markdown","2f01823c":"markdown","84ed9e7b":"markdown","962ed914":"markdown","b58e7f0c":"markdown","5fd3fed4":"markdown","eedcadab":"markdown","ca954323":"markdown","77e519da":"markdown","27b89b57":"markdown","f5f39478":"markdown","c64a8641":"markdown","ba0dfb7a":"markdown","ca80afe0":"markdown","24421dee":"markdown","c4a53494":"markdown","0067ffac":"markdown","91fcc099":"markdown","c23c5110":"markdown","ece2f6a8":"markdown"},"source":{"7cad2266":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm.notebook import tqdm\n\nfrom yellowbrick.regressor import ResidualsPlot\nfrom sklearn.decomposition  import FactorAnalysis\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import model_selection\n#implement linear regression\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge\nfrom sklearn.model_selection import train_test_split\n\n\n","0bf10852":"train = pd.read_csv('..\/input\/data-analysis-2020\/train.csv')\ntest = pd.read_csv('..\/input\/data-analysis-2020\/test.csv')","2e03c3ac":"train =train[train['next_day_ret']>-1000]\ntrain['next_day_ret'].hist()","30498863":"train.date=pd.to_datetime(train.date,format='%Y-%m-%d')\ntrain['currReturn']=train.groupby(['company'])['close'].pct_change().fillna(0)\ntrain['nextReturn']=train.groupby(['company'])['currReturn'].shift(-1) # defined in the course ","dc9daed9":"corrMatrix = train[['currReturn','nextReturn','next_day_ret']].corr()\nsns.heatmap(corrMatrix, annot=True)\nplt.show()","213d08da":"train = train[train['company']=='ADWYA']\ny = train['next_day_ret'].values.reshape(-1,1)\nx = train['nextReturn']\nplt.scatter(x, y, alpha=0.5)\nplt.title('nextReturn vs next_day_ret')\nplt.xlabel('nextReturn')\nplt.ylabel('next_day_ret')\nplt.show()","d101111e":"train= train[train['company']=='ADWYA']\ntrain= train[train['nextReturn']==0]\ntrain['value'] = train['value'].fillna(train['value'].mean())\ncorrmat = train.corr()\nk = 10 #number of variables for heatmap\ncols = abs(corrmat).nlargest(k, 'next_day_ret')['next_day_ret'].index\ncm = np.corrcoef(train[cols].values.T)\nplt.figure(figsize=(20,20))\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","05dfef69":"train = pd.read_csv('..\/input\/data-analysis-2020\/train.csv')\ntest = pd.read_csv('..\/input\/data-analysis-2020\/test.csv')\ntrain =train[train['next_day_ret']>-1000] # drop outliers \ntrain_len = len(train)\nall_data = pd.concat([train,test], ignore_index=True)","91008fe1":"all_data.date=pd.to_datetime(all_data.date,format='%Y-%m-%d')\nall_data['currReturn']=all_data.groupby(['company'])['close'].pct_change().fillna(0)\nall_data['nextReturn']=all_data.groupby(['company'])['currReturn'].shift(-1) # defined in the course ","074116fa":"all_data['nextReturn'].hist()","6c91ae45":"all_data['currReturn_square'] = all_data['currReturn']**2","5185fec9":"#Resduial analysis \n# Instantiate the linear model and visualizer\nmodel = LinearRegression()\nvisualizer = ResidualsPlot(model)\ndata = all_data.iloc[:train_len]\ndata = data[data['company']=='ADWYA']\ndata = data[data['nextReturn']>-0.01]\nX_train = data[['currReturn','currReturn_square']]\ny_train = data['nextReturn']\nvisualizer.fit(X_train, y_train) \nvisualizer.score(X_train, y_train)  # Fit the training data to the visualizer\n","27f05895":"AGRO_AlIMENTAIRE = ['LAND OR', 'STE TUN. DU SUCRE', 'SFBT', 'DELICE HOLDING', 'CEREALIS', 'POULINA GP HOLDING', 'SOPAT', 'ELBENE INDUSTRIE']\nATUO = ['ASSAD', 'STEQ', 'GIF-FILTER', 'STIP']\nBANQUES =['BT', 'ATTIJARI BANK', 'BH', 'BIAT', 'STB', 'AMEN BANK', 'ATB', 'WIFACK INT BANK', 'UBCI', 'UIB', 'BNA', 'BTE (ADP)']\nBATIMENT = ['SITS', 'SIMPAR', 'SANIMED', 'ESSOUKNA', 'SOTEMAIL', 'SOMOCER']\nTECHNOLOGIE = ['SERVICOM', 'ONE TECH HOLDING', 'TELNET HOLDING', 'SOTETEL', 'EURO-CYCLES', 'AETECH', 'HEXABYTE']\nSERVICES_FINANCIERS = ['MAGHREB INTERN PUB', 'HANNIBAL LEASE', 'ASS MULTI ITTIHAD', 'BEST LEASE', 'TAWASOL GP HOLDING', 'CIL', 'ATTIJARI LEASING', 'TUNINVEST-SICAR', 'MODERN LEASING', 'ATL', 'SPDIT - SICAF', 'PLAC. TSIE-SICAF', 'TUNISAIR']\nASSURANCES = ['STAR', 'TUNIS RE', 'ASTREE']\nSERVICE_INDUS = ['MPBS', 'SOTUVER', 'NEW BODY LINE', 'ELECTROSTAR', 'SOTIPAPIER', 'SOTRAPIL', 'SIAME', 'ATELIER MEUBLE INT', 'AMS', 'OFFICEPLAST']\nPHARMACEUTIC =['SIPHAT', 'ADWYA', 'ICF', 'UNIMED', 'ALKIMIA', 'AIR LIQUDE TSIE']\nMAT_PREMIERES = ['TPR', 'CIMENTS DE BIZERTE', 'CARTHAGE CEMENT']\nDISTRIBUTION =['UADH', 'SAH', 'CELLCOM', 'SOTUMAG', 'CITY CARS', 'MAGASIN GENERAL', 'MONOPRIX', 'ARTES', 'ENNAKL AUTOMOBILES', 'SITEX']","9eff6d3f":"all_company = {'AGRO_AlIMENTAIRE+DISTRIBUTION+MAT_PREMIERES+SERVICE_INDUS':AGRO_AlIMENTAIRE+DISTRIBUTION+MAT_PREMIERES+SERVICE_INDUS\n               ,'SERVICES_FINANCIERS+ASSURANCES+BANQUES':SERVICES_FINANCIERS+ASSURANCES+BANQUES\n               ,'PHARMACEUTIC+TECHNOLOGIE+ATUO+BATIMENT':PHARMACEUTIC+TECHNOLOGIE+ATUO+BATIMENT}","51d68904":"company_map = dict()\nfor sector in all_company.keys() : \n  for company in all_company[sector] : \n    company_map[company] = sector\nall_data['sector'] = all_data['company'].map(company_map)","f5b5f0a8":"## all_company current return \nreturn_data=all_data.pivot(index='date',columns='company',values='currReturn').replace([np.inf, -np.inf],0).fillna(0)\nreturn_data.head()","e9dbf8c0":"all_data['date']=all_data['date'].astype('str')\n# correlated sector \nsector_features = []\nfor se in all_company.keys() : \n  companies = all_company[se]\n  for n_compent in range(1,6) :\n    pca = PCA(n_components=n_compent)\n    pca.fit(return_data[companies].values)\n    if sum(pca.explained_variance_ratio_)> 0.70 : \n      break \n  print(f'{se}  : number of PC is {n_compent} and explain {100*sum(pca.explained_variance_ratio_)} % of the total  Data variance')\n  components = pca.transform(return_data[companies].values)\n  col = [se+'_pc_'+str(i) for i in range(n_compent)]\n  sector_features += col\n  new_compontes = pd.DataFrame(data = components , columns=col  )\n  new_compontes['date'] = return_data.index.tolist()\n  new_compontes['date'] = new_compontes['date'].astype(str)\n  all_data =pd.merge(all_data,new_compontes,how = 'left' , on= 'date')\n  all_data = all_data.fillna(0)","477c6188":"for stock in tqdm(all_data['company'].unique()) :\n  index = all_data[all_data['company']==stock].index\n  all_data.loc[index, 'Momentum_1D'] = (all_data.loc[index, 'currReturn'] - all_data.loc[index, 'currReturn'].shift(1)).fillna(0)\n  all_data.loc[index, 'Momentum'] = (all_data.loc[index, 'currReturn'] - all_data.loc[index, 'currReturn'].shift(5)).fillna(0)","56477447":"for company in all_data['company'].unique() : \n  sub_set = all_data[all_data['company']==company][['date','currReturn']]\n  index = sub_set.index\n  sub_set = sub_set.sort_values('date')\n  for i in range(1,20) : \n    col = 'return( t-'+str(i)+' )'\n    sub_set[col] = sub_set['currReturn'].shift(i)\n    all_data.loc[index,col] = sub_set[col]\ncol = ['return( t-'+str(i)+' )' for i in range(1,6) ] + ['currReturn']\nall_data['mean_last_week'] = all_data[col].mean(axis=1) \nall_data['std_last_week'] = all_data[col].std(axis=1)\nall_data['max_last_week'] = all_data['mean_last_week'] +all_data['std_last_week']\nall_data['min_last_week'] = all_data['mean_last_week'] -all_data['std_last_week']\ncol = ['return( t-'+str(i)+' )' for i in range(1,12) ] + ['currReturn']\nall_data['mean_last_2week'] = all_data[col].mean(axis=1) \nall_data['std_last_2week'] = all_data[col].std(axis=1)\nall_data['max_last_2week'] = all_data['mean_last_2week'] +all_data['std_last_2week']\nall_data['min_last_2week'] = all_data['mean_last_2week'] -all_data['std_last_2week']\ncol = ['return( t-'+str(i)+' )' for i in range(1,20) ] + ['currReturn']\nall_data['mean_last_month'] = all_data[col].mean(axis=1) \nall_data['std_last_month'] = all_data[col].std(axis=1)\nall_data['max_last_month'] = all_data['mean_last_month'] +all_data['std_last_month']\nall_data['min_last_month'] = all_data['mean_last_month'] -all_data['std_last_month']","4cf7be95":"last_mean = ['mean_last_week','max_last_week','min_last_week','mean_last_2week','max_last_2week','min_last_2week','mean_last_month','max_last_month','min_last_month']","864fe6ea":"for stock in tqdm(all_data['company'].unique()):\n  index = all_data[all_data['company']==stock].index\n  all_data.loc[index,'EMA'] = all_data.loc[index,'currReturn'].ewm(span=5,min_periods=1,adjust=True,ignore_na=False).mean()\n  all_data.loc[index,'TEMA'] = (3 * all_data.loc[index,'EMA'] - 3 * all_data.loc[index,'EMA']) + (all_data.loc[index,'EMA']*all_data.loc[index,'EMA']*all_data.loc[index,'EMA'])\n","58639eed":"all_data['TEMA_square'] = all_data['TEMA']**2","7295ce90":"for stock in tqdm(all_data['company'].unique()):\n  index = all_data[all_data['company']==stock].index     \n  all_data.loc[index,'26_ema'] =  all_data.loc[index,'currReturn'].ewm(span=26,min_periods=0,adjust=True,ignore_na=False).mean()\n  all_data.loc[index,'12_ema'] =   all_data.loc[index,'currReturn'].ewm(span=12,min_periods=0,adjust=True,ignore_na=False).mean()\n  all_data.loc[index,'MACD'] = all_data.loc[index,'12_ema'] -  all_data.loc[index,'26_ema']\n","27c1b631":"def bbands(price, length=30, numsd=2):\n    \"\"\" returns average, upper band, and lower band\"\"\"\n    #ave = pd.stats.moments.rolling_mean(price,length)\n    ave = price.rolling(window = length, center = False).mean()\n    #sd = pd.stats.moments.rolling_std(price,length)\n    sd = price.rolling(window = length, center = False).std()\n    upband = ave + (sd*numsd)\n    dnband = ave - (sd*numsd)\n    return np.round(ave,3), np.round(upband,3), np.round(dnband,3)\nfor stock in tqdm(all_data['company'].unique()) :\n    index = all_data[all_data['company']==stock].index\n    all_data.loc[index,'BB_Middle_Band'],all_data.loc[index,'BB_Upper_Band'], all_data.loc[index,'BB_Lower_Band'] = bbands(all_data.loc[index,'currReturn'], length=14, numsd=1)\n    all_data.loc[index,'BB_Middle_Band'] = all_data.loc[index,'BB_Middle_Band'].fillna(0)\n    all_data.loc[index,'BB_Upper_Band'] = all_data.loc[index,'BB_Upper_Band'].fillna(0)\n    all_data.loc[index,'BB_Lower_Band'] = all_data.loc[index,'BB_Lower_Band'].fillna(0)","5e2a989d":"def aroon(df, tf=25):\n  aroonup = []\n  aroondown = []\n  x = tf\n  while x< len(df['date']):\n    aroon_up = ((df['currReturn'][x-tf:x].tolist().index(max(df['currReturn'][x-tf:x])))\/float(tf))*100\n    aroon_down = ((df['currReturn'][x-tf:x].tolist().index(min(df['currReturn'][x-tf:x])))\/float(tf))*100\n    aroonup.append(aroon_up)\n    aroondown.append(aroon_down)\n    x+=1\n  return aroonup, aroondown","ac84149a":"for stock in tqdm(all_data['company'].unique()) :\n  index = all_data[all_data['company']==stock].index\n  listofzeros = [0] * 25\n  up, down = aroon(all_data.loc[index])\n  aroon_list = [x - y for x, y in zip(up,down)]\n  if len(aroon_list)==0:\n    aroon_list = [0] *     all_data.loc[index].shape[0]\n    all_data.loc[index,'Aroon_Oscillator'] = aroon_list\n  else:\n    all_data.loc[index,'Aroon_Oscillator'] = listofzeros+aroon_list","5a551149":"all_data['Aroon_current'] = (all_data['Aroon_Oscillator'])+all_data['currReturn']","e9297381":"def CCI(df, n, constant):\n    TP = df['currReturn'] \n    CCI = pd.Series((TP - TP.rolling(window=n, center=False).mean()) \/ (constant * TP.rolling(window=n, center=False).std()))\n    return CCI\nfor stock in tqdm(all_data['company'].unique()):\n  index = all_data[all_data['company']==stock].index    \n  all_data.loc[index,'CCI'] = CCI( all_data.loc[index], 20, 0.015)","3d3a757c":"all_data['CCI'] = all_data['CCI'].replace([np.inf],500)\nall_data['CCI'] = all_data['CCI'].replace([-np.inf],-500)\nall_data['CCI'] = all_data['CCI'] \/50\nall_data['CCI_current']  = all_data['currReturn'] * all_data['CCI']","987fdb0e":"all_data.date=pd.to_datetime(all_data.date,format='%Y-%m-%d')\nall_data['year']=all_data['date'].dt.year \nall_data['month']=all_data['date'].dt.month \nall_data['dayofweek_num']=all_data['date'].dt.dayofweek  \nall_data['quarter']=all_data['date'].dt.quarter\nall_data['day']=all_data['date'].dt.day","36ebdfb1":"new = all_data.groupby(['company','year','month'])['currReturn'].agg({'mean'})\nnew = new.reset_index()\nname = 'Month_mean'\nlast_mean.append(name)\nnew.columns = ['company','year','month',name] \nall_data =pd.merge(all_data,new,how = 'left',on = ['company','year','month'])","1d72d814":"get_time = pd.get_dummies(all_data[['quarter','month','dayofweek_num']].astype('str'))\nall_data =pd.concat([all_data,get_time],axis=1)\ntime = get_time.columns.tolist()","1cbd81bb":"train = all_data.iloc[:train_len].fillna(0)\ntest =  all_data.iloc[train_len:].copy()","04a0c783":"num_features= ['MACD', 'BB_Middle_Band', 'BB_Upper_Band','EMA','Momentum']+last_mean+[ '12_ema', '26_ema']+['currReturn','currReturn_square','TEMA','Aroon_current','CCI_current','currReturn_square','TEMA_square']\nfor num in num_features : \n  scale = preprocessing.MinMaxScaler()\n  scale.fit(train[num].values.reshape(-1,1))\n  train[num] = scale.transform(train[num].values.reshape(-1,1))\n  test[num] = scale.transform(test[num].values.reshape(-1,1))","4c71d60b":"def heatmap(train,target='next_day_ret',k=20) : \n  corrmat = train.corr()\n  cols = abs(corrmat).nlargest(k, target)[target].index.tolist()\n  if 'Month_mean' in cols : \n    cols.remove('Month_mean')\n  cm = np.corrcoef(train[cols].values.T)\n  plt.figure(figsize=(20,20))\n  hm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols, xticklabels=cols)\n  plt.show()\n  return cols\n\ncols = heatmap(train)","82fded41":"feature_selected = [ 'next_day_ret','currReturn','currReturn_square','TEMA','Aroon_current','CCI_current','currReturn_square','TEMA_square']\ntech = ['MACD', 'BB_Middle_Band', 'BB_Upper_Band','EMA','Momentum']\nother = [ '12_ema', 'mean_last_week', 'max_last_week', '26_ema']\nto_reduce = {'tech':tech , 'other':other}","c0af59d4":"def pca_function(train,test=[],to_reduce=to_reduce , deb=True ) : \n\n  pca_col = []\n  train_data = train.copy()\n  test_data = test.copy()\n  for to_red in to_reduce.keys() : \n    features = to_reduce[to_red]\n    df = train_data[features]\n    df_test = test_data[features]\n    for n_compent in range(len(features)-1) :\n        pca = PCA(n_components=n_compent)\n        pca.fit(df.values)\n        if sum(pca.explained_variance_ratio_)> 0.95 : \n          break \n    if deb : \n      print(f'{to_red} : number of PC is {n_compent} and explain {100*sum(pca.explained_variance_ratio_)} % of the total  Data variance')\n    components = pca.transform(df.values)\n    col = [to_red+'_pc_'+str(i) for i in range(n_compent)]\n    new_compontes = pd.DataFrame(data = components , columns=col  )\n    new_compontes['index'] = df.index\n    pca_col += col\n    new_compontes = new_compontes.set_index('index')\n    train_data[col] = new_compontes[col]\n\n    if len(test) != 0: \n      components = pca.transform(df_test.values)\n      new_compontes = pd.DataFrame(data = components , columns=col  )\n      new_compontes['index'] = df_test.index\n      new_compontes = new_compontes.set_index('index')\n      test_data[col] = new_compontes[col]\n    else : \n      test_data=-1\n  return train_data,test_data,pca_col","e300a245":"def factor_analysis_function(train,test=[],to_reduce=to_reduce ) : \n  factor_col = []\n  train_data = train.copy()\n  test_data = test.copy()\n  for to_red in to_reduce.keys() : \n    features = to_reduce[to_red]\n    df = train_data[features]\n    df_test = test_data[features]\n    n_compent = len(features)\/\/2 + 1 \n\n    factor = FactorAnalysis(n_components=n_compent, random_state=0)\n    factor = factor.fit(df.values)\n    components = factor.transform(df.values)\n    col = [to_red+'_factor_'+str(i) for i in range(n_compent)]\n    new_compontes = pd.DataFrame(data = components , columns=col  )\n    new_compontes['index'] = df.index\n    factor_col += col\n    new_compontes = new_compontes.set_index('index')\n    train_data[col] = new_compontes[col]\n    train_data.drop(features,1,inplace=True)\n\n    if len(test) != 0: \n      components = factor.transform(df_test.values)\n      new_compontes = pd.DataFrame(data = components , columns=col  )\n      new_compontes['index'] = df_test.index\n      new_compontes = new_compontes.set_index('index')\n      test_data[col] = new_compontes[col]\n      test_data.drop(features,1,inplace=True)\n    else : \n      test_data=-1\n  return train_data,test_data,factor_col","0232c4b1":"def simplelinear(train,valid): \n  \"this function create a linear Model that predict nextReturn\"\n  x_train = train.drop('nextReturn',1)\n  y_train = train['nextReturn']\n  x_valid = valid.drop('nextReturn',1)\n  y_valid = valid['nextReturn']\n  model = LinearRegression()\n  model.fit(x_train,y_train)\n  preds = model.predict(x_valid)\n  rms=np.sqrt(np.mean(np.power((np.array(y_valid)-np.array(preds)),2)))\n  return model , rms","cd6af489":"def simplelinear_next_day(train,valid): \n  \"this function create a linear Model that predict next_day_ret\"\n  x_train = train.drop('next_day_ret',1)\n  y_train = train['next_day_ret']\n  x_valid = valid.drop('next_day_ret',1)\n  y_valid = valid['next_day_ret']\n  model = LinearRegression()\n  model.fit(x_train,y_train)\n  preds = model.predict(x_valid)\n  rms=np.sqrt(np.mean(np.power((np.array(y_valid)-np.array(preds)),2)))\n  return model , rms","a5d454b4":"feature_selected = [ 'nextReturn','currReturn','currReturn_square','TEMA','TEMA_square'] # features selected to predict the nextReturn \ntech = ['MACD', 'BB_Middle_Band', 'BB_Upper_Band','EMA','Momentum','Aroon_current'] \nto_reduce = {'tech':tech}","8ead6728":"df_train,df_test,col=pca_function(train,test,to_reduce,deb=False) # dimensionality reduction phase ","e0cf6eda":"f = feature_selected+col","640faf14":"f = feature_selected+col\ntrain1 , valid = train_test_split(df_train[f],test_size=0.2,random_state=70)\nmodel,rms= simplelinear(train1,valid)\nprint(f'RMSE = {rms}')","6e2db2f8":"cofficents = dict()\nfor i in range(1,len(f)) : \n  cofficents[f[i]] = model.coef_[i-1]\ncofficents","1e341334":"folds = 5\ndf_train =df_train.reset_index(drop=True)\nkf = model_selection.KFold(n_splits=folds)\nvaild_rmse = np.zeros(folds)\nfor fold, (train_idx, val_idx) in tqdm(enumerate(kf.split(df_train)),total=folds):\n    train1 = df_train.iloc[train_idx].copy()\n    valid = df_train.iloc[val_idx].copy()\n    model,rms= simplelinear(train1[f],valid[f])\n    vaild_rmse[fold] = rms\n    preds = model.predict(df_test[f[1:]])\n    df_test.loc[:,f'fold_{fold}'] = preds\n    preds = model.predict(df_train[f[1:]])\n    df_train.loc[:,f'fold_{fold}'] = preds\nprint(f'RMSE_MEAN = {vaild_rmse.mean()} std = {vaild_rmse.std()}')","617fe0f7":"fold = [f'fold_{fold}' for fold in range(folds)]\ndf_test['NextReturn_pred'] = df_test[fold].mean(axis=1)\ndf_train['NextReturn_pred'] = df_train[fold].mean(axis=1)","adbe1648":"to_reduce = {'last_mean':last_mean}\ndf_train,df_test,col=pca_function(df_train,df_test,to_reduce,deb=False) # dimensionality reduction phase ","e196ca1c":"featues_selected_2 = ['next_day_ret','NextReturn_pred'] + col \ntrain1 , valid = train_test_split(df_train[featues_selected_2],test_size=0.2,random_state=70)\nmodel,rms= simplelinear_next_day(train1,valid)\nprint(f'RMSE = {rms}')","8383af20":"cofficents = dict()\nfor i in range(1,len(featues_selected_2)) : \n  cofficents[featues_selected_2[i]] = model.coef_[i-1]\ncofficents","182d6b4e":"folds = 5\ndf_train =df_train.reset_index(drop=True)\nkf = model_selection.KFold(n_splits=folds)\nvaild_rmse = np.zeros(folds)\nfor fold, (train_idx, val_idx) in tqdm(enumerate(kf.split(df_train)),total=folds):\n    train1 = df_train.iloc[train_idx].copy()\n    valid = df_train.iloc[val_idx].copy()\n    model,rms= simplelinear_next_day(train1[featues_selected_2],valid[featues_selected_2])\n    vaild_rmse[fold] = rms\n    preds = model.predict(df_test[featues_selected_2[1:]])\n    df_test.loc[:,f'fold_{fold}'] = preds\n    preds = model.predict(df_train[featues_selected_2[1:]])\n    df_train.loc[:,f'fold_{fold}'] = preds\n \nprint(f'RMSE_MEAN = {vaild_rmse.mean()} std = {vaild_rmse.std()}')","756bf852":"fold = [f'fold_{fold}' for fold in range(folds)]\ndf_test['next_day_ret'] = df_test[fold].mean(axis=1)\ndf_train['next_day_ret_pred'] = df_train[fold].mean(axis=1)","fc81191c":"sample = pd.read_csv('..\/input\/data-analysis-2020\/sample_submission.csv')\ntest_copy = df_test.set_index('Id')\nsample =sample.set_index('Id')\nsample['next_day_ret'] =test_copy['next_day_ret'] # Make a prediction with all companies model ","a514b13d":"company = df_train[df_train['company']=='ADWYA']\nY = df_train['next_day_ret'].iloc[150:250]\ny = df_train['next_day_ret_pred'].iloc[150:250]\nplt.figure(figsize=(10,10))\nplt.plot(Y, label = 'real')\nplt.plot(y, label = 'predicted')\nplt.legend()\nplt.title('next_day_ret')\nplt.xlabel('date')\nplt.ylabel('next_day_ret')\nplt.show()","36da7eec":"sector = 'SERVICES_FINANCIERS+ASSURANCES+BANQUES'\ntrain_sector = train[train['sector']==sector].copy()\ntest_sector = test[test['sector']==sector].copy()","f7b866d0":"feature_selected = [ 'nextReturn','TEMA','Aroon_current']\ntech = ['MACD', 'BB_Middle_Band', 'BB_Upper_Band','EMA','Momentum']+['currReturn','currReturn_square']\nto_reduce = {'tech':tech }","176eccd3":"train_sector,test_sector ,col = pca_function(train_sector,test_sector,to_reduce,deb=False)\nlen_sector = len(train_sector)\n\ndata = pd.concat([train_sector,test_sector],ignore_index=True)\ncompany = pd.get_dummies(data['company'].astype('str'))\ndata = pd.concat([data,company],axis=1)\n\ntrain_sector = data.iloc[:len_sector].copy()\ntest_sector = data.iloc[len_sector:].copy()","dcad6c3c":"corrmat = train_sector[sector_features+['nextReturn']].corr()\nselected = [sector+'_pc_'+str(i) for i in range(5)] # sector factors\nf = feature_selected + selected + col","004bf50d":"folds = 5\ntrain_sector =train_sector.reset_index(drop=True)\nkf = model_selection.KFold(n_splits=folds)\nvaild_rmse = np.zeros(folds)\nfor fold, (train_idx, val_idx) in tqdm(enumerate(kf.split(train_sector)),total=folds):\n    train1 = train_sector.iloc[train_idx].copy()\n    valid = train_sector.iloc[val_idx].copy()\n    model,rms= simplelinear(train1[f],valid[f])\n    vaild_rmse[fold] = rms\n    preds = model.predict(test_sector[f[1:]])\n    test_sector.loc[:,f'fold_{fold}'] = preds\n    preds = model.predict(train_sector[f[1:]])\n    train_sector.loc[:,f'fold_{fold}'] = preds\nprint(f'RMSE_MEAN = {vaild_rmse.mean()} std = {vaild_rmse.std()}')","a90a8e34":"fold = [f'fold_{fold}' for fold in range(folds)]\ntest_sector['NextReturn_pred'] = test_sector[fold].mean(axis=1)\ntrain_sector['NextReturn_pred'] = train_sector[fold].mean(axis=1)","a6ac3335":"featues_selected_2 = ['next_day_ret','NextReturn_pred','last']+last_mean","4375222e":"folds = 5\ntrain_sector =train_sector.reset_index(drop=True)\nkf = model_selection.KFold(n_splits=folds)\nvaild_rmse = np.zeros(folds)\nfor fold, (train_idx, val_idx) in tqdm(enumerate(kf.split(train_sector)),total=folds):\n    \n    train1 = train_sector.iloc[train_idx].copy()\n    valid = train_sector.iloc[val_idx].copy()\n    model,rms= simplelinear_next_day(train1[featues_selected_2],valid[featues_selected_2])\n    vaild_rmse[fold] = rms\n    preds = model.predict(test_sector[featues_selected_2[1:]])\n    test_sector.loc[:,f'fold_{fold}'] = preds\n \nprint(f'RMSE_MEAN = {vaild_rmse.mean()} std = {vaild_rmse.std()}')","a1cb94bb":"class LinearModelsectors() : \n  \n  def __init__(self,train,test, feature_selected ,  to_reduce) : \n  \n    self.train = train\n    self.test = test \n    self.feature_selected = feature_selected \n    self.to_reduce = to_reduce\n    self.subission_file='not_yet_genrated'\n  \n  def prep_data(self,train_sector,test_sector,to_reduce,sector) : \n    \n    train_sector,test_sector ,col = pca_function(train_sector,test_sector,to_reduce,deb=False)\n    len_sector = len(train_sector)\n\n    data = pd.concat([train_sector,test_sector],ignore_index=True)\n    company = pd.get_dummies(data['company'].astype('str'))\n    data = pd.concat([data,company],axis=1)\n\n    train_sector = data.iloc[:len_sector].copy()\n    test_sector = data.iloc[len_sector:].copy()\n    corrmat = train_sector[sector_features+['nextReturn']].corr()\n    selected = [sector+'_pc_'+str(i) for i in range(5)]\n\n    selected = selected+col\n    \n    return train_sector,test_sector,selected\n\n  def predict_one_sector(self,train_sector,test_sector,folds,sector) : \n   \n    train_sector , test_sector,selected = self.prep_data(train_sector,test_sector,self.to_reduce,sector)\n    train_sector =train_sector.reset_index(drop=True)\n    kf = model_selection.KFold(n_splits=folds)\n    vaild_rmse = np.zeros(folds)\n\n    f = self.feature_selected + selected\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_sector)):\n      train1 = train_sector.iloc[train_idx].copy()\n      valid = train_sector.iloc[val_idx].copy()\n      model,rms= simplelinear(train1[f],valid[f])\n      vaild_rmse[fold] = rms\n      preds = model.predict(test_sector[f[1:]])\n      test_sector.loc[:,f'fold_{fold}'] = preds\n      preds = model.predict(train_sector[f[1:]])\n      train_sector.loc[:,f'fold_{fold}'] = preds\n\n    fold = [f'fold_{fold}' for fold in range(folds)]\n    test_sector['NextReturn_pred'] = test_sector[fold].mean(axis=1)\n    train_sector['NextReturn_pred'] = train_sector[fold].mean(axis=1)\n    featues_selected_2 = ['next_day_ret','NextReturn_pred','last']+last_mean\n\n    train_sector =train_sector.reset_index(drop=True)\n    kf = model_selection.KFold(n_splits=folds)\n    vaild_rmse = np.zeros(folds)\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(train_sector)):\n        \n      train1 = train_sector.iloc[train_idx].copy()\n      valid = train_sector.iloc[val_idx].copy()\n      model,rms= simplelinear_next_day(train1[featues_selected_2],valid[featues_selected_2])\n      vaild_rmse[fold] = rms\n      preds = model.predict(test_sector[featues_selected_2[1:]])\n      test_sector.loc[:,f'fold_{fold}'] = preds\n\n    fold = [f'fold_{fold}' for fold in range(folds)]\n    df_test['next_day_ret'] = df_test[fold].mean(axis=1)\n\n    return df_test , vaild_rmse.mean() , vaild_rmse.std()\n  def kflod_predict(self,folds=5) : \n  \n    self.sector_rmse = dict()\n    self.sector_std = dict()\n    \n    sector_rmse = np.zeros(11)\n    index = 0  \n    test_copy = self.test.copy().set_index('Id')\n\n    for sector in tqdm(all_company.keys(),total=len(all_company.keys())) :\n\n      train_sector = self.train[self.train['sector']==sector].copy()\n      test_sector  = self.test[self.test['sector']==sector].copy()\n      test_sector , mean_sector , std_sector = self.predict_one_sector(train_sector,test_sector,folds,sector)\n      test_sector = test_sector.set_index('Id')\n      test_copy.loc[test_sector.index,'next_day_ret'] = test_sector['next_day_ret']\n      sector_rmse[index] = mean_sector\n      self.sector_rmse[sector] =mean_sector\n      self.sector_std[sector] = std_sector\n      index +=1 \n    \n      sample = pd.read_csv('..\/input\/data-analysis-2020\/sample_submission.csv')\n      sample.loc[:,'next_day_ret'] =test_copy['next_day_ret'].values\n      self.submission_file = sample\n    ","ed38da75":"feature_selected = [ 'nextReturn','TEMA','Aroon_current']\ntech = ['MACD', 'BB_Middle_Band', 'BB_Upper_Band','EMA','Momentum']+['currReturn','currReturn_square']\nto_reduce = {'tech':tech }","833d2b49":"sector_model = LinearModelsectors(train,test,feature_selected,to_reduce)\nsector_model.kflod_predict(5)","64007462":"sector_model.sector_rmse","782801e4":"sector_model.sector_std","83e47552":"sector_sub= sector_model.submission_file.set_index('Id')","80038df7":"sample.loc[sample[sample['next_day_ret']<-1000].index,'next_day_ret'] =-600 # remove outliers from prediction \nsector_sub.loc[sector_sub[sector_sub['next_day_ret']<-1000].index,'next_day_ret'] =-600# remove outliers from prediction ","b4496118":"sample['next_day_ret'] = sector_sub['next_day_ret']*0.3 + sample['next_day_ret']*0.7","50ce135d":"sample.to_csv('submission.csv') ","7903f217":"sample['next_day_ret'].hist()","bf6d7910":"in this part,i will create tow linear models the first model  predict the nextReturn that we have defined in the course and in the seconde model i will use the nextReturn predicted and some other varibales like time varibales and price varibales to predict our target. ","4e14a326":"In this section, I will use the boursa.tn data to cluster the companies into different sectors .","cd7b3dad":"the previous return is important to predict the current return .  In other words, the value at time t is greatly affected by the value at time t-1. The past values are known as lags, so t-1 is lag 1, t-2 is lag 2, and so on. ","b4c7a80e":"# **Data preprocessing**","ebc776e4":"### **Calculation of Bollinger Bands**","9466057a":"## **date features** ","1802a411":"### **Calculation of Aroon Oscillator**","14211a32":"# **Feature engineering**","8ca51050":"# Make a submission ","82e7cdf5":"we can see that our model can predict well the lower fluctuations. ","7fbac148":"we can see that our target is highly corrlated with the nextReturn .","e3e0dd0b":"Aftre a resdiual analysis , i will use a seconde degree polynomail of current return to predict nextReturn ","6de66fac":"Momentum is the measurement of the speed or velocity of price changes. The formula for momentum is: \n$$ M = V - V_x $$ \nwhere : \n\n* $ V = $  The lastest current return \n\n* $ V_x = $ The current return x number of day's ago ","7684d4b4":"* from this plot   we can identify a linear relationship between next next_day_ret and nextReturn so we can assume that : \n\n$$ nextdayret = W \\: NextReturn + f( other \\: variables  \\:?) $$ \n\n* $f$ seems  to be a linear function that depends on a categorical variable . \n\n* let's try to focus on the data when nextReturn is null and try to indentify $f$ ","1991f950":"we have defined in our course the NextReturn for stocks  as :\n$$NextReturn= \\frac{closePriceDay_{N+1}}{closePriceDay_N}\u22121$$ \nin this part , I will analyze and try to find the relationship between our  target (next_day_ret) and NextReturn .","183e97d2":"### **Calculation of Commodity Channel Index**\n","034bd04d":"## **Sectors Model** ","b96d6675":"# **Feature selection + Dimensionality reduction**","6074da0e":"An exponential moving average (EMA) is a type of moving average (MA) that places a greater weight and significance on the most recent data points. The Formula for EMA Is \n\n$$ EMA_{Today} = (currentReturn \\: \\frac{Smoothing}{1+Days} +EMA_{Yesterday} \\: (1 - \\frac{Smoothing}{1+Days} ) $$ \n\n\n\n\u200b\t","6c123cbd":"### **Lag featues + Moving avreage**","1f4a50ce":"# Sector  return factors  ","7ed251c5":"### **Momentum Indicators**","c45accb1":"## Current Return","d133f6cd":"The Aroon Oscillator is a trend-following indicator that uses aspects of the Aroon Indicator (Aroon Up and Aroon Down) to gauge the strength of a current trend and the likelihood that it will continue. Readings above zero indicate that an uptrend is present, while readings below zero indicate that a downtrend is present. Traders watch for zero line crossovers to signal potential trend changes. They also watch for big moves, above 50 or below -50 to signal strong price moves.The Formula for the Aroon Oscillator is\n\n\\begin{aligned} &\\text{Aroon Oscillator}=\\text{Aroon Up}-\\text{Aroon Down}\\\\ &\\text{Aroon Up}=100*\\frac{\\left(25 - \\text{Periods Since 25-daily return High}\\right)}{25}\\\\ &\\text{Aroon Down}=100*\\frac{\\left(25 - \\text{Periods Since 25-Period -daily return Low}\\right)}{25}\\\\ \\end{aligned} \n\u200b\t  \n\n\u200b\t \n\u200b\t","b58a0f19":"A Bollinger Band is a technical analysis tool defined by a set of trendlines plotted two standard deviations (positively and negatively) away from a simple moving average (SMA) of a security's cuurent return , but which can be adjusted to user preferences.\n\nHere is this Bollinger Band formula:\n\n$$BOLU=MA(TP,n)+m\u2217\u03c3[TP,n] $$\n$$BOLD=MA(TP,n)\u2212m\u2217\u03c3[TP,n] $$\nwhere:\n* $BOLU=$ Upper Bollinger Band\n* $BOLD=$ Lower Bollinger Band\n* $ MA=$ Moving average\n* $TP (typical price)= currentreturn $\n* $ n= $ Number of days in smoothing period \n* $ m= $ Number of standard deviations \n* $\u03c3[TP,n]=$ Standard Deviation over last n periods of TP\n\u200b\t","7cd63317":"this class creates a model for each sector with the same strategy demonstrated in the previous part.","f6c6c84a":"## **Traget**","7a06c0c1":"## sectors","990d68e5":"# **Importations** ","69464578":"### **Calculation of current return Triple Exponential Moving Average**\n","8478de81":"# **Load the Data**","6a3ce33f":"Ref : https:\/\/www.investopedia.com\/","2f01823c":"the variables that $f$ depends on most probably are related to time or a price varaibales . but since we haven't a higher correlation we can't assume anything.\n\n\nso we can conclude that \n$$ nextdayret = W \\: NextReturn + f_{linear}(x?(most \\: probably \\: categorical)) $$ \n\nobviously we can't use NextReurn to predict next_day_ret because to calculate NextReturn(t=N) we used close(t=N+1). but  we create a model that predict NextReturn or use the features that NextReturn depends on in a model that predict next_day_ret  \n","84ed9e7b":"The triple exponential moving average was designed to smooth current return fluctuations, thereby making it easier to identify trends without the lag associated with traditional moving averages (MA). It does this by taking multiple exponential moving averages (EMA) of the original EMA and subtracting out some of the lag.\n\nThe TEMA is used like other MAs. It can help identify trend direction, signal potential short-term trend changes or pullbacks, and provide support or resistance.Formula and Calculation for TEMA\n\n\n\\begin{aligned} &\\text{Triple Exponential Moving Average (TEMA)} \\\\ &\\;\\;\\;= \\left( 3*EMA_1\\right) - \\left( 3*EMA_2\\right) + EMA_3\\\\ &\\textbf{where:}\\\\ &EMA_1=\\text{Exponential Moving Average (EMA)}\\\\ &EMA_2=EMA\\;\\text{of}\\;EMA_1\\\\ &EMA_3=EMA\\;\\text{of}\\;EMA_2\\\\ \\end{aligned} ","962ed914":"let's try to visualize how well is our model  ","b58e7f0c":"## Model for all the companies","5fd3fed4":"After a lot of experiments and analysis, I have selected those features and I have decided to use PCA to reduce the dimensionality of some other technical indicators. to predict nextReturn.","eedcadab":"Ref :  https:\/\/www.investopedia.com\/","ca954323":"<font color='blue'><h1><center> Data Analysis - Final project<\/center> <\/h1><\/font>\n<h1><center> BY : Naim Houes<\/center> <\/h1>","77e519da":"k-Fold Cross-Validation ","27b89b57":"and know aftre that i have create the first model ,i will use the nextReturn predicted from the model and some other Moving avreage variables to predict our target ","f5f39478":"Ref :  https:\/\/www.investopedia.com\/","c64a8641":"## PCA","ba0dfb7a":"Moving Average Convergence Divergence (MACD) is a trend-following momentum indicator that shows the relationship between two moving averages of a security\u2019s current return. The MACD is calculated by subtracting the 26-period Exponential Moving Average (EMA) from the 12-period EMA.\n\nThe result of that calculation is the MACD line. A nine-day EMA of the MACD called the \"signal line,\" is then plotted on top of the MACD line, which can function as a trigger for buy and sell signals. Traders may buy the security when the MACD crosses above its signal line and sell - or short - the security when the MACD crosses below the signal line. Moving Average Convergence Divergence (MACD) indicators can be interpreted in several ways, but the more common methods are crossovers, divergences, and rapid rises\/falls.The Formula for MACD Is:\n\\begin{aligned}\\text{MACD}=\\text{12-Period EMA }-\\text{ 26-Period EMA}\\end{aligned}\n\n\n","ca80afe0":"# Modeling  ","24421dee":"since there are many companies in each sector and the nextReturn of an comapany depends on the other companies. decided to use the principle component analysis to create returns factors to each sector \n\n","c4a53494":"the second model that I have created is the sectors Model, I decided to use the return factors that I created with the PCA, so I have created a model for each sector and used the same strategy as the previous model but have added the sector factors features to predict nextReturn.   ","0067ffac":"Ref : https:\/\/www.investopedia.com\/","91fcc099":"###  **Calculation of MACD**\n","c23c5110":"## **Technical indicators**","ece2f6a8":"# Class Model "}}