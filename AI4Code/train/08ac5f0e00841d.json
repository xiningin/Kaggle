{"cell_type":{"0127bad6":"code","2adef9b1":"code","4b8034cd":"code","b1ab1ff5":"code","e63ca2fd":"code","d824ab3e":"code","aecb144d":"code","e5a4bb9c":"code","635be949":"code","e3328721":"code","fd20f3cc":"code","29a893bb":"code","1854212b":"code","13ac90c6":"code","8133ddc3":"code","c49d7ee7":"code","46896401":"code","6a92dd11":"code","c30c5df5":"code","232e5fa7":"code","fa646990":"code","20ac5caf":"code","c3b82e1b":"code","b7b05a46":"code","6685d2ab":"code","c26d5759":"code","3b6ae9d3":"code","d3d1617b":"code","d868eba8":"code","9a031562":"code","1ece5fbd":"code","e0fefbac":"code","adf046c7":"markdown","7b0ef758":"markdown","4770f491":"markdown","2c47329e":"markdown","36aa6da9":"markdown","c67259d6":"markdown","8676e2f5":"markdown","148fcdb4":"markdown","5afb3db1":"markdown","9405d6fa":"markdown","96a501c4":"markdown","7be723ab":"markdown","8c84875a":"markdown","5b123716":"markdown","b41b5f38":"markdown","c3b0b4bb":"markdown","0b12e880":"markdown","22ab43a5":"markdown","bef07719":"markdown","9327f61b":"markdown","b5fed237":"markdown"},"source":{"0127bad6":"!pip install segmentation-models-pytorch\n!pip install torchmetrics","2adef9b1":"import segmentation_models_pytorch as smp\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\nfrom torch.utils.data import Dataset,DataLoader,random_split\n\n#from torchmetrics import F1Score, JaccardIndex\n#from loss_functions import AsymLoss, DiceTopKLoss, ExpLog_loss\n#from class_balanced_loss import CB_loss\n\nfrom albumentations import (HorizontalFlip, VerticalFlip, \n                            ShiftScaleRotate, Normalize, Resize, \n                            Compose, GaussNoise)\nfrom albumentations.pytorch import ToTensorV2\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle,Patch\nfrom matplotlib.colors import to_rgb\n\nimport numpy as np\nimport cv2\nfrom PIL import Image\nimport os\nimport time\nimport gc\nimport json\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4b8034cd":"class config():\n    \n    #ENV\n    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(\"Device : \",DEVICE)\n    KAGGLE = True\n    DS_PATH = \"..\/input\/tiger-grand-challenge-data-dlv3plus-pre\" if KAGGLE else \".\/\"\n    WSIROI_PATH = os.path.join(DS_PATH,\"wsirois\")\n    WSIROI_TISSUE_CELLS_PATH = os.path.join(WSIROI_PATH,\"roi-level-annotations\/tissue-cells\/tissue-cells\")\n    print(WSIROI_TISSUE_CELLS_PATH)\n    \n    #DATA PREPROCESSING\n    N_CLASSES = 7\n    RESIZE_SHAPE=(128,128)\n    VAL_SPLIT = 0.2\n    \n    #LOSS AND OPTIMIZATION\n    LR = 0.005\n    ALPHA = 10.0\n    GAMMA = 2.0\n    \n    #TRAINING\n    BATCH_SIZE = 64\n    EPOCHS = 50\n    \n    PREDS_THRESHOLD = 0.9\nconfig()","b1ab1ff5":"def read_img(img_path,resize_shape=config.RESIZE_SHAPE, mono=False):\n    \n    img = cv2.imread(img_path)\n    \n    if mono:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    else:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n    if resize_shape:\n        img = cv2.resize(img, resize_shape, interpolation = cv2.INTER_NEAREST)\n        \n    return img\n\ndef print_dict(obj):\n    if type(obj) == list:\n        for i in obj:\n            print(\"=\"*12)\n            for key,value in i.items():\n                print(f\"{key} : {value}\")\n        print(\"=\"*12)\n    else:\n        print(\"=\"*12)\n        for key,value in obj.items():\n            print(f\"{key} : {value}\")\n        print(\"=\"*12)","e63ca2fd":"tissue_cells_dict = json.load(open(os.path.join(config.WSIROI_TISSUE_CELLS_PATH, \"tiger-coco.json\")))","d824ab3e":"print(\"N# Images :\",len(tissue_cells_dict['images']),\"\\n\")\nprint(\"Image N#343 : \\n\")\nprint_dict(dict(list(filter((lambda x: x['id'] == 453),tissue_cells_dict['images']))[0]))\nprint(\"\\nImage N#343 Annotations\/Masks : \\n\")\nprint_dict(list(filter((lambda x: x['image_id'] == 453),tissue_cells_dict['annotations'])))","aecb144d":"sample_tissue_cell_img = read_img(os.path.join(config.WSIROI_TISSUE_CELLS_PATH,list(filter((lambda x: x['id'] == 231),tissue_cells_dict['images']))[0]['file_name']))","e5a4bb9c":"plt.imshow(sample_tissue_cell_img)","635be949":"labels_name_enum = {\n    0: \"no_label\",\n    1: \"invasive_tumor\",\n    2: \"tumor_associated_stroma\",\n    3: \"in_situ_tumor\",\n    4: \"healthy_glands\",\n    5: \"necrosis_not_in_situ\",\n    6: \"inflamed_stroma\",\n    7: \"rest\"\n}\nlabels_colors_enum = {\n    0: \"black\",\n    1: \"red\",\n    2: \"blue\",\n    3: \"orange\",\n    4: \"green\",\n    5: \"fuchsia\",\n    6: \"lime\",\n    7: \"blueviolet\"\n}","e3328721":"def show_mask(mask_path,rand_color=False):\n    sample_mask=read_img(mask_path)\n    for label in np.unique(sample_mask):\n            \n        if rand_color:\n            color = np.random.randint(low=0, high=256, size=3)\n        else:\n            color = np.multiply(to_rgb(labels_colors_enum[label]),256).astype(np.uint8)\n            \n        mask = (sample_mask == list( (label,) * 3) ).all(axis=2)\n        sample_mask[ mask ] = color\n    plt.imshow(sample_mask)\n\nshow_mask(os.path.join(config.WSIROI_TISSUE_CELLS_PATH, \"masks\/100B_[10779, 11621, 12102, 12874].png\"))","fd20f3cc":"def convert_bbox_to_anchor_w_h(bbox):\n    x1, y1, x2, y2 = bbox\n    h = abs(y2 - y1)\n    w = abs(x2- x1)\n    anchor_y = np.min([y1, y2])\n    anchor_x = np.min([x1, x2])\n    return [anchor_x, anchor_y, w, h]\n\ndef show_bboxes(img_id,alpha=1):\n    \n    img_rel_path = list( filter( (lambda x: x['id'] == img_id), tissue_cells_dict['images'] ))[0]['file_name']\n    img_path = os.path.join(config.WSIROI_TISSUE_CELLS_PATH, img_rel_path)\n    image = read_img(img_path, resize_shape=None)\n    \n    annotations = list(filter((lambda x: x['image_id'] == img_id), tissue_cells_dict['annotations']))\n    \n    fig, ax = plt.subplots(1,1, figsize=(8,8))\n    ax.imshow(image)\n    \n    for annotation in annotations:\n        \n        label = annotation['category_id']\n        bbox = convert_bbox_to_anchor_w_h(annotation['bbox'])\n        \n        #print(f\"{labels_name_enum[label]} ({label}) at anchor : {bbox[0]}, {bbox[1]}\")\n        \n        patch = Rectangle((bbox[0],bbox[1]), bbox[2], bbox[3], alpha=alpha,\n                          color=labels_colors_enum[label], label=labels_name_enum[label],\n                          fill=False, linewidth = 6)\n        \n        ax.add_patch(patch)\n    \n    plt.legend(loc=\"best\")\n    plt.show()\n\nshow_bboxes(985)\nshow_bboxes(763)\nshow_bboxes(819)","29a893bb":"occurrence_list = [0 for i in range(8)]\nimages_paths = [img_dict['file_name'] for img_dict in tissue_cells_dict['images']]\nfor image_path in images_paths:\n    mask_path = os.path.join(config.WSIROI_TISSUE_CELLS_PATH, \"masks\",os.path.split(image_path)[-1])\n    mask = read_img(mask_path, resize_shape=None, mono=True)\n    for label in np.unique(mask):\n        occurrence_list[label] += 1\nprint(occurrence_list)","1854212b":"#def convert_bbox_to_shape(bbox, target_shape):","13ac90c6":"def preprocess_mask(mask):\n    out_channels = []\n    for c in range(1, config.N_CLASSES + 1):\n        channel_mask = (mask == c).astype(np.uint8)\n        out_channels.append(channel_mask)\n    return np.stack(out_channels, axis = 0)","8133ddc3":"class TissueCellsDataset(Dataset):\n    def __init__(self, transforms = None, norm_fn=None):\n        super(TissueCellsDataset, self).__init__()\n        \n        self.images_paths = [img_dict['file_name'] for img_dict in tissue_cells_dict['images']]\n        self.transforms = transforms\n        self.norm_fn = norm_fn\n        \n    def __len__(self):\n        return len(self.images_paths)\n    \n    def __getitem__(self, idx):\n        \n        image = read_img(os.path.join(config.WSIROI_TISSUE_CELLS_PATH, self.images_paths[idx]), resize_shape=config.RESIZE_SHAPE)\n        \n        mask_path = os.path.join(config.WSIROI_TISSUE_CELLS_PATH, \"masks\", os.path.split(self.images_paths[idx])[-1])\n        mask=read_img(mask_path, resize_shape=config.RESIZE_SHAPE, mono = True)\n        \n        \n        ##PREPROCESSING\n        if self.norm_fn:\n            image = norm_fn(image)\n        #image = preprocess_input(image)\n        mask = preprocess_mask(mask)\n        \n        if self.transforms:\n            aug = self.transforms(image=image, mask=mask)\n            image, mask = aug['image'], aug['mask']\n    \n        return image, mask\n        ","c49d7ee7":"tissue_cells_transforms = Compose([\n    #Resize(config.RESIZE_SHAPE[0], config.RESIZE_SHAPE[1]),\n    #Normalize(mean=)\n    VerticalFlip(p=0.5),\n    HorizontalFlip(p=0.5),\n    ToTensorV2()\n])\n\npreprocess_input=smp.encoders.get_preprocessing_fn('se_resnext101_32x4d', pretrained='imagenet')\n\ntissue_cells_ds = TissueCellsDataset(transforms=tissue_cells_transforms,norm_fn=None)\n\n# train val split\ntrain_len = round(len(tissue_cells_ds) - len(tissue_cells_ds) * config.VAL_SPLIT)\nval_len = len(tissue_cells_ds) - train_len\nprint(\"Train len : \",train_len)\nprint(\"Val len len : \",val_len)\ntissue_cells_train_ds, tissue_cells_val_ds = random_split(tissue_cells_ds,[train_len, val_len])\n\n\ntissue_cells_train_loader = DataLoader(tissue_cells_train_ds,\n                                       batch_size=config.BATCH_SIZE,shuffle=False)\n\ntissue_cells_val_loader = DataLoader(tissue_cells_val_ds,\n                                     batch_size=config.BATCH_SIZE,shuffle=False)","46896401":"sample_batch = next(iter(tissue_cells_train_loader))","6a92dd11":"print(\"imgs_batch: \",sample_batch[0].shape)\nprint(\"masks_batch: \",sample_batch[1].shape)","c30c5df5":"plt.imshow(sample_batch[0][1].detach().numpy().transpose(1,2,0).astype(np.uint8))","232e5fa7":"for n in range(17,25):\n    print(\"=\"*50+\"\\n\"+\"=\"*50)\n    print(\"IMAGE N# : \",n)\n    print(\"=\"*50+\"\\n\"+\"=\"*50)\n    fig, axs = plt.subplots(4, 2, figsize=(12,12))\n    z=0\n    for i in range(4):\n        for j in range(2):\n            if z == 7:\n                axs[-1, -1].imshow(np.zeros((128,128)))\n                axs[-1, -1].set_title(\"no_label\")\n                continue\n            axs[i, j].imshow(sample_batch[1][n][z])\n            axs[i, j].set_title(labels_name_enum[z+1])\n            z+=1\n    plt.subplots_adjust(hspace=0.5)\n    plt.show()\n    ","fa646990":"model = smp.DeepLabV3Plus(\n    encoder_name=\"se_resnext101_32x4d\",\n    encoder_weights=\"imagenet\",\n    in_channels=3,\n    classes=7,\n)\nmodel = model.float().to(config.DEVICE)\nmodel","20ac5caf":"class DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, pred, target, smooth=1):\n        \n        #pred = F.sigmoid(pred)       \n        \n        #flatten label and prediction tensors\n        pred = pred.view(-1)\n        target = target.view(-1)\n        \n        intersection = (pred * target).sum()                            \n        dice = (2.*intersection + smooth)\/(pred.sum() + target.sum() + smooth)  \n        \n        return 1 - dice\n    \nclass FocalLoss(nn.Module):\n    \n    def __init__(self, gamma):\n        super(FocalLoss, self).__init__()\n        self.gamma = gamma\n\n    def forward(self, pred, target):\n        if not (target.size() == pred.size()):\n            raise ValueError(\"Target size ({}) must be the same as input size ({})\"\n                             .format(target.size(), input.size()))\n        max_val = (-pred).clamp(min=0)\n        loss = pred - pred * target + max_val + \\\n            ((-max_val).exp() + (-pred - max_val).exp()).log()\n        invprobs = F.logsigmoid(-pred * (target * 2.0 - 1.0))\n        loss = (invprobs * self.gamma).exp() * loss\n        return loss.mean()\n\n\nclass DiceFocalLoss(nn.Module):\n    def __init__(self, alpha, gamma, weight=None, size_avrage=True):\n        super(DiceFocalLoss, self).__init__()\n        self.alpha = alpha\n        self.focal = FocalLoss(gamma)\n        self.dice = DiceLoss()\n\n    def forward(self, pred, target):\n        loss = self.alpha*self.focal(pred, target) - torch.log(self.dice(pred, target))\n        return loss.mean()\n\nclass FocalTverskyLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(FocalTverskyLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1, alpha=0.5, beta=0.5, gamma=1):\n        \n        #inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #True Positives, False Positives & False Negatives\n        TP = (inputs * targets).sum()    \n        FP = ((1-targets) * inputs).sum()\n        FN = (targets * (1-inputs)).sum()\n        \n        Tversky = (TP + smooth) \/ (TP + alpha*FP + beta*FN + smooth)  \n        FocalTversky = (1 - Tversky)**gamma\n                       \n        return FocalTversky","c3b82e1b":"def get_ins_weights(n_classes, n_classes_samples, batch_labels = None, power = 1):\n    w = 1 \/ np.array(np.power(n_classes_samples, power)) #invsqrt if power=0.5\n    w \/= (np.sum(w) * n_classes)\n    \n    #normalizing sample batch\n    if batch_labels:\n        batch_labels = batch_labels.numpy()\n        w = torch.tensor(w).float()\n        w = torch.tensor(np.array(w.unsqueeze(0).repeat(batch_labels.shape[0], 1) * batch_labels))\n        w = w.sum(1).unsqueeze(1)\n        return w.repeat(1, n_classes)\n    return w","b7b05a46":"loss_ins_weights = get_ins_weights(config.N_CLASSES, occurrence_list[1:])\nloss_ins_weights","6685d2ab":"norm_loss_weight = (sum(occurrence_list[1:]) \/ torch.tensor([occurrence_list[1:]])).squeeze(0)\nnorm_loss_weight \/= norm_loss_weight.sum()\nprint(norm_loss_weight.sum())\nnorm_loss_weight","c26d5759":"#metrics = [\n#    F1Score(num_classes=config.N_CLASSES),\n#    JaccardIndex(num_classes=config.N_CLASSES)\n#]\n#asym_loss = AsymLoss(batch_dice=True)\n#dicetopk_loss = DiceTopKLoss({\"batch_dice\": True}, {})\n#explog_loss = ExpLog_loss({\"batch_dice\": True}, {})\ndicefocal_loss = DiceFocalLoss(config.ALPHA, config.GAMMA)\nfocaltversky_loss = FocalTverskyLoss(weight = norm_loss_weight)\ncrossentroy_loss = nn.CrossEntropyLoss(weight = norm_loss_weight)\n\nloss_fn = crossentroy_loss\n\noptimizer = torch.optim.Adam(model.parameters(), config.LR)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3,verbose=True)","3b6ae9d3":"def print_metrics(metrics):\n    print(\"Metrics :\")\n    for metric in metrics:\n        print(f\"\\t{metric.__class__.__name__} : {metrics.compute():.2f}\")","d3d1617b":"train_losses=[]\nval_losses=[]\n\nstart = time.time()\n\nbest_val_loss=1e6\nfor n_epoch in range(1,config.EPOCHS + 1):\n    print(\"EPOCH : \"+str(n_epoch)+\"\/\"+str(config.EPOCHS))\n    \n    running_train_loss=0.0\n    running_val_loss=0.0\n    \n    ## TRAINING\n    model.train()\n    for train_batch_idx,train_batch in enumerate(tissue_cells_train_loader):\n        optimizer.zero_grad()\n\n        inputs,targets = train_batch\n        inputs,targets = inputs.float().to(config.DEVICE),targets.float().to(config.DEVICE)\n\n        preds=model(inputs)\n        loss = loss_fn(preds,targets)\n        loss.backward()\n\n        optimizer.step()\n        \n        running_train_loss+=loss.item()\n        train_losses.append(loss.item())\n            \n        gc.collect()\n        del train_batch,inputs,targets,preds\n        \n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        \n    ###VALIDATION\n    model.eval()\n    with torch.no_grad():\n        for val_batch_idx,val_batch in enumerate (tissue_cells_val_loader):\n            \n            inputs,targets = val_batch\n            inputs,targets=inputs.float().to(config.DEVICE),targets.float().to(config.DEVICE)\n            preds=model(inputs)\n            loss=loss_fn(preds,targets)\n            \n            running_val_loss+=loss.item()\n            val_losses.append(loss.item())\n            \n            #for metric in metrics:\n            #    metric(preds, targets.int())\n            \n            gc.collect()\n            del val_batch,inputs,targets,preds\n            \n    running_train_loss \/= train_batch_idx+1\n    running_val_loss \/= val_batch_idx+1\n    \n    scheduler.step(running_val_loss)\n    \n    print(f\"EPOCH : {n_epoch}\\tTrain Loss : {running_train_loss:.5f}, Val Loss : {running_val_loss:.5f}\")\n    #print_metrics(metrics)\n    \n    #for metric in metrics:\n        #metric.reset()\n    \n    if(running_val_loss < best_val_loss):\n        torch.save(model.state_dict(), \".\/DPLV3Plus_SE_RESNEXT101_best_model.pth\")\n        print(\"Model Saved\")\n        best_val_loss=running_val_loss\n        \ntime_elapsed = time.time() - start\nprint('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\nprint('Best Val Loss: {:4f}'.format(best_val_loss))","d868eba8":"plt.plot(train_losses)","9a031562":"plt.plot(val_losses)","1ece5fbd":"def one_hot_to_rgb(mask,colors_dict=labels_colors_enum,name_dict=labels_name_enum):\n    if type(mask) == torch.Tensor:\n        mask = mask.detach().numpy()\n    out = np.zeros(shape=(mask.shape[1], mask.shape[2], 3))\n    for channel in range(mask.shape[0]):\n        channel_mask = mask[channel,:,:].astype(bool)\n        #print(channel_mask.shape)\n        rgb_channel_mask = np.tile(channel_mask, (3,1,1) ).transpose(1,2,0)\n        #print(rgb_channel_mask.shape)\n        #print(np.unique(rgb_channel_mask[:,:,0] == rgb_channel_mask[:,:,1]), np.unique(rgb_channel_mask[:,:,1] == rgb_channel_mask[:,:,2]))\n        color = to_rgb(labels_colors_enum[channel+1])\n        color = np.multiply(color, np.array([256,256,256])).astype(int)\n        #print(color)\n        #print(out.shape)\n        out[np.all(rgb_channel_mask, axis=-1)] = color\n    return out\n\ndef get_legend_handles(x):\n    if type(x) == torch.Tensor:\n        x = x.detach().numpy()\n    legend_handles = []\n    for class_idx,c in enumerate(x):\n        if c.astype(bool).any():\n            legend_handles.append(Patch(color=labels_colors_enum[class_idx + 1],\n                                        label = labels_name_enum[class_idx + 1]))\n    return legend_handles\n\ndef apply_treshold(img,threshold):\n    img[img < threshold] = 0\n    img[img >= threshold] = 1\n    return img\n    ","e0fefbac":"fig, axs = plt.subplots(64,2,figsize=(16, 256))\n\nmask_alpha = 0.6\nmodel.load_state_dict(torch.load(\".\/DPLV3Plus_SE_RESNEXT101_best_model.pth\"))\nmodel.eval()\nfor inputs,targets in tissue_cells_train_loader:\n    inputs,targets=inputs.float().to(config.DEVICE),targets.float().to(config.DEVICE)\n    preds=model(inputs)\n    #print(preds[i].max(),preds[i].min(),preds[i].mean())\n    preds=apply_treshold(preds,config.PREDS_THRESHOLD)\n    \n    for i in range(len(inputs)):\n        rgb_preds, rgb_targets = one_hot_to_rgb(preds[i]), one_hot_to_rgb(targets[i])\n        \n        #########PREDS\n        axs[i, 0].imshow(inputs[i].detach().numpy().transpose(1,2,0).astype(np.uint8))\n        axs[i, 0].imshow(rgb_preds.astype(np.uint8),alpha=mask_alpha)\n        axs[i, 0].set_title(\"Prediction\")\n        \n        axs[i, 0].legend(handles=get_legend_handles(preds[i]))\n        ##########\n        \n        ##########GROUND TRUTH\n        axs[i, 1].imshow(inputs[i].detach().numpy().transpose(1,2,0).astype(np.uint8))\n        axs[i, 1].imshow(rgb_targets.astype(np.uint8),alpha=mask_alpha)\n        axs[i, 1].set_title(\"Ground Truth\")\n        \n        axs[i, 1].legend(handles=get_legend_handles(targets[i]))\n        ##########\n    break #only 1 batch\nplt.subplots_adjust(hspace=.4,wspace=.4)\nplt.show()","adf046c7":"#### Already trained 50 Epochs locally , best_val_crossentropy_loss = 0.06400","7b0ef758":"# ENV","4770f491":"# Model","2c47329e":"# Dataset","36aa6da9":"### Why HER2 and TNBC?\nThe clinical focus of the TIGER challenge is on Her2 positive and Triple Negative breast cancers (TNBC, negative to all receptors). Studies and clinical evidence show that Her2 positive and Triple Negative breast cancers are the ones with the worst prognosis, and therefore subject of a large corpus of research in prognostic and predictive biomarkers, aiming at improving patient management and prognosis.\n\n### Why the TILs?\nThe treatment of breast cancer in women and men is largely determined by the biology of the tumor. It is becoming more evident that a patient's immunity can be an important indicator of what treatment is needed and how their own immune system can significantly contribute to their chances of long-term survival. In recent years, the role of the tumor microenvironment (TME) has received increasing attention in the immuno-oncology scientific community, with a particular focus on the interaction between tumor cells and the host immune system. In particular, the role of tumor-infiltrating lymphocytes (TILs), immune cells that are part of a person's biology.  TILs are proving to be an important biomarker in cancer patients as they can play a part in killing tumor cells, particularly in some types of breast cancer. Identifying and measuring TILs can help to better target treatments, particularly immunotherapy, and may result in lower levels of other more aggressive treatments, including chemotherapy.","c67259d6":"TIGER is the first challenge on fully automated assessment of tumor-infiltrating lymphocytes (TILs) in H&E breast cancer slides. It is organized by the Diagnostic Image Analysis Group (DIAG) of the Radboud University Medical Center (Radboudumc) in Nijmegen (The Netherlands), in close collaboration with the  International Immuno-Oncology Biomarker working Group (www.tilsinbreastcancer.org).\n\nThe goal of this challenge is to evaluate new computer algorithms for the automated assessment of tumor-infiltrating lymphocytes (TILs) in Her2 positive and Triple Negative breast cancer (BC) histopathology slides. In recent years, several studies have shown the predictive and prognostic value of visually scored TILs in BC as well as in other cancer types, making TILs a powerful biomarker that can potentially be used in the clinic. With TIGER, we aim at developing computer algorithms that can automatically generate a \"TIL score\" with a high prognostic value.","8676e2f5":"### Inverse of Number of Samples (INS)\n\n## $W_{n,c} =  \\frac{1}{N\\_of\\_samples\\_in\\_class}$","148fcdb4":"### instead of a batch norm, i used a standard normalization for all samples,without accounting the specific varaince of classes occurrence inside a batch :","5afb3db1":"# Loss and Optimization","9405d6fa":"## Sample Weighting\n#### We are going to weight the loss function to account for the high class imbalanced in this dataset in order to weight the minority classes' losses more without using dumb methods such as oversampling the minority classes or undersampling the majority ones.","96a501c4":"``\ntraining\/\n\t|_wsibulk\/\n\t|\t|__annotations-tumor-bulk\/\t\t\t\t* manual annotations of \"tumor bulk\" regions (see https:\/\/tiger.grand-challenge.org\/Data\/ for details)\n\t|\t|\t|___masks\/\t\t\t\t\t\t\t* annotations in multiresolution TIF format\n\t|\t|\t|___xmls\/\t\t\t\t\t\t\t* annotations in XML format\n\t|\t|__images\/\t\t\t\t\t\t\t\t* whole-slide images\n\t|\t|__tissue-masks\/\t\t\t\t\t\t* tissue-background masks\n\t|\n\t|_wsirois\/\n\t|\t|__roi-level-annotations\/\t\t\t\t* manual annotations of tissue and cells on cropped reegions of interest\n\t|\t|\t|___tissue-bcss\/\t\t\t\t\t* manual annotations of tissue only in larger ROIs adapted from the BCSS project\n\t|\t|\t|\t|____images\/\t\t\t\t\t* images in PNG format of ROIs extracted from whole-slide images (TCGA data only)\n\t|\t|\t|\t|____masks\/\t\t\t\t\t\t* manual annotations of tissue in ROIs\n\t|\t|\t|___tissue-cells\/\t\t\t\t\t* manual annotations of tissue and cells ROIs adapted from the NuCLS+BCSS project and from RUMC and JB data\n\t|\t|\t\t|____images\/\t\t\t\t\t* images in PNG format of ROIs extracted from whole-slide images (TCGA + RUMC + JB)\n\t|\t|\t\t|____masks\/\t\t\t\t\t\t* manual annotations of tissue in ROIs\n\t|\t|\t\t|____tiger-coco.json\t\t\t* manual annotations of cells in ROIs as bounding boxes in COCO format\n\t|\t|__wsi-level-annotations\/\t\t\t\t* manual annotations of tissue and cells on whole-slide images\n\t|\t\t|___annotations-tissue-bcss-masks\/\t* manual annotations (in TIF format) of tissue only adapted from the BCSS project\n\t|\t\t|___annotations-tissue-bcss-xmls\/\t* manual annotations (in XML format) of tissue only adapted from the BCSS project\n\t|\t\t|___annotations-tissue-cells-masks\/\t* manual annotations (in TIF format) of tissue and cells from RUMC and JB + data adapted from the NuCLS+BCSS project \n\t|\t\t|___annotations-tissue-cells-xmls\/\t* manual annotations (in XML format) of tissue and cells from RUMC and JB + data adapted from the NuCLS+BCSS project \n\t|\t\t|___images\/\t\t\t\t\t\t\t* whole-slide images\n\t|\t\t|___tissue-masks\/\t\t\t\t\t* tissue-background masks\n\t|\n\t|_wsitils\/\t\t\t\t\t\t\t\t\t\n\t|\t|__images\/\t\t\t\t\t\t\t\t* whole-slide images\n\t|\t|__tissue-masks\/\t\t\t\t\t\t* tissue-background masks\n\t|\t|__tiger-tils-scores-wsitils.csv\t\t* CSV file containing TILs scores for each WSI\n\t|\n\t|_data-structure.txt\t\t\t\t\t\t* this file\n``","7be723ab":"# TRAINING","8c84875a":"### As we can see this dataset is kind of unbalanced with Invasive tumor (1) and Tumor-associated stroma (2) making up most of the dataset , a classic problem of Medical Imaging. We are thus going to use sample weighting techniques and compound loss functions since they are the most robust, especially for the highly imbalanced segmentation tasks.","5b123716":"# EDA","b41b5f38":"### Training Loss","c3b0b4bb":"# Data Preprocessing","0b12e880":"### Val Loss","22ab43a5":"# Tumor InfiltratinG lymphocytes in breast cancER (TIGER)","bef07719":"### The goal of the TIGER challenge\nThe goal of this challenge is two-fold. First, allow the scientific community to develop AI models for automated quantification of TILs in TNBC and Her2 positive (i.e, Her2 enriched and Luminal B) breast cancer. In this way, we aim at enabling the creation of open AI-based solutions that can automate TILs assessment. Second, validate the prognostic value of AI-based automated TILs scores using a large independent test dataset that includes cases from both clinical routine and from a phase 3 clinical trial, which is not directly accessible by participants. We envision the results of TIGER being the first step towards the introduction of automated TIL scoring in clinical practice.\n\n### The tasks in the TIGER challenge\nParticipants in the TIGER challenge will have to develop computer algorithms to analyze H&E-stained whole-slide images of breast cancer histopathology, to perform three tasks:\n\n* **Detection of TILs**, i.e., lymphocytes and plasma cells, which are the main types of cells considered as tumor-infiltrating lymphocytes;\n* **Segmentation of invasive tumor and tumor-associated stroma**, which are the main tissue compartments considered when identifying relevant regions for the TILs;\n* **Compute an automated TILs score**, one score per slide, based on the output of detection and segmentation.","9327f61b":"For now,we're going to work with:\n## WSIROIS: Whole-slide images with manual annotations in regions of interest\nIn this set, there are n=196 whole-slide images of breast cancer, both (core-needle) biopsies and surgical resections, with regions of interest (ROI) selected and manually annotated. This dataset contains images and annotations from multiple sources:\n\n* **TCGA**: n=151 WSIs of TNBC cases from the TGCA-BRCA archive (the original slides can also be downloaded from the GDC Data Portal). Annotations are extracted and adapted from the publicly available BCSS and NuCLS datasets.\n* **RUMC**: n=27 WSIs of TNBC and Her2+ cases from Radboud University Medical Center (Netherlands). Annotations were made by a panel of board-certified breast pathologists.\n* **JB**: n=18 WSIs of TNBC and Her2+ cases from Jules Bordet Institute (Belgium). Annotations were made by a panel of board-certified breast pathologists.\n\nIn each WSI, ROIs are manually annotated with both polygons indicating different tissue compartments, and with point annotations indicating lymphocytes and plasma cells.\n\n### In each ROI, the following regions are annotated, with the corresponding labels:\n\n* **Invasive tumor** (label=1): this class contains regions of the invasive tumor, including several morphological subtypes, such as invasive ductal carcinoma and invasive lobular carcinoma;\n* **Tumor-associated stroma** (label=2): this class contains regions of stroma (i.e., connective tissue) that are associated with the tumor. This means stromal regions contained within the main bulk of the tumor and in its close surrounding; in some cases, the tumor-associated stroma might resemble the \"healthy\" stroma, typically found outside of the tumor bulk; \n* **In-situ tumor** (label=3): this class contains regions of in-situ malignant lesions, such as ductal carcinoma in situ (DCIS) or lobular carcinoma in situ (LCIS).\n* **Healthy glands** (label=4): this class contains regions of glands with healthy epithelial cells;\n* **Necrosis not in-situ** (label=5): this class contains regions of necrotic tissue that are not part of in-situ tumor; for example, ductal carcinoma in situ (DCIS) often presents a typical necrotic pattern, which can be considered as part of the lesion itself, such a necrotic region is not annotated as \"necrosis\" but as \"in-situ tumor\";\n* **inflamed stroma** (label=6): this class contains tumor-associated stroma that has a high density of lymphocytes (i.e., it is \"inflamed\"). When it comes to assessing the TILs, inflamed stroma and tumor-associated stroma can be considered together, but were annotated separately to take into account for differences in their visual patterns;\n* **rest** (label=7): this class contains regions of several tissue compartments that are not specifically annotated in the other categories; examples are healthy stroma, erythrocytes, adipose tissue, skin, nipple, etc.\n\nAdditionally, most ROIs contain annotations of lymphocytes and plasma cells in the form of bounding boxes. Cells were annotated using point annotations and then squared bounding boxes of 8x8 microns were constructed centered on the point annotation. This fixed-size bounding box size is inspired by previous work on lymphocyte detection [3], where an average equivalent diameter of 8 microns was used for lymphocytes.\n\nAnnotations at WSI level are released in two formats, both as a multiresolution TIF image and as an XML file in ASAP format.","b5fed237":"![](https:\/\/i.imgur.com\/MSj1qpc.png)"}}