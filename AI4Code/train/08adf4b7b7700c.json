{"cell_type":{"0978dd39":"code","2acfe3d5":"code","dac2579a":"code","624949aa":"code","502c92b9":"code","20526c39":"code","9775e1bd":"code","b023e7f6":"code","008b4b13":"code","9595f789":"code","8f837aad":"code","dc0eb40b":"code","90ea6886":"code","2d8c196d":"code","bebec000":"code","3436a441":"code","7471ca10":"code","3c2cf61c":"code","30a22c25":"code","c3608d40":"markdown","83b3c1c7":"markdown","ba531258":"markdown","896b2b8e":"markdown"},"source":{"0978dd39":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","2acfe3d5":"data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","dac2579a":"pd.value_counts(data['Class']).plot.bar()\nplt.title('Fraud class histogram')\nplt.xlabel('Class')\nplt.ylabel('Frequency')\ndata['Class'].value_counts()\nplt.show()\n","624949aa":"print('No Frauds = ', data['Class'].value_counts()[0] , ' transactions' ,'--- with a percentage ' ,\n      round(data['Class'].value_counts()[0]\/len(data) * 100,2), '% of the dataset')\nprint('Frauds = ' ,data['Class'].value_counts()[1] , ' transactions' ,' --- with percentage  ', \n      round(data['Class'].value_counts()[1]\/len(data) * 100,2), '% of the dataset')","502c92b9":"from sklearn.preprocessing import StandardScaler\n\nstd_scaler = StandardScaler()\n\ndata['scaled_amount'] = std_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\ndata['scaled_time'] = std_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n\ndata.drop(['Time','Amount'], axis=1, inplace=True)\nscaled_amount = data['scaled_amount']\nscaled_time = data['scaled_time']\n\ndata.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndata.insert(0, 'scaled_amount', scaled_amount)\ndata.insert(1, 'scaled_time', scaled_time)\n","20526c39":"data.head()\n","9775e1bd":"X = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values","b023e7f6":"from imblearn.under_sampling import ClusterCentroids\nfrom collections import Counter\n\ncc = ClusterCentroids(random_state=0)\nX_resampled, y_resampled = cc.fit_resample(X, y)\nprint(sorted(Counter(y_resampled).items()))","008b4b13":"data.columns.values","9595f789":"balanced_data = pd.DataFrame(X_resampled,columns = ['scaled_amount', 'scaled_time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6',\n       'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16',\n       'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25',\n       'V26', 'V27', 'V28'])\nbalanced_data['Class'] = y_resampled\nbalanced_data.describe()","8f837aad":"f, axes = plt.subplots( nrows=1 , ncols= 3, figsize=(20,4))\n\n\nsns.boxplot(x=\"Class\", y=\"V11\", data=balanced_data, palette=\"Set3\", ax=axes[0])\naxes[0].set_title('V11 vs Class positive Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=balanced_data, palette=\"Set3\", ax=axes[1])\naxes[1].set_title('V4 vs Class positive Correlation')\n\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=balanced_data, palette=\"Set3\", ax=axes[2])\naxes[2].set_title('V2 vs Class positive Correlation')\nplt.show()","dc0eb40b":"# Removing outliers V11 Feature\nV11_fraud = balanced_data['V11'].values\nq25, q75 = np.percentile(V11_fraud, 25), np.percentile(V11_fraud, 75)\n\nV11_iqr = q75 - q25\nV11_cut_off = V11_iqr * 1.5\nV11_lower, V11_upper = q25 - V11_cut_off, q75 + V11_cut_off\n\nprint('V11 Lower: ', V11_lower)\nprint('V11 Upper: ', V11_upper)\noutliers = [x for x in V11_fraud if x < V11_lower or x > V11_upper]\n\nprint('V11 outliers:' , outliers)\nprint('Feature V11 Outliers for Fraud Cases:' , len(outliers) )\n\nbalanced_data = balanced_data.drop(balanced_data[(balanced_data['V11'] > V11_upper) | (balanced_data['V11'] < V11_lower)].index)\nprint('Number of Instances after outliers removal: ', len(balanced_data))","90ea6886":"# Removing outliers V4 Feature\nV4_fraud = balanced_data['V4'].values\nq25, q75 = np.percentile(V4_fraud, 25), np.percentile(V4_fraud, 75)\n\nV4_iqr = q75 - q25\nV4_cut_off = V4_iqr * 1.5\nV4_lower, V4_upper = q25 - V4_cut_off, q75 + V4_cut_off\n\nprint('V4 Lower: ', V4_lower)\nprint('V4 Upper: ', V4_upper)\noutliers = [x for x in V4_fraud if x < V4_lower or x > V4_upper]\n\nprint('V4 outliers:' , outliers)\nprint('Feature V4 Outliers for Fraud Cases:' , len(outliers) )\n\nbalanced_data = balanced_data.drop(balanced_data[(balanced_data['V4'] > V4_upper) | (balanced_data['V4'] < V4_lower)].index)\nprint('Number of Instances after outliers removal: ', len(balanced_data))\n","2d8c196d":"# Removing outliers V2 Feature\n### in this feature i have increased the cut off factor to 2.5 becasue we many outliers in this feature , if we removed all of them we might have the risk of losing too much information ###\nV2_fraud = balanced_data['V2'].values\nq25, q75 = np.percentile(V2_fraud, 25), np.percentile(V2_fraud, 75)\n\nV2_iqr = q75 - q25\nV2_cut_off = V2_iqr * 2.5\nV2_lower, V2_upper = q25 - V2_cut_off, q75 + V2_cut_off\n\nprint('V2 Lower: ', V2_lower)\nprint('V2 Upper: ', V2_upper)\noutliers = [x for x in V2_fraud if x < V2_lower or x > V2_upper]\n\nprint('V2 outliers:' , outliers)\nprint('Feature V2 Outliers for Fraud Cases:' , len(outliers) )\n\nbalanced_data = balanced_data.drop(balanced_data[(balanced_data['V2'] > V2_upper) | (balanced_data['V2'] < V2_lower)].index)\nprint('Number of Instances after outliers removal: ', len(balanced_data))\n","bebec000":"print('No Frauds = ', balanced_data['Class'].value_counts()[0] , ' transactions' ,'--- with a percentage ' ,\n      round(balanced_data['Class'].value_counts()[0]\/len(balanced_data) * 100,2), '% of the dataset')\nprint('Frauds = ' ,balanced_data['Class'].value_counts()[1] , ' transactions' ,' --- with percentage  ', \n      round(balanced_data['Class'].value_counts()[1]\/len(balanced_data) * 100,2), '% of the dataset')\n","3436a441":"X = balanced_data.iloc[:, :-1].values\ny = balanced_data.iloc[:, -1].values","7471ca10":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","3c2cf61c":"from xgboost import XGBClassifier\nclassifier = XGBClassifier(max_depth = 5 , gamma = 0.2 ,colsample_bytree = 0.8,\n                           subsample = 0.8,objective= 'binary:logistic')\nclassifier.fit(X_train, y_train)","30a22c25":"from sklearn.metrics import confusion_matrix, accuracy_score , classification_report\n\ny_pred = classifier.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\naccuracy_score(y_test, y_pred)\nprint(classification_report(y_test, y_pred))\n\n\ny_actual = [\"non fraud\",\" fraud\"]\ny_predicted = [\"non fraud\",\" fraud\"]\n\ndf_cm = pd.DataFrame(cm, columns=(y_predicted), index = (y_actual))\ndf_cm.index.name = 'Actual'\ndf_cm.columns.name = 'Predicted'\n\nsns.heatmap(df_cm,linewidths=3, cmap=\"Set3\", annot=True)\nplt.show()","c3608d40":"![image.png](attachment:image.png)","83b3c1c7":"# Exploring the data\u00b6\n","ba531258":"# Scaling","896b2b8e":"# - features distributions are gaussian like , with some outliers specially for feature V2\n# - using interquartile range method to remove extreme outliers.\n# - you can see illustration below\n"}}