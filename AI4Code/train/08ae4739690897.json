{"cell_type":{"75bc5349":"code","16b38345":"code","c285557f":"code","c91b0a79":"code","40892f7a":"code","6cfe37d3":"markdown","d571521d":"markdown","2565ba5b":"markdown","7377a9ed":"markdown","362bc264":"markdown","90de24e8":"markdown","0f4bb3e4":"markdown","da1a00e7":"markdown","356544a3":"markdown"},"source":{"75bc5349":"import os, sys\nimport time, math\nimport argparse, random\nfrom math import exp\nimport numpy as np\n\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nimport torch.utils.data as data\nfrom torch.utils.data import DataLoader\nfrom torch.backends import cudnn\nfrom torch.autograd import Variable\n\nimport torchvision\nimport torchvision.transforms as tfs\nfrom torchvision.transforms import ToPILImage\nfrom torchvision.transforms import functional as FF\nimport torchvision.utils as vutils\nfrom torchvision.utils import make_grid\nfrom torchvision.models import vgg16\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","16b38345":"# Device name\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n# Num residual_groups\ngps = 3\n# Num residual_blocks\nblocks = 19\n# Directory of test imgs\nimg_dir = '..\/input\/synthetic-objective-testing-set-sots-reside\/indoor\/hazy\/'\n# Pre-trained checkpoint dir\npretrained_model_dir = '..\/input\/ffanet-pretrained-weights\/' + f'its_train_ffa_{gps}_{blocks}.pk'\n# Output dir to save predicted de-hazed images\noutput_dir = f'pred_FFA_its\/'\n\nif not os.path.exists(output_dir):\n    os.mkdir(output_dir)","c285557f":"def tensorShow(tensors,titles=None):\n    '''t:BCWH'''\n    fig=plt.figure()\n    for tensor, title, i in zip(tensors, titles, range(len(tensors))):\n        img = make_grid(tensor)\n        npimg = img.numpy()\n        ax = fig.add_subplot(211+i)\n        ax.imshow(np.transpose(npimg, (1, 2, 0)))\n        ax.set_title(title)\n    plt.show()","c91b0a79":"def default_conv(in_channels, out_channels, kernel_size, bias=True):\n    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size\/\/2), bias=bias)\n    \n    \nclass PALayer(nn.Module):\n    def __init__(self, channel):\n        super(PALayer, self).__init__()\n        self.pa = nn.Sequential(\n                nn.Conv2d(channel, channel \/\/ 8, 1, padding=0, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channel \/\/ 8, 1, 1, padding=0, bias=True),\n                nn.Sigmoid()\n        )\n    def forward(self, x):\n        y = self.pa(x)\n        return x * y\n\n    \nclass CALayer(nn.Module):\n    def __init__(self, channel):\n        super(CALayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.ca = nn.Sequential(\n                nn.Conv2d(channel, channel \/\/ 8, 1, padding=0, bias=True),\n                nn.ReLU(inplace=True),\n                nn.Conv2d(channel \/\/ 8, channel, 1, padding=0, bias=True),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        y = self.avg_pool(x)\n        y = self.ca(y)\n        return x * y\n\n    \nclass Block(nn.Module):\n    def __init__(self, conv, dim, kernel_size,):\n        super(Block, self).__init__()\n        self.conv1 = conv(dim, dim, kernel_size, bias=True)\n        self.act1 = nn.ReLU(inplace=True)\n        self.conv2 = conv(dim, dim, kernel_size, bias=True)\n        self.calayer = CALayer(dim)\n        self.palayer = PALayer(dim)\n\n    def forward(self, x):\n        res = self.act1(self.conv1(x))\n        res = res+x \n        res = self.conv2(res)\n        res = self.calayer(res)\n        res = self.palayer(res)\n        res += x \n        return res\n\n    \nclass Group(nn.Module):\n    def __init__(self, conv, dim, kernel_size, blocks):\n        super(Group, self).__init__()\n        modules = [Block(conv, dim, kernel_size)  for _ in range(blocks)]\n        modules.append(conv(dim, dim, kernel_size))\n        self.gp = nn.Sequential(*modules)\n\n    def forward(self, x):\n        res = self.gp(x)\n        res += x\n        return res\n\n    \nclass FFA(nn.Module):\n    def __init__(self,gps,blocks,conv=default_conv):\n        super(FFA, self).__init__()\n        self.gps = gps\n        self.dim = 64\n        kernel_size = 3\n        pre_process = [conv(3, self.dim, kernel_size)]\n        assert self.gps==3\n        self.g1 = Group(conv, self.dim, kernel_size,blocks=blocks)\n        self.g2 = Group(conv, self.dim, kernel_size,blocks=blocks)\n        self.g3 = Group(conv, self.dim, kernel_size,blocks=blocks)\n        self.ca = nn.Sequential(*[\n            nn.AdaptiveAvgPool2d(1),\n            nn.Conv2d(self.dim*self.gps,self.dim\/\/16,1,padding=0),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(self.dim\/\/16, self.dim*self.gps, 1, padding=0, bias=True),\n            nn.Sigmoid()\n            ])\n        self.palayer = PALayer(self.dim)\n\n        post_process = [\n            conv(self.dim, self.dim, kernel_size),\n            conv(self.dim, 3, kernel_size)]\n\n        self.pre = nn.Sequential(*pre_process)\n        self.post = nn.Sequential(*post_process)\n\n    def forward(self, x1):\n        x = self.pre(x1)\n        res1 = self.g1(x)\n        res2 = self.g2(res1)\n        res3 = self.g3(res2)\n        w = self.ca(torch.cat([res1,res2,res3],dim=1))\n        w = w.view(-1,self.gps, self.dim)[:,:,:,None,None]\n        out = w[:,0,::] * res1 + w[:,1,::] * res2+w[:,2,::] * res3\n        out = self.palayer(out)\n        x = self.post(out)\n        return x + x1","40892f7a":"ckp = torch.load(pretrained_model_dir, map_location=device)\nnet = FFA(gps=gps, blocks=blocks)\nnet = nn.DataParallel(net)\nnet.load_state_dict(ckp['model'])\nnet.eval()\n\nimg_paths = sorted(os.listdir(img_dir))\nimg_paths = [img_path for img_path in img_paths if '_9.' in img_path]\n\nfor im in img_paths:\n    haze = Image.open(img_dir+im)\n    haze1 = tfs.Compose([\n        tfs.ToTensor(),\n        tfs.Normalize(mean=[0.64, 0.6, 0.58],std=[0.14,0.15, 0.152])\n    ])(haze)[None,::]\n    haze_no = tfs.ToTensor()(haze)[None,::]\n    with torch.no_grad():\n        pred = net(haze1)\n    ts = torch.squeeze(pred.clamp(0,1).cpu())\n    # tensorShow([haze_no, pred.clamp(0,1).cpu()],['haze', 'pred'])\n    \n    haze_no = make_grid(haze_no, nrow=1, normalize=True)\n    ts = make_grid(ts, nrow=1, normalize=True)\n    image_grid = torch.cat((haze_no, ts), -1)\n    vutils.save_image(image_grid, output_dir+im.split('.')[0]+'_FFA.png')","6cfe37d3":"## Acknowledgements\n\n### This work was inspired by and borrows code from the authors' [original FFA-Net implementation](https:\/\/github.com\/zhilin007\/FFA-Net).","d571521d":"### Libraries \ud83d\udcda\u2b07","2565ba5b":"### Model Definition","7377a9ed":"### Utility Functions","362bc264":"<h3><center>Indoor Image Dehazing with FFA-Net<\/center><\/h3>\n<img src=\"https:\/\/storage.googleapis.com\/groundai-web-prod\/media%2Fusers%2Fuser_297673%2Fproject_398618%2Fimages%2Ffig1.jpg\" width=\"500\" height=\"500\"\/>\n<h4><\/h4>\n<h4><center><a href=\"https:\/\/arxiv.org\/abs\/1911.07559v2\">Source: FFA-Net [Xu Qin & Zhilin Wang et. al.]<\/a><\/center><\/h4>","90de24e8":"### Test FFA-Net","0f4bb3e4":"## Introduction\n\n### In this notebook we use pre-trained [Feature Fusion Attention Network (FFA-Net)](https:\/\/arxiv.org\/abs\/1911.07559v2) to perform Indoor Image Dehazing on [Synthetic Objective Testing Set [RESIDE-Standard Dataset]](https:\/\/sites.google.com\/view\/reside-dehaze-datasets\/reside-standard?authuser=0).","da1a00e7":"<h3><center>FFA-Net Model Architecture<\/center><\/h3>\n<img src=\"https:\/\/storage.googleapis.com\/groundai-web-prod\/media%2Fusers%2Fuser_297673%2Fproject_398618%2Fimages%2Ffig2.jpg\" width=\"750\" height=\"750\"\/>\n<h4><\/h4>\n<h4><center><a href=\"https:\/\/arxiv.org\/abs\/1911.07559v2\">Source: FFA-Net [Xu Qin & Zhilin Wang et. al.]<\/a><\/center><\/h4>","356544a3":"### Settings \u2699\ufe0f"}}