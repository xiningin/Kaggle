{"cell_type":{"feb3da8e":"code","eb2259a1":"code","23fa9e51":"markdown"},"source":{"feb3da8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","eb2259a1":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Load the data in. Remember to drop the ID column or put it as the index\ntrain_data = pd.read_csv(\"\/kaggle\/input\/learn-together\/train.csv\", index_col='Id')\ntest_data = pd.read_csv(\"\/kaggle\/input\/learn-together\/test.csv\", index_col='Id')\n\nX_train = train_data.drop('Cover_Type', axis='columns')\ny_train = train_data['Cover_Type']\n\n# This just puts the model into a pipeline. Useful for when things get more hairy later.\nrf = RandomForestClassifier()\nmodel = Pipeline(steps=[('model',rf),])\n\n#====== IMPORTANT =========\n# I left the commented lines running for a few hours this morning, and then used the best parameters\n# from it.\n\n# In this notebook, I just use the parameters from this random search that gave me the best results\n#param_grid = {'model__n_estimators': np.logspace(2,3.5,8).astype(int),\n#              'model__max_features': [0.1,0.3,0.5,0.7,0.9],\n#              'model__max_depth': np.logspace(0,3,10).astype(int),\n#              'model__min_samples_split': [2, 5, 10],\n#              'model__min_samples_leaf': [1, 2, 4],\n#              'model__bootstrap':[True, False]}\n\n# Here, I just manually put in the optimal parameters found above.\n\nparam_grid = {'model__n_estimators': [719],\n              'model__max_features': [0.3],\n              'model__max_depth': [464],\n              'model__min_samples_split': [2],\n              'model__min_samples_leaf': [1],\n              'model__bootstrap':[False]}\n\n\ngrid = RandomizedSearchCV(estimator=model, \n                          param_distributions=param_grid, \n                          n_iter=1, # This was set to 100 in my offline version\n                          cv=3, \n                          verbose=3, \n                          n_jobs=1,\n                          scoring = {'NLL':'neg_log_loss', 'Accuracy':'accuracy'}, \n                          refit='NLL')\n\n# fit, predict, and output!\ngrid.fit(X_train, y_train)\npreds = grid.predict(test_data)\n\noutput = pd.DataFrame({'Id': test_data.index,\n                       'Cover_Type': preds})\noutput.to_csv('submission.csv', index=False)\n","23fa9e51":"Nothing special or any crazy techniques here. \n\nJust a random forest which I left randomly searching parameter space in the morning!\n\nBest parameters detailed below."}}