{"cell_type":{"66375370":"code","4f8a6cd1":"code","751b6563":"code","e174060e":"code","0b5aec73":"code","5785327d":"code","4cfc3d05":"code","df0acb2e":"code","d94882fc":"code","1331361f":"code","cd31850a":"code","206f91db":"code","a5905e9c":"code","ec444ab5":"code","570d8dbb":"code","6849606c":"code","d9d060fe":"code","a55b73fe":"code","92351bfd":"code","13d2c3ef":"code","6f452cb2":"code","40db7abe":"code","08635da1":"code","80bbbda6":"code","24bffce8":"code","3901425d":"code","e544fe51":"code","55816bef":"code","f124997c":"code","3c604527":"code","0c364446":"code","7ab5dbef":"code","659885a8":"code","338f3c28":"code","d001f2b3":"code","25178709":"code","3a2c4b3b":"code","337f4cc4":"code","8d3a5aad":"code","1c933214":"code","ac2e3ad4":"code","b8bcacbe":"code","c26984ab":"code","7abe7a68":"code","5e5208a6":"code","4c8a4d78":"code","47609495":"code","3ccfaf80":"code","7c3f841f":"code","5124129a":"code","60baa3ca":"code","d52ebd5c":"code","25e1d14a":"code","76e285b5":"code","0fef76f4":"code","a8c0ef72":"code","36ca34be":"code","1330cedb":"code","d42d2bd9":"code","0a2ed96f":"code","65078d1a":"code","49fa3730":"code","7dbe3b1e":"code","f1392bc3":"code","5aeac00b":"code","5cec0f20":"code","4a257988":"code","230943cf":"code","dd12d83d":"code","01883f6a":"code","f952bda8":"code","b3f504e5":"code","b4ca510c":"markdown"},"source":{"66375370":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","4f8a6cd1":"train  = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")","751b6563":"print(\"Train : \",train.shape)\nprint(\"*\"*10)\nprint(\"Test : \",test.shape)","e174060e":"train.head(10)\n#train.tail(10)","0b5aec73":"train = train.drop_duplicates().reset_index(drop=True)","5785327d":"sns.countplot(x= \"target\",data=train)","4cfc3d05":"train.target.value_counts()","df0acb2e":"train.isnull().sum()","d94882fc":"test.isnull().sum()","1331361f":"print(train.keyword.nunique(), test.keyword.nunique())","cd31850a":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"keyword\",\n              data=train,\n              order=train.keyword.value_counts().iloc[:15].index\n             )\nplt.title('Top Keywords')\nplt.show()","206f91db":"dist = train[train.target==1].keyword.value_counts().head()\n#dist\nplt.figure(figsize=(9,6))\nsns.barplot(dist,dist.index)\nplt.show()","a5905e9c":"nondist = train[train.target==0].keyword.value_counts().head()\n#nondist\nplt.figure(figsize=(9,6))\nsns.barplot(nondist,nondist.index)\nplt.show()","ec444ab5":"distribution_dist = train.groupby('keyword').mean()['target'].sort_values(ascending=False).head(10)\n#distribution_dist\n\nplt.figure(figsize=(9,6))\nsns.barplot(distribution_dist,distribution_dist.index)\nplt.title(\"Distribution of keywords for higher risk\")\nplt.show()","570d8dbb":"distribution_nondist = train.groupby('keyword').mean()['target'].sort_values().head(10)\n#distribution_nondist\nplt.figure(figsize=(9,6))\nsns.barplot(distribution_nondist,distribution_nondist.index)\nplt.title(\"Distribution of Non Disasters keywords for lower risk\")\nplt.show()","6849606c":"print (train.location.nunique(), test.location.nunique())","d9d060fe":"plt.figure(figsize=(9,6))\nsns.countplot(y=train.location, order = train.location.value_counts().iloc[:15].index)\nplt.title('Top locations')\nplt.show()","a55b73fe":"for clmn in ['keyword','location']:\n    train[clmn]= train[clmn].fillna('None')\n    test[clmn]= test[clmn].fillna('None')","92351bfd":"def setlocname(x):\n    if x == 'None':\n        return 'None'\n    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n        return 'World'\n    elif 'New York' in x or 'NYC' in x:\n        return 'New York'    \n    elif 'London' in x:\n        return 'London'\n    elif 'Mumbai' in x:\n        return 'Mumbai'\n    elif 'Washington' in x and 'D' in x and 'C' in x:\n        return 'Washington DC'\n    elif 'San Francisco' in x:\n        return 'San Francisco'\n    elif 'Los Angeles' in x:\n        return 'Los Angeles'\n    elif 'Seattle' in x:\n        return 'Seattle'\n    elif 'Chicago' in x:\n        return 'Chicago'\n    elif 'Toronto' in x:\n        return 'Toronto'\n    elif 'Sacramento' in x:\n        return 'Sacramento'\n    elif 'Atlanta' in x:\n        return 'Atlanta'\n    elif 'California' in x:\n        return 'California'\n    elif 'Florida' in x:\n        return 'Florida'\n    elif 'Texas' in x:\n        return 'Texas'\n    elif 'United States' in x or 'USA' in x:\n        return 'USA'\n    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n        return 'UK'\n    elif 'Canada' in x:\n        return 'Canada'\n    elif 'India' in x:\n        return 'India'\n    elif 'Kenya' in x:\n        return 'Kenya'\n    elif 'Nigeria' in x:\n        return 'Nigeria'\n    elif 'Australia' in x:\n        return 'Australia'\n    elif 'Indonesia' in x:\n        return 'Indonesia'\n    else: return 'Others'\n    ","13d2c3ef":"import string","6f452cb2":"train['locations'] = train['location'].apply(lambda x: setlocname(str(x)))\ntest['locations'] = test['location'].apply(lambda x:setlocname(str(x)))","40db7abe":"plt.figure(figsize=(9,6))\nsns.countplot(y=train.locations, order = train.locations.value_counts().iloc[:15].index)\nplt.title('Top Updated locations')\nplt.show()","08635da1":"top_l2 = train.groupby('locations').mean()['target'].sort_values(ascending=False)\nplt.figure(figsize=(14,6))\nsns.barplot(x=top_l2.index, y=top_l2)\nplt.axhline(np.mean(train.target))\nplt.xticks(rotation=80)\nplt.show()","80bbbda6":"leak = pd.read_csv(\"..\/input\/disasters-on-social-media\/socialmedia-disaster-tweets-DFE.csv\", encoding='latin_1')\nleak['target'] = (leak['choose_one']=='Relevant').astype(int)\nleak['id'] = leak.index\nleak = leak[['id', 'target','text']]\nmerged_df = pd.merge(test, leak, on='id')\nsub1 = merged_df[['id', 'target']]\nsub1.to_csv('submit_1.csv', index=False)","24bffce8":"#Clean text data berfore converting as vector\nimport re\n    \ndef preprocessing_text(text):\n    text = re.sub(r'https?:\/\/\\S+','',text)\n    text = re.sub(r'\\n',' ',text)\n    text = re.sub('\\s+',' ',text).strip()\n    return text","3901425d":"dummystr = train.loc[417,'text']\nprint(dummystr)\nprint(preprocessing_text(dummystr))","e544fe51":"def find_hashtags(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'","55816bef":"def find_mentions(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'","f124997c":"def find_links(tweet):\n    return \" \".join([match.group(0)[:] for match in re.finditer(r\"https?:\/\/\\S+\", tweet)]) or 'no'","3c604527":"def text_process(df):\n    df['text_clean'] = df['text'].apply(lambda x: preprocessing_text(x))\n    df['hash'] = df['text'].apply(lambda x: find_hashtags(x))\n    df['mention'] = df['text'].apply(lambda x: find_mentions(x))\n    df['links'] = df['text'].apply(lambda x: find_links(x))\n    \n    return df","0c364446":"train = text_process(train)\ntest = text_process(test)","7ab5dbef":"train.head()\nfrom wordcloud import STOPWORDS\nimport string","659885a8":"def make_wordcloud(df):\n    df['text_len'] = df['text_clean'].apply(len)\n    df['wordcount'] = df['text_clean'].apply(lambda x : len(str(x).split()))\n    df['stop_word_count'] = df['text_clean'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n    df['punctuation_count'] = df['text_clean'].apply(lambda x :len([c for c in str(x) if c in string.punctuation]))\n    df['hashtag_count'] = df['hash'].apply(lambda x :len(str(x).split()))\n    df['mention_count'] = df['mention'].apply(lambda x:len(str(x).split()))\n    df['link_count'] = df['links'].apply(lambda x:len(str(x).split()))\n    df['caps_count'] = df['text_clean'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))\n    df['caps_ratio'] = df['caps_count'] \/ df['text_len']\n    return df\n    ","338f3c28":"train = make_wordcloud(train)\ntest = make_wordcloud(test)\ntrain.head()","d001f2b3":"train.corr()","25178709":"TCor = train.corr()\nmask = np.triu(np.ones_like(TCor, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(TCor, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","3a2c4b3b":"train.corr()['target'].drop('target').sort_values()","337f4cc4":"#http:\/\/www.clker.com\/cliparts\/a\/1\/a\/5\/1242249442627091102Flammable-symbol.svg.med.png\n#from PIL import Image\n#import requests\n#from wordcloud import WordCloud\n#mask = np.array(Image.open(requests.get('http:\/\/www.clker.com\/cliparts\/a\/1\/a\/5\/1242249442627091102Flammable-symbol.svg.med.png', stream=True).raw))\n\n# This function takes in your text and your mask and generates a wordcloud. \n#def generate_wordcloud(words, mask):\n #   word_cloud = WordCloud(width = 512, height = 512, background_color='white', stopwords=STOPWORDS, mask=mask).generate(words)\n #   plt.figure(figsize=(10,8),facecolor = 'white', edgecolor='blue')\n #   plt.imshow(word_cloud)\n #   plt.axis('off')\n #   plt.tight_layout(pad=0)\n#  plt.show()\n\n#generate_wordcloud(train['text_clean'], mask)","8d3a5aad":"import category_encoders as ce","1c933214":"features = ['keyword','locations']\nencoder = ce.TargetEncoder(cols=features)\nencoder.fit(train[features],train['target'])\n\ntrain = train.join(encoder.transform(train[features]).add_suffix('_target'))\ntest = test.join(encoder.transform(test[features]).add_suffix('_target'))","ac2e3ad4":"from sklearn.feature_extraction.text import CountVectorizer","b8bcacbe":"vec_links = CountVectorizer(min_df=5,analyzer='word',token_pattern = r'https?:\/\/\\S+')\nlink_vec = vec_links.fit_transform(train['links'])\nlink_vec_test = vec_links.transform(test['links'])\nX_train_link = pd.DataFrame(link_vec.toarray(), columns=vec_links.get_feature_names())\nX_test_link = pd.DataFrame(link_vec_test.toarray(), columns=vec_links.get_feature_names())","c26984ab":"vec_men = CountVectorizer(min_df = 5)\nmen_vec = vec_men.fit_transform(train['mention'])\nmen_vec_test = vec_men.transform(test['mention'])\nX_train_men = pd.DataFrame(men_vec.toarray(), columns=vec_men.get_feature_names())\nX_test_men = pd.DataFrame(men_vec_test.toarray(), columns=vec_men.get_feature_names())","7abe7a68":"vec_hash = CountVectorizer(min_df = 5)\nhash_vec = vec_hash.fit_transform(train['hash'])\nhash_vec_test = vec_hash.transform(test['hash'])\nX_train_hash = pd.DataFrame(hash_vec.toarray(), columns=vec_hash.get_feature_names())\nX_test_hash = pd.DataFrame(hash_vec_test.toarray(), columns=vec_hash.get_feature_names())","5e5208a6":"hash_rank = (X_train_hash.transpose().dot(train['target']) \/ X_train_hash.sum(axis=0)).sort_values(ascending=False)\nprint('Hashtags with which 100% of Tweets are disasters: ')\nprint(list(hash_rank[hash_rank==1].index))\nprint('Total: ' + str(len(hash_rank[hash_rank==1])))\nprint('Hashtags with which 0% of Tweets are disasters: ')\nprint(list(hash_rank[hash_rank==0].index))\nprint('Total: ' + str(len(hash_rank[hash_rank==0])))","4c8a4d78":"from sklearn.feature_extraction.text import TfidfVectorizer","47609495":"vec_text = TfidfVectorizer(min_df = 10 ,ngram_range=(1,2),stop_words= 'english')\ntext_vec = vec_text.fit_transform(train['text_clean'])\ntext_vec_test = vec_text.transform(test['text_clean'])\nX_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names())\nX_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names())","3ccfaf80":"print(X_train_text.shape,X_test_text.shape)","7c3f841f":"train = train.join(X_train_link, rsuffix='_link')\ntrain = train.join(X_train_men, rsuffix='_mention')\ntrain = train.join(X_train_hash ,rsuffix='_hashtag')\ntrain = train.join(X_train_text ,rsuffix='_text')\n\ntest = test.join(X_test_link, rsuffix='_link')\ntest = test.join(X_test_men, rsuffix='_mention')\ntest = test.join(X_test_hash ,rsuffix='_hashtag')\ntest = test.join(X_test_text ,rsuffix='_text')\n\nprint (train.shape, test.shape)","5124129a":"#Logistic","60baa3ca":"from sklearn.linear_model import LinearRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler","d52ebd5c":"features_to_drop = ['id', 'keyword','location','text','locations','text_clean', 'hash', 'mention','links']","25e1d14a":"scale = MinMaxScaler()","76e285b5":"train.head()\n#\/X_train = train.drop(columns = features_to_drop + ['target'] )","0fef76f4":"X_train = train.drop(columns = features_to_drop + ['target'] )","a8c0ef72":"X_train.head()","36ca34be":"X_test = test.drop(columns = features_to_drop)","1330cedb":"X_test.head()","d42d2bd9":"y_train = train.target","0a2ed96f":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='liblinear', random_state=777)\npipeline = Pipeline ([('scale', scale),('lr',lr)])\npipeline.fit(X_train,y_train)\ny_test = pipeline.predict(X_test)\n\nsub_sample = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsubmit = sub_sample.copy()\nsubmit.target = y_test\nsubmit.to_csv('submit_lr.csv',index=False)","65078d1a":"print ('Accuracy for Train: %.4f' % pipeline.score(X_train, y_train))","49fa3730":"from sklearn.metrics import f1_score\nprint ('F1 score: %.4f' % f1_score(y_train, pipeline.predict(X_train)))","7dbe3b1e":"from sklearn.metrics import confusion_matrix\npd.DataFrame(confusion_matrix(y_train, pipeline.predict(X_train)))","f1392bc3":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit","5aeac00b":"cv = ShuffleSplit(n_splits=12, test_size=0.2,random_state=143)\ncv_score = cross_val_score(pipeline, X_train, y_train, cv=cv, scoring='f1')\n\nprint('Cross validation F-1 score: %.3f' %np.mean(cv_score))","5cec0f20":"from sklearn.feature_selection import RFECV\n\nsteps = 20\nn_features = len(X_train.columns)\nX_range = np.arange(n_features - (int(n_features\/steps)) * steps, n_features+1, steps)\n\nrfecv = RFECV(estimator=lr, step=steps, cv=cv, scoring='f1')\n\npipeline2 = Pipeline([('scale',scale), ('rfecv', rfecv)])\npipeline2.fit(X_train, y_train)\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(np.insert(X_range, 0, 1), rfecv.grid_scores_)\nplt.show()","4a257988":"selected_features = X_train.columns[rfecv.ranking_ == 1]\nX_train2 = X_train[selected_features]\nX_test2 = X_test[selected_features]","230943cf":"pipeline.fit(X_train2, y_train)\ncv2 = ShuffleSplit(n_splits=5, test_size=0.2, random_state=456)\ncv_score2 = cross_val_score(pipeline, X_train2, y_train, cv=cv2, scoring='f1')\nprint('Cross validation F-1 score: %.3f' %np.mean(cv_score2))","dd12d83d":"from sklearn.model_selection import GridSearchCV\n\ngrid={\"C\":np.logspace(-2,2,5), \"penalty\":[\"l1\",\"l2\"]}\nlr_cv = GridSearchCV(LogisticRegression(solver='liblinear', random_state=20), grid, cv=cv2, scoring = 'f1')\n\npipeline_grid = Pipeline([('scale',scale), ('gridsearch', lr_cv),])\n\npipeline_grid.fit(X_train2, y_train)\n\nprint(\"Best parameter: \", lr_cv.best_params_)\nprint(\"F-1 score: %.3f\" %lr_cv.best_score_)","01883f6a":"y_test2 = pipeline_grid.predict(X_test2)\nsubmit2 = sub_sample.copy()\nsubmit2.target = y_test2\nsubmit2.to_csv('submit_lr2.csv',index=False)","f952bda8":"y_hat = pipeline_grid.predict_proba(X_train2)[:,1]\nchecker = train.loc[:,['text','keyword','location','target']]\nchecker['pred_prob'] = y_hat\nchecker['error'] = np.abs(checker['target'] - checker['pred_prob'])\n\n# Top 50 mispredicted tweets\nerror50 = checker.sort_values('error', ascending=False).head(50)\nerror50 = error50.rename_axis('id').reset_index()\nerror50.target.value_counts()","b3f504e5":"pd.options.display.max_colwidth = 200\n\nerror50.loc[0:10,['text','target','pred_prob']]","b4ca510c":"Preprocessing"}}