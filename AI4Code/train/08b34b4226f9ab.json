{"cell_type":{"6518983b":"code","4154c12f":"code","92697d70":"code","65204d83":"code","57f1515c":"code","7534cccf":"code","1fee4006":"code","1fb331a5":"code","14beed79":"code","31f56063":"code","48a6d02d":"code","d36f91fc":"code","aa4f4dc0":"code","389966c8":"code","96ad8a45":"code","d6ae9882":"code","fd92eb82":"code","023bdaa8":"markdown","1638511c":"markdown","3780c35e":"markdown","e354106c":"markdown"},"source":{"6518983b":"# Import Required Libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\nfrom scipy.stats import mode\n\nimport matplotlib.pyplot as plt\nimport optuna\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, BatchNormalization\nimport keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler,MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nimport seaborn as sns\nimport gc\n\nfrom scipy.spatial import distance\ngc.enable()\n\npd.set_option('display.float_format', lambda x: '%.3f' % x)","4154c12f":"# Read 5 Fold Train, Test and Sample Submission Files\ndf_train = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/test.csv\")\ndf_submission = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")","92697d70":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","65204d83":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","57f1515c":"# Remove Row corresponding to Label 5 , since it has only 1 example\ndf_train = df_train[df_train.Cover_Type!=5]","7534cccf":"for col_name in ['Hillshade_9am','Hillshade_Noon','Hillshade_3pm']:\n    df_train.loc[df_train[col_name] < 0,col_name ] = 0\n    df_test.loc[df_test[col_name] < 0,col_name ] = 0\n\n    df_train.loc[df_train[col_name] > 255,col_name ] = 255\n    df_test.loc[df_test[col_name] > 255,col_name ] = 255\n","1fee4006":"# Removing Soil_Type7 and Soil_Type15 since all values are 0\ndf_train = df_train[[col for col in df_train.columns if col not in ('Soil_Type15','Soil_Type7')]]\ndf_test = df_test[[col for col in df_test.columns if col not in ('Soil_Type15','Soil_Type7')]]","1fb331a5":"def feature_transform(df):\n    # Adjusting Aspect\n    df.loc[df['Aspect'] < 0,'Aspect' ] += 360\n    df.loc[df['Aspect'] > 360,'Aspect' ] -= 360\n    \n    # Creating direction variables based on Aspect\n    df['N'] = np.where((((df['Aspect'] >= 0) & (df['Aspect'] <= 22.5)) | ((df['Aspect'] >= 337.5) & (df['Aspect'] <= 360))),1,0)\n    df['NE'] = np.where(((df['Aspect'] > 22.5) & (df['Aspect'] <= 67.5)),1,0)\n    df['E'] = np.where(((df['Aspect'] > 67.5) & (df['Aspect'] <= 112.5)),1,0)\n    df['SE'] = np.where(((df['Aspect'] > 112.5) & (df['Aspect'] <= 157.5)),1,0)\n    df['S'] = np.where(((df['Aspect'] > 157.5) & (df['Aspect'] <= 202.5)),1,0)\n    df['SW'] = np.where(((df['Aspect'] > 202.5) & (df['Aspect'] <= 247.5)),1,0)\n    df['W'] = np.where(((df['Aspect'] > 247.5) & (df['Aspect'] <= 292.5)),1,0)\n    df['NW'] = np.where(((df['Aspect'] > 292.5) & (df['Aspect'] < 337.5)),1,0)\n    \n    # Creating New distances\n    \n    df['manh_dist'] = np.abs((df['Horizontal_Distance_To_Hydrology'].astype(np.int32)) + (df['Vertical_Distance_To_Hydrology'].astype(np.int32)))\n    df['euc_dist'] = np.sqrt((df['Horizontal_Distance_To_Hydrology'].astype(np.int32))**2 + (df['Vertical_Distance_To_Hydrology'].astype(np.int32))**2)\n       \n    \n    # Creating new columns based on Winderness Area\n    \n    df['total_wilderness_area'] = df[[c for c in df.columns if c.startswith(\"Wilderness_Area\")]].sum(axis=1)\n    df[\"total_soil_type\"] = df[[c for c in df.columns if c.startswith(\"Soil_Type\")]].sum(axis=1)\n    \n    return df","14beed79":"cont_cols = ['Elevation', 'Aspect', 'Slope',\n       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points',\n            'manh_dist','euc_dist','total_wilderness_area','total_soil_type','Hillshade_9am','Hillshade_Noon','Hillshade_3pm']","31f56063":"for x in cont_cols:\n    try:\n        sns.displot(data=df_train, x=x,hue=\"Cover_Type\",kind = \"kde\", fill=True)\n    except:\n        continue","48a6d02d":"# Function to Remove outliers from data\ndef remove_outliers(x,method):\n    if method == 'mean':\n        upper_limit = x.mean() + (3*x.std())\n        lower_limit = x.mean() - (3*x.std())\n        return np.where(x > upper_limit,upper_limit,np.where(x <lower_limit,lower_limit,x))\n    elif method == 'median':\n        upper_limit = x.median() + (1.5*x.quantile(0.75))\n        lower_limit = x.median() - (1.5*x.quantile(0.25))\n        return np.where(x > upper_limit,upper_limit,np.where(x <lower_limit,lower_limit,x))\n    else:\n        return x","d36f91fc":"# Function to Scale and transform dataset\ndef data_scaler_fit(option,df):\n    if option == 1:\n        transformer = StandardScaler().fit(df)\n    if option == 2 :\n        transformer = RobustScaler().fit(df)\n    if option == 3 :\n        transformer = MinMaxScaler().fit(df)\n    return transformer","aa4f4dc0":"df_train = feature_transform(df_train)\ndf_test = feature_transform(df_test)","389966c8":"useful_features = [c for c in df_train.columns if c not in (\"Id\", \"Cover_Type\", \"kfold\")]\n\ncat_cols = [c for c in useful_features if c not in cont_cols]\n\n# Using Standard scalar for all non categorical columns as original distribution for each column is close to normal\ntransformer = data_scaler_fit(2,df_train[cont_cols])\n\ndf_test = np.concatenate((transformer.transform(df_test[cont_cols].apply(lambda x: remove_outliers(x,'mean'))),df_test[cat_cols].to_numpy()),axis = 1)","96ad8a45":"df_train = df_train.reset_index(drop=True)\ny = df_train['Cover_Type'].copy()\nX = df_train.drop('Cover_Type',axis = 1)","d6ae9882":"final_test_predictions = []\nfinal_valid_predictions = {}\n\nscores = []\n    \n\nprint(\"Training using Keras NN..\")\n\nbatch_size = 1024\nepochs = 100\nverbose = 1\n\ncv = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n    xtrain, ytrain = X.iloc[idx_train], y[idx_train]\n    xvalid, yvalid = X.iloc[idx_valid], y[idx_valid]\n    \n    xtest = df_test.copy()\n\n    # Store IDs of validation Dataset\n    valid_ids = xvalid.Id.values.tolist()\n    \n    #Save a copy of yvalid\n    true_valid = yvalid\n    \n    #Label encoding Y\n    le = preprocessing.LabelEncoder().fit(ytrain)\n\n    ytrain = le.transform(ytrain)\n    yvalid = le.transform(yvalid)\n    \n\n    n_class = len(np.unique(ytrain))\n\n    xtrain = np.concatenate((transformer.transform(xtrain[cont_cols].apply(lambda x: remove_outliers(x,'mean'))),xtrain[cat_cols].to_numpy()),axis = 1)\n    xvalid = np.concatenate((transformer.transform(xvalid[cont_cols].apply(lambda x: remove_outliers(x,'mean'))),xvalid[cat_cols].to_numpy()),axis = 1)\n\n    model = keras.models.Sequential([\n            keras.layers.Flatten(input_shape=[xtest.shape[1],]),\n            keras.layers.Dense(600, activation=\"selu\"),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dense(400, activation=\"selu\"),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dropout(0.3),\n            keras.layers.Dense(300, activation=\"selu\"),\n            keras.layers.BatchNormalization(),\n            keras.layers.Dense(100, activation=\"selu\"),\n            keras.layers.Dense(n_class, activation=\"softmax\")\n            ])\n\n    callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=20, verbose = verbose)\n\n    reducelr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose = verbose)\n\n\n    model.compile(loss=\"sparse_categorical_crossentropy\",\n                    optimizer=\"adam\",\n                    metrics=['accuracy'],\n                 )\n\n    # model.summary()\n\n    model.fit(xtrain, ytrain, batch_size = batch_size, epochs=epochs,shuffle=True , validation_data=(xvalid, yvalid),callbacks=[callback,reducelr],verbose = verbose)\n\n    preds_valid = le.inverse_transform(np.argmax(model.predict(xvalid), axis = 1))\n\n    test_preds = le.inverse_transform(np.argmax(model.predict(xtest), axis = 1))\n\n    final_test_predictions.append(test_preds)\n\n    final_valid_predictions.update(dict(zip(valid_ids, preds_valid)))\n\n    acc_scr = accuracy_score(true_valid, preds_valid)\n    \n    \n    print('_'*65)\n    \n    print(f\"Fold {fold+1} || Accuracy : {acc_scr}\")\n    \n    print('_'*65)\n    \n    print('\\n')\n    \n    scores.append(acc_scr)\n    \n    gc.collect()\n    \n    keras.backend.clear_session()","fd92eb82":"df_submission.Cover_Type = mode(np.column_stack(final_test_predictions), axis=1)[0]\ndf_submission.columns = [\"Id\", \"Cover_Type\"]\ndf_submission.to_csv(\"submission.csv\", index=False)","023bdaa8":"There are 7 continuous variables :\n- Elevation - Elevation in meters\n- Aspect - Aspect in degrees azimuth\n- Slope - Slope in degrees\n- Horizontal_Distance_To_Hydrology - Horz Dist to nearest surface water features\n- Vertical_Distance_To_Hydrology - Vert Dist to nearest surface water features\n- Horizontal_Distance_To_Roadways - Horz Dist to nearest roadway\n- Horizontal_Distance_To_Fire_Points - Horz Dist to nearest wildfire ignition points","1638511c":"> **All follow normal distribution**","3780c35e":"As per [data](https:\/\/www.kaggle.com\/c\/forest-cover-type-prediction\/data) definitions, following 3 columns should be between 0 to 255, but we can observe data <0 and >255, hence transforming to restrict between the range \n- Hillshade_9am (0 to 255 index) - Hillshade index at 9am, summer solstice\n- Hillshade_Noon (0 to 255 index) - Hillshade index at noon, summer solstice\n- Hillshade_3pm (0 to 255 index) - Hillshade index at 3pm, summer solstice","e354106c":"- Aspect is in degress, which should be between 0 and 360, but we can observer data <0 and >360. Adding 360 to < 0 and subtracting 360 from >360 should do the work\n- Calculating two new distance to Hydrology : Euclidean and Cosine\n- Creating new column based on wilderness Area and Soil type"}}