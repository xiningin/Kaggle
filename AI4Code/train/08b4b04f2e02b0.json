{"cell_type":{"f1dd1656":"code","23ffeb51":"code","6e8104cc":"code","c3a10171":"code","4975a33e":"code","bc962ac9":"code","7cb0302c":"code","8c2b45e0":"code","dac3e546":"code","d9bc4aa4":"code","d68a71c3":"code","007aa1bf":"code","93a92ee0":"code","b776929c":"code","a39b0458":"code","9c33d4c8":"code","6e4568e1":"code","7ec475ee":"code","637da149":"code","66506fbb":"code","be4f5909":"code","7d160d30":"code","1badbd58":"code","da4e91fb":"code","c0023985":"code","eaea141b":"markdown","53ec9cef":"markdown","ab297901":"markdown","472855a9":"markdown","a0da6bb9":"markdown","c217544d":"markdown","b03b175c":"markdown","9175902c":"markdown"},"source":{"f1dd1656":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.decomposition import PCA\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,ExtraTreesClassifier\nimport os\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\nfrom sklearn.utils import shuffle\nprint(os.listdir(\"..\/input\"))\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\n# Any results you write to the current directory are saved as output.","23ffeb51":"df=pd.read_csv('..\/input\/train.csv')\ndf.head()","6e8104cc":"x_testFINAL=pd.read_csv('..\/input\/test.csv')\nsub=pd.read_csv('..\/input\/sample_submission.csv')\nx_testFINAL.head()","c3a10171":"print(df.groupby('target').describe())\nprint(\"shape of dataset::\" +str(df.shape))","4975a33e":"df=shuffle(df)\ndf_test=df.iloc[:12000,:]\ndf_train=df.iloc[12000:,:]","bc962ac9":"df_train.shape","7cb0302c":"import seaborn as sns\nx_train=df_train.iloc[:,2:]\ny_train=df_train['target']\nsns.boxplot(x=x_train['var_2'])","8c2b45e0":"from scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(df_train.iloc[:,1:]))\nthreshold = 3\ndf_train_rm = df_train[(z < threshold).all(axis=1)]\nprint('df_train_rm shape :'+str(df_train_rm.shape))\nprint('df_train shape :'+str(df_train.shape))","dac3e546":"def plot_2d_space(X, y, label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()","d9bc4aa4":"x_train1=df_train_rm.iloc[:,2:]\ny_train1=df_train_rm.iloc[:,1]\nmin_max_scaler =MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(x_train1)\nx_train1 = pd.DataFrame(np_scaled)\n\npca = PCA(n_components=2)\nx_plot1 = pca.fit_transform(x_train1)\nplot_2d_space(x_plot1,y_train1, 'after removing outlier dataset (2 PCA components)')\nsns.boxplot(x=x_train1.iloc[:,5])","d68a71c3":"x_train=df_train.iloc[:,2:]\ny_train=df_train['target']\nmin_max_scaler =MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(x_train)\nx_train = pd.DataFrame(np_scaled)\n\npca = PCA(n_components=2)\nx_plot= pca.fit_transform(x_train)\nplot_2d_space(x_plot,y_train, 'before removing outlier dataset (2 PCA components)')\nsns.boxplot(x=x_train.iloc[:,5])","007aa1bf":"# Class count\ncount_class_0, count_class_1 = df_train_rm.target.value_counts()\n\n# Divide by class\ndf_train_rm_class_0 = df_train_rm[df_train_rm['target'] == 0]\ndf_train_rm_class_1 = df_train_rm[df_train_rm['target'] == 1]\nfrac=.5\ndf_train_rm_class_1_over = df_train_rm_class_1.sample(int(count_class_0*frac), replace=True)\ndf_test_over = pd.concat([df_train_rm_class_0, df_train_rm_class_1_over], axis=0)\ndf_test_over=shuffle(df_test_over)\nprint('Random over-sampling:')\nprint(df_test_over.target.value_counts())\n\ndf_test_over.target.value_counts().plot(kind='bar', title='Count (target)'); ","93a92ee0":"df_ov=df_test_over.iloc[:,2:]\nmin_max_scaler =MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(df_ov)\ndf_normov = pd.DataFrame(np_scaled)\ndf_normov.head()","b776929c":"X_test=df_test.iloc[:,1:]\ny_test=df_test['target']","a39b0458":"X=df_normov\nY=df_test_over['target']\ndf_testnorm=df_test.iloc[:,2:]\nmin_max_scaler =MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(df_testnorm)\ndf_testnorm = pd.DataFrame(np_scaled)\n\nX_test=df_testnorm\ny_test=df_test['target']","9c33d4c8":"def plot_2d_space(X, y, label='Classes'):   \n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    plt.title(label)\n    plt.legend(loc='upper right')\n    plt.show()","6e4568e1":"model = XGBClassifier(n_estimators=300,max_dapth=10)\nmodel.fit(X, Y)\ny_pred_xgb = model.predict(X_test)\n\naccuracy = accuracy_score(y_test, y_pred_xgb)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","7ec475ee":"#y_predoverrfc = clf.predict(X)\n#accuracy = accuracy_score(Y, y_predoverrfc)\n#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","637da149":"#clf_ada= AdaBoostClassifier(DecisionTreeClassifier(max_depth=10),n_estimators=100)\n#clf_ada.fit(X,Y)\n#y_predoverada = clf_ada.predict(X_test)\n#accuracy = accuracy_score(y_test, y_predoverada)\n#print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","66506fbb":"bag=BaggingClassifier(DecisionTreeClassifier(),max_samples=.3,max_features=1.0,n_estimators=300)\nbag.fit(X,Y)\ny_predoverbag = bag.predict(X_test)\naccuracy = accuracy_score(y_test, y_predoverbag)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","be4f5909":"svm=SVC()","7d160d30":"from sklearn.metrics import confusion_matrix\nfrom matplotlib import pyplot as plt\n\nconf_mat = confusion_matrix(y_true=y_test, y_pred=y_pred_xgb)\nprint('Confusion matrix:\\n', conf_mat)\n\nlabels = ['Class 0', 'Class 1']\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(conf_mat, cmap=plt.cm.Blues)\nfig.colorbar(cax)\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('Expected')\nplt.show()","1badbd58":"mysat0=conf_mat[0,0]\/(conf_mat[0,0]+conf_mat[0,1])\nprint('acc_0::'+str(mysat0))\nmysat1=conf_mat[1,1]\/(conf_mat[1,0]+conf_mat[1,1])\nprint('acc_1::'+str(mysat1))","da4e91fb":"df_ov=x_testFINAL.iloc[:,1:]\nmin_max_scaler =MinMaxScaler()\nnp_scaled = min_max_scaler.fit_transform(df_ov)\nx_test_final = pd.DataFrame(np_scaled)","c0023985":"y_predFINAL = pd.DataFrame(model.predict(x_test_final))\ny_predexp=y_predFINAL\nsub['target']=y_predexp.iloc[:,:]\nsub.to_csv('submission.csv', index=False)","eaea141b":"REMOVE OUTLIER","53ec9cef":"Upload train,test,sub","ab297901":"**data loaded**","472855a9":"Splitted into train and test","a0da6bb9":"Box Plot","c217544d":"** Discover outliers with visualization tools**","b03b175c":"**Feature Engineering**","9175902c":"**Imbalanced by Over Sampling**"}}