{"cell_type":{"04d08625":"code","d0bc2735":"code","8962af0e":"code","d5392fff":"code","88bbccc3":"code","d50a5817":"code","eb75c5af":"code","8a451bce":"code","0473276b":"code","85651e83":"code","9805d8f9":"code","e1f8cf8d":"code","13e663e4":"code","a605e485":"code","25e9ef63":"code","f9e71a8e":"code","2b985a4b":"code","c5c8646d":"code","2b150e37":"code","a7b60cd2":"code","70efb6fb":"code","49fe91a8":"code","c39ff5bf":"code","11192058":"code","d3880b8c":"code","8236d056":"code","80d32ab6":"code","bd886157":"code","eb48178a":"code","5bc7c2d6":"code","8c8370d2":"code","dcbbfe70":"code","b285e787":"code","405a3b22":"code","ac54101e":"code","e973dbb9":"markdown","d38dfbdd":"markdown","9433d023":"markdown","683a9dd7":"markdown","3ea5d978":"markdown","dfff3ab2":"markdown","5f56f607":"markdown","c6d6e9e9":"markdown","20734f2b":"markdown","bbbc0b57":"markdown","70ab916d":"markdown","187d64ed":"markdown","42cc80d3":"markdown","7b837520":"markdown","438a18e0":"markdown","0fa48a25":"markdown","809bade0":"markdown","dce9d1ec":"markdown","263d3270":"markdown","10474527":"markdown","e474cf7f":"markdown","f264c47d":"markdown","6c22654e":"markdown","9f6cd008":"markdown","d9f40bfb":"markdown","7b819719":"markdown","bf71e91f":"markdown","b2f92f06":"markdown","1d6881cd":"markdown","1b2f5ffc":"markdown","e4323963":"markdown","49c9c995":"markdown","50a74c8e":"markdown","cb73f2cd":"markdown","54d7ee79":"markdown","237da7dc":"markdown","52135dfd":"markdown","ebe9e118":"markdown","09c379c3":"markdown","5db43b23":"markdown","62ac698d":"markdown","9557e560":"markdown","5fd01da0":"markdown","06020248":"markdown","ef1a7b4a":"markdown","1083df76":"markdown","29d502a7":"markdown","ca6b890d":"markdown","15c27fd4":"markdown","ceb2fe1a":"markdown","54bae9a9":"markdown","698cc1d5":"markdown","a12dd186":"markdown","dfd7e717":"markdown","1f64e362":"markdown","84b50b60":"markdown","c4bb2ab3":"markdown","26c13a99":"markdown","4d810785":"markdown"},"source":{"04d08625":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\n#algorithms to use\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Metrics to evaluate the model\nfrom sklearn.metrics import confusion_matrix, classification_report, recall_score\nfrom sklearn import metrics\n\n#for hyperparameter tuning\nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d0bc2735":"data = pd.read_csv(\"CreditRisk.csv\")\ndata.head()","8962af0e":"data.info()","d5392fff":"data.Loan_ID.nunique()","88bbccc3":"data.drop(columns=['Loan_ID'], inplace=True)","d50a5817":"#Creating a list of numerical columns\nnum_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term']\n\ndata[num_cols].describe().T","eb75c5af":"#Converting the scale of loan term from months to years\ndata['Loan_Amount_Term']=data['Loan_Amount_Term']\/12","8a451bce":"#Adding the applicant and co-applicant income to get the total income per application\ndata['total_income']=data['ApplicantIncome'] + data['CoapplicantIncome']","0473276b":"#Dropping the columns as we created a new column which captures the same information\ndata.drop(columns=['ApplicantIncome', 'CoapplicantIncome'], inplace=True)","85651e83":"#Creating list of categorical columns\ncat_col= ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History','Property_Area', 'Loan_Status']\n\nfor col in cat_col:\n    print(data[col].value_counts(normalize=True))  #The parameter normalize=True gives the percentage of each category\n    print('*'*40)                                  #Print the * 40 times to separate different variables","9805d8f9":"#Imputing missing values with mode for the categorical variables \nfor col in ['Gender', 'Married', 'Dependents', 'Self_Employed', 'Loan_Amount_Term', 'Credit_History']:\n    data[col].fillna(value=data[col].mode()[0], inplace=True)","e1f8cf8d":"#Replacing 0's with null values in loan amount \ndata.LoanAmount.replace(0, np.nan, inplace=True)\n\n#Imputing null values in loan amount with the median\ndata.LoanAmount.fillna(value=data.LoanAmount.median(), inplace=True)","13e663e4":"#Separating target variable and other variables\nX=data.drop(columns='Loan_Status')\nY=data['Loan_Status']","a605e485":"#Creating dummy variables \n#drop_first=True is used to avoid redundant variables\nX = pd.get_dummies(X, drop_first=True)","25e9ef63":"#Splitting the data into train and test sets\nX_train,X_test,y_train,y_test=train_test_split(X, Y, test_size=0.30, random_state=7)","f9e71a8e":"#function to print classification report and get confusion matrix in a proper format\n\ndef metrics_score(actual, predicted):\n    print(classification_report(actual, predicted))\n    cm = confusion_matrix(actual, predicted)\n    plt.figure(figsize=(8,5))\n    sns.heatmap(cm, annot=True,  fmt='.2f', xticklabels=['Not Eligible', 'Eligible'], yticklabels=['Not Eligible', 'Eligible'])\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    plt.show()","2b985a4b":"#Fitting the decision tree classifier on the training data\nd_tree =  DecisionTreeClassifier( random_state=7)\nd_tree.fit(X_train, y_train)","c5c8646d":"#Checking performance on the training data\ny_pred_train1 =  d_tree.predict(X_train)\nmetrics_score(y_train,y_pred_train1)","2b150e37":"#Checking performance on the testing data\ny_pred_test1 = d_tree.predict(X_test)\nmetrics_score(y_test,y_pred_test1)","a7b60cd2":"# Choose the type of classifier \nd_tree_tuned = DecisionTreeClassifier(random_state=7, class_weight={0:0.7, 1:0.3})\n\n# Grid of parameters to choose from\nparameters = {'max_depth': np.arange(2,10), \n              'criterion': ['gini', 'entropy'],\n              'min_samples_leaf': [5, 10, 20, 25]\n             }\n\n# Type of scoring used to compare parameter combinations - recall score for class 0\nscorer = metrics.make_scorer(recall_score, pos_label=0)\n\n# Run the grid search\ngrid_obj = GridSearchCV(d_tree_tuned, parameters, scoring=scorer, cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nd_tree_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nd_tree_tuned.fit(X_train, y_train)","70efb6fb":"#Checking performance on the training data\ny_pred_train2 = d_tree_tuned.predict(X_train) \nmetrics_score(y_train,y_pred_train2)","49fe91a8":"#Checking performance on the testing data\ny_pred_test2 = d_tree_tuned.predict(X_test)\nmetrics_score(y_test,y_pred_test2)","c39ff5bf":"features = list(X.columns)\n\nplt.figure(figsize=(20,20))\n\ntree.plot_tree(d_tree_tuned,feature_names=features,filled=True,fontsize=9,node_ids=True,class_names=True)\nplt.show()","11192058":"# importance of features in the tree building\n\nprint (pd.DataFrame(d_tree_tuned.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))","d3880b8c":"#Plotting the feature importance\nimportances = d_tree_tuned.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(10,10))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","8236d056":"#Fitting the decision tree classifier on the training data\nrf_estimator = RandomForestClassifier(criterion=\"entropy\", random_state=7)\nrf_estimator.fit(X_train,y_train)","80d32ab6":"#Checking performance on the training data\ny_pred_train3 = rf_estimator.predict(X_train)\nmetrics_score(y_train, y_pred_train3)","bd886157":"#Checking performance on the testing data\ny_pred_test3 = rf_estimator.predict(X_test)\nmetrics_score(y_test,y_pred_test3)","eb48178a":"# Choose the type of classifier. \nrf_estimator_tuned = RandomForestClassifier(criterion=\"entropy\", random_state=7)\n\n# Grid of parameters to choose from\nparameters = {\"n_estimators\": [100, 110, 120],\n    \"max_depth\": [5, 6, 7],\n    \"max_features\": [0.8, 0.9, 1]\n             }\n\n# Type of scoring used to compare parameter combinations - recall score for class 0\nscorer = metrics.make_scorer(recall_score, pos_label=0)\n\n# Run the grid search\ngrid_obj = GridSearchCV(rf_estimator_tuned, parameters, scoring=scorer, cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nrf_estimator_tuned = grid_obj.best_estimator_","5bc7c2d6":"#Fitting the best algorithm to the training data\nrf_estimator_tuned.fit(X_train, y_train)","8c8370d2":"#Checking performance on the training data\ny_pred_train4 = rf_estimator_tuned.predict(X_train)\nmetrics_score(y_train, y_pred_train4)","dcbbfe70":"# Choose the type of classifier. \nrf_estimator_tuned = RandomForestClassifier(criterion=\"entropy\", random_state=7)\n\n# Grid of parameters to choose from\nparameters = {\"n_estimators\": [110, 120],\n    \"max_depth\": [6, 7],\n    \"min_samples_leaf\": [20, 25],\n    \"max_features\": [0.8, 0.9],\n    \"max_samples\": [0.9, 1],\n    \"class_weight\": [{0:0.7, 1:0.3}, \"balanced\", {0:0.4, 1:0.1}]\n             }\n\n# Type of scoring used to compare parameter combinations - recall score for class 0\nscorer = metrics.make_scorer(recall_score, pos_label=0)\n\n# Run the grid search on the training data using scorer=scorer and cv=5\ngrid_obj = GridSearchCV(rf_estimator_tuned, parameters, scoring=scorer, cv=5)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Save the best estimator to variable rf_estimator_tuned\nrf_estimator_tuned = grid_obj.best_estimator_\n\n#Fit the best algorithm to the training data\nrf_estimator_tuned.fit(X_train, y_train)","b285e787":"#Checking performance on the training data\ny_pred_train5 = rf_estimator_tuned.predict(X_train)\nmetrics_score(y_train, y_pred_train5)","405a3b22":"#Checking performance on the testing data\ny_pred_test5 = rf_estimator_tuned.predict(X_test)\nmetrics_score(y_test, y_pred_test5)","ac54101e":"importances = rf_estimator_tuned.feature_importances_\nindices = np.argsort(importances)\nfeature_names = list(X.columns)\n\nplt.figure(figsize=(12,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","e973dbb9":"### **Question 2:**\n- **Check the performance on both training and testing data (2 Marks)**\n- **Compare the results with the results from the decision tree model with default parameters and write your observations (2 Marks)**","d38dfbdd":"## **Conclusion and Recommendations**","9433d023":"Now that we are done with the data preprocessing. Let's move onto the model building section.","683a9dd7":"### **Question 7:**\n\n**Write your conclusions on the key factors that drive the approval of loans and write your recommendations to the business on how can they minimize credit risk. (3 Marks)**","3ea5d978":"**Let's confirm this by checking its performance on the testing data:**","dfff3ab2":"**Let's check the performance on the training data:**","5f56f607":"**Observations:**\n\n- As we notice tuned model is not performing as well as the previous\/base model. \n- Performance has gone down significantly on the testing dataset as compared to the training dataset.   \n- As per the \"precision\" score from the test data, we can conclude that there is a **49% chance that the model will predict a customer is not eligible for a loan when the customer is actually eligible for a loan, which leads to loss of opportunity.**","c6d6e9e9":"**Observations:**\n\n- **The feature importance plot of the Random Forest is not similar to the Decision Tree**\n- **As same to the Decision Tree, credit history is the most important feature** **followed by total income and loan amount**\n- **Married_Yes, Property_Area_Semiurban, and Loan_Amount_Term are also keys feature in deciding the customer's loan eligibility.**\n- Education_Not Graduate and Self_Employed_Yes also seem to have good important in deciding the loan eligibility of the customers.\n- Besides, Property_Area_Urban, total numbers of dependents, and Gender_Male also seem to have some important in explaining the loan approval and predicting the \"Non-eligible customers\"","20734f2b":"Let's try **tuning some of the important hyperparameters of the Random Forest Classifier**. \n\nWe will **not** tune the `criterion` hyperparameter as we know from hyperparameter tuning for decision trees that `entropy` is a better splitting criterion for this data.","bbbc0b57":"## **Importing the necessary libraries and overview of the dataset**","70ab916d":"**Observations:**\n\n- The Decision Tree is predicting a 100% result on the training dataset.","187d64ed":"#### Preparing data for modeling","42cc80d3":"## **Checking the percentage of each category for categorical variables**","7b837520":"**Observations:**\n\n- **Performance on the training data set has gone up significantly** as compare to the base\/previous model.","438a18e0":"### Note\n- We have already seen the classification performance earlier using Logistic Regression and the KNN classifier.\n- Here, **we will aim to get better recall for class 0** as the company would want the model to correctly identify as many of the 'non-eligible' customers as possible.\n\nAlso, let's create a function to calculate and print the classification report and confusion matrix so that we don't have to rewrite the same code repeatedly for each model.","0fa48a25":"## **Summary Statistics for numerical columns**","809bade0":"- We can see that **all the entries of this column are unique.** Hence, this column would not add any value to our analysis. \n- Let's drop this column.","dce9d1ec":"**Before training the model, let's choose the appropriate model evaluation criterion as per the problem on hand.**\n\n### Model evaluation criterion:\n\n#### Since this is a binary classification problem, the model can make wrong predictions in one of two ways:\n1. Predicting a customer is not eligible for a loan when the customer actually is eligible - **Loss of opportunity**\n2. Predicting a customer is eligible for a loan when the customer is actually not - **Financial Loss**\n\n#### Which case is more important? \n* **Predicting that the customer is eligible when he is not**, because the person might default or not be able to return the loan, which would result in financial loss to the company.\n\n#### How to reduce this loss i.e need to reduce False Negatives?\n* In classification, the class of interest is considered the positive class. Here, the class of interest is 0 i.e. identifying non-eligible customers. So, the company wants to minimize the number of false negatives, in other words **Recall** is the evaluation criterion to be maximized.","263d3270":"#### Imputing missing values","10474527":"### **Question 4:** \n- **Fit the random forest classifier on the training data (use random_state=7) (2 Marks)**\n- **Check the performance on both training and testing data (use metrics_score function) (2 Marks)**\n- **Write your observations (2 Marks)**","e474cf7f":"## **Exploratory Data Analysis and Data Preprocessing**","f264c47d":"**Let's check the performance of the model on the training data:**","6c22654e":"**Observations:**\n\n- The average income of applicants is about 5.4K dollars. It has a large range of values from 150 to 81,000. It would be interesting to see if low applicant income implies a low loan amount.\n- The average co-applicant income is about 1.6K dollars which is much lower than the applicant's income. The 25th percentile value is 0 and the median value is 1,188 dollars which implies that the number of joint home loans is more than the number of non-joint loans.\n- The two columns `ApplicantIncome` and `CoapplicantIncome` give the same information i.e. income of applicants. It would be better to have that information in one column only. We can add these two columns to get the total income per application.\n- The 25th, 50th, and 75th percentile value of the loan term is 360 months i.e. 30 years. This implies that the majority of home loans in this dataset are for 30 years of term.\n- We can convert the scale of the loan term from months to years to make it easier to read.\n- The average loan amount is 1.41K dollars. It has a large range of values, which is to be expected. \n- The loan amount has zero values, which is absurd since loan amounts are meant to be non-zero. So we can treat 0's as missing values.\n\nBefore imputing the missing values and 0's in `LoanAmount`, let's do some feature engineering and check the distributions, counts, and outliers for different variables.","9f6cd008":"- **As observed earlier, credit history is the most important feature** **followed by total income and loan amount** which makes sense.\n- **Married_Yes and Property_Area_Semiurban have some importance** and the rest of the variables have no impact in this model, while deciding loan eligibility.\n\nNow let's build another model - **a random forest classifier**","d9f40bfb":"**Observations:**\n- As the recall score is 0.48 (same as earlier), the model performs poorly on the training dataset even after tunning the random forest classifier using GridSearchCV.\n- Precision score is 0.98, and performing very well on the train dataset.","7b819719":"**Let's check the performance of the tuned model:**","bf71e91f":"## **Decision Tree - Hyperparameter Tuning**\n\nWe will use the class_weight hyperaparameter with value equal to {0:0.7, 1:0.3} which is approximately the opposite of the imbalance in the original data. \n\n**This would tell the model that 0 is the important class here.**","b2f92f06":"### **Question 5:**\n- **Tune the random forest classifier using GridSearchCV (2 Marks)**\n- **Check the performance on both training and testing data (2 Marks)**\n- **Compare the results with the results from the random forest model with default parameters and write your observations (2 Marks)**","1d6881cd":"## **Decision Tree**\n\nIn this section we will implement a decision tree classifier on the data.\n","1b2f5ffc":"**Observations:**\n\n- Recall score of the testing data set is .49 where as training data set has the recall score of 1. Thus, the Random Forest classifier is overfitting on the dataset. \n- Precision score is .81 in the testing data set.Thus, there is a **19% chance that the model will predict a customer is not eligible for a loan when the customer is actually eligible for a loan, which leads to loss of opportunity.**","e4323963":"**Let's try hyperparameter tuning using GridSearchCV to find the optimal max_depth** in order to reduce overfitting of the model. We can tune some other hyperparameters as well.","49c9c995":"**Let's look at the feature importance** of the tuned decision tree model:","50a74c8e":"**Let's check the model performance on the testing data**","cb73f2cd":"## **Checking the info of the data**","54d7ee79":"# **Project: Decision Trees and Random Forest - Loan Eligibility Prediction**\n\n#### Durga P. Dulal - MIT Applied Data Science ####\n\n\nWelcome to the project on classification using decision trees and random forests. \n\nWe will continue our work on the Loan Eligibility Prediction dataset for this project.\n\n----------------\n## **Context:** \n----------------\n\n**Credit risk is the risk of financial loss resulting from the failure by a borrower to repay the principal and interest owed to the lender.** The lender uses the interest payments from the loan to compensate for the risk of potential losses. When the borrower defaults on his\/her obligations, it causes an interruption in the cash flow of the lender.\n\nIn the banking sector, this is an important factor to be considered before approving the loan of an applicant in order to cushion the lender from loss of cash flow and reduce the severity of losses. \n\n------------------\n## **Objective:**\n------------------\n\nEasy House is a finance company that deals in several varieties of home loans. They have a presence across urban, semi-urban, and rural areas. Currently the customer first applies for a home loan, after which the company validates the customer's eligibility for that loan. \n\nNow, the company wants to automate this loan eligibility process. They want to harness their past customers' data to **build a model to predict whether the loan should be approved or not.** This would help the company prevent potential losses, save time and focus more on eligible customers.\n\n--------------------------\n## **Data Dictionary:**\n--------------------------\n\nThe data contains the following attributes:\n\n* **Loan_ID**: Unique Loan ID\n* **Gender**: Gender of the applicant - Male\/Female\n* **Married**: Whether the applicant is married or not (Yes\/No)\n* **Dependents**: Number of dependents of the applicant\n* **Education**: Applicant's education (Graduate\/Not Graduate)\n* **Self_Employed**: Whether the applicant is self-employed (Yes\/No)\n* **ApplicantIncome**: The income of the applicant (\\$)\n* **CoapplicantIncome**: The co-applicant's income in case of a joint loan and 0 otherwise (\\$)\n* **LoanAmount**: Loan amount (dollars in thousands) \n* **Loan_Amount_Term**: Term of loan in months\n* **Credit_History**: Whether the applicant's credit history meets required guidelines\n* **Property_Area**: The area the property pertaining to the loan belongs to - Urban\/Semi-Urban\/Rural\n* **Loan_Status**: Loan approved (1 - Yes, 0 - No)","237da7dc":"**Observations:**\n\n- Percentage of male customers (approx 81%) is more than the percentage of female customers (approx 19%).\n- About 65% of customers are married. This indicates that married people apply more for the home loan as compared to non-married people.\n- The majority of customers have 0 or 1 dependents. Some of these entries are 3+ which means 3 or more dependents. Since we don't know the actual number, we can treat this as a separate category.\n- About 78% of customers are graduate. It looks like the majority of customers are educated.\n- Only ~14% of customers are self-employed. This indicates that salaried people apply more for the home loan.\n- The majority of customers, about 84%, have credit history as per the required guidelines.\n- The count of observations is approximately evenly distributed among Urban, Semi-Urban, and Rural areas. This implies that the company has a presence in all kinds of areas.\n- The number of home loans approved is comparatively higher than the number of those not approved. About 69% of applied loans get approved. Although we have limited data, the reason for this could either be a fairly lenient attitude from the company towards approving loans OR that many of the customers applying for these home loans are actually eligible for them.","52135dfd":" **Let's see if we can get a better model by tuning the random forest classifier:**","ebe9e118":"## **Loading the data**","09c379c3":"**Let's check the performance of the model on the testing data:**","5db43b23":"**Note:** Blue leaves represent the eligible customer for a loan i.e. **y[1]** and the orange leaves represent the non-eligible customer for a loan i.e. **y[0]**. Also, the more the number of observations in a leaf, the darker its color gets.\n\n**Observations:**\n- Credit History is the root node, which play an important role in deciding the loan eligibility of the customer. \n- Credit History, total income, and loan amount are three important factors in deciding one's loan eligibility.\n- Customer with the poor credit history and looking to quality for the \"loan amount <=129.0\" have high chance of disqualifying for the loan and falling into category of \"Non-eligible customers\"\n- Unmarried customer with good credit history, and \"total income <=6157.5\" are most likely unqualified for a loan and fall into the category of \"Non-eligible customers\"\n- Married customer with good credit history, looking to get the \"loan amount of <=259.5\" are not qualified for the loan. And they fall into the categories of the \"Non-eligible customers\" \n- Customers, even if they have an total income of <=8895.0, but unmarried have high chance of falling into the category of \"Non-eligible\" for a loan approval. \n- Married customers looking to get approval for the \"loan amount of <=127\" at \"Property_Area_Semiurban\" also have some chances of not qualifying for the loan.Thus, these customers also \"Non eligible ones\"    ","62ac698d":"## **Feature Engineering**","9557e560":"**Observations:**\n\n- Our class of interest identifies non-eligible customers (0), which means we must minimize the number of false negatives by maximizing the recall. **The recall score is the same as the training dataset (0.48) and performs poorly on the test dataset.** Besides, the precision score also drops from 0.98 (training dataset) to 0.91 on the testing dataset.","5fd01da0":"**One of the drawbacks of ensemble models is that we lose the ability to obtain an interpretation of the model. We cannot observe the decision rules for random forests the way we did for decision trees. So, let's just check the feature importances of the model.**","06020248":"### **Question 1:** \n- **Fit the decision tree classifier on the training data (use random_state=7) (2 Marks)**\n- **Check the performance on both training and testing data (use metrics_score function) (2 Marks)**\n- **Write your observations (2 Marks)**","ef1a7b4a":"## **Building Classification Models**","1083df76":"### **Question 3: Write your observations from the below visualization of the tuned decision tree (3 Marks)**","29d502a7":"**Conclusions:**\n\n- Credit history is the most important feature followed by total income and loan amount. Married_Yes, Property_Area_Semiurban, and Loan_Amount_Term are also keys feature in deciding the customer's loan eligibility.\n\n- The best model so far is the tuned decision tree model, which is giving the recall score of the ~ 62%. Tuned Decision Tree model have recall score of  ~ 62% on the test dataset. However, the tuned Random forest gives the best precision score, i.e., 91%, on the test dataset, which means there is only a 9% chance that the model will predict a customer is not eligible for a loan when the customer is actually eligible for a loan.\n\n\n**Recommendations:**\n\n- The tuned Random Forest model gives us the highest precision, 91%, on the test data. However, the recall score dropped from 61% (Tuned Decision Tree) to 48% (Tuned Random Forest) on the test data. We aim to minimize the number of false negatives and maximize the evaluation criteria (recall). However, as we notice from the random forest model, the recall score goes down when the precision score increases. This might be the case of a \"precision\/recall trade-off.\" For better results in the future, the company might try to increase the dataset's volume or use the sampling methods. \n\n- As we notice from our the model that **credit history is the important target variable in customer's loan eligibility. Thus, company (Easy House) should place major focus on customer's credit history while determining non-eligible customers.** Besides, company (Easy House) can also increase the credit score threshold while lowering the interest rate for the loan eligibility to minimize the credit risk.\n\n- **Total income and loan amount are two major feature in loan eligibility process after the credit score. Thus, company should invest time, and capital on verifying customer's total income and loan amount they are looking for approval to avoid qualifying customers as eligible for a loan when actually they are not.**  \n\n- Martial status, property area, and loan term are also others important variables in loan eligibility process. Married customer's with the higher salary are most likely to repay their principle and interest on time. \n\n- Applicant's education level (Not graduate), self-employment status (yes), total numbers of the dependent and gender (male) are also some influential target variables. ","ca6b890d":"#### Observations:\n\n- <b> Decison Tree (model) is predicting 100 result on the train dataset. ","15c27fd4":"### **Question 6:** \n\n**Write your observations on the below plot for feature importance of the random forest classifier (2 Marks)**","ceb2fe1a":"**Note:** We have explored this data earlier in the project for classification. Here, we will simply look at some basic univariate analysis and data preprocessing and move to the model building section.","54bae9a9":"**Observations:**\n\n- There are **614 observations and 13 columns** in the data.\n- ApplicantIncome, CoapplicantIncome, Loan_Amount_Term, Credit_History, and Loan_Status are numeric data types. **The rest of the variables are of the object data type.**\n- There are several columns with less than 614 non-null entries i.e. **these columns have missing values.**\n- **Loan_ID column is an identifier.** Let's check if each entry of the column is unique.","698cc1d5":"## **Random Forest Classifier**","a12dd186":"## **Random Forest Classifier - Hyperparameter Tuning**","dfd7e717":"**Let's visualize the tuned decision tree** and observe the decision rules:","1f64e362":"**Observations:**\n\n- The recall value is 0.54 on test data where as the recall value is 1 for the training dataset. Thus, the Decision Tree is working well on training dataset and its **not working very well on the test dataset.**\n\n- As per the \"precision\" score from the test data, we can conclude that there is a **35% chance that the model will predict a customer is not eligible for a loan when the customer is actually eligible for the loan, which leads to loss of opportunity**","84b50b60":"We have tuned the model. Now, **let's fit the tuned model on the training data** and check the model performance on the training and testing data.","c4bb2ab3":"- We can see that after hyperparameter tuning, the model is performing poorly on the train data as well.\n- We can try adding some other hyperparameters and\/or changing values of some hyperparameters to tune the model and see if we can get a better performance.\n\n**Note:** **GridSearchCV can take a long time to run** depending on the number of hyperparameters and the number of values tried for each hyperparameter. **Therefore, we have reduced the number of values passed to each hyperparameter.** ","26c13a99":"**Let's check the model performance on the test data:**","4d810785":"## **Dropping the Loan_ID column**"}}