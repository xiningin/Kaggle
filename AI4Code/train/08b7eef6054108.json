{"cell_type":{"022b082f":"code","1d777381":"code","c8e4b65f":"code","eee557a9":"code","8b698af5":"code","d8bb31ee":"code","afdd94c6":"code","01ebc745":"code","3f62d45d":"code","a1df32c4":"code","6dcc0e1a":"code","82b9e436":"code","60b545cc":"code","b70461bc":"code","bb79dd1c":"code","911cd2dd":"code","52d6b1e9":"code","2c29c5bd":"code","71688db3":"code","d758992e":"code","35b2c0fd":"code","9a70d6fa":"code","c19a5225":"code","619028df":"code","a9493e59":"code","02790eb5":"code","0cf219ca":"code","eb0dfc2f":"code","a834e84f":"code","a7c74f49":"code","5c010e3a":"code","45fdbd16":"code","e980d3af":"code","efb92264":"code","b766c183":"code","3f6bb6dc":"code","4140f56b":"code","a6dc84e4":"code","0d18cfef":"code","463ba9c1":"code","6f813069":"code","c5487147":"code","18aedc72":"code","378117ad":"code","fe8334e0":"code","7c64f2e1":"code","2cc31cc4":"code","9cc9cc72":"code","8dd80df2":"code","4139cb89":"code","e12c5833":"code","f37d4cf5":"code","14d1b026":"code","5f5fd846":"code","90769473":"code","70250994":"code","11391165":"code","55933a76":"code","8869ca10":"code","7901c191":"code","577ef11a":"code","a5481699":"markdown","fc231ab9":"markdown","9e689568":"markdown","c4a0177b":"markdown","3bef1062":"markdown","94d9771d":"markdown","d19ea690":"markdown","3dfb9311":"markdown","560cecc4":"markdown","f9236f35":"markdown","aa131275":"markdown","75d14f2b":"markdown","b6363f8f":"markdown","c5dab7cd":"markdown","8f526b18":"markdown","498326f3":"markdown","cfe845c0":"markdown","193b61a8":"markdown","9a439cbd":"markdown","1a1b5dee":"markdown","6d7f0b78":"markdown","13223e96":"markdown"},"source":{"022b082f":"#Import all the required libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nfrom PIL import Image\nfrom collections import Counter\nfrom tqdm import tqdm\nimport time\nimport os\nimport random\nfrom IPython.display import display\nfrom PIL import ImageFont\nfrom PIL import ImageDraw \nimport seaborn as sns\nfrom pickle import dump,load\nimport time\n\n","1d777381":"#Import the dataset and read the image into a seperate variable\n\nimages='\/kaggle\/input\/flickr8k\/Images'\n\nall_imgs = glob.glob(images + '\/*.jpg',recursive=True)\nprint(\"The total images present in the dataset: {}\".format(len(all_imgs)))","c8e4b65f":"#Visualise both the images & text present in the dataset\nImage.open(all_imgs[random.randrange(20, 50, 3)])\n","eee557a9":"#Import the dataset and read the text file into a seperate variable\n\ndef load_doc(filename):\n    \n    #your code here\n    text=open(filename).read()   \n    return text\n   \ntext_file = '\/kaggle\/input\/flickr8k\/captions.txt'\ndoc = load_doc(text_file)\nprint(doc[:300])","8b698af5":"def get_img_ids_and_captions(text):\n    keys=[]\n    values=[]\n    key_paths=[]\n    text=text.splitlines()[1:]\n    for line in text:\n        com_idx=line.index(\",\")\n        im_id,im_cap=line[:com_idx],line[com_idx+1:]\n        keys.append(im_id)\n        values.append(im_cap)\n        key_paths.append(images+'\/'+im_id)\n    return keys,values,key_paths\n\nall_img_id,annotations,all_img_vector= get_img_ids_and_captions(doc)\ndf = pd.DataFrame(list(zip(all_img_id, all_img_vector,annotations)),columns =['ID','Path', 'Captions']) \n    \ndf","d8bb31ee":"random_num =random.randrange(20, 50, 3)","afdd94c6":"print(all_img_id[random_num])","01ebc745":"print(annotations[random_num])","3f62d45d":"Image.open(all_img_vector[random_num])","a1df32c4":"#Create a list which contains all the captions\nannotations=df.Captions.apply(lambda z:\"<start>\"+\" \"+z+\" \"+\"<end>\")\n\n#add the <start> & <end> token to all those captions as well\n\n#Create a list which contains all the path to the images\nall_img_path=df.Path.to_list()\n\nprint(\"Total captions present in the dataset: \"+ str(len(annotations)))\nprint(\"Total images present in the dataset: \" + str(len(all_img_path)))","6dcc0e1a":"#Create the vocabulary & the counter for the captions\ndef voc_fetcher(frame,column):\n    out=[]\n    for i in frame[column]:\n        out+=i.split(\" \")\n    return out\n\n\nvocabulary=voc_fetcher(df,\"Captions\")\nval_count=Counter(vocabulary)\nval_count","82b9e436":"#Visualise the top 30 occuring words in the captions\nimport matplotlib.pyplot as plt\ndef get_top_words_based_on_cnt(words_dict,n_words):\n    n_words+=1\n    keys=list(words_dict.keys())\n    values=list(words_dict.values())\n    sorted_values=sorted(values,reverse=True)[:n_words]\n    sorted_keys=[]\n    for i in sorted_values:\n        if sorted_values.count(i)==1:\n            sorted_keys.append(keys[values.index(i)])\n        elif sorted_values.count(i)==2:\n            f_idx=values.index(i)\n            s_idx=values[f_idx+1:].index(i)\n            s_idx+=f_idx+1\n            a,b=keys[f_idx],keys[s_idx]\n            if a not in sorted_keys and b not in sorted_keys:\n                sorted_keys.append(a)\n                sorted_keys.append(b)\n    plt.figure(figsize=(10,6))\n    sns.barplot(x=sorted_keys,y=sorted_values)\n    plt.xticks(rotation=60)\n    plt.show()\n    #write your code here\nget_top_words_based_on_cnt(val_count,30)","60b545cc":"def caption_and_image_plotter(image_id,frame):\n    #get captions\n    caps=(\"\\n\"*2).join(frame[frame['ID']==image_id].Captions.to_list())\n    fig, ax = plt.subplots()\n    ax.set_axis_off()\n    idx=df.ID.to_list().index(image_id)\n    im=Image.open(df.Path.iloc[idx])\n    w,h=im.size[0],im.size[-1]\n    ax.imshow(im)\n    ax.text(w+50,h,caps,fontsize=20,color='green')\ncaption_and_image_plotter(df.ID.iloc[8049],df)","b70461bc":"def run_caption_and_image_plotter_for_a_range(start,end,frame):\n    for i in range(start,end):\n        caption_and_image_plotter(frame.ID.drop_duplicates().iloc[i],frame)\nrun_caption_and_image_plotter_for_a_range(0,5,df)","bb79dd1c":"#importing keras related imports\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nimport tensorflow as tf\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import train_test_split\n\nfrom nltk.corpus import stopwords\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n                         Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization, Conv2D\nfrom tensorflow.keras.optimizers import Adam, RMSprop\n\nfrom keras.layers.wrappers import Bidirectional\nfrom keras.layers.merge import add\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom tensorflow.keras import Input, layers\nfrom tensorflow.keras import optimizers\nfrom keras.applications.inception_v3 import preprocess_input\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tqdm import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","911cd2dd":"# create the tokenizer\ntop_word_cnt = 5000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_word_cnt,oov_token=\"<unk>\",filters='!\"#$%&()*+.,-\/:;=?@[\\]^_`{|}~ ')\ntokenizer.fit_on_texts(annotations)\ntrain_seqs = tokenizer.texts_to_sequences(annotations)","52d6b1e9":"tokenizer.word_counts","2c29c5bd":"# Create word-to-index and index-to-word mappings.\n\ntrain_seqs[:5]\ntokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'\ntrain_seqs = tokenizer.texts_to_sequences(annotations)\ntokenizer.word_counts","71688db3":"# Create a word count of your tokenizer to visulize the Top 30 occuring words after text processing\nget_top_words_based_on_cnt(tokenizer.word_counts,30)\n","d758992e":"def min_max_for_nested_array(nested_array):\n    array=[len(e) for e in nested_array]\n    return min(array),max(array)\nmin_l,max_l=min_max_for_nested_array(train_seqs)","35b2c0fd":"# Pad each vector to the max_length of the captions ^ store it to a vairable\n\ncap_vector= tf.keras.preprocessing.sequence.pad_sequences(train_seqs,padding='post',maxlen=max_l)\n\nprint(\"The shape of Caption vector is :\" + str(cap_vector.shape))","9a70d6fa":"#write your code here to create the dataset consisting of image paths\ndef load_the_image(file_path):\n    my_img = tf.io.read_file(file_path)\n    my_img = tf.image.decode_jpeg(my_img, channels=3)\n    my_img = tf.image.resize(my_img, (299, 299))\n    my_img = tf.keras.applications.inception_v3.preprocess_input(my_img)\n    return my_img,file_path","c19a5225":"encode_train_set = sorted(set(all_img_vector))\n\nfeature_dict = {}\n\nimage_data_set = tf.data.Dataset.from_tensor_slices(encode_train_set)\nimage_data_set = image_data_set.map(load_the_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(32)\n\n\n\n","619028df":"image_data_set","a9493e59":"image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n\nnew_input = image_model.input \nhidden_layer = image_model.layers[-1].output \n\nimage_features_extract_model = keras.Model(new_input, hidden_layer) ","02790eb5":"image_features_extract_model.summary()","0cf219ca":"# write the code to apply the feature_extraction model to your earlier created dataset which contained images & their respective paths\n# Once the features are created, you need to reshape them such that feature shape is in order of (batch_size, 8*8, 2048)\nfor image,path in tqdm(image_data_set):\n    batch_features = image_features_extract_model(image)\n    batch_features = tf.reshape(batch_features,(batch_features.shape[0], -1, batch_features.shape[3]))\n    for batch_f, p in zip(batch_features, path):\n        path_of_feature = p.numpy().decode(\"utf-8\")\n        feature_dict[path_of_feature] =  batch_f.numpy()\n","eb0dfc2f":"#write your code here\n\nimage_train, image_test, caption_train, caption_test = train_test_split(all_img_vector,\n                                                                        cap_vector,\n                                                                        test_size=0.2,\n                                                                        random_state=33)","a834e84f":"print(\"Training data for images: \" + str(len(image_train)))\nprint(\"Testing data for images: \" + str(len(image_test)))\nprint(\"Training data for Captions: \" + str(len(caption_train)))\nprint(\"Testing data for Captions: \" + str(len(caption_test)))","a7c74f49":"# Create a function which maps the image path to their feature. \n# This function will take the image_path & caption and return it's feature & respective caption.\n\ndef map_function(image_name,capt):\n    image_tensor = feature_dict[image_name.decode('utf-8')]\n    return image_tensor,capt","5c010e3a":"# create a builder function to create dataset which takes in the image path & captions as input\n# This function should transform the created dataset(img_path,cap) to (features,cap) using the map_func created earlier\n\nBATCH_SIZE = 32\nBUFFER_SIZE = 1000\ndef generate_dataset(images_data, captions_data):\n    \n    dataset = tf.data.Dataset.from_tensor_slices((images_data, captions_data))\n    dataset = dataset.shuffle(BUFFER_SIZE)\n\n    dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n          map_function, [item1, item2], [tf.float32, tf.int32]),\n          num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(BATCH_SIZE)\n\n\n    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n    return dataset\n\n","45fdbd16":"train_dataset=generate_dataset(image_train,caption_train)\ntest_dataset=generate_dataset(image_test,caption_test)","e980d3af":"sample_img_batch, sample_cap_batch = next(iter(train_dataset))\nprint(sample_img_batch.shape)  #(batch_size, 8*8, 2048)\nprint(sample_cap_batch.shape) #(batch_size,40)","efb92264":"embedding_dim = 256 \nunits = 512\nvocab_size = 5001 #top 5,000 words +1\ntrain_num_steps = len(image_train) \/\/ BATCH_SIZE\ntest_num_steps = len(image_test) \/\/ BATCH_SIZE","b766c183":"class Encoder(Model):\n    def __init__(self,embed_dim):\n        super(Encoder, self).__init__()\n        self.fc = layers.Dense(embed_dim)\n        self.dropout = layers.Dropout(0.5)\n        \n    def call(self, features):\n        features = self.fc(features)\n        features = tf.nn.relu(features)\n        return features","3f6bb6dc":"encoder=Encoder(embedding_dim)","4140f56b":"class Attention_model(Model):\n    def __init__(self, units):\n        super(Attention_model, self).__init__()\n        self.W1 = layers.Dense(units)\n        self.W2 = layers.Dense(units)\n        self.V = layers.Dense(1)\n        self.units=units\n\n    def call(self, features, hidden):\n        hidden_with_time_axis =  tf.expand_dims(hidden, 1)\n        score = keras.activations.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n        attention_weights =  keras.activations.softmax(self.V(score), axis=1)\n        context_vector = attention_weights * features\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n        \n\n        return context_vector, attention_weights","a6dc84e4":"class RNN_Decoder(Model):\n    def __init__(self, embedding_dim, units, vocab_size):\n\n        super(RNN_Decoder, self).__init__()\n        self.units = units\n\n        self.embedding = layers.Embedding(vocab_size, embedding_dim)\n        self.gru = layers.GRU(self.units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n        \n        self.fc1 = layers.Dense(self.units)\n        self.fc2 = layers.Dense(vocab_size)\n\n        self.attention = Attention_model(self.units)\n\n    def call(self, x, features, hidden):\n    # defining attention as a separate model\n        context_vector, attention_weights = self.attention(features, hidden)\n\n    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n        x = self.embedding(x)\n\n    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n    # passing the concatenated vector to the GRU\n        output, state = self.gru(x)\n\n    # shape == (batch_size, max_length, hidden_size)\n        x = self.fc1(output)\n\n    # x shape == (batch_size * max_length, hidden_size)\n        x = tf.reshape(x, (-1, x.shape[2]))\n\n    # output shape == (batch_size * max_length, vocab)\n        x = self.fc2(x)\n\n        return x, state, attention_weights\n\n    def init_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","0d18cfef":"class Decoder(Model):\n    def __init__(self, embed_dim, units, vocab_size):\n        super(Decoder, self).__init__()\n        self.units=units\n        self.attention = Attention_model(self.units)\n        self.embed = layers.Embedding(vocab_size, embed_dim,mask_zero=False)\n        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n        self.d1 = layers.Dense(self.units)\n        self.d2 = layers.Dense(vocab_size)   \n        self.dropout = Dropout(0.5)\n\n    def call(self,x,features, hidden):\n        context_vector, attention_weights = self.attention(features, hidden)\n        embed = self.dropout(self.embed(x)) \n        mask = self.embed.compute_mask(x)\n        embed =  tf.concat([tf.expand_dims(context_vector, 1), embed], axis=-1)\n        output,state = self.gru(embed,mask=mask)\n        output = self.d1(output)\n        output = tf.reshape(output, (-1, output.shape[2])) \n        output = self.d2(output) \n\n        return output,state, attention_weights\n    \n    def init_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","463ba9c1":"decoder=Decoder(embedding_dim, units, vocab_size)","6f813069":"features=encoder(sample_img_batch)\n\nhidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\ndec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n\npredictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\nprint('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\nprint('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\nprint('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)","c5487147":"import tensorflow\nfrom tensorflow.keras.optimizers import Adam, RMSprop\n\noptimizer = tensorflow.keras.optimizers.Adam()\nloss_object = keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')","18aedc72":"def loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)","378117ad":"checkpoint_path = \"flickr8k\/File4\/\"\nckpt = tf.train.Checkpoint(encoder=encoder,\n                           decoder=decoder,\n                           optimizer = optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt,\n                                          checkpoint_path,\n                                          max_to_keep=5)","fe8334e0":"start_epoch = 0\nif ckpt_manager.latest_checkpoint:\n    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])","7c64f2e1":"@tf.function\ndef train_step(img_tensor, target):\n    loss = 0\n    hidden = decoder.init_state(batch_size=target.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n    with tf.GradientTape() as tape:\n        features = encoder(img_tensor)\n        for i in range(1, target.shape[1]):\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n            loss += loss_function(target[:, i], predictions)\n            dec_input = tf.expand_dims(target[:, i], 1)\n        avg_loss = (loss\/int(target.shape[1]))\n        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n        gradients = tape.gradient(loss, trainable_variables)\n        optimizer.apply_gradients(zip(gradients, trainable_variables))\n        return loss, avg_loss","2cc31cc4":"# @tf.function\ndef test_step(img_tensor, target):\n    loss = 0\n    hidden = decoder.init_state(batch_size=target.shape[0])\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n\n    with tf.GradientTape() as tape:\n        features = encoder(img_tensor)\n\n        for i in range(1, target.shape[1]):\n            predictions, hidden, _ = decoder(dec_input, features, hidden)\n            loss += loss_function(target[:, i], predictions)\n            dec_input = tf.expand_dims(target[:, i], 1)\n\n        avg_loss = (loss \/ int(target.shape[1]))\n\n        trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n        gradients = tape.gradient(loss, trainable_variables)\n        optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n        return loss, avg_loss","9cc9cc72":"def test_loss_cal(test_dataset):\n    total_loss = 0\n    for (batch, (img_tensor, target)) in enumerate(test_dataset):\n        batch_loss, t_loss = test_step(img_tensor, target)\n        total_loss += t_loss\n    avg_test_loss=total_loss\/test_num_steps\n    return avg_test_loss","8dd80df2":"loss_plot = []\ntest_loss_plot = []\nEPOCHS = 15\n\nbest_test_loss=100\nfor epoch in tqdm(range(0, EPOCHS)):\n    start = time.time()\n    total_loss = 0\n\n    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n        batch_loss, t_loss = train_step(img_tensor, target)\n        total_loss += t_loss\n        avg_train_loss=total_loss \/ train_num_steps\n        \n    loss_plot.append(avg_train_loss)    \n    test_loss = test_loss_cal(test_dataset)\n    test_loss_plot.append(test_loss)\n    \n    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n    \n    if test_loss < best_test_loss:\n        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n        best_test_loss = test_loss\n        ckpt_manager.save()","4139cb89":"plt.plot(loss_plot)\nplt.plot(test_loss_plot)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","e12c5833":"def evaluate(image):\n    max_length=max_l\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.init_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_the_image(image)[0], 0) #process the input image to desired format before extracting features\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n    result = []\n\n    for i in range(max_length):\n        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n\n        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n\n        predicted_id = tf.argmax(predictions[0]).numpy()\n        result.append(tokenizer.index_word[predicted_id])\n\n        if tokenizer.index_word[predicted_id] == '<end>':\n            return result, attention_plot,predictions\n\n        dec_input = tf.expand_dims([predicted_id], 0)\n\n    attention_plot = attention_plot[:len(result), :]\n    return result, attention_plot,predictions","f37d4cf5":"def beam_evaluate(image, beam_index = 3):\n    max_length=max_l\n    start = [tokenizer.word_index['<start>']]\n    result = [[start, 0.0]]\n\n    attention_plot = np.zeros((max_length, attention_features_shape))\n\n    hidden = decoder.init_state(batch_size=1)\n\n    temp_input = tf.expand_dims(load_the_image(image)[0], 0)\n    img_tensor_val = image_features_extract_model(temp_input)\n    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n\n    features = encoder(img_tensor_val)\n\n    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n\n    while len(result[0][0]) < max_length:\n        i=0\n        temp = []\n        for s in result:\n            predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n            attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n            i=i+1\n            word_preds = np.argsort(predictions[0])[-beam_index:]\n          \n            for w in word_preds:\n                next_cap, prob = s[0][:], s[1]\n                next_cap.append(w)\n            \n                prob += np.log(predictions[0][w])\n                    \n                temp.append([next_cap, prob])\n        result = temp\n        result = sorted(result, reverse=False, key=lambda l: l[1])\n        result = result[-beam_index:]\n        \n        \n        predicted_id = result[-1]\n        pred_list = predicted_id[0]\n        \n        prd_id = pred_list[-1] \n        if(prd_id!=3):\n            dec_input = tf.expand_dims([prd_id], 0)  \n        else:\n            break\n    \n    \n    result2 = result[-1][0]\n    \n    intermediate_caption = [tokenizer.index_word[i] for i in result2]\n    final_caption = []\n    for i in intermediate_caption:\n        if i != '<end>':\n            final_caption.append(i)\n            \n        else:\n            break\n\n    attention_plot = attention_plot[:len(result), :]\n    final_caption = ' '.join(final_caption[1:])\n    return final_caption\n\n","14d1b026":"def plot_attmap(caption, weights, image):\n\n    fig = plt.figure(figsize=(10, 10))\n    temp_img = np.array(Image.open(image))\n    \n    len_cap = len(caption)\n    for cap in range(len_cap):\n        weights_img = np.reshape(weights[cap], (8,8))\n        weights_img = np.array(Image.fromarray(weights_img).resize((224, 224), Image.LANCZOS))\n        \n        ax = fig.add_subplot(len_cap\/\/2, len_cap\/\/2, cap+1)\n        ax.set_title(caption[cap], fontsize=15)\n        \n        img=ax.imshow(temp_img)\n        \n        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent())\n        ax.axis('off')\n    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n    plt.show()","5f5fd846":"from nltk.translate.bleu_score import sentence_bleu","90769473":"def filt_text(text):\n    filt=['<start>','<unk>','<end>'] \n    temp= text.split()\n    [temp.remove(j) for k in filt for j in temp if k==j]\n    text=' '.join(temp)\n    return text","70250994":"import numpy as np","11391165":"features_shape = batch_f.shape[1]\nattention_features_shape = batch_f.shape[0]","55933a76":"rid = np.random.randint(0, len(image_test))\nprint(rid)\ntest_image = image_test[rid]\n\nreal_caption = ' '.join([tokenizer.index_word[i] for i in caption_test[rid] if i not in [0]])\nresult, attention_plot,pred_test = evaluate(test_image)\n\n\nreal_caption=filt_text(real_caption)      \n\n\npred_caption=' '.join(result).rsplit(' ', 1)[0]\n\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = pred_caption.split()\n\nscore = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\nprint(f\"BELU score: {score*100}\")\n\nprint('Real Caption:', real_caption)\nprint('Prediction Caption:', pred_caption)\nplot_attmap(result, attention_plot, test_image)\n\n\nImage.open(test_image)","8869ca10":"captions=beam_evaluate(test_image)\nprint(captions)","7901c191":"! pip install gTTS","577ef11a":"from gtts import gTTS\nimport os\n\n# Language in which you want to convert\nlanguage = 'en'\n  \n# Passing the text and language to the engine, \nmyobj = gTTS(text=pred_caption, lang=language, slow=False)\n  \n# Saving the converted audio in a mp3 file named\nmyobj.save(\"Predicted_text.mp3\")\n  \n# Playing the converted file\nos.system(\".\/Predicted_text.mp3\")","a5481699":"### Encoder","fc231ab9":"## Model Building\n1.Set the parameters\n\n2.Build the Encoder, Attention model & Decoder","9e689568":"* While creating the training step for your model, you will apply Teacher forcing.\n* Teacher forcing is a technique where the target\/real word is passed as the next input to the decoder instead of previous prediciton.","c4a0177b":"### FAQs on how to load the features:\n* You can load the features using the dictionary created earlier OR\n* You can store using numpy(np.load) to load the feature vector.","3bef1062":"### Greedy Search","94d9771d":"## Load the pretrained Imagenet weights of Inception net V3\n\n1.To save the memory(RAM) from getting exhausted, extract the features of the images using the last layer of pre-trained model. Including this as part of training will lead to higher computational time.\n\n2.The shape of the output of this layer is 8x8x2048. \n\n3.Use a function to extract the features of each image in the train & test dataset such that the shape of each image should be (batch_size, 8*8, 2048)\n\n","d19ea690":"### Decoder","3dfb9311":"### FAQs on how to resize the images::\n* Since you have a list which contains all the image path, you need to first convert them to a dataset using <i>tf.data.Dataset.from_tensor_slices<\/i>. Once you have created a dataset consisting of image paths, you need to apply a function to the dataset which will apply the necessary preprocessing to each image. \n* This function should resize them and also should do the necessary preprocessing that it is in correct format for InceptionV3.\n","560cecc4":"Let's read the dataset","f9236f35":"## Model Evaluation\n1.Define your evaluation function using greedy search\n\n2.Define your evaluation function using beam search ( optional)\n\n3.Test it on a sample data using BLEU score","aa131275":"## Pre-Processing the captions\n1.Create the tokenized vectors by tokenizing the captions fore ex :split them using spaces & other filters. \nThis gives us a vocabulary of all of the unique words in the data. Keep the total vocaublary to top 5,000 words for saving memory.\n\n2.Replace all other words with the unknown token \"UNK\" .\n\n3.Create word-to-index and index-to-word mappings.\n\n4.Pad all sequences to be the same length as the longest one.","75d14f2b":"#### NOTE: \n* Since there is a difference between the train & test steps ( Presence of teacher forcing), you may observe that the train loss is decreasing while your test loss is not. \n* This doesn't mean that the model is overfitting, as we can't compare the train & test results here, as both approach is different.\n* Also, if you want to achieve better results you can run it more epochs, but the intent of this capstone is to give you an idea on how to integrate attention mechanism with E-D architecture for images. The intent is not to create the state of art model. ","b6363f8f":"## Dataset creation\n1.Apply train_test_split on both image path & captions to create the train & test list. Create the train-test spliit using 80-20 ratio & random state = 42\n\n2.Create a function which maps the image path to their feature. \n\n3.Create a builder function to create train & test dataset & apply the function created earlier to transform the dataset\n\n2.Make sure you have done Shuffle and batch while building the dataset\n\n3.The shape of each image in the dataset after building should be (batch_size, 8*8, 2048)\n\n4.The shape of each caption in the dataset after building should be(batch_size, max_len)\n","c5dab7cd":"### FAQs on how to store the features:\n* You can store the features using a dictionary with the path as the key and values as the feature extracted by the inception net v3 model OR\n* You can store using numpy(np.save) to store the resulting vector.","8f526b18":"## Pre-processing the images\n\n1.Resize them into the shape of (299, 299)\n\n3.Normalize the image within the range of -1 to 1, such that it is in correct format for InceptionV3. ","498326f3":"### Attention model","cfe845c0":"Create a dataframe which summarizes the image, path & captions as a dataframe\n\nEach image id has 5 captions associated with it therefore the total dataset should have 40455 samples.","193b61a8":"## Model training & optimization\n1.Set the optimizer & loss object\n\n2.Create your checkpoint path\n\n3.Create your training & testing step functions\n\n4.Create your loss function for the test dataset","9a439cbd":"* While creating the test step for your model, you will pass your previous prediciton as the next input to the decoder.","1a1b5dee":"## Data understanding\n1.Import the dataset and read image & captions into two seperate variables\n\n2.Visualise both the images & text present in the dataset\n\n3.Create a dataframe which summarizes the image, path & captions as a dataframe\n\n4.Create a list which contains all the captions & path\n\n5.Visualise the top 30 occuring words in the captions\n\n","6d7f0b78":"# EYE FOR BLIND\nThis notebook will be used to prepare the capstone project 'Eye for Blind'","13223e96":"### Beam Search(optional)"}}