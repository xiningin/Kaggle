{"cell_type":{"2776b654":"code","69112232":"code","5c190375":"code","02b4cf81":"code","f1ee33ae":"code","f924f392":"code","f913c2c3":"code","6d2df385":"code","8caad1e0":"code","f6d8b849":"code","cdfcffc0":"code","1f4dddae":"code","e591d067":"code","35aad9d0":"code","228137ca":"code","035de6e1":"code","6f5b0c31":"code","3f014c20":"code","b22a187c":"markdown","5ab4fdc0":"markdown","f436f089":"markdown","a8c4ff3f":"markdown","62c1640d":"markdown","6ed2dc32":"markdown","b4f30449":"markdown","82b12738":"markdown","43fa1a56":"markdown","f8af97ff":"markdown","c045dd48":"markdown","e158609e":"markdown","976f7bc7":"markdown","7a3b90ac":"markdown","50241b08":"markdown","9013a314":"markdown"},"source":{"2776b654":"import tensorflow as tf\n\n# Loading some tests to verify your answers to the exercises below\nfrom shutil import copyfile\ncopyfile(src = \"..\/input\/tfdata-notebook-tests\/tfdata_notebook_tests.py\", dst = \"..\/working\/tfdata_notebook_tests.py\")\nfrom tfdata_notebook_tests import *\n\n# Let's work in the tensorflow's eager mode, which is more intuitive\ntf.enable_eager_execution()","69112232":"# We will read two columns, both are type tf.float32, if empty, assign 0.0 as its value\nrecord_defaults = [tf.constant([0.0], dtype=tf.float32)] * 2\n\nmelbourne_file_path = '..\/input\/melbourne-housing-market\/Melbourne_housing_FULL.csv'\n\n# Let's now read the CSV file and create our Dataset\n# In addition to passing the file name (notice that it's an array, so we can supply multiple files),\n# we also indicate that the first row of the CSV file is a header row, so we can skip it.\n# Let's only select the 'Landsize' and 'Price' columns (columns number 4 and 13)\ndataset = tf.data.experimental.CsvDataset([melbourne_file_path], \n                                          record_defaults, \n                                          header=True, \n                                          select_cols=[4, 13]) ","5c190375":"for idx, row in enumerate(dataset):\n    print (row)\n    if idx > 3:\n        break","02b4cf81":"# TODO: Shuffle the dataset\nshuffled_dataset = ","f1ee33ae":"# TODO: create mini-batches of size 4\nbatched_dataset = ","f924f392":"for idx, row in enumerate(batched_dataset):\n    print ('Sample #', idx, ' shape: (', row[0].get_shape(), ',', row[1].get_shape(), ')')\n    if idx > 3:\n        break","f913c2c3":"test_batched_dataset(batched_dataset)","6d2df385":"def convert_aus_to_usd(price, landsize):\n    return price * 0.7, landsize\n\ndataset_usd = dataset.map(convert_aus_to_usd)","8caad1e0":"for idx, row in enumerate(tf.data.Dataset.zip((dataset, dataset_usd))):\n    print ('Sample #', idx, ' Price: $', row[0][0].numpy(), 'AUD, ', row[1][0].numpy(), 'USD')\n    if idx > 3:\n        break","f6d8b849":"def convert_meters_to_feet(price, landsize):\n    # TODO: convert landsize to square feet\n    return  , \n\ndataset_feet = dataset.map(convert_meters_to_feet)","cdfcffc0":"for row in tf.data.Dataset.zip((dataset.take(5), dataset_feet.take(5))):\n    print ( 'Landsize in sq meters: ', row[0][1].numpy(), ', Landsize in sq feet:', row[1][1].numpy(), '')\n","1f4dddae":"test_landsize_conversion(dataset, dataset_feet)","e591d067":"def filter_empty_values(price, landsize):    \n    return tf.cond(\n        tf.math.logical_or(tf.equal(price, 0.0),\n                            tf.equal(landsize, 0.0)), \n        lambda: False, \n        lambda: True\n    )","35aad9d0":"dataset_filtered = dataset.filter(filter_empty_values)","228137ca":"for idx, row in enumerate(dataset):\n    print ('Sample #', idx, ' Price: $', row[0].numpy(), ', Landsize:', row[1].numpy(), '')\n    if idx > 3:\n        break","035de6e1":"def filter_smaller_houses(price, landsize):  \n    # TODO: Complete the filtering function False would be retruned if\n    # landsize was less than 200\n    return tf.cond(     , \n        lambda: False, \n        lambda: True\n    )\ndataset_small_filtered = dataset.filter(filter_smaller_houses)\n\nfor idx, row in enumerate(dataset_small_filtered):\n    print ('Sample #', idx, ' Landsize:', row[1].numpy(), '')\n    if idx > 3:\n        break","6f5b0c31":"test_small_house_filtering(dataset_small_filtered)","3f014c20":"# # Nevermind this part. Just verification for the tests that verify exercise answers\n# # Should fail first assertion \n# test_batched_dataset([1,2])\n\n# # Should fail second assertion \n# batched_dataset_2 = shuffled_dataset.batch(2)\n# test_batched_dataset(batched_dataset_2)\n\n# # Should fail second assertion \n# test_landsize_conversion(dataset, dataset)\n\n# # Fails first assertion\n# test_small_house_filtering([1])\n\n# # Fails second assertion\n# test_small_house_filtering(dataset)","b22a187c":"### Exercise: Batch\nLet's create mini-batches, each containing 4 samples. Use the [batch()](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#batch)  method on `shuffled_dataset`:","5ab4fdc0":"Peak inside `batched_dataset`. If we've done things correctly, each example should have tensors of shape `(4,)` inside a tuple:","f436f089":"Simple test to verify your filtering answer:","a8c4ff3f":"### Exercise: Filter Houses Smaller than 200 meters\nTry to write your own filter function to filter out houses where the landsize is less than 200. ([hint](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/math\/less))","62c1640d":"Simple test to verify your conversion answer:","6ed2dc32":"\n\n### Filtering with Dataset.filter()\nLooking at the values, we can see a problem we need to address. A number of tensors have the value 0.0. Feeding those to a model would confuse it and lead to no good. So let's use [Dataset.filter()](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#filter) to remove those records from the dataset.\n\nDataset.filter() follows the common filter pattern used in functional programming -- it takes a predicate function and runs it against every sample -- if the predicate returns True for the sample, then the sample is included in the returned dataset. If that predicate  returns False for a sample, then the sample is filtered and not included in the dataset.\n\nThe logic for this predicate function goes like this (if we were to write it as a simple python function):\n```\ndef filter_empty_values(price, landsize):\n    if price == 0.0 or landsize == 0.0:\n        return False\n    else\n        return True\n```\nWe can think of that as looking like this:\n<br \/>\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data-filter.png\" \/>\n\nWe'll define this function in tensorflow operation form, which looks like this:","b4f30449":"### Transformation with Dataset.map()\nCrikey! The price column in our dataset is in Australian dollars. Let's convert it to US dollars using `map()`. Let's use the conversaion rate: \\$1 Australian = $0.7 USD.\n\nThe [map()](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#map) function follows the common `map` paradigm -- it takes a functions, applies it to each element of the dataset, and constructs a new Dataset from all the return values.","82b12738":"We can treat `dataset` as an iterator. Simply doing `for row in dataset:` would loop through the dataset.\n\nWe can see that it returned the columns we selected. Each record is in the shape (tensor, tensor). The `numpy` parameter in the tensor is the actual value of the tensor.\n\n### Exercise: Shuffle\nTry shuffling the dataset using the [shuffle()](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset#shuffle) method:","43fa1a56":"# Intro to Building Data Input Pipelines With tf.data\n\nPreparing a dataset and feeding it to a machine learning model are two cornerstone activities in the machine learning process. If you are using TensorFlow, then [tf.data](https:\/\/www.tensorflow.org\/guide\/datasets) is the recommended module for you to build your data input pipeline. This goes for beginners who just want to try out a linear regression model using the [tf.estimators](https:\/\/www.tensorflow.org\/guide\/estimators) API using a small numpy dataset. But it also goes for power-users operating on terabytes of image data who need to apply transformations to the dataset and feed them in parallel to a cluster of hungry GPUs at blazing speeds.\n\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data.png\" \/>\n\n## Consuming Data\n\nThe first step in a data pipeline is often to read data stored somewhere, tf.data can consumer a number of data formats (in-memory or as files from local disk, Google Cloud Storage, S3, or elsewhere):\n\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data-read-data.png\" \/>\n\nIf you come from a database or data warehousing background, you should be able to spot the pattern being used here -- tf.data is indeed an API to build an [ETL (Extract, Transform, and Load) pipeline](https:\/\/en.wikipedia.org\/wiki\/Extract,_transform,_load) to feed data to a tensorflow model. If you're new to this concept, an ETL process is simply the procedure of copying and transforming data from a source system\/representation to a target system\/representation. \n\n## Processing a Dataset With Transformations\n\nIn ML usecases, common next steps after consuming data would be to shuffle the dataset, then break it into mini-batches. These steps fall under the \"Transform\" umbrella:\n\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data-simple-pipeline.png\" \/>\n\nThe central abstraction that tf.data uses throughout this process is [tf.data.Dataset](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset) -- a representation of a dataset that a TensorFlow can immediately consume as a sequence of tensors.\n\n## Evolution of a Pipeline\n### Consuming data from CSV \nLet's look at the process of creating and transforming a Dataset. We'll start by consuming a CSV dataset, and then do the simple steps mentioned above (create Dataset, shuffle, then break into mini-batches of size 2):\n\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data-pipeline-1.png\" \/>\n\nIn the first step, tf.data reads the CSV file and creates a Dataset object representing the dataset. If we're to pass this Dataset to the model, it would take one of the rows in each training iteration. It's important to note that the Dataset object does not make these transformations right away -- if the a dataset is 2 TB in size and the CPU tf.data is running on only has 32GBs of RAM available, we'd be in trouble. The Dataset object acknowledges the processing plan and the transformations required, and then applies them when needed on a batch-by-batch basis.\n\n### Shuffling the examples\nIf we were to shuffle() the Dataset, we can think of it as looking like this:\n\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data-pipeline-2.png\" \/>\n\nYou can see the order of the examples has been shuffled. \n\n### Repeating for several epochs\nNow, models are trained over multiple epochs -- with the training dataset being fed to the model in each epoch. So let's tell tf.data that we want to use the Dataset for two epochs. That's done using the repeat() method:\n\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data-pipeline-3.png\" \/>\n\nYou can see that we now have double the number of rows -- the first half would be epoch #1 and the second half is epoch number #2.\n\n### Creating mini-batches\nAll that's left now is to specify a batch size using batch():\n\n<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data-pipeline-4.png\" \/>\n\nAnd now we have a proper Dataset we can hand over to our model for training. \n\nYou might have noticed that the pattern of these calls allows us to chain all these API calls to make our code more concise. And you would be right. The code can be rewrittern as follows:\n```\ndataset = tf.data.experimental.CsvDataset(\u201cdataset.csv\u201d, record_defaults)\ndataset = dataset.shuffle(10000).repeat(2).batch(2)\n```\n\nA possible next step from here is to create an [iterator](https:\/\/www.tensorflow.org\/guide\/datasets#creating_an_iterator) to specify how the data is fed to your model. Since we're in eager mode, we can simply loop over the Dataset object, however.\n\n## Code Example\nLet's dive into a simple code example. For this example, we'll use the [Melbourne housing](https:\/\/www.kaggle.com\/anthonypino\/melbourne-housing-market) dataset from Kaggle. \n\n","f8af97ff":"And now we have the first version of our dataset. Let's peak at the first 5 elements:","c045dd48":"\n## Where to go next\n\nOnce you're happy with the shape and contents of your Dataset, you can proceed to supplying it to your model. This can be through high-level APIs like tf.estimators which need their dataset as a tf.data.Dataset. But it can also be a gritty deep-learning model crunching thousands of images on tens of GPUs. \n\n## You've made it here!\n\nCongrats for making it this far! Let's summarize what we've covered:\n\n * tf.data is TensorFlow's recommended tool for building data-input pipelines (you can probably think of it as a replacement to `feed_dict`)\n * It's useful to think of your model's data input pipeline as an 'Extract, Transform, and Load' process. Those stages nicely categorize tf.data's areas of functionality.\n * **Extract**: \n   * Multiple formats are supported by tf.data out of the box -- from simple ones like CSV or numpy arrays, standard text and image formats, and even the binary TFRecord for high-performance use-cases.\n * **Transform**: \n   * You have direct access to common ML transformations like `batch()`, `shuffle()`, and `repeat()`. \n   * You also have acccess to `map()` and `filter()` for your custom transformation needs.\n * **Load**: \n   * Your prepared dataset can be consumed by tf.estimators. \n   * If you're in eager mode, you can loop over it in in a simple `for sample in dataset:` loop.\n   * You can define iterators to help your model consume the Dataset\n   * For high-performance loads, tf.data gives you parallel I\/O, parallel processing of batches, and prefetching of batches amongst other performance\n \n\n","e158609e":"Let's peak into the dataset now:","976f7bc7":"<img src=\"https:\/\/storage.googleapis.com\/jalammar-ml\/tf.data\/images\/tf.data-map-example.png\" \/>\n\n#### Exercise: Convert from square meters to square feet\nIt's now your turn to transform this dataset. use `map()` to convert from square meters to square feet. Use the conversion rate: 1 square meter  = 10.8 square feet.","7a3b90ac":"Simple test to verify your batching answer:","50241b08":"And then we can apply it:","9013a314":"### Consuming the CSV file\nLet's start by consuming the CSV file `Melbourne_housing_FULL.csv`. To simplify the example, we'll only read two columns from the dataset:\n * Landsize -- the area of a house\n * Price -- the price of the house\n \nBefore calling [CsvDataset](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/experimental\/CsvDataset), we need to prepare its `record_defaults` parameter telling it what value types we expect the columns to have, and what the default value for the column is (in case the dataset did not have a value in this column for a certain sample)"}}