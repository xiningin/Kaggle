{"cell_type":{"26f7f2f3":"code","f6072e0d":"code","a7877996":"code","5775d99c":"code","1efd7f50":"code","c035344f":"code","f933ba98":"code","5e994ffa":"code","67825073":"code","e59d2ae3":"code","4c44b960":"code","3eefef1a":"code","4ddb4918":"code","9accea1b":"code","c9bde2b8":"code","a2364946":"code","9361b543":"code","d16b99d3":"code","f0e0e459":"code","91758c0a":"code","a57b8bd8":"code","a36aed27":"code","98b77143":"code","4e0090e2":"code","419f9cc1":"code","a66ff842":"code","0f49bf5d":"code","c017da39":"code","e05dd3d8":"code","2c3acc7b":"code","2ee1c467":"code","812071f5":"code","4aff99d1":"code","14f1a302":"code","6b24a1db":"code","ee7af00e":"code","33b4b64a":"code","08ee2014":"code","807532de":"code","237d1a75":"code","e1d83ccd":"code","d8fb0e6d":"code","4b283ba0":"code","ce335664":"code","5e8befae":"code","ab41ff97":"code","f6522e27":"code","95385dbd":"code","d71bd1b6":"code","a1521d03":"code","508b0479":"code","7908a1a0":"code","491228a1":"code","a7770730":"code","cc4c8f7e":"code","d159b9a7":"code","3e9f42e4":"code","fca6caf8":"code","1cc76656":"code","674f8391":"code","e580f59d":"code","1c09c9b2":"code","8cc09525":"code","e87e2009":"code","bb0f7caf":"code","99c7d066":"code","9bf7ac10":"code","5adf49c0":"code","b82da545":"code","4bf016ec":"code","de942619":"markdown","fccbbdd9":"markdown","ded3750a":"markdown","948458f5":"markdown","3458084d":"markdown","e0fa0d79":"markdown","819e6f32":"markdown","573907db":"markdown","4692855e":"markdown","11133083":"markdown","bd015b2e":"markdown","2c582c2d":"markdown","bf930787":"markdown","32dc6112":"markdown","c9457cb6":"markdown","f389f8b1":"markdown","5e840738":"markdown","7200a500":"markdown"},"source":{"26f7f2f3":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f6072e0d":"train_dir = '..\/input\/titanic\/train.csv'\ntest_dir = '..\/input\/titanic\/test.csv'","a7877996":"df=pd.read_csv(train_dir)\ndf.head()","5775d99c":"df = df.drop(['PassengerId','Name','Ticket'],axis=1)\ndf.head()","1efd7f50":"df.info()\n","c035344f":"df.describe()","f933ba98":"corr_matrix = df.corr().abs()\nsns.heatmap(corr_matrix)","5e994ffa":"len(df['Survived'].unique())","67825073":"numerical_features = [feature for feature in df.columns if df[feature].dtypes != 'O']\nlen(numerical_features)","e59d2ae3":"numerical_features","4c44b960":"discrete_feature=[feature for feature in numerical_features if len(df[feature].unique())<25]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","3eefef1a":"continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","4ddb4918":"for feature in continuous_feature:\n    data=df.copy()\n    data[feature].hist(bins=25)\n    plt.xlabel(feature)\n    plt.ylabel(\"Count\")\n    plt.title(feature)\n    plt.show()","9accea1b":"sns.displot(x='Age', hue='Survived', data=df, alpha=0.6)\nplt.show()","c9bde2b8":"survived = df[df['Survived']==1]\nsns.displot(survived.Age, kind='kde')\nplt.show()","a2364946":"sns.displot(survived.Age, kind='ecdf')\nplt.grid(True)\nplt.show()","9361b543":"ranges = [0, 30, 40, 50, 60, 70, np.inf]\nlabels = ['0-30', '30-40', '40-50', '50-60', '60-70', '70+']\n\nsurvived['Age'] = pd.cut(survived['Age'], bins=ranges, labels=labels)\nsurvived['Age'].head()","d16b99d3":"sns.countplot(survived.Age)","f0e0e459":"survived.head()","91758c0a":"sns.displot(survived.Fare,kind='kde'),sns.displot(df.Fare,kind='kde')","a57b8bd8":"for feature in continuous_feature:\n    data=df.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()","a36aed27":"features = [i for i in df.columns]","98b77143":"D= df[(df['Survived'] != 0)]\nH = df[(df['Survived'] == 0)]","4e0090e2":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\n%matplotlib inline","419f9cc1":"features_with_na=[features for features in df.columns if df[features].isnull().sum()>1]\n## 2- step print the feature name and the percentage of missing values\nfor feature in features_with_na:\n    print(feature, np.round(df[feature].isnull().mean(), 4),  ' % missing values')\nfeatures_with_na","a66ff842":"df = df.drop(['Cabin'],axis=1)","0f49bf5d":"def impute_nan_most_frequent_category(DataFrame,ColName):\n    # .mode()[0] - gives first category name\n     most_frequent_category=DataFrame[ColName].mode()[0]\n    \n    # replace nan values with most occured category\n     DataFrame[ColName + \"_Imputed\"] = DataFrame[ColName]\n     DataFrame[ColName + \"_Imputed\"].fillna(most_frequent_category,inplace=True)\n#2. Call function to impute most occured category\nfor Columns in ['Embarked']:\n    impute_nan_most_frequent_category(df,Columns)\n    \n# Display imputed result\ndf[['Embarked','Embarked_Imputed']].head(10)\n#3. Drop actual columns\ndf = df.drop(['Embarked'], axis = 1)","c017da39":"def plot_distribution(data_select, size_bin) :  \n    \n    tmp1 = D[data_select]\n    tmp2 = H[data_select]\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['Survived']\n    colors = ['#00FA9A']\n\n    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, \n                             bin_size = size_bin, curve_type='kde')\n    \n    fig['layout'].update(title = data_select)\n\n    py.iplot(fig)","e05dd3d8":"def plot_outliers(df, feat):\n    \n    trace0 = go.Box(\n        y = df[feat],\n        name = \"All Points\",\n        jitter = 0.3,\n        pointpos = -1.8,\n        boxpoints = 'all',\n        marker = dict(\n            color = 'rgb(32,178,170)'),\n        line = dict(\n            color = 'rgb(32,178,170)')\n    )\n    trace1 = go.Box(\n        y = df[feat],\n        name = \"Only Whiskers\",\n        boxpoints = False,\n        marker = dict(\n            color = 'rgb(0,128,128)'),\n        line = dict(\n            color = 'rgb(0,128,128)')\n    )\n\n    trace2 = go.Box(\n        y = df[feat],\n        name = \"Suspected Outliers\",\n        boxpoints = 'suspectedoutliers',\n        marker = dict(\n            color = 'rgb(0,250,154)',\n            outliercolor = '#FF69B4',\n            line = dict(\n                outliercolor = '#FF69B4',\n                outlierwidth = 2)),\n        line = dict(\n            color = 'rgb(0,250,154)')\n    )\n    trace3 = go.Box(\n        y = df[feat],\n        name = \"Whiskers and Outliers\",\n        boxpoints = 'outliers',\n        marker = dict(\n            color = 'rgb(47,79,79)'),\n        line = dict(\n            color = 'rgb(47,79,79)')\n    )\n\n    data = [trace0,trace1,trace2,trace3]\n\n    layout = go.Layout(\n        title = \"{} Outliers\".format(feat)\n    )\n\n    fig = go.Figure(data=data,layout=layout)\n    py.iplot(fig)","2c3acc7b":"def plot_all_feature():\n    for feat in features[:1]:\n        plot_distribution(feat, 0)\n        plot_outliers(df, feat)\n    plot_outliers(df, features[0])","2ee1c467":"def removeOutliers(df_out, feature, drop=False):\n\n    valueOfFeature = df_out[feature]\n    \n    # Q1 (25th percentile) for the given feature\n    Q1 = np.percentile(valueOfFeature, 25.)\n\n    # Q3 (75th percentile) for the given feature\n    Q3 = np.percentile(valueOfFeature, 75.)\n    \n    step = 1.5*(Q3-Q1)\n\n    outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].index.tolist()\n    feature_outliers = valueOfFeature[~((valueOfFeature >= Q1 - step) & (valueOfFeature <= Q3 + step))].values\n\n    # Remove the outliers, if specified\n    print (\"Number of outliers (inc duplicates): {} and outliers: {}\".format(len(outliers), feature_outliers))\n    if drop:\n        good_data = df_out.drop(df_out.index[outliers]).reset_index(drop = True)\n        print (\"New dataset with removed outliers has {} samples with {} features each.\".format(*good_data.shape))\n        return good_data\n    else: \n        print (\"Nothing happens, df.shape = \",df_out.shape)\n        return df_out","812071f5":"df_clean = removeOutliers(df, features[0], True)\nplot_outliers(df_clean, features[0])","4aff99d1":"df_clean = removeOutliers(df_clean, features[1], True)\nplot_outliers(df_clean, features[1])","14f1a302":"df['Age'].fillna(df['Age'].median(), inplace=True)","6b24a1db":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import  BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier","ee7af00e":"discrete_feature=[feature for feature in numerical_features if len(df[feature].unique())<25]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","33b4b64a":"continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","08ee2014":"numerical_features = [feature for feature in df.columns if df[feature].dtypes != 'O']\ndiscrete_feature=[feature for feature in numerical_features if len(df[feature].unique())<25]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","807532de":"continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","237d1a75":"categorical_feature = [feature for feature in df.columns if df[feature].dtypes == 'O']\ncategorical_feature","e1d83ccd":"scaler = StandardScaler()\n\n# define the columns to be encoded and scaled\n\n\n# encoding the categorical columns\ndata = pd.get_dummies(df, columns = categorical_feature, drop_first = True)\nX = data.drop(['Survived'],axis=1)\ny = data[['Survived']]\n\n\ndata[continuous_feature] = scaler.fit_transform(X[continuous_feature])\n\n# defining the features and target\nX = data.drop(['Survived'],axis=1)\ny = data[['Survived']]","d8fb0e6d":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3)","4b283ba0":"lr = LogisticRegression(random_state=42)\n\nknn = KNeighborsClassifier()\npara_knn = {'n_neighbors':np.arange(1, 50)}\n\ngrid_knn = GridSearchCV(knn, param_grid=para_knn, cv=5)\n\ndt = DecisionTreeClassifier()\npara_dt = {'criterion':['gini','entropy'],'max_depth':np.arange(1, 100), 'min_samples_leaf':[1,2,4,5,10,20,30,40,80,100]}\ngrid_dt = GridSearchCV(dt, param_grid=para_dt, cv=5)\n\nrf = RandomForestClassifier()\n\n# Define the dictionary 'params_rf'\nparams_rf = {\n    'n_estimators':[100, 350, 500],\n    'min_samples_leaf':[2, 10, 30]\n}\ngrid_rf = GridSearchCV(rf, param_grid=params_rf, cv=5)","ce335664":"\ndt = DecisionTreeClassifier(criterion='gini', max_depth=20, min_samples_leaf=5, random_state=42)\nknn = KNeighborsClassifier(n_neighbors=3)\nrf = RandomForestClassifier(n_estimators=500, min_samples_leaf=2, random_state=42)","5e8befae":"classifiers = [('Logistic Regression', lr), ('K Nearest Neighbours', knn), ('Classification Tree', dt), ('Random Forest', rf)]","ab41ff97":"accuracy_list= []\nmodel_name = []\nfor clf_name, clf in classifiers:    \n \n    # Fit clf to the training set\n    clf.fit(X_train, y_train)    \n   \n    # Predict y_pred\n    y_pred = clf.predict(X_test)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(y_pred, y_test) \n    accuracy_list.append(accuracy)\n    model_name.append(clf_name)\n    \n   \n    # Evaluate clf's accuracy on the test set\n    print('{:s} : {:.3f}'.format(clf_name, accuracy))","f6522e27":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(base_estimator=rf, n_estimators=100, random_state=1)\n\nada.fit(X_train, y_train)\n\ny_pred = ada.predict(X_test)\n\naccuracy = accuracy_score(y_pred, y_test)\naccuracy_list.append(accuracy)\nmodel_name.append('Adaboost')\naccuracy","95385dbd":"importances = pd.Series(data=rf.feature_importances_,\n                        index= X_train.columns)\n\n# Sort importances\nimportances_sorted = importances.sort_values()\n\n# Draw a horizontal barplot of importances_sorted\nplt.figure(figsize=(10, 10))\nimportances_sorted.plot(kind='bar',color='orange')\nplt.title('Features Importances')\nplt.show()","d71bd1b6":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score\n\ndef cross_val(X, y, model, params, folds=5):\n\n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=21)\n    for fold, (train_idx, test_idx) in enumerate(skf.split(X, y)):\n        print(f\"Fold: {fold}\")\n        x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        x_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        alg = model(**params)\n        alg.fit(x_train, y_train,\n                eval_set=[(x_test, y_test)],\n                early_stopping_rounds=100,\n                verbose=400)\n\n        pred = alg.predict(x_test)\n        accuracy = accuracy_score(y_test, pred)\n#         log_loss_score = log_loss(y_test,pred)\n        print(f\" accuracy : {accuracy}\")\n        print(\"-\"*50)\n    return alg","a1521d03":"lgb_params= {'learning_rate': 0.0001, \n             'n_estimators': 20000, \n             'max_bin': 94,\n             'num_leaves': 5, \n             'max_depth': 30, \n             'reg_alpha': 8.457, \n             'reg_lambda': 6.853, \n             'subsample': 0.749}","508b0479":"from lightgbm import LGBMClassifier\nlgb_model = cross_val(X, y, LGBMClassifier, lgb_params)","7908a1a0":"from xgboost import XGBClassifier\nclassifier = XGBClassifier(n_estimators = 10000,predictor = 'gpu_predictor',tree_method = 'gpu_hist',learning_rate = 0.01,max_depth=29,max_leaves = 31,eval_metric = 'mlogloss',verbosity = 3)\nclassifier.fit(X,y)\n","491228a1":"y_pred=classifier.predict(X_test)\ny_test=np.array(y_test)\naccuracy=accuracy_score(y_pred,y_test)\nprint(\"accuracy_score_XGBOOST: \",accuracy)\naccuracy_list.append(accuracy)\nmodel_name.append('XGboost')\n","a7770730":"plt.bar(model_name, accuracy_list , color ='green',\n        width = 0.1)","cc4c8f7e":"test_df= pd.read_csv('..\/input\/titanic\/test.csv')\ntest_df.head()","d159b9a7":"features_with_na=[features for features in test_df.columns if test_df[features].isnull().sum()>1]\n## 2- step print the feature name and the percentage of missing values\nfor feature in features_with_na:\n    print(feature, np.round(test_df[feature].isnull().mean(), 4),  ' % missing values')\nfeatures_with_na","3e9f42e4":"test_df=test_df.drop(['Cabin'],axis=1)\ntest_df.head()","fca6caf8":"test_df['Age'].fillna(test_df['Age'].median(), inplace=True)\ntest_df.head()","1cc76656":"passenger_id = test_df['PassengerId']","674f8391":"test_df=test_df.drop(['PassengerId','Name','Ticket'],axis=1)\ntest_df","e580f59d":"categorical_feature = [feature for feature in test_df.columns if test_df[feature].dtypes == 'O']\ncategorical_feature","1c09c9b2":"numerical_features = [feature for feature in test_df.columns if test_df[feature].dtypes != 'O']\ndiscrete_feature=[feature for feature in numerical_features if len(test_df[feature].unique())<25]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","8cc09525":"continuous_feature=[feature for feature in numerical_features if feature not in discrete_feature]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","e87e2009":"continuous_feature","bb0f7caf":"test_df","99c7d066":"scaler = StandardScaler()\n\n# define the columns to be encoded and scaled\n\n\n# encoding the categorical columns\ntest_df= pd.get_dummies(test_df, columns = categorical_feature, drop_first = True)\n\n\n\ntest_df[continuous_feature] = scaler.fit_transform(test_df[continuous_feature])\n\n# defining the features and target\ntest_df.head()","9bf7ac10":"test_df = test_df.rename(columns = {'Embarked_Q': 'Embarked_Imputed_Q', 'Embarked_S': 'Embarked_Imputed_S'}, inplace = False)\ntest_df","5adf49c0":"values = classifier.predict(test_df)","b82da545":"df1= pd.DataFrame(passenger_id,columns=['PassengerId'])\ndf2= pd.DataFrame(values,columns=['Survived'])\ntest_submission= pd.concat([df1,df2],axis=1)\ntest_submission","4bf016ec":"test_submission.to_csv('submission.csv',index=False)\n","de942619":"## Handling Outliers","fccbbdd9":"## Model Performances ","ded3750a":"### Finding out Feature Importances","948458f5":"## Number of Numerical Features","3458084d":"### Adaboost Classifier ","e0fa0d79":"# Preprocessing the Data","819e6f32":"### Handling Nan Values with the most frequent category","573907db":"### Light GBM","4692855e":"## Getting Categorical Variables","11133083":"# MODEL BUILDING","bd015b2e":"# So we see that XGBOOST Performs The Best Among All Of Them","2c582c2d":"**Since Cabin has almost 77% Nan values, we are dropping it.**","bf930787":"# Submission","32dc6112":"## Discrete Features ","c9457cb6":"## Finding the amount of Null Values ","f389f8b1":"# EDA","5e840738":"## Visualising Outliers ","7200a500":"### XG Boost "}}