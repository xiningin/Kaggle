{"cell_type":{"60bcf1e4":"code","19ece257":"code","dc72f4b2":"code","7be1e610":"code","c5f6282e":"code","1720b081":"code","ad8759d3":"code","ba9a84f0":"code","c7862283":"code","d7eb5597":"code","7a3f3a32":"code","7d721af1":"code","3bbe9234":"code","200deedd":"code","4f052599":"code","d85d61c6":"code","f6b107ce":"code","67a010aa":"code","690dea24":"code","f695727f":"code","60633844":"markdown","f50ac61c":"markdown","95a1e6ae":"markdown","f46b5133":"markdown","83bc20c1":"markdown","0492cd9c":"markdown","e0156fb1":"markdown","05acc963":"markdown","a94adbbe":"markdown"},"source":{"60bcf1e4":"batch_size = 32\nepochs = 1000\nchannels = 3\nimg_height = 256\nimg_width = 256\nlearning_rate = 0.0001\ntrain_dir = '..\/input\/rgbeurosat\/RBG\/train'\nval_dir = '..\/input\/rgbeurosat\/RBG\/val'\ntest_dir = '..\/input\/rgbeurosat\/RBG\/test'","19ece257":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_data_generator = ImageDataGenerator(\n    rescale = 1.\/255,\n    width_shift_range = 0.15,\n    height_shift_range = 0.15,\n    horizontal_flip=True,\n    zoom_range=0.3\n)","dc72f4b2":"validation_data_generator = ImageDataGenerator(\n    rescale=1.\/255\n)","7be1e610":"train_data = train_data_generator.flow_from_directory(\n    batch_size=batch_size,\n    directory=train_dir,\n    shuffle=True,\n    target_size=(img_height, img_width),\n    class_mode='categorical'\n)","c5f6282e":"val_data = validation_data_generator.flow_from_directory(\n    batch_size=batch_size,\n    directory=val_dir,\n    shuffle=True,\n    target_size=(img_height, img_width),\n    class_mode='categorical'\n)","1720b081":"# importing libraries\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPool2D, Flatten, Dense, InputLayer, BatchNormalization, Dropout\nfrom keras.optimizers import Adam\n\n# build a sequential model\n\nmodel = Sequential()\nmodel.add(InputLayer(input_shape=(img_height, img_width, channels)))\n\n# 1st conv block\n\nmodel.add(Conv2D(8, (3, 3), activation='relu', strides=(1, 1), padding='same'))\nmodel.add(MaxPool2D(pool_size=(2, 2), padding='same'))\nmodel.add(BatchNormalization())\n\n# 2nd conv block\n\nmodel.add(Conv2D(16, (3, 3), activation='relu', strides=(1, 1), padding='same'))\nmodel.add(MaxPool2D(pool_size=(2, 2), padding='same'))\nmodel.add(BatchNormalization())\n\n# 3rd conv block\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', strides=(1, 1), padding='same'))\nmodel.add(BatchNormalization())\n\n# 4th conv block\n\nmodel.add(Conv2D(16, (3, 3), activation='relu', strides=(1, 1), padding='same'))\nmodel.add(BatchNormalization())\n\n# 5th conv block\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', strides=(1, 1), padding='same'))\nmodel.add(MaxPool2D(pool_size=(2, 2), padding='same'))\nmodel.add(BatchNormalization())\n\n# 6th conv block\n\nmodel.add(Conv2D(64, (3, 3), activation='relu', strides=(1, 1), padding='same'))\nmodel.add(BatchNormalization())\n\n# 7th conv block\n\nmodel.add(Conv2D(32, (3, 3), activation='relu', strides=(1, 1), padding='same'))\nmodel.add(BatchNormalization())\n\n# 8th conv block\n\nmodel.add(Conv2D(64, (3, 3), activation='relu', strides=(1, 1), padding='same'))\nmodel.add(MaxPool2D(pool_size=(2, 2), padding='same'))\nmodel.add(BatchNormalization())\n\n# 9th conv block\n\nmodel.add(Conv2D(128, (3, 3), activation='relu', strides=(1, 1), padding='same'))\nmodel.add(BatchNormalization())\n\n# 10th conv bock\n\nmodel.add(Conv2D(64, (3, 3), activation='relu', strides=(1, 1), padding='same'))\nmodel.add(BatchNormalization())\n\n# ANN block\n\nmodel.add(Flatten())\nmodel.add(Dense(units=100, activation='relu'))\nmodel.add(Dense(units=100, activation='relu'))\nmodel.add(Dropout(0.25))\n\n# output layer\n\nmodel.add(Dense(units=10, activation='softmax'))\n\n\n# compile model\n\nopt = Adam(learning_rate=learning_rate)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n\n# model summary\n\nmodel.summary()","ad8759d3":"from keras.callbacks import ModelCheckpoint\n\nmcp_save = ModelCheckpoint('PKNet.h5', save_best_only=True, monitor='val_accuracy', mode='max')","ba9a84f0":"history = model.fit_generator(\n    train_data,\n    epochs=epochs,\n    validation_data=val_data,\n    steps_per_epoch=24,\n    validation_steps=10,\n    callbacks=[mcp_save]\n)","c7862283":"import matplotlib.pyplot as plt","d7eb5597":"plt.figure(figsize = (20,10))\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'vaidation'], loc='upper left')\nplt.show()","7a3f3a32":"plt.figure(figsize = (20,10))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'vaidation'], loc='upper left')\nplt.show()","7d721af1":"test_data_generator = ImageDataGenerator(\n    rescale=1.\/255\n)","3bbe9234":"test_data = test_data_generator.flow_from_directory(\n    batch_size=1,\n    directory=test_dir,\n    shuffle=False,\n    target_size=(img_height, img_width),\n    class_mode='categorical'\n)\nidx2label_dict = {test_data.class_indices[k]: k for k in test_data.class_indices}","200deedd":"import time \n\nmodel.load_weights('PKNet.h5')\ninference_times = []\nfor i in range(5):\n    start_time = time.time()\n    y_pred = model.predict_generator(test_data, steps=2700)\n    inference_time = time.time() - start_time\n    inference_times.append(inference_time)\nprint('Average inference time: %.2f seconds' % (sum(inference_times)\/len(inference_times)))\ny_true = test_data.classes","4f052599":"import numpy as np\n\ny_pred = np.argmax(y_pred, axis = 1)","d85d61c6":"from sklearn.metrics import confusion_matrix\nimport seaborn as sn\nimport pandas as pd","f6b107ce":"def get_key(mydict,val): \n    for key, value in mydict.items(): \n         if val == value: \n             return key ","67a010aa":"def find_metrics(y_true, y_pred, idx2label_dict, class_name):\n    cm = confusion_matrix(y_true, y_pred)\n    out1 = np.sum(cm, axis = 1)\n    out2 = np.sum(cm, axis = 0)\n    id = get_key(idx2label_dict, class_name)\n    r1 = cm[id][id]\/out1[id]\n    r2 = cm[id][id]\/out2[id]\n    s = cm[id][id]\n    return (r1, r2, s)","690dea24":"import prettytable\n\ntable = prettytable.PrettyTable(['Class', 'Recall', 'Precision', 'Accuracy', 'F1 Score'])\nclass_names = ['AnnualCrop', 'Forest', 'HerbaceousVegetation', 'Highway', 'Industrial', 'Pasture', 'PermanentCrop', 'Residential', 'River', 'SeaLake']\nsum = 0\n\ncm = confusion_matrix(y_true, y_pred)\ncm_sum = np.sum(cm)\ncol_sum = np.sum(cm, axis = 0)\nrow_sum = np.sum(cm, axis = 1)\n\nclass_acc = []\n\nrow = len(cm)\n\nfor x in range(0,row):\n    tp = cm[x][x] \n    fp = row_sum[x] - cm[x][x]\n    fn = col_sum[x] - cm[x][x]\n    tn = cm_sum - row_sum[x]- col_sum[x] + cm[x][x]\n\n    temp = (tp+tn)\/(tp+fn+fp+tn)\n    class_acc.append(temp)\n\ntemp = 0    \nfor _class in class_names:\n    result1, result2, s = find_metrics(y_true, y_pred, idx2label_dict, _class)\n    sum += s\n    f1 = (2*(result1* result2))\/ (result1 + result2)\n    table.add_row([_class, round(result1, 2), round(result2, 2), round(class_acc[temp], 2), round(f1, 2)])\n    temp +=1\nprint(table)\nprint(\"Accuracy: %.2f\" % (sum\/2700*100))","f695727f":"cm = confusion_matrix(y_true, y_pred)\ndf_cm = pd.DataFrame(cm, index = [idx2label_dict[int(i)] for i in \"0123456789\"], columns = [idx2label_dict[int(i)] for i in \"0123456789\"])\nplt.figure(figsize = (20,10))\nsn.heatmap(df_cm, annot=True, linewidths=.5, fmt=\"d\")","60633844":"# Declaring paths and variables","f50ac61c":"# Plotting Metrics","95a1e6ae":"# Confusion Matrix","f46b5133":"# Loading test data","83bc20c1":"# Loading data","0492cd9c":"# Running Model","e0156fb1":"# Predicting values","05acc963":"# Precision and Recall","a94adbbe":"# Building model"}}