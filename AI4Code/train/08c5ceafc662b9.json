{"cell_type":{"fa311c16":"code","b0220730":"code","7b39e9c5":"code","b486a2b0":"code","ccb9b575":"code","1dfdf6f4":"code","721dfee9":"code","6bd8060a":"code","e8885e54":"code","082f2a6d":"code","4f5b9e59":"code","a1b47eb2":"code","07bc535b":"code","ca186ef1":"code","3e8e0a3e":"code","61645a3d":"markdown","0fd6c9eb":"markdown","a796fe27":"markdown","248d7eac":"markdown","bce972f3":"markdown","e5abf9ac":"markdown","289e43b4":"markdown","fe7fd3b3":"markdown","649c97ea":"markdown","b5cddfd9":"markdown","aa022e8e":"markdown"},"source":{"fa311c16":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error as mse\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","b0220730":"data_path = '..\/input\/train.csv'\ndf = pd.read_csv(data_path)","7b39e9c5":"df.head()","b486a2b0":"df.describe()","ccb9b575":"df.info()","1dfdf6f4":"y = df.target\nfeatures = [col for col in df.columns if 'var' in col]\nX = df[features]\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size = 0.3, random_state=7)","721dfee9":"#fig,ax = plt.subplots(figsize=(200, 200))\n#sb.heatmap(df.corr(), ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\")\n#plt.show()\n# -> bad idea, it's too big\n\n#fig,ax = plt.subplots(figsize=(50, 50))\n#sb.heatmap(df[0:50].corr(), ax=ax, annot=True, linewidths=0.05, fmt= '.2f',cmap=\"magma\")\n#plt.show()\n# -> also too big","6bd8060a":"lr = LinearRegression()\nlr.fit(train_X, train_y)\nlr_predicts = lr.predict(val_X)\nlr_mse = mse(lr_predicts, val_y)\nprint(lr_mse)","e8885e54":"# submitting the very first predictions\ntest_data_path = '..\/input\/test.csv'\ntest_data = pd.read_csv(test_data_path)\ntest_X = test_data[features]\n#test_preds = lr.predict(test_X)\n\n#output = pd.DataFrame({'ID_code': test_data.ID_code,\n#                       'target': test_preds})\n#output.to_csv('submission.csv', index=False)","082f2a6d":"#dt = DecisionTreeRegressor(max_leaf_nodes=100, random_state=0)\n#dt.fit(train_X, train_y)\n#dt_predicts = dt.predict(val_X)\n#mse_dt = mse(dt_predicts, val_y)\n#print(mse_dt)","4f5b9e59":"from sklearn.linear_model import RidgeCV\nrCV = RidgeCV(normalize=True, cv=10)\nrCV.fit(train_X, train_y)\nrCV_predicts = rCV.predict(val_X)\nmse_rCV=mse(rCV_predicts, val_y)\nprint(mse_rCV)","a1b47eb2":"print(mse_rCV)","07bc535b":"test_data_path = '..\/input\/test.csv'\ntest_data = pd.read_csv(test_data_path)\ntest_X = test_data[features]\ntest_preds = rCV.predict(test_X)\n\noutput = pd.DataFrame({'ID_code': test_data.ID_code,\n                       'target': test_preds})\noutput.to_csv('submission.csv', index=False)","ca186ef1":"from sklearn.linear_model import LogisticRegression\nlogr = LogisticRegression()\nlogr.fit(train_X, train_y)\nlogr_predicts = logr.predict(val_X)\n\nfrom sklearn.metrics import accuracy_score\nacc_logr = accuracy_score(logr_predicts, val_y)\nprint(acc_logr)","3e8e0a3e":"test_data_path = '..\/input\/test.csv'\ntest_data = pd.read_csv(test_data_path)\ntest_X = test_data[features]\ntest_preds = logr.predict(test_X)\n\noutput = pd.DataFrame({'ID_code': test_data.ID_code,\n                       'target': test_preds})\noutput.to_csv('submission.csv', index=False)","61645a3d":"Linear regression: MSE = 0.0742733 (score = 0.86096)","0fd6c9eb":"Ridge with CV: MSE = 0.07439 (almost no improvement over the simple linear regression)","a796fe27":"Worse than linear regression.\n## 4. Ridge Regression with 10fold CV","248d7eac":"### 1.2 Pairwise correlations for X","bce972f3":"## 2. Basic Linear Regression\n","e5abf9ac":"Private ranking is slightly better, BUT\nobviously, it's better to account for the fact that the target variable is binary.\n\n## 5. LogisticRegression","289e43b4":"## 1. Reading data","fe7fd3b3":"### Preliminary setup","649c97ea":"So, we don't understand the meaning of the features at all. Ok.","b5cddfd9":"### 1.1 Brief look at the data","aa022e8e":"## 3. Decision Trees"}}