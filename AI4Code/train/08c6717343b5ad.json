{"cell_type":{"b4bc4141":"code","2e2fe2d9":"code","f86afb90":"code","95e5819d":"code","dfab877e":"code","1160ebdc":"code","536785b0":"code","0a4b56f7":"code","2876273c":"code","950cf243":"code","4fec2c99":"code","535280f3":"code","e4f341d5":"code","7bc53ce7":"code","4999d335":"code","6770216a":"code","e8c4cfdc":"code","8dbf5516":"code","4e5a5702":"code","b3289707":"code","32949f6c":"code","3f2857c9":"code","f7ef93b9":"code","d9593301":"code","4a4be9db":"code","a421b477":"code","9d725b93":"code","0d6fa803":"code","f5d10aa9":"code","f9ae4a90":"code","b145c2ee":"code","25232376":"code","269e216c":"code","cc668ba6":"code","7c0c6396":"code","7f94c4af":"code","9262505d":"code","6b73d276":"code","30c6c358":"code","74190333":"code","7a2d22a1":"code","43bcf257":"code","2cc6453f":"code","b3ca3796":"code","5426b48f":"code","37c01e91":"code","030322e8":"code","a75f38b4":"code","f6cd6cff":"code","4e46d12f":"code","1d585be4":"code","1a920b48":"code","c9b66d1d":"code","9407b225":"code","346a9af9":"code","4d057e72":"markdown","ecfb80ea":"markdown","cc859bc7":"markdown","e77e20d1":"markdown","eab4ef80":"markdown","9b6f03e5":"markdown","514df239":"markdown","2d87bb53":"markdown","d843fa7a":"markdown","d4e25d9a":"markdown"},"source":{"b4bc4141":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2e2fe2d9":"# Read CSV train data file into DataFrame\ntrain_df = pd.read_csv(\"sample_data\/train.csv\")\n\n# Read CSV test data file into DataFrame\ntest_df = pd.read_csv(\"sample_data\/test.csv\")\n\nprint('Train data : ' , train_df.shape)\nprint('Test data : ' , test_df.shape)\n# convert inknown to nan\ntrain_df = train_df.replace('unknown', np.nan)\ntest_df = test_df.replace('unknown', np.nan)\n# convert y to 1,0b\ntrain_df['y']= np.where(train_df['y'] == 'yes',1,0)","f86afb90":"train_df.describe()","95e5819d":"# preview train data\ntrain_df.head()","dfab877e":"# preview test data\ntest_df.head()","1160ebdc":"#Exploratory Train Data Analysis : check null\nsns.heatmap(train_df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","536785b0":"#Exploratory Test Data Analysis : check null\nsns.heatmap(test_df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","0a4b56f7":"# Visualizing some more of the data\n# converst y to 1,0\nsns.countplot(x='y',data=train_df)","2876273c":"sns.heatmap(train_df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","950cf243":"#2.1 Job\ntrain_df['job'].value_counts(dropna=False)","4fec2c99":"# Preview job align age tiers\ndef tier_age(age):\n  if age <=20 :\n    tier_age = '<=20'\n  elif age <=30 :\n    tier_age = '<=30'\n  elif age <=40 :\n    tier_age = '<=40'\n  elif age <=50 :\n    tier_age = '<=50'\n  elif age <=60 :\n    tier_age = '<=60'\n  else :\n    tier_age = '>60'\n  \n  return(tier_age)\n\ntrain_df['tier_age'] = train_df['age'].apply(tier_age)\ntest_df['tier_age'] = test_df['age'].apply(tier_age)\ntrain_df.groupby(['tier_age', 'job']).size().reset_index(name='counter').sort_values(by=['tier_age','counter'],ascending=False)","535280f3":"train_df['job'].value_counts(dropna=False)","e4f341d5":"# Preview education align age tiers\ntrain_df.groupby(['tier_age', 'education']).size().reset_index(name='counter').sort_values(by=['tier_age','counter'],ascending=False)","7bc53ce7":"train_df['marital'].value_counts(dropna=False)","4999d335":"#Preview Default\ntrain_df['default'].value_counts(dropna=False)\n# print(train_df['default'].unique())","6770216a":"# Preview Default by loan and housing\n%matplotlib inline\n\ntrain_df_de = train_df.copy()\ntrain_df_de['default'] = train_df_de['default'].replace(np.nan,'unknown')\n\n# Default\nfig, (ax1, ax2, ax3) = plt.subplots(nrows = 1, ncols = 3, figsize = (20,8))\nsns.countplot(x = 'default', data = train_df_de, ax = ax1, label = ['no', 'unknown', 'yes'])\nax1.tick_params(labelsize=15)\n\n# housing\nsns.countplot(x = 'housing', data = train_df_de, ax = ax2, label = ['no', 'unknown', 'yes'])\nax2.set_title('housing', fontsize=15)\nax2.tick_params(labelsize=15)\n\n# loan\nsns.countplot(x = 'loan', data = train_df_de, ax = ax3, label = ['no', 'unknown', 'yes'])\nax3.set_title('loan', fontsize=15)\nax3.tick_params(labelsize=15)\n\n\n# poutcome\nsns.countplot(x = 'poutcome', data = train_df_de, ax = ax3, label = ['no', 'unknown', 'yes'])\nax3.set_title('poutcome', fontsize=15)\nax3.tick_params(labelsize=15)\n\nplt.subplots_adjust(wspace=0.25)","e8c4cfdc":"train_df['housing'].value_counts(dropna=False)","8dbf5516":"train_df['loan'].value_counts(dropna=False)","4e5a5702":"train_df['pdays'].value_counts(dropna=False)","b3289707":"train_df['duration'].value_counts(dropna=False)","32949f6c":"train_df['poutcome'].value_counts(dropna=False)","3f2857c9":"train_df['contact'].value_counts(dropna=False)","f7ef93b9":"train_df['day_of_week'].value_counts(dropna=False)","d9593301":"train_df.head()","4a4be9db":"test_df.head()","a421b477":"# Data cleaning function and Feature engineering\nimport math\n# clean data\ndef clean_data(df):\n# Age\n    def tier_age(age):\n      if age <=20 :\n        tier_age = '<=20'\n      elif age <=30 :\n        tier_age = '<=30'\n      elif age <=40 :\n        tier_age = '<=40'\n      elif age <=50 :\n        tier_age = '<=50'\n      elif age <=60 :\n        tier_age = '<=60'\n      else :\n        tier_age = '>60'\n      return tier_age\n      \n#tier age\n    df['tier_age'] = df['age'].apply(tier_age)\n#job : Imputation (max job align age)\n    df['job'] = np.where(((df['tier_age'] =='<=20')&(df['job'].isnull())),'student', df['job'] )\n    df['job'] = np.where(((df['tier_age'] =='<=30')&(df['job'].isnull())),'admin.', df['job'] )\n    df['job'] = np.where(((df['tier_age'] =='<=40')&(df['job'].isnull())),'admin.', df['job'] )\n    df['job'] = np.where(((df['tier_age'] =='<=50')&(df['job'].isnull())),'blue-collar', df['job'] )\n    df['job'] = np.where(((df['tier_age'] =='<=60')&(df['job'].isnull())),'blue-collar', df['job'] )\n    df['job'] = np.where(((df['tier_age'] =='>60')&(df['job'].isnull())),'retired', df['job'] )\n#marital : Grouping Operations\n    df['marital'].replace(['married', 'single','divorced',np.nan],[1, 2, 3, 0], inplace=True)\n#education : Grouping Operation and Imputation\n    basic_grps = ['basic.4y', 'basic.6y', 'basic.9y'] \n    df['education'] = np.where(df['education'].isin(basic_grps), 'Basic', df['education'])\n    df['education'].replace(['university.degree.'],['university.degree'],inplace = True)\n\n    df['education'] = np.where(((df['tier_age'] =='<=20')&(df['education'].isnull())),'high.school', df['education'] )\n    df['education'] = np.where(((df['tier_age'] =='<=30')&(df['education'].isnull())),'university.degree', df['education'])\n    df['education'] = np.where(((df['tier_age'] =='<=40')&(df['education'].isnull())),'university.degree', df['education'])\n    df['education'] = np.where(((df['tier_age'] =='<=50')&(df['education'].isnull())),'Basic', df['education'] )\n    df['education'] = np.where(((df['tier_age'] =='<=60')&(df['education'].isnull())),'Basic', df['education'] )\n    df['education'] = np.where(((df['tier_age'] =='>60')&(df['education'].isnull())),'Basic', df['education'] )\n#default : Grouping Operations\n    # df['default'] = df['default'].replace(np.nan,'yes')\n    df['default'].replace(['yes', 'no', np.nan],[1, 2, 3], inplace=True)\n#housing : Grouping Operations\n    # df['housing'] = df['housing'].replace(np.nan,'no')\n    df['housing'].replace(['yes', 'no', np.nan],[1, 2, 3], inplace=True)\n#loan: Grouping Operations\n    # df['loan'] = df['loan'].replace(np.nan,'no')\n    df['loan'].replace(['yes', 'no', np.nan],[1, 2, 3], inplace=True)\n#contact\n    df['contact'].replace(['cellular', 'telephone'],[1, 2], inplace=True)\n#poutcome (new add 2020-11-11)\n    df['poutcome'].replace(['nonexistent', 'failure','success'],[0,0,1], inplace=True)\n    df = df.drop(columns = 'tier_age')\n    return df\n\ntrain_df2 = clean_data(train_df)\ntest_df2 = clean_data(test_df)\ntrain_df2.head()","9d725b93":"# check null data\nsns.heatmap(train_df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","0d6fa803":"# One-Hot Encoding\n\n# convert str to dummy\ntrain_df3 = pd.get_dummies(data=train_df2, columns=['job','education','month','day_of_week'])\ntest_df3 = pd.get_dummies(data=test_df2, columns=['job','education','month','day_of_week'])\ntrain_df3.head()","f5d10aa9":"# Visualizing some more of the data\nsns.countplot(x='y',data=train_df3)","f9ae4a90":"train_df3.groupby(['y']).agg(['count'])","b145c2ee":"#Over-sampling using SMOTE and Standardization\n\n#Create X and y datasets for training \nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\n\ndef oversampled(X, y):\n  X_resampled, y_resampled  = SMOTE(random_state=1234).fit_sample(X, y)\n  return X_resampled, y_resampled\n    \n#Standardization\ndef Standardization(X):\n    train =  StandardScaler().fit_transform(X)\n    data = pd.DataFrame(data=train[0:,0:])\n    count=0\n    for i in X.columns:\n      data=data.rename(columns={count : i})\n      count+=1\n    return data\n\ndf_visualize = train_df3\nX = train_df3.drop(columns = 'y')\nX = Standardization(X)\ny = train_df3[['y']]\ntest_df3 = Standardization(test_df3)\n\n#add column header\ndef add_column(new,old):\n  a = pd.DataFrame(data=new[0:,0:])\n  count=0\n  for i in old.columns:\n      a=a.rename(columns={count : i})\n      count+=1\n  return a\n\n#Split data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)\n# Over Sample\nX_train2, y_train2 = oversampled(X_train,y_train)\n# SMOTE(random_state=1234,sampling_strategy=0.40).fit_sample(X, y) >> for SVM\n\nX_train = add_column(X_train2,X_train)\ny_train = pd.DataFrame(data=y_train2, columns=[\"y\"])","25232376":"X_train.head()","269e216c":"print(\"old data is : \",len(train_df3))\nprint(\"length of oversampled data X is \",len(X_train))\nprint(\"length of oversampled data y is \",len(y_train))","cc668ba6":"# Visualizing some more of the data\nsns.countplot(x='y',data=y_train)","7c0c6396":"#Model Performance\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score,recall_score,confusion_matrix,roc_auc_score,auc,f1_score,accuracy_score\n\ndef Pfm(name,model,x,y):\n    predict = model.predict(x)\n    predict_prob = model.predict_proba(x)[:,1]\n    \n    precition = precision_score(y,predict)\n    recall  = recall_score(y,predict)\n    f1_score = metrics.f1_score(y,predict)\n    accuracy = accuracy_score(y,predict)\n    auc = roc_auc_score(y, predict_prob)\n    \n    print(f'Model : {name}\\n Accuracy : {accuracy:0.4f}, Precition : {precition:0.4f}, Recall : {recall:0.4f},F1_score : {f1_score:0.4f} and AUC : {auc:0.4f}' )","7f94c4af":"#Overall model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nmodels = [RandomForestClassifier(), SVC(probability=True), XGBClassifier()]\nmodel_names = ['RandomForestClassifier', 'SVC', 'XGBClassifier']\n\naccuracy_train = []\naccuracy_test = []\nf1_train = []\nf1_test = []\nauc_train = []\nauc_test = []\n\nfor model in models:\n    model.fit(X_train, y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    predict_prob_train = model.predict_proba(X_train)[:,1]\n    predict_prob_test = model.predict_proba(X_test)[:,1]\n\n    accuracy_train.append(accuracy_score(y_train, y_pred_train))\n    accuracy_test.append(accuracy_score(y_test, y_pred_test))\n    f1_train.append(metrics.f1_score(y_train,y_pred_train))\n    f1_test.append(metrics.f1_score(y_test,y_pred_test))\n    auc_train.append(roc_auc_score(y_train, predict_prob_train))\n    auc_test.append(roc_auc_score(y_test, predict_prob_test))\n\ndata = {'Model' : model_names, 'Train Accuracy' : accuracy_train, 'Test Accuracy' : accuracy_test, 'Train f1-score' : f1_train, 'Test f1-score' : f1_test, 'Train AUC' : auc_train, 'Test AUC' : auc_test}\ndata = pd.DataFrame(data)\ndata.sort_values(by = 'Test AUC', ascending = False)","9262505d":"#SVM\nfrom sklearn.svm import SVC\nsvm_model = SVC(probability=True)\nsvm = svm_model.fit(X_train, y_train)\n\nPfm('svm',svm,X_train,y_train)\nPfm('svm',svm,X_test,y_test)","6b73d276":"# add gridsearch\nfrom sklearn.model_selection import GridSearchCV\nparam = {'C': [100, 500],\n         'gamma': [0.01,0.001], \n         'kernel': ['rbf']} \n\nGCV_svm_model = GridSearchCV(svm_model,param_grid=param, refit = True , verbose = 3)\nGCV_svm_model = GCV_svm_model.fit(X_train,y_train)\nPfm('GCV_svm',GCV_svm_model,X_train,y_train)\nPfm('GCV_svm',GCV_svm_model,X_test,y_test)\nprint(GCV_svm_model.best_estimator_)","30c6c358":"#GridSearch show\nprint(\"Best : %f using %s\" % (GCV_svm_model.best_score_,GCV_random_forest_model.best_params_))\nmean = GCV_svm_model.cv_results_['mean_test_score']\nstds = GCV_svm_model.cv_results_['std_test_score']\nparams = GCV_svm_model.cv_results_['params']\nfor mean,stdev,param in zip (mean , stds , params):\n  print(\"%f (%f) with: %r\" % (mean,stdev,param))","74190333":"#model\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n \nxgb_model = xgb.XGBClassifier(colsample_bytree = 0.6, gamma = 0.5, max_depth = 4, min_child_weight = 1, subsample = 0.6).fit(X_train, y_train)\n \nPfm('XGB',xgb_model,X_train,y_train)\nPfm('XGB',xgb_model,X_test,y_test)","7a2d22a1":"# gridsearch\nfrom sklearn.model_selection import GridSearchCV\n \nparam = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8],\n        'max_depth': [3, 4]\n        }\n \nGCV_xgb_model = GridSearchCV(xgb_model,param_grid=param, cv=3, verbose=True, n_jobs=-1)\nGCV_xgb_model = GCV_xgb_model.fit(X_train,y_train)\nPfm('GCV_xgb',GCV_xgb_model,X_train,y_train)\nPfm('GCV_xgb',GCV_xgb_model,X_test,y_test)","43bcf257":"print(\"Best : %f using %s\" % (GCV_xgb_model.best_score_,GCV_xgb_model.best_params_))\nmean = GCV_xgb_model.cv_results_['mean_test_score']\nstds = GCV_xgb_model.cv_results_['std_test_score']\nparams = GCV_xgb_model.cv_results_['params']\nfor mean,stdev,param in zip (mean , stds , params):\n  print(\"%f (%f) with: %r\" % (mean,stdev,param))","2cc6453f":"#Random_forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest_model = RandomForestClassifier(n_estimators=500,min_samples_split=10,random_state =1000)\nrandom_forest = random_forest_model.fit(X_train,y_train)\n\n#Model Performance\nPfm('RF',random_forest,X_train,y_train)\nPfm('RF',random_forest,X_test,y_test)","b3ca3796":"#feature importance\n\n# Get numerical feature importances\nfeature_list = list(X_train.columns)\nrf_exp = random_forest_model\nimportances = list(rf_exp .feature_importances_)\n#print(f\"importance = {importances}\")\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 5)) for feature, importance in zip(feature_list, importances)]\n#print(f\"{feature_importances}\")\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n#print(f\"feature_importances = {feature_importances}\")\n\ndef Top_Feature_Imp(Top_Percentage):\n  sum_important = 0\n  use_feature = list()\n  for i in feature_importances:\n    use_feature.append(i[0])\n    sum_important = sum_important + i[1]\n    if sum_important >= (Top_Percentage\/100):\n      return use_feature,sum_important\n  \n# used important 95%\npercent_need = int(95)\nTop_Feature = Top_Feature_Imp(percent_need)\nuse_feature = Top_Feature[0]\npercent_gain = Top_Feature[1] * 100\n\nprint(f\"All feature : {len(X_train.columns)}\" )\nprint(f\"selected feature : {use_feature} \")\nprint(f\"Number of Feature Imporant for used : {len(use_feature)} \")\nprint(f\"feature above give us : {percent_gain:.2f} %\")\n\n\n#Graph\n# list of x locations for plotting\ncum_im_x = list(range(len(importances)))\n# List of features sorted from most to least important\nsorted_importances = [importance[1] for importance in feature_importances]\nsorted_features = [importance[0] for importance in feature_importances]\n# Cumulative importances\ncumulative_importances = np.cumsum(sorted_importances)\n# Make a line graph\nplt.plot(cum_im_x, cumulative_importances, 'g-')\n# Draw line at required percent from importance retained\nplt.hlines(y = (percent_need\/100), xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n# Format x ticks and labels\nplt.xticks(cum_im_x, sorted_features, rotation = 'vertical')\n# Axis labels and title\nplt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');\n\nplt.show()","5426b48f":"#selected feature 95%\nX_train = X_train[use_feature]\nX_test = X_test[use_feature]\ntest_df3 = test_df3[use_feature]\n\n# Test new feature\nrandom_forest = random_forest_model.fit(X_train,y_train)\ny_train_pred_rf = random_forest.predict(X_train)\ny_train_pred_probs = random_forest.predict_proba(X_train)\n\n#Model Performance\nPfm('RF',random_forest,X_train,y_train)\nPfm('RF',random_forest,X_test,y_test)","37c01e91":"#GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\nparam = { \n    'n_estimators': [500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [9],\n    'min_samples_split' : [9],   \n    'criterion' :['entropy','gini'],\n    'bootstrap' : [False]\n}\n\nGCV_random_forest_model = GridSearchCV(random_forest_model,param_grid=param, cv=3, verbose=True, n_jobs=-1)\nGCV_random_forest_model = GCV_random_forest_model.fit(X_train,y_train)\nPfm('GCV_RF',GCV_random_forest_model,X_train,y_train)\nPfm('GCV_RF',GCV_random_forest_model,X_test,y_test)\nprint( GCV_random_forest_model.best_params_ )","030322e8":"#GridSearch show\nprint(\"Best : %f using %s\" % (GCV_random_forest_model.best_score_,GCV_random_forest_model.best_params_))\nmean = GCV_random_forest_model.cv_results_['mean_test_score']\nstds = GCV_random_forest_model.cv_results_['std_test_score']\nparams = GCV_random_forest_model.cv_results_['params']\nfor mean,stdev,param in zip (mean , stds , params):\n  print(\"%f (%f) with: %r\" % (mean,stdev,param))","a75f38b4":"random_forest_model_t = RandomForestClassifier(n_estimators= 500,max_features='auto', \n    max_depth=9,\n    criterion='entropy', \n    min_samples_split=9  \n    )\n\nrandom_forest_t = random_forest_model_t.fit(X_train,y_train)\n\n#Model Performance\nPfm('RF',random_forest_t,X_train,y_train)\nPfm('RF',random_forest_t,X_test,y_test)\n","f6cd6cff":"#K-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\n\ndef kFold(model,x,y,cv):\n    scores_accuracy = cross_val_score(model,x,y, cv=cv, scoring='accuracy')\n    scores_auc      = cross_val_score(model,x,y, cv=cv, scoring='roc_auc')\n    \n    print('K-fold cross-validation results:',model.__class__.__name__)\n    print(model.__class__.__name__+\" average accuracy is %2.3f\" % scores_accuracy.mean())\n    print(model.__class__.__name__+\" average auc is %2.3f\" % scores_auc.mean())","4e46d12f":"#Check model\n# Random Forest\nkFold(random_forest_t,X_train,y_train,3)\nkFold(random_forest_t,X_test,y_test,3)","1d585be4":"#xgboost\nkFold(GCV_xgb_model,X_train,y_train,3)\nkFold(GCV_xgb_model,X_test,y_test,3)","1a920b48":"#SVM\nkFold(GCV_svm_model,X_train,y_train,3)\nkFold(GCV_svm_model,X_test,y_test,3)","c9b66d1d":"# predict#Random forest\ny_test_pred_dt = random_forest_t.predict(test_df3)\ny_test_pred_probs = random_forest_t.predict_proba(test_df3)[:,1]","9407b225":"# add column\ndf_y_predict_probs = pd.DataFrame(data=y_test_pred_probs, columns=[\"y\"])\ndf_y_predict_probs['Id'] = np.arange(len(y_test_pred_probs))+1\ndf_y_predict_probs = df_y_predict_probs[['Id','y']]\nprint(df_y_predict_probs)","346a9af9":"#export data\nfrom datetime import datetime\nversion = datetime.now().strftime(\"%Y%m%d_%H%M\")\npredict = df_y_predict_probs\nsubmit_file = predict\ndisplay(submit_file.head(10))\n\nsubmit_file.to_csv(f\".\/submission_cv_{version}.csv\", index=False)","4d057e72":"**4.3 Random_forest**","ecfb80ea":"**2. Data cleaning and 3. Feature engineering**","cc859bc7":"**Todo**\n1. Data collection\n2. Data cleaning\n3. Feature engineering\n4. Model training\n5. Model evaluation (Cross validation)","e77e20d1":"**4.2 XG BOOST**","eab4ef80":"**1. Data collection**","9b6f03e5":"**Import Lib**","514df239":"**4. Model training**","2d87bb53":"**5. Model evaluation (Cross validation)**","d843fa7a":"**4.1 SVM**","d4e25d9a":"**6. Final Model**"}}