{"cell_type":{"9bc69ec4":"code","ff67672a":"code","399e68d6":"code","d4a45787":"code","842d65bf":"code","91ae4815":"code","3814d652":"code","d67040d0":"code","13af86d3":"code","abee6194":"code","2f92869b":"code","b2b75d04":"code","af0a01f7":"code","079809c3":"code","e7259381":"code","f00301ac":"code","f6eac243":"code","0fb308d7":"code","e3344fb9":"code","be814c85":"code","c6ae0912":"code","3783709f":"code","18b1952f":"code","f5bf2840":"code","d62ec242":"code","06af8395":"code","15d595aa":"code","67dbd04f":"code","fc6c8704":"code","ade91ece":"code","5eff43fd":"code","2aaf359c":"code","35a043af":"code","9025eace":"code","6d35a604":"code","40c629e4":"code","3d403ab1":"code","15336824":"code","9f90463b":"code","ed70bd30":"code","081b33bb":"code","811cb9ab":"code","9cf7ffbf":"code","82c21b56":"code","d5224e4d":"code","aee08ad5":"code","732da69a":"code","d3c9a909":"code","86df3c1b":"code","029d1b59":"code","23a6e107":"code","e7090e6e":"code","551b5209":"code","dfe189c0":"code","7522f3cc":"code","ac93e0da":"markdown","bbd9c6a6":"markdown","642cf03c":"markdown","dcb5ab8d":"markdown","0f01936f":"markdown","7bfb582f":"markdown","abe09dc9":"markdown","6ffbf660":"markdown","3da7c631":"markdown","ee2de9e1":"markdown","062b64f5":"markdown","1a99d1c8":"markdown","cbce521d":"markdown","d263a4a1":"markdown","3def9590":"markdown","30dc7a1f":"markdown","6ee886f2":"markdown","4bee8a55":"markdown","7170d21e":"markdown","278d92b5":"markdown","92ec6cfb":"markdown","7d755cbf":"markdown","efeb0ebc":"markdown","8eedb546":"markdown","d6356b3e":"markdown","008a1442":"markdown","b146d9a8":"markdown","84efae6b":"markdown","eb8f829b":"markdown","05ce8f77":"markdown","f63f1cbf":"markdown","35f3f9af":"markdown","771fa9d5":"markdown","fb22eaeb":"markdown","d121430b":"markdown","6d050c03":"markdown","3be29291":"markdown","ebca96e8":"markdown","1d3b930c":"markdown","e70ce5ca":"markdown","0e88c047":"markdown","ef2ceeb5":"markdown","4f17805c":"markdown","d94dbecc":"markdown","53a7d641":"markdown","55e2f30b":"markdown","31608fa6":"markdown"},"source":{"9bc69ec4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport string\nimport re # Regular expressions\n\n# Sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n\n# Visualization\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# NLTK\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\nfrom nltk.stem import PorterStemmer\n\n# Keras\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.preprocessing import text\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential, load_model\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import Adadelta, Adam, RMSprop\nfrom keras.utils import np_utils\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Flatten\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers import GlobalAveragePooling1D,Lambda,Input,GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D,TimeDistributed","ff67672a":"X_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\n\nX_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","399e68d6":"X_train.head()","d4a45787":"X_test.head()","842d65bf":"print('Train dataset shape: {}'.format(X_train.shape))\nprint('Test dataset shape: {}'.format(X_test.shape))","91ae4815":"print('Disaster tweet:')\nprint(X_train.text[0])\nprint('---------------------------')\nprint('Normal tweet:')\nprint(X_train.text[24])","3814d652":"print('Amount of missing data by column in the training dataset')\nprint(X_train.isnull().sum())\nprint('------------------------------------')\nprint('Porcentagem de keywords faltantes no train dataset: {:.3f}%'.format(X_train.keyword.isnull().sum()\/X_train.shape[0]*100))\nprint('Porcentagem de location faltantes no train dataset: {:.3f}%'.format(X_train.location.isnull().sum()\/X_train.shape[0]*100))","d67040d0":"print('Amount of missing data by column in the training dataset:')\nprint(X_test.isnull().sum())\nprint('------------------------------------')\nprint('Porcentagem de keywords faltantes no test dataset: {:.3f}%'.format(X_test.keyword.isnull().sum()\/X_test.shape[0]*100))\nprint('Porcentagem de location faltantes no test dataset: {:.3f}%'.format(X_test.location.isnull().sum()\/X_test.shape[0]*100))","13af86d3":"# Criando a representa\u00e7\u00e3o, \u00e1rea de plot\nfig1, ax1 = plt.subplots(figsize = (4,4))\n\n# Conjunto de dados a ser representado\nsns.set(style=\"darkgrid\")\ntargets = X_train.target.value_counts()\nlabels = ['Not Disaster', 'Disaster']\n\n# Criando o g\u0155afico\nax1.pie(targets, labels = labels, autopct = '%1.1f%%', shadow = True, startangle = 90, colors = ['skyblue',(0.8,0.4,0.4)])\n\n# Op\u00e7\u00f5es Adicionais\nplt.title('Target class distribution')\nax1.axis('equal')\n\n# Mostrando o g\u0155afico\nplt.show()","abee6194":"def plot_distplot(disaster_data, non_disater_data, feature):\n    fig, axes = plt.subplots(figsize = [5,5], nrows = 1, ncols = 1, dpi = 100)\n\n    sns.set(style = 'darkgrid')\n    sns.distplot(non_disater_data, label = 'Not Disaster', ax = axes, color = 'blue')\n    sns.distplot(disaster_data, label = 'Disaster', ax = axes, color = 'red')\n    axes.set_xlabel('')\n    axes.tick_params(axis='x', labelsize=12)\n    axes.tick_params(axis='y', labelsize=12)\n    axes.legend()\n    axes.set_title(f'{feature} Distribution in Training set')\n\n    plt.show()\n","2f92869b":"# Number of characteres in tweets\ndisaster_char_len = X_train[X_train['target']==1]['text'].map(lambda x: len(str(x)))\nnon_disaste_char_len = X_train[X_train['target']==0]['text'].map(lambda x: len(str(x)))\n\nplot_distplot(disaster_char_len,non_disaste_char_len, 'Number of Characters' )","b2b75d04":"# Number of words\ntotal_words_disaster = X_train[X_train.target == 1]['text'].map(lambda x: len(list(x.split())))\n\ntotal_words_non_disaster = X_train[X_train.target == 0]['text'].map(lambda x: len(list(x.split())))\n\nplot_distplot(total_words_disaster,total_words_non_disaster, feature = 'Total Words')","af0a01f7":"# Mean word lenght\n\nmean_word_disaster = X_train[X_train.target == 1]['text'].map(lambda x: np.mean([len(item) for item in list(x.split())]))\nmean_word_non_disaster = X_train[X_train.target == 0]['text'].map(lambda x: np.mean([len(item) for item in list(x.split())]))\n\nplot_distplot(mean_word_disaster,mean_word_non_disaster, feature = 'Mean Word Lenght' )","079809c3":"# Amount of hashtags\n\nhashtags_disaster = X_train[X_train.target == 1]['text'].map(lambda x: str(x).count('#'))\nhashtags_not_disaster = X_train[X_train.target == 0]['text'].map(lambda x: str(x).count('#'))\n\nfig, axes = plt.subplots(figsize = [5,5], nrows = 1, ncols = 1, dpi = 100)\n\nsns.set(style = 'darkgrid')\nsns.distplot(hashtags_not_disaster, label = 'Not Disaster', ax = axes, color = 'blue', kde_kws = {'bw':0.1})\nsns.distplot(hashtags_disaster, label = 'Disaster', ax = axes, color = 'red',kde_kws = {'bw':'scott'})\naxes.set_xlabel('')\naxes.tick_params(axis='x', labelsize=12)\naxes.tick_params(axis='y', labelsize=12)\naxes.legend()\naxes.set_title('Amount of Hashtags Distribution in Training set')\n\nplt.show()","e7259381":"data = {'I':[1,1], 'like': [1,0], 'hate': [0,1], 'databases': [1,1]}\nDTM_Example = pd.DataFrame(data, index = ['D1', 'D2'])\n\nDTM_Example","f00301ac":"def document_term_matrix(dataframe, ngrams = (1,1)):\n    \"\"\"\n    Returns a DataFrame that is a DTM with the specified N-Grams and the text data as input\n    \"\"\"\n    cv = CountVectorizer(stop_words = \"english\", ngram_range=ngrams, min_df = 5)\n    data_cv = cv.fit_transform(dataframe.text) #Learn the vocabulary dictionary and return document-term matrix.\n    \n    # data_cv.toarray() returns an array representation of the bag of words\n    # get_feature_names returns all the words in the corpus in array-like form\n    data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names()) # create the dtm_dataframe\n    \n    # reset the index to match de input dataframe index\n    data_dtm.index = dataframe.index\n    data_dtm['target_label'] = X_train.target\n    return data_dtm\n\n\n","f6eac243":"# Top words for each category\n\ndef top_ngrams(k, df):\n    '''\n    Returns the k top commom ngrams for disaster and for not disaster tweets, given a DTM.\n    '''\n    \n    # Not Disaster\n    \n    # Get only the normal tweets\n    target_label_df_ndisaster = pd.DataFrame(df[df['target_label'] == 0])\n    \n    # Aggregate the top ngrams\n    word_counts_ndisaster = [target_label_df_ndisaster[column].sum() for column in target_label_df_ndisaster.columns]\n    target_label_df_ndisaster = pd.DataFrame(word_counts_ndisaster, target_label_df_ndisaster.columns)\n    \n    # Sort from descending order\n    top_words_ndisaster = target_label_df_ndisaster[0].sort_values(ascending = False)[0:k]\n    \n    \n    # Disaster\n    \n    # Get only the disaster tweets\n    target_label_df_disaster = pd.DataFrame(df[df['target_label'] == 1]).drop('target_label', axis = 1)\n    \n    # Aggregate the top ngrams\n    word_counts_disaster = [target_label_df_disaster[column].sum() for column in target_label_df_disaster.columns]\n    target_label_df_disaster = pd.DataFrame(word_counts_disaster, target_label_df_disaster.columns)\n    \n    # Sort from descending order\n    top_words_disaster = target_label_df_disaster[0].sort_values(ascending = False)[0:k]\n    \n    return top_words_ndisaster, top_words_disaster\n    \n    \n","0fb308d7":"def plot_top_ngrams(not_disaster_ngrams, disaster_ngrams):\n    \"\"\"\n    Plots a bar graph showing the top n-grams for disaster and non disaster tweets\n    \"\"\"\n    # Gets the N of N-gram\n    n_gram = len((not_disaster_ngrams.index[0]).split())\n    n_gram_dict = {1: 'Words', 2: 'Bigrams', 3: 'Trigrams'}\n    \n    \n    fig, axes = plt.subplots(figsize = [24,12], nrows = 1, ncols = 2)\n    sns.set(style=\"darkgrid\")\n    sns.barplot(y = not_disaster_ngrams.index, x = not_disaster_ngrams.values, ax = axes[0])\n    sns.barplot(y = disaster_ngrams.index, x = disaster_ngrams.values, ax = axes[1])\n\n    axes[0].set_title(f'Top Non Disaster {n_gram_dict[n_gram]}')\n    axes[0].set_xlabel(f'{n_gram_dict[n_gram]} Count')\n    axes[0].set_ylabel(f'{n_gram_dict[n_gram]}')\n\n    axes[1].set_title(f'Top Disaster {n_gram_dict[n_gram]}')\n    axes[1].set_xlabel(f'{n_gram_dict[n_gram]} Count')\n    axes[1].set_ylabel(f'{n_gram_dict[n_gram]}')\n\n    plt.tight_layout()\n    plt.show()","e3344fb9":"def generate_display_wordclouds(dataframe):\n    full_text_non_disaster = ''.join(dataframe[dataframe['target'] == 0].text)\n    full_text_disaster = ''.join(dataframe[dataframe['target'] == 1].text)\n\n        \n    wc_not_disaster = WordCloud(background_color=\"white\", colormap=\"Dark2\",\n                   max_font_size=150, random_state=42)\n    \n    wc_disaster = WordCloud(background_color=\"white\", colormap=\"Dark2\",\n                   max_font_size=150, random_state=42)\n    \n    wc_not_disaster.generate(full_text_non_disaster)\n    wc_disaster.generate(full_text_disaster)\n    \n    \n    fig, axes = plt.subplots(figsize = [20,8], nrows = 1, ncols = 2)\n    \n    axes[0].imshow(wc_not_disaster, interpolation = 'bilinear')\n    axes[1].imshow(wc_disaster, interpolation = 'bilinear')\n    axes[0].axis(\"off\")\n    axes[1].axis(\"off\")\n    \n    \n    axes[0].set_title('Not Disaster')\n    axes[1].set_title('Disaster')\n    \n    \n    \n    \n    \n\n    ","be814c85":"# Document Term Matrix from the original input\nraw_data_dtm = document_term_matrix(X_train)\n\n# Get the top disaster and not disaster words\ntop_ndisaster_words, top_disaster_words = top_ngrams(20, raw_data_dtm)\n","c6ae0912":"\n# Plot the top words\nplot_top_ngrams(top_ndisaster_words,top_disaster_words)\n","3783709f":"# Generate the raw data word clouds\ngenerate_display_wordclouds(X_train)","18b1952f":"abbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"glhf\" : \"good luck have fun\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"prime minister\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","f5bf2840":"contractions = { \n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how has\",\n\"i'd\": \"i had\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i shall\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it had \",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that had\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","d62ec242":"def fix_abreviations(text):\n    \"\"\"\n    Expand te abreviations (slangs) into its formal word\n    \"\"\"\n    words = text.split(' ')\n    clean_text = []\n    for word in words:\n        clean_text.append(abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word)\n    \n    clean_text = ' '.join(clean_text)\n    return clean_text\n\ndef fix_contractions(text):\n    \"\"\"\n    Expands the english contractions\n    \"\"\"\n    words = text.split(' ')\n    clean_text = []\n    for word in words:\n        clean_text.append(contractions[word.lower()] if word.lower() in contractions.keys() else word)\n    \n    clean_text = ' '.join(clean_text)\n    \n    return clean_text\n\ndef clean_data(text):\n    text = text.lower()\n    \n    # Fix contractions\n    text = fix_contractions(text)\n    \n    # Fix abreviations\n    \n    text = fix_abreviations(text)\n    \n    \n    text = re.sub('[!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~]', '', text) # remove punctuations\n    text = re.sub(r'\\n', ' ', text)  # remove line breaks\n    text = re.sub(r'https?:\/\/\\S+', '', text) # remove links\n    text = re.sub(r'\\s+', ' ', text) # remove unnecessary spacings\n    text = re.sub(r'\\w*\\d\\w*', '', text) # remove words containing numbers\n\n    # Remove Special characters -  adapted from https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert#3.-Target-and-N-grams\n    text = re.sub(r\"\u00fb_\", \"\", text)\n    text = re.sub(r\"\u00fb\u00f2\", \"\", text)\n    text = re.sub(r\"\u00fb\u00f2\u00e5\u00ea\", \"\", text)\n    text = re.sub(r\"\u00fb\u00f3\", \"\", text)\n    text = re.sub(r\"\u00fb\u00f3\", \"\", text)\n    text = re.sub(r\"\u00fb\u00f7\", \"\", text)\n    text = re.sub(r\"\u00fb\u00aa\", \"\", text)\n    text = re.sub(r\"\u00e5_\", \"\", text)\n    text = re.sub(r\"\u00fb\u00ef\", \"\", text)\n    text = re.sub(r\"\u00e5\u00ea\", \"\", text)\n    text = re.sub(r\"\u00e5\u00e8\", \"\", text)\n    text = re.sub(r\"\u00e5\u00a8\", \"\", text)\n    text = re.sub(r\"\u00e5\u00c7\", \"\", text)\n    text = re.sub(r\"\u00ec\u00f1\", \"\", text)\n    text = re.sub(r\"\u00ec\u00f1\", \"\", text)\n    text = re.sub(r\"\u00ec\u00fc\", \"\", text)\n    text = re.sub(r\"\u00e2\u00e2\", \"\", text)\n    text = re.sub(r\"\u00e5\u00e7\", \"\", text)\n\n    \n    \n    return text\n    \n","06af8395":"clean_df = pd.DataFrame(X_train['text'].apply(clean_data))\nclean_df['target'] = X_train.target\n\nclean_df.head()","15d595aa":"print('Disaster tweets cleaned')\nprint('Before:\\n {} \\n After:\\n {}'.format(X_train.text[5], clean_df.text[5]))\nprint('##############################################')\nprint('Before:\\n {} \\n After:\\n {}'.format(X_train.text[8], clean_df.text[8]))\nprint('##############################################')\nprint('Before:\\n {} \\n After:\\n {}'.format(X_train.text[12], clean_df.text[12]))\nprint('##############################################')\nprint('---------------------------------------------')\nprint('Normal tweets cleaned')\nprint('Before:\\n {} \\n After:\\n {}'.format(X_train.text[30], clean_df.text[30]))\nprint('##############################################')\nprint('Before:\\n {} \\n After:\\n {}'.format(X_train.text[34], clean_df.text[34]))\nprint('##############################################')\nprint('Before:\\n {} \\n After:\\n {}'.format(X_train.text[42], clean_df.text[42]))\n","67dbd04f":"# Document Term Matrix cleaned\nclean_data_dtm = document_term_matrix(clean_df)\ndtm_bigrams = document_term_matrix(clean_df, ngrams = (2,2))\ndtm_trigrams = document_term_matrix(clean_df, ngrams = (3,3))\n\n\n# Get the top disaster and not disaster words\ntop_ndisaster_words, top_disaster_words = top_ngrams(20, clean_data_dtm)\n\ntop_ndisaster_bigrams, top_disaster_bigrams = top_ngrams(20, dtm_bigrams)\n\ntop_ndisaster_trigrams, top_disaster_trigrams = top_ngrams(20, dtm_trigrams)\n\n","fc6c8704":"plot_top_ngrams(top_ndisaster_words,top_disaster_words)","ade91ece":"plot_top_ngrams(top_ndisaster_bigrams,top_disaster_bigrams)","5eff43fd":"plot_top_ngrams(top_ndisaster_trigrams,top_disaster_trigrams)","2aaf359c":"generate_display_wordclouds(clean_df)","35a043af":"def remove_words_in_commom(top_disaster_words, top_ndisaster_words):\n    top_disaster_words = top_disaster_words.index\n    top_ndisaster_words = top_ndisaster_words.index\n\n    remove_word = [word for word in top_ndisaster_words if word in top_disaster_words]\n    remove_word = remove_word[0:-1]\n    remove_word.append('wa')\n    return remove_word","9025eace":"def preprocessing(text):\n    # clean the text using the clean_data function\n    preprocessed_text = clean_data(text)\n    \n    #  tokenizes the sentences into words based on whitespaces\n    tokens = [word for sent in nltk.sent_tokenize(preprocessed_text) for word in nltk.word_tokenize(sent)]\n    stopwds = stopwords.words('english')\n    tokens = [token for token in tokens if token not in stopwds]\n    \n    \n    # Lemmatization\n    tagged_corpus = pos_tag(tokens)\n    Noun_tags = ['NN','NNP','NNPS','NNS']\n    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n    lemmatizer = WordNetLemmatizer()\n    \n    # The following function, prat_lemmatize, has been created only for the reasons of mismatch \n    # between the pos_tag function and intake values of lemmatize function. \n    def prat_lemmatize(token, tag):\n        if tag in Noun_tags:\n            return lemmatizer.lemmatize(token,'n')\n        elif tag in Verb_tags:\n            return lemmatizer.lemmatize(token,'v')\n        else:\n            return lemmatizer.lemmatize(token,'n')\n     \n    pre_proc_text =   \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])\n    return pre_proc_text\n    ","6d35a604":"# Getting only the target column\ny = X_train.target \n\n# Getting only the text column\n\nX = X_train.text\n\n# Splitting the data into training and validation\nx_train, x_test, y_train, y_test = train_test_split(\n                                    X, y, test_size=0.33, random_state=42)","40c629e4":"# Preprocessing train and test sets\nx_train = x_train.apply(preprocessing)\nx_test = x_test.apply(preprocessing)\n\n# Creating a TfidfVectorizer object.\n# words and bigrams that appear less then 5 times in the vocabulary dont add a lot of information and are removed\ntfidf = TfidfVectorizer(min_df = 5, ngram_range = (1,2), stop_words = 'english')\n\n# Learning the vocabulary from the train set\ntext_vec = tfidf.fit_transform(x_train)\n\n# Transforming the test set into a TF-IDF representation\ntest_vec_test = tfidf.transform(x_test)\n\n# Getting the results dataframes\nx_train = pd.DataFrame(text_vec.toarray(), columns = tfidf.get_feature_names())\nx_test = pd.DataFrame(test_vec_test.toarray(), columns = tfidf.get_feature_names())","3d403ab1":"lr = LogisticRegression(solver='liblinear', random_state=777)\n\n# This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.\nscaler = MinMaxScaler()\npipeline = Pipeline([('scale',scaler), ('lr', lr),])\n\npipeline.fit(x_train, y_train)\n\nprint ('Training accuracy: %.4f' % pipeline.score(x_train, y_train))\nprint ('Test accuracy: %.4f' % pipeline.score(x_test, y_test))\n","15336824":"# Splitting the data into training and validation\nx_train, x_test, y_train, y_test = train_test_split(\n                                    X, y, test_size=0.33, random_state=42)","9f90463b":"# Cleaning the train and test sets\nx_train = pd.DataFrame(x_train.apply(clean_data))\nx_test = pd.DataFrame(x_test.apply(clean_data))\n","ed70bd30":"# Creating the tokenizer object\ntokenizer = text.Tokenizer()\n\n# Leaning the toknes\ntokenizer.fit_on_texts(x_train.text)\n\n# Size of the vocabulary\nvocab_size = len(tokenizer.word_index) + 1\n\n# Applying the tokenization \nencoded_docs = tokenizer.texts_to_sequences(x_train.text)\nencoded_docs_test = tokenizer.texts_to_sequences(x_test.text)\n\nmax_lenght = 25\n\n# Padding the documents\npadded_docs = sequence.pad_sequences(encoded_docs, maxlen= max_lenght, padding = 'post')\npadded_docs_test = sequence.pad_sequences(encoded_docs_test, maxlen= max_lenght, padding = 'post')","081b33bb":"padded_docs.shape","811cb9ab":"model = Sequential()\nmodel.add(Embedding(vocab_size, 100, input_length=max_lenght))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n# summarize the model\nprint(model.summary())","9cf7ffbf":"# fit the model\nmodel.fit(padded_docs, y_train, epochs=3, verbose=1, validation_data = (padded_docs_test, y_test))\n# evaluate the model\nloss, accuracy = model.evaluate(padded_docs, y_train, verbose=0)\nloss, accuracy_val = model.evaluate(padded_docs_test, y_test, verbose=0)\n\nprint('Train Accuracy: %f' % (accuracy*100))\nprint('Test Accuracy: %f' % (accuracy_val*100))","82c21b56":"# Splitting the data into training and validation\nx_train, x_test, y_train, y_test = train_test_split(\n                                    X, y, test_size=0.33, random_state=42)\n\n# Cleaning the train and test sets\nx_train = pd.DataFrame(x_train.apply(clean_data))\nx_test = pd.DataFrame(x_test.apply(clean_data))\n\n# Creating the tokenizer object\ntokenizer = text.Tokenizer()\n\n# Leaning the toknes\ntokenizer.fit_on_texts(x_train.text)\n\n# Size of the vocabulary\nvocab_size = len(tokenizer.word_index) + 1\n\n# Applying the tokenization \nencoded_docs = tokenizer.texts_to_sequences(x_train.text)\nencoded_docs_test = tokenizer.texts_to_sequences(x_test.text)\n\nmax_lenght = 25\n\n# Padding the documents\npadded_docs = sequence.pad_sequences(encoded_docs, maxlen= max_lenght, padding = 'post')\npadded_docs_test = sequence.pad_sequences(encoded_docs_test, maxlen= max_lenght, padding = 'post')","d5224e4d":"embeddings_index = dict()\nwith open('..\/input\/glovetwitter\/glove.twitter.27B.100d.txt','r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\nf.close()\nprint('Loaded %s word vectors.' % len(embeddings_index))","aee08ad5":"# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab_size, 100))\nfor word, i in tokenizer.word_index.items():\n\tembedding_vector = embeddings_index.get(word)\n\tif embedding_vector is not None:\n\t\tembedding_matrix[i] = embedding_vector","732da69a":"model2 = Sequential()\ne = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=25, trainable=False)\nmodel2.add(e)\nmodel2.add(SpatialDropout1D(0.2))\nmodel2.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel2.add(Dense(1, activation='sigmoid'))\nmodel2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nprint(model2.summary())\n\n","d3c9a909":"# CallBack Function\n\nmc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)","86df3c1b":"# Training\nhistory = model2.fit(padded_docs, y_train, epochs=5, verbose=1, validation_data = (padded_docs_test, y_test), callbacks=[mc])\n# evaluate the model\nloss, accuracy = model2.evaluate(padded_docs_test, y_test, verbose=0)\nprint('Validation Accuracy: %f' % (accuracy*100))","029d1b59":"saved_model = load_model('best_model.h5')","23a6e107":"y_pred = np.round(saved_model.predict(padded_docs_test))\ncm = confusion_matrix(y_test, y_pred)\/y_test.shape[0]*100","e7090e6e":"\nsns.set()\nfig, ax0 = plt.subplots(figsize = (14,7))\n\nax = sns.heatmap(data = cm, annot=True, fmt = '.1f', square=1, linewidths=.5, cmap=\"YlGnBu\")\nax0.set_title('Confusion Matrix')\nfor t in ax.texts: t.set_text(t.get_text() + \" %\")\nplt.show()","551b5209":"# Applying the preprocessing steps to the X_test dataset\nX_test = pd.DataFrame(X_test.text.apply(clean_data))\nX_test = tokenizer.texts_to_sequences(X_test.text)\nX_test = sequence.pad_sequences(X_test, maxlen= max_lenght, padding = 'post')","dfe189c0":"# Predicting the results\n\npredictions = saved_model.predict(X_test)\n\n# Making the submission dataframe\nX_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nX_submission['target'] = np.round(predictions)\nX_submission['target'] = X_submission['target'].astype(int)","7522f3cc":"X_submission.to_csv('sub.csv', index = False)","ac93e0da":"**6.1 Modelling with Keras Embedding Layer**\n\nOnce again, we start by splitting the data into a training and a test set","bbd9c6a6":"As we can see, we have a significant amount of overfitting with this model. Altough we already got a test accuracy near 80%. \n\nAltough the TF-IDF representation is an effective method for extracting features from text, due to the inherent nature of the model being just a bag of unstructured words, we lose additional information like the semantics, structure, sequence and context around nearby words in each text document.\n\nThis gives enough motivation to try a different approach: word embeddings.","642cf03c":"Next, we need to create a matrix of one embedding for each word in the training dataset. We can do that by enumerating all unique words in the Tokenizer.word_index and locating the embedding weight vector from the loaded GloVe embedding.\n\nThe result is a matrix of weights only for words we will see during training","dcb5ab8d":"Finally, we can create a logistic regression model to train and predict the data. ","0f01936f":"We also define two more functions: one to return the k top N-Grams given a DTM; and finally one to show graph bars of these top k N-Grams for both classes of tweets. ","7bfb582f":"* The words clouds point to a much clearer difference in the vocabulary of the two classes of tweets. However, there are still some words of apparently little importance, such as 'wa' and 'amp'. We can then consider removing these words, as well as some other words in common.\n","abe09dc9":"# 7. Submission","6ffbf660":"We now use these functions to create a DTM from the raw text input data, to plot the top Words (1-Gram) and to generate word clouds for both classes of tweets.","3da7c631":"# 6. Word Embeddings Representation\n\nTo overcome the shortcomings of losing out semantics and feature sparsity in bag of words model based features, we need to make use of Vector Space Models (VSMs) in such a way that we can embed word vectors in this continuous vector space based on semantic and contextual similarity.\n\nEmbeddings (in general, not only in Keras) are methods for learning vector representations of categorical data. They are most commonly used for working with textual data. Word2vec and GloVe are two popular frameworks for learning word embeddings. What embeddings do, is they simply learn to map the one-hot encoded categorical variables to vectors of floating point numbers of smaller dimensionality then the input vectors. For example, one-hot vector representing a word from vocabulary of size 50 000 is mapped to real-valued vector of size 100. Then, the embeddings vector is used for whatever you want to use it as features.\n\n\nAlthough there are many packages to handle representation in word embeddings, we will bring you two very common approaches. First, we will develop a neural network with the first layer being an embedding layer. With embeddings learned as a layer of a neural network, the network may be trained to predict whatever you want. For example, you can train your network to predict sentiment of a text. In such case, the embeddings would learn features that are relevant for this particular problem. As a side effect, they can learn also some general things about the language, but the network is not optimized for such task.\n\nThen, we will use the pre trained Glove model for embedding. GloVe model stands for Global Vectors which is an unsupervised learning model which can be used to obtain dense word vectors similar to Word2Vec. The glove has a word vector trained from 2 bilions tweets, with a vocabulary of 1.2 milion terms and representations in 25, 50, 100 or 200 dimensions.\n\n","ee2de9e1":"**3.1 Missing Data**","062b64f5":"We can then look again at the top N-Grams and to the word clouds to see if the classes of tweets now have a different vocabulary. This time, we will also see the bigrams and trigrams.","1a99d1c8":"**Findigs**\n\n* Both the most common words and the word clouds indicates to us that a lot of cleaning will be necessary to handle these text entries.\n\n* Among the most common words in both classes, the word 'http' stands out a lot. This is due to the large amount of links present in the tweets. Therefore, when cleaning data, we must remove such links.\n\n* Some words like 'amp', 'wa', '\u00fb_' appear a lot in both classes of tweets and add little information so that we can distinguish the content of the texts.\n\n* Some words are repeated between both classes and we can test removing them from the set of texts.\n\n","cbce521d":"# 1. Introduction\n\nNatural Language Processing (NLP) is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. Data can come in many forms: time stamps, sensor readings, images, categorical labels, and so much more. But text is still some of the most valuable data out there. Although the text data contains a lot of information, it is highly unstructured, and that is especially hard when we are trying to build an intelligent system which interprets and understands free flowing natural language just like humans. We need to be able to process and transform noisy, unstructured textual data into some structured, vectorized formats which can be understood by any machine learning algorithm.\n\nWithin this context, this project has the goal to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t, with access to a dataset of 10,000 tweets that were hand classified ([Real or Not? NLP with Disaster Tweets](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/overview)). \n\n","d263a4a1":"Finally we create a neural network architecture. ","3def9590":"**Observations:**\n\n* Altough the validation accuracy has slightly improved (reaching almost 81%), the model has a lot of overfitting.\n\n* Different architectures of neural networks could be used to improve the modelling using the keras embedding layer.\n\n* In a different approach, we will now try to use the Glove pre trained embedding layer, with tweet inputs.","30dc7a1f":"# 4. Cleaning Data\n\nText data is well known for being highly unstructured. Because of that, one the most important steps in building NLP projects is cleaning the data. The number one rule we follow is: \u201cYour model will only ever be as good as your data.\u201d\n\nThe following steps were taken to clean the texts:\n\n* Removal of words that are not relevant, such as \u201c@\u201d twitter mentions or urls\n* Convertion of all characters to lowercase, in order to treat words such as \u201chello\u201d, \u201cHello\u201d, and \u201cHELLO\u201d the same\n* Removal of words containing non alphanumeric characters in it\n* Removal of all of the punctuation\n* Removal of urls\n* Expansion of contractions such as expanding we'll into we will. Adapted from https:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python.\n* Expansion of abreviation. For example, 'ppl' is transformed into people. Adapted from https:\/\/www.kaggle.com\/nmaguette\/up-to-date-list-of-slangs-for-text-preprocessing\n* Removal of unnecesssary spacings, line breaking and special characters\n","6ee886f2":"Instead of applying the traditional preprocessing steps, we'll only apply the clean_data function. Since  the embedding representation can capture the semantics of the texts, removing stop words actually makes the model perform worse.","4bee8a55":"By looking at the graphics at section 3, we can see that the vast majority of tweets has a maximum word count of 25. Thus, we will use 25 as the max lenght of the input text data.","7170d21e":"# 8. Conclusions","278d92b5":"# **3. Exploratory Data Analisys**\n\nAs we know, we are dealing with textual data from tweets. This is a platform that contains a lot of informal language, so some special care is needed so that we can extract important information to develop a classifier. Thus, in addition to the basic and common exploration of data, we will also look at some special features of text data, such as: number of words contained in tweets, mean word lenght, amount of characters, among others. We will also look at Word Clouds, which are great ways to view the most significant words in texts.\n\n\nWe start by gathering the most basic informations from the datasets, such as their shapes.","92ec6cfb":"**3.3 Text Features**\n\nNext, we'll start exploring some specific features of textual data: number of characters in tweets, number of words, mean word lenght and the amount of hashtags. Some distribution charts show such features for tweets classified as disasters and for normal tweets.","7d755cbf":"The keras embedding requires that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras.\n\nThe Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training datase. It must specify 3 arguments:\n\n\n* input_dim: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n* output_dim: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n* input_length: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.","efeb0ebc":"Next, we need to load the entire GloVe word embedding file into memory as a dictionary of word to embedding array.","8eedb546":"Again, we would like to check a few tweets from each class, before and after the cleaning function.","d6356b3e":"**2.1 Train and Test Set Importing**\n\nFirst, we import the necessary libraries and training and test sets","008a1442":"We can also take a look into tweets from both classes","b146d9a8":"# 5. TF-IDF Representation\n\nNow that we have a much cleaner corpus than the initial data entry, we can apply NLP techniques so that machine learning models can perform the task of classifying tweets.\n\nWe can build a vocabulary of all the unique words in our dataset, and associate a unique index to each word in the vocabulary. Each sentence is then represented as a list that is as long as the number of distinct words in our vocabulary. At each index in this list, we mark how many times the given word appears in our sentence. This is called a Bag of Words model, since it is a representation that completely ignores the order of words in our sentence.\n\nThe TD-IDF is a slight improvement to the Bag of Words models. TFIDF, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf\u2013idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. tf\u2013idf is one of the most popular term-weighting schemes today.\n\nTo apply the TF-IDF representation we will follow the following steps:\n\n* Tokenize the texts: this is a process of breaking the piece of texts into many pieces, such as sentences or words\n\n* Removal of stop words: stop words are very commom words in texts and they add little extra information to the messages. In english, commom stop words are: 'for', 'to', 'the'.\n\n* Lemmatize the text: lemmatization refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma.","84efae6b":"The following function returns a clean, tokenized and lemmatized text.","eb8f829b":"There is no big discrepancy between the amount of tweets about disasters and the amount of normal tweets. As there is a 57% amount of disaster tweets, we should expect the classifier to have an accuracy at least greater than that.\n\n\n\n","05ce8f77":"**Results**\n\n* For the validation set, the best model registered an accuracy of 81.53%. The confusion matrix indicates that 11.1% predictions were false positive. This indicates that a significant number of tweets are being classified as a disaster, even when they are normal tweets. When looking at the vocabulary, we can see that words that denote tragedies such as 'emergency' is one of the most common words for normal tweets, that can be a reason for such a high value.\n\n* In the submission set, an accuracy of 80.2% was achieved\n\n\n**Possible Improvements**\n* Although the training and test datasets had features other than text, they were not used. The location feature has a lot of input errors, but with a good enough data cleanup, it could have been used to add information to the model.\n\n\n* More complex neural network architectures can be used to improve the model. At the moment, a very simple architecture was employed, with only one layer of LSTM\n\n* In addition to an improvement in the model, a more careful cleaning process can be used for the improvement. Handling of foreign language words is an example.","f63f1cbf":"As we can see, with the Glove embedding layer, overfitting has drastically reduced. The validation accuracy has also increased, which indicates that it is the most promising model.\n\nThis model will be used to make predictions for submission to the kaggle competition.","35f3f9af":"\n**Summary**\n\n1. Introduction\n2. Data Importing\n3. Exploratory Data Analysis\n\n    3.1 Missing Data\n    \n    3.2 Target Class Distribution in the Train Dataset\n    \n    3.3 Text Features\n    \n    3.4 Top N-Grams and Word Clouds\n    \n    \n4. Cleaning Data\n5. TF-IDF Representation\n\n    5.1 Modelling with Logistic Regression\n    \n    \n6. Word Embeddings Representation\n\n    6.1 Modelling with Keras Embedding Layer\n    \n    6.2 Modelling using Glove\n    \n    \n7. Submission\n\n8. Conclusions","771fa9d5":"**3.2 Target Class Distribution in the Train Dataset**","fb22eaeb":"* There are still a few top words that are shared between the two classes. Altough, we got a lot more of meaning from the top 20 words.\n\n* The most used words for disaster tweets reflect real tragedies, such as 'disaster', 'suicide', 'fires', among othes\n\n* The most used words for normal tweets are much more 'commom', such as 'love', 'video', 'youtube'.","d121430b":"**5.1 Modelling with Logistic Regression**\n\nAs a  general good practice we'll start with a simple tool that can solve the task. Whenever it comes to classifying data, a common favorite for its versatility and explainability is Logistic Regression. It is very simple to train and the results are interpretable as you can easily extract the most important coefficients from the model.\n\nWe split our data in to a training set used to fit our model and a test set to see how well it generalizes to unseen data. ","6d050c03":"# 2. Data Importing","3be29291":"**6.2 Modelling using Glove**\n\nWe follow the exact same steps from the previous sub section in order to prepare the train and test inputs to the layer. Altough this time we will import the pretrained embedding layer from Glove.","ebca96e8":"<img src=\"https:\/\/gizmodo.uol.com.br\/wp-content\/blogs.dir\/8\/files\/2012\/07\/twitter-new-logo.png\">","1d3b930c":"**3.4 Top Words and Word Clouds**\n\nWe can also see which words appear more for each of the tweet classes, both by bar graphs and word clouds. We hope that this will give us a sense of the most used vocabulary for each of the classes. If we notice a big difference between words used in each type of tweet, we can have more confidence in a good classification model.\n\nWe start by defining a function that uses the CountVectorizer class from the sklearn package. This class allows us to build the Document-Term Matrix representation. The document-term matrix or term-document matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. Thus, each of the text data is represented by the amount of unique words that it contains. This representation is also called bag of words and it will help us to extract the top N-Grams from the text. An n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application\n\nIf we had two sentences:\n\n* D1 = I like databases\n* D2 = I hate databases\n\nThen, the document-term matrix for this would be","e70ce5ca":"<img src=\"https:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1539984207\/stemminglemmatization_n8bmou.jpg\" width=\"600px\">","0e88c047":"**Findings:**\n* In general, the distribution of these features for the two classes of tweets is quite similar and adds little information to classification models. The classification of tweets should be strongly based on NLP techniques.\n\n* The number of characters, the number of words and the mean word lenght of disaster tweets is slightly greater than for normal tweets. This is consistent with a more formal language used in disaster tweets, which could be expected for messages of this nature.","ef2ceeb5":"* There are no top bigrams or trigrams in commom between both classes becaus the context is much clearer.\n\n* Again we can see that the top bigrams and top trigrams reflect the nature of their text. Bigrams such as 'suicide bomber' or 'california wildfire' clearly are about tragedies. Trigrams such as 'liked youtube video' clearly are not.","4f17805c":"Next, we apply the preprocessing function to the train and test inputs. Then, we transform it into a TF-IDF matrix and train the logistic regression model","d94dbecc":"As we can see, a significant amount of the feature location is missing from the dataset. This is a feature that will not be explored in this work. The focus will be on textual data, which has no missing data.","53a7d641":"**References**\n\n* [GloVe: Global Vectors for Word Representation](https:\/\/nlp.stanford.edu\/projects\/glove\/)\n*  [Deep Learning Methods for Text Data](https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n* [How to Use Word Embedding Layers for Deep Learning with Keras](https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/)\n* [Natural Language Processing in Python](https:\/\/www.youtube.com\/watch?v=xvqsFTUsOmc)\n* [Basic EDA Cleaning and Glove](https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\/input)\n* [NLP With Disaster Tweets - EDA Cleaning and Bert](https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert#7.-Model)\n* [Keras LSTM Tutorial](https:\/\/adventuresinmachinelearning.com\/keras-lstm-tutorial\/)","55e2f30b":"After defining the cleaning fuctions, we can apply that to the input text data.","31608fa6":"Finally, we apply a simple neural network model to train the data. In addition to the embedding layer, we will also add a LSTM layer.\n\nA LSTM network is a kind of recurrent neural network. A recurrent neural network is a neural network that attempts to model time or sequence dependent behaviour \u2013 such as language, stock prices, electricity demand and so on."}}