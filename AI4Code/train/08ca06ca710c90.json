{"cell_type":{"7b6a0887":"code","de6aa146":"code","67cd539e":"code","99e75864":"code","5f8d1e6c":"code","92321275":"code","a6d03fe5":"code","d16dd43b":"code","73d6dd4d":"code","9ad4d3c0":"code","0262523e":"code","35bbd385":"markdown","9cff1aa3":"markdown","77913441":"markdown","c5aeabeb":"markdown","cab18fdd":"markdown","8807a0fa":"markdown","cdd1098e":"markdown"},"source":{"7b6a0887":"import cupy as cp\nimport cudf\n\nimport pandas as pd\nimport numpy as np\n\nimport cuml\nimport glob\nfrom tqdm import tqdm\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\n\nimport matplotlib.pyplot as plt\n\nPATH = \"\/kaggle\/input\/optiver-realized-volatility-prediction\"","de6aa146":"# Clusters for feature engeneering\n\ndef load_data(mode, path=\"\/kaggle\/input\/optiver-realized-volatility-prediction\"):\n    # mode = \"train\"\/\"test\"\n    file_name = f'{path}\/{mode}.csv'\n    return pd.read_csv(file_name)\n\ndev_df = load_data(\"train\", path=PATH)\ntrain_p = dev_df.pivot(index='time_id', columns='stock_id', values='target')\ntrain_p.head()\n\n# use correlation matrix for relative error\n\ncorr = train_p.corr()\nids = corr.index\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\nfor lst in l:\n    print(lst)\n    \nclusters = [l[0], l[1], l[3], l[4], l[6]]\n\n# oreder and trade books\norder_book_training = glob.glob(f'{PATH}\/book_train.parquet\/*\/*')\norder_book_test = glob.glob(f'{PATH}\/book_test.parquet\/*\/*')\n\ntrades_training = glob.glob(f'{PATH}\/trade_train.parquet\/*\/*')\ntrades_test = glob.glob(f'{PATH}\/trade_test.parquet\/*\/*')","67cd539e":"%cd \/kaggle\/input\/rapids-kaggle-utils\/","99e75864":"import cu_utils.transform as cutran\n\ndef log_diff(df, in_col):\n    null_val = -9999\n    df[\"logx\"] = df[in_col].log()\n    shifted = (df[[\"time_id\", in_col]].groupby(\"time_id\")\n                             .apply_grouped(cutran.get_cu_shift_transform(shift_by=1, null_val=null_val),\n                                            incols={in_col: 'x'},\n                                            outcols=dict(y_out=cp.float32),\n                                            tpb=32)[\"y_out\"])\n    res = df[in_col].log() - shifted.log()\n    res[shifted == null_val] = 0.0\n    return res\n\ndef realized_volatility(s):\n    return s.sum()\n\ndef extract_raw_book_features(df, null_val=-9999):\n    df['wap1']=(df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    df['wap2']=(df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])     \n    df['wap3']=(df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    df['wap4']=(df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])                                                                                  \n    for n in [1,2,3,4]:\n        df[f\"square_log_return{n}\"] = log_diff(df, in_col=f\"wap{n}\")**2    \n    \n    df['wap_balance'] = df['wap1'] - df['wap2']\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread1'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    return df\n\n\ndef extract_raw_trade_features(df):\n    df[\"trade_log_return\"] = log_diff(df, in_col=f\"price\")\n    df[\"trade_square_log_return\"] = df[\"trade_log_return\"] ** 2\n    df[\"tendency\"] = df[\"trade_log_return\"] * df['size']\n    df[\"goes_up\"] = df[\"trade_log_return\"] > 0\n    df[\"goes_down\"] = df[\"trade_log_return\"] < 0\n    df[\"amount\"] = df['price'] * df['size']\n    \n    return df\n\n\ndef agg(df, feature_dict):\n    agg_df = df.groupby(\"time_id\").agg(feature_dict).reset_index()\n    def f(x):\n        if x[1] == \"\":\n            return x[0]\n        return x[0] + \"_\" + x[1]\n    \n    agg_df.columns = [f(x) for x in agg_df.columns]\n    col_vol=[col for col in agg_df.columns if 'square_log_return' in col and ('mean' in col or 'sum' in col)]\n    if col_vol:\n        agg_df[col_vol]=agg_df[col_vol].sqrt()\n    return agg_df    \n\n\ndef extract_book_stats(df):\n    feature_dict = {\n        'wap1': [\"sum\", \"std\"],\n        'wap2': [\"sum\", \"std\"],\n        'wap3': [\"sum\", \"std\"],\n        'wap4': [\"sum\", \"std\"],\n        'square_log_return1': [\"sum\"],\n        'square_log_return2': [\"sum\"],\n        'square_log_return3': [\"sum\"],\n        'square_log_return4': [\"sum\"],\n        'price_spread': [\"sum\", \"max\"],\n        'price_spread1': [\"sum\", \"max\"],\n        'wap_balance': [\"sum\", \"max\"],\n        'bid_spread': [\"sum\", \"max\"],\n        'ask_spread': [\"sum\", \"max\"],\n        'total_volume': [\"sum\", \"max\"],\n        'volume_imbalance': [\"sum\", \"max\"],\n        \"bid_ask_spread\":[\"sum\", \"max\"],\n    }\n    \n    return agg(df, feature_dict)\n    \n    \ndef extract_trade_stats(df):\n    feature_dict = {\n        'price': ['std'],\n        'trade_square_log_return': [\"sum\"],\n        'seconds_in_bucket':[\"nunique\"],\n        'size': [\"sum\",'max', 'min', 'std'],\n        'order_count': [\"sum\",'max'],\n        'amount':['sum','max', 'min'],\n        'tendency': ['sum'],\n        'goes_up': ['sum'],\n        'goes_down': ['sum'],\n    }\n    return agg(df, feature_dict)\n\n\ndef extract_book_stats_time(df):\n    feature_dict = {\n        'square_log_return1': [\"sum\"],\n        'square_log_return2': [\"sum\"],\n        'square_log_return3': [\"sum\"],\n        'square_log_return4': [\"sum\"],\n    }\n    return agg(df, feature_dict)\n\ndef extract_trade_stats_time(df):\n    feature_dict = {\n        'trade_square_log_return': [\"sum\"],\n        'seconds_in_bucket': [\"nunique\"],\n        'size': [\"sum\"],\n        'order_count': [\"sum\"],\n    }\n    return agg(df, feature_dict)\n\n\ndef time_constraint_fe(df, stats_df, seconds_from, func):\n    sub_df = df[df[\"seconds_in_bucket\"] >= seconds_from].reset_index(drop=True)\n    return stats_df.merge(func(sub_df), on=\"time_id\", how=\"left\", suffixes=('', f'_{seconds_from}'))\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    vol_cols = ['square_log_return1_sum', 'square_log_return2_sum', 'square_log_return1_sum_400', 'square_log_return2_sum_400', \n                'square_log_return1_sum_300', 'square_log_return2_sum_300', 'square_log_return1_sum_200', 'square_log_return2_sum_200', \n                'trade_square_log_return_sum', 'trade_square_log_return_sum_400', 'trade_square_log_return_sum_300', 'trade_square_log_return_sum_200']\n\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.rename(columns = (lambda x: x + '_' + 'stock'))\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.rename(columns = (lambda x: x + '_' + 'time'))\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n\ndef feature_engineering(book_path, trade_path):\n    book_df = cudf.read_parquet(book_path)\n    book_df = extract_raw_book_features(book_df)\n    # full window starts for book\n    book_stats = extract_book_stats(book_df)\n    \n    trade_df = cudf.read_parquet(trade_path)\n    trade_df = extract_raw_trade_features(trade_df)\n    # full window stats for trade\n    trade_stats = extract_trade_stats(trade_df)\n    \n    # partial window stats\n    for s in [100, 200, 300, 400, 500]:\n        book_stats = time_constraint_fe(book_df, book_stats, s, extract_book_stats_time)\n        trade_stats = time_constraint_fe(trade_df, trade_stats, s, extract_trade_stats_time)\n    \n    trade_stats = trade_stats.rename(columns = (lambda x: \"trade_\" + x if x.startswith(\"seconds_in_bucket\") else x))\n    return book_stats.merge(trade_stats, on=\"time_id\", how=\"left\")\n    \ndef add_tau_features(df):\n    df['size_tau'] = np.sqrt( 1\/ df['trade_seconds_in_bucket_nunique'] )\n    df['size_tau_400'] = np.sqrt( 1\/ df['trade_seconds_in_bucket_nunique_400'] )\n    df['size_tau_300'] = np.sqrt( 1\/ df['trade_seconds_in_bucket_nunique_300'] )\n    df['size_tau_200'] = np.sqrt( 1\/ df['trade_seconds_in_bucket_nunique_200'] )\n    \n    df['size_tau2'] = np.sqrt( 1\/ df['order_count_sum'] )\n    df['size_tau2_400'] = np.sqrt( 0.33\/ df['order_count_sum'] )\n    df['size_tau2_300'] = np.sqrt( 0.5\/ df['order_count_sum'] )\n    df['size_tau2_200'] = np.sqrt( 0.66\/ df['order_count_sum'] )\n\n    # delta tau\n    df['size_tau2_d'] = df['size_tau2_400'] - df['size_tau2']\n    return df\n\ndef add_cluster_features(df, l):\n    features = [\n         'square_log_return1_sum',\n         'total_volume_sum',\n         'size_sum',\n         'order_count_sum',      \n         'price_spread_sum',  \n         'bid_spread_sum',\n         'ask_spread_sum',   \n         'volume_imbalance_sum',      \n         'bid_ask_spread_sum',\n         'size_tau2',\n    ]\n    \n    mat = []\n    for n, ind in enumerate(l):\n        newdf = df[['time_id'] + features][df['stock_id'].isin(ind)]\n        if newdf.shape[0] > 0:\n            newdf = newdf.groupby(['time_id']).agg(\"mean\").reset_index()\n            newdf['stock_id'] = \"c\" + str(n)\n            #print(newdf.columns)\n            mat.append(newdf)\n        else:\n            newdf = cudf.DataFrame()\n            newdf['time_id'] = df['time_id'].unique()\n            for f in features:\n                newdf[f] = 0.0\n            \n            newdf['stock_id'] = \"c\" + str(n)\n            #print(newdf.columns)\n            mat.append(newdf)\n\n    mat1 = cudf.concat(mat).reset_index()\n    if 'index' in mat1.columns:\n        mat1 = mat1.drop(['index'], axis=1)\n    mat1 = mat1.pivot(index='time_id', columns='stock_id')\n    mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n    mat1.reset_index(inplace=True)\n    \n    return cudf.merge(df, mat1, how='left', on='time_id')        \n    \n\ndef process_data(order_book_paths, trade_paths, clusters):\n    stock_dfs = []\n    for book_path, trade_path in tqdm(list(zip(order_book_paths, trade_paths))):\n        stock_id = int(book_path.split(\"=\")[1].split(\"\/\")[0])\n\n        df = feature_engineering(book_path, trade_path)\n        df[\"stock_id\"] = stock_id\n        stock_dfs.append(df)\n    total_df = cudf.concat(stock_dfs)\n    total_df = get_time_stock(total_df)\n    total_df = add_tau_features(total_df)\n    total_df = add_cluster_features(total_df, clusters)\n    \n    return total_df\n\ntrain = process_data(order_book_training, trades_training, clusters).to_pandas()\ntest = process_data(order_book_test, trades_test, clusters).to_pandas()\n\ndef add_row_id(df):\n    df['row_id'] = df.apply(lambda x: f\"{int(x['stock_id'])}-{int(x['time_id'])}\", axis=1)\n    return df\n\ntrain = add_row_id(train)\ntest = add_row_id(test)","5f8d1e6c":"for i, name in enumerate(test.columns):\n    print(f\"{i}.\\t{name}\\t{train.columns[i]}\")","92321275":"%cd \/kaggle\/working\/","a6d03fe5":"# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\ntrain_y, test_y = read_train_test()\n\n# Merge with books with target\n\ntrain['row_id'] = train['row_id'].astype(str)\ntrain_y['row_id'] = train_y['row_id'].astype(str)\n\ntrain_ = train_y.drop(['stock_id', 'time_id'], axis=1).merge(train, on = ['row_id'], how = 'left')\ntest_ = test_y.drop(['stock_id', 'time_id'], axis=1).merge(test, on = ['row_id'], how = 'left')","d16dd43b":"# Function to early stop with root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\ndef train_and_evaluate_lgb_cvtimeid(train, test, iters=500):\n    # Hyperparammeters (just basic)\n    \n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n    cath_idx = features.index('stock_id')\n\n    seed0=2021\n    seed1=2022\n    seed2=2023\n    seed3=2024\n    params = {\n        'objective': 'rmse',\n        'boosting_type': 'gbdt',\n        'max_depth': -1,\n        'max_bin':100,\n        'min_data_in_leaf':500,\n        'learning_rate': 0.05,\n        'subsample': 0.72,\n        'subsample_freq': 4,\n        'feature_fraction': 0.5,\n        'lambda_l1': 0.5,\n        'lambda_l2': 1.0,\n        'categorical_column':[cath_idx],\n        'seed':seed0,\n        'feature_fraction_seed': seed0,\n        'bagging_seed': seed0,\n        'drop_seed': seed0,\n        'data_random_seed': seed0,\n        'n_jobs':-1,\n        'verbose': -1}\n    \n    y = train['target']\n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n    # Iterate through each fold\n    \n    train_time_ids = train['time_id'].unique()\n    for fold, (time_trn_ind, time_val_ind) in enumerate(kfold.split(train_time_ids)):\n        trn_msk = train['time_id'].isin(train_time_ids[time_trn_ind])\n        val_msk = ~trn_msk\n        \n        print(f'Training fold {fold + 1}')\n        \n        x_train, x_val = train[trn_msk], train[val_msk]\n        y_train, y_val = y[trn_msk], y[val_msk]\n        \n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        \n        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n        \n        model = lgb.train(params = params,\n                          num_boost_round=iters,\n                          train_set=train_dataset, \n                          valid_sets=[train_dataset, val_dataset], \n                          verbose_eval = 0,\n                          early_stopping_rounds=0,\n                          feval = feval_rmspe)\n        \n        # Add predictions to the out of folds array\n        oof_predictions[val_msk] = model.predict(x_val[features])\n        \n        # Predict the test set\n        test_predictions += model.predict(test[features]) \/ 5\n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    #lgb.plot_importance(model,max_num_features=20)\n    # Return test predictions\n    return test_predictions, oof_predictions\n\ndef train_and_evaluate_lgb_cvtimeid_stockid(train, test, iters=500):\n    # Hyperparammeters (just basic)\n    \n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n    cath_idx = features.index('stock_id')\n\n    seed0=2021\n    seed1=2022\n    seed2=2023\n    seed3=2024\n    params = {\n        'objective': 'rmse',\n        'boosting_type': 'gbdt',\n        'max_depth': -1,\n        'max_bin':100,\n        'min_data_in_leaf':500,\n        'learning_rate': 0.05,\n        'subsample': 0.72,\n        'subsample_freq': 4,\n        'feature_fraction': 0.5,\n        'lambda_l1': 0.5,\n        'lambda_l2': 1.0,\n        'categorical_column':[cath_idx],\n        'seed':seed0,\n        'feature_fraction_seed': seed0,\n        'bagging_seed': seed0,\n        'drop_seed': seed0,\n        'data_random_seed': seed0,\n        'n_jobs':-1,\n        'verbose': -1}\n    \n    y = train['target']\n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = 5, random_state = 2021, shuffle = True)\n    # Iterate through each fold\n    \n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        \n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        \n        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n        \n        model = lgb.train(params = params,\n                          num_boost_round=iters,\n                          train_set=train_dataset, \n                          valid_sets=[train_dataset, val_dataset], \n                          verbose_eval = 0,\n                          early_stopping_rounds=0,\n                          feval = feval_rmspe)\n        \n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val[features])\n        \n        # Predict the test set\n        test_predictions += model.predict(test[features]) \/ 5\n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    #lgb.plot_importance(model,max_num_features=20)\n    # Return test predictions\n    return test_predictions, oof_predictions","73d6dd4d":"def get_split_train_test(train, frac, seed=414928305):\n    all_time_ids = train['time_id'].unique()\n    \n    np.random.seed(seed)\n    msk = np.random.rand(len(all_time_ids)) < frac\n    \n    train_time_ids = all_time_ids[msk]\n    valid_time_ids = all_time_ids[~msk]\n    \n    train_split = train[train['time_id'].isin(train_time_ids)]\n    valid_split = train[train['time_id'].isin(valid_time_ids)]\n    \n    return train_split, valid_split\n\ndef do_split_experiment(train, train_and_eval_func, seed=57001):\n    train_split, valid_split = get_split_train_test(train, 0.8, seed=seed)\n    train_split_target = np.zeros(len(train_split))\n    valid_split_target = np.zeros(len(valid_split))\n    for i, x in enumerate(train_split['target']):\n        train_split_target[i] = x\n    for i, x in enumerate(valid_split['target']):\n        valid_split_target[i] = x\n    \n    # learn LGBM\n    iters = [300, 600, 900, 1200, 1500]\n    errs_oof = []\n    errs_val = []\n    for n_iters in iters:\n        predictions_lgb, oof_predictions_lgb = train_and_eval_func(\n            train_split, valid_split, iters=n_iters)\n        errs_oof.append(rmspe(train_split_target, oof_predictions_lgb))\n        errs_val.append(rmspe(valid_split_target, predictions_lgb))\n        \n    plt.clf()\n    plt.plot(iters, errs_oof, color='blue')\n    plt.plot(iters, errs_val, color='red')\n    plt.show() ","9ad4d3c0":"do_split_experiment(train_, train_and_evaluate_lgb_cvtimeid_stockid)","0262523e":"do_split_experiment(train_, train_and_evaluate_lgb_cvtimeid)","35bbd385":"# Feature engeneering\n\ntaken mostly from here \nhttps:\/\/www.kaggle.com\/nishanthaddagatla\/lgbm-baseline\n\nand here\nhttps:\/\/www.kaggle.com\/alexioslyon\/accelerating-trading-on-gpu-via-rapids","9cff1aa3":"# OOF by pairs ['time_id', 'stock_id']\n\nWe compare the OOF score with out-of-sample score, where out-of-sample is taken randomly from the train set by taking 20% of the time_ids, and the remaining 80% are treated as the training set","77913441":"# OOF by 'time_id'\n\nWe compare the OOF score with out-of-sample score, where out-of-sample is taken randomly from the train set by taking 20% of the time_ids","c5aeabeb":"# LGBM\n\n","cab18fdd":"Some of the comments in the notebooks posted here remarked that improvements the out of folds often lead to worse score. I believe this is related to the way the sampling is done in the CV.\n\nFor instance, this notebook\n\nhttps:\/\/www.kaggle.com\/nishanthaddagatla\/lgbm-baseline\n\nand many others split the whole matrix consisting of pairs ['stock_id', 'time_id'] into five folds.\n\nIn the training dataset majority of the time id's include all of the 112 stock ids, with little of them including just 111. This means that the sampling (or splitting in training and test dataset) is done by 'time_id' rather than by the pair ['stock_id', 'time_id']. This suggests that the CV should do the same.\n\nI tried to do CV both ways. Doing CV by pairs ['stock_id', 'time_id'] suggests that LGBM improves when we take the number of iterations up to 2000 (notice that LGBM rarely finishes through early stopping this way, which means you don't even need the validation set). However with this amount of iterations the score worsens.\n\nIf you do the splitting into folds via the time id, it indeed shows that the score worsens with 2000 iterations.","8807a0fa":"this looks better (colors same as above)","cdd1098e":"In the above the blue line is the OOF score and the red line is the out-of-sample score. We can see that the two lines are inconsistent."}}