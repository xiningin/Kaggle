{"cell_type":{"a9290a5a":"code","edb2cae5":"code","e0a8c8f0":"code","ad52c64a":"code","7c880121":"code","1b09c631":"code","2d5a1c93":"code","a33ab191":"code","7845da2e":"code","f1e37d74":"code","a18a8da6":"code","c80886b2":"code","8380b2f0":"markdown","12537d2e":"markdown","b930d426":"markdown","ce4d5463":"markdown","290a3a47":"markdown","0e567d57":"markdown","e186a0fb":"markdown","ff3c60d4":"markdown","519d58e1":"markdown","386f330c":"markdown","e0124e6d":"markdown","a335e78c":"markdown"},"source":{"a9290a5a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ndf = pd.read_csv('..\/input\/train.csv')\ndf.head()","edb2cae5":"enumSeries = []\nnumSeries = []\nnonDiscreteSeries = []\nfor colname in df.columns:\n    datatype = str(df[colname].dtype)\n    if colname == 'Id' or colname == 'SalePrice':\n        pass\n    elif datatype == 'int64' or datatype == 'float64':\n        numSeries.append(colname)\n    else:\n        enumSeries.append(colname)\ndf.fillna(pd.Series([0,1970,0], index=['LotFrontage', 'GarageYrBlt','MasVnrArea']), inplace=True)\nassert df[pd.isnull(df[numSeries]).any(axis=1)][numSeries].shape[0] == 0\ndf.head()","e0a8c8f0":"from scipy.stats import shapiro\n\ntotalCount = df.shape[0]\ncandidateForLog10 = []\nfor seriesName in numSeries:\n    vcount = df[seriesName].value_counts().shape[0]\n    zeros = np.sum((df[seriesName] == 0).values)\n    if (vcount > totalCount \/ 10 and zeros == 0):\n        shapLog10 = shapiro(np.log10(df[seriesName].values))[0]\n        shapAsIs = shapiro(df[seriesName].values)[0]\n        candidateForLog10.append((seriesName, shapLog10, shapAsIs, shapLog10 - shapAsIs))\ncandidateForLog10","ad52c64a":"df['LotAreaLog10'] = np.log10(df['LotArea'].values)\ndf['1stFlrSFLog10'] = np.log10(df['1stFlrSF'].values)\ndf['GrLivAreaLog10'] = np.log10(df['GrLivArea'].values)\ndf['SalePriceLog10'] = np.log10(df['SalePrice'].values)\nif 'LotArea' in numSeries:\n    numSeries.remove('LotArea')\n    numSeries.append('LotAreaLog10')\nif '1stFlrSF' in numSeries:\n    numSeries.remove('1stFlrSF')\n    numSeries.append('1stFlrSFLog10')\nif 'GrLivArea' in numSeries:\n    numSeries.remove('GrLivArea')\n    numSeries.append('GrLivAreaLog10')\ndf[numSeries].head()","7c880121":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\n\nheaddf = df.head(int(3.0 * df.shape[0] \/ 4.0) - 1)\ntaildf = df.tail(int(1.0 * df.shape[0] \/ 4.0))\nprint(str(headdf.shape[0]) + ' of the training set for training')\n\nridge = Ridge()\nparameters = { 'alpha': [ 1e-15, 1e-10, 1e-8, 1e-4, 1e-3, 1e-2, 1, 5, 10, 20 ]}\nridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=5)\nridge_regressor.fit(headdf[numSeries].values, headdf['SalePriceLog10'])\nprint(ridge_regressor.best_params_)","1b09c631":"from sklearn.metrics import mean_absolute_error\nridge = Ridge(alpha=1)\nridge.fit(headdf[numSeries].values, headdf['SalePriceLog10'])\nridgePredictions = np.power(10, ridge.predict(taildf[numSeries].values))\ntargets = np.power(10, taildf['SalePriceLog10'])\nmae = mean_absolute_error(ridgePredictions, targets)\nmae","2d5a1c93":"numSeries = abs(df[numSeries].corrwith(df['SalePriceLog10'])).sort_values(ascending=False).keys().values.tolist()\nnumSeries","a33ab191":"from sklearn.metrics import mean_absolute_error\nimport matplotlib.pyplot as plt\n\nmaeResults = []\nmaeNames = []\nfor endIndex in range(1, len(numSeries)):\n    ridge = Ridge(alpha=1)\n    ridge.fit(headdf[numSeries[0:endIndex]].values, headdf['SalePriceLog10'])\n    ridgePredictions = np.power(10, ridge.predict(taildf[numSeries[0:endIndex]].values))\n    targets = np.power(10, taildf['SalePriceLog10'])\n    mae = mean_absolute_error(ridgePredictions, targets)\n    maeResults.append(mae)\n    if endIndex > 1:\n        maeNames.append(str(endIndex) + ' characteristics')\n    else:\n        maeNames.append(str(endIndex) + ' characteristic')\nmaeResultsSeries = pd.Series(maeResults, index=maeNames)\nplt.plot(range(1, maeResultsSeries.size + 1), maeResultsSeries, marker='o',);","7845da2e":"queries = []\nqueries.append('MSZoning != \"RM\" and GarageFinish != \"Fin\" and ExterQual != \"Gd\"')\nqueries.append('MSZoning != \"RM\" and GarageFinish != \"Fin\" and ExterQual == \"Gd\"')\nqueries.append('MSZoning != \"RM\" and GarageFinish == \"Fin\" and ExterQual != \"Gd\"')\nqueries.append('MSZoning != \"RM\" and GarageFinish == \"Fin\" and ExterQual == \"Gd\"')\nqueries.append('MSZoning == \"RM\"')\nmaeResults = []\nmaeNames = []\nridgePredictions = []\ntargets = []\nfor query in queries:\n    ridge = Ridge(alpha=1)\n    ridge.fit(headdf.query(query)[numSeries].values, headdf.query(query)['SalePriceLog10'])\n    preds = np.power(10, ridge.predict(taildf.query(query)[numSeries].values))\n    ridgePredictions.extend(preds)\n    targets.extend(np.power(10, taildf.query(query)['SalePriceLog10']))\nmae = mean_absolute_error(ridgePredictions, targets)\nmae","f1e37d74":"from sklearn.linear_model import Lasso\n\nlasso = Lasso()\nparameters = { 'alpha': [ 0.01, 0.1, 1, 5, 10, 20 ]}\nlasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=5)\nlasso_regressor.fit(headdf[numSeries].values, headdf['SalePriceLog10'])\nprint(lasso_regressor.best_params_)","a18a8da6":"maeResults = []\nmaeNames = []\nlassoPredictions = []\ntargets = []\nfor query in queries:\n    lasso = Lasso(alpha=0.01)\n    lasso.fit(headdf.query(query)[numSeries].values, headdf.query(query)['SalePriceLog10'])\n    preds = np.power(10, lasso.predict(taildf.query(query)[numSeries].values))\n    lassoPredictions.extend(preds)\n    targets.extend(np.power(10, taildf.query(query)['SalePriceLog10']))\nmae = mean_absolute_error(lassoPredictions, targets)\nmae","c80886b2":"tdf = pd.read_csv('..\/input\/test.csv')\ntdf.fillna(pd.Series([0,1970,0, 5, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0], index=['LotFrontage', 'GarageYrBlt', 'MasVnrArea', 'OverallQual', 'GrLivArea10', 'GarageCars', '1stFlrSF10', 'TotalBsmtSF', 'GarageArea', 'BsmtFullBath', 'BsmtUnfSF', 'BsmtHalfBath', 'BsmtFinSF2', 'BsmtFinSF1']), inplace=True)\ntdf['LotAreaLog10'] = np.log10(tdf['LotArea'].values)\ntdf['1stFlrSFLog10'] = np.log10(tdf['1stFlrSF'].values)\ntdf['GrLivAreaLog10'] = np.log10(tdf['GrLivArea'].values)\nassert tdf[pd.isnull(tdf[numSeries]).any(axis=1)][numSeries].shape[0] == 0\nmaeResults = []\nmaeNames = []\nridgePredictions = []\nridgePredictionIds = []\nfor query in queries:\n    ridge = Ridge(alpha=1)\n    ridge.fit(df.query(query)[numSeries].values, df.query(query)['SalePriceLog10'])\n    preds = np.power(10, ridge.predict(tdf.query(query)[numSeries].values))\n    ridgePredictions.extend(preds)\n    ridgePredictionIds.extend(tdf.query(query)['Id'].values)\nres = pd.DataFrame([ridgePredictionIds, ridgePredictions]).transpose().rename(columns={0: 'Id', 1: 'SalePrice'}).sort_values(by=['Id'])\nres.Id = res.Id.astype(int)\nres.to_csv('submission.csv', index=False)\nres","8380b2f0":"So the conclusion is $\\alpha = 1$. Let's get the mean absolute error for this model.","12537d2e":"In the previous kernel we sort of shot-from-the-hip and took the logarithm of `1stFlrSF` and `GrLivArea`. We were only considering four numeric values in our linear model so it was easy to evaluate each individually. In this kernel, evaluating each individually is more challenging. So I decided to take the logarithm of series that have a) no zero values, b) a great diversity of values and c) whose W statistic improves when the logarithm is used. Turns out that just leaves three series to take the logarithm of anyway.","b930d426":"Let's import the data per usual...","ce4d5463":"Possibly an improvement over the previous linear model. Ridge regression penalises co-efficient size but having smaller co-efficients is not quite the same as having fewer features outright. I wonder how the performance changes if we reduce the size of our feature vectors. First I want to rearrange the `numSeries` so we drop some of our worst features first.","290a3a47":"It's good. There's a rumour that lasso regression which peanlises co-efficients in a different way further improves performance. Lasso is likely to set certain co-efficients to exactly 0 - so partly solves our feature selection problem. However we once again need to find an $\\alpha$.","0e567d57":"In this instance the lasso has not exceeded the ridge reegression's performance. I'd like to convert all enumerated series to numeric values and retry a ridge and lasso regression to gauge performance. But before that I want to make a submission with the five-choice ridge regression. So here goes...","e186a0fb":"I'm going to extract my numeric and enumerated series. With a little more experience I'm going to immediately fill in the `NaN`s and assert that there are none present in the numeric series.","ff3c60d4":"With this value let's once again try our five-choice approach.","519d58e1":"Now let's repeat the above systematically dropping features. I'm going to keep $\\alpha = 1$ for the moment.","386f330c":"The result is pretty clear, there's not a lot we don't want in there. I'm tempted to grab our previous five-choice approach and see how that affects performance.","e0124e6d":"Let's put the above information to good use...","a335e78c":"After this prep, we can undertake a ridge regression. I want to partition our training set this time because I want to experiment a bit this time."}}