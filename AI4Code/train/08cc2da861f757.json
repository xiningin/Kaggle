{"cell_type":{"88e470c7":"code","7cb7eabb":"code","dfc0c473":"code","a38ed916":"code","910c147c":"code","536bfe51":"code","866e25df":"code","4966eeaa":"code","8e252967":"code","d3794437":"code","3dc9d3b0":"code","ab87422e":"code","adc0091e":"code","e159aba7":"code","a71b69d8":"code","80de3226":"code","dbf2b59c":"code","dea8112f":"code","2e5a26e5":"code","5aef77a7":"code","e70199b9":"code","2f9b6ae4":"code","1592c466":"code","ef694f8f":"code","3718b236":"code","72a029bb":"code","07943944":"code","06203ded":"markdown","37cffb07":"markdown","e08d42e3":"markdown","0ccbeee5":"markdown","4dd2c14c":"markdown","bd410cfd":"markdown","d07ef804":"markdown","b41085da":"markdown","f4d29af0":"markdown","b89e92f5":"markdown","f686cd8c":"markdown","e574b42b":"markdown"},"source":{"88e470c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7cb7eabb":"import os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\nwarnings.filterwarnings('ignore')","dfc0c473":"# import datasets\ntrain_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","a38ed916":"train_df['num_nulls'] = train_df.drop(['id', 'claim'], axis = 1).isna().sum(axis = 1)\ntest_df['num_nulls'] = test_df.drop(['id'], axis = 1).isna().sum(axis = 1)","910c147c":"%%time\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import QuantileTransformer,  KBinsDiscretizer\nfrom sklearn.impute import SimpleImputer\n\nfeatures = [col for col in train_df.columns if col not in ['claim', 'id']]\npipe = Pipeline([\n        ('imputer', SimpleImputer(strategy='median',missing_values=np.nan)),\n        (\"scaler\", QuantileTransformer(n_quantiles=64,output_distribution='uniform')),\n        ('bin', KBinsDiscretizer(n_bins=64, encode='ordinal',strategy='uniform'))\n        ])\ntrain_df[features] = pipe.fit_transform(train_df[features])\ntest_df[features] = pipe.transform(test_df[features])","536bfe51":"# import packages\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBClassifier \nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom tqdm import tqdm\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nimport optuna","866e25df":"OPTUNA_OPTIMIZATION = True\n\ndef objective(trial):    \n    params = {\n        'iterations':trial.suggest_int(\"iterations\", 1000, 20000),\n        'objective': trial.suggest_categorical('objective', ['Logloss', 'CrossEntropy']),\n        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n        'od_wait':trial.suggest_int('od_wait', 500, 2000),\n        'learning_rate' : trial.suggest_uniform('learning_rate',0.001,1),\n        'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n        'random_strength': trial.suggest_uniform('random_strength', 10, 50),\n        'depth': trial.suggest_int('depth',1 , 15),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 30),\n        'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations', 1, 15),\n        'task_type' : 'GPU',\n        'devices' : '0'\n    }\n    \n    if params['bootstrap_type'] == 'Bayesian':\n        params['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n    elif params['bootstrap_type'] == 'Bernoulli':\n        params['subsample'] = trial.suggest_float('subsample', 0.1, 1)\n\n    training_df, validation_df =  train_test_split(train_df, test_size=0.2, shuffle=True, random_state=1)\n\n    model = CatBoostClassifier(**params)\n    \n    x_train = training_df.drop(['claim', 'id'],axis=1) \n    y_train = training_df['claim']\n    x_valid = validation_df.drop(['claim', 'id'],axis=1) \n    y_valid = validation_df['claim']\n    model.fit(\n        x_train , y_train,\n        eval_set=[(x_valid, y_valid)],\n        early_stopping_rounds=100,\n        use_best_model=True,\n        verbose=0\n    )\n    \n    return roc_auc_score(y_valid,  model.predict_proba(x_valid)[:,1])","4966eeaa":"%%time\nstudy = optuna.create_study(\n    direction='maximize',\n    study_name='CatbClf'\n)\n\nstudy.optimize(\n    objective,\n    n_trials=100\n)","8e252967":"print(f\"Best Trial: {study.best_trial.value}\")\nprint(f\"Best Params: {study.best_trial.params}\")","d3794437":"optuna.visualization.plot_optimization_history(study)","3dc9d3b0":"optuna.visualization.plot_parallel_coordinate(study)","ab87422e":"optuna.visualization.plot_param_importances(study)","adc0091e":"del study","e159aba7":"OPTUNA_OPTIMIZATION = True\n\ndef objective(trial):    \n    params = {\n            'n_estimators':trial.suggest_int(\"n_estimators\", 1000, 20000),\n            'learning_rate' : trial.suggest_uniform('learning_rate', 0.001, 1),\n            'subsample': trial.suggest_uniform('subsample', 0.1, 1),\n            'colsample_bytree':trial.suggest_uniform('colsample_bytree', 0.1, 1),\n            'max_depth': trial.suggest_categorical('max_depth', [1,3,5,7,9,11,13,15,17,20]),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-5, 10.0),\n            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-5, 10.0),\n            'tree_method': 'gpu_hist'\n        }\n    \n    model = XGBClassifier(**params)\n    \n    training_df, validation_df =  train_test_split(train_df, test_size=0.2, shuffle=True, random_state=1)\n\n    x_train = training_df.drop(['claim', 'id'],axis=1) \n    y_train = training_df['claim']\n    x_valid = validation_df.drop(['claim', 'id'],axis=1) \n    y_valid = validation_df['claim']\n\n    model.fit(\n        x_train , y_train,\n        eval_set=[(x_valid, y_valid)],\n        early_stopping_rounds=100,\n        verbose=0\n    )\n    \n    return roc_auc_score(y_valid,  model.predict_proba(x_valid)[:,1])","a71b69d8":"%%time\nstudy = optuna.create_study(\n    direction='maximize',\n    study_name='XG_boost'\n)\n\nstudy.optimize(\n    objective,\n    n_trials=100\n)","80de3226":"print(f\"Best Trial: {study.best_trial.value}\")\nprint(f\"Best Params: {study.best_trial.params}\")","dbf2b59c":"optuna.visualization.plot_optimization_history(study)","dea8112f":"optuna.visualization.plot_parallel_coordinate(study)","2e5a26e5":"optuna.visualization.plot_param_importances(study)","5aef77a7":"del study","e70199b9":"OPTUNA_OPTIMIZATION = True\n\ndef objective(trial):    \n    params = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"verbose\": -1,\n        \"boosting_type\": \"gbdt\",\n        'n_estimators':trial.suggest_int(\"n_estimators\", 1000, 20000),\n        'learning_rate' : trial.suggest_uniform('learning_rate', 0.001, 1),\n        \"reg_alpha\": trial.suggest_loguniform(\"reg_alpha\", 1e-5, 10.0),\n        \"reg_lambda\": trial.suggest_loguniform(\"reg_lambda\", 1e-5, 10.0),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.1, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.1, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 300),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 300),\n        'colsample_bytree':trial.suggest_uniform('colsample_bytree', 0.1, 1),\n        \"device_type\" : \"gpu\"\n    }\n\n    training_df, validation_df =  train_test_split(train_df, test_size=0.2, shuffle=True, random_state=1)\n\n    model = LGBMClassifier(**params)\n    \n    x_train = training_df.drop(['claim', 'id'],axis=1) \n    y_train = training_df['claim']\n    x_valid = validation_df.drop(['claim', 'id'],axis=1) \n    y_valid = validation_df['claim']\n    model.fit(\n        x_train , y_train,\n        eval_set=[(x_valid, y_valid)],\n        early_stopping_rounds=100,\n        verbose=-1,\n    )\n    \n    return roc_auc_score(y_valid,  model.predict_proba(x_valid)[:,1])","2f9b6ae4":"%%time\nstudy = optuna.create_study(\n    direction='maximize',\n    study_name='LGDM'\n)\n\nstudy.optimize(\n    objective,\n    n_trials=100\n)","1592c466":"print(f\"Best Trial: {study.best_trial.value}\")\nprint(f\"Best Params: {study.best_trial.params}\")","ef694f8f":"optuna.visualization.plot_optimization_history(study)","3718b236":"optuna.visualization.plot_parallel_coordinate(study)","72a029bb":"optuna.visualization.plot_param_importances(study)","07943944":"del study","06203ded":"Lets look at params dependencies with accuracy","37cffb07":"# LGDM CLASSIFIER Best Params","e08d42e3":"Lets look at params dependencies with accuracy","0ccbeee5":"Delete Study","4dd2c14c":"# Removal of null values. (Also a bit of Preprocessing) \nBut we can't drop the rows owing to the large amount single null rows","bd410cfd":"Lets look at params dependencies with accuracy","d07ef804":"# Ok now Lets find the params for our models \n3 models to be looked here are -> **XGBoost, CatBoost, LGDM**","b41085da":"Delete study from RAM","f4d29af0":"# CAT Boost Best Params","b89e92f5":"Hi,\\\n**Welcome to my Notebook !!**.\\\nThe following notebook deals with finding the best params with optuna study.\n\n**Please leave an upvote or feedback, if you like or use at part of it. \\\nWould encourage me more to share.**\n\n\nI have finally used the obtained params in this notebook:- \\\nhttps:\/\/www.kaggle.com\/skiller\/a-z-predictive-modelling-welcome \n\nI have **not doing any EDA in the current notebook** as i have already taken care of it in the above notebook. \n\nWarm Regards","f686cd8c":"# XGBOOST Best Params","e574b42b":"Delete Study from RAM"}}