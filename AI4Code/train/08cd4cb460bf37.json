{"cell_type":{"9b20b6f6":"code","cfc8741d":"code","5d07a239":"code","91c556ab":"code","57988e97":"code","8e98c5f5":"code","ad246bc9":"code","624b89e1":"code","8b9b9e56":"code","5eab07a8":"code","71122bd7":"code","ca85edde":"code","f390be00":"code","dc9fd60d":"code","9e1b4e2a":"code","95f2b1c4":"code","53533a78":"code","3ddb75c7":"code","eeae647a":"code","b8c3ba1d":"code","78e149f0":"code","d4fc59b5":"code","b144bd81":"code","5cfe667b":"code","c98b1220":"code","697fa1cc":"code","3c5fe47a":"code","77eec8dd":"markdown","d41b39e4":"markdown","c9d44753":"markdown","fe892139":"markdown","f71b30c6":"markdown"},"source":{"9b20b6f6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cfc8741d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n# File system manangement\nimport os\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score ,classification_report\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split , cross_val_score , RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler ,MinMaxScaler ,LabelEncoder\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n# Suppress warnings \nimport warnings\nwarnings.filterwarnings('ignore')\n","5d07a239":"#load the input data\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\nprint(train.shape, test.shape)","91c556ab":"#lets do some data analysis\nprint(\"Null in Train data\")\nprint(train.isnull().sum())\nprint(\"Null in Test data\")\nprint(test.isnull().sum())","57988e97":"print (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())","8e98c5f5":"print (train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean())\n# gender also has an impact on results","ad246bc9":"#feature engineering \n# create a new column Family\ntrain['Family'] = train['SibSp'] + train['Parch'] + 1\ntest['Family'] = test['SibSp'] + test['Parch'] + 1\n# create another feature isAlone, dervied from family\ntrain['isAlone'] = np.where(train['Family'] > 1, 0,1)\ntest['isAlone'] = np.where(test['Family'] > 1, 0,1)\nprint (train[[\"isAlone\", \"Survived\"]].groupby(['isAlone'], as_index=False).mean())\n# handle missing values im embarked and fare columns\n# fill in null values in Embarked column with mode\ntrain['Embarked'].fillna(train['Embarked'].mode()[0],inplace=True)\ntest['Embarked'].fillna(test['Embarked'].mode()[0],inplace=True)\n#create a new feature and fill in null value of Fare in test dataset\n# UnitFare is the Fare per person, calculating from Fare.\ntest['Fare'].fillna(test['Fare'].median(),inplace=True)\ntrain['UnitFare'] = train['Fare']\/train['Family']\ntest['UnitFare'] = test['Fare']\/test['Family']","624b89e1":"train.head(5)","8b9b9e56":"# the functions below gets the title from the Name column,does some more post processing,data cleanup\ndef get_title(name):\n\ttitle_search = re.search(' ([A-Za-z]+)\\.', name)\n\t# If the title exists, extract and return it.\n\tif title_search:\n\t\treturn title_search.group(1)\n\treturn \"\"\ndef convert_title(title):\n    if title in  ['Dr','Rev','Major','Col','Don','Dona','Capt','Sir','Jonkheer' ]:\n        title2='Mr'\n    elif title in ['Lady' ,'Mme' ,'Countess']:\n        title2='Mrs'\n    elif title in ['Mlle','Ms'] :\n        title2 = 'Miss'\n    else :\n        title2 = title\n    return title2\ndef correct_title(cols):\n    Age = cols[0]\n    title = cols[1]\n    Sex = cols[2]\n    if Age is not None and Age <=12  and Sex == 'male':\n        return 'Master'\n    elif Age is not None and Age <=12  and Sex == 'female':\n        return 'Miss'\n    else:\n        return title\n   ","5eab07a8":"import re\ntrain['title'] = train['Name'].apply(get_title)\ntest['title'] = test['Name'].apply(get_title)\ntrain['title'] = train['title'].apply(convert_title)\ntest['title'] = test['title'].apply(convert_title)\ntrain['title'] = train[['Age','title','Sex']].apply(correct_title,axis=1)\ntest['title'] = test[['Age','title','Sex']].apply(correct_title,axis=1)\n","71122bd7":"train['title'].value_counts()","ca85edde":"sns.countplot(x='Survived' ,hue='title' ,data = train)","f390be00":"# Handle missing values in Age column, I had many experiments, I tried to replace it by mean value by  title also but \n# decided to go with Pclass.\n#handle missing values in age column\ndef get_age(cols):\n    Age = cols[0]\n    title = cols[1]\n    if pd.isnull(Age) :\n        return int(train[train[\"Pclass\"] == title][\"Age\"].mean())\n    else:\n        return Age\ntrain['Age'] = train[['Age','Pclass']].apply(get_age,axis=1)\ntest['Age']  = test[['Age','Pclass']].apply(get_age,axis=1)","dc9fd60d":"#label encode few non numeric columns\n#Label encode columns\nle1 = LabelEncoder()\ntrain['gender'] = le1.fit_transform(train['Sex'])\ntest['gender'] = le1.transform(test['Sex'])\n#Handle missing values in Embarked columns and label encode it\nle2 = LabelEncoder()\ntrain['tcode'] = le2.fit_transform(train['title'])\ntest['tcode'] = le2.transform(test['title'])\nle3 = LabelEncoder()\ntrain['ecode'] = le3.fit_transform(train['Embarked'])\ntest['ecode'] = le3.transform(test['Embarked'])","9e1b4e2a":"# function to encode the UnitFare\ndef get_fcode(fare):\n    if fare <=8 :\n        fcode=1\n    elif fare >8 and fare <=26 :\n        fcode =2\n    else: \n        fcode =3\n    return fcode\ntrain['Fcode'] = train['UnitFare'].apply(get_fcode) \ntest['Fcode'] = test['UnitFare'].apply(get_fcode) ","95f2b1c4":"# lets now encode age column also.\ndef get_age_cat(x):\n    if x <= 12 :\n        cat = 1\n    elif x > 12 and x <= 18 :\n        cat = 2\n    elif  x > 18 and x <= 35  :\n        cat = 3\n    elif x > 35 and x <=60 :\n        cat = 4\n    else:\n        cat = 5\n    return cat","53533a78":"train['agecode'] = train['Age'].apply(get_age_cat)\ntest['agecode'] = test['Age'].apply(get_age_cat)","3ddb75c7":"sns.countplot(x='Survived' ,hue='agecode' ,data = train)","eeae647a":"train.columns","b8c3ba1d":"columns = ['gender','ecode','Pclass','agecode','Fcode','tcode','isAlone','Parch','SibSp']","78e149f0":"# define X,y and split the X ,y in train test datasets\nX = train[columns]\ny = train['Survived']\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2,random_state=10)","d4fc59b5":"X.head(5)","b144bd81":"print('----logistcis regression classifier results------')\nlr = LogisticRegression(random_state=0)\nlr.fit(train_X, train_y)\npredictions = lr.predict(val_X)\naccuracy = accuracy_score(val_y, predictions)\nprint(accuracy)\nfrom sklearn.metrics import classification_report\nprint(classification_report(val_y,predictions))\nprint('----xgboost claissifier results------------------')\nxr = XGBClassifier(random_state=0)\nxr.fit(train_X, train_y)\npredictions = xr.predict(val_X)\naccuracy = accuracy_score(val_y, predictions)\nprint(accuracy)\nfrom sklearn.metrics import classification_report\nprint(classification_report(val_y,predictions))\n\nprint('----random forest classification results-----')\nrf =RandomForestClassifier(random_state=0)\nrf.fit(train_X, train_y)\npredictions = rf.predict(val_X)\naccuracy = accuracy_score(val_y, predictions)\nprint(accuracy)\nfrom sklearn.metrics import classification_report\nprint(classification_report(val_y,predictions))\n","5cfe667b":"scores = cross_val_score(xr,X,y,cv=5)\nprint(\"xgboot :\",scores.mean(),scores.std())\nscores = cross_val_score(lr,X,y,cv=5)\nprint(\"lr :\",scores.mean(),scores.std())\nscores = cross_val_score(rf,X,y,cv=5)\nprint(\"rf :\",scores.mean(),scores.std())","c98b1220":"test = test[columns]\npredictions_test =xr.predict(test)","697fa1cc":"submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission_df = pd.DataFrame({'PassengerId' : submission['PassengerId'],\n                              'Survived':predictions_test})\nsubmission_df.head(10)","3c5fe47a":"submission_df.to_csv(\"submission.csv\",index=False)","77eec8dd":"Thank you for reading the notebook, please upvote if you like it. I have referenced few notebooks here and I have upvoted them.  I will continue to improve on this.","d41b39e4":"so the cross validation score is almost same for all 3 models tried above. and of course there is opporunity is improve the accuracy. this is first version.","c9d44753":"Pclass has some corelation with survived column.","fe892139":"it is pretty obvious that female gender and childs had higher survival rate.","f71b30c6":"persons with age between 18 and 35 and childerns seems to have high survival percentage."}}