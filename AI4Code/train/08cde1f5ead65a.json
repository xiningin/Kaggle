{"cell_type":{"b60fe196":"code","b9897a77":"code","7a167d7d":"code","d4931b3e":"code","e9c75b9e":"code","08313df9":"code","6927a48f":"code","ed032e89":"code","101b8a52":"code","a551b1e5":"code","c4f46d32":"code","6c86a695":"code","a9aba052":"code","164d68a6":"code","88cac3ca":"code","1afd7414":"code","7ebf352c":"code","e8e93eee":"code","1028f4a3":"code","d5f084b2":"code","dad04b70":"code","e142b6f7":"code","61430fd0":"code","a43ae666":"code","0e5b804a":"code","e39a3bae":"code","17f95d5b":"code","9dc9e8fc":"code","76a65988":"code","7a6e4f77":"code","5bc3c6a0":"code","9dc8be57":"code","92cc1a17":"code","72a84ab6":"code","23312515":"code","2cab417c":"code","ec25c4f8":"code","318c1dbe":"code","12faad8d":"code","f82c86a0":"code","54ff77d3":"code","9014c203":"code","d5196d18":"code","bab28cbb":"code","14d67c28":"code","2a66ff2b":"code","4751c4b2":"code","3ae7ebfe":"code","bbd4ac5a":"code","38e5d527":"code","126dd530":"code","0fd840ef":"code","2294f124":"code","d3e2d424":"code","fdc8419d":"code","3d722d62":"code","41353017":"code","ab767e1d":"code","a917523c":"code","0cbbc0f9":"code","88014957":"code","9313e629":"code","4c0148b6":"code","8f54c9c9":"code","a15cb0d3":"code","dba213b3":"code","b86655c8":"code","8fa8f992":"code","39ca1f0e":"code","985f8ccc":"code","78f94dfc":"markdown","7df94266":"markdown","e77021a1":"markdown","60890504":"markdown","939ab1b5":"markdown","3d6e921d":"markdown","2a81b63c":"markdown","3e54b726":"markdown","be787f3f":"markdown","aff3d1c6":"markdown","a94b3109":"markdown","c54bb801":"markdown","e201669c":"markdown","39e407e4":"markdown","415bc212":"markdown","662a1331":"markdown","cfe8654f":"markdown","d50f5d13":"markdown","32595e1a":"markdown","3488a3a7":"markdown","90ec41c0":"markdown","83bcd831":"markdown","be213c70":"markdown","913d5234":"markdown","2f559d92":"markdown","5336a636":"markdown","61a5ab28":"markdown","757730ab":"markdown","02545b73":"markdown","0b6a784c":"markdown","26c972e6":"markdown","fe9db81b":"markdown","46217d30":"markdown","3479c9ef":"markdown","df3df755":"markdown","639511fa":"markdown","bbf520e2":"markdown","856bdd49":"markdown","b343df44":"markdown","8d6202d2":"markdown","cfc2aba6":"markdown","34eec0db":"markdown","cc964814":"markdown"},"source":{"b60fe196":"import re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b9897a77":"def random_colours(number_of_colors):\n    '''\n    Simple function for random colours generation.\n    Input:\n        number_of_colors - integer value indicating the number of colours which are going to be generated.\n    Output:\n        Color in the following format: ['#E86DA4'] .\n    '''\n    colors = []\n    for i in range(number_of_colors):\n        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n    return colors","7a167d7d":"train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nss = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","d4931b3e":"print(train.shape)\nprint(test.shape)","e9c75b9e":"train.info()","08313df9":"train.dropna(inplace=True)","6927a48f":"test.info()","ed032e89":"train.head()","101b8a52":"train.describe()","a551b1e5":"temp = train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","c4f46d32":"plt.figure(figsize=(12,6))\nsns.countplot(x='sentiment',data=train)","6c86a695":"fig = go.Figure(go.Funnelarea(\n    text =temp.sentiment,\n    values = temp.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n    ))\nfig.show()","a9aba052":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","164d68a6":"results_jaccard=[]\n\nfor ind,row in train.iterrows():\n    sentence1 = row.text\n    sentence2 = row.selected_text\n\n    jaccard_score = jaccard(sentence1,sentence2)\n    results_jaccard.append([sentence1,sentence2,jaccard_score])","88cac3ca":"jaccard = pd.DataFrame(results_jaccard,columns=[\"text\",\"selected_text\",\"jaccard_score\"])\ntrain = train.merge(jaccard,how='outer')","1afd7414":"train['Num_words_ST'] = train['selected_text'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ntrain['Num_word_text'] = train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ntrain['difference_in_words'] = train['Num_word_text'] - train['Num_words_ST'] #Difference in Number of words text and Selected Text","7ebf352c":"train.head()","e8e93eee":"hist_data = [train['Num_words_ST'],train['Num_word_text']]\n\ngroup_labels = ['Selected_Text', 'Text']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels,show_curve=False)\nfig.update_layout(title_text='Distribution of Number Of words')\nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=700,\n    paper_bgcolor=\"LightSteelBlue\",\n)\nfig.show()","1028f4a3":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train['Num_words_ST'], shade=True, color=\"r\").set_title('Kernel Distribution of Number Of words')\np1=sns.kdeplot(train['Num_word_text'], shade=True, color=\"b\")","d5f084b2":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['difference_in_words'], shade=True, color=\"b\").set_title('Kernel Distribution of Difference in Number Of words')\np2=sns.kdeplot(train[train['sentiment']=='negative']['difference_in_words'], shade=True, color=\"r\")","dad04b70":"plt.figure(figsize=(12,6))\nsns.distplot(train[train['sentiment']=='neutral']['difference_in_words'],kde=False)","e142b6f7":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['jaccard_score'], shade=True, color=\"b\").set_title('KDE of Jaccard Scores across different Sentiments')\np2=sns.kdeplot(train[train['sentiment']=='negative']['jaccard_score'], shade=True, color=\"r\")\nplt.legend(labels=['positive','negative'])","61430fd0":"plt.figure(figsize=(12,6))\nsns.distplot(train[train['sentiment']=='neutral']['jaccard_score'],kde=False)","a43ae666":"k = train[train['Num_word_text']<=2]","0e5b804a":"k.groupby('sentiment').mean()['jaccard_score']","e39a3bae":"k[k['sentiment']=='positive']","17f95d5b":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","9dc9e8fc":"train['text'] = train['text'].apply(lambda x:clean_text(x))\ntrain['selected_text'] = train['selected_text'].apply(lambda x:clean_text(x))","76a65988":"train.head()","7a6e4f77":"train['temp_list'] = train['selected_text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","5bc3c6a0":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","9dc8be57":"def remove_stopword(x):\n    return [y for y in x if y not in stopwords.words('english')]\ntrain['temp_list'] = train['temp_list'].apply(lambda x:remove_stopword(x))","92cc1a17":"top = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","72a84ab6":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","23312515":"train['temp_list1'] = train['text'].apply(lambda x:str(x).split()) #List of words in every row for text\ntrain['temp_list1'] = train['temp_list1'].apply(lambda x:remove_stopword(x)) #Removing Stopwords","2cab417c":"top = Counter([item for sublist in train['temp_list1'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","ec25c4f8":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","318c1dbe":"Positive_sent = train[train['sentiment']=='positive']\nNegative_sent = train[train['sentiment']=='negative']\nNeutral_sent = train[train['sentiment']=='neutral']","12faad8d":"#MosT common positive words\ntop = Counter([item for sublist in Positive_sent['temp_list'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","f82c86a0":"fig = px.bar(temp_positive, x=\"count\", y=\"Common_words\", title='Most Commmon Positive Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","54ff77d3":"#MosT common negative words\ntop = Counter([item for sublist in Negative_sent['temp_list'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(20))\ntemp_negative = temp_negative.iloc[1:,:]\ntemp_negative.columns = ['Common_words','count']\ntemp_negative.style.background_gradient(cmap='Reds')","9014c203":"fig = px.treemap(temp_negative, path=['Common_words'], values='count',title='Tree Of Most Common Negative Words')\nfig.show()","d5196d18":"#MosT common Neutral words\ntop = Counter([item for sublist in Neutral_sent['temp_list'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(20))\ntemp_neutral = temp_neutral.loc[1:,:]\ntemp_neutral.columns = ['Common_words','count']\ntemp_neutral.style.background_gradient(cmap='Reds')","bab28cbb":"fig = px.bar(temp_neutral, x=\"count\", y=\"Common_words\", title='Most Commmon Neutral Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","14d67c28":"fig = px.treemap(temp_neutral, path=['Common_words'], values='count',title='Tree Of Most Common Neutral Words')\nfig.show()","2a66ff2b":"raw_text = [word for word_list in train['temp_list1'] for word in word_list]","4751c4b2":"def words_unique(sentiment,numwords,raw_words):\n    '''\n    Input:\n        segment - Segment category (ex. 'Neutral');\n        numwords - how many specific words do you want to see in the final result; \n        raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n\n    '''\n    allother = []\n    for item in train[train.sentiment != sentiment]['temp_list1']:\n        for word in item:\n            allother .append(word)\n    allother  = list(set(allother ))\n    \n    specificnonly = [x for x in raw_text if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in train[train.sentiment == sentiment]['temp_list1']:\n        for word in item:\n            mycounter[word] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n    \n    return Unique_words","3ae7ebfe":"Unique_Positive= words_unique('positive', 20, raw_text)\nprint(\"The top 20 unique words in Positive Tweets are:\")\nUnique_Positive.style.background_gradient(cmap='Greens')","bbd4ac5a":"fig = px.treemap(Unique_Positive, path=['words'], values='count',title='Tree Of Unique Positive Words')\nfig.show()","38e5d527":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Positive['count'], labels=Unique_Positive.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Positive Words')\nplt.show()","126dd530":"Unique_Negative= words_unique('negative', 10, raw_text)\nprint(\"The top 10 unique words in Negative Tweets are:\")\nUnique_Negative.style.background_gradient(cmap='Reds')","0fd840ef":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.rcParams['text.color'] = 'black'\nplt.pie(Unique_Negative['count'], labels=Unique_Negative.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Negative Words')\nplt.show()","2294f124":"Unique_Neutral= words_unique('neutral', 10, raw_text)\nprint(\"The top 10 unique words in Neutral Tweets are:\")\nUnique_Neutral.style.background_gradient(cmap='Oranges')","d3e2d424":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Neutral['count'], labels=Unique_Neutral.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Neutral Words')\nplt.show()","fdc8419d":"def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'u', \"im\"}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=400, \n                    height=200,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \nd = '\/kaggle\/input\/masks-for-wordclouds\/'","3d722d62":"pos_mask = np.array(Image.open(d+ 'twitter_mask.png'))\nplot_wordcloud(Neutral_sent.text,mask=pos_mask,color='white',max_font_size=100,title_size=30,title=\"WordCloud of Neutral Tweets\")","41353017":"plot_wordcloud(Positive_sent.text,mask=pos_mask,title=\"Word Cloud Of Positive tweets\",title_size=30)","ab767e1d":"plot_wordcloud(Negative_sent.text,mask=pos_mask,title=\"Word Cloud of Negative Tweets\",color='white',title_size=30)","a917523c":"df_train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\ndf_submission = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","0cbbc0f9":"df_train['Num_words_text'] = df_train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main Text in train set","88014957":"df_train = df_train[df_train['Num_words_text']>=3]","9313e629":"def save_model(output_dir, nlp, new_model_name):\n    ''' This Function Saves model to \n    given output directory'''\n    \n    output_dir = f'..\/working\/{output_dir}'\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)","4c0148b6":"# pass model = nlp if you want to train on top of existing model \n\ndef train(train_data, output_dir, n_iter=20, model=None):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    \"\"\n    if model is not None:\n        nlp = spacy.load(output_dir)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n    \n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    # add labels\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # sizes = compounding(1.0, 4.0, 1.001)\n        # batch up the examples using spaCy's minibatch\n        if model is None:\n            nlp.begin_training()\n        else:\n            nlp.resume_training()\n\n\n        for itn in tqdm(range(n_iter)):\n            random.shuffle(train_data)\n            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts,  # batch of texts\n                            annotations,  # batch of annotations\n                            drop=0.5,   # dropout - make it harder to memorise data\n                            losses=losses, \n                            )\n            print(\"Losses\", losses)\n    save_model(output_dir, nlp, 'st_ner')","8f54c9c9":"def get_model_out_path(sentiment):\n    '''\n    Returns Model output path\n    '''\n    model_out_path = None\n    if sentiment == 'positive':\n        model_out_path = 'models\/model_pos'\n    elif sentiment == 'negative':\n        model_out_path = 'models\/model_neg'\n    return model_out_path","a15cb0d3":"def get_training_data(sentiment):\n    '''\n    Returns Trainong data in the format needed to train spacy NER\n    '''\n    train_data = []\n    for index, row in df_train.iterrows():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return train_data","dba213b3":"sentiment = 'positive'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n# For DEmo Purposes I have taken 3 iterations you can train the model as you want\ntrain(train_data, model_path, n_iter=3, model=None)","b86655c8":"sentiment = 'negative'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n\ntrain(train_data, model_path, n_iter=3, model=None)","8fa8f992":"def predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text","39ca1f0e":"selected_texts = []\nMODELS_BASE_PATH = '..\/input\/tse-spacy-model\/models\/'\n\nif MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", MODELS_BASE_PATH)\n    model_pos = spacy.load(MODELS_BASE_PATH + 'model_pos')\n    model_neg = spacy.load(MODELS_BASE_PATH + 'model_neg')\n        \n    for index, row in df_test.iterrows():\n        text = row.text\n        output_str = \"\"\n        if row.sentiment == 'neutral' or len(text.split()) <= 2:\n            selected_texts.append(text)\n        elif row.sentiment == 'positive':\n            selected_texts.append(predict_entities(text, model_pos))\n        else:\n            selected_texts.append(predict_entities(text, model_neg))\n        \ndf_test['selected_text'] = selected_texts","985f8ccc":"df_submission['selected_text'] = df_test['selected_text']\ndf_submission.to_csv(\"submission.csv\", index=False)\ndisplay(df_submission.head(10))","78f94dfc":"We can see that there is similarity between text and selected text .Let's have closer look","7df94266":"### Cleaning the Corpus\nNow Before We Dive into extracting information out of words in text and selected text,let's first clean the data","e77021a1":"There are no null Values in the test set","60890504":"We have one null Value in the train , as the test field for value is NAN we will just remove it","939ab1b5":"I have added more words like im , u (that we say were there in the most common words,disturbing our analysis) as stopwords","3d6e921d":"So We have 27486 tweets in the train set and 3535 tweets in the test set","2a81b63c":"## Conclusion Of EDA\n\n* We can see from the jaccard score plot that there is peak for negative and positive plot around score of 1 .That means there is a cluster of tweets where there is a high similarity between text and selected texts ,if we can find those clusters then we can predict text for selected texts for those tweets irrespective of segment\n\nLet's see if we can find those clusters,one interesting idea would be to check tweets which have number of words lesss than 3 in text, because there the text might be completely used as text","3e54b726":"# Modelling\n\nThis is the first kaggle competition , I am participating in and this might be the case with lot of us.Due to the unique structure of the problem statement, it is hard for any first timer or a competitions noob to answer the question\"Which Model to Use\"?.My initial thoughts was this competition is not for me and I am done here,but then I remembered something, I was at the KaggleDays Meetup Delhi this year and I had this wonderful oppurtunity to meet Grandmaster Abhishek Thakur and during the Q&A session I asked him that kaggle competitions are so diverse ,unique ,require a lot of background knowledge and thus is scary to participate, to which he replied and I quote \"Scary Yes!But so is walking into a dark room,you will never learn if you won't participate\".\n\nSo here I am fighting my way through this competition and trying to learn different things and I urge everyone to do the same , I might not be so well established to give advices but I really wanted to share that story to motivate people.\n\nAfter going through the discussion forums,taking advices from experts and watching Abhishek Sir's tutorial last night ,this problem can be modelled as following:-\n* Named Entity Recognition\n* Q&A Problem\n* I also found a simple approach shared by Nick in his beautiful kernel where he has the concept of Gini Impurity to give weights to words present in tweets and then predicting using the weight of those words : https:\/\/www.kaggle.com\/nkoprowicz\/a-simple-solution-using-only-word-counts\/notebook .Do check it out.\n* Other Modelling Ideas :- https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/139803 --> Here is a very Nice Idea\n* Another useful Idea :- https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/139335\n\nResources :\n* For Modelling Problem as NER : https:\/\/www.kaggle.com\/rohitsingh9990\/ner-training-using-spacy-0-628-lb\n* For Modelling Problem AS Q&A : https:\/\/www.kaggle.com\/jonathanbesomi\/question-answering-starter-pack ---> This is a complete Guide and From scratch","be787f3f":"# Reading the Data","aff3d1c6":"SO we can see the Most common words in Selected text and Text are almost the same,which was obvious","a94b3109":"## What do we currently Know About our Data:\n\nBefore starting let's look at some things that we already know about the data and will help us in gaining more new insights:\n* We Know that selected_text is a subset of text\n* We know that selected_text contains only one segment of text,i.e,It does not jump between two sentences.For Eg:- If text is 'Spent the entire morning in a meeting w\/ a vendor, and my boss was not happy w\/ them. Lots of fun.  I had other plans for my morning' The selected text can be 'my boss was not happy w\/ them. Lots of fun' or 'Lots of fun' but cannot be 'Morning,vendor and my boss,\n* Thanks to this discussion:https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/138520 We know that neutral tweets have a jaccard similarity of 97 percent between text and selected_text\n* Also as discussed here https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/138272 ,there are rows where selected_text starts from between the words and thus selected_texts dont always make sense and since we do not know whether the output of test set contain these descrepancies or not ,we are not sure that preprocessing and removing punctuations would be a good idea or not","c54bb801":"**Below is a helper Function which generates random colors which can be used to give different colors to your plots.Feel free to use it**","e201669c":"Let's draw a Funnel-Chart for better visualization","39e407e4":"Thus its clear that most of the times , text is used as selected text.We can improve this by preprocessing the text which have word length less than 3.We will remember this information and use it in model building","415bc212":"**By Looking at the Unique Words of each sentiment,we now have much more clarity about the data,these unique words are very strong determiners of Sentiment of tweets**","662a1331":"# EDA","cfe8654f":"I was not able to plot kde plot for neutral tweets because most of the values for difference in number of words were zero. We can see it clearly now ,if we had used the feature in the starting we would have known that text and selected text are mostly the same for neutral tweets,thus its always important to keep the end goal in mind while performing EDA","d50f5d13":"I was not able to plot kde of jaccard_scores of neutral tweets for the same reason,thus I will plot a distribution plot","32595e1a":"OOPS!While we cleaned our dataset we didnt remove the stop words and hence we can see the most coomon word is 'to' . Let's try again after removing the stopwords","3488a3a7":"### Predicting with the trained Model","90ec41c0":"**Now It will be more interesting to see the differnce in number of words and jaccard_scores across different Sentiments**","83bcd831":"# Most Common words in Text\n\nLet's also look at the most common words in Text","be213c70":"# About this Competition\n\nYou are someone who is recently starting on NLP or has become a master ,irrespective of where you lie in the learning chain , I can bet you have worked on sentiment analysis and if not you will be going to, you just can't bypass it. Can You?. <b>Sentiment analysis<\/b> is for NLP <b>'what Happy Birthday to You'<\/b> is for Guitar players Right? You start here <br>\n<br>\nIn case you are not aware about sentiment analysis here is a very good article : https:\/\/towardsdatascience.com\/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17\n<br><br>\nRecently Kaggle Lauched a new competition admist the COVID-19 Scare , named Twitter Sentiment Extraction ,I know right its a twitter sentiment analysis competition,But kaggle never disappoints you,it could not have been this straightforward, afterall it has go on for two months.So what this competition asks for is not the sentiment scores but the part of the tweet (word or phrase) that reflects the sentiment., Interesting it isn't it? This competition is special,so if you want to level up your NLP skills , this competition is for you\n\n# Acknowledgements\n* https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes --> WORDCLOUDS FUNCTION\n* https:\/\/www.kaggle.com\/rohitsingh9990\/ner-training-using-spacy-0-628-lb --> For understanding how to train spacy NER on custom inputs\n\n\n# About this Notebook\n\nIn this kernel, I will briefly explain the structure of dataset.I will generate and analyze metafeatures. Then, I will visualize the dataset using Matplotlib, seaborn and Plotly to gain as much insight as I can . Also I will approach this problem as an NER problem to build a model\n<br><br>\nIn case you are just starting with NLP here is a guide to Approach almost any NLP Problem by Grandmaster @Abhishek Thakur\nhttps:\/\/www.slideshare.net\/abhishekkrthakur\/approaching-almost-any-nlp-problem\n\n\n<b> This kernel is a work in Progress,and I will keep on updating it as the competition progresses and I learn more and more things about the data<\/b>\n\n**<span style=\"color:Red\">If you find this kernel useful, Please Upvote it , it motivates me to write more Quality content**","913d5234":"Selected_text is a subset of text ","2f559d92":"* We can see words like get,go,dont,got,u,cant,lol,like are common in all three segments . That's interesting because words like dont and cant are more of negative nature and words like lol are more of positive nature.Does this mean our data is incorrectly labelled , we will have more insights on this after N-gram analysis\n* It will be interesting to see the word unique to different sentiments","5336a636":"Let's look at the distribution of Meta-Features","61a5ab28":"We can see some interesting trends here:\n* Positive and negative tweets have high kurtosis and thus values are concentrated in two regions narrow and high density \n* Neutral tweets have a low kurtosis value and their is bump in density near values of 1\n\nFor those who don't know :\n* Kurtosis is the measure of how peaked a distribution is and how much spread it is around that peak\n* Skewness measures how much a curve deviates from a normal distribution","757730ab":"* The number of words plot is really interesting ,the tweets having number of words greater than 25 are very less and thus the number of words distribution plot is right skewed","02545b73":"### Positive Tweets","0b6a784c":"# Importing Necesseties ","26c972e6":"**In the previous versions of this notebook,I used Number of words in selected text and main text ,Length of words in text and selected as main meta features,but in the context of this competition where we have to predict selected_text which is a subset of text, more useful features to generate would be** :-\n* Difference In Number Of words of Selected_text and Text\n* Jaccard Similarity Scores between text and Selected_text\n\nThus it will not be useful for us to generate features we used before as they are of no importance here\n\nFor what who don't know what Jaccard Similarity is : https:\/\/www.geeksforgeeks.org\/find-the-jaccard-index-and-jaccard-distance-between-the-two-given-sets\/","fe9db81b":"So the first two common word was I'm so I removed it and took data from second row","46217d30":"## Most Common words in our Target-Selected Text","3479c9ef":"## Let's Look at Unique Words in each Segment\n\nWe will look at unique words in each segment in the Following Order:\n* Positive\n* Negative\n* Neutral","df3df755":"# Most common words Sentiments Wise\n\nLet's look at the most common words in different sentiments","639511fa":"#### Training models for Positive and Negative tweets","bbf520e2":"## 1)Modelling the Problem as NER\n\nNamed Entity Recognition (NER) is a standard NLP problem which involves spotting named entities (people, places, organizations etc.) from a chunk of text, and classifying them into a predefined set of categories.\nFor understanding NER here is very good article : https:\/\/towardsdatascience.com\/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n\nWe will be using spacy for creating our own customised NER model or models (seperate for each Sentiment).The motivation for this approach is off course the kernel shared by Rohit Singh,so if you find his kernel useful please upvote it.\n\nWhat will be different with my solution:\n* I will use text as selected_text for all neutral tweets due to their high jaccard similarity\n* Also I will use text as selected_text for all tweets having number of words less than 3 in text as explained before\n* I will train two different models for Positive and Negtive tweets\n* I will not preprocess the data because the selected text contains raw text","856bdd49":"# End Notes\nKaggle always provide a lot of days for a competition which one can utilize to learn and grow.As Promised I have presented my first model,along with explanation,you can read spacy's documentation and Rohit singh's kernel as all the code comes from their.If you understand any part of code feel free to comment and ask,I will try to resolve it.\nAs This is my first competition I am also learning along the way ,I will be back with more original ideas and some great more models as I learn more and more about question\/answering , different other texhniques , various forms of BERT and Data itself\n\n** Thanks for the enormous love and appreciation , I'm Sorry that I have not updated the kernel with Q and A approach,I'm Still learning all the techniques required , will update soon!**\n<br><br>STAY TUNED!\n\n<span style=\"color:Red\"> I hope you Liked my kernel. An upvote is a gesture of appreciation and encouragement that fills me with energy to keep improving my efforts ,be kind to show one ;-)","b343df44":"#### WORDCLOUD OF NEUTRAL TWEETS\n\nWe Have already visualized our Most Common Negative words ,but Wordclouds Provide us much more clarity","8d6202d2":"**For Full Understanding of the how to train spacy NER with custom inputs, please read the spacy documentation along with the code presentation in this notebook : https:\/\/spacy.io\/usage\/training#ner Follow along from Updating Spacy NER**","cfc2aba6":"## It's Time For WordClouds\n\nWe will be building wordclouds in the following order:\n\n* WordCloud of Neutral Tweets\n* WordCloud of Positive Tweets\n* WordCloud of Negative Tweets\n","34eec0db":"Lets look at the distribution of tweets in the train set","cc964814":"## Generating Meta-Features"}}