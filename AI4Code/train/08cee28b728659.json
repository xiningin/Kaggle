{"cell_type":{"33a39ebe":"code","5041fd67":"code","67bfcdff":"code","35bd5920":"code","b8cc6c46":"code","f9e7aecb":"code","9b9a7fad":"code","8bfad44f":"code","e0bed9fb":"code","078b28e4":"code","5006c2b7":"code","c72d2de9":"code","b6b3bf06":"code","44da6861":"markdown","77a599ea":"markdown","7782cc97":"markdown","133c3afb":"markdown"},"source":{"33a39ebe":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5041fd67":"!pip install 'tensorflow==1.15.0'\n!pip install 'stable-baselines[mpi]==2.10.0'","67bfcdff":"import json\nimport cudf\nimport random\nimport datetime\nimport janestreet\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport gym\nfrom gym import spaces\nfrom stable_baselines import PPO2\nfrom stable_baselines.common.policies import MlpPolicy, MlpLnLstmPolicy\nfrom stable_baselines.common.vec_env import DummyVecEnv","35bd5920":"js_env = janestreet.make_env() # initialize the environment\niter_test = js_env.iter_test() # an iterator which loops over the test set","b8cc6c46":"%%time\ntrain_cudf  = cudf.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\ntrain = train_cudf.to_pandas()\ndel train_cudf\n# For non-gpu: train = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv', nrows=(round(2390491 * 0.3)))\nfeatures = pd.read_csv('..\/input\/jane-street-market-prediction\/features.csv')\ntest = pd.read_csv('..\/input\/jane-street-market-prediction\/example_test.csv')\nsample_prediction_df = pd.read_csv('..\/input\/jane-street-market-prediction\/example_sample_submission.csv')\nprint (\"Data is loaded!\")","f9e7aecb":"train.head()","9b9a7fad":"timestamp = datetime.datetime.now()\nMODEL_ID = f\"jane_street_ppo_{timestamp.strftime('%s')}\"","8bfad44f":"def fillNaN(df):\n    f_mean = df.mean()\n    df = df.fillna(f_mean)\n    return df","e0bed9fb":"train = fillNaN(train)\ntest = fillNaN(test)","078b28e4":"class JaneStreetEnv(gym.Env):\n\n    def __init__(self, df):\n        super(JaneStreetEnv, self).__init__()\n        self.df = df\n        self.n_samples = df.shape[0]\n        self.weight = df['weight'].values\n        self.resp = df['resp'].values\n        self.features = [col for col in list(self.df.columns) if 'feature' in col]\n        self.states = df[self.features].values\n        self.idx = 0    \n        \n        # Possible actions = Sell | Hold\n        self.action_space = spaces.Discrete(2)\n\n        # Prices contains the OHCL values for the last five prices\n        self.observation_space = spaces.Box(low=-8.215050, high=5.872849e+01, shape=(df[self.features].shape[1], 6))\n\n    def _next_observation(self):\n        obs = np.array([self.df.loc[self.idx: self.idx + 5, feature].values for feature in self.features])\n        return obs\n\n\n    def step(self, action):\n#         delay_modifier = (self.current_step \/ MAX_STEPS)\n#         reward = self.balance * delay_modifier\n        \n        obs = self._next_observation()\n        reward = self.weight[self.idx] * self.resp[self.idx] * action\n        self.idx += 1\n        if self.idx >= self.n_samples - 5:\n            done = True\n            self.idx = 0\n        else:\n            done = False          \n        return obs, reward, done, {}\n\n    def reset(self):\n        self.idx = 0\n        return self._next_observation()\n\n    def render(self):\n        print(f'Step: {self.idx}')\n        \nenv = DummyVecEnv([lambda: JaneStreetEnv(train)])","5006c2b7":"def learn(timesteps=20000):\n    model = PPO2(MlpLnLstmPolicy, env, verbose=1, nminibatches=1)\n    model.learn(total_timesteps=timesteps)\n    model.save(MODEL_ID)\n    print(f\"Saved model: {MODEL_ID}\")\nlearn()","c72d2de9":"def evaluate(model_name):\n    model = PPO2.load(model_name)\n    obs = env.reset()\n    submission_df = pd.DataFrame()\n    for (test_df, sample_prediction_df) in tqdm(iter_test):\n        action, _states = model.predict(obs) # _states are only useful when using LSTM policies\n        obs, rewards, done, info = env.step(action)        \n        sample_prediction_df.action = action #make your 0\/1 prediction here\n        submission_df = submission_df.append(sample_prediction_df)\n        js_env.predict(sample_prediction_df)\n        # env.render()\n    return submission_df\n\nsubmission_df = evaluate(MODEL_ID)","b6b3bf06":"submission_df = submission_df.rename_axis(None, axis=1).rename_axis('ts_id', axis=0)\nsubmission_df.to_csv('submission.csv')","44da6861":"## Load data","77a599ea":"## Training and evaluation","7782cc97":"## Gym env","133c3afb":"## Utilities"}}