{"cell_type":{"1c2382d8":"code","278dc1a7":"code","83dfe5f8":"code","cef50771":"code","61736368":"code","761c669e":"code","578b52bc":"code","0c8e19b3":"code","9552af17":"code","d7eeded7":"code","7ee5ca70":"code","343f16e1":"code","a31ad904":"code","bc9c02bb":"code","47cd0321":"code","8d15b7af":"code","4e0220b1":"code","f23c32fa":"code","8c02e6c0":"code","28d12801":"code","2b3da6ec":"code","a56f70a7":"code","576d343f":"code","4690c77a":"code","691e3adf":"code","50e87ecb":"code","56888245":"code","2171d171":"code","0da515b7":"code","6059c8f2":"code","5f92a59b":"markdown","48acedc4":"markdown","be17990a":"markdown","7ec26e53":"markdown","972bc4eb":"markdown","99c9c330":"markdown","9a11f930":"markdown","90619dd7":"markdown","85a8505c":"markdown","562b414c":"markdown","115d173a":"markdown","4fbc5d5a":"markdown","18f15e2e":"markdown","61dd67f1":"markdown","9654a507":"markdown"},"source":{"1c2382d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib\nimport matplotlib.pyplot as plt # visualization and graph\nimport seaborn as sns # visualization and graph\n\nfrom sklearn.model_selection import train_test_split # to create train and test data set \n\nfrom sklearn import metrics # to check accuracy score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","278dc1a7":"data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ndata.head()","83dfe5f8":"data.shape","cef50771":"data.isnull().sum().sum()","61736368":"data['label'].value_counts()","761c669e":"plt.figure(figsize=(10,8))\nsns.countplot(data['label'])","578b52bc":"X = data.drop('label', axis =1).values\ny = data['label'].values","0c8e19b3":"X = X\/255","9552af17":"n = 100\nplt.imshow(X[n].reshape(28,28), cmap=matplotlib.cm.binary)\nplt.title(y[n])\nplt.show()","d7eeded7":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","7ee5ca70":"X_train.shape, X_test.shape","343f16e1":"from sklearn.svm import SVC \n\nsvm = SVC()\nsvm.fit(X_train, y_train)\n\npred = svm.predict(X_test)\n\nmetrics.accuracy_score(y_test, pred)","a31ad904":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\n\nrf.fit(X_train, y_train)\n\npred_rf = rf.predict(X_test)\n\nmetrics.accuracy_score(y_test, pred_rf)","bc9c02bb":"from sklearn.neighbors import KNeighborsClassifier\n\nacc = []\n\nfor i in range(1, 21, 1):\n    knn = KNeighborsClassifier(n_neighbors=i, n_jobs=-1)\n    knn.fit(X_train, y_train)\n    \n    pred_knn = knn.predict(X_test)\n    acc.append(metrics.accuracy_score(y_test, pred_knn))","47cd0321":"plt.plot(range(1,21,1), acc, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)","8d15b7af":"knn = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\nknn.fit(X_train, y_train)\n\npred_knn = knn.predict(X_test)\nmetrics.accuracy_score(y_test, pred_knn)","4e0220b1":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier(n_jobs=-1)\n\nxgb.fit(X_train, y_train)\n\npred_xgb = xgb.predict(X_test)\nmetrics.accuracy_score(y_test, pred_xgb)","f23c32fa":"from sklearn.decomposition import PCA\n\npca = PCA()\n\npca.fit(X_train)","8c02e6c0":"variance = pd.Series(pca.explained_variance_ratio_).sort_values(ascending=False)\nplt.plot(np.cumsum(variance))\nplt.plot(variance)\nplt.legend(['cum_variance', 'variance'])","28d12801":"pca_95 = PCA(n_components=152)\n\nX_train_pca = pca_95.fit_transform(X_train)\nX_test_pca = pca_95.transform(X_test)","2b3da6ec":"plt.figure(figsize=(15,8))\nscatter = plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='Spectral')\nhandels, labels = scatter.legend_elements()\nplt.legend(handels, labels)\nplt.title('Principal Component Analysis')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.show()","a56f70a7":"svm.fit(X_train_pca, y_train)\n\npred = svm.predict(X_test_pca)\n\nmetrics.accuracy_score(y_test, pred)","576d343f":"acc = []\n\nfor i in range(1, 21, 1):\n    knn = KNeighborsClassifier(n_neighbors=i, n_jobs=-1)\n    knn.fit(X_train_pca, y_train)\n    \n    pred_knn = knn.predict(X_test_pca)\n    acc.append(metrics.accuracy_score(y_test, pred_knn))","4690c77a":"plt.plot(range(1,21,1), acc, color='blue', linestyle='dashed', marker='o', markerfacecolor='red', markersize=10)","691e3adf":"knn = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\nknn.fit(X_train_pca, y_train)\n\npred_knn = knn.predict(X_test_pca)\nmetrics.accuracy_score(y_test, pred_knn)","50e87ecb":"rf.fit(X_train_pca, y_train)\n\npred_rf = rf.predict(X_test_pca)\n\nmetrics.accuracy_score(y_test, pred_rf)","56888245":"xgb.fit(X_train, y_train)\n\npred_xgb = xgb.predict(X_test)\n\nmetrics.accuracy_score(y_test, pred_xgb)","2171d171":"test_data = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\ntest_data = test_data\/255\n\ntest_data_pca = pca_95.transform(test_data)\n\npred_test_data = svm.predict(test_data_pca)\npred_test_data.shape","0da515b7":"my_submission = pd.DataFrame({'ImageId':range(1,28001), 'Label':pred_test_data})\nmy_submission","6059c8f2":"my_submission.to_csv('submission.csv', index=False)","5f92a59b":"## PCA","48acedc4":"## Lets view one of the image from dataset","be17990a":"**We can see that we have got max accuracy for neighbour 1 and 3**","7ec26e53":"**Out of 4 algo's both SVM AND XgBoost has perfomed better as compared to KNN and Random Forest**\n\n\nNow lets try reducing our features using PCA and see if our accuracy improves or not","972bc4eb":"**As we can see that most of the variance is explained by 150 - 300 PC's so we will take those PC's which can explain about 95% of the variance**","99c9c330":"## What else I could have done in this model:\n1. Hyperparameter tuning: As the dataset are quite large so for hyperparameter tuning computation time will be very high as sklearn libraries arent supported by kaggle gpu\n2. Used roc-auc curve","9a11f930":"## Train and Test dataset","90619dd7":"**As we can see using PCA accuracy of SVM and KNN has improved little bit, theres no change in the accuracy of XgBoost and in case of Random Forest accuracy has dropped**","85a8505c":"## Lets train a basic SVC, Random Forest, KNN, XgBoost model and check the accuracy","562b414c":"## As it is a classification problem lets check whether we have balanced or imbalanced classes ","115d173a":"The dataset is balanced as we have approximately same count for each class","4fbc5d5a":"**For KNN I will check for different neighbours ranging from 1 to 20 and will select neighbour with max accuracy**","18f15e2e":"## Lets check if there are any null values","61dd67f1":"**Lets first plot first 2 PC's**","9654a507":"## Lets scale down our data\n**By dividing it by max value of pixel i.e. 255** "}}