{"cell_type":{"dffb4184":"code","494ccbf0":"code","60e4edaa":"code","d149fd67":"code","ffdc1710":"code","c423d655":"code","cd791255":"code","7511eb9e":"code","902ed1ca":"code","f3b926e2":"code","c9d540c1":"code","fa61d3e0":"code","4f6db610":"code","7959faa5":"code","fdae27c9":"code","298992ae":"code","292e83d4":"code","0fa1cc11":"code","12712c9f":"code","86bc3fed":"code","267d2a2d":"code","105f836c":"code","ab57eeaf":"code","1b91a263":"code","b8a97c9c":"markdown","7db91d44":"markdown","1099bbae":"markdown","9a7ffe23":"markdown","42c87589":"markdown","0419fb1d":"markdown","0acbcb3c":"markdown","8b19c1eb":"markdown","eb1ee9cf":"markdown","4a48abd1":"markdown"},"source":{"dffb4184":"# for basic operations\nimport numpy as np\nimport pandas as pd\n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport squarify\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\n# for defining path\nimport os\nprint(os.listdir('..\/input\/'))\n\n# for market basket analysis\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules","494ccbf0":"# reading the dataset\n\ndata = pd.read_csv('..\/input\/veggie-data\/Market_Basket_Optimisation.csv', header = None)\n","60e4edaa":"data.head()","d149fd67":"data.tail()","ffdc1710":"data.describe()","c423d655":"data.isnull().sum()","cd791255":"data.dtypes","7511eb9e":"plt.rcParams['figure.figsize'] = (18, 7)\ncolor = plt.cm.magma(np.linspace(0, 1, 40))\ndata[0].value_counts().head(40).plot.bar(color = color)\nplt.title('frequency of most popular items', fontsize = 20)\nplt.xticks(rotation = 90 )\nplt.grid()\nplt.show()","902ed1ca":"y = data[0].value_counts().head(50).to_frame()\ny.index\n\n# plotting a tree map\n\nplt.rcParams['figure.figsize'] = (20, 20)\ncolor = plt.cm.RdYlGn(np.linspace(0, 1, 50))\nsquarify.plot(sizes = y.values, label = y.index, alpha=.8, color = color)\nplt.title('Tree Map for Popular Items')\nplt.axis('off')\nplt.show()","f3b926e2":"data['food'] = 'Food'\nfood = data.truncate(before = -1, after = 15)\n\n\nimport networkx as nx\n\nfood = nx.from_pandas_edgelist(food, source = 'food', target = 0, edge_attr = True)\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (20, 20)\npos = nx.spring_layout(food)\ncolor = plt.cm.autumn(np.linspace(0, 15, 1))\nnx.draw_networkx_nodes(food, pos, node_size = 15000, node_color = color)\nnx.draw_networkx_edges(food, pos, width = 3, alpha = 0.6, edge_color = 'black')\nnx.draw_networkx_labels(food, pos, font_size = 20, font_family = 'sans-serif')\nplt.axis('off')\nplt.grid()\nplt.title('Top 15 First Choices', fontsize = 40)\nplt.show()","c9d540c1":"data['secondchoice'] = 'Second Choice'\nsecondchoice = data.truncate(before = -1, after = 15)\nsecondchoice = nx.from_pandas_edgelist(secondchoice, source = 'food', target = 1, edge_attr = True)\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (20, 20)\npos = nx.spring_layout(secondchoice)\ncolor = plt.cm.summer(np.linspace(0, 15, 1))\nnx.draw_networkx_nodes(secondchoice, pos, node_size = 15000, node_color = color)\nnx.draw_networkx_edges(secondchoice, pos, width = 3, alpha = 0.6, edge_color = 'Yellow')\nnx.draw_networkx_labels(secondchoice, pos, font_size = 20, font_family = 'sans-serif')\nplt.axis('off')\nplt.grid()\nplt.title('Top 15 Second Choices', fontsize = 40)\nplt.show()","fa61d3e0":"data['thirdchoice'] = 'Third Choice'\nsecondchoice = data.truncate(before = -1, after = 15)\nsecondchoice = nx.from_pandas_edgelist(secondchoice, source = 'food', target = 2, edge_attr = True)\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.rcParams['figure.figsize'] = (20, 20)\npos = nx.spring_layout(secondchoice)\ncolor = plt.cm.Wistia(np.linspace(0, 15, 1))\nnx.draw_networkx_nodes(secondchoice, pos, node_size = 15000, node_color = color)\nnx.draw_networkx_edges(secondchoice, pos, width = 3, alpha = 0.6, edge_color = 'Yellow')\nnx.draw_networkx_labels(secondchoice, pos, font_size = 20, font_family = 'white')\nplt.axis('off')\nplt.grid()\nplt.title('Top 15 Third Choices', fontsize = 40)\nplt.show()","4f6db610":"\n\n# making each customers shopping items an identical list\ntrans = []\nfor i in range(0, 7501):\n    trans.append([str(data.values[i,j]) for j in range(0, 20)])\n\n# conveting it into an numpy array\ntrans = np.array(trans)\n\n# checking the shape of the array\nprint(trans.shape)","7959faa5":"import pandas as pd\nfrom mlxtend.preprocessing import TransactionEncoder\n\nte = TransactionEncoder()\ndata = te.fit_transform(trans)\ndata = pd.DataFrame(data, columns = te.columns_)\n\n# getting the shape of the data\ndata.shape","fdae27c9":"import warnings\nwarnings.filterwarnings('ignore')\n\n# getting correlations for 121 items would be messy \n# so let's reduce the items from 121 to 50\n\ndata = data.loc[:, ['mineral water', 'burgers', 'turkey', 'chocolate', 'frozen vegetables', 'spaghetti',\n                    'shrimp', 'grated cheese', 'eggs', 'cookies', 'french fries', 'herb & pepper', 'ground beef',\n                    'tomatoes', 'milk', 'escalope', 'fresh tuna', 'red wine', 'ham', 'cake', 'green tea',\n                    'whole wheat pasta', 'pancakes', 'soup', 'muffins', 'energy bar', 'olive oil', 'champagne', \n                    'avocado', 'pepper', 'butter', 'parmesan cheese', 'whole wheat rice', 'low fat yogurt', \n                    'chicken', 'vegetables mix', 'pickles', 'meatballs', 'frozen smoothie', 'yogurt cake']]\n\n# checking the shape\ndata.shape","298992ae":"from mlxtend.frequent_patterns import apriori\n\n#Now, let us return the items and itemsets with at least 5% support:\napriori(data, min_support = 0.01, use_colnames = True)","292e83d4":"frequent_itemsets = apriori(data, min_support = 0.05, use_colnames=True)\nfrequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\nfrequent_itemsets","0fa1cc11":"# getting th item sets with length = 2 and support more han 10%\n\nfrequent_itemsets[ (frequent_itemsets['length'] == 2) &\n                   (frequent_itemsets['support'] >= 0.01) ]","12712c9f":"# getting th item sets with length = 2 and support more han 10%\n\nfrequent_itemsets[ (frequent_itemsets['length'] == 1) &\n                   (frequent_itemsets['support'] >= 0.01) ]","86bc3fed":"frequent_itemsets[ frequent_itemsets['itemsets'] == {'eggs', 'mineral water'} ]","267d2a2d":"frequent_itemsets[ frequent_itemsets['itemsets'] == {'mineral water'} ]","105f836c":"frequent_itemsets[ frequent_itemsets['itemsets'] == {'chicken'} ]","ab57eeaf":"frequent_itemsets[ frequent_itemsets['itemsets'] == {'frozen vegetables'} ]","1b91a263":"frequent_itemsets[ frequent_itemsets['itemsets'] == {'chocolate'} ]","b8a97c9c":"## Association Minning ","7db91d44":"## How does the Apriori Algorithm in Data Mining work?\n\n* Consider a supermarket scenario where the itemset is I = {Onion, Burger, Potato, Milk, Beer}. The database consists of six transactions where 1 represents the presence of the item and 0 the absence.\n\n![](https:\/\/www.digitalvidya.com\/wp-content\/uploads\/2018\/11\/01.jpg)\n\n#### Step -01 \nCreate a frequency table of all the items that occur in all the transactions. Now, prune the frequency table to include only those items having a threshold support level over 50%. We arrive at this frequency table.\n\n* All subsets of a frequent itemset should be frequent.\n* In the same way, the subsets of an infrequent itemset should be infrequent.\n* Set a threshold support level. In our case, we shall fix it at 50%\n\n![](https:\/\/www.digitalvidya.com\/wp-content\/uploads\/2018\/11\/02.jpg)\n\n#### Step -02 \n\n* Make pairs of items such as OP, OB, OM, PB, PM, BM. This frequency table is what you arrive at.\n![](https:\/\/www.digitalvidya.com\/wp-content\/uploads\/2018\/11\/03.jpg)\n\n#### Step -03\nApply the same threshold support of 50% and consider the items that exceed 50% (in this case 3 and above).\nThus, you are left with OP, OB, PB, and PM\n#### Step -04\nStep 4\nLook for a set of three items that the customers buy together. Thus we get this combination.\n* OP and OB gives OPB\n* PB and PM gives PBM\n#### Step -05\n![](https:\/\/www.digitalvidya.com\/wp-content\/uploads\/2018\/11\/04.jpg)\n*  Determine the frequency of these two itemsets. You get this frequency table.If you apply the threshold assumption, you can deduce    that the set of three items frequently purchased by the customers is OPB.\n* We have taken a simple example to explain the apriori algorithm in data mining. In reality, you have hundreds and thousands of such combinations.\n#### Apriori Algorithm \u2013 Pros\n* Easy to understand and implement\n* Can use on large itemsets\n#####  Apriori Algorithm \u2013 Cons\n* At times, you need a large number of candidate rules. It can become computationally expensive.\n* It is also an expensive method to calculate support because the calculation has to go through the entire database.\n##### Apriori Algorithm \u2013 Limitations\n* The process can sometimes be very tedious.\n* How to Improve the Efficiency of the Apriori Algorithm?\n\n#### Use the following methods to improve the efficiency of the apriori algorithm.\n\n* Transaction Reduction \u2013 A transaction not containing any frequent k-itemset becomes useless in subsequent scans.\n* Hash-based Itemset Counting \u2013 Exclude the k-itemset whose corresponding hashing bucket count is less than the threshold is an infrequent itemset.\n* There are other methods as well such as partitioning, sampling, and dynamic itemset counting","1099bbae":"## Frequent Itemsets via Apriori Algorithm\nApriori function help  to extract frequent itemsets for Association rule mining. We have, a dataset of a mall with 7500 transactions of different customers buying different items from the store. We have to find correlations between the different items in the store. so that we can know if a customer is buying apple, banana and mango. what is the next item, The customer would be interested in buying from the store.**","9a7ffe23":"## Data Preprocessing ","42c87589":"The advantage of working with pandas DataFrames is that we can use its convenient features to filter the results. For instance, let's assume we are only interested in itemsets of length 2 that have a support of at least 80 percent. First, we create the frequent itemsets via apriori and add a new column that stores the length of each itemse\n\n\n## Selecting and Filtering the Results\n\n","0419fb1d":"### How does Apriori Algorithm Work ?\nA key concept in Apriori algorithm is the anti-monotonicity of the support measure. It assumes that\n\nAll subsets of a frequent itemset must be frequent\nSimilarly, for any infrequent itemset, all its supersets must be infrequent too\n\nStep 1: Create a frequency table of all the items that occur in all the transactions.\n\nStep 2: We know that only those elements are significant for which the support is greater than or equal to the threshold support.\n\nStep 3: The next step is to make all the possible pairs of the significant items keeping in mind that the order doesn\u2019t matter, i.e., AB is same as BA.\n\nStep 4: We will now count the occurrences of each pair in all the transactions.\n\nStep 5: Again only those itemsets are significant which cross the support threshold\n\nStep 6: Now let\u2019s say we would like to look for a set of three items that are purchased together. We will use the itemsets found in step 5 and create a set of 3 items.","0acbcb3c":"#### Make A View On my Kernal ,and Give A review on Presentation \n\n![](https:\/\/1eu.funnyjunk.com\/gifs\/My_0ca40e_5479296.gif)","8b19c1eb":"### About Aprior Algorithm \n\n* Inventation \n> This algorithm, introduced by R Agrawal and R Srikant in 1994 has great significance in data mining. We shall see the importance of the apriori algorithm in data mining in this article\n* Brief Introduction with Examples \nThis small story will help you understand the concept better. You must have noticed that the local vegetable seller always bundles onions and potatoes together. He even offers a discount to people who buy these bundles.\n\n**Why does he do so?** \n\nHe realises that people who buy potatoes also buy onions. Therefore, by bunching them together, he makes it easy for the customers. At the same time, he also increases his sales performance. It also allows him to offer discounts.Similarly, you go to a supermarket, and you will find bread, butter, and jam bundled together. It is evident that the idea is to make it comfortable for the customer to buy these three food items in the same place.\n\n\nThe Walmart beer diaper parable is another example of this phenomenon. People who buy diapers tend to buy beer as well. The logic is that raising kids is a stressful job. People take beer to relieve stress. Walmart saw a spurt in the sale of both diapers and beer.\n\n* What is the Apriori Algorithm? \n\n**Apriori algorithm, a classic algorithm, is useful in mining frequent itemsets and relevant association rules. Usually, you operate this algorithm on a database containing a large number of transactions. One such example is the items customers buy at a supermarket.**\n\nIt helps the customers buy their items with ease, and enhances the sales performance of the departmental store.\n\nThis algorithm has utility in the field of healthcare as it can help in detecting adverse drug reactions (ADR) by producing association rules to indicate the combination of medications and patient characteristics that could lead to ADRs\n\n* Apriori Algorithm \u2013 An Odd Name\nIt has got this odd name because it uses \u2018prior\u2019 knowledge of frequent itemset properties. The credit for introducing this algorithm goes to Rakesh Agrawal and Ramakrishnan Srikant in 1994. We shall now explore the apriori algorithm implementation in detail.\n\n## Detailed Theory \n\n**Three significant components comprise the apriori algorithm. They are as follows.**\n\n* Support\n* Confidence\n* Lift\n\n**This example will make things easy to understand. Let us suppose you have 2000 customer transactions in a supermarket. You have to find the Support, Confidence, and Lift for two items, say bread and jam. It is because people frequently bundle these two items together.**\nOut of the 2000 transactions, 200 contain jam whereas 300 contain bread. These 300 transactions include a 100 that includes bread as well as jam. Using this data, we shall find out the support, confidence, and lift.\n### SUPPORT \nSupport is the default popularity of any item. You calculate the Support as a quotient of the division of the number of transactions containing that item by the total number of transactions. Hence, in our example,\n\n* Support (Jam) = (Transactions involving jam) \/ (Total Transactions)\n\n                       *  = 200\/2000 = 10%\n### Confidence \nIt is the likelihood that customer bought both bread and jam. Dividing the number of transactions that include both bread and jam by the total number of transactions will give the Confidence figure.\n\n* Confidence = (Transactions involving both bread and jam) \/ (Total Transactions involving jam)\n\n                    = 100\/200 = 50%                      \n                       \n  * It implies that 50% of customers who bought jam bought bread as well\n  \n###  Lift\n**According to our example, Lift is the increase in the ratio of the sale of bread when you sell jam. The mathematical formula of Lift is as follows.**\n\n* Lift = (Confidence (Jam\u0362\u0362 \u2013 Bread)) \/ (Support (Jam))\n\n      = 50 \/ 10 = 5\nIt says that the likelihood of a customer buying both jam and bread together is 5 times more than the chance of purchasing jam alone. If the Lift value is less than 1, it entails that the customers are unlikely to buy both the items together. Greater the value, the better is the combination.\n  ![](https:\/\/image.slidesharecdn.com\/apriorialgorithm-140619035225-phpapp02\/95\/apriori-algorithm-8-638.jpg?cb=1403150201)","eb1ee9cf":"### Using Transaction One hot Encoder ","4a48abd1":"![](https:\/\/myleapmagazine.ca\/wp-content\/uploads\/2018\/04\/Leap-Veggies-Thinkstock.gif)"}}