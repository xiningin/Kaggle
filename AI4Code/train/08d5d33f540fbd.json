{"cell_type":{"2c51f727":"code","69c9416e":"code","b8081d4b":"code","63582989":"code","9a8dccba":"code","4481954e":"code","271785c5":"code","1441c41c":"code","27613281":"code","67e1220f":"code","899f940f":"code","5b07d42b":"code","e73acbbc":"code","c0974813":"code","43c67233":"code","62b7e592":"code","f570146d":"code","305c645e":"code","94869c46":"code","61e5fc76":"code","62fa9699":"code","5bad58e2":"code","fa2c0598":"code","b1814720":"code","f8770d12":"code","cd2123c6":"code","8bbd1f8a":"code","a24f86b9":"code","54597cd0":"code","83de835e":"code","4fd18faa":"code","f8df687c":"code","8ffdb977":"code","78fbfad3":"code","558cebb7":"code","490140c6":"code","a4a518c9":"code","61feb71e":"code","37999fe8":"code","3e061df4":"code","cb53a90b":"code","47869b72":"code","cd526fe3":"code","afa39b05":"code","de3958a1":"code","f223ff6f":"code","aae3b98c":"code","b8c01295":"code","53e67459":"code","0db78b18":"code","2bf519da":"code","d5908290":"markdown","7e78d355":"markdown","75e2ed61":"markdown","addbbe93":"markdown","a2449f09":"markdown","d21889a7":"markdown","4a765639":"markdown","57c3415c":"markdown","f8d1cfb4":"markdown","b7cbc76e":"markdown","4347ded8":"markdown","40e655e2":"markdown","698c8439":"markdown","112007fb":"markdown","3ad44f70":"markdown","43ac0a0c":"markdown","4e673cf8":"markdown","6c8c5644":"markdown","ede3bfed":"markdown","440d019f":"markdown","7b2d7a37":"markdown","62e33f4b":"markdown","7c09170b":"markdown","38318014":"markdown","88f05c20":"markdown","0fc3f999":"markdown","778d93b1":"markdown","de9b244d":"markdown","d91747d7":"markdown","ba492808":"markdown","a239196f":"markdown","b3989148":"markdown","b5d434c6":"markdown","a59f25dc":"markdown","d39fcfef":"markdown","96e3d72e":"markdown","df7b791b":"markdown","ac9434b8":"markdown","61809e00":"markdown","512644ee":"markdown","ec1b92e7":"markdown"},"source":{"2c51f727":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","69c9416e":"data=pd.read_csv('..\/input\/league-of-legends-diamond-ranked-games-10-min\/high_diamond_ranked_10min.csv')\ndata.head()","b8081d4b":"# Checking the shape of the data\ndata.shape","63582989":"# Checking null values\ndata.isnull().sum().sum()","9a8dccba":"# checking data types of the columns\ndata.info()","4481954e":"#checking for quasi constants\ndata.nunique()","271785c5":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFE\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB","1441c41c":"X=data.drop(['blueWins', 'gameId'], axis=1)\ny=data['blueWins']","27613281":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)","67e1220f":"sel_rf=SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=1))\n\nsel_rf.fit(X_train, y_train)","899f940f":"sel_rf.get_support()","5b07d42b":"print(\"Total number of features in the database: \", len(X_train.columns))\nprint(\"Total number of features after removing according to RF feature importances: \", sel_rf.get_support().sum())\nprint(\"Total features removed: \", int(len(X_train.columns)-sel_rf.get_support().sum()))","e73acbbc":"X_train_rfc=sel_rf.transform(X_train)\nX_test_rfc=sel_rf.transform(X_test)","c0974813":"# Let's check the shape of the data now to confirm that they have 16 features now\nX_train_rfc.shape, X_test_rfc.shape","43c67233":"def classifier_model(X_train, X_test, y_train, y_test, method, data):\n    rf_clf=RandomForestClassifier(n_estimators=1000, random_state=1)\n    rf_clf.fit(X_train, y_train)\n    y_pred_rf=rf_clf.predict(X_test)\n    score_rlf=accuracy_score(y_test, y_pred_rf)\n    print(\"---Feature Selection method: {}---\". format(method))\n    print(\"---Checking Accuracy with {}---\".format(data))\n    print(\"The accuracy score of Random Forest:\", score_rlf)\n    \n    \n    gb_clf=GradientBoostingClassifier(n_estimators=1000, random_state=1)\n    gb_clf.fit(X_train, y_train)\n    y_pred_gb=gb_clf.predict(X_test)\n    score_gb=accuracy_score(y_test, y_pred_gb)\n    print(\"The accuracy score of Gradient Boosting:\", score_rlf)","62b7e592":"classifier_model(X_train_rfc, X_test_rfc, y_train, y_test, \"Random Forest Feature importance\", \"Reduced Features\")","f570146d":"classifier_model(X_train, X_test, y_train, y_test, \"Random Forest Feature importance\", \"All Features\")","305c645e":"sel_rfe=RFE(RandomForestClassifier(n_estimators=100, random_state=1),n_features_to_select=20)\nsel_rfe.fit(X_train, y_train)\n","94869c46":"# Total features selected:\nsel_rfe.get_support().sum()","61e5fc76":"#### Let's transform the data now;\nX_train_rfe=sel_rfe.transform(X_train)\nX_test_rfe=sel_rfe.transform(X_test)","62fa9699":"classifier_model(X_train_rfe, X_test_rfe, y_train, y_test, \"Recursive feature extraction with RF\", \"Reduced Features\")","5bad58e2":"classifier_model(X_train, X_test, y_train, y_test, \"Recursive feature extraction with RF\", \"All Features\")","fa2c0598":"sel_rfe_gb=RFE(GradientBoostingClassifier(n_estimators=100, random_state=1), n_features_to_select=22)\nsel_rfe_gb.fit(X_train, y_train)\n\nX_train_rfe_gb=sel_rfe_gb.transform(X_train)\nX_test_rfe_gb=sel_rfe_gb.transform(X_test)\n\n    ","b1814720":"classifier_model(X_train_rfe_gb, X_test_rfe_gb, y_train, y_test, \"Recursive feature extraction with GB\", \"Reduced Features\")","f8770d12":"for index in range(14,39):\n    sel_rfe_gb=RFE(GradientBoostingClassifier(n_estimators=100, random_state=1), n_features_to_select=index)\n    sel_rfe_gb.fit(X_train, y_train)\n\n    X_train_rfe_gb=sel_rfe_gb.transform(X_train)\n    X_test_rfe_gb=sel_rfe_gb.transform(X_test)\n    \n    clf_gb=GradientBoostingClassifier(n_estimators=200, random_state=1)\n    clf_gb.fit(X_train_rfe_gb, y_train)\n    y_pred_gb=clf_gb.predict(X_test_rfe_gb)\n    score_gb=accuracy_score(y_test, y_pred_gb)\n    print(\"Number of features: \", index)\n    print(\"Accuracy: \", score_gb)\n    print()","cd2123c6":"sel_rfe_gb_new=RFE(GradientBoostingClassifier(n_estimators=1000, random_state=1), n_features_to_select=16)\nsel_rfe_gb_new.fit(X_train, y_train)\n\nX_train_final=sel_rfe_gb_new.transform(X_train)\nX_test_final=sel_rfe_gb_new.transform(X_test)","8bbd1f8a":"gb_clf_1=GradientBoostingClassifier(n_estimators=400, random_state=1)\n\ngb_clf_1.fit(X_train_final, y_train)\ny_pred_gb_1=gb_clf_1.predict(X_test_final)\n\nscore_gb_1=accuracy_score(y_test, y_pred_gb_1)\n\nprint(\"Accuracy:\" ,score_gb_1)","a24f86b9":"params_grid_gb={'n_estimators' : [100,200,400,600,1000,1200],\n                'min_samples_split': [100,200,300,400],\n                'min_samples_leaf' : [10,20,30,40,60,100],\n                'max_depth' : [2,4,6,8],\n                'learning_rate' : [0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n               }","54597cd0":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\ngridsearch_gb=RandomizedSearchCV(estimator=GradientBoostingClassifier(), param_distributions=params_grid_gb, cv=5, scoring='accuracy')\n","83de835e":"gridsearch_gb.fit(X_train_final, y_train)","4fd18faa":"gridsearch_gb.best_score_","f8df687c":"gridsearch_gb.best_params_","8ffdb977":"#### Checking accuracy on Test set\ny_pred_final_gb=gridsearch_gb.predict(X_test_final)\n\nprint(\"Accuracy of GBM with accuracy_scoreced features on test set\", accuracy_score(y_test, y_pred_final_gb))","78fbfad3":"gridsearch_gb.fit(X_train, y_train)","558cebb7":"gridsearch_gb.best_score_","490140c6":"gridsearch_gb.best_params_","a4a518c9":"#### Checking accuracy on Test set\ny_pred_final_gb_all=gridsearch_gb.predict(X_test)\n\nprint(\"Accuracy of GBM with all features on test set\", accuracy_score(y_test, y_pred_final_gb_all))","61feb71e":"data.head()","37999fe8":"# Let's calculate the difference of values b\/w Blue and red teams in all the columns","3e061df4":"cols=[x[4:] for x in data.columns if \"blue\" in x and x[4:]!= 'Wins']\ncols","cb53a90b":"# Below columns to be dropped  because they are already the difference of blue and red\ncols_to_drop=['GoldDiff', 'ExperienceDiff']\nfinal_cols=[x for x in cols if x not in cols_to_drop]","47869b72":"final_cols","cd526fe3":"data_new=pd.DataFrame()\n\nfor col in final_cols:\n    data_new[f'Diff_{col}'] =data[f'blue{col}']-data[f'red{col}']\n\n    ","afa39b05":"# Keeping values corresponding to only Red in ['GoldDiff', 'ExperienceDiff'] i.e redGoldDiff and redExperienceDiff\nfor col_ in cols_to_drop:\n    data_new[col_]=data[f'red{col_}']","de3958a1":"data_new.head()","f223ff6f":"# Now split the dataset into train and test\nX_new=data_new\ny_new=data['blueWins']\nX_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new, y_new, test_size=0.2, random_state=1, stratify=y)   \n    ","aae3b98c":"### Create 2 datasets tuples in order to run the model easily on new dataset ( feature engineered) and old dataset( original)\n\n#Originaldata\ndataset_1=(X_train, X_test, y_train, y_test, 'dataset_1')\n\n#Featureengineered data\ndataset_2=(X_train_new, X_test_new, y_train_new, y_test_new, 'dataset_2')","b8c01295":"def run_classifier(model, dataset):\n    model.fit(dataset[0], dataset[2])\n    y_pred=model.predict(dataset[1])\n    score_=accuracy_score(dataset[3], y_pred)\n    return f'{round(score_, 4)*100}%'","53e67459":"model_dict={ 'Decision Tree' : DecisionTreeClassifier(max_depth=6,random_state=1),\n            'Random Forest' : RandomForestClassifier(n_estimators=100, random_state=1),\n           'Support Vector Classification': SVC(random_state=1), \n           'Gaussian Naive Bayes': GaussianNB(),\n           'Gradient Boosting Classifier': GradientBoostingClassifier(random_state=1),\n           'XG Boost Classifier': XGBClassifier()\n                 \n          }","0db78b18":"for model in model_dict:\n    print(f'model:{model} -accuracy: {run_classifier(model_dict[model],dataset_1)}')","2bf519da":"for model in model_dict:\n    print(f'model:{model} -accuracy: {run_classifier(model_dict[model],dataset_2)}')","d5908290":"### checking the model with all features","7e78d355":"### Glossary","75e2ed61":"- Warding totem: An item that a player can put on the map to reveal the nearby area. Very useful for map\/objectives control.\n- Minions: NPC that belong to both teams. They give gold when killed by players.\n- Jungle minions: NPC that belong to NO TEAM. They give gold and buffs when killed by players.\n- Elite monsters: Monsters with high hp\/damage that give a massive bonus (gold\/XP\/stats) when killed by a team.\n- Dragons: Elite monster which gives team bonus when killed. The 4th dragon killed by a team gives a massive stats bonus. The - - 5th dragon (Elder Dragon) offers a huge advantage to the team.\n- Herald: Elite monster which gives stats bonus when killed by the player. It helps to push a lane and destroys structures.\n- Towers: Structures you have to destroy to reach the enemy Nexus. They give gold.\n- Level: Champion level. Start at 1. Max is 18.","addbbe93":"#### How many features remain after above procedure","a2449f09":"### Recursive Feature extraction using Gradient Boosting","d21889a7":"#### Create a function to test different classifiers","4a765639":"As seen above, there is no column with 1 or same value throughout","57c3415c":"# Method 1: Feature Selection using different methods and checking with different models","f8d1cfb4":"### On feature Engineered dataset","b7cbc76e":"### Accuracy with Reduced features","4347ded8":"This dataset contains the first 10min. stats of approx. 10k ranked games (SOLO QUEUE) from a high ELO (DIAMOND I to MASTER). Players have roughly the same level.\n\nEach game is unique. The gameId can help you to fetch more attributes from the Riot API.\n\nThere are 19 features per team (38 in total) collected after 10min in-game. This includes kills, deaths, gold, experience, level\u2026 It's up to you to do some feature engineering to get more insights.\n\nThe column blueWins is the target value (the value we are trying to predict). A value of 1 means the blue team has won. 0 otherwise.\n\nSo far I know, there is no missing value","40e655e2":"These are the columns which require to be differenced b\/w Blue and Red teams","698c8439":"### It is clear from above that best selection of features are 16:","112007fb":"### Accuracy with reduced features","3ad44f70":"### A quick run on different algorithms","43ac0a0c":"### Let's run the classifiers now","4e673cf8":"### Let's create a function with RandomForest and Gradient Boost Classifier, once we find the best classifier, we can further fine tune it using hyperparameter tuning","6c8c5644":"As you can see, lot of features have been set as False depicting that they are not as important as other features","ede3bfed":"### As we see that maximum accuracy achieved was , Now let's do some feature engineering to further improve the accuracy","440d019f":"### Importing major libraries","7b2d7a37":"## `Feature Selection using Feature importance of Random Forest Classifier","62e33f4b":"Dropping gameID column as it is only an ID and has different value for each row","7c09170b":"## Gradient boosting algorithm had the highest accuracy. Now let's check how many number of features will give the best accuracy","38318014":"#### Now transforming the data with 16 features only and then running on different models to select the best model","88f05c20":"### Accuracy with reduced features","0fc3f999":"### Content","778d93b1":"### On original dataset","de9b244d":"#### Importing required libraries","d91747d7":"### Context","ba492808":"### Accuracy with All features","a239196f":"#### Let's transformed the data now and check the accuracy","b3989148":"### Accuracy with all features","b5d434c6":"## GRADIENT BOOST CLASSIFIER","a59f25dc":"#### Splitting the data into train and test set","d39fcfef":"### Basic EDA & Preprocessing the data","96e3d72e":"##### As you can see above, accuracy has reduced after feature removal, hence let's check some other method to reduce the feature space","df7b791b":"League of Legends is a MOBA (multiplayer online battle arena) where 2 teams (blue and red) face off. There are 3 lanes, a jungle, and 5 roles. The goal is to take down the enemy Nexus to win the game.","ac9434b8":"### Checking with reduced and important features","61809e00":"# Method 2: Performing feature Engineering and checking with different models now","512644ee":"## Let's run the model\n","ec1b92e7":"## Feature Selection using Recursive feature extraction (RFE)"}}