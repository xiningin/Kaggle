{"cell_type":{"c92d0a48":"code","8f417d2c":"code","0ab36bae":"code","76b4c2d2":"code","002007e1":"code","b522bdff":"code","6963ca0c":"code","390c7995":"code","d28eb150":"code","5fc93889":"code","23b24972":"code","9c2978bd":"markdown","8e6c4b4c":"markdown","18a152a7":"markdown","fcaf7c45":"markdown","d4fffd9f":"markdown","edfcb09d":"markdown","86f4f7c9":"markdown","c2404e69":"markdown"},"source":{"c92d0a48":"import os\nimport pathlib\nimport imageio\nimport numpy as np\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch.utils.data import Dataset\n\n#print(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.","8f417d2c":"print('Parent Directory: ', os.listdir(\"..\/input\"))\nprint('train Directory:  ', os.listdir(\"..\/input\/train\"))\nprint('test Directory:   ', os.listdir(\"..\/input\/train\"))","0ab36bae":"depths_df = pd.read_csv('..\/input\/depths.csv')\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\nsample_submission_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\n","76b4c2d2":"train_df.head()","002007e1":"depths_df.head()\n","b522bdff":"sample_submission_df.head()","6963ca0c":"class TGSSaltDataset(Dataset):\n    \n    def __init__(self, root_path, file_list):\n        self.root_path = root_path\n        self.file_list = file_list\n    \n    def __len__(self):\n        return len(self.file_list)\n    \n    def __getitem__(self, index):\n        if index not in range(0, len(self.file_list)):\n            return self.__getitem__(np.random.randint(0, self.__len__()))\n        \n        file_id = self.file_list[index]\n        \n        image_folder = os.path.join(self.root_path, \"images\")\n        image_path = os.path.join(image_folder, file_id + \".png\")\n        \n        mask_folder = os.path.join(self.root_path, \"masks\")\n        mask_path = os.path.join(mask_folder, file_id + \".png\")\n        \n        image = np.array(imageio.imread(image_path), dtype=np.uint8)\n        mask = np.array(imageio.imread(mask_path), dtype=np.uint8)\n        \n        return image, mask","390c7995":"depths_df = pd.read_csv('..\/input\/train.csv')\n\ntrain_path = \"..\/input\/train\/\"\nfile_list = list(depths_df['id'].values)","d28eb150":"dataset = TGSSaltDataset(train_path, file_list)","5fc93889":"def plot2x2Array(image, mask):\n    f, axarr = plt.subplots(1,2)\n    axarr[0].imshow(image)\n    axarr[1].imshow(mask)\n    axarr[0].grid()\n    axarr[1].grid()\n    axarr[0].set_title('Image')\n    axarr[1].set_title('Mask')","23b24972":"for i in range(5):\n    image, mask = dataset[np.random.randint(0, len(dataset))]\n    plot2x2Array(image, mask)","9c2978bd":"## Directory List","8e6c4b4c":"## Submission Guideline\nSubmission File\n\nIn order to reduce the submission file size, our metric uses run-length encoding on the pixel values. Instead of submitting an exhaustive list of indices for your segmentation, you will submit pairs of values that contain a start position and a run length. E.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n\nThe competition format requires a space delimited list of pairs. For example, '1 3 10 5' implies pixels 1,2,3,10,11,12,13,14 are to be included in the mask. The pixels are one-indexed and numbered from top to bottom, then left to right: 1 is pixel (1,1), 2 is pixel (2,1), etc.\n\nThe metric checks that the pairs are sorted, positive, and the decoded pixel values are not duplicated. It also checks that no two predicted masks for the same image are overlapping.\n\nThe file should contain a header and have the following format. Each row in your submission represents a single predicted salt segmentation for the given image.\n\nid,                     rle_mask\n\n3e06571ef3,   1 1\n\na51b08d882,  1 1\n\nc32590b06f,  1 1\n\netc.","18a152a7":"## Import Libraries","fcaf7c45":"## Load & View Data using Pytorch Dataset Class\nPytorch Data Loading is neat. The official tutorial is [here](http:\/\/https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html). The below work is taken from [this excellent kernel](http:\/\/https:\/\/www.kaggle.com\/skainkaryam\/basic-data-visualization-using-pytorch-dataset)","d4fffd9f":"## Read Data","edfcb09d":"## Exploring Image Data\nWe will explore the image data in an ameture way following [this kernel](https:\/\/www.kaggle.com\/stkbailey\/teaching-notebook-for-total-imaging-newbies), [this kernel](http:\/\/https:\/\/www.kaggle.com\/skainkaryam\/basic-data-visualization-using-pytorch-dataset). Special thanks to them. \n","86f4f7c9":"## Problem Statement: \nSeveral areas of Earth with large accumulations of oil and gas also have huge deposits of salt below the surface.\n\nBut unfortunately, knowing where large salt deposits are precisely is very difficult. Professional seismic imaging still requires expert human interpretation of salt bodies. This leads to very subjective, highly variable renderings. More alarmingly, it leads to potentially dangerous situations for oil and gas company drillers.\n\nTo create the most accurate seismic images and 3D renderings, TGS (the world\u2019s leading geoscience data company) is hoping Kaggle\u2019s machine learning community will be able to build an algorithm that automatically and accurately identifies if a subsurface target is salt or not.","c2404e69":"## Introduction\nThis is a learning and researching purpose kernel. In this journey, we will explore other kernels and works around the internet while will be trying to apply newly learned materials. Stay tuned. "}}