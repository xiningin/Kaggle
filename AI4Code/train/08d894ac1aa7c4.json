{"cell_type":{"2aa20088":"code","4bbfd71b":"code","ba24dd9c":"code","3e475367":"code","f956a7b1":"code","ffb4b3a8":"code","6eb1c73f":"code","59ca0d2c":"code","98c54e03":"code","b50ea0cb":"code","e27f374f":"code","6cc0c3c8":"code","5038f162":"markdown","2a3c52d8":"markdown","81e8faf8":"markdown","c354ebeb":"markdown","dd9c20e8":"markdown","0ac056e0":"markdown"},"source":{"2aa20088":"import numpy as np\nimport matplotlib.pyplot as plt\nnp.random.seed(42)\n\nm = 100\nX = 4*np.random.rand(m, 1) - 2.5\ny = X**3 + 3*X**2 + 2*X + np.random.randn(m, 1)\nnon_random_y = X**3 + 3*X**2 + 2*X","4bbfd71b":"plt.plot(X, y, 'ro')\nplt.plot(X, non_random_y, 'go')\nplt.show()","ba24dd9c":"from sklearn.preprocessing import PolynomialFeatures\npoly_converter = PolynomialFeatures(degree=3, include_bias=False)\n\nx_poly_features = poly_converter.fit_transform(X)","3e475367":"X[0]","f956a7b1":"x_poly_features[0]","ffb4b3a8":"X[0]**2","6eb1c73f":"X[0]**3","59ca0d2c":"from sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(x_poly_features, y)\nlin_reg.coef_","98c54e03":"y_pred = lin_reg.predict(x_poly_features)","b50ea0cb":"plt.plot(X, y, 'ro')\nplt.plot(X, y_pred, 'bo')\nplt.show()","e27f374f":"plt.plot(X, non_random_y, 'go')\nplt.plot(X, y_pred, 'bo')\nplt.show()","6cc0c3c8":"from sklearn.metrics import mean_squared_error\nmean_squared_error(y_true=non_random_y, y_pred=y_pred)","5038f162":"As shown above, it adds 2 more columns to our data. The second column looks like the squared version of $x$ and the third looks like the cubed version of $x$. Let's confirm this.","2a3c52d8":"# What is Polynomial Regression?\nNot all data is just a simple line. Most of the time it's a bit more complicated than that. That doesn't mean you can't use a linear model to predict a non linear dataset. Instead of only using one column of data we'll use 3 this time. We'll train our model on this data by giving each column a higher power. So column 1 will be to the power of 1, column 2 will be to the power of 2 and column 3 will be to the power of 3.\n\n## Creating our dataset\nLet's start by generating our data. This time we'll use the formula:\n\n$ y = x^3 + 3x^2 + 2x $","81e8faf8":"## Creating our model\nThe data we have created can not be predicted by a straight line. So we'll have to find a way to get 3 different $\\thetha$'s to create a model that does work. Let's start by creating 3 $x$ columns instead of one. These will be for $x^1$, $x^2$ and $x^3$. To do this we are going to be using the _sklearn.preprocessing_ package. This package has a _PolynomialFeatures_ preprocessor that can help us with creating new columns.","c354ebeb":"Correct! This means that all PolynomialFeatures does is adding columns that are equal to $x^n$ where $n$ is the column number.\n\nNow let's try creating a model that predicts our data. We are going to use the _LinearRegression_ from Scikit-Learn this time, as we have already built our own model once before in a previous kernel.","dd9c20e8":"That's a good score! A MSE of 0.05 means that it's near perfect at predicting our training data. \n\n## Conclusion\nPolynomial Regression is not that different from regular Linear Regression, though it is a lot more versatile. Most of the time a linear model won't work on a real world dataset, but it's still a good model to use when analyzing numeric data that has some noise. \n\n### Previous Kernel\n[What is Gradient Descent?](https:\/\/www.kaggle.com\/veleon\/what-is-gradient-descent)\n### Next Kernel\n[What are Support Vector Machines?](https:\/\/www.kaggle.com\/veleon\/what-are-support-vector-machines)","0ac056e0":"That looks quite accurate! But we can also check it against our y without randomness added."}}