{"cell_type":{"1e697b3f":"code","c9ddf14b":"code","944ede97":"code","7e92d4b2":"code","f7db3d69":"code","0340ebea":"code","f0f635b1":"code","e7a4f967":"code","9f816899":"code","26593527":"code","94fdf4e0":"code","93183f7b":"code","096da06d":"code","e465e6c1":"code","f1ec297c":"code","eb02a907":"code","d13be737":"code","c9223373":"code","23bd3409":"code","3261732b":"code","93fc6975":"code","20f0e3f7":"code","5a595394":"code","386478ac":"code","06785f37":"code","a81729e1":"code","44239573":"code","b96c56e0":"code","028308f8":"code","f548281c":"code","8498f1f3":"code","b049f327":"code","866e676c":"code","c86afbe5":"code","cbcfaf57":"code","dd7daabe":"code","81ca0f95":"code","8b4824ff":"code","ca3c350f":"code","67ff7710":"code","3906094c":"code","03e7b2c2":"code","c735f2c4":"code","955bcc18":"code","d022aaac":"code","8046d11b":"code","5891ce26":"code","a0d6d73a":"code","e994eb80":"markdown","dd9d455a":"markdown","fd5e2e3d":"markdown","40060d7d":"markdown","dbf22a55":"markdown","04f8b448":"markdown","d7c8dd2c":"markdown","ec7399c8":"markdown","5235ba96":"markdown","33726040":"markdown","a7aafb58":"markdown","c99467c4":"markdown","0f828f79":"markdown","70525bb2":"markdown"},"source":{"1e697b3f":"# Pacotes basicos\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport itertools\nimport imblearn\nimport math\n\n# Metricas e Graficos\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom seaborn import countplot, lineplot, barplot\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom scipy.stats import kurtosis, skew\n\n# Modelos\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n\n# Tratamento de warning e exibi\u00e7\u00e3o no Jupyter\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\npd.set_option('display.max_columns', None)\n\n# Matplot \nimport matplotlib.pyplot as plt\nimport matplotlib.style as style \n%matplotlib inline\nstyle.use('ggplot')\n\n# Outras libs\nimport pickle\nimport os\nfrom time import time\nimport gc\ngc.enable()","c9ddf14b":"treino = pd.read_csv('..\/input\/competicao-dsa-machine-learning-sep-2019\/X_treino.csv')\nteste = pd.read_csv('..\/input\/competicao-dsa-machine-learning-sep-2019\/X_teste.csv')\ntarget = pd.read_csv('..\/input\/competicao-dsa-machine-learning-sep-2019\/y_treino.csv')\nsub = pd.read_csv('..\/input\/competicao-dsa-machine-learning-sep-2019\/sample_submission.csv')","944ede97":"# Primeiros registros do dataset de treino\ntreino.head()","7e92d4b2":"# Primeiros registros do dataset de teste\nteste.head()","f7db3d69":"# Primeiros registros do dataset target\ntarget.head()","0340ebea":"# Analise estat\u00edstica do dataset de treino\ntreino.describe().T","f0f635b1":"# Analise estat\u00edstica do dataset de teste\nteste.describe().T","e7a4f967":"# Analise estat\u00edstica do dataset target\ntarget.describe()","9f816899":"# Cada serie tem 128 medidas\nlen(treino.measurement_number.value_counts())","26593527":"# Verificar se existem dados nulos no dataset de treino\ntreino.isnull().values.any() ","94fdf4e0":"# Verificar se existem dados nulos no dataset de teste\nteste.isnull().values.any() ","93183f7b":"# Existem 6 series a mais no dataset de teste\n(teste.shape[0] - treino.shape[0]) \/ 128","096da06d":"# Existe 73 grupos unicos no dataset target \ntarget['group_id'].nunique()","e465e6c1":"# Visualizando todos os tipos de superf\u00edcie do dataset target, ordenado pela quantidade de registros\nsns.countplot(y = 'surface',\n              data = target,\n              order = target['surface'].value_counts().index)\nplt.show()","f1ec297c":"# Visualizando a distribui\u00e7\u00e3o das features: group_id e surface\n# Cr\u00e9ditos: https:\/\/www.kaggle.com\/gpreda\/robots-need-help\nfig, ax = plt.subplots(1,1,figsize=(26,8))\ngrp = pd.DataFrame(target.groupby(['group_id', 'surface'])['series_id'].count().reset_index())\npiv = grp.pivot(index='surface', columns='group_id', values='series_id')\ngrafico = sns.heatmap(piv, linewidths=.1, linecolor='black', annot=True, cmap=\"YlGnBu\")\ngrafico.set_title('Surface x Grupo_id', size=16)\nplt.show()","eb02a907":"# Grafico de contador de numero de registros por group_id, ordenado\nplt.figure(figsize=(23,5)) \ncountplot(x=\"group_id\", data=target, order=target['group_id'].value_counts().index)\nplt.show()","d13be737":"series_dict = {}\nfor series in (treino['series_id'].unique()):\n    series_dict[series] = treino[treino['series_id'] == series]  ","c9223373":"def plot_series(series_id):\n    plt.figure(figsize=(28, 16))\n    print(target[target['series_id'] == series_id]['surface'].values[0].title())\n    for i, col in enumerate(series_dict[series_id].columns[3:]):\n        if col.startswith(\"o\"):\n            color = 'red'\n        elif col.startswith(\"a\"):\n            color = 'green'\n        else:\n            color = 'blue'\n        if i >= 7:\n            i+=1\n        plt.subplot(3, 4, i + 1)\n        plt.plot(series_dict[series_id][col], color=color, linewidth=3)\n        plt.title(col)","23bd3409":"# Visualizando a serie de c\u00f3digo 0\nid_series = 0\nplot_series(id_series)","3261732b":"f,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(treino.iloc[:,3:].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","93fc6975":"def plot_distribution(df1, df2, label1, label2, features,a=2,b=5):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(a,b,figsize=(17,9))\n\n    for feature in features:\n        i += 1\n        plt.subplot(a,b,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","20f0e3f7":"# Gr\u00e1fico de distribui\u00e7\u00e3o por dataset (treino x teste)\nfeatures = treino.columns.values[3:]\nplot_distribution(treino, teste, 'Treino', 'Teste', features)","5a595394":"def plot_classes_distribution(classes, tt, features,a=5,b=2):\n    i = 0\n    plt.figure()\n    fig, ax = plt.subplots(a,b,figsize=(16,24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(a,b,i)\n        for cl in classes:\n            ttc = tt[tt['surface']==cl]\n            sns.kdeplot(ttc[feature], bw=0.5,label=cl)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","386478ac":"# Gr\u00e1fico de distribui\u00e7\u00e3o por classe\nclasses = (target['surface'].value_counts()).index\naux = treino.merge(target, on='series_id', how='inner')\nplot_classes_distribution(classes, aux, features)","06785f37":"# Funcao para converter Quaternions para Angulos de Euler\ndef quaternion_to_euler(x, y, z, w):\n\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z\n\n# Fun\u00e7\u00f5es para cria\u00e7\u00e3o de features estat\u00edsticas\ndef _kurtosis(x):\n    return kurtosis(x)\n\ndef skewness(x):\n    return skew(x)\n\ndef SSC(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    \n    xn_i1 = x[0:len(x)-2]  \n    ans = np.heaviside((xn-xn_i1)*(xn-xn_i2),0)\n    return sum(ans[1:]) \n\ndef wave_length(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    \n    return sum(abs(xn_i2-xn))\n    \ndef norm_entropy(x):\n    tresh = 3\n    return sum(np.power(abs(x),tresh))\n\ndef SRAV(x):    \n    SRA = sum(np.sqrt(abs(x)))\n    return np.power(SRA\/len(x),2)\n\ndef mean_abs(x):\n    return sum(abs(x))\/len(x)\n\ndef zero_crossing(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1\n    return sum(np.heaviside(-xn*xn_i2,0))","a81729e1":"# Fun\u00e7\u00e3o para cria\u00e7\u00e3o de novas features\ndef fn_features_01(df):\n    df['totl_anglr_vel'] = (df['angular_velocity_X']**2 + df['angular_velocity_Y']**2 + df['angular_velocity_Z']**2)** 0.5\n    df['totl_linr_acc'] = (df['linear_acceleration_X']**2 + df['linear_acceleration_Y']**2 + df['linear_acceleration_Z']**2)**0.5\n    df['totl_xyz'] = (df['orientation_X']**2 + df['orientation_Y']**2 + df['orientation_Z']**2)**0.5\n    df['acc_vs_vel'] = df['totl_linr_acc'] \/ df['totl_anglr_vel']\n    df['norm_quat'] = (df['orientation_X']**2 + df['orientation_Y']**2 + df['orientation_Z']**2 + df['orientation_W']**2)\n    df['mod_quat'] = (df['norm_quat'])**0.5\n    df['norm_X'] = df['orientation_X'] \/ df['mod_quat']\n    df['norm_Y'] = df['orientation_Y'] \/ df['mod_quat']\n    df['norm_Z'] = df['orientation_Z'] \/ df['mod_quat']\n    df['norm_W'] = df['orientation_W'] \/ df['mod_quat']\n    \n    x, y, z, w = df['norm_X'].tolist(), df['norm_Y'].tolist(), df['norm_Z'].tolist(), df['norm_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    df['euler_x'] = nx\n    df['euler_y'] = ny\n    df['euler_z'] = nz\n\n    return df","44239573":"# Fun\u00e7\u00e3o para cria\u00e7\u00e3o de novas features, agrupando por series_id\ndef fn_features_02(data):\n    df = pd.DataFrame()\n    \n    def mean_change_of_abs_change(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n    \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number', 'orientation_X', 'orientation_Y', 'orientation_Z', 'orientation_W']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        df[col + '_maxtoMin'] = df[col + '_max'] \/ df[col + '_min']\n        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_mean_change_of_abs_change'] = data.groupby('series_id')[col].apply(mean_change_of_abs_change)\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])\/2\n        \n        # Advanced Features\n        df[col + '_skew'] = data.groupby(['series_id'])[col].skew()\n        df[col + '_mad'] = data.groupby(['series_id'])[col].mad()\n        df[col + '_q25'] = data.groupby(['series_id'])[col].quantile(0.25)\n        df[col + '_q75'] = data.groupby(['series_id'])[col].quantile(0.75)\n        df[col + '_q95'] = data.groupby(['series_id'])[col].quantile(0.95)\n        df[col + '_iqr'] = df[col + '_q75'] - df[col + '_q25']\n        df[col + '_SSC'] = data.groupby(['series_id'])[col].apply(SSC) \n        df[col + '_skewness'] = data.groupby(['series_id'])[col].apply(skewness)\n        df[col + '_wave_lenght'] = data.groupby(['series_id'])[col].apply(wave_length)\n        df[col + '_norm_entropy'] = data.groupby(['series_id'])[col].apply(norm_entropy)\n        df[col + '_SRAV'] = data.groupby(['series_id'])[col].apply(SRAV)\n        df[col + '_kurtosis'] = data.groupby(['series_id'])[col].apply(_kurtosis) \n        df[col + '_zero_crossing'] = data.groupby(['series_id'])[col].apply(zero_crossing) \n\n    return df\n    ","b96c56e0":"# Aplicando novas features nos datasets de treino e teste\ntreino = fn_features_01(treino)\nteste = fn_features_01(teste)\ntreino.shape, teste.shape","028308f8":"# Aplicando novas features nos datasets de treino e teste\n# Esta celula demora um pouco para concluir (cerca de 10min)\ntreino = fn_features_02(treino)\nteste = fn_features_02(teste)\ntreino.shape, teste.shape","f548281c":"# Visualizando os primeiros registros do dataset de treino com as novas features\ntreino.head()","8498f1f3":"# Preenchendo os valores NA e inf com zero\n# Acontece ap\u00f3s a cria\u00e7\u00e3o das novas vari\u00e1veis estat\u00edsticas\ntreino.fillna(0,inplace=True)\ntreino.replace(-np.inf,0,inplace=True)\ntreino.replace(np.inf,0,inplace=True)\n\nteste.fillna(0,inplace=True)\nteste.replace(-np.inf,0,inplace=True)\nteste.replace(np.inf,0,inplace=True)","b049f327":"# Transformando a feature 'surface' de string para num\u00e9rico\nle = preprocessing.LabelEncoder()\ntarget['surface'] = le.fit_transform(target['surface'])","866e676c":"# Utilizando o m\u00e9todo StratifiedKFold para realizar os grupos de treinamento\nfolds = StratifiedKFold(n_splits=10, shuffle=True, random_state=2019)","c86afbe5":"previsao = np.zeros((teste.shape[0],9))\nreal = np.zeros((treino.shape[0]))\nscore = 0","cbcfaf57":"# Execu\u00e7\u00e3o da cria\u00e7\u00e3o e treinamento do modelo\n# Utiliza\u00e7\u00e3o do algoritmo RANDOM FOREST CLASSIFIER\n\nfor times, (trn_idx, val_idx) in enumerate(folds.split(treino.values, target['surface'].values)):\n    rf = RandomForestClassifier(n_estimators=500, n_jobs = -1)\n    rf.fit(treino.iloc[trn_idx], target['surface'][trn_idx])\n    real[val_idx] = rf.predict(treino.iloc[val_idx])\n    previsao += rf.predict_proba(teste) \/ folds.n_splits\n    score += rf.score(treino.iloc[val_idx], target['surface'][val_idx])\n    print(\"Fold: {} score: {}\".format(times, rf.score(treino.iloc[val_idx], target['surface'][val_idx])))\n    gc.collect()","dd7daabe":"print('Acuracia Media: ', score \/ folds.n_splits)","81ca0f95":"confusion_matrix(real, target['surface'])","8b4824ff":"sub['surface'] = le.inverse_transform(previsao.argmax(axis=1))\n#sub.to_csv('submission_rf.csv', index=False)\nsub.head()","ca3c350f":"# Carregando novamente os datasets\ntt_treino = pd.read_csv('..\/input\/competicao-dsa-machine-learning-sep-2019\/X_treino.csv')\ntt_teste = pd.read_csv('..\/input\/competicao-dsa-machine-learning-sep-2019\/X_teste.csv')\ntt_y_treino = pd.read_csv('..\/input\/competicao-dsa-machine-learning-sep-2019\/y_treino.csv')\nss = pd.read_csv('..\/input\/competicao-dsa-machine-learning-sep-2019\/sample_submission.csv')","67ff7710":"# Concatenando os datasets de treino e teste\nfull = pd.concat([tt_treino, tt_teste])\nfull = full.iloc[:,3:].values.reshape(-1,128,10)","3906094c":"# Fun\u00e7\u00f5es para calcular a distancia entre as amostras de dados\n# O objetivo \u00e9 identificar links entre os datasets de treino e teste\n# Caso exista rela\u00e7\u00f5es, estas ser\u00e3o utilizadas para avalia\u00e7\u00e3o no Kaggle\ndef sq_dist(a,b):\n    return np.sum((a-b)**2, axis=1)\n\ndef find_run_edges(data, edge):\n    if edge == 'left':\n        border1 = 0\n        border2 = -1\n    elif edge == 'right':\n        border1 = -1\n        border2 = 0\n    else:\n        return False\n    \n    edge_list = []\n    linked_list = []\n    \n    for i in range(len(data)):\n        dist_list = sq_dist(data[i, border1, :4], data[:, border2, :4])\n        min_dist = np.min(dist_list)\n        closest_i   = np.argmin(dist_list)\n        if closest_i == i:\n            closest_i = np.argsort(dist_list)[1]\n        dist_list = sq_dist(data[closest_i, border2, :4], data[:, border1, :4])\n        rev_dist = np.min(dist_list)\n        closest_rev = np.argmin(dist_list)\n        if closest_rev == closest_i:\n            closest_rev = np.argsort(dist_list)[1]\n        if (i != closest_rev):\n            edge_list.append(i)\n        else:\n            linked_list.append([i, closest_i, min_dist])\n            \n    return edge_list, linked_list\n\ndef find_runs(data, left_edges, right_edges):\n    data_runs = []\n\n    for start_point in left_edges:\n        i = start_point\n        run_list = [i]\n        while i not in right_edges:\n            tmp = np.argmin(sq_dist(data[i, -1, :4], data[:, 0, :4]))\n            if tmp == i: # self-linked sample\n                tmp = np.argsort(sq_dist(data[i, -1, :4], data[:, 0, :4]))[1]\n            i = tmp\n            run_list.append(i)\n        data_runs.append(np.array(run_list))\n    \n    return data_runs","03e7b2c2":"# Procurando por link entre os dados\ntrain_left_edges, train_left_linked  = find_run_edges(full, edge='left')\ntrain_right_edges, train_right_linked = find_run_edges(full, edge='right')\ntrain_runs = find_runs(full, train_left_edges, train_right_edges)\nprint('Found', len(train_left_edges), 'left edges and', len(train_right_edges), 'right edges.')","c735f2c4":"ss['surface'] = ''\ndf_surface = ''\n\nfor i in range(151):\n    x = train_runs[i]\n    x = np.sort(x)\n    if x[0]<3810:\n        df_surface = tt_y_treino['surface'][x[0]]\n        for j in range(len(train_runs[i])):\n            if train_runs[i][j]-3810>-1:\n                ss['surface'][train_runs[i][j]-3810] = df_surface","955bcc18":"ss.head()","d022aaac":"y_train_runs = ss.copy()\n\nsub_final = {}\nfor i in range(0, sub.shape[0]):\n    sub_final.update({sub.iloc[i]['series_id'] : sub.iloc[i]['surface'] })\n    \ny_train_runs.head()","8046d11b":"resultado = []\nfor i in range(0, y_train_runs.shape[0]):\n    if (y_train_runs.surface[i] == ''):\n        resultado.append(sub_final[i])\n    else:\n        resultado.append(y_train_runs.surface[i])\n        \ny_train_runs['surface'] = resultado\ny_train_runs.to_csv('best_submission_rf.csv', index=False)\ny_train_runs.head()","5891ce26":"y_train_runs.surface.value_counts()","a0d6d73a":"y_train_runs.surface.value_counts() \/ y_train_runs.shape[0]","e994eb80":"# Submiss\u00e3o Kaggle","dd9d455a":"# Cria\u00e7\u00e3o e Valida\u00e7\u00e3o dos Modelos de Machine Learning","fd5e2e3d":"## Visualiza\u00e7\u00e3o das Series","40060d7d":"# An\u00e1lise Explorat\u00f3ria (EAD)","dbf22a55":"# Importando as bibliotecas que ser\u00e3o utilizadas neste projeto","04f8b448":"## Correla\u00e7\u00e3o","d7c8dd2c":"### Gerando submiss\u00e3o usando link entre dataset de treino e teste\n##### Cr\u00e9ditos: https:\/\/www.kaggle.com\/friedchips\/the-missing-link","ec7399c8":"# Carregando os dados","5235ba96":"## Visualiza\u00e7\u00e3o dos Grupos","33726040":"## Gr\u00e1ficos de Distribui\u00e7\u00e3o","a7aafb58":"# Competi\u00e7\u00e3o DSA de Machine Learning\n# Edi\u00e7\u00e3o Setembro\/2019\n\n##### Referencias\n- https:\/\/www.kaggle.com\/c\/career-con-2019\n- https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive\n- https:\/\/www.kaggle.com\/gpreda\/robots-need-help\n- https:\/\/www.kaggle.com\/friedchips\/the-missing-link\n- https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/87239","c99467c4":"### Criando fun\u00e7\u00f5es auxiliares para defini\u00e7\u00e3o de novas features","0f828f79":"# Feature Enginnering","70525bb2":"### Gerando a melhor submiss\u00e3o para o Kaggle"}}