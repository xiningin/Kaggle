{"cell_type":{"e11447a5":"code","05302975":"code","4edccfdb":"code","e9476ba4":"code","85fabaf2":"code","4fb2ca8a":"code","afe4fba9":"markdown"},"source":{"e11447a5":"import re\nimport sys\nimport spacy\n\nREPLACE_WITH_SPACE = ['\\n', '\\r', '\\t', '\\xa0', '\/', '|', '\\\\', '\\\\n', '\\\\r', '\\\\t', '~', '_', '*', '#', '=', ':',\n                      '&', '(', ')', '[', ']', '<', '>', '{', '}', '-', '+', '\\\\xa']\nREPLACE_WITH_COMMA = ['\u3001', '\uff64']\nREPLACE_WITH_EMPTY_STR = ['\"', \"'\", \"`\", \"\u00b4\", \"^\", \"\u00a8\"]\n\n\ndef clean_str(s):\n    \"\"\"\n    A simple string cleaning function for filtering typical noise that the text may have.\n    :param s: str\n    :return: str\n        A cleaned string.\n    \"\"\"\n    s = str(s).lower()  # ensure that it is indeed a str + lower\n\n    # CASE 1: Remove Internet noise.\n    s = re.sub(r'<.*?>', ' ', s)  # Remove HTML tags.\n    s = re.sub(r'http\\S+\\b', '', s)  # Remove links\n    s = re.sub(r'\\S*@\\S*\\s?\\b', ' ', s)  # Remove email addresses, even faulty ones and @-mentions such as @mary\n    \n    # CASE 2: Normalize spacing and listings given noise present in data and remove difficult characters.\n    for char in REPLACE_WITH_SPACE:\n        s = s.replace(char, ' ')\n    for char in REPLACE_WITH_COMMA:\n        s = s.replace(char, ',')\n    for char in REPLACE_WITH_EMPTY_STR:\n        s = s.replace(char, '')\n\n    s = re.sub(' +', ' ', s, re.UNICODE)  # Replace multiple spaces with one space.\n    return s.strip()  # Remove leading and trailing whitespaces.\n\n\nclass NLP:\n\n    def __init__(self, lang):\n        self.lang = lang\n        self.nlp = spacy.load(self.lang)\n\n    def _tokenize(self, s):\n        return [token.text for token in self.nlp(s)]\n\n    def normalize_text(self, text):\n        text = clean_str(text)\n        return self._tokenize(text)\n\n","05302975":"from os import chdir\nfrom os.path import join\nimport glob\nimport json\n\nfrom gensim.models.doc2vec import TaggedDocument\nfrom tqdm import tqdm\n\nROOT_DIR = '\/kaggle\/input\/CORD-19-research-challenge\/'\n\n\n# Configure what kind of datasets exist\nMIN_DOC_LEN = 20\nBIORXIV_DATADIR = join(ROOT_DIR, 'biorxiv_medrxiv', 'biorxiv_medrxiv', 'pdf_json')\nCOMM_USE_DATADIR1 = join(ROOT_DIR, 'comm_use_subset', 'comm_use_subset', 'pdf_json')\nCOMM_USE_DATADIR2 = join(ROOT_DIR, 'comm_use_subset', 'comm_use_subset', 'pmc_json')\nCUSTOM_LICENSE_DATADIR1 = join(ROOT_DIR, 'custom_license', 'custom_license', 'pdf_json')\nCUSTOM_LICENSE_DATADIR2 = join(ROOT_DIR, 'custom_license', 'custom_license', 'pmc_json')\nNONCOMM_USE_SUBSET_DATADIR1 = join(ROOT_DIR, 'noncomm_use_subset', 'noncomm_use_subset', 'pdf_json')\nNONCOMM_USE_SUBSET_DATADIR2 = join(ROOT_DIR, 'noncomm_use_subset', 'noncomm_use_subset', 'pmc_json')\n\nDATA_DIRS = [BIORXIV_DATADIR, COMM_USE_DATADIR1, COMM_USE_DATADIR2, CUSTOM_LICENSE_DATADIR1, CUSTOM_LICENSE_DATADIR2,\n             NONCOMM_USE_SUBSET_DATADIR1, NONCOMM_USE_SUBSET_DATADIR2]\n\nBIORXIV_DATASET_NAME = 'biorxiv'\nCOMM_USE_DATASET1_NAME = 'comm_use_1'\nCOMM_USE_DATASET2_NAME = 'comm_use_2'\nCUSTOM_LICENSE_DATASET1_NAME = 'custom_license_1'\nCUSTOM_LICENSE_DATASET2_NAME = 'custom_license_2'\nNONCOMM_USE_SUBSET_DATASET1_NAME = 'noncomm_use_subset_1'\nNONCOMM_USE_SUBSET_DATASET2_NAME = 'noncomm_use_subset_2'\n\n\nDATASETS = [BIORXIV_DATASET_NAME, COMM_USE_DATASET1_NAME, COMM_USE_DATASET2_NAME, CUSTOM_LICENSE_DATASET1_NAME,\n            CUSTOM_LICENSE_DATASET2_NAME, NONCOMM_USE_SUBSET_DATASET1_NAME, NONCOMM_USE_SUBSET_DATASET2_NAME]\nDATASETS_PATHS = {k: v for (k, v) in zip(DATASETS, DATA_DIRS)}\n\n# TODO: terminology is a bit bad here. In reality datasets can consists of document sets which consists of documents,\n# TODO i.e. a document set would be the appropriate abstraction level from which documents are derived with their\n# TODO own \"recipes\", i.e. how to construct a document for machine learning purposes.\n\n\nclass BaseDataset:\n\n    def __init__(self, path, name, **kwargs):\n        self.path = path\n        self.name = name\n        self.extension = kwargs.get('extension', '.json')\n        self.data_cols = kwargs.get('data_cols', {'metadata': ['title'], 'abstract': ['text']})  # abstract\n        self.tag_cols = kwargs.get('tag_cols', {'root': ['paper_id']})\n\n    @staticmethod\n    def __get_entry_from_line(line, key, val):\n        if key == 'root':\n            return line.get(val, None)\n        lines = line.get(key, {})\n        if type(lines) == list:\n            entries = []\n            for l in lines:\n                entry = l.get(val, None)\n                if entry:\n                    entries.append(entry)\n            return '\\n'.join(entries)\n        return lines.get(val, None)\n\n    def get_document_from_line(self, line):\n        docs = []\n        for key, vals in self.data_cols.items():\n            for val in vals:\n                entry = self.__get_entry_from_line(line, key, val)\n                if entry:\n                    docs.append(entry)\n        return '\\n'.join(docs)\n\n    def get_tags_from_line(self, line):\n        tags = []\n        for key, vals in self.tag_cols.items():\n            for val in vals:\n                entry = self.__get_entry_from_line(line, key, val)\n                if entry:\n                    tags.append(entry)\n        return tags\n\n    def get_documents_labels(self):\n        chdir(self.path)\n        for fname in glob.glob('*' + self.extension):\n            with open(fname, 'r') as f:\n                data = json.load(f)\n                doc = self.get_document_from_line(data)\n                tags = self.get_tags_from_line(data)\n                if tags and len(doc) >= MIN_DOC_LEN:  # Do not even consider too short documents\n                    yield doc, tags\n\n\ndef initialize_dataset(name, **kwargs):\n    return BaseDataset(path=DATASETS_PATHS[name], name=name, **kwargs)\n\n\nclass Datasets:\n\n    def __init__(self, lang='en', **kwargs):\n        \"\"\"\n        Do not use this object for anything else besides training models because normalization of documents\n        takes a while.\n        TODO: in an actual solution documents should be normalized and those normalized documents should be stored\n        TODO: as corpus for fast loading when initiating a training procedure.\n        :param lang: str\n        :param kwargs: dict\n        \"\"\"\n        self.datasets = kwargs.get('datasets', DATASETS)\n        self.nlp = NLP(lang)\n        self.tagged_docs = []\n        self.__init_tagged_docs()\n\n    def __init_tagged_docs(self):\n        for dataset_name in self.datasets:\n            dataset = initialize_dataset(dataset_name)\n            for doc, labels in dataset.get_documents_labels():\n                self.tagged_docs.append(TaggedDocument(words=self.nlp.normalize_text(doc), tags=labels))\n\n    def __iter__(self, **kwargs):\n        for tagged_document in self.tagged_docs:\n            yield tagged_document\n\n\nclass DatasetsSearcher:\n\n    def __init__(self, **kwargs):\n        \"\"\"\n        A handy searcher object that only has dataset titles and labels present.\n        TODO: this object is slow to load. In an ideal solution this would be done in a database.\n        :param lang: str\n        :param kwargs: dict\n        \"\"\"\n        self.datasets = kwargs.get('datasets', DATASETS)\n        self.documents = {}\n        self.__init_documents()\n\n    def __init_documents(self):\n        print(\"Loading datasets, this may take a couple of minutes...\")\n        for dataset_name in self.datasets:\n            print(\"loading dataset {}\".format(dataset_name))\n            dataset = initialize_dataset(dataset_name, **{'data_cols': {'metadata': ['title']}})\n            for doc, labels in tqdm(dataset.get_documents_labels()):\n                # In this case labels only have one item and it's always unique.\n                self.documents[labels[0]] = doc\n\n    def find_title_by_label(self, label):\n        return self.documents.get(label, None)\n","4edccfdb":"import logging\n\nfrom gensim.models import Doc2Vec\n\nMODELS_DIR = '\/kaggle\/input\/covid19-doc2vec\/'\n\n# No need to deviate from common good parameters except for:\n# epochs - because the dataset is small\n# min_count - because the dataset is small\n\n# In some recommender system applications negative ns_exponent values (such as -0.5) may be better; please\n# refer to Gensim documentation to find that paper, if you are interested.\n\nDOC2VEC_PARAMS = {\n    'dm': 0,\n    'hs': 0,\n    'negative': 5,\n    'ns_exponent': 0.75,\n    'sample': 10e-5,\n    'dbow_words': 1,\n    'workers': -1,\n    'vector_size': 300,\n    'min_count': 3,\n    'window': 5,\n    'epochs': 10,\n    'alpha': 0.025,\n    'min_alpha': 0.0025,\n}\n\n\nclass Doc2VecEmbedding:\n\n    def __init__(self, lang='en'):\n        self.lang = lang\n        self.model = None\n        self.nlp = NLP(self.lang)\n\n    def __get_model_fpath(self):\n        return join(MODELS_DIR, 'doc2vec_%s.model' % self.lang)\n\n    def load(self):\n        self.model = Doc2Vec.load(self.__get_model_fpath())\n\n    def fit(self, **kwargs):\n        logging.basicConfig(format='%(levelname)s : %(message)s', level=logging.INFO)\n        logging.root.level = logging.INFO\n        datasets = Datasets()\n        params = DOC2VEC_PARAMS\n        params['documents'] = datasets\n        self.model = Doc2Vec(**params)\n        self.model.save(self.__get_model_fpath())\n\n    def vectorize(self, text):\n        tokens = self.nlp.normalize_text(text)\n        return self.model.infer_vector(tokens, steps=256)\n\n    def find_most_similar_docs(self, text, topn=10):\n        vector = self.vectorize(text)\n        return self.model.docvecs.most_similar(positive=[vector], topn=topn)\n","e9476ba4":"\n# All queries. Note that queries were modified to help improve the results, e.g.\n# e.g. by replacing demonstrative pronouns with proper nouns\n# \"this disease\" => \"Covid-19 disease\"\n# \"xxx of the virus\" => \"xxx of the corona virus\"\n# Note that since the model is trained with only Corona virus research, not mentioning it by name should not be a\n# big issue\n\nTRANSMISSION_QUERIES = [\n    \"Covid-19 incubation periods in humans\",\n    \"Covid-19 incubation periods in humans with respect to age and health status\",\n    \"Prevalence of asymptomatic shedding and transmission\",\n    \"Transmission and children\",\n    \"Physical science of the corona virus\",\n    \"Corona virus charge distribution\",\n    \"Corona virus adhesion to hydrophilic and hydrophobic surfaces\"\n    \"Corona virus environmental survival\",\n    \"Corona virus decontamination of affected areas\",\n    \"Persistence and stability of Corona virus on substrates and sources (nasal discharge, sputum, urine, fecal matter, blood)\",\n    \"Natural history of the Corona virus and shedding from an infected person\",\n    \"Implementation of diagnostics and products to improve clinical processes\",\n    \"Corona virus disease models\",\n    \"Corona virus animal models for infection, disease and transmission\",\n    \"Phenotypic change and adaptation of Corona virus\",\n    \"Immune response and immunity\",\n    \"Effectiveness of movement control strategies to prevent secondary transmission in health care and community settings\",\n    \"Effectiveness of personal protective equirement (PPE)\",\n    \"Personal protective equipment (PPE) usefulness to reduce risk of transmission in health care and community settings\",\n    \"Role of the environment in transmission\"\n]\n\nRISK_FACTORS_QUERIES = [\n    \"Corona virus risk factors smoking\",\n    \"Corona virus risk factors plumonary disease\",\n    \"Corona virus co-infections and other co-mobidities\",\n    \"Neonates and pregnant women\",\n    \"Socio-economic and behavioral factors to understand the conomic impact of the Corona virus\",\n    \"Transmission dynamics of the Corona virus\",\n    \"Corona virus basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\",\n    \"Severity of Covid-19 disease\",\n    \"Covid-19 fatality among symptomatic hospitalized patients and high-risk patient groups\",\n    \"Corona virus susceptibility of populations\",\n    \"Public health mitigation measures that could be effective for Corona virus control\"\n\n]\n\nVIRUS_GENETIC_QUERIES = [\n    \"Real-time tracking of whole genomes and a mechanism for coordinating the rapid dissemination of that information to inform the development of diagnostics and therapeutics and to track variations of the Corona virus over time\",\n    \"Geographic distribution and genomic differences of Corona virus\",\n    \"Corona virus strains\",\n    \"Corona virus farmers\",\n    \"Southeast Asia wildlife and livestock Corona virus\",\n    \"Corona virus host range\",\n    \"Corona virus animal hosts and continued spill-over to humans\",\n    \"Socioeconomic and behavioral risk factors of spill-over\",\n    \"Sustainable risk reduction strategies\"\n]\n\nVACCINES_QUERIES = [\n    \"Effectiveness of drugs being developed and tried to treat COVID-19 patients\",\n    \"Clinical and bench trials to investigate less common viral inhibitors against COVID-19 such as naproxen, clarithromycin, and minocyclinethat that may exert effects on viral replication\",\n    \"Exploration of use of best animal models and their predictive value for a human vaccine\",\n    \"Capabilities to discover a therapeutic for the disease, and clinical effectiveness studies to discover therapeutics, to include antiviral agents\",\n    \"Models to aid decision makers in determining how to prioritize and distribute therapeutics when production ramps up\",\n    \"Identifying approaches for expanding production capacity to ensure equitable and timely distribution to populations in need\",\n    \"Efforts targeted at a universal corona virus vaccine.\",\n    \"Efforts to develop animal models and standardize challenge studies\",\n    \"Efforts to develop prophylaxis clinical studies and prioritize in healthcare workers\",\n    \"Approaches to evaluate risk for enhanced disease after vaccination\",\n    \"Assays to evaluate vaccine immune response and process development for vaccines, alongside suitable animal models\"\n]\n\nMEDICAL_CARE_QUERIES = [\n    \"Resources to support skilled nursing facilities and long term care facilities.\",\n    \"Mobilization of surge medical staff to address shortages in overwhelmed communities\",\n    \"Age-adjusted mortality data for Acute Respiratory Distress Syndrome (ARDS) with other organ failure\",\n    \"Extracorporeal membrane oxygenation (ECMO) outcomes data of COVID-19 patients\",\n    \"Outcomes data for COVID-19 after mechanical ventilation adjusted for age\",\n    \"COVID-19 extrapulmonary manifestations cardiomyopathy and cardiac arrest\",\n    \"Application of regulatory standards (e.g., EUA, CLIA) and ability to adapt care to crisis standards of care level\",\n    \"Encouraging and facilitating the production of elastomeric respirators\",\n    \"Best telemedicine practices, barriers and faciitators\",\n    \"Guidance on the simple things people can do at home to take care of sick people and manage disease\",\n    \"Oral medications that might potentially work\",\n    \"Use of AI in real-time health care delivery to evaluate interventions, risk factors and outcomes\",\n    \"Hospital flow and organization and workforce protection best practices\",\n    \"The natural history of disease to inform clinical care, public health interventions, infection prevention control, transmission, and clinical trials\",\n    \"Core clinical outcome set to maximize usability of data across a range of trials\",\n    \"Determine adjunctive and supportive interventions that can improve the clinical outcomes of infected patients (e.g. steroids, high flow oxygen)\"\n]\n\n\nNON_PHARMACEUTICAL_QUERIES = [\n    \"Guidance on ways to scale up NPIs in a more coordinated way (e.g., establish funding, infrastructure and authorities to support real time, authoritative (qualified participants) collaboration with all states to gain consensus on consistent guidance and to mobilize resources to geographic areas where critical shortfalls are identified) to give us time to enhance our health care delivery system capacity to respond to an increase in cases\",\n    \"Rapid design and execution of experiments to examine and compare NPIs currently being implemented. DHS Centers for Excellence could potentially be leveraged to conduct these experiments\",\n    \"Rapid assessment of the likely efficacy of school closures, travel bans, bans on mass gatherings of various sizes, and other social distancing approaches\",\n    \"Control the spread in communities\",\n    \"Models to predict costs and benefits that take account factors such as race, income, disability, age, geographic location, immigration status, housing status, employment status and health insurance status.\"\n    \"Policy changes necessary to enable the compliance of individuals with limited resources and the underserved with NPIs.\",\n    \"Why people fail to comply with public health advice\",\n    \"Economic impact of pandemic\"\n]\n\nDIAGNOSTICS_QUERIES = [\n    \"How widespread is the current Covid-19 exposure to make immediate policy recommendations on mitigation measures\",\n    \"Sampling methods to determine asymptotic cases, e.g. convalescent samples and early detection of disease such as screening of neutralizing antibodies, ELISAs\",\n    \"Efforts to increase capacity on existing diagnostic platforms and tap into existing surveillance platforms\",\n    \"Recruitment, support and coordination of local expertise and capacity\",\n    \"National guidance and guidelines about best practices to states\",\n    \"Development of point of care test (rapid influenza test) and rapid bed-side tests\",\n    \"Rapid design and execution of targeted surveillance experiments calling for all potential testers using PCR\",\n    \"Separation of assay development issues from instruments, and the role of the private sector to help quickly migrate assays onto those devices\",\n    \"Track down the evolution of the Corona virus (genetic drift or mutations) avoid locking into specific reagents and surveillance\/detection schemes\",\n    \"Latency issues and sufficient viral load to detect the pathogen and what is needed in terms of biological and environmental sampling\",\n    \"Use of diagnostics such as host response markers (e.g., cytokines) to detect early disease\",\n    \"Predict severe disease progression\",\n    \"Policies and protocols for screening and testing\",\n    \"Policies to mitigate the effects on supplies associated with mass testing, including swabs and reagents\",\n    \"Technology roadmap for Corona virus diagnostics\",\n    \"Scaling up new diagnostic tests, future coalition and accelerator models\",\n    \"New platforms and technology (CRISPR) to improve response times and employ more holistic approaches to COVID-19 and future diseases\",\n    \"Coupling genomics and diagnostic testing on a large scale\",\n    \"Enhance capabilities for rapid sequencing and bioinformatics to target regions of the genome that will allow specificity for a particular variant\",\n    \"Enhance capacity (people, technology, data) for sequencing with advanced analytics for unknown pathogens, and explore capabilities for distinguishing naturally-occurring pathogens from intentional\",\n    \"One Health surveillance of humans and potential sources of future spillover or ongoing exposure for this organism and future pathogens, including both evolutionary hosts (e.g., bats) and transmission hosts (e.g., heavily trafficked and farmed wildlife and domestic food and companion species), inclusive of environmental, demographic, and occupational risk factors\"\n]\n\nETHICAL_QUERIES = [\n    \"Ethical principles and standards to salient issues in COVID-2019\",\n    \"Embed ethics across all thematic areas, engage with novel ethical issues that arise and coordinate to minimize duplication of oversight\",\n    \"Support sustained education, access and capacity building in ethics\",\n    \"Establish a team at WHO that will be will be integrated within multidisciplinary research and operational platforms and that will connect with existing and expanded global networks of social sciences\",\n    \"Develop qualitative assessment frameworks to systematically collect information related to local barriers and enablers for the uptake and adherence to public health measures for prevention and control\",\n    \"Identify how the burden of responding to the outbreak and implementing public health measures affects the physical and psychological health of those providing care for Covid-19 patients and identify the immediate needs that must be addressed\",\n    \"Drivers of fear, anxiety and stigma that fuel misinformation and rumor, particularly through social media\"\n\n]\n\nINFORMATION_SHARING_QUERIES = [\n    \"Coordinate data gathering with standardized nomenclature\",\n    \"Share response information among planners and providers\",\n    \"Mitigating barriers to information sharing\",\n    \"Recruit, support and coorinate local (non-Federal) expertise and capacity relevant to public health emergency response\",\n    \"Integration of federal, state, local public health surveillance systems\",\n    \"Value of investments in baseline public health response infrastructure preparedness\",\n    \"Modes of communicating with target high-risk populations (elderly and health care workers)\",\n    \"Risk communication and guidelines\",\n    \"Communication that indicates potential risk of disease to all population groups\",\n    \"Misunderstanding around containment and mitigation\",\n    \"Mitigate gaps and problems of inequity in the Nation\u2019s public health capability, capacity, and funding to ensure all citizens in need are supported and can access information, surveillance, and treatment\",\n    \"Measures to reach marginalized and disadvantaged populations\",\n    \"Data systems and research priorities and agendas incorporate attention to the needs and circumstances of disadvantaged populations and underrepresented minorities\",\n    \"Mitigating threats to incarcerated people from COVID-19, assuring access to information, prevention, diagnosis, and treatment\",\n    \"Understanding coverage policies related to testing, treatment and care\"\n]\n","85fabaf2":"class QueryModel:\n\n    def __init__(self, lang='en'):\n        self.doc2vec = Doc2VecEmbedding(lang)\n        self.doc2vec.load()\n        self.searcher = DatasetsSearcher()\n\n    def query(self, query):\n        \"\"\"\n        Vectorizes a query through Doc2Vec and uses DatasetSearcher to find the research paper with the title\n        :param query: str\n        :return: list[list, list, list...]\n            A list of results with format: [label, similarity, title]\n        \"\"\"\n        res = []\n        sims = self.doc2vec.find_most_similar_docs(query)\n        for sim in sims:\n            label = sim[0]\n            res.append([label, sim[1], self.searcher.find_title_by_label(label)])\n        return res\n","4fb2ca8a":"QUERIES = [TRANSMISSION_QUERIES, RISK_FACTORS_QUERIES, VIRUS_GENETIC_QUERIES,\n           VACCINES_QUERIES, MEDICAL_CARE_QUERIES, NON_PHARMACEUTICAL_QUERIES, DIAGNOSTICS_QUERIES,\n           ETHICAL_QUERIES, INFORMATION_SHARING_QUERIES]\n\nQUERY_NAMES = ['transmission queries', 'risk factors queries', 'virus genetic queries', 'vaccines queries',\n               'medical care queries', 'non-pharmaceutical queries', 'diagnostics queries', 'ethical queries',\n               'information sharing queries']\n\nqm = QueryModel()\nfor query_list, query_name in zip(QUERIES, QUERY_NAMES):\n    print('\\n\\n' + query_name + '\\n\\n')\n    for query in query_list:\n        print_out = \"Query: {}\".format(query) + '\\n\\n'\n        hits = qm.query(query)\n        for res in hits:\n            print_out += '{}:; {} {:3f}'.format(res[0], res[2], res[1]) + '\\n'\n        print(print_out)","afe4fba9":"**Methodology**\n\nThis notebook contains a content based recommender system for COVID-19 related research questions (i.e. queries). This works by comparing a query to the body of research articles with which a vector space model (Doc2Vec-DBOW) has been trained, and the query will be compared against statements presented in research articles.\n\nA Doc2Vec-DBOW model is trained with tokenized research articles as document tokens, and their unique paper-id as the only training tag. These training examples are fit to a Doc2Vec-DBOW model which then vectorizes the research articles (their abstract and title). A query is constructed by tokenizing a string input and then vectorizing that list of tokens. Stopwords and punctuation is preserved because stopwords can influence the semantics of the query. Tokenization is done using SpaCy. No stemming or lemmatization is used. In case of English tokens are usually good enough but with morphologically rich languages that have conjugations and compound words (such as German or Finnish), lemmatization and compound word splitting may be needed.\n\nThe most similar results from all research papers in the Doc2Vec document embedding space are retrived by using cosine similarity to determine the most similar articles with the query. Cosine similar is a good choice because the length of the vector does not change it (i.e. short queries can be compared with abstracts and with lengthier bodies of text), and it is nicely bound between {-1, 1} (although in high dimensional vector spaces the lower threshold will approach 0 in practice). Also, cosine similarities tend to get lower with lengthier queries (because of the sheer probability of constructing a similar query going down with each new token added) but the order of results should still be reasonably relevant.\n\nThe most similar (in terms of document vector embedding cosine similarities) results are retrieved from the body of research papers and listed in descending order of similarity with their titles, similarities and paper IDs.\n\n**How to improve the approach**\n\nI opted to use Doc2Vec instead of more modern transfer learning methods (such as BERT) because in my experience working with a limited and specific domain needs a model that can be trained for that specific domain. Also, BERT does not have as good document level representation capabilities (yet) as Doc2Vec which has been created for the purpose of representing lengthier text data in vector space. ELMo could have been another viable approach to address the problem as sentence level statements rather than as whole abstracts or documents, i.e. it could do a better job at finding the exact statement. Also Doc2Vec can be further enhanced if additional tags are used. I have experienced significant improvement on Doc2Vec models with alternative tagging approaches. Since these are research papers, there are alternative tagging strategies available already using their metadata.\n\n**How to decipher the results**\n\nThere are no cosine similarity thresholds that are set to stone to determine what is a good match and what is not. The cosine similarity range is dependent on domain and the dimensionality of the vector space. As an example from my previous projects where I have modelled job ads with a similar approach, job ads that have very specified language (such as a specialist surgeon) tend to have much higher similarities than job ads that do not have specified language (e.g. a general secretary). Similarly with the COVID-19 dataset there are documents and queries related to general topics (e.g. social topics) that have lower similarities in general compared to more specific fields of research which uses more specialized language. Also lower similarity scores may indicate that that area is not researched yet throughout or that the query is bad.\n\nRepository related to this notebook: https:\/\/github.com\/jjlatval\/covid-19\n\n"}}