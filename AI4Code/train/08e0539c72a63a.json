{"cell_type":{"9696906a":"code","91744c3d":"code","f742bbe1":"code","135bb1d9":"code","37112669":"code","0f5cef5a":"code","313c0b9f":"code","18e29acf":"code","50e7bd4a":"code","501ba47d":"code","f8e2efad":"code","bbffe68e":"code","4ccc1923":"code","61cfd601":"code","5a758ad6":"code","111ecb0e":"code","f767ccb1":"code","2ce67abd":"code","5d4a93ce":"code","6c64c788":"code","ce4da96d":"code","4e261435":"code","83a3b593":"code","52073d04":"code","c9aad350":"code","e1d7891f":"code","3bb745ac":"code","9d8ab837":"code","a931de82":"code","9547ab90":"code","cc26cb97":"code","4d0af1ef":"code","587ec8f3":"code","b2227e82":"code","5ce98b28":"code","d6cc4303":"code","e1cbc68d":"code","70952390":"code","4897c6ac":"code","8c581ea2":"code","b7db1646":"code","c135d588":"code","13f9d5b3":"code","e80fc01e":"code","0dc2216a":"code","205ff530":"code","3e6f80ad":"code","c46848c4":"code","7d9df84b":"code","a8f89164":"code","4a4f1dde":"code","7e8edc3a":"code","456a2be3":"code","1df70c05":"code","5e03dd9a":"code","c20359b5":"code","77b5e53e":"code","ce02233c":"code","437b7074":"code","155a5c87":"code","08747d91":"code","3b310406":"code","52660196":"code","e02c073f":"code","b1096f94":"code","3592d653":"code","40543a8e":"code","9c2d2345":"code","67acd3d9":"code","0d58a369":"code","4457ee12":"code","cd2dea03":"code","3abe973d":"code","735aa4ad":"code","03952c7e":"code","48498c50":"code","e429e496":"code","4825695f":"code","0074bf9a":"code","a2c5f25f":"code","2471a816":"code","d6927dc8":"code","c6ce6b8c":"code","d4cb728c":"code","b5d8e5ee":"code","b6ac4385":"code","f1582d62":"code","ee8e7ad9":"code","2d402906":"code","755e2094":"markdown","f0b28d96":"markdown","778b0f3e":"markdown","b8aa9431":"markdown","63e9625d":"markdown","d34190c2":"markdown","370532e7":"markdown","fe048744":"markdown","6b220968":"markdown","be61fab0":"markdown","b0864dc5":"markdown","56b3961a":"markdown","c941b73a":"markdown","ba6a0b42":"markdown","d9019597":"markdown","02601870":"markdown","96dfc89c":"markdown","38f973b9":"markdown","70bfe73a":"markdown","648a510c":"markdown","d06ca50a":"markdown","40730857":"markdown","d4e4568d":"markdown","f25c85a0":"markdown","577fb1fa":"markdown","f236e509":"markdown","5f2be205":"markdown","cd34c918":"markdown","23f61b69":"markdown","3988fcae":"markdown","bf788e51":"markdown","28696862":"markdown","7236fc46":"markdown","5ffafaad":"markdown","b3b59b4f":"markdown","3010edb3":"markdown"},"source":{"9696906a":"!pip uninstall -y torch\n!conda install -y pytorch=0.3.1.0 cuda80 -c soumith\nimport torch\n!pip install fastai==0.7.0\n!pip uninstall -y torchtext\n!pip install torchtext==0.2.3","91744c3d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport gc\nimport datetime\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\n\n#https:\/\/www.kaggle.com\/bguberfain\/a-simple-model-using-the-market-and-news-data\nfrom itertools import chain\n\nfrom fastai.text import *\nimport html\n\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","f742bbe1":"(market_train_df, news_train_df) = env.get_training_data()","135bb1d9":"BOS = 'xbos'  # beginning-of-sentence tag\nFLD = 'xfld'  # data field tag\n\nPATH=Path('\/kaggle\/working')","37112669":"CLAS_PATH=Path('classifier\/')\nCLAS_PATH.mkdir(exist_ok=True)\n\nLM_PATH=Path('lang_model\/')\nLM_PATH.mkdir(exist_ok=True)","0f5cef5a":"toy = True\nif toy:\n    market_train_df = market_train_df.tail(4_000)\n    news_train_df = news_train_df.tail(12_000)\nelse:\n    market_train_df = market_train_df.tail(3_000_000)\n    news_train_df = news_train_df.tail(6_000_000)","313c0b9f":"# Split date into before and after 22h (the time used in train data)\n# E.g: 2007-03-07 23:26:39+00:00 -> 2007-03-08 00:00:00+00:00 (next day)\n#      2009-02-25 21:00:50+00:00 -> 2009-02-25 00:00:00+00:00 (current day)\nnews_train_df['time'] = (news_train_df['time'] - np.timedelta64(22,'h')).dt.ceil('1D')\n\n# Round time of market_train_df to 0h of curret day\nmarket_train_df['time'] = market_train_df['time'].dt.floor('1D')","18e29acf":"#function to combine dataframes. Stole and adapted from https:\/\/www.kaggle.com\/bguberfain\/a-simple-model-using-the-market-and-news-data\ndef join_market_news(market_train_df, news_train_df):\n    # Fix asset codes (str -> list)\n    news_train_df['assetCodes'] = news_train_df['assetCodes'].str.findall(f\"'([\\w\\.\/]+)'\")    \n    \n    #rename headline column to text for later\n    news_train_df = news_train_df.rename(columns={'headline':'text'})\n    \n    # Expand assetCodes\n    assetCodes_expanded = list(chain(*news_train_df['assetCodes']))\n    assetCodes_index = news_train_df.index.repeat( news_train_df['assetCodes'].apply(len) )\n    \n    assert len(assetCodes_index) == len(assetCodes_expanded)\n    df_assetCodes = pd.DataFrame({'level_0': assetCodes_index, 'assetCode': assetCodes_expanded})\n\n    # Create expanded news (will repeat every assetCodes' row)\n    news_cols = ['time', 'assetCodes', 'text']\n    news_train_df_aggregated = pd.merge(df_assetCodes, news_train_df[news_cols], left_on='level_0', right_index=True, suffixes=(['','_old']))\n\n    # Free memory\n    del news_train_df, df_assetCodes\n\n    # Flat columns\n    #news_train_df_aggregated.columns = ['_'.join(col).strip() for col in news_train_df_aggregated.columns.values]\n\n    # Join with train\n    market_train_df = pd.merge(market_train_df, news_train_df_aggregated, on=['time', 'assetCode'], how='inner')\n    #market_train_df = market_train_df.join(news_train_df_aggregated, on=['time', 'assetCode'])\n\n    # Free memory\n    del news_train_df_aggregated\n    \n    return market_train_df","50e7bd4a":"df = join_market_news(market_train_df, news_train_df)\ndf.reset_index(inplace=True)","501ba47d":"#collect free memory if any\ndel news_train_df\ndel market_train_df\ngc.collect()","f8e2efad":"#reduce file size from https:\/\/www.kaggle.com\/c\/two-sigma-financial-news\/discussion\/68265 and https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype.name\n        \n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else:\n            #df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\ndf = reduce_mem_usage(df)","bbffe68e":"CLASSES = ['neg', 'pos']","4ccc1923":"df['labels'] = df.returnsOpenNextMktres10.apply(lambda x: 1 if x>0 else 0)","61cfd601":"news = df[['labels', 'text']]","5a758ad6":"del df\ngc.collect()","111ecb0e":"#Creating train\/val set\ndf_trn, df_val = train_test_split(news, test_size=0.33, random_state=42)","f767ccb1":"df_trn.head(10)","2ce67abd":"df_trn.to_csv(CLAS_PATH\/'train.csv',header=False, index=False)\ndf_val.to_csv(CLAS_PATH\/'test.csv',header=False, index=False)\n(CLAS_PATH\/'classes.txt').open('w', encoding='utf-8').writelines(f'{o}\\n' for o in CLASSES)","5d4a93ce":"trn_texts,val_texts = sklearn.model_selection.train_test_split(\n    np.concatenate([df_trn.iloc[:,1],df_val.iloc[:,1]]), test_size=0.1)","6c64c788":"df_trn = pd.DataFrame({'text':trn_texts, 'labels':[0]*len(trn_texts)})\ndf_val = pd.DataFrame({'text':val_texts, 'labels':[0]*len(val_texts)})\n\ndf_trn.to_csv(LM_PATH\/'train.csv', header=False, index=False)\ndf_val.to_csv(LM_PATH\/'test.csv', header=False, index=False)","ce4da96d":"re1 = re.compile(r'  +')\n\ndef fixup(x):\n    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n        '<br \/>', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n    return re1.sub(' ', html.unescape(x))","4e261435":"def get_texts(df, n_lbls=1):\n    labels = df.iloc[:,range(n_lbls)].values.astype(np.int64)\n    texts = f'\\n{BOS} {FLD} 1 ' + df[n_lbls].astype(str)\n    for i in range(n_lbls+1, len(df.columns)): texts += f' {FLD} {i-n_lbls} ' + df[i].astype(str)\n    texts = list(texts.apply(fixup).values)\n\n    tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n    return tok, list(labels)","83a3b593":"def get_all(df, n_lbls):\n    tok, labels = [], []\n    for i, r in enumerate(df):\n        print(i)\n        tok_, labels_ = get_texts(r, n_lbls)\n        tok += tok_;\n        labels += labels_\n    return tok, labels","52073d04":"chunksize=24000","c9aad350":"df_trn = pd.read_csv(LM_PATH\/'train.csv', header=None, chunksize=chunksize)\ndf_val = pd.read_csv(LM_PATH\/'test.csv', header=None, chunksize=chunksize)","e1d7891f":"#tokenising\ntok_trn, trn_labels = get_all(df_trn, 0)\ntok_val, val_labels = get_all(df_val, 0)","3bb745ac":"(LM_PATH\/'tmp').mkdir(exist_ok=True)","9d8ab837":"np.save(LM_PATH\/'tmp'\/'tok_trn.npy', tok_trn)\nnp.save(LM_PATH\/'tmp'\/'tok_val.npy', tok_val)","a931de82":"tok_trn = np.load(LM_PATH\/'tmp'\/'tok_trn.npy')\ntok_val = np.load(LM_PATH\/'tmp'\/'tok_val.npy')","9547ab90":"freq = Counter(p for o in tok_trn for p in o)\nfreq.most_common(25)","cc26cb97":"max_vocab = 60000\nmin_freq = 2","4d0af1ef":"itos = [o for o,c in freq.most_common(max_vocab) if c>min_freq]\nitos.insert(0, '_pad_')\nitos.insert(0, '_unk_')","587ec8f3":"stoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\nlen(itos)","b2227e82":"trn_lm = np.array([[stoi[o] for o in p] for p in tok_trn])\nval_lm = np.array([[stoi[o] for o in p] for p in tok_val])","5ce98b28":"np.save(LM_PATH\/'tmp'\/'trn_ids.npy', trn_lm)\nnp.save(LM_PATH\/'tmp'\/'val_ids.npy', val_lm)\npickle.dump(itos, open(LM_PATH\/'tmp'\/'itos.pkl', 'wb'))","d6cc4303":"trn_lm = np.load(LM_PATH\/'tmp'\/'trn_ids.npy')\nval_lm = np.load(LM_PATH\/'tmp'\/'val_ids.npy')\nitos = pickle.load(open(LM_PATH\/'tmp'\/'itos.pkl', 'rb'))","e1cbc68d":"vs=len(itos)\nvs,len(trn_lm)","70952390":"! wget -nH -r -np -P {PATH} http:\/\/files.fast.ai\/models\/wt103\/bwd_wt103.h5\n! wget -nH -r -np -P {PATH} http:\/\/files.fast.ai\/models\/wt103\/bwd_wt103_enc.h5\n! wget -nH -r -np -P {PATH} http:\/\/files.fast.ai\/models\/wt103\/fwd_wt103.h5\n! wget -nH -r -np -P {PATH} http:\/\/files.fast.ai\/models\/wt103\/fwd_wt103_enc.h5\n! wget -nH -r -np -P {PATH} http:\/\/files.fast.ai\/models\/wt103\/itos_wt103.pkl","4897c6ac":"em_sz,nh,nl = 400,1150,3","8c581ea2":"PRE_PATH = PATH\/'models'\/'wt103'\nPRE_LM_PATH = PRE_PATH\/'fwd_wt103.h5'","b7db1646":"wgts = torch.load(PRE_LM_PATH, map_location=lambda storage, loc: storage)","c135d588":"enc_wgts = to_np(wgts['0.encoder.weight'])\nrow_m = enc_wgts.mean(0)","13f9d5b3":"itos2 = pickle.load((PRE_PATH\/'itos_wt103.pkl').open('rb'))\nstoi2 = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itos2)})","e80fc01e":"new_w = np.zeros((vs, em_sz), dtype=np.float32)\nfor i,w in enumerate(itos):\n    r = stoi2[w]\n    new_w[i] = enc_wgts[r] if r>=0 else row_m","0dc2216a":"wgts['0.encoder.weight'] = T(new_w)\nwgts['0.encoder_with_dropout.embed.weight'] = T(np.copy(new_w))\nwgts['1.decoder.weight'] = T(np.copy(new_w))","205ff530":"wd=1e-7\nbptt=70\nbs=10\nopt_fn = partial(optim.Adam, betas=(0.8, 0.99))","3e6f80ad":"trn_dl = LanguageModelLoader(np.concatenate(trn_lm), bs, bptt)\nval_dl = LanguageModelLoader(np.concatenate(val_lm), bs, bptt)\nmd = LanguageModelData(PATH, 1, vs, trn_dl, val_dl, bs=bs, bptt=bptt)","c46848c4":"drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.9","7d9df84b":"learner= md.get_model(opt_fn, em_sz, nh, nl, \n    dropouti=drops[0], dropout=drops[1], wdrop=drops[2], dropoute=drops[4], dropouth=drops[4])\n\nlearner.metrics = [accuracy]\nlearner.freeze_to(-1)","a8f89164":"learner.model.load_state_dict(wgts)","4a4f1dde":"lr=1e-3\nlrs = lr","7e8edc3a":"learner.fit(lrs\/2, 1, wds=wd, use_clr=(32,2), cycle_len=1)","456a2be3":"learner.save('lm_last_ft')\nlearner.load('lm_last_ft')\nlearner.unfreeze()","1df70c05":"# search for a learning rate, then run for 15 epoches\nlearner.lr_find(start_lr=lrs\/10, end_lr=lrs*10, linear=True)\nlearner.sched.plot()","5e03dd9a":"learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=15)","c20359b5":"#Saving the model\n#learner.save('lm1')\n\n#Saving the RNN encoder (rnn_enc)\nlearner.save_encoder('lm1_enc')\nlearner.sched.plot_loss()","77b5e53e":"df_trn = pd.read_csv(CLAS_PATH\/'train.csv', header=None, chunksize=chunksize)\ndf_val = pd.read_csv(CLAS_PATH\/'test.csv', header=None, chunksize=chunksize)","ce02233c":"#get tokens\ntok_trn, trn_labels = get_all(df_trn, 1)\ntok_val, val_labels = get_all(df_val, 1)","437b7074":"(CLAS_PATH\/'tmp').mkdir(exist_ok=True)\n\nnp.save(CLAS_PATH\/'tmp'\/'tok_trn.npy', tok_trn)\nnp.save(CLAS_PATH\/'tmp'\/'tok_val.npy', tok_val)\n\nnp.save(CLAS_PATH\/'tmp'\/'trn_labels.npy', trn_labels)\nnp.save(CLAS_PATH\/'tmp'\/'val_labels.npy', val_labels)","155a5c87":"tok_trn = np.load(CLAS_PATH\/'tmp'\/'tok_trn.npy')\ntok_val = np.load(CLAS_PATH\/'tmp'\/'tok_val.npy')","08747d91":"itos = pickle.load((LM_PATH\/'tmp'\/'itos.pkl').open('rb'))\nstoi = collections.defaultdict(lambda:0, {v:k for k,v in enumerate(itos)})\nlen(itos)","3b310406":"trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\nval_clas = np.array([[stoi[o] for o in p] for p in tok_val])","52660196":"np.save(CLAS_PATH\/'tmp'\/'trn_ids.npy', trn_clas)\nnp.save(CLAS_PATH\/'tmp'\/'val_ids.npy', val_clas)","e02c073f":"trn_clas = np.load(CLAS_PATH\/'tmp'\/'trn_ids.npy')\nval_clas = np.load(CLAS_PATH\/'tmp'\/'val_ids.npy')","b1096f94":"trn_labels = np.squeeze(np.load(CLAS_PATH\/'tmp'\/'trn_labels.npy'))\nval_labels = np.squeeze(np.load(CLAS_PATH\/'tmp'\/'val_labels.npy'))","3592d653":"bptt,em_sz,nh,nl = 70,400,1150,3\nvs = len(itos)\nopt_fn = partial(optim.Adam, betas=(0.8, 0.99))\nbs = 48","40543a8e":"min_lbl = trn_labels.min()\ntrn_labels -= min_lbl\nval_labels -= min_lbl\nc=int(trn_labels.max())+1","9c2d2345":"trn_ds = TextDataset(trn_clas, trn_labels)\nval_ds = TextDataset(val_clas, val_labels)\ntrn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), bs=bs\/\/2)\nval_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\ntrn_dl = DataLoader(trn_ds, bs\/\/2, transpose=True, num_workers=1, pad_idx=1, sampler=trn_samp)\nval_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, pad_idx=1, sampler=val_samp)\nmd = ModelData(PATH, trn_dl, val_dl)","67acd3d9":"# part 1\ndps = np.array([0.4, 0.5, 0.05, 0.3, 0.1])","0d58a369":"dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.5","4457ee12":"m = get_rnn_classifer(bptt, 20*70, c, vs, emb_sz=em_sz, n_hid=nh, n_layers=nl, pad_token=1,\n          layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n          dropouti=dps[0], wdrop=dps[1], dropoute=dps[2], dropouth=dps[3])","cd2dea03":"opt_fn = partial(optim.Adam, betas=(0.7, 0.99))","3abe973d":"learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\nlearn.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\nlearn.clip=.25\nlearn.metrics = [accuracy]","735aa4ad":"lr=3e-3\nlrm = 2.6\nlrs = np.array([lr\/(lrm**4), lr\/(lrm**3), lr\/(lrm**2), lr\/lrm, lr])","03952c7e":"lrs=np.array([1e-4,1e-4,1e-4,1e-3,1e-2])","48498c50":"wd = 1e-7\nwd = 0\nlearn.load_encoder('lm1_enc')","e429e496":"learn.freeze_to(-1)","4825695f":"learn.lr_find(lrs\/1000)\nlearn.sched.plot()","0074bf9a":"learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))","a2c5f25f":"learn.save('clas_0')","2471a816":"learn.load('clas_0')","d6927dc8":"learn.freeze_to(-2)","c6ce6b8c":"learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))","d4cb728c":"learn.save('clas_1')","b5d8e5ee":"learn.load('clas_1')","b6ac4385":"learn.unfreeze()","f1582d62":"learn.fit(lrs, 1, wds=wd, cycle_len=14, use_clr=(32,10))","ee8e7ad9":"learn.sched.plot_loss()","2d402906":"learn.save('clas_2')","755e2094":"ULMFiT involves 2 main types of models:\n1. Language Model (LM)\n1. Classifier Model (CLAS)\n\nWe first train a language model on a large generic language corpus like wikipedia, then use **discriminative differential fine-tuning** and **slanted triangular learning rates** on the news headlines dataset to fine tune the model. After that, we fit a 'linear classifier head' on top of the language model to do our classification.","f0b28d96":"itos stands for integer to string. It's essentially indexing each word in our vocabulary like a dictionary. Adding 2 more tokens to the dictionary, padding and unknown, i.e. new words","778b0f3e":"We save the encoder, not the entire model as that is what we will use as the 'backbone' to attach our classifier head","b8aa9431":"Helper functions clean up, grab the text and labels while adding the tags defined earlier and tokenises the text using spacy. Thanks to fastai, there is multi-processing which helps to speed things up.\n\nFor a quick explanation of tokenisation and other NLP terms, [spacy's docs site](https:\/\/spacy.io\/usage\/spacy-101#annotations-token) has great code snippets that you can run to see what each NLP term really means. ","63e9625d":"Average weight is assigned to tokens that are unknown to the imdb model. The model can then be fine tuned to its correct weight when training.","d34190c2":"Assigning mean weights to unknown tokens (-1)","370532e7":"#### Creating Classifier tokens\nLoading classifier data (with labels) and tokenising them. Steps are identical to when we were creating our bespoke language model.","fe048744":"The metric we keep track of is accuracy. In classification models, often times we use cross entropy loss as our metric. However, cross entropy loss encourages the model to be very confident in its classification. In our case, we really just care whether the model is correct or not, so accuracy is a better metric.","6b220968":"Assigning weights to encoders\/decoders in a tensor","be61fab0":"### Data Preparation\nWe will need to take the 2 dataframes and clean them up.","b0864dc5":"Creating binary labels, neg and pos. I kept it simple, with either 1 for positive returns and 0 for none\/negative returns.","56b3961a":"We use LanguageModelLoader to load our training and validation text concatenated together by bs and bptt and load it into our LanguageModelData object.","c941b73a":"Classifier model reads one headline at a time. Headlines are of varying lengths, so padding is required - this is done automagically in fastai. SortishSampler helps to create batches that are of similar length so padding is reduced.","ba6a0b42":"Showing top 25 most common words. Most of the tokens are stop words. Only interesting ones to me are research and $. In case you're wondering, t_up means the word after the tag is in caps.","d9019597":"Going the way round, mapping string to integer. As you can see, we don't have that many tokens.","02601870":"I've been taking the [fastai MOOC](https:\/\/course.fast.ai\/) and wanted to get my hands dirty implementing its Universal Language Model Fine-tuning for Text Classification (\"ULMFiT\")[1]. However,  it seems impossible to use ULMFiT without breaking the competition rules (no internet\/gpu). So instead of joining the competition, I thought I'd just use this as a learning experience.\n\nNote: I stole most of the code from the original fastai jupyter notebook [2]. If you are new to deep learning, I highly recommend checking out their [website](https:\/\/course.fast.ai\/).","96dfc89c":"Dropouts are placed in different layers - this is a featuer of the AWD-LSTM model. Jeremy recommends keeping the dropout ratios as follows, and tweaking the factor instead. The default is 0.7, but for smaller datasets, it's better to have a higher dropout to avoid overfitting. I choose 0.9 to test things out.","38f973b9":"Chunking helps to reduce the memory load as pandas will not read the entire file at once, but in chunks. Probably not very important our use case as I'm using a very small sample.","70bfe73a":"Running the learner for one epoch to train the embeddings in the last layer. Earlier on, we initialised unknown tokens with mean weights, so this will improve the embeddings for those new tokens.","648a510c":"### Creating our universal language model using wikitext103\n\nNow that we have gotten our data in the right shape and form, we are going to create our universal language model. fastai has made available the weights that are used in wikitext103 model, so if we download it and initialise the model with the weights, we don't need to train the model again.","d06ca50a":"We achieved pretty decent accuracy without doing anything different from the original notebook and using a really small subset of the data.","40730857":"#### Gradual Unfreezing\nTo prevent catastrophic forgetting, i.e. overfitting the classifier model, we unfreeze the last layer first, fine-tune that layer only, then proceed to fine-tune the one below it, until all layers are fine-tuned.","d4e4568d":"Assigning -1 to each token that is not in wikitext corpus using defaultdict","f25c85a0":"### References\n[1] Universal Language Model Fine-tuning for Text Classification, https:\/\/arxiv.org\/abs\/1801.06146\n\n[2] Fastai Lesson 10, https:\/\/github.com\/fastai\/fastai\/blob\/master\/courses\/dl2\/imdb.ipynb\n\n","577fb1fa":"**Definitions: **\n* wd = weight decay (similar idea to L2 regularisation)\n* bptt = backpropagation through time, i.e. how long a sequence of words we will feed the GPU at one time.\n* bs = batch size (Because of the small sample used, we lower the batch size.)\n* optn_fn = optimisation function, set as Adam. For RNN models, we need to lower the momentum (beta1) from it's default 0.9.","f236e509":"Limiting the maximum vocabulary we will allow in our model to 60,000 (reason explained in the [lesson notebook](https:\/\/github.com\/fastai\/fastai\/blob\/master\/courses\/dl2\/imdb.ipynb)) and the minimum frequency of a word to be considered in the vocabulary. ","5f2be205":"Aligning the time in both datasets for joining later.","cd34c918":"#### Concat pooling\nBecause signals may be present in different parts of the headline, we may lose information if we only consider the last hidden state. Hence, we combine the last hidden state, the max pooled and average representation of the hidden states.","23f61b69":"#### Creating Classifier Model\nThe steps taken are quite similar to creating a language model","3988fcae":"Loading back the same vocabulary so the index is the same.","bf788e51":"To match the weights of wiki103, our language model needs a embedding size of 400, 1150 hidden layers and 3 layers.","28696862":"#### Discriminative (differential) learning rates\nFor different layers, we apply different learning rates. For lower layers, we use lower learning rates - this idea is analogous to deep learning in computer vision, where lower layers contain more generic information hence not needing much fine tuning.\n\nThe general formula to decrease the learning rate is by dividing the learning rate of the previous layer by 2.6.","7236fc46":"**Slanted triangular learning rate**\n\nuse_clr implements slanted triangular learning rate, which essentially increases your learning rate quickly and slowly decays it over a longer period. Visually, it looks like a slanted triangle, hence the name. The first parameter is the ratio between the highest and the lowest learning rate; the second parameter is the ratio between the first peak and the last peak. ","5ffafaad":"Due to kernel limitations and lack of patience, I've shrunk the dataset to a really tiny subsample. ULMFiT is supposed to work even for really small datasets (100 labelled examples) - this is one of the breakthroughs of the paper [2]. I guess this can be another test to see if it works for news headlines as well.","b3b59b4f":"Loading weights in pytorch","3010edb3":"Defining tags for the language model. This allows the model to learn when a sentence starts and also data fields, if any."}}