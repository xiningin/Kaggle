{"cell_type":{"2ccc313f":"code","306589a7":"code","67d0dcba":"code","d5d126e7":"code","fd685f10":"code","fdbc8a2c":"code","9822df29":"code","62e8c037":"code","d5edd1fc":"code","3ada3b5e":"code","3757239e":"code","5a615014":"code","04df4a46":"code","0b9c3380":"code","21dbf634":"code","0f66efbb":"code","8171a0e0":"code","3a64c779":"code","4ee05304":"code","b36a6635":"code","b7bded1f":"code","80706ba7":"code","14d6fcad":"code","97c0452f":"code","fd14870b":"code","feb96f65":"code","0a30ca54":"code","3f30177e":"code","dec292bb":"code","8bb4508f":"code","8d282951":"code","253a37ed":"code","0e2a693e":"code","90f5874c":"code","30ec936f":"code","2b196805":"code","10992773":"code","32bc720b":"code","40e9e085":"markdown","1014d31c":"markdown","134aafde":"markdown","6d4d0956":"markdown","acd70df7":"markdown","43e1be32":"markdown","6f9cba04":"markdown","b407d1df":"markdown","132f5f74":"markdown","d8d5bc7d":"markdown","1067c974":"markdown","ffd7148b":"markdown","6b98f71f":"markdown","5d9ab3a2":"markdown","ce8e08d0":"markdown","fd1ab66e":"markdown","e3e38776":"markdown","8874e299":"markdown","6cc5b3d9":"markdown","98f6009f":"markdown","17236c7b":"markdown","ce90fd42":"markdown","3ebbc334":"markdown","537985e4":"markdown","93d90f74":"markdown","e84b8d30":"markdown","366eb5ea":"markdown","b22cd442":"markdown","c18dcdbf":"markdown","0565de82":"markdown","2c21b411":"markdown","58195355":"markdown","1ef9b72b":"markdown"},"source":{"2ccc313f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","306589a7":"import matplotlib.pyplot as plt, matplotlib.image as mpimg\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm, tree\n#%matplotlib inline","67d0dcba":"# load the data\nlabeled_images = pd.read_csv('..\/input\/train.csv')\nimages = labeled_images.iloc[0:5000,1:] # -> X\nlabels = labeled_images.iloc[0:5000,:1] # -> y\ntrain_images, test_images,train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=0)","d5d126e7":"labels.describe()\n#train_labels.describe()\n#test_labels.describe()","fd685f10":"i=1\n#print(train_images.iloc[i])\nimg=train_images.iloc[i].values\nimg=img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(train_labels.iloc[i,0])","fdbc8a2c":"a = np.unique(train_labels).tolist() # mencari label 0 - 9 pada train_labels dan mengubah tipe-nya menjadi list\n#print(a)\nb = train_labels.values.tolist() #mengubah type data train_label menjadi list\n#print(b)\narr = []\nfor k in b:\n    for j in k:\n       arr.append(j)\n\n#print(arr)\n#print(a)\nfor k in range(len(a)):\n    tmp = arr.index(a[k])\n    img=train_images.iloc[tmp].values\n    img=img.reshape((28,28))\n    print(\"Label:\", a[k])\n    plt.imshow(img,cmap='gray')\n    plt.title(train_labels.iloc[tmp,0])\n    plt.show()\n    #print(tmp)","9822df29":"print(\"Index:\", i)\n#print(train_images.iloc[i].values)\n#print(type(train_images.iloc[i]))\n#print(i)\nplt.title(i)\nplt.xlabel('panjang pixel')\nplt.ylabel('jumlah kolom')\nplt.hist(train_images.iloc[i])","62e8c037":"count_data = train_labels[\"label\"]\ncount_data.value_counts()","d5edd1fc":"#create histogram for each class (data merged per class)\n# Todo\n#print(train_labels.iloc[:5])\ndata1 = train_images.iloc[1]\ndata2 = train_images.iloc[3]\ndata1 = np.array(data1)\ndata2 = np.array(data2)\ndata3 = np.append(data1,data2)\n#print(len(data3))\nplt.title('Label 1 dan 3')\nplt.xlabel('panjang pixel')\nplt.ylabel('jumlah kolom')\nplt.hist(data3)","3ada3b5e":"for k in range(len(a)):\n    tmp = arr.index(a[k])\n    data = train_images.iloc[tmp]\n    data = np.array(data)\n    print(\"Label\",a[k])\n    print(\"Index\",tmp)\n    plt.title(a[k])\n    plt.xlabel('panjang pixel')\n    plt.ylabel('jumlah kolom')\n    plt.hist(data)\n    plt.show()","3757239e":"S_Data = []\nS_Data = np.array(S_Data)\n#print(type(S_Data))\n#print(S_Data)\nfor k in range(len(a)):\n    tmp = arr.index(a[k])\n    data = train_images.iloc[tmp]\n    data = np.array(data)\n    S_Data = np.append(S_Data, data)\n\n#membuat diagram semua kelas di gabungkan\nplt.title('Semua Kelas')\nplt.xlabel('panjang pixel')\nplt.ylabel('jumlah kolom')\nplt.hist(S_Data)\nplt.show()","5a615014":"clf = svm.SVC()\nclf.fit(train_images, train_labels.values.ravel())\nclf.score(test_images,test_labels)","04df4a46":"# Put your verification code here\n# Todo\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nmodel = DecisionTreeRegressor(random_state = 0)\nmodel.fit(train_images, train_labels)\nprediction = model.predict(test_images)\nmae = mean_absolute_error(prediction, test_labels)\n\nprint(train_labels.values.ravel())\nprint(np.unique(test_labels)) # to see class number\nprint(mae)","0b9c3380":"#print(\"Index:\", i)\ntest_images[test_images>0] = 1\ntrain_images[train_images>0] = 1\n\nimg=train_images.iloc[i].values.reshape((28,28))\nplt.imshow(img,cmap='binary')\nplt.title(train_labels.iloc[i])\n\n#print(train_images.iloc[i].values)","21dbf634":"print(\"Index\",i)\nplt.hist(train_images.iloc[i])","0f66efbb":"clf = svm.SVC()\nclf.fit(train_images, train_labels.values.ravel())\nclf.score(test_images,test_labels)","8171a0e0":"# Test again to data test\ntest_data=pd.read_csv('..\/input\/test.csv')\ntest_data[test_data>0]=1\nresults=clf.predict(test_data[0:5000])","3a64c779":"# separate code section to view the results\nprint(results)\nprint(len(results))","4ee05304":"# dump the results to 'results.csv'\ndf = pd.DataFrame(results)\ndf.index.name='ImageId'\ndf.index+=1\ndf.columns=['Label']\ndf.to_csv('results.csv', header=True)","b36a6635":"#check if the file created successfully\nprint(os.listdir(\".\"))","b7bded1f":"# from https:\/\/www.kaggle.com\/rtatman\/download-a-csv-file-from-a-kernel\n\n# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(df)","80706ba7":"from sklearn.model_selection import GridSearchCV\nkernels = ['poly', 'rbf', 'linear']\ngammas = [0.001, 0.0001]\nCs = [1, 10, 100]\n\ncandidate_parameters = {'kernel': kernels, 'gamma': gammas, 'C': Cs}\n\nclf = GridSearchCV(estimator=svm.SVC(), param_grid = candidate_parameters)\nclf.fit(train_images,train_labels.values.ravel())\nbest_kernel = clf.best_estimator_.kernel\nbest_gamma = clf.best_estimator_.gamma\nbest_C = clf.best_estimator_.C\n\nprint('Best Kernel:',best_kernel)\nprint('Best C:',best_C) \nprint('Best Gamma:',best_gamma)","14d6fcad":"psv = svm.SVC(C=best_C,kernel=best_kernel, gamma = best_gamma)\npsv.fit(train_images, train_labels.values.ravel())\nscores = psv.score(test_images,test_labels)\nprint(\"SVM:\",scores)","97c0452f":"from sklearn.tree import DecisionTreeClassifier","fd14870b":"def get_mae_train_classifier(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model_train = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state = 0)\n    model_train.fit(train_X, train_y)\n    predik_train = model_train.predict(train_X)\n    mae = mean_absolute_error(train_y, predik_train)\n    return(mae)","feb96f65":"def get_mae_test_classifier(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model_test = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state = 0)\n    model_test.fit(train_X, train_y)\n    predik_test = model_test.predict(val_X)\n    mae = mean_absolute_error(val_y, predik_test)\n    return(mae)","0a30ca54":"train_mae_classifier = []\ntest_mae_classifier = []\n\nbest_train_classifier = {}\nbest_test_classifier = {}\n#train_images, test_images,train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=0)\ncandidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500, 1000, 2500, 5000, 10000]\nfor max_leaf_nodes in candidate_max_leaf_nodes:\n    my_mae_train = get_mae_train_classifier(max_leaf_nodes = max_leaf_nodes, train_X = train_images, val_X = test_images, train_y = train_labels, val_y = test_labels)\n    train_mae_classifier.append(my_mae_train)\n    best_train_classifier.update({max_leaf_nodes : my_mae_train})\n    my_mae_test = get_mae_test_classifier(max_leaf_nodes = max_leaf_nodes, train_X = train_images, val_X = test_images, train_y = train_labels, val_y = test_labels)\n    test_mae_classifier.append(my_mae_test)\n    best_test_classifier.update({max_leaf_nodes : my_mae_test})\n\n    \nbest_tree_size_train_classifier  = min(best_train_classifier, key=best_train_classifier.get)\nbest_tree_size_test_classifier = min(best_test_classifier, key=best_test_classifier.get)\nprint(\"Best tree size for train\", best_tree_size_train_classifier)\nprint(\"Best tree size for test\", best_tree_size_test_classifier)\nprint(train_mae_classifier)\nprint(test_mae_classifier)\n","3f30177e":"plt.figure()\nplt.plot(candidate_max_leaf_nodes, train_mae_classifier, color=\"blue\")\nplt.plot(candidate_max_leaf_nodes, test_mae_classifier, color=\"red\")\nplt.xlabel(\"Tree Depth\")\nplt.ylabel(\"MAE\")\nplt.title(\"Decision Tree Classifier\")\nplt.show()","dec292bb":"CLS = DecisionTreeClassifier(max_leaf_nodes=best_tree_size_train_classifier, random_state=0)\nCLS.fit(train_images,train_labels)\nscoreCLS = CLS.score(test_images,test_labels)\nprint(\"Score DTC:\",scoreCLS)\nprint(\"Score SVM:\", scores)","8bb4508f":"#Decisiion Tree Regresor\ndef get_mae_train(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model_train = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model_train.fit(train_X, train_y)\n    predik_train = model_train.predict(train_X)\n    mae = mean_absolute_error(train_y, predik_train)\n    return(mae)","8d282951":"def get_mae_test(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model_test = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model_test.fit(train_X, train_y)\n    predik_test = model_test.predict(val_X)\n    mae = mean_absolute_error(val_y, predik_test)\n    return(mae)","253a37ed":"train_mae = []\ntest_mae = []\n\nbest_train = {}\nbest_test = {}\n#train_images, test_images,train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=0)\ncandidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500, 1000, 2500, 5000, 10000]\nfor max_leaf_nodes in candidate_max_leaf_nodes:\n    my_mae_train = get_mae_train(max_leaf_nodes = max_leaf_nodes, train_X = train_images, val_X = test_images, train_y = train_labels, val_y = test_labels)\n    train_mae.append(my_mae_train)\n    best_train.update({max_leaf_nodes : my_mae_train})\n    my_mae_test = get_mae_test(max_leaf_nodes = max_leaf_nodes, train_X = train_images, val_X = test_images, train_y = train_labels, val_y = test_labels)\n    test_mae.append(my_mae_test)\n    best_test.update({max_leaf_nodes : my_mae_test})\n\nbest_tree_size_train  = min(best_train, key=best_train.get)\nbest_tree_size_test = min(best_test, key=best_test.get)\nprint(\"Best tree size for train\", best_tree_size_train)\nprint(\"Best tree size for test\", best_tree_size_test)\nprint(train_mae)\nprint(test_mae)","0e2a693e":"plt.figure()\nplt.plot(candidate_max_leaf_nodes, train_mae, color=\"blue\")\nplt.plot(candidate_max_leaf_nodes, test_mae, color=\"red\")\nplt.xlabel(\"Tree Depth\")\nplt.ylabel(\"MAE\")\nplt.title(\"Decision Tree Regression\")\nplt.show()","90f5874c":"RGS = DecisionTreeRegressor(max_leaf_nodes=best_tree_size_train, random_state=0)\nRGS.fit(train_images,train_labels)\nscoreRGS = RGS.score(test_images,test_labels)\nprint(\"Score DTR:\",scoreRGS)\nprint(\"Score DTC:\",scoreCLS)\nprint(\"Score SVM:\",scores)","30ec936f":"# load the data\nlabeled_images = pd.read_csv('..\/input\/train.csv')\nimages = labeled_images.iloc[0:5000,1:] # -> X\nlabels = labeled_images.iloc[0:5000,:1] # -> y\ntrain_images, test_images,train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=0)","2b196805":"def get_mae_train_classifier(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model_train = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state = 0)\n    model_train.fit(train_X, train_y)\n    predik_train = model_train.predict(train_X)\n    mae = mean_absolute_error(train_y, predik_train)\n    return(mae)\n\ndef get_mae_test_classifier(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model_test = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state = 0)\n    model_test.fit(train_X, train_y)\n    predik_test = model_test.predict(val_X)\n    mae = mean_absolute_error(val_y, predik_test)\n    return(mae)","10992773":"train_mae_classifier = []\ntest_mae_classifier = []\n\nbest_train_classifier = {}\nbest_test_classifier = {}\n#train_images, test_images,train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=0)\ncandidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500, 1000, 2500, 5000, 10000]\nfor max_leaf_nodes in candidate_max_leaf_nodes:\n    my_mae_train = get_mae_train_classifier(max_leaf_nodes = max_leaf_nodes, train_X = train_images, val_X = test_images, train_y = train_labels, val_y = test_labels)\n    train_mae_classifier.append(my_mae_train)\n    best_train_classifier.update({max_leaf_nodes : my_mae_train})\n    my_mae_test = get_mae_test_classifier(max_leaf_nodes = max_leaf_nodes, train_X = train_images, val_X = test_images, train_y = train_labels, val_y = test_labels)\n    test_mae_classifier.append(my_mae_test)\n    best_test_classifier.update({max_leaf_nodes : my_mae_test})\n\n    \nbest_tree_size_train_classifier  = min(best_train_classifier, key=best_train_classifier.get)\nbest_tree_size_test_classifier = min(best_test_classifier, key=best_test_classifier.get)\nprint(\"Best tree size for train\", best_tree_size_train_classifier)\nprint(\"Best tree size for test\", best_tree_size_test_classifier)\nprint(train_mae_classifier)\nprint(test_mae_classifier)","32bc720b":"plt.figure()\nplt.plot(candidate_max_leaf_nodes, train_mae_classifier, color=\"blue\")\nplt.plot(candidate_max_leaf_nodes, test_mae_classifier, color=\"red\")\nplt.xlabel(\"Tree Depth\")\nplt.ylabel(\"MAE\")\nplt.title(\"Decision Tree Classifier (unnormalized dataset)\")\nplt.show()","40e9e085":"### Decisiion Tree Regresor","1014d31c":"## Q6\nAlhamdulillah, we have completed our experiment. Here's things to do for your next task:\n\n* What is the overfitting factor of SVM algorithm?. Previously on decision tree regression, the factor was max_leaf nodes. Do similar expriment using SVM by seeking SVM documentation!\n* Apply Decision Tree Classifier on this dataset, seek the best overfitting factor, then compare it with results of SVM.\n* Apply Decision Tree Regressor on this dataset, seek the best overfitting factor, then compare it with results of SVM & Decision Tree Classifier. Provides the results in table\/chart. I suspect they are basically the same thing.\n* Apply Decision Tree Classifier on the same dataset, use the best overfitting factor & value. But use the unnormalized dataset, before the value normalized to [0,1]\n","134aafde":"### Catatan:\nDi atas kita mendapatkan nlai score sebesar 0.1 dan nilai MAE sebesar 1.033\n* nilai score semakin mendekati 1 maka akurasi nya semakin bagus\n* nilai MAE semakin mendekati 0 maka akurasi nya semakin bagus","6d4d0956":"## Q4\nIn above, did you see score() function?, open SVM.score() dokumentation at SKLearn, what does it's role?. Does it the same as MAE discussed in class previously?. Ascertain it through running the MAE. Now does score() and mae() prooduce the same results?.","acd70df7":"## Q5\nBased on this finding, Can you explain why if the value is capped into [0,1] it improved the performance significantly?. Perharps you need to do several self designed test to see why.\n\n\ndengan mengubah nilai pixel 0 menjadi tetap 0 dan pixel di atas 0 menjadi 1, dengan ini kita bisa mendapatkan gambar yang lebih jelas dimana 0 sebagai putih dan 1 sebagai warna hitam","43e1be32":"## Retrain The Model\nUsing the now adjusted data, let's retrain our model to see the improvement","6f9cba04":"### Improving Performance\nDid you noticed, that the performance is so miniscule in range of ~0.1. Before doing any improvement, we need to analyze what are causes of the problem?. But allow me to reveal one such factor. It was due to pixel length in [0, 255]. Let's see if we capped it into [0,1] how the performance are going to improved.","b407d1df":"## Load Import\nIn case you haven't imported Digit Recognizer dataset from the competition, please do so. Then load the data with pandas. For simplicity we'll only load first 5000 train images then split them again into our personal train & test set for testing.","132f5f74":"### Catatan:\nHistogram di atas adalah banyak nya panjang pixel tiap kolom pada kelas label 1 dan 3","d8d5bc7d":"## Library Import\nFor starter import any machine libary we wanted to use. SKLearn is good choice for beginner, the question is what the algorithm we interested to test. Here's what we are going to need:\n\n   1. At least a classification algorithm (SVM or Decision Tree is a Good Choice)\n   2. Matplotlib\n   3. Preprocessing tools\n   4. Train test split And since we have been import numpy and panda no need to import them.\n\n","1067c974":"| No | Model          | Score  |\n| ---- |-------------| -----|\n| 1 | Decision Tree Regressor | 0.5863115887929868 |\n| 2 | Decision Tree Classifier |   0.779 |\n| 3 | SVM | 0.907 |\n\nTabel di atas merupakan tabel nilai score dari 3 model algoritma yang berbeda\n","ffd7148b":" ## Q1","6b98f71f":"### Catatan:\nKode di atas membuat histogram tentang banyaknya panjang pixels (0 - 255) dalam suatu index\n<Enter>contoh: panjang pixel 0 sebanyak lebih dari 600 dalam index ke - 1","5d9ab3a2":"Notice in the above we used _images.iloc?, can you confirm on the documentation? what is the role?****","ce8e08d0":"### Train The model\nNow we are ready to train the model, for starter let's use SVM. For the learning most model in SKLearn adopt the usual fit() and predict().","fd1ab66e":"### Catatan\nkode di atas membagi *features* dan *target* dari \"train.csv\"\n\nvaribale images menjadi *features* dengan membaca (dari fungsi .iloc[]) 5,000 baris pertama dan kolom ke-2 (index ke-1) dan dan seterusnya\n\nvariabel labels menjadi *target* dengan membaca (dari fungsi .iloc[]) 5,000 baris pertama dan kolom pertama (index ke-0)","e3e38776":"### Catatan:\nHistogram di atas menunjukkan banyak nya panjang pixel tiap kolom pada kelas label yang berbeda (0 - 9)","8874e299":"### Q3\nCan you check in what class does this histogram represent?. How many class are there in total for this digit data?. How about the histogram for other classes","6cc5b3d9":"### Catatan:\nMungkin anda akan menemukan tanda **catatan** seperti ini, ini akan menunjukan bagaimana kode di atas bekerja","98f6009f":"### Catatan:\nKode di atas menampilkan gambar-gambar sesuai banyak nya kelas label yaitu 0 - 9","17236c7b":"## Data Download\nWe have the file, can listed it but how we are take it from sever. Thus we also need to code the download link.","ce90fd42":"## Disclaimer\nThe data in this notebook is mostly copied from https:\/\/www.kaggle.com\/archaeocharlie\/a-beginner-s-approach-to-classification . I intended to do modification later to the tutorial, so please permit me for using it.","3ebbc334":"faktor yang mempengaruhi overfitting pada algoritma SVM adalah , C, kernel, dan gamma. \n\ndengan menggunakan GridSearchCV kita dapat mencari C, kernel, atau gamma terbaik dari beberapa kandidat","537985e4":"### Catatan:\nKode di atas untuk menunjukkan jumlah label\n<Enter>contoh: Kelas label 1 memiliki jumlah 458","93d90f74":"### Prediction labelling\nIn Kaggle competition, we don't usually submit the end test data performance on Kaggle. But what to be submitted is CSV of the prediction label stored in a file.","e84b8d30":"### SVM","366eb5ea":"### Decisiion Tree Classifier","b22cd442":"### Catatan:\nHistogram di atas adalah gabungan dari setiap histogram setiap kelas label","c18dcdbf":"### Catatan:\nkode di atas hanyalah 'import' biasam untuk mengimport library yang di butuh kan","0565de82":"Now plot the histogram within img","2c21b411":"kode diatan menjalankan .loc[], dimana ini berguna untuk mengambil index tertentu dari suatu baris dan kolom\n\nberikut Synyax-nya\n> pandas.DataFrame.iloc\n\nparameter di dalam .iloc terdapat *rows* (baris) dan *columns* (kolom)\n.iloc[*rows*, *columns*]","58195355":"## Q2\nNow plot an image for each image class","1ef9b72b":"### Decision Tree Clasifier (unnormalized dataset)"}}