{"cell_type":{"9ee14d52":"code","392c91a4":"code","3409acb1":"code","a5fda0c7":"code","3b73f59a":"code","d7378b1e":"code","898b13de":"code","fcb8a43e":"code","ecc94256":"code","42cb01c7":"code","e1d7fc07":"code","ec181f6f":"code","b41fd818":"code","ed32ea9b":"code","e44f1216":"code","6469586e":"code","1d731447":"code","a3bfd294":"code","bdeb6176":"code","631e0c0c":"code","54d6ee35":"code","5a173dbf":"code","5ac614be":"code","e0907c16":"code","a1cb27a0":"code","8535c72d":"code","d7ad4ce4":"code","d5db5a00":"code","be66cd7b":"code","16f0d4c0":"code","a9113291":"code","034b4a93":"code","d0c00b48":"code","a5faa663":"markdown","8c12144d":"markdown","66702055":"markdown","2add638e":"markdown","21a27917":"markdown","700ab76f":"markdown","b3d1425c":"markdown"},"source":{"9ee14d52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","392c91a4":"train_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","3409acb1":"train_df.head()","a5fda0c7":"train_df.info()","3b73f59a":"train_df.isnull().sum()","d7378b1e":"object_cols_train = train_df.select_dtypes(\"object\").columns\nprint(object_cols_train)","898b13de":"test_df.info()","fcb8a43e":"test_df.isnull().sum()","ecc94256":"object_cols_test = test_df.select_dtypes(\"object\").columns\nprint(object_cols_test)","42cb01c7":"import seaborn as sns #data visualization is more grafical and user friendly\nimport matplotlib.pyplot as plt #data visualization\n\n#see if the our target is balances\nsns.countplot(x=\"Survived\", data=train_df)","e1d7fc07":"sns.countplot(x=\"Survived\", data=train_df, hue=\"Sex\")","ec181f6f":"#Correlation of all features\nplt.figure(figsize=(12, 8))\nplt.title('Titanic Correlation of Features', y=1.05, size=15)\nsns.heatmap(train_df.corr().abs(), linewidths=0.1, vmax=1.0, square=True, linecolor='white', annot=True)","b41fd818":"#correlation: if the correlation of 2 features are too high it means it is redundant\ntrain_df.corr().abs()","ed32ea9b":"#correlation based on one parameter only\ntrain_df.corr().Survived.abs().sort_values()","e44f1216":"#distriuation of all columns based on each other\nsns.pairplot(train_df)","6469586e":"#distribution plot\nsns.distplot(train_df['Age'], bins=24, color='b')","1d731447":"#Because Cabin has alot of null drop it\n#Name PassengerId are non related so drop it\n#default is axis = 0 which mean row wise but we want column wise\ntrain_df_v2 = train_df.drop(['PassengerId', 'Name', 'Cabin'], axis = 1)\n\n#fillna: We can also propagate non-null values forward or backward.\ntrain_df_v2.Embarked.fillna(method='ffill', inplace=True)\ntrain_df_v2.isnull().sum()","a3bfd294":"#Age and Pclass are correlated so we estimate the missing Age based on Pclass\ndef predict_age(row_age_pclass):\n    age = row_age_pclass[0]\n    pclass = row_age_pclass[1]\n    if pd.isnull(age):\n        if pclass == 1:\n            return 38\n        elif pclass == 2:\n            return 29\n        else:\n            return 25\n    else:\n        return age\n    \n#apply get a column and apply a change row by row\ntrain_df_v3 = train_df_v2.copy()\ntrain_df_v3['Age'] = train_df_v3[['Age', 'Pclass']].apply(predict_age, axis=1) \ntrain_df_v3.isnull().sum()","bdeb6176":"#converting categorical features to numerical\n#Encode target labels with value between 0 and (n_classes-1)\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nobject_cols_train = train_df_v3.select_dtypes('object').columns\ntrain_df_v4 = train_df_v3.copy()\ntrain_df_v4[object_cols_train] = train_df_v4[object_cols_train].apply(le.fit_transform) \ntrain_df_v4.info()","631e0c0c":"train_df_v4.corr().Survived.abs().sort_values()","54d6ee35":"test_df_v2 = test_df.drop(['Name', 'Cabin'], axis = 1)\n\n#fillna: We can also propagate non-null values forward or backward.\ntest_df_v2.Fare.fillna(method='ffill', inplace=True)\n#apply get a column and apply a change row by row\ntest_df_v3 = test_df_v2.copy()\ntest_df_v3['Age'] = test_df_v3[['Age', 'Pclass']].apply(predict_age, axis=1) \ntest_df_v3.isnull().sum()","5a173dbf":"#converting categorical features to numerical\n#Encode target labels with value between 0 and (n_classes-1)\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nobject_cols_test = test_df_v3.select_dtypes('object').columns\ntest_df_v4 = test_df_v3.copy()\ntest_df_v4[object_cols_test] = test_df_v4[object_cols_test].apply(le.fit_transform) \ntest_df_v4.info()","5ac614be":"X = train_df_v4.drop('Survived', axis=1)\ny = train_df_v4.Survived\n#Scaling: standardize features by removing the mean and scaling to unit variance\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nscale.fit(X)\nX = scale.transform(X)","e0907c16":"from sklearn.model_selection import train_test_split\n#split the training data t0 70%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","a1cb27a0":"# Disabling warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nfrom sklearn.linear_model import LogisticRegression\nlogmodel = LogisticRegression()\n#train\nlogmodel.fit(X_train, y_train)\n#predict\nlogmodel_predict = logmodel.predict(X_test)\n#evaluate\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, logmodel_predict))\nprint(confusion_matrix(y_test, logmodel_predict))","8535c72d":"#cross validation\nfrom sklearn.model_selection import cross_val_score\nCVS = cross_val_score(logmodel, X, y, scoring='accuracy', cv=5)\nprint(CVS)\nprint(\"\\nMean accuracy of cross-validation: \", CVS.mean())","d7ad4ce4":"from sklearn.ensemble import RandomForestClassifier\n#train\nrandmodel = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=1)\nrandmodel.fit(X_train, y_train)\n#predict\nrandmodel_predict = randmodel.predict(X_test)\n#evaluate\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, randmodel_predict))\nprint(confusion_matrix(y_test, randmodel_predict))","d5db5a00":"#cross validation\nfrom sklearn.model_selection import cross_val_score\nCVS = cross_val_score(randmodel, X, y, scoring='accuracy', cv=5)\nprint(CVS)\nprint(\"\\nMean accuracy of cross-validation: \", CVS.mean())","be66cd7b":"X_train_nn = X_train.values\ny_train_nn = y_train.values\nX_test_nn = X_test.values\ny_test_nn = y_test.values","16f0d4c0":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout\nnn_model = Sequential()\n#add layer\nnn_model.add(Dense(units=8, activation='relu')) #unit is the # nodes which is roughly # of features\nnn_model.add(Dropout(0.5)) #prevent NN from overfitting by disabling 50% of the activation nodes\n#add layer\nnn_model.add(Dense(units=4, activation='relu'))\nnn_model.add(Dropout(0.5)) \n#add layer (final layer)\nnn_model.add(Dense(units=1, activation='sigmoid'))\nnn_model.compile(loss='binary_crossentropy', optimizer='adam')\n#if model not working stop before the iterations (epochs)\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)\n#train\nnn_model.fit(x=X_train_nn, y=y_train_nn, epochs=200, validation_data=(X_test_nn, y_test_nn), verbose=1, callbacks=[early_stop])","a9113291":"nn_model_loss = pd.DataFrame(nn_model.history.history)\nnn_model_loss.plot()","034b4a93":"#predict\nnn_model_predict = nn_model.predict_classes(X_test_nn)\n#evaluate\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test_nn, nn_model_predict))\nprint(confusion_matrix(y_test_nn, nn_model_predict))","d0c00b48":"from sklearn.svm import SVC\nsvc_model = SVC()\n#train\nsvc_model.fit(X_train, y_train)\n#predict\nsvc_predict = svc_model.predict(X_test)\n#evaluate\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test, svc_predict))\nprint(confusion_matrix(y_test, svc_predict))","a5faa663":"**Neural Network**","8c12144d":"**Pipeline**","66702055":"**Random Forest**","2add638e":"**Split the data**","21a27917":"**SVC**","700ab76f":"**Cleaning the Test Data**","b3d1425c":"**Logistic Regression**"}}