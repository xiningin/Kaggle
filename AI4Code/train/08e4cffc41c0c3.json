{"cell_type":{"8f6905cf":"code","025ff4c0":"code","92aa019f":"code","8464ffbc":"code","4c77cbe3":"code","fd29dea1":"code","d6a751c2":"code","e3068677":"code","96ebcca7":"code","6906ed36":"code","40067d44":"code","83b92cb3":"code","61f515be":"code","573de818":"code","e6c12a59":"code","a81767f0":"code","4cc7eaf2":"code","e20bc4d8":"code","69e436ad":"code","ddb75b10":"code","e3c1ca19":"code","531b9857":"code","32287761":"code","1e1ca269":"code","bf8870cf":"code","0b50e9e7":"code","ac32d35f":"code","0e4bfba9":"code","7a1c70c8":"code","2f01d026":"code","cac69933":"code","dedc97ae":"code","6bec0a12":"code","cff06c0f":"code","902608d9":"code","3a78b2c4":"code","caba77e9":"code","649c67e0":"code","05199dab":"code","49774258":"code","89dfa896":"code","5b3ad3ff":"code","9c8c0a48":"code","f1be6678":"code","88a2997e":"code","acf21d76":"code","07a0fd39":"code","aa4f8426":"code","21a904eb":"code","fc8cd28d":"code","4709aab7":"code","42d62d21":"code","8687eaf1":"code","913da1d3":"code","cad8f3d5":"markdown","0d32db12":"markdown","88142c7d":"markdown","c575a4b6":"markdown","95706f3f":"markdown","21ba017f":"markdown","c2002ed0":"markdown","9f547d3b":"markdown","e7f0b116":"markdown","6f47dc6d":"markdown","49295dec":"markdown","aa99604b":"markdown","5a51ffe3":"markdown","9b0df79b":"markdown","a57574b4":"markdown","2d4dced5":"markdown","6ddea587":"markdown","f3bfc138":"markdown","47ccc78b":"markdown","cf6ce380":"markdown","0ebd0cb4":"markdown","bb5d7190":"markdown","40e3a48c":"markdown","f0d132a4":"markdown","e7624ef1":"markdown","8869daeb":"markdown","7c5e787b":"markdown","36bc1dec":"markdown"},"source":{"8f6905cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\n#load packages\nimport sys #access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport numpy as np # linear algebra\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\n#misc libraries\nimport random\nimport time\n\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","025ff4c0":"from pandas import Series,DataFrame\nfrom matplotlib import style\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\n# machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n\ntrain_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain_data.head(5)","92aa019f":"train_data.shape # 891","8464ffbc":"test_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest_data.head(5)","4c77cbe3":"test_data.shape # 418","fd29dea1":"# train_data.info()\n# train_data.describe()\n# total = train_data.isnull().sum().sort_values(ascending=False)\n# percent_1 = train_data.isnull().sum()\/train_data.isnull().count()*100\n# percent_2 = (round(percent_1, 1)).sort_values(ascending=False)\n# missing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\n# missing_data\n\nprint(pd.isnull(train_data).sum())","d6a751c2":"print(pd.isnull(test_data).sum())","e3068677":"# drop unnecessary columns, these columns won't be useful in analysis and prediction\ntrain_data = train_data.drop(['PassengerId','Name','Ticket'],axis=1)\ntrain_data.head()\ntest_data = test_data.drop(['Name','Ticket'],axis=1)\ntest_data.tail()","96ebcca7":"# visualization\n\n#data = [train_data, test_data]\n#for dataset in data:\n#    dataset['relatives'] = dataset['SibSp'] + dataset['Parch']\n#    dataset.loc[dataset['relatives'] > 0, 'not_alone'] = 0\n#    dataset.loc[dataset['relatives'] == 0, 'not_alone'] = 1\n#    dataset['not_alone'] = dataset['not_alone'].astype(int)\n#train_data['not_alone'].value_counts()","6906ed36":"# visualization\nFacetGrid = sns.FacetGrid(train_data, row='Embarked', height=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","40067d44":"# only in train data, fill the two missing values with the most occurred value, which is \"S\".\ntrain_data[\"Embarked\"] = train_data[\"Embarked\"].fillna(\"S\")","83b92cb3":"# plot\nsns.factorplot('Embarked','Survived', data=train_data,size=4,aspect=3)\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(15,5))\nsns.countplot(x='Embarked', data=train_data, ax=axis1)\nsns.countplot(x='Survived', hue=\"Embarked\", data=train_data, order=[1,0], ax=axis2)\n# group by embarked, and get the mean for survived passengers for each value in Embarked\nembark_perc = train_data[[\"Embarked\", \"Survived\"]].groupby(['Embarked'],as_index=False).mean()\nsns.barplot(x='Embarked', y='Survived', data=embark_perc,order=['S','C','Q'],ax=axis3)\n# Either to consider Embarked column in predictions,\n# and remove \"S\" dummy variable, \n# and leave \"C\" & \"Q\", since they seem to have a good rate for Survival.\n# OR, don't create dummy variables for Embarked column, just drop it, \n# because logically, Embarked doesn't seem to be useful in prediction.\nembark_dummies_titanic  = pd.get_dummies(train_data['Embarked'])\nembark_dummies_titanic.drop(['S'], axis=1, inplace=True)\nembark_dummies_test  = pd.get_dummies(test_data['Embarked'])\nembark_dummies_test.drop(['S'], axis=1, inplace=True)\ntrain_data = train_data.join(embark_dummies_titanic)\ntest_data = test_data.join(embark_dummies_test)\ntrain_data.drop(['Embarked'], axis=1,inplace=True)\ntest_data.drop(['Embarked'], axis=1,inplace=True)","61f515be":"# only for test data, since there is a missing \"Fare\" values\ntest_data[\"Fare\"].fillna(test_data[\"Fare\"].median(), inplace=True)","573de818":"# convert from float to int\ntrain_data['Fare'] = train_data['Fare'].astype(int)\ntest_data['Fare'] = test_data['Fare'].astype(int)\n\n# get fare for survived & didn't survive passengers \nfare_not_survived = train_data[\"Fare\"][train_data[\"Survived\"] == 0]\nfare_survived = train_data[\"Fare\"][train_data[\"Survived\"] == 1]\n\n# get average and std for fare of survived\/not survived passengers\navgerage_fare = DataFrame([fare_not_survived.mean(), fare_survived.mean()])\nstd_fare = DataFrame([fare_not_survived.std(), fare_survived.std()])\n\n# plot\ntrain_data['Fare'].plot(kind='hist', figsize=(15,3),bins=100, xlim=(0,50))\n\navgerage_fare.index.names = std_fare.index.names = [\"Survived\"]\navgerage_fare.plot(yerr=std_fare,kind='bar',legend=False)","e6c12a59":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Original Age values - Titanic')\naxis2.set_title('New Age values - Titanic')\n\n# get average, std, and number of NaN values in train data\navg_age_train = train_data[\"Age\"].mean()\nstd_age_train = train_data[\"Age\"].std()\ncount_nan_age_train = train_data[\"Age\"].isnull().sum()\n\n# get average, std, and number of NaN values in test data\navg_age_test = test_data[\"Age\"].mean()\nstd_age_test = test_data[\"Age\"].std()\ncount_nan_age_test = test_data[\"Age\"].isnull().sum()\n\n# generate random numbers between (mean - std) & (mean + std)\nrand_1 = np.random.randint(avg_age_train - std_age_train, avg_age_train + std_age_train, size = count_nan_age_train)\nrand_2 = np.random.randint(avg_age_test - std_age_test, avg_age_test + std_age_test, size = count_nan_age_test)\n\n# plot original Age values\n# NOTE: drop all null values, and convert to int\ntrain_data['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n# test_data['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n\n# fill NaN values in Age column with random values generated\ntrain_data[\"Age\"][np.isnan(train_data[\"Age\"])] = rand_1\ntest_data[\"Age\"][np.isnan(test_data[\"Age\"])] = rand_2\n\n# convert from float to int\ntrain_data['Age'] = train_data['Age'].astype(int)\ntest_data['Age'] = test_data['Age'].astype(int)\n\n# plot new Age Values\ntrain_data['Age'].hist(bins=70, ax=axis2)\n# test_data['Age'].hist(bins=70, ax=axis4)","a81767f0":"# .... continue with plot Age column\n\n# peaks for survived\/not survived passengers by their age\nfacet = sns.FacetGrid(train_data, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, train_data['Age'].max()))\nfacet.add_legend()\n\n# average survived passengers by age\nfig, axis1 = plt.subplots(1,1,figsize=(18,4))\naverage_age = train_data[[\"Age\", \"Survived\"]].groupby(['Age'],as_index=False).mean()\nsns.barplot(x='Age', y='Survived', data=average_age)","4cc7eaf2":"train_data.drop(['Cabin'],axis=1,inplace=True)\ntrain_data.head()\ntest_data.drop(['Cabin'],axis=1,inplace=True)\ntest_data.head()","e20bc4d8":"# Family\n\n# Instead of having two columns Parch & SibSp, \n# we can have only one column represent if the passenger had any family member aboard or not,\n# Meaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.\ntrain_data['Family'] =  train_data[\"Parch\"] + train_data[\"SibSp\"]\ntrain_data['Family'].loc[train_data['Family'] > 0] = 1\ntrain_data['Family'].loc[train_data['Family'] == 0] = 0\n\ntest_data['Family'] = test_data[\"Parch\"] + test_data[\"SibSp\"]\ntest_data['Family'].loc[test_data['Family'] > 0] = 1\ntest_data['Family'].loc[test_data['Family'] == 0] = 0\n\n# drop Parch & SibSp\ntrain_data = train_data.drop(['SibSp','Parch'], axis=1)\ntest_data = test_data.drop(['SibSp','Parch'], axis=1)\n\n# plot\nfig, (axis1,axis2) = plt.subplots(1,2,sharex=True,figsize=(10,5))\nsns.countplot(x='Family', data=train_data, order=[1,0], ax=axis1)\n\n# average of survived for those who had\/didn't have any family member\nfamily_perc = train_data[[\"Family\", \"Survived\"]].groupby(['Family'],as_index=False).mean()\nsns.barplot(x='Family', y='Survived', data=family_perc, order=[1,0], ax=axis2)\n\naxis1.set_xticklabels([\"With Family\",\"Alone\"], rotation=0)","69e436ad":"# visualization\nsurvived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_data[train_data['Sex']=='female']\nmen = train_data[train_data['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde = False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde = False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend() \nax.set_title('Male')","ddb75b10":"# % of women who survived: train\nwomen = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nif len(women) > 0 :\n    rate_women = sum(women)\/len(women)\nprint(\"% of women who survived:\", rate_women)\n\n# % of men who survived: train\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = 0\nif len(men) > 0 :\n    rate_men = sum(men)\/len(men)\nprint(\"% of men who survived:\", rate_men)","e3c1ca19":"sns.barplot(x='Sex', y='Survived', data=train_data)","531b9857":"# Sex\n\n# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.\n# So, we can classify passengers as males, females, and child\ndef get_person(passenger):\n    age,sex = passenger\n    return 'child' if age < 16 else sex\n    \ntrain_data['Person'] = train_data[['Age','Sex']].apply(get_person,axis=1)\ntest_data['Person'] = test_data[['Age','Sex']].apply(get_person,axis=1)\n\n# No need to use Sex column since we created Person column\ntrain_data.drop(['Sex'],axis=1,inplace=True)\ntest_data.drop(['Sex'],axis=1,inplace=True)\n\n# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers\nperson_dummies_titanic = pd.get_dummies(train_data['Person'])\nperson_dummies_titanic.columns = ['Child','Female','Male']\nperson_dummies_titanic.drop(['Male'], axis=1, inplace=True)\n\nperson_dummies_test = pd.get_dummies(test_data['Person'])\nperson_dummies_test.columns = ['Child','Female','Male']\nperson_dummies_test.drop(['Male'], axis=1, inplace=True)\n\ntrain_data = train_data.join(person_dummies_titanic)\ntest_data    = test_data.join(person_dummies_test)\n\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,5))\n\nsns.countplot(x='Person', data=train_data, ax=axis1)\n\n# average of survived for each Person(male, female, or child)\nperson_perc = train_data[[\"Person\", \"Survived\"]].groupby(['Person'],as_index=False).mean()\nsns.barplot(x='Person', y='Survived', data=person_perc, ax=axis2, order=['male','female','child'])\n\ntrain_data.drop(['Person'],axis=1,inplace=True)\ntest_data.drop(['Person'],axis=1,inplace=True)","32287761":"# axes = sns.factorplot(x='relatives',y='Survived',hue='Sex', data=train_data, aspect = 2.5 )","1e1ca269":"# axes = sns.catplot(x='relatives',y='Survived',hue='Sex',data=train_data, aspect = 2.5 )","bf8870cf":"# train_data['Sex'].replace(['female','male'],[0,1],inplace=True)\n# test_data['Sex'].replace(['female','male'],[0,1],inplace=True)\n\n# train_data['Embarked'].replace(['Q','S','C'],[0,1,2],inplace=True)\n# test_data['Embarked'].replace(['Q','S','C'],[0,1,2],inplace=True)","0b50e9e7":"grid = sns.FacetGrid(train_data, col='Survived', row='Pclass', height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","ac32d35f":"sns.barplot(x='Pclass', y='Survived', data=train_data)","0e4bfba9":"# Pclass\n\n# sns.factorplot('Pclass',data=train_data,kind='count',order=[1,2,3])\nsns.factorplot('Pclass','Survived',order=[1,2,3], data=train_data,size=5)\n\n# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers\npclass_dummies_titanic  = pd.get_dummies(train_data['Pclass'])\npclass_dummies_titanic.columns = ['Class_1','Class_2','Class_3']\npclass_dummies_titanic.drop(['Class_3'], axis=1, inplace=True)\n\npclass_dummies_test  = pd.get_dummies(test_data['Pclass'])\npclass_dummies_test.columns = ['Class_1','Class_2','Class_3']\npclass_dummies_test.drop(['Class_3'], axis=1, inplace=True)\n\ntrain_data.drop(['Pclass'],axis=1,inplace=True)\ntest_data.drop(['Pclass'],axis=1,inplace=True)\n\ntrain_data = train_data.join(pclass_dummies_titanic)\ntest_data    = test_data.join(pclass_dummies_test)","7a1c70c8":"print(pd.isnull(train_data).sum())","2f01d026":"print(pd.isnull(test_data).sum())","cac69933":"# bins = [0,8,15,18,25,40,60,100]\n# names = ['1','2','3','4','5','6','7']\n# train_data['Age'] = pd.cut(train_data['Age'],bins,labels=names)\n# test_data['Age'] = pd.cut(test_data['Age'],bins,labels=names)","dedc97ae":"print(train_data.shape)","6bec0a12":"print(test_data.shape)","cff06c0f":"print(train_data.head())","902608d9":"print(test_data.head())","3a78b2c4":"train_data.head()","caba77e9":"test_data.head()","649c67e0":"# train_test_split\nX = np.array(train_data.drop(['Survived'],1))\ny = np.array(train_data['Survived'])\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\nids = test_data.PassengerId","05199dab":"model = LogisticRegression(max_iter=1000)\nmodel.fit(X_train, y_train)\nscore_logreg = model.score(X_train, y_train)\nprint('Logistic Regression score:', score_logreg)\nscore_logreg = model.score(X_test, y_test)\nprint('Logistic Regression score:', score_logreg)\nY_pred = model.predict(test_data.drop('PassengerId',axis=1)) # X_test\nout_logreg = pd.DataFrame({'PassengerId':ids,'Survived':Y_pred})\nprint(out_logreg.head())\npeople = out_logreg.loc[out_logreg.Survived == 1][\"Survived\"]\nrate_people = 0\nif len(people) > 0 :\n    rate_people = len(people)\/len(out_logreg)\nprint(\"Logistic Regression % of people who survived:\", rate_people)","49774258":"yes = len(people) # len(df[df['target'] == 1])\nno = len(out_logreg) - yes # len(df[df['target']== 0])\nimport matplotlib.pyplot as plt\ny = ('yes', 'no')\ny_pos = np.arange(len(y))\nx = (yes, no)\nlabels = 'yes', 'no'\nsizes = [yes, no]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes,  labels=labels, autopct='%1.1f%%', startangle=90) \nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title('Percentage of target', size=16)\nplt.show() # Pie chart, where the slices will be ordered and plotted counter-clockwise:","89dfa896":"model = SVC()\nmodel.fit(X_train, y_train)\nscore_svc = model.score(X_train, y_train)\nprint('Support Vector Clustering score: ', score_svc)\nscore_svc = model.score(X_test, y_test)\nprint('Support Vector Clustering score: ', score_svc)\nY_pred = model.predict(test_data.drop('PassengerId',axis=1)) # X_test\nout_svc = pd.DataFrame({'PassengerId':ids,'Survived':Y_pred})\nprint(out_svc.head())\npeople = out_svc.loc[out_svc.Survived == 1][\"Survived\"]\nrate_people = 0\nif len(people) > 0 :\n    rate_people = len(people)\/len(out_svc)\nprint(\"Support Vector Clustering % of people who survived:\", rate_people)","5b3ad3ff":"yes = len(people) # len(df[df['target'] == 1])\nno = len(out_logreg) - yes # len(df[df['target']== 0])\nimport matplotlib.pyplot as plt\ny = ('yes', 'no')\ny_pos = np.arange(len(y))\nx = (yes, no)\nlabels = 'yes', 'no'\nsizes = [yes, no]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes,  labels=labels, autopct='%1.1f%%', startangle=90) \nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title('Percentage of target', size=16)\nplt.show() # Pie chart, where the slices will be ordered and plotted counter-clockwise:","9c8c0a48":"model = KNeighborsClassifier()\nmodel.fit(X_train, y_train)\nscore_knc = model.score(X_train, y_train)\nprint('K Neighbors Classifier score:', score_knc)\nscore_knc = model.score(X_test, y_test)\nprint('K Neighbors Classifier score:', score_knc)\nY_pred = model.predict(test_data.drop('PassengerId',axis=1)) # X_test\nout_knc = pd.DataFrame({'PassengerId':ids,'Survived':Y_pred})\nprint(out_knc.head())\npeople = out_knc.loc[out_knc.Survived == 1][\"Survived\"]\nrate_people = 0\nif len(people) > 0 :\n    rate_people = len(people)\/len(out_knc)\nprint(\"K Neighbors Classifier % of people who survived:\", rate_people)","f1be6678":"yes = len(people) # len(df[df['target'] == 1])\nno = len(out_logreg) - yes # len(df[df['target']== 0])\nimport matplotlib.pyplot as plt\ny = ('yes', 'no')\ny_pos = np.arange(len(y))\nx = (yes, no)\nlabels = 'yes', 'no'\nsizes = [yes, no]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes,  labels=labels, autopct='%1.1f%%', startangle=90) \nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title('Percentage of target', size=16)\nplt.show() # Pie chart, where the slices will be ordered and plotted counter-clockwise:","88a2997e":"model = RandomForestClassifier(n_estimators=100) # , max_depth=5, random_state=1\nmodel.fit(X_train, y_train)\nscore_rfc = model.score(X_train, y_train)\nprint('Random Forest Classifier score: ', score_rfc)\nscore_rfc = model.score(X_test, y_test)\nprint('Random Forest Classifier score: ', score_rfc)\nY_pred = model.predict(test_data.drop('PassengerId',axis=1)) # X_test\nout_rfc = pd.DataFrame({'PassengerId':ids, 'Survived': Y_pred})\nprint(out_rfc.head())\npeople = out_rfc.loc[out_rfc.Survived == 1][\"Survived\"]\nrate_people = 0\nif len(people) > 0 :\n    rate_people = len(people)\/len(out_rfc)\nprint(\"Random Forest Classifier % of people who survived:\", rate_people)","acf21d76":"yes = len(people) # len(df[df['target'] == 1])\nno = len(out_logreg) - yes # len(df[df['target']== 0])\nimport matplotlib.pyplot as plt\ny = ('yes', 'no')\ny_pos = np.arange(len(y))\nx = (yes, no)\nlabels = 'yes', 'no'\nsizes = [yes, no]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes,  labels=labels, autopct='%1.1f%%', startangle=90) \nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title('Percentage of target', size=16)\nplt.show() # Pie chart, where the slices will be ordered and plotted counter-clockwise:","07a0fd39":"decision_tree = DecisionTreeClassifier(max_depth=5) \ndecision_tree.fit(X_train, y_train)  \nscore_dtc = decision_tree.score(X_train, y_train)\nprint('Decision Tree Classifier score =', score_dtc)\nscore_dtc = decision_tree.score(X_test, y_test)\nprint('Decision Tree Classifier score =', score_dtc)\nY_pred = model.predict(test_data.drop('PassengerId',axis=1)) # X_test\nout_dtc = pd.DataFrame({'PassengerId':ids, 'Survived': Y_pred})\nprint(out_dtc.head())\npeople = out_dtc.loc[out_dtc.Survived == 1][\"Survived\"]\nrate_people = 0\nif len(people) > 0 :\n    rate_people = len(people)\/len(out_dtc)\nprint(\"Decision Tree Classifier % of people who survived:\", rate_people)","aa4f8426":"yes = len(people) # len(df[df['target'] == 1])\nno = len(out_logreg) - yes # len(df[df['target']== 0])\nimport matplotlib.pyplot as plt\ny = ('yes', 'no')\ny_pos = np.arange(len(y))\nx = (yes, no)\nlabels = 'yes', 'no'\nsizes = [yes, no]\nfig1, ax1 = plt.subplots()\nax1.pie(sizes,  labels=labels, autopct='%1.1f%%', startangle=90) \nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\nplt.title('Percentage of target', size=16)\nplt.show() # Pie chart, where the slices will be ordered and plotted counter-clockwise:","21a904eb":"# Fixing random state for reproducibility\nplt.rcdefaults()\nfig, ax = plt.subplots()\npeople = ('Logistic Regression', 'Support Vector Clustering', 'K Neighbors Classifier', 'Random Forest Classifier', 'Decision Tree Classifier')\ny_pos = np.arange(len(people))\nx = (score_logreg,score_svc,score_knc,score_rfc,score_dtc) # scores\nax.barh(y_pos, x, align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(people)\nax.invert_yaxis() # labels read top-to-bottom\nax.set_xlabel('Performance')\nax.set_title('Which one is the best algorithm?')\nfor i, v in enumerate(x):\n    ax.text(v + 1, i, str(v), color='black', va='center', fontweight='normal')\nplt.show()","fc8cd28d":"score = score_logreg\noutput = out_logreg\nprint(\"Logistic Regression!\", score)\nif score_svc > score:\n    score = score_svc\n    output = out_svc\n    print(\"Support Vector Clustering!\", score)\nif score_knc > score:\n    score = score_knc\n    output = out_knc\n    print(\"K Neighbors Classifier!\", score)\nif score_rfc > score:\n    score = score_rfc\n    output = out_rfc\n    print(\"Random Forest Classifier!\", score)\nif score_dtc > score:\n    score = score_dtc\n    output = out_dtc\n    print(\"Decision Tree Classifier!\", score)","4709aab7":"output.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","42d62d21":"output.shape","8687eaf1":"output.head()","913da1d3":"output.tail()","cad8f3d5":"### Cabin","0d32db12":">Hello! My name is [Mauricio Ruanova](https:\/\/mruanova.com) and I am following the [Decision Making](https:\/\/en.wikipedia.org\/wiki\/Decision-making) process.\n\n![Titanic](https:\/\/mruanova.com\/leo.jpg)\n\nTable of Contents\n1. [Step 1 - Identify The Problem](#step1)\n1. [Step 2 - Explore Your Options](#step2)\n1. [Step 3 - Evaluate The Outcomes](#step3)\n1. [Step 4 - Decide And Act](#step4)\n1. [Change Log](#changelog)\n1. [Credits](#credits)\n\n<a id=\"step1\"><\/a>\n# Step 1 Identify The Problem\nIn this problem we have a clear goal: to predict survival on the Titanic,\nusing [Exploratory Data Analysis](https:\/\/en.wikipedia.org\/wiki\/Exploratory_data_analysis) \nand [Machine Learning](https:\/\/en.wikipedia.org\/wiki\/Machine_learning).\n\n**How-to use the Titanic data set:** Please take a look at the [Data Dictionary](https:\/\/www.kaggle.com\/c\/titanic\/data)\n\n## Background Information\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg.\n\nUnfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we were asked to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n### Data used in this competition\n\nI am using two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled `train.csv` and the other is titled `test.csv`.\n\nTrain.csv contains the details of a subset of the passengers on board (891 to be exact) and reveals whether they survived or not, also known as the \u201cground truth\u201d.\n\nThe `test.csv` dataset contains similar information but does not disclose the \u201cground truth\u201d for each passenger. It\u2019s used to predict these outcomes.\n\n### Submission\n\nGenerated a prediction file `my_submission.csv` and submitted my prediction to the Kaggle competition to get a score in the leaderboard.","88142c7d":"### pclass","c575a4b6":"<a id=\"step4\"><\/a>\n# Step 4 Decide And Act","95706f3f":"<a id=\"changelog\"><\/a>\n# Change Log\n- 2020-08-25 Version 1.0\n- 2020-10-05 Table of contents added\n- 2021-01-12 Pie chart percentage of survival","21ba017f":"## DecisionTreeClassifier accuracy score\nDecision Tree Classifier % of people who survived: %","c2002ed0":"### Family","9f547d3b":"### Sex","e7f0b116":"## Create bins by age for Data Visualization","6f47dc6d":"# <a id=\"credits\"><\/a>\n# Credits\nThanks to my teachers Andrew Ng, Ligdimar Gonz\u00e1lez and Isaac Faber","49295dec":"### New column: Not Alone","aa99604b":"### Missing Values by column","5a51ffe3":"<a id=\"step3\"><\/a>\n# Step 3 Evaluate The Outcomes","9b0df79b":"## RandomForestClassifier accuracy score\nRandom Forest Classifier % of people who survived: %","a57574b4":"### PassengerId, Name, Ticket","2d4dced5":"<a id=\"step2\"><\/a>\n# Step 2 Explore Your Options","6ddea587":"\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","f3bfc138":"### Age","47ccc78b":"## Support Vector Clustering accuracy score\nSupport Vector Clustering % of people who survived: %","cf6ce380":"### Verify no more missing values","0ebd0cb4":"## Verify that the number of rows is correct","bb5d7190":"## Using scikit learn compare different methods and choose the best\n- LogisticRegression \n- Support Vector Clustering (SVC)\n- KNeighborsClassifier\n- RandomForestClassifier","40e3a48c":"### Fare","f0d132a4":"## KNeighborsClassifier accuracy score\nK Neighbors Classifier % of people who survived: %","e7624ef1":"## Logistic Regression accuracy score\nLogistic Regression % of people who survived: %","8869daeb":"### Embarked","7c5e787b":"## Export file my_submission.csv\nUpload to kaggle to get a rank in the leaderboard","36bc1dec":"## Make the columns numeric for easier data manipulation"}}