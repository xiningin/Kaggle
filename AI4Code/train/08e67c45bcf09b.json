{"cell_type":{"9bfe31c3":"code","eb22dd05":"code","0f5d9d1f":"code","b61765a4":"code","7523821e":"code","634ccdab":"code","9a854f62":"code","2993a2ff":"code","59409bce":"code","90672e98":"code","9e2be45d":"code","2406ce7c":"code","890cd022":"code","2baf315e":"code","f160df6f":"code","c66c67e8":"code","9effc90b":"code","0fa2c318":"code","a112c483":"code","f7047b19":"code","157c19c1":"code","eec64a0f":"code","c821d095":"code","33f7bdf1":"code","b7840e5e":"code","f1a351ed":"markdown","464c00cc":"markdown","41f9cb94":"markdown"},"source":{"9bfe31c3":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom glob import glob\nfrom skimage import io\nimport cv2\nimport random\n\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.models import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import add\nfrom keras.layers import Activation\nfrom keras.layers import UpSampling2D\nfrom keras.layers import Dropout\nfrom keras.layers import BatchNormalization\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Lambda\nfrom keras.layers import multiply\nfrom keras.layers import concatenate\n\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.metrics import MeanIoU\nfrom sklearn.model_selection import train_test_split\n","eb22dd05":"path = \"..\/input\/lgg-mri-segmentation\/kaggle_3m\/\"","0f5d9d1f":"df = pd.read_csv(path + \"data.csv\")\ndf.head()","b61765a4":"df.info()","7523821e":"# Separate images and masks in the directory\nmasks_dir = glob(path + \"*\/*_mask*\")\n\nimages_dir = []\nfor img in masks_dir:\n    images_dir.append(img.replace(\"_mask\", \"\"))","634ccdab":"# Create a new datafarme for brain images and masks\ndata_brain = pd.DataFrame(data={\n                                \"file_images\":images_dir, \n                                \"file_masks\":masks_dir\n                                })\ndata_brain.head()","9a854f62":"def positive_negative_diagnosis(file_masks):\n    mask = cv2.imread(file_masks)\n    value = np.max(mask)\n    if value > 0:\n        return 1\n    else:\n        return 0","2993a2ff":"data_brain[\"mask\"] = data_brain[\"file_masks\"].apply(lambda x: positive_negative_diagnosis(x))","59409bce":"data_brain","90672e98":"data_brain[\"mask\"].value_counts()","9e2be45d":"def show_image(df):\n\n  fig, ax = plt.subplots(nrows=10, ncols=3, figsize=(20, 40))\n  count = 0\n  i = 0\n  for mask in df[\"mask\"]:\n    if mask == 1:\n      # Show images\n      image = io.imread(df.file_images[i])\n      ax[count][0].title.set_text(\"Brain MRI\")\n      ax[count][0].imshow(image)\n\n      # Show masks\n      mask = io.imread(df.file_masks[i])\n      ax[count][1].title.set_text(\"Mask Brain MRI\")\n      ax[count][1].imshow(mask, cmap=\"gray\")\n\n      # Show MRI Brain with mask\n      image[mask == 255] = (0, 255, 0)    # Here, we want to modify the color of pixel at the position of mask\n      ax[count][2].title.set_text(\"MRI Brain with mask\")\n      ax[count][2].imshow(image)\n      count += 1\n    i += 1\n    if count == 10:\n      break\n  fig.tight_layout()\n  plt.show()","2406ce7c":"show_image(data_brain)","890cd022":"# Load all images and masks\nclass ImageLoading():\n\n  def __init__(self, img_path, mask_path):\n    self.img_path = img_path\n    self.mask_path = mask_path\n    self.IMG_HEIGHT = 256\n    self.IMG_WIDTH = 256\n    # The number of classes for segmentation\n    self.NUM_CLASSES = 1\n\n    # Load images\n    self.images_training = self.resize_images()\n    print(self.images_training.shape)\n    # Load masks\n    self.masks_training = self.resize_masks()\n    print(self.masks_training.shape)\n\n\n  # resise the image\n  def resize_images(self):\n    images_training = []\n    for imagePath in self.img_path:\n      image = cv2.imread(imagePath)\n      image = cv2.resize(image, (self.IMG_WIDTH, self.IMG_HEIGHT))\n      images_training.append(image)\n    # Convert to numpy array\n    images_training = np.array(images_training)\n    return images_training\n\n  # resise the mask\n  def resize_masks(self):\n    masks_training = []\n    for maskPath in self.mask_path:\n      mask = cv2.imread(maskPath, 0)\n      mask = cv2.resize(mask, (self.IMG_WIDTH, self.IMG_HEIGHT), interpolation=cv2.INTER_NEAREST)\n      masks_training.append(mask)\n    # Convert to numpy array\n    masks_training = np.array(masks_training)\n    return masks_training\n","2baf315e":"image_loader = ImageLoading(images_dir[:1000], masks_dir[:1000])\nimages_train = image_loader.resize_images()\nmasks_train = image_loader.resize_masks()","f160df6f":"# Normalize images\nimages_train = np.array(images_train) \/ 255.\nmasks_train = np.expand_dims((np.array(masks_train)), 3) \/255.","c66c67e8":"X_train, X_test, Y_train, Y_test = train_test_split(images_train, masks_train, test_size=0.2, random_state=42)\n\nprint(\"X_train shape = {}\".format(X_train.shape))\nprint(\"X_test shape = {}\".format(X_test.shape))\nprint(\"Y_train shape = {}\".format(Y_train.shape))\nprint(\"Y_test shape = {}\".format(Y_test.shape))","9effc90b":"IMG_HEIGHT = X_train.shape[1]\nIMG_WIDTH = X_train.shape[2]\nIMG_CHANNELS = X_train.shape[3]\n# Binary class\nNUM_CLASS = 1\nBATCH_SIZE = 8\ninput_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)","0fa2c318":"# loss function and metrics\ndef dice_coef(y_pred, Y):\n    y_flatten = K.flatten(Y)\n    y_pred_flatten = K.flatten(y_pred)\n    intersection = K.sum(y_flatten * y_pred_flatten)\n    dice = (0.2 * intersection + 1.0) \/ (K.sum(y_flatten) + K.sum(y_pred_flatten) + 1.0)\n    return dice\n\ndef jacard_coef(y_pred, Y):\n    y_flatten = K.flatten(Y)\n    y_pred_flatten = K.flatten(y_pred)\n    intersection = K.sum(y_flatten * y_pred_flatten)\n    jacard = (intersection + 1.0) \/ (K.sum(y_flatten) + K.sum(y_pred_flatten) - intersection + 1.0)\n    return jacard\n\ndef jacard_coef_loss(y_pred, Y):\n    return -jacard_coef(y_pred, Y)\n\n\ndef dice_coef_loss(y_pred, Y):\n    return -dice_coef(y_pred, Y)\n","a112c483":"# Attention Unet\nclass Attention_UNet():\n    def __init__(self, input_shape, num_classes=1, dropout=0, BatchNorm=True):\n        super(Attention_UNet, self).__init__()\n        self.max_pooling = True\n        self.num_classes = num_classes\n        self.input_shape = input_shape\n        # number of basic filters for the first layer\n        self.num_filters = 64\n        # size of the convolutional filter\n        self.filter_size = 3\n        # size of upsampling filters\n        self.upsampling_filter = 2\n\n\n    def make_conv_block(self, input_layer, filter_size, num_filters, dropout=0, BatchNorm=False):\n        conv_layer = Conv2D(num_filters, (filter_size, filter_size), padding=\"same\")(input_layer)\n        if BatchNorm is True:\n            conv_layer = BatchNormalization(axis=3)(conv_layer)\n        conv_layer = Activation(\"relu\")(conv_layer)\n\n        conv_layer = Conv2D(num_filters, (filter_size, filter_size), padding=\"same\")(conv_layer)\n        if BatchNorm is True:\n            conv_layer = BatchNormalization(axis=3)(conv_layer)\n        conv_layer = Activation(\"relu\")(conv_layer)\n\n        if dropout > 0:\n            conv_layer = Dropout(dropout)(conv_layer)\n\n        return conv_layer\n\n\n    def make_repeat_elements(self, tensor, rep):\n        \"\"\"\n           This function will repeat the elements of a tensor along an axis through a factor of rep using lambda function.\n           For instance, if tensor has shape (None, 256,256,3), lambda will return a tensor of shape (None, 256,256,6),\n           if specified axis=3 and rep=2\n        \"\"\"\n        return Lambda(lambda x, repnum: K.repeat_elements(x, repnum, axis=3), arguments={\"repnum\":rep})(tensor)\n\n\n    def _gate_signal(self, input_channels, output_channels, BatchNorm=False):\n        \"\"\"\n            This function will resize the downsample layer feature map into the same dimension as the upsample\n            layer feature map through 1x1 convolution\n\n           return:\n                the gating feature map with the same dimension of the up layer feature map\n        \"\"\"\n        g = Conv2D(output_channels, (1,1), padding=\"same\")(input_channels)\n        if BatchNorm:\n            g = BatchNormalization()(g)\n        g = Activation(\"relu\")(g)\n\n        return g\n\n    # We add attention block after shortcut connection in UNet\n    def make_attention_block(self, input_layer, gating, num_filters):\n        input_layer_shape = K.int_shape(input_layer)\n        gating_shape = K.int_shape(gating)\n\n        # Here, we should get the input_layer signal to the same shape as the gating signal\n        input_layer_theta = Conv2D(num_filters, (2, 2), strides = (2, 2), padding = \"same\")(input_layer)\n        input_layer_theta_shape = K.int_shape(input_layer_theta)\n\n        # we should get the gating signal to the same number of filters as the num_filters\n        gating_phi = Conv2D(num_filters, (1, 1), padding = \"same\")(gating)\n        gating_upsample = Conv2DTranspose(num_filters,\n                                          (3, 3),\n                                          strides = (input_layer_theta_shape[1] \/\/ gating_shape[1],\n                                                     input_layer_theta_shape[2] \/\/ gating_shape[2]),\n                                          padding = \"same\")(gating_phi)\n        concat_layer = add([gating_upsample, input_layer_theta])\n        concat_layer = Activation(\"relu\")(concat_layer)\n        concat_layer = Conv2D(1, (1, 1), padding=\"same\")(concat_layer)\n        concat_layer = Activation(\"sigmoid\")(concat_layer)   # To get weigth between 0 and 1\n        concat_layer_shape = K.int_shape(concat_layer)\n        concat_layer_upsampling = UpSampling2D(size = (input_layer_shape[1] \/\/ concat_layer_shape[1],\n                                                       input_layer_shape[2] \/\/ concat_layer_shape[2]))(concat_layer)\n\n        concat_layer_upsampling = self.make_repeat_elements(concat_layer_upsampling, input_layer_shape[3])\n\n        y = multiply([concat_layer_upsampling, input_layer])\n\n        # Final layer\n        conv_result = Conv2D(input_layer_shape[3], (1, 1), padding=\"same\")(y)\n        conv_result_batchNorm = BatchNormalization()(conv_result)\n\n        return conv_result_batchNorm\n\n    def build_attention_unit(self, dropout=0, BatchNorm=True):\n        input_layer = Input(self.input_shape, dtype=tf.float32)\n\n        ############ Add downsampling layer ############\n        # Block 1, 128\n        encoder_128 = self.make_conv_block(input_layer, self.filter_size, self.num_filters, dropout=dropout, BatchNorm=BatchNorm)\n        if self.max_pooling:\n            encoder_pool_64 = MaxPooling2D(pool_size=(2, 2))(encoder_128)\n        # Block 2, 64 layer\n        encoder_64 = self.make_conv_block(encoder_pool_64, self.filter_size, 2 * self.num_filters, dropout=dropout, BatchNorm=BatchNorm)\n        if self.max_pooling:\n            encoder_pool_32 = MaxPooling2D(pool_size=(2, 2))(encoder_64)\n        # Block 3, 32 layer\n        encoder_32 = self.make_conv_block(encoder_pool_32, self.filter_size, 4 * self.num_filters, dropout=dropout, BatchNorm=BatchNorm)\n        if self.max_pooling:\n            encoder_pool_16 = MaxPooling2D(pool_size=(2, 2))(encoder_32)\n        # Block 4, 8 layer\n        encoder_16 = self.make_conv_block(encoder_pool_16, self.filter_size, 8 * self.num_filters, dropout=dropout, BatchNorm=BatchNorm)\n        if self.max_pooling:\n            encoder_pool_8 = MaxPooling2D(pool_size=(2, 2))(encoder_16)\n        # Block 5, just convolutional block\n        encoder_8 = self.make_conv_block(encoder_pool_8, self.filter_size, 16 * self.num_filters, dropout=dropout, BatchNorm=BatchNorm)\n\n        ############ Upsampling layers #############\n        # Block 6, attention gated concatenation + upsampling + double residual convolution\n        gate_16 = self._gate_signal(encoder_8, 8 * self.num_filters, BatchNorm=BatchNorm)\n        attention_block_16 = self.make_attention_block(encoder_16, gate_16, 8 * self.num_filters)\n        decoder_16 = UpSampling2D(size=(self.upsampling_filter, self.upsampling_filter), data_format=\"channels_last\")(\n            encoder_8)\n        decoder_16 = concatenate([decoder_16, attention_block_16], axis=3)\n        decoder_conv_16 = self.make_conv_block(decoder_16, self.filter_size, 8 * self.num_filters, dropout=dropout, BatchNorm=BatchNorm)\n        # Block 7\n        gate_32 = self._gate_signal(decoder_conv_16, 4 * self.num_filters, BatchNorm=BatchNorm)\n        attention_block_32 = self.make_attention_block(encoder_32, gate_32, 4 * self.num_filters)\n        decoder_32 = UpSampling2D(size=(self.upsampling_filter, self.upsampling_filter), data_format=\"channels_last\")(\n            decoder_conv_16)\n        decoder_32 = concatenate([decoder_32, attention_block_32], axis=3)\n        decoder_conv_32 = self.make_conv_block(decoder_32, self.filter_size, 4 * self.num_filters, dropout=dropout, BatchNorm=BatchNorm)\n        # Block 8\n        gate_64 = self._gate_signal(decoder_conv_32, 2 * self.num_filters, BatchNorm=BatchNorm)\n        attention_block_64 = self.make_attention_block(encoder_64, gate_64, 2 * self.num_filters)\n        decoder_64 = UpSampling2D(size=(self.upsampling_filter, self.upsampling_filter), data_format=\"channels_last\")(\n            decoder_conv_32)\n        decoder_64 = concatenate([decoder_64, attention_block_64], axis=3)\n        decoder_conv_64 = self.make_conv_block(decoder_64, self.filter_size, 2 * self.num_filters, dropout=dropout, BatchNorm=BatchNorm)\n        # Block 9\n        gate_128 = self._gate_signal(decoder_conv_64, self.num_filters, BatchNorm=BatchNorm)\n        attention_block_128 = self.make_attention_block(encoder_128, gate_128, self.num_filters)\n        decoder_128 = UpSampling2D(size=(self.upsampling_filter, self.upsampling_filter), data_format=\"channels_last\")(decoder_conv_64)\n        decoder_128 = concatenate([decoder_128, attention_block_128], axis=3)\n        decoder_conv_128 = self.make_conv_block(decoder_128, self.filter_size, self.num_filters, dropout=dropout, BatchNorm=BatchNorm)\n\n        # Final convolutional layers (1 * 1)\n        final_conv_lr = Conv2D(self.num_classes, kernel_size=1)(decoder_conv_128)\n        final_conv_lr = BatchNormalization(axis=3)(final_conv_lr)\n        # If a binary classification, we need to set \"sigmoid\" while for multichannel we should change to softmax\n        final_conv_lr = Activation(\"sigmoid\")(final_conv_lr)\n\n        # Set the model\n        model = Model(input_layer, final_conv_lr, name=\"Attention_UNet\")\n        print(model.summary())\n\n        return model","f7047b19":"unet_model = Attention_UNet(input_shape)","157c19c1":"att_unet_model = unet_model.build_attention_unit()","eec64a0f":"att_unet_model.compile(optimizer=\"adam\", \n                       loss = \"binary_crossentropy\",\n                       metrics=[\"accuracy\", jacard_coef])","c821d095":"history = att_unet_model.fit(X_train, Y_train, \n                             verbose=1,\n                             batch_size = 8,\n                             validation_data=(X_test, Y_test),\n                             shuffle=False,\n                             epochs=25)","33f7bdf1":"# Check and plot the loss and accuracy\nplt.figure(1)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.legend(['training', 'validation'])\nplt.title('loss')\nplt.xlabel('epoch')\n\nplt.figure(2)\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.legend(['training', 'validation'])\nplt.title('Acuracy')\nplt.xlabel('epoch')\nplt.show()","b7840e5e":"num_img_test = random.randint(0, X_test.shape[0]-1)\nimg_test = X_test[num_img_test]\ntest_label = Y_test[num_img_test]\n\nimg_test_input = np.expand_dims(img_test, 0)\npred = (att_unet_model.predict(img_test_input)[0,:,:,0] > 0.5).astype(np.uint8)\n\n# Visualize the result \nplt.figure(figsize=(15, 10))\nplt.subplot(231)\nplt.title('Testing Image')\nplt.imshow(img_test, cmap='gray')\nplt.subplot(232)\nplt.title('Testing Label')\nplt.imshow(test_label[:,:,0], cmap='gray')\nplt.subplot(233)\nplt.title('Prediction on test image')\nplt.imshow(pred, cmap='gray')","f1a351ed":"**Separate train, test, and val set**","464c00cc":"**Load images and masks**","41f9cb94":"**Data Visualization**"}}