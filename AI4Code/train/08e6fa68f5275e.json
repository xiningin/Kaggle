{"cell_type":{"821a5b2f":"code","9b810876":"code","6a5f7c96":"code","dcbfb733":"code","5177c4ca":"code","618bc738":"code","16a0da1e":"code","fe5bcfbf":"code","1854dae3":"code","34c5f0e2":"code","c0863a6a":"code","9ad6a21f":"code","dbf616bc":"code","9cc12d3a":"code","af7cc0f9":"code","8c3bc8e6":"code","180783de":"code","2f6ecbbb":"code","c64c8d5c":"code","a612cf35":"markdown","bc2410fa":"markdown","b1fc3af6":"markdown","81050511":"markdown","82e975ae":"markdown","786ab079":"markdown","098916d3":"markdown","68da7c89":"markdown","f33cf391":"markdown"},"source":{"821a5b2f":"import transformers \nimport torch.nn as nn \nimport torch \nfrom tqdm import tqdm\nimport torch \nimport torch.nn as nn \nimport pandas as pd\nimport torch.nn as nn\nimport numpy as np\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","9b810876":"MAX_Len = 512 \nTRAIN_BATCH_SIZE =8 \nVALID_BATCH_SIZE = 4\nBERT_PATH = '..\/input\/bert-base-uncased'\nTOKENZIER = transformers.BertTokenizer.from_pretrained(BERT_PATH ,do_lower_case = True )","6a5f7c96":"class BertBaseUncased(nn.Module) :\n    def __init__(self) : \n        super(BertBaseUncased,self).__init__() \n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH) \n        self.bert_drop = nn.Dropout(0.4) \n        self.out = nn.Linear(768,1) \n    def forward(self,ids,mask,token_type_ids) : \n        out1,out2 = self.bert( \n            ids , \n            attention_mask = mask , \n            token_type_ids = token_type_ids \n        )\n        bo = self.bert_drop(out2) \n        output = self.out(bo) \n        return output ","dcbfb733":"class BERTDataset : \n    def __init__(self,df) : \n        \n        self.text = df['text'].values\n        self.target = df['target'].values \n        self.tokenizer = TOKENZIER \n        self.max_len = MAX_Len \n    def __len__(self) : \n        return len(self.text) \n    def __getitem__(self, item):\n        text = str(self.text[item])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n                text,\n                None,\n                add_special_tokens=True,\n                max_length=self.max_len\n            )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        padding_length = self.max_len - len(ids)\n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n\n        return {\n                'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long),\n                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n                'targets': torch.tensor(self.target[item], dtype=torch.float)\n            }","5177c4ca":"def loss_fn(outputs, targets):\n    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))","618bc738":"def train_fn(data_loader, model, optimizer, scheduler):\n    model.train()\n\n    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets = d[\"targets\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n\n        optimizer.zero_grad()\n        outputs = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n       \n","16a0da1e":" def eval_fn(data_loader, model):\n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n    with torch.no_grad():\n        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            outputs = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","fe5bcfbf":"DEVICE =torch.device(\"cuda\")\ndevice = torch.device(\"cuda\")\ndef run(model,EPOCHS):\n    dfx = pd.read_csv('..\/input\/nlp-getting-started\/train.csv').fillna(\"none\")\n    df_train, df_valid = model_selection.train_test_split(\n        dfx,\n        test_size=0.1,\n        random_state=42,\n        stratify=dfx.target.values\n    )\n\n\n\n    train_dataset = BERTDataset(\n        df_train\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        num_workers=4\n    )\n\n    valid_dataset = BERTDataset(\n        df_valid\n\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=1\n    )\n\n    device = torch.device(\"cuda\")\n    \n    \n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n\n    num_train_steps = int(len(train_data_loader)) * EPOCHS\n    optimizer = AdamW(optimizer_parameters, lr=1e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n\n\n    model = nn.DataParallel(model)\n\n    best_accuracy = 0\n    for epoch in range(EPOCHS):\n        train_fn(train_data_loader, model, optimizer, scheduler)\n        outputs, targets = eval_fn(valid_data_loader, model)\n        outputs = np.array(outputs) >= 0.5\n        accuracy = metrics.accuracy_score(targets, outputs)\n        print(f\"Accuracy Score = {accuracy}\")\n        scheduler.step()\n","1854dae3":"model = BertBaseUncased()\nmodel.to(device)\ngetattr(tqdm, '_instances', {}).clear()\nrun(model,1)","34c5f0e2":"def sentence_prediction(sentence):\n    tokenizer = TOKENZIER\n    max_len = MAX_Len\n    text = str(sentence)\n    text = \" \".join(text.split())\n\n    inputs = tokenizer.encode_plus(\n        text,\n        None,\n        add_special_tokens=True,\n        max_length=max_len\n    )\n\n    ids = inputs[\"input_ids\"]\n    mask = inputs[\"attention_mask\"]\n    token_type_ids = inputs[\"token_type_ids\"]\n\n    padding_length = max_len - len(ids)\n    ids = ids + ([0] * padding_length)\n    mask = mask + ([0] * padding_length)\n    token_type_ids = token_type_ids + ([0] * padding_length)\n\n    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0)\n\n    ids = ids.to(DEVICE, dtype=torch.long)\n    token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)\n    mask = mask.to(DEVICE, dtype=torch.long)\n\n    outputs = model(\n        ids=ids,\n        mask=mask,\n        token_type_ids=token_type_ids\n    )\n\n    outputs = torch.sigmoid(outputs).cpu().detach().numpy()\n    return outputs[0][0]","c0863a6a":"test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest['target'] = test['text'].apply(sentence_prediction)","9ad6a21f":"test","dbf616bc":"sub = test[['id','target']]","9cc12d3a":"sub['target'] = sub['target'].round().astype('int')","af7cc0f9":"sub['target'] ","8c3bc8e6":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntrain = train.fillna('None')\nag = train.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'Count', 'target':'Disaster Probability'})\nag.sort_values('Disaster Probability', ascending=False).head(10)","180783de":"keyword_list = list(ag[(ag['Count']>2) & (ag['Disaster Probability']>=0.9)].index)\nkeyword_list","2f6ecbbb":"ids = test['id'][test.keyword.isin(keyword_list)].values\nsub['target'][sub['id'].isin(ids)] = 1\nsub.head()","c64c8d5c":"sub.to_csv('submission.csv',index=False)","a612cf35":"# Make Prediction ","bc2410fa":"# Using keywords for prediction improvement","b1fc3af6":"# Engine ","81050511":"# Creat Model ","82e975ae":"I hope you find this kernel useful and enjoyable.\n\n\nand here you will find my notebook about working with text data in pandas : https:\/\/www.kaggle.com\/nhoues1997\/working-with-text-data-in-pandas\n\n\nYour comments and feedback are most welcome. \n","786ab079":"# Import libraries and Config ","098916d3":"# Training the model ","68da7c89":"# Bert Model with Pytorch \n\nThis notebook is basic on Abhishek Thakur YouTube video \"Training Sentiment Model Using BERT and Serving it with Flask API\" you can find the video here : https:\/\/www.youtube.com\/watch?v=hinZO--TEk4 ","f33cf391":"# Data set "}}