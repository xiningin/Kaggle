{"cell_type":{"dbd67adb":"code","30454635":"code","113b389f":"code","3e2e8306":"code","5227f2d2":"code","05b4d3b9":"code","1dad13da":"code","edc2fd76":"code","7ac3e000":"code","2c6c3852":"code","c0897c37":"code","36f61692":"code","e0e92e10":"code","b9181800":"code","bd46ff57":"code","d1856c8a":"code","4e206960":"code","5bfb505d":"code","99c90de6":"code","beadbc48":"code","7979b0d7":"code","970af7cb":"code","d2bfbb25":"code","8e14d3f6":"code","bbc10790":"code","2c0a3e4c":"code","2f4f0fb8":"code","f9af2503":"code","a404035c":"code","a5339b6b":"code","65c7d423":"code","943a09f0":"code","d9895604":"code","4b53497f":"code","d54169db":"code","0fa11985":"code","1d8b0d92":"code","3fd33e9e":"code","baee2ca6":"code","7f4acb37":"code","1b43a23e":"code","05ec10a8":"code","f1c068aa":"code","230af5b5":"code","090b1c87":"code","1930fab2":"code","16abf269":"code","e114c5c0":"code","c37bda76":"code","5919498b":"code","f41df08a":"code","2afa61cf":"code","1a054dd1":"code","eaaefd71":"code","9f5495b3":"code","7e1b4fb3":"code","cb2da9cc":"code","2d675410":"code","0708cc8f":"code","db85f0fa":"code","d51e916f":"code","6759f648":"code","ab28c386":"code","7d1fcec6":"code","96624cf7":"code","3730af8a":"code","4eaadd77":"code","59002d1d":"code","eabc9120":"code","2502de56":"code","11c0cfde":"code","c0514e68":"code","f9ecefe7":"code","e2ed1c59":"code","61a80682":"code","3fbaf5a7":"code","7bb19354":"code","f175a265":"code","18f9a0de":"code","a634fcee":"code","a55c09c8":"code","89afca3d":"code","a5836ec4":"code","d19616a8":"code","190cc8f9":"code","8588092a":"code","241906a9":"code","2775bcbc":"code","abe3b704":"code","e9f5982e":"code","09708b1c":"code","476f8cb6":"markdown","56dad1d1":"markdown","4f79de71":"markdown","4313859e":"markdown","57bc43e6":"markdown","bb02b0b5":"markdown","d30bd399":"markdown","46790e97":"markdown","a0bc856a":"markdown","8c923f01":"markdown","8de14883":"markdown","49973012":"markdown","2d2887c4":"markdown","5d8f7a00":"markdown","738b4769":"markdown","0192e525":"markdown","f9eaa003":"markdown","1b1a7825":"markdown","9ff9b679":"markdown","0ae36324":"markdown","89653a90":"markdown","f2c5c9b0":"markdown","66cdf222":"markdown","8ca0f06e":"markdown","5b3aad2f":"markdown","b9f2add7":"markdown","c746bf71":"markdown","bf30d594":"markdown","82bfbd19":"markdown","7a1682c1":"markdown","fa874ba9":"markdown","25bd5ea1":"markdown","8b14b43f":"markdown","c3f7d902":"markdown","3ad23775":"markdown","942d479e":"markdown","b37c8cf3":"markdown","9e2da666":"markdown","30db3a74":"markdown","8e5ca430":"markdown","d7fcd56f":"markdown","98901655":"markdown","b25c3e21":"markdown","8d274f9d":"markdown","3767b264":"markdown","b3363602":"markdown","a53b6fc5":"markdown","7f3b780a":"markdown","5687d559":"markdown","2b1f2918":"markdown","4b2f84e4":"markdown","07998b9b":"markdown","7eff9c43":"markdown","557804c4":"markdown","1280c338":"markdown","b344f50f":"markdown","77ba3e9a":"markdown","f384001e":"markdown","4014aa6a":"markdown","71f472bd":"markdown","36001612":"markdown","94412349":"markdown","f7d1c333":"markdown","ae71c335":"markdown","4c187d83":"markdown","dceea13b":"markdown","70f49c7b":"markdown","78db8f98":"markdown","882d9046":"markdown","42d0a14f":"markdown","4bf1b99d":"markdown","18fed4ef":"markdown","6fbc7028":"markdown","0d524298":"markdown","32106526":"markdown","7e6ae660":"markdown","7944d781":"markdown","e6c4f8a5":"markdown","2f47bd1d":"markdown","3e11c909":"markdown","701f4a22":"markdown","59b96c22":"markdown","e273d897":"markdown","13478062":"markdown","679c8b8e":"markdown","3b9075d3":"markdown","1bbf1205":"markdown","05c60094":"markdown","73a07f0f":"markdown","db6ce34f":"markdown","3d1ed724":"markdown","9abe5e3d":"markdown","b076c868":"markdown","bdc4a23e":"markdown","91beff14":"markdown","35bd4112":"markdown","331504dc":"markdown","16e18d2a":"markdown","13d2daca":"markdown","5c827ae7":"markdown","f1e50942":"markdown","01071dd9":"markdown","abec693d":"markdown","245a4988":"markdown","e4e679e5":"markdown","4b21817f":"markdown","8733ec41":"markdown","6546daa8":"markdown","3fd2a0e0":"markdown","e0402539":"markdown","90a3aa07":"markdown","0e2986d1":"markdown","675ec84c":"markdown","5aaa0477":"markdown","809271bb":"markdown","0e7599ca":"markdown","b31c6269":"markdown","2556ae31":"markdown","2cf2ac5f":"markdown","f446e111":"markdown","9ef39865":"markdown","44da37ba":"markdown","05ddbb4c":"markdown","92cebddf":"markdown","60c1b34e":"markdown","9747a7da":"markdown","ddae62f7":"markdown","a5923d78":"markdown","7cdb900d":"markdown","8a236b0c":"markdown","5bdb9c82":"markdown","37a35ca8":"markdown","78f90f3e":"markdown","93758f36":"markdown","590770bb":"markdown","b3292b8e":"markdown"},"source":{"dbd67adb":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import classification_report, confusion_matrix\ntrain = pd.read_csv(\"..\/input\/random-linear-regression\/train.csv\") \ntest = pd.read_csv(\"..\/input\/random-linear-regression\/test.csv\") \ntrain = train.dropna()\ntest = test.dropna()\ntrain.head()\n","30454635":"X_train = np.array(train.iloc[:, :-1].values)\ny_train = np.array(train.iloc[:, 1].values)\nX_test = np.array(test.iloc[:, :-1].values)\ny_test = np.array(test.iloc[:, 1].values)\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\n\nplt.plot(X_train, model.predict(X_train), color='green')\nplt.show()\nprint(accuracy)","113b389f":"from sklearn.linear_model import  TheilSenRegressor\nmodel = TheilSenRegressor()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\nprint(accuracy)","3e2e8306":"from sklearn.linear_model import  RANSACRegressor\nmodel = RANSACRegressor()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\nprint(accuracy)","5227f2d2":"from sklearn.linear_model import  HuberRegressor\nmodel = HuberRegressor()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\nprint(accuracy)","05b4d3b9":"import sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import r2_score\nfrom statistics import mode\n\n\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest  = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain.head()","1dad13da":"ports = pd.get_dummies(train.Embarked , prefix='Embarked')\ntrain = train.join(ports)\ntrain.drop(['Embarked'], axis=1, inplace=True)\ntrain.Sex = train.Sex.map({'male':0, 'female':1})\ny = train.Survived.copy()\nX = train.drop(['Survived'], axis=1) \nX.drop(['Cabin'], axis=1, inplace=True) \nX.drop(['Ticket'], axis=1, inplace=True) \nX.drop(['Name'], axis=1, inplace=True) \nX.drop(['PassengerId'], axis=1, inplace=True)\nX.Age.fillna(X.Age.median(), inplace=True) \n","edc2fd76":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(max_iter = 500000)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\nprint(accuracy)\n","7ac3e000":"print(confusion_matrix(y_test,y_pred))\n","2c6c3852":"print(classification_report(y_test,y_pred))","c0897c37":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nmodel = GaussianProcessClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\nprint(accuracy)\n","36f61692":"print(classification_report(y_test,y_pred))","e0e92e10":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\ndata_svm = pd.read_csv(\"..\/input\/svm-classification\/UniversalBank.csv\")\ndata_svm.head()","b9181800":"X = data_svm.iloc[:,1:13].values\ny = data_svm.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\naccuracies.mean()","bd46ff57":"print(classification_report(y_test,y_pred))","d1856c8a":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.svm import NuSVC\nnu_svm = pd.read_csv(\"..\/input\/svm-classification\/UniversalBank.csv\")\nnu_svm.head()","4e206960":"X = nu_svm.iloc[:,1:13].values\ny = nu_svm.iloc[:, -1].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\nclassifier = NuSVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_test)\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\naccuracies.mean()","5bfb505d":"print(classification_report(y_test,y_pred))","99c90de6":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\ndata = pd.read_csv('..\/input\/classification-suv-dataset\/Social_Network_Ads.csv')\ndata_nb = data\ndata_nb.head()","beadbc48":"X = data_nb.iloc[:, [2,3]].values\ny = data_nb.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=GaussianNB()\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","7979b0d7":"print(classification_report(y_test,y_pred))","970af7cb":"X = data_nb.iloc[:, [2,3]].values\ny = data_nb.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=BernoulliNB()\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","d2bfbb25":"print(classification_report(y_test,y_pred))","8e14d3f6":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import KNeighborsRegressor\nknn = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\nknn.head()","bbc10790":"X = knn.iloc[:, [1,2,3,4]].values\ny = knn.iloc[:, 5].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","2c0a3e4c":"print(classification_report(y_test,y_pred))","2f4f0fb8":"from sklearn.neighbors import KNeighborsRegressor\ntrain = pd.read_csv(\"..\/input\/random-linear-regression\/train.csv\") \ntest = pd.read_csv(\"..\/input\/random-linear-regression\/test.csv\") \ntrain = train.dropna()\ntest = test.dropna()\nX_train = np.array(train.iloc[:, :-1].values)\ny_train = np.array(train.iloc[:, 1].values)\nX_test = np.array(test.iloc[:, :-1].values)\ny_test = np.array(test.iloc[:, 1].values)","f9af2503":"model = KNeighborsRegressor(n_neighbors=2)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\naccuracy = model.score(X_test, y_test)\nprint(accuracy)","a404035c":"from sklearn.linear_model import Perceptron\nfrom sklearn.neighbors import KNeighborsClassifier\np = pd.read_csv(\"..\/input\/iris\/Iris.csv\")\np.head()","a5339b6b":"X = p.iloc[:, [1,2,3,4]].values\ny = p.iloc[:, 5].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=Perceptron()\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","65c7d423":"print(classification_report(y_test,y_pred))","943a09f0":"from sklearn.ensemble import RandomForestClassifier\nrf = pd.read_csv(\"..\/input\/mushroom-classification\/mushrooms.csv\")\nrf.head()","d9895604":"X = rf.drop('class', axis=1)\ny = rf['class']\nX = pd.get_dummies(X)\ny = pd.get_dummies(y)\nX_train, X_test, y_train, y_test = train_test_split(X, y)\nmodel = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=1)\nmodel.fit(X_train, y_train)\nmodel.score(X_test, y_test)","4b53497f":"from sklearn.tree import DecisionTreeClassifier\ndt = data\ndt.head()","d54169db":"X = dt.iloc[:, [2,3]].values\ny = dt.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=DecisionTreeClassifier(criterion=\"entropy\",random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","0fa11985":"from sklearn.ensemble import ExtraTreesClassifier\net = data\net.head()","1d8b0d92":"X = et.iloc[:, [2,3]].values\ny = et.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=ExtraTreesClassifier(criterion=\"entropy\",random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","3fd33e9e":"from sklearn.ensemble import AdaBoostClassifier\nac = data\nac.head()","baee2ca6":"X = ac.iloc[:, [2,3]].values\ny = ac.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=AdaBoostClassifier(random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","7f4acb37":"from sklearn.linear_model import PassiveAggressiveClassifier\npac = data\npac.head()","1b43a23e":"X = pac.iloc[:, [2,3]].values\ny = pac.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=PassiveAggressiveClassifier(random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","05ec10a8":"from sklearn.ensemble import BaggingClassifier\nbc = data\nbc.head()","f1c068aa":"X = bc.iloc[:, [2,3]].values\ny = bc.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nclassifier=BaggingClassifier(random_state=0)\nclassifier.fit(X_train,y_train)\ny_pred=classifier.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","230af5b5":"from sklearn.ensemble import GradientBoostingClassifier\ngb = data\ngb.head()","090b1c87":"X = gb.iloc[:, [2,3]].values\ny = gb.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\npred = gbk.predict(X_test)\nacc=accuracy_score(y_test, y_pred)\nprint(acc)","1930fab2":"import lightgbm as lgbm\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn import preprocessing\n\n\ntrain = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndata = pd.concat([train, test], sort=False)\ndata = data.reset_index(drop=True)\ndata.head()","16abf269":"nans=pd.isnull(data).sum()\n\ndata['MSZoning']  = data['MSZoning'].fillna(data['MSZoning'].mode()[0])\ndata['Utilities'] = data['Utilities'].fillna(data['Utilities'].mode()[0])\ndata['Exterior1st'] = data['Exterior1st'].fillna(data['Exterior1st'].mode()[0])\ndata['Exterior2nd'] = data['Exterior2nd'].fillna(data['Exterior2nd'].mode()[0])\n\ndata[\"BsmtFinSF1\"]  = data[\"BsmtFinSF1\"].fillna(0)\ndata[\"BsmtFinSF2\"]  = data[\"BsmtFinSF2\"].fillna(0)\ndata[\"BsmtUnfSF\"]   = data[\"BsmtUnfSF\"].fillna(0)\ndata[\"TotalBsmtSF\"] = data[\"TotalBsmtSF\"].fillna(0)\ndata[\"BsmtFullBath\"] = data[\"BsmtFullBath\"].fillna(0)\ndata[\"BsmtHalfBath\"] = data[\"BsmtHalfBath\"].fillna(0)\ndata[\"BsmtQual\"] = data[\"BsmtQual\"].fillna(\"None\")\ndata[\"BsmtCond\"] = data[\"BsmtCond\"].fillna(\"None\")\ndata[\"BsmtExposure\"] = data[\"BsmtExposure\"].fillna(\"None\")\ndata[\"BsmtFinType1\"] = data[\"BsmtFinType1\"].fillna(\"None\")\ndata[\"BsmtFinType2\"] = data[\"BsmtFinType2\"].fillna(\"None\")\n\ndata['KitchenQual']  = data['KitchenQual'].fillna(data['KitchenQual'].mode()[0])\ndata[\"Functional\"]   = data[\"Functional\"].fillna(\"Typ\")\ndata[\"FireplaceQu\"]  = data[\"FireplaceQu\"].fillna(\"None\")\n\ndata[\"GarageType\"]   = data[\"GarageType\"].fillna(\"None\")\ndata[\"GarageYrBlt\"]  = data[\"GarageYrBlt\"].fillna(0)\ndata[\"GarageFinish\"] = data[\"GarageFinish\"].fillna(\"None\")\ndata[\"GarageCars\"] = data[\"GarageCars\"].fillna(0)\ndata[\"GarageArea\"] = data[\"GarageArea\"].fillna(0)\ndata[\"GarageQual\"] = data[\"GarageQual\"].fillna(\"None\")\ndata[\"GarageCond\"] = data[\"GarageCond\"].fillna(\"None\")\n\ndata[\"PoolQC\"] = data[\"PoolQC\"].fillna(\"None\")\ndata[\"Fence\"]  = data[\"Fence\"].fillna(\"None\")\ndata[\"MiscFeature\"] = data[\"MiscFeature\"].fillna(\"None\")\ndata['SaleType']    = data['SaleType'].fillna(data['SaleType'].mode()[0])\ndata['LotFrontage'].interpolate(method='linear',inplace=True)\ndata[\"Electrical\"]  = data.groupby(\"YearBuilt\")['Electrical'].transform(lambda x: x.fillna(x.mode()[0]))\ndata[\"Alley\"] = data[\"Alley\"].fillna(\"None\")\n\ndata[\"MasVnrType\"] = data[\"MasVnrType\"].fillna(\"None\")\ndata[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(0)\nnans=pd.isnull(data).sum()\nnans[nans>0]","e114c5c0":"_list = []\nfor col in data.columns:\n    if type(data[col][0]) == type('str'): \n        _list.append(col)\n\nle = preprocessing.LabelEncoder()\nfor li in _list:\n    le.fit(list(set(data[li])))\n    data[li] = le.transform(data[li])\n\ntrain, test = data[:len(train)], data[len(train):]\n\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\n\ntest = test.drop(columns=['SalePrice', 'Id'])","c37bda76":"kfold = KFold(n_splits=5, random_state = 2020, shuffle = True)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\nmodel_lgb.fit(X, y)\nr2_score(model_lgb.predict(X), y)\n","5919498b":"import xgboost as xgb\n#Data is used the same as LGB\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\nX.head()","f41df08a":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_xgb.fit(X, y)\nr2_score(model_xgb.predict(X), y)\n","2afa61cf":"from catboost import CatBoostRegressor\n#Data is used the same as LGB\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\nX.head()","1a054dd1":"cb_model = CatBoostRegressor(iterations=500,\n                             learning_rate=0.05,\n                             depth=10,\n                             random_seed = 42,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 50,\n                             od_wait=20)\ncb_model.fit(X, y)\nr2_score(cb_model.predict(X), y)\n","eaaefd71":"from sklearn.linear_model import SGDRegressor\n#Data is used the same as LGB\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\nX.head()","9f5495b3":"SGD = SGDRegressor(max_iter = 100)\nSGD.fit(X, y)\nr2_score(SGD.predict(X), y)\n","7e1b4fb3":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n#Data is used the same as LGB\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\nX.head()","cb2da9cc":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nlasso.fit(X, y)\nr2_score(lasso.predict(X), y)\n","2d675410":"from sklearn.linear_model import RidgeClassifierCV\n#Data is used the same as LGB\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\nX.head()","0708cc8f":"rcc = RidgeClassifierCV()\nrcc.fit(X, y)\nr2_score(rcc.predict(X), y)\n","db85f0fa":"from sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n#Data is used the same as LGB\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\nX.head()","d51e916f":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nKRR.fit(X, y)\nr2_score(KRR.predict(X), y)","6759f648":"from sklearn.linear_model  import BayesianRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n#Data is used the same as LGB\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\nX.head()","ab28c386":"BR = BayesianRidge()\nBR.fit(X, y)\nr2_score(BR.predict(X), y)","7d1fcec6":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n#Data is used the same as LGB\nX = train.drop(columns=['SalePrice', 'Id']) \ny = train['SalePrice']\nX.head()","96624cf7":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nENet.fit(X, y)\nr2_score(ENet.predict(X), y)","3730af8a":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = data\nlda.head()","4eaadd77":"X = lda.iloc[:, [2,3]].values\ny = lda.iloc[:, 4].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\nModel=LinearDiscriminantAnalysis()\nModel.fit(X_train,y_train)\ny_pred=Model.predict(X_test)\nprint('accuracy is ',accuracy_score(y_pred,y_test))","59002d1d":"from sklearn.cluster import KMeans\nkm = pd.read_csv(\"..\/input\/k-mean\/km.csv\")\nkm.head()","eabc9120":"K_clusters = range(1,8)\nkmeans = [KMeans(n_clusters=i) for i in K_clusters]\nY_axis = km[['latitude']]\nX_axis = km[['longitude']]\nscore = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]\nplt.plot(K_clusters, score)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Score')\nplt.show()","2502de56":"kmeans = KMeans(n_clusters = 3, init ='k-means++')\nkmeans.fit(km[km.columns[1:3]])\nkm['cluster_label'] = kmeans.fit_predict(km[km.columns[1:3]])\ncenters = kmeans.cluster_centers_\nlabels = kmeans.predict(km[km.columns[1:3]])\nkm.cluster_label.unique()","11c0cfde":"km.plot.scatter(x = 'latitude', y = 'longitude', c=labels, s=50, cmap='viridis')\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=100, alpha=0.5)\n","c0514e68":"from sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nimport tensorflow as tf\ntrain_data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntrain_data.head()","f9ecefe7":"X = np.array(train_data.drop(\"label\", axis=1)).astype('float32')\ny = np.array(train_data['label']).astype('float32')\nfor i in range(9):\n    plt.subplot(3,3,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(X[i].reshape(28, 28), cmap=plt.cm.binary)\n    plt.xlabel(y[i])\nplt.show()\n\nX = X \/ 255.0\nX = X.reshape(-1, 28, 28, 1)\ny = to_categorical(y)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\nX_test = np.array(test_data).astype('float32')\nX_test = X_test \/ 255.0\nX_test = X_test.reshape(-1, 28, 28, 1)\nplt.figure(figsize=(10,10))\n","e2ed1c59":"model = Sequential()\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\nmodel.summary()\nfrom tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model1.png')\n","61a80682":"#increse to epochs to 30 for better accuracy\nmodel.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=85, validation_data=(X_val, y_val))","3fbaf5a7":"accuracy = history.history['accuracy']\nval_accuracy = history.history['val_accuracy']\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.show()\n\nprint(model.evaluate(X_val, y_val))\n","7bb19354":"prediction = model.predict_classes(X_test)\nsubmit = pd.DataFrame(prediction,columns=[\"Label\"])\nsubmit[\"ImageId\"] = pd.Series(range(1,(len(prediction)+1)))\nsubmission = submit[[\"ImageId\",\"Label\"]]\nsubmission.to_csv(\"submission.csv\",index=False)\n","f175a265":"import math\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nlstm = pd.read_csv(\"..\/input\/nyse\/prices.csv\")\nlstm = lstm[lstm['symbol']==\"NFLX\"]\nlstm['date'] = pd.to_datetime(lstm['date'])\nlstm.set_index('date',inplace=True)\nlstm = lstm.reset_index()\nlstm.head()","18f9a0de":"data = lstm.filter(['close'])\ndataset = data.values \ntraining_data_len = math.ceil(len(dataset)*.75)  \nscaler = MinMaxScaler(feature_range=(0,1))\nscaled_data = scaler.fit_transform(dataset)\ntrain_data = scaled_data[0:training_data_len, :]\nx_train = []\ny_train = []\nfor i in range(60,len(train_data)):\n    x_train.append(train_data[i-60:i, 0])\n    y_train.append(train_data[i,0])\nx_train,y_train = np.array(x_train), np.array(y_train)\nx_train = np.reshape(x_train,(x_train.shape[0],x_train.shape[1],1))\n","a634fcee":"model =Sequential()\nmodel.add(LSTM(64,return_sequences=True, input_shape=(x_train.shape[1],1)))\nmodel.add(LSTM(64, return_sequences= False))\nmodel.add(Dense(32))\nmodel.add(Dense(1))\nmodel.summary()\nfrom tensorflow.keras.utils import plot_model \nplot_model(model, to_file='model1.png')\n","a55c09c8":"model.compile(optimizer='adam', loss='mean_squared_error')\nmodel.fit(x_train,y_train, batch_size=85, epochs=20)\n","89afca3d":"test_data= scaled_data[training_data_len-60:, :]\nx_test = []\ny_test = dataset[training_data_len:,:]\nfor i in range(60,len(test_data)):\n    x_test.append(test_data[i-60:i,0])\nx_test = np.array(x_test)\nx_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1],1))\npredictions = model.predict(x_test)\npredictions = scaler.inverse_transform(predictions)\nrmse = np.sqrt(np.mean(predictions - y_test)**2)\nrmse","a5836ec4":"from sklearn.datasets import make_blobs\nfrom sklearn import datasets\nclass PCA:\n  def __init__(self, n_components):\n    self.n_components = n_components\n    self.components = None\n    self.mean = None\n\n  def fit(self, X):\n    self.mean = np.mean(X, axis=0)\n    X = X - self.mean\n    cov = np.cov(X.T)\n\n    evalue, evector = np.linalg.eig(cov)\n\n    eigenvectors = evector.T\n    idxs = np.argsort(evalue)[::-1]\n    \n    evalue = evalue[idxs]\n    evector = evector[idxs]\n    self.components = evector[0:self.n_components]\n\n  def transform(self, X):\n    #project data\n    X = X - self.mean\n    return(np.dot(X, self.components.T))\n\ndata = datasets.load_iris()\nX = data.data\ny = data.target\n\npca = PCA(2)\npca.fit(X)\nX_projected = pca.transform(X)\n\n\n\nx1 = X_projected[:,0]\nx2 = X_projected[:,1]\n\nplt.scatter(x1,x2,c=y,edgecolor='none',alpha=0.8,cmap=plt.cm.get_cmap('viridis',3))\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.colorbar()\nplt.show()","d19616a8":"df = pd.read_csv('..\/input\/supermarket\/GroceryStoreDataSet.csv',names=['products'],header=None)\ndata = list(df[\"products\"].apply(lambda x:x.split(',')))\ndata","190cc8f9":"from mlxtend.frequent_patterns import apriori\nfrom mlxtend.preprocessing import TransactionEncoder\nte = TransactionEncoder()\nte_data = te.fit(data).transform(data)\ndf = pd.DataFrame(te_data,columns=te.columns_)\ndf1 = apriori(df,min_support=0.01,use_colnames=True)\ndf1.head()","8588092a":"import plotly.offline as py\nimport plotly.express as px\nfrom fbprophet import Prophet\nfrom fbprophet.plot import plot_plotly, add_changepoints_to_plot\n\npred = pd.read_csv(\"..\/input\/coronavirus-2019ncov\/covid-19-all.csv\")\npred = pred.fillna(0)\npredgrp = pred.groupby(\"Date\")[[\"Confirmed\",\"Recovered\",\"Deaths\"]].sum().reset_index()\npred_cnfrm = predgrp.loc[:,[\"Date\",\"Confirmed\"]]\npr_data = pred_cnfrm\npr_data.columns = ['ds','y']\npr_data.head()","241906a9":"m=Prophet()\nm.fit(pr_data)\nfuture=m.make_future_dataframe(periods=15)\nforecast=m.predict(future)\nforecast\n","2775bcbc":"fig = plot_plotly(m, forecast)\npy.iplot(fig) \n\nfig = m.plot(forecast,xlabel='Date',ylabel='Confirmed Count')","abe3b704":"import datetime\nfrom statsmodels.tsa.arima_model import ARIMA\nar = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\nar.date=ar.date.apply(lambda x:datetime.datetime.strptime(x, '%d.%m.%Y'))\nar=ar.groupby([\"date_block_num\"])[\"item_cnt_day\"].sum()\nar.index=pd.date_range(start = '2013-01-01',end='2015-10-01', freq = 'MS')\nar=ar.reset_index()\nar=ar.loc[:,[\"index\",\"item_cnt_day\"]]\nar.columns = ['confirmed_date','count']\nar.head()","e9f5982e":"model = ARIMA(ar['count'].values, order=(1, 2, 1))\nfit_model = model.fit(trend='c', full_output=True, disp=True)\nfit_model.summary()\n\n","09708b1c":"fit_model.plot_predict()\nplt.title('Forecast vs Actual')\npd.DataFrame(fit_model.resid).plot()\nforcast = fit_model.forecast(steps=6)\npred_y = forcast[0].tolist()\npred = pd.DataFrame(pred_y)\n","476f8cb6":"**Compiling Model**","56dad1d1":"**Library and Data**","4f79de71":"**Library and Data**","4313859e":"**Library and Data**","57bc43e6":"**Library and Data**","bb02b0b5":"# Unsupervised Machine Learning\n\n**Unsupervised learning is the training of an algorithm using information that is neither classified nor labeled and allowing the algorithm to act on that information without guidance.The main idea behind unsupervised learning is to expose the machines to large volumes of varied data and allow it to learn and infer from the data. However, the machines must first be programmed to learn from data. **\n\n** Unsupervised learning problems can be further grouped into clustering and association problems.  \n**\n1. Clustering: A clustering problem is where you want to discover the inherent groupings in the data, such as grouping customers by purchasing behaviour. \n2. Association: An association rule learning problem is where you want to discover rules that describe large portions of your data, such as people that buy X also tend to buy Y. \n\n\n","d30bd399":"**Model**","46790e97":"**Library and Data**","a0bc856a":"# Lasso","8c923f01":"**Library and Data**","8de14883":"**Model and Accuracy**","49973012":"# Kernel Ridge Regression","2d2887c4":"# TheilSen Regressor","5d8f7a00":"**Library and Data**","738b4769":"**Machine Learning is the science of getting computers to learn and act like humans do, and improve their learning over time in autonomous fashion, by feeding them data and information in the form of observations and real-world interactions.\nThere are many algorithm for getting machines to learn, from using basic decision trees to clustering to layers of artificial neural networks depending on what task you\u2019re trying to accomplish and the type and amount of data that you have available.  \n**","0192e525":"**LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:**\n\n1. Faster training speed and higher efficiency.\n2. Lower memory usage.\n3. Better accuracy.\n4. Support of parallel and GPU learning.\n5. Capable of handling large-scale data.","f9eaa003":"# If you like this notebook, do hit upvote\n# Thanks\n\n","1b1a7825":"# Perceptron ","9ff9b679":"# Prophet","0ae36324":"# K-Means Algorithm \nK-means clustering is a type of unsupervised learning, which is used when you have unlabeled data and the goal of this algorithm is to find groups in the data \n\n**Steps to use this algorithm:-**\n* 1-Clusters the data into k groups where k is predefined. \n* 2-Select k points at random as cluster centers. \n* 3-Assign objects to their closest cluster center according to the Euclidean distance function. \n* 4-Calculate the centroid or mean of all objects in each cluster. \n\n**Examples: Behavioral segmentation like segment by purchase history or by activities on application, website, or platform Separate valid activity groups from bots  **\n","89653a90":"**LSTM  blocks are part of a recurrent neural network structure. Recurrent neural networks are made to utilize certain types of artificial memory processes that can help these artificial intelligence programs to more effectively imitate human thought.It is  capable of learning order dependence \nLSTM can be used for machine translation, speech recognition, and more.**","f2c5c9b0":"**Libraries and Data**","66cdf222":"# **LDA**","8ca0f06e":"**Model and Accuracy**","5b3aad2f":"**Libraries and Data**","b9f2add7":"**Report**","c746bf71":"**Model and Accuracy**","bf30d594":"**Model and Accuracy**","82bfbd19":"**Model and Accuracy**","7a1682c1":"**Model and Accuracy**","fa874ba9":"**Confusion Matrix**","25bd5ea1":"**Model and Forecast**","8b14b43f":"**Elastic net is a hybrid of ridge regression and lasso regularization.It combines feature elimination from Lasso and feature coefficient reduction from the Ridge model to improve your model's predictions.**","c3f7d902":"**Library and Data**","3ad23775":"# **XGBoost**","942d479e":"**Model with plots and accuracy**","b37c8cf3":"**Libraries and Data**","9e2da666":"**Model and Accuracy**","30db3a74":"**Preprocessing and Data Split**","8e5ca430":"**Model and  Accuracy**","d7fcd56f":"**Model and Accuracy**","98901655":"# RANSAC Regressor","b25c3e21":"1. Prophet only takes data as a dataframe with a ds (datestamp) and y (value we want to forecast) column. So first, let\u2019s convert the dataframe to the appropriate format.\n1. Create an instance of the Prophet class and then fit our dataframe to it.\n2. Create a dataframe with the dates for which we want a prediction to be made with make_future_dataframe(). Then specify the number of days to forecast using the periods parameter.\n3. Call predict to make a prediction and store it in the forecast dataframe. What\u2019s neat here is that you can inspect the dataframe and see the predictions as well as the lower and upper boundaries of the uncertainty interval.\n","8d274f9d":"**Library and Data**","3767b264":"**Prediction and Accuracy**","b3363602":"# Principle Component Analysis","a53b6fc5":"**Model and Accuracy**","7f3b780a":"# Bagging Classifier","5687d559":"# Extra Tree","2b1f2918":"**Library and Data**","4b2f84e4":"\nProphet is an extremely easy tool for analysts to produce reliable forecasts","07998b9b":"**Model and Accutacy**","7eff9c43":"**Library and Data**","557804c4":"**Library and Data**","1280c338":"**It is a categorisation algorithm attempts to operate on database records, particularly transactional records, or records including certain numbers of fields or items.It is mainly used for sorting large amounts of data. Sorting data often occurs because of association rules. **\n* Example: To analyse data for frequent if\/then patterns and using the criteria support and confidence to identify the most important relationships. ","b344f50f":"# **Evaluate Algorithms** \n**The evaluation of algorithm consist three following steps:- **\n1. Test Harness  \n2. Explore and select algorithms \n3. Interpret and report results \n\n","77ba3e9a":"**Library and Data **","f384001e":"**Library and Data**","4014aa6a":"# Table of Content\n1. Machine Learning and Types\n2. Application of Machine Learning\n3. Steps of Machine Learning\n4. Factors help to choose algorithm\n5. Algorithm\n         Linear Regression\n         TheilSenRegressor\n         RANSAC Regressor\n         HuberRegressor\n         Logistic Regression\n         GaussianProcessClassifier\n         Support Vector Machine\n         Nu-Support Vector Classification\n         Naive Bayes Algorithm\n         KNN\n         Perceptron\n         Random Forest\n         Decision Tree\n         Extra Tree\n         AdaBoost Classifier\n         PassiveAggressiveClassifier\n         Bagging Classifier\n         Gradient Boosting\n                 Light GBM\n                 XGBoost\n                 Catboost\n                 Stochastic Gradient Descent\n         Lasso\n         RidgeC lassifier CV\n         Kernel Ridge Regression\n         Bayesian Ridge\n         Elastic Net Regression\n         LDA\n         K-Means Algorithm\n         CNN\n         LSTM\n         PCA\n         Apriori\n         Prophet\n         ARIMA\n6. Evaluate Algorithms\n                \n   \n","71f472bd":"**Prediction**","36001612":"**Library and Data**","94412349":"# Arima","f7d1c333":"**Model and Accuracy**","ae71c335":"**Libraries and Data**","4c187d83":"**Compiling model**","dceea13b":"**Plotting Clusters**","70f49c7b":"**Fitting Model**","78db8f98":"# Decision Tree\n**Decision tree algorithm is classification algorithm under supervised machine learning and it is simple to understand and use in data.The idea of Decision tree is to split the big data(root) into smaller(leaves)**","882d9046":"**As Regression**","42d0a14f":"**Model and Accuracy**","4bf1b99d":"**Model**","18fed4ef":"** Bayesian regression, is a regression model defined in probabilistic terms, with explicit priors on the parameters. The choice of priors can have the regularizing effect.Bayesian approach is a general way of defining and estimating statistical models that can be applied to different models.**","6fbc7028":"**Library and Data**","0d524298":"#  Supervised Machine Learning \n\n**It is a type of learning in which both input and desired output data are provided. Input and output data are labeled for classification to provide a learning basis for future data processing.This algorithm consist of a target \/ outcome variable (or dependent variable) which is to be predicted from a given set of predictors (independent variables). Using these set of variables, we generate a function that map inputs to desired outputs. The training process continues until the model achieves a desired level of accuracy on the training data.   \n**","32106526":"# Support Vector Machine \n**Support Vector Machines are perhaps one of the most popular and talked about machine learning algorithms.It is primarily a classier method that performs classification tasks by constructing hyperplanes in a multidimensional space that separates cases of different class labels. SVM supports both regression and classification tasks and can handle multiple continuous and categorical variables \n**\n\n**Example: One class is linearly separable from the others like if we only had two features like Height and Hair length of an individual, we\u2019d first plot these two variables in two dimensional space where each point has two co-ordinates **","7e6ae660":"** It is single layer neural network and used for classification **","7944d781":"# For learning Data Visualizaiton and NLP do check following notebooks\n# [Data Visualization](https:\/\/www.kaggle.com\/vanshjatana\/data-visualization)\n # [NLP](https:\/\/www.kaggle.com\/vanshjatana\/text-classification)","e6c4f8a5":"**Gaussian NB**","2f47bd1d":"# Linear Regression \n**It is a basic and commonly used type of predictive analysis. These regression estimates are used to explain the relationship between one dependent variable and one or more independent variables. \nY = a + bX where **\n* Y \u2013 Dependent Variable \n* a \u2013 intercept \n* X \u2013 Independent variable \n* b \u2013 Slope \n\n**Example: University GPA' = (0.675)(High School GPA) + 1.097**","3e11c909":"**Checking for number of clusters**","701f4a22":"# Ridge Classifier CV","59b96c22":"# BayesianRidge","e273d897":"# Application of Reinforcement Machine Learning \n1. Resources management in computer clusters \n2. Traffic Light Control \n3. Robotics \n4. Web System Configuration \n5. Personalized Recommendations \n6. Deep Learning \n","13478062":"**Model and Accuracy**","679c8b8e":"# Light GBM","3b9075d3":"**Libraries and Data**","1bbf1205":"**Catboost is a type of gradient boosting algorithms which can  automatically deal with categorical variables without showing the type conversion error, which helps you to focus on tuning your model better rather than sorting out trivial errors.Make sure you handle missing data well before you proceed with the implementation.\n**","05c60094":"# Passive Aggressive Classifier","73a07f0f":"# Machine Learning","db6ce34f":"**Model and Accuracy**","3d1ed724":"# Reinforcement Machine Learning \n**Reinforcement Learning is a type of Machine Learning which allows machines to automatically determine the ideal behaviour within a specific context, in order to maximize its performance. Simple reward feedback is required for the agent to learn its behaviour; this is known as the reinforcement signal.It differs from standard supervised learning, in that correct input\/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is on performance, which involves finding a balance between exploration of uncharted territory and exploitation of current knowledge  \n**\n","9abe5e3d":"**Preprocessing**","b076c868":"**There are three types of machine learning** \n1. Supervised Machine Learning \n2. Unsupervised Machine Learning \n3. Reinforcement Machine Learning ","bdc4a23e":"**Model**","91beff14":"**A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix.Itis  used in statistics, pattern recognition, and machine learning to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.**","35bd4112":"**Preprocessing**","331504dc":"# AdaBoost Classifier","16e18d2a":"**Model and Accuracy**","13d2daca":"# Logistic Regression \n**It\u2019s a classification algorithm, that is used where the response variable is categorical. The idea of Logistic Regression is to find a relationship between features and probability of particular outcome.**   \n* odds= p(x)\/(1-p(x)) = probability of event occurrence \/ probability of not event occurrence \n\n**Example- When we have to predict if a student passes or fails in an exam when the number of hours spent studying is given as a feature, the response variable has two values, pass and fail. \n**","5c827ae7":"# Gradient Boosting\n**Gradient boosting is an alogithm under supervised machine learning, boosting means converting weak into strong. In this new tree is boosted over the previous tree**","f1e50942":"**Library and Data**","01071dd9":"**KRR combine Ridge regression and classification with the kernel trick.It is similar to Support vector Regression but relatively very fast.This is suitable for smaller dataset (less than 100 samples)**","abec693d":"# Gaussian Process Classifier","245a4988":"# Stochastic Gradient Descent","e4e679e5":"# Huber Regressor","4b21817f":"# Factors help to choose algorithm \n1. Type of algorithm \n2. Parametrization \n3. Memory size \n4. Overfitting tendency \n5. Time of learning \n6. Time of predicting","8733ec41":"**Library and Data**","6546daa8":"**It's an important method for dimension reduction.It extracts low dimensional set of features from a high dimensional data set with a motive to capture as much information as possible and to visualise high-dimensional data, it also reduces noise and finally makes other algorithms to work better because we are injecting fewer inputs.**\n* Example: When we have to bring out strong patterns in a data set or to make data easy to explore and visualize","3fd2a0e0":"**Model and Accuracy**","e0402539":"**Model and Accuracy**","90a3aa07":"# Random Forest \n**Random forest is collection of tress(forest) and it builds multiple decision trees and merges them together to get a more accurate and stable prediction.It can be used for both classification and regression problems.**\n\n**Example: Suppose we have a bowl of 100 unique numbers from 0 to 99. We want to select a random sample of numbers from the bowl. If we put the number back in the bowl, it may be selected more than once. \n**","0e2986d1":"**BernoulliNB**","675ec84c":"**Model and Accuracy**","5aaa0477":"**Library and Data**","809271bb":"# Nu Support Vector Classification","0e7599ca":"# We can apply machine learning model by following six steps:-\n1. Problem Definition \n2. Analyse Data \n3. Prepare Data \n4. Evaluate Algorithm \n5. Improve Results \n6. Present Results \n","b31c6269":"# KNN \n**KNN does not learn any model. and stores the entire training data set which it uses as its representation.The output can be calculated as the class with the highest frequency from the K-most similar instances. Each instance in essence votes for their class and the class with the most votes is taken as the prediction \n**\n\n**Example: Should the bank give a loan to an individual? Would an individual default on his or her loan? Is that person closer in characteristics to people who defaulted or did not default on their loans? **\n","2556ae31":"# Application of Unsupervised Machine Learning \n1. Human Behaviour Analysis \n2. Social Network Analysis to define groups of friends. \n3. Market Segmentation of companies by location, industry, vertical. \n4. Organizing computing clusters based on similar event patterns and processes. \n","2cf2ac5f":"# Apriori","f446e111":"# Application of Supervised Machine Learning \n1. Bioinformatics \n2. Quantitative structure \n3. Database marketing \n4. Handwriting recognition \n5. Information retrieval \n6. Learning to rank \n7. Information extraction \n8. Object recognition in computer vision \n9. Optical character recognition \n10. Spam detection \n11. Pattern recognition \n\n","9ef39865":"# CNN","44da37ba":"# Naive Bayes Algorithm \n**A naive Bayes classifier is not a single algorithm, but a family of machine learning algorithms which use probability theory to classify data with an assumption of independence between predictors It is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods    \n**\n\n**Example: Emails are given and we have to find the spam emails from that.A spam filter looks at email messages for certain key words and puts them in a spam folder if they match.**","05ddbb4c":"**Stochastic means random , so in Stochastic Gradient Descent dataset sample is choosedn random instead of the whole dataset.hough, using the whole dataset is really useful for getting to the minima in a less noisy or less random manner, but the problem arises when our datasets get really huge and for that SGD come in action**","92cebddf":"# LSTM ","60c1b34e":"# Catboost","9747a7da":"**Libraries and data**","ddae62f7":"**As Classifier**","a5923d78":"**Library and Data**","7cdb900d":"# Elastic Net Regression \n","8a236b0c":"**Library and  Data**","5bdb9c82":"**XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks.It is a perfect combination of software and hardware optimization techniques to yield superior results using less computing resources in the shortest amount of time.**","37a35ca8":"**Libraries and Data**","78f90f3e":"**Model and Accuracy**","93758f36":"**Model and  Accuracy**","590770bb":"**Model and Accuracy**","b3292b8e":"**In statistics and machine learning, lasso (least absolute shrinkage and selection operator; also Lasso or LASSO) is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces. Though originally defined for least squares, lasso regularization is easily extended to a wide variety of statistical models including generalized linear models, generalized estimating equations, proportional hazards models, and M-estimators, in a straightforward fashion**"}}