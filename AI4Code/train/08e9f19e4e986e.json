{"cell_type":{"daf70250":"code","7a1230b7":"code","d0bfdc84":"code","aadedf33":"code","f9311854":"code","a8562ad2":"code","dd94e018":"code","d6519f01":"code","44650f49":"code","cc93d41d":"code","2e4a8920":"code","34a4ee6b":"code","a64958df":"code","4f6d5815":"code","27786481":"code","dd0ce5e4":"code","365b5258":"code","808ae060":"code","05f6963f":"code","74ec881b":"code","df2c972d":"code","5f558989":"code","5e88bc13":"markdown","79e51fe0":"markdown","2734bd67":"markdown","5016355e":"markdown","7346c674":"markdown","27433136":"markdown","7e1c12fc":"markdown","aa62581f":"markdown","93a84736":"markdown","de37992d":"markdown","36dd75c9":"markdown","542aa6e3":"markdown","0ff86ff5":"markdown","520f4e5c":"markdown","491f98da":"markdown","aa02e5d3":"markdown","32dee774":"markdown","ab98bb6b":"markdown"},"source":{"daf70250":"import nltk\n\nnltk.download(\"gutenberg\")","7a1230b7":"hamlet_raw = nltk.corpus.gutenberg.raw('shakespeare-hamlet.txt')\nprint(hamlet_raw[:1000])","d0bfdc84":"from nltk.tokenize import sent_tokenize\n\nsentences = sent_tokenize(hamlet_raw)\n\nprint(sentences[:10])\n","aadedf33":"from nltk.tokenize import word_tokenize\n\nwords = word_tokenize(sentences[0])\n\nprint(words)","f9311854":"from nltk.corpus import stopwords\n\nstopwords_list = stopwords.words('english')\n\nprint(stopwords_list)\n\n#stopwords_list = stopwords.words('portuguese')\n\n#print(stopwords_list)","a8562ad2":"non_stopwords = [w for w in words if not w.lower() in stopwords_list]\nprint(non_stopwords)","dd94e018":"import string\npunctuation = string.punctuation\nprint(punctuation)","d6519f01":"non_punctuation = [w for w in non_stopwords if not w in punctuation]\n\n\nprint(non_punctuation)","44650f49":"from nltk import pos_tag\n\npos_tags = pos_tag(words)\n\nprint(pos_tags)","cc93d41d":"from nltk.stem.snowball import SnowballStemmer\n\nstemmer = SnowballStemmer('english')\n\nsample_sentence = \"He has already gone\"\nsample_words = word_tokenize(sample_sentence)\n\nstems = [stemmer.stem(w) for w in sample_words]\n\nprint(stems)","2e4a8920":"nltk.download('wordnet')","34a4ee6b":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nlemmatizer = WordNetLemmatizer()\n\npos_tags = nltk.pos_tag(sample_words)\n\nlemmas = []\nfor w in pos_tags:\n    if w[1].startswith('J'):\n        pos_tag = wordnet.ADJ\n    elif w[1].startswith('V'):\n        pos_tag = wordnet.VERB\n    elif w[1].startswith('N'):\n        pos_tag = wordnet.NOUN\n    elif w[1].startswith('R'):\n        pos_tag = wordnet.ADV\n    else:\n        pos_tag = wordnet.NOUN\n        \n    lemmas.append(lemmatizer.lemmatize(w[0], pos_tag))\n    \nprint(lemmas)","a64958df":"from nltk import word_tokenize\n\nfrase = 'o cachorro correu atr\u00e1s do gato'\n\n\nngrams = [\"%s %s %s\" % (nltk.word_tokenize(frase)[i], \\\n                      nltk.word_tokenize(frase)[i+1], \\\n                      nltk.word_tokenize(frase)[i+2]) \\\n          for i in range(len(nltk.word_tokenize(frase))-2)]\n\nprint(ngrams)\n","4f6d5815":"non_punctuation = [w for w in words if not w.lower() in punctuation]\n\nn_grams_3 = [\"%s %s %s\"%(non_punctuation[i], non_punctuation[i+1], non_punctuation[i+2]) for i in range(0, len(non_punctuation)-2)]\n\nprint(n_grams_3)","27786481":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vect = CountVectorizer(ngram_range=(2,3))\n\nimport numpy as np\n\nsent2 = ['o cachorro correu atr\u00e1s do gato que correu atr\u00e1s do rato',\n        'o gato correu atr\u00e1s do rato',\n        'o rato comeu a ra\u00e7\u00e3o',\n        'o cachorro comeu o rato que comeu a ra\u00e7\u00e3o',\n        'o gato comeu o rato']\n\narr = np.array(sent2)\n\nprint(arr)\n\nn_gram_counts = count_vect.fit_transform(arr)\n\nprint(n_gram_counts.toarray())\n\nprint(count_vect.vocabulary_)","dd0ce5e4":"arr = np.array(sentences)\n\nn_gram_counts = count_vect.fit_transform(arr)\n\nprint(n_gram_counts.toarray()[:20])\n\nprint([k for k in count_vect.vocabulary_.keys()][:20])","365b5258":"from nltk.corpus import reuters\narticles = reuters.fileids()\nreuters_list = \"\"\nfor name in articles:\n    reuters_list =  reuters_list + nltk.corpus.reuters.raw( name ) \nreuters_list","808ae060":"\nreuters = nltk.corpus.reuters.raw('test\/14826')\n\nsentences = sent_tokenize(reuters)\n\nwords = word_tokenize(reuters_list)\n#words = word_tokenize(sentences[0])\n#words = word_tokenize(reuters)","05f6963f":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nlemmatizer = WordNetLemmatizer()\n\npos_tags = nltk.pos_tag(words)\n\nlemmas = []\nfor w in pos_tags:\n    if w[1].startswith('J'):\n        pos_tag = wordnet.ADJ\n    elif w[1].startswith('V'):\n        pos_tag = wordnet.VERB\n    elif w[1].startswith('N'):\n        pos_tag = wordnet.NOUN\n    elif w[1].startswith('R'):\n        pos_tag = wordnet.ADV\n    else:\n        pos_tag = wordnet.NOUN\n        \n    lemmas.append(lemmatizer.lemmatize(w[0], pos_tag))\n    \n#print(lemmas)","74ec881b":"### Remover stop words e pontua\u00e7\u00f5es\nfrom nltk.corpus import stopwords\nimport string\n\nstopwords_list = stopwords.words('english')\npunctuation = string.punctuation\n\nlemmas_non_stopwords = [w for w in lemmas if not w.lower() in stopwords_list]\nlemmas_non_stopwords_punctuation = [w for w in lemmas_non_stopwords if not w in punctuation]\n","df2c972d":"word_list = {}\n\nfor key in lemmas_non_stopwords_punctuation:\n\n    # python check if key in dict using \"in\"\n    if key in word_list:\n        #print(f\"Yes, key: '{key}' exists in dictionary\")]\n        word_list[key] = word_list[key]+1\n    else:\n        #print(f\"No, key: '{key}' does not exists in dictionary\")\n        word_list[key] = 1\nword_list","5f558989":"\nsort_words = sorted(word_list.items(), key=lambda x: x[1], reverse=True)\n\nfor i in sort_words:\n    print(i[0], i[1])","5e88bc13":"As tags indicam a classifica\u00e7\u00e3o sint\u00e1tica de cada palavra no texto. Ver <a href=\"https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html\" target=\"blank\">https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html<\/a> para uma lista completa","79e51fe0":"<b>4. Removendo stopwords e pontua\u00e7\u00e3o<\/b>","2734bd67":"<h2> T\u00e9cnicas para Pr\u00e9-Processamento <\/h2>","5016355e":"<b>7. N-gramas<\/b>","7346c674":"<b>6. Stemming e Lemmatization<\/b>","27433136":"<h1 align=\"center\"> Aplica\u00e7\u00f5es em Processamento de Linguagem Natural <\/h1>\n<h2 align=\"center\"> Aula 02 - T\u00e9cnicas de Pr\u00e9-Processamento de Texto<\/h2>\n<h3 align=\"center\"> Prof. Fernando Vieira da Silva MSc.<\/h3>","7e1c12fc":"Tamb\u00e9m podemos usar a classe <b>CountVectorizer<\/b>, do scikit-learn:","aa62581f":"<p>Vamos avaliar as t\u00e9cnicas mais comuns para prepararmos o texto para usar com algoritmos de aprendizado de m\u00e1quina logo mais.<\/p>\n<p>Como estudo de caso, vamos usar o texto de <i>Hamlet<\/i>, encontrado no corpus <i>Gutenberg<\/i> do pacote <b>NLTK<\/b><\/p>","93a84736":"<b>3. Segmenta\u00e7\u00e3o de senten\u00e7as e tokeniza\u00e7\u00e3o de palavras<\/b>","de37992d":"Al\u00e9m da t\u00e9cnica de <i>Bag-of-Words<\/i>, outra op\u00e7\u00e3o \u00e9 utilizar n-gramas (onde \"n\" pode variar)","36dd75c9":"Consiste em dividir o texto em senten\u00e7as. Por\u00e9m, n\u00e3o \u00e9 uma tarefa t\u00e3o trivial quanto usar uma fun\u00e7\u00e3o split(\".\") no Python, por exemplo. Isso porque o uso do ponto (.) nem sempre \u00e9 empregado como ponto final em uma senten\u00e7a. Por exemplo:\n\n\"Dr. Rodolfo S. Silva, um renomado cardiologista, adquiriu um novo consult\u00f3rio na Av. Edson A. Nascimento pela quantia de R$ 4.5 milh\u00f5es, que atender\u00e1 pacientes do S.U.S. e do plano Sa\u00fade Mais Inc.\"\n\nUma das t\u00e9cnicas mais conhecidas para resolver esse problema \u00e9 o algoritmo Punkt. Trata-se de um modelo que consiste em identificar abrevia\u00e7\u00f5es, retic\u00eancias, iniciais e n\u00fameros ordinais para, a\u00ed ent\u00e3o, identificar os pontos finais em senten\u00e7as.\n\n![Arquitetura Punkt](https:\/\/www.researchgate.net\/profile\/Jan_Strunk\/publication\/220355311\/figure\/fig1\/AS:277323623485449@1443130512700\/Architecture-of-the-Punkt-System.png)","542aa6e3":"<b>5. Part of Speech (POS) Tags <\/b>","0ff86ff5":"<b>1. Baixando o corpus Gutenberg<\/b>","520f4e5c":"<p><b>Exerc\u00edcio 2:<\/b>Exiba 10 lemmas mais frequentes do corpus Reuters, ignorando pontua\u00e7\u00f5es e stopwords.<\/p>\n\n","491f98da":"J\u00e1 lemmatization vai al\u00e9m de somente remover sufixos, obtendo a raiz lingu\u00edstica da palavra. Vamos usar as tags POS obtidas anteriormente para otimizar o lemmatizer.","aa02e5d3":"Agora, vamos contar os n-grams (no nosso caso, trigramas) de todas as senten\u00e7as do texto:","32dee774":"Stemming permite obter a \"raiz\" da palavra, removendo sufixos, por exemplo.","ab98bb6b":"<b>2. Exibindo o texto \"Hamlet\"<\/b>"}}