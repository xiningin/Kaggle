{"cell_type":{"9f67674e":"code","a1e9c23e":"code","6bd73923":"code","00097c22":"code","584ed510":"code","c5675ccd":"code","ae03951c":"code","284015f3":"code","51380b0b":"code","255eb61c":"code","9201fd5d":"code","96d1a4cc":"code","3e88360b":"code","59f47e04":"code","7f59f3f6":"code","7bd16adf":"code","67357d44":"code","ae4747a1":"code","e3855efe":"code","725ebaf2":"code","6797c37c":"code","fe7e40cd":"code","c90438bc":"markdown"},"source":{"9f67674e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# suppressing warnings\nimport warnings\nwarnings.filterwarnings('ignore')","a1e9c23e":"# Read the data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\nX_test  = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\")\n\nX = train.drop('target', axis=1)\n# Select target\ny = pd.DataFrame([train['target']]).transpose()","6bd73923":"print(\"Train shape: \", train.shape, \"\\nTest shape: \", X_test.shape)","00097c22":"# Checking if there are missing values in the datasets\ntrain.isna().sum().sum(), X_test.isna().sum().sum()","584ed510":"#Getting summary statistics\ntrain.describe().T","c5675ccd":"X_test.describe().T","ae03951c":"# Select categorical columns \ncategorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]","284015f3":"# Select numerical columns\nnumerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]","51380b0b":"\n# Visualisation of all categorical variable relation \nfor col in categorical_cols: \n    sns.catplot(x=col, kind=\"count\", palette=\"ch:.25\", data=X)\n    \n","255eb61c":"# Visualisation of all numerical variables with Taget Variable\nfor col in numerical_cols: \n    plt.figure(figsize = (10,5))\n    sns.distplot(a = X[col], kde = False) ","9201fd5d":"plt.figure(figsize = (10,5))\nsns.distplot(a = y['target'], kde = False)","96d1a4cc":"# Checks if there is any variables with zero variance\nX.std() \n# Drops variables with 0 variance\n\nZero_std_cols = X.std()[X.std() == 0].index\n\nX = X.drop(Zero_std_cols, axis = 1) ","3e88360b":"# Dropping Categorical Variables with Zero Cardinality\nzero_cardinality = [] \n\nfor i in categorical_cols: \n    if len(X[i].value_counts().index) == 1: \n        zero_cardinality.append(i) \nprint('Categorical cols with zero_cardinality:', zero_cardinality)   \n        \nX = X.drop(zero_cardinality, axis = 1)\n\n# Dropping Categorical Variables with Many Levels (High Cardinality) \nhigh_cardinality = [] \n\nfor i in categorical_cols: \n    if len(X[i].value_counts().index) > 15: \n        high_cardinality.append(i) \n\nprint('Categorical cols with high_cardinality:', high_cardinality)       \n\nX = X.drop(high_cardinality, axis = 1)","59f47e04":"# Select categorical columns \nupdated_categorical_cols = [cname for cname in X.columns if X[cname].dtype == \"object\"]\n# Select numerical columns\nupdated_numerical_cols = [cname for cname in X.columns if X[cname].dtype in ['int64', 'float64']]\nupdated_numerical_cols.remove('id')\n\nuseful_features = updated_categorical_cols + updated_numerical_cols\nuseful_features","7f59f3f6":"from sklearn import preprocessing\n# creates a min max scaler for numerical columns\ndata_scaler = preprocessing.MinMaxScaler(feature_range=(0,1)) \nX[updated_numerical_cols] = data_scaler.fit_transform(X[updated_numerical_cols])\nX_test[updated_numerical_cols] = data_scaler.transform(X_test[updated_numerical_cols])","7bd16adf":"X_test[updated_numerical_cols]","67357d44":"#Ordinal endoing of categorical values\nordinal_encoder = preprocessing.OrdinalEncoder()\nX[updated_categorical_cols] = ordinal_encoder.fit_transform(X[updated_categorical_cols])\nX_test[updated_categorical_cols] = ordinal_encoder.transform(X_test[updated_categorical_cols])","ae4747a1":"#One_hot_encodoing categorical values\n#dummy_cat_df = pd.get_dummies(X[updated_categorical_cols], drop_first=True) \n# Drops categorical variables from the df\n#X = X.drop(updated_categorical_cols, axis = 1) \n# Adds the newly created dummy variables instead\n#X = pd.concat([X, dummy_cat_df], axis = 1) \n\n#dummy_cat_df_test = pd.get_dummies(X_test[updated_categorical_cols], drop_first=True) \n# Drops categorical variables from the df\n#X_test = X_test.drop(updated_categorical_cols, axis = 1) \n# Adds the newly created dummy variables instead\n#X_test = pd.concat([X_test, dummy_cat_df_test], axis = 1) ","e3855efe":"from sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n#Setting the kfold parameters\nkf = KFold(n_splits = 5, shuffle = True, random_state = 42)\n\n#print(type(kf))\n\n#oof_preds = np.zeros((X.shape[0],))\n\n#print('oof_preds:', oof_preds)\n\nmodel_fi = 0\nfinal_predictions = []\nscores = []\n\nfor num, (train_id, valid_id) in enumerate(kf.split(X)):\n    #print('length', len(train_id), len(valid_id))\n    #print ('num, train_id, valid_id', num, train_id, valid_id )\n    X_train, X_valid = X.loc[train_id], X.loc[valid_id]\n    y_train, y_valid = y.loc[train_id], y.loc[valid_id]\n    \n    print('shape', X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)\n    \n    model = XGBRegressor(n_estimators=1000, learning_rate=0.05, tree_method = 'gpu_hist')\n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             eval_metric = \"rmse\",\n             early_stopping_rounds = 100)\n      \n    #Mean of feature importance\n    model_fi += model.feature_importances_ \/ 5  #splits\n    #model_fi.append(model_fi_l)\n  \n    #Out of Fold predictions\n    #oof_preds[valid_id] = model.predict(X_valid)\n    val_preds = model.predict(X_valid)\n    fold_rmse = np.sqrt(mean_squared_error(y_valid, val_preds))\n    print(f\"Fold {num} | RMSE: {fold_rmse}\")\n    scores.append(fold_rmse)\n    \n    test_preds = model.predict(X_test)\n    final_predictions.append(test_preds)\n    \nprint('Overall RMSE & Std of Overall RMSE:', np.mean(scores), np.std(scores))\n","725ebaf2":"preds = np.mean(np.column_stack(final_predictions), axis=1)\npreds","6797c37c":"print('sub shape:', sub.shape)\nsub.target = preds\nsub.head()","fe7e40cd":"sub.to_csv(\"submission3.csv\", index=False)","c90438bc":"DATA VALIDATION AND PRE-PROCESSING"}}