{"cell_type":{"b4eba371":"code","c8b5bf5f":"code","5a848914":"code","f5329251":"code","d3aa228f":"code","e789cc82":"code","28f00f5c":"code","b47aa333":"code","10b0d6ac":"code","4abbe075":"code","09e9ded0":"code","41d21e9b":"code","fb8de55b":"code","4d0fb79b":"code","07a50af6":"code","4aac2b15":"code","6e920704":"code","6e5f7c18":"code","ae908448":"code","3354f4dc":"code","e3dfeba4":"code","9b5c4ae9":"code","069c2239":"code","4ba1d6bf":"code","b44ee9fa":"code","22e383b9":"code","59a73366":"code","f8be8ac2":"code","eb4946fb":"code","1d553d54":"code","02048432":"code","c138883c":"code","35420091":"code","56a8c9a5":"code","fa595ea6":"code","6727dac9":"code","30c2cf9b":"code","fa9213b8":"code","6e177db7":"code","9502a116":"code","b21866c3":"code","169679ed":"code","cba885a8":"code","f579cc1d":"code","6273c899":"code","81238314":"code","89e68328":"code","70aeb38e":"code","c89fca46":"code","a703f974":"code","e5a80091":"code","0d8e46ea":"code","301d9af4":"code","71ef4340":"code","f85762b7":"code","fc43cb6c":"markdown","ff71c517":"markdown","37fe3726":"markdown","10baca9a":"markdown","34f2aa7c":"markdown","004142c6":"markdown","81c0add1":"markdown","554dabdf":"markdown","2a02dcb8":"markdown","446f5d4b":"markdown","e94ccd1d":"markdown","9ff236ee":"markdown","da8e684f":"markdown","137c6f2f":"markdown","184e0f65":"markdown","ab21ef18":"markdown","a3caf703":"markdown","75347cfe":"markdown","0471f3f8":"markdown","f5d1a88a":"markdown","1bb2a1f7":"markdown","9d06f894":"markdown","7dbbf173":"markdown","a585f410":"markdown","43d256ce":"markdown","19d8fba2":"markdown","200d57e6":"markdown","6665ecfc":"markdown","10491a7e":"markdown","93ba7000":"markdown","25cac80a":"markdown","1317f47c":"markdown","3eae2ccc":"markdown","08a5aeee":"markdown","ad162dd5":"markdown","0c225b8d":"markdown","68d77fa0":"markdown","5e3c05dc":"markdown","c18f2b60":"markdown","ce6894c8":"markdown"},"source":{"b4eba371":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set(style='ticks', rc={'figure.figsize':(15, 10)})\n\n# machine learning\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier \nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve","c8b5bf5f":"data = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","5a848914":"data.head()","f5329251":"def detailed_analysis(df):\n  obs = df.shape[0]\n  types = df.dtypes\n  counts = df.apply(lambda x: x.count())\n  nulls = df.apply(lambda x: x.isnull().sum())\n  distincts = df.apply(lambda x: x.unique().shape[0])\n  missing_ratio = (df.isnull().sum() \/ obs) * 100\n  uniques = df.apply(lambda x: [x.unique()])\n  skewness = df.skew()\n  kurtosis = df.kurt()\n  print('Data shape:', df.shape)\n\n  cols = ['types', 'counts', 'nulls', 'distincts', 'missing ratio', 'uniques', 'skewness', 'kurtosis']\n  details = pd.concat([types, counts, nulls, distincts, missing_ratio, uniques, skewness, kurtosis], axis=1)\n\n  details.columns = cols \n  dtypes = details.types.value_counts()\n  print('________________________\\nData types:\\n', dtypes)\n  print('________________________')\n\n  return details","d3aa228f":"details = detailed_analysis(data)\ndetails","e789cc82":"data.describe()","28f00f5c":"values = data.target.value_counts()\nindexes = values.index\n\nsns.barplot(indexes, values)","b47aa333":"sns.pairplot(data=data, vars=data.columns.values[:-1], hue='target')","10b0d6ac":"sns.barplot(x='cp', y='age', data=data, hue='sex', ci=None)","4abbe075":"fig = plt.figure(figsize=(20, 25))\npalettes = ['Greens', 'Purples', 'YlOrRd', 'RdBu', 'BrBG', 'cubehelix'] * 2\n\nfor x in range(10):\n    fig1 = fig.add_subplot(5, 2, x+1)\n    sns.barplot(x='cp', y=data.columns.values[x+3], data=data, hue='target', ci=None, palette=palettes[x])","09e9ded0":"correlation = data.corr()\n\nfig = plt.figure(figsize=(12, 10))\nsns.heatmap(correlation, annot=True, center=1, cmap='RdBu')","41d21e9b":"sns.jointplot(x='slope', y='oldpeak', data=data, height=10)","fb8de55b":"fig = plt.figure(figsize=(20, 25))\npalettes = ['Greens', 'Purples', 'YlOrRd', 'RdBu', 'BrBG', 'cubehelix'] * 2\n\nfor x in range(12):\n    fig1 = fig.add_subplot(6, 2, x+1)\n    sns.violinplot(x='target', y=data.columns.values[x], data=data, palette=palettes[x])","4d0fb79b":"X = data.drop('target', axis=1)\ny = data.target\n\nselector = SelectKBest(score_func=chi2, k=5)\nfitted = selector.fit(X, y)\nfeatures_scores = pd.DataFrame(fitted.scores_)\nfeatures_columns = pd.DataFrame(X.columns)\n\nbest_features = pd.concat([features_columns, features_scores], axis=1)\nbest_features.columns = ['Feature', 'Score']\nbest_features.sort_values(by='Score', ascending=False, inplace=True)\nbest_features","07a50af6":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nprint('Input train shape', X_train.shape)\nprint('Output train shape', y_train.shape)\nprint('Input test shape', X_test.shape)\nprint('Output test shape', y_test.shape)","4aac2b15":"sc = StandardScaler()\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\n\ntype(X_train), type(X_test)","6e920704":"scores = pd.DataFrame(columns=['Model', 'Score'])","6e5f7c18":"def show_metrics():\n    fig = plt.figure(figsize=(25, 10))\n\n    # Confusion matrix\n    fig.add_subplot(121)\n    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)\n\n    # ROC Curve\n    fig.add_subplot(122)\n\n    ns_probs = [0 for _ in range(len(y_test))]\n    p_probs = model.predict_proba(X_test)[:, 1]\n\n    ns_fpr, ns_tpr, _ = roc_curve(y_test, ns_probs)\n    lr_fpr, lr_tpr, _ = roc_curve(y_test, p_probs)\n\n    plt.plot(ns_fpr, ns_tpr, linestyle='--', label='No Skill')\n    plt.plot(lr_fpr, lr_tpr, marker='o', label='Logistic')\n\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n\n    plt.legend()\n    plt.show()","ae908448":"model = LogisticRegression(solver='lbfgs')\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'Logistic Regression', 'Score': accuracy}, ignore_index=True)","3354f4dc":"show_metrics()","e3dfeba4":"model = SVC(probability=True)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'SVC', 'Score': accuracy}, ignore_index=True)","9b5c4ae9":"show_metrics()","069c2239":"model = GridSearchCV(estimator=RandomForestClassifier(), param_grid={'n_estimators': [50, 100, 200, 300], 'max_depth': [2, 3, 4, 5]}, cv=4)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100, model.best_params_)\nscores = scores.append({'Model': 'Random Forest', 'Score': accuracy}, ignore_index=True)","4ba1d6bf":"show_metrics()","b44ee9fa":"model = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'Gradient Boosting', 'Score': accuracy}, ignore_index=True)","22e383b9":"show_metrics()","59a73366":"model = GridSearchCV(estimator=ExtraTreesClassifier(), param_grid={'n_estimators': [50, 100, 200, 300], 'max_depth': [2, 3, 4, 5]}, cv=4)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'Extra Trees', 'Score': accuracy}, ignore_index=True)","f8be8ac2":"show_metrics()","eb4946fb":"model = GridSearchCV(estimator=KNeighborsClassifier(), param_grid={'n_neighbors': [1, 2, 3]}, cv=4)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'K-Neighbors', 'Score': accuracy}, ignore_index=True)","1d553d54":"show_metrics()","02048432":"model = GaussianNB()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'Gaussian NB', 'Score': accuracy}, ignore_index=True)","c138883c":"show_metrics()","35420091":"model = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'Decision Tree', 'Score': accuracy}, ignore_index=True)","56a8c9a5":"show_metrics()","fa595ea6":"model = XGBClassifier()\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'XGB Classifier', 'Score': accuracy}, ignore_index=True)","6727dac9":"show_metrics()","30c2cf9b":"scores.sort_values(by='Score', ascending=False)","fa9213b8":"sns.lineplot(x='Model', y='Score', data=scores)","6e177db7":"params = {\n  'max_depth': range(2, 8, 2),\n  'min_child_weight': range(1, 8, 2)\n  }\n\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=200, objective='binary:logistic',\n                    silent=True, nthread=4, max_depth=6, min_child_weight=1, tree_method='gpu_hist',\n                    gamma=0, subsample=1, colsample_bytree=1, scale_pos_weight=1, seed=228)\n\ngrid_search = GridSearchCV(xgb, params, n_jobs=2, cv=5, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_train, y_train)","9502a116":"grid_search.best_params_, grid_search.best_score_","b21866c3":"params = {\n  'max_depth': [3, 4, 5],\n  'min_child_weight': [4, 5, 6]\n  }\n\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=200, objective='binary:logistic',\n                    silent=True, nthread=4, max_depth=4, min_child_weight=5, tree_method='gpu_hist',\n                    gamma=0, subsample=1, colsample_bytree=1, scale_pos_weight=1, seed=228)\n\ngrid_search = GridSearchCV(xgb, params, n_jobs=2, cv=5, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_train, y_train)","169679ed":"grid_search.best_params_, grid_search.best_score_","cba885a8":"params = {\n  'min_child_weight': [1, 4, 6, 7, 8, 10, 12]\n  }\n\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=200, objective='binary:logistic',\n                    silent=True, nthread=4, max_depth=3, min_child_weight=6, tree_method='gpu_hist',\n                    gamma=0, subsample=1, colsample_bytree=1, scale_pos_weight=1, seed=228)\n\ngrid_search = GridSearchCV(xgb, params, n_jobs=2, cv=5, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_train, y_train)","f579cc1d":"grid_search.best_params_, grid_search.best_score_","6273c899":"params = {\n  'gamma': [0, 0.1, 0.2, 0.3, 0.4, 0.5]\n  }\n\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=200, objective='binary:logistic',\n                    silent=True, nthread=4, max_depth=3, min_child_weight=6, tree_method='gpu_hist',\n                    gamma=0, subsample=1, colsample_bytree=1, scale_pos_weight=1, seed=228)\n\ngrid_search = GridSearchCV(xgb, params, n_jobs=1, cv=5, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_train, y_train)","81238314":"grid_search.best_params_, grid_search.best_score_","89e68328":"  params = {\n    'subsample': [0.6, 0.7, 0.8, 0.9],\n    'colsample_bytree': [0.6, 0.7, 0.8, 0.9]\n  }\n\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=200, objective='binary:logistic',\n                    silent=True, nthread=4, max_depth=3, min_child_weight=6, tree_method='gpu_hist',\n                    gamma=0, subsample=1, colsample_bytree=1, scale_pos_weight=1, seed=228)\n\ngrid_search = GridSearchCV(xgb, params, n_jobs=1, cv=5, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_train, y_train)","70aeb38e":"grid_search.best_params_, grid_search.best_score_","c89fca46":"params = {\n    'subsample': [0.55, 0.6, 0.65],\n    'colsample_bytree': [0.65, 0.7, 0.75]\n  }\n\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=200, objective='binary:logistic',\n                    silent=True, nthread=4, max_depth=3, min_child_weight=6, tree_method='gpu_hist',\n                    gamma=0, subsample=0.6, colsample_bytree=0.7, scale_pos_weight=1, seed=228)\n\ngrid_search = GridSearchCV(xgb, params, n_jobs=1, cv=5, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_train, y_train)","a703f974":"grid_search.best_params_, grid_search.best_score_","e5a80091":"params = {\n  'reg_alpha': [0, 0.001, 0.005, 0.01, 0.05]\n  }\n\nxgb = XGBClassifier(learning_rate=0.1, n_estimators=200, objective='binary:logistic',\n                    silent=True, nthread=4, max_depth=3, min_child_weight=6, tree_method='gpu_hist',\n                    gamma=0, subsample=0.6, colsample_bytree=0.7, scale_pos_weight=1, seed=228, reg_alpha=0)\n\ngrid_search = GridSearchCV(xgb, params, n_jobs=1, cv=5, scoring='accuracy', verbose=1)\n\ngrid_search.fit(X_train, y_train)","0d8e46ea":"grid_search.best_params_, grid_search.best_score_","301d9af4":"grid_search.best_estimator_","71ef4340":"model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=0.7, gamma=0,\n              learning_rate=0.001, max_delta_step=0, max_depth=3,\n              min_child_weight=6, missing=None, n_estimators=5000, n_jobs=1,\n              nthread=4, objective='binary:logistic', random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=228,\n              silent=True, subsample=0.6, tree_method='gpu_hist', verbosity=1)\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nprint('Accuracy:', accuracy * 100)\nscores = scores.append({'Model': 'XGB Classifier', 'Score': accuracy}, ignore_index=True)","f85762b7":"# Here soon","fc43cb6c":"XGB Classifier","ff71c517":"**Gradien Boosting Classifier**","37fe3726":"**Finally, let's review our scores**","10baca9a":"# **Model**","34f2aa7c":"**Analysis of different chest types and their influence to the target value**\n<br>Types of pain:\n- Value 0: typical angina\n- Value 1: atypical angina\n- Value 2: non-anginal pain\n- Value 3: asymptomatic","004142c6":"**Actually, I don't know why the tuned model performs worse than the standart one** even though we have better score while tuning. So, I'm probably gonna fix this later, but now we're gonna do **ensemble stacking**","81c0add1":"**Import packages**","554dabdf":"**Pair plot between all variables**","2a02dcb8":"**Relationship between slope and oldpeak**\n<br><br>This plot confirms our statement that lower slope is better. According to the jointplot lower slope values have higher oldpeak values which mean our patient is healthy","446f5d4b":"**Also we define a function to show additional metrics (Confusion Matrix and ROC Curve)**","e94ccd1d":"**Extra Trees Classifier**","9ff236ee":"**Tune regularization parameters**","da8e684f":"**So, we can test some classification algorithms on our data. Also we create a DataFrame to collect scores**","137c6f2f":"**K-Neighbors Classifier**","184e0f65":"**Violin plots for all variables**\n<br><br>Here we can investigate things about features importance too. If plots for 0 and 1 are the same it means that correlation is low. Moreover we can see smooth values distribution for each variable","ab21ef18":"**SelectKBest**\n<br><br>Finally for EDA we're gonna check the best features using SelectKBest","a3caf703":"# **Introduction and uploading data**","75347cfe":"**Let's split our data to test and train**","0471f3f8":"# **Exploratory Data Analysis (EDA)**","f5d1a88a":"**Top-3 are Random Forest, K-Neighbors and Extra Trees.**","1bb2a1f7":"**Gaussian Naive Bayes**","9d06f894":"**Support Vector Classifier (SVC)**","7dbbf173":"**Decision Tree Classifier**","a585f410":"Let's go deeper now","43d256ce":"Here we can see that people with the same chest pain have almost the same age regardless of the sex","19d8fba2":"**Acquire data**","200d57e6":"**Correlation heatmap**","6665ecfc":"![](https:\/\/img.webmd.com\/dtmcms\/live\/webmd\/consumer_assets\/site_images\/articles\/health_tools\/did_you_know_this_could_lead_to_heart_disease_slideshow\/493ss_thinkstock_rf_heart_illustration.jpg)<br>**Hi, everyone! That's my analysis and classification for Heart Disease UCI.** Here you can find general analysis, comparison between different variables and investigation of features importance. If you find my notebook interesting and helpful, please **UPVOTE.** Enjoy the analysis :)","10491a7e":"**Tune subsample and colsample_bytree**","93ba7000":"# Tuning and Ensemble Stacking","25cac80a":"**Now let's perform our new model**","1317f47c":"**Relationship between chest pain and different variables separated by target value.**\n1. Here we can find out that fbs has significantly various values which are dependent on the chest pain \n2. Resting ecg results with normal values mean that patient hasn't heart diseases (exception: asymptomatic chest pain, value 3)\n3. If exang is 1 a patient must be healthy (exception: asymptomatic chest pain, value 3)\n4. If oldpeak is high a patient must be healthy (exception: asymptomatic chest pain, value 3)\n5. It's better if slope has low value (again asymptomatic chest pain as an exception)\n6. High number of ca (major vessels) is always great\n7. It's good when thal nearly equals 3","3eae2ccc":"**Target value distribution**","08a5aeee":"**Let's learn some info about our data.** For that I create a function which can show us missing ratio, distincts, skewness, etc.","ad162dd5":"**Ok, now let's tune XGBoost Classifier and try to get better score.** We select our params and model. We'll tune it gradually to save time. **At first we tune max_depth and min_child_weight**","0c225b8d":"**Now we're gonna scale our data**","68d77fa0":"**Logistic Regression**","5e3c05dc":"**Now let's tune gamma**","c18f2b60":"Wow, our data is totally clear, so we can visualize some things","ce6894c8":"**Random Forest Classifier**"}}