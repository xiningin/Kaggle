{"cell_type":{"08e832d0":"code","356e4ab5":"code","d11918b2":"code","425cf7bc":"code","e0249317":"code","d1ccbb1b":"code","2e05cc98":"code","4e823c6b":"code","054bf70b":"code","7fde6fc5":"code","049c6c2f":"code","579a56b7":"code","ed5fd373":"code","98d9ef2f":"code","f7a6c71d":"code","dbc015a9":"code","434340f9":"code","bd018096":"code","e6a5922c":"code","3f94624c":"code","5555a58c":"code","dd5a36c7":"code","699cca81":"code","883fc17a":"code","cf7cce9b":"code","bc8a7858":"code","0b20fb61":"code","a2bfef7e":"code","627566d6":"code","a5a6354d":"code","f83a299b":"code","9d968ea6":"code","87debba6":"code","f597c571":"code","a0cd5bef":"code","f272b961":"code","1910570c":"code","88fecdfb":"code","70a01436":"code","dd925866":"code","c4607c49":"code","c2b3d52f":"code","eb8d2390":"code","085d2f99":"code","27649594":"code","30e70564":"code","7f296c23":"code","fd6d3c85":"code","89d5915c":"code","3e17fdf7":"code","270620b5":"code","18800734":"code","822999a8":"code","51e03e6c":"code","cc8046ee":"code","0b540e43":"code","1915253a":"code","b8795f35":"code","9f882395":"code","fb0128de":"code","6188295f":"code","b9e739ab":"code","438ac34e":"code","a44dcf25":"code","c96c7367":"code","f572919c":"code","bff2f04a":"code","3c5425d4":"code","3ef57336":"code","d5b6e557":"code","7826d69a":"code","4f26945a":"code","6a35ed8e":"code","1c2078da":"code","b8d56989":"code","3d8b0213":"code","dd81895e":"code","3e74d87e":"code","4f82f650":"code","de2ee417":"code","304f12af":"code","bab7fa82":"code","25d03a5c":"code","e4c0c872":"code","ca74c9d4":"code","d269d5f3":"code","af164028":"code","b57dbeeb":"code","0528e258":"code","d9636459":"code","3d972760":"code","2c7b12de":"code","9c8026e3":"code","3e867c54":"markdown","439393bb":"markdown","9b633e35":"markdown","e8a4c052":"markdown","667aca84":"markdown","5851f254":"markdown","a50c135d":"markdown","e95443c5":"markdown","cb44520d":"markdown","0831da85":"markdown","cd7918cb":"markdown","27a0e3e8":"markdown","b8b5dbf5":"markdown","1364a644":"markdown","ec44f202":"markdown","6bfa7421":"markdown","de16a2d5":"markdown","aef36cc5":"markdown","fd7f7708":"markdown","87270468":"markdown","9d6572e3":"markdown","3a3a66fe":"markdown","f125ae61":"markdown","635c918b":"markdown","3f419540":"markdown","c1dc8d9e":"markdown","2446958c":"markdown","bc85c760":"markdown","83f1895c":"markdown","b416075d":"markdown","240a54cf":"markdown","71aa131f":"markdown","35efb039":"markdown","5a44ea97":"markdown","ccc37ec4":"markdown","25cc5c2c":"markdown","6f872b9e":"markdown","ecd5073f":"markdown"},"source":{"08e832d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport seaborn as sns\nimport os\nimport xgboost\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","356e4ab5":"df_train_o = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test_o = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","d11918b2":"print(df_train_o.shape)\nprint(df_test_o.shape)\n# ~1460 rows with ~80 attributes ","425cf7bc":"df_train_o.head()","e0249317":"df_train_o.info()","d1ccbb1b":"def getNumCatFeatures(df_train_o):\n    numerical_feats = df_train_o.dtypes[df_train_o.dtypes != \"object\"].index\n    print(\"Number of Numerical features: \", len(numerical_feats))\n\n    categorical_feats = df_train_o.dtypes[df_train_o.dtypes == \"object\"].index\n    print(\"Number of Categorical features: \", len(categorical_feats))\n    \n    return numerical_feats, categorical_feats\n\nnumerical_feats, categorical_feats = getNumCatFeatures(df_train_o)","2e05cc98":"print(df_train_o[numerical_feats].columns)\nprint(\"*\"*100)\nprint(df_train_o[categorical_feats].columns)","4e823c6b":"df_train_o['ExterCond'].head()","054bf70b":"sns.distplot(df_train_o['SalePrice'])","7fde6fc5":"df_train_o['SalePrice_Logged'] = np.log(df_train_o['SalePrice'])","049c6c2f":"sns.distplot(df_train_o['SalePrice_Logged'])","579a56b7":"quantitative = [f for f in df_train_o.columns if df_train_o.dtypes[f] != 'object']\n#print(quantitative)\nquantitative.remove('SalePrice')\nquantitative.remove('Id')\nquantitative.remove('SalePrice_Logged')\n\nf = pd.melt(df_train_o, value_vars=quantitative)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False)\ng = g.map(sns.histplot, \"value\")","ed5fd373":"listSkew = ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\nfor col in listSkew:\n    df_train_o[col+'_Logged'] = np.log(df_train_o[col])\n    df_train_o.drop(columns=[col])\nfor i in range(len(listSkew)):\n    listSkew[i] = listSkew[i]+'_Logged'\nquantitative = listSkew\nf = pd.melt(df_train_o, value_vars=quantitative)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False)\ng = g.map(sns.histplot, \"value\")","98d9ef2f":"f = pd.melt(df_train_o, value_vars=['OverallQual', 'OverallCond'])\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False)\ng = g.map(sns.histplot, \"value\")","f7a6c71d":"mul = df_train_o['OverallCond'] * df_train_o['OverallQual']\n#mul = np.log(mul)\nsns.histplot(mul)","dbc015a9":"mul2 = df_train_o['SalePrice'] * df_train_o['OverallCond']\nmul2 = np.log(mul2)\nsns.histplot(mul2)","434340f9":"def multiplier(df, features):\n    for col in features:\n        mul = df['SalePrice'] * df[col]\n        mul = np.log(mul)\n        df[col+\"_Logged\"] = mul\n    \n    f = pd.melt(df, value_vars=features)\n    g = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False)\n    g = g.map(sns.histplot, \"value\")\n    \n    return df\n\ndef multiplier2(df, features):\n    mul = df[features[0]] * df[features[1]]\n    #mul = np.log(mul)\n    df[\"QualxCond\"] = mul\n    \n    f = pd.melt(df, value_vars='QualxCond')\n    g = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False)\n    g = g.map(sns.histplot, \"value\")\n    \n    return df\n\ndf_train_o = multiplier2(df_train_o, ['OverallCond', 'OverallQual'])\ndf_test_o = multiplier2(df_test_o, ['OverallCond', 'OverallQual'])","bd018096":"def checkINFNULL(X):\n    for col in X.columns:\n        if (X[col].isnull().any()):\n            print('null')\n            break\n    print('no null')\n    print(np.isinf(X).values.sum())","e6a5922c":"for x in listSkew:\n    print(x)\n    print(np.isinf(df_train_o[x]).values.sum())","3f94624c":"for col in df_train_o.columns:\n    nbr = df_train_o[col].isnull().sum()\n    if nbr\/1460 > 0.5:\n        print(f'{col}:{nbr\/df_train_o.shape[0]}')","5555a58c":"df_train = df_train_o.drop(columns = ['Alley','PoolQC','Fence','MiscFeature','Id'])\ndf_test = df_test_o.drop(columns = ['Alley','PoolQC','Fence','MiscFeature'])","dd5a36c7":"cols_with_missing = [col for col in df_train.columns if col != 'SalePrice' and df_train[col].isnull().any()]\ncols_with_missing","699cca81":"def getColsWithMissingValue(df):\n    cols_with_missing = [col for col in df.columns if col != 'SalePrice' and df[col].isnull().any()]\n    cols_with_missing_num = []\n    cols_with_missing_cat = []\n    if len(cols_with_missing) == 0:\n        print(f'There is no null\/NA in this df')\n        return cols_with_missing_num, cols_with_missing_cat\n    for col in cols_with_missing:\n        #print(df[col].dtypes)\n        if df[col].dtypes != object:\n            cols_with_missing_num.append(col)\n            print(f'mean of {col}:{df[col].mean()}')\n            print(f'median of {col}:{df[col].median()}')\n            print(f'mode of {col}:{df[col].mode()}')\n        else:\n            cols_with_missing_cat.append(col)\n            print(f'Value counts of {col}:\\n{df[col].value_counts()}')\n    return cols_with_missing_num, cols_with_missing_cat\ncols_with_missing_num, cols_with_missing_cat = getColsWithMissingValue(df_train)","883fc17a":"sns.distplot(df_train['MasVnrArea'])","cf7cce9b":"df_train['CentralAir'].replace({'N':0, 'Y':1}, inplace=True)\ndf_test['CentralAir'].replace({'N':0, 'Y':1}, inplace=True)","bc8a7858":"def gradingconverter(df, listToBeConverted, quality):\n    for i in listToBeConverted:\n        df[i].replace(quality, inplace=True)\n    return df\n\nquality = {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1}\nquality_2 = {'Ex':8, 'Gd':6, 'TA':4, 'Fa':3, 'Po':2}\nx = ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual', 'GarageQual','GarageCond', 'FireplaceQu']\ndf_train = gradingconverter(df_train, x, quality_2)\ndf_test = gradingconverter(df_test, x, quality_2)\n\n\nquality2 = {'Gd':4, 'Av':3, 'Mn':2, 'No':0}\nquality2_2 = {'Gd':6, 'Av':4, 'Mn':2, 'No':1}\nx = ['BsmtExposure']\ndf_train = gradingconverter(df_train, x, quality2_2)\ndf_test = gradingconverter(df_test, x, quality2_2)","0b20fb61":"qua_na = ['BsmtQual','FireplaceQu','GarageQual','BsmtCond','GarageCond','BsmtExposure']\n\ndef pre_replaceNA_with_0(df, qua_na):\n    for col in qua_na:\n        df[col].fillna(value=0, inplace=True)\n    return df\n\ndf_train = pre_replaceNA_with_0(df_train, qua_na)\ndf_test = pre_replaceNA_with_0(df_test, qua_na)","a2bfef7e":"for col in cols_with_missing_num:\n    print(f'mean of {col}:\\n{df_train[col].mean()}')\n    print(f'median of {col}:\\n{df_train[col].median()}')\n    print(f'mode of {col}:\\n{df_train[col].mode()}')","627566d6":"numerical_cols, categorical_cols = getNumCatFeatures(df_train)","a5a6354d":"numerical_cols","f83a299b":"categorical_cols","9d968ea6":"num_cols_with_missing, cat_cols_with_missing = getColsWithMissingValue(df_train)","87debba6":"num_cols_with_missing","f597c571":"num_cols_with_missing_test, cat_cols_with_missing_test = getColsWithMissingValue(df_test)","a0cd5bef":"num_cols_with_missing_test","f272b961":"def replaceNA_with_Median(df, num_cols_with_missing):\n    for col in num_cols_with_missing:\n        median = df[col].median()\n        df[col].fillna(value=median, inplace=True)\n        df[col+'was_missing'] = df[col].isnull().astype(int)\n    return df\n\ndf_train = replaceNA_with_Median(df_train, num_cols_with_missing)\ndf_test = replaceNA_with_Median(df_test, num_cols_with_missing_test)","1910570c":"num_cols_with_missing, cat_cols_with_missing = getColsWithMissingValue(df_train)","88fecdfb":"num_cols_with_missing","70a01436":"num_cols_with_missing_test, cat_cols_with_missing_test = getColsWithMissingValue(df_test)","dd925866":"num_cols_with_missing_test","c4607c49":"df_train.shape","c2b3d52f":"'LotArea_Logged' in df_train.columns","eb8d2390":"df_test.shape","085d2f99":"'LotArea_Logged' in df_test.columns","27649594":"cat_cols_with_missing","30e70564":"for col in cat_cols_with_missing:\n    print(f'{col}:\\n{df_train[col].value_counts()}')","7f296c23":"sumofnullrow = 0\nfor col in cat_cols_with_missing:\n    nbr = df_train_o[col].isnull().sum()\n    print(f'{col}:{nbr\/df_train_o.shape[0]}')\n    sumofnullrow += nbr","fd6d3c85":"sumofnullrow","89d5915c":"def replaceNA_with_0(df, cat_cols_with_missing): # for categorical features only\n    for col in cat_cols_with_missing:\n        df[col].fillna(value='0', inplace=True)\n        df[col+'was_missing'] = df[col].isnull().astype(int)\n    return df\n\ndf_train = replaceNA_with_0(df_train, cat_cols_with_missing)\ndf_test = replaceNA_with_0(df_test, cat_cols_with_missing_test)","3e17fdf7":"num_cols_with_missing, cat_cols_with_missing = getColsWithMissingValue(df_train)","270620b5":"num_cols_with_missing_test, cat_cols_with_missing_test = getColsWithMissingValue(df_test)","18800734":"nums, cats = getNumCatFeatures(df_train)\nnums_test, cats_test = getNumCatFeatures(df_test)","822999a8":"cats","51e03e6c":"cats_test","cc8046ee":"def transformCatToNum_FrequencyEncoding(df, cats):\n    for col in cats:\n        x = df[col].value_counts()\n        xdict = x.to_dict()\n        #print(xdict)\n        df[col].replace(xdict, inplace=True)\n    return df\n\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\n\n\ndef transformCatToNum_LabelEncoding(df, cats):\n    for col in cats:\n        df[col] = le.fit_transform(df[col])\n    return df\n\ndef transformCatToNum_Factorizing(df, cats):\n    for col in cats:\n        df[col], _ = df[col].factorize()\n    return df\n\ndef transformCatToNum_OneHot(df, cats):\n    return pd.get_dummies(df)\n    \nlabelencoding = ['ExterQual','ExterCond','BsmtQual','BsmtCond','HeatingQC','KitchenQual', 'GarageQual','GarageCond', 'FireplaceQu','BsmtExposure']\n# Change below for different encoding\ndf_train = transformCatToNum_FrequencyEncoding(df_train, cats)\ndf_test = transformCatToNum_FrequencyEncoding(df_test, cats_test)\n","0b540e43":"nums, cats = getNumCatFeatures(df_train)","1915253a":"nums_test, cats_test = getNumCatFeatures(df_test)","b8795f35":"np.setdiff1d(df_train.columns, df_test.columns)","9f882395":"np.setdiff1d(df_test.columns, df_train.columns)","fb0128de":"nums, cats = getNumCatFeatures(df_train)","6188295f":"nums_test, cats_test = getNumCatFeatures(df_test)","b9e739ab":"df_train.head()","438ac34e":"from sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.inspection import permutation_importance\nfrom matplotlib import pyplot as plt\nfrom xgboost import XGBRegressor\nfrom sklearn.feature_selection import mutual_info_regression, f_regression","a44dcf25":"df_train.filter(regex='Logged',axis=1).columns","c96c7367":"X = df_train.loc[:, df_train.columns != 'SalePrice_Logged']\nX = X.loc[:, X.columns != 'SalePrice']\ny = df_train['SalePrice']\ny_logged = df_train['SalePrice_Logged']","f572919c":"#xgb = XGBRegressor(**xgb_params)\nxgb_feat = XGBRegressor(n_estimators=100)\nxgb_feat.fit(X, y_logged) # Using logged y\nsorted_idx = xgb_feat.feature_importances_.argsort()\nplt.barh(X.columns[sorted_idx], xgb_feat.feature_importances_[sorted_idx])\nplt.xlabel(\"Xgboost Feature Importance\")\nplt.figure(dpi=120, figsize=(8, 20))","bff2f04a":"X.columns[sorted_idx]","3c5425d4":"xgb_feat.feature_importances_[sorted_idx]","3ef57336":"# get features > 0\ndef extractFeatures(X, threshold, xgb):\n    nbrFeat = 0\n    FeatExtracted = []\n    x = 0\n    for i in X.columns[sorted_idx]:\n        if xgb.feature_importances_[sorted_idx][x] > threshold:\n            #print(i)\n            FeatExtracted.append(i)\n            nbrFeat += 1\n        x += 1\n    print(f'no. of features ori:{X.shape[1]}')\n    print(f'no. of features:{nbrFeat}')\n    return FeatExtracted\nFeatExtractedXGB = extractFeatures(X, 0.002, xgb_feat)","d5b6e557":"sns.distplot(xgb_feat.feature_importances_[sorted_idx])","7826d69a":"FeatExtractedXGB","4f26945a":"X1 = X[FeatExtractedXGB] # features by XGB\n\ndf_test1 = df_test[FeatExtractedXGB] # df_test","6a35ed8e":"print(X1.shape)\nprint(df_test1.shape)","1c2078da":"X_train, X_test, y_train, y_test = train_test_split(X1, y_logged, test_size=0.25, random_state=12)","b8d56989":"xgb_params = dict(\n    max_depth=7,                           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n    n_estimators=900,                     # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=1,                    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=0.4,   # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=0.4,          # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0.09888,        # L1 regularization (like LASSO) - try 0.0 to 10.0\n    num_parallel_tree=7,                   # set > 1 for boosted random forests\n)\n\nxgb = XGBRegressor(**xgb_params)","3d8b0213":"xgb.fit(X_train, y_train)\npredictions = (xgb.predict(X_test))\n#mean_squared_error(y_test,predictions, squared=False)                 # Run this if y is not logged\nmean_squared_error(np.exp(y_test), np.exp(predictions), squared=False) # Run this if y is logged","dd81895e":"r2_score(y_test, predictions)","3e74d87e":"import sklearn.linear_model as linear_model\nimport sklearn.kernel_ridge as kernel_ridge\nimport sklearn.ensemble as ensemble","4f82f650":"clf = linear_model.BayesianRidge()\n#clf = kernel_ridge.KernelRidge(alpha=0.001)\nclf.fit(X_train, y_train)\npreds = clf.predict(X_test)\n#mean_squared_error(y_test,predictions, squared=False)                 # Run this if y is not logged\nmean_squared_error(np.exp(y_test), np.exp(preds), squared=False) # Run this if y is logged","de2ee417":"r2_score(y_test, preds)","304f12af":"regressor = ensemble.RandomForestRegressor(n_estimators=100, min_samples_split=2, random_state=4)\nregressor.fit(X_train, y_train)\npreds = regressor.predict(X_test)\n#mean_squared_error(y_test,predictions, squared=False)                 # Run this if y is not logged\nmean_squared_error(np.exp(y_test), np.exp(preds), squared=False) # Run this if y is logged","bab7fa82":"r2_score(y_test, preds)","25d03a5c":"df_test.head()","e4c0c872":"#df_test = df_test[df_test.columns.drop(list(df_test.filter(regex='was_missing')))]","ca74c9d4":"df_test1.shape","d269d5f3":"import joblib\ndef PredictionPipeline(model, X, y, df_test, filename, modelname, df_test_o=df_test_o):\n    model.fit(X, y)\n    predictions = np.exp(model.predict(df_test))\n    df_submission = pd.DataFrame()\n    df_submission['Id'] = df_test_o['Id']\n    df_submission['SalePrice'] = predictions\n    df_submission.to_csv(filename, index=False)\n    try:\n        model.save_model(modelname)\n    except:\n        joblib.dump(model, modelname+'.joblib')\n    print(\"model and prediction are saved\")","af164028":"PredictionPipeline(xgb, X1, y_logged, df_test1, \"df_submission_final3.csv\", \"XGBmodel_final3.json\") # By notebook in OneDrive","b57dbeeb":"PredictionPipeline(xgb, X1, y_logged, df_test1, \"df_submission_final5.csv\", \"XGBmodel_final5.json\") #QualxCond","0528e258":"PredictionPipeline(xgb, X1, y_logged, df_test1, \"df_submission_final6.csv\", \"XGBmodel_final6.json\") #QualxCond + multiplying grading ","d9636459":"PredictionPipeline(xgb, X1, y_logged, df_test1, \"df_submission_final8.csv\", \"XGBmodel_final8.json\") #QualxCond + multiplying grading - factorize() ","3d972760":"PredictionPipeline(xgb, X1, y_logged, df_test1, \"df_submission_final10.csv\", \"XGBmodel_final10.json\") #QualxCond + multiplying grading + frequency + threshold 0.002","2c7b12de":"PredictionPipeline(regressor, X1, y_logged, df_test1, \"df_submission_final_RF.csv\", \"XGBmodel_final_RF.json\")","9c8026e3":"model_xgb_2 = xgboost.XGBRegressor()\nmodel_xgb_2.load_model(\"XGBmodel_final3.json\")","3e867c54":"#### Random forest regressor","439393bb":"#### Fill null value for numerical features with its mode\/median\/mean which depends on distribution","9b633e35":"### EDA","e8a4c052":"#### Import libraries for model and eval","667aca84":"#### Load models back","5851f254":"### Now transforming categorical to numerical by frequency\/label encoding","a50c135d":"#### Replace NA with median in numerical features","e95443c5":"#### Fill nan with 0","cb44520d":"#### Now handle categorical features with NA","0831da85":"#### df_train","cd7918cb":"#### Now deal with other NA","27a0e3e8":"#### Other models","b8b5dbf5":"### Preprocessing","1364a644":"#### np.exp() to tranform back to real price","ec44f202":"#### XGB model","6bfa7421":"#### Number of null in each col in df","de16a2d5":"#### Transform some features to normal distribution in df_train only","aef36cc5":"### Apply the model to test.csv","fd7f7708":"### XGBoost\n#### provides a XGBRegressor and permutation_importance that can find important feature\nhttps:\/\/mljar.com\/blog\/feature-importance-xgboost\/","87270468":"#### All NA in numerical features are filled with their median","9d6572e3":"#### new features are added","3a3a66fe":"#### Include the features with score > 0","f125ae61":"### Now train the model with the extracted features","635c918b":"#### Transform to normal distribution ","3f419540":"Get the columns with missing value","c1dc8d9e":"Convert quality except 'NA'","2446958c":"#### r2_score: 0.9728663100221 if new cols added: OverallQual and Cond multi by SalePrice (try double regression!) <- failed","bc85c760":"#### df_train and df_test","83f1895c":"#### Now all categorical data has transformed to numerical","b416075d":"By LabelEncoder()\n0.8862649501692593\n","240a54cf":"### Data preprocessing","71aa131f":"#### 1. Convert the attributes that can be numeric straightly","35efb039":"### Random forest regressor","5a44ea97":"#### pre-dealling NA values for above features","ccc37ec4":"#### df_test","25cc5c2c":"0 for No and 1 for Yes","6f872b9e":"## To do\n### 1. categorical data -> numerical\n    target encoding (index each category) or frequency encoding (mean\/median) or one hot (too many features)\n### 2. null\/na value handling\n    filled with its mean\/median AND\/OR add a indicating coloumn for that\n### 3. feature selection","ecd5073f":"#### Features above can be removed from training because almost all are missing"}}