{"cell_type":{"44d4f01c":"code","92f20f98":"code","024746ab":"code","a1941434":"code","3e3930d1":"code","d85daa5c":"code","02f4f608":"code","f24048ee":"code","6241dd7e":"code","ded0e411":"code","fb22eff6":"code","a6a17255":"code","dcc55410":"code","e3b789a3":"code","23123530":"code","09cb82a3":"code","19e419b8":"code","8055fd4b":"code","41009365":"code","4088edb6":"code","b30910c6":"code","0791a825":"code","c95b3fcb":"code","f3e51f8a":"code","d0994a3f":"code","ca10d37a":"code","fbec4ae2":"code","e221a987":"code","2f02a681":"code","adf47087":"code","062fce8b":"code","bb771b31":"code","40a622ad":"markdown","b1efedec":"markdown","898598cf":"markdown","c3d564b9":"markdown","e9747260":"markdown","01db54e4":"markdown","8508824a":"markdown","554081f1":"markdown","d89d3838":"markdown","e4d6c5d7":"markdown","70b7c560":"markdown","f6ee5c57":"markdown","f18b703c":"markdown","36cb830c":"markdown","36e39bb0":"markdown","4ead0788":"markdown","f70c299f":"markdown","43c8d521":"markdown","e4ea237d":"markdown"},"source":{"44d4f01c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # data visualization\nfrom sklearn.model_selection import train_test_split # tran and test data split\nfrom sklearn.linear_model import LogisticRegression # Logistic Regression \nfrom sklearn.svm import SVC #Support Vector Machine \nfrom sklearn.ensemble import RandomForestClassifier # Random Rorest Classifier \nfrom sklearn.metrics import roc_auc_score # ROC and AUC \nfrom sklearn.metrics import accuracy_score # Accuracy \nfrom sklearn.metrics import recall_score # Recall \nfrom sklearn.metrics import precision_score # Prescison \nfrom sklearn.metrics import classification_report # Classification Score Report \n%matplotlib inline \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","92f20f98":"data = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","024746ab":"data.sample(10, random_state = 10)","a1941434":"data.info()","3e3930d1":"# Summary statistics for the variables\ndata.describe(include = \"all\")","d85daa5c":"# Check the missing data status\nprint(\"Is the data containing any missing values: {}\".format(data.isnull().sum().any()))","02f4f608":"data['Class'] = data['Class'].astype('str')\nsample_data = data.sample(60000)","f24048ee":"plt.style.use('ggplot')\nsample_data.hist(figsize = (25, 25), color = 'steelblue', bins = 20)\nplt.show()","6241dd7e":"sns.countplot(x = 'Class', data = data, color = 'steelblue')\nplt.title(\"Transaction Fraud Distribution\")\nplt.xticks(range(2), [\"Normal\", \"Fraud\"])\nplt.ylabel(\"Frequency\")\nplt.show()","ded0e411":"# Print the count for fraud and non-fraud transactions\ndata[\"Class\"].value_counts(sort = True)","fb22eff6":"# Time distribution in hours and with Transaction Amount \nplt.subplot(2, 2, 1)\n(data['Time']\/3600).hist(figsize=(15,15), color = \"steelblue\", bins = 20)\nplt.title(\"Distribution of Time\")\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Frequency\")\n\n# Transaction amount distribution by hours\nplt.subplot(2, 2, 2)\nplt.scatter(x = data['Time']\/3600, y = data['Amount'], alpha = .8)\nplt.title(\"Distribution of Transaction Amount by Time\")\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Frequency\")\nplt.show()","a6a17255":"# For Normal Transactions\n# Look at the time distribution\nplt.subplot(2, 2, 1)\n(data[data['Class'] == '0']['Time']\/3600).hist(figsize=(15, 15), bins = 20, color = 'blue')\nplt.title(\"Time Distribution for Normal Transactions\")\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Frequency\")\n\n# Transaction Amount Distribution\nplt.subplot(2, 2, 2)\ndata[data['Class'] == \"0\"]['Amount'].hist(bins = [0, 100, 200, 300, 400 ,500, 600, 700, 800, 900, 1000], color = 'blue')\nplt.title(\"Amount Distribution for Normal Transactions\")\nplt.xlabel(\"Transaction Amount\")\nplt.ylabel(\"Frequency\")\nplt.xlim(0, 1500)\nplt.show()","dcc55410":"# For Fraud Transactions\n# Look at the time distribution\nplt.subplot(2, 2, 1)\n(data[data['Class'] == '1']['Time']\/3600).hist(figsize=(15, 15), bins = 20)\nplt.title(\"Time Distribution for Normal Transactions\")\nplt.xlabel(\"Hour\")\nplt.ylabel(\"Frequency\")\n\n# Transaction Amount Distribution\nplt.subplot(2, 2, 2)\ndata[data['Class'] == \"1\"]['Amount'].hist(bins = 20)\nplt.title(\"Amount Distribution for Normal Transactions\")\nplt.xlabel(\"Transaction Amount\")\nplt.ylabel(\"Frequency\")\nplt.xlim(0, 1500)\nplt.show()","e3b789a3":"# Correlation Plot for all numeric variables\ncorr = data.corr()\ncorr.style.background_gradient()","23123530":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\n# Make the X and Y inputs\ndata_x = data.drop('Class', axis = 1)\ndata_y = data['Class']\n\n# Split the data into training and testing, use 30% data to evaluate the models \ntrain_x, test_x, train_y, test_y = train_test_split(data_x, data_y, test_size = 0.3, random_state = 123)\n\ntrain_x = scaler.fit_transform(train_x)\ntest_x = scaler.transform(test_x)\n\nprint(\"Training data has {} rows and {} variables\".format(train_x.shape[0], train_x.shape[1]))\nprint(\"Testing data has {} rows and {} variables\".format(test_x.shape[0], test_x.shape[1]))","09cb82a3":"from sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import confusion_matrix\n\ndummy = DummyClassifier(strategy=\"stratified\")\ndummy.fit(train_x, train_y)\ndummy_pred = dummy.predict(test_x)\n\n# Print test outcome \nprint(confusion_matrix(test_y, dummy_pred))\nprint('\\n')\nprint(classification_report(test_y, dummy_pred))","19e419b8":"# Build the model and make predictions using the test data \nlogreg = LogisticRegression(random_state = 100)\nlogreg.fit(train_x, train_y)\n\npd.DataFrame(logreg.coef_, columns = data.drop('Class', axis = 1).columns)\npred_y = logreg.predict(test_x)","8055fd4b":"# Print the confusion matrix for the model \nconf_matrix = confusion_matrix(test_y, pred_y)\n\nplt.figure(figsize = (7,7))\nsns.heatmap(conf_matrix, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r')\nplt.title(\"Accuracy Score: {}\".format(accuracy_score(test_y, pred_y)))\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Actual\")\nplt.show()","41009365":"# Since the data is highly imbalanced, it is not enough to purly rely on the accuracy score to evaluate the model. Recall and Prescision here plays more important roles\nprint(\"The recall score for prediction is {:0.2f}\".format(recall_score(test_y, pred_y, pos_label='1')))\nprint(\"The prescision score for predion is {:0.2f}\".format(precision_score(test_y, pred_y, pos_label='1')))\nprint(\"\\n\")\nprint(classification_report(test_y, pred_y))","4088edb6":"# Print out the Recall-Precision Plot\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\n\nplt.figure(figsize = (7,7))\nplot_precision_recall_curve(logreg, test_x, test_y)\nplt.title(\"Precision-Recall curve for Logistic Regression Classifier\")","b30910c6":"# Radial kernel will be used as the kernel function \nfrom sklearn.svm import SVC \n\nsvc = SVC(kernel='rbf', C = 1, gamma='scale')\nsvc.fit(train_x, train_y)\n\npred_y_svc = svc.predict(test_x)","0791a825":"# Print the confusion matrix for the model \nconf_matrix = confusion_matrix(test_y, pred_y_svc)\n\nplt.figure(figsize = (7,7))\nsns.heatmap(conf_matrix, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r')\nplt.title(\"Accuracy Score: {}\".format(accuracy_score(test_y, pred_y_svc)))\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Actual\")\nplt.show()","c95b3fcb":"# Generate the Recall and Prescision scores for the predictions \nprint(\"The recall score for prediction is {:0.2f}\".format(recall_score(test_y, pred_y_svc, pos_label='1')))\nprint(\"The prescision score for predion is {:0.2f}\".format(precision_score(test_y, pred_y_svc, pos_label='1')))\nprint(\"\\n\")\nprint(classification_report(test_y, pred_y_svc))","f3e51f8a":"# Print out the Recall-Precision Plot\nplt.figure(figsize = (7,7))\nplot_precision_recall_curve(svc, test_x, test_y)\nplt.title(\"Precision-Recall curve for Support Vector Classifier\")","d0994a3f":"# Here I will apply GridSearch to find out the best hyperparameters \nfrom sklearn.model_selection import GridSearchCV\nfrom pprint import pprint\n\n# Create the Random Forest Classifier and \nrfc = RandomForestClassifier(random_state = 100)\npprint(rfc.get_params())","ca10d37a":"# In this case three parameters are tuned: Number of trees used in prediction (n_estimaters), Maximum number of features randomly selected in each decision tree (max_features) and maximum number of levels in decision tree\nparams = {'n_estimators': [10, 50, 100],\n          'max_features': ['auto', 'sqrt'],\n          'max_depth': [10, 25, 50]}\n\n# Initiate the grid search model and fit the training data \ngrid_search = GridSearchCV(rfc, \n                           param_grid = params, \n                           cv = 5, \n                           n_jobs = -1, \n                           verbose = 2)\n\n# fit the training data\ngrid_search.fit(train_x, train_y)\ngrid_search.best_params_","fbec4ae2":"grid_search.best_estimator_","e221a987":"features = data.drop('Class', axis = 1).columns\nfeature_importances = grid_search.best_estimator_.feature_importances_\nfeature_indices = np.argsort(feature_importances)\nfeatures = [features[i] for i in feature_indices]\n\nplt.figure(figsize = (10,10))\nplt.barh(range(len(feature_indices)), feature_importances[feature_indices], color='steelblue')\nplt.title('Feature Importances from Random Forest Classifier')\nplt.xlabel('Importances')\nplt.yticks(range(0, len(features)), features)\nplt.show()","2f02a681":"# Make predictions on testing data \npred_y_rfc = grid_search.best_estimator_.predict(test_x)","adf47087":"# Print the confusion matrix\nconf_matrix = confusion_matrix(test_y, pred_y_rfc)\n\nplt.figure(figsize = (7,7))\nsns.heatmap(conf_matrix, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r')\nplt.title(\"Accuracy Score: {}\".format(accuracy_score(test_y, pred_y_rfc)))\nplt.xlabel(\"Prediction\")\nplt.ylabel(\"Actual\")\nplt.show()","062fce8b":"# Generate the Recall and Prescision scores for the predictions \nprint(\"The recall score for prediction is {:0.2f}\".format(recall_score(test_y, pred_y_rfc, pos_label='1')))\nprint(\"The prescision score for predion is {:0.2f}\".format(precision_score(test_y, pred_y_rfc, pos_label='1')))\nprint(\"\\n\")\nprint(classification_report(test_y, pred_y_rfc))","bb771b31":"# Print out the Recall-Precision Plot\nplt.figure(figsize = (7,7))\nplot_precision_recall_curve(grid_search.best_estimator_, test_x, test_y)\nplt.title(\"Precision-Recall curve for Random Forest Classifier\")","40a622ad":"## Modeling Process ","b1efedec":"## Import Data ","898598cf":"## Credict Card Fraud Prediction \n\nFraud detection for credit card is a common methodology applied by financial institations in the globe. As a person having strong interest in FinTech and risk-management, it's great to apply a seriea of machine learning models to analyse and make fraud predictions based on the credit card transaction data in September 2013 by european cardholders. It is also a great opportunity for me to learn the magic of conducting machine learning using Scikit-learn. \n\nThree Models I will apply:\n\n1. Logistic Regression Classifier \n4. Support Vector Machine (SVM)\n5. Random Forest Classidier (RFC)\n\nThen I will apply ROC&AUC, Recall, Precision and Accuracy to evaluate the model performances.\n\nNow, let's take a look at the data:","c3d564b9":"The dataset has 284,806 rows and 31 variables. It has no missing value. Now it's time to visualize the variables.","e9747260":"## Import Packages","01db54e4":"### Support Vector Machine (SVM)","8508824a":"## Data Exploration ","554081f1":"### Random Forest Classifier (RFC)","d89d3838":"From the model performances, the Random Forest Classifier has done the best prediction, having the accuracy score of 0.99, recall score of 0.79 and precision score of 0.97. ","e4d6c5d7":"Make the plot of features importance","70b7c560":"Take a look at the fraud composition","f6ee5c57":"### Logistic Regression Classifier","f18b703c":"### Base Model: Dummy Classifier (Only used to as the benchmark to indicate how other machine learning models improved the prediction","36cb830c":"Since the two plots above are presenting the time and amount distribution for all transactions, it's time to take a look at these two variables compared by fraud and non-fraud transactions.","36e39bb0":"Given time could an important feature in determining if the transaction is fraud, let's take a closer look at the relationship between time and amount, also the relationship between time and fraud transaction","4ead0788":"No strong correlation exists within the numeric variables, which make sense since they may come from the PCA process. Then, it's time to prepare data for the modeling progress.","f70c299f":"From the comparison, it's clear that the majority of fraud transaction happened within 30 hours, and most of the transactions are lower than 200. Since the other variables do not have names, it is important to take a look at their relationship to each other by seeing the correlations ","43c8d521":"Check the overall distribution of all the numeric variables. Given the large size of the data, I will use a sample of 60,000 to present its distributions","e4ea237d":"The data is strongly imbalanced, there are only 492 fraud transactions within the whole transaction record. "}}