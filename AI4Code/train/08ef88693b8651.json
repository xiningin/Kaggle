{"cell_type":{"8c6f124a":"code","6349a8ee":"code","4dc1092b":"code","bd620dd3":"code","e43298df":"code","9434d746":"code","6f08b27a":"code","8e5010fc":"code","ec0cec31":"code","952ba535":"code","f900a173":"code","a1943b38":"code","04e3ec5c":"code","f9e6e87c":"code","b114b2cc":"code","3681658d":"code","8b59b105":"code","89f44da5":"code","86887d49":"code","c1eef7ee":"code","9d3c77a9":"code","0f0e5244":"code","b9c8b57c":"markdown","1bc4cbd2":"markdown","181c054f":"markdown","56a0fef3":"markdown","333394a2":"markdown","d0c2a736":"markdown","43f26d04":"markdown","292a9071":"markdown","c9f121d8":"markdown","46353e4d":"markdown","118bc033":"markdown","6e708497":"markdown","11b58898":"markdown"},"source":{"8c6f124a":"import gc\nimport glob\nimport os\nimport random\nimport time\nfrom datetime import date, datetime\n\nimport joblib\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport shap\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n%matplotlib inline","6349a8ee":"pd.set_option(\"display.max_columns\", 96)\npd.set_option(\"display.max_rows\", 96)\n\nplt.rcParams['figure.figsize'] = (12, 9)\nplt.style.use('ggplot')\n\nshap.initjs()","4dc1092b":"# Size of the dataset is limited to just n_debug_samples, because SHAP calculation is quite time-consuming.\n\ndebug = True\nn_debug_samples = 10000","bd620dd3":"from multiprocessing import Pool\n\n\ndef create_lag(df_code, n_lag=[3, 7, 14, ], shift_size=1):\n    code = df_code['assetCode'].unique()\n\n    for col in return_features:\n        for window in n_lag:\n            rolled = df_code[col].shift(shift_size).rolling(window=window)\n            lag_mean = rolled.mean()\n            lag_max = rolled.max()\n            lag_min = rolled.min()\n            lag_std = rolled.std()\n            df_code['%s_lag_%s_mean' % (col, window)] = lag_mean\n            df_code['%s_lag_%s_max' % (col, window)] = lag_max\n            df_code['%s_lag_%s_min' % (col, window)] = lag_min\n\n    return df_code.fillna(-1)\n\n\ndef generate_lag_features(df, n_lag=[3, 7, 14]):\n    \n    \n    features = ['time', 'assetCode', 'assetName', 'volume', 'close', 'open',\n                'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n                'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n                'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n                'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n                'returnsOpenNextMktres10', 'universe']\n\n    assetCodes = df['assetCode'].unique()\n    print(assetCodes)\n    all_df = []\n    df_codes = df.groupby('assetCode')\n    df_codes = [df_code[1][['time', 'assetCode'] + return_features]\n                for df_code in df_codes]\n    print('total %s df' % len(df_codes))\n\n    pool = Pool(4)\n    all_df = pool.map(create_lag, df_codes)\n\n    new_df = pd.concat(all_df)\n    new_df.drop(return_features, axis=1, inplace=True)\n    pool.close()\n\n    return new_df\n\n\ndef mis_impute(data):\n    for i in data.columns:\n        if data[i].dtype == \"object\":\n            data[i] = data[i].fillna(\"other\")\n        elif (data[i].dtype == \"int64\" or data[i].dtype == \"float64\"):\n            data[i] = data[i].fillna(data[i].mean())\n        else:\n            pass\n    return data\n\n\ndef data_prep(market_train):\n    lbl = {k: v for v, k in enumerate(market_train['assetCode'].unique())}\n    market_train['assetCodeT'] = market_train['assetCode'].map(lbl)\n    market_train = market_train.dropna(axis=0)\n    return market_train\n\n\ndef exp_loss(p, y):\n    y = y.get_label()\n    grad = -y * (1.0 - 1.0 \/ (1.0 + np.exp(-y * p)))\n    hess = -(np.exp(y * p) * (y * p - 1) - 1) \/ ((np.exp(y * p) + 1)**2)\n    return grad, hess","e43298df":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\nprint('Done!')\n\n(market_train_df, news_train_df) = env.get_training_data()","9434d746":"market_train_df['time'] = market_train_df['time'].dt.date\nmarket_train_df = market_train_df.loc[market_train_df['time'] >= date(\n    2010, 1, 1)]\n\n\nreturn_features = ['returnsClosePrevMktres10',\n                   'returnsClosePrevRaw10', 'open', 'close']\nn_lag = [3, 7, 14]\nnew_df = generate_lag_features(market_train_df, n_lag=n_lag)\nmarket_train_df = pd.merge(market_train_df, new_df,\n                           how='left', on=['time', 'assetCode'])\n\nmarket_train_df = mis_impute(market_train_df)\nmarket_train_df = data_prep(market_train_df)","6f08b27a":"if debug:\n    market_train_df = market_train_df.iloc[:n_debug_samples, :]\n\n\nup = market_train_df['returnsOpenNextMktres10'] >= 0\nuniverse = market_train_df['universe'].values\nd = market_train_df['time']\n\n\nfcol = [c for c in market_train_df if c not in [\n    'assetCode', 'assetCodes', 'assetCodesLen', 'assetName', 'audiences',\n    'firstCreated', 'headline', 'headlineTag', 'marketCommentary', 'provider',\n    'returnsOpenNextMktres10', 'sourceId', 'subjects', 'time', 'time_x',\n    'universe', 'sourceTimestamp']]\n\n\nX = market_train_df[fcol]\nup = up.values\nr = market_train_df.returnsOpenNextMktres10.values\n\nmins = np.min(X, axis=0)\nmaxs = np.max(X, axis=0)\nrng = maxs - mins\nX = 1 - ((maxs - X) \/ rng)\nassert X.shape[0] == up.shape[0] == r.shape[0]","8e5010fc":"X_train, X_test, up_train, up_test, r_train, r_test, u_train, u_test, d_train, d_test = model_selection.train_test_split(\n    X, up, r, universe, d, test_size=0.25, random_state=99)\n\n\ntrain_cols = X_train.columns.tolist()\n\ntrain_data = lgb.Dataset(X_train, label=up_train.astype(int), \n                         feature_name=train_cols)\ntest_data = lgb.Dataset(X_test, label=up_test.astype(int), \n                        feature_name=train_cols, reference=train_data)","ec0cec31":"# LGB parameters:\nparams = {'learning_rate': 0.05,\n          'boosting': 'gbdt', \n          'objective': 'binary',\n          'num_leaves': 2000,\n          'min_data_in_leaf': 200,\n          'max_bin': 200,\n          'max_depth': 16,\n          'seed': 2018,\n          'nthread': 10,}\n\n\n# LGB training:\nlgb_model = lgb.train(params, train_data, \n                      num_boost_round=1000, \n                      valid_sets=(test_data,), \n                      valid_names=('valid',), \n                      verbose_eval=25, \n                      early_stopping_rounds=20)","952ba535":"# DF, based on which importance is checked\nX_importance = X_test\n\n# Explain model predictions using shap library:\nexplainer = shap.TreeExplainer(lgb_model)\nshap_values = explainer.shap_values(X_importance)","f900a173":"# Plot summary_plot\nshap.summary_plot(shap_values, X_importance)","a1943b38":"# Plot summary_plot as barplot:\nshap.summary_plot(shap_values, X_importance, plot_type='bar')","04e3ec5c":"X_importance.returnsClosePrevRaw10_lag_3_mean.value_counts()","f9e6e87c":"plt.hist(X_importance.returnsClosePrevRaw10_lag_3_mean, bins=100)","b114b2cc":"shap.dependence_plot(\"returnsClosePrevRaw10_lag_3_mean\", shap_values, X_importance)","3681658d":"shap.dependence_plot(\"volume\", shap_values, X_importance)","8b59b105":"X_interaction = X_importance.iloc[:500,:]\n\nshap_interaction_values = shap.TreeExplainer(lgb_model).shap_interaction_values(X_interaction)","89f44da5":"shap.summary_plot(shap_interaction_values, X_interaction)","86887d49":"# Raw dependence plot:\n\nshap.dependence_plot(\n    \"returnsClosePrevRaw10_lag_3_mean\",\n    shap_values, X_importance)","c1eef7ee":"# Interaction values dependence plot capturing main effects:\n\nshap.dependence_plot(\n    (\"returnsClosePrevRaw10_lag_3_mean\", \"returnsClosePrevRaw10_lag_3_mean\"),\n    shap_interaction_values, X_interaction)","9d3c77a9":"# Interaction values dependence plot capturing interaction effects:\n\nshap.dependence_plot(\n    (\"returnsClosePrevRaw10_lag_3_mean\", \"returnsOpenPrevMktres10\"),\n    shap_interaction_values, X_interaction)","0f0e5244":"shap_sum = np.abs(shap_values).mean(axis=0)\nimportance_df = pd.DataFrame([X_importance.columns.tolist(), shap_sum.tolist()]).T\nimportance_df.columns = ['column_name', 'shap_importance']\nimportance_df = importance_df.sort_values('shap_importance', ascending=False)\nimportance_df","b9c8b57c":"## SHAP importance:","1bc4cbd2":"### Get important features according to SHAP:","181c054f":"### Summary plot - most important features:\n\nWe will begin analysis of importance with most important features for a model based on validation set.\nHere, we will use `summary_plot`. This type of plot aggregates SHAP values for all the features and all samples in the selected set.\nThen SHAP values are sorted, so the first one shown is the most important feature. In addition to that, we are provided with information of how each feature affects the model output.\n\nTwo most important features are `returnsClosePrevRaw10_lag_3_mean` and `_max`. But here an interesting pattern can be noticed - low values of both of these features are clustered in a very dense region (blue blob), whereas features such as `returnsOpenPrevRaw10` and `returnsClosePrevRaw10` have much more uniform distribution, where for the first one (`returnsOpenPrevRaw10`) its high values push the prediction to 1 and for the second (`returnsClosePrevRaw10`) its low values push the prediction to 1.","56a0fef3":"### Interaction - a summary:","333394a2":"### SHAP Interaction Values\n\nAs cited after [shap notebook](https:\/\/github.com\/slundberg\/shap\/blob\/master\/notebooks\/tree_explainer\/NHANES%20I%20Survival%20Model.ipynb):\n\n\"See the Tree SHAP paper for more details, but briefly, SHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented in the latest version of XGBoost with the pred_interactions flag. With this flag XGBoost returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. The main effects are similar to the SHAP values you would get for a linear model, and the interaction effects captures all the higher-order interactions are divide them up among the pairwise interaction terms. Note that the sum of the entire interaction matrix is the difference between the model's current output and expected output, and so the interaction effects on the off-diagonal are split in half (since there are two of each). When plotting interaction effects the SHAP package automatically multiplies the off-diagonal values by two to get the full interaction effect.\"\n\n\nFor this one, we will further limit the dataset size to speed up the computation.","d0c2a736":"When interaction between `returnsClosePrevRaw10_lag_3_mean` and `returnsOpenPrevMktres10` is checked, one interesting result is that for returnsClosePrevRaw10_lag_3_mean equal to 0, higher values of returnsOpenPrevMktres10 seem to push the model output lower, whereas higher values of returnsOpenPrevMktres10 for values of returnsClosePrevRaw10_lag_3_mean above 0.6 push the model output higher.","43f26d04":"A lot of values are simply 0, probably this is the blue cluster on SHAP plot.","292a9071":"Vertical disperion for interaction plot, which captures main effects, is lower than in the original plot, which was to be expected.","c9f121d8":"### Dependence plot:\n\nIntepretation of an influence of a single feature on the model output is given by dependence plots.\nPlot below represents the influence of `returnsClosePrevRaw10_lag_3_mean`.  \nVertical dispersion at a single value of `returnsClosePrevRaw10_lag_3_mean` represents interaction with other features from the data.\nDependence plot automatically selected `returnsOpenPrevMktres10` as a feature for coloring.\nWe can see that high values of returnsClosePrevRaw10_lag_3_mean influences the model output more significantly for observations, where returnsOpenPrevMktres10 has also high values.\nWorth noting is again the cluster containing values of returnsClosePrevRaw10_lag_3_mean equal to 0.","46353e4d":"## Explaining your model with SHAP values\n\nMost of the models optimized for performance, like GBMs or Neural Networks are black-box models.\nFor those models gaining an intuition of what is happening inside - how certain features influence its output, may be difficult.\nEven though for LightGBM or XGBoost feature importance can be given, it isn't always reliable. More details about why this is the case can be found in a great [article](https:\/\/towardsdatascience.com\/interpretable-machine-learning-with-xgboost-9ec80d148d27) by one of SHAP authors.\nA proposed remedy is using [**SHAP**](https:\/\/github.com\/slundberg\/shap), which provides a unified approach for intepreting output of machine learning methods.\nDetailed information about how it works can be found in the above linked article and their [NIPS paper](http:\/\/papers.nips.cc\/paper\/7062-a-unified-approach-to-interpreting-model-predictions).\n\nBeing able to explain model output is especially important in cases, when it is going to be deployed as a service or application. Knowing whether it learns useful features is crucial. \nOn tabular data, where domain knowledge is the key, one can check feature importance and deduce, if most important features according to the model can influence the predicted values in reality. Based on this, model reliability can be estimated. To give a basic example, when trying to predict price of a house, factors such as it's area or location should directly influence the price. If instead of those, a model will consider totally random feature important (like, let's say, mean temperature of a random month during last year), it is easy to arrive at a conclusion that something is definitely wrong.\n\nIn case of competitions, being able to interpret model output gives a direction of feature engineering, which is worth following.\nHere, we do not care about importance making sense this much but rather creating new features further improving model performance.\nWhen we know which of the features are considered important, there is a chance that features derived from those will provide model with additional information, enabling a boost in score.\nAnother thing to consider is the differences between distribution of certain features in train and test data. If train set feature distribution is very different from its test set distribution and it is considered very important by a model, there may be a risk that model performance on the test set may be not very reliable.\n\nNow,  let's check what kind of information SHAP will give us in case of the Two Sigma competition!\nBecause the aim of the kernel is to showcase model importance output, basis for feature engineering is taken from a kernel, which scores well on LB. \n\n#### Data processing & feature engineering taken from: https:\/\/www.kaggle.com\/qqgeogor\/eda-script-67. Thanks!","118bc033":"### Interaction - dependence plots:\n\nUsing dependence plots with interaction values enables assessment of main and interaction effects between features.\nFirst, let's take a look at raw `returnsClosePrevRaw10_lag_3_mean` dependence plot again and then compare it to interaction plots capturing both main and interaction effects.","6e708497":"![](http:\/\/)Let's take a closer look at the feature:","11b58898":"### Dependence plot - `volume`\n\nLet's take a look at how `volume` will affect the model output.\nThis time, our selected feature is plotted against `returnsClosePrevRaw10_lag_3_mean`.\nA direct relationship between volume and returnsClosePrevRaw10_lag_3_mean is difficult to notice.\nThere are some values of volume significantly higher than the most, but their influence on model output isn't very big and their values of returnsClosePrevRaw10_lag_3_mean are both low and high."}}