{"cell_type":{"12b2950e":"code","9238bd96":"code","507e3d4c":"code","5a6f67b4":"code","ddb2269d":"code","132970f4":"code","e61e5728":"code","4c94d22c":"code","224015ea":"code","869b1d22":"code","2125789a":"code","1affd27c":"code","df324ee9":"markdown","fdaba2d0":"markdown","38c860ed":"markdown","ea9a283e":"markdown","26a61436":"markdown","e92c7c9a":"markdown","d2177f91":"markdown","2a144b5b":"markdown","ef9b4069":"markdown","09eebd2f":"markdown","3194033c":"markdown"},"source":{"12b2950e":"# importacao das bibliotecas\n\nimport pandas as pd;\nimport numpy as np;\nimport matplotlib.pyplot as plt;\nimport matplotlib.image as mpimg;\nimport seaborn as sns;\n# %matplotlib inline\n\nnp.random.seed(2);\n\nfrom sklearn.model_selection import train_test_split;\nfrom sklearn.metrics import confusion_matrix, classification_report;\nimport itertools;\n\nfrom keras.utils.np_utils import to_categorical;\nfrom keras.models import Sequential;\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D;\nfrom keras.optimizers import RMSprop;\nfrom keras.preprocessing.image import ImageDataGenerator;\nfrom keras.callbacks import ReduceLROnPlateau;\n\nsns.set(style='white', context='notebook', palette='deep');","9238bd96":"# import do dataset\ndf = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\n\n# seperacao do y_train\nY = df[\"label\"];\n\n# seperacao do x_train\nX = df.drop(labels = [\"label\"],axis = 1) ;","507e3d4c":"g = sns.countplot(Y)\nY.value_counts()","5a6f67b4":"# exibicao do primeiro elemento do dataframe\nplt.imshow(X.values[0].reshape(28,28), cmap=plt.cm.binary)\nplt.show()\nprint('Correct label: {}'.format(Y[0]))","ddb2269d":"num_classes = 10\n\nX = (X \/ 255.0);\n\nx = X.values.reshape(-1,28,28,1);\n\ny = to_categorical(Y, num_classes);","132970f4":"# Separando uma parte para treino (70%) e outra para valida\u00e7\u00e3o (30%)\n\nrandom_seed = 2;\nX_train, X_val, Y_train, Y_val = train_test_split(x, y, test_size = 0.3, random_state=random_seed)\n\nprint('Qtde de treino: {}'.format(len(X_train)))\nprint('Qtde de valida\u00e7\u00e3o: {}'.format(len(X_val)))","e61e5728":"model = Sequential();\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)));\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'));\nmodel.add(MaxPool2D(pool_size=(2,2)));\nmodel.add(Dropout(0.2));\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'));\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'));\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)));\nmodel.add(Dropout(0.2));\n\n\nmodel.add(Flatten());\nmodel.add(Dense(256, activation = \"relu\"));\nmodel.add(Dropout(0.2));\nmodel.add(Dense(10, activation = \"softmax\"));\nmodel.summary()","4c94d22c":"optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0);\n\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"]);\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001);\nepochs = 30;\nbatch_size = 86;\n\nhistory = model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, \n          validation_data = (X_val, Y_val), verbose = 2);","224015ea":"fig, ax = plt.subplots(1,2, figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","869b1d22":"import itertools\n\n#Plot the confusion matrix. Set Normalize = True\/False\ndef plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize=(10,10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        cm = np.around(cm, decimals=2)\n        cm[np.isnan(cm)] = 0.0\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","2125789a":"# Vendo alguns reports# Vendo alguns reports\n# Usando sklearn\nimport numpy as np\n\n# Classificando toda base de teste\ny_pred = model.predict_classes(X_val)\n# voltando pro formato de classes\ny_test_c = np.argmax(Y_val, axis=1)\ntarget_names = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n\n#Confution Matrix\ncm = confusion_matrix(y_test_c, y_pred)\nplot_confusion_matrix(cm, target_names, normalize=False, title='Confusion Matrix')\n\nprint('Classification Report')\nprint(classification_report(y_test_c, y_pred, target_names=target_names))","1affd27c":"# Gerando sa\u00edda para dataset de teste\n\n#Carrega dataset de teste\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nprint(\"Qtde de testes: {}\".format(len(test)))\n# Bota no formato numpy e normaliza\nx_test = test.values.reshape(len(test),28,28,1)\nx_test = x_test.astype('float32')\nx_test \/= 255\n\n# Faz classifica\u00e7\u00e3o para dataset de teste\ny_pred = model.predict_classes(x_test)\n\n# Verficando algum exemplo\ni = 0\nplt.imshow(test.values[i].reshape(28,28), cmap=plt.cm.binary)\nplt.show()\nprint('Previsto: {}'.format(y_pred[i]))\n\n# Botando no formato de sa\u00edda (competi\u00e7\u00e3o Kaggle)\nresults = pd.Series(y_pred,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,len(y_pred)+1),name = \"ImageId\"),results],axis = 1)\nprint(submission.head(10))\n#Salvando Arquivo\nsubmission.to_csv(\"out.csv\",index=False)","df324ee9":"## Manipula\u00e7\u00e3o dos dados\n\n#### Esta c\u00e9lula plota um grafico para ilustrar como os dados est\u00e3o distribuidos de acordo com o conjunto universo, nesse caso sendo `de 0 a 9` para ter uma no\u00e7\u00e3o se a base de dados n\u00e3o \u00e9 tendenciosa, ou seja n\u00e3o esta bem distribuida sobre o conjunto de dados","fdaba2d0":"## Criando e treinando o Modelo","38c860ed":"# Gerando Sa\u00edda","ea9a283e":"#### Esta celula exibibe o primeiro elemento da base de dados de forma grafica, matriz (28x28)","26a61436":"### Criando o modelo Sequential\n\n#### Conv2D: Cria\u00e7\u00e3o de 4 camadas 2 com 32 filtros e 2 com 64 filtros\n#### MaxPooling2D: Duas camadas 2x2, uma para cada dupla de camadas Conv2D\n#### Flatten: Camada adicionada ao final\n#### Dense: 2 Camadas\n* 1 para cada valor do pixel (1 a 255 normalizado)\n* 1 para cada valor de saida (10 possibilidades)\n\n#### Dropout: 3 Camadas \n* 2 de 0.2 para a dupla de camadas Conv2D\n* 1 de 0.2 para camada de sa\u00edda\n","e92c7c9a":"## Justificativas\n\n#### **Conv2D** - Ja que a profundidade da sa\u00edda de uma camada de convolu\u00e7\u00e3o \u00e9 proporcional a quantidade de filtros aplicados. Quanto mais profundas s\u00e3o as camadas das convolu\u00e7\u00f5es, mais detalhados s\u00e3o os tra\u00e7os identificados com o activation map. Sendo assim, foi optado por criar 4 cadas convolucionais separadas em 2 duplas (1 dupla com 32 filtros e 1 dupla com 64 filtros). O padding utilizado foi o padrao para que as camadas n\u00e3o diminuam muito mais r\u00e1pido do que \u00e9 necess\u00e1rio para o aprendizado.\n* 32 Filtros - Tamnho encontrado atravez de pesquisas em outras resolucoes para problemas similares que tratam com escrita, como por exemplo letras do alfabeto\n* 64 Filtros - Camada para refinar as saidas do primeiro conjunto de camadas, tambem encontrando em problemas similares\n\n#### **Kernel_size** - 2 tipos, sendo eles (5x5) e (3x3). Vale ressaltar que nao houve grande ganho ao impregar essa modifica\u00e7\u00e3o\n* (5x5) - Segundo pesquisas decorre para compensar a quantidade menor de filtros\n* (3x3) - Segundo pesquisas decorre para compensar a quantidade maior de filtros\n\n#### **Epochs** - Quantidade de epocas usada foi de 30 para se ter uma quantidade maior de iteracoes de treino no modelo\n\n#### **MaxPooling2D** - Adiciona-se uma camada de Pooling entre as camadas convolucionais sucessivas da CNN para reduzir progressivamente o tamanho espacial da representa\u00e7\u00e3o para reduzir a quantidade de par\u00e2metros e computa\u00e7\u00e3o na rede e, portanto, tamb\u00e9m controlar o overfitting. A forma mais comum \u00e9 uma camada de agrupamento com filtros de tamanho 2x2 aplicados com um passo de 2 amostras abaixo de cada fatia de profundidade na entrada de 2 ao longo de largura e altura, descartando assim 75% das ativa\u00e7\u00f5es.\n\n#### **Dropout** - Foi ultilizado 3 camadas de dropout 2 para as duplas de convolucao para eveitar o overfit com valor padrao de 0.2 e uma entre as camdas densas tambem com o valor padrao de 0.2\n\n#### **Fun\u00e7\u00e3o de ativa\u00e7\u00e3o** - A Funcao usada foi ReLU (Rectified Linear Units), que nada mais \u00e9 que uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o t\u00edpica de uma rede neural. Para cada valor da imagem \u00e9 aplicada a fun\u00e7\u00e3o f(x) = max(0, x), ou seja, caso algum valor da pilha de imagens seja negativo, ele \u00e9 alterado para zero facilitando assim o calculo.\n\n#### **Camadas densas** - Usadas para representar cada classe de saida (0 a 10). A segunda para tratar do valor de range do pixels antes da normalizacao (0 a 255)\n\n```\nEm resumo, como o projeto fornecido como parametro de base ja obtinha um resulta muito bom de 99% a tarefa de melhora-lo nao houve grandes ganhos obtendo-se assim 99.189% no entando vale ressaltar que adicionando as complexidades extras houve um aumento siginificavel no tempo de treino do modelo, levando a considerar as modifica\u00e7\u00f5es feitas\n```","d2177f91":"# Bibliotecas e Dados","2a144b5b":"#### Esta c\u00e9lula formata os dados preditores para o formato de matriz 28x28 e em sequencia normaliza os valores para ficarem entre `0 e 1` para garantir que todos os dados estejam no conjunto de dominio da fun\u00e7\u00e3o. A partir disso tambem \u00e9 feito a categoriza\u00e7\u00e3o dos atributos preditivos.","ef9b4069":"# Avaliando o Modelo","09eebd2f":"#### Esta celula faz a separa\u00e7\u00e3o dos dados de treino e de dados para valida\u00e7\u00e3o sendo eles:\n* 70% Para dados de treino\n* 30% Para dados de valida\u00e7\u00e3o\n\n#### Essa divis\u00e3o foi usada aleatoriamente de acordo com algumas pesquisas de outros problemas e disciplinas cursadas anteriormente essa era a propor\u00e7\u00e3o utilizada.","3194033c":"## Leitura dos dados\n\n#### Esta c\u00e9lula faz a leitura da base de dados e a separa\u00e7\u00e3o dos atributos preditivos e dos preditores, `Y_train` e `X_train`"}}