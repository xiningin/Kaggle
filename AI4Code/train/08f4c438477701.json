{"cell_type":{"11c942a1":"code","d43c2934":"code","97f88e78":"code","34a72cb8":"code","54f418b0":"code","320603a1":"code","e659c8be":"code","7461488e":"code","ebe699c1":"code","e0d02fa2":"code","5d781845":"code","523648e2":"code","c2d34cf2":"code","07ddf7b0":"code","0422e701":"code","95e9b820":"code","b81f418d":"code","e29a1631":"code","cc784685":"code","0078b5c4":"code","56cf9943":"code","62a6405a":"code","17673fb3":"code","972ce799":"code","b2d06d89":"code","c759f274":"code","b7232380":"code","6f50571e":"code","219da309":"code","ce98b77c":"code","f00f9d86":"code","87442f42":"code","385879ee":"code","0d99bc99":"code","75e3a6c7":"code","478a68fd":"code","c06ea0e5":"code","e72f1e20":"code","b469b248":"code","d94ead7e":"code","32a03fef":"code","c3cf355b":"code","e30cab5e":"code","a0b63b92":"code","40700354":"code","0f272b82":"markdown","8c7271e7":"markdown","5fde89a1":"markdown","c019707c":"markdown","987ce712":"markdown","f7fcb7aa":"markdown","771b222d":"markdown","84b84d64":"markdown","67f32f85":"markdown","75f1f411":"markdown","33bd7af3":"markdown","1d599677":"markdown","dcd29e42":"markdown"},"source":{"11c942a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d43c2934":"df = pd.read_json('..\/input\/arxiv-papers-2010-2020\/arXiv_title_abstract_20200809_2011_2020.json')\ndf.head(2)","97f88e78":"df.shape","34a72cb8":"df.sort_values(\"title\", inplace = True) \nbool_series = df[\"title\"].duplicated() \ndf[bool_series][:2]","54f418b0":"for i in range(len(df[bool_series][:5])):\n    print(\"title:\",df['title'][i])","320603a1":"#Search for duplicate\ndf.loc[df['title'] == \"The World as Evolving Information\"]","e659c8be":"df[\"title\"].duplicated().sum()","7461488e":"df[\"title\"].isna().sum()","ebe699c1":"df.drop_duplicates(subset=['title'],inplace=True)  #dropping duplicates\ndf.dropna(axis=0,inplace=True)   #dropping na","e0d02fa2":"df.shape","5d781845":"#Dictionary that we will use for expanding the contractions:\ncontraction_mapping = {\n\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n\"i'd\": \"i would\",\"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\"you're\": \"you are\", \"you've\": \"you have\"}","523648e2":"#Clean text by removing unnecessary characters and altering the format of words.\n\nimport re\ndef clean_text(text):\n    row = text.lower()\n    row = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in row.split(\" \")]) \n\n    row=re.sub(\"(\\\\t)\", ' ', str(row))\n    row=re.sub(\"(\\\\r)\", ' ', str(row))\n    row=re.sub(\"(\\\\n)\", ' ', str(row))\n    row=re.sub(\"(__+)\", ' ', str(row))   #remove _ if it occors more than one time consecutively\n    row=re.sub(\"(--+)\", ' ', str(row))   #remove - if it occors more than one time consecutively\n    row=re.sub(\"(~~+)\", ' ', str(row))   #remove ~ if it occors more than one time consecutively\n    row=re.sub(\"(\\+\\++)\", ' ', str(row))   #remove + if it occors more than one time consecutively\n    row=re.sub(\"(\\.\\.+)\", ' ', str(row))   #remove . if it occors more than one time consecutively\n    row=re.sub(r\"[<>()|:{}#&+$\u00a9\u00f8\\[\\]\\'\\\",;?~*!]\", ' ', str(row)) \n    row=re.sub(\"(mailto:)\", ' ', str(row)) #remove mailto:\n    row=re.sub(r\"(\\\\x9\\d)\", ' ', str(row)) #remove \\x9* in text\n    row=re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(row)) #replace INC nums to INC_NUM\n    row=re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', str(row)) #replace CM# and CHG# to CM_NUM\n    row=re.sub(\"(\\.\\s+)\", ' ', str(row)) #remove full stop at end of words(not between)\n    row=re.sub(\"(\\-\\s+)\", ' ', str(row)) #remove - at end of words(not between)\n    row=re.sub(\"(\\:\\s+)\", ' ', str(row)) #remove : at end of words(not between)\n    row=re.sub(\"(\\s+.\\s+)\", ' ', str(row))  #remove any single charecters hanging between 2 spaces\n\n    #Replace any url as such https:\/\/abc.xyz.net\/browse\/sdf-5327 ====> abc.xyz.net\n    try:\n        url = re.search(r'((https*:\\\/*)([^\\\/\\s]+))(.[^\\s]+)', str(row))\n        repl_url = url.group(3)\n        row = re.sub(r'((https*:\\\/*)([^\\\/\\s]+))(.[^\\s]+)',repl_url, str(row))\n    except:\n        pass #there might be emails with no url in them\n\n    row = re.sub(\"(\\s+)\",' ',str(row)) #remove multiple spaces\n    #Should always be last\n    row=re.sub(\"(\\s+.\\s+)\", ' ', str(row)) #remove any single charecters hanging between 2 spaces\n\n    return row","c2d34cf2":"#Abstract Cleaning\nabstract = [clean_text(doc) for doc in df['abstract']]\n\n#Title Cleaning\n#add the START and END special tokens at the beginning and end of the title:\ntitle = ['_START_ '+ clean_text(doc) + ' _END_' for doc in df['title']]","07ddf7b0":"#Take a look at the top 3 abstract and their title:\nfor i in range(5):\n    print(\"abstract:\",abstract[i])\n    print(\"title:\",title[i])\n    print(\"\\n\")","0422e701":"cleaned_df = pd.DataFrame({'abstract':abstract ,'title':title})","95e9b820":"df=cleaned_df.copy()","b81f418d":"import matplotlib.pyplot as plt\n\nabstract_count = []\ntitle_count = []\nlongest_abstract = 0\nlongest_title=0\n\nfor sent in df['abstract']:\n    abstract_count.append(len(sent.split()))\n    if len(sent.split()) > longest_abstract:\n        longest_abstract = len(sent.split())\n        \nfor sent in df['title']:\n    title_count.append(len(sent.split()))\n    if len(sent.split()) > longest_title:\n        longest_title = len(sent.split())\n\nlength_df = pd.DataFrame({'abstract':abstract_count, 'title':title_count})\nlength_df.hist(bins = 10)\nplt.show()","e29a1631":"longest_abstract,longest_title","cc784685":"#Check how much % of abstract have 0-300 words\ncnt=0\nfor i in df['abstract']:\n    if(len(i.split())<=320):\n        cnt=cnt+1\nprint(cnt\/len(df['abstract']))","0078b5c4":"#Check how much % of title have 0-20 words\ncnt=0\nfor i in df['title']:\n    if(len(i.split())<=20):\n        cnt=cnt+1\nprint(cnt\/len(df['title']))","56cf9943":"max_len_abstract=longest_abstract \nmax_len_title=longest_title","62a6405a":"val_df = df.sample(frac=0.1, random_state=1007)\ntrain_df = df.drop(val_df.index)\ntest_df = train_df.sample(frac=0.1, random_state=1007)\n\ntrain_df.drop(test_df.index, inplace=True)\n(train_df.shape,val_df.shape,test_df.shape)","17673fb3":"x_train=train_df['abstract']\ny_train=train_df['title']\nx_val=val_df['abstract']\ny_val=val_df['title']","972ce799":"from keras.preprocessing.text import Tokenizer \nfrom keras.preprocessing.sequence import pad_sequences","b2d06d89":"#prepare a tokenizer for abstract on training data\nx_tokenizer = Tokenizer()\nx_tokenizer.fit_on_texts(list(x_train))\n\n#convert abstract sequences into integer sequences\nx_train   =   x_tokenizer.texts_to_sequences(x_train) \nx_val   =   x_tokenizer.texts_to_sequences(x_val)\n\n#padding zero upto maximum length\nx_train    =   pad_sequences(x_train,  maxlen=max_len_abstract, padding='post',truncating='post') \nx_val     =   pad_sequences(x_val, maxlen=max_len_abstract, padding='post',truncating='post')\n\nx_voc_size   =  len(x_tokenizer.word_index) +1\nprint(\"Size of vocabulary in x = {}\".format(x_voc_size))","c759f274":"#preparing a tokenizer for title on training data \ny_tokenizer = Tokenizer()\ny_tokenizer.fit_on_texts(list(y_train))\n\n#convert title sequences into integer sequences\ny_train   =   y_tokenizer.texts_to_sequences(y_train) \ny_val   =   y_tokenizer.texts_to_sequences(y_val) \n\n#padding zero upto maximum length\ny_train    =   pad_sequences(y_train, maxlen=max_len_title, padding='post',truncating='post')\ny_val   =   pad_sequences(y_val, maxlen=max_len_title, padding='post',truncating='post')\n\ny_voc_size  =   len(y_tokenizer.word_index) +1\n#len(y_tokenizer.word_index) +1\nprint(\"Size of vocabulary in Y = {}\".format(y_voc_size))","b7232380":"x_train[:1]","6f50571e":"#Upload attention Layer\nfrom shutil import copyfile\n\n# copy our file into the working directory (make sure it has .py suffix)\ncopyfile(src = \"..\/input\/attention\/attention.py\", dst = \"..\/working\/attention.py\")","219da309":"from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional,dot,Activation\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nfrom attention import AttentionLayer\nimport warnings\nfrom keras import backend as K \npd.set_option(\"display.max_colwidth\", 200)\nwarnings.filterwarnings(\"ignore\")\n","ce98b77c":"K.clear_session()\n\nlatent_dim = 300\nembedding_dim=100\n\n# Encoder\nencoder_inputs = Input(shape=(max_len_abstract,))\n\n#embedding layer\nenc_emb =  Embedding(x_voc_size, embedding_dim,trainable=True)(encoder_inputs)\n\n#encoder lstm 1\nencoder_lstm1 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_output1, state_h1, state_c1 = encoder_lstm1(enc_emb)\n\n#encoder lstm 2\nencoder_lstm2 = LSTM(latent_dim,return_sequences=True,return_state=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_output2, state_h2, state_c2 = encoder_lstm2(encoder_output1)\n\n#encoder lstm 3\nencoder_lstm3=LSTM(latent_dim, return_state=True, return_sequences=True,dropout=0.4,recurrent_dropout=0.4)\nencoder_outputs, state_h, state_c= encoder_lstm3(encoder_output2)\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None,))\n\n#embedding layer\ndec_emb_layer = Embedding(y_voc_size, embedding_dim,trainable=True)\ndec_emb = dec_emb_layer(decoder_inputs)\n\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True,dropout=0.4,recurrent_dropout=0.2)\ndecoder_outputs,decoder_fwd_state, decoder_back_state = decoder_lstm(dec_emb,initial_state=[state_h, state_c])\n\n# Attention layer\nattn_layer = AttentionLayer(name='attention_layer')\nattn_out, attn_states = attn_layer([encoder_outputs, decoder_outputs])\n\n# Concat attention input and decoder LSTM output\ndecoder_concat_input = Concatenate(axis=-1, name='concat_layer')([decoder_outputs, attn_out])\n\n#dense layer\ndecoder_dense =  TimeDistributed(Dense(y_voc_size, activation='softmax'))\ndecoder_outputs = decoder_dense(decoder_concat_input)\n\n# Define the model \nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n\nmodel.summary()","f00f9d86":"model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy',metrics=[\"accuracy\"])\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=2)\nhistory=model.fit([x_train,y_train[:,:-1]], y_train.reshape(y_train.shape[0],y_train.shape[1], 1)[:,1:] ,\n                  epochs=20,\n                  callbacks=[es],\n                  batch_size=128, \n                  validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))","87442f42":"model.save(\"my_model\")","385879ee":"from tensorflow import keras\nmodel = keras.models.load_model(\"my_model\")","0d99bc99":"from matplotlib import pyplot \nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\n\nplt.figure(figsize=(20, 8))\nplt.subplot(1, 2, 1)\nplt.plot( acc, label='Training Accuracy')\nplt.plot( val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot( loss, label='Training Loss')\nplt.plot( val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","75e3a6c7":"#build the dictionary to convert the index to word for target and source vocabulary\nreverse_target_word_index=y_tokenizer.index_word \nreverse_source_word_index=x_tokenizer.index_word \ntarget_word_index=y_tokenizer.word_index\n","478a68fd":"# Encode the input sequence to get the feature vector\nencoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n\n# Decoder setup\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(max_len_abstract,latent_dim))\n\n# Get the embeddings of the decoder sequence\ndec_emb2= dec_emb_layer(decoder_inputs) \n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\n#attention inference\nattn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\ndecoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n\n# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_outputs2 = decoder_dense(decoder_inf_concat) \n\n# Final decoder model\ndecoder_model = Model(\n    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n    [decoder_outputs2] + [state_h2, state_c2])","c06ea0e5":"# encoder inference\nencoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n\n# decoder inference\n# Below tensors will hold the states of the previous time step\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_hidden_state_input = Input(shape=(max_len_abstract,latent_dim))\n\n# Get the embeddings of the decoder sequence\ndec_emb2= dec_emb_layer(decoder_inputs)\n\n# To predict the next word in the sequence, set the initial states to the states from the previous time step\ndecoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n\n#attention inference\nattn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\ndecoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n\n# A dense softmax layer to generate prob dist. over the target vocabulary\ndecoder_outputs2 = decoder_dense(decoder_inf_concat)\n\n# Final decoder model\ndecoder_model = Model(\n[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n[decoder_outputs2] + [state_h2, state_c2])","e72f1e20":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    e_out, e_h, e_c = encoder_model.predict(input_seq)\n    \n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1,1))\n    \n    # Populate the first word of target sequence with the start word.\n    target_seq[0, 0] = target_word_index['start']\n\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n      \n        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_token = reverse_target_word_index[sampled_token_index]\n        \n        if(sampled_token!='end'):\n            decoded_sentence += ' '+sampled_token\n\n        # Exit condition: either hit max length or find stop word.\n        if (sampled_token == 'end'  or len(decoded_sentence.split()) >= (max_len_title-1)):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1,1))\n        target_seq[0, 0] = sampled_token_index\n\n        # Update internal states\n        e_h, e_c = h, c\n\n    return decoded_sentence","b469b248":"def seq2summary(input_seq):\n    newString=''\n    for i in input_seq:\n        if((i!=0 and i!=target_word_index['start']) and i!=target_word_index['end']):\n            newString=newString+reverse_target_word_index[i]+' '\n    return newString\n\ndef seq2text(input_seq):\n    newString=''\n    for i in input_seq:\n        if(i!=0):\n            newString=newString+reverse_source_word_index[i]+' '\n    return newString","d94ead7e":"for i in range(5):\n    print(\"abstract:\",seq2text(x_val[i]))\n    print(\"Original title:\",seq2summary(y_val[i]))\n    print(\"Predicted title:\",decode_sequence(x_val[i].reshape(1,max_len_abstract)))\n    print(\"\\n\")","32a03fef":"test_df\nx_test=test_df[\"abstract\"]\ny_test=test_df[\"title\"]\nx_test[:1]","c3cf355b":"x_test   =   x_tokenizer.texts_to_sequences(x_test) \nx_test   =   pad_sequences(x_test,  maxlen=max_len_abstract, padding='post')\n\ny_test   =   y_tokenizer.texts_to_sequences(y_test) \ny_test   =   pad_sequences(y_test,  maxlen=max_len_title, padding='post') \n","e30cab5e":"from nltk.translate.bleu_score import sentence_bleu\nfor i in range(10):\n    print(\"abstract:\",seq2text(x_test[i]))\n    original_title=seq2summary(y_test[i])\n    predicted_title=decode_sequence(x_test[i].reshape(1,max_len_abstract))\n    \n    print(\"Original title:\",original_title)\n    print(\"Predicted title:\",predicted_title)\n    \n    \n    print(\"Bleu Score Match:\",sentence_bleu([original_title.split()], predicted_title.split()))\n    print(\"\\n\")","a0b63b92":"x_test[:200]","40700354":"from nltk.translate.bleu_score import corpus_bleu\nlist_of_references=[]\nlist_of_hypotheses=[]\n\nfor i in range(1000):\n    original_title=seq2summary(y_test[i])\n    predicted_title=decode_sequence(x_test[i].reshape(1,max_len_abstract))\n    \n    reference = original_title.split()\n    hypothesis= predicted_title.split()\n    \n    list_of_references.append([references])\n    list_of_hypotheses.append(hypothesis)\n\nprint(\"First 1000 title corpus Score Match:\", corpus_bleu(list_of_references, list_of_hypotheses))\n","0f272b82":"## 9) Visualize training results","8c7271e7":"## 2) Drop Duplicates and NA values","5fde89a1":"## 7) Build the model\n","c019707c":"## 5) Preparing the Tokenizer, sequences & padding","987ce712":"## 11) Test Data and BLUE Score","f7fcb7aa":"## 12) BLUE Score","771b222d":"## 4) Siplt Data ","84b84d64":"## 10) Inference","67f32f85":"## 1) Read Data Set","75f1f411":"## 5) Upload attention Layer","33bd7af3":"## 6) Load libraries","1d599677":"## 8) Train the model","dcd29e42":"## 3) Text Cleaning"}}