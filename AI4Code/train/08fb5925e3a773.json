{"cell_type":{"b99d1e36":"code","111661fd":"code","d22c0a7a":"code","162c4d71":"code","d38386d4":"code","bcfe964a":"code","49ca7fb9":"code","4921694d":"code","973882e7":"code","78f2d537":"markdown","78f907b2":"markdown"},"source":{"b99d1e36":"cd  ..\/input\/tensorflow-rnn-shakespeare-master\/tensorflow-rnn-shakespeare-master\/\n","111661fd":"import tensorflow as tf\nfrom tensorflow.contrib import layers\nfrom tensorflow.contrib import rnn  \nimport os\nimport time\nimport math\nimport numpy as np\nimport my_txtutils as txt\ntf.set_random_seed(0)","d22c0a7a":"SEQLEN = 30\nBATCHSIZE = 200\nALPHASIZE = txt.ALPHASIZE\nINTERNALSIZE = 512\nNLAYERS = 3\nlearning_rate = 0.001  \ndropout_pkeep = 0.8    \n\n# load data\nshakedir = \"shakespeare\/*.txt\"\ncodetext, valitext, bookranges = txt.read_data_files(shakedir, validation=True)\n\n# display some stats on the data\nepoch_size = len(codetext) \/\/ (BATCHSIZE * SEQLEN)\ntxt.print_data_stats(len(codetext), len(valitext), epoch_size)\n\n\nlr = tf.placeholder(tf.float32, name='lr')  \npkeep = tf.placeholder(tf.float32, name='pkeep')  \nbatchsize = tf.placeholder(tf.int32, name='batchsize')\n\n# inputs\nX = tf.placeholder(tf.uint8, [None, None], name='X')    \nXo = tf.one_hot(X, ALPHASIZE, 1.0, 0.0)                 \n# expected outputs = same sequence shifted by 1 since we are trying to predict the next character\nY_ = tf.placeholder(tf.uint8, [None, None], name='Y_')  \nYo_ = tf.one_hot(Y_, ALPHASIZE, 1.0, 0.0)               \n# input state\nHin = tf.placeholder(tf.float32, [None, INTERNALSIZE*NLAYERS], name='Hin')  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n\n# using a NLAYERS=3 layers of GRU cells, unrolled SEQLEN=30 times\n# dynamic_rnn infers SEQLEN from the size of the inputs Xo\n\n\ncells = [rnn.GRUCell(INTERNALSIZE) for _ in range(NLAYERS)]\n\ndropcells = [rnn.DropoutWrapper(cell,input_keep_prob=pkeep) for cell in cells]\nmulticell = rnn.MultiRNNCell(dropcells, state_is_tuple=False)\nmulticell = rnn.DropoutWrapper(multicell, output_keep_prob=pkeep)  # dropout for the softmax layer\n\nYr, H = tf.nn.dynamic_rnn(multicell, Xo, dtype=tf.float32, initial_state=Hin)\n\n\nH = tf.identity(H, name='H')  # just to give it a name\n\n","162c4d71":"\nYflat = tf.reshape(Yr, [-1, INTERNALSIZE])    # [ BATCHSIZE x SEQLEN, INTERNALSIZE ]\nYlogits = layers.linear(Yflat, ALPHASIZE)     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\nYflat_ = tf.reshape(Yo_, [-1, ALPHASIZE])     # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\nloss = tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits, labels=Yflat_)  # [ BATCHSIZE x SEQLEN ]\nloss = tf.reshape(loss, [batchsize, -1])      # [ BATCHSIZE, SEQLEN ]\nYo = tf.nn.softmax(Ylogits, name='Yo')        # [ BATCHSIZE x SEQLEN, ALPHASIZE ]\nY = tf.argmax(Yo, 1)                          # [ BATCHSIZE x SEQLEN ]\nY = tf.reshape(Y, [batchsize, -1], name=\"Y\")  # [ BATCHSIZE, SEQLEN ]\ntrain_step = tf.train.AdamOptimizer(lr).minimize(loss)\n\n# stats for display\nseqloss = tf.reduce_mean(loss, 1)\nbatchloss = tf.reduce_mean(seqloss)\naccuracy = tf.reduce_mean(tf.cast(tf.equal(Y_, tf.cast(Y, tf.uint8)), tf.float32))\nloss_summary = tf.summary.scalar(\"batch_loss\", batchloss)\nacc_summary = tf.summary.scalar(\"batch_accuracy\", accuracy)\nsummaries = tf.summary.merge([loss_summary, acc_summary])","d38386d4":"# Init Tensorboard stuff. This will save Tensorboard information into a different\n# folder at each run named 'log\/<timestamp>\/'. Two sets of data are saved so that\n# you can compare training and validation curves visually in Tensorboard.\ntimestamp = str(math.trunc(time.time()))\nsummary_writer = tf.summary.FileWriter(\"\/kaggle\/working\/log\/\" + timestamp + \"-training\")\nvalidation_writer = tf.summary.FileWriter(\"\/kaggle\/working\/log\/\" + timestamp + \"-validation\")\n\n# Init for saving models. They will be saved into a directory named 'checkpoints'.\n# Only the last checkpoint is kept.\nif not os.path.exists(\"\/kaggle\/working\/checkpoints\"):\n    os.mkdir(\"\/kaggle\/working\/checkpoints\")\nsaver = tf.train.Saver(max_to_keep=1000)\n\n# for display: init the progress bar\nDISPLAY_FREQ = 50\n_50_BATCHES = DISPLAY_FREQ * BATCHSIZE * SEQLEN\nprogress = txt.Progress(DISPLAY_FREQ, size=111+2, msg=\"Training on next \"+str(DISPLAY_FREQ)+\" batches\")\n\n# init\nistate = np.zeros([BATCHSIZE, INTERNALSIZE*NLAYERS])  # initial zero input state\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\nstep = 0","bcfe964a":"# training loop\nfor x, y_, epoch in txt.rnn_minibatch_sequencer(codetext, BATCHSIZE, SEQLEN, nb_epochs=10):\n\n    # train on one minibatch\n    feed_dict = {X: x, Y_: y_, Hin: istate, lr: learning_rate, pkeep: dropout_pkeep, batchsize: BATCHSIZE}\n    _, y, ostate = sess.run([train_step, Y, H], feed_dict=feed_dict)\n\n    # log training data for Tensorboard display a mini-batch of sequences (every 50 batches)\n    if step % _50_BATCHES == 0:\n        feed_dict = {X: x, Y_: y_, Hin: istate, pkeep: 1.0, batchsize: BATCHSIZE}  # no dropout for validation\n        y, l, bl, acc, smm = sess.run([Y, seqloss, batchloss, accuracy, summaries], feed_dict=feed_dict)\n        txt.print_learning_learned_comparison(x, y, l, bookranges, bl, acc, epoch_size, step, epoch)\n        summary_writer.add_summary(smm, step)\n\n    \n    if step % _50_BATCHES == 0 and len(valitext) > 0:\n        VALI_SEQLEN = 1*1024  \n        bsize = len(valitext) \/\/ VALI_SEQLEN\n        txt.print_validation_header(len(codetext), bookranges)\n        vali_x, vali_y, _ = next(txt.rnn_minibatch_sequencer(valitext, bsize, VALI_SEQLEN, 1))  \n        vali_nullstate = np.zeros([bsize, INTERNALSIZE*NLAYERS])\n        feed_dict = {X: vali_x, Y_: vali_y, Hin: vali_nullstate, pkeep: 1.0,  \n                     batchsize: bsize}\n        ls, acc, smm = sess.run([batchloss, accuracy, summaries], feed_dict=feed_dict)\n        txt.print_validation_stats(ls, acc)\n        # save validation data for Tensorboard\n        validation_writer.add_summary(smm, step)\n\n    # display a short text generated with the current weights and biases (every 150 batches)\n    if step \/\/ 3 % _50_BATCHES == 0:\n        txt.print_text_generation_header()\n        ry = np.array([[txt.convert_from_alphabet(ord(\"K\"))]])\n        rh = np.zeros([1, INTERNALSIZE * NLAYERS])\n        for k in range(1000):\n            ryo, rh = sess.run([Yo, H], feed_dict={X: ry, pkeep: 1.0, Hin: rh, batchsize: 1})\n            rc = txt.sample_from_probabilities(ryo, topn=10 if epoch <= 1 else 2)\n            print(chr(txt.convert_to_alphabet(rc)), end=\"\")\n            ry = np.array([[rc]])\n        txt.print_text_generation_footer()\n\n    # save a checkpoint (every 500 batches)\n    if step \/\/ 10 % _50_BATCHES == 0:\n        saved_file = saver.save(sess, '\/kaggle\/working\/checkpoints\/rnn_train_' + timestamp, global_step=step)\n        print(\"Saved file: \" + saved_file)\n\n    # display progress bar\n    progress.step(reset=step % _50_BATCHES == 0)\n\n    # loop state around\n    istate = ostate\n    step += BATCHSIZE * SEQLEN\n\n","49ca7fb9":"import tensorflow as tf\nimport numpy as np\nimport my_txtutils\n\nALPHASIZE = my_txtutils.ALPHASIZE\nNLAYERS = 3\nINTERNALSIZE = 512","4921694d":"!ls \/kaggle\/working\/checkpoints\/","973882e7":"\n#shakespeareB10 = \"checkpoints\/rnn_train_1495440473-102000000\"\nshakespeareB10=\"\/kaggle\/working\/checkpoints\/rnn_train_1565257844-9000000\"\n\nauthor = shakespeareB10\n\nncnt = 0\nwith tf.Session() as sess:\n    new_saver = tf.train.import_meta_graph(\"\/kaggle\/working\/checkpoints\/rnn_train_1565257844-0.meta\")\n    new_saver.restore(sess, author)\n    x = my_txtutils.convert_from_alphabet(ord(\"L\"))\n    x = np.array([[x]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\n\n    # initial values\n    y = x\n    h = np.zeros([1, INTERNALSIZE * NLAYERS], dtype=np.float32)  # [ BATCHSIZE, INTERNALSIZE * NLAYERS]\n    for i in range(1000000000):\n        yo, h = sess.run(['Yo:0', 'H:0'], feed_dict={'X:0': y, 'pkeep:0': 1., 'Hin:0': h, 'batchsize:0': 1})\n        c = my_txtutils.sample_from_probabilities(yo, topn=2)\n        y = np.array([[c]])  # shape [BATCHSIZE, SEQLEN] with BATCHSIZE=1 and SEQLEN=1\n        c = chr(my_txtutils.convert_to_alphabet(c))\n        print(c, end=\"\")\n\n        if c == '\\n':\n            ncnt = 0\n        else:\n            ncnt += 1\n        if ncnt == 100:\n            print(\"\")\n            ncnt = 0","78f2d537":"From the above list of files, choose the appropriate path for shakespeareB10 and the respective \".meta\" file","78f907b2":"To see the sample output, run the following code:"}}