{"cell_type":{"3fbce2c2":"code","80026cd9":"code","2823dd9d":"code","9aa1f0b6":"code","619d654a":"code","af6cbf50":"code","9e704d2f":"code","1a30749d":"code","4ea5aa04":"code","d403ad77":"code","369e57c8":"code","3d64a52d":"code","6a84a4d6":"code","6fb94e32":"code","b79df61f":"code","1af11213":"code","956d5852":"markdown","3b5ba2fd":"markdown"},"source":{"3fbce2c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80026cd9":"import collections\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\n\nsubmission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain_len = len(train)\n\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest_len = len(test)\n\n\ndata = pd.concat([train,test], sort = False)\nSex = collections.Counter(data[\"Sex\"])\nEmbarked = collections.Counter(data[\"Embarked\"])\nCabin = collections.Counter(data[\"Cabin\"])\nprint(Embarked)\nprint(Cabin)\n\n#\u524d\u51e6\u7406\u306f\u6b20\u640d\u5024\u306e\u51e6\u7406\u3092\u3057\u3066\u3044\u304f\n#Cabin\u5217\u306f\u524a\u9664\n#Embarked\u306e\u6b20\u640d\u5024\u5217\u306fS\u3067\u57cb\u3081\u308b\ndata[\"Sex\"].replace([\"male\",\"female\"], [0, 1], inplace = True)\ndata[\"Fare\"].fillna(np.mean(data[\"Fare\"]), inplace = True)\ndata = data[[\"Survived\",\"Pclass\",\"Sex\", \"SibSp\", \"Parch\", \"Embarked\",\"Fare\"]]\ndata[\"Embarked\"].fillna(\"S\", inplace = True) #S\u304c\u591a\u3044\u306e\u3067\u53d6\u308a\u6562\u3048\u305aS\u3067\u57cb\u3081\u308b\ndata[\"Embarked\"].replace([\"S\",\"C\", \"Q\"], [1,2,3], inplace=True)","2823dd9d":"Scaler1 = StandardScaler()\n\ndata_columns = data.columns\ndata = pd.DataFrame(Scaler1.fit_transform(data))\ndata.columns = data_columns\n\nX = data.iloc[:train_len]\ny = X[\"Survived\"]\nX = X.drop(\"Survived\", axis = 1)\ntest = data.iloc[train_len:]\ntest.drop(\"Survived\", axis = 1, inplace = True)","9aa1f0b6":"from torch import nn, optim\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom statistics import mean\nfrom torch.autograd import Variable\n\n\n\n\nX_train, X_test, y_train, y_test =train_test_split(X, y, random_state = 10)\n\nnet = nn.Sequential(\n    nn.Linear(6,20),\n    nn.ReLU(),\n    nn.Dropout(0.1),\n    nn.Linear(20,20),\n    nn.ReLU(),\n    nn.Dropout(0.1),\n    nn.Linear(20,2)\n)\n\nloss_fn = nn.CrossEntropyLoss()\n\noptimizer = optim.Adam(net.parameters())\n#\u640d\u5931\u95a2\u6570\u306e\u30ed\u30b0\ntrain_losses = []\ntest_losses = []\ncorrect_rate = []\nnum_right = []\n\nX_train = torch.tensor(X_train.values, dtype = torch.float32)\ny_train = torch.tensor(y_train.values, dtype = torch.float32)\nX_test = torch.tensor(X_test.values, dtype = torch.float32)\ny_test = torch.tensor(y_test.values, dtype = torch.float32)\n\n\n#\u30df\u30cb\u30d0\u30c3\u30c1\u7528\nds = TensorDataset(X_train, y_train)\nloader = DataLoader(ds, batch_size = 64, shuffle=True)","619d654a":"\nfor epoc in range(200):\n    running_loss = 0.0\n    \n    #train\u30e2\u30fc\u30c9\n    net.train()\n    for i,(xx, yy) in enumerate(loader):\n        optimizer.zero_grad()\n        y_pred = net(X_train)\n        \n        y_train = y_train.long()\n        \n        loss = loss_fn(y_pred,y_train)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n    train_losses.append(running_loss \/ i)\n    \n    #\u8a55\u4fa1\u30e2\u30fc\u30c9\n    net.eval()\n    h = net(X_test)\n    y_test = y_test.long()\n    \n    loss = loss_fn(h,y_test)\n    loss.backward()\n    optimizer.step()\n    test_losses.append(loss.item())\n    \n    values, labels = torch.max(h, 1)\n    num_right.append(np.sum(labels.data.numpy() == y_test.numpy()))\n    \n    # \u4e88\u6e2c\u7d50\u679c\u306e\u78ba\u8a8d (y\u306fFloatTensor\u306a\u306e\u3067ByteTensor\n    # \u306b\u5909\u63db\u3057\u3066\u304b\u3089\u6bd4\u8f03\u3059\u308b\uff09\nprint(mean(num_right)\/len(y_test))","af6cbf50":"plt.subplot()\nplt.plot(train_losses)\nplt.plot(test_losses, c=\"r\")\nplt.legend()","9e704d2f":"test = torch.tensor(test.values, dtype = torch.float32)\n\ntest_var = Variable(torch.FloatTensor(test), requires_grad=False) \nwith torch.no_grad():\n    test_result = net(test_var)\nvalues, labels = torch.max(test_result, 1)\nsurvived = labels.data.numpy()","1a30749d":"submission = pd.DataFrame({'PassengerId': submission['PassengerId'], 'Survived': survived})\nsubmission.to_csv('submission.csv', index=False)","4ea5aa04":"import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable","d403ad77":"class CustomLinear(nn.Module):\n    def __init__(self, in_features, out_features, bias = True, p = 0.1):\n        super().__init__()\n        self.linear = nn.Linear(in_features, out_features, bias)\n        self.relu = nn.ReLU()\n        self.drop = nn.Dropout(p)\n        \n    def forward(self, x):\n        x = self.linear(x)\n        x = self.relu(x)\n        x = self.drop(x)\n        return x\n\n# Part1\n#mlp = nn.Sequential(\n    #CustomLinear(6,200),\n    #CustomLinear(200,200),\n    #CustomLinear(200,200),\n    #nn.Linear(200, 1))","369e57c8":"#Part2\nclass MyMLP(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n        self.ln1 = CustomLinear(in_features, 20)\n        self.ln2 = CustomLinear(20, 20)\n        self.ln3 = CustomLinear(20, 20)\n        self.ln4 = nn.Linear(20, out_features)\n        \n    def forward(self, x):\n        x = self.ln1(x)\n        x = self.ln2(x)\n        x = self.ln3(x)\n        x = self.ln4(x)\n        return x\n\nmlp = MyMLP(X_train.shape[1],2)","3d64a52d":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(mlp.parameters(), lr = 0.01)","6a84a4d6":"batch_size = 64\nn_epochs = 405\nbatch_no = len(X_train) \/\/ batch_size\n\ntrain_loss = 0\ntrain_loss_min = np.Inf\n\nfor epoch in range(n_epochs):\n    mlp.train()\n    for i in range(batch_no):\n        start = i*batch_size\n        end = start+batch_size\n        x_var = Variable(torch.FloatTensor(X_train[start:end]))\n        y_var = Variable(torch.LongTensor(y_train[start:end])) \n        \n        optimizer.zero_grad()\n        output = mlp(x_var)\n        loss = criterion(output,y_var)\n        loss.backward()\n        optimizer.step()\n        \n        values, labels = torch.max(output, 1)\n        num_right = np.sum(labels.data.numpy() == y_train[start:end])\n        train_loss += loss.item()*batch_size\n    \n    train_loss = train_loss \/ len(X_train)\n    if train_loss <= train_loss_min:\n        print(\"Validation loss decreased ({:6f} ===> {:6f}). Saving the model...\".format(train_loss_min,train_loss))\n        torch.save(mlp.state_dict(), \"model.pt\")\n        train_loss_min = train_loss\n    \n    if epoch % 200 == 0:\n        print('')\n        print(\"Epoch: {} \\tTrain Loss: {} \\tTrain Accuracy: {}\".format(epoch+1, train_loss,num_right \/ len(y_train[start:end]) ))\n","6fb94e32":"plt.plot(train_losses)","b79df61f":"X_test_var = Variable(torch.FloatTensor(test), requires_grad=False) \nwith torch.no_grad():\n    test_result = mlp(X_test_var)\nvalues, labels = torch.max(test_result, 1)\nsurvived = labels.data.numpy()","1af11213":"submission = pd.DataFrame({'PassengerId': submission['PassengerId'], 'Survived': survived})\nsubmission.to_csv('submission_2.csv', index=False)","956d5852":"# \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u30e2\u30b8\u30e5\u30fc\u30eb\u5316","3b5ba2fd":"# \u5358\u7d14\u306aDNN\n### ReLU, CrossEntropy, dropout, \u30df\u30cb\u30d0\u30c3\u30c1"}}