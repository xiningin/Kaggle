{"cell_type":{"3c3661a7":"code","477f5270":"code","74e81791":"code","8f3cb789":"code","3ec9c2ca":"code","eb1b42a4":"code","d4a1568c":"code","34b11200":"code","386eafff":"code","28dbad4c":"code","e3873ff0":"markdown","cf1de209":"markdown","b139796d":"markdown","3eb6f884":"markdown","d5939425":"markdown"},"source":{"3c3661a7":"import gc\nimport os\nimport librosa\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom pathlib import Path\nfrom librosa import display\nimport matplotlib.pyplot as plt\nfrom scipy.io.wavfile import read\nfrom IPython.display import HTML, Audio, display_html\nfrom IPython.display import display as display_ipython\n\npd.set_option('display.max_colwidth', 500)","477f5270":"label_dict = {\n    'speech': 0,\n    'music': 1,\n    'noise': 2    \n}","74e81791":"data_folder = '..\/..\/kaggle\/input\/silero-audio-classifier'","8f3cb789":"def read_audio(path):\n    sr, wav = read(path)\n    assert sr == 16000\n    assert len(wav) == 16000 * 3\n    assert len(wav.shape) == 1\n    return wav\n\n\ndef read_audio_norm(path):\n    wav = read_audio(path)\n    abs_max = np.abs(wav).max()\n    wav = wav.astype('float32')\n    if abs_max > 0:\n        wav *= 1 \/ abs_max\n    return wav\n\n\ndef audio_player(audio_path):\n    return f'<audio preload=\"none\" controls=\"controls\"><source src=\"{audio_path}\" type=\"audio\/wav\"><\/audio>'\n\n\ndef display_manifest(df):\n    display_df = df\n    display_df['wav'] = [audio_player(path) for path in display_df.wav_path]\n    audio_style = '<style>audio {height:44px;border:0;padding:0 20px 0px;margin:-10px -20px -20px;}<\/style>'\n    display_df = display_df[['wav', 'label']]\n    display_ipython(HTML(audio_style + display_df.to_html(escape=False)))\n    del display_df\n    gc.collect()","3ec9c2ca":"train_path = Path(data_folder) \/ 'train'\ntrain_df = pd.read_csv(Path(data_folder) \/ 'train.csv')\ntrain_df['target'] = train_df['label'].apply(lambda x: label_dict.get(x, -1))\n\nassert set([os.path.relpath(path, data_folder + '\/train') \n            for path \n            in glob(f'{train_path}\/train\/*\/*.wav')]) == set(train_df.wav_path.values)\n\ntrain_df['wav_path'] = train_df['wav_path'].apply(lambda x: str(Path(data_folder) \/ Path('train') \/ x))","eb1b42a4":"train_df.label.value_counts().values\n","d4a1568c":"wav = read_audio(train_df.sample(n=1).iloc[0].wav_path)\nAudio(wav, rate=16000, autoplay=True)","34b11200":"window_size = 0.02\nwindow_stride = 0.01\nsample_rate = 16000\n\nn_fft = int(sample_rate * (window_size + 1e-8))\nwin_length = n_fft\nhop_length = int(sample_rate * (window_stride + 1e-8))\n\nkwargs = {\n    'n_fft': n_fft,\n    'hop_length': hop_length,\n    'win_length': n_fft\n}\n\ndef stft(wav):\n    D = librosa.stft(wav,\n                     **kwargs)\n    mag, phase = librosa.magphase(D)    \n    return mag","386eafff":"def visualize_spect(spects,\n                    labels):\n    assert len(spects) == 3\n    assert len(labels) == 3\n    \n    plt.figure(figsize=(20, 10))\n\n    plt.subplot(3, 3, 1)\n    librosa.display.specshow(spects[0], y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(f'Log-frequency power spectrogram, {labels[0]}')\n\n    plt.subplot(3, 3, 2)\n    librosa.display.specshow(spects[1], y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(f'Log-frequency power spectrogram, {labels[1]}')\n\n    plt.subplot(3, 3, 3)\n    librosa.display.specshow(spects[2], y_axis='log')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(f'Log-frequency power spectrogram, {labels[2]}')\n\n    plt.tight_layout()\n    plt.show()","28dbad4c":"spects = []\nlabels = []\n\n_ = ['speech', 'music', 'noise']\n\nfor i in range(0, 3):\n    s = train_df[train_df.label == _[i]].sample(n=1).iloc[0]\n    spects.append(stft(read_audio_norm(s.wav_path)))\n    labels.append(s.label)\n    \nvisualize_spect(spects,\n                labels)    ","e3873ff0":"## Random visualization","cf1de209":"## Read and validate the data","b139796d":"## Read a random file","3eb6f884":"## Utils and imports","d5939425":"# EDA"}}