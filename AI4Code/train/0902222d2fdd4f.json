{"cell_type":{"9afa3807":"code","a19216b9":"code","6d02c414":"code","75aedbff":"code","c5f860a5":"code","abb7b08e":"code","914e5380":"code","8e48f44b":"code","b0a02319":"code","38b861df":"code","7d1a92f4":"code","72bcc7ff":"code","b1041d91":"code","796e07df":"code","ab4d50d3":"code","04cc31c1":"code","a5cc80e0":"code","92105010":"code","926156ee":"code","5565fb39":"code","3e4df5ee":"code","ddf90fd0":"code","8afd6703":"code","4c8d8f3a":"code","b05c3550":"code","77fc0f65":"code","0cfa38fd":"code","900a6af0":"code","af6fd7c8":"code","cdff4ebf":"code","7e959b33":"code","ea179a2c":"code","87ea40f0":"code","e29624a2":"code","90451460":"code","739ad481":"code","3ca4d335":"code","db6e47d4":"code","1d265497":"code","70b79e80":"code","595c35c1":"code","17af8c16":"code","1116c104":"code","98fe6bb9":"code","ca886147":"code","06f54298":"markdown","2c5fe256":"markdown","e3af9d2c":"markdown","a72e3c29":"markdown","3abb4bd0":"markdown","7dd17649":"markdown","4984e829":"markdown","85d8bf95":"markdown","1874a3f5":"markdown","54457e3f":"markdown","cdc5623d":"markdown","47e3ba6d":"markdown","c308be82":"markdown","54aca3e3":"markdown","facc72b2":"markdown","a8529586":"markdown","c6ae8d97":"markdown","7ed57b34":"markdown","7fde3f90":"markdown","9608c1ed":"markdown","74f6a315":"markdown","60881486":"markdown","a124f65b":"markdown","9fd6c7a8":"markdown","1ba8606d":"markdown","0d87f11c":"markdown","a2987355":"markdown","162d1fba":"markdown"},"source":{"9afa3807":"#basic fundamental libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n#Stratified sampling, SMOTE for oversampling technique, make_pipeline for pipeline creation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n\n#color text + Statistics libraires\nfrom scipy.stats import chi2_contingency\nimport colorama\nfrom colorama import Fore\nimport scipy.stats as stats\nimport statsmodels.api as sma\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n#best parameter finding libraries\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV \n\n#metrics\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, f1_score, roc_auc_score, accuracy_score, classification_report\n\nimport warnings\nwarnings.filterwarnings('ignore')","a19216b9":"data = pd.read_csv('..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","6d02c414":"data.head()","75aedbff":"#deleting these rows as total charges have space in between which is leading to the string category.\ndata = data[data['TotalCharges'] != ' ']\n\n#Multiple lines, Online Security, Online Backup, Device Protection, Tech support, Streaming Service, Streaming Movies, \ndata['MultipleLines'] = data['MultipleLines'].replace(\"No phone service\", \"No\")\ndata['OnlineSecurity'] = data['OnlineSecurity'].replace(\"No internet service\", \"No\")\ndata['OnlineBackup'] = data['OnlineBackup'].replace(\"No internet service\", \"No\")\ndata['DeviceProtection'] = data['DeviceProtection'].replace(\"No internet service\", \"No\")\ndata['TechSupport'] = data['TechSupport'].replace(\"No internet service\", \"No\")\ndata['StreamingTV'] = data['StreamingTV'].replace(\"No internet service\", \"No\")\ndata['StreamingMovies'] = data['StreamingMovies'].replace(\"No internet service\", \"No\")\n\n#converting senior citizen into category\ndata['SeniorCitizen'] = data['SeniorCitizen'].replace(1,\"Yes\")\ndata['SeniorCitizen'] = data['SeniorCitizen'].replace(0,\"No\")\ndata['TotalCharges'] = data['TotalCharges'].astype(float)\ndata.drop('customerID', axis = 1, inplace= True)","c5f860a5":"data.info()","abb7b08e":"cat_cols = [col for col in data.columns if data[col].dtype==\"object\"]\ncat_cols","914e5380":"def chisq(df,name):\n    stat, p, df, arr = chi2_contingency(pd.crosstab(df, data.Churn))\n    if p < 0.05:\n     print(Fore.GREEN +'We can reject the Null Hypothesis for {} category {}'.format(name, p))\n    else:\n     print(Fore.RED +'We fail to reject the Null Hypothesis for {} category {}'.format(name, p))","8e48f44b":"for i in cat_cols:\n    chisq(data[i],i)","b0a02319":"data.drop(['gender', 'PhoneService'], axis = 1, inplace = True)","38b861df":"temp = data\ntemp[\"Churn\"] =temp[\"Churn\"].replace(\"Yes\", 1)\ntemp[\"Churn\"] =temp[\"Churn\"].replace(\"No\", 0)\nfig = px.parallel_categories(temp, dimensions=['SeniorCitizen', 'Partner', 'Dependents',\n                                               'Contract', 'PaperlessBilling', 'PaymentMethod'],\n                color=\"Churn\", color_continuous_scale=px.colors.sequential.Inferno,\n                labels={'SeniorCitizen':'Senior Citizen', 'Partner':'Partner',\n                        'Dependents':'Dependents', 'Contract':'Contract',\n                        'PaperlessBilling':'Paperless Billing', 'PaymentMethod':'PaymentMethod'})\nfig.show()","7d1a92f4":"num_cols = [col for col in data.columns if data[col].dtype==\"int64\" or data[col].dtype==\"float64\"]\nnum_cols","72bcc7ff":"data.hist(figsize = (20,10), layout = (2,4))","b1041d91":"data.skew().sort_values(ascending = False)","796e07df":"#did transformation on Total Charges column\ndata['TotalCharges'] = np.sqrt(data['TotalCharges'])\ndata.skew().sort_values(ascending = False)","ab4d50d3":"data.hist(figsize = (20,10), layout = (2,4))","04cc31c1":"corr = data.corr()\nplt.figure(figsize = (10,10))\nax = sns.heatmap(corr, vmin = -1, center = 0, annot = True, cmap = 'mako')","a5cc80e0":"#tenure\ntenure_data = data['tenure'].values\nq25, q75 = np.percentile(tenure_data, 25), np.percentile(tenure_data, 75) #q1 and a3 assigned\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\ntenure_iqr = q75 - q25\nprint('iqr: {}'.format(tenure_iqr))\n\ntenure_cut_off = tenure_iqr * 1.5\ntenure_lower, tenure_upper = q25 - tenure_cut_off, q75 + tenure_cut_off\nprint('Cut Off: {}'.format(tenure_cut_off))\nprint('tenure Lower Band: {}'.format(tenure_lower))\nprint('tenure Upper band: {}'.format(tenure_upper))\n\ntenure_outliers = [x for x in tenure_data if x < tenure_lower or x > tenure_upper]\nprint('Feature tenure Outliers Total: {}'.format(len(tenure_outliers)))\nprint('tenure outliers:{}'.format(tenure_outliers))\n\ndata = data.drop(data[(data['tenure'] > tenure_upper) | (data['tenure'] < tenure_lower)].index)","92105010":"#Outlier Analysis\n#MonthlyCharges\nMonthlyCharges_data = data['MonthlyCharges'].values\nq25, q75 = np.percentile(MonthlyCharges_data, 25), np.percentile(MonthlyCharges_data, 75) #q1 and a3 assigned\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nMonthlyCharges_iqr = q75 - q25\nprint('iqr: {}'.format(MonthlyCharges_iqr))\n\nMonthlyCharges_cut_off = MonthlyCharges_iqr * 1.5\nMonthlyCharges_lower, MonthlyCharges_upper = q25 - MonthlyCharges_cut_off, q75 + MonthlyCharges_cut_off\nprint('Cut Off: {}'.format(MonthlyCharges_cut_off))\nprint('MonthlyCharges Lower Band: {}'.format(MonthlyCharges_lower))\nprint('MonthlyCharges Upper band: {}'.format(MonthlyCharges_upper))\n\nMonthlyCharges_outliers = [x for x in MonthlyCharges_data if x < MonthlyCharges_lower or x > MonthlyCharges_upper]\nprint('Feature MonthlyCharges Outliers Total: {}'.format(len(MonthlyCharges_outliers)))\nprint('MonthlyCharges outliers:{}'.format(MonthlyCharges_outliers))\n\ndata = data.drop(data[(data['MonthlyCharges'] > MonthlyCharges_upper) | (data['MonthlyCharges'] < MonthlyCharges_lower)].index)","926156ee":"#Outlier Analysis\n#TotalCharges\nTotalCharges_data = data['TotalCharges'].values\nq25, q75 = np.percentile(TotalCharges_data, 25), np.percentile(TotalCharges_data, 75) #q1 and a3 assigned\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nTotalCharges_iqr = q75 - q25\nprint('iqr: {}'.format(TotalCharges_iqr))\n\nTotalCharges_cut_off = TotalCharges_iqr * 1.5\nTotalCharges_lower, TotalCharges_upper = q25 - TotalCharges_cut_off, q75 + TotalCharges_cut_off\nprint('Cut Off: {}'.format(TotalCharges_cut_off))\nprint('TotalCharges Lower Band: {}'.format(TotalCharges_lower))\nprint('TotalCharges Upper band: {}'.format(TotalCharges_upper))\n\nTotalCharges_outliers = [x for x in TotalCharges_data if x < TotalCharges_lower or x > TotalCharges_upper]\nprint('Feature TotalCharges Outliers Total: {}'.format(len(TotalCharges_outliers)))\nprint('TotalCharges outliers:{}'.format(TotalCharges_outliers))\n\ndata = data.drop(data[(data['TotalCharges'] > TotalCharges_upper) | (data['TotalCharges'] < TotalCharges_lower)].index)","5565fb39":"cat_cols = [col for col in data.columns if data[col].dtype==\"object\"]\ndata = pd.get_dummies(data, columns = cat_cols, prefix_sep='_', drop_first = True)","3e4df5ee":"#normalise numerical columns\nsc = StandardScaler()\nsc.fit(data[['tenure','MonthlyCharges','TotalCharges']])\nvalues_std = sc.transform(data[['tenure','MonthlyCharges','TotalCharges']])\nvalues_std = pd.DataFrame(values_std, columns=['tenure','MonthlyCharges','TotalCharges'])\ndata.drop(['tenure','MonthlyCharges','TotalCharges'], axis = 1, inplace = True)\ndata = pd.concat([data, values_std], axis=1)","ddf90fd0":"data.head()","8afd6703":"data.columns","4c8d8f3a":"sns.countplot('Churn', data=data)\nplt.title('Class Distributions \\n (0: Ruke hue employees || 1: Chale gye employees)', fontsize=14)","b05c3550":"print('No Churn: 0', round(data['Churn'].value_counts()[0]\/len(data) * 100,2), '% of the dataset')\nprint('Churn: 1', round(data['Churn'].value_counts()[1]\/len(data) * 100,2), '% of the dataset')\n\ndata.dropna(inplace = True)\nX = data.drop('Churn', axis=1)\ny = data['Churn']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]","77fc0f65":"original_ytrain.value_counts() #perfectly distributed 73:27 ratio","0cfa38fd":"original_ytest.value_counts()","900a6af0":"#Moving on to model building\ndata = data.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nchurn_df = data.loc[data['Churn'] == 1]\nnon_churn_df = data.loc[data['Churn'] == 0][:1866]\n\nnormal_distributed_df = pd.concat([churn_df, non_churn_df])\n\n# Shuffle dataframe rows\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\nnew_df['Churn'] = new_df['Churn'].astype(int)\nnew_df.head()","af6fd7c8":"corr = new_df.corr()\nplt.figure(figsize = (10,10))\nax = sns.heatmap(corr, vmin = -1, center = 0, cmap = 'mako')","cdff4ebf":"X = new_df.drop('Churn', axis=1)\ny = new_df['Churn']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","7e959b33":"classifiers = {\"DecisionTreeClassifier\": DecisionTreeClassifier()}","ea179a2c":"for key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","87ea40f0":"tree_params = {\"criterion\": [\"gini\", \"entropy\"], \"splitter\": [\"best\", \"random\"],\n               \"max_depth\": list(range(2,8,1)), \"min_samples_leaf\": list(range(5,10,1)),\n               \"max_features\": ['auto', 'sqrt', 'log2']}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\n\n# tree best estimator\ntree_clf = grid_tree.best_estimator_\nprint('Best Estimators: ', tree_clf)","e29624a2":"tree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')","90451460":"#Model creation\ntree = DecisionTreeClassifier(criterion='entropy', max_depth=6, max_features='auto',\n                       min_samples_leaf=5)\ntree.fit(X_train, y_train)","739ad481":"y_pred_x = tree.predict(X_test)\nprint(Fore.RED + \"Prone To Overfitting Results:\\n\")\nprint(Fore.RED + classification_report(y_test, y_pred_x))","3ca4d335":"y_pred = tree.predict(original_Xtest)\nprint(Fore.GREEN + \"Accurate Results using Undersampling Technique:\\n\")\nprint(Fore.GREEN + classification_report(original_ytest, y_pred))","db6e47d4":"print('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))","1d265497":"tree_over = DecisionTreeClassifier()\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"splitter\": [\"best\", \"random\"],\n               \"max_depth\": list(range(2,8,1)), \"min_samples_leaf\": list(range(5,10,1)),\n               \"max_features\": ['auto', 'sqrt', 'log2']}\nrandom_grid_tree = RandomizedSearchCV(DecisionTreeClassifier(), tree_params, n_iter=4)\n\nfor train_index, test_index in sss.split(original_Xtrain, original_ytrain):\n    original_Xtrain1, original_Xtest1 = original_Xtrain.iloc[train_index], original_Xtrain.iloc[test_index]\n    original_ytrain1, original_ytest1 = original_ytrain.iloc[train_index], original_ytrain.iloc[test_index]","70b79e80":"pipeline_creation = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), random_grid_tree)\nmodel = pipeline_creation.fit(original_Xtrain1, original_ytrain1)\nrandom_grid_tree.best_estimator_","595c35c1":"best_est = random_grid_tree.best_estimator_\nprediction = best_est.predict(original_Xtest1)\nprint(Fore.GREEN + \"Accurate Results using Oversaampling using SMOTE Technique:\\n\")\nprint(Fore.GREEN + classification_report(original_ytest1, prediction))","17af8c16":"path = best_est.cost_complexity_pruning_path(original_Xtrain1, original_ytrain1)\nccp_alphas, impurities = path.ccp_alphas, path.impurities","1116c104":"fig, ax = plt.subplots()\nax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\nax.set_xlabel(\"effective alpha\")\nax.set_ylabel(\"total impurity of leaves\")\nax.set_title(\"Total Impurity vs effective alpha for training set\")","98fe6bb9":"clfs = []\nfor ccp_alpha in ccp_alphas:\n    clf = DecisionTreeClassifier(max_depth=7, max_features='log2', min_samples_leaf=7, ccp_alpha = ccp_alpha)\n    clf.fit(original_Xtrain1, original_ytrain1)\n    clfs.append(clf)\nprint(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(clfs[-1].tree_.node_count, ccp_alphas[-1]))","ca886147":"train_scores = [clf.score(original_Xtrain1, original_ytrain1) for clf in clfs]\ntest_scores = [clf.score(original_Xtest1, original_ytest1) for clf in clfs]\n\nfig, ax = plt.subplots(figsize = (15,10))\n\nax.set_xlabel(\"alpha\")\nax.set_ylabel(\"accuracy\")\nax.set_title(\"Accuracy vs alpha for training and testing sets\")\nax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n        drawstyle=\"steps-post\")\nax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n        drawstyle=\"steps-post\")\nax.legend()\nplt.show()","06f54298":"![](https:\/\/miro.medium.com\/max\/1200\/1*WqId29D5dN_8DhiYQcHa2w.png)","2c5fe256":"# Import Libraries","e3af9d2c":"**Normalise numerical columns**","a72e3c29":"# Read Dataset","3abb4bd0":"# Numerical Features","7dd17649":"**Now, we will train using effective alphas.**\n**Note**\n- The last value of ccp_alpha. We will remove that, because it will prune the whole tree. So avoiding that value by doing ccp_alphas[-1] from the list","4984e829":"# Chi Square Siginifcance Test For Categorical Variables","85d8bf95":"# Dummy And Standarize","1874a3f5":"# Skewness","54457e3f":"**Analysis**\n- Total Charges column is positively skewed.","cdc5623d":"**Analysis**\n- No outliers in TotalCharges Column","47e3ba6d":"# Categorical Features: Significance Check","c308be82":"**We will be looking into churn modeling dataset. I will walkthrough with Undersampling and Oversampling Techniques. Please Upvote, if you like my notebook. :)**","54aca3e3":"# Final Dataframe","facc72b2":"**Undersampling Technique (Variable name: new_df)**","a8529586":"# Model Building","c6ae8d97":"**Analysis**\n- We will remove the last node of the clfs list. Because that tree consist of only one node.","7ed57b34":"# Oversampling Technique during Cross Validation","7fde3f90":"**Analysis**\n- I will extract original test values dataframe.\n- Before going to random undersampling or oversampling technique, I want to test in the original dataframe whether it is  unbalanced.","9608c1ed":"**Analysis**\n- No outliers in MonthlyCharges Column","74f6a315":"# Outlier Analysis","60881486":"# Cost Complexity Pruning with Decision Tree","a124f65b":"- As alpha increases, more of the tree is pruned, which increses total impurity of the leaves of the DT.","9fd6c7a8":"**Analysis**\n- No outliers in Tenure Column","1ba8606d":"# Data Preparation","0d87f11c":"**Analysis**\n- Removing those insignificant columns","a2987355":"# Graph: Accuracy vs alpha for training and testing sets","162d1fba":"# The End"}}