{"cell_type":{"0469d4d5":"code","1a7596cb":"code","991a3608":"code","16b8af68":"code","3df1f5dc":"code","e1ed1dc7":"code","f941460e":"code","e70ca947":"code","78d7a7d5":"code","5208e591":"code","739b3862":"code","5dc2fe48":"code","73cb0a1e":"code","fb477161":"code","358f2b11":"code","0ec81f03":"code","ec54c143":"code","e32a443b":"code","a216992f":"code","babf3c11":"code","877585a0":"code","769a13cb":"code","d8dc94b1":"code","9c2fdb1e":"code","fccfd21c":"code","76040948":"code","63f88698":"code","0b87a4dc":"code","6b33a3b1":"markdown","60f67ad4":"markdown","de2eb35b":"markdown","7baf23ea":"markdown","fb3e9a57":"markdown","7da970b8":"markdown","14dd97e6":"markdown","f60089d8":"markdown","85129b87":"markdown","b3d883a8":"markdown","229077e8":"markdown","5b145b95":"markdown","44c15d84":"markdown","b3b6191c":"markdown","cda5a955":"markdown","8d9121bf":"markdown","95e7181b":"markdown","5d3fcf6a":"markdown","f14742d1":"markdown","e40fc909":"markdown","3c4f33da":"markdown","76307543":"markdown","d9a49297":"markdown","ac9cf0f5":"markdown","dc609ee4":"markdown","11daee4c":"markdown"},"source":{"0469d4d5":"# Import necessaries librairies\nimport pandas as pd\nimport numpy as np \nimport tensorflow_datasets as tfds\nimport tensorflow as tf \ntf.__version__","1a7596cb":"# Loading document txt function\ndef load_doc(url):\n  df = pd.read_csv(url, delimiter=\"\\t\", header=None)\n  return df","991a3608":"# Loading document txt\ndoc = load_doc(\"..\/input\/bilingual-sentence-pairs\/fra.txt\")\ndoc=doc.iloc[:,:2]\ndoc.columns=['english','french']\ndoc.head()","16b8af68":"# Let's just take a sample of 5000 sentences to avoid slowness. \ndoc = doc.sample(5000)","3df1f5dc":"tokenizer_fr = tf.keras.preprocessing.text.Tokenizer()\ntokenizer_en = tf.keras.preprocessing.text.Tokenizer()","e1ed1dc7":"tokenizer_en.fit_on_texts(doc.iloc[:,0])\ntokenizer_fr.fit_on_texts(doc.iloc[:,1])","f941460e":"doc[\"fr_indices\"] = tokenizer_fr.texts_to_sequences(doc.iloc[:,1])\ndoc[\"en_indices\"] = tokenizer_en.texts_to_sequences(doc.iloc[:,0])","e70ca947":"doc.head()","78d7a7d5":"# Use of Keras to create token sequences of the same length\npadded_fr_indices = tf.keras.preprocessing.sequence.pad_sequences(doc[\"fr_indices\"], padding=\"post\")\npadded_en_indices = tf.keras.preprocessing.sequence.pad_sequences(doc[\"en_indices\"], padding=\"post\")","5208e591":"# Visualization of the shape of one of the tensors\npadded_fr_indices.shape","739b3862":"padded_en_indices.shape","5dc2fe48":"# Application of the categorization of the target variable \nbinarized_en_indices = tf.keras.utils.to_categorical(padded_en_indices)\nbinarized_en_indices.shape","73cb0a1e":"# Creation of tf.data.Dataset for each of the French and English tensors\nfr_ds = tf.data.Dataset.from_tensor_slices(padded_fr_indices)\nen_ds = tf.data.Dataset.from_tensor_slices(binarized_en_indices)","fb477161":"# Create a tensorflow dataset complet\ntf_ds = tf.data.Dataset.zip((fr_ds, en_ds))","358f2b11":"fr_tensor, en_tensor = next(iter(tf_ds))\r\nprint(fr_tensor)\r\nprint(en_tensor)","0ec81f03":"# Shuffle & Batch\nBATCH_SIZE = 32\nvocab_size_fr = len(tokenizer_fr.word_index)\nvocab_size_en = len(tokenizer_en.word_index)","ec54c143":"# Train Test Split\nTAKE_SIZE = int(0.7*len(doc))\n\ntrain_data = tf_ds.take(TAKE_SIZE)\ntest_data = tf_ds.skip(TAKE_SIZE)\n\ntrain_data = train_data.shuffle(TAKE_SIZE).batch(BATCH_SIZE)\ntest_data = test_data.shuffle(len(doc) - TAKE_SIZE).batch(BATCH_SIZE)\n","e32a443b":"# Create the model\nmodel = tf.keras.Sequential([\n                  # Input Word Embedding layer        \n                  tf.keras.layers.Embedding(vocab_size_fr+1, 64, input_shape=[fr_tensor.shape[0]]),\n\n                  # LSTM Bidirectional layer\n                  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n                  \n                  # LSTM Bidirectionnal new layer\n                  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\n           \n])","a216992f":"model.summary()","babf3c11":"# Create the model\r\nmodel = tf.keras.Sequential([\r\n                  # Input Word Embedding layer        \r\n                  tf.keras.layers.Embedding(vocab_size_fr+1, 64, mask_zero=True),\r\n\r\n                  # LSTM Bidirectional layer\r\n                  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\r\n                  \r\n                  # LSTM Bidirectionnal new layer\r\n                  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False)),\r\n\r\n                  # Repeat Vector\r\n                  tf.keras.layers.RepeatVector(binarized_en_indices.shape[1]),\r\n\r\n                  # LSTM new layer\r\n                  tf.keras.layers.LSTM(32, return_sequences=True),               \r\n\r\n                  # Output layer with number of output neurons equal to class number with softmax function\r\n                  tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size_en+1, activation=\"softmax\"))\r\n           \r\n])","877585a0":"model.summary()","769a13cb":"# \"Random\" prediction to test our model \ninput_text, output_text = next(iter(train_data))\nprint(input_text.numpy().shape)\nprint(model.predict(input_text).shape)\nprint(output_text.numpy().shape)","d8dc94b1":"# Using a simple compiler with an Adam optimizer to compute our gradients \noptimizer= tf.keras.optimizers.Adam()\n\nmodel.compile(optimizer=optimizer,\n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=[tf.keras.metrics.CategoricalAccuracy()])","9c2fdb1e":"# Application of the model on 200 epochs\nhistory = model.fit(train_data,\n                    validation_data=test_data,\n                    epochs=200)","fccfd21c":"# Testing a translation\nfor input_text, translation in test_data.take(1):\n  pred = model.predict(input_text)","76040948":"tokenizer_fr.sequences_to_texts(input_text.numpy())","63f88698":"tokenizer_en.sequences_to_texts(tf.argmax(translation, axis=-1).numpy())","0b87a4dc":"tokenizer_en.sequences_to_texts(tf.argmax(pred, axis=-1).numpy())","6b33a3b1":"7. Padding sequences to have entries of the same size","60f67ad4":"13. Define an object `BATCH_SIZE` equal to 32 and an object `vocab_size_fr` and `vocab_size_en` equal respectively to the number of unique words in the french sentences and the english sentences.","de2eb35b":"6. Create two new columns in the Dataframe for the encoded french and english sentences.","7baf23ea":"## Part 5 : Modelling <a class=\"anchor\" id=\"chapter5\"><\/a>\n\nOnce we have that, we can move on to modeling. To create the model, we'll need : \n\n* A layer of [Embedding](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Embedding?hl=en)\n* Layer [LSTM](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/LSTM?hl=en) & [Bidirectional](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Bidirectional?hl=en)\n* A layer [RepeatVector](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/RepeatVector?hl=en)\n* A [Dense](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Dense?hl=en) & [TimeDistributed](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/TimeDistributed?hl=en) output layer\n\nThe objective being to have in input, a tensor of dimension `(batch_size, max_len_of_an_english_sentence)` and in output a tensor of dimension `(batch_size, max_len_of_an_english_sentence, num_of_classes).","fb3e9a57":"8. Just check the shapes of the arrays created for the french and english sentences?","7da970b8":"Create the model's architecture using `tf.keras.Sequential` :\r\n* The first layer will be Embedding for the french sentences\r\n* Then we will use `tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True))r?\r\n* The third layer will be `tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=False\n`ture.\r\n\r\n","14dd97e6":"Try and apply the `__call__` method of the model on an input batch and compare the shape of the output with the shape of a target batch.","f60089d8":"7. Let's train the model on 200 epochs","85129b87":"2. Create an object `doc` containing a sample of 5000 rows from the file.","b3d883a8":"11. Bring the two tensor dataset together.","229077e8":"![LSTM.png](attachment:5b67f91b-b070-487f-8395-6baf3a7cdda2.png)\n\nImage by [Tenserflow by BackProp](https:\/\/tensorflow.backprop.fr\/)","5b145b95":"Define an Adam optimizer and compile the model using the appropriate loss and metric.","44c15d84":"## Part 4 : Preprocessing <a class=\"anchor\" id=\"chapter4\"><\/a>\n\nThe whole purpose of the preprocessing is to express the (French) entry sentence in a sequence of clues.\n\ni.e. :\n\n* Je suis malade ---> $[123, 21, 34, 0, 0, 0, 0, 0]$\n\nThis gives a *shape* -> `(batch_size, max_len_of_a_sentence)`.\n\nThe clues correspond to a number that we will have to assign for each word token. \n\nThe zeros correspond to what are called [*padded_sequences*](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/sequence\/pad_sequences) which allow all word sequences to have the same length (mandatory for your algorithm). \n\nThe transformation of your target sentence will not be exactly the same as that of your input sentence. In addition to all the steps you will have performed for the input sentences, you will also have to *categorize* your target sentence. In other words, an example tensor would look like : \n\n* Je suis malade ---> $\\begin{bmatrix} 1&0&0&...&0&0 \\\\ 0&0&0&...&1&0 \\\\ ... \\\\ 0&1&0&...&0&0 \\end{bmatrix}$\n\nThis gives a *shape* -> `(batch_size, max_len_of_an_english_sentence, num_of_classes)`.\n\nTo help do this, we can use : \n\n* `Pandas` or `Numpy` for reading the text file.\n* `Spacy` for Tokenization \n* `Tensorflow` for [padded_sequence](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/sequence\/pad_sequences) & [categorization](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/to_categorical?hl=en)","b3b6191c":"4. Create two objects : `tokenizer_fr` and `tokenizer_en` that will be instances of the `tf.keras.preprocessing.text.Tokenizer` class. ","cda5a955":"## Part 3 : Importing Data  <a class=\"anchor\" id=\"chapter3\"><\/a>","8d9121bf":"9. Represent the words in the target sentences as dummy variables.","95e7181b":"10. Create a tensor dataset for the french sentences, and one for the english sentences.","5d3fcf6a":"It looks like the output of the last `LSTM` layer is (None, 128) a flat output, however the shape of the output must correspond to the shape of the english tensors which is (BATCH_SIZE, lenght_en_sentence, vocab_size_en) which is not flat.\r\n* We will have to use a trick in order to get the right shape at the end. Read the documentation of `tf.keras.layers.RepeatVector` in order to change the flt output into a sequence.\r\n* Add this layer `tf.keras.layers.LSTM(32, return_sequences=True`    \r\n* And finish off the model with this layetf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size_en+1, activation=\"softmax\"))`oblem?\r\n","f14742d1":"14. Finalize the preprocessing with the following steps:\r\n* Create a `train_data` and a `test_data` object using the `take` and `skip` methods.\r\n* Shuffle the two datasets\r\n* Batch the two datasets using the pre-defined batch size","e40fc909":"## Part 1 : Project description  <a class=\"anchor\" id=\"chapter1\"><\/a>\n\nLet's see how we can create a French --> English translator with TensorFlow.\n\nWe'll see that with LSTMs, we can do some pretty powerful things like: *translators* ! \n\n**This is not an easy project, this is why I give only tips on the main steps to follow.**\n\nRecurrent neural networks (RNN) are widely used in artificial intelligence as soon as a temporal notion intervenes in the data (sometimes \"hidden\" as in text analysis).\n\nBut these RNNs suffer from limitations in their internal structure that make them inoperable in many situations. This is where LSTM and GRU cells come in, whose performance is only matched by their complexity!\n\n## Part 2 : LSTM architecture  <a class=\"anchor\" id=\"chapter2\"><\/a>","3c4f33da":"# Conclusion \n\nThe predictions are not very good. Although the model has been trained, it overfit.\n\nLSTMs are very difficult and time consuming types of RNNs to train.\n\nTo improve the quality of the translation, the number of samples taken at the beginning of the kernel should be increased. At the moment we have taken a sample of 5,000 but this is far too few to translate text. I would suggest to improve the predictions by taking a sample of minimum 100 000, but having tried with such a sample, my CPU is not powerful enough! If you have the chance to have a GPU it would be interesting to try and see if the predictions are better.\n\n","76307543":"Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. This is a behavior required in complex problem domains like machine translation, speech recognition, and more.\n\nLSTMs make small modifications to the information by multiplications and additions. With LSTMs, the information flows through a mechanism known as cell states. This way, LSTMs can selectively remember or forget things. \n\n**The information at a particular cell state has three different dependencies :**\n\n- The memory from previous cell (i.e. the information that was present in the memory after the previous time step) \n- The output from the previous cell (i.e. this is the same as the output of the previous cell)\n- The input at the current time step (i.e. the new information that is being fed in at that moment)","d9a49297":"# NLP Translation French --> English \ud83d\udd01\ud83d\udd01\n\n## Table of Contents\n\n* [Part 1 : Project description](#chapter1)\n    \n* [Part 2 : LSTM Architecture](#chapter2)\n\n* [Part 3 : Importing Data](#chapter3)\n\n* [Part 4 : Preprocessing](#chapter4)\n\n* [Part 5 : Modelling](#chapter5)","ac9cf0f5":"1. Load the data. We can read this using `pd.read_csv` with the `\"\\t\"` delimiter and `header=None`","dc609ee4":"5. Fit the tokenizers on the french and english sentences respectively.","11daee4c":"12. Print out the first tensor in your full dataset."}}