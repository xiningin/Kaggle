{"cell_type":{"471b6e50":"code","c597bd07":"code","c11e10ec":"code","ae16e34f":"code","4fbbb137":"code","3690f9aa":"code","b57149cf":"code","6280db72":"code","2871cef7":"code","ad96e1e8":"code","9ed90e97":"code","16a12250":"code","e3ddbff7":"code","563370a9":"code","c56299e2":"code","562d6b84":"code","7fcb8963":"code","e61fdfbb":"code","860dbf17":"code","579c648b":"code","fe679dd0":"code","136b8de1":"code","a50f0452":"code","b4256bdc":"code","be5b685b":"code","0b06e480":"code","07040f51":"code","aba7ba2f":"code","fbe04951":"markdown","36112326":"markdown","eabbaa3d":"markdown","0feeda09":"markdown","18076c51":"markdown","f06ab5bc":"markdown","c845b580":"markdown","3d73e0e1":"markdown","b563fd94":"markdown","1cbcbe01":"markdown","4f9c78b1":"markdown","c586b277":"markdown","a245aee8":"markdown","d64baa0a":"markdown","7a75ec0f":"markdown","a1106642":"markdown","861ca9dc":"markdown","b9c027ec":"markdown","0ea3d800":"markdown"},"source":{"471b6e50":"!pip install tf_explain\n#!pip install split-folders\n#!conda install -y gdown","c597bd07":"import os\nimport pandas as pd\n\nimport xml.etree.ElementTree as ET\n#import gdown\nimport time\nimport math\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as image\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nfrom keras.applications.xception import Xception, preprocess_input\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.models import Sequential\n\nfrom keras.utils import np_utils\nfrom keras.utils import Sequence\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tf_explain.core.activations import ExtractActivations\n\nfrom tensorflow.keras.applications.xception import decode_predictions\n%matplotlib inline\nfrom sklearn.metrics import classification_report\n\nfrom PIL import Image\nimport matplotlib.image as mpimg\nfrom imgaug import augmenters as iaa\nprint(\"Loaded all libraries\")","c11e10ec":"image_path = '..\/input\/stanford-dogs-dataset\/images\/Images'\n#image_path ='\/media\/marco\/DATA\/OC_Machine_learning\/section_6\/DATA\/Images\/'\nnum_of_categories = 120\nimage_size = 299\nbatch_size = 16","ae16e34f":"breed_list = sorted(os.listdir(image_path))\n\nnum_classes = len(breed_list)\nprint(\"{} breeds\".format(num_classes))","4fbbb137":"# Define a time counter function to test the algorythms performance \n_start_time = time.time()\n\ndef process_time_starts():\n    global _start_time \n    _start_time = time.time()\n\ndef time_elapsed():\n    t_sec = round(time.time() - _start_time)\n    (t_min, t_sec) = divmod(t_sec,60)\n    (t_hour,t_min) = divmod(t_min,60) \n    print('The process took: {}hour:{}min:{}sec'.format(t_hour,t_min,t_sec))","3690f9aa":"\n\n# copy from https:\/\/www.kaggle.com\/gabrielloye\/dogs-inception-pytorch-implementation\n# reduce the background noise\n\nos.mkdir('data')\nfor breed in breed_list:\n    os.mkdir('data\/' + breed)\nprint('Created {} folders to store cropped images of the different breeds.'.format(len(os.listdir('data'))))\n\n","b57149cf":"%%time\nfor breed in os.listdir('data'):\n    for file in os.listdir('..\/input\/stanford-dogs-dataset\/annotations\/Annotation\/{}'.format(breed)):\n        img = Image.open('..\/input\/stanford-dogs-dataset\/images\/Images\/{}\/{}.jpg'.format(breed, file))\n        tree = ET.parse('..\/input\/stanford-dogs-dataset\/annotations\/Annotation\/{}\/{}'.format(breed, file))\n        xmin = int(tree.getroot().findall('object')[0].find('bndbox').find('xmin').text)\n        xmax = int(tree.getroot().findall('object')[0].find('bndbox').find('xmax').text)\n        ymin = int(tree.getroot().findall('object')[0].find('bndbox').find('ymin').text)\n        ymax = int(tree.getroot().findall('object')[0].find('bndbox').find('ymax').text)\n        img = img.crop((xmin, ymin, xmax, ymax))\n        img = img.convert('RGB')\n        img = img.resize((image_size, image_size))\n        img.save('data\/' + breed + '\/' + file + '.jpg')","6280db72":"plt.figure(figsize=(10, 10))\nfor i in range(9):\n    plt.subplot(331 + i) # showing 9 random images\n    breed = np.random.choice(breed_list) # random breed\n    dog = np.random.choice(os.listdir('..\/input\/stanford-dogs-dataset\/annotations\/Annotation\/' + breed)) # random image \n    img = Image.open('..\/input\/stanford-dogs-dataset\/images\/Images\/' + breed + '\/' + dog + '.jpg') \n    tree = ET.parse('..\/input\/stanford-dogs-dataset\/annotations\/Annotation\/' + breed + '\/' + dog) # init parser for file given\n    root = tree.getroot() # idk what's it but it's from documentation\n    objects = root.findall('object') # finding all dogs. An array\n    plt.imshow(img) # displays photo\n    for o in objects:\n        bndbox = o.find('bndbox') # reading border coordinates\n        xmin = int(bndbox.find('xmin').text)\n        ymin = int(bndbox.find('ymin').text)\n        xmax = int(bndbox.find('xmax').text)\n        ymax = int(bndbox.find('ymax').text)\n        plt.plot([xmin, xmax, xmax, xmin, xmin], [ymin, ymin, ymax, ymax, ymin]) # showing border\n        plt.text(xmin, ymin, o.find('name').text, bbox={'ec': None}) # printing breed","2871cef7":"label_maps = {}\nlabel_maps_rev = {}\nfor i, v in enumerate(breed_list):\n    label_maps.update({v: i})\n    label_maps_rev.update({i : v})","ad96e1e8":"def paths_and_labels():\n    paths = list()\n    labels = list()\n    targets = list()\n    for breed in breed_list:\n        base_name = \".\/data\/{}\/\".format(breed)\n        for img_name in os.listdir(base_name):\n            paths.append(base_name + img_name)\n            labels.append(breed)\n            targets.append(label_maps[breed])\n    return paths, labels, targets\n\npaths, labels, targets = paths_and_labels()\n\nassert len(paths) == len(labels)\nassert len(paths) == len(targets)\n\ntargets = np_utils.to_categorical(targets, num_classes=num_classes)","9ed90e97":"class ImageGenerator(Sequence):\n    \n    def __init__(self, paths, targets, batch_size, shape, augment=False):\n        self.paths = paths\n        self.targets = targets\n        self.batch_size = batch_size\n        self.shape = shape\n        self.augment = augment\n        \n    def __len__(self):\n        return int(np.ceil(len(self.paths) \/ float(self.batch_size)))\n    \n    def __getitem__(self, idx):\n        batch_paths = self.paths[idx * self.batch_size : (idx + 1) * self.batch_size]\n        x = np.zeros((len(batch_paths), self.shape[0], self.shape[1], self.shape[2]), dtype=np.float32)\n        y = np.zeros((self.batch_size, num_classes, 1))\n        for i, path in enumerate(batch_paths):\n            x[i] = self.__load_image(path)\n        y = self.targets[idx * self.batch_size : (idx + 1) * self.batch_size]\n        return x, y\n    \n    def __iter__(self):\n        for item in (self[i] for i in range(len(self))):\n            yield item\n            \n    def __load_image(self, path):\n        image = cv2.imread(path)\n        image = preprocess_input(image)\n        if self.augment:\n            seq = iaa.Sequential([\n                iaa.OneOf([\n                    iaa.Fliplr(0.5),\n                    iaa.Flipud(0.5),\n                    iaa.Sometimes(0.5,\n                    \n                    ),\n                    iaa.Affine(\n                        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n                        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n                        rotate=(-40, 40),\n                        shear=(-8, 8)\n                    )\n                ])\n            ], random_order=True)\n            image = seq.augment_image(image)\n        return image","16a12250":"x_train, x_test, y_train, y_test = train_test_split(paths, targets, test_size=0.2, random_state=42)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=1)\n\ntrain_ds = ImageGenerator(x_train, y_train, batch_size=32, shape=(image_size, image_size,3), augment=True)\nval_ds = ImageGenerator(x_test, y_test, batch_size=32, shape=(image_size, image_size,3), augment=False)\ntest_ds = ImageGenerator(x_test, y_test, batch_size=32, shape=(image_size, image_size,3), augment=False)","e3ddbff7":"\n\n#url = 'https:\/\/drive.google.com\/uc?id=1aCFGR5c7Ap4JPzryR_RSgRYc7FRYgbyG'\n\n#output = 'xception_weights.h5'\n\n#gdown.download(url, output, quiet=False)","563370a9":"base_model = tf.keras.applications.xception.Xception(weights='imagenet',include_top=False, pooling='avg')#Summary of Xception Model\n\nbase_model.trainable = False\n\n\n#pre_trained_model.summary()\n\n","c56299e2":"flat_dim = 5 * 5 * 2048\n\nmy_model = Sequential(base_model)\n\n#my_model.add(Flatten())\n#my_model.add(Dropout(0.1)) # dropout added\nmy_model.add(Dense(1032, activation='relu',input_dim=flat_dim))\nmy_model.add(Dense(512, activation='relu'))\n#my_model.add(Dropout(0.1))\nmy_model.add(Dense(256, activation='relu'))\nmy_model.add(Dense(120, activation='softmax'))\n\n","562d6b84":"###################\ntotal_epoch = 8\nlearning_rate_init = 0.00001\n###################\n\ndef lr_scheduler(epoch):\n    epoch += 1\n   \n    if epoch == 1:\n        return learning_rate_init\n    \n    elif epoch >= 2 and epoch <= 40:\n        return (0.2*epoch**3)*math.exp(-0.45*epoch)*learning_rate_init\n    \n    else:\n        return lr_scheduler(40-1)\n    \n\nstage = [i for i in range(0,25)]\nlearning_rate = [lr_scheduler(x) for x in stage]\nplt.plot(stage, learning_rate)\nprint(learning_rate)","7fcb8963":"# Callbacks\n\nscheduler = keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)\nearly_stop = EarlyStopping(monitor='val_accuracy', patience = 6, mode='max', min_delta=1, verbose=1)","e61fdfbb":"my_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","860dbf17":"process_time_starts()\n\nhist = my_model.fit_generator(generator=train_ds, steps_per_epoch=400, validation_data=val_ds,  validation_steps=90, epochs=8, callbacks=[scheduler])\n\n                  ","579c648b":"          \n\ntime_elapsed()","fe679dd0":"fig, ax = plt.subplots(1, 2, figsize=(10, 3))\nax = ax.ravel()\n\nfor i, met in enumerate(['accuracy', 'loss']):\n    ax[i].plot(hist.history[met])\n    ax[i].plot(hist.history['val_' + met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs')\n    ax[i].set_ylabel(met)\n    ax[i].legend(['train', 'val'])","136b8de1":"my_model.save('my_model.h5', overwrite=True) \nmy_model.save_weights('dog_breed_xcept_weights.h5', overwrite=True)\nprint(\"Saved model to disk\")","a50f0452":"test_loss, test_accuracy = my_model.evaluate_generator(generator=test_ds,steps=int(100))\n\nprint(\"Test results \\n Loss:\",test_loss,'\\n Accuracy',test_accuracy)","b4256bdc":"\n#report = classification_report(test_ds.classes, pred, target_names=class_to_id)\n#print(report)","be5b685b":"def download_and_predict(url, filename):\n    # download and save\n    os.system(\"curl -s {} -o {}\".format(url, filename))\n    img = Image.open(filename)\n    img = img.convert('RGB')\n    img = img.resize((299, 299))\n    img.save(filename)\n    # show image\n    plt.figure(figsize=(4, 4))\n    plt.imshow(img)\n    plt.axis('off')\n    # predict\n    img = image.imread(filename)\n    img = preprocess_input(img)\n    probs = my_model.predict(np.expand_dims(img, axis=0))\n    for idx in probs.argsort()[0][::-1][:5]:\n        print(\"{:.2f}%\".format(probs[0][idx]*100), \"\\t\", label_maps_rev[idx].split(\"-\")[-1])","0b06e480":"download_and_predict(\"https:\/\/cdn.pixabay.com\/photo\/2018\/08\/12\/02\/52\/belgian-mallinois-3599991_1280.jpg\",\n                     \"test_1.jpg\")\n\n","07040f51":"download_and_predict(\"http:\/\/giandonet.altervista.org\/Marco\/ala.JPG\",\n                     \"test_2.jpg\")\n","aba7ba2f":"download_and_predict(\"http:\/\/giandonet.altervista.org\/Marco\/surfingdog.jpg\",\n                     \"test_3.jpg\")","fbe04951":"### 1.1 Libraries and data","36112326":"### 5. Save model and parameters","eabbaa3d":"### 2.2 Fully connected layer","0feeda09":"\nI will be performing following steps :\n1. Importing the data \n2. Data Argumentation And Visualization \n3. Importing the Xception ( Transfer learning ) \n4. Fully Connected layer \n5. Model Training \n6. Accuracy And Loss Visualization\n7. Test the model\n\n\n","18076c51":"### 6. Test model accuracy","f06ab5bc":"### 4. LOSS AND ACCURACY VISUALIZATION ","c845b580":"## 1. DATASET EXPLORATION ","3d73e0e1":"### 2.1 Importing the Xception CNN","b563fd94":"### 2.3 Define callbacks and compile the model","1cbcbe01":"## 2. MODEL PREPARATION ","4f9c78b1":"### Xception - Stanford Dogs dataset classification\n\nIf you like this script, consider adopting a dog, and never abandon yours\n\n![Never abandon a dog](http:\/\/www.the-teen-spirit.dk\/films\/vanwilder\/image.jpg)\n","c586b277":"### 1.4 Split X and y into train, validation and test","a245aee8":"### 1.3 Define my own image generator with custom augmentation","d64baa0a":"### 1.2 Generate a data folder with cropped pictures ","7a75ec0f":"## 7. Predict new images","a1106642":"Print cropped images","861ca9dc":"# **ABOUT THE DATASET **\n\nThe Stanford Dogs dataset contains images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization. It was originally collected for fine-grain image categorization, a challenging problem as certain dog breeds have near identical features or differ in colour and age.\n","b9c027ec":"## 3. TRAINING THE MODEL","0ea3d800":"### 1.2.1 Define paths (X) and labels (y)"}}