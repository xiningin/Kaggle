{"cell_type":{"e8d23178":"code","ef9d96c5":"code","eee2b461":"code","2808d59f":"code","11267fc8":"code","a33d0ab5":"code","03a8796e":"code","86a1fbbd":"code","15c95397":"code","fd5f0645":"code","9b343a4e":"code","e70f4ada":"code","97ace6be":"code","dca60897":"code","b00bc9b2":"code","7d1d3eff":"code","50b68656":"code","147eb3ab":"code","93ef695e":"code","2fcdb85c":"code","e87e8bda":"code","860a9b2a":"code","193b1c27":"code","28542045":"code","98d08246":"code","44158cd5":"code","4115a329":"code","d02d90ba":"code","0abcc4c3":"code","082809a6":"code","ea3e3468":"code","aabebf09":"code","ae1a0f66":"code","0a2aaefb":"code","0687b872":"code","cd9727f3":"code","db2066a2":"code","3077e489":"code","a1e28b34":"code","adefa11d":"code","16c159b4":"code","ece6b2e4":"code","b887a38f":"code","b95f27c1":"code","17b743dd":"code","a6b2bdf6":"code","c267e513":"markdown","3766a2df":"markdown","cc4a3804":"markdown","305fd455":"markdown","0e7c54c1":"markdown","f102ac12":"markdown","5e9aeb1a":"markdown"},"source":{"e8d23178":"# Standard Libraries\nimport pandas as pd\nimport numpy as np\nimport json\n\n# Data Preprocessing & NLP\nimport nltk\nimport re\nimport string\nimport gensim\nfrom textblob import Word\n\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\n\nfrom gensim.utils import simple_preprocess\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer, SnowballStemmer\nfrom nltk.stem.porter import *\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import MultiLabelBinarizer\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, AdaBoostClassifier, BaggingRegressor, GradientBoostingClassifier,BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics.pairwise import linear_kernel\n\n# Performance metrics\nfrom sklearn.metrics import confusion_matrix, f1_score\nfrom sklearn.model_selection import cross_val_score, KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, make_scorer, roc_curve, roc_auc_score\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Visualization Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nsns.set()\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","ef9d96c5":"news_df = pd.read_csv('..\/input\/bbc-news\/bbc-text.csv')","eee2b461":"news_df.info()","2808d59f":"news_df['category'].value_counts()","11267fc8":"#Associe os nomes de 'category' ao \u00edndice num\u00e9rico e salve-o na nova coluna category_id\nnews_df['category_id'] = news_df['category'].factorize()[0]\n","a33d0ab5":"#Distribui\u00e7\u00e3o de artigos de not\u00edcias por categoria\nnews_df.groupby('category').category_id.count().plot.bar()","03a8796e":"news_df.info()","86a1fbbd":"# Crie um novo dataframe \"category_id_df\", que tem apenas categorias exclusivas, tamb\u00e9m classificando esta lista na ordem dos valores de category_id\ncategory_id_df = news_df[['category', 'category_id']].drop_duplicates().sort_values('category_id')\n\n# Crie um dicion\u00e1rio (estrutura de dados python - como uma tabela de pesquisa) que \n# pode facilmente converter nomes de categorias em category_ids e vice-versa\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'category']].values)\n\ncategory_to_id\n#category_to_id['tech']","15c95397":"# Eliminar dados duplicados\nnews_df.drop_duplicates(subset=['category', 'text'], inplace=True)","fd5f0645":"news_df.info()","9b343a4e":"# Limpeza de Dados\ndef clean_text(text):\n    # remova tudo exceto alfabetos\n    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n    # remover espa\u00e7os em branco\n    text = ' '.join(text.split())\n    text = text.lower()\n    \n    return text","e70f4ada":"# criando uma coluna com texto limpo\nnews_df['clean_text'] = news_df['text'].apply(clean_text)","97ace6be":"news_df.text[0]","dca60897":"news_df.clean_text[0]","b00bc9b2":"news_df.head()","7d1d3eff":"#https:\/\/www.nltk.org\/\nall_words = ' '.join([text for text in news_df['clean_text']])\nall_words = all_words.split()\nfreq_dist = nltk.FreqDist(all_words)\nwords_df = pd.DataFrame({'word':list(freq_dist.keys()), 'count':list(freq_dist.values())})\nwords_df.head()    ","50b68656":"def freq_words(x, terms = 30):\n    all_words = ' '.join([text for text in x])\n    all_words = all_words.split()\n    \n    #https:\/\/www.nltk.org\/\n    freq_dist = nltk.FreqDist(all_words)\n    words_df = pd.DataFrame({'word':list(freq_dist.keys()), 'count':list(freq_dist.values())})\n    \n    fig = plt.figure(figsize=(21,16))\n    ax1 = fig.add_subplot(2,1,1)\n    wordcloud = WordCloud(width=1000, height=300, background_color='black', \n                          max_words=1628, relative_scaling=1,\n                          normalize_plurals=False).generate_from_frequencies(freq_dist)\n    \n    ax1.imshow(wordcloud, interpolation=\"bilinear\")\n    ax1.axis('off')\n    \n    ax2 = fig.add_subplot(2,1,2)\n    d = words_df.nlargest(columns=\"count\", n = terms) \n    ax2 = sns.barplot(data=d, palette = sns.color_palette('BuGn_r'), x= \"count\", y = \"word\")\n    ax2.set(ylabel= 'Word')\n    plt.show()","147eb3ab":"# plot 25 most frequent words including stop words\nfreq_words(news_df['clean_text'], 25)","93ef695e":"#https:\/\/pypi.org\/project\/stop-words\/\n#stop_words = set(stopwords.words('portuguese')) \nstop_words = set(stopwords.words('english'))\n#stop_words","2fcdb85c":"stop_wordsPTBR = set(stopwords.words('portuguese'))\n#stop_wordsPTBR","e87e8bda":"# stopwords-to compare text data with and without stopwords\n\nstop_words = set(stopwords.words('english'))\n\n# function to remove stopwords\ndef remove_stopwords(text):\n    no_stopword_text = [w for w in text.split() if not w in stop_words]\n    return ' '.join(no_stopword_text)\n  \nnews_df['clean_text'] = news_df['clean_text'].apply(lambda x: remove_stopwords(x))","860a9b2a":"# plot 25 most frequent words without stopwords\nfreq_words(news_df['clean_text'], 25)","193b1c27":"news_df.info()","28542045":"# Processo de lematiza\u00e7\u00e3o\n'''\nPalavras na terceira pessoa s\u00e3o alteradas para primeira pessoa e os verbos no passado e no futuro s\u00e3o alterados para o presente pelo\nprocesso de lematiza\u00e7\u00e3o.\n'''\nlemmatizer = WordNetLemmatizer()\n\ndef tokenize_and_lemmatize(text):\n    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n    filtered_tokens = []\n    \n    for token in tokens:\n        if re.search('[a-zA-Z]', token):\n            filtered_tokens.append(token)\n    lem = [lemmatizer.lemmatize(t) for t in filtered_tokens]\n    return lem","98d08246":"# Defining a Count Vectorizer object\ncount_vec = CountVectorizer(stop_words='english', max_features=10000)\ncount_vec","44158cd5":"# Defining a TF-IDF Vectorizer\ntfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1, 2), tokenizer=tokenize_and_lemmatize, max_features=10000, use_idf=True)\ntfidf_vec","4115a329":"features = tfidf_vec.fit_transform(news_df.clean_text).toarray() # Remaps the words in the 2225 articles in the text column of \n                                                  # data frame into features (superset of words) with an importance assigned \n                                                  # based on each words frequency in the document and across documents\n\nlabels = news_df.category_id                           # represents the category of each of the 2225 articles\n","d02d90ba":"#Get a feel of the features identified by tfidf\nfeatures.shape # How many features are there ? ","0abcc4c3":"# Remember the dictionary created to map category names to a number ? \ncategory_to_id.items()","082809a6":"# The sorted function Converts dictionary items into a (sorted) list. \n# In subsequent steps - We will use this list to iterate over the categories\nsorted(category_to_id.items())","ea3e3468":"news_df","aabebf09":"news_df.to_csv('out_df.csv', index=False)\nnews_df","ae1a0f66":"# Use chi-square analysis to find corelation between features (importantce of words) and labels(news category) \nfrom sklearn.feature_selection import chi2\n\nN = 3  # We are going to look for top 3 categories\n\n#For each category, find words that are highly corelated to it\nfor category, category_id in sorted(category_to_id.items()):\n    features_chi2 = chi2(features, labels == category_id)                   # Do chi2 analyses of all items in this category\n    indices = np.argsort(features_chi2[0])                                  # Sorts the indices of features_chi2[0] - the chi-squared stats of each feature\n    feature_names = np.array(tfidf_vec.get_feature_names())[indices]            # Converts indices to feature names ( in increasing order of chi-squared stat values)\n    unigrams = [v for v in feature_names if len(v.split(' ')) == 1]         # List of single word features ( in increasing order of chi-squared stat values)\n    bigrams = [v for v in feature_names if len(v.split(' ')) == 2]          # List for two-word features ( in increasing order of chi-squared stat values)\n    print(\"# '{}':\".format(category))\n    print(\"  . Most correlated unigrams:\\n       . {}\".format('\\n       . '.join(unigrams[-N:]))) # Print 3 unigrams with highest Chi squared stat\n    print(\"  . Most correlated bigrams:\\n       . {}\".format('\\n       . '.join(bigrams[-N:]))) # Print 3 bigrams with highest Chi squared stat","0a2aaefb":"features_chi2","0687b872":"news_df.info()","cd9727f3":"X = news_df.loc[:,'clean_text']\ny = news_df.loc[:,'category_id']","db2066a2":"# Basic validation: splitting the data 80-20-20 train\/test\nX_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(X, y, news_df.index, test_size=0.2, random_state=55)\n#X_train, X_cv, y_train, y_cv = train_test_split(X,y,test_size=.25, random_state=10)\n","3077e489":"# Tf-Idf transformation \nxtrain_tfidf = tfidf_vec.fit_transform(X_train)\nxtest_tfidf = tfidf_vec.transform(X_test)\n#xcv_tfidf = tfidf_vec.fit_transform(X_cv)\nxtrain_tfidf.shape","a1e28b34":"# Count Vectorizer transformation\nxtrain_cv = count_vec.fit_transform(X_train)\nxtest_cv = count_vec.transform(X_test)\nxtrain_cv.shape","adefa11d":"#create list of model and accuracy dicts\nperform_list = []","16c159b4":"def run_model(model_name, est_c, est_pnlty):\n    mdl=''\n    if model_name == 'Logistic Regression':\n        mdl = LogisticRegression()\n    elif model_name == 'Random Forest':\n        mdl = RandomForestClassifier(n_estimators=100)\n    elif model_name == 'Multinomial Naive Bayes':\n        mdl = MultinomialNB()\n    elif model_name == 'Linear SVC':\n        mdl = LinearSVC()\n    elif model_name == 'Logistic Regression GridSearchCV':\n        mdl = LogisticRegression(C=est_c, penalty=est_pnlty)        \n\n    oneVsRest = OneVsRestClassifier(mdl)\n    oneVsRest.fit(xtrain_tfidf, y_train)\n    y_pred = oneVsRest.predict(xtest_tfidf)\n    \n    \n    # Performance metrics\n    accuracy = round(accuracy_score(y_test, y_pred) * 100, 2)\n    # Get precision, recall, f1 scores\n    precision, recall, f1score, support = score(y_test, y_pred, average='micro')\n\n    print(f'Test Accuracy Score of Basic {model_name}: % {accuracy}')\n    print(f'Precision : {precision}')\n    print(f'Recall    : {recall}')\n    print(f'F1-score   : {f1score}')\n\n    # Add performance parameters to list\n    perform_list.append(dict([\n        ('Model', model_name),\n        ('Test Accuracy', round(accuracy, 2)),\n        ('Precision', round(precision, 2)),\n        ('Recall', round(recall, 2)),\n        ('F1', round(f1score, 2))\n         ]))","ece6b2e4":"run_model('Random Forest', est_c=None, est_pnlty=None)","b887a38f":"run_model('Logistic Regression', est_c=None, est_pnlty=None)","b95f27c1":"run_model('Multinomial Naive Bayes', est_c=None, est_pnlty=None)","17b743dd":"run_model('Linear SVC', est_c=None, est_pnlty=None)","a6b2bdf6":"model_performance = pd.DataFrame(data=perform_list)\nmodel_performance = model_performance[['Model', 'Test Accuracy', 'Precision', 'Recall', 'F1']]\nmodel_performance","c267e513":"# Notebook de Classifica\u00e7\u00e3o de texto\nO presente documento foi criado a partir [desse aqui](https:\/\/github.com\/cigdemtuncer\/NewsTextClassification\/blob\/main\/bbc_news.ipynb) ","3766a2df":"## Part 4: Model Training and Evaluation","cc4a3804":"## Part 2. Vizualiza\u00e7\u00e3o","305fd455":"## Part 1: Setup & Preprocessing","0e7c54c1":"### Performance metrics of models","f102ac12":"# Quais s\u00e3o as palavras mais frequentes no nosso dataframe?","5e9aeb1a":"## Part 3: Heading to Machine Learning"}}