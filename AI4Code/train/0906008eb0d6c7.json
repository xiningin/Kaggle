{"cell_type":{"725e0bcd":"code","d6568b62":"code","a0d2639a":"code","c146117a":"code","f7b9937d":"code","545ad754":"code","534d29f0":"code","35116cd5":"code","d0563c54":"code","6ea53c0f":"code","f1c79221":"code","1da728b5":"code","294990b1":"code","14e40a2e":"code","4ae7081b":"code","7dac600b":"code","4555dbe9":"code","c8518ae0":"code","22cd721c":"code","a4fd98c4":"code","ce705ccc":"code","9530e9a4":"code","3a819cf0":"code","be25307e":"code","616ce2ce":"code","8db0e3a5":"code","6381d959":"code","c95a5f89":"markdown","b5b28955":"markdown","e29adfd0":"markdown","254e0569":"markdown","02fdbc26":"markdown","ef00f42f":"markdown","48d9e634":"markdown","9ea5cdcb":"markdown","007f4894":"markdown","ac384493":"markdown","1d6b8522":"markdown"},"source":{"725e0bcd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# Because we only care about errors\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Because thats where they keep the files\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n#machine learning libraries\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, average_precision_score, f1_score, recall_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import scale\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nseed = 42","d6568b62":"# Load everything because we are crazy like that\nprevious_application = pd.read_csv('..\/input\/previous_application.csv', sep=',')\nPOS_CASH_balance = pd.read_csv('..\/input\/POS_CASH_balance.csv', sep=',')\ninstallments_payments = pd.read_csv('..\/input\/installments_payments.csv', sep=',')\ncredit_card_balance = pd.read_csv('..\/input\/credit_card_balance.csv', sep=',')\nbureau_balance = pd.read_csv('..\/input\/bureau_balance.csv', sep=',')\nbureau = pd.read_csv('..\/input\/bureau.csv', sep=',')\napplication_train = pd.read_csv('..\/input\/application_train.csv', sep=',')\napplication_test = pd.read_csv('..\/input\/application_test.csv', sep=',')","a0d2639a":"#Nothing fancy just grouping by the primary key\nbureau_balance_dumm = bureau_balance.groupby(['SK_ID_BUREAU']).sum()\nbureau_balance_dumm = bureau_balance_dumm.reset_index('SK_ID_BUREAU')\nbureau_balance_dumm.head()","c146117a":"\nbureau_merge = pd.merge(bureau, bureau_balance_dumm,  how='left')\nbureau_merge_dumm = bureau_merge.drop(['SK_ID_BUREAU'],axis = 1)\nbureau_merge_dumm = bureau_merge_dumm.groupby(['SK_ID_CURR']).sum()\nbureau_merge_dumm = bureau_merge_dumm.reset_index('SK_ID_CURR')\nbureau_merge_dumm.head()","f7b9937d":"POS_CASH_balance_loc = POS_CASH_balance.drop(['SK_ID_PREV'],axis = 1)\nPOS_CASH_balance_dumm = POS_CASH_balance_loc.groupby(['SK_ID_CURR']).sum()\nPOS_CASH_balance_dumm = POS_CASH_balance_dumm.reset_index('SK_ID_CURR')\nPOS_CASH_balance_dumm.head()","545ad754":"previous_application_loc = previous_application.drop(['SK_ID_PREV'],axis = 1)\nprevious_application_dumm = previous_application_loc.groupby(['SK_ID_CURR']).sum()\nprevious_application_dumm = previous_application_dumm.reset_index('SK_ID_CURR')\nprevious_application_dumm.head()","534d29f0":"installments_payments_loc = installments_payments.drop(['SK_ID_PREV'],axis = 1)\ninstallments_payments_dumm = installments_payments_loc.groupby(['SK_ID_CURR']).sum()\ninstallments_payments_dumm = installments_payments_dumm.reset_index('SK_ID_CURR')\ninstallments_payments_dumm.head()","35116cd5":"credit_card_balance_loc = credit_card_balance.drop(['SK_ID_PREV'],axis = 1)\ncredit_card_balance_dumm = credit_card_balance_loc.groupby(['SK_ID_CURR']).sum()\ncredit_card_balance_dumm = credit_card_balance_dumm.reset_index('SK_ID_CURR')\ncredit_card_balance_dumm.head()","d0563c54":"merge1 = pd.merge(application_train, bureau_merge_dumm,  how='left')\nmerge2 = pd.merge(merge1, POS_CASH_balance_dumm,   how='left')\nmerge3 = pd.merge(merge2, previous_application_dumm,  how='left')\nmerge4 = pd.merge(merge3, installments_payments_dumm,  how='left')\nmerge5 = pd.merge(merge4, credit_card_balance_dumm,  how='left')","6ea53c0f":"merge1_test = pd.merge(application_test, bureau_merge_dumm, how='left')\nmerge2_test = pd.merge(merge1_test, POS_CASH_balance_dumm,  how='left')\nmerge3_test = pd.merge(merge2_test, previous_application_dumm,  how='left')\nmerge4_test = pd.merge(merge3_test, installments_payments_dumm,  how='left')\nmerge5_test = pd.merge(merge4_test, credit_card_balance_dumm, how='left')","f1c79221":"# Training function\ndef data_prep_train(data_df):\n    \n    #how to handle types\n    data_df = data_df.set_index('SK_ID_CURR')\n    data_df_num = data_df.select_dtypes(exclude=object)\n    data_df_obj = data_df.select_dtypes(include=object)\n\n    #how to handle nan in numeric type\n    data_df_num_columns = data_df_num.drop(['TARGET'], axis=1).columns\n    for column in data_df_num_columns:\n        data_df_num[column] = data_df_num[column].fillna(data_df_num[column].mean())\n        #data_df_num[column] = scale(data_df_num[column])\n    \n    #how to handle nan in object type\n    data_df_obj_columns = data_df_obj.columns\n    for column in data_df_obj_columns:\n        data_df_obj[column] = data_df_obj[column].fillna(\"UNKNOWN\")\n        data_df_obj[column] = data_df_obj[column].astype('category')\n        data_df_obj[column] = data_df_obj[column].cat.codes\n    # stage for sampling\n    data_stage = pd.DataFrame(pd.concat([data_df_num, data_df_obj],axis=1))\n    \n    # sample count positive class \n    minority_count = data_stage.TARGET.value_counts().min()\n    majority_count = data_stage.TARGET.value_counts().max()\n    \n    # sampled data (down sampling)\n    data_stage0 = data_stage[data_stage['TARGET']==0].sample(majority_count, replace = False)\n    data_stage1 = data_stage[data_stage['TARGET']==1].sample(minority_count, replace = False)\n    \n    return pd.DataFrame(pd.concat([data_stage0, data_stage1],axis=0))\n\n\n# Testing function\ndef data_prep_test(data_df):\n    \n    #how to handle types\n    data_df = data_df.set_index('SK_ID_CURR')\n    data_df_num = data_df.select_dtypes(exclude=object)\n    data_df_obj = data_df.select_dtypes(include=object)\n\n    #how to handle nan in numeric type\n    data_df_num_columns = data_df_num.columns\n    for column in data_df_num_columns:\n        data_df_num[column] = data_df_num[column].fillna(data_df_num[column].mean())\n        #data_df_num[column] = scale(data_df_num[column])\n    \n    #how to handle nan in object type\n    data_df_obj_columns = data_df_obj.columns\n    \n    for column in data_df_obj_columns:\n        data_df_obj[column] = data_df_obj[column].fillna(\"UNKNOWN\")#.astype('object')\n        data_df_obj[column] = data_df_obj[column].astype('category')\n        data_df_obj[column] = data_df_obj[column].cat.codes\n    # stage for sampling\n    data_stage = pd.DataFrame(pd.concat([data_df_num, data_df_obj],axis=1))\n    \n    \n    return pd.DataFrame(pd.concat([data_df_num, data_df_obj],axis=1))\n\n","1da728b5":"#Train\ndata_train = data_prep_train(merge5)\n\n#Test\n#data_pred = pd.read_csv('application_test.csv', sep=',')\ndata_pred = data_prep_test(merge5_test)\n","294990b1":"data_train.head()","14e40a2e":"data_pred.head()","4ae7081b":"missing_values = [\"CNT_INSTALMENT\",\"CNT_INSTALMENT_FUTURE\",\"SK_DPD\",\"SK_DPD_DEF\",\"AMT_APPLICATION\",\n                  \"AMT_DOWN_PAYMENT\",\"NFLAG_LAST_APPL_IN_DAY\",\"RATE_DOWN_PAYMENT\",\"RATE_INTEREST_PRIMARY\",\n                  \"RATE_INTEREST_PRIVILEGED\",\"DAYS_DECISION\",\"SELLERPLACE_AREA\",\"CNT_PAYMENT\",\"DAYS_FIRST_DRAWING\",\n                  \"DAYS_FIRST_DUE\",\"DAYS_LAST_DUE_1ST_VERSION\",\"DAYS_LAST_DUE\",\"DAYS_TERMINATION\",\n                  \"NFLAG_INSURED_ON_APPROVAL\",\"AMT_INST_MIN_REGULARITY\",\"AMT_PAYMENT_CURRENT\",\n                  \"AMT_PAYMENT_TOTAL_CURRENT\",\"AMT_RECEIVABLE_PRINCIPAL\",\"AMT_RECIVABLE\",\"AMT_TOTAL_RECEIVABLE\",\n                  \"CNT_DRAWINGS_ATM_CURRENT\",\"CNT_DRAWINGS_CURRENT\",\"CNT_DRAWINGS_OTHER_CURRENT\",\n                  \"CNT_DRAWINGS_POS_CURRENT\",\"CNT_INSTALMENT_MATURE_CUM\",\"AMT_BALANCE\",\"AMT_CREDIT_LIMIT_ACTUAL\",\n                  \"AMT_DRAWINGS_ATM_CURRENT\",\"AMT_DRAWINGS_CURRENT\",\"AMT_DRAWINGS_OTHER_CURRENT\",\"AMT_DRAWINGS_POS_CURRENT\" ]\n","7dac600b":"data_train = data_train.drop(missing_values, axis=1)","4555dbe9":"data_pred = data_pred.drop(missing_values, axis = 1 )","c8518ae0":"(data_train.isna().sum()>1).sum(), (data_pred.isna().sum()>1).sum()","22cd721c":"data_train.shape, data_pred.shape","a4fd98c4":"# train test split doesnt actually split\n# Will be training on 95% of the data and not 100%, because then i will have to rewrite part of\n# this code and i am lazy.\nX_train, X_test, y_train, y_test = train_test_split(data_train.drop(['TARGET'],axis = 1),\n                                                    data_train['TARGET'], test_size = 0.05,\n                                                    random_state = seed)\nX_test_final = data_pred\n\n\n# Using smote to increase the number of under-represented class\nsm = SMOTE(random_state = seed, ratio = 'minority')\n\n","ce705ccc":"X_train_sm, y_train_sm = sm.fit_sample(X_train, y_train)\nX_train_sm.shape, y_train_sm.shape","9530e9a4":"import lightgbm as lgb\ndef run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"binary\",\n        \"metric\" : \"auc\",\n        \"num_leaves\" : 40,\n        \"learning_rate\" : 0.005,\n        #\"bagging_fraction\" : 0.6,\n        \"feature_fraction\" : 0.6,\n        \"bagging_frequency\" : 6,\n        \"bagging_seed\" : 42,\n        \"verbosity\" : -1,\n        \"seed\": 42\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lgb.train(params, lgtrain, 5000, \n                      valid_sets=[lgtrain, lgval], \n                      early_stopping_rounds=100, \n                      verbose_eval=150, \n                      evals_result=evals_result)\n    \n    pred_test_y = (model.predict(test_X, num_iteration=model.best_iteration))\n    return pred_test_y, model, evals_result","3a819cf0":"# Training LGB with SMOTE\n#pred_test_SMOTE, model, evals_result = run_lgb(X_train_sm, y_train_sm, X_test, y_test, data_pred)\n#print(\"LightGBM with SMOTE Training Completed ...\")","be25307e":"# Training LGB without sampling\npred_test, model, evals_result = run_lgb(X_train, y_train ,X_test, y_test, data_pred)\nprint(\"LightGBM Training Completed...\")","616ce2ce":"sub = pd.read_csv('..\/input\/sample_submission.csv')\n\n#sub_lgb_SMOTE = pd.DataFrame()\n#sub_lgb_SMOTE[\"TARGET\"] = pred_test_SMOTE\n\nsub_lgb = pd.DataFrame()\nsub_lgb[\"TARGET\"] = pred_test\n\n# The SMOTE version is overfitting. \n# This is evidenced by the difference in train test you can check it out\n#sub[\"TARGET\"] = (sub_lgb_SMOTE[\"TARGET\"])* 0.0 + sub_lgb[\"TARGET\"] * 1.0)\n\nsub[\"TARGET\"] = sub_lgb[\"TARGET\"]","8db0e3a5":"print(sub.head())\nsub.to_csv('submission_lazy.csv', index=False)","6381d959":"0.74429","c95a5f89":"The point of this kernel is to show that you can sometimes get an acceptable baseline model without digging deep into the data. I am in no way encouraging this way of doing data science, because it is not half as much fun :)\nI am so impressed by the calibre of people in this community. I just wanted to share something quick and easy. If you like this kernel, please upvote. If you dont like it, then definetely comment, that way I like so many of you can be a better DS tomorrow, than I am today. Cheers to us.","b5b28955":"Drop all columns which has NA's, these will be categorical NA's as we have handled numerical NAs already","e29adfd0":"No missing values now :)","254e0569":"Drop primary key and get ready for joins post grouping by 'SK_ID_CURR'","02fdbc26":"This function cleans and does some basic operation to get the model ready for scoring, feel free to improve on it.","ef00f42f":"<h1><center><font size=\"6\">Home Credit Default Risk baseline model<\/font><\/center><\/h1>\n> ****\n\nMany people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this population is often taken advantage of by untrustworthy lenders.\n\nHome Credit Group\n\nHome Credit strives to broaden financial inclusion for the unbanked population by providing a positive and safe borrowing experience. In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.\n\nWhile Home Credit is currently using various statistical and machine learning methods to make these predictions, they're challenging Kagglers to help them unlock the full potential of their data. Doing so will ensure that clients capable of repayment are not rejected and that loans are given with a principal, maturity, and repayment calendar that will empower their clients to be successful.","48d9e634":"<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/home-credit\/about-us-home-credit.jpg\"><\/img>","9ea5cdcb":"Merge both train and test files for final prediction","007f4894":"> Go ahead and check why i didnt use SMOTE in the final model scoring :)","ac384493":"This model scores about 0.74429 without almost any feature engineering in the public leaderboard","1d6b8522":"Checking if SMOTE is better than no SMOTE, if you dont know what SMOTE is google it, sometimes for some data sets it is awesome. But then again there is no free meal, you got to try everything. Now that i think of it, i could have also tried downsampling, but personally I dont like throwing away data. But for some data sets down sampling could be the key. But as i have said before, this is the lazy approach ;)"}}