{"cell_type":{"f9f18205":"code","b6b9914b":"code","65589dba":"code","2ba20ea2":"code","d8deca70":"code","47ee1dc4":"markdown","1003ad7b":"markdown"},"source":{"f9f18205":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf # data pipeline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b6b9914b":"def decode(serialized_example):\n    \"\"\" Parses a set of features and label from the given `serialized_example`.\n        \n        It is used as a map function for `dataset.map`\n\n    Args:\n        serialized_example (tf.Example): A serialized example containing the\n            following features:\n                \u2013 sensor_feature_0 \u2013 [int64]\n                \u2013 sensor_feature_1 \u2013 [int64]\n                \u2013 sensor_feature_2 \u2013 [int64]\n                \u2013 sensor_feature_3 \u2013 [int64]\n                \u2013 sensor_feature_4 \u2013 [int64]\n                \u2013 sensor_feature_5 \u2013 [int64]\n                \u2013 sensor_feature_6 \u2013 [int64]\n                \u2013 sensor_feature_7 \u2013 [int64]\n                \u2013 sensor_feature_8 \u2013 [int64]\n                \u2013 sensor_feature_9 \u2013 [int64]\n                \u2013 label_feature    \u2013 int64\n        is_test (bool, optional): Whether to allow for the label feature\n        \n    Returns:\n        A decoded tf.data.Dataset object representing the tfrecord dataset\n    \"\"\"\n    # Defaults are not specified since both keys are required.\n    feature_dict = {\n        'sensor_feature_0': tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True),\n        'sensor_feature_1': tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True),\n        'sensor_feature_2': tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True),\n        'sensor_feature_3': tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True),\n        'sensor_feature_4': tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True),\n        'sensor_feature_5': tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True),\n        'sensor_feature_6': tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True),\n        'sensor_feature_7': tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True),\n        'sensor_feature_8': tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True),\n        'sensor_feature_9': tf.io.FixedLenSequenceFeature(shape=(), dtype=tf.int64, allow_missing=True),\n        'label_feature': tf.io.FixedLenFeature(shape=(), dtype=tf.int64),\n        }\n    \n  \n    # Define a parser\n    features = tf.io.parse_single_example(serialized_example, features=feature_dict)\n        \n    # Decode the data and capture the label feature\n    sensors = [tf.cast(features[f\"sensor_feature_{i}\"], tf.int16) for i in range(10)]\n    \n    label = tf.cast(features[\"label_feature\"], tf.int32)\n    return sensors, label\n\ndef get_tfrecord_ds(tfrecord_dir):\n    tfrecord_paths = [os.path.join(tfrecord_dir, f_name) \\\n                      for f_name in os.listdir(tfrecord_dir) \\\n                      if f_name.endswith('.tfrec')]\n    return tf.data.TFRecordDataset(tfrecord_paths)","65589dba":"train_ds = get_tfrecord_ds(\"..\/input\/ingv-volcanic-eruption-training-tfrecords\/train\")\nval_ds = get_tfrecord_ds(\"..\/input\/ingv-volcanic-eruption-training-tfrecords\/val\")\n\ntrain_ds = train_ds.map(decode)\nval_ds = val_ds.map(decode)\n\nprint(train_ds, val_ds)","2ba20ea2":"print(\"\\n\u2013\u2013 FOR TRAIN DATASET \u2013\u2013\\n\")\nfor x,y in train_ds.take(1):\n    print(f\"INPUTS:\\n\\t{x.numpy()}\")\n    print(f\"\\n\\nLABELS:\\n\\t{y.numpy()}\")\n    \nprint(\"\\n\\n\\n\u2013\u2013 FOR VAL DATASET \u2013\u2013\\n\")\nfor x,y in val_ds.take(1):\n    print(f\"INPUTS:\\n\\t{x.numpy()}\")\n    print(f\"\\n\\nLABELS:\\n\\t{y.numpy()}\")","d8deca70":"def _bytes_feature(value, is_list=False):\n    \"\"\"Returns a bytes_list from a string \/ byte.\"\"\"\n    if isinstance(value, type(tf.constant(0))):\n        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n    \n    if not is_list:\n        value = [value]\n    \n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\ndef _float_feature(value, is_list=False):\n    \"\"\"Returns a float_list from a float \/ double.\"\"\"\n        \n    if not is_list:\n        value = [value]\n        \n    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n\ndef _int64_feature(value, is_list=False):\n    \"\"\"Returns an int64_list from a bool \/ enum \/ int \/ uint.\"\"\"\n        \n    if not is_list:\n        value = [value]\n        \n    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef serialize(sensor_features, label_feature):\n    \"\"\"\n    Creates a tf.Example message ready to be written to a file from 4 features.\n\n    Args:\n        sensor_features (numpy array): Each row represents the following\n            sensor_feature_0 (numpy array of integer values): TBD\n            sensor_feature_1 (numpy array of integer values): TBD\n            sensor_feature_2 (numpy array of integer values): TBD\n            sensor_feature_3 (numpy array of integer values): TBD\n            sensor_feature_4 (numpy array of integer values): TBD\n            sensor_feature_5 (numpy array of integer values): TBD\n            sensor_feature_6 (numpy array of integer values): TBD\n            sensor_feature_7 (numpy array of integer values): TBD\n            sensor_feature_8 (numpy array of integer values): TBD\n            sensor_feature_9 (numpy array of integer values): TBD\n        label_feature (int, optional)  : TBD    \n    Returns:\n        A tf.Example Message ready to be written to file\n    \"\"\"\n\n    # Create a dictionary mapping the feature name to the \n    # tf.Example-compatible data type.\n    feature = {\n        'sensor_feature_0': _int64_feature(sensor_features[:, 0], is_list=True),\n        'sensor_feature_1': _int64_feature(sensor_features[:, 1], is_list=True),\n        'sensor_feature_2': _int64_feature(sensor_features[:, 2], is_list=True),\n        'sensor_feature_3': _int64_feature(sensor_features[:, 3], is_list=True),\n        'sensor_feature_4': _int64_feature(sensor_features[:, 4], is_list=True),\n        'sensor_feature_5': _int64_feature(sensor_features[:, 5], is_list=True),\n        'sensor_feature_6': _int64_feature(sensor_features[:, 6], is_list=True),\n        'sensor_feature_7': _int64_feature(sensor_features[:, 7], is_list=True),\n        'sensor_feature_8': _int64_feature(sensor_features[:, 8], is_list=True),\n        'sensor_feature_9': _int64_feature(sensor_features[:, 9], is_list=True),\n    }\n\n    feature['label_feature']=_int64_feature(label_feature, is_list=False)\n\n    # Create a Features message using tf.train.Example.\n    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n    return example_proto.SerializeToString()\n\n\ndef generate_train_tf_records(master_df, train_paths, output_dir, per_shard=80):\n    def __get_fname_and_label(master_df):\n        for x in master_df.iterrows():\n            yield str(x[1][0]), x[1][1]\n    \n    # Validate output directory exists\n    if not os.path.isdir(output_dir):\n        os.makedirs(os.path.join(output_dir, \"train\"), exist_ok=True)\n        os.makedirs(os.path.join(output_dir, \"val\"), exist_ok=True)\n\n    # Create generator\n    fname_label_gen = __get_fname_and_label(master_df)\n\n    # Compute the number of required shards\n    n_shards = math.ceil(len(train_paths)\/per_shard)\n\n    # Loop one shard at a time and write {per_shard} df's to the tfrecord\n    for i in tqdm(range(n_shards), total=n_shards):\n        # Set the filename using a prefix and a n of m count schema\n        if i!=(n_shards-1):\n            tfrec_f_name = os.path.join(output_dir, \"train\",\n                                        f\"ingv_{i+1:03d}-of-{n_shards:03d}.tfrec\")\n        else:\n            tfrec_f_name = os.path.join(output_dir, \"val\",\n                                        f\"ingv_{1:03d}-of-{1:03d}.tfrec\")\n        with tf.io.TFRecordWriter(tfrec_f_name) as writer:\n            for j in tqdm(range(per_shard), total=per_shard):\n                \n                # Get the filename required for the dataframe and the label\n                f_name, lbl = next(fname_label_gen)\n                \n                # Find the correct path\n                ex_path = train_paths[[True if f_name in path \\\n                                       else False \\\n                                       for path in train_paths].index(True)]\n\n                # Open the csv as a df and fill nan values with 0\n                # prior to casting it as a numpy array of dtype int16\n                # and passing it to the serialize fn along with the label\n                #\n                # This results in a generated tfExample\n                ex = pd.read_csv(ex_path, dtype=\"Int16\")\n                ex.fillna(0, inplace=True)\n                tf_example = serialize(sensor_features=ex.to_numpy().astype(np.int16), label_feature=lbl)\n\n                # Write the generated tfExample into the open tfrecord file\n                writer.write(tf_example)\n                \ndef generate_test_tf_records(test_paths, output_dir, per_shard=80):\n    \n    # Validate output directory exists\n    if not os.path.isdir(output_dir):\n        os.makedirs(os.path.join(output_dir, \"test\"), exist_ok=True)\n\n    # Compute the number of required shards\n    n_shards = math.ceil(len(test_paths)\/per_shard)\n\n    # Loop one shard at a time and write {per_shard} df's to the tfrecord\n    for i in tqdm(range(n_shards), total=n_shards):\n        # Set the filename using a prefix and a n of m count schema\n        tfrec_f_name = os.path.join(output_dir, \"test\",\n                                    f\"ingv_{i+1:03d}-of-{n_shards:03d}.tfrec\")\n\n        with tf.io.TFRecordWriter(tfrec_f_name) as writer:\n            for j in tqdm(range(per_shard), total=per_shard):\n                \n                # Exit before error\n                if i*per_shard+j==4520:\n                    break\n                \n                # Find the correct path\n                ex_path = test_paths[i*per_shard+j]\n                f_name = int(ex_path.rsplit(\"\/\", 1)[1][:-4])\n                \n                # Open the csv as a df and fill nan values with 0\n                # prior to casting it as a numpy array of dtype int16\n                # and passing it to the serialize fn along with the label\n                #\n                # This results in a generated tfExample\n                ex = pd.read_csv(ex_path, dtype=\"Int16\")\n                ex.fillna(0, inplace=True)\n                tf_example = serialize(sensor_features=ex.to_numpy().astype(np.int16),\n                                       label_feature=f_name)\n\n                # Write the generated tfExample into the open tfrecord file\n                writer.write(tf_example)","47ee1dc4":"# Create a tf.data.Dataset From TFRecord Files","1003ad7b":"# Appendix: Functions Used to Create the TFRecords"}}