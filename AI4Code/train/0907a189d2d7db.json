{"cell_type":{"f54d688e":"code","c6a9c87b":"code","02b1e317":"code","cea2af23":"code","067acdfa":"code","fd74e79c":"code","ba50077d":"code","0b814e90":"code","c04e5796":"code","1d1a1dc7":"code","d481a997":"code","0aa4e4b9":"code","763018ed":"code","e1023753":"code","828465e6":"code","ccd4e469":"code","9739d534":"markdown","6f663ce8":"markdown","69debbc2":"markdown","d301ee0f":"markdown","598efefa":"markdown"},"source":{"f54d688e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6a9c87b":"#importing basic liberaries\nimport pandas as pd \nimport plotly.express as px\nimport seaborn as sns\nimport numpy as np","02b1e317":"# Loading the data\ndf = pd.read_csv('\/kaggle\/input\/social-network-ads\/Social_Network_Ads.csv')","cea2af23":"#checking top 5 rows\ndf.head(5)","067acdfa":"# Getting the size of data\ndf.shape","fd74e79c":"df.info()","ba50077d":"# Pairplot\nfig = sns.pairplot(df);","0b814e90":"# heatmap\nsns.heatmap(df.corr(), annot = True)","c04e5796":"# Purchased \npur = pd.crosstab(index = df['Purchased'], columns = 'Count')\npur.plot.bar();","1d1a1dc7":"# age and purchased\nfig = px.histogram(df, x = 'Age', color = 'Purchased')\nfig.show()\nfig2 = px.box(df, x = 'Age', color = 'Purchased')\nfig2.show()","d481a997":"# Salary and purchased\nfig = px.histogram(df, x = df['EstimatedSalary'], color = 'Purchased')\nfig.show()\nfig2 = px.box(df, x = df['EstimatedSalary'], color = 'Purchased')\nfig2.show()","0aa4e4b9":"# Getting Features\nfeature = ['Age', 'EstimatedSalary']\nx = df[feature]\n\n# getting predicting value\ny = df['Purchased']","763018ed":"# Splitting trainning and testing data\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y, test_size = 1\/9, random_state = 252)","e1023753":"# Scaling the data\nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nx_train = scale.fit_transform(x_train)\nx_test = scale.fit_transform(x_test)","828465e6":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier, Perceptron, RidgeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC,NuSVC\nfrom sklearn.neighbors import KNeighborsClassifier, NearestCentroid\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.metrics import precision_score\n\nmodels =[(\"LR\", LogisticRegression()),(\"SVC\", SVC()),('KNN',KNeighborsClassifier()),(\"DTC\", DecisionTreeClassifier()),(\"GNB\", GaussianNB()),(\"SGDC\", SGDClassifier()),(\"Perc\", Perceptron()),(\"NC\",NearestCentroid()),(\"Ridge\", RidgeClassifier()),(\"NuSVC\", NuSVC()),(\"BNB\", BernoulliNB()),('RF',RandomForestClassifier()),('ADA',AdaBoostClassifier()),('XGB',GradientBoostingClassifier()),('PAC',PassiveAggressiveClassifier())]\npred = []\nnames = []\nmodelsprecision = []\n\nfor name,model in models:\n    model.fit(x_train, y_train)\n    prediction = model.predict(x_test)\n    score = precision_score(y_test, prediction,average = 'macro')\n    pred.append(score)\n    names.append(name)\n    modelsprecision.append((name,score))\n    \nmodelsprecision.sort(key=lambda k:k[1],reverse=True)","ccd4e469":"modelsprecision","9739d534":"# Preprocessing Data","6f663ce8":"**Visualization**","69debbc2":"# Creating Models","d301ee0f":"**Please Leave Your Valuable feedback in the comments**","598efefa":"# Summary\n* **AdaBoostClassifier** worked best here with an percision of **96.87%**\n* **Perceptron** was just behind it with the percision of **94.11%**"}}