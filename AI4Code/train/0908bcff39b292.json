{"cell_type":{"354f74aa":"code","d9e7bbd0":"code","14bb3d0c":"code","f0d7d64e":"code","956197a0":"code","b30b232b":"code","73063da3":"code","8b7d37d9":"code","798ef367":"code","4010a48f":"code","33be935c":"code","edcd83bb":"code","099c82c4":"code","b860c714":"code","2d4424f8":"code","6ee9cd85":"code","8a16ca2a":"code","53b4895b":"code","731856e4":"code","cc41d524":"code","276b01d6":"code","926fafee":"code","fccd588e":"code","8644c1c9":"code","2e0dc770":"code","dcfca4d6":"code","fedba462":"code","4325ae8c":"code","eb29f3c0":"code","ed7d5e7b":"code","ddedb3d0":"code","33aa1159":"code","482156e2":"code","e5a16c65":"code","be399142":"code","3ab6d695":"code","966c7b7b":"code","1fbfab72":"code","ed58225f":"code","04d72bfe":"code","f20f6bd2":"markdown","2dd55b78":"markdown","ba7861f3":"markdown","e575b04f":"markdown","98f7718a":"markdown","0b98f1db":"markdown","dc46a926":"markdown","3b5e05de":"markdown"},"source":{"354f74aa":"pip install researchpy","d9e7bbd0":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt \nfrom matplotlib.ticker import MaxNLocator           \n%matplotlib inline \nimport seaborn as sns   \n\nimport scipy.stats as ss\nfrom scipy import stats \nfrom scipy.stats import skew, boxcox_normmax, norm\nfrom scipy.stats import chi2_contingency\nfrom researchpy import crosstab\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","14bb3d0c":"train_data = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv')\ntest_data = pd.read_csv('..\/input\/loan-prediction-problem-dataset\/test_Y3wMUE5_7gLdaTN.csv')","f0d7d64e":"train_data[(train_data['Gender'].isnull())]","956197a0":"train_data.shape","b30b232b":"train_data.dtypes","73063da3":"train_data.describe()","8b7d37d9":"numerical_columns = train_data.dtypes[train_data.dtypes != 'object'].index\nnumerical_columns","798ef367":"categorical_columns = train_data.dtypes[train_data.dtypes == 'object'].index\ncategorical_columns","4010a48f":"# Target Variable is Loan Status\n# Type of problem is a classification problem","33be935c":"# Distribution Plot \/ Histogram\nfor col in numerical_columns:\n  print(\"Skewness of \", col,\": \" , train_data[col].skew());\n  print(\"Kurtosis of \",col,\": \" , train_data[col].kurtosis());\n  print(\"---------------------------\")\n  sns.set_style('white');\n  plt.figure();\n  sns.distplot(train_data[col], fit = norm); ","edcd83bb":"## Features such as ApplicantIncome & Coapplicant Income are highly skewed (positively), could be normalized using Log Transformation\n## LoanAmount is also positively skewed, can be normalized using Log Transformation","099c82c4":"# Box Plots\nfor col in numerical_columns:\n  sns.set_style('white');\n  plt.figure();\n  sns.boxplot(train_data[col]); ","b860c714":"print(train_data['Credit_History'].unique())\nprint(\"---\"*30)\nprint(train_data['Loan_Amount_Term'].unique())","2d4424f8":"## Feature Credit_History must be a categorical feature as it only holds values 1 & 0\n## Loan_Amount_Term can also be labeled as a categorical feature\n## CoapplicantIncome & ApplicantIncome are a highly disperesed features","6ee9cd85":"# Count Plots \nfor col in categorical_columns:\n  sns.countplot(train_data[col]); \n  plt.figure();","8a16ca2a":"## The data is biased\/imbalanced towards males, and most of the loans present in the dataset have been approved\n## Loan_ID is an unecessary feature that can be removed","53b4895b":"# Converting Loan_Amount_Term & Credit_History to categorical features\ntrain_data['Loan_Amount_Term'] = train_data['Loan_Amount_Term'].astype(object)\ntrain_data['Credit_History'] = train_data['Credit_History'].astype(object)","731856e4":"# Updating numerical and categorical columns\nnumerical_columns = train_data.dtypes[train_data.dtypes != 'object'].index\ncategorical_columns = train_data.dtypes[train_data.dtypes == 'object'].index","cc41d524":"# Removing Loan_ID from categorical features\ncategorical_columns = categorical_columns.drop('Loan_ID')","276b01d6":"def srt_reg(df):\n    for i in train_data[[col for col in numerical_columns]]:\n      for k in train_data[[col for col in numerical_columns[::-1]]]:\n        if i == k:\n          continue;\n        sns.regplot(x=i, y=k, data=df, color='#e74c3c', line_kws={'color': 'black'}, scatter_kws={'alpha':0.4})\n        plt.figure();\n     \nsrt_reg(train_data)","926fafee":"## LoanAmount & ApplicantIncome follow a linear relationship\n## Most of the CoapplicantIncome is 0, which must be handled as it makes our data biased, otherwise it is linearly related to LoanAmount","fccd588e":"print(\"Number of rows with 0 CoapplicantIncome: \", train_data.CoapplicantIncome.value_counts()[0])\nprint(\"Percentage:\", (train_data.CoapplicantIncome.value_counts()[0]\/train_data.CoapplicantIncome.count()*100))","8644c1c9":"# A new feature Has_CoapplicantIncome can be useful","2e0dc770":"# Segregating Nominal and Ordinal Columns\nnominal_columns = ['Gender', 'Married', 'Education', 'Self_Employed', 'Property_Area', 'Loan_Status', 'Credit_History']\nordinal_columns = ['Dependents', 'Loan_Amount_Term']","dcfca4d6":"# Cross Tabulation between Loan_Status and other categorical features\ndef cross_tab(col, df):\n  cat = df.dtypes[train_data.dtypes == 'object'].index\n  for i in cat:\n        if i == col:\n          continue;\n        print(pd.crosstab(index = train_data[i], columns = train_data[col], normalize='index'))\n        print(\"----\"*30)\n\ncross_tab('Loan_Status', train_data)","fedba462":"## Married applicants have greater chances for loan approval\n## Graduated applicants have greater approval chances\n## Applicants with credit history have greater approval chances\n## SemiUrban property holders have greater approval chances\n## Applicants with 2 Dependents have high chances for approval","4325ae8c":"# Performing Chi-Square Test between Loan_Status and other features\ndef chi_sq(col, df):\n  cat = df.dtypes[train_data.dtypes == 'object'].index\n  for i in cat:\n        if i == col:\n          continue;\n        cross = pd.crosstab(index = train_data[i], columns = train_data[col])\n        chisq_res = chi2_contingency(cross)\n        print(\"p - value for test between \", col, \" and \", i, \" is:\", chisq_res[1])\n        if chisq_res[1] > 0.05:\n          print(col, \" and \", i, \" are not correlated.\")\n        print(\"----\"*30)\n\nchi_sq('Loan_Status', train_data)","eb29f3c0":"## Loan_Status is not correlated with features such as Self_Employed, Loan_Amount_Term, Dependents and Gender","ed7d5e7b":"# Box Plots\ndef srt_box(df):\n    for i in df[[col for col in numerical_columns]]:\n      for k in df[[col for col in categorical_columns]]:\n        plt.figure(figsize=(10,8))\n        sns.boxplot( x=k, y=i, data=df)\n        plt.figure();\n\n\nsrt_box(train_data)","ddedb3d0":"## There are some male applicants which have high ApplicantIncome, which may be classified as outliers\n## Graduate Applicants have high ApplicantIncome\n## Self Employed applicants also have higher income\n## Some Female & Male co-applicants have exceptionally high CoapplicantIncome, therefore might be outliers\n## Male applicants have greater LoanAmount\n## Married applicants also have greater LoanAmount than Unmarried applicants\n## Loan Amount has a linear relationship with the number of Dependents, as number of dependents increase, loan amount increases\n## Graduate Applicants have high LoanAmount","33aa1159":"null_value_train = pd.DataFrame(train_data[[col for col in categorical_columns]].isnull().sum()).reset_index()\nnull_value_train = null_value_train.rename(columns = {'index': 'Column Name', 0: 'Number of Null Values'}, inplace = False)\nnull_value_train['Percentage of Null Values'] = (null_value_train['Number of Null Values']\/len(train_data)*100) \nnull_value_train.sort_values(by = 'Percentage of Null Values', ascending = False).head(10)","482156e2":"columns_fillnone = ['Credit_History', 'Self_Employed', 'Dependents', 'Loan_Amount_Term', 'Gender', 'Married']\n\nfor col in columns_fillnone:\n    train_data[col] = train_data[col].fillna(train_data[col].mode()[0])","e5a16c65":"null_value_train = pd.DataFrame(train_data[[col for col in categorical_columns]].isnull().sum()).reset_index()\nnull_value_train = null_value_train.rename(columns = {'index': 'Column Name', 0: 'Number of Null Values'}, inplace = False)\nnull_value_train['Percentage of Null Values'] = (null_value_train['Number of Null Values']\/len(train_data)*100) \nnull_value_train.sort_values(by = 'Percentage of Null Values', ascending = False).head()","be399142":"null_value_train = pd.DataFrame(train_data[[col for col in numerical_columns]].isnull().sum()).reset_index()\nnull_value_train = null_value_train.rename(columns = {'index': 'Column Name', 0: 'Number of Null Values'}, inplace = False)\nnull_value_train['Percentage of Null Values'] = (null_value_train['Number of Null Values']\/len(train_data)*100) \nnull_value_train.sort_values(by = 'Percentage of Null Values', ascending = False).head()","3ab6d695":"# Null Values in LoanAmount\n## Loan amount is directly proportional with ApplicantIncome\n## Loan amount is directly proportional with number of dependents\n## Married couples have high LoanAmount\n## Male Applicants have higher LoanAmount\n## Graduate Applicants have hight LoanAmount\nnull_loanamt = train_data[train_data['LoanAmount'].isnull()]","966c7b7b":"# ChiSquared for LoanAmount\ndef cramers_v(x, y):\n    import scipy.stats as ss\n    confusion_matrix = pd.crosstab(x,y)\n    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n    n = confusion_matrix.sum().sum()\n    phi2 = chi2\/n\n    r,k = confusion_matrix.shape\n    phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n    rcorr = r-((r-1)**2)\/(n-1)\n    kcorr = k-((k-1)**2)\/(n-1)\n    return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\n\n\n\nfor i in categorical_columns: \n  print(i, \":\")\n  print(cramers_v(train_data['LoanAmount'], train_data[i]))\n  print(\"---\"*15)\n\nprint(\"***\"*30)\n\nfor i in numerical_columns: \n  print(i, \":\")\n  print(cramers_v(train_data['LoanAmount'], train_data[i]))\n  print(\"---\"*15)","1fbfab72":"## LoanAmount is correlated strongly with Credit_History, Dependents, Applicant Income and Property_Area","ed58225f":"train_data['LoanAmount'] = train_data.groupby('Dependents')['LoanAmount'].transform(lambda x: x.fillna(x.median()))","04d72bfe":"null_value_train = pd.DataFrame(train_data[[col for col in numerical_columns]].isnull().sum()).reset_index()\nnull_value_train = null_value_train.rename(columns = {'index': 'Column Name', 0: 'Number of Null Values'}, inplace = False)\nnull_value_train['Percentage of Null Values'] = (null_value_train['Number of Null Values']\/len(train_data)*100) \nnull_value_train.sort_values(by = 'Percentage of Null Values', ascending = False).head()","f20f6bd2":"# Univariate Analysis: Categorical Features","2dd55b78":"# Univariate Analysis: Numerical Features","ba7861f3":"# Handling Missing Values: Categorical Features\n","e575b04f":"# Bi-Variate Analysis: Continuous vs. Categorical","98f7718a":"# Variable Identification\n","0b98f1db":"# Bi-Variate Analysis: Categorical vs. Categorical","dc46a926":"# Handling Missing Values: Numerical Features","3b5e05de":"# Bi-Variate Analysis: Continuous vs. Continuous "}}