{"cell_type":{"031a20ce":"code","a22449fb":"code","3dc2531e":"code","54d59d9d":"code","79afd2e9":"code","61d6d1c3":"code","b1eeedc8":"code","723df459":"code","1a29d1e6":"code","06474db1":"code","06415f01":"code","ee667ff5":"code","65f1c996":"code","45781a7c":"code","85fb1eed":"code","f0cb3c5a":"code","936bab63":"code","77e25336":"code","123a8210":"code","8ef0b208":"code","4ad89990":"markdown","46479a02":"markdown","3e8324a8":"markdown","2ae2f4d8":"markdown","e317607b":"markdown","c181d15d":"markdown","85c19152":"markdown","2a8225b8":"markdown","e1ee8234":"markdown","bdf23cdb":"markdown","7e5f5708":"markdown","a06a0c9a":"markdown","392fc82a":"markdown","6efdfa66":"markdown","53307e5c":"markdown","ee510f47":"markdown","e95bc1e7":"markdown","e9d3c671":"markdown","e227682f":"markdown","82c6b27f":"markdown","420309d3":"markdown","8d71980a":"markdown","24c1fdf7":"markdown","20594d96":"markdown","9cf0fdc2":"markdown"},"source":{"031a20ce":"# library import\nimport pandas as pd \nimport numpy as np \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport warnings \nfrom scipy.stats import norm\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\ndef coef_plot(model, X, abs_ = False, n = 5):\n    if abs_ == False:\n        coefs = pd.DataFrame({'name': X.columns, 'coef' : model.coef_}).sort_values('coef', ascending = False)\n    else:\n        coefs = pd.DataFrame({'name': X.columns, 'coef' : np.abs(model.coef_)}).sort_values('coef', ascending = False)\n    plt.figure(figsize = (16, 8))\n    sns.pointplot(y=\"name\", x=\"coef\",\n                  data=coefs.head(n), ci=None, color = 'C0')\n    sns.barplot(y = \"name\", x= \"coef\", data=coefs.head(n), ci=None, color = 'C0', alpha = 0.2)\n    plt.title('Coeficient Plot')\n    plt.tight_layout()","a22449fb":"# Use pandas to read in CSV files\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n# row and column number in train set \nprint(\"train rows amd columns\", train.shape)\nprint(\"test rows amd columns\", test.shape)","3dc2531e":"# plot histograms of all the numeric columns\ntrain.hist(figsize = (12, 7), bins = 40)\nplt.tight_layout()\nplt.show()","54d59d9d":"plt.figure(figsize = (12, 6))\ntrain.skew().sort_values().plot(kind = 'bar', color = 'C0')\nplt.title('Skew of Variables')","79afd2e9":"fig, ax = plt.subplots(2, 1, figsize = (12, 8)) \n\nsns.distplot(train['SalePrice'], fit = norm, ax = ax[0])\nsns.distplot(np.log(train['SalePrice']), fit = norm, ax = ax[1])\nplt.show()","61d6d1c3":"ax = sns.clustermap(train.corr(method='spearman'), center = np.median(train.corr(method='spearman')))\nplt.show()","b1eeedc8":"plt.figure(figsize = (12, 8))\ncormat = train.corr(method = 'spearman')[['SalePrice']].sort_values(by = 'SalePrice', ascending = True)[:-1]\nsns.heatmap(cormat, annot = True, center = np.median(cormat))\nplt.title(\"Numeric Variable's Correaltion with Sale Price\")\nplt.show()","723df459":"train.select_dtypes('object').describe()","1a29d1e6":"plt.figure(figsize = (12, 6))\ntrain['Neighborhood'].value_counts().plot(kind = 'barh', color = 'C0')\nplt.xlabel('Count')\nplt.title(\"Woah! That's a Lot of Neighborhoods\")\nsns.despine()\nplt.show()","06474db1":"for col in train.select_dtypes('object').columns:\n    top = train[col].value_counts().head(10)\n    train[col] = [x if x in top else \"other\" for x in train[col]]\n    test[col] = [x if x in top else \"other\" for x in test[col]]","06415f01":"train.isna().sum()[train.isna().sum() > 0].sort_values().plot(kind = 'barh', color = 'C0', figsize = (10, 4))\nplt.title('Missing Values')\nplt.show()","ee667ff5":"print(pd.get_dummies(train).shape)\nprint(pd.get_dummies(test).shape)","65f1c996":"from sklearn.linear_model import Lasso\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.impute import SimpleImputer\n\ny = train['SalePrice']\nX = train.drop(['SalePrice'], 1)\n\ntrain_objs_num = len(X)\ndataset = pd.concat([X, test])\ndataset = pd.get_dummies(dataset)\nX = dataset[:train_objs_num].copy()\ntest = dataset[train_objs_num:].copy()","45781a7c":"import warnings \nfrom sklearn.exceptions import ConvergenceWarning\n\n\n# these warnings are telling me that some of the alpha values are too low, I know this. I am attempting to do a wide grid search.\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning,)\n\nalphas = []\nscores = []\n\nfor i in [1e-6, 1e-4, 1e-2,.1, 1, 5, 10, 20, 30, 50, 100,200,250, 300,400,500,750,1000]:\n    \n    pipe = Pipeline([\n                 ('Imputer', SimpleImputer(strategy = 'most_frequent')),\n                 ('scaler', RobustScaler()),\n                ('lasso', Lasso(alpha= i, max_iter= 10000))\n            ])\n    pipe.fit(X, y)\n    score = cross_val_score(pipe, X, y, cv = 5)\n    # nested loops in python are really ugly and should be avoided by I'm lazy\n    for x in score:\n        scores.append(x)\n        alphas.append(i)\nridge_frame = pd.DataFrame({'alpha': alphas, 'score': scores})\n\ntop_alpha = ((ridge_frame.groupby('alpha', as_index=False).mean().sort_values('score')))\n\ntop_alpha = top_alpha.iloc[-1:, 0].values\nprint(top_alpha[0])","85fb1eed":"plt.figure(figsize = (12, 8))\nsns.lineplot('alpha', 'score', data = ridge_frame, ci = \"sd\")\nplt.title('Crossvalidated Alpha vs R^2 Score + or - 1 sd')\nplt.axvline(top_alpha, color = \"C1\")\nplt.xscale('log')\nplt.show()","f0cb3c5a":"pipe = Pipeline([\n                  ('Imputer', SimpleImputer(strategy = 'most_frequent')),  \n                 ('scaler', RobustScaler()),\n                ('lasso', Lasso(alpha= top_alpha))\n            ])\npipe.fit(X, y)\nscore = cross_val_score(pipe, X, y, cv = 5)\n\nrcv = pipe.named_steps['lasso']\n\ncoef_plot(rcv, X, abs_ = True, n = 40)\nprint(np.mean(scores))\nprint(np.std(scores))\nplt.show()","936bab63":"from sklearn.linear_model import Ridge\nalphas = []\nscores = []\n\nfor i in [1e-6, 1e-4, 1e-2,.1, 1, 5, 10, 20, 30, 50, 100,200,250, 300,400,500,750, 1000,1500, 2000,2500, 3000, 5000]:\n    \n    pipe = Pipeline([\n                 ('Imputer', SimpleImputer(strategy = 'most_frequent')),\n                 ('scaler', RobustScaler()),\n                ('ridge', Ridge(alpha= i, max_iter= 10000))\n            ])\n    pipe.fit(X, y)\n    score = cross_val_score(pipe, X, y, cv = 5)\n    # nested loops in python are really ugly and should be avoided by I'm lazy\n    for x in score:\n        scores.append(x)\n        alphas.append(i)\nridge_frame = pd.DataFrame({'alpha': alphas, 'score': scores})\n\ntop_alpha = ((ridge_frame.groupby('alpha', as_index=False).mean().sort_values('score')))\n\ntop_alpha = top_alpha.iloc[-1:, 0].values","77e25336":"pipe = Pipeline([\n                  ('Imputer', SimpleImputer(strategy = 'most_frequent')),  \n                 ('scaler', RobustScaler()),\n                ('ridge', Ridge(alpha= top_alpha))\n            ])\npipe.fit(X, y)\nscore = cross_val_score(pipe, X, y, cv = 5)\n\nrcv = pipe.named_steps['ridge']\n\ncoef_plot(rcv, X, abs_ = True, n = 40)\nprint(np.mean(scores))\nprint(np.std(scores))\nplt.show()","123a8210":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\n\npipe = Pipeline([('imp', SimpleImputer(strategy = 'most_frequent')),\n                 ('scaler', RobustScaler()),\n                ('rf', RandomForestRegressor())\n            ])\npipe.fit(X, y)\nparams_rf = {'rf__n_estimators' : [100, 350, 500, 1000, 2000],\n            'rf__max_features': ['log2', 'auto', 'sqrt'],\n            'rf__min_samples_leaf': [1, 2, 5, 10, 30],\n            \"rf__min_samples_split\": [2, 3, 5, 7,9,11,13,15,17],\n            \"rf__max_depth\" : [None, 1, 3, 5, 7, 9, 11]}\n\n# Import GridSearchCV\n\n# Instantiate grid_rf\ngrid_rf = RandomizedSearchCV(estimator=pipe,\n                       param_distributions=params_rf,\n                       cv=5,\n                       verbose=0,\n                       n_jobs=-1,\n                       n_iter = 30)\n\ngrid_rf.fit(X,y)\nbestmodel = grid_rf.best_estimator_\n\nprint(grid_rf.best_score_)","8ef0b208":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_log_error\n\npipe = Pipeline([('imp', SimpleImputer(strategy = 'most_frequent')),\n                 ('scaler', RobustScaler()),\n                ('gbm', GradientBoostingRegressor())\n            ])\n\npipe.fit(X, y)\nparams_gbm = {'gbm__n_estimators' : [100, 350, 500, 1000, 2000],\n            'gbm__max_features': ['log2', 'auto', 'sqrt'],\n            'gbm__min_samples_leaf': [1, 2, 10, 30],\n            \"gbm__min_samples_split\": [2, 3,5,7,9,11],\n            \"gbm__learning_rate\" : [1e-5,1e-4, 1e-3, 1e-2, 0.1, 1]}\n# Import GridSearchCV\n\n# Instantiate grid_rf\ngrid_rf = RandomizedSearchCV(estimator=pipe,\n                       param_distributions=params_gbm,\n                       cv=5,\n                       verbose=0,\n                       n_jobs=-1, \n                       n_iter = 30)\n\ngrid_rf.fit(X,y)\nbestmodel = grid_rf.best_estimator_\nprint(grid_rf.best_score_)","4ad89990":"### Basic Models\n\nWe want to use our training data as efficiently as possible when fitting models.  To do that we will use a process called cross-validation.  \n\nI see many people reccomend to start off with OLS linear regression when modeling.  I disagree with this idea, especially when you already know that there is high correlation between input variables.  Lasso and ridge regression create linear models, similar to OLS regression, but with ridge and lasso regression we shrink coefficients to deal with a problem that arises in OLS regerssion called multi-colinearity.","46479a02":"### Histogram of all numeric variables:","3e8324a8":"The random forest performs a bit better than the linear models which tells me that the underlying relationship may be non-linear.  Lets try another non-linear estimator","2ae2f4d8":"### Visualize Correlation between numeric X variables and Outcome","e317607b":"In order to figure out the most important variables, I can take the absolute value of the coefficients.","c181d15d":"* A lot of skewed \/ non-normal distributions here!\n\n* Skewed vairables can be a problem for some learners, particularly linear regression","85c19152":"### Examine Missing Values","2a8225b8":"In the chunk below I create dummies for the train and test set and define the X matrix and y vector.","e1ee8234":"It seems that an alpha of 200 produces the highest cross validation accuracy","bdf23cdb":"### Data import","7e5f5708":"Sklearn does have a LassoCV object that performs quick easy crossvalidation to select a top alpha value. Unfortunatley unless you use loo cross validation, it doesn't save the scores. Because of this, I ussually tune the alpha parameter for ridge regerssion with a loop like this. This way I can make a plot of how the model metric changes with different values of lambda","a06a0c9a":"Lets try a random forest regressor to compare performance","392fc82a":"* There is a high correlation between many of our predictors, this can be a problem for some learners, particularly linear regression\n\n* High correlation between variables causes multi-colinearity\n","6efdfa66":"We need to one hot econde our categorical variables. There is a problem that we have to deal with. If we onehot encode the two datasets seperately, the test set will have less columns, because some of the unique categories in the train set don't show up in the test set. The easiest way to deal with this is to put them together to onehot encode the matrix.  This does have the potential to introduce leakage into some problems (I'd argue it doesn't here).  ","53307e5c":"* Outcome somewhat right skewed\n\n* A log transformation might help with this","ee510f47":"## Lasso Regression","e95bc1e7":"The lasso regression outperforms the ridge, which tells me that some feature selection may be necessary. (Lasso shrinks some coefficients to 0)","e9d3c671":"# Housing Regression in Python\n","e227682f":"* Some of these variables have a lot of unique values, I should probably get rid of some of the unique values with few observations in the columns that are high cardinaltiy (have a lot of unique values), especially the Neighborhood column.","82c6b27f":"* A lot of missing values values missing in PoolQC, this is probably because alot of houses don't have pools","420309d3":"### Categorical variables\n\n* When we one-hot encode a column, we create a column for each unique value of the column\n\n* If we use the describe function with just object type columns we can get some summary stats","8d71980a":"Note: We won't have any missing values in the categorical columns they will have already been replaced with the string \"other\". I have found in practice this actually works quite well and solves a lot of problems that you can run into with categorical variables (especially in production).","24c1fdf7":"### Distribution of Our Outcome or y variable","20594d96":"lets get rid of some of these:","9cf0fdc2":"### First steps: \n\n### 1) Look at variable distributions \n\n### 2) look at correlations with our outcome variable\n"}}