{"cell_type":{"6c2f81e5":"code","83af5b2b":"code","f2fca457":"code","f92802df":"code","53eb0801":"markdown","5923c39e":"markdown","9a1e38bb":"markdown","acc0c024":"markdown","fe812f77":"markdown"},"source":{"6c2f81e5":"import csv\nimport numpy as np\nimport os\n\ndataset=[]\nlabels=[]\ntimewindow=10 # timesteps used for making an instance\nrawdata = os.scandir('..\/input\/3w-dataset\/3W\/')\nc=0\n\nfor entry in rawdata:\n    group=os.scandir('..\/input\/3w-dataset\/3W\/'+entry.name)\n    foldername='..\/input\/3w-dataset\/3W\/'+entry.name\n    for entry in group:\n        dataset.append(foldername+'\/'+entry.name)\n        labels.append(c)\n    c+=1\nprint(len(labels))\n\ndef img(a): #function to process extracted raw data\n    B=np.zeros((timewindow))\n    maxV=max(a)\n    if maxV>0:\n        B=np.asarray(a)\n    return(B)\n\ni=0 #extracting raw data\nfor row in dataset:\n    n=dataset[i]\n    with  open(n, newline='') as f:\n        reader = csv.reader(f)\n        p0,p1,t1,p2,t2=[],[],[],[],[]\n        ii=0\n        for row in reader:      \n            ii+=1\n            if ii>1 and ii<= timewindow +1:                \n                try:\n                    p0.append(float(row[1])) \n                except ValueError:\n                    p0.append(0) \n                try:\n                    p1.append(float(row[2])) \n                except ValueError:\n                    p1.append(0) \n                try:\n                    t1.append(float(row[3])) \n                except ValueError:\n                    t1.append(0)\n                try:\n                    p2.append(float(row[4])) \n                except ValueError:\n                    p2.append(0) \n                try:\n                    t2.append(float(row[5])) \n                except ValueError:\n                    t2.append(0) \n        Str=np.concatenate([[labels[i]],img(p0),img(p1),img(t1),img(p2),img(t2)])\n        if i==0:\n                rez=Str  \n        else:\n                rez=np.vstack((rez,Str))\n    i+=1\n\nnp.savetxt(\"1.csv\",rez, delimiter=\",\")        \n","83af5b2b":"import numpy as np\nimport csv\n\ndataset=[]\nwith open(r'.\/1.csv', newline='') as f:\n    reader = csv.reader(f)\n    for row in reader:\n        dataset.append(row)\n    \nnp.array(dataset,dtype=np.float32)\nnp.random.shuffle(dataset)\n\nnp.savetxt('data1.csv', dataset, fmt='%s',delimiter=',')\n\nfrom  tensorflow import keras\nfrom tensorflow.keras import layers\nfrom  tensorflow import math\n\n\nx=10 #size of sample in timesteps\ndatast=[]\nwith open(r'.\/data1.csv', newline='') as f:\n    reader = csv.reader(f)\n    for row in reader:\n        datast.append(row)\n        \ndataset=np.asarray(datast,dtype=float)\nlabels,p0,p1,t1,p2,t2=np.hsplit(dataset,[1,x+1,2*x+1,3*x+1,4*x+1])\nlabel=labels.flatten()\np0,p1,t1,p2,t2= p0\/np.max(p0), p1\/np.max(p1), t1\/np.max(t1), p2\/np.max(p2), t2\/np.max(t2) # here we rescale features with the dataset global maximum values, hence, absolute value feature is still preserved\np0_data=keras.Input(shape=(x,), name='p0')\np1_data=keras.Input(shape=(x,), name='p1')\nt1_data=keras.Input(shape=(x,), name='t1')\np2_data=keras.Input(shape=(x,), name='p2')\nt2_data=keras.Input(shape=(x,), name='t2')\n\nx1=layers.Dense(4*x, activation=\"relu\", use_bias=False)(p0_data)\nx2=layers.Dense(4*x, activation=\"relu\", use_bias=False)(p1_data)\nx3=layers.Dense(4*x, activation=\"relu\", use_bias=False)(t1_data)\nx4=layers.Dense(4*x, activation=\"relu\", use_bias=False)(p2_data)\nx5=layers.Dense(4*x, activation=\"relu\", use_bias=False)(t2_data)\n\nxx=layers.concatenate([x1,x2,x3,x4,x5])\n\nxx=layers.Dense(10*x,activation=\"relu\", use_bias=False)(xx)\n\ncatN=layers.Dense(9,activation=\"softmax\", use_bias=False, name='output')(xx)\n\nmodel = keras.Model(inputs=[p0_data,p1_data,t1_data,p2_data,t2_data], outputs=catN)\nmodel.summary()\nopt = keras.optimizers.Adam(learning_rate=0.005)\nmodel.compile(loss='sparse_categorical_crossentropy', \n              optimizer=opt,  metrics=[keras.metrics.SparseCategoricalAccuracy()])\nmodel.fit({'p0':p0,'p1':p1, 't1':t1, 'p2':p2,'t2':t2},{'output':label}, \n          epochs=200, validation_split=0.5)\n\n","f2fca457":"dataset=[]\nlabels=[]\n\npr=500# timesteps prior to event starting point\naf=500# timesteps after event starting point\nm=700 #size of sample in timesteps\nslide=100\ntimewindow=pr+af\nrawdata = os.scandir('..\/input\/3w-dataset\/3W\/')\nc=0\n\nfor entry in rawdata:\n    group=os.scandir('..\/input\/3w-dataset\/3W\/'+entry.name)\n    foldername='..\/input\/3w-dataset\/3W\/'+entry.name\n    for entry in group:\n        dataset.append(foldername+'\/'+entry.name)\n        labels.append(c)\n    c+=1\n\n\ndef img(a):\n    A=np.zeros((m+1))\n    for i in range(4): # cycle for extracting  4 samples from timewindow\n        startP=i*100\n        if i==0:\n            A=a[startP:startP+m]\n            maxV=max(A)\n            minV=min(A)\n            d=maxV-minV\n            for j in range(m):\n                A[j]=(A[j]-minV)\/d # erasing absolute value feature by rescaling\n            A.append(d\/maxV) # adding scale coefficient sv\n        else:\n            B=a[startP:startP+m]\n            maxV=max(B)\n            minV=min(B)\n            d=maxV-minV\n            for j in range(m):\n                B[j]=(B[j]-minV)\/d# erasing absolute value feature by rescaling\n            B.append(d\/maxV) # adding scale coefficient sv\n            A=np.vstack((A,B))\n    return(A)\n\n\ni=0\nfor row in dataset:\n    n=dataset[i]\n    with  open(n, newline='') as f:\n        reader = csv.reader(f)\n        p0,p1,t1,p2,t2,pd=[],[],[],[],[],[]\n        p00,p01,t01,p02,t02,pd=[],[],[],[],[],[]\n        ii=0\n        state=1 # flag that subsequently changes its value at event\u2019s starting point to the corresponding ii value\n        for row in reader:\n            ii+=1\n            if ii>1 and state>0:                \n                p00.append(1) if float(row[1]+'0')<=0 else p00.append(0)\n                p01.append(1) if float(row[2]+'0')<=0 else p01.append(0)\n                t01.append(1) if float(row[3]+'0')<=0 else t01.append(0) \n                p02.append(1) if float(row[4]+'0')<=0 else p02.append(0)\n                t02.append(1) if float(row[5]+'0')<=0 else t02.append(0)              \n                p0.append(float(row[1])*(1+(np.random.sample()*1-1)\/100)) if row[1]!='' else p0.append('NaN') # erasing signal #noise feature by adding random noise\n                p1.append(float(row[2])*(1+(np.random.sample()*1-1)\/100)) if row[2]!='' else p1.append('NaN') # erasing signal #noise feature by adding random noise\n                t1.append(float(row[3])*(1+(np.random.sample()*1-1)\/100)) if row[3]!='' else t1.append('NaN') # erasing signal #noise feature by adding random noise\n                p2.append(float(row[4])*(1+(np.random.sample()*1-1)\/100)) if row[4]!='' else p2.append('NaN') # erasing signal #noise feature by adding random noise\n                t2.append(float(row[5])*(1+(np.random.sample()*1-1)\/100)) if row[5]!='' else t2.append('NaN')     # erasing signal #noise feature by adding random noise          \n                if float('0'+row[9])>100 and state==1: state=ii #locating event\u2019s starting point in instance ( inner labeling of #timesteps is in the last column)\n                if ii-state>=(af-1) and state>1: state=0\n        if pr+af<=len(p0): #cropping raw data and centering the  starting  point. If an instance doesn\u2019t include the starting point #(classes 4 and 5) then the last 1000 timesteps of the instance are extracted \n            del p0[:-(pr+af)]\n            del p1[:-(pr+af)]\n            del t1[:-(pr+af)]\n            del p2[:-(pr+af)]\n            del t2[:-(pr+af)]\n            del p00[:-(pr+af)]\n            del p01[:-(pr+af)]\n            del t01[:-(pr+af)]\n            del p02[:-(pr+af)]\n            del t02[:-(pr+af)]\n            p0s=sum(p00)\n            p1s=sum(p01)\n            t1s=sum(t01)\n            p2s=sum(p02)\n            t2s=sum(t02)\n            if p1s==0 and p2s==0 and t2s==0: # erasing missing data pattern feature by deleting p0, t1 from samples and deleting samples with missing data p1,p2, and t2 \n                Str=np.hstack([[[labels[i]],[labels[i]],[labels[i]],[labels[i]]],img(p1),img(p2),img(t2),[[1],[2],[3],[4]]])\n                if i==0:\n                    rez=Str  \n                else:\n                    rez=np.vstack((rez,Str))\n    i+=1\n\nnp.savetxt(\"2.csv\",rez, delimiter=\",\")\n    \n","f92802df":"dataset=[]\nwith open(r'.\/2.csv', newline='') as f:\n    reader = csv.reader(f)\n    for row in reader:\n        dataset.append(row)\n    \nnp.array(dataset,dtype=np.float32)\nnp.random.shuffle(dataset)\n\nnp.savetxt('data2.csv', dataset, fmt='%s',delimiter=',')\n\nvs=0.5\nx=700# size of sample in timesteps\ndst=[]\nwith open(r'.\/data2.csv', newline='') as f:\n    reader = csv.reader(f)\n    for row in reader:\n        dst.append(row)\n        \ndataset=np.asarray(dst,dtype=float)\nlabel,p1,ts1,p2,ts2,t2,ts3,interlabel=np.hsplit(dataset,[1,x+1,x+2,2*x+2,2*x+3,3*x+3,3*x+4])# interlabel is a sample\u2019s parameter showing its position in the time window. This parameter equals to 1,2,3, or 4 and is isolated from the NN\u2019s input\nlabel=label.flatten()\n\nsc=np.concatenate([ts1,ts2,ts3],axis=1) # forming vector of scale factors\n\np1_=keras.Input(shape=(x,), name='p1')\np2_=keras.Input(shape=(x,), name='p2')\nt2_=keras.Input(shape=(x,), name='t2')\nsc_=keras.Input(shape=(3,), name='sc')\n\nsc_00=layers.Reshape((1,3))(sc_)\nsc_00=layers.UpSampling1D(size=int(x\/35))(sc_00)\n\np1_00=layers.Reshape((x,1))(p1_)\np2_00=layers.Reshape((x,1))(p2_)\nt2_00=layers.Reshape((x,1))(t2_)\n\np1_0=layers.MaxPooling1D(pool_size=35,strides=None)(p1_00)\np2_0=layers.MaxPooling1D(pool_size=35,strides=None)(p2_00)\nt2_0=layers.MaxPooling1D(pool_size=35,strides=None)(t2_00)\n\np1_1=layers.AveragePooling1D(pool_size=35,strides=None)(p1_00)\np2_1=layers.AveragePooling1D(pool_size=35,strides=None)(p2_00)\nt2_1=layers.AveragePooling1D(pool_size=35,strides=None)(t2_00)\n\npt=layers.concatenate([p1_0,p1_1,p2_0,p2_1,t2_0,t2_1,sc_00],axis=2)\n\npt =layers.Conv1D(\n    64,\n    3,\n    strides=1,\n    padding=\"valid\",\n    activation='relu',\n    use_bias=True,\n    kernel_initializer=\"glorot_uniform\",\n    bias_initializer=\"zeros\",\n    kernel_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    bias_constraint=None,\n    )(pt)\n\npt =layers.MaxPooling1D(pool_size=3,strides=3)(pt)\n\npt =layers.LSTM(\n    30,\n    activation=\"tanh\",\n    recurrent_activation=\"sigmoid\",\n    use_bias=True,\n    kernel_initializer=\"glorot_uniform\",\n    recurrent_initializer=\"orthogonal\",\n    bias_initializer=\"zeros\",\n    unit_forget_bias=True,\n    kernel_regularizer=None,\n    recurrent_regularizer=None,\n    bias_regularizer=None,\n    activity_regularizer=None,\n    kernel_constraint=None,\n    recurrent_constraint=None,\n    bias_constraint=None,\n    dropout=0.6,\n    recurrent_dropout=0.0,\n    return_sequences=False,\n    return_state=False,\n    go_backwards=False,\n    stateful=False,\n    time_major=False,\n    unroll=False,)(pt)\n\npt =layers.Dense(27,activation=\"relu\", use_bias=True)(pt)\ncatN=layers.Dense(9,activation=\"softmax\", use_bias=False, name='output')(pt)\n\nmodel = keras.Model(inputs=[p1_,p2_,t2_,sc_], outputs=catN)\nmodel.summary()\nopt = keras.optimizers.Adam(learning_rate=0.001)\nmodel.compile(loss='sparse_categorical_crossentropy', \n              optimizer=opt,  metrics=[keras.metrics.SparseCategoricalAccuracy()])\nfitting=model.fit({'p1':p1,'p2':p2,'t2':t2,'sc':sc},{'output':label}, \n          epochs=500, validation_split=vs) \n\n","53eb0801":"In the code above, with the parameter \"timewindow\" set to 10 we extract first 10 recodrings of sensors' data that contain a neglicable amount of information about the events they are related to. It is worth suggestng, the features that are extracted in the proposed way may be misleading for a classifier and result in a biased output. To find it out, we deploy an NN with the following simplistic design:","5923c39e":"Now, we can see that  the NN has more than 90% accuracy. But we know a priory that the instances contain the minimum of relevant inforamtion about the events as there is no hypothetical technique to destinguish undesired event in a well from ten seconds of sensor readings. Knowing that there is zero information in the constructed instances we would expect 30% accuracy from the NN,   as the benchmark result which corresponds to performance of the neural network with zero input that would simply classify all the events as the most represented class (class 0 with 597 instances). But the classifier successfully uses irrelevant features to solve the task. Further, those irrelevant features may serve as an obstacle for obtaining a classifier that works properly with instances outside the dataset. So, getting rid of the Missing data, Signal noise and Absolute value features the following data processing takes place...\n","9a1e38bb":"The above procedure cleans the raw data from the irrelevant features  and extracts samples from the original instances  as described by the following picture ( normal, transcient and faulty regimes are defined according to  labeling of sensor data)\n\n\n![image.png](attachment:86180d8c-aa4b-4b7f-94db-9cf9fe0b48b0.png)\n\n \nIf we apply the same feature cleansing to the instances of the previous NN input, we obtain 30.37% accuracy that is the desirable result","acc0c024":"With the obtained resampled and cleansed dataset we trained the LSTM neural network  achieving 89% accuracy.  The results of numerical experiments using this network demonstrate that the algorithm shows the maximum performance when trained with samples  that include  at least 300 seconds prior the starting point of an identified event.\n\n![image.png](attachment:cd1e09f4-9a5a-4912-81ea-ac8cd22e87f8.png)\n![image.png](attachment:1dc766af-d9b0-47c0-b4e7-5a047d07eb0a.png) ","fe812f77":"The following piece of code extracts raw data and replaces missing values with zeros. Particular missing data patterns may distribute unevenly in the dataset.\nFor example, the label 8 instances normally include P0 input, while label 0 events usually don't. This procedure saves absolute values of pressure and temperature. Signal noise feature is also available. It should be noted that   the dataset comprising instances of raw sensor readings, simulated data, and digitized hand-written inputs can be distinguished from one another by signal noise characteristics. The CAD-obtained and handwritten samples comprise smooth time series while the real data instances have a significant signal noise."}}