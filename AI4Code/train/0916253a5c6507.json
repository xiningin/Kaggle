{"cell_type":{"7901951c":"code","0e6db63f":"code","727fdca9":"code","ff1116f5":"code","e565d454":"code","7ce66106":"code","678968c3":"code","bd5f3185":"code","2643ecde":"code","5a75e528":"code","a9f80862":"markdown","d5ff2c06":"markdown","ec4122ae":"markdown","cad55eac":"markdown","82bbe46c":"markdown","c001c698":"markdown","07de54d7":"markdown","f6ab2645":"markdown","75a38cdb":"markdown","eaf90520":"markdown"},"source":{"7901951c":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Dense, Activation,Dropout,Conv2D, MaxPooling2D,BatchNormalization, Flatten\nfrom tensorflow.keras.optimizers import Adam, Adamax\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model, load_model, Sequential\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport seaborn as sns\nsns.set_style('darkgrid')\nfrom IPython.core.display import display, HTML\n# stop annoying tensorflow warning messages\nimport logging\nlogging.getLogger(\"tensorflow\").setLevel(logging.ERROR)","0e6db63f":"def print_in_color(txt_msg,fore_tupple,back_tupple,):\n    #prints the text_msg in the foreground color specified by fore_tupple with the background specified by back_tupple \n    #text_msg is the text, fore_tupple is foregroud color tupple (r,g,b), back_tupple is background tupple (r,g,b)\n    rf,gf,bf=fore_tupple\n    rb,gb,bb=back_tupple\n    msg='{0}' + txt_msg\n    mat='\\33[38;2;' + str(rf) +';' + str(gf) + ';' + str(bf) + ';48;2;' + str(rb) + ';' +str(gb) + ';' + str(bb) +'m' \n    print(msg .format(mat), flush=True)\n    print('\\33[0m', flush=True) # returns default print color to back to black\n    return","727fdca9":"def preprocess (csvpath, trsplit):    \n    df=pd.read_csv(csvpath)\n    df=df.copy()\n    # we do not need the patient id column so frop it\n    df=df.drop('patientId', axis=1)\n    # all columns are float values, split data set into X and y where the y column is 'reference' and X is all the other columns\n    y=df['reference']\n    X=df.drop(['reference'], axis=1)\n    # split into a train set and a test set\n    Xtrain, Xtest, ytrain, ytest =train_test_split(X,y, train_size=trsplit, shuffle=True, random_state=123)\n    # from df.describe it appears the X columns have already been scaled so no need to use a scalar\n    return Xtrain, Xtest, ytrain, ytest\n    \n    ","ff1116f5":"csvpath=r'..\/input\/ct-slice-localization\/slice_localization_data.csv'\ntrsplit=.8 # set % of X to be used for training\nXtrain, Xtest, ytrain, ytest = preprocess(csvpath, trsplit)\nprint (Xtrain.head())\nprint (ytrain.head())","e565d454":"inputs= tf.keras.Input(shape=384,)\nx = Dense(256, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(inputs)\nx=Dropout(rate=.3, seed=123)(x)\nx=Dense(64, kernel_regularizer = regularizers.l2(l = 0.016),activity_regularizer=regularizers.l1(0.006),\n                bias_regularizer=regularizers.l1(0.006) ,activation='relu')(x)\noutput=Dense(1, activation='linear')(x)\nmodel=Model(inputs=inputs, outputs=output)\nmetrics=[tf.keras.metrics.RootMeanSquaredError( name=\"root_mean_squared_error\", dtype=None)]\nmodel.compile(Adamax(learning_rate=.001), loss='mse',metrics=metrics) ","7ce66106":"vsplit=.2\nepochs=100\nrlronp=tf.keras.callbacks.ReduceLROnPlateau(  monitor=\"val_loss\", factor=0.5, patience=2, verbose=1)\nestop=tf.keras.callbacks.EarlyStopping( monitor=\"val_loss\", patience=5,  verbose=1,restore_best_weights=True)\ncallbacks=[rlronp, estop]\nhistory=model.fit(Xtrain, ytrain,validation_split= vsplit, batch_size=32, epochs=epochs, callbacks=callbacks )\n    ","678968c3":"start_epoch=0\ntr_loss=history.history['loss']\ntr_rmse=history.history['root_mean_squared_error']\nv_loss=history.history['val_loss']\nv_rmse=history.history['val_root_mean_squared_error']\nEpoch_count=len(tr_loss)\nEpochs=[]\nfor i in range (0 ,Epoch_count):\n    Epochs.append(i+1)   \nindex_loss=np.argmin(v_loss)#  this is the epoch with the lowest validation loss\nval_lowest=vloss[index_loss]# this is the value of the lowest loss\nindex_acc=np.argmin(v_rmse)\nacc_highest=v_rmse[index_acc]\nplt.style.use('fivethirtyeight')\nsc_label='best epoch= '+ str(index_loss+1 +start_epoch)\nvc_label='best epoch= '+ str(index_acc + 1+ start_epoch)\nfig,axes=plt.subplots(nrows=1, ncols=2, figsize=(20,8))\naxes[0].plot(Epochs,tr_loss, 'r', label='Training loss')\naxes[0].plot(Epochs,v_loss,'g',label='Validation loss' )\naxes[0].scatter(index_loss+1 +start_epoch,val_lowest, s=150, c= 'blue', label=sc_label)\naxes[0].set_title('Training and Validation Loss')\naxes[0].set_xlabel('Epochs')\naxes[0].set_ylabel('Loss')\naxes[0].legend()\naxes[1].plot (Epochs,tr_rmse,'r',label= 'Training RMSE')\naxes[1].plot (Epochs,v_rmse,'g',label= 'Validation RMSE')\naxes[1].scatter(index_acc+1 +start_epoch,acc_highest, s=150, c= 'blue', label=vc_label)\naxes[1].set_title('Training and Validation RSME')\naxes[1].set_xlabel('Epochs')\naxes[1].set_ylabel('RMSE')\naxes[1].legend()\nplt.tight_layout\n#plt.style.use('fivethirtyeight')\nplt.show()\n","bd5f3185":"ypred=np.squeeze(model.predict(Xtest))\nerror= ytest-ypred\nerror_sq=error**2\nmean_sq_error=np.mean(error_sq)\nmean_error=np.sqrt(mean_sq_error)\nmsg=f'RMSE on the test set is {mean_error:6.4f} '\nprint_in_color(msg, (0,255,0),(55,65,80))\n","2643ecde":"working_dir=r'.\/'\nsubject='CAT SCAN AXIS '\nsave_id=f'{subject}- {mean_error:6.4f}.h5'\nmodel_save_loc=os.path.join(working_dir, save_id)\nmodel.save(model_save_loc)\nprint_in_color ('model was saved as ' + model_save_loc, (0,255,0),(55,65,80))","5a75e528":"sum_of_model_squared_errors=np.sum((ytest-ypred)**2)\nsum_of_mean_squared_errors=np.sum((ytest-ytest.mean())**2)\nR_sq=1-sum_of_model_squared_errors\/sum_of_mean_squared_errors\nmsg=f'R Squared Score on the test set is {R_sq:6.4f} '\nprint_in_color(msg, (0,255,0),(55,65,80))","a9f80862":"### mean_error of .8152 is a good result given the range of y is from 0 to 180 so save the model","d5ff2c06":"### Define  a function to print text in RGB foreground and background colors","ec4122ae":"### create the model","cad55eac":"### define an early stopping callback and an adjust learning rate on plateau callback then train the model","82bbe46c":"### Since max R Squared score is 1 , .9987 is a very good score","c001c698":"### Calculate the R squared score","07de54d7":"### plot the training results","f6ab2645":"### define function to preprocess the dataframe and create train,  and test data frames","75a38cdb":"### evaluate model on the test set ","eaf90520":"### call preprocessor function"}}