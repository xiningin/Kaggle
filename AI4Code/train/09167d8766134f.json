{"cell_type":{"6cdbf833":"code","32df0c9f":"code","f13270d3":"code","00eac725":"code","eb385900":"code","f5f8f84b":"code","08a6f76f":"code","9a67d15e":"code","47af807d":"code","662441e7":"code","5825bd0c":"code","2f49cea4":"code","7b850718":"code","6371133d":"code","f59557b6":"code","d28c349e":"code","3b863744":"code","09e94ef3":"code","ca87d478":"code","997b79c2":"code","ad9dcf9d":"code","c5040a86":"code","b7273bc8":"code","5fafc05b":"code","7744d734":"code","9b96d47f":"code","753770eb":"code","af641002":"code","05247ff4":"code","edf6f900":"code","73dfb65e":"code","b18741d5":"code","e38533c5":"code","af6208d9":"markdown","c22d144e":"markdown","f6ca4235":"markdown","45598d7c":"markdown","5c303dcc":"markdown","4838dd75":"markdown","58bc433e":"markdown"},"source":{"6cdbf833":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, roc_auc_score, f1_score, roc_curve, auc, precision_score, recall_score\nimport lightgbm as lgb\nfrom sklearn.svm import SVC\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","32df0c9f":"data_df = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","f13270d3":"data_df.head()","00eac725":"data_df.isnull().sum()","eb385900":"sns.countplot(data_df['Outcome'])","f5f8f84b":"sns.boxplot(data=data_df, y='Glucose', x='Outcome')","08a6f76f":"np.log(data_df['Pregnancies'])","9a67d15e":"sns.boxplot(data=data_df, y='Pregnancies', x='Outcome')","47af807d":"sns.boxplot(y=np.log(data_df['Pregnancies'] + 1), x=data_df['Outcome'])","662441e7":"sns.boxplot(data=data_df, y='BloodPressure', x='Outcome')","5825bd0c":"sns.boxplot(y=np.log(data_df['SkinThickness'] + 1), x=data_df['Outcome'])","2f49cea4":"sns.boxplot(data=data_df, y='SkinThickness', x='Outcome')","7b850718":"sns.boxplot(data=data_df, y='Insulin', x='Outcome')","6371133d":"sns.boxplot(data=data_df, y='BMI', x='Outcome')","f59557b6":"sns.boxplot(data=data_df, y='Age', x='Outcome')","d28c349e":"sns.boxplot(y=np.log(data_df['DiabetesPedigreeFunction'] + 1), x=data_df['Outcome'])","3b863744":"sns.boxplot(data=data_df, y='DiabetesPedigreeFunction', x='Outcome')","09e94ef3":"data_df.groupby('Outcome').describe()","ca87d478":"def build_model(model):\n    model_pipeline = Pipeline([\n        ('scaler', StandardScaler()),\n        ('model', model)\n    ])\n    return model","997b79c2":"def build_lgbm(X_train, X_test, y_train, y_test):\n    params = {\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': {'auc'},\n        'colsample_bytree': 0.7, \n        'max_depth': 15, \n        'min_split_gain': 0.3, \n        'n_estimators': 400, \n        'num_leaves': 50, \n        'reg_alpha': 1.1, \n        'reg_lambda': 1.1, \n        'subsample': 0.8, \n        'subsample_freq': 20\n    }\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_test, y_test)\n    return lgb_train, lgb_eval, params","ad9dcf9d":"def perform_SMOTE(X, y):\n    sm = SMOTE(random_state=42)\n    X_res, y_res = sm.fit_resample(X, y)\n    return X_res, y_res","c5040a86":"def compute_metrics(y_pred, y_test, threshold):\n    binary_pred = np.array([1 if pred > threshold else 0 for pred in y_pred])\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n    print('AUC Score: ', auc(fpr, tpr))\n    print('AUC ROC Score: ', roc_auc_score(y_test, y_pred))\n    print('F1 Score: ', f1_score(y_test, binary_pred))\n    print('Accuracy Score: ', accuracy_score(y_test, binary_pred))\n    print('Precision Score: ', precision_score(y_test, binary_pred))\n    print('Recall Score: ', recall_score(y_test, binary_pred))\n    print('tn, fp, fn, tp: ', confusion_matrix(y_test, binary_pred).ravel())\n    print(sns.heatmap(confusion_matrix(y_test, binary_pred), annot=True, \n                      xticklabels=['Pred 0', 'Pred 1'], \n                      yticklabels=['Actual 0', 'Actual 1']));","b7273bc8":"def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n                       do_probabilities = False):\n    gs = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid, \n        cv=cv, \n        n_jobs=-1, \n        scoring=scoring_fit,\n        verbose=2\n    )\n    fitted_model = gs.fit(X_train_data, y_train_data)\n    \n    if do_probabilities:\n        pred = fitted_model.predict_proba(X_test_data)\n    else:\n        pred = fitted_model.predict(X_test_data)\n    \n    return fitted_model, pred","5fafc05b":"y = data_df['Outcome']\nX = data_df.drop('Outcome', axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)","7744d734":"X_train, y_train = perform_SMOTE(X_train, y_train)\nlgb_train, lgb_eval, params = build_lgbm(X_train, X_test, y_train, y_test)\ngbm = lgb.train(params,\n                    lgb_train,\n                    num_boost_round=10000,\n                    valid_sets=lgb_eval,\n                    early_stopping_rounds=100)\ny_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\ncompute_metrics(y_pred, y_test, 0.5)","9b96d47f":"model = lgb.LGBMClassifier()\nparam_grid = {\n    'n_estimators': [400, 700, 1000],\n    'colsample_bytree': [0.7, 0.8],\n    'max_depth': [15,20,25],\n    'num_leaves': [50, 100, 200],\n    'reg_alpha': [1.1, 1.2, 1.3],\n    'reg_lambda': [1.1, 1.2, 1.3],\n    'min_split_gain': [0.3, 0.4],\n    'subsample': [0.7, 0.8, 0.9],\n    'subsample_freq': [20]\n}\n\nmodel, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n                                 param_grid, cv=5, scoring_fit='f1')\n\nprint(model.best_score_)\nprint(model.best_params_)","753770eb":"compute_metrics(pred, y_test, 0.5)","af641002":"model = build_model(LogisticRegression(max_iter=1000))\nmodel.fit(X_train,y_train)\ncompute_metrics(model.predict_proba(X_test)[:,1], y_test, 0.5)","05247ff4":"model = build_model(LogisticRegression(max_iter=1000))\nparam_grid = {\n    'C': [0.01, 0.1, 1, 10],\n}\nmodel, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n                                 param_grid, cv=5, scoring_fit='f1', do_probabilities= True)\n\nprint(model.best_params_)\n\ncompute_metrics(pred[:,1:], y_test, 0.5)","edf6f900":"model = build_model(SVC())\nmodel.fit(X_train,y_train)\ncompute_metrics(model.predict(X_test), y_test, 0.5)","73dfb65e":"model = build_model(SVC())\nparam_grid = {\n    'C': [0.01, 0.1, 1, 10],\n}\nmodel, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n                                 param_grid, cv=5, scoring_fit='f1', do_probabilities= False)\n\nprint(model.best_params_)\n\ncompute_metrics(pred, y_test, 0.5)","b18741d5":"model = build_model(RandomForestClassifier())\nmodel.fit(X_train,y_train)\ncompute_metrics(model.predict_proba(X_test)[:,1:], y_test, 0.5)","e38533c5":"model = build_model(RandomForestClassifier())\nparam_grid = {\n    'n_estimators': [100, 1000, 10000],\n    'max_depth': [2, 5, 10],\n    'min_samples_split': [2, 5, 7]\n}\nmodel, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n                                 param_grid, cv=2, scoring_fit='f1', do_probabilities= True)\n\nprint(model.best_params_)\n\ncompute_metrics(pred[:,1:], y_test, 0.5)","af6208d9":"## Support Vector Classifier","c22d144e":"## Perform some EDA","f6ca4235":"## Random Forest Classifier","45598d7c":"Since the primary objective of building this model is centered on early interventions for high-risk diabetes patients, we are primarily focused on reducing the number of false positive and false negative cases. False negative cases occur when we predict a patient to not have diabetes when they actually have it. In camparison, false positive cases occur when we suspect a patient to have diabetes when they have it in reality.\n\nWhen looking at the impact of false negative predictions, the biggest risk is failing to provide early medication to these patients let alone providing actual treatments to them. This impact can seriously hamper the credibility of the business as well as its capacity to treat patients. On the other hand, having a lot of false positive cases would also harm the company's reputation. Patients who don't have diabetes but take diabetes medications are in the risk of developing complications from taking such. Overall, both false negative and false positive occurences impose tremendous harm to the business. \n\nConsidering all items mentioned above, we need to look at the overall F1 score as well as the AUC ROC score. The F1 score will allow us to assess the overall effectiveness of these models with the consideration of false positives and false negatives. Higher F1 score and AUC ROC score would mean a better model. As such the light gradient boosted model is selected to be the best model to use for predicting a patient's risk to have diabetes.","5c303dcc":"## Logistic Regression","4838dd75":"## Light Gradient Boosting Model","58bc433e":"### Define Helper Functions"}}