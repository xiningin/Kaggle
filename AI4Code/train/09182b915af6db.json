{"cell_type":{"35bf16cc":"code","3839295a":"code","23eb7977":"code","daab5fc9":"code","e5ba14dc":"code","7d5acf16":"code","5c90834d":"code","9ef5119f":"code","ca511472":"code","f3afa662":"code","6f3ff1b0":"code","c485248d":"code","7d20ea28":"code","9fd13518":"code","d6dcd943":"code","fe888c1b":"code","10647f0a":"code","8d9eac4d":"code","2ee0415a":"code","448dff18":"code","ccdf6971":"code","f27f1926":"code","7db09799":"code","5ab5440e":"code","cffd6c9f":"code","fec2d150":"code","7fef5de7":"code","2efbb911":"code","d2dd5bd8":"code","970d91c7":"code","d8fb532d":"code","976a2b3a":"markdown","b713a254":"markdown","bff3f12c":"markdown","d5b48bbc":"markdown","03ca2165":"markdown","71ba33f6":"markdown","df78e5cd":"markdown","099d6a70":"markdown","2616597e":"markdown","59de1f04":"markdown","062b7e0c":"markdown","5d52988e":"markdown","7bbfdbac":"markdown","2238c6b5":"markdown","4827b042":"markdown","0c9ddbe2":"markdown","5738846e":"markdown"},"source":{"35bf16cc":"import os\nimport math\nimport datetime\nimport numpy as np \nimport pandas as pd\nimport scipy as sc\nimport matplotlib.pyplot as plt\nimport numexpr\nfrom PIL import Image\nimport seaborn as sns\n\nRANDOM_SEED = 111\n\nnp.random.seed(RANDOM_SEED)\n\nfrom numpy.random import default_rng\nrng = default_rng(RANDOM_SEED)\n\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score, accuracy_score, mean_squared_log_error\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, StandardScaler, OneHotEncoder, Binarizer, KBinsDiscretizer, QuantileTransformer, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, KFold, StratifiedKFold, StratifiedShuffleSplit, ShuffleSplit\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import set_config\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow.python.keras.losses import mean_squared_logarithmic_error\n\ntf.random.set_seed(RANDOM_SEED)\n\nINPUT_DIR = '\/kaggle\/input\/tabular-playground-series-jul-2021'\nBATCH_SIZE = 1024","3839295a":"def season(month):\n  if (month == 12 or month == 1 or month == 2):   #winter\n        return 0        \n  elif(month == 3 or month == 4 or month == 5):   #spring\n        return 1       \n  elif(month == 6 or month == 7 or month == 8):   #summer\n        return 2       \n  else:                                           #outemn\n        return 3 \n\ndef daytime(hour):\n  if (hour > 5 and hour < 17):      #light\n    return 0\n  else:                             #darkness\n    return 1\n\ntrain_df = pd.read_csv(os.path.join(INPUT_DIR, 'train.csv'), index_col='date_time')\ntest_df = pd.read_csv(os.path.join(INPUT_DIR,'test.csv'), index_col='date_time')\n\ntrain_df.index = pd.to_datetime(train_df.index)\ntest_df.index = pd.to_datetime(test_df.index)\n\nlabels = train_df[['target_carbon_monoxide','target_benzene','target_nitrogen_oxides']]\n\ntrain_df.drop(labels.columns, axis=1, inplace=True)\ntotal_df = train_df.append(test_df)     #pd.concat()\n\ntotal_df['dew_point'] = total_df['deg_C'].apply(lambda x: (17.27 * x) \/ (237.7 + x)) + total_df['absolute_humidity'].apply(lambda x: math.log (x))\ntotal_df['partial_pressure'] = (total_df['deg_C'].apply(lambda x: (237.7 + x) * 286.8) * total_df['absolute_humidity']) \/ 100000\ntotal_df['saturated_wvd'] = (total_df['absolute_humidity'] * 100) \/ total_df['relative_humidity']\n\ntotal_df['dt_hour'] = [x.hour for x in total_df.index]\ntotal_df['dt_weekday'] = [x.weekday() for x in total_df.index]\ntotal_df['dt_month'] = [x.month for x in total_df.index]\ntotal_df['dt_season'] = [season(x.month) for x in total_df.index]\ntotal_df['dt_lights'] = [daytime(x.hour) for x in total_df.index]\ntotal_df['dt_month_s'] = np.sin(np.pi * (total_df['dt_month']-1)\/6)\ntotal_df['dt_month_c'] = np.cos(np.pi * (total_df['dt_month']-1)\/6)\n\ntotal_df['dt_month_s'] = total_df['dt_month_s'].astype('category').cat.codes\ntotal_df['dt_month_c'] = total_df['dt_month_c'].astype('category').cat.codes\n\ntotal_df[\"dt_working_hours\"] = total_df[\"dt_hour\"].isin(np.arange(8, 21, 1)).astype(\"int\")\ntotal_df[\"dt_weekend\"] = (total_df[\"dt_weekday\"] >= 5).astype(\"int\")\n\nsensors = ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5']\n#total_df[[x+'_1h' for x in sensors]] = total_df[sensors].diff(1).fillna(0)\n\ncat_cols = np.array([col for col in total_df.columns if 'dt_' in col])\nnum_cols = np.array([col for col in total_df.columns if not 'dt_' in col])\ntotal_cols = np.concatenate([num_cols,cat_cols])\ncat_cols_idx = [np.where(total_df.columns == x)[0][0] for x in cat_cols]","23eb7977":"pd.concat((total_df.min(), total_df.max(), total_df.mean(), total_df.std(), total_df.nunique()), axis=1)","daab5fc9":"sns.clustermap(total_df.corr(), annot=True, square=True)\nplt.show()","e5ba14dc":"total_df[num_cols].plot(subplots=True, layout=(3,4), figsize=(20,5))\nplt.show()","7d5acf16":"total_df[num_cols].pct_change().plot(subplots=True, layout=(3,4), figsize=(20,5))\nplt.show()","5c90834d":"total_df.loc['2011-02-06','deg_C'].pct_change().plot();","9ef5119f":"total_df.loc['2011-02-06T08:00':'2011-02-06T14:00','deg_C']","ca511472":"total_df.loc['2011-02-05T08:00':'2011-02-05T14:00','deg_C']","f3afa662":"seasonality_dict = {ts: seasonal_decompose(total_df[ts], period=255).seasonal for ts in num_cols}\nseasonality_corr = pd.DataFrame(seasonality_dict).corr()\n\nsns.clustermap(seasonality_corr, annot=True)\nplt.show();","6f3ff1b0":"from statsmodels.graphics import tsaplots\ntsaplots.plot_acf(total_df[\"deg_C\"], lags=30, title='deg_C')\ntsaplots.plot_acf(labels[labels.columns[0]], lags=30, title=labels.columns[0])\nplt.show()","c485248d":"total_df[\"deg_C\"].rolling(24).std().std()","7d20ea28":"from statsmodels.tsa.stattools import adfuller\n\nresults = adfuller(labels[labels.columns[0]])\n\nprint(f\"ADF Statistic: {results[0]}\")\nprint(f\"p-value: {results[1]}\")\nprint(\"Critical Values:\")\nfor key, value in results[4].items():\n    print(\"\\t%s: %.3f\" % (key, value))","9fd13518":"pd.concat((labels.min(), labels.max(), labels.mean(), labels.nunique()), axis=1)","d6dcd943":"sns.clustermap(labels.corr(), annot=True, figsize=(5,5))\nplt.show();","fe888c1b":"fft = tf.signal.rfft(total_df['deg_C'])\nf_per_dataset = np.arange(0, len(fft))\n\nn_samples_h = len(total_df['deg_C'])\nhours_per_year = 24*365.2524\nyears_per_dataset = n_samples_h\/(hours_per_year)\n\nf_per_year = f_per_dataset\/years_per_dataset\nplt.step(f_per_year, np.abs(fft))\nplt.xscale('log')\nplt.ylim(0, 50000)\nplt.xlim([0.1, max(plt.xlim())])\nplt.xticks([1, 365.2524], labels=['1\/Year', '1\/day'])\n_ = plt.xlabel('Frequency (log scale)')","10647f0a":"pipe_pre = ColumnTransformer([\n  #('poly', PolynomialFeatures(interaction_only=True, include_bias=False), sensors),\n  ('num', Pipeline([\n      ('sensors', ColumnTransformer([\n        ('poly', PolynomialFeatures(interaction_only=True, include_bias=False), sensors)\n      ], remainder='passthrough')),\n      #('scale', StandardScaler()),\n      #('gauss', QuantileTransformer(output_distribution=\"normal\")),\n      #('minmax', MinMaxScaler()),\n      ('kbins', KBinsDiscretizer(n_bins=32, encode='ordinal')),  #strategy='uniform'\n      #('onehot', OneHotEncoder(sparse=False))\n  ]), num_cols),\n  #('cat', OrdinalEncoder(), cat_cols)\n  ('cat', OneHotEncoder(sparse=False), cat_cols)\n  #('cat', MyVectorizer(cols=cat_cols, hashing=16), cat_cols)\n], remainder='passthrough')\n\npipe_pre.fit(total_df)\ntotal_data = pipe_pre.transform(total_df).astype('float')\n\ntrain_data, test_data = total_data[:train_df.index.shape[0]], total_data[train_df.index.shape[0]:]","8d9eac4d":"wsize = 3\ndata = [1,2,3,4,5,6,7,8,9,10,11,12]\ntarget = [-1,-2,-3,-4,-5,-6,-7,-8,-9,-10,-11,-12]\n\nexp_data = [data[i:-wsize+i+1] for i in range(wsize-1)]\nexp_data.append(data[wsize-1:])\nexp_data = np.dstack(exp_data)[0]\n\nexp_target = target[wsize-1:]\n\nprint(exp_data)\nprint(exp_target)","2ee0415a":"# after reducing LR continue from the largest score weights\nclass CustomReduceLROnPlateau(tf.keras.callbacks.ReduceLROnPlateau):\n  def on_epoch_end(self, epoch, logs=None):\n    current = (logs or {}).get(self.monitor)\n    if not self.monitor_op(current, self.best) and not self.in_cooldown():\n      if self.wait+1 >= self.patience:\n        self.model.load_weights(\"filepath.h5\")\n    \n    super().on_epoch_end(epoch, logs)\n\n\ndef create_windows(data, window_size):\n  exp_data = [data[i:-window_size+i+1] for i in range(window_size-1)]\n  exp_data.append(data[window_size-1:])\n  exp_data = np.moveaxis(np.dstack(exp_data),1,2)\n  return exp_data\n\n\ndef append_label(data, target):\n  target = np.moveaxis(np.expand_dims([target for x in range(264)], axis=0), 2,0)\n  data = np.append(data, target, axis=1)\n  return data\n\n\ndef RMSLE(y_true, y_pred):\n  return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(tf.keras.backend.log(1+y_pred) - tf.keras.backend.log(1+y_true))))\n\n\ndef train_model(train_tensor, test_tensor, model):\n  tf.keras.backend.clear_session()\n\n  early_stop  = tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss', mode='min', min_delta=0.00001)\n  check_point = tf.keras.callbacks.ModelCheckpoint(filepath='filepath.h5', monitor='val_loss', mode='min', save_best_only=True, save_weights_only=True)\n\n  model.compile(loss=RMSLE, optimizer=tfa.optimizers.SWA(tf.keras.optimizers.Adam(0.01)))\n  model.fit(train_tensor, validation_data=train_tensor, epochs=10, callbacks=[early_stop, check_point], verbose=1)\n  model.load_weights('filepath.h5')\n    \n  score = model.evaluate(train_tensor)\n  test_predict = model.predict(test_tensor)\n    \n  return score, test_predict\n\n\nwindow_size = 24\nBATCH_SIZE = 1024\ndims = train_data.shape[1]","448dff18":"img_array = np.array(Image.open('..\/input\/jul21plot\/model_2d.png'))\nplt.figure(figsize = (20,10))\nplt.imshow(img_array)","ccdf6971":"def create_model_2d(dims):\n  inp = tf.keras.layers.Input(shape=(dims,))\n\n  branch = [None] * 3\n  for i in range(3):\n    branch[i] = tf.keras.layers.Dense(dims, activation=\"relu\")(inp)\n    branch[i] = tf.keras.layers.Dense(dims\/\/2, activation=\"relu\")(branch[i])\n    branch[i] = tf.keras.layers.Dense(dims\/\/4, activation=\"relu\")(branch[i])\n    branch[i] = tf.keras.layers.Dense(1, activation=\"relu\")(branch[i])\n\n  y = tf.keras.layers.Concatenate()(branch)\n  model = tf.keras.Model(inputs=inp, outputs=y)\n  #tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"TB\")\n  #print(model.summary())\n  return model\n\n\ntrain_tensor = tf.data.Dataset.from_tensor_slices((train_data, labels)).batch(BATCH_SIZE, drop_remainder=True).cache()\ntest_tensor = tf.data.Dataset.from_tensor_slices(test_data).batch(BATCH_SIZE).cache()\n\nmodel = create_model_2d(dims)\nscore, test_predict = train_model(train_tensor, test_tensor, model)","f27f1926":"img_array = np.array(Image.open('..\/input\/jul21plot\/model_3d.png'))\nplt.figure(figsize = (20,10))\nplt.imshow(img_array)","7db09799":"def simple_model_3d(dims, wsize):\n  inp = tf.keras.layers.Input(shape=(wsize, dims))\n  x = tf.keras.layers.SimpleRNN(wsize, return_sequences=True)(inp)\n  x = tf.keras.layers.SimpleRNN(wsize\/\/2)(x)\n  y = tf.keras.layers.Dense(1, activation=\"relu\")(x)\n  model = tf.keras.Model(inputs=inp, outputs=y)\n  print(model.summary())\n  return model\n\n\ndef create_model_3d(dims, wsize):\n  inp = tf.keras.layers.Input(shape=(wsize, dims))\n\n  branch = [None] * 3\n  for i in range(3):\n    branch[i] = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(wsize, return_sequences=True))(inp)\n    branch[i] = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(wsize))(branch[i])\n    branch[i] = tf.keras.layers.Dense(wsize, activation=\"relu\")(branch[i])\n    branch[i] = tf.keras.layers.Dense(wsize\/\/2, activation=\"relu\")(branch[i])\n    branch[i] = tf.keras.layers.Dense(1, activation=\"relu\")(branch[i])\n\n  y = tf.keras.layers.Concatenate()(branch)\n  model = tf.keras.Model(inputs=inp, outputs=y)\n  tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"TB\")\n  #print(model.summary())\n  return model\n\n\nexp_labels = labels.values[window_size-1:]\nexp_train_data = create_windows(train_data, window_size)\n\nexp_test_data = np.concatenate([train_data[-window_size:,:], test_data[1:]])\nexp_test_data = create_windows(exp_test_data, window_size)\n\ntrain_tensor = tf.data.Dataset.from_tensor_slices((exp_train_data, exp_labels)).batch(BATCH_SIZE, drop_remainder=True).cache()\ntest_tensor = tf.data.Dataset.from_tensor_slices(exp_test_data).batch(BATCH_SIZE).cache()\n\nmodel = create_model_3d(dims, window_size)\nscore, test_predict = train_model(train_tensor, test_tensor, model)","5ab5440e":"img_array = np.array(Image.open('..\/input\/jul21plot\/model_4d.png'))\nplt.figure(figsize = (20,10))\nplt.imshow(img_array)","cffd6c9f":"def create_model_4d(dims, wsize):\n  inp = tf.keras.layers.Input(shape=(wsize, dims))\n  #x = tf.keras.layers.Embedding(dims, dims\/\/2)(inp)\n  #x = tf.keras.layers.Reshape(target_shape=(dims\/\/2))(x)\n  #x = tf.keras.layers.GlobalAveragePooling2D()(x)\n  #x = tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(x)\n  #x = tf.keras.layers.BatchNormalization()(inp)\n\n  branch = [None] * 3\n  for i in range(3):\n    branch[i] = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(wsize\/\/2, return_sequences=True))(inp)\n    branch[i] = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(wsize\/\/2, return_sequences=True))(branch[i])\n    branch[i] = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(wsize\/\/2))(branch[i])\n    branch[i] = tf.keras.layers.Dense(1, activation=\"relu\")(branch[i])\n\n  y = tf.keras.layers.Concatenate()(branch)\n  model = tf.keras.Model(inputs=inp, outputs=y)\n\n  #tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"TB\")\n  #print(model.summary())\n  return model\n\n\nexp_labels = create_windows(labels, window_size) \nexp_train_data = create_windows(train_data, window_size)\n\nexp_test_data = np.concatenate([train_data[-window_size:,:], test_data[1:]])\nexp_test_data = create_windows(exp_test_data, window_size)\n\ntrain_tensor = tf.data.Dataset.from_tensor_slices((exp_train_data, exp_labels)).batch(BATCH_SIZE, drop_remainder=True).cache()\ntest_tensor = tf.data.Dataset.from_tensor_slices(exp_test_data).batch(BATCH_SIZE).cache()\n\nmodel = create_model_4d(dims, window_size)\nscore, test_predict = train_model(train_tensor, test_tensor, model)\ntest_predict = test_predict[:,:,0]","fec2d150":"output_res = pd.DataFrame(index=test_df.index, data={'date_time':test_df.index.values})\noutput_res[labels.columns] = test_predict\noutput_res.to_csv('.\/submission.csv', index=False)","7fef5de7":"exp_labels = labels.values[window_size-1:]\ntrain_predict = pd.read_csv('..\/input\/predicted-train-ds\/train_predict.csv')\nresiduals = pd.DataFrame(data=train_predict-exp_labels, columns=labels.columns)\n\nprint('mean: \\n', residuals.mean(axis=0))\nprint('std: \\n', residuals.std(axis=0))","2efbb911":"loss = np.sqrt(mean_squared_log_error(train_predict, exp_labels))\nprint('loss function:', loss)","d2dd5bd8":"residuals.plot(subplots=True, figsize=(20,5))\nplt.show()","970d91c7":"tsaplots.plot_acf(residuals[residuals.columns[0]], lags=30, title=residuals.columns[2])\nplt.show()","d8fb532d":"seasonal_decompose(residuals[residuals.columns[0]], period=255).seasonal.plot()\nseasonal_decompose(residuals[residuals.columns[0]], period=255).trend.plot()","976a2b3a":"# Upload dataset","b713a254":"# Build 2D model\n\nThis is the simplest, flat DNN model that doesn't use windows.<br\/>\nAll models are split to 3 identical branches that return 1 target label each (corresponds to the sklearn's `MultiOutputRegressor`) because I recieved worse results when was training with 1 shared branch:<br\/>\n`tf.keras.layers.Dense(3, activation=\"sigmoid\")`<br\/>\nThe activation function is always `relu` because when I was training all models with `sigmoid` and `softmax` they returned the worse results.<br\/>\nThe 2D model returned the worst result.","bff3f12c":"# Analyse features","d5b48bbc":"# Preprocess features","03ca2165":"The rolling STD for 24h windows is not constant, so it is not a \"white noise\".","71ba33f6":"As we can see, in both cases the autocorrelation is highest in a period of 24h","df78e5cd":"All input features, expesially temperature (`deg_C`) are seasonal, so we can use here windowed sequences with RNN models. ","099d6a70":"# Validate the 3D model residuals\n\nAs we can see, the residuals still contain the autocorrelations and trend\/seasonality patterns.","2616597e":"NOTE: There is a temperature anomaly at \"2011-02-06\". The very low morning temperature and a shart jump to the normal midday temperature.<br\/>\nI tried to use feature value returns instead of the absolute values but received worse results.","59de1f04":"We will split the dataset into the 24h windows","062b7e0c":"# Build 3D model\n\nThis is the RNN model that uses 24h windows for input features and one-step-prediction for target labels.<br\/>\nI tried other window sizes, like: 8h, 12h, 7d, 11d, 30d; and they all returned worse results.<br\/>\nThis model returned the best local result of `0.1564716398715973` (`0.2` in Private Score) after about the 150 steps.","5d52988e":"...and it is not a \"random walk\" - p<0.05","7bbfdbac":"# Save results to CSV file","2238c6b5":"# Build 4D model\n\nThis is the RNN model that uses 24h windows for input features and the 24h multi-step-prediction for target labels.<br\/>\nAn example is described here: https:\/\/mobiarch.wordpress.com\/2020\/11\/13\/preparing-time-series-data-for-rnn-in-tensorflow\/<br\/>\nThis model returned the local result of `0.3389951288700104` after about the 150 steps.","4827b042":"Correlation between seasonalities","0c9ddbe2":"Example of how the windowing is working.<br\/>\nThere ar emany methods that do windowing. Each of them has pros and cons. \n\n`test_data = skimage.util.view_as_windows(test_data, (16, 2000)).reshape((-1, 16, 2000, 1))`\n\n`tf.keras.preprocessing.timeseries_dataset_from_array(train_data, exp_labels[1], sequence_length=window_size, batch_size=BATCH_SIZE)`\n\n`tf.keras.preprocessing.sequence.TimeseriesGenerator`\n\n```exp_test_data = [np.roll(test_data, -i, axis=0) for i in range(window_size)]\nexp_test_data = np.moveaxis(np.stack(exp_test_data),0,-1)```\n  \nWe will use the simplest:","5738846e":"Target labels are highly correlated."}}