{"cell_type":{"a8036c99":"code","b4e2771b":"code","ab7a40ad":"code","7444bc55":"code","5e4b31c4":"code","c4fc86fa":"code","f243fcb6":"code","00d43db6":"code","87a3a126":"code","36b3c2df":"code","41567f78":"code","8c36e675":"code","812840f4":"code","cc232023":"code","9a32b287":"code","d77e3ead":"code","181226f0":"code","460a4cd5":"code","26abcb10":"code","560b7028":"code","d2148a4c":"markdown","71707cfc":"markdown","a46c83db":"markdown","61449306":"markdown","3707c8da":"markdown","2b27d75a":"markdown","2171c085":"markdown","86f3363f":"markdown","35404599":"markdown","d376a598":"markdown","67f21196":"markdown","0660999d":"markdown","3926bf60":"markdown"},"source":{"a8036c99":"import pandas as pd\nimport numpy as np\nimport random\nimport os\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import VarianceThreshold\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nimport copy\n\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nimport seaborn as sns\n\nfrom tensorboardX import SummaryWriter\n\nfrom collections import Counter","b4e2771b":"#Change this to your train predictions.\ntrain_pred = pd.read_csv('..\/input\/moa-pytorch-nn-starter\/oof.csv')","ab7a40ad":"train_pred = pd.read_csv('..\/input\/moa-pytorch-nn-starter\/oof.csv')\n\ntrain_data = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\n\ntrain_data = pd.merge(train_data, train_targets, on=['sig_id'])\n\ntrain_data = train_data[train_data.cp_type == 'trt_cp'].reset_index(drop=True)\n\ntarget_columns = [c for c in train_targets.columns if c != 'sig_id']\ntarget_columns_pred = [f'{c}_pred' for c in target_columns]\ntarget_columns_loss = [f'{c}_loss' for c in target_columns]\n\ntrain_data['activated_moas'] = train_data.loc[:, target_columns].values.sum(axis=1)","7444bc55":"def calculate_log_loss(predicted_df, train_df, target_columns):\n    \n    predicted_df = predicted_df[target_columns + ['sig_id']].reset_index(drop=True)\n    predicted_df = predicted_df.sort_values(by=['sig_id'])\n    predicted_df = predicted_df.drop('sig_id', axis=1)\n\n    true_df = train_df[target_columns + ['sig_id']].reset_index(drop=True)\n    true_df = true_df.sort_values(by=['sig_id'])\n    true_df = true_df.drop('sig_id', axis=1)\n\n    predicted_values = predicted_df.values\n    true_values = true_df.values\n    \n    score = 0\n    loss_per_class = []\n    for moa_idx in range(predicted_values.shape[1]):        \n        _score = log_loss(true_values[:, moa_idx].astype(np.float), predicted_values[:, moa_idx].astype(np.float), eps=1e-15, labels=[1,0])\n        _score = _score \/ predicted_values.shape[1]\n        score += _score\n        loss_per_class.append(_score)\n        \n\n    return score, loss_per_class\n\n\ndef calculate_log_loss_per_row(predicted_df, train_df, target_columns):\n    predicted_df = predicted_df[target_columns + ['sig_id']].reset_index(drop=True)\n    predicted_df = predicted_df.sort_values(by=['sig_id'])\n    \n    sig_ids = predicted_df.sig_id.values\n    \n    predicted_df = predicted_df.drop('sig_id', axis=1)\n\n    true_df = train_df[target_columns + ['sig_id']].reset_index(drop=True)\n    true_df = true_df.sort_values(by=['sig_id'])\n    true_df = true_df.drop('sig_id', axis=1)\n    \n    predicted_values = predicted_df.values\n    true_values = true_df.values\n    \n    eps = 1e-15\n    pred_cliped = np.clip(predicted_values, eps, 1 - eps)\n    pred_cliped[true_values == 0] = 1 - pred_cliped[true_values == 0]\n    loss_per_prediction = -np.log(pred_cliped)\n    \n    loss_per_prediction \/= true_values.shape[0] * true_values.shape[1]\n        \n    loss_per_prediction_df = pd.DataFrame(data=loss_per_prediction, columns=[f'{c}_loss' for c in target_columns])\n    loss_per_prediction_df['sig_id'] = sig_ids\n    loss_per_prediction_df['sample_loss'] = loss_per_prediction.sum(axis=1)\n    \n    return loss_per_prediction_df","5e4b31c4":"train_losses = calculate_log_loss_per_row(train_pred, train_data, target_columns)\ntotal_loss, loss_per_class = calculate_log_loss(train_pred, train_data, target_columns)","c4fc86fa":"print(f'Train loss: {total_loss}')","f243fcb6":"train_data = pd.merge(train_data, train_losses, on=['sig_id'])\ntrain_pred.columns = [v if v == 'sig_id' else f'{v}_pred' for i, v in enumerate(train_pred.columns)] \ntrain_data = pd.merge(train_data, train_pred, on=['sig_id'])","00d43db6":"data = []\n\nfor i in [0, 1, 2, 3, 4, 5, 7]:\n    temp_mean = train_data[train_data.activated_moas == i][target_columns_pred].sum(axis=1).mean()\n    data.append([i, temp_mean])\n    \ndf = pd.DataFrame(data=data, columns=['activated_moas', 'average_prediction_per_sample'])\nfig = px.line(df, x=\"activated_moas\", y=\"average_prediction_per_sample\", title='Average prediction sum per samples')\nfig.show()","87a3a126":"loss_per_moa = train_data[target_columns_loss].values.sum(axis=0)\nloss_per_moa_idx = loss_per_moa.argsort()[::-1]\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=[target_columns[i] for i in loss_per_moa_idx], y=[loss_per_moa[i] for i in loss_per_moa_idx],\n                    mode='lines',\n                    name='Loss per moa'))\n\nfig.show()","36b3c2df":"loss_per_moa_activations = train_data.groupby('activated_moas')['sample_loss'].sum()\n\ndf = pd.DataFrame(data=[[i, (v \/ total_loss) * 100] for i, v in loss_per_moa_activations.items()], columns=['activated_moas', 'loss_percentage'])\nfig = px.bar(df, x=\"activated_moas\", y=\"loss_percentage\")\nfig.show()","41567f78":"grouped_by_activations = train_data.groupby(['activated_moas'])[target_columns_pred + target_columns_loss].sum()\n\ndata = [] # active moas, moa_id, no_train_samples, pred_value, loss_value\nfor i, row in grouped_by_activations.iterrows():\n    for j, c in enumerate(target_columns_pred):\n        train_len = len(train_data[(train_data[target_columns[j]] == 1) & (train_data.activated_moas == i)])\n        data.append([i, target_columns[j], train_len, row[c], row[target_columns_loss[j]]])\n        \ngrouped_by_activations_df = pd.DataFrame(data=data, columns=['activated_moas', 'moa_name', 'train_examples', 'pred_value', 'loss_value'])\n\nmost_active_moas = train_data[target_columns_pred].values.sum(axis=0)\n\ntrain_samples_per_moa = train_data[target_columns].values.sum(axis=0)\nmost_active_moa_samples_ids = train_samples_per_moa.argsort()[::-1]\nmost_active_moa_names = [target_columns[i] for i in most_active_moa_samples_ids]\n\nfig = go.Figure()\n\nfig.add_trace(go.Scatter(x=most_active_moa_names, y=[most_active_moas[i] for i in most_active_moa_samples_ids],\n                    mode='lines',\n                    name='Sum of MOA activations'))\n\nfig.add_trace(go.Scatter(x=most_active_moa_names, y=[train_samples_per_moa[i] for i in most_active_moa_samples_ids],\n                    mode='lines',\n                    name='Number of train samples'))\n\nfig.show()","8c36e675":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=most_active_moa_names, y=[loss_per_moa[i] for i in most_active_moa_samples_ids],\n                    mode='lines',\n                    name='Loss per moa'))\n\nfig.show()","812840f4":"fig = px.scatter(grouped_by_activations_df[grouped_by_activations_df.activated_moas > 0], x=\"train_examples\", y=\"loss_value\", size=\"pred_value\", color='moa_name')\nfig.show()","cc232023":"df = grouped_by_activations_df[grouped_by_activations_df.moa_name.isin(most_active_moa_names[:16])].reset_index(drop=True)\nfig = px.bar(df, x=\"moa_name\", y=\"loss_value\", color=\"activated_moas\", title='15 most active MOAs')\nfig.show()","9a32b287":"sns.distplot(train_data['proteasome_inhibitor_pred'], color='Red')\nsns.distplot(train_data['cyclooxygenase_inhibitor_pred'], color='Blue')","d77e3ead":"#Change this to point to your submission\nsubmission = pd.read_csv('..\/input\/moa-pytorch-nn-starter\/submission.csv')","181226f0":"\npredictions_per_moa = submission[target_columns].values.sum(axis=0) \nvalid_predictions_per_moa = train_data[target_columns_pred].values.sum(axis=0)\n\npredictions_per_moa_mean = submission[target_columns].values.mean(axis=0) \nvalid_predictions_per_moa_mean = train_data[target_columns_pred].values.mean(axis=0)\n","460a4cd5":"data = [(i, v) for i, v in enumerate(sorted(submission[target_columns].sum(axis=1)))]\n    \ndf = pd.DataFrame(data=data, columns=['row_number', 'average_prediction_per_sample'])\nfig = px.line(df, x=\"row_number\", y=\"average_prediction_per_sample\", title='Sum prediction per samples')\nfig.show()","26abcb10":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=most_active_moa_names, y=[predictions_per_moa[i] for i in most_active_moa_samples_ids],\n                    mode='lines',\n                    name='Predictions in test data'))\n\nfig.add_trace(go.Scatter(x=most_active_moa_names, y=[valid_predictions_per_moa[i] for i in most_active_moa_samples_ids],\n                    mode='lines',\n                    name='Predictions in valid data'))\n\nfig.show()","560b7028":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=most_active_moa_names, y=[predictions_per_moa_mean[i] for i in most_active_moa_samples_ids],\n                    mode='lines',\n                    name='Mean predictions in test data per moa'))\n\nfig.add_trace(go.Scatter(x=most_active_moa_names, y=[valid_predictions_per_moa_mean[i] for i in most_active_moa_samples_ids],\n                    mode='lines',\n                    name='Mean predictions in valid data per moa'))\n\nfig.show()","d2148a4c":"# Grouping the predictions by the number of activated MOAs, how much is the influence of each group in the final loss ?","71707cfc":"# How is the validation loss distributed through the most active MOAs ?","a46c83db":"Lets plot the distributions of the predictions for **proteasome_inhibitor** and **cyclooxygenase_inhibitor** since those two MOAs have lowest and highest loss value respectively.","61449306":"Lets see the losses of the most active MOAs with respect to the activated moas in the samples.","3707c8da":"Here I think that the **'Sum of MOA activations'** line is better to be below the **'Number of train samples'** because the loss penalizes a lot the confident incorrect predictions.","2b27d75a":"# Average predictions per samples with same number of activated moas\n\n","2171c085":"# Notebook goal\n\nThe goal of this notebook is to provide basic information about the model predictions for the training dataset. \n\nIt should be easily extendable, meaning that you can just plug your train_predictions.csv file that contains all the **target columns** + **sig_id** and you should get all the visualization.\n\nI am using [this](https:\/\/www.kaggle.com\/yasufuminakama\/moa-pytorch-nn-starter) public notebook as an example.\n\nThis notebook tries to address the following questions:\n\n* Average predictions per samples with same number of activated moas\n* Which MOAs have the highest loss ?\n* Grouping the predictions by the number of activated MOAs, how much is the influence of each group in the final loss ?\n* What is the relation between the predicted sum of MOA activations and the actual one ?\n* What is the relation between the loss of the most active MOAs and the number of training samples for each MOA ?\n* How is the loss distributed through the most active MOAs ?","86f3363f":"My intuition here is that trend should be linear and the differences between two consecutive *activated_moas* point should be roughly the same.\n* We can see that the model is having hard time predicting samples that have 4 or 7 activated moas.\n* Maybe the difference between average prediction with 0 and 1 MOA activations is too small ?","35404599":"# What is the relation between the predicted sum of MOA activations and the actual one ?","d376a598":"\n\n# Which MOAs have the highest loss ?","67f21196":"* We observe that the model is pretty sure when is predicting **proteasome_inhibitor**. This can be dangerous property since it can be penalized a lot by the loss function.\n* On the other hand, when predicting **cyclooxygenase_inhibitor** the model is never predicting near 1. Honestly, I don't know how to interpret this but one guess is that the loss for this MOA is the highest because the model is thinking a little bit that **cyclooxygenase_inhibitor** is active in every sample and if we sum this across all the samples it adds up a lot.","0660999d":"# Submission Analysis","3926bf60":"# What is the relation between the loss of the most active MOAs and the number of training samples for each MOA ?"}}