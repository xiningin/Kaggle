{"cell_type":{"1b10103d":"code","5591b995":"code","da63143e":"code","5977dc2a":"code","a3d0be11":"code","2f34db54":"code","7315334b":"code","6df5edbb":"code","adec5eee":"code","b6391c80":"code","0be28ba5":"code","2b049d85":"code","2d636fc5":"code","14729f2b":"code","ed34c7d5":"code","76f056b5":"code","ccfaac5e":"code","d84f162d":"code","92d6de74":"code","0e67c824":"code","e443cd54":"code","1641ad3b":"code","dcb86fe4":"code","af820f2c":"code","0ac6ea0e":"code","2450e13e":"code","97d97890":"code","19bdd6c6":"code","c9335967":"code","3cfddef0":"code","6948889e":"code","a359a69d":"code","f53ba7ba":"code","98845b87":"code","135f6506":"code","1c985784":"code","39037a5e":"code","4e4b3cf4":"code","8e85f391":"code","85362470":"code","4d1ce9d1":"code","f5cc5ad3":"code","a74b291f":"code","4e956f4a":"code","d78eeec0":"code","91218ce5":"code","881e6fd4":"code","7703e46c":"code","a8d9ec77":"code","42fcea76":"code","af185133":"code","87e49da5":"code","67299d6f":"code","1f05c524":"code","ebaafa26":"code","b4733514":"code","90502164":"code","e6338b1b":"markdown","1fff8f95":"markdown","b26980eb":"markdown","929e0f32":"markdown","533548e4":"markdown","401d9e6a":"markdown","9722d579":"markdown","3b55ba25":"markdown","35f085b9":"markdown","c66a2e51":"markdown","be89e998":"markdown","934b48f7":"markdown","ea20c4f5":"markdown","471b5067":"markdown","cf7ec4f9":"markdown","e2d921bb":"markdown","d2652538":"markdown","aa60dcb1":"markdown","4ba4fe3d":"markdown","d34fc8c2":"markdown","f0cbbd2c":"markdown","57762061":"markdown","b4c5541d":"markdown","89e8199e":"markdown","8bab208a":"markdown","2263aedf":"markdown","a604e8c7":"markdown","95947b56":"markdown","a66638eb":"markdown","1ff49468":"markdown","d920cbe4":"markdown","dd34e877":"markdown","d7e20853":"markdown","e339dc3b":"markdown","8105dad5":"markdown","7f7640f1":"markdown","093b1fdd":"markdown","cb14e33d":"markdown","d9a20f06":"markdown","8528588a":"markdown","bf8c90b4":"markdown","106c7e47":"markdown","ed8703b8":"markdown","845ce189":"markdown","60b89629":"markdown","5b3fdf28":"markdown","3a5437aa":"markdown","9e88411b":"markdown","2b0df2f9":"markdown"},"source":{"1b10103d":"import numpy as np\nimport pandas as pd\nimport datetime\nfrom scipy.spatial import distance\n\nimport ipywidgets as widgets\nfrom ipywidgets import IntSlider\n\nfrom datascience import *\nfrom datetime import timedelta\nfrom datetime import date\nfrom datetime import datetime\nimport time\n\nfrom IPython.display import display\nfrom IPython.display import HTML\n\n# from __future__ import print_function\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nimport ipywidgets as widgets\n\n%matplotlib inline\nimport matplotlib.pyplot as plots\nplots.style.use('fivethirtyeight')\n\n\nimport locale\n%load_ext line_profiler\nimport os\nimport xlrd as xlrd","5591b995":"raw_directory = \"..\/input\/\"\n\n# Declare column names to allow for auto completion :)\n\nCOL_SALE_DATE = 'SALE DATE'\nCOL_PURCHASE_DATE = 'PURCHASE DATE'\nCOL_SOLD_DATE = 'SOLD DATE'\nCOL_PURCHASE_PRICE = 'PURCHASE PRICE'\nCOL_SOLD_PRICE = 'SOLD PRICE'\nCOL_FULL_ADDRESS = 'FULL ADDRESS'\nCOL_PRICE_CHANGE = 'PRICE CHANGE'\nCOL_PERIOD = 'PERIOD'\n\nCOL_SALE_YEAR = 'SALE_YEAR'\nCOL_SALE_MONTH = 'SALE_MONTH'\nCOL_SALE_PRICE = 'SALE PRICE'\n\n# Daily price change column name\nCOL_DAILY_PRICE_CHANGE = 'DAILY PRICE CHANGE'\n\nCOL_PURCHASE_DATE_SU = 'PURCHASE DATE SU'\nCOL_PURCHASE_PRICE_SU = 'PURCHASE PRICE SU'\nCOL_SOLD_DATE_SU = 'SOLD DATE SU'","da63143e":"def standard_units(any_numbers):\n    \"Convert any array of numbers to standard units.\"\n    return (any_numbers - np.mean(any_numbers)) \/ np.std(any_numbers)\n\n\ndef correlation(t, x, y):\n    return np.mean(standard_units(t.column(x)) * standard_units(t.column(y)))\n\n\ndef slope(table, x, y):\n    r = correlation(table, x, y)\n    return r * np.std(table.column(y)) \/ np.std(table.column(x))\n\n\ndef intercept(table, x, y):\n    a = slope(table, x, y)\n    return np.mean(table.column(y)) - a * np.mean(table.column(x))\n\n\ndef fit(table, x, y):\n    a = slope(table, x, y)\n    b = intercept(table, x, y)\n    return a * table.column(x) + b\n\n\ndef residual(table, x, y):\n    return table.column(y) - fit(table, x, y)\n\n\ndef scatter_fit(table, x, y):\n    plots.scatter(table.column(x), table.column(y), s=20)\n    plots.plot(table.column(x), fit(table, x, y), lw=2, color='gold')\n    plots.xlabel(x)\n    plots.ylabel(y)\n\n\n# Helpers Functions\n\ndef print_stats(data):\n    '''Prints common stats for a data array'''\n\n    data_mean = np.mean(data)\n    data_std = np.std(data)\n    data_min = min(data)\n    data_max = max(data)\n\n    percent_5 = percentile(5, data)\n    percent_95 = percentile(95, data)\n    percent_1 = percentile(1, data)\n    percent_99 = percentile(99, data)\n\n    percent_25 = percentile(25, data)\n    percent_50 = percentile(50, data)\n    percent_75 = percentile(75, data)\n\n    print(\"Avg:\", data_mean, \"\\tStd:\", data_std, \"\\tMin:\", data_min, \"\\tMax:\", data_max)\n    print(\" 5%:\", percent_5, \"\\t95%:\", percent_95)\n    print(\" 1%:\", percent_1, \"\\t99%:\", percent_99)\n    print(\"25%:\", percent_25, \"\\t50%:\", percent_50, '\\t75%', percent_75)\n\n\ndef print_col_stats(table, col_name):\n    ''' Print the stats For column named'''\n\n    print(col_name, \"Stats\")\n    data = table.column(col_name)\n    print_stats(data)\n\n\ndef draw_hist(table: Table, col_name, offset_percent=0):\n    ''' Draw a histogram for table with an additional offset percent'''\n    data = table.column(col_name)\n    offset_start = percentile(offset_percent, data)\n    offset_end = percentile(100 - offset_percent, data)\n    table.hist(col_name, bins=np.arange(offset_start, offset_end, (offset_end - offset_start) \/ 20))\n\n\ndef col_stats(table, col_name):\n    ''' Prints state for a column in table'''\n    print_col_stats(table, col_name)\n    draw_hist(table, col_name)\n","5977dc2a":"# List data files and directories in current directory\nexcel_files = os.listdir(raw_directory)\n\n# Select only tje xls files\nexcel_files = [k for k in excel_files if '.xls' in k]","a3d0be11":"# Create an data frame to store\nall_sales_data = pd.DataFrame()\n\n# Load individual excel files. \nfor excel_file in excel_files:\n    print(excel_file)\n    \n    # Read excel, Note the headers could in row 4 or row 5 (index=3 or 4). \n    yearly_sales_data = pd.read_excel(raw_directory+excel_file, header=3, encoding='sys.getfilesystemencoding()')\n   \n    # Check if the first column is \"BOROUGH\"\n    if not yearly_sales_data.columns[0].startswith('BOROUGH'):\n        # Otherwise the data starts from row 5.\n         yearly_sales_data = pd.read_excel(raw_directory+excel_file, header=4, encoding='sys.getfilesystemencoding()')\n    \n    yearly_sales_data.rename(columns=lambda x: x.strip(), inplace=True)\n    \n    all_sales_data = all_sales_data.append(yearly_sales_data)","2f34db54":"all_sales_data.sample(5)","7315334b":"# Check for duplicate entries\nprint('Duplicate rows:', sum(all_sales_data.duplicated(all_sales_data.columns)))\n\n#Delete the duplicates and check that it worked\nall_sales_data = all_sales_data.drop_duplicates(all_sales_data.columns, keep='last')\nsum(all_sales_data.duplicated(all_sales_data.columns))","6df5edbb":"#SALE DATE is object but should be datetime\nall_sales_data[COL_SALE_DATE] = pd.to_datetime(all_sales_data[COL_SALE_DATE], errors='coerce')\nall_sales_data['APARTMENT NUMBER'] = all_sales_data['APARTMENT NUMBER'].astype(str)\n\n# remove additional whitespace in strings\nall_sales_data = all_sales_data.applymap(lambda x: x.strip() if type(x) is str else x)","adec5eee":"all_sales_data = Table.from_df(all_sales_data)","b6391c80":"all_sales_data = all_sales_data.where(COL_SALE_PRICE, are.above(10000))","0be28ba5":"all_sales_data.labels","2b049d85":"# Remove columns we don't actually need. e.g. lot, block etc\n\nall_sales_data = all_sales_data.select(['SALE DATE', 'SALE PRICE', 'ADDRESS','APARTMENT NUMBER', 'YEAR BUILT', 'NEIGHBORHOOD', 'ZIP CODE', 'BUILDING CLASS AT TIME OF SALE', 'BUILDING CLASS CATEGORY'])","2d636fc5":"def combine_address(address, aptNo):\n    \"\"\"Combine the address and Apartment into a single result\"\"\"\n    temp = address.strip()\n    if len(aptNo.strip()) > 0:\n        temp = temp + ', ' + aptNo.strip()\n    return temp\n\nfull_address = all_sales_data.apply(combine_address, ['ADDRESS', 'APARTMENT NUMBER'])\n\n# Add a Full Address column\nall_sales_data =  all_sales_data.with_column(COL_FULL_ADDRESS, full_address)","14729f2b":"all_sales_data.group(['BUILDING CLASS AT TIME OF SALE', 'BUILDING CLASS CATEGORY']).sort('count', descending=True).show(20)","ed34c7d5":"condos = all_sales_data.where('BUILDING CLASS AT TIME OF SALE', are.contained_in(\"R1R2R3R4R6\"))\n\n# Spot Check condo data\ncondos.sample(5)","76f056b5":"condos.group(COL_FULL_ADDRESS).sort(1, descending=True)","ccfaac5e":"multi_sale_condos = condos.where(COL_FULL_ADDRESS, are.containing(',')).group(COL_FULL_ADDRESS).sort(1, descending=True).where('count', are.above(1))\nmulti_sale_condos","d84f162d":"multi_sale_condos = multi_sale_condos.join(COL_FULL_ADDRESS, condos)","92d6de74":"purchase_dates = multi_sale_condos.select(COL_FULL_ADDRESS, COL_SALE_DATE).group([0], min)\nsold_dates = multi_sale_condos.select(COL_FULL_ADDRESS, COL_SALE_DATE).group([0], max)\n\n# Note for the purposes of this analysis, we can ignore any additnal sales between min and max dates\n\n# Spot check data\npurchase_dates.show(5)\nsold_dates.show(5)","0e67c824":"# Update Labels\n\npurchase_dates = purchase_dates.relabel(1, COL_PURCHASE_DATE)\nsold_dates = sold_dates.relabel(1, COL_SOLD_DATE)","e443cd54":"# Join with Condos to get the sale price\npurchase_dates = purchase_dates.join(COL_FULL_ADDRESS, condos, COL_FULL_ADDRESS).where(COL_SALE_DATE, are.equal_to, COL_PURCHASE_DATE)\npurchase_dates = purchase_dates.select( COL_FULL_ADDRESS, COL_PURCHASE_DATE, COL_SALE_PRICE)\n\n\nsold_dates = sold_dates.join(COL_FULL_ADDRESS, condos, COL_FULL_ADDRESS).where(COL_SALE_DATE, are.equal_to, COL_SOLD_DATE)\nsold_dates = sold_dates.select( COL_FULL_ADDRESS, COL_SOLD_DATE, COL_SALE_PRICE)\n","1641ad3b":"purchase_dates.show(5)\nsold_dates.show(5)\n\n# Hmm earlier we had 15871 now we have more! Could we have multiple sale records for the same date??","dcb86fe4":"purchase_dates.groups([COL_FULL_ADDRESS, COL_PURCHASE_DATE]).sort(2, descending=True).where(2, are.above(1))","af820f2c":"condos.where(COL_FULL_ADDRESS, are.equal_to('2 EAST 55 STREET, 921'))","0ac6ea0e":"purchase_dates = purchase_dates.group([COL_FULL_ADDRESS, COL_PURCHASE_DATE], min)\nsold_dates = sold_dates.group([COL_FULL_ADDRESS, COL_SOLD_DATE], max)","2450e13e":"# Relabel and join the first and last sale tables to create a new condo sales table\n\npurchase_dates = purchase_dates.relabel(2, COL_PURCHASE_PRICE)\nsold_dates = sold_dates.relabel(2, COL_SOLD_PRICE)\n\ncondo_sales = purchase_dates.join(COL_FULL_ADDRESS, sold_dates, COL_FULL_ADDRESS)\ncondo_sales","97d97890":"price_diffs = condo_sales.column(COL_SOLD_PRICE) - condo_sales.column(COL_PURCHASE_PRICE)\ndate_diffs = condo_sales.column(COL_SOLD_DATE) - condo_sales.column(COL_PURCHASE_DATE)\n\ndate_diffs = [ d.days for d in date_diffs ]\n\ncondo_sales = condo_sales.with_column( COL_PRICE_CHANGE, price_diffs, COL_PERIOD, date_diffs)\ncondo_sales.set_format(COL_PRICE_CHANGE, NumberFormatter())","19bdd6c6":"col_stats(condo_sales, COL_PRICE_CHANGE)","c9335967":"# strip out the Price chnage outliers . \npercent_1 = percentile(1, price_diffs)\npercent_99 = percentile(99, price_diffs)\n\nlargest_losses = condo_sales.where(COL_PRICE_CHANGE, are.below_or_equal_to(percent_1))\nlargest_gains = condo_sales.where(COL_PRICE_CHANGE, are.above_or_equal_to(percent_99))\n\ncondo_sales = condo_sales.where(COL_PRICE_CHANGE, are.between(percent_1, percent_99))\ncol_stats(condo_sales, COL_PRICE_CHANGE)","3cfddef0":"condo_sales.where( COL_PURCHASE_DATE, are.equal_to, COL_SOLD_DATE )\n","6948889e":"# Let's spot check these\n\nall_sales_data.where(COL_FULL_ADDRESS, are.equal_to('100 CENTRAL PARK SOUTH, 4B')).sort(0)\n\n# These look to be duplicate records, Let's ignore them","a359a69d":"# Ignore multiple sales on same date\ncondo_sales = condo_sales.where( COL_PURCHASE_DATE, are.not_equal_to, COL_SOLD_DATE )","f53ba7ba":"condo_sales.sort(COL_PERIOD)","98845b87":"condo_sales = condo_sales.where( COL_PERIOD, are.above(90) )\ncol_stats(condo_sales, COL_PERIOD)","135f6506":"daily_change = condo_sales.column(COL_PRICE_CHANGE) \/ condo_sales.column(COL_PERIOD) \n\ncondo_sales = condo_sales.with_column(COL_DAILY_PRICE_CHANGE , daily_change ).sort(COL_DAILY_PRICE_CHANGE, descending=True)\ncol_stats(condo_sales, COL_DAILY_PRICE_CHANGE)","1c985784":"# strip out the Dailys Price change outliers. \nprice_change_diffs = condo_sales.column(COL_DAILY_PRICE_CHANGE)\n\npercent_1 = percentile(1, price_change_diffs)\npercent_99 = percentile(99, price_change_diffs)\n\ncondo_sales = condo_sales.where(COL_DAILY_PRICE_CHANGE, are.between(percent_1, percent_99))\n","39037a5e":"years = [ d.year for d in condos.column(COL_SALE_DATE) ]\n\nmonths = [ d.month for d in condos.column(COL_SALE_DATE) ]\n\ncondos = condos.with_column('SALE_YEAR', years, 'SALE_MONTH', months)\n\ncondo_mean = condos.select(COL_SALE_YEAR, COL_SALE_PRICE).group(COL_SALE_YEAR, np.mean).sort(0)\ncondo_mean.plot(COL_SALE_YEAR)\n\n","4e4b3cf4":"neighborhoods = condos.group('NEIGHBORHOOD').sort(0).column(0)\n\ndef plot_neighborhood(neighborhood:str):\n    '''Plot the average sale for a specified neign'''\n    condos.where('NEIGHBORHOOD', are.equal_to(neighborhood)).select(COL_SALE_YEAR, COL_SALE_PRICE).group(0, np.mean).plot(0, label=neighborhood)\n    plots.title = neighborhood\n    print(neighborhood)\n    plots.plot(condo_mean.column(0), condo_mean.column(1), color='gold', label=neighborhood )\n    return\n\n# ignore = interact(plot_neighborhood, neighborhood=neighborhoods)\n","8e85f391":"plot_neighborhood('ALPHABET CITY')\n","85362470":"plot_neighborhood('MIDTOWN EAST')","4d1ce9d1":"sales_2010 = condo_sales.where(COL_PURCHASE_DATE, are.between( datetime(2010, 1, 1), datetime(2010, 12, 31)))\n\nTable().with_columns(\n    'PERIOD',  sales_2010.column(COL_PERIOD), \n    'PRICE CHANGE', sales_2010.column(COL_PRICE_CHANGE)\n).scatter(0, 1, fit_line=True)\n\nprint('Correlation betweeen Price Change and Time: ', correlation(sales_2010, COL_PERIOD, COL_PRICE_CHANGE))","f5cc5ad3":"Table().with_columns(\n    'PURCHASED PRICE',  sales_2010.column(COL_PURCHASE_PRICE), \n    'SOLD PRICE', sales_2010.column(COL_SOLD_PRICE)\n).scatter(0, fit_line=True)\n\nprint('Correlation betweeen Purchase Price and Sold Price: ', correlation(sales_2010, COL_PURCHASE_PRICE, COL_SOLD_PRICE))","a74b291f":"a = slope(condo_sales, COL_PURCHASE_PRICE, COL_SOLD_PRICE)\nb = intercept(condo_sales, COL_PURCHASE_PRICE, COL_SOLD_PRICE)\n\nfirst_prices = condo_sales.column(COL_PURCHASE_PRICE)\n\npredicted = first_prices * a + b\n\nerrors = condo_sales.column(COL_SOLD_PRICE) - predicted\n\n\nTable().with_columns(\n    'PURCHASE PRICE',  condo_sales.column(COL_PURCHASE_PRICE), \n    'ERRORS', errors\n).scatter(0, fit_line=True)\n\n","4e956f4a":"price_band = condo_sales.where(COL_PURCHASE_PRICE, are.between(750000, 1000000))\n\nprice_band.scatter(COL_PURCHASE_PRICE, COL_SOLD_PRICE, fit_line=True)\nprint('Correlation betweeen Purchase Price and Sold Price for apts between 750K-1MM: ', correlation(price_band, COL_PURCHASE_PRICE, COL_SOLD_PRICE))","d78eeec0":"col_stats(condo_sales, COL_PURCHASE_PRICE)","91218ce5":"purchase_prices = condo_sales.column(COL_PURCHASE_PRICE)\n\npercent_1 = percentile(1, purchase_prices)\npercent_99 = percentile(99, purchase_prices)\n\ncondo_sales = condo_sales.where(COL_PURCHASE_PRICE, are.between(percent_1, percent_99))\ncol_stats(condo_sales, COL_PURCHASE_PRICE)","881e6fd4":"percents = sales_2010.column(COL_PRICE_CHANGE) \/ sales_2010.column(COL_PURCHASE_PRICE) * 100\n\nCOL_PRICE_PERCENT = 'PRICE CHANGE %'\n\nsales_2010 = sales_2010.with_column(COL_PRICE_PERCENT, percents)\ncol_stats(sales_2010, COL_PRICE_PERCENT)\n# draw_hist(sales_2010, COL_PRICE_PERCENT, 2)","7703e46c":"# strip out the Price Percent change outliers. \n\nprice_changes = sales_2010.column(COL_PRICE_PERCENT)\n\npercent_2 = percentile(2, price_changes)\npercent_98 = percentile(98, price_changes)\n\nsales_2010 = sales_2010.where(COL_PRICE_PERCENT, are.between(percent_2, percent_98))","a8d9ec77":"sales_2010.scatter(COL_PERIOD, COL_PRICE_PERCENT, fit_line=True)\nprint('Correlation Price Change, Time Period', correlation(sales_2010, COL_PERIOD, COL_PRICE_PERCENT))","42fcea76":"\nperiods = sales_2010.column(COL_PERIOD)\n\nmin_period = min(periods)\nmax_period = max(periods)\n\n\nperiod_groups = []\nperiod_sales = []\n\nfor i in np.arange(min_period, max_period, 30 ):\n    period_groups.append(i)\n    period_sales.append(np.mean(sales_2010.where(COL_PERIOD, are.between(i, i+30)).column(COL_PRICE_PERCENT)))\n    \n\nTable().with_columns(\n    COL_PERIOD,  period_groups, \n    COL_PRICE_PERCENT, period_sales\n).scatter(0)","af185133":"# Trim the outliers. \npercents = condo_sales.column(COL_PRICE_CHANGE) \/ condo_sales.column(COL_PURCHASE_PRICE) * 100\n\ncondo_sales = condo_sales.with_column(COL_PRICE_PERCENT, percents)\n\npercent_1 = percentile(1, percents)\npercent_99 = percentile(99, percents)\n\ncondo_sales = condo_sales.where(COL_PRICE_PERCENT, are.between(percent_1, percent_99))\n\ndef plot_price_change_year(year):\n    ''' Plot the price change % for a given year'''\n    valid_sales = condo_sales.where(COL_PURCHASE_DATE, are.between_or_equal_to( datetime(year, 1, 1), datetime(year+1, 1, 1)))\n\n    min_period = min(periods)\n    max_period = max(periods)\n\n\n    period_groups = []\n    period_sales = []\n\n    for i in np.arange(min_period, max_period, 30 ):\n        period_groups.append(i)\n        period_sales.append(np.mean(valid_sales.where(COL_PERIOD, are.between(i, i+30)).column(COL_PRICE_PERCENT)))\n\n    Table().with_columns(\n        COL_PERIOD,  period_groups, \n        COL_PRICE_PERCENT, period_sales\n    ).scatter(0)\n\n# Removed interact for published\n# _ = interact(plot_price_change_year, year=np.arange(2003,2019) )\n\n","87e49da5":"print('2007')\nplot_price_change_year(2007)\n","67299d6f":"print('2013')\nplot_price_change_year(2012)","1f05c524":"# Convert to standard units\npurchase_dates_timestamps = [ date.timestamp() for date in condo_sales.column(COL_PURCHASE_DATE)]\npurchase_dates_su = standard_units(purchase_dates_timestamps)\n\nsold_dates_timestamps = [ date.timestamp() for date in condo_sales.column(COL_SOLD_DATE)]\nsold_dates_su = standard_units(sold_dates_timestamps)\n\npurchase_price_su = standard_units(condo_sales.column(COL_PURCHASE_PRICE))\n\ncondo_sales_su = condo_sales.with_column(\n    COL_PURCHASE_DATE_SU, purchase_dates_su, \n    COL_PURCHASE_PRICE_SU, purchase_price_su, \n    COL_SOLD_DATE_SU, sold_dates_su,\n)\n\n# Create the training and testing sets.\ntraining_sales, test_sales = condo_sales_su.split(int(condo_sales.num_rows * 0.6))\ntraining_sales\n","ebaafa26":"# Columns used to the calculate the distance between two point i.e. 2 properties that were purchased and sold. \n# We picked the purchase date & price and the sold date converted to standard units as the important columns\n# to use for calculating the distance.\ndistance_columns = [COL_PURCHASE_DATE_SU, COL_PURCHASE_PRICE_SU, COL_SOLD_DATE_SU]\n\ndef all_distances(training, new_point):\n    \"\"\"Returns an array of distances\n    between each point in the training set\n    and the new point (which is a row of attributes)\"\"\"\n    attributes = training.select(distance_columns)\n    return distance.cdist( attributes.to_array().tolist(), [new_point]).flatten()\n\ndef table_with_distances(training, new_point):\n    \"\"\"Augments the training table \n    with a column of distances from new_point\"\"\"\n    return training.with_column('Distance', all_distances(training, new_point))\n\ndef closest(training, new_point, k):\n    \"\"\"Returns a table of the k rows of the augmented table\n    corresponding to the k smallest distances\"\"\"\n    with_dists = table_with_distances(training, new_point)\n    sorted_by_distance = with_dists.sort('Distance')\n    topk = sorted_by_distance.take(np.arange(k))\n    return topk\n\ndef estimate(training, purchase_point, k):\n    \"\"\"Estimate a price based on nearest neighbours\"\"\"\n    close_points = closest(training_sales, purchase_point, k)\n    avg_price_change = np.mean(close_points.column(COL_PRICE_PERCENT))\n    return avg_price_change\n\ndef evaluate_accuracy(training, test, k):\n    \"\"\"Evalute the accuracy of the model generating using training data on test data\"\"\"\n    # select the columns to compare\n    test_attributes = test.select(distance_columns)\n\n    # compute the predicted change for each test row\n    def price_testrow(row):\n        return estimate(training, row, k)\n\n    # Calculate the predicted price and error\n    c = test_attributes.apply(price_testrow)\n    \n    estimated = test[COL_PURCHASE_PRICE] * (100 + c )\/100\n    error = (test[COL_SOLD_PRICE] - estimated  ) \/ test[COL_SOLD_PRICE] * 100\n    \n    return test.with_column(\"Estimated\", estimated, 'Error %', error)","b4733514":"estimates = evaluate_accuracy(training_sales, test_sales, 10)","90502164":"estimates.scatter(COL_SOLD_PRICE, 'Error %')\n\ncol_stats(estimates, \"Error %\")","e6338b1b":"Spot check some duplicate addresses to understand what's going on","1fff8f95":"## Sampling\n\nLet's take a deeper dive at apartment sales in 2010 as a sample. **Note:** we could have selected any range. This is just a random selection to reduce the noise in the data\n\n","b26980eb":"## Cleanup Addresses\n\nThe address data here messy, sometimes the address column contains apartment numbers, other times it's seperated into the Apt No column. **Solution:** Let's create a full address column that combines them into a single address\n","929e0f32":"**Not bad!** 90% of the time we're withing -37% to 28% of the sale price","533548e4":"# Constants\n\nLet's define some constants to help make our code more readable","401d9e6a":"# Further Explorations\n\nIt would be interesting furhter explore this data by:\n\n1. Applying different weights to the distanace calculations. e.g. The purchase price is  more important than dates. \n2. Including physical location when calculating the distance between two properties. \n\nAny other suggestions or thoughts? Please let me know in the comments section below. \n\nThank you for reading, please upvote! :) ","9722d579":"Hmm, apt numbers are missing for lot of the  sales. In order to focus on a typical NYC apartment let's ignore anything without an apt number. i.e. anything without a ',' (comma) in the Full Address","3b55ba25":"Again there are some significant outliers. These could be for a variety of reasons . Perhaps they underpriced for when purchased and then corrected when sold later. Again I'd be intresting to investigate further, but for the purposes of this analysis let's ignore the significant outliers. ","35f085b9":"# Price Predictor\n\nCheck out the interactive  [Price Predictor](https:\/\/colab.research.google.com\/github\/somya\/nyc_apt_price_predictor\/blob\/master\/index.ipynb#scrollTo=qvHipZ9AG27O) based on this approach. **Note** You will need to run the linked notebook using your google account.","c66a2e51":"There are signifiant outliers here. The 99 percentile is 5MM but the max is 44 MM a significant outlier. The min value is -183MM, i.e. a 183 million dollor loss! \n\nLet's remove these outliers so they dont impact pricing analysis, but we definitely need to come back and look into what happened here. ","be89e998":"## Average Daily Price Change\n\nLet's calulate the average daily price change to spot any other odd data. ","934b48f7":"# Step 1: Import data\n\nThis kernel uses data available from NYC Open Data:\n\n1. [Annualized Sales Data](https:\/\/www1.nyc.gov\/site\/finance\/taxes\/property-annualized-sales-update.page)\n2. [Rolling Sale data](https:\/\/www1.nyc.gov\/site\/finance\/taxes\/property-rolling-sales-data.page)","ea20c4f5":"The residuals are not evenly spread out, also some large outliers are skewing the results so a liner regression model isn't the right approach here.\n\nLet's take a deeper look into a particular price bands to see what could be going on. ","471b5067":"Let's look into the correlation betweeen the change in price and the time between sales","cf7ec4f9":"## Time period between sales","e2d921bb":"# Step 3: NYC Sale data analysis","d2652538":"Wow! that's a really high correlation. According to this we could predict the last sale price of a property, just based on it's first sale price. i.e. independent of the time between sales! \n\nSomehting doesn't seem right, we know market moves over time. Let's dig in a little deeper and plot the residuals.","aa60dcb1":"## Price Change\nCalculate Price and date diffs","4ba4fe3d":"Now let's find condos with multiple sales, so we can start to build a picture of how prices have changed over time.","d34fc8c2":"## Time between sales\n\nLooking into the time between a purchase and sale for the same apartment we find some unusal data. There are at times only 1 day between when a proprty was bought and sold. ","f0cbbd2c":"There could be a number of things going on.  Instead of speculating, let's just take the min and max value for each date to keep moving along.\n","57762061":"Convert the data frame into a datacience Table for analysis","b4c5541d":"Let's review  the data","89e8199e":"## Sales by neighbourhood \n\nAlso helpful to usederstand how prices have changed in different neighborhoods. ","8bab208a":"Let's look that how this algorithm performs","2263aedf":"There are a lot less records with apt numbers, but still roughly 16K records, enough to proceed.\n\nLet's define a new table for condos with multiple sales. ","a604e8c7":"We can now say there is a a correlation between the price change and time reflective of the overall movement in the market over time. \n\nLet's make some predictions. \n\n# Step 4: Predicting NYC property prices","95947b56":"## Prediction model: nearest neighbor\n\nOur prediction model should use both the purchase price and date for prediction. Let's create it now.\n\nWe will first convert purchase date, price and sold dates into standard units so they can be used to compute distance. \n\nThen we will create the training and testing set.","a66638eb":"Let's remove the ouliers so we can get a better picture of the data. ","1ff49468":"# Predicting NYC apartment's value with open data\n\nAny good property broker in NYC will tell you, apartment valuations are based on a variety of things, most importantly:\n\n1. Recent sales in the building \/ neighbourhood\n2. Price per sq\/ft for recent sales\n3. Renovation status\n4. Views, closeness to subway, # of bedrooms etc. \n\nUnfortunately this data does not come easily. It's available on Streeteasy and other [REBNY](https:\/\/www.rebny.com\/) members but not downloadable for us data scentists! :)\n\nHere I present an alternative approach to pricing NYC apartments using only public data from [NYC Open Data](https:\/\/opendata.cityofnewyork.us\/). \n\n**tl;dr** Boldly predict a NYC apartments market value using only public \/ open data. Check out the [Price Predictor](https:\/\/colab.research.google.com\/github\/somya\/nyc_apt_price_predictor\/blob\/master\/index.ipynb#scrollTo=qvHipZ9AG27O) \n","d920cbe4":"much better!","dd34e877":"Mostly condos and co-ops as expected\n\n**Question** what's R5 -  COMMERCIAL CONDOS? We'll be ignoring these for now for now, Let's pick out the condos for now. \nReference Data: [NYC Building Codes](https:\/\/www1.nyc.gov\/assets\/finance\/jump\/hlpbldgcode.html) \n\nLet's focus on condos for now","d7e20853":"## Duplicates","e339dc3b":"Yep duplicate sales on the same day!","8105dad5":"# Functions\n\nAlso lets define some functions to assist in our analysis","7f7640f1":"Unfortunately not all the data files have the same format. Some have the header in row 4, others in row 5. We can check by making sure 'BOROUGH' is the first column in the imported dataset ","093b1fdd":"Understand the data labels we might be interested in","cb14e33d":"Let's dive into price change data","d9a20f06":"# Price change % \n\nNow let's calculate the price change as a % of the purchase price. \nLooking into how  the price change % data is distributed, again we need to filter out the outliers.","8528588a":"# Understanding the data\n\nLet's try to get an overview of the data by looking at the movement of the average sale price per year.","bf8c90b4":"## Building Codes\nLet's understand  Building Class Codes. What are the most common codes?","106c7e47":"# Step 2. Clean up the data and arrange for analysis","ed8703b8":"The data is starting to look a lot more reasonable now. Although, there are still some sales that are on recorded twice for the same date!","845ce189":"Remove any sales less than $10,000 they are likely to be a property transfer rather than an actual sale that we're interested in.","60b89629":"For most regular sale cycles we expect a gap of atleast 60-90 days. Let's ignore anything less that 3 months apart i.e. 90 days. \n","5b3fdf28":"Now the correlation is much lower, The data is more spread out. Also a average seems to skew a little higher do the outliers a visible in the chart.\n\n## Removing Outliers\n\nLet's look at the purchase price outliers","3a5437aa":"Defintitely shows a trend! \n\nLet's take a look at other years","9e88411b":"That's a low correlation, I was expectating a closer relationship that is roghly keeping tracking with avg overall sale price we plotted earlier.\n\n\nOk, let's look at the correlation between the first and last sale price. ","2b0df2f9":"There look to be an upward trend here and price changes increase over time. However, the correlation isn't linear. Smoothing the data into monthly (30) intervals look like the following"}}