{"cell_type":{"adf4b670":"code","0b1829eb":"code","287064a3":"code","4c68cb83":"code","1664db77":"code","0cfc6863":"code","13739cf8":"code","a4acd3ac":"code","7d700ea0":"code","7fc1b876":"code","64795418":"markdown","a6872aca":"markdown","72e5c339":"markdown","e3ad4cfc":"markdown","36c8e8fe":"markdown","cff58cd4":"markdown","914b3c65":"markdown","a0dc83ac":"markdown","21170a23":"markdown","c17d9094":"markdown","a2b0f5df":"markdown","180978cb":"markdown","9a018344":"markdown"},"source":{"adf4b670":"# Import libraries\nimport random as rd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom math import sqrt\nrandomSeed = \"0\"\nrd.seed(randomSeed)","0b1829eb":"class Bandit():\n    def __init__(self, distribution_method, lowerLimit, upperLimit):\n        self.distribution_methode = distribution_method\n        self.lowerLimit= lowerLimit\n        self.upperLimit = upperLimit\n        self.Q = 0\n        self.N = 0\n    def get_value(self):\n        R = rd.uniform(self.lowerLimit, self.upperLimit)\n        #R = rd.gauss(self.lowerLimit, self.upperLimit) #mu, sigma\n        #R = rd.expovariate(2\/(self.lowerLimit+self.upperLimit))\n        self.N += 1\n        self.Q += 1\/self.N* (R - self.Q)\n        return self.Q\n    def setQ(self, Q):\n        self.Q = Q\n    \n    def get_N(self):\n        return self.N\n    def get_averageQ(self):\n        return self.Q\n        ","287064a3":"def epsilonGreedy(eps, n):\n    #BanditList = [Bandit(0,-3,5), Bandit(0,2,8), Bandit(0,4,10), Bandit(0,3,7), Bandit(0,-3, 2)]\n    BanditList = [Bandit(0,-3,5), Bandit(0,2,8), Bandit(0,4,10), Bandit(0,3,7), Bandit(0,-3, 2)]\n    T = []\n    M = []\n    acc = 0\n    for j in range(len(BanditList)):\n        T.append(BanditList[j].get_averageQ())\n    for i in range(n): \n        p = rd.random()\n        if p > eps:\n            k = T.index(max(T))\n            T[k] = BanditList[k].get_value()\n            acc+= T[k]\n        else:\n            x = rd.randint(0, len(BanditList)-1)\n            T[x] = BanditList[x].get_value()\n            acc+= T[x]\n        M.append(acc\/(i+1))\n    print(T)\n    return M\n","4c68cb83":"arg_eps = [0,0.01,0.05,0.2,0.5]\nfor i in arg_eps:\n     rd.seed(randomSeed)\n     plt.plot([k for k in range(1000)], epsilonGreedy(i,1000),label=str(i)) \nplt.legend(loc=\"lower right\") ","1664db77":"def optimisticEpsGreedy(eps, n, Q):\n    BanditList = [Bandit(0,-3,5), Bandit(0,2,8), Bandit(0,4,10), Bandit(0,3,7), Bandit(0,-3, 2)]\n    for i in range(len(BanditList)):\n        BanditList[i].setQ(Q)\n    T = []\n    M = []\n    acc = 0\n    for j in range(len(BanditList)):\n        T.append(BanditList[j].get_averageQ())\n    for i in range(n): \n        p = rd.random()\n        if p > eps:\n            k = T.index(max(T))\n            T[k] = BanditList[k].get_value()\n            acc+= T[k]\n        else:\n            x = rd.randint(0, len(BanditList)-1)\n            T[x] = BanditList[x].get_value()\n            acc+= T[x]\n        M.append(acc\/(i+1))\n    print(T)\n    return M","0cfc6863":"arg_opt_eps = [0,0.01,0.05,0.2,0.5]\nfor i in arg_opt_eps:\n     rd.seed(randomSeed)\n     plt.plot([k for k in range(1000)], optimisticEpsGreedy(i,1000, 5),label=str(i)) \nplt.legend(loc=\"lower right\") ","13739cf8":"arg_opt_eps = [0,1,2,4,5]\nfor i in arg_opt_eps:\n     rd.seed(randomSeed)\n     plt.plot([k for k in range(1000)], optimisticEpsGreedy(0.2,1000, i),label=str(i)) \nplt.legend(loc=\"lower right\") ","a4acd3ac":"def upperConfidenceBound(n, c):\n    BanditList = [Bandit(0,-3,5), Bandit(0,2,8), Bandit(0,4,10), Bandit(0,3,7), Bandit(0,-3, 2)]\n    T = []\n    L = []\n    M = []\n    acc = 0\n    for j in range(len(BanditList)):\n        T.append(BanditList[j].get_averageQ())\n        L.append(BanditList[j].get_averageQ())\n    for i in range(n): \n        for j in range(len(BanditList)):\n            if BanditList[j].get_N() == 0:\n                L[j]= T[j] + 2**64\n            else:\n                L[j]= T[j] + c*sqrt(np.log(i+1)\/BanditList[j].get_N())\n        k = L.index(max(L))\n        T[k] = BanditList[k].get_value()\n        acc+= T[k]\n        M.append(acc\/(i+1))\n    print(T)\n    return M\n","7d700ea0":"arg_c = [0,0.01,0.05,2,5]\nfor i in arg_c:\n     rd.seed(randomSeed)\n     plt.plot([k for k in range(1000)], upperConfidenceBound(1000, i),label=str(i)) \nplt.legend(loc=\"lower right\") ","7fc1b876":"# Plot Epsilon Greedy\nrd.seed(\"0\")\nplt.plot([k for k in range(1000)], epsilonGreedy(0.2,1000),label='Epsilon Greedy') \n# Plot Optimistic Epsilon Greedy \nrd.seed(\"0\")\nplt.plot([k for k in range(1000)], optimisticEpsGreedy(0.2,1000, 6),label='Optmistic Epsilon Greedy')     \n# Plot Upper Confidence Bound \nrd.seed(\"0\")\nplt.plot([k for k in range(1000)], upperConfidenceBound(1000, 0.02),label='UCB')\n\nplt.legend(loc='best')","64795418":"## Conclusion","a6872aca":"> ## Initialization of framework ","72e5c339":"### Test of Algorithm 3 - c","e3ad4cfc":"## Algorithm 1 - Epsilon Greedy\n","36c8e8fe":"### Test of Algorithm 2 - Bias","cff58cd4":"## Algorithm 2 - Optimistic Epsilon Greedy","914b3c65":"### Test of algorithm 1 - Epsilon","a0dc83ac":"Algorithm 1: \n- The algorithm needs to explore inorder to get be efficient.\n\nAlgorithm 2: \n- More effecient, if you know which value Q converges to. \n\nAlgorithm 3: \n- Works well for uniform distribution. Not for other distributions (gaussian and expovariate). \n\nComparison: \n- Performance of algorithms changes according to distribution.","21170a23":"### Test of Algorithm 2 - epsilon","c17d9094":"## Exercise 1\n\nGroup Members:\n- Christian Eberhardt\n- Eliott Pierret\n- Jakob Gr\u00f8ftehauge\n\nThe scope of this exercise is to explore different reinforcement learning algorithms and the effect of hyperparameters. In the end a comparising between the algorithms will be carried out. \n\n","a2b0f5df":"## Algorithm 4","180978cb":"## Performance comparision","9a018344":"## Algorithm 3 - Upper Confidence Bound"}}