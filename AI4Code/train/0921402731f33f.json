{"cell_type":{"b402785d":"code","16e9aa0e":"code","075889fc":"code","d97f58ee":"code","22357d37":"code","a925a5b4":"code","19c358d4":"code","e8f84b73":"code","c948df1a":"code","8b55d0fa":"code","7bce4d4b":"code","fb5d9694":"code","ac4eb69d":"code","543f03e8":"markdown","3ab4e51d":"markdown","216bfdfd":"markdown","9e77cb5b":"markdown","55a6f380":"markdown","72053b19":"markdown","5fdff1b2":"markdown","ad6cd60c":"markdown","ca39ee05":"markdown","fe89e73c":"markdown","aae7c3a1":"markdown","2fd5e1dd":"markdown","bc2164d7":"markdown","aea82ec5":"markdown","4aa153ca":"markdown","5f5aa574":"markdown","4ed463f6":"markdown","6b13edee":"markdown","66026c51":"markdown","e6be132d":"markdown"},"source":{"b402785d":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd","16e9aa0e":"#uploading data\ndf = pd.read_csv('..\/input\/missin-data\/missing_data.csv')\n\nprint(df.head())","075889fc":"df.isnull().sum()\n\n# There are null data in the age column.  ","d97f58ee":"#For missing data, we can use SimpleImputer. \n# For more, https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html\n\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nage = df.iloc[:,1:4].values\n\nimputer = imputer.fit(age[:,1:4])\n\nage[:,1:4] = imputer.transform(age[:,1:4])\n\n#With indicating mean strategy, it will put the mean of age column to the null data. \nprint(np.isnan(age).sum())\n#As we expect, we will see zero.","22357d37":"from sklearn import preprocessing\n\ncountry = df.iloc[:,0:1].values\n\nLE = preprocessing.LabelEncoder()\n\ncountry[:,0] = LE.fit_transform(df.iloc[:,0])\n\nprint(country)\n","a925a5b4":"OHE = preprocessing.OneHotEncoder()\ncountry  = OHE.fit_transform(country).toarray()\nprint(country)","19c358d4":"gender = df.iloc[:,4:].values\n\nOHE = preprocessing.OneHotEncoder()\ngender  = OHE.fit_transform(gender).toarray()\nprint(gender)","e8f84b73":"len(country)\nprint(country.shape)","c948df1a":"result = pd.DataFrame(data=country, index = range(22), columns = ['fr','tr','us'])\nprint(result)\n\nresult_2 = pd.DataFrame(data=age, index = range(22), columns = ['height','weight','age'])\nprint(result_2)\n\nresult_3 = pd.DataFrame(data=gender, index = range(22), columns = [\"male\",\"female\"])\nprint(result_3)\n\n#Data Concating\n\nr_1=pd.concat([result,result_2], axis=1)\nprint(r_1)\n\nr_2=pd.concat([r_1,result_3], axis=1)\nprint(r_2)","8b55d0fa":"from sklearn.model_selection import train_test_split\n\nx_train, x_test,y_train,y_test = train_test_split(result,result_3,test_size=0.33, random_state=0)","7bce4d4b":"from sklearn.linear_model import LinearRegression\n\nLR = LinearRegression()\nLR.fit(x_train,y_train)\n\ny_pred = LR.predict(x_test)\n\nheight = r_2.iloc[:,3:4].values\nprint(height)\n\n# r_2 was the last DataFrame that we concatted.\n","fb5d9694":"left = r_2.iloc[:,:3]\nright = r_2.iloc[:,4:]\n\ndata = pd.concat([left,right],axis=1)\n\nx_train, x_test,y_train,y_test = train_test_split(data,height,test_size=0.33, random_state=0)\n\n\nr2 = LinearRegression()\nr2.fit(x_train,y_train)\n\ny_pred = r2.predict(x_test)\n\n# Elimanation method starts from here\n\nimport statsmodels.api as sm\n\n# We assign one to all values.\n\nX = np.append(arr = np.ones((22,1)).astype(int), values=data, axis=1)\n\nX_l = data.iloc[:,[0,1,2,3,4,5]].values \n# We can develop the best model by making changes in this part.\n\nX_l = np.array(X_l,dtype=float)\nmodel = sm.OLS(height,X_l).fit()\nprint(model.summary())\n\n\nX_l = data.iloc[:,[0,1,2,3,5]].values\nX_l = np.array(X_l,dtype=float)\nmodel = sm.OLS(height,X_l).fit()\nprint(model.summary())\n\n\n\nX_l = data.iloc[:,[0,1,2,3]].values\nX_l = np.array(X_l,dtype=float)\nmodel = sm.OLS(height,X_l).fit()\nprint(model.summary())\n\n# This process continues until all values are less than the P value.\n# We should check P>|t|","ac4eb69d":"# Ex Code\n\nfrom sklearn.model_selection import GridSearchCV\nparameters = {\"var_smoothing\":[1e-9, 1e-5, 1e-1]}\ngs_clf = GridSearchCV(\n        GaussianNB(), parameters)\ngs_clf.fit(X_train.toarray(),y_train)","543f03e8":"> \"Setting the optimal values of the hyper-parameters can be challenging and resource-demanding.  Imagine how many permutations we need to determine the best parameter values. Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters. \" ","3ab4e51d":"Before the train test split let's look at some theoretical part more!\n\nWhat is Linear Regression?\n\n","216bfdfd":"# Forecasting vs Prediction #\n\n**Forecasting**: Foresight has no examples yet and you guess it.\n\n**Prediction**: Predicting historical data. There is missing data, for example, someone does not have a salary in the data set, such as it's estimation. Finding incorrect data can be also mentioned as prediction. ","9e77cb5b":"# Models: # \n\n1. Prediction\n2. Classification\n3. Clustering\n4. ARM\n5. Reinforced Learning\n6. Text Processing\n\n**Extras:**\n\n7. Deep Learning\n8. Dimensional reduction (PCA etc.) \n\n","55a6f380":"# Backward Elimination\n\n\n![image.png](attachment:8e0c1363-e6cb-44f1-a726-7d07c594b442.png)\n\n\nSome variables have a high effect on the system, while some are less. Elimination of some variables that have little effect on the system allows us to build a better model. We can create better models using the Backward Elimination method.\n\n* Choose the significance level (SL).(Usually this value will be 0.05)\n* Build a model in which you include all arguments.\n* If the P-value is greater than that specified for the model (P>SL), this independent variable is removed from the model. Run it again.\n* If all p values are less than the value we have determined, our model is ready.\n\n\nThere can be different approaches:\n\n1. Backward Elimination\n2. Forward Selection \n3. Bidirectional Elimination\n4. Score Comparison\n\n\n","72053b19":"# **Using Scikit Learn library for Missing Data**","5fdff1b2":"# Data Types # \n<img src=\"https:\/\/bookdown.org\/ejvanholm\/Textbook\/images\/DataTypes.png\" width=\"400\">","ad6cd60c":"# p-value\n\nH0: Null Hypothesis \nH1: Alternative Hypothesis\np-value : probability value ( generally we take 0.05)\n\n**The smaller the P value, the more likely it is to be H0 erroneous.**\n\n**The greater the P value, the more likely it is to be H1 erroneous.**","ca39ee05":"# Grid Search","fe89e73c":"# **-Examples of ML usage** # \n*Object Recognition (gender, child coming to the market, etc.)\n\n*Image processing (11 vehicles, 5 pedestrians etc.) >> Deep learning is especially used.\n\n*Face recognition.\n\n*VR\n\n*Intelligent devices, robots and their movements.\n\n*Marketing (advertisements etc.)\n\n*Recommendation algorithms\n\n*Health >> linked to what gene on the gene.\n\n*Connected cars are convenient parking --- objects will speak --- warns of collapsed trees.\n","aae7c3a1":"**Multiple Linear Regression**\n\n![image.png](attachment:e92a843d-6f7c-48f9-b5c2-4863fdee04ff.png)\n\n*  **Eg: height = b0 + b(weight) + b2(age) + b3(shoes size) + e**","2fd5e1dd":"# What is Dummy Variables?\n\nk = Female\ne = Male\n\nIt may fail in algorithm structures, though not always. Dummy variable would be a trap here. Why? If K is 0 then E means 1. Everything gets mixed up, even if you go and use gender both e and k. Your data is important to use cleanly!","bc2164d7":"# What is Linear Regression \n\n\n**Simple Linear Regression**\n\n![image.png](attachment:a992538c-a8b5-4207-bb7d-e0820e874b18.png)\n\n* **Eg: Sales = b0 + b(month)+e**\n\n","aea82ec5":"# What is methodolgy? # \n<img src=\"https:\/\/storage.ning.com\/topology\/rest\/1.0\/file\/get\/2808314343?profile=original\" width=\"400\">","4aa153ca":"It is the kernel that I have tried and compiled from the courses of Udemy Machine Learning with Python (22 hours) Prof. Dr. Sadi Evren Seker and original library documents that I use every time. I'll also create ML-2 Notebook to continue more.\n\n**If you like this kernel please share your valueable response.**","5f5aa574":"# **Label Encoder vs OneHot Encoder vs Ordinal Encoder** \nAs indicated at sklearn documentary: \n\n**Label Encoder**\nEncode target labels with value between 0 and n_classes-1.\n\n**OrdinalEncoder**\nEncode categorical features using an ordinal encoding scheme.\n\n**OneHotEncoder**\nEncode categorical features as a one-hot numeric array. (can be shown with just 1 and 0)","4ed463f6":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n\nContent:\n\n    \n              \n\n<\/div>\n\n   **Part-1**\n    \n    \n              -Examples of ML usage\n    \n              -Methodology\n    \n              -Models \n    \n              -Data Types \n    \n              -Forecasting or Prediction\n              \n              -Using Scikit Learn library for Missing Data\n              \n              -Label Encoder vs OneHot Encoder vs Ordinal Encoder\n              \n              -What is Dummy Variables?\n              \n              -Converting from Numpy Array to DataFrame shape\n              \n              -What is Linear Regression?\n              \n              -p-value\n              \n              -Backward Elimination (with codes)\n              \n\n\n   **Part-2**   \n   \n               - Grid Search\n             ","6b13edee":"**Before how we can use backward elimination with codes, let's do train test split.**","66026c51":"This is just the introduction. I'll share some datasets and codes with the next chapters.","e6be132d":"# Converting from Numpy Array to DataFrame shape\n\n\n"}}