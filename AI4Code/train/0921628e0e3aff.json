{"cell_type":{"249046ec":"code","4bef8c96":"code","bc73fe51":"code","70046465":"code","3c7edeb5":"code","ccb45cc1":"code","24960360":"code","ba6f7a27":"code","642dea7a":"code","d86f0113":"code","9d31294e":"code","772d15d6":"code","0477dbac":"code","1ed012ad":"code","3219412d":"code","f86eaea4":"code","11855141":"markdown","b3b9ec52":"markdown","6e897db6":"markdown","4e848b85":"markdown","776ebeed":"markdown","67de85bf":"markdown","da5c4a99":"markdown","672d5ea4":"markdown","2e2473ea":"markdown","c92e3901":"markdown","bd437acc":"markdown","cc9d00c9":"markdown","0d8decd6":"markdown"},"source":{"249046ec":"from IPython.display import Image\nImage(\"..\/input\/images\/Images\/NN_img2.png\")","4bef8c96":"Image(\"..\/input\/images\/Images\/Layers.jpg\")","bc73fe51":"# import useful packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import OrderedDict","70046465":"class Affine:\n    \"\"\"Define Affine  that runs matrix operations on the inputs, weights and biases\n    \"\"\"\n    def __init__(self, W, b):\n        \"\"\"Initialize parameters\n        Input\n        -----\n        W: np.array\n            an array taht contains the values of each weight edge\n        b: np.array\n            an array that contains the values of each bias edge\n        \"\"\"\n        self.W = W\n        self.b = b\n        self.x = None\n        self.dW = None\n        self.db = None\n    \n    def forward(self, x):\n        \"\"\"Return the Affine transformation of inputs, weights and biases as numpy array\n        \"\"\"\n        self.x = x\n        out = np.dot(x,self.W) + self.b\n        \n        return out\n    \n    def backward(self, dout):\n        dx = np.dot(dout, self.W.T)\n        self.dW = np.dot(self.x.T, dout)\n        self.db = np.sum(dout, axis = 0)\n        \n        return dx","3c7edeb5":"def relu(x):\n    return np.maximum(0, x)\n\nx = np.arange(-5.0, 5.0, 0.1)\ny = relu(x)\nplt.plot(x, y)\nplt.ylim(-1.0, 5.5)\nplt.title('ReLU function')\nplt.xlabel('x')\nplt.ylabel('h(x)')\nplt.show()","ccb45cc1":"class Relu:\n    \"\"\"Define ReLu operation that returns either a positive or 0\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        mask: indicates which input is greater than 0\n        \"\"\"\n        self.mask = None \n        \n    def forward(self, x):\n        self.mask = (x <= 0)\n        out = x.copy()\n        out[self.mask] = 0\n        \n        return out\n    \n    def backward(self,dout):\n        dout[self.mask] = 0\n        dx = dout\n        \n        return dx","24960360":"def softmax(x):\n    \"\"\"Define Softmax operation\n    \"\"\"\n    x = x - np.max(x, axis=-1, keepdims=True)   # prevent overflow by subtracting all inputs by max\n    return np.exp(x) \/ np.sum(np.exp(x), axis=-1, keepdims=True)","ba6f7a27":"def cross_entropy_error (y, t):\n    \"\"\"Define cross entropy operation\n    \"\"\"\n# convert 1-d to 2-d array for various data sizes\n    if y.ndim == 1:\n        t = t.reshape(1, t.size)\n        y = y.reshape(1, y.size)\n\n    # if training data is one-hot, get the index of training label\n    if t.size == y.size:\n        t = t.argmax(axis=1)\n    batch_size = y.shape[0]\n\n    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) \/ batch_size","642dea7a":"class SoftmaxWithLoss:\n    \"\"\"This class defines the last layer of the neural network\n    \"\"\"\n    def __init__(self):\n        \"\"\"Initialize parameters\n        self.loss: store crossentropy error as loss\n        self.y: store softmax output\n        self.t: store the training data as one-hot vector\n        \"\"\"\n        self.loss = None\n        self.y = None\n        self.t = None\n    \n    def forward(self, x, t):\n        self.t = t\n        self.y = softmax(x)\n        self.loss = cross_entropy_error(self.y, self.t)\n        \n        return self.loss\n    \n    def backward(self, dout = 1):\n        batch_size = self.t.shape[0]\n        dx = (self.y - self.t) \/ batch_size\n        \n        return dx","d86f0113":"class Momentum:\n    def __init__(self, lr = 0.01, momentum = 0.9):\n        self.lr = lr\n        self.momentum = momentum \n        self.v = None\n        \n    def update(self, params, grads):\n        if self.v is None:\n            self.v = {}\n            for key, val in params.items():\n                self.v[key] = np.zeros_like(val)\n                \n        for key in params.keys():\n            self.v[key] = self.momentum*self.v[key] - self.lr*grads[key]\n            params[key] += self.v[key]","9d31294e":"class TwoLayerNet:\n    \"\"\"Runs forward and back propagations\n    \"\"\"\n    \n    def __init__(self, input_size, hidden_size, output_size):\n        \"\"\"Initializes the neural network\n        Options:\n        size of the input,hidden and output layers\n        intensity of parameter randomization\n        \"\"\"\n        \n        # initializing parameters. This methos uses a technique called Xavier initialization to improve the learning.\n        self.params = {}\n        self.params['W1'] = np.random.randn(input_size, hidden_size) \/ np.sqrt(input_size)\n        self.params['b1'] = np.zeros(hidden_size)\n        self.params['W2'] = np.random.randn(hidden_size, output_size) \/ np.sqrt(hidden_size)\n        self.params['b2'] = np.zeros(output_size)\n      \n        #generate layers\n        self.layers = OrderedDict() #a dictionary that remembers the order in which the data is stored\n        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n        self.layers['ReLu1'] = Relu()\n        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n        \n        self.lastlayer = SoftmaxWithLoss()\n    \n    def predict(self, x):\n        \"\"\"Call forward propagation for each layer\n        \"\"\"\n        for layer in self.layers.values():\n            x = layer.forward(x)\n            \n        return x\n    \n    def loss(self, x, t):\n        y = self.predict(x)\n        return self.lastlayer.forward(y, t)\n    \n    def accuracy(self, x, t):\n        y = self.predict(x)\n        y = np.argmax(y, axis = 1)\n        if t.ndim!= 1:\n            t = np.argmax(t, axis = 1)\n        \n        accuracy = np.sum(y==t) \/ float(x.shape[0])\n        return accuracy\n    \n    def gradient(self, x, t):\n        #forward\n        self.loss(x, t)\n        \n        #backward\n        dout = 1\n        dout = self.lastlayer.backward(dout)\n        \n        layers = list(self.layers.values())\n        layers.reverse()\n        for layer in layers:\n            dout = layer.backward(dout)\n            \n        grads = {}\n        grads['W1'] = self.layers['Affine1'].dW\n        grads['b1'] = self.layers['Affine1'].db\n        grads['W2'] = self.layers['Affine2'].dW\n        grads['b2'] = self.layers['Affine2'].db\n        \n        return grads","772d15d6":"#load data\ntrain_data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\n\n#get target labels\ntrain_t = pd.get_dummies(train_data.label).values\ntrain_x = train_data.copy().drop('label', axis = 1).values\nprint(train_t.shape, train_x.shape)","0477dbac":"# train the model\nnetwork = TwoLayerNet(input_size = 784, hidden_size = 50, output_size = 10)\niters_num = 10000\ntrain_size = train_x.shape[0]\nbatch_size = 100\nlearning_rate = 0.01\n\ntrain_loss_list = []\ntrain_acc_list = []\ntest_acc_list = []\n\niter_per_epoch = max(train_size \/ batch_size , 1)\n\nfor i in range(iters_num):\n    batch_mask = np.random.choice(train_size, batch_size)\n    x_batch = train_x[batch_mask]\n    t_batch = train_t[batch_mask]\n    \n    grad = network.gradient(x_batch, t_batch)\n    \n    optimizer = Momentum()\n    optimizer.update(network.params, grad)\n        \n    loss = network.loss(x_batch, t_batch)\n    train_loss_list.append(loss)\n    \n    if i % iter_per_epoch == 0:\n        train_acc = network.accuracy(train_x, train_t)\n        train_acc_list.append(train_acc)\n        print(train_acc)","1ed012ad":"#load test data\ntest_x = test_data.values","3219412d":"# example 1\nimg_index = 4\n\nprint(\"Prediction:\", np.argmax(network.predict(test_x[img_index])))\n\nimg = test_x[img_index].reshape(28, 28)\nplt.imshow(np.array(img))","f86eaea4":"# example 2\nnew_index = 1\nprint(\"Prediction:\", np.argmax(network.predict(test_x[new_index])))\n\nimg = test_x[new_index].reshape(28, 28)\nplt.imshow(np.array(img))","11855141":"### 2.1 Affine\n\nAffine is a geometric transformatin by matrix multiplication and vector addition; this layer makes predictions by multiplying inputs and weights, then adding bias term to each node. The partial derivative of the difference between the actual value and the prediction with respect to weights, input and biases are calculated in this layer to revise the parameters.\n\nForward propagation is a composite of inputs, weights, and biases:\n\n\\begin{equation}\n\\boldsymbol{Y} = \\boldsymbol{WX} + \\boldsymbol{b}\n\\end{equation}\n\n\nWith calculus and linear algebra, the partial derivatives of the loss become: \n\n\\begin{equation}\n\\frac{\\partial L}{\\partial \\boldsymbol{X}} = \\frac{\\partial L}{\\partial \\boldsymbol{Y}} \\boldsymbol{W}^T\n\\end{equation}\n\n\\begin{equation}\n\\frac{\\partial L}{\\partial \\boldsymbol{W}} = \\boldsymbol{X}^T \\frac{\\partial L}{\\partial \\boldsymbol{Y}}\n\\end{equation}\n\n\\begin{equation}\n\\frac{\\partial L}{\\partial \\boldsymbol{b}} = \\frac{\\partial L}{\\partial \\boldsymbol{Y}}\n\\end{equation}\n\nWe propagate these gradients to change the strengths of weights and biases. ","b3b9ec52":"## References\nSaito, K. (2017). Creating Deep Learning from scratch. O'Reilly Japan.\n\nNg, A. (2021). Neural Networks and Deep Learning. Coursera.","6e897db6":"This is it! I prioritized to describe the overview of the algorithm rather than digging into the logical rigor. Neural networks iteratively execute forward propagation, loss calculation and back propargation to repeatedly predict and learn. Once the learing is complete, we store the weights and biases, then apply the algorithm to predict from new data. For the sake of parsimony, this notebook skipped other important ascpects of machine learning (e.g., cross-validation, overfitting, hyperparameter tuning). We can use more techniques to improve the prediction accuracy.","4e848b85":"The degree of misprediction is calculated by **loss function**. There are various loss functions and each of them has its own [pros and cons](https:\/\/towardsdatascience.com\/loss-functions-when-to-use-which-one-718ebad36e0). One example is cross-entropy error. It takes logarithm of the softmax value for one training label. Since we are interested in the average error for a cohort of data, we divide the output by the batch size.","776ebeed":"## 2.Neural Networks\n\nPerceptron was just a single neuron, whereas neural networks are comprised of many perceptrons. What really distinguish the two models is the idea of ***learning***. With perceptron, once we fixed the values of $w$ and $\\theta$, we never revised them. However, with neural networks we iteratively predict and revise. This is why we include neural networks as part of deep learning algorithms; the model learns better parameters by reflecting on its own mispredictions. This learning is achieved by iteratively repeating processes called forward and back propagations. In this section we will create each component of neural networks.\n\n<!-- <div align = 'center'>\n<img src=attachment:8f0123c6-4fa4-4ce4-a5f8-63fc2c1c4a6b.jpg>\n<\/div> -->","67de85bf":"### 2.4  Composing the network\n\nThe following code aggregates previous classes to generate a network. It is comprised of the following steps:\n1. Initialize parameters (randomly generate weight and biases. For better predictions, the random values are normalized by Xavier Initialization)\n2. Set up layers and store them in an ordered dictionary (by default, python dictionary is indifferent about the order in which keys are stored)\n3. Make predictions (i.e., forward propagation)\n4. Calculate mispredictions\n5. Adjust parameters based on gradient (i.e., back propagation)\n4. Repeat step 3-5 for a pre-determined iternation count (or sufficient accuracy)\n","da5c4a99":"## 1.Perceptrons\n\nLet's first go over the very fundamental building block of neural networks: ***perceptron***.  Rosenblatt (1957) proposed the idea of perceptron by mimicing how information is processed in our brains. Here is an example of a single-layered perceptron that generates a single output ($y$) based on two inputs ($x_1$ and $x_2$). Each weight ($w_1$ and $w_2$) is multiplied to each input to change its strength to the final output. If the product of the inputs passes a threshold, then the perceptron fires 1. If the input is below the threshold (i.e weak input), then no signal is fired.\u3000By adjusting the weights and the threshold, we can make the perceptron return AND operation. For example, if we set $w_1 = 0.5$, $w_2 = 0.5$, and $\\theta = 1$ then the perceptron returns the following values.\n\n<!-- <div align = 'center'>\n<img src=\"attachment:cac81c90-fef8-48ab-ac10-6a3a21b18088.png\" width=\"600\"\/>\n<\/div> -->\n\n\n\n|  $x_1$  |  $x_2$  |  $w_1$   |  $w_2$  |  $\\theta$ | $x_1 w_1 + x_2 w_2$  |  $y$   |\n| ---- | ---- |---- | ---- | ---- |---- | ---- |\n|  0 (False)  |   0 (False)  | 0.5 | 0.5 |1|0 + 0 = 0 $<$  1   |  **0 (False)**  |\n|  0 (False)  |   1 (True)   | 0.5 | 0.5 |1|0 + 0.5 = 0.5 $<$ 1|  **0 (False)**   |\n|  1 (True)   |   0 (False)  | 0.5 | 0.5 |1|0.5 + 0 = 0.5 $<$ 1 |  **0 (False)**  |\n|  1 (True)   |   1 (True)   | 0.5 | 0.5 |1|0.5 + 0.5 = 1 $\\geq$ 1| **1 (True)**  |  \n\n<!-- <div align = 'center'>\n<img src=\"attachment:98ca57a2-91bd-4328-90a9-d6d553415de2.png\" width=\"600\"\/>\n <\/div> -->","672d5ea4":"## 0.Introduction\n\nWhat are nerual networks? There are many ways to define the algorithm: in a way, it's a simple set of linear algebra multiplications; from another look, it's an iteration of partial derivatives; or it can also be described as an array of probabilities. Ultimately, all these math techniques are combined to achieve one goal; to let machines derive patterns from data by themselves, without humans explicitly coding what to look for.","2e2473ea":"### 2.2 Activation function\n\nWith perceptron, we just compared the value of the product to the threshold. With neural networks we process Affine products through **activation functions**, before feeding them to the next layer. The neural network algorithm takes advantage of non-linear activation functions (e.g. step function, sigmoid, ReLU) to achieve learning. If the activation function was linear, then the complex network gets [constricted to a single-layer](https:\/\/www.kaggle.com\/general\/196486#1102364). Here's why: if we set a linear activation function as $h(x) = cx$, then a three-layered network becomes  $f(x) = c^3x$. Since $c$ is an arbitrary number, the coefficient can be replaced by a new value $c'$. This diminishes the effect of multiple layers  to a computation by a single layer network. In addition, non-linear activation functions have non-flat derivatives which are necessary to make back propagation. (If the derivative was flat, like in linear-activation functions, then the network cannot back propagate adjustments since the gradient becomes 0). An example of non-linear activation function is Rectified Linear Unit (ReLU). It is defined as: \n\n\\begin{equation}\n  h(x)=\\begin{cases}\n    x & \\text{$(x > 0)$}\\\\\n    0 & \\text{$(x \\leq 0)$}\n  \\end{cases}\n\\end{equation}\n\nThe derivative of this function is:\n\n\\begin{equation}\n  \\frac{\\partial h(x)}{\\partial x}=\\begin{cases}\n    1 & \\text{$(x > 0)$}\\\\\n    0 & \\text{$(x \\leq 0)$}\n  \\end{cases}\n\\end{equation}\n\nThis graph can be visualized as:","c92e3901":"### 2.3 Output layer\n\nThe final layer runs three operations; completing the prediction, measuring mispredictions and initiating revision. For the prediction (i.e., forward propagation), the algorithm feeds the input from the previous layer into Softmax function. Softmax function formats all outputs between 0 and 1. This serves as a quasi-probability distribution and approximately indicates the likelihood of the prediction. Softmax is a composite of exponential functions. This function continously and monotonically increases, preserving the order between each output value.","bd437acc":"This is my first notebook publication on Kaggle. Here, I attempt to introduce the fundamentals of neural neworks without referencing any AI packages. All feedback is welcomed!","cc9d00c9":"## 4.Examples (MNIST)\nMNIST is a database comprised of 70,000 handwritten digits from American Census Bureau employees and American high school students. Here, we run the above neural networks to recognize the digits ([Many other datasets](https:\/\/en.wikipedia.org\/wiki\/List_of_datasets_for_machine-learning_research) for machine learning experiments are available). Here we train the neural networks to correctly label digits.","0d8decd6":"### 2.4 Fine tuning some parameters\n\nNeural networks can improve accuracy by adjusting other variables, like the variance of randomly generated weights and biases (Xavier Initialization), formatting inputs (batch normalization), changing activation functions and improving the learning rate. Here, we implement **Momentum** to change the learning rate depending on the degree of mispredictions."}}