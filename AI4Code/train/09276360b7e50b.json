{"cell_type":{"6adf1542":"code","53ac9381":"code","c1eab973":"code","ee8d8d13":"code","294b8328":"code","2e834482":"code","23af6412":"code","12e3350c":"code","207bb325":"code","d5ce2932":"code","e26348a5":"code","bf92f140":"code","8d89cbf6":"code","e43fd0e7":"code","f6f6103e":"code","51975af6":"code","4f3807d9":"code","84acdde7":"code","8cabc7e6":"code","9b36b88b":"code","8f48d088":"code","a0af19e2":"code","2edd288f":"code","8cfc4579":"code","7b6680b1":"code","ec7e8484":"code","fa14c7c8":"code","97892421":"code","d438f182":"code","c39208f3":"code","b4cc7cac":"code","693cc4b4":"code","0d5cea5e":"code","855b5945":"code","9bb88043":"code","3b637328":"markdown","cec82796":"markdown","0188bf8b":"markdown","ec8ea1ce":"markdown","affac034":"markdown","9dbc373a":"markdown","a15cc60c":"markdown","153fba2f":"markdown","7ec2a6fc":"markdown","08688656":"markdown","bc474f23":"markdown","5c79a26c":"markdown","9fa095d8":"markdown","5a2e045b":"markdown","9a8e06de":"markdown","8d453ed7":"markdown","c96993fc":"markdown","f7f4bcb0":"markdown","0a3f9e1f":"markdown","807350e9":"markdown","968836de":"markdown","ba7a0746":"markdown","9c1c8330":"markdown","cf85c98d":"markdown","96998934":"markdown","e47ff205":"markdown","22fd21ae":"markdown","74a0b600":"markdown","6f24d085":"markdown","970a61dd":"markdown","782d9572":"markdown"},"source":{"6adf1542":"import warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nfrom IPython.display import display, Markdown, Latex","53ac9381":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport pickle\nimport joblib\nfrom sklearn.externals import joblib\n\nimport os\nimport math\n\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize \nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction import text\n\nfrom sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n\nfrom sklearn.cluster import KMeans","c1eab973":"articles = pd.read_csv('..\/input\/clean-and-steamed-corpus\/articles_clean.csv' ,header='infer')\narticles.head()","ee8d8d13":"plt.figure(figsize=(10,8))\nax = sns.distplot(articles.length.apply(math.log))\nax.set(title='Histogram of the articles number of characters log transformed.',xlabel='Log of Article length', ylabel='Proportions')\nplt.show()\n\nprint('There are in total {} articles containing a text body with their lengths normally distributed around the average length of: {} words.'.format(len(articles.length),round(np.mean(articles.length))))","294b8328":"refdoc = \"Household Infrastructure Containment Ecological Historical Perception internalization financial support aide bailout policy policies politics social economic socioeconomic cultural technology Environment Confinement Stay-at-home Shelter-in-place Inequalities Iniquities Disparities Communication Mass communication Information Language barrier Propaganda Poverty Homelessness homeless Beliefs Religion religious Behaviors Education Educational achievement Access Health insurance Public transportation Food security insecurity stamp Housing instability Internet access Conspiracy theory Vulnerable populations Low-income Middle-income Social status Socioeconomic status Social aide Government aide Government subsidies Remote villages Churches Schools School closure Social distancing Physical distancing Utilities Abuse  Domectic violence Neighborhood Social safety Social determinants health Traditional medicine Traditions traditional Legal Law Prisons prisoner Underdeveloped countries Developed countries Health systems Race Ethnicity urban rural areas cities villages\"\nrefdoc = refdoc.lower()","2e834482":"shortword = re.compile(r'\\W*\\b\\w{1,5}\\b')\nsubs=\"\"\n\narticles['article'] = articles['article'].map(lambda x: re.sub(shortword, subs, x))\narticles['length'] = articles['article'].map(lambda x: len(x))","23af6412":"refdf = pd.Series({'article':refdoc,'length':len(refdoc),'lang':'en'})\n\nrefdf=pd.DataFrame(refdf)\narticles = pd.concat([refdf.T, articles], ignore_index=True)\narticles.head()","12e3350c":"#### Tokenising | Loading for speed of processing\n\ntokenized_articles = articles['article'].apply(lambda x: x.split())\n\n#### Removing English stopwords\nstopwords = text.ENGLISH_STOP_WORDS\n\ntokenized_articles = tokenized_articles.apply(lambda x: [i for i in x if i not in stopwords])\n\n#### Stemming will help us reduce considerably the dimension of our idf by removing words repetitions due to grammar variations such as plural, conjugated words and so on.\n\nstemmer = SnowballStemmer(\"english\")\n\n## Reading from prestemmed dataset\nstemmed_articles = tokenized_articles.apply(lambda x: [stemmer.stem(token) for token in x])\n#stemmed_articles = pd.read_csv('..\/input\/moredata\/',header='infer')\n\n#### Detokenization\n\n#### detokenization\ndetokenized_articles = []\nfor i in range(len(stemmed_articles)):\n    if i in stemmed_articles:\n        t = ' '.join(stemmed_articles[i])\n        detokenized_articles.append(t)\n        \n\n#### Loading the detokenized words for faster processing.\nwith open('tokenized_articles.data', 'wb') as filehandle:\n    # store the data as binary data stream\n    tokenized_articles=pickle.load(filehandle)\n","207bb325":"\ndef preprocessor(tokens):\n    r = re.sub( '(\\d)+', '', tokens.lower() )\n    return r\n\nngram_range=(1,3)\n\nvectorizer = TfidfVectorizer(stop_words=stopwords,max_features=10000, max_df = 0.5, use_idf = True, ngram_range=ngram_range)\n  \n### Vectorizing abstract series\ncv = CountVectorizer( preprocessor=preprocessor, stop_words = stopwords,lowercase = True, ngram_range = ngram_range,max_features = 100 )\n\n### Creating the bag of words | Reading instead of running\nbow_articles = vectorizer.fit_transform(detokenized_articles)\n\n### Getting the list of features from the bag of words\nterms = vectorizer.get_feature_names()","d5ce2932":"bow_articles = joblib.load('..\/input\/nparrays\/bow_articles.pkl')\nprint('We end-up with a bag of word in form of a matrix of {} unique words within {} documents in rows, ready to be clustered.'.format(bow_articles.shape[1], bow_articles.shape[0]))","e26348a5":"##### Using K-means Algorythm, we predefine 10 as the number of clusters for grouping documents.\nnum_clusters = 10\n\nkm = KMeans(n_clusters=num_clusters, n_jobs=10)\nkm.fit(bow_articles)","bf92f140":"## Loading pre-trained model\nkm = joblib.load('..\/input\/models\/km_compressed.pkl')\n\n## Getting prediction\ny_kmeans = km.predict(bow_articles)\n\n### Getting the clusters\nclusters = km.labels_.tolist()","8d89cbf6":"clusters_articles = pd.Series(clusters).value_counts().sort_index()\n\nclusters_info = pd.DataFrame({'ClusterId':list(clusters_articles.index),'Members':clusters_articles})\n\nprint('Discovered clusters Ids and membership counts:\\n\\n {}'.format(clusters_info))\n\nplt.figure(figsize=(10,8))\nax = sns.barplot(x='ClusterId', y='Members', data=clusters_info)\n#ax.set(xlim=(0,13000))\nax.set(xlabel='Clusters', ylabel='Membership')\n\nplt.title('Histogram number of articles per cluster.', fontsize=20)\n\nfor index, row in clusters_info.iterrows():\n    ax.text(row.ClusterId,row.Members, round(row.Members,0), color='black', ha=\"center\")\n    \nplt.show()","e43fd0e7":"### Plotting the cluster\ncenters = km.cluster_centers_\n\nx=centers[:,0]\ny=centers[:,1]\n\nx=centers[:,0]\ny=centers[:,1]\n\ncolor=clusters_info['ClusterId']\nsize=clusters_info['Members']\n\nplt.figure(figsize=(15, 10), dpi=80)\n\nplt.scatter(np.log(x), np.log(y),  c=color, s=size, alpha=0.5, cmap = 'hsv')\n\nplt.title('Clusters size and reference plot', fontsize=20)\n\n# zip joins x and y coordinates in pairs\ni=0\nfor a,b in zip(x,y):\n\n    label = \"Id {}: {}\".format(clusters_info['ClusterId'][i],clusters_info['Members'][i])\n\n    plt.annotate(label, # this is the text\n                 (np.log(a),np.log(b)), # this is the point to label\n                 textcoords=\"offset points\", # how to position the text\n                 xytext=(0,10), # distance from text to points (x,y)\n                 ha='center') # horizontal alignment can be left, right or center\n    i=i+1\n\nplt.show()\n\n### Getting clusters Ids associated to the documents Ids\ndf_clust = pd.DataFrame({'DocumentId':np.arange(0,len(y_kmeans),1),'ClusterId':y_kmeans})\n\nprint('Our bait document id is 0 hence will be displayed on the fisrt row of the below output with its cluster id')\ndf_clust.head()","f6f6103e":"indices = df_clust[df_clust['ClusterId']==0].index","51975af6":"cluster0_corpus = [stemmed_articles[i] for i in indices if i!=0]\n\ncluster0_corpus = pd.Series(cluster0_corpus)\n\ncluster0_corpus.index=indices[1:]\n\n### Verification of original dataframe indices for tracking articles.\ncluster0_corpus.index\n\n#### detokenization\ndetokenized_corpus0 = []\n\nfor i in range(len(cluster0_corpus)):\n    if i in cluster0_corpus:\n        t = ' '.join(cluster0_corpus[i])\n        detokenized_corpus0.append(t)","4f3807d9":"ngram_range=(2,2)\n\n### Vectorizing abstract series\ncv = CountVectorizer( preprocessor=preprocessor, stop_words = stopwords,lowercase = True, ngram_range = ngram_range, max_features = 500 )\n\n### Creating the bag of words\nbow_c0 = cv.fit_transform( detokenized_corpus0 ).todense()\n\n### Getting Cluster0 list of features\nfeatures_c0 = cv.get_feature_names()","84acdde7":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n########################################################################\n### Take the TF_IDF and converts it into a WordCloud object to be plotted\ndef WordCloud_covid(tfidf): \n    x = tfidf\n    x[ 'word' ] = x.index\n    dt_c0 = pd.Series( x.frequency, index = x.word.values ).to_dict()\n\n    ##stopwords = set(STOPWORDS)\n\n\n    ## plotting the word cloud\n    wordcloud_c0 = WordCloud( \n        stopwords = stopwords_med,\n        background_color = 'white',\n        width = 5000,\n        height = 3000,\n        random_state = 40 ).generate_from_frequencies( dt_c0 )\n    return(wordcloud_c0)\n\n########################################################################\n\ndef WordCloudplot(wordcloud, size=(18,10)):\n    plt.figure(figsize=size,facecolor = 'white', edgecolor='blue')\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show();\n\n### Functions for TFIDF and WordCloud\n\n### This function generates the tfidf from the Bag of Words and the Features\n\ndef tfidf_covid19(bow_c0, features_c0):\n    ### Creating the DF-ITF\n    ## Building the documents term matrix in a dataframe\n    df_dtm_c0 = pd.DataFrame( data = bow_c0, columns = features_c0 )\n\n    ###################################################################\n    ##\n    ## Display the top words and their frequency.\n    ## Find column sums and create a DataFrame.\n    ##\n    x = df_dtm_c0.sum( axis = 0 )\n    df_tf_c0 = pd.DataFrame( { 'frequency':x } )\n    ##\n    ## Display the top five records.\n    ##\n    topn = df_tf_c0.nlargest( columns = 'frequency', n = 10 )\n\n    # topn.style.set_caption( 'Top Ten Words' )\n    ##\n    ##\n    ##\n    ## Calculate the tf-idf weights.\n    ##\n    transformer = TfidfTransformer( norm = 'l2' )\n    tfidf_mat_c0 = transformer.fit_transform( bow_c0 )\n    data = np.round( tfidf_mat_c0.todense(), 3 )\n    df_tfidf_c0 = pd.DataFrame( data = data, columns = features_c0 )\n    ##\n    ## Transforming the df_tfidf into a DataFrame\n    ##\n    x1 = df_tfidf_c0.sum( axis = 0 )\n    df_tfidf_c0 = pd.DataFrame( { 'frequency':x1 } )\n    \n    return(df_tfidf_c0)\n\n\n### Function for Exploratory Data Analysis on TF-IDF\n\ndef TfIdf_Stats(df_tfidf_c0):\n    ### Descriptive statistics on TFIDF \n    \n    global tfidf_mean,tfidf_median, tfidf_sd, conf_int95, conf_int64, iqr_low, iqr_high\n\n    ### Printing the histogram of the word TFIDF\n    display(Markdown('#### <font color=\\'blue\\'>Line plot of the TF-IDF of the corpus'))\n    ## print('Line plot of the TF-IDF of the corpus \\n\\n')\n\n    df_tfidf_c0.plot(rot=45, figsize=(12,6))\n    # Add title and axis names\n    plt.title('Term Frequency plot by n-grams', fontsize=14, color='blue')\n    plt.xlabel('Expressions')\n    plt.ylabel('TF-IDF')\n    plt.show()\n\n    ### Printing the histogram of the word TFIDF\n    display(Markdown('#### <p><font color=\\'blue\\'>Histogram plot of the TF-IDF of n-grams in the corpus'))\n    ## print('Histogram plot of the TF-IDF of n-grams in the corpus \\n\\n')\n\n    df_tfidf_c0['frequency'].hist(bins=50, figsize=(15,5))\n    # Add title and axis names\n    plt.title('Histogram of Term Frequency', fontsize=20, color='blue')\n    plt.xlabel('TF-IDF')\n    plt.ylabel('Terms count')\n    plt.show()\n\n    ## investigating the Mean and confidence interval and median\n    display(Markdown('#### <p><font color=\\'blue\\'>Investigating the Mean and confidence interval and median.'))\n    tfidf_mean = np.log(df_tfidf_c0['frequency']).mean()\n    ##\n    tfidf_sd = np.log(df_tfidf_c0['frequency']).std()\n\n    tfidf_median = np.log(df_tfidf_c0['frequency']).median()\n\n    print('\\n\\n Words log(TFIDF) have a Mean of {:.2f}, Median of {:.2f} and Standard Deviation of {:.2f}:'.format(tfidf_mean, tfidf_median, tfidf_sd)+'\\n\\n')\n\n    ## 95% confidence interval to the mean log(tfidf)\n    conf_int95 = [round(tfidf_mean-2*tfidf_sd,2), round(tfidf_mean+2*tfidf_sd,2)]\n\n    ## 64% confidence interval to the mean log(tfidf)\n    conf_int64 = [round(tfidf_mean-tfidf_sd,2), round(tfidf_mean+tfidf_sd,2)]\n\n    print('95% of the Word have their log(TFIDF) between [{}-{}]:'.format(conf_int95[0], conf_int95[1])+'\\n\\n')\n\n    print('68% of the Word have their log(TFIDF) between [{}-{}]:'.format(conf_int64[0], conf_int64[1])+'\\n\\n')\n    #\n    ## investigating the Median and IQR (Robust Statistics)\n    #\n    display(Markdown('#### <p><font color=\\'blue\\'>Investigating the Median and IQR (Robust Statistics)'))\n\n    iqr_low = np.log(df_tfidf_c0['frequency']).quantile(0.25)\n\n    iqr_high = np.log(df_tfidf_c0['frequency']).quantile(0.75)\n\n    print('IQR range of word log(TFIDF) is [{}-{}]:'.format(iqr_low, iqr_high)+'\\n\\n')\n\n    ### Transforming and plotting the log transformed TFIDF\n    display(Markdown('#### <p><font color=\\'blue\\'>Transforming and plotting the log transformed TFIDF'))\n    \n    np.log(df_tfidf_c0['frequency']).hist(bins=50, figsize=(15,5), alpha=0.7)\n    \n    # Add title and axis names\n    plt.title('Log of Term Frequency plot by bigram', fontsize=14, color='blue')\n    plt.axvline(x=conf_int95[0], color='r', linestyle='dashed', linewidth=1)\n    plt.axvline(x=conf_int95[1], color='r', linestyle='dashed', linewidth=1)\n    plt.axvline(x=conf_int64[0], color='g', linestyle='dashed', linewidth=1)\n    plt.axvline(x=conf_int64[1], color='g', linestyle='dashed', linewidth=1)\n    plt.xlabel('log(TF-IDF)')\n    plt.ylabel('Terms count')\n    plt.show()\n\n    ### Plotting the different sections of the line plot of TFIDF\n    display(Markdown('#### <p><font color=\\'blue\\'>Plotting the different sections of the line plot of TFIDF'))\n\n    ### lower 25% of the TFIDF\n    print('Plotting the lower 25% of TFIDF \\n\\n')\n    df_tfidf_c0['frequency'][np.log(df_tfidf_c0['frequency'])<iqr_low].plot(rot=45, figsize=(15,10))\n    plt.show()\n\n    print('Plotting the TFIDF range between [25% 75%]\\n\\n')\n    df_tfidf_c0['frequency'][(np.log(df_tfidf_c0['frequency'])<=iqr_low) | (np.log(df_tfidf_c0['frequency'])>=iqr_high)].plot(rot=45, figsize=(15,10))\n    plt.show()\n\n    print('Plotting the higher 75% of TFIDF \\n\\n')\n\n    df_tfidf_c0['frequency'][np.log(df_tfidf_c0['frequency'])>iqr_high].plot(rot=45, figsize=(15,10));\n    plt.show()\n    \n#####################################################\n## Building the Bag of Words\n#####################################################\ndef bow_gen(articles):\n    \n    ### Creating the bag of words\n    bow_articles4 = vectorizer.fit_transform(articles)\n    \n    return(bow_articles4)\n\n#####################################################\n## Cleaning corpora of articles\n#####################################################\ndef CleanText(articles):\n    \n    #### Tokenising\n    \n    tokenized_cluster4 = articles['article'].apply(lambda x: x.split())\n\n    tokenized_cluster4 = tokenized_cluster4.apply(lambda x: [i for i in x if i not in stopwords])\n\n    ### Stemming\n    stemmed_cluster4 = tokenized_cluster4.apply(lambda x: [stemmer.stem(token) for token in x])\n\n\n    ### Detokenizing\n    #### detokenization\n    detokenized_cluster4 = []\n    for i in range(len(stemmed_cluster4)):\n        if i in stemmed_cluster4:\n            t = ' '.join(stemmed_cluster3[i])\n            detokenized_cluster3.append(t)\n    return(detokenized_cluster3)\n\n\n#####################################################\n## This function is used to plot a Kmean Cluster around centroids\n## Plotting the cluster\n## in this case model = km3 and prediction = y_means3\n#####################################################\ndef ClusterPlot(model, prediction):\n    \n    clusters_v3 = model.labels_.tolist()\n\n    clusters_v3_articles = pd.Series(clusters_v3).value_counts().sort_index()\n\n    clusters_v3_articles\n\n    clusters_v3_info = pd.DataFrame({'ClusterId':list(clusters_v3_articles.index),'Members':clusters_v3_articles})\n\n    clusters_v3_info\n\n\n    ### Plotting the cluster\n    centers_v3 = model.cluster_centers_\n\n    x3=centers_v3[:,0]\n    y3=centers_v3[:,1]\n\n    clusters_v3_info = pd.Series(prediction).value_counts().sort_index()\n    clusters_v3_info=pd.DataFrame({'ClusterId':clusters_v3_info.index, 'Members':clusters_v3_info})\n    clusters_v3_info\n\n\n    ### Plotting the cluster\n    centers_v3 = model.cluster_centers_\n\n    plt.figure(figsize=(15, 10), dpi=80)\n\n    plt.scatter(np.log(x3), np.log(y3),  c=color, s=s, alpha=0.5, cmap = 'hsv')\n\n    plt.title('Clusters size and reference plot', fontsize=20)\n\n    # zip joins x and y coordinates in pairs\n    i=0\n    for a3,b3 in zip(x3,y3):\n\n        label = \"Id {}: {}\".format(clusters_v3_info['ClusterId'][i],clusters_v3_info['Members'][i])\n\n        plt.annotate(label, # this is the text\n                     (np.log(a3),np.log(b3)), # this is the point to label\n                     textcoords=\"offset points\", # how to position the text\n                     xytext=(0,10), # distance from text to points (x,y)\n                     ha='center') # horizontal alignment can be left, right or center\n        i=i+1\n\n    plt.show();","8cabc7e6":"### Generating and sorting the TFIDF\ndf_tfidf_c0 = tfidf_covid19(bow_c0=bow_c0, features_c0=features_c0)\n\ndf_tfidf_c0 = df_tfidf_c0.sort_values('frequency')\n###","9b36b88b":"df_tfidf_c0 = joblib.load('..\/input\/data002\/df_tfidf_c0.pkl')","8f48d088":"morewords ='disease, diseases, disorder, symptom, symptoms, drug, drugs, problems, problem,prob, probs, med, meds,pill,  pills,  medicine,  medicines,  medication,  medications,  treatment,  treatments,  caps,  capsules,  capsule, tablet,  tablets,  tabs,  doctor,  dr,  dr.,  doc,  physician,  physicians,  test,  tests,  testing,  specialist, specialists, side-effect, side-effects, pharmaceutical, pharmaceuticals, pharma, diagnosis, diagnose, diagnosed, exam, challenge,  device,  condition,  conditions,  suffer,  suffering  ,suffered,  feel,  feeling,  prescription,  prescribe, prescribed, over-the-counter, ot'\n\nprint('Content of our reference document:\\n\\n {}'.format(morewords))\n\nstemmer = SnowballStemmer(\"english\")\n\nmorewords = morewords.replace(' ', '').split(',')\n\nmorewords = [stemmer.stem(token) for token in morewords]\nmorewords = set(morewords)\n\nstopwords = text.ENGLISH_STOP_WORDS\nstopwords_med = set(stopwords).union(morewords)","a0af19e2":"TfIdf_Stats(df_tfidf_c0)","2edd288f":"#wordcloud2 = WordCloud_covid(tfidf=df_tfidf_c0[np.log(df_tfidf_c0['frequency'])>conf_int64[1]])\n## Testing IQR approach\nwordcloud2 = WordCloud_covid(tfidf=df_tfidf_c0[np.log(df_tfidf_c0['frequency'])>iqr_high])\n\ndisplay(Markdown('#### <p><font color=\\'blue\\'>Wordcloud of the upper 75% important words'))\n\nWordCloudplot(wordcloud2)","8cfc4579":"## wordcloud3 = WordCloud_covid(tfidf=df_tfidf_c0[(np.log(df_tfidf_c0['frequency'])>=conf_int64[0])&(np.log(df_tfidf_c0['frequency'])<=conf_int64[1])])\n\nwordcloud3 = WordCloud_covid(tfidf=df_tfidf_c0[(np.log(df_tfidf_c0['frequency'])>=iqr_low)&(np.log(df_tfidf_c0['frequency'])<=iqr_high)])\n\nWordCloudplot(wordcloud3)","7b6680b1":"## wordcloud4 = WordCloud_covid(tfidf=df_tfidf_c0[(np.log(df_tfidf_c0['frequency'])<conf_int64[0])])\nwordcloud4 = WordCloud_covid(tfidf=df_tfidf_c0[(np.log(df_tfidf_c0['frequency'])<iqr_low)])\n\n## Generating the wordcloud\ndisplay(Markdown('#### <p><font color=\\'blue\\'>Wordcloud of the 25% less words in our list'))\n\nWordCloudplot(wordcloud4)","ec7e8484":"original_text = pd.read_csv(\"original_text.csv\")\n\narticles_cluster0 = original_text.loc[indices,]","fa14c7c8":"### Loading original data and stored intermediary results\n\noriginal_text = joblib.load('..\/input\/originaldata\/original_raw_data.pkl')\n\narticles_cluster0 = joblib.load('..\/input\/data001\/articles_cluster0.pkl')\n\n\n### Creating Search pattern\n\npat10=r'social distancing'\npat11=r'effectiveness'\n\n### Rewritting the columns name\noriginal_text.columns=['article', 'length', 'article1']\narticles_cluster0.columns=['article']","97892421":"original_text.head()","d438f182":"orig_match = original_text['article'][original_text['article'].apply(lambda x: re.search(pat10,str(x))).notnull()]\norig_count= orig_match[orig_match.apply(lambda x: re.search(pat11,str(x))).notnull()].count()\n\nprint(\"Number of articles with the keyword: social distancing effectiveness in the original pool of articles was: {}\".format(orig_count))","c39208f3":"new_match = articles_cluster0['article'][articles_cluster0['article'].apply(lambda x: re.search(pat10,str(x))).notnull()]\nsocial_dist = new_match[new_match.apply(lambda x: re.search(pat11,str(x))).notnull()]\nnew_count=new_match[new_match.apply(lambda x: re.search(pat11,str(x))).notnull()].count()\n\nprint(\"Number of articles with the keyword: social distancing effectiveness in sub-Cluster of our reference document is: {}\".format(new_count))","b4cc7cac":"original_filtered = original_text.loc[social_dist.index]['article1']\noriginal_filtered","693cc4b4":"import json\n\nfrom pandas.io.json import json_normalize  \n\n### Detection of the articles language\n!pip install langdetect\nfrom langdetect import detect\n\n### Summarizer package\n!pip install bert-extractive-summarizer\n# pip install summarizer\nfrom summarizer import Summarizer\n!pip install langdetect","0d5cea5e":"model = Summarizer()","855b5945":"results = [model(x, max_length=160) for x in original_filtered[:5]]","9bb88043":"[print('Article {} summary:\\n\\n {}'.format(results.index(i),i.capitalize())) for i in results]","3b637328":"## Step 1: Creating and introducing the bait or reference article\n\nour bait is created with th intention of attracting all articles with specific keywords for:\n\n* Understand reshaping population habits by mean of social distancing can help overcome the outbreak\n* Capturing the Socio-economic impact of a medical outbreaks such as COVID-19\n* find Policies related articles for better understanding policies put in place in the past that helped in similar situation\n* Understand infrastructure requirements during such an outbreak\n\nHence below bait was created:","cec82796":"#### Creating the Tf-idf to generate the document term natrix","0188bf8b":"### **Here we summarize 5 of the 28 articles just to demonstrate their relevance to our topic of interest and to the words defined in our bait \/ reference document.**","ec8ea1ce":"# **Methodology: Fish and Bait**\n<div style=\\\"text-align: justify\\\"> \nThe methodology developed to analyze the CORD-19 articles was done with the intent of replicating to any other semantic analysis regardless of the research area.\nThe concept lies on the existing text analysis technic using term frequency and inverse document frequency matrix (tf-idf) and then clustering documents based on the calculated distanced between them using Euclidian or Cosine distance. Our approach simply created a reference document with a set of keywords related to the field of interest, then to insert this reference document into the corpora before running the clustering algorithm that remains unsupervised.<\/div><p>\n\n<div style=\\\"text-align: justify\\\"> \nAfter the cluster has converged, we identify the cluster in which the algorithm has placed our reference document and we focus on topic analysis within the Cluster. This technic helps reducing the dimension of the corpora (set of articles) and focus on discovering within the dataset.\nIn the case of the COVID-19 articles analysis, we will focus on discovering socio-economic impact and policies established to mitigate similar epidemic and pandemic situation.<\/div><p>\n\n***Acknowledgement***: Work accomplished in collaboration with the MFONDOUM Center for community development: <b>Dr. yolande Pengetnze, MD, Mr. Roy MFONDOUM, MSc., Ms. Clemence CHINTOUO, BSc. Mr. Roland NGOUYAMSA MSc.<\/b>\n","affac034":"##### Wordcloud of the 25% to 75% range of important words","9dbc373a":"##### Articles length distribution","a15cc60c":"Our cluster 0 has only 28 articles related otour question and we will focus on them to produce our literature review.","153fba2f":"## Step 2: Clustering articles around the bait\n\nIn this section we will:\n\n* Prepare the dataset for clustering\n* find similarity between existing articles and our reference articles using Clustering with KMeans. ","7ec2a6fc":"### Summary:\nFrom 45000 articles down to 28 articles filtered by relevance using this clustering approach.","08688656":"## <font color='blue'>Fish and bait steps:<\/font>\n\n* step 1: Introduce a bait article into the dataset\n* step 2: Attract similar articles in a Cluster around the bait articles using K-Mean and LDA\n* step 3: Identify the cluster to which our bait was assign\n* step 4: Extract articles from the cluster and remove the bait\n* step 5: Vizualise the result using WordCloud\n* step 6: Repeat all of the above until our baith keywords are part of the term frequency median\n* step 7: Summarise the articles selected using BERT summariser or manual review depending on the final size of the Cluster","bc474f23":"#### Exploratory data analysis on words TF-IDF","5c79a26c":"##### Calculating the new Term Frequency Inverse Document Frequency\n\nUpdating the stopwords with common medical words to remove them from the dataset.","9fa095d8":"## Summarized output\n* The reference document at position 0 is found to be in the cluster id 0\n* Cluster 0 contains 5438 documents that were attracted by the bait document 0.\n* We have reduced by approximately 88% the size of our article pool down from 45000\n* This is what th bait approach is about.\n\nThis process could be repeted iteratively until we get to the minimum size without loss of information.\nNext: we will plot the wordcloud of our cluster 0 to see how relevant it is to the bait we used.","5a2e045b":"#### Wordcloud of the 25% less words in our list","9a8e06de":"1- The green lines show the 64% confidence interval around the mean.\n\n2- The red line represent the 95% confidence interval around the mean.\n\nWe will go for the 64% confidence interval at this stage to ensure we capture as many terms as possible.","8d453ed7":"Before vectorizing, let's reduce the processing time and ensure that we do not get too many undesirable words, we will get read of all words shorted or equal to 5 letters and remove all numbers and special characters from our corpus.","c96993fc":"We can see the distribution of clusters and the articles membership to them. Increasing the number of clusters may even reduce more the size of the clusters.","f7f4bcb0":"# Automated Literature review on the 28 articles","0a3f9e1f":"## Exploratory analysis of the articles\n\nWe wll retrieve the articles here, and analyse the distribtions around their length.","807350e9":"# Step 5: WordCloud Visualization of the bait article cluster","968836de":"## Step 4: Cluster 0 articles filtering and analysis\n\nWe excluded the reference document from the cluster to make sure that the word list we introduced does not influence articles WordCloud outcome.","ba7a0746":"# Conclusion:\n\n<div style=\"text-align: justify\">We can clearly see that by using this new method we named **Fish and Bait**  we were able not only reduce the size of the search scope to 28 articles but we finetuned the relevance of our research to articles discussing more the socio-economic aspects of the pandemic as well as the policy recommendations to contain the spread of the virus. Adding the document summarizer help to quickly proof the point as we can clearly see that the output summaries generated automatically by the algorythm are absoluty relevant to our research.\n\nThis method can be proven much more efficient than only using a simple term search from the begining as it returned 191 articles in which some may have mentioned the expression once while remaining focused on the scientific aspect of the research on the Corona virus. Getting them in the review list would be a perfect wast of time. The **Fish and Bait** approach automatically excluded them from the relevance give the words inserted in our bait.\n\nThis can be generalized to any topic of interest and pool of articles. It would simply be a matter of inserting a different bait with relevant keywords and the clustering will achieve the most relevant filtering and provide better results than a simple search.\n\nThe shortcoming of the **Fish and bait** method are the time it take to generate the cluster. We are currently testing it with other topics and with different clustering algorithm to set a benchmark and optimize the performance. This will be published in subsequent articles on Kaggle.<\/div>\n","9c1c8330":"![image.png](attachment:image.png)","cf85c98d":"Now that we have our bait(reference document) inserted in the row 0 of our corpora we can start the tokenization. ","96998934":"### Top 5 content summaries","e47ff205":"## Topic: Effectiveness of social distancing\n\nBased on the Wordcloud form the bottom 25% word frequencies of our cluster, we decided to go for the question:\n\n### **Question: What is the impact effectiveness of social distancing in containing a pandemic?**\n\nWe will lockup these keywords in the original corpora of all articles and compare the output to the one of our reduced corpora from our Cluster 0 articles.","22fd21ae":"# COVID-19 Open Research Dataset Challenge (CORD-19)\n### An AI challenge with AI2, CZI, MSR, Georgetown, NIH & The White House\n\n##### Title: The \u201cBait-and-Fish\u201d Method - A Novel AI-Powered Method for Discovering Hidden\/Rare Topics in a Large Dataset using Unsupervised Machine Learning Techniques.\n##### Focus: Application of the Novel \u201cBait-and-Fish\u201d Method to Examining Socio-Economic and Policy Characteristics of Coronavirus Epidemics\/Pandemics and Implications for Low- and Middle-Income Countries.\n","74a0b600":"# Step 7: Literature review based on subset of the bait cluster","6f24d085":"#### Wordcloud of the upper 75% important words","970a61dd":"## Step 3: Finding our bait cluster.","782d9572":"## Summary\nFrom the above wordlcoud generated from the 5438 filtered articles, we can clearly see that our bigrams (set of expressions with 2 wordS) generated have successfully aggregated the topics of interests indicated by the keywords set defined in our original reference document.\n\n* The reference document was remove from the articles dataframe before generating the wordcloud to avoid contamination.\n* The Document Term Frequency and inverse document Frequency matrix was analysed and was deviced into 3 blocks. \n    - A bloc with their term frequencies below the 1st quartile\n    - A bloc with their term frequencies between the 1st(excluded) and the 4th quartile (excluded) also known as inter-quartile range\n    - A bloc with their term frequencies above the 4th quartile. \n* A pattern clearly suggest that there are very few words below the lower quartile and above the higher quartile.\n* Our hypothesis is that after the 1st fish and bait clustering execution, most of our keyword will be present in the lower-quartile as rare words given that these articles are mostly scientific and we are looking for Socio-Economic and policy relates terms.\n* Wordplot of the lower quartile confirms our hypothesis as we can immediately spot terms such as: \n    - household member\n    - Knoweledge attitude\n    - economic social\n    - effective prevention\n    - high income countries\n    - low income countries\n    - health infrastructure\n    - health program\n    ..."}}