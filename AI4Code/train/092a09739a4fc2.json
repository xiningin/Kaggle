{"cell_type":{"12145751":"code","fa4fbc8d":"code","ad86f9c5":"code","4146cdf9":"code","b17a9364":"code","4f1811cb":"code","f10a9db8":"code","d4a3ef73":"code","965c103c":"code","f8324da3":"code","6efd3d2f":"code","be3446c3":"code","37053b02":"code","d4c806db":"code","5a4e1452":"code","378b2518":"code","99f16bff":"code","4f1e9c87":"code","87b33fd2":"code","05fd76c3":"code","47ed9b77":"code","f7403ce5":"markdown","8ec19e57":"markdown","834551ec":"markdown","8448c82b":"markdown","8e11889a":"markdown","079d9c00":"markdown","8b30a617":"markdown","1a34cd89":"markdown","5efe0d17":"markdown","d8839c0e":"markdown","77b76312":"markdown","60a3dd8c":"markdown","811ce230":"markdown","ce10a915":"markdown","35a95241":"markdown","33427244":"markdown","d3a06933":"markdown","1d0fd05c":"markdown","33b1f204":"markdown","7f262464":"markdown","fdee2606":"markdown","84c17bcd":"markdown","62a57671":"markdown","554ae105":"markdown","6afc69fa":"markdown","88fcd7ac":"markdown","0fba0fd1":"markdown","60bb3d2d":"markdown","41580a01":"markdown","233ac5c4":"markdown"},"source":{"12145751":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n!pip install pandas-bokeh # plotting tool for Pandas\n# !conda install -c patrikhlobil pandas-bokeh\nimport pandas_bokeh\n\n!pip install https:\/\/github.com\/pandas-profiling\/pandas-profiling\/archive\/master.zip \nfrom pandas_profiling import ProfileReport\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, plot_roc_curve, precision_recall_curve, confusion_matrix\n\nimport tensorflow as tf\nfrom keras import models, layers, optimizers, initializers, metrics, losses #, regularizers\n","fa4fbc8d":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\n\ndf.describe()","ad86f9c5":"# Generate html sumarization report about the data -> quick glance at the data\nprof = ProfileReport(df)\nprof.to_file(output_file='report.html')\n","4146cdf9":"df['Amount_logprice'] = np.log(df['Amount']+0.001)\ndf\n","b17a9364":"# Split train and test data and remain class ratio (for test model performance at last)\ntrain_df, test_df = train_test_split(df, test_size=0.2, stratify=df['Class'])\n\n# Split train and validation data and remain class ratio (to monitor model performance in training)\ntrain_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['Class'])\n\n# Split training data and target data\ndef Splitting(df):\n  X = df.drop(['Time', 'Amount', 'Class'], axis=1)\n  y = df['Class']\n  return X, y\n\nX_train, y_train = Splitting(train_df)\nX_val, y_val = Splitting(val_df)\nX_test, y_test = Splitting(test_df)\n\nmean = X_train.mean(axis=0)\nX_train -= mean\nstd = X_train.std(axis=0)\nX_train \/= std\nX_test -= mean\nX_test \/= std\nX_val -= mean\nX_val \/= std\n\nprint('Class 1 in train data: ', train_df.loc[train_df.Class==1].shape[0], \n      '\\nClass 1 in validation data: ', val_df.loc[val_df.Class==1].shape[0],\n      '\\nClass 1 in test data: ', test_df.loc[test_df.Class==1].shape[0])","4f1811cb":"# Set monitoring metrics in training process\nMETRICS = [\n      metrics.BinaryAccuracy(name='accuracy'),\n      metrics.Precision(name='precision'),\n      metrics.Recall(name='recall'),\n      metrics.AUC(name='auc', curve='PR'), # Precision-Recall-curve\n]\n\n# Build our model\nmodel_1 = models.Sequential([\n    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    layers.Dropout(0.4),\n    layers.Dense(1, activation='sigmoid')])\nmodel_1.compile(loss='binary_crossentropy', \n                optimizer=optimizers.Adam(lr=1e-3), \n                metrics=METRICS)\n\nEPOCHS = 100\nBATCH_SIZE = 2048\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc',\n    verbose=1,\n    patience=10,\n    mode='max',\n    restore_best_weights=True\n    )\n\n# Print model info summary\nmodel_1.summary()\n\n# Start training the model\nnormal_training = model_1.fit(X_train, y_train, \n                              batch_size=BATCH_SIZE, \n                              epochs=EPOCHS,\n                              callbacks=early_stopping,\n                              validation_data=(X_val, y_val), \n                              verbose=1)\n\n# Save the model (we should make a good habit of always saving our models after training)\nmodel_1.save(\"Model_1\")\n","f10a9db8":"temp = pd.DataFrame(normal_training.history)\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\ntemp[['loss', 'val_loss']].plot.line(ax=axes[0,0])\ntemp[['precision', 'val_precision']].plot.line(ax=axes[0,1])\ntemp[['recall', 'val_recall']].plot.line(ax=axes[1,0])\ntemp[['auc', 'val_auc']].plot.line(ax=axes[1,1])\nplt.show()\n","d4a3ef73":"results = model_1.evaluate(X_test, y_test, verbose=1)\n","965c103c":"# Get predicted result by the model in the test data\npredict = model_1.predict(X_test)\n\ntemp = precision_recall_curve(y_test, predict)\ntemp = np.transpose(pd.DataFrame(list(temp)))\n\ntemp_df = pd.DataFrame(temp)\ntemp_df.columns=['Precision', 'Recall', 'Threshold']\n\npandas_bokeh.output_notebook()\ntemp_df[-500:].plot_bokeh(\n    kind='line',\n    x=temp_df[-500:].index,\n    y=['Precision', 'Recall', 'Threshold'],\n    title='Precision_Recall',\n)\n","f8324da3":"predict = (predict>=0.1)\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, predict)\ncnf_matrix\n\nax= plt.subplot()\nsns.heatmap(cnf_matrix, annot=True, ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['0', '1']); ax.yaxis.set_ticklabels(['0', '1']);\n","6efd3d2f":"# Calculate and set the class weight\nfraud = df.loc[df.Class == 1].shape[0]\nno_fraud = df.loc[df.Class == 0].shape[0]\nweight_0 = (1 \/ no_fraud)*(fraud+no_fraud)\/2.0 \nweight_1 = (1 \/ fraud)*(fraud+no_fraud)\/2.0\nclass_weight = {0: weight_0, 1: weight_1}\nprint(weight_0, weight_1)\n\n# Build the model\nmodel_2 = models.Sequential([\n    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    layers.Dropout(0.4),\n    layers.Dense(1, activation='sigmoid')\n    ])\nmodel_2.compile(loss='binary_crossentropy', \n                optimizer=optimizers.Adam(lr=1e-3), \n                metrics=METRICS)\n\nEPOCHS = 100\nBATCH_SIZE = 2048\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_auc', \n    verbose=1,\n    patience=20,\n    mode='max',\n    restore_best_weights=True\n    )\n\n# Print model info summary\nmodel_2.summary()\n\n# Start training the model\nclass_weight_training = model_2.fit(X_train, y_train, \n                              batch_size=BATCH_SIZE, \n                              epochs=EPOCHS,\n                              callbacks=early_stopping,\n                              validation_data=(X_val, y_val), \n                              class_weight=class_weight,\n                              verbose=1)\n\n# Save the model (we should make a good habit of always saving our models after training)\nmodel_2.save(\"model_2\")\n","be3446c3":"import matplotlib.pyplot as plt\n\ntemp = pd.DataFrame(class_weight_training.history)\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\ntemp[['loss', 'val_loss']].plot.line(ax=axes[0,0])\ntemp[['precision', 'val_precision']].plot.line(ax=axes[0,1])\ntemp[['recall', 'val_recall']].plot.line(ax=axes[1,0])\ntemp[['auc', 'val_auc']].plot.line(ax=axes[1,1])\nplt.show()\n","37053b02":"results = model_2.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=1)\n","d4c806db":"# Get predicted result by the model in the test data\npredict = model_2.predict(X_test)\n\ntemp = precision_recall_curve(y_test, predict)\ntemp = np.transpose(pd.DataFrame(list(temp)))\n\ntemp_df = pd.DataFrame(temp)\ntemp_df.columns=['Precision', 'Recall', 'Threshold']\n\npandas_bokeh.output_notebook()\ntemp_df[-5000:].plot_bokeh(\n    kind='line',\n    x=temp_df[-5000:].index,\n    y=['Precision', 'Recall', 'Threshold'],\n    # xlabel='Category',\n    # ylabel='Annual Sales',\n    title='Precision_Recall',\n)\n","5a4e1452":"predict = model_2.predict(X_test)\npredict = (predict>=0.398)\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, predict)\ncnf_matrix\n\nax= plt.subplot()\nsns.heatmap(cnf_matrix, annot=True, ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['0', '1']); ax.yaxis.set_ticklabels(['0', '1']);\n","378b2518":"# Generate the new data\nfrom imblearn.over_sampling import SMOTE\n\noversample = SMOTE()\nX_oversample, y_oversample = oversample.fit_resample(X_train, y_train)\n\nprint('Original training data count: \\nClass 1: ', \n      X_train.loc[y_train == 1].shape[0], '\\nClass 0:', \n      X_train.loc[y_train == 0].shape[0], '\\n')\nprint('Oversampled training data count: \\nClass 1: ', \n      X_oversample.loc[y_oversample == 1].shape[0], '\\nClass 0:', \n      X_oversample.loc[y_oversample == 0].shape[0], '\\n')\n","99f16bff":"# Build the model\nmodel_3 = models.Sequential([\n    layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n    layers.Dropout(0.4),\n    layers.Dense(1, activation='sigmoid')])\nmodel_3.compile(loss='binary_crossentropy', \n                optimizer=optimizers.Adam(lr=1e-3), \n                metrics=METRICS)\n\nEPOCHS = 100\nBATCH_SIZE = 2048\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_auc', \n                                                   verbose=1,\n                                                    patience=20,\n                                                    mode='max',\n                                                    restore_best_weights=True)\n\n# Print model info summary\nmodel_3.summary()\n\n# Start training the model\noversample_history = model_3.fit(X_oversample, y_oversample, \n                                  batch_size=BATCH_SIZE, \n                                  epochs=EPOCHS,\n                                  callbacks=early_stopping,\n                                  validation_data=(X_val, y_val), \n                                  verbose=1)\n\n# Save the model (we should make a good habit of always saving our models after training)\nmodel_3.save(\"model_3\")\n","4f1e9c87":"temp = pd.DataFrame(oversample_history.history)\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(14, 10))\ntemp[['loss', 'val_loss']].plot.line(ax=axes[0,0])\ntemp[['precision', 'val_precision']].plot.line(ax=axes[0,1])\ntemp[['recall', 'val_recall']].plot.line(ax=axes[1,0])\ntemp[['auc', 'val_auc']].plot.line(ax=axes[1,1])\nplt.show()\n","87b33fd2":"results = model_3.evaluate(X_test, y_test, batch_size=BATCH_SIZE, verbose=1)\n# print(\"Loss: {:0.4f}\".format(results[0]))","05fd76c3":"# Get predicted result by the model in the test data\npredict = model_3.predict(X_test)\n\ntemp = precision_recall_curve(y_test, predict)\ntemp = np.transpose(pd.DataFrame(list(temp)))\n\ntemp_df = pd.DataFrame(temp)\ntemp_df.columns=['Precision', 'Recall', 'Threshold']\n\npandas_bokeh.output_notebook()\ntemp_df[-5000:].plot_bokeh(\n    kind='line',\n    x=temp_df[-5000:].index,\n    y=['Precision', 'Recall', 'Threshold'],\n    # xlabel='Category',\n    # ylabel='Annual Sales',\n    title='Precision_Recall',\n)\n","47ed9b77":"predict = model_3.predict(X_test)\npredict = (predict>=0.357)\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, predict)\ncnf_matrix\n\nax= plt.subplot()\nsns.heatmap(cnf_matrix, annot=True, ax = ax); #annot=True to annotate cells\n\n# labels, title and ticks\nax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \nax.set_title('Confusion Matrix'); \nax.xaxis.set_ticklabels(['0', '1']); ax.yaxis.set_ticklabels(['0', '1']);\n","f7403ce5":"The Threshold we see in the graph is the value in which we decide whether our prediciton is 1 (fraud) or 0 (non-fraud). As our model's prediction is a probability value from 0-1. If the prediction is greater or equal to threshold value, it would be labeled as 1, and if it is smaller than threshold, it would be labeled as 0. The default threshold is 0.5. \n\nAs we want high value of Recall and not too low Precision, from the graph we can see that to get high value of both recall and precision, threshold must be as low as 0.03, which is not really possible. And the higher threshold, the lower recall we get, at threshold of 0.1, we can get a decent value of both recall and precision.\n\nLet's see the Confusion matrix for our model at threshold of 0.1 (Confusion matrix is a table that summarize the actual vs. predicted labels where the X axis is the predicted label and the Y axis is the actual label) ","8ec19e57":"Plot training history of metrics to check for any problem in the training process (overfitting, etc.)","834551ec":"# Conclusion\n\nIn this notebook, I have tried to solve the following problems:\n\n* How to classify highly imbalanced dataset\n* How to fine tune the model using Precision, Recall and Confusion Matrix instead of Accuracy metric\n\nFrom this problem I have come to the conclusion that for each problem we have, we need to consider carefully how special our data is and how to evaluate the model to have the best performance based on our need specifically. For some, accuracy metric is enough, for other cases, the precision or recall or any other metrics would be much more useful.\n\n\n","8448c82b":"From confusion matrix, we can get decently high recall of 0.918 (90\/(90+8)) at the cost of lower precision but acceptable in our case (90\/(90+1800)).\n\nThis performance of model 2 is quite good and acceptable, let's see if we can improve some more in the next model.","8e11889a":"Precision Recall curve of model 2:","079d9c00":"The training return decent result (on validation data: val_accuracy: 0.9995 - val_precision: 0.9242 - val_recall: 0.7722 - val_auc: 0.8797), let's analyze more to see if it is really good and applicable to our real problem.","8b30a617":"We can see in the Confusion Matrix and Precision Recall curve, we got a good Recall value with threshold of 0.357 and almost the same performance as model 2 trained with class weight.","1a34cd89":"In this model, I will try to make it pay more attention to the under-represented class (fraud) by setting more weight to the fraud class (1) and less for the non-fraud class (0) to compensate the imbalance.\n\nLet's see what would happen:","5efe0d17":"Let's train new model based on the newly constructed data:","d8839c0e":"Evaluate the model with test data","77b76312":"# Build and train model\n# Model 1 (normal training)\n\nIn this model, I would try just train the model normally without any adjustment about the imbalanced data.\nIn training process, I set the monitoring metrics to accuracy, precision, recall and auc (area under the curve, Precision-Recall-curve). And I set early stopping based on the value of auc (the reason will be explained later), the training would stop if auc in validation data is not improved for 10 epochs, and the training parameters will be restored back to the best trained weights.\n","60a3dd8c":"By checking the data, we can see that:\n* Except for Time and Amounnt columns, all V- columns (which are converted by PCA Dimensionality reduction to protect user identities and sensitive features) are normalized to mean of 0 and standard deviation of 1-2.\n* The dataset have non NaN value sample, which is good as we can skip the process of handling missing value\n* There are 1081 duplicate rows, account for 0.4%, which is quite small. We can not confirm (yet) why there are duplicated data, is it erroneously generated multiples times or separated real transactions. Therefore I would leave these duplicate samples without any process and presume that they are all separated real transactions.\n","811ce230":"As explained above, in this imblanced dataset, we should not adjust our model based on accuracy metrics, as it would easily misleading and overfitting towards the over-presented class. Instead, we should focus more on the Precision and Recall metrics.\n* Recall\u00a0is the fraction of\u00a0correctly predicted positives among all positive class\n* Precision\u00a0is the fraction of\u00a0correctly predicted positives among all predicted positive\n\nActually, between Precision and Recall there is a trade-off, that is when we have high precision, the recall will be probably low and vice versa. In our credit card fraud dection problem, we want our model to have higher recall, which is filter out as many fraud as possible even it will lead to more non-fraud transactions will be labeled as fraud (lower precision). But we also do not want to have too low precision, which will label too many transactions as fraud and reduce customer's good feeling for our product (they may have to receive too many confirmation calls or emails for suspicious transactions). This is the reason why I set early-stopping in training model to monitor the auc metric (Area Under the Curve of Precision Recall curve), which is a score that to find a balance between high precision and high recall for our model.\n\nLet's check the Precision Recall curve of our 1st model (in test data):\n","ce10a915":"Plot training history of metrics to check for any problem in the training process (overfitting, etc.)","35a95241":"# Model 2 (Class weight)","33427244":"Plot training history of metrics to check for any problem in the training process (overfitting, etc.)","d3a06933":"As the transaction mean value is relatively small (mean = 88.3) but the outliers is too high (75% percentile value is only 77 vs max value of 25600). Therefore I will process the log transformation for the Amount column first before normalize the whole dataset.\n","1d0fd05c":"Normalize the whole dataset to mean of 0 and standard deviation of 1 to make it easier for deep learning algorithm to converge","33b1f204":"# Model 3 (Over-sampling)","7f262464":"# Data pre-processing","fdee2606":"# Setup","84c17bcd":"From the Confusion matrix, we got a quite high False Negative value of 19 (low recall) compare to lower True Negative (17) (high prediction), which mean we predicted 79 fraud cases correctly and 19 cases wrongly labelled as non-fraud (80% recall), which is not so good given the threshold is already as low as 0.1.\n\nThe reason would be our model have treated both class (fraud and non-fraud) the same regardless the imbalance.\nLet's try to make some change to the model to see if there is any improvement.\n","62a57671":"Evaluate the model with test data","554ae105":"References:\n\n[1] Fran\u00e7ois Chollet, Deep Learning with Python, 2018\n\n[2] Aurelien Geron, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n\n[3] https:\/\/www.tensorflow.org\/tutorials\/structured_data\/imbalanced_data \n\n[4] https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall \n\n[5] https:\/\/www.kaggle.com\/joparga3\/in-depth-skewed-data-classif-93-recall-acc-now","6afc69fa":"Precision Recall curve of model 3:","88fcd7ac":"The dataset of Credit Card Fraud is quite special dataset (from normal training datasets) which is highly imbalanced data but we will face quite frequently in real life. The class \"Fraud\" data only account for 0.17% of the whole dataset while the remaining 99.83% belong to non-fraud class transactions.\n\nIn this notebook, I will approach the problem to meet the following goals:\n* How to classify highly imbalanced dataset (in which one class's samples outnumber another)\n* How to fine tune the model using Precision, Recall and Confusion Matrix instead of Accuracy metric\n\n\nTo deal with imbalanced dataset, we can approach by the following methods:\n* Consider to collect more (fraud) data, which is impossible in our case\n* Resample the data (to make the class ratio closer to 50-50)\n    * Under-sampling (and cross validation): delete samples from over-presented class to meet 50-50 ratio. This method will be wasteful as we can not take advantage of the abundent of data we have in the dataset, therefore I will not present this in the notebook\n    * Oversampling: add more dupplicated samples of the under-represented class or generate new samples of the class from the existing dataset.\n* Change model's training weights of each class to make it consider more on the imbalance of dataset\n* Use different metrics: Precision and Recall, Confusion matrix, F1-score\n\n\n\nAs the data is highly imbalanced, it is very easy to be misleading if we keep measure model's performance by accuracy metric as every other (more) balanced dataset. Because our model can just learn to predict all test data as high as 99.8% of accuracy by predict all data is non-fraud (without learning at all) which would be useless and we want to avoid the most. Therefore, in this notebook I will adjust the model based on Precision, Recall and Confusion Matrix to deal with this highly imbalanced dataset, which will be explained more detail later.\n","0fba0fd1":"In this model 3, I will try to generate new data of fraud using SMOTE (Synthetic Minority Over-Sampling Technique) library to balance the 2 classes. In SMOTE, the new data of under-presented class is constructed from existing data of the class by algorithm (not by replicating them)\n\nIn the following resultc we can see the number of class 1 (fraud) samples have been increased to the same as class 0, in which the balance have changed to 50:50 ratio\n","60bb3d2d":"In the Precision Recall curve, we can get maximum recall value of 0.929, which is higher than maximum recall value of model 1 (0.888). At Threshold of 0.398, I can get the decent value of recall, and also not too low value of precision. Let's see Confusion matrix at threshold of 0.398 as following:","41580a01":"# Import and check the data","233ac5c4":"By evaluating the model with test data, the model performance was not as good as in training and validation data, but still decently good result.\n"}}