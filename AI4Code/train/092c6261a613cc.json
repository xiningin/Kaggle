{"cell_type":{"520d5665":"code","f355054c":"code","9cce42b1":"code","d364a2fb":"code","4e7c7491":"code","e39c1f65":"code","da96a7cd":"code","48a0caa7":"code","affcd7a9":"code","cc276660":"code","2131155b":"code","1ea22dfb":"code","db6fd77a":"code","105026a6":"code","3937a3b1":"code","2e844b7f":"code","d5363484":"code","20d04b1d":"code","11ad58b2":"code","b32dc5be":"markdown","bc3ab6d7":"markdown","35c05bef":"markdown","5c052197":"markdown","d58ee70f":"markdown","e656eaff":"markdown","e8441b64":"markdown","3f80ceea":"markdown","b09e5daa":"markdown","8606505c":"markdown","0be8dc1c":"markdown","1a2e9bb7":"markdown","be0fff03":"markdown","a10dd84e":"markdown","40b01006":"markdown"},"source":{"520d5665":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f355054c":"import pandas as pd #pandas - for data manipulation\nimport datetime as dt\nfrom dateutil import parser\nnew_data = pd.read_csv('\/kaggle\/input\/wildfire-satellite-data\/fire_nrt_M6_156000.csv') #load new data (June 2020->present)\nold_data = pd.read_csv('\/kaggle\/input\/wildfire-satellite-data\/fire_archive_M6_156000.csv') #load old data (Sep 2010->June 2020)\nfire_data = pd.concat([old_data.drop('type',axis=1), new_data]) #concatenate old and new data\nfire_data = fire_data.reset_index().drop('index',axis=1)\nfire_data = fire_data[fire_data.satellite != \"Aqua\"]\nfire_data = fire_data.sample(frac=0.1)\nfire_data = fire_data.reset_index().drop(\"index\", axis=1)","9cce42b1":"print(f\"Shape of data: {fire_data.shape}\")\nfire_data.rename(columns={\"acq_date\":\"Date\"}, inplace=True)\nfire_data[\"WEI Value\"] = 0\nfire_data['month'] = fire_data['Date'].apply(lambda x:int(x.split('-')[1]))\nfire_data.head()","d364a2fb":"wei = pd.read_excel('\/kaggle\/input\/weekly-economic-index-wei-federal-reserve-bank\/Weekly Economic Index.xlsx')\nwei.drop('WEI as of 7\/28\/2020',axis=1,inplace=True)\nwei = wei.set_index(\"Date\")\nwei.head()","4e7c7491":"from tqdm import tqdm\nfor index in tqdm(range(len(fire_data))):\n    fire_date = (fire_data[\"Date\"][index]) \n    fire_date = parser.parse(fire_date)\n    min_wei_date_value = wei.iloc[wei.index.get_loc(fire_date,method='nearest')][\"WEI\"]\n    fire_data.loc[index, \"WEI Value\"] = min_wei_date_value","e39c1f65":"fire_data['daynight'] = fire_data['daynight'].map({'D':0,'N':1})\nfire_data.drop('instrument', axis=1, inplace=True)","da96a7cd":"fire_data.head()","48a0caa7":"x = fire_data[['latitude','longitude','month','brightness','scan','track',\n               'acq_time','bright_t31','daynight','frp', 'confidence']]\ny = fire_data['WEI Value']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)","affcd7a9":"# from keras.models import Sequential\n# from keras.layers import Dense, Dropout, BatchNormalization\n# model = Sequential()\n# model.add(Dense(32,input_shape=(11,),activation='relu'))\n# model.add(BatchNormalization())\n# model.add(Dense(64,activation='relu'))\n# model.add(Dense(64,activation='relu'))\n# model.add(Dense(64,activation='relu'))\n# model.add(Dense(64,activation='relu'))\n# model.add(Dense(32,activation='relu'))\n# model.add(Dropout(0.2))\n# model.add(Dense(1,activation='linear'))\n# model.compile(optimizer='adam',metrics=['mae'],loss='mse')\n# model.fit(X_train, y_train, epochs=20)\nprint(\"Fail\")","cc276660":"# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.metrics import mean_absolute_error as mae\n# model = RandomForestRegressor(n_estimators = 200, max_depth = 15)\n# model.fit(X_train, y_train)\n# mae(model.predict(X_train), y_train)\n# print(f\"Train MAE: {mae(model.predict(X_train), y_train)}\")\n# print(f\"Test MAE: {mae(model.predict(X_test), y_test)}\")","2131155b":"# from sklearn.ensemble import RandomForestRegressor\n# parameters = {'n_estimators':[300,500,1000], 'max_depth':[5, 10, 20, 50, 100]}\n# from sklearn.model_selection import GridSearchCV\n# model2 = RandomForestRegressor()\n# clf = GridSearchCV(model2, parameters, cv=3)\n# clf.fit(X_train, y_train)","1ea22dfb":"# clf.best_params_","db6fd77a":"try:\n    from sklearn.ensemble import GradientBoostingRegressor\n    parameters = {'n_estimators':[100,500,1000], 'max_depth':[3, 5, 15, 50],\n                  \"learning_rate\":[0.05,0.1,0.2]}\n    from sklearn.model_selection import GridSearchCV\n    model2 = GradientBoostingRegressor()\n    clf = GridSearchCV(model2, parameters, n_jobs=-1, cv=3)\n    clf.fit(X_train, y_train)#, verbose=True)\nexcept:\n    from sklearn.ensemble import GradientBoostingRegressor\n    parameters = {'n_estimators':[100,300,500,1000], 'max_depth':[3, 5, 15, 50],\n                  \"learning_rate\":[0.05,0.1,0.2]}\n    from sklearn.model_selection import GridSearchCV\n    model2 = GradientBoostingRegressor()\n    clf = GridSearchCV(model2, parameters, cv=3)\n    clf.fit(X_train, y_train)#, verbose=True)\n\n\n# from sklearn.ensemble import GradientBoostingRegressor\n# model1 = GradientBoostingRegressor(n_estimators = 400, learning_rate=0.1,\n#                                   max_depth = 10, random_state = 0, loss = 'ls')\n# model1.fit(X_train, y_train)\n# print(f\"Train MAE: {mae(model1.predict(X_train), y_train)}\")\n# print(f\"Test MAE: {mae(model1.predict(X_test), y_test)}\")","105026a6":"clf.best_params_","3937a3b1":"from sklearn.metrics import mean_absolute_error as mae\nprint(f\"Train MAE: {mae(clf.predict(X_train), y_train)}\")\nprint(f\"Test MAE: {mae(clf.predict(X_test), y_test)}\")","2e844b7f":"# import shap\n# explainer = shap.TreeExplainer(model1)\n# shap_values = explainer.shap_values(X_test)\n# shap.summary_plot(shap_values, X_test)#, plot_type=\"bar\")","d5363484":"# import shap\n# explainer = shap.TreeExplainer(model1)\n# shap_values = explainer.shap_values(X_test)\n# shap.summary_plot(shap_values, X_test, plot_type=\"bar\")","20d04b1d":"# import eli5\n# from eli5.sklearn import PermutationImportance\n# perm = PermutationImportance(model1, random_state=1).fit(X_test, y_test)\n# eli5.show_weights(perm, feature_names = X_test.columns.tolist())","11ad58b2":"# from pdpbox import pdp, info_plots\n# import matplotlib.pyplot as plt\n# base_features = X.columns.values.tolist()\n# for column in X.columns:\n#     feat_name = column\n#     pdp_dist = pdp.pdp_isolate(model=model1, dataset=X_test, model_features=base_features, feature=feat_name)\n#     pdp.pdp_plot(pdp_dist, feat_name)\n#     plt.show()","b32dc5be":"## Neural Network Approach","bc3ab6d7":"# Assessing the Economic Impact of Wildires","35c05bef":"Loading weekly economic index data:","5c052197":"## Prediction Code","d58ee70f":"## Find best hyperparameters","e656eaff":"Checking back with our data:","e8441b64":"## Gradient Boosting Approach","3f80ceea":"### Merging Data Sets ","b09e5daa":"# WORK IN PROGRESS","8606505c":"## Simple Satellite Data Preprocessing","0be8dc1c":"Viewing data:","1a2e9bb7":"## Random Forest Approach","be0fff03":"# Explanations","a10dd84e":"## Loading Data","40b01006":"Checklist:\n- [X] Remove arbitary `instrument` variable\n- [X] Convert `daynight` variable into numerical"}}