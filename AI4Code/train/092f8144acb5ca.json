{"cell_type":{"c6a9f823":"code","7110d0d0":"code","db2dfd26":"code","070ae39f":"code","afb410f6":"code","4a8bd61c":"code","3dcdca55":"code","d8604aaf":"code","4f6be30b":"code","1e2d751d":"markdown","d586f00f":"markdown","e5512b2b":"markdown","64528490":"markdown","8d7a32e1":"markdown","c2957180":"markdown","4b579a2f":"markdown","cda8de07":"markdown","685167d6":"markdown","38d9ef7f":"markdown","6e3db100":"markdown","4007130d":"markdown","9b3b29a0":"markdown","157a5d79":"markdown","3ee77ce8":"markdown"},"source":{"c6a9f823":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport os\nimport tensorflow as tf","7110d0d0":"!nvidia-smi","db2dfd26":"train_path = '..\/input\/shopee-product-detection-student\/train\/train\/train\/'\ntest_path = '..\/input\/shopee-product-detection-student\/test\/test\/test\/'\n\nbroken_fnames = []\nfor label in os.listdir(train_path):\n    label_path = train_path + label + '\/'\n    for filename in os.listdir(label_path):\n        if len(filename) > 36:\n            print(label_path + filename)\n            broken_fnames.append(label_path + filename)\n            \nprint()\nfor filename in os.listdir(test_path):\n    if len(filename) > 36:\n        print(test_path + filename)\n        broken_fnames.append(test_path + filename)\n        \nf = open('broken-file-names.txt', 'w')\nf.write('\\n'.join(broken_fnames))\nf.close()","070ae39f":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nIMAGE_SIZE = (224, 224)\nBATCH_SIZE = 128\nSEED = 0\n\ndef get_set():\n    train_path = '..\/input\/shopee-product-detection-student\/train\/train\/train\/'\n    test_path = '..\/input\/shopee-product-detection-student\/test\/test\/'\n\n    train_gen = ImageDataGenerator(rescale=1.\/255, validation_split=3007.\/105390)\n    train_set = train_gen.flow_from_directory(train_path, target_size=IMAGE_SIZE, \\\n                                              batch_size=BATCH_SIZE, seed=SEED, \\\n                                              subset='training')\n    val_set = train_gen.flow_from_directory(train_path, target_size=IMAGE_SIZE, \\\n                                            batch_size=BATCH_SIZE, seed=SEED, \\\n                                            subset='validation')\n\n    test_gen = ImageDataGenerator(rescale=1.\/255)\n    test_set = train_gen.flow_from_directory(test_path, target_size=IMAGE_SIZE, \\\n                                             batch_size=BATCH_SIZE, seed=SEED, \\\n                                             shuffle=False, class_mode=None)\n    \n    return train_set, val_set, test_set\n\ntrain_set, val_set, test_set = get_set()","afb410f6":"from tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model\n\ndef get_model():\n    base = MobileNetV2(input_shape=IMAGE_SIZE+(3,), include_top=False, \\\n                       pooling='avg', weights='imagenet')\n    base.trainable = False\n    dense = Dense(42, activation='softmax', name='dense')(base.output)\n\n    model = Model(inputs=base.inputs, outputs=dense, name='mobilenetv2')\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n    return model\n\nmodel = get_model()","4a8bd61c":"EPOCHS = 3\n\nhist = model.fit(train_set, epochs=EPOCHS, batch_size=BATCH_SIZE, \\\n                 validation_data=val_set, shuffle=False)\nmodel.save('model-mobilenetv2.hdf5')","3dcdca55":"loss, acc = model.evaluate(val_set, batch_size=BATCH_SIZE)\nprint('Validation acc (percent): %.2f' % (100 * acc))","d8604aaf":"def generate_prediction(model, save_name):\n    subm = pd.read_csv('..\/input\/shopee-product-detection-student\/test.csv')\n    subm = subm.sort_values(by='filename', ignore_index=True)\n    \n    fnames = sorted(os.listdir('..\/input\/shopee-product-detection-student\/test\/test\/test'))\n    unbroken_index = np.where(np.vectorize(len)(np.array(fnames)) == 36)[0]\n    \n    y_pred = model.predict(test_set, batch_size=BATCH_SIZE)\n    pred = y_pred.argmax(axis=1)\n    pred = pred[unbroken_index]\n    subm['category'] = pred\n    subm['category'] = subm['category'].apply(lambda x : '%02d' % x) # zero pad\n    \n    subm.to_csv(save_name, index=False)\n    return subm","4f6be30b":"from tensorflow.keras.models import load_model\n\nmodel = load_model('model-mobilenetv2.hdf5')\nsubm = generate_prediction(model, '.\/submission.csv')\nsubm","1e2d751d":"We instantiate the MobileNetV2 as the feature extractor and set it as untrainable to train faster. The weights used are that used on ImageNet, and at the top of the extractor, we used global average pooling layer to minimize the output vector dimension. For the classifier, we only use a single softmax layer to output the predictions. Adam is used as the optimizer with its default parameter.","d586f00f":"## The Model","e5512b2b":"# Product Detection with Pre-Trained MobileNetV2 on Tensorflow 2.0","64528490":"Now is time to prepare the generator for the dataset. There are some notes for choosing the training and testing specification:\n\n- All of the images (train, validation, and test) are resized to shape of 224x224 px (the default image size MobileNetV2 is trained on)\n- Only 2,990 samples out of 105,390 training samples (~2%) are used for the validation set. This should be enough to validate the training result, with the remaining 102,400 samples for training are also large enough for the model to learn\n- Batching of the datasets with a size of 128 samples each. You could set smaller batch size if you prefer a more smooth gradient and more steps in one epoch, or larger batch size for more samples to be considered by the model to step over on the loss surface\n\nHowever we don't need to clean or preprocess it to keep the time short, and let the AI do the magic.","8d7a32e1":"We could recheck the final accuracy for the validation set.","c2957180":"## Dataset","4b579a2f":"This kernel is just used as a baseline, since the goal is only to get fairly enough accuracy in a short time. Further improvements could be done by augmenting the data or try different feature extractor, classifier, or hyperparameters.","cda8de07":"## Improvements","685167d6":"Since we have 102,400 training samples and batch size of 128, then there should be 102,400\/128 = 800 steps or weights updates per epoch. It should be enough to train the model for 3 epoch (total 2,400 weights updates). At the end of the training phase, we save the model to a `model-mobilenetv2.hdf5` file.","38d9ef7f":"This notebook demonstrate product detection using the pre-trained MobileNetV2 model from Tensorflow 2.0 framework. The goal is to get an accuracy score between 50 to 60 percent with minimal parameters and model size, so the experiment could be done in a short time.\n\nAccording to [this page](https:\/\/keras.io\/api\/applications\/) on Keras documentation, MobileNetV2 has the least model parameters (~3M) and least memory required (14MB). It is worth to note that with this minimal specifications, the accuracy on ImageNet validation set is equal to VGG model which has ~37x larger model parameters and ~46x larger memory size.\n\nLet's get started by importing library needed.","6e3db100":"There are roughly 105k samples on the training set and ~12k samples on the testing set provided in this competition. However, there are samples considered to be broken, that apparently they have file name with length more than 36 (32 characters long for hex encoding and `.jpg` format). We can detect their path using this script and generate it into a `.txt` file for deleting purposes, though keeping them won't cause any problem when loading the data.","4007130d":"## Generate Prediction","9b3b29a0":"It is recommended to use GPU for fast model training, inference, or dataset loading using ImageDataGenerator class provided by Tensorflow. To check what GPU is we assigned to, we could check using this Linux command.","157a5d79":"We make a function to generate `.csv` prediction with the model and saving directory as the arguments.","3ee77ce8":"## Training Phase"}}