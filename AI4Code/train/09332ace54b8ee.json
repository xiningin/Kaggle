{"cell_type":{"09ad2389":"code","eb6086b8":"code","94e27613":"code","a705a859":"code","d5f1aae3":"code","23f77e7c":"code","db8465e6":"code","6353bfc3":"code","f79de612":"code","ae9178d3":"code","3b646a82":"code","f4f5ff6d":"code","8ee7d34b":"code","37d2b00d":"code","015ff3ee":"code","9993a083":"code","2c1ff2a4":"code","74ef8a21":"code","35b6fdf2":"code","26d171a1":"code","44a3ed32":"code","f24c55bb":"code","ce2f1629":"code","c91de3a6":"code","db9c838e":"code","dc50cc58":"code","37df4a71":"code","e6d39e01":"code","f45fff33":"code","774f766e":"code","63ac033c":"code","380bc276":"code","0fdc30af":"code","832cfd73":"code","216e02c6":"code","544a25c1":"code","74b09805":"code","e8a336c9":"code","bd1ba92a":"code","397b1687":"code","65f14e91":"code","16b02e7d":"code","b11d3930":"code","f318904d":"code","e2425aa7":"code","184fbcc1":"code","4aebcc0f":"code","e859f1e7":"code","efb85afb":"code","9444eb93":"code","4372a24f":"code","99d39caf":"code","5387b12a":"code","cb06a482":"markdown","049e3267":"markdown","c02c61c0":"markdown","b9dcc1c2":"markdown","e6b2dfd2":"markdown","7b81bfa0":"markdown","7cb1c882":"markdown","d075f04b":"markdown","e3c136fb":"markdown","ce3ea779":"markdown","886f8f3b":"markdown","acfdf335":"markdown","f031064f":"markdown","a78398ea":"markdown","ea7ccfc9":"markdown","aebe3d5b":"markdown","26f4fb29":"markdown","ca2f03f6":"markdown","8fbe6bfb":"markdown","49134ecf":"markdown","af5b2929":"markdown","91f9c076":"markdown","a857be36":"markdown","27b28059":"markdown"},"source":{"09ad2389":"import pandas as pd\nimport numpy as np\nimport os\nimport string\nimport operator\nimport re\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop = set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nimport time\nfrom datetime import datetime\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam\n\n! pip install pyspellchecker\nfrom spellchecker import SpellChecker\n\nfrom google.cloud import storage\nfrom google.cloud import automl_v1beta1 as automl\n\n# workaround to fix gapic_v1 error\nfrom google.api_core.gapic_v1.client_info import ClientInfo\n\nfrom automlwrapper import AutoMLWrapper","eb6086b8":"# ABDOULAYE GCP ACCOUNT KEYS\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"bucket_name\")\nsecret_value_1 = user_secrets.get_secret(\"gcp_project\")","94e27613":"# Set your own values for these. bucket_name should be the project_id + '-lcm'.\nPROJECT_ID = secret_value_1\nbucket_name = secret_value_0\n\nregion = 'us-central1' # Region must be us-central1\ndataset_display_name = 'kaggle_tweets_bear'\nmodel_display_name = 'kaggle_tweets_bear_model1'\n\nstorage_client = storage.Client(project=PROJECT_ID)\n\n# adding ClientInfo here to get the gapic_v1 call in place\nclient = automl.AutoMlClient(client_info=ClientInfo())\n\nprint(f'Starting AutoML notebook at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')","a705a859":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d5f1aae3":"df_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ndf_submission = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n\ndef callback(operation_future):\n    result = operation_future.result()","23f77e7c":"# word_count\ndf_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\ndf_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ndf_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\ndf_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n\n# stop_word_count\ndf_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ndf_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# url_count\ndf_train['url_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\ndf_test['url_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n# mean_word_length\ndf_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ndf_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ndf_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\ndf_test['char_count'] = df_test['text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ndf_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ndf_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\ndf_train['hashtag_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\ndf_test['hashtag_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\ndf_train['mention_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\ndf_test['mention_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@']))","db8465e6":"# df = pd.concat([df_train, df_test])","6353bfc3":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","f79de612":"df_train['text'] = df_train['text'].apply(lambda x : remove_URL(x))\ndf_test['text'] = df_test['text'].apply(lambda x : remove_URL(x))","ae9178d3":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","3b646a82":"df_train['text'] = df_train['text'].apply(lambda x : remove_html(x))\ndf_test['text'] = df_test['text'].apply(lambda x : remove_html(x))","f4f5ff6d":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","8ee7d34b":"df_train['text'] = df_train['text'].apply(lambda x: remove_emoji(x))\ndf_test['text'] = df_test['text'].apply(lambda x: remove_emoji(x))","37d2b00d":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","015ff3ee":"df_train['text'] = df_train['text'].apply(lambda x: remove_punct(x))\ndf_test['text'] = df_test['text'].apply(lambda x: remove_punct(x))","9993a083":"# spell = SpellChecker()\n# def correct_spellings(text):\n#     corrected_text = []\n#     misspelled_words = spell.unknown(text.split())\n#     for word in text.split():\n#         if word in misspelled_words:\n#             corrected_text.append(spell.correction(word))\n#         else:\n#             corrected_text.append(word)\n#     return \" \".join(corrected_text)","2c1ff2a4":"# df_train['text'] = df_train['text'].apply(lambda x: correct_spellings(x))\n# df_test['text'] = df_test['text'].apply(lambda x: correct_spellings(x))","74ef8a21":"# df = pd.concat([df_train, df_test])","35b6fdf2":"# def cv(data):\n#     count_vectorizer = CountVectorizer()\n#     emb = count_vectorizer.fit_transform(data)\n#     return emb, count_vectorizer\n\n# list_corpus = df[\"text\"].tolist()\n# list_labels = df[\"target\"].tolist()\n\n# X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \n#                                                                                 random_state=40)\n\n# X_train_counts, count_vectorizer = cv(X_train)\n# X_test_counts = count_vectorizer.transform(X_test)","26d171a1":"# def cv(data):\n#     count_vectorizer = CountVectorizer()\n#     emb = count_vectorizer.fit_transform(data)\n#     return emb, count_vectorizer\n\n\n# list_corpus = df[\"text\"].tolist()\n\n# df_counts, _ = cv(list_corpus)","44a3ed32":"# def tfidf(data):\n#     tfidf_vectorizer = TfidfVectorizer()\n#     train = tfidf_vectorizer.fit_transform(data)\n#     return train, tfidf_vectorizer\n\n# X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n# X_test_tfidf = tfidf_vectorizer.transform(X_test)","f24c55bb":"# df = pd.concat([df_train, df_test])","ce2f1629":"# def create_corpus_new(df):\n#     corpus = []\n#     for tweet in tqdm(df['text']):\n#         words = [word.lower() for word in word_tokenize(tweet)]\n#         corpus.append(words)\n#     return corpus","c91de3a6":"# corpus = create_corpus_new(df)","db9c838e":"# ! wget http:\/\/nlp.stanford.edu\/data\/glove.twitter.27B.zip\n# ! unzip glove.twitter.27B.zip","dc50cc58":"\n# embedding_dict = {}\n\n# with open('\/kaggle\/working\/glove.twitter.27B.200d.txt','r') as f:\n#     for line in f:\n#         values = line.split()\n#         word = values[0]\n#         vectors = np.asarray(values[1:],'float32')\n#         embedding_dict[word] = vectors\n        \n# f.close()","37df4a71":"# MAX_LEN = 50\n# tokenizer_obj = Tokenizer()\n# tokenizer_obj.fit_on_texts(corpus)\n# sequences = tokenizer_obj.texts_to_sequences(corpus)\n# tweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')","e6d39e01":"# word_index = tokenizer_obj.word_index\n# print('Number of unique words:', len(word_index))","f45fff33":"# num_words = len(word_index) + 1\n# embedding_matrix = np.zeros((num_words, 200))\n\n# for word,i in tqdm(word_index.items()):\n#     if i < num_words:\n#         emb_vec = embedding_dict.get(word)\n#         if emb_vec is not None:\n#             embedding_matrix[i] = emb_vec   ","774f766e":"# model = Sequential()\n\n# embedding = Embedding(input_dim=num_words,\n#                       output_dim=200, \n#                       embeddings_initializer=Constant(embedding_matrix), \n#                       input_length=MAX_LEN,\n#                       trainable=False)\n\n# model.add(embedding)\n# model.add(SpatialDropout1D(0.2))\n# model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n# model.add(Dense(1, activation='sigmoid'))\n\n# optimzer = Adam(learning_rate=1e-5)\n# model.compile(loss='binary_crossentropy', \n#               optimizer=optimzer, \n#               metrics=['accuracy'])","63ac033c":"# model.summary()","380bc276":"# train = tweet_pad[:df_train.shape[0]]\n# test = tweet_pad[df_train.shape[0]:]","0fdc30af":"# X_train, X_test, y_train, y_test = train_test_split(train, df_train['target'].values, test_size=0.15)\n\n# print('Shape of train', X_train.shape)\n# print(\"Shape of Validation\", X_test.shape)","832cfd73":"# history = model.fit(X_train, \n#                     y_train, \n#                     batch_size=4, \n#                     epochs=100, \n#                     validation_data=(X_test,y_test), \n#                     verbose=2)","216e02c6":"# y_pre = model.predict(test)\n# y_pre = np.round(y_pre).astype(int).reshape(3263)\n# sub = pd.DataFrame({'id': df_submission['id'].values.tolist(), 'target': y_pre})\n# sub.to_csv('submission.csv', index=False)","544a25c1":"def upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket. https:\/\/cloud.google.com\/storage\/docs\/ \"\"\"\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}'.format(\n        source_file_name,\n        'gs:\/\/' + bucket_name + '\/' + destination_blob_name))\n    \ndef download_to_kaggle(bucket_name,destination_directory,file_name,prefix=None):\n    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n    os.makedirs(destination_directory, exist_ok = True)\n    full_file_path = os.path.join(destination_directory, file_name)\n    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n    for blob in blobs:\n        blob.download_to_filename(full_file_path)","74b09805":"bucket = storage.Bucket(storage_client, name=bucket_name)\n\nif not bucket.exists():\n    bucket.create(location=region)\n    \nprint('GCP bucket created')","e8a336c9":"del df_train['location']\ndel df_train['keyword']","bd1ba92a":"# Select the text body and the target value, for sending to AutoML NL\ndf_train[['text','target']].to_csv('train.csv', index=False, header=False)\ndf_train.to_csv('fulltrain.csv', index=False, header=False) # train with handcrafted features","397b1687":"from sklearn.preprocessing import StandardScaler\ndf_train.iloc[:,3:] = StandardScaler().fit_transform(df_train.iloc[:,3:])","65f14e91":"print('starting data upload')\n\ntraining_gcs_path0 = 'uploads\/kaggle\/train.csv'\ntraining_gcs_path = 'uploads\/kaggle\/full_train.csv'\nupload_blob(bucket_name, 'train.csv', training_gcs_path0)\nupload_blob(bucket_name, 'fulltrain.csv', training_gcs_path)\n\nprint('data upload completed')","16b02e7d":"amw = AutoMLWrapper(client=client, \n                    project_id=PROJECT_ID, \n                    bucket_name=bucket_name, \n                    region='us-central1', \n                    dataset_display_name=dataset_display_name, \n                    model_display_name=model_display_name)\n\nprint('AutoML wrapper created')","b11d3930":"print(f'Getting dataset ready at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\nif not amw.get_dataset_by_display_name(dataset_display_name):\n    print('dataset not found')\n    amw.create_dataset()\n    amw.import_gcs_data(training_gcs_path)\n\namw.dataset\nprint(f'Dataset ready at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n","f318904d":"print(f'Getting model trained at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n\nif not amw.get_model_by_display_name():\n    print(f'Training model at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n    amw.train_model()\n\nprint(f'Model trained. Ensuring model is deployed at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\namw.deploy_model()\namw.model\nprint(f'Model trained and deployed at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n","e2425aa7":"amw.model_full_path","184fbcc1":"df_test.head()","4aebcc0f":"print(f'Begin getting predictions at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')\n\n# Create client for prediction service.\nprediction_client = automl.PredictionServiceClient()\namw.set_prediction_client(prediction_client)\n\npredictions_df = amw.get_predictions(df_test, \n                                     input_col_name='text', \n#                                      ground_truth_col_name='target', # we don't have ground truth in our test set\n                                     limit=None, \n                                     threshold=0.5,\n                                     verbose=False)\n\nprint(f'Finished getting predictions at {datetime.fromtimestamp(time.time()).strftime(\"%Y-%m-%d, %H:%M:%S UTC\")}')","e859f1e7":"amw.undeploy_model()","efb85afb":"predictions_df.head()","9444eb93":"submission_df = pd.concat([df_test['id'], predictions_df['class']], axis=1)\nsubmission_df.head()","4372a24f":"# predictions_df['class'].iloc[:10]\n# nlp_test_df['id']","99d39caf":"submission_df = submission_df.rename(columns={'class':'target'})\nsubmission_df.head()","5387b12a":"submission_df.to_csv(\"submission.csv\", index=False, header=True)","cb06a482":"### Glove","049e3267":"## Spelling correction","c02c61c0":"## (optional) Undeploy model\nUndeploy the model to stop charges","b9dcc1c2":"### Export to CSV and upload to GCS","e6b2dfd2":"## Create our class instance","7b81bfa0":"### Removing Emojis","7cb1c882":"### Removing Punctuations","d075f04b":"## Submit predictions to the competition!","e3c136fb":"### TF IDF","ce3ea779":"## Bag of Words Counts","886f8f3b":"### Baseline Model","acfdf335":"## Create submission output","f031064f":"## Kick off the training for the model\nAnd retrieve the training info after completion. \nStart model deployment.","a78398ea":"### Making Submission","ea7ccfc9":"## Meta Features","aebe3d5b":"## Prediction\nNote that prediction will not run until deployment finishes, which takes a bit of time.\nHowever, once you have your model deployed, this notebook won't re-train the model, thanks to the various safeguards put in place. Instead, it will take the existing (trained) model and make predictions and generate the submission file.","26f4fb29":"### GCS upload\/download utilities\nThese functions make upload and download of files from the kernel to Google Cloud Storage easier. This is needed for AutoML.","ca2f03f6":"### Removing HTML tags","8fbe6bfb":"```\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_2 = user_secrets.get_secret(\"google_bucket_name\")\nsecret_value_3 = user_secrets.get_secret(\"google_project_id\")\n```","49134ecf":"## Data Cleaning","af5b2929":"### Removing urls","91f9c076":"This notebook utilizes a utility script that wraps much of the AutoML Python client library, to make the code in this notebook easier to read. Feel free to check out the utility for all the details on how we are calling the underlying AutoML Client Library!","a857be36":"## Create (or retreive) dataset\nCheck to see if this dataset already exists. If not, create it","27b28059":"### Standardize data"}}