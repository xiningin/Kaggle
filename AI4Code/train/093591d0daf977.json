{"cell_type":{"2a3468f8":"code","e3ce9e25":"code","dd1bf4fb":"code","5ba24e70":"code","00234dc1":"code","12827e98":"code","ad95f65c":"code","748396df":"code","985ff8d4":"code","4d0d1cd3":"code","b0c84809":"code","4b8aa332":"code","2c483d42":"code","05cdfcce":"code","58fb8cf4":"code","c0cc78e1":"code","8a1eab6c":"code","05f2e77e":"code","42372fba":"code","1801d601":"code","2dfc0f4b":"code","c3a3ee59":"code","aec43d69":"code","e4696f66":"code","32726104":"code","f8e66bdd":"code","19e95164":"code","f8685e0f":"code","3c65a14c":"code","e8c9d5d6":"code","d154e3ad":"code","bea2a58b":"code","b9f5b3e5":"code","29459581":"code","d5445316":"code","1fb4c88e":"code","06bef9c4":"code","71770e4d":"code","2c2fdb0f":"markdown","070f306e":"markdown","412668c4":"markdown","e93b4161":"markdown","486f5c98":"markdown","e0adbe43":"markdown","589e578d":"markdown","a5935b62":"markdown","12f356e7":"markdown","25dd420d":"markdown","95e95ded":"markdown","17135bde":"markdown","0f7fac22":"markdown","5b80f071":"markdown","6242e509":"markdown","acdce0ca":"markdown","72a5c55a":"markdown","6d804785":"markdown","1153e028":"markdown","ccfdd150":"markdown","5b974658":"markdown","158a8c3a":"markdown","0ac17db1":"markdown","83e1d776":"markdown","53f6f4d5":"markdown","0c25f00e":"markdown","b86c5bff":"markdown","3c1b76d8":"markdown","5ed776b8":"markdown","c96a43d3":"markdown","1b9ff9a1":"markdown"},"source":{"2a3468f8":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=UserWarning)\nsns.set(style=\"white\")\n\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\n\nfrom scipy.cluster.vq import kmeans, vq\n\nfrom sklearn.cluster import KMeans\n\nfrom collections import Counter\n\ndf = pd.read_csv('..\/input\/goodreadsbooks\/books.csv', error_bad_lines=False)","e3ce9e25":"df.head()","dd1bf4fb":"columns_to_drop = ['bookID', 'isbn', 'isbn13']\n\ndf.drop(columns=columns_to_drop, inplace=True)","5ba24e70":"df.info()","00234dc1":"df['publication_date'] = pd.to_datetime(df['publication_date'], errors='coerce')","12827e98":"df['publication_date'].head()","ad95f65c":"df.isnull().sum()","748396df":"df.loc[df['publication_date'].isnull()]","985ff8d4":"df.loc[8177, 'publication_date'] = pd.to_datetime('1999-08-01')\ndf.loc[11094, 'publication_date'] = pd.to_datetime('1975-01-01')","4d0d1cd3":"df[df.duplicated()]","b0c84809":"df.describe()","4b8aa332":"numeric_features = df.select_dtypes(include=[np.number])","2c483d42":"def plot_histogram(column_name):\n    \n    \"\"\"this function takes the name of the column\n    returns plotly histogram\n    \n    arguments: column_name: str\"\"\"\n    \n    fig = px.histogram(data_frame=df, x=column_name, \n                       color_discrete_sequence=['mintcream'], log_y = True,\n                      nbins=30)\n\n    \n    fig.update_layout(font=dict(family='Lato', size=18, color='white'), \n                title=dict(text=f'<b>{column_name}<b>',\n                        font=dict(size=30), x=.5), \n                paper_bgcolor= 'goldenrod', plot_bgcolor='goldenrod',\n                xaxis=dict(title=column_name, showgrid=False),\n                yaxis=dict(title=f'count', showgrid=False))\n    \n    fig.show()","05cdfcce":"for feature in numeric_features:\n    plot_histogram(feature)","58fb8cf4":"df['year'] = df['publication_date'].dt.year","c0cc78e1":"plot_histogram('year')","8a1eab6c":"def bar_plot(column_name, data_frame=df, tribe='value_counts', by=None, limit=3, **kwargs):\n    \n    \"\"\"arguments:\n    column_name:str name of column from pandas data drame\n    data_frame:pandas data frame default df\n    tribe:str value counts by default, change to 'sort'\n    by:str works only if sort tribe chosed, takes name of the column\n        you want to sort values by\"\n    limit:int limit of values displayed\"\"\"\n    \n    \n    # if column is not str type or column not in columns of data frame\n    if type(column_name) != str or column_name not in data_frame.columns:\n        raise ValueError('Incorect column name or type')\n        \n    if type(limit) != int:\n        raise ValueErroc(f'excepted int type, got {type(limit)}')\n    \n    \n    # sorted data frame\n    if tribe == 'sort' :\n        # if column is not in data frame\n        if not by or by not in data_frame.columns:\n            raise ValueError('by paramter must be column from data frame!')\n        data = data_frame.sort_values(by=by, ascending=False).head(limit)\n        y = data[by].values\n        x = data[column_name]\n        title = by\n    \n    # value counts\n    elif tribe == 'value_counts':\n        data = data_frame[column_name].value_counts().head(limit)\n        x=data.index\n        y=data.values\n        title = column_name\n    \n    \n    fig = px.bar(x=x, \n                 y=y,\n            color_discrete_sequence=['mintcream']\n                )\n\n    fig.update_layout(font=dict(family='Lato', size=18, color='white'), \n                  title=dict(text=f'<b>{title}<b>',\n                           font=dict(size=30), x=.5), \n                  paper_bgcolor= 'goldenrod', plot_bgcolor='goldenrod',\n                 xaxis=dict(title=f'', showgrid=False),\n                 yaxis=dict(title=f'count', showgrid=False))\n        \n    fig.show()","05f2e77e":"bar_plot('publisher', limit=10)","42372fba":"bar_plot('authors', limit=10)","1801d601":"bar_plot('language_code')","2dfc0f4b":"df.rename(columns={'  num_pages': 'num_pages'}, inplace=True)\nbar_plot('title', tribe='sort', by='num_pages', limit=5)","c3a3ee59":"bar_plot('title', tribe='sort', by='ratings_count')","aec43d69":"def correlation_heatmap(data_frame=df):\n    \"\"\"arguments: data_frame:pandas DataFrame\n       returns: correlation heatmap\"\"\"\n    \n    #  setting the context\n    sns.set(context='paper', font='moonspace')\n    \n    #  making correlation object and saving it into variable\n    correlation = df.corr()\n    \n    #  creating heatmap figure object (paper) and ax object (the plot)\n    fig, ax = plt.subplots(figsize=(12, 8))\n    \n    #  generating color palettes\n    cmap = sns.diverging_palette(220, 10, center='light', as_cmap=True)\n    \n    #  draw the heatmap\n    heatmap = sns.heatmap(correlation, vmax=1,vmin=-1,center=0, square=False, annot=True, cmap=cmap,\n                         lw=2, cbar=False)\n    \n    return heatmap","e4696f66":"correlation_heatmap();","32726104":"def scatter_plot(x, y, data_frame=df):\n    \"\"\"this function takes x axis name and y axis name\n    returns: plotly scatter plot\"\"\"\n    \n    fig = px.scatter(data_frame=data_frame, x=x, y=y, trendline='ols',\n                    color_discrete_sequence=['mintcream'],\n                    trendline_color_override=\"lime\")\n    \n    fig.update_layout(font=dict(family='Lato', size=18, color='white'), \n                  title=dict(text=f'<b>{x} and {y}<b>',\n                           font=dict(size=30), x=.5), \n                  paper_bgcolor= 'maroon', plot_bgcolor='maroon',\n                 xaxis=dict(title=f'', showgrid=False),\n                 yaxis=dict(title=f'count', showgrid=False))\n    \n    fig.show()","f8e66bdd":"scatter_plot(x='average_rating',y='num_pages')","19e95164":"scatter_plot(x='text_reviews_count',y='ratings_count')","f8685e0f":"train = df.copy()","3c65a14c":"def detect_outliers(data_frame=train, n=2):\n    \"\"\"this funtion takes data frame and finds indexes of rows\n    where it's more than n outliers\n    arguments: data_frame: pandas DataFrame, default train\n    n:int - takes the number n and \n    \"\"\"\n    \n    numeric_columns = data_frame.select_dtypes(include=[np.number])\n    outlires_indicies = []\n    \n    for column in numeric_columns:\n        \n        #  first quartile - 25 %\n        Q1 = np.nanpercentile(data_frame[column], 25)\n        \n        #  second quartile - 75%\n        Q2 = np.nanpercentile(data_frame[column], 75)\n        \n        #  calculating interquartile range\n        IQR = Q2 - Q1\n        \n        # setting the bounds\n        lower_bound = Q1 - (1.5 * IQR)\n        upper_bound = Q2 + (1.5 * IQR)\n        \n        # indexes of the outliers\n        outliers = data_frame[(data_frame[column] < lower_bound) | (data_frame[column] > upper_bound)].index\n        \n        #  extending the indicies to the list\n        outlires_indicies.extend(outliers)\n    \n    #  this frequency table is useful to see occurences of outliers in rows\n    freq_table = Counter(outlires_indicies)\n    \n    #  showing only indicies where theres more than n outliers\n    return [index for index, occurences in freq_table.items() if occurences > n]","e8c9d5d6":"outliers = detect_outliers()\ntrain.drop(index=outliers, inplace=True)\nf'outliers droped - {len(outliers)}'","d154e3ad":"X = train[['average_rating', 'ratings_count']].values","bea2a58b":"distorsions = []\n\nfor x in range (2, 31):\n    k_means = KMeans(n_clusters=x)\n    k_means.fit(X)\n    distorsions.append(k_means.inertia_)","b9f5b3e5":"fig = px.line(x=range(2, 31), y=distorsions)\n\nfig.update_layout(xaxis=dict(title='number of clusters'),\n                 yaxis=dict(title='distorsion'),\n                 title=dict(text='Elbow method', x=.5, \n                           font=dict(size=30)),\n                 font=dict(family='Lato', size=16))\n\nfig.add_vline(x=5, line_width=3, line_dash='dash', line_color='green')\n\n\nfig.show()","29459581":"centroids, _ = kmeans(X, 5) # making 5 clusters\n\nidx, _ = vq(X, centroids)","d5445316":"train['clusters'] = idx","1fb4c88e":"fig = px.scatter(data_frame=train, x='average_rating', y='ratings_count', color='clusters',\n          hover_data=['title'],height=700)\n\nfig.update_coloraxes(showscale=False)\n\nfig.update_layout(xaxis=dict(title='average rating'),\n                 yaxis=dict(title='ratings_count'),\n                 title=dict(text='Goodreads - clustering', x=.5, \n                           font=dict(size=30)),\n                 font=dict(family='Lato', size=16))\n\nfig","06bef9c4":"train = train.loc[train['ratings_count'] <= 3e6]","71770e4d":"fig = px.scatter(data_frame=train, x='average_rating', y='ratings_count', color='clusters',\n          hover_data=['title'],height=700)\n\nfig.update_coloraxes(showscale=False)\n\n\nfig.update_layout(xaxis=dict(title='average rating'),\n                 yaxis=dict(title='ratings_count'),\n                 title=dict(text='Goodreads - clustering', x=.5, \n                           font=dict(size=30)),\n                 font=dict(family='Lato', size=16))\n\nfig","2c2fdb0f":"Looks like theres no duplicates in this dataset, let's fall into EDA","070f306e":"## 1. Data preprocessing","412668c4":"### 1.2 DataTypes","e93b4161":"The best number of clusters **is 5.**","486f5c98":"### 1.3 NULL  and DUPLICATES","e0adbe43":"From the charts above the following conclusions can be drawn:\n\n* average_rating - mostly rates are between 2 and 5 but there's some outliers which have less\n* num_pages - this distribution is **right skewed** - the biggest amount of books have 0 - 500 pages but there's one outlier which have even number between 6500 - 6999\n* ratings_count - ratings count is also **right skewed** as well with visible outliers\n* text_reviews_count - distribution is also **right skewed** with some visible outliers ","589e578d":"**LANGUAGE**","a5935b62":"**Duplicates**","12f356e7":"Now let's look for the best books and writers\n","25dd420d":"**AUTHORS**","95e95ded":"KMeans clustering is type of the unsupervied learning which groups the data in unlabelled data.","17135bde":"Only 2 null values in publication_date, let's see:","0f7fac22":"in this case it is possible to enter blank values manually, so:\n\n[In Pursuit of the Proper Sinner](https:\/\/www.goodreads.com\/book\/show\/31373.In_Pursuit_of_the_Proper_Sinner) - August 1999\n\n[Montaillou village occitan de 1294 \u00e0 1324](https:\/\/en.wikipedia.org\/wiki\/Montaillou_(book)) - month in this case is not provided but there's year - 1975. I'm going to set date 1st of January\n","5b80f071":"**CORRELATION**","6242e509":"**PUBLISHER**","acdce0ca":"From the plot above we can see because of one outlier - The Twilight all clustering is skewed. I'm going to left only the books with rating_count less than 4M.","72a5c55a":"## 2. EDA","6d804785":"**RATINGS COUNT**","1153e028":"One of the most important and time consuming thing in every data analysis is handling with null values.","ccfdd150":"At first glance we can see that number of books increasing from 1900 to 2000 with, then it stared to decreasing","5b974658":"Let's see the distributions of numeric values:","158a8c3a":"Before going futher to modeling the data and making clusters with Kmeans I think that good idea is to find outliers and drop them from train data","0ac17db1":"From the plot above, we can see that the whole system can be classified to the clusters. \nAs the count increases, the rating would end up near the cluster given above.\nAs the rating count seems to decrease, the average rating seems to become sparser, with higher volatility and less accuracy.","83e1d776":"There's weak correlation between number of pages and average rating and much bigger correlation between text_reviews_count and ratings_count","53f6f4d5":"## 3. MODELING","0c25f00e":"Publication date should be converted to datetime, I think that rest is OK.","b86c5bff":"Basic informations:\n\n\nRows: 11123\n\nColumns: 12\n\n\n\n\nThis is really inspiring dataset, questions which comes to my mind right now are:\n\n* books with how much pages people like to read the best?\n* which authors are the most popular ?","3c1b76d8":"Next thing may be interesting is distribution of the books per year.","5ed776b8":"### 2.1 DESCRIBE METHOD","c96a43d3":"### 1.1 Basic inspections","1b9ff9a1":"Columns:\n\n* bookID\n* isbn - unique number to identify the book, the International Standard Book Number.\n* isbn13- A 13-digit ISBN to identify the book, instead of the standard 11-digit ISBN.\n\ndon't contain any useful informations so they should be droped"}}