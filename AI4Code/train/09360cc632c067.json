{"cell_type":{"435f59d4":"code","4d71d1fe":"code","08cfa31d":"code","56e98fe4":"code","7f4ee76a":"code","dee6c29d":"code","b1c9f878":"code","5c0de2df":"code","b2456d3c":"code","f75331c9":"code","3fc11753":"code","28d4a7bf":"code","97175092":"code","45056ebe":"code","c08c6d5b":"code","24fa8d3f":"markdown","35c2e399":"markdown","aff88b75":"markdown","6e8c048c":"markdown"},"source":{"435f59d4":"!pip install -q git+https:\/\/github.com\/tensorflow\/examples.git","4d71d1fe":"import tensorflow as tf\nfrom tensorflow_examples.models.pix2pix import pix2pix\nimport tensorflow_datasets as tfds\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt\n#I will use the Oxford-IIIT Pets dataset, that is already included in Tensorflow:\n\ndataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)\ndataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)","08cfa31d":"import warnings\n\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()","56e98fe4":"def normalize(input_image, input_mask):\n  input_image = tf.cast(input_image, tf.float32) \/ 255.0\n  input_mask -= 1\n  return input_image, input_mask\n\n@tf.function\ndef load_image_train(datapoint):\n  input_image = tf.image.resize(datapoint['image'], (128, 128))\n  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n\n  if tf.random.uniform(()) > 0.5:\n    input_image = tf.image.flip_left_right(input_image)\n    input_mask = tf.image.flip_left_right(input_mask)\n\n  input_image, input_mask = normalize(input_image, input_mask)\n\n  return input_image, input_mask\n\ndef load_image_test(datapoint):\n  input_image = tf.image.resize(datapoint['image'], (128, 128))\n  input_mask = tf.image.resize(datapoint['segmentation_mask'], (128, 128))\n\n  input_image, input_mask = normalize(input_image, input_mask)\n\n  return input_image, input_mask","7f4ee76a":"TRAIN_LENGTH = info.splits['train'].num_examples\nBATCH_SIZE = 64\nBUFFER_SIZE = 1000\nSTEPS_PER_EPOCH = TRAIN_LENGTH \/\/ BATCH_SIZE\n\ntrain = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE)\ntest = dataset['test'].map(load_image_test)\n\ntrain_dataset = train.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\ntrain_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\ntest_dataset = test.batch(BATCH_SIZE)","dee6c29d":"def display(display_list):\n  plt.figure(figsize=(15, 15))\n\n  title = ['Input Image', 'True Mask', 'Predicted Mask']\n\n  for i in range(len(display_list)):\n    plt.subplot(1, len(display_list), i+1)\n    plt.title(title[i])\n    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n    plt.axis('off')\n  plt.show()\n  \n  \nfor image, mask in train.take(1):\n  sample_image, sample_mask = image, mask\ndisplay([sample_image, sample_mask])","b1c9f878":"base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n\n# Use the activations of these layers\nlayer_names = [\n    'block_1_expand_relu',   # 64x64\n    'block_3_expand_relu',   # 32x32\n    'block_6_expand_relu',   # 16x16\n    'block_13_expand_relu',  # 8x8\n    'block_16_project',      # 4x4\n]\nbase_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n\n# Create the feature extraction model\ndown_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n\ndown_stack.trainable = False","5c0de2df":"up_stack = [\n    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n]\n\n\ndef unet_model(output_channels):\n  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n\n  # Downsampling through the model\n  skips = down_stack(inputs)\n  x = skips[-1]\n  skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n  for up, skip in zip(up_stack, skips):\n    x = up(x)\n    concat = tf.keras.layers.Concatenate()\n    x = concat([x, skip])\n\n  # This is the last layer of the model\n  last = tf.keras.layers.Conv2DTranspose(\n      output_channels, 3, strides=2,\n      padding='same')  #64x64 -> 128x128\n\n  x = last(x)\n\n  return tf.keras.Model(inputs=inputs, outputs=x)","b2456d3c":"OUTPUT_CHANNELS = 3","f75331c9":"model = unet_model(OUTPUT_CHANNELS)\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","3fc11753":"tf.keras.utils.plot_model(model, show_shapes=True)","28d4a7bf":"def create_mask(pred_mask):\n  pred_mask = tf.argmax(pred_mask, axis=-1)\n  pred_mask = pred_mask[..., tf.newaxis]\n  return pred_mask[0]\n\ndef show_predictions(dataset=None, num=1):\n  if dataset:\n    for image, mask in dataset.take(num):\n      pred_mask = model.predict(image)\n      display([image[0], mask[0], create_mask(pred_mask)])\n  else:\n    display([sample_image, sample_mask,\n             create_mask(model.predict(sample_image[tf.newaxis, ...]))])\nshow_predictions()","97175092":"class DisplayCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    clear_output(wait=True)\n    show_predictions()\n    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))\n    \n    \nEPOCHS = 20\nVAL_SUBSPLITS = 5\nVALIDATION_STEPS = info.splits['test'].num_examples\/\/BATCH_SIZE\/\/VAL_SUBSPLITS\n\nmodel_history = model.fit(train_dataset, epochs=EPOCHS,\n                          steps_per_epoch=STEPS_PER_EPOCH,\n                          validation_steps=VALIDATION_STEPS,\n                          validation_data=test_dataset,\n                          callbacks=[DisplayCallback()])","45056ebe":"loss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\n\nepochs = range(EPOCHS)\n\nplt.figure()\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'bo', label='Validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss Value')\nplt.ylim([0, 1])\nplt.legend()\nplt.show()","c08c6d5b":"show_predictions(test_dataset, 3)","24fa8d3f":"This was an image segmentage article from AMAN KHARWAL that I found this perfect article from this link:\n\nI learn alot from it\nhttps:\/\/thecleverprogrammer.com\/2020\/07\/22\/image-segmentation\/","35c2e399":"Being a practitioner in Machine Learning, you must have gone through an image classification, where the goal is to assign a label or a class to the input image. Now, suppose you want to get where the object is present inside the image, the shape of the object, or what pixel represents what object. In such a case, you have to play with the segment of the image, from which I mean to say to give a label to each pixel of the image. The goal of Image Segmentation is to train a Neural Network which can return a pixel-wise mask of the image.","aff88b75":"#                                                       Image Segmentation","6e8c048c":"This was a image segmentage article from AMAN KHARWAL that I found this perfect note from this link:\n\nhttps:\/\/thecleverprogrammer.com\/2020\/07\/22\/image-segmentation\/"}}