{"cell_type":{"2d2be1f6":"code","6f6b6dbe":"code","aca0cb73":"code","cad48683":"code","f2c74744":"code","a3650979":"code","f203956a":"code","4f71a3f6":"code","24725c19":"code","d99b9e7e":"code","5328f0a3":"code","f06fc111":"code","5b862a91":"code","82d1540b":"code","b1f6c08c":"code","ab918279":"code","c07885ac":"code","3d8ac3be":"code","cac704ae":"code","73554ba7":"code","179eb0f7":"code","9c4f7caf":"code","53b76875":"code","1c3ec733":"code","872fcbe4":"code","62852dee":"code","64cb2a4a":"code","d91e6a01":"code","9d129719":"markdown","9ce72f06":"markdown","fa7ab881":"markdown","f651dc35":"markdown","891e1811":"markdown","4a6caee2":"markdown","f8e935ab":"markdown","62b1da39":"markdown","b527994b":"markdown","1313b067":"markdown","f9ad58f7":"markdown","d7cca3d0":"markdown"},"source":{"2d2be1f6":"!pip install visualkeras","6f6b6dbe":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport visualkeras\nfrom math import sin, cos, pi\nimport cv2, os\nfrom tqdm.auto import tqdm\nfrom keras import layers, callbacks, utils, applications, optimizers\nfrom keras.models import Sequential, Model, load_model","aca0cb73":"class config:           #\u5b9a\u4e49\u7c7b\u51b3\u5b9a\u6570\u636e\u589e\u5f3a\u7684\u65b9\u6848\n    horizontal_flip = False\n    rotation_augmentation = True\n    brightness_augmentation = True\n    shift_augmentation = True\n    random_noise_augmentation = True\n\n    rotation_angles = [12]    # Rotation angle in degrees (includes both clockwise & anti-clockwise rotations)\n    pixel_shifts = [12]    # Horizontal & vertical shift amount in pixels (includes shift from all 4 corners)\n\n    NUM_EPOCHS = 100\n    BATCH_SIZE = 64","cad48683":"print(\"Contents of input\/facial-keypoints-detection directory: \")\n!ls ..\/input\/facial-keypoints-detection\/\n\nprint(\"\\nExtracting .zip dataset files to working directory ...\")\n!unzip -u ..\/input\/facial-keypoints-detection\/test.zip\n!unzip -u ..\/input\/facial-keypoints-detection\/training.zip\n\nprint(\"\\nCurrent working directory:\")\n!pwd\nprint(\"\\nContents of working directory:\")\n!ls","f2c74744":"%%time\n\ntrain_file = 'training.csv'\ntest_file = 'test.csv'\nidlookup_file = '..\/input\/facial-keypoints-detection\/IdLookupTable.csv'\ntrain_data = pd.read_csv(train_file)\ntest_data = pd.read_csv(test_file)\nidlookup_data = pd.read_csv(idlookup_file)","a3650979":"def plot_sample(image, keypoint, axis, title):\n    image = image.reshape(96,96)\n    axis.imshow(image, cmap='gray')\n    axis.scatter(keypoint[0::2], keypoint[1::2], marker='x', s=20)\n    plt.title(title)","f203956a":"train_data.head()     #\u5305\u542b\u4e86\u7528\u4e8e\u8bad\u7ec3\u7684\u4eba\u8138\u5173\u952e\u70b9\u5750\u6807\u548c\u56fe\u50cf","4f71a3f6":"test_data.head()      #\u5305\u542b\u4e86\u7528\u4e8e\u6d4b\u8bd5\u7684\u4eba\u8138\u5173\u952e\u70b9\u56fe\u50cf\uff0c\u6ca1\u6709\u6807\u6ce8\u5173\u952e\u70b9\u5750\u6807","24725c19":"idlookup_data.head()   #\u6d4b\u8bd5\u96c6\u5173\u952e\u70b9\u7684\u4f4d\u7f6e\u7684\u5bf9\u5e94\u540d\u79f0","d99b9e7e":"print(\"Length of train data:\", len(train_data))","5328f0a3":"train_data.isnull().sum()   #\u5224\u65ad\u54ea\u4e9b\u5217\u5305\u542b\u7f3a\u5931\u503c","f06fc111":"#\u4e24\u79cd\u64cd\u4f5c\n\nclean_train_data = train_data.dropna()  #\u5220\u9664\u6240\u6709\u5305\u542bNaN\u7684\u884c\nprint(\"clean_train_data shape:\", np.shape(clean_train_data))\n\nunclean_train_data = train_data.fillna(method = 'ffill')  #\u9ed8\u8ba4\u4e3a\u2019ffill\u2019\uff0c\u5411\u524d\u586b\u5145\nprint(\"unclean_train_data shape:\", np.shape(unclean_train_data))","5b862a91":"%%time\n\ndef load_images(image_data):\n    images = []\n    for idx, sample in image_data.iterrows():\n        image = np.array(sample['Image'].split(' '), dtype=int)\n        image = np.reshape(image, (96,96,1))\n        images.append(image)\n    images = np.array(images)\/255.\n    return images\n\ndef load_keypoints(keypoint_data):\n    keypoint_data = keypoint_data.drop('Image',axis = 1)\n    keypoint_features = []\n    for idx, sample_keypoints in keypoint_data.iterrows():\n        keypoint_features.append(sample_keypoints)\n    keypoint_features = np.array(keypoint_features, dtype = 'float')\n    return keypoint_features\n\nclean_train_images = load_images(clean_train_data)\nprint(\"Shape of clean_train_images:\", np.shape(clean_train_images))\nclean_train_keypoints = load_keypoints(clean_train_data)\nprint(\"Shape of clean_train_keypoints:\", np.shape(clean_train_keypoints))\ntest_images = load_images(test_data)\nprint(\"Shape of test_images:\", np.shape(test_images))\n\ntrain_images = clean_train_images        #\u5bf9clean\u64cd\u4f5c\u540e\u7684images\u548ckeypoints\u8fdb\u884c\u6570\u636e\u589e\u5f3a\ntrain_keypoints = clean_train_keypoints\nfig, axis = plt.subplots()\nplot_sample(clean_train_images[19], clean_train_keypoints[19], axis, \"Sample image & keypoints\")\n\nunclean_train_images = load_images(unclean_train_data)\nprint(\"Shape of unclean_train_images:\", np.shape(unclean_train_images))\nunclean_train_keypoints = load_keypoints(unclean_train_data)\nprint(\"Shape of unclean_train_keypoints:\", np.shape(unclean_train_keypoints))\n\n#\u62fc\u63a5\u6570\u7ec4train_images\u548cunclean_train_images\ntrain_images = np.concatenate((train_images, unclean_train_images))\ntrain_keypoints = np.concatenate((train_keypoints, unclean_train_keypoints))","82d1540b":"def left_right_flip(images, keypoints):\n    flipped_keypoints = []\n    flipped_images = np.flip(images, axis=2)   # Flip column-wise (axis=2)\n    for idx, sample_keypoints in enumerate(keypoints):\n        flipped_keypoints.append([96.-coor if idx%2==0 else coor for idx,coor in enumerate(sample_keypoints)])    # Subtract only X co-ordinates of keypoints from 96 for horizontal flipping\n    return flipped_images, flipped_keypoints\n\nif config.horizontal_flip:\n    flipped_train_images, flipped_train_keypoints = left_right_flip(clean_train_images, clean_train_keypoints)\n    print(\"Shape of flipped_train_images:\", np.shape(flipped_train_images))\n    print(\"Shape of flipped_train_keypoints:\", np.shape(flipped_train_keypoints))\n    train_images = np.concatenate((train_images, flipped_train_images))\n    train_keypoints = np.concatenate((train_keypoints, flipped_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(flipped_train_images[19], flipped_train_keypoints[19], axis, \"Horizontally Flipped\") ","b1f6c08c":"def rotate_augmentation(images, keypoints):\n    rotated_images = []\n    rotated_keypoints = []\n    print(\"Augmenting for angles (in degrees): \")\n    for angle in config.rotation_angles:    # Rotation augmentation for a list of angle values\n        for angle in [angle,-angle]:\n            print(f'{angle}', end='  ')\n            M = cv2.getRotationMatrix2D((48,48), angle, 1.0)\n            angle_rad = -angle*pi\/180.     # Obtain angle in radians from angle in degrees (notice negative sign for change in clockwise vs anti-clockwise directions from conventional rotation to cv2's image rotation)\n            # For train_images\n            for image in images:\n                rotated_image = cv2.warpAffine(image, M, (96,96), flags=cv2.INTER_CUBIC)\n                rotated_images.append(rotated_image)\n            # For train_keypoints\n            for keypoint in keypoints:\n                rotated_keypoint = keypoint - 48.    # Subtract the middle value of the image dimension\n                for idx in range(0,len(rotated_keypoint),2):\n                    # https:\/\/in.mathworks.com\/matlabcentral\/answers\/93554-how-can-i-rotate-a-set-of-points-in-a-plane-by-a-certain-angle-about-an-arbitrary-point\n                    rotated_keypoint[idx] = rotated_keypoint[idx]*cos(angle_rad)-rotated_keypoint[idx+1]*sin(angle_rad)\n                    rotated_keypoint[idx+1] = rotated_keypoint[idx]*sin(angle_rad)+rotated_keypoint[idx+1]*cos(angle_rad)\n                rotated_keypoint += 48.   # Add the earlier subtracted value\n                rotated_keypoints.append(rotated_keypoint)\n            \n    return np.reshape(rotated_images,(-1,96,96,1)), rotated_keypoints\n\nprint()\n\n#\u6b63\u8d1f\u4e24\u79cd\u89d2\u5ea6\nif config.rotation_augmentation:\n    rotated_train_images, rotated_train_keypoints = rotate_augmentation(clean_train_images, clean_train_keypoints)\n    print(\"Shape of rotated_train_images:\", np.shape(rotated_train_images))\n    print(\"Shape of rotated_train_keypoints:\", np.shape(rotated_train_keypoints))\n    train_images = np.concatenate((train_images, rotated_train_images))\n    train_keypoints = np.concatenate((train_keypoints, rotated_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(rotated_train_images[19], rotated_train_keypoints[19], axis, \"Rotation Augmentation\")","ab918279":"def alter_brightness(images, keypoints):\n    altered_brightness_images = []\n    inc_brightness_images = np.clip(images*1.2, 0.0, 1.0)    # Increased brightness by a factor of 1.2 & clip any values outside the range of [-1,1]\n    dec_brightness_images = np.clip(images*0.6, 0.0, 1.0)    # Decreased brightness by a factor of 0.6 & clip any values outside the range of [-1,1]\n    altered_brightness_images.extend(inc_brightness_images)\n    altered_brightness_images.extend(dec_brightness_images)\n    return altered_brightness_images, np.concatenate((keypoints, keypoints))\n\n#\u660e\u6697\u4e24\u79cd\u53d8\u6362\nif config.brightness_augmentation:\n    altered_brightness_train_images, altered_brightness_train_keypoints = alter_brightness(clean_train_images, clean_train_keypoints)\n    print(\"Shape of altered_brightness_train_images:\", np.shape(altered_brightness_train_images))\n    print(\"Shape of altered_brightness_train_keypoints:\", np.shape(altered_brightness_train_keypoints))\n    train_images = np.concatenate((train_images, altered_brightness_train_images))\n    train_keypoints = np.concatenate((train_keypoints, altered_brightness_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(altered_brightness_train_images[19], altered_brightness_train_keypoints[19], axis, \"Increased Brightness\") \n    fig, axis = plt.subplots()\n    plot_sample(altered_brightness_train_images[len(altered_brightness_train_images)\/\/2+19], altered_brightness_train_keypoints[len(altered_brightness_train_images)\/\/2+19], axis, \"Decreased Brightness\") ","c07885ac":"def shift_images(images, keypoints):\n    shifted_images = []\n    shifted_keypoints = []\n    for shift in config.pixel_shifts:    # Augmenting over several pixel shift values\n        for (shift_x,shift_y) in [(-shift,-shift),(-shift,shift),(shift,-shift),(shift,shift)]:\n            M = np.float32([[1,0,shift_x],[0,1,shift_y]])\n            for image, keypoint in zip(images, keypoints):\n                shifted_image = cv2.warpAffine(image, M, (96,96), flags=cv2.INTER_CUBIC)\n                shifted_keypoint = np.array([(point+shift_x) if idx%2==0 else (point+shift_y) for idx, point in enumerate(keypoint)])\n                if np.all(0.0<shifted_keypoint) and np.all(shifted_keypoint<96.0):\n                    shifted_images.append(shifted_image.reshape(96,96,1))\n                    shifted_keypoints.append(shifted_keypoint)\n    shifted_keypoints = np.clip(shifted_keypoints,0.0,96.0)\n    return shifted_images, shifted_keypoints\n\n#\u56db\u4e2a\u65b9\u5411\u4f4d\u79fb\uff08\u79fb\u52a8\u6709\u9608\u503c\uff09\nif config.shift_augmentation:\n    shifted_train_images, shifted_train_keypoints = shift_images(clean_train_images, clean_train_keypoints)\n    print(\"Shape of shifted_train_images:\", np.shape(shifted_train_images))\n    print(\"Shape of shifted_train_keypoints:\", np.shape(shifted_train_keypoints))\n    train_images = np.concatenate((train_images, shifted_train_images))\n    train_keypoints = np.concatenate((train_keypoints, shifted_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(shifted_train_images[19], shifted_train_keypoints[19], axis, \"Shift Augmentation\")","3d8ac3be":"#\u6dfb\u52a0\u968f\u673a\u566a\u58f0\ndef add_noise(images):\n    noisy_images = []\n    for image in images:\n        noisy_image = cv2.add(image, 0.008*np.random.randn(96,96,1))    # Adding random normal noise to the input image & clip the resulting noisy image between [-1,1]\n        noisy_images.append(noisy_image.reshape(96,96,1))\n    return noisy_images\n\nif config.random_noise_augmentation:\n    noisy_train_images = add_noise(clean_train_images)\n    print(\"Shape of noisy_train_images:\", np.shape(noisy_train_images))\n    train_images = np.concatenate((train_images, noisy_train_images))\n    train_keypoints = np.concatenate((train_keypoints, clean_train_keypoints))\n    fig, axis = plt.subplots()\n    plot_sample(noisy_train_images[19], clean_train_keypoints[19], axis, \"Random Noise Augmentation\")","cac704ae":"print(\"Shape of final train_images:\", np.shape(train_images))   #\u589e\u5f3a\u524d\u589e\u5f3a\u540e\u6240\u6709\u6570\u636e\u76f8\u52a0\nprint(\"Shape of final train_keypoints:\", np.shape(train_keypoints))\n\nprint(\"\\nClean Train Data: \")\nfig = plt.figure(figsize=(20,8))\nfor i in range(10):\n    axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n    plot_sample(clean_train_images[i], clean_train_keypoints[i], axis, \"\")\nplt.show()\n\nprint(\"Unclean Train Data: \")\nfig = plt.figure(figsize=(20,8))\nfor i in range(10):\n    axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n    plot_sample(unclean_train_images[i], unclean_train_keypoints[i], axis, \"\")\nplt.show()\n\nif config.horizontal_flip:\n    print(\"Horizontal Flip Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(flipped_train_images[i], flipped_train_keypoints[i], axis, \"\")\n    plt.show()\n\nif config.rotation_augmentation:\n    print(\"Rotation Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(rotated_train_images[i], rotated_train_keypoints[i], axis, \"\")\n    plt.show()\n    \nif config.brightness_augmentation:\n    print(\"Brightness Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(altered_brightness_train_images[i], altered_brightness_train_keypoints[i], axis, \"\")\n    plt.show()\n\nif config.shift_augmentation:\n    print(\"Shift Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(shifted_train_images[i], shifted_train_keypoints[i], axis, \"\")\n    plt.show()\n    \nif config.random_noise_augmentation:\n    print(\"Random Noise Augmentation: \")\n    fig = plt.figure(figsize=(20,8))\n    for i in range(10):\n        axis = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plot_sample(noisy_train_images[i], clean_train_keypoints[i], axis, \"\")\n    plt.show()","73554ba7":"model = Sequential()\n\n#\u5c06\u5904\u7406\u8fc7\u7684\u56fe\u7247\u9001\u5165\u9884\u8bad\u7ec3\u7684\u7279\u5f81\u7f51\u7edc\u4e2d\n#Application\u63d0\u4f9b\u4e86\u5e26\u6709\u9884\u8bad\u7ec3\u6743\u91cd\u7684Keras\u6a21\u578b\npretrained_model = applications.mobilenet_v2.MobileNetV2(input_shape=(96, 96, 3), include_top=False, weights='imagenet')   #\u4e3b\u5e72\u7f51\u7edcMobileNetV2\u505a\u7279\u5f81\u63d0\u53d6\n\npretrained_model.trainable = True    #\u4e0d\u51bb\u7ed3\n\nmodel.add(layers.Convolution2D(3, (1, 1), padding='same', input_shape=(96,96,1)))\nmodel.add(layers.LeakyReLU(alpha = 0.1))\nmodel.add(pretrained_model)\nmodel.add(layers.GlobalAveragePooling2D())    #\u5c06\u6bcf\u4e00\u5f20\u7279\u5f81\u56fe\u8ba1\u7b97\u6240\u6709\u50cf\u7d20\u70b9\u7684\u5747\u503c\uff0c\u8f93\u51fa\u4e00\u4e2a\u6570\u636e\u503c\nmodel.add(layers.Dropout(0.1))\nmodel.add(layers.Dense(30))\nmodel.summary()","179eb0f7":"utils.plot_model(model, show_shapes=True, expand_nested=True)\nvisualkeras.layered_view(model)","9c4f7caf":"#\u9884\u8bad\u7ec3\nearly_stop = callbacks.EarlyStopping(\n    monitor='loss', patience=30, verbose=1, mode='min', baseline=None, restore_best_weights=True\n)              #\u76d1\u63a7\u6307\u6807loss\uff1b\u7528patience\u68c0\u67e5epoch\u6570\u91cf\n\nrlp = callbacks.ReduceLROnPlateau(\n    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-15, mode='min', verbose=1\n)\n\n#\u914d\u7f6e\u8bad\u7ec3\u65b9\u6cd5\nmodel.compile(\n    optimizer='adam', loss='mean_squared_error', metrics=['mae', 'acc']\n)\n\n#\u5c06\u8bad\u7ec3\u8fc7\u7a0b\u8bb0\u5f55\u5728history\u4e2d\n#model.fit\u6267\u884c\u8bad\u7ec3\u8fc7\u7a0b\nhistory = model.fit(\n    train_images, train_keypoints, epochs=int(1.5*config.NUM_EPOCHS), batch_size=config.BATCH_SIZE, \n    validation_split=0.05, callbacks=[early_stop, rlp]\n)","53b76875":"sns.set_style('darkgrid')\n\nfig, ax = plt.subplots(3, 1, figsize=(20, 10))\ndf = pd.DataFrame(history.history)\ndf[['mae', 'val_mae']].plot(ax=ax[0])       #validation\u9a8c\u8bc1\u96c6\ndf[['loss', 'val_loss']].plot(ax=ax[1])\ndf[['acc', 'val_acc']].plot(ax=ax[2])\nax[0].set_title('Model MAE', fontsize=12)     #\u5e73\u65b9\u7edd\u5bf9\u8bef\u5dee\nax[1].set_title('Model Loss', fontsize=12)\nax[2].set_title('Model Acc', fontsize=12)\nfig.suptitle('Model Metrics', fontsize=18);","1c3ec733":"%%time\n#\u5fae\u8c03\nearly_stop = callbacks.EarlyStopping(\n    monitor='loss', patience=30, verbose=1, mode='min', baseline=None, restore_best_weights=True\n)\n\nrlp = callbacks.ReduceLROnPlateau(\n    monitor='loss', factor=0.5, patience=5, min_lr=1e-15, mode='min', verbose=1\n)\n\n\nmodel.compile(\n    optimizer=optimizers.Adam(learning_rate=history.history['lr'][-1]), loss='mean_squared_error', metrics=['mae', 'acc']\n)\n\nhistory = model.fit(\n    train_images, train_keypoints, epochs=2*config.NUM_EPOCHS, batch_size=config.BATCH_SIZE, callbacks=[early_stop, rlp]\n)","872fcbe4":"fig, ax = plt.subplots(3, 1, figsize=(20, 10))\ndf = pd.DataFrame(history.history)\ndf[['mae']].plot(ax=ax[0])\ndf[['loss']].plot(ax=ax[1])\ndf[['acc']].plot(ax=ax[2])\nax[0].set_title('Model MAE', fontsize=12)\nax[1].set_title('Model Loss', fontsize=12)\nax[2].set_title('Model Acc', fontsize=12)\nfig.suptitle('Model Metrics', fontsize=18);","62852dee":"%%time\n \ntest_preds = model.predict(test_images)","64cb2a4a":"fig = plt.figure(figsize=(20,16))\nfor i in range(20):\n    axis = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[])\n    plot_sample(test_images[i], test_preds[i], axis, \"\")","d91e6a01":"feature_names = list(idlookup_data['FeatureName'])\nimage_ids = list(idlookup_data['ImageId']-1)\nrow_ids = list(idlookup_data['RowId'])\n\nfeature_list = []\nfor feature in feature_names:\n    feature_list.append(feature_names.index(feature))\n    \npredictions = []\nfor x,y in zip(image_ids, feature_list):\n    predictions.append(test_preds[x][y])\n    \nrow_ids = pd.Series(row_ids, name = 'RowId')\nlocations = pd.Series(predictions, name = 'Location')\nlocations = locations.clip(0.0,96.0)\nsubmission_result = pd.concat([row_ids,locations],axis = 1)\nsubmission_result.to_csv('submission.csv',index = False)","9d129719":"## Predicting on Test Set","9ce72f06":"# Model","fa7ab881":"**Find columns having Null values and their counts**","f651dc35":"# Augmentation","891e1811":"## Training the model","4a6caee2":"## Exploring Data","f8e935ab":"## Generating Submission File","62b1da39":"## Visualizing Test Predictions","b527994b":"#### We can observe that approx. 68% of data is missing for several keypoints","1313b067":"# Loading Data","f9ad58f7":"## Visualize Train images & corresponding Keypoints","d7cca3d0":"### Fit the model on full dataset"}}