{"cell_type":{"6b629ea9":"code","0e1ac1bc":"code","c0bddc00":"code","badab042":"code","a2ae22e6":"code","cfe20295":"code","f42a9348":"code","96b6f7a3":"code","5b41de88":"code","43c237a9":"code","ef2a1e63":"code","843edf8a":"code","a5c73b7f":"code","c0f5cef6":"code","d4c47ca4":"code","5b19a401":"code","2c805eae":"code","35b04391":"code","3baf0909":"code","31b26d32":"code","cee257cf":"code","a5094016":"markdown","976350e9":"markdown","4259f275":"markdown","439a601e":"markdown","5d2c2119":"markdown","3ae1e363":"markdown","8d00a392":"markdown","3326bd8f":"markdown","0bf30410":"markdown","96052926":"markdown","a7d5c29a":"markdown","bda2f50f":"markdown"},"source":{"6b629ea9":"# Loading relevant libraries\nimport numpy as np \nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math","0e1ac1bc":"# Reading the dataset\ndf = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf.set_index('PassengerId', inplace=True)\nprint(\"Shape : \", df.shape)\nprint(df.dtypes)\ndf.head()","c0bddc00":"df =  df.drop([\"Name\",\"Ticket\"], axis=1) # Remove non-relevant values","badab042":"df.describe()","a2ae22e6":"missing_columns = df.columns[df.isnull().any()] # Which columns have missing values?\ndf.isnull().sum(axis=0).loc[missing_columns] # Age: average imputation, Cabin: drop column due to too much missing values, impute Embarked ","cfe20295":"df = df.drop([\"Cabin\"], axis=1)","f42a9348":"def plot_survival_rate(df, variable, i): # Helper function\n    obs = df.shape[0]\n    plot_df = df.groupby(variable)[\"Survived\"].agg([\"sum\",\"count\"]).rename(columns = { \"sum\":\"survived\", \"count\":\"exposure\"}).reset_index(drop=False)\n    plot_df[\"survived\"] = plot_df[\"survived\"]\/plot_df[\"exposure\"]\n    plot_df[\"exposure\"] = plot_df[\"exposure\"]\/obs\n\n    ax = fig.add_subplot(2, 3, i)\n    sns.lineplot(x=plot_df[variable],y=plot_df[\"survived\"],ax=ax, ci=None) # How does the respons behave?\n    sns.barplot(x=plot_df[variable],y=plot_df[\"exposure\"],ax=ax, ci=None) # How is the explanatory variable distributed?\n","96b6f7a3":"fig = plt.figure(figsize=(15,8))\nplot_survival_rate(df, \"Sex\",1)\nplot_survival_rate(df, \"Pclass\",2)\nplot_survival_rate(df, \"SibSp\",3)\nplot_survival_rate(df, \"Parch\",4)\nplot_survival_rate(df, \"Embarked\",5)\nfig.show()","5b41de88":"df_temp = df.copy()\ndf_temp[\"binAge\"] = np.round(df_temp[\"Age\"])\ndf_temp[\"binFare\"] = np.minimum(np.round(df_temp[\"Fare\"]),50)\n\nfig = plt.figure(figsize=(20,10))\nplot_survival_rate(df_temp, \"binAge\",1)\nplot_survival_rate(df_temp, \"binFare\",2)\nfig.show()","43c237a9":"df.columns\ndf.dtypes","ef2a1e63":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\nX_test = test_data.copy()\nX_test = X_test.drop([\"Name\",\"Ticket\",\"Cabin\",\"PassengerId\"], axis=1) # Remove non-relevant values\nX_test.head()","843edf8a":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\n\n# Separate target from predictors\ny = df.Survived\nX = df.drop(['Survived'], axis=1)\n\n\n# Select numerical and categorical columns\nnumerical_cols = [cname for cname in X if X[cname].dtype in ['int64', 'float64']]\ncategorical_cols = [cname for cname in X if X[cname].dtype == \"object\"]\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='mean')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor_temp = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\npreprocessor = Pipeline(steps=[\n    ('preprocessor_temp', preprocessor_temp),\n    ('scaler', StandardScaler())])\n\npreprocessor.fit(X)","a5c73b7f":"X.iloc[0,:]","c0f5cef6":"dim_trans = preprocessor.transform(X).shape[1]\npreprocessor.transform(X)[0,:]","d4c47ca4":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nparams_knn = {\"kNN__n_neighbors\": np.arange(3,10), \"kNN__weights\":[\"uniform\", \"distance\"]}\n\nkNN = KNeighborsClassifier()\npipe = Pipeline(steps=[('preprocessor', preprocessor), ('kNN', kNN)])\nknnpipe = GridSearchCV(pipe, params_knn, n_jobs=-1)\nknnpipe.fit(X, y)\nprint(\"Best parameter (CV score=%0.3f):\" % knnpipe.best_score_)\nprint(knnpipe.best_params_)\nprint()\nprint(\"Accuracy training set %0.3f\" % accuracy_score(y, knnpipe.predict(X)))","5b19a401":"from sklearn.linear_model import LogisticRegression\n\nparams_logit = {\"logit__penalty\": [\"l1\",\"l2\"], \"logit__C\":np.arange(0.5,1.5,0.1)}\n\nlogit = LogisticRegression()\npipe = Pipeline(steps=[('preprocessor', preprocessor), ('logit', logit)])\nlogitpipe = GridSearchCV(pipe, params_logit, n_jobs=-1)\nlogitpipe.fit(X, y)\nprint(\"Best parameter (CV score=%0.3f):\" % logitpipe.best_score_)\nprint(logitpipe.best_params_)\nprint()\nprint(\"Accuracy training set %0.3f\" % accuracy_score(y, logitpipe.predict(X)))","2c805eae":"from sklearn.svm import SVC\n\nparams_svm = {\"svm__kernel\": [\"linear\",\"poly\",\"rbf\"], \"svm__C\":np.arange(0.5,1.5,0.1)}\n\nsvm = SVC()\npipe = Pipeline(steps=[('preprocessor', preprocessor), ('svm', svm)])\nsvmpipe = GridSearchCV(pipe, params_svm, n_jobs=-1)\nsvmpipe.fit(X, y)\nprint(\"Best parameter (CV score=%0.3f):\" % svmpipe.best_score_)\nprint(svmpipe.best_params_)\nprint()\nprint(\"Accuracy training set %0.3f\" % accuracy_score(y, svmpipe.predict(X)))","35b04391":"from keras.layers import Dense, Dropout\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Dense\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import load_model\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state = 1)\n\npreprocessor.fit(X_train)\nX_train = preprocessor.transform(X_train)\nX_valid = preprocessor.transform(X_valid)\n\nnetwork = Sequential([\n  Dense(16, activation='relu', input_shape=(dim_trans,)),\n  Dense(16, activation='relu'),\n  Dense(1, activation='sigmoid'),\n])\n\nnetwork.compile(loss='binary_crossentropy', # Cross-entropy\n                optimizer='rmsprop', # Root Mean Square Propagation\n                metrics=['accuracy']) # Accuracy performance metric\n\nes = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=10)\nmc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n\n# Train neural network\nhistory = network.fit(X_train, # Features\n                      y_train, # Target vector\n                      epochs=1000, # Number of epochs\n                      verbose=1, # Print description after each epoch\n                     validation_data=(X_valid,y_valid),# Data for evaluation\n                     callbacks=[es, mc]) \n\nsaved_model = load_model('best_model.h5')\nprint(\"Accuracy training set %0.3f\" % accuracy_score(y, saved_model.predict(preprocessor.transform(X))>0.5))","3baf0909":"plt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","31b26d32":"from xgboost import XGBClassifier\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state = 1)\n\npreprocessor.fit(X_train)\nX_train = preprocessor.transform(X_train)\nX_valid = preprocessor.transform(X_valid)\n\nxgb = XGBClassifier(n_estimators=1000, learning_rate=0.05, n_jobs=-1)\nxgb.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=True)\n\nprint(\"Accuracy training set %0.3f\" % accuracy_score(y, xgb.predict(preprocessor.transform(X),ntree_limit=xgb.best_ntree_limit)))","cee257cf":"predictions = xgb.predict(preprocessor.transform(X_test),ntree_limit=xgb.best_ntree_limit)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","a5094016":"## k-NN","976350e9":"## Feed-forward neural network","4259f275":"Quick check:","439a601e":"## Continuous variables","5d2c2119":"# Exploratory data analysis (EDA)\nI start by getting a feeling of the dataset by answering the following quastions:\n* How are the explanatory variables distributed?\n* What is the observed univariate impact of a variable on the survival rate?\n","3ae1e363":"## Categorical\/discrete variables","8d00a392":"# Modelling","3326bd8f":"### Data pre-processing","0bf30410":"## SVM","96052926":"# Data preparation\n\n1. Reading in the data\n2. Handling missing values","a7d5c29a":"## Logistic regression","bda2f50f":"## XGBoost"}}