{"cell_type":{"f61e405f":"code","680aa6b6":"code","50b1add9":"code","3b50ce3f":"code","0ec3107e":"code","cbda8104":"code","a9a1cb7d":"code","17702645":"code","9cde4086":"code","8dabc54a":"code","a83dff80":"code","e9f9b2e9":"code","ca54a3bd":"code","94c467ef":"code","560c8f75":"code","edd014a0":"code","4d411fca":"code","fbafca8d":"code","4c9e2783":"code","f5ccdfdf":"code","e942e66f":"code","5c47dc3f":"code","930488c5":"code","84a69258":"code","ffe7508e":"code","e8d79456":"code","3cbd5204":"code","83a5ec44":"code","c59d7b3b":"markdown","f488d30b":"markdown","aad0ad72":"markdown","6bc86884":"markdown","779829df":"markdown","5715e48f":"markdown","beca5ab2":"markdown","fe24a98e":"markdown","92d3ce15":"markdown","83d3e92b":"markdown","30525934":"markdown","3c6de3be":"markdown","c379efdb":"markdown","44aa3367":"markdown","5aa62035":"markdown","b838d43b":"markdown"},"source":{"f61e405f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","680aa6b6":"import seaborn as sns\nimport matplotlib.pyplot as plt \nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder","50b1add9":"train = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/health-insurance-cross-sell-prediction\/test.csv')","3b50ce3f":"## gender to binary integer\ntrain[\"Gender\"][train[\"Gender\"] == \"Male\"] = 1\ntrain[\"Gender\"][train[\"Gender\"] == \"Female\"] = 0\ntrain[\"Gender\"] = train[\"Gender\"].astype(int)\n## Vehicle Age from cat to integer\ntrain[\"Vehicle_Age\"][train[\"Vehicle_Age\"] == \"< 1 Year\"] = 1\ntrain[\"Vehicle_Age\"][train[\"Vehicle_Age\"] == \"1-2 Year\"] = 2\ntrain[\"Vehicle_Age\"][train[\"Vehicle_Age\"] == \"> 2 Years\"] = 3\ntrain[\"Vehicle_Age\"] = train[\"Vehicle_Age\"].astype(int)\n## Vehicle Damage to binary integer\ntrain[\"Vehicle_Damage\"][train[\"Vehicle_Damage\"] == \"Yes\"] = 1\ntrain[\"Vehicle_Damage\"][train[\"Vehicle_Damage\"] == \"No\"] = 0\ntrain[\"Vehicle_Damage\"] = train[\"Vehicle_Damage\"].astype(int)\n\ntrain['Policy_Sales_Channel'] = train['Policy_Sales_Channel'].apply(lambda x: np.int(x))\ntrain['Region_Code'] = train['Region_Code'].apply(lambda x: np.int(x))\n\ntrain['Drive_exp'] = train['Age'] - train['Age'].min() ## new feature - drive experience + some new features\ntrain['Low_exp'] = train['Drive_exp'].map(lambda s:1 if s<9 else 0)\ntrain['High_exp'] = train['Drive_exp'].map(lambda s:1 if s>20 else 0)\ntrain['Mid_exp'] = train['Drive_exp'].map(lambda s:1 if s<=20 & s>=9 else 0)\ntrain = train.drop('Age', axis=1)\n## some new features based on Annual_Premium, later we'll remove unnecessary\ntrain['Annual_log'] = np.log(train.Annual_Premium + 0.01)\nss = StandardScaler() \ntrain['Annual_scaled'] = ss.fit_transform(train['Annual_Premium'].values.reshape(-1,1))\n\nmm = MinMaxScaler() \ntrain['Annual_minmax'] = mm.fit_transform(train['Annual_Premium'].values.reshape(-1,1))\n## new features based on frequency of 28th region and 152nd channel  in dataset\ntrain['Region_Code_28'] = train['Region_Code'].map(lambda s:1 if s==28 else 0)\ntrain['Policy_Sales_Channel_152'] = train['Policy_Sales_Channel'].map(lambda s:1 if s==152 else 0)\n\n\ntrain['Annual_Premium_10'] = train['Annual_Premium'].map(lambda s:1 if s<=10000 else 0)\n\n# the same for test dataset:\ntest[\"Gender\"][test[\"Gender\"] == \"Male\"] = 1\ntest[\"Gender\"][test[\"Gender\"] == \"Female\"] = 0\ntest[\"Gender\"] = test[\"Gender\"].astype(int)\n\ntest[\"Vehicle_Age\"][test[\"Vehicle_Age\"] == \"< 1 Year\"] = 1\ntest[\"Vehicle_Age\"][test[\"Vehicle_Age\"] == \"1-2 Year\"] = 2\ntest[\"Vehicle_Age\"][test[\"Vehicle_Age\"] == \"> 2 Years\"] = 3\ntest[\"Vehicle_Age\"] = test[\"Vehicle_Age\"].astype(int)\n\ntest[\"Vehicle_Damage\"][test[\"Vehicle_Damage\"] == \"Yes\"] = 1\ntest[\"Vehicle_Damage\"][test[\"Vehicle_Damage\"] == \"No\"] = 0\ntest[\"Vehicle_Damage\"] = test[\"Vehicle_Damage\"].astype(int)\n\ntest['Policy_Sales_Channel'] = test['Policy_Sales_Channel'].apply(lambda x: np.int(x))\ntest['Region_Code'] = test['Region_Code'].apply(lambda x: np.int(x))\n\ntest['Drive_exp'] = test['Age'] - test['Age'].min() ## new feature - drive experience + some new features\ntest['Low_exp'] = test['Drive_exp'].map(lambda s:1 if s<9 else 0)\ntest['High_exp'] = test['Drive_exp'].map(lambda s:1 if s>20 else 0)\ntest['Mid_exp'] = test['Drive_exp'].map(lambda s:1 if s<=20 & s>=9 else 0)\ntest = test.drop('Age', axis=1)\n\ntest['Annual_log'] = np.log(test.Annual_Premium + 0.01)\nss = StandardScaler() \ntest['Annual_scaled'] = ss.fit_transform(test['Annual_Premium'].values.reshape(-1,1))\n\nmm = MinMaxScaler() \ntest['Annual_minmax'] = mm.fit_transform(test['Annual_Premium'].values.reshape(-1,1))\n\ntest['Region_Code_28'] = test['Region_Code'].map(lambda s:1 if s==28 else 0)\ntest['Policy_Sales_Channel_152'] = test['Policy_Sales_Channel'].map(lambda s:1 if s==152 else 0)\ntest['Annual_Premium_10'] = test['Annual_Premium'].map(lambda s:1 if s<=10000 else 0)\n","0ec3107e":"train = train[:20000] ## let's take one part from dataset \ntrain","cbda8104":"def basic_details(df):\n    b = pd.DataFrame()\n    b['Missing value, %'] = round(df.isnull().sum()\/df.shape[0]*100)\n    b['N unique value'] = df.nunique()\n    b['dtype'] = df.dtypes\n    return b\nbasic_details(train)","a9a1cb7d":"train.describe().T","17702645":"def basic_analysis(df1, df2):\n    '''the function compares the average values of  2 dataframes'''\n    b = pd.DataFrame()\n    b['Response df_mean'] = round(df1.mean(),2)\n    b['Not Response df_mean'] = round(df2.mean(),2)\n    c = (b['Response df_mean']\/b['Not Response df_mean'])\n    if [c<=1]:\n        b['Variation, %'] = round((1-((b['Response df_mean']\/b['Not Response df_mean'])))*100)\n    else:\n        b['Variation, %'] = round(((b['Response df_mean']\/b['Not Response df_mean'])-1)*100)\n        \n    b['Influence'] = np.where(abs(b['Variation, %']) <= 9, \"feature's effect on the target is not defined\", \n                              \"feature value affects the target\")\n\n    return b","9cde4086":"response = train.drop(train[train['Response'] != 1].index)\nnot_response = train.drop(train[train['Response'] != 0].index)\nbasic_analysis(response,not_response)","8dabc54a":"sns.countplot(train.Response)\ntrain['Response'].value_counts(normalize=True)","a83dff80":"## distribution of cat features\n   \ncat_features = train[[ 'Vehicle_Damage', 'Previously_Insured', 'Gender','Vehicle_Damage', 'Vehicle_Age', 'Driving_License']].columns\nfor i in cat_features:\n    sns.barplot(x=\"Response\",y=i,data=train)\n    plt.title(i+\" by \"+\"Response\")\n    plt.show()","e9f9b2e9":"## distribution and checking for outliers in numeric features\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfeatures = train[['Drive_exp', 'Policy_Sales_Channel', 'Vintage', 'Region_Code']].columns\n\nfor i in features:\n    sns.boxplot(x=\"Response\",y=i,data=train)\n    plt.title(i+\" by \"+\"Response\")\n    plt.show()","ca54a3bd":"## PPS matrix of correlation of non-linear relations between features\n%pip install ppscore # installing ppscore, library used to check non-linear relationships between our variables\nimport ppscore as pps # importing ppscore\nmatrix_pps = pps.matrix(train.drop('id', axis=1))[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\nmatrix_pps = matrix_pps.apply(lambda x: round(x, 2))\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(matrix_pps, vmin=0, vmax=1, cmap=\"icefire\", linewidths=0.75, annot=True)","94c467ef":"def spearman(frame, features):\n    spr = pd.DataFrame()\n    spr['feature'] = features\n    spr['spearman'] = [frame[f].corr(frame['Response'], 'spearman') for f in features]\n    spr = spr.sort_values('spearman')\n    plt.figure(figsize=(6, 0.25*len(features)))\n    f, ax = plt.subplots(figsize=(12, 9))\n    sns.barplot(data=spr, y='feature', x='spearman', orient='h')\n    \nfeatures = train.drop(['Response', 'id'], axis = 1).columns\nspearman(train, features)","560c8f75":"corrMatrix = train.corr()\nf, ax = plt.subplots(figsize=(20, 10))\nsns.heatmap(corrMatrix, annot = True)\nplt.show()","edd014a0":"# Let's explore the Annual Premium by Response and see the distribuition of Amount transactions\nfig , axs = plt.subplots(nrows = 1 , ncols = 4 , figsize = (16,4))\n\nsns.boxplot(x =\"Response\",y=\"Annual_Premium\",data=train, ax = axs[0])\naxs[0].set_title(\"Response vs Annual_Premium\")\n\nsns.boxplot(x =\"Response\",y=\"Annual_log\",data=train, ax = axs[1])\naxs[1].set_title(\"Response vs Log Annual_Premium\")\n\nsns.boxplot(x =\"Response\",y=\"Annual_scaled\",data=train, ax = axs[2])\naxs[2].set_title(\"Response vs Scaled Annual_Premium\")\n\nsns.boxplot(x =\"Response\",y=\"Annual_minmax\",data=train, ax = axs[3])\naxs[3].set_title(\"Response vs Min-Max Annual_Premium\")\n\nplt.show()","4d411fca":"## let's drop some features from dataset\ntrain = train.drop(['Annual_Premium', 'Annual_minmax', 'Annual_log'], axis=1)\ntest = test.drop(['Annual_Premium', 'Annual_minmax', 'Annual_log'], axis=1)\ntrain = train.drop('Driving_License', axis=1)\ntest = test.drop('Driving_License', axis=1)","fbafca8d":"def reduce_memory_usage(df):\n    \"\"\" The function will reduce memory of dataframe\n    Note: Apply this function after removing missing value\"\"\"\n    intial_memory = df.memory_usage().sum()\/1024**2\n    print('Intial memory usage:',intial_memory,'MB')\n    for col in df.columns:\n        mn = df[col].min()\n        mx = df[col].max()\n        if df[col].dtype != object:            \n            if df[col].dtype == int:\n                if mn >=0:\n                    if mx < np.iinfo(np.uint8).max:\n                        df[col] = df[col].astype(np.uint8)\n                    elif mx < np.iinfo(np.uint16).max:\n                        df[col] = df[col].astype(np.uint16)\n                    elif mx < np.iinfo(np.uint32).max:\n                        df[col] = df[col].astype(np.uint32)\n                    elif mx < np.iinfo(np.uint64).max:\n                        df[col] = df[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        df[col] = df[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        df[col] = df[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        df[col] = df[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        df[col] = df[col].astype(np.int64)\n            if df[col].dtype == float:\n                df[col] =df[col].astype(np.float32)\n    \n    red_memory = df.memory_usage().sum()\/1024**2\n    print('Memory usage after complition: ',red_memory,'MB')\n    \nreduce_memory_usage(train)","4c9e2783":"X = train.drop(['Response', 'id'], axis=1)\ny = train.Response\ntest_id = test.id.values\n\nsm = SMOTE(random_state=42)\nX_sm, y_sm = sm.fit_sample(X, y)\n\nrus = RandomUnderSampler(random_state=42)\nX_rus, y_rus = rus.fit_resample(X, y)\n\nros = RandomOverSampler(random_state=42)\nX_ros, y_ros= ros.fit_resample(X, y)\n\nadasyn = ADASYN(random_state=42)\nX_ad, y_ad = adasyn.fit_resample(X, y)\n\nx_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=10)\nx_train, y_train= rus.fit_resample(x_train, y_train)\n\n\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\n\n\nparams = {\n        'objective':'binary:hinge',\n        'n_estimators': 800,\n        'max_depth':2,\n        'learning_rate':0.01,\n        'eval_metric':'auc',\n        'min_child_weight':4,\n        'subsample':0.1,\n        'colsample_bytree':0.6,\n        'seed':29,\n        'reg_lambda':2.5,\n        'reg_alpha':7,\n        'gamma':0.01,\n        'scale_pos_weight':0,\n        'nthread':-1\n}\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\nnrounds=5000\nmodel = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=800, \n                           maximize=True, verbose_eval=10)","f5ccdfdf":"y_pred = model.predict(d_valid)\nprint('Accuracy :{0:0.5f}'.format(metrics.accuracy_score(y_valid, y_pred))) \nprint('AUC : {0:0.5f}'.format(metrics.roc_auc_score(y_valid, y_pred)))\nprint('Precision : {0:0.5f}'.format(metrics.precision_score(y_valid, y_pred)))\nprint('Recall : {0:0.5f}'.format(metrics.recall_score(y_valid, y_pred)))\nprint('F1 : {0:0.5f}'.format(metrics.f1_score(y_valid, y_pred)))","e942e66f":"confusion_matrix(y_valid, y_pred)","5c47dc3f":"xgb.plot_importance(model)","930488c5":"\nfrom sklearn.metrics import roc_curve, auc\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_valid, y_pred)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(false_positive_rate, true_positive_rate, 'b', label='AUC = %0.2f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.2])\nplt.ylim([-0.1,1.2])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')","84a69258":"var = train.drop(['id', 'Response'], axis=1).columns.values\n\ni = 0\nt0 = train.loc[train['Response'] == 0]\nt1 = train.loc[train['Response'] == 1]\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(3,5,figsize=(22,28))\n\nfor feature in var:\n    i += 1\n    plt.subplot(3,5,i)\n    sns.kdeplot(t0[feature], bw=0.5,label=\"Class = 0\")\n    sns.kdeplot(t1[feature], bw=0.5,label=\"Class = 1\")\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\nplt.show();","ffe7508e":"dmatrix_data = xgb.DMatrix(data=X_ros, label=y_ros)\n\ncv_params = {\n        'objective':'binary:hinge',\n        'max_depth':13,\n        'learning_rate':0.1,\n        'eval_metric':'auc',\n        'min_child_weight':1,\n        'subsample':1,\n        'colsample_bytree':0.6,\n        'seed':29,\n        'reg_lambda':2.79,\n        'reg_alpha':7,\n        'gamma':0.01,\n        'scale_pos_weight':0,\n        'nthread':-1\n}\ncross_val = xgb.cv(\n    params=cv_params,\n    dtrain=dmatrix_data, \n    nfold=5,\n    num_boost_round=5000, \n    early_stopping_rounds=1000, \n    metrics='auc', \n    as_pandas=True, \n    seed=29)\nprint(cross_val.tail(1))\n","e8d79456":"# from sklearn.model_selection import GridSearchCV\n\n# clf = xgb.XGBClassifier()\n# parameters = {\n#      \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n#      \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n#      \"min_child_weight\" : [ 1, 3, 5, 7 ],\n#      \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n#      \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n#      }\n\n# grid = GridSearchCV(clf,\n#                     parameters, n_jobs=4,\n#                     scoring=\"neg_log_loss\",\n#                     cv=3)\n\n# grid.fit(x_train, y_train)","3cbd5204":"# ## model for prediction\n# d_train = xgb.DMatrix(x_train, label=y_train)\n# d_valid = xgb.DMatrix(x_valid, label=y_valid)\n# X_test = test.drop('id', axis=1)\n# d_test = xgb.DMatrix(X_test)\n# params = {\n#         'objective':'binary:logistic',\n#         'n_estimators': 500,\n#         'max_depth':12,\n#         'learning_rate':0.1,\n#         'eval_metric':'auc',\n#         'min_child_weight':1,\n#         'subsample':1,\n#         'colsample_bytree':0.6,\n#         'seed':29,\n#         'reg_lambda':2.79,\n#         'reg_alpha':7,\n#         'gamma':0.01,\n#         'scale_pos_weight':1,\n#         'nthread':-1\n# }\n\n# watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n# nrounds=10000\n# model_1 = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=800, \n#                            maximize=True, verbose_eval=10)","83a5ec44":"# sub = pd.DataFrame()\n# sub['ID'] = test['id']\n# sub['Response'] = model_1.predict(d_test)\n# sub.to_csv('submission.csv', index=False)\n\n# sub.head()","c59d7b3b":"modelling...","f488d30b":"loading datasets.....","aad0ad72":"distribution and checking for outliers in numeric features:","6bc86884":"distribution of cat features:","779829df":"let's split dataset into two datasets depending on Response and compare them....","5715e48f":"PPS matrix of correlation of non-linear relations between features:","beca5ab2":"Spearman correlation matrix","fe24a98e":"features overview....","92d3ce15":"Pls upvote if you liked this kernel","83d3e92b":"Model results:","30525934":"Let's explore the Annual Premium by Response and see the distribuition of Amount transactions","3c6de3be":"...and reduce memory usage...","c379efdb":"some plots for some important features...","44aa3367":"some preliminary feature engeneering","5aa62035":"Corr matrix:","b838d43b":"...and let's check for imbalanced target feature...."}}