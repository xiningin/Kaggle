{"cell_type":{"2389aea4":"code","2cc89db0":"code","2431aa2c":"code","8b7c9373":"code","b5003b8e":"code","5588dfbe":"code","980562a0":"code","a54f94f9":"code","4f695ed5":"code","dcf51709":"code","51e1cbaa":"code","5037337d":"code","d7745e46":"code","c02a5b32":"code","11fdde81":"code","90c54759":"code","6e660fd6":"code","4daf2253":"code","ce15fccb":"code","e853ffbf":"code","4505201d":"code","3ea7bf8a":"code","0c43324b":"code","9ef1d749":"code","86c8abaf":"code","df2e2697":"code","890857a6":"code","12ea3393":"code","e00197d2":"code","1159cb69":"code","7b033a98":"code","a26cb05f":"code","ab825b36":"code","8d65b356":"code","3834f05d":"code","9360bebf":"code","7392b650":"code","0388bc0c":"code","069eadc4":"code","54e342ce":"code","4f48138b":"code","a286bf07":"markdown","1b60eda9":"markdown","67e36ecc":"markdown"},"source":{"2389aea4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport re\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom scipy.sparse import coo_matrix\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom tqdm import tqdm\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2cc89db0":"import pandas as pd\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")","2431aa2c":"train","8b7c9373":"test","b5003b8e":"x=train.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","5588dfbe":"train['word_count'] = train['text'].apply(lambda x: len(str(x).split(\" \")))\ntrain[['text','word_count']].head()","980562a0":"train.word_count.describe()","a54f94f9":"#Most common words\nfreq = pd.Series(' '.join(train['text']).split()).value_counts()[:20]\nfreq","4f695ed5":"#Identify uncommon words\nfreq1 =  pd.Series(' '.join(train \n         ['text']).split()).value_counts()[-20:]\nfreq1","dcf51709":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\n# Applying the cleaning function to both test and training datasets\ntrain['text'] = train['text'].apply(lambda x: clean_text(x))\ntest['text'] = test['text'].apply(lambda x: clean_text(x))","51e1cbaa":"text = \"Are you coming , aren't you\"\ntokenizer1 = nltk.tokenize.WhitespaceTokenizer()\ntokenizer2 = nltk.tokenize.TreebankWordTokenizer()\ntokenizer3 = nltk.tokenize.WordPunctTokenizer()\ntokenizer4 = nltk.tokenize.RegexpTokenizer(r'\\w+')\n\nprint(\"Example Text: \",text)\nprint(\"------------------------------------------------------------------------------------------------\")\nprint(\"Tokenization by whitespace:- \",tokenizer1.tokenize(text))\nprint(\"Tokenization by words using Treebank Word Tokenizer:- \",tokenizer2.tokenize(text))\nprint(\"Tokenization by punctuation:- \",tokenizer3.tokenize(text))\nprint(\"Tokenization by regular expression:- \",tokenizer4.tokenize(text))","5037337d":"# Tokenizing the training and the test set\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain['text'] = train['text'].apply(lambda x: tokenizer.tokenize(x))\ntest['text'] = test['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain['text'].head()","d7745e46":"def remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words\ntrain['text'] = train['text'].apply(lambda x : remove_stopwords(x))\ntest['text'] = test['text'].apply(lambda x : remove_stopwords(x))\ntrain.head()","c02a5b32":"def combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain['text'] = train['text'].apply(lambda x : combine_text(x))\ntest['text'] = test['text'].apply(lambda x : combine_text(x))\ntrain['text']\ntrain.head()","11fdde81":"def text_preprocessing(text):\n    \"\"\"\n    Cleaning and parsing the text.\n\n    \"\"\"\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    \n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n    combined_text = ' '.join(remove_stopwords)\n    return combined_text","90c54759":"##Creating a list of stop words and adding custom stopwords\nstop_words = set(stopwords.words(\"english\"))\n##Creating a list of custom stopwords\nnew_words = [\"using\", \"show\", \"result\", \"large\", \"also\", \"iv\", \"one\", \"two\", \"new\", \"previously\", \"shown\"]\nstop_words = stop_words.union(new_words)","6e660fd6":"corpus = []\nfor i in range(0, 3847):\n    #Remove punctuations\n    text = re.sub('[^a-zA-Z]', ' ', train['text'][i])\n    \n    #Convert to lowercase\n    text = text.lower()\n    \n    #remove tags\n    text=re.sub(\"&lt;\/?.*?&gt;\",\" &lt;&gt; \",text)\n    \n    # remove special characters and digits\n    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n    \n    ##Convert to list from string\n    text = text.split()\n    \n    ##Stemming\n    ps=PorterStemmer()\n    #Lemmatisation\n    lem = WordNetLemmatizer()\n    text = [lem.lemmatize(word) for word in text if not word in  \n            stop_words] \n    text = \" \".join(text)\n    corpus.append(text)","4daf2253":"#Word cloud\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n%matplotlib inline\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stop_words,\n                          max_words=100,\n                          max_font_size=50, \n                          random_state=42\n                         ).generate(str(corpus))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()\nfig.savefig(\"word1.png\", dpi=900)","ce15fccb":"#Most frequently occuring words\ndef get_top_n_words(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n                   vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                       reverse=True)\n    return words_freq[:n]\n#Convert most freq words to dataframe for plotting bar plot\ntop_words = get_top_n_words(corpus, n=20)\ntop_df = pd.DataFrame(top_words)\ntop_df.columns=[\"Word\", \"Freq\"]\n#Barplot of most freq words\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\ng = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\ng.set_xticklabels(g.get_xticklabels(), rotation=30)","e853ffbf":"#Most frequently occuring Bi-grams\ndef get_top_n2_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(2,2),  \n            max_features=2000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop2_words = get_top_n2_words(corpus, n=20)\ntop2_df = pd.DataFrame(top2_words)\ntop2_df.columns=[\"Bi-gram\", \"Freq\"]\nprint(top2_df)\n#Barplot of most freq Bi-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nh=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\nh.set_xticklabels(h.get_xticklabels(), rotation=45)","4505201d":"#Most frequently occuring Tri-grams\ndef get_top_n3_words(corpus, n=None):\n    vec1 = CountVectorizer(ngram_range=(3,3), \n           max_features=2000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]\ntop3_words = get_top_n3_words(corpus, n=20)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"Tri-gram\", \"Freq\"]\nprint(top3_df)\n#Barplot of most freq Tri-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nj=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\nj.set_xticklabels(j.get_xticklabels(), rotation=45)","3ea7bf8a":"example=\"We always try to bring the heavy. #metal #RT http:\/\/t.co\/YAo1e0xngw\"","0c43324b":"#remove URL\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","9ef1d749":"train['text']=train['text'].apply(lambda x : remove_URL(x))\ntest['text']=test['text'].apply(lambda x : remove_URL(x))","86c8abaf":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","df2e2697":"train['text']=train['text'].apply(lambda x: remove_emoji(x))\ntest['text']=test['text'].apply(lambda x: remove_emoji(x))","890857a6":"import string\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas\"\nprint(remove_punct(example))","12ea3393":"train['text']=train['text'].apply(lambda x : remove_punct(x))\ntest['text']=test['text'].apply(lambda x : remove_punct(x))","e00197d2":"count_vectorizer = CountVectorizer()\ntrain_vectors = count_vectorizer.fit_transform(train['text'])\ntest_vectors = count_vectorizer.transform(test[\"text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_vectors[0].todense())","1159cb69":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(train['text'])\ntest_tfidf = tfidf.transform(test[\"text\"])","7b033a98":"clf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","a26cb05f":"clf.fit(train_vectors, train[\"target\"])","ab825b36":"# Fitting a simple Logistic Regression on TFIDF\nclf_tfidf = LogisticRegression(C=1.0)\nscores = model_selection.cross_val_score(clf_tfidf, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","8d65b356":"# Fitting a simple Naive Bayes on Counts\nclf_NB = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","3834f05d":"clf_NB.fit(train_vectors, train[\"target\"])","9360bebf":"# Fitting a simple Naive Bayes on TFIDF\nclf_NB_TFIDF = MultinomialNB()\nscores = model_selection.cross_val_score(clf_NB_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","7392b650":"clf_NB_TFIDF.fit(train_tfidf, train[\"target\"])","0388bc0c":"import xgboost as xgb\nclf_xgb = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb, train_vectors, train[\"target\"], cv=5, scoring=\"f1\")\nscores","069eadc4":"import xgboost as xgb\nclf_xgb_TFIDF = xgb.XGBClassifier(max_depth=7, n_estimators=200, colsample_bytree=0.8, \n                        subsample=0.8, nthread=10, learning_rate=0.1)\nscores = model_selection.cross_val_score(clf_xgb_TFIDF, train_tfidf, train[\"target\"], cv=5, scoring=\"f1\")\nscores","54e342ce":"def submission(submission_file_path,model,test_vectors):\n    sample_submission = pd.read_csv(submission_file_path)\n    sample_submission[\"target\"] = model.predict(test_vectors)\n    sample_submission.to_csv(\"submission.csv\", index=False)","4f48138b":"submission_file_path = \"..\/input\/nlp-getting-started\/sample_submission.csv\"\ntest_vectors=test_tfidf\nsubmission(submission_file_path,clf_NB_TFIDF,test_vectors)","a286bf07":"Lets check the traget variable's distribution","1b60eda9":".. to be continued. Need to optimise this model","67e36ecc":"Lets fetch the word count for each value in the text column"}}