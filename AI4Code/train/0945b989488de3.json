{"cell_type":{"62733f1b":"code","f883b508":"code","805c0685":"code","102b674e":"code","e31c4947":"code","a747ecfa":"code","7d940cbe":"code","78675e8b":"code","a5d48338":"code","b883362e":"code","77c17b84":"code","eb5cddf1":"code","2e8f6ec6":"code","7c86961e":"code","5b8984e2":"code","5f9e2d1c":"code","831c91dc":"code","ed5dded1":"code","cf647197":"code","ef1f3136":"code","4ef364dd":"code","a297122b":"code","7a5cf3a0":"code","078d4984":"code","3b13cab9":"code","00f18ba9":"code","27ba53a4":"code","4233c210":"code","cb3d5d44":"code","1173d294":"code","c9a3e558":"code","328d6d4b":"code","91c1b69f":"code","3ee7c306":"code","324dec7e":"code","3ec16ca8":"code","bb7f7748":"code","86d63841":"code","017cd703":"code","a463e7d6":"code","d612f1ce":"code","ff134f73":"code","7c680d8b":"code","0ed4fe99":"code","2ae1f099":"code","f488e8e7":"code","ace4ed8d":"code","4e22eb94":"code","1b3e17f1":"code","4b896a42":"code","a692dd68":"code","348baa54":"code","4cc92acb":"code","01b28290":"code","dd9d526f":"code","211ea81f":"code","db252d7a":"code","ea154cd6":"code","349b6de9":"code","b0e3d8a0":"code","d559eaee":"code","5b812896":"code","cb66cef4":"code","12ecb068":"code","55bb4746":"code","88183420":"code","d1adfda5":"code","461b37fd":"code","8403e178":"code","39b29be9":"code","0e7ce166":"code","209b99e6":"code","2a314c41":"code","20154a4a":"code","6182da3c":"code","049f8341":"code","0164200c":"code","8aa4aabd":"code","10d2f0d3":"code","8f19f27d":"code","42e12c04":"code","ceeb7846":"code","1b1aaa8a":"code","6930ab75":"code","1b654413":"code","fe02cc2a":"code","96084eba":"markdown","cdb6fbe7":"markdown","0f061504":"markdown","2907ddc9":"markdown","5a6b1b7b":"markdown","218c0294":"markdown","adb1761f":"markdown","f2666606":"markdown","d07462d6":"markdown","0e93a512":"markdown","1f99626f":"markdown","e63e3036":"markdown","fbf4035c":"markdown","49888221":"markdown","7159375e":"markdown","37d52479":"markdown","13545999":"markdown","7474d825":"markdown","3f496855":"markdown","29e97de0":"markdown","be7ba2db":"markdown","bad804dd":"markdown","d994d7d1":"markdown","c59b2b57":"markdown"},"source":{"62733f1b":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns","f883b508":"test_df       = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', keep_default_na = True) \ntrain_df      = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', keep_default_na = True) \nsample_sub_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","805c0685":"train_df.head()","102b674e":"train_df.shape","e31c4947":"for df in [train_df, test_df]:\n    df.set_index(\"Id\", inplace=True)\ntrain_df.head()","a747ecfa":"train_df.info()","7d940cbe":"ntrain = train_df.shape[0]\nntest = test_df.shape[0]\ny_train = train_df['SalePrice'].values\nall_data = pd.concat((train_df, test_df), axis=0)\nall_data.drop(['SalePrice'], axis=1, inplace=True)","78675e8b":"numerical_features = all_data.select_dtypes([int, float]).columns","a5d48338":"numerical_features","b883362e":"all_data[numerical_features].isna().any()","77c17b84":"all_data[['LotFrontage','MasVnrArea','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF',\n          'TotalBsmtSF','BsmtFullBath','BsmtHalfBath','GarageYrBlt','GarageCars','GarageArea']].isna().sum()","eb5cddf1":"all_data[['LotFrontage','MasVnrArea','GarageYrBlt']].describe()","2e8f6ec6":"all_data[['LotFrontage','MasVnrArea','GarageYrBlt']].hist(bins=20,figsize=(20, 6))","7c86961e":"def impute_numeric(train_df):\n  train_df['LotFrontage'].fillna(train_df['LotFrontage'].mean(), inplace=True)\n  train_df['MasVnrArea'].fillna(train_df['MasVnrArea'].mean(), inplace=True)\n  train_df['GarageYrBlt'].fillna(train_df['YearBuilt'], inplace=True)\n  for col in ('GarageArea', 'GarageCars'):\n    train_df[col].fillna(0,inplace=True)\n  for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    train_df[col].fillna(0,inplace=True)\n  return train_df","5b8984e2":"all_data = impute_numeric(all_data)","5f9e2d1c":"all_data[numerical_features].isna().sum()","831c91dc":"categorical_features = all_data.select_dtypes([object]).columns\ncategorical_features","ed5dded1":"all_data[categorical_features].isna().sum()","cf647197":"train_df[['MSZoning','Alley','Utilities','Exterior1st','Exterior2nd','MasVnrType','BsmtQual','BsmtCond',\n          'BsmtExposure','BsmtFinType1','BsmtFinType2','Electrical','KitchenQual','Functional','FireplaceQu',\n          'GarageType','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature','SaleType']].dtypes","ef1f3136":"train_df[['MSZoning','Alley','Utilities','Exterior1st','Exterior2nd','MasVnrType','BsmtQual','BsmtCond',\n          'BsmtExposure','BsmtFinType1','BsmtFinType2','Electrical','KitchenQual','Functional','FireplaceQu',\n          'GarageType','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature','SaleType']].isna().sum()","4ef364dd":"test_df[['MSZoning','Alley','Utilities','Exterior1st','Exterior2nd','MasVnrType','BsmtQual','BsmtCond',\n          'BsmtExposure','BsmtFinType1','BsmtFinType2','Electrical','KitchenQual','Functional','FireplaceQu',\n          'GarageType','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature','SaleType']].isna().sum()","a297122b":"def categorical_impute(all_data):\n  all_data[\"PoolQC\"].fillna(\"None\", inplace=True)\n  all_data[\"MiscFeature\"].fillna(\"None\", inplace=True)\n  all_data[\"Alley\"].fillna(\"None\", inplace=True)\n  all_data[\"Fence\"].fillna(\"None\", inplace=True)\n  all_data[\"FireplaceQu\"].fillna(\"None\", inplace=True)\n  for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n      all_data[col].fillna('None', inplace=True)\n  for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n      all_data[col].fillna('None', inplace=True)\n  all_data[\"MasVnrType\"].fillna(\"None\", inplace=True)\n  all_data[\"Electrical\"].fillna(\"SBrkr\", inplace=True)\n  return all_data","7a5cf3a0":"all_data=categorical_impute(all_data)","078d4984":"all_data[categorical_features].isna().sum()","3b13cab9":"for categories in ['MSZoning','Utilities','Exterior1st','Exterior2nd','KitchenQual','Functional','SaleType']:\n    print(categories)\n    print(pd.concat([train_df, test_df])[categories].sort_values().unique())\n    print('\\n')","00f18ba9":"def categorical_impute_rest(all_data):\n  all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0], inplace=True)\n  all_data['Utilities'].fillna(all_data['Utilities'].mode()[0], inplace=True)\n  all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0], inplace=True)\n  all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0], inplace=True)\n  all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0], inplace=True)\n  all_data['Functional'].fillna(all_data['Functional'].mode()[0], inplace=True)\n  all_data['SaleType'].fillna(all_data['SaleType'].mode()[0], inplace=True)\n  return all_data","27ba53a4":"all_data=categorical_impute_rest(all_data)","4233c210":"all_data[categorical_features].isna().sum()","cb3d5d44":"def features_type(all_data):\n  categorical_features = all_data.select_dtypes([object]).columns\n  numerical_features = all_data.select_dtypes([int, float]).columns\n  print('Number of categorical nominal features: ',len(categorical_features))\n  print('Number of numerical features: ',len(numerical_features))\n  print('Number of total features: ',len(categorical_features)+len(numerical_features))","1173d294":"features_type(all_data)","c9a3e558":"all_data.info()","328d6d4b":"all_data[categorical_features].isna().any()","91c1b69f":"first_class=[]\nfor l in categorical_features:\n  first_class.append(all_data[l].value_counts(normalize=True)[0])","3ee7c306":"second_class=[]\nfor l in categorical_features:\n  second_class.append(all_data[l].value_counts(normalize=True)[0]+all_data[l].value_counts(normalize=True)[1])","324dec7e":"pd.concat([pd.DataFrame(categorical_features,columns=['Feature']),\n           pd.DataFrame(first_class, columns=['Top 1']),\n           pd.DataFrame(second_class, columns=['Top 1 & 2'])],axis=1)","3ec16ca8":"all_data.drop(['Street', 'Utilities', 'Condition2', 'RoofMatl', 'Heating', 'PoolQC'], axis=1, inplace=True)","bb7f7748":"all_data[numerical_features].isna().any()","86d63841":"features_type(all_data)","017cd703":"sns.countplot(all_data.MSSubClass)","a463e7d6":"sns.countplot(all_data.MoSold)","d612f1ce":"for feat in [['MSSubClass','MoSold']]:\n    all_data[feat] = all_data[feat].astype(str)","ff134f73":"features_type(all_data)","7c680d8b":"all_data['TotalBath'] = all_data['BsmtFullBath'] + all_data['BsmtHalfBath'] * 0.5 + all_data['FullBath'] + all_data['HalfBath'] * 0.5\nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\nall_data['TotalBsmtSF'] = all_data['TotalBsmtSF'] + all_data['BsmtFinSF1'] + all_data['BsmtFinSF2']\nall_data['Age'] = all_data['YrSold'] - all_data['YearRemodAdd']\nall_data['Total_porch_sf'] = (all_data['OpenPorchSF'] + all_data['3SsnPorch'] + all_data['EnclosedPorch'] + all_data['ScreenPorch'] + all_data['WoodDeckSF'])","0ed4fe99":"features_type(all_data)","2ae1f099":"categorical_features = all_data.select_dtypes([object]).columns\nnumerical_features = all_data.select_dtypes([int, float]).columns","f488e8e7":"fig = plt.figure(figsize=(25,40)) #figure size\na = 13  # number of rows\nb = 3  # number of columns\nc = 1  # initialize plot counter\n\nfor feat in numerical_features:\n    plt.subplot(a, b, c)\n    sns.kdeplot(x=all_data[feat])\n    c+=1\n    \nplt.tight_layout()\nplt.show()","ace4ed8d":"skewed_feat=[]\nfor k in numerical_features:\n  if(np.abs(all_data[k].skew())>1.0):\n    print('Skew of feature: ',k,' is: ',all_data[k].skew())\n    skewed_feat.append(k)\n  else:\n    pass\n\nprint('Number of skewed features: ',len(skewed_feat))","4e22eb94":"for col in skewed_feat:\n  all_data[col] = np.log1p(all_data[col])","1b3e17f1":"features_type(all_data)","4b896a42":"dummy_cols=0\nfor c in categorical_features:\n  dummy_cols=dummy_cols+(len(all_data[c].unique())-1)\n\ndummy_cols","a692dd68":"all_data_dummy= pd.get_dummies(all_data[categorical_features], drop_first=True)\nall_data=pd.concat([all_data,all_data_dummy],axis=1) # joining converted dummy feature and original df_all dataset\nall_data= all_data.drop(all_data[categorical_features],axis=1) #removing original categorical columns\nall_data.shape","348baa54":"all_data.describe().T","4cc92acb":"all_data","01b28290":"train_df['SalePrice'].hist(bins=50)","dd9d526f":"train_df['SalePrice'].skew()","211ea81f":"from scipy.stats.mstats import normaltest","db252d7a":"normaltest(train_df['SalePrice'].values)","ea154cd6":"label=np.log(train_df['SalePrice'])","349b6de9":"plt.hist(label)","b0e3d8a0":"pd.DataFrame(label).skew()","d559eaee":"#ntrain = train_df.shape[0]\nntrain","5b812896":"#ntest = test_df.shape[0]\nntest","cb66cef4":"training=all_data[:ntrain]\ntesting=all_data[ntrain:]","12ecb068":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom scipy.special import inv_boxcox\n\ndef error_metrics(y_pred,y_val):\n  print('MSE: ',mean_squared_error(y_pred,y_val))\n  print('RMSE: ',np.sqrt(mean_squared_error(y_pred,y_val)))\n  print('Coefficient of determination: ',r2_score(y_pred,y_val))","55bb4746":"from sklearn.model_selection import train_test_split","88183420":"X_train, X_val, label_train, label_val = train_test_split(training, label, test_size=0.25, random_state=42)","d1adfda5":"X_train.shape, label_train.shape, X_val.shape, label_val.shape","461b37fd":"from sklearn.preprocessing import StandardScaler\ns = StandardScaler()\n\nX_train_s = s.fit_transform(X_train)\nX_val_s = s.transform(X_val)","8403e178":"from sklearn.linear_model import RidgeCV\n\nalphas = [1e-3, 5e-3, 0.02, 0.05]\n\nridgeCV = RidgeCV(alphas=alphas, cv=4).fit(X_train_s, label_train)\n\nridgeCV_pre = ridgeCV.predict(X_val_s)\n\nprint('Alpha found: ',ridgeCV.alpha_)","39b29be9":"error_metrics(ridgeCV_pre,label_val)","0e7ce166":"from sklearn.linear_model import LassoCV\n\nalphas2 = np.array([5e-8, 1e-7, 1e-6])\n\nlassoCV = LassoCV(alphas=alphas2, max_iter=5e4, cv=3).fit(X_train_s, label_train)\n\nlassoCV_pre = lassoCV.predict(X_val_s)\n\nprint('Alpha found: ',lassoCV.alpha_)","209b99e6":"error_metrics(lassoCV_pre,label_val)","2a314c41":"from sklearn.linear_model import ElasticNetCV\n\nl1_ratios = np.linspace(0.8, 0.99, 10)\n\nelasticNetCV = ElasticNetCV(alphas=alphas2, l1_ratio=l1_ratios, max_iter=1e4).fit(X_train_s, label_train)\nelasticNetCV_pre = elasticNetCV.predict(X_val_s)\n\nprint('Alpha found: ',elasticNetCV.alpha_)\nprint('l1_ratio: ', elasticNetCV.l1_ratio_)","20154a4a":"error_metrics(elasticNetCV_pre,label_val)","6182da3c":"!pip install xgboost","049f8341":"from xgboost import XGBRegressor","0164200c":"XGB = XGBRegressor(colsample_bytree=0.2,\n                    gamma=0.0,\n                    learning_rate=0.01,\n                    max_depth=3,\n                    min_child_weight=1.5,\n                    n_estimators=9500,                                                                  \n                    reg_alpha=0.7,\n                    reg_lambda=0.7,\n                    subsample=0.2,\n                    seed=42,\n                    silent=1).fit(X_train_s, label_train)\n\nXGB_pre = XGB.predict(X_val_s)","8aa4aabd":"error_metrics(XGB_pre,label_val)","10d2f0d3":"#np.sqrt(mean_squared_error(ridgeCV_pre,label_val))\n#np.sqrt(mean_squared_error(lassoCV_pre,label_val))\n#np.sqrt(mean_squared_error(elasticNetCV_pre,label_val))\nnp.sqrt(mean_squared_error(XGB_pre,label_val))","8f19f27d":"data = {'RidgeCV': [mean_squared_error(ridgeCV_pre,label_val),np.sqrt(mean_squared_error(ridgeCV_pre,label_val)),r2_score(ridgeCV_pre,label_val)],\n        'LassoCV': [mean_squared_error(lassoCV_pre,label_val),np.sqrt(mean_squared_error(lassoCV_pre,label_val)),r2_score(lassoCV_pre,label_val)],\n        'ElasticNetCV': [mean_squared_error(elasticNetCV_pre,label_val),np.sqrt(mean_squared_error(elasticNetCV_pre,label_val)),r2_score(elasticNetCV_pre,label_val)],\n        'XGBoost': [mean_squared_error(XGB_pre,label_val),np.sqrt(mean_squared_error(XGB_pre,label_val)),r2_score(XGB_pre,label_val)]}\n \npd.DataFrame(data, index=['MSE','RMSE','R2 score'])","42e12c04":"testing_s = s.transform(testing)\ntest_prediction=XGB.predict(testing_s)","ceeb7846":"test_prediction","1b1aaa8a":"np.exp(test_prediction)","6930ab75":"testing.index","1b654413":"test_XGB_pred = np.exp(XGB.predict(testing_s))\nXGB_submission = pd.DataFrame({\n        \"Id\": testing.index,\n        \"SalePrice\": test_XGB_pred\n    })\n\nXGB_submission.set_index('Id',inplace=True)\nXGB_submission.to_csv(\"XGB_submission.csv\")","fe02cc2a":"XGB_submission.head()","96084eba":"Now, we need to know which of these features above are 'signifcantly skewed' in other words if there is a class which is almost in all instances the same, for this I will show the distribution of classes for each feature in percentage. For example MSSubclass, its top class corresponds to 37% of instances and top 2 classes correspond to 56%.","cdb6fbe7":"Encoding of categorical nominal features:","0f061504":"### Label normalization\n\nConsist in applying a function which can transform the distribution of our label to a gaussian-shape, this is because machine learning models are based on normal distributions either features or label, therefore in order to obtain a considerably high accuracy in our prediction our dataset must meet such criteria, the function we will use is logarithmic, thus we have to remember that after the prediction we must get our label back to its 'dimension', this is achieved by applying exponential to such predictions.   ","2907ddc9":"For LotFrontage will be imputed the mean, MasVnrArea the mean. For GarageYrBult 79% of instances explains that garage was built the same year as the house, therefore will be imputed the year when house was built.","5a6b1b7b":"Those features in which the first class representst more than 98% means that a huge amount of instances are of the same class, which in other words does not add too much information  and therefore will be dropped.\n\nBecause of this will be dropped the following features:\n\n- Street\n- Utilities\n- Condition2\n- RoofMatl\n- Heating\n- PoolQC","218c0294":"Therefore, after one-hot encoding of these features we will have a total of:\n\n263 = 38 (numerical) + 225 (categorical_encoded)","adb1761f":"I would like to know any feedback in order to increase the performance of the models or tell me if you found a different one even better!\n\nIf you liked this notebook I would appreciate so much your upvote if you want to see more projects\/tutorials like this one. I encourage you to see my projects portfolio, am sure you will love it.\n\nThank you!","f2666606":"As p-value is much lower than 0.05 we reject H0, therefore we have to find a proper method to convert our label.","d07462d6":"## XGBoost Regressor","0e93a512":"## ElasticNetCV","1f99626f":"As a typical step we have to split our dataset into training and validation sets in order to know the behaviour of the model for out-of-bag instances and given the metrics improve by setting the hyperparameters.","e63e3036":"Let's see now if our numerical values are complete:","fbf4035c":"Now, let's deal with categorical features:","49888221":"Remember when we normalized our label using log?, Now that we have predicted for testing these are in such 'dimension', therefore we have to apply exponential to return such values to currencies:","7159375e":"The hyperparameters used in the following model were found using several GridSearch and some were due to past projects, I would just recommend you to change a bit these values and see if it can improve even more, but strongly advice you to learn the best ways to find such values. ","37d52479":"Let's create more features derived from the numerical in order to increase the information given to the model, below I have created 4 features and changed one:","13545999":"We will use the statistical test D'Agostino, which needs to declare a null hypothesis and alternative hypothesis.\nThis test outputs a \"p-value\". The higher this p-value is the closer the distribution is to normal.\nDefining a threshold of 0.05 means that if such value is lower we reject the null hypothesis that the distribution is normal and viceversa.\n\n- H0: The distribution is Normal.\n- H1: The distribution is not Normal.","7474d825":"MSSubClass and MoSold correspond to categorical nominal features, but are misrepresented as numerical, so now I will change its type:","3f496855":"In the summary above we can see XGBoost slightly outperformed the other models, but such difference becomes significant in scoring, because of that I will continue with this model and predict the label of the instances contained in the test file.\n\nAs our model was trained with stardardized features we have to do the same with testing:","29e97de0":"# Feature Engineering","be7ba2db":"Now, show the distribution and compute the skewness of each numerical feature and apply log transform to each one with value higher than 1.0:","bad804dd":"# Modeling\n\nThe following models will be built and compared using their corresponding error measurements:\n\n- RidgeCV\n- LassoCV\n- ElasticNetCV\n- XGBoost Regressor\n\nBefore building the different models let's declare some error metrics in order to compare the performace of each one:","d994d7d1":"## LassoCV","c59b2b57":"## RidgeCV"}}