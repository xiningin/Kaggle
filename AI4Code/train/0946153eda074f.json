{"cell_type":{"9fff6523":"code","2ce70a1b":"code","64b531b1":"code","dbbd3e2f":"code","cc5c9de1":"code","c8922ba9":"code","633e3a04":"code","917fc3bd":"code","37f3965a":"code","f1e01307":"code","1159463e":"code","ff86e299":"markdown","78e7cd64":"markdown","e5abbd08":"markdown","daf393a3":"markdown","5332cd2b":"markdown","0ddba371":"markdown","5bcc33f1":"markdown","f7ba61b5":"markdown","edbac72b":"markdown","28bd2375":"markdown"},"source":{"9fff6523":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2ce70a1b":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom lightgbm import LGBMRegressor","64b531b1":"df = pd.read_csv('\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv')\n\ndef weird_division(n, d):\n    return n \/ d if d else 0\n\ndf['CPM'] = df.apply(lambda x: weird_division(((x['total_revenue'] * 100)), x['measurable_impressions']) * 1000 , axis=1)","dbbd3e2f":"df = df.drop('total_revenue', axis = 1)\ndf = df.drop(['integration_type_id', 'revenue_share_percent'], axis = 1)","cc5c9de1":"df['date'] = pd.to_datetime(df['date'])\ndf = df.sort_values('date')\n\ndf_train = df[df['date'] < pd.to_datetime('22.06.2019')]\ndf_train = df_train[df_train['CPM'] >= 0 ]\ndf_train = df_train[df_train['CPM'] < df_train['CPM'].quantile(.95)]\n\ndf_test= df[df['date'] >= pd.to_datetime('22.06.2019')]\ndf_test = df_test[df_test['CPM'] >= 0]\ndf_test = df_test[df_test['CPM'] < df_test['CPM'].quantile(.95)]","c8922ba9":"for col in df_train.columns: \n    if '_id' in col:\n        train_means = df_train.groupby(col)['CPM'].mean()\n        df_train[col + '_mean'] = df_train[col].map(train_means)\n        df_test[col + '_mean'] = df_test[col].map(train_means)  ","633e3a04":"x = df_train.drop(['CPM','date'], axis = 1)\ny = df_train['CPM']\n\nholdout_x = df_test.drop(['CPM','date'], axis = 1)\nholdout_y = df_test['CPM']","917fc3bd":"model = LGBMRegressor()\nmodel.fit(x, y)\nholdout_pred = model.predict(holdout_x)\nprint('Default LGBM MSE', mean_squared_error(holdout_pred, holdout_y))","37f3965a":"SEED = 42\nnp.random.seed(SEED)\n\ntscv = TimeSeriesSplit(n_splits=2)\ni = 0\nscore = []\n\nfor train_index, val_index in tscv.split(x):\n    if (i == 0): \n        i = 1 \n        continue\n    \n    x_train, x_val = x.iloc[train_index], x.iloc[val_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n\n    print(\" N | steps | depth | leaves | MSE\")\n    for j in range(20):\n        n_estimators = np.random.randint(100, 1500)\n        num_leaves = np.random.randint(10, 500)\n        max_depth = np.random.randint(2, 12)\n        \n        model = LGBMRegressor( \n                    max_depth=max_depth, \n                    n_estimators=n_estimators, \n                    num_leaves=num_leaves, \n                    random_state=SEED, \n                    n_jobs=-1)\n        model.fit(x_train, y_train)\n        y_pred = model.predict(x_val)\n        print(f\"{j}\\t{n_estimators}\\t{max_depth}\\t{num_leaves}\\t{mean_squared_error(y_pred, y_val):.2f}\")\n        score.append([j, n_estimators, max_depth, num_leaves, mean_squared_error(y_pred, y_val)])\n    i += 1","f1e01307":"top_scores = sorted(score, key = lambda x: x[4])[:5]\nbest_n_estimators = top_scores[0][1]     \nbest_max_depth = top_scores[0][2]     \nbest_num_leaves = top_scores[0][3] \ntop_scores","1159463e":"best_model = LGBMRegressor(\n                            n_estimators=best_n_estimators, \n                            max_depth=best_max_depth, \n                            num_leaves=best_num_leaves, \n                            random_state=SEED,\n                          )\nbest_model.fit(x, y)\nholdout_pred = best_model.predict(holdout_x)\nprint('Tuned LGBM MSE:', mean_squared_error(holdout_pred, holdout_y))\nholdout_pred = holdout_pred * (holdout_pred >= 0) \nprint('MSE with negative predictions set to 0:', mean_squared_error(holdout_pred, holdout_y))","ff86e299":"Target mean encoding for categorical features (\"*_id\")","78e7cd64":"Drop total_revenue and non-informative columns","e5abbd08":"Best parameters","daf393a3":"Model with best parameters    ","5332cd2b":"Default LGBM ","0ddba371":"Read data and apply CPM function ","5bcc33f1":"Preparing for fit\/predict","f7ba61b5":"# Final MSE = 2600","edbac72b":"Hyperparameters random search","28bd2375":"Train\/Holdout split by date, 0-95% CPM filtering"}}