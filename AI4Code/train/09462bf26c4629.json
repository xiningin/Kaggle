{"cell_type":{"e4087902":"code","6ddf3b1a":"code","7fe5efb2":"code","8ee300b5":"code","2d982e1c":"code","ddf0ee10":"code","0abab0eb":"code","10c49a6b":"code","1a92f2a6":"code","ea79bf59":"code","4cb3ac3b":"code","217507fb":"code","93fb40e7":"code","0ef4dcc8":"code","6c44c9a9":"code","30f4aeff":"code","3e2e9ad9":"code","8f92fc87":"code","30b4b3e9":"code","6de152fe":"code","bac6be5e":"code","915674a2":"code","f425ac16":"code","299eee61":"code","0c80ece7":"code","544b071a":"code","c677ec63":"code","a09e365e":"markdown","e51448bc":"markdown","85763040":"markdown","df3a1bf8":"markdown","b1cfa70b":"markdown","39cdc43e":"markdown","7e966a6c":"markdown","da80cac3":"markdown","77573763":"markdown","6990e588":"markdown","c3568fa4":"markdown","9132f8fe":"markdown","854e34c5":"markdown","4a874bc8":"markdown","ed5aeedf":"markdown","32348dbc":"markdown","e6720cf7":"markdown","e45fa3a5":"markdown","41dcca33":"markdown","f68ed35c":"markdown","cd2624ae":"markdown","caf8002b":"markdown","feb91e00":"markdown","adee62af":"markdown","275e9f57":"markdown","3cdfec69":"markdown","3e20a7c6":"markdown","d42256d9":"markdown","4984b012":"markdown","9c864e12":"markdown","08acac71":"markdown","d9854cf9":"markdown","6acfd5d3":"markdown","37b211fa":"markdown"},"source":{"e4087902":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score as acs\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","6ddf3b1a":"# Reading in datasets\ndf = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","7fe5efb2":"# Shape of dataset\ndf.shape","8ee300b5":"# Checking for null values in %\nround((df.isnull().sum()\/len(df))*100)","2d982e1c":"HEIGHT = 500\nWIDTH = 900\nNBINS = 50\nSCATTER_SIZE=700\n\n\ndef plot_histogram(dataframe, column, color, bins, title, width=WIDTH, height=HEIGHT):\n    '''\n        Description:\n        ----------\n        This function plots a histogram.\n\n        Parameters\n        ----------\n        dataframe : pandas dataframe\n            Complete Dataframe\n        column : Name of column as a str, that will be used in x axis \n            Example: \"age\"\n        color: Name of column as str, that will be represented by color\n            Example: \"sex\"\n        bins: Numeric amount, to determine the width of bars\n        title: Title of the plot, can be feeded directly as str\n            Example: \"Plot for xyz\"\n        width and height: Both are numeric\n\n        Output\n        ------\n        A histogram plot\n    '''\n    figure = px.histogram(\n        dataframe, \n        column, \n        color=color,\n        nbins=bins, \n        title=title, \n        width=width,\n        height=height\n    )\n    figure.update_layout({\n            'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n            'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        })\n    figure.show()","ddf0ee10":"def plot_violin(dataframe, X, y, title, width=WIDTH, height=HEIGHT):\n    fig = px.violin(\n        dataframe, \n        X, \n        y, \n        points = 'all',\n        title = title,\n        width = width,\n        height = height,\n        box = True\n    )\n    fig.update_layout({\n        'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n        'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n    })\n    fig.show()","0abab0eb":"plot_histogram(df, 'age', 'sex', NBINS, 'Patients Age and Sex Plot')","10c49a6b":"plot_violin(df, 'sex', 'age', 'Patients Age Death Distribution')","1a92f2a6":"plot_histogram(df, 'age', 'DEATH_EVENT', NBINS, 'Patients Age Death Plot')","ea79bf59":"plot_violin(df, 'DEATH_EVENT', 'age', 'Patients Age Death Distribution')","4cb3ac3b":"def feature_Select(x, y):\n    # Lib import\n    from sklearn.ensemble import ExtraTreesClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    \n    models = [\n        ('Extra Trees Classifier:', ExtraTreesClassifier()),\n        ('Random Forest Classifier:', RandomForestClassifier()),\n        ]\n    for name, model in models:\n\n        model.fit(x,y)\n        feat_importances = pd.Series(model.feature_importances_, index=x.columns).sort_values(ascending=False)\n\n        # Displaying values\n        figure = px.bar(feat_importances,\n                        x = feat_importances.values, \n                        y = feat_importances.keys(), \n                        text = np.round(feat_importances.values, 2),\n                        title = name + ' Feature Selection Plot')\n        figure.update_layout({\n            'plot_bgcolor': 'rgba(0, 0, 0, 0)',\n            'paper_bgcolor': 'rgba(0, 0, 0, 0)',\n        })\n        figure.show()","217507fb":"# Feature Selection\n\nx = df.iloc[:, :-1]\ny = df.iloc[:,-1]","93fb40e7":"feature_Select(x, y)","0ef4dcc8":"# Selecting x and y\nx = df.iloc[:, [4,7,11]].values\ny = df.iloc[:,-1].values","6c44c9a9":"# Splitting the dataset into training set and test set\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state =0)","30f4aeff":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","3e2e9ad9":"!pip install lazypredict","8f92fc87":"!pip install --upgrade pandas","30b4b3e9":"from lazypredict.Supervised import LazyClassifier\n\nclf = LazyClassifier(verbose=0,ignore_warnings=True, custom_metric=None)\nmodels,predictions = clf.fit(X_train, X_test, y_train, y_test)\n\nprint(models)","6de152fe":"def det_Leaves(X_train, X_test, y_train, y_test):\n\n    acc = []\n    \n    for leaves in range(10,20):\n        clf = DecisionTreeClassifier(max_leaf_nodes = leaves, random_state=0, criterion='entropy')\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        acc.append(acs(y_test, y_pred))\n    \n    plt.plot(list(range(10,20)), acc)\n    plt.show()","bac6be5e":"def det_Estimators(X_train, X_test, y_train, y_test):\n    \n    acc = []\n    for estimators in range(15,25):\n        clf = ExtraTreesClassifier(n_estimators = estimators, random_state=0, criterion='entropy')\n        #clf = RandomForestClassifier(n_estimators = estimators, random_state=0, criterion='entropy')\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        acc.append(acs(y_test, y_pred))\n    plt.figure(figsize=(15, 5))\n    plt.title('Estimators')\n    plt.plot(list(range(15,25)), acc)\n    plt.show()","915674a2":"det_Estimators(X_train, X_test, y_train, y_test)","f425ac16":"classifier = ExtraTreesClassifier(n_estimators = 17, criterion='entropy', random_state=0)\nclassifier.fit(X_train,y_train)\n\n\ny_pred_ext = classifier.predict(X_test)\nacs(y_test, y_pred_ext)","299eee61":"det_Leaves(X_train, X_test, y_train, y_test)","0c80ece7":"classifier = DecisionTreeClassifier(max_leaf_nodes = 20, criterion='entropy', random_state=0)\nclassifier.fit(X_train,y_train)\n\n\ny_pred_dtc = classifier.predict(X_test)\nacs(y_test, y_pred_dtc)","544b071a":"def model_eval(y_test, y_pred):\n    '''\n    Description:\n    ----------\n    This function plots a confusion matrix.\n    \n    Parameters\n    ----------\n    y_test : True Y values of test set\n    y_pred : Predicted Y values\n\n    Output\n    ------\n    Labelled confusion matrix \n    '''\n    from sklearn.metrics import confusion_matrix\n    import matplotlib.pyplot as plt\n    \n    # Calculate confusion matrix\n    cf_matrix = confusion_matrix(y_test, y_pred)\n\n    # Visualize it\n    group_names = ['True Neg','False Pos','False Neg','True Pos']\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                    cf_matrix.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cf_matrix.flatten()\/np.sum(cf_matrix)]\n\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n              zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    \n    ## Set size of confusion matrix\n    plt.figure(figsize = (8,5))\n    \n    ## Plot the heatmap\n    sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Greens', cbar=False)","c677ec63":"model_eval(y_test, y_pred_ext)","a09e365e":"### Violin Plot Func","e51448bc":"[Return to TOC](#toc)","85763040":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:gray; border:0' role=\"tab\" aria-controls=\"home\"><center>Visual Exploration<\/center><\/h4>","df3a1bf8":"**Deaths increases as the age increases and there are fewer rows of data when death occurs**","b1cfa70b":"Accuracies were improved of the models but ExtraTreesClassifer wins with the highest accuracy. Now, I will move on to evaluating the models in terms of specificty, sensitivity and etc.","39cdc43e":"<a id=\"toc\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center>Table of Contents<\/center><\/h2>\n\n1. [About the Dataset](#Intro)\n2. [Task](#Obj)\n3. [Exploratory Data Analysis](#EDA)\n    1. Data Exploration\n    2. Visual Exploration\n4. [Model Building](#Model)\n    1. Train and Test Split\n    2. Lazy Prediction\n    3. Fine Tuning Best Model\n5. [Evaluation](#Eval)","7e966a6c":"**There are no null values in the columns so, no special pre-processing is required.**","da80cac3":"[Return to TOC](#toc)","77573763":"# <center>Is Your Heart Healthy ?<\/center><br>\n<img src = \"https:\/\/h2hcardiaccenter.com\/blog\/wp-content\/uploads\/2018\/07\/shutterstock_556072003-1160x650-1024x574.jpg\"><\/img><br>\n#### <div align='right'>Made by: **Asad Mahmood<\/div>**","6990e588":"<a name=\"Eval\"><\/a>\n\n<a name=\"Model\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center>Evaluation<\/center><\/h3>","c3568fa4":"Create a model for predicting mortality caused by Heart Failure using claasification techniques of your choice.","9132f8fe":"Using two algos for feature selection.\n\n+ **ExtraTreesClassfier:** The purpose of the ExtraTreesClassifier is to fit a number of randomized decision trees to the data, and in this regard is a from of ensemble learning. Particularly, random splits of all observations are carried out to ensure that the model does not overfit the data.\n<br>\n+ **Step forward and backward feature selection:** This is a \u201cwrapper-based\u201d feature selection method, where the feature selection is based on a specific machine learning algorithm (in this case, the RandomForestClassifier). \n    - For forward-step selection, each individual feature is added to the model one at a time, and the features with the highest ROC_AUC score are selected as the best features. \n    - When conducting backward feature selection, this process happens in reverse \u2014 whereby each feature is dropped from the model one at a time, i.e. the features with the lowest ROC_AUC scores are dropped from the model.\n<br>\n\nThe reason I'm using both of these is to demonstrate both of these strategies in action and also to double check my selection.","854e34c5":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:gray; border:0' role=\"tab\" aria-controls=\"home\"><center>Data Exploration<\/center><\/h4>","4a874bc8":"**Selected parameters:**\n+ Leaves: 18\n+ Estimators: 16","ed5aeedf":"<a name=\"Intro\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center>About the Dataset<\/center><\/h3>\n","32348dbc":"### 1. ExtraTreesClassifier","e6720cf7":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:gray; border:0' role=\"tab\" aria-controls=\"home\"><center>Fine Tuning Model<\/center><\/h4>","e45fa3a5":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:gray; border:0' role=\"tab\" aria-controls=\"home\"><center>Lazy Prediction<\/center><\/h4>","41dcca33":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:gray; border:0' role=\"tab\" aria-controls=\"home\"><center>Feature Scaling<\/center><\/h4>","f68ed35c":"**Its seems that there are more males than females in this data and both genders have a greater density around the age of 55 to 65.**","cd2624ae":"<a name=\"Model\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center>Model Building<\/center><\/h3>","caf8002b":"### Selected Model for Evaluation: Extra Trees Classifier","feb91e00":"[Return to TOC](#toc)","adee62af":"<a name=\"Obj\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center>Task<\/center><\/h3>","275e9f57":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:gray; border:0' role=\"tab\" aria-controls=\"home\"><center>Feature Extraction<\/center><\/h4>","3cdfec69":"### 2. DecisionTreesClassifier","3e20a7c6":"### Histogram Func","d42256d9":"<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h4 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:gray; border:0' role=\"tab\" aria-controls=\"home\"><center>Train and Test Split<\/center><\/h4>","4984b012":"**Cardiovascular diseases** (CVDs) are the number 1 cause of death globally, taking an estimated **17.9** million lives each year, which accounts for **31%** of all deaths worlwide. Heart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\n\nMost cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n\nThe dataset can be found at: https:\/\/www.kaggle.com\/andrewmvd\/heart-failure-clinical-data","9c864e12":"I am selecting the top two most accurate models and going to fine tune them so as to get an even better results.<br>","08acac71":"[Return to TOC](#toc)","d9854cf9":"<a name=\"EDA\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:black; border:0' role=\"tab\" aria-controls=\"home\"><center>Exploratory Data Analysis<\/center><\/h3>","6acfd5d3":"I will be choosing top 3 features that have the highest importance score i.e \"time\", \"serum_creatinine\" and \"ejection_fraction\"","37b211fa":"[Return to TOC](#toc)"}}