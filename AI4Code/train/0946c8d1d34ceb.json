{"cell_type":{"e46b72b2":"code","9e4ca868":"code","d2c07f34":"code","fff68479":"code","bb12e480":"code","f3cd2021":"code","1026078f":"code","3cad6c60":"code","bfcea23d":"code","9c32f3c1":"code","a545fad6":"code","a666579c":"code","5a9b8d05":"code","f69060a8":"code","6790330d":"code","303dac1a":"code","b7af8859":"code","1c115eff":"code","50805274":"code","a9de71a7":"code","d11afadf":"code","1a16b305":"code","9e091733":"code","044eb179":"code","4efd9f31":"code","1328acad":"code","0040a284":"code","823d98f8":"code","8a0d2f9d":"code","7c5d2762":"markdown","d3bcb2a0":"markdown","dd3410a8":"markdown","fd898dcf":"markdown","65cf8b9a":"markdown","92c98611":"markdown","b664506c":"markdown","dc28da6f":"markdown","93c5326d":"markdown","62047da6":"markdown","bd05b230":"markdown","6803a70a":"markdown","ddd4e90a":"markdown","9d9ecf03":"markdown"},"source":{"e46b72b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9e4ca868":"def one_hot_encoder(dataframe, col_name):\n    return pd.concat([dataframe, pd.get_dummies(dataframe[col_name])], axis=1).drop([col_name],axis=1)","d2c07f34":"def new_feature(og_name,new_name,thresh,train):\n    train.loc[train[og_name] > 150000, new_name] = 1   \n    train.loc[train[og_name] <= 150000, new_name] = 0 \n    return train","fff68479":"def one_hot_multi_cat(df,cat_name_list,col_name):\n    try:\n        new_df=pd.get_dummies(df[col_name])\n        for cat in cat_name_list:\n            df[str(cat)]=new_df[cat]\n        return df\n    except:\n        pass","bb12e480":"def most_freq_cats(df,n,col_name):\n    return df[col_name].value_counts()[:n].index.tolist()","f3cd2021":"def bar_plotter(df,col_name):\n    index_list=df[col_name].value_counts().index.tolist()\n    for i in range(len(index_list)):\n        index_list[i]=str(index_list[i])\n    info=list(df[col_name].value_counts())\n    for i in range(len(info)):\n        info[i]=(info[i]\/df.shape[0])*100\n    plt.bar(index_list[:8], info[:8], color ='maroon',\n        width = 0.4)\n    plt.xlabel(col_name)\n    plt.ylabel(\"Percentage\")\n    plt.title(\"\")\n    plt.show()","1026078f":"train=pd.read_csv(\"..\/input\/amazon-employee-access-challenge\/train.csv\")\nactual_train=train\ntest=pd.read_csv(\"..\/input\/amazon-employee-access-challenge\/test.csv\")\npositive_df_mask=train[\"ACTION\"]==1\nnegative_df_mask=train[\"ACTION\"]==0\npositive_df=train[positive_df_mask]\nnegative_df=train[negative_df_mask]\ny_train=train[\"ACTION\"]\nx_train=train.drop([\"ACTION\"],axis=1)\ntrain=pd.concat([x_train,test])","3cad6c60":"train.nunique()","bfcea23d":"train_corr=actual_train.corr()\ntrain_corr['ACTION'].sort_values(ascending=False)","9c32f3c1":"bar_plotter(negative_df,\"ROLE_ROLLUP_1\")\nbar_plotter(positive_df,\"ROLE_ROLLUP_1\")","a545fad6":"cats=most_freq_cats(train,30,\"ROLE_ROLLUP_1\")\ntrain=one_hot_multi_cat(train,cats,\"ROLE_ROLLUP_1\")","a666579c":"bar_plotter(negative_df,\"RESOURCE\")\nbar_plotter(positive_df,\"RESOURCE\")","5a9b8d05":"bar_plotter(negative_df,\"ROLE_ROLLUP_2\")\nbar_plotter(positive_df,\"ROLE_ROLLUP_2\")","f69060a8":"cats=most_freq_cats(train,30,\"ROLE_ROLLUP_2\")\ntrain=one_hot_multi_cat(train,cats,\"ROLE_ROLLUP_2\")","6790330d":"bar_plotter(negative_df,\"ROLE_DEPTNAME\")\nbar_plotter(positive_df,\"ROLE_DEPTNAME\")","303dac1a":"cats=most_freq_cats(train,20,\"ROLE_DEPTNAME\")\ntrain=one_hot_multi_cat(train,cats,\"ROLE_DEPTNAME\")","b7af8859":"bar_plotter(negative_df,\"ROLE_CODE\")\nbar_plotter(positive_df,\"ROLE_CODE\")","1c115eff":"cats=most_freq_cats(train,20,\"ROLE_CODE\")\ntrain=one_hot_multi_cat(train,cats,\"ROLE_CODE\")","50805274":"plt.figure(figsize=(9, 6))\nsns.catplot('ACTION', 'ROLE_CODE', data=actual_train)","a9de71a7":"train=new_feature(\"ROLE_CODE\",\"1\",140000,train)","d11afadf":"bar_plotter(negative_df,\"ROLE_TITLE\")\nbar_plotter(positive_df,\"ROLE_TITLE\")","1a16b305":"bar_plotter(negative_df,\"ROLE_FAMILY\")\nbar_plotter(positive_df,\"ROLE_FAMILY\")","9e091733":"cats=most_freq_cats(train,20,\"ROLE_FAMILY\")\ntrain=one_hot_multi_cat(train,cats,\"ROLE_FAMILY\")","044eb179":"train=train.drop([\"id\",\"RESOURCE\",\"ROLE_TITLE\"],axis=1)","4efd9f31":"plt.figure(figsize=(9, 6))\nsns.catplot('ACTION', 'MGR_ID', data=actual_train)","1328acad":"train=new_feature(\"MGR_ID\",\"MGR_thresh\",140000,train)","0040a284":"from xgboost import XGBRegressor\n# Make a decision tree and train\ntree = XGBRegressor()\ntrain=pd.DataFrame(np.asarray(train))\ntree.fit(train[:x_train.shape[0]], y_train)","823d98f8":"import pickle\nfilename = 'finalized_model.sav'\npickle.dump(tree, open(filename, 'wb'))","8a0d2f9d":"predictions=tree.predict(train[x_train.shape[0]:])\nsub_df=pd.read_csv(\"..\/input\/amazon-employee-access-challenge\/sampleSubmission.csv\")\nsub_df['Action'] = predictions\nsub_df.to_csv(\"submission.csv\",index=False)","7c5d2762":"On further analyzing ROLE_CODE we can see that a new feature can be created, which will be 1 if the \"ROLE_CODE\" is above a certain value","d3bcb2a0":"We are using the XGBRegressor here, This was an empirical choice as many other classifiers were tried and this gave a better score consistently, robably beacuse it uses an ensemble.\nA single decision tree Regressor gives a score of approx 0.81147.\nA random forest with 1000 trees does not even cross 0.64 even with the same paramenters. \nA possible explanation I have come up with is this. In the given dataset there are some features that are much more important than others. While in a Decision Tree they get significant importance, in a Random Forest it is possible that because of the randomness in feature selection, the more important features get drowned out by other less important and much more numerous features. \nHowever this may as much be a case of the way this particular data analysis has been done, and a different procedure may yield the usual results of a Random Forest exceeding a single Decision Tree in accuracy.\ntwo hyper-parameters that have been set were found out empirically by multiple trials.","dd3410a8":"The above function returns a dataframe with the given column being dropped and its categories all one-hot-encoded","fd898dcf":"We observe that it is possible to create another category denoting if the \"MGR_ID\" is above a certain threshold.","65cf8b9a":"The number of categories in both \"ROLE_ROLLUP_1\" and \"ROLL_ROLLUP_2\" are reasonably small. So we can one-hot-encode the most common as a large percentage of the categories will provide important information. ","92c98611":"Some components have been inspired by:\nhttps:\/\/www.kaggle.com\/kickitlikeshika\/employee-access-eda-data-cleaning#EDA","b664506c":"As stated earlier, the percentegae for even the highest of the categories is quite small.","dc28da6f":"Again there seem to be a large number of categories so we will pick up 50 of the most common and encode them since ROLE_CODE has the highest correlation with the target.","93c5326d":"Here we cannot claim that the categories will not have significant number of examples, nor do we wish to one-hot-enode all of this column as the number of unique values is still very large, so we will just pick the 20 most frequent in the dataset and one-hot=encode them.","62047da6":"Both \"ROLE_CODE\" and \"ROLE_TITLE\" have the same number of unique values and their graphs look exactly the same. There is a one-to-one correspondence between the elements of these two columns. Hence one of them is extraneous and can be removed.","bd05b230":"There are more significant categories here so as stated earlier, we will one-hot-encode this entire column.","6803a70a":"In all of these columns, there are a large number of unique values. So right off that bat, we will drop the ids since they are unique for everyone. And we will also drop the \"RESOURCE\" column. The reasoning is that the large number of categories makes it impossible for any generalization to take place for any one of the categories because of the very few available examples. The other columns will be dealt with one by one,","ddd4e90a":"Now let us look at the \"MGR_ID\" part","9d9ecf03":"ROLE_CODE, ROLE_ROLLUP_1 have the highest magnitude of correlation and we must focus on them a bit more."}}