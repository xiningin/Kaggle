{"cell_type":{"92a2642b":"code","2be37cb2":"code","5c12ebb2":"code","d9debe58":"code","433e8ebe":"code","484a8cc8":"code","2286e425":"code","088022b7":"code","0662ca62":"code","d3f68fd0":"code","4cb3c83c":"code","b8fa15b3":"code","4d0bef38":"code","ab6e5cae":"code","7baa2bae":"code","33778783":"code","74e3b220":"code","e8f75867":"code","98fb3720":"code","c609ca29":"code","ba9310a1":"code","7b126c10":"code","720f3cc8":"code","ac9b6a3e":"code","41b3fe38":"code","225d7f48":"code","fdb79b5a":"code","ede88df4":"code","e6971280":"code","6ae0011a":"code","711f7033":"code","6abd9aeb":"code","2d1affa4":"code","006b85b7":"code","bcb35398":"code","2519e8f3":"code","0d5ff9be":"code","687e1f06":"code","8ecedfe3":"code","5131b257":"code","c44aefc8":"code","ff6c3d09":"code","9a2b3852":"code","5d83ef38":"code","1e1c0d80":"code","6b8636d2":"markdown","6503898c":"markdown"},"source":{"92a2642b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2be37cb2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os,gc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score,roc_curve,auc,confusion_matrix\nfrom sklearn.model_selection import RandomizedSearchCV\nimport xgboost as xgb\nimport time\nplt.style.use('seaborn-whitegrid')\nimport warnings\nwarnings.simplefilter(\"ignore\")\ncolors = [x['color'] for x in plt.rcParams['axes.prop_cycle']]","5c12ebb2":"train_trn = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_transaction.csv\")","d9debe58":"train_id = pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")","433e8ebe":"test_trn  = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_transaction.csv\")\ntest_id = pd.read_csv(\"..\/input\/ieee-fraud-detection\/test_identity.csv\")","484a8cc8":"df_train = train_trn.merge(train_id,on=['TransactionID'],how='left')\n#print(f'Shape of identity train data: {train_id.shape}')","2286e425":"del train_trn\ndel train_id","088022b7":"id_cols = [col for col in test_id.columns if col[0]+col[1] == 'id']\nrename_cols = {i:'id_'+str(i[-2]+i[-1]) for i in id_cols}\ntest_id = test_id.rename(columns=rename_cols)\ndf_test = test_trn.merge(test_id,on=['TransactionID'],how='left')\ndel test_trn\ndel  test_id","0662ca62":"df_train.drop(columns='V107',inplace=True)\ndf_test.drop(columns='V107',inplace=True)","d3f68fd0":"null_cols = [col for col in df_train.columns if df_train[col].isnull().sum() \/ df_train.shape[0] > 0.9]\ndf_train.drop(null_cols,axis=1,inplace=True)\n\n\n#null_col = [col for col in df_test.columns if df_test[col].isnull().sum() \/ df_test.shape[0]>.9]\n#df_test.drop(null_cols,axis=1,inplace=True)\n","4cb3c83c":"for i in df_train.columns:\n    if df_train[i].dtypes=='object':     #filling null alues with mode for categorical variables\n        df_train[i].fillna(df_train[i].mode()[0],inplace=True)\n\n#for i in df_test.columns:\n#    if df_test[i].dtypes=='object':     #filling null alues with mode for categorical variables\n#        df_test[i].fillna(df_test[i].mode()[0],inplace=True)        ","b8fa15b3":"float_col = df_train.select_dtypes(include='float64').columns\n#int_col = df_train.select_dtypes(include='int64').columns\ndf_train[float_col] = df_train[float_col].astype('float32')","4d0bef38":"for i in df_train.columns:\n    if df_train[i].dtypes=='int64' or df_train[i].dtypes==\"float32\":     #filling null alues with mode for categorical variables\n        df_train[i].fillna(df_train[i].mean(),inplace=True)","ab6e5cae":"#float_col = df_test.select_dtypes(include='float64').columns\n#df_test[float_col] = df_test[float_col].astype('float32')","7baa2bae":"#del float_col","33778783":"## transaction \ndf_train['transaction_day_of_the_week'] = np.floor((df_train['TransactionDT'] \/ (3600 * 24) - 1) % 7)\ndf_train['Transaction_hour'] = np.floor(df_train['TransactionDT'] \/ 3600) % 24","74e3b220":"df_train['device_name'] = df_train['DeviceInfo'].str.split('\/', expand=True)[0]  #categorical","e8f75867":"df_train['device_version'] = df_train['DeviceInfo'].str.split('\/', expand=True)[1]  #categorical","98fb3720":"## Most of the column of id_12 - id_38 don't give any useful information without id 30, 31 and 34\n\ndf_train[\"device_type\"] = df_train[\"id_30\"].str.split(' ', expand = True)[0]","c609ca29":"df_train['device_version_id_30'] = df_train['id_30'].str.split(' ', expand=True)[1]   #categorical","ba9310a1":"df_train['browser_id_31'] = df_train['id_31'].str.split(' ', expand=True)[0]   #categorical\ndf_train['version_id_31'] = df_train['id_31'].str.split(' ', expand=True)[1]   #categorical\ndf_train['id_34'] = df_train['id_34'].str.split(':', expand=True)[1]","7b126c10":"df_train.loc[df_train['device_name'].str.contains('SM', na=False), 'device_name'] = 'Samsung'\ndf_train.loc[df_train['device_name'].str.contains('SAMSUNG', na=False), 'device_name'] = 'Samsung'\ndf_train.loc[df_train['device_name'].str.contains('GT-', na=False), 'device_name'] = 'Samsung'\ndf_train.loc[df_train['device_name'].str.contains('Moto G', na=False), 'device_name'] = 'Motorola'\ndf_train.loc[df_train['device_name'].str.contains('Moto', na=False), 'device_name'] = 'Motorola'\ndf_train.loc[df_train['device_name'].str.contains('moto', na=False), 'device_name'] = 'Motorola'\ndf_train.loc[df_train['device_name'].str.contains('LG-', na=False), 'device_name'] = 'LG'\ndf_train.loc[df_train['device_name'].str.contains('rv:', na=False), 'device_name'] = 'RV'\ndf_train.loc[df_train['device_name'].str.contains('HUAWEI', na=False), 'device_name'] = 'Huawei'\ndf_train.loc[df_train['device_name'].str.contains('ALE-', na=False), 'device_name'] = 'Huawei'\n\ndf_train.loc[df_train['device_name'].str.contains('-L', na=False), 'device_name'] = 'Huawei'\n\ndf_train.loc[df_train['device_name'].str.contains('Blade', na=False), 'device_name'] = 'ZTE'\ndf_train.loc[df_train['device_name'].str.contains('BLADE', na=False), 'device_name'] = 'ZTE'\ndf_train.loc[df_train['device_name'].str.contains('Linux', na=False), 'device_name'] = 'Linux'\ndf_train.loc[df_train['device_name'].str.contains('XT', na=False), 'device_name'] = 'Sony'\ndf_train.loc[df_train['device_name'].str.contains('HTC', na=False), 'device_name'] = 'HTC'\ndf_train.loc[df_train['device_name'].str.contains('ASUS', na=False), 'device_name'] = 'Asus'\ndf_train.loc[df_train.device_name.isin(df_train.device_name.value_counts()[df_train.device_name.value_counts() < 200].index), 'device_name'] = \"Others\"","720f3cc8":"#df_train.loc[df_train['device_name']=='Samsung'].count()","ac9b6a3e":"df_train = df_train[df_train['TransactionAmt'] < 30000]","41b3fe38":"#def detectmissing(df):\n#    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])\n#    summary = summary.reset_index()\n#    summary['Uniques'] = df.nunique().values\n    \n #   return summary","225d7f48":"#train_missing = detectmissing(df_train)\n#train_missing\n#test_missing = detectmissing(test)\n# test_missing","fdb79b5a":"test_dummy = pd.get_dummies(df_test, drop_first=True)\n#test_dummy.info()\nfloat_col = test_dummy.select_dtypes(include='float64').columns\ntest_dummy[float_col] = test_dummy[float_col].astype(\"float32\")","ede88df4":"del df_test","e6971280":"train_dummy = pd.get_dummies(df_train, drop_first = True)\ntrain_dummy.drop(columns = \"TransactionID\", inplace = True)","6ae0011a":"del df_train","711f7033":"#obj_cols = ['id_12','id_15', 'id_16','id_28', 'id_29','id_30', 'id_31', 'id_33', 'id_34', 'id_35', \n#            'id_36', 'id_37', 'id_38', 'device_type', 'DeviceInfo', 'ProductCD', 'card4', 'card6', 'M4','P_emaildomain',\n#            'R_emaildomain', 'addr1', 'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9','device_name','device_version','OS_id_30','device_version_id_30','browser_id_31','version_id_31']","6abd9aeb":"#df_train.info()","2d1affa4":"#from sklearn.preprocessing import LabelEncoder\n#le = LabelEncoder()\n#for i in obj_cols:\n#    if i in df_train.columns:\n#        df_train[i] = le.fit_transform(df_train[i].astype(str).values)","006b85b7":"#y= df_train['isFraud']\n#x = df_train.drop(['isFraud','TransactionDT','TransactionID'],axis=1)","bcb35398":"#del train_dummy","2519e8f3":"#from sklearn.model_selection import train_test_split\n#x_train,x_test,y_train,y_test = train_test_split(x,y,stratify = y,test_size = 0.3, random_state=1)","0d5ff9be":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(\n    train_dummy.drop(columns='isFraud'), train_dummy['isFraud'], test_size=0.25, random_state=0)","687e1f06":"import lightgbm as lgb\nd_train = lgb.Dataset(X_train,label=y_train)\nd_test = lgb.Dataset(X_valid, label = y_valid)","8ecedfe3":"#del df_train","5131b257":"params = {'num_leaves': 250,\n          'task':'train',\n          'application':'classification',\n          'min_child_samples': 80,\n          'objective': 'binary',\n          'max_depth': 15,\n          'learning_rate': 0.03,\n          \"boosting_type\": \"gbdt\",\n          \"subsample_freq\": 3,\n          \"subsample\": 0.9,\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3,\n          'reg_lambda': 0.3,\n          'random_state': 1,\n          'colsample_bytree': 0.9,\n          'is_unbalance':True}# unbalanced dataset 3.5%\n          \n\n              \nmodel1 = lgb.train(params, d_train, num_boost_round= 500, valid_sets=[d_train,d_test], verbose_eval=50, early_stopping_rounds=50)","c44aefc8":"y_pred1 = model1.predict(test_dummy,predict_disable_shape_check=True)","ff6c3d09":"#from sklearn.ensemble import RandomForestClassifier #importing the RandomForestClassifier library for our decision tree classifier\n#model2 = RandomForestClassifier(max_depth=20,n_estimators=100,random_state=1)    #creating the instance of classifier with 100 trees\n#model2.fit(X_train,y_train)                          #fitting our model on training data","9a2b3852":"#y_pred2 = model2.predict(test_dummy)","5d83ef38":"#y_pred = (y_pred1+y_pred2)\/2","1e1c0d80":"submission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv')\nsubmission['isFraud'] = y_pred1  #_final\nsubmission.to_csv('submission.csv',index=False)\nsubmission.head()","6b8636d2":"Let's add some extra column based on the information of the existing column","6503898c":"Feature V107 has one unique value"}}