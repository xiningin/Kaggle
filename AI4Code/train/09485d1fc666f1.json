{"cell_type":{"3267725e":"code","6ac71214":"code","09c9a824":"code","8de50ea8":"code","a437d43f":"code","617b0699":"code","5c771faf":"code","dbee3c2e":"code","44b6925c":"code","b055e92d":"code","0641879a":"code","e50b39e7":"code","f33b4a00":"code","7a5aa31f":"code","12108f2e":"code","349fc779":"code","3cbe9ff1":"code","75e8a241":"code","fcce6f35":"code","27a00b78":"code","db2547f3":"code","00908519":"code","b2415438":"code","337ed8bf":"code","a94b5ea8":"code","266f1ef2":"code","18aa3be0":"code","e1b72cad":"code","49874361":"code","c3ac8f4f":"code","af7090ce":"code","c3884642":"code","43b42833":"code","f2de68eb":"code","9d06974d":"code","3115b3a2":"code","50f18ba6":"code","afa6ab1e":"code","72fafd9b":"code","fb4615a8":"code","4599e3ae":"code","0c27274f":"markdown","b0832732":"markdown","05a95c05":"markdown","5329c14a":"markdown","459bde69":"markdown","7c9c1c1d":"markdown","5f3ca37e":"markdown","e2037d6f":"markdown","daa482ee":"markdown","955409d0":"markdown","035e267a":"markdown","abbffbab":"markdown","7c249cc7":"markdown","86db1a64":"markdown","7def44bd":"markdown","1a439b38":"markdown","ef729a42":"markdown","68b6c7d3":"markdown","1c1f9c22":"markdown","c90ba324":"markdown","02609c15":"markdown","3837716c":"markdown","5d145df2":"markdown","9b96c71f":"markdown","02854803":"markdown","a257c473":"markdown","9da3f43c":"markdown","e5e3c159":"markdown","29f7e649":"markdown","3d0c535d":"markdown","2668602b":"markdown","8a552da7":"markdown","10486c14":"markdown","105573b8":"markdown","b60ddd31":"markdown","7dfebd5a":"markdown"},"source":{"3267725e":"!pip uninstall scikit-learn -y\n!pip install scikit-learn==0.23.2","6ac71214":"!pip install kmodes","09c9a824":"# Standards Librairies import\nimport os\nfrom time import time\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import homogeneity_score, adjusted_rand_score, adjusted_mutual_info_score\n\n# Clustering Librairies import\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import MeanShift, estimate_bandwidth\nfrom yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer, InterclusterDistance\nfrom kmodes.kprototypes import KPrototypes\n\n# Personnal scripts\nimport olist_functions as fct\n\nmpl.rcParams[\"figure.figsize\"] = (12, 8)\ninit_notebook_mode()\nwarnings.filterwarnings('ignore')","8de50ea8":"# root path for cleaned dataset\nroot_path = '..\/input\/olistcustomerssegmentation'\ndata = pd.read_csv(root_path + \"\/olist-customers-segmentation.csv\",\n                   index_col=0)\ndata.head(3)","a437d43f":"data.drop([\"customer_zip_code_prefix\",\n           \"customer_city\",\n           \"customer_state\",\n           \"Flag and name\",\n           \"mean_price_order\",\n           \"mean_nb_items\"], axis=1, inplace=True)","617b0699":"categorical_features = list(data.select_dtypes(exclude=['int64', 'float64', 'uint8']).columns)\ncategorical_features","5c771faf":"numerical_features = list(data.select_dtypes(include=['int64','float64', 'uint8']).columns)\nnumerical_features","dbee3c2e":"scaler = MinMaxScaler()\n\npreprocessor = ColumnTransformer([\n    ('scaler', scaler, numerical_features)])","44b6925c":"X = data.copy()\n\n# Elbow method\nkmeans_visualizer = Pipeline([\n    (\"preprocessor\", preprocessor),\n    (\"kelbowvisualizer\", KElbowVisualizer(KMeans(),K=(4,12)))])\nkmeans_visualizer.fit(X)\nkmeans_visualizer.named_steps['kelbowvisualizer'].show()","b055e92d":"# Best K in Elbow\nK = kmeans_visualizer.named_steps['kelbowvisualizer'].elbow_value_\n\n# Silhouette Visualizer\nsilhouette_visualizer = Pipeline([\n    (\"preprocessor\", preprocessor),\n    (\"silhouettevisualizer\", SilhouetteVisualizer(KMeans(K)))])\nsilhouette_visualizer.fit(X)\nsilhouette_visualizer.named_steps['silhouettevisualizer'].show()","0641879a":"# Intercluster distance Map with best k\ndistance_visualizer = Pipeline([\n    (\"preprocessor\", preprocessor),\n    (\"distancevisualizer\", InterclusterDistance(KMeans(K)))])\ndistance_visualizer.fit(X)\ndistance_visualizer.named_steps['distancevisualizer'].show()","e50b39e7":"# KMeans Pipeline with best K\nkmeans_model = Pipeline([(\"preprocessor\", preprocessor),\n                         (\"kmeans\", KMeans(K))])\nkmeans_model.fit(X)\n\n# Kmeans labels\nkmeans_labels = kmeans_model.named_steps['kmeans'].labels_\ndata[\"kmeans_label\"] = kmeans_labels","f33b4a00":"kmeans_clusters_means = data.groupby(\"kmeans_label\").mean().reset_index()\nkmeans_clusters_means","7a5aa31f":"X_scaled = preprocessor.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled, index=X.index, columns=X.columns)\nX_scaled[\"kmeans_label\"] = kmeans_labels\nX_scaled_clusters = X_scaled.groupby(\"kmeans_label\").mean()\nX_scaled_clusters","12108f2e":"def plot_radars(data, group):\n\n    scaler = MinMaxScaler()\n    data = pd.DataFrame(scaler.fit_transform(data), \n                        index=data.index,\n                        columns=data.columns).reset_index()\n    \n    fig = go.Figure()\n\n    for k in data[group]:\n        fig.add_trace(go.Scatterpolar(\n            r=data[data[group]==k].iloc[:,1:].values.reshape(-1),\n            theta=data.columns[1:],\n            fill='toself',\n            name='Cluster '+str(k)\n        ))\n\n    fig.update_layout(\n        polar=dict(\n        radialaxis=dict(\n          visible=True,\n          range=[0, 1]\n        )),\n        showlegend=True,\n        title={\n            'text': \"Comparaison des moyennes par variable des clusters\",\n            'y':0.95,\n            'x':0.5,\n            'xanchor': 'center',\n            'yanchor': 'top'},\n        title_font_color=\"blue\",\n        title_font_size=18)\n\n    fig.show()","349fc779":"plot_radars(data=X_scaled_clusters,\n            group=\"kmeans_label\")","3cbe9ff1":"categories_col = ['books_cds_media',\n                  'fashion_clothing_accessories',\n                  'flowers_gifts',\n                  'groceries_food_drink',\n                  'health_beauty',\n                  'home_furniture',\n                  'other',\n                  'sport',\n                  'technology',\n                  'toys_baby']\nX_bis = X.drop(categories_col, axis=1)\nX_bis.head(3)","75e8a241":"def clustering_eval(preprocessor, model, data, metric, elbow=True, mds=False, KBest=None):\n    \n    if((elbow==True) & (mds==True)):\n        ncols=3\n    elif((elbow==False) | (mds==False)):\n        ncols=2\n    else:\n        ncols=1\n        \n    fig, axes = plt.subplots(nrows=1, ncols=ncols, sharex=False, sharey=False, figsize=(24,8))\n    \n    ax=0\n    if(elbow==True):\n        # Elbow visualizer\n        kmeans_visualizer = Pipeline([\n            (\"preprocessor\", preprocessor),\n            (\"kelbowvisualizer\", KElbowVisualizer(model,K=(4,12), metric=metric, ax=axes[ax]))])\n        kmeans_visualizer.fit(data)\n        KBest = kmeans_visualizer.named_steps['kelbowvisualizer'].elbow_value_\n        kmeans_visualizer.named_steps['kelbowvisualizer'].finalize()\n        ax+=1\n    \n    # Set best K\n    K = KBest\n    model.set_params(n_clusters=K)\n\n    # Silhouette Visualizer\n    silhouette_visualizer = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"silhouettevisualizer\", SilhouetteVisualizer(model, ax=axes[ax]))])\n    silhouette_visualizer.fit(data)\n    silhouette_visualizer.named_steps['silhouettevisualizer'].finalize()\n    ax+=1\n    \n    # Intercluster distance Map with best k\n    if(mds==True):\n        distance_visualizer = Pipeline([\n            (\"preprocessor\", preprocessor),\n            (\"distancevisualizer\", InterclusterDistance(model, ax=axes[ax]))])\n        distance_visualizer.fit(data)\n        distance_visualizer.named_steps['distancevisualizer'].finalize()\n    \n    return K\n    plt.show()","fcce6f35":"K = clustering_eval(preprocessor=MinMaxScaler(), \n                model=KMeans(), \n                data=X_bis, \n                metric=\"distortion\",\n                elbow=True,\n                mds=False,\n                KBest=None)","27a00b78":"# KMeans Pipeline with best K\nkmeans_model_bis = Pipeline([(\"preprocessor\", MinMaxScaler()),\n                             (\"kmeans\", KMeans(K))])\nkmeans_model_bis.fit(X_bis)\n\n# Kmeans labels\nkmeans_labels_bis = kmeans_model_bis.named_steps['kmeans'].labels_\n\n# Scale X\nscaler = MinMaxScaler()\nX_scaled_bis = scaler.fit_transform(X_bis)\nX_scaled_bis = pd.DataFrame(X_scaled_bis, index=X_bis.index, columns=X_bis.columns)\nX_scaled_bis[\"kmeans_label\"] = kmeans_labels_bis\n\n# Group by cluster\nX_scaled_clusters_bis = X_scaled_bis.groupby(\"kmeans_label\").mean()\n\n# Plot Radar chart\nplot_radars(data=X_scaled_clusters_bis,\n            group=\"kmeans_label\")","db2547f3":"# PCA Pipeline\npca = Pipeline([(\"preprocessor\", MinMaxScaler()),\n                (\"pca\", PCA(svd_solver='full'))])\npca.fit(X_bis)\nX_projected = pca.transform(X_bis)","00908519":"# Explained variance\nvarexpl = pca.named_steps['pca'].explained_variance_ratio_*100\n\n# Plot of cumulated variance\nplt.figure(figsize=(12,8))\nplt.bar(np.arange(len(varexpl))+1, varexpl)\n\ncumSumVar = varexpl.cumsum()\nplt.plot(np.arange(len(varexpl))+1, cumSumVar,c=\"red\",marker='o')\nplt.axhline(y=95, linestyle=\"--\", \n            color=\"green\",\n            linewidth=1)\n\nlimit = 95\nvalid_idx = np.where(cumSumVar >= limit)[0]\nmin_plans = valid_idx[cumSumVar[valid_idx].argmin()]+1\nplt.axvline(x=min_plans, linestyle=\"--\", \n            color=\"green\",\n            linewidth=1)\n\nplt.xlabel(\"rang de l'axe d'inertie\")\nplt.xticks(np.arange(len(varexpl))+1)\nplt.ylabel(\"pourcentage d'inertie\")\nplt.title(\"{}% de la variance totale est expliqu\u00e9e\"\\\n          \" par les {} premiers axes\".format(limit,\n                                            min_plans))\nplt.show(block=False)","b2415438":"def cerle_corr(pcs, n_comp, pca, axis_ranks, \n               labels=None, label_rotation=0):\n    \n    fig=plt.figure(figsize=(20,n_comp*5))\n    count=1\n    for d1, d2 in axis_ranks:\n        if d2 < n_comp:\n            \n            # initialisation de la figure\n            #fig.subplots_adjust(left=0.1,right=0.9,bottom=0.1,top=0.9)\n            ax=plt.subplot(int(n_comp\/2),2,count)\n            ax.set_aspect('equal', adjustable='box') \n            \n            #d\u00e9termination des limites du graphique\n            ax.set_xlim(-1,1) \n            ax.set_ylim(-1,1) \n\n            #affichage des fl\u00e8ches \n            ax.quiver(np.zeros(pcs.shape[1]), np.zeros(pcs.shape[1]),\n                       pcs[d1,:],pcs[d2,:], \n                       angles='xy', scale_units='xy', scale=1, \n                       color=\"grey\", alpha=0.5)\n            # et noms de variables\n            for i,(x,y) in enumerate(pcs[[d1,d2]].T):\n                ax.annotate(labels[i],(x,y),\n                             ha='center', va='center',\n                             fontsize='14',color=\"#17aafa\", alpha=0.8) \n\n            #ajouter les axes \n            ax.plot([-1,1],[0,0],linewidth=1, color='grey', ls='--') \n            ax.plot([0,0],[-1,1],linewidth=1, color='grey', ls='--')\n\n            #ajouter un cercle \n            cercle = plt.Circle((0,0),1,color='#17aafa',fill=False) \n            ax.add_artist(cercle) \n\n            # nom des axes, avec le pourcentage d'inertie expliqu\u00e9\n            ax.set_xlabel('F{} ({}%)'.format(d1+1, \n                                          round(100*pca.explained_variance_ratio_[d1],1)))\n            ax.set_ylabel('F{} ({}%)'.format(d2+1, \n                                          round(100*pca.explained_variance_ratio_[d2],1)))\n\n            ax.set_title(\"Cercle des corr\u00e9lations (F{} et F{})\".format(d1+1, d2+1))\n            count+=1\n            \n    plt.suptitle(\"Cercles des corr\u00e9lations sur les {} premiers axes\".format(n_comp),\n                 y=.9, color=\"blue\", fontsize=18)        \n    plt.show(block=False)","337ed8bf":"# Principal component space\npcs = pca.named_steps['pca'].components_\n\n# Plot correlation circle\ncerle_corr(pcs,\n           6,\n           pca.named_steps['pca'],\n           [(0,1),(2,3),(4,5)],\n           labels = np.array(X_bis.columns))","a94b5ea8":"# KMeans Pipeline with best K for PCA results\nkmeans_model_pca = Pipeline([(\"preprocessor\", MinMaxScaler()),\n                             (\"kmeans\", KMeans(K))])\nkmeans_model_pca.fit(X_projected[:,:4])\n\n# Kmeans labels\npca_kmeans_labels = kmeans_model_pca.named_steps['kmeans'].labels_\nX_scaled_bis[\"kmeans_label_pca\"] = pca_kmeans_labels\nX_scaled_clusters_pca = X_scaled_bis.groupby(\"kmeans_label_pca\").mean()\nX_scaled_clusters_pca.iloc[:,:-1]","266f1ef2":"plot_radars(data=X_scaled_clusters_pca.iloc[:,:-1],\n            group=\"kmeans_label_pca\")","18aa3be0":"clustering_eval(preprocessor=MinMaxScaler(), \n                model=KMeans(), \n                data=X_projected[:,:4], \n                metric=\"distortion\",\n                elbow=False,\n                mds=True,\n                KBest=K)","e1b72cad":"labels = pca_kmeans_labels\npca_data = X_projected[:,:4]","49874361":"def bench_k_means(model, name, data, labels):\n    t0 = time()\n    estimator = make_pipeline(MinMaxScaler(), model).fit(data)\n    fit_time = time() - t0\n    results = [name, fit_time, estimator[1].inertia_]\n    \n    # Test differents metrics on pred labels\n    clustering_metrics = [\n        homogeneity_score,\n        adjusted_rand_score,\n        adjusted_mutual_info_score]\n    results += [m(labels, estimator[1].labels_) for m in clustering_metrics]\n    \n    # Show the results\n    formatter_result = (\"{:9s}\\t{:.3f}s\\t{:.0f}\\t{:.3f}\"\n                        \"\\t{:.3f}\\t{:.3f}\")\n    print(formatter_result.format(*results))","c3ac8f4f":"print(\"Scores de stabilit\u00e9 \u00e0 l'initialisation\")\nprint(53 * '_')\nprint('Iteration\\tFitTime\\tInertia\\tHomo\\tARI\\tAMI')\nprint(53 * '_')\n\nfor i in range(10):\n    imodel = KMeans(n_clusters=K, n_init=1, init=\"k-means++\")\n    bench_k_means(model = imodel, name=\"Iter \"+str(i), \n                  data=pca_data, labels=labels)\n\nprint(53 * '_')","af7090ce":"X_ter = X.iloc[:,:-1]\nX_ter[\"favorite_sale_month\"] = pd.to_datetime(X_ter[\"favorite_sale_month\"], format='%m').dt.month_name()\nX_ter[\"mean_payment_sequential\"] = np.where(X_ter[\"mean_payment_sequential\"] > 1, \"Multiple\", \"Single\")\nX_ter[\"mean_payment_installments\"] = np.where(X_ter[\"mean_payment_installments\"] > 1, \"Multiple\", \"Single\")\nX_ter.head(3)","c3884642":"for c in X_ter.select_dtypes(exclude=\"object\").columns:\n    scaler = MinMaxScaler()\n    X_ter[c] = scaler.fit_transform(np.array(X_ter[c]).reshape(-1, 1))","43b42833":"# Categorical features index\ncat_cols = [X_ter.columns.get_loc(c) for c in X_ter.select_dtypes(include=\"object\").columns]","f2de68eb":"kproto = KPrototypes(n_clusters= 6, init='Cao')\nkproto_labels = kproto.fit_predict(X_ter, categorical=cat_cols)","9d06974d":"X_ter[\"kproto_labels\"] = kproto_labels\nX_ter.groupby(\"kproto_labels\").mean()","3115b3a2":"# Initial period of 12 months\ndata_init = fct.make_dataset(dpath=\"..\/input\/brazilian-ecommerce\/\", initial=True, period=2)","50f18ba6":"# Remove categories\ndata_init.drop(categories_col, axis=1, inplace=True)\ndata_init.head(3)","afa6ab1e":"olist_path = \"..\/input\/brazilian-ecommerce\/\"\norders = pd.read_csv(olist_path+\"olist_orders_dataset.csv\")\norders = orders.loc[orders.order_status == \"delivered\", \n                    \"order_purchase_timestamp\"]\norders = pd.to_datetime(orders)\ntime_delta = int((orders.max() - orders.min())\/np.timedelta64(1,'M'))\nprint(\"La p\u00e9riode compl\u00e8te des commandes porte sur {} mois.\".format(time_delta))","72fafd9b":"# Kmeans on initial period\nkmeans_init = Pipeline([(\"preprocessor\", MinMaxScaler()),\n                        (\"kmeans\", KMeans(K, random_state=42))])\nkmeans_init.fit(data_init)\ninit_labels = kmeans_init.named_steps['kmeans'].labels_","fb4615a8":"ari_score = []\nfor p in np.arange(2,(time_delta-12),2):\n    # Create dataset for period\n    data_period = fct.make_dataset(dpath=\"..\/input\/brazilian-ecommerce\/\", \n                                   initial=False, \n                                   period=p)\n    data_period.drop(categories_col, axis=1, inplace=True)\n    \n    # Filter on initial customer client\n    data_period = data_period[data_period.index.isin(data_init.index)]\n    \n    # K-Means\n    kmeans_p = Pipeline([(\"preprocessor\", MinMaxScaler()),\n                         (\"kmeans\", KMeans(K, random_state=42))])\n    kmeans_p.fit(data_period)\n    p_labels = kmeans_p.named_steps['kmeans'].labels_\n    \n    # Calculate ARI score\n    ari_p = adjusted_rand_score(init_labels, kmeans_p[1].labels_)\n    ari_score.append([p,ari_p])","4599e3ae":"ari_score = pd.DataFrame(ari_score, columns=[\"periode\", \"ARI\"])\n\n# plot ARI Score\nfig = plt.figure(figsize=(12,8))\nsns.lineplot(data=ari_score, x=\"periode\", y=\"ARI\")\nplt.axhline(y=ari_score.iloc[2][1], linestyle=\"--\", \n            color=\"green\",\n            xmax=0.5,\n            linewidth=1)\nplt.axvline(x=ari_score.iloc[2][0], linestyle=\"--\", \n            color=\"green\",\n            ymax=1.1-(ari_score.iloc[2][1]),\n            linewidth=1)\nplt.xlabel(\"P\u00e9riode (mois)\")\nplt.ylabel(\"Score ARI\")\nplt.title(\"Stabilit\u00e9 temporelle de la segmentation par K-Means\",\n          fontsize=18,\n          color=\"b\")\nplt.show()","0c27274f":"![k_cost](http:\/\/www.mf-data-science.fr\/images\/projects\/cost_k_prototype.jpg)","b0832732":"# <font color=\"#476bff\" id=\"section_6\">Stabilit\u00e9 temporelle de la segmentation<\/font>\nDans le but d'\u00e9tablir un contrat de maintenance de l'algorithme de segmentation client, nous devons tester sa stabilit\u00e9 dans le temps et voir, par exemple, \u00e0 quel moment les clients changent de Cluster.\n\nPour cela, nous devons recalculer toutes les features en fonction d'une p\u00e9riode donn\u00e9e. Le script r\u00e9alisant ces calculs est disponible dans le module annexe `olist_functions`","05a95c05":"Malheureusement ici, la segmentation se base principalement sur les cat\u00e9gories de produit achet\u00e9es. **Le poids de ces features masque les autres axes de cat\u00e9gorisation**, nous allons donc r\u00e9aliser un nouveau K-Means en supprimant ces variables *(nous pourrons ensuite ajouter la valeur la plus fr\u00e9quente pour chaque groupe)*\n\n### Clustering sans les cat\u00e9gories produits","5329c14a":"Il faut donc conserver 5 axes principaux pour expliquer la variance \u00e0 95%.\n\n### Cercle des corr\u00e9lations","459bde69":"![metrics_compare](http:\/\/www.mf-data-science.fr\/images\/projects\/metrics_compare.jpg)","7c9c1c1d":"### Eboulis des valeurs propres","5f3ca37e":"### Interpr\u00e9tation m\u00e9tier des clusters\n\n- **Groupe 1** : Clients proches g\u00e9ographiquement avec de courts d\u00e9lais de livraison, commandant principalement en d\u00e9but d'ann\u00e9e pour des montants faibles. Ils paient avec 1 type de moyen de paiement et avec un nombre faible d'\u00e9ch\u00e9ances. Les avis de ces clients sont tr\u00e8s bons.\n- **Groupe 2** : Clients de fin d'ann\u00e9e. Ils r\u00e8glent avec plusieurs moyens de paiement pour des montants faibles. Ces clients sont g\u00e9ographiquement peu \u00e9loign\u00e9s et les d\u00e9lais de livraison sont courts. Les avis de ces clients sont \u00e9galement tr\u00e8s bons.\n- **Groupe 3** : Regroupe les clients qui utilisent plusieurs moyens de paiement et un nombre important d'\u00e9ch\u00e9ances. Ils ont tendence \u00e0 espacer les d\u00e9lais entre 2 commandes. Les avis de ces clients sont \u00e9galement tr\u00e8s bons.\n- **Groupe 4** : Ce sont des clients m\u00e9contants (les avis sont mauvais). Les d\u00e9lais de livraison sont tr\u00e8s importants et les frais de port \u00e9lev\u00e9s en raison de l'\u00e9loignement g\u00e9ographique. Ce sont cependant les clients qui ont le plus d\u00e9pens\u00e9 et ont achet\u00e9 un grand nombre d'articles. En revanche, le nombre de commandes pass\u00e9es sur le site est moyen.\n- **Groupe 5** : Ces clients ont pass\u00e9 un nombre important de commandes et sont satisfaits. ils paient comptant pour un montant moyen de commande. Ils sont g\u00e9ographiquement proches.\n\nNous allons \u00e0 pr\u00e9sent r\u00e9aliser une r\u00e9duction dimensionnelle pour v\u00e9rifier si le clustering est r\u00e9alisable sur un nombre r\u00e9duit de variables sans perturber les groupes.\n\n# <font color=\"#476bff\" id=\"section_2\">R\u00e9duction dimensionnelle - PCA<\/font>\nNous allons r\u00e9aliser une Analyse en Composantes Principales pour v\u00e9rifier s'il est possible de cat\u00e9goriser nos clients avec moins de variables. L'objectif \u00e9tant de conserver une inertie de 95%.","e2037d6f":"Gr\u00e2ce \u00e0 la m\u00e9thode du coude bas\u00e9e sur le score de distortion *(somme moyenne des carr\u00e9s des distances aux centres)*, une segementation en $\\large K = 6$ clusters serait la meilleure option.\n\n### Coefficient de silhouette\nPour v\u00e9rifier si ce clustering fonctionne, nous allons utiliser `SilhouetteVisualizer` pour afficher le **coefficient de silhouette pour un \u00e9chantillonage de chaque cluster**. Cela permet de visualiser la densit\u00e9 et la s\u00e9paration des clusters. ","daa482ee":"# <font color=\"#476bff\" id=\"section_1\">Clustering avec l'algorithme du K-Means<\/font>\n\n<details>\n  <summary style=\"color:#1ade9e; font-weight:bold;\">Explication de l'algorithme<\/summary>\n  \n  ## K-Means\n  C\u2019est l\u2019un des algorithmes de clustering les plus r\u00e9pandus. Il permet d\u2019analyser un jeu de donn\u00e9es caract\u00e9ris\u00e9es par un ensemble de descripteurs, afin de regrouper les donn\u00e9es \u201csimilaires\u201d en groupes *(ou clusters)*.\n    \n   Apr\u00e8s avoir initialis\u00e9 des centro\u00efdes en prenant des donn\u00e9es au hasard dans le jeu de donn\u00e9es, K-means alterne plusieurs fois ces deux \u00e9tapes pour optimiser les centro\u00efdes et leurs groupes :\n\n        1. Regrouper chaque objet autour du centro\u00efde le plus proche.\n        2. Replacer chaque centro\u00efde selon la moyenne des descripteurs de son groupe.\n    \n    L\u2019algorithme a converg\u00e9 apr\u00e8s un certain nombre d'it\u00e9rations et trouve un d\u00e9coupage stable du jeu de donn\u00e9es.\n    \n    Cependant, le nombre de Clusters est \u00e0 d\u00e9finir par l'utilisateur.\n  \n  ## M\u00e9thode du coude\n    KElbowVisualizer de Yellowbrick impl\u00e9mente la m\u00e9thode du \u00abcoude\u00bb pour s\u00e9lectionner le nombre optimal de clusters en ajustant le mod\u00e8le K-Means avec une plage de valeurs pour K. Si le graphique en courbes ressemble \u00e0 un bras, alors le \u00abcoude\u00bb (le point d'inflexion sur la courbe) est une bonne indication du nombre de K optimal.\n<\/details>","955409d0":"Puis on initialise l'algorithme :","035e267a":"Nous allons dans un premier temps grouper les individus par cluster pour analyser les moyennes :","abbffbab":"Sur ce plot des scores ARI obtenus sur les it\u00e9rations par p\u00e9riode de 2 mois, on remarque une forte inflexion apr\u00e8s 6 mois sur les clients initiaux.\n\nIl faudra donc **pr\u00e9voir la maintenance du programme de segmentation tous les 6 mois** dans un premier temps puis re-tester cette stabilit\u00e9 temporelle au fil du temps afin de l'affiner. Il sera donc n\u00e9cessaire de red\u00e9finir les segments clients \u00e0 chaque maintenance.","7c249cc7":"### Distances intercluster","86db1a64":"Pour d\u00e9terminer le moment o\u00f9 les clients changent de cluster, nous allons **it\u00e9rer le K-Means sur toute la p\u00e9riode avec des deltas de 2 mois et calculer le score ARI**, en prenant garde \u00e0 bien comparer les m\u00eames clients *(ceux des 12 mois initiaux)*. \n\n*Ici le d\u00e9roul\u00e9 est cr\u00e9\u00e9 directement dans le notebook pour explication, en production, toutes ses op\u00e9rations seront regroup\u00e9es dans une classe Python pour automatiser le processus.*","7def44bd":"On remarque donc ici que **la r\u00e9duction de dimenssion offre les m\u00eames axes de segmentation**. Il est donc possible de r\u00e9duire le nombre de features en entr\u00e9e et d'utiliser les variables synth\u00e9tiques de la PCA pour segmenter notre fichier client, ce d'autant que **les scores silhouette sont ici meilleurs compar\u00e9s au donn\u00e9es brutes**.","1a439b38":"```Python\n# Elbow method with differents metrics\nmetrics = [\"silhouette\", \"calinski_harabasz\"]\ni = 0\n\nfig, axes = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(24,8))\nfor m in metrics:\n    kmeans_visualizer = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"kelbowvisualizer\", KElbowVisualizer(KMeans(),\n                                              K=(4,10),\n                                              metric=m,\n                                              ax=axes[i]))])\n    kmeans_visualizer.fit(X)\n    kmeans_visualizer.named_steps['kelbowvisualizer'].finalize()\n    i+=1\n    \nplt.show()\n```","ef729a42":"Les diff\u00e9rentes it\u00e9rations montrent des inerties proches, une bonne homog\u00e9n\u00e9it\u00e9 et un score AMI proche de 1. Nous pouvons donc en d\u00e9duire que **la stabilit\u00e9 \u00e0 l'initialisation du mod\u00e8le K-Means est bonne**.","68b6c7d3":"![banniere](http:\/\/mf-data-science.fr\/images\/projects\/banniere.jpg)","1c1f9c22":"Sur cette projection en 2D, on remarque que les diff\u00e9rents clusters sont bien s\u00e9par\u00e9s sur les 2 premi\u00e8res composantes principales. Le clustering semble donc performant et il faut a pr\u00e9sent **identifier les composantes m\u00e9tier de chaque cluster**.\n\n### Analyse des diff\u00e9rents clusters\n\nA pr\u00e9sent, nous allons entrainer notre KMeans avec le K optimal s\u00e9lectionn\u00e9 et affecter son cluster \u00e0 chaque client. Ainsi, nous pourrons analyser les diff\u00e9rences entre chaque cluster :","c90ba324":"On peut ainsi voir parfaitement les variables qui contribuent le plus \u00e0 chaque axe. Par exemple, la variable synth\u00e9tique $\\large F_2$ repr\u00e9sentera les p\u00e9riodes d'achats. La variable synth\u00e9tique $\\large F_4$ quant \u00e0 elle repr\u00e9sente l'aspect g\u00e9ographique.\n\nNous allons donc int\u00e9grer \u00e0 notre pipeline Kmeans une PCA sur 5 composantes pour **v\u00e9rifier si la r\u00e9duction dimensionnelle r\u00e9duit la qualit\u00e9 de la segmentation** :","02609c15":"# <font color=\"#476bff\" id=\"section_5\">Clustering avec l'algorithme du K-Prototypes<\/font>\n\n<details>\n  <summary style=\"color:#1ade9e; font-weight:bold;\">Explication de l'algorithme<\/summary>\n  \n  ## K-Prototypes\n  Le K-Prototype est l'algorithme de clustering qui est la combinaison de K-Means et K-Mode. Dans l'algorithme du K-modes, la distance est mesur\u00e9e par le nombre d'attributs cat\u00e9goriels communs partag\u00e9s par les deux points de donn\u00e9es.\n    K-Prototypes mesure la distance entre les variables num\u00e9riques \u00e0 l'aide de la distance euclidienne (comme le K-Means), mais mesure \u00e9galement la distance entre les entit\u00e9s cat\u00e9gorielles en utilisant le nombre de cat\u00e9gories correspondantes. \n    \n    Les principales \u00e9tapes sont les suivantes :\n        1. S\u00e9lection k prototypes initiaux dans l'ensemble de donn\u00e9es X. Il doit y en avoir un pour chaque cluster.\n        2. Allouer chaque objet de X \u00e0 un cluster dont le prototype est le plus proche de lui. Cette allocation se fait en consid\u00e9rant la mesure de dissimilarit\u00e9.\n        3. Une fois tout les objets allou\u00e9s \u00e0 un cluster, tester \u00e0 nouveau la similitude des objets par rapport aux prototypes actuels. Si un individu est plus proche d'un autre cluster, les prototypes des deux clusters sont mis \u00e0 jour.\n<\/details>","3837716c":"On remarque ici que pour la m\u00e9trique silhouette score, le nombre de K est \u00e9galement de 6. Pour le score Calinski Harabasz, le meilleur K est plus incertain. Les scores sur la r\u00e9partition en 6 clusters semblent \u00eatre meilleurs. Nous allons donc **conserver ce clustering en 6 groupes**.","5d145df2":"# <font color=\"#476bff\" id=\"section_4\">Stabilit\u00e9 \u00e0 l'initialisation du K-Means<\/font>\n\nNotre algorithme du K-Means test\u00e9 ici est initialis\u00e9 avec la m\u00e9thode K-Means++, ce qui r\u00e9duit d\u00e9j\u00e0 les effets al\u00e9atoires de l'initialisation des centro\u00efdes. Cependant, vous allons **tester la stabilit\u00e9 \u00e0 l'initialisation en entrainant plusieurs fois le mod\u00e8le sans fixer le RandomState**.\n\nPour les labels initiaux, nous allons conserver les clusters calcul\u00e9s dans le dernier mod\u00e8le *(avec PCA)* puis les comparer gr\u00e2ce \u00e0 l'**indice de Rand ajust\u00e9** *(ARI)* pour chacune des it\u00e9rations.","9b96c71f":"Pour cet algorithme traitant les donn\u00e9es mixtes, nous allons convertir quelques variables en variables cat\u00e9gorielles *(comme les mois d'achat par exemple)* pour leur donner plus de sens.","02854803":"# <font color=\"#476bff\" id=\"section_3\">K-Means apr\u00e8s r\u00e9duction de dimenssions<\/font>","a257c473":"```Python\n#Choosing optimal K\ncost = []\nfor num_clusters in list(range(4,10)):\n    kproto = KPrototypes(n_clusters=num_clusters, init='Cao')\n    kproto.fit_predict(X_ter, categorical=cat_cols)\n    cost.append(kproto.cost_)\n\nplt.figure(figsize=(12,8))\nplt.plot(cost)\nplt.xticks(np.arange(0,6,1), np.arange(4,10,1))\nplt.axvline(x=2, linestyle=\"--\", \n            color=\"green\",\n            linewidth=1)\nplt.xlabel(\"K\")\nplt.ylabel(\"Cost\")\nplt.title(\"Cost of K-Prototypes for K between 4 and 9\",\n          fontsize=18, color=\"b\")\nplt.show()\n```","9da3f43c":"Le score de chaque \u00e9chantillon est calcul\u00e9 en faisant la moyenne du coefficient de silhouette (diff\u00e9rence entre la distance moyenne intra-cluster et la distance moyenne du cluster le plus proche pour chaque \u00e9chantillon), normalis\u00e9e par la valeur maximale. Cela nous donne un score entre -1 et 1, qui nous permet de d\u00e9terminer si la s\u00e9paration est efficace ou si les points sont assign\u00e9s au mauvais cluster.\n\nIci, **les clusters semblent relativement bien r\u00e9partis et les s\u00e9parations sont claires** avec cependant quelques erreurs sur l'un des clusters.\n\nA pr\u00e9sent, nous allons **tester d'autres types de m\u00e9triques** pour trouver le meilleur K :\n- Silouhette : *rapport moyen entre la distance intra-cluster et la distance du cluster le plus proche*,\n- Calinski Harabasz : *rapport entre la dispersion des grappes dans et entre les groupes*.","e5e3c159":"On remarque cette fois encore que les r\u00e9sultats obtenus sont similaires au K-Means, les cat\u00e9gories produits l'emportent sur les autres variables. Le K-Prototypes ayant un temps d'entrainement et de pr\u00e9diction plus long, **nous conserverons donc la segmentation avec le mod\u00e8le K-Means**.","29f7e649":"Projetons \u00e0 pr\u00e9sent ces diff\u00e9rentes moyennes sur un Radar plot avec la librairie Ploty pour visualiser les diff\u00e9rences entre cluster :","3d0c535d":"On prepare ensuite le preprocessor :","2668602b":"<div style=\"font-size:32px; text-align:center; color:#2941ec;\"><b>Segmentation des clients du site E-Commerce Olist<\/b><\/div>\n    \n<hr style=\"text-align:center; width:50%;\" \/>\nOlist souhaite obtenir une segmentation de ses clients utilisable au quotidien par leur \u00e9quipe marketing dans leurs campagnes de communication.\n\nL'objectif est de comprendre les diff\u00e9rents types d'utilisateurs gr\u00e2ce \u00e0 leur comportement et \u00e0 leurs **donn\u00e9es personnelles anonymis\u00e9es**.\n\nNous allons donc fournir ici une description actionable de la segmentation et de sa logique sous-jacente pour une utilisation optimale. Nous devrons \u00e9galement fournir une analyse de la stabilit\u00e9 des segments au cours du temps *(dans le but d'\u00e9tablir un contrat de maintenance)*.\n\nNous utilisererons donc des **m\u00e9thodes non supervis\u00e9es** pour regrouper ensemble des clients de profils similaires.\n<hr style=\"text-align:center; width:50%\"\/>\n\n# <font color=\"#476bff\">Sommaire<\/font>\n1. [Clustering avec l'algorithme du K-Means](#section_1)\n2. [R\u00e9duction dimensionnelle - PCA](#section_2)\n3. [K-Means apr\u00e8s r\u00e9duction de dimenssions](#section_3)\n4. [Stabilit\u00e9 \u00e0 l'initialisation du K-Means](#section_4)\n5. [Clustering avec l'algorithme du K-Prototypes](#section_5)\n5. [Stabilit\u00e9 temporelle de la segmentation](#section_6)","8a552da7":"Pour l'algorithme K-Prototypes, nous n'avons pas besoin d'encoder les variables cat\u00e9gorielles. Nous allons donc normliser uniquement les varaibles num\u00e9riques :","10486c14":"Nous allons supprimer `customer_zip_code_prefix` et `customer_city`, `customer_state` pour ne conserver que la variable `haversine_distance` pour indiquer une localisation \"large\" des clients, ce qui \u00e9vitera de focaliser la segmentation sur la localisation du client.\n\nNous allons \u00e9galement supprimer `mean_price_order` et `mean_nb_items` puisque nous avons vu dans le [Notebook de nettoyage et exploration Olist](https:\/\/www.kaggle.com\/michaelfumery\/e-commerce-cleaning-and-eda) que ces variables sont fortement corr\u00e9l\u00e9es aux totaux pour le moment.","105573b8":"### M\u00e9thode du coude : d\u00e9termination du meilleur K\nUn pipeline SKLearn est cr\u00e9\u00e9 pour y inclure le preprocessing et la visualisation de la m\u00e9thode du coude de notre KMeans.","b60ddd31":"Nous allons ensuite d\u00e9terminer la p\u00e9riode totale \u00e0 couvrir :","7dfebd5a":"### Preprocessing"}}