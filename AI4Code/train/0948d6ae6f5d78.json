{"cell_type":{"8feb02ca":"code","dbed4c74":"code","58fec134":"code","b3ee8f71":"code","b66b035a":"code","aff5d5e2":"code","88ae52f3":"code","bf0a1861":"code","5d198997":"code","67cbae9b":"code","1d1b716d":"code","f99526a1":"code","79637fa7":"code","f94ebb4b":"code","6d5ca128":"code","24cc1881":"code","3293514f":"code","6d7d66c7":"markdown","0f63c3c4":"markdown","76733563":"markdown","928f9e99":"markdown","cb6ca279":"markdown","3acbe14f":"markdown","70cde219":"markdown","265edcf8":"markdown","c50e930f":"markdown"},"source":{"8feb02ca":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n# from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport copy\nimport operator\nimport datetime\nimport warnings\nfrom itertools import product\nwarnings.filterwarnings(\"ignore\")\n\n#useful way to visualize directory\n!ls ..\/input\n!ls ..\/input\/*","dbed4c74":"TRAIN_SIZE = 100000\n#we'll not be using the full training set, change this parameter if you wish to use more\/less\ntrain = pd.read_csv(\"..\/input\/food-com-recipes-and-user-interactions\/interactions_train.csv\")\nvalidation = pd.read_csv(\"..\/input\/food-com-recipes-and-user-interactions\/interactions_validation.csv\")\nrecipes_RAW = pd.read_csv(\"..\/input\/food-com-recipes-and-user-interactions\/RAW_recipes.csv\")\nrecipes_RAW = recipes_RAW.rename({\"id\":\"recipe_id\"},axis=1)\ninteractions_RAW = pd.read_csv(\"..\/input\/food-com-recipes-and-user-interactions\/RAW_interactions.csv\")\n\n#since we'll be using k-fold validation, there is not need to use a seperate validation set\ntrain = pd.concat([train,validation],axis=0)\ntest = pd.read_csv(\"..\/input\/food-com-recipes-and-user-interactions\/interactions_test.csv\")\ntrain = train.iloc[:TRAIN_SIZE,:]","58fec134":"train = train.merge(interactions_RAW.drop([\"rating\",\"date\"],axis=1),on=[\"user_id\",\"recipe_id\"])\ntrain = train.merge(recipes_RAW,on=\"recipe_id\")\ntest = test.merge(interactions_RAW.drop([\"rating\",\"date\"],axis=1),on=[\"user_id\",\"recipe_id\"])\ntest = test.merge(recipes_RAW,on=\"recipe_id\")\ntrain.head(2)","b3ee8f71":"train_copy = train.copy()","b66b035a":"train[\"date\"] = pd.to_datetime(train[\"date\"])\ntest[\"date\"] = pd.to_datetime(test[\"date\"])\ntrain[\"submitted\"] = pd.to_datetime(train[\"submitted\"])\ntest[\"submitted\"] = pd.to_datetime(test[\"submitted\"])\ntrain.info()","aff5d5e2":"class FeatureTransformer(TransformerMixin, BaseEstimator):\n    def __init__(self, vectorize = True, time=True, drop = True):\n        self.vectorize = vectorize\n        self.drop = drop\n        self.time = time\n        if vectorize:\n            self.vectorizer1 = CountVectorizer(lowercase=True,max_df=0.3,min_df=0.005,max_features=3000) #review\n            self.vectorizer2 = CountVectorizer(lowercase=True,max_df=0.3,min_df=0.005,max_features=3000) #description\n            self.vectorizer3 = CountVectorizer(lowercase=True,max_df=0.3,min_df=0.005,max_features=3000) #steps\n            self.vectorizer4 = CountVectorizer(lowercase=True,max_df=0.8,min_df=0.01,max_features=200)  #tags\n            self.vectorizer5 = CountVectorizer(lowercase=True,max_df=0.8,min_df=0.01,max_features=300)  #ingredients\n        \n    def fit(self, X, y=None):\n        X.loc[X.description.isnull(), \"description\"] = \" \"\n        if self.vectorize:\n            self.vectorizer1.fit(X[\"review\"])\n            self.vectorizer2.fit(X[\"description\"])\n            self.vectorizer3.fit(X[\"steps\"])\n            self.vectorizer4.fit(X[\"tags\"])\n            self.vectorizer5.fit(X[\"ingredients\"])\n        return self\n    \n    def transform(self, X, y=None):\n        processed = [\"nutrition\"]\n        X = copy.deepcopy(X)\n        X = X.drop(columns=[\"u\",\"i\",\"user_id\",\"recipe_id\",\"name\",\"contributor_id\"])\n        \n        X.loc[X.description.isnull(), \"description\"] = \" \"\n        X[\"has_description\"] = 1\n        X.loc[X.description == \" \", \"has_description\"] = 0\n        X[\"calories\"] = X[\"nutrition\"].apply(lambda x: np.array(eval(x)).sum())\n\n        if self.vectorize:\n            processed.append(\"review\")\n            transformed = pd.DataFrame(self.vectorizer1.transform(X[\"review\"]).toarray()).add_prefix(\"review_\")\n            X = pd.concat([X, transformed],axis=1)\n            \n            processed.append(\"description\")\n            transformed = pd.DataFrame(self.vectorizer2.transform(X[\"description\"]).toarray()).add_prefix(\"description_\")\n            X = pd.concat([X, transformed],axis=1)\n            \n            processed.append(\"steps\")\n            #works because CountVectorizer ignores puctuations\n            transformed = pd.DataFrame(self.vectorizer3.transform(X[\"steps\"]).toarray()).add_prefix(\"steps_\")\n            X = pd.concat([X, transformed],axis=1)\n            \n            processed.append(\"tags\")\n            #works because CountVectorizer ignores puctuations\n            transformed = pd.DataFrame(self.vectorizer4.transform(X[\"tags\"]).toarray()).add_prefix(\"tags_\")\n            X = pd.concat([X, transformed],axis=1)\n            \n            processed.append(\"ingredients\")\n            #works because CountVectorizer ignores puctuations\n            transformed = pd.DataFrame(self.vectorizer5.transform(X[\"ingredients\"]).toarray()).add_prefix(\"ingredients_\")\n            X = pd.concat([X, transformed],axis=1)\n            \n        if self.time:\n            processed.extend([\"date\",\"submitted\"])\n            X[\"year\"] = [x.year for x in X[\"date\"]]\n            X[\"month\"] = [x.month for x in X[\"date\"]]\n            X[\"day_of_week\"] = [x.dayofweek for x in X[\"date\"]]\n            \n            X[\"upload_year\"] = [x.year for x in X[\"submitted\"]]\n            \n        if self.drop:\n            X = X.drop(columns=processed)\n        return X","88ae52f3":"now = datetime.datetime.now()\ntransformer = FeatureTransformer()\ntransformer.fit(train)\ntrain = transformer.transform(train)\nend = datetime.datetime.now()\nprint(\"Train Transformation time: {:5f} minutes\".format((end - now).total_seconds() \/ 60))","bf0a1861":"now = datetime.datetime.now()\ntest.loc[test.review.isnull(),\"review\"] = \" \" \n#one review is missing for who knows what reason\ntest = transformer.transform(test)\nend = datetime.datetime.now()\nprint(\"Test Transformation time: {:5f} minutes\".format((end - now).total_seconds() \/ 60))","5d198997":"y_train, X_train = train[\"rating\"], train.iloc[:,1:]\ny_test, X_test = test[\"rating\"], test.iloc[:,1:]\nprint(\"Train shape: {}\\nTest shape:  {}\".format(train.shape, test.shape))","67cbae9b":"class GBDT():\n    def __init__(self, args = {}, random_state=42, verbose=0):\n        self.random_state = random_state\n        self.args = args\n        self.verbose = verbose\n        self.models = []\n        self.func = GradientBoostingClassifier\n        \n        self.feature_importances = {}\n    def fit(self, X_train_, y_train_, cv=1, analyze=False):\n        self.models = []\n        classification_train(self,X_train_, y_train_, cv,func=self.func,analyze=analyze)\n            \n    def predict(self, X_test, verbose=0):\n        preds = np.zeros(len(X_test))\n        if verbose!=0: print(\"[\",end=\"\")\n        for model in self.models:\n            if verbose!=0:print(\"-\",end=\"\")\n            preds += model.predict(X_test)\n        if verbose!=0:print(\"]\")\n        preds \/= len(self.models)\n        return [int(round(i)) for i in preds]\n    \n    def evaluate(self, X_test, y_test, return_score=True):\n        preds = self.predict(X_test)\n        acc = accuracy_score(y_test, preds)\n        print(\"Accuracy: {:5f}\".format(acc))\n        if return_score:\n            return acc\n        \n    def copy(self):\n        return self.func(random_state=self.random_state, verbose=self.verbose, **self.args)","1d1b716d":"def classification_train(this, X_train_, y_train_, cv=1, func=None, analyze=False):\n    v = 1 if this.verbose == 2 else 0\n    if cv == 1:\n        this.models.append(func(random_state=this.random_state, verbose=v, **this.args))\n        this.models[0].fit(X_train_,y_train_)\n        preds_train = [round(i) for i in this.models[0].predict(X_train_)]\n        print(\"Train Accuracy: {:5f}\".format(accuracy_score(y_train_, preds_train)))\n        if analyze:\n            this.feature_importances = dict(zip(this.models[0].feature_importances_, X_train_.columns.values))\n            this.feature_importances = sorted(this.feature_importances.items(), key=operator.itemgetter(0),reverse=True)[:20]\n            plt.figure(figsize=(30,10))\n            plt.bar([x[1] for x in this.feature_importances],[x[0] for x in this.feature_importances],color=\"red\")\n            plt.title(\"Feature Importances\")\n            plt.show()\n    else:\n        importances = np.zeros(len(X_train.columns))\n        val_accuracies = []\n        train_accuracies = []\n        size = len(X_train)\n        train_size = 1 - 1 \/ cv\n        \n        for i in range(cv):\n            this.models.append(func(random_state=this.random_state, verbose=v, **this.args))\n            if this.verbose != 0: print(\"{}\/{}\".format(i+1,cv))\n            X_fold, X_val, y_fold, y_val = train_test_split(X_train_, y_train_, train_size=train_size)\n            if this.verbose != 0: print(\"Training on {} instances, Validating on {} instances\".format(len(X_fold),len(X_val)))\n            this.models[i].fit(X_fold,y_fold)\n            val_preds = [round(i) for i in this.models[i].predict(X_val)]\n            val_accuracies.append(accuracy_score(y_val,val_preds))\n            importances += this.models[i].feature_importances_\n            if this.verbose != 0: print(\"Validation Accuracy: {:5f}\\n\".format(accuracy_score(y_val,val_preds)))\n            \n        if analyze:\n            this.feature_importances = dict(zip(importances, X_train.columns.values))\n            this.feature_importances = sorted(this.feature_importances.items(), key=operator.itemgetter(0),reverse=True)\n            print(\"Mean Validation Accuracy: {:5f}\".format(np.mean(val_accuracies)))\n            f,axs = plt.subplots(1, 2,figsize=(30,10))\n            axs = axs.flatten()\n            axs[0].bar([x[1] for x in this.feature_importances],[x[0] for x in this.feature_importances],color=\"red\")\n            axs[0].set_title(\"Feature Importances\")\n            plt.setp(axs[0].xaxis.get_majorticklabels(), rotation=90)\n            \n            axs[1].hist(val_preds)\n            axs[1].set_title(\"Prediction Distribution\")\n            plt.show()","f99526a1":"class GridSearch():\n    def __init__(self, model, params, verbose=0):\n        self.verbose = verbose\n        self.model = model\n        self.best_params = None\n        self.params = params\n        self.features = list(params.keys())\n        self.combinations = list(product(*list(params.values())))\n        \n    def fit(self, X, y, train_size=0.8, cv=1, stratify=True, custom=True):\n        print(\"Total Combinations: \" + str(len(self.combinations)))\n        if cv == 1:\n            accuracies = []\n            highest, count = 0, 0\n            if stratify: X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=train_size,stratify=y,shuffle=True)\n            else: X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=train_size,shuffle=True)\n            start = datetime.datetime.now()\n            for combination in self.combinations:\n                count += 1\n                model = self.model(args=dict(zip(self.features,combination)))\n                model.fit(X_train, y_train)\n                preds = model.predict(X_test)\n                acc = accuracy_score(y_test,preds)\n                accuracies.append(acc)\n                if acc > highest:\n                    self.best_params = dict(zip(self.features,combination))\n                    highest = acc\n                end = datetime.datetime.now()\n                duration = (end-start).total_seconds()\n                if self.verbose != 0: print(\"Train Time: {:5f}\\tTest Accuracy: {:5f}\\t\\tETA: {:5f}s\\t{}\/{}\".format(duration,acc,duration\/count*(len(self.combinations)-count),count,len(self.combinations)))\n        print(\"Search Complete\\nBest params: {}\\tHighest accuracy: {:5f}\".format(str(self.best_params),highest))","79637fa7":"model = GBDT(verbose=2,args={'min_samples_leaf': 2})\nmodel.fit(X_train,y_train,cv=1,analyze=True)\n#change cv to > 1 for cross validation","f94ebb4b":"vocab = transformer.vectorizer1.vocabulary_\nwords = [192, 880, 336, 108, 332, 504, 254, 505, 89, 459, 444, 703, 789, 376, 886, 566, 846, 401, 426, 449]\nfor idx in words:\n    print(list(vocab)[list(vocab.values()).index(idx)],end=\"   \")","6d5ca128":"model.evaluate(X_test,y_test)\nmodel.evaluate(X_train,y_train,return_score=False)","24cc1881":"X_train.year.mean(), X_test.year.mean()","3293514f":"f,axs = plt.subplots(1,2,figsize=(30,10))\naxs = axs.flatten()\naxs[0].hist(X_train.year,bins=X_train.year.nunique())\naxs[0].set_title(\"Train Set\")\naxs[1].hist(X_test.year,bins=X_test.year.nunique())\naxs[1].set_title(\"Test Set\")\nplt.show()","6d7d66c7":"# GradientBoostingClassifier\nIn this kernel, we'll be trying to predict a user's rating. <br>I always find tree-based algorithms really helpful just because the data does not need much preprocessing (no scaling and centering needed)<br>In this section, we are using a [Gradient Boosing Decision Tree](https:\/\/towardsdatascience.com\/machine-learning-part-18-boosting-algorithms-gradient-boosting-in-python-ef5ae6965be4). It is one of the most powerful machine learning models and is frequently used for competitions. I've written a wrappers (and a method) for GDBT. You can look through them if you wish but they are really nothing special, just an implementation of cross-validation and a bit of visualization.","0f63c3c4":"There are nulls in the `description` column, so we'll have to deal with that later. Let's also convert the `date` column to `datetime64` objects","76733563":"Some of the words (\"delicious\", \"great\", \"good\") suggest a positive review while others (\"but\", \"bland\", \"sorry\") suggest a negative review","928f9e99":"# Loading the Data\nWe will not use `PP_recipies.csv` and `PP_users.csv` but instead will vectorize the data ourselves. The index columns of this dataset is kind of confusing since there are two columns (`u` and `user_id`) for user ids and two columns (`i` and `recipe_id`) for recipe ids.","cb6ca279":"# Loading Neccessary Libraries","3acbe14f":"# Conclusion\nThere is a 15% gap between train and test accuracy, which suggest that our model is likely overfitting, one can probably achieve better results if the hyperparameters are fine tuned and if the model is regularized properly.\n<br>In addtion, if we look at the mean year in `X_test` and `X_train`, we can see that one is around 2003.7 and the other 2008.3, meaning that the test set is on average later than the train set, which may be the reason behind the train\/test accuracy gap.","70cde219":"As one may expect, the most important features are all from the reviews. Surprisingly, however, features regarding the recipe itself do not play a major role in predicting. Let's look at some of the most important words:","265edcf8":"Training time for me was around 80 minutes on this kaggle kernel","c50e930f":"# EDA and FE\nInspired by Andrew Lukyanenko, I want to experiment with OOP in machine learning to improve reusability, so we'll write a class for adding\/processing features. The class `FeatureTransformer` will to the following:\n* remove unecessary columns (`u`, `i`, `user_id`, `recipe_id`, `contributor_id`).\n* `year`, `month`, `day_of_week`: three features extracted from `date`.\n* `upload_year`: the year when the recipe is uploaded\n* `calories`: I assumed that all the numbers in the `nutrition` column represent heat contained in the food, so I created this column as the sum of all the numbers.\n* four `CountVectorizer`s for the columns `[\"review\", \"description\", \"steps\", \"ingredients\"]`. To learn more about `CountVectorizer`, check out the [documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html).\n* There are many other things that one can do here. Play around and see what changes new features can make if you'd like to.\n<br><br>\\*Note: In `PP_recipes.csv` there are data that can be used to calculate the averge rating of a recipe, but I didn't use it since it will become the dominating feature and ruin the fun."}}