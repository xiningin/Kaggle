{"cell_type":{"1d0eda74":"code","6b7ec4ac":"code","eda13ec6":"code","33772d1a":"code","9644d0ac":"code","be4ed4ce":"code","7c3ad7ac":"code","fc9ae7c5":"code","4afdca0f":"code","f0274ac6":"code","ff6ed585":"code","927a9e69":"code","3fa77da4":"code","2fa02604":"code","8678bd28":"code","07370cf5":"code","cdc25db3":"code","17b32166":"code","71fd63e5":"markdown","32ad8876":"markdown","0bc563f6":"markdown","75d109e3":"markdown","9754f496":"markdown","31199d0a":"markdown","2c0d35bc":"markdown","9ee4852f":"markdown"},"source":{"1d0eda74":"\n%%bash\npip install pytorch-pretrained-bert","6b7ec4ac":"import csv\nimport pandas as pd\nfrom pathlib import Path\nimport matplotlib.cm as cm\nfrom fastai import *\nfrom fastai.text import *\nfrom fastai.callbacks import *\nfrom fastai.metrics import *\n#import utils, bert_fastai, bert_helper\nimport numpy as np\nimport pandas as pd\n\nfrom pathlib import Path\nfrom typing import *\n\nimport torch\nimport torch.optim as optim","eda13ec6":"class Config(dict):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        for k, v in kwargs.items():\n            setattr(self, k, v)\n    \n    def set(self, key, val):\n        self[key] = val\n        setattr(self, key, val)\n\n\nconfig = Config(\n    testing=False,\n    bert_model_name=\"bert-base-uncased\",\n    max_lr=3e-5,\n    epochs=4,\n    use_fp16=True,\n    bs=32,\n    discriminative=False,\n    max_seq_len=256,\n)","33772d1a":"from pytorch_pretrained_bert import BertTokenizer\nbert_tok = BertTokenizer.from_pretrained(\n    config.bert_model_name,\n)","9644d0ac":"class FastAiBertTokenizer(BaseTokenizer):\n    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n        self._pretrained_tokenizer = tokenizer\n        self.max_seq_len = max_seq_len\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n    def tokenizer(self, t:str) -> List[str]:\n        \"\"\"Limits the maximum sequence length\"\"\"\n        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]","be4ed4ce":"fastai_tokenizer = Tokenizer(tok_func=FastAiBertTokenizer(bert_tok, max_seq_len=config.max_seq_len), pre_rules=[], post_rules=[])\n","7c3ad7ac":"fastai_bert_vocab = Vocab(list(bert_tok.vocab.keys()))","fc9ae7c5":"#read the Training Data\n\ntrain_df=pd.read_excel('..\/input\/Train_Data.xlsx')","4afdca0f":"train_df.head()","f0274ac6":"train_df.info()","ff6ed585":"#Split data into Train and Validation \nfrom sklearn.model_selection import train_test_split\ntrain, val = train_test_split(train_df)","927a9e69":"databunch = TextDataBunch.from_df(\".\", train, val, \n                  tokenizer=fastai_tokenizer,\n                  vocab=fastai_bert_vocab,\n                  include_bos=False,\n                  include_eos=False,\n                  text_cols=\"question_text\",\n                  label_cols=\"target\",\n                  bs=config.bs,\n                  collate_fn=partial(pad_collate, pad_first=False, pad_idx=0),\n             )","3fa77da4":"databunch.show_batch()","2fa02604":"databunch.classes","8678bd28":"\nfrom pytorch_pretrained_bert.modeling import BertConfig, BertForSequenceClassification\nbert_model = BertForSequenceClassification.from_pretrained(config.bert_model_name, num_labels=2)","07370cf5":"learner = Learner(\n    databunch, bert_model,\n    metrics=[accuracy]\n)\nlearner.callbacks.append(ShowGraph(learner))","cdc25db3":"learner.lr_find()\nlearner.recorder.plot(suggestion=True)","17b32166":"learner.fit_one_cycle(3, max_lr=3e-5)","71fd63e5":"\nwe use the same hyperparameters as mentioned in the BERT paper(https:\/\/github.com\/google-research\/bert)","32ad8876":"\nwe will have to use the BERT tokenizer and BERT vocabulary while using the BERT model,we cant use the default tokenzer and vocabulary of Fastai. \n\nHence we will have to write this custom code in Pytorch \n","0bc563f6":"Please refer https:\/\/www.kaggle.com\/keitakurita\/bert-with-fastai-example for the original code of BERT in Fastai","75d109e3":"One of the major breakthroughs in deep learning in 2018 was the development of effective transfer learning methods in NLP. One method that took the NLP community by storm was BERT.\n\nBERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.\n","9754f496":"we will use a Pretrained BERT base uncased model which has 24-layers, 1024-hidden, 16-heads, 340M parameters","31199d0a":"Install Pretrained BERT for Pytorch in the below step","2c0d35bc":"This notebook uses Python >= 3.6 and fastai >=1.0.52","9ee4852f":"**we see that just with 3 iterations, we are able to get 96% accuracy on this data set without any feature engineering and training. Hence Transfer Learning is a good approach for Text classification\n**"}}