{"cell_type":{"7fa2f64c":"code","864eaeb6":"code","357a025f":"code","666c683a":"code","b6a02cd8":"code","73c64fdc":"code","9db30189":"code","21f696eb":"code","b14021b2":"code","1b17444b":"code","2e94fc62":"code","d8d8ff11":"code","e5792624":"code","ae946ed6":"code","4f0c25f9":"code","6c002000":"code","0b9b9825":"code","98ed298e":"code","2ddc9e85":"code","178c0793":"code","355fbfac":"code","5c86ef0a":"code","2606e2dc":"code","9d77afd2":"code","13b21274":"code","730a2c97":"code","fdde344c":"code","cb18c49f":"code","a32383cd":"markdown","7acd2b60":"markdown","94d415b1":"markdown"},"source":{"7fa2f64c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","864eaeb6":"import csv\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom matplotlib.pyplot import figure\nimport numpy as np\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nimport statistics\nfrom sklearn.preprocessing import StandardScaler","357a025f":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\nimport os","666c683a":"\ntrainData= pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ntestData = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')\nout1=testData['Id']","b6a02cd8":"trainData.head()","73c64fdc":"testData.head()","9db30189":"trainData.columns","21f696eb":"# checking for null values for both training and testing data\ntrainData.isnull().sum().sum()","b14021b2":"testData.isnull().sum().sum()","1b17444b":"# Checking for class imbalance\ntrainData.groupby(\"Cover_Type\").Cover_Type.hist()\ntrainData['Cover_Type'].value_counts()","2e94fc62":"## cover type = 5 has just 1 dataset for training. This is of very little help to us. So, dropping it.\ntrainData.drop(trainData[trainData['Cover_Type']==5].index,inplace=True)","d8d8ff11":"# Dividing the training dataset into continuous and one-hot encoded ones for futher EDA\ntrainData.columns\ntrainDataSet1 = trainData[['Elevation', 'Aspect', 'Slope',\n       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points','Cover_Type']]\n\ntrainDataSet2=trainData[['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n       'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40',\n       'Cover_Type']]","e5792624":"# Doing a box plot for first level visualisation of continuous features\nfor column in trainDataSet1:\n    plt.figure()\n    sns.boxplot(y = trainDataSet1[column],\n            x = trainDataSet1['Cover_Type'])","ae946ed6":"# Checking the correlation matrix\ncorr_matrix1 = trainDataSet1.corr()\ncorr_matrix2 = trainDataSet2.corr()\ncorr_matrix3 = trainData.corr()","4f0c25f9":"# plotting correlation heatmap\ndataplot1 = sns.heatmap(corr_matrix1, cmap=\"YlGnBu\", annot=False)","6c002000":"# plotting correlation heatmap\ndataplot2 = sns.heatmap(corr_matrix2, cmap=\"YlGnBu\", annot=False)","0b9b9825":"dataplot3 = sns.heatmap(corr_matrix3, cmap=\"YlGnBu\", annot=False)","98ed298e":"## dropping columns 'Soil_Type7' and 'Soil_Type15' and 'Id' from training dataset\n\ntrainData=trainData.drop(['Id','Soil_Type7','Soil_Type15'],axis=1)\ntestData=testData.drop(['Id','Soil_Type7','Soil_Type15'],axis=1)","2ddc9e85":"# Prepping the data \n# separating features and their labels\nX=trainData.drop('Cover_Type',axis=1)\ny=trainData['Cover_Type']\n","178c0793":"# Standardisation\n# for training data\nscaler = StandardScaler()\nscaler.fit(X)\nscaler.transform(X)","355fbfac":"# for testing Data\nscaler.transform(testData)","5c86ef0a":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_memory = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_memory = df.memory_usage().sum() \/ 1024**2\n    if verbose: \n        print(f\"Memory usage of dataframe after reduction {end_memory} MB\")\n        print(f\"Reduced by {100 * (start_memory - end_memory) \/ start_memory} % \")\n    return df","2606e2dc":"X = reduce_mem_usage(X)\ntestData = reduce_mem_usage(testData)","9d77afd2":"tps_clf = RandomForestClassifier(random_state=42)","13b21274":"tps_clf.fit(X,y)","730a2c97":"del trainData\ndel trainDataSet1\ndel trainDataSet2\ndel corr_matrix1\ndel corr_matrix2\ndel corr_matrix3","fdde344c":"y_pred=tps_clf.predict(testData)","cb18c49f":"out1 = pd.DataFrame(out1, columns = ['Id'])\ny_pred = pd.DataFrame(y_pred, columns = ['Cover_Type'])\nframe3 = pd.concat([out1, y_pred], axis=1)\nframe3.to_csv('kaggle_TPS_submission.csv')","a32383cd":"# Pre-processing","7acd2b60":"# Exploratory Data Analysis (EDA)","94d415b1":"# Import data"}}