{"cell_type":{"37ed388a":"code","1bbfda5d":"code","13e2d438":"code","117d0dd5":"code","3f6723c5":"code","3099ae96":"code","a456c368":"code","310b261a":"code","dcba9a73":"code","198895ef":"code","7168132a":"code","c47d5eac":"code","875757ae":"code","c9b6c816":"code","a8cbee2f":"code","a6d11d5a":"code","c90a09a2":"code","12503cc1":"code","0fc4517e":"code","040e0bc2":"code","4fe8b7ba":"code","65a9c9da":"code","26f9cced":"code","b72350ca":"code","2530ee67":"code","0371d13d":"code","46f075c3":"code","62854ff3":"code","7a3a79b4":"code","2bbb9c2c":"code","84e58ffe":"code","9ce4631d":"code","c3b3835f":"code","a7393ab2":"code","5b5c32e6":"code","b2d194d3":"code","f696b8d6":"code","9aa53378":"code","6054e79c":"code","f4d6ceee":"code","ef77386e":"code","ddaee76d":"code","4977cd71":"code","fc1bdacb":"code","63013351":"code","94b08a97":"code","914b1668":"code","b17367d7":"code","ed0ad0a7":"code","cb1d80c9":"code","0291e77f":"code","90662bb1":"code","16968592":"code","0d9bfcba":"code","5c8e6bcb":"code","05680e09":"code","cc4884ad":"code","93a7939b":"code","80dc7619":"code","fca2bc30":"code","f8c836ea":"code","8c376da2":"code","287a8714":"code","a17a73a8":"code","fee1de49":"code","1ade2196":"code","1419e1c2":"code","08ed63b0":"code","85ae5e26":"code","7bd8691b":"code","39fa2aea":"code","45063ebc":"code","6c3de16f":"code","67aa10bc":"code","c3885283":"code","41659b5e":"code","45909586":"code","7ad12ebc":"code","6c372459":"code","794c5747":"code","e750ebad":"code","92750aad":"code","8c2f17f1":"code","1ec06517":"code","efbf91af":"code","f82fcb28":"code","609e2517":"code","8d9b0b68":"code","d0f61694":"code","b9bbb8f0":"code","a7aa2d8e":"code","62dd14d2":"code","4dd71ceb":"code","d68fc572":"code","f8b6962e":"markdown","93d5c627":"markdown","3e640307":"markdown","ef64afa1":"markdown","c69c3ac0":"markdown","f27c3c2a":"markdown","47365d4a":"markdown","2c75476c":"markdown","c95285cb":"markdown","8b37fa7a":"markdown","0bcdb2e1":"markdown","eaec4597":"markdown","e648d4b1":"markdown","e179a024":"markdown","307d54b2":"markdown","94765d3d":"markdown","1cd53322":"markdown","1d446e11":"markdown","44866d78":"markdown","87429236":"markdown","60890143":"markdown","a1a049dd":"markdown","46ef6c0b":"markdown","f627f4bd":"markdown","c4f5e10d":"markdown","d076dcfc":"markdown","ad3065be":"markdown","9232ec3c":"markdown","3c7269a3":"markdown","8fed5772":"markdown","2b6321a4":"markdown","020d2550":"markdown","bc06db86":"markdown","fca73645":"markdown","befd0339":"markdown","09a01f75":"markdown","e1d36ea1":"markdown","1c46838c":"markdown","51223cfc":"markdown","8fc8b10f":"markdown","e7a773d4":"markdown","64143c00":"markdown","de65dfa7":"markdown","29034df0":"markdown","2324f323":"markdown","a12bc1ab":"markdown","93fe881b":"markdown"},"source":{"37ed388a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nimport statsmodels.api as sm\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\n\n# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nfrom scipy.stats import skew\n\n\n#Display full output in each jupyter cell, not jut the last statement\nfrom IPython.core.interactiveshell import InteractiveShell\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)","1bbfda5d":"housing = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","13e2d438":"housing.head()","117d0dd5":"housing.shape","3f6723c5":"test.shape","3099ae96":"housing.info()","a456c368":"housing.describe([0.25,0.50,0.75,0.99])","310b261a":"#Since 'Id' column is unnecessary for the prediction process., dropping Id column\nhousing.drop('Id', axis=1,inplace=True)\ntest.drop('Id', axis=1,inplace=True)","dcba9a73":"#Looking at columns with NaN Values\nhousing.isnull().sum().sort_values(ascending=False).head(20)","198895ef":"#Any rows with greater than 1 null value?\nhousing[housing.isnull().sum(axis=1)>1]","7168132a":"#Looking at the percentage of null values\nround(housing.isnull().sum()*100\/housing.shape[0],2).sort_values(ascending=False).head(20)","c47d5eac":"threshold =10\ndrop_cols = round(housing.isnull().sum()*100\/housing.shape[0],2)[round(housing.isnull().sum()*100\/housing.shape[0],2)>threshold].index.tolist()\ndrop_cols","875757ae":"housing.drop(columns=drop_cols, inplace=True)\ntest.drop(columns=drop_cols, inplace=True)\nhousing.shape","c9b6c816":"housing.head()","a8cbee2f":"round(housing.isnull().sum()*100\/housing.shape[0],2)[round(housing.isnull().sum()*100\/housing.shape[0],2)>0].sort_values(ascending=False)","a6d11d5a":"round(test.isnull().sum()*100\/test.shape[0],2)[round(test.isnull().sum()*100\/test.shape[0],2)>0].sort_values(ascending=False)","c90a09a2":"housing['GarageYrBlt'] = 2021-housing['GarageYrBlt']\nhousing['YearBuilt'] = 2021-housing['YearBuilt']\nhousing['YearRemodAdd'] = 2021-housing['YearRemodAdd']\nhousing['YrSold'] = 2021-housing['YrSold']\nhousing[['GarageYrBlt','YearBuilt','YearRemodAdd','YrSold']].head()","12503cc1":"test['GarageYrBlt'] = 2021-test['GarageYrBlt']\ntest['YearBuilt'] = 2021-test['YearBuilt']\ntest['YearRemodAdd'] = 2021-test['YearRemodAdd']\ntest['YrSold'] = 2021-test['YrSold']\ntest[['GarageYrBlt','YearBuilt','YearRemodAdd','YrSold']].head()","0fc4517e":"# NA in GarageType, GarageFinish, GarageQual & GarageCond means 'No Garage', so we will replace NA by 'No Garage'\nhousing['GarageFinish'].fillna('No Garage', inplace=True)\nhousing['GarageType'].fillna('No Garage', inplace=True)\nhousing['GarageQual'].fillna('No Garage', inplace=True)\nhousing['GarageCond'].fillna('No Garage', inplace=True)","040e0bc2":"# Imputing GarageYrBlt with -1, since these houses don't have garage \nhousing['GarageYrBlt'].fillna(-1, inplace=True)","4fe8b7ba":"# NA in BsmtExposure, BsmtFinType2, BsmtQual,BsmtCond & BsmtFinType1 means 'No Basement', so we will replace NA by 'No Basement'\nhousing['BsmtExposure'].fillna('No Basement', inplace=True)\nhousing['BsmtFinType1'].fillna('No Basement', inplace=True)\nhousing['BsmtQual'].fillna('No Basement', inplace=True)\nhousing['BsmtCond'].fillna('No Basement', inplace=True)\nhousing['BsmtFinType2'].fillna('No Basement', inplace=True)","65a9c9da":"housing['MasVnrType'].fillna('None', inplace=True)\nhousing['MasVnrArea'].fillna(0, inplace=True)","26f9cced":"# Dropping remaining rows with na\nhousing.dropna(axis=0, inplace=True)","b72350ca":"housing.shape","2530ee67":"# NA in GarageType, GarageFinish, GarageQual & GarageCond means 'No Garage', so we will replace NA by 'No Garage'\ntest['GarageFinish'].fillna('No Garage', inplace=True)\ntest['GarageType'].fillna('No Garage', inplace=True)\ntest['GarageQual'].fillna('No Garage', inplace=True)\ntest['GarageCond'].fillna('No Garage', inplace=True)\n# Imputing GarageYrBlt with -1, since these houses don't have garage \ntest['GarageYrBlt'].fillna(-1, inplace=True)\n# NA in BsmtExposure, BsmtFinType2, BsmtQual,BsmtCond & BsmtFinType1 means 'No Basement', so we will replace NA by 'No Basement'\ntest['BsmtExposure'].fillna('No Basement', inplace=True)\ntest['BsmtFinType1'].fillna('No Basement', inplace=True)\ntest['BsmtQual'].fillna('No Basement', inplace=True)\ntest['BsmtCond'].fillna('No Basement', inplace=True)\ntest['BsmtFinType2'].fillna('No Basement', inplace=True)\ntest['MasVnrType'].fillna('None', inplace=True)\ntest['MasVnrArea'].fillna(0, inplace=True)\ntest.shape","0371d13d":"housing.describe([0.25,0.50,0.75,0.99])","46f075c3":"num_col = list(housing.dtypes[housing.dtypes !='object'].index)\ndef drop_outliers(x):\n    list = []\n    for col in num_col:\n        Q1 = x[col].quantile(.25)\n        Q3 = x[col].quantile(.99)\n        IQR = Q3-Q1\n        x =  x[(x[col] >= (Q1-(1.5*IQR))) & (x[col] <= (Q3+(1.5*IQR)))] \n    return x\nhousing = drop_outliers(housing)","62854ff3":"housing.shape","7a3a79b4":"housing.describe([0.25,0.50,0.75,0.99])","2bbb9c2c":"# Dropping PoolArea column, since all values are 0 after removing outliers\nhousing.drop(columns=['PoolArea'], inplace=True)\ntest.drop(columns=['PoolArea'], inplace=True)","84e58ffe":"plt.title('SalePrice')\nsns.distplot(housing['SalePrice'], bins=10)\nplt.show()","9ce4631d":"housing['SalePrice'] = np.log1p(housing['SalePrice'])\n\nplt.title('SalePrice')\nsns.distplot(housing['SalePrice'], bins=10)\nplt.show()","c3b3835f":"#Get list of numeric variables\nnum_vars = list(housing.dtypes[housing.dtypes !='object'].index)\n\n#Let's review the numeric variables\nhousing[num_vars].head()","a7393ab2":"# Check the numerical values using pairplots\n\nplt.figure(figsize=(10,5))\nsns.pairplot(housing, x_vars=['MSSubClass','LotArea', 'MasVnrArea'], y_vars='SalePrice',height=4, aspect=1,kind='scatter')\nsns.pairplot(housing, x_vars=['OverallQual', 'OverallCond','OpenPorchSF'], y_vars='SalePrice',height=4, aspect=1,kind='scatter')\nsns.pairplot(housing, x_vars=['BsmtFinSF1', 'BsmtUnfSF','TotalBsmtSF'], y_vars='SalePrice',height=4, aspect=1,kind='scatter')\nsns.pairplot(housing, x_vars=['1stFlrSF','2ndFlrSF', 'GrLivArea'], y_vars='SalePrice',height=4, aspect=1,kind='scatter')\nsns.pairplot(housing, x_vars=['BsmtFullBath','FullBath', 'HalfBath'], y_vars='SalePrice',height=4, aspect=1,kind='scatter')\nsns.pairplot(housing, x_vars=['BedroomAbvGr','TotRmsAbvGrd', 'Fireplaces'], y_vars='SalePrice',height=4, aspect=1,kind='scatter')\nsns.pairplot(housing, x_vars=['GarageCars','GarageArea', 'WoodDeckSF'], y_vars='SalePrice',height=4, aspect=1,kind='scatter')\nsns.pairplot(housing, x_vars=['3SsnPorch','MiscVal','KitchenAbvGr'], y_vars='SalePrice',height=4, aspect=1,kind='scatter')\nplt.show()","5b5c32e6":"# Dropping & Clipping\nhousing.drop(columns=[ 'MSSubClass','3SsnPorch','MiscVal'], inplace=True)\nhousing.drop(columns=[ 'KitchenAbvGr','BsmtFullBath','BsmtHalfBath'], inplace=True)\nhousing['GrLivArea'] = housing['GrLivArea'].clip(0, 3000)\nhousing['TotalBsmtSF'] = housing['TotalBsmtSF'].clip(0, 3000)\nhousing['1stFlrSF'] = housing['1stFlrSF'].clip(0, 3000)\nhousing['GarageArea'] = housing['GarageArea'].clip(0, 1200)\nhousing['BsmtFinSF1'] = housing['BsmtFinSF1'].clip(0, 2500)\nhousing['OpenPorchSF'] = housing['OpenPorchSF'].clip(0, 400)\nhousing['LotArea'] = housing['LotArea'].clip(0, 60000)\nhousing.shape","b2d194d3":"# Dropping & Clipping\ntest.drop(columns=[ 'MSSubClass','3SsnPorch','MiscVal'], inplace=True)\ntest.drop(columns=[ 'KitchenAbvGr','BsmtFullBath','BsmtHalfBath'], inplace=True)\ntest['GrLivArea'] = test['GrLivArea'].clip(0, 3000)\ntest['TotalBsmtSF'] = test['TotalBsmtSF'].clip(0, 3000)\ntest['1stFlrSF'] = test['1stFlrSF'].clip(0, 3000)\ntest['GarageArea'] = test['GarageArea'].clip(0, 1200)\ntest['BsmtFinSF1'] = test['BsmtFinSF1'].clip(0, 2500)\ntest['OpenPorchSF'] = test['OpenPorchSF'].clip(0, 400)\ntest['LotArea'] = test['LotArea'].clip(0, 60000)\ntest.shape","f696b8d6":"# Let's check the correlation coefficients to see which variables are highly correlated\n\nplt.figure(figsize = (30, 20))\nsns.heatmap(housing.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","9aa53378":"# Dropping GarageArea, TotRmsAbvGrd, 1stFlrSF, 2ndFlrSF, BsmtFullBath, FullBath and GarageYrBlt\nhousing.drop(columns=['GarageArea','TotRmsAbvGrd','1stFlrSF','2ndFlrSF','FullBath','GarageYrBlt', 'YearRemodAdd'], inplace=True) #,'BsmtFullBath','FullBath','GarageYrBlt', 'YearRemodAdd'\nhousing.shape","6054e79c":"# Dropping GarageArea, TotRmsAbvGrd, 1stFlrSF, 2ndFlrSF, BsmtFullBath, FullBath and GarageYrBlt\ntest.drop(columns=['GarageArea','TotRmsAbvGrd','1stFlrSF','2ndFlrSF','FullBath','GarageYrBlt', 'YearRemodAdd'], inplace=True) #,'BsmtFullBath','FullBath','GarageYrBlt', 'YearRemodAdd'\ntest.shape","f4d6ceee":"numerical_columns = housing.select_dtypes(include=['int64', 'float64'])\nskewness_of_feats = numerical_columns.apply(lambda x: skew(x)).sort_values(ascending=False)\nprint(skewness_of_feats)","ef77386e":"# Using log transformation for fixing skewness within variables\nhousing['LowQualFinSF'] = np.log1p(housing['LowQualFinSF'])\nhousing['LotArea'] = np.log1p(housing['LotArea'])\nhousing['BsmtFinSF2'] = np.log1p(housing['BsmtFinSF2'])\nhousing['ScreenPorch'] = np.log1p(housing['ScreenPorch'])\nhousing['EnclosedPorch'] = np.log1p(housing['EnclosedPorch'])\nhousing['MasVnrArea'] = np.log1p(housing['MasVnrArea'])\nhousing['OpenPorchSF'] = np.log1p(housing['OpenPorchSF'])\nhousing['WoodDeckSF'] = np.log1p(housing['WoodDeckSF'])\nhousing['BsmtUnfSF'] = np.log1p(housing['BsmtUnfSF'])","ddaee76d":"# Using log transformation for fixing skewness within variables\ntest['LowQualFinSF'] = np.log1p(test['LowQualFinSF'])\ntest['LotArea'] = np.log1p(test['LotArea'])\ntest['BsmtFinSF2'] = np.log1p(test['BsmtFinSF2'])\ntest['ScreenPorch'] = np.log1p(test['ScreenPorch'])\ntest['EnclosedPorch'] = np.log1p(test['EnclosedPorch'])\ntest['MasVnrArea'] = np.log1p(test['MasVnrArea'])\ntest['OpenPorchSF'] = np.log1p(test['OpenPorchSF'])\ntest['WoodDeckSF'] = np.log1p(test['WoodDeckSF'])\ntest['BsmtUnfSF'] = np.log1p(test['BsmtUnfSF'])","4977cd71":"plt.figure(figsize = (20,20)) \nplt.subplot(3,3,1)\nsns.boxplot(x='MSZoning', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,2)\nsns.boxplot(x='BldgType', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,3)\nsns.boxplot(x='Street', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,4)\nsns.boxplot(x='LotShape', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,5)\nsns.boxplot(x='HouseStyle', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,6)\nsns.boxplot(x='Utilities', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,7)\nsns.boxplot(x='RoofStyle', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,8)\nsns.boxplot(x='LandSlope', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,9)\nsns.boxplot(x='Neighborhood', y=\"SalePrice\", data=housing)\nplt.show()","fc1bdacb":"plt.figure(figsize = (20,20)) \nplt.subplot(3,3,1)\nsns.boxplot(x='ExterQual', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,2)\nsns.boxplot(x='Foundation', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,3)\nsns.boxplot(x='BsmtQual', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,4)\nsns.boxplot(x='Heating', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,5)\nsns.boxplot(x='CentralAir', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,6)\nsns.boxplot(x='Electrical', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,7)\nsns.boxplot(x='KitchenQual', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,8)\nsns.boxplot(x='GarageType', y=\"SalePrice\", data=housing)\nplt.subplot(3,3,9)\nsns.boxplot(x='GarageQual', y=\"SalePrice\", data=housing)\nplt.show()\nplt.figure(figsize = (20,5)) \nplt.subplot(1,2,1)\nsns.boxplot(x='SaleType', y=\"SalePrice\", data=housing)","63013351":"housing.drop(columns=['Utilities'], inplace=True)","94b08a97":"test.drop(columns=['Utilities'], inplace=True)","914b1668":"cat_vars = list(housing.dtypes[housing.dtypes =='object'].index)\nhousing[cat_vars].head(10)","b17367d7":"housing[['LandSlope','ExterQual','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2',\n            'HeatingQC','CentralAir',  'KitchenQual','GarageFinish','GarageQual','GarageCond',\n             'ExterCond','LotShape']].head()","ed0ad0a7":"housing['LandSlope'] = housing.LandSlope.map({'Sev':0,'Mod':1,'Gtl':2})\nhousing['ExterQual'] = housing.ExterQual.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\nhousing['BsmtQual'] = housing.BsmtQual.map({'No Basement':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\nhousing['BsmtCond'] = housing.BsmtCond.map({'No Basement':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\nhousing['BsmtExposure'] = housing.BsmtExposure.map({'No Basement':0,'No':1,'Mn':2,'Av':3,'Gd':4})\nhousing['BsmtFinType1'] = housing.BsmtFinType1.map({'No Basement':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\nhousing['BsmtFinType2'] = housing.BsmtFinType2.map({'No Basement':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\nhousing['HeatingQC'] = housing.HeatingQC.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\nhousing['CentralAir'] = housing.CentralAir.map({'N':0,'Y':1})\nhousing['KitchenQual'] = housing.KitchenQual.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\nhousing['GarageFinish'] = housing.GarageFinish.map({'No Garage':0,'Unf':1,'RFn':2,'Fin':3})\nhousing['GarageQual'] = housing.GarageQual.map({'No Garage':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\nhousing['GarageCond'] = housing.GarageCond.map({'No Garage':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\nhousing['ExterCond'] = housing.ExterCond.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\nhousing['LotShape'] = housing.LotShape.map({'IR1':0,'IR2':1,'IR3':2,'Reg':3})","cb1d80c9":"housing.head()","0291e77f":"test['LandSlope'] = test.LandSlope.map({'Sev':0,'Mod':1,'Gtl':2})\ntest['ExterQual'] = test.ExterQual.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntest['BsmtQual'] = test.BsmtQual.map({'No Basement':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest['BsmtCond'] = test.BsmtCond.map({'No Basement':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest['BsmtExposure'] = test.BsmtExposure.map({'No Basement':0,'No':1,'Mn':2,'Av':3,'Gd':4})\ntest['BsmtFinType1'] = test.BsmtFinType1.map({'No Basement':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ntest['BsmtFinType2'] = test.BsmtFinType2.map({'No Basement':0,'Unf':1,'LwQ':2,'Rec':3,'BLQ':4,'ALQ':5,'GLQ':6})\ntest['HeatingQC'] = test.HeatingQC.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntest['CentralAir'] = test.CentralAir.map({'N':0,'Y':1})\ntest['KitchenQual'] = test.KitchenQual.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntest['GarageFinish'] = test.GarageFinish.map({'No Garage':0,'Unf':1,'RFn':2,'Fin':3})\ntest['GarageQual'] = test.GarageQual.map({'No Garage':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest['GarageCond'] = test.GarageCond.map({'No Garage':0,'Po':1,'Fa':2,'TA':3,'Gd':4,'Ex':5})\ntest['ExterCond'] = test.ExterCond.map({'Po':0,'Fa':1,'TA':2,'Gd':3,'Ex':4})\ntest['LotShape'] = test.LotShape.map({'IR1':0,'IR2':1,'IR3':2,'Reg':3})","90662bb1":"cat_vars = list(housing.dtypes[housing.dtypes =='object'].index)\ncat_vars","16968592":"#Converting remaining Categorical features to dummy variables using using one-hot encoding.\nhousing = pd.get_dummies(data=housing,columns=cat_vars,drop_first=True)","0d9bfcba":"housing.info()","5c8e6bcb":"test = pd.get_dummies(data=test,columns=cat_vars,drop_first=True)\ntest.info()","05680e09":"# Get missing columns in the training test\nmissing_cols = set( housing.columns ) - set( test.columns )\n# Add a missing column in test set with default value equal to 0\nfor c in missing_cols:\n    test[c] = 0\n# Ensure the order of column in the test set is in the same order than in train set\nhousing, test = housing.align(test, axis=1)","cc4884ad":"test.info()","93a7939b":"housing.describe()\n# Putting all feature variable to X\n\nX = housing.drop(['SalePrice'], axis=1)\nX.head()","80dc7619":"# Putting response variable to y\n\ny = housing['SalePrice']\ny.head()","fca2bc30":"# scaling the features\n\nfrom sklearn.preprocessing import scale\n\n# storing column names in cols\n# scaling (the dataframe is converted to a numpy array)\n\ncols = X.columns\nX = pd.DataFrame(scale(X))\nX.columns = cols\nX.columns","f8c836ea":"cols = test.columns\ntest = pd.DataFrame(scale(test))\ntest.columns = cols\ntest.columns","8c376da2":"np.random.seed(0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size = 0.3, random_state=42)","287a8714":"len(X_train.index)","a17a73a8":"len(X_test.index)","fee1de49":"# Running RFE with the output number of the variable equal to 50\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 50) # running RFE\nrfe = rfe.fit(X_train, y_train)","1ade2196":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","1419e1c2":"col = X_train.columns[rfe.support_]\ncol","08ed63b0":"X_train.columns[~rfe.support_]","85ae5e26":"# Creating X_train & X_test dataframes with RFE selected variables\nX_train_rfe = X_train[col]\nX_test_rfe = X_test[col]","7bd8691b":"test_rfe = test[col]","39fa2aea":"# list of alphas\n\nparams = {'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.02, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, \n                    9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\nridge = Ridge()\n\nfolds = 5\nridge_model_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nridge_model_cv.fit(X_train_rfe, y_train)","45063ebc":"ridge_cv_results = pd.DataFrame(ridge_model_cv.cv_results_)\nridge_cv_results[['param_alpha', 'mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values(by = ['rank_test_score'])","6c3de16f":"# plotting mean test and train scoes with alpha \n\nridge_cv_results['param_alpha'] = ridge_cv_results['param_alpha'].astype('int32')\n\n# plotting\nplt.figure(figsize=(16,8))\n\nplt.plot(ridge_cv_results['param_alpha'], ridge_cv_results['mean_train_score'])\nplt.plot(ridge_cv_results['param_alpha'], ridge_cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.xscale('log')\nplt.ylabel('R2 Score')\nplt.legend(['train score', 'test score'], loc='upper right')\nplt.show()","67aa10bc":"#checking the value of optimum number of parameters\nprint(ridge_model_cv.best_params_)","c3885283":"# Building the model with alpha\nridge = Ridge(alpha=ridge_model_cv.best_params_['alpha'])\n\nridge.fit(X_train_rfe, y_train)\ny_train_pred = ridge.predict(X_train_rfe)\ny_test_pred = ridge.predict(X_test_rfe)\n\nprint(r2_score(y_true=y_train,y_pred=y_train_pred))\nprint(r2_score(y_true=y_test,y_pred=y_test_pred))","41659b5e":"# Check the mean squared error\n\nmean_squared_error(y_test, y_test_pred)","45909586":"model_param = list(ridge.coef_)\nmodel_param.insert(0,ridge.intercept_)\ncols = X_train_rfe.columns\ncols.insert(0,'const')\nridge_coef = pd.DataFrame(list(zip(cols,model_param,(abs(ele) for ele in model_param))))\nridge_coef.columns = ['Feature','Coef','Mod']\nridge_coef.sort_values(by='Mod',ascending=False).head(10)","7ad12ebc":"lasso = Lasso()\n\nfolds = 10\nlasso_model_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \nlasso_model_cv.fit(X_train_rfe, y_train)","6c372459":"lasso_cv_results = pd.DataFrame(lasso_model_cv.cv_results_)\nlasso_cv_results[['param_alpha', 'mean_train_score', 'mean_test_score', 'rank_test_score']].sort_values(by = ['rank_test_score'])","794c5747":"# plotting mean test and train scoes with alpha \n\nlasso_cv_results['param_alpha'] = lasso_cv_results['param_alpha'].astype('int32')\n\n# plotting\nplt.figure(figsize=(16,8))\n\nplt.plot(lasso_cv_results['param_alpha'], lasso_cv_results['mean_train_score'])\nplt.plot(lasso_cv_results['param_alpha'], lasso_cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('R2 Score')\nplt.legend(['train score', 'test score'], loc='upper right')\nplt.show()","e750ebad":"#checking the value of optimum number of parameters\nprint(lasso_model_cv.best_params_)","92750aad":"# Building the model with alpha 0.0001\nlasso = Lasso(alpha=lasso_model_cv.best_params_['alpha'])\n\nlasso.fit(X_train_rfe, y_train)\ny_train_pred = lasso.predict(X_train_rfe)\ny_test_pred = lasso.predict(X_test_rfe)\n\nprint(r2_score(y_true=y_train,y_pred=y_train_pred))\nprint(r2_score(y_true=y_test,y_pred=y_test_pred))","8c2f17f1":"# Check the mean squared error\n\nmean_squared_error(y_test, y_test_pred)","1ec06517":"model_param = list(lasso.coef_)\nmodel_param.insert(0,lasso.intercept_)\ncols = X_train_rfe.columns\ncols.insert(0,'const')\nlasso_coef = pd.DataFrame(list(zip(cols,model_param,(abs(ele) for ele in model_param))))\nlasso_coef.columns = ['Feature','Coef','Mod']\nlasso_coef.sort_values(by='Mod',ascending=False).head(20)","efbf91af":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_pred), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","f82fcb28":"plt.scatter(y_train, (y_train - y_train_pred))\nplt.xlabel(\"Fitted values\")\nplt.ylabel(\"Residuals\")\nplt.show()","609e2517":"print('The Durbin-Watson value for Final Model is',round(sm.stats.stattools.durbin_watson((y_train - y_train_pred)),4))","8d9b0b68":"lasso = Lasso(alpha=0.0001)\n\nlasso.fit(X_train_rfe, y_train)\ny_train_pred = lasso.predict(X_train_rfe)\ny_test_pred = lasso.predict(X_test_rfe)","d0f61694":"test_rfe = test_rfe.fillna(test_rfe.interpolate())\npreds = lasso.predict(test_rfe)\nfinal_predictions = np.exp(preds)","b9bbb8f0":"test.index = test.index + 1461\nsubmission = pd.DataFrame({'Id': test.index ,'SalePrice': final_predictions })\nsubmission.to_csv(\"submission.csv\",index=False)","a7aa2d8e":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_test_pred)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16) ","62dd14d2":"#Let's visualize Actual vs Predicted for Test Data\n\nc = [i for i in range(1,433,1)]\nfig = plt.figure(figsize=(20,8))\nplt.plot(c,y_test, color=\"blue\", linewidth=2.5, linestyle=\"-\")\nplt.plot(c,y_test_pred, color=\"red\",  linewidth=2.5, linestyle=\"-\")\nfig.suptitle('Actual vs Predicted Test Data', fontsize=20)              # Plot heading \nplt.xlabel('Index', fontsize=18)                               # X-label\nplt.ylabel('Count', fontsize=16)                               # Y-label","4dd71ceb":"#Let's get the r-square for test data\nr2_score(y_test, y_test_pred)","d68fc572":"model_param = list(lasso.coef_)\nmodel_param.insert(0,lasso.intercept_)\ncols = X_train_rfe.columns\ncols.insert(0,'const')\nlasso_coef = pd.DataFrame(list(zip(cols,model_param,(abs(ele) for ele in model_param))))\nlasso_coef.columns = ['Feature','Coef','Mod']\nlasso_coef.sort_values(by='Mod',ascending=False).head(10)","f8b6962e":"<a id=\"3\"><\/a>\n## Step 3: Data Preparation","93d5c627":"<a id='6'><\/a>\n## Step 6: Making Prediction using the Final Model And Evaluation","3e640307":"# Table of Contents\n\n* [1. Data Loading, Understanding and Cleaning the Data](#1)\n * [1.1 Loading the data ](#1.1)\n * [1.2 Analysing the dataframe ](#1.2)\n * [1.3 Cleaning the dataframe ](#1.3)\n* [2. Visualising the Data](#2)\n * [2.1 Visualising the Target Variables ](#2.1)\n * [2.2 Visualising Numeric Variables ](#2.2)\n * [2.3 Visualising Categorical Variables](#2.3)\n* [3. Data Preparation](#3)\n * [3.1 Converting categorical data into numerical data](#3.1)\n * [3.2 Dummy Variables](#3.1)\n * [3.3 Splitting the Data into Training and Testing Sets](#3.2)\n * [3.4 Rescaling the Features](#3.3)\n* [4. Building a Linear Model](#4)\n * [4.1 Using RFE for Initial Feature Selection](#4.1)\n * [4.2 Building model using Ridge Regression](#4.2)\n * [4.3 Building model using Lasso Regression](#4.2)\n* [5. Validating the assumptions of Linear Regression](#5)\n * [5.1 Residual Analysis on the train data](#5.1)\n * [5.2 Preserving Homoscedasticity](#5.2)\n * [5.3 Observations are independent of each other](#5.3)\n * [5.4 No Multicolinearity](#5.4)\n* [6. Making Prediction using the Final Model And Evaluation](#6)\n * [6.1 Model Evaluation](#6.1)\n * [6.2 Conclusion](#6.2)","ef64afa1":"<a id=\"5.3\"><\/a>\n### 5.3: Observations are independent of each other\nWe can use the Durbin-Watson test for verification. The test will output values between 0 and 4. The closer it is to 2, the less auto-correlation there is between the various variables.","c69c3ac0":"#### <u> Observations <\/u>: \n\n- MsZoning with of type 'Fv' has high Saleprice and type 'C' has least sale price\n- The Street of type 'Pave' has more Sale Price when compared to 'Grvl' the utlities coulms have most of its values as 'AllPub' So we this column have give much of an informration. - Its not an important feature.\n- The house with Exterior Quality of type Excellent has the highest SalePrice.\n- The house with Basement Quality of type Excellent has the highest SalePrice.\n- The house with Kitchen Quality of type Excellent has the highest SalePrice.\n- The house with Garage Quality of type Excellent has the highest SalePrice.","f27c3c2a":"<a id=\"5.2\"><\/a>\n### 5.2: Preserving Homoscedasticity\nThe probability distribution of the errors has constant variance. We can look at residual vs fitted values plot","47365d4a":"<a id='6.1'><\/a>\n### 6.1 Final Model\n#### Building the Final model using Lasso with Optimal alpha 0.0001","2c75476c":"<a id=\"3.3\"><\/a>\n### Step 3.3 Rescaling the Features\nLet's bring all numeric variables to the same scale so as to simplify model evaluation and interpretation.","c95285cb":"<a id=\"2.1\"><\/a>\n### 2.1 Visualising the Target Variables","8b37fa7a":"#### <u> Observations <\/u>:  \n- `SalePrice` seems to be correlated with `OverallQual`, `GrLivArea` and `GarageCars` most\n- `GarageCars` and `GarageArea` seems to be highly  correlated with each other.\n- `GrLivArea` and `TotRmsAbvGrd` seems to be highly  correlated with each other.\n- `TotalBsmtSF` and `1stFlrSF` seems to be highly  correlated with each other.\n- `GrLivArea` and `2ndFlrSF` seems to be highly  correlated with each other.\n- `GrLivArea` and `FullBath` seems to be highly  correlated with each other.\n- `YearBuilt` and `GarageYrBlt` seems to be highly  correlated with each other.\n- `YearBuilt` and `YearRemodAdd` seems to be highly  correlated with each other.","0bcdb2e1":"### Observation: \nAfter creating model in both Ridge and Lasso we can see that the r2_scores are almost same for both of them but as lasso will penalize more on the dataset and can also help in feature elemination i am going to consider that as my final model.","eaec4597":"#### Lets check for the below columns here we can clearly see that these are having some kind of order and hence we can say these are ordinal in nature","e648d4b1":"<a id=\"1.1\"><\/a>\n## 1.1 Loading the data","e179a024":"<a id=\"4.3\"><\/a>\n### Step 4.3 Building model using Lasso Regression","307d54b2":"### Checking remaining columns with null values and imputing them","94765d3d":"<a id=\"5.1\"><\/a>\n### 5.1: Residual Analysis on the train data\ni.e. are the error terms normally distributed","1cd53322":"### Checking for skewness within independent variables","1d446e11":"#### <u> Observations <\/u>:  \n- The scatter plot doesn't show any funnel shape pattern, then we say that Homoscedasticity is well preserved","44866d78":"<a id=\"2\"><\/a>\n## Step 2: Visualising the Data\n\nLet's now spend some time doing what is arguably the most important step - **understanding the data**.\n- If there is some obvious multicollinearity going on, this is the first place to catch it\n- Here's where you'll also identify if some predictors directly have a strong association with the outcome variable","87429236":"<a id=\"3.4\"><\/a>\n### Step 3.4 Splitting the Data into Training and Testing Sets\n    -As you know, the first basic step for regression is performing a train-test split.\n    -We will split the data into 2 parts : train data and test data","60890143":"### Let's start with importing all the required libraries for the analysis.","a1a049dd":"<a id=\"5\"><\/a>\n## Step 5: Validating the assumptions of Linear Regression\nLet's verify that the model fulfills the assumptions of linear regression ","46ef6c0b":"<a id=\"2.3\"><\/a>\n### 2.3 Visualising Categorical Variables\n\nAs you might have noticed, there are a many categorical variables as well. Let's make a boxplot for some of these variables.","f627f4bd":"<a id=\"1\"><\/a>\n## Step 1: Data Loading, Understanding and Cleaning the Data","c4f5e10d":"<a id=\"2.2\"><\/a>\n### 2.2 Visualising Numeric Variables","d076dcfc":"#### <u> Observations <\/u>:  \nError terms are normally distributed with a mean of zero, so we can use this model to do predictions","ad3065be":"#### <u> Observations <\/u>:  \n- `MSSubClass`,`3SsnPorch` & `MiscVal` don't seem to have a relationship with `SalePrice`so can be dropped.\n- `KitchenAbvGr`,`BsmtFullBath`, `BsmtHalfBath` don't seem to have a relationship with `SalePrice`so can be dropped.\n- `GrLivArea`, `TotalBsmtSF` and `1stFlrSF` have a similar issue with extreme outliers so I'll clip them all back to a maximum vlue of 3000.\n- `GarageArea` also have some outliers","9232ec3c":"#### <u> Observation <\/u>:  After One-Hot Encoding  we have `183` numeric columns","3c7269a3":"<a id=\"4.2\"><\/a>\n### Step 4.2 Building model using Ridge Regression","8fed5772":"### Dividing into X and Y sets for the model building","2b6321a4":"### Observation : \nNow we can see our target variable `SalePrice` is skewed, so doing log transformation","020d2550":"<a id=\"3.1\"><\/a>\n### 3.1 Converting categorical data into numerical data","bc06db86":"<a id='6.2'><\/a>\n\n## Conclusion :\n\n- The optimal lambda value in case of Ridge and Lasso is as below:\n    - Ridge - 0.1\n    - Lasso - 0.0001\n    \n- The Mean Squared error in case of Ridge and Lasso are:\n    - Ridge - 0.01922\n    - Lasso - 0.01904\n    \n- The r2_score for test data in case of Ridge and Lasso are:\n    - Ridge - 87.9%\n    - Lasso - 88.04%\n\n- The Mean Squared Error of Lasso is slightly lower than that of Ridge\n\n- Also, since Lasso helps in feature reduction, Lasso has a better edge over Ridge.\n  \n- Hence based on Lasso, the factors that generally affect the price are the :\n    - Lot Area\n    - MSZoning\n    - KitchenQual\n    - Neighborhood\n    - SaleCondition\n    - Overall quality\n    \nTherefore, the variables predicted by Lasso are significant variables for predicting the price of a house.","fca73645":"### Observation : \nNow we can see all `null` values are taken care off.","befd0339":"### Null value treatment\n#### Instead of dropping the null values which will result in a data loss, we will impute the null values according the data dictionary provided with the data.","09a01f75":"<a id=\"4\"><\/a>\n## Step 4: Building a Linear Model","e1d36ea1":"#### <u> Observation <\/u>:  We have now 1007 rows in training dataset and 432 rows in test dataset ","1c46838c":"### Removing outliers, taking the lower and upper quantile as 0.25 & 0.99 respectively","51223cfc":"<a id=\"1.2\"><\/a>\n##  1.2 Analysing the dataframe","8fc8b10f":"### Converting years to number of years for GarageYrBlt, YearBuilt , YearRemodAdd & YrSold","e7a773d4":"<h2><center> If you liked this notebook, please don't forget to comment and upvote. <\/center><\/h2>\n<h2><center>Thank you!<\/center><\/h2>","64143c00":"<a id=\"3.2\"><\/a>\n### 3.2 Dummy Variables\nCreating Dummy Variables for Categorical Features","de65dfa7":"### Considering 10% as threshold and dropping columns having more than threshold NaN values","29034df0":"<a id=\"1.3\"><\/a>\n##  1.3 Cleaning the dataframe","2324f323":"<h1><center>House Price Prediction - Ridge and Lasso Regression with RFE<\/center><\/h1>","a12bc1ab":"<a id=\"4.1\"><\/a>\n### Step 4.1 Using RFE for Initial Feature Selection\nLet's use Recursive Feature Elimination (RFE) to automatically select 50 best features.","93fe881b":"### Outlier Treatment"}}