{"cell_type":{"29fb0e5c":"code","c25fdcef":"code","d7a7177a":"code","4c182dca":"code","2f0c4251":"code","eb606ff8":"code","bfc06e56":"code","2156067f":"code","a1f2ba28":"code","fb3b3b14":"code","71f2893d":"code","7a6478a5":"code","b026bf9b":"code","3159319f":"code","08d96577":"code","3f6acf1e":"code","f78d2ae6":"code","866d4814":"code","16395f94":"code","cb21a88a":"code","9ebd85de":"code","ce383622":"code","628a01e7":"code","04519579":"code","18ba4c3b":"code","702087c5":"code","6949e1aa":"code","35711cae":"code","aeccc26f":"code","752f0294":"code","00fe9036":"code","f7812fce":"code","4ade6ad6":"code","7caec854":"code","c5147aa7":"code","4b6f9821":"code","98d3f06a":"code","8cd4a3fa":"code","b9b1a392":"markdown","5c2f4dd7":"markdown","b6aee1bc":"markdown","d1aebc52":"markdown","f647f4d4":"markdown","2ff35f95":"markdown","b2fb1857":"markdown","55e48d80":"markdown","1e12f764":"markdown","5224b978":"markdown","19d31f1b":"markdown","598414a7":"markdown","95b7fc9c":"markdown","952cc0ee":"markdown","681e293a":"markdown","aba4a7c2":"markdown","79657f9e":"markdown","2cbadecb":"markdown","09d44f24":"markdown","90b6266b":"markdown","ffe51af8":"markdown","b493b0b1":"markdown","738591ce":"markdown","b1572c26":"markdown","52573046":"markdown","3680a7eb":"markdown","27512e9c":"markdown"},"source":{"29fb0e5c":"# need for rembert\n!pip install -U --no-build-isolation --no-deps ..\/input\/transformers-master\/ -qq","c25fdcef":"import pandas as pd\nfrom collections import Counter\nfrom transformers import AutoTokenizer\nimport plotly.express as px\n\ndf = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv\")","d7a7177a":"tokenizer = AutoTokenizer.from_pretrained(\"deepset\/xlm-roberta-base-squad2\")\n\ncontext_tokens = [tokenizer(txt)[\"input_ids\"] for txt in df[\"context\"]]\nquestion_tokens = [tokenizer(txt)[\"input_ids\"] for txt in df[\"question\"]]\nanswer_tokens = [tokenizer(txt)[\"input_ids\"] for txt in df[\"answer_text\"]]\n\ndf[\"num_tokens_context\"] = [len(tok) for tok in context_tokens]\ndf[\"num_chars_context\"] = [len(tok) for tok in df[\"context\"]]\ndf[\"num_tokens_question\"] = [len(tok) for tok in question_tokens]\ndf[\"num_chars_question\"] = [len(tok) for tok in df[\"question\"]]\ndf[\"num_tokens_answer\"] = [len(tok) for tok in answer_tokens]\ndf[\"num_chars_answer\"] = [len(tok) for tok in df[\"answer_text\"]]","4c182dca":"context_tokens_flat = sum(context_tokens, [])\nquestion_tokens_flat = sum(question_tokens, [])\nanswer_tokens_flat =  sum(answer_tokens, [])\n\nunk = tokenizer.unk_token_id\n\nunk in context_tokens_flat, unk in question_tokens_flat, unk in answer_tokens_flat","2f0c4251":"num_unk_tokens = sum([tok == unk for tok in context_tokens_flat])\nnum_unk_tokens, num_unk_tokens\/len(context_tokens_flat)","eb606ff8":"tokenizer_muril = AutoTokenizer.from_pretrained(\"google\/muril-base-cased\")\n\ncontext_tokens_muril = [tokenizer_muril(txt)[\"input_ids\"] for txt in df[\"context\"]]\nquestion_tokens_muril = [tokenizer_muril(txt)[\"input_ids\"] for txt in df[\"question\"]]\nanswer_tokens_muril = [tokenizer_muril(txt)[\"input_ids\"] for txt in df[\"answer_text\"]]\n\ncontext_tokens_flat_muril = sum(context_tokens_muril, [])\nquestion_tokens_flat_muril = sum(question_tokens_muril, [])\nanswer_tokens_flat_muril =  sum(answer_tokens_muril, [])\n\nunk_muril = tokenizer_muril.unk_token_id\n\nprint(\"Unk token in context, question, answer\")\nprint(unk_muril in context_tokens_flat_muril, unk_muril in question_tokens_flat_muril, unk_muril in answer_tokens_flat_muril)\n\nprint(\"Num unk tokens in context, question, answer\")\nsum([tok == unk_muril for tok in context_tokens_flat_muril]), sum([tok == unk_muril for tok in question_tokens_flat_muril]), sum([tok == unk_muril for tok in answer_tokens_flat_muril])","bfc06e56":"tokenizer_rembert = AutoTokenizer.from_pretrained(\"google\/rembert\")\n\ncontext_tokens_rembert = [tokenizer_rembert(txt)[\"input_ids\"] for txt in df[\"context\"]]\nquestion_tokens_rembert = [tokenizer_rembert(txt)[\"input_ids\"] for txt in df[\"question\"]]\nanswer_tokens_rembert = [tokenizer_rembert(txt)[\"input_ids\"] for txt in df[\"answer_text\"]]\n\ncontext_tokens_flat_rembert = sum(context_tokens_rembert, [])\nquestion_tokens_flat_rembert = sum(question_tokens_rembert, [])\nanswer_tokens_flat_rembert =  sum(answer_tokens_rembert, [])\n\nunk_rembert = tokenizer_rembert.unk_token_id\n\nprint(\"Unk token in context, question, answer\")\nprint(unk_rembert in context_tokens_flat_rembert, unk_rembert in question_tokens_flat_rembert, unk_rembert in answer_tokens_flat_rembert)\n\nprint(\"Num unk tokens in context, question, answer\")\nsum([tok == unk_rembert for tok in context_tokens_flat_rembert]), sum([tok == unk_rembert for tok in question_tokens_flat_rembert]), sum([tok == unk_rembert for tok in answer_tokens_flat_rembert])","2156067f":"contexts = df[\"context\"]\nanswers = df[\"answer_text\"]\n\nall_chars_ctx = \"\".join(contexts)\nall_chars_ans = \"\".join(answers)\n\nunq_chars_ctx = sorted(list(set(all_chars_ctx)))\nunq_chars_ans = sorted(list(set(all_chars_ans)))","a1f2ba28":"print(\"Contexts: \", len(contexts), contexts.nunique())\nprint(\"Answers: \", len(answers), answers.nunique())","fb3b3b14":"\"\".join(unq_chars_ctx)","71f2893d":"\"\".join(unq_chars_ans)","7a6478a5":"answers[answers.str.contains(r\"\\.\")]","b026bf9b":"# Arabic\nresults = answers[answers.str.contains(r\"[0123456789]{4}\")]\nprint(len(results))\nresults.tolist()","3159319f":"# Devanagari\nresults = answers[answers.str.contains(r\"[\u0966\u0967\u0968\u0969\u096a\u096b\u096c\u096d\u096e\u096f]{4}\")]\nprint(len(results))\nresults.tolist()","08d96577":"# Tamil\nresults = answers[answers.str.contains(r\"[\u0be6\u0be7\u0be8\u0be9\u0bea\u0beb\u0bec\u0bed\u0bee\u0bef\u0bf0]{4}\")]\nprint(len(results))\nresults.tolist()","3f6acf1e":"answers[answers.str.contains(r\"\\-\")] # I don't understand where the '-' is in 648","f78d2ae6":"answers[answers.str.startswith(r\",\")].tolist(), answers[answers.str.endswith(r\",\")].tolist()","866d4814":"answers[answers.str.contains(r\",\")].tolist()","16395f94":"most_common = Counter(all_chars_ctx).most_common(50)\n\"\".join([x[0] for x in most_common])","cb21a88a":"px.bar(x=[x[0] for x in most_common], y=[x[1] for x in most_common], labels={\"x\": \"character\", \"y\": \"count\"})","9ebd85de":"most_common_ans = Counter(all_chars_ans).most_common(50)\npx.bar(x=[x[0] for x in most_common_ans], y=[x[1] for x in most_common_ans], labels={\"x\": \"character\", \"y\": \"count\"})","ce383622":"px.histogram(df, x=\"num_tokens_context\", color=\"language\")","628a01e7":"# What fraction of the contexts are below a certain length?\n\nonly_hindi = df[df[\"language\"]==\"hindi\"]\nonly_tamil = df[df[\"language\"]==\"tamil\"]\nnum_hindi = len(only_hindi)\nnum_tamil = len(only_tamil)\n\nlengths = list(range(0, df[\"num_tokens_context\"].max(), 25))\nhindi_counts = []\ntamil_counts = []\nfor l in lengths:\n    hindi_counts.append((only_hindi[\"num_tokens_context\"]<=l).sum()\/num_hindi)\n    tamil_counts.append((only_tamil[\"num_tokens_context\"]<=l).sum()\/num_tamil)\n\ncounts_df = pd.DataFrame(data={\"count\": hindi_counts+tamil_counts, \"length\": lengths*2, \"language\": [\"hindi\"]*len(hindi_counts)+[\"tamil\"]*len(tamil_counts)})\n    \npx.line(counts_df, x=\"length\", y=\"count\", color=\"language\", labels={\"count\": \"fraction below length\"})","04519579":"px.histogram(df, x=\"num_chars_context\", color=\"language\")","18ba4c3b":"px.histogram(df, x=\"num_tokens_answer\", color=\"language\")","702087c5":"px.histogram(df, x=\"num_chars_answer\", color=\"language\")","6949e1aa":"px.histogram(df, x=\"num_tokens_question\", color=\"language\")","35711cae":"px.histogram(df, x=\"num_chars_question\", color=\"language\")","aeccc26f":"px.histogram(df, x=\"answer_start\", color=\"language\")","752f0294":"px.scatter(df, x=\"num_tokens_answer\", y=\"answer_start\", color=\"language\")","00fe9036":"px.scatter(df, x=\"num_tokens_context\", y=\"answer_start\", color=\"language\")","f7812fce":"px.scatter(df, x=\"num_tokens_context\", y=\"num_tokens_question\", color=\"language\")","4ade6ad6":"px.scatter(df, x=\"num_tokens_context\", y=\"num_tokens_answer\", color=\"language\")","7caec854":"px.scatter(df, x=\"num_tokens_question\", y=\"num_tokens_answer\", color=\"language\")","c5147aa7":"def count_word_overlap(question, answer):\n    if question.endswith(\"?\"):\n        question = question[:-1]\n    q_splits = question.split()\n    a_splits = answer.split()\n    return sum([a in q_splits for a in a_splits])\n    \ndf[\"num_overlap\"] = [count_word_overlap(question, answer) for question, answer in df[[\"question\", \"answer_text\"]].to_numpy()]\n\npx.histogram(df, x=\"num_overlap\", color=\"language\")","4b6f9821":"def breakup_question(question):\n    if question.endswith(\"?\"):\n        question = question[:-1]\n    return question.split()\n\nhindi_df = df[df[\"language\"]==\"hindi\"]\n\nno_overlap_words = Counter(sum([breakup_question(q) for q in hindi_df[hindi_df[\"num_overlap\"]==0][\"question\"]], []))\n\nmost_common_no_overlap_words = no_overlap_words.most_common(50)\npx.bar(x=[x[0] for x in most_common_no_overlap_words], y=[x[1] for x in most_common_no_overlap_words], labels={\"x\": \"word\", \"y\": \"count\"}, title=\"Hindi words in question with 0 overlap with answer words\")","98d3f06a":"tamil_df = df[df[\"language\"]==\"tamil\"]\n\nno_overlap_words = Counter(sum([breakup_question(q) for q in tamil_df[tamil_df[\"num_overlap\"]==0][\"question\"]], []))\n\nmost_common_no_overlap_words = no_overlap_words.most_common(50)\npx.bar(x=[x[0] for x in most_common_no_overlap_words], y=[x[1] for x in most_common_no_overlap_words], labels={\"x\": \"word\", \"y\": \"count\"}, title=\"Tamil words in question with 0 overlap with answer words\")","8cd4a3fa":"import re\n\ndf[\"num_occurrences\"] = [len(re.findall(ans, ctx)) for ans, ctx in df[[\"answer_text\", \"context\"]].values]\n\nmultiple_answers_df = df[df[\"num_occurrences\"] > 1]\npercent_mult_ans = len(multiple_answers_df)\/len(df)\n\none_answer = df[df[\"num_occurrences\"] == 1]\npercent_one_ans = len(one_answer)\/len(df)\n\n\nprint(\"Num multiple answers in context:\", len(multiple_answers_df))\nprint(\"As a percentage of all samples:\",  percent_mult_ans)\nprint(\"Percentage with one answer:\", percent_one_ans)\nprint(\"Sanity check: percentage with no answer:\", len(df[df[\"num_occurrences\"] == 0]))","b9b1a392":"# Looking at characters by count\n\n#### Note: some figures might be hard to read, but you can use the zoom features to see what the label is for each bar","5c2f4dd7":"# Does the answer start correlate with the length of the context?","b6aee1bc":"# Tokenizing \nI'm using XLM-R","d1aebc52":"# Answer characters\n\nSome numbers showing up","f647f4d4":"# Looking at what types of characters are in the context\n\nIt turns out there are many languages in addition to Hindi and Tamil. I see:\n- English\n- Latin\n- Greek\n- Japanese\n- Chinese\n- Arabic\n- Nepali\n- and many more...\n\nA multi-lingual model will be *very* important","2ff35f95":"# Again, I think ending in a comma is an annotator mistake","b2fb1857":"# Looking at character level","55e48d80":"# Answers with periods are often for numbers or dates.\n\n\u0b95\u0bbf.\u0bae\u0bc1 means BC and \u0b95\u0bbf.\u0baa\u0bbf means AD  \n\u0908.\u092a\u0942. means BC and \u0908 means AD\n\nI would guess that the answers that end in ... or start with . are annotator mistakes.  There are some inconsistencies with AD and BC ending in periods (263 and 288) though it might have to do with the source text. Might be worth probing.","1e12f764":"# Number of tokens in context\n\nLong contexts are probably harder because the model can not look at everything at once. Even Big Bird can't do 14k tokens. Maybe there could be an approach to identify the chunk of 1,000 or 500 tokens from where the answer is likely, and then that smaller chunk goes into the QA model.","5224b978":"# How many words in the question are in the answer?\n\nIn the scenario where your model doesn't produce anything, your best bet might be to take a word from the answer.  Putting nothing is definitely wrong, but using a word from the question has a very small chance of getting points.","19d31f1b":"# Does the length of the question correlate with the length of the context?","598414a7":"# Does the answer start correlate with the length of the answer?","95b7fc9c":"# Does the length of the answer correlate with the length of the question?","952cc0ee":"# Of those words in the questions with no overlap, what are the most common words?\n\nThese are probably stopwords that you should not put in your answer if you are randomly choosing a word from the question.\n\n#### Note: some figures might be hard to read, but you can use the zoom features to see what the label is for each bar","681e293a":"There are actually 212 unknown tokens in all contexts which is a tiny fraction of all tokens. I don't think it would be worth adding new tokens to the tokenizer because the associated embeddings wouldn't be trained very well. If you disagree, please comment!","aba4a7c2":"# Does the length of the answer correlate with the length of the context?","79657f9e":"# Checking for unknown tokens\n\nEdit: September 8, 2021\n\nFor XLM-R, it looks like there are unknowns in the context tokens!\n\nThank you @harveenchadha for finding the mistake in my code!!","2cbadecb":"# How bad could the training set be?\n\nGiven that we know the annotations for the training set are noisy and the `answer_start` values are just the first occurrence of `answer_text`, what is the worst case scenario? The following cell checks each context for how many times the answer is mentioned. If the answer is mentioned more than once, it has a chance of being incorrectly labelled. If it is incorrectly labelled, your model will be penalized when it may have gotten the right answer because the loss is based off of `answer_start`.\n\nAssuming that all the answers are correct (which we know is wrong, [see thread here](https:\/\/www.kaggle.com\/c\/chaii-hindi-and-tamil-question-answering\/discussion\/264395)), it looks like at least 58% of `answer_start` values are correct, but about 41% of the `answer_start` values could be wrong.","09d44f24":"# Now with RemBERT\n\nRemBERT has 119 unknown tokens in the contexts","90b6266b":"# Exploratory Data Analysis \ud83d\udd75\ufe0f\n## Looking at number of tokens, characters, unknown tokens, what languages are in text, and more\n\n#### There might be some useful post-processing approaches that could be explored based on this work\n\n#### Note: some figures might be hard to read in viewer mode, but you can use the plotly zoom features to see what the label is for each tick. Alternatively you can copy and edit the notebook to do your own investigating.","ffe51af8":"# Muril unknown tokens\n\nIt looks like Muril has even more unknowns.","b493b0b1":"# Nothing starts or ends with dashes","738591ce":"# Is it more common for dates to be in Arabic Numerals (0123456789), Devanagari Numerals(\u0966\u0967\u0968\u0969\u096a\u096b\u096c\u096d\u096e\u096f), or Tamil Numerals(\u0be6\u0be7\u0be8\u0be9\u0bea\u0beb\u0bec\u0bed\u0bee\u0bef\u0bf0)?","b1572c26":"# Context characters\nLooks mostly Hindi and Tamil in the top 50 characters","52573046":"## Answer start is probably skewed left because data creators just took the first index of the answer string","3680a7eb":"# Answer characters are less varied\nI noticed one of my predictions having a parenthesis around it which made the jaccard score 0. (1990 compared to 1990. It might be a good idea to clean un-balanced punctuation or non-letters (commas, periods) at the beginning\/end of answers. I might be wrong, but it looks like the only languages in the answer are English, Hindi, and Tamil.","27512e9c":"# About 190 contexts and 120 answers are duplicates"}}