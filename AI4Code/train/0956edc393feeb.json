{"cell_type":{"4b76f84c":"code","442fffaf":"code","122463b6":"code","a9a7117f":"code","66c5fe06":"code","772795d8":"code","accdc95c":"code","7cd543ce":"code","483ce1f4":"code","9bb412d0":"code","75008d68":"code","d29b4c4e":"code","a4847568":"code","47d8a280":"code","b245eae2":"code","f64f3762":"code","7db8163a":"code","800080de":"code","3b5b4c61":"code","238496c7":"code","a23521f5":"code","a37acd43":"code","afed5851":"code","8ba055c5":"code","831edb96":"code","ac092cbd":"code","e0b76181":"code","ccff5ce8":"code","624c4a0d":"code","4f4f7f67":"code","51eb9265":"code","b3ce91b8":"code","75d627ae":"code","3233580d":"markdown","9094ffa6":"markdown","55d67c49":"markdown","566d7b67":"markdown","c150441d":"markdown","818c2665":"markdown","b564266a":"markdown","54329d0c":"markdown","bc55faf9":"markdown","4a912d68":"markdown","1842a78f":"markdown","c274667b":"markdown","cef4ad27":"markdown","e56ac35d":"markdown","6fde4602":"markdown","ae1e268a":"markdown","426fffc6":"markdown","a23ba276":"markdown","2c8bde99":"markdown","cd44f3b5":"markdown","bca56ecd":"markdown","2f41e3ba":"markdown","567ce34a":"markdown","87b1d3d5":"markdown","5ef313a0":"markdown","8d953a08":"markdown","995bd390":"markdown","2c473328":"markdown","899a9f0e":"markdown","32aa0b25":"markdown","7d089d25":"markdown"},"source":{"4b76f84c":"! pip install selenium","442fffaf":"from datetime import datetime as dt\nStartofFile = dt.now()\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.spatial.distance import cosine\n\n\nimport cv2\nimport os\nfrom numba import cuda\nfrom statistics import mode\n\n\n# web-scraping\nimport time\nfrom selenium import webdriver\nimport requests\nimport io\nfrom PIL import Image\nimport os\nimport hashlib\n# import signal\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n#from tensorflow.keras.layers import Rescaling\nfrom tensorflow.keras.applications import VGG16","122463b6":"start = dt.now()\n\nif len(os.listdir('..\/input\/frames-of-video\/train_frame')) == 0:\n    # Opens the Video file\n    cap= cv2.VideoCapture('.\/dataset\/train.mp4')\n\n    #Breaking it into frames\n    i=0\n    while(cap.isOpened()):\n        ret, frame = cap.read()\n        if ret == False:\n            break\n        cv2.imwrite('.\/train_frame\/frame_'+str(i)+'.jpg', frame)\n        i+=1\n        \n        if not i % 100:\n            print(\"Frame Processed : \", i)\n    \n    cap.release()\n    cv2.destroyAllWindows()\n\nelse:    \n    print(\"Frames Already Processed.\")\n\nprint(\"Time for Execution :\", dt.now() - start)","a9a7117f":"train_df = pd.read_csv(\"..\/input\/dataset-hackerearth-dravya\/train.csv\")\ntrain_df.info()","66c5fe06":"start = dt.now()\n\nif len(os.listdir('..\/input\/frames-of-video\/test_frame')) == 0:\n    # Opens the Video file\n    cap= cv2.VideoCapture('.\/dataset\/test.mp4')\n\n    #Breaking it into frames\n    i=0\n    while(cap.isOpened()):\n        ret, frame = cap.read()\n        if ret == False:\n            break\n        cv2.imwrite('.\/test_frame\/frame_'+str(i)+'.jpg', frame)\n        i+=1\n        \n        if not i % 100:\n            print(\"Frame Processed : \", i)\n    \n    cap.release()\n    cv2.destroyAllWindows()\n\nelse:    \n    print(\"Frames Already Processed.\")\n\nprint(\"Time for Execution :\", dt.now() - start)","772795d8":"start = dt.now()\n\n# All Train Frames\ntrain_frame_dir=\"..\/input\/frames-of-video\/train_frame\/train_frame\"\ntrain_frame_arr = os.listdir(train_frame_dir)\n\ntrain_frame_nparray = []\n\nfor i in range(len(train_frame_arr)):\n    img = cv2.imread(train_frame_dir + \"\/frame_\" + str(i) + \".jpg\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (150,150))\n    \n    # Convert i to str(i) to avoid auto sorting.\n    train_frame_nparray.append(img) \n    \ntrain_frame_nparray = np.array(train_frame_nparray)\/255.0\ntrain_frame_nparray = train_frame_nparray.reshape(-1, 150,150, 3)\n\nprint(\"Time for Execution :\", dt.now() - start)","accdc95c":"print(\"Frames Extacted from train.mp4 Shape: \", train_frame_nparray.shape)","7cd543ce":"start = dt.now()\n\n# All Train Frames\ntest_frame_dir=\"..\/input\/frames-of-video\/test_frame\/test_frame\"\ntest_frame_arr = os.listdir(test_frame_dir)\n\ntest_frame_nparray = []\n\nfor i in range(len(test_frame_arr)):\n    img = cv2.imread(test_frame_dir + \"\/frame_\" + str(i) + \".jpg\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (150,150))\n    \n    # Convert i to str(i) to avoid auto sorting.\n    test_frame_nparray.append(img) \n    \ntest_frame_nparray = np.array(test_frame_nparray)\/255.0\ntest_frame_nparray = test_frame_nparray.reshape(-1, 150,150, 3)\n\nprint(\"Time for Execution :\", dt.now() - start)","483ce1f4":"print(\"Frames Extacted from train.mp4 Shape: \", test_frame_nparray.shape)","9bb412d0":"conv_base = VGG16(weights='imagenet',\n                  include_top=False,\n                  pooling='max',\n                  input_shape=(150, 150, 3))","75008d68":"conv_base.summary()","d29b4c4e":"start = dt.now()\nif not os.path.isfile('..\/input\/private-hackerearth\/VGG16_test_raw_features.npy'):\n    output_test = conv_base.predict(test_frame_nparray, batch_size=32)\n    np.save('.\/VGG16_test_raw_features.npy', output_test) # save\nelse:\n    output_test = np.load('..\/input\/private-hackerearth\/VGG16_test_raw_features.npy')\ntry:\n    del test_frame_nparray # releasing memory\nexcept:\n    pass\n\nprint(\"Time for Execution: \", dt.now() - start)\n","a4847568":"start = dt.now()\nif not os.path.isfile('..\/input\/private-hackerearth\/VGG16_train_raw_features.npy'):\n    output_train = conv_base.predict(train_frame_nparray, batch_size=32)\n    np.save('.\/VGG16_train_raw_features.npy', output_test) # save\nelse:\n    output_train = np.load('..\/input\/private-hackerearth\/VGG16_train_raw_features.npy')\n    \ntry:\n    del train_frame_nparray # releasing memory\nexcept:\n    pass\n\nprint(\"Time for Execution: \", dt.now() - start)","47d8a280":"# Reset Graphics Card Memory\ncuda.select_device(0)\ncuda.close()\ncuda.select_device(0)","b245eae2":"print(\"train.mp4 Data :\", output_train.shape)\nprint(\"test.mp4 Data :\", output_test.shape)","f64f3762":"cosine_similarities_train = []\ncosine_similarities_test = []\n\n\nfor i in range(1, len(output_train)):\n    cosine_similarities_train.append(cosine(output_train[i], output_train[i-1]))\n    \nfor i in range(1, len(output_test)):    \n    cosine_similarities_test.append(cosine(output_test[i], output_test[i-1]))\n    ","7db8163a":"threshold_similarity = 0.1 # \nframe_cuts_train = []\nframe_cuts_test = []\n\n\nfor index,i in enumerate(cosine_similarities_train):\n    if i > threshold_similarity:\n        frame_cuts_train.append(round(index\/30))\n        \nfor index,i in enumerate(cosine_similarities_test):\n    if i > threshold_similarity:\n        frame_cuts_test.append(round(index\/18.32))       \n","800080de":"print(\"train.mp4 cuts\")\nprint(*frame_cuts_train)","3b5b4c61":"print(\"test.mp4 cuts\")\nprint(*frame_cuts_test)","238496c7":"def fetch_image_urls(query:str, max_links_to_fetch:int, wd:webdriver, sleep_between_interactions:int=1):\n    def scroll_to_end(wd):\n        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n        time.sleep(sleep_between_interactions)    \n    \n    # build the google query\n    search_url = \"https:\/\/www.google.com\/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n\n    # load the page\n    wd.get(search_url.format(q=query))\n\n    image_urls = set()\n    image_count = 0\n    results_start = 0\n    while image_count < max_links_to_fetch:\n        scroll_to_end(wd)\n\n        # get all image thumbnail results\n        thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n        number_results = len(thumbnail_results)\n        \n        print(f\"Found: {number_results} search results. Extracting links from {results_start}:{number_results}\")\n        \n        for img in thumbnail_results[results_start:number_results]:\n            # try to click every thumbnail such that we can get the real image behind it\n            try:\n                img.click()\n                time.sleep(sleep_between_interactions)\n            except Exception:\n                continue\n\n            # extract image urls    \n            actual_images = wd.find_elements_by_css_selector('img.n3VNCb')\n            for actual_image in actual_images:\n                if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n                    image_urls.add(actual_image.get_attribute('src'))\n\n            image_count = len(image_urls)\n\n            if len(image_urls) >= max_links_to_fetch:\n                print(f\"Found: {len(image_urls)} image links, done!\")\n                break\n        else:\n            print(\"Found:\", len(image_urls), \"image links, looking for more ...\")\n            time.sleep(30)\n            return\n            load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n            if load_more_button:\n                wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n\n        # move the result startpoint further down\n        results_start = len(thumbnail_results)\n\n    return image_urls\n\ndef persist_image(folder_path:str,url:str):\n    try:\n        image_content = requests.get(url).content\n\n    except Exception as e:\n        print(f\"ERROR - Could not download {url} - {e}\")\n\n    try:\n        image_file = io.BytesIO(image_content)\n        image = Image.open(image_file).convert('RGB')\n        file_path = os.path.join(folder_path,hashlib.sha1(image_content).hexdigest()[:10] + '.jpg')\n        with open(file_path, 'wb') as f:\n            image.save(f, \"JPEG\", quality=85)\n        print(f\"SUCCESS - saved {url} - as {file_path}\")\n    except Exception as e:\n        print(f\"ERROR - Could not save {url} - {e}\")\n\n# function Distabled not working in Windows\n'''\ndef handler(signum, frame):\n    print ('Signal handler called with signal', signum)\n         # Set the signal handler and a 5-second alarm\n        #signal.signal(signal.SIGALRM, handler)\n        #signal.alarm(5)\n'''\n\ndef search_and_download(search_term:str,driver_path:str,target_path='.\/images',number_images=100):\n    target_folder = os.path.join(target_path,'_'.join(search_term.lower().split(' ')))\n\n    if not os.path.exists(target_folder):\n        os.makedirs(target_folder)\n\n    with webdriver.Chrome(executable_path=driver_path) as wd:\n        res = fetch_image_urls(search_term, number_images, wd=wd, sleep_between_interactions=0.5)\n    \n    print(len(res))\n    for index,elem in enumerate(res):\n        print(index)\n        \n        # Set the signal handler and a 5-second alarm\n        #signal.signal(signal.SIGALRM, handler)\n        #signal.alarm(5)\n\n        # This function may hang indefinitely\n        persist_image(target_folder,elem)\n\n        #signal.alarm(0)\n        ","a23521f5":"start = dt.now()\n\nClasses = [\"Parrot\", \"Squirrel\", \"Monkey\", \"Duck\", \"Peacock\", \"Deer\", \n           \"Elephant\", \"Tiger\", \"Pelican\", \"Bear\", \"Penguin\", \"Turtle\"]\n\nPath = \"..\/input\/web-scraped-animals-data\/\"\n\nif len(os.listdir(Path)) == 0:\n    for cl in Classes:\n        search_and_download(cl, \".\/chromedriver.exe\", \".\/Scraped_Data\/\")  \n    print(\"Data Scraped Success.\")\nelse:\n    print(\"Scraped Data already exists.\")\n        \nprint(\"Time for Execution: \", dt.now() - start)","a37acd43":"start = dt.now()\n\ndata = pd.DataFrame(columns = ['index','image_path','class'])\n\nindex = 0\nfor cl in Classes:\n    files = os.listdir(Path + cl)\n    #print(len(files))\n    for file in files:\n        index += 1\n        img_path = Path + cl + '\/' + file\n        #print(img_path)\n        data.loc[len(data.index)] = [index, img_path, cl] \n        #row_dict = {'index': index,'image_path': img_path,'class': cl}\n        #data.append(row_dict, ignore_index=True)\n        #print(cl,file)\n        #break\n\ndata.to_csv('.\/data_no_image_read.csv',index=False)\n\nprint(\"Time For Execution: \", dt.now()-start)","afed5851":"start = dt.now()\n\nClasses = [\"parrot\", \"squirrel\", \"monkey\", \"duck\", \"peacock\", \"deer\", \n           \"elephant\", \"tiger\", \"pelican\", \"bear\", \"penguin\", \"turtle\"]\n\nPath = \"..\/input\/web-scraped-animals-data\/\"\n\ndata = pd.DataFrame(columns = ['index','image_path','class'])\n\nindex = 0\nfor cl in Classes:\n    files = os.listdir(Path + cl)\n    #print(len(files))\n    for file in files:\n        index += 1\n        img_path = Path + cl + '\/' + file\n        #print(img_path)\n        data.loc[len(data.index)] = [index, img_path, cl] \n        #row_dict = {'index': index,'image_path': img_path,'class': cl}\n        #data.append(row_dict, ignore_index=True)\n        #print(cl,file)\n        #break\n\n#data.to_csv('.\/data_no_image_read.csv',index=False)\n\ndata_images_read = []\n\nfor index, row in data.iterrows():\n    \n    \n    img = cv2.imread(row['image_path'])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (150,150))\n    \n    # Convert i to str(i) to avoid auto sorting.\n    data_images_read.append(img) \n\nprint(\"Time For Execution: \", dt.now()-start)","8ba055c5":"def one_hot_encode_labels(string):\n    one_hot = np.array([0,0,0,0,0,0,0,0,0,0,0,0])\n    for i in range(0,11):\n        if string == Classes[i]:\n            one_hot[i] = 1\n    return one_hot    ","831edb96":"data['images_numpy'] = data_images_read\ndata['labels_one_hot'] = data['class'].apply(one_hot_encode_labels)\n\nX_train = np.array(list(data['images_numpy']))\/255.0\ny_train = np.array(list(data['labels_one_hot']))","ac092cbd":"Config = {\n    'input_': (150,150,3),\n    'epochs': 100,\n    'epochs_fine_tuning': 20,\n    'learning_rate': 1e-5\n}","e0b76181":"base_model = keras.applications.Xception(\n    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n    input_shape=Config['input_'],\n    include_top=False,\n)  # Do not include the ImageNet classifier at the top.\n\n\ndata_augmentation = keras.Sequential(\n    [\n        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n        layers.experimental.preprocessing.RandomRotation(0.1),\n    ], name = \"DataAugmentationModel\"\n)\n\n\n# Freeze the base_model\nbase_model.trainable = False\n\n# Create new model on top\ninputs = keras.Input(Config['input_'])\n#x=inputs\nx = data_augmentation(inputs)  # Apply random data augmentation\n# from (0, 255) to a range (-1., +1.), the normalization layer\n# does the following, outputs = (inputs - mean) \/ sqrt(var)\nnorm_layer = keras.layers.experimental.preprocessing.Normalization()\n# Here keeping mean as 127.5 is the trick now values lower \n#than that will go in negative side\nmean = np.array([127.5] * 3)\nvar = mean ** 2\n# Scale inputs to [-1, +1]\n\n# These Weights must be added to the model after it is added to to model otherwise it will not know how number of weights needed.??\n# This is Normalization layer is mind F**King me now.\n# Read this later https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/experimental\/preprocessing\/Normalization\nx = norm_layer(x)\nnorm_layer.set_weights([mean, var])\n\n# The base model contains batchnorm layers. We want to keep them in inference mode\n# when we unfreeze the base model for fine-tuning, so we make sure that the\n# base_model is running in inference mode here.\nx = base_model(x, training=True)\nx = keras.layers.GlobalAveragePooling2D()(x)\nx = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\noutputs = keras.layers.Dense(12, activation='softmax')(x)\n# This will return the probability score for all the 12 classes.\n\nmodel = keras.Model(inputs, outputs)\n\nmodel.summary()","ccff5ce8":"model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n\nmodel.fit(x=X_train, y=y_train, epochs=Config['epochs'])","624c4a0d":"# Unfreeze the base_model. Note that it keeps running in inference mode\n# since we passed `training=False` when calling it. This means that\n# the batchnorm layers will not update their batch statistics.\n# This prevents the batchnorm layers from undoing all the training\n# we've done so far.\nbase_model.trainable = True\nmodel.summary()\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-5),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n\nmodel.fit(x=X_train, y=y_train, epochs=Config['epochs_fine_tuning'])\n","4f4f7f67":"start = dt.now()\n\n\ntest_images_read = []\n\nfor i in range(2071):\n    \n    img = cv2.imread(\"..\/input\/frames-of-video\/test_frame\/test_frame\/frame_\" + str(i) + \".jpg\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (150,150))\n    \n    # Convert i to str(i) to avoid auto sorting.\n    test_images_read.append(img) \n\n    \nX_train = np.array(list(test_images_read))\/255.0\n\ny_pred = model.predict(X_train)\n\nprint(\"Time For Execution: \", dt.now()-start)","51eb9265":"y_pred[0:2]","b3ce91b8":"labels_df = pd.DataFrame(columns=['Frame_ID','Animal'])\n\nClas = [\"Parrot\", \"Squirrel\", \"Monkey\", \"Tiger\", \"Peacock\", \"Deer\", \n           \"Elephant\", \"Duck\", \"Pelican\", \"Bear\", \"Penguin\", \"Turtle\"]\n\nprint(len(y_pred))\n\ndict_frame = {}\n\nfor i in range(0,113):\n    dict_frame[i] = []\n\nfor index, i in enumerate(y_pred):\n    i = list(i)\n    dict_frame[int(index\/\/18.32)].append(i.index(max(i)))\n    \n    #labels_df.loc[len(labels_df.index)] = [] \n    #rint(index\/\/18.32, i.index(max(i)))\n    \nfor i in range(0,113):\n   # print(\"Test_\" + str(i) + \".jpg\")\n    #print(Clas[mode(dict_frame[i])])\n    #print([\"Test_\" + str(i) + \".jpg\", Clas[mode(dict_frame[i])]])\n    labels_df.loc[len(labels_df)] = [\"Test_\" + str(i) + \".jpg\", Clas[mode(dict_frame[i])] ]\n    \nlabels_df.to_csv('sample_submission_DL.csv', index = False)","75d627ae":"####END OF FILE","3233580d":"**Note:** In the real word we must take atleast 5000 images for each class in a 12 Class Classification Problem and apply data cleaning but for this notebook due to time constraint, I am just considing 50-100 images and even less just to make sure I convey concept clearly because I think intent of this task is understand the concept applied rather than result in 6 hours, since length of test.mp4 is just 112 seconds(it will take less than 2 mins for manual labelling). And whatever class is not able to classified we will consider it as unknown.","9094ffa6":"# Web Scraping Data","55d67c49":"_This code may halt in between multiple times because of 1. network issue and 2. Google may block the request many times._ \n\n_If this error persist please run for each class individually._","566d7b67":"### FEATURE EXTRACTION using VGG16","c150441d":"## Build a model","818c2665":"_Lighter model may be preffered to avoid OFM Error._","b564266a":"## train.mp4","54329d0c":"## Table of Content\n\n1. INTRODUCTION\n2. EXPLORING DATA<br>\n&emsp; 2.1. train.mp4<br>\n&emsp; 2.2 train.csv\n3. PRE-PROCESSING DATA<br>\n&emsp; 3.1. TRAIN DATA<br>\n&emsp; 3.2. TEST DATA\n4. WEB SCRAPING DATA\n5. DEEP LEARNING MODEL","bc55faf9":"I want to stress more than INSCUFFICENT DATA.","4a912d68":"#### Xception is complex architecture.","1842a78f":"This is used to find scene cuts in video.","c274667b":"### Do a round of fine-tuning of the entire model","cef4ad27":"9### Solution Mini Challenge 1\n\n1. Upon few experiments we will fill the majority voter in segment of frames.\n2. There is very less training data per category 1-2 clips per class of animals. Thus suggesting INSUFFICENT DATA.","e56ac35d":"# Deep Learning Model","6fde4602":"### Mini - Challenge 1\n\n1. How to decide which frame in video belong to frame in train.csv\n2. How many mini-clips are present to classify 1 class","ae1e268a":"### Mini - Observation\n<br><br>\nframes_per_seconds = (Total Frames\/Total Length of Video)\n\n\n- train.csv contain 408 Frames and length of video is about 406 Second. i.e. 1 Second is considered as 1 Frame Approximately\n- train.mp4 has 12195 Frames approximately 30 frames_per_seconds.\n<br><br>\n- test.csv contain 112 Frames and length of video is about 112 Second. i.e. 1 Second is considered as 1 Frame Approximately\n- test.mp4 has 2071 Frames approximately 18.32 frames_per_seconds.\n\n","426fffc6":"Disclaimer: Here I have not used any pre trained model other than pretrained VGG16 & Xception on imagenet data to show the reader about the concept which can be used if we use deep learning approch. For the best result we can we pre_defined_api by Hugging face(transformer) or pre-trained model. I observed serious lack of training time and training data.\n\n- For submission I have used solution from library : https:\/\/huggingface.co\/victor\/animals-classifier which has accuracy of 98.2%.","a23ba276":"### Train the top layer","2c8bde99":"# PRE-PROCESSING DATA","cd44f3b5":"My name is **Yash Gupta**. eryash15@gmail.com, +91-8871015475<br>\n\nI am excited to introduce myself as a potential candidate for opportunities stated with company. I am currently working as Machine Learning Engineer with Tata Consultancy Services and previously as Data Engineer. <br>\n\nI am Kaggle Expert (Dataset & Discussions). Published about 20 Notebooks & 25 Articles. <br>\n\nYour job listing mentions a need for someone with expertise and experience in Deep Learning. I have worked professionally in most of the edge cutting technologies in the market. I expertise in the skills in: \n<br>\n**Deep Learning \u00b7 Transformers (Hugging Face) \u00b7 Supervised Learning \u00b7 Unsupervised Learning \u00b7 Reinforcement Learning \u00b7 Natural Language Processing \u00b7 Computer Vision \u00b7 Transfer Learning**\n<br>\nAlthough, I feel most important skill of 2021 is googling problems.<br>\n\nI look forward to expressing my diligence for this role at company and how can I be a valuable resource to this esteemed organisation? <br>\nThank you in advance, for following up to tell me about your decision.\n","bca56ecd":"- We add a Rescaling layer to scale input values (initially in the [0, 255] range) to the [-1, 1] range.\n- We add a Dropout layer before the classification layer, for regularization.\n- We make sure to pass training=False when calling the base model, so that it runs in inference mode, so that batchnorm statistics don't get updated even after we unfreeze the base model for fine-tuning.","2f41e3ba":"# EXPLORING DATA","567ce34a":"## test.mp4","87b1d3d5":"Refference:  https:\/\/towardsdatascience.com\/image-scraping-with-python-a96feda8af2d","5ef313a0":"### Finding Frame Cuts in Video","8d953a08":"# INTRODUCTION","995bd390":"Cell below is highly likely to give Memory error. I suggest 2 solution.\n1. Use GPU memory around 24 GB Like I have used.\n2. Clean all the variable and process and mini-batches.","2c473328":"### Prediction on Train and Test Data","899a9f0e":"## train.csv","32aa0b25":"## TRAIN DATA","7d089d25":"## TEST DATA"}}