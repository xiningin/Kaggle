{"cell_type":{"d6b95044":"code","ad5ff646":"code","77388be1":"code","634d1562":"code","4f3673c1":"code","b44c8339":"code","b3d8353b":"code","71ecf2fe":"code","1cfcd901":"code","28f665d8":"code","cb6b0b6c":"code","050065b0":"code","782739d4":"code","00c841e2":"code","381db00d":"code","b2dda3d1":"code","79a712ae":"code","91cd1784":"code","8a36b33c":"code","4fa1807f":"code","394d20b6":"markdown","3e731534":"markdown","83fb5fb3":"markdown","80000101":"markdown","a1e21053":"markdown","7510b3c3":"markdown","de470989":"markdown","14db3952":"markdown","07a1d2d3":"markdown","a64fa25e":"markdown","c277e67c":"markdown","5b8abfed":"markdown","023ada1e":"markdown"},"source":{"d6b95044":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\nimport seaborn as sns\n%matplotlib inline\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport sklearn \nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom sklearn.model_selection import train_test_split\nfrom nltk.corpus import stopwords\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import TreebankWordTokenizer\nfrom sklearn.feature_extraction.text import TfidfTransformer \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nfrom wordcloud import WordCloud","ad5ff646":"data = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\",encoding='latin-1')\ndata.head()","77388be1":"Sum = data.isnull().sum()\nPercentage = (data.isnull().sum()\/data.isnull().count())\nvalues = pd.DataFrame([Sum,Percentage])\nprint(values)","634d1562":"data = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\",encoding='latin-1')\ndata.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1, inplace = True)\ndata.columns =['label','text']\n\ndata.head()","4f3673c1":"colors = ['#ff9999','#66b3ff']\ndata[\"label\"].value_counts().plot(kind = 'pie',colors = colors ,explode = (0.1,0),autopct = '%1.1f%%')","b44c8339":"stop_words = set(stopwords.words('english'))\nspam_text = data.loc[data['label'] == 'spam']\nham_text = data.loc[data['label'] == 'ham']\n\ncount_Ham = Counter(\" \".join(data[data['label']=='ham'][\"text\"]).split()).most_common(100)\ncommon_Ham = pd.DataFrame.from_dict(count_Ham)[0]\ncommon_ham = common_Ham.str.cat(sep=' ')\n\ncount_Spam = Counter(\" \".join(data[data['label']=='spam'][\"text\"]).split()).most_common(100)\ncommon_Spam = pd.DataFrame.from_dict(count_Spam)[0]\ncommon_spam = common_Spam.str.cat(sep=' ')\n\nwordcloud = WordCloud(stopwords=stop_words,background_color = \"white\")\nwordcloud.generate(common_ham)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n\n\nwordcloud = WordCloud(stopwords=stop_words,background_color = \"white\")\nwordcloud.generate(common_spam)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","b3d8353b":"data.groupby(\"label\").describe()","71ecf2fe":"data['length'] = data['text'].apply(len)\ndata =pd.get_dummies(data, columns=['label'], prefix = 'Dummy' ,drop_first = True)","1cfcd901":"# stop  = stopwords.words('english')\n# data['text'].apply(lambda x: [item for item in x if item not in stop])\n# data.head()","28f665d8":"all_sent = []\nfor text in data.text:\n    all_sent.append(text.lower())\n\ncommon_sent = nltk.FreqDist(all_sent).most_common(10)\ndisplay(common_sent)","cb6b0b6c":"X=data['length'].values[:,None]\ny= data['Dummy_spam']\nX_train,X_test,y_train,y_test=train_test_split(X,y)","050065b0":"plt.style.use('seaborn-pastel')\ndata.hist(column='length',by='Dummy_spam',figsize=(10,5), bins=100, label = (\"Ham\",\"Spam\") )\nplt.xlim(-40,800)\nplt.ioff()","782739d4":"models = []\nmodels.append(['LR', LogisticRegression(solver='lbfgs')])\nmodels.append(['SVM', svm.SVC(gamma='auto')])\nmodels.append(['RF', RandomForestClassifier(n_estimators=1000, max_depth=10)])\nmodels.append(['NN', MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(150, 10))])\nmodels.append(['KNN', KNeighborsClassifier()])\nmodels.append(['DTC', DecisionTreeClassifier()])\nmodels.append(['MNB', MultinomialNB(alpha=0.2)])\nmodels.append(['ABC', AdaBoostClassifier(n_estimators=100)])\nprint('Done')","00c841e2":"results = []\nfor name, model in models:\n    wine_model = model\n    wine_model.fit(X_train, y_train)\n    pred = wine_model.predict(X_test)\n    acc = accuracy_score(y_test, pred)\n    precision = precision_score(y_test, pred, average=None)\n    recall = recall_score(y_test, pred, average= None)\n    error_Rate = 1- acc\n    cm = pd.DataFrame(metrics.confusion_matrix(y_test,pred),index=['ham','spam'],columns=['ham','spam'])\n    print('Model tested: {}'.format(name))\n    print('Confusion Matrix')\n    print(cm)\n    print('Accuracy= {}'.format(acc))\n    print('Error Rate= {}'.format(error_Rate))\n    print('Recall Rate= {}'.format(recall))\n    print(\"Precision Rate: {}\".format(precision))\n    print(metrics.classification_report(y_test,pred))\n    print()\n    results.append([name, precision])","381db00d":"spam_text = data[data[\"Dummy_spam\"] == 1][\"text\"]\nham_text = data[data[\"Dummy_spam\"] == 0][\"text\"]\n","b2dda3d1":"X = data['text']\ny = data['Dummy_spam']\nX_train,X_test,y_train,y_test=train_test_split(X,y)\nresults2= []\n\nfor name, model in models:\n    text_clf=Pipeline([('tfidf',TfidfVectorizer(stop_words='english')),(name,model)])\n    text_clf.fit(X_train, y_train)\n    pred = text_clf.predict(X_test)\n    acc = accuracy_score(y_test, pred)\n    precision = precision_score(y_test, pred, average=None)\n    recall = recall_score(y_test, pred, average= None)\n    error_Rate = 1- acc\n    cm = pd.DataFrame(metrics.confusion_matrix(y_test,pred),index=['ham','spam'],columns=['ham','spam'])\n    print('Model tested: {}'.format(name))\n    print('Confusion Matrix')\n    print(cm)\n    print('Accuracy= {}'.format(acc))\n    print('Error Rate= {}'.format(error_Rate))\n    print('Recall Rate= {}'.format(recall))\n    print(\"Precision Rate: {}\".format(precision))\n    print(metrics.classification_report(y_test,pred))\n    print()\n    results2.append([name,precision])","79a712ae":"df1 = pd.DataFrame.from_items(results,orient='index', columns=['Accuracy length'])\ndf2 = pd.DataFrame.from_items(results2,orient='index', columns=['Accuracy words of bag'])\ndf = pd.concat([df1,df2],axis=1)\ndf.plot(kind='bar', figsize=(12,6), align='center')\nplt.xticks(np.arange(9), df.index)\nplt.ylabel('Accuracy Score')\nplt.title('Accuracy by Classifier')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2)","91cd1784":"def get_final_text(stemmed_text):\n    final_text=\" \".join([word for word in stemmed_text])\n    return final_text","8a36b33c":"stemmer = PorterStemmer()\ndata = pd.read_csv(\"..\/input\/sms-spam-collection-dataset\/spam.csv\",encoding='latin-1')\ndata.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1, inplace = True)\ndata.columns =['label','text']\ndata.text = data['text'].str.split()\ndata['stemmed_text'] = data['text'].apply(lambda x: [stemmer.stem(y) for y in x])\ndata['final_text']=data.stemmed_text.apply(lambda row : get_final_text(row))\ndata.head()","4fa1807f":"X = data['final_text']\ny = data['label']\nX_train,X_test,y_train,y_test=train_test_split(X,y)\nresults3= []\n\nfor name, model in models:\n    text_clf=Pipeline([('tfidf',TfidfVectorizer(stop_words='english')),(name,model)])\n    text_clf.fit(X_train, y_train)\n    pred = text_clf.predict(X_test)\n    acc = accuracy_score(y_test, pred)\n    precision = precision_score(y_test, pred, average=None)\n    recall = recall_score(y_test, pred, average= None)\n    error_Rate = 1- acc\n    cm = pd.DataFrame(metrics.confusion_matrix(y_test,pred),index=['ham','spam'],columns=['ham','spam'])\n    print('Model tested: {}'.format(name))\n    print('Confusion Matrix')\n    print(cm)\n    print('Accuracy= {}'.format(acc))\n    print('Error Rate= {}'.format(error_Rate))\n    print('Recall Rate= {}'.format(recall))\n    print(\"Precision Rate: {}\".format(precision))\n    print(metrics.classification_report(y_test,pred))\n    print()\n    results3.append([name,precision])","394d20b6":"* Import Libraries ","3e731534":"remove stopwords ","83fb5fb3":"* Last columns seem to be unnecessary, check if there are values in there.","80000101":"13.4% of dataset are spam messages, other 86.6% are not","a1e21053":"* Add variable length \n* Change spam to dummy -> spam = 1, ham = 0","7510b3c3":"* Read in the Data and check first 5 elements","de470989":"* Most popular \"ham\" text: 'Sorry, I'll call later', recurse 30 times.\n* Most popular \"spam\" text= 'Please call our customer service representative ...', recurse 4 times.","14db3952":"1. Classification based on length of text","07a1d2d3":"TODO: Word Cloud ","a64fa25e":"* More than 99% empty decide to remove columns \n* Rename columns \n* Other columns have no missing values ","c277e67c":"occurrences to frequencies\nhttps:\/\/scikit-learn.org\/stable\/tutorial\/text_analytics\/working_with_text_data.html#from-occurrences-to-frequencies","5b8abfed":"2. Countvectorizer: \n    - TF-IDF Vectorizer: \n    - stopwords get removed \n    - Words NOT Stemmed","023ada1e":"3. Countvectorizer: \n    - TF-IDF Vectorizer: \n    - stopwords get removed \n    - Words Stemmed"}}