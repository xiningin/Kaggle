{"cell_type":{"6b516a56":"code","363d643b":"code","e8c45676":"code","d7fa68b5":"code","ff08bca4":"code","49d2e430":"code","c5034fc6":"code","05b69df3":"code","67bbf7e2":"code","6e66c044":"code","5623b071":"code","8ad30f3e":"code","03bb6b1d":"code","b9fd9fbd":"code","a1471a83":"code","ecd7ef7b":"code","fbb36007":"code","97812854":"code","40727fda":"code","94d33629":"code","4069ac8a":"code","fdde83d7":"code","572f4a13":"code","771c9721":"code","2036c125":"code","c252fd83":"code","ffb5e274":"code","c2b85d4a":"code","b42b8440":"code","9590996f":"code","7bc38940":"code","d26a0d0a":"code","899519e4":"code","69605313":"code","68c0b248":"code","c36858c8":"markdown","b95955a2":"markdown","a692b358":"markdown","76d35e51":"markdown","7c565eab":"markdown"},"source":{"6b516a56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Lasso\nfrom scipy import stats\nfrom sklearn.preprocessing import QuantileTransformer\n\nplt.style.use('ggplot')\n\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","363d643b":"test_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')\ntrain_data = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')","e8c45676":"train_data.info()","d7fa68b5":"test_data.info()","ff08bca4":"plt.figure(figsize=(15,10))\nsns.scatterplot(train_data.id, train_data.target)\nplt.title('Target against ID')","49d2e430":"train_cols = [col for col in list(train_data) if col != 'id']\ntest_cols = train_cols.copy()\ntest_cols.pop(-1)","c5034fc6":"train_data.describe()","05b69df3":"test_data.describe()","67bbf7e2":"train_data[train_cols].hist(figsize=(20,20), bins=100)\nplt.show()","6e66c044":"test_data[test_cols].hist(figsize=(20,20), bins=100)\nplt.show()","5623b071":"fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(20,20))\n\nfor i, col in enumerate(test_cols):\n    sns.kdeplot(train_data[col], ax=axes[i\/\/4, i%4])\n    sns.kdeplot(test_data[col], ax=axes[i\/\/4, i%4])","8ad30f3e":"plt.figure(figsize=(20,20))\nsns.pairplot(train_data, diag_kind='kde')\nplt.show()","03bb6b1d":"train_data = train_data[train_data['target'] > 0]","b9fd9fbd":"plt.figure(figsize=(20,20))\nsns.heatmap(train_data[train_cols].corr(), annot=True, square=True)\nplt.show()","a1471a83":"plt.figure(figsize=(20,20))\nsns.heatmap(test_data[test_cols].corr(), annot=True, square=True)\nplt.show()","ecd7ef7b":"plt.figure(figsize=(15,7))\nsns.distplot(train_data['cont1'])","fbb36007":"train_data['cont1_level1'] = train_data['cont1'].apply(lambda x: int(x <= 0.41))\ntrain_data['cont1_level2'] = train_data['cont1'].apply(lambda x: int(0.41 < x <= 0.57))\ntrain_data['cont1_level3'] = train_data['cont1'].apply(lambda x: int(0.57 < x <= 0.78))\ntrain_data['cont1_level4'] = train_data['cont1'].apply(lambda x: int(x > 0.78))\n\ntrain_data['cont5_level1'] = train_data['cont5'].apply(lambda x: int(x < 0.29))\n\ntrain_data['cont14_level1'] = train_data['cont14'].apply(lambda x: int(x < 0.53))\ntrain_data['cont13_level1'] = train_data['cont13'].apply(lambda x: int(x < 0.41))","97812854":"plt.figure(figsize=(15,7))\nsns.distplot(train_data['cont13_level1'])\nplt.show()","40727fda":"new_cols = ['cont1_level1', 'cont1_level2', 'cont1_level3', 'cont1_level4', 'cont5_level1', 'cont13_level1', 'cont14_level1']","94d33629":"transformer = QuantileTransformer()\n\nfor col in train_cols:\n    new_col = 'quantile_' + col\n    train_data[new_col] = transformer.fit_transform(np.array(train_data[col]).reshape(-1,1))","4069ac8a":"train_cols = list(train_data)\ntrain_cols.pop(-15)\ntrain_cols.pop(0)","fdde83d7":"train_cols += new_cols","572f4a13":"x = np.array(train_data[train_cols])\ny = np.array(train_data['target'])","771c9721":"x = x[:10000]\ny = y[:10000]","2036c125":"plt.figure(figsize=(20,20))\nsns.heatmap(train_data[train_cols].corr(), annot=True, square=True)\nplt.show()","c252fd83":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)","ffb5e274":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n\npipe = make_pipeline(StandardScaler(), \n                     PolynomialFeatures(), \n                     SelectFromModel(estimator=Lasso(alpha=0.01)), \n                     RandomForestRegressor())","c2b85d4a":"cross_val_score(pipe, x_train, y_train, scoring=make_scorer(mean_squared_error), cv=3, n_jobs=-1, verbose=True)","b42b8440":"np.sqrt(np.array([0.515116  , 0.51311002, 0.53723786]).mean())","9590996f":"cross_val_score(pipe, x_train, y_train, scoring=make_scorer(mean_squared_error), cv=3, n_jobs=-1, verbose=True)","7bc38940":"scaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\nrf = RandomForestRegressor(random_state=42)\nrf.fit(x_train, y_train)\n\ny_pred = rf.predict(x_test)\nprint('RMSE = ', np.sqrt(mean_squared_error(y_pred, y_test)))","d26a0d0a":"cross_val_score(rf, x_train, y_train, scoring=make_scorer(mean_squared_error), cv=3, n_jobs=-1, verbose=True)","899519e4":"sorted(list(zip(train_cols, rf.feature_importances_)), key=lambda x: x[1], reverse=True)","69605313":"lasso = Lasso(alpha=0.01, random_state=42)\nlasso.fit(x_train, y_train)\n\ny_pred = lasso.predict(x_test)\nprint('RMSE = ', np.sqrt(mean_squared_error(y_pred, y_test)))","68c0b248":"sorted(list(zip(train_cols, lasso.coef_)), key=lambda x: x[1], reverse=True)","c36858c8":"\u0412\u0441\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043e\u0431\u043b\u0430\u0434\u0430\u044e\u0442 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u043c\u0438 \u0448\u043a\u0430\u043b\u0430\u043c\u0438, \u043f\u0440\u0438\u043c\u0435\u0440\u043d\u043e \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u0435 min, max \u0432 train \u0438 test.","b95955a2":"\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0438\u0437 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0432 train \u0438 test \u043d\u0430 \u0438\u0434\u0435\u043d\u0442\u0438\u0447\u0435\u043d\u043e\u0441\u0442\u044c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0439:","a692b358":"\u0412\u0438\u0434\u043d\u043e \u044f\u0432\u043d\u043e \u0441\u043a\u043e\u0440\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0435 \u043f\u0430\u0440\u044b \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432.","76d35e51":"\u0412\u0438\u0434\u043d\u043e, \u0447\u0442\u043e \u0435\u0441\u0442\u044c \u0432\u044b\u0431\u0440\u043e\u0441 \u0443 target-a. \u0412 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u043e\u043c \u043a\u0430\u043a\u0438\u0445-\u0442\u043e \u044f\u0432\u043d\u044b\u0445 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u0439 \u043c\u0435\u0436\u0434\u0443 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c\u0438 \u044f \u043d\u0435 \u0432\u0438\u0436\u0443.","7c565eab":"ID \u043d\u0438\u043a\u0430\u043a \u043d\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u0442 \u043e\u0442 \u0442\u0430\u0440\u0433\u0435\u0442\u0430, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0435\u0433\u043e \u043c\u043e\u0436\u0435\u043c \u0441\u043f\u043e\u043a\u043e\u0439\u043d\u043e \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0438\u0445 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430\u0445."}}