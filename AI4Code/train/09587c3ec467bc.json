{"cell_type":{"f1cd11ac":"code","ec521186":"code","5fa2aa01":"code","648ccdde":"code","8c7fe05b":"code","c70af9be":"code","33eef7bc":"code","3cdaf598":"code","966d2a12":"code","cb664783":"code","fb1beb0e":"code","99f48379":"code","91779d72":"code","716ed4c1":"markdown","0b7d12b9":"markdown","d3dd016f":"markdown","aa6c1367":"markdown"},"source":{"f1cd11ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec521186":"f = '\/kaggle\/input\/hubness-for-high-dimensional-datasets\/X_Dexter300from_scikit_hubness.csv'\nX = pd.read_csv(f ,  index_col= 0  )\nX","5fa2aa01":"f = '\/kaggle\/input\/hubness-for-high-dimensional-datasets\/y_Dexter300from_scikit_hubness.csv'\ny = pd.read_csv(f ,  index_col= 0  )\ny","648ccdde":"y = y.values.ravel()","8c7fe05b":"(y==1).sum()\/len(y)","c70af9be":"!pip install scikit-hubness\n","33eef7bc":"%%time \n#from skhubness.data import load_dexter\n#X , y = load_dexter ()\nfrom sklearn.neighbors import KNeighborsClassifier as KNeighborsClassifier_sklearn\nknn_vanilla = KNeighborsClassifier_sklearn(n_neighbors =5 , metric = \"cosine\", )\nfrom sklearn.model_selection import  cross_val_score\nacc_vanilla = cross_val_score(knn_vanilla , X , y , cv =5)\n# Accuracy ( vanilla kNN ):\nprint(f\"{ acc_vanilla.mean():.3f}\")\n# 0.793\n\n#from skhubness.data import load_dexter\n#X , y = load_dexter ()\nfrom skhubness.neighbors import KNeighborsClassifier\nknn_mp = KNeighborsClassifier ( n_neighbors =5 , metric =\"cosine\", hubness=\"mutual_proximity\" )\nfrom sklearn.model_selection import cross_val_score\nacc_mp = cross_val_score (knn_mp , X , y , cv =5)\n# Accuracy ( hubness - reduced kNN )\nprint(f\" { acc_mp.mean():.3f}\")\n# 0.893","3cdaf598":"300*4\/5 == 240 #  Cannot set dim reduction more than that in pca , for CV =5","966d2a12":"from sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\npca = PCA(n_components= 233)\n#pipe = Pipeline([('pca', pca), ('knn_mp', knn_mp )])\n\npipe = Pipeline([('pca', pca), ('knn_vanilla', knn_vanilla )])\n\nacc_mp = cross_val_score (pipe , X , y , cv =5)\nprint(f\" { acc_mp.mean():.3f}\")\n","cb664783":"from sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\npca = PCA(n_components= 234)\npipe = Pipeline([('pca', pca), ('knn_mp', knn_mp )])\n\npipe = Pipeline([('pca', pca), ('knn_vanilla', knn_vanilla )])\n\nacc_mp = cross_val_score (pipe , X , y , cv =5)\nprint(f\" { acc_mp.mean():.3f}\")\n","fb1beb0e":"from sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\npca = PCA(n_components= 239)\nknn_mp = KNeighborsClassifier ( n_neighbors =5 , metric =\"cosine\", hubness='local_scaling')# \"mutual_proximity\" )\n\npipe = Pipeline([('pca', pca), ('knn_mp', knn_mp )])\n\n#pipe = Pipeline([('pca', pca), ('knn_vanilla', knn_vanilla )])\n\nacc_mp = cross_val_score (pipe , X , y , cv =5)\nprint(f\" { acc_mp.mean():.3f}\")\n","99f48379":"from sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\npca = PCA(n_components= 234)\nknn_mp = KNeighborsClassifier ( n_neighbors =5 , metric =\"cosine\", hubness='mutual_proximity')# \"mutual_proximity\" )\n\npipe = Pipeline([('pca', pca), ('knn_mp', knn_mp )])\n\n#pipe = Pipeline([('pca', pca), ('knn_vanilla', knn_vanilla )])\n\nacc_mp = cross_val_score (pipe , X , y , cv =5)\nprint(f\" { acc_mp.mean():.3f}\")\n","91779d72":"from sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\npca = PCA(n_components= 201) # 200 and 202 worsen \nknn_mp = KNeighborsClassifier ( n_neighbors =5 ,  hubness='dis_sim_local')# \"mutual_proximity\" )\n\npipe = Pipeline([('pca', pca), ('knn_mp', knn_mp )])\n\n#pipe = Pipeline([('pca', pca), ('knn_vanilla', knn_vanilla )])\n\nacc_mp = cross_val_score (pipe , X , y , cv =5)\nprint(f\" { acc_mp.mean():.3f}\")\n","716ed4c1":"# Advantage of hubness reduction - example from the paper\n\nstandard KNN from sklearn gives accuracy score 0.793 , with hubness reduction can achieve 0.893","0b7d12b9":"## Dimensional reduction by PCA\n\nreducing to 234-239 (max possible) dimensions + KNN  we can achieve accuracy 0.903 with standard KNN which is better than hub-reduced KNN and dim-red + hub-Red KNN ","d3dd016f":"## Standard vs Hub-reduced KNN classifier","aa6c1367":"# What is about \n\nHere we observe that pca-dimensional reduction combined with standard KNN classifier can be better than Hubness reduced KNN classifier. \n\n\n**Context:** It discussed in some papers that \"hubness phenomena\" (i.e. presense of nodes with high degree in graphs ) creates problems for many machine learning algorithms. In particular for KNN classifier algorithm.\n\nThe Python package \"scikit-hubness\" ( https:\/\/pypi.org\/project\/scikit-hubness\/ , https:\/\/arxiv.org\/abs\/1912.00706 \"scikit-hubness: Hubness Reduction and Approximate Neighbor Search\" ) proposes (in particular) certain \"hubness reduction\" algorithms which sometimes improve the scores for KNN classifiers. (See some in example in the paper.)\n\n**Present notebook:** Notebook presents example from the original paper.\nKNN with hubness reduction provides improvement in accuracy from 0.793 to 0.893 comparing to standard KNN classifier.\nThe dataset is part of Dexter Dataset. \n\nPS\n\nSome other examples of hubness reduction notebooks on kaggle: \n\nhttps:\/\/www.kaggle.com\/alexandervc\/moa-improve-knn-via-scikit-hubness-test-of-idea\n\nhttps:\/\/www.kaggle.com\/alexandervc\/gendatasetsfromopenml-2-hubnes\n"}}