{"cell_type":{"e4bbdd81":"code","7b712c99":"code","f455c060":"code","fd68914b":"code","61e9f2d1":"code","15ed75c6":"code","29f72fa4":"code","f7663e68":"code","0e3f11a5":"code","d239de1a":"code","7a748ad2":"code","03163377":"code","55ef684b":"code","887ab076":"code","0bc96beb":"code","ca1f84ef":"code","add6ba04":"code","fb3a543b":"code","3d841782":"code","4c0fd709":"code","dc8d4fd0":"code","753295ff":"code","57ade495":"code","ad59d993":"code","9672ffd7":"code","bde77a84":"code","8836af57":"code","7ff21825":"code","b5c07a47":"code","2def04b5":"markdown","3fb6b200":"markdown","c72159fe":"markdown","e1a147da":"markdown","c2635466":"markdown","048d29df":"markdown","cc0ae1cc":"markdown"},"source":{"e4bbdd81":"!pip uninstall fastai -y\n!pip install fastai==2.2.7\n!pip install fastaudio\n!pip install fastcore==1.3.19","7b712c99":"### Libraries\n\nimport os, random, math\nimport numpy as np\nimport pandas as pd \nimport geopandas as gpd \nimport matplotlib.pyplot as plt \nimport seaborn as sns \nexec(open('..\/input\/plting\/plt-apple-dark.py').read())\n\nfrom fastaudio.core.all import *\nfrom fastaudio.augment.all import *\nfrom fastai.torch_basics import *\nfrom fastai.vision.all import *\n\nimport torchaudio\nimport librosa.display\nimport librosa\nimport librosa.display\nfrom IPython.display import Audio\nfrom pathlib import Path\n","f455c060":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything()","fd68914b":"torch.cuda.is_available()","61e9f2d1":"training_metadata_df=pd.read_csv('..\/input\/birdclef-2021\/train_metadata.csv')\nprint(f\"Len training data: {len(training_metadata_df.index)} \\n\\n\")\ntraining_metadata_df.sample(2)","15ed75c6":"print(f'Number of different species: {training_metadata_df.primary_label.nunique()}')\nprint(f\"Number of species > 100 audio records: {sum(training_metadata_df['common_name'].value_counts()<100)}\")\npercentage_low = 133\/397*100\npercentage_low = \"{:.2f}\".format(percentage_low)\nprint(f\"At least {percentage_low}% of the records are infrequent\")","29f72fa4":"print(f\"Minimum longitude: {training_metadata_df['longitude'].min()}\")\nprint(f\"Maximum longitude: {training_metadata_df['longitude'].max()}\")\nprint(f\"Minimum latitude: {training_metadata_df['latitude'].min()}\")\nprint(f\"Maximum latitude: {training_metadata_df['latitude'].max()}\")","f7663e68":"df = training_metadata_df.iloc[:, [0,3,4,8,9]]\ngdf = gpd.GeoDataFrame(\n    df, geometry=gpd.points_from_xy(df.longitude, df.latitude))\ngdf_1 = gdf[(gdf.latitude<35) & (gdf.longitude>-30)]\ngdf_2 = gdf[(gdf.latitude<10) & (gdf.longitude<-20)]\ngdf_3 = gdf[(gdf.latitude>=35) & (gdf.longitude>=-40)]\ngdf_4 = gdf[(gdf.latitude>=10) & (gdf.longitude<=-40)]\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nax = world.plot(color='white', edgecolor='black')\n# We can now plot our ``GeoDataFrame``.\ngdf_1.plot(ax=ax, markersize=1)\ngdf_2.plot(ax=ax, markersize=1)\ngdf_3.plot(ax=ax, markersize=1)\ngdf_4.plot(ax=ax, markersize=1)\nplt.show()","0e3f11a5":"total = np.sum([len(gdf_1), len(gdf_2), len(gdf_3), len(gdf_4)])\nassert total == len(training_metadata_df.index)\nprint('The number of records for each quadrant of the earth: \\n' +\n       f'     -> {[len(gdf_1), len(gdf_3), len(gdf_2), len(gdf_4)]}\\n' +\n       f'     -> {[\"Af\", \"EU\", \"SA\", \"NA\"]}')","d239de1a":"audio_files = get_audio_files('..\/input\/audio-flacs-birdclef21\/audio_flac\/')\naudio_files","7a748ad2":"y, sr = torchaudio.load(audio_files[0])\nprint(\"Sample rate:\", sr)\nprint(\"Signal Length:\", len(y))\nprint(\"Duration:\", len(y)\/sr, \"seconds\")\ny = y.numpy()[0]\nprint(\"Signal: \", y)\nprint(\"Shape:\", y.shape)","03163377":"# Anna's Hummingbird\n\nannhum, sr = torchaudio.load('..\/input\/audio-flacs-birdclef21\/audio_flac\/annhum\/XC57971.ogg.flac')\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(annhum.numpy()[0], sr=sr)\nannhum_audio = '..\/input\/audio-flacs-birdclef21\/audio_flac\/annhum\/XC57971.ogg.flac'\nAudio(annhum_audio)","55ef684b":"# Bluejay\n\nblujay, sr = torchaudio.load('..\/input\/audio-flacs-birdclef21\/audio_flac\/blujay\/XC108404.ogg.flac')\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(blujay.numpy()[0], sr=sr)\nblujay_audio = '..\/input\/audio-flacs-birdclef21\/audio_flac\/blujay\/XC108404.ogg.flac'\nAudio(blujay_audio)","887ab076":"# Plot previous species\n\ngdf_blujay = gdf[gdf.primary_label == 'blujay']\ngdf_annhum = gdf[gdf.primary_label == 'annhum']\nworld = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\nax = world.plot(color='white', edgecolor='black')\ngdf_blujay.plot(ax=ax, markersize=0.3, label='bluejay')\ngdf_annhum.plot(ax=ax, markersize=0.3, label='annhum')\nplt.legend()\nplt.xlim([-150, -20])\nplt.ylim([10, 75])\nplt.show()","0bc96beb":"fig, ax = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(15, 10))\n\nsg0 = librosa.stft(annhum.numpy()[0])\nsg_mag, sg_phase = librosa.magphase(sg0)\nsg1 = librosa.feature.melspectrogram(S=sg_mag, sr=sr)\nsg2 = librosa.amplitude_to_db(sg1, ref=np.min)\nlibrosa.display.specshow(sg2, sr=sr, y_axis='mel', fmax=8000, x_axis='time', ax=ax[0])\nax[0].set(title='Annas hummingbird Mel spectrogram')\nax[0].label_outer()\n\nsg0 = librosa.stft(blujay.numpy()[0])\nsg_mag, sg_phase = librosa.magphase(sg0)\nsg1 = librosa.feature.melspectrogram(S=sg_mag, sr=sr)\nsg2 = librosa.amplitude_to_db(sg1, ref=np.min)\nlibrosa.display.specshow(sg2, sr=sr, y_axis='mel', fmax=8000, x_axis='time', ax=ax[1])\nax[1].set(title='Bluejay Mel spectrogram')\nax[1].label_outer()","ca1f84ef":"audio_files","add6ba04":"training_metadata_df.sample(2)","fb3a543b":"class AudioConfig:\n    \"\"\"\n    Custom `AudioToSpec` transform for birdclef \n    \"\"\"\n    birds = config_from_func(\n        transforms.MelSpectrogram,\n        \"Voice\",\n        mel=\"True\",\n        to_db=\"True\",\n        f_min=50.0,\n        f_max=8000.0,\n        n_fft=2048,\n        n_mels=64,\n        hop_length=int(2048 \/\/ 4)\n    )","3d841782":"cfg = AudioConfig.birds()\n\nbatch_tfms = [AudioToSpec.from_cfg(cfg)]\n\nget_y = lambda x: re.split('[\/]', x.as_posix())[-2]\n\naudio_db = DataBlock(blocks = (AudioBlock, CategoryBlock),\n                     get_items = get_audio_files,\n                     splitter = RandomSplitter(),\n                     batch_tfms = batch_tfms, #augments wouldn't work on batch\n                     get_y=get_y)","4c0fd709":"dbunch = audio_db.dataloaders('..\/input\/audio-flacs-birdclef21\/audio_flac', bs=128)\ndbunch.show_batch(figsize=(10, 5))","dc8d4fd0":"audio_db.summary('..\/input\/audio-flacs-birdclef21\/audio_flac')","753295ff":"from fastai.callback.data import CudaCallback\n\nlearn = cnn_learner(dbunch, \n            densenet121,\n            n_in=1,\n            loss_func=CrossEntropyLossFlat(),\n            metrics=[error_rate],\n            cbs=[CudaCallback]).to_fp16()","57ade495":"learn.lr_find() ","ad59d993":"learn.fit_one_cycle(3, 6e-3)","9672ffd7":"learn.unfreeze()\nlearn.lr_find() ","bde77a84":"learn.fit_one_cycle(3, lr_max=slice(1e-6,1e-4))","8836af57":"preds_list = list()\nfor i in range(0, len(targs)):\n    preds_list.append(preds[i].argmax().item())\n\npreds_tensor = TensorCategory(preds_list)","7ff21825":"f1score = F1Score(average='micro')\npreds, targs = learn.tta()\nprint(f'Micro average F1 validation: {f1score(preds_tensor, targs).item()}')","b5c07a47":"np.savetxt(\"raw_pred_probabilities.csv\", np.array(preds), delimiter=\",\")\nnp.savetxt(\"final_preds.csv\", np.array(preds_list), delimiter=\",\")","2def04b5":"* Looks like training after unfreezing doesn't move the needle much","3fb6b200":"### Acknowledgments\n\n---\n\n* [EDA kickstarter](https:\/\/www.kaggle.com\/virajkadam\/birdclef-exploratory-data-analysis)\n* [torch-librosa help](https:\/\/www.kaggle.com\/whurobin\/training-pipeline-in-pytorch-lightning\/data)\n* [fastaudio help]( https:\/\/colab.research.google.com\/drive\/1hTRtTq3Tr9kgld0i78ao8gVrq_0yPTgW#scrollTo=UktmdDZ7wt8T)","c72159fe":"## BirdCLEF Call Identification Pre-Modeling\n---\n\n1. MetaData EDA\n2. Audio EDA\n3. Toy model with fastai","e1a147da":"### MetaData EDA observations\n---\n\nMost of the recordings are in North and South America, as well as West and North Europe. The rest of the map has varying sparcity.\n\nAs per the Kaggle competition intro:\n\n* \"Some bird species may have local call 'dialects,' so you may want to seek geographic diversity in your training data\". In addition, \"while some bird calls can be made year round, such as an alarm call, some are restricted to a specific season. You may want to seek temporal diversity in your training data.\" \n\nOur EDA confirms that this approach, seeking geospatial and temporal diversity, may be critical to preserve lower density types of audio records. For example, only 0.5% (0.005) of our data is from the Southeast quadrant of the globe.","c2635466":"## 2. Audio EDA\n\n---\n\n* Added subtle Pink noise and removed silence\n* Cropped to seven seconds\n* Converted to uint8 data to save memory\n* Loaded into kaggle: audio-flacs-birdclef21 dataset","048d29df":"## 1. MetaData EDA\n---\n\n* Basic EDA before modeling.\n* Where were the records taken?\n  * Are the records evenly distributed across space or unevenly?\n","cc0ae1cc":"### 3. Build a toy model with fastai to experiment\n---\n\n* Audio to melspec batch transform\n* densenet121, as per [last comp's winning solution](https:\/\/www.kaggle.com\/c\/birdsong-recognition\/discussion\/183208)"}}