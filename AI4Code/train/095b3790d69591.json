{"cell_type":{"bed2a966":"code","74c89869":"code","c0be9226":"code","0d54d663":"code","ba57485c":"code","698dab12":"code","53c42cbe":"code","9dde0c03":"code","21f9c3a5":"code","adeeef52":"code","2b2010ef":"code","911182cb":"code","04fd7eb4":"code","f53a293b":"code","d5ea1164":"code","e1bcec65":"code","0cc3fd39":"code","6d1c4f6e":"code","040345ad":"code","959dc1c1":"code","60f65a17":"code","478e2cbf":"code","879b7427":"code","3f46a367":"markdown","23460103":"markdown","51246963":"markdown","5a9aabc3":"markdown","4d84721d":"markdown","b6047e3c":"markdown","c1fbe593":"markdown","e0d58047":"markdown"},"source":{"bed2a966":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom collections import Counter\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.decomposition import PCA\n\nimport time","74c89869":"digit = pd.read_csv('..\/input\/digit-recognizer\/train.csv')","c0be9226":"# check the five rows\ndigit.head()","0d54d663":"# printing the shape of the database\ndigit.shape","ba57485c":"# print some imformation about database\ndigit.info()","698dab12":"# print descriptive statistic on numerical columns\ndigit.describe()","53c42cbe":"# Target variable \ndigit.label.value_counts()","9dde0c03":"y = digit[\"label\"]\nx = digit.loc[:, digit.columns != \"label\"]","21f9c3a5":"X_std = StandardScaler().fit_transform(x)","adeeef52":"X_train, X_test, y_train, y_test = train_test_split(X_std, y, test_size = 0.25, random_state = 42, stratify = y)","2b2010ef":"print(\"X_train: \", X_train.shape)\nprint(\"X_test: \", X_test.shape)\nprint(\"y_train: \", y_train.shape)\nprint(\"y_test: \", y_test.shape)","911182cb":"log  = LogisticRegression(random_state = 42, multi_class=\"multinomial\", solver=\"saga\", max_iter=200)\n\nstart_time = time.time()\nlog.fit(X_train, y_train)\nend_time = time.time()\n\ntime1 = end_time-start_time\nprint(\"Time elapsed: \",time1)\n\ny_pred = log.predict(X_test)\n\n# Accuracy Estimation\nprint('Accuracy Score (Train Data):', np.round(log.score(X_train, y_train), decimals = 3))\nprint('Accuracy Score (Test Data):', np.round(log.score(X_test, y_test), decimals = 3))\n\n# Classification Report\nlogistic_report = classification_report(y_test, y_pred)\nprint(logistic_report)","04fd7eb4":"# Here we will aim to explain 98% of the variance with PCA. We could reduce or increase it as per the needs of our project\npca = PCA(.96)","f53a293b":"lower_dimensional_data = pca.fit_transform(X_train)\n\napproximation = pca.inverse_transform(lower_dimensional_data)","d5ea1164":"pca.n_components_","e1bcec65":"var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)* 100)\nplt.ylabel('% Variance Explained')\nplt.xlabel('Number of Features')\nplt.title('PCA Analysis')\nplt.ylim(30,100.5)\nplt.style.context('seaborn-whitegrid')\nplt.plot(var)\nplt.show()","0cc3fd39":"plt.figure(figsize=(8,4));\n\n# Original Image\nplt.subplot(1, 2, 1);\nplt.imshow(x.values[1].reshape(28,28),\n              cmap = plt.cm.gray, interpolation='nearest',\n              clim=(0, 255));\nplt.xlabel('784 components', fontsize = 14)\nplt.title('Original Image', fontsize = 20);\n\n# 154 principal components\nplt.subplot(1, 2, 2);\nplt.imshow(approximation[1].reshape(28, 28),\n              cmap = plt.cm.gray, interpolation='nearest',\n              clim=(0, 255));\nplt.xlabel('270 components', fontsize = 14)\nplt.title('95% of Explained Variance', fontsize = 20);","6d1c4f6e":"# fit and transform  the data\npca = PCA(n_components=443, random_state = 0)\nX_pca_t = pca.fit_transform(X_train)\nprint(X_pca_t.shape)","040345ad":"# transform  the data\nX_std_t = pca.transform(X_std)\nprint(X_std_t.shape)","959dc1c1":"X_train, X_test, y_train, y_test = train_test_split(X_std_t, y, test_size=0.25, random_state=42, stratify = y)","60f65a17":"print(\"X_train: \", X_train.shape)\nprint(\"X_test: \", X_test.shape)\nprint(\"y_train: \", y_train.shape)\nprint(\"y_test: \", y_test.shape)","478e2cbf":"log  = LogisticRegression(random_state = 42, multi_class=\"multinomial\", solver=\"saga\", max_iter=200)\n\nstart_time = time.time()\n\nlog.fit(X_train, y_train)\n\nend_time = time.time()\ntime1 = end_time-start_time\nprint(\"Time elapsed: \",time1)\n\ny_pred = log.predict(X_test)\n\n# Accuracy Estimation\nprint('Accuracy Score (Train Data):', np.round(log.score(X_train, y_train), decimals = 3))\nprint('Accuracy Score (Test Data):', np.round(log.score(X_test, y_test), decimals = 3))\n\n# Classification Report\nlogistic_report = classification_report(y_test, y_pred)\nprint(logistic_report)","879b7427":"# https:\/\/github.com\/mGalarnyk\/Python_Tutorials\/blob\/master\/Sklearn\/PCA\/PCA_Image_Reconstruction_and_such.ipynb","3f46a367":"# Feature scaling","23460103":"# split into training and test set","51246963":"# Import library","5a9aabc3":"# PCA","4d84721d":"# Logistic regression ","b6047e3c":"# Separating the label values and column values","c1fbe593":"# Logistic Regression on PCA reduced dataset\n","e0d58047":"# Load dataset"}}