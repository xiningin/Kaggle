{"cell_type":{"b19b8893":"code","8e7862ae":"code","bd6087e1":"code","58a708ff":"code","0e849357":"code","8c17196c":"code","29f98a25":"code","9aea7986":"code","e7016394":"code","e82dd91d":"code","3bb86cf3":"code","98c17874":"code","797a8606":"code","7a0347be":"code","b94f8b33":"code","9188fd4d":"code","268bf103":"code","b75b4419":"code","b09b8759":"code","c4ac22cc":"code","a891d720":"code","45ba11b0":"code","545a35c4":"code","2d8bfc69":"code","5e79f851":"markdown","5a8c705c":"markdown","9c8b781b":"markdown","2921bb80":"markdown","63380648":"markdown"},"source":{"b19b8893":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport json\nfrom multiprocessing import Pool\nimport random\nimport pickle\nimport re\nfrom functools import reduce\n# Any results you write to the current directory are saved as output.","8e7862ae":"filenames_list = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for each_filename in filenames:\n        filenames_list.append(os.path.join(dirname, each_filename))","bd6087e1":"filenames_list[:20]","58a708ff":"len(filenames_list)","0e849357":"\n# for filename in random.sample(filenames_list, 2):\n#     if filename.split(\".\")[-1] == \"json\":\n#         ifp = open(os.path.join(dirname, filename))\n#         research_paper = json.load(ifp)\n#         title = research_paper[\"metadata\"][\"title\"]\n#         print(title, \"\\n\\n\")\n#         abstract_text = \" \".join([each[\"text\"] for each in research_paper[\"abstract\"]])\n#         print(abstract_text, \"\\n\\n\")\n#         body_text = \" \".join([each[\"text\"] for each in research_paper[\"body_text\"]])\n#         print(body_text)\n\n            ","8c17196c":"ifp = open(\"\/kaggle\/input\/stopwords-compiled\/stopwords_compiled.txt\", \"r\")\nstopwords = ifp.read().split(\"\\n\")","29f98a25":"stopwords[:20]","9aea7986":"def clean_phrase(phrase_str):\n    phrase_split = phrase_str.strip().split(\".\")\n    if len(phrase_split) == 1:\n        return phrase_split[0]\n    else:\n        return phrase_split[-1]","e7016394":"def jargons_extractor(text):\n    abbreviation_jargon_pairs = []\n    potential_phrases = re.split(\" \" + \" | \".join(list(stopwords)), text)\n#     print(potential_phrases)\n    for each_phrase in potential_phrases:\n        phrase_jargon_splits = re.split(\"\\(|\\)\", each_phrase)\n        if len(phrase_jargon_splits) >= 2 and len(phrase_jargon_splits[0]) > 1 and len(phrase_jargon_splits[1]) > 1 and len(phrase_jargon_splits[1].split(\" \")) == 1 and len(clean_phrase(phrase_jargon_splits[0]).split(\" \")) > 1 and phrase_jargon_splits[1].isnumeric() == False and phrase_jargon_splits[1][-1] != \"%\":\n#             print(clean_phrase(phrase_jargon_splits[0]).strip(), \" -> \", phrase_jargon_splits[1].strip())\n            abbreviation_jargon_pairs.append((phrase_jargon_splits[1].strip(), clean_phrase(phrase_jargon_splits[0]).strip()))\n    return abbreviation_jargon_pairs","e82dd91d":"def process_research_paper(filename_path):\n    if filename_path.split(\".\")[-1] == \"json\":\n#         print(filename_path)\n        ifp = open(filename_path, \"r\")\n        research_paper = json.load(ifp)\n        title = research_paper[\"metadata\"][\"title\"]\n        abstract_text = \" \".join([each[\"text\"] for each in research_paper[\"abstract\"]])\n        body_text = \" \".join([each[\"text\"] for each in research_paper[\"body_text\"]])\n        all_text = \" {} {} {} \".format(title, abstract_text, body_text)\n#         print(all_text[:10])\n        return jargons_extractor(all_text)","3bb86cf3":"with Pool(processes=100) as pool:\n    lists_of_jargon_lists = pool.map(process_research_paper, filenames_list)","98c17874":"len(lists_of_jargon_lists)","797a8606":"lists_of_jargon_lists[90:92]","7a0347be":"short_hand_jargon_pairs = reduce(lambda x, y: x + y, [each_list for each_list in lists_of_jargon_lists if each_list])","b94f8b33":"len(short_hand_jargon_pairs)","9188fd4d":"random.sample(short_hand_jargon_pairs, 100)","268bf103":"short_hand_set = set([each_pair[0] for each_pair in short_hand_jargon_pairs])","b75b4419":"len(short_hand_set)","b09b8759":"short_hand_pair_dict = dict(zip(list(short_hand_set), [[]] * len(short_hand_set)))","c4ac22cc":"# short_hand_pair_dict","a891d720":"for each_pair in short_hand_jargon_pairs:\n    short_hand_pair_dict[each_pair[0]] = short_hand_pair_dict[each_pair[0]] + [each_pair[1]]","45ba11b0":"short_hand_pair_dict['gp41']","545a35c4":"for each_key in random.sample(short_hand_pair_dict.keys(), 100):\n    print(each_key, \" -> \", short_hand_pair_dict[each_key])","2d8bfc69":"short_hand_pair_dict[\"aa\"]","5e79f851":"## A lot of jargon come in the papers which are difficult to track as to what it means. In this kernel, I am trying to create a dictionary of jargons with their short hand notation","5a8c705c":"## observe above amino acid can be extracted by analysing the different jargons associated with \"aa\".\n## I leave the further cleaning for later","9c8b781b":"I have taken the stopwords from here:\nhttps:\/\/gist.github.com\/sebleier\/554280#gistcomment-2838837","2921bb80":"## In Jargons extractor:\nI have noticed that often the short hand notation of a jargon is in the form **stopword** **jargon** (**short-hand-notation**) **stopword**\nSo I i first find those pairs as candidates and then apply filtering \n*     such that short-hand-notation is not all numeric, in one word (might be in a combination of alpha, numeric, hyphen, etc) but not a percentage value\n*     the jargon is atleast two words","63380648":"## I create a combined list of jargons for each short-hand-notation so that the exact jargon can be extracted by finding the common part of the phrase among all jargons for that short-hand-notation"}}