{"cell_type":{"5ab38ddd":"code","bcd92ae3":"code","ac1ef105":"code","0b2a4e4c":"code","ec500f70":"code","fff03089":"code","1301b59b":"code","3b13755e":"code","e89d2b5e":"code","50c84a49":"code","228a6b8e":"markdown","5d1854aa":"markdown","8f7df9de":"markdown","391c1dc2":"markdown","3487ab93":"markdown","28de1b70":"markdown","4fd55b74":"markdown","2181d36a":"markdown","ca39cca3":"markdown","cb364dc7":"markdown"},"source":{"5ab38ddd":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import roc_curve, confusion_matrix, f1_score, recall_score, precision_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\n\nfile_path = '\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv'\nquality = pd.read_csv(file_path)\ndisplay(quality)","bcd92ae3":"correlation = quality.iloc[:,1:].corr()\ndisplay(correlation)","ac1ef105":"quality.isna().sum()","0b2a4e4c":"plt.hist(quality['quality'])","ec500f70":"isHQ_split = np.percentile(quality['quality'],50)\nprint('I will split wine quality values at ' + str(isHQ_split) + ' to build my supervisor.')\nquality['IsHighQualityWine'] = np.where(quality['quality'] >= isHQ_split,1,0)\nquality.head(10)","fff03089":"def BuildModel(df, Algorithm, ScaleData = 0):\n    # Separate data into independent and dependent variables\n    X = df.iloc[:,1:-2] # features\n    Y = df.iloc[:,-1] # supervisor\n\n    if ScaleData == 1:\n        from sklearn.preprocessing import StandardScaler\n        scale = StandardScaler()\n        X_scaled = scale.fit_transform(X)\n        X_scaled = pd.DataFrame(X_scaled)\n        X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size = 0.2, random_state = 0)\n    else:\n        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n    \n    # Save training set if need to reference in future when evaluating model\n    TrainingSet = pd.merge(X_train, pd.DataFrame(Y_train), left_index  = True, right_index = True)\n    \n    if Algorithm == 'LogisticRegression':\n        Classifier = LogisticRegression(max_iter=1000)\n\n    if Algorithm == 'DecisionTree':\n        Classifier = tree.DecisionTreeClassifier()\n\n    if Algorithm == 'RandomForest':\n        Classifier = RandomForestClassifier(n_estimators = 1000)\n    \n    if Algorithm == 'NaiveBayes':\n        Classifier = GaussianNB()\n            \n    Classifier = Classifier.fit(X_train,Y_train)\n    Y_pred = Classifier.predict(X_test)\n    \n    # Evaluate model with confusion matrix\n    cf = confusion_matrix(Y_test,Y_pred)\n    f_score = f1_score(Y_test, Y_pred)\n    precision = precision_score(Y_test,Y_pred)\n    recall = recall_score(Y_test,Y_pred)\n    accuracy = accuracy_score(Y_test,Y_pred)\n\n    print('Using the {} classifier, we receive a precision value of {}, recall value of {}, and f-score of {}. \\n' \\\n          .format(Algorithm,round(precision,4),round(recall,4),round(f_score,4)))","1301b59b":"BuildModel(quality,'LogisticRegression')","3b13755e":"BuildModel(quality,'LogisticRegression', 1)","e89d2b5e":"def BuildAndCompareModels(df, AlgArray, ScaleData = 0):\n    for i in range(0,len(AlgArray)):\n        BuildModel(df,AlgArray[i],ScaleData)","50c84a49":"BuildAndCompareModels(quality,['LogisticRegression','NaiveBayes','DecisionTree','RandomForest'])","228a6b8e":"# Summary:\n* Logistic Regression, Naive Bayes, and Decision Tree all performed about the same\n* We got slightly higher performance with Random Forest\n* Random Forest compute time was only marginally slower than the other three algorithms\n\nIn order to get the best predictions at a low compute cost, I would choose to deploy the Random Forest model. It's also worth noting that Random Forest is not much more difficult to explain to a client than something like Logistic Regression, so if there was a business need to keep the deployed model simple, this selection would meet that need.","5d1854aa":"Get an idea of vairable correlation:","8f7df9de":"Check for nulls in the dataset:","391c1dc2":"I'll build a logistic regression model to test my function, first without scaling the features and then with scaling:","3487ab93":"Find the 50th percentile of wine quality ratings and use that to build a supervisor, IsHighQualityWine:","28de1b70":"I'll build all four algorithms and see how they compare.","4fd55b74":"My BuildModel function allows the user to train and test a model, evaluate the model, and output the results. It has the ability to use any of four algorithms from: Logistic Regression, Decision Tree, Random Forest, and Naive Bayes.","2181d36a":"The BuildAndCompareModels function below calls the BuildModel function and allows a user to create and compare models built using different algorithms. To use this function, I'll input an array such as ['LogisticRegression','DecisionTree'] and it will loop through to build each model specified. The output will be the precision, recall, and f-score values from each model that was built.","ca39cca3":"# Building a Model to Predict Red Wine Quality\n\nUsing the UCI Machine Learning Red Wine Quality dataset, I'll demonstrate how to quickly train and compare different algorithms to build a model that classifies whether a wine is high quality or not.\n\nBelow is an outline of my work:\n\n1. Import libraries and read in the dataset.\n2. Review variable correlation and check dataset for nulls, of which there are none.\n3. Plot wine quality ratings in a histogram and find the 50th percentile.\n4. Build my supervisor ('IsHighQualityWine') based off of 50th percentile value of wine quality column.\n5. All of the data is numerical, so I can move on and separate it into X (independent variables) and Y (our supervisor) to prepare to train models.\n6. Build a function so I can input which algorithm I'd like to train and test with and output metrics such as precision, recall, and f-score.\n7. Finally, build one last function that allows me to run multiple iterations of the BuildModel function with various algorithms.","cb364dc7":"Look at frequeny of wine quality ratings:"}}