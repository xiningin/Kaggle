{"cell_type":{"ee0f2c43":"code","b13f3e01":"code","e07e3806":"code","669892f2":"code","1fdd1c08":"code","ab27afa4":"code","5a579c95":"code","5a46ce29":"code","b7784f99":"code","2e9ac47a":"code","f4ba26ff":"code","ed06817c":"code","06d013f2":"code","5921f15e":"code","338107d3":"code","054cb2fa":"code","94f33da6":"code","d04b4a60":"code","87224e77":"code","3c4ba595":"code","6c4f7f9f":"code","71da83eb":"code","4d21dca4":"code","2bfef6c2":"code","736065a2":"code","a312ab9f":"code","f5b17de2":"code","0e292245":"code","628cd44f":"code","b1e7e125":"code","753be36d":"code","38b2e851":"code","f130b23c":"code","d47654c7":"code","cfbcbbfa":"code","4e05e20a":"code","e24f7e6c":"code","7d8a3a2e":"code","28defcea":"code","00b91d77":"code","0d8e65d5":"code","dace1876":"code","0bb9b488":"code","fa98e58d":"code","e243bb22":"code","ba86b8a6":"code","2657be2b":"code","68074557":"code","67a9384c":"code","cabb9f16":"code","27bbe766":"code","58e63585":"code","1e46a1c5":"code","d90f40c6":"code","0596aeca":"code","a79d0ac8":"code","a8d4ab9c":"code","5af2dcb0":"code","44f2985d":"code","05526740":"code","8bfff700":"markdown","0ce0bbd7":"markdown","368e0987":"markdown","d2f7af65":"markdown","a5f222b2":"markdown","284effc4":"markdown","953cc01c":"markdown","aa0e3b25":"markdown","19491e49":"markdown","b5d92aff":"markdown","19a22c5d":"markdown","0d38a279":"markdown","131ace45":"markdown","8188b2bb":"markdown","eb31b740":"markdown","dd2692dc":"markdown","c0afc18d":"markdown","d77573b6":"markdown","06ff4e26":"markdown","a32d70da":"markdown","ca29369a":"markdown","b6d9265f":"markdown","0af17b15":"markdown","c63e0acb":"markdown","bfc8b127":"markdown","39066bf9":"markdown","5b8b4991":"markdown","a60f686c":"markdown","9640e11e":"markdown","d5c423d7":"markdown","f0a0e53c":"markdown","11385eeb":"markdown","d636ac4b":"markdown","6415b078":"markdown","70433462":"markdown","7f2f6008":"markdown","45868975":"markdown","4a06a5af":"markdown","00b3851a":"markdown"},"source":{"ee0f2c43":"pwd","b13f3e01":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport pandas as pd\nfrom pandas import DataFrame\nimport pylab as pl\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline","e07e3806":"banknote=pd.read_csv(\"..\/input\/banknote.csv\") #Importing Data","669892f2":"banknote.head()","1fdd1c08":"print(banknote.shape)\nbanknote.info()","ab27afa4":"pd.options.display.float_format = '{:.4f}'.format\ndata_summary=banknote.describe()\ndata_summary.T","5a579c95":"for k, v in banknote.items():\n    q1 = v.quantile(0.25)\n    q3 = v.quantile(0.75)\n    irq = q3 - q1\n    v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n    perc = np.shape(v_col)[0] * 100.0 \/ np.shape(banknote)[0]\n    print(\"Column %s outliers = %.2f%%\" % (k, perc))","5a46ce29":"plt.figure(figsize=(12,5))\nbanknote.boxplot(patch_artist=True,vert=False)","b7784f99":"my_corr=banknote.corr()\nprint(my_corr)\nplt.figure(figsize=(12,5))\nsns.heatmap(my_corr,linewidth=0.5)\nplt.show()","2e9ac47a":"print(banknote['Class'].value_counts())\nsns.countplot(x='Class',data=banknote)","f4ba26ff":"sns.pairplot(banknote, hue='Class',kind='reg') \nplt.show()","ed06817c":"banknote.columns","06d013f2":"predictor_var= banknote[['Variance', 'Skewness', 'Curtosis', 'Entropy']] #all columns except the last one\ntarget_var= banknote['Class'] #only the last column","5921f15e":"print(predictor_var.shape)\nprint(target_var.shape)","338107d3":"from sklearn.model_selection import train_test_split ","054cb2fa":"X_train, X_test, Y_train, Y_test = train_test_split(predictor_var,target_var, test_size=0.3, random_state=123)","94f33da6":"from sklearn.tree import DecisionTreeClassifier","d04b4a60":"tree = DecisionTreeClassifier(max_depth=3,max_features=4)","87224e77":"tree.fit(X_train, Y_train)","3c4ba595":"tree.feature_importances_\npd.Series(tree.feature_importances_,index=predictor_var.columns).sort_values(ascending=False)","6c4f7f9f":"predictions = tree.predict(X_test)","71da83eb":"df=pd.DataFrame({'Actual':Y_test, 'Predicted':predictions})\ndf.head(5)","4d21dca4":"from sklearn.metrics import accuracy_score, confusion_matrix,classification_report","2bfef6c2":"accuracy_score(Y_test, predictions) #Calculate number of correctly classified observations.","736065a2":"accuracy_score(Y_test, predictions, normalize=False)","a312ab9f":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f5b17de2":"confusion_mat = confusion_matrix(Y_test, predictions)\nconfusion_df = pd.DataFrame(confusion_mat, index=['Class 0','Class 1'],columns=['Class 0','Class 1'])","0e292245":"print(confusion_df)\n_=sns.heatmap(confusion_df, cmap='coolwarm', annot=True)","628cd44f":"tn, fp, fn, tp = confusion_matrix(Y_test, predictions).ravel()\nprint(\"True Negatives: \", tn)\nprint(\"False Positives: \", fp)\nprint(\"False Negatives: \", fn)\nprint(\"True Positives: \", tp)","b1e7e125":"print(classification_report(Y_test, predictions))","753be36d":"Specificity = tn\/(tn+fp)\nprint(\"The probability of predicting whether a bank note is authentic or fake\",Specificity)","38b2e851":"Sensitivity = tp\/(tp+fn)\nprint(\"The probability of predicting whether a bank note is authentic or fake correctly is\",Sensitivity)","f130b23c":"from sklearn.tree import export_graphviz\nimport graphviz","d47654c7":"dot_data = export_graphviz(tree, filled=True, rounded=True, feature_names=predictor_var.columns, out_file=None)","cfbcbbfa":"graphviz.Source(dot_data)","4e05e20a":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV","e24f7e6c":"param_grid = [{\"max_depth\":[3, 4, 5, None], \"max_features\":[4,5,6,7]}]","7d8a3a2e":"gs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=123),param_grid = param_grid,cv=10)","28defcea":"gs.fit(X_train, Y_train)","00b91d77":"gs.cv_results_['params']","0d8e65d5":"gs.best_params_","dace1876":"gs.best_estimator_","0bb9b488":"tree = DecisionTreeClassifier(max_depth=None,max_features=4)","fa98e58d":"tree.fit(X_train,Y_train)","e243bb22":"predictions = tree.predict(X_test)","ba86b8a6":"df=pd.DataFrame({'Actual':Y_test, 'Predicted':predictions})\ndf.head(5)","2657be2b":"from sklearn.metrics import accuracy_score, confusion_matrix,classification_report","68074557":"accuracy_score(Y_test, predictions)#Calculate number of correctly classified observations.","67a9384c":"accuracy_score(Y_test, predictions, normalize=False) ","cabb9f16":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","27bbe766":"confusion_mat = confusion_matrix(Y_test, predictions)\nconfusion_df = pd.DataFrame(confusion_mat, index=['Class 0','Class 1'],columns=['Class 0','Class 1'])","58e63585":"print(confusion_df)\n_=sns.heatmap(confusion_df, cmap='coolwarm', annot=True)","1e46a1c5":"tn, fp, fn, tp = confusion_matrix(Y_test, predictions).ravel()\nprint(\"True Negatives: \", tn)\nprint(\"False Positives: \", fp)\nprint(\"False Negatives: \", fn)\nprint(\"True Positives: \", tp)","d90f40c6":"print(classification_report(Y_test, predictions))","0596aeca":"Specificity = tn\/(tn+fp) \nprint(\"The probability of predicting whether a bank note is authentic or fake is:\",Specificity)","a79d0ac8":"Sensitivity = tp\/(tp+fn)\nprint(\"The probability of predicting whether a bank note is authentic or fake is:\",Sensitivity)","a8d4ab9c":"from sklearn.tree import export_graphviz\nimport graphviz","5af2dcb0":"dot_data = export_graphviz(tree, filled=True, rounded=True, feature_names=predictor_var.columns, out_file=None)","44f2985d":"graphviz.Source(dot_data)","05526740":"DT_Classifier=[['Max_Depth',3,'None'],['Max_Feature',4,4],['Accuracy Score',0.93,0.985],['f1 Score for Class0',0.94,0.99],['f1 Score for Class1',0.93,0.98],['Specificity',0.95,0.99],['Sensitivity',0.92,0.98],['Misclassified',26,6]]\nResult_Summary2= pd.DataFrame(DT_Classifier, columns = ['Parameters','Without Grid Search','With Grid Search'])\nResult_Summary2","8bfff700":"##### We see that the predictions are pretty accurate. Let's evaluate the prediction accuracy.\n\n","0ce0bbd7":"##### Check the top 5 predictions and actual values.","368e0987":"##### Calculate accuracy of prediction\n\n","d2f7af65":"#### Use this dataset to predict whether a bank note is authentic or fake depending upon the four different attributes of the image of the note. The attributes are Variance of wavelet transformed image, curtosis of the image, entropy, and skewness of the image.\n\nUse Decision Trees to build a classifier for the give data. Use Grid Search to find the optimal value for the hyperparameters, and evaluate your model on suitable metrics. Use graphviz to visualize your decision tree.","a5f222b2":" ##### Inside of the classifier, specify criterion=\"gini\" so we can see the information gain of each node.","284effc4":"##### Viz 1: Correlation Matrix along with Heatmap ","953cc01c":"##### Print best hyperparameters combination.\n\n","aa0e3b25":"##### Split the data into train and test sets.","19491e49":"##### Fit the training data\n\n","b5d92aff":"##### Print the complete DecisionTree estimator.\n\n","19a22c5d":"##### Knowing the feature importances","0d38a279":"F1 Score: Single metric that combines recall and precision using the harmonic mean. So closer the F1-Score for Class 0 & Class 1 is close to 1.","131ace45":"##### Print out all the hyperparameters combinations that the GridSearchCV has tried.\n\n","8188b2bb":"##### We can also visualize the tree output using graphviz\n\n","eb31b740":"##### Specificity after Applying CV & GridSearch","dd2692dc":"**Now train_test_split will return 4 different parameters. We will name them:\nX_trainset, X_testset, y_trainset, y_testset**\n\n**The train_test_split will need the parameters:\nX, y, test_size=0.3, and random_state=123.**\n\n**The X and y are the arrays required before the split, the test_size represents the ratio of the testing dataset, and the random_state ensures that we obtain the same splits.**","c0afc18d":"##### Sensitivity","d77573b6":"##### Checking for Outliers present in Data","06ff4e26":"##### Splitting data into Traget and Predictor Variables & Knowing the Shape of the new variables","a32d70da":"##### Sensitivity after Applying CV & GridSearch","ca29369a":"**Model  has 98.5 % Accuracy score after aplying Grid Search which is good but might be chance of Overfitting. This can be vizualized by plotting Decision_Boundary for being more sure about overfitting** ","b6d9265f":"### Explaining Decision Tree for Classification:\nIn the above Decision tree every node is a condition and we are splitting values in a single feature. the condition is based on Imputity (Gini Impurity\/Information gain(entropy)) for classification and variance for regression but as it a classificaton problem we will consider gini impurity as a criteria.\nTo start the tree I have computed feature importances i.e. how each feature contribute to decreasing weighted impurity. and on calculating the feature imnportance i found variance Feature contributes most and sonit appears as a root of this tree.\n\nThe everybox in decision tree tells how many samples at the node falls into each category.\n\nWe can see that the root node starts with 960 samples of two class(0 &1) with a gini index of 0.495. In this node the feature that best split the different class of data is Variance using threshold value of 0.32. This results in two decision nodes Skewness and Curtosis. the Skewness has gini index of 0.302 and out of 960 smaples 470 samples are trained with class values of (87,383) & The one with curtosis has Gini of 0.177 with 490 Samples out of which Class values are (442 & 48).\n\nThe Skewness and Curtosis is again distributed into two decision nodes i.e. Variances.\n\nAt the end we can see that Children nodes generated from variance which is one of the decision node.\n\nThe nodes whose Gini=0 are the leaves and seems the purest nodes.","0af17b15":"References:\n1. https:\/\/scikit-learn.org\/stable\/auto_examples\/tree\/plot_unveil_tree_structure.html\n2. https:\/\/webfocusinfocenter.informationbuilders.com\/wfappent\/TLs\/TL_rstat\/source\/DecisionTree47.html\n3. https:\/\/machinelearningmastery.com\/classification-and-regression-trees-for-machine-learning\/","c63e0acb":"##### Importing Packages & Reading the file","bfc8b127":"##### in the above Bar graph I have tried to plot the count of  Genuine notes versus Forged notes.","39066bf9":"### Using Grid Search and Cross-Validation","5b8b4991":"##### Plotting decision Tree using Graphviz after selecting best hyperparameters that is obtained after Using GridSearch and CV ","a60f686c":"##### Confusion Matrix","9640e11e":"##### Knowing the data shape, information and Summary Stats","d5c423d7":"##### Let's use GridSearchCV to try max_depth values 3,4,5,None and 5,6,7 value for max_features hyperparameters. Specify 10 number of folds.\n\n","f0a0e53c":"##### Make predictions for the test set.\n\n","11385eeb":"##### We can see out of 412 datapoints only 6 are misclasified from the above accuracy score.\n","d636ac4b":"### Description of Dataset\nData were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. Wavelet Transform tool were used to extract features from images.\n\n\nAttribute Information:\n\n1. variance of Wavelet Transformed image (continuous)\n2. skewness of Wavelet Transformed image (continuous)\n3. curtosis of Wavelet Transformed image (continuous)\n4. entropy of image (continuous)\n5. class (integer)","6415b078":"#### After Applying the best parameters with the help of Grid Search we see that the model accuracy score has improved. \n\nBy looking at the below reference table we easy compare the results of Decision tree Model with and Without Pruning.","70433462":"##### We can see out of 412 datapoints only 26 are misclasified from the above accuracy score.\n\n\nLet us Plot Confusion Metrix","7f2f6008":"##### Import Decision Tree Classifier and fit the model to the training data.\n\n","45868975":"##### Viz 2: In the below shown Pairplot shown we can see the regression plot for different features along different categories as per target variable. The blue dots represented by 0( i.e Defective Notes) and Orange Dots represented by 1 (i.e.Not a defective notes). The Entropy and variance features has positive regression line which says as entropy of the image of notes increases the variance will increase. Similar is for curtosis entropy. For others the slope of regression lines are negative. ","4a06a5af":"##### Applying the best estimator in decision tree & Predicting result\n\nInside of the classifier, specify criterion=\"gini\" so we can see the information gain of each node.","00b3851a":"##### Specificity"}}