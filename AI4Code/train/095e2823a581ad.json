{"cell_type":{"e7c22dcd":"code","12203534":"code","a063befd":"code","70bc8218":"code","ae6dcac5":"code","534001ec":"code","95aeed05":"code","49d56409":"code","b3f06bd3":"code","567398fc":"code","6423d836":"code","f30ba1f5":"markdown","8b9da76c":"markdown","327690d7":"markdown","502106b4":"markdown"},"source":{"e7c22dcd":"import numpy as np\nimport pandas as pd\nimport os\nimport optuna\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nimport lightgbm as lgbm\nfrom lightgbm import LGBMClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport catboost\nfrom catboost import CatBoostClassifier\nfrom optuna.integration import LightGBMPruningCallback, XGBoostPruningCallback\nimport warnings\nwarnings.filterwarnings(\"ignore\")","12203534":"#Load Data\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv').drop('id', axis=1)\ntrain.head()","a063befd":"#Imputation Pipeline\npipeline = Pipeline([('impute', SimpleImputer(strategy='mean')), ('scale', StandardScaler())])","70bc8218":"#Drop ID and claim columns, latter temporarily\ntemp = train['claim']\ntrain.drop(columns = ['claim', 'id'], inplace=True)","ae6dcac5":"#Feature Engineering\ntrain['min'] = train.min(axis=1)\ntrain['max'] = train.max(axis=1)\ntrain['sum'] = train.isna().sum(axis=1)\ntrain['mean'] = train.mean(axis=1)\ntrain['std'] = train.std(axis=1)\ntest['min'] = test.min(axis=1)\ntest['max'] = test.max(axis=1)\ntest['sum'] = test.isna().sum(axis=1)\ntest['mean'] = test.mean(axis=1)\ntest['std'] = test.std(axis=1)","534001ec":"#Restore datasets with missing data + new features\ntrain = pd.DataFrame(columns = train.columns, data=pipeline.fit_transform(train))\ntest = pd.DataFrame(columns = test.columns, data=pipeline.fit_transform(test))\ntrain['claim'] = temp\ntrain.head()","95aeed05":"#Split into X and Y\ntrain_x = train\ntrain_y = train['claim']\ntrain_x.drop(columns = ['claim'], inplace=True)\ntrain_x.head()","49d56409":"def Optuna(argument):\n    N_TRIALS = 20\n    N_SPLITS = 5\n    #Credit here for Objective Function: https:\/\/www.kaggle.com\/bextuychiev\/lgbm-optuna-hyperparameter-tuning-w-understanding\n    def LGBMObjective(trial, train_x = train_x, train_y = train_y):\n        param = {\n            \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 8, 4096),\n            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n            \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 200, 10000),\n            \"lambda_l1\": trial.suggest_int(\"lambda_l1\", 0, 100),\n            \"lambda_l2\": trial.suggest_int(\"lambda_l2\", 0, 100),\n            \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 15),\n            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 0.95),\n            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 0.95),\n            \"device_type\": 'gpu',\n            \"n_estimators\": 10000,\n            \"bagging_freq\": 1,\n            \"metric\": 'auc',\n            \"objective\": 'binary'\n        }\n        cv = StratifiedKFold(n_splits=5, shuffle=True)\n        cv_scores = np.empty(5)\n        for idx, (train_idx, test_idx) in enumerate(cv.split(train_x, train_y)):\n            X_train, X_valid = train_x.iloc[train_idx], train_x.iloc[test_idx]\n            y_train, y_valid = train_y[train_idx], train_y[test_idx]\n            model = LGBMClassifier(**param)\n            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], eval_metric=\"roc_auc_score\", early_stopping_rounds=100,\n                callbacks=[LightGBMPruningCallback(trial, \"auc\")], verbose = False\n            )\n            preds = model.predict_proba(X_valid)\n            cv_scores[idx] = roc_auc_score(y_valid, preds[:,1])\n        return np.mean(cv_scores)\n    #Credit here for objective function: https:\/\/www.kaggle.com\/mohammadkashifunique\/xgboost-hyperparametertuning-optuna\n    def XGBObjective(trial, x_train = train_x, y_train = train_y): \n        param = {\n            'max_depth': trial.suggest_int('max_depth', 6, 10), \n            'n_estimators': trial.suggest_int('n_estimators', 400, 4000, 400), \n            'eta': trial.suggest_float('eta', 0.007, 0.013), \n            'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 0.9, 0.1),\n            'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.2, 0.9, 0.1),\n            'colsample_bylevel': trial.suggest_discrete_uniform('colsample_bylevel', 0.2, 0.9, 0.1),\n            'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4), \n            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4),\n            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4), \n            'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4),\n            'predictor': \"gpu_predictor\",\n            'eval_metric' : 'auc',\n            'objective' : 'binary:logistic',\n            'tree_method': 'gpu_hist',\n        }\n        cv = StratifiedKFold(n_splits=5, shuffle=True)\n        cv_scores = np.empty(5)\n        for idx, (train_idx, test_idx) in enumerate(cv.split(train_x, train_y)):\n            X_train, X_valid = train_x.iloc[train_idx], train_x.iloc[test_idx]\n            y_train, y_valid = train_y[train_idx], train_y[test_idx]\n            model = XGBClassifier(**param)\n            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], early_stopping_rounds=100, \n                      callbacks=[XGBoostPruningCallback(trial, 'validation_0-auc')], verbose = 0)\n            preds = model.predict_proba(X_valid)\n            cv_scores[idx] = roc_auc_score(y_valid, preds[:,1])\n        return np.mean(cv_scores)\n    #Credit here for objective function: https:\/\/www.kaggle.com\/mlanhenke\/tps-09-optuna-study-catboostclassifier\n    def CatBoostObjective(trial, x_train = train_x, y_train = train_y):\n        param = {\n            'iterations':trial.suggest_int(\"iterations\", 1000, 20000),\n            'objective': trial.suggest_categorical('objective', ['Logloss', 'CrossEntropy']),\n            'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n            'od_wait':trial.suggest_int('od_wait', 500, 2000),\n            'learning_rate' : trial.suggest_uniform('learning_rate',0.02,1),\n            'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n            'random_strength': trial.suggest_uniform('random_strength',10,50),\n            'depth': trial.suggest_int('depth',1,15),\n            'min_data_in_leaf': trial.suggest_int('min_data_in_leaf',1,30),\n            'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n            'verbose': False,\n            'task_type' : 'GPU',\n            'devices' : '0',\n            'eval_metric':'AUC',\n            'od_type': 'IncToDec',\n            'od_pval': 1e-7, \n            'od_wait' : 100,\n        }\n        if param['bootstrap_type'] == 'Bayesian':\n            param['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0, 10)\n        elif param['bootstrap_type'] == 'Bernoulli':\n            param['subsample'] = trial.suggest_float('subsample', 0.1, 1)\n        cv = StratifiedKFold(n_splits=5, shuffle=True)\n        cv_scores = np.empty(5)\n        for idx, (train_idx, test_idx) in enumerate(cv.split(train_x, train_y)):\n            X_train, X_valid = train_x.iloc[train_idx], train_x.iloc[test_idx]\n            y_train, y_valid = train_y[train_idx], train_y[test_idx]\n            model = CatBoostClassifier(**param)\n            #Notice: Optuna Callback not supported on CatBoost\n            model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)])\n            preds = model.predict_proba(X_valid)\n            cv_scores[idx] = roc_auc_score(y_valid, preds[:,1])\n        return np.mean(cv_scores)\n    def ObjectiveSelector(argument):\n        objective = {\n            'LGBM': LGBMObjective,\n            'XGB': XGBObjective,\n            'CatBoost': CatBoostObjective            \n        }\n        return objective.get(argument, \"Invalid Selection\")\n    def ModelSelector(argument, trial): #Switch case not usable here without crashing. \n        if(argument == 'LGBM'):\n            return LGBMClassifier(**trial.params)\n        elif(argument == 'XGB'):\n            return XGBClassifier(**trial.params)\n        elif(argument == 'CatBoost'):\n            return CatBoostClassifier(**trial.params)\n        return \"Invalid Model\"\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(ObjectiveSelector(argument), n_trials=N_TRIALS)\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    print(\"Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n    model = ModelSelector(argument, trial)\n    model.fit(train_x, train_y)\n    predictions_optuna = model.predict_proba(test)\n    return predictions_optuna","b3f06bd3":"argument = 'XGB'\npredictions_optuna = Optuna(argument)","567398fc":"sample_solution = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsample_solution['claim'] = predictions_optuna[:, 1]\nsample_solution.head()","6423d836":"sample_solution.to_csv('submission.csv', index=False)","f30ba1f5":"# Submission","8b9da76c":"# Optuna","327690d7":"# Libraries","502106b4":"# Missing Values and Feature Engineering"}}