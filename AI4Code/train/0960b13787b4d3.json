{"cell_type":{"e97c065c":"code","d3b020e1":"code","94a0b3d7":"code","07d16378":"code","6d93dce0":"code","fff593a4":"code","42cffa41":"code","114bcf76":"code","3bfd1213":"code","4780a1a4":"code","c5824f82":"code","512090b3":"code","a2220b04":"code","1ea8d423":"code","dca4e7e1":"code","18bbc7ae":"code","a349c775":"markdown","c3e77e19":"markdown","6efb93ee":"markdown","d41a51cc":"markdown","8ddb9a39":"markdown","506f5fc6":"markdown","0992cd42":"markdown","8b836c45":"markdown","12c3cee8":"markdown","450e475f":"markdown","92394b07":"markdown"},"source":{"e97c065c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d3b020e1":"#Code By Paul Mooney  https:\/\/www.kaggle.com\/paultimothymooney\/starter-notebook-for-end-als-kaggle-challenge\n\nexample_bulbar_limb = '\/kaggle\/input\/end-als\/end-als\/transcriptomics-data\/DESeq2\/bulbar_vs_limb.csv'","94a0b3d7":"#Code by Paul Mooney  https:\/\/www.kaggle.com\/paultimothymooney\/starter-notebook-for-end-als-kaggle-challenge\n\nbulbar_limb = pd.read_csv(example_bulbar_limb)\nbulbar_limb.to_csv('\/kaggle\/working\/bulbar_vs_limb.csv')\n\nsns.histplot(bulbar_limb.SiteOnset_Class);","07d16378":"#Code by Tanay Mehta \n\n# Let's plot Timestamp\nplt.style.use(\"classic\")\nsns.distplot(bulbar_limb['SiteOnset_Class'], color='red')\nplt.title(f\"ALS Site Onset [\\u03BC : {bulbar_limb['SiteOnset_Class'].mean():.2f} class | \\u03C3 : {bulbar_limb['SiteOnset_Class'].std():.2f} class]\")\nplt.xlabel(\"SiteOnset_Class\")\nplt.ylabel(\"Count\")\nplt.show()","6d93dce0":"bulbar_limb = pd.get_dummies(bulbar_limb)","fff593a4":"from sklearn.model_selection import train_test_split, KFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, fbeta_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import GaussianNB","42cffa41":"X = bulbar_limb.drop(['SiteOnset_Class', 'Participant_ID_NEUYL149PRF'], axis=1).values\ny = bulbar_limb['SiteOnset_Class'].values\n\n# train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)","114bcf76":"bulbar_limb.head()","3bfd1213":"#Code by Shoaib Mehmood https:\/\/www.kaggle.com\/shoaibmnagi\/credit-risk-modelling-german-data\/notebook\n\nlog = LogisticRegression()\nlog.fit(X_train, y_train)\ny_pred_log = log.predict(X_test)\nprint(accuracy_score(y_pred_log, y_test))\nprint(confusion_matrix(y_test, y_pred_log))\nprint(classification_report(y_test, y_pred_log))","4780a1a4":"\"\"\"knn = KNeighborsClassifier()\ngrid = GridSearchCV(knn, param_grid={'n_neighbors':range(1,20)}, scoring='recall')\ngrid.fit(X_train, y_train)\n\ngrid.best_params_\n\nfor i in range(0, len(grid.cv_results_['mean_test_score'])):\n    print('N_Neighbors {}: {} '.format(i+1, grid.cv_results_['mean_test_score'][i]*100))\"\"\"\n    \n# recall peaks at k = 1\n\nknn = KNeighborsClassifier(n_neighbors = 1)\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\nprint(accuracy_score(y_pred_knn, y_test))\nprint(confusion_matrix(y_test, y_pred_knn))\nprint(classification_report(y_test, y_pred_knn))","c5824f82":"\"\"\"param_grid_svc = {\"gamma\": [0.1,0.5,1,5,10,50,100],\n                  \"C\": [0.1,0.5,1,5,10,50,100]}\n\nsvc = SVC(kernel='linear')\n\ngs_svc = GridSearchCV(svc, param_grid = param_grid_svc, cv=5, scoring='recall', verbose=4)\ngs_svc.fit(X_train, y_train)\n\ngs_svc.best_params_ # gamma = , C = \"\"\"\n\nsvc = SVC(kernel='linear', gamma=10, C=0.8)\nsvc.fit(X_train, y_train)\ny_pred_svc = svc.predict(X_test)\nprint(accuracy_score(y_pred_svc, y_test))\nprint(confusion_matrix(y_test, y_pred_svc))\nprint(classification_report(y_test, y_pred_svc))","512090b3":"\"\"\"param_grid_rf = {\"max_depth\": range(3,10),\n                  \"max_features\": [3,5,7,9,11,13,15,17,20],\n                  \"min_samples_leaf\": [5,10,15,20,25,30],\n                  \"n_estimators\": [3,5,10,25,50,150]}\n\nrf = RandomForestClassifier()\ngs_rf = GridSearchCV(rf, param_grid=param_grid_rf, cv=5, scoring=\"recall\", verbose=4)\ngs_rf.fit(X_train, y_train)\n\ngs_rf.best_params_\n\"\"\"\n# {'max_depth': 9, 'max_features': 15, 'min_samples_leaf': 5, 'n_estimators': 25}\n\n\nrf = RandomForestClassifier(max_depth=9, max_features=15, min_samples_leaf=5, n_estimators=25)\nrf.fit(X_train, y_train)\ny_pred_rf = rf.predict(X_test)\nprint(accuracy_score(y_pred_rf, y_test))\nprint(confusion_matrix(y_test, y_pred_rf))\nprint(classification_report(y_test, y_pred_rf))","a2220b04":"##Code by Shoaib Mehmood https:\/\/www.kaggle.com\/shoaibmnagi\/credit-risk-modelling-german-data\/notebook\n\nnb = GaussianNB()\nnb.fit(X_train, y_train)\ny_pred_nb = nb.predict(X_test)\nprint(accuracy_score(y_pred_nb, y_test))\nprint(confusion_matrix(y_test, y_pred_nb))\nprint(classification_report(y_test, y_pred_nb))","1ea8d423":"\"\"\"param_grid_xgb = {\"max_depth\": range(3,10),\n                  \"subsample\": [0.5,0.6,0.7,0.8,0.9,1],\n                  \"eta\": [0.01,0.03,0.05,0.07,0.09,0.14,0.19],\n                  \"colsample_bytree\": [0.5,0.6,0.7,0.8,0.9,1],\n                  \"n_estimators\": [3,5,10,25,50,150]}\n\nxgb = XGBClassifier()\ngs_xgb = GridSearchCV(xgb, param_grid=param_grid_xgb, cv=5, scoring=\"recall\", verbose=4)\ngs_xgb.fit(X_train, y_train)\n\ngs_xgb.best_params_ \"\"\"\n\n\"\"\"{'colsample_bytree': 1,\n 'eta': 0.19,\n 'max_depth': 8,\n 'n_estimators': 150,\n 'subsample': 0.8}\"\"\"\n\nxgb = XGBClassifier(eta=0.19, max_depth=8, n_estimators=150, subsample=0.8, colsample_bytree=1)\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)\nprint(accuracy_score(y_pred_xgb, y_test))\nprint(confusion_matrix(y_test, y_pred_xgb))\nprint(classification_report(y_test, y_pred_xgb))","dca4e7e1":"#Code by Shoaib Mehmood https:\/\/www.kaggle.com\/shoaibmnagi\/credit-risk-modelling-german-data\/notebook\n\nx_training, x_valid, y_training, y_valid = train_test_split(X_train, y_train,\n                                                            test_size=0.5,\n                                                            random_state=42)\n#specify models\nmodel1 = LogisticRegression()\nmodel2 = SVC(kernel='linear', gamma=10, C=0.8)\nmodel3 = GaussianNB()\nmodel4 = XGBClassifier(eta=0.19, max_depth=8, n_estimators=150, subsample=0.8, colsample_bytree=1)\n#fit models\nmodel1.fit(x_training, y_training)\nmodel2.fit(x_training, y_training)\nmodel3.fit(x_training, y_training)\nmodel4.fit(x_training, y_training)\n#make pred on validation\npreds1 = model1.predict(x_valid)\npreds2 = model2.predict(x_valid)\npreds3 = model3.predict(x_valid)\npreds4 = model4.predict(x_valid)\n#make pred on test\ntestpreds1 = model1.predict(X_test)\ntestpreds2 = model2.predict(X_test)\ntestpreds3 = model3.predict(X_test)\ntestpreds4 = model4.predict(X_test)\n#form new dataset from valid and test\nstackedpredictions = np.column_stack((preds1, preds2, preds3, preds4))\nstackedtestpredictions = np.column_stack((testpreds1, testpreds2,\n                                              testpreds3, testpreds4))\n#make meta model\nmetamodel = LogisticRegression()\nmetamodel.fit(stackedpredictions, y_valid)\nfinal_predictions = metamodel.predict(stackedtestpredictions)\n    \nprint(accuracy_score(final_predictions, y_test))\nprint(confusion_matrix(y_test, final_predictions))\nprint(classification_report(y_test, final_predictions))","18bbc7ae":"from sklearn.metrics import roc_curve, roc_auc_score\n\nresults_table = pd.DataFrame(columns = ['models', 'fpr','tpr','auc'])\n\npredictions = {'LR': y_pred_log, 'SVC': y_pred_svc, 'NB': y_pred_nb, 'XGB': y_pred_xgb, 'Stacked': final_predictions}\n\nfor key in predictions:\n    fpr, tpr, _ = roc_curve(y_test, predictions[key])\n    auc = roc_auc_score(y_test, predictions[key])\n    \n    results_table = results_table.append({'models': key,\n                                         'fpr' : fpr,\n                                         'tpr' : tpr,\n                                         'auc' : auc}, ignore_index=True)\n    \nresults_table.set_index('models', inplace=True)\n\nprint(results_table)\n\nfig = plt.figure(figsize = (8,6))\n\nfor i in results_table.index:\n    plt.plot(results_table.loc[i]['fpr'], \n             results_table.loc[i]['tpr'], \n             label = \"{}, AUC={:.3f}\".format(i, results_table.loc[i]['auc']))\n    \nplt.plot([0,1], [0,1], color = 'black', linestyle = '--')\n\nplt.xticks(np.arange(0.0, 1.1, step=0.1))\nplt.xlabel(\"False Positive Rate\", fontsize=15)\n\nplt.yticks(np.arange(0.0, 1.1, step=0.1))\nplt.ylabel(\"True Positive Rate\", fontsize=15)\n\nplt.title('ROC Curve Analysis', fontweight='bold', fontsize=15)\nplt.legend(prop = {'size':13}, loc = 'lower right')\n\nplt.show()","a349c775":"<h1 style=\"background-color: #DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">K-Nearest Neighbors<\/h1>","c3e77e19":"<h1 style=\"background-color:#DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Support Vector Classification<\/h1>","6efb93ee":"![](https:\/\/media.springernature.com\/lw685\/springer-static\/image\/art%3A10.1186%2Fs12883-017-0854-x\/MediaObjects\/12883_2017_854_Fig1_HTML.gif)bmcneurol.biomedcentral.com","d41a51cc":"<h1 style=\"background-color: #DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\"> Stacked Model<\/h1>","8ddb9a39":"<h1 style=\"background-color: #DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Prognostic factors in ALS: A critical review<\/h1>\n\nAuthors: Adriano Chio, Giancarlo Logroscino, Orla Hardiman, Robert Swingler, Douglas Mitchell, Ettore Beghi,6 Bryan G. Traynor.\n\nAmyotroph Lateral Scler. 2009 Oct-Dec; 10(5-6): 310\u2013323. - doi: 10.3109\/17482960802566824\n\n\n<font color=\"#EC7063\">Overall survival<\/font>\n\n\n\"The median survival from onset to death in ALS is reported to vary from 20 to 48 months with ALS referral centres reporting longer survival times. This wide range narrows when considering population-based studies, which are more likely to reflect the experience of the general ALS population (20\u201336 months). All studies report that 5 to 10% of ALS patients survive for more than 10 years.\"\n\n<font color=\"#EC7063\">Specific Prognostic Factors<\/font>\n\nDemographic factors: AGE, GENDER.\n\nClinical factors: Site of onset Bulbar onset disease is associated with a worse prognosis than spinal onset, \n\nDiagnostic delay, Family history of ALS, Rate of disease progression\n\nPsychosocial factors: Although often overlooked by clinicians, psychosocial factors seem to play an important role in ALS outcome.\n\nTherapeutic factors: Therapeutic interventions (licensing of Riluzole in 1996).\n\nInterdisciplinary care:  Being followed by a neurologist versus a non-neurologist.\n\nImplication for clinical trials. Use of El Escorial classification for the enrolment of patients for clinical trials.\n\nPatient stratification for placebo-controlled trials.\n\nhttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3515205\/","506f5fc6":"<h1 style=\"background-color: #DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">XGBoost<\/h1>","0992cd42":"<h1 style=\"background-color: #DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Conclusion<\/h1>","8b836c45":"I'll try to interpret those results.\n\nXGBoost classifier had the higher Accuracy: 0.78\n\nAnd while plotting ROC curves and calculate the AUC for all our models XGBoost again, with AUC=0.583\n\nThe Stacked model: accuracy 0.74","12c3cee8":"<h1 style=\"background-color:#DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Logistic Regression<\/h1>","450e475f":"<h1 style=\"background-color:#DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Random Forest<\/h1>","92394b07":"<h1 style=\"background-color: #DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">Naive-Bayes<\/h1>"}}