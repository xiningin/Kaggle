{"cell_type":{"84496507":"code","20130a7f":"code","28b1ba2b":"code","00a09a04":"code","f3ff4492":"code","a9f98bb1":"code","125e6d54":"code","a2f70786":"code","492521a3":"code","533c7b5c":"code","33fd33c2":"code","dab78286":"code","d36d2150":"markdown","da1e3cc6":"markdown","d284aa9c":"markdown","e7ce099a":"markdown","4c4a7bec":"markdown","254e35f2":"markdown","ab6ed592":"markdown","ed5220cf":"markdown","c12072dc":"markdown","73184b7a":"markdown","e8a30293":"markdown","b6e4aaf2":"markdown","9b3b5420":"markdown","70a3fe7e":"markdown","1cb648bd":"markdown"},"source":{"84496507":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n# Any results you write to the current directory are saved as output.","20130a7f":"# load data set\nx_l = np.load('..\/input\/Sign-language-digits-dataset\/X.npy')\nY_l = np.load('..\/input\/Sign-language-digits-dataset\/Y.npy')\n\nimg_size = 64\nplt.subplot(1,2,1)\nplt.imshow(x_l[960].reshape(img_size,img_size))\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.imshow(x_l[2060].reshape(img_size,img_size))\nplt.axis('off')\n","28b1ba2b":"X = np.concatenate((x_l[822:1027], x_l[1855:2060] ), axis=0) # from 0 to 204 is 'one' sign and from 205 to 410 is 'five' sign \nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z, o), axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \" , X.shape)\nprint(\"Y shape: \" , Y.shape)","00a09a04":"# Then lets create x_train, y_train, x_test, y_test arrays\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]","f3ff4492":"X_train_flatten = X_train.reshape(number_of_train,X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test .reshape(number_of_test,X_test.shape[1]*X_test.shape[2])\nprint(\"X train flatten\",X_train_flatten.shape)\nprint(\"X test flatten\",X_test_flatten.shape)","a9f98bb1":"x_train = X_train_flatten.T\nx_test = X_test_flatten.T\ny_train = Y_train.T\ny_test = Y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","125e6d54":"\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension, 1), 0.01)\n    b = 0.0\n    return w,b\n\n# Sigmoid function\n# z = np.dot(w.T, x_train) +b\ndef sigmoid(z):\n    y_head = 1\/(1 + np.exp(-z))  # sigmoid function finding formula\n    return y_head\nsigmoid(0)  # o should result in 0.5\n\n# w,b = initialize_weights_and_bias(4096)","a2f70786":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","492521a3":"\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n#parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.009,number_of_iterarion = 200)","533c7b5c":" # prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n","33fd33c2":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 4096\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 150)","dab78286":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\nprint(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","d36d2150":"<a id=\"3\"><\/a> <br>\n# Logistic Regression\n* The first thing that comes to mind when we talk about binary classification (0 and 1 output) is logistic regression.\n* Logistic regression is actually a very simple neural network .That's why it is important. \n","da1e3cc6":"<a id=\"1\"><\/a> <br>\n# Preparation of Data Set\n\n* We will use \"sign language digits data set\" for this tutorial.\n* If you want to know the details of the data, click on the <u>data<\/u> (you will see on the top this kernel ) and click <u> overview<\/u> section to get all the information about the data.\n* Also you can find overview for data  https:\/\/github.com\/ardamavi\/Sign-Language-Digits-Dataset\n* Briefly, this data consists of images. We should first   <u>np.array<\/u>  format convert to <u>image<\/u> . So we will do some arrange \n* Lets prepare our X and Y arrays. X is image array (one and five signs) and Y is label array (0 and 1).","d284aa9c":"* We fit the data\n* Time to predict now","e7ce099a":"** *  ***<font color='purple' >\n    \n ##  Welcome to   Logistic Regression with Sign Language  \n* My goal is to share with you what I have learned . I will  explain most thing  about Logistic Regression.\n    \n*  I will explain this briefly , and I am going to emphasize mostly  keywords .\n\n* Lets look at content.\n \n\n<font color='red'>\n<br>Content:\n* [Introduction](#1)\n* [Preparation of Data Set](#2)\n* [Logistic Regression](#3) \n   * [Initializing parameters](#4)\n   * [Forward  and Backward Propagation](#5)\n   * [Updating  Parameters](#6)\n   * [Logistic Regression with Sklearn](#7)\n* [Conclusion](#8)\n    \n    \n","4c4a7bec":"<a id=\"5\"><\/a> <br>\n## Forward  and Backward Propagation\n\n### Forward Propagation :\n* find z = w.T*x+b\n* y_head = sigmoid(z)\n* loss(error) = loss(y,y_head)\n* cost = sum(loss)\n\n### Backward Propagation :\nTo reduce losses, we need to update our cost with the Gradient Descent Method. So we do Backward Propagation.\n","254e35f2":"<a id=\"8\"><\/a> <br>\n# Conclusion\n\n* If this tutorial is not enough you can check Deep Learning Tutorial for Beginners prepared by DATAI TEAM.\nhttps:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners\n* After this tutorial, my aim is to prepare 'kernel' which is connected to artificial neural network.\n* If you have any suggestions, can you write for me? I wil be glad :)\n* Thank you for your suggestion and votes :)\n\n","ab6ed592":"* In order to create image array, I concatenate one sign and five sign arrays\n* Then I create label array 0 for 'one' sign images and 1 for 'five' sign images.\n\nI'm guessing maybe you get confused but ;\n\n## Don't get confused\n\nThe pictures I've compared are 'one' sign images and  for 'five' sign images. \nI mean, If I get 0 as a result of output estimate 'one' sign picture.\nIf I get 1 as a result of output estimate 'five' sign picture","ed5220cf":"* We maked prediction.\n* Now lets put them all together.","c12072dc":"* Now we have 3 dimensional input array (X) so we need to make it flatten (2D) in order to use as input for our first deep learning model.\n* Our label array (Y) is already flatten(2D) so we leave it like that.\n* Lets flatten X array(images array).","73184b7a":"* The shape of the X is (410, 64, 64)\n    * 410 means that we have 410 images (zero and one signs)\n    * 64 means that our image size is 64x64 (64x64 pixels)\n* The shape of the Y is (410,1)\n    *  410 means that we have 410 labels (0 and 1) \n* Lets split X and Y into train and test sets.\n    * test_size = percentage of test size. test = 15% and train = 85%\n    * random_state = use same seed while randomizing. It means that if we call train_test_split repeatedly, it always creates same train and test distribution because we have same random_state.","e8a30293":"<a id=\"6\"><\/a> <br>\n# Updating  Parameters\n* To reduce our cost function, we update parameters w, b.\n* w = w - learning_rate * gradients[\"derivative_weight\"]\n* b = b - learning_rate * gradients[\"derivative_bias\"]","b6e4aaf2":"* As shown in the output of the above cod, we have 348 images and each image has 4096 pixels in image train array.\n* Also, we have 82 images and each image has 4096 pixels in image test array.\n* Now we should take transpose. Actually there is no technical answer but we can say do for take input and output.","9b3b5420":"<a id=\"7\"><\/a> <br>\n## Logistic Regression with Sklearn","70a3fe7e":"<a id=\"1\"><\/a> <br>\n# INTRODUCTION\n\n\n*  In fact, logistic regression is a  <u> classification <\/u> algorithm, unlike other regression models.\n* Logistic Regression is very important for entering deep learning . \n* At the end of this tutorial you will have enought information about logistic regression.\n* After understanding this topic, you will be able to easily learning to Artificial Neural Network.\n \n####  Let's use the import library","1cb648bd":"<a id=\"4\"><\/a> <br>\n## Initializing parameters\n* Our parameters are \"w\" and \"b\" respectively \"Weights\" and \"Bias\"\n* Each pixels have own weights.\n* Also initial bias is 0\n* The first step is multiplying each pixels with their own weights.\n*  We can say => z = b + px1w1 + px2w2 + ... + px4096*w409\n* Sigmoid function makes z between zero and one so that is probability.\n"}}