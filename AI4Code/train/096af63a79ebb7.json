{"cell_type":{"23f68c6e":"code","664845aa":"code","e9ab995d":"code","0d4fb917":"code","3dca0c22":"code","55127dde":"code","c4d2a8ba":"code","0c865887":"code","c9a3edc1":"code","1ad3ed7f":"code","7cc8c01d":"code","4d945ef6":"code","71e30049":"code","56810092":"code","33301577":"code","548efb73":"code","a209de88":"markdown","f9354376":"markdown","50338d30":"markdown","59bb519c":"markdown","376468a4":"markdown","8bcfed68":"markdown","4d97ef8b":"markdown","e1e2e175":"markdown","763abd45":"markdown","3e9a0035":"markdown"},"source":{"23f68c6e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nimport random\n# Any results you write to the current directory are saved as output.","664845aa":"train_data = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")","e9ab995d":"X_train = train_data.values\nX_train_all = X_train[:, 1:]\ny_train_all = train_data.label.to_numpy()\n\nX_test, X_train = np.split(X_train_all, [5000], axis=0)\ny_test, y_train = np.split(y_train_all, [5000], axis=0)\n\nX_real_test = test_data.values","0d4fb917":"print(\"There are %s digits for training with %s pixels each\" %(X_train.shape[0], X_train.shape[1]))\nprint(\"There are %s digits for testing\" %(X_test.shape[0]))\nprint(\"There are %s digits for the final test\" %(X_real_test.shape[0]))","3dca0c22":"fig, axs = plt.subplots(figsize=(7, 7), nrows=2, ncols=5)\nfor row in range(0,2):\n    for col in range(0,5):\n        rand_int = random.randint(0, 36999)\n        rand_digit = X_train[rand_int].reshape((28,28))\n        pos = axs[row][col].imshow(rand_digit, cmap=\"binary\")\n        axs[row][col].set_title(\"Label: %s\" %(y_train[rand_int]))\nplt.tight_layout()","55127dde":"fig, ax = plt.subplots(figsize=(10, 5), ncols=2)\ncounts1 = np.bincount(y_test)\nax[0].bar(range(10), counts1, width=0.8, align='center', color=\"bisque\", edgecolor=\"black\")\nax[0].set(xticks=range(10), xlim=[-1, 10])\nax[0].set_title('Distribution of testing digits')\nax[0].set_ylabel(\"count\")\nax[0].set_xlabel(\"digits\")\n\ncounts2 = np.bincount(y_train)\nax[1].bar(range(10), counts2, width=0.8, align='center', color=\"burlywood\", edgecolor=\"black\")\nax[1].set(xticks=range(10), xlim=[-1, 10])\nax[1].set_title('Distribution of training digits')\nax[1].set_xlabel(\"digits\")\nplt.show()","c4d2a8ba":"class Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\n    \nclass Net(nn.Module):\n    def __init__(self, hls):\n        super(Net, self).__init__()\n        self.bn = nn.BatchNorm2d(1)\n        \n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv1_bn = nn.BatchNorm2d(10)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_bn = nn.BatchNorm2d(20)        \n#         self.conv2_drop = nn.Dropout2d()\n        \n        self.flatten = Flatten()\n        self.fc1 = nn.Linear(320, hls)\n        self.fc2 = nn.Linear(hls, 10)\n        \n\n    def forward(self, x):\n#         x = F.relu(F.max_pool2d(self.conv1(x), 2))\n#         x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = self.bn(x)\n        x = F.relu(F.max_pool2d(self.conv1_bn(self.conv1(x)), 2))\n        x = F.relu(F.max_pool2d(self.conv2_bn(self.conv2(x)), 2))     \n#         x = self.conv2_drop(x)\n\n        x = self.flatten(x)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n    \ncnet = Net(310)\nprint(\"The network:\")\nprint(cnet)","0c865887":"def score_cnn(net, criterion):\n    net.eval()\n    test_loss = 0\n    correct = 0\n\n    batch_sz = 1000\n    N, D = X_test.shape\n    n_batches = N \/\/ batch_sz\n    for j in range(n_batches):\n        Xbatch = X_test[j*batch_sz:(j*batch_sz+batch_sz)]\n        Ybatch = y_test[j*batch_sz:(j*batch_sz+batch_sz)]\n        Xbatch = Xbatch.reshape(Xbatch.shape[:-1] + (28,28))\n        Xbatch = np.expand_dims(Xbatch, axis=1)\n\n        data_test, target_test = torch.tensor(Xbatch), torch.tensor(Ybatch)\n        data_test = data_test.type(torch.FloatTensor)\n        net_out= net(data_test)\n        # sum up batch loss\n        target_test = target_test.type(torch.LongTensor)\n        # get the index of the max log-probability (one hot encoding -> array of predictions)\n        pred = net_out.data.max(1)[1]\n        correct += pred.eq(target_test.data).sum()\n    return (100. * correct.item() \/ X_test.shape[0])","c9a3edc1":"def train(X_train, y_train, max_tries=30, epochs=40, hls=310, lr=12e-4):\n    batch_sz = 1000\n    N, D = X_train.shape\n    print_period = 100\n\n    best_validation_rate = 0\n    best_hls = None\n    best_lr = None\n    best_losses = None\n    best_cnet = None\n\n    for _ in range(max_tries):\n        cnet = Net(hls)\n        # Adam worked better than the stochastic gradient descent optimizer\n        optimizer = optim.Adam(cnet.parameters(), lr=lr)\n        # Negative log likelihood loss\n        criterion = nn.NLLLoss()\n        cnet.train()\n\n        n_batches = N \/\/ batch_sz\n        losses = []\n        for epoch in range(epochs):\n            if n_batches > 1:\n                X_train, y_train = shuffle(X_train, y_train)\n            for j in range(n_batches):\n                Xbatch = X_train[j*batch_sz:(j*batch_sz+batch_sz)]\n                Ybatch = y_train[j*batch_sz:(j*batch_sz+batch_sz)]\n                Xbatch = Xbatch.reshape(Xbatch.shape[:-1] + (28,28))\n                Xbatch = np.expand_dims(Xbatch, axis=1)\n                data = torch.from_numpy(Xbatch)\n                target = torch.from_numpy(Ybatch)\n                data, target=data.type(torch.DoubleTensor),target.type(torch.DoubleTensor)\n\n                # initialize weights to zero\n                optimizer.zero_grad()\n                # log softmax output\n                net_out = cnet(data.float())\n                # negative log likelihood loss between the output of our network and our target batch data\n                target = target.type(torch.LongTensor)\n                loss = criterion(net_out, target)\n                # gradients computed\n                loss.backward()\n                # gradient step\n                optimizer.step()\n                losses.append(loss.data.item())\n\n                if j*batch_sz % 37000 == 0:\n                    print('Train Epoch: {}  \\tLoss: {:.6f}'.format(\n                            epoch, loss.data.item()))\n\n        validation_accuracy = score_cnn(cnet, criterion)\n        print(\n          \"validation_accuracy: %.4f, settings: %s, %s\" %\n            (validation_accuracy, hls, lr)\n        )\n        print(\"\")\n        if validation_accuracy > best_validation_rate:\n            best_validation_rate = validation_accuracy\n            best_hls = hls\n            best_lr = lr\n            best_losses = losses\n            best_cnet = cnet\n\n        # select new hyperparams\n        hls = best_hls + np.random.randint(-2, 4)*20\n        lr = best_lr + np.random.randint(-2, 4)*1e-4\n\n    print(\"Best validation_accuracy:\", best_validation_rate)\n    print(\"Best settings:\")\n    print(\"hidden layer size:\", best_hls)\n    print(\"learning_rate:\", best_lr)\n    \n    return best_losses, best_cnet","1ad3ed7f":"best_losses, best_cnet = train(X_train=X_train, y_train=y_train, max_tries=5, epochs=30, hls=310, lr=11e-4)","7cc8c01d":"plt.plot(best_losses)\nplt.title(\"Final loss: %.3f\" % best_losses[-1])\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.show()","4d945ef6":"# # losses = train(X_train=X_train_all, y_train=y_train_all, max_tries=1, epochs=50, hls=310, lr=11e-4)\n# batch_sz = 1000\n# N, D = X_train_all.shape\n# print(X_train_all.shape)\n# print_period = 100\n# epochs = 30\n# hls = 310\n# lr = 12e-4\n\n# cnet = Net(hls)\n# # stochastic gradient descent optimizer\n# #optimizer = optim.SGD(cnet.parameters(), lr=lr, momentum=m)\n# optimizer = optim.Adam(cnet.parameters(), lr=lr)\n# # Negative log likelihood loss\n# criterion = nn.NLLLoss()\n# cnet.train()\n\n# n_batches = N \/\/ batch_sz\n# for epoch in range(epochs):\n#     if n_batches > 1:\n#         X, Y = shuffle(X_train_all, y_train_all)\n#     for j in range(n_batches):\n#         Xbatch = X[j*batch_sz:(j*batch_sz+batch_sz)]\n#         Ybatch = Y[j*batch_sz:(j*batch_sz+batch_sz)]\n#         Xbatch = Xbatch.reshape(Xbatch.shape[:-1] + (28,28))\n#         Xbatch = np.expand_dims(Xbatch, axis=1)\n#         data = torch.from_numpy(Xbatch)\n#         target = torch.from_numpy(Ybatch)\n#         data, target=data.type(torch.DoubleTensor),target.type(torch.DoubleTensor)\n\n#         # initialize weights to zero\n#         optimizer.zero_grad()\n#         # log softmax output\n#         net_out = cnet(data.float())\n#         # negative log likelihood loss between the output of our network and our target batch data\n#         target = target.type(torch.LongTensor)\n#         loss = criterion(net_out, target)\n#         # gradients computed\n#         loss.backward()\n#         # gradient step\n#         optimizer.step()\n\n#         if j*batch_sz % 60000 == 0:\n#             print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n#                     epoch, j * batch_sz, X_train_all.shape[0],\n#                            100. * j * batch_sz \/ X_train_all.shape[0], loss.data.item()))","71e30049":"best_cnet.eval()\nN, D = X_real_test.shape\nX_real = X_real_test.reshape(X_real_test.shape[:-1] + (28,28))\nX_real = np.expand_dims(X_real, axis=1)\ndata_test= torch.tensor(X_real)\ndata_test = data_test.type(torch.FloatTensor)\nnet_out = best_cnet(data_test)\npred = net_out.data.max(1)[1]\npred = pred.detach().numpy()","56810092":"output = pd.DataFrame({'ImageId':range(1, 28001), 'Label':pred})\noutput","33301577":"fig, axs = plt.subplots(figsize=(7, 7), nrows=5, ncols=5)\ni = 0\nfor row in range(0,5):\n    for col in range(0,5):\n        rand_digit = X_real_test[i].reshape((28,28))\n        pos = axs[row][col].imshow(rand_digit, cmap=\"binary\")\n        axs[row][col].set_title('Digit: %s' % output.Label[i])\n        i = i+1\nplt.tight_layout()","548efb73":"output.to_csv('submission.csv', index=False)","a209de88":"# What do the handwritten digits look like?\nHere are a few handwritten digits of the training group:","f9354376":"# After finding a decent model, it is time for the final test\nThe optimal parameters that were found above will be used for the final test (the optimal parameters might not be the same every time). \n\n~~Notice that all of the original training data will be used to train the model this time. There are 42000 digits in the total training data.~~","50338d30":"Getting the data ready","59bb519c":"# Setting up the network\nThe distribution is mostly even for the data that will be used for the training\/validation. Now a random search will be used to find the best parameters for the model first.","376468a4":"Plotted some test digits with predicted labels below for a sanity check. Some of them may be incorrect","8bcfed68":"The model with the best validation accuracy is saved and will be used to predict the final test set.","4d97ef8b":"# The purpose of this notebook is to explore setting up a neural network using PyTorch","e1e2e175":"# The training data is split into two sets for training and testing.\nThe testing data will have 5000 digits and the training will be left with 37000 digits.\n\nThis is so testing can be done to improve the model before the X_real_test set will be used for the final output.","763abd45":"# Let's make sure that the data put aside for testing is distributed with all of the digits being represented equally","3e9a0035":"* The final losses are plotted below for the model with the best validation_accuracy:"}}