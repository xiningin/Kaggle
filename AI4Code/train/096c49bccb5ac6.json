{"cell_type":{"31460abe":"code","e0109307":"code","3bfe0f35":"code","95ebf2f4":"code","fd78ac4e":"code","3f7f6b63":"code","8c2d315f":"code","0e84f739":"code","6d229f97":"code","c4c41d28":"code","e080820a":"code","2402c327":"code","86d8cc47":"code","df2a393f":"code","5860a02d":"code","0362ce22":"code","9f203d66":"code","6157330b":"code","0cc78ad0":"code","83784235":"code","74e041dc":"code","059153f0":"code","475ce053":"code","a3841a8e":"code","5b0f5061":"code","114e1c04":"code","937c0937":"code","40dbcefb":"code","f3607953":"code","cbe177bc":"code","3dddbcef":"code","49c9c966":"code","e63e0c95":"code","5aa171d5":"code","9d790e62":"code","bcb2e572":"code","b13dfab4":"code","6cea63be":"code","244f0444":"code","4ebd3e91":"code","d93559ec":"code","ccc2a19a":"code","9fe5d083":"code","ce5aea35":"code","91597e03":"code","2296d581":"code","118ba91b":"markdown","53214c00":"markdown","8f55c858":"markdown","867052ba":"markdown","1d279e43":"markdown","49f455ec":"markdown","c5343f2f":"markdown","e4990383":"markdown","1cde4323":"markdown","3c3c81d1":"markdown","6852994c":"markdown","1cf5af7c":"markdown","73f50728":"markdown","38472d70":"markdown","81ab5212":"markdown","3b62e725":"markdown","7890dad8":"markdown","b18fd761":"markdown","a22a3e9c":"markdown","f7a8823b":"markdown","b1254a41":"markdown","0c43440b":"markdown","550352eb":"markdown","10069440":"markdown","03ee013d":"markdown","9b8b4150":"markdown","7f286883":"markdown","41e05cad":"markdown","833785f8":"markdown","1f78fef8":"markdown","6b31d5cf":"markdown","4b15f25c":"markdown","488b120b":"markdown","160c0419":"markdown","a34bdc68":"markdown","9e2df73c":"markdown","22ce2dfb":"markdown","71f0f1bf":"markdown","83ccbd85":"markdown","0a90c285":"markdown","76a305b0":"markdown","ec9a08c4":"markdown","13fc4fc9":"markdown","49a9283c":"markdown","425d1787":"markdown","46c47b42":"markdown","04a5da63":"markdown","992da308":"markdown","be86aba9":"markdown","f5cda7c9":"markdown","abc65748":"markdown","7f6ff936":"markdown","6774c9fd":"markdown","d4f330c8":"markdown","b11cb225":"markdown"},"source":{"31460abe":"# Import necessary modules\n\nimport numpy as np \nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e0109307":"# Read in dataframe\ndf = pd.read_csv('\/kaggle\/input\/airline-passenger-satisfaction\/train.csv', index_col='id')","3bfe0f35":"# Drop unneeded row and sort by ascending ID's\ndf = df.drop('Unnamed: 0', axis=1)\ndf = df.sort_values('id', ascending=True)","95ebf2f4":"df.head()","fd78ac4e":"df.shape","3f7f6b63":"df.info()","8c2d315f":"df.nunique()[:10].sort_values(ascending=False)","0e84f739":"df.isnull().sum()","6d229f97":"# Dropping rows with NaN values\n\ndf = df.dropna().copy()\n\ndf.shape","c4c41d28":"df.duplicated().any()","e080820a":"df.describe()","2402c327":"sns.boxplot(x=df['Departure Delay in Minutes'])","86d8cc47":"sns.boxplot(x=df['Arrival Delay in Minutes'])","df2a393f":"df.loc[df['Departure Delay in Minutes'] > 1300]\n\ndf.loc[df['Arrival Delay in Minutes'] > 1250]","5860a02d":"df.shape","0362ce22":"outliers = df[df['Arrival Delay in Minutes'] > 1250].index\ndf.drop(outliers, inplace=True)\ndf.shape","9f203d66":"sns.boxplot(x=df['Departure Delay in Minutes'])","6157330b":"sns.boxplot(x=df['Arrival Delay in Minutes'])","0cc78ad0":"df['satisfaction'].value_counts()","83784235":"df['satisfaction'] = pd.get_dummies(df['satisfaction'])\ndf['satisfaction']\n\n# 0 = neutral or dissatisfied\n# 1 = satisfied","74e041dc":"df.dtypes","059153f0":"df['Gender'] = pd.get_dummies(df['Gender'])\ndf['Customer Type'] = pd.get_dummies(df['Customer Type'])\ndf['Type of Travel'] = pd.get_dummies(df['Type of Travel'])\ndf['Class'] = pd.get_dummies(df['Class'])\ndf.dtypes","475ce053":"corr = df.corr()\n\nnp.fill_diagonal(corr.values, 0)\n\ncorr.replace(0, np.nan, inplace=True)\n\ncorr","a3841a8e":"plt.figure(figsize = (18,9))\nsns.heatmap(corr, annot=True)","5b0f5061":"corr.unstack().sort_values(kind='quicksort', na_position='first').drop_duplicates(keep='first')","114e1c04":"# We call the absolute value func. because whether the variables are positively or negatively correlated to our y-variable is irrelevant, as they're still highly correlated\n\ndf.corr().abs()['satisfaction'].sort_values(ascending = False)","937c0937":"sns.boxplot(x='Inflight wifi service', y='Online boarding', data=df)","40dbcefb":"sns.boxplot(x='satisfaction', y='Online boarding', data=df)","f3607953":"sns.lmplot(x='Arrival Delay in Minutes', y='Departure Delay in Minutes', \n                hue='satisfaction', data=df)","cbe177bc":"sns.scatterplot(x='Inflight wifi service', y='Ease of Online booking', \n                hue='satisfaction', data=df)","3dddbcef":"df['Inflight wifi service'].value_counts()","49c9c966":"df['Ease of Online booking'].value_counts()","e63e0c95":"import plotly.express as px\n\nfig = px.scatter_3d(df.head(1000), x='On-board service', y='Leg room service', z='Cleanliness', \n                   color='satisfaction')\nfig.show()","5aa171d5":"import plotly.express as px\n\nfig = px.scatter_3d(df.head(1000), x='Online boarding', y='Inflight entertainment', z='Seat comfort', \n                   color='satisfaction')\nfig.show()","9d790e62":"X = df.drop('satisfaction', axis=1)\ny = df['satisfaction']","bcb2e572":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)","b13dfab4":"models = {'KNN': KNeighborsClassifier(),\n          'Decision Tree' : DecisionTreeClassifier(),\n         'Random Forest': RandomForestClassifier()}\n\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    np.random.seed(42)\n    \n    model_scores = {}\n    \n    for name, model in models.items():\n        model.fit(X_train, y_train)\n        \n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","6cea63be":"model_scores = fit_and_score(models=models, \n                             X_train=X_train,\n                            X_test=X_test,\n                            y_train=y_train,\n                            y_test=y_test)\nmodel_scores","244f0444":"model_comp = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_comp.T.plot.bar();","4ebd3e91":"# rf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n#           \"max_depth\": [None, 3, 5, 10],\n#           \"min_samples_split\": np.arange(2, 20, 2),\n#           \"min_samples_leaf\": np.arange(1, 20, 2)}\n\n# rs_rf = RandomizedSearchCV(RandomForestClassifier(),\n#                           param_distributions=rf_grid,\n#                           cv=5,\n#                           verbose=True)\n\n# rs_rf.fit(X_train, y_train);\n\n# rs_rf.best_params_","d93559ec":"# rf = RandomForestClassifier(n_estimators=210, min_samples_split=14, min_samples_leaf=15, max_depth=None)\n# rf.fit(X_train, y_train)\n# rf_pred = rf.predict(X_test)\n# rf.score(X_test, y_test)","ccc2a19a":"xgb = XGBClassifier(eval_metric='logloss', use_label_encoder=False)\n\nxgb.fit(X_train, y_train)\nxgb_pred = xgb.predict(X_test)\nxgb.score(X_test, y_test)","9fe5d083":"params_xgb = {'n_estimators': [50,100,250,400,600,800,1000], 'learning_rate': [0.2,0.5,0.8,1]}\nrs_xgb =  RandomizedSearchCV(xgb, param_distributions=params_xgb, cv=5)\nrs_xgb.fit(X_train, y_train)\nxgb_pred_2 = rs_xgb.predict(X_test)\nrs_xgb.score(X_test, y_test)","ce5aea35":"print(confusion_matrix(y_test, xgb_pred))","91597e03":"sns.set(font_scale=1.5) # Increase font size\n\ndef plot_conf_mat(y_test, xgb_pred):\n    \n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, xgb_pred),\n                     annot=True, # Annotate the boxes\n                     cbar=False,\n                    fmt='g', # don't use scientic notation\n                    cmap='Blues')\n    \n    plt.xlabel(\"true label\", weight='bold')\n    plt.ylabel(\"predicted label\", weight='bold')\n    \nplot_conf_mat(y_test, xgb_pred)","2296d581":"print(classification_report(y_test, xgb_pred))","118ba91b":"#### 1.2.3. Outliers\n<a id=\"1.2.3.\"><\/a>","53214c00":"### 1.3. Final Data Preparation\n<a id=\"1.3.\"><\/a>","8f55c858":"People who get better Inflight wifi service likely had a better online boarding experience","867052ba":"Above we see how on-board service, leg room service, and cleanliness affect a passenger's satisifaction. Looking at the features in the plotly graph above, we see that the higher ranking a passenger gives to each category the more likely they are to be satisified with their overall airline experience. Specificially, we see the highest importance of a passenger's satisfaction with their on-board service experience.","1d279e43":"Looking at the scores of our models we see that our Decision Tree and Random Forest models both perform very well! However, our KNN model doesn't perform well; therefore, we'll discard it. Now, let's visualize our results!","49f455ec":"First, we'll look at the key numbers of the datset (mean, std, etc) to see if we can detect any anomalies; we'll use the describe function.","c5343f2f":"It looks like our XGBoost model performs the best out of all our models -- with an +96% score! Now, let's tune its hyperparameters.","e4990383":"We check to see if there are any duplicate values; luckily there are no duplicates values. Now, let's move on to handling outliers in our dataset.","1cde4323":"Next, we'll print out the strongest correlations between all of our variables.","3c3c81d1":"Once again, we see that tuning our model's hyperparameters doesn't make much of a difference.","6852994c":"As we can see, dropping the rows with NaN values had a very small impact on the data as a whole.","1cf5af7c":"Here, we'll convert our y-variable's 2 categorical values (neutral or dissatisfied and satisfied) into 0 and 1 in order for our machine learning models to be able to classify the data","73f50728":"#### 1.2.1. NaN Values\n<a id=\"1.2.1.\"><\/a>","38472d70":"### 1.2. Data Cleaning\n <a id=\"1.2.\"><\/a>","81ab5212":"Above we see how online boarding, inflight entertainment, and seat comfort all contribute to a passenger's airline satisfaction. Overall, the insights from this plot are similar to the first plotly graph above.","3b62e725":"#### 2.2.1. XGBoost: Hyperparameter tuning\n<a id=\"2.2.1.\"><\/a>","7890dad8":"### 2.4. Classification Report\n<a id=\"2.4.\"><\/a>","b18fd761":"### 2.1. KNN, Decision Tree, Random Forest\n<a id=\"2.1.\"><\/a>","a22a3e9c":"### 1.4. Data Visualization\n<a id=\"1.4.\"><\/a>","f7a8823b":"So, we see that we have 310 missing values in the Arrival Delay in Minutes row. At this point, we could either impute these missing values or remove them. We're going to drop the rows with NaN values because it's not worth the possibility of somewhat skewing the data by imputing the mean when we have over 100k observations (rows).","b1254a41":"Above, we see the strong correlation between Departure Delay and Arrival Delay; we can also see whether the passenger was satisified or not","0c43440b":"We will use these insights to guide our data visualization.","550352eb":"### 2.3. Confusion Matrix\n<a id=\"2.3.\"><\/a>","10069440":"Above we sucessfully removed the 2 rows that contained outliers. Now if we look at a boxplot of these 2 columns again, we can see the data is more concentrated than before.","03ee013d":"# 3. Conclusion\n<a id=\"3.\"><\/a>","9b8b4150":"Finally, we'll print out the highest correlated variables to our y-variable (satisfaction).","7f286883":"Now, let's perform a train-test-split on our data to split our data into train and test sets.","41e05cad":"First, we'll create our X and y variables","833785f8":"The following features: Gender, Customer Type, Type of Travel, and Class are all currently categorical data (dtype: \"object\"). However, we need to convert it to numerical data in order for our Machine Learning model(s) to be able understand the data. Therefore, we'll do just that using the get_dummies function from pandas. ","1f78fef8":"For some people, even though they had a good online boarding experience, they weren't satisified. On the other hand, some people who only had a decent online boarding experience turned out to be satisified with their overall flying experience. Very interesting!","6b31d5cf":"Gender: Gender of the passengers (Female, Male)\n\nCustomer Type: The customer type (Loyal customer, disloyal customer)\n\nAge: The actual age of the passengers\n\nType of Travel: Purpose of the flight of the passengers (Personal Travel, Business Travel)\n\nClass: Travel class in the plane of the passengers (Business, Eco, Eco Plus)\n\nFlight distance: The flight distance of this journey\n\nInflight wifi service: Satisfaction level of the inflight wifi service (0:Not Applicable;1-5)\n\nDeparture\/Arrival time convenient: Satisfaction level of Departure\/Arrival time convenient\n\nEase of Online booking: Satisfaction level of online booking\n\nGate location: Satisfaction level of Gate location\n\nFood and drink: Satisfaction level of Food and drink\n\nOnline boarding: Satisfaction level of online boarding\n\nSeat comfort: Satisfaction level of Seat comfort\n\nInflight entertainment: Satisfaction level of inflight entertainment\n\nOn-board service: Satisfaction level of On-board service\n\nLeg room service: Satisfaction level of Leg room service\n\nBaggage handling: Satisfaction level of baggage handling\n\nCheck-in service: Satisfaction level of Check-in service\n\nInflight service: Satisfaction level of inflight service\n\nCleanliness: Satisfaction level of Cleanliness\n\nDeparture Delay in Minutes: Minutes delayed when departure\n\nArrival Delay in Minutes: Minutes delayed when Arrival\n\nSatisfaction: Airline satisfaction level(Satisfaction, neutral or dissatisfaction)","4b15f25c":"Now, let's visualize our correlation matrix using a heatmap.","488b120b":"This is because the data points for both variables are evaluated on a scale of 0-5; this is why the graph looks odd. Rest assured, though, that it is valid.","160c0419":"*Not going to run Randomized Search CV on final project as it takes up a lot of computational power and therefore takes a very long time to complete. However, rest assured, that after having run it a few times, it does not increase the accuracy of the Random Forest model*","a34bdc68":"**Overall, we see that our best performing model is our XGBoost model.** We'll now evaluate this model using other metrics. ","9e2df73c":"***","22ce2dfb":"# 2. Machine Learning: Prediction\n<a id=\"2.\"><\/a>","71f0f1bf":"We'll now take our best performing model (Random Forest) and tune its hyperparameters.","83ccbd85":"Overall, our XGBoost model performs extremely well across all metrics beyond just the accuracy score. This makes sense because it is commonly known among the DS\/ML community that XGBoost is the best performing model with tabular data.\n\nBeyond our model, however, this data shows that there are many important factors that go into if a passenger is satisified with their experience flying with an airline or not. The most important features of which are: **Class, Online boarding, Type of Travel, Inflight entertainment, Seat comfort, On-board service, Leg room service, Cleanliness, and Flight distance.** We saw that by using these features, along with other flying-experience metrics, we can predict with a very high level of confidence **(+96%)** whether or not a passenger is satisified with their airline flying experience.","0a90c285":"### 2.2. XGBoost\n<a id=\"2.2.\"><\/a>","76a305b0":"#### 1.2.2. Duplicate Values\n<a id=\"1.2.2.\"><\/a>","ec9a08c4":"These visualizations confirm that these two max's are significantly larger than their counterparts. Furthermore, there is another anomaly shown above (around 1300). Overall, these observations seem to be natural, however, they are definetly outliers. Although these observations seem to be natural, our dataset is extremely large, and we do not want them to significantly skew our data (especially when we perform Machine Learning later in this project). Therefore, we will remove these 2 data points.","13fc4fc9":"The max values of Departure Delay in Minutes and Arrival Delay in Minutes look to be extremely large: 1592 and 1584. We'll make a boxplot to visualize this.","49a9283c":"Note that the above variables in the graph above have a ~70% correlation. When we think of correlation this graph is not what we expect. So why does this graph look so weird?","425d1787":"Next, we'll use plotly to see some 3d visualizations on how 3 variables all highly correlated to a passenger's satisfaction affect a passenger's overall airline satisfaction.","46c47b42":"Tuning our Random Forest model's hyperparameters doesn't make much of a difference in the accuracy score of the model.\n\nNow, let's use **XGBoost** to see how well it scores!","04a5da63":"# 1. Data Preprocessing \/ Exploratory Data Analysis\n<a id=\"1.\"><\/a>","992da308":"We can see that the variables most highly correlated to our y-variable are: \n- Class (~50%)\n- Online boarding (~50%)\n- Type of Travel (~44%)\n- Inflight entertainment (~40%)\n- Seat comfort (~35%)\n- On-board service (~32%)\n- Leg room service (~31%)\n- Cleanliness (~31%)\n- Flight distance (~30%)","be86aba9":"### 1.1. Data Familiarization\n<a id=\"1.1.\"><\/a>","f5cda7c9":"#### 2.1.1. Random Forest: Hyperparameter tuning\n<a id=\"2.1.1.\"><\/a>","abc65748":"# Airline Passenger Satisfication Prediction Table of Contents:\n\n## [1. Data Preprocessing \/ Exploratory Data Analysis](#1.)\n\n#### [1.1. Data Familiarization](#1.1.)\n\n#### [1.2. Data Cleaning](#1.2.)\n- [1.2.1. NaN Values](#1.2.1.)\n- [1.2.2. Duplicate Values](#1.2.2.)\n- [1.2.3. Outliers](#1.2.3.)\n\n#### [1.3. Final Data Preparation](#1.3.)\n\n#### [1.4. Data Visualization](#1.4.)\n- [1.4.1. Correlation Matrix](#1.4.1.)\n- [1.4.2. Features Visualization](#1.4.2.)\n\n\n## [2. Machine Learning: Prediction](#2.)\n\n#### [2.1. KNN, Decision Tree, Random Forest](#2.1.)\n\n- [2.1.1. Random Forest: Hyperparameter tuning](#2.1.1.)\n\n#### [2.2. XGBoost](#2.2.)\n- [2.2.1. XGBoost: Hyperparameter tuning](#2.2.1.)\n\n#### [2.3. Confusion Matrix](#2.3.)\n\n#### [2.4. Classification Report](#2.4.)\n\n\n## [3. Conclusion](#3.)","7f6ff936":"### 1.4.2. Features Visualization\n<a id=\"1.4.2.\"><\/a>","6774c9fd":"#### 1.4.1. Correlation Matrix\n<a id=\"1.4.1.\"><\/a>","d4f330c8":"# **Thanks for reading my notebook! My objective for this notebook was to clearly explain my thought process to help guide readers. Feel free to upvote this notebook and leave feedback via the comment section!**","b11cb225":"#### Data Description -> below"}}