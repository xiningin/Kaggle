{"cell_type":{"234eb7eb":"code","1a1cf75f":"code","d567b2b6":"code","ff688702":"code","65fd99b8":"code","02ccd45f":"code","a881d984":"code","1ceef6ec":"code","87666597":"code","4853a59f":"code","b8afc9a6":"code","e09af029":"code","1b63df93":"code","438587e0":"code","cabce310":"code","5a0c9041":"code","c118f40b":"code","5e2dee9c":"code","0d4318cb":"code","3dc3751e":"code","b40beb47":"code","654164ed":"code","85c86b4d":"code","f10b810f":"markdown","a3741f43":"markdown","2a7d1351":"markdown","4ccf9359":"markdown","faf8948c":"markdown","9267c2ce":"markdown","d89b7166":"markdown","27e6ed3a":"markdown","46d280ee":"markdown","508f3104":"markdown","64ba104a":"markdown","685dfdbe":"markdown","046a9da6":"markdown","4ba2fd27":"markdown","a8f584e2":"markdown","3a00c612":"markdown"},"source":{"234eb7eb":"import pandas as pd\nimport os\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.nn as nn\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nimport transformers\nfrom transformers import AdamW","1a1cf75f":"# gpu\/cpu\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","d567b2b6":"# model loading path\nMODEL_PATH = '..\/input\/huggingface-bert\/bert-base-uncased'","ff688702":"train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain","65fd99b8":"train_data = train.iloc[:,3]\ntrain_target = train.iloc[:,4]\n# for visualization\ntrain_set = pd.concat([train_data,train_target], axis=1)\ntrain_set","02ccd45f":"num_data = len(train_set)\nmsk = np.random.rand(num_data)<1 # we can change it to 1 to make all train data as the train set\ntraining = train_data[msk]\ntesting = train_data[~msk]\ntraining_target = train_target[msk]\ntesting_target = train_target[~msk]\ntrain_sample = pd.concat([training, training_target], axis=1)\ntest_sample = pd.concat([testing, testing_target], axis=1)\ntrain_sample, test_sample","a881d984":"train_sample = train_sample.values","1ceef6ec":"# partition data into 5 folds\nkf = KFold(shuffle=True)\n\n# check the data partition\nfor train_index, valid_index in kf.split(train_sample):\n    print(len(train_sample[train_index]))\n    print(len(train_sample[valid_index]))","87666597":"# tokenizer from BERT\ntokenizer = transformers.BertTokenizer.from_pretrained(MODEL_PATH)","4853a59f":"class BERTDataSet(Dataset):\n    \n    def __init__(self,excerpts,targets):\n        \n        self.excerpts = excerpts\n        self.targets = targets\n        \n    def __len__(self):\n        \n        return len(self.excerpts)\n    \n    def __getitem__(self,idx):\n        \n        excerpt = self.excerpts[idx]\n        \n        bert_excerpts = tokenizer.encode_plus(\n                                excerpt,\n                                add_special_tokens = True, \n                                max_length = 314,\n                                # pad_to_max_length = True, \n                                padding='max_length',\n                                return_attention_mask = True,\n                                truncation=True)\n\n        ids = torch.tensor(bert_excerpts['input_ids'], dtype=torch.long)\n        mask = torch.tensor(bert_excerpts['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(bert_excerpts['token_type_ids'], dtype=torch.long)\n     \n            \n        target = torch.tensor(self.targets[idx],dtype=torch.float)\n        \n        return {\n                'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n                'targets': target\n            }","b8afc9a6":"LR=2e-5\n# model = transformers.BertForSequenceClassification.from_pretrained(MODEL_PATH,num_labels=1)\n# model.to(device)\n# optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2)\n# model_original_stat_dict = model.state_dict() # store the original weight matrix","e09af029":"# minibatch for dataloader and training epochs\nbatchSize = 16\nepochs = 20","1b63df93":"All_train_losses = []\nvalidate_losses = []\n# model_matrix = []\nscaler = torch.cuda.amp.GradScaler()\nfold = 0\n# model_original_stat_dict = model.state_dict()\nfor train_index, valid_index in kf.split(train_sample):\n    model = transformers.BertForSequenceClassification.from_pretrained(MODEL_PATH,num_labels=1)\n    optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2)\n    model.to(device)\n    train_input = train_sample[train_index]\n    valid_input = train_sample[valid_index]\n    # print(valid_input.shape)\n    train_input = BERTDataSet(train_input[:,0],train_input[:,1])\n    valid_input = BERTDataSet(valid_input[:,0],valid_input[:,1])\n    # print(train_input)\n    train_dataloader = DataLoader(train_input, batch_size = batchSize,shuffle = True,num_workers=4,pin_memory=True)\n    valid_dataloader = DataLoader(valid_input, batch_size = batchSize,shuffle = True,num_workers=4,pin_memory=True)\n    train_losses = []\n    bestScore = None\n    for epoch in tqdm(range(epochs)):\n        # train phase\n        model.train()\n        batch_pred = []\n        batch_target = []\n        for step, batch in enumerate(train_dataloader):\n            optimizer.zero_grad()\n            ids = batch[\"ids\"].to(device,non_blocking=True)\n            mask = batch[\"mask\"].to(device,non_blocking=True)\n            tokentype = batch[\"token_type_ids\"].to(device,non_blocking=True)\n\n            # print(step)\n            output = model(ids,mask)\n            output = output[\"logits\"].squeeze(-1)\n\n            target = batch[\"targets\"].to(device,non_blocking=True)\n\n            loss = nn.MSELoss()(output,target)\n            batch_pred += list(output.detach().cpu().numpy())\n            batch_target += list(target.detach().cpu().numpy())\n\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        train_losses.append(np.sqrt(mean_squared_error(batch_pred, batch_target)))\n\n        # print(\"train_end\")\n    \n        # validate phase\n#         if epoch+1==epochs:\n        with torch.no_grad():\n            valid_pred = []\n            valid_targets = []\n            model.eval()\n            for valid_step, valid_batch in enumerate(valid_dataloader):\n                valid_ids = valid_batch[\"ids\"].to(device,non_blocking=True)\n                valid_mask = valid_batch[\"mask\"].to(device,non_blocking=True)\n                valid_tokentype = valid_batch[\"token_type_ids\"].to(device,non_blocking=True)\n\n                valid_output = model(valid_ids,valid_mask)\n                valid_output = valid_output[\"logits\"].squeeze(-1)\n\n                valid_target = valid_batch[\"targets\"].to(device,non_blocking=True)\n\n                # v_loss = nn.MSELoss()(output,target)\n                # valid_loss.append(v_loss.item())\n                valid_pred += list(valid_output.detach().cpu().numpy())\n                valid_targets += list(valid_target.detach().cpu().numpy())\n\n            if bestScore is None:\n                bestScore = np.sqrt(mean_squared_error(valid_pred,valid_targets))\n                state = {\n                            'state_dict': model.state_dict(),\n                            'optimizer_dict': optimizer.state_dict(),\n                            \"bestscore\":bestScore\n                        }\n                torch.save(state, \"model\" + str(fold) + \".pth\")\n            elif bestScore > np.sqrt(mean_squared_error(valid_pred,valid_targets)):\n                bestsSore = np.sqrt(mean_squared_error(valid_pred,valid_targets))\n                state = {\n                            'state_dict': model.state_dict(),\n                            'optimizer_dict': optimizer.state_dict(),\n                            \"bestscore\":bestScore\n                        }\n                torch.save(state, \"model\"+ str(fold) + \".pth\")\n            else:\n                pass\n    validate_losses.append(bestScore)\n            \n    All_train_losses.append(train_losses)\n\n    print('Fold [%d\/%d] Train Loss: %.4f  Validate Loss: %.4f'\n                  % (fold, 5, All_train_losses[fold-1][-1], validate_losses[fold-1]))\n  #     if not os.path.exists(os.path.join('BERT_pretrained',str(fold))):\n#         os.makedirs(os.path.join('BERT_pretrained',str(fold)))\n#     torch.save(model.state_dict(), os.path.join('BERT_pretrained',str(fold),'baseline3.pth'))\n#     model_matrix.append(model.state_dict())\n    fold += 1\n","438587e0":"print(np.mean(validate_losses))","cabce310":"model = transformers.BertForSequenceClassification.from_pretrained(MODEL_PATH,num_labels=1)","5a0c9041":"# test_sample = test_sample.values\n# test_input = BERTDataSet(test_sample[:,0], test_sample[:,1])\n# test_dataloader = DataLoader(test_input,batch_size=int(batchSize),shuffle=True,num_workers=4,pin_memory=True)\n\n# test_scores = []\n\n# for i in range(5):\n#     model_weight = model_matrix[i]\n#     fold = i + 1\n#     model.load_state_dict(model_weight)\n#     model.to(device)\n#     model.eval()\n#     test_pred = []\n#     test_targets = []\n#     with torch.no_grad():\n#         for test_step, test in enumerate(test_dataloader):\n#             test_ids = test[\"ids\"].to(device,non_blocking=True)\n#             test_mask = test[\"mask\"].to(device,non_blocking=True)\n#             test_output = model(test_ids, test_mask)\n#             test_output = test_output[\"logits\"].squeeze(-1)\n#             test_target = test[\"targets\"].to(device,non_blocking=True)\n#             test_pred += list(test_output.detach().cpu().numpy())\n#             test_targets += list(test_target.detach().cpu().numpy())\n\n#     test_scores.append(np.sqrt(mean_squared_error(test_pred,test_targets)))\n#     print(np.sqrt(mean_squared_error(test_pred,test_targets)))\n# print(np.mean(test_scores))","c118f40b":"All_test_preds = []\npathes = [os.path.join(\".\/\",s) for s in os.listdir(\".\/\") if \".pth\" in s]\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest_input = test_df[\"excerpt\"]\ntest_input = test_input.values\ntest_target = np.zeros(len(test_input))\n# test_id = test_df[\"id\"]\ntest_input = BERTDataSet(test_input,test_target)\nfor path in pathes:\n    state = torch.load(path)\n    model.load_state_dict(state[\"state_dict\"])    \n\n    model.to(device)\n    model.eval()\n  \n\n    testloader = DataLoader(test_input,batch_size=32,shuffle=False,num_workers=4,pin_memory=True)\n  \n    test_preds = []\n    with torch.no_grad():\n        for test_step, test_batch in enumerate(testloader):\n            test_ids = test_batch[\"ids\"].to(device)\n            test_mask = test_batch[\"mask\"].to(device)\n            test_output = model(test_ids, test_mask)\n            test_output = test_output[\"logits\"].squeeze(-1)\n            test_pred = test_output.detach().cpu().numpy()\n            test_preds.append(test_pred)\n        test_preds = np.concatenate(test_preds)\n        All_test_preds.append(test_preds)","5e2dee9c":"All_test_preds","0d4318cb":"scoring = pd.DataFrame(All_test_preds)\nscoring = scoring.T","3dc3751e":"aver_score = scoring.mean(axis=1)\naver_score","b40beb47":"sample = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsample","654164ed":"sample[\"target\"] = aver_score\nsample","85c86b4d":"sample.to_csv(\"submission.csv\",index = False)","f10b810f":"We implement the k-fold with 5 folds. Each iteration, we use only one fold as the validation data, and the rest as the training data.","a3741f43":"This is a baseline model based on BERT for the CommonLitReadabilityPrize competition. The implementation refers to [BERT beginner](https:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room\/notebook).","2a7d1351":"# Table of Contents\n1. [Preparation](#preparation)\n2. [K-fold](#k-fold)\n3. [Model and Training](#model-and-training)\n4. [Testing](#testing)\n5. [Scoring](#scoring)","4ccf9359":"**Loading training data**","faf8948c":"# 1. Preparation\n**Import all dependencies**","9267c2ce":"# 4. Testing\nIf we partition train data into train set and test set, we can use the test set to check the performance.","d89b7166":"# 5. Scoring\nUse the test data in \"test.csv\" to get the submission file.","27e6ed3a":"We import all dependencies here and define the model path","46d280ee":"Then we will tokenize the data and make a dataloader for mini-batch training.","508f3104":"Now, we can implement the training phase.","64ba104a":"We will only use the \"excerpt\" and \"target\" to train the model. So we merely extract the column \"excerpt\" and \"target\" from raw data.","685dfdbe":"We can show the average validation score","046a9da6":"# 1. K-fold","4ba2fd27":"Define a dataset","a8f584e2":"# 3. Model and Training\n\nDefine the optimizer and the model","3a00c612":"We can partition the train data into train set and test set. So we can use the test set to check the performance during implementing the model. The test set is not the data in \"test.csv\"."}}