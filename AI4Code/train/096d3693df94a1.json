{"cell_type":{"578f0dae":"code","7e68c55c":"code","e8cd5ab1":"code","6231b0aa":"code","92907388":"code","c132f090":"code","5fdf814c":"code","7265060a":"code","e29bb574":"code","c000872c":"code","88c9fb69":"code","e4377983":"code","a4766ab7":"code","90003042":"code","ebec3aa1":"markdown","580d8c55":"markdown","ab26f1fe":"markdown","f40272dd":"markdown","284a7420":"markdown","830b4c4c":"markdown","adc309aa":"markdown","9dbe00aa":"markdown","c98a1dc5":"markdown","cbc282df":"markdown","f4e28d96":"markdown","a4604001":"markdown","cc56ed29":"markdown","aaac6741":"markdown","dd077de5":"markdown","c74fcf87":"markdown","c9a97f5d":"markdown","a061cea1":"markdown"},"source":{"578f0dae":"%%capture\n!pip install wandb\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n# Weights and Biases\nimport wandb\nfrom wandb.keras import WandbCallback\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"WANDB_API_KEY\")\nwandb.login(key=api_key);\nwandb.init(project='TPS May 2021', entity='sauravmaheshkar')","7e68c55c":"# Basic Paths\ntrain = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\ntrain.head()","e8cd5ab1":"for i in range(50):\n    mean, std = train[f'feature_{i}'].mean(), train[f'feature_{i}'].std()\n    train[f'feature_{i}'] = train[f'feature_{i}'].apply(lambda x : (x-mean)\/std)","6231b0aa":"# transform target column into 0,1,2,3 values\nlabel_dict = {val:idx for idx, val in enumerate(sorted(train['target'].unique()))}\ntrain['target'] = train['target'].map(label_dict)\n\ntrain['target'] = tf.keras.utils.to_categorical(train['target'])","92907388":"train, test = train_test_split(train, test_size=0.2)\ntrain, val = train_test_split(train, test_size=0.4)\n\nprint(len(train), 'train examples')\nprint(len(val), 'validation examples')\nprint(len(test), 'test examples')","c132f090":"# A utility method to create a tf.data dataset from a Pandas Dataframe\ndef df_to_dataset(dataframe, shuffle=True, batch_size=32):\n    dataframe = dataframe.copy()\n    labels = dataframe.pop('target')\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    if shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(batch_size)\n    return ds","5fdf814c":"from tensorflow import feature_column\n\nfeature_columns = []\n\nfor i in range(50):\n    feature_columns.append(feature_column.numeric_column(f'feature_{i}'))","7265060a":"feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n\nbatch_size = 64\ntrain_ds = df_to_dataset(train, batch_size=batch_size)\nval_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\ntest_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)","e29bb574":"model = tf.keras.models.Sequential([\n        feature_layer,\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(4, activation='softmax')])\n\nmodel.compile(optimizer='sgd',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])","c000872c":"model.fit(train_ds,epochs = 10, verbose = 2,\n          validation_data=val_ds,\n          validation_steps = 100,\n          callbacks = [WandbCallback()])","88c9fb69":"loss, accuracy = model.evaluate(test_ds)\nprint(\"Accuracy\", accuracy)\nprint(\"Loss\", loss)","e4377983":"train_df = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\ntrain_df = train_df.drop('id', axis = 1)\ntest_df = test_df.drop('id', axis=1)\n\nfor i in range(50):\n    mean, std = train_df[f'feature_{i}'].mean(), train_df[f'feature_{i}'].std()\n    test_df[f'feature_{i}'] = test_df[f'feature_{i}'].apply(lambda x : (x-mean)\/std)","a4766ab7":"def df_to_dataset_test(dataframe, batch_size=32):\n    dataframe = dataframe.copy()\n    ds = tf.data.Dataset.from_tensor_slices(dict(dataframe))\n    ds = ds.batch(batch_size)\n    ds = ds.prefetch(batch_size)\n    return ds\n\ntest_df = df_to_dataset_test(test_df, batch_size=batch_size)","90003042":"sample_submission[['Class_1','Class_2', 'Class_3', 'Class_4']] = model.predict(test_df)\nsample_submission.to_csv('densefeatures_submission.csv',index = False)\nsample_submission.head()","ebec3aa1":"## Disclaimer\n\nThis kernel builds on top of [@subinium](https:\/\/www.kaggle.com\/subinium)'s kernel [TPS-May:Deeplearning Pipeline for Beginner](https:\/\/www.kaggle.com\/subinium\/tps-may-deeplearning-pipeline-for-beginner)","580d8c55":"# Submission","ab26f1fe":"We can evaluate on our test set and see the loss and accuracy. As, we can see the model overfits to a large extent.","f40272dd":"Rather than using dataframes we'll create a `tf.data.Dataset` from our original dataframe. This also allows us to efficiently shuffle, prefetch and create batches.","284a7420":"# Table of Content\n\n1. [Packages \ud83d\udce6 and Basic Setup](#basic)\n2. [Pre-Processing \ud83d\udc4e\ud83c\udffb -> \ud83d\udc4d](#preprocess)\n3. [The Model \ud83d\udc77\u200d\u2640\ufe0f](#model)\n4. [Training \ud83d\udcaa\ud83c\udffb](#train)","830b4c4c":"<a id='model'><\/a>\n# The Model \ud83d\udc77\u200d\u2640\ufe0f","adc309aa":"![](https:\/\/github.com\/SauravMaheshkar\/Tabular-Playground-Series-May-2021\/blob\/main\/assets\/Banner.png?raw=true)","9dbe00aa":"<a id='train'><\/a>\n# Training \ud83d\udcaa\ud83c\udffb","c98a1dc5":"<a id = \"basic\"> <\/a>\n\n# Packages \ud83d\udce6 and Basic Setup","cbc282df":"We also convert the `target` columns into binary class matrices using the `tf.keras.utils.to_categorical()` function.","f4e28d96":"<a id = 'preprocess'> <\/a>\n# Pre-Processing \ud83d\udc4e\ud83c\udffb -> \ud83d\udc4d\n\nIn this kernel I'll highlight the relatively new `tensorflow.feature_column` submodule which contains a ton of useful methods to deal with structured data. For more details kindly visit the [documentation](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/feature_column). Tensorflow offers a ton of feature columns for us to experiment with, viz :\n\n* Numeric columns\n* Bucketized columns\n* Categorical columns\n* Embedding columns\n* Hashed feature columns\n* Crossed feature columns\n\nHave a look at [this tutorial](https:\/\/www.tensorflow.org\/tutorials\/structured_data\/feature_columns) for more details.","a4604001":"As we can see, the dataset consists of 50 feature columnns with 4 classes. ","cc56ed29":"Lastly, we split out dataset into train, validation and test splits. Most deep learning models will overfit, so having a nice split ratio is key \ud83d\udd11","aaac6741":"We create a `feature_layer`, which will act as the first layer in our model. A batch size of 64 was arbitrarily chosen.","dd077de5":"A numeric column is the simplest type of feature column. It is used to represent real valued features. When using this column, our model will receive the column value from the dataframe **unchanged**. The output of a feature column will become the input to the model.","c74fcf87":"We train for a mere 10 epochs and the overfitting is quite obvious here. Better Regularization strategies + better feature selection can be extremely helpful moving forward. We'll also log our metrics to [Weights and Biases](https:\/\/wandb.ai\/site) for efficient experiment tracking.","c9a97f5d":"We just add a Dense layer in the end to act as a classification head for the model and compile using the `sgd` optimizer and the `sparse_categorical_crossentropy` loss function.","a061cea1":"Upon a closer look, we realize that most features are left skewed in this dataset. Thus, Normalization seems ideal."}}