{"cell_type":{"ea9f5c52":"code","db202625":"code","f447937a":"code","9a921345":"code","2d5f1d4c":"code","9295f840":"code","dc67d5d9":"code","b8aff35b":"code","874ffc6e":"code","6639b018":"code","c6287c32":"code","e71cfccb":"code","6883fa90":"code","bf9c62e0":"code","e29055fd":"code","2e1400a5":"code","fb3f0c53":"code","79b30bf7":"code","bac00b7e":"code","d422475c":"code","b816d2a3":"code","1fbee514":"code","31c039b7":"code","c9282049":"code","ee7c1178":"code","8f6dfc1f":"code","ccbf675f":"code","f958a563":"code","a035e964":"code","2eb5aac1":"code","589f1970":"code","f9056710":"code","b6ef498a":"code","f20dcd49":"code","38378fc1":"code","7557e1eb":"code","46dc5175":"markdown","5f007476":"markdown","332a40f3":"markdown","1ebea2a1":"markdown","7d426cb8":"markdown","403b658f":"markdown","b3c7d165":"markdown","d68eb04b":"markdown","702f079c":"markdown","94840860":"markdown","8b888b9e":"markdown","06c57a5f":"markdown","6308f5b6":"markdown","4187aa8f":"markdown"},"source":{"ea9f5c52":"from scipy import misc\nimport matplotlib.pyplot as plt\n\n\nimg = misc.face()  #  1024 x 768, color image of a raccoon face\nplt.imshow(img);","db202625":"import cv2\nimg = cv2.resize(img, (224, 224))\nplt.imshow(img);","f447937a":"p = 16\nh, w = img.shape[:2]\n\nfig, ax = plt.subplots(h\/\/p, w\/\/p, figsize=(6,6))\nfor i in range(h\/\/p):\n    for j in range(w\/\/p):\n        patch = img[i*p:(i+1)*p, j*p:(j+1)*p]\n        ax[i, j].imshow(patch)\n        ax[i, j].axis('off')\nplt.show()","9a921345":"patch.shape","2d5f1d4c":"patch.flatten().shape","9295f840":"import sys\n\npackage_path = '..\/input\/vision-transformer-pytorch\/VisionTransformer-Pytorch'\npackage_path2 = '..\/input\/t2tvit'\npackage_path3 = '..\/input\/timm-pytorch-image-models\/pytorch-image-models-master'\npackage_path4 = '..\/input\/vision-transformer-pytorch\/vision_transformer_pytorch'\nsys.path.append(package_path)\nsys.path.append(package_path2)\nsys.path.append(package_path3)\nsys.path.append(package_path4)","dc67d5d9":"import os\nimport pandas as pd\n\nimport time\nimport datetime\nimport copy\nimport matplotlib.pyplot as plt\nimport json\nimport seaborn as sns\nimport cv2\nimport albumentations as albu\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, train_test_split\n\n\n# ALBUMENTATIONS\nimport albumentations as albu\n\nfrom albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\n    \nfrom albumentations.pytorch import ToTensorV2\n\nWIDTH, HEIGHT = (384, 384)","b8aff35b":"# !pip install -q timm\n# !pip install -q vision_transformer_pytorch\n# !pip3 install -q adamp\n\nos.chdir('\/kaggle\/input\/adamp-timm-vision-transformer-pytorch')\n!pip install -q timm-0.4.5-py3-none-any.whl\n!pip install -q vision_transformer_pytorch-1.0.3-py2.py3-none-any.whl\n# !easy_install -q adamp-0.3.0-py3.8.egg\nos.chdir('\/kaggle\/working\/')","874ffc6e":"# BASE_DIR=\"..\/input\/plant-pathology-2021-fgvc8\/\"\nBASE_DIR = '..\/input\/plant-pathology-2021-224x224\/'\nTRAIN_IMAGES_DIR = os.path.join(BASE_DIR, 'train_imgs')","6639b018":"train_df = pd.read_csv('..\/input\/plant-pathology-2021-fgvc8\/train.csv')\ntrain_df.head()","c6287c32":"print(\"Count of training images {0}\".format(len(os.listdir(TRAIN_IMAGES_DIR))))","e71cfccb":"class_name = train_df['labels'].value_counts().index\nclass_count = train_df['labels'].value_counts().values","6883fa90":"display(train_df['labels'].nunique())\ntrain_df.labels.value_counts()","bf9c62e0":"plt.figure(figsize=(8,5))\nsns.countplot(data=train_df, y='labels');","e29055fd":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ntrain_df['labels'] = le.fit_transform(train_df.labels)","2e1400a5":"def visualize_images(image_ids, labels):\n    plt.figure(figsize=(16, 12))\n    for idx, (image_id, label) in enumerate(zip(image_ids, labels)):\n        plt.subplot(3, 3, idx+1)\n        image = cv2.imread(os.path.join(TRAIN_IMAGES_DIR, image_id))[:, ::-1]  # rgb img\n        plt.imshow(image)\n        plt.title(f\"Class: {label}\", fontsize=12)\n        plt.axis(\"off\")\n    plt.show()\n    \n\ndef plot_augmentation(image_id, transform):\n    plt.figure(figsize=(16, 4))\n    plt.axis('off')\n    img = cv2.imread(os.path.join(TRAIN_IAMGES_DIR, image_id))[:, ::-1] \n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n#     plt.axis('off')\n    \n    plt.subplot(1, 3, 2)\n    x = transform(image=img)['image']\n    plt.imshow(x)\n#     plt.axis('off')\n    \n    plt.subplot(1, 3, 3)\n    x = transform(image=img)['image']\n    plt.imshow(x)\n    \ndef visualize(images, transform):\n    '''\n    Plot images and their transformations\n    '''\n    fig = plt.figure(figsize=(32, 16))\n    \n    for i, im in enumerate(images):\n        ax = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n        plt.imshow(im)\n        \n    for i, im in enumerate(images):\n        ax = fig.add_subplot(2, 5, i+6, xticks=[], yticks=[])\n        plt.imshow(transform(image=im)['image'])","fb3f0c53":"# CUSTOM DATASET CLASS\nclass PlantDataset(Dataset):\n    def __init__(\n        self, df:pd.DataFrame, imfolder:str, train:bool=True, transforms=None\n    ):\n        self.df = df\n        self.imfolder = imfolder\n        self.train = train\n        self.transforms = transforms\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        im_path = os.path.join(self.imfolder, self.df.iloc[index]['image'])\n        im = cv2.imread(im_path, cv2.IMREAD_COLOR)\n        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n        \n        if (self.transforms):\n            im = self.transforms(image=im)['image']\n            \n        if (self.train):\n            label = self.df.iloc[index]['labels']\n            return im, label\n        else:\n            return im","79b30bf7":"# AUGMENTATIONS\n\n\ntrain_augs = albu.Compose([\n    albu.RandomResizedCrop(height=HEIGHT, width=WIDTH, p=1.0),\n    albu.HorizontalFlip(p=0.5),\n    albu.VerticalFlip(p=0.5),\n    albu.RandomBrightnessContrast(p=0.6),\n    albu.ShiftScaleRotate(p=0.6),\n    albu.Normalize(    \n        mean=[0.431, 0.498,  0.313],\n        std=[0.237, 0.239, 0.227],), # [0.431, 0.498,  0.313], [0.237, 0.239, 0.227]\n    CoarseDropout(p=0.5),\n    Cutout(p=0.5),\n    ToTensorV2(),\n])\n\n\nvalid_augs = albu.Compose([\n    albu.Resize(height=HEIGHT, width=WIDTH, p=1.0),\n    albu.Normalize(\n        mean=[0.431, 0.498,  0.313],\n        std=[0.237, 0.239, 0.227],),\n    ToTensorV2(),\n])\n","bac00b7e":"# DATA SPLIT\ntrain, valid = train_test_split(\n    train_df,\n    test_size=0.2,\n    random_state=162,\n    stratify=train_df.labels.values\n)\n\n# reset index on both dataframes\ntrain = train.reset_index(drop=True)\nvalid = valid.reset_index(drop=True)\n\n# targets in train,valid datasets\ntrain_targets = train.labels.values\nvalid_targets = valid.labels.values","d422475c":"# DEFINE PYTORCH CUSTOM DATASET\ntrain_dataset = PlantDataset(\n    df=train,\n    imfolder=TRAIN_IMAGES_DIR,\n    train=True,\n    transforms=train_augs\n)\n\nvalid_dataset = PlantDataset(\n    df=valid,\n    imfolder=TRAIN_IMAGES_DIR,\n    train=True,\n    transforms=valid_augs\n)","b816d2a3":"def plot_image_from_dataset(img_list):\n    image_tensor = img_list[0]\n    target = img_list[1]\n    print(target)\n    image = image_tensor.permute(1, 2, 0)\n    plt.imshow(image)","1fbee514":"plot_image_from_dataset(train_dataset[7])","31c039b7":"# MAKE PYTORCH DATALOADER\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    num_workers=4,\n    shuffle = True\n)\n\nvalid_loader = DataLoader(\n    valid_dataset,\n    batch_size=16,\n    num_workers=4,\n    shuffle = False\n)","c9282049":"# TRAIN\ndef train_model(datasets, dataloaders, model, criterion, optimizer, scheduler, num_epochs, device):\n    since = time.time()\n    \n    best_model_wts = copy.deepcopy(model.state_dict()) # store best model weights\n    best_acc = 0.0\n    \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch+1}\/{num_epochs}')\n        print('=' * 10)\n        \n        for phase in ['train', 'valid']:\n            if phase == 'train':\n                model.train()\n            else:\n                model.eval()\n                \n            running_loss = 0.0\n            running_corrects = 0.0\n            running_total = 0.0\n            \n            for step, (inputs, labels) in enumerate(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                model = model.to(device)\n\n                # Zero out the grads\n                optimizer.zero_grad()\n                \n                \n                with torch.set_grad_enabled(phase == 'train'):\n                    # Forward\n                    outputs = model(inputs)\n                    probabilities, preds = torch.max(outputs, 1) \n                    loss = criterion(outputs, labels)\n                    \n                    # Backward\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                # Statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n                running_total += len(labels.data)\n                \n                if (step + 1) % 100 == 0:\n                    print(f'[{step + 1}\/{len(dataloaders[phase])}].')\n                    print(f'Loss {running_loss \/ (step + 1)}. Accuracy {running_corrects \/ running_total}')\n            \n            if phase == 'train':\n                scheduler.step(running_loss \/ len(datasets[phase]))\n                \n            epoch_loss = running_loss \/ len(datasets[phase])\n            epoch_acc = running_corrects.double() \/ len(datasets[phase])\n            \n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n            \n            if phase == 'valid' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n                torch.save(model, f'best_model_ViT-B_16_{epoch}ep_{epoch_acc:.2f}acc.pth')\n        print()\n    \n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed \/\/ 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best val Acc: {best_acc:.4f}')\n    \n    model.load_state_dict(best_model_wts)\n    \n    return model","ee7c1178":"def load_state_dict(checkpoint_path, use_ema=False, num_classes=1000):\n    if checkpoint_path and os.path.isfile(checkpoint_path):\n        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n        state_dict_key = 'state_dict'\n        if isinstance(checkpoint, dict):\n            if use_ema and 'state_dict_ema' in checkpoint:\n                state_dict_key = 'state_dict_ema'\n        if state_dict_key and state_dict_key in checkpoint:\n            new_state_dict = OrderedDict()\n            for k, v in checkpoint[state_dict_key].items():\n                # strip `module.` prefix\n                name = k[7:] if k.startswith('module') else k\n                new_state_dict[name] = v\n            state_dict = new_state_dict\n        else:\n            state_dict = checkpoint\n#         _logger.info(\"Loaded {} from checkpoint '{}'\".format(state_dict_key, checkpoint_path))\n#         if num_classes != 1000:\n#             # completely discard fully connected for all other differences between pretrained and created model\n#             del state_dict['head' + '.weight']\n#             del state_dict['head' + '.bias']\n\n        return state_dict\n    else:\n#         _logger.error(\"No checkpoint found at '{}'\".format(checkpoint_path))\n        raise FileNotFoundError()\n\n\ndef load_for_transfer_learning(model, checkpoint_path, use_ema=False, strict=False, num_classes=1000):\n    state_dict = load_state_dict(checkpoint_path, use_ema, num_classes)\n    model.load_state_dict(state_dict, strict=strict)\n","8f6dfc1f":"stat = torch.load('..\/input\/t2tvit\/80.7_T2T_ViT_t_14.pth.tar')\n\ndel stat['state_dict_ema']['head.weight']\ndel stat['state_dict_ema']['head.bias']\n\ntorch.save(stat['state_dict_ema'],'80.7_T2T_ViT_t_14.pth.tar')","ccbf675f":"### from models.t2t_vit import *\n# create model\n### model = T2t_vit_14(img_size=224, num_classes=12)\n# load_for_transfer_learning(model, '80.7_T2T_ViT_t_14.pth.tar', num_classes=12)","f958a563":"from vision_transformer_pytorch import VisionTransformer\n# from adamp import AdamP\nimport copy\n\n\n# from vision_transformer_pytorch import VisionTransformer\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ndatasets = {'train': train_dataset,\n            'valid': valid_dataset}\n\ndataloaders = {'train': train_loader,\n               'valid': valid_loader}\n\n# To continue training process - load wheights from previous training\nPRETRAINED_MODEL_PATH = '..\/input\/best-model-vitb-16-8ep-083accpth\/best_model_ViT-B_16_8ep_0.83acc.pth'\nmodel = torch.load(PRETRAINED_MODEL_PATH)\n\n# LOAD PRETRAINED ViT MODEL\n# model = VisionTransformer.from_pretrained('ViT-B_16', num_classes=12) \n\n# OPTIMIZER\n# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.001)\n# optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=0.001)\n# optimizer = AdamP(model.parameters(), lr=1e-4, weight_decay=0.001)\n\n# LEARNING RATE SCHEDULER\n# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n#                                                        mode='min', \n#                                                        factor=0.1, \n#                                                        patience=3, \n#                                                        threshold=0.001, \n#                                                        threshold_mode='rel', \n#                                                        cooldown=0, \n#                                                        min_lr=0, \n#                                                        eps=1e-08, \n#                                                        verbose=True)\n\n\ncriterion = nn.CrossEntropyLoss()\nnum_epochs = 5","a035e964":"# torch.save(model, 'full_plant_pathology_ViT-B_16.pt')","2eb5aac1":"len(dataloaders['train'])","589f1970":"# MODEL TRAIN\n# trained_model = train_model(datasets, dataloaders,\n#                             model, criterion,\n#                             optimizer, scheduler,\n#                             num_epochs, device)","f9056710":"# Save the mode after training\n# model.save(trained_model, f't2t-vit-14_{num_epochs}_epoch.pth')","b6ef498a":"# os.chdir(r'.\/')\n\n# from IPython.display import FileLink\n# FileLink(r'80.7_T2T_ViT_t_14.pth.tar')","f20dcd49":"# LOAD PRETRAINED ViT MODEL\nmodel_path = '..\/input\/best-model-vitb-16-8ep-083accpth\/best_model_ViT-B_16_8ep_0.83acc.pth'\nmodel = torch.load(model_path)\n# model.load_state_dict(torch.load(model_path))\nmodel.eval()","38378fc1":"from pathlib import Path\n\ndf_preds = pd.DataFrame()\nfor path in Path('..\/input\/plant-pathology-2021-fgvc8\/test_images').iterdir():\n    img = cv2.imread(str(path))[:, ::-1]\n    img = valid_augs(image=img)['image'].cuda()\n    pred = model(img[None])\n    \n    df_preds = df_preds.append(\n        {'image': path.parts[-1], 'labels': le.inverse_transform(torch.argmax(pred.cpu(), dim=1))[0]},\n        ignore_index=True)\n    \ndf_preds","7557e1eb":"df_preds.to_csv('submission.csv', index=False)","46dc5175":"### Training","5f007476":"\n3. \u041f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u0439 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433 \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u043a \u043f\u0430\u0442\u0447\u0443, \u0447\u0442\u043e\u0431\u044b \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c \u043f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u043d\u0443\u044e \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e\n\n\u042d\u0442\u043e \u043e\u0431\u0443\u0447\u0430\u0435\u043c\u0430\u044f \u0442\u0430\u0431\u043b\u0438\u0446\u0430 \u0441 \u043f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u043c\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u0430\u043c\u0438.\n\n\n\u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u044b \u043d\u0435 \u0437\u0430\u0432\u0438\u0441\u044f\u0442 \u043e\u0442 \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u044b \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0434\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u043e\u0431\u0443\u0447\u0430\u0435\u043c\u044b\u0445 \u043f\u043e\u0437\u0438\u0446\u0438\u043e\u043d\u043d\u044b\u0445 \u044d\u043c\u0431\u044d\u0434\u0434\u0438\u043d\u0433\u043e\u0432 \u0432 \u043a\u0430\u0436\u0434\u044b\u0439 \u043f\u0430\u0442\u0447 \u043f\u043e\u0437\u0432\u043e\u043b\u0438\u0442 \u043c\u043e\u0434\u0435\u043b\u0438 \u0443\u0437\u043d\u0430\u0442\u044c \u043e \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f.\n\n\n\n\u042d\u0442\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u043f\u0430\u0442\u0447\u0435\u0439 \u0431\u0443\u0434\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0432 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0432\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0434\u043b\u044f \u044d\u043d\u043a\u043e\u0434\u0435\u0440\u0430 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u0430.","332a40f3":"\n2. \u0412\u044b\u0442\u044f\u043d\u0443\u0442\u044c 2D \u043f\u0430\u0442\u0447\u0438 \u0432 1D \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435\n\n\u041a\u0430\u0436\u0434\u044b\u0439 \u043f\u0430\u0442\u0447 \u0432\u044b\u0442\u044f\u0433\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0432 1D \u043f\u0430\u0442\u0447 \u043f\u0443\u0442\u0435\u043c \u043a\u043e\u043d\u043a\u0430\u0442\u0435\u043d\u0430\u0446\u0438\u0438 \u0432\u0441\u0435\u0445 \u043f\u0438\u043a\u0441\u0435\u043b\u0435\u0439 \u0438 \u0437\u0430\u0442\u0435\u043c \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u0430\u044f \u043f\u0440\u043e\u0435\u043a\u0446\u0438\u044f \u0434\u043e \u0436\u0435\u043b\u0430\u0435\u043c\u043e\u0439 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 - \u044d\u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u043f\u0430\u0447\u0442 \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430. (\u042d\u0442\u0430 \u043c\u0430\u0442\u0440\u0438\u0446\u0430 \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0430 \u0434\u043b\u044f \u0432\u0441\u0435\u0445 \u043f\u0430\u0442\u0447\u0435\u0439 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u0430\u044f)","1ebea2a1":"1. \u0418\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u043d\u0443\u0436\u043d\u043e \u043f\u043e\u0434\u0435\u043b\u0438\u0442\u044c \u043d\u0430 \u043f\u0430\u0447\u0442\u0438 \u0444\u0438\u043a\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u043e\u0433\u043e \u0440\u0430\u0437\u043c\u0435\u0440\u0430\n\n\u0415\u0441\u0442\u044c 2D \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u0430  H * W, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u043c\u043e\u0436\u0435\u0442 \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c\u0441\u044f \u043d\u0430 N \u043f\u0430\u0442\u0447\u0435\u0439, \u0433\u0434\u0435 $N=\\frac{H * W}{P^2}$. \u0415\u0441\u043b\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 48x48, \u0440\u0430\u0437\u043c\u0435\u0440 \u043f\u0430\u0442\u0447\u0430 16x16, \u0442\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u0441\u044f $N=\\frac{48 * 48}{16^2} = 9$.","7d426cb8":"### Self-Attention\n\u0411\u0443\u0434\u0435\u043c \u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441\u043e \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u043c \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435\u043c:\n\n\u201dThe animal didn't cross the street because it was too tired\u201d\n\n\u0427\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 it \u0432 \u044d\u0442\u043e\u043c \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0438? It \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0441\u044f \u043a \u0443\u043b\u0438\u0446\u0435 \u0438\u043b\u0438 \u043a \u0436\u0438\u0432\u043e\u0442\u043d\u043e\u043c\u0443? \u041a\u043e\u0433\u0434\u0430 \u043c\u043e\u0434\u0435\u043b\u044c \u043e\u0431\u0440\u0430\u0431\u0430\u0442\u044b\u0432\u0430\u0435\u0442 \u0441\u043b\u043e\u0432\u043e it, self-attention \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0435\u0439 \u0430\u0441\u0441\u043e\u0446\u0438\u0438\u0440\u043e\u0432\u0430\u0442\u044c it \u0441 animal, \u043d\u043e it \u0435\u0449\u0435 \u0438 \u0441\u0432\u044f\u0437\u0430\u043d\u043e \u0441 tired.\n\nSelf-attention \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u043d\u0435\u0439\u0440\u043e\u0441\u0435\u0442\u043a\u0435 \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043d\u0430 \u0434\u0440\u0443\u0433\u0438\u0435 \u0441\u043b\u043e\u0432\u0430 \u0432\u043e \u0432\u0445\u043e\u0434\u043d\u043e\u0439 \u043f\u043e\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 \u0432 \u043f\u043e\u0438\u0441\u043a\u0430\u0445 \u043f\u043e\u0434\u0441\u043a\u0430\u0437\u043e\u043a, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0433\u0443\u0442 \u043f\u043e\u043c\u043e\u0447\u044c \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u043a \u043b\u0443\u0447\u0448\u0435\u043c\u0443 \u043a\u043e\u0434\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044e \u044d\u0442\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430.\n\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_self-attention_visualization.png' width=400>","403b658f":"This is notebook is based on [CassavaLeaf ViT baseline Notebook](https:\/\/www.kaggle.com\/szuzhangzhi\/vision-transformer-vit-cuda-as-usual) and Adjusted for Plant pathology 2021 dataset.","b3c7d165":"### Transformer Encoder\n\n![image.png](attachment:6a08e9ff-e266-493e-880d-5ab34014dc73.png)\n\n\u042d\u043d\u043a\u043e\u0434\u0435\u0440 \u0441\u043e\u0441\u0442\u043e\u0438\u0442 \u0438\u0437\n- Multi-Head Self Attention Layer(MSP) \u0434\u043b\u044f \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0433\u043e \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0435\u043d\u0438\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u0438\u0445 \u0432\u044b\u0445\u043e\u0434\u043e\u0432 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u0432 \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0438\u0438 \u0441 \u043e\u0436\u0438\u0434\u0430\u0435\u043c\u044b\u043c\u0438 \u0440\u0430\u0437\u043c\u0435\u0440\u0430\u043c\u0438. \u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e Multi-Head \u043f\u043e\u043c\u043e\u0433\u0430\u044e\u0442 \u0438\u0437\u0443\u0447\u0430\u0442\u044c \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u0438 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u0432 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0438.\n- \u041c\u043d\u043e\u0433\u043e\u0441\u043b\u043e\u0439\u043d\u044b\u0435 \u043f\u0435\u0440\u0441\u0435\u043f\u0442\u0440\u043e\u043d\u044b (MLP) \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0442 \u0434\u0432\u0430 \u0441\u043b\u043e\u044f \u0441 GELU (Gaussian Error Linear Unit)\n- Layer Norm (LN) \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u043f\u0435\u0440\u0435\u0434 \u043a\u0430\u0436\u0434\u044b\u043c \u0431\u043b\u043e\u043a\u043e\u043c, \u0442\u0430\u043a \u043a\u0430\u043a \u043e\u043d\u0430 \u043d\u0435 \u0432\u0432\u043e\u0434\u0438\u0442 \u043d\u0438\u043a\u0430\u043a\u0438\u0445 \u043d\u043e\u0432\u044b\u0445 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0435\u0439 \u043c\u0435\u0436\u0434\u0443 \u043e\u0431\u0443\u0447\u0430\u044e\u0449\u0438\u043c\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c\u0438. \u041f\u043e\u043c\u043e\u0433\u0430\u0435\u0442 \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u043e\u0431\u043e\u0431\u0449\u0435\u043d\u0438\u044f\n- Residual connections \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u044e\u0442\u0441\u044f \u043f\u043e\u0441\u043b\u0435 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0431\u043b\u043e\u043a\u0430\n\n\u0414\u043b\u044f \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u044f \u0440\u0435\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d\u0430 \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c MLP \u0441 \u043e\u0434\u043d\u0438\u043c \u0441\u043a\u0440\u044b\u0442\u044b\u043c \u0441\u043b\u043e\u0435\u043c.\n\u0412\u0435\u0440\u0445\u043d\u0438\u0435 \u0441\u043b\u043e\u0438 ViT \u0438\u0437\u0443\u0447\u0430\u044e\u0442 \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u044b, \u0442\u043e\u0433\u0434\u0430 \u043a\u0430\u043a \u043d\u0438\u0436\u043d\u0438\u0435 \u0441\u043b\u043e\u0438 \u0438\u0437\u0443\u0447\u0430\u044e\u0442 \u043a\u0430\u043a \u0433\u043b\u043e\u0431\u0430\u043b\u044c\u043d\u044b\u0435, \u0442\u0430\u043a \u0438 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u044b\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u044b. \u042d\u0442\u043e \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 ViT \u0438\u0437\u0443\u0447\u0430\u0442\u044c \u0431\u043e\u043b\u0435\u0435 \u043e\u0431\u0449\u0438\u0435 \u043f\u0430\u0442\u0442\u0435\u0440\u043d\u044b.\n","d68eb04b":"\n**\u041f\u0435\u0440\u0432\u044b\u0439 \u044d\u0442\u0430\u043f \u0432 \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u0438 self-attention** - \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0442\u0440\u0435\u0445 \u0432\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u0438\u0437 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0432\u0445\u043e\u0434\u043d\u043e\u0433\u043e \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430. \u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430 \u0441\u043e\u0437\u0434\u0430\u0435\u0442\u0441\u044f \u0432\u0435\u043a\u0442\u043e\u0440 Query, Key \u0438 Value. \u042d\u0442\u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u044b \u0441\u043e\u0437\u0434\u0430\u044e\u0442\u0441\u044f \u043f\u0443\u0442\u0435\u043c \u0443\u043c\u043d\u043e\u0436\u0435\u043d\u0438\u044f \u044d\u043c\u0431\u0435\u0434\u0434\u0438\u043d\u0433\u0430 \u043d\u0430 \u0442\u0440\u0438 \u043c\u0430\u0442\u0440\u0438\u0446\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043e\u0431\u0443\u0447\u0430\u044e\u0442\u0441\u044f.\n- Query - \u0441\u043b\u043e\u0432\u043e, \u0441 \u043a\u043e\u0442\u043e\u0440\u043e\u0433\u043e \u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0432\u0441\u0451 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u043e\u0435\n- Key - \u0441\u043b\u043e\u0432\u043e, \u043d\u0430 \u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0441\u043c\u043e\u0442\u0440\u0438\u043c\n- Value - \u0437\u0434\u0435\u0441\u044c \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442\u0441\u044f \u0441\u043c\u044b\u0441\u043b \u0441\u043b\u043e\u0432\u0430\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_self_attention_vectors.png' width=500>\n\n**\u0412\u0442\u043e\u0440\u043e\u0439 \u044d\u0442\u0430\u043f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f self-attention** \u2013 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u0435 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u0430 (score). \u041a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442 \u043f\u043e\u0434\u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0441\u043a\u0430\u043b\u044f\u0440\u043d\u043e\u0433\u043e \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0438\u044f \u0432\u0435\u043a\u0442\u043e\u0440\u0430 Query \u0438 \u0432\u0435\u043a\u0442\u043e\u0440\u0430 Key \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0435\u0433\u043e \u0441\u043b\u043e\u0432\u0430. \u041f\u0440\u0438 \u043f\u0435\u0440\u0435\u043c\u043d\u043e\u0436\u0435\u043d\u0438\u0438 Query \u043d\u0430 Key \u043c\u044b \u043f\u043e\u043d\u0438\u043c\u0430\u0435\u043c \u043d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u043b\u043e\u0432\u043e \u0441 Key \u0440\u0435\u043b\u0435\u0432\u0430\u043d\u0442\u043d\u043e \u0441\u043b\u043e\u0432\u0443 \u0441 Query.\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_self_attention_score.png' width=500>\n\n**\u0422\u0440\u0435\u0442\u0438\u0439 \u0438 \u0447\u0435\u0442\u0432\u0435\u0440\u0442\u044b\u0439 \u044d\u0442\u0430\u043f\u044b** \u2013 \u0440\u0430\u0437\u0434\u0435\u043b\u0438\u0442\u044c \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b \u043d\u0430 $\\sqrt{d}$ (\u043a\u0432\u0430\u0434\u0440\u0430\u0442\u043d\u044b\u0439 \u043a\u043e\u0440\u0435\u043d\u044c \u0438\u0437 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 Key \u0432\u0435\u043a\u0442\u043e\u0440\u0430; \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u0431\u044b\u043b\u0438 \u0431\u043e\u043b\u0435\u0435 \u0441\u0442\u0430\u0431\u0438\u043b\u044c\u043d\u044b\u0435 \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u044b), \u0430 \u0437\u0430\u0442\u0435\u043c \u043f\u0440\u043e\u043f\u0443\u0441\u0442\u0438\u0442\u044c \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442 \u0447\u0435\u0440\u0435\u0437 \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0441\u043e\u0444\u0442\u043c\u0430\u043a\u0441 (softmax). \u0414\u0430\u043d\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u0442 \u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442\u044b \u0442\u0430\u043a, \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u0438 \u0431\u044b\u043b\u0438 \u043f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u043c\u0438 \u0438 \u0432 \u0441\u0443\u043c\u043c\u0435 \u0434\u0430\u0432\u0430\u043b\u0438 1.\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/self-attention_softmax.png' width=500>\n\n**\u041f\u044f\u0442\u044b\u0439 \u044d\u0442\u0430\u043f** \u2013 \u0443\u043c\u043d\u043e\u0436\u0438\u0442\u044c \u043a\u0430\u0436\u0434\u044b\u0439 \u0432\u0435\u043a\u0442\u043e\u0440 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f (Value) \u043d\u0430 \u0441\u043e\u0444\u0442\u043c\u0430\u043a\u0441-\u043a\u043e\u044d\u0444\u0444\u0438\u0446\u0438\u0435\u043d\u0442.\n\n**\u0428\u0435\u0441\u0442\u043e\u0439 \u044d\u0442\u0430\u043f** \u2013 \u0441\u043b\u043e\u0436\u0438\u0442\u044c \u0432\u0437\u0432\u0435\u0448\u0435\u043d\u043d\u044b\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u044b \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f. \u042d\u0442\u043e \u0438 \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c \u0441\u043e\u0431\u043e\u0439 \u0432\u044b\u0445\u043e\u0434 \u0441\u043b\u043e\u044f \u0432\u043d\u0443\u0442\u0440\u0435\u043d\u043d\u0435\u0433\u043e \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u0432 \u0434\u0430\u043d\u043d\u043e\u0439 \u043f\u043e\u0437\u0438\u0446\u0438\u0438 (\u0434\u043b\u044f \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430).\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/self-attention-output.png' width=500>\n\n\n**\u041c\u0430\u0442\u0440\u0438\u0447\u043d\u0430\u044f \u043a\u0440\u0430\u0442\u043a\u0430\u044f \u0437\u0430\u043f\u0438\u0441\u044c**\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/self-attention-matrix-calculation-2.png' width=400>","702f079c":"## Testing","94840860":"### Multi-Head Self-Attention\n\n\u0423\u043b\u0443\u0447\u0448\u0430\u0435\u0442 \u0440\u0430\u0431\u043e\u0442\u0443 \u0441\u043b\u043e\u044f \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u0434\u0432\u0443\u043c\u044f \u0441\u043f\u043e\u0441\u043e\u0431\u0430\u043c\u0438:\n\n1. \u0420\u0430\u0441\u0448\u0438\u0440\u044f\u0435\u0442 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e\u0441\u0442\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u0444\u043e\u043a\u0443\u0441\u0438\u0440\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043d\u0430 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u0441\u043b\u043e\u0432\u0430\u0445.\n\n2. \u0414\u0430\u0435\u0442 \u0441\u043b\u043e\u044e \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u044f \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u201c\u043f\u043e\u0434\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f\u201d. \u0421 Multi-Head self-attention \u0435\u0441\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0432\u0435\u0441\u043e\u0432\u044b\u0445 \u043c\u0430\u0442\u0440\u0438\u0446 query\/key\/value. \u041a\u0430\u0436\u0434\u044b\u0439 \u0438\u0437 \u044d\u0442\u0438\u0445 \u043d\u0430\u0431\u043e\u0440\u043e\u0432 \u0438\u043d\u0438\u0446\u0438\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0441\u043b\u0443\u0447\u0430\u0439\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c. \u0417\u0430\u0442\u0435\u043c, \u043f\u043e\u0441\u043b\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f, \u043a\u0430\u0436\u0434\u044b\u0439 \u043d\u0430\u0431\u043e\u0440 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u0434\u043b\u044f \u043f\u0440\u043e\u0435\u0446\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0432\u0445\u043e\u0434\u043d\u044b\u0445 \u0432\u043b\u043e\u0436\u0435\u043d\u0438\u0439 \u0432 \u0434\u0440\u0443\u0433\u043e\u0435 \u043f\u043e\u0434\u043f\u0440\u043e\u0441\u0442\u0440\u0430\u043d\u0441\u0442\u0432\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044f.\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_attention_heads_qkv.png' width=600>\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_attention_heads_z.png' width=600>\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_attention_heads_weight_matrix_o.png' width=600>\n\n<img src='http:\/\/jalammar.github.io\/images\/t\/transformer_self-attention_visualization_2.png' width=400>\n\n\n\n\n","8b888b9e":"\u041f\u043e\u043b\u0435\u0437\u043d\u044b\u0435 \u0441\u0441\u044b\u043b\u043a\u0438:\n1. [Swin-Transformer](https:\/\/github.com\/microsoft\/Swin-Transformer)\n2. [VisionTransformer-Pytorch](https:\/\/github.com\/tczhangzhi\/VisionTransformer-PyTorch)\n3. [vit-pytorch](https:\/\/github.com\/lucidrains\/vit-pytorch)","06c57a5f":"Self-Attention \u043c\u0435\u0436\u0434\u0443 \u043f\u0438\u043a\u0441\u0435\u043b\u044f\u043c\u0438. \u041a \u043f\u0440\u0438\u043c\u0435\u0440\u0443, \u0435\u0441\u043b\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430 \u0440\u0430\u0437\u043c\u0435\u0440\u0430 640x640, \u043c\u043e\u0434\u0435\u043b\u044c\u043a\u0435 \u043d\u0430\u0434\u043e \u043f\u043e\u0441\u0447\u0438\u0442\u0430\u0442\u044c self-attention \u0434\u043b\u044f 409\u043a \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u0439. \u041d\u043e \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u0435\u0439 \u0432\u0441\u0435\u0433\u043e, \u0441\u0430\u043c\u044b\u0439 \u0432\u0435\u0440\u0445\u043d\u0438\u0439 \u043f\u0440\u0430\u0432\u044b\u0439 \u043f\u0438\u043a\u0441\u0435\u043b\u044c \u0432\u0440\u044f\u0434 \u043b\u0438 \u0431\u0443\u0434\u0435\u0442 \u0438\u043c\u0435\u0442\u044c \u0437\u043d\u0430\u0447\u0438\u043c\u043e\u0435 \u0432\u043b\u0438\u044f\u043d\u0438\u0435 \u043d\u0430 \u043d\u0438\u0436\u043d\u0438\u0439 \u043b\u0435\u0432\u044b\u0439 \u043f\u0438\u043a\u0441\u0435\u043b\u044c. ViT \u0441\u043f\u0440\u0430\u0432\u0438\u043b\u0441\u044f \u0441 \u044d\u0442\u043e\u0439 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u043e\u0439 \u0437\u0430 \u0441\u0447\u0435\u0442 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438 \u043d\u0430 \u043c\u0430\u043b\u0435\u043d\u044c\u043a\u0438\u0435 \u043f\u0430\u0442\u0447\u0438 (\u043a \u043f\u0440\u0438\u043c\u0435\u0440\u0443, 16x16).\n\n![image.png](attachment:cded490a-0e7b-4b0e-99c5-cd91584bb995.png)","6308f5b6":"\u0421\u0441\u044b\u043b\u043a\u0438:\n1. [Vision Transformers: A New Computer Vision Paradigm](https:\/\/medium.com\/swlh\/visual-transformers-a-new-computer-vision-paradigm-aa78c2a2ccf2)\n2. [Visual Transformers: Token-based Image Representation and Processing for Computer Vision](https:\/\/arxiv.org\/pdf\/2006.03677.pdf)\n3. [Vision Transformer (ViT) - An image is worth 16x16 words | Paper Explained](https:\/\/www.youtube.com\/watch?v=j6kuz_NqkG0)\n4. [\u041f\u0440\u0438\u043a\u043b\u0430\u0434\u043d\u043e\u0435 \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 4. Self-Attention. Transformer overview](https:\/\/youtu.be\/UETKUIlYE6g)","4187aa8f":"\u041e\u0431\u044b\u0447\u043d\u043e \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0443 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e\u0442 \u043a\u0430\u043a 3D array (\u0432\u044b\u0441\u043e\u0442\u0430, \u0448\u0438\u0440\u0438\u043d\u0430, \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u0430\u043d\u0430\u043b\u043e\u0432) \u0438 \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u0442\u0441\u044f \u043a \u043d\u0438\u043c \u0441\u0432\u0435\u0440\u0442\u043e\u0447\u043d\u044b\u0435 \u0441\u043b\u043e\u0438. \u041d\u043e \u0442\u0443\u0442 \u0435\u0441\u0442\u044c \u0440\u044f\u0434 \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043a\u043e\u0432:\n\n- \u043d\u0435 \u0432\u0441\u0435 \u043f\u0438\u043a\u0441\u0435\u043b\u0438 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u043e \u043f\u043e\u043b\u0435\u0437\u043d\u044b;\n- \u0441\u0432\u0435\u0440\u0442\u043a\u0438 \u043d\u0435 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u0445\u043e\u0440\u043e\u0448\u043e \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442 \u0441 \u043f\u0438\u043a\u0441\u0435\u043b\u044f\u043c\u0438, \u043d\u0430\u0445\u043e\u0434\u044f\u0449\u0438\u043c\u0438\u0441\u044f \u0434\u0430\u043b\u0435\u043a\u043e \u0434\u0440\u0443\u0433 \u043e\u0442 \u0434\u0440\u0443\u0433\u0430;\n- \u0441\u0432\u0435\u0440\u0442\u043a\u0438 \u043d\u0435\u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u044d\u0444\u0444\u0435\u043a\u0442\u0438\u0432\u043d\u044b \u0432 \u043e\u0447\u0435\u043d\u044c \u0433\u043b\u0443\u0431\u043e\u043a\u0438\u0445 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0445 \u0441\u0435\u0442\u044f\u0445.\n\n\u0412 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 \u0430\u0432\u0442\u043e\u0440\u044b \u043f\u0440\u0435\u0434\u043b\u0430\u0433\u0430\u044e\u0442 \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0432 \u0432\u0438\u0437\u0443\u0430\u043b\u044c\u043d\u044b\u0435 \u0442\u043e\u043a\u0435\u043d\u044b \u0438 \u043f\u043e\u0434\u0430\u0432\u0430\u0442\u044c \u0438\u0445 \u0432 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440.\n\n- \u0412\u043d\u0430\u0447\u0430\u043b\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u0442\u0441\u044f \u043e\u0431\u044b\u0447\u043d\u044b\u0439 backbone \u0434\u043b\u044f \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u0438\u044f feature maps\n- \u0414\u0430\u043b\u0435\u0435 feature map \u043a\u043e\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u0432 \u0432\u0438\u0437\u0443\u0430\u043b\u044c\u043d\u044b\u0435 \u0442\u043e\u043a\u0435\u043d\u044b\n- \u0422\u043e\u043a\u0435\u043d\u044b \u043f\u043e\u0434\u0430\u044e\u0442\u0441\u044f \u0432 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u044b\n- \u0412\u044b\u0445\u043e\u0434 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u0430 \u043c\u043e\u0436\u0435\u0442 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447 \u043a\u043b\u0430\u0441\u0441\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0438\n- \u0410 \u0435\u0441\u043b\u0438 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u0438\u0442\u044c \u0432\u044b\u0445\u043e\u0434 \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0435\u0440\u0430 \u0441 feature map, \u0442\u043e \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0434\u043b\u044f \u0437\u0430\u0434\u0430\u0447 \u0441\u0435\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438"}}