{"cell_type":{"73a278e5":"code","5adb9fe5":"code","1171978d":"code","072c3bbb":"code","6731cc3a":"code","b3851310":"code","dee70ed3":"code","26183cd0":"code","dde95471":"code","5b83c46e":"code","dbd6c25d":"code","bf4191b1":"code","20876b4a":"code","95e30abb":"code","49f159db":"code","4f92a3e9":"code","82fff0df":"code","40fe2448":"code","cb82a844":"code","edd959d8":"code","89e6c7b8":"code","59fb4fd3":"code","99f4b96d":"code","97839c33":"code","d3fd7c1b":"code","36088afc":"code","006392be":"code","e3e37701":"code","ac7d7d77":"code","241ea9f0":"code","55dcd510":"code","c7e88963":"code","59a4d944":"code","21876c81":"code","9e5542f3":"code","1c0ba0b2":"code","8572d430":"code","5353a936":"code","7755ebd0":"code","53701634":"code","c0d2f198":"code","a16cf542":"code","7ca8f9ee":"markdown","fc1c0f37":"markdown","f3c4f1f8":"markdown","57c882e4":"markdown","5b46cb43":"markdown","0229a26c":"markdown","a8b4d2a0":"markdown","694e46ff":"markdown","1000b79f":"markdown","9e033c10":"markdown","d0dba18e":"markdown","dd6d6967":"markdown","6d4b343a":"markdown","3d5c2dd6":"markdown","cc2c6a1f":"markdown","955ddac8":"markdown","3a5589ac":"markdown","16c4737a":"markdown","50de4b62":"markdown","b64f3df0":"markdown","11838910":"markdown"},"source":{"73a278e5":"# Importing dependancies\n\nimport os\nimport warnings\nimport pickle\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom IPython.display import display\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Input, GRU, TimeDistributed, Dense\nfrom tensorflow.keras.models import Model, save_model","5adb9fe5":"# Setting to use seaborn style for plots\nmatplotlib.style.use(\"seaborn\")\n\n# Suppress warnings\nwarnings.filterwarnings(\"ignore\")","1171978d":"# Loads data\ndata = pd.read_csv(\"..\/input\/nmtitalian2english\/ita.txt\", sep=\"\\t\")","072c3bbb":"# Sets column names\ndata.columns = [\"english\", \"italian\", \"attrib\"]","6731cc3a":"data.drop([\"attrib\"], inplace=True, axis=1)","b3851310":"display(data.head())\ndisplay(data.info())","dee70ed3":"# Drops null values\ndata.dropna(inplace=True)\ndata.info()","26183cd0":"# Adds 'sos' and 'eos' to start and end of target (Italian) text\n# 'sos' --> Denotes start of sequence; 'eos' --> Denotes end of sequence\n\ndata[\"italian\"] = data[\"italian\"].apply(lambda x: \"sos \" + x + \" eos\")","dde95471":"# Train data: 60%; Validation data: 20%; Test data: 10000 sentences;\neng_train, eng_remaining, it_train, it_remaining  = train_test_split(data[\"english\"].values, \n                                                                     data[\"italian\"].values, \n                                                                     test_size=0.4, \n                                                                     random_state=22)\n\neng_val, eng_test, it_val, it_test = train_test_split(eng_remaining, \n                                                      it_remaining, \n                                                      test_size=0.5, \n                                                      random_state=22)\n\n# Taking 10k sentences as test set\neng_test = eng_test[:10000]\nit_test = it_test[:10000]","5b83c46e":"print(len(eng_train))\nprint(len(eng_val))\nprint(len(eng_test))","dbd6c25d":"print(eng_train[:5])\nprint(eng_val[:5])\nprint(eng_test[:5])\nprint(it_train[:5])\nprint(it_val[:5])\nprint(it_test[:5])","bf4191b1":"def plot_seq_length(df_series, kind=\"hist\"):\n    \n    \"\"\" Plots length of all sentences in a pandas series\n     \n    Args:\n        df_series (pd.Series): Pandas series containing text data.\n        kind (str): Type of visualisation. 'hist' or 'box'.\n    \n    Returns:\n        Nothing\n     \n    \"\"\"\n    \n    if kind==\"hist\":\n        ax1 = df_series.str.split().map(lambda x: len(x)).hist(figsize=(10, 6))\n        ax1.set(xlabel=\"( Number of Tokens )\", ylabel=\"( Number of Observations )\")\n        plt.show()\n    \n    elif kind==\"box\":\n        ax3 = df_series.str.split().map(lambda x: len(x)).plot.box(figsize=(6,8))\n        ax3.set_ylabel(\"( Number of Tokens )\")\n        plt.show()","20876b4a":"# Plotting histogram for number of tokens in English sentences\nplot_seq_length(data.english)","95e30abb":"# Plotting histogram for number of tokens in Italian sentences\nplot_seq_length(data.italian)","49f159db":"# Plotting boxplot for number of tokens in English sentences\nplot_seq_length(data.italian, kind=\"box\")","4f92a3e9":"# Plotting boxplot for number of tokens in Italian sentences\nplot_seq_length(data.italian, kind=\"box\")","82fff0df":"# Instantiating and fitting the Tokenizer on english text\neng_tokenizer_train_data = data[\"english\"]\neng_tok = Tokenizer(oov_token=\"UNK\")\neng_tok.fit_on_texts(eng_tokenizer_train_data.values)\n\n# Instantiating and fitting the Tokenizer on italian text\nit_tokenizer_train_data = data[\"italian\"]\nit_tok = Tokenizer(oov_token=\"UNK\")\nit_tok.fit_on_texts(it_tokenizer_train_data.values)","40fe2448":"def sent_to_seq(sequences, tokenizer, vocab_size=None, reverse=False, onehot=False):\n    \n    \"\"\" Converts text data into sequences supported by model input layers.\n    \n    Args:\n        sequences (list): List of text data.\n        tokenizer (tf.keras.preprocessing.text.Tokenizer): Tensorflow tokenizer object.\n        vocab_size (int): Number of words in the whole vocabulary.\n        reverse (bool): Reverses the padded sequence if set True. Defaults False.\n                        (Eg: if set True, [1 2 3 0 0] becomes [0 0 3 2 1])\n        onehot (bool): Creates onehot representation of the padded sequence if set True.\n                       Defaults False.\n                       \n    Returns:\n        preprocessed_seq (list): List of preprocessed sequences.\n        \n    \"\"\"\n    \n    # Tokenizing\n    seq = tokenizer.texts_to_sequences(sequences)\n    \n    # Padding\n    preprocessed_seq = pad_sequences(seq, padding='post', truncating='post', maxlen=20)\n    \n    # Reversing\n    if reverse:\n        preprocessed_seq = preprocessed_seq[:, ::-1]\n    \n    # Onehot encoding\n    if onehot:\n        preprocessed_seq = to_categorical(preprocessed_seq, num_classes=vocab_size) \n    \n    return preprocessed_seq\n","cb82a844":"def generate_batch(X, y, batch_size):\n    \n    \"\"\" Generator function to preprocess the data in batches and\n        feed them to the model.\n        \n    Args:\n        X (list): List of text data.\n        y (list): List of text data.\n        batch_size (int): Number of items to be preprocessed and fed\n                          in a single batch.\n    Yields:\n        en_x (list): Preprocessed data supported by encoder input layer.\n        de_x (list): Preprocessed data supported by decoder input layer.\n                    (1 timestep behind de_y, since \n                     TimeDistributed layer is being used for training)\n        de_y (list): Preprocessed actual outputs of decoder. \n                     (1 timestep ahead of de_x, since \n                     TimeDistributed layer is being used for training)\n    \"\"\"\n    \n    while True:   \n        for batch in range(0, len(X), batch_size):\n            en_x = sent_to_seq(X[batch: batch+batch_size], \n                               tokenizer = eng_tok, \n                               vocab_size = eng_vocab_size,\n                               onehot = True, \n                               reverse = True)\n\n            de_xy = sent_to_seq(y[batch: batch+batch_size],\n                                tokenizer = it_tok,\n                                vocab_size = it_vocab_size,\n                                onehot = True)\n\n            de_x = de_xy[:,:-1,:]\n            de_y = de_xy[:,1:,:]\n\n            yield([en_x, de_x], de_y)","edd959d8":"eng_seq_len = 20    # First dimension of encoder Input shape\neng_vocab_size = len(eng_tok.word_index)+1    # Second dimension of encoder Input shape\ngru_units = 2048    # Number of GRU units","89e6c7b8":"# Encoder  \nen_inputs = Input(shape=(eng_seq_len, eng_vocab_size)) \nen_gru = GRU(gru_units, return_state=True) \nen_out, en_state = en_gru(en_inputs)","59fb4fd3":"it_seq_len = 20     # First dimension of decoder Input shape\nit_vocab_size = len(it_tok.word_index)+1     # Second dimension of decoder Input shape","99f4b96d":"# Decoder\nde_inputs = Input(shape=(it_seq_len-1, it_vocab_size))\nde_gru = GRU(gru_units, return_sequences=True)\nde_out = de_gru(de_inputs, initial_state=en_state)","97839c33":"# Decoder output architecture\n# TimeDistributed layer is used for training purpose\nde_dense = TimeDistributed(Dense(it_vocab_size, activation='softmax'))    \nde_pred = de_dense(de_out)","d3fd7c1b":"# Compiling\nnmt = Model(inputs=[en_inputs, de_inputs], outputs=de_pred) \nnmt.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"acc\"])","36088afc":"print(nmt.summary())","006392be":"def plot_history(history, metric = \"loss\"):\n    \n    \"\"\" Plots training history of models.\n    \n    Args:\n        history (History): Tensorflow History object containing training \n                           history of the model.\n        metric (str): 'accuracy' or 'loss'. Metric to be plotted. \n                           \n    Returns:\n        Nothing\n        \n    \"\"\"\n    \n    history_dict = history.history\n    epochs = range(1, len(history_dict['loss'])+1)\n    \n    if metric == \"loss\":\n        train_loss = history_dict['loss']    # Training loss over epochs\n        val_loss = history_dict['val_loss']    # Validation loss over epochs\n        plt.plot(epochs, train_loss,'b', label='Training error')\n        plt.plot(epochs, val_loss,'b', color=\"orange\", label='Validation error')\n        plt.title('Training and Validation error')\n        plt.xlabel('Epochs')\n        plt.ylabel('Error')\n        plt.legend()\n        plt.show()\n    \n    elif metric == \"accuracy\":\n        train_acc = history_dict['acc']    # Training accuracy over epochs\n        val_acc = history_dict['val_acc']    # Validation accuracy over epochs\n        plt.plot(epochs, train_acc,'b', label='Training accuracy')\n        plt.plot(epochs, val_acc,'b', color=\"orange\", label='Validation accuracy')\n        plt.title('Training and Validation accuracy')\n        plt.xlabel('Epochs')\n        plt.ylabel('Accuracy')\n        plt.legend()\n        plt.show()\n    \n    ","e3e37701":"n_epochs, batch_size = 10, 64","ac7d7d77":"# Training and validation\nhistory = nmt.fit(generate_batch(eng_train, it_train, batch_size=batch_size),\n                  steps_per_epoch = len(eng_train)\/\/batch_size,\n                  epochs = n_epochs,\n                  validation_data = generate_batch(eng_val, it_val, batch_size=batch_size),\n                  validation_steps = len(eng_val)\/\/batch_size)","241ea9f0":"plot_history(history)","55dcd510":"plot_history(history, metric=\"accuracy\")","c7e88963":"# Evaluating on completely unseen data\nresults = nmt.evaluate(generate_batch(eng_test, it_test, batch_size=batch_size), steps=batch_size)\nprint(f\"========== Test set loss: {results[0]} - Test set Accuracy: {results[1]*100} % ==========\")","59a4d944":"en_gru_weights = en_gru.get_weights()   # Encoder GRU layer weights\nde_gru_weights = de_gru.get_weights()    # Decoder GRU layer weights\nde_dense_weights = de_dense.get_weights()    # Decoder Dense layer weights","21876c81":"# Encoder\nen_inputs = Input(shape=(eng_seq_len, eng_vocab_size)) \nen_gru = GRU(gru_units, return_state=True) \nen_out, en_state = en_gru(en_inputs)\n\nencoder = Model(inputs=en_inputs, outputs=en_state)","9e5542f3":"# Assigns weights of trained model layer to inference model layer \nen_gru.set_weights(en_gru_weights)","1c0ba0b2":"# Decoder\nde_inputs = Input(shape=(1, it_vocab_size))\n\n# Following layer is added in inference model\n# to take context vector as input\nde_state_in = Input(shape=(gru_units,))     \n\nde_gru = GRU(gru_units, return_state=True) \nde_out, de_state_out = de_gru(de_inputs, initial_state=de_state_in) \n\n# TimeDistributed layer is not used since\n# we will be using .predict() method for\n# each timestep manually\nde_dense = Dense(it_vocab_size, activation='softmax') \nde_pred = de_dense(de_out)\n\ndecoder = Model(inputs=[de_inputs, de_state_in], outputs=[de_pred, de_state_out]) ","8572d430":"# Assigns weights of trained model layers to inference model layers \nde_gru.set_weights(de_gru_weights)\nde_dense.set_weights(de_dense_weights)","5353a936":"print(decoder.summary())","7755ebd0":"def word_to_onehot(tokenizer, word, vocab_size):\n    \n    \"\"\" Converts a single word into onehot representation.\n    \n    Args:\n        tokenizer (tf.keras.preprocessing.text.Tokenizer): Tensorflow tokenizer object.\n        word (str): Word to be tokenized and onehot encoded.\n        vocab_size (int): Number of words in the whole vocabulary.\n    \n    Returns:\n        de_onhot (list): Onehot representation of given word.\n        \n    \"\"\"\n    \n    de_seq = tokenizer.texts_to_sequences([[word]])\n    de_onehot = to_categorical(de_seq, num_classes=vocab_size).reshape(1, 1, vocab_size)  \n    \n    return de_onehot\n\n\ndef translate(eng_sentence):\n    \n    \"\"\" Returns Italian translation of given english sentence.\n    \n    Args:\n        eng_sentence (str): English text to be translated.\n        \n    Returns:\n        it_sent (str): Italian translated text.\n        \n    \"\"\"\n    \n    en_seq = sent_to_seq([eng_sentence], \n                         tokenizer=eng_tok, \n                         reverse=True, \n                         onehot=True, \n                         vocab_size=eng_vocab_size)\n    \n    en_st = encoder.predict(en_seq)\n    de_seq = word_to_onehot(it_tok, \"sos\", it_vocab_size)\n    it_sent = \"\"\n    for i in range(it_seq_len):         \n        de_prob, en_st = decoder.predict([de_seq, en_st])\n        index = np.argmax(de_prob[0, :], axis=-1)\n        de_w = it_tok.index_word[index]\n        de_seq = word_to_onehot(it_tok, de_w, it_vocab_size) \n        if de_w == 'eos': break\n        it_sent += de_w + ' '\n        \n    return it_sent","53701634":"for sent in eng_test[: 25]:\n    print(f\"English: {sent}\")\n    print(f\"Translated text: {translate(sent)}\")","c0d2f198":"# English\neng_tok_file = open(\"English Tokenizer.pkl\", \"wb\")\npickle.dump(eng_tok, eng_tok_file)\n\n# Italian\nit_tok_file = open(\"Italian Tokenizer.pkl\", \"wb\")\npickle.dump(it_tok, it_tok_file)","a16cf542":"# Encoder\nsave_model(encoder, \".\/encoder\")\n\n# Decoder\nsave_model(decoder, \".\/decoder\")","7ca8f9ee":"By : [Balamurugan P](https:\/\/www.linkedin.com\/in\/bala-murugan-62073b212\/)","fc1c0f37":"![image.png](attachment:bc538a71-f043-4027-bbde-9fbd218f0865.png)","f3c4f1f8":"# **Saving Tokenizers :**","57c882e4":"# **Saving NMT components :**","5b46cb43":"# **Training, Evaluating and plotting results :**","0229a26c":"# **Why TimeDistributed layer is being used and what is teahcer forcing?**\n<br><\/br>\nWe are developing NMT model with **teacher forcing algorithm**. So, What is teacher forcing? Normally we will use a repeat vector layer to repeat the context vector provided by the encoder as input to decoder at each timestep. Instead, In teacher forcing algorithm, we will be supplying the ouput of the decoder at previous time step as input to the decoder at current time step. Same applies for the context vector. Hence achieving better results.  \n<br><\/br>\n**Time Distributed** layer is being used for training purpose. We will not be using this layer in inference model. The layer takes the whole inputs and feeds the vectors and compares results one by one for each time step.","a8b4d2a0":"# **Overview :**\n\nI have developed **Neural machine translation model with teacher forcing** using Tensorflow to translate text from **English to Italian**. This model has been trained with over 200k+ sequences with consistent performance. The model has **encoder-decoder architecture** which is efficient for **sequence to sequence** tasks.","694e46ff":"# **Translation in action :**","1000b79f":"# **Copying trained layers' weights :**","9e033c10":"# **Loading data and basic preprocessing :**","d0dba18e":"# **Dependancies :**","dd6d6967":"# **Basic EDA :**","6d4b343a":"# **Defining Model Architecture :**","3d5c2dd6":"# **Defining inference model Architecture :**","cc2c6a1f":"# **Configuration :**","955ddac8":"# **Fitting tokenizer and defining preprocessing functions :**","3a5589ac":"# **Splitting Data :**","16c4737a":"# **Data :**\n\n**Link to the dataset :** https:\/\/www.kaggle.com\/balamurugan1603\/nmtitalian2english\n<br>The dataset contains more than 350k English-Italian Translations.","50de4b62":"# **Neural Machine Translation with Teacher Forcing using Tensorflow**","b64f3df0":"*'sos' and 'eos' tokens are necessary for making inference from the model while using teacher forcing algorithm. So we have to train the model with these tokens in the sentences.*","11838910":"*So, we can clearly see that the sequence length is around 15-17 on average.*"}}