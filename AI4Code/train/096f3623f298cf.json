{"cell_type":{"56cbe7fb":"code","424c9dfa":"code","77a11428":"code","5ac9cf5b":"code","9c1ea6ee":"code","2075e8fc":"code","f3f9840d":"code","f520f74f":"code","eecec846":"code","e8be2d63":"code","2ed0b733":"code","eab09295":"code","cdfd1aaf":"code","cd2c897f":"code","2d9c3466":"markdown","1a018683":"markdown","01f46b73":"markdown","9bae5818":"markdown","64077001":"markdown","68f7219a":"markdown","51939604":"markdown","4cc12380":"markdown","2c377a4a":"markdown","4091b495":"markdown","eb560b2a":"markdown","5e4a1944":"markdown","23e32bfd":"markdown","f48a0066":"markdown","e4ece941":"markdown","97134a40":"markdown"},"source":{"56cbe7fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","424c9dfa":"#load data set\nx_l = np.load('..\/input\/sign-language-digits-dataset\/X.npy')\nY_l = np.load('..\/input\/sign-language-digits-dataset\/Y.npy')\nimg_size= 64\nplt.subplot(1,2,1)# \"121\" means \"1x2 grid, first subplot\" and \"234\" means \"2x3 grid, 4th subplot\".\nplt.imshow(x_l[260].reshape(img_size, img_size))\nplt.axis('off')\nplt.subplot(1,2,2)# \"122\" means \"1x2 grid, second subplot\".\nplt.imshow(x_l[900].reshape(img_size, img_size))\nplt.axis('off')","77a11428":"#Join a sequence of arrays along an row axis.\nX = np.concatenate((x_l[204:409], x_l[822:1027]), axis=0)\n#From 0 to 204 is zero sign and from 205 to 410 is one sign.\nz = np.zeros(205)\no = np.ones(205)\nY = np.concatenate((z,o), axis=0).reshape(X.shape[0],1)\nprint(\"X shape: \", X.shape)\nprint(\"Y shape: \", Y.shape)","5ac9cf5b":"#Lets create x_train, y_train, x_test, y_test array.\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]","9c1ea6ee":"X_train_flatten = X_train.reshape(number_of_train, X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test.reshape(number_of_test, X_test.shape[1]*X_test.shape[2])\nprint(\"X train flatten\", X_train_flatten.shape)\nprint(\"X test flatten\", X_test_flatten.shape)","2075e8fc":"x_train = X_train_flatten.T\nx_test = X_test_flatten.T\ny_train = Y_train.T\ny_test = Y_test.T\nprint(\"x train: \", x_train.shape)\nprint(\"x test: \", x_test.shape)\nprint(\"y train: \", y_train.shape)\nprint(\"y test: \", y_test.shape)","f3f9840d":"# lets initialixe parameters\n# dimension 4096 that is number of pixels as a parameter for our initialize method(def)\n\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension, 1), 0.01)\n    b = 0.0\n    return w,b\nw,b= initialize_weights_and_bias(4096)\nprint(w)\nprint(b)\n\n","f520f74f":"# calculation of z\n# z = np.dot(w.T, x_train)+b\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head\ny_head = sigmoid(0)\ny_head","eecec846":"# Forward propagation steps: \n# Find z = w.T * x + b\n# y_head = sigmoid(z)\n# loss(error) = loss(y, y_head)\n# cost = sum(loss)\ndef forward_propagation(w, b, x_train, y_train):\n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z) # probabilistic 0-1\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]# x_train.shape[1] is for scaling.\n    \n    return cost\n    ","e8be2d63":"# In backward propagation we will use y_head that found in forward propagation\n# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation \ndef forward_backward_propagation(w, b, x_train, y_train):\n    # forward propagation \n    z = np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]# x_train.shape[1]  is for scaling\n    # backward propagation\n    derivative_weight = (np.dot(x_train, ((y_head-y_train).T)))\/x_train.shape[1]# x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]# x_train.shape[1]is for scaling\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    return cost, gradients","2ed0b733":"# Updating (learning parameters)\ndef update (w, b, x_train, y_train, learning_rate, number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iteration times\n    for i in range(number_of_iteration):\n        #makee forward and backward propagation and find cost and gradients\n        cost, gradients = forward_backward_propagation(w, b, x_train, y_train)\n        cost_list.append(cost)\n        #lets update\n        w = w-learning_rate * gradients[\"derivative_weight\"]\n        b = b-learning_rate * gradients[\"derivative_bias\"]\n        \n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w, \"bias\": b}\n    plt.plot(index, cost_list2)\n    plt.xticks(index, rotation = 'vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\nparameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.009, number_of_iteration = 200)\n\n            ","eab09295":"# prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation.\n    z = sigmoid(np.dot(w.T, x_test)+b)\n    Y_prediction = np.zeros((1, x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1)\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0)\n    for i in range(z.shape[1]):\n        if z[0, i]<= 0.5:\n            Y_prediction[0, i] = 0\n        else:\n            Y_prediction[0, i] = 0\n            \n    return Y_prediction\n\npredict (parameters[\"weight\"], parameters[\"bias\"], x_test)\n","cdfd1aaf":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    # initialize\n    dimension = x_train.shape[0] # that is 4096.\n    w, b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, num_iterations)\n    y_prediction_test = predict(parameters[\"weight\"], parameters[\"bias\"], x_test)\n    y_prediction_train = predict(parameters[\"weight\"], parameters[\"bias\"], x_train)\n    \n    # Print train\/test Errors\n    print(\"train accuracy : {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy : {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n\nlogistic_regression(x_train, y_train, x_test, y_test, learning_rate = 0.01, num_iterations = 150)","cd2c897f":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42, max_iter = 150)\nprint(\"test accuracy: {}\".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {}\".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","2d9c3466":"* We make prediction.\n* Now lets put them all together. ","1a018683":"<a id = \"1\"><\/a><br>\n# **INTRODUCTON**\n\nContent: \n\n1. [Introduction](#1)\n1. [Overview the Data Set](#2)\n1. [Logistic Regression](#3)\n     * [Initialize Parameters](#4)\n     * [Forward Propagation](#5)\n        * Sigmoid Function\n        * Loss(error) Function\n        * Cost Function\n     * [Optimization Algorithm with Gradient Descent](#6)   \n        * Backward Propagation\n        * Updating Parameters\n1. [Logistic Regression with Sklearn](#7) ","01f46b73":"<a id = \"2\"><\/a><br>\n# **Overview the Data Set**","9bae5818":"* The shape of the X is (410, 64, 64)\n  * 410 means that we have 410 images (zero and one signs)\n  * 64 means that our image size is 64x64 (64x64 pixels)\n* The shape of the Y is (410, 1)\n  * 410 means that we have 410 labels(0 and 1).\n* Lets split X and Y into train and test sets.\n  * test_size = percentage of test of size. test = 15% and train = 75%\n  * random_state =  use same  seed while randomizing. It means that if we call train_test_split repeatedly, it always creates samee train and test distribution because we have same random_state.","64077001":"<a id = \"6\"><\/a><br>\n# **Optimization Algorithm with Gradient Descent**\n* Therefore, we need to decrease cost because as we know if cost is high it means that we make wrong prediction. Lets think first step, every thing starts with initializing weights and bias. Therefore cost is dependent with them. In order to decrease cost, we need to update weights and bias. In other words, our model needs to learn the parameters weights and bias that minimize cost function. This technique is called gradient descent.\n","68f7219a":"* As you can see, we have 348 images and each image has 4096 pixels in image train array. \n* Also, wee have 62 images and each image has 4096 pixels in imagee test arrray.\n* Then lets take transpose. You can say that WHYY, actually there is no tecnical answer. I just write the code(code that you will see oncoming parts) according to it.","51939604":"* Up to the this point we learn our parameters. It means we fit the data.\n* In onder to predict we have parameters.Therefore, lets predict.\n* In predict step we have x_test as input and while using it, we make forward predict.\n","4cc12380":"* In order to create image array, I concatenate zero and one sign arrays.\n* Then I create label array 0 for zero sign images and 1 for one sign images.","2c377a4a":"* Now we have 3 dimensional input array (X) so we need to make it flatten (2D) in order to use as input for out first deep learnin model.\n* Our label array (Y) is already flatten(2D) so we leave it like that.\n* Lets flatten X array(images array).","4091b495":"* We learn logical behind simple neural network(logical regression) and how to implement it.\n* Now that we have learned logic, we can use sklearn library which is easier than implementing all steps with hand for logical regression.","eb560b2a":"<a id = \"5\"><\/a><br>\n# **Forward Propagation**\n* All steps from pixels to cost is called forward propagation\n  * z = (w.T)x + b ==> in tthis equation we know x that is pixel array, we know w (weights) and b (bias) so the rest is calculation.(T is transpose)\n  * Then we put z into sigmoid function that returns y_head(probability). When your mind is confused go and look at computation graph. Also equation of sigmoid function is in computation graph.\n  * Then we calculate loss(error) function. \n  * Cost function is summation of all loss(error).\n  * Lets start with z and the write sigmoid definition(method) that takes z as input parameters and returns y_head(probability).","5e4a1944":"* The loss function is summation of loss function. Each image creates loss function. Cost function is summation of loss functions that is created by each input image.","23e32bfd":"<a id = \"4\"><\/a><br>\n# **Initial Parameters**\n\n* We are first step that is multplying each pixels with own weights. Each pixels have own weights. Initial weights are 0.01 and initial bias is 0.","f48a0066":"<a id = \"7\"><\/a><br>\n# **Logical Regression with Sklearn**\n\n* In sklearn library, there is a logical regression method that ease implementing logical regression.","e4ece941":"* Up to this point we learn\n  * In\u0131tializing parameters(implemented)\n  * Finding cost with forward propagation and cost function (implemented)\n  * Updating(learning) parameters (weight and bias). Now lets implement it.","97134a40":"<a id = \"3\"><\/a><br>\n# **Logistic Regression**\n* Our parameters are weight and bias.\n* Weights: coefficients of each pixels.\n* Bias: intercept(yakalamak, durdurmak)\n* z = (w.t).x + b ==> \n* z = b + px1w1+ px2w2+ .....+ px4096 * w4096\n* y_head = sigmoid(z)\n*\n"}}