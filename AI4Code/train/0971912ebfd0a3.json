{"cell_type":{"86d92c00":"code","3a72e3e5":"code","4b8f3809":"code","89dba08c":"code","d65c1731":"code","367eef41":"code","38871a3f":"code","0ba8790a":"code","2fd56571":"code","a559d960":"code","c5416529":"code","1bc96ffb":"code","2b93d281":"code","b52df662":"code","2ed51525":"code","4e99f00b":"code","6b78475b":"code","5e53c359":"code","708ec7dc":"code","db9dca6d":"code","0aa9e95e":"code","0203070a":"code","4a5cb8c4":"code","8c18836d":"code","f53a77e0":"code","265b2d3c":"code","6c0063ac":"code","3f6b1d01":"code","e0875614":"code","ad72577f":"code","7d743e64":"code","5ca49f04":"code","fcad8969":"code","0ab7a71c":"code","493e48cd":"code","58a7911e":"code","5a337c4d":"code","845d0e29":"code","66e46d9a":"code","a6dbd9e4":"code","cac813b5":"code","9a0f6361":"code","7a62ba4f":"code","add15405":"code","d630f48c":"code","88c43b69":"code","8beb384f":"code","c53ef9f8":"code","0dfcf63f":"markdown","744b4a7a":"markdown","096372bd":"markdown","3409db56":"markdown","74a1ba51":"markdown","da4a585a":"markdown","079e4796":"markdown","0bb22cb0":"markdown","e93504f4":"markdown","148f9673":"markdown","6cc24d84":"markdown","1a3f824a":"markdown","287acf61":"markdown","1e76e4e8":"markdown"},"source":{"86d92c00":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.utils import resample\nimport itertools\n\nimport pywt\ntry:\n    import pathlib\nexcept ImportError:\n    import pathlib2 as pathlib\nimport scipy.signal as signal\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import normalize\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import f1_score\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout, InputLayer, LSTM, GRU, BatchNormalization, Bidirectional, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.optimizers import SGD, RMSprop\nfrom tensorflow.keras.utils import to_categorical","3a72e3e5":"#\u043a\u043b\u0430\u0441\u0441 \u0434\u0435\u0442\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u0434\u043b \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u043f\u0438\u043a\nclass Detectors:\n    \"\"\"ECG heartbeat detection algorithms\n    General useage instructions:\n    r_peaks = detectors.the_detector(ecg_in_samples)\n    The argument ecg_in_samples is a single channel ECG in volt\n    at the given sample rate.\n    \"\"\"\n    \n    def __init__(self, sampling_frequency):\n        \"\"\"\n        The constructor takes the sampling rate in Hz of the ECG data.\n        \"\"\"\n\n        self.fs = sampling_frequency\n        # this is set to a positive value for benchmarking\n        self.engzee_fake_delay = 0\n\n    def hamilton_detector(self, unfiltered_ecg):\n        \"\"\"\n        P.S. Hamilton, \n        Open Source ECG Analysis Software Documentation, E.P.Limited, 2002.\n        \"\"\"\n        \n        f1 = 8\/self.fs\n        f2 = 16\/self.fs\n\n        b, a = signal.butter(1, [f1*2, f2*2], btype='bandpass')\n\n        filtered_ecg = signal.lfilter(b, a, unfiltered_ecg)\n\n        diff = abs(np.diff(filtered_ecg))\n\n        b = np.ones(int(0.08*self.fs))\n        b = b\/int(0.08*self.fs)\n        a = [1]\n\n        ma = signal.lfilter(b, a, diff)\n\n        ma[0:len(b)*2] = 0\n\n        n_pks = []\n        n_pks_ave = 0.0\n        s_pks = []\n        s_pks_ave = 0.0\n        QRS = [0]\n        RR = []\n        RR_ave = 0.0\n\n        th = 0.0\n\n        i=0\n        idx = []\n        peaks = []  \n\n        for i in range(len(ma)):\n\n            if i>0 and i<len(ma)-1:\n                if ma[i-1]<ma[i] and ma[i+1]<ma[i]:\n                    peak = i\n                    peaks.append(i)\n\n                    if ma[peak] > th and (peak-QRS[-1])>0.3*self.fs:        \n                        QRS.append(peak)\n                        idx.append(i)\n                        s_pks.append(ma[peak])\n                        if len(n_pks)>8:\n                            s_pks.pop(0)\n                        s_pks_ave = np.mean(s_pks)\n\n                        if RR_ave != 0.0:\n                            if QRS[-1]-QRS[-2] > 1.5*RR_ave:\n                                missed_peaks = peaks[idx[-2]+1:idx[-1]]\n                                for missed_peak in missed_peaks:\n                                    if missed_peak-peaks[idx[-2]]>int(0.360*self.fs) and ma[missed_peak]>0.5*th:\n                                        QRS.append(missed_peak)\n                                        QRS.sort()\n                                        break\n\n                        if len(QRS)>2:\n                            RR.append(QRS[-1]-QRS[-2])\n                            if len(RR)>8:\n                                RR.pop(0)\n                            RR_ave = int(np.mean(RR))\n\n                    else:\n                        n_pks.append(ma[peak])\n                        if len(n_pks)>8:\n                            n_pks.pop(0)\n                        n_pks_ave = np.mean(n_pks)\n\n                    th = n_pks_ave + 0.45*(s_pks_ave-n_pks_ave)\n\n                    i+=1\n\n        QRS.pop(0)\n\n        return QRS\n\n    \n    def christov_detector(self, unfiltered_ecg):\n        \"\"\"\n        Ivaylo I. Christov, \n        Real time electrocardiogram QRS detection using combined \n        adaptive threshold, BioMedical Engineering OnLine 2004, \n        vol. 3:28, 2004.\n        \"\"\"\n        total_taps = 0\n\n        b = np.ones(int(0.02*self.fs))\n        b = b\/int(0.02*self.fs)\n        total_taps += len(b)\n        a = [1]\n\n        MA1 = signal.lfilter(b, a, unfiltered_ecg)\n\n        b = np.ones(int(0.028*self.fs))\n        b = b\/int(0.028*self.fs)\n        total_taps += len(b)\n        a = [1]\n\n        MA2 = signal.lfilter(b, a, MA1)\n\n        Y = []\n        for i in range(1, len(MA2)-1):\n            \n            diff = abs(MA2[i+1]-MA2[i-1])\n\n            Y.append(diff)\n\n        b = np.ones(int(0.040*self.fs))\n        b = b\/int(0.040*self.fs)\n        total_taps += len(b)\n        a = [1]\n\n        MA3 = signal.lfilter(b, a, Y)\n\n        MA3[0:total_taps] = 0\n\n        ms50 = int(0.05*self.fs)\n        ms200 = int(0.2*self.fs)\n        ms1200 = int(1.2*self.fs)\n        ms350 = int(0.35*self.fs)\n\n        M = 0\n        newM5 = 0\n        M_list = []\n        MM = []\n        M_slope = np.linspace(1.0, 0.6, ms1200-ms200)\n        F = 0\n        F_list = []\n        R = 0\n        RR = []\n        Rm = 0\n        R_list = []\n\n        MFR = 0\n        MFR_list = []\n\n        QRS = []\n\n        for i in range(len(MA3)):\n\n            # M\n            if i < 5*self.fs:\n                M = 0.6*np.max(MA3[:i+1])\n                MM.append(M)\n                if len(MM)>5:\n                    MM.pop(0)\n\n            elif QRS and i < QRS[-1]+ms200:\n                newM5 = 0.6*np.max(MA3[QRS[-1]:i])\n                if newM5>1.5*MM[-1]:\n                    newM5 = 1.1*MM[-1]\n\n            elif QRS and i == QRS[-1]+ms200:\n                if newM5==0:\n                    newM5 = MM[-1]\n                MM.append(newM5)\n                if len(MM)>5:\n                    MM.pop(0)    \n                M = np.mean(MM)    \n            \n            elif QRS and i > QRS[-1]+ms200 and i < QRS[-1]+ms1200:\n\n                M = np.mean(MM)*M_slope[i-(QRS[-1]+ms200)]\n\n            elif QRS and i > QRS[-1]+ms1200:\n                M = 0.6*np.mean(MM)\n\n            # F\n            if i > ms350:\n                F_section = MA3[i-ms350:i]\n                max_latest = np.max(F_section[-ms50:])\n                max_earliest = np.max(F_section[:ms50])\n                F = F + ((max_latest-max_earliest)\/150.0)\n\n            # R\n            if QRS and i < QRS[-1]+int((2.0\/3.0*Rm)):\n\n                R = 0\n\n            elif QRS and i > QRS[-1]+int((2.0\/3.0*Rm)) and i < QRS[-1]+Rm:\n\n                dec = (M-np.mean(MM))\/1.4\n                R = 0 + dec\n\n\n            MFR = M+F+R\n            M_list.append(M)\n            F_list.append(F)\n            R_list.append(R)\n            MFR_list.append(MFR)\n\n            if not QRS and MA3[i]>MFR:\n                QRS.append(i)\n            \n            elif QRS and i > QRS[-1]+ms200 and MA3[i]>MFR:\n                QRS.append(i)\n                if len(QRS)>2:\n                    RR.append(QRS[-1]-QRS[-2])\n                    if len(RR)>5:\n                        RR.pop(0)\n                    Rm = int(np.mean(RR))\n\n        QRS.pop(0)\n        \n        return QRS\n\n    \n    def engzee_detector(self, unfiltered_ecg):\n        \"\"\"\n        C. Zeelenberg, A single scan algorithm for QRS detection and\n        feature extraction, IEEE Comp. in Cardiology, vol. 6,\n        pp. 37-42, 1979 with modifications A. Lourenco, H. Silva,\n        P. Leite, R. Lourenco and A. Fred, \u201cReal Time\n        Electrocardiogram Segmentation for Finger Based ECG\n        Biometrics\u201d, BIOSIGNALS 2012, pp. 49-54, 2012.\n        \"\"\"\n                \n        f1 = 48\/self.fs\n        f2 = 52\/self.fs\n        b, a = signal.butter(4, [f1*2, f2*2], btype='bandstop')\n        filtered_ecg = signal.lfilter(b, a, unfiltered_ecg)\n\n        diff = np.zeros(len(filtered_ecg))\n        for i in range(4, len(diff)):\n            diff[i] = filtered_ecg[i]-filtered_ecg[i-4]\n\n        ci = [1,4,6,4,1]        \n        low_pass = signal.lfilter(ci, 1, diff)\n\n        low_pass[:int(0.2*self.fs)] = 0\n      \n        ms200 = int(0.2*self.fs)\n        ms1200 = int(1.2*self.fs)        \n        ms160 = int(0.16*self.fs)\n        neg_threshold = int(0.01*self.fs)\n\n        M = 0\n        M_list = []\n        neg_m = []\n        MM = []\n        M_slope = np.linspace(1.0, 0.6, ms1200-ms200)\n\n        QRS = []\n        r_peaks = []\n\n        counter = 0\n\n        thi_list = []\n        thi = False\n        thf_list = []\n        thf = False\n\n        for i in range(len(low_pass)):\n\n            # M\n            if i < 5*self.fs:\n                M = 0.6*np.max(low_pass[:i+1])\n                MM.append(M)\n                if len(MM)>5:\n                    MM.pop(0)\n\n            elif QRS and i < QRS[-1]+ms200:\n\n                newM5 = 0.6*np.max(low_pass[QRS[-1]:i])\n\n                if newM5>1.5*MM[-1]:\n                    newM5 = 1.1*MM[-1]\n\n            elif QRS and i == QRS[-1]+ms200:\n                MM.append(newM5)\n                if len(MM)>5:\n                    MM.pop(0)    \n                M = np.mean(MM)    \n            \n            elif QRS and i > QRS[-1]+ms200 and i < QRS[-1]+ms1200:\n\n                M = np.mean(MM)*M_slope[i-(QRS[-1]+ms200)]\n\n            elif QRS and i > QRS[-1]+ms1200:\n                M = 0.6*np.mean(MM)\n\n            M_list.append(M)\n            neg_m.append(-M)\n\n\n            if not QRS and low_pass[i]>M:\n                QRS.append(i)\n                thi_list.append(i)\n                thi = True\n            \n            elif QRS and i > QRS[-1]+ms200 and low_pass[i]>M:\n                QRS.append(i)\n                thi_list.append(i)\n                thi = True\n\n            if thi and i<thi_list[-1]+ms160:\n                if low_pass[i]<-M and low_pass[i-1]>-M:\n                    #thf_list.append(i)\n                    thf = True\n                    \n                if thf and low_pass[i]<-M:\n                    thf_list.append(i)\n                    counter += 1\n                \n                elif low_pass[i]>-M and thf:\n                    counter = 0\n                    thi = False\n                    thf = False\n            \n            elif thi and i>thi_list[-1]+ms160:\n                    counter = 0\n                    thi = False\n                    thf = False                                        \n            \n            if counter>neg_threshold:\n                unfiltered_section = unfiltered_ecg[thi_list[-1]-int(0.01*self.fs):i]\n                r_peaks.append(self.engzee_fake_delay+\n                               np.argmax(unfiltered_section)+thi_list[-1]-int(0.01*self.fs))\n                counter = 0\n                thi = False\n                thf = False\n\n        return r_peaks\n\n    \n    def matched_filter_detector(self, unfiltered_ecg, template_file = \"\"):\n        \"\"\"\n        FIR matched filter using template of QRS complex.\n        Template provided for 250Hz and 360Hz. Optionally provide your\n        own template file where every line has one sample.\n        Uses the Pan and Tompkins thresholding method.\n        \"\"\"\n        current_dir = pathlib.Path(__file__).resolve()\n\n        if len(template_file) > 1:\n            template = np.loadtxt(template_file)\n        else:\n            if self.fs == 250:\n                template_dir = current_dir.parent\/'templates'\/'template_250hz.csv'\n                template = np.loadtxt(template_dir)\n            elif self.fs == 360:\n                template_dir = current_dir.parent\/'templates'\/'template_360hz.csv'\n                template = np.loadtxt(template_dir)\n            else:\n                print('\\n!!No template for this frequency!!\\n')\n                return False\n\n        f0 = 0.1\/self.fs\n        f1 = 48\/self.fs\n\n        b, a = signal.butter(4, [f0*2, f1*2], btype='bandpass')\n\n        prefiltered_ecg = signal.lfilter(b, a, unfiltered_ecg)\n\n        matched_coeffs = template[::-1]  #time reversing template\n\n        detection = signal.lfilter(matched_coeffs, 1, prefiltered_ecg)  # matched filter FIR filtering\n        squared = detection*detection  # squaring matched filter output\n        squared[:len(template)] = 0\n\n        squared_peaks = panPeakDetect(squared, self.fs)\n  \n        return squared_peaks\n\n    \n    def swt_detector(self, unfiltered_ecg):\n        \"\"\"\n        Stationary Wavelet Transform \n        based on Vignesh Kalidas and Lakshman Tamil. \n        Real-time QRS detector using Stationary Wavelet Transform \n        for Automated ECG Analysis. \n        In: 2017 IEEE 17th International Conference on \n        Bioinformatics and Bioengineering (BIBE). \n        Uses the Pan and Tompkins thresolding.\n        \"\"\"\n        \n        swt_level=3\n        padding = -1\n        for i in range(1000):\n            if (len(unfiltered_ecg)+i)%2**swt_level == 0:\n                padding = i\n                break\n\n        if padding > 0:\n            unfiltered_ecg = np.pad(unfiltered_ecg, (0, padding), 'edge')\n        elif padding == -1:\n            print(\"Padding greater than 1000 required\\n\")    \n\n        swt_ecg = pywt.swt(unfiltered_ecg, 'db3', level=swt_level)\n        swt_ecg = np.array(swt_ecg)\n        swt_ecg = swt_ecg[0, 1, :]\n\n        squared = swt_ecg*swt_ecg\n\n        f1 = 0.01\/self.fs\n        f2 = 10\/self.fs\n\n        b, a = signal.butter(3, [f1*2, f2*2], btype='bandpass')\n        filtered_squared = signal.lfilter(b, a, squared)       \n\n        filt_peaks = panPeakDetect(filtered_squared, self.fs)\n        \n        return filt_peaks\n\n\n    def pan_tompkins_detector(self, unfiltered_ecg):\n        \"\"\"\n        Jiapu Pan and Willis J. Tompkins.\n        A Real-Time QRS Detection Algorithm. \n        In: IEEE Transactions on Biomedical Engineering \n        BME-32.3 (1985), pp. 230\u2013236.\n        \"\"\"\n        \n        f1 = 5\/self.fs\n        f2 = 15\/self.fs\n\n        b, a = signal.butter(1, [f1*2, f2*2], btype='bandpass')\n\n        filtered_ecg = signal.lfilter(b, a, unfiltered_ecg)        \n\n        diff = np.diff(filtered_ecg) \n\n        squared = diff*diff\n\n        N = int(0.12*self.fs)\n        mwa = MWA(squared, N)\n        mwa[:int(0.2*self.fs)] = 0\n\n        mwa_peaks = panPeakDetect(mwa, self.fs)\n\n        return mwa_peaks\n\n\n    def two_average_detector(self, unfiltered_ecg):\n        \"\"\"\n        Elgendi, Mohamed & Jonkman, \n        Mirjam & De Boer, Friso. (2010).\n        Frequency Bands Effects on QRS Detection.\n        The 3rd International Conference on Bio-inspired Systems \n        and Signal Processing (BIOSIGNALS2010). 428-431.\n        \"\"\"\n        \n        f1 = 8\/self.fs\n        f2 = 20\/self.fs\n\n        b, a = signal.butter(2, [f1*2, f2*2], btype='bandpass')\n\n        filtered_ecg = signal.lfilter(b, a, unfiltered_ecg)\n\n        window1 = int(0.12*self.fs)\n        mwa_qrs = MWA(abs(filtered_ecg), window1)\n\n        window2 = int(0.6*self.fs)\n        mwa_beat = MWA(abs(filtered_ecg), window2)\n\n        blocks = np.zeros(len(unfiltered_ecg))\n        block_height = np.max(filtered_ecg)\n\n        for i in range(len(mwa_qrs)):\n            if mwa_qrs[i] > mwa_beat[i]:\n                blocks[i] = block_height\n            else:\n                blocks[i] = 0\n\n        QRS = []\n\n        for i in range(1, len(blocks)):\n            if blocks[i-1] == 0 and blocks[i] == block_height:\n                start = i\n            \n            elif blocks[i-1] == block_height and blocks[i] == 0:\n                end = i-1\n\n                if end-start>int(0.08*self.fs):\n                    detection = np.argmax(filtered_ecg[start:end+1])+start\n                    if QRS:\n                        if detection-QRS[-1]>int(0.3*self.fs):\n                            QRS.append(detection)\n                    else:\n                        QRS.append(detection)\n\n        return QRS\n\n\ndef MWA(input_array, window_size):\n\n    mwa = np.zeros(len(input_array))\n    for i in range(len(input_array)):\n        if i < window_size:\n            section = input_array[0:i]\n        else:\n            section = input_array[i-window_size:i]\n        \n        if i!=0:\n            mwa[i] = np.mean(section)\n        else:\n            mwa[i] = input_array[i]\n\n    return mwa\n\n\ndef normalise(input_array):\n\n    output_array = (input_array-np.min(input_array))\/(np.max(input_array)-np.min(input_array))\n\n    return output_array\n\n\ndef panPeakDetect(detection, fs):    \n\n    min_distance = int(0.25*fs)\n\n    signal_peaks = [0]\n    noise_peaks = []\n\n    SPKI = 0.0\n    NPKI = 0.0\n\n    threshold_I1 = 0.0\n    threshold_I2 = 0.0\n\n    RR_missed = 0\n    index = 0\n    indexes = []\n\n    missed_peaks = []\n    peaks = []\n\n    for i in range(len(detection)):\n\n        if i>0 and i<len(detection)-1:\n            if detection[i-1]<detection[i] and detection[i+1]<detection[i]:\n                peak = i\n                peaks.append(i)\n\n                if detection[peak]>threshold_I1 and (peak-signal_peaks[-1])>0.3*fs:\n                        \n                    signal_peaks.append(peak)\n                    indexes.append(index)\n                    SPKI = 0.125*detection[signal_peaks[-1]] + 0.875*SPKI\n                    if RR_missed!=0:\n                        if signal_peaks[-1]-signal_peaks[-2]>RR_missed:\n                            missed_section_peaks = peaks[indexes[-2]+1:indexes[-1]]\n                            missed_section_peaks2 = []\n                            for missed_peak in missed_section_peaks:\n                                if missed_peak-signal_peaks[-2]>min_distance and signal_peaks[-1]-missed_peak>min_distance and detection[missed_peak]>threshold_I2:\n                                    missed_section_peaks2.append(missed_peak)\n\n                            if len(missed_section_peaks2)>0:           \n                                missed_peak = missed_section_peaks2[np.argmax(detection[missed_section_peaks2])]\n                                missed_peaks.append(missed_peak)\n                                signal_peaks.append(signal_peaks[-1])\n                                signal_peaks[-2] = missed_peak   \n\n                else:\n                    noise_peaks.append(peak)\n                    NPKI = 0.125*detection[noise_peaks[-1]] + 0.875*NPKI\n\n                threshold_I1 = NPKI + 0.25*(SPKI-NPKI)\n                threshold_I2 = 0.5*threshold_I1\n\n                if len(signal_peaks)>8:\n                    RR = np.diff(signal_peaks[-9:])\n                    RR_ave = int(np.mean(RR))\n                    RR_missed = int(1.66*RR_ave)\n\n                index = index+1      \n    \n    signal_peaks.pop(0)\n\n    return signal_peaks","4b8f3809":"def detect(unfiltered_ecg, fs=250):\n    detectors = Detectors(fs)\n    r_peaks = []\n    # r_peaks_two_average = detectors.two_average_detector(unfiltered_ecg)\n    #r_peaks = detectors.matched_filter_detector(unfiltered_ecg,\"templates\/template_250hz.csv\")\n    # r_peaks_swt = detectors.swt_detector(unfiltered_ecg)\n    # r_peaks_engzee = detectors.engzee_detector(unfiltered_ecg)\n    # r_peaks_christ = detectors.christov_detector(unfiltered_ecg)\n    # r_peaks_ham = detectors.hamilton_detector(unfiltered_ecg)\n    r_peaks = detectors.pan_tompkins_detector(unfiltered_ecg)\n    # r_peaks.append(sum(r_peaks_two_average, r_peaks_swt, r_peaks_engzee, \\\n    #               r_peaks_christ, r_peaks_ham, r_peaks_pan_tom)\/(r_peaks_two_average, \\\n    #               r_peaks_swt, r_peaks_engzee, \\\n    #               r_peaks_christ, r_peaks_ham, r_peaks_pan_tom).count())\n\n    return r_peaks\ndef visualize(unfiltered_ecg, r_peaks):\n    plt.figure()\n    plt.plot(unfiltered_ecg)\n    plt.plot(r_peaks, unfiltered_ecg[r_peaks], 'ro')\n    plt.title('Detected R-peaks')\n\n    plt.show()","89dba08c":"data_test = pd.read_csv('..\/input\/test-ecg\/new_test_df.csv')\ndata_train = pd.read_csv('..\/input\/train-ecg\/new_train_df.csv')\ndata_valid = pd.read_csv('..\/input\/valid-ecg\/new_valid_df.csv')","d65c1731":"# \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0430\u044f, \u0442\u0435\u0441\u0442\u043e\u0432\u0430\u044f \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0438 X\nX_cols = []\nfor col in data_train.columns.tolist():\n    if 'channel' in col:\n        X_cols.append(col)\ntrain_ptb = data_train[X_cols] \n","367eef41":"test_ptb = data_test[X_cols] ","38871a3f":"valid_ptb = data_valid[X_cols] ","0ba8790a":"# \u0442\u0430\u0440\u0433\u0435\u0442\u044b \u0431\u0443\u0434\u0435\u043c \u0432\u044b\u0431\u0438\u0440\u0430\u0442\u044c \u043f\u043e \u043e\u0447\u0435\u0440\u0435\u0434\u0438\ny_variables = data_train.columns.tolist()[-5:]","2fd56571":"# \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u0430\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430 y - \u0441\u0438\u043d\u0443\u0441\u043e\u0432\u044b\u0439 \u0440\u0438\u0442\u043c\nout_train_ptb = data_train[y_variables[0]]\n","a559d960":"out_test_ptb = data_test[y_variables[0]]\n","c5416529":"out_valid_ptb = data_valid[y_variables[0]]\n","1bc96ffb":"#Normalizing the training, validation & test data \n# train_ptb = normalize(train_ptb, axis=0, norm='max')\nvalid_ptb = normalize(valid_ptb, axis=0, norm='max')\ntest_ptb = normalize(test_ptb, axis=0, norm='max')","2b93d281":"# Reshaping the dataframe into a 3-D Numpy array (batch, Time Period, Value)\n# x_train_ptb = train_ptb.reshape(len(train_ptb),train_ptb.shape[1],1)\nx_valid_ptb = valid_ptb.reshape(len(valid_ptb),valid_ptb.shape[1],1)\nx_test_ptb = test_ptb.reshape(len(test_ptb),test_ptb.shape[1],1)\n\n# Converting the output into a categorical array\n# y_train_ptb = to_categorical(out_train_ptb)\ny_valid_ptb = to_categorical(out_valid_ptb)\ny_test_ptb = to_categorical(out_test_ptb)","b52df662":"# \u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u044e\u0449\u0430\u044f \u0444\u0440\u0435\u0439\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u043e \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u043e\u043c\u0443 \u044d\u043a\u0433\ndef get_unfiltered_ecg(df, ecg_id, cut_target_data=True):\n\n    ecg_unique = df[df['ecg_id']==ecg_id]\n    \n            \n    return ecg_unique[ecg_unique.columns[1:-5]] if cut_target_data else ecg_unique","2ed51525":"# \u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u044e\u0449\u0430\u044f \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0438\u0437 \u043f\u0438\u043a\u043e\u0432 \u043a \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u043c\u0443 \u044d\u043a\u0433\ndef get_peaks(df, ecg_id):\n    list_of_peaks = []\n    num_channels = []\n    ecg_unique = get_unfiltered_ecg(df, ecg_id)\n    ecg_data = ecg_unique.to_numpy()\n\n# \u043f\u043e\u0434\u0430\u0432\u0430\u0442\u044c \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u043a\u0430\u043d\u0430\u043b\u043e\u0432 (\u043f\u0438\u043a\u0438 \u043f\u043e\u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e)\n    for num_channel in range(ecg_data.shape[1]):\n        unfiltered_ecg = ecg_data[:,num_channel]\n        r_peaks = detect(unfiltered_ecg, 100)\n        # visualize(unfiltered_ecg, r_peaks)\n        list_of_peaks.append(r_peaks)\n        num_channels.append(f'channel-{num_channel}')\n\n        dict_peaks = dict(list(zip(num_channels, list_of_peaks)))\n#     dict_peaks_list[ecg_id_data] = dict_peaks\n\n#     peaks_data = pd.DataFrame(dict_peaks_list).T\n    return dict_peaks\n\n# \u0444\u0443\u043d\u043a\u0446\u0438\u044f, \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u044e\u0449\u0430\u044f \u0444\u0440\u0435\u0439\u043c \u0438\u0437 \u043f\u0438\u043a\u043e\u0432 \u0434\u043b\u044f \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u0433\u043e \u0441\u043b\u0438\u044f\u043d\u0438\u044f \u0441 \u043e\u0431\u0449\u0438\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u043f\u043e \u044d\u0442\u043e\u043c\u0443 \u044d\u043a\u0433\ndef pad_nan_peaks(dict_peaks, pad_value = 9999):\n    len_max = 0\n    for channel, list_peaks in dict_peaks.items():\n        if len_max < len(list_peaks):\n            len_max = len(list_peaks)\n    \n    for channel, list_peaks in dict_peaks.items():\n        dict_peaks[channel] = np.pad(list_peaks, (0, len_max-len(list_peaks)), 'constant', \n                 constant_values=pad_value)\n        \n    peaks_df = pd.DataFrame(dict_peaks)\n        \n    return peaks_df","4e99f00b":"def merge_peaks_to_data(df, ecg_id):\n    ecg_unique = get_unfiltered_ecg(df, ecg_id)\n    peaks_df = pad_nan_peaks(get_peaks(df, ecg_id))\n    df_with_peaks = pd.concat([ecg_unique, peaks_df], ignore_index=True)\n    df_with_peaks['ecg_id'] = ecg_id\n    return df_with_peaks","6b78475b":"def df_with_peaks(df):\n    labels = []\n    for ecg_id in df['ecg_id'].unique():\n        if ecg_id == min(df['ecg_id'].unique()):\n            df_with_peaks = merge_peaks_to_data(df, ecg_id)\n            label = np.pad([], (0, df_with_peaks.shape[0]), 'constant', \n                 constant_values=ecg_id)\n            labels.append(label)\n\n        else:\n            df_with_peaks = pd.concat([df_with_peaks, merge_peaks_to_data(df, ecg_id)])\n            \n            label = np.pad([], (0, merge_peaks_to_data(df, ecg_id).shape[0]), 'constant', \n                 constant_values=ecg_id)\n            labels.append(label)\n            \n    return df_with_peaks\n","5e53c359":"# \u0435\u0441\u043b\u0438 y \u0441 \u043f\u0438\u043a\u0430\u043c\u0438 \u0438 \u043d\u0430\u0434\u043e \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u0442\u044c \u0434\u043b\u0438\u043d\u0443 \u0442\u0430\u0440\u0433\u0435\u0442\u0430\ndef find_y(df_with_peaks, start_df):\n    ecg_id = df_with_peaks['ecg_id'][0]\n    y_value = start_df['sinus_rythm'].tolist()[0]\n    df_with_peaks['target'] = y_value\n    return df_with_peaks['target']","708ec7dc":"find_y(merge_peaks_to_data(data_train[:2000], 1), get_unfiltered_ecg(data_train[:2000], 1, False))","db9dca6d":"df_with_peaks = merge_peaks_to_data(data_valid, 17)\nstart_df = get_unfiltered_ecg(data_valid, 17, False)\n# y_valid_ptb = find_y(df_with_peaks, get_unfiltered_ecg(data_valid, 17, False))\necg_id = df_with_peaks['ecg_id'][0]\nstart_df['sinus_rythm'].tolist()[0]\n# df_with_peaks['target'] = y_value","0aa9e95e":"# \u0434\u043e\u0441\u0442\u0430\u0442\u044c \u0441\u0440\u0430\u0437\u0443 \u043f\u0438\u043a\u0438 \u0441 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u043c\u0438 \u0441 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\u043c\u0438 \u0434\u043b\u044f \u0432\u0445\u043e\u0434\u0430 \u0432 \u043c\u043e\u0434\u0435\u043b\u044c (\u0442\u044f\u0436\u0435\u043b\u044b\u0435, \u043d\u0435 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u0442\u0441\u044f)\n\n# def X_Y_valid(data_valid):\n#     y_valid_list = []\n#     x_valid_list = []\n#     for ecg_id in data_valid['ecg_id'].unique():\n#         df_with_peaks = merge_peaks_to_data(data_valid, ecg_id)\n#         y_valid_ptb = find_y(df_with_peaks, get_unfiltered_ecg(data_valid, ecg_id, False))\n#         valid_ptb = normalize(df_with_peaks, axis=0, norm='max')\n#         x_valid_ptb = valid_ptb.reshape(len(valid_ptb),valid_ptb.shape[1],1)\n#         y_valid_list.append(y_valid_ptb)\n#         x_valid_list.append(x_valid_ptb)\n\n#         y_valid_ptb = list(itertools.chain(*y_valid_list))\n#         x_valid_ptb = list(itertools.chain(*x_valid_list))\n\n#     return y_valid_list, x_valid_list\n# y_valid_list, x_valid_list = X_Y_valid(data_valid)","0203070a":"tf.keras.backend.clear_session()\n\ndef build_conv1d_model(input_shape):\n    model = keras.models.Sequential()\n    \n    model.add(Conv1D(32,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(64,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(128,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(256,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(512,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(2, activation=\"softmax\"))\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tfa.metrics.F1Score(2,\"micro\")])\n    return model\n\ndef summary_model():\n    \n\n    model_conv1d_ptb= build_conv1d_model(input_shape=(12, 1))\n    model_conv1d_ptb.summary()\n    return model_conv1d_ptb\n\n","4a5cb8c4":"def df_X_y_from_ecg_id(data_train, x_valid_ptb, y_valid_ptb):\n#     checkpoint_cb = ModelCheckpoint(\"conv1d_ptb.h5\", save_best_only=True)\n#     earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n#     model_conv1d_ptb = summary_model()\n    history_conv1d_ptb = None\n    \n    for ecg_id in data_train['ecg_id'].unique():\n        \n        df_with_peaks = merge_peaks_to_data(data_train, ecg_id)\n        y_train_ptb = find_y(df_with_peaks, get_unfiltered_ecg(data_train, ecg_id, False))\n        train_ptb = normalize(df_with_peaks, axis=0, norm='max')\n        x_train_ptb = train_ptb.reshape(len(train_ptb),train_ptb.shape[1],1)\n        \n        history_conv1d_ptb = (model_conv1d_ptb, checkpoint_cb, earlystop_cb,\n                              x_train_ptb, y_train_ptb, x_valid_ptb, y_valid_ptb)\n        print(ecg_id,sep='',end=\"\\r\",flush=True)\n\n    return model_conv1d_ptb\n","8c18836d":"\nmodel_conv1d_ptb = df_X_y_from_ecg_id(data_train[:10000], x_valid_ptb, y_valid_ptb)\n# model_conv1d_ptb.load_weights(\"conv1d_ptb.h5\")\n# model_conv1d_ptb.evaluate(x_test_ptb,y_test_ptb)","f53a77e0":"model_conv1d_ptb.load_weights(\"conv1d_ptb.h5\")\nmodel_conv1d_ptb.evaluate(x_test_ptb,y_test_ptb)","265b2d3c":"\ncheckpoint_cb = ModelCheckpoint(\"conv1d_ptb.h5\", save_best_only=True)\nearlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\nmodel_conv1d_ptb = summary_model()\n\n# checkpoint_cb = ModelCheckpoint(\"conv1d_ptb.h5\", save_best_only=True)\n# earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n# model_conv1d_ptb= build_conv1d_model(input_shape=(x_train_ptb.shape[1], x_train_ptb.shape[2]))\n# model_conv1d_ptb.summary()","6c0063ac":"df_with_peaks(data_train[:2000])\n","3f6b1d01":"tf.keras.backend.clear_session()\n\n#Function to build Convolutional 1D Networks\n# def build_conv1d_model_old (input_shape=(x_train_ptb.shape[1],1)):\n#     model = keras.models.Sequential()\n    \n#     model.add(Conv1D(32,7, padding='same', input_shape=input_shape))\n#     model.add(BatchNormalization())\n#     model.add(tf.keras.layers.ReLU())\n#     model.add(MaxPool1D(5,padding='same'))\n\n#     model.add(Conv1D(64,7, padding='same'))\n#     model.add(BatchNormalization())\n#     model.add(tf.keras.layers.ReLU())\n#     model.add(MaxPool1D(5,padding='same'))\n\n#     model.add(Conv1D(128,7, padding='same', input_shape=input_shape))\n#     model.add(BatchNormalization())\n#     model.add(tf.keras.layers.ReLU())\n#     model.add(MaxPool1D(5,padding='same'))\n\n#     model.add(Conv1D(256,7, padding='same'))\n#     model.add(BatchNormalization())\n#     model.add(tf.keras.layers.ReLU())\n#     model.add(MaxPool1D(5,padding='same'))\n\n#     model.add(Conv1D(512,7, padding='same', input_shape=input_shape))\n#     model.add(BatchNormalization())\n#     model.add(tf.keras.layers.ReLU())\n#     model.add(MaxPool1D(5,padding='same'))\n\n#     model.add(Flatten())\n#     model.add(Dense(512, activation='relu'))\n#     model.add(Dropout(0.5))\n#     model.add(Dense(256, activation='relu'))\n#     model.add(Dropout(0.5))\n#     model.add(Dense(128, activation='relu'))\n#     model.add(Dense(64, activation='relu'))\n#     model.add(Dense(32, activation='relu'))\n#     model.add(Dense(2, activation=\"softmax\"))\n#     model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tfa.metrics.F1Score(2,\"micro\")])\n#     return model\ndef build_conv1d_model(input_shape):\n    model = keras.models.Sequential()\n    \n    model.add(Conv1D(32,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(64,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(128,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(256,7, padding='same'))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Conv1D(512,7, padding='same', input_shape=input_shape))\n    model.add(BatchNormalization())\n    model.add(tf.keras.layers.ReLU())\n    model.add(MaxPool1D(5,padding='same'))\n\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(2, activation=\"softmax\"))\n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tfa.metrics.F1Score(2,\"micro\")])\n    return model","e0875614":"# checkpoint_cb = ModelCheckpoint(\"conv1d_ptb.h5\", save_best_only=True)\n\n# earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n\n# model_conv1d_ptb= build_conv1d_model(input_shape=(x_train_ptb.shape[1], x_train_ptb.shape[2]))\n# model_conv1d_ptb.summary()\ndef summary_model(x_train_ptb):\n    checkpoint_cb = ModelCheckpoint(\"conv1d_ptb.h5\", save_best_only=True)\n\n    earlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n\n    model_conv1d_ptb= build_conv1d_model(input_shape=(x_train_ptb.shape[1], x_train_ptb.shape[2]))\n    return model_conv1d_ptb.summary()\n    ","ad72577f":"def history_model(model_conv1d_ptb, checkpoint_cb, earlystop_cb,\n                  x_train_ptb, y_train_ptb, x_valid_ptb, y_valid_ptb):\n    history_conv1d_ptb = model_conv1d_ptb.fit(x_train_ptb, y_train_ptb,\n                                                  epochs=1, batch_size=32, \n                                                  steps_per_epoch = 100 ,\n                                                  validation_data=(x_valid_ptb, y_valid_ptb), \n                                              validation_steps = 50,\n                                                  callbacks=[checkpoint_cb, earlystop_cb]\n                                             )\n    return history_conv1d_ptb","7d743e64":"# for i in list(range(2)):\n#     history = model.fit(training_set_1,epochs=1)\n#     history = model.fit(training_set_2,epochs=1)\nhistory_conv1d_ptb = model_conv1d_ptb.fit(x_train_ptb, y_train_ptb,\n                                              epochs=10, batch_size=32, \n                                              steps_per_epoch = 1000 ,\n    #                                           class_weight=class_weight, \n                                              validation_data=(x_valid_ptb, y_valid_ptb), \n                                          validation_steps = 500,\n                                              callbacks=[checkpoint_cb, earlystop_cb])\n#     history_conv1d_ptb = model_conv1d_ptb.fit(x_train_ptb[100000:200000], y_train_ptb[100000:200000],\n#                                               epochs=1, batch_size=32, \n# #                                           class_weight=class_weight, \n#                                           validation_data=(x_valid_ptb[100000:200000], y_valid_ptb[100000:200000]),  \n#                                           callbacks=[checkpoint_cb, earlystop_cb])","5ca49f04":"# history_conv1d_ptb = model_conv1d_ptb.fit(x_train_ptb, y_train_ptb, epochs=40, batch_size=32, \n# #                                           class_weight=class_weight, \n#                                           validation_data=(x_valid_ptb, y_valid_ptb),  \n#                                           callbacks=[checkpoint_cb, earlystop_cb])","fcad8969":"data_test","0ab7a71c":"def evaluate_model(x_train_ptb, y_train_ptb, x_valid_ptb, y_valid_ptb,\n                   x_test_ptb, y_test_ptb):\n    model_conv1d_ptb = history_model(x_train_ptb, y_train_ptb, x_valid_ptb, y_valid_ptb)\n    model_conv1d_ptb.evaluate(x_test_ptb,y_test_ptb)","493e48cd":"model_conv1d_ptb.load_weights(\"conv1d_ptb.h5\")\nmodel_conv1d_ptb.evaluate(x_test_ptb,y_test_ptb)","58a7911e":"# Calculating the predictions based on the highest probability class\nconv1d_pred_proba_ptb = model_conv1d_ptb.predict (x_test_ptb)\nconv1d_pred_ptb = np.argmax(conv1d_pred_proba_ptb, axis=1)","5a337c4d":"print(classification_report(out_test_ptb, conv1d_pred_ptb > 0.5, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","845d0e29":"print(roc_auc_score(conv1d_pred_proba_ptb, out_test_ptb))\nprint(balanced_accuracy_score(conv1d_pred_proba_ptb, out_test_ptb))\nprint(f1_score(conv1d_pred_proba_ptb, out_test_ptb))","66e46d9a":"# Plotting the training and validatoin results\nplt.figure(figsize=(25,12))\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['loss'],\n           color='r', label='Train loss')\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['val_loss'],\n           color='b', label='Val loss' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['f1_score'],\n           color='g', label='Train F1')\nplt.plot(history_conv1d_ptb.epoch, history_conv1d_ptb.history['val_f1_score'],\n           color='c', label='Val F1' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","a6dbd9e4":"def build_conv1d_res_model (input_shape=(x_train_ptb.shape[1],1)):\n    model = keras.models.Sequential()\n    \n    input_ = tf.keras.layers.Input (shape=(input_shape))\n    \n    conv1_1 = Conv1D(64,7, padding='same', input_shape=input_shape) (input_)\n    conv1_1 = BatchNormalization() (conv1_1)\n    conv1_1 = tf.keras.layers.ReLU() (conv1_1)\n\n    conv1_2 = Conv1D(64,7, padding='same') (conv1_1)\n    conv1_2 = BatchNormalization() (conv1_2)\n    conv1_2 = tf.keras.layers.ReLU() (conv1_2)\n   \n    conv1_3 = Conv1D(64,7, padding='same') (conv1_2)\n    conv1_3 = BatchNormalization() (conv1_3)\n    conv1_3 = tf.keras.layers.ReLU() (conv1_3)\n\n    concat_1 = Concatenate()([conv1_1 , conv1_3 ])\n    max_1 = MaxPool1D(5, padding=\"same\") (concat_1)\n    \n    conv1_4 = Conv1D(128,7, padding='same') (max_1)\n    conv1_4 = BatchNormalization() (conv1_4)\n    conv1_4 = tf.keras.layers.ReLU() (conv1_4)\n\n    conv1_5 = Conv1D(128,7, padding='same', input_shape=input_shape) (conv1_4)\n    conv1_5 = BatchNormalization() (conv1_5)\n    conv1_5 = tf.keras.layers.ReLU() (conv1_5)\n    \n    conv1_6 = Conv1D(128,7, padding='same', input_shape=input_shape) (conv1_5)\n    conv1_6 = BatchNormalization() (conv1_6)\n    conv1_6 = tf.keras.layers.ReLU() (conv1_6)\n\n    concat_2 = Concatenate()([conv1_4, conv1_6])\n    max_2 = MaxPool1D(5, padding=\"same\") (concat_2)\n\n    flat = Flatten() (max_2)\n    dense_1 = Dense(512, activation='relu') (flat)\n    drop_1 = Dropout(0.5) (dense_1)\n    dense_2 = Dense(256, activation='relu') (drop_1)\n    drop_2 = Dropout(0.5) (dense_2)\n    dense_3 = Dense(128, activation='relu') (drop_2)\n    dense_4 = Dense(64, activation='relu') (dense_3)\n    dense_5 = Dense(32, activation='relu') (dense_4)\n    dense_6 = Dense(2, activation=\"softmax\") (dense_5)\n    \n    model = Model (inputs=input_ , outputs=dense_6)\n    \n    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tfa.metrics.F1Score(2,\"micro\")])\n    return model","cac813b5":"checkpoint_cb = ModelCheckpoint(\"conv1d_res_ptb.h5\", save_best_only=True)\n\nearlystop_cb = EarlyStopping(patience=5, restore_best_weights=True)\n\ninp_shape = (x_train_ptb.shape[1], x_train_ptb.shape[2])\nmodel_conv1d_res_ptb= build_conv1d_res_model(input_shape=(x_train_ptb.shape[1], x_train_ptb.shape[2]))\n#model_conv1d_res_ptb.build(inp_shape)","9a0f6361":"history_conv1d_res_ptb = model_conv1d_res_ptb.fit(x_train_ptb, y_train_ptb,\n                                              epochs=10, batch_size=32, \n                                              steps_per_epoch = 1000 ,\n    #                                           class_weight=class_weight, \n                                              validation_data=(x_valid_ptb, y_valid_ptb), \n                                          validation_steps = 500,\n                                              callbacks=[checkpoint_cb, earlystop_cb])","7a62ba4f":"# history_conv1d_res_ptb = model_conv1d_res_ptb.fit(x_train_ptb, y_train_ptb, epochs=40, batch_size=32, \n#                                           class_weight=class_weight, validation_data=(x_valid_ptb, y_valid_ptb),  \n#                                           callbacks=[checkpoint_cb, earlystop_cb])","add15405":"model_conv1d_res_ptb.load_weights(\"conv1d_res_ptb.h5\")\nmodel_conv1d_res_ptb.evaluate(x_test_ptb,y_test_ptb)","d630f48c":"# Calculating the predictions based on the highest probability class\nconv1d_res_pred_proba_ptb = model_conv1d_res_ptb.predict (x_test_ptb)\nconv1d_res_pred_ptb = np.argmax(conv1d_res_pred_proba_ptb, axis=1)","88c43b69":"print(classification_report(out_test_ptb, conv1d_res_pred_ptb > 0.5, target_names=[PTB_Outcome[i] for i in PTB_Outcome]))","8beb384f":"print(roc_auc_score(conv1d_res_pred_ptb, out_test_ptb))\nprint(balanced_accuracy_score(conv1d_res_pred_ptb, out_test_ptb))\nprint(f1_score(conv1d_res_pred_ptb, out_test_ptb))","c53ef9f8":"# Plotting the training and validatoin results\nplt.figure(figsize=(25,12))\nplt.plot(history_conv1d_res_ptb.epoch, history_conv1d_res_ptb.history['loss'],\n           color='r', label='Train loss')\nplt.plot(history_conv1d_res_ptb.epoch, history_conv1d_res_ptb.history['val_loss'],\n           color='b', label='Val loss' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(history_conv1d_res_ptb.epoch, history_conv1d_res_ptb.history['f1_score'],\n           color='g', label='Train F1')\nplt.plot(history_conv1d_res_ptb.epoch, history_conv1d_res_ptb.history['val_f1_score'],\n           color='c', label='Val F1' , linestyle=\"--\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","0dfcf63f":"## 5.1 \u0414\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c \u0441 \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u044b\u043c \u042d\u041a\u0413 \u043f\u043e \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0443 \u0438 \u043d\u043e\u043c\u0435\u0440\u0443 ecg_id","744b4a7a":"# 2. \u0432\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435 \u0444\u0443\u043d\u043a\u0446\u0438\u0438\n## 2.1 \u043a\u043b\u0430\u0441\u0441 \u0434\u0435\u0442\u0435\u043a\u0442\u043e\u0440\u043e\u0432 \u0434\u043b\u044f \u043d\u0430\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u044f \u043f\u0438\u043a","096372bd":"# Loading the data and exploring its shape and values\n\n","3409db56":"## 5.3 \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0441 \u043d\u0435\u043e\u0442\u0444\u0438\u043b\u044c\u0442\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u0438 \u043f\u0438\u043a\u0430\u043c\u0438 \u043f\u043e\u043a\u0430\u043d\u0430\u043b\u044c\u043d\u043e \u043f\u043e \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0443 \u0438 \u042d\u041a\u0413","74a1ba51":"## 4.1 \u0422\u0430\u0440\u0433\u0435\u0442 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 - 1","da4a585a":"## 2.2 \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0432\u044b\u0440\u0430\u043d\u043d\u043e\u0433\u043e \u0434\u0435\u0442\u0435\u043a\u0442\u043e\u0440\u0430","079e4796":"## Defining Conv1D Residual model for PTB\n\nCreating a model based on a series of Conv1D layers with 2 residual blocks that are connected to another series of full connected dense layers","0bb22cb0":"# 4. \u0412\u044b\u0431\u043e\u0440 \u0442\u0430\u0440\u0433\u0435\u0442\u0430","e93504f4":"## 4.2 \u0422\u0430\u0440\u0433\u0435\u0442 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 - 2 \u043f\u043e \u043a\u043e\u043d\u043a\u0440\u0435\u0442\u043d\u043e\u043c\u0443 \u042d\u041a\u0413","148f9673":"## Defining Conv1D model for PTB\n\nCreating a model based on a series of Conv1D layers that are connected to another series of full connected dense layers","6cc24d84":"## 5.2 \u0424\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0441 \u043f\u0438\u043a\u0430\u043c\u0438 \u043f\u043e\u043a\u0430\u043d\u0430\u043b\u044c\u043d\u043e \u043f\u043e \u0443\u043a\u0430\u0437\u0430\u043d\u043d\u044b\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0443 \u0438 \u042d\u041a\u0413","1a3f824a":"# 1. \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a\u0438","287acf61":"# 3. \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043b\u0435\u043d\u043d\u044b\u0445 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u0432","1e76e4e8":"# data_train.shape[1]"}}