{"cell_type":{"0232a6d4":"code","21252c13":"code","5416c573":"code","89408093":"code","aa417bf4":"code","d8d684f0":"code","a9e977c9":"code","b1947890":"code","bf68bef3":"code","a99961af":"code","490e1cb0":"code","7f8c0126":"code","378eba22":"code","db6e6169":"code","d2a6b3c8":"code","165e4538":"markdown","6009984a":"markdown","94c8a2fc":"markdown","e1da8bba":"markdown","811d3ec7":"markdown","62428774":"markdown"},"source":{"0232a6d4":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport torch \nimport torch.nn as nn \nimport torch.nn.functional as F \nimport torch.optim as optim \nfrom torch.distributions import Categorical\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(device)\n\nimport os, gc, random\nif device == 'cuda':\n    import cudf\n    import cupy as cp\nimport pandas as pd\nimport numpy as np\nimport janestreet\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom joblib import dump, load","21252c13":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","5416c573":"# print('Loading...')\n# if device == 'cuda':\n#     train = cudf.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\n# else:\n#     train = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\ntrain = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv', nrows = 3)\nfeatures = [c for c in train.columns if 'feature' in c]\n\n# print('Filling...')\n# f_mean = train[features[1:]].mean()\n# f_std = train[features[1:]].std()\n# train = train.query('weight > 0').reset_index(drop = True)\n# train[features[1:]] = train[features[1:]].fillna(f_mean)\n# train[features[1:]] = (train[features[1:]] - f_mean) \/ f_std\n# train['action'] = (train['resp'] > 0).astype('int')\n\n# print('Converting...')\n# if device == 'cuda':\n#     train = train.to_pandas()\n#     f_mean = f_mean.values.get()\n#     f_std = f_std.values.get()\n# else:\n#     f_mean = f_mean.values\n#     f_std = f_std.values\n# np.save('f_mean.npy', f_mean)\n# np.save('f_std.npy', f_std)\n\n# print('Finish.')","89408093":"# def utility_score(date, weight, resp, action):\n#     count_i = len(np.unique(date))\n#     Pi = np.bincount(date, weight * resp * action)\n#     t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n#     u = np.clip(t, 0, 6) * np.sum(Pi)\n#     return u\n\n# def utility_score_pd(date, weight, resp, action):\n#     count_i = len(pd.unique(date))\n#     Pi = np.bincount(date, weight * resp * action)\n#     t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n#     u = np.clip(t, 0, 6) * np.sum(Pi)\n#     return u\n\n# def utility_score_max(date, weight, resp, action):\n#     count_i = date.max() + 1\n#     Pi = np.bincount(date, weight * resp * action)\n#     t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n#     u = np.clip(t, 0, 6) * np.sum(Pi)\n#     return u\n\n# def utility_score_last(date, weight, resp, action):\n#     count_i = date[-1] + 1\n#     Pi = np.bincount(date, weight * resp * action)\n#     t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n#     u = np.clip(t, 0, 6) * np.sum(Pi)\n#     return u","aa417bf4":"# %timeit utility_score(train['date'].values, train['weight'].values, train['resp'].values, train['action'].values)\n# %timeit utility_score_pd(train['date'].values, train['weight'].values, train['resp'].values, train['action'].values)\n# %timeit utility_score_max(train['date'].values, train['weight'].values, train['resp'].values, train['action'].values)\n# %timeit utility_score_last(train['date'].values, train['weight'].values, train['resp'].values, train['action'].values)","d8d684f0":"class JSEnv:\n    \n    def __init__(self, df, feats):\n\n        self.n_samples = df.shape[0]\n        self.weight = torch.FloatTensor(df['weight'].values)\n        self.resp = torch.FloatTensor(df['resp'].values)\n        self.states = torch.FloatTensor(df[feats].values)\n        self.observation_space = df[feats].shape[1]\n        self.action_space = 2\n        self.idx = 0\n    \n    def reset(self):\n        self.idx = 0\n        return self.states[self.idx].view(1, -1)\n    \n    def step(self, action):\n        reward = self.weight[self.idx] * self.resp[self.idx] * action\n        self.idx += 1\n        if self.idx >= self.n_samples:\n            done = True\n            self.idx = 0\n        else:\n            done = False\n        info = 0\n        return self.states[self.idx].view(1, -1), reward, done, info","a9e977c9":"import numpy as np \nimport gym \nimport torch \nimport random\nfrom argparse import ArgumentParser \nimport os \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nfrom scipy.ndimage.filters import gaussian_filter1d\nfrom IPython.display import display, clear_output\n\ndef save(agent, rewards): \n\n    torch.save(agent.q.state_dict(), 'Net.pt')\n\n#     plt.gca().cla()\n#     plt.plot(rewards, c = 'r', alpha = 0.3)\n#     plt.plot(gaussian_filter1d(rewards, sigma = 5), c = 'r', label = 'Rewards')\n#     plt.xlabel('Frames x 1000')\n#     plt.ylabel('Cumulative reward')\n#     plt.title('Dueling DDQN: JS')\n#     plt.legend()\n#     plt.savefig('reward.png')\n#     plt.show()\n    \n#     clear_output(wait = True)\n#     plt.pause(0.5)\n\n#     pd.DataFrame(rewards, columns = ['Reward']).to_csv('rewards.csv', index = False)\n\nclass AgentConfig:\n\n    def __init__(self, \n                 epsilon_start = 1.,\n                 epsilon_final = 0.01,\n                 epsilon_decay = 8000,\n                 gamma = 0.99, \n                 lr = 1e-4, \n                 target_net_update_freq = 1000, \n                 memory_size = 100000, \n                 batch_size = 128, \n                 learning_starts = 5000,\n                 max_frames = 10000000): \n\n        self.epsilon_start = epsilon_start\n        self.epsilon_final = epsilon_final\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_by_frame = lambda i: self.epsilon_final + (self.epsilon_start - self.epsilon_final) * np.exp(-1. * i \/ self.epsilon_decay)\n\n        self.gamma =gamma\n        self.lr =lr\n\n        self.target_net_update_freq =target_net_update_freq\n        self.memory_size =memory_size\n        self.batch_size =batch_size\n\n        self.learning_starts = learning_starts\n        self.max_frames = max_frames\n\nclass ExperienceReplayMemory:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n\n    def push(self, transition):\n        self.memory.append(transition)\n        if len(self.memory) > self.capacity:\n            del self.memory[0]\n\n    def sample(self, batch_size):\n        \n        batch = random.sample(self.memory, batch_size)\n        states = []\n        actions = []\n        rewards = []\n        next_states = [] \n        dones = []\n\n        for b in batch: \n            states.append(b[0])\n            actions.append(b[1])\n            rewards.append(b[2])\n            next_states.append(b[3])\n            dones.append(b[4])\n\n        return states, actions, rewards, next_states, dones\n\n    def __len__(self):\n        return len(self.memory)","b1947890":"import torch \nimport torch.nn as nn \nimport torch.nn.functional as F \nimport torch.optim as optim \nfrom torch.distributions import Categorical \n\nclass DuelingNetwork(nn.Module): \n\n    def __init__(self, obs, ac): \n\n        super().__init__()\n\n        self.model = nn.Sequential(nn.utils.weight_norm(nn.Linear(obs, 512)),\n                                   nn.ReLU(), \n                                   nn.utils.weight_norm(nn.Linear(512, 256)),\n                                   nn.ReLU(),\n                                  )\n\n        self.value_head = nn.utils.weight_norm(nn.Linear(256, 1))\n        self.adv_head = nn.utils.weight_norm(nn.Linear(256, ac))\n\n    def forward(self, x): \n\n        out = self.model(x)\n\n        value = self.value_head(out)\n        adv = self.adv_head(out)\n\n        q_val = value + adv - adv.mean(1).reshape(-1,1)\n        return q_val","bf68bef3":"from tqdm.notebook import tqdm\nimport torch \nimport torch.nn as nn \nimport torch.nn.functional as F \nimport torch.optim as optim \nfrom torch.distributions import Categorical \n\nimport numpy as np \nimport gym \nimport random \n\nclass DuelingDDQN(nn.Module): \n\n    def __init__(self, obs, ac, config): \n\n        super().__init__()\n\n        self.q = DuelingNetwork(obs, ac).to(device)\n        self.target = DuelingNetwork(obs, ac).to(device)\n\n        self.target.load_state_dict(self.q.state_dict())\n\n        self.target_net_update_freq = config.target_net_update_freq\n        self.update_counter = 0\n\n    def get_action(self, x):\n        \n        x = torch.FloatTensor(x).to(device)\n        with torch.no_grad(): \n            a = self.q(x).max(1)[1]\n\n        return a.item()\n\n    def update_policy(self, adam, memory, params): \n\n        b_states, b_actions, b_rewards, b_next_states, b_masks = memory.sample(params.batch_size)\n\n        states = torch.FloatTensor(b_states).to(device)\n        actions = torch.LongTensor(b_actions).reshape(-1,1).to(device)\n        rewards = torch.FloatTensor(b_rewards).reshape(-1,1).to(device)\n        next_states = torch.FloatTensor(b_next_states).to(device)\n        masks = torch.FloatTensor(b_masks).reshape(-1,1).to(device)\n\n        current_q_values = self.q(states).gather(1, actions)\n\n        # print(current_q_values[:5])\n\n        with torch.no_grad():\n\n            max_next_q_vals = self.target(next_states).max(1)[0].reshape(-1,1)\n            # max_next_q_vals = self.\n        expected_q_vals = rewards + max_next_q_vals*0.99*masks\n        # print(expected_q_vals[:5])\n        loss = F.mse_loss(expected_q_vals, current_q_values)\n\n        # input(loss)\n\n        # print('\\n'*5)\n        \n        adam.zero_grad()\n        loss.backward()\n\n        for p in self.q.parameters(): \n            p.grad.data.clamp_(-1.,1.)\n        adam.step()\n\n        self.update_counter += 1\n        if self.update_counter % self.target_net_update_freq == 0: \n            self.update_counter = 0 \n            self.target.load_state_dict(self.q.state_dict())","a99961af":"# env = JSEnv(train, features)       \n# config = AgentConfig(epsilon_start = 1.,\n#                      epsilon_final = 0.01,\n#                      epsilon_decay = 8000,\n#                      gamma = 0.99, \n#                      lr = 1e-4, \n#                      target_net_update_freq = 1000, \n#                      memory_size = env.n_samples \/\/ 100, \n#                      batch_size = 128, \n#                      learning_starts = 5000,\n#                      max_frames = env.n_samples)\n# memory = ExperienceReplayMemory(config.memory_size)\n# agent = DuelingDDQN(env.observation_space, env.action_space, config)\n# adam = optim.Adam(agent.q.parameters(), lr = config.lr) \n\n# s = env.reset()\n# ep_reward = 0. \n# recap = []\n# cum_rewards = []\n\n# p_bar = tqdm(total = config.max_frames)\n# for frame in range(config.max_frames):\n\n#     epsilon = config.epsilon_by_frame(frame)\n\n#     if np.random.random() > epsilon: \n#         action = agent.get_action(s)\n#     else: \n#         action = np.random.randint(0, env.action_space)\n\n#     ns, r, done, infos = env.step(action)\n#     ep_reward += r \n#     if done:\n#         ns = env.reset()\n#         recap.append(ep_reward)\n#         p_bar.set_description('Rew: {:.3f}'.format(ep_reward))\n#         ep_reward = 0.\n\n#     memory.push((s.reshape(-1).numpy().tolist(), action, r, ns.reshape(-1).numpy().tolist(), 0. if done else 1.))\n#     s = ns  \n\n#     p_bar.update(1)\n\n#     if frame > config.learning_starts:\n#         agent.update_policy(adam, memory, config)\n\n#     if frame % 1000 == 0:\n#         print(f'{frame + 1}\/{config.max_frames}:', ep_reward.item(), end = '\\r')\n# #         cum_rewards.append(ep_reward.item())\n#         save(agent, cum_rewards)\n\n# p_bar.close()","490e1cb0":"# checkpoint_path = '.\/Net.pt'\ncheckpoint_path = '..\/input\/js-dqn\/Net.pt'\n\nmodel = DuelingNetwork(len(features), 2).to(device)\nmodel.load_state_dict(torch.load(checkpoint_path, map_location = device))\nmodel.eval()","7f8c0126":"# f_mean = np.load('.\/f_mean.npy')\n# f_std = np.load('.\/f_std.npy')\n\nf_mean = np.load('..\/input\/js-dqn\/f_mean.npy')\nf_std = np.load('..\/input\/js-dqn\/f_std.npy')","378eba22":"test = pd.read_csv('..\/input\/jane-street-market-prediction\/example_test.csv')\ntest[features[1:]] = test[features[1:]].fillna(dict(zip(features[1:], f_mean)))\ntest[features[1:]] = (test[features[1:]] - f_mean) \/ f_std\npred = model(torch.FloatTensor(test[features].values).to(device)).detach().cpu().numpy()\nprint(pred)","db6e6169":"env = janestreet.make_env()\nenv_iter = env.iter_test()","d2a6b3c8":"for (test_df, pred_df) in tqdm(env_iter):\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        x_tt[:, 1:] = (x_tt[:, 1:] - f_mean) \/ f_std\n        x_tt = torch.FloatTensor(x_tt).to(device)\n        pred_df.action = model(x_tt).max(1)[1].cpu().numpy().item()\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","165e4538":"# Load Model","6009984a":"# Jane Street: Deep Reinforcement Learning Approach\n\nI try implementing a deep Q-Network to solve the prediction problem.","94c8a2fc":"# Preprocessing","e1da8bba":"# Submitting","811d3ec7":"# Train Agent","62428774":"# DQN Model Functions\n\nmodified from https:\/\/github.com\/MoMe36\/DuelingDDQN"}}