{"cell_type":{"47e6d114":"code","d78384b8":"code","ab8d84c9":"code","a67c3ed4":"code","417e9f86":"code","96b0fde7":"code","6a1e8d59":"code","2e67b543":"code","89dd4df6":"code","9cb83001":"code","53963944":"code","95b3abef":"code","5e231b56":"code","8f765007":"code","c43058cd":"code","84225fe4":"code","3393d04b":"code","a92b928f":"code","b86d2200":"code","c7e4f7ed":"code","4aad282e":"code","5b3f8200":"code","7379d54d":"code","fd5a498f":"code","0518fd24":"code","55ab1451":"code","44d970d4":"code","5ac08ef1":"code","4bab2e68":"markdown","a98afeed":"markdown","ae2e7385":"markdown","b453693d":"markdown","ef5fe794":"markdown","51d68fcb":"markdown","435cabb7":"markdown","6fbf2b8e":"markdown","15dcdc04":"markdown","53248b27":"markdown","48d2bbf1":"markdown","507f67eb":"markdown","556bcf33":"markdown","e84765c8":"markdown"},"source":{"47e6d114":"import numpy as np\nimport pandas as pd\nfrom os.path import join as opj\nfrom tqdm.notebook import tqdm","d78384b8":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nsns.set_context('notebook')","ab8d84c9":"from sklearn.model_selection import ParameterGrid, KFold\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\ntf.__version__","a67c3ed4":"path = '\/kaggle\/input\/trends-feature-exploration-engineering\/datasets'","417e9f86":"# Load target values and corresponding scaler\nimport joblib\nscaler_targets = joblib.load(opj(path, 'targets_scaler.pkl'))\n\ntargets = pd.read_hdf(opj(path, 'targets.h5'))\ntargets.head()","96b0fde7":"def load_dataset_and_scale(dataset_id='merge', scale_values=[1, 1, 1, 1]):\n\n    # Load dataset\n    X_tr = pd.read_hdf(opj(path, '%s_train.h5' % dataset_id))\n    X_te = pd.read_hdf(opj(path, '%s_test.h5' % dataset_id))\n\n    #\u00a0Specify target\n    y = pd.read_hdf(opj(path, 'targets.h5'))\n    y_tr = y.loc[X_tr.index, 'domain2_var1':]\n\n    # Remove missing values\n    missval = y_tr.isnull().sum(axis=1)!=0\n    idx = ~missval\n    X_tr = X_tr[idx]\n    X_te = X_te\n    y_tr = y_tr[idx]\n    print('Removing %s missing values from target dataset.' % missval.sum())\n    \n    # Centralize corr_coef features\n    median_offset = X_tr.iloc[:, X_tr.columns.str.contains('corr_coef')].median().mean()\n    X_tr.iloc[:, X_tr.columns.str.contains('corr_coef')] -= median_offset\n    X_te.iloc[:, X_te.columns.str.contains('corr_coef')] -= median_offset\n\n    # Establish masks for different kinds of data\n    mask_ids = []\n    for m in ['IC_', '_vs_', 'corr_coef', '^c[0-9]+_c[0-9]+']:\n        mask_ids.append(X_tr.columns.str.contains(m))\n    mask_ids = np.array(mask_ids)\n\n    # Data scaling\n    for i, m in enumerate(mask_ids):\n\n        if m.sum()==0:\n            continue\n        \n        # Apply Scale\n        scale_value = scale_values[i]\n        unify_mask_scale = np.percentile(X_tr.iloc[:, m].abs(), 90)\n\n        X_te.iloc[:, m] \/= unify_mask_scale\n        X_tr.iloc[:, m] \/= unify_mask_scale\n    \n        X_te.iloc[:, m] *= scale_value\n        X_tr.iloc[:, m] *= scale_value\n\n    # Drop irrelevant measurements\n    X_tr.dropna(axis=1, inplace=True)\n    X_te.dropna(axis=1, inplace=True)\n    \n    # Drop duplicate rows\n    X_tr = X_tr.T.drop_duplicates().T\n    X_te = X_te.T.drop_duplicates().T\n    \n    print('Size of dataset (train\/test): ', X_tr.shape, X_te.shape)\n    \n    X_tr = X_tr.values\n    X_te = X_te.values\n    y_tr = y_tr.values\n    \n    return X_tr, X_te, y_tr","6a1e8d59":"# Load the data\nX_tr, X_te, y_tr = load_dataset_and_scale(dataset_id='merge', scale_values=[1, 1, 1, 1])","2e67b543":"# Revert target transformation to original\ny_tr_orig = y_tr*scaler_targets.scale_[3:]+scaler_targets.mean_[3:]\ny_tr_orig[:, :3] = np.power(y_tr_orig[:, :3], 1.\/1.5)","89dd4df6":"# Plot domain2 relationship with and without rotation\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n\nax[0].set_title('Domain2 relationship (orig)')\nax[0].scatter(y_tr_orig[:, 0], y_tr_orig[:, 1], s=1, alpha=0.5, c='b')\nax[0].axis('equal')\n\nax[1].set_title('Domain2 relationship (rotated)')\nax[1].scatter(y_tr_orig[:, 2], y_tr_orig[:, 3], s=1, alpha=0.5, c='r')\nax[1].axis('equal')\n\nplt.tight_layout()\nplt.show();","9cb83001":"class CustomLoss_focus(keras.losses.Loss):\n    def __init__(self, scale=None, mean=None, focus=0, name=\"loss\"):\n        super().__init__(name=name + '_%d' % (focus + 1))\n        self.scale = np.array(scale, dtype='float32')\n        self.mean = np.array(mean, dtype='float32')\n        self.focus = np.array(focus, dtype='int32')\n\n    @tf.function\n    def call(self, y_true, y_pred):\n\n        #\u00a0Rescale values\n        y_true = tf.math.add(tf.math.multiply(self.scale, y_true), self.mean)\n        y_pred = tf.math.add(tf.math.multiply(self.scale, y_pred), self.mean)\n\n        # Revert power transformation\n        y_true = tf.transpose(tf.stack([\n            tf.math.pow(y_true[:, 0], 1.\/1.5),\n            tf.math.pow(y_true[:, 1], 1.\/1.5)]))\n        y_pred = tf.transpose(tf.stack([\n            tf.math.pow(y_pred[:, 0], 1.\/1.5),\n            tf.math.pow(y_pred[:, 1], 1.\/1.5)]))\n\n        # Appli competition loss function\n        scores = tf.math.divide(tf.math.reduce_sum(tf.math.abs(y_true - y_pred), axis=0),\n                                tf.math.reduce_sum(tf.math.abs(y_true), axis=0))\n        \n        # Focus on var1 or var2\n        score = scores[self.focus]\n\n        return score","53963944":"def build_model(input_size=100,\n                activation='relu',\n                dropout=0.5,\n                kernel_initializer='glorot_uniform',\n                lr=1e-2,\n                use_batch=True,\n                use_dropout=True,\n                hidden=(32, 16),\n                hinge=0.5,\n               ):\n\n    # Specify layer structure\n    inputs = keras.Input(shape=input_size)\n    \n    for i, h in enumerate(hidden):\n        if i==0:\n            x = layers.Dense(h, kernel_initializer=kernel_initializer)(inputs)\n        else:\n            x = layers.Dense(h, kernel_initializer=kernel_initializer)(x)\n        if use_batch:\n            x = layers.BatchNormalization()(x)\n        \n        x = layers.Activation(activation)(x)\n\n        #\u00a0Add Dropout layer if requested\n        if use_dropout:\n            x = layers.Dropout(dropout)(x)\n\n    out = layers.Dense(2)(x)\n    out1 = layers.Activation(None, name=\"loss1\")(out)\n    out2 = layers.Activation(None, name=\"loss2\")(out)\n\n    # Create Model\n    model = keras.Model(inputs, [out1, out2])\n\n    # Define loss function, optimizer and metrics to track during training\n    optimizer = keras.optimizers.Adagrad(\n            learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n                lr, decay_steps=10000, decay_rate=0.96, staircase=True))\n    \n    l1 = CustomLoss_focus(scale=scaler_targets.scale_[3:5],\n                          mean=scaler_targets.mean_[3:5],\n                          focus=0)\n    l2 = CustomLoss_focus(scale=scaler_targets.scale_[3:5],\n                          mean=scaler_targets.mean_[3:5],\n                          focus=1)\n    \n    losses = {'loss1': l1, 'loss2': l2}\n    lossWeights = {\"loss1\": hinge, \"loss2\": 1-hinge}\n\n    model.compile(\n        optimizer = optimizer,\n        loss = losses,\n        loss_weights = lossWeights\n    )\n    return model","95b3abef":"def create_grid(input_size=0):\n    \n    # Define parameter grid\n    param_grid = [\n    { \n        'nn__input_size': [input_size],\n        'nn__lr': [0.05],\n        'nn__activation': ['sigmoid'],\n        'nn__hidden': [(128, 32),],\n        'nn__dropout': [0.25],\n        'nn__kernel_initializer': ['glorot_uniform'], #['glorot_uniform', 'normal', 'uniform'],\n        'nn__use_batch': [True],\n        'nn__use_dropout': [True],\n        'nn__hinge': [0., 0.5, 1.],\n        'batch_size': [32],\n        'epochs': [50],\n    }]\n    \n    #\u00a0Create Parameter Grid and return it\n    grids = ParameterGrid(param_grid)\n    return grids\n\nlen(create_grid())","5e231b56":"def run_grid(grid, X_tr, X_te, y_tr, cv=5, n=2):\n    \n    x_train = X_tr.copy()\n    x_test = X_te.copy()\n    print('Parameters:', grid)\n        \n    # Create callback object\n    callbacks = [keras.callbacks.EarlyStopping(\n        monitor=\"loss\", min_delta=1e-5, patience=5, verbose=1)]\n\n    # Build NN model\n    nn_grid ={}\n    for k in grid.keys():\n        if 'nn__' in k:\n            nn_grid[k[4:]] = grid[k]\n\n    # Run NN fit on multiple folds\n    history_runs = {'loss': [], 'val_loss': []}\n    for i in range(n):\n        history_runs['loss%d' % (i+1)] = []\n        history_runs['val_loss%d' % (i+1)] = []\n\n    kfold = KFold(n_splits=cv, shuffle=True)\n    \n    preds_cv_tr = []\n    preds_cv_te = []\n    counter = 0\n    for train_idx, val_idx in kfold.split(x_train):\n        \n        # Prepare data for split\n        x_train_k = x_train[train_idx]\n        x_val_k = x_train[val_idx]\n\n        y_train_k = y_tr[train_idx]\n        y_val_k = y_tr[val_idx]\n\n        # Prepare model\n        model = build_model(**nn_grid)\n\n        #\u00a0Fit model\n        history = model.fit(\n            x_train_k,\n            y_train_k,\n            validation_data=(x_val_k, y_val_k),\n            epochs=grid['epochs'],\n            batch_size=grid['batch_size'],\n            #callbacks=callbacks,\n            shuffle=True, verbose=0\n        )\n        history_runs['loss'].append(history.history['loss'])\n        history_runs['val_loss'].append(history.history['val_loss'])\n        \n        for i in range(n):\n            history_runs['loss%d' % (i+1)].append(history.history['loss%d_loss' % (i+1)])\n            history_runs['val_loss%d' % (i+1)].append(history.history['val_loss%d_loss' % (i+1)])\n        \n        # Compute predictions\n        preds_cv_tr.append(model.predict(x_train)[0])\n        preds_cv_te.append(model.predict(x_test)[0])\n        \n        # Report counter\n        print('CV: %02d\/%02d' % (counter+1, cv))\n        counter += 1\n\n    # Compute mean and standard deviation\n    for l in [''] + [str(i+1) for i in range(n)]:\n        history_runs['loss'+l+'_mean'] = pd.DataFrame(history_runs['loss'+l+'']).T.mean(axis=1).values\n        history_runs['val_loss'+l+'_mean'] = pd.DataFrame(history_runs['val_loss'+l+'']).T.mean(axis=1).values\n        history_runs['loss'+l+'_std'] = pd.DataFrame(history_runs['loss'+l+'']).T.std(axis=1).values\n        history_runs['val_loss'+l+'_std'] = pd.DataFrame(history_runs['val_loss'+l+'']).T.std(axis=1).values\n    \n    # Collect mean and median predictions\n    df_pred_mean_tr = pd.DataFrame(np.mean([e for e in preds_cv_tr if not np.isnan(e[0, 0])], axis=0), columns=['domain21', 'domain22'])\n    df_pred_mean_te = pd.DataFrame(np.mean([e for e in preds_cv_te if not np.isnan(e[0, 0])], axis=0), columns=['domain21', 'domain22'])\n\n    df_pred_median_tr = pd.DataFrame(np.median([e for e in preds_cv_tr if not np.isnan(e[0, 0])], axis=0), columns=['domain21', 'domain22'])\n    df_pred_median_te = pd.DataFrame(np.median([e for e in preds_cv_te if not np.isnan(e[0, 0])], axis=0), columns=['domain21', 'domain22'])\n\n    #\u00a0Assign closest value from training set\n    for tid in range(2):\n        unique_values = np.unique(y_tr[:, tid])\n        for d in [df_pred_mean_tr, df_pred_mean_te, df_pred_median_tr, df_pred_median_te]:\n            for i in range(len(d)):\n                d.iloc[i, tid] = unique_values[np.argmin(np.abs(d.iloc[i, tid]-unique_values))]\n\n    return x_train, x_test, history_runs, model, df_pred_mean_tr, df_pred_mean_te, df_pred_median_tr, df_pred_median_te","8f765007":"def get_scores(y_tr, pred_tr, scaler=None, title=''):\n\n    scores = []\n    preds = []\n    trues = []\n    for i, tidx in enumerate([3, 4]):\n\n        # Invert scaler\n        t_true = scaler.inverse_transform(np.transpose([y_tr[:, i]] * 7))[:, tidx]\n        t_pred = scaler.inverse_transform(np.transpose([pred_tr.iloc[:, i]] * 7))[:, tidx]\n\n        # Invert power transformation\n        t_true = np.power(t_true, 1.\/1.5)\n        t_pred = np.power(t_pred, 1.\/1.5)\n\n        # Compute the score\n        score = np.mean(np.sum(np.abs(t_true - t_pred), axis=0) \/ np.sum(t_true, axis=0))\n        scores.append(score)\n        preds.append(t_pred)\n        trues.append(t_true)\n\n    scores = np.array(scores)\n    preds = np.array(preds)\n    trues = np.array(trues)\n\n    pred_score = list(np.round(scores, 5))\n    plt.figure(figsize=(6, 6))\n    plt.title('Score on whole train set (var1, var2): ' + title + ' - ' + str(pred_score))\n    print('Score on whole train set (var1, var2) %s: ' % title, pred_score)\n    plt.scatter(trues[0], trues[1], s=0.5, alpha=0.5);\n    plt.scatter(preds[0], preds[1], s=0.5, alpha=0.5);\n    plt.axis('equal')\n\n    plt.show()","c43058cd":"def plot_history_and_predictions(history, pred_tr, y_tr, cutoff=0, n=2):\n\n    # Plot history results\n    for l in [''] + [str(i+1) for i in range(n)]:\n        fig = plt.figure(constrained_layout=True, figsize=(14, 3.5))\n        plt.plot(history['loss'+l+'_mean'][cutoff:], label='train loss'+l)\n        plt.plot(history['val_loss'+l+'_mean'][cutoff:], label='val loss'+l, linestyle='dotted')\n\n        for m in ['loss'+l, 'val_loss'+l]:\n            plt.fill_between(np.arange(len(history['%s_mean' % m][cutoff:])),\n                             history['%s_mean' % m][cutoff:]-history['%s_std' % m][cutoff:],\n                             history['%s_mean' % m][cutoff:]+history['%s_std' % m][cutoff:],\n                             alpha=0.3)\n        plt.title('Validation loss{}: {:.4f} (mean last 5)'.format(str(l),\n            np.mean(history['val_loss'+l+'_mean'][-5:])\n        ))\n        plt.xlabel('epoch')\n        plt.ylabel('loss'+l+' value')\n        plt.legend()\n        plt.show()\n\n    # Categories\n    categories = ['domain2_var1', 'domain2_var2']\n\n    # Create subplot grid\n    fig = plt.figure(constrained_layout=True, figsize=(9, 4))\n    gs = fig.add_gridspec(1, 2)\n    ax = [fig.add_subplot(gs[i]) for i in range(2)]\n\n    # Plot discrepancy on training set\n    for i, axt in enumerate(ax):\n        sns.regplot(x=pred_tr[0][i, :], y=y_tr[:, i], marker='.', ax=axt,\n                    scatter_kws={'alpha': 0.33, 's': 1})\n        sns.regplot(x=pred_tr[1][i, :], y=y_tr[:, i], marker='.', ax=axt,\n                    scatter_kws={'alpha': 0.33, 's': 1})\n        axt.set_title('Prediction X_tr: %s' % categories[i])\n        axt.set_xlim(-3, 3)\n        axt.set_ylim(-3, 3)\n        axt.legend(['Mean', 'Median'])\n        axt.set_aspect('equal')\n\n    plt.show()","84225fe4":"def get_scores(y_tr, pred_tr, scaler=None):\n\n    scores = []\n    preds = []\n    trues = []\n    for j, tidx in enumerate([3, 4]):\n\n        # Invert scaler\n        t_true = scaler.inverse_transform(np.transpose([y_tr[:, j]] * 7))[:, tidx]\n        t_pred = scaler.inverse_transform(np.transpose([pred_tr.iloc[:, j]] * 7))[:, tidx]\n\n        # Invert power transformation\n        t_true = np.power(t_true, 1.\/1.5)\n        t_pred = np.power(t_pred, 1.\/1.5)\n\n        # Compute the score\n        score = np.mean(np.sum(np.abs(t_true - t_pred), axis=0) \/ np.sum(t_true, axis=0))\n        scores.append(score)\n        preds.append(t_pred)\n        trues.append(t_true)\n\n    return np.array(scores), np.array(preds), np.array(trues)","3393d04b":"def plot_predictions_m(df_pred_mean_tr, df_pred_median_tr, y_tr, scaler=None):\n\n    # Categories\n    categories = ['mean', 'median']\n    preds_m = [df_pred_mean_tr, df_pred_median_tr]\n\n    # Create subplot grid\n    fig = plt.figure(constrained_layout=True, figsize=(11, 5))\n    gs = fig.add_gridspec(1, 2)\n    ax = [fig.add_subplot(gs[i]) for i in range(2)]\n\n    # Plot discrepancy on training set\n    for i, axt in enumerate(ax):\n        scores, preds, trues = get_scores(y_tr, preds_m[i], scaler=scaler)\n        pred_score = list(np.round(scores, 5))\n        axt.scatter(trues[0], trues[1], s=0.5, alpha=0.5);\n        axt.scatter(preds[0], preds[1], s=0.5, alpha=0.5);\n        axt.set_title('Score on whole train set (var1, var2):\\n' + categories[i] + ' - ' + str(pred_score))\n        axt.legend(['Target', 'Prediction'])\n        axt.set_aspect('equal')\n\n    plt.show()","a92b928f":"def extract_score_info(history, n=2):\n    tr_mean = np.round(np.mean(history['loss_mean'][-5:]), 6)\n    te_mean = np.round(np.mean(history['val_loss_mean'][-5:]), 6)\n\n    res = [tr_mean, te_mean]\n    \n    for i in range(n):\n        res.append(np.round(np.mean(history['loss%d_mean' % (i+1)][-5:]), 6))\n        res.append(np.round(np.mean(history['val_loss%d_mean' % (i+1)][-5:]), 6))\n\n    return res","b86d2200":"def run_prediction(dataset_id='merge', cv=5, n=2, scale_values=None):\n\n    # Extract dataset\n    X_tr, X_te, y_tr = load_dataset_and_scale(dataset_id, scale_values=scale_values)\n    \n    # Reduce target to number of outputs\n    y_tr = y_tr[:, :2]\n\n    # Create grid search object\n    grids = create_grid(input_size=X_tr.shape[1])\n    print('%0d grid points will be checked!' % len(grids))\n    print('\\n---Start_Grid_Exploration---\\n')\n \n    # Go through the grids\n    preds_tr = []\n    preds_te = []\n    df_grids = []\n    for gidx, grid in enumerate(tqdm(grids)):\n\n        #\u00a0Run grid\n        print('\\nGrid point: %04d\/%04d' % (gidx + 1, len(grids)))\n        x_train, x_test, history, model, df_pred_mean_tr, df_pred_mean_te, df_pred_median_tr, df_pred_median_te = run_grid(grid, X_tr, X_te, y_tr, cv=cv, n=n)\n\n        # Compute predictions\n        preds_tr.append([df_pred_mean_tr, df_pred_median_tr])\n        preds_te.append([df_pred_mean_te, df_pred_median_te])\n\n        #\u00a0Plot history results and predictions\n        plot_history_and_predictions(history, [df_pred_mean_tr.T.values,df_pred_median_tr.T.values], y_tr, cutoff=0, n=n)\n        plot_predictions_m(df_pred_mean_tr, df_pred_median_tr, y_tr, scaler=scaler_targets)\n        \n        # Extract scores\n        score_means = extract_score_info(history, n=n)\n\n        # Store everything in a grid\n        df_grid = pd.DataFrame(columns=grid.keys())\n        for k in grid:\n            df_grid.loc[0, k] = str(grid[k])\n        df_grid.insert(0, 'tr_mean', score_means[0])\n        df_grid.insert(1, 'te_mean', score_means[1])\n        for i in range(n):\n            df_grid.insert(2+(i*2), 'tr_mean%d' % (i+1), score_means[2+(i*2)])\n            df_grid.insert(3+(i*2), 'te_mean%d' % (i+1), score_means[3+(i*2)])\n        df_grids.append(df_grid)\n\n    # Summarize fit in datatable\n    df_fit = pd.concat(df_grids).reset_index().drop(columns='index')\n    display(df_fit)\n    \n    return preds_tr, preds_te, df_fit.iloc[0], df_grid, y_tr, history","c7e4f7ed":"# Define feature set scales (~average from my second notebook)\nscale_values = [0.25, 0.025, 0.04, 0.02]\n\n# Run model fit\npreds_tr, preds_te, top_grid, df_grid, y_tr, history = run_prediction(cv=8, n=2, scale_values=scale_values)\n\n#\u00a0Feedback of overall score\nprint('Best score at %s \/ %s' % (top_grid['te_mean'], top_grid['tr_mean']))","4aad282e":"class CustomLoss_dist(keras.losses.Loss):\n    def __init__(self, name=\"loss\"):\n        super().__init__(name=name)\n\n    @tf.function\n    def call(self, y_true, y_pred):\n\n        #\u00a0Compute distance to target\n        score = tf.math.pow(tf.reduce_mean(tf.math.pow(tf.norm(tf.math.subtract(y_true, y_pred), axis=1), 3.)), 1.\/3.)\n        \n        return score","5b3f8200":"class CustomLoss_dist_rot(keras.losses.Loss):\n    def __init__(self, scale=None, mean=None, name=\"loss_rot\"):\n        super().__init__(name=name)\n        self.scale = np.array(scale, dtype='float32')\n        self.mean = np.array(mean, dtype='float32')\n\n    @tf.function\n    def call(self, y_true, y_pred):\n\n        #\u00a0Rescale values\n        y_true = tf.math.add(tf.math.multiply(self.scale[:2], y_true), self.mean[:2])\n        y_pred = tf.math.add(tf.math.multiply(self.scale[:2], y_pred), self.mean[:2])\n\n        # Revert power transformation\n        y_true = tf.transpose(tf.stack([\n            tf.math.pow(y_true[:, 0], 1.\/1.5),\n            tf.math.pow(y_true[:, 1], 1.\/1.5)]))\n        y_pred = tf.transpose(tf.stack([\n            tf.math.pow(y_pred[:, 0], 1.\/1.5),\n            tf.math.pow(y_pred[:, 1], 1.\/1.5)]))\n\n        # Apply rotation for true and preds\n        d2_rot = 0.90771256655\n        radians = tf.constant(d2_rot, dtype='float32')\n        yt_rot = tf.transpose(tf.stack([\n            tf.math.add(tf.math.multiply(y_true[:, 0], [tf.cos(radians)]),\n                        tf.math.multiply(y_true[:, 1], [tf.sin(radians)])),\n            tf.math.add(-tf.math.multiply(y_true[:, 0], [tf.sin(radians)]),\n                        tf.math.multiply(y_true[:, 1], [tf.cos(radians)]))\n        ]))\n        yp_rot = tf.transpose(tf.stack([\n            tf.math.add(tf.math.multiply(y_pred[:, 0], [tf.cos(radians)]),\n                        tf.math.multiply(y_pred[:, 1], [tf.sin(radians)])),\n            tf.math.add(-tf.math.multiply(y_pred[:, 0], [tf.sin(radians)]),\n                        tf.math.multiply(y_pred[:, 1], [tf.cos(radians)]))\n        ]))        \n\n        #\u00a0Rescale values\n        yt_rot = tf.math.divide(tf.math.subtract(yt_rot, self.mean[2:]), self.scale[2:])\n        yp_rot = tf.math.divide(tf.math.subtract(yp_rot, self.mean[2:]), self.scale[2:])\n                \n        #\u00a0Compute distance to target\n        score_dist = tf.math.pow(tf.reduce_mean(tf.math.pow(tf.norm(tf.math.subtract(yt_rot, yp_rot), axis=1), 3.)), 1.\/3.)\n\n        return score_dist","7379d54d":"def build_model(input_size=100,\n                activation='relu',\n                dropout=0.5,\n                kernel_initializer='glorot_uniform',\n                lr=1e-2,\n                use_batch=True,\n                use_dropout=True,\n                hidden=(32, 16),\n                hinge_dist=0.5,\n               ):\n\n    # Specify layer structure\n    inputs = keras.Input(shape=input_size)\n    \n    for i, h in enumerate(hidden):\n        if i==0:\n            x = layers.Dense(h, kernel_initializer=kernel_initializer)(inputs)\n        else:\n            x = layers.Dense(h, kernel_initializer=kernel_initializer)(x)\n        if use_batch:\n            x = layers.BatchNormalization()(x)\n        \n        x = layers.Activation(activation)(x)\n\n        #\u00a0Add Dropout layer if requested\n        if use_dropout:\n            x = layers.Dropout(dropout)(x)\n\n    out = layers.Dense(2)(x)\n    out1 = layers.Activation(None, name=\"loss1\")(out)\n    out2 = layers.Activation(None, name=\"loss2\")(out)\n\n    # Create Model\n    model = keras.Model(inputs, [out1, out2])\n    \n    # Define loss function, optimizer and metrics to track during training\n    optimizer = keras.optimizers.Adagrad(\n            learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n                lr, decay_steps=10000, decay_rate=0.96, staircase=True))\n    \n    l1 = CustomLoss_dist()\n    l2 = CustomLoss_dist_rot(scale=scaler_targets.scale_[3:],\n                             mean=scaler_targets.mean_[3:])\n    \n    losses = {'loss1': l1, 'loss2': l2}\n    lossWeights = {\"loss1\": hinge_dist, \"loss2\": 1-hinge_dist}\n\n    model.compile(\n        optimizer = optimizer,\n        loss = losses,\n        loss_weights = lossWeights\n    )\n    return model","fd5a498f":"def create_grid(input_size=0):\n    \n    # Define parameter grid\n    param_grid = [\n    { \n        'nn__input_size': [input_size],\n        'nn__lr': [0.05],\n        'nn__activation': ['sigmoid'],\n        'nn__hidden': [(128, 32),],\n        'nn__dropout': [0.25],\n        'nn__kernel_initializer': ['glorot_uniform'], #['glorot_uniform', 'normal', 'uniform'],\n        'nn__use_batch': [True],\n        'nn__use_dropout': [True],\n        'nn__hinge_dist': [0., 0.5, 1.],\n        'batch_size': [32],\n        'epochs': [50],\n    }]\n    \n    #\u00a0Create Parameter Grid and return it\n    grids = ParameterGrid(param_grid)\n    return grids\n\nlen(create_grid())","0518fd24":"# Define feature set scales (~average from my second notebook)\nscale_values = [0.25, 0.025, 0.04, 0.02]\n\n# Run model fit\npreds_tr, preds_te, top_grid, df_grid, y_tr, history = run_prediction(cv=8, n=2, scale_values=scale_values)\n\n#\u00a0Feedback of overall score\nprint('Best score at %s \/ %s' % (top_grid['te_mean'], top_grid['tr_mean']))","55ab1451":"def build_model(input_size=100,\n                activation='relu',\n                dropout=0.5,\n                kernel_initializer='glorot_uniform',\n                lr=1e-2,\n                use_batch=True,\n                use_dropout=True,\n                hidden=(32, 16),\n                hinge=0.5,\n                hinge_dist=0.5,\n               ):\n\n    # Specify layer structure\n    inputs = keras.Input(shape=input_size)\n    \n    for i, h in enumerate(hidden):\n        if i==0:\n            x = layers.Dense(h, kernel_initializer=kernel_initializer)(inputs)\n        else:\n            x = layers.Dense(h, kernel_initializer=kernel_initializer)(x)\n        if use_batch:\n            x = layers.BatchNormalization()(x)\n        \n        x = layers.Activation(activation)(x)\n\n        #\u00a0Add Dropout layer if requested\n        if use_dropout:\n            x = layers.Dropout(dropout)(x)\n\n    out = layers.Dense(2)(x)\n    out1 = layers.Activation(None, name=\"loss1\")(out)\n    out2 = layers.Activation(None, name=\"loss2\")(out)\n    out3 = layers.Activation(None, name=\"loss3\")(out)\n    out4 = layers.Activation(None, name=\"loss4\")(out)\n\n    # Create Model\n    model = keras.Model(inputs, [out1, out2, out3, out4])\n\n    # Define loss function, optimizer and metrics to track during training\n    optimizer = keras.optimizers.Adagrad(\n            learning_rate=tf.keras.optimizers.schedules.ExponentialDecay(\n                lr, decay_steps=10000, decay_rate=0.96, staircase=True))\n    \n    l1 = CustomLoss_focus(scale=scaler_targets.scale_[3:5],\n                          mean=scaler_targets.mean_[3:5],\n                          focus=0)\n    l2 = CustomLoss_focus(scale=scaler_targets.scale_[3:5],\n                          mean=scaler_targets.mean_[3:5],\n                          focus=1)\n    l3 = CustomLoss_dist()\n    l4 = CustomLoss_dist_rot(scale=scaler_targets.scale_[3:],\n                             mean=scaler_targets.mean_[3:])\n    \n    losses = {'loss1': l1, 'loss2': l2, 'loss3': l3, 'loss4': l4}\n    adjustment_factor = 1.\/10.\n    lossWeights = {\"loss1\": hinge,\n                   \"loss2\": 1-hinge,\n                   \"loss3\": (hinge_dist)*adjustment_factor,\n                   \"loss4\": (1-hinge_dist)*adjustment_factor}\n\n    model.compile(\n        optimizer = optimizer,\n        loss = losses,\n        loss_weights = lossWeights\n    )\n    return model","44d970d4":"def create_grid(input_size=0):\n    \n    # Define parameter grid\n    param_grid = [\n    { \n        'nn__input_size': [input_size],\n        'nn__lr': [0.05],\n        'nn__activation': ['sigmoid'],\n        'nn__hidden': [(128, 32),],\n        'nn__dropout': [0.25],\n        'nn__kernel_initializer': ['glorot_uniform'], #['glorot_uniform', 'normal', 'uniform'],\n        'nn__use_batch': [True],\n        'nn__use_dropout': [True],\n        'nn__hinge': [0., 0.5, 1.],\n        'nn__hinge_dist': [0., 0.5, 1.],\n        'batch_size': [32],\n        'epochs': [50],\n    }]\n    \n    #\u00a0Create Parameter Grid and return it\n    grids = ParameterGrid(param_grid)\n    return grids\n\nlen(create_grid())","5ac08ef1":"# Define feature set scales (~average from my second notebook)\nscale_values = [0.25, 0.025, 0.04, 0.02]\n\n# Run model fit\npreds_tr, preds_te, top_grid, df_grid, y_tr, history = run_prediction(cv=8, n=4, scale_values=scale_values)\n\n#\u00a0Feedback of overall score\nprint('Best score at %s \/ %s' % (top_grid['te_mean'], top_grid['tr_mean']))","4bab2e68":"# 7. Dual `domain2` prediction based on distance metric\n\nThe thought here is the following. What we try to minmize is the distance between a predicted point and the target point, where x and y coordinates are the var1 and var2 values. The hope was, that by including the rotated distance metric as well, the prediction of the original values would improve. Because only if the prediction (i.e. the x and y coordinates) are close enough, the rotation makes sense.","a98afeed":"# 8. Dual `domain2` prediction based on MAE and distance metric\n\nThe hope was, by combining the MAE and distance metric loss functions, we can restrict the predictions to its optimal place. However, this didn't really seem to work.","ae2e7385":"##\u00a0Observation 3\n\nAlso here, while the `hinge=0.5` and `hinge_dist=0.5` approach leads to the best model fit, the predictions are not helpfull to improve the final score.","b453693d":"To refresh our memories about which `domain2` relationship we want to profit from, let's visualize it.","ef5fe794":"## Observation 1\n\nDepending on the focus (i.e. focus on `var1` or `var2`, the prediction is spreading into the direction of the component and ignoring the other. A simultaneous fitting doesn't seem to improve the prediction quality, it only helps to reduce standard deviation between the folds.\n\nAlso, the network clearly starts to overfit and the standard deviation between the different folds is rather big. No reasonable improvement in prediction score can be achieved in such a way.","51d68fcb":"# 2. Define scoring functions for `domain2`\n\nNow that the data is ready, let's prepare a simple scoring function that either focuses on `var1` or on `var2`.","435cabb7":"# 3. Define function to create NN model\n\nThe neural network has a rather simple architecture. It is a dense, fully connected neural network, with the potential for `BatchNormalization` and `Dropout` after every layer. The final output layer contains two output neurons with as many activation functions as loss functions.\n\nThe loss functions can be weighted to be between var1 and var2 with a hinge function, so that the wait for loss1 and loss2 always equals 1.","6fbf2b8e":"To load the feature matrix, let's write a function that performs the following steps:\n\n1. Load the train and test set.\n2. Select the two `domain2` targets.\n3. Centralize the corr_coef features to the median, which is about 0.689\n4. Scale the four feature datasets (IC, FNC, intra and inter correlations) according to a scaler called `scale_values`. Before this scaler is applied, all feature within a dataset are scaled to the 90% value (i.e. top 10%). No centralization was applied.\n5. Missing values are dropped.","15dcdc04":"# 5. Write a few supportive visualization functions","53248b27":"# 1. Load targets and features\n\nFirst things first, let's load the adapted targets and prepared feature datasets from my [second notebook](https:\/\/www.kaggle.com\/miykael\/trends-feature-exploration-engineering).","48d2bbf1":"# 4. Define parameter grid and function to run the grid (using KFold)","507f67eb":"# 6. Dual `domain2` prediction based on MAE score","556bcf33":"# Dual prediction of `domain2` using multi-loss neural network\n\nGiven the particular relationship between `domain2_var1` and `domain2_var2`, which I showed in [first notebook](https:\/\/www.kaggle.com\/miykael\/trends-exploration-of-the-targets)), I pursued an dense neural network modeling approach, with multi-loss. The idea was to find a prediction for the two `domain2` variables that is close to the original values, but where the rotated values are also close to the rotated predictions.\n\nAs a note, in this notebook I will be using the adapted targets, as well as the engineered features I've explored in my [second notebook](https:\/\/www.kaggle.com\/miykael\/trends-feature-exploration-engineering)).","e84765c8":"## Observation 2\n\nThis approach doesn't really seem to work. While it stretches the prediction into something that is orthogonal to the orientation of `domain2_var2_rotated`, in the `hinge=0` condition, it creates only round blobs in the others."}}