{"cell_type":{"b043d462":"code","37e76133":"code","71f28c9a":"code","1b3097bc":"code","5fa4b4a5":"code","988a0e1f":"code","b8849e97":"code","5d77e3c3":"code","7045e8f0":"code","47ef6617":"code","436bae5c":"code","e8276c85":"code","a584bc92":"code","db1b308d":"code","2efd1d73":"code","e9d82bb6":"code","686b1951":"code","6d42bf5d":"code","111a6bb1":"code","0e90bd95":"code","39ebd6d1":"code","7311c274":"code","a41e137b":"code","a7e61864":"code","a988d631":"code","2c47207d":"code","674dd749":"code","fe493828":"code","e18a31f0":"code","51e82fa5":"code","098e7b69":"code","34f12da6":"code","390cde60":"code","dfc7db12":"code","7645310b":"code","e210bb12":"code","0a90f773":"code","55413354":"markdown","499bedd3":"markdown","5b8cc7a0":"markdown","931753d8":"markdown","3b8240be":"markdown","a178e87c":"markdown","4fec2ef2":"markdown","83d15686":"markdown","b267a7ee":"markdown","025cee2f":"markdown","4dbb9240":"markdown","b291819d":"markdown","aa125c43":"markdown","321643c7":"markdown","a88af969":"markdown","12d8b465":"markdown","bd39d38a":"markdown","05a08489":"markdown","0ae291f4":"markdown","788b86c0":"markdown","59dfd033":"markdown","c3005c81":"markdown"},"source":{"b043d462":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n!pip install pandas\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n# !pip install pycomp\nfrom pandas_datareader import data\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        \ntrain_df = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","37e76133":"print('Rows and Columns in train dataset:', train_df.shape)\nprint('Rows and Columns in test dataset:', test_df.shape)\nprint()\nprint('Missing values in train dataset:', sum(train_df.isnull().sum()))\nprint('Missing values in test dataset:', sum(test_df.isnull().sum()))\nprint()\nprint('Missing values per columns in train dataset')\nfor col in train_df.columns:\n    temp_col = train_df[col].isnull().sum()\n    print(f'{col}: {temp_col}')\nprint()\nprint('Missing values per columns in test dataset')\nfor col in test_df.columns:\n    temp_col = test_df[col].isnull().sum()\n    print(f'{col}: {temp_col}')\n    ","71f28c9a":"train_df.head(1)","1b3097bc":"plt.rcParams['figure.dpi'] = 300\nfig = plt.figure(figsize=(5, 5), facecolor='#f6f5f5')\ngs = fig.add_gridspec(3, 2)\ngs.update(wspace=0.4, hspace=0.8)\n\nbackground_color = \"#f6f5f5\"\n\n# background_color = \"#f6f5f5\"\ncolumn = 'target'\ncolor_map = ['#eeb977', 'lightgray','plum','paleturquoise']\nsns.set_palette(sns.color_palette(color_map))\ntemp_train = pd.DataFrame(train_df[column].value_counts()).reset_index(drop=False)\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_color)\nax0.tick_params(axis = \"y\", which = \"both\", left = False)\nax0.text(-1, 83, 'Target distribution on the training data', color='black', fontsize=7, ha='left', va='bottom', weight='bold')\n# ax0.text(-1, 82, 'Survival Rate ', color='#292929', fontsize=5, ha='left', va='top')\n# ax0.text(1.18, 73.3, 'for age and fare', color='#292929', fontsize=4, ha='left', va='bottom')\nax0_sns = sns.barplot(ax=ax0, x=temp_train['index'], y=temp_train[column]\/1000, zorder=2)\nax0_sns.set_xlabel(\"Target\",fontsize=5, weight='bold')\nax0_sns.set_ylabel('')\nax0.yaxis.set_major_formatter(ticker.PercentFormatter())\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE')\nax0_sns.tick_params(labelsize=5)\nax0_sns.legend(['Class 1','Class 2','Class 3','Class 4'], ncol=2, facecolor=background_color, edgecolor=background_color, fontsize=4, bbox_to_anchor=(-0.26, 1.3), loc='upper left')\nleg = ax0_sns.get_legend()\nleg.legendHandles[0].set_color('#eeb977')\nleg.legendHandles[1].set_color('lightgray')\nleg.legendHandles[2].set_color('plum')\nleg.legendHandles[3].set_color('paleturquoise')","5fa4b4a5":"print(\"Training Data Statistical Distributions\")\n\ntrain_df.describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","988a0e1f":"print(\"Test Data Statistical Distributions\")\n\ntest_df.describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","b8849e97":"# feature_13\n# feature_36\n# feature_22\n\nprint(\"Features with Categories less than 10\")\nprint()\n\ncolor_map = ['#eeb977', 'lightgray','plum','paleturquoise']\nplt.rcParams['figure.dpi'] = 300\nfig = plt.figure(figsize=(5, 5), facecolor='#f6f5f5')\ngs = fig.add_gridspec(4, 3)\ngs.update(wspace=0.4, hspace=0.8)\n\nbackground_color = \"#f6f5f5\"\n\ncolumn = 'feature_2'\n# color_map = ['#eeb977', 'lightgray']\nsns.set_palette(sns.color_palette(color_map))\ntemp_train = pd.DataFrame(train_df[column].value_counts()).reset_index(drop=False)\nax0 = fig.add_subplot(gs[0, 0])\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_color)\nax0.tick_params(axis = \"y\", which = \"both\", left = False)\n# ax0.text(-1, 83, 'Survival Rate', color='black', fontsize=7, ha='left', va='bottom', weight='bold')\n# ax0.text(-1, 82, 'feature_2 ', color='#292929', fontsize=5, ha='left', va='top')\n# ax0.text(1.18, 73.3, 'for age and fare', color='#292929', fontsize=4, ha='left', va='top')\nax0_sns = sns.barplot(ax=ax0, x=temp_train['index'], y=temp_train[column]\/1000, zorder=2)\nax0_sns.set_xlabel(\"feature_2\",fontsize=5, weight='bold')\nax0_sns.set_ylabel('')\nax0.yaxis.set_major_formatter(ticker.PercentFormatter())\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE')\nax0_sns.tick_params(labelsize=5)\n# ax0_sns.legend(['Survived', 'Not Survived'], ncol=2, facecolor=background_color, edgecolor=background_color, fontsize=4, bbox_to_anchor=(-0.26, 1.3), loc='upper left')\n# leg = ax0_sns.get_legend()\n# leg.legendHandles[0].set_color('#eeb977')\n# leg.legendHandles[1].set_color('lightgray')\n\ncolumn = 'feature_13'\n# color_map = ['#eeb977', 'lightgray', 'lightgray']\nsns.set_palette(sns.color_palette(color_map))\ntemp_train = train_df.groupby(column)['feature_13'].sum()\nax1 = fig.add_subplot(gs[0, 1])\nfor s in [\"right\", \"top\"]:\n    ax1.spines[s].set_visible(False)\nax1.set_facecolor(background_color)\nax1.tick_params(axis = \"y\", which = \"both\", left = False)\nax1_sns = sns.barplot(ax=ax1, x=temp_train.index, y=temp_train\/1000, zorder=2)\nax1_sns.set_xlabel(\"feature_13\",fontsize=5, weight='bold')\nax1_sns.set_ylabel('')\nax1.yaxis.set_major_formatter(ticker.PercentFormatter())\nax1_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax1_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE')\nax1_sns.tick_params(labelsize=5)\n\ncolumn = 'feature_36'\n# color_map = ['#eeb977', 'lightgray']\nsns.set_palette(sns.color_palette(color_map))\ntemp_train = train_df.groupby(column)['feature_36'].sum()\nax2 = fig.add_subplot(gs[0, 2])\nfor s in [\"right\", \"top\"]:\n    ax2.spines[s].set_visible(False)\nax2.set_facecolor(background_color)\nax2.tick_params(axis = \"y\", which = \"both\", left = False)\nax2_sns = sns.barplot(ax=ax2, x=temp_train.index, y=temp_train\/1000, zorder=2)\nax2_sns.set_xlabel(\"feature_36\",fontsize=5, weight='bold')\nax2_sns.set_ylabel('')\nax2.yaxis.set_major_formatter(ticker.PercentFormatter())\nax2_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax2_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE')\nax2_sns.tick_params(labelsize=5)\n\n\ncolumn = 'feature_22'\n# color_map = ['lightgray' for _ in range(7)]\ncolor_map[0] = '#eeb977'\nsns.set_palette(sns.color_palette(color_map))\ntemp_train = train_df.groupby(column)['feature_22'].sum()\nax4 = fig.add_subplot(gs[1, 1])\nfor s in [\"right\", \"top\"]:\n    ax4.spines[s].set_visible(False)\nax4.set_facecolor(background_color)\nax4.tick_params(axis = \"y\", which = \"both\", left = False)\nax4_sns = sns.barplot(ax=ax4, x=temp_train.index, y=temp_train\/1000, zorder=2)\nax4_sns.set_xlabel(\"feature_22\",fontsize=5, weight='bold')\nax4_sns.set_ylabel('')\nax4.yaxis.set_major_formatter(ticker.PercentFormatter())\nax4_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax4_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE')\nax4_sns.tick_params(labelsize=5)\n","5d77e3c3":"plt.rcParams['figure.dpi'] = 300\nfig = plt.figure(figsize=(5, 2))\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\n\nax0.tick_params(axis = \"y\", which = \"both\", left = False)\n\n# KDE plots\nax0_sns = sns.kdeplot(ax=ax0, x=train_df['feature_14'], zorder=2, shade=True)\nax0_sns = sns.kdeplot(ax=ax0, x=test_df['feature_14'], zorder=2, shade=True)\n\n# Axis and grid customization\nax0_sns.set_xlabel(\"feature_14\",fontsize=5, weight='bold')\nax0_sns.set_ylabel('')\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE')\n\n# Legend params\nax0.legend(['train', 'test'], prop={'size': 5})\nax0_sns.tick_params(labelsize=5)\n\nplt.show()","7045e8f0":"plt.rcParams['figure.dpi'] = 300\nfig = plt.figure(figsize=(5, 2))\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\n\nax0.tick_params(axis = \"y\", which = \"both\", left = False)\n\n# KDE plots\nax0_sns = sns.kdeplot(ax=ax0, x=train_df['feature_38'], zorder=2, shade=True)\nax0_sns = sns.kdeplot(ax=ax0, x=test_df['feature_38'], zorder=2, shade=True)\n\n# Axis and grid customization\nax0_sns.set_xlabel(\"feature_38\",fontsize=5, weight='bold')\nax0_sns.set_ylabel('')\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE')\n\n# Legend params\nax0.legend(['train', 'test'], prop={'size': 5})\nax0_sns.tick_params(labelsize=5)\n\nplt.show()","47ef6617":"fig, ax = plt.subplots(figsize=(12, 7))\n\n# x = [f'feature_{i}' for i in range(50)]\ny = sorted([len(train_df[f'feature_{i}'].unique()) for i in range(50)])\n\nax.bar(range(50), y, zorder=10)\nax.set_xticks([])\nax.set_yticks(range(0, 80, 5))\nax.margins(0.02)\n\nax.set_title('TRAIN : # of Features Unique Values', loc='left', fontweight='bold')\nax.grid(axis='y', linestyle='--', zorder=5)\nplt.show()","436bae5c":"fig, ax = plt.subplots(figsize=(12, 7))\n\n# x = [f'feature_{i}' for i in range(50)]\ny = sorted([len(test_df[f'feature_{i}'].unique()) for i in range(50)])\n\nax.bar(range(50), y, zorder=10)\nax.set_xticks([])\nax.set_yticks(range(0, 80, 5))\nax.margins(0.02)\n\nax.set_title('TEST : # of Features Unique Values', loc='left', fontweight='bold')\nax.grid(axis='y', linestyle='--', zorder=5)\nplt.show()","e8276c85":"train = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\")\n\ntrain_p = train\nlic = []\nfor col in train_p.columns[1:-1]:\n    lic.append(col)\n    \ndef plot(col):\n    plt.figure(figsize = (18, 8))\n    g = sns.countplot(x = col, hue = 'target', data = train_p)\n    plt.legend(loc='upper right')\n    plt.title(\"Distribution of \"+ col,fontsize=15)\n    plt.show();\n\nfor col in lic:\n    plot(col)","a584bc92":"train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv',index_col='id')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv',index_col='id')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv',index_col='id')","db1b308d":"# Let's assume all features are categorical. But some values exist only in train or test\n# Clean it manually\n\ntest.loc[test['feature_3']==25,'feature_3']=26\ntest.loc[test['feature_4']==36,'feature_4']=37\ntest.loc[test['feature_21']==31,'feature_21']=36\ntest.loc[test['feature_25']==24,'feature_25']=23\ntest.loc[test['feature_34']==26,'feature_34']=25\ntest.loc[test['feature_49']==21,'feature_49']=20\n\nprint(len(train))\ntrain = train[train['feature_5']!=10]\n\ntrain = train[train['feature_6']!=26]\ntrain = train[train['feature_6']!=27]\n\ntrain = train[train['feature_7']!=30]\ntrain = train[train['feature_7']!=31]\n\ntrain = train[train['feature_9']!=17]\n\ntrain = train[train['feature_10']!=16]\n\ntrain = train[train['feature_11']!=12]\n\ntrain = train[train['feature_15']!=20]\n\ntrain = train[train['feature_16']!=18]\n\ntrain = train[train['feature_23']!=18]\ntrain = train[train['feature_23']!=19]\n\ntrain = train[train['feature_27']!=29]\n\ntrain = train[train['feature_28']!=23]\n\ntrain = train[train['feature_29']!=13]\n\ntrain = train[train['feature_33']!=24]\n\ntrain = train[train['feature_32']!=26]\ntrain = train[train['feature_32']!=27]\n\ntrain = train[train['feature_35']!=43]\ntrain = train[train['feature_35']!=-2]\ntrain = train[train['feature_35']!=38]\ntrain = train[train['feature_35']!=39]\n\n\ntrain = train[train['feature_38']!=65]\ntrain = train[train['feature_38']!=55]\ntrain = train[train['feature_38']!=-8]\ntrain = train[train['feature_38']!=-3]\ntrain = train[train['feature_38']!=-2]\ntrain = train[train['feature_38']!=63]\n\ntrain = train[train['feature_39']!=65]\ntrain = train[train['feature_39']!=66]\ntrain = train[train['feature_39']!=-5]\ntrain = train[train['feature_39']!=-3]\ntrain = train[train['feature_39']!=-2]\ntrain = train[train['feature_39']!=63]\n\ntrain = train[train['feature_42']!=37]\ntrain = train[train['feature_42']!=-2]\ntrain = train[train['feature_42']!=-1]\n\ntrain = train[train['feature_43']!=33]\ntrain = train[train['feature_43']!=31]\n\nprint(len(train))","2efd1d73":"columns = train.columns[:-1]\ncat_features = ['feature_0',\n                'feature_2',\n                'feature_3',\n                'feature_4',\n                'feature_5',\n                'feature_6',\n                'feature_7',\n                'feature_9',\n                'feature_10',\n                'feature_11',\n                'feature_12',\n                'feature_13',\n                'feature_15',\n                'feature_16',\n                'feature_17',\n                'feature_18',\n                'feature_20',\n                'feature_21',\n                'feature_22',\n                'feature_23',\n                'feature_25',\n                'feature_26',\n                'feature_27',\n                'feature_28',\n                'feature_29',\n                'feature_32',\n                'feature_33',\n                'feature_34',\n                'feature_35',\n                'feature_36',\n                'feature_38',\n                'feature_39',\n                'feature_42',\n                'feature_43',\n                'feature_44',\n                'feature_46',\n                'feature_49']","e9d82bb6":"from catboost import Pool, CatBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold\ndef train_models(df, columns, cat_features, random_state, n_splits):\n\n    models = []\n    folds = StratifiedKFold(n_splits=n_splits, shuffle=True,\n                            random_state=random_state)  # create folds\n    X_train = df[columns]\n    y_train = df['target']\n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(X_train,  y_train)):\n        print('Fold {}\/{}'.format(n_fold, n_splits))\n        train_X, train_y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n        valid_X, valid_y = X_train.iloc[valid_idx], y_train.iloc[valid_idx]\n        dataset = Pool(train_X, train_y, cat_features)\n        evalset = Pool(valid_X, valid_y, cat_features)\n        model = CatBoostClassifier(\n            task_type=\"GPU\",\n            depth=4,\n            max_ctr_complexity=15,\n            # border_count=1024,\n            iterations=50000,\n            od_wait=1000, od_type='Iter',\n            # l2_leaf_reg=0.01,\n            learning_rate=0.01,\n            min_data_in_leaf=1,\n            use_best_model=True,\n            loss_function='MultiClass'\n\n        )\n        model.fit(dataset, plot=False, verbose=500, eval_set=evalset)        \n        _record = {\n            'model': model,\n        }\n        models.append(_record)\n\n    return models","686b1951":"models = train_models(df=train, columns=columns, cat_features=cat_features, random_state=43, n_splits=9)","6d42bf5d":"c1_columns = []\nc2_columns = []\nc3_columns = []\nc4_columns = []\n\nfor idx,m in enumerate(models):\n    print(idx)\n    _proba = m['model'].predict_proba(test[columns])\n    \n    c1_name = 'm{}_c1'.format(idx)\n    c1_columns.append(c1_name)\n    test[c1_name] = _proba[:,0]\n    \n    c2_name = 'm{}_c2'.format(idx)\n    c2_columns.append(c2_name)\n    test[c2_name] = _proba[:,1]\n    \n    c3_name = 'm{}_c3'.format(idx)\n    c3_columns.append(c3_name)\n    test[c3_name] = _proba[:,2]\n    \n    c4_name = 'm{}_c4'.format(idx)\n    c4_columns.append(c4_name)\n    test[c4_name] = _proba[:,3]","111a6bb1":"test['Class_1'] = test[c1_columns].apply(np.mean,axis=1)\ntest['Class_2'] = test[c2_columns].apply(np.mean,axis=1)\ntest['Class_3'] = test[c3_columns].apply(np.mean,axis=1)\ntest['Class_4'] = test[c4_columns].apply(np.mean,axis=1)\ntest.to_csv('test3.csv')","0e90bd95":"submission['Class_1'] = test['Class_1']\nsubmission['Class_2'] = test['Class_2']\nsubmission['Class_3'] = test['Class_3']\nsubmission['Class_4'] = test['Class_4']\nsubmission.to_csv('catboost-3.csv')","39ebd6d1":"import shap\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nfrom tqdm import tqdm\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7311c274":"train = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\", index_col = 'id')\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\", index_col = 'id')\ntrain = train[~train.drop('target', axis = 1).duplicated()]\n\nX = pd.DataFrame(train.drop(\"target\", axis = 1))\n\nlencoder = LabelEncoder()\ny = pd.DataFrame(lencoder.fit_transform(train['target']), columns=['target'])","a41e137b":"test_preds = None\ntrain_rmse = 0\nval_rmse = 0\nn_splits = 10\n\nmodel =  LGBMClassifier()\n\nskf = StratifiedKFold(n_splits = n_splits, shuffle = True,  random_state = 0)\n\nfor tr_index , val_index in tqdm(skf.split(X.values , y.values), total=skf.get_n_splits(), desc=\"k-fold\"):\n\n    x_train_o, x_val_o = X.iloc[tr_index] , X.iloc[val_index]\n    y_train_o, y_val_o = y.iloc[tr_index] , y.iloc[val_index]\n\n    eval_set = [(x_val_o, y_val_o)]\n\n    model.fit(x_train_o, y_train_o, eval_set = eval_set, early_stopping_rounds=100, verbose=False)\n\n    train_preds = model.predict(x_train_o)\n    train_rmse += mean_squared_error(y_train_o ,train_preds , squared = False)\n\n    val_preds = model.predict(x_val_o)\n    val_rmse += mean_squared_error(y_val_o , val_preds , squared = False)\n\n    if test_preds is None:\n        test_preds = model.predict_proba(test.values)\n    else:\n        test_preds += model.predict_proba(test.values)\n\n    print(f\"\\nAverage Training RMSE : {train_rmse \/ n_splits}\")\n    print(f\"Average Validation RMSE : {val_rmse \/ n_splits}\\n\")\n\n    test_preds \/= n_splits","a7e61864":"explainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(test)","a988d631":"shap.summary_plot(shap_values, X)","2c47207d":"selected_features = [\"feature_25\", \"feature_14\", \"feature_15\", \"feature_31\"]\n\nplt.figure(figsize=(20,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","674dd749":"test[selected_features].describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","fe493828":"selected_features = [\"feature_14\", \"feature_15\", \"feature_6\", \"feature_16\", \"feature_31\", \"feature_37\"]\n\nplt.figure(figsize=(15,10))\nc = 1\nfor feat in selected_features:\n    plt.subplot(2, 3, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","e18a31f0":"test[selected_features].describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","51e82fa5":"shap.summary_plot(shap_values[0], test)","098e7b69":"selected_features = [\"feature_25\", \"feature_37\", \"feature_23\", \"feature_38\"]\n\nplt.figure(figsize=(30,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","34f12da6":"shap.summary_plot(shap_values[1], test)","390cde60":"selected_features = [\"feature_14\", \"feature_6\", \"feature_28\", \"feature_37\"]\n\nplt.figure(figsize=(30,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","dfc7db12":" shap.summary_plot(shap_values[2], test)","7645310b":"selected_features = [\"feature_15\", \"feature_14\", \"feature_2\", \"feature_11\"]\n\nplt.figure(figsize=(30,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","e210bb12":" shap.summary_plot(shap_values[3], test)","0a90f773":"selected_features = [\"feature_31\", \"feature_24\", \"feature_14\", \"feature_16\"]\n\nplt.figure(figsize=(30,5))\nc = 1\nfor feat in selected_features:\n    plt.subplot(1, 4, c)\n    sns.histplot(x = feat, data = test, bins=10)\n    c = c + 1    \nplt.show()","55413354":"# Model Explainablity - SHAP","499bedd3":"# Basic Introduction :\n\nCategorical data is a collection of information that is divided into groups. I.e, if an organisation or agency is trying to get a biodata of its employees, the resulting data is referred to as categorical\n\n**In statistics, a categorical variable is a variable that can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property**\n\nMachine learning and deep learning models, like those in Keras, require all input and output variables to be numeric.\n\nThis means that if your data contains categorical data, you must encode it to numbers before you can fit and evaluate a model.\n\nThe two most popular techniques are an integer encoding and a one hot encoding, although a newer technique called learned embedding may provide a useful middle ground between these two methods.\n\n\n\n**The Challenge With Categorical Data**\nA categorical variable is a variable whose values take on the value of labels.\n\nFor example, the variable may be \u201ccolor\u201d and may take on the values \u201cred,\u201d \u201cgreen,\u201d and \u201cblue.\u201d\n\nSometimes, the categorical data may have an ordered relationship between the categories, such as \u201cfirst,\u201d \u201csecond,\u201d and \u201cthird.\u201d This type of categorical data is referred to as ordinal and the additional ordering information can be useful.\n\nMachine learning algorithms and deep learning neural networks require that input and output variables are numbers.\n\nThis means that categorical data must be encoded to numbers before we can use it to fit and evaluate a model.\n\nThere are many ways to encode categorical variables for modeling, although the three most common are as follows:\n\n- **Integer Encoding**: Where each unique label is mapped to an integer.\n- **One Hot Encoding**: Where each label is mapped to a binary vector.\n- **Learned Embedding**: Where a distributed representation of the categories is learned.","5b8cc7a0":"# Analysis\nThe first thing we are going to check is the distribution of the target feature. It's important to know if the class is balanced or not. If so, we would probably have to handle it.","931753d8":"# Model - CATBOOST","3b8240be":"# Importing the data","a178e87c":"### Thank you for going through the plots I have created as of now! \n\n### I will keep adding more and try to do a more detailed EDA this time! \n**Do upvote and check out my other kernels!!**\n\n***Follow me for more content!!!***\n\nSome of my other works:\n- [**TPS APRIL**](https:\/\/www.kaggle.com\/udbhavpangotra\/tps-apr21-eda-model )\n- [**Heart Attacks!!! Find out what factors influence them the most!** ](https:\/\/www.kaggle.com\/udbhavpangotra\/heart-attacks-extensive-eda-and-visualizations) \n- [**What are the Birtish watching on Youtube!**](https:\/\/www.kaggle.com\/udbhavpangotra\/what-do-people-use-youtube-for-in-great-britain)","4fec2ef2":"**Observations:**\n\n- There is no obvious patters here, this is why model has problem with class 0 prediction - it works almost randomly (I think but I could be wrong)\nas you can see reds are on the left and right - but we can see that most \"high\" values are on right side of chart\n- almost all features are 0 balanced ... so 0 can drive model to Class 0 and .... rest of Class -> ZERO plays main role in this competition.\n- Feature 25 - we can see that \"medium\" feature value (purple) drives model to predict class 0\n- Feature 37 - the highest value (red) then class 0 is predicted,\n- Feature 23 - high values drive model to predict class 0\n- there is almost no uniform distribution","83d15686":"**Observations:**\n\n- Looks like most of values is ZERO(!) This is why you can observe some patterns later during class prediction analysis.\n- This could be a potential problem with the model. Which will divide classes with respect to 0. And if there is a majority of them then ...\n","b267a7ee":"**Observations:**\n- No missing data\n- The scale of the mean and deviation varies.\n- The median is mostly 0, and there are 2 columns with median 1.\n- The representative statistics of train and test are almost similar.\n- Features are highly skewed.Applying feature transformations can help us build a better model.","025cee2f":"FOR THIS I WILL BE USING A LGBM MODEL!! CHEERS! ","4dbb9240":"**Observaitons**\n\n- Feature_14 and feature_15 affect the model the most, but you can also see the balance between classes (feature_15 plays big role in class_2 prediction) - they are definitely important variables\n- The most important feature for class_0 is feature_25, class_1 -> feature_14, class_2 -> feature_15, class_3 -> feature_31","b291819d":"**Conclusion:**\n\n- if most of features (till 20) are \"low\" they drives model to other class (not class 1, probably for class 3)\n- feature 14 and \"low\" values says that probaly there is no Class_1\n- feature 14->37 are uniform - \"higher\" values drive model to predict Class_1\n- feature_34 - \"higher\" values drive model to Class_0","aa125c43":"SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model. It connects optimal credit allocation with local explanations using the classic Shapley values from game theory and their related extensions (see papers for details and citations).\n\n\n![SHAP](https:\/\/shap.readthedocs.io\/en\/latest\/_images\/shap_header.png)","321643c7":"**Conclusion:**\n\n- From feature 31 to 23 ... we can see that small values drive model to Class_3. This is probably 0 (we shold see data in these feature).\n- The bigger number in features the less chance that Class 3 will be predicted.","a88af969":"References for the code : \n\n*model* : \nhttps:\/\/www.kaggle.com\/belov38\/catboost-catfeatures\n\n*shap* : \nhttps:\/\/www.kaggle.com\/remekkinas\/shap-lgbm-looking-for-best-features","12d8b465":"From the statistical summary, we know that the data range of Feature 38 and Feature 14 column is considerably larger than the other numeric columns.\nFeature 38 and Feature 14 has median 1 and all the remaining columns median is zero.","bd39d38a":"**Data Distibution**\nNow lets look at the statistical distribution for the Training and the Test data:\n","05a08489":"**Observations :**\nIn the target variable, class2 has more data points compared to the remaining labels.so, we probably have to address class imbalance problem.","0ae291f4":"**Conclusion:**\n\n- The most important feature is 15 and 14. Here we can see obvious pattens. In feature_15 small values (probalby 0) drives model to Class 2. Feature_14 - high values drive to Class 2.\n- Feature_2 there is seen good separation - all positive values (blue blob is 0) highly influence model in Class_2 direction.\n- Almost in all features from list we can see that \"high\" values drives model to Class_2","788b86c0":"# Data Overview\nOberservations after having a quick look at the data:\n\n- 100,000 data points of type integer\n- 50 features (feature_0 through feature_49)\n- 1 target\n- 0 missing values\n\n\nCategorical Features:\n\nA categorical variable is one that has two or more categories and each value in that feature can be categorised by them.For example, gender is a categorical variable having two categories (male and female). Now we cannot sort or give any ordering to such variables. They are also known as Nominal Variables.\n\nOrdinal Features:\n\nAn ordinal variable is similar to categorical values, but the difference between them is that we can have relative ordering or sorting between the values. For eg: If we have a feature like Height with values Tall, Medium, Short, then Height is a ordinal variable. Here we can have a relative sort in the variable","59dfd033":"Hi there,\n\nI here have tried to do some exploratory data analysis and create visualizations for the same.\nHope they help you as well in trying to understand the dataset we have for this competition.\n\n\n**Hope you gain some insights from it and find it useful! Do upvote and share it if you like it! :)**\n\n\nSome of the basics first.\n\n**Competition Name**        : Tabular Playground Series - May 2021\n**Competition Description** : The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.\n**Competition evaluation Metric** : Submissions are evaluated using multi-class logarithmic loss. Each row in the dataset has been labeled with one true Class. For each row, you must submit the predicted probabilities that the product belongs to each class label. The formula is:\n\n**log loss = \u22121\/N \u2211\u2211 yijlog(pij)**\n\n\nwhere N is the number of rows in the test set, M is the number of class labels, log is the natural logarithm, yij is 1 if observation i is in class j and 0 otherwise, and pij is the predicted probability that observation i belongs to class j.\n\nThe submitted probabilities for a given product are not required to sum to one; they are rescaled prior to being scored, each row is divided by the row sum. In order to avoid the extremes of the log function, predicted probabilities are replaced with max(min(p,1\u221210\u221215),10\u221215).\n\n**Competition Timeline** : \n- Start Date - May 1, 2021\n- Entry deadline - Same as the Final Submission Deadline\n- Team Merger deadline - Same as the Final Submission Deadline\n- Final submission deadline - May 31, 2021\n\nPoints to note:\n- This competition does not award ranking points\n- This competition does not count towards tiers","c3005c81":"**The kernel will have 5 parts:**\n- Basic Introduction\n- EDA\n- Plots\n- Model\n- SHAP"}}