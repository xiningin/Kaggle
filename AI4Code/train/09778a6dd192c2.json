{"cell_type":{"8a8eb55b":"code","6f876eb9":"code","80d549c3":"code","63d38672":"code","e2454ee9":"code","816e5963":"code","a28bfb66":"code","f5920168":"code","a367f97f":"code","81f7de11":"code","f2ebca2a":"code","bba0e0d7":"code","4bbe4699":"code","ad6018a8":"code","849728fe":"code","f263e659":"code","a2980574":"code","4ab908d8":"code","7170ce86":"code","f06f398b":"code","50be4bdf":"code","512f46a1":"code","72fb40c4":"code","d6f8e838":"code","114b2e61":"markdown","4ce669b6":"markdown","bfaf08db":"markdown","7c0169df":"markdown","5c29cd34":"markdown","7b1ef45b":"markdown","cb20f7d6":"markdown","be39aa08":"markdown"},"source":{"8a8eb55b":"import os\nimport gc\nimport warnings\n\nimport pandas as pd\nfrom pandas.plotting import register_matplotlib_converters\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_columns\", 500)\npd.set_option(\"display.max_rows\", 500)\nregister_matplotlib_converters()\nsns.set()","6f876eb9":"import IPython\n\n\ndef display(*dfs, head=True):\n    for df in dfs:\n        IPython.display.display(df.head() if head else df)","80d549c3":"def on_kaggle():\n    return \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ","63d38672":"if on_kaggle():\n    os.system(\"pip install --quiet mlflow_extend\")","e2454ee9":"def reduce_mem_usage(df, verbose=False):\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    int_columns = df.select_dtypes(include=[\"int\"]).columns\n    float_columns = df.select_dtypes(include=[\"float\"]).columns\n\n    for col in int_columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"integer\")\n\n    for col in float_columns:\n        df[col] = pd.to_numeric(df[col], downcast=\"float\")\n\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(\n            \"Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)\".format(\n                end_mem, 100 * (start_mem - end_mem) \/ start_mem\n            )\n        )\n    return df","816e5963":"def read_data():\n    INPUT_DIR = \"\/kaggle\/input\" if on_kaggle() else \"input\"\n    INPUT_DIR = f\"{INPUT_DIR}\/m5-forecasting-accuracy\"\n\n    print(\"Reading files...\")\n\n    calendar = pd.read_csv(f\"{INPUT_DIR}\/calendar.csv\").pipe(reduce_mem_usage)\n    prices = pd.read_csv(f\"{INPUT_DIR}\/sell_prices.csv\").pipe(reduce_mem_usage)\n\n    #sales = pd.read_csv(f\"{INPUT_DIR}\/sales_train_validation.csv\",).pipe(\n    #    reduce_mem_usage\n    #)\n    sales = pd.read_csv(f\"{INPUT_DIR}\/sales_train_evaluation.csv\",).pipe(\n        reduce_mem_usage\n    )\n    submission = pd.read_csv(f\"{INPUT_DIR}\/sample_submission.csv\").pipe(\n        reduce_mem_usage\n    )\n\n    print(\"sales shape:\", sales.shape)\n    print(\"prices shape:\", prices.shape)\n    print(\"calendar shape:\", calendar.shape)\n    print(\"submission shape:\", submission.shape)\n\n    # calendar shape: (1969, 14)\n    # sell_prices shape: (6841121, 4)\n    # sales_train_val shape: (30490, 1919)\n    # submission shape: (60980, 29)\n\n    return sales, prices, calendar, submission","a28bfb66":"sales, prices, calendar, submission = read_data()\n\nNUM_ITEMS = sales.shape[0]  # 30490\nDAYS_PRED = submission.shape[1] - 1  # 28","f5920168":"def encode_categorical(df, cols):\n    for col in cols:\n        # Leave NaN as it is.\n        le = LabelEncoder()\n        not_null = df[col][df[col].notnull()]\n        df[col] = pd.Series(le.fit_transform(not_null), index=not_null.index)\n\n    return df\n\n\ncalendar = encode_categorical(\n    calendar, [\"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]\n).pipe(reduce_mem_usage)\n\nsales = encode_categorical(\n    sales, [\"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n).pipe(reduce_mem_usage)\n\nprices = encode_categorical(prices, [\"item_id\", \"store_id\"]).pipe(reduce_mem_usage)","a367f97f":"def extract_num(ser):\n    return ser.str.extract(r\"(\\d+)\").astype(np.int16)\n\n\ndef reshape_sales(sales, submission, d_thresh=0, verbose=True):\n    # melt sales data, get it ready for training\n    # change column names.\n\n    id_columns = [\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"]\n    #vals_columns = [\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\n    evals_columns = [\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n    # separate test dataframes.\n    #vals = sales[vals_columns]#submission[submission[\"id\"].str.endswith(\"validation\")]\n    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n\n    # get product table.\n    \n    \n    product = sales[id_columns]\n    sales = sales.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\",)\n    sales = reduce_mem_usage(sales)\n\n\n    #vals.columns  = vals_columns#[\"id\"] + [f\"d_{d}\" for d in range(1914, 1914 + DAYS_PRED)]\n    evals.columns = evals_columns#[\"id\"] + [f\"d_{d}\" for d in range(1942, 1942 + DAYS_PRED)]\n\n    # merge with product table\n    # may be edit\n    #vals[\"id\"] = vals[\"id\"].str.replace(\"_validation\", \"_evaluation\")\n    #vals = vals.merge(product, how=\"left\", on=\"id\")\n    evals = evals.merge(product, how=\"left\", on=\"id\")\n    #vals[\"id\"] = vals[\"id\"].str.replace(\"_evaluation\",\"_validation\")\n\n    if verbose:\n        #print(\"validation\")\n        #display(vals)\n\n        print(\"evaluation\")\n        display(evals)\n\n    #vals = vals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n    evals = evals.melt(id_vars=id_columns, var_name=\"d\", value_name=\"demand\")\n\n    sales[\"part\"] = \"train\"\n    #sales[\"part\"][~(sales[\"d\"]<1914)] = \"validation\"\n    evals[\"part\"] = \"evaluation\"\n\n    data = pd.concat([sales, evals], axis=0)\n\n    del sales, evals\n\n    data[\"d\"] = extract_num(data[\"d\"])\n    data = data[data[\"d\"] >= d_thresh]\n\n    # delete evaluation for now.\n    # data = data[data[\"part\"] != \"evaluation\"]\n\n    gc.collect()\n\n    if verbose:\n        print(\"data\")\n        display(data)\n\n    return data\n\n\ndef merge_calendar(data, calendar):\n    calendar = calendar.drop([\"weekday\", \"wday\", \"month\", \"year\"], axis=1)\n    return data.merge(calendar, how=\"left\", on=\"d\")\n\n\ndef merge_prices(data, prices):\n    return data.merge(prices, how=\"left\", on=[\"store_id\", \"item_id\", \"wm_yr_wk\"])","81f7de11":"data = reshape_sales(sales, submission, d_thresh=1941 - int(365 * 2  + 7))\ndel sales\ngc.collect()\n\ncalendar[\"d\"] = extract_num(calendar[\"d\"])\ndata = merge_calendar(data, calendar)\ndel calendar\ngc.collect()\n\ndata = merge_prices(data, prices)\ndel prices\ngc.collect()\n\ndata = reduce_mem_usage(data)","f2ebca2a":"def add_demand_features(df):\n    # DAYS_PRED = 28,then\n    for diff in [0, 28]:\n        shift = DAYS_PRED + diff\n        df[f\"shift_t{shift}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(shift)\n        )\n    print(\"shift done\")\n    gc.collect()\n    ### I edited diff parameters. \n    ### we predict 28 days ahead, so I created diff and roll features \n    ### based on 28 days.\n    diff = 28\n    for window in [28, 56, 84]:\n        df[f\"shift_t{diff}_rolling_std_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(diff).rolling(window).std()\n        )\n        df[f\"shift_t{diff}_rolling_mean_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(diff).rolling(window).mean()\n        )\n        df[f\"rolling_min_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(diff).rolling(window).min()\n        )\n        df[f\"rolling_max_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(diff).rolling(window).max()\n        )\n        df[f\"rolling_sum_t{window}\"] = df.groupby([\"id\"])[\"demand\"].transform(\n            lambda x: x.shift(diff).rolling(window).sum()\n        )\n    print(\"window done\")\n    df[\"rolling_skew_t28\"] = df.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(28).skew()\n    )\n    df[\"rolling_kurt_t28\"] = df.groupby([\"id\"])[\"demand\"].transform(\n        lambda x: x.shift(DAYS_PRED).rolling(28).kurt()\n    )\n    return df\n\n\ndef add_price_features(df):\n    df[\"shift_price_t1\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1)\n    )\n    df[\"price_change_t1\"] = (df[\"shift_price_t1\"] - df[\"sell_price\"]) \/ (\n        df[\"shift_price_t1\"]\n    )\n    df[\"rolling_price_max_t365\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.shift(1).rolling(365).max()\n    )\n    df[\"price_change_t365\"] = (df[\"rolling_price_max_t365\"] - df[\"sell_price\"]) \/ (\n        df[\"rolling_price_max_t365\"]\n    )\n\n    df[\"rolling_price_std_t7\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(7).std()\n    )\n    df[\"rolling_price_std_t30\"] = df.groupby([\"id\"])[\"sell_price\"].transform(\n        lambda x: x.rolling(30).std()\n    )\n    return df.drop([\"rolling_price_max_t365\", \"shift_price_t1\"], axis=1)\n\n\ndef add_time_features(df, dt_col):\n    df[dt_col] = pd.to_datetime(df[dt_col])\n    attrs = [\n        \"year\",\n        \"quarter\",\n        \"month\",\n        \"week\",\n        \"day\",\n        \"dayofweek\",\n    ]\n\n    for attr in attrs:\n        dtype = np.int16 if attr == \"year\" else np.int8\n        df[attr] = getattr(df[dt_col].dt, attr).astype(dtype)\n\n    df[\"is_weekend\"] = df[\"dayofweek\"].isin([5, 6]).astype(np.int8)\n    df['month_day']  = df['month'] * 100 + df['day']\n    return df\n\n## Kien has added some features, but not worked...\ndef add_original_features(df):\n    df['shift_t28_log']   = np.log(df['shift_t28'] + 1)\n    df['shift_t28_sqrt']  = np.sqrt(df['shift_t28'])\n    df['shift_t56_log']   = np.log(df['shift_t56'] + 1)\n    df['shift_t56_sqrt']  = np.sqrt(df['shift_t56'])\n\n    df['shift_t28_diff_t7'] = df.groupby('id')['shift_t28'].diff(7)\n    df['shift_t56_diff_t7'] = df.groupby('id')['shift_t56'].diff(7)\n    \n    return df\n    ","bba0e0d7":"data = add_demand_features(data).pipe(reduce_mem_usage)\nprint('add_demand_features done')\ndata = add_price_features(data).pipe(reduce_mem_usage)\nprint('add_price_features done')\ndata = add_original_features(data).pipe(reduce_mem_usage)\nprint('add_original_features done')\ndt_col = \"date\"\ndata = add_time_features(data, dt_col).pipe(reduce_mem_usage)\ndata = data.sort_values(\"date\")\n\nprint(\"start date:\", data[dt_col].min())\nprint(\"end date:\", data[dt_col].max())\nprint(\"data shape:\", data.shape)","4bbe4699":"class CustomTimeSeriesSplitter:\n    def __init__(self, n_splits=5, train_days=80, test_days=20, day_col=\"d\"):\n        self.n_splits = n_splits\n        self.train_days = train_days\n        self.test_days = test_days\n        self.day_col = day_col\n\n    def split(self, X, y=None, groups=None):\n        SEC_IN_DAY = 3600 * 24\n        sec = (X[self.day_col] - X[self.day_col].iloc[0]) * SEC_IN_DAY\n        duration = sec.max()\n\n        train_sec = self.train_days * SEC_IN_DAY\n        test_sec = self.test_days * SEC_IN_DAY\n        total_sec = test_sec + train_sec\n\n        if self.n_splits == 1:\n            train_start = duration - total_sec\n            train_end = train_start + train_sec\n\n            train_mask = (sec >= train_start) & (sec < train_end)\n            test_mask = sec >= train_end\n\n            yield sec[train_mask].index.values, sec[test_mask].index.values\n\n        else:\n            # step = (duration - total_sec) \/ (self.n_splits - 1)\n            step = DAYS_PRED * SEC_IN_DAY\n\n            for idx in range(self.n_splits):\n                # train_start = idx * step\n                shift = (self.n_splits - (idx + 1)) * step\n                train_start = duration - total_sec - shift\n                train_end = train_start + train_sec\n                test_end = train_end + test_sec\n\n                train_mask = (sec > train_start) & (sec <= train_end)\n\n                if idx == self.n_splits - 1:\n                    test_mask = sec > train_end\n                else:\n                    test_mask = (sec > train_end) & (sec <= test_end)\n\n                yield sec[train_mask].index.values, sec[test_mask].index.values\n\n    def get_n_splits(self):\n        return self.n_splits","ad6018a8":"day_col = \"d\"\ncv_params = {\n    \"n_splits\": 3,\n    \"train_days\": int(365 * 1.5),\n    \"test_days\": 28,#DAYS_PRED,\n    \"day_col\": day_col,\n}\ncv = CustomTimeSeriesSplitter(**cv_params)","849728fe":"features = [\n    \"item_id\",\n    \"dept_id\",\n    \"cat_id\",\n    \"store_id\",\n    \"state_id\",\n    \"event_name_1\",\n    \"event_type_1\",\n    \"event_name_2\",\n    \"event_type_2\",\n    \"snap_CA\",\n    \"snap_TX\",\n    \"snap_WI\",\n    \"sell_price\",\n    # demand features\n    \"shift_t28\",\n    \"shift_t56\",\n    # std\n    \"shift_t28_rolling_std_t28\",\n    \"shift_t28_rolling_std_t56\",\n    \"shift_t28_rolling_std_t84\",\n    # mean\n    \"shift_t28_rolling_mean_t28\",\n    \"shift_t28_rolling_mean_t56\",\n    \"shift_t28_rolling_mean_t84\",\n    # min,\n    \"rolling_min_t28\",\n    # max\n    \"rolling_max_t28\",\n    \"rolling_max_t56\",\n    # sum\n    \"rolling_sum_t28\",    \n    \"rolling_sum_t56\",        \n    \"rolling_kurt_t28\",    \n    \"price_change_t365\",\n    \"rolling_price_std_t30\",\n    # time features\n    \"year\",\n    \"quarter\",\n    \"month\",\n    \"week\",\n    \"day\",\n    \"dayofweek\",\n    \"is_weekend\",\n    \"month_day\",\n    # original features\n    \"shift_t28_log\",\n    \"shift_t28_sqrt\",\n    \"shift_t56_log\",\n    \"shift_t56_sqrt\",\n    \"shift_t28_diff_t7\",\n    'shift_t56_diff_t7'\n    #\"self_diff_t7\",\n    #\"self_diff_t28\"\n    \n]\n# prepare training and test data.\n# 2011-01-29 ~ 2016-04-24 : d_1    ~ d_1913\n# 2016-04-25 ~ 2016-05-22 : d_1914 ~ d_1941 (public)\n# 2016-05-23 ~ 2016-06-19 : d_1942 ~ d_1969 (private)\n\nis_train = (data[\"d\"] < 1942)# & (data[\"d\"]>=1544)\n\nis_private = (data[\"d\"] >= 1942)\nis_public = ~(data[\"d\"] < 1914) & ~(is_private)\n\n\n# Attach \"d\" to X_train for cross validation.\nX_train = data[is_train][[day_col] + features].reset_index(drop=True)\ny_train = data[is_train][\"demand\"].reset_index(drop=True)\nX_test_pub = data[is_public][features].reset_index(drop=True)\nX_test_pri = data[is_private][features].reset_index(drop=True)\n\n# keep these two columns to use later.\nid_date_pub = data[is_public][[\"id\", \"date\"]].reset_index(drop=True)\nid_date_pri = data[is_private][[\"id\", \"date\"]].reset_index(drop=True)\n\ndel data\ngc.collect()\n\nprint(X_train['d'][0])\nprint(\"X_train shape:\", X_train.shape)\nprint(\"X_test_pub shape:\", X_test_pub.shape)\nprint(\"X_test_pri shape:\", X_test_pri.shape)\nprint(\"id_date_pub shape:\", id_date_pub.shape)\nprint(\"id_date_pri shape:\", id_date_pri.shape)","f263e659":"def train_lgb(bst_params, fit_params, X, y, cv, drop_when_train=None):\n    models = []\n\n    if drop_when_train is None:\n        drop_when_train = []\n\n    for idx_fold, (idx_trn, idx_val) in enumerate(cv.split(X, y)):\n        print(f\"\\n----- Fold: ({idx_fold + 1} \/ {cv.get_n_splits()}) -----\\n\")\n\n        X_trn, X_val = X.iloc[idx_trn], X.iloc[idx_val]\n        y_trn, y_val = y.iloc[idx_trn], y.iloc[idx_val]\n        \n        print(f'\\n train d min: {X_trn[\"d\"].min()} \\n valid d min: {X_val[\"d\"].min()} \\n')\n        \n        train_set = lgb.Dataset(\n            X_trn.drop(drop_when_train, axis=1),\n            label=y_trn,\n            categorical_feature=[\"item_id\"],\n        )\n        val_set = lgb.Dataset(\n            X_val.drop(drop_when_train, axis=1),\n            label=y_val,\n            categorical_feature=[\"item_id\"],\n        )\n        eval_result = {}\n        model = lgb.train(\n            bst_params,\n            train_set,\n            valid_sets=[train_set, val_set],\n            valid_names=[\"train\", \"valid\"],\n            evals_result=eval_result,\n            **fit_params,\n        )\n        models.append(model)\n\n        del idx_trn, idx_val, X_trn, X_val, y_trn, y_val\n        gc.collect()\n\n    return models, eval_result","a2980574":"bst_params =  {'lambda_l1': 0.00021301070633699974,\n               'lambda_l2': 9.242591106708853e-07,\n               'num_leaves': 31, \n               'feature_fraction': 0.584,\n               'bagging_fraction': 1.0, \n               'bagging_freq': 0, \n               'min_child_samples': 20, \n               'boosting_type': 'gbdt',\n               'metric': 'rmse',\n               'objective': 'poisson',\n               'n_jobs': -1,\n               'seed': 42,\n               'learning_rate': 0.03,\n               'min_data_in_leaf': 20}\n\nfit_params = {\n    \"num_boost_round\": 100_000,\n    \"early_stopping_rounds\": 100,\n    \"verbose_eval\": 100,\n}\n\nmodels, evals = train_lgb(\n    bst_params, fit_params, X_train, y_train, cv, drop_when_train=[day_col]\n)\n\ndel X_train, y_train\ngc.collect()","4ab908d8":"def rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","7170ce86":"lgb.plot_metric(evals)","f06f398b":"imp_type = \"gain\"\nimportances = np.zeros(X_test_pub.shape[1])\npreds_pub = np.zeros(X_test_pub.shape[0])\npreds_pri = np.zeros(X_test_pri.shape[0])\n\nfor model in models:\n    importances += model.feature_importance(imp_type)\n\npreds_pub = models[2].predict(X_test_pub)\npreds_pri = models[2].predict(X_test_pri)\nimportances = importances \/ cv.get_n_splits()","50be4bdf":"from mlflow_extend import mlflow, plotting as mplt\n\nwith mlflow.start_run():\n    mlflow.log_params_flatten({\"bst\": bst_params, \"fit\": fit_params, \"cv\": cv_params})\n\n\nfeatures = models[0].feature_name()\n_ = mplt.feature_importance(features, importances, imp_type, limit=30)","512f46a1":"## may be edit.\n\ndef make_submission(test_pub, test_pri, submission):\n    preds_pub = test_pub[[\"id\", \"date\", \"demand\"]]\n    preds_pub['id'] = preds_pub['id'].str.replace(\"_evaluation\", \"_validation\")\n    print(preds_pub['id'].head())\n    preds_pri = test_pri[[\"id\", \"date\", \"demand\"]]\n    # 01-28: validation\n    # 29-56: evaluation\n    val_dur  = preds_pub[\"date\"]<\"2016-05-23\"\n\n    preds_val  = preds_pub[val_dur]\n    preds_eval = preds_pri#[eval_dur]\n\n    preds_val = preds_val.pivot(index=\"id\", columns=\"date\", values=\"demand\").reset_index()\n    preds_eval = preds_eval.pivot(index=\"id\", columns=\"date\", values=\"demand\").reset_index()\n    \n    print(preds_val.shape)\n    print(preds_eval.shape)\n    \n    \n    preds_val.columns = [\"id\"] + [\"F\" + str(d+1) for d in range(28)]\n    preds_eval.columns = [\"id\"] + [\"F\" + str(d+1) for d in range(28)]\n    \n    \n    preds_val = preds_val[preds_val['id'].str.endswith(\"validation\")]\n    preds_eval = preds_eval[preds_eval['id'].str.endswith(\"evaluation\")]\n    \n    \n    vals = submission[submission[\"id\"].str.endswith(\"validation\")]\n    vals = submission[[\"id\"]].merge(preds_val, how=\"inner\", on=\"id\")\n    \n    evals = submission[submission[\"id\"].str.endswith(\"evaluation\")]\n    evals = submission[[\"id\"]].merge(preds_eval, how=\"inner\", on=\"id\")\n    \n    final = pd.concat([vals, evals])\n    \n    \n    return final","72fb40c4":"output = make_submission(id_date_pub.assign(demand=preds_pub), id_date_pri.assign(demand=preds_pri), submission)","d6f8e838":"output.to_csv(\"submission_private.csv\", index=False)","114b2e61":"## submission\nI also submitted public LB to check improvement my model.\n(ranking was not worked, but it worked for checking model improvement)","4ce669b6":"## Feature Selection\nI selected feature by feature importance in previous version notebook.  \nIn evaluation phase(after June 1st), I used `d_1914 - d_1941` sales as a train.","bfaf08db":"## model\nI used single LightGBM model.  \nOur task is predict amount of sales, so I selected `poisson` regression as `objective`.  \nI used `rmse` metric for loss function.\n\nI also tried `tweedie` regression, but `poisson` was better than that.  \nIn hyperparameter tuning, I used [Optuna](https:\/\/optuna.org\/) on my local PC because it took more than 9 hours to tuning.  \n\nThis notebook shows tuned parameters and that's score was the best in my all submission,   \nbut I selected \"NOT tuned model\" as a final submission...","7c0169df":"## feature engineering\nOur task was to predict sales for 28 days, so I make diff and roll features based on 28 days.  \nI also created more additional features, but allmost of them was not worked...","5c29cd34":"## libraries and helper function\n\n I did not edit these helper functions because these functions ware useful.  \nI only edited `reshape_sales` function for replace dataset (from `sales_train_validation.csv` to `sales_train_evaluation.csv`)","7b1ef45b":"## CV\nI used `CustomTimeSeriesSplitter`, which is based on [harupy's notebook](https:\/\/www.kaggle.com\/harupy\/m5-baseline\/).  \nI tryed other CV(TimeSeriesSplit from sklearn, group K-Fold),   \nbut not improve scores in my environment..","cb20f7d6":"## prediction\nI used only most recent timeseries model(fold 3) for prediction, not averaging folds.","be39aa08":"# Acknowledgment\nFirst of all, thanks to the organizers, Kaggle team and all participants.   \nAnd congratulations to all the winners and medal finishers!!  \nI will share my solution.  I'm sorry for poor English documentation...\n\n## base notebeook\nThis notebook was based on [m5 baseline](https:\/\/www.kaggle.com\/harupy\/m5-baseline) by [harupy](https:\/\/www.kaggle.com\/harupy).  \nThanks for sharing simple baseline model!  \n\n## mainly referenced EDA notebook\nI also referenced to [Back to (predict) the future - Interactive M5 EDA](https:\/\/www.kaggle.com\/headsortails\/back-to-predict-the-future-interactive-m5-eda) by [headsortails](https:\/\/www.kaggle.com\/headsortails).  \nThanks to this notebook, I could get many insights and ideas for feature engineering and modeling."}}