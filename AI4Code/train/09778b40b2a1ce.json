{"cell_type":{"7c8e3bb0":"code","91d97fc5":"code","8ae479b5":"code","79d1fb49":"code","5ae86c7b":"code","dbae7f03":"code","53e0cc6a":"code","9d916e79":"code","75ceadb8":"code","d9fbfd0a":"code","71aabda4":"code","c635a21e":"code","935ef216":"code","a652bc90":"code","f18852e6":"code","910202a3":"code","9f156f13":"code","d8161280":"code","42b9d4cf":"code","dc5c74ee":"code","ba8aa8c4":"code","d76f578e":"code","d228428c":"code","3bfeae20":"code","aead0c5b":"code","a220a857":"code","37527d16":"code","9c4cf562":"code","07a8d0a9":"code","316cc3e8":"code","c736b6b2":"code","41336285":"code","47ea9a3c":"code","7fad262e":"code","f44d489a":"code","e0aad7be":"code","e712b342":"code","e33b9994":"markdown","1fb54396":"markdown","9d8dcbf4":"markdown","e2b59788":"markdown","b413fcbe":"markdown","77a43bbd":"markdown","e1d4bccd":"markdown","45e0bf9c":"markdown","039f2569":"markdown","516b3ac0":"markdown","5f7730ea":"markdown","4c9a42cf":"markdown","deb5bf8a":"markdown","c4d78c3a":"markdown","06d86f24":"markdown","090282d0":"markdown","430011d7":"markdown","98718102":"markdown","f11d44cc":"markdown","b9c0fef3":"markdown","4b133299":"markdown","f4a33f70":"markdown","e3415769":"markdown","6af87d15":"markdown"},"source":{"7c8e3bb0":"## importing the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sbn\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split,KFold,RepeatedKFold,cross_val_score,cross_val_predict\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBRegressor\nfrom tqdm import tqdm","91d97fc5":"## loading the training and testing dataset \ntrain_data=pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\")\ntest_data=pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")","8ae479b5":"print(\"Training dataset\")\ndisplay(train_data.head())\nprint(\"\")\nprint(\"Testing Dataset\")\ndisplay(test_data.head())","79d1fb49":"print(\"Number of Rows in the training Dataset :\",train_data.shape[0])\nprint(\"NUmber of Rows in the Testing Dataset :\",test_data.shape[0])\n\nprint(\"Number of Columns in the Training dataset :\",train_data.shape[1])\nprint(\"Number of Columns in the Testing Dataset :\",test_data.shape[1])","5ae86c7b":"## descriptive stats of training dataset \ntrain_data.describe()","dbae7f03":"### describing about the test dataset\ntest_data.describe()","53e0cc6a":"print(\"MIssing values in the Training Dataset :\",train_data.isna().sum().sum())\nprint(\"Total Missing values in the Testing Dataset :\",test_data.isna().sum().sum())","9d916e79":"## Droping the misteseaus value from the dataset \ntrain_data.drop(train_data.loc[train_data.target<=0].index,axis=0,inplace=True)","75ceadb8":"train_data.reset_index(drop=True,inplace=True)","d9fbfd0a":"plt.figure(figsize=(15,5))\nplt.subplot(121)\nsbn.kdeplot(train_data.target)\nplt.title(\"Distribution of target\",size=15)\nplt.subplot(122)\nsbn.boxplot(train_data.target)\nplt.title(\"Boxplot of Target variable\",size=15)","71aabda4":"plt.figure(figsize=(10,5))\nfor i in range(1,8):\n    plt.subplot(240+i)\n    sbn.kdeplot(train_data.iloc[:,i])\nplt.suptitle(\"The Distribution first 7 Predicators\",size=14)","c635a21e":"plt.figure(figsize=(15,7))\nfor i in range(1,8):\n    plt.subplot(240+i)\n    sbn.boxplot(train_data.iloc[:,i])\nplt.suptitle(\"The Distribution first 7 Predicators\",size=14)","935ef216":"plt.figure(figsize=(10,5))\nfor i in range(1,8):\n    plt.subplot(240+i)\n    sbn.kdeplot(train_data.iloc[:,i+7])\nplt.suptitle(\"The Distribution Next 7 Predicators\",size=14)","a652bc90":"plt.figure(figsize=(15,7))\nfor i in range(1,8):\n    plt.subplot(240+i)\n    sbn.boxplot(train_data.iloc[:,i+7])\nplt.suptitle(\"Boxplots of Next 7 Predicators\",size=14)","f18852e6":"### lets define the correlation between the variables \ncorr=train_data.iloc[:,1:].corr().abs()\n### heatmap the highly correlated variables\nplt.figure(figsize=(12,8))\nsbn.heatmap(corr,cmap=\"Blues_r\",annot=True)","910202a3":"columns=list(corr.index)\ncorel=corr.values\nvar1=[]\nvar2=[]\nvalue=[]\nfor i in range(len(columns)):\n    for j in range(i+1,len(columns)):\n        var1.append(columns[i])\n        var2.append(columns[j])\n        value.append(corel[i,j])\ncorr_df=pd.DataFrame()\ncorr_df[\"var1\"]=var1\ncorr_df[\"var2\"]=var2\ncorr_df[\"value\"]=value\ncorr_df.sort_values(ascending=False,by=\"value\",inplace=True)","9f156f13":"corr_df.head(10)","d8161280":"## Lets observe the  relation between the target variable and other features \ncorr_df.loc[corr_df.var2==\"target\"].sort_values(by=\"value\",ascending=False)","42b9d4cf":"## LEts checking the first 7 feattures from both the datasets\nplt.figure(figsize=(15,6))\nfor i in range(1,8):\n    plt.subplot(240+i)\n    sbn.kdeplot(train_data.iloc[:,i],label=\"train\")\n    sbn.kdeplot(test_data.iloc[:,i],label=\"test\")\n    plt.title(columns[i-1])\nplt.suptitle(\"The Distributions of Features in Training and Testing dataset :\",size=14)","dc5c74ee":"plt.figure(figsize=(15,6))\nfor i in range(1,8):\n    plt.subplot(240+i)\n    sbn.kdeplot(train_data.iloc[:,i+7],label=\"train\")\n    sbn.kdeplot(test_data.iloc[:,i+7],label=\"test\")\n    plt.title(columns[i+6])\nplt.suptitle(\"The Distributions of Features in Training and Testing dataset :\",size=14)","ba8aa8c4":"def Feature_engineering(data):\n    Agg_df=pd.DataFrame()\n    Agg_df[\"mean\"]=data.mean(axis=1)\n    Agg_df[\"max\"]=data.max(axis=1)\n    Agg_df[\"min\"]=data.min(axis=1)\n    Agg_df[\"std\"]=data.std(axis=1)\n    Agg_df[\"kurtosis\"]=data.kurtosis(axis=1)\n    Agg_df[\"median\"]=data.median(axis=1)\n    Agg_df[\"skew\"]=data.skew(axis=1)\n    Agg_df[\"feat1\"]=train_data.cont5+train_data.cont4+train_data.cont11+train_data.cont3\n    Agg_df[\"feat2\"]=train_data.cont2+train_data.cont7+train_data.cont11+train_data.cont3\n    Agg_df[\"feat3\"]=train_data.cont7*train_data.cont2*train_data.cont3\n    Agg_df[\"feat4\"]=train_data.cont7*train_data.cont2*train_data.cont11\n    Agg_df[\"feat5\"]=train_data.cont7*train_data.cont2*train_data.cont12\n    Agg_df[\"feat6\"]=train_data.cont7*train_data.cont2*train_data.cont6\n    return Agg_df\n","d76f578e":"agg_df=Feature_engineering(train_data.iloc[:,1:-1])","d228428c":"plt.figure(figsize=(15,5))\nfor i in range(9):\n    plt.subplot(250+i+1)\n    sbn.kdeplot(agg_df.iloc[:,i])\nplt.suptitle(\"The Distribution of Agregated Features\",size=14)","3bfeae20":"agg_df.corrwith(train_data.target)","aead0c5b":"agg_df.corr()","a220a857":"import time","37527d16":"x=train_data.drop(columns=[\"id\",\"target\"])\n#x=pd.concat([x,agg_df.iloc[:,:]],axis=1)\ny=train_data[\"target\"]\n\n","9c4cf562":"### splitting the dataset \nx_train,x_val,y_train,y_val=train_test_split(x,y,test_size=0.2)\nprint(\"The shape of the training set :\",x_train.shape,y_train.shape)\nprint(\"The shape of the validation set :\",x_val.shape,y_val.shape)\n","07a8d0a9":"time.time()","316cc3e8":"%%time\n\ndt=DecisionTreeRegressor(max_depth=8)\ncv = KFold(n_splits=5, random_state=1, shuffle=True)\ni=0\nscore_train=[]\nscore_test=[]\n\nfor train_ind,test_ind in cv.split(x):\n    start=time.time()\n    i+=1\n    print(\"{}st Fold\".format(i))\n    x_train=x.iloc[train_ind,]\n    y_train=y[train_ind]\n    x_val=x.iloc[test_ind,]\n    y_val=y[test_ind]\n    dt.fit(x_train,y_train)\n\n    y_train_pre=dt.predict(x_train)\n    y_val_pre=dt.predict(x_val)\n    tr_error=np.sqrt(metrics.mean_squared_error(y_train,y_train_pre))\n    te_error=np.sqrt(metrics.mean_squared_error(y_val,y_val_pre))\n    score_test.append(te_error)\n    score_train.append(tr_error)\n    print(\"-------------->time taken {0:.2f} sec\".format(time.time()-start))\n    \n\nprint(\"=\"*50)\nprint(\"The Total Training  Error is :\",sum(score_train)\/len(score_train))\nprint(\"The Total validation Error is :\",sum(score_test)\/len(score_test))\n\n","c736b6b2":"%%time\nrf=RandomForestRegressor(max_depth=10,n_estimators=30,n_jobs=-1)\ncv = KFold(n_splits=5, random_state=1, shuffle=True)\ni=0\nscore_train=[]\nscore_test=[]\n\nfor train_ind,test_ind in cv.split(x):\n    start=time.time()\n    i+=1\n    print(\"{}st Fold\".format(i))\n    x_train=x.iloc[train_ind,]\n    y_train=y[train_ind]\n    x_val=x.iloc[test_ind,]\n    y_val=y[test_ind]\n    ## fitting the dataset \n    rf.fit(x_train,y_train)\n\n    y_train_pre=rf.predict(x_train)\n    y_val_pre=rf.predict(x_val)\n    tr_error=np.sqrt(metrics.mean_squared_error(y_train,y_train_pre))\n    te_error=np.sqrt(metrics.mean_squared_error(y_val,y_val_pre))\n    score_test.append(te_error)\n    score_train.append(tr_error)\n    print(\"-------------->time taken {0:.2f} sec\".format(time.time()-start))\n    \n\nprint(\"=\"*50)\nprint(\"The Total Training  Error is :\",sum(score_train)\/len(score_train))\nprint(\"The Total validation Error is :\",sum(score_test)\/len(score_test))\n","41336285":"%%time\nxgb=XGBRegressor(n_estimators=4000,learning_rate=0.01,max_depth=3,tree_method='gpu_hist')\ncv = KFold(n_splits=5, random_state=1, shuffle=True)\ni=0\nscore_train=[]\nscore_test=[]\n\nfor train_ind,test_ind in cv.split(x):\n    start=time.time()\n    i+=1\n    print(\"{}st Fold\".format(i))\n    x_train=x.iloc[train_ind,]\n    y_train=y[train_ind]\n    x_val=x.iloc[test_ind,]\n    y_val=y[test_ind]\n    ## fitting the dataset \n    xgb.fit(x_train,y_train)\n    \n    ## prediction\n    y_train_pre=xgb.predict(x_train)\n    y_val_pre=xgb.predict(x_val)\n    ##\n    ## error metric\n    tr_error=np.sqrt(metrics.mean_squared_error(y_train,y_train_pre))\n    te_error=np.sqrt(metrics.mean_squared_error(y_val,y_val_pre))\n    score_test.append(te_error)\n    score_train.append(tr_error)\n    print(\"-------------->time taken {0:.2f} sec\".format(time.time()-start))\n    \n\nprint(\"=\"*50)\nprint(\"The Total Training  Error is :\",sum(score_train)\/len(score_train))\nprint(\"The Total validation Error is :\",sum(score_test)\/len(score_test))\n","47ea9a3c":"regressor = XGBRegressor(\n                 colsample_bytree=0.5,\n                 alpha=0.01563,\n                 #gamma=0.0,\n                 learning_rate=0.01,\n                 max_depth=10,\n                 min_child_weight=257,\n                 n_estimators=4000,                                                                  \n                 #reg_alpha=0.9,\n                 reg_lambda=0.003,\n                 subsample=0.7,\n                 random_state=2020,\n                 metric_period=100,\n                 tree_method='gpu_hist',\n                 silent=1)\n\nregressor.fit(x_train, y_train, early_stopping_rounds=6, eval_set=[(x_val, y_val)], verbose=1)","7fad262e":"#aa=Feature_engineering(test_data.iloc[:,1:])\n#x_test=pd.concat([test_data,aa],axis=1)\ntest_pre=regressor.predict(test_data.iloc[:,1:])","f44d489a":"val_pre=regressor.predict(x_train)\nmse=metrics.mean_squared_error(y_train,val_pre)\nnp.sqrt(np.abs(mse))","e0aad7be":"submit=pd.DataFrame()\nsubmit[\"id\"]=test_data[\"id\"]\nsubmit[\"target\"]=test_pre","e712b342":"submit.to_csv(\"submission1.csv\",index=False)","e33b9994":"### Treatment of Outliers\n- The Predicators cont7 and cont9 has an outliers \n- And also Target variable also got some outliers\n\nActually Tree based Methods are robust to Ouliers So we are not going to treat them and not sure about Target variable\n\n**Even though Applied some trasformations to change the distribution as well as the adjust the outlier , there will be no use**","1fb54396":"### Getting top 10 highly correlated variables","9d8dcbf4":"## Experiment 1: with out adding derived Features\n\n### 1. Decision Trees","e2b59788":"- These features are also doesn't maintain the good relationship with the target variable.","b413fcbe":"## Feature Engineering\n- creating some aggreate features \n- like mean, max , min, standardeviation, kurtosis , median , skew","77a43bbd":"### Checking tthe Missing values ","e1d4bccd":"- Some of the Features are following the colse to the normal distribution that are mean , skew,std","45e0bf9c":"- Seems to be training dataset and testing dataset are came from the same distributions","039f2569":"## Exploratory Data Analysis\n\n**Univariate Analysis : Target**\n- let see how the target variable is distributed","516b3ac0":"- There is no strong relation between the features and target variable.","5f7730ea":"- The distribution is not following the Normal distribution \n\n**why we care about the predicator , or target variables need to to follow Normal distribution?**\n- IT is easier to decribe about the data\n- The entire distribution is describe by the two number i.e standard deviation and mean\n- it makes maths simple.\n- It is good to reduce the outlier effect.\n\n\n- In training u can use transformation techniques like box-cos, log,sqrt to get a normal distribution \n- At the time predition u can convert by using inverse transormation to get the actual values.\n\nIf we are using the linear regression:\n- let we have all the predicators and target are following the gaussian distribution .then resultant resgreesion line will suffer from little variance when predicting on unseen data.\n\nSo usually if features is follows a close normal distribution then the model will perform well.","4c9a42cf":"### Correlation Matrix\n\n- This will tells how the relation between the features and also it will tells relation betweent the features and Targets.\n- If you want to know the relation beween the features visulally you can use the scatter plots","deb5bf8a":"### XGboost Regressor","c4d78c3a":"### Creating a submission File.","06d86f24":"### Basic Descriptive statistics","090282d0":"### Plot the distribution from count1 to count7 predicators\n","430011d7":"**What if we have a different distribution . how to deal with that**\n\nThere are different methods to solve this :\nusing subsampling  https:\/\/maxhalford.github.io\/blog\/subsampling-1\/\n    - reweighting the training data so that the distribution of training is closer to the distribution of test using Kullback-Leibler Importance Estimation Procedure ","98718102":"- There are some outliers in the count_7 and count_9 \n\n**Algorithms :**\n- Linear regression, svm not robust to outliers\n- Tree based algorthim like Gradient boosting, Random Forest are robust to outliers\n\n**If we are planning to use the linear regression or svm then we need to handle the outlier instead of removing the ouliers use some transformation techniques or adjusting the esteeems**\n","f11d44cc":"- Here the Predicator copunt1 ---> has high correlation with the count12,count9,count10\n- and count6--> exhibits high correlation with the count13, count10, count12, count10, count12\n\n**Having the Multicorrelated varible doesn't effect the predictive power of a model .There will effect in the coefficient of the features. SO that if we have a Mulitcorrelated variable we cann't interpret the model properly**\n\n-> so Linear regression or ridge or lasso regressions are not robust with the highly correlated varible","b9c0fef3":"### Random Forest Model Selection","4b133299":"### Comparing the Distributions of  Features in Training and Testing dataset\n- Generally we will make an assumption that the unseen data will come from the same distribution as Training data.\n- But in real cases it not possible always \n- Mostly we can see the different distribution of datataset in time series problems. because the data will be varies with the time.\n\nLets check!!","f4a33f70":"## Algorithm Selection and Model Selection using k-fold","e3415769":"### Plotting the distribution of count8 to count 14","6af87d15":"- The variable \"skew\" and \"median\" has high correlation\n- The \"mean\" is high correlated with the 25_quantile and 75_quantiles and with median"}}