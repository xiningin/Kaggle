{"cell_type":{"a36c4f76":"code","f4719643":"code","99cf5d9f":"code","a91a1c24":"code","c8f671f6":"code","b2770a5a":"code","defc9b60":"code","b24dcc5a":"code","9c9b0388":"code","ae7c87cc":"code","02f01e0d":"code","5bf4af05":"code","2a170512":"code","a297b345":"code","032b1240":"code","ef7df0cd":"code","181452e9":"code","9edd1abc":"code","c12c139d":"code","352a868c":"code","428c02c5":"code","d74380cc":"code","e7dc421e":"code","42f87a6d":"code","4bac484b":"code","2f0a43cc":"code","5effa840":"code","658e4c04":"code","1e2e154a":"code","6d8acdd8":"code","9024a85e":"code","8c8a7b06":"code","e1a5e938":"code","83cf8d14":"code","57db110a":"code","ae351128":"markdown","3d45bc9a":"markdown","280a101d":"markdown","e69823dc":"markdown","8d7e7630":"markdown","a923bda9":"markdown","487e23b0":"markdown","3cc189f8":"markdown","1fc479a2":"markdown","b43386d4":"markdown","d1ec626f":"markdown","088c19b0":"markdown"},"source":{"a36c4f76":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f4719643":"#data visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#for reproducible results\n\nrandom_state=42\n\n#for ignoring warnings while executing the code cell\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","99cf5d9f":"#loading the data\n\ntrain=pd.read_csv('..\/input\/titanic\/train.csv')\ntest=pd.read_csv('..\/input\/titanic\/test.csv')","a91a1c24":"train.head()","c8f671f6":"test.head()","b2770a5a":"#checking the submission format\ngender=pd.read_csv('..\/input\/titanic\/gender_submission.csv')\ngender","defc9b60":"train.shape","b24dcc5a":"#Features in training set including the target variable- Survived\ntrain.columns","9c9b0388":"train.info()","ae7c87cc":"#Separating the target variable from predictors\n\nX=train.drop('Survived',axis=1)\ny=train['Survived']","02f01e0d":"X.shape,y.shape","5bf4af05":"#making our training set and validation set\nfrom sklearn.model_selection import train_test_split\nx_train,x_val,y_train,y_val=train_test_split(X,y,test_size=0.2,random_state=random_state)","2a170512":"#a={col:[train[col].isnull().sum(),len(train)-train[col].isnull().sum()] for col in train.columns if train[col].isnull().sum()>0}\nplt.figure(figsize=(14,5))\nsns.set_style('dark')\nf=[col for col in train.columns if train[col].isnull().any()]\nfor i in range(3):\n    plt.subplot(1,3,i+1)\n    sns.countplot(x=f[i],data=train.isnull())\n    plt.xticks(np.arange(2),['Non-Null','Null'],rotation=20)\nplt.tight_layout()","a297b345":"#Lets visualize the distribution of some continuous numerical variable(float64) to check their skewness\nplt.figure(figsize=(14,5))\ncol=['Age','Fare']\nfor i in range(len(col)):\n    plt.subplot(1,2,i+1)\n    sns.distplot(a=train[col[i]],kde_kws={'color':'green'},color='y')\nplt.tight_layout()\n# sns.kdeplot(data=train[col[0]],shade=True)\n# sns.kdeplot(data=train[col[1]],shade=True)","032b1240":"col1=['Pclass','SibSp','Parch']\nsns.catplot(x=col1[0],data=train,hue='Sex',kind='count',col='Survived')\nsns.catplot(x=col1[1],data=train,hue='Sex',kind='count',col='Survived')\nsns.catplot(x=col1[2],data=train,hue='Sex',kind='count',col='Survived')","ef7df0cd":"#This also verifies our observation of female passengers lives getting more priority.\ntab=pd.pivot_table(train,index='Sex',values='Survived',aggfunc=np.sum)\ndf=pd.DataFrame({'% of survived':[tab.loc['female','Survived']\/np.sum(tab)['Survived']*100,tab.loc['male','Survived']\/np.sum(tab)['Survived']*100]})\ndf.index=tab.index\ntab=pd.concat([tab,df],axis=1)\ntab","181452e9":"#This plot is not as significant but if we carefully see the high fare (around 500) their survival percentage is 100% which clearly indicates that upper class is being given the priority.\nsns.swarmplot(x=train['Survived'],y=train['Fare'])","9edd1abc":"categorical={col:train[col].nunique() for col in train.columns if str(train[col].dtype)=='object'}\nplt.title('Cardinality of Different Categorical Features')\ndf1=pd.DataFrame({'Categorical Features':list(categorical.keys()),'Cardinality':list(categorical.values())})\nsns.barplot(x=df1['Categorical Features'],y=df1['Cardinality'],palette='Blues_d',order=sorted(categorical,key=categorical.get))","c12c139d":"#dropping Name,Ticket,Cabin columns as discussed above.\nx_train.drop(columns=['Name','Ticket','Cabin'],inplace=True,axis=1)\nx_val.drop(columns=['Name','Ticket','Cabin'],inplace=True,axis=1)","352a868c":"#imputation for Embarked Column,since it is categorical we are treating it individually from rest of the column\nfrom sklearn.impute import SimpleImputer\n\nimpute=SimpleImputer(strategy='most_frequent')\n\nx_train_1=pd.DataFrame(impute.fit_transform(x_train[['Embarked']]))\nx_val_1=pd.DataFrame(impute.transform(x_val[['Embarked']]))\n\n# x_train.columns=cols\n# x_val.columns=cols\nx_train_1.columns=['Embarked']\nx_val_1.columns=['Embarked']\nx_train_1.index=x_train.index\nx_val_1.index=x_val.index\n\nx_train.drop('Embarked',axis=1,inplace=True)\nx_val.drop('Embarked',axis=1,inplace=True)\nx_train=pd.concat([x_train,x_train_1],axis=1)\nx_val=pd.concat([x_val,x_val_1],axis=1)\n\nx_train","428c02c5":"#Age is nearly uniformly distributed so ,we will impute its value with mean\nimpute_1=SimpleImputer(strategy='mean')\n\nx_train_2=pd.DataFrame(impute_1.fit_transform(x_train[['Age']]))\nx_val_2=pd.DataFrame(impute_1.transform(x_val[['Age']]))\n\n\nx_train_2.columns=['Age']\nx_val_2.columns=['Age']\nx_train_2.index=x_train.index\nx_val_2.index=x_val.index\n\nx_train.drop('Age',axis=1,inplace=True)\nx_val.drop('Age',axis=1,inplace=True)\nx_train=pd.concat([x_train,x_train_2],axis=1)\nx_val=pd.concat([x_val,x_val_2],axis=1)\n\nx_train","d74380cc":"print(\"{} null-values left in training-set\".format(x_train.isnull().any().sum()),end='\\n')\nprint(\"{} null-values left in validation-set\".format(x_val.isnull().any().sum()),end='\\n')","e7dc421e":"#fare is left-skewed ,so we need to scale it\nx_train.select_dtypes(include=['object'])","42f87a6d":"#one hot encoding the Sex and Embarked columns\nfrom sklearn.preprocessing import OneHotEncoder\nencode=OneHotEncoder(handle_unknown='ignore',sparse=False)\n\ncategorical_cols=list(x_train.select_dtypes(include=['object']).columns)\ncategorical_cols\n\nx_train_oh=pd.DataFrame(encode.fit_transform(x_train[categorical_cols]))\nx_val_oh=pd.DataFrame(encode.transform(x_val[categorical_cols]))\n\nx_train_oh.index=x_train.index\nx_val_oh.index=x_val.index\n\nx_train.drop(columns=categorical_cols,axis=1,inplace=True)\nx_val.drop(columns=categorical_cols,axis=1,inplace=True)\n\nx_train=pd.concat([x_train,x_train_oh],axis=1)\nx_val=pd.concat([x_val,x_val_oh],axis=1)\n\nx_train","4bac484b":"#Centering and Scaling happens independently on each feature,so we will pass all features in a list together which we need to transform\nfrom sklearn.preprocessing import StandardScaler\n\ncol_scale=['Age','Fare']\n\nscale=StandardScaler()\n\nx_train_scale=pd.DataFrame(scale.fit_transform(x_train[col_scale]))\nx_val_scale=pd.DataFrame(scale.transform(x_val[col_scale]))\n\nx_train_scale.index=x_train.index\nx_val_scale.index=x_val.index\n\nx_train_scale.columns=col_scale\nx_val_scale.columns=col_scale\n\nx_train.drop(columns=col_scale,axis=1,inplace=True)\nx_val.drop(columns=col_scale,axis=1,inplace=True)\n\nx_train=pd.concat([x_train,x_train_scale],axis=1)\nx_val=pd.concat([x_val,x_val_scale],axis=1)\n\nx_train","2f0a43cc":"#Effect of scaling the Age and Fare attribute\nsns.kdeplot(data=x_train.Age,shade=True)\nsns.kdeplot(data=x_train.Fare,shade=True)","5effa840":"#So,now we have dealt with categorical columns too.\nx_train.select_dtypes(include=['object']).columns","658e4c04":"#Test set preprocessing\ntest.drop(columns=['Ticket','Cabin','Name'],axis=1,inplace=True)","1e2e154a":"test_1=pd.DataFrame(impute_1.transform(test[['Age']]))\n\ntest_1.columns=['Age']\ntest_1.index=test.index\n\ntest.drop('Age',axis=1,inplace=True)\n\ntest=pd.concat([test,test_1],axis=1)\ntest\n\n\n\n","6d8acdd8":"#On further examining the test set i found that Fare column is also missing some values .So,I applied imputation here too.\nimpute_3=SimpleImputer(strategy='most_frequent')\n\ntest_2=pd.DataFrame(impute_3.fit_transform(test[['Fare']]))\n\ntest_2.columns=['Fare']\ntest_2.index=test.index\n\ntest.drop('Fare',axis=1,inplace=True)\n\ntest=pd.concat([test,test_2],axis=1)\ntest\n\n\n","9024a85e":"test_oh=pd.DataFrame(encode.transform(test[categorical_cols]))\n\ntest_oh.index=test.index\n\ntest.drop(columns=categorical_cols,axis=1,inplace=True)\n\ntest=pd.concat([test,test_oh],axis=1)\ntest","8c8a7b06":"test_scale=pd.DataFrame(scale.transform(test[col_scale]))\n\ntest_scale.columns=col_scale\ntest_scale.index=test.index\n\ntest.drop(columns=col_scale,axis=1,inplace=True)\n\ntest=pd.concat([test,test_scale],axis=1)\n\ntest","e1a5e938":"#We have just tried default models without any hyperparameter tweaking,i will try hyperparameter tuning in the next version of the notebook.\nfrom sklearn.linear_model import LogisticRegression as lr\nfrom sklearn.linear_model import SGDClassifier as sg\nfrom sklearn.ensemble import RandomForestClassifier as rf\nfrom sklearn.tree import DecisionTreeClassifier as dt\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nlog_reg=lr(random_state=random_state)\nsgd=sg(random_state=random_state)\nrandom_forest=rf(random_state=random_state)\ntree_model=dt(random_state=random_state)\nsvm_model=SVC(random_state=random_state)\nxgb=XGBClassifier(random_state=random_state)\n\nmodels={'Logistic Regression':log_reg,'SGD':sgd,'Random Forest':random_forest,'Decision Tree':tree_model,'SVM':svm_model,'XGBoost':xgb}\n\nfor model_name,model in models.items():\n    model.fit(x_train,y_train)\n    rslt=model.predict(x_val)\n    print('{}:{}'.format(model_name,np.sum(rslt==y_val)\/len(y_val)),end='\\n')","83cf8d14":"result=random_forest.predict(test)","57db110a":"output=pd.DataFrame({'PassengerId':test.PassengerId,\n                      'Survived':result})\noutput.to_csv('submission.csv',index=False)","ae351128":"> Insights from above plot:\n* Ticket and Name doesn't seem significant attributes for our task and their cardinality also is very high.We can't use one hot encoding for them,neither we can use label encoding since label encoding assumes a inherit order among their values,which doesn't seem the case here.So,it is better to drop both Name and Ticket columns.(Alongwith Cabin too,which we have covered before.)\n* We will use one hot encoding for Sex and Embarked due to their low cardinality.\n","3d45bc9a":"## Data Preprocessing","280a101d":"# Steps\n\n1. Importing the necessary libraries.\n2. Loading the data\/Understanding the Data.\n3. Data Visualization.\n4. Data Preprocessing.\n5. Trying Different Models.\n\n","e69823dc":"> Insights from above plot\n* Pclass plot indicated that upper class lives were given more priority than the lower ones.\n* We know from the movie,that female passengers were among the first being send to life boats,and it is well depicted in our plots too.\n* Families survival rate doesn't look as promising because there were not enough life boats to save each member of a family.Most of them saved were as individual but not as a whole family.","8d7e7630":"## Trying Different Models","a923bda9":"## Importing the necessary libraries","487e23b0":"> Random Forest looks most promising here.So we will try it.","3cc189f8":"*If you like this notebook,Please Upvote!!*","1fc479a2":"> Insights From above plot:\n* Age is almost uniformly distributed,but Fare is left-skewed so we will go with feature scaling to standardise their distribution.","b43386d4":"> Insights from above plot:\n* Cabin column will be dropped since it has so many null values.\n* We will impute the Age and Embarked columns to get rid of null values.","d1ec626f":"## Data Visualization","088c19b0":"## Loading The Data"}}