{"cell_type":{"e8c4662c":"code","08b4437b":"code","b53fae92":"code","96c9bd8e":"code","5cadff12":"code","c762a166":"code","430883f0":"code","8e8b6633":"code","6f14bf73":"code","61899d42":"code","8e5837fa":"code","1c90e142":"code","c8175884":"code","f81b7ef2":"code","8fb86bfb":"code","3e4e3e20":"code","54fd7d17":"code","97a9e7d9":"code","177da499":"code","19324885":"code","b702875f":"code","03aeb040":"code","6e3c9d43":"code","8cf273d5":"code","f62c4990":"code","927b7665":"markdown","207dfac2":"markdown","d3d8da84":"markdown","deb3157a":"markdown","a01676b2":"markdown","0b3552a8":"markdown","8d3aeacd":"markdown","8ec72796":"markdown","b62b685e":"markdown","8cefb405":"markdown","0731a565":"markdown","0685eb04":"markdown","cccfa6cd":"markdown","082ab259":"markdown","9e64c2a2":"markdown","42e9ca84":"markdown","06a9659e":"markdown","65418f76":"markdown","87abb4a7":"markdown","5166ce56":"markdown","6b549906":"markdown","5c0624d1":"markdown","7c931ab8":"markdown"},"source":{"e8c4662c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","08b4437b":"from __future__ import print_function\nfrom keras.callbacks import LambdaCallback\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import RMSprop\nfrom keras.optimizers import Adam\nfrom keras.utils.data_utils import get_file\nimport numpy as np\nimport random\nimport sys\nimport io\n\ndf = pd.read_csv('..\/input\/Donald-Tweets!.csv')\nprint(df.shape)\ndf.head()","b53fae92":"# lowercase all\ntext = df['Tweet_Text'].str.lower()\n","96c9bd8e":"np.random.seed(12345)\nnp.random.choice(text,10)","5cadff12":"print('BEFORE:')\nprint(text[0])\ntext = text.map(lambda s: ' '.join([x for x in s.split() if 'http' not in x]))\nprint('AFTER:')\nprint(text[0])","c762a166":"print('max tweet len:',text.map(len).max())\nprint('min tweet len:',text.map(len).min())\ntext.map(len).hist();\n","430883f0":"text = text[text.map(len)>60]\nlen(text)","8e8b6633":"chars = sorted(list(set(''.join(text))))\nprint('total chars:', len(chars))\nchar_indices = dict((c, i) for i, c in enumerate(chars))\nindices_char = dict((i, c) for i, c in enumerate(chars))\n","6f14bf73":"chars","61899d42":"for c in chars[-19:]:\n    print('\\nCHAR:', c)\n    smple = [x for x in text if c in x]\n    print(random.sample(smple,min(3,len(smple))))","8e5837fa":"import re\nfor c in chars[-19:]:\n    text = text.str.replace(c,'')","1c90e142":"chars = sorted(list(set(''.join(text))))\nprint('total chars:', len(chars))\nchar_indices = dict((c, i) for i, c in enumerate(chars))\nindices_char = dict((i, c) for i, c in enumerate(chars))\n\nchars","c8175884":"# cut the text in semi-redundant sequences of maxlen characters\nmaxlen = 40\nstep = 1\nsentences = []\nnext_chars = []\nfor x in text:\n    for i in range(0, len(x) - maxlen, step):\n        sentences.append(x[i: i + maxlen])\n        next_chars.append(x[i + maxlen])\nprint('nb sequences:', len(sentences))\n","f81b7ef2":"## check example\nfor i in range(3):\n    print(sentences[i],'==>',next_chars[i])","8fb86bfb":"text[0]","3e4e3e20":"print('Vectorization...')\nx = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\ny = np.zeros((len(sentences), len(chars)), dtype=np.bool)\nfor i, sentence in enumerate(sentences):\n    for t, char in enumerate(sentence):\n        x[i, t, char_indices[char]] = 1\n    y[i, char_indices[next_chars[i]]] = 1\n","54fd7d17":"print('Build model...')\nmodel = Sequential()\nmodel.add(LSTM(128, input_shape=(maxlen, len(chars))))\nmodel.add(Dense(len(chars), activation='softmax'))\n\n# optimizer = RMSprop(lr=0.01)\noptimizer = Adam()\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\n","97a9e7d9":"def sample(preds, temperature=1.0):\n    # helper function to sample an index from a probability array\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) \/ temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds \/ np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n","177da499":"for temperature in [0.1, 0.2, 0.3,  0.5, 1.0, 1.2, 1.3]:\n    print(sample([.1,.3,.5,.1],temperature=temperature))","19324885":"def on_epoch_end(epoch, _):\n    # Function invoked at end of each epoch. Prints generated text.\n    print()\n    print('----- Generating text after Epoch: %d' % epoch)\n    \n#     start_index = random.randint(0, len(text) - maxlen - 1)\n    tweet = np.random.choice(text) # select random tweet\n    start_index = 0\n\n    for diversity in [0.2, 0.5, 1.0, 1.2]:\n        print('----- diversity:', diversity)\n\n        generated = ''\n        sentence = tweet[start_index: start_index + maxlen]\n        generated += sentence\n        print('----- Generating with seed: \"' + sentence + '\"')\n        sys.stdout.write(generated)\n\n        for i in range(120):\n            x_pred = np.zeros((1, maxlen, len(chars)))\n            for t, char in enumerate(sentence):\n                x_pred[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(x_pred, verbose=0)[0]\n            next_index = sample(preds, diversity)\n            next_char = indices_char[next_index]\n\n            generated += next_char\n            sentence = sentence[1:] + next_char\n\n            sys.stdout.write(next_char)\n            sys.stdout.flush()\n        print()\n","b702875f":"epochs = 5\n\nprint_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n\nmodel.fit(x, y,\n          batch_size=128,\n          epochs=epochs,\n          callbacks=[print_callback])\n","03aeb040":"print('Build model...')\nmodel2 = Sequential()\nmodel2.add(LSTM(128, input_shape=(maxlen, len(chars)),return_sequences=True))\nmodel2.add(Dropout(0.2))\nmodel2.add(LSTM(128))\nmodel2.add(Dropout(0.2))\nmodel2.add(Dense(len(chars), activation='softmax'))\n\n# optimizer = RMSprop(lr=0.01)\noptimizer = Adam()\nmodel2.compile(loss='categorical_crossentropy', optimizer=optimizer)\n","6e3c9d43":"epochs = 60\n\nprint_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n\nmodel2.fit(x, y,\n          batch_size=128,\n          epochs=epochs,\n          callbacks=[print_callback])\n","8cf273d5":"def generate_w_seed(sentence,diversity):\n    sentence = sentence[0:maxlen]\n    print(f'seed: {sentence}')\n    print(f'diversity: {diversity}')\n    generated = ''\n    generated += sentence\n    \n    sys.stdout.write(generated)\n\n    for i in range(120):\n        x_pred = np.zeros((1, maxlen, len(chars)))\n        for t, char in enumerate(sentence):\n            x_pred[0, t, char_indices[char]] = 1.\n\n        preds = model.predict(x_pred, verbose=0)[0]\n        next_index = sample(preds, diversity)\n        next_char = indices_char[next_index]\n\n        generated += next_char\n        sentence = sentence[1:] + next_char\n\n        sys.stdout.write(next_char)\n        sys.stdout.flush()\n    print()\n    return","f62c4990":"for s in random.sample(list(text),5):\n    for diversity in [0.2, 0.5, 1.0, 1.2]:\n        generate_w_seed(s,diversity)\n        print()","927b7665":"maybe last ones are emojis?  \nLet's take a look at sentences with the weird chars.","207dfac2":"emojis?","d3d8da84":"# Print Test Sentence","deb3157a":"still weird....  \nlet's remove them","a01676b2":"<a id='1'><\/a>\n# Inspect Text","0b3552a8":"# Sampler\nWe don't want the next character to be the one with the highest probaility (we'll get the same results every time).  \nSo we sample with temperature parameter","8d3aeacd":"<a id='2'><\/a>\n# build the model: a single LSTM\n1. Define we have a sequential model\n2. Add an LSTM layer with 128 units. Input shape is a matrix of `maxlen` characters, where each character is a vector of `len(chars)`\n3. Add a dense layer (fully connected layer) and have the softmax activation pick a winner from the `len(chars)` possible characters.\n4. Pick an optimizer for the network and choose `categorical_crossentropy` loss function (used in multiclass classification).","8ec72796":"We will train the LSTM on 7,000 of Trump's tweets and end up with a machine that can tweet like the president.  \nAnd again, __this is easier than it sounds!__\n# Steps\n## [Preprocessing](#1)\n1. Inspect and preprocess the tweets\n2. Create dictionary of available characters and assign indeces.\n3. Create training set. We need to build our training set such that our model gets as input N characters and needs to predict character N+1.\n4. Vectorization: This means turning a character into a one-hot vector. If we have 30 possible characters, the letter 'd' (the fourth letter) will be represented by a vector of size 30, where all the values are zero except the fourth which will be one. When we have a sentence of 40 characters, we will have 40 such vectors.  \n\n## [Model Building](#2)\nEasy peasy with Keras. 4 lines of code (more details once we get to that section).  \nWe define some extra helper functions, train and score.  \nSince the model predicts letter N+1, we run scoring in a loop, and letter by letter, generate a presidential tweet.\n\n## [Experiment](#3)\nWe built a basic network.  \nLet's try and improve with a bigger network.\n","b62b685e":"Well, the model is not perfect, but this was a quick baseline model.  \nThings that can help with this and any other LSTM\n* adding more data (Kaggle has more Trumpian corpus that can help train our generator)\n* increase the network size\n* train longer\n* use more sophisticated architecture","8cefb405":"We will build the LSTM network described in https:\/\/keras.io\/examples\/lstm_text_generation\/. \nIf you want to learn more about LSTM, take a look at this post - http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/.  \nThe jist or it is that an LSTM does not only take a bunch of features and makes predicitons, but also takes the timeseries aspect of the features into consideration.  \n","0731a565":"example of what the function does:","0685eb04":"The purpose of this notebook is very simple:\n__To show you how easy it is to train and use an LSTM for text generation__","cccfa6cd":"# TODO","082ab259":"We gave the function an array where highest probability was index number 2 (.5).  \nWhen temperature was low, we got what we expected.  \nAs we increased the temperature, the function got more creative license choosing the max.  \nSo:\n* temperature helps us not get the same text generated every time\n* low temperature = text similar to trained data\n* high temperature = more creative generation\n* too high temperature = nonsense.","9e64c2a2":"# Create Input Data\ninput - 40 characters of a tweet  \noutput - next character  \n(make sure we don't combine different tweets into same corpus)","42e9ca84":"# Get Chars Dictionary","06a9659e":"<a id='3'><\/a>\n# Can we get better results on a bigger network?\n* Add epochs for better model\n* Add another LSTM layer and dropouts to avoid overfitting.  \n* Make sure to set `return_sequences` to True on the first LSTM layer so it will return the entire sequence not just the last output\n","65418f76":"Use only tweets of over 60 chars","87abb4a7":"# Generate Text at Epoch End","5166ce56":"### Inspect Chars","6b549906":"## Any super short tweets?","5c0624d1":"### any sentences we want to drop?\nwe probably want to keep the hashtags, but if there are for example random characters (like links), we want to drop them so they won't affct the precictions","7c931ab8":"# Vectorization\n1. Turn X into matrix of (numer_of_sentences,max_len_of_sentence,num_chars).  \nIf char i is number j char in sentence k, there will be a 1 in location (k,j,i)\n2. Turn y into a vector of (number_of_sentences,num_chars).  \nIf character z is the next character in sentence k, there will be a 1 in locaiton (k,z)"}}