{"cell_type":{"7f370e4c":"code","ff9fabf9":"code","e64bde23":"code","571a22f5":"code","30503a9f":"markdown","70529f36":"markdown","8f43cb39":"markdown","174c3501":"markdown","eb62661f":"markdown"},"source":{"7f370e4c":"df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\nprint( df.head())\nprint(df['text'])","ff9fabf9":"from nltk import sent_tokenize\n\ndf['tokenized_sents'] = df.apply(lambda column: sent_tokenize(column['text']), axis=1)\nprint(df.head())","e64bde23":"import re\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\n\nstop = stopwords.words('english')\ndf['tokenized_sents'] = df['text'].str.lower()\ndf['tokenized_sents'] = df['tokenized_sents'].apply(lambda x: re.sub(r'''(?i)\\b((?:https?:\/\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))''', '', str(x),flags=re.MULTILINE))\n\ndf['tokenized_sents'] = df['tokenized_sents'].apply(lambda x: re.sub(\"[^a-zA-Z]\",  \" \", str(x)))\ndf['tokenized_sents'] = df['tokenized_sents'].str.strip()\ndf['tokenized_sents'] = df.apply(lambda column: word_tokenize(column['tokenized_sents']), axis=1)\nprint(df[\"tokenized_sents\"])","571a22f5":"from nltk.stem.porter import PorterStemmer\nporter_stemmer = PorterStemmer()\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nimport numpy as np\n# stopword removal, stemming and lemmatization and then finding unique words\nallwords = []\nall_sentences = df['tokenized_sents'].tolist()\nfor slist in all_sentences:\n    for s in slist:\n        if s not in stop: \n            allwords.append(wordnet_lemmatizer.lemmatize(porter_stemmer.stem(s)))\n\nset_allwords = set(allwords)\nallwords_unique = list(set_allwords)\nprint(\"length of unique words \", len(allwords_unique))\n\n# finding tfidf for unique words \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nvectorizer = TfidfVectorizer(stop)\nX = vectorizer.fit_transform(allwords_unique)\n\n# Training K-means for k=2 (real or not ) \ntrue_k = 2\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\nmodel.fit(X)\nlabels = set(model.labels_)\nlabels = list(labels)\ncount =0\n\n#testing the trained model on test dataset\n\n# read test.csv\nimport pandas as pd\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\n\n# dataframe for storing test results based on sample submission format\ndf_test_results = pd.DataFrame( columns=[\"id\", \"target\"])\n# for each tweet text in test dataset, preprocess \"text\" and then transform it and used the transformed representation for prediction\nfor x in df_test[\"text\"]:\n    text = re.sub(r'''(?i)\\b((?:https?:\/\/|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}\/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?\u00ab\u00bb\u201c\u201d\u2018\u2019]))''', '', x.lower(),flags=re.MULTILINE)\n    text = re.sub(\"[^a-zA-Z]\",  \" \", text)\n    text = text.strip()\n    words = word_tokenize(text)\n    for w in words:\n        w = wordnet_lemmatizer.lemmatize(porter_stemmer.stem(w))\n    X = vectorizer.transform(list(set(words)))\n    predicted = model.predict(X)\n    # the predicted model returns a list of integers containing predicted label for each word. The idea is to assign the most frequently occuring label to the overall tweet.\n    if np.count_nonzero(predicted == labels[0]) >  np.count_nonzero(predicted == labels[1]) :\n        predicted_label = labels[0]\n#         print(predicted_label)\n        df_test_results.at[count,'target'] = predicted_label\n    else:\n        predicted_label = labels[1]\n#         print(predicted_label)\n        df_test_results.at[count,'target'] = predicted_label\n\n#     count = count + 1\ndf_test_results[\"id\"] = df_test[\"id\"]\n\n# print(df_test_results)\nexport_csv = df_test_results.to_csv (r'ArjumandFatima_NLP_DisasterTweet_Submission1.csv', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n\n","30503a9f":"# The overall idea\nThis is my first attempt in building a prediction model. Not sure how to do that, but just giving it a try by using one of the simplest classification techniques i.e. K-Means. The first step is to read the train.csv file into a dataframe. As keywords and location attributes contain missing values for some instances so I am ignoring these attributes for the time-being. At the simplest level, I will consider text (tweets) only. \nNow that we have decided to use \"text\" attribute for building our model, the next step is to transform \"text\" into a suitable representation which can be fed to the prediction model. For this purpose, we can use TF-IDF. \nA simplest way is to pass the whole \"text\" column of our dataframe into the tfidf vectorizer for transformation without any text-preprocessing. This method resulted in an accuracy of 42.965979 % on the training dataset. In order to improve it a bit, we can do the following preprocessing steps.\n1. Tokenize sentences\n2. Convert into lowercase\n3. Remove urls\/hyperlinks\n4. Remove digits\n5. Remove starting and ending spaces\n6. Tokenize into words\nAfter applying the above steps, we store the processed text for each instance in a seperate column; lets call it \"tokenized_sents\"  \nThen we convert this column into a list of sentences (containing all tweets with preprocessing applied). Next step is to perform stemming and lemmatization and removing stop words, after which we get a list of words. We can convert this list to set, for getting unique words and then convert that set back to list for further processing. This list of unique words is then transformed using tfidf vectorizer and then used to train k-means prediction model. ","70529f36":"Save tokenized sentences of \"text\" in another column","8f43cb39":"# Step by step working\nReading the training dataset into a dataframe","174c3501":"The next step is to remove stopwords, do stemming and lemmatization. After performing these steps, we find unique words and then vectorize them using tf-idf. These vectorized words are then using for training our model. ","eb62661f":"Applying preprocessing on \"tokenized_sents\""}}