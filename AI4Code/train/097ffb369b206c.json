{"cell_type":{"6fccc233":"code","d059178f":"code","8ba6d7ba":"code","99569b12":"code","54c641d7":"code","cf4a358c":"code","58cfe9de":"code","e5fa7b27":"code","e3d827ee":"code","ff3ee312":"code","8688d88f":"code","d5ff6080":"code","c63384d0":"code","c7c8e86e":"code","abe2a819":"code","e5893df3":"code","49525422":"code","48cd26f3":"code","0207cba0":"code","77f45989":"code","82821ab8":"code","f438ad1b":"code","e790b7c1":"code","cabcb0e2":"code","d800034e":"code","95e509a9":"code","0e81ff6a":"code","abfc9435":"code","54550921":"code","eba61596":"code","c2778f0e":"code","100f7f73":"code","095513df":"code","1dbd17e4":"code","c2f697f2":"code","a6bbeb45":"code","339b4e66":"code","44566083":"code","afa7c100":"code","33486f85":"code","f049d72c":"code","fefe3c94":"code","4e43c4c4":"code","6f7ca27a":"code","53cd0f8c":"markdown","de234fae":"markdown","005099b2":"markdown","52d49443":"markdown","b94b7611":"markdown","87999445":"markdown","e1ec1ac8":"markdown","073694f4":"markdown","8f5a260e":"markdown"},"source":{"6fccc233":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d059178f":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom collections import Counter\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom nltk import word_tokenize","8ba6d7ba":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","99569b12":"train.head()","54c641d7":"train.isna().sum()","cf4a358c":"train.info()","58cfe9de":"train.describe()","e5fa7b27":"train.location.value_counts()","e3d827ee":"train.keyword.value_counts()","ff3ee312":"tokens = [word_tokenize(texts) for texts in train.text]\nlen_tokens=[]\n\nfor i in range(len(tokens)):\n    len_tokens.append(len(tokens[i]))","8688d88f":"train[\"text_tokens\"] = len_tokens","d5ff6080":"train.text = train.text.str.replace('[^\\w\\s]','')\ntrain.text = train.text.str.lower()\ntrain.head()","c63384d0":"X = train.drop(\"target\", axis = 1)\ny = train[\"target\"]\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)\nX_train.shape, y_train.shape, X_val.shape, y_val.shape","c7c8e86e":"cat = [\"keyword\", \"location\"]\nimputer = SimpleImputer(strategy=\"constant\", fill_value=\"missing\")\ntransformer = ColumnTransformer([(\"imputer\", imputer, cat)], remainder=\"passthrough\")\n\n# fill simple imputer with X values since you want to fill only the features, not the target\nfilled_X = transformer.fit_transform(X_train)\n\nfilled = pd.DataFrame(filled_X, columns=[\"id\", \"keyword\", \"location\", \"text\", \"text_tokens\"])\nfilled.head()","abe2a819":"# removing URLs\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    # replace the pattern url in text with None\n    return url.sub(r'',text)","e5893df3":"# remove html tags\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)","49525422":"# remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","48cd26f3":"# remove stopwords\ndef remove_stopwords(text):\n    words = [w for w in text if w not in stopwords]\n    return words","0207cba0":"import re","77f45989":"train['text'] = train['text'].apply(lambda x : remove_URL(x))\ntest['text'] = test['text'].apply(lambda x : remove_URL(x))\ntrain.head()","82821ab8":"train['text'] = train['text'].apply(lambda x : remove_html(x))\ntest['text'] = test['text'].apply(lambda x : remove_html(x))\ntrain.head()","f438ad1b":"train['text'] = train['text'].apply(lambda x : remove_emoji(x))\ntest['text'] = test['text'].apply(lambda x : remove_emoji(x))\ntrain.head()","e790b7c1":"from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nstopwords = ENGLISH_STOP_WORDS","cabcb0e2":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)","d800034e":"stopwords = set(STOPWORDS)\ntext = \" \".join(text for text in train.text)\ncloud = WordCloud(stopwords=stopwords, background_color=\"white\").generate(text)\nplt.imshow(cloud);","95e509a9":"# missing values\na = sns.heatmap(train.isnull())\nplt.suptitle(\"Null Values\");\nplt.xticks(rotation=90);","0e81ff6a":"zero = train[train.target==0].text_tokens\none = train[train.target==1].text_tokens","abfc9435":"fig, (ax0, ax1) = plt.subplots(1,2, figsize=(10,5))\nax0.hist(zero,color='purple')\nfig.suptitle('Number of Tokens in Text')\nax0.set_title(\"Not Disaster\")\nax1.set_title(\"Disaster\")\nax1.hist(one,color='blue');","54550921":"zero = train[train.target==0].text.str.len()\none = train[train.target==1].text.str.len()","eba61596":"fig, (ax0, ax1) = plt.subplots(1,2, figsize=(10,5))\nax0.hist(zero,color='purple')\nax0.set_title(\"Not Disaster\")\nax1.set_title(\"Disaster\")\nfig.suptitle(\"Number of Characters in Text\")\nax1.hist(one,color='blue');","c2778f0e":"sns.catplot(x=\"target\",data=train,  kind=\"count\")\nplt.suptitle(\"Target Comparison\");","100f7f73":"corr = train.corr()\nsns.set(rc={'figure.figsize':(5,5)})\nsns.heatmap(corr)\nplt.suptitle(\"Correlation\");","095513df":"zero = train[train.target==0].text\none = train[train.target==1].text","1dbd17e4":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\n\nword = one.str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax = ax1,color='blue')\nax1.set_title('Disaster')\n\nword = zero.str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='red')\nax2.set_title('Not Disaster')\nfig.suptitle('Average Word Length');","c2f697f2":"from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS","a6bbeb45":"stopwords = ENGLISH_STOP_WORDS","339b4e66":"vect = TfidfVectorizer(max_features = 10, stop_words=stopwords)\ntfIdf = vect.fit(train.text)\nX = vect.transform(train.text)\nX_df = pd.DataFrame(X.toarray(), columns = vect.get_feature_names())\nX_df","44566083":"vectorizer = CountVectorizer(ngram_range=(1,3), max_features = 100, max_df=500, stop_words= stopwords)\nvectorizer.fit(train.text)\nX = vectorizer.transform(train.text)\nBoW = pd.DataFrame(X.toarray(), columns= vectorizer.get_feature_names())\nBoW","afa7c100":"X = BoW\ny = train[\"target\"]","33486f85":"X_train , X_test , y_train , y_test = train_test_split(X, y,test_size=0.20, random_state=55, shuffle =True)","f049d72c":"from sklearn.tree import DecisionTreeClassifier\n\n\n\ndecisionTreeModel = DecisionTreeClassifier(criterion= 'entropy',\n                                           max_depth = None, \n                                           splitter='best', \n                                           random_state=55)\n\ndecisionTreeModel.fit(X_train, y_train);\n\n\n# ### Gradient Boosting\n\n\n\n\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n\n\n\n\ngradientBoostingModel = GradientBoostingClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 100,\n                                                   max_depth = 30,\n                                                   random_state=55)\n\ngradientBoostingModel.fit(X_train,y_train);\n\n\n# ### K-Nearest Neighbors\n\n\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\n\n\n\nKNeighborsModel = KNeighborsClassifier(n_neighbors = 7,\n                                       weights = 'distance',\n                                      algorithm = 'brute')\n\nKNeighborsModel.fit(X_train,y_train);\n\n\n# ### Logistic Regression Model\n\n\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n\n\n\n\nLogisticRegression = LogisticRegression(penalty='l2', \n                                        solver='saga', \n                                        random_state = 55)  \n\nLogisticRegression.fit(X_train,y_train);\n\n\n# ### Bernoulli Naive Bayes Model\n\n\n\nfrom sklearn.naive_bayes import BernoulliNB\n\n\n\n\nbernoulliNBModel = BernoulliNB(alpha=0.1)\nbernoulliNBModel.fit(X_train,y_train);","fefe3c94":"from sklearn.metrics import f1_score\n\n\n\nmodels = [decisionTreeModel, gradientBoostingModel, KNeighborsModel, LogisticRegression, bernoulliNBModel]\n\nfor model in models:\n    print(type(model).__name__,' Train Score is   : ' ,model.score(X_train, y_train))\n    print(type(model).__name__,' Test Score is    : ' ,model.score(X_test, y_test))\n    \n    y_pred = model.predict(X_test)\n    print(type(model).__name__,' F1 Score is      : ' ,f1_score(y_test ,y_pred))\n    print('********************************************************************')","4e43c4c4":"test","6f7ca27a":"TRAIN_FEATURES = [\"id\", \"keyword\", \"location\", \"text\", \"target\",\"text_tokens\"]\nTEST_FEATURES = [\"id\", \"keyword\", \"location\", \"text\"]\n\ntrain[TRAIN_FEATURES].to_pickle('train.pkl')\ntest[TEST_FEATURES].to_pickle('test.pkl')\n\nprint('Training Set Shape = {}'.format(train[TRAIN_FEATURES].shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(train[TRAIN_FEATURES].memory_usage().sum() \/ 1024**2))\nprint('Test Set Shape = {}'.format(test[TEST_FEATURES].shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(test[TEST_FEATURES].memory_usage().sum() \/ 1024**2))","53cd0f8c":"### TF-IDF","de234fae":"### Bag of Words","005099b2":"## Fill Missing Data","52d49443":"# Preprocessing","b94b7611":"# Fit a Model","87999445":"# Importing Libraries","e1ec1ac8":"# Data Cleaning","073694f4":"## Word Cloud","8f5a260e":"## Data Visualization"}}