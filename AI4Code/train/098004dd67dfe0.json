{"cell_type":{"a91e26a4":"code","fd973b57":"code","ea3f3259":"code","3212eef0":"code","3b5c69ea":"code","93f6053e":"code","4dff9f3e":"code","10f83dc3":"code","7e291a42":"code","024160cc":"code","92df617a":"code","14a6f042":"code","dccb05fb":"code","cd1128a7":"code","f2e5d1e9":"code","1b3022ac":"code","dfeaabed":"code","5abe7bb5":"code","73bc1147":"code","07c7a58a":"code","5cbaa6c4":"code","46f11e27":"code","8cb9baa2":"code","757421ba":"code","45894726":"code","426a643c":"code","f90b1f3d":"code","1ddd6f49":"code","26a35f96":"code","7a15b572":"code","510f9c6c":"code","92406d2d":"code","fe712304":"code","dfd4504d":"code","c88641b0":"code","2e5f864d":"code","bc7d18f6":"code","206b6371":"code","8853bfae":"code","04417980":"markdown","e668cb6a":"markdown","c54d0553":"markdown","f60569f4":"markdown","35098912":"markdown","5606a9b3":"markdown","1ec0035a":"markdown","7d821b8c":"markdown","b398d544":"markdown","f7281a82":"markdown","d848f889":"markdown","b2b715ae":"markdown","dfb13bf5":"markdown","ed287113":"markdown","f8c721ce":"markdown","8304b484":"markdown","3c176cf1":"markdown"},"source":{"a91e26a4":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","fd973b57":"dataset = pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')","ea3f3259":"dataset.isnull().sum()","3212eef0":"sns.set(style=\"whitegrid\")","3b5c69ea":"sns.distplot(dataset.salary)","93f6053e":"dataset['salary']=dataset['salary'].fillna(0)","4dff9f3e":"sns.barplot(x = dataset['gender'],y = dataset['salary'])","10f83dc3":"sns.barplot(x = dataset['gender'],y = dataset['ssc_p'])","7e291a42":"sns.barplot(x = dataset['gender'],y = dataset['hsc_p'])","024160cc":"sns.barplot(x = dataset['gender'],y = dataset['degree_p'])","92df617a":"sns.barplot(x = dataset['gender'],y = dataset['mba_p'])","14a6f042":"sns.boxplot(x = dataset['status'], y = dataset['mba_p'])","dccb05fb":"sns.boxplot(x = dataset['status'], y = dataset['degree_p'])","cd1128a7":"sns.boxplot(x = dataset['status'], y = dataset['hsc_p'])","f2e5d1e9":"sns.boxplot(x = dataset['status'], y = dataset['ssc_p'])","1b3022ac":"#dataset=dataset[dataset.salary<600000]","dfeaabed":"sns.jointplot(x = dataset['ssc_p'], y = dataset['salary'], kind='hex')","5abe7bb5":"sns.jointplot(x = dataset['hsc_p'], y = dataset['salary'], kind='hex')","73bc1147":"sns.jointplot(x = dataset['degree_p'], y = dataset['salary'], kind='hex')","07c7a58a":"sns.jointplot(x = dataset['mba_p'], y = dataset['salary'], kind='hex')","5cbaa6c4":"sns.jointplot(x = dataset['etest_p'], y = dataset['salary'], kind='hex')","46f11e27":"numeric_data = dataset.select_dtypes(include=[np.number])\ncat_data = dataset.select_dtypes(exclude=[np.number])\nprint (\"There are {} numeric and {} categorical columns in train data\".format(numeric_data.shape[1],cat_data.shape[1]))","8cb9baa2":"dataset.dtypes","757421ba":"# Label Encoding\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndef encoder(df,col_name):\n    df[col_name] = le.fit_transform(dataset[col_name])\n    \nencoder(dataset,'gender')\nencoder(dataset,'ssc_b')\nencoder(dataset,'hsc_b')\nencoder(dataset,'hsc_s')\nencoder(dataset,'degree_t')\nencoder(dataset,'workex')\nencoder(dataset,'specialisation')\nencoder(dataset,'status')","45894726":"# splitting dataset\n# removing salary feature as it is a dependent feature\ntarget = dataset['status']\ndrop = ['sl_no','status','salary']\ntrain = dataset.drop(drop,axis=1)","426a643c":"# Using SMOTE to balance the categories\n\nfrom imblearn.combine import SMOTETomek\nsmk = SMOTETomek(random_state = 42)\ntrain, target = smk.fit_sample(train,target)","f90b1f3d":"#Now we will split the dataset in the ratio of 75:25 for train and test\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train, target, test_size = 0.25, random_state = 0)","1ddd6f49":"from sklearn.tree import DecisionTreeClassifier\nmodel=DecisionTreeClassifier(criterion='gini', splitter='best',\n                             max_depth=5, min_samples_split=2,\n                             min_samples_leaf=1, min_weight_fraction_leaf=0.0,\n                             max_features=None, random_state=None,\n                             max_leaf_nodes=None, min_impurity_decrease=0.0, \n                             min_impurity_split=None, class_weight=None, \n                             presort='deprecated', ccp_alpha=0.0)\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)","26a35f96":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\naccuracy=accuracy_score(y_test,y_pred) \nprecision=precision_score(y_test,y_pred,average='weighted')\nrecall=recall_score(y_test,y_pred,average='weighted')\nf1=f1_score(y_test,y_pred,average='weighted')\n\nprint('Accuracy - {}'.format(accuracy))\nprint('Precision - {}'.format(precision))\nprint('Recall - {}'.format(recall))\nprint('F1 - {}'.format(f1))","7a15b572":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, y_pred)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","510f9c6c":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\n\ndisp = plot_precision_recall_curve(model, X_test, y_test)\ndisp.ax_.set_title('2-class Precision-Recall curve: '\n                   'AP={0:0.2f}'.format(average_precision))","92406d2d":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        \n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","fe712304":"from sklearn.metrics import confusion_matrix\ncnf_matrix = confusion_matrix(y_test,y_pred)\nnp.set_printoptions(precision=2)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","dfd4504d":"from sklearn.naive_bayes import GaussianNB\nnb_model=GaussianNB()\nnb_model.fit(X_train,y_train)\ny_pred=nb_model.predict(X_test)","c88641b0":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\naccuracy=accuracy_score(y_test,y_pred) \nprecision=precision_score(y_test,y_pred,average='weighted')\nrecall=recall_score(y_test,y_pred,average='weighted')\nf1=f1_score(y_test,y_pred,average='weighted')\n\nprint('Accuracy - {}'.format(accuracy))\nprint('Precision - {}'.format(precision))\nprint('Recall - {}'.format(recall))\nprint('F1 - {}'.format(f1))","2e5f864d":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(y_test, y_pred)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","bc7d18f6":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt\n\ndisp = plot_precision_recall_curve(model, X_test, y_test)\ndisp.ax_.set_title('2-class Precision-Recall curve: '\n                   'AP={0:0.2f}'.format(average_precision))","206b6371":"import itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        \n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","8853bfae":"from sklearn.metrics import confusion_matrix\ncnf_matrix = confusion_matrix(y_test,y_pred)\nnp.set_printoptions(precision=2)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cnf_matrix\n                      , classes=class_names\n                      , title='Confusion matrix')\nplt.show()","04417980":"# Data Pre-processing","e668cb6a":"Almost each and every student has to go through placement process during the final year of their respective university courses. But what factors really affect the placement of a student? Is it their college degree percentage? School percentage? Performance in the entrance test of company?\nWe will be studying about all these factors in the given notebook. Not only this, we will also be developing a predictive model to classify if a given student can get a placement or not.","c54d0553":"# Exploratory Data Analysis","f60569f4":"## Naive Baye's","35098912":"Observation 2 - Students with higher percentages\/better academic results were able to perform well during placements compared to those who had relatively lower academic results","5606a9b3":"Observation 3 - Students with average percentage of 60-70 are able to get around 250000 INR anually. Higher percentage does not neccesarily corresponds to higher salary package.","1ec0035a":"# Introduction","7d821b8c":"## Gender based analysis","b398d544":"We will fill missing salaries with 0s as they represent students who have not got placement offer. ","f7281a82":"Observation 1 - Men are getting higher salaries than women even when women scored higher percentages during their school and college degrees.","d848f889":"## Placement status based analysis","b2b715ae":"Decision Trees are performing better with an accuracy of 82% in comparison to Naive Baye's which has an accuracy of 72%. We can further improve the accuracy by using ensembling techniques like Random Forest and Gradient Boosted Trees. Salary feature was removed because it was a dependent feature. ","dfb13bf5":"### We will be using two models for comparison, one tree based and one non-tree based.\n\n### 1. Decision Trees\n### 2. Naive Baye's","ed287113":"## Decision Trees","f8c721ce":"## Salary vs Academic results","8304b484":"# Model Building","3c176cf1":"# Conclusion"}}