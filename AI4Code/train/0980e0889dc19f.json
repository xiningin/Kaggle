{"cell_type":{"339efee3":"code","89ce45a0":"code","28e10a39":"code","91cd1a11":"code","a64034ef":"code","7364215c":"code","a0a45df3":"code","ee2c84c8":"code","8519b440":"code","cd89aab0":"code","f14d5205":"code","5007b244":"code","b92e97b2":"code","eec92d0f":"code","c141685b":"code","783de65b":"code","44988378":"code","701d1094":"code","42510186":"code","d4ecc9fe":"code","d5b93586":"code","605a3eb7":"code","f5de8481":"code","930a17d6":"code","e46b2b30":"code","f73809cf":"code","e4ef2263":"code","3fda724a":"code","8465afc7":"markdown","50c4c17f":"markdown","355bbba2":"markdown","050b38f8":"markdown","22b5bf9d":"markdown","92243c41":"markdown","866dcaf1":"markdown","fd8bb8c0":"markdown","91c1349a":"markdown","b0ea6e6c":"markdown","737c6342":"markdown","799b3fd9":"markdown","67041154":"markdown","08576256":"markdown","b6e81173":"markdown","f4a74356":"markdown","15c75634":"markdown","6ffd3f9a":"markdown","3d16d8e3":"markdown","57cf2d2a":"markdown","6511db01":"markdown","d6cca051":"markdown","e4193d2b":"markdown","6d940947":"markdown","3690f187":"markdown","2c4917d8":"markdown"},"source":{"339efee3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","89ce45a0":"data=pd.read_csv('\/kaggle\/input\/usi-nlp-practicum-2\/imdb_train.csv')#.sample(1000).reset_index(drop=True)","28e10a39":"data.head()","91cd1a11":"data.iloc[0,0]","a64034ef":"{lbl:i for i,lbl in enumerate(data['sentiment'].unique().tolist())}","7364215c":"import transformers\nimport torch\nimport torch.nn as nn","a0a45df3":"class config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 16\n    VALID_BATCH_SIZE = 16\n    EPOCHS = 100\n    BERT_PATH = \"..\/input\/bert-base-uncased\/\"\n    MODEL_PATH = \"trained_model.bin\"\n    TRAINING_FILE = '\/kaggle\/input\/usi-nlp-practicum-2\/imdb_train.csv'\n    TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)","ee2c84c8":"config.TOKENIZER","8519b440":"config.TOKENIZER.tokenize('xsxxxifdse') #[ ...]","cd89aab0":"config.TOKENIZER.convert_tokens_to_ids(['i', 'like', 'the', 'movie'])","f14d5205":"class Dataset:\n    def __init__(self, df,text_col=None,target_col=None):\n        self.df = df\n        self.text_col = text_col\n        self.target = target_col\n        self.target_mapping={lbl:i for i,lbl in enumerate(self.df[self.target].unique().tolist())}\n        self.tokenizer = config.TOKENIZER\n        self.max_len = config.MAX_LEN\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, item):\n        text = str(self.df.loc[item,self.text_col])\n        text= \" \".join(text.split())\n        target= self.target_mapping[self.df.loc[item,self.target]]\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n        )\n\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        return {\n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n            \"targets\": torch.tensor(target, dtype=torch.long),\n        }","5007b244":"dataset=Dataset(data,'review','sentiment')","b92e97b2":"dataset[0]","eec92d0f":"targets_mapping=dataset.target_mapping\ntargets_mapping","c141685b":"class BERTBaseUncased(nn.Module):\n    def __init__(self, labels=2):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH)\n        self.bert_drop = nn.Dropout(0.1)\n        self.out = nn.Linear(768,labels)\n\n    def forward(self, ids, mask, token_type_ids):\n        _, o2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        bo = self.bert_drop(o2)\n        output = self.out(bo)\n        return output","783de65b":"labels= len(targets_mapping)\nmodel=BERTBaseUncased(labels)","44988378":"print(model)","701d1094":"def loss_fn(outputs, targets):\n    loss_fct = nn.CrossEntropyLoss()\n    loss = loss_fct(outputs, targets)\n    return loss","42510186":"def train_fn(data_loader, model, optimizer, device, scheduler):\n    model.train()\n\n    for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets = d[\"targets\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.long)\n\n        optimizer.zero_grad()\n        outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()","d4ecc9fe":"def eval_fn(data_loader, model, device):\n    model.eval()\n    fin_targets = []\n    fin_outputs = []\n    with torch.no_grad():\n        for bi, d in tqdm(enumerate(data_loader), total=len(data_loader)):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n            #\n            outputs = model(ids=ids, mask=mask, token_type_ids=token_type_ids)\n            output_class=torch.argmax(torch.softmax(outputs,dim=-1),dim=-1)\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(output_class.cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","d5b93586":"from sklearn import model_selection\nfrom sklearn import metrics\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","605a3eb7":"dfx = pd.read_csv(config.TRAINING_FILE).fillna(\"none\")\n\ndf_train, df_valid = model_selection.train_test_split(\n    dfx, test_size=0.1, random_state=42\n)\n\ndf_train = df_train.reset_index(drop=True)\ndf_valid = df_valid.reset_index(drop=True)\n\ntrain_dataset = Dataset(df_train,'review','sentiment')\ntrain_data_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=config.TRAIN_BATCH_SIZE\n)\n\nvalid_dataset = Dataset(df_valid,'review','sentiment')\n\nvalid_data_loader = torch.utils.data.DataLoader(\n    valid_dataset, batch_size=config.VALID_BATCH_SIZE\n)","f5de8481":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","930a17d6":"# Get all of the model's parameters as a list of tuples.\nparams = list(model.named_parameters())\n\nprint('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n\nprint('==== Embedding Layer ====\\n')\n\nfor p in params[0:5]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== First Transformer ====\\n')\n\nfor p in params[5:21]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n\nprint('\\n==== Output Layer ====\\n')\n\nfor p in params[-4:]:\n    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))","e46b2b30":"param_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {\n        \"params\": [\n            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n        ],\n        \"weight_decay\": 0.001,\n    },\n    {\n        \"params\": [\n            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n        ],\n        \"weight_decay\": 0.0,\n    },\n]","f73809cf":"num_train_steps = int(len(df_train) \/ config.TRAIN_BATCH_SIZE * config.EPOCHS)\noptimizer = AdamW(optimizer_parameters, lr=3e-5)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, num_warmup_steps=0, num_training_steps=num_train_steps\n)\n","e4ef2263":"from tqdm import tqdm\nfrom sklearn.metrics import f1_score","3fda724a":"best_accuracy = 0\nfor epoch in range(config.EPOCHS):\n    train_fn(train_data_loader, model, optimizer, device, scheduler)\n    outputs, targets = eval_fn(valid_data_loader, model, device)\n    accuracy = metrics.accuracy_score(targets, outputs)\n#     accuracy=f1_score(targets, outputs,average='micro').round(3)\n    print(f\"Accuracy Score = {accuracy}\")\n    if accuracy > best_accuracy:\n        torch.save(model.state_dict(), config.MODEL_PATH)\n        best_accuracy = accuracy","8465afc7":"\nThis block essentially tells the optimizer to not apply weight decay to the bias terms (e.g., $ b $ in the equation $ y = Wx + b $ ) and LayerNormWeight\n\nWeight decay is a form of regularization--after calculating the gradients, we multiply them by, e.g., 0.99.","50c4c17f":"The `Attention Mask` is simply an array of 1s and 0s indicating which tokens are padding and which aren't.\n\nThe `Token Type Ids` is to differentiate between different sentence type (zero for all tokes in classification tasks)","355bbba2":"### Reading the data and dividing into train and validation","050b38f8":"This creates the dataset required for BERT Model. It takes the dataframe as input and coverts sentence into input ids, mask id (1,0) and  token type ids (0)","22b5bf9d":"These are the different parameters used to train model ","92243c41":"This tutorial explains how to apply BERT Model for Text classification\n====","866dcaf1":"### Loading the data","fd8bb8c0":"For classification tasks, we must prepend the special `[CLS]` token to the beginning of every sentence. \\n\n\n\n\nAt the end of every sentence, we need to append the special `[SEP]` token.","91c1349a":"### Training the BERT model and saving the best weights ","b0ea6e6c":"#### transfering the model to gpu if it is available","737c6342":"![cls_token](http:\/\/mccormickml.com\/assets\/BERT\/CLS_token_500x606.png)","799b3fd9":"![](http:\/\/)","67041154":"### Importing transformer library","08576256":"![image.png](attachment:image.png)","b6e81173":"Typical components of deep learning approach for NLP\n- `Preprocessing and tokenization`\n- `Generating vocabulary of unique tokens and converting words to indices (Numericalization)`\n- `Loading pretrained vectors`\n- `Padding text with zeros in case of variable lengths`\n-  `Dataloading and batching`\n- `Model creation and training`\n- `Prediction using trained Model`","f4a74356":"Unique different labels and their mappings","15c75634":"This is cross- entropy loss","6ffd3f9a":"### loading the data","3d16d8e3":"This is BERT architecture for classification tasks. We are taking pre-trained model and adding one linear layer on top of it to do classification.","57cf2d2a":"The dataset object required to train BERT Model","6511db01":"Reference: https:\/\/github.com\/abhishekkrthakur\/bert-sentiment\/","d6cca051":"Padding is done with a special `[PAD]` token, which is at index 0 in the BERT vocabulary. The below illustration demonstrates padding out to a \"MAX_LEN\" of 8 tokens.","e4193d2b":"This is training function that takes dataloader, model, optimizer, device and scheduler to train the model for a epoch","6d940947":"This is evaluation function required to validating the model","3690f187":"#### BERT parameter in different layer","2c4917d8":"- BERT TOKENIZER tokenizes the sentence using wordpiece tokenizer and converts int numerical form. BERT has total 30522 tokens available\n- Preprocessing and tokenization\n- Generating vocabulary of unique tokens and converting words to indices (Numericalization)"}}