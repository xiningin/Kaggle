{"cell_type":{"372e1ce3":"code","6bfa6ade":"code","0dc68088":"code","c72a3894":"code","732c27f5":"code","bf2bbeaa":"code","7a7fb064":"code","fd2c5d9a":"code","0f8725ca":"code","0ce4e136":"code","83fc23fd":"code","9990f322":"code","7b348c7a":"code","657c8fd5":"code","42227e81":"code","fb0afa7c":"code","562f8e78":"code","7bb84145":"code","6352c8ac":"code","2fc5b666":"code","c28ee230":"code","f1ce6aea":"code","b557cf74":"code","2140d341":"code","d2258ea6":"code","a068c366":"code","25832811":"code","1c0dee5c":"code","f484e006":"code","bd0df9be":"code","cde47cc4":"code","65d14bd3":"code","f7fbf5dc":"code","9d02647e":"code","8de325ff":"markdown","ed93fbe7":"markdown","cf5a8db4":"markdown","0880825d":"markdown","53643860":"markdown","b5746e5d":"markdown","ad9131ce":"markdown","5ef0344b":"markdown","9ae843fa":"markdown","d1733a9d":"markdown","d8efde67":"markdown","56c68a13":"markdown","a8b479fe":"markdown","31dd1f0a":"markdown","e5efa60c":"markdown","0f334f19":"markdown","b236abec":"markdown","b91829bd":"markdown","e4af806a":"markdown","2ad3675c":"markdown","b870d816":"markdown","1882162e":"markdown","077fba26":"markdown","164e2569":"markdown","cbe011bf":"markdown","6bf5a4bd":"markdown","bf06ea52":"markdown","11c1c4a4":"markdown","0c41a662":"markdown","58fc6408":"markdown","c24d50b3":"markdown","ab7afbea":"markdown","303d3008":"markdown","78e8864d":"markdown","748f91ef":"markdown","39538b4f":"markdown","5995eeec":"markdown","0fdcd35f":"markdown","a3c5cc1d":"markdown","01463b6b":"markdown","ca963b92":"markdown","58d5a557":"markdown","9bdcdbe8":"markdown","325f223e":"markdown","0b6aef36":"markdown","3c84c2bb":"markdown","6d23d3de":"markdown","0efaa01a":"markdown","73600bab":"markdown","a08a1111":"markdown","3c6d991e":"markdown","d36c9d76":"markdown","a7289956":"markdown","c3356c17":"markdown","8ca4b398":"markdown","d2b1073c":"markdown","11175022":"markdown","bc9fd63a":"markdown","0968e433":"markdown","efe2a88b":"markdown","52ee6509":"markdown","a7899526":"markdown","a12d4db3":"markdown","7e217f32":"markdown","c5185c1d":"markdown","df90dca4":"markdown","df05d4ae":"markdown","26ae5ac9":"markdown","c2d40fda":"markdown","4c76e006":"markdown","1da3bee9":"markdown","54d9c794":"markdown","aab14126":"markdown","bcb20657":"markdown","22526315":"markdown","9ef3ffb6":"markdown","f85e7639":"markdown","7c41dcf7":"markdown","672c80a9":"markdown","a4ec5509":"markdown","6fcef71d":"markdown","3e8f52e5":"markdown","33de2e0a":"markdown","1325c596":"markdown","0d26c209":"markdown","ac367cfd":"markdown","90034414":"markdown","20482494":"markdown","d8736723":"markdown","b8bdb26a":"markdown","6f660cf3":"markdown","a8e335a9":"markdown","59ce1ab4":"markdown"},"source":{"372e1ce3":"# Importing the required packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings(action='ignore', category=DeprecationWarning)","6bfa6ade":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","0dc68088":"train.head()","c72a3894":"print(train.info())\nprint('-----------------------------------------------------------\\n')\nprint(test.info())","732c27f5":"plt.hist(train['Embarked'])\nplt.show()","bf2bbeaa":"train['Embarked'] = train['Embarked'].fillna('S')","7a7fb064":"test['Fare'] = test['Fare'].fillna(test['Fare'].mean())","fd2c5d9a":"# Create dataframe that holds survived passengers\nsurvival = train[train['Survived'] == 1]\n\n# Create dataframe for all observations with ages that are not null\ntrain_with_age = train[train['Age'].notnull()]\n\n# Preparing the standard sizes and background color\nplt.rcParams['figure.figsize'] = 20, 5\nsns.set_style('darkgrid')","0f8725ca":"# Create dataframe to represent some basic statistics\ngender_total = pd.DataFrame(columns=['Gender', 'Passengers', 'Survival'])\n\n# Adding numbers to the above dataframe\nfor gender in ['male', 'female']:\n    percentage = str(round(len(survival[survival['Sex'] == gender]) \/ len(train[train['Sex'] == gender]) * 100, 2))\n    gender_total.loc[len(gender_total)] = gender, len(train[train['Sex'] == gender]), (percentage + ' %')\n\n# Create histogram \nplt.hist(train['Sex'], bins=9, rwidth=0.9,  color = 'steelblue', label='Total')\nplt.hist(survival['Sex'], bins=9, rwidth=0.9, color = 'darkseagreen', label='Survived')\nplt.xlabel('Gender')\nplt.ylabel('Count')\nplt.legend(loc='upper left')\nplt.grid(axis='x')\n\n# Create comparison charts (Box and swarm plots)\nf, axes = plt.subplots(2, 2, figsize=(17, 10))\na1 = sns.boxplot(x='Sex', y='Age', data=train_with_age, hue='Survived', palette=['steelblue','darkseagreen'], showfliers=False, ax=axes[0, 0])\na2 = sns.swarmplot(x='Sex', y='Age', data=train_with_age, hue='Survived', palette=['steelblue','darkseagreen'], dodge=True, ax=axes[1, 0])\na3 = sns.boxplot(x='Sex', y='Fare', data=train, hue='Survived', palette=['steelblue','darkseagreen'], showfliers=False, ax=axes[0, 1])\na4 = sns.swarmplot(x='Sex', y='Fare', data=train, hue='Survived',palette=['steelblue','darkseagreen'], dodge=True, ax=axes[1, 1])\n\n# Create comparison charts (Box and swarm plots)\nb1 = a1.legend(loc='upper right')\nb2 = a2.legend(loc='upper right')\nb3 = a3.legend(loc='upper right')\nb4 = a4.legend(loc='upper right')\n\nplt.show()\ngender_total","0ce4e136":"# Create dataframe to represent some basic statistics\nembarkation_port = pd.DataFrame(columns=['Embarkation Port', 'Passengers', 'Port Survival', 'Port Survival (F)', 'Port Survival (M)'])\n\n# Adding numbers to the above dataframe\nfor embark in ['C', 'Q', 'S']:\n    percentage = str(round(len(survival[survival['Embarked'] == embark]) \/ len(train[train['Embarked'] == embark]) * 100, 2))\n    female = str(round(len(survival[(survival['Embarked'] == embark) & (survival['Sex'] == 'female')]) \/ len(train[(train['Embarked'] == embark)]) * 100, 2))\n    male = str(round(len(survival[(survival['Embarked'] == embark) & (survival['Sex'] == 'male')]) \/ len(train[(train['Embarked'] == embark)]) * 100, 2))\n    embarkation_port.loc[len(embarkation_port)] = embark, len(train[train['Embarked'] == embark]), (percentage + ' %'), (female + ' %'), (male + ' %')\n\n# Create histogram \nplt.hist(train['Embarked'], bins=9, rwidth=0.9,  color = 'steelblue', label='Total')\nplt.hist(survival['Embarked'], bins=9, rwidth=0.9, color = 'darkseagreen', label='Survived')\nplt.xlabel('Embarkation Port')\nplt.ylabel('Count')\nplt.legend(loc='upper left')\nplt.grid(axis='x')\n\n# Create comparison charts (Box and swarm plots)\nf, axes = plt.subplots(2, 2, figsize=(17, 10))\na1 = sns.boxplot(x='Embarked', y='Age', data=train_with_age, hue='Survived', palette=['steelblue','darkseagreen'], showfliers=False, ax=axes[0, 0])\na2 = sns.swarmplot(x='Embarked', y='Age', data=train_with_age, hue='Survived', palette=['steelblue','darkseagreen'], dodge=True, ax=axes[1, 0])\na3 = sns.boxplot(x='Embarked', y='Fare', data=train, hue='Survived', palette=['steelblue','darkseagreen'], showfliers=False, ax=axes[0, 1])\na4 = sns.swarmplot(x='Embarked', y='Fare', data=train, hue='Survived',palette=['steelblue','darkseagreen'], dodge=True, ax=axes[1, 1])\n\n# Adjust comparison charts' legends locations\nb1 = a1.legend(loc='upper right')\nb2 = a2.legend(loc='upper right')\nb3 = a3.legend(loc='upper right')\nb4 = a4.legend(loc='upper right')\n\nplt.show()\ndisplay(embarkation_port)","83fc23fd":"# Create dataframe to represent some basic statistics\np_class = pd.DataFrame(columns=['Pclass', 'Passengers', 'Class Survival', 'Class Survival (F)', 'Class Survival (M)'])\n\n# Adding numbers to the above dataframe\nfor pclass in range(1, 4):\n    percentage = str(round(len(survival[survival['Pclass'] == pclass]) \/ len(train[train['Pclass'] == pclass]) * 100, 2))\n    female = str(round(len(survival[(survival['Pclass'] == pclass) & (survival['Sex'] == 'female')]) \/ len(train[(train['Pclass'] == pclass)]) * 100, 2))\n    male = str(round(len(survival[(survival['Pclass'] == pclass) & (survival['Sex'] == 'male')]) \/ len(train[(train['Pclass'] == pclass)]) * 100, 2))\n    p_class.loc[len(p_class)] = pclass, len(train[train['Pclass'] == pclass]), (percentage + ' %'), (female + ' %'), (male + ' %')\n\n# Create histogram \nplt.hist(train['Pclass'], bins=9, rwidth=0.9,  color = 'steelblue', label='Total')\nplt.hist(survival['Pclass'], bins=9, rwidth=0.9, color = 'darkseagreen', label='Survived')\nplt.xlabel('Pclass')\nplt.ylabel('Count')\nplt.legend(loc='upper left')\nplt.grid(axis='x')\n\n# Create comparison charts (Box and swarm plots)\nf, axes = plt.subplots(2, 2, figsize=(17, 10))\na1 = sns.boxplot(x='Pclass', y='Age', data=train_with_age, hue='Survived', palette=['steelblue','darkseagreen'], showfliers=False, ax=axes[0, 0])\na2 = sns.swarmplot(x='Pclass', y='Age', data=train_with_age, hue='Survived', palette=['steelblue','darkseagreen'], dodge=True, ax=axes[1, 0])\na3 = sns.boxplot(x='Pclass', y='Fare', data=train, hue='Survived', palette=['steelblue','darkseagreen'], showfliers=False, ax=axes[0, 1])\na4 = sns.swarmplot(x='Pclass', y='Fare', data=train, hue='Survived',palette=['steelblue','darkseagreen'], dodge=True, ax=axes[1, 1])\n\n# Create comparison charts (Box and swarm plots)\nb1 = a1.legend(loc='upper right')\nb2 = a2.legend(loc='upper right')\nb3 = a3.legend(loc='upper right')\nb4 = a4.legend(loc='upper right')\n\nplt.show()\ndisplay(p_class)","9990f322":"train['Family'] = train['SibSp'] + train['Parch']\ntest['Family'] = test['SibSp'] + test['Parch']","7b348c7a":"# Merging both datasets (train and test) data\ncombined = [train, test]\n\n# Loop over each observation to extract the title from the name\nfor dataset in combined:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n    \n# Now, let's have a look on which titles were more likely to survive.\npd.crosstab(train['Title'], train[train['Survived'] == 1]['Survived'])","657c8fd5":"# Aggregating the unfrequented titles under (Others)\nfor dataset in combined:\n    dataset['Title'] = dataset['Title'].replace(['Don', 'Rev', 'Dr', 'Mme', 'Ms',\n       'Major', 'Lady', 'Sir', 'Mlle', 'Col', 'Capt', 'Countess',\n       'Jonkheer', 'Dona'], 'Others')\n\n# Merging the datasets (train & test) again after the previous amendments\ncombine = pd.concat(combined, ignore_index=True)","42227e81":"for dataset in combined:\n    dataset['Sex'] = dataset['Sex'].map( {'male': 0, 'female': 1} ).astype(int)\n    \nfor dataset in combined:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\nfor dataset in combined:\n    dataset['Title'] = dataset['Title'].map( {'Mr': 1, 'Miss': 2, 'Mrs': 3, 'Master': 4, 'Others': 5} ).astype(int)\n    \ncombine = pd.concat(combined, ignore_index=True)","fb0afa7c":"for dataset in combined:\n    # Setting counter to mark each \"Fare\" group with a specific number\n    counter = 0\n    \n    # Grouping \"Fares\" below 5 dollars\n    dataset.loc[dataset['Fare'] <= 5, 'Fare'] = counter\n    counter += 1\n    \n    # Grouping \"Fares\" between 5 and 30 dollars\n    for number in range(5, 30, 5):\n        dataset.loc[(dataset['Fare'] > number) & (dataset['Fare'] <= (number + 5)), 'Fare'] = counter\n        counter += 1\n                                  \n    # Grouping \"Fares\" between 30 and 200 dollars\n    for number in range(30, 200, 10):\n        dataset.loc[(dataset['Fare'] > number) & (dataset['Fare'] <= (number + 10)), 'Fare'] = counter\n        counter += 1\n                                        \n    # Grouping \"Fares\" between 200 and 300 dollars\n    for number in range(200, 300, 20):\n        dataset.loc[(dataset['Fare'] > number) & (dataset['Fare'] <= (number + 20)), 'Fare'] = counter\n        counter += 1\n\n    # Grouping \"Fares\" between 300 and 400 dollars\n    for number in range(300, 400, 50):\n        dataset.loc[(dataset['Fare'] > number) & (dataset['Fare'] <= (number + 50)), 'Fare'] = counter\n        counter += 1\n                        \n    # Grouping \"Fares\" above 400 dollars\n    dataset.loc[(dataset['Fare'] > 400), 'Fare'] = counter","562f8e78":"for dataset in combined:\n    # Setting counter to mark each \"Age\" group with a specific number\n    counter = 0\n\n    # Grouping \"Ages\" below 10 years\n    dataset.loc[ dataset['Age'] <= 10, 'Age'] = counter\n    counter += 1\n\n    # Grouping \"Ages\" between 10 and 80 years\n    for number in range(10, 80, 10):\n        dataset.loc[(dataset['Age'] > number) & (dataset['Age'] <= (number + 10)), 'Age'] = counter\n        counter += 1","7bb84145":"combine = pd.concat(combined, ignore_index=True)\n\n# Creating predictors (X_train) and dependent (y_train) dataframes containing all observations with existing ages\nX_train = combine[combine['Age'].notnull()].loc[:, ['Embarked', 'Family', 'Fare', 'Pclass', 'Sex', 'Title']].values\ny_train = combine[combine['Age'].notnull()].loc[:, 'Age'].values\n\n# Creating classifier object using XGBoost\nclassifier = XGBClassifier(colsample_bytree=0.34, learning_rate=0.1, max_depth=3, min_child_weight=5.01, \n    n_estimators=105, reg_lambda=0.000001, subsample=0.6)\n\n# Fitting the classifier\nclassifier.fit(X_train, y_train)","6352c8ac":"# Creating predictors dataframe containing all observations with missing ages\nX_age = combine[combine['Age'].isnull()].loc[:, ['Embarked', 'Family', 'Fare', 'Pclass', 'Sex', 'Title']].values\n\n# Predicting missing ages\nX_prediction = classifier.predict(X_age)\n\n# Adding the related (PassengerIDs) for all of the missing ages to be able to locate them in the training and test dataframes and replace the corresponding missing ages.\nPassengerId = combine[combine['Age'].isnull()].iloc[:, 7].values\n\nfor id in (range(0, len(PassengerId))):\n    # Looping through the missing ages in train dataframe and replace them with the corresponding predictions obtained from the random forest model. \n    for row in range(0, len(train)):\n        if train.iloc[row, 0] == PassengerId[id]:\n            train.iloc[row, 5] = X_prediction[id]\n\n    # Looping through the missing ages in test dataframe and replace them with the corresponding predictions obtained from the random forest model.\n    for row in range(0, len(test)):\n        if test.iloc[row, 0] == PassengerId[id]:\n            test.iloc[row, 4] = X_prediction[id]\n\ncombine = pd.concat(combined, ignore_index=True)","2fc5b666":"# Create X, y vectors.\nX_train = train.loc[:, ['Embarked', 'Sex', 'Title', 'Pclass', 'Age', 'Fare', 'Family']].values\ny_train = train.loc[:, 'Survived'].values\n\n# Create a dataframe that will hold each model's prediction accuracy calculated using cross validation.\naccuracy_dataframe = pd.DataFrame(columns=['Model', 'K_Fold_Score'])\n\n# Create function to check the best hyperparameters for each algorithm\ndef checker (algo, parameters, x, y):\n    grid_search = GridSearchCV(estimator = algo, param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\n    grid_search = grid_search.fit(x, y)\n    \n    # summarize the result\n    print(\"\\nBest score is: %.2f %s using %s\\n\" % ((round((grid_search.best_score_) * 100, 2)), '%', grid_search.best_params_))","c28ee230":"parameters = dict(C= [i\/10 for i in range(1, 31)], solver= ['newton-cg', 'lbfgs', 'liblinear'])\nchecker(LogisticRegression(), parameters, X_train, y_train)","f1ce6aea":"# Fitting Logistic Regression to the Training set.\nclassifier = LogisticRegression(C = 0.2, solver='newton-cg')\nclassifier.fit(X_train, y_train)\n\n# Applying K-Fold Cross Validation.\naccuracy = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nK_fold = str(round((accuracy.mean() * 100), 2))\n\n# Adding the name of the model and K_fold result to the accuracy_dataframe\naccuracy_dataframe.loc[len(accuracy_dataframe)] = 'Logistic Regression', (K_fold + ' %')\n","b557cf74":"parameters = dict(n_neighbors = range(1,51), metric = ['euclidean', 'manhattan', 'chebyshev', 'minkowski'], p=[1, 2])\nchecker(KNeighborsClassifier(), parameters, X_train, y_train)","2140d341":"# Fitting the classifier to the Training set\nclassifier = KNeighborsClassifier(metric = 'manhattan', n_neighbors = 32, p = 1)\nclassifier.fit(X_train, y_train)\n\n# Applying K-Fold Cross Validation\naccuracy = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nK_fold = str(round((accuracy.mean() * 100), 2))\n\n# Adding the name of the model and K_fold result to the accuracy_dataframe\naccuracy_dataframe.loc[len(accuracy_dataframe)] = 'K-NN', (K_fold + ' %')","d2258ea6":"parameters = dict(C = [i\/100 for i in range(100, 131)], kernel = ['linear', 'rbf', 'sigmoid'], gamma = [0.01, 0.1, 1])\nchecker(SVC(), parameters, X_train, y_train)","a068c366":"# Fitting SVM to the Training set\nclassifier = SVC( C=1.19, gamma=0.1, kernel = 'rbf')\nclassifier.fit(X_train, y_train)\n\n# Applying K-Fold Cross Validation\naccuracy = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nK_fold = str(round((accuracy.mean() * 100), 2))\n\n# Adding the name of the model and K_fold result to the accuracy_dataframe\naccuracy_dataframe.loc[len(accuracy_dataframe)] = 'SVM', (K_fold + ' %')","25832811":"# Fitting Naive Bayes to the Training set\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\n# Applying K-Fold Cross Validation\naccuracy = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nK_fold = str(round((accuracy.mean() * 100), 2))\n\n# Adding the name of the model and K_fold result to the accuracy_dataframe\naccuracy_dataframe.loc[len(accuracy_dataframe)] = 'Naive Bayesn', (K_fold + ' %')","1c0dee5c":"parameters = dict(n_estimators=range(60, 76), criterion = ['gini', 'entropy'], max_depth = range(4, 7), random_state=[3])\nchecker(RandomForestClassifier(), parameters, X_train, y_train)","f484e006":"# Fitting Random Forest to the Training set (The below hyperparameters were the result of the Grid Search if grid search was running on Spyder)\nclassifier = RandomForestClassifier(criterion = 'entropy', max_depth=4, n_estimators = 61, random_state=3)\nclassifier.fit(X_train, y_train)\n\n# Applying K-Fold Cross Validation\naccuracy = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nK_fold = str(round((accuracy.mean() * 100), 2))\n\n# Adding the name of the model and K_fold result to the accuracy_dataframe\naccuracy_dataframe.loc[len(accuracy_dataframe)] = 'Random Forest', (K_fold + ' %')","bd0df9be":"parameters = dict(n_estimators=range(80, 101), learning_rate=[0.01],\nmax_depth=[3], gamma = [0.00001], subsample=[i\/10 for i in range(3, 8)], colsample_bytree=[i\/10 for i in range(8, 11)],\n            reg_lambda=[0.000001])\nchecker(XGBClassifier(), parameters, X_train, y_train)","cde47cc4":"# Fitting Random Forest to the Training set (The below hyperparameters were the result of the Grid Search if grid search was running on Spyder)\nclassifier = XGBClassifier(colsample_bytree=0.8, gamma=1e-05, learning_rate=0.01, max_depth=3, n_estimators=89, \n                          reg_lambda=1e-06, subsample=0.6)\nclassifier.fit(X_train, y_train)\n\n# Applying K-Fold Cross Validation\naccuracy = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nK_fold = str(round((accuracy.mean() * 100), 2))\n\n# Adding the name of the model and K_fold result to the accuracy_dataframe\naccuracy_dataframe.loc[len(accuracy_dataframe)] = 'XGBoost', (K_fold + ' %')","65d14bd3":"accuracy_dataframe = accuracy_dataframe.sort_values(['K_Fold_Score'], ascending=False)\naccuracy_dataframe.reset_index(drop=True)","f7fbf5dc":"# Predicting the test data\ntest_dataset = test.loc[:, ['Embarked', 'Sex', 'Title', 'Pclass', 'Age', 'Fare', 'Family']].values\n\n# Predict the survival of test dataset\nprediction = {'PassengerId': test['PassengerId'], 'Survived': classifier.predict(test_dataset)}","9d02647e":"# Creating prediction file\nsolution = pd.DataFrame(prediction)\nsolution.to_csv('Final_Solution.csv', index=False)","8de325ff":"---","ed93fbe7":"---","cf5a8db4":"### 3.1 Predict Test Data Survival:","0880825d":"We can replace the missing (Fare) value with the mean:","53643860":"* Discrete: Pclass, Family Members (SibSp + Parch)\n* Continuous: Age, Fare","b5746e5d":"## 1.2.2. Quantitative Features:","ad9131ce":"---","5ef0344b":"---","9ae843fa":"From the above charts, we can notice that:<br><br>\n1- Although the number of males was almost double the females, the percentage of females survival among total females is almost 4 times the percentage of males survival among total males. <br>\n2- Most of the survived passengers were between 18 and 38 years old. <br>\n3- Females who paid higher fare than the average were more likely to survive.","d1733a9d":"<br>The above results demonstrate the survival of each title. Since the title can be important in the prediction of the survival, we need to decrease the number of classes (titles) to avoid creating too many columns when applying one hot encoder (decrease the number of columns to avoid the curse of dimensionality).<br><br>","d8efde67":"## 1.2.1. Categorical Features:","56c68a13":"### Categorical Features","a8b479fe":"## 2.2 K-NN:","31dd1f0a":"# 1.2 Data Visiualization:","e5efa60c":"#### The above prediction scored (0.79425) on Kaggle's public leadership board.","0f334f19":"## 2.5 Random Forest:","b236abec":"## 2.3 SVM:","b91829bd":"## 1.2.1.1 Sex:","e4af806a":"* **1. Data Exploration:**\n   * **1.1 Fill Necessary Data:**\n     * 1.1.1 Fill the (2) missing values in (Embarked) in train dataset.<br>\n     * 1.1.2 Fill the (1) missing value in (Fare) in test dataset.<br>\n   * **1.2 Data Visiualization:**\n     * **1.2.1 Categorical Features:**\n       * 1.2.1.1 Sex.<br>\n       * 1.2.1.2 Embarked.<br>\n     * **1.2.2 Quantitative Features:**\n       * 1.2.2.1 Pclass.\n   * **1.3 Features Engineering:**\n     * 1.3.1 Merge (SibSp) and (Parch) variables into one variable (Family).<br>\n     * 1.3.2 Extract the titles from (Name) feature.<br>\n     * 1.3.3 Transform categorical features to quantitative.<br>\n     * 1.3.4 Predict the missing ages.\n* **2. Modeling:**\n  * 2.1 Logistic Regression.\n  * 2.2 K-NN.<br>\n  * 2.3 SVM.<br>\n  * 2.4 Naive Bayes.<br>\n  * 2.5 Random Forest.<br>\n  * 2.6 XGBoost.\n* **3. Final Prediction:**\n  * 3.1 Predict Test Data Survival.<br>\n  * 3.2 Create Submission File.","2ad3675c":"### 1.1.2. Fill the missing (Fare) value in test dataset:","b870d816":"# 2. Modeling:","1882162e":"## 2.6 XGBoost:","077fba26":"### Quantitative Features","164e2569":"## 1.2.2.1 Pclass:","cbe011bf":"##### First, we will group fares into 30 bins to make the prediction of missing ages easier and avoid overfitting as much as we can.","6bf5a4bd":"## 2.1 Logistic Regression:","bf06ea52":"#### The features we will explore visually can be categorized as follows:","11c1c4a4":"### Fitting the algorithm:","0c41a662":"---","58fc6408":"---","c24d50b3":"### 1.2. Data Visualization Summary:","ab7afbea":"### Fitting the algorithm:","303d3008":"##### It's obvious that Southampton \"S\" is the most repeated port of embarkation. So, we will replace the missing (Embarked) values with 'S'.","78e8864d":"##### After that, we will run XGBoost to predict missing ages (the below hyperparameters are the result of running few iterations of Grid Search (outside this notebook) to optimize the hyperparameters used with XGBoost).","748f91ef":"By visualizing the relationship between different features, we can come up with:\n\n1- Gender had a great influence on determining the surviving likelihood. <br>\n2- Most of the survived passengers were between 20 and 38 years old. <br>\n3- High fare passengers survival was higher than average fare passengers.<br>\n4- The highest surviving rate among embarkations' ports passengers was 'C' (Cherbourg), where 55% of 'C' passengers survived.<br>\n5- Passengers who embarked from 'S' and 'C' and paid more than the average fare were more likely to survive.<br>\n6- More than 50% of the passengers were 3rd class.<br>\n7- Between all classes, 1st class was the highest in terms of survival among their passengers (63%).<br>\n8- The average age and fare of survived passengers in 1st class were higher than the other classes.","39538b4f":"### Determine the best hyperparameters to be used:","5995eeec":"## 2.4 Naive Bayes:","0fdcd35f":"---","a3c5cc1d":"### Fitting the algorithm:","01463b6b":"To better prepare the data, we can combine both features ('SibSp' and 'Parch') under one variable (let's name it 'Family') to decrease the number of features in order to avoid the curse of dimensionality.","ca963b92":"First, we will start off by preparing our predictors, dependent variables and create function to help us checking the best hyperparameters for different algorithms using Grid Search.","58d5a557":"---","9bdcdbe8":"---","325f223e":"### Determine the best hyperparameters to be used:","0b6aef36":"## 1.2.1.2 Embarked:","3c84c2bb":"#### Now, let's have a visual look over the different features:","6d23d3de":"After having a quick look over the data through the above tables and charts, we will notice that we need to:<br>\n1. Merge (SibSp) and (Parch) variables into one variable (Family) \"to decrease the number of features to avoid the curse of dimensionality\".<br>\n2. Extract the titles from (Name) feature.<br>\n3. Transform categorical features to quantitative to prevent overfitting.<br>\n4. Predict the missing ages.<br>","0efaa01a":"### 1.2.2. Quantitative Features:\n\n1. Pclass.","73600bab":"#### From the above dataframe, we can begin by determining the categorical and quantitative features","a08a1111":"##### It seems that XGBoost is one of the best classifiers to go on with.","3c6d991e":"### Now, let's have a look on the scores for each model ranked from highest to lowest:","d36c9d76":"# Table of Contents: ","a7289956":"### 1.3.4. Predict the missing ages:","c3356c17":"### 1.2.2.1 Conclusion","8ca4b398":"---","d2b1073c":"---","11175022":"---","bc9fd63a":"### Importing libraries:","0968e433":"---","efe2a88b":"## 1.1 Fill Necessary Data:","52ee6509":"### 1.1.1. Fill the missing (Embarked) values in train dataset:","a7899526":"---","a12d4db3":"### 3.2 Create Submission File:","7e217f32":"### 1.2.1. Categorical Features:\n1. Sex.<br>\n2. Embarked.","c5185c1d":"#### Now, we will start off by uploading training and test datasets and explore them.","df90dca4":"---","df05d4ae":"### Fitting the algorithm:","26ae5ac9":"### Determine the best hyperparameters to be used:","c2d40fda":"### Fitting the algorithm:","4c76e006":"### Fitting the algorithm:","1da3bee9":"### Determine the best hyperparameters to be used:","54d9c794":"* Ordinal: Ticket, Cabin\n* Nominal: Name, Sex, Embarked","aab14126":"##### Now, let's predict the missing ages and update train and test dataframes with the predictions.","bcb20657":"---","22526315":"---","9ef3ffb6":"# 1. Data Exploration:","f85e7639":"### 1.2.1.1 Conclusion","7c41dcf7":"From the above charts, we can notice that:<br><br>\n1- More than 50% of the passengers were 3rd class.<br>\n2- Between all classes, 1st class was the highest in terms of survival among their passengers (63%).<br>\n3- The average age and fare of survived passengers in 1st class were higher than the other classes.","672c80a9":"### 1.3.1. Merge (SibSp) and (Parch) variables into one variable (Family):","a4ec5509":"##### We can do the same grouping task also with \"Age\" (group it within 8 bins).","6fcef71d":"From the above charts, we can notice that:<br><br>\n1- Almost 75% of the passengers embarked from 'S' (Southampton).<br>\n2- The highest surviving rate among embarkations' ports passengers was 'C' (Cherbourg), where 55% of 'C' passengers survived.<br>\n3- Passengers who embarked from 'S' and 'C' and paid more than the average fare were more likely to survive.","3e8f52e5":"We will try the below algorithms and see which one will become the best classification model according to the obtained accuracy:<br><br>\n2.1. Logistic Regression.<br>\n2.2. K-NN.<br>\n2.3. SVM.<br>\n2.4. Naive Bayes.<br>\n2.5. Random Forest.<br>\n2.6. XGBoost.<br>","33de2e0a":"# 3. Final Prediction:","1325c596":"---","0d26c209":"### 1.3.2. Extract the titles from (Name) feature:","ac367cfd":"---","90034414":"### 1.2.1.2 Conclusion","20482494":"# 1.3. Features Engineering:","d8736723":"### Determine the best hyperparameters to be used:","b8bdb26a":"### 1.3.3. Transform categorical features to quantitative:","6f660cf3":"---","a8e335a9":"As for (Embarked), let's see which value is the dominant","59ce1ab4":"From the above info summary, we can see that there are two missing \"Fare\" values in training dataset and one \"Fare\" in test dataset."}}