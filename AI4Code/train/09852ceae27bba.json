{"cell_type":{"fc6d8c66":"code","ca599779":"code","ae6e8215":"code","fb475c12":"code","dc7d74a1":"code","ed0f0380":"code","b770d916":"code","dff32d16":"code","ab9dc941":"code","35bae427":"code","d8aa4885":"code","0353b35d":"code","1dff1b47":"code","556a956a":"code","c8966ed7":"code","89ba88db":"code","128388bc":"code","e3727faa":"code","ba37ce17":"code","23dba164":"code","bad66aa2":"code","e43a74a7":"code","8adc9468":"code","839e3d39":"code","aaa64c13":"code","b95209f3":"code","726f6c64":"code","6d492907":"code","471369e2":"code","3e11dbcc":"code","6d4685a8":"code","2b0228fd":"code","8b852b37":"code","e44ff1a3":"code","910fc1d2":"code","86d5e5b6":"code","34cc9584":"code","55d35a00":"code","c669c4db":"code","4056d581":"code","3a4fcbcc":"code","5180a4b4":"markdown","26f1178f":"markdown","600fdd0b":"markdown","ee1c6c40":"markdown","ec233261":"markdown","0e4f3d45":"markdown","28608ef6":"markdown","c7f85dc3":"markdown","3d6737ea":"markdown","fb812dd6":"markdown","3f103805":"markdown","53ac1b1c":"markdown","1f72c60f":"markdown","efbb67f1":"markdown","981564a5":"markdown","777fe645":"markdown","d6711661":"markdown","cd0b8667":"markdown","5a6b26f9":"markdown","6de13561":"markdown"},"source":{"fc6d8c66":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ca599779":"df = pd.read_csv('..\/input\/clinvar_conflicting.csv')\n#df = df.dropna()\n#df.info()","ae6e8215":"fig = plt.figure(figsize = (10, 10))\nsns.countplot(x= 'CLASS', data = df, hue = 'CHROM', palette='icefire')","fb475c12":"fig = plt.figure(figsize = (10, 10))\nsns.heatmap(df.isnull(), cmap = 'viridis', cbar = False)","dc7d74a1":"toBeConsidered = ['CHROM', 'POS', 'REF', 'ALT', 'AF_ESP', 'AF_EXAC', 'AF_TGP',\n       'CLNDISDB', 'CLNDN', 'CLNHGVS', 'CLNVC','MC', 'ORIGIN', 'CLASS',\n       'Allele', 'Consequence', 'IMPACT', 'SYMBOL', 'Feature_type',\n       'Feature', 'BIOTYPE', 'STRAND','CADD_PHRED', 'CADD_RAW']\ndf2 = df[toBeConsidered]\ndf2 = df2.dropna()\n","ed0f0380":"cutdowns = []\nfor i in df2.columns.values:\n    if df2[i].nunique() < 1000:\n        cutdowns.append(i)\nprint(\"The selected Columns for training are : \", cutdowns)","b770d916":"df_final = df2[cutdowns]\n#df_final.info()","dff32d16":"df_final['CHROM'] = df_final['CHROM'].astype(str)","ab9dc941":"from sklearn.feature_extraction import FeatureHasher\nfh = FeatureHasher(n_features = 5, input_type = 'string')\nhashed1 = fh.fit_transform(df_final['REF'])\nhashed1 = hashed1.toarray()\nhashedFeatures1 = pd.DataFrame(hashed1)","35bae427":"nameList = {}\nfor i in hashedFeatures1.columns.values:\n    nameList[i] = \"REF\"+str(i+1)\n\n\nhashedFeatures1.rename(columns = nameList, inplace = True)\nprint(\"The Hashed REF table is somethinng like this : \\n\",hashedFeatures1.head())","d8aa4885":"#df['ALT']\n#fh = FeatureHasher(n_features = 5, input_type = 'string')\nhashed2 = fh.fit_transform(df_final['ALT'])\nhashed2 = hashed2.toarray()\nhashedFeatures2 = pd.DataFrame(hashed2)\n\nnameList2 = {}\nfor i in hashedFeatures2.columns.values:\n    nameList2[i] = \"ALT\"+str(i+1)\n\n\nhashedFeatures2.rename(columns = nameList2, inplace = True)\nprint(\"The Hashed ALT table is somethinng like this : \\n\",hashedFeatures2.head())","0353b35d":"binaryFeature1 = pd.get_dummies(df_final['CLNVC'])\nprint(\"While the One hot encoded matrix of CLNVC Columns is like this : \\n\")\nbinaryFeature1.head()","1dff1b47":"df_final = df_final.drop(columns=['MC'], axis = 1)","556a956a":"hashed0 = fh.fit_transform(df_final['CHROM'])\nhashed0 = hashed0.toarray()\nhashedFeatures0 = pd.DataFrame(hashed0)\n\nnameList0 = {}\nfor i in hashedFeatures0.columns.values:\n    nameList0[i] = \"CHROM\"+str(i+1)\n\n\nhashedFeatures0.rename(columns = nameList0, inplace = True)\nhashedFeatures0.head()","c8966ed7":"hashed3 = fh.fit_transform(df_final['Allele'])\nhashed3 = hashed3.toarray()\nhashedFeatures3 = pd.DataFrame(hashed3)\n\nnameList3 = {}\nfor i in hashedFeatures3.columns.values:\n    nameList3[i] = \"Allele\"+str(i+1)\n\n\nhashedFeatures3.rename(columns = nameList3, inplace = True)\nhashedFeatures3.head()","89ba88db":"hashed4 = fh.fit_transform(df_final['Consequence'])\nhashed4 = hashed4.toarray()\nhashedFeatures4 = pd.DataFrame(hashed4)\n\nnameList4 = {}\nfor i in hashedFeatures4.columns.values:\n    nameList4[i] = \"Consequence\"+str(i+1)\n\n\nhashedFeatures4.rename(columns = nameList4, inplace = True)\nhashedFeatures4.head()","128388bc":"df_final['IMPACT'].nunique()","e3727faa":"binaryFeature3 = pd.get_dummies(df_final['IMPACT'])\nbinaryFeature3.head()","ba37ce17":"df_final = df_final.drop(columns=['Feature_type'], axis = 1)","23dba164":"binaryFeature4 = pd.get_dummies(df_final['BIOTYPE'], drop_first=True)\nbinaryFeature4.head()","bad66aa2":"binaryFeature5 = pd.get_dummies(df_final['STRAND'], drop_first=True)\nbinaryFeature5.head()","e43a74a7":"df3 = pd.concat([binaryFeature1, binaryFeature3, binaryFeature4, binaryFeature5, hashedFeatures1 , hashedFeatures2, hashedFeatures3, hashedFeatures4,hashedFeatures0, df_final['CLASS']], axis=1)\ndf3 = df3.dropna()\ndf3.rename(columns={1 : \"one\", 16 : \"sixteen\"}, inplace = True)\nprint(df3.columns.values)\ndf3.head()","8adc9468":"y = df3['CLASS']\nX = df3.drop(columns=['CLASS'], axis = 1)","839e3d39":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","aaa64c13":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)","b95209f3":"lr = LogisticRegression()\nlr.fit(X_train, y_train)\npred_lr = lr.predict(X_test)\nprint( \"Classification Report :\\n \", classification_report(y_test, pred_lr))","726f6c64":"dt = DecisionTreeClassifier(max_depth=6)\ndt.fit(X_train, y_train)\npred_dt = dt.predict(X_test)\nprint( \"Classification Report :\\n \", classification_report(y_test, pred_dt))","6d492907":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\npred_rf = rf.predict(X_test)\nprint( \"Classification Report :\\n \", classification_report(y_test, pred_rf))","471369e2":"gra = GradientBoostingClassifier()\ngra.fit(X_train, y_train)\npred_gra = gra.predict(X_test)\nprint( \"Classification Report :\\n \", classification_report(y_test, pred_gra))","3e11dbcc":"from collections import OrderedDict","6d4685a8":"feature_imp = {}\nfor i in zip(X.columns, lr.coef_[0]):\n    feature_imp[i[0]] = i[1]\nfinal_imp = OrderedDict(feature_imp)\ndf_features = pd.DataFrame(final_imp, index = range(1)).T\ndf_features.rename(columns={0: \"Importance_lr\"}, inplace = True)\n\nmy_colors = ['g', 'b']*5\n\ndf_features.plot(kind='bar',figsize = (20,5), color = my_colors)\n#list(feature_imp.values())","2b0228fd":"feature_imp2 = {}\nfor i in zip(X.columns, rf.feature_importances_):\n    feature_imp2[i[0]] = i[1]\n\nfinal_imp2 = OrderedDict(feature_imp2)\n#print(feature_imp2)\ndf_features2 = pd.DataFrame(final_imp2, index = range(1)).T\ndf_features2.rename(columns={0: \"Importance_rf\"}, inplace = True)\ndf_features2.plot(kind='bar',figsize = (15, 5), color = my_colors)","8b852b37":"df_compare = pd.concat([df_features, df_features2], axis = 1)\ndf_compare.plot(kind='bar',figsize = (20, 5))","e44ff1a3":"from keras.models import Sequential\nfrom keras.layers import (Dense, Flatten, Dropout, BatchNormalization)","910fc1d2":"model = Sequential()\nmodel.add(Dense(128 , input_dim = 38, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.33))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.33))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(1, activation = 'sigmoid'))","86d5e5b6":"model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])","34cc9584":"model.summary()","55d35a00":"model.fit(X, y, batch_size=64, epochs = 20, verbose=1)","c669c4db":"prediction = model.predict(X_test)","4056d581":"def finalPredictions(x):\n    if x<0.5 : \n        return 0\n    else:\n        return 1\npred_deep = []\nfor i in prediction:\n    pred_deep.append(finalPredictions(i))\n    ","3a4fcbcc":"print(classification_report(y_test, pred_deep))","5180a4b4":"# Conclusion : \nBy this, we can conclude that Random Forest Classifier is our best bet.\n\n### Why did our Deep Learning architecture failed?\nWe can refer to it failing heavily because of extensive beinary features and hashing in our Dataset. also, the bias doesn't really help our cause.","26f1178f":"# Inference for Random Forest Classifier\n\nWe can easily see that the feature importance is way different for Random forest classifier which focusses primely on **Chromosomes (CHROM)** Hashes","600fdd0b":"We will be applying Feature Hashers on the columns with > 10 unique values and One Hot Encoding schemes otherwise.","ee1c6c40":"# The model summary of what we will be using ","ec233261":"# Importing the Machine Learning Libraries","0e4f3d45":"# Making the Final Table\n\nWe will be storing the final values in the table *df3* and you can see the new columns after Feature Engineering as follows","28608ef6":"# What does the deep intuition say?","c7f85dc3":"# Decision Trees\n\nThis classifier provides a way better performance with a total precision of *65%* . Thankfully, it shows significant  improvement in *Recall* score of **non conflicting genes** and the Precision in **conflicting Genes**","3d6737ea":"# Random Forest\n\nThis classifier doesn't really performs any better, but the thing to take away from this classifier is the improvement in **Recall** and **f1 - score** for **conficting cases**","fb812dd6":"As you can observe in the graph given below, the dataset happens to be heavily biased towards the **non - conflicting** genes and that too with the **CHROM == 2** standing out as the clear bias winner.\n\n### What to take out from this graph?\n\nSince the incidents where the genes are recorded to be **conflicting**, we can assume that our classifiers won't be doing much of a great job and we can assume that most of them would take **CHROM** 2 as their most important feature if we don't feature hash it into something of a lower dimension","3f103805":"# Inference for Logistic Regression\nFrom the graph below, we can easily understand that the most important features taken into consideration by our Logistic Regressor are **Inversion**,** Microsatellite** and **LOW**, while the chromosomes are given very little or no importance","53ac1b1c":"# For a side by side comparison of features, refer to this:","1f72c60f":"# Logistic Regression\n\nSadly, this provides a mere 56% accuracy in terms of total precision. \nOne more thing to notice is that it didn't crack even one case where the **genes were conflicting**","efbb67f1":"# Feature Engineering","981564a5":"# Comparing Feature Importance\n\nWe should have a look at how **Random Forest** and **Logistic regression**, the best and the worst performers in this classification task came about ranking the importance of their given features!","777fe645":"# Exploratory Data Analysis\n ","d6711661":"# Adding Deep Learning in the Sauce","cd0b8667":"# Cleaning our data\n\nAs you can see from the graph given below, the properties in *yellow* are the null values. Simply by looking at the columns, one can judge that they are not really worth estimating due to massive information lack in our dataset. \n\n### So, what are the measures that we should take?\n\nWe can simply try considering the ones with minimum losses and consider the features with  <1000 unique features","5a6b26f9":"# Gradient Boost Classifier\n\nProvides the best overall accuracy, but no support for Recall and F1-score. ","6de13561":"# Kernel Objectives\nExploring the Data and checking out what to really expect from different Classifiers and determining what really matters to each classifier.\n\n### Also, Checking out why our Deep Learning Model Failed on the same features on which our random Forest Classifier excelled\n\n![](https:\/\/www.york.ac.uk\/media\/study\/courses\/undergraduate\/biology\/Genetics-bsc-banner.jpg)"}}