{"cell_type":{"6c16ffca":"code","15ff5ba7":"code","2db03911":"code","6eb996d1":"code","c7232047":"code","afe85271":"code","fbd077d6":"code","e52247bb":"code","4fd1e48b":"code","63eff5d9":"code","f300417a":"code","90f84312":"code","220f4a65":"code","5bd21ea8":"code","72a1541e":"code","b208b092":"code","ef683179":"code","106f422c":"code","3d3a47a3":"code","b621f93b":"code","211079a8":"code","60422478":"code","43c9e5b9":"code","eab7e906":"code","ab7a7162":"code","ac95dd13":"code","ec885ce8":"code","2658b565":"code","349507fb":"code","1911b535":"code","ba6405dd":"code","c584363d":"code","b0bb8206":"code","5e13a0f7":"code","999997d5":"code","d0801b8c":"code","a39cf30b":"code","55a64657":"code","bd8d0737":"code","1dc31852":"code","af3a5708":"code","d2a4672c":"markdown","0b1786e5":"markdown","af92df40":"markdown","e0517c0e":"markdown","3abec0b8":"markdown","1790ad9c":"markdown","036383f6":"markdown","a0018523":"markdown","8b444939":"markdown","bc4b36a4":"markdown","6411f31a":"markdown","c815f7b8":"markdown","b1df582f":"markdown"},"source":{"6c16ffca":"import os\nprint(os.listdir('..\/input\/iphone13\/testData\/testData'))","15ff5ba7":"print(os.listdir('..\/input\/iphone13\/iphone\/iphone'))","2db03911":"dataPath = '\/kaggle\/input\/iphone13\/iphone\/iphone'","6eb996d1":"# Import Libraries\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.applications.mobilenet import preprocess_input, decode_predictions\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom sklearn.metrics import classification_report, confusion_matrix","c7232047":"import numpy as np\nimport cv2\nimport glob\nimport random\n\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt","afe85271":"def prepare_image(filepath):\n    img = cv2.imread(filepath)\n    img_resized = cv2.resize(img, (224, 224), interpolation=cv2.INTER_CUBIC)\n    img_result  = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n    return img_result","fbd077d6":"dirList = glob.glob(dataPath+'\/*') # list of all directories in dataPath\ndirList.sort() # sorted in alphabetical order\nprint(dirList)","e52247bb":"Y_data = []\nfor i in range(len(dirList)):\n    fileList = glob.glob(dirList[i]+'\/*')\n    [Y_data.append(i) for file in fileList]\nprint(Y_data)","4fd1e48b":"X_data = []\nfor i in range(len(dirList)):\n    fileList = glob.glob(dirList[i]+'\/*')\n    [X_data.append(prepare_image(file)) for file in fileList]\nX_data = np.asarray(X_data)\nprint(X_data.shape)","63eff5d9":"## random shuffle\nfrom sklearn.utils import shuffle\nX_data, Y_data = shuffle(X_data, Y_data, random_state=0)","f300417a":"print(Y_data)","90f84312":"testNum = random.randint(0,len(X_data))\nprint(testNum)\nplt.imshow(X_data[testNum])","220f4a65":"num_classes = len(dirList) \nlabels = [dir.replace(dataPath+\"\/\", \"\") for dir in dirList]\nprint(labels)","5bd21ea8":"equilibre = []\n[equilibre.append(Y_data.count(i)) for i in range(len(dirList))]\nprint(equilibre)","72a1541e":"# plot the circle of value counts in dataset\nplt.figure(figsize=(5,5))\nmy_circle=plt.Circle( (0,0), 0.5, color='white')\nplt.pie(equilibre, labels=labels, colors=['red','orange','yellow','green','blue','purple','black','silver'],autopct='%1.1f%%')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.show()","b208b092":"# Data Normalisation\nX_train = X_data \/ 255.0\nprint(X_train.shape)","ef683179":"# One-hot encoding\nY_train = to_categorical(Y_data)\nprint(Y_train.shape)","106f422c":"input_shape = (224, 224, 3)","3d3a47a3":"# use MobieNet V2 as base model\nbase_model=MobileNetV2(input_shape=(224,224,3),weights='imagenet',include_top=False) \n\n# add Fully-Connected Layers to Model\nx=base_model.output\nx=GlobalAveragePooling2D()(x)\nx=Dense(1024,activation='relu')(x) # FC layer 1\nx=Dense(64,activation='relu')(x)   # FC layer 2\npreds=Dense(num_classes,activation='softmax')(x) #final layer with softmax activation\n\nmodel=Model(inputs=base_model.input,outputs=preds)\nmodel.summary()","b621f93b":"# Check layers no. & name\nfor i,layer in enumerate(model.layers):\n    print(i,layer.name)","211079a8":"# set extra layers to trainable \nfor layer in model.layers[:155]:\n    layer.trainable=False\nfor layer in model.layers[155:]:\n    layer.trainable=True","60422478":"model.summary()","43c9e5b9":"# Compile Model\nmodel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","eab7e906":"# Train Model (target is loss <0.01)\nbatch_size= 16\nnum_epochs = 10\nhistory = model.fit(X_train,Y_train,batch_size=batch_size,epochs=num_epochs)","ab7a7162":"# Save Model\nmodel.save('tl_iphone.h5')","ac95dd13":"def classify_worms(imageFile):\n    testData = prepare_image(imageFile).reshape(1,224,224,3)\n    testData = testData \/ 255.0\n    predictions = model.predict(testData)\n    maxindex = int(np.argmax(predictions))\n    print(predictions[0][maxindex],labels[maxindex])\n    return labels[maxindex]","ec885ce8":"imageFile=dirList[0]+'\/iphone11-black-select-2019.png'\nplt.imshow(prepare_image(imageFile))\nclassify_worms(imageFile)","2658b565":"# test model\ntestData = prepare_image(imageFile).reshape(1,224,224,3)\ntestData = testData \/ 255.0\npredictions = model.predict(testData)\nmaxindex = int(np.argmax(predictions))\nprint(predictions[0][maxindex],labels[maxindex])","349507fb":"imageFile=dirList[1]+'\/iphone-12-red-select-2020.png'\nplt.imshow(prepare_image(imageFile))\nclassify_worms(imageFile)","1911b535":"imageFile=dirList[2]+'\/iphone-12-mini-blue-select-2020.png'\nplt.imshow(prepare_image(imageFile))\nclassify_worms(imageFile)","ba6405dd":"imageFile=dirList[3]+'\/iphone-13-midnight-select-2021.png'\nplt.imshow(prepare_image(imageFile))\nclassify_worms(imageFile)","c584363d":"imageFile=dirList[4]+'\/iphone-13-mini-pink-select-2021.png'\nplt.imshow(prepare_image(imageFile))\nclassify_worms(imageFile)","b0bb8206":"imageFile=dirList[5]+'\/iphone-13-pro-graphite-select.png'\nplt.imshow(prepare_image(imageFile))\nclassify_worms(imageFile)","5e13a0f7":"imageFile=dirList[6]+'\/iphone-13-pro-max-silver-select.png'\nplt.imshow(prepare_image(imageFile))\nclassify_worms(imageFile)","999997d5":"imageFile=dirList[7]+'\/iphone-se-red-select-2020.png'\nplt.imshow(prepare_image(imageFile))\nclassify_worms(imageFile)","d0801b8c":"Y_pred = model.predict(X_train)\ny_pred = np.argmax(Y_pred,axis=1)\n#y_label= [labels[k] for k in y_pred]\ncm = confusion_matrix(Y_data, y_pred)\nprint(cm)","a39cf30b":"print(classification_report(Y_data, y_pred, target_names=labels))","55a64657":"TP = cm[1, 1]\nTN = cm[0, 0]\nFP = cm[0, 1]\nFN = cm[1, 0]\nspecificity = TN \/ float( TN + FP)\nsensitivity = TP \/ float(FN + TP)\nprint('Specificity:',specificity)\nprint('Sensitivity:',sensitivity)","bd8d0737":"import itertools\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n        \n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","1dc31852":"plot_confusion_matrix(cm, \n                      normalize=False,\n                      target_names = labels,\n                      title=\"Confusion Matrix, not Normalized\")","af3a5708":"print(classification_report(Y_data, y_pred, target_names=labels))","d2a4672c":"## Transfer Learning setup","0b1786e5":"## Save Model","af92df40":"## Dataset = iphone (iphone 11, iphone 12, iphone 12 mini, iphone 13, iphone 13 mini, iphone 13 pro, iphone 13 pro max, iphone SE)","e0517c0e":"## Plot Confusion Matrix","3abec0b8":"## set FC-layers to trainable","1790ad9c":"### check 1 picture per category","036383f6":"## shuffle data","a0018523":"## Load MobileNet v2 model & add FC-layers","8b444939":"## Prepare Data","bc4b36a4":"## Confusion Matrix report","6411f31a":"## Test Model","c815f7b8":"# iPhone Classification\n## Transfer Learning : Mobilenet V2","b1df582f":"## Data Normalisation"}}