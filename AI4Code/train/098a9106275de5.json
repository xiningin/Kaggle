{"cell_type":{"447b98ff":"code","7b3b46fa":"code","0adadcd9":"code","1144479a":"code","8f143b4c":"code","49c065a2":"code","43dbdf71":"code","36475ee4":"code","462010be":"code","5d682a18":"code","567a223d":"code","df1d09f6":"code","625949e6":"code","62332c00":"code","c042cc3f":"code","d82b8885":"code","7500b501":"code","d1a33d99":"code","23171482":"code","fd19d4f0":"code","abd71266":"code","3ededf5d":"code","88a38b19":"code","3cde932b":"code","68e3679d":"code","ca62ed0d":"code","0794beb4":"code","b4e292ba":"code","32f600a7":"code","d70fa425":"code","952a08e5":"code","2d398eec":"code","8046c1e8":"code","b304bbe7":"code","74d1484d":"code","c5dfd99c":"code","b5006a09":"code","bad86ff5":"code","a39814ce":"code","8bcd0893":"code","b77faea4":"code","61db9c49":"code","017cabdf":"code","d0722551":"code","eeaff1bf":"code","88a53063":"code","efcdd8c0":"code","9eb32df7":"code","75afbd16":"code","568d5eb9":"code","fdcf5a74":"code","dedf0526":"code","241ea89e":"code","ad78bb61":"markdown","1071f56d":"markdown","c434d098":"markdown","c6b9deb3":"markdown"},"source":{"447b98ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7b3b46fa":"# read dataset \ntitanic_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntitanic_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntitanic_train.head()","0adadcd9":"# import packages \nimport warnings\nimport pandas as  pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np    # linear algebra\nimport matplotlib.pyplot as plt # visulization\nimport seaborn as sns # visulization\nimport missingno as msno # handling missing value\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler # dataset range (0 to 1)\nfrom sklearn.model_selection import train_test_split # splitting the test dataset and train dataset\nfrom sklearn.metrics import confusion_matrix,accuracy_score # model perfomance \nfrom sklearn.linear_model import LogisticRegression  # clasiifier\nfrom sklearn.neighbors import KNeighborsClassifier # # clasiifier\nfrom sklearn.tree import DecisionTreeClassifier # # clasiifier\nfrom sklearn.ensemble import RandomForestClassifier , AdaBoostClassifier # clasiifier\nfrom sklearn.svm import SVC # clasiifierpackages  \nimport xgboost as xgb # clasiifierpackages \nfrom sklearn import preprocessing","1144479a":"#  show  top 5 row of train data set\ntitanic_train.head(5)","8f143b4c":"#  show  top 5 row of test data set\ntitanic_test.head(5)","49c065a2":"# numbers of columns in datasets\ntitanic_train.columns","43dbdf71":"# dataset shape (row ,columns)\nprint(titanic_train.shape)\nprint(titanic_test.shape)","36475ee4":"# data types in datasets\ntitanic_train.dtypes","462010be":"# Get some statistical information\ntitanic_train.hist(bins=20,figsize=(20,10),color=\"#F1948B\")","5d682a18":"# What is the distribution of numerical feature values across the samples?\n\ntitanic_train.describe()","567a223d":"titanic_train.describe().T","df1d09f6":"# What is the distribution of categorical features?\ntitanic_train.describe(include=\"object\")","625949e6":"# Survived by pclass\ntitanic_train[[\"Pclass\",\"Survived\"]].groupby(\"Pclass\",as_index=False).mean().sort_values(by=\"Survived\",ascending=False)","62332c00":"titanic_train[[\"Parch\",\"Survived\"]].groupby([\"Parch\"],as_index=False).mean().sort_values(by=\"Survived\",ascending=False)","c042cc3f":"# Survived by Sex\ntitanic_train[[\"Survived\",\"Sex\"]].groupby(\"Sex\").mean().sort_values(by=\"Survived\",ascending=False)","d82b8885":"titanic_train[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"],as_index=False).mean().sort_values(by=\"Survived\",ascending=False)","7500b501":"# visualizing age by survived\nage_survived=sns.FacetGrid(titanic_train,col=\"Survived\").map(plt.hist,\"Age\",bins=20)","d1a33d99":"pclass_survived=sns.FacetGrid(titanic_train,col=\"Survived\").map(plt.hist,\"Age\",bins=20)","23171482":"titanic_train[titanic_train.Survived==1].Sex.value_counts()","fd19d4f0":"#Deleting columns which are of no use\n\n# Train Set\n\ntitanic_train=titanic_train.drop([\"PassengerId\",\"Ticket\"],axis=1)\n\n# Submission\nsubmission=pd.DataFrame(columns=[\"PassengerId\", \"Survived\"])\nsubmission[\"PassengerId\"]=titanic_test[\"PassengerId\"]\n\n# test dataset\ntitanic_test = titanic_test.drop([\"PassengerId\", \"Ticket\"], axis=1)\n","abd71266":"# Check for missing values\ntitanic_train.isnull().sum()","3ededf5d":"# bar plot missing value\n\nfig, axis=plt.subplots(1,2 ,figsize=(20,5),sharey=True)\n\nmsno.bar(titanic_train,ax=axis[0],color='#2FA353')\n\nmsno.bar(titanic_test,ax=axis[1], color='#2F7DA3')","88a38b19":"#Filling Age missing values of training & test data set with Median\n\ntitanic_train[\"Age\"].fillna(titanic_train[\"Age\"].median(), inplace=True)\ntitanic_test[\"Age\"].fillna(titanic_test[\"Age\"].median(), inplace=True)","3cde932b":"# As we saw earlier also in the graph\ntitanic_train[\"Embarked\"].value_counts()","68e3679d":"# Fill NAN of Embarked in training set with 'S'\ntitanic_train[\"Embarked\"].fillna(\"S\", inplace = True)","ca62ed0d":"# Fill Missing Values for Cabin in training set with 0\ntitanic_train[\"Cabin\"] = titanic_train[\"Cabin\"].apply(lambda x: str(x)[0])\ntitanic_train.groupby([\"Cabin\", \"Pclass\"])[\"Pclass\"].count()","0794beb4":"titanic_train[\"Cabin\"] = titanic_train[\"Cabin\"].replace(\"n\", 0)\ntitanic_train[\"Cabin\"] = titanic_train[\"Cabin\"].replace([\"A\", \"B\", \"C\", \"D\", \"E\", \"T\"], 1)\ntitanic_train[\"Cabin\"] = titanic_train[\"Cabin\"].replace(\"F\", 2)\ntitanic_train[\"Cabin\"] = titanic_train[\"Cabin\"].replace(\"G\", 3)","b4e292ba":"titanic_test[\"Cabin\"] = titanic_test[\"Cabin\"].apply(lambda x: str(x)[0])\ntitanic_test.groupby([\"Cabin\", \"Pclass\"])[\"Pclass\"].count()","32f600a7":"titanic_test[\"Cabin\"] = titanic_test[\"Cabin\"].replace(\"n\", 0)\ntitanic_test[\"Cabin\"] = titanic_test[\"Cabin\"].replace([\"A\", \"B\", \"C\", \"D\", \"E\"], 1)\ntitanic_test[\"Cabin\"] = titanic_test[\"Cabin\"].replace(\"F\", 2)\ntitanic_test[\"Cabin\"] = titanic_test[\"Cabin\"].replace(\"G\", 3)","d70fa425":"#Creating new variable Family Size & Alone\n# Train Set\ntitanic_train[\"Family\"] = titanic_train[\"SibSp\"]+titanic_train[\"Parch\"]\n\n#Test Set\ntitanic_test[\"Family\"] = titanic_test[\"SibSp\"]+titanic_test[\"Parch\"]","952a08e5":"# 1 If alone & 0 if it has family members\ntitanic_train[\"Alone\"] = titanic_train[\"Family\"].apply(lambda x:1 if x==0 else 0)\ntitanic_test[\"Alone\"] = titanic_test[\"Family\"].apply(lambda x:1 if x==0 else 0)","2d398eec":"titanic_test.head(3)","8046c1e8":"# Considering the other features, filling the NAN value of Fare accordingly\nm_fare = titanic_test[(titanic_test[\"Pclass\"] == 3) & (titanic_test[\"Embarked\"] == \"S\") & (titanic_test[\"Alone\"] == 1)][\"Fare\"].mean()\nm_fare","b304bbe7":"titanic_test[\"Fare\"] = titanic_test[\"Fare\"].fillna(m_fare)\ntitanic_train[\"Fare\"] = titanic_train[\"Fare\"].fillna(m_fare)","74d1484d":"def title(name):\n    for string in name.split():\n        if \".\" in string:\n            return string[:-1]\n\ntitanic_train[\"Title\"] = titanic_train[\"Name\"].apply(lambda x: title(x))\ntitanic_test[\"Title\"] = titanic_test[\"Name\"].apply(lambda x: title(x))\n\nprint(titanic_train[\"Title\"].value_counts())\nprint(titanic_test[\"Title\"].value_counts())","c5dfd99c":"for titanic in [titanic_train, titanic_test]:\n    titanic[\"Title\"] = titanic[\"Title\"].replace([\"Dr\", \"Rev\", \"Major\", \"Col\", \"Capt\", \"Lady\", \"Jonkheer\", \"Sir\", \"Don\", \"Countess\", \"Dona\"], \"Others\")\n    titanic[\"Title\"] = titanic[\"Title\"].replace(\"Mlle\", \"Miss\")\n    titanic[\"Title\"] = titanic[\"Title\"].replace(\"Ms\", \"Miss\")\n    titanic[\"Title\"] = titanic[\"Title\"].replace(\"Mme\", \"Mr\")","b5006a09":"# Remove few more columns\n\ntitanic_train = titanic_train.drop([\"Name\", \"SibSp\", \"Parch\"], axis=1)\ntitanic_test = titanic_test.drop([\"Name\", \"SibSp\", \"Parch\"], axis=1)","bad86ff5":"# Print the unique values of the categorical columns\nprint(titanic_train['Sex'].unique())\nprint(titanic_train['Embarked'].unique())\nprint(titanic_train['Title'].unique())","a39814ce":" titanic_train.isnull().sum()","8bcd0893":"from sklearn.preprocessing import LabelEncoder\nlabel_encode = LabelEncoder()\nvar_mod = ['Sex','Embarked','Title']\nfor i in var_mod:\n    titanic_train[i] = label_encode.fit_transform(titanic_train[i])\n    titanic_test[i] = label_encode.fit_transform(titanic_test[i])","b77faea4":"titanic_train = pd.get_dummies(titanic_train, columns =['Sex','Embarked','Cabin', 'Pclass', 'Title'])\ntitanic_test = pd.get_dummies(titanic_test, columns =['Sex','Embarked', 'Cabin', 'Pclass', 'Title'])","61db9c49":"# Split the titanic_train data set into features ``x`` & label ``y``\nx = titanic_train.iloc[:,1:22].values\ny = titanic_train.iloc[:,0].values\n\ntitanic_train.columns","017cabdf":"# Splitting the data set into 80% Training & 20% Testing\ntrain_x, test_x, train_y, test_y = train_test_split(x,y, test_size = 0.2, random_state = 42)\ntrain_x.shape, test_x.shape, train_y.shape, test_y.shape","d0722551":"feature_scale = StandardScaler()\ntrain_x = feature_scale.fit_transform(train_x)\ntest_x = feature_scale.transform(test_x)","eeaff1bf":"# from sklearn import utils\n# lab_enc = preprocessing.LabelEncoder()\n# train_y = lab_enc.fit_transform(train_y)\n# print(train_y)\n# print(utils.multiclass.type_of_target(train_y))\n# print(utils.multiclass.type_of_target(train_y.astype('int')))\n# print(utils.multiclass.type_of_target(train_y))","88a53063":"# Scaling titanic_test data set as well\nscale_test_data = feature_scale.fit_transform(titanic_test)","efcdd8c0":"scale_test_data.shape","9eb32df7":"def models(train_x, train_y):\n    \n    #Logistic Regression\n    log_reg = LogisticRegression(random_state = 42)\n    log_reg.fit(train_x,train_y)\n    \n    #KNN\n    knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n    knn.fit(train_x, train_y)\n    \n    #SVC Linear\n    svc_lin = SVC(kernel = 'linear', random_state=42)\n    svc_lin.fit(train_x, train_y)\n    \n    #SVC RBF\n    svc_rbf = SVC(kernel = 'rbf', random_state=42)\n    svc_rbf.fit(train_x, train_y)\n    \n    #Decision Tree Classifier\n    dec_tree = DecisionTreeClassifier(criterion='entropy', random_state=42)\n    dec_tree.fit(train_x, train_y)\n    \n    #Random Forest Classifier\n    rf = RandomForestClassifier(n_estimators=10, criterion='entropy', max_depth=10, random_state=42)\n    rf.fit(train_x, train_y)\n    \n    model=DecisionTreeClassifier(criterion=\"entropy\",max_depth=1)\n    AdaBoost=AdaBoostClassifier(base_estimator=model,n_estimators=100,learning_rate=1)\n    bostmodel=AdaBoost.fit(train_x,train_y)\n    \n    xgb_cls =xgb.XGBClassifier(learning_rate =0.1,n_estimators=1000, max_depth=4,min_child_weight=6,gamma=0, subsample=0.8,\n                            colsample_bytree=0.8,reg_alpha=0.005,objective= 'binary:logistic',nthread=4,scale_pos_weight=1,\n                            seed=27)\n    xgb_cls.fit(train_x,train_y)\n    #Printing accuracy for every model\n    print('[0] Logistic Regression training accuracy: ', log_reg.score(train_x, train_y))\n    print('[1] KNN training accuracy: ', knn.score(train_x, train_y))\n    print('[2] SVC_Linear training accuracy: ', svc_lin.score(train_x, train_y))\n    print('[3] SVC_RBF training accuracy: ', svc_rbf.score(train_x, train_y))\n    print('[4] Decision Tree training accuracy: ', dec_tree.score(train_x, train_y))\n    print('[5] Random Forest training accuracy: ', rf.score(train_x, train_y))\n    \n    print('[6] AdaBoostClassifie training accuracy: ', AdaBoost.score(train_x, train_y))\n    print('[7] XGBoostClassifie training accuracy: ', xgb_cls.score(train_x, train_y))\n        \n    return log_reg ,knn, svc_lin, svc_rbf, dec_tree, rf, xgb_cls,AdaBoost","75afbd16":"# Get and Train all the models\nmodel = models(train_x, train_y)","568d5eb9":"# Printing the prediction of Random Forest\npred = model[4].predict(test_x)\nprint(pred)\n\nprint()\n\n# Printing the actual values\nprint(test_y)","fdcf5a74":"# Creating confusion matrix and see the accuracy for all the models for test data\n\nfor i in range( len(model) ):\n    cm  = confusion_matrix(test_y, model[i].predict(test_x))\n    \n    # Extract the confusion matrix parameters\n    TN, FP, FN, TP = confusion_matrix(test_y, model[i].predict(test_x)).ravel()\n    \n    test_score = (TP+TN) \/ (TP+TN+FP+FN)\n    \n    print(cm)\n    print('Model[{}] Testing Accuracy =\"{}\"'.format(i, test_score))\n    print()","dedf0526":"dt= model[7]\npred_rand_for = dt.predict(scale_test_data)\nsubmission[\"Survived\"] = pred_rand_for","241ea89e":"submission.head(6)","ad78bb61":" # Creating a Function with multiple models","1071f56d":"# Which features are numerical?\nThese values change from sample to sample.Within numerical features are the values discrete, continuous, or timeseries based? Among other things this helps us select the appropriate plots for visualization.\n\nContinous: Age, Fare. Discrete: SibSp, Parch","c434d098":"\n\n# Which features are categorical?\nThese values classify the samples into sets of similar samples.Within categorical features are the values nominal, ordinal, ratio, or interval based? Among other things this helps us select the appropriate plots for visualization.\n\nCategorical: Survived, Sex, and Embarked. Ordinal: Pclass.","c6b9deb3":"we can replace a few titles: like there is `Ms which can be replaced with Miss, as they refer to the same gender group. And few like Major, Capt, etc replace them with others"}}