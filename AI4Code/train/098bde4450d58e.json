{"cell_type":{"7cc71daf":"code","df0130ba":"code","e581d1c9":"code","26ae9d61":"code","d181948b":"code","78b75eab":"code","6e52d253":"code","dcc4da6b":"code","7863750a":"code","76ebbc92":"code","f844ccf0":"code","ea4fedc1":"code","8daa445d":"code","725afc9b":"code","830292b8":"code","04147248":"code","72bed1b3":"code","d7b0ec60":"code","4563d9d5":"code","152a7720":"code","898d4aad":"code","4d07820c":"code","e4adcf46":"code","b90edf26":"code","a65cf1a1":"code","70a6bceb":"code","e50c96f6":"code","0f0db543":"code","d82b3af5":"code","fabd6e38":"code","467a6693":"code","76e1086b":"code","c57efb81":"code","8d7103c2":"code","ca8b283b":"code","0e2516e4":"code","b63e33b1":"code","b7efa7b3":"code","eac94c8d":"code","fd0754a3":"code","c6356007":"code","cfba900b":"code","77e7b006":"code","2be99f13":"code","3e093b34":"code","1bc0a948":"code","18fb3361":"code","4910bb61":"code","60a36aa1":"code","b9fa2590":"code","1f598d7e":"code","ef2b55b4":"code","c15e252b":"code","c5ca0e89":"code","e48ca31a":"code","51e32ee8":"code","563e857d":"code","a01cfad5":"markdown","87dc9419":"markdown","ab9ead4a":"markdown","73e72180":"markdown","91199cb7":"markdown","6637e62d":"markdown","f3a908c8":"markdown","0b743a36":"markdown","fcb2c383":"markdown","7a3a9d44":"markdown","0fc49c18":"markdown","c9270a68":"markdown","b74e5585":"markdown","bd37c4ac":"markdown","c5816a22":"markdown","d9efcbb8":"markdown","da03a236":"markdown","d4951943":"markdown","f706dc62":"markdown","463e39ec":"markdown","419a236e":"markdown","8ae09844":"markdown","f1968aa3":"markdown","73392be2":"markdown","a322b05c":"markdown","044ae62c":"markdown","cc0c1d60":"markdown","c1b6b5ff":"markdown"},"source":{"7cc71daf":"# required libraries\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\nimport seaborn as sns\n\nimport re\nimport string\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import wordnet\nfrom nltk import pos_tag\nfrom nltk.stem import WordNetLemmatizer\n\nimport spacy\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom collections import Counter\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.utils import shuffle\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom xgboost import XGBClassifier\n\nfrom transformers import pipeline\n\nimport warnings\nwarnings.filterwarnings('ignore')","df0130ba":"# load dataset \ndf = pd.read_csv('\/kaggle\/input\/consumer-reviews-of-amazon-products\/1429_1.csv')\ndf.head()","e581d1c9":"df.info()","26ae9d61":"data = df[[\"reviews.text\",\"reviews.rating\"]]\ndata.head()","d181948b":"data.info()","78b75eab":"# drop missing values\ndata.dropna(inplace=True)\ndata.isnull().sum()","6e52d253":"# explore some of the reviews\nimport random\n\nn_samples = 5\n\nfor _ in range(n_samples):\n    i = random.choice(range(data.shape[0]))\n    print(f\"REVIEW TEXT:\\n{data['reviews.text'][i]} \\n\\nRATE:\\n{data['reviews.rating'][i]}\")\n    print('\\n', 90*\"-\", '\\n')","dcc4da6b":"# descriptive statistics\ndata.describe()","7863750a":"# distribution of rating\ndata['reviews.rating'].value_counts().sort_index(ascending=False)","76ebbc92":"# distribution of rating\nsns.countplot(data['reviews.rating'], palette='Blues')\n\nplt.title('Distribution of rating scores')\nplt.xlabel('Rating')\nplt.ylabel('Count')\nplt.show()","f844ccf0":"# load the other data\ndata2 = pd.read_csv(\"\/kaggle\/input\/consumer-reviews-of-amazon-products\/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products_May19.csv\")\ndata2 = data2[['reviews.text', 'reviews.rating']]\n# use only data of rating lower than or equal to 3\ndata2 = data2[data2[\"reviews.rating\"] <= 3].reset_index(drop=True) # reset index after filtering rows\n\ndata3 = pd.read_csv(\"\/kaggle\/input\/consumer-reviews-of-amazon-products\/Datafiniti_Amazon_Consumer_Reviews_of_Amazon_Products.csv\")\ndata3 = data3[['reviews.text', 'reviews.rating']]\n# use only data of rating lower than or equal to 3\ndata3 = data3[data3[\"reviews.rating\"] <= 3].reset_index(drop=True) # reset index after filtering rows","ea4fedc1":"# distribution of rating\ndata2['reviews.rating'].value_counts().sort_index(ascending=False)","8daa445d":"# distribution of rating\ndata3['reviews.rating'].value_counts().sort_index(ascending=False)","725afc9b":"# concatenation\ndata = pd.concat([data, data2, data3])\n\n# assert data.shape[0] == data.shape[0] + data2.shape[0] + data3.shape[0]\ndata.head()","830292b8":"# distribution of rating\ndata['reviews.rating'].value_counts().sort_index(ascending=False)","04147248":"# distribution of rating\nsns.countplot(data['reviews.rating'], palette='Blues')\n\nplt.title('Distribution of rating scores')\nplt.xlabel('Rating')\nplt.ylabel('Count')\nplt.show()","72bed1b3":"# map ratings 1, 2, 3 to 0 (NEGATIVE) and 4, 5 to 1 (POSITIVE) \nsentiment_score = {1: 0,\n                   2: 0,\n                   3: 0,\n                   4: 1,\n                   5: 1}\n\nsentiment = {0: 'NEGATIVE',\n             1: 'POSITIVE'}\n\n\n# mapping\ndata['sentiment_score'] = data['reviews.rating'].map(sentiment_score)\ndata['sentiment'] = data['sentiment_score'].map(sentiment)\n\ndata.head()","d7b0ec60":"# distribution of sentiment\nplt.figure(figsize = (8, 8))\n\nlabels = ['POSITIVE', 'NEGATIVE']\ncolors = ['#189AB4', '#D4F1F4']\nplt.pie(data['sentiment'].value_counts(), autopct='%0.2f%%',colors=colors)\n\nplt.title('Distribution of sentiment', size=14, y=-0.01)\nplt.legend(labels, ncol=2, loc=9)\nplt.show()","4563d9d5":"# get all used words \nall_words = pd.Series(' '.join(data['reviews.text']).split())","152a7720":"# plot word cloud\nwordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(all_words))\n\nplt.figure(figsize=(15,8))\n\nplt.imshow(wordcloud)\nplt.title(\"Most used words in all reviws\", size=16)\n\nplt.axis(\"off\")\nplt.show()","898d4aad":"# get words used positive reivews \npositiveWords = pd.Series(' '.join(data[data['sentiment']=='POSITIVE']['reviews.text']).split())","4d07820c":"# plot word cloud\nwordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(positiveWords))\n\nplt.figure(figsize=(15,8))\n\nplt.imshow(wordcloud)\nplt.title(\"Most used words in positive reviews\", size=16)\n\nplt.axis(\"off\")\nplt.show()","e4adcf46":"# get words used negative reivews \nnegativeWords = pd.Series(' '.join(data[data['sentiment']=='NEGATIVE']['reviews.text']).split())","b90edf26":"# plot word cloud\nwordcloud = WordCloud(width = 1000, height = 500).generate(' '.join(negativeWords))\n\nplt.figure(figsize=(15,8))\n\nplt.imshow(wordcloud)\nplt.title(\"Most used words in negative reviews\", size=16)\n\nplt.axis(\"off\")\nplt.show()","a65cf1a1":"def clean_text(text:str):\n    \"\"\" Return cleaned text:\n            - lowercase\n            - remove whitespaces\n            - remove HTML tags\n            - replace digit with spaces\n            - replace punctuations with spaces\n            - remove extra spaces and tabs\n        ------\n        input: text (str)    \n        output: cleaned text (str)\n    \"\"\"\n    text = str(text)\n    \n    text = text.lower()\n    text = text.strip()\n    \n    text = re.sub(' \\d+', ' ', text)\n    text = re.compile('<.*?>').sub('', text)\n    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)\n    text = re.sub('\\s+', ' ', text)\n    \n    text = text.strip()\n    \n    return text","70a6bceb":"# test\ntext = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  26 adjacent spaces and tabs     .  \"\nprint(text, '\\n')\nclean_text(text)","e50c96f6":"def remove_stopwords(text:str):\n    \"\"\" Remove stopwords from text:\n        ------\n        input: text (str)    \n        output: cleaned text (str)\n    \"\"\"\n    text = str(text)\n    filtered_sentence = []\n\n    # Stop word lists can be adjusted for your problem\n    stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"]\n\n    # Tokenize the sentence\n    words = word_tokenize(text)\n    for w in words:\n        if w not in stop_words:\n            filtered_sentence.append(w)\n    text = \" \".join(filtered_sentence)\n    \n    return text","0f0db543":"# test\ntext = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  26 adjacent spaces and tabs     .  \"\nprint(text, '\\n')\ntext = clean_text(text)\nremove_stopwords(text)","d82b3af5":"def stemm_text(text:str):\n    \"\"\" Stemm text:\n    ------\n    input: text (str)    \n    output: Stemmed text (str)\n    \"\"\"\n    text = str(text)\n    # Initialize the stemmer\n    snow = SnowballStemmer('english')\n\n    stemmed_sentence = []\n    # Tokenize the sentence\n    words = word_tokenize(text)\n    for w in words:\n        # Stem the word\/token\n        stemmed_sentence.append(snow.stem(w))\n    text = \" \".join(stemmed_sentence)\n    \n    return text","fabd6e38":"# test\ntext = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  26 adjacent spaces and tabs     .  \"\nprint(text, '\\n')\ntext = clean_text(text)\ntext = remove_stopwords(text)\nstemm_text(text)","467a6693":"# This is a helper function to map NTLK position tags\n# Full list is available here: https:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","76e1086b":"def lemmatize(text:str):\n    \"\"\" lemmatize text:\n    ------\n    input: text (str)    \n    output: lemmatized text (str)\n    \"\"\"\n    text = str(text)\n    \n    # Initialize the lemmatizer\n    wl = WordNetLemmatizer()\n\n    lemmatized_sentence = []\n\n    # Tokenize the sentence\n    words = word_tokenize(text)\n    # Get position tags\n    word_pos_tags = nltk.pos_tag(words)\n    # Map the position tag and lemmatize the word\/token\n    for idx, tag in enumerate(word_pos_tags):\n        lemmatized_sentence.append(wl.lemmatize(tag[0], get_wordnet_pos(tag[1])))\n\n    lemmatized_text = \" \".join(lemmatized_sentence)\n    \n    return lemmatized_text","c57efb81":"# test\ntext = \"   This is a message to be cleaned. It may involve some things like: <br>, ?, :, ''  26 adjacent spaces and tabs     .  \"\nprint(text, '\\n')\ntext = clean_text(text)\ntext = remove_stopwords(text)\n# text = stemm_text(text)\nlemmatize(text)","8d7103c2":"# clean text\ndata['text'] = data['reviews.text'].apply(clean_text)\n# remove stopwords\ndata['text'] = data['text'].apply(remove_stopwords)\n# lemmatize\ndata['text'] = data['text'].apply(lemmatize)","ca8b283b":"# check some processed reviews\nimport random\n\ni = random.choice(range(len(data)))\n\nprint(f\"Original review: \\n{data['reviews.text'].iloc[i]}\\n\")\nprint(f\"Processed review: \\n{data['text'].iloc[i]}\")      ","0e2516e4":"# export cleaned data to csv\ndata.to_csv('cleaned_data.csv', index=False)","b63e33b1":"data = pd.read_csv('cleaned_data.csv')\ndata.info()","b7efa7b3":"data[data['text'].isnull()]","eac94c8d":"data.dropna(inplace=True)\ndata.info()","fd0754a3":"vectorizer = TfidfVectorizer(max_features=700)\nvectorizer.fit(data['text'])\nfeatures = vectorizer.transform(data['text'])\n\nfeatures.toarray()","c6356007":"tf_idf = pd.DataFrame(features.toarray(), columns=vectorizer.get_feature_names())\n# tf_idf.drop('50', axis=1, inplace=True)\ntf_idf.head()","cfba900b":"X_train, X_test, y_train, y_test = train_test_split(tf_idf, data['sentiment_score'], test_size=0.2, random_state=42)\n\nprint (f'Train set shape\\t:{X_train.shape}\\nTest set shape\\t:{X_test.shape}')","77e7b006":"yy = pd.DataFrame(y_train)\ntrain_data = pd.concat([X_train, yy],axis=1)\ntrain_data.head()\n","2be99f13":"train_data['sentiment_score'].value_counts()","3e093b34":"target_count = train_data['sentiment_score'].value_counts()\nnegative_class = train_data[train_data['sentiment_score'] == 0]\npositive_class = train_data[train_data['sentiment_score'] == 1]\n\nnegative_over = negative_class.sample(target_count[1], replace=True)\n\ndf_train_over = pd.concat([positive_class, negative_over], axis=0)\ndf_train_over = shuffle(df_train_over)\ndf_train_over.head()","1bc0a948":"df_train_over.dropna(inplace=True)","18fb3361":"df_train_over['sentiment_score'].value_counts()","4910bb61":"X_train = df_train_over.iloc[:,:-1]\ny_train = df_train_over['sentiment_score']","60a36aa1":"def modeling(Model, Xtrain = X_train, Xtest = X_test):\n    \"\"\"\n    This function apply countVectorizer with machine learning algorithms. \n    \"\"\"\n    \n    # Instantiate the classifier: model\n    model = Model\n    \n    # Fitting classifier to the Training set (all features)\n    model.fit(Xtrain, y_train)\n    \n    global y_pred\n    # Predicting the Test set results\n    y_pred = model.predict(Xtest)\n    \n    # Assign f1 score to a variable\n    print(classification_report(y_test, y_pred))\n    print ('AUC ',roc_auc_score(y_test, y_pred))\n    #cm = confusion_matrix(y_test, y_pred)\n    confusion_matrix = pd.crosstab(index=y_test, columns=np.round(y_pred), rownames=['Actual'], colnames=['Predictions']).astype(int)\n    plt.figure(figsize = (8,8))\n\n    '''\n    cmapGR = LinearSegmentedColormap.from_list(\n        name='test', \n        colors=['red','green']\n    )\n    '''\n    sns.heatmap(confusion_matrix, annot=True,annot_kws={\"fontsize\":12}, fmt='.2f', cmap='Blues').set_title('Confusion Matrix') ","b9fa2590":"modeling(MultinomialNB())","1f598d7e":"modeling(XGBClassifier());","ef2b55b4":"# Build sentiment analysis pipeline\nsentiment_analyzer = pipeline('sentiment-analysis')\n","c15e252b":"# test\nsentiment_analyzer(['Good product', 'Bad custemer service'])","c5ca0e89":"def transformer_check(n=1000):\n    \"\"\" Return Dataframe of reviews sentimental prediction and actual sentimental\"\"\"\n    res_list = []\n    for _ in range(n):\n        res_dict = {}\n        i = random.choice(range(len(data)))\n        text_ = data['text'].iloc[i]\n        sen_act = data['sentiment'].iloc[i]\n        sen_pred = sentiment_analyzer(text_)[0]['label']\n        res_dict['review'] = text_\n        res_dict['sen_act'] = sen_act\n        res_dict['sen_pred'] = sen_pred\n        res_dict['match'] = sen_act == sen_pred\n        \n        res_list.append(res_dict)\n        \n    return pd.DataFrame(res_list)","e48ca31a":"results = transformer_check()","51e32ee8":"results.sample(10)","563e857d":"print(result['match'].mean())","a01cfad5":"### Create TF-IDF","87dc9419":"> This looks better than the stemming result.","ab9ead4a":"There is a huge imbalnce in the data to the high rate classes<br>\n> We will try to add more data with low rate classes.","73e72180":"<a id='stop'><\/a>\n### 2. Remove Stopwords\nThere can be some words in our sentences that occur very frequently and don't contribute too much to the overall meaning of the sentences. We usually have a list of these words and remove them from each our sentences. For example: \"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\" in this example.","91199cb7":"> You can see above that stemming operation is NOT perfect. We have mistakes such as \"messag\", \"involv\", \"adjac\". It is a rule based method that sometimes mistakely remove suffixes from words. Nevertheless, it runs fast.","6637e62d":"### Oversampling","f3a908c8":"<a id='stem'><\/a>\n### 3. Stemming\nStemming is a rule-based system to convert words into their root form. It removes suffixes from words. This helps us enhace similarities (if any) between sentences. \n\nExample:\n> \"jumping\", \"jumped\" -> \"jump\"\n>\n> \"cars\" -> \"car\"","0b743a36":"# Sentiment Analysis\nPredict positive or negative sentiments using Naive Bayes, XGBoost and HuggingFace Transformers.","fcb2c383":"<a id='clean'><\/a>\n### 1. Clean Text\nWe will go over some simple techniques to clean and prepare text data for modeling with machine learning.","7a3a9d44":"### Apply text processing functions ","0fc49c18":"#### All Reviews","c9270a68":"Since we are interested in sentiment analysis, we will only use `reviews.text` and `reviews.rating`","b74e5585":"<a id='lemm'><\/a>\n### 4. Lemmatization\nIf we are not satisfied with the result of stemming, we can use the Lemmatization instead. It usually requires more work, but gives better results. As mentioned in the class, lemmatization needs to know the correct word position tags such as \"noun\", \"verb\", \"adjective\", etc. and we will use another NLTK function to feed this information to the lemmatizer.","bd37c4ac":"### Most used words","c5816a22":"<a id='nb'><\/a>\n### Naiave Bayes","d9efcbb8":"<a id='imp'><\/a>\n# Importing packages and loading data","da03a236":"<a id='ml'><\/a>\n# Models","d4951943":"### Splitting Dataset into Train and Test Set","f706dc62":"# Table of Content\n\n* [Intro](#intro)\n* [Importing packages and loading data](#imp)\n* [Exploratory Data Analysis (EDA)](#eda)\n* [Text Processing](#pro)\n    * [1. Clean Text](#clean)\n    * [2. Remove Stopwords](#stop)\n    * [3. Stemming](#stem)\n    * [3. Lemmatization](#lemm)\n* [Feature Engineering and Selection](#fe)\n* [Models](#ml)\n    * [Naive Bayes](#nb)\n    * [XGBoost](#xgb)\n    * [HuggingFace Transformers](#tr)","463e39ec":"#### Negative Reviews","419a236e":"<a id='eda'><\/a>\n# Exploratory Data Analysis (EDA)\n","8ae09844":"> It looks like we have a perfect accuracy of 100% ","f1968aa3":"<a id='pro'><\/a>\n## Text Processing","73392be2":"#### Positive Reviews","a322b05c":"<a id='xbg'><\/a>\n### XGBoost","044ae62c":"<a id='fe'><\/a>\n# Feature Engineering and Selection","cc0c1d60":"<a id='intro'><\/a>\n## Intro\nSentiment Analysis has been a very popular task since the dawn of Natural Language Processing (NLP). It belongs to a subtask or application of text classification, where sentiments or subjective information from different texts are extracted and identified. Today, many businesses around the world use sentiment analysis to understand more deeply their customers and clients by analyzing sentiments across different target groups. It also has wide applications in different sources of information, including product reviews, online social media, survey feedback, etc.\n\n### About dataset\nThis is a list of over 34,000 consumer reviews for Amazon products like the Kindle, Fire TV Stick, and more provided by Datafiniti's Product Database. The dataset includes basic product information, rating, review text, and more for each product.\n\nCheck [kaggle](https:\/\/www.kaggle.com\/datafiniti\/consumer-reviews-of-amazon-products) for the data ana more information","c1b6b5ff":"<a id='tr'><\/a>\n### HuggingFace Transformer\n\nTransformers library by Huggingface. We will be using pretrained transformers rather than fine-tuning our own, so a low setup cost is needed."}}