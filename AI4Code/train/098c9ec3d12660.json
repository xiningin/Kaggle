{"cell_type":{"ed374437":"code","d48d8417":"code","ac45f09b":"code","52166f06":"code","b9a813b8":"code","2cefa724":"code","f1f67d45":"code","112a91d1":"code","45ce8578":"code","eca7738e":"code","e7dec0a3":"code","f1bf4597":"code","37057f1e":"code","8c73b64a":"code","ccecdd6a":"code","37c62a8d":"code","2ceb2c90":"code","072d5086":"code","d7deca41":"code","823c30ab":"code","25e7393c":"code","95f82bbe":"code","22ea7c5b":"code","e9d696dc":"code","466d598a":"code","57c05aaf":"code","c3671dfc":"code","bc230166":"code","df56e8d7":"code","39becaf6":"code","77148e59":"code","58ae8aa7":"code","97fad2b5":"markdown","d7ee8652":"markdown","6d637869":"markdown","379c0881":"markdown","3afae3af":"markdown","a41eaf35":"markdown","2515a26e":"markdown","ba17e82c":"markdown","373c790b":"markdown","9183e71b":"markdown"},"source":{"ed374437":"# Tensforflow libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.layers import Flatten, Dense, Dropout, BatchNormalization\nfrom keras.layers import Conv1D, MaxPool1D\n\n# Data processing Libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\n\nprint(tf.__version__)","d48d8417":"data = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")","ac45f09b":"data.head()","52166f06":"data.info()","b9a813b8":"data.describe()","2cefa724":"data['Class'].value_counts()","f1f67d45":"# checking for any null value\ndata.isnull().sum()","112a91d1":"# splitting fraud and non-fraud rows\nnon_fraud_data = data[data['Class']==0]\nfraud_data = data[data['Class']==1]","45ce8578":"non_fraud_data.shape, fraud_data.shape","eca7738e":"non_fraud_data_sample = non_fraud_data.sample(fraud_data.shape[0])\n\nnon_fraud_data_sample.shape","e7dec0a3":"# balance dataset: rows 492(fraud) + 492 (non-fraud)\nbal_data = fraud_data.append(non_fraud_data_sample,ignore_index = True) \nbal_data","f1bf4597":"bal_data['Class'].value_counts()","37057f1e":"features = bal_data.drop('Class',axis=1)\nlabels = bal_data['Class']","8c73b64a":"features_train,features_test,labels_train,labels_test = train_test_split(features,labels,test_size=.25,random_state=41,stratify = labels)","ccecdd6a":"features_train.shape, features_test.shape","37c62a8d":"labels_test.value_counts()\nfeatures_train.head()","2ceb2c90":"scaler = StandardScaler()\nfeatures_train = scaler.fit_transform(features_train)\nfeatures_test = scaler.fit_transform(features_test)\n\n# converting features into dataframe\nfeatures_train = pd.DataFrame(features_train)\nfeatures_test = pd.DataFrame(features_test)","072d5086":"features_train = features_train.to_numpy()\nfeatures_test = features_test.to_numpy()","d7deca41":"features_train = features_train.reshape(features_train.shape[0],features_train.shape[1],1)\nfeatures_test = features_test.reshape(features_test.shape[0],features_test.shape[1],1)\n\nfeatures_train.shape, features_test.shape","823c30ab":"# defining the hyperparameter\nepochs = 45\nmodel = Sequential()\n\n# FIRST LAYER\nmodel.add(Conv1D(32,2,activation = 'relu',input_shape = features_train[0].shape))\nmodel.add(BatchNormalization())\n'''Batch normalization is a technique for training very deep neural networks \n   that standardizes the inputs to a layer for each mini-batch. This \n   has the effect of stabilizing the learning process and dramatically\n   reducing the number of training epochs required to train deep networks'''\nmodel.add(Dropout(0.2)) # prevents over-fitting (randomly remove some neurons)\n\n# SECOND LAYER\nmodel.add(Conv1D(64,2,activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\n# Flattening the layer ( multidimentional data into vector)\nmodel.add(Flatten())\nmodel.add(Dense(64,activation = 'relu'))\nmodel.add(Dropout(0.5))\n\n# FINAL LAYER\nmodel.add(Dense(1,activation='sigmoid')) # binary classification \n","25e7393c":"model.summary()","95f82bbe":"# optimizing the model \nmodel.compile(optimizer = Adam(lr=0.0001),loss = 'binary_crossentropy',metrics=['accuracy'])","22ea7c5b":"# Training the model, Calculating the accuracy \nhistory = model.fit(features_train, labels_train, epochs = epochs,\n                    validation_data = (features_test,labels_test),verbose = 1)","e9d696dc":"def plot_learning_curve(history,epochs):\n    \n    # plot training and validation accuracy \n    epoch_range = range(1,epochs+1)\n    plt.plot(epoch_range,history.history['accuracy'])\n    plt.plot(epoch_range,history.history['val_accuracy'])\n    plt.title('Model Accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['Train','Val'],loc='upper left')\n    plt.show()\n    \n    # plot training and validation loss\n    plt.plot(epoch_range,history.history['loss'])\n    plt.plot(epoch_range,history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['Train','Val'],loc='upper left')\n    plt.show()","466d598a":"plot_learning_curve(history,epochs)","57c05aaf":"# defining the hyperparameter\nepochs = 100\nmodel = Sequential()\n\n# FIRST LAYER\nmodel.add(Conv1D(64,2,activation = 'relu',input_shape = features_train[0].shape))\nmodel.add(BatchNormalization())\n'''Batch normalization is a technique for training very deep neural networks \n   that standardizes the inputs to a layer for each mini-batch. This \n   has the effect of stabilizing the learning process and dramatically\n   reducing the number of training epochs required to train deep networks'''\nmodel.add(MaxPool1D(2))\n'''Max pooling is done to in part to help over-fitting by providing an abstracted form of the\n   representation. As well, it reduces the computational cost by reducing the \n   number of parameters to learn and provides basic translation invariance to \n   the internal representation.'''\nmodel.add(Dropout(0.2)) # prevents over-fitting (randomly remove some neurons)\n\n# SECOND LAYER\nmodel.add(Conv1D(128,2,activation = 'relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.5))\n\n# Flattening the layer ( multidimentional data into vector)\nmodel.add(Flatten())\nmodel.add(Dense(128,activation = 'relu'))\nmodel.add(Dropout(0.5))\n\n# FINAL LAYER\nmodel.add(Dense(1,activation='sigmoid')) # binary classification ","c3671dfc":"model.summary()","bc230166":"# optimizing the model \nmodel.compile(optimizer = Adam(lr=0.0001),loss = 'binary_crossentropy',metrics=['accuracy'])\n\n# Training the model, Calculating the accuracy \nhistory = model.fit(features_train, labels_train, epochs = epochs,\n                    validation_data = (features_test,labels_test),verbose = 1)","df56e8d7":"# plotting learning curve (MAX POOL)\nplot_learning_curve(history,epochs)","39becaf6":"final_acc = model.evaluate(features_train,labels_train)\nfinal_acc","77148e59":"pred = model.predict(features_test)\npred_label = pred.argmax(axis=1)","58ae8aa7":"cm =confusion_matrix(labels_test,pred.round())\ncm","97fad2b5":"## Exploring the Dataset\n* This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions\n* features V1 - V28 are a result of the PCA transformation and are simply numerical representations.\n* Amount is the value in dollars of the transaction\n* Time variable is the amount of time that passed from the time when the first transaction took place.\n* Fraud = 1 , Not Fraud = 0","d7ee8652":"## Splitting Features and labels\n* Features = Time, v1, v2 ....\n* Label\/Target = Class","6d637869":"## Adding MaxPool\n\n#### **Pooling layers** are used to **reduce the dimensions** of the feature maps. Thus, it **reduces** the number of **parameters** to learn and the amount of **computation** performed in the network.","379c0881":"## Reshaping the database into 3D\n* We are using CNN for prediction so converting 2D dataset into 3D","3afae3af":"## Scaling the Database","a41eaf35":"# Credit Card Fraud Detection\n\n![credit card pic](https:\/\/www.xenonstack.com\/wp-content\/uploads\/xenonstack-credit-card-fraud-detection.png)\n- Credit card companies are able to **recognize fraudulent** credit card transactions so that customers are **not charged** for items that they did **not purchase**.\n- Problem Type: **Classification**\n- Library Used: **TensorFlow, Sklearn, Keras**","2515a26e":"## Build CNN (Convolutional Neural Network)","ba17e82c":"## Predicting","373c790b":"## Importing Data from CSV","9183e71b":"## Plotting a Learning Curve"}}