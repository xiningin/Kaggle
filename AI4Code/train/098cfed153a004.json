{"cell_type":{"f58e64fb":"code","ae04d49e":"code","bb8cff29":"code","1703cbff":"code","91593c8b":"code","122dcd36":"code","fe45a571":"code","55d9709c":"code","87c25052":"code","c796cbba":"code","7e992e38":"code","a7149f11":"code","f3b3568a":"code","f588a57f":"code","2aa430c7":"code","d53dccf0":"code","25dadeab":"code","0883b575":"code","66f2b7ff":"code","0284500b":"code","9c065a49":"code","fe595a11":"code","9cc3dff1":"code","6ff37494":"code","89547f86":"code","0b061327":"code","dea54cec":"code","fa2425b0":"code","19ebbc04":"code","d9ae2d65":"code","aece6e8e":"code","33a417ef":"code","71924b2e":"code","1b9382fa":"code","0cfde87d":"code","0aa081d0":"code","b58f210b":"code","5c32ad31":"code","960bbf53":"code","ba7049d7":"code","e50c4f71":"code","10cc393f":"code","2a22926f":"code","03d3f711":"code","1a753d0b":"code","a0984214":"code","9930ab4a":"code","bc330afa":"markdown","4ff75d49":"markdown","abb3bec0":"markdown","3e221447":"markdown","b1a5efe5":"markdown","527de719":"markdown","e9c9e6f8":"markdown","b930f592":"markdown","eb2d3fae":"markdown","d1ed8747":"markdown","4d68857e":"markdown","087bab73":"markdown","215b13e7":"markdown","d42bb136":"markdown","2b79011b":"markdown","5c3ac61e":"markdown","0b956cae":"markdown","2891bf92":"markdown","53604db1":"markdown","267d0b1e":"markdown","5c2e2852":"markdown"},"source":{"f58e64fb":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport itertools\nfrom datetime import date\nfrom pandas_profiling import ProfileReport\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import chi2\nfrom sklearn.ensemble import RandomForestClassifier,ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB, CategoricalNB\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\nfrom sklearn.metrics import roc_auc_score,roc_curve,auc\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ae04d49e":"pd.set_option('display.max_columns', None)","bb8cff29":"datadir = '\/kaggle\/input\/dft-accident-data\/Accidents0515.csv'","1703cbff":"accidents_df = pd.read_csv(datadir, low_memory=False)\nprint('Records:', accidents_df.shape[0], '\\nColumns:', accidents_df.shape[1])\naccidents_df.tail()","91593c8b":"accidents_df.info()","122dcd36":"# Converting Date column to isolate specific year\naccidents_df['Date'] = pd.to_datetime(accidents_df['Date'])","fe45a571":"# Filtering on 2015\naccidents_df = accidents_df[accidents_df['Date'] > '31-12-2014']","55d9709c":"accidents_df.describe()","87c25052":"print('Proportion of Missing Values in Accidents Table:', \n      round(accidents_df.isna().sum().sum()\/len(accidents_df),3), '%')","c796cbba":"print('Missing Values per column in Accidents Table:') \naccidents_df.isna().sum()","7e992e38":"accidents_df = accidents_df.drop('LSOA_of_Accident_Location',axis=1)","a7149f11":"accidents_df = accidents_df.dropna()\n\n# check if we have no NaN's anymore\naccidents_df.isna().sum().sum()","f3b3568a":"accidents_df.shape","f588a57f":"#Generate the report.\nprofile = ProfileReport(accidents_df, title='Pandas Profiling Report')","2aa430c7":"## !!!ATTENTION!!! in case you want to uncomment below line. \n## It takes long time to visualize due to high cardinality of some columns.\n\n# profile.to_notebook_iframe()","d53dccf0":"plt.rcParams['figure.figsize'] = [12, 8]\npd.options.plotting.backend = \"plotly\"","25dadeab":"sns.heatmap(accidents_df.corr())","0883b575":"accidents_df['Did_Police_Officer_Attend_Scene_of_Accident'].hist()","66f2b7ff":"# remove cases that we do not know the response variable\naccidents_df = accidents_df[accidents_df['Did_Police_Officer_Attend_Scene_of_Accident']!=-1]\n# group police officer attend response variable to binary:\naccidents_df.loc[accidents_df['Did_Police_Officer_Attend_Scene_of_Accident']==2,'Did_Police_Officer_Attend_Scene_of_Accident'] = 0\naccidents_df.loc[accidents_df['Did_Police_Officer_Attend_Scene_of_Accident']==3,'Did_Police_Officer_Attend_Scene_of_Accident'] = 0","0284500b":"sns.histplot(data=accidents_df, x=\"Did_Police_Officer_Attend_Scene_of_Accident\",stat=\"probability\", discrete=True)","9c065a49":"accidents_df['1st_Road_Number'].hist()","fe595a11":"accidents_df['2nd_Road_Number'].hist()","9cc3dff1":"accidents_df['Date']= pd.to_datetime(accidents_df['Date'], format=\"%d\/%m\/%Y\")\n\n# slice first and second string from time column\naccidents_df['Hour'] = accidents_df['Time'].str[0:2]\n\n# convert new column to numeric datetype\naccidents_df['Hour'] = pd.to_numeric(accidents_df['Hour'])\n\n# cast to integer values\naccidents_df['Hour'] = accidents_df['Hour'].astype('int')\n\n#remove extreme values and bin to helps the decision boundary\naccidents_df = accidents_df[accidents_df['1st_Road_Number'] < 10000]\naccidents_df['1st_Road_Number'] = pd.qcut(accidents_df['1st_Road_Number'], 4, labels=False,duplicates='drop')\naccidents_df.loc[accidents_df['2nd_Road_Number'] > 0,'2nd_Road_Number'] = 1","6ff37494":"XY_df = accidents_df[['Accident_Severity',\n       'Number_of_Vehicles', 'Number_of_Casualties', 'Day_of_Week','Hour',\n       '1st_Road_Class', '1st_Road_Number', 'Road_Type', 'Speed_limit',\n       'Junction_Detail', 'Junction_Control', '2nd_Road_Class',\n       '2nd_Road_Number', 'Pedestrian_Crossing-Human_Control',\n       'Pedestrian_Crossing-Physical_Facilities', 'Light_Conditions',\n       'Weather_Conditions', 'Road_Surface_Conditions',\n       'Special_Conditions_at_Site', 'Carriageway_Hazards',\n       'Urban_or_Rural_Area', 'Did_Police_Officer_Attend_Scene_of_Accident']]","89547f86":"X=XY_df.drop(['Did_Police_Officer_Attend_Scene_of_Accident'], axis=1) #drop\ny=XY_df['Did_Police_Officer_Attend_Scene_of_Accident']","0b061327":"X.info()","dea54cec":"# Transform the variables to categorical\nord_enc = OrdinalEncoder()\nX_cat = ord_enc.fit_transform(X)","fa2425b0":"chi_scores = chi2(X_cat,y)","19ebbc04":"chi_scores","d9ae2d65":"print('We do not reject the null hypothesis for the top ranked features')\np_values = pd.Series(chi_scores[1],index = X.columns)\np_values.sort_values(ascending = False , inplace = True)\np_values.plot.bar()","aece6e8e":"print('Chi2 top ranked features')\nc_values = pd.Series(chi_scores[0],index = X.columns)\nc_values.sort_values(ascending = False , inplace = True)\nc_values.plot.bar()","33a417ef":"def plot_confusion_matrix(y_test, y_pred,classes,normalize=False,title='Confusion Matrix',cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \"\"\"\n    # Compute confusion matrix\n    cm = confusion_matrix(y_test, y_pred)\n\n    np.set_printoptions(precision=2)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n   \n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    plt.grid(False)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            plt.text(j, i, format(cm[i, j], fmt),\n                horizontalalignment=\"center\",\n                color=\"white\" if cm[i, j] > thresh else \"black\")\n    \n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\ndef print_custom_report(y_test,predicted):\n    accuracy = accuracy_score(y_test, predicted)\n    print(f'Mean accuracy score: {accuracy:.3}')\n    f1 = f1_score(y_test, predicted)\n    print(f'f1 score: {f1:.3}')\n    print(classification_report(y_test, predicted))\n\ndef plot_auc_roc(md, X_test, y_test):\n    \"\"\" calculate the fpr and tpr for all thresholds of the classification and plot AUC ROC\n    \"\"\"\n    probs = md.predict_proba(X_test)\n    preds = probs[:,1]\n    fpr, tpr, threshold = roc_curve(y_test, preds)\n    roc_auc = auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')","71924b2e":"X_train, X_test, y_train, y_test=train_test_split(X_cat,y, test_size=0.20)","1b9382fa":"md = DummyClassifier(strategy='stratified')\nmd.fit(X_train, y_train)","0cfde87d":"predicted = md.predict(X_test)\nprint_custom_report(y_test,predicted)","0aa081d0":"plot_confusion_matrix(y_test, predicted, [0,1], normalize=True, title='Confusion Matrix')","b58f210b":"plot_auc_roc(md, X_test, y_test)","5c32ad31":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\nrf.fit(X_train, y_train)","960bbf53":"predicted = rf.predict(X_test)\nprint(f'Out-of-bag score estimate: {rf.oob_score_:.3}')\nprint_custom_report(y_test,predicted)","ba7049d7":"plot_confusion_matrix(y_test, predicted, [0,1], normalize=True, title='Confusion Matrix')","e50c4f71":"plot_auc_roc(rf, X_test, y_test)","10cc393f":"importances = rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Plot the impurity-based feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.barh(range(X.shape[1]), importances[indices],\n        color=\"r\", xerr=std[indices], align=\"center\")\nnam_l = [X.columns[x] for x in indices]\nplt.yticks(range(X.shape[1]),nam_l)\nplt.ylim([-1, X.shape[1]])\nplt.show()","2a22926f":"chi_select = SelectKBest(score_func=chi2, k=15)\nX_train_new = chi_select.fit_transform(X_train, y_train)\nX_test_new = chi_select.transform(X_test)","03d3f711":"rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)\nrf.fit(X_train_new, y_train)","1a753d0b":"predicted = rf.predict(X_test_new)\nprint(f'Out-of-bag score estimate: {rf.oob_score_:.3}')\nprint_custom_report(y_test,predicted)","a0984214":"plot_confusion_matrix(y_test, predicted, [0,1], normalize=True, title='Confusion Matrix')","9930ab4a":"#TODO\n# There is an adjustment file..\n# Add casualties and vehicles files\n# Add more years\n# Add the descriptions of the variables\n# Add time-date features (rush hours, weekends, holidays, months...) and perfrom time-series analysis\n# Add geospatial data police deparment\/patrol features\n# Check more thoroghly the impact of  missing values in the dataset (-1)\n# RF -> GBM -> (SVM?) -> NN\n# imbalanced dataset, under -oversample?\n# Grid search, Stratified - Cross validation","bc330afa":"#### Remarks\n- We drop 'Police_Force','Time', 'Local_Authority_(District)', 'Local_Authority_(Highway)','Date' due to high cardinality.\n- We will consider lat, lon in a future model.\n- There is some extreme values and some incosistencies we can fix.\n- Some Feature engineering will be performed.","4ff75d49":"The classification problem is imbalanced, so we will take care of that by using statified strategies to split our training\/test dataset.\nWe will investigate the performance of our model using precision, recall and AUC.\nFor now we create a simple baseline of a random-dummy model and start iteratively improving.","abb3bec0":"#### Analysis\nThere is some common ground in the random forest and chi2 feature importances. However, we also see some contradicting results for the features Hour, Day of Week. It could be due to the high cardinality of the features. More investigation is required to get a definite answer.","3e221447":"## Importing and analysing the data","b1a5efe5":"#### Analysis\nWe are already way above the baseline of a dummy model, with just a fist pass of Random Forest model. However, many steps are still needed to ensure consistensy.\nFurther more we are scoring low on the recall on non-attendance of the model. We produce a lot of False Positives.","527de719":"The goal of this report is to build a model that predicts if a police officer is likely to attend an accident or not using the accidents data provided in: https:\/\/data.gov.uk\/dataset\/cb7ae6f0-4be6-4935-9277-47e5ce24a11f\/road-safety-data","e9c9e6f8":"#### Analysis\n\nWe are winning in precision but losing in recall by using the best feature from the chi2 test.\nDefinately more research is required ","b930f592":"### Create a profile report","eb2d3fae":"We used the accidents data for the past year 2015","d1ed8747":"#### The Approach (Algorithms and Tools)\n\nWe started by looking at the data and ran minor exploratory data analysis to understand the features and their relationships with each other and with the target. \n\nWe created a baseline and then tested a more powerful model, i.e. Random Forest. The choice was based on the following factors:\n- Random Forest can handle mixed type of features.\n- On the top-performing algorithms that avoid overfitting.\n- Few parameters to tune to get good results.\n- Can handle non-linearity.\n- Faster than GBM, NNs.\nThen we moved to some optimization tests. Definately, more work is required.\n\nTools: Numpy and Pandas for data analysis, Scikit-Learn for machine learning, and Seaborn and Matplotlib for data visualizations.\n\n#### What were the main challenges?\n\nThe challenges can be summarized as follows:\n- Due to time constrains there was not enough time to properly analyse and understand the data.\n- What would be the best model and features, some results need further exploration.\n- The lack of business knowledge.\n- The selection of the model was based mostly on past experience and simplification of the problem.\n- Our model produces a lot of False Positives, further optimization\/improvement needed.\n\n\n#### What insight did you gain from working with the data?\n\nThe police attendance is an imbalanced classification problem and there is definately predictive power within the available dataset to answer the question.\nHowever, the most valuable insights arise from the features that affect\/correlate with the police attendance. Namely, weather condition and speed limit of the road.\n\n####  How useful is the model?\n\nThe Random Forest provides much better than random results, however the recall on the negative attendance is very low. Further feature\/model exploration and optimization is required.\nFrom a Business perspective, in the next step the most useful model would maybe be to predict attendance per police department\/authority.\n\n####  What might you do differently if you had more time\/resource?\n\nFirstly, I first analyse geospatial and other available data and then perform time series analysis to gain insight on the behaviour of the different police departments\/authorities over the years.\nIn addition, a lot of available model and optimization methods were not tested due to lack of time.\nFinaly, I would add extra data sources available to enrich the features of the model and gain further insight (casualties and vehicles data).\n\nMany actions were left out from this first-pass exercise:\n- Geospatial data were not considered for simplicity reasons.\n- Same for trends and many time series features.\n- A possible model testing path would have been : RF -> GBM -> (SVM?) -> NN.\n- Over\/Under Sampling methods.\n- Grid parameter search \n- Cross Validation Stratified\n- More data\/features\/sources...","4d68857e":"### Dummy Model","087bab73":"### Feature selection with chi2","215b13e7":"### Random Forest","d42bb136":"#### Feature engineering, Removing extreme values and preparing for classification","2b79011b":"### Detecting (Un)-Useful Features","5c3ac61e":"# UK Police Officer Attendance Prediction","0b956cae":"#### Remark\nWe will not use the LSOA_of_Accident_Location feature, as it contains a lot of na values","2891bf92":"- We used a recent subset of the accident dataset of the UK (2015).\n- Only the accident files were considered and not the casualties and vehicles extra info\/files.\n- The model did not consider trend and time-series analysis features.\n- The model disregards for simplicity reasons the lat,lon of the accidents.","53604db1":"### First pass model","267d0b1e":"We remove columns :\n'Accident_Index', 'Location_Easting_OSGR', 'Location_Northing_OSGR', 'Longitude', 'Latitude', 'Police_Force',\n 'LSOA_of_Accident_Location','Time', 'Local_Authority_(District)', 'Local_Authority_(Highway)','Date'","5c2e2852":"### General Remarks"}}