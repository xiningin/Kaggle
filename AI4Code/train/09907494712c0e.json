{"cell_type":{"7dfef63b":"code","8fd4e1ad":"code","d7cc5ac3":"code","94eb2f68":"code","ce5d1068":"markdown","3374a90a":"markdown","4b6a370e":"markdown"},"source":{"7dfef63b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8fd4e1ad":"import numpy as np # data analysis\nimport pandas as pd # data analysis\nimport matplotlib.pyplot as plt # data vizualization\nimport seaborn as sns # data visualization\nfrom sklearn.model_selection import train_test_split # machine learning\nfrom sklearn.ensemble import RandomForestClassifier # machine learning\nfrom sklearn.cluster import KMeans\n\nexample_clinical_data_path_1 = '\/kaggle\/input\/end-als\/end-als\/clinical-data\/filtered-metadata\/metadata\/clinical\/Demographics.csv'\nexample_clinical_data_path_2 = '\/kaggle\/input\/end-als\/end-als\/clinical-data\/filtered-metadata\/metadata\/clinical\/ALSFRS_R.csv'\nexample_transcriptomics_DESEQ2_data_path_1 = '\/kaggle\/input\/end-als\/end-als\/transcriptomics-data\/DESeq2\/bulbar_vs_limb.csv'\nexample_transcriptomics_DESEQ2_data_path_2 = '\/kaggle\/input\/end-als\/end-als\/transcriptomics-data\/DESeq2\/ctrl_vs_case.csv'\nexample_transcriptomics_3counts_data_path = '\/kaggle\/input\/end-als\/end-als\/transcriptomics-data\/L3_counts\/CASE-NEUZX521TKK\/CASE-NEUZX521TKK-5793-T\/CASE-NEUZX521TKK-5793-T_P85.exon.txt'\n\ndemographics = pd.read_csv(example_clinical_data_path_1)\ndemographics.to_csv('\/kaggle\/working\/demographics.csv')\nalsfrs_scores = pd.read_csv(example_clinical_data_path_2)\nalsfrs_scores.to_csv('\/kaggle\/working\/alsfrs_scores.csv')\nbulbar_vs_limb = pd.read_csv(example_transcriptomics_DESEQ2_data_path_1)\nbulbar_vs_limb.to_csv('\/kaggle\/working\/bulbar_vs_limb.csv')\nctrl_vs_case = pd.read_csv(example_transcriptomics_DESEQ2_data_path_2)\nctrl_vs_case.to_csv('\/kaggle\/working\/ctrl_vs_case.csv')\nexample_transcriptomics_3counts_data = pd.read_csv(example_transcriptomics_3counts_data_path,delim_whitespace=True,skiprows=1,low_memory=False)\nexample_transcriptomics_3counts_data.to_csv('\/kaggle\/working\/L3_counts.csv')","d7cc5ac3":"def sort_feature_importances(df, visualize = False):\n    '''\n    Adapted from https:\/\/github.com\/WillKoehrsen\/feature-selector\n    '''\n    #Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    #Normalise the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    #Make a horizontal bar chart of feature importances\n    \n    if(visualize):\n        plt.figure(figsize = (10,6))\n        ax = plt.subplot()\n        #Need to reverse the index to plot most important on top\n        ax.barh(list(reversed(list(df.index[:15]))),\n               df['importance_normalized'].head(15),\n               align = 'center', edgecolor = 'k')\n        #Set the yticks and labels\n        ax.set_yticks(list(reversed(list(df.index[:15]))))\n        ax.set_yticklabels(df['feature'].head(15))\n        #Plot labeling\n        plt.xlabel('Normalized Importance'); plt.title('Feature Importance')\n        plt.show()\n    \n    return df\n\ndef important_clusters(XClusterLabels, Y, numClusters, threshold = 0.8, labelOfInterest = 1):\n    \"\"\" Check which clusters express a given class label in a ratio greater than a threshold\n    \n    Arguments:\n        XClusterLabels: ndarray of shape (n_samples,) cluster predictions for the training data\n        Y: ndarray of shape (n_samples,), training labels\n        numCluster: an integer representing the number of clusters\n        threshold: a float representing the ratio threshold for a cluster to be significant, defaults to 0.8\n        labelOfInterest: an integer representing the class label of interest\n    \n    Returns:\n        An ndarray containing 0 (not exceeding the threshold) or 1 (exceeding the threshold) for each cluster,\n        and an ndarray containing the ratio for each cluster\n    \"\"\"\n    meaningfulList = np.zeros((numClusters))\n    ratioList = np.zeros((numClusters))\n    \n    for i in np.arange(numClusters):\n        YClusterLabels = Y[XClusterLabels == i]\n        ratio = YClusterLabels[YClusterLabels == labelOfInterest].shape[0] \/ YClusterLabels.shape[0]\n        if ratio >= threshold:\n            meaningfulList[i] = 1\n        ratioList[i] = ratio\n    return meaningfulList, ratioList\n\ndef extract_important_features(X, XClusterLabels, clusterOfInterest, numFeatures=5000, visualize=False):\n    \"\"\" Find which features are important in a random forest classifier with two classes: \n    being in the cluster of interest, and not being in it.\n\n    Arguments:\n        X: Pandas DataFrame containing the training data\n        XClusterLabels: ndarray of shape (n_samples,) cluster predictions for the training data\n        clusterOfInterest: an integer representing the cluster of interest\n        numFeatures: an integer representing the number of important features to return, defaults to 5000\n        visualize: a boolean representing whether to visualize the important features, defaults to False\n\n    Returns:\n        A Pandas DataFrame containing the top numFeatures most important features\n    \"\"\"\n    \n    clf = RandomForestClassifier()\n    newClusterLabels = np.zeros(XClusterLabels.shape)\n    newClusterLabels[XClusterLabels == clusterOfInterest] = 1\n    clf.fit(X, newClusterLabels)\n\n    feature_importance_values = clf.feature_importances_\n    features = list(X.columns)\n    feature_importances = pd.DataFrame({'feature': features, 'importance':feature_importance_values})\n    return sort_feature_importances(feature_importances, visualize)[:numFeatures]","94eb2f68":"training_dataTest = bulbar_vs_limb.drop(['SiteOnset_Class','Participant_ID'],axis=1)\nlabelsTest = bulbar_vs_limb['SiteOnset_Class']\nX_train, X_test, y_train, y_test = train_test_split(training_dataTest, labelsTest, train_size=0.9)\n\nnumClusters=10\nkmeans = KMeans(n_clusters=numClusters).fit(X_train)\nXclusterLabels = kmeans.predict(X_train)\nmeaningList, ratioList = important_clusters(XclusterLabels, y_train, numClusters, threshold = 0.9)\n\nfor i in np.arange(numClusters):\n    if meaningList[i] == 1:\n        test = extract_important_features(X_train, XclusterLabels, i, visualize=True)\n        print(test[:10])","ce5d1068":"# Test Run","3374a90a":"# Functions","4b6a370e":"# Import Packages and Data"}}