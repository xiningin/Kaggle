{"cell_type":{"efec398f":"code","9852675b":"code","749c29ba":"code","ec13578c":"code","32b851d3":"code","db96e16f":"code","a11f79d0":"code","919168f4":"code","7ef193d7":"code","f7725db7":"code","cac97d33":"code","08047d18":"code","898cfea6":"code","e9db492d":"code","8773ff83":"code","6a36dc05":"code","3d9a8618":"code","f3e357ff":"code","316ce38e":"code","9dfc6ae9":"code","394c18f7":"code","6e8a889d":"code","5a7aa62c":"code","f8af3963":"markdown","b16b2d47":"markdown","853449cb":"markdown","cc198aa0":"markdown","52fb4145":"markdown","03addf19":"markdown","eefff009":"markdown","2245497d":"markdown","4c428112":"markdown","9b092518":"markdown","fc106a60":"markdown"},"source":{"efec398f":"# Familiar imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn import preprocessing\n# For ordinal encoding categorical variables, splitting data\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\n\n\nfrom category_encoders import MEstimateEncoder\nfrom sklearn.model_selection import train_test_split,KFold\n\n#LightGBM model\nfrom lightgbm import LGBMRegressor\n\n#optimizer\nfrom functools import partial\nimport optuna\n# For training random forest model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\n\n# Set Matplotlib defaults (from Excercise: Mutual information)\nplt.style.use(\"seaborn-whitegrid\")\nplt.rc(\"figure\", autolayout=True)\nplt.rc(\n    \"axes\",\n    labelweight=\"bold\",\n    labelsize=\"large\",\n    titleweight=\"bold\",\n    titlesize=14,\n    titlepad=10,\n)\n\n# Utility functions from Tutorial (from Excercise: Mutual information in Feature Engineering)\ndef make_mi_scores(X, y):\n    X = X.copy()\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    # All discrete features should now have integer dtypes\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=0)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\n\ndef plot_mi_scores(scores):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    plt.barh(width, scores)\n    plt.yticks(width, ticks)\n    plt.title(\"Mutual Information Scores\")\n    \n# for Box-Cox Transformation\nfrom scipy import stats\n\n# for min_max scaling\nfrom mlxtend.preprocessing import minmax_scaling\nfrom scipy import stats","9852675b":"# Load the training data\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\n\n# Preview the data\ntrain.head()\n","749c29ba":"features_1 = ['cont0','cont1','cont2','cont3','cont4','cont5','cont6','cont7','cont8','cont9','cont10','cont11','cont12','cont13']\nsns.relplot(x='value', y ='target', col = 'variable', data = train.melt(id_vars = 'target',value_vars=features_1),facet_kws =dict(sharex = False))\nsns.set(rc={'figure.figsize':(50,50)}) ","ec13578c":"# Separate target from features\ny = train['target']\nfeatures = train.drop(['target'], axis=1)\n\n# Preview features\ntrain.shape","32b851d3":"features","db96e16f":"mi_scores = make_mi_scores(features,y)","a11f79d0":"print(mi_scores.head(24))\n\nplt.figure(dpi=100, figsize=(8,5))\nplot_mi_scores(mi_scores)","919168f4":"#Drop cat 2 since it has 0 mutual infomration. \nfeatures = features.drop(['cat2'], axis=1)\n","7ef193d7":"# List of categorical columns\nobject_cols = [col for col in features.columns if 'cat' in col]\n\n# Get number of unique entries in each column with categorical data\nobject_nunique = list(map(lambda col: features[col].nunique(), object_cols))\nd = dict(zip(object_cols, object_nunique))\n\n# Print number of unique entries by column, in ascending order\nsorted(d.items(), key=lambda x: x[1])","f7725db7":"features['cat9'].value_counts()","cac97d33":"## from https:\/\/www.kaggle.com\/ryanholbrook\/feature-engineering-for-house-prices\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=5)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) \/ len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","08047d18":"encoder = CrossFoldEncoder(MEstimateEncoder, m =1)\nfeatures['cat9'] = encoder.fit_transform(features,y, cols = [\"cat9\"])\ntest['cat9'] = encoder.transform(test.drop(['cat2'], axis=1))","898cfea6":"features= features.rename(columns={'cat9':'cont14'})\ntest =test.rename(columns={'cat9':'cont14'})\n\nobject_cols = [col for col in features.columns if 'cat' in col]","e9db492d":"#mix-max scale the data\nfeatures.cont14 = minmax_scaling(features.cont14, columns=[0])\ntest.cont14 = minmax_scaling(test.cont14, columns =[0])","8773ff83":"test","6a36dc05":"low_cardinality_cols = [col for col in object_cols if features[col].nunique() <10]\nhigh_cardinality_cols = list(set(object_cols) -set(low_cardinality_cols))","3d9a8618":"\n# One Hot Encoding for categorical columns\nX = features.copy()\nX_test = test.copy().drop(['cat2'], axis=1)   \nOH_encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X[low_cardinality_cols]))\nOH_cols_test = pd.DataFrame(OH_encoder.transform(X_test[low_cardinality_cols]))\n\nOH_cols_train.index = X.index\nOH_cols_test.index = X_test.index\n\nnum_X_train= X.drop(object_cols,axis=1)\nnum_X_test= X_test.drop(object_cols,axis=1)\n\n\nOH_X_train = pd.concat([num_X_train,OH_cols_train],axis=1)\nOH_X_test = pd.concat([num_X_test,OH_cols_test],axis=1)\n\n\n","f3e357ff":"features","316ce38e":"X_train, X_valid, y_train, y_valid = train_test_split(OH_X_train, y, random_state=0)","9dfc6ae9":"def objective(trial,X,y, name='xgb'):\n    params = {'max_depth':trial.suggest_int('max_depth', 5, 50),\n              'n_estimators':trial.suggest_int('n_estimators',30,5000),\n              'boosting':trial.suggest_categorical('boosting', ['gbdt']),\n              'subsample': trial.suggest_uniform('subsample', 0.2, 1.0),\n              'colsample_bytree':trial.suggest_uniform('colsample_bytree', 0.2, 1.0),\n              'learning_rate':trial.suggest_uniform('learning_rate', 0.007, 0.02),\n              'reg_lambda':trial.suggest_uniform('reg_lambda', 0.01, 50),\n              'reg_alpha':trial.suggest_uniform('reg_alpha', 0.01, 50),\n              'min_child_samples':trial.suggest_int('min_child_samples', 5, 100),\n              'num_leaves':trial.suggest_int('num_leaves', 10, 200),\n              'n_jobs' : -1,\n              'metric':'rmse',\n              'max_bin':trial.suggest_int('max_bin', 300, 1000),\n              'cat_smooth':trial.suggest_int('cat_smooth',5,100)\n              }\n\n    model = LGBMRegressor(**params)\n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=0)\n    \n    model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)],\n              eval_metric=['rmse'],\n              early_stopping_rounds=250, \n              callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n              verbose=0)\n\n    train_score = np.round(np.sqrt(mean_squared_error(y_train, model.predict(X_train))), 5)\n    test_score = np.round(np.sqrt(mean_squared_error(y_valid, model.predict(X_valid))), 5)\n                  \n    print(f'TRAIN RMSE : {train_score} || TEST RMSE : {test_score}')\n                  \n    return test_score\n\n","394c18f7":"optimize = partial(objective,X=X_train,y=y_train)\n\n#study_lgbm = optuna.create_study(direction ='minimize')\n#study_lgbm.optimize(optimize,n_trials=300)\n\n\n\n#1 finished with value: 0.71595 and parameters: \n#{'max_depth': 36, 'n_estimators': 2093, 'boosting': 'gbdt',\n#'subsample': 0.6685349373084133, 'colsample_bytree': 0.2716120120086581,\n#'learning_rate': 0.018254266760531337, 'reg_lambda': 13.53563268633886,\n#'reg_alpha': 38.8715816855484, 'min_child_samples': 10, 'num_leaves': 80,\n#'max_bin': 882, 'cat_smooth': 82}. Best is trial 1 with value: 0.71595.","6e8a889d":"paras = {'max_depth': 36, 'n_estimators': 2093, 'boosting': 'gbdt',\n'subsample': 0.6685349373084133, 'colsample_bytree': 0.2716120120086581,\n'learning_rate': 0.018254266760531337, 'reg_lambda': 13.53563268633886,\n'reg_alpha': 38.8715816855484, 'min_child_samples': 10, 'num_leaves': 80,\n'max_bin': 882, 'cat_smooth': 82}\n\nModel = LGBMRegressor(**paras).fit(X_train,y_train, eval_set=[(X_valid,y_valid)],\neval_metric =['rmse'],\nearly_stopping_rounds=5,\nverbose=False)\npreds_valid = Model.predict(X_valid)\nprint(mean_squared_error(y_valid,preds_valid,squared=False))\n\n\n#split=KFold(n_splits=5)\n\n#preds_list_base = []\n#preds_list_final_iteration = []\n#preds_list_all = []\n\n#for train_idx, val_idx in split.split(X_train):\n#            X_tr = X_train.iloc[train_idx]\n#            X_val = X_train.iloc[val_idx]\n#            y_tr = y_train.iloc[train_idx]\n#            y_val = y_train.iloc[val_idx]\n            \n#            Model = LGBMRegressor(**paras).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n#                          eval_metric=['rmse'],\n#                          early_stopping_rounds=5, \n#                          #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n#                          verbose=0)\n            \n#            preds_list_base.append(Model.predict(OH_X_test))\n#            preds_list_all.append(Model.predict(OH_X_test))\n#            print(f'RMSE for Base model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n#            first_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n#            params = paras.copy()\n            \n#            for i in range(1, 8):\n#                if i >2:    \n                    \n#                    # reducing regularizing params if \n                    \n#                    params['reg_lambda'] *= 0.9\n#                    params['reg_alpha'] *= 0.9\n#                    params['num_leaves'] += 40\n                    \n#                params['learning_rate'] = 0.003\n#                Model = LGBMRegressor(**params).fit(X_tr, y_tr, eval_set=[(X_val, y_val)],\n#                          eval_metric=['rmse'],\n#                          early_stopping_rounds=200, \n#                          #callbacks=[optuna.integration.LightGBMPruningCallback(trial, metric='rmse')],\n#                          verbose=0,\n#                          init_model=Model)\n                \n#                preds_list_all.append(Model.predict(OH_X_test))\n#                print(f'RMSE for Incremental trial {i} model is {np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))}')\n#            last_rmse = np.sqrt(mean_squared_error(y_val, Model.predict(X_val)))\n#            print('',end='\\n\\n')\n#            print(f'Improvement of : {first_rmse - last_rmse}')\n#            print('-' * 100)\n#            preds_list_final_iteration.append(Model.predict(OH_X_test))\n\n\n\n#0.7207048528321405","5a7aa62c":"# Use the model to generate predictions\npredictions = Model.predict(OH_X_test)\n\n# Save the predictions to a CSV file\noutput = pd.DataFrame({'Id': X_test.index,\n                       'target': predictions})\noutput.to_csv('submission.csv', index=False)","f8af3963":"# Step 1: Import helpful libraries\n\nWe begin by importing the libraries we'll need.  Some of them will be familiar from the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** course and the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course.","b16b2d47":"# 4. Define a model with best parameters","853449cb":"# Step 2.1 Explore Data","cc198aa0":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","52fb4145":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","03addf19":"# Step 3 Process Categorical columns","eefff009":"Next, we break off a validation set from the training data.","2245497d":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`).","4c428112":"Once you have run the code cell above, follow the instructions below to submit to the competition:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.","9b092518":"# Step 4.0 Find best parameters with optimization methods\n\nReferred to the notebook (https:\/\/www.kaggle.com\/awwalmalhi\/extreme-fine-tuning-lgbm-using-7-step-training)\nto find out best parameters.","fc106a60":"# Step 6: Keep Learning!\n\nIf you're not sure what to do next, you can begin by trying out more model types!\n1. If you took the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course, then you learned about **[XGBoost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n\n2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https:\/\/www.kaggle.com\/svyatoslavsokolov\/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset."}}