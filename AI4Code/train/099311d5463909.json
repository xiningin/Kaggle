{"cell_type":{"c3f0349c":"code","08238121":"code","c32a8473":"code","395c03b7":"code","a0d493f8":"code","c60cc384":"code","4a2b8a26":"code","b28e7ff4":"code","62c9b222":"code","d92eba7f":"code","a501b1fe":"code","e6452786":"code","17c5d360":"code","bd839e6c":"code","344c73be":"code","aa5d5877":"code","b1fee952":"code","1074a3a6":"code","a7b04e76":"code","34463bb7":"code","69e8ed51":"code","574633a2":"code","15eb2a0e":"code","b458cc8a":"code","9ce313ae":"code","ad26651b":"code","e46b59b8":"code","7b93713f":"code","fd239532":"code","b76e81e4":"code","b08b612d":"code","217f5bcc":"code","6d0a856b":"code","9865a774":"code","ad1e9265":"code","d113c7ac":"code","7fd241b8":"code","03b0dd82":"code","20238c61":"code","5bc8d1f6":"code","54780fc3":"markdown","e18e6f93":"markdown"},"source":{"c3f0349c":"# !pip install pyparsing==2.4.0\n!pip install tensorflow==2.6.0\n!pip install keras==2.6.0\nimport os\n# os.chdir('\/kaggle\/working\/w')\n# import shutil\n# shutil.rmtree(\"\/kaggle\/working\/workspace\")\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nfrom PIL import Image, ImageDraw\nimport tensorflow as tf\n\n\nimport ast\nimport sys\nimport time\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport greatbarrierreef\n","08238121":"# data imports\nDATA_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef'\nimages_path = os.path.join(DATA_PATH,'train_images')\ndf_test = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/test.csv\")\ndf_train = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/example_sample_submission.csv\")\nexample = np.load(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/example_test.npy\")","c32a8473":"df_train['img_path'] = os.path.join('..\/input\/tensorflow-great-barrier-reef\/train_images')+\"\/video_\"+df_train.video_id.astype(str)+\"\/\"+df_train.video_frame.astype(str)+\".jpg\"\ndf_train['annotations'] = df_train['annotations'].apply(lambda x: ast.literal_eval(x))\ndf_train['Number_bbox'] = df_train['annotations'].apply(lambda x:len(x)) ","395c03b7":"def bbox_areas(annotations):\n    if not annotations:\n        return [0]\n    area_list = []\n    for annotation in annotations:\n        area_list.append(annotation['width']*annotation['height'])\n    return area_list\ndf_train[\"bbox_area\"] = df_train[\"annotations\"].apply(bbox_areas)\ndf_train[\"max_area\"] = df_train[\"bbox_area\"].apply(lambda x : max(x))\ndf_train[\"min_area\"] = df_train[\"bbox_area\"].apply(lambda x : min(x))\n\n","a0d493f8":"def img_viz(df_train, id):\n    image = df_train['img_path'][id]\n    img = Image.open(image)\n    \n    for box in df_train['annotations'][id]:\n        shape = [box['x'], box['y'], box['x']+box['width'], box['y']+box['height']]\n        ImageDraw.Draw(img).rectangle(shape, outline =\"red\", width=3)\n    display(img)\ndf_train.sort_values(\"max_area\", ascending=False).head()\n","c60cc384":"# data imports\nDATA_PATH = '\/kaggle\/input\/tensorflow-great-barrier-reef'\nimages_path = os.path.join(DATA_PATH,'train_images')\ndf_test = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/test.csv\")\ndf_train = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/example_sample_submission.csv\")\nexample = np.load(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/example_test.npy\")\ndata = pd.read_csv(\"\/kaggle\/input\/datagreatbarrier\/data.csv\") #processed df","4a2b8a26":"data","b28e7ff4":"actual_train_data = data.query(\"Number_bbox>0\")","62c9b222":"!git clone https:\/\/github.com\/tensorflow\/models.git\n# !cd models\/research\n# !export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`\/slim\n# !protoc object_detection\/protos\/*.proto --python_out=.","d92eba7f":"!wget -O protobuf.zip https:\/\/github.com\/protocolbuffers\/protobuf\/releases\/download\/v3.19.0\/protoc-3.19.0-linux-x86_64.zip -q\n!unzip -o protobuf.zip\n!rm protobuf.zip","a501b1fe":"!pwd\n!cd models\/research\n!pwd","e6452786":"# %bash cd models\/research\nos.chdir('models\/research')\n# !pwd\n!protoc object_detection\/protos\/*.proto --python_out=.","17c5d360":"import os\n\nos.environ['AUTOGRAPH_VERBOSITY'] = '0'\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nos.environ['PYTHONPATH']=os.environ['PYTHONPATH']+':\/kaggle\/models\/research\/slim:\/kaggle\/models\/research'\nos.environ['PYTHONPATH']\n\n","bd839e6c":"!cp object_detection\/packages\/tf2\/setup.py .\n!python -m pip install --use-feature=2020-resolver .","344c73be":"!pwd\n!python object_detection\/builders\/model_builder_tf2_test.py","aa5d5877":"data['annotations'] = data['annotations'].apply(eval)\n","b1fee952":"os.chdir(\"\/kaggle\/working\")\n!mkdir workspace workspace\/train_images workspace\/test_images","1074a3a6":"! mkdir \/kaggle\/working\/workspace\/annotations\nlabel_map = \"\"\"item {\n    id: 1\n    name: 'starfish'\n}\"\"\"\nwith open(\"\/kaggle\/working\/workspace\/annotations\/label_map.pbtxt\", \"w\") as label_file:\n    label_file.write(label_map)\nlabel_file.close()\n    ","a7b04e76":"# !cp \/kaggle\/input\/reef-labels\/labelmap.pbtxt \/kaggle\/working\/workspace\/annotations","34463bb7":"from tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom PIL import ImageDraw\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport json\nimport copy\nimport os\nimport cv2\nimport ast\n\n# functions\n","69e8ed51":"image_annotation_dict = dict(zip(data.img_path, data.annotations))\nannotated_images = {key:value for key, value in image_annotation_dict.items() if len(value)>0}","574633a2":"def get_xml_template(image_path, folder=\"\/kaggle\/working\/workspace\/train_images\"):\n    img = Image.open(image_path)\n    img_width, img_height = img.size\n    file_name = '_'.join(image_path.split('\/')[-2:])\n    image_info_dict = {\n    \"folder\": folder.split('\/')[-1],\n    \"filename\": file_name,\n    \"path\": folder + '\/' + file_name,\n    \"source\":{\"database\": \"Unknown\"},\n    \"size\":{\"width\": str(img_width), \"height\": str(img_height), \"depth\":\"3\"},\n    \"segmented\": \"0\",\n        }\n    return image_info_dict\n\ndef get_annotation_template(annotation):\n    annotation_dict = {\"name\": \"starfish\",\n                      \"pose\": \"unknown\",\n                      \"difficult\": \"0\",\n                      \"bndbox\":{\"xmin\": str(annotation['x']),\n                              \"xmax\": str(annotation['x']+annotation['width']),\n                              \"ymin\": str(annotation['y']),\n                               \"ymax\": str(annotation['y']+annotation['height'])}}\n    return annotation_dict","15eb2a0e":"import xml.etree.ElementTree as ET\n\ndef add_child(key, value, parent):\n    child = ET.SubElement(parent, key)\n    if isinstance(value, dict):\n        for key_child, value_child in list(value.items()):\n            add_child(key_child, value_child, child)\n    else:\n        child.text = value\n    return  \n\ndef get_xml(image_data,folder=\"\/kaggle\/working\/workspace\/train_images\"):\n    image_info = get_xml_template(image_data[0], folder=folder)\n    file_path = folder+'\/'+image_info[\"filename\"]\n    xml_file_path = file_path.split('.')[0] +'.xml'\n    annotation_list = image_data[1]\n    root = ET.Element(\"annotations\")\n    for k,v in image_info.items():\n        add_child(k,v,root)\n    for annotation in annotation_list:\n        annotation_root = ET.SubElement(root, \"object\")\n        annotation_info = get_annotation_template(annotation)\n        for k,v in annotation_info.items():\n            add_child(k,v,annotation_root)\n    return root,file_path,xml_file_path\n\n","b458cc8a":"data.columns","9ce313ae":"actual_train_data = data[data['Number_bbox']>0]","ad26651b":"from sklearn.model_selection import train_test_split\nimport shutil\ntrain_data, test_data = train_test_split(actual_train_data,random_state=22, test_size=0.15)","e46b59b8":"def transfer_images_xml(image_data_list, folder=\"\/kaggle\/working\/workspace\/train_images\"):\n    for image_data in tqdm(image_data_list):\n        xml_details, image_path, xml_path = get_xml(image_data, folder=folder)\n        shutil.copy(image_data[0], image_path)\n        xml_tree = ET.ElementTree(xml_details)\n        xml_tree.write(xml_path)","7b93713f":"test_image_annotation_dict = tuple(zip(test_data.img_path, test_data.annotations))\n","fd239532":"transfer_images_xml(test_image_annotation_dict, folder=\"\/kaggle\/working\/workspace\/test_images\")","b76e81e4":"# !rm \/kaggle\/working\/workspace\/test_images\/*\n# !rm \/kaggle\/working\/workspace\/train_images\/*\n\n","b08b612d":"!mkdir \/kaggle\/working\/workspace\/scripts\n!wget https:\/\/tensorflow-object-detection-api-tutorial.readthedocs.io\/en\/latest\/_downloads\/da4babe668a8afb093cc7776d7e630f3\/generate_tfrecord.py -P \/kaggle\/working\/workspace\/scripts","217f5bcc":"!python \/kaggle\/working\/workspace\/scripts\/generate_tfrecord.py -x \/kaggle\/working\/workspace\/test_images -l \/kaggle\/working\/workspace\/annotations\/label_map.pbtxt -o \/kaggle\/working\/workspace\/annotations\/test.record","6d0a856b":"train_image_annotation_dict = tuple(zip(train_data.img_path, train_data.annotations))\ntransfer_images_xml(train_image_annotation_dict, folder=\"\/kaggle\/working\/workspace\/train_images\")\n","9865a774":"!python \/kaggle\/working\/workspace\/scripts\/generate_tfrecord.py -x \/kaggle\/working\/workspace\/train_images -l \/kaggle\/working\/workspace\/annotations\/label_map.pbtxt -o \/kaggle\/working\/workspace\/annotations\/train.record","ad1e9265":"!wget download.tensorflow.org\/models\/object_detection\/tf2\/20200711\/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz -P \/kaggle\/working\/workspace\/pre-trained-models\/\n!tar -xvzf \/kaggle\/working\/workspace\/pre-trained-models\/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8.tar.gz\n","d113c7ac":"config = \"\"\"# Faster R-CNN with Resnet-50 (v1)\n# Trained on COCO, initialized from Imagenet classification checkpoint\n\n# Achieves -- mAP on COCO14 minival dataset.\n\n# This config is TPU compatible.\n\nmodel {\n  faster_rcnn {\n    num_classes: 1\n    image_resizer {\n      keep_aspect_ratio_resizer {\n        min_dimension: 640\n        max_dimension: 640\n        pad_to_max_dimension: true\n      }\n    }\n    feature_extractor {\n      type: 'faster_rcnn_resnet50_keras'\n      batch_norm_trainable: true\n    }\n    first_stage_anchor_generator {\n      grid_anchor_generator {\n        scales: [0.25, 0.5, 1.0, 2.0]\n        aspect_ratios: [0.5, 1.0, 2.0]\n        height_stride: 16\n        width_stride: 16\n      }\n    }\n    first_stage_box_predictor_conv_hyperparams {\n      op: CONV\n      regularizer {\n        l2_regularizer {\n          weight: 0.0\n        }\n      }\n      initializer {\n        truncated_normal_initializer {\n          stddev: 0.01\n        }\n      }\n    }\n    first_stage_nms_score_threshold: 0.0\n    first_stage_nms_iou_threshold: 0.7\n    first_stage_max_proposals: 300\n    first_stage_localization_loss_weight: 2.0\n    first_stage_objectness_loss_weight: 1.0\n    initial_crop_size: 14\n    maxpool_kernel_size: 2\n    maxpool_stride: 2\n    second_stage_box_predictor {\n      mask_rcnn_box_predictor {\n        use_dropout: false\n        dropout_keep_probability: 1.0\n        fc_hyperparams {\n          op: FC\n          regularizer {\n            l2_regularizer {\n              weight: 0.0\n            }\n          }\n          initializer {\n            variance_scaling_initializer {\n              factor: 1.0\n              uniform: true\n              mode: FAN_AVG\n            }\n          }\n        }\n        share_box_across_classes: true\n      }\n    }\n    second_stage_post_processing {\n      batch_non_max_suppression {\n        score_threshold: 0.0\n        iou_threshold: 0.6\n        max_detections_per_class: 100\n        max_total_detections: 300\n      }\n      score_converter: SOFTMAX\n    }\n    second_stage_localization_loss_weight: 2.0\n    second_stage_classification_loss_weight: 1.0\n    use_static_shapes: true\n    use_matmul_crop_and_resize: true\n    clip_anchors_to_image: true\n    use_static_balanced_label_sampler: true\n    use_matmul_gather_in_matcher: true\n  }\n}\n\ntrain_config: {\n  batch_size: 64\n  sync_replicas: true\n  startup_delay_steps: 0\n  replicas_to_aggregate: 8\n  num_steps: 25000\n  optimizer {\n    momentum_optimizer: {\n      learning_rate: {\n        cosine_decay_learning_rate {\n          learning_rate_base: .04\n          total_steps: 25000\n          warmup_learning_rate: .013333\n          warmup_steps: 2000\n        }\n      }\n      momentum_optimizer_value: 0.9\n    }\n    use_moving_average: false\n  }\n  fine_tune_checkpoint_version: V2\n  fine_tune_checkpoint: \"\/kaggle\/working\/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8\/checkpoint\/ckpt-0\"\n  fine_tune_checkpoint_type: \"classification\"\n  data_augmentation_options {\n    random_horizontal_flip {\n    }\n  }\n\n  max_number_of_boxes: 100\n  unpad_groundtruth_tensors: false\n  use_bfloat16: true  # works only on TPUs\n}\n\ntrain_input_reader: {\n  label_map_path: \"\/kaggle\/working\/workspace\/annotations\/label_map.pbtxt\"\n  tf_record_input_reader {\n    input_path: \"\/kaggle\/working\/workspace\/annotations\/train.record\"\n  }\n}\n\neval_config: {\n  metrics_set: \"coco_detection_metrics\"\n  use_moving_averages: false\n  batch_size: 1;\n}\n\neval_input_reader: {\n  label_map_path: \"\/kaggle\/working\/workspace\/annotations\/label_map.pbtxt\"\n  shuffle: false\n  num_epochs: 1\n  tf_record_input_reader {\n    input_path: \"\/kaggle\/working\/workspace\/annotations\/test.record\"\n  }\n}\n\"\"\"","7fd241b8":"file = open(\"\/kaggle\/working\/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8\/pipeline.config\", 'w')\nfile.truncate()\nfile.write(config)\nfile.close()","03b0dd82":"!wget https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/research\/object_detection\/model_main_tf2.py -P \/kaggle\/working\/workspace\/scripts","20238c61":"# !pip install tensorflow-estimator==2.6.0\n# !pip install tensorflow==2.6.0\n# !pip install keras==2.6.0","5bc8d1f6":"# !cd \/kaggle\/working\/workspace\/scripts\n\n!python \/kaggle\/working\/workspace\/scripts\/model_main_tf2.py  --logtostderr --model_dir=\/kaggle\/working\/training --pipeline_config_path=\/kaggle\/working\/faster_rcnn_resnet50_v1_640x640_coco17_tpu-8\/pipeline.config","54780fc3":"Command to remove moved images","e18e6f93":"The below code will be useful in setting up TF object detection API. Part of it will be in the next notebook.Cheers! You can also refer to (https:\/\/tensorflow-object-detection-api-tutorial.readthedocs.io\/en\/latest\/install.html) for details."}}