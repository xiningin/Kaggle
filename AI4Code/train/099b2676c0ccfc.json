{"cell_type":{"78e492db":"code","85d1fa55":"code","8b956448":"code","877b65ff":"code","6e3a3af7":"code","2fbf0a10":"code","ec831f92":"code","982ce2b0":"code","c983f97d":"code","5a9123c0":"code","785c7691":"code","dbef0d24":"code","9db9b2d4":"code","fdcbd7b0":"code","d87c1321":"code","3c235e29":"markdown","87c978d3":"markdown"},"source":{"78e492db":"!pip install optuna\n!pip install pandas\n!pip install tqdm\n!pip install -Uqq fastai\n!pip install torchinfo\n!pip install torchviz","85d1fa55":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom torch.utils.data import Dataset\nimport torch\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nfrom fastai.data.core import DataLoaders\nfrom fastai.learner import Learner\nfrom fastai.callback.progress import ProgressCallback\nfrom fastai.optimizer import OptimWrapper\nfrom torch import optim\nfrom torchinfo import summary\nfrom torchviz import make_dot\nfrom fastai.losses import MSELossFlat, L1LossFlat\nfrom fastai.callback.schedule import Learner\nfrom fastai.callback.tracker import EarlyStoppingCallback, ReduceLROnPlateau\nfrom fastai.data.transforms import IndexSplitter\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import KFold\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport random\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\n\nfrom IPython.display import display","8b956448":"DEBUG = False\nTRAIN_MODEL = True\nINFER_TEST = True\nONE_FOLD_ONLY = True\nCOMPUTE_LSTM_IMPORTANCE = False\nMODEL_SUMMARY = False\nOOF = False","877b65ff":"path_to_data = '..\/input\/ventilator-pressure-prediction\/'\n","6e3a3af7":"train_name = 'train.csv'\ntest_name = 'test.csv'\ntrain = pd.read_csv(path_to_data + train_name)\ntest = pd.read_csv(path_to_data + test_name)\n\npressure_values = np.sort( train.pressure.unique() )\nsubmission = pd.read_csv(path_to_data + 'sample_submission.csv')\n\nif DEBUG:\n    train = train[:80*1000]\n    test = test[:80*1000]","2fbf0a10":"train.head()","ec831f92":"def add_features(df):\n    df['area'] = df['time_step'] * df['u_in']\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    \n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    df = df.fillna(0)\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_out__max'] = df.groupby(['breath_id'])['u_out'].transform('max')\n    \n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    \n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    df['cross']= df['u_in']*df['u_out']\n    df['cross2']= df['time_step']*df['u_out']\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    return df\n\ntrain = add_features(train)\ntest = add_features(test)\n\nprint('Train dataframe shape',train.shape)\ntrain.head()","982ce2b0":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\n#train.drop(['pressure','id', 'breath_id','one','count','breath_id_lag','breath_id_lag2','breath_id_lagsame','breath_id_lag2same','u_out_lag2'], axis=1, inplace=True)\n#test = test.drop(['id', 'breath_id','one','count','breath_id_lag','breath_id_lag2','breath_id_lagsame','breath_id_lag2same','u_out_lag2'], axis=1)\n\ntrain.drop(['pressure','id', 'breath_id'], axis=1, inplace=True)\ntest = test.drop(['id', 'breath_id'], axis=1)","c983f97d":"train.shape","5a9123c0":"COLS = list(train.columns)\nprint('Number of feature columns =', len(COLS) )\n\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\ntest = RS.transform(test)\n\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])","785c7691":"class VentilatorDataset(Dataset):\n    def __init__(self, data, target):\n        self.data = torch.from_numpy(data).float()\n        if target is not None:\n            self.targets = torch.from_numpy(target).float()\n                \n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        if hasattr(self, 'targets'): return self.data[idx], self.targets[idx]\n        else: return self.data[idx]","dbef0d24":"class RNNModel(nn.Module):\n    def __init__(self, input_size=train.shape[-1]):\n        hidden = [400, 300, 200, 100]\n        super().__init__()\n        self.lstm1 = nn.LSTM(input_size, hidden[0],\n                             batch_first=True, bidirectional=True)\n        self.lstm2 = nn.LSTM(2 * hidden[0], hidden[1],\n                             batch_first=True, bidirectional=True)\n        self.lstm3 = nn.LSTM(2 * hidden[1], hidden[2],\n                             batch_first=True, bidirectional=True)\n        self.lstm4 = nn.LSTM(2 * hidden[2], hidden[3],\n                             batch_first=True, bidirectional=True)\n        self.fc1 = nn.Linear(2 * hidden[3], 50)\n        self.selu = nn.SELU()\n        self.fc2 = nn.Linear(50, 1)\n        self._reinitialize()\n\n    def _reinitialize(self):\n        \"\"\"\n        Tensorflow\/Keras-like initialization\n        \"\"\"\n        for name, p in self.named_parameters():\n            if 'lstm' in name:\n                if 'weight_ih' in name:\n                    nn.init.xavier_uniform_(p.data)\n                elif 'weight_hh' in name:\n                    nn.init.orthogonal_(p.data)\n                elif 'bias_ih' in name:\n                    p.data.fill_(0)\n                    # Set forget-gate bias to 1\n                    n = p.size(0)\n                    p.data[(n \/\/ 4):(n \/\/ 2)].fill_(1)\n                elif 'bias_hh' in name:\n                    p.data.fill_(0)\n            elif 'fc' in name:\n                if 'weight' in name:\n                    nn.init.xavier_uniform_(p.data)\n                elif 'bias' in name:\n                    p.data.fill_(0)\n\n    def forward(self, x):\n        x, _ = self.lstm1(x)\n        x, _ = self.lstm2(x)\n        x, _ = self.lstm3(x)\n        x, _ = self.lstm4(x)\n        x = self.fc1(x)\n        x = self.selu(x)\n        x = self.fc2(x)\n\n        return x","9db9b2d4":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n#device = torch.device(\"cpu\")\nprint(device)","fdcbd7b0":"EPOCH = 250\nBATCH_SIZE = 512\nNUM_FOLDS = 10\n\n\nif 1:\n    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\n    test_preds = []\n    oof_preds = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):        \n        \n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        \n        checkpoint_filepath = f\"folds{fold}.hdf5\"\n        \n        if TRAIN_MODEL:\n            train_dataset = VentilatorDataset(X_train, y_train)\n            valid_dataset = VentilatorDataset(X_valid, y_valid)\n            \n            train_loader = DataLoader(train_dataset, batch_size = BATCH_SIZE, shuffle=True)\n            valid_loader = DataLoader(valid_dataset, batch_size = BATCH_SIZE, shuffle=False)\n\n            dls = DataLoaders(train_loader, valid_loader)\n            model = RNNModel()\n            \n            learn = Learner(dls, model, loss_func=L1LossFlat())\n            lrs  = learn.lr_find()\n                        \n            learn.fit_one_cycle(EPOCH, lr_max=lrs.valley, cbs=ReduceLROnPlateau(monitor='valid_loss', min_delta=0.5, patience=10))\n            \n            torch.save(model.state_dict(), checkpoint_filepath)\n            \n        else:                                   \n            model = RNNModel()\n            model.load_state_dict(torch.load(checkpoint_filepath))\n            model = model.to(device)\n        \n        if MODEL_SUMMARY:\n            print(summary(model, input_size=(train.shape[0], train.shape[1], train.shape[2])))\n            #plot_model(model, to_file='Google_Brain_Keras_Model_vpp.png', show_shapes=True, show_layer_names=True)\n            \n    \n        if OOF:\n            print(' Predicting OOF data...')            \n            oof_dataset = VentilatorDataset(X_valid, None)\n            oof_loader = DataLoader(oof_dataset, batch_size = BATCH_SIZE, shuffle=False)\n            preds = []\n            with torch.no_grad():\n                for data in oof_loader:                    \n                    #pred = model(data.to('cpu')).squeeze(-1).flatten()\n                    pred = model(data.to(device)).squeeze(-1)\n                    preds.extend(pred.detach().cpu().numpy())\n            oof = preds            \n            baseline_mae = np.mean(np.abs( oof-y_valid ))\n            oof_preds.append(baseline_mae)\n            print('OOF MAE = {0}'.format(baseline_mae))\n            print(' Done!')\n                       \n        if INFER_TEST:\n            print(' Predicting test data...')\n            test_dataset = VentilatorDataset(test, None)\n            test_loader = DataLoader(test_dataset, batch_size = BATCH_SIZE, shuffle=False)\n            preds = []\n            with torch.no_grad():\n                for data in test_loader:\n                    pred = model(data.to(device)).squeeze(-1).flatten()\n                    preds.extend(pred.detach().cpu().numpy())            \n            test_preds.append(np.array(preds))\n            print(' Done!')\n                    \n        if COMPUTE_LSTM_IMPORTANCE:\n            results = []\n            print(' Computing LSTM feature importance...')\n            \n            # COMPUTE BASELINE (NO SHUFFLE)\n            oof_dataset = VentilatorDataset(X_valid, None)\n            oof_loader = DataLoader(oof_dataset, batch_size = BATCH_SIZE, shuffle=False)\n            preds = []\n            with torch.no_grad():\n                for data in oof_loader:                    \n                    #pred = model(data.to('cpu')).squeeze(-1).flatten()\n                    pred = model(data.to(device)).squeeze(-1)                    \n                    preds.extend(pred.detach().cpu().numpy())\n            oof = preds\n            baseline_mae = np.mean(np.abs( oof-y_valid ))            \n            results.append({'feature':'BASELINE','mae':baseline_mae})\n                        \n            for k in tqdm(range(len(COLS))):\n                \n                # SHUFFLE FEATURE K\n                save_col = X_valid[:,:,k].copy()\n                np.random.shuffle(X_valid[:,:,k])\n                \n                # COMPUTE OOF MAE WITH FEATURE K SHUFFLED\n                oof_dataset = VentilatorDataset(X_valid, None)\n                oof_loader = DataLoader(oof_dataset, batch_size = BATCH_SIZE, shuffle=False)\n                preds = []\n                with torch.no_grad():\n                    for data in oof_loader:                    \n                        #pred = model(data.to('cpu')).squeeze(-1).flatten()\n                        pred = model(data.to(device)).squeeze(-1)                        \n                        preds.extend(pred.detach().cpu().numpy())\n                oof = preds\n                mae = np.mean(np.abs( oof-y_valid ))            \n                results.append({'feature':COLS[k],'mae':mae})                               \n\n                X_valid[:,:,k] = save_col\n         \n            # DISPLAY LSTM FEATURE IMPORTANCE\n            print()\n            df = pd.DataFrame(results)\n            df = df.sort_values('mae')\n            plt.figure(figsize=(10,20))\n            plt.barh(np.arange(len(COLS)+1),df.mae)\n            plt.yticks(np.arange(len(COLS)+1),df.feature.values)\n            plt.title('LSTM Feature Importance',size=16)\n            plt.ylim((-1,len(COLS)+1))\n            plt.plot([baseline_mae,baseline_mae],[-1,len(COLS)+1], '--', color='orange',\n                     label=f'Baseline OOF\\nMAE={baseline_mae:.3f}')\n            plt.xlabel(f'Fold {fold+1} OOF MAE with feature permuted',size=14)\n            plt.ylabel('Feature',size=14)\n            plt.legend()\n            plt.show()\n                               \n            # SAVE LSTM FEATURE IMPORTANCE\n            df = df.sort_values('mae',ascending=False)\n            df.to_csv(f'lstm_feature_importance_fold_{fold+1}.csv',index=False)\n                               \n        # ONLY DO ONE FOLD\n        if ONE_FOLD_ONLY: break","d87c1321":"if INFER_TEST:\n    PRESSURE_MIN = pressure_values[0]\n    PRESSURE_MAX = pressure_values[-1]\n    PRESSURE_STEP = pressure_values[1] - pressure_values[0]\n\n    # NAME POSTFIX\n    postfix = ''\n    if ONE_FOLD_ONLY: \n        NUM_FOLDS = 1\n        postfix = '_fold_1'\n        \n    # ENSEMBLE FOLDS WITH MEAN\n    submission[\"pressure\"] = sum(test_preds)\/NUM_FOLDS\n    submission.to_csv(f'submission_mean{postfix}.csv', index=False)\n\n    # ENSEMBLE FOLDS WITH MEDIAN\n    submission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\n    submission.to_csv(f'submission_median{postfix}.csv', index=False)\n\n    # ENSEMBLE FOLDS WITH MEDIAN AND ROUND PREDICTIONS\n    submission[\"pressure\"] =\\\n        np.round( (submission.pressure - PRESSURE_MIN)\/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\n    submission.pressure = np.clip(submission.pressure, PRESSURE_MIN, PRESSURE_MAX)\n    submission.to_csv(f'submission_median_round{postfix}.csv', index=False)\n    \n    # DISPLAY SUBMISSION.CSV\n    print(f'__submission{postfix}.csv head')\n    display( submission.head() )","3c235e29":"# Main drivers of pipeline","87c978d3":"This simple version is based on Chris Deotte's pipeline for feature importance and DHoa's FastAI version:\nhttps:\/\/www.kaggle.com\/cdeotte\/lstm-feature-importance\nhttps:\/\/www.kaggle.com\/dienhoa\/ventillator-fastai-lb-0-169-no-kfolds-no-blend\/notebook?scriptVersionId=77679435\n\nThe main goal is the simplest (but full) effective pipeline based on flags (DEBUG, TRAIN_MODEL,INFER_TEST etc)."}}