{"cell_type":{"255e2fec":"code","e2785cdf":"code","0a00c640":"code","84d6b5a6":"code","2250fa81":"code","1124f148":"code","2c82a721":"code","bd01b3a5":"code","78e1d427":"code","9b5fa7a4":"markdown","6705be46":"markdown","6c598da4":"markdown","7b3fa4e1":"markdown","5288d322":"markdown","9b07a2d0":"markdown"},"source":{"255e2fec":"'''!pip install tensorflow==2.3.0\n!pip install gym\n!pip install keras\n!pip install keras-rl2'''","e2785cdf":"\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pprint import pprint\nimport tensorflow as tf\nfrom kaggle_environments import make, evaluate\nfrom gym import spaces\n\nimport matplotlib.pyplot as plt\nfrom collections import deque\nfrom tensorflow import keras\nfrom tqdm import tqdm\nimport plotly.graph_objects as go\nimport plotly.express as px\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import translate, adjacent_positions, min_distance\n","0a00c640":"for action in Action:\n    print(action)","84d6b5a6":"class HungryGeeseGym:\n    def __init__(self, agent2=\"..\/input\/coward-goose\/submission-ralph-coward.py\"):\n        self.ks_env = make(\"hungry_geese\", debug=False)\n        self.env = self.ks_env.train([None, agent2,agent2,agent2])\n        self.rows = self.ks_env.configuration.rows\n        self.columns = self.ks_env.configuration.columns\n        self.observation_space_size = self.rows * self.columns\n        # Learn about spaces here: http:\/\/gym.openai.com\/docs\/#spaces\n        # Permitted actions\n        # ['NORTH', 'EAST', 'SOUTH', 'WEST']\n        actL = []\n        self.debug = True\n        for action in Action:\n            actL.append(action)\n        self.actions = actL\n        \n        #print(actL)\n        # Defined Action Space(Must)\n        self.action_space = spaces.Discrete(len(self.actions))\n        \n        self.observation_space = spaces.Box(low=np.zeros(shape=(11,), dtype=int), high=np.zeros(shape=(11,), dtype=int)+10)\n        \n        # Tuple corresponding to the min and max possible rewards\n        self.reward_range = (-10, 1000)\n        # StableBaselines throws error if these are not defined\n        self.spec = None\n        self.metadata = None\n        self.last = self.actions[self.action_space.sample()]\n    def reset(self):\n        self.last = self.actions[self.action_space.sample()]\n        \n        self.obs = self.env.reset()\n        #print(self.obs)\n        return self.sensors(self.get_grid(self.obs))\n    \n    def opposite(self,action):\n        if action == Action.NORTH:\n            return Action.SOUTH\n        if action == Action.SOUTH:\n            return Action.NORTH\n        if action == Action.EAST:\n            return Action.WEST\n        if action == Action.WEST:\n            return Action.EAST\n        \n    def step(self, action):\n        action = self.actions[action]\n        # Check if agent's move is valid\n        is_valid = action != self.last\n\n\n        \n        self.last = self.opposite(action)\n\n\n        cont = len(self.obs['geese'][self.obs['index']])\n        #print(cont)\n        reward, done, _ = -50, True, {}\n        if is_valid: # Play the move\n            self.obs, old_reward, done, _ = self.env.step(action.name)\n            #print(self.obs)\n            if(done):\n                reward = 100 # se ganhou\n                if(self.obs['geese'][self.obs['index']] == []):\n                    reward = -5 # se perdeu\n            else:\n                if(len(self.obs['geese'][self.obs['index']])>cont ):\n                    reward = 20 # se comeu\n                else:\n                    if(len(self.obs['geese'][self.obs['index']])<cont ):\n                        reward = -10 \n                        #print(\"Step: \"+str(self.obs['step']) + \" \/  Perdeu tamanho... \" + str(reward)+\" Pontos\")\n                    else:\n                        reward = -1 # se n\u00e3o fez nada\n\n\n                \n        grid = self.get_grid(self.obs)\n        \n        sensores = self.sensors(grid)\n        if(self.debug):\n            print(sensores.shape)\n            print(sensores)\n            print(grid.reshape(1,-1)[0].shape)\n\n        # o obs tem que ser [indice da ultima a\u00e7\u00e3o. [sens1], [sens2], [sens3]]\n        return sensores, reward, done, _\n    \n    def sensors(self,grid):\n       \n        if(len(self.obs['geese'][self.obs['index']])==0):\n            return np.array([0,0,0,0,0,0,0,0,0,0,0])\n        px,py = row_col(self.obs['geese'][self.obs['index']][0],self.columns)\n        actL = []\n        for action in Action:\n            actL.append(action)\n        frente = 0\n        if(self.last):\n            for i in range(0,len(actL)):\n                if(actL[i] == self.opposite(self.last)):\n                    frente = i\n                    break\n            actL.remove(self.last)\n        else: # Diz que esta indo para o norte e remove o oposto (SUL)\n            actL.remove(action.SOUTH)\n            frente = 0\n       \n\n        direita = frente+1\n        if(direita>3):\n            direita = direita-4\n        esquerda = frente + 3\n        if(esquerda>3):\n            esquerda = esquerda - 4\n\n        movimentos = [\n            [-1,0],     #Norte\n            [0,1],      #Leste\n            [1,0],      #Sul\n            [0,-1],     #Oeste\n        ]\n        sensor_frente = [0,0] #[distancia,tipo (0:inimigo,1:food)]\n        for i in range(0,11):\n            px_a,py_a = (px+movimentos[frente][0]*(i+1),py+movimentos[frente][1]*(i+1))\n            if(px_a>6):\n                px_a = px_a - 7\n            if(py_a>10):\n                py_a = py_a - 11\n            \n            if(px_a<0):\n                px_a = 7 + px_a\n            if(py_a<0):\n                py_a = 11 + py_a\n            \n            if(grid[px_a][py_a] == 2 or grid[px_a][py_a] == 1):\n                break\n            if(grid[px_a][py_a] == 3):\n                sensor_frente[1] = 1\n                break\n            else:\n                sensor_frente[0]+=1\n        \n        sensor_esquerda = [0,0] #[distancia,tipo (0:inimigo,1:food)]\n        for i in range(0,11):\n            px_a,py_a = (px+movimentos[esquerda][0]*(i+1),py+movimentos[esquerda][1]*(i+1))\n            if(px_a>6):\n                px_a = px_a - 7\n            if(py_a>10):\n                py_a = py_a - 11\n            \n            if(px_a<0):\n                px_a = 7+ px_a\n            if(py_a<0):\n                py_a = 11 + py_a\n            \n            if(grid[px_a][py_a] == 2 or grid[px_a][py_a] == 1):\n                break\n            if(grid[px_a][py_a] == 3):\n                sensor_esquerda[1] = 1\n                break\n            else:\n                sensor_esquerda[0]+=1\n        \n        sensor_direita = [0,0] #[distancia,tipo (0:inimigo,1:food)]\n        for i in range(0,11):\n            px_a,py_a = (px+movimentos[direita][0]*(i+1),py+movimentos[direita][1]*(i+1))\n            if(px_a>6):\n                px_a = px_a - 7\n            if(py_a>10):\n                py_a = py_a - 11\n            \n            if(px_a<0):\n                px_a = 7 + px_a\n            if(py_a<0):\n                py_a = 11 + py_a\n            \n            \n            if(grid[px_a][py_a] == 2 or grid[px_a][py_a] == 1):\n                break\n            if(grid[px_a][py_a] == 3):\n                sensor_direita[1] = 1\n                break\n            else:\n                sensor_direita[0]+=1\n        \n        # Verificando as diagonais\n        tras = frente + 2\n        if(tras>=4):\n            tras-=4\n        \n        px_a,py_a = (px+    movimentos[direita][0]  + movimentos[frente][0] )  ,  (py+     movimentos[direita][1]  + movimentos[frente][1])   \n        if(px_a>6):\n            px_a = px_a - 7\n        if(py_a>10):\n            py_a = py_a - 11\n        \n        if(px_a<0):\n            px_a = 7+ px_a\n        if(py_a<0):\n            py_a = 11 + py_a\n        frente_direita = grid[px_a,py_a]\n\n        px_a,py_a =   (px+    movimentos[esquerda][0] + movimentos[frente][0] ) ,   (py+     movimentos[esquerda][1] + movimentos[frente][1])\n        if(px_a>6):\n            px_a = px_a - 7\n        if(py_a>10):\n            py_a = py_a - 11\n        \n        if(px_a<0):\n            px_a = 7+ px_a\n        if(py_a<0):\n            py_a = 11 + py_a\n        frente_esquerda = grid[px_a,py_a]\n\n        px_a,py_a =      (px+    movimentos[esquerda][0] + movimentos[tras][0]   ) ,   (py+     movimentos[esquerda][1] + movimentos[tras][1]  )\n        if(px_a>6):\n            px_a = px_a - 7\n        if(py_a>10):\n            py_a = py_a - 11\n        \n        if(px_a<0):\n            px_a = 7+ px_a\n        if(py_a<0):\n            py_a = 11 + py_a\n        tras_esqueda = grid[px_a,py_a]\n\n\n        px_a,py_a=      (px+    movimentos[direita][0]  + movimentos[tras][0]   ) ,   (py+     movimentos[direita][1]  + movimentos[tras][1]  )\n        if(px_a>6):\n            px_a = px_a - 7\n        if(py_a>10):\n            py_a = py_a - 11\n        \n        if(px_a<0):\n            px_a = 7+ px_a\n        if(py_a<0):\n            py_a = 11 + py_a\n        tras_direita = grid[px_a,py_a]\n\n        if(self.debug):\n            print(\"x:\"+str(px)+\" \/ y:\"+str(py))\n            print(\"Frente (\"+str(self.actions[frente].name) +\"):\"+str(sensor_frente) + \" \/ Esquerda (\"+str(self.actions[esquerda].name) +\"):\"+str(sensor_esquerda) + \" \/ Direita (\"+str(self.actions[direita].name) +\"):\"+str(sensor_direita))\n            print(grid)\n        return np.array([frente,sensor_frente[0],sensor_frente[1],sensor_esquerda[0],sensor_esquerda[1],sensor_direita[0],sensor_direita[1],frente_direita,frente_esquerda,tras_direita,tras_esqueda])\n\n    def get_grid(self,obs):\n        mapa = []\n        for i in range(0,self.rows):\n            mapa.append([])\n            for j in range(0,self.columns):\n                achou = False\n                for food in obs['food']:\n                    x,y = row_col(food,self.columns)\n                    if(x == i and y == j):\n                        mapa[i].append(3)\n                        achou = True\n                        break\n                if(achou):\n                    continue\n                aux = 0\n                for goose in obs['geese']:\n                    gs = 2\n                    if(aux == obs['index']):\n                        gs = 1\n                    aux = aux +1\n                    for part in goose:\n                        x,y = row_col(part,self.columns)\n                        if(x == i and y == j):\n                            mapa[i].append(gs)\n                            achou = True\n                            break\n                    if(achou):\n                        break\n                if(achou):\n                    continue\n                mapa[i].append(0)\n\n        return np.array(mapa)","2250fa81":"class DeepQNetwork():\n    def __init__(self, states, actions, alpha, gamma, epsilon,epsilon_min, epsilon_decay,layers_num,layer_neuron_num):\n        self.nS = states\n        self.nA = actions\n        self.memory = deque([], maxlen=2500)\n        self.alpha = alpha\n        self.gamma = gamma\n        #Explore\/Exploit\n        self.epsilon = epsilon\n        self.epsilon_min = epsilon_min\n        self.epsilon_decay = epsilon_decay\n        self.model = self.build_model(layers_num,layer_neuron_num)\n        self.loss = []\n        \n    def build_model(self,layers_num,layer_neuron_num):\n        model = keras.Sequential() #linear stack of layers https:\/\/keras.io\/models\/sequential\/\n        model.add(keras.layers.Dense(11, input_dim=self.nS, activation='relu')) #[Input]\n        for i in range(0,layers_num):\n            model.add(keras.layers.Dense(layer_neuron_num, activation='relu'))\n        model.add(keras.layers.Dense(self.nA, activation='linear')) #[output]\n        #   Size has to match the output (different actions)\n        #   Linear activation on the last layer\n        model.compile(loss='mean_squared_error', #Loss function: Mean Squared Error\n                      optimizer=keras.optimizers.Adam(lr=self.alpha)) #Optimaizer: Adam (Feel free to check other options)\n        return model\n\n    def action(self, state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.nA) #Explore\n        action_vals = self.model.predict(state) #Exploit: Use the NN to predict the correct action from this state\n        return np.argmax(action_vals[0])\n\n    def test_action(self, state): #Exploit\n        action_vals = self.model.predict(state)\n        return np.argmax(action_vals[0])\n\n    def store(self, state, action, reward, nstate, done):\n        #Store the experience in memory\n        self.memory.append( (state, action, reward, nstate, done) )\n\n    def experience_replay(self, batch_size):\n        #Execute the experience replay\n        minibatch = random.sample( self.memory, batch_size ) #Randomly sample from memory\n\n        #Convert to numpy for speed by vectorization\n        x = []\n        y = []\n        np_array = np.array(minibatch,dtype=object)\n        st = np.zeros((0,self.nS)) #States\n        nst = np.zeros( (0,self.nS) )#Next States\n        for i in range(len(np_array)): #Creating the state and next state np arrays\n            st = np.append( st, np_array[i,0], axis=0)\n            nst = np.append( nst, np_array[i,3], axis=0)\n        st_predict = self.model.predict(st) #Here is the speedup! I can predict on the ENTIRE batch\n        nst_predict = self.model.predict(nst)\n        index = 0\n        for state, action, reward, nstate, done in minibatch:\n            x.append(state)\n            #Predict from state\n            nst_action_predict_model = nst_predict[index]\n            if done == True: #Terminal: Just assign reward much like {* (not done) - QB[state][action]}\n                target = reward\n            else:   #Non terminal\n                target = reward + self.gamma * np.amax(nst_action_predict_model)\n            target_f = st_predict[index]\n            target_f[action] = target\n            y.append(target_f)\n            index += 1\n        #Reshape for Keras Fit\n        x_reshape = np.array(x).reshape(batch_size,self.nS)\n        y_reshape = np.array(y)\n        epoch_count = 1 #Epochs is the number or iterations\n        hist = self.model.fit(x_reshape, y_reshape, epochs=epoch_count, verbose=0)\n        #Graph Losses\n        for i in range(epoch_count):\n            self.loss.append( hist.history['loss'][i] )\n        #Decay Epsilon\n        if self.epsilon > self.epsilon_min:\n            self.epsilon *= self.epsilon_decay","1124f148":"os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\nimport math\ndef train(layers_num,layer_neuron_num):\n\n    try:\n        os.mkdir(\".\/data\/\"+str(layers_num)+'-'+str(layer_neuron_num))\n        print(\"-------- Iniciando treino para o modelo com \"+str(layers_num)+\" Hidden Layers e com \"+str(layer_neuron_num)+\" Neuronios em cada Layer --------\")\n    except:\n        print(\"Treino ja Executado anteriormente... Tudo ser\u00e1 sobrescrito!\")\n        \n    env = HungryGeeseGym()\n\n    #Global Variables\n    EPISODES = 5000\n    TRAIN_END = 0\n    #Hyper Parameters\n    discount_rate = 0.95 #Gamma\n    learning_rate = 0.0001 #Alpha\n    batch_size = 24 #Size of the batch used in the experience replay\n\n    #Create the agent\n    nS = env.observation_space.shape[0]\n    nA = env.action_space.n\n    try:\n        del dqn\n    except:\n        discount_rate\n    dqn = DeepQNetwork(nS, nA, learning_rate, discount_rate, 1, 0.001, (0.222)*(1\/(EPISODES\/2)),layers_num,layer_neuron_num )\n    #Training\n    rewards = [] #Store rewards for graphing\n    epsilons = [] # Store the Explore\/Exploit\n    TEST_Episodes = 0\n    \n    # Para deixar sem printar nada no ambiente\n    env.debug = False\n    \n    for e in tqdm(range(EPISODES),desc = (str(layers_num)+'-'+str(layer_neuron_num)),leave = False):\n        #if(e%int(EPISODES\/100) == 0):\n        #    print(str(layers_num)+'-'+str(layer_neuron_num)+\": \"+str(math.floor(100*e\/EPISODES)) + \"%\")\n        state = env.reset()\n        state = np.reshape(state, [1, nS]) # Resize to store in memory to pass to .predict\n        tot_rewards = 0\n        for time in range(200): \n            action = dqn.action(state)\n            nstate, reward, done, _ = env.step(action)\n            nstate = np.reshape(nstate, [1, nS])\n            tot_rewards += reward\n            dqn.store(state, action, reward, nstate, done) # Resize to store in memory to pass to .predict\n            state = nstate\n            if done:\n                rewards.append(tot_rewards)\n                epsilons.append(dqn.epsilon)\n                '''print(\"episode: {}\/{}, score: {}, e: {}\"\n                    .format(e, EPISODES, tot_rewards, dqn.epsilon))'''\n                break\n            #Experience Replay\n            if len(dqn.memory) > batch_size:\n                dqn.experience_replay(batch_size)\n    \n    dqn.model.save('.\/data\/'+str(layers_num)+'-'+str(layer_neuron_num)+'\/model')\n    df0 = pd.DataFrame()\n    df0['Reward'] = np.array(rewards)\n    df0.to_csv('.\/data\/'+str(layers_num)+'-'+str(layer_neuron_num)+\"\/modelData-total.csv\")\n    med = 0\n    y_values = []\n    x_values = []\n    eps_graph = []\n    eps = 0\n    aux = 0\n    BATCH = 10\n    for i in range(0,len(rewards)): \n        med+=rewards[i]\n        eps+=epsilons[i]\n        if i%(BATCH-1) == 0 and i != 0:\n            x_values.append(aux)\n            aux+=1\n            y_values.append(med\/BATCH)\n            eps_graph.append(40*eps\/BATCH)\n            eps = 0\n            med = 0\n\n    df = pd.DataFrame()\n    #print(x_values)\n    df[\"Score\"] = np.array(y_values)\n    df['Epslon'] = np.array(eps_graph)\n    df[\"round\"] = np.array(x_values)\n    fig = px.line(df, x='round', y=['Score','Epslon'])\n    fig.show()\n    df.to_csv('.\/data\/'+str(layers_num)+'-'+str(layer_neuron_num)+\"\/modelData.csv\")\n    fig.write_html('.\/data\/'+str(layers_num)+'-'+str(layer_neuron_num)+\"plot.html\")\n\n","2c82a721":"train(7,49)","bd01b3a5":"actions = []\nfor act in Action:\n        actions.append(act)\ndef get_grid_from_obs(obs,columns,rows):\n        mapa = []\n        for i in range(0,rows):\n            mapa.append([])\n            for j in range(0,columns):\n                achou = False\n                for food in obs['food']:\n                    x,y = row_col(food,columns)\n                    if(x == i and y == j):\n                        mapa[i].append(3)\n                        achou = True\n                        break\n                if(achou):\n                    continue\n                aux = 0\n                for goose in obs['geese']:\n                    gs = 2\n                    if(aux == obs['index']):\n                        gs = 1\n                    aux = aux +1\n                    for part in goose:\n                        x,y = row_col(part,columns)\n                        if(x == i and y == j):\n                            mapa[i].append(gs)\n                            achou = True\n                            break\n                    if(achou):\n                        break\n                if(achou):\n                    continue\n                mapa[i].append(0)\n        return np.array(mapa)\ndef opposite(action):\n    if action == Action.NORTH:\n        return Action.SOUTH\n    if action == Action.SOUTH:\n        return Action.NORTH\n    if action == Action.EAST:\n        return Action.WEST\n    if action == Action.WEST:\n        return Action.EAST\ndef get_sensors_from_grid(grid,columns,obs,last,debug):\n    print()\n    \n    global actions\n    if(len(obs['geese'][obs['index']])==0):\n        return np.array([0,0,0,0,0,0,0])\n    px,py = row_col(obs['geese'][obs['index']][0],columns)\n    actL = []\n    for action in Action:\n        actL.append(action)\n    frente = 0\n    if(debug):\n        print(last.name)\n\n    if(last):\n        for i in range(0,len(actL)):\n            if(actL[i] == opposite(last)):\n                frente = i\n                break\n        actL.remove(last)\n    else: # Diz que esta indo para o norte e remove o oposto (SUL)\n        actL.remove(action.SOUTH)\n        frente = 0\n    \n\n    direita = frente+1\n    if(direita>3):\n        direita = direita-4\n    esquerda = frente + 3\n    if(esquerda>3):\n        esquerda = esquerda - 4\n\n    movimentos = [\n        [-1,0],     #Norte\n        [0,1],     #Leste\n        [1,0],      #Sul\n        [0,-1],      #Oeste\n    ]\n    sensor_frente = [0,0] #[distancia,tipo (0:inimigo,1:food)]\n    for i in range(0,11):\n        px_a,py_a = (px+movimentos[frente][0]*(i+1),py+movimentos[frente][1]*(i+1))\n        if(px_a>6):\n            px_a = px_a - 7\n        if(py_a>10):\n            py_a = py_a - 11\n        \n        if(px_a<0):\n            px_a = 7 + px_a\n        if(py_a<0):\n            py_a = 11 + py_a\n        \n        if(grid[px_a][py_a] == 2 or grid[px_a][py_a] == 1):\n            break\n        if(grid[px_a][py_a] == 3):\n            sensor_frente[1] = 1\n            break\n        else:\n            sensor_frente[0]+=1\n    \n    sensor_esquerda = [0,0] #[distancia,tipo (0:inimigo,1:food)]\n    for i in range(0,11):\n        px_a,py_a = (px+movimentos[esquerda][0]*(i+1),py+movimentos[esquerda][1]*(i+1))\n        if(px_a>6):\n            px_a = px_a - 7\n        if(py_a>10):\n            py_a = py_a - 11\n        \n        if(px_a<0):\n            px_a = 7+ px_a\n        if(py_a<0):\n            py_a = 11 + py_a\n        \n        if(grid[px_a][py_a] == 2 or grid[px_a][py_a] == 1):\n            break\n        if(grid[px_a][py_a] == 3):\n            sensor_esquerda[1] = 1\n            break\n        else:\n            sensor_esquerda[0]+=1\n    \n    sensor_direita = [0,0] #[distancia,tipo (0:inimigo,1:food)]\n    for i in range(0,11):\n        px_a,py_a = (px+movimentos[direita][0]*(i+1),py+movimentos[direita][1]*(i+1))\n        if(px_a>6):\n            px_a = px_a - 7\n        if(py_a>10):\n            py_a = py_a - 11\n        \n        if(px_a<0):\n            px_a = 7 + px_a\n        if(py_a<0):\n            py_a = 11 + py_a\n        \n        \n        if(grid[px_a][py_a] == 2 or grid[px_a][py_a] == 1):\n            break\n        if(grid[px_a][py_a] == 3):\n            sensor_direita[1] = 1\n            break\n        else:\n            sensor_direita[0]+=1\n    \n    # Verificando as diagonais\n    tras = frente + 2\n    if(tras>=4):\n        tras-=4\n    \n    px_a,py_a = (px+    movimentos[direita][0]  + movimentos[frente][0] )  ,  (py+     movimentos[direita][1]  + movimentos[frente][1])   \n    if(px_a>6):\n        px_a = px_a - 7\n    if(py_a>10):\n        py_a = py_a - 11\n    \n    if(px_a<0):\n        px_a = 7+ px_a\n    if(py_a<0):\n        py_a = 11 + py_a\n    frente_direita = grid[px_a,py_a]\n\n    px_a,py_a =   (px+    movimentos[esquerda][0] + movimentos[frente][0] ) ,   (py+     movimentos[esquerda][1] + movimentos[frente][1])\n    if(px_a>6):\n        px_a = px_a - 7\n    if(py_a>10):\n        py_a = py_a - 11\n    \n    if(px_a<0):\n        px_a = 7+ px_a\n    if(py_a<0):\n        py_a = 11 + py_a\n    frente_esquerda = grid[px_a,py_a]\n\n    px_a,py_a =      (px+    movimentos[esquerda][0] + movimentos[tras][0]   ) ,   (py+     movimentos[esquerda][1] + movimentos[tras][1]  )\n    if(px_a>6):\n        px_a = px_a - 7\n    if(py_a>10):\n        py_a = py_a - 11\n    \n    if(px_a<0):\n        px_a = 7+ px_a\n    if(py_a<0):\n        py_a = 11 + py_a\n    tras_esqueda = grid[px_a,py_a]\n\n\n    px_a,py_a=      (px+    movimentos[direita][0]  + movimentos[tras][0]   ) ,   (py+     movimentos[direita][1]  + movimentos[tras][1]  )\n    if(px_a>6):\n        px_a = px_a - 7\n    if(py_a>10):\n        py_a = py_a - 11\n    \n    if(px_a<0):\n        px_a = 7+ px_a\n    if(py_a<0):\n        py_a = 11 + py_a\n    tras_direita = grid[px_a,py_a]\n    if(debug):\n        print(\"x:\"+str(px)+\" \/ y:\"+str(py))\n        print(\"Frente (\"+str(actions[frente].name) +\"):\"+str(sensor_frente) + \" \/ Esquerda (\"+str(actions[esquerda].name) +\"):\"+str(sensor_esquerda) + \" \/ Direita (\"+str(actions[direita].name) +\"):\"+str(sensor_direita))\n        print(grid)\n    return np.array([frente,sensor_frente[0],sensor_frente[1],sensor_esquerda[0],sensor_esquerda[1],sensor_direita[0],sensor_direita[1],frente_direita,frente_esquerda,tras_direita,tras_esqueda])\nmodel = keras.models.load_model('.\/data\/7-49\/model')\nlast = Action.SOUTH\ndef agent(obs,config):\n    global last\n    global model\n    state = get_grid_from_obs(obs,config.columns,config.rows)\n    state = get_sensors_from_grid(state,config.columns,obs,last,True)\n    print()\n    state = np.reshape(state, [1, 11])\n    print(state)\n    action =np.argmax(model.predict(state)[0]) \n    actL = []\n    for act in Action:\n            actL.append(act)\n    print(actL)\n    last = opposite(actL[action])\n    return actL[action].name\n    ","78e1d427":"env1 = make(\"hungry_geese\", debug=False) #set debug to True to see agent internals each step\n\nenv1.reset()\nop1 = [agent,\"..\/input\/coward-goose\/submission-ralph-coward.py\", \"..\/input\/coward-goose\/submission-ralph-coward.py\",\"..\/input\/coward-goose\/submission-ralph-coward.py\"]\nop2 = [agent,agent,agent,agent]\nenv1.run(op1)\nenv1.render(mode=\"ipython\",width=700, height=600)","9b5fa7a4":"\n## Libs necess\u00e1rias","6705be46":"# Configurando o ambiente de Treino","6c598da4":"## Classe para o ambiente do Ganso","7b3fa4e1":"# Teste","5288d322":"## Treino com DQN","9b07a2d0":"# Criando a RNA e Treinando"}}