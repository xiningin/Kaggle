{"cell_type":{"f0ef26a5":"code","b9f7fdeb":"code","9bcdc417":"code","42c7fc57":"code","d342e390":"code","34f4d121":"code","39e25012":"code","4ba4cb9a":"code","af0111ec":"code","911cab74":"code","588642a4":"code","df6a1aa2":"code","9f9958a1":"code","ecf6a854":"code","b41ad9bc":"code","197cc3a4":"code","78506544":"code","f16cb333":"code","5a16c719":"code","bd7c06df":"code","1402cc3e":"code","62e1eb2e":"code","41809fe5":"code","1bb482e0":"code","39ce29e3":"code","43cfd1aa":"code","db608540":"code","ff755bcb":"markdown","f4cf510d":"markdown","69f31716":"markdown","1e786ae3":"markdown","50ce047d":"markdown","420e971d":"markdown","8e8b65be":"markdown","3b22f168":"markdown","04e7478c":"markdown","c10f652b":"markdown","13e0118c":"markdown","d5dbeaac":"markdown","6c8e7107":"markdown","5aaf1ece":"markdown","ceea8a86":"markdown","3e651aaf":"markdown","a1f6e2c9":"markdown","c7a588d3":"markdown","2081d51a":"markdown","3d9b4f2b":"markdown","fffff07d":"markdown","29d75a2f":"markdown"},"source":{"f0ef26a5":"# import numpy, matplotlib, etc.\nimport math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# sklearn imports\nimport sklearn\nfrom sklearn import metrics\nfrom sklearn import pipeline\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\n\n# define plt settings\nsns.set_theme()\nplt.rcParams[\"font.size\"] = 20\nplt.rcParams[\"axes.labelsize\"] = 20\nplt.rcParams[\"xtick.labelsize\"] = 20\nplt.rcParams[\"ytick.labelsize\"] = 20\nplt.rcParams[\"legend.fontsize\"] = 20\nplt.rcParams[\"legend.markerscale\"] = 1.5\nplt.rcParams[\"figure.figsize\"] = (20, 10)\nplt.rcParams[\"legend.title_fontsize\"] = 20","b9f7fdeb":"test_df  = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain_df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_df","9bcdc417":"test_df","42c7fc57":"# function to drop a column from the df\ndef drop_col(df, col_name):\n    df.drop([col_name], axis=1, inplace=True)","d342e390":"# Drop PassengerId, since it's not relevant to survival\ndrop_col(train_df, 'PassengerId')\n# Can't drop for test_df since it's required for submission","34f4d121":"train_df['Ticket'].describe()","39e25012":"drop_col(train_df, 'Ticket')\ndrop_col(test_df,  'Ticket')\ndrop_col(train_df, 'Name')\ndrop_col(test_df,  'Name')","4ba4cb9a":"# calling this to look for incomplete columns\ntrain_df.info()","af0111ec":"# replace all empty values to np.NaN values\ntrain_df.replace('', np.NaN, inplace=True)\ntrain_df.fillna(np.NaN, inplace=True)\ntest_df.replace('', np.NaN, inplace=True)\ntest_df.fillna(np.NaN, inplace=True)\n\n# 'U' stands for `Unknown`\ntrain_df['Embarked'].fillna(\"U\", inplace=True)\ntest_df['Embarked'].fillna(\"U\", inplace=True)","911cab74":"# count empty values in each column\ndef count_empty_values_in_each_column(df: pd.DataFrame):\n  print('empty values')\n  print('------------\\n')\n  \n  for col in df.columns:\n    print(f\"{col}: {df[col].isna().sum()}\")\n\n\ncount_empty_values_in_each_column(train_df)","588642a4":"# replace all the column's empty values with the column's mean value or a random one (50% chance for value around mean) \n# this is to avoid stack too many rows with the same exact value, but still being better than random.\ndef fill_na_random_or_mean(df, column_name):\n    df_not_null = df[~df[column_name].isnull()]\n    df_null = df[df[column_name].isnull()]\n    mean = round(df_not_null[column_name].mean())\n    std = round(df_not_null[column_name].std())\n    df[column_name] = df[column_name].apply(\n        lambda x: np.random.choice([\n            mean,\n            mean - std,\n            mean + std,\n            np.random.choice(df_not_null[column_name]),\n            np.random.choice(df_not_null[column_name]),\n            np.random.choice(df_not_null[column_name])\n        ]) if pd.isnull(x) else x\n    )\n\n    \nfill_na_random_or_mean(train_df, 'Age')\nfill_na_random_or_mean(test_df, 'Age')\nfill_na_random_or_mean(test_df, 'Fare')\ntrain_df","df6a1aa2":"import re\nLetterSearcher = re.compile(\"(\\w)\")\n\ndef cabin_to_deck(df):\n    df['Cabin'].fillna(\"U\", inplace=True) # U - Unknown\n    df['Deck']  = df['Cabin'].map(lambda cabin: cabin[0])\n    drop_col(df, 'Cabin')\n\ncabin_to_deck(train_df)\ncabin_to_deck(test_df)\ntrain_df","9f9958a1":"train_graph_df = train_df.copy()\n\ndef encode_str(s: str) -> int:\n    return ord(s[0]) - ord('A') + 1\n\ntrain_graph_df[\"Sex\"] = train_graph_df[\"Sex\"].map(encode_str)\ntrain_graph_df[\"Embarked\"] = train_graph_df[\"Embarked\"].map(encode_str)\ntrain_graph_df[\"Deck\"] = train_graph_df[\"Deck\"].map(encode_str)","ecf6a854":"# show how many survived and didn't survive.\nplt.figure(figsize=(2,5))\nsns.countplot(x='Survived', data=train_df)\nplt.show()","b41ad9bc":"# show correlation between all features\nplt.figure(figsize=(12,10))\ncor = train_graph_df.corr()\nsns.heatmap(cor, annot=True, vmin=-1, vmax=1)\nplt.show()","197cc3a4":"# show count and survival rate from each column\nplt.figure(figsize=(20,55))\n\nplt.subplot(9,2,1)\nsns.barplot(x='Sex',y='Survived', data=train_df, estimator=np.sum)\nplt.subplot(9,2,2)\nsns.countplot(x='Sex', data=train_df, hue='Survived')\n\nplt.subplot(9,2,3)\nsns.lineplot(x='Age',y='Survived', data=train_df)\nplt.subplot(9,2,4)\nsns.histplot(x='Age', data=train_df, hue='Survived')\n\nplt.subplot(9,2,5)\nsns.barplot(x='Pclass',y='Survived', data=train_df, estimator=np.sum)\nplt.subplot(9,2,6)\nsns.countplot(x='Pclass', data=train_df, hue='Survived')\n\nplt.subplot(9,2,7)\nsns.lineplot(x='Fare',y='Survived', data=train_df)\nplt.subplot(9,2,8)\nsns.histplot(x='Fare', data=train_df, hue='Survived')\n\nplt.subplot(9,2,9)\nsns.barplot(x='Parch',y='Survived', data=train_df, estimator=np.sum)\nplt.subplot(9,2,10)\nsns.countplot(x='Parch', data=train_df,hue='Survived')\n\nplt.subplot(9,2,11)\nsns.barplot(x='SibSp',y='Survived', data=train_df, estimator=np.sum)\nplt.subplot(9,2,12)\nsns.countplot(x='SibSp', data=train_df,hue='Survived')\n\nplt.subplot(9,2,13)\nsns.barplot(x='Embarked',y='Survived', data=train_df, estimator=np.sum)\nplt.subplot(9,2,14)\nsns.countplot(x='Embarked', data=train_df,hue='Survived')\n\nplt.subplot(9,2,15)\nsns.barplot(x='Deck',y='Survived', data=train_df, estimator=np.sum)\nplt.subplot(9,2,16)\nsns.countplot(x='Deck', data=train_df,hue='Survived')\n\nplt.show()","78506544":"datasets = [train_df, test_df, train_graph_df]\n\n# Add age group category\nfor df in datasets:\n    df[\"AgeGroup\"] = df[\"Age\"].map(lambda age: round(age\/10))\n\n# Add AgeClass category\nfor df in datasets:\n    df['Age_Class'] = df['Age'] * df['Pclass']","f16cb333":"# show correlation between all features after adding a new one\nplt.figure(figsize=(17,17))\ncor = train_graph_df.corr()\nsns.heatmap(cor, vmin=-1, vmax=1, annot=True)\nplt.show()","5a16c719":"from sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\n\ntrain_df_enc = train_df.drop([\"Survived\"], axis=1)\ntest_df_enc  = test_df.drop([\"PassengerId\"], axis=1)\ntrain_df_enc['Sex'] = label.fit_transform(train_df_enc['Sex'])\ntest_df_enc['Sex'] = label.fit_transform(test_df_enc['Sex'])\ntrain_df_enc['Embarked'] = label.fit_transform(train_df_enc['Embarked'])\ntest_df_enc['Embarked'] = label.fit_transform(test_df_enc['Embarked'])\ntrain_df_enc['Deck'] = label.fit_transform(train_df_enc['Deck'])\ntest_df_enc['Deck'] = label.fit_transform(test_df_enc['Deck'])","bd7c06df":"train_df_enc.info() # show to check all Dtypes are numerics","1402cc3e":"test_df_enc.info() # show to check all Dtypes are numerics","62e1eb2e":"# split the training dataframe into `X` - features, and `t` - target\n\nX = train_df_enc\nt = train_df[\"Survived\"].copy()","41809fe5":"from sklearn import neural_network\nimport plotly.express as px\n\nmlp = neural_network.MLPClassifier(activation='logistic', solver='sgd', alpha=0, max_iter=10000)\nlogistic_regression = pipeline.make_pipeline(preprocessing.StandardScaler(), linear_model.SGDClassifier(loss='log', alpha=0, learning_rate='constant', eta0=0.001))\n\ndef show_ce_acc_graphs(graph_points):\n    for k, v in graph_points.items():\n        best_value = max(v.values()) if 'acc' in k else min(v.values())\n        best_index = np.argmax(list(v.values())) if 'acc' in k else np.argmin(list(v.values()))\n        color = 'red' if 'train' in k else 'blue'\n        fig = px.scatter(x=v.keys(), y=v.values(), title=f'{k}, best value: x={best_index + 1}, y={best_value}', color_discrete_sequence=[color])\n        fig.data[0].update(mode='markers+lines')\n        fig.show()\n\ndef plot_score_and_loss_by_split(_X, _t, model):\n    graph_points = { 'train_CE' : {} , 'valid_CE' : {} , 'train_acc' : {}, 'valid_acc' : {} }\n    for size in range(10, 100, 10):\n        X_train, X_val, t_train, t_val = model_selection.train_test_split(_X, _t, test_size=size\/100, random_state=42)\n        # create the classifier and predict the probabilities of the train and validation data\n        model_cls = model.fit(X_train, t_train)\n        y_train_prob = model_cls.predict_proba(X_train)\n        y_val_prob = model_cls.predict_proba(X_val)\n        graph_points['train_CE'][size\/100] = metrics.log_loss(t_train, y_train_prob)\n        graph_points['valid_CE'][size\/100] = metrics.log_loss(t_val, y_val_prob)\n        graph_points['train_acc'][size\/100] = model_cls.score(X_train, t_train)\n        graph_points['valid_acc'][size\/100] = model_cls.score(X_val, t_val)\n    show_ce_acc_graphs(graph_points)\n\nplot_score_and_loss_by_split(X, t, logistic_regression)","1bb482e0":"plot_score_and_loss_by_split(X, t, mlp)","39ce29e3":"TEST_SIZE = 0.3\n\n# get accuracy and CE loss for a praticular x, t pair\ndef print_ce_acc_for_x(_X, _t, model):\n    X_train, X_val, t_train, t_val = model_selection.train_test_split(_X, _t, test_size=TEST_SIZE, random_state=42)\n    # create the classifier and predict the probabilities of the train and validation data\n    model_cls = model.fit(X_train, t_train)\n    y_train_prob = model_cls.predict_proba(X_train)\n    y_val_prob = model_cls.predict_proba(X_val)\n    print('CE on train', metrics.log_loss(t_train, y_train_prob))\n    print('CE on validation', metrics.log_loss(t_val, y_val_prob))\n    print('Accuracy score on train', model_cls.score(X_train, t_train))\n    print('Accuracy score on validation', model_cls.score(X_val, t_val))\n    print('-------')\n\nprint_ce_acc_for_x(X, t, logistic_regression) # all features","43cfd1aa":"# testing for different subroups - dropping columns with less correlation to survival or high correlation between them\nprint_ce_acc_for_x(X.drop(['SibSp'], axis=1), t, logistic_regression)\nprint_ce_acc_for_x(X.drop(['Parch'], axis=1), t, logistic_regression)\nprint_ce_acc_for_x(X.drop(['SibSp', 'Parch'], axis=1), t, logistic_regression)\nprint()\nprint_ce_acc_for_x(X.drop(['AgeGroup'], axis=1), t, logistic_regression)\nprint_ce_acc_for_x(X.drop(['Age_Class'], axis=1), t, logistic_regression)\nprint_ce_acc_for_x(X.drop(['AgeGroup', 'Age_Class'], axis=1), t, logistic_regression)\nprint()\nprint_ce_acc_for_x(X.drop(['Age_Class', 'Parch'], axis=1), t, logistic_regression)\nprint_ce_acc_for_x(X.drop(['Age_Class', 'Parch', 'AgeGroup'], axis=1), t, logistic_regression)","db608540":"X = train_df_enc.drop(['AgeGroup'], axis=1)\nt = train_df[\"Survived\"].copy()\nX_test = test_df_enc.drop(['AgeGroup'], axis=1)\n\nclassifer = logistic_regression.fit(X, t)\nprediction = classifer.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId' : test_df['PassengerId'], 'Survived' : prediction})\noutput.to_csv('submission.csv',index=False)","ff755bcb":"![leaderboard](https:\/\/i.imgur.com\/bVnusgQ.png)\n\n\n![runs](https:\/\/i.imgur.com\/oLuVHqH.png)","f4cf510d":"We can see that the features most correlated with survival are `Sex`, `PClass` and `Deck`, let's try to analyze those better.","69f31716":"Same but for `MLP`:","1e786ae3":"## Imports and Definitions","50ce047d":"These results give us some idea what to focus on when trying to improve submission results. (I removed weaker results which I tested as they are less relevant for results comparing)","420e971d":"Observations:\n- Females survived at a better rate than men, and also in greater overall numbers.\n- First class passengers survived at a better rate and quantity than other classes, while having fewer passengers\n- Passengers that embarked from port `C` at a better rate of survival than other ports, but those from `S` port had the most survivors overall\n\nSeems like we don't have enough features with high correlation, so let's add some hyper-features.","8e8b65be":"#### Let's test MLP vs Logistic Regression for each possible 10% jump of temp_train \/ validation ratio:","3b22f168":"We can see that in `Cabin` we are missing 687 out of 891 values, so we will consider dropping it if we won't be able to extrapolate something meaninful for the valid values it has.\n\nWe can see that in `Age` we are missing 177 out of 891 values, so we will fill those with the columns mean.","04e7478c":"# Submission","c10f652b":"# Thoughts and summary\n- The hyper-features we added didn't help us much, looking at top scoring notebooks, there are smarter ways to create hyper-features, plus using data we ignored like `Ticket` and `Name`\n- `MLP` performed noticably worse than `Logistic-Regression` at submissions, this is known that in practice `MLP` is harder to train and use than `log-reg` for `Binary Classification`, even though theoretically it should not perform worse.\n- It was hard to see what will work on the submissions from testing different inputs for the validation tests since most of the results vary from run to run, perhaps there is a better way than my implementation.\n- There are a ton of approches for each step of the way!","13e0118c":"## Data Analysis\n\nLet's display the cleaned data, and analyze it.\nWe will encode the the data just to show to graphically better (in some specific graphs), this is just a temporary measure as later we will encode it differently specifically for the model.","d5dbeaac":"Adding `Age_class` was OK, but `AgeGroup` doesn't seem like it will help us.\n\nWe are now ready to move on to selecting and training our model.","6c8e7107":"## Data Investigation and Preprocessing\n\n---","5aaf1ece":"#  Training and Validation\n-----------------------------","ceea8a86":"**We can see that the `logistic regression` model gives a smaller best loss, so we will choose it and we will choose a point where the valid_ce ~ train_ce (0.3)**\n\nNow we will try to see which feature subgroups from the weaker ones will give us the best result.","3e651aaf":"### Before we analyze the data, let's first clean it up.","a1f6e2c9":"# Kaggle Results","c7a588d3":"# Sources Used:\n- https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial\n- https:\/\/towardsdatascience.com\/predicting-the-survival-of-titanic-passengers-30870ccc7e8\n- https:\/\/stackoverflow.com\/\n- https:\/\/scikit-learn.org\/stable\/user_guide.html\n- https:\/\/seaborn.pydata.org\/tutorial.html","2081d51a":"As we can see, `Ticket` which is not numerical has 681 unique values, and will be irrelavant or too difficult to categorize, so we can remove it.\nWe will also remove the `Name` column for the same reason.","3d9b4f2b":"Cabins are in the format of `<deck-letter><room-number>` we can transform the column into a category of `Deck` for better encoding, instead of flat out removing the entire column.","fffff07d":"### Before we select our model and train it, we ***encode*** the data so the model can process our data.","29d75a2f":"Elazar Fine\n\nhttps:\/\/www.kaggle.com\/elfein\n\nIn this competition we are given a dataset of passengers that were aboard the titanic when it drowned, with features for each passenger like `Sex`, `Age` `Pclass` etc and if they survived. \nWe are to predict (`Binary Classification`) wether or not other passengers which we are not told wether they survived or not, in fact did.\n\n\n"}}