{"cell_type":{"3da6dc7a":"code","85d06047":"code","64e6b9dc":"code","d9c62f02":"code","7dd6fb3c":"code","69f7d32e":"code","f1e5927f":"code","cadf35c3":"code","888666f8":"code","4c8776ec":"code","eced3b2e":"code","8e24d240":"code","214a1836":"code","0de19364":"code","d85feef5":"code","06682509":"code","39b62a50":"code","22c82126":"code","056f1b40":"code","3e2638dd":"code","8a78f1bc":"code","599c8aa6":"code","00b1641e":"markdown","aeb5a07e":"markdown","7d23ed07":"markdown","577d7659":"markdown","640d001f":"markdown","e5346071":"markdown","a5f2dd5f":"markdown","6798dd10":"markdown","cdd4a2e8":"markdown","85ff3c0a":"markdown","36ed7661":"markdown","178834b9":"markdown","018c8b15":"markdown","550fc24a":"markdown","6daccccd":"markdown","6f444f0e":"markdown","12b62944":"markdown","34a71707":"markdown","6d751103":"markdown","3d45950f":"markdown"},"source":{"3da6dc7a":"from plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport numpy as np\ninit_notebook_mode(connected=True)\n\n## generate random data\nN = 50\nrandom_x = np.linspace(2, 10, N)\nrandom_y1 = np.linspace(2, 10, N)\nrandom_y2 = np.linspace(2, 10, N)\n\ntrace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\", name=\"Actual Data\")\ntrace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\", name=\"Model\")\nlayout = go.Layout(title=\"2D Data Repersentation Space\", xaxis=dict(title=\"x2\", range=(0,12)), \n                   yaxis=dict(title=\"x1\", range=(0,12)), height=400, \n                   annotations=[dict(x=5, y=5, xref='x', yref='y', text='This 1D line is the Data Manifold (where data resides)',\n                   showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n                   ax=-120, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8)])\nfigure = go.Figure(data = [trace1], layout = layout)\niplot(figure)","85d06047":"random_y3 = [2 for i in range(100)]\nrandom_y4 = random_y2 + 1\ntrace4 = go.Scatter(x = random_x[4:24], y = random_y4[4:300], mode=\"lines\")\ntrace3 = go.Scatter(x = random_x, y = random_y3, mode=\"lines\")\ntrace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\")\ntrace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\")\nlayout = go.Layout(xaxis=dict(title=\"x1\", range=(0,12)), yaxis=dict(title=\"x2\", range=(0,12)), height=400,\n                   annotations=[dict(x=2, y=2, xref='x', yref='y', text='A', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, \n                                     arrowcolor='#636363', ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8), \n                                dict(x=6, y=6, xref='x', yref='y', text='B', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n                                     ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='yellow', opacity=0.8), dict(\n                                     x=4, y=5, xref='x', yref='y',text='d', ay=-40), \n                                dict(x=2, y=2, xref='x', yref='y', text='angle L', ax=80, ay=-10)], title=\"2D Data Representation Space\", showlegend=False)\ndata = [trace1, trace2, trace3, trace4]\nfigure = go.Figure(data = data, layout = layout)\niplot(figure)\n\n\n\n#################\n\nrandom_y3 = [2 for i in range(100)]\nrandom_y4 = random_y2 + 1\ntrace4 = go.Scatter(x = random_x[4:24], y = random_y4[4:300], mode=\"lines\")\ntrace3 = go.Scatter(x = random_x, y = random_y3, mode=\"lines\")\ntrace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\")\ntrace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\")\nlayout = go.Layout(xaxis=dict(title=\"u1\", range=(1.5,12)), yaxis=dict(title=\"u2\", range=(1.5,12)), height=400,\n                   annotations=[dict(x=2, y=2, xref='x', yref='y', text='A', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, \n                                     arrowcolor='#636363', ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8), \n                                dict(x=6, y=6, xref='x', yref='y', text='B', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n                                     ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='yellow', opacity=0.8), dict(\n                                     x=4, y=5, xref='x', yref='y',text='d', ay=-40), \n                                dict(x=2, y=2, xref='x', yref='y', text='angle L', ax=80, ay=-10)], title=\"Latent Distance View Space\", showlegend=False)\ndata = [trace1, trace2, trace3, trace4]\nfigure = go.Figure(data = data, layout = layout)\niplot(figure)","64e6b9dc":"import matplotlib.pyplot as plt \nimport numpy as np\nfs = 100 # sample rate \nf = 2 # the frequency of the signal\nx = np.arange(fs) # the points on the x axis for plotting\ny = [ np.sin(2*np.pi*f * (i\/fs)) for i in x]\n\n% matplotlib inline\nplt.figure(figsize=(15,4))\nplt.stem(x,y, 'r', );\nplt.plot(x,y);","d9c62f02":"## load the libraries \nfrom keras.layers import Dense, Input, Conv2D, MaxPool2D, UpSampling2D, SpatialDropout2D\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom imgaug import augmenters\nimport pandas as pd\nimport numpy as np","7dd6fb3c":"### read dataset \ntrain = pd.read_csv(\"..\/input\/fashion-mnist_train.csv\")\ntrain_x = train[list(train.columns)[1:]].values\ntrain_y = train['label'].values\n\n## normalize and reshape the predictors  \ntrain_x = train_x \/ 255\n\n## create train and validation datasets\ntrain_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2)\n\n## reshape the inputs\ntrain_x = train_x.reshape(-1, 784)\nval_x = val_x.reshape(-1, 784)","69f7d32e":"## input layer\ninput_layer = Input(shape=(784,))\n\n## encoding architecture\nencode_layer1 = Dense(600, activation='relu')(input_layer)\nencode_layer2 = Dense(300, activation='relu')(encode_layer1)\nencode_layer3 = Dense(128, activation='relu')(encode_layer2)\n\n## latent view\nlatent_view   = Dense(10, activation='sigmoid')(encode_layer3)\n\n## decoding architecture\ndecode_layer1 = Dense(128, activation='relu')(latent_view)\ndecode_layer2 = Dense(300, activation='relu')(decode_layer1)\ndecode_layer3 = Dense(600, activation='relu')(decode_layer2)\n\n## output layer\noutput_layer  = Dense(784)(decode_layer3)\n\nmodel = Model(input_layer, output_layer)","f1e5927f":"model.summary()","cadf35c3":"model.compile(optimizer='adam', loss='mse')\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, mode='auto')\nmodel.fit(train_x, train_x, epochs=2, batch_size=2048, validation_data=(val_x, val_x), callbacks=[early_stopping])","888666f8":"preds = model.predict(val_x)","4c8776ec":"from PIL import Image \nf, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5):\n    ax[i].imshow(val_x[i].reshape(28, 28))\nplt.show()","eced3b2e":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5):\n    ax[i].imshow(preds[i].reshape(28, 28))\nplt.show()","8e24d240":"# encoded_layer1 = SpatialDropout2D(0.3)(input_layer) # try with spatial dropout!","214a1836":"## recreate the train_x array and val_x array\ntrain_x = train[list(train.columns)[1:]].values\ntrain_x, val_x = train_test_split(train_x, test_size=0.2)\n\n## normalize and reshape\ntrain_x = train_x\/255.\nval_x = val_x\/255.","0de19364":"train_x = train_x.reshape(-1, 28, 28, 1)\nval_x = val_x.reshape(-1, 28, 28, 1)","d85feef5":"# Lets add sample noise - Salt and Pepper\nnoise = augmenters.SaltAndPepper(0.1)\nseq_object = augmenters.Sequential([noise])\n\ntrain_x_n = seq_object.augment_images(train_x)\nval_x_n = seq_object.augment_images(val_x)","06682509":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(train_x[i].reshape(28, 28))\nplt.show()","39b62a50":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(train_x_n[i].reshape(28, 28))\nplt.show()","22c82126":"### Original:\n\n\n# input layer\ninput_layer = Input(shape=(28, 28, 1))\n\n# encoding architecture\nencoded_layer1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\nencoded_layer1 = MaxPool2D( (2, 2), padding='same')(encoded_layer1)\nencoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded_layer1)\nencoded_layer2 = MaxPool2D( (2, 2), padding='same')(encoded_layer2)\nencoded_layer3 = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer2)\nlatent_view    = MaxPool2D( (2, 2), padding='same')(encoded_layer3)\n\n# decoding architecture\ndecoded_layer1 = Conv2D(16, (3, 3), activation='relu', padding='same')(latent_view)\ndecoded_layer1 = UpSampling2D((2, 2))(decoded_layer1)\ndecoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded_layer1)\ndecoded_layer2 = UpSampling2D((2, 2))(decoded_layer2)\ndecoded_layer3 = Conv2D(64, (3, 3), activation='relu')(decoded_layer2)\ndecoded_layer3 = UpSampling2D((2, 2))(decoded_layer3)\noutput_layer   = Conv2D(1, (3, 3), padding='same')(decoded_layer3)\n\n# compile the model\nmodel_2 = Model(input_layer, output_layer)\nmodel_2.compile(optimizer='adam', loss='mse')","056f1b40":"# ### Add spatial dropout\n\n# # input layer\n# input_layer = Input(shape=(28, 28, 1))\n\n# # encoding architecture\n# encoded_layer1 = SpatialDropout2D(0.3)(input_layer) # try with spatial dropout!\n# encoded_layer1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\n# encoded_layer1 = MaxPool2D( (2, 2), padding='same')(encoded_layer1)\n# encoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded_layer1)\n# encoded_layer2 = MaxPool2D( (2, 2), padding='same')(encoded_layer2)\n# encoded_layer3 = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer2)\n# latent_view    = MaxPool2D( (2, 2), padding='same')(encoded_layer3)\n\n# # decoding architecture\n# decoded_layer1 = Conv2D(16, (3, 3), activation='relu', padding='same')(latent_view)\n# decoded_layer1 = UpSampling2D((2, 2))(decoded_layer1)\n# decoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded_layer1)\n# decoded_layer2 = UpSampling2D((2, 2))(decoded_layer2)\n# decoded_layer3 = Conv2D(64, (3, 3), activation='relu')(decoded_layer2)\n# decoded_layer3 = UpSampling2D((2, 2))(decoded_layer3)\n# output_layer   = Conv2D(1, (3, 3), padding='same')(decoded_layer3)\n\n# # compile the model\n# model_2 = Model(input_layer, output_layer)\n# model_2.compile(optimizer='adam', loss='mse')","3e2638dd":"model_2.summary()","8a78f1bc":"early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=2, verbose=5, mode='auto')\nhistory = model_2.fit(train_x_n, train_x, epochs=10, batch_size=2048, validation_data=(val_x_n, val_x), callbacks=[early_stopping])","599c8aa6":"## Uncomment and run the predictions for the trained model\n\npreds = model_2.predict(val_x_n[:10])\nf, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(preds[i].reshape(28, 28))\nplt.show()","00b1641e":"**Predicted : Autoencoder Output**","aeb5a07e":"In this autoencoder network, we will add convolutional layers because convolutional networks works really well with the image inputs. To apply convolutions on image data, we will reshape our inputs in the form of 28 * 28 matrix. For more information related to CNN,  refer to my previous [kernel](https:\/\/www.kaggle.com\/shivamb\/a-very-comprehensive-tutorial-nn-cnn).  ","7d23ed07":"But the key question here is with what logic or rule, point B can be represented in terms of A and angle L. Or in other terms, what is the equation among B, A and L. The answer is straigtforward, there is no fixed equation but a best possible equation is obtained by the unsupervised learning process. In simple terms, the learning process can be defined as a rule \/ equation which converts B in the form of A and L. Lets understand this process from a autoencoder perspective. \n\nConsider the autoencoder with no hidden layers, the inputs x1 and x2 are encoded to lower repersentation d which is then further projected into x1 and x2. \n\n![](https:\/\/i.imgur.com\/lfq4eEy.png)\n\n<br>\n**Step1 : Repersent the points in Latent View Space**   \n\nIf the coordinates of point A and B in the data representation space are: \n\n- Point A : (x1A, x2A)  \n- Point B : (x1B, x2B)   \n\nthen their coordinates in the latent view space will be:   \n\n(x1A, x2A) ---> (0, 0)  \n(x1B, x2B) ---> (u1B, u2B)  \n\n- Point A : (0, 0)  \n- Point B : (u1B, u2B)   \n\nWhere u1B and u2B can be represented in the form of distance between the point and the reference point  \n\nu1B = x1B - x1A  \nu2B = x2B - x2A\n\n**Step2 : Represent the points with distance d and angle L **    \n\nNow, u1B and u2B can represented as a combination of distance d and angle L. And if we rotate this by angle L, towards the horizontal axis, L will become 0. ie.  \n\n**=> (d, L)**     \n**=> (d, 0)**   (after rotation)   \n\nThis is the output of the encoding process and repersents our data in low dimensions.  If we recall the fundamental equation of a neural network with weights and bias of every layer, then \n\n**=> (d, 0) = W. (u1B, u2B)**    \n==> (encoding)    \n\nwhere W is the weight matrix of hidden layer.  Since, we know that the decoding process is the mirror image of the encoding process. \n\n**=> (u1B, u2B) = Inverse (W) . (d, 0)**    \n==> (decoding)  \n\nThe reduced form of data (x1, x2) is (d, 0) in the latent view space which is obtained from the encoding architecture. Similarly, the decoding architecture converts back this representation to original form (u1B, u2B) and then (x1, x2). An important point is that Rules \/ Learning function \/ encoding-decoding equation will be different for different types of data. For example, consider the following data in 2dimentional space.  \n\n\n## Different Rules for Different data\n\nSame rules cannot be applied to all types of data. For example, in the previous example, we projected a linear data manifold in one dimention and eliminated the angle L. But what if the data manifold cannot be projected properly. For example consider the following data manifold view. ","577d7659":"Lets plot the original and predicted image\n\n**Inputs: Actual Images**","640d001f":"Lets now create the model architecture for the autoencoder. Lets understand what type of network needs to be created for this problem. \n\n**Encoding Architecture:**   \n\nThe encoding architure is composed of 3 Convolutional Layers and 3 Max Pooling Layers stacked one by one. Relu is used as the activation function in the convolution layers and padding is kept as \"same\". Role of max pooling layer is to downsample the image dimentions. This layer applies a max filter to non-overlapping subregions of the initial representation.  \n\n* Additionally, we add spatial dropout (drop pixels in the 2d featuremap) for feature robustness! (easier and more stable than adding salt and pepper noise)\n\n**Decoding Architecture:**   \n\nSimilarly in decoding architecture, the convolution layers will be used having same dimentions (in reverse manner) as the encoding architecture. But instead of 3 maxpooling layers, we will be adding 3 upsampling layers. Again the activation function will be same (relu), and padding in convolution layers will be same as well.  Role of upsampling layer is to upsample the dimentions of a input vector to a higher resolution \/ dimention. The max pooling operation is non-invertible, however an approximate inverse can be obtained by recording the locations of the maxima within each pooling region. Umsampling layers make use of this property to project the reconstructions from a low dimentional feature space.   \n\n","e5346071":"In this implementation, I have not traiened this network for longer epoochs, but for better predictions, you can train the network for larger number of epoochs say somewhere in the range of 500 - 1000. \n\n### Excellent References\n\n1. https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/unsupervised-deep-learning-computer-vision\/\n2. https:\/\/towardsdatascience.com\/applied-deep-learning-part-3-autoencoders-1c083af4d798\n3. https:\/\/blog.keras.io\/building-autoencoders-in-keras.html\n4. https:\/\/cs.stanford.edu\/people\/karpathy\/convnetjs\/demo\/autoencoder.html\n\n\nThanks for viewing the kernel, please upvote if you liked it. ","a5f2dd5f":"### Noisy Images \n\nWe can intentionally introduce the noise in an image. I am using imaug package which can be used to augment the images with different variations. One such variation can be introduction of noise. Different types of noises can be added to the images. For example: \n\n- Salt and Pepper Noise  \n- Gaussian Noise  \n- Periodic Noise  \n- Speckle Noise  \n\nLets introduce salt and pepper noise to our data which is also known as impulse noise. This noise introduces sharp and sudden disturbances in the image signal. It presents itself as sparsely occurring white and black pixels","6798dd10":"Before adding noise","cdd4a2e8":"To repersent this data, we are currently using 2 dimensions - X and Y. But it is possible to reduce the dimensions of this space into lower dimensions ie. 1D. If we can define following : \n\n- Reference Point on the line : A  \n- Angle L with a horizontal axis  \n\nthen any other point, say B, on line A can be repersented in terms of Distance \"d\" from A and angle L.  ","85ff3c0a":"In this type of data, the key problem will be to obtain the projection of data in single dimention without loosing information. When this type of data is projected in latent space, a lot of information is lost and it is almost impossible to deform and project it to the original shape. No matter how much shifts and rotation are applied, original data cannot be recovered. \n\nSo how does neural networks solves this problem ? The intution is, In the manifold space, deep neural networks has the property to bend the space in order to obtain a linear data fold view. Autoencoder architectures applies this property in their hidden layers which allows them to learn low level representations in the latent view space. \n\nThe following image describes this property: \n\n![](https:\/\/i.imgur.com\/gKCOdiL.png)\n\nLets implement an autoencoder using keras that first learns the features from an image, and then tries to project the same image as the output.  \n\n### 1. Load the required libraries\n","36ed7661":"Generate the predictions on validation data. ","178834b9":"### 2. Dataset Prepration \n\nLoad the dataset, separate predictors and target, normalize the inputs.","018c8b15":"Here is the model summary","550fc24a":"So we can see that an autoencoder trained with 20 epoochs is able to reconstruct the input images very well. Lets look at other use-case of autoencoders - Image denoising or removal of noise from the image.  \n\n## Image Denoising\n\nAutoencoders are pretty useful, lets look at another application of autoencoders - Image denoising. Many a times input images contain noise in the data, autoencoders can be used to get rid of those images. Lets see it in action. First lets prepare the train_x and val_x data contianing the image pixels. \n\n![](https:\/\/www.learnopencv.com\/wp-content\/uploads\/2017\/11\/denoising-autoencoder-600x299.jpg)","6daccccd":"Next, we will train the model with early stopping callback.\n\n* with original architecture (1,500 , 1000, 500):\n    * Epoch 20\/20  - loss: 0.0183 - val_loss: 0.0187\n* with new architecture (600 , 300, 128):\n    * Epoch 20\/20 - loss: 0.0198 - val_loss: 0.0195","6f444f0e":"* Fork from : https:\/\/www.kaggle.com\/shivamb\/how-autoencoders-work-intro-and-implementation\n* change hyperparams\n\n## How Autoencoders work - Understanding the math and implementation\n\n\nAutoencoders are a special type of neural network architectures in which the output is same as the input. Autoencoders are trained in an unsupervised manner in order to learn the exteremely low level repersentations of the input data. These low level features are then deformed back to project the actual data. An autoencoder is a regression task where the network is asked to predict its input (in other words, model the identity function). These networks has a tight bottleneck of a few neurons in the middle, forcing them to create effective representations that compress the input into a low-dimensional code that can be used by the decoder to reproduce the original input.\n\nA typical autoencoder architecture comprises of three main components: \n\n- **Encoding Architecture :** The encoder architecture comprises of series of layers with decreasing number of nodes and ultimately reduces to a latent view repersentation.  \n- **Latent View Repersentation :** Latent view repersents the lowest level space in which the inputs are reduced and information is preserved.  \n- **Decoding Architecture :** The decoding architecture is the mirro image of the encoding architecture but in which number of nodes in every layer increases and ultimately outputs the similar (almost) input.  \n\n![](https:\/\/i.imgur.com\/Rrmaise.png)\n\nA highly fine tuned autoencoder model should be able to reconstruct the same input which was passed in the first layer. In this kernel, I will walk you through the working of autoencoders and their implementation.  Autoencoders are widly used with the image data and some of their use cases are: \n\n- Dimentionality Reduction   \n- Image Compression   \n- Image Denoising   \n- Image Generation    \n- Feature Extraction  \n\n\n\n## How Autoencoders work \n\nLets understand the mathematics behind autoencoders. The main idea behind autoencoders is to learn a low level repersenation of a high level dimentional data. Lets try to understand the encoding process with an example.  Consider a data repersentation space (N dimentional space which is used to repersent the data) and consider the data points repersented by two variables : x1 and x2. Data Manifold is the space inside the data repersentation space in which the true data resides. ","12b62944":"After adding noise","34a71707":"Here is the summary of our autoencoder architecture.","6d751103":"Train the model with early stopping callback. Increase the number of epochs to a higher number for better results. \n\n* Default autoEnc settings with 10 epochs, 2 step early stopping gives:\n    * Epoch 10\/10  - loss: 0.0942 - val_loss: 0.0941","3d45950f":"### 3. Create Autoencoder architecture\n\nIn this section, lets create an autoencoder architecture. The encoding part comprises of three layers with 2000, 1200, and 500 nodes. Encoding architecture is connected to latent view space comprising of 10 nodes which is then connected to decoding architecture with 500, 1200, and 2000 nodes. The final layer comprises of exact number of nodes as the input layer."}}