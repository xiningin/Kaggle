{"cell_type":{"4f81cf2a":"code","96c594fd":"code","298ef6b3":"code","11769367":"code","df51f002":"code","121be254":"code","328ccc86":"code","81a9fa00":"code","fa10ff92":"code","74d61fc4":"code","0c37e8bc":"code","ce9d5c2b":"code","1c564238":"code","464d0c2b":"code","b3fb0226":"code","7d5813a5":"code","5f6b2947":"code","2cc4399b":"code","f211dde0":"code","0319eb3c":"code","b58f08d3":"code","d906f510":"code","03af0545":"code","4141f338":"code","a65f3852":"code","1474cf3b":"code","e0e9f881":"code","76cc63d0":"code","6dff19e8":"code","b5a4530a":"code","e217644a":"code","dbf0bc1b":"code","a310be50":"code","1c6c6832":"code","08982f8e":"code","7df84f0c":"code","17f95697":"code","eb3c86f7":"code","ee815235":"code","846aed2a":"code","f71919c7":"code","8fedf9ef":"code","7e080d38":"code","80cec26d":"code","1edf7076":"code","92601d11":"code","7a0c8d80":"code","0a1132fe":"code","74551588":"code","7d7f0b2f":"code","bd1559a0":"code","1b879471":"code","497eadc9":"code","611cbef6":"code","0fd161d9":"code","6e5f25bc":"code","dc584852":"code","6fdc98ec":"code","48f54d06":"code","ee8d08a0":"code","904bdda1":"code","d5235900":"code","4e476b2b":"code","880fa5bd":"code","0bf32370":"code","aeeea890":"code","a2600c30":"code","96276b38":"code","81030300":"code","d21df23b":"code","d3100199":"code","6d721a44":"markdown","f42e075a":"markdown","bf534861":"markdown","f2ed7b15":"markdown","b832ed25":"markdown","bc0a6090":"markdown","54202d99":"markdown","1644413c":"markdown","3599f349":"markdown","2d775c9f":"markdown","7e5c52d7":"markdown","d1411b11":"markdown","8bd50c1d":"markdown","ad1af29a":"markdown","9aab8728":"markdown","36e18ca2":"markdown","6917e1f9":"markdown","607d5228":"markdown","2ec1b2c1":"markdown","13475c15":"markdown","05d7cca0":"markdown","46f60878":"markdown","eba56955":"markdown"},"source":{"4f81cf2a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score,train_test_split\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer,SnowballStemmer\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nimport re\nimport os\nimport sqlite3\nimport time\nfrom sklearn.manifold import TSNE\nimport matplotlib.patches as mpatches\nfrom sklearn.model_selection import cross_val_score,train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nimport datetime\nfrom sklearn.metrics import confusion_matrix,f1_score,precision_recall_curve,precision_score,recall_score,roc_auc_score,roc_curve,accuracy_score,classification_report\n\nimport scikitplot.metrics as skplt\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","96c594fd":"def LoadData(filename,FullDataLoad=False):\n    path=os.path.join('\/kaggle\/input','amazon-fine-food-reviews',filename)\n    Connection=sqlite3.connect(path)\n    if not FullDataLoad:\n        return pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score !=3 LIMIT 5000\"\"\",Connection),Connection\n    return pd.read_sql_query(\"\"\"SELECT * FROM Reviews WHERE Score !=3\"\"\",Connection),Connection    ","298ef6b3":"Dataset_Name='database.sqlite'\nAmazon_Food_Reviews,Table_Connection = LoadData(Dataset_Name,FullDataLoad=False)\nAmazon_Food_Reviews['Score']= Amazon_Food_Reviews['Score'].apply(lambda x: 1 if x>3 else 0)","11769367":"Amazon_Food_Reviews.head()","df51f002":"Amazon_Food_Reviews_Sorted_Data= Amazon_Food_Reviews.sort_values('ProductId',axis=0, inplace=False, kind='quicksort', na_position='last')\nAmazon_Food_Reviews_Duplicates_Dropped= Amazon_Food_Reviews_Sorted_Data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep='first',inplace=False)\nprint(\"{:07.3f} % of data removed during duplicates removal operation\".format((1-(Amazon_Food_Reviews_Duplicates_Dropped.shape[0])\/(Amazon_Food_Reviews_Sorted_Data.shape[0]))*100))","121be254":"Amazon_Food_Reviews_Preprocessed=Amazon_Food_Reviews_Duplicates_Dropped[Amazon_Food_Reviews_Duplicates_Dropped.HelpfulnessNumerator<=Amazon_Food_Reviews_Duplicates_Dropped.HelpfulnessDenominator]\nprint(\"{:07.3f} % of data removed\".format(np.round((1-(Amazon_Food_Reviews_Preprocessed.shape[0])\/(Amazon_Food_Reviews_Duplicates_Dropped.shape[0]))*100)))","328ccc86":"def DeContraction(document):\n    document= re.sub(r\"won\\'t\",\"will not\",document)\n    document= re.sub(r\"can\\'t\",\"can not\",document)\n    document= re.sub(r\"\\'t\",\"not\",document)\n    document= re.sub(r\"\\'m\",\"am\",document)\n    document= re.sub(r\"\\'re\",\"are\",document)\n    document= re.sub(r\"\\'ve\",\"have\",document)\n    document= re.sub(r\"\\'d\",\"would\",document)\n    document= re.sub(r\"\\'s\",\"is\",document)\n    document= re.sub(r\"\\'ll\",\"will\",document)\n    return document","81a9fa00":"Custom_StopWords = [word for word in stopwords.words('english') if word not in ['no','nor','not']]","fa10ff92":"from tqdm import tqdm\ndef Preprocessing(Text_Data):\n    Preprocessed_Text =[]\n    for document in tqdm(Text_Data):\n        document= re.sub(r\"http\\S+\",\"\",document)\n        document = BeautifulSoup(document,'lxml').get_text()\n        document=DeContraction(document)\n        document = re.sub(r\"\\S*\\d\\S*\",\"\",document).strip()\n        document = re.sub(\"[^a-zA-Z]+\",' ',document)\n        document = ' '.join(word.lower() for word in document.split() if word.lower() not in Custom_StopWords)\n        Preprocessed_Text.append(document)\n    return Preprocessed_Text","74d61fc4":"Preprocessed_Amazon_Reviews = Preprocessing(Amazon_Food_Reviews_Preprocessed['Text'].values)\nnp.random.seed(42)\nprint(\"{} :{}\".format(np.random.randint(Amazon_Food_Reviews_Preprocessed.shape[0]),Preprocessed_Amazon_Reviews[np.random.randint(Amazon_Food_Reviews_Preprocessed.shape[0])]))","0c37e8bc":"from sklearn.feature_extraction.text import CountVectorizer\ncnt_vectorizer= CountVectorizer()\nCount_Vectorized_Reviews=cnt_vectorizer.fit_transform(Preprocessed_Amazon_Reviews)\nprint(\"Type of Output:{},Size:{}\".format(type(Count_Vectorized_Reviews),Count_Vectorized_Reviews.get_shape()))","ce9d5c2b":"#Count Vector With Bi-Grams,Words considering whose frequency greater than 10\nCnt_Vectorizer_Bigrams=CountVectorizer(ngram_range=(1,2),min_df=10)\nCount_Vectorized_Reviews_Bigram = Cnt_Vectorizer_Bigrams.fit_transform(Preprocessed_Amazon_Reviews)\nprint(\"Type of Output:{},Size:{}\".format(type(Count_Vectorized_Reviews_Bigram),Count_Vectorized_Reviews_Bigram.get_shape()))","1c564238":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer_bigram = TfidfVectorizer(ngram_range=(1,2),min_df=10)\nTFIDF_Vectorized_Reviews_Bigram=tfidf_vectorizer_bigram.fit_transform(Preprocessed_Amazon_Reviews)\nprint(\"Type of Output:{},Size:{}\".format(type(TFIDF_Vectorized_Reviews_Bigram),TFIDF_Vectorized_Reviews_Bigram.get_shape()))","464d0c2b":"List_Of_document=[]\nfor document in Preprocessed_Amazon_Reviews:\n    List_Of_document.append(document.split())","b3fb0226":"from gensim.models import Word2Vec\ndef Train_Word2Vec(List_Of_document,want_to_train_own_word2vec_model=True):\n    if want_to_train_own_word2vec_model:\n        return Word2Vec(List_Of_document,min_count=5,size=128,workers=os.cpu_count()-2)","7d5813a5":"Word2Vec_Model = Train_Word2Vec(List_Of_document)\nw2v_words=list(Word2Vec_Model.wv.vocab)","5f6b2947":"def Weighted_Word2Vec(Word2Vec_Model,List_Of_document):\n    Documents_Vector=[]\n    for document in tqdm(List_Of_document):\n        Document_Vector=np.zeros(128)\n        count_words=0\n        for word in document:\n            if word in w2v_words:\n                word_vector=Word2Vec_Model.wv[word]\n                Document_Vector+=word_vector\n                count_words+=1\n        if count_words!=0:\n            Document_Vector\/=count_words\n        Documents_Vector.append(Document_Vector)\n    return Documents_Vector","2cc4399b":"Weighted_Word2Vec_Vectorization=Weighted_Word2Vec(Word2Vec_Model,List_Of_document)","f211dde0":"tfidf_model =TfidfVectorizer()\ntfidf_model.fit(Preprocessed_Amazon_Reviews)\nIDF_Dictionary= dict(zip(tfidf_model.get_feature_names(),tfidf_model.idf_))","0319eb3c":"def TFIDF_WORD2VEC(tfidf_model,IDF_Dictionary,List_Of_document):\n    TFIDF_FEATURES = tfidf_model.get_feature_names()\n    TFIDF_Documents_Vector=[]\n    row=0\n    for Document in tqdm(List_Of_document):\n        Document_Vector=np.zeros(128)\n        weighted_sum=0\n        for word in Document:\n            if word in w2v_words and word in TFIDF_FEATURES:\n                word_vector=Word2Vec_Model.wv[word]\n                word_tfidf= IDF_Dictionary[word]*(Document.count(word)\/len(Document))\n                Document_Vector+=word_vector*word_tfidf\n                weighted_sum+=word_tfidf\n        if weighted_sum!=0:\n            Document_Vector\/=weighted_sum\n        TFIDF_Documents_Vector.append(Document_Vector)\n        row+=1\n    return TFIDF_Documents_Vector","b58f08d3":"TFIDF_Word2Vec_Vectorization=TFIDF_WORD2VEC(tfidf_model,IDF_Dictionary,List_Of_document)","d906f510":"import time\nos.mkdir('\/kaggle\/working\/Visualization\/')\ndef Visualization(Dataset,Score,Title,no_of_dims=2,perplexity=50,learning_rate=200):\n    plt.figure(figsize=(12,10))\n    TSNE_Model = TSNE(n_components=no_of_dims,perplexity=perplexity,learning_rate=learning_rate,n_jobs=-1,n_iter=5000)\n    start_time=time.time()\n    X_TSNE=TSNE_Model.fit_transform(Dataset)\n    stop_time=time.time()\n    Data = np.hstack((X_TSNE,Score.reshape(-1,1)))\n    TSNE_DF=pd.DataFrame(Data,columns=['Dimension_x','Dimension_y','Score'])\n    colors = {0:'red', 1:'blue'}\n    plt.scatter(TSNE_DF['Dimension_x'], TSNE_DF['Dimension_y'], c=TSNE_DF['Score'].apply(lambda x: colors[x]))\n    plt.title(Title)\n    green_patch = mpatches.Patch(color='green', label='perplexity :'+str(perplexity))\n    black_patch = mpatches.Patch(color='black', label='learning rate :'+str(learning_rate))\n    plt.xlabel('Dimension_x')\n    plt.ylabel('Dimension_y')\n    plt.legend(handles=[green_patch, black_patch])\n    Image = os.path.join('\/kaggle\/working\/Visualization\/',Title+'n_components'+'_'+str(no_of_dims)+'_'+'perplexity'+'_'+str(perplexity)+'_'+'learning_rate'+'_'+str(learning_rate)+'.png')\n    plt.savefig(fname =Image,dpi=500,format='png')\n    Total_Time=(stop_time-start_time)\/60\n    print('Execution Time:{}'.format(Total_Time))","03af0545":"Visualization(Dataset=Count_Vectorized_Reviews.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='Count Vectoization TSNE',no_of_dims=2,perplexity=10,learning_rate=200)","4141f338":"Visualization(Dataset=Count_Vectorized_Reviews.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='Count Vectoization TSNE',no_of_dims=2,perplexity=30,learning_rate=200)","a65f3852":"Visualization(Dataset=Count_Vectorized_Reviews.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='Count Vectoization TSNE',no_of_dims=2,perplexity=20,learning_rate=200)","1474cf3b":"Visualization(Dataset=Count_Vectorized_Reviews.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='Count Vectoization TSNE',no_of_dims=2,perplexity=40,learning_rate=200)","e0e9f881":"Visualization(Dataset=Count_Vectorized_Reviews.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='Count Vectoization TSNE')","76cc63d0":"Visualization(Dataset=Count_Vectorized_Reviews.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='Count Vectoization TSNE',no_of_dims=2,perplexity=100,learning_rate=200)","6dff19e8":"Visualization(Dataset=Count_Vectorized_Reviews_Bigram.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='BiGRAM Count Vectoization TSNE',no_of_dims=2,perplexity=10,learning_rate=200)","b5a4530a":"Visualization(Dataset=Count_Vectorized_Reviews_Bigram.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='BiGRAM Count Vectoization TSNE',no_of_dims=2,perplexity=20,learning_rate=200)","e217644a":"Visualization(Dataset=Count_Vectorized_Reviews_Bigram.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='BiGRAM Count Vectoization TSNE',no_of_dims=2,perplexity=30,learning_rate=200)","dbf0bc1b":"Visualization(Dataset=Count_Vectorized_Reviews_Bigram.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='BiGRAM Count Vectoization TSNE',no_of_dims=2,perplexity=40,learning_rate=200)","a310be50":"Visualization(Dataset=Count_Vectorized_Reviews_Bigram.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='BiGRAM Count Vectoization TSNE')","1c6c6832":"Visualization(Dataset=Count_Vectorized_Reviews_Bigram.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='BiGRAM Count Vectoization TSNE',no_of_dims=2,perplexity=100,learning_rate=200)","08982f8e":"Visualization(Dataset=TFIDF_Vectorized_Reviews_Bigram.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='BiGRAM TFIDF Vectoization TSNE',no_of_dims=2,perplexity=10,learning_rate=200)","7df84f0c":"Visualization(Dataset=TFIDF_Vectorized_Reviews_Bigram.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='BiGRAM TFIDF Vectoization TSNE',no_of_dims=2,perplexity=20,learning_rate=200)","17f95697":"Visualization(Dataset=TFIDF_Vectorized_Reviews_Bigram.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='BiGRAM TFIDF Vectoization TSNE',no_of_dims=2,perplexity=30,learning_rate=200)","eb3c86f7":"Visualization(Dataset=TFIDF_Vectorized_Reviews_Bigram.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='BiGRAM TFIDF Vectoization TSNE',no_of_dims=2,perplexity=40,learning_rate=200)","ee815235":"Visualization(Dataset=TFIDF_Vectorized_Reviews_Bigram.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='BiGRAM TFIDF Vectoization TSNE')","846aed2a":"Visualization(Dataset=TFIDF_Vectorized_Reviews_Bigram.toarray(),Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='BiGRAM TFIDF Vectoization TSNE',no_of_dims=2,perplexity=100,learning_rate=200)","f71919c7":"Visualization(Dataset=Weighted_Word2Vec_Vectorization,Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='AVG WORD2VEC Vectoization TSNE',no_of_dims=2,perplexity=10,learning_rate=200)","8fedf9ef":"Visualization(Dataset=Weighted_Word2Vec_Vectorization,Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='AVG WORD2VEC Vectoization TSNE',no_of_dims=2,perplexity=20,learning_rate=200)","7e080d38":"Visualization(Dataset=Weighted_Word2Vec_Vectorization,Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='AVG WORD2VEC Vectoization TSNE',no_of_dims=2,perplexity=30,learning_rate=200)","80cec26d":"Visualization(Dataset=Weighted_Word2Vec_Vectorization,Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='AVG WORD2VEC Vectoization TSNE',no_of_dims=2,perplexity=40,learning_rate=200)","1edf7076":"Visualization(Dataset=Weighted_Word2Vec_Vectorization,Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='AVG WORD2VEC Vectoization TSNE')","92601d11":"Visualization(Dataset=Weighted_Word2Vec_Vectorization,Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='AVG WORD2VEC Vectoization TSNE',no_of_dims=2,perplexity=100,learning_rate=200)","7a0c8d80":"Visualization(Dataset=TFIDF_Word2Vec_Vectorization,Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='TFIDF WORD2VEC Vectoization TSNE',no_of_dims=2,perplexity=10,learning_rate=200)","0a1132fe":"Visualization(Dataset=TFIDF_Word2Vec_Vectorization,Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='TFIDF WORD2VEC Vectoization TSNE',no_of_dims=2,perplexity=20,learning_rate=200)","74551588":"Visualization(Dataset=TFIDF_Word2Vec_Vectorization,Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='TFIDF WORD2VEC Vectoization TSNE',no_of_dims=2,perplexity=30,learning_rate=200)","7d7f0b2f":"Visualization(Dataset=TFIDF_Word2Vec_Vectorization,Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='TFIDF WORD2VEC Vectoization TSNE',no_of_dims=2,perplexity=40,learning_rate=200)","bd1559a0":"Visualization(Dataset=TFIDF_Word2Vec_Vectorization,Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='TFIDF WORD2VEC Vectoization TSNE')","1b879471":"Visualization(Dataset=TFIDF_Word2Vec_Vectorization,Score=Amazon_Food_Reviews_Preprocessed['Score'].values,Title='TFIDF WORD2VEC Vectoization TSNE',no_of_dims=2,perplexity=100,learning_rate=200)","497eadc9":"def Cleaning_Pipeline(filename,FullDataLoad=False):\n    Amazon_Food_Reviews,Table_Connection = LoadData(Dataset_Name,FullDataLoad=FullDataLoad)\n    Amazon_Food_Reviews['Score']= Amazon_Food_Reviews['Score'].apply(lambda x: 1 if x>3 else 0)\n    print(Amazon_Food_Reviews.head())\n    \n    Amazon_Food_Reviews_Sorted_Data= Amazon_Food_Reviews.sort_values('ProductId',axis=0, inplace=False, kind='quicksort', na_position='last')\n    Amazon_Food_Reviews_Duplicates_Dropped= Amazon_Food_Reviews_Sorted_Data.drop_duplicates(subset={\"UserId\",\"ProfileName\",\"Time\",\"Text\"},keep='first',inplace=False)\n    print(\"{:07.3f} % of data removed during duplicates removal operation\".format((1-(Amazon_Food_Reviews_Duplicates_Dropped.shape[0])\/(Amazon_Food_Reviews_Sorted_Data.shape[0]))*100))\n    Amazon_Food_Reviews_Preprocessed=Amazon_Food_Reviews_Duplicates_Dropped[Amazon_Food_Reviews_Duplicates_Dropped.HelpfulnessNumerator<=Amazon_Food_Reviews_Duplicates_Dropped.HelpfulnessDenominator]\n    print(\"{:07.3f} % of data removed\".format(np.round((1-(Amazon_Food_Reviews_Preprocessed.shape[0])\/(Amazon_Food_Reviews_Duplicates_Dropped.shape[0]))*100)))\n    return Amazon_Food_Reviews_Preprocessed","611cbef6":"Dataset_Name='database.sqlite'\n#cleaning 5000 records\nAmazon_Food_Reviews_Preprocessed= Cleaning_Pipeline(Dataset_Name,FullDataLoad=True)","0fd161d9":"def Balanced_Data(Total_Dataset):\n    return pd.concat([Total_Dataset[Total_Dataset['Score']==1].sample(frac=0.035,random_state=1),Total_Dataset[Total_Dataset['Score']==0].sample(frac=0.15,random_state=1)],axis=0)","6e5f25bc":"Balanced_Amazon_Data_Preprocessed=Balanced_Data(Total_Dataset=Amazon_Food_Reviews_Preprocessed)","dc584852":"Balanced_Data_Time_Sorted=Balanced_Amazon_Data_Preprocessed.sort_values('Time',axis=0,kind='quicksort',ascending=True).reset_index(drop=True)","6fdc98ec":"Balanced_Data_Time_Sorted['Score'].value_counts().plot(kind='bar')","48f54d06":"def Find_Optimal_K_In_KNN(X_Train,Y_Train,K_Values_List):\n    #creating odd list of K values\n    neighbors= list(filter(lambda k: k%2!=0,K_Values_List))\n    CV_Scores=[]\n    for k in tqdm(neighbors):\n        knn = KNeighborsClassifier(n_neighbors=k,n_jobs=-1)\n        scores = cross_val_score(knn,X_Train,Y_Train,cv=10,scoring='accuracy',n_jobs=-1)\n        CV_Scores.append(scores.mean())\n    MSE = [1-x for x in CV_Scores]\n    Optimal_K = neighbors[MSE.index(min(MSE))]\n    print('\\nThe optimal number of neighbors is {}'.format(Optimal_K))\n    plt.figure(figsize=(12,10))\n    plt.plot(neighbors,MSE,color='blue', linestyle='dashed',marker='o',markerfacecolor='red', markersize=10)\n    plt.title('Error Rate vs. K Value')\n    plt.xlabel('K')\n    plt.ylabel('Error Rate')\n    print(\"the misclassification error for each k value is : \", np.round(MSE,3))\n    return Optimal_K","ee8d08a0":"X_Train,X_Test,Y_Train,Y_Test = train_test_split(Balanced_Data_Time_Sorted['Text'],Balanced_Data_Time_Sorted['Score'],test_size=0.25)","904bdda1":"from sklearn.preprocessing import StandardScaler\n\ncnt_vect = CountVectorizer(ngram_range=(1,2),min_df=10,max_features=5000)\nX_Train_Features=cnt_vect.fit_transform(X_Train)\nX_Test_Features = cnt_vect.transform(X_Test)","d5235900":"K_Neighbours_List = list(range(0,20))\nOptimal_K = Find_Optimal_K_In_KNN(X_Train_Features.toarray() ,Y_Train,K_Neighbours_List)","4e476b2b":"knn = KNeighborsClassifier(n_neighbors=Optimal_K)\nknn.fit(X_Train_Features, Y_Train)\nkn_pred = knn.predict(X_Test_Features)","880fa5bd":"\nknn_confusion_matrix=confusion_matrix(Y_Test,kn_pred)\nskplt.plot_confusion_matrix(Y_Test,kn_pred)","0bf32370":"from sklearn.metrics import accuracy_score,classification_report\nprint(accuracy_score(Y_Test,kn_pred))","aeeea890":"print(classification_report(Y_Test,kn_pred))","a2600c30":"from sklearn.feature_extraction.text import TfidfVectorizer\n#X_Train,X_Test,Y_Train,Y_Test = train_test_split(Balanced_Data_Time_Sorted['Text'],Balanced_Data_Time_Sorted['Score'],test_size=0.25)\nTFIDF_vect = TfidfVectorizer(ngram_range=(1,2),min_df=10,max_features=5000)\nX_Train_Features=TFIDF_vect.fit_transform(X_Train)\nX_Test_Features = TFIDF_vect.transform(X_Test)","96276b38":"K_Neighbours_List = list(range(0,20))\nOptimal_K = Find_Optimal_K_In_KNN(X_Train_Features.toarray() ,Y_Train,K_Neighbours_List)\n","81030300":"knn = KNeighborsClassifier(n_neighbors=Optimal_K)\nknn.fit(X_Train_Features, Y_Train)\nknn_TFIDF_pred = knn.predict(X_Test_Features)","d21df23b":"\nknn_confusion_matrix=confusion_matrix(Y_Test,knn_TFIDF_pred)\nskplt.plot_confusion_matrix(Y_Test,knn_TFIDF_pred)\nprint(accuracy_score(Y_Test,knn_TFIDF_pred))","d3100199":"print(classification_report(Y_Test,knn_TFIDF_pred))","6d721a44":"## [4.2] Bi-Grams and n-Grams.","f42e075a":"## [5.1] COUNT VECTORIZATION","bf534861":"<b> Conclusions:- <\/b>\n\n1. AS none of TSNE representation gives a well separated both +ve and -ve reviews.\n2. We can not simply draw a plane to separate -ve and +ve reviews. Although, By looking at only visual representation of data \n    we can not take decision whether to draw a plane or not.\n3. We will have some alternative method by that we will look at into this problem like how we can separate -ve and +ve reviews.","f2ed7b15":"## [5.2] BIGRAM COUNT VECTORIZATION ","b832ed25":"## [4.1] BAG OF WORDS","bc0a6090":"# [4] FEATURIZATION OF ENTIRE DATASET","54202d99":"## [5.4.2] TFIDF WEIGHTED WORD2VEC VECTORIZATION ","1644413c":"## [6.1] K-NEAREST NEIGHBOURS","3599f349":"## [6.1.1] TFIDF VECTORIZATION","2d775c9f":"### [4.4.1] AVG WORD2VEC","7e5c52d7":"## [4.4] Word2Vec","d1411b11":"# [6] MODELLING","8bd50c1d":"## TIME BASED SPLITTING","ad1af29a":"### [4.4.2] TFIDF WORD2VEC","9aab8728":"<b>Observation:-<\/b> This plot also looks like the bow, tfidf and avg word2vec.Both +ve and -ve reviwes are not well seperated they overlapped each other.","36e18ca2":"# [2].LOADING DATA & EDA","6917e1f9":"## [5.4.1] AVG WEIGHTED WORD2VEC VECTORIZATION ","607d5228":"## [4.3] TF-IDF","2ec1b2c1":"# [ 5] VISUALIZATION","13475c15":"## [6.1.1] COUNT VECTORIZATION","05d7cca0":"# [1].LOADING LIBRARIES","46f60878":"# [3].  Text Preprocessing.\n\nNow that we have finished deduplication our data requires some preprocessing before we go on further with analysis and making the prediction model.\n\nHence in the Preprocessing phase we do the following in the order below:-\n\n1. Begin by removing the html tags\n2. Remove any punctuations or limited set of special characters like , or . or # etc.\n3. Check if the word is made up of english letters and is not alpha-numeric\n4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n5. Convert the word to lowercase\n6. Remove Stopwords\n7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)<br>\n\nAfter which we collect the words used to describe positive and negative reviews","eba56955":"## [5.3] TFIDF BIGRAM VECTORIZATION "}}