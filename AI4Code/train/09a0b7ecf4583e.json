{"cell_type":{"42262591":"code","93665072":"code","31dd0c57":"code","bb4e4e01":"code","abefa4cf":"code","21a9ff6f":"code","2c232d0f":"code","1e574726":"code","dd51926f":"code","fcc5ab97":"code","483a221e":"code","a6988d3d":"code","20af1c73":"code","7578660d":"code","f0575df8":"code","5dd7f99f":"code","14b9f7ea":"code","f72be39d":"code","fb9aee85":"code","629ffe8b":"code","7ca005ae":"code","19e6f582":"code","0445a410":"code","1947f676":"code","3c6d5c07":"code","fbebebab":"code","f8b014ba":"code","7e62dc7d":"code","4fdc516f":"code","abc29c60":"code","20bf2778":"code","74749a68":"code","c3fefecc":"code","b2f6cc86":"code","261b70ec":"code","e9c834ce":"code","b3dbaf53":"code","e2107642":"code","0890fe93":"code","6dcd8417":"code","76ab5b50":"code","44264598":"code","e53a96b6":"markdown","ba16e22e":"markdown","51beb6d5":"markdown","e26d0d9b":"markdown","57ae17de":"markdown","61c40f05":"markdown","088743fb":"markdown","b307ec27":"markdown","3ec18e9c":"markdown","4bb5a38d":"markdown","e50be650":"markdown","11007d62":"markdown","ad83d45b":"markdown","87112443":"markdown"},"source":{"42262591":"#Import the neccesary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nsns.set()\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm","93665072":"path = '\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n\ndf = pd.read_csv(path)","31dd0c57":"df.head()","bb4e4e01":"df.describe(include = 'all')","abefa4cf":"df.info()","21a9ff6f":"#Value count of the column\ndf['TotalCharges'].value_counts()","2c232d0f":"#Replacing the empty value with zero \ndf['TotalCharges'].replace(' ', 0, inplace = True)","1e574726":"df[df['TotalCharges'].apply (lambda x: x== ' ')]","dd51926f":"#Changing the column datatype to float\ndf['TotalCharges'] = df['TotalCharges'].astype('float')","fcc5ab97":"sns.distplot(df['tenure']) #This distribution plot appears to be normal with no outlier","483a221e":"sns.distplot(df['MonthlyCharges']) #This distribution plot appears to be normal with no outlier","a6988d3d":"sns.distplot(df['TotalCharges'])#This distribution plot appears to be having a few outliers. Let's explore it further","20af1c73":"df.describe()","7578660d":"#Selecting Totalcharges above 8500 to see if they are outliers\ndf[df['TotalCharges'].apply (lambda x: x > 8500)]\n#Upon further exploration they are not outliers","f0575df8":"#Getting Variables in our dataframe\ndf.columns.values","5dd7f99f":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\n# the target column (in this case 'churn') should not be included in variables\n#Categorical variables already turned into dummy indicator may or maynot be added if any\nvariable = df[['tenure', 'MonthlyCharges','TotalCharges',]]\nX = add_constant(variable)\nvif = pd.DataFrame()\nvif['VIF']  = [variance_inflation_factor(X.values, i) for i in range (X.shape[1])]\nvif['features'] = X.columns\n\nvif\n#Using 10 as the minimum vif values i.e any independent variable 10 and above will have to be dropped\n#From the results all independent variable are below 10","14b9f7ea":"#Selecting the variable\nscale_int = df[['MonthlyCharges']]\n\nscaler = StandardScaler()#Selecting the standardscaler\nscaler.fit(scale_int)#fitting our independent variables","f72be39d":"df['scaled_monthly']= scaler.transform(scale_int)#scaling","fb9aee85":"scale_int = df[['tenure']] #Selecting the variable\n\nscaler = StandardScaler()#Selecting the standardscaler\nscaler.fit(scale_int)#fitting our independent variables","629ffe8b":"df['scaled_tenure']= scaler.transform(scale_int)#scaling","7ca005ae":"scale_int = df[['tenure']] #Selecting the variable\n\nscaler = StandardScaler()#Selecting the standardscaler\nscaler.fit(scale_int)#fitting our independent variables","19e6f582":"df['scaled_charges']= scaler.transform(scale_int)#scaling","0445a410":"df.describe()# Checking our scaled results","1947f676":"df.describe(include = 'all')","3c6d5c07":"#Dropping columns not needed\ndf.drop(['tenure','MonthlyCharges','customerID', 'TotalCharges'], axis = 1, inplace = True)","fbebebab":"#Turning Churn to a dummy indicator with 1 standing yes and 0 standing for no\ndf['Churn'] = df['Churn'].map({'Yes':1, 'No':0})","f8b014ba":"#Variables in our dataframe\ndf.columns.values","7e62dc7d":"#new dataframe with dummies\ndf_dummies = pd.get_dummies(df, drop_first = True)\n\ndf_dummies","4fdc516f":"#Declaring independent variable i.e x\n#Declaring Target variable i.e y\nx = df_dummies.drop('Churn', axis = 1)\ny = df_dummies['Churn']","abc29c60":"#Splitting our data into train and test dataframe\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 24)","20bf2778":"reg = LogisticRegression() #Selecting the model\nreg.fit(x_train, y_train) #training the model with x_train and y_train","74749a68":"#Predicting with our already trained model using x_test\ny_hat = reg.predict(x_test)","c3fefecc":"#Getting the accuracy of our model\nacc = metrics.accuracy_score(y_hat, y_test)\nacc","b2f6cc86":"#The intercept for our regression\nreg.intercept_","261b70ec":"#Coefficient for all our variables\nreg.coef_","e9c834ce":"cm = confusion_matrix(y_hat,y_test)\ncm","b3dbaf53":"# Format for easier understanding\ncm_df = pd.DataFrame(cm)\ncm_df.columns = ['Predicted 0','Predicted 1']\ncm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\ncm_df","e2107642":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier # for K nearest neighbours\nfrom sklearn import svm #for Support Vector Machine (SVM) ","0890fe93":"dt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ny1 = dt.predict(x_test)\nacc1 = metrics.accuracy_score(y1, y_test)\nacc1","6dcd8417":"kk = KNeighborsClassifier()\nkk.fit(x_train,y_train)\ny2 = kk.predict(x_test)\nacc2 = metrics.accuracy_score(y2, y_test)\nacc2","76ab5b50":"sv = svm.SVC()\nsv.fit(x_train,y_train)\ny3 = sv.predict(x_test)\nacc3 = metrics.accuracy_score(y3, y_test)\nacc3","44264598":"result = pd.DataFrame(data = x.columns.values, columns = ['features'] )\nresult['weight'] = np.transpose(reg.coef_)\nresult['odds'] = np.exp(np.transpose(reg.coef_))\n\nresult","e53a96b6":"### LOGISTIC REGRESSION","ba16e22e":"### Standardization\n\n#### Standardizing helps to give our independent varibles a more standard and relatable numeric scale, it also helps in improving model accuracy\n#### We are going to standardize only our numerical variables then use new columns to hold the resulting values","51beb6d5":"#### After comparison with some other model  logistic regression gave us the best accuracy with ~81.8% followed closely by svm model with ~81.7%","e26d0d9b":"### INTRODUCTION\n\n#### Predicting the customer churn mainly through logistic regression\n#### churn class was classified into two categories  No(0) and Yes(1)\n#### Steps taken in preprocessing includes Data cleaning, Standardization etc\n#### Other models where used to compare accuracy\n\n### SIDE NOTE\n#### You can leave your question about any unclear part in the comment section\n#### Any correction will be highly welcomed","57ae17de":"### CHECKING OLS ASSUMPTIONS\n\n#### Let's check that our dataset are not violating any of this assumptions which includes:\n#### 1. No Endogeneity\n#### 2. Normality and Homoscedasticity\n#### 3.No Autocorrelation\n#### 4.NO multicollinearity: making sure our independents variables are not strongly related(correlated) with each other\n\n####  We are not violating  assumptions 1 through 3 but for NO multicollinearity we need to check","61c40f05":"#### Our model predicted '0' correctly 954 times while  predicting '0' incorrectly 154 times\n#### Also it predicted  '1'  correctly 198 times while predicting '1' incorrectly  103","088743fb":"#### DATA CLEANING","b307ec27":"### CONFUSION MATRIX","3ec18e9c":"#### This data is clean but on further analysis TotalCharges includes some empty value which we will replace with 0","4bb5a38d":"### LOADING THE DATAFRAME","e50be650":"###  CONCLUSION\n#### Let's try to make a table and with weight(BIAS) and odds ","11007d62":"### OTHER MODELS","ad83d45b":"### Dummy Variables\n#### churn is a categorical variable so we need  to turn it into a dummy indicator before we can perform our regression\n#### For other categorical variable we will use get_dummies","87112443":"### DATASET ANALYSIS AND OUTLIERS REMOVAL\n\n#### we will plot the distribution of all the numeric variables in other to be able to identify outliers and any other abnormalities\n#### Outliers will be dealt with by removing either top 1% or the bottom 1%\n"}}