{"cell_type":{"d0c46249":"code","ed7d5d35":"code","7c8846b1":"code","0e69f2fe":"code","ca86d930":"code","af98d657":"markdown","5b6b223e":"markdown","7c48d774":"markdown","175f8a85":"markdown","23e2cb6d":"markdown","28455482":"markdown","6a92d5c1":"markdown","0c9294a8":"markdown","51eea4ef":"markdown"},"source":{"d0c46249":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport optuna\nfrom tqdm.notebook import tqdm\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold\nfrom lightgbm import LGBMRegressor, plot_importance\n\ntrain = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-feb-2021\/test.csv\")\n\ncont_features = [f for f in train.columns.tolist() if f.startswith('cont')]\ncat_features = [f for f in train.columns.tolist() if f.startswith('cat')]\nfeatures = cat_features + cont_features\ndata = train[features]\ntarget = train['target']\n\nall_data = pd.concat([data, test])","ed7d5d35":"fig, ax = plt.subplots(5, 3, figsize=(14, 24))\nfor i, feature in enumerate(cont_features):\n    plt.subplot(5, 3, i+1)\n    sns.histplot(all_data[feature][::100], \n                 color=\"blue\", \n                 kde=True, \n                 bins=100)\n    plt.xlabel(feature, fontsize=9)\nplt.show()","7c8846b1":"inits = [[0.3, 0.5, 0.7, 0.9], \n         [0.039, 0.093, 0.24, 0.29, 0.35, 0.42, 0.49, 0.56, 0.62, 0.66, 0.76],\n         [0.176, 0.322, 0.416, 0.495, 0.548, 0.618, 0.707, 0.937],\n         [0.2, 0.35, 0.44, 0.59, 0.75, 0.83],\n         [0.28, 0.31, 0.42, 0.5, 0.74, 0.85],\n         [0.25, 0.38, 0.43, 0.58, 0.75, 0.9],\n         [0.34, 0.48, 0.7, 0.88],\n         [0.25, 0.29, 0.35, 0.48, 0.61, 0.68, 0.78, 0.9],\n         [0.11, 0.2, 0.3, 0.35, 0.45, 0.6, 0.76, 0.9],\n         [0.22, 0.32, 0.38, 0.44, 0.53, 0.63, 0.71, 0.81, 0.87],\n         [0.19, 0.27, 0.37, 0.46, 0.56, 0.61, 0.71, 0.86],\n         [0.23, 0.35, 0.52, 0.7, 0.84],\n         [0.27, 0.32, 0.35, 0.49, 0.63, 0.7, 0.79, 0.88],\n         [0.22, 0.29, 0.35, 0.4, 0.47, 0.58, 0.68, 0.72, 0.8]]\ngmms = []\nfor feature, init in zip(cont_features, inits):\n    X_ = np.array(all_data[feature].tolist()).reshape(-1, 1)\n    means_init = np.array(init)[:,None]\n    gmm_ = GaussianMixture(n_components=len(init),\n                           means_init=means_init,\n                           random_state=0).fit(X_)\n    gmms.append(gmm_)\n    preds = gmm_.predict(X_)\n    all_data[f'{feature}_gmm'] = preds\n    train[f'{feature}_gmm'] = preds[:len(train)]\n    test[f'{feature}_gmm'] = preds[len(train):]","0e69f2fe":"fig, ax = plt.subplots(5, 3, figsize=(24, 30))\nfor i, feature in enumerate(cont_features):\n    plt.subplot(5, 3, i+1)\n    sns.scatterplot(x=feature, \n                    y=\"target\", \n                    data=train[::150], \n                    hue=f'{feature}_gmm', \n                    palette='muted')\n    plt.xlabel(feature, fontsize=9)\nplt.show()","ca86d930":"fig, ax = plt.subplots(5, 3, figsize=(24, 30))\nfor i, feature in enumerate(cont_features):\n    plt.subplot(5, 3, i+1)\n    sns.histplot(x=feature, \n                 data=train[::150], \n                 hue=f'{feature}_gmm', \n                 kde=True, \n                 bins=100, \n                 palette='muted')\n    plt.xlabel(feature, fontsize=9)\nplt.show()","af98d657":"Thank you for reading this notebook. I'm new to Kaggle and machine-learing algorithms, and this competition is the second one for me after TPS-January. I didn't use any special techniques, but used GBDT modules I found common in Kaggle (LightGBM, XGBoost, and CatBoost). In this notebook I wrote down the basic flows I used in this competition. I don't suppose this will interest those who has been familiar with Kaggle, but I would appreciate it if you could read this and give me some advice. I'm also glad if this notebook would help other beginners.","5b6b223e":"# Training\n- I found training for different random seeds and averaging them improve PB scores [[Apolo's notebook](https:\/\/www.kaggle.com\/shkanda\/random-seed-averaging-lgb-xgb)]. Is this kind of ensemble?\n\n- I also found that a small learning rate could improve PB scores [[szdr's notebook](https:\/\/www.kaggle.com\/shogosuzuki\/0-69713-lightgbm-with-small-learning-rate)]. So I didn't tuned the learing rate with Optuna.\n\nCodes look like this. I used [Tawara's notebook](https:\/\/www.kaggle.com\/ttahara\/tps-feb-2021-3gbdts-ensemble-baseline) as a guide.\n\n```python\nNUM_FOLDS = 10\nseed_list = [0,1,2]\n\ntest_pred = np.zeros(len(test))\nval_pred = np.zeros(len(train))\n\nfor seed in tqdm(seed_list):\n    tmp_test_pred = np.zeros(len(test))\n    tmp_val_pred = np.zeros(len(train))\n    kf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=seed)\n    for f, (train_idx, val_idx) in tqdm(enumerate(kf.split(train[features], target))):\n        print(\"*\" * 20)\n        print(f\"Seed-#{seed};  Fold-#{f}\")        \n        train_x, val_x = train.iloc[train_idx][features], train.iloc[val_idx][features]\n        train_y, val_y = target[train_idx], target[val_idx]\n\n        model = LGBMRegressor(metric = 'rmse',\n                              random_state=seed, \n                              learning_rate = 0.002,\n                              n_estimators = 20000,\n                              **study.best_params)\n        model.fit(train_x,train_y,eval_set=[(val_x,val_y)],early_stopping_rounds=100,verbose=5000)\n    \n        temp_oof = model.predict(val_x)\n        temp_test = model.predict(test[features])\n\n        tmp_test_pred += temp_test\n        tmp_val_pred[val_idx] = temp_oof\n        print(mean_squared_error(temp_oof, val_y, squared=False))\n    \n    print(\"*\" * 20)\n    print(f\"Seed-#{seed}\\n{mean_squared_error(tmp_val_pred, target, squared=False)}\")\n    val_pred += tmp_val_pred\n    test_pred += tmp_test_pred \/ NUM_FOLDS\n\nval_pred \/= len(seed_list)\ntest_pred \/= len(seed_list)\nprint(\"*\" * 20)\nprint(mean_squared_error(val_pred, target, squared=False))\n```","7c48d774":"# Ensemble\nEnsembling different models was necessary to improve scores [e.g., [Somayyeh Gholami's notebook](https:\/\/www.kaggle.com\/somayyehgholami\/comparative-method-tabular-feb-301)].\nI repeated the above process with other GBDT models and ensembled them.\n\n```python\nlgbm_1 = pd.read_csv(\"..\/input\/tps2-submissions\/submission1.csv\") \nlgbm_2 = pd.read_csv(\"..\/input\/tps2-submissions\/submission2.csv\")\nxgb_1 = pd.read_csv(\"..\/input\/tps2-submissions\/submission3.csv\")                          \ncat_1 = pd.read_csv(\"..\/input\/tps2-submissions\/submission4.csv\") \n\nmodels = [lgbm_1, lgbm_2, xgb_1, cat_1]\nweights = [10., 5., 2., 1.]\nsample_submission.target = 0\n\nfor model, weight in zip(models, weights):\n    sample_submission.target += weight * model.target \/ sum(weights)\n\nsample_submission.to_csv('sub-ensemble.csv', index=False)\n```\n\nThe weights are random.\nHow can I get to quantitatively know appropriate weights to ensemble?","175f8a85":"I calculated the standard deviations for each group and added them as new features.\n```python\nfor feature in cont_features:\n    mu = all_data.groupby(f'{feature}_gmm')[feature].transform(\"mean\")\n    sigma = all_data.groupby(f'{feature}_gmm')[feature].transform(\"std\")\n    \n    train[f'{feature}_gmm_dev'] = (train[feature] - mu[:len(train)])\/sigma[:len(train)]\n    test[f'{feature}_gmm_dev'] = (test[feature] - mu[len(train):])\/sigma[len(train):]\n```\n\nFor categorical features, I used label-encoding (`sklearn.preprocessing.LabelEncoder`).\n```python\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train[feature])\n    train[feature] = le.transform(train[feature])\n    test[feature] = le.transform(test[feature])\n    \nfeatures = [col for col in train.columns.to_list() if col not in ['id','target']]\n```","23e2cb6d":"# Feature Engineering\n\nI did a slight feature-engineering.\nHistograms of the cont features show multiple components. For instance, the cont1 has 7 discrete peaks as shown below. I thought these characteristics could be used as an additional feature.\nSo, I tried `sklearn.mixture.GaussianMixture` to devide into several groups [Ref: [Notebooks of TPS-Jan. by Dave E](https:\/\/www.kaggle.com\/davidedwards1\/jan21-tabplayground-nn-final-fewer-features)].\n\nSee also https:\/\/scikit-learn.org\/stable\/modules\/mixture.html#gmm for Gaussian Mixture Models.\n\nThe scatter plots below show the cont-feature values and target, with the results of GMM.\nThe bottom histgrams also show the results of GMM.","28455482":"**Import modules and dataset \u2192**","6a92d5c1":"# Hyperparameter Tuning\n\n\nI learned that the hyperparameter tuning is necessary to improve scores.\nHere is the example for tuning LightGBM by Optuna.\nI don't really know what parameters to tune and what range to input (I don't even know what each parameter means\ud83d\ude25 ). Please let me know if I'm missing the point.\n\n```python\ndef objective(trial, data=train[features], target=target):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.2, random_state=41)\n    param = {\n        'metric': 'rmse', \n        'random_state': 41,\n        'n_estimators': 20000,\n        'learning_rate': 0.01,\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-2, 100.),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.01, 1.0),\n        'subsample': trial.suggest_categorical('subsample', [0.2,0.3,0.4,0.5,0.6,0.7,0.8,1.0]),\n        'subsample_freq': trial.suggest_int('subsample_freq', 1, 20),\n        'max_depth': trial.suggest_categorical('max_depth', [-1,30,100,300]),\n        'num_leaves' : trial.suggest_int('num_leaves', 2, 500),\n        'min_child_samples': trial.suggest_int('min_child_samples', 1, 200),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-3, 10),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 1, 100)\n    }\n    \n    model = LGBMRegressor(**param)  \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n\n    preds = model.predict(test_x)    \n    rmse = mean_squared_error(test_y, preds,squared=False)\n    \n    return rmse\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nprint('Best trial:', study.best_params)\n```\n\nFor hyperparamers of 3 GBDTs, see also the offical documents below.\n- https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html \n- https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#module-xgboost.plotting \n- https:\/\/catboost.ai\/docs\/concepts\/python-reference_parameters-list.html#python-reference_parameters-list\n\nI also found code examples for optuna are available on the official GitHub page:\nhttps:\/\/github.com\/optuna\/optuna\/tree\/master\/examples\n\n\n```python\nstudy.trials_dataframe()\noptuna.visualization.plot_param_importances(study)\noptuna.visualization.plot_parallel_coordinate(study)\noptuna.visualization.plot_contour(study, params=['num_leaves', 'max_depth', 'subsample', 'min_child_samples', 'colsample_bytree'])\noptuna.visualization.plot_optimization_history(study)\n```\nThe above codes will illustrate how the optimizations proceed.\\\nFor the output graphs, have a look at [Hamza's notebook](https:\/\/www.kaggle.com\/hamzaghanmi\/lgbm-hyperparameter-tuning-using-optuna) for instance and the offical document:\nhttps:\/\/optuna.readthedocs.io\/en\/stable\/reference\/visualization\/","0c9294a8":"### Thank you very much for reading!","51eea4ef":"## Tip for beginners like me\n\nThe Kaggle notebook turns idle of there are no interaction for maybe 40 minutes. When it becomes idle, the ongoing calculation gets revoked. Because some calculations like Optuna take more than 40 minitues, it frustrates me.\nIn such a situation, I did \"Save & Run All\". \nAlthough the outputs of each cell don't show up, the all output can be seen after the run-all finishes. \nMeantime I can't check the outputs, so it's good to post RMSs to slack. \nNote that the total elapsed time must be within 9 hours. Otherwise, nothing will be saved.\n\nhttps:\/\/api.slack.com\/messaging\/webhooks\n\nAre there any good way to run a long calculation?\n\n```python\nimport json\nimport urllib.request\n\ndef post_slack(message):\n    url = 'https:\/\/hooks.slack.com\/services\/<<YOUR_SLACK_URL>>'\n    headers = {'Content-Type': 'application\/json'}\n    data = {\"channel\": \"#general\",\n            \"username\": \"webhookbot\", \n            \"text\": message, \n            \"icon_emoji\": \":ghost:\"}\n\n    req = urllib.request.Request(url, json.dumps(data).encode(), headers)\n    with urllib.request.urlopen(req) as res:\n        body = res.read()\n\npost_slack(f\"Fold-{f} finished.\\nRMSE: {rmse}\")\n```"}}