{"cell_type":{"52256f0f":"code","14c0d94d":"code","b9d563bd":"code","c427f562":"code","33f53181":"code","842bdc60":"code","8a162c59":"code","e4f6f1c8":"code","6f63e8eb":"code","58832fcc":"code","63fc47fe":"code","8f0a078f":"code","5c66c40e":"code","c103addf":"code","909a691f":"code","acff99c2":"code","21f3f0ae":"code","7652406f":"code","9e5a0b5a":"code","30e67997":"code","04f94954":"code","30d90ca2":"code","302e8e4a":"code","91e67788":"code","49c0ec9b":"code","f5c65400":"markdown","3dfab0ea":"markdown","7a206ed4":"markdown","644ae678":"markdown","1321e375":"markdown","999b1305":"markdown","b321f022":"markdown","cf5ae98d":"markdown","7e439eb0":"markdown","d8717edf":"markdown","a61cea1b":"markdown","6d5310eb":"markdown","af98b74c":"markdown","f726607f":"markdown"},"source":{"52256f0f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","14c0d94d":"# Loading the data\ndataset = pd.read_csv('..\/input\/ccpp-data\/CCPP_data.csv')\ndataset.head()","b9d563bd":"# Let's see what we got\ndataset.describe()","c427f562":"# Checking if data type are correct\ndataset.info()","33f53181":"#bivariate analysis\nplt.figure(figsize=(12,6))\nsns.scatterplot(data=dataset, x='AT', y='PE', hue='PE')","842bdc60":"plt.figure(figsize=(12,6))\nsns.scatterplot(data=dataset, x='V', y='PE', hue='PE')","8a162c59":"plt.figure(figsize=(12,6))\nsns.scatterplot(data=dataset, x='AP', y='PE', hue='PE')","e4f6f1c8":"plt.figure(figsize=(12,6))\nsns.scatterplot(data=dataset, x='RH', y='PE', hue='PE')","6f63e8eb":"plt.figure(figsize=(12,6))\nsns.heatmap(dataset.corr(), annot=True, linewidths=.5)","58832fcc":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n#from sklearn.metrics import mean_absolute_percentage_error","63fc47fe":"X1 = dataset.iloc[:, 0:1].values # We choose AT since correlation between AT and PE is -0.95\ny1 = dataset.iloc[:, -1].values # PE","8f0a078f":"X1","5c66c40e":"X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size = 0.2, random_state = 0)","c103addf":"lr_model = LinearRegression()\nlr_model.fit(X1_train, y1_train)","909a691f":"# Predicting the Test set results\ny1_pred = lr_model.predict(X1_test) \nnp.set_printoptions(precision=2)  # to print 2 decimals\nprint(np.concatenate(\n    (y1_pred.reshape(len(y1_pred),1), y1_test.reshape(len(y1_test),1)),axis=1)\n) # so we can compare prediction and test values","acff99c2":"import math\n# Evaluating the Model Performance\nlr_score = r2_score(y1_test, y1_pred)\n#lr_MAPE_score = mean_absolute_percentage_error(y1_test, y1_pred)\n\n#Make it into a percentage\n#lr_MAPE_score = lr_MAPE_score*100\n#lr_MAPE_score = round(lr_MAPE_score, 2)\n\n\nprint('R square is',lr_score)\n#print('MAPE is',lr_MAPE_score,'%')","21f3f0ae":"# Visualising the Test set results\nplt.scatter(X1_test, y1_test, color = 'red')  \nplt.plot(X1_test, lr_model.predict(X1_test), color = 'blue')   \nplt.title('AT vs PE')\nplt.xlabel('AT')\nplt.ylabel('PE')\nplt.show()","7652406f":"X2 = dataset.iloc[:, :-1].values\ny2 = dataset.iloc[:, -1].values","9e5a0b5a":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size = 0.2, random_state = 0)","30e67997":"# Training the Decision Tree Regression model on the Training set\nfrom sklearn.tree import DecisionTreeRegressor\ndtr_model = DecisionTreeRegressor(random_state = 0)\ndtr_model.fit(X2_train, y2_train)","04f94954":"# Predicting the Test set results\ny2_pred = dtr_model.predict(X2_test)\nnp.set_printoptions(precision=2)\nprint(np.concatenate\n      ((y2_pred.reshape(len(y2_pred),1), y2_test.reshape(len(y2_test),1)),\n       axis=1))","30d90ca2":"#Evaluating the Model Performance\ndtr_score = r2_score(y2_test, y2_pred)\n#dtr_MAPE_score = mean_absolute_percentage_error(y2_test, y2_pred)\n\n#Make it into a percentage\n#dtr_MAPE_score = dtr_MAPE_score*100\n#dtr_MAPE_score = round(dtr_MAPE_score, 2)\n\nprint('R square is',dtr_score)\n#print('MAPE is',dtr_MAPE_score,'%')","302e8e4a":"print('The Linear Regression R square result is', lr_score,'%')\nprint('The Decision Tree R square result is', dtr_score,'%')","91e67788":"# The judge function\nprint('Annnnnd the winner is...')\n\nif lr_score < dtr_score:\n  print('Decision Tree with a R2 of', dtr_score,'% \ud83d\udc96')\nelse:\n    print('LR IS WINNER with a R2 of', lr_score,'% \ud83d\udc96')","49c0ec9b":"input_array = [[22]]\ntest_pred = lr_model.predict(input_array)\ntest_pred","f5c65400":"2\ufe0f\u20e3 Decision Tree Regression","3dfab0ea":"Determine which possible features we may want to use in the model, and identify the different algorithms we might consider.\n\n","7a206ed4":"1\ufe0f\u20e3 Linear Regression","644ae678":"# Try it yourself","1321e375":"Since we've seen a clean scatterplot that shows that could be a correlation, and ans few outliers. Let's plot a correlation heatmap.","999b1305":"Conclusion: it's clean enough, no need to perform data cleaning or further Exploratory Data Analysis. We can see a high correlation between the temperature and the energy output, our target. Also with Exhaust Vacuum. This insight we will help us for the modeling part.","b321f022":"Evaluate the performance of your final model using the output metric you defined earlier.  \n\n","cf5ae98d":"### The problem\nWe need to predict the electrical energy output of a Combined Cycle Power Plant, which uses a combination of gas turbines, steam turbines, and heat recovery steam generators to generate power.  We have a set of 9568 hourly average ambient environmental readings from sensors at the power plant which we will use in our model.\n\n\n### The solution\nSince we're targeting a metric, we have to build a regression model that takes into consideration Temperature (T), Ambient Pressure (AP), Relative Humidity (RH), Exhaust Vacuum (V) as variables, features to predict a target Net hourly electrical energy output (PE). I'm thinking to try two regression models that are simple and parametric: Linear Regression and Decision tree Regression.\n\n### The prefered metric\nMAPE is calculated as the absolute value of the difference between the actual value and the predictions regenerating divided by the actual values. We sum that up and divide by the total number of productions to get our MAPE value.\n\nWhy MAPE? It converts error into a percentage, very easy to interpret and compare.","7e439eb0":"# Predict the electrical energy output of a Combined Cycle Power Plant \ud83c\udf31","d8717edf":"### Exploratory Data Analysis","a61cea1b":"Split your data to create a test set to evaluate the performance of your final model.  Then, using your training set, determine a validation strategy for comparing different models - a fixed validation set or cross-validation.  Depending on whether you are using Excel, Python or AutoML for your model building, you may need to manually split your data to create the test set and validation set \/ cross validation folds.\n\nUse your validation approach to compare at least two different models (which may be either 1) different algorithms, 2) the same algorithm with different combinations of features, or 3) the same algorithm and features with different values for hyperparameters).  From among the models you compare, select the model with the best performance on your validation set as your final model.","6d5310eb":"### Modeling","af98b74c":"If you want to try by yourself, you can modify the number inside the input_array variable and run it.\nIt will fire the Linear regression model.","f726607f":"### Results\n"}}