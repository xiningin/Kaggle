{"cell_type":{"1cf07a41":"code","51ab684b":"code","11832721":"code","3b93449e":"code","8e09e724":"code","8a8aabe1":"code","34d0e38a":"code","07b111ff":"code","d292a38a":"code","e05a6866":"code","3e7fa458":"code","8f716873":"code","dcad4a37":"code","f5831995":"code","7ff5cdf3":"code","eca0dceb":"code","c0cef144":"code","6c38de5c":"code","415e1ab5":"code","5e2a321d":"code","ce462e7f":"markdown","9b606d75":"markdown","b078e7b0":"markdown","724d7148":"markdown","ed565ba3":"markdown","9f0291cc":"markdown","7901aace":"markdown","d669863e":"markdown","e171ac4f":"markdown"},"source":{"1cf07a41":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score\n","51ab684b":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# test.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\ndf1 = pd.read_csv('..\/input\/amazon-ml-engineer-hiring\/train.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'test.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","11832721":"df1.head()","3b93449e":"fig, ax = plt.subplots(1,2, figsize=(20, 6))\ndata_q1 = df1['customer_active_segment'].value_counts().sort_index()\nax[0].bar(data_q1.index, data_q1, width=0.55, \n       edgecolor='gray', color='#F4B8DD',\n       linewidth=0.7)\n\nfor i in data_q1.index:\n    ax[0].annotate(f\"{data_q1[i]}\", \n                   xy=(i, data_q1[i] + 30),\n                   va = 'center', ha='center',fontweight='light', fontfamily='serif',\n                   color='#311D34')\n\n\nfor s in ['top', 'left', 'right']:\n    ax[0].spines[s].set_visible(False)\n\nax[0].set_ylim(0, 600)    \nax[0].set_xticklabels(data_q1.index, fontfamily='serif')\nax[0].set_yticklabels(np.arange(0, 700, 100),fontfamily='serif')\nfig.text(0.1, 0.95, 'customer_active_segment distribution', fontsize=18, fontweight='bold', fontfamily='serif')    \nax[0].grid(axis='y', linestyle='-', alpha=0.4)\n\ndata_x1 = df1['X1'].value_counts().sort_index()\nax[1].bar(data_x1.index, data_x1, width=0.55,\n         edgecolor='gray', color='#A9EFDA', linewidth=0.7)\nfor i in data_x1.index:\n    ax[1].annotate(f\"{data_x1[i]}\", \n                   xy=(i, data_x1[i] + 30),\n                   va = 'center', ha='center',fontweight='light', fontfamily='serif',\n                   color='#0D4534')\nfor s in ['top', 'left', 'right']:\n    ax[1].spines[s].set_visible(False)\n\nax[1].set_ylim(0, 600)    \nax[1].set_xticklabels(data_q1.index, fontfamily='serif')\nax[1].set_yticklabels(np.arange(0, 700, 100),fontfamily='serif')\nax[1].text(-1.8, 650, 'X1 distribution', fontsize=18, fontweight='bold', fontfamily='serif')    \nax[1].grid(axis='y', linestyle='-', alpha=0.4)\nplt.show()","8e09e724":"import seaborn as sns\npd.set_option('mode.chained_assignment', None)\ncol = list(df1.columns)[1:9]\ndf = pd.DataFrame(np.nan, index=range(8000), columns = ['Group', 'Value','target_var'])\nfig, ax = plt.subplots(figsize=(20,8))\n\nfor column in col:\n    for i in df1.index:\n        df['Group'][i+col.index(column)*1000] = column\n        #print(df)\n        df['Value'][i+col.index(column)*1000] = df1[column][i]\n        df['target_var'][i+col.index(column)*1000] = df1['customer_category'][i]\nmy_pal = {0: \"#A9EFDA\", 1: \"#F4B8DD\"}\nax.grid(axis='y', linestyle='-', alpha=0.4)\nsns.boxplot(ax=ax, x='Group', y='Value', hue='target_var', data=df, palette=my_pal)\n#sns.plt.show()","8a8aabe1":"for column in col:\n    for i in df1.index:\n        df['Value'][i+col.index(column)*1000] = (df1[column][i]-min(df1[column]))\/(max(df1[column])-min(df1[column]))\n        df1[column][i]=(df1[column][i]-min(df1[column]))\/(max(df1[column])-min(df1[column]))\n        \nfig, ax = plt.subplots(figsize=(20,8))\nsns.boxplot(ax=ax, x='Group', y='Value', hue='target_var', data=df, palette=my_pal)\nax.grid(axis='y', linestyle='-', alpha=0.4)\nplt.show()","34d0e38a":"df1['customer_category'].unique()\nsecond = sum(df1['customer_category'])\/len(df1['customer_category'])*100","07b111ff":"labels = ['False', 'True']\nsizes = [100-second, second]\nexplode = (0, 0.1) \n\nfig1, ax1 = plt.subplots(figsize=(20, 6))\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90, colors=['#98EBD2','#F4B8DD'], textprops={'fontfamily':'serif','fontsize':14}, wedgeprops={'alpha':0.8})\nax1.axis('equal') \nax1.text(-1.4,1.6,\"Target variable customer_category\", fontsize=18, fontfamily='serif', fontweight='bold')\n\nplt.show()","d292a38a":"df1.customer_active_segment = pd.Categorical(df1.customer_active_segment)\ndf1.X1 = pd.Categorical(df1.X1)\n\ndf1['X1'] = df1.X1.cat.codes\ndf1['customer_active_segment'] = df1.customer_active_segment.cat.codes\ndf1.head()","e05a6866":"df1.isnull().sum()","3e7fa458":"df1 = df1.fillna(df1.mean())","8f716873":"x = df1.iloc[:,1:len(df1.columns)-1]\ny = df1.iloc[:,len(df1.columns)-1]\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.33,random_state=42)","dcad4a37":"sc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n\n\n#-----Upsampling----\nfrom sklearn.utils import resample\nfrom collections import Counter\n\nprint(\"Before Upsampling:-\")\nprint(Counter(y_train))\n\n# X_train_upsampled, y_train_upsampled = resample(x_train[y_train == 1],\n#                                                 y_train[y_train == 1],\n#                                                 replace=True,\n#                                                 n_samples=x_train[y_train == 0].shape[0],\n#                                                 random_state=123)\n\n\n# Let's use SMOTE to oversample\nfrom imblearn.over_sampling import SMOTE\noversample = SMOTE()\nx_train_upsampled, y_train_upsampled = oversample.fit_resample(x_train,y_train)\n\nprint(\"After Upsampling:-\")\nprint(Counter(y_train_upsampled))","f5831995":"print(\"\\n\\n\\n\\n AFTER UPSAMPLING\\n\\n\")\nclassifier = RandomForestClassifier(n_estimators = 50, random_state = 0)\nclassifier.fit(x_train_upsampled, y_train_upsampled)\n# Predicting result for training set and validation set\npredict_val_rf = classifier.predict(x_test)\n\n\n# Model Performance\n\nprint(\"Accuracy : \", accuracy_score(y_test, predict_val_rf) *  100)\nprint(\"Recall : \", recall_score(y_test, predict_val_rf) *  100)\nprint(\"Precision : \", precision_score(y_test, predict_val_rf) *  100)\nprint(confusion_matrix(y_test, predict_val_rf))\nprint(classification_report(y_test, predict_val_rf))","7ff5cdf3":"df1.head()","eca0dceb":"fig, ax = plt.subplots(figsize =(20,8))\ncolors=['#98EBD2','#F4B8DD']\nfor i in range(2):\n    x = []\n    y = []\n    for a in df1.index:\n        if df1['customer_category'][a] == i:\n            x.append(df1['customer_order_score'][a])\n            y.append(df1['customer_stay_score'][a])\n    #scale = 200.0 * np.random.rand(n)\n    ax.scatter(x, y, c=colors[i], label=colors[i],\n                   alpha=1, edgecolors='none')\n\nax.legend()\nax.grid(True)\n\nplt.show()","c0cef144":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE\n\ncols = df1.columns[1:len(df1.columns)-1]\nX = df1[cols]\ny = df1['customer_category']\n# Build a logreg and compute the feature importances\nmodel = LogisticRegression()\n# create the RFE model and select 8 attributes\nrfe = RFE(model, 8)\nrfe = rfe.fit(X, y)\n# summarize the selection of the attributes\nprint('Selected features: %s' % list(X.columns[rfe.support_]))","6c38de5c":"from sklearn.feature_selection import RFECV\n\nrfecv = RFECV(estimator=LogisticRegression(), step=1, cv=10, scoring='accuracy')\nrfecv.fit(X, y)\n\nprint(\"Optimal number of features: %d\" % rfecv.n_features_)\nprint('Selected features: %s' % list(X.columns[rfecv.support_]))\n\n# Plot number of features VS. cross-validation scores\nplt.figure(figsize=(10,6))\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score (nb of correct classifications)\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","415e1ab5":"Selected = ['customer_visit_score', 'customer_product_search_score', 'customer_ctr_score', 'customer_stay_score', 'customer_product_variation_score', 'customer_order_score', 'customer_affinity_score']\nX = df1[Selected]\n\nplt.subplots(figsize=(8, 5))\nsns.heatmap(X.corr(), annot=True, cmap=\"RdYlGn\")\nplt.show()","5e2a321d":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score \nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc, log_loss\n\n# create X (features) and y (response)\nX = df1[Selected]\ny = df1['customer_category']\n\n# use train\/test split with different random_state values\n# we can change the random_state values that changes the accuracy scores\n# the scores change a lot, this is why testing scores is a high-variance estimate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n\n# check classification scores of logistic regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\ny_pred_proba = logreg.predict_proba(X_test)[:, 1]\n[fpr, tpr, thr] = roc_curve(y_test, y_pred_proba)\nprint('Train\/Test split results:')\nprint(logreg.__class__.__name__+\" accuracy is %2.3f\" % accuracy_score(y_test, y_pred))\nprint(logreg.__class__.__name__+\" log_loss is %2.3f\" % log_loss(y_test, y_pred_proba))\nprint(logreg.__class__.__name__+\" auc is %2.3f\" % auc(fpr, tpr))\n\nidx = np.min(np.where(tpr > 0.95)) # index of the first threshold for which the sensibility > 0.95\n\nplt.figure()\nplt.plot(fpr, tpr, color='coral', label='ROC curve (area = %0.3f)' % auc(fpr, tpr))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot([0,fpr[idx]], [tpr[idx],tpr[idx]], 'k--', color='blue')\nplt.plot([fpr[idx],fpr[idx]], [0,tpr[idx]], 'k--', color='blue')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - specificity)', fontsize=14)\nplt.ylabel('True Positive Rate (recall)', fontsize=14)\nplt.title('Receiver operating characteristic (ROC) curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\nprint(\"Using a threshold of %.3f \" % thr[idx] + \"guarantees a sensitivity of %.3f \" % tpr[idx] +  \n      \"and a specificity of %.3f\" % (1-fpr[idx]) + \n      \", i.e. a false positive rate of %.2f%%.\" % (np.array(fpr[idx])*100))","ce462e7f":"Much better. Now we are able to see that some of variables are highly correlated with target value which will give us an ability to perform a binary classification. Number of outliers are manageble so we not gonna clean them.\n\nAnd finally: hof big is every fraction of target value?","9b606d75":"Dataset contains 12 columns, 4 of them are catergorical and other 8 are continous floats. Last column is a target value which means, that we are going to predict it, using other 11 columns. And first one is usless since it contains a user_id which does not contain any useful information.\n\nTo approach task, first we need to visualize data to get an understending of what we are daling with.\nFirst we need to visualize the distribution of categorical variable.","b078e7b0":"Split dataset to train and test and fit the RandomForestClassifier model","724d7148":"In this Notebook I overview dataset https:\/\/www.kaggle.com\/aquib5559\/amazon-ml-engineer-hiring visualize it and build a simple model to approach task of binary classification.","ed565ba3":"Now every categorical value is replaced with coresponded integer. Last thing we need to do on the data is checking if data contains nissing values and replace them with mean value of column so thay doesn't influence a classification.","9f0291cc":"First upload dataset to the dataframe and print the shape.","7901aace":"Now we need to visualize continuous variables, so we get an idea about distribution of different variables, depends of the value of target value. We use a boxplot visualisation so we will also see how many outliers data contains.","d669863e":"We are ready to perform classification but we need to do something before. Categorical variables per se could not be used in classification. We need to decode them as integers.","e171ac4f":"Not very informative and we are clearly suffering from outliers in the data. We need to clean up the data and normalize them!"}}