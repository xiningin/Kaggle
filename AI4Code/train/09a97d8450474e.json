{"cell_type":{"f671416d":"code","3fed1709":"code","cf723139":"code","e4885907":"code","439cc72a":"code","228ea945":"code","7c9e3cc0":"code","40f4d0ca":"code","1790da1c":"code","45910a2c":"code","0a0be3b2":"code","16cd1928":"code","92a117f1":"code","547aabfb":"code","34ba7610":"code","84b341c7":"code","f4b1bf58":"code","bca12589":"code","15dd9491":"code","e1d2e0f9":"code","b84f22a1":"code","b6206597":"code","0f470c4a":"code","b7bbd7b1":"code","f1466ab1":"code","3cc01e51":"code","1d088d96":"code","1d606ba5":"code","f1268e92":"code","730e52d0":"code","62cccb89":"code","94da5709":"code","7826d358":"code","6d3582ae":"code","83d0d8b4":"code","0237f7fb":"code","b48cdb53":"code","21c40ff3":"code","f2def9c1":"code","a71cb488":"code","936d6b8a":"code","71779680":"code","d94a20f7":"code","2f1e0e1e":"markdown","487fdfdb":"markdown","8b7002d0":"markdown","08e24698":"markdown","f85123a0":"markdown","f9c54ec0":"markdown","64de6b22":"markdown","a566cdec":"markdown","ddc38785":"markdown","3af0e95e":"markdown","6647ca47":"markdown","27cde1bc":"markdown","337265a6":"markdown","641c4c21":"markdown","fed9c94b":"markdown","8e700fbf":"markdown","0d1dec52":"markdown","2d48a673":"markdown","19c27613":"markdown","1865b5a2":"markdown","4c76f621":"markdown","50ab684e":"markdown","5d2b7a5f":"markdown","336d852d":"markdown","03ab73b1":"markdown","31f78df9":"markdown","9117fef8":"markdown","4eec0291":"markdown","fe0b4c05":"markdown","7c2e8e58":"markdown"},"source":{"f671416d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom imblearn.over_sampling import SMOTE\nimport matplotlib.pyplot as plt\nimport scipy.stats\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport plotly.express as px\nfrom tabulate import tabulate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom scipy.stats import randint\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3fed1709":"#Import dataset\ndf = pd.read_csv(\"\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")","cf723139":"df.head()","e4885907":"df.info()","439cc72a":"df.describe()","228ea945":"# Checking missing values\ndf.isna().sum()","7c9e3cc0":"# histogram plot for all variables\ndf.hist(figsize=(20,20));","40f4d0ca":"# Data Visualization - Age x DEATH_EVENT\nfig_1 = px.histogram(df, 'age', color='DEATH_EVENT', nbins=50, title='Data distribution per age')\nfig_1.show()\n\nfig_2 = px.box(df, x=\"DEATH_EVENT\", y=\"age\", title='Box Plot (DEATH_EVENT x Age)')\nfig_2.show()","1790da1c":"# Data Visualization - creatinine_phosphokinase x DEATH_EVENT\nfig_3 = px.histogram(df, 'creatinine_phosphokinase', color='DEATH_EVENT', nbins=50, title='Data distribution per creatinine_phosphokinase')\nfig_3.show()\n\nfig_4 = px.box(df, x=\"DEATH_EVENT\", y=\"creatinine_phosphokinase\", title='Box Plot (DEATH_EVENT x creatinine_phosphokinase)')\nfig_4.show()","45910a2c":"# Data Visualization - ejection_fraction x DEATH_EVENT\nfig_5 = px.histogram(df, 'ejection_fraction', color='DEATH_EVENT', nbins=50, title='Data distribution per ejection_fraction')\nfig_5.show()\n\nfig_6 = px.box(df, x=\"DEATH_EVENT\", y=\"ejection_fraction\", title='Box Plot (DEATH_EVENT x ejection_fraction)')\nfig_6.show()","0a0be3b2":"# Data Visualization - platelets x DEATH_EVENT\nfig_7 = px.histogram(df, 'platelets', color='DEATH_EVENT', nbins=50, title='Data distribution per platelets')\nfig_7.show()\n\nfig_8 = px.box(df, x=\"DEATH_EVENT\", y=\"platelets\", title='Box Plot (DEATH_EVENT x platelets)')\nfig_8.show()","16cd1928":"# Data Visualization - serum_creatinine x DEATH_EVENT\nfig_9 = px.histogram(df, 'serum_creatinine', color='DEATH_EVENT', nbins=50, title='Data distribution per serum_creatinine')\nfig_9.show()\n\nfig_10 = px.box(df, x=\"DEATH_EVENT\", y=\"serum_creatinine\", title='Box Plot (DEATH_EVENT x serum_creatinine)')\nfig_10.show()","92a117f1":"# Data Visualization - serum_sodium x DEATH_EVENT\nfig_11 = px.histogram(df, 'serum_sodium', color='DEATH_EVENT', nbins=50, title='Data distribution per serum_sodium')\nfig_11.show()\n\nfig_12 = px.box(df, x=\"DEATH_EVENT\", y=\"serum_sodium\", title='Box Plot (DEATH_EVENT x serum_sodium)')\nfig_12.show()","547aabfb":"# Data Visualization - time x DEATH_EVENT\nfig_13 = px.histogram(df, 'time', color='DEATH_EVENT', nbins=50, title='Data distribution per time')\nfig_13.show()\n\nfig_14 = px.box(df, x=\"DEATH_EVENT\", y=\"time\", title='Box Plot (DEATH_EVENT x time)')\nfig_14.show()","34ba7610":"# counter plot for all categorical variables\nfig, ((axis1, axis2), (axis3, axis4), \n      (axis5, axis6)) = plt.subplots(3,2, figsize=(20,50))\n\nsns.countplot(x='smoking', hue='DEATH_EVENT', data=df, ax=axis1)\nsns.countplot(x='anaemia', hue='DEATH_EVENT', data=df, ax=axis2)\nsns.countplot(x='sex', hue='DEATH_EVENT', data=df, ax=axis3);\nsns.countplot(x='diabetes', hue='DEATH_EVENT', data=df, ax=axis4)\nsns.countplot(x='high_blood_pressure', hue='DEATH_EVENT', data=df, ax=axis5)","84b341c7":"# Class distribution\ndf.groupby('DEATH_EVENT').size()","f4b1bf58":"# SMOTE\nsmote_bal = SMOTE()\n\nX = df.iloc[:, 0:12]\ny = df.iloc[:, 12]\nX_res, y_res = smote_bal.fit_resample(X, y)\n\n# Plot \nsns.countplot(y_res, palette = \"OrRd\")\nplt.box(False)\nplt.xlabel('Death No (0) \/ Yes (1)', fontsize = 11)\nplt.ylabel('Total', fontsize = 11)\nplt.title('Counting deaths\\n')\nplt.show()","bca12589":"df_1 = pd.concat([X_res, y_res], axis=1)\ndf_1.isna().sum()","15dd9491":"#Separate the Categorical and Numerical variables\nnum = [name for name in df_1.columns if df_1[name].nunique() > 3]\ncat = [name for name in df_1.columns if df_1[name].nunique() < 3]\ndf_num = df_1[num]\ndf_num[\"DEATH_EVENT\"] = df_1[\"DEATH_EVENT\"]\ndf_cat = df_1[cat]","e1d2e0f9":"plt.figure(figsize = (10,5))\nsns.heatmap(df_cat.corr(), cmap=\"Blues\")","b84f22a1":"a, b = scipy.stats.pearsonr(df_cat.DEATH_EVENT, df_cat.anaemia)\ntable = [['Correlation Coefficient', 'p-value'], [a, b]]\nprint(tabulate(table))","b6206597":"plt.figure(figsize = (10,5))\nsns.heatmap(df_num.corr(), cmap=\"Blues\")","0f470c4a":"a, b = scipy.stats.pearsonr(df_num.DEATH_EVENT, df_num.platelets)\nc,d = scipy.stats.pearsonr(df_num.DEATH_EVENT, df_num.creatinine_phosphokinase)\ntable = [['Correlation Coefficient', 'p-value'], [a, b], [c, d]]\nprint(tabulate(table))","b7bbd7b1":"df_2 = df_1.drop(columns = [\"platelets\",\"creatinine_phosphokinase\",\"anaemia\", \"serum_sodium\"])\ndf_2.head()","f1466ab1":"# Separating array into input and output components\narray = df_2.values\n\nX = array[:,0:8]\nY = array[:,8]\n\n# Creating normalized data\nscaler = Normalizer().fit(X)\nnormalizedX = scaler.transform(X)","3cc01e51":"#Separating train and test data\nX_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.7)","1d088d96":"# Creating logistic Regression object\nmodel_v1a = LogisticRegression(solver ='liblinear', max_iter=1000)\n\n# Trainning the model with data train and checking the score\nmodel_v1a.fit(X_train, y_train)\nmodel_v1a.score(X_train, y_train)\n\n# Accurancy the model - train data\npredict_train_a = model_v1a.predict(X_train)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_train, predict_train_a)))\nprint()","1d606ba5":"# Accurancy the model - test data\npredict_test_a = model_v1a.predict(X_test)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, predict_test_a)))\nprint()","f1268e92":"# Tuning Hyperparameters\nvalores_grid = {'penalty': ['l1','l2'], 'C': [0.001,0.01,0.1,1,10,100,1000]}\n\n# Creating model\nmodel = LogisticRegression(solver ='liblinear', max_iter=1000)\n\n# Creating grid\nmodel_v1 = GridSearchCV(estimator = model, param_grid = valores_grid)\nmodel_v1.fit(X_train, y_train)\n\nprint(\"Accuracy: %.3f\" % (model_v1.best_score_ * 100))\nprint(\"Best Model Parameters:\\n\", model_v1.best_estimator_)","730e52d0":"predict_test = model_v1.predict(X_test)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, predict_test)))\nprint()","62cccb89":"# Creating Decision Tree object\nmodel_v2 = tree.DecisionTreeClassifier() \n\n# Trainning the model with data train and checking the score\nmodel_v2.fit(X_train, y_train)\nmodel_v2.score(X_train, y_train)\n\n# Accurancy the model - train data\npredict_train_v2 = model_v2.predict(X_train)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_train, predict_train_v2)))\nprint()","94da5709":"# Accurancy the model - test data\npredict_test_v2 = model_v2.predict(X_test)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, predict_test_v2)))\nprint()","7826d358":"# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"max_depth\": [3, None],\n              \"max_features\": randint(1, 8),\n              \"min_samples_leaf\": randint(1, 8),\n              \"criterion\": [\"gini\", \"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n\n# Fit it to the data\ntree_cv.fit(X_train, y_train)\n\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))","6d3582ae":"predict_test_a2 = tree_cv.predict(X_test)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, predict_test_a2)))\nprint()","83d0d8b4":"# Creating GaussianNB object\nmodel_v3 = GaussianNB()\n\n# Trainning the model with data train and checking the score\nmodel_v3.fit(X_train, y_train)\nmodel_v3.score(X_train, y_train)\n\n# Accurancy the model - train data\npredict_train_v3 = model_v3.predict(X_train)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_train, predict_train_v3)))\nprint()","0237f7fb":"# Accurancy the model - test data\npredict_test_v3 = model_v3.predict(X_test)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, predict_test_v3)))\nprint()","b48cdb53":"# Creating the model\nmodel_v4 = svm.SVC(kernel = 'rbf')\n\n# Grid values\nC_range = np.array([50., 100., 400.])\ngamma_range = np.array([0.03*0.001,0.0001,0.3*0.001])\n\n# Hyperparameters grid\nsvm_param_grid = dict(gamma = gamma_range, C = C_range)\n\n# Grid Search\nmodel_v4_grid_search_rbf = GridSearchCV(model_v4, svm_param_grid, cv = 7)\n\n# Trainning the model with data train and checking the score\nmodel_v4_grid_search_rbf.fit(X_train, y_train)\n\n# Accurancy the model - train data\nprint(f\"Acur\u00e1cia em Treinamento: {model_v4_grid_search_rbf.best_score_ :.2%}\")\nprint(\"\")\nprint(f\"Hiperpar\u00e2metros Ideais: {model_v4_grid_search_rbf.best_params_}\")","21c40ff3":"# Accurancy the model - test data\npredict_test_v4 = model_v4_grid_search_rbf.predict(X_test)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, predict_test_v4)))\nprint()","f2def9c1":"# Creating Gradient Boosting object\nmodel_v5 = GradientBoostingClassifier()\n\n# Trainning the model with data train and checking the score\nmodel_v5.fit(X_train, y_train)\nmodel_v5.score(X_train, y_train)\n\n# Accurancy the model - train data\npredict_train_v5 = model_v5.predict(X_train)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_train, predict_train_v5)))\nprint()","a71cb488":"# Accurancy the model - test data\npredict_test_v5 = model_v5.predict(X_test)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, predict_test_v5)))\nprint()","936d6b8a":"# Creating Random Forest object\nmodel_v6 = RandomForestClassifier(n_estimators = 500)\n\n# Trainning the model with data train and checking the score\nmodel_v6.fit(X_train, y_train)\nmodel_v6.score(X_train, y_train)\n\n# Accurancy the model - train data\npredict_train_v6 = model_v6.predict(X_train)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_train, predict_train_v6)))\nprint()","71779680":"# Accurancy the model - test data\npredict_test_v6 = model_v6.predict(X_test)\nprint(\"Accuracy: {0:.4f}\".format(metrics.accuracy_score(y_test, predict_test_v6)))\nprint()","d94a20f7":"modelos = []\n\nmodelos.append(('LR', model_v1))\nmodelos.append(('DTC', model_v2))\nmodelos.append(('NB', model_v3))\nmodelos.append(('SVM', model_v4_grid_search_rbf))\nmodelos.append(('GBC', model_v5))\nmodelos.append(('RFC', model_v6))\n\nfor nome, modelo in modelos:\n    predict_test = modelo.predict(X_test)\n    met = metrics.accuracy_score(y_test, predict_test)\n    msg = \"Accuracy for \" \"%s: %f\" % (nome, met)\n    print(msg)","2f1e0e1e":"<ul><li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Considering this dataset, it seems that \"Anaemia\" have no correlation with the number of deaths \"DEATH_EVENT\".<\/li><\/ul><\/p>\n<ul><li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Let's use another technic to analyze this correlation. Let's suppose that \"Anaemia\" has correlation with \"DEATH_EVENT\", i.e., our null hypothesis.<\/li><\/ul><\/p>\n\n<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\"><b>Using stats.pearsonr, we can evaluate the p-value for this variable:<\/b><\/p>","487fdfdb":"<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:arial\">1) Logistic Regression<\/h2>","8b7002d0":"<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:arial\">6) Random Forest Classifier<\/h2>","08e24698":"<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:arial\">Correlation between Numerical variables<\/h2>","f85123a0":"<center><h1 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\" > 3 - Feature Selection<\/h1><\/center>","f9c54ec0":"<center><h1 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\" > 5 - Predictive Analysis<\/h1><\/center>","64de6b22":"**With our new dataset balanced, Let's verify if no NA's were introduced into the dataset.**","a566cdec":"<center><h1 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\" > 2 - Exploratory Data Analysis<\/h1><\/center>","ddc38785":"<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:arial\">2) Decision Tree<\/h2>","3af0e95e":"<ul>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">As a final conclusion, the algorithm <b>Random Forest Classifier<\/b> presented the best level of accurancy (85%!!!) \ud83d\ude03\ud83d\ude01\ud83d\udcaa\ud83d\udc4f<\/p><\/li>    \n<\/ul>","6647ca47":"<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">As we can see, both values are higher than 5% (second value). Thus, we can reject the null hypotesis.<\/p>\n\n<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\"><b>As conclusion, based on these 2 techniques, we have evidences to reject the null hypotesis and we can remove these variables of our dataset.<\/b><\/p>","27cde1bc":"<center><h1 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\" > 1 - Introduction<\/h1><\/center>","337265a6":"<center><h1 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\" > 4 - Normalizing Data<\/h1><\/center>","641c4c21":"<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:arial\">Correlation between Cathegorical variables<\/h2>","fed9c94b":"<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide. Heart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.<\/p>\n\n<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Most cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.<\/p>\n\n<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.<\/p>\n\n<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">For this project, we will predict if one person has a heart failure according to some risk factors. This project was elaborated for \n<a href=\"https:\/\/www.kaggle.com\/andrewmvd\/heart-failure-clinical-data\">Kaggle Challenge<\/a><\/p>\n\n<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">The dataset used in this project was obtained from the <a href=\"https:\/\/archive.ics.uci.edu\/ml\/datasets\/Heart+failure+clinical+records\">UCI Machine Learning Repository<\/a>.<\/p>","8e700fbf":"<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Predictive analysis is the phase that we evaluate our model using several Machine Learning techniques. In this project, it was used the following algorithms:<\/p>\n\n<ul>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Logistic Regression<\/p><\/li>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Decision Tree<\/p><\/li>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Naive Bayes<\/p><\/li>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">SVM<\/p><\/li>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Gradient Boosting Classifier<\/p><\/li>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Random Forest Classifier<\/p><\/li>\n     \n<\/ul>","0d1dec52":"<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">After evaluating the charts, balanced the target variable and analyzing the correlation between the variables, some observations were made:<\/p>\n\n<ul>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\"><b>Age<\/b> --> Most case of deaths is between 60-75 years<\/p><\/li>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\"><b>CPK<\/b> --> Normal range is 26-308 (mcg\/L). Most case of deaths is between 128-582 mcg\/L<\/p><\/li>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\"><b>ejection_fraction<\/b> --> Normal range is 50-70%. Most case of deaths is under 40%<\/p><\/li>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\"><b>platelets<\/b> --> Normal range is 150k-450k. Most case of deaths is between 200k-310k<\/p><\/li>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\"><b>serum_creatinine<\/b> --> Normal range is 0.6-1.35 (md\/dL). Most case of deaths is between 1-2 (md\/dL)<\/p><\/li>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\"><b>serum_sodium<\/b> --> Normal range is 135-145 (mmEq\/L). Most case of deaths is between 133-140 (mmEq\/L)<\/p><\/li>\n     <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\"><b>time<\/b> --> Most case of deaths is under 100 days<\/p><\/li>\n<\/ul>\n\n<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">We can take some conclusions with these observations:<\/p>\n\n<ol>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Age is proportional to heart failure;<\/p><\/li>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Most of the deaths cases have a \"serum_sodium\" in a normal range (between 135-145 (mmEq\/L)). For this dataset, this characteristis does not impact the number of deaths. Consequently, we can remove this variable;<\/p><\/li>\n    <li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">We can remove \"platelets\", \"creatinine_phosphokinase\" and \"anaemia\" according to correlation analysis.<\/p><\/li>\n<\/ol>\n\n<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">For the train and test phase, we will remove <i>\"platelets\"<\/i>, <i>\"creatinine_phosphokinase\"<\/i>, <i>\"serum_sodium\"<\/i> and <i>\"anaemia\"<\/i>. \ud83e\udd18<\/p>","2d48a673":"<html>\n<style>\nol.a {list-style-type: number;}\nul.b {list-style-type: circle;}\n<\/style>\n\n<dl>\n  <dt><b>age<\/b>: age of the patient (years) [40 to 95]<\/dt>\n  <dt><b>anaemia<\/b>: decrease of red blood cells or hemoglobin (boolean) [0 and 1]<\/dt>\n    <dd>- Hematocrit is the percentage of red cells in your blood.<\/dd>\n    <dd>- Normal levels from men 41% to 50% and woman 36% to 48%<\/dd>\n  <dt><b>creatinine phosphokinase (CPK)<\/b>: level of the CPK enzyme in the blood (mcg\/L) [23 to 7861]<\/dt>\n    <dd>- CPK is an enzyme in the body that causes the phosphorylation of creatine.<\/dd>\n    <dd>- CPK is found in the skeletal muscle, cardiac muscle, brain, bladder, stomach and colon.<\/dd>\n    <dd>- CPK leaks into the blood when a muscles tissue is damaged, and as such high levels of CPK is indicative of stress or injury to the heart or other muscles<\/dd>\n    <dd>- The CPK normal range for a male is between 39 \u2013 308 U\/L, while in females is between 26 \u2013 192 U\/L.<\/dd>\n\n  <dt><b>diabetes<\/b>: if the patient has diabetes (boolean) [0 and 1]<\/dt>\n  <dt><b>ejection fraction<\/b>: percentage of how much blood the left ventricle pumps out with each contraction [14 to 80]<\/dt>\n    <dd>- This indication of how well your heart is pumping out blood can help to diagnose and track heart failure<\/dd>\n    <dd>- A normal heart\u2019s ejection fraction may be between 50 and 70 percent.<\/dd>\n\n  <dt><b>high blood pressure<\/b>: if the patient has hypertension (boolean) [0 and 1]<\/dt>\n  <dt><b>platelets<\/b>: platelets in the blood (kiloplatelets\/mL) [25,100 to 850,000]<\/dt>\n    <dd>- Too many platelets can lead to heart attack and stroke<\/dd>\n    <dd>- A normal platelet count ranges from 150,000 to 450,000<\/dd>\n\n  <dt><b>serum creatinine<\/b>: level of serum creatinine in the blood (mg\/dL) [0,5 to 9,4]<\/dt>\n    <dd>- A creatinine test is a measure of how well your kidneys are performing their job of filtering waste from your blood.<\/dd>\n    <dd>- For men, 0.74 to 1.35 mg\/dL. For women, 0.59 to 1.04 mg\/dL.<\/dd>\n    <dd>- If a patient has high levels of serum creatinine, it may indicate renal dysfunction<\/dd>\n    \n  <dt><b>serum sodium<\/b>: level of serum sodium in the blood (mEq\/L) [113 to 148]<\/dt>\n    <dd>- Hyponatremia occurs when the concentration of sodium in your blood is low. Sodium helps regulate the amount of water that's in and around your cells.<\/dd>\n    <dd>- A normal blood sodium level is between 135 and 145 mEq\/L<\/dd>\n    <dd>- An abnormally low level of sodium in the blood might be caused by heart failure<\/dd>\n    \n  <dt><b>sex<\/b>: woman or man (binary) [0 and 1]<\/dt>\n  <dt><b>smoking<\/b>: if the patient smokes or not (boolean) [0 and 1]<\/dt>\n  <dt><b>time<\/b>: follow-up period (days) [4 to 285]<\/dt>\n  <dt><b>[target] death event<\/b>: if the patient deceased during the follow-up period (boolean) [0 and 1]<\/dt>\n\n<\/dl>\n<\/html>","19c27613":"<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:arial\">Checking if the target variable is balanced<\/h2>","1865b5a2":"<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:arial\">Conclusions about Feature Selection<\/h2>","4c76f621":"<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Considering this dataset, it seems that \"platelets\" and \"creatinine_phosphokinase\" have no correlation with the number of deaths \"DEATH_EVENT\". According to correlation table, both variables present a value next to 0.<\/p>\n<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Let's use another technic to analyze this correlation. Let's suppose that both variables have correlation with \"DEATH_EVENT\", i.e., our null hypothesis.<\/p>\n\n<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\"><b>Using stats.pearsonr, we can evaluate the p-value for both variables:<\/b><\/p>","50ab684e":"<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:arial\">4) Support Vector Regression<\/h2>","5d2b7a5f":"<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:arial\">5) Gradient Boosting Classifier<\/h2>","336d852d":"In this project, it was possible to evaluate the accurancy of the models according to the algorithms used previously. For a better visualization, here is presented an extract of the test data accuracy for each model evaluated","03ab73b1":"<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:arial\"> Import Library <\/h2>","31f78df9":"<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:arial\"> Variables description <\/h2>","9117fef8":"<center><h1 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\" > PREDICTING HEART ATTACK<\/h1><\/center>","4eec0291":"<h2 style = \"background:black;color:white;border:0;border-radius:3px;font-family:arial\">3) Naive Bayes<\/h2>","fe0b4c05":"<ul><li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">According to correlation table, \"Anaemia\" presents a value next to 0.<\/li><\/ul><\/p>\n<ul><li><p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\">Calculating p-value using scipy.stats, \"Anaemia\" presents a value > 0,05.<\/li><\/ul><\/p>\n\n<p style = \"color:black;font-weight:200;text-indent:0px;font-size:15px\"><b>With these two evidences, we can remove this variable.<\/b><\/p>","7c2e8e58":"<center><h1 style = \"background:black;color:white;border:0;border-radius:3px;font-family:verdana\" > 6 - Conclusion<\/h1><\/center>"}}