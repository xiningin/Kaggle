{"cell_type":{"fd3692d8":"code","2f5073cd":"code","681b113f":"code","f94eed5c":"code","8fec6758":"code","6d9effee":"code","8188b3bf":"code","d438092f":"markdown","561e8c53":"markdown","cd5e03c7":"markdown","55eafff8":"markdown","039caf64":"markdown","0f29824a":"markdown","a01ed694":"markdown"},"source":{"fd3692d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f5073cd":"\nimport pandas as pd\nfrom sklearn.datasets import make_classification, make_regression\nfrom sklearn.model_selection import train_test_split\n                                     \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\n\nfrom collections import Counter","681b113f":"X, y = make_classification(n_samples=500, \n                           n_features=10,\n                           random_state=42)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                    test_size=0.2, \n                                                    stratify=y, \n                                                    random_state=42)\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","f94eed5c":"\nclf_list = [('decision tree', DecisionTreeClassifier()),\n            ('logistic regression', LogisticRegression()),\n            ('knn', KNeighborsClassifier()),\n            ('naive bayes classifier', GaussianNB())]","8fec6758":"for model_tuple in clf_list:\n    model = model_tuple[1]\n    if 'random_state' in model.get_params().keys():\n        model.set_params(random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    acc = accuracy_score(y_pred, y_test)\n    print(f\"{model_tuple[0]}'s accuracy: {acc:.2f}\")","6d9effee":"voting_clf = VotingClassifier(clf_list, voting='hard')\nvoting_clf.fit(X_train, y_train)\ny_pred = voting_clf.predict(X_test)\nprint(f\"Voting Classifier's accuracy: {accuracy_score(y_pred, y_test):.2f}\")","8188b3bf":"voting_clf = VotingClassifier(clf_list, voting='soft')\nvoting_clf.fit(X_train, y_train)\ny_pred = voting_clf.predict(X_test)\nprint(f\"Voting Classifier's accuracy: {accuracy_score(y_pred, y_test):.2f}\")","d438092f":"# References:\n\n1.https:\/\/medium.com\/analytics-vidhya\/voting-classifier-in-machine-learning-9534504eba39\n\n2.https:\/\/levelup.gitconnected.com\/ensemble-learning-using-the-voting-classifier-a28d450be64d\n\n3.[https:\/\/iq.opengenus.org\/voting-classifier\/](http:\/\/)","561e8c53":"# Voting Classifier:\n\n  Voting classifier is one of the most powerful methods of ensemble methods. Many researchers and business people have adopted it because of the following nature.\n\n   1.Non-bias nature\n   \n   2.Different models are taken into consideration\n  \n# Types of Voting Classifier:\n\n* Soft voting\n* Hard voting\n\n***Soft Voting:***\n\n   The predicted probability vectors of each models are summed and averaged. The class with the highest value will be the winner and is given as output.\n![](https:\/\/imgur.com\/uPV0aEv.png)   \n \n   \n***Hard Voting:***\n\n   The classification output of all the individual models are calculated and mode value of the combined output is given as the final output value.\n![](http:\/\/imgur.com\/HqRWLNk.png)","cd5e03c7":"# Hard Voting Classifier","55eafff8":"# Fitting the Models","039caf64":"# Dataset Classification","0f29824a":"# Soft Voting Classifier","a01ed694":"**Pros:**\n\n* Voting classifier is a powerful method and can be a very good option when a single method shows bias towards a particular factor.\n* This method can be used to derive a generalized fit of all the individual models.\n\n**Cons:**\n\n* We should wisely chose the correct model to be used.\n* We have to make sure that we do not follow similar mathematical technique."}}