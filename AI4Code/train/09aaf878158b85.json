{"cell_type":{"37299760":"code","c7593e33":"code","5e09d90f":"code","7b36c7a7":"code","15261e4c":"code","2b7d99dd":"code","fd20016c":"code","1410b481":"code","96fa0538":"code","40a89de9":"code","b9e4edc2":"code","fcfac50f":"code","80eb3658":"code","b84befd2":"code","b91a2dee":"code","9969c8d1":"code","6c4ebfae":"code","5b3dc82a":"code","1544d2b0":"code","42288170":"code","19d24293":"code","bc5f10d6":"code","c4f3b3e1":"code","f3e0df89":"code","42b97bae":"code","893d2ddb":"code","7441d1a6":"code","0598ef1d":"markdown","81c57251":"markdown","4b8dde28":"markdown","9f4689b2":"markdown","d5e6094b":"markdown","7abbf8a8":"markdown","cffcecee":"markdown","8618bc82":"markdown","f5878edc":"markdown","df088c7a":"markdown","b0e7b865":"markdown","3039ff9a":"markdown","91517c66":"markdown","d45987e0":"markdown","2767eea7":"markdown","6269bbef":"markdown","df10b2c9":"markdown","2c9e3f3b":"markdown","2bec60f6":"markdown","baf2926a":"markdown","155625ad":"markdown","d266362d":"markdown","0507dcab":"markdown","df6a0cf8":"markdown","961dafa8":"markdown","f4723141":"markdown"},"source":{"37299760":"import os\nimport re\nimport numpy as np\nimport pandas as pd\nimport math\nimport statistics as st\nimport itertools\n\nfrom scipy import stats\nfrom statsmodels import robust\n\nimport warnings\nwarnings.filterwarnings('ignore')","c7593e33":"def correlation(data1, data2):\n    corr, p = stats.pearsonr(data1, data2)\n    if math.isnan(corr):\n        return 0\n    else:\n        return corr\n\ndef sma(x, y, z):\n    sum_ = 0\n    X = list(x)\n    Y = list(y)\n    Z = list(z)\n    for i in range(len(X)):\n        sum_ += abs(X[i]) + abs(Y[i]) + abs(Z[i])\n    return sum_ \/ len(X)\n\ndef calc_entropy(data):\n    entropy = stats.entropy(data, base=2)\n    if math.isinf(entropy) or math.isnan(entropy):\n        return -1\n    else:\n        return entropy\n\ndef energy(data):\n    sum_ = 0\n    for d in data:\n        sum_ += d ** 2\n        \n    return sum_ \/ len(data)\n\ndef iqr(data):\n    return np.subtract(*np.percentile(data, [75, 25]))","5e09d90f":"def read_dataset(dirName):\n    print(\"Reading Raw Data...\")\n    raw_datasets = []\n    raw_datalabels = []\n\n    for filename in os.listdir(dirName):\n        if filename.endswith(\".csv\"):\n            raw = pd.read_csv(os.path.join(dirName, filename))\n            cols = raw.columns\n            cols = cols.str.replace('([\\(\\[]).*?([\\)\\]])', '')\n            cols = cols.str.replace('\\s','_')\n            raw.columns = cols\n            to_drop = []\n            for col in raw.columns:\n                if not (\"_X_\" in col or \"_Y_\" in col or \"_Z_\" in col):\n                    to_drop.append(col)\n            raw = raw.drop(to_drop, axis=1)\n            column_names = raw.columns\n            raw = raw.drop_duplicates(keep='last')\n            raw_datasets.append(raw)\n            raw_datalabels.append(filename.split(\".\")[0])\n            \n    return raw_datasets, raw_datalabels, column_names","7b36c7a7":"def process_raw_data(raw_datasets, raw_datalabels, column_names):\n    print(\"Processing Data...\")\n    datasets = dict()\n    statistics = [\"mean\", \"mad\", \"max\", \"min\", \"std\", \"energy\", \"iqr\", \"entropy\"]\n    diff_col_names = []\n    for col in range(0,len(column_names),3):\n        diff_col_names.append(column_names[col][:len(column_names[col])-3])\n\n    for col in column_names:\n        for stat in statistics:\n            key = col + \"~\" + stat\n            datasets[key] = []\n\n    for col in diff_col_names:\n        datasets[col+\"_XY_~correlation\"] = []\n        datasets[col+\"_YZ_~correlation\"] = []\n        datasets[col+\"_ZX_~correlation\"] = []\n        datasets[col+\"_XYZ_~sma\"] = []\n\n    datasets[\"Activity\"] = []\n\n    for ind,raw_data in enumerate(raw_datasets):\n        for d in range(0, len(raw_data), 5):\n            if d+5 < len(raw_data):\n                data = raw_data[d:d+5]\n            else:\n                break\n            for c in diff_col_names:\n                col_X = c + \"_X_\"\n                col_Y = c + \"_Y_\"\n                col_Z = c + \"_Z_\"\n\n                datasets[col_X+\"~mean\"].append(st.mean(data[col_X])) # mean X\n                datasets[col_Y+\"~mean\"].append(st.mean(data[col_Y])) # mean Y\n                datasets[col_Z+\"~mean\"].append(st.mean(data[col_Z])) # mean Z\n\n                datasets[col_X+\"~mad\"].append(robust.mad(np.array(data[col_X]))) # median absolute deviation X\n                datasets[col_Y+\"~mad\"].append(robust.mad(np.array(data[col_Y]))) # median absolute deviation Y\n                datasets[col_Z+\"~mad\"].append(robust.mad(np.array(data[col_Z]))) # median absolute deviation Z\n\n                datasets[col_X+\"~max\"].append(max(data[col_X])) # maximum X\n                datasets[col_Y+\"~max\"].append(max(data[col_Y])) # maximum Y\n                datasets[col_Z+\"~max\"].append(max(data[col_Z])) # maximum Z\n\n                datasets[col_X+\"~min\"].append(min(data[col_X])) # minimum X\n                datasets[col_Y+\"~min\"].append(min(data[col_Y])) # minimum Y\n                datasets[col_Z+\"~min\"].append(min(data[col_Z])) # minimum Z\n\n                datasets[col_X+\"~std\"].append(st.stdev(data[col_X])) # standard deviation X\n                datasets[col_Y+\"~std\"].append(st.stdev(data[col_Y])) # standard deviation Y\n                datasets[col_Z+\"~std\"].append(st.stdev(data[col_Z])) # standard deviation Z\n\n                datasets[col_X+\"~energy\"].append(energy(data[col_X])) # energy X\n                datasets[col_Y+\"~energy\"].append(energy(data[col_Y])) # energy Y\n                datasets[col_Z+\"~energy\"].append(energy(data[col_Z])) # energy Z\n\n                datasets[col_X+\"~iqr\"].append(iqr(data[col_X])) # interquartile range X\n                datasets[col_Y+\"~iqr\"].append(iqr(data[col_Y])) # interquartile range Y\n                datasets[col_Z+\"~iqr\"].append(iqr(data[col_Z])) # interquartile range Z\n\n                datasets[col_X+\"~entropy\"].append(calc_entropy(data[col_X])) # entropy X\n                datasets[col_Y+\"~entropy\"].append(calc_entropy(data[col_Y])) # entropy Y\n                datasets[col_Z+\"~entropy\"].append(calc_entropy(data[col_Z])) # entropy Z\n\n                datasets[c+\"_XY_~correlation\"].append(correlation(data[col_X], data[col_Y])) # correlation between X and Y\n                datasets[c+\"_YZ_~correlation\"].append(correlation(data[col_Y], data[col_Z])) # correlation between Y and Z\n                datasets[c+\"_ZX_~correlation\"].append(correlation(data[col_Z], data[col_X])) # correlation between Z and X\n\n\n                datasets[c+\"_XYZ_~sma\"].append(sma(data[col_X], data[col_Y], data[col_Z]))\n\n            datasets[\"Activity\"].append(raw_datalabels[ind])\n            data = []\n        print(raw_datalabels[ind],\" collected and processed\")\n    print(\"Done\")\n    return datasets","15261e4c":"og_raw_dataset, og_raw_datalabels, og_column_names = read_dataset(\"OriginalRawDataSet\")\noriginal_dataset = process_raw_data(og_raw_dataset, og_raw_datalabels, og_column_names)\ndf = pd.DataFrame.from_dict(original_dataset, orient=\"columns\")\ndf.to_csv(\"Processed_DataSet\/OriginalProcessedData.csv\",index=False)\nprint(\"Saved to OriginalProcessedData.csv\")","2b7d99dd":"new_raw_dataset, new_raw_datalabels, new_column_names = read_dataset(\"NewRawDataSet\")\nnew_dataset = process_raw_data(new_raw_dataset, new_raw_datalabels, new_column_names)\ndf = pd.DataFrame.from_dict(new_dataset, orient=\"columns\")\ndf.to_csv(\"Processed_DataSet\/NewProcessedData.csv\",index=False)\nprint(\"Saved to NewProcessedData.csv\")","fd20016c":"def split_data(processed_file, test_size=0.3):\n    processed = pd.read_csv(\"Processed_DataSet\/\"+processed_file)\n\n    print(\"No of NAN in processed: {}\".format(processed.isnull().values.sum()))\n    print(\"No of duplicates in processed: {}\".format(sum(processed.duplicated())))\n\n    train, test = train_test_split(processed, test_size=test_size, shuffle=True)\n\n    y_train = train.Activity\n    X_train = train.drop(['Activity'], axis=1)\n    y_test = test.Activity\n    X_test = test.drop(['Activity'], axis=1)\n    \n    return X_train, y_train, X_test, y_test","1410b481":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import train_test_split","96fa0538":"print(\"Obtaining Original Processed Data\")\nog_X_train, og_y_train, og_X_test, og_y_test = split_data(\"OriginalProcessedData.csv\")\nog_labels = ['Sitting', 'Standing','Walking', 'WalkingDownStairs', 'WalkingUpStairs', 'Laying']\n\nprint(\"\\nObtaining New Processed Data\")\nnew_X_train, new_y_train, new_X_test, new_y_test = split_data(\"NewProcessedData.csv\")\nnew_labels = ['Cycling', 'Football', 'Swimming', 'Jogging', 'Pushups', 'JumpRope']","40a89de9":"# performs t-sne with different perplexity values and their repective plots\n\ndef perform_tsne(X_data, y_data, perplexities, markers, n_iter=1000):\n    \n    for index, perplexity in enumerate(perplexities):\n        # perform t-sne\n        print(\"\\nPerforming tsne with perplexity {} and with {} iterations at max\".format(perplexity, n_iter))\n        X_reduced = TSNE(verbose=2, perplexity=perplexity).fit_transform(X_data)\n        print('Done..')\n        \n        # prepare data for seaborn\n        print(\"Creating plot for this t-sne visualization\")\n        df = pd.DataFrame({'x':X_reduced[:,0], 'y':X_reduced[:,1], 'label':y_data})\n        \n        # draw the plot in appropriate palce in the grid\n        sns.lmplot(data=df, x='x', y='y', hue='label', fit_reg=False, height=8, palette=\"Set1\",markers=markers)\n        plt.title(\"perplexity : {} and max_iter: {}\".format(perplexity, n_iter))\n        plt.show()\n        print(\"Done\")","b9e4edc2":"perform_tsne(X_data = og_X_train, y_data = og_y_train, perplexities = [5,10,20], markers = ['^', 'v', 'o', 's', '1', '2'])","fcfac50f":"perform_tsne(X_data = new_X_train, y_data = new_y_train, perplexities = [5,10,20], markers = ['^', 'v', 'o', 's', '1', '2'])","80eb3658":"import matplotlib.colors as colors\n\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom datetime import datetime","b84befd2":"# function to plot the confusion matrix\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2\n    \n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n        \n    plt.tight_layout()\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')","b91a2dee":"# generic function to run any model specified\ndef perform_model(model, X_train, y_train, X_test, y_test, class_labels, cm_normalize=True, print_cm=True, cm_map=plt.cm.Greens):\n    # to store results at various phases\n    results = dict()\n    \n    # time at which model starts training\n    train_start_time = datetime.now()\n    print('training the model..')\n    model.fit(X_train, y_train)\n    print(\"Done\\n\\n\")\n    train_end_time = datetime.now()\n    results['training_time'] = train_end_time - train_start_time\n    print('training_time(HH:MM:SS.ms) - {}\\n\\n'.format(results['training_time']))\n    \n    # predict test data\n    print('Predicting test data')\n    test_start_time = datetime.now()\n    y_pred = model.predict(X_test)\n    test_end_time = datetime.now()\n    print('Done\\n\\n')\n    results['testing_time'] = test_end_time - test_start_time\n    print('testing_time(HH:MM:SS.ms) - {}\\n\\n'.format(results['testing_time']))\n    results['predicted'] = y_pred\n    \n    # calculate overall accuracty of the model\n    accuracy = metrics.accuracy_score(y_true=y_test, y_pred=y_pred)\n    # store accuracy in results\n    results['accuracy'] = accuracy\n    print('-----------------------')\n    print('|       Accuracy      |')\n    print('-----------------------')\n    print('\\n      {}\\n\\n'.format(accuracy))\n    \n    # confusion matrix\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    results['confusion_matrix'] = cm\n    if print_cm:\n        print('-----------------------')\n        print('|   Confusion Matrix  |')\n        print('-----------------------')\n        print('\\n {}'.format(cm))\n    \n    # plot confusion matrix\n    plt.figure(figsize=(8,8))\n    plt.grid(b=False)\n    plot_confusion_matrix(cm, classes=class_labels, normalize=True, title='Normalized Confusion Matrix', cmap=cm_map)\n    plt.show()\n    \n    # get classification report\n    print('-----------------------------')\n    print('|   Classification Report   |')\n    print('-----------------------------')\n    classification_report = metrics.classification_report(y_test, y_pred)\n    \n    # store report in results\n    results['classification_report'] = classification_report\n    print(classification_report)\n    \n    # add the trained model to the results\n    results['model'] = model\n    \n    return results","9969c8d1":"# function to print the gridsearch Attributes\ndef print_grid_search_attributes(model):\n    # Estimator that gave highest score among all the estimators formed in GridSearch\n    print('-----------------------')\n    print('|    Best Estimator   |')\n    print('-----------------------')\n    print('\\n\\t{}\\n'.format(model.best_estimator_))\n    \n    # parameters that gave best results while performing grid search\n    print('-----------------------')\n    print('|   Best Parameters   |')\n    print('-----------------------')\n    print('\\tParameters of best estimator : \\n\\n\\t{}\\n'.format(model.best_params_))\n    \n    # number of cross validation splits\n    print('--------------------------------')\n    print('|  No of CrossValidation sets  |')\n    print('--------------------------------')\n    print('\\n\\tTotal number of cross validation sets: {}\\n'.format(model.n_splits_))\n    \n    # Average cross validated score of the best estimator, from the Grid Search\n    print('-----------------------')\n    print('|      Best Score     |')\n    print('-----------------------')\n    print('\\n\\tAverage Cross Validate scores of best estimator : \\n\\n\\t{}\\n'.format(model.best_score_))","6c4ebfae":"from sklearn import svm\nfrom sklearn import neighbors\nfrom sklearn import linear_model\nfrom sklearn import tree","5b3dc82a":"rbf_kernel = svm.SVC(kernel='rbf')\nrbf_params = {'C':[55, 60, 65, 81, 90], 'gamma':['scale', 0.01, 0.03, 0.05, 0.07]}\nrbf_svc_grid = GridSearchCV(rbf_kernel, param_grid=rbf_params, n_jobs=-1, verbose=1)\nprint(\"Original DataSet\")\nog_rbf_svc_grid_results = perform_model(rbf_svc_grid, og_X_train, og_y_train, og_X_test, og_y_test,class_labels=og_labels)\nprint(\"\\nNew DataSet\")\nnew_rbf_svc_grid_results = perform_model(rbf_svc_grid, new_X_train, new_y_train, new_X_test, new_y_test,class_labels=new_labels)","1544d2b0":"print(\"Original DataSet\")\nprint_grid_search_attributes(og_rbf_svc_grid_results['model'])\nprint(\"\\nNew DataSet\")\nprint_grid_search_attributes(new_rbf_svc_grid_results['model'])","42288170":"poly_kernel = svm.SVC(kernel='poly')\npoly_params = {'C':[0.125, 0.5, 1, 2, 8, 16], 'degree': [1, 2, 3, 4, 5]}\npoly_svc_grid = GridSearchCV(poly_kernel, param_grid=poly_params, n_jobs=-1, verbose=1)\nprint(\"Original DataSet\")\nog_poly_svc_grid_results = perform_model(poly_svc_grid, og_X_train, og_y_train, og_X_test, og_y_test,class_labels=og_labels)\nprint(\"\\nNew DataSet\")\nnew_poly_svc_grid_results = perform_model(poly_svc_grid, new_X_train, new_y_train, new_X_test, new_y_test,class_labels=new_labels)","19d24293":"print(\"Original DataSet\")\nprint_grid_search_attributes(og_poly_svc_grid_results['model'])\nprint(\"\\nNew DataSet\")\nprint_grid_search_attributes(new_poly_svc_grid_results['model'])","bc5f10d6":"knn_params = {'n_neighbors':[7, 9, 11, 13, 17], 'weights': ['uniform','distance'], 'algorithm': ['ball_tree', 'kd_tree','brute']}\nknn_kernel = neighbors.KNeighborsClassifier()\nknn_grid = GridSearchCV(knn_kernel, param_grid=knn_params, n_jobs=-1, verbose=1)\nprint(\"Original DataSet\")\nog_knn_grid_results = perform_model(knn_grid, og_X_train, og_y_train, og_X_test, og_y_test,class_labels=og_labels)\nprint(\"\\nNew DataSet\")\nnew_knn_grid_results = perform_model(knn_grid, new_X_train, new_y_train, new_X_test, new_y_test,class_labels=new_labels)","c4f3b3e1":"print(\"Original DataSet\")\nprint_grid_search_attributes(og_knn_grid_results['model'])\nprint(\"\\nNew DataSet\")\nprint_grid_search_attributes(new_knn_grid_results['model'])","f3e0df89":"reg_params = {'C':[0.001, 0.125, 0.5, 1, 8, 16, 50], 'class_weight':['balanced',None], \n              'solver':[ 'newton-cg', 'sag', 'saga', 'lbfgs'], 'multi_class':['ovr', 'multinomial']}\nreg_kernel = linear_model.LogisticRegression()\nreg_grid = GridSearchCV(reg_kernel, param_grid=reg_params, n_jobs=-1, verbose=1)\nprint(\"Original DataSet\")\nog_reg_grid_results = perform_model(reg_grid, og_X_train, og_y_train, og_X_test, og_y_test,class_labels=og_labels)\nprint(\"\\nNew DataSet\")\nnew_reg_grid_results = perform_model(reg_grid, new_X_train, new_y_train, new_X_test, new_y_test,class_labels=new_labels)","42b97bae":"print(\"Original DataSet\")\nprint_grid_search_attributes(og_reg_grid_results['model'])\nprint(\"\\nNew DataSet\")\nprint_grid_search_attributes(new_reg_grid_results['model'])","893d2ddb":"dt_params = {'max_depth':[3, 5, 7, 9], 'splitter': ['best','random'], 'criterion': ['gini', 'entropy'], 'class_weight':['balanced',None]}\ndt_kernel = tree.DecisionTreeClassifier()\ndt_grid = GridSearchCV(dt_kernel, param_grid=dt_params, n_jobs=-1, verbose=1)\nprint(\"Original DataSet\")\nog_dt_grid_results = perform_model(dt_grid, og_X_train, og_y_train, og_X_test, og_y_test,class_labels=og_labels)\nprint(\"\\nNew DataSet\")\nnew_dt_grid_results = perform_model(dt_grid, new_X_train, new_y_train, new_X_test, new_y_test,class_labels=new_labels)","7441d1a6":"print(\"Original DataSet\")\nprint_grid_search_attributes(og_dt_grid_results['model'])\nprint(\"\\nNew DataSet\")\nprint_grid_search_attributes(new_dt_grid_results['model'])","0598ef1d":"## Setting up environment","81c57251":"## Function for Splitting up Data","4b8dde28":"## Apply t-sne on the data","9f4689b2":"## K-Nearest Neighbours","d5e6094b":"### Radial Basis Function","7abbf8a8":"## Function for Processing Raw Data","cffcecee":"## Setting up the environment","8618bc82":"### Performing Model","f5878edc":"## t-sne for New DataSet","df088c7a":"## Predicting Activities","b0e7b865":"### Polynomial","3039ff9a":"## Function for reading the Datasets","91517c66":"### Importing Different Models","d45987e0":"## Reading and Splitting Data","2767eea7":"## Processing Our Version of the Original","6269bbef":"## Logistic Regression","df10b2c9":"## t-sne for Original DataSet","2c9e3f3b":"## Processing Our New DataSet","2bec60f6":"## Support Vector Machines","baf2926a":"### Determine Best Parameters from GridSearch","155625ad":"# Data Analysis","d266362d":"## Functions for Statistical Calculations","0507dcab":"### Setting up Confusion Matrix","df6a0cf8":"## Modelling Data","961dafa8":"# Human Activity Recognition","f4723141":"## Decision Trees"}}