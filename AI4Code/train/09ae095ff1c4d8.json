{"cell_type":{"8e21d535":"code","3c3b466c":"code","898669a8":"code","00fa0310":"code","43ba6faa":"code","9b543143":"code","d12fb54a":"code","e3d9f5aa":"code","facf3a6c":"code","827add16":"code","0cba4d56":"code","e5045d64":"code","24f758bd":"code","5ccb6067":"code","129645a3":"code","527f532f":"code","4b8c127f":"code","3b487e41":"code","919f9e80":"code","3efd05b9":"code","e75c2ee4":"code","6ad24de6":"code","bac546aa":"code","697d9b89":"code","1ee79669":"code","eea8ce87":"code","67ce96d3":"code","b1ba1ce1":"code","f8c4de4a":"code","a671d310":"code","24c457c0":"code","f0a357d2":"code","b7e71192":"code","9c280d16":"code","041be4bf":"code","0621f0b9":"code","7b4fd7fd":"code","cd730195":"code","8d06ad1e":"code","c3c75237":"code","cab0182a":"code","949f416d":"code","a01fc94a":"code","4780a740":"code","b4043962":"code","3a899962":"code","ab712e41":"code","95ce072f":"code","977d7a89":"code","0e7ec1de":"code","637178e0":"code","6a23ceeb":"code","a7ad9153":"code","2c9626a6":"code","4a4c6f0b":"code","ae2be58e":"code","48322688":"code","552adbb2":"code","4af7978f":"code","1030dcec":"code","3f510b36":"code","3c1ca988":"code","7892210f":"code","1cc80852":"code","79aa8604":"code","63aa3395":"markdown","aaae31fb":"markdown","58a6d8ca":"markdown","fe5f61f0":"markdown","7ad0910a":"markdown","52d1b29b":"markdown","71906eb2":"markdown","75460f17":"markdown","9a666d14":"markdown","1f22321d":"markdown","c376bc22":"markdown","616c2a64":"markdown","ec111812":"markdown","a614e79d":"markdown","c50a0ef5":"markdown","f4a3ff67":"markdown","b800c263":"markdown","148acc15":"markdown","0cb80222":"markdown","58b06ccf":"markdown","a855d438":"markdown","42e7e4e9":"markdown","d1760e38":"markdown","70388264":"markdown","6be7db03":"markdown","971c35a2":"markdown","aaea16b9":"markdown","6dc8cf90":"markdown","7087e274":"markdown","5d25e891":"markdown","ceec566c":"markdown","2d143af8":"markdown","a5ac7774":"markdown","958461b0":"markdown","68a690ca":"markdown","99a8b6fa":"markdown","0f8e000b":"markdown","ec03a9fa":"markdown","3f20209c":"markdown","da1bae1e":"markdown","f9203017":"markdown","dffce951":"markdown","183dfa64":"markdown","1e6b72ef":"markdown","89495a55":"markdown","60f9686a":"markdown","74fa7145":"markdown","7b8f4990":"markdown","6567ed40":"markdown","5033acaf":"markdown","31f8148d":"markdown","67dd40f9":"markdown","363f7002":"markdown","2faac023":"markdown","82d7adaa":"markdown","2b7bd99e":"markdown","7535aaf5":"markdown","b8e9b6e3":"markdown","8541e3c4":"markdown"},"source":{"8e21d535":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom string import punctuation\n\nfrom gensim.models import KeyedVectors\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers.merge import concatenate\nfrom keras.models import Model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# #nlp augmentation\n!pip install --quiet googletrans\nfrom googletrans import Translator\n\n#for fast parallel processing\nfrom dask import bag, diagnostics","3c3b466c":"# load train data which is provided in the competition link\ndata_train=pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\n\ndata_text=pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\n\n","898669a8":"# here I try to plot EDA For the Given data\nplt.figure(figsize=[20,10])\nx=data_train.groupby(['language','label'])['label'].count()\nsns.countplot(x='language',hue='label',data=data_train)","00fa0310":"# lets see some example to understand it properly\n# so let's play with sherlock Dialogues\n\n# as we can pass list, tuple, iterator, generator in the text . so I try to pass a list of  sherlock Dialogues and try to translate it ..,.,.\n# we already import the required files above .,..\n\n# we have to create a object for the Translator\n\ntrans=Translator()\n\n# after creating object we call the translate from trans\n# 'en' to 'ja'\n\nTranslated=trans.translate([ 'You have a grand gift for silence, Watson. It makes you quite invaluable as a companion.','Partial deafness in ear.','First point of attack. '],dest='ja',src=\"auto\")\n\n# as we know it return list so we use iteration to see the transation\n\nfor i in Translated:\n    print(i.origin ,i.text)\n    print()\n\n# 'en' to 'hi'\nTranslated=trans.translate(['You have a grand gift for silence, Watson. It makes you quite invaluable as a companion.','Partial deafness in ear.','First point of attack. '],dest='hi',src=\"auto\")\n\nfor i in Translated:\n    print(i.origin ,i.text)\n    print()\n    \n# 'en' to 'hi'\nTranslated=trans.translate(['You have a grand gift for silence, Watson. It makes you quite invaluable as a companion.','Partial deafness in ear.','First point of attack. '],dest='is',src=\"auto\")\n\nfor i in Translated:\n    print(i.origin ,i.text)\n    print()    \n    \n","43ba6faa":"# these are the all languages supported in Googletrans\nfrom googletrans import LANGUAGES\n\nfor  lang in LANGUAGES:\n    print(LANGUAGES[lang])","9b543143":"# lets see some example to understand it properly\n# we use the object which we created above\n\n## BASIC USE\nprint(\"BASIC USE\")\nprint(trans.detect('\uc774 \ubb38\uc7a5\uc740 \ud55c\uae00\ub85c \uc4f0\uc5ec\uc84c\uc2b5\ub2c8\ub2e4.'))\n\nprint(trans.detect('\u0906\u092a\u0915\u0947 \u092a\u093e\u0938 \u092e\u094c\u0928, \u0935\u093e\u091f\u0938\u0928 \u0915\u0947 \u0932\u093f\u090f \u090f\u0915 \u092d\u0935\u094d\u092f \u0909\u092a\u0939\u093e\u0930 \u0939\u0948\u0964 \u092f\u0939 \u0906\u092a\u0915\u094b \u090f\u0915 \u0938\u093e\u0925\u0940 \u0915\u0947 \u0930\u0942\u092a \u092e\u0947\u0902 \u0915\u093e\u092b\u0940 \u0905\u092e\u0942\u0932\u094d\u092f \u092c\u0928\u093e\u0924\u093e \u0939\u0948\u0964'))\n\n## ADVANCE USE OF DETECT\n\nprint('ADVANCE USE OF DETECT')\n\nlangs = trans.detect(['\u0906\u092a\u0915\u0947 \u092a\u093e\u0938 \u092e\u094c\u0928, \u0935\u093e\u091f\u0938\u0928 \u0915\u0947 \u0932\u093f\u090f \u090f\u0915 \u092d\u0935\u094d\u092f \u0909\u092a\u0939\u093e\u0930 \u0939\u0948\u0964 \u092f\u0939 \u0906\u092a\u0915\u094b \u090f\u0915 \u0938\u093e\u0925\u0940 \u0915\u0947 \u0930\u0942\u092a \u092e\u0947\u0902 \u0915\u093e\u092b\u0940 \u0905\u092e\u0942\u0932\u094d\u092f \u092c\u0928\u093e\u0924\u093e \u0939\u0948\u0964', '\u65e5\u672c\u8a9e', '\u8033\u306e\u90e8\u5206\u7684\u306a\u96e3\u8074\u3002', 'le fran\u00e7ais'])\n\nfor i in langs:\n    print(i.lang,i.confidence)\n","d12fb54a":"# Here I created two function which I talked above\nfrom time import sleep\ndef inte(X):\n    sleep(1)\n    return(X+1)\ndef add(x,y):\n    sleep(1)\n    return(x+y)","e3d9f5aa":"# now here we use normal python coding for implementation\n%time\nx=inte(1)\ny=inte(2)\nz=add(x,y)\n","facf3a6c":"from dask import delayed\n%time\nx = delayed(inte)(1)\ny = delayed(inte)(2)\n\nz = delayed(add)(x,y)\n\n\n","827add16":"z.compute()","0cba4d56":"z.visualize()","e5045d64":"# creating a random list\n\ndata=[1,2,3,4,4,5,6,6,7,8,9]\n# create a empty list and then I call inte and store in yand then append it in results all this procees occure in a secquence\n%time\nresults = []\nfor x in data:\n    y = inte(x)\n    results.append(y)\n    \ntotal = sum(results)","24f758bd":"# we create allthe same things created above but using dask \n\n%time\nresults=[]\n\nfor x in data:\n    y=delayed(inte)(x)\n    results.append(y)\ntotal = delayed(sum)(results) \n","5ccb6067":"total.compute()","129645a3":"total.visualize()","527f532f":"# take example of numpy\n\nimport numpy as np\n\nx=np.ones(15)\nx","4b8c127f":"# implementing dask array\nimport dask.array as da\n\nx=da.ones(15,chunks=(5,))\n\nx","3b487e41":"x.visualize()","919f9e80":"x.sum().compute()","3efd05b9":"import dask.array as da\n\nx=da.ones((15,15,15),chunks=(5,5,5))\nx","e75c2ee4":"x.compute()","6ad24de6":"x.visualize()","bac546aa":"import dask.array as da\n\nx=da.ones((15,15),chunks=(5,5))\nx","697d9b89":"x.compute()","1ee79669":"x.visualize()","eea8ce87":"import dask.dataframe as dd\n\ndf=dd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\n\ndf.head()","67ce96d3":"df.language.value_counts().compute()","b1ba1ce1":"df.language.value_counts().visualize(rankdir=\"LR\", size=\"10, 10!\")","f8c4de4a":"# A SORT EXAMPLE TO UNDERSTAND THE BAG\nimport dask.bag as db\nb = db.from_sequence(range(5))\nlist(b.filter(lambda x: x % 2 == 0).map(lambda x: x * 10))\n","a671d310":"# TASK GRAPG \n# here 5 chunks are made for parallel processing\nb.visualize()","24c457c0":"from concurrent.futures import ThreadPoolExecutor\nfrom time import sleep\n\ndef task(message):\n    sleep(2)\n    return message\n\ndef main():\n    executor=ThreadPoolExecutor(5)\n    future=executor.submit(task,(\"completed\"))\n    print(future.done())\n    \n    sleep(2)\n    \n    print(future.done())\n\n    print(future.result())\n\nif __name__ == '__main__':\n    main()\n    \n    ","f0a357d2":"from concurrent.futures import ThreadPoolExecutor\nfrom concurrent.futures import as_completed\nvalues = [2,3,4,5]\n\ndef square(n):\n    return n * n\n   \n\ndef main():\n    executor=ThreadPoolExecutor(max_workers = 3)\n    results = executor.map(square, values)\n    for result in results:\n          print(result)\n            \nif __name__ == '__main__':\n   main()","b7e71192":"from concurrent.futures import ProcessPoolExecutor\nfrom time import sleep\n\n\ndef task(message):\n    sleep(2)\n    return message\n\ndef main():\n    executor = ProcessPoolExecutor(5)\n    future = executor.submit(task, (\"Completed\"))\n    print(future.done())\n    sleep(2)\n    print(future.done())\n    sleep(10)\n    print(future.done())\n    print(future.result())\n    \n    \nif __name__ == '__main__':\n    main()    ","9c280d16":"from dask.distributed import Client\n\n#  # start local workers as processes\nclient= Client()\n\n# or start local workers as threads\n\nclient =Client()","041be4bf":"def inc(x):\n    return x + 1\n\ndef add(x, y):\n    return x + y\n\na=client.submit(inc,10)\n\na.result()","0621f0b9":"import dask\ndask.config.set(scheduler='threads')","7b4fd7fd":"import dask.multiprocessing\ndask.config.set(scheduler='processes')  # overwrite default with multiprocessing scheduler","cd730195":"import dask\ndask.config.set(scheduler='synchronous')  # overwrite default with single-threaded scheduler","8d06ad1e":"from dask.distributed import Client\nclient = Client()\n# or\nclient = Client(processes=False)","c3c75237":"#  Now I create fuction for translation \n\n# As a attribute we pass sequence as a text and lang as language in which we want to translate\n\ndef translation(sequence,lang):\n      #instantiate translator\n      \n    translator=Translator()\n    \n    # translate to new language and convert back it to orig.\n    \n    translated=translator.translate(sequence,dest= lang).text\n    \n    return translated\n\n\ndef translation_parallel(dataset, lang):\n    prem_bag=bag.from_sequence(dataset['premise'].tolist()).map(lambda x: translation(x, lang = lang))\n    \n    hypo_bag=bag.from_sequence(dataset['hypothesis'].tolist()).map(lambda x: translation(x, lang= lang))\n    \n    with diagnostics.ProgressBar():\n        prems = prem_bag.compute()\n            \n        hyps = hypo_bag.compute()\n\n    #pair premises and hypothesis\n    dataset[['premise', 'hypothesis']] = list(zip(prems, hyps))\n    \n    \n    return dataset\n","cab0182a":"train_data=translation_parallel(data_train,'en')\n\n","949f416d":"text_data=translation_parallel(data_text,'en')","a01fc94a":"train_data","4780a740":"text_data","b4043962":"df=train_data[['premise','hypothesis','label']]\n\ndf_text=text_data[['premise','hypothesis']]","3a899962":"from dask import dataframe as dd\n\nmy_dask_df = dd.from_pandas(df, npartitions=4)\nmy_dask_df_text = dd.from_pandas(df_text, npartitions=4)\nmy_dask_df.persist()\nmy_dask_df_text.persist()","ab712e41":"def remove_noice(text):\n    text=re.sub('[^a-zA-Z]',' ',text)\n    text=text.lower()\n    text=text.split()\n    \n    return text\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet as wn\nlemmatizer=WordNetLemmatizer()\ncorpus=[]\n\ndef remove_stopwords(text):\n    \n    wn.ensure_loaded()\n    text=[lemmatizer.lemmatize(word) for word in text if word not in stopwords.words('english')]\n    text=' '.join(text)\n    \n    return text\n\ndef clean_text(df):\n    df['premise']=df.premise.map(remove_noice).map(remove_stopwords)\n    \n    df['hypothesis']=df.hypothesis.map(remove_noice).map(remove_stopwords)\n    \n    return df","95ce072f":"df=my_dask_df.map_partitions(clean_text,meta=df)\ndf_text=my_dask_df_text.map_partitions(clean_text,meta=df_text)","977d7a89":"df.persist()\ndf_text.persist()","0e7ec1de":"y= df['label'].compute()\ndf=df.drop('label',axis=1)\ndf=df.persist()\n\n","637178e0":"df[\"comb\"] = df.apply(lambda x:\" \".join(x), axis=1,meta=(\"str\"))\ndf_text[\"comb\"]=df_text.apply(lambda x:\" \".join(x), axis=1,meta=(\"str\"))","6a23ceeb":"df.persist()","a7ad9153":"x=df[\"comb\"].compute().tolist()\n\nx_text=df_text[\"comb\"].compute().tolist()","2c9626a6":"import tensorflow as tf","4a4c6f0b":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept ValueError:\n    strategy = tf.distribute.get_strategy() ","ae2be58e":"from tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\n\n\n\n### vocabulary size\n\nvoc_size=10000\n\nsent_length=60\n\ndef oneEmbed(x):\n    \n    onehot_repr=[one_hot(words,voc_size)for words in x]\n    \n    embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n    \n    return embedded_docs\n    \nx=oneEmbed(x)\nx_text=oneEmbed(x_text)","48322688":"x_text","552adbb2":"### Creating Model\n\n\n    \nembedding_vector_features= 40\n    \nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(10,return_sequences=True))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\n\nmodel.add(Dense(3,activation='softmax'))\n    \nopt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.2, nesterov=True)\n    \nmodel.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n    \n    ","4af7978f":"x_final=np.array(x)\nx_final_text=np.array(x_text)\ny_final=np.array(y).reshape((-1,1))","1030dcec":"from dask_ml.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(x_final,y_final,test_size=0.2, random_state=40)","3f510b36":"y_train.shape\n","3c1ca988":"model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=50,batch_size=324)","7892210f":"y_pred1=model.predict_classes(x_test)\nprint(y_pred1)\ny_pred=model.predict_classes(x_final_text)\nprint(y_pred)","1cc80852":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test,y_pred1)","79aa8604":"from sklearn.metrics import accuracy_score\n\naccuracy_score(y_test,y_pred1)","63aa3395":"Dask is composed of two parts:\n\n* Dynamic task scheduling - optimized for computation\n* \u201cBig Data\u201d collections - like parallel arrays, dataframes, and lists that extend common interfaces like NumPy, Pandas\n\n\nFamiliar user interface as follows :\n\n* Dask DataFrame mimics Pandas \n* Dask Array mimics NumPy \n* Dask Bag mimics iterators, Toolz, and PySpark\n* Dask Delayed mimics for loops and wraps custom code \n* concurrent.futures interface provides general submission of custom tasks\n\nfor more detailed info follow LINK-https:\/\/docs.dask.org\/en\/latest\/\n","aaae31fb":"Now let's see how a detector works :\n\n## **detect(text)**  \n\nDetect language of the input text\n\n**Parameters**:\ttext (UTF-8 str; unicode; string sequence (list, tuple, iterator, generator)) \u2013 The source text(s) whose language you want to identify. Batch detection is supported via sequence input.\n\n**Return type**:\tlist (when a list is passed)","58a6d8ca":"**This ran immediately, since nothing has really happened yet.The z object is a lazy Delayed object. This object holds everything we need to compute the final result, including references to all of the functions that are required and their inputs and relationship to one-another.**\n","fe5f61f0":"# **Googletrans**\n\nGoogletrans is a free and unlimited python library that implemented Google Translate API. This uses the Google Translate Ajax API to make calls to such methods as detect and translate.\n","7ad0910a":"LINK -https:\/\/docs.dask.org\/en\/latest\/bag.html\n\n\nALL the function that are use in the bag are provied in the link -https:\/\/docs.dask.org\/en\/latest\/bag-api.html#dask.bag.Bag.map","52d1b29b":"**NOTE - THIS NOTEBOOK IS CONTINUE TO BE EDITED .I HOPE YOU ALL LIKE MY EFFORT AND IF U FIND IT VALUABLE PLEASE UPVOTE AND PUT COMMENT IF ANY DOUBT**","71906eb2":"# **Contents**","75460f17":"Pandas is great for tabular datasets that fit in memory. Dask becomes useful when the dataset you want to analyze is larger than your machine's RAM. dask.dataframe will scale to datasets much larger than memory. \"dask.dataframe\" module implements a blocked parallel DataFrame object that mimics a large subset of the Pandas DataFrame.One Dask DataFrame is comprised of many in-memory pandas DataFrames separated along the index. One operation on a Dask DataFrame triggers many pandas operations on the constituent pandas DataFrames in a way that is mindful of potential parallelism and memory constraints.\n\n","9a666d14":"IT IS BIT FASTER THEN PANDAS DATA FRAME. IN THIRD CIRCLE IT IS CALCULATED THAT HOW MUCH CHUNK IS NEEDED TO MINIMIXE THE CORE USE.","1f22321d":"## If you find my kernel useful and my efforts appreciable, Please Upvote it , it motivates me to write more Quality content and you can follow me on kaggle for further updates.\n\n**LINK**-https:\/\/www.kaggle.com\/adityajohnny","c376bc22":"**IMPORTING SOME FILES FOR FURTHER USES**","616c2a64":"Let see some basic overview of the above interface as follows:\n\n# **Dask Array** \n\nDask Array implements a subset of the NumPy ndarray interface using blocked algorithms, cutting up the large array into many small arrays. This lets us compute on arrays larger than memory using all of our cores. We coordinate these blocked algorithms using Dask graphs.\n\nDask Array is used in fields like atmospheric and oceanographic science, large scale imaging, genomics, numerical algorithms for optimization or statistics, and more.\n\n![](http:\/\/docs.dask.org\/en\/latest\/_images\/dask-array-black-text.svg)","ec111812":"FOR MORE DETAIL STUDY OF FUNCTIONS USED IN FUTURES FOLLOW LINK -https:\/\/docs.dask.org\/en\/latest\/futures.html#start-dask-client","a614e79d":"# **Process Pool Executor** -","c50a0ef5":"Before using the Googletrans let's know some more thing about this :\n\n   **Install**- for installation use **\"!pip install --quiet googletrans\"** as used above in the import section ...\n   \n   **Problem and solution**- In our problem we have to translate the language so after installation of googletrans we import  Translator like this **\"from googletrans import Translator\"** as we used above in the import section.\n   \n   \n   \n  **Now let's see how a translator works** :\n   \n   \n## **translate(text, dest='en', src='auto')**\n   Translate text from source language to destination language\n\n**Parameters:**\n**text** (UTF-8 str; unicode; string sequence (list, tuple, iterator, generator)) \u2013 The source text(s) to be translated. Batch translation is supported via sequence input.\n\n**dest**\u2013 The language to translate the source text into. The value should be one of the language codes listed in googletrans.LANGUAGES or one of the language names listed in googletrans.LANGCODES.\ndest \u2013 str; unicode\n\n**src** \u2013 The language of the source text. The value should be one of the language codes listed in googletrans.LANGUAGES or one of the language names listed in googletrans.LANGCODES. If a language is not specified, the system will attempt to identify the source language automatically.\nsrc \u2013 str; unicode\n\n**Return type**:\t\nTranslated\n\n**Return type**: list (when a list is passed)\n   \n   \nFOR MORE INFO FOLLOW LINK-https:\/\/py-googletrans.readthedocs.io\/en\/latest\/ ,\n                          https:\/\/www.youtube.com\/watch?v=yRFkI8miPHA\n\n","f4a3ff67":"## **Task Graph**","b800c263":"For more depth you can follow this Link-https:\/\/docs.dask.org\/en\/latest\/array-creation.html","148acc15":"Chunks are basically, as the name suggests, chunks of your big arrays split into smaller arrays (in Dask -> Numpy ndarrays).\nThere exists a certain sweet spot of computational efficiency (parallelization potential) and memory capability (in-memory computations) for which a chunk size is optimal.\nGiven your amount of CPU cores, available RAM and file size, Dask find the optimal way to split your data to chunks which would be best optimally processed by your machine.\n\nYou might ask: \"Isn't fetching only a certain number of data rows each time is sufficient?\" \nWell not exactly, the bottleneck maybe be at the column dimension (e.g. 10 rows, 2M columns), so row operations should split each row into chunks of columns for more efficient computation.","0cb80222":"Dask Bag implements operations like map, filter, fold, and groupby on collections of generic Python objects. It does this in parallel with a small memory footprint using Python iterators.Dask bags are often used to parallelize simple computations on unstructured or semi-structured data like text data, log files, JSON records, or user defined Python objects.\n\nExecution on bags provide two benefits:\n\nParallel: data is split up, allowing multiple cores or machines to execute in parallel\n\nIterating: data processes lazily, allowing smooth execution of larger-than-memory data, even on a single machine within a single partition","58b06ccf":"# **Futures**","a855d438":"![](http:\/\/docs.dask.org\/en\/latest\/_images\/dask-dataframe.svg)","42e7e4e9":"# **Task Graph**","d1760e38":"In this notebook I cover all those topics related to the project which I am going to use. It will cover the Following: \n\n* Googletrans\n* Dask \n* nltk\n* Stemming And Lemmatization\n* Bag of words\n* TF-IDF\n* Word2vec\n* Word Embedding\n* RNN's\n* LSTM's \n\n\nIN THIS NOTEBOOK EVERY SECTION CONTAIN THREE SECTION :    \n \n* Basic Overview\n* Understanding(attach links of articles and videos to learn about the topic in depth)\n* Code-Implementation and Code Explanation","70388264":"**Single Thread :**\n\nThe single-threaded synchronous scheduler executes all computations in the local thread with no parallelism at all. This is particularly valuable for debugging and profiling, which are more difficult when using threads or processes.\n\nFor example, when using IPython or Jupyter notebooks, the **%debug, %pdb, or %prun** magics will not work well when using the parallel Dask schedulers.However, if you run into an exception and want to step into the debugger, you may wish to rerun your computation under the single-threaded scheduler where these tools will function properly.","6be7db03":"### **INTRODUCTION**\n\nNatural Language Processing,is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable.\n\nLINKs- \n* https:\/\/becominghuman.ai\/a-simple-introduction-to-natural-language-processing-ea66a1747b32#:~:text=Natural%20Language%20Processing%2C%20usually%20shortened,a%20manner%20that%20is%20valuable.\n\n* https:\/\/www.youtube.com\/watch?v=xvqsFTUsOmc\n","971c35a2":"Dask supports a real-time task framework that extends Python\u2019s **concurrent.futures** interface.This interface is good for arbitrary task scheduling like dask.delayed,but is immediate rather than lazy, which provides some more flexibility in situations where the computations may evolve over time.These features depend on the second generation task scheduler found in dask.distributed (which, despite its name, runs very well on a single machine).\n\nNow before moving with futures we have to know about the councurrent.futures.......","aaea16b9":"### As we see in our given data there are around 10+ diffrent languages and most common is \"English\" .So we try to convert all other languages text to \"English\" language.For doing so we have to take help of **Googletrans** .","6dc8cf90":"The Python map() function is widely used in a number of tasks. One such task is to apply a certain function to every element within iterables. Similarly, we can map all the elements of an iterator to a function and submit these as independent jobs to out ThreadPoolExecutor. \n\nLINK-https:\/\/docs.python.org\/3\/library\/concurrent.futures.html","7087e274":"Hello Kagglers, I hope u all are doing well. One day I am playing with my Alexa and doing some crazy pieces of stuff and I found it exciting how it's working. So I decided to know more about it. So I google some stuff and found a term **NPL**.**Natural Language Processing**, usually shortened as **NLP**, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language. The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable.After reading this I decided to do some work on it and while searching I found a new competition on Kaggle.\n\nLINK- https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson\n\nAs I joined the competition and since I was a complete beginner with Deep Learning Techniques for NLP,I faced lot of problem while entering the competition.**\u201cLife is infinitely stranger than anything which the mind of man could invent.\u201d** So I think there so many kaggler facing same problem that's why i decided to contribute to the society .After searching and learning some good stuff I decided to share with you all.\n\n **\"I invite you all to come and learn alongside with me\"**","5d25e891":"**TIME TO SEE SOME MORE GOOD STUFF**\n\n# Parallelizing a \"for loop\"","ceec566c":"***\u201cMy name is Sherlock Holmes. It is my business to know what other people do not know.\u201d***","2d143af8":"**Local Processes :**\n\nThe multiprocessing scheduler executes computations with a local multiprocessing.Pool. It is lightweight to use and requires no setup. Every task and all of its dependencies are shipped to a local process, executed, and then their result is shipped back to the main process. This means that it is able to bypass issues with the GIL and provide parallelism even on computations that are dominated by pure Python code, such as those that process strings, dicts, and lists.\n\n**This is common in basic data ingestion workloads, such as those are common in Dask Bag, where the multiprocessing scheduler is the default.**","a5ac7774":"**Local Threads :**\n\nThe threaded scheduler executes computations with a local multiprocessing.pool.ThreadPool. It is lightweight and requires no setup. It introduces very little task overhead (around 50us per task) and, because everything occurs in the same process, it incurs no costs to transfer data between tasks. However, due to Python\u2019s Global Interpreter Lock (GIL), this scheduler only provides parallelism when your computation is dominated by non-Python code, as is primarily the case when operating on numeric data in NumPy arrays, Pandas DataFrames, or using any of the other C\/C++\/Cython based projects in the ecosystem.\n\nThe threaded scheduler is the default choice for Dask Array, Dask DataFrame, and Dask Delayed. However, if your computation is dominated by processing pure Python objects like strings, dicts, or lists, then you may want to try one of the process-based schedulers ","958461b0":"**let's see how above code works**\n\nWith the help of concurrent.futures module and its concrete subclass Executor, we can easily create a pool of threads. For this, we need to construct a ThreadPoolExecutor with the number of threads we want in the pool. By default, the number is 5.Then we can submit a task to the thread pool. When we submit() a task, we get back a Future.The Future object has a method called done(), which tells if the future has resolved.With this, a value has been set for that particular future object. When a task finishes, the thread pool executor sets the value to the future object.\n\nIn the above example, a ThreadPoolExecutor has been constructed with 5 threads. Then a task, which will wait for 2 seconds before giving the message, is submitted to the thread pool executor. As seen from the output, the task does not complete until 2 seconds, so the first call to done() will return False. After 2 seconds, the task is done and we get the result of the future by calling the result() method on it.\n\n\nLET'S SEE SOME MORE EXAMPLES","68a690ca":"**Submit Tasks**\n\n* Client.submit\n* Client.map\n* Future.result\n","99a8b6fa":"## **NOW WE ARE READY TO LEARN FUTURES**\n\n**Start Dask Client** -You must start a Client to use the futures interface. This tracks state among the various worker processes or threads","0f8e000b":"## **Now move to our original data on which we have to apply translator**","ec03a9fa":"# **THREAD POOL EXECUTOR** - ","3f20209c":"Dask DataFrames coordinate many Pandas DataFrames\/Series arranged along the index. A Dask DataFrame is partitioned row-wise, grouping rows by index value for efficiency. These Pandas objects may live on disk or on other machines.\n\n\nLink-https:\/\/docs.dask.org\/en\/latest\/dataframe.html","da1bae1e":"![](http:\/\/docs.dask.org\/en\/latest\/_images\/dask-overview.svg)","f9203017":"Those two increment calls could be called in parallel, because they are totally independent of one-another.But since I want to keep the original functions for comparison, I'll call it directly.\n\n\n\n","dffce951":"**NOTE** :\nBy default, dask.bag uses dask.multiprocessing for computation. As a benefit, Dask bypasses the GIL and uses multiple cores on pure Python objects. As a drawback, Dask Bag doesn\u2019t perform well on computations that include a great deal of inter-worker communication. For common operations this is rarely an issue as most Dask Bag workflows are embarrassingly parallel or result in reductions with little data moving between workers.\n\nBecause the multiprocessing scheduler requires moving functions between multiple processes, we encourage that Dask Bag users also install the cloudpickle library to enable the transfer of more complex functions.\n\nSome operations, like groupby, require substantial inter-worker communication. On a single machine, Dask uses partd to perform efficient, parallel, spill-to-disk shuffles. When working in a cluster, Dask uses a task based shuffle.","183dfa64":"# **Dask**","1e6b72ef":"All of the large-scale Dask collections like Dask Array, Dask DataFrame, and Dask Bag and the fine-grained APIs like delayed and futures generate task graphs where each node in the graph is a normal Python function and edges between nodes are normal Python objects that are created by one task as outputs and used as inputs in another task. After Dask generates these task graphs, it needs to execute them on parallel hardware. This is the job of a task scheduler. Different task schedulers exist, and each will consume a task graph and compute the same result, but with different performance characteristics.\n\n![](http:\/\/docs.dask.org\/en\/latest\/_images\/collections-schedulers.png)\n\n\n\nDask has two families of task schedulers:\n\n* Single machine scheduler:  This scheduler provides basic features on a local process or thread pool. This scheduler was made first and is the    default. It is simple and cheap to use, although it can only be used on a single machine and does not scale.\n\n* Distributed scheduler:This scheduler is more sophisticated, offers more features, but also requires a bit more effort to set up. It can run locally or distributed across a cluster.\n\n\n**NOW THE QUESTION IS THAT WHICH ONE TO USE AND WHEN**\n\nSome criterias are as follows:\n\n* Local Threads\n* Local Processes\n* Single Thread\n* Dask Distributed (local)\n* Dask Distributed (Cluster) LINK- https:\/\/distributed.dask.org\/en\/latest\/\n","89495a55":"**Let's play with basic concepts**\n\nI use to make to basic function like inte and add and apply some time delay in each of the function.The basic idea here is to see how the paralle processing of the dask is faster then then normal numpy and python.","60f9686a":"When to use ProcessPoolExecutor and ThreadPoolExecutor?\n\nNow that we have studied about both the Executor classes \u2013 ThreadPoolExecutor and ProcessPoolExecutor, we need to know when to use which executor. We need to choose ProcessPoolExecutor in case of CPU-bound workloads and ThreadPoolExecutor in case of I\/O-bound workloads.\n\nIf we use ProcessPoolExecutor, then we do not need to worry about GIL because it uses multiprocessing. Moreover, the execution time will be less when compared to ThreadPoolExecution. Consider the following Python script example to understand this.\n\n\n* I\/O bound refers to a condition in which the time it takes to complete a computation is determined principally by the period spent waiting for input\/output operations to be completed\n\n* In computer science, a computer is CPU-bound (or compute-bound) when the time for it to complete a task is determined principally by the speed of the central processor: processor utilization is high, perhaps at 100% usage for many seconds or minutes.\\\n\nLINK-https:\/\/docs.python.org\/3\/library\/concurrent.futures.html","74fa7145":"# **Scheduling**","7b8f4990":"## **LIST OF SUPPORTED LANGUAGES**","6567ed40":"# **DASK DATAFRAME**","5033acaf":"**concurrent.futures** -The concurrent.futures module provides a high-level interface for asynchronously executing callables.\nThe asynchronous execution can be performed with threads, using **ThreadPoolExecutor**, or separate processes, using **ProcessPoolExecutor**.Both implement the same interface, which is defined by the **abstract Executor class**.\n\n\n**Abstract Executor Class** - Executor class is an abstract class which is not used directly but through its concrete subclasses :\n\n* ThreadPoolExecutor  (uses multi threading )\n\n* ProcessPoolExecutor (uses multi-processing )\n\n## **NOTE** - In both case, we get a pool of threads or processes and we can submit tasks to this pool. The pool would assign tasks to the available resources (threads or processes) and schedule them to run.","31f8148d":"# **ABOUT THIS NOTEBOOK**","67dd40f9":"**TASK GRAPH** -  Task graphs where each node in the graph is a normal Python function and edges between nodes are normal Python objects that are created by one task as outputs and used as inputs in another task.","363f7002":"Dask represents parallel computations with task graphs. These directed acyclic graphs may have arbitrary structure, which enables both developers and users the freedom to build sophisticated algorithms and to handle messy situations not easily managed by the map\/filter\/groupby paradigm common in most data engineering frameworks.Basically dask is a flexible library for parallel computing in Python.","2faac023":"**\u201cYour life is not your own. Keep your hands off it.\u201d**","82d7adaa":"**Task Graph**","2b7bd99e":"# **Dask Bag**","7535aaf5":"In the above code is exicuted line by line or we say sequentially .It take around 8.34 ms\n\n**NOW WE TRY DASK**\n\n**delayed**-Sometimes problems don\u2019t fit into one of the collections like dask.array or dask.dataframe. In these cases, users can parallelize custom algorithms using the simpler dask.delayed interface. This allows one to create graphs directly with a light annotation of normal python code.","b8e9b6e3":"**Let's see what happned here **\n\nWith the help of the concurrent.futures module and its concrete subclass Executor, we can easily create a pool of process. For this, we need to construct a ProcessPoolExecutor with the number of processes we want in the pool. By default, the number is 5. This is followed by submitting a task to the process pool.\n\nWe will now consider the same example that we used while creating thread pool, the only difference being that now we will use ProcessPoolExecutor instead of ThreadPoolExecutor .\n\n\n\n","8541e3c4":"**Dask Distributed (local) :**\n\nThe Dask distributed scheduler can either be setup on a cluster or run locally on a personal machine. Despite having the name \u201cdistributed\u201d, it is often pragmatic on local machines for a few reasons:\n\n* It provides access to asynchronous API, notably Futures\n* It provides a diagnostic dashboard that can provide valuable insight on performance and progress\n* It handles data locality with more sophistication, and so can be more efficient than the multiprocessing scheduler on workloads that require multiple processes"}}