{"cell_type":{"1393ca98":"code","ea0e4803":"code","1fd3226a":"code","53ab7c00":"code","4534ae02":"code","ea1e8ad5":"code","795cb4c3":"code","93e59844":"code","cd7794a0":"code","578bbd8f":"code","cba75467":"code","897642eb":"code","13479d7d":"code","01c487c2":"code","156f7356":"code","221dd632":"code","5686b936":"code","72eca7ff":"code","eff60a58":"code","286f249c":"code","6b37d9fa":"code","12634d2b":"code","02772fdd":"code","266e8652":"code","9bfd2c1f":"code","29c3602f":"code","0b154f21":"code","25fb85fe":"code","6bd60dc9":"code","32dd192a":"code","09e71567":"code","cc0a76ec":"code","aef8d5c3":"code","45eb0c84":"code","53b5b6e1":"markdown","bfcd9b14":"markdown","fe8211ae":"markdown","7f41271c":"markdown","e67f8d97":"markdown","4febf6b7":"markdown","931173e1":"markdown","a61b358f":"markdown","a6cc6e13":"markdown","73fe8bd4":"markdown","bac73e89":"markdown","f82cfec6":"markdown","9196a392":"markdown","4fea5adc":"markdown","d4faa419":"markdown","cc32d8ef":"markdown"},"source":{"1393ca98":"import tensorflow as tf\nprint(tf.__version__)","ea0e4803":"# Load the diabetes dataset\nfrom sklearn.datasets import load_diabetes\n\ndiabetes_dataset = load_diabetes()\nprint(diabetes_dataset['DESCR'])","1fd3226a":"# Save the input and target variables\n\ndata =  diabetes_dataset[\"data\"]\ntargets = diabetes_dataset[\"target\"]","53ab7c00":"\ntargets.shape","4534ae02":"# Normalise the target data (this will make clearer training curves)\ntargets =  (targets - targets.mean(axis = 0))\/targets.std()\ntargets","ea1e8ad5":"# Split the data into train and test sets\n\nfrom sklearn.model_selection import train_test_split\ntrain_data, test_data , train_targets , test_targets = train_test_split(data,targets,test_size = 0.1)\n\nprint(train_data.shape,train_targets.shape)\nprint(test_data.shape,test_targets.shape)","795cb4c3":"train_data.shape[1]","93e59844":"# Build the model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten,Dense\n\ndef get_model():\n    model = Sequential([\n    Dense(128,activation = 'relu',input_shape = (train_data.shape[1],)),\n    Dense(128,activation = 'relu'),\n    Dense(128,activation = 'relu'),\n    Dense(128,activation = 'relu'),\n    Dense(128,activation = 'relu'),\n    Dense(128,activation = 'relu'),\n    Dense(1)\n])\n    \n    return(model)\n\nmodel = get_model()\n","cd7794a0":"# Print the model summary\n\nmodel.summary()","578bbd8f":"# Compile the model\nopt = tf.keras.optimizers.Adam()\nmae = tf.keras.metrics.MeanAbsoluteError()\nmodel.compile(optimizer = opt,\n             loss = 'mse',\n             metrics = [mae])\n","cba75467":"# Train the model, with some of the data reserved for validation\nhistory = model.fit(train_data,train_targets,epochs = 10, batch_size =64,validation_split = 0.15,verbose = False)\n","897642eb":"# Evaluate the model on the test set\n\nmodel.evaluate(test_data,test_targets,verbose = 2)","13479d7d":"import matplotlib.pyplot as plt\n%matplotlib inline","01c487c2":"# Plot the training and validation loss\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show()","156f7356":"from tensorflow.keras.layers import Dropout\nfrom tensorflow.keras import regularizers","221dd632":"def get_regularised_model(wd, rate):\n    model = Sequential([\n        Dense(128, activation=\"relu\", kernel_regularizer = regularizers.l2(wd),input_shape=(train_data.shape[1],)),\n        Dropout(rate),\n        Dense(128, activation=\"relu\",kernel_regularizer = regularizers.l2(wd)),\n         Dropout(rate),\n        Dense(128, activation=\"relu\",kernel_regularizer = regularizers.l2(wd)),\n         Dropout(rate),\n        Dense(128, activation=\"relu\",kernel_regularizer = regularizers.l2(wd)),\n         Dropout(rate),\n        Dense(128, activation=\"relu\",kernel_regularizer = regularizers.l2(wd)),\n         Dropout(rate),\n        Dense(128, activation=\"relu\",kernel_regularizer = regularizers.l2(wd)),\n         Dropout(rate),\n        Dense(1)\n    ])\n    return model","5686b936":"# Re-build the model with weight decay and dropout layers\n\nmodel = get_regularised_model(1e-5,0.3)\n","72eca7ff":"# Compile the model\nopt = tf.keras.optimizers.Adam()\nmae = tf.keras.metrics.MeanAbsoluteError()\nmodel.compile(optimizer = opt,\n             loss = 'mse',\n             metrics = [mae])","eff60a58":"# Train the model, with some of the data reserved for validation\nhistory = model.fit(train_data,train_targets,epochs = 10, batch_size =64,validation_split = 0.15,verbose = False)\n\n","286f249c":"# Evaluate the model on the test set\nmodel.evaluate(test_data,test_targets,verbose = 2)\n","6b37d9fa":"# Plot the training and validation loss\n\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\nplt.show()","12634d2b":"# Write a custom callback\nfrom tensorflow.keras.callbacks import Callback\n\nclass TrainingCallback(Callback):\n    \n    def on_train_begin(self,logs = None):\n        print(\"On Begin of Train\")\n        \n    def on_train_end(self,logs = None):\n        print(\"On End of Train\")\n        \n    def on_epoch_begin(self,epoch, logs=None):\n        print(f'Begin Epoch is {epoch}')\n        \n    def on_epoch_end(self,epoch, logs=None):\n        print(f'End Epoch is {epoch}')\n    \n    def on_train_batch_begin(self,batch, logs=None):\n        print(f'Begin Batch is {batch}')\n        \n    def on_train_batch_end(self,batch, logs=None):\n        print(f'End Batch is {batch}')\n        \n\n","02772fdd":"# Write a custom callback\nfrom tensorflow.keras.callbacks import Callback\n\nclass TestingCallback(Callback):\n    \n    def on_test_begin(self,logs = None):\n        print(\"On Begin of Test\")\n        \n    def on_test_end(self,logs = None):\n        print(\"On End of Test\")\n        \n    def on_test_batch_begin(self,batch, logs=None):\n        print(f'Begin Test Batch is {batch}')\n        \n    def on_test_batch_end(self,batch, logs=None):\n        print(f'End Test Batch is {batch}')","266e8652":"# Write a custom callback\nfrom tensorflow.keras.callbacks import Callback\n\nclass PredictionCallback(Callback):\n    \n    def on_predict_begin(self,logs = None):\n        print(\"On Begin of Prediction\")\n        \n    def on_predict_end(self,logs = None):\n        print(\"On End of Prediction\")\n        \n    def on_predict_batch_begin(self,batch, logs=None):\n        print(f'Begin Prediction Batch is {batch}')\n        \n    def on_predict_batch_end(self,batch, logs=None):\n        print(f'End Prediction Batch is {batch}')","9bfd2c1f":"# Re-build the model\n\nmodel = get_regularised_model(1e-5,0.3)","29c3602f":"# Compile the model\nopt = tf.keras.optimizers.Adam()\nmae = tf.keras.metrics.MeanAbsoluteError()\nmodel.compile(optimizer = opt,\n             loss = 'mse',\n             metrics = [mae])\n","0b154f21":"# Train the model, with some of the data reserved for validation\n\n# Train the model, with some of the data reserved for validation\nhistory = model.fit(train_data,train_targets,epochs = 10, batch_size =64,validation_split = 0.15,verbose = False, \n                    callbacks = [TrainingCallback()])\n","25fb85fe":"# Evaluate the model\n# Evaluate the model on the test set\nmodel.evaluate(test_data,test_targets,verbose = 2,callbacks = [TestingCallback()])","6bd60dc9":"# Make predictions with the model\nmodel.predict(test_data,verbose = 2,callbacks = [PredictionCallback()])\n","32dd192a":"# Re-train the unregularised model\nunregularised_model = get_model()\n\nopt = tf.keras.optimizers.Adam()\nmae = tf.keras.metrics.MeanAbsoluteError()\nunregularised_model.compile(optimizer = opt,\n             loss = 'mse',\n             metrics = [mae])\n\nunreg_history = unregularised_model.fit(train_data,train_targets,epochs = 10, \n                                        batch_size =64,validation_split = 0.15,verbose = False, \n                                        callbacks = [tf.keras.callbacks.EarlyStopping(patience = 5)])","09e71567":"# Evaluate the model on the test set\nunregularised_model.evaluate(test_data,test_targets,verbose = 2)","cc0a76ec":"# Re-train the regularised model\nregularised_model = get_regularised_model(1e-8,0.2)\n\nopt = tf.keras.optimizers.Adam()\nmae = tf.keras.metrics.MeanAbsoluteError()\nregularised_model.compile(optimizer = opt,\n             loss = 'mse',\n             metrics = [mae])\n\nreg_history = unregularised_model.fit(train_data,train_targets,epochs = 10, \n                                        batch_size =64,validation_split = 0.15,verbose = False, \n                                        callbacks = [tf.keras.callbacks.EarlyStopping(patience = 5)])\n","aef8d5c3":"# Evaluate the model on the test set\nunregularised_model.evaluate(test_data,test_targets,verbose = 2)","45eb0c84":"# Plot the training and validation loss\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(12, 5))\n\nfig.add_subplot(121)\n\nplt.plot(unreg_history.history['loss'])\nplt.plot(unreg_history.history['val_loss'])\nplt.title('Unregularised model: loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\n\nfig.add_subplot(122)\n\nplt.plot(reg_history.history['loss'])\nplt.plot(reg_history.history['val_loss'])\nplt.title('Regularised model: loss vs. epochs')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training', 'Validation'], loc='upper right')\n\nplt.show()","53b5b6e1":"***\n<a id=\"coding_tutorial_4\"><\/a>\n## Early stopping \/ patience","bfcd9b14":"#### Plot the learning curves","fe8211ae":" ## Coding tutorials\n #### [1. Validation sets](#coding_tutorial_1)\n #### [2. Model regularisation](#coding_tutorial_2)\n #### [3. Introduction to callbacks](#coding_tutorial_3)\n #### [4. Early stopping \/ patience](#coding_tutorial_4)","7f41271c":"***\n<a id=\"coding_tutorial_2\"><\/a>\n## Model regularisation","e67f8d97":"# Validation, regularisation and callbacks","4febf6b7":"#### Re-train the models with early stopping","931173e1":"#### Train a feedforward neural network model","a61b358f":"#### Load the data","a6cc6e13":"These notebooks are part of Coding Tutorials of TensorFlow 2 for Deep Learning Specialization. I have just replicated it from the videos.","73fe8bd4":"#### Plot the learning curves","bac73e89":"#### Train the model with the callback","f82cfec6":"***\n<a id=\"coding_tutorial_3\"><\/a>\n## Introduction to callbacks","9196a392":"***\n<a id=\"coding_tutorial_1\"><\/a>\n## Validation sets","4fea5adc":"#### Example training callback","d4faa419":"#### Adding regularisation with weight decay and dropout","cc32d8ef":"#### Plot the learning curves"}}