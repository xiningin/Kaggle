{"cell_type":{"b768a1a2":"code","176fa16d":"code","b9c9b17e":"code","c705557d":"code","63d66745":"code","d900d8e3":"code","3bfec682":"code","8f17f184":"code","6101679a":"code","79bcd506":"code","0cbd9dd6":"code","a8428258":"code","29ce8f0c":"code","2195d25d":"code","f477ac7e":"code","eac946b5":"code","157f72c3":"code","69705c3b":"code","d9a9756c":"code","8184eeaf":"code","fd9fa9c9":"code","117f3623":"code","489ab6f1":"code","647d3c42":"code","8098461f":"code","883a7064":"code","9f686911":"code","1d91106b":"code","3376ddc7":"code","d4720c18":"code","f84574b3":"code","7fa278a9":"code","1e73f17a":"code","c4eb6496":"code","5e0208a6":"code","3a29424a":"code","1d5f932c":"code","480a449b":"code","9f086160":"code","c6bc27ea":"code","7856d370":"code","31012ad9":"code","693d34db":"code","fe7c7a33":"code","277d232c":"code","9776d353":"markdown","146484a1":"markdown","c80d75c0":"markdown","18044a56":"markdown","42cfc839":"markdown","9697bcfa":"markdown","4c8fab57":"markdown","d2fe5f53":"markdown","d8aab572":"markdown","4987b73e":"markdown","424ec3d0":"markdown","f3dc0d7b":"markdown","ab3d5244":"markdown","4ed1f833":"markdown","f71f5757":"markdown","cee213b0":"markdown","114344d8":"markdown","cdfa6097":"markdown","76b52d67":"markdown","cdfd744e":"markdown","f03d63af":"markdown","46f8c26c":"markdown","9e744b57":"markdown","1768764b":"markdown","82292804":"markdown","871c8892":"markdown","fa521df1":"markdown","7af19206":"markdown","907f773d":"markdown","67e8b079":"markdown","ec51fc87":"markdown","b83f4aac":"markdown","7fe2906a":"markdown","30f4a6eb":"markdown","1cf48878":"markdown","63696564":"markdown","488b106c":"markdown","a5836d63":"markdown","6e58353b":"markdown","c4ea6d27":"markdown","54467dc6":"markdown","e544f9f7":"markdown","fd047ab9":"markdown","010fa7cd":"markdown","bfe3615e":"markdown","70ff6a74":"markdown","2df269cd":"markdown","16530e5f":"markdown","2802a4ff":"markdown","0786a37b":"markdown","63466b5b":"markdown","aa41b7d7":"markdown","15a587cf":"markdown","4eff47c9":"markdown","5afc9068":"markdown","73202ac3":"markdown","e3cfef71":"markdown","e0b4ebc7":"markdown","92cbd00f":"markdown","2e3f430e":"markdown","e650f967":"markdown","517a63c1":"markdown","05815a26":"markdown","58175217":"markdown","97e36df3":"markdown","c996c2a2":"markdown","6bfe4f00":"markdown","3cb7bb6e":"markdown","991796f1":"markdown","1a632521":"markdown","cc532edb":"markdown","253daadc":"markdown","d46dc27a":"markdown","07feb231":"markdown","694a1ac4":"markdown","408354bd":"markdown"},"source":{"b768a1a2":"import re\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import AdaBoostRegressor, VotingRegressor\nfrom sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split, cross_val_score\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_absolute_error\n\nwarnings.filterwarnings(\"ignore\")","176fa16d":"# path config\ntest_folder_path = \"\/kaggle\/input\/LANL-Earthquake-Prediction\/test\"\nsample_submission_path = \"\/kaggle\/input\/LANL-Earthquake-Prediction\/sample_submission.csv\"\ntrain_path = \"\/kaggle\/input\/LANL-Earthquake-Prediction\/train.csv\"","b9c9b17e":"%%time\n\n# create dataframe from csv file\ndf_train = pd.read_csv(train_path, dtype={\"acoustic_data\": np.int16, \"time_to_failure\": np.float64})\ndf_train.shape","c705557d":"df_train.head()","63d66745":"# setting 10 digits for float type\npd.options.display.precision = 11\ndf_train.head()","d900d8e3":"%%time\n\n# create dataframe from first csv test file\ndf_test = pd.read_csv(test_folder_path + \"\/seg_00030f.csv\", dtype={\"acoustic_data\": np.int16})\ndf_test.shape","3bfec682":"# calculate missing value percentage and plot\ndef plot_null_percentage():\n    # total null value in both datasets\n    null_train_col_1 = df_train[\"acoustic_data\"].isnull().sum()\n    null_train_col_2 = df_train[\"time_to_failure\"].isnull().sum()\n    null_test = df_test[\"acoustic_data\"].isnull().sum()\n    \n    # create figure and subplots\n    figure, axs = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n    figure.set_facecolor(\"white\")\n    figure.suptitle(\"Missing value percentage of train and test datasets\")\n\n    # plot in pie\n    axs[0].pie([null_train_col_1, len(df_train) - null_train_col_1], labels=[\"missing\", \"filled\"], explode=(0, 0.01), autopct=\"%1.1f%%\", labeldistance=1.1, startangle=55, colors=[\"red\", \"green\"])\n    axs[0].set_title(\"df_train: acoustic_data\", pad=10)\n    axs[1].pie([null_train_col_2, len(df_train) - null_train_col_2], labels=[\"missing\", \"filled\"], explode=(0, 0.01), autopct=\"%1.1f%%\", labeldistance=1.1, startangle=55, colors=[\"red\", \"green\"])\n    axs[1].set_title(\"df_train: time_to_failure\", pad=10)\n    axs[2].pie([null_test, len(df_test) - null_test], labels=[\"missing\", \"filled\"], explode=(0, 0.01), autopct=\"%1.1f%%\", labeldistance=1.1, startangle=55, colors=[\"red\", \"green\"])\n    axs[2].set_title(\"df_test: acoustic_data\", pad=10)\n    plt.show()","8f17f184":"plot_null_percentage()","6101679a":"# plot to see trend of value changing\ndef plot_value_changing(slice):\n    figure, axis_1 = plt.subplots(figsize=(20, 8))\n    \n    # plot acoustic_data by slice\n    plt.title(\"acoustic_data and time_to_failure value changing in {}% datapoints\".format(slice \/\/ 100))\n    plt.plot(df_train[\"acoustic_data\"].values[::slice], color=\"orange\")\n    axis_1.set_ylabel(\"acoustic_data\")\n    plt.legend([\"acoustic_data\"], loc=\"upper left\")\n\n    # duplicate plot and apply to time_to_failure\n    axis_2 = axis_1.twinx()\n    plt.plot(df_train[\"time_to_failure\"].values[::slice], color=\"green\")\n    axis_2.set_ylabel(\"time_to_failure\")\n    plt.legend([\"time_to_failure\"], loc=\"upper right\")\n\n    plt.show()","79bcd506":"plot_value_changing(1000)","0cbd9dd6":"plot_value_changing(100)","a8428258":"# plot to see trend of value changing in first 10 million datapoints\ndef plot_10m_value_changing():\n    figure, axis_1 = plt.subplots(figsize=(20,10))\n    axis_1.plot(df_train.index.values[:10000000], df_train[\"acoustic_data\"].values[:10000000], color=\"orange\")\n    axis_1.set_ylabel(\"acoustic_data\")\n    plt.legend([\"acoustic_data\"], loc=\"upper left\")\n\n    axis_2 = axis_1.twinx()\n    axis_2.plot(df_train.index.values[:10000000], df_train[\"time_to_failure\"].values[:10000000], color=\"green\")\n    axis_2.set_ylabel(\"time_to_failure\")\n    plt.legend([\"time_to_failure\"], loc=\"upper right\")\n\n    plt.show()","29ce8f0c":"plot_10m_value_changing()","2195d25d":"df_train[:10000000].describe()","f477ac7e":"# acoustic_data distribution in first 10 million datapoints\ndf_train[\"acoustic_data\"][:10000000].hist(bins=30, range=[-15,15], align=\"mid\")\nplt.title(\"acoustic_data distribution\")\nplt.xlabel(\"acoustic data\")\nplt.show()","eac946b5":"# plot to see trend of value changing in a specific slice\ndef plot_time_to_failure(slice):\n    plt.plot(df_train.index.values[:slice], df_train[\"time_to_failure\"].values[:slice], color=\"green\")\n    plt.ylabel(\"time_to_failure\")\n    plt.title(\"time_to_failure in {} data points\".format(slice))\n    plt.show()","157f72c3":"plot_time_to_failure(150000)","69705c3b":"plot_time_to_failure(10000)","d9a9756c":"plot_time_to_failure(4000)","8184eeaf":"# plot data in csv test files\ndef plot_test_file(test_files):\n    figure, axs = plt.subplots(nrows=1, ncols=3, figsize=(30, 8))\n    for i in range(len(test_files)):\n        segment = pd.read_csv(test_folder_path  + \"\/\" + test_files[i])\n        axs[i].plot(segment[\"acoustic_data\"].values, color=\"green\")\n        axs[i].set_ylabel(\"acoustic_data\")\n        axs[i].set_title(\"test: \" + test_files[i])\n    \n    plt.show()","fd9fa9c9":"plot_test_file([\"seg_00030f.csv\", \"seg_063865.csv\", \"seg_0c12cc.csv\"])","117f3623":"rows = df_test.shape[0]                                 # total records of 1 test file\/seg_id\nsegments = int(np.floor(df_train.shape[0] \/ rows))      # total measurement segments in df_train\nprint(\"Number of segments: \", segments)","489ab6f1":"# create x_train and y_train dataframe\nx_train = pd.DataFrame(index=range(segments), dtype=np.float64)\ny_train = pd.DataFrame(index=range(segments), dtype=np.float64, columns=[\"time_to_failure\"])","647d3c42":"def generate_feature(df, x, seg):\n#     series_x = pd.Series(seg[\"acoustic_data\"].values)\n    \n    df.loc[seg, \"mean\"] = x.mean() \n    df.loc[seg, \"mean_abs\"] = np.abs(x).mean() # normal mean might be around 0 values\n\n    df.loc[seg, \"std\"] = x.std()\n    df.loc[seg, \"std_abs\"] = np.abs(x).std()\n    df.loc[seg, \"std_first_10000\"] = x[:10000].std()\n    df.loc[seg, \"std_last_10000\"]  =  x[-10000:].std()\n    df.loc[seg, \"std_first_50000\"] = x[:50000].std()\n    df.loc[seg, \"std_last_50000\"]  =  x[-50000:].std()\n\n    df.loc[seg, \"max\"] = x.max()\n    df.loc[seg, \"max_abs\"] = np.abs(x).max()\n\n    df.loc[seg, \"min\"] = x.min()\n    df.loc[seg, \"min_abs\"] = np.abs(x).min()\n\n    df.loc[seg, \"sum\"] = x.sum()\n\n    df.loc[seg, \"mad\"] = x.mad() # mean absolute deviation\n    df.loc[seg, \"skew\"] = x.skew()\n        \n    df.loc[seg, \"quant_01\"] = np.quantile(x,0.01)\n    df.loc[seg, \"quant_01_abs\"] = np.quantile(np.abs(x), 0.01)\n    df.loc[seg, \"quant_05\"] = np.quantile(x,0.05)\n    df.loc[seg, \"quant_05_abs\"] = np.quantile(np.abs(x), 0.05)\n    df.loc[seg, \"quant_95\"] = np.quantile(x,0.95)\n    df.loc[seg, \"quant_95_abs\"] = np.quantile(np.abs(x), 0.95)\n    df.loc[seg, \"quant_99\"] = np.quantile(x,0.99)\n    df.loc[seg, \"quant_99_abs\"] = np.quantile(np.abs(x), 0.99)\n    \n    # compute the q-th percentile of the data\n    df.loc[seg, \"q_10\"] = np.percentile(x, 0.10)\n    df.loc[seg, \"q_25\"] = np.percentile(x, 0.25)\n    df.loc[seg, \"q_50\"] = np.percentile(x, 0.50)\n    df.loc[seg, \"q_75\"] = np.percentile(x, 0.75)\n    df.loc[seg, \"q_90\"] = np.percentile(x, 0.90)\n    \n    # fft\n    fft = np.fft.fft(x)\n    real_fft = np.real(fft)\n    imag_fft = np.imag(fft)\n    df.loc[seg, \"real_mean\"] = real_fft.mean()\n    df.loc[seg, \"real_std\"] = real_fft.std()\n    df.loc[seg, \"real_max\"] = real_fft.max()\n    df.loc[seg, \"real_min\"] = real_fft.min()\n    df.loc[seg, \"real_mean_last_5000\"] = real_fft[-5000:].mean()\n    df.loc[seg, \"real_std_last_5000\"] = real_fft[-5000:].std()\n    df.loc[seg, \"real_max_last_5000\"] = real_fft[-5000:].max()\n    df.loc[seg, \"real_min_last_5000\"] = real_fft[-5000:].min()\n    df.loc[seg, \"real_mean_last_15000\"] = real_fft[-15000:].mean()\n    df.loc[seg, \"real_std_last_15000\"] = real_fft[-15000:].std()\n    df.loc[seg, \"real_max_last_15000\"] = real_fft[-15000:].max()\n    df.loc[seg, \"real_min_last_15000\"] = real_fft[-15000:].min()\n\n    df.loc[seg, \"imag_mean\"] = imag_fft.mean()\n    df.loc[seg, \"imga_std\"] = imag_fft.std()\n    df.loc[seg, \"imga_max\"] = imag_fft.max()\n    df.loc[seg, \"imag_min\"] = imag_fft.min()\n    \n    # features generation using rolling\n    for w in [10, 100, 1000, 10000]:\n        x_roll_abs_mean = x.abs().rolling(w).mean().dropna().values\n        x_roll_std = x.rolling(w).std().dropna().values\n        x_roll_min = x.rolling(w).min().dropna().values\n        x_roll_max = x.rolling(w).max().dropna().values\n        x_roll_mean = x.rolling(w).max().dropna().values\n        \n        df.loc[seg, \"mean_roll_std_\" + str(w)] = x_roll_std.mean()\n        df.loc[seg, \"mean_roll_abs_mean_\" + str(w)] = x_roll_abs_mean.mean()\n        df.loc[seg, \"mean_roll_mean_\" + str(w)] = x_roll_mean.mean()\n\n        df.loc[seg, \"std_roll_std_\" + str(w)] = x_roll_std.std()\n        df.loc[seg, \"std_roll_max_\" + str(w)] = x_roll_max.std()\n        df.loc[seg, \"std_roll_mean_\" + str(w)] = x_roll_mean.std()\n        df.loc[seg, \"std_roll_abs_mean_\" + str(w)] = x_roll_abs_mean.std()\n        df.loc[seg, \"std_roll_min_\" + str(w)] = x_roll_min.std()\n\n        df.loc[seg, \"max_roll_std_\" + str(w)] = x_roll_std.max()\n        df.loc[seg, \"max_roll_mean_\" + str(w)] = x_roll_mean.max()\n        df.loc[seg, \"max_roll_min_\" + str(w)] = x_roll_min.max()\n        df.loc[seg, \"min_roll_mean_\" + str(w)] = x_roll_mean.min()\n        df.loc[seg, \"min_roll_std_\" + str(w)] = x_roll_std.min()\n        df.loc[seg, \"min_roll_max_\" + str(w)] = x_roll_max.min()\n\n        df.loc[seg, \"quant_01_roll_std_\" + str(w)] = np.quantile(x_roll_std, 0.01)\n        df.loc[seg, \"quant_05_roll_std_\" + str(w)] = np.quantile(x_roll_std, 0.05)\n        df.loc[seg, \"quant_10_roll_std_\" + str(w)] = np.quantile(x_roll_std, 0.10)\n        df.loc[seg, \"quant_95_roll_std_\" + str(w)] = np.quantile(x_roll_std, 0.95)\n        df.loc[seg, \"quant_99_roll_std_\" + str(w)] = np.quantile(x_roll_std, 0.99)\n        df.loc[seg, \"quant_05_roll_mean_\" + str(w)] = np.quantile(x_roll_mean, 0.05)\n        df.loc[seg, \"quant_95_roll_mean_\" + str(w)] = np.quantile(x_roll_mean, 0.95)\n        df.loc[seg, \"quant_05_roll_abs_mean_\" + str(w)] = np.quantile(x_roll_abs_mean, 0.05)\n        df.loc[seg, \"quant_95_roll_abs_mean_\" + str(w)] = np.quantile(x_roll_abs_mean, 0.95)\n        df.loc[seg, \"quant_05_roll_min_\" + str(w)] = np.quantile(x_roll_min, 0.05)\n        df.loc[seg, \"quant_95_roll_min_\" + str(w)] = np.quantile(x_roll_min, 0.95)\n        df.loc[seg, \"quant_05_roll_max_\" + str(w)] = np.quantile(x_roll_max, 0.05)\n        df.loc[seg, \"quant_95_roll_max_\" + str(w)] = np.quantile(x_roll_max, 0.95)\n    return df","8098461f":"# generate features for each measurement segments\nfor segment in tqdm(range(segments)):\n    seg = df_train.iloc[segment * rows: segment * rows + rows]\n    x = pd.Series(seg[\"acoustic_data\"].values)\n    y = seg[\"time_to_failure\"].values[-1]\n    y_train.loc[segment, \"time_to_failure\"] = y\n    x_train = generate_feature(x_train, x, segment)","883a7064":"x_train.head()","9f686911":"x_train.describe()","1d91106b":"# plot all generated features\ndef plot_feature(feature, x=x_train):\n    fig, axs = plt.subplots(figsize=(20, 8)) \n    axs.set_xlabel(feature)\n    axs.set_ylabel(\"time_to_failure\")\n    plt.title(\"{} - time_to_falure\".format(feature))\n    plt.scatter(x=x[feature], y=y_train, color=\"green\")\n    plt.show()","3376ddc7":"for feature in x_train.columns:\n    plot_feature(feature)","d4720c18":"x_train = x_train.drop(columns=[\"min_abs\", \"quant_01_abs\"])","f84574b3":"# drop useless features\nx_train = x_train.drop(columns=[\"sum\", \"mean\", \"mean_roll_mean_10\", \n                                \"mean_roll_mean_100\", \"mean_roll_mean_10000\", \n                                \"min_roll_std_100\", \"mean_roll_mean_1000\", \n                                \"quant_05_roll_abs_mean_1000\", \"quant_05_roll_abs_mean_100\", \n                                \"quant_05_roll_mean_10000\", \"quant_95_roll_mean_10000\", \n                                \"quant_05_roll_abs_mean_10000\"])","7fa278a9":"x_train","1e73f17a":"# standardize\nscaler = StandardScaler()\nscaler.fit(x_train)\nscaled_x_train = pd.DataFrame(scaler.transform(x_train), columns=x_train.columns)","c4eb6496":"scaled_x_train","5e0208a6":"submission = pd.read_csv(sample_submission_path, index_col=\"seg_id\")\nx_test = pd.DataFrame(columns=scaled_x_train.columns, dtype=np.float64, index=submission.index)\n\n# generate feautres for csv test files\nfor i in tqdm(x_test.index):\n    seg = pd.read_csv(test_folder_path + \"\/\" + i + \".csv\")\n    x = pd.Series(seg[\"acoustic_data\"].values)\n    x_test = generate_feature(x_test, x, i)","3a29424a":"# drop useless features\nx_test = x_test.drop(columns=[\"sum\", \"mean\", \"mean_roll_mean_10\", \n                              \"mean_roll_mean_100\", \"mean_roll_mean_10000\", \n                              \"min_roll_std_100\", \"mean_roll_mean_1000\", \n                              \"quant_05_roll_abs_mean_1000\", \"quant_05_roll_abs_mean_100\", \n                              \"quant_05_roll_mean_10000\", \"quant_95_roll_mean_10000\", \n                              \"quant_05_roll_abs_mean_10000\", \"min_abs\", \"quant_01_abs\"])","1d5f932c":"x_test","480a449b":"scaled_x_test = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns)","9f086160":"scaled_x_test","c6bc27ea":"# create n folds and rename columns with re\nfolds = KFold(n_splits=5, shuffle=True, random_state=42)\nscaled_x_train = scaled_x_train.rename(columns = lambda x:re.sub(\"[^A-Za-z0-9_]+\", \"\", x))","7856d370":"# params\nparams = {\"n_estimators\": [50, 100],\n          \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n          \"loss\": [\"linear\", \"square\", \"exponential\"]}\n\nrandom_ada = RandomizedSearchCV(AdaBoostRegressor(), param_distributions=params, cv=folds, n_iter=10, n_jobs=-1, scoring=\"neg_mean_absolute_error\", verbose=1)\n\n# get result\nresult = random_ada.fit(scaled_x_train, y_train.values.ravel())\n\nprint(\"Best MAE score: {}\".format(-result.best_score_))\nprint(\"Best params: {}\".format(result.best_params_))","31012ad9":"# params\nparams = {\"num_leaves\": [31, 51], \"min_data_in_leaf\": [10], \n          \"objective\": [\"regression\"], \"max_depth\": [-1],\n          \"learning_rate\": [0.01, 0.1], \"boosting_type\": [\"gbdt\"],\n          \"feature_fraction\": [0.9], \"bagging_freq\": [1],\n          \"bagging_fraction\": [0.9], \"bagging_seed\": [42],\n          \"metric\": [\"mae\"], \"lambda_l1\": [0.1],\n          \"n_estimators\": [1000, 10000]}\n\n# fit and predict with each fold, calculate MAE and get best params\nrandom_light_gbm = RandomizedSearchCV(lgb.LGBMRegressor(), param_distributions=params, cv=folds, n_iter=10, n_jobs=-1, scoring=\"neg_mean_absolute_error\", verbose=1)\nresult = random_light_gbm.fit(scaled_x_train, y_train, callbacks=[lgb.log_evaluation(1000)])\n\nprint(\"Best MAE score: {}\".format(-result.best_score_))\nprint(\"Best params: {}\".format(result.best_params_))","693d34db":"params = {\"alpha\": [0.01, 0.05, 0.5], \"solver\": [\"auto\"], \"tol\": [1e-5, 1e-3, 0.01]}\n\n# increase number of folds\nfolds_ = KFold(n_splits=50, shuffle=True, random_state=42)\n\nrandom_ridge = RandomizedSearchCV(Ridge(), param_distributions=params, cv=folds_, n_iter=10, n_jobs=-1, scoring=\"neg_mean_absolute_error\", verbose=1)\nresult = random_ridge.fit(scaled_x_train, y_train)\n\nprint(\"Best MAE score: {}\".format(-result.best_score_))\nprint(\"Best params: {}\".format(result.best_params_))","fe7c7a33":"best_estimators = [(\"AdaBoostRegressor\", random_ada.best_estimator_), (\"LightGBMRegressor\", random_light_gbm.best_estimator_), (\"Ridge\", random_ridge.best_estimator_)]\n\nvoting = VotingRegressor(estimators=best_estimators, n_jobs=-1)\nvoting.fit(scaled_x_train, y_train)\n\n# score of voting model\ncvs_voting = cross_val_score(voting, scaled_x_train, y_train, cv=folds, scoring=\"neg_mean_absolute_error\")\nprint(\"Voting: {}\".format(-cvs_voting.mean()))","277d232c":"# predict on scaled_x_test dataframe\nsubmission.time_to_failure = voting.predict(scaled_x_test)\n# submission.time_to_failre = random_light_gbm.predict(scaled_x_test)\n\n# write to csv file for submit\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","9776d353":"<p style=\"font-size:18px\"> Ta c\u0169ng s\u1ebd c\u00e0i \u0111\u1eb7t m\u00f4 h\u00ecnh Ridge t\u01b0\u01a1ng t\u1ef1 nh\u01b0 c\u00e1c m\u00f4 h\u00ecnh tr\u01b0\u1edbc \u0111\u00f3. <\/p>","146484a1":"<p style=\"font-size:18px\">\nD\u1eef li\u1ec7u \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng cho vi\u1ec7c hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh g\u1ed3m 2 tr\u01b0\u1eddng trong file csv: <code>train.csv<\/code>\n<\/p>\n\n<p style=\"font-size:18px\">\n    <code>acoustic_data<\/code>: \u0110\u00e2y l\u00e0 d\u1eef li\u1ec7u \u0111\u1ecba ch\u1ea5n thu \u0111\u01b0\u1ee3c d\u1ef1a tr\u00ean nh\u1eefng ph\u01b0\u01a1ng ph\u00e1p \u0111o th\u1ef1c t\u1ebf. L\u00e0 c\u01b0\u1eddng \u0111\u1ed9 s\u00f3ng thu \u0111\u01b0\u1ee3c t\u1eeb l\u00f2ng \u0111\u1ea5t v\u1edbi ki\u1ec3u s\u1ed1 nguy\u00ean <i>int16<\/i>.<br>\n    <code>time_to_failure<\/code>: \u0110\u00e2y l\u00e0 th\u1eddi gian m\u00e0 tr\u1eadn \u0111\u1ed9ng \u0111\u1ea5t ti\u1ebfp theo s\u1ebd x\u1ea3y ra tr\u00ean th\u1eddi gian th\u1ef1c t\u00ednh b\u1eb1ng gi\u00e2y v\u1edbi ki\u1ec3u s\u1ed1 th\u1ef1c <i>float64<\/i>.\n<\/p>\n\n<p style=\"font-size:18px\"> Trong m\u1ed7i l\u1ea7n th\u1eed nghi\u1ec7m, d\u1eef li\u1ec7u \u0111\u1ecba ch\u1ea5n hay c\u01b0\u1eddng \u0111\u1ed9 s\u00f3ng v\u00e0 kho\u1ea3ng th\u1eddi gian c\u00f2n l\u1ea1i tr\u01b0\u1edbc khi \u0111\u1ed9ng \u0111\u1ea5t x\u1ea3y ra \u0111\u01b0\u1ee3c \u0111o trong 0.0375s (\u0111\u01b0\u1ee3c s\u1eafp x\u1ebfp theo th\u1eddi gian) v\u00e0 t\u1ea7n s\u1ed1 4MHz (th\u00f4ng tin \u0111\u01b0\u1ee3c cung c\u1ea5p b\u1edfi BTC cu\u1ed9c thi). T\u1eeb \u0111\u00f3, ta s\u1ebd c\u00f3 150000 data points cho m\u1ed7i m\u1ed9t measurement segment. File <code>train.csv<\/code> bao g\u1ed3m 629145480 records t\u01b0\u01a1ng \u1ee9ng v\u1edbi kho\u1ea3ng 4194 measurement segments.\n<\/p>\n    \n<p style=\"font-size:18px\">\n<code>test<\/code>: l\u00e0 m\u1ed9t folder bao g\u1ed3m nhi\u1ec1u file csv, m\u1ed7i m\u1ed9t file l\u00e0 d\u1eef li\u1ec7u \u0111\u1ecba ch\u1ea5n c\u1ee7a 1 measurement segment g\u1ed3m 150000 data points.<br>\nM\u1ed7i file test csv \u1ee9ng v\u1edbi m\u1ed9t <code>seg_id<\/code>, l\u00e0 id c\u1ee7a measurement segment t\u01b0\u01a1ng \u1ee9ng \u0111\u01b0\u1ee3c th\u1eed nghi\u1ec7m, k\u1ebft qu\u1ea3 c\u1ee7a b\u00e0i to\u00e1n s\u1ebd l\u00e0 k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n th\u1ef1c hi\u1ec7n tr\u00ean c\u00e1c <code>seg_id<\/code> n\u00e0y.\n<\/p>\n\n<p style=\"font-size:18px\">\n<code>sample_submission.csv<\/code>: l\u00e0 m\u1eabu submission file c\u1ee7a cu\u1ed9c thi.\n<\/p>","c80d75c0":"## 6.2. M\u00f4 h\u00ecnh AdaBoostRegressor","18044a56":"<p style=\"font-size:18px\"> Ngo\u00e0i ra c\u00e1c features c\u00f3 \u0111\u1ed3 th\u1ecb kh\u00f4ng bi\u1ec3u di\u1ec5n \u0111\u01b0\u1ee3c th\u00f4ng tin g\u00ec \u0111\u1eb7c bi\u1ec7t, kh\u00f4ng ph\u1ea3i d\u1ea1ng linear c\u0169ng kh\u00f4ng ph\u1ea3i non-linear n\u00ean ta c\u0169ng s\u1ebd lo\u1ea1i b\u1ecf nh\u1eefng features n\u00e0y \u0111\u1ec3 kh\u00f4ng l\u00e0m nhi\u1ec5u m\u00f4 h\u00ecnh d\u1eef li\u1ec7u. <\/p>","42cfc839":"# 8. T\u00e0i li\u1ec7u tham kh\u1ea3o\n### [K- Fold Cross Validation For Parameter Tuning](https:\/\/medium.datadriveninvestor.com\/k-fold-cross-validation-for-parameter-tuning-75b6cb3214f)\n### [LightGBM with RandomizedSearchCV](https:\/\/www.kaggle.com\/binilg\/lightgbm-with-randomsearchcv-and-feature-imp)\n### [Boosting](https:\/\/www.kaggle.com\/fengdanye\/machine-learning-7-boosting)\n### [GradientBoosting](https:\/\/viblo.asia\/p\/gradient-boosting-tat-tan-tat-ve-thuat-toan-manh-me-nhat-trong-machine-learning-YWOZrN7vZQ0)\n### [H\u1ed3i qui Ridge](https:\/\/phamdinhkhanh.github.io\/deepai-book\/ch_ml\/RidgedRegression.html)","9697bcfa":"<p style=\"font-size:18px\"> C\u00e0i \u0111\u1eb7t c\u00e1c \u0111\u01b0\u1eddng d\u1eabn: <\/p>","4c8fab57":"<p style=\"font-size:18px\"> AdaBoost (Adaptive Boost) l\u00e0 m\u1ed9t thu\u1eadt to\u00e1n boosting d\u00f9ng \u0111\u1ec3 x\u00e2y d\u1ef1ng classifer cho c\u00e1c b\u00e0i to\u00e1n ph\u00e2n l\u1edbp ho\u1eb7c regressor cho c\u00e1c b\u00e0i to\u00e1n h\u1ed3i quy. <br>\nBoosting l\u00e0 thu\u1eadt to\u00e1n h\u1ecdc qu\u1ea7n th\u1ec3 b\u1eb1ng c\u00e1ch x\u00e2y d\u1ef1ng nhi\u1ec1u thu\u1eadt to\u00e1n h\u1ecdc c\u00f9ng l\u00fac nh\u01b0 c\u00e2y quy\u1ebft \u0111\u1ecbnh v\u00e0 k\u1ebft h\u1ee3p ch\u00fang l\u1ea1i. M\u1ee5c \u0111\u00edch l\u00e0 t\u1ea1o ra m\u1ed9t c\u1ee5m ho\u1eb7c m\u1ed9t nh\u00f3m c\u00e1c weak learner sau \u0111\u00f3 k\u1ebft h\u1ee3p ch\u00fang l\u1ea1i \u0111\u1ec3 t\u1ea1o ra m\u1ed9t strong learner duy nh\u1ea5t. <br>\n<br>\n\u00dd t\u01b0\u1edfng c\u1ee7a AdaBoost xu\u1ea5t ph\u00e1t t\u1eeb Boosting nh\u01b0ng n\u00f3 l\u1ea1i s\u1eed d\u1ee5ng to\u00e0n b\u1ed9 d\u1eef li\u1ec7u \u0111\u1ec3 hu\u1ea5n luy\u1ec7n t\u1eebng m\u00f4 h\u00ecnh con v\u1edbi tr\u1ecdng s\u1ed1 \u0111\u01b0\u1ee3c ph\u00e2n ph\u1ed1i l\u1ea1i sau m\u1ed7i l\u1ea7n hu\u1ea5n luy\u1ec7n sao cho m\u00f4 h\u00ecnh sau quan t\u00e2m nhi\u1ec1u h\u01a1n \u0111\u1ebfn nh\u1eefng l\u1ed7i sai c\u1ee7a m\u00f4 h\u00ecnh tr\u01b0\u1edbc \u0111\u00f3. <br>\n<br>\nM\u00f4 h\u00ecnh s\u1ebd g\u00e1n tr\u1ecdng s\u1ed1 cho t\u1ea5t c\u1ea3 c\u00e1c \u0111i\u1ec3m d\u1eef li\u1ec7u:  $\\sum\\limits_{i}w_i = 1$, $w_i = w_k$ <br>\nV\u1edbi m\u1ed7i l\u1ea7n boosting:<br>\n- Hu\u1ea5n luy\u1ec7n c\u00e1c m\u00f4 h\u00ecnh con: $C_i = train(x, y, w)$ <br>\n- D\u1ef1 \u0111o\u00e1n nh\u00e3n: $\\hat y = predict(C_j, x)$ <br>\n- T\u1ed5ng l\u1ed7i s\u1ebd \u0111\u01b0\u1ee3c t\u00ednh theo c\u00f4ng th\u1ee9c: $\\epsilon = \\sum\\limits_{i} w_i.(\\hat y_i! = y_i)$. T\u1ed5ng l\u1ed7i c\u00f3 \u0111\u1ed9 l\u1edbn trong kho\u1ea3ng [0, 1], khi t\u1ea5t c\u1ea3 c\u00e1c d\u1ef1 \u0111o\u00e1n l\u00e0 \u0111\u00fang th\u00ec t\u1ed5ng l\u1ed7i b\u1eb1ng 0 v\u00e0 ng\u01b0\u1ee3c l\u1ea1i. <br>\n- H\u1ec7 s\u1ed1 $a_i = 0.5\\log{1 - \\epsilon \\over \\epsilon}$. T\u01b0\u1ee3ng tr\u01b0ng cho \u1ea3nh h\u01b0\u1edfng c\u1ee7a m\u00f4 h\u00ecnh con th\u1ee9 i l\u00ean to\u00e0n b\u1ed9 m\u00f4 h\u00ecnh l\u1edbn. Khi m\u00f4 h\u00ecnh con \u0111\u00f3 c\u00f3 \u00edt d\u1ef1 \u0111o\u00e1n sai, $\\epsilon$ s\u1ebd th\u1ea5p, do \u0111\u00f3 m\u00f4 h\u00ecnh \u0111\u00f3ng g\u00f3p nhi\u1ec1u cho d\u1ef1 \u0111o\u00e1n cu\u1ed1i c\u00f9ng. <br>\n- C\u1eadp nh\u1eadt tr\u1ecdng s\u1ed1: $w = w * \\exp(-a_i * \\hat y * y)$\n<\/p>","d2fe5f53":"<p style=\"font-size:18px\"> K\u1ebft qu\u1ea3 submit sau khi s\u1eed d\u1ee5ng Voting Regressor. <\/p>","d8aab572":"<p style=\"font-size:18px\">\u0110\u00e2y l\u00e0 qu\u00e1 tr\u00ecnh t\u1ea1o ra th\u00eam c\u00e1c features m\u1edbi t\u1eeb m\u1ed9t ho\u1eb7c c\u00e1c features hi\u1ec7n c\u00f3, c\u00f3 kh\u1ea3 n\u0103ng s\u1eed d\u1ee5ng trong qu\u00e1 tr\u00ecnh d\u1ef1 \u0111o\u00e1n v\u00e0 x\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh. \u0110\u1ed1i v\u1edbi b\u00e0i to\u00e1n n\u00e0y, feature \u0111\u01b0\u1ee3c cung c\u1ea5p ch\u1ec9 c\u00f3 duy nh\u1ea5t m\u1ed9t n\u00ean ta s\u1ebd ph\u1ea3i t\u1ea1o th\u00eam c\u00e1c features m\u1edbi t\u1eeb field <code>acoustic_data<\/code>.<\/p>","4987b73e":"# 6.Hu\u1ea5n luy\u1ec7n \n","424ec3d0":"<p style=\"font-size:18px\"> Nh\u1eadn th\u1ea5y tr\u00ean m\u1ed7i kho\u1ea3ng 4000 data points, <code>time_to_failure<\/code> c\u00f3 xu h\u01b0\u1edbng gi\u1ea3m theo m\u1ed9t \u0111\u01b0\u1eddng tuy\u1ebfn t\u00ednh.<\/p>","f3dc0d7b":"## 6.1. L\u1ef1a ch\u1ecdn m\u00f4 h\u00ecnh","ab3d5244":"<p style=\"font-size:18px\"> Nh\u01b0 v\u1eady, dataframe x_test s\u1ebd c\u00f3 c\u00f9ng s\u1ed1 l\u01b0\u1ee3ng features v\u1edbi x_train l\u00e0 138 v\u00e0 c\u00f3 2624 records t\u01b0\u01a1ng \u1ee9ng v\u1edbi s\u1ed1 l\u01b0\u1ee3ng file test csv. Sau \u0111\u00f3, ta s\u1ebd ti\u1ebfn h\u00e0nh chu\u1ea9n h\u00f3a d\u1eef li\u1ec7u c\u1ee7a v\u1ec1 c\u00f9ng m\u1ed9t mi\u1ec1n gi\u00e1 tr\u1ecb b\u1eb1ng <code>StandardScaler<\/code> t\u01b0\u01a1ng t\u1ef1 nh\u01b0 \u0111\u00e3 l\u00e0m \u1edf tr\u00ean.<\/p>","4ed1f833":"## Nh\u1eadn x\u00e9t:\n<p style=\"font-size:18px\"> Nh\u01b0 v\u1eady, m\u1ed7i file hay m\u1ed7i <code>seg_id<\/code> l\u00e0 m\u1ed9t segment g\u1ed3m 150000 data points. Nh\u01b0 3 bi\u1ec3u \u0111\u1ed3 \u1edf tr\u00ean, ta c\u00f3 th\u1ec3 th\u1ea5y c\u1ea3 3 <code>seg_id<\/code> n\u00e0y \u0111\u1ec1u c\u00f3 x\u1ea3y ra \u0111\u1ed9ng \u0111\u1ea5t do bi\u00ean \u0111\u1ed9 dao \u0111\u1ed9ng c\u1ee7a <code>acoustic_data<\/code> l\u00e0 r\u1ea5t l\u1edbn.<\/p>","f71f5757":"<p style=\"font-size:18px\"> Ta s\u1ebd kh\u1ea3o s\u00e1t s\u1ef1 thay \u0111\u1ed5i tr\u00ean 1 slice nh\u1ecf h\u01a1n.<\/p>","cee213b0":"### Nh\u1eadn x\u00e9t\n<p style=\"font-size:18px\"> Ta c\u00f3 th\u1ec3 th\u1ea5y m\u1ed7i l\u1ea7n <code>time_to_failure<\/code> c\u00f3 gi\u00e1 tr\u1ecb 0 \u0111\u1ed3ng th\u1eddi gi\u00e1 tr\u1ecb c\u1ee7a <code>acoustic_data<\/code> c\u0169ng c\u00f3 s\u1ef1 thay \u0111\u1ed5i l\u1edbn, t\u1ea1i c\u00e1c m\u1ed1c n\u00e0y \u0111\u00e3 x\u1ea3y ra \u0111\u1ed9ng \u0111\u1ea5t.<br>\nNgo\u00e0i ra, ta c\u00f3 th\u1ec3 \u0111\u1eb7t gi\u1ea3 thuy\u1ebft trong to\u00e0n b\u1ed9 dataset c\u00f3 16 l\u1ea7n x\u1ea3y ra \u0111\u1ed9ng \u0111\u1ea5t. Gi\u1ea3 thuy\u1ebft n\u00e0y c\u0169ng \u0111\u00fang v\u1edbi 1% d\u1eef li\u1ec7u.<\/p>\n","114344d8":"<p style=\"font-size:18px\"> x_test sau chu\u1ea9n ho\u00e1. <\/p>","cdfa6097":"## 2.1. Chu\u1ea9n b\u1ecb th\u01b0 vi\u1ec7n v\u00e0 \u0111\u01b0\u1eddng d\u1eabn","76b52d67":"<p style=\"font-size:18px\">Do pandas kh\u00f4ng th\u1ec3 hi\u1ec7n th\u1ecb to\u00e0n b\u1ed9 c\u00e1c ch\u1eef s\u1ed1 sau d\u1ea5u ph\u1ea9y, n\u00ean ta s\u1ebd hi\u1ec3n th\u1ecb 10 ch\u1eef s\u1ed1 sau d\u1ea5u ph\u1ea9y c\u1ee7a d\u1eef li\u1ec7u s\u1ed1 \u0111\u1ec3 th\u1ea5y \u0111\u01b0\u1ee3c s\u1ef1 thay \u0111\u1ed5i c\u1ee7a tr\u01b0\u1eddng <code>time_to_failure<\/code> (th\u1eddi gian). <\/p>","cdfd744e":"<p style=\"font-size:18px\"> Ti\u1ebfp theo, ta s\u1ebd bi\u1ec3u di\u1ec5n d\u1eef li\u1ec7u c\u1ee7a m\u1ed9t s\u1ed1 file test \u0111\u1ec3 ki\u1ec3m tra xem <code>seg_id<\/code> \u0111\u00f3 c\u00f3 x\u1ea3y ra \u0111\u1ed9ng \u0111\u1ea5t hay kh\u00f4ng. <\/p>","f03d63af":"<p style=\"font-size:18px\"> Ngo\u00e0i ra, ta c\u0169ng s\u1ebd ti\u1ebfn h\u00e0nh xem x\u00e9t m\u1ed9t s\u1ed1 chi ti\u1ebft v\u1ec1 d\u1eef li\u1ec7u c\u1ee7a dataset. <\/p>","46f8c26c":"# 3. Ph\u00e2n t\u00edch d\u1eef li\u1ec7u ","9e744b57":"## 5.1. Training data","1768764b":"![image.png](attachment:5d2c04ba-4d60-4908-ade8-9208c9111c9a.png)","82292804":"## 3.2. S\u1ef1 thay \u0111\u1ed5i gi\u00e1 tr\u1ecb c\u1ee7a acoustic_data v\u00e0 time_to_failure trong 10% d\u1eef li\u1ec7u\n<p style=\"font-size:18px\"> \u1ede \u0111\u00e2y ta s\u1ebd bi\u1ec3u di\u1ec5n \u0111\u1ed3ng th\u1eddi 2 fields <code>acoustic_data<\/code> v\u00e0 <code>time_to_failure<\/code> tr\u00ean c\u00f9ng m\u1ed9t \u0111\u1ed3 th\u1ecb \u0111\u1ec3 th\u1ea5y \u0111\u01b0\u1ee3c s\u1ef1 t\u01b0\u01a1ng quan gi\u1eefa ch\u00fang.<br>\nCh\u00fang ta s\u1ebd xem s\u00e9t s\u1ef1 thay \u0111\u1ed5i v\u1ec1 gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c fields trong 10% d\u1eef li\u1ec7u b\u1eb1ng c\u00e1ch s\u1eed d\u1ee5ng c\u00e1c slice v\u1edbi b\u01b0\u1edbc nh\u1ea3y 1000, t\u1ea1o \u0111\u01b0\u1ee3c s\u1ef1 bao qu\u00e1t to\u00e0n b\u1ed9 t\u1eadp d\u1eef li\u1ec7u thay v\u00ec ph\u1ea3i duy\u1ec7t to\u00e0n b\u1ed9 629145480 records. <\/p>","871c8892":"#### Do cu\u1ed9c thi \u0111\u00e3 k\u1ebft th\u00fac n\u00ean em kh\u00f4ng th\u1ec3 l\u1ea5y \u0111\u01b0\u1ee3c ranking c\u1ee5 th\u1ec3 c\u1ee7a submisson, ch\u1ec9 c\u00f3 Private\/Public Score nh\u01b0 d\u01b0\u1edbi \u0111\u00e2y.\n<p style=\"font-size:18px\"> K\u1ebft qu\u1ea3 submit v\u1edbi LightGBMRegressor (c\u00f3 MAE score th\u1ea5p nh\u1ea5t trong 3 m\u00f4 h\u00ecnh \u0111\u00e3 th\u1eed nghi\u1ec7m). <\/p>","fa521df1":"### Nh\u1eadn x\u00e9t\n<p style=\"font-size:18px\"> Qua \u0111\u00e2y ta c\u00f3 th\u1ec3 th\u1ea5y, s\u1ef1 thay \u0111\u1ed5i c\u1ee7a <code>time_to_failure<\/code> l\u00e0 kh\u00f4ng \u0111\u1ec1u nhau. <br> \u0110\u1ed3ng th\u1eddi, gi\u00e1 tr\u1ecb c\u1ee7a time_to_failure tr\u00ean m\u1ed7i 4000 data points c\u00f3 xu h\u01b0\u1edbng gi\u1ea3m tuy\u1ebfn t\u00ednh.<\/p>\n","7af19206":"# 1. Gi\u1edbi thi\u1ec7u b\u00e0i to\u00e1n\n","907f773d":"<p style=\"font-size:18px\"> Nh\u01b0 v\u1eady, k\u1ebft qu\u1ea3 submit v\u1edbi Voting Regressor cao h\u01a1n m\u1ed9t ch\u00fat so v\u1edbi LightGBM Regressor.<\/p>","67e8b079":"<p style=\"font-size:18px\"> Ti\u1ebfp theo, ch\u00fang ta s\u1ebd bi\u1ec3u di\u1ec5n s\u1ef1 t\u01b0\u01a1ng quan gi\u1eefa 2 t\u1eadp n\u00e0y. Func <code>plot_feature<\/code> \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 m\u00f4 t\u1ea3 s\u1ef1 t\u01b0\u01a1ng quan c\u1ee7a t\u1eebng feature c\u1ee7a x_train so v\u1edbi y_train. Feature n\u00e0o kh\u00f4ng ph\u00f9 h\u1ee3p ho\u1eb7c kh\u00f4ng c\u1ea7n thi\u1ebft cho c\u00e1c qu\u00e1 tr\u00ecnh ti\u1ebfp theo th\u00ec ch\u00fang ta s\u1ebd lo\u1ea1i b\u1ecf \u0111\u1ec3 tr\u00e1nh g\u00e2y nhi\u1ec5u m\u00f4 h\u00ecnh . <\/p>","ec51fc87":"## 6.4. M\u00f4 h\u00ecnh h\u1ed3i qui Ridge","b83f4aac":"## 6.3. M\u00f4 h\u00ecnh LightGBM Regressor","7fe2906a":"<p style=\"font-size:18px\"> Dataframe x_train sau khi \u0111\u00e3 \u0111\u01b0\u1ee3c chu\u1ea9n ho\u00e1.  <\/p>","30f4a6eb":"<p style=\"font-size:18px\"> C\u00e1c file test s\u1ebd c\u00f3 k\u00edch th\u01b0\u1edbc (150000, 1) t\u01b0\u01a1ng \u0111\u01b0\u01a1ng v\u1edbi 1 measurement segment, do \u0111\u00f3 ph\u1ea3i d\u1ef1 \u0111o\u00e1n 150000 m\u1ed1c th\u1eddi gian <code>time_to_failure<\/code> cho m\u1ed7i data points.\n<\/p>","1cf48878":"<p style=\"font-size:18px\"> Dataframe x_test: <\/p>","63696564":"<p style=\"font-size:18px\"> Tr\u01b0\u1edbc ti\u00ean, ch\u00fang ta c\u1ea7n import c\u00e1c th\u01b0 vi\u1ec7n, package s\u1ebd s\u1eed d\u1ee5ng trong notebook. <\/p>","488b106c":"<p style=\"font-size:18px\">Ngo\u00e0i ra, cu\u1ed9c thi c\u00f2n \u0111\u01b0a ra h\u00e0m l\u1ed7i <code>mean_absoluted_error<\/code> \u0111\u1ec3 t\u00ednh \u0111i\u1ec3m khi submit k\u1ebft qu\u1ea3 b\u00e0i to\u00e1n, c\u00f3 c\u00f4ng th\u1ee9c nh\u01b0 sau: <br><\/p>\n<h3><center> $MAE = $$\\sum\\limits_{x=1}^{n} {\\mid y_i - x_i \\mid \\over n} = $$\\sum\\limits_{x=1}^{n} {e_i \\over n}$ <\/center><\/h3>\n<p style=\"font-size:18px\">\u1ede \u0111\u00e2y ch\u00fang ta c\u0169ng s\u1ebd s\u1eed d\u1ee5ng MAE \u0111\u1ec3 so s\u00e1nh c\u00e1c m\u00f4 h\u00ecnh. <\/p>","a5836d63":"## 3.1. Ki\u1ec3m tra t\u1ec9 l\u1ec7 thi\u1ebfu gi\u00e1 tr\u1ecb v\u1edbi c\u1ea3 2 datasets df_train v\u00e0 df_test\n<p style=\"font-size:18px\"> Ta s\u1ebd s\u1eed d\u1ee5ng th\u01b0 vi\u1ec7n <code>matplotlib<\/code> - th\u01b0 vi\u1ec7n h\u1ed7 tr\u1ee3 v\u1ebd v\u00e0 bi\u1ec3u di\u1ec5n c\u00e1c bi\u1ec3u \u0111\u1ed3 d\u1eef li\u1ec7u.<br>\n\u1ede \u0111\u00e2y ch\u00fang ta s\u1ebd t\u00ednh to\u00e1n t\u1ec9 l\u1ec7 thi\u1ebfu gi\u00e1 tr\u1ecb tr\u00ean m\u1ed7i tr\u01b0\u1eddng c\u1ee7a 2 datasets.\n<\/p>","6e58353b":"### Nh\u1eadn x\u00e9t\n<p style=\"font-size:18px\">\nNh\u01b0 v\u1eady, c\u1ea3 2 datasets \u0111\u1ec1u kh\u00f4ng c\u00f3 c\u00e1c gi\u00e1 tr\u1ecb null v\u00e0 ta kh\u00f4ng c\u1ea7n ti\u1ebfn h\u00e0nh fill c\u00e1c gi\u00e1 tr\u1ecb null.\n<\/p>","c4ea6d27":"## 5.2. Testing data","54467dc6":"<p style=\"font-size:18px\"> \u0110\u1ec3 hu\u1ea5n luy\u1ec7n cho m\u00f4 h\u00ecnh, ta s\u1ebd th\u1eed v\u1edbi m\u1ed9t s\u1ed1 m\u00f4 h\u00ecnh nh\u01b0 AdaBoostRegressor, LightGBM v\u00e0 Ridge. Sau \u0111\u00f3 so s\u00e1nh c\u00e1c k\u1ebft qu\u1ea3 v\u00e0 l\u1ef1a ch\u1ecdn m\u00f4 h\u00ecnh th\u00edch h\u1ee3p nh\u1ea5t ho\u1eb7c s\u1eed d\u1ee5ng <code>VotingRegressor<\/code> \u0111\u1ec3 t\u1ea1o ra m\u00f4 h\u00ecnh cu\u1ed1i c\u00f9ng.<br>\n<br>\nTa s\u1ebd d\u00f9ng <code>K-Fold<\/code> \u0111\u1ec3 chia b\u1ed9 d\u1eef li\u1ec7u th\u00e0nh c\u00e1c folds (k folds), m\u1ed7i fold s\u1ebd \u0111\u01b0\u1ee3c thay phi\u00ean s\u1eed d\u1ee5ng l\u00e0m t\u1eadp test m\u1ed9t l\u1ea7n v\u00e0 c\u00e1c folds c\u00f2n l\u1ea1i s\u1ebd \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng l\u00e0m t\u1eadp train.<br> \nNh\u01b0 v\u1eady, ta s\u1ebd \u0111\u00e1nh gi\u00e1 \u0111\u01b0\u1ee3c ch\u00ednh x\u00e1c h\u01a1n \u0111\u1ed9 kh\u1ea3 d\u1ee5ng c\u1ee7a m\u00f4 h\u00ecnh \u0111\u1ed1i v\u1edbi b\u00e0i to\u00e1n. \u1ede \u0111\u00e2y, ch\u00fang ta s\u1ebd chia th\u00e0nh 5 folds, nhi\u1ec1u h\u01a1n c\u00f3 th\u1ec3 g\u00e2y ra t\u00ecnh tr\u1ea1ng tr\u00e0n RAM v\u00e0 crash session \u0111\u1ed1i v\u1edbi m\u1ed9t s\u1ed1 m\u00f4 h\u00ecnh.<\/p>","e544f9f7":"<p style=\"font-size:18px\"> Ta c\u0169ng s\u1ebd ti\u1ebfn h\u00e0nh lo\u1ea1i b\u1ecf nh\u1eefng features kh\u00f4ng d\u00f9ng t\u1edbi ho\u1eb7c g\u00e2y nhi\u1ec5u nh\u01b0 x_train. <\/p>","fd047ab9":"<p style=\"font-size:18px\"> Do l\u01b0\u1ee3ng data r\u1ea5t l\u1edbn, l\u00ean \u0111\u1ebfn h\u01a1n 600 tri\u1ec7u data points, n\u00ean ch\u00fang ta s\u1ebd chia nh\u1ecf data th\u00e0nh c\u00e1c measurement segments v\u1edbi size l\u00e0 150000 t\u01b0\u01a1ng \u1ee9ng v\u1edbi l\u01b0\u1ee3ng records c\u1ee7a m\u1ed9t file test hay m\u1ed9t <code>seg_id<\/code>.<\/p>","010fa7cd":"### C\u00e0i \u0111\u1eb7t\n<p style=\"font-size:18px\"> \u0110\u1ec3 c\u00e0i \u0111\u1eb7t m\u00f4 h\u00ecnh AdaBoostRegressor, ta s\u1ebd s\u1eed d\u1ee5ng th\u01b0 vi\u1ec7n scikit-learn \u0111\u1ec3 g\u1ecdi h\u00e0m <code>AdaBoostRegressor()<\/code>. <br>\nDo l\u01b0\u1ee3ng d\u1eef li\u1ec7u t\u01b0\u01a1ng \u0111\u1ed1i l\u1edbn, n\u00ean ch\u00fang ta s\u1ebd s\u1eed d\u1ee5ng <code>RandomizedSearchCV<\/code> v\u1edbi m\u1ed9t s\u1ed1 b\u1ed9 params \u0111\u1ec3 \u0111i\u1ec1u ch\u1ec9nh tham s\u1ed1 cho qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n v\u00e0 d\u1ef1 \u0111o\u00e1n k\u1ebft qu\u1ea3. T\u1eeb \u0111\u00f3 c\u00f3 \u0111\u01b0\u1ee3c estimator t\u1ed1t nh\u1ea5t.\n<\/p>","bfe3615e":"<p style=\"font-size:18px\"> x_train sau khi \u0111\u00e3 \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o th\u00eam c\u00e1c features. <\/p>","70ff6a74":"<p style=\"font-size:18px\"> LightGBM l\u00e0 m\u1ed9t framework \u0111\u1ec3 x\u1eed l\u00fd thu\u1eadt to\u00e1n t\u0103ng c\u01b0\u1eddng \u0111\u1ed9 d\u1ed1c (Gradient Boosting) do Microsoft ph\u00e1t tri\u1ec3n.<br> \n<br>\nGradient Boosting l\u00e0 m\u1ed9t thu\u1eadt to\u00e1n xu\u1ea5t ph\u00e1t t\u1eeb c\u00e2y quy\u1ebft \u0111\u1ecbnh (Decision Tree), thu\u1eadt to\u00e1n n\u00e0y th\u1ef1c hi\u1ec7n vi\u1ec7c x\u00e2y d\u1ef1ng tu\u1ea7n t\u1ef1 nhi\u1ec1u c\u00e2y quy\u1ebft \u0111\u1ecbnh v\u00e0 ti\u1ebfn h\u00e0nh qu\u00e1 tr\u00ecnh h\u1ecdc. Ngo\u00e0i LigtGBM, XGBoost c\u0169ng s\u1eed d\u1ee5ng thu\u1eadt to\u00e1n Gradient Boosting.<br>\n<br>\nLightGBM s\u1eed d\u1ee5ng histogram-based algorithms thay th\u1ebf cho c\u00e1c presort-based algorithms th\u01b0\u1eddng \u0111\u01b0\u1ee3c d\u00f9ng trong c\u00e1c boosting tool kh\u00e1c, \u0111\u1ec3 t\u00ecm ki\u1ebfm split point th\u00edch h\u1ee3p trong qu\u00e1 tr\u00ecnh x\u00e2y d\u1ef1ng c\u00e2y quy\u1ebft \u0111\u1ecbnh. LightGBM ph\u00e1t tri\u1ec3n c\u00e2y d\u1ef1a tr\u00ean leaf-wise, l\u1ef1a ch\u1ecdn n\u00fat \u0111\u1ec3 ph\u00e1t tri\u1ec3n c\u00e2y d\u1ef1a tr\u00ean t\u1ed1i \u01b0u to\u00e0n b\u1ed9 c\u00e2y. LightGBM ch\u1ec9 th\u00edch h\u1ee3p cho c\u00e1c b\u1ed9 d\u1eef li\u1ec7u l\u1edbn b\u1edfi leaf-wise v\u1edbi c\u00e1c b\u1ed9 d\u1eef li\u1ec7u nh\u1ecf th\u01b0\u1eddng d\u1eabn \u0111\u1ebfn overfit s\u1edbm.<br>\n<br>\nSo v\u1edbi c\u00e1c framework kh\u00e1c nh\u01b0 XGBoost, pGBRT, v.v, LightGBM s\u1eed d\u1ee5ng hai k\u0129 thu\u1eadt m\u1edbi l\u00e0 Gradient-based One-Side Sampling (GOSS) v\u00e0 Exclusive Feature Bundling (EFB). Hai k\u1ef9 thu\u1eadt n\u00e0y gi\u00fap m\u00f4 h\u00ecnh LightGBRM c\u1ea3i thi\u1ec7n \u0111\u01b0\u1ee3c t\u1ed1c \u0111\u1ed9 t\u00ednh to\u00e1n v\u00e0 \u0111\u1ed9 ch\u00ednh x\u00e1c khi s\u1eed d\u1ee5ng v\u1edbi nh\u1eefng b\u1ed9 d\u1eef li\u1ec7u l\u1edbn.<br>\nGOSS s\u1ebd l\u1ef1a ch\u1ecdn nh\u1eefng data instance c\u00f3 gradient l\u1edbn nh\u1ea5t v\u00e0 ch\u1ecdn ng\u1eabu nhi\u00ean m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng nh\u1ea5t \u0111\u1ecbnh nh\u1eefng data instance c\u00f3 gradient nh\u1ecf \u0111\u1ec3 s\u1eed d\u1ee5ng l\u00e0m d\u1eef li\u1ec7u cho qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n. Trong \u0111\u00f3, nh\u1eefng data c\u00f3 gradient l\u1edbn, theo \u0111\u1ecbnh ngh\u0129a v\u1ec1 m\u1ee9c t\u0103ng th\u00f4ng tin th\u00ec nh\u1eefng data instance n\u00e0y s\u1ebd mang l\u1ea1i nhi\u1ec1u th\u00f4ng tin h\u01a1n. Do \u0111\u00f3, GOSS \u01b0u ti\u00ean l\u1ef1a ch\u1ecdn v\u00e0 gi\u1eef l\u1ea1i nh\u1eefng data c\u00f3 gradient l\u1edbn. <br>\nEFB \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 c\u1ea3i thi\u1ec7n t\u1ed1c \u0111\u1ed9 c\u1ee7a qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n trong m\u00f4 h\u00ecnh LightGBM b\u1eb1ng c\u00e1ch downsample d\u1eef li\u1ec7u, \u1edf \u0111\u00e2y l\u00e0 gi\u1ea3m s\u1ed1 chi\u1ec1u, features c\u1ee7a d\u1eef li\u1ec7u. Nh\u1eefng features kh\u00f4ng \u0111\u1ed3ng th\u1eddi b\u1eb1ng 0 s\u1ebd \u0111\u01b0\u1ee3c EFB chu\u1ea9n ho\u00e1 v\u1ec1 th\u00e0nh m\u1ed9t feature duy nh\u1ea5t \u0111\u1ec3 gi\u1ea3m s\u1ed1 chi\u1ec1u c\u1ee7a d\u1eef li\u1ec7u, t\u1eeb \u0111\u00f3 \u0111\u1ea9y nhanh t\u1ed1c \u0111\u1ed9 hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh. <\/p>\n\n\n### C\u00e0i \u0111\u1eb7t\n<p style=\"font-size:18px\"> \u0110\u1ec3 c\u00e0i \u0111\u1eb7t m\u00f4 h\u00ecnh n\u00e0y, ta s\u1ebd s\u1eed d\u1ee5ng <code>lightgbm<\/code> \u0111\u1ec3 g\u1ecdi h\u00e0m <code>LGBMRegressor()<\/code>. Ngo\u00e0i ra, ch\u00fang ta c\u0169ng s\u1ebd s\u1eed d\u1ee5ng <code>RandomizedSearchCV<\/code> \u0111\u1ec3 tuning hyperparamters cho m\u00f4 h\u00ecnh.<br><\/p>","2df269cd":"# 2. M\u00f4 t\u1ea3 d\u1eef li\u1ec7u\n","16530e5f":"### Nh\u1eadn x\u00e9t\n<p style=\"font-size:18px\"> C\u00e1c features \u0111\u01b0\u1ee3c t\u1ea1o ra tr\u00ean nhi\u1ec1u mi\u1ec1n gi\u00e1 tr\u1ecb kh\u00e1c nhau (th\u1eddi gian, t\u1ea7n s\u1ed1, ...), do \u0111\u00f3 ta s\u1ebd chu\u1ea9n ho\u00e1, bi\u1ebfn \u0111\u1ed5i nh\u1eefng features n\u00e0y v\u1ec1 c\u00f9ng m\u1ed9t mi\u1ec1n gi\u00e1 tr\u1ecb. C\u0169ng nh\u01b0 lo\u1ea1i b\u1ecf c\u00e1c gi\u00e1 tr\u1ecb trung b\u00ecnh v\u00e0 chia t\u1ef7 l\u1ec7 theo ph\u01b0\u01a1ng sai \u0111\u01a1n v\u1ecb. \u0110i\u1ec1u \u0111\u00f3 s\u1ebd gi\u00fap cho c\u00e1c d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n v\u00e0 test thu\u1ed9c c\u00f9ng m\u1ed9t mi\u1ec1n gi\u00e1 tr\u1ecb,c\u00f3 l\u1ee3i h\u01a1n cho m\u00f4 h\u00ecnh trong qu\u00e1 tr\u00ecnh h\u1ecdc v\u00e0 d\u1ef1 \u0111o\u00e1n k\u1ebft qu\u1ea3.<br\n>\n\u1ede \u0111\u00e2y ta s\u1ebd s\u1eed d\u1ee5ng <code>StandardScaler<\/code> \u0111\u1ec3 th\u1ef1c hi\u1ec7n vi\u1ec7c chu\u1ea9n h\u00f3a.<\/p>","2802a4ff":"# 5. X\u1eed l\u00fd d\u1eef li\u1ec7u cho training v\u00e0 testing data","0786a37b":"<p style=\"font-size:18px\"> B\u00e0i to\u00e1n t\u1ed1i \u01b0u h\u00e0m m\u1ea5t m\u00e1t c\u1ee7a h\u1ed3i qui Ridge v\u1ec1 b\u1ea3n ch\u1ea5t l\u00e0 t\u1ed1i \u01b0u song song hai th\u00e0nh ph\u1ea7n bao g\u1ed3m t\u1ed5ng b\u00ecnh ph\u01b0\u01a1ng ph\u1ea7n d\u01b0 v\u00e0 th\u00e0nh ph\u1ea7n \u0111i\u1ec1u chu\u1ea9n. H\u1ec7 s\u1ed1 $ \\alpha $  c\u00f3 t\u00e1c d\u1ee5ng \u0111i\u1ec1u ch\u1ec9nh \u0111\u1ed9 l\u1edbn c\u1ee7a th\u00e0nh ph\u1ea7n \u0111i\u1ec1u chu\u1ea9n t\u00e1c \u0111\u1ed9ng l\u00ean h\u00e0m m\u1ea5t m\u00e1t.<br>\n- $ \\alpha = 0 $, th\u00e0nh ph\u1ea7n \u0111i\u1ec1u chu\u1ea9n b\u1ecb ti\u00eau gi\u1ea3m v\u00e0 b\u00e0i to\u00e1n \u0111\u01b0\u1ee3c quy v\u1ec1 h\u1ed3i qui tuy\u1ebfn t\u00ednh. <br>\n- $ \\alpha $ nh\u1ecf th\u00ec vai tr\u00f2 c\u1ee7a th\u00e0nh ph\u1ea7n \u0111i\u1ec1u chu\u1ea9n tr\u1edf n\u00ean \u00edt quan tr\u1ecdng h\u01a1n. M\u1ee9c \u0111\u1ed9 ki\u1ec3m so\u00e1t t\u00ecnh tr\u1ea1ng overfitting c\u1ee7a m\u00f4 h\u00ecnh s\u1ebd tr\u1edf n\u00ean k\u00e9m h\u01a1n. <br>\n- $ \\alpha $ l\u1edbn, gia t\u0103ng m\u1ee9c \u0111\u1ed9 ki\u1ec3m so\u00e1t l\u00ean c\u00e1c h\u1ec7 s\u1ed1 \u01b0\u1edbc l\u01b0\u1ee3ng v\u00e0 gi\u1ea3m b\u1ee3t hi\u1ec7n t\u01b0\u1ee3ng overfitting. <br>\n<\/p>\n\n### C\u00e0i \u0111\u1eb7t","63466b5b":"<p style=\"font-size:18px\"> H\u1ed3i qui Ridge l\u00e0 m\u1ed9t k\u1ef9 thu\u1eadt \u0111\u1ec3 ph\u00e2n t\u00edch d\u1eef li\u1ec7u h\u1ed3i qui nhi\u1ec1u l\u1ea7n. Khi x\u1ea3y ra \u0111a c\u1ed9ng tuy\u1ebfn, c\u00e1c \u01b0\u1edbc l\u01b0\u1ee3ng b\u00ecnh ph\u01b0\u01a1ng nh\u1ecf nh\u1ea5t l\u00e0 kh\u00f4ng ch\u1ec7ch v\u00e0 ph\u01b0\u01a1ng sai l\u1edbn, d\u1eabn \u0111\u1ebfn c\u00e1c gi\u00e1 tr\u1ecb d\u1ef1 \u0111o\u00e1n sai l\u1ec7ch r\u1ea5t l\u1edbn v\u1edbi gi\u00e1 tr\u1ecb th\u1ef1c t\u1ebf. M\u1ed9t m\u1ee9c \u0111\u1ed9 ch\u1ec7ch \u0111\u01b0\u1ee3c th\u00eam v\u00e0o c\u00e1c \u01b0\u1edbc l\u01b0\u1ee3ng h\u1ed3i qui v\u00e0 k\u1ebft qu\u1ea3 l\u00e0 h\u1ed3i qui l\u00e0m gi\u1ea3m c\u00e1c sai s\u1ed1 ti\u00eau chu\u1ea9n. <br>\n<br>\nH\u00e0m m\u1ea5t m\u1ea5t trong h\u1ed3i qui Ridge s\u1ebd c\u00f3 s\u1ef1 thay \u0111\u1ed5i so v\u1edbi h\u1ed3i qui tuy\u1ebfn t\u00ednh l\u00e0 th\u00e0nh ph\u1ea7n \u0111i\u1ec1u chu\u1ea9n - regularization term \u0111\u01b0\u1ee3c c\u1ed9ng th\u00eam v\u00e0o h\u00e0m m\u1ea5t m\u00e1t:\n<\/p>\n<h2><center> $ L(w) = {1 \\over N} \\begin{Vmatrix} \\bar Xw - y \\end{Vmatrix}_{2}^2 + \\alpha \\begin{Vmatrix} w \\end{Vmatrix}_{2}^2 $ $ = {1 \\over N} \\begin{Vmatrix} \\bar Xw - y \\end{Vmatrix}_{2}^2 +  \\underset{regularization-term}{\\alpha R(w)} $ <\/center><\/h2>","aa41b7d7":"![image.png](attachment:6249d4fc-4962-4089-90e1-154d9f1ec40f.png)","15a587cf":"<p style=\"font-size:18px\"> S\u1ef1 ph\u00e2n b\u1ed1 c\u1ee7a <code>acoustic_data<\/code> nh\u01b0 sau:\n<\/p>","4eff47c9":"<p style=\"font-size:18px\"> C\u00e1c file csv trong folder test t\u1eeb data c\u1ee7a cu\u1ed9c thi d\u00f9ng \u0111\u1ec3 submit k\u1ebft qu\u1ea3 cho b\u00e0i to\u00e1n. <br>\nTa s\u1ebd d\u1ecdc file <code>sample_submission.csv<\/code> v\u00e0 t\u1eeb \u0111\u00f3 t\u1ea1o dataframe x_test c\u00f3 c\u1ea5u tr\u00fac, s\u1ed1 c\u1ed9t b\u1eb1ng v\u1edbi s\u1ed1 c\u1ed9t c\u1ee7a x_train, s\u1ed1 h\u00e0ng t\u01b0\u01a1ng \u1ee9ng v\u1edbi s\u1ed1 file csv trong folder test. <br>\nD\u1eef li\u1ec7u t\u1eeb m\u1ed7i file test csv s\u1ebd l\u00e0 1 record\/h\u00e0ng c\u1ee7a dataframe x_test v\u00e0 l\u1eb7p l\u1ea1i qu\u00e1 tr\u00ecnh t\u1ea1o features t\u1eeb 150000 datapoints nh\u01b0 \u0111\u00e3 l\u00e0m v\u1edbi x_train tr\u01b0\u1edbc \u0111\u00f3.\n<\/p>","5afc9068":"<p style=\"font-size:18px\"> Dataframe x_train sau khi lo\u1ea1i b\u1ecf c\u00e1c features g\u00e2y nhi\u1ec5u s\u1ebd c\u00f3 size l\u00e0 (4194, 138), trong \u0111\u00f3 4194 records t\u01b0\u01a1ng \u1ee9ng v\u1edbi 4194 measurement segments v\u00e0 138 features. <\/p>","73202ac3":"<p style=\"font-size:18px\"> Trong b\u1ed1i c\u1ea3nh s\u1ef1 n\u00f3ng l\u00ean c\u1ee7a to\u00e0n c\u1ea7u \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn kh\u00ed h\u1eadu Tr\u00e1i \u0110\u1ea5t v\u00e0 s\u1ef1 di\u1ec5n ra kh\u00f3 l\u01b0\u1eddng c\u1ee7a c\u00e1c thi\u00ean tai, hi\u1ec7n t\u01b0\u1ee3ng t\u1ef1 nhi\u00ean kh\u1eafc nhi\u1ec7t, d\u1ef1 b\u00e1o \u0111\u1ed9ng \u0111\u1ea5t l\u00e0 m\u1ed9t trong nh\u1eefng v\u1ea5n \u0111\u1ec1 quan tr\u1ecdng c\u1ee7a khoa h\u1ecdc v\u00e0 ng\u00e0nh kh\u00ed t\u01b0\u1ee3ng n\u00f3i chung do nh\u1eefng h\u1eadu qu\u1ea3, thi\u1ec7t h\u1ea1i t\u00e0n kh\u1ed1c m\u00e0 ch\u00fang g\u00e2y ra. Nh\u1eefng nghi\u00ean c\u1ee9u hi\u1ec7n nay v\u1ec1 v\u1ea5n \u0111\u1ec1 n\u00e0y th\u01b0\u1eddng t\u1eadp trung v\u00e0o ba y\u1ebfu t\u1ed1: di\u1ec5n ra khi n\u00e0o, t\u1ea1i khu v\u1ef1c n\u00e0o v\u00e0 quy m\u00f4 ra sao. <\/p>\n<p style=\"font-size:18px\"> Trong b\u00e0i to\u00e1n n\u00e0y, m\u1ee5c ti\u00eau c\u1ee7a ch\u00fang ta d\u1ef1 \u0111o\u00e1n khi \u0111\u1ed9ng \u0111\u1ea5t x\u1ea3y ra. C\u1ee5 th\u1ec3 l\u00e0 d\u1ef1 \u0111o\u00e1n kho\u1ea3ng th\u1eddi gian c\u00f2n l\u1ea1i tr\u01b0\u1edbc khi c\u00e1c tr\u1eadn \u0111\u1ed9ng \u0111\u1ea5t x\u1ea3y ra t\u1eeb d\u1eef li\u1ec7u \u0111\u1ecba ch\u1ea5n theo th\u1eddi gian th\u1ef1c. N\u1ebfu d\u1ef1 \u0111o\u00e1n \u0111\u01b0\u1ee3c c\u00e0ng s\u1edbm v\u00e0 c\u00e0ng ch\u00ednh x\u00e1c th\u00ec ch\u00fang ta s\u1ebd c\u00f3 \u0111\u01b0\u1ee3c nh\u1eefng c\u1ea3nh b\u00e1o v\u00e0 \u0111\u01b0a ra nh\u1eefng bi\u1ec5n ph\u00e1p \u0111\u1ec3 gi\u1ea3m thi\u1ec3u h\u1eadu qu\u1ea3, r\u1ee7i ro v\u00e0 c\u00f3 th\u1ec3 c\u1ee9u s\u1ed1ng r\u1ea5t nhi\u1ec1u ng\u01b0\u1eddi. <\/p>\n","e3cfef71":"## 2.3. File test","e0b4ebc7":"## 3.5. D\u1eef li\u1ec7u trong c\u00e1c file test","92cbd00f":"# 4. Features Generation","2e3f430e":"## 3.4. S\u1ef1 thay \u0111\u1ed5i c\u1ee7a time_to_failure trong measurement segment \u0111\u1ea7u","e650f967":"<p style=\"font-size:18px\"> File train.csv c\u00f3 dung l\u01b0\u1ee3ng 9.56GB n\u00ean th\u1eddi gian \u0111\u1ecdc r\u1ea5t l\u00e2u v\u00e0 t\u01b0\u01a1ng \u0111\u1ed1i t\u1ed1n RAM. N\u00ean ch\u00fang ta s\u1eed d\u1ee5ng th\u01b0 vi\u1ec7n Pandas \u0111\u1ec3 h\u1ed7 tr\u1ee3 \u0111\u1ecdc file CSV th\u00e0nh m\u1ed9t dataframe. <\/p>","517a63c1":"<p style=\"font-size:18px\"> Nh\u01b0 v\u1eady, dataframe df_train c\u00f3 t\u1ea5t c\u1ea3 4194 segments. Ti\u1ebfp theo, ta s\u1ebd kh\u1edfi t\u1ea1o gi\u00e1 tr\u1ecb x_train v\u00e0 y_train. Trong \u0111\u00f3 s\u1ed1 l\u01b0\u1ee3ng index l\u00e0 s\u1ed1 l\u01b0\u1ee3ng segments, y_train c\u00f3 1 c\u1ed9t field d\u1eef li\u1ec7u l\u00e0 <code>time_to_failure<\/code> c\u00f2n c\u00e1c features c\u1ee7a x_train s\u1ebd \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o t\u1eeb field <code>acoustic_data<\/code>.<\/p>","05815a26":"## 2.2. File train.csv","58175217":"### Nh\u1eadn x\u00e9t\n<p style=\"font-size:18px\"> <code>acoustic_data<\/code> l\u00e0 d\u1eef li\u1ec7u th\u00f4 thu \u0111\u01b0\u1ee3c t\u1eeb m\u00e1y \u0111o c\u01b0\u1eddng \u0111\u1ed9 s\u00f3ng \u0111\u1ecba ch\u1ea5t t\u1eeb l\u00f2ng \u0111\u1ea5t, nh\u01b0 v\u1eady ta kh\u00f4ng th\u1ec3 s\u1eed d\u1ee5ng field n\u00e0y nh\u01b0 m\u1ed9t feautre \u0111\u1ec3 hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh.<br>\nV\u00ec v\u1eady, ch\u00fang ta ph\u1ea3i t\u1ea1o ra c\u00e1c features m\u1edbi t\u1eeb <code>acoustic_data<\/code> v\u00e0 l\u1ef1a ch\u1ecdn features ph\u00f9 h\u1ee3p \u0111\u1ec3 s\u1eed d\u1ee5ng cho qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh.<br>\nC\u00e1c features s\u1ebd \u0111\u01b0\u1ee3c kh\u1edfi t\u1ea1o bao g\u1ed3m nh\u1eefng features v\u1ec1 s\u1ed1 li\u1ec7u c\u01a1 b\u1ea3n nh\u01b0 mean (trung b\u00ecnh), std (\u0111\u1ed9 l\u1ec7ch chu\u1ea9n), quantile (l\u01b0\u1ee3ng t\u1eed\/ph\u00e2n v\u1ecb), skew (t\u00ednh b\u1ea5t \u0111\u1ed1i x\u1ee9ng c\u1ee7a ph\u00e2n ph\u1ed1i x\u00e1c su\u1ea5t), v.v c\u1ee7a m\u1ed7i segment ho\u1eb7c c\u1ee7a m\u1ed9t s\u1ed1 slice nh\u1ea5t \u0111\u1ecbnh. Ngo\u00e0i ra ta t\u1ea1o th\u00eam c\u00e1c features b\u1eb1ng c\u00e1ch bi\u1ebfn \u0111\u1ed5i d\u1eef li\u1ec7u t\u1eeb mi\u1ec1n th\u1eddi gian v\u1ec1 mi\u1ec1n t\u1ea7n s\u1ed1 v\u1edbi thu\u1eadt to\u00e1n FFT (Fast Fourier Transform). <br>\n<br>Sau khi t\u1ea1o th\u00eam c\u00e1c features, ch\u00fang ta s\u1ebd c\u1ea7n ph\u1ea3i xem x\u00e9t c\u00e1c bi\u1ec3u \u0111\u1ed3 d\u1eef li\u1ec7u \u0111\u1ec3 \u0111\u00e1nh gi\u00e1 t\u00ednh ph\u00f9 h\u1ee3p c\u1ee7a c\u00e1c features \u0111\u00f3.\n<\/p>","97e36df3":"## test_train\n* <p style=\"font-size:18px\"> Dataframe test_train c\u00f3 l\u01b0\u1ee3ng d\u1eef li\u1ec7u t\u01b0\u01a1ng \u0111\u01b0\u01a1ng v\u1edbi m\u1ed9t measurement segment v\u00e0 c\u1ea7n d\u1ef1 \u0111o\u00e1n v\u1ec1 kho\u1ea3ng th\u1eddi gian c\u00f2n l\u1ea1i tr\u01b0\u1edbc khi x\u1ea3y ra \u0111\u1ed9ng \u0111\u1ea5t . <\/p>","c996c2a2":"<p style=\"font-size:18px\"> Ngo\u00e0i ra, ch\u00fang ta c\u0169ng s\u1ebd th\u1eed submit k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n c\u1ee7a m\u00f4 h\u00ecnh LightGBMRegressor do c\u00f3 \u0111i\u1ec3m MAE th\u1ea5p nh\u1ea5t v\u00e0 so s\u00e1nh v\u1edbi k\u1ebft qu\u1ea3 d\u1ef1 \u0111o\u00e1n sau khi s\u1eed d\u1ee5ng VotingRegressor.<\/p>","6bfe4f00":"## df_train\n* <p style=\"font-size:18px\"> Data frame df_train c\u00f3 l\u01b0\u1ee3ng d\u1eef li\u1ec7u r\u1ea5t l\u1edbn v\u00e0 l\u00e0 d\u1eef li\u1ec7u th\u00f4 l\u1ea5y t\u1eeb th\u1ef1c nghi\u1ec7m. <\/p>\n* <p style=\"font-size:18px\"> Field <code>time_to_failure<\/code> gi\u1ea3m d\u1ea7n theo m\u1ed7i record v\u00e0 m\u1ed7i  measurement segment.  <\/p>\n","3cb7bb6e":"### Nh\u1eadn x\u00e9t\n\n<p style=\"font-size:18px\"> Nh\u01b0 v\u1eady, ta th\u1ea5y \u0111\u01b0\u1ee3c trong 10 tri\u1ec7u data points ta c\u00f3 duy nh\u1ea5t m\u1ed9t l\u1ea7n x\u1ea3y ra \u0111\u1ed9ng \u0111\u1ea5t. <br>\nNgo\u00e0i ra, ta c\u00f3 th\u1ec3 kh\u1eb3ng \u0111\u1ecbnh \u0111\u01b0\u1ee3c gi\u1ea3 thuy\u1ebft \u0111\u00e3 n\u00eau \u1edf m\u1ee5c 3.2, tr\u01b0\u1edbc khi tr\u1eadn \u0111\u1ed9ng \u0111\u1ea5t x\u1ea3y ra t\u1ee9c kh\u1edfi \u0111i\u1ec3m <code>time_to_failure<\/code> \u0111\u1ea1t 0 th\u00ec bi\u00ean \u0111\u1ed9 dao \u0111\u1ed9ng c\u1ee7a gi\u00e1 tr\u1ecb \u0111\u1ecba ch\u1ea5n <code>acoustic_data<\/code> t\u0103ng \u0111\u1ed9t bi\u1ebfn.<br>\nSau \u0111\u00f3 l\u00e0 m\u1ed9t kho\u1ea3ng th\u1eddi gian \u00edt x\u1ea3y ra thay \u0111\u1ed5i l\u1edbn v\u1ec1 gi\u00e1 tr\u1ecb cho t\u1edbi l\u1ea7n \u0111\u1ed9ng \u0111\u1ea5t k\u1ebf ti\u1ebfp.<br>\nTrong 10 tri\u1ec7u data points, gi\u00e1 tr\u1ecb <code>acoustic_data<\/code> ph\u1ea7n l\u1edbn t\u1eadp trung t\u1ea1i kho\u1ea3ng 0 - 10. Ta c\u00f3 th\u1ec3 d\u00f9ng d\u1eef ki\u1ec7n n\u00e0y l\u00e0m c\u01a1 s\u1edf \u0111\u1ec3 lo\u1ea1i b\u1ecf c\u00e1c gi\u00e1 tr\u1ecb g\u00e2y nhi\u1ec5u. <\/p>\n<\/p>\n","991796f1":"# 7. K\u1ebft qu\u1ea3","1a632521":"## Nh\u1eadn x\u00e9t:\n<p style=\"font-size:18px\"> Ta c\u00f3 th\u1ec3 th\u1ea5y MAE score c\u1ee7a m\u00f4 h\u00ecnh LightGBMRegressor l\u00e0 th\u1ea5p nh\u1ea5t trong 3 m\u00f4 h\u00ecnh \u0111\u00e3 \u0111\u01b0\u1ee3c th\u1eed nghi\u1ec7m. AdaBoostRegressor c\u00f3 MAE kh\u00f4ng l\u1edbn h\u01a1n nhi\u1ec1u so v\u1edbi LightGBM, trong khi \u0111\u00f3 m\u00f4 h\u00ecnh Ridge c\u0169ng ch\u1ec9 t\u0103ng MAE tt\u01b0\u01a1ng \u0111\u1ed1i cao so v\u1edbi 2 m\u00f4 h\u00ecnh c\u00f2n l\u1ea1i. Vi\u1ec7c t\u0103ng s\u1ed1 l\u01b0\u1ee3ng fold cho m\u00f4 h\u00ecnh Ridge c\u0169ng kh\u00f4ng l\u00e0m thay \u0111\u1ed5i \u0111\u00e1ng k\u1ec3, MAE score ch\u1ec9 gi\u1ea3m t\u1ed1i \u0111a 0.01.<br>\n<br>\nCu\u1ed1i c\u00f9ng, ch\u00fang ta s\u1ebd s\u1eed d\u1ee5ng th\u01b0 vi\u1ec7n <code>sklearn<\/code> \u0111\u1ec3 g\u1ecdi <code>VotingRegressor<\/code>, l\u00e0 m\u1ed9t c\u00f4ng c\u1ee5 k\u1ebft h\u1ee3p c\u00e1c m\u00f4 h\u00ecnh h\u1ecdc m\u00e1y kh\u00e1c nhau d\u1ef1a tr\u00ean to\u00e0n b\u1ed9 t\u1eadp d\u1eef li\u1ec7u v\u00e0 tr\u1ea3 v\u1ec1 gi\u00e1 tr\u1ecb d\u1ef1 \u0111o\u00e1n trung b\u00ecnh. T\u1eeb \u0111\u00f3 t\u1ea1o th\u00e0nh d\u1ef1 \u0111o\u00e1n cu\u1ed1i c\u00f9ng.\n<\/p>","cc532edb":"<p style=\"font-size:18px\"> Tr\u01b0\u1edbc h\u1ebft ta s\u1ebd lo\u1ea1i b\u1ecf c\u00e1c features c\u00f3 \u0111\u1ed3 th\u1ecb ch\u1ec9 c\u00f3 gi\u00e1 tr\u1ecb 0 nh\u01b0 <code>min_abs<\/code> v\u00e0 <code>quant_01_abs<\/code>.<\/p>","253daadc":"<p style=\"font-size:18px\"> M\u00f4 t\u1ea3 v\u1ec1 x_train. <\/p>","d46dc27a":"<p style=\"font-size:18px\"> Ta s\u1ebd xem x\u00e9t s\u1ef1 thay \u0111\u1ed5i gi\u00e1 tr\u1ecb <code>time_to_failure<\/code> c\u1ee7a 150000 \u0111i\u1ec3m d\u1eef li\u1ec7u \u0111\u1ea7u, t\u01b0\u01a1ng \u1ee9ng v\u1edbi measurement segment \u0111\u1ea7u ti\u00ean.<\/p>","07feb231":"<p style=\"font-size:18px\"> Ta s\u1ebd ti\u1ebfn h\u00e0nh kh\u1edfi t\u1ea1o c\u00e1c features cho x_train b\u1eb1ng c\u00e1ch kh\u1edfi t\u1ea1o features l\u1ea7n l\u01b0\u1ee3t tr\u00ean t\u1eebng segment b\u1eb1ng func <code>generate_feature<\/code> \u1edf tr\u00ean. <\/p>","694a1ac4":"## 3.3. S\u1ef1 thay \u0111\u1ed5i gi\u00e1 tr\u1ecb c\u1ee7a time_to_failure v\u00e0 acoustic_data trong kho\u1ea3ng 10 tri\u1ec7u data points \u0111\u1ea7u\n<p style=\"font-size:18px\"> Ti\u1ebfp theo, ch\u00fang ta s\u1ebd xem x\u00e9t trong 10 tri\u1ec7u d\u1eef li\u1ec7u \u0111\u1ea7u ti\u00ean s\u1ebd c\u00f3 bao nhi\u00eau l\u1ea7n x\u1ea3y ra \u0111\u1ed9ng \u0111\u1ea5t v\u00e0 xu h\u01b0\u1edbng thay \u0111\u1ed5i gi\u00e1 tr\u1ecb c\u1ee7a c\u00e1c field ra sao. <\/p>","408354bd":"<p style=\"font-size:18px\">Data frame df_train c\u00f3 k\u00edch th\u01b0\u1edbc (629145480, 2)\n. T\u1eeb b\u1ed9 d\u1eef li\u1ec7u n\u00e0y, ch\u00fang ta s\u1ebd c\u1ea7n t\u1ea1o ra c\u00e1c feature \u0111\u1ec3 l\u00e0m d\u1eef li\u1ec7u hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh.<br>\nTrong \u0111\u00f3, 5 d\u00f2ng \u0111\u1ea7u ti\u00ean c\u1ee7a df_train:\n<\/p>"}}