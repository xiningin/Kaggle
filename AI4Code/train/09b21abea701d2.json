{"cell_type":{"6bcbe4e9":"code","47d14461":"code","baeb17a6":"code","edfac468":"code","20446d8b":"code","a291e101":"code","3831bcbf":"code","c8a5486b":"code","8cdc0378":"code","3f58e12b":"code","8c55a05c":"code","4118fbf8":"code","6cae9dc2":"code","ce6f0fc5":"code","bb7e8a0c":"code","2980f980":"code","86942b33":"code","dad76e39":"code","c887e4ab":"code","d1397028":"markdown"},"source":{"6bcbe4e9":"#Imports\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","47d14461":"df_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","baeb17a6":"df_train.head(5)","edfac468":"df_train.describe()","20446d8b":"sns.distplot(df_train['SalePrice'], bins=30)\nplt.show()","a291e101":"fig, ax = plt.subplots(figsize=(40,20))   \nsns.heatmap(df_train.corr(),cmap=\"YlGnBu\", annot = True,ax=ax)","3831bcbf":"corr_matrix = df_train.corr().abs()\ndf_train_corr=np.where(corr_matrix['SalePrice']>0.5)\ndf_train_corr=[corr_matrix.columns[x] for x in zip(*df_train_corr)]\ndf_train_hi_corr = df_train[df_train_corr]","c8a5486b":"import plotly.express as px\n\ndf = df_train_hi_corr.copy()\n\nfor i in df.columns:\n    if i == 'SalePrice':\n        break\n    fig = px.scatter(df, x='SalePrice', y=i, trendline='ols', color='SalePrice')\n    #fig.update_traces(marker_line=dict(width=1, color='DarkSlateGray'))\n    fig.show()","8cdc0378":"from scipy import stats\n\n#Now lets remove outliers and Scale the data.\ndf_out = df_train_hi_corr.copy()\n\ndef outliers(df):\n    return df[(np.abs(stats.zscore(df)) < 3).all(axis=1)]\n\ndf_out = outliers(df_out)\nprint('Total outliers removed: {}'.format(len(df_train_hi_corr)-len(df_out)))","3f58e12b":"#Train the scaled X features on the unscaled y. hmm\nX = df_out.drop('SalePrice', axis=1)\ny = df_out['SalePrice'].values.reshape(-1,1)","8c55a05c":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)","4118fbf8":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","6cae9dc2":"score_map = pd.DataFrame()","ce6f0fc5":"from sklearn import linear_model\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\n\nkf = KFold(n_splits=10)\n\nmodel = linear_model.BayesianRidge()\nscores = cross_val_score(model, X_train, y_train.ravel(), cv=kf, scoring='neg_mean_squared_error', verbose=0)\nscore_map['BayesianRidge'] = scores\nmodel.fit(X_train, y_train.ravel())\nmodel.score(X_test, y_test)","bb7e8a0c":"from sklearn.metrics import mean_squared_error, r2_score\n\n#Make predictions\ny_pred = model.predict(X_test)\ny_pred = np.reshape(y_pred, (-1,1))\n\n# The coefficients\nprint(\"Coefficients: \\n\", model.coef_)\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))","2980f980":"# Lets try polinomial regression with L2 with degree for the best fit\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\n\nmodel = Pipeline([('pf', PolynomialFeatures(degree=3)), ('rg',linear_model.Ridge())])\nscores = cross_val_score(model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\nscore_map['PolyRidge'] = scores\nmodel.fit(X_test, y_test)\n#Make predictions\ny_pred = model.predict(X_test)\ny_pred = np.reshape(y_pred, (-1,1))\n\n# The coefficients\nprint(\"Coefficients: \\n\", model.named_steps.rg.coef_)\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))","86942b33":"model = linear_model.LinearRegression()\nkf = KFold(n_splits=10)\nscores = cross_val_score(model, X_train, y_train, cv=kf, scoring='neg_mean_squared_error')\nmodel.fit(X_train, y_train)\nscore_map['linear_reg'] = scores\n\n#Make predictions\ny_pred = model.predict(X_test)\ny_pred = np.reshape(y_pred, (-1,1))\n\n# The coefficients\nprint(\"Coefficients: \\n\", model.coef_)\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))","dad76e39":"from sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\n\nsvr_rbf = SVR(kernel='rbf', C=1e3, gamma=0.1)\ngrid_sv = GridSearchCV(svr_rbf, cv=kf, param_grid={\"C\": [1e0, 1e1, 1e2, 1e3], \"gamma\": np.logspace(-2, 2, 5)}, scoring='neg_mean_squared_error')\ngrid_sv.fit(X_train, y_train.ravel())\nprint(\"Best classifier :\", grid_sv.best_estimator_)\nscores = cross_val_score(svr_rbf, X_train, y_train.ravel(), cv=kf, scoring='neg_mean_squared_error')\nscore_map['SVR'] = scores\nprint(\"MSE: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std()))\n\n#Make predictions\ny_pred = model.predict(X_test)\ny_pred = np.reshape(y_pred, (-1,1))\n\n# The coefficients\nprint(\"Coefficients: \\n\", model.coef_)\n\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))","c887e4ab":"score_map.plot(kind='box')","d1397028":"Some darker squares, which mean higher correlation. The bottom row, SalePrice is the most important. We are going to grab a few columns and initally train the NN on them and see how it does. "}}