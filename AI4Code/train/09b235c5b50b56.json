{"cell_type":{"e2e8efce":"code","65dc6369":"code","a1893ef3":"code","4e457ab4":"code","9ae8a59d":"code","161e6944":"code","76e59f2a":"code","edf40a1a":"code","1c18df20":"code","6b7abeee":"code","6aabf1a0":"code","81ea8189":"code","c31b47e8":"code","b899e18f":"code","004fbec4":"code","4a9de981":"code","2d00a532":"code","92910ea7":"code","4b636917":"markdown","81ce8654":"markdown","092f5d78":"markdown","d481054f":"markdown","79ec37de":"markdown","7cdcd125":"markdown","05c99070":"markdown","629d32ca":"markdown","7b0572a5":"markdown","9a67b4db":"markdown","868bd593":"markdown"},"source":{"e2e8efce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","65dc6369":"# Any results you write to the current directory are saved as output.\ndf = pd.read_csv(\"..\/input\/Admission_Predict_Ver1.1.csv\")\ndf.head()","a1893ef3":"#To check for missing values and data types\ndf.info()","4e457ab4":"#To ignore serial number\ndf = df.iloc[:, 1:]\n#To check the data distribution except for the serial numbers\ndf.describe()","9ae8a59d":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n#Since Research is categorical, correlation plot would not be ideal for that\ndf_lin = df.drop([\"Research\"], axis = 1)\n#print(df_lin.head())\ncorr = df_lin.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nplt.figure(figsize = (10,6))\nsns.heatmap(corr, mask=mask, square=True, linewidths=.5, annot = True, cmap = \"YlGnBu\")","161e6944":"#To select correlations more than 70% and onmly plot them asa function of \"Chance of Admit\"\ndf_corr = corr.iloc[:,corr.shape[1] - 1] > .70\nfeature_list = list(df_corr[df_corr == True].index)\ntarget = feature_list.pop()\nfeature_list, target","76e59f2a":"#Visualising CGPA, GRE Score and TOEFL vs Chance of Admit, separating them by \"Research\"\nfor i, feature in enumerate(feature_list):\n    plt.figure(figsize = (10,10))\n    sns.lmplot(x = target, y = feature, data = df, palette = \"YlGnBu\", fit_reg = True, scatter_kws = {'s': 5}, hue = \"Research\")\n    plt.ylabel(feature)\n    plt.xlabel(target)\n    plt.show()","edf40a1a":"#I have seelcted GRE, TOEFL, CGPA and Research to predict chances of admit. To add research to the features list:\nfeature_list.append(\"Research\")\nfeature_list","1c18df20":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(df[feature_list], df[target], test_size=0.25, random_state=0)\nX_train2, X_test2, y_train2, y_test2 = train_test_split(df.iloc[:, :-1], df[target], test_size=0.25, random_state=0)\nscaler = StandardScaler()\nX_train1 = scaler.fit_transform(X_train1)\nX_test1 = scaler.transform(X_test1)\nX_train2 = scaler.fit_transform(X_train2)\nX_test2 = scaler.transform(X_test2)","6b7abeee":"from sklearn.linear_model import LinearRegression\nregressor1 = LinearRegression()\nregressor2 = LinearRegression()\nregressor1.fit(X_train1, y_train1)\ny_pred_selected_features = regressor1.predict(X_test1)\nregressor2.fit(X_train2, y_train2)\ny_pred_all_features = regressor2.predict(X_test2)","6aabf1a0":"from sklearn.metrics import mean_squared_error\nerror_selected_features = mean_squared_error(y_test1, y_pred_selected_features)\nerror_all_features = mean_squared_error(y_test2, y_pred_all_features)\nerror_selected_features, error_all_features, error_selected_features>error_all_features","81ea8189":"my_data = np.array([340, 120, 4, 3.5, 3.5, 8.2, 1]).reshape(1, -1)\nprint(my_data.shape)\n#Using Linear Regression\nmy_data_scaled = scaler.transform(my_data)\nmy_data_pred = regressor2.predict(my_data_scaled)\n#y_pred_me\nprint(my_data)\nprint(my_data_scaled)\nprint(my_data_pred)","c31b47e8":"from sklearn.ensemble import GradientBoostingRegressor\ngbr1 = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls')\ngbr2 = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls')\ngbr1.fit(X_train1, y_train1)\ny_pred_gbr_selected_features = gbr1.predict(X_test1)\ngbr2.fit(X_train2, y_train2)\ny_pred_gbr_all_features = gbr2.predict(X_test2)","b899e18f":"from sklearn.metrics import mean_squared_error\nerror_gbr_selected_features = mean_squared_error(y_test1, y_pred_gbr_selected_features)\nerror_gbr_all_features = mean_squared_error(y_test2, y_pred_gbr_all_features)\nerror_gbr_selected_features, error_gbr_all_features, error_gbr_selected_features>error_gbr_all_features","004fbec4":"my_data = np.array([340, 120, 4, 3.5, 3.5, 8.2, 1]).reshape(1, -1)\nprint(my_data.shape)\n#Using Linear Regression\nmy_data_scaled = scaler.transform(my_data)\nmy_data_pred = gbr2.predict(my_data_scaled)\n#y_pred_me\nprint(my_data)\nprint(my_data_scaled)\nprint(my_data_pred)","4a9de981":"feature_importance = gbr2.feature_importances_\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nprint(feature_importance)\nsorted_idx = np.argsort(feature_importance)\nsorted_fi = np.sort(feature_importance)\nsorted_features = df.columns[sorted_idx]\nprint(sorted_fi)\nprint(sorted_features)\nplt.figure(figsize = (20,5))\nsns.barplot(sorted_fi, sorted_features, palette = \"YlGnBu\")\n#plt.yticks(feature_importance, sorted_features)\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","2d00a532":"#New Splits and Scaling\nX_train_cgpa, X_test_cgpa, y_train_cgpa, y_test_cgpa = train_test_split(np.array(df[\"CGPA\"]).reshape(-1,1), df[target], test_size=0.25, random_state=0)\nX_train_cgpa = scaler.fit_transform(X_train_cgpa)\nX_test_cgpa = scaler.transform(X_test_cgpa)\n#Fitting Linear Regression Model\nreg_cgpa = LinearRegression()\nreg_cgpa.fit(X_train_cgpa, y_train_cgpa)\ny_pred_lin_cgpa = reg_cgpa.predict(X_test_cgpa)\nerror_lin_cgpa = mean_squared_error(y_test_cgpa, y_pred_lin_cgpa)\n\n#Fitting Gradient boosting Regression Model\ngbr_cgpa = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,max_depth=1, random_state=0, loss='ls')\ngbr_cgpa.fit(X_train_cgpa, y_train_cgpa)\ny_pred_gbr_cgpa = gbr_cgpa.predict(X_test_cgpa)\nerror_gbr_cgpa = mean_squared_error(y_test_cgpa, y_pred_gbr_cgpa)\nprint(error_lin_cgpa, error_gbr_cgpa)","92910ea7":"print(X_train2)","4b636917":"**Predict My Data**","81ce8654":"**Observations**\n\nThe linear regression model with all features still wins the show. However, with Gradient Boosting, the predicatbility of admission using selected features performed insignificantly slightly better than the model with all features.","092f5d78":"**Fitting Linear Regression Model**","d481054f":"**Observation**\nThe MSEs are ~0 which indicates that the predictions are most likely to be reliable. However, the predictability of linear regression model with all the features work insignificantly slightly better than that with selected features.","79ec37de":"**Observations**\n\nCorrelation analysis indicated CGPA, GRE and TOEFL are highly correlated to chances of admission. Gradient Boosting Regressor models agrees with CGPA but ranks GRE and TOEFL significantly much lower than CGPA.\n\nI tested the importance of CGPA for th","7cdcd125":"**Fitting Gradient Boosting Regressor Model**","05c99070":"**EDA, Feature Selection and Admission Predictions using Linear Regression and Gradient Boosting Regression**","629d32ca":"**Question:How good will be the prediction of chances of admission based on CGPA only?**","7b0572a5":"**Observations:**\n1. GRE Score: Good performers have scored good in GRE, TOEFL as well as in GPAs. Univeristy rating does not affect the performance of students.\n2. Chances of Admit is highly correrlated to (in order of corr co-eff) i. CGPA , ii. GRE iii. TOEFL scores","9a67b4db":"**Predict My Data**","868bd593":"**Feature Importance from Gradient Boosting Regression Model**"}}