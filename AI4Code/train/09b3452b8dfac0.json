{"cell_type":{"4912aa57":"code","b09941cc":"code","8fd73a29":"code","e4525d4f":"code","19f6aa96":"code","93113a22":"code","3eca4ff8":"code","961715bd":"code","f77e9e2f":"code","9d97a9e7":"code","e7e08184":"code","8f3176c1":"code","301c61a0":"code","392eb7e8":"code","98a4e562":"code","c34c59cf":"code","8176ebfe":"code","4b5ca9b0":"code","bb8b717f":"code","1a5b5687":"code","731720de":"code","02f0e4a6":"code","fe177f45":"code","4080fb82":"code","fdd26842":"code","5d428ab4":"code","e2b6d8da":"code","05f9024c":"code","f34e241d":"code","eb09ae0e":"code","1ab30139":"code","810c73d9":"code","9a5e5f99":"code","4210b119":"code","4b4ac2d9":"code","d8d9bba2":"code","45de6a91":"code","c3bcf5d6":"code","fb8dba5d":"code","d9a5cc5e":"code","1c0866e3":"code","f251e484":"code","0b6a2194":"code","293a8859":"code","4661d6eb":"code","846c26d3":"code","7c155555":"code","7c5bbe98":"code","f4fda311":"code","7eacdb37":"code","d9315e54":"code","a436c591":"code","8489fd71":"code","1b324885":"code","2d7ae06e":"markdown","eff5cbcd":"markdown","570f55fb":"markdown","8e7fc34a":"markdown","f219f678":"markdown","06b9940c":"markdown","c514cb64":"markdown","f57b0d66":"markdown","ac29eb51":"markdown","e99989c9":"markdown","4d7d507e":"markdown","648025c1":"markdown","db4c2bfe":"markdown","79d141f8":"markdown","5808a3c5":"markdown"},"source":{"4912aa57":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","b09941cc":"data = pd.read_csv(\"..\/input\/heart-failure-prediction\/heart.csv\")","8fd73a29":"data","e4525d4f":"data.info()","19f6aa96":"data.describe()","93113a22":"data.isnull().sum()","3eca4ff8":"data['Sex'].value_counts()","961715bd":"sns.set_style('whitegrid')\nplt.figure(figsize=(12,6))\nsns.countplot(x=\"RestingECG\", data=data, palette='Blues');","f77e9e2f":"sns.set_style('whitegrid')\nplt.figure(figsize=(12,6))\nsns.countplot(x=\"Sex\", data=data, palette='Blues');","9d97a9e7":"data.shape","e7e08184":"data.corr()","8f3176c1":"plt.figure(figsize=(15,10))\nmatrix=np.triu(data.corr())\nsns.heatmap(data.corr(), annot=True, mask=matrix,square=True, cmap=\"icefire\")\nplt.yticks(rotation=0)\nplt.show()","301c61a0":"data.columns","392eb7e8":"mean_col=['Age', 'Sex', 'ChestPainType', 'RestingBP', 'Cholesterol', 'FastingBS',\n       'RestingECG', 'MaxHR', 'ExerciseAngina', 'Oldpeak', 'ST_Slope',\n       'HeartDisease']\nsns.pairplot(data[mean_col], hue='Sex', palette='vlag')","98a4e562":"plt.hist(data[\"Age\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Count\")\nplt.title(\"How many people belong to different ages in the dataset\")\nplt.show()","c34c59cf":"data.columns","8176ebfe":"sns.set_style('whitegrid')\nplt.figure(figsize=(12,6))\nsns.countplot(x=\"ChestPainType\", data=data, palette='Spectral');\nplt.title(\"Plot show the count according to the ChestPainType\")","4b5ca9b0":"plt.figure(figsize =(20, 10))\ndata.boxplot()\nplt.show()","bb8b717f":"px.box(data,x = \"Age\", y = \"ChestPainType\")","1a5b5687":"px.box(data,x = \"Age\", y = \"RestingBP\")","731720de":"px.box(data,x = \"Sex\", y = \"RestingBP\")","02f0e4a6":"from sklearn.preprocessing import LabelEncoder\n\nencode = LabelEncoder()","fe177f45":"data.head(2)","4080fb82":"cat = ['Sex','ChestPainType','ExerciseAngina','ST_Slope','RestingECG']\nfor i in cat:\n    data[i] = encode.fit_transform(data[i])\ndata.head(3)","fdd26842":"x=data.iloc[:,:-1]\nprint(x)\ny = data.iloc[:,-1]\nprint (y)","5d428ab4":"from sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state=42)","e2b6d8da":"x_train.shape","05f9024c":"y_train.shape","f34e241d":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()","eb09ae0e":"x_train = scaler.fit_transform(x_train)","1ab30139":"x_test = scaler.fit_transform(x_test)","810c73d9":"from sklearn.linear_model import LogisticRegression\nlr_model = LogisticRegression()\nlr_model.fit(x_train, y_train)","9a5e5f99":"y_pred=lr_model.predict(x_test)\nfrom sklearn.metrics import accuracy_score, classification_report,confusion_matrix, mean_squared_error\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\nprint(\"Training Score:\", lr_model.score(x_train, y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Logistic Regression Model is: \", accuracy_score(y_test, y_pred)*100 , \"%\")\nlr_model_results=accuracy_score(y_test, y_pred)*100","4210b119":"#Checking out the actual vs predicted values with the \ndata = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndata","4b4ac2d9":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=6, random_state=123)\ndtree.fit(x_train, y_train)","d8d9bba2":"y_pred=dtree.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",dtree.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Decision Tree Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\ndtree_results=accuracy_score(y_test, y_pred)*100","45de6a91":"from sklearn.ensemble import RandomForestClassifier\nrfc=RandomForestClassifier()\nrfc.fit(x_train, y_train)","c3bcf5d6":"y_pred=rfc.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",rfc.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Random Forest Classifier Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\nrfc_results=accuracy_score(y_test, y_pred)*100","fb8dba5d":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=7)\n\nknn.fit(x_train, y_train)","d9a5cc5e":"y_pred=knn.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",knn.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the K Neareset Neighbors Classifier Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\nknn_results=accuracy_score(y_test, y_pred)*100","1c0866e3":"from sklearn.svm import SVC\nsvc=SVC()\n\nsvc.fit(x_train, y_train)","f251e484":"y_pred=svc.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",svc.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Support Vector Machine Classification Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\nsvc_results=accuracy_score(y_test, y_pred)*100","0b6a2194":"from sklearn.ensemble import AdaBoostClassifier\nadb=AdaBoostClassifier(base_estimator = None)\n\nadb.fit(x_train, y_train)","293a8859":"y_pred=adb.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",adb.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Ada Boost Classifier Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\nadb_results=accuracy_score(y_test, y_pred)*100","4661d6eb":"from sklearn.ensemble import GradientBoostingClassifier\ngbc=GradientBoostingClassifier()\n\ngbc.fit(x_train, y_train)","846c26d3":"y_pred=gbc.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",gbc.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Gradient Boost Classifier Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\ngbc_results=accuracy_score(y_test, y_pred)*100","7c155555":"from xgboost import XGBClassifier\nxgb=XGBClassifier(objective ='reg:linear', colsample_bytree= 0.3, learning_rate=0.1,\n                  max_depth=5, alpha=10, n_estimators=10)\n\nxgb.fit(x_train, y_train)","7c5bbe98":"y_pred=xgb.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",xgb.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Extreme Gradient Boosting Classifier Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\nxgb_results=accuracy_score(y_test, y_pred)*100","f4fda311":"from sklearn.naive_bayes import GaussianNB\ngnb=GaussianNB()\n\ngnb.fit(x_train, y_train)","7eacdb37":"y_pred=gnb.predict(x_test)\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Training Score: \",gnb.score(x_train,y_train)*100)\n#Printing the accuracy of the model\nprint(\"The accuracy of the Naive Bayes Model is: \", accuracy_score(y_test, y_pred)*100, \"%\")\ngnb_results=accuracy_score(y_test, y_pred)*100","d9315e54":"from sklearn.linear_model import SGDClassifier\n\nsgdc = SGDClassifier()","a436c591":"sgdc.fit(x_train,y_train)\npred=sgdc.predict(x_test)","8489fd71":"from sklearn.metrics import accuracy_score,classification_report,confusion_matrix,mean_squared_error\nprint(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))\n\nprint(\"Training Score: \",sgdc.score(x_train,y_train)*100)\n\nprint(classification_report(y_test, pred))\nprint(\"The accuracy of the SGD Model is: \",accuracy_score(y_test,pred)*100,\"%\")\nsgdc_result =accuracy_score(y_test,pred)*100","1b324885":"print(\"Logistic Regression: \",lr_model_results)\nprint(\"Decision Tree: \",dtree_results)\nprint(\"Random Forest: \",rfc_results)\nprint(\"K Nearest Neighbors: \",knn_results)\nprint(\"Support Vector Machine: \",svc_results)\nprint(\"Ada Boost: \",adb_results)\nprint(\"Gradient Boost: \",gbc_results)\nprint(\"Extreme Gradient Boost: \",xgb_results)\nprint(\"Naive Bayes: \",gnb_results)\nprint(\"Stochastic Gradient Descent (SGD): \",sgdc_result)","2d7ae06e":"### Scaling the dataset ","eff5cbcd":"## 8.Extreme Gradient Boosting(XGB) Classifier ","570f55fb":"## 3.Random Forest Classifier ","8e7fc34a":"##### Box plot to detect the outliers ","f219f678":"## 1.Logistic Regression","06b9940c":"## 4. K Nearest Neighbors Classifier (KNN)","c514cb64":"## 7.Gradient Boost Classifier","f57b0d66":"## 9.Naive Bayes ","ac29eb51":"## 6. ADA Boost Classifier ","e99989c9":"## 10.Stochastic Gradient Descent (SGD)","4d7d507e":"## Support Vector Machine Classifier (SVM)","648025c1":"### Exploring data ","db4c2bfe":"### Model building ","79d141f8":"## 2. Decision Tree Classifier","5808a3c5":"### Creating a test and train set "}}