{"cell_type":{"39a22b45":"code","e49f44cb":"code","1aaf72d4":"code","92602e04":"code","d702e22d":"code","f234344d":"code","4081246a":"code","c7cb92c4":"code","78e80453":"code","005807ff":"code","c4195234":"code","79c6294d":"code","1afbecd4":"code","48190c51":"code","b714e7be":"code","96221e36":"code","61c9270d":"code","cdd10cf1":"code","6fad9a07":"code","c4f585fc":"code","56ce88e1":"code","7eeec177":"code","f933a8c7":"markdown","70c54b2f":"markdown"},"source":{"39a22b45":"import pandas as pd\nimport numpy as np\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline \nfrom sklearn.utils import shuffle\nimport matplotlib.pyplot as plt\nfrom numpy import array\nfrom numpy import argmax\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_selection import chi2\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Activation\nfrom keras.utils import np_utils\nimport re\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier","e49f44cb":"# Set your own project id here\nPROJECT_ID = 'your-google-cloud-project'\nfrom google.cloud import storage\nstorage_client = storage.Client(project=PROJECT_ID)","1aaf72d4":"names=['URL','Category']\n#df=pd.read_csv( \"..\/input\/website-classification-using-url\/URL Classification.csv\")\n#df=pd.read_csv('..\/input\/Website classification using URL\/URL Classification.csv')\ndf=pd.read_csv('..\/input\/website-classification-using-url\/URL Classification.csv',names=names, na_filter=False)","92602e04":"\"\"\"import re\ndef create_dataset(text):\n  urls = list()\n  for rows in text:\n      row = re.sub('[^a-zA-Z]', ' ', rows)\n      row = row.split()\n      row = ' '.join(row[2:9])\n      urls.append(row)\n  print('this is URL',df['URL'][1004:1008])\n  print('-'*30)\n  print('this is processed urls\\n', urls[1004:1008])\n  dataset = pd.DataFrame(list(zip(urls, df['Category'])), columns = ['URL', 'Category'])\n  return dataset\n\ndataset = create_dataset(text = df['URL'])\ndataset[:2]\"\"\"","d702e22d":"dataset = df[:]\nadult = dataset[1:2000]\narts = dataset[50000:52000]\nbusiness = dataset[520000:522000]\ncomputers = dataset[535300:537300]\ngames = dataset[650000:652000]\nhealth = dataset[710000:712000]\nhome =  dataset[764200:766200]\nkids =  dataset[793080:795080]\nnews =  dataset[839730:841730]\nrecreation =  dataset[850000:852000]\nreference =  dataset[955250:957250]\nscience =  dataset[1013000:1015000]\nshopping =  dataset[1143000:1145000]\nsociety =  dataset[1293000:1295000]\nsports =  dataset[1492000:1494000]\n\ntest_data = pd.concat([adult, arts, business, computers, games, health, home, \n              kids, news, recreation, reference,science,shopping, society, sports], axis=0)\n\ndataset.drop(dataset.index[1:2000],inplace= True)\ndataset.drop(dataset.index[50000:52000],inplace= True)\ndataset.drop(dataset.index[520000:522000],inplace= True)\ndataset.drop(dataset.index[535300:537300],inplace= True)\ndataset.drop(dataset.index[650000:652000],inplace= True)\ndataset.drop(dataset.index[710000:712000],inplace= True)\ndataset.drop(dataset.index[764200:766200],inplace= True)\ndataset.drop(dataset.index[793080:795080],inplace= True)\ndataset.drop(dataset.index[839730:841730],inplace= True)\ndataset.drop(dataset.index[850000:852000],inplace= True)\ndataset.drop(dataset.index[955250:957250],inplace= True)\ndataset.drop(dataset.index[1013000:1015000],inplace= True)\ndataset.drop(dataset.index[1143000:1145000],inplace= True)\ndataset.drop(dataset.index[1293000:1295000],inplace= True)\ndataset.drop(dataset.index[1492000:1494000],inplace= True)\ndataset.tail()","f234344d":"import seaborn as sns\nax = sns.countplot(y=\"Category\",  data=df )\nplt.title(\"Visualization of the dataset\", y=1.01, fontsize=20)\nplt.ylabel(\"Name of the Category\", labelpad=15)\nplt.xlabel(\"Number of Categories of URLs\", labelpad=15)\ndf.shape","4081246a":"ax = sns.countplot(y = \"Category\",  data = dataset )\nplt.title(\"Visualization of the train dataset\", y=1.01, fontsize=20)\nplt.ylabel(\"Name of the Category\", labelpad=15)\nplt.xlabel(\"Number of Categories of URLs\", labelpad=15)\ndataset.shape","c7cb92c4":"ax = sns.countplot(y = \"Category\",  data = test_data , color = 'gray')\nplt.title(\"Visualization of the test dataset\", y=1.01, fontsize=20)\nplt.ylabel(\"Name of the Category\", labelpad=15)\nplt.xlabel(\"Number of Categories of URLs\", labelpad=15)","78e80453":"dataset = pd.get_dummies(dataset  ,prefix='Category', columns = ['Category'])\ntest_data = pd.get_dummies(test_data  ,prefix='Category', columns = ['Category'])\n#print('df', df[:2])\nbackup_df = df\n#print('df1', backup_df[:2])\n#print('df2', df2[:2])\ndataset[:2]\ntest_data[:2]","005807ff":"X_train=dataset['URL']\ny_train=dataset.iloc[: , 1:16].values\nprint(y_train)\nprint( 'y_train shape' , y_train.shape)\n\nX_test=test_data['URL']\ny_test=test_data.iloc[: , 1:16].values\nprint( 'y_test shape' , y_test.shape)","c4195234":"from keras.preprocessing.text import Tokenizer\ndef create_and_train_tokenizer(texts):\n    tokenizer=Tokenizer()\n    tokenizer.fit_on_texts(texts)\n    return tokenizer\n\nfrom keras.preprocessing.sequence import pad_sequences\ndef encode_reviews(tokenizer, max_length, docs):\n    encoded=tokenizer.texts_to_sequences(docs) \n    padded=pad_sequences(encoded, maxlen=max_length, padding=\"post\")\n    return padded\n\ntokenizer=create_and_train_tokenizer(texts = X_train)\nvocab_size=len(tokenizer.word_index) + 1\nprint(\"Vocabulary size:\", vocab_size)\n\nmax_length=max([len(row.split()) for row in X_train])\nprint(\"Maximum length:\",max_length)\n\nX_train_encoded = encode_reviews(tokenizer, max_length, X_train)\nX_test_encoded = encode_reviews(tokenizer, max_length, X_test)\nprint('x_train shape:', X_train_encoded.shape)\nprint('x_test shape:', X_test_encoded.shape)","79c6294d":"X_train_encoded[:2]","1afbecd4":"from keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Embedding\nfrom keras.layers import Conv1D, GlobalMaxPooling1D\nfrom keras.datasets import imdb\nfrom keras import layers, models\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout","48190c51":"from keras import layers, models\nimport tensorflow as tf\nfrom tensorflow.keras import regularizers\nfrom keras.regularizers import l2\n# detect the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\nstrategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# use TPUStrategy scope to define model\nwith strategy.scope():\n    def create_embedding_model(vocab_size, max_length):\n        model=models.Sequential()\n        model.add(layers.Embedding(vocab_size, 100, input_length=max_length))\n\n        model.add(layers.Conv1D(256, 2 , activation=\"relu\"))\n        #model.add(layers.BatchNormalization())\n        model.add(layers.MaxPooling1D())\n\n        model.add(layers.Conv1D(256, 2, activation=\"relu\"))\n        #model.add(layers.BatchNormalization())\n        model.add(layers.MaxPooling1D())\n        \n        model.add(layers.Conv1D(256, 2, activation=\"relu\"))\n        #model.add(layers.BatchNormalization())\n        model.add(layers.MaxPooling1D())\n\n        model.add(layers.Flatten())\n        model.add(layers.Dropout(0.5))\n        model.add(layers.Dense(128, kernel_regularizer=l2(0.01), bias_regularizer=l2(0.01), activation=\"relu\"))\n        \n        model.add(layers.Dense(15,  activation=\"softmax\"))   \n        return model\n\n    \nembedding_model = create_embedding_model(vocab_size=vocab_size , max_length=max_length)\nembedding_model.summary()","b714e7be":"from keras.optimizers import SGD\nopt = SGD(lr=0.01, momentum=0.9)\nembedding_model.compile(loss='categorical_crossentropy',\n              optimizer= opt,\n              metrics=['accuracy'])\n    \n#earlyStopping = EarlyStopping(monitor=\"val_accuracy\", patience=1)\nmodelHistory = embedding_model.fit(X_train_encoded, \n                                   y_train, \n                                   validation_data=(X_test_encoded, y_test),\n                                   epochs= 40)              ","96221e36":"print(len(y_train) + len(y_test))\nprint(len(X_train_encoded))","61c9270d":"_, acc = embedding_model.evaluate(X_train_encoded, y_train, verbose=0)\nprint(\"Train accuracy:{:.2f}\".format(acc*100))\n_,acc= embedding_model.evaluate(X_test_encoded, y_test, verbose=0)\nprint(\"Test accuracy:{:.2f}\".format(acc*100))","cdd10cf1":"# show a nicely formatted classification report\ny_pred = (embedding_model.predict(X_test_encoded))\ny_pred = (y_pred > 0.5) \n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred, digits = 4))","6fad9a07":"import tensorflow as tf\n#saving the model\nembedding_model.save('classification_model(opt(2,2)).h5')\n# Recreate the exact same model, including its weights and the optimizer\nnew_model = tf.keras.models.load_model('classification_model(opt(2,2)).h5')\n# Show the model architecture","c4f585fc":"from keras.callbacks import History \nhistory = History()\ntry:\n    loss_train = modelHistory.history['loss']\n    loss_val = modelHistory.history['val_loss']\n    epochs = range(1,41)\n    plt.plot(epochs, loss_train, 'g', label='Training loss')\n    plt.plot(epochs, loss_val, 'b', label='validation loss')\n    plt.title('Training and Validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend(),\n    plt.show()\nexcept:\n    pass","56ce88e1":"try:\n    loss_train = modelHistory.history['accuracy']\n    loss_val = modelHistory.history['val_accuracy']\n    epochs = range(1,41)\n    plt.plot(epochs, loss_train, 'g', label='Training accuracy')\n    plt.plot(epochs, loss_val, 'b', label='validation accuracy')\n    plt.title('Training and Validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\nexcept:\n    pass","7eeec177":"from nltk.tokenize import word_tokenize \nurl = ['http:\/\/www.bdnews24.com', 'http:\/\/www.bdnews24.com',  'http:\/\/www.yuzutree.com', 'http:\/\/www.fcbarcelona.com']\n#,'http:\/\/www.bdnews24.com', 'http:\/\/www.yuzutree.com'\n# url1 = perform_stemming(websites = url)\noutput = list()\nword1 = list()\ndef processed_urls(website):\n    for url in website:\n        if len(url)>20:\n            url_data = url[:20]\n        else:\n            url_data = url\n            \n        #word1 = ' '.join(url_data)\n        output.append(url_data)\n        \n    return output\n\nfrom keras.preprocessing.text import Tokenizer\ntokenizer=Tokenizer()\nprocessed_url = processed_urls(url)\npadded = encode_reviews(tokenizer, max_length ,processed_url)\n\ncategories = ['adult', 'arts', 'business', 'computers', 'games', 'health', 'home', \n              'kids', 'news', 'recreation', 'reference','science','shopping', 'society', 'sports']\n\ny_pred1= embedding_model.predict(padded)\ncount  = 0\ncategory = 0\nfor numbers in y_pred1:\n  max_number = 1e-10\n  count = 0;\n  for number in numbers:\n    count+=1\n    if number > max_number:\n      max_number = number;\n      category = count\n  print('category is :', categories[category-1:category])\n\nprint(category)\n#y_pred\n#from sklearn.metrics import classification_report\n#print(classification_report(y_test, y_pred, digits = 4))","f933a8c7":"**This is for understandind the basics of stemming it transforms the words to its root**\n* >  stem_vectorizer = stemmed_count_vect\n* > text1 = '\u00ef am so much bored because of your meaningless behaviour'\n* > print(stem_vectorizer.fit_transform([text1]))\n* > print(stem_vectorizer.get_feature_names())","70c54b2f":"import re\nimport nltk\nfrom sklearn.pipeline import Pipeline\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\nclass StemmedCountVectorizer(CountVectorizer):\n    def build_analyzer(self):\n        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n    \nstemmed_count_vect = StemmedCountVectorizer(stop_words='english', ngram_range=(2,2))\n\ngs_clf = Pipeline([('vect', stemmed_count_vect),\n                   ('tfidf', TfidfTransformer()),\n                   ('clf', SGDClassifier(loss='perceptron', penalty='l2',\n                    alpha =1e-4 , max_iter=20 ,tol=None)),\n   ])\ngs_clf = gs_clf.fit(X_train, y_train)\n"}}