{"cell_type":{"f3c7fc3f":"code","fc505340":"code","25709873":"code","dee50df7":"code","5e8ca18b":"code","8113fa01":"code","cc3ac064":"code","e5b6265e":"code","ac932d00":"code","1f953435":"code","f5b3b468":"code","66699673":"code","c97b8fa4":"code","3939ea05":"code","b27846c7":"code","abad095e":"code","7d3fbd03":"code","8501a2af":"code","95020244":"code","7001e5a8":"code","fb9b9052":"markdown","f2a31e10":"markdown","1191807d":"markdown","bbb7b048":"markdown","e4d149df":"markdown","72e57167":"markdown","380305fe":"markdown","19e66357":"markdown","67bece44":"markdown","c3e647c9":"markdown","5e6b7d92":"markdown","e0b83704":"markdown"},"source":{"f3c7fc3f":"!pip install -q \"monai-weekly[gdown, nibabel, tqdm, itk]\"","fc505340":"import os\nimport shutil\nimport tempfile\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport cv2\nfrom sklearn.metrics import classification_report\n\nimport torch\n\nfrom monai.apps import download_and_extract\nfrom monai.config import print_config\nfrom monai.metrics import ROCAUCMetric\nfrom monai.networks.nets import DenseNet121\nfrom monai.transforms import *\nfrom monai.data import Dataset, DataLoader\nfrom monai.utils import set_determinism\n\n#print_config()","25709873":"data_dir = '..\/input\/gujarati-ocr-typed-gujarati-characters\/Gujarati\/Train' \ntest_dir='..\/input\/gujarati-ocr-typed-gujarati-characters\/Gujarati\/Test'\n\nclass_names0 = os.listdir(data_dir)\nclass_names=sorted(class_names0)\n\nnum_class = len(class_names)\nimage_files = [[os.path.join(data_dir, class_name, x) \n               for x in os.listdir(os.path.join(data_dir, class_name))] \n               for class_name in class_names]\n\ntimage_files = [[os.path.join(test_dir, class_name, x) \n               for x in os.listdir(os.path.join(test_dir, class_name))] \n               for class_name in class_names]","dee50df7":"image_file_list = []\nimage_label_list = []\nfor i, class_name in enumerate(class_names):\n    image_file_list.extend(image_files[i])\n    image_label_list.extend([i] * len(image_files[i]))\nnum_total = len(image_label_list)","5e8ca18b":"timage_file_list = []\ntimage_label_list = []\nfor i, class_name in enumerate(class_names):\n    timage_file_list.extend(timage_files[i])\n    timage_label_list.extend([i] * len(timage_files[i]))\ntnum_total = len(timage_label_list)","8113fa01":"image_width, image_height = Image.open(image_file_list[0]).size\n\nprint('Total image count:', num_total)\nprint(\"Image dimensions:\", image_width, \"x\", image_height)\nprint(\"Label names:\", class_names)\nprint(\"Label counts:\", [len(image_files[i]) for i in range(num_class)])","cc3ac064":"plt.subplots(3,3, figsize=(12,12))\nfor i,k in enumerate(np.random.randint(num_total, size=9)):\n    im = Image.open(image_file_list[k])\n    arr = np.array(im)\n    plt.subplot(3,3, i+1)\n    plt.xlabel(class_names[image_label_list[k]])\n    plt.imshow(arr, cmap='gray', vmin=0, vmax=255)\nplt.tight_layout()\nplt.show()","e5b6265e":"valid_frac = 0.1\ntrainX,trainY = [],[]\nvalX,valY = [],[]\nfor i in range(num_total):\n    rann = np.random.random()\n    if rann < valid_frac:\n        valX.append(image_file_list[i])\n        valY.append(image_label_list[i])\n    else:\n        trainX.append(image_file_list[i])\n        trainY.append(image_label_list[i])\n\nprint(len(trainX),len(valX))","ac932d00":"testX,testY = [],[] \nfor i in range(tnum_total):   \n    testX.append(timage_file_list[i])\n    testY.append(timage_label_list[i])\n    \nprint(len(testX))","1f953435":"trainX=np.array(trainX)\ntrainY=np.array(trainY)\nvalX=np.array(valX)\nvalY=np.array(valY)\ntestX=np.array(testX)\ntestY=np.array(testY)","f5b3b468":"class SumDimension(Transform):\n    def __init__(self, dim=1):\n        self.dim = dim\n\n    def __call__(self, inputs):\n        return inputs.sum(self.dim)","66699673":"class Astype(Transform):\n    def __init__(self, type='uint8'):\n        self.type = type\n    def __call__(self, inputs):\n        return inputs.astype(self.type)","c97b8fa4":"train_transforms = Compose([\n    LoadImage(image_only=True),\n    SumDimension(2),    \n    NormalizeIntensity(),\n    Astype(),\n    #ScaleIntensity(),\n    #Astype(),\n    #RandRotate(range_x=10, prob=0.2, keep_size=True),\n    #RandFlip(spatial_axis=0, prob=0.5),\n    #RandZoom(min_zoom=1.0, max_zoom=1.1, prob=0.3, keep_size=True),\n    AddChannel(),\n    ToTensor(),\n])\n\nval_transforms = Compose([\n    LoadImage(image_only=True),\n    SumDimension(2),    \n    NormalizeIntensity(),\n    Astype(),\n    #ScaleIntensity(),\n    #Astype(),    \n    AddChannel(),\n    ToTensor(),\n])\n\nact = Activations(softmax=True)\nto_onehot = AsDiscrete(to_onehot=num_class, n_classes=num_class)","3939ea05":"class MedNISTDataset(Dataset):\n\n    def __init__(self, image_files, labels, transforms):\n        self.image_files = image_files\n        self.labels = labels\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, index):\n        return self.transforms(self.image_files[index]), self.labels[index]","b27846c7":"train_ds = MedNISTDataset(trainX, trainY, train_transforms)\ntrain_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n\nval_ds = MedNISTDataset(valX, valY, val_transforms)\nval_loader = DataLoader(val_ds, batch_size=64, num_workers=2)\n\ntest_ds = MedNISTDataset(testX, testY, val_transforms)\ntest_loader = DataLoader(test_ds, batch_size=64, num_workers=2)","abad095e":"device = torch.device(\"cuda:0\")   #\"cuda:0\",cpu\nmodel = DenseNet121(\n    spatial_dims=2,            \n    in_channels=1,\n    out_channels=num_class,\n).to(device)\n\nloss_function = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), 1e-5)\nepoch_num = 10    ###10\nval_interval = 1","7d3fbd03":"best_metric = -1\nbest_metric_epoch = -1\nepoch_loss_values = list()\nauc_metric = ROCAUCMetric()\nmetric_values = list()\n\nfor epoch in range(epoch_num):\n    print('-' * 10)\n    print(f\"epoch {epoch + 1}\/{epoch_num}\")\n    model.train()\n    epoch_loss = 0\n    step = 0\n\n    for batch_data in train_loader:\n        step += 1\n        inputs, labels = batch_data[0].to(device), batch_data[1].to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs.float())     ##### .float()\n        loss = loss_function(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        epoch_loss += loss.item()\n        print(f\"{step}\/{len(train_ds) \/\/ train_loader.batch_size}, train_loss: {loss.item():.4f}\")\n        epoch_len = len(train_ds) \/\/ train_loader.batch_size\n\n    epoch_loss \/= step\n    epoch_loss_values.append(epoch_loss)\n    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n\n    if (epoch + 1) % val_interval == 0:\n        model.eval()\n        with torch.no_grad():\n            y_pred = torch.tensor([], dtype=torch.float32, device=device)\n            y = torch.tensor([], dtype=torch.long, device=device)\n            for val_data in val_loader:\n                val_images, val_labels = val_data[0].to(device), val_data[1].to(device)\n                y_pred = torch.cat([y_pred, model(val_images.float())], dim=0)    ##### .float()\n                y = torch.cat([y, val_labels], dim=0)\n                \n            y_onehot = [to_onehot(i) for i in y]\n            y_pred_act = [act(i) for i in y_pred]\n            auc_metric(y_pred_act, y_onehot)\n            auc_result = auc_metric.aggregate()\n            auc_metric.reset()\n            del y_pred_act, y_onehot\n            metric_values.append(auc_result)\n            acc_value = torch.eq(y_pred.argmax(dim=1), y)\n            acc_metric = acc_value.sum().item() \/ len(acc_value)\n            \n            if acc_metric > best_metric:\n                best_metric = acc_metric\n                best_metric_epoch = epoch + 1\n                torch.save(model.state_dict(), 'best_metric_model.pth')\n                print('saved new best metric model')\n                \n            print(f\"current epoch: {epoch + 1} current AUC: {auc_result:.4f}\"\n                  f\" current accuracy: {acc_metric:.4f} best AUC: {best_metric:.4f}\"\n                  f\" at epoch: {best_metric_epoch}\")\n            \nprint(f\"train completed, best_metric: {best_metric:.4f} at epoch: {best_metric_epoch}\")\n","8501a2af":"plt.figure('train', (12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Epoch Average Loss\")\nx = [i + 1 for i in range(len(epoch_loss_values))]\ny = epoch_loss_values\nplt.xlabel('epoch')\nplt.plot(x, y)\nplt.subplot(1, 2, 2)\nplt.title(\"Validation: Area under the ROC curve\")\nx = [val_interval * (i + 1) for i in range(len(metric_values))]\ny = metric_values\nplt.xlabel('epoch')\nplt.plot(x, y)\nplt.show()","95020244":"model.load_state_dict(torch.load('best_metric_model.pth'))\nmodel.eval()\ny_true = list()\ny_pred = list()\n\nwith torch.no_grad():\n    for test_data in test_loader:\n        test_images, test_labels = test_data[0].to(device), test_data[1].to(device)\n        pred = model(test_images.float()).argmax(dim=1)   ##### .float()\n        for i in range(len(pred)):\n            y_true.append(test_labels[i].item())\n            y_pred.append(pred[i].item())","7001e5a8":"print(classification_report(y_true, y_pred, target_names=class_names, digits=4))","fb9b9052":"## Install MONAI","f2a31e10":"# Gujarati Characters Classify MONAI Pytorch\nThis notebook referred to MONAI's Image Classification Tutorial with the MedNIST Dataset<br\/>\nhttps:\/\/colab.research.google.com\/drive\/1wy8XUSnNWlhDNazFdvGBHLfdkGvOHBKe","1191807d":"## Prepare training, validation and test data lists","bbb7b048":"## Define MONAI transforms, Dataset and Dataloader to pre-process data","e4d149df":"```\nver5 result\n   accuracy                          0.4007     23100\n   macro avg     0.4868    0.4007    0.3870     23100\nweighted avg     0.4868    0.4007    0.3870     23100\n```","72e57167":"## Read image filenames from the dataset folders","380305fe":"https:\/\/docs.monai.io\/en\/0.1.0\/_modules\/monai\/transforms\/transforms.html<br\/>\nhttps:\/\/docs.monai.io\/en\/0.1.0\/transforms.html\n","19e66357":"#### Caution\n- Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same\n- `to_onehot=True\/False` is deprecated, please use `to_onehot=num_classes` instead.","67bece44":"## Visualise some examples","c3e647c9":"```\nver2 result\n    accuracy                         0.4632     23100\n   macro avg     0.5483    0.4632    0.4527     23100\nweighted avg     0.5483    0.4632    0.4527     23100\n\n```","5e6b7d92":"##### Caution\ny values must be 0 or 1, can not be all 0 or all 1.","e0b83704":"## Plot the loss and metric"}}