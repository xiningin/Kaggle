{"cell_type":{"69ebfb10":"code","b4849e83":"code","80e22efc":"code","5d264e9f":"code","eceef32d":"code","686c913f":"code","12d3c147":"code","10cbb507":"code","e0d2d3c9":"code","19029bfd":"code","fc8247f6":"code","82ddc002":"code","795eaad9":"code","f90d16d8":"code","7bb2d852":"code","9274edda":"code","2f528cde":"code","b6d0507d":"code","84393f93":"code","a4542168":"code","fbb76233":"code","f13fe654":"code","4d0b10ef":"code","d9a064f7":"code","b3665e3a":"code","2b0b398f":"code","00d473e2":"code","7ffc8a67":"code","fcb20b33":"code","5c01350d":"code","60796193":"markdown","be3c27c7":"markdown","97e9a5aa":"markdown","4bc9a2ba":"markdown","c6692c8e":"markdown","55a0ec86":"markdown","dbaa8cbb":"markdown","b38a4fa9":"markdown","bb843aff":"markdown","b70331fe":"markdown","fcf18d6c":"markdown","b2b9cc69":"markdown","ed42b07f":"markdown","d3bca72e":"markdown","00a5d650":"markdown","220a0d52":"markdown","7d7952e5":"markdown","1a603411":"markdown","0422cfa8":"markdown","c1f0fd75":"markdown","d1daf17b":"markdown","059e88eb":"markdown","b9f0b3ee":"markdown","78b59398":"markdown","67cb2210":"markdown","e8905392":"markdown","a334b4f5":"markdown","b249de89":"markdown"},"source":{"69ebfb10":"%matplotlib inline\n\n\n# Numerical libraries\nimport numpy as np   \n\n# Import Linear Regression machine learning library\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\n\nfrom sklearn.metrics import r2_score\n\n# to handle data in form of rows and columns \nimport pandas as pd    \n\n# importing ploting libraries\nimport matplotlib.pyplot as plt   \n\n#importing seaborn for statistical plots\nimport seaborn as sns","b4849e83":"mpg_df = pd.read_csv(\"..\/input\/autompg-dataset\/auto-mpg.csv\")","80e22efc":"mpg_df.head() #sample 5 records","5d264e9f":"mpg_df.info() #information about the data","eceef32d":"mpg_df[mpg_df['horsepower'].str.isnumeric()==False]","686c913f":"mpg_df=mpg_df.replace('?',np.nan)","12d3c147":"mpg_df=mpg_df.drop('car name',axis=1)","10cbb507":"mpg_df=mpg_df.apply(lambda x: x.fillna(x.median()),axis=0)","e0d2d3c9":"mpg_df.info()","19029bfd":"#'mpg' is dependent variable so drop it . Copying rest of the columns to X\nX = mpg_df.drop('mpg', axis=1)\n\n#Copying the 'mpg' column alone into the y dataframe. This is the dependent variable\ny = mpg_df[['mpg']]\n","fc8247f6":"a=['cylinders', 'displacement', 'horsepower', 'weight', 'acceleration', 'model year']\nfor i in a:\n    plt.figure()\n    sns.distplot(X[i])","82ddc002":"a=['origin']\nfor i in a:\n    plt.figure()\n    sns.countplot(X[i])","795eaad9":"sns.distplot(y['mpg'])","f90d16d8":"from sklearn import preprocessing\n\n# scale all the columns of the mpg_df. This will produce a numpy array\nX_scaled = preprocessing.scale(X)\nX_scaled = pd.DataFrame(X_scaled, columns=X.columns)  # ideally the training and test should be \n\ny_scaled = preprocessing.scale(y)\ny_scaled = pd.DataFrame(y_scaled, columns=y.columns)  # ideally the training and test should be ","7bb2d852":"\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.30, random_state=1)","9274edda":"regression_model = LinearRegression()\nregression_model.fit(X_train, y_train)\n\nfor idx, col_name in enumerate(X_train.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, regression_model.coef_[0][idx]))","2f528cde":"intercept = regression_model.intercept_[0]\n\nprint(\"The intercept for our model is {}\".format(intercept))","b6d0507d":"ridge = Ridge(alpha=.3) #coefficients are prevented to become too big by this alpha value\nridge.fit(X_train,y_train)\nfor i,col in enumerate(X_train.columns):   \n    print (\"Ridge model coefficients for {} is {}:\".format(col,ridge.coef_[0][i]))","84393f93":"lasso = Lasso(alpha=0.1)\nlasso.fit(X_train,y_train)\nfor i,col in enumerate(X_train):\n    print (\"Lasso model coefficients for {} is {}:\".format(col,lasso.coef_[i]))","a4542168":"print(regression_model.score(X_train, y_train))\nprint(regression_model.score(X_test, y_test))\n","fbb76233":"print(ridge.score(X_train, y_train))\nprint(ridge.score(X_test, y_test))","f13fe654":"print(lasso.score(X_train, y_train))\nprint(lasso.score(X_test, y_test))","4d0b10ef":"from sklearn.preprocessing import PolynomialFeatures","d9a064f7":"poly = PolynomialFeatures(degree = 2, interaction_only=True)","b3665e3a":"X_poly = poly.fit_transform(X_scaled)\nX_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.30, random_state=1)\nX_train.shape","2b0b398f":"regression_model.fit(X_train, y_train)\nprint(regression_model.coef_[0])","00d473e2":"ridge = Ridge(alpha=.3)\nridge.fit(X_train,y_train)\nprint (\"Ridge model:\", (ridge.coef_))","7ffc8a67":"print(ridge.score(X_train, y_train))\nprint(ridge.score(X_test, y_test))\n","fcb20b33":"lasso = Lasso(alpha=0.01)\nlasso.fit(X_train,y_train)\nprint (\"Lasso model:\", (lasso.coef_))\n","5c01350d":"print(lasso.score(X_train, y_train))\nprint(lasso.score(X_test, y_test))\n","60796193":"**Here we can see large values for coefficients. Since dimensions are more with less number of data points this model is clearly indicates the overfit**","be3c27c7":"# Regularized Ridge Model","97e9a5aa":"# Simple linear model","4bc9a2ba":"# Regularized Ridge Model ","c6692c8e":"# We will use polynomial features to understand ridge and lasso better","55a0ec86":"## Comparing the scores","dbaa8cbb":"**Here the performance of lasso slightly gone down but this model used only 5 dimensions while other two used 8 dimensions. This model is feasible compared to other two because  dimensions are reduced**","b38a4fa9":"# Regularized Lasso Model","bb843aff":"**We can see less coefficients values compared to linear regression. Since it is smoother model we will not see much difference in coefficient.**","b70331fe":"**We can see coefficients are reduced. It will reduce close to zero but not exactly equal to zero. The 0 value shown up above is rounded up value**","fcf18d6c":"# Regularized LASSO Model","b2b9cc69":"**Here the coeficient values are relatively smaller. So we can say this is the smoother model.**","ed42b07f":"# Distribution of Independent Variables","d3bca72e":"**Accuracy of linear and ridge are more or less same because both coefficients values are similar**","00a5d650":"**\"displacement\",\"horsepower\",\"weight\" columns are right skewed.It has little peakness towards right**\n\n**\"acceleration\" is normally distributed**\n\n**\"model year\" seems normal but wider at the center**\n\n**\"cylinders\" has only certain values which repeating often**","220a0d52":"**Many of the coefficients have become 0 so we can drop of those dimensions from the model.It has taken only 5 dimensions to build the model.Lasso is also used for feature selection.**","7d7952e5":"**Lasso making many coeficients to zero. As mentioned above these are not used for model building**","1a603411":"# Separate independent and dependent variables","0422cfa8":"**Lasso is getting 86% accuracy with nearly half the number of dimensions.**\n\n**Lasso is also used for feature selection and dimensionality reduction technique**","c1f0fd75":"# Importing Neccessary Packages","d1daf17b":"**Our dimensions are increased to 29**","059e88eb":"**To build machine learning model we need to have numbers for every features. \"horsepower\" and \"car name\" has data type as object.**\n\n**Car name does not add any value so we will drop it.**\n\n**We need to look into \"horsepower\" column because it may contain any junk values**","b9f0b3ee":"**origin 1 is more compared to others**\n\n","78b59398":"# Simple non regularized linear model on poly features-","67cb2210":"**Little Skewness towards right**","e8905392":"# Distribution of dependent Variable","a334b4f5":"**We can see there are six values which has ?We can replace with nan values**","b249de89":"# We have seen Lasso and Ridge regression implementation and also compared the accuracy with simple Linear model. Please do upvote if this notebook is useful for you. Thanks in advance :)"}}