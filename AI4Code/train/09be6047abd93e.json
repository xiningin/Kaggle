{"cell_type":{"d2b311e1":"code","18a0e007":"code","3707326a":"code","51d7bdab":"code","e2a2ad6d":"code","e0ddb8b2":"code","827d8f09":"code","b2eab8fb":"code","22bdecda":"code","2cda9bbf":"code","f672b2a7":"code","25ef8058":"code","6805f738":"code","89b6485d":"code","621fd8dd":"code","3f6a412a":"code","40b7f31d":"code","c58ae013":"code","eb5239f9":"code","40818484":"code","1790ac79":"code","313e7bfc":"code","418ec0ec":"code","0a788f6d":"code","bf8271f0":"code","e4037527":"code","d51068da":"code","dd5df04c":"code","ba2146e4":"code","1a93ffe0":"code","00a7c555":"code","38b1883e":"code","75a8cbdd":"code","ea667f39":"code","0d347da2":"markdown","668a8904":"markdown","07c97cfe":"markdown"},"source":{"d2b311e1":"import numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\n\n#For Preprocessing\nimport re    # RegEx for removing non-letter characters\nimport nltk  # natural language processing\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# For Building the model\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport seaborn as sns\n\n#For data visualization\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\n%matplotlib inline\n\npd.options.plotting.backend = \"plotly\"","18a0e007":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","3707326a":"# Load Tweet dataset\ndf0 = pd.read_csv('..\/input\/us-election-2020-tweets\/hashtag_joebiden.csv')\ndf0","51d7bdab":"df=df0[['tweet','likes','retweet_count']][0:10000]\ndf","e2a2ad6d":"def tweet_to_words(tweet):\n    ''' Convert tweet text into a sequence of words '''\n    \n    # convert to lowercase\n    text = tweet.lower()\n    # remove non letters\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n    # tokenize\n    words = text.split()\n    # remove stopwords\n    words = [w for w in words if w not in stopwords.words(\"english\")]\n    # apply stemming\n    words = [PorterStemmer().stem(w) for w in words]\n    # return list\n    return words","e0ddb8b2":"cleantext=[]\nfor item in tqdm(df['tweet']):\n    words=tweet_to_words(str(item))\n    cleantext+=[words]\ndf['cleantext']=cleantext\ndf","827d8f09":"# Apply data processing to each tweet\n# X = list(map(tweet_to_words, df['content']))\n# print(X==cleantext) #True","b2eab8fb":"def unlist(list):\n    words=''\n    for item in list:\n        words+=item+' '\n    return words","22bdecda":"def compute_vader_scores(df, label):\n    sid = SentimentIntensityAnalyzer()\n    df[\"vader_neg\"] = df[label].apply(lambda x: sid.polarity_scores(unlist(x))[\"neg\"])\n    df[\"vader_neu\"] = df[label].apply(lambda x: sid.polarity_scores(unlist(x))[\"neu\"])\n    df[\"vader_pos\"] = df[label].apply(lambda x: sid.polarity_scores(unlist(x))[\"pos\"])\n    df[\"vader_comp\"] = df[label].apply(lambda x: sid.polarity_scores(unlist(x))[\"compound\"])\n    df['cleantext2'] = df[label].apply(lambda x: unlist(x))\n    return df","2cda9bbf":"df2 = compute_vader_scores(df,'cleantext')\ndf2.info()","f672b2a7":"sns.jointplot(data=df2,x='vader_pos', y='vader_neg', kind=\"kde\")","25ef8058":"sns.jointplot(data=df2,x='vader_pos', y='vader_comp', kind=\"kde\")","6805f738":"df2","89b6485d":"df2=df2.fillna(0)\ndf2.info()","621fd8dd":"df2['likes'].unique()","3f6a412a":"df2['retweet_count'].unique()","40b7f31d":"likes=[]\nfor item in df2['likes']:\n    if type(item)==str:\n        likes+=[int(item.replace('.0',''))]\n    elif type(item)==int:\n        likes+=[item]\ndf2['likes']=likes","c58ae013":"retweet=[]\nfor item in df2['retweet_count']:\n    if type(item)==float:\n        retweet+=[int(item)]\n    elif type(item)==int:\n        retweet+=[item]\ndf2['retweet_count']=retweet","eb5239f9":"fig,ax = plt.subplots(figsize=(6,6))\nax.set_title('Favorites and Retweets',fontsize=20)\nax.set_xlabel('Favorites',fontsize=12)\nax.set_ylabel('Retweets',fontsize=12)\nax.scatter(df2.likes.astype(int),df2.retweet_count.astype(int))","40818484":"class0=[]\nfor i in range(len(df2)):\n    if df2.loc[i,'vader_pos']>0.1:\n        class0+=[2]\n    elif df2.loc[i,'vader_neg']>0.1:\n        class0+=[0]        \n    else:\n        class0+=[1]     ","1790ac79":"df['class']=class0\ndf['class'].value_counts()","313e7bfc":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nmax_words = 5000\nmax_len=50\n\ndef tokenize_pad_sequences(text):\n    '''\n    This function tokenize the input text into sequnences of intergers and then\n    pad each sequence to the same length\n    '''\n    # Text tokenization\n    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n    tokenizer.fit_on_texts(text)\n    # Transforms text to a sequence of integers\n    X = tokenizer.texts_to_sequences(text)\n    # Pad sequences to the same length\n    X = pad_sequences(X, padding='post', maxlen=max_len)\n    # return sequences\n    return X, tokenizer\n\nprint('Before Tokenization & Padding \\n', df['cleantext2'][0])\nX, tokenizer = tokenize_pad_sequences(df['cleantext2'])\nprint('After Tokenization & Padding \\n', X[0])","418ec0ec":"print(X.shape)","0a788f6d":"y = pd.get_dummies(df['class'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)\nprint('Train Set: ', X_train.shape, y_train.shape)\nprint('Validation Set: ', X_val.shape, y_val.shape)\nprint('Test Set: ', X_test.shape, y_test.shape)","bf8271f0":"import tensorflow.keras.backend as K\n\ndef f1_score(precision, recall):\n    ''' Function to calculate f1 score '''\n    \n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","e4037527":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout\nfrom tensorflow.keras.metrics import Precision, Recall\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras import datasets\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import History\nfrom tensorflow.keras import losses","d51068da":"vocab_size = 5000\nembedding_size = 32\nepochs=20\nlearning_rate = 0.1\ndecay_rate = learning_rate \/ epochs\nmomentum = 0.8","dd5df04c":"sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n# Build model\nmodel= Sequential()\nmodel.add(Embedding(vocab_size, embedding_size, input_length=max_len))\nmodel.add(Conv1D(filters=32, kernel_size=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(3, activation='softmax'))","ba2146e4":"import tensorflow as tf\ntf.keras.utils.plot_model(model, show_shapes=True)","1a93ffe0":"model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy', Precision(), Recall()])","00a7c555":"history = model.fit(X_train,y_train,validation_data=(X_val, y_val),batch_size=64,epochs=epochs,verbose=1)","38b1883e":"# Evaluate model on the test set\nloss, accuracy, precision, recall = model.evaluate(X_test, y_test, verbose=0)\n# Print metrics\nprint('')\nprint('Accuracy  : {:.4f}'.format(accuracy))\nprint('Precision : {:.4f}'.format(precision))\nprint('Recall    : {:.4f}'.format(recall))\nprint('F1 Score  : {:.4f}'.format(f1_score(precision, recall)))","75a8cbdd":"def plot_training_hist(history):\n    '''Function to plot history for accuracy and loss'''\n    \n    fig, ax = plt.subplots(1,2, figsize=(10,4))\n    # first plot\n    ax[0].plot(history.history['accuracy'])\n    ax[0].plot(history.history['val_accuracy'])\n    ax[0].set_title('Model Accuracy')\n    ax[0].set_xlabel('epoch')\n    ax[0].set_ylabel('accuracy')\n    ax[0].legend(['train', 'validation'], loc='best')\n    \n    # second plot\n    ax[1].plot(history.history['loss'])\n    ax[1].plot(history.history['val_loss'])\n    ax[1].set_title('Model Loss')\n    ax[1].set_xlabel('epoch')\n    ax[1].set_ylabel('loss')\n    ax[1].legend(['train', 'validation'], loc='best')\n    \nplot_training_hist(history)","ea667f39":"from sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(model, X_test, y_test):\n    '''Function to plot confusion matrix for the passed model and the data'''\n    \n    sentiment_classes =  ['Negative','Neutral', 'Positive']\n    # use model to do the prediction\n    y_pred = model.predict(X_test)\n    # compute confusion matrix\n    cm = confusion_matrix(np.argmax(y_pred, axis=1),np.argmax(np.array(y_test),axis=1))\n    \n    print(pd.Series(np.argmax(np.array(y_test),axis=1)).value_counts())\n    print(pd.Series(np.argmax(y_pred, axis=1)).value_counts())\n    \n    # plot confusion matrix\n    plt.figure(figsize=(8,6))\n    sns.heatmap(cm, cmap=plt.cm.Blues, annot=True, fmt='d', \n                xticklabels=sentiment_classes,\n                yticklabels=sentiment_classes)\n    plt.title('Confusion matrix', fontsize=16)\n    plt.xlabel('Actual label', fontsize=12)\n    plt.ylabel('Predicted label', fontsize=12)\n    \nplot_confusion_matrix(model, X_test, y_test)","0d347da2":"# def tokenize_pad_sequences(text):","668a8904":"# Model","07c97cfe":"# Evaluate"}}