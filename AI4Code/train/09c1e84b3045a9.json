{"cell_type":{"82db9dbf":"code","4c79bedb":"code","45ea8980":"code","ba2b74f4":"code","a4df9017":"code","13c2ed86":"code","ffe1ad12":"code","4e0ca456":"code","6d764725":"code","456267d1":"code","3dd2831d":"code","64ac1dcf":"code","2ac41dd8":"code","0f681afa":"code","6d0976f1":"code","708d1431":"code","728d27ee":"code","38343054":"code","d4ff3c96":"code","a6fa5380":"code","e113fd89":"code","8d8f65aa":"code","79f1481f":"code","5b1e7399":"code","3b68b9a8":"code","45112841":"code","7a3b2eab":"code","1ff8796f":"code","60f3a4d5":"code","bb80d375":"code","fc48e03a":"code","227271d9":"code","76e8eebb":"code","61f5092d":"code","3e704450":"code","17d06d61":"code","a647a062":"code","76bd3a0e":"code","eb8c34e5":"code","aa83d965":"code","598d9262":"code","b8bc94fa":"code","f40c2fc0":"code","3246c091":"code","db189f37":"code","d4dede3f":"code","3c4c145c":"code","9d6efc91":"code","9aee550d":"code","d0291854":"code","0cfddb36":"code","083ff95c":"code","a120ff82":"code","b921959b":"code","f9ec18eb":"code","de5c752b":"code","b04a2b56":"code","985ef6b4":"markdown","1daf428b":"markdown","fabb82e2":"markdown","602d73b4":"markdown","6736c3cb":"markdown","40c42271":"markdown","8da0621a":"markdown","0807fb67":"markdown","d69759b2":"markdown","4fb6e422":"markdown","984aeb77":"markdown","2a9649a7":"markdown","dccfaaaf":"markdown","7b499f84":"markdown","77c58c0a":"markdown","bb989f68":"markdown","6b79c223":"markdown","c39973fa":"markdown","fe5ed7be":"markdown","30adac3e":"markdown","406fec4a":"markdown","96b27e77":"markdown","7fd0a89d":"markdown","5bdc5edc":"markdown","8cb9fbc6":"markdown","dfc363d2":"markdown","3bf9bb7c":"markdown","5f6807ec":"markdown","da75c7b1":"markdown","6833f1ef":"markdown","fe2e137b":"markdown","ca9f3c9e":"markdown","c7e5f0f6":"markdown","5349e0af":"markdown","eb37bb02":"markdown","de6d004e":"markdown","bf288af9":"markdown","870a832d":"markdown","132482a4":"markdown","1dce0a3a":"markdown","ea62907d":"markdown","8ea249cf":"markdown","4eeed316":"markdown","53aec713":"markdown","5bcc2314":"markdown","d8ee317f":"markdown","ccaa88cf":"markdown","2e3bdba4":"markdown","d70bd8e1":"markdown","7d20178a":"markdown"},"source":{"82db9dbf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBRegressor\n\n# Input data files are available in the \"..\/input\/\" directory.\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4c79bedb":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ncombined = [train, test]","45ea8980":"train.head()","ba2b74f4":"train.describe()","a4df9017":"sns.barplot(x=\"Sex\", y=\"Survived\", hue=\"Pclass\", data=train)","13c2ed86":"group_by_age = pd.cut(train[\"Age\"], np.arange(0, 90, 10))\nage_grouping = train.groupby(group_by_age).mean()\nage_grouping['Survived'].plot.bar()","ffe1ad12":"sns.swarmplot(x=\"Pclass\", y=\"Age\", hue=\"Sex\", data=train)\nplt.legend(bbox_to_anchor=(1.1, 1), loc=2, borderaxespad=0.)\nplt.title(\"Age distribution vs Class\", fontsize=15)","4e0ca456":"g = sns.FacetGrid(train, col=\"Pclass\")\ng.map(sns.boxplot, \"Sex\", \"Age\", palette=\"Set3\")","6d764725":"for dataset in combined:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\ntrain.head()","456267d1":"train.isnull().any()","3dd2831d":"test.isnull().any()","64ac1dcf":"train['Age'] = train['Age'].fillna(train['Age'].mean())\ntest['Age'] = test['Age'].fillna(test['Age'].mean())","2ac41dd8":"train.isnull().any()","0f681afa":"test.isnull().any()","6d0976f1":"train['Cabin'].fillna('U', inplace=True)\ntrain['Cabin'] = train['Cabin'].apply(lambda x: x[0])\ntrain['Cabin'].unique()","708d1431":"test['Cabin'].fillna('U', inplace=True)\ntest['Cabin'] = test['Cabin'].apply(lambda x: x[0])\ntest['Cabin'].unique()","728d27ee":"replacement = {\n    'T': 0,\n    'U': 1,\n    'A': 2,\n    'G': 3,\n    'C': 4,\n    'F': 5,\n    'B': 6,\n    'E': 7,\n    'D': 8\n}\n\ntrain['Cabin'] = train['Cabin'].apply(lambda x: replacement.get(x))\ntrain['Cabin'] = StandardScaler().fit_transform(train['Cabin'].values.reshape(-1, 1))\ntrain.head()['Cabin']","38343054":"test['Cabin'] = test['Cabin'].apply(lambda x: replacement.get(x))\ntest['Cabin'] = StandardScaler().fit_transform(test['Cabin'].values.reshape(-1, 1))\ntest.head()['Cabin']","d4ff3c96":"train['Embarked'].fillna('N', inplace=True)\ntrain['Embarked'] = train['Embarked'].apply(lambda x: x[0])\ntrain['Embarked'].unique()","a6fa5380":"replacement = {\n    'S': 0,\n    'C': 1,\n    'Q': 2,\n    'N': 3\n}\n\ntrain['Embarked'] = train['Embarked'].apply(lambda x: replacement.get(x))\ntrain['Embarked'] = StandardScaler().fit_transform(train['Embarked'].values.reshape(-1, 1))\ntrain.head()['Embarked']","e113fd89":"test['Embarked'] = test['Embarked'].apply(lambda x: replacement.get(x))\ntest['Embarked'] = StandardScaler().fit_transform(test['Embarked'].values.reshape(-1, 1))\ntest.head()['Embarked']","8d8f65aa":"train['Fare'] = train['Fare'].fillna(train['Fare'].mean())\ntest['Fare'] = test['Fare'].fillna(test['Fare'].mean())","79f1481f":"def process_family_train():\n    \n    # introducing a new feature : the size of families (including the passenger)\n    train['FamilySize'] = train['Parch'] + train['SibSp'] + 1\n    \n    # introducing other features based on the family size\n    train['Singleton'] = train['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n    train['SmallFamily'] = train['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\n    train['LargeFamily'] = train['FamilySize'].map(lambda s: 1 if 5 <= s else 0)\n    \n    return train","5b1e7399":"train = process_family_train()\ntrain.head()","3b68b9a8":"def process_family_test():\n    \n    # introducing a new feature : the size of families (including the passenger)\n    test['FamilySize'] = test['Parch'] + test['SibSp'] + 1\n    \n    # introducing other features based on the family size\n    test['Singleton'] = test['FamilySize'].map(lambda s: 1 if s == 1 else 0)\n    test['SmallFamily'] = test['FamilySize'].map(lambda s: 1 if 2 <= s <= 4 else 0)\n    test['LargeFamily'] = test['FamilySize'].map(lambda s: 1 if 5 <= s else 0)\n    \n    return test","45112841":"test = process_family_test()\ntest.head()","7a3b2eab":"train['Age*Class']=train['Age']*train['Pclass']\ntest['Age*Class']=train['Age']*train['Pclass']","1ff8796f":"train['FamilySize*Pclass']=train['FamilySize']*train['Pclass']\ntest['FamilySize*Pclass']=train['FamilySize']*train['Pclass']","60f3a4d5":"train['Singleton*Pclass']=train['Singleton']*train['Pclass']\ntest['Singleton*Pclass']=train['Singleton']*train['Pclass']","bb80d375":"train['SmallFamily*Pclass']=train['SmallFamily']*train['Pclass']\ntest['SmallFamily*Pclass']=train['SmallFamily']*train['Pclass']","fc48e03a":"# Grab title from passenger names\n\ntrain[\"Name\"].replace(to_replace='(.*, )|(\\\\..*)', value='', inplace=True, regex=True)\ntrain.head()","227271d9":"# Show title counts by sex\n\ntrain.groupby([\"Sex\", \"Name\"]).size().unstack(fill_value=0)","76e8eebb":"# Titles with very low cell counts to be combined to \"rare\" level\n\nrare_titles = ['Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']\ntrain.replace(rare_titles, \"Rare title\", inplace=True)\n\n# Also reassign mlle, ms, and mme accordingly\n\ntrain.replace([\"Mlle\",\"Ms\", \"Mme\"], [\"Miss\", \"Miss\", \"Mrs\"], inplace=True)","61f5092d":"# Show title counts by sex\n\ntrain.groupby([\"Sex\", \"Name\"]).size().unstack(fill_value=0)","3e704450":"#Now we can create a method to map\/replace the titles\n\nreplacement = {\n    'Master': 0,\n    'Miss': 1,\n    'Mr': 2,\n    'Mrs': 3,\n    'Rare title': 4\n}\n\ntrain['Name'] = train['Name'].apply(lambda x: replacement.get(x))\ntrain['Name'] = StandardScaler().fit_transform(train['Name'].values.reshape(-1, 1))\ntrain.head()['Name']","17d06d61":"train.head()","a647a062":"test[\"Name\"].replace(to_replace='(.*, )|(\\\\..*)', value='', inplace=True, regex=True)\ntest.head()","76bd3a0e":"test.groupby([\"Sex\", \"Name\"]).size().unstack(fill_value=0)","eb8c34e5":"rare_titles = ['Dona', 'Lady', 'the Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer']\ntest.replace(rare_titles, \"Rare title\", inplace=True)\n\ntest.replace([\"Mlle\",\"Ms\", \"Mme\"], [\"Miss\", \"Miss\", \"Mrs\"], inplace=True)","aa83d965":"# Show title counts by sex\n\ntest.groupby([\"Sex\", \"Name\"]).size().unstack(fill_value=0)","598d9262":"#Now we can create a method to map\/replace the titles\n\nreplacement = {\n    'Master': 0,\n    'Miss': 1,\n    'Mr': 2,\n    'Mrs': 3,\n    'Rare title': 4\n}\n\ntest['Name'] = test['Name'].apply(lambda x: replacement.get(x))\ntest['Name'] = StandardScaler().fit_transform(test['Name'].values.reshape(-1, 1))\ntest.head()['Name']","b8bc94fa":"test.head()","f40c2fc0":"train_df = train.drop(['Ticket', 'PassengerId'], axis=1)\ntest_df = test.drop(['Ticket'], axis=1)\ncombined = [train_df, test_df]\ntrain_df.shape, test_df.shape","3246c091":"X_train = train_df.drop(\"Survived\", axis=1)\nY_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","db189f37":"X_train.head()","d4dede3f":"X_test.head()","3c4c145c":"regr = LogisticRegression()\nregr.fit(X_train, Y_train)\nY_pred = regr.predict(X_test)\nacc_log = round(regr.score(X_train, Y_train) * 100, 2)\nacc_log","9d6efc91":"svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","9aee550d":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","d0291854":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","0cfddb36":"perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","083ff95c":"linear_svc = LinearSVC(max_iter=10000)\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","a120ff82":"sgd = SGDClassifier(max_iter=16050)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","b921959b":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","f9ec18eb":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","de5c752b":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","b04a2b56":"subm = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubm.to_csv('subm.csv', index=False)","985ef6b4":"Let's check the data again to see if the substitution is OK.","1daf428b":"Adding new features can help the model (more data is better is a principle that can be generally applied). \n\nWe will add now create two new features: Family Size and Age*Class.","fabb82e2":"**FamilySize*Class**","602d73b4":"Let's now replace the NaN values with the mean of the value of the column.","6736c3cb":"# **Linear SVC**","40c42271":"An alternative to the swarmplot could be using a box plot, as shown below.\n\nThanks to these two plots, we discovered that 1st class passengers seems older: probabily, according to the age, they can afford a expensive ticket.","8da0621a":"At this point, we will shape the dataframes dropping a few columns","0807fb67":"# **Evaluation of the models**","d69759b2":"**Age * Class**\n\nThis is an interaction term, since age and class are both numbers we can just multiply them.","4fb6e422":"The code below will import the data and create Pandas Dataframes to manage them.\n\nThe combined variable create a dataframe that is the union of the train and test dataframes.","984aeb77":"**SmallFamily*Class**","2a9649a7":" As a starting point, we will applying all the following models and then compare the results.\n\n- Logistic Regression\n- KNN or k-Nearest Neighbors\n- Support Vector Machines\n- Naive Bayes classifier\n- Decision Tree\n- Random Forest\n- Perceptron\n- Artificial neural network\n- RVM or Relevance Vector Machine","dccfaaaf":"# **Interaction terms**","7b499f84":"Let's apply the same logic to the test dataframe","77c58c0a":"# **Preparing the Data**","bb989f68":"# **Random Forest**","6b79c223":"# **Submission**","c39973fa":"**Family Size**","fe5ed7be":"# **Perceptron**","30adac3e":"**Categorical features analysis**","406fec4a":"Categorical features needs to be converted in order to apply our models.\n\nThis needs to be done because Machine Learning algorithms cannot elaborate strings.\n\n'Sex' is the first feature we will convert.","96b27e77":"# **Support Vector Machines**","7fd0a89d":"# **Using a facet grid to create a box plot of Age Distribution**","5bdc5edc":"# **Stochastic Gradient Descent**","8cb9fbc6":"# **Logistic Regression**","dfc363d2":"We can apply the same logic developed for the Cabin column to the Embarked column\nThe possible values are:\nC = Cherbourg, Q = Queenstown, S = Southampton\nWe will check first if we find any NaN value (that we will replace with N) and then we will transform this feature in a list of numbers","3bf9bb7c":"# **Data importing**","5f6807ec":"# **First review of the data**","da75c7b1":"# **New Features**","6833f1ef":"NaN values (not a number) needs to be replaced\/dropped or the Machine Learning algorithms will not work.\nIn the two lines below, we will check which columns has NaN values (if True, there is at least a NaN record in the column)","fe2e137b":"What comes out is that the higher the class of the passenger, the more its possibilities to survive.\n\nAlso, women survival rate is greater than men' ones.\n\nThis aspect is strictly correlated to the maritime tradition of evacuating women and children first.\n\nIn fact, if we group the data above for age, what we see is that children have a higher survival rate.","ca9f3c9e":"# Other Features","c7e5f0f6":"We will now work on the 'Cabin' column.\n\nLet's starting filling NaN values","5349e0af":"# **Decision Tree**","eb37bb02":"We will not use the 'Name' column, but we can at least extract the Title from the name. \n\nThere are quite a few titles going around, but I want to reduce them all to Mrs, Miss, Mr and Master.  ","de6d004e":"# **k-Nearest Neighbors**","bf288af9":"# **Machine Learning Algorithms**","870a832d":"We will now use a swarmplot to see the relations between Age, Class and Sex.","132482a4":"As the first step all the necessary libraries will be imported; this list will be updated as we are going forward.","1dce0a3a":"We will now work with the Fare column (filling the NaN values with the mean of the other columns.","ea62907d":"# **Machine Learning**","8ea249cf":"# **Titanic Machine Learning **\n\nThis is my first machine learning competition and I am trying to structure this notebook in order to:\n\n1) Have a starting point for each future competition (libraries, techniques, code..);\n\n2) Help people like me (self-learners without a technical background) understanding how a machine learning competition works, how to make data analysis and predictions using ML techniques.\n\nThis notebook is a work in progress: I have put it together as a starting point to work on in my free time.\n\nHave fun!","4eeed316":"# **Module Importing**","53aec713":"From the 'describe' function above we can see (row 'mean', column 'Survived') that the mean of the passenger survived is 0.383838.\n\nWe can now analyze the survival rate using as a criteria the Passenger Class and the Sex.","5bcc2314":"**Singleton*Class**","d8ee317f":"**Title**","ccaa88cf":"# **Plotting Age Distribution and its relation with the Passengers Class**","2e3bdba4":"# **Gaussian Naive Bayes**","d70bd8e1":"# **How many passengers survives?**","7d20178a":"**Research of NaN values**"}}