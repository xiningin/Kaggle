{"cell_type":{"8f6c816f":"code","4f6149aa":"code","a8e04013":"code","3fa7953d":"code","863f5df0":"code","f6603974":"code","0708ecd8":"code","18743f61":"code","1314db97":"code","b77b4ac3":"code","de1b28c4":"markdown","be2889cc":"markdown","44e2adcc":"markdown","bdf75a3c":"markdown","25474292":"markdown","ff43521e":"markdown","95f23352":"markdown","0e14e432":"markdown","3b2ecdaf":"markdown","f908eba5":"markdown"},"source":{"8f6c816f":"MAX_LEN = 128\nBATCH_SIZE = 16 # per TPU core\nTOTAL_STEPS = 2000  # thats approx 4 epochs\nEVALUATE_EVERY = 200\nLR =  1e-5\n\nPRETRAINED_MODEL = 'jplu\/tf-xlm-roberta-large'\nD = '\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/'\n\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nprint(tf.__version__)\nfrom tensorflow.keras.optimizers import Adam\nimport transformers\nfrom transformers import TFAutoModelWithLMHead, AutoTokenizer\nimport logging\n# no extensive logging \nlogging.getLogger().setLevel(logging.NOTSET)\n\nAUTO = tf.data.experimental.AUTOTUNE","4f6149aa":"def connect_to_TPU():\n    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n\n    return tpu, strategy, global_batch_size\n\n\ntpu, strategy, global_batch_size = connect_to_TPU()\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","a8e04013":"val_df = pd.read_csv(D+'validation.csv')\ntest_df = pd.read_csv(D+'test.csv')","3fa7953d":"%%time\n\ndef regular_encode(texts, tokenizer, maxlen=512):\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])\n    \n\ntokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\nX_val = regular_encode(val_df.comment_text.values, tokenizer, maxlen=MAX_LEN)\nX_test = regular_encode(test_df.content.values, tokenizer, maxlen=MAX_LEN)","863f5df0":"def prepare_mlm_input_and_labels(X):\n    # 15% BERT masking\n    inp_mask = np.random.rand(*X.shape)<0.15 \n    # do not mask special tokens\n    inp_mask[X<=2] = False\n    # set targets to -1 by default, it means ignore\n    labels =  -1 * np.ones(X.shape, dtype=int)\n    # set labels for masked tokens\n    labels[inp_mask] = X[inp_mask]\n    \n    # prepare input\n    X_mlm = np.copy(X)\n    # set input to [MASK] which is the last token for the 90% of tokens\n    # this means leaving 10% unchanged\n    inp_mask_2mask = inp_mask  & (np.random.rand(*X.shape)<0.90)\n    X_mlm[inp_mask_2mask] = 250001  # mask token is the last in the dict\n\n    # set 10% to a random token\n    inp_mask_2random = inp_mask_2mask  & (np.random.rand(*X.shape) < 1\/9)\n    X_mlm[inp_mask_2random] = np.random.randint(3, 250001, inp_mask_2random.sum())\n    \n    return X_mlm, labels\n\n\n# use validation and test data for mlm\nX_train_mlm = np.vstack([X_test, X_val])\n# masks and labels\nX_train_mlm, y_train_mlm = prepare_mlm_input_and_labels(X_train_mlm)","f6603974":"def create_dist_dataset(X, y=None, training=False):\n    dataset = tf.data.Dataset.from_tensor_slices(X)\n\n    ### Add y if present ###\n    if y is not None:\n        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n        \n    ### Repeat if training ###\n    if training:\n        dataset = dataset.shuffle(len(X)).repeat()\n\n    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n\n    ### make it distributed  ###\n    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n\n    return dist_dataset\n    \n    \ntrain_dist_dataset = create_dist_dataset(X_train_mlm, y_train_mlm, True)","0708ecd8":"%%time\n\ndef create_mlm_model_and_optimizer():\n    with strategy.scope():\n        model = TFAutoModelWithLMHead.from_pretrained(PRETRAINED_MODEL)\n        optimizer = tf.keras.optimizers.Adam(learning_rate=LR)\n    return model, optimizer\n\n\nmlm_model, optimizer = create_mlm_model_and_optimizer()\nmlm_model.summary()","18743f61":"def define_mlm_loss_and_metrics():\n    with strategy.scope():\n        mlm_loss_object = masked_sparse_categorical_crossentropy\n\n        def compute_mlm_loss(labels, predictions):\n            per_example_loss = mlm_loss_object(labels, predictions)\n            loss = tf.nn.compute_average_loss(\n                per_example_loss, global_batch_size = global_batch_size)\n            return loss\n\n        train_mlm_loss_metric = tf.keras.metrics.Mean()\n        \n    return compute_mlm_loss, train_mlm_loss_metric\n\n\ndef masked_sparse_categorical_crossentropy(y_true, y_pred):\n    y_true_masked = tf.boolean_mask(y_true, tf.not_equal(y_true, -1))\n    y_pred_masked = tf.boolean_mask(y_pred, tf.not_equal(y_true, -1))\n    loss = tf.keras.losses.sparse_categorical_crossentropy(y_true_masked,\n                                                          y_pred_masked,\n                                                          from_logits=True)\n    return loss\n\n            \n            \ndef train_mlm(train_dist_dataset, total_steps=2000, evaluate_every=200):\n    step = 0\n    ### Training lopp ###\n    for tensor in train_dist_dataset:\n        distributed_mlm_train_step(tensor) \n        step+=1\n\n        if (step % evaluate_every == 0):   \n            ### Print train metrics ###  \n            train_metric = train_mlm_loss_metric.result().numpy()\n            print(\"Step %d, train loss: %.2f\" % (step, train_metric))     \n\n            ### Reset  metrics ###\n            train_mlm_loss_metric.reset_states()\n            \n        if step  == total_steps:\n            break\n\n\n@tf.function\ndef distributed_mlm_train_step(data):\n    strategy.experimental_run_v2(mlm_train_step, args=(data,))\n\n\n@tf.function\ndef mlm_train_step(inputs):\n    features, labels = inputs\n\n    with tf.GradientTape() as tape:\n        predictions = mlm_model(features, training=True)[0]\n        loss = compute_mlm_loss(labels, predictions)\n\n    gradients = tape.gradient(loss, mlm_model.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, mlm_model.trainable_variables))\n\n    train_mlm_loss_metric.update_state(loss)\n    \n\ncompute_mlm_loss, train_mlm_loss_metric = define_mlm_loss_and_metrics()","1314db97":"%%time\ntrain_mlm(train_dist_dataset, TOTAL_STEPS, EVALUATE_EVERY)","b77b4ac3":"mlm_model.save_pretrained('.\/')","de1b28c4":"### Mask inputs and create appropriate labels for masked language modelling\n\nFollowing the BERT recipee: 15% used for prediction. 80% of that is masked. 10% is random token, 10% is just left as is.","be2889cc":"## Tokenize  it with the models own tokenizer\n","44e2adcc":"## Finally train it\n\n\n- Note it takes some time\n","bdf75a3c":"## Build model from pretrained transformer\n\nWe just use the pretrained model loaded as a model with a language modelling head.\n\n\n\n- Note: Downloading the model takes some time!\n- Note reported number of total params, and the numbers printed in the column Param # do not match, as the weights of the mbedding matrix are shared between the encoder and the decoder.","25474292":"## Create distributed tensorflow dataset\n","ff43521e":"## About this notebook\n\n\nDomain specific masked language model (MLM) fine tuning was reported to improve the performance of BERT models [(arxiv\/1905.05583)](https:\/\/arxiv.org\/abs\/1905.05583)\n\nMLM fine tuning is also regularly mentioned as one of the things winning teams have tried in previous kaggle NLP competitions (sometimes it is reported to help, sometimes not).\n\n\nHuggingface provides a pytorch script to fine tune models, however there is no TensorFlow solution which just works out of the box.\n\n\nHere I share a notebook with my implementation of MLM fine tuning on TPUs with a custom training loop in TensorFlow 2.\n\nIn my experience, starting from an MLM finetuned model makes training faster and more stable, and possibly more accurate.\n\nSuggestions\/improvements are appreciated!\n\n---\n\n### References:\n\n- This notebook relies on the great [notebook]((https:\/\/www.kaggle.com\/xhlulu\/\/jigsaw-tpu-xlm-roberta) by, Xhulu: [@xhulu](https:\/\/www.kaggle.com\/xhulu\/) \n- The tensorflow distrubuted training tutorial: [Link](https:\/\/www.tensorflow.org\/tutorials\/distribute\/custom_training)","95f23352":"## Save finetuned model\n\nThis fine tuned model can be loaded just as the original to build a classification model from it.","0e14e432":"## Connect to TPU","3b2ecdaf":" ## Load text data into memory\n \n - Now we will only use the multilingual test and validation data.","f908eba5":"### Define stuff for the masked language modelling, and the custom training loop\n\nWe will need:\n- 1, MLM loss, and a loss metric. These need to be defined in the scope of the distributed strategy. The MLM loss is only evauluated at the 15% of the tokens, so we need to make a custom masked sparse categorical crossentropy. Labels with -1 value are ignored during loss calculation.\n- 2, A full training loop\n- 3, A distributed train step called in the training loop, which uses a single replica train step\n"}}