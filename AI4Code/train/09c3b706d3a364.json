{"cell_type":{"930d8773":"code","59415dfd":"code","19d8c53c":"code","935baf9a":"code","a247d4bc":"code","9315f4dd":"code","487fd552":"code","e6670842":"code","27d82d35":"code","a69a239c":"code","bad06bc8":"code","6a3883ee":"code","288a8a90":"code","d3163017":"code","7583a5b2":"code","eeafc02a":"code","81e2e01a":"code","1b77e272":"code","8a16a5bb":"code","20597c79":"markdown","bc67e7ef":"markdown","a972201a":"markdown","f2519869":"markdown"},"source":{"930d8773":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","59415dfd":"!pip install -U sentence-transformers\n","19d8c53c":"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')","935baf9a":"episode_name_list = os.listdir('\/kaggle\/input\/chai-time-data-science\/Cleaned Subtitles')","a247d4bc":"def preprocess(df):\n    df = df[df['Speaker']!='Sanyam Bhutani']['Text']\n    return df","9315f4dd":"results  = {}\nfor episode in episode_name_list:\n    df = pd.read_csv(\"\/kaggle\/input\/chai-time-data-science\/Cleaned Subtitles\/\"+episode)\n    text = preprocess(df)\n    sentence_embeddings = model.encode(text)\n    results[episode.replace('.csv','')] = sentence_embeddings","487fd552":"episode_embeddings = {k:np.mean(v,axis=0) for k,v in results.items()}","e6670842":"for k, v in episode_embeddings.items():\n    if v.shape == ():\n        print(k)","27d82d35":"del episode_embeddings['E69']","a69a239c":"episode_embeddings_list = list(episode_embeddings.values())\nepisode_ids = list(episode_embeddings.keys())\n","bad06bc8":"num_clusters = 5\nclustering_model = KMeans(n_clusters=num_clusters)\nclustering_model.fit(episode_embeddings_list)\ncluster_assignment = clustering_model.labels_","6a3883ee":"cluster_assignment","288a8a90":"clustered_episodes = [[] for i in range(num_clusters)]\nfor episode_id, cluster_id in enumerate(cluster_assignment):\n    clustered_episodes[cluster_id].append(episode_ids[episode_id])\n","d3163017":"clustered_episodes","7583a5b2":"episode_names = pd.read_csv(\"\/kaggle\/input\/chai-time-data-science\/Episodes.csv\")","eeafc02a":"episode_descriptions = pd.read_csv(\"\/kaggle\/input\/chai-time-data-science\/Description.csv\")","81e2e01a":"episode_names.head()","1b77e272":"episode_mapping = pd.Series(episode_names['episode_name'].values,index=episode_names['episode_id']).to_dict()","8a16a5bb":"clustered_names = []\nfor index, cluster in enumerate(clustered_episodes):\n    print(\"\\n\")\n    print(\"Cluster \",index+1)\n    print(\"\\n\")\n    cluster_list = []\n    for episode in cluster:\n        print(episode_mapping[episode])\n        cluster_list.append(episode_mapping[episode])\n\n        clustered_names.append(cluster_list)","20597c79":"Overview : \n\n* Used [sentence transformers](https:\/\/github.com\/UKPLab\/sentence-transformers) to get the sentence embeddings. BERT\/RoBERTa\/XLM RoBERTa produces out of the box sentence embeddings which are then finetuned with a siamese or triplet network structure to produce semantically meaning sentence embeddings by sentence transformers to use in semantic search or finding similarity.\n* So for each episode sentence embeddings are encoded from the pretrained NLI models. From the average of sentence embeddings we find the document embedding or episode embedding in this case. Then K-means clustering is used for clustering the episodes into groups to find similar episodes.\n","bc67e7ef":"Upon observation in cluster 5 we can see multiple mentions of Japan. Machine Learning Tokyo community related interview (Suzana Ilic) and Kuzushiji recognition competitions related interviews(Tarin and Anokas), all Japanese language\/culture related interviews fell into cluster 4. We can also see five mentions of fastai(Sylvain Gugger, Even Oldridge, Robert Bracco, Hamel Husain, Jason Antic). I'd say in this cluster all these five people must have mentioned the impact of fastai in accelerating their learning. The self studying and learning to learn sort of episodes fall into the cluster 5. However in Cluster 0 too I saw self study\/getting started in data science\/learning to learn sort of advocates(Jeremy Howard, Goku Mohandas, Parul Pandey, Daniel Bourke, Edouard Harris). \n\nCluster 2 has multiple mentions of deep learning researchers. Pierre Stock and Christine Payne talks about their research on model quantization and MuseNet, Rachel Thomas is active in applied AI ethics research. Tim Dettmers interview also mentions research. Second theme in cluster 2 seems to be kaggle grandmasters(Dr Olivier Grellier, Artgor,Andres Torrubia).\n\nIn Cluster 4 there are multiple mentions of Google Quest Q&A labelling competition(Dmitry Danevskiy, Christof Henkel) and mentions of computer vision but other than that I can't discern much theme by eyeballing. Since different variety of people are using fastai, educators, researchers, kagglers, that theme is kind of present in all clusters.","a972201a":"# Match the Episode Names to the Episode Ids","f2519869":"# Finding Episode Clusters from Mean Sentence Embeddings"}}