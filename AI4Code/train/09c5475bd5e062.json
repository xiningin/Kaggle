{"cell_type":{"254e1b36":"code","4d53017b":"code","ca42d3b9":"code","3e1a48ed":"code","fc80be57":"code","3c7c8766":"code","eb8726ce":"code","39370c65":"code","e505f922":"code","df416e12":"code","52d876a6":"code","a86ee493":"code","e283c927":"code","78ba508a":"code","0128d8aa":"code","1ee10381":"code","3e6dfb16":"code","eae13ceb":"code","859506dc":"markdown","ec4b58b6":"markdown","fe8180bb":"markdown","35db87df":"markdown","a9927ac2":"markdown","154ca537":"markdown","3e266710":"markdown","88d080ac":"markdown","f2b0d413":"markdown","6442ddaf":"markdown","ea25bcef":"markdown"},"source":{"254e1b36":"import numpy as np\nimport PIL\nimport os\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nimport cv2\nimport random\n\nMAIN_PATH = \"..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\"\n","4d53017b":"image_paths = glob(MAIN_PATH+\"\/*\")\n","ca42d3b9":"len(image_paths)","3e1a48ed":"def readImage(path,image_size=(256,256)):\n    img = np.asarray(PIL.Image.open(path).resize(image_size))\n    # img - 127.5 \/ 127.5 ==> compress between [-1,1]\n    img = ((img - 127.5) \/ 127.5).astype(\"float32\")\n    return img","fc80be57":"test_img = readImage(\"..\/input\/celeba-dataset\/img_align_celeba\/img_align_celeba\/000007.jpg\")\nprint(test_img.shape)\n\nplt.imshow(test_img)\nplt.show()","3c7c8766":"BATCH_SIZE = 128\nSTEPS_PER_EPOCH = len(image_paths) \/\/ BATCH_SIZE\nprint(\"Steps per epochh are\",STEPS_PER_EPOCH)\ndef dataGenerator(batch_size):\n    while True:\n        paths = random.choices(image_paths,k=batch_size)\n        batch = []\n        for p in paths:\n            batch.append(readImage(p))\n        \n        yield np.asarray(batch)\n\ndataGen = dataGenerator(BATCH_SIZE)\nprint(next(dataGen).shape)\n","eb8726ce":"WEIGHT_INIT = tf.keras.initializers.RandomNormal(mean=0.0,stddev=0.2)\ndef make_generator():\n    model = tf.keras.Sequential()\n    \n    # Random noise to 16x16x256 image\n    model.add(layers.Dense(16*16*256,use_bias=False,input_shape=(100,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    model.add(layers.Reshape((16,16,256)))\n    \n    assert model.output_shape == (None,16,16,256)\n    \n    model.add(layers.Conv2DTranspose(128,(5,5),strides=(2,2),use_bias=False,padding=\"same\",kernel_initializer=WEIGHT_INIT))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    assert model.output_shape == (None,32,32,128)\n    \n    model.add(layers.Conv2DTranspose(128,(5,5),strides=(2,2),use_bias=False,padding=\"same\",kernel_initializer=WEIGHT_INIT))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    assert model.output_shape == (None,64,64,128)\n    \n    model.add(layers.Conv2DTranspose(64,(5,5),strides=(2,2),use_bias=False,padding=\"same\",kernel_initializer=WEIGHT_INIT))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n    \n    assert model.output_shape == (None,128,128,64)\n    \n    model.add(layers.Conv2DTranspose(3,(5,5),strides=(2,2),use_bias=False,padding=\"same\",kernel_initializer=WEIGHT_INIT,\n                                     activation=\"tanh\"\n                                    ))\n              # Tanh activation function compress values between -1 and 1. \n              # This is why we compressed our images between -1 and 1 in readImage function.\n    assert model.output_shape == (None,256,256,3)\n    return model","39370c65":"generator = make_generator()","e505f922":"generator.summary()","df416e12":"# we'll use cross entropy loss\ncross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n\ndef generator_loss(fake_output):\n    # First argument of loss is real labels\n    # We've labeled our images as 1 (real) because\n    # we're trying to fool discriminator\n    return cross_entropy(tf.ones_like(fake_output),fake_output)\n\ngen_optimizer = tf.keras.optimizers.Adam(lr=1e-4)\n","52d876a6":"def make_discriminator():\n    model = tf.keras.Sequential()\n    \n    model.add(layers.Conv2D(64,(5,5),strides=(2,2),padding=\"same\",input_shape=(256,256,3)))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(128,(5,5),strides=(2,2),padding=\"same\"))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Conv2D(265,(5,5),strides=(2,2),padding=\"same\"))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n    \n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n    \n    return model","a86ee493":"discriminator = make_discriminator()\n","e283c927":"def discriminator_loss(real_images,fake_images):\n    real_loss = cross_entropy(tf.ones_like(real_images),real_images)\n    fake_loss = cross_entropy(tf.zeros_like(fake_images),fake_images)\n    total_loss = real_loss + fake_loss\n    return total_loss\n","78ba508a":"discriminator_optimizer = tf.keras.optimizers.Adam(lr=1e-4)","0128d8aa":"EPOCHS = 1\nNOISE_DIM = 100\n\n@tf.function\ndef train_step(images):\n    # We've created random seeds\n    noise = tf.random.normal([BATCH_SIZE,NOISE_DIM])\n    \n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        # Generator generated images\n        generated_images = generator(noise,training=True)\n        \n        # We've sent our real and fake images to the discriminator\n        # and taken the decisions of it.\n        real_output = discriminator(images,training=True)\n        fake_output = discriminator(generated_images,training=True)\n        \n        # We've computed losses of generator and discriminator\n        gen_loss = generator_loss(fake_output)\n        disc_loss = discriminator_loss(real_output,fake_output)\n    \n    # We've computed gradients of networks and updated variables using those gradients.\n    gradients_of_generator = gen_tape.gradient(gen_loss,generator.trainable_variables)\n    gradients_of_discriminator = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n    \n    gen_optimizer.apply_gradients(zip(gradients_of_generator,generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator,discriminator.trainable_variables))\n        ","1ee10381":"import time\nimport sys\ndef train(epochs):\n    for epoch in range(epochs):\n        start = time.time()\n        for step in range(STEPS_PER_EPOCH):\n            train_step(next(dataGen))\n    \n            sys.stdout.write(f\"\\rSTEP: {step}\/{STEPS_PER_EPOCH}\")\n            sys.stdout.flush()\n            \n        finish_time = round(time.time() - start,2)\n        print(f\"Epoch {epoch}\/{epochs} Process Time : {finish_time}\")\n        print(\"-\"*15)\n        ","3e6dfb16":"train(EPOCHS)","eae13ceb":"noise = tf.random.normal([16,100])\ngenerated_images = np.asarray(generator(noise,training=False))\n\nfig = plt.figure(figsize=(6,6))\nfor i in range(16):\n    plt.subplot(4,4,i+1)\n    plt.imshow((generated_images[i,:,:,:]*127.5+127.5).astype(\"int\"))\n    plt.axis(\"off\")\n    \nplt.show()","859506dc":"* Now we'll build our discriminator network.\n* Discriminator network is a CNN based image classifier that classify images real or fake.","ec4b58b6":"* I said we'll read images as batches so now we'll define a generator that yield one batch data.","fe8180bb":"* Let's generate and display some synthetic images.","35db87df":"* We have 202K images in CelebA dataset, so we won't read all images, we'll read them as batches.\n\nLet's define a function. This function will take path argument read image from that path.","a9927ac2":"**Thanks for your attention. If you liked this kernel and upvote I'd be glad**","154ca537":"* Now we'll train model. ","3e266710":"* First, we are going to create a list that contains paths of images. We'll get random paths from that list.","88d080ac":"* Let's test our function.","f2b0d413":"* Our data is ready, let's start to build our GANs\n\nFirst, we'll create our Generator. Generator will upsample our seed (I explained it in introduction) using convolutional transpose layers (upsampling layers)","6442ddaf":"# Introduction\nWelcome community, in this kernel I am going to create a Generative Adverserial Network that generate human faces from seeds (random uniform matrices)\n\nLet's start.","ea25bcef":"* I trained the network for one epoch so model can just learned basics of human face, but if you train it 20-25 epochs you will see detailed faces. \n\n* And If I can find time I will train more too :)"}}