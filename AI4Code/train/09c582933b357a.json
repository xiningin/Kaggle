{"cell_type":{"07c15419":"code","fd677b3e":"code","d11e2d7a":"code","e5a0dc49":"code","bac034b8":"code","3fc0bac2":"code","b1de657e":"code","5fbed4e2":"code","4a1e8f08":"code","877027f6":"code","d9b9db11":"code","7d23f999":"code","366b8341":"code","84f3e09f":"code","c75724e4":"code","b8a0c435":"code","c8386d23":"code","9be5656e":"code","8ccd3d7b":"code","da3b1dad":"code","e128d141":"code","ddad05e4":"code","02b3934b":"code","345d4b19":"code","17a0be81":"code","502d5860":"code","ea179ca8":"code","63ab818f":"code","7ad686fb":"code","60d8769a":"code","1bd88fe2":"code","d7eef033":"code","956b8120":"code","4a7dce60":"code","fa2e52ad":"code","6299eea5":"markdown","dd6e9b6c":"markdown","c8c4c06d":"markdown","1465f544":"markdown","050c76e6":"markdown","4ab35782":"markdown","bdc31701":"markdown"},"source":{"07c15419":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.utils as ku\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import style","fd677b3e":"data = pd.read_csv('..\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')","d11e2d7a":"data.head()","e5a0dc49":"'''Analysing the distribution of tweets per class'''\nstyle.use('ggplot')\nplt.bar(['0', '1'], height=[data['label'].value_counts()[0], data['label'].value_counts()[1]], color='b')\nplt.show()","bac034b8":"'''Analysing the avarage length of tweets per class'''\ntemp_data = data[data['label'] == 0]['tweet']\nlen_avg0 = sum([len(tweet) for tweet in temp_data]) \/ len(temp_data)\n\ntemp_data = data[data['label'] == 1]['tweet']\nlen_avg1 = sum([len(tweet) for tweet in temp_data]) \/ len(temp_data)\n\nplt.bar(['0', '1'], [len_avg0, len_avg1])\n","3fc0bac2":"'''Forming the corpus for tokenization'''\ncorpus = []\nfor tweet in data['tweet']:\n    corpus.append(tweet)","b1de657e":"'''Creating unique key-value pair with tokenizer'''\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\ntotal_words = len(tokenizer.word_index) + 1","5fbed4e2":"'''Preparing data for tarining by padding the sequences uniformly'''\nmax_seq_len = max([len(i) for i in corpus])\ninput_sequences = []\nfor tweet in corpus:\n    token_list = tokenizer.texts_to_sequences([tweet])[0] # converting words to corresponding values from 'word_index'\n    input_sequences.append(token_list)\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre')) # padding","4a1e8f08":"print(len(input_sequences), len(data['tweet']))","877027f6":"labels = np.array(data['label'])\nlen(labels)","d9b9db11":"'''Splitting the data in training and validation set'''\nsplit = int(.2 * len(corpus)) # 20% for validation\nrand_row_num = np.random.randint(0, len(corpus), split)\n\nX_test = np.array([input_sequences[i] for i in rand_row_num])\ny_test = np.array([labels[i] for i in rand_row_num])\n\nX_train = np.delete(input_sequences, rand_row_num, axis=0)\ny_train = np.delete(labels, rand_row_num, axis=0)","7d23f999":"len(X_train)","366b8341":"'''Converting Sparsed labels to categorical'''\ny_train = ku.to_categorical(y_train, num_classes=2)\ny_test = ku.to_categorical(y_test, num_classes=2)","84f3e09f":"model = tf.keras.Sequential([\n    layers.Embedding(total_words, 5, input_length=max_seq_len),\n    layers.Bidirectional(layers.LSTM(8, return_sequences=True)),\n    layers.Dropout(0.2),\n    layers.LSTM(8),\n    layers.Dense(8, activation='relu'),\n    layers.Dense(2, activation='softmax')\n])","c75724e4":"model.summary()","b8a0c435":"'''Creating a callback for each epoch thereby knowing optimal learning rate'''\nlr_scheduler = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-6 * 10 ** epoch)","c8386d23":"model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.CategoricalCrossentropy(),\n             metrics=['accuracy'])","9be5656e":"history = model.fit(X_train, y_train, epochs=5, batch_size=64, callbacks=[lr_scheduler])","8ccd3d7b":"'''Visualizing Learning rate vs loss function plot to decide the value of learning rate'''\nloss = history.history['loss']\nlr = history.history['lr']\nstyle.use('ggplot')\nplt.semilogx(1e-6, 1, 0, 0.5)\nplt.title('lr vs training loss')\nplt.xlabel('lr')\nplt.ylabel('training_loss')\nplt.plot(lr, loss, 'r')","da3b1dad":"model = tf.keras.Sequential([\n    layers.Embedding(total_words, 5, input_length=max_seq_len),\n    layers.Bidirectional(layers.LSTM(8, return_sequences=True)),\n    layers.Dropout(0.2),\n    layers.LSTM(8),\n    layers.Dense(8, activation='relu'),\n    layers.Dense(2, activation='softmax')\n])","e128d141":"model.compile(optimizer=tf.keras.optimizers.Adam(1e-2), loss=tf.keras.losses.CategoricalCrossentropy(),\n             metrics=['accuracy'])","ddad05e4":"history = model.fit(X_train, y_train, batch_size=64, epochs=5)","02b3934b":"'''Visualizing training accuracy and loss per epoch to eliminate the potential threat of overfitting'''\nplt.figure(figsize=(10, 6))\nplt.plot(np.arange(len(history.history['accuracy'])), history.history['accuracy'], 'b', label='training acc')\nplt.plot(np.arange(len(history.history['accuracy'])), history.history['loss'], 'r', label='training loss')\nplt.legend(loc='best')\nplt.show()","345d4b19":"model.evaluate(X_test, y_test, batch_size=64)","17a0be81":"y_test = np.array([labels[i] for i in rand_row_num])","502d5860":"encoder_dict = {val: key for key, val in tokenizer.word_index.items()}","ea179ca8":"preds = model.predict(X_test)\npre = []\nfor pred in preds:\n    pre.append(np.argmax(pred))\npre = np.array(pre)\nnp.unique(pre, return_counts=True)","63ab818f":"'''Analysing false negative predictions'''\npreds = model.predict(X_test)\nfor pred in range(len(preds)):\n    temp = np.argmax(preds[pred])\n    if temp != y_test[pred]:\n        tweet = ''\n        for word in X_test[pred]:\n            if word == 0:\n                continue\n            else:\n                tweet += encoder_dict[word]\n                tweet += ' '\n        print(tweet)\n        print(f'predicted: {preds[pred]}\\nactual: {y_test[pred]}')\n        print('--------------------------------------------------')","7ad686fb":"test_data = pd.read_csv('..\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv')","60d8769a":"test_data.head()","1bd88fe2":"test_tweets = []\nfor tweet in test_data['tweet']:\n    token_list = tokenizer.texts_to_sequences([tweet])[0]\n    test_tweets.append(token_list)\ntest_tweets = np.array(pad_sequences(test_tweets, maxlen=max_seq_len, padding='pre'))","d7eef033":"pred = model.predict(test_tweets)","956b8120":"pre = []\nfor pred in preds:\n    pre.append(np.argmax(pred))\npre = np.array(pre)","4a7dce60":"final_preds = pd.DataFrame(data['tweet'], index=pre)","fa2e52ad":"final_preds.head()","6299eea5":"# Importing Dependencies","dd6e9b6c":"# Evaluating Model's performance with the validation set","c8c4c06d":"# Final predictions on Test data","1465f544":"## Creating the encoder dict to understand the pattern in the wrong predictions","050c76e6":"# Reading in Training data","4ab35782":"# Data Analysis and pre-processing","bdc31701":"# Model Building & Architecture"}}