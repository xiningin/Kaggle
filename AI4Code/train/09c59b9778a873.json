{"cell_type":{"4eafb8c8":"code","90b5a037":"code","e0609573":"code","1a5c61e9":"code","714098bb":"code","1ec60467":"code","39de45ad":"code","1dc7a17b":"code","cf3df300":"code","ffd0b7b1":"code","8ff1a470":"code","7bd2c223":"code","21ae7273":"code","66617481":"code","28d610e3":"code","b4bd9e6f":"code","7a84ef94":"code","41db8aaa":"code","40fb417d":"code","66b98453":"code","16b2cae7":"code","b0832972":"code","e2d9e0b1":"code","4224acf8":"code","eb263e0b":"code","4b3daed1":"code","691dc945":"code","fcc7bdc2":"code","4133dfa0":"code","7391a205":"code","bad80e7b":"code","daed94bb":"code","51a8a99a":"code","286b8828":"code","0f9ede2a":"code","1bce1132":"code","b1759173":"markdown","32743061":"markdown","0ea72dc9":"markdown","8044ac8d":"markdown","7ff8c102":"markdown","540fc659":"markdown","e0e3eb8f":"markdown","9a100e69":"markdown","4845232c":"markdown","213c1c0e":"markdown","2155dd3c":"markdown","7552e7b2":"markdown","e10a8587":"markdown"},"source":{"4eafb8c8":"import numpy as np\nimport random\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nimport seaborn as sns\nimport pandas as pd\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nfrom utilities_x_ray import read_xray,showXray\nfrom tqdm import tqdm\nimport pydicom\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom keras import preprocessing, layers\nfrom keras.models import Sequential\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import train_test_split\nimport os\nimport cv2\nfrom PIL import Image\nimport pickle\nfrom pathlib import Path\n\nimport imageio\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\n\nimport os","90b5a037":"# Import Libraries\nimport matplotlib.pyplot as plt\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nfrom torch.autograd import Variable","e0609573":"### To test whether GPU instance is present in the system of not.\nuse_cuda = torch.cuda.is_available()\nprint('Using PyTorch version:', torch.__version__, 'CUDA:', use_cuda)","1a5c61e9":"df_train = pd.read_csv('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train.csv')\nss = pd.read_csv('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/sample_submission.csv')","714098bb":"df_train.head()","1ec60467":"ids = df_train.image_id[:10]\nimgs = []\nlabels = []\nnps = []\nim_label_set = []\nn_images = 100\n\ntrain = os.listdir('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train')\ntest = os.listdir('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/test')\n\ntest_img = ('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/test\/')\ntrain_img = ('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/')","39de45ad":"ss.head()","1dc7a17b":"plt.figure(figsize=(8,10))\nplt.imshow(read_xray('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/0108949daa13dc94634a7d650a05c0bb.dicom'),cmap=plt.cm.bone)","cf3df300":"print(\"Number of rows in train dataframe: {}\".format(df_train.shape[0]))\nprint(\"Number of Unique images in train set: {}\".format(df_train.image_id.nunique()))\nprint(\"Number of Classes: {}\\n\".format(df_train.class_name.nunique()))\nprint(\"Class Names: {}\".format(list(df_train.class_name.unique())))","ffd0b7b1":"print(\"Null Values:\")\ndf_train.isna().sum().to_frame().rename(columns={0:'Null Value count'}).style.background_gradient('viridis')","8ff1a470":"plt.figure(figsize=(9,6))\nsns.countplot(df_train[\"class_id\"]);\nplt.title(\"Class Distributions\");","7bd2c223":"plt.figure(figsize=(9,6))\nsns.countplot(df_train[\"rad_id\"]);\nplt.title(\"rad_id Distributions\");","21ae7273":"class_names = sorted(df_train.class_name.unique())\ndel class_names[class_names.index('No finding')]\nclass_names = class_names+['No finding']\nclasses = dict(zip(list(range(15)),class_names))","66617481":"def prepareDataFrame(train_df= df_train):\n    train_df = train_df.fillna(0)\n    cols = ['image_id','label']+list(range(4*len(class_names[:-1])))\n    return_df = pd.DataFrame(columns=cols)\n    \n    for image in tqdm(train_df.image_id.unique()):\n        df = train_df.query(\"image_id==@image\")\n        label = np.zeros(15)\n        for cls in df.class_id.unique():\n            label[int(cls)]=1\n        bboxes_df = df.groupby('class_id')[['x_min','y_min','x_max','y_max']].mean().round()\n        \n        bboxes_list = [0 for i in range(60)]\n        for ind in list(bboxes_df.index):\n            bboxes_list[4*ind:4*ind+4] = list(bboxes_df.loc[ind,:].values)\n        return_df.loc[len(return_df),:] = [image]+[label]+bboxes_list[:-4]\n    return return_df\ntrain_df = prepareDataFrame()","28d610e3":"train_df.head(2)","b4bd9e6f":"temp=train_df[train_df.image_id=='50a418190bc3fb1ef1633bf9678929b3']\nnp.array([temp.iloc[0,1],temp.iloc[0,2:].values])","7a84ef94":"### To test whether GPU instance is present in the system of not.\nuse_cuda = torch.cuda.is_available()\nprint('Using PyTorch version:', torch.__version__, 'CUDA:', use_cuda)                ","41db8aaa":"device = torch.device(\"cuda\" if use_cuda else \"cpu\")\ndevice","40fb417d":"class DataLoader:\n    def __init__(self,path = None,train_df=train_df):\n        self.path = path\n        self.files = os.listdir(self.path)\n        np.random.shuffle(self.files)\n        self.df = train_df\n    \n    def read_image(self):\n        for img in self.files:\n            im_name = img.split('.dicom')[0]\n            image = read_xray(self.path+img)\n            image = cv2.resize(image,(256,256),cv2.INTER_AREA)\n            image = np.expand_dims(image,axis=2)\n            temp = self.df[self.df.image_id==im_name]\n            c_label,bb = temp.iloc[0,1],temp.iloc[0,2:].values.astype('float')\n            yield image,c_label,bb\n    \n    \n    def batch_generator(self,items,batch_size):\n        a=[]\n        i=0\n        for item in items:\n            a.append(item)\n            i+=1\n\n            if i%batch_size==0:\n                yield a\n                a=[]\n        if len(a) is not 0:\n            yield a\n            \n    def flow(self,batch_size):\n        \"\"\"\n        flow from given directory in batches\n        ==========================================\n        batch_size: size of the batch\n        \"\"\"\n        while True:\n            for bat in self.batch_generator(self.read_image(),batch_size):\n                batch_images = []\n                batch_c_labels = []\n                batch_bb = []\n                for im,im_c_label,im_bb in bat:\n                    batch_images.append(im)\n                    batch_c_labels.append(im_c_label)\n                    batch_bb.append(im_bb)\n                batch_images = np.stack(batch_images,axis=0)\n                batch_labels =  (np.stack(batch_c_labels,axis=0),np.stack(batch_bb,axis=0))\n                yield batch_images,batch_labels","66b98453":"dl = DataLoader('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/')\ntrain_set = dl.flow(batch_size=32)","16b2cae7":"for n, id_ in enumerate(ids):\n    dicom_path = train_img + id_ + '.dicom'\n    dicom = pydicom.read_file(dicom_path)\n    data = apply_voi_lut(dicom.pixel_array, dicom)\n    \n    data = data - np.min(data)\n    \n    data = data \/ np.max(data)\n    \n    data = (data * 255).astype(np.uint8)\n    new_shape = tuple([int(x \/ 3) for x in data.shape])\n    data = cv2.resize(data, (new_shape[1], new_shape[0]))\n    im = Image.fromarray(data)\n    \n    #imgs.append(im)\n    \n    new_im = im.resize((256,256))\n    npdata = np.asarray(new_im)\n    new = preprocessing.image.img_to_array(npdata)\n    nps.append(new)\n    \n    #im.save('test'+str(n)+'.jpg')\n    \n    df = df_train[df_train.image_id == id_]\n    label = df.iloc[0,2]\n    #label = tf.io.decode_raw(label, tf.uint8)\n    #label = tf.reshape(label, label.shape)\n    #label = tf.one_hot(label, 10)\n    labels.append(label) ","b0832972":"# compile data into a keras readable format\n\nlabels = np.stack(labels)\ndata = np.stack(nps)","e2d9e0b1":"dl = DataLoader('..\/input\/vinbigdata-chest-xray-abnormalities-detection\/train\/')\ntrain_set = dl.flow(batch_size=32)","4224acf8":"def build_cnn():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu',strides=(2,2), input_shape=(256, 256, 3)))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    model.add(tf.keras.layers.Conv2D(64, (3, 3), strides=(2,2),activation='relu'))\n    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n    model.add(tf.keras.layers.Conv2D(128, (3, 3), strides=(2,2),activation='relu'))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(128, activation='relu'))\n    model.add(tf.keras.layers.Dense(15, activation='softmax'))\n    return model","eb263e0b":"# Build CNN model\nmodel = build_cnn()\n#Compile the model with optimizer and loss function\nmodel.compile(optimizer='adam',loss='categorical_crossentropy',\nmetrics=['accuracy'])","4b3daed1":"# split data into train & test\n\nX_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=34)","691dc945":"batch_size = 256\nepochs = 50\nhistory = model.fit(X_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=0)\nscore = model.evaluate(X_test, y_test, verbose=0)\n","fcc7bdc2":"# create ohe of labels\n\ncomp_labels = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14]\n\nmlb = MultiLabelBinarizer()\n\nmlb.fit([labels])","4133dfa0":"# verify all classes are present\n\nmlb.classes_","7391a205":"# examine X_train shape before feeding model \n\nX_train.shape","bad80e7b":"# view model summary to get an idea of its parameters and steps\n\nmodel.summary()","daed94bb":"# Save the CNN model to disk for later use.\nmodel_path = \"models\/pneumiacnn\"\nmodel.save(filepath=model_path)","51a8a99a":"# evaluate model on the given input data from only 3000 of the training images\n\nmodel.evaluate(X_test, y_test)","286b8828":"# visualize model steps \n\ntf.keras.utils.plot_model(model)","0f9ede2a":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.head()\n","1bce1132":"test_df.to_csv('submission.csv', index=False)","b1759173":"## 1. DataFrames","32743061":"## 2. Images","0ea72dc9":"<h1 style=\"display:inline\"><a id=\"third\"> An Intuition of the Data<\/a><\/h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https:\/\/toppng.com\/uploads\/preview\/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"><\/a><br><br>\n<h5>Before proceeding further let us try and get an intuition of the data and what exactly we need to do.<\/h5>\n<h5> In this competition we have been given 15000 images for training. Parallelly we have a dataframe containing the ground truths for various abnormalities. Every sample in the datframe contains:<\/h5>\n  <ul>\n      <li>the image id<\/li><li>the id of the radiologist who annoted it<\/li><li>the name of the corresponding class<\/li><li>the class id<\/li><li>the bounding box coordinates<\/li>\n  <\/ul>\n<b style=\"font-weight:700\">Important points to be noted here are:<\/b>\n<ul>\n    <li>Each image may have multiple corresponding abnormalities. Therefore this is a multilabel prediction<\/li>\n    <li>Bounding boxes for each image have been annoted by multiple radiologists. Therefore for every sample we have multiple ground truths. A naive way to deal with this is to take mean of bounding box coordinates by every radiologists for a particular abnormality<\/li>\n    <li>There is a significant class imbalance which is likely to affect the performance of models a lot.<\/li>\n<\/ul>\n<h4 style=\"font-weight:700\">Information about dicom can be found: <a href=\"https:\/\/en.wikipedia.org\/wiki\/DICOM\" style=\"font-size:1em\">Here<\/a><\/h4>\n<h4 style=\"font-weight:700\">Procedure to extract DICOM metadata can be found in: <a href=\"https:\/\/www.kaggle.com\/mrutyunjaybiswal\/vbd-chest-x-ray-abnormalities-detection-eda\" style=\"font-size:1em\">this notebook<\/a><\/h4>","8044ac8d":"<h1 style=\"display:inline\"> <a id=\"first\"> First Look at the data<\/a><\/h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https:\/\/toppng.com\/uploads\/preview\/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"><\/a>","7ff8c102":"<h1 style=\"display:inline\"><a id=\"fifth\">Model Building and Training<\/a><\/h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https:\/\/toppng.com\/uploads\/preview\/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"><\/a>","540fc659":"The submission file must contain the image id and the prediction string in the format \"a b (c,d,e,f)\"<br>where\n<ul>\n    <li>a = predicted class ; 14 for no abnormality<\/li>\n    <li>b= confidence<\/li>\n    <li>(c,d,e,f) = (x_min,y_min,x_max,y_max)<\/li>\n<\/ul>","e0e3eb8f":"<h1 style=\"display:inline\"><a id=\"second\">EDA<\/a><\/h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https:\/\/toppng.com\/uploads\/preview\/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"><\/a>","9a100e69":"### The Distribution of Classes\nWe can see there is a huge class imbalance. The number of negative examples are very high and a few abnormalities have very few examples ","4845232c":"The number of null values are same as the number of samples that do not have any abnormality","213c1c0e":"<ul>\n<li><code>image_id<\/code> - unique image identifier<\/li>\n<li><code>class_name<\/code>&nbsp;- the name of the class of detected object (or \"No finding\")<\/li>\n<li><code>class_id<\/code>&nbsp;- the ID of the class of detected object<\/li>\n<li><code>rad_id<\/code>&nbsp;- the ID of the radiologist that made the observation<\/li>\n<li><code>x_min<\/code>&nbsp;- minimum X coordinate of the object's bounding box<\/li>\n<li><code>y_min<\/code>&nbsp;- minimum Y coordinate of the object's bounding box<\/li>\n<li><code>x_max<\/code>&nbsp;- maximum X coordinate of the object's bounding box<\/li>\n<li><code>y_max<\/code>&nbsp;- maximum Y coordinate of the object's bounding box<\/li>\n<\/ul>","2155dd3c":"<img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/24800\/logos\/header.png?t=2020-12-17-19-26-15\">\n<center>\n    <h1 style=\"color:red;font-weight:900;font-size:2.5em\">VinBigData Chest X-ray Abnormalities Detection<\/h1>\n    <h3>Automatically localize and classify thoracic abnormalities from chest radiographs<\/h3>\n<\/center>\n<br>\n<br>\n<hr>\n<h2 style=\"color:blue;font-weight:600\"> About Competition <\/h2>\n<p>\n    Radiologists diagnose and treat medical conditions using imaging techniques like CT and PET scans, MRIs, and, of course, X-rays. Yet, as it happens when working with such a wide variety of medical tools, radiologists face many daily challenges, perhaps the most difficult being the chest radiograph. The interpretation of chest X-rays can lead to medical misdiagnosis, even for the best practicing doctor. Computer-aided detection and diagnosis systems (CADe\/CADx) would help reduce the pressure on doctors at metropolitan hospitals and improve diagnostic quality in rural areas.\n<\/p>\n<p>\n    In this competition we are to predict the thoracic abnormalities in given X-Ray images and also locate those abnormalities. The data provided include:\n    <ul>\n    <li>Train and Test X-Ray images in folders <b style=\"font-weight:700\">Train<\/b> and <b style=\"font-weight:700\">Test<\/b>\n    <li> sample submission file in sample_submission.csv\n    <li> train dataframe in train.csv\n    <\/ul>\n<\/p>\n<hr>\n<br>\n<a id=\"home\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\" style=\"background: rgb(49,114,163);\nbackground: radial-gradient(circle, rgba(49,114,163,1) 0%, rgba(26,136,181,1) 15%, rgba(1,159,200,1) 52%, rgba(0,212,255,1) 60%, rgba(0,182,224,1) 64%, rgba(0,145,186,1) 69%, rgba(1,66,104,1) 82%, rgba(2,33,70,1) 95%, rgba(24,23,50,1) 100%);\">\n  <h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\"style=\"background: rgb(49,114,163);\nbackground: radial-gradient(circle, rgba(49,114,163,1) 0%, rgba(26,136,181,1) 15%, rgba(1,159,200,1) 52%, rgba(0,212,255,1) 60%, rgba(0,182,224,1) 64%, rgba(0,145,186,1) 69%, rgba(1,66,104,1) 82%, rgba(2,33,70,1) 95%, rgba(24,23,50,1) 100%);\">Table of Contents<\/h3>\n    <center>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#first\" role=\"tab\" aria-controls=\"profile\">First Look at the Data<span class=\"badge badge-primary badge-pill\">1<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#second\" role=\"tab\" aria-controls=\"profile\">EDA<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#third\" role=\"tab\" aria-controls=\"profile\">An insight of the Data<span class=\"badge badge-primary badge-pill\">2<\/span><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#fourth\" role=\"tab\" aria-controls=\"messages\">Data Preparation<span class=\"badge badge-primary badge-pill\">3<\/span><\/a>\n    <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#fifth\" role=\"tab\" aria-controls=\"messages\">Model Building and training<span class=\"badge badge-primary badge-pill\">4<\/span><\/a>\n    <\/center>\n<\/div>\n<hr>\n<h1 style=\"color:red\">Note:<\/h1>\n<h5 style=\"color:red\">The utilities_x_ray module used here is a script that I have written(can be found <a href=\"https:\/\/www.kaggle.com\/bibhash123\/utilities-x-ray\">here<\/a>). It contains some functions for visualization of the X-Ray images. The dicom image reading pipeline is taken from <a href=\"https:\/\/www.kaggle.com\/raddar\/popular-x-ray-image-normalization-techniques\"> this Notebook<\/a> by <a href=\"https:\/\/www.kaggle.com\/raddar\">@raddar<\/a><\/h5>","7552e7b2":"### Distribution of Radiologists","e10a8587":"<h1 style=\"display:inline\"><a id=\"fourth\">Data Preparation<\/a><\/h1>&emsp;&emsp;&emsp;&emsp;&emsp;<a href=\"#home\" style=\"color:blue\"><img src=\"https:\/\/toppng.com\/uploads\/preview\/light-blue-up-arrow-11550117759k4je61afsa.png\" style=\"display:inline;width:2em;height:2em\"><\/a>"}}