{"cell_type":{"23ed3d55":"code","91b8feef":"code","c098b8b7":"code","4ae9f238":"code","9a88834b":"code","883ee5c1":"code","6ea607a8":"code","6a80649f":"code","4c47d767":"code","81d1b5d6":"code","6ce9e246":"code","716d9608":"code","0738b4b9":"code","f43718ae":"code","e899307b":"code","df3e5aa7":"code","6da9913f":"code","fbfda4ef":"code","20b4ee33":"code","0f6c8669":"code","18014043":"code","6682dbc2":"markdown","da0471db":"markdown","f0366cb2":"markdown","feb61b64":"markdown","9961f10e":"markdown","0b548d3b":"markdown","f4ff9c67":"markdown","6fd41b6e":"markdown"},"source":{"23ed3d55":"!pip install alibi\n!pip install lime","91b8feef":"import numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport spacy\nfrom alibi.explainers import AnchorText\nfrom alibi.datasets import fetch_movie_sentiment\nfrom alibi.utils.download import spacy_model\nimport pandas as pd\nimport sklearn\nimport sklearn.ensemble\nimport sklearn.metrics\nfrom lime import lime_text\nfrom sklearn.pipeline import make_pipeline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c098b8b7":"movies = fetch_movie_sentiment()\nmovies.keys()","4ae9f238":"data = movies.data\nlabels = movies.target\ntarget_names = movies.target_names","9a88834b":"train, test, train_labels, test_labels = train_test_split(data, labels, test_size=.2, random_state=42)\ntrain, val, train_labels, val_labels = train_test_split(train, train_labels, test_size=.1, random_state=42)\ntrain_labels = np.array(train_labels)\ntest_labels = np.array(test_labels)\nval_labels = np.array(val_labels)","883ee5c1":"vectorizer = CountVectorizer(min_df=1)\nvectorizer.fit(train)","6ea607a8":"np.random.seed(0)\nclf = LogisticRegression(solver='liblinear')\nclf.fit(vectorizer.transform(train), train_labels)","6a80649f":"predict_fn = lambda x: clf.predict(vectorizer.transform(x))","4c47d767":"preds_train = predict_fn(train)\npreds_val = predict_fn(val)\npreds_test = predict_fn(test)\nprint('Train accuracy', accuracy_score(train_labels, preds_train))\nprint('Validation accuracy', accuracy_score(val_labels, preds_val))\nprint('Test accuracy', accuracy_score(test_labels, preds_test))","81d1b5d6":"model = 'en_core_web_md'\nspacy_model(model=model)\nnlp = spacy.load(model)","6ce9e246":"explainer = AnchorText(nlp, predict_fn)","716d9608":"class_names = movies.target_names","0738b4b9":"text = data[4]\nprint(text)\n","f43718ae":"pred = class_names[predict_fn([text])[0]]\nalternative =  class_names[1 - predict_fn([text])[0]]\nprint('Prediction: %s' % pred)","e899307b":"np.random.seed(0)\nexplanation = explainer.explain(text, threshold=0.95, use_unk=True)","df3e5aa7":"print('Anchor: %s' % (' AND '.join(explanation.anchor)))\nprint('Precision: %.2f' % explanation.precision)\nprint('\\nExamples where anchor applies and model predicts %s:' % pred)\nprint('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\nprint('\\nExamples where anchor applies and model predicts %s:' % alternative)\nprint('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))\n","6da9913f":"np.random.seed(0)\nexplanation = explainer.explain(text, threshold=0.95, use_unk=False, sample_proba=0.5)","fbfda4ef":"print('Anchor: %s' % (' AND '.join(explanation.anchor)))\nprint('Precision: %.2f' % explanation.precision)\nprint('\\nExamples where anchor applies and model predicts %s:' % pred)\nprint('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\nprint('\\nExamples where anchor applies and model predicts %s:' % alternative)\nprint('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))","20b4ee33":"explanation = explainer.explain(data[8], threshold=0.95, use_similarity_proba=True, use_unk=False,\n                                sample_proba=0.5, top_n=20, temperature=0.2)","0f6c8669":"print('Anchor: %s' % (' AND '.join(explanation.anchor)))\nprint('Precision: %.2f' % explanation.precision)\nprint('\\nExamples where anchor applies and model predicts %s:' % pred)\nprint('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\nprint('\\nExamples where anchor applies and model predicts %s:' % alternative)\nprint('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))","18014043":"np.random.seed(0)\nexplanation = explainer.explain(text, threshold=0.95, use_similarity_proba=True, sample_proba=0.5,\n                                use_unk=False, top_n=20, temperature=.2)\n\nprint('Anchor: %s' % (' AND '.join(explanation.anchor)))\nprint('Precision: %.2f' % explanation.precision)\nprint('\\nExamples where anchor applies and model predicts %s:' % pred)\nprint('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_true']]))\nprint('\\nExamples where anchor applies and model predicts %s:' % alternative)\nprint('\\n'.join([x for x in explanation.raw['examples'][-1]['covered_false']]))","6682dbc2":"###  perturbation distribution sample words that are more similar to the ground truth word with increase Precision\n1. top_n:  Smaller values (default=100) should result in sentences that are more coherent and thus more in the distribution of natural language which could influence the returned anchor.\n2. use_probability_proba=True:  the sampling distribution for perturbed tokens is proportional to the similarity score between the possible perturbations and the original word.\n3. temperature: Lower values of temperature increase the sampling weight of more similar words.","da0471db":"### Movie Review Dataset","f0366cb2":"Prediction","feb61b64":"### Load spaCy model\nEnglish multi-task CNN trained on OntoNotes, with GloVe vectors trained on Common Crawl. Assigns word vectors, context-specific token vectors, POS tags, dependency parse and named entities.","9961f10e":"## Explaining Prediction","0b548d3b":"### Explanation with UNK keyword:\nuse_unk=True means we will perturb examples by replacing words with UNKs. Let us now take a look at the anchor. The word \u2018exercise\u2019 basically guarantees a negative prediction.","f4ff9c67":"## Moview Review Dataset Anchors TestCase\n\n### Changing the perturbation distribution. changing UNK word with similar words:\nLet\u2019s try this with another perturbation distribution, namely one that replaces words by similar words instead of UNKs","6fd41b6e":"The anchor  shows that we need more to guarantee the negative prediction"}}