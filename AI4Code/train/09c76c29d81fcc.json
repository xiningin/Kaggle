{"cell_type":{"28554c7e":"code","9687122f":"code","bd4e9e4e":"code","5aef4edb":"code","c4e685bd":"code","80366f7b":"code","347c1c4f":"code","38bba03e":"code","d6ab6475":"code","2e027ed1":"code","f63eb1fe":"code","330f5891":"code","0d5ecb31":"code","afc58132":"code","559c48ef":"code","698a22d7":"code","733a0baf":"code","86211917":"code","c29284f7":"code","9fa06a48":"code","75c6884b":"code","a451381b":"code","d6f0c244":"code","6d7dfb2f":"code","8014e544":"code","f13e18f0":"code","868995ae":"code","869b800c":"code","4888a49e":"code","74b34ab8":"code","f23015c2":"code","f88092bb":"code","61df2a94":"code","0fe7a671":"code","2ed66ef6":"code","1add7319":"code","947b389a":"code","527725a6":"code","6797095d":"code","312f0949":"code","98274151":"code","65cf557d":"code","e571f143":"code","09318734":"code","005d6dab":"code","61cec455":"code","7b29aa2a":"code","23c81512":"code","96937a2e":"code","bbff1a9c":"code","d862b80b":"code","aaa5cf7d":"code","dec594f8":"code","0d897aa6":"code","a95cb885":"code","eea7473c":"code","a145ea21":"code","4f8fbebc":"code","3cf9328b":"code","c6676943":"code","4535b989":"code","52220c91":"code","e53a9c52":"code","4357a97c":"code","c6148a16":"code","b3393f9c":"code","eda243d0":"code","b666fb68":"markdown","120761f9":"markdown","1e33a479":"markdown","eb551bfb":"markdown","c4c1bdf7":"markdown","b2bcb9fe":"markdown","bf24e943":"markdown","058ad830":"markdown","0f249343":"markdown","2156fbf8":"markdown","136154f5":"markdown","f7a34d8f":"markdown","86dbb0b3":"markdown","de2712ad":"markdown","b7b55ce0":"markdown","150b49d7":"markdown","9fdb4c1d":"markdown","b6ffd3b4":"markdown","65f71dae":"markdown","56878d48":"markdown","174d8e65":"markdown","4fa85aec":"markdown","c6b9c10d":"markdown"},"source":{"28554c7e":"#Importing Libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))\n","9687122f":"#Loading the dataset\ntrain_data = pd.read_csv(\"..\/input\/train-data\/train_cab.csv\", sep=\",\")\ntest_data  = pd.read_csv(\"..\/input\/test-data\/test.csv\", sep=\",\")","bd4e9e4e":"#Missing Value Analysis\nmissing_val = pd.DataFrame(train_data.isnull().sum())\nmissing_val = missing_val.reset_index()\nmissing_val = missing_val.rename(columns={'index':'variables',0:'Missing_values'})\nmissing_val['Missing_Value_Percentage'] = (missing_val.Missing_values\/len(train_data))*100\nmissing_val = missing_val.sort_values('Missing_Value_Percentage',ascending=False).reset_index(drop=True)\nmissing_val","5aef4edb":"#Dropping Missing values(NA values) very few viables have missing values and its better remove them.\ntrain_data.drop(train_data[train_data.fare_amount.isnull()==True].index,axis=0,inplace=True)\ntrain_data.drop(train_data[train_data.passenger_count.isnull()==True].index,axis=0,inplace=True)","c4e685bd":"#Rechecking Missing Value \nmissing_val = pd.DataFrame(train_data.isnull().sum())\nmissing_val = missing_val.reset_index()\nmissing_val = missing_val.rename(columns={'index':'variables',0:'Missing_values'})\nmissing_val","80366f7b":"#Checking the Data\ntrain_data.head()","347c1c4f":"#Checking the datatypes\ntrain_data.dtypes","38bba03e":"#Reordering incurrect datatypes of Variables\ntrain_data['fare_amount'] = pd.to_numeric(train_data['fare_amount'],errors='coerce')\ntrain_data['pickup_datetime'] = pd.to_datetime(train_data['pickup_datetime'],infer_datetime_format=True,errors='coerce')\ntrain_data['passenger_count'] = train_data['passenger_count'].astype('int')","d6ab6475":"train_data.dtypes","2e027ed1":"from math import radians, cos, sin, asin, sqrt\ndef distance(pickup_lat, pickup_lon, dropoff_lat, dropoff_lon):\n    \"\"\"\n    Return distance along great radius between pickup and dropoff coordinates.\n    \"\"\"\n    #Define earth radius (km)\n    R_earth = 6371\n    \n    #Convert degrees to radians\n    pickup_lat, pickup_lon, dropoff_lat, dropoff_lon = map(np.radians,\n                                                             [pickup_lat, pickup_lon, \n                                                              dropoff_lat, dropoff_lon])\n    #Compute distances along lat, lon dimensions\n    dlat = dropoff_lat - pickup_lat\n    dlon = dropoff_lon - pickup_lon\n    \n    #Compute haversine distance\n    a = np.sin(dlat\/2.0)**2 + np.cos(pickup_lat) * np.cos(dropoff_lat) * np.sin(dlon\/2.0)**2\n    \n    return 2 * R_earth * np.arcsin(np.sqrt(a))\n\ndef date_time_info(data):\n    data['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'], format=\"%Y-%m-%d %H:%M:%S UTC\")\n    \n    data['hour'] = data['pickup_datetime'].dt.hour\n    data['day']  = data['pickup_datetime'].dt.day\n    data['month'] = data['pickup_datetime'].dt.month\n    data['weekday'] = data['pickup_datetime'].dt.weekday\n    data['year']    = data['pickup_datetime'].dt.year\n    \n    return data\n\n#Applying on train_data\ntrain_data = date_time_info(train_data)\ntrain_data['distance'] = distance(train_data['pickup_latitude'], \n                                     train_data['pickup_longitude'],\n                                     train_data['dropoff_latitude'] ,\n                                     train_data['dropoff_longitude'])\n#Preprocessing on test data\n#Applying distance and date_time_info function on test_data\ntest_data = date_time_info(test_data)\ntest_data['distance'] = distance(test_data['pickup_latitude'], \n                                     test_data['pickup_longitude'],\n                                     test_data['dropoff_latitude'] ,\n                                     test_data['dropoff_longitude'])\n\ntest_key = pd.DataFrame({'key_date':test_data['pickup_datetime']})\ntest_data = test_data.drop(columns=['pickup_datetime'],axis=1)\n\ntrain_data.head()","f63eb1fe":"#weekday starts from 0 to 6\ntrain_data.describe()","330f5891":"continuous_variables = ['year','month','fare_amount','passenger_count','pickup_longitude',\n                        'pickup_latitude','dropoff_longitude','dropoff_latitude','distance']\nfor i in continuous_variables:\n    plt.hist(train_data[i],bins=18)\n    plt.title(\"Checking Distribution for Variable \"+str(i))\n    plt.ylabel(\"Density\")\n    plt.xlabel(i)\n    plt.show()","0d5ecb31":"train_data.dtypes","afc58132":"#Outlier Visualizations\ncol = ['fare_amount', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude']\nfor i in col:\n    sns.boxplot(y=train_data[i])\n    fig=plt.gcf()\n    fig.set_size_inches(5,5)\n    plt.show()","559c48ef":"#Fare_amount data distribution by using Scatter plot for all the observations\nplt.scatter(x=train_data.fare_amount,y=train_data.index)\nplt.ylabel('Index')\nplt.xlabel('fare_amount')\nplt.show()","698a22d7":"#Fare_amount data distribution by using Scatter plot for selected observations (of x lim range from 1 to 70)\n#because from 70 onwards all observations are extreme outliers.\nplt.scatter(x=train_data.fare_amount,y=train_data.index)\nplt.ylabel('Index')\nplt.xlim(1,70)\nplt.xlabel('fare_amount')\nplt.show()","733a0baf":"#passenger_count data distribution by using Scatter plot for all the observations,\n#because from 70 onwards all observations are extreme outliers.\nplt.scatter(x=train_data.passenger_count,y=train_data.index)\nplt.ylabel('Index')\nplt.xlabel('passenger_count')\nplt.show()","86211917":"    #passenger_count data distribution by using Scatter plot for sected observations ( x lim range from 1 to 10)\n    #because from 70 onwards all observations are extreme outliers.\n    plt.scatter(x=train_data.passenger_count,y=train_data.index)\n    plt.ylabel('Index')\n    plt.xlim(1,10)\n    plt.xlabel('passenger_count')\n    plt.show()","c29284f7":"#Scatter plot for distance variable for all the observations\nplt.scatter(x=train_data.distance,y=train_data.index)\nplt.ylabel('Index')\n#plt.xlim(1,10)\nplt.xlabel('ditsance')\nplt.show()","9fa06a48":"    #Scatter plot for distance variable for selected index range of observations\n    plt.scatter(x=train_data.distance,y=train_data.index)\n    plt.ylabel('Index')\n    plt.xlim(1,30)\n    plt.xlabel('ditsance')\n    plt.show()","75c6884b":"#Finding the longitude min and max of test data\nlon_min=min(test_data.pickup_longitude.min(),test_data.dropoff_longitude.min())\nlon_max=max(test_data.pickup_longitude.max(),test_data.dropoff_longitude.max())\nprint(lon_min,',',lon_max)\n\n#Finding the latitude min and max of test data\nlat_min=min(test_data.pickup_latitude.min(),test_data.dropoff_latitude.min())\nlat_max=max(test_data.pickup_latitude.max(),test_data.dropoff_latitude.max())\nprint(lat_min,',',lat_max)","a451381b":"train_data.describe()","d6f0c244":"#1 - Removing -ve values from the fare_amount variable\ntrain_data_new = train_data  \ntrain_data=train_data.drop(train_data[(train_data.fare_amount<=0) | (train_data.fare_amount>=65)].index,axis=0)\n\n#2 - Removing null values from passenger count\n#From the the test data passenger count lies between min is 1 and max is 6 \ntrain_data=train_data.drop(train_data[(train_data.passenger_count<=0) | (train_data.passenger_count>6)].index,axis=0)\n\n#4 - Removing pickup_latitude,dropoff_latitude, pickup_longitude, and dropoff_longitude\ntrain_data=train_data.drop(train_data[(train_data.pickup_latitude <lat_min)| (train_data.pickup_latitude >lat_max)].index,axis=0)\ntrain_data=train_data.drop(train_data[(train_data.dropoff_latitude <lat_min) | (train_data.dropoff_latitude >lat_max)].index,axis=0)\ntrain_data=train_data.drop(train_data[(train_data.pickup_longitude <lon_min) | (train_data.pickup_longitude >lon_max)].index,axis=0)\ntrain_data=train_data.drop(train_data[(train_data.dropoff_longitude <lon_min) | (train_data.dropoff_longitude >lon_max)].index,axis=0)\n\n#4 - Removing Outliers in the Distance variable outliers\ntrain_data=train_data.drop(train_data[(train_data.distance <=0)].index,axis=0)\n","6d7dfb2f":"train_data.shape","8014e544":"train_data.isna().sum()","f13e18f0":"    train_data.dropna(axis=0,inplace=True)\n    train_data.isnull().sum()","868995ae":"train_data.shape","869b800c":"train_data.describe()","4888a49e":"test_data.describe()","74b34ab8":"continuous_variables = ['year','month','fare_amount','passenger_count','pickup_longitude',\n                        'pickup_latitude','dropoff_longitude','dropoff_latitude','distance']\nfor i in continuous_variables:\n    plt.hist(train_data[i],bins=18)\n    plt.title(\"Checking Distribution for Variable \"+str(i))\n    plt.ylabel(\"Density\")\n    plt.xlabel(i)\n    plt.show()","f23015c2":"#Plot for fare_amount variation across distance\nplt.scatter(y=train_data['distance'],x=train_data['fare_amount'])\nplt.xlabel('fare')\nplt.ylabel('distance')\nplt.show()","f88092bb":"sns.countplot(train_data['passenger_count'])","61df2a94":"train_data.columns","0fe7a671":"plt.figure(figsize=(15,7))\nplt.scatter(x=train_data['day'], y=train_data['fare_amount'], s=1.5)\nplt.xlabel('Day')\nplt.ylabel('Fare')\nplt.show()","2ed66ef6":"    plt.figure(figsize=(15,7))\n    plt.scatter(x=train_data['hour'], y=train_data['fare_amount'], s=1.5)\n    plt.xlabel('Hour')\n    plt.ylabel('Fare')\n    plt.show()","1add7319":"    plt.figure(figsize=(15,7))\n    plt.hist(train_data['passenger_count'], bins=15)\n    plt.xlabel('No. of Passengers')\n    plt.ylabel('Frequency')\n    plt.show()","947b389a":"plt.figure(figsize=(15,7))\nplt.scatter(x=train_data['passenger_count'], y=train_data['fare_amount'], s=1.5)\nplt.xlabel('No. of Passengers')\nplt.ylabel('Fare')\nplt.show()","527725a6":"plt.figure(figsize=(15,7))\nplt.hist(train_data['weekday'], bins=100)\nplt.xlabel('Day of Week')\nplt.ylabel('Frequency')\nplt.show()","6797095d":"#Normality check\n#%matplotlib inline  \nplt.hist(train_data['fare_amount'], bins='auto')\n","312f0949":"#taking copy of the data\ntrain_data_df = train_data.copy()","98274151":"#Nomalization\n\"\"\"\ncnames =['fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'hour',\n       'day', 'month', 'weekday', 'year', 'distance']\nfor i in cnames:\n    print(i)\n    train_data[i] = (train_data[i] - min(train_data[i]))\/(max(train_data[i]) - min(train_data[i]))\n\"\"\"","65cf557d":"#train_data = train_data_df.copy()\n#correlation between numerical variables\nnum = pd.DataFrame(train_data.select_dtypes(include=np.number))\ncor = num.corr()        \ncor","e571f143":"num","09318734":"train_data.describe()","005d6dab":"#Coorelation Plot to check the Coorelation\ncontinuous_variables =['fare_amount', 'pickup_datetime', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'hour',\n       'day', 'month', 'weekday', 'year', 'distance']\n\ndf_cor = train_data.loc[:,continuous_variables]\nf, ax = plt.subplots(figsize=(10,10))\n\n#Generate correlation matrix\ncor_mat = df_cor.corr()\n\n#Plot using seaborn library\nsns.heatmap(cor_mat, mask=np.zeros_like(cor_mat, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=False),\n            square=True, ax=ax)\nplt.plot()","61cec455":"train_data.passenger_count","7b29aa2a":"#VIF to check the Correlation\n#pick_up date is correlated to its extracted columns i.e day, year, month, weekday\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\nX = add_constant(num)\npd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)","23c81512":"#Removing variable 'Pickup datetime' beacause day,year,month carries all the information  from it\ndel train_data['pickup_datetime']\n\n#Selected variables for model building\ntrain_data.columns\n\n\"\"\"\nIndex(['fare_amount', 'pickup_longitude', 'pickup_latitude',\n       'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'hour',\n       'day', 'month', 'weekday', 'year', 'distance'],\n      dtype='object')\n\"\"\"","96937a2e":"train_data.columns","bbff1a9c":"#Splitting data into test and train\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n\n#-> For Train data -> train_data\n\ny = train_data['fare_amount']\nX = train_data.drop(columns=['fare_amount'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\ndef evaluate(model, test_features, test_actual):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_actual)\n    rmse = np.sqrt(mean_squared_error(test_actual,predictions))\n    mape = 100 * np.mean(errors \/ test_actual)\n    accuracy = 100 - mape\n    rsquared = r2_score(test_actual, predictions)\n    df_pred = pd.DataFrame({'actual':test_actual,'predicted':predictions})\n    print('<---Model Performance--->')\n    print('R-Squared Value = {:0.2f}'.format(rsquared))\n    print('RMSE = {:0.2f}'.format(rmse))\n    print('MAPE = {:0.2f}'.format(mape))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    return rmse\n","d862b80b":"#Linear regression\nfrom sklearn.linear_model import LinearRegression\nmodel_lr = LinearRegression().fit(X_train,y_train)\n\n#predicting and testing on train data\nevaluate(model_lr, X_test, y_test)\n","aaa5cf7d":"#Decision Tree\nfrom sklearn.tree import DecisionTreeRegressor\nmodel_dt = DecisionTreeRegressor(random_state = 123).fit(X_train,y_train)\n\n#predicting and testing on train data\nevaluate(model_dt, X_test, y_test)\n","dec594f8":"#Random Forest\nfrom sklearn.ensemble import RandomForestRegressor\nmodel_rf = RandomForestRegressor().fit(X_train,y_train)\n\n#predicting and testing on train data\nevaluate(model_rf, X_test, y_test)","0d897aa6":"#Parameters of base model\nmodel_rf.get_params()","a95cb885":"X_train.columns","eea7473c":"#Printing Feature importance of the model\nfeat_importances = pd.Series(model_rf.feature_importances_, index=X_train.columns)\nfeat_importances.plot(kind='barh')","a145ea21":"X_train = x_train.copy()\n","4f8fbebc":"#SVM\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import classification_report, accuracy_score\nmodel = SVR()\nmodel.fit(X_train, y_train)\n#predicting and testing on train data\nevaluate(model, X_test, y_test)","3cf9328b":"from xgboost import XGBClassifier\nclf = XGBClassifier()\nclf\nclf.fit(X_train, y_train)\n#predicting and testing on train data\nevaluate(clf, X_test, y_test)","c6676943":"from sklearn.neighbors import KNeighborsRegressor\n# Instantiate learning model (k = 3)\nregresor = KNeighborsRegressor(n_neighbors=3).fit(X_train, y_train)\n\n#predicting and testing on train data\nevaluate(regresor, X_test, y_test)\n","4535b989":"#Hyperparameter tuning using GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [20, 25, 30],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [2,3],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [300,500,600,800]\n}\n# Create a base model\nrf = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","52220c91":"grid_search.fit(X_train,y_train)\ngrid_search.best_params_\nbest_grid = grid_search.best_estimator_\n\n#Applying gridsearchcsv to test data\ngrid_accuracy = evaluate(best_grid,X_test, y_test)","e53a9c52":"#Getting the best Parameters\n#grid_search.best_params_\n#or\ngrid_search.best_estimator_","4357a97c":"    #Printing Feature importance by visualizations\n    feat_importances_hyp = pd.Series(grid_search.best_estimator_.feature_importances_, index=X_train.columns)\n    feat_importances_hyp.plot(kind='barh')","c6148a16":"#Building Random Forest model with hypertuned parameters\nfrom sklearn.ensemble import RandomForestRegressor\nmodel_rf = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n           max_features='sqrt', max_leaf_nodes=None,\n           min_impurity_decrease=0.0, min_impurity_split=None,\n           min_samples_leaf=2, min_samples_split=10,\n           min_weight_fraction_leaf=0.0, n_estimators=800, n_jobs=None,\n           oob_score=False, random_state=None, verbose=0, warm_start=False).fit(X_train,y_train)\n\n#predicting and testing on train data\nevaluate(model_rf, X_test, y_test)\n","b3393f9c":"#Checking the given test data\ntest_data.describe()","eda243d0":"    predictions = model_rf.predict(test_data)\n    predicted_test = pd.DataFrame({'pickup_datetime':test_key['key_date'],'Predicted_Fare':predictions})\n    predicted_test.head(10)","b666fb68":"Modelling Using XGBoost","120761f9":"Feature Selection","1e33a479":"**Feature Scaling**","eb551bfb":"Model Building for Train data","c4c1bdf7":"single passengers are the most frequent travellers, and the highest fare also seems to come from cabs which carry just 1 passenger.\n\n","b2bcb9fe":"![](http:\/\/)2 - Number of Passengers vs Fare","bf24e943":"**Hyper Parameter tuning**","058ad830":"Modelling Using SVM Classifier","0f249343":"> > Feature Engineering\n- Here pickup and drop locations are related to fare_amont of the data,\n- So we need to find the distance using pickup and drop location coordinates by using \"Haversine distance formula\" ","2156fbf8":"    3 - Does the day of the week affect the fare?\n","136154f5":"The fares throught the month mostly seem uniform\n\n","f7a34d8f":"    **Asumptions for Hypothesis**","86dbb0b3":"Manualy removing outliers based on the test data feature conditions","de2712ad":"1. 1. Checking data distribution Before Outlier Analysis","b7b55ce0":"Outlier Analysis\n- from above data fare_amount has negative values and it can't be -ve values \n- Found negative value of fare amount.\n  Fare never be negative let's drop those rows which are having negative\n  fare amount and also remove outliers\n- passenger_count should be  > 0 and < =6\n- Latitude should be between min is 40.568973 and max is 41.709555\n- Longitude should be min is -74.263242 and max is  -72.986532\n","150b49d7":"**Predicting on given test data **","9fdb4c1d":"Modelling Using KNN","b6ffd3b4":"Checking data distribution after Outlier Analysis","65f71dae":"The time of day definitely plays an important role. The frequency of cab rides seem to be the lowest at 5AM","56878d48":"train_data.shape","174d8e65":"day of the week doesn't seem to have the effect on the number of cab rides\n\n","4fa85aec":"    1)Check the pickup date and time affect the fare or not","c6b9c10d":"    **Applying hyperparameters tuned  base model on test data **"}}