{"cell_type":{"2ed04d29":"code","22ff8760":"code","adafaf91":"code","055347e7":"code","0d965c25":"code","f9d86838":"code","1f2b48c5":"code","85b54bb0":"code","586ae761":"code","b9ae2c5e":"code","97e1bd8b":"code","30f1ee8c":"code","13c4d482":"code","290577c2":"code","93746e6f":"code","6984f92e":"code","fc7a13fe":"code","3ece6e2c":"code","e8bf6b8b":"code","30329430":"code","d291f635":"code","a90c6c83":"code","9c3d3ddf":"code","a801e645":"code","7eef9c7d":"markdown","181d01d9":"markdown","955d89c3":"markdown","1fa70c56":"markdown","37aa4f30":"markdown","9ed1a63e":"markdown","32a24bbb":"markdown","d5a35287":"markdown","a8cf3232":"markdown","2269f736":"markdown","ff45d331":"markdown","d10b9740":"markdown","b3a90e5e":"markdown","3dd78357":"markdown","bcd4a2b2":"markdown","ce5620f5":"markdown","b2f6bc5f":"markdown","233dbb46":"markdown","c3db5433":"markdown","e820efad":"markdown","0a531f2a":"markdown","0f65d9a9":"markdown","4a8a0f48":"markdown","16779fe1":"markdown"},"source":{"2ed04d29":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","22ff8760":"pima_df = pd.read_csv (\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")","adafaf91":"import pandas_profiling as pp\npp.ProfileReport(pima_df)","055347e7":"# Installation\n\n!pip install dataprep\n","0d965c25":"from dataprep.eda import plot, plot_correlation\nplot(pima_df)","f9d86838":" plot_correlation(pima_df)","1f2b48c5":"from dataprep.eda import plot_missing","85b54bb0":"plot_missing(pima_df)","586ae761":"plot_missing(pima_df,\"Glucose\")","b9ae2c5e":"# For numerical column usingplot:\n\nplot(pima_df,\"BloodPressure\")","97e1bd8b":"# For categorical column usingplot:\n\nplot(pima_df,\"Outcome\")","30f1ee8c":"!pip install git+git:\/\/github.com\/AutoViML\/AutoViz.git\n!pip install xlrd","13c4d482":"from autoviz.AutoViz_Class import AutoViz_Class\n\nAV = AutoViz_Class()\ndftc = AV.AutoViz(\n    filename='', \n    sep='' , \n    depVar='Outcome', \n    dfte=pima_df, \n    header=0, \n    verbose=1, \n    lowess=False, \n    chart_format='png', \n    max_rows_analyzed=300000, \n    max_cols_analyzed=30\n)","290577c2":"#Installation\n\n!pip install sweetviz\n","93746e6f":"import sweetviz as sv\nmy_report = sv.analyze(pima_df)\nmy_report.show_notebook()","6984f92e":"!pip3 install Dora","fc7a13fe":"from Dora import Dora\ndora = Dora()\ndora.configure(output = 'Outcome', data = '\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndora.data","3ece6e2c":"# plot a single feature against the output variable\ndora.plot_feature('BMI')\n\n# render plots of each feature against the output variable\ndora.explore()","e8bf6b8b":"#Installation\n\n!pip install pandas-summary","30329430":"#The DataFrameSummary expect a pandas DataFrame to summarise.\nfrom pandas_summary import DataFrameSummary\n\ndfs = DataFrameSummary(pima_df)","d291f635":"dfs.columns_types\n","a90c6c83":"dfs.columns_stats\n","9c3d3ddf":"#Installation Steps\n\n!pip install ExploriPy\n\n","a801e645":"from ExploriPy import EDA\nimport pandas as pd\ndf = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv',na_values = 'nan')\nCategoricalFeatures = ['Outcome']\neda = EDA(df,CategoricalFeatures,OtherFeatures=['Age'],title='Exploratory Data Analysis for Pima Indian')\neda.TargetAnalysis('Outcome') # For Target Specific Analysis","7eef9c7d":"<a id=\"3.5\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>3.5 Bamboolib <\/b><\/font>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n\n\nBamboolib was developed by data scientists for data scientists for making common data wrangling and exploration task fast,easy and fun.\n\n**Install**\n\nhttps:\/\/docs.bamboolib.8080labs.com\/documentation\/how-tos\/installation-and-setup\/install-bamboolib\n\nimport bamboolib as bam\n\nimport pandas as pd\n\ndf = pd.read_csv(bam.titanic_csv)\n\ndf\n\n![image.png](attachment:10a1e951-0362-4d82-8e88-40c5d3033c40.png)![image.png](attachment:299e5d5e-1d57-40ad-b3ff-bbd0d1bc2c48.png)\n\nClicking on the green button will show the detailed report \n\n","181d01d9":"<a id=\"top\"><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Content<\/h3>\n    \n   * [1. Background](#1)\n   * [2. Introduction](#2)\n   * [3. EDA Tools](#3)\n    -  [3.1 Pandas Profiling](#3.1)\n    -  [3.2 dataprep](#3.2)\n    -  [3.3 Autoviz](#3.3)\n    -  [3.4 SweetVIZ](#3.4)\n    -  [3.5 Bamboolib](#3.5)\n    -  [3.6 Dora](#3.6)\n    -  [3.7 TPOT](#3.7)\n    -  [3.8 pandas_summary](#3.8)\n    -  [3.9 ExploriPy](#3.9)\n    -  [3.10 Holoviews](#3.10)\n   * [4.Conclusion ](#4)\n","955d89c3":"- If we want to understand the correlation between columns? Use plot_correlation.It takes the dataframe object as the parameter\n","1fa70c56":"- We can drill down to get more information by given plot, plot_correlation and plot_missing a column name.: E.g. for plot_missing\n","37aa4f30":"  #                            ***Data will talk if you are willing to listen -Jim Bergeson***","9ed1a63e":"<a id=\"4\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>4. Conclusion <\/b><\/font>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n\nAfter discussing all this libraries we can say that :-\n\n- These libraries can increase up the speed of the analytics. It will be easier for the data scientist to perform the analysis more quickly with little or no human effort.\n- It also reduces the human error as most of the calculations are carried out by the libraries in a much efficient manner.\n- It allows the Data scientists to make use of the time available for the other purposes like business development and proposal writing.\n- I believe that the above mentioned reasons provides a solid justification for learning these libraries.\n","32a24bbb":"<a id=\"3.3\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>3.3 Autoviz <\/b><\/font>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n\nThis is one of the python library which is used for automatic visualizations. Automatically Visualize any dataset, any size with a single line of code.Given any input file (CSV, txt or json) and AutoViz will visualize it.\n\n**Install**\n\nTo clone Autoviz library , it\u2019s better to create a new environment and then install the required dependencies\n\n","d5a35287":"**Installation**\n\ngit clone git:\/\/github.com\/holoviz\/holoviews.git\ncd holoviews\npip install -e \n\n**Usage**\nOnce we\u2019ve installed HoloViews, you can get a copy of all the examples shown on the website:\n\nholoviews --install-examples\ncd holoviews-examples\n\n![image.png](attachment:1e036cda-fe12-4876-b296-26ded189a823.png)\n","a8cf3232":"<a id=\"3.10\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>3.10 Holoviews <\/b><\/font>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n\nAutomated visualization tool based on the short data annotations.\nStop plotting your data \u2014 annotate your data and let it visualize itself.\nHoloViews is an open-source Python library designed to make data analysis and visualization seamless and simple. With HoloViews, you can usually express what you want to do in very few lines of code, letting you focus on what you are trying to explore and convey, not on the process of plotting.\n\n","2269f736":"<a id=\"3.9\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>3.9 ExploriPy <\/b><\/font>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n\n\nIt is the python library for performing EDA tasks. ExploriPy reduces the effort of the data analyst \u2018s efforts significantly in the initial EDA. It helps in performing the initial EDA, and statistical tests including Analysis of Variance, Chi Square Test of Independence, Weight of Evidence, Information Value and Tukey Honest Significance Difference.It expects a Pandas DataFrame, along with a list of categorical variables, as input. Output will be a presentable HTML document, with the result of analysis and statistical tests, represented through several interactive charts, and tables.\n","ff45d331":"There are 3 main functions for creating reports:\n\n- analyze(\u2026)\n- compare(\u2026)\n- compare_intra(\u2026)","d10b9740":"<a id=\"3\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>3. EDA Tools <\/b><\/font>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>","b3a90e5e":"<a id=\"3.8\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>3.8 pandas_summary <\/b><\/font>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n\nThis is the extension of the python describe functions. The module contains DataFrameSummary object that extend describe() with:\n\n- properties\n\n(i) dfs.columns_stats: counts, uniques, missing, missing_perc, and type per column\n\n(ii) dsf.columns_types: a count of the types of columns\n\n(iii) dfs[column]: more in depth summary of the column\n\nfunction: summary(): extends the describe() function with the values with columns_stats","3dd78357":"**Website** :- https:\/\/sfu-db.github.io\/dataprep\/index.html\n\n**EDA**\n\nTasks during EDA like examining the column distribution or analyzing the correlation between the modules.The EDA module of the dataprep library categorizes the EDA tasks into functions\n\n- If we want to understand the distributions for each DataFrame column? Use the plot.It takes the dataframe object as the parameter.\n","bcd4a2b2":"<a id=\"3.6\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>3.6 Dora <\/b><\/font>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n\nDora is used for data cleaning, featuring engineering and simple modeling tools. It is designed to automate the painful part of exploratory data analysis. This library contains functions for data cleaning, feature selection & extraction, visualization, partitioning data for model validation, and versioning transformations of data.\n\nThe library uses and is intended to be a helpful addition to common Python data analysis tools such as pandas, scikit-learn, and matplotlib.\n\n","ce5620f5":"### Connector\n\nConnector provides a simple way to collect data from different websites:\n\n- A unified API: we can fetch data using one or two lines of code to get data from many websites.\n- Auto Pagination: it automatically does the pagination for us so that we can specify the desired count of the returned results without even considering the count-per-request restriction from the API.\n- Smart API request strategy: it can issue API requests in parallel while respecting the rate limit policy\n\nIn the given example , we can download the Yelp business search result into a pandas Dataframe, using only two lines of code, without looking deeply into the yelp documentation: More examples can be found here :-\n\n![image.png](attachment:fd6b41d8-ada8-4e93-8345-85a47ab73921.png)","b2f6bc5f":"**Notes**:\n- AutoViz will visualize any sized file using a statistically valid sample.\n- COMMA is assumed as default separator in file. But we can change it.\n- Assumes first row as header in file but we can change it.\n\n**Arguments**\n- max_rows_analyzed - limits the max number of rows that is used to display charts\n- max_cols_analyzed - limits the number of continuous vars that can be analyzed\n\n**verbose**\n- if 0, does not print any messages and goes into silent mode. This is the default.\n- if 1, print messages on the terminal and also display charts on terminal.\n- if 2, print messages but will not display charts, it will simply save them.\n","233dbb46":"<a id=\"2\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>2. Introduction <\/b><\/font>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n\nExploratory data analysis (EDA) is an essential step in any research analysis. The purpose of the exploratory data analysis is to convert the available data from their raw form to an informative one , in which the main features of the data are illuminated. It has the following main purposes :-\n\n- EDA aims to examine the data for distributions , outliers and anomalies to direct specific testing of our hypothesis.\n- EDA display and summarize data that are obtained from a sample, by means of visualization and statistics techniques.\n- EDA helps us to find out the natural patterns and the features which are of utmost significance.\n- EDA allows us to prepare the report containing the details of the data insights which can be shared with the business owners and    stakeholders.\n\nThere are certain libraries which are developed to make the data scientists perform EDA must faster and in a much systematic manner. It helps us in dealing with the complex data with much ease.\nIn this Kernel , we are going to see such libraries in details . Let\u2019s get started without wasting mush time","c3db5433":"<a id=\"3.1\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>3.1 Pandas Profiling <\/b><\/font>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n\nThe most commonly used library for getting quick data summaries and correlation analysis. It generates the profile reports for the mentioned dataframe. Usually we use df.describe() function for this but it is not sufficient for in depth exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis.\n\nDepending upon the relevant data type of the column , following details are present in an interactive HTML report :-\n\n- Type inference: detect the types of columns in a dataframe.\n- Essentials: type, unique values, missing values\n- Quantile statistics like minimum value, Q1, median, Q3, maximum, range, interquartile range\n- Descriptive statistics like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness\n- Most frequent values\n- Histogram\n- Correlations highlighting of highly correlated variables, Spearman, Pearson and Kendall matrices\n- Missing values matrix, count, heatmap and dendrogram of missing values\n- Text analysis learn about categories (Uppercase, Space), scripts (Latin, Cyrillic) and blocks (ASCII) of text data.\n- File and Image analysis extract file sizes, creation dates and dimensions and scan for truncated images or those containing EXIF information.\n\nWe can start using the library by first importing the module and then applying it to the existing or created dataset\n\nWe can generate the HTML report using following :-","e820efad":"<a id=\"3.4\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>3.4 SweetVIZ <\/b><\/font>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n\nThis python library helps us in creating visualizations . It is one of the open source library that generates beautiful, high-density visualizations to kick-start EDA with a single line of code. The output is in the form of HTML application.The system is built around quickly visualizing target values and comparing datasets. The goal is to to keep the quick analysis of the of target characteristics, training vs testing data, and other such data characterization tasks.\n\n\n### Features of the swetviz library\n\n- Target analysis:- How target values (boolean or numerical) relate to other features\n- Visualize and compare Distinct datasets (e.g. training vs test data)and Intra-set characteristics (e.g. male versus female)\n- Mixed-type associations: Sweetviz integrates associations for numerical (Pearson\u2019s correlation), categorical (uncertainty coefficient) and categorical-numerical (correlation ratio) datatypes seamlessly, to provide maximum information for all data types.\n- Type inference: automatically detects numerical, categorical and text features, with optional manual overrides\n- Summary information:Type, unique values, missing values, duplicate rows, most frequent values\n- Numerical analysis:min\/max\/range, quartiles, mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness\n\n","0a531f2a":"<a id=\"3.7\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>3.7 TPOT <\/b><\/font>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n\n\nIt is one of the AutoML tool with feature engineering module. TPOT stands for Tree-based Pipeline Optimization Tool.It works more as a Data Scientist assistant. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming.\n\n![image.png](attachment:9eefc590-4ba7-4ec7-b5dc-2322fba4d5f5.png)\n","0f65d9a9":"<a id=\"3.2\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>3.2 dataprep <\/b><\/font>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n\nDataprep helps data scientists in preparing our data using a single library with a few lines of code. Currently we can use dataprep to:\n\n- Collecting data from common data sources (through dataprep.connector)\n- Do the necessary exploratory data analysis (through dataprep.eda)\n\nThis library is still in the building stage , so more modules are coming in future stage :-\n","4a8a0f48":"- Or, if we want to understand the impact of the missing values for each column, use plot_missing.\n","16779fe1":"<a id=\"1\"><\/a>\n<font color=\"darkslateblue\" size=+2.5><b>1. Background <\/b><\/font>\n<a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\">Go to TOC<\/a>\n\nData Scientists nowadays spends lot of time in exploratory data analysis to get the data into the form with which they can work and also to get insights on the hidden patterns which eventually helps in the business decisions. In fact lot of Data scientists argue about the initial steps of obtaining ,cleaning data and performing analysis which constitutes about 80 % of the job.\n\nTherefore , if you are just jumping into the field of data science , it is important for the aspiring data scientists to know the libraries which makes their life easier. Apart from the basic libraries like numpy, pandas , seaborn and matplotlib , there are certain libraries which makes the analysis much faster and easier . This saves a lot of time and help data scientists for vast data exploration."}}