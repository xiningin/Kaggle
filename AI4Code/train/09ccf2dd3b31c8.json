{"cell_type":{"4fdaf390":"code","dcd5dbea":"code","04ce5cf2":"code","467ebbe4":"code","cab8a2ed":"code","2b2e4152":"code","7966fd21":"code","0351a190":"code","e30a1663":"code","21a2f24f":"code","da78f4aa":"code","24629a65":"code","9b8a3bdd":"code","3bae4180":"code","f49f3851":"code","80dfd9e7":"code","8b998129":"code","787805cd":"code","5015fa03":"code","e3226551":"code","51ee75a6":"code","86d2eb1b":"code","b47478f3":"code","046e3c2f":"code","28eb187d":"code","f8592286":"code","78b0ed5e":"code","f89a8789":"code","d4685922":"code","9699e6a6":"code","9a3cba1f":"code","156e7b0a":"code","39f3bb1f":"code","f5d9fc77":"code","71feed66":"code","4252b49b":"markdown","c893564e":"markdown","ca0d0ea6":"markdown","a15b931d":"markdown","aec1edf9":"markdown","4cf89c04":"markdown","e5604f47":"markdown","dfbedd4a":"markdown","66f19d30":"markdown","0b1ce648":"markdown","8600dd9a":"markdown","b258dc5b":"markdown","bf01a64c":"markdown","5a60f059":"markdown"},"source":{"4fdaf390":"# Importing libraries\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport os","dcd5dbea":"# Show dataset names\nfor dirname, _, filenames in os.walk('..\/input\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","04ce5cf2":"# Load datasets\ntraining_data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv')\ntest_output = test_data.copy() #for final output predictions\nprint(training_data.columns)","467ebbe4":"# Print dataset head\ntraining_data.head()","cab8a2ed":"# Understand nature of data (object types and null-values)\ntraining_data.info()","2b2e4152":"# Check labels to see if dataset is balanced\ntraining_data.groupby(['Survived']).size()","7966fd21":"# Understand numeric data\ntraining_data.describe()","0351a190":"# Categorical analysis\ndef bar_plot(variable):\n    var = training_data[variable]\n    varValue = var.value_counts()\n    plt.figure(figsize=(9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))\n    \ncategory = [\"Survived\",\"Pclass\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"]\nfor c in category:\n    bar_plot(c)","e30a1663":"# Numerical analysis\ndef plot_hist(variable):\n    plt.figure(figsize =  (9,3))\n    plt.hist(training_data[variable], bins=50)\n    plt.xlabel(variable)\n    plt.ylabel('Frequency')\n    plt.title(\"{} distribution with hist\".format(variable))\n    plt.show()\n\nnumeric = [\"Age\", \"Fare\", \"SibSp\",\"Parch\"]\nfor n in numeric:\n    plot_hist(n)","21a2f24f":"# Compare survival rate across Age, SibSp, Parch, and Fare (average rate based on survived or not)\npd.pivot_table(training_data, index = 'Survived', values = ['Pclass', 'Age', 'SibSp',\n       'Parch', 'Fare'])","da78f4aa":"#Pclass vs Survived\nprint(training_data[[\"Pclass\",\"Survived\"]].groupby([\"Pclass\"],as_index=False).mean().sort_values(by=\"Survived\",ascending=False))\ntraining_data.groupby(['Pclass','Survived']).size().unstack().plot(kind='bar')","24629a65":"#Sex vs Survived\nprint(training_data[[\"Sex\",\"Survived\"]].groupby([\"Sex\"],as_index=False).mean().sort_values(by=\"Survived\",ascending=False))\ntraining_data.groupby(['Sex','Survived']).size().unstack().plot(kind='bar')","9b8a3bdd":"#Age vs Survived (divide age in intervals)\ntraining_data[\"Age_Bins\"] = pd.cut(training_data[\"Age\"], bins=6, labels=[\"0-17\", \"18-30\", \"30-40\", \"40-50\",\"50-65\", \"over 65\"])\nprint(training_data[[\"Age_Bins\",\"Survived\"]].groupby([\"Age_Bins\"],as_index=False).mean().sort_values(by=\"Survived\",ascending=False))\ntraining_data.groupby(['Age_Bins','Survived']).size().unstack().plot(kind='bar')\ntraining_data = training_data.drop(['Age_Bins'],axis=1)","3bae4180":"#SibSp vs Survived\nprint(training_data[[\"SibSp\",\"Survived\"]].groupby([\"SibSp\"],as_index=False).mean().sort_values(by=\"Survived\",ascending=False))\ntraining_data.groupby(['SibSp','Survived']).size().unstack().plot(kind='bar')","f49f3851":"#Parch vs Survived\nprint(training_data[[\"Parch\",\"Survived\"]].groupby([\"Parch\"],as_index=False).mean().sort_values(by=\"Survived\",ascending=False))\ntraining_data.groupby(['Parch','Survived']).size().unstack().plot(kind='bar')","80dfd9e7":"#Fare vs Survived (divide fare in intervals)\ntraining_data[\"Fare_Bins\"] = pd.cut(training_data[\"Fare\"], bins=5, labels=[\"0-100\", \"101-200\", \"201-300\", \"301-400\",\"401-500\"])\nprint(training_data[[\"Fare_Bins\",\"Survived\"]].groupby([\"Fare_Bins\"],as_index=False).mean().sort_values(by=\"Survived\",ascending=False))\ntraining_data.groupby(['Fare_Bins','Survived']).size().unstack().plot(kind='bar')\ntraining_data = training_data.drop(['Fare_Bins'],axis=1)","8b998129":"# Heatmap for measuring correlation features\nfig,ax = plt.subplots(figsize=(12,4))\nsns.heatmap(training_data[[\"Age\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(), annot = True)\nax.set_title('Heatmap')","787805cd":"training_data.drop(labels = [\"PassengerId\", \"Name\",\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\ntest_data.drop(labels = [\"PassengerId\", \"Name\",\"Cabin\", \"Ticket\"], axis = 1, inplace = True)","5015fa03":"fig, axs = plt.subplots(2, 2, figsize=(10,10))\ntraining_data.boxplot(column=\"Fare\", ax=axs[0,0])\ntraining_data.boxplot(column=\"Age\", ax=axs[0,1])\ntraining_data.boxplot(column=\"SibSp\", ax=axs[1,0])\ntraining_data.boxplot(column=\"Parch\", ax=axs[1,1])\nplt.show()","e3226551":"from collections import Counter\ndef detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        #1st quartile\n        Q1 = np.percentile(df[c],25)\n        #3st quartile\n        Q3 = np.percentile(df[c],75)\n        #IQR\n        IQR=Q3-Q1\n        #Outlier step\n        outlier_step = IQR * 1.5\n        #detect outlier and their indices\n        outlier_list_col=df[(df[c]< Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        #store_indices\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i,v in outlier_indices.items() if v>2)\n    \n    return multiple_outliers","51ee75a6":"training_data.loc[detect_outliers(training_data,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])]","86d2eb1b":"# Drop outliers\ntraining_data=training_data.drop(detect_outliers(training_data,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"]),axis=0).reset_index(drop=True)","b47478f3":"training_data.isnull().sum()","046e3c2f":"test_data.isnull().sum()","28eb187d":"# Fill numerical data for training set (using median)\ntraining_data[\"Age\"] = training_data[\"Age\"].fillna(training_data[\"Age\"].median())\n#Remove Embarked missing data since there are only 2\ntraining_data.dropna(subset=['Embarked'],inplace = True)","f8592286":"# Fill numerical data for test set (using median)\ntest_data[\"Age\"] = test_data[\"Age\"].fillna(test_data[\"Age\"].median())\ntest_data[\"Fare\"] = test_data[\"Fare\"].fillna(test_data[\"Fare\"].median())","78b0ed5e":"training_data.isnull().sum()","f89a8789":"test_data.isnull().sum()","d4685922":"training_data = pd.get_dummies(training_data, columns= [\"Pclass\", \"Sex\", \"Embarked\"])\ntest_data = pd.get_dummies(test_data, columns= [\"Pclass\", \"Sex\", \"Embarked\"])\ntraining_data.head()","9699e6a6":"print(training_data.shape)","9a3cba1f":"# Standardization scaling\nfrom sklearn.preprocessing import StandardScaler\ny = training_data['Survived']\nX = training_data.drop(['Survived'],axis=1)\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\ntest_data = scaler.fit_transform(test_data)","156e7b0a":"# Split dataset in training and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=42)\nprint(\"X_train size: \",len(X_train))\nprint(\"X_test size: \",len(X_test))\nprint(\"y_train size: \",len(y_train))\nprint(\"y_test size: \",len(y_test))","39f3bb1f":"# Measure the best accuracy between different classifiers and the time taken\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\nimport time\n\nmodels = []\nresults = []\nnames= []\nscoring = 'accuracy'\n\nmodels.append(('LR',LogisticRegression(max_iter=2000)))\nmodels.append(('LDA',LinearDiscriminantAnalysis()))\nmodels.append(('KNN',KNeighborsClassifier()))\nmodels.append(('DTC',DecisionTreeClassifier()))\nmodels.append(('GNB',GaussianNB()))\nmodels.append(('SVC',SVC())) # Slow\nmodels.append(('RFC',RandomForestClassifier(random_state = 1)))\nmodels.append(('XGB',XGBClassifier(random_state =1, eval_metric='logloss')))\n# View the sklearn documentation for each model's hyperparameters\n            \nfor name,model in models:\n    t = time.process_time()\n    cv_results = cross_val_score(model,X,y,cv=5,scoring=scoring)\n    t = time.process_time() - t\n    results.append(cv_results)\n    names.append(name)\n    msg=\"%s -> Accuracy: %f(%f), Time: %f seconds\"%(name,cv_results.mean(),cv_results.std(),t)\n    print(msg)\n\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","f5d9fc77":"# Grid Search Parameter Tuning\nfrom sklearn.model_selection import GridSearchCV\n\nnames = []\nmodels = []\nparams = []\nbest_results = []\nbest_params = []\nscoring = 'accuracy'\n\n# Logistic Regression\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\nmodels.append(('LR',LogisticRegression(), param_grid))\n\n# KNN Classifier\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\nmodels.append(('KNN',KNeighborsClassifier(), param_grid))\n\n# SVC\nparam_grid = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10], 'C': [.1, 1, 10, 100, 1000]}]\n              #{'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]}]\n              #{'kernel': ['poly'], 'degree' : [2,3], 'C': [.1, 1, 10, 100, 1000]}]\nmodels.append(('SVC',SVC(), param_grid))\n\n# Randon Forest Classifier\nparam_grid =  {'n_estimators': [400,450,500,550],\n               'random_state' : [1],\n               'criterion':['gini','entropy'],\n               'bootstrap': [True],\n               'max_depth': [15, 20, 25],\n               'max_features': ['auto','sqrt', 10],\n               'min_samples_leaf': [2,3],\n               'min_samples_split': [2,3]}\nmodels.append(('RFC',RandomForestClassifier(), param_grid))\n\n# XGBoost\nparam_grid = {\n    'n_estimators': [450,500,550],\n    'random_state' : [1],\n    'eval_metric' : ['logloss'],\n    'colsample_bytree': [0.75,0.8,0.85],\n    'max_depth': [None],\n    'reg_alpha': [1],\n    'reg_lambda': [2, 5, 10],\n    'subsample': [0.55, 0.6, .65],\n    'learning_rate':[0.5],\n    'gamma':[.5,1,2],\n    'min_child_weight':[0.01],\n    'sampling_method': ['uniform']\n}\nmodels.append(('XGB',XGBClassifier(), param_grid))\n\n\nfor name, model, param_grid in models:\n    t = time.process_time()\n    grid = GridSearchCV(estimator=model,param_grid=param_grid,cv=5)\n    grid.fit(X,y)\n    result = grid.best_score_\n    params = str(grid.best_params_)\n    t = time.process_time() - t\n    best_results.append(result)\n    best_params.append(params)\n    names.append(name)\n    msg=\"%s -> Best result: %f, Best params: %s, Time: %f seconds\"%(name,result,params,t)\n    print(msg)\n\n\nfig, ax = plt.subplots()\nnames_list = np.arange(len(names))\nax.barh(names_list, best_results, align='center')\nax.set_yticks(names_list)\nax.set_yticklabels(names)\nax.invert_yaxis()\nax.set_xlim([0.7, 1])\nax.set_xlabel('Accuracy')\nax.set_title('Tuned algorithm comparison')\nplt.show()","71feed66":"final_model = XGBClassifier(colsample_bytree=0.85, eval_metric='logloss', gamma=1, learning_rate=0.5, \n                            max_depth=None, min_child_weight=0.01, n_estimators=450, random_state=1, \n                            reg_alpha=1, reg_lambda=5, sampling_method='uniform', subsample=0.65)\nfinal_model.fit(X_train, y_train)\nsubmission_output = final_model.predict(test_data)\nfinal_data = {'PassengerId': test_output.PassengerId, 'Survived': submission_output}\nsubmission = pd.DataFrame(data=final_data)\nsubmission.to_csv(\"titanic.csv\", index = False)","4252b49b":"# 2) Feature Engineering","c893564e":"## 2.1) Drop irrelevant columns","ca0d0ea6":"# 5) Final predictions","a15b931d":"# 1) Data understanding","aec1edf9":"## 2.4) Convert categorical data to numeric","4cf89c04":"# Titanic Notebook\n\nThe goal of this notebook is to predict if someone is survived on the Titanic shipwreck given info on each passenger","e5604f47":"## 2.3) Inputing missing values","dfbedd4a":"## 1.1) Univariate Variable Analysis\n\n<b>Categorical Variable:<\/b> Survived,Sex,Pclass, Embarked, Cabin, Name, Ticket\n\n<b>Numerical Variable:<\/b> Age, Fare, Sibsp and Parch\n","66f19d30":"# 3) Modeling","0b1ce648":"# 4) Hyperparameter tuning","8600dd9a":"# Contents\n\n## 1) Data understanding\n### 1.1) Univariate Variable Analysis\n### 1.2) Multivariate Variable Analysis\n## 2) Feature Engineering\n### 2.1) Drop irrelevant column\n### 2.2) Outlier Detection\n### 2.3) Inputing missing values\n### 2.4) Convert categorical data into numeric\n### 2.5) Data preprocessing\n## 3) Modeling\n## 4) Hyperparameter tuning\n## 5) Final predictions","b258dc5b":"## 2.5) Data preprocessing","bf01a64c":"## 2.2) Outlier detection","5a60f059":"## 1.2) Multivariate Variable Analysis"}}