{"cell_type":{"c70b944a":"code","46bbf9ba":"code","ef00bce2":"code","abfb4740":"code","47fcfb33":"code","92c2c4f0":"code","0bfa4522":"code","b6cbaddd":"code","563b10b5":"code","7e86a582":"markdown","98057dcb":"markdown","9d30ab63":"markdown","29ad5051":"markdown","ff3ae930":"markdown","3b65c69b":"markdown","da11cc6f":"markdown","33ddf3f5":"markdown","c894891e":"markdown","7201c665":"markdown"},"source":{"c70b944a":"# Imports\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport math\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import NearestCentroid\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.svm import NuSVC\nfrom sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import ComplementNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import CategoricalNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nfrom sklearn.metrics import precision_score\nfrom sklearn.ensemble import VotingClassifier\nimport pickle\nimport csv","46bbf9ba":"# Get the data\ntrainFile = \"..\/input\/titanic\/train.csv\"\ndataset = pd.read_csv(trainFile)\nprint(dataset.shape)","ef00bce2":"# Cleaning\n#   -- There is a lot of AGE missing\n# Remove rows where both cabin and age are null\n# Rows where only cabin is NaN are ignored since we do not use cabin\nnull_columns=dataset.columns[dataset.isnull().any()]\nageIsNull = dataset[dataset[\"Age\"].isnull()][\"PassengerId\"]\ncabinIsNull = dataset[dataset[\"Cabin\"].isnull()][\"PassengerId\"]\nage_cabin_isnull = pd.Series(list(set(ageIsNull) & set(cabinIsNull)))\n\nfor index,value in age_cabin_isnull.items():\n    indexVal = dataset[ dataset['PassengerId'] == value ].index\n    dataset.drop(indexVal , inplace=True)\n\n# Fill in age with the mean (Could research a better method)\nageIsNull = dataset[dataset[\"Age\"].isnull()][\"PassengerId\"]\ndataset.fillna(dataset.mean(), inplace=True)\n#print(dataset.isnull().sum())\n","abfb4740":"# 1) Understand the data - Using descriptive stats and visulization\n# Clean up and modification done after first viewing the data\ndataset = dataset.drop_duplicates()\ndataset = dataset.drop([\"Name\", \"Ticket\", \"Cabin\", \"Embarked\"], axis=1)\ndataset[\"Sex\"] = dataset[\"Sex\"].replace([\"male\"],1)\ndataset[\"Sex\"] = dataset[\"Sex\"].replace([\"female\"],0)\n\n# To know what method I need to use to find outliers, I need to know the distribution\narrayOfMean_Variance = []\nfor index,value in dataset.var(axis=0).items():\n    for index2,value2 in dataset.mean(axis=0).items():\n        if(index==index2):\n            arrayOfMean_Variance.append((index, value, value2))\n\n# Draw the distribution for each\nfor tuple in arrayOfMean_Variance:\n    mu = tuple[2]\n    variance = tuple[1]\n    sigma = math.sqrt(variance)\n    x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n    plt.plot(x, stats.norm.pdf(x, mu, sigma))\n    plt.title(tuple[0])\n    #plt.show(block=True)\n\n# From the graphs drawn I can see that all the values have a normal distribution.\n# This means I can use the standard deviation method to remove outliers\nfor tuple in arrayOfMean_Variance:\n    # calculate summary statistics\n    data_std = dataset.std(axis=0)[tuple[0]]\n    # identify outliers\n    cut_off = data_std * 3\n    lower, upper = tuple[2] - cut_off, tuple[2] + cut_off\n    dataset = dataset[dataset[tuple[0]] > lower]\n    dataset = dataset[dataset[tuple[0]] < upper]\n\n# Descriptive stats\nprint(dataset.shape)\nprint(dataset.head())\nprint(dataset.describe())\nprint(dataset.dtypes)\nprint(\" \")\n\n# Visulise data\nhist = dataset.plot.hist()\nboxplot = dataset.plot(kind=\"box\")\n#plt.show(block=False)\n\n# Basic data preperation\n# -- Id needs to be removed\n# -- Name needs to be removed\n# -- Convert sex to 1 or 0\n# -- Ticket number needs to be removed\n# -- Cabin to be removed (although could prove useful later)\n# -- Embarked to be removed (although could prove useful later)\n# -- Outliers need to be removed\n","47fcfb33":"# 2) Pre-processing data\n# Assumptions (Add after initial models to improve performance)\n#   -- Age and sex will have a big influence on the model\n#   -- Ticket class will have a low but still significant impact\n#   -- Some have a FARE of 0, we could use a mean of a class to setimate the real fare it would cost\n# Data not avaliable but could be useful\n#   -- None (more research on topic to find some)\n# Transformation\n#   -- Normalise\nscaler = preprocessing.MinMaxScaler(feature_range=(0,1))\nscaler.fit(dataset)\nnames = [\"PassengerId\",\"Survived\",\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\"]\ndataset = pd.DataFrame(data=scaler.transform(dataset), columns=names)\n\n#   -- Standardise (May have made a mess!!!!) SHOWS UP WITH ERROR\n#scaler = preprocessing.StandardScaler()\n#scaler.fit(dataset)\n#dataset = pd.DataFrame(data=scaler.transform(dataset), columns=names)\n\n# Split data into test and train\nX_train, X_test, y_train, y_test = train_test_split(dataset.drop(['Survived'],axis='columns'),dataset[\"Survived\"],test_size=0.2)\n#print(X_test)","92c2c4f0":"# 3) Spot checking algorithms\nmodels = []\nmodels.append((\"LR\", LogisticRegression()))\nmodels.append((\"SVC\", SVC()))\nmodels.append((\"LDA\", LinearDiscriminantAnalysis()))\nmodels.append((\"KNC\", KNeighborsClassifier()))\nmodels.append((\"DTC\", DecisionTreeClassifier()))\nmodels.append((\"GNB\", GaussianNB()))\nmodels.append((\"LSVC\", LinearSVC()))\nmodels.append((\"SGDC\", SGDClassifier()))\nmodels.append((\"NC\", NearestCentroid()))\nmodels.append((\"GPC\", GaussianProcessClassifier()))\nmodels.append((\"PAC\", PassiveAggressiveClassifier()))\nmodels.append((\"Perc\", Perceptron()))\nmodels.append((\"Ridge\", RidgeClassifier()))\nmodels.append((\"NuSVC\", NuSVC()))\nmodels.append((\"RNC\", RadiusNeighborsClassifier()))\nmodels.append((\"MNB\", MultinomialNB()))\nmodels.append((\"CNB\", ComplementNB()))\nmodels.append((\"CatNB\", CategoricalNB()))\nmodels.append((\"BNB\", BernoulliNB()))\n\nresults = []\nnames = []\ncombResults = []\nscoring = \"accuracy\"\nfor name,model in models:\n    model.fit(X_train, y_train)\n    model_results = model.predict(X_test)\n    score = precision_score(y_test, model_results, average='macro')\n    results.append(score)\n    names.append(name)\n    combResults.append((name,score))\n\n# Pick top 4 models to run parameter tunning on\ncombResults = sorted(combResults, key=lambda tup: tup[1])\n#for i in range(len(combResults)-4, len(combResults)):\n    #print(\"%s: %f\" % (combResults[i][0], combResults[i][1]))\n","0bfa4522":"# 4) Parameter tunning\ncv = RepeatedStratifiedKFold(n_splits=4, n_repeats=2, random_state=1)\n\n# PassiveAggressiveClassifier\nmodel = PassiveAggressiveClassifier()\nC = range(1, 10)\n# -- fit_intercept = false if data is centered\nmax_iter = range(500, 2500, 100)\nearly_stopping = [True, False]\naverage = [True,False]\ngrid = dict(C=C,max_iter=max_iter,early_stopping=early_stopping,average=average)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0,verbose=2)\ngrid_result = grid_search.fit(X_train, y_train)\nmodel1_params = grid_result.best_params_\n\n#SVC\nmodel = SVC()\nC = range(1, 10)\nkernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\ndegree = range(1,10)\nshrinking = [True,False]\nprobability = [True,False]\ndecision_function_shape = [\"ovo\", \"ovr\"]\ngrid = dict(C=C,kernel=kernel,degree=degree,shrinking=shrinking,probability=probability,decision_function_shape=decision_function_shape)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0,verbose=2)\ngrid_result = grid_search.fit(X_train, y_train)\nmodel2_params = grid_result.best_params_\n\n#GPC\nmodel = GaussianProcessClassifier()\nn_restarts_optimizer = range(0,5)\nrandom_state = range(1,5)\nmulti_class = [\"one_vs_rest\",\"one_vs_one\"]\n#n_jobs = -1 Could spped up\ngrid = dict(n_restarts_optimizer=n_restarts_optimizer,random_state=random_state,multi_class=multi_class)\ngrid_search = GridSearchCV(estimator=model,param_grid=grid,n_jobs=-1,cv=cv,scoring=\"accuracy\",error_score=0,verbose=2)\ngrid_result = grid_search.fit(X_train,y_train)\nmodel3_params = grid_result.best_params_\n\n#SGDC\nmodel = SGDClassifier()\nloss = [ \"hinge\", \"log\", \"modified_huber\", \"squared_hinge\", \"perceptron\", \"squared_loss\", \"huber\", \"epsilon_insensitive\", \"squared_epsilon_insensitive\"]\npenalty = [\"l1\", \"elasticnet\"]\nfit_intercept = [False] # Already centered data\nshuffle = [True]\nlearning_rate = [\"constant\", \"optimal\", \"invscaling\", \"adaptive\"]\neta0 = np.arange(0.1,0.9,0.1)\ngrid = dict(loss=loss, penalty=penalty, fit_intercept=fit_intercept, shuffle=shuffle, learning_rate=learning_rate, eta0=eta0)\ngrid_search = GridSearchCV(estimator=model,param_grid=grid,n_jobs=-1,cv=cv,scoring=\"accuracy\",error_score=0,verbose=2)\ngrid_result = grid_search.fit(X_train,y_train)\nmodel4_params = grid_result.best_params_","b6cbaddd":"# 5) Improve results using ensemble methods\nfinal_model1 = PassiveAggressiveClassifier(**model1_params)\nfinal_model2 = SVC(**model2_params)\nfinal_model3 = GaussianProcessClassifier(**model3_params)\nfinal_model4 = SGDClassifier(**model4_params)\nFINAL_MODEL = VotingClassifier(estimators=[('PAC', final_model1), ('SVC', final_model2), (\"GPC\", final_model3), (\"SGDC\", final_model4)],\n                         voting='hard', weights=[1,1,1,1])\nFINAL_MODEL.fit(X_train, y_train)\nmodel_results = FINAL_MODEL.predict(X_test)\n#print(model_results)\nscore = precision_score(y_test, model_results, average='macro')\nprint(score)","563b10b5":"def prepareData(dataset, isTest):\n    # Cleaning\n    # Use \"honor\" title, where there is none set to 0\n    # Find out what titles there are\n    titleSet = dataset[\"Name\"].map(lambda x: (re.findall(', [A-Za-z]*.', x))[0].replace(\".\",\"\").replace(\",\",\"\").replace(\" \", \"\"))\n\n    # mr -> 1 , miss -> 2,  mrs -> 3, master -> 5, dr -> 4, Major -> 6\n    titleSet = titleSet.replace([\"Mr\"],1)\n    titleSet = titleSet.replace([\"Miss\"],2)\n    titleSet = titleSet.replace([\"Mrs\"],3)\n    titleSet = titleSet.replace([\"Master\"],5)\n    titleSet = titleSet.replace([\"Dr\"],4)\n    titleSet = titleSet.replace([\"Major\"],6)\n    titleSet = titleSet.replace(to_replace=\"[A-Ba-b]*\", value=0, regex=True)\n    dataset[\"Name\"] = titleSet\n\n    # Make all values numerical\n    # Sex\n    dataset[\"Sex\"] = dataset[\"Sex\"].replace([\"male\"],1)\n    dataset[\"Sex\"] = dataset[\"Sex\"].replace([\"female\"],0)\n\n    # From the graphs drawn I can see that all the values have a normal distribution.\n    # This means I can use the standard deviation method to remove outliers\n    arrayOfMean_Variance = []\n    for index,value in dataset.var(axis=0).items():\n        for index2,value2 in dataset.mean(axis=0).items():\n            if(index==index2):\n                arrayOfMean_Variance.append((index, value, value2))\n    if(not isTest):\n        for tuple in arrayOfMean_Variance:\n            # calculate summary statistics\n            data_std = dataset.std(axis=0)[tuple[0]]\n            # identify outliers\n            cut_off = data_std * 3\n            lower, upper = tuple[2] - cut_off, tuple[2] + cut_off\n            dataset = dataset[dataset[tuple[0]] > lower]\n            dataset = dataset[dataset[tuple[0]] < upper]\n\n    # Fill in misssing values (age and fare -> mean)\n    dataset.fillna(dataset.mean(), inplace=True)\n    dataset[\"Embarked\"].fillna(\"S\", inplace=True)\n    dataset[\"Cabin\"].fillna(1, inplace=True)\n\n    # Since there are multiple people in a cabin, a person with\n    # more people in the cabin could have a higher survival chance\n    valueCounts = dataset['Cabin'].value_counts()\n    new_set = dataset['Cabin']\n    for cabin in dataset['Cabin']:\n        if cabin != 1:\n            value = valueCounts[cabin]\n            new_set = new_set.replace([cabin], value)\n    dataset['Cabin'] = new_set\n\n    # Convert embarked to numbers\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].replace([\"S\"], 3)\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].replace([\"C\"], 2)\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].replace([\"Q\"], 1)\n\n    # Remove duplicates and unused features\n    if(not isTest):\n        dataset = dataset.drop_duplicates()\n        dataset = dataset.drop([\"PassengerId\",\"Ticket\"], axis=1)\n    else:\n        dataset = dataset.drop([\"Ticket\"], axis=1)\n\n    # 2) Pre-processing data\n    # Normalise\n    scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n    scaler.fit(dataset)\n    if (isTest):\n        return dataset\n    else:\n        names = [\"Survived\",\"Pclass\",\"Name\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"Cabin\",\"Embarked\"]\n        dataset = pd.DataFrame(data=scaler.transform(dataset), columns=names)\n    return dataset","7e86a582":"# Better data prep function\nThis function takes a dataset, and a boolean saying if this is test or train data. Then the function returns the prepared dataset.","98057dcb":"# Data Clean Up\nThere were alot of missing age and cabin data. \nI removed all rows in which both age and cabin were missing.\nRows where only cabin were missing are ignored since I dont use cabin for trainning the model.\nWhere just age is missing, I filled in this data using the age mean.","9d30ab63":"# Parameter Tunning\nFor each of my top 4 models I chose a few parameters which I think will have the biggest affect on the score and then used a grid search to find the best combinations.\nThis code takes a **very long** time to execute.","29ad5051":"# Improving results and final test\nFirstly I made each model with the best fitted parameters.\nThen I used voting to enseble the models and come out with a final model.","ff3ae930":"# Conclusion\nAs I mentioned at the start,this is my first ML project so any advice would be a great help.\nI wanted to note that I will dive deeper into data preperation and feature selection as that (in my oppinion) is my weakest area.","3b65c69b":"# Introduction\nThis is my first ML project, below is my code and the best explanation I can give for it. Any feedback on this project is greatly appreciated.\n# Imports","da11cc6f":"# Import Dataset","33ddf3f5":"# Pre-processing\nFirstly I wrote down a few assumptions about the data.\nThen I go ahead and normalise the data.\nNext I tried to standardise the data however I recived a lot of errors and couldnt figure it out.\nAnd lastly I split the dataset into taining and testing. ","c894891e":"# Spot-checking\nFirstly I made an array of all the models to check.\nAnd then I ran a loop to get a precision score from each model and pick the top four performing models.\nTo pick the top 4 models I had to run the loop multiple times and note down the frequincies of how often each algorithm appears in the top four, this was all done manualy but will be automated.","7201c665":"# Understanding Data\nThis is one of the harder sections for me to understand. \nThe first block of code is the data modifications I chose to do after looking at some descriptive statistics. (Remove duplicates, remove unwanted features and assign numbers to the sex)\n\nNext two blocks of code help me find out the distribution of the data by drawing graphs. I discovered it had a normal distribution which is why I chose the standard deviation method for removing outliers. (Code block 4)\n\nThe rest of the code prints some stats and draws a few graphs."}}