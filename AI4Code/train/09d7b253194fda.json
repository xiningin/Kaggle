{"cell_type":{"24cabf95":"code","e99dd24b":"code","da4b6831":"code","a1d98081":"code","5512325f":"code","130b98ad":"code","1728e1b0":"code","0153f09f":"code","e746f0ea":"code","e86d7486":"code","2ca01bca":"code","a360c518":"code","1da02eed":"code","1a96a0db":"code","5f2152e4":"code","96c2dba7":"code","506f214c":"code","757d833c":"code","1393617f":"code","696760b2":"code","6ba92ed6":"code","8fac5944":"code","72de190c":"code","712bf758":"code","1ad78e4e":"code","cdf69188":"code","296a6e55":"code","c211e06f":"code","297198ba":"code","cdc6c5b0":"code","cdac1f36":"code","5700c147":"code","4071b097":"code","094a5907":"code","4d9e31b6":"code","2ecd9650":"markdown"},"source":{"24cabf95":"!pip install torchnet","e99dd24b":"import pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torchnet.meter.confusionmeter as cm","da4b6831":"train_image_dir = '..\/input\/train\/'\ntest_image_dir = '..\/input\/test\/'\n\n# Loading csv files\ntrain_data = pd.read_csv('..\/input\/train.csv')\n#train_data.head()","a1d98081":"print('No of classes : {}\\nClasses : {}'.format(len(train_data['target'].unique()),train_data['target'].unique()))","5512325f":"train_image_names = os.listdir(train_image_dir)\nprint(len(train_image_names))\nprint(train_data.shape[0])\n\ntrain_image_names = set(train_image_names)\ninfo_images = set(train_data['Image'])\nno_info_images = train_image_names - info_images\nprint(no_info_images)","130b98ad":"train_data['Image'].apply(lambda t : t.split('.')[-1]).unique()","1728e1b0":"class_names = ['manipuri', 'bharatanatyam', 'odissi', 'kathakali', 'kathak', 'sattriya', 'kuchipudi', 'mohiniyattam']","0153f09f":"class_encoding = {\n    'manipuri':0,\n    'bharatanatyam':1,\n    'odissi':2,\n    'kathakali':3,\n    'kathak':4,\n    'sattriya':5,\n    'kuchipudi':6,\n    'mohiniyattam':7,}\n\nget_labels = {v: k for k, v in class_encoding.items()}\n\ntrain_data['target'] = train_data['target'].map(class_encoding)\ntrain_data['Image'] = train_data['Image'].apply(lambda t : os.path.join(train_image_dir, t))\ntrain_data.head()","e746f0ea":"sns.countplot(x = 'target', data = train_data)","e86d7486":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms, utils, models\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.optim as optim\nimport time\nimport copy\nfrom torch.optim import lr_scheduler\nfrom PIL import Image","2ca01bca":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","a360c518":"class Dance (Dataset) :\n    def __init__(self, dataframe, image_dir, transformations = None) :\n        self.dataframe = dataframe\n        self.image_dir = image_dir\n        self.transforms = transformations\n        self.to_tensor = transforms.ToTensor()\n        # Modify image names to contain path\n        \n    def __getitem__(self, idx) :\n        image_path = self.dataframe['Image'][idx]\n        image_class = self.dataframe['target'][idx]\n        #print(image_path)\n        image = Image.open(image_path)\n        # apply transformations\n        if self.transforms is not None :\n            image = self.transforms(image)\n        #image = self.to_tensor(image)\n        return (image, image_class)\n    \n    def __len__(self) :\n        return self.dataframe.shape[0]\n    \nclass Dance_test(Dataset) :\n    def __init__(self, image_dir, transformations = None) :\n        self.image_dir = image_dir\n        self.images = os.listdir(self.image_dir)\n        self.images_with_paths = [os.path.join(self.image_dir, x) for x in os.listdir(self.image_dir)]\n        self.transforms = transformations\n        self.to_tensor = transforms.ToTensor()\n        # Modify image names to contain path\n        \n    def __getitem__(self, idx) :\n        image_name = self.images[idx]\n        image_path = self.images_with_paths[idx]\n        #print(image_path)\n        image = Image.open(image_path)\n        # apply transformations\n        if self.transforms is not None :\n            image = self.transforms(image)\n        #image = self.to_tensor(image)\n        return image_name, image\n    \n    def __len__(self) :\n        return len(self.images)","1da02eed":"data_transforms = {'train': transforms.Compose([transforms.Resize ((256, 256)),\n                                      transforms.RandomHorizontalFlip(),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                          std=[0.229, 0.224, 0.225]),]),\n                   'val' : transforms.Compose([transforms.Resize ((256, 256)),\n                                      transforms.RandomHorizontalFlip(),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                          std=[0.229, 0.224, 0.225]),]),\n                   'test' : transforms.Compose([transforms.Resize ((256, 256)),\n                                      transforms.CenterCrop(256),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                          std=[0.229, 0.224, 0.225]),])}\n\ntest_transforms = transforms.Compose([transforms.Resize ((256, 256)),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                                          std=[0.229, 0.224, 0.225]),])\n\n\ntrain_images = Dance(train_data, train_image_dir, transformations = data_transforms['train'])\ntest_images = Dance_test(test_image_dir, transformations = test_transforms)","1a96a0db":"ex_image, ex_label = train_images[0]\nprint(ex_image.shape)\nprint(type(ex_image))\nprint(ex_label)","5f2152e4":"def get_train_valid_splits(dataset, batch_size = 4, validation_split = .3, shuffle_dataset = True, random_seed= 23, test = False) :\n    # Creating data indices for training and validation splits:\n    dataset_size = len(dataset)\n    indices = list(range(dataset_size))\n    split = int(np.floor(validation_split * dataset_size))\n    if shuffle_dataset :\n        np.random.seed(random_seed)\n        np.random.shuffle(indices)\n    train_indices, val_indices = indices[split:], indices[:split]\n    \n    if test :\n        dataset_size = split\n        indices = list(range(dataset_size))\n        split = int(np.floor(validation_split * dataset_size))\n        if shuffle_dataset :\n            np.random.seed(random_seed)\n            np.random.shuffle(indices)\n        val_indices, test_indices = indices[split:], indices[:split]\n        train_sampler = SubsetRandomSampler(train_indices)\n        valid_sampler  = SubsetRandomSampler(val_indices)\n        test_sampler  = SubsetRandomSampler(test_indices)\n        return train_sampler, valid_sampler, test_sampler\n\n    # Creating PT data samplers and loaders:\n    train_sampler = SubsetRandomSampler(train_indices)\n    valid_sampler  = SubsetRandomSampler(val_indices)\n    \n    return train_sampler, valid_sampler \n\n#train_sampler, valid_sampler  = get_train_valid_splits(train_images)\ntrain_sampler, valid_sampler, test_sampler  = get_train_valid_splits(train_images, test = True)","96c2dba7":"batch_size = 4\ntrain_image_loader = DataLoader(dataset = train_images,\n                                sampler = train_sampler,\n                                batch_size = batch_size,\n                               num_workers = batch_size)\n    \nvalid_image_loader = DataLoader(dataset = train_images,\n                               sampler = valid_sampler ,\n                                batch_size = batch_size,\n                               num_workers = batch_size)\n\ntest_image_loader = DataLoader(dataset = train_images,\n                               sampler = test_sampler ,\n                                batch_size = batch_size,\n                              num_workers = batch_size)","506f214c":"test_loader = DataLoader(dataset = test_images,\n                        batch_size = batch_size,\n                        num_workers = batch_size)","757d833c":"print('Final Test Images:', len(test_loader) * batch_size)","1393617f":"print('Train Images : ', len(train_image_loader) * batch_size)\nprint('Validation Images  ', len(valid_image_loader) * batch_size)","696760b2":"dataloaders = {'train' : train_image_loader, 'val' : valid_image_loader}","6ba92ed6":"def plot_images(train_image_loader, padding= 8, pad_value = 255) :\n    for batch_idx, (images, labels) in enumerate(train_image_loader):\n        print('Image Dtype:',type(images))\n        print('Image Batch Shape:',images.shape)\n        #print('labels Batch Shape:',labels.shape)\n        classes = [get_labels[x] for x in np.array(labels)]\n        print('Labels: ', classes)\n\n        # We do single_batch[0] because each batch is a list \n        # where the 0th index is the image tensor and 1st index is the\n        # output label.\n        single_batch_grid = utils.make_grid(images, nrow=4, normalize=True, padding= padding, pad_value = pad_value)\n        plt.figure(figsize = (15,15))\n        plt.imshow(single_batch_grid.permute(1, 2, 0))\n        break\n        \nplot_images(train_image_loader)","8fac5944":"classifier = models.resnet18(pretrained=True)\nnum_ftrs = classifier.fc.in_features\nclassifier.fc = nn.Linear(num_ftrs, 8)\n\n#for VGG16_BN\n#model_ft = models.vgg16_bn(pretrained=True)\n#model_ft.classifier[6].out_features = 8\n\nclassifier = classifier.to(device)","72de190c":"#dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\nepoch_counter_train = []\nepoch_counter_val = []\ntrain_loss = []\nval_loss = []\ntrain_acc = []\nval_acc = []","712bf758":"\ndef train_model(model, criterion, optimizer, scheduler, data_loader, num_epochs = 3) :\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    for epoch in range(num_epochs) :\n        print('Epoch {}\/{}'.format(epoch +1, num_epochs))\n        print('-' * 10)\n        # each model has training and validation phase\n        for phase in ['train','val'] :\n            if phase == 'train' :\n                scheduler.step()\n                model.train() # Set model to training mode\n            else :\n                model.eval() # Set model to evaluation mode\n            running_loss = 0.0\n            running_corrects = 0\n            \n            #Iterate over data\n            d_size = 0\n            for batch, (inputs, labels) in enumerate(dataloaders[phase]) :\n                d_size += 1\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                # zero previous gradients\n                optimizer.zero_grad()\n                \n                # Forward Pass\n                # Pass history in train phase\n                with torch.set_grad_enabled(phase == 'train') :\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n                    \n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n                \n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n        \n        \n            #For graph generation\n            if phase == \"train\":\n                train_loss.append(running_loss\/(d_size * batch_size))\n                train_acc.append(running_corrects.double() \/ (d_size * batch_size))\n                epoch_counter_train.append(epoch)\n            if phase == \"val\":\n                val_loss.append(running_loss\/ (d_size * batch_size))\n                val_acc.append(running_corrects.double() \/ (d_size * batch_size))\n                epoch_counter_val.append(epoch)\n\n            epoch_loss = running_loss \/ (d_size * batch_size)\n            epoch_acc = running_corrects.double() \/ (d_size * batch_size)\n\n            #for printing        \n            if phase == \"train\":    \n                epoch_loss = running_loss \/ (d_size * batch_size)\n                epoch_acc = running_corrects.double() \/ (d_size * batch_size)\n            if phase == \"val\":    \n                epoch_loss = running_loss \/ (d_size * batch_size)\n                epoch_acc = running_corrects.double() \/ (d_size * batch_size)\n\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the best model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","1ad78e4e":"criterion = nn.CrossEntropyLoss()\n\n# Using Adam as the parameter optimizer\noptimizer_ft = optim.Adam(classifier.parameters(), lr = 0.0001, betas=(0.9, 0.999))\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.1)       \n\n\nclassifier = train_model(classifier, criterion, optimizer_ft, exp_lr_scheduler, train_image_loader, num_epochs= 10)        \n","cdf69188":"#Plot the train & validation losses\nplt.figure(1)\nplt.title(\"Training Vs Validation Losses\")\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.plot(epoch_counter_train,train_loss,color = 'r', label=\"Training Loss\")\nplt.plot(epoch_counter_val,val_loss,color = 'g', label=\"Validation Loss\")\nplt.legend()\nplt.show()\n","296a6e55":"#Plot the accuracies in train & validation\nplt.figure(2)\nplt.title(\"Training Vs Validation Accuracies\")\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.plot(epoch_counter_train,train_acc,color = 'r', label=\"Training Accuracy\")\nplt.plot(epoch_counter_val,val_acc,color = 'g', label=\"Validation Accuracy\")\nplt.legend()\nplt.show()","c211e06f":"#Test the accuracy with test data\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for i, (inputs, labels) in enumerate(test_image_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = classifier(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the test images: %d %%' % (\n    100 * correct \/ total))","297198ba":"#Class wise testing accuracy\nclass_correct = list(0. for i in range(8))\nclass_total = list(0. for i in range(8))\nwith torch.no_grad():\n    for i, (inputs, labels) in enumerate(test_image_loader):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            outputs = classifier(inputs)\n            _, predicted = torch.max(outputs, 1)\n            point = (predicted == labels).squeeze()\n            for j in range(len(labels)):\n                label = labels[j]\n                class_correct[label] += point[j].item()\n                class_total[label] += 1","cdc6c5b0":"#Get the confusion matrix for testing data\nconfusion_matrix = cm.ConfusionMeter(8)\nwith torch.no_grad():\n    for i, (inputs, labels) in enumerate(test_image_loader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        outputs = classifier(inputs)\n        _, predicted = torch.max(outputs, 1)\n        confusion_matrix.add(predicted, labels)\n    #print(confusion_matrix.conf)\n\n#Confusion matrix as a heatmap\ncon_m = confusion_matrix.conf\ndf_con_m = pd.DataFrame(con_m, index= [i for i in class_names], columns = [i for i in class_names])\nsns.set(font_scale= 1.1)\nsns.heatmap(df_con_m, annot=True,fmt='g' ,  annot_kws={\"size\" : 10}, cbar = False, cmap=\"Blues\") \n","cdac1f36":"#Test the accuracy with test data\npreds = []\nnames = []\ntotal = 0\nwith torch.no_grad():\n    for i, (name, inputs) in enumerate(test_loader):\n            inputs = inputs.to(device)\n            outputs = classifier(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            names.append(name)\n            preds.append(predicted)\n\n# print(names)\n# print(preds)","5700c147":"for i in range(len(preds)):\n    preds[i] = preds[i].cpu().numpy()","4071b097":"preds = np.array(preds).ravel()\npreds = [get_labels[e] for e in preds] \nnames = np.array(names).ravel()\nprint(preds[:10])\nprint(names[:10])","094a5907":"sub = pd.DataFrame({'Image':names, 'target':preds})\nsub.head()","4d9e31b6":"sub.to_csv('Submission.csv', index = False)","2ecd9650":"**Conclusion** : All images are of one extension '.jpg'"}}