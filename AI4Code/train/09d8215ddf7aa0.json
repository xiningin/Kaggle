{"cell_type":{"f1a1d93d":"code","1f7a5d3c":"code","baffc8c5":"code","889e2a97":"code","31e0ebfb":"code","f94cd26d":"code","59883712":"code","6621327c":"code","e1724370":"code","28d40647":"code","6ee34e98":"code","08a91525":"code","85f3cb8a":"code","bb6cdf60":"code","c699a494":"code","3f1e2dbb":"code","e3b752a2":"code","8e719f61":"code","f552c482":"code","68a91f28":"code","56d3194e":"code","f5fb0af6":"code","1c9bb23c":"code","b6aa9d30":"code","4bf4f013":"code","aac17839":"code","89440f7d":"code","1598f24b":"code","54dfcffe":"code","1657760a":"code","44dbc7d8":"code","e1acf9ee":"code","c6ecd1f2":"code","5df34988":"code","17e1e104":"code","2ce4f8f7":"code","b06a6e9e":"code","6e93a2e1":"code","bd42fe70":"code","a75b9a1b":"code","108992cd":"code","a5d4ca6a":"code","ae1771db":"code","67097a8a":"code","8a3a1450":"code","381e492d":"code","f8e16827":"code","1d1e098b":"code","f0dfe316":"code","3545b740":"code","11c26ddd":"code","09c9844f":"code","0185212d":"code","95e43f00":"code","91c1f364":"markdown","9e543b6b":"markdown","7a61fdcc":"markdown","35375eee":"markdown","c76b6ad9":"markdown","ec8d1528":"markdown","634835ec":"markdown","3e9d3a98":"markdown","cd4e1d9a":"markdown","0a4010d3":"markdown","c9d52667":"markdown","4e33f3b8":"markdown","ba6efcc6":"markdown","39fb399b":"markdown","4ca43376":"markdown","38ee9758":"markdown","92108da5":"markdown","629aae64":"markdown","0a67c029":"markdown","f3b3b934":"markdown","8dd4a5e9":"markdown"},"source":{"f1a1d93d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","1f7a5d3c":"df=pd.read_csv(\"..\/input\/stock-sentiment-analysis\/Stock_Dataa.csv\",encoding=\"ISO-8859-1\")","baffc8c5":"df.head(5)","889e2a97":"print(\"*** Shape of dataset ***\")\nprint()\nprint(df.shape)","31e0ebfb":"print(\"*** Value Counts of Label Column ***\")\nprint()\ndf['Label'].value_counts()","f94cd26d":"print(\"*** Null values in the dataset ***\")\nprint()\ndf.isnull().sum()","59883712":"print(\"*** Data type of columns in dataset ***\")\nprint()\ndf.dtypes","6621327c":"print(\"*** Basic information about the dataset ***\")\nprint()\ndf.info()","e1724370":"def without_hue(data,feature,ax):\n    \n    total=float(len(data))\n    bars_plot=ax.patches\n    \n    for bars in bars_plot:\n        percentage = '{:.1f}%'.format(100 * bars.get_height()\/total)\n        x = bars.get_x() + bars.get_width()\/2.0\n        y = bars.get_height()\n        ax.text(x, y,(percentage,bars.get_height()),ha='center',fontweight='bold',fontsize=14)","28d40647":"#setting theme\nsns.set_theme(context='notebook',style='white',font_scale=3)\n\n#setting the background and foreground color\nfig=plt.figure(figsize=(8,5))\nax=plt.axes()\nax.set_facecolor(\"#F2EDD7FF\")\nfig.patch.set_color(\"#F2EDD7FF\")\n\n#Dealing with spines\nfor i in ['left','top','right']:\n    ax.spines[i].set_visible(False)\n    \nax.grid(linestyle=\"--\",axis='y',color='gray')\n\n#countplot\na=sns.countplot(data=df,x='Label',saturation=3,palette='cool')\n\nwithout_hue(df,'target',a)\n\nplt.title(\"Label Distribution\",weight='bold',fontsize=15)\nplt.xlabel('Label', fontsize=18)\nplt.ylabel('Count', fontsize=18)\nplt.show()","6ee34e98":"print(\"Starting date in dataset ==> \",df['Date'].min())\n\nprint(\"Ending date in dataset ==> \",df['Date'].max())","08a91525":"#Splitting into training and test dataset\ntrain = df[df['Date'] < '20150101']\ntest = df[df['Date'] > '20141231']","85f3cb8a":"#Getting shape of both training and test dataset\nprint(\"*** Shape of training dataset *** \", train.shape)\nprint(\"*** Shape of test dataset *** \", test.shape)","bb6cdf60":"#Removing special characters\n\ndata_train=train.iloc[:,2:27]\n\ndata_train.replace(\"[^a-zA-Z]\",\" \",regex=True,inplace=True)","c699a494":"#Making all the sentences into lower cases\n\ncols=list(data_train.columns)\n\nfor sent in cols:\n    data_train[sent]=data_train[sent].str.lower()","3f1e2dbb":"#Combining all the top headlines into one paragraph \n\nheadlines=[]\n\nfor row in range(0,len(data_train)):\n    headlines.append(\" \".join(str(x) for x in data_train.iloc[row,0:25]))\n    ","e3b752a2":"#Importing necessary libraries\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom wordcloud import WordCloud\nimport optuna\nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.metrics import accuracy_score , classification_report , f1_score , confusion_matrix","8e719f61":"#Combined First row\nprint(\"*** Combined first row ***\")\nprint()\nheadlines[0]","f552c482":"paragraph=\" \"\nfor paras in headlines:\n    paragraph+=paras","68a91f28":"text=paragraph\n\nwordcloud=WordCloud(width=2000,height=1000,background_color='#F2EDD7FF',stopwords=stopwords.words('english')).generate(text)\n\nplt.figure(figsize=(20,30))\nplt.imshow(wordcloud)\n\nplt.show()","56d3194e":"text=headlines[0]\n\nwordcloud=WordCloud(width=2000,height=1000,background_color='#F2EDD7FF',stopwords=stopwords.words('english')).generate(text)\n\nplt.figure(figsize=(20,30))\nplt.imshow(wordcloud)\n\nplt.show()","f5fb0af6":"#Countvectorizer\n\ncv=CountVectorizer(ngram_range=(2,2))\n\ntrain_dataset=cv.fit_transform(headlines)","1c9bb23c":"train_dataset.shape","b6aa9d30":"random_classifier= RandomForestClassifier(n_estimators=200,criterion='entropy', random_state=42)\nrandom_classifier.fit(train_dataset,train['Label'])","4bf4f013":"#Preprocessing for test dataset\n\ntest_transform= []\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset = cv.transform(test_transform)\npredictions = random_classifier.predict(test_dataset)","aac17839":"print(\"*** Accuracy score using bag of words method ***\" ,round(accuracy_score(test['Label'],predictions)*100,2))","89440f7d":"print(classification_report(test['Label'],predictions))","1598f24b":"'''data_test=test.iloc[:,2:27]\n\ndata_test.replace(\"[^a-zA-Z]\",\" \",regex=True,inplace=True)\n\ncols=list(data_test.columns)\n\nfor sent in cols:\n    data_test[sent]=data_test[sent].str.lower()\n    \nheadlines_test=[]\n\nfor row in range(0,len(data_test)):\n    headlines_test.append(\" \".join(str(x) for x in data_test.iloc[row,0:25]))\n\n    \nwordnet=WordNetLemmatizer()\nfor head in range(len(headlines_test)):\n    rev=headlines_test[head].split()\n    rev1=[wordnet.lemmatize(word) for word in rev if not word in set(stopwords.words('english'))]\n    rev=\" \".join(rev1)\n    headlines_test[head]=rev\n\n\ntest_dataset_wl= cv.transform(headlines_test)'''","54dfcffe":"#Importing TF-IDF vectorizer\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer","1657760a":"tf_idf=TfidfVectorizer(ngram_range=(2,2))\n\ntrain_dataset_tf=tf_idf.fit_transform(headlines)","44dbc7d8":"random_classifier_tf= RandomForestClassifier(n_estimators=200,criterion='entropy', random_state=42)\nrandom_classifier_tf.fit(train_dataset_tf,train['Label'])","e1acf9ee":"#Preprocessing test dataset\n\ntest_transform_tf= []\nfor row in range(0,len(test.index)):\n    test_transform_tf.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset_tf = tf_idf.transform(test_transform)\npredictions_tf = random_classifier_tf.predict(test_dataset_tf)","c6ecd1f2":"print(\"*** Accuracy using TF-IDF method ***\" , round(accuracy_score(test['Label'],predictions_tf)*100,2))","5df34988":"print(classification_report(test['Label'],predictions_tf))","17e1e104":"from sklearn.tree import DecisionTreeClassifier","2ce4f8f7":"dt_bow=DecisionTreeClassifier(random_state=42)\n\ndt_bow.fit(train_dataset,train['Label'])","b06a6e9e":"test_transform_dt= []\nfor row in range(0,len(test.index)):\n    test_transform_dt.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset_dt = cv.transform(test_transform_dt)\npredictions_dt = dt_bow.predict(test_dataset_dt)","6e93a2e1":"print(\"*** Acuuracy score decision tree plus bag of words ***\", accuracy_score(test['Label'],predictions_dt))","bd42fe70":"print(\"*** Classification report ***\")\nprint()\nprint(classification_report(test[\"Label\"],predictions_dt))","a75b9a1b":"tf_idf_dt=TfidfVectorizer(ngram_range=(2,2))\n\ntrain_dataset_tf_dt=tf_idf_dt.fit_transform(headlines)","108992cd":"dt_bow_tf=DecisionTreeClassifier(random_state=42)\n\ndt_bow_tf.fit(train_dataset_tf_dt,train['Label'])","a5d4ca6a":"test_transform_dt_tf= []\nfor row in range(0,len(test.index)):\n    test_transform_dt_tf.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset_dt_tf = tf_idf_dt.transform(test_transform_dt_tf)\npredictions_dt_tf = dt_bow_tf.predict(test_dataset_dt_tf)","ae1771db":"print(\"*** Acuuracy score decision tree plus TF-IDF ***\", accuracy_score(test['Label'],predictions_dt_tf))\nprint()\nprint(\"*** Classification report ***\")\nprint()\nprint(classification_report(test[\"Label\"],predictions_dt_tf))","67097a8a":"from sklearn.naive_bayes import MultinomialNB\n","8a3a1450":"naive = MultinomialNB()\n\nnaive.fit(train_dataset,train['Label'])","381e492d":"test_transform= []\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset_nb = cv.transform(test_transform)\npredictions_nb = naive.predict(test_dataset_nb)","f8e16827":"print(\"*** Accuracy score naive bayes classifier plus bag of words ***\", accuracy_score(test['Label'],predictions_nb))\nprint()\nprint(\"*** Classification report ***\")\nprint()\nprint(classification_report(test[\"Label\"],predictions_nb))","1d1e098b":"tf_nb=TfidfVectorizer(ngram_range=(2,2))\n\ntrain_dataset_nb=tf_nb.fit_transform(headlines)","f0dfe316":"naive_tf = MultinomialNB()\n\nnaive_tf.fit(train_dataset,train['Label'])","3545b740":"test_transform= []\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset_nb_tf = tf_nb.transform(test_transform)\npredictions_nb_tf = naive_tf.predict(test_dataset_nb_tf)","11c26ddd":"print(\"*** Acuuracy score naive bayes classifier plus tf-idf ***\", accuracy_score(test['Label'],predictions_nb_tf))\nprint()\nprint(\"*** Classification report ***\")\nprint()\nprint(classification_report(test[\"Label\"],predictions_nb_tf))","09c9844f":"acc_score_rf1 , acc_score_rf2 = round(accuracy_score(test['Label'],predictions)*100,2) ,round(accuracy_score(test['Label'],predictions_tf)*100,2)\n\nacc_score_dt1 , acc_score_dt2 = round(accuracy_score(test['Label'],predictions_dt)*100,2) , round(accuracy_score(test['Label'],predictions_dt_tf)*100,2)\n\nacc_score_nb1 , acc_score_nb2 = round(accuracy_score(test['Label'],predictions_nb)*100,2) , round(accuracy_score(test['Label'],predictions_nb_tf)*100,2)\nmodels = pd.DataFrame({\n    'Model':['Random Forest (Bow)', 'Random Forest (TF-IDF)', ' Naive Bayes (BoW) ', 'Naive Bayes (TF-IDF)',\"Decision Tree (BoW)\" , \"Decision Tree (TF-IDF)\"],\n    'Accuracy_score' : [acc_score_rf1, acc_score_rf2,acc_score_dt1, acc_score_dt2, acc_score_nb1,acc_score_nb2]\n})\n\n","0185212d":"models","95e43f00":"plt.figure(figsize=(20,15))\nsns.barplot(y=models['Model'],x=models['Accuracy_score'],palette='cool')","91c1f364":"<a id=5.3.2><\/a>\n#### Using TF-IDF Method","9e543b6b":"[Slide to top](#top)\n<a id=\"3\"><\/a>\n## Word Cloud of all the text in dataset","7a61fdcc":"<a id=5.2.1><\/a>\n#### Using Bag of words method","35375eee":"**ISO-8859 Encoding** : https:\/\/mincong.io\/2019\/04\/07\/understanding-iso-8859-1-and-utf-8\/","c76b6ad9":"[Slide to top](#top)\n<a id=\"5\"><\/a>\n## **Modelling** \u270c","ec8d1528":"##### **Thanks for reading my notebook . This is my very first work on any NLP problem . Please give it an upvote if you like my work**\n\n##### **Please let me know if you find any errors** \n\n##### **Thanks you , Have a nice day**","634835ec":"<a id=\"5.2.2\"><\/a>\n#### Using Tf-Idf Method","3e9d3a98":"[Slide to top](#top)\n<a id=5.3><\/a>\n### **Naive Bayes Classifier**","cd4e1d9a":"<a id=5.3.1><\/a>\n#### Using bag of words method","0a4010d3":"[Slide to top](#top)\n<a id=1><\/a>\n## **Basic Overview** \ud83d\udcfa","c9d52667":"<a id=5.1.2><\/a>\n#### Using Tf-Idf Method","4e33f3b8":"![SA](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2018\/07\/1.8.jpg)","ba6efcc6":"[Slide to top](#top)\n<a id=5.2><\/a>\n### **Decision Tree Classifier**","39fb399b":"[Silde to top](#top)\n<a id=6><\/a>\n## **Comparision of Models**","4ca43376":"## **Stock Sentiment Analysis \ud83d\udcb9**","38ee9758":"* Dataset have a shape 4101 rows and 27 columns in which 25 columns are top 25 headlines along with their respective dates and labels\n\n* Dataset is not that imbalanced , as it contains 2166 \"1\" labels and 1935 \"0\" labels\n\n* Except \"label\" column all other columns are object type i.e. string\n\n* Columns \"Top23\" , \"Top24\" and \"Top25\" have few null values i.e. 1 , 3 , 3 null values respectively\n","92108da5":"[Slide to top](#top)\n<a id=\"2\"><\/a>\n## **Preprocessing and Analysis** \ud83d\udcca","629aae64":"[Slide to top](#top)\n<a id=\"4\"><\/a>\n## Wordcloud of first row in dataset","0a67c029":"<a id=\"top\"><\/a>\n## **Table of Contents \u23e9**\n\n* [Basic Overview \ud83d\udcfa](#1)\n\n* [Preprocessing and Analysis \ud83d\udcca](#2)\n\n* [Word Cloud of all the text](#3)\n\n* [Word Cloud of first row](#4)\n\n* [Modelling \u270c](#5)\n  * [Random Forest Classifier](#5.1)\n    * [Using Bag of Words method](#5.1.1)\n    * [Using TF-IDF method](#5.1.2)\n    \n  * [Decision Tree CLassifier](#5.2)\n    * [Using Bag of Words method](#5.2.1)\n    * [Using TF-IDF method](#5.2.2)\n    \n  * [Naive Bayes Classifier](#5.3)\n    * [Using Bag of Words method](#5.3.1)\n    * [Using TF-IDF method](#5.3.2)\n    \n \n* [Comparision of Models](#6)\n  \n  ","f3b3b934":"<a id='5.1'><\/a>\n### **Random Forest Classifier** ","8dd4a5e9":"<a id=5.1.1><\/a>\n#### Using Bag of Words method"}}