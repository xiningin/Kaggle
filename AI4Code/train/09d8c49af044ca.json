{"cell_type":{"56f1fc7b":"code","3527caf1":"code","753dd394":"code","981f0080":"code","ad4f5a3f":"code","4aba9bea":"code","628409d9":"code","fc6eed28":"code","a72438e0":"code","39996716":"code","0fb1f356":"code","301d8cd8":"code","bbed5c2c":"code","c1123ddb":"code","e85fcf6f":"code","97997812":"code","f61c83f6":"code","492a9242":"code","9876679a":"code","d80fd904":"code","9301cf05":"code","eb4817df":"code","8a11d6a3":"markdown","15be8cc2":"markdown","7dbb5155":"markdown","b893a65b":"markdown","6583b9d5":"markdown","af82fb45":"markdown","49d78abd":"markdown"},"source":{"56f1fc7b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3527caf1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_selection import chi2\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy import stats\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nimport seaborn as sns","753dd394":"# reading the data from the disk and display the head of dataframe\ndf = pd.read_csv('\/kaggle\/input\/soucefuseproblem1\/Train_data.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/soucefuseproblem1\/Test_data.csv')\ndf.head()","981f0080":"# dataset info\ndf.info()","ad4f5a3f":"print('Total unique labels are {}'.format(df.Label.nunique()))\ndf.Label.unique()","4aba9bea":"# Creating a new column in dataframe for label id\nlabel_list = df.Label.factorize()[1]\ndf['label_id'] = df.Label.factorize()[0]\ndf.head()","628409d9":"#category_id_df = df2[['Product', 'category_id']].drop_duplicates()\nlabel_id_df = df[['Label', 'label_id']].drop_duplicates()\n\n#Dictionaries for future use\nlabel_to_id = dict(label_id_df.values)\nid_to_label = dict(label_id_df[['label_id','Label']].values)","fc6eed28":"def plot():\n    fig = plt.figure(figsize=(10,10))\n    df.groupby('Label').Text.count().sort_values().plot.barh(ylim =0, title= 'Number of records per label\\n')\n    plt.xlabel('Number of records.')\n    ","a72438e0":"plot()","39996716":"# create a object of term-frequency, inverse document frequency \ntfidf = TfidfVectorizer(min_df = 5, lowercase = True,max_features= 21, ngram_range=(1,2), stop_words = 'english', sublinear_tf = True)\n\n# transform each lebel into vector\nfeature = tfidf.fit_transform(df.Text).toarray()","0fb1f356":"# cross validation score\nlabels = df.label_id\nmodels = [\n    RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0),\n    XGBClassifier(),\n    LinearSVC(),\n    MultinomialNB()\n]\n\ncross_value_scored = []\nfor model in models:\n    model_name = model.__class__.__name__\n    accuracies= cross_val_score(model, feature, labels, scoring = 'accuracy', cv = 5)\n    for accuracy in accuracies:\n        cross_value_scored.append((model_name, accuracy))\n        ","301d8cd8":"# make a dataframe of cross_val_scor\ndf_cv = pd.DataFrame(cross_value_scored, columns =['model_name', 'accuracy'])\nacc = pd.concat([df_cv.groupby('model_name').accuracy.mean(),df_cv.groupby('model_name').accuracy.std()], axis= 1,ignore_index=True)\nacc.columns = ['Mean Accuracy', 'Standard deviation']\nacc","bbed5c2c":"# hyperparameter tunning of XGBClassifier\nclf_xgb = XGBClassifier()\n# hypermeter setting\nparam_dist = {'n_estimators': np.random.randint(150, 500,100),\n              'learning_rate': [0.01,0.1,0.2,0.3,0.4, 0.59],\n              'max_depth': [3, 4, 5, 6, 7, 8, 9],\n              'min_child_weight': [1, 2, 3, 4]\n             }\n\nkfold_5 = KFold(shuffle = True, n_splits = 5)\n\nclf = RandomizedSearchCV(clf_xgb, \n                         param_distributions = param_dist,\n                         cv = kfold_5,  \n                         n_iter = 5, \n                         scoring = 'roc_auc', \n                         verbose = 3, \n                         n_jobs = -1)","c1123ddb":"#clf.fit(feature, labels)","e85fcf6f":"# spliting the data into test and train\nx_train, x_test, y_train, y_test = train_test_split(feature, labels, random_state=0, test_size=0.2)","97997812":"# creating and fiting the instance of the xgb classifier\nxgb = XGBClassifier()\nxgb.fit(x_train,y_train)","f61c83f6":"y_pred = xgb.predict(x_test)\nprint('Classification report')\nprint(classification_report(y_test, y_pred))","492a9242":"# confusion_matrx \nconf_matrix = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(11,11))\nsns.heatmap(conf_matrix, annot=True, cmap = 'Reds', fmt='d',xticklabels = label_id_df.Label.values,\n           yticklabels = label_id_df.Label.values,square = True )\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.title('Confusion Matrix.')","9876679a":"print('Accuracy of the model {}'.format(accuracy_score(y_test, y_pred)))","d80fd904":"# converting the test text into numerical vector form\nfeatures = tfidf.transform(df_test.Text)\ny_pred_test = xgb.predict(features)","9301cf05":"# function for converting label_id to label name\ndef replace_id_to_label(x):\n    return id_to_label[x]","eb4817df":"#\nsubmit = pd.DataFrame({'Text':df_test.Text, 'Predicted Label':y_pred_test})\nsubmit['Predicted Label']=submit['Predicted Label'].apply(lambda x: replace_id_to_label(x))\nsubmit.to_csv('Result1.csv',index=False)","8a11d6a3":"### Text-Preprocessing","15be8cc2":"* In below bar chart showing the number of records per label","7dbb5155":"#### **NOTE: Hyperparameter tunning time consuming so I am skiping this step**","b893a65b":"* There are not missing values and total 16130 records","6583b9d5":"* There are 56 unique classes. Machine leanring algorithm understand only numerical values so Now we need to represent each classes as number ","af82fb45":"### predicting the result","49d78abd":"### Model selection"}}