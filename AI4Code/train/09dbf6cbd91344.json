{"cell_type":{"df8538e3":"code","8adc5d52":"code","60eefd35":"code","76665f0a":"code","ec0baa2b":"code","bd3ad3c2":"code","5fb5e0e0":"code","06398c7e":"code","2704c294":"code","2fca9a3e":"code","fe4f8f2b":"code","8b83f5a6":"code","8a1334c4":"code","57f48ca8":"code","40e2bd41":"code","893d1d07":"code","c8264c11":"code","31179216":"code","087b84e4":"code","6ad8172e":"code","1a069c52":"code","81eb91cd":"code","af1e82e0":"markdown","73c51c73":"markdown","c3ba218d":"markdown","ebb0eab5":"markdown","34bb0096":"markdown","ec9d789f":"markdown"},"source":{"df8538e3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom kaggle.competitions import twosigmanews # competition data\nimport matplotlib.pyplot as plt # graphs\n\nenv = twosigmanews.make_env()\n(market_train, news_train) = env.get_training_data() # load training data","8adc5d52":"# Summary of raw target variable.\ntarget = market_train['returnsOpenNextMktres10']\nprint(f\"Data for the target variable is centered at {target.mean():.3f} with a standard deviation (SD) of \\\n{target.std():.3f}. The mean \\nseems reasonable, but when veiwing a histogram of the data, it is clear that \\\noutliers are driving the \\nSD. Out of {len(target)} total values, the biggest negative target value is \\\n{target.min():.0f} and there are {sum(target < -0.5)} values under -0.5; the largest posative number is \\\n{target.max():.0f} and there are {sum(market_train['returnsOpenNextMktres10'] > 1)} values over 0.5. The \\\nhistogram below shows the shape of target variable for values over -0.5 and under 0.5 with the data \\\n'clipped' using pandas clip method at -0.5 and 0.5. The outliers can be seen graphically (with a good \\\nmonitor) at -0.5 and 0.5 on the x-axis.\")\n    \ntarget.clip(-.5,0.5).hist(bins=400, figsize=(7,5))\nplt.title('Target Variable Distribution', size=14)\nplt.xlim(-0.6, 0.6)\nplt.show()","60eefd35":"# I made a graph to look at the high target values against stock open price\n\ndef check_bad_opens(asset_code, df=market_train, show_graph=True):\n    \"\"\" Expects market_train data frame format. Returns pyplot graphing opeject or plots graph.\n    \n        Parameters\n        ----------\n        asset_code (string): assetCode from market_trian df to plot\n        df (pandas.DataFrame): market_train df from two sigmas\n        show_graph (boolean): can be set to False to make further modifications\n    \"\"\"\n    \n    dat = df[df['assetCode'] == asset_code]\n    name_for_title = dat['assetName'][0]\n    dat.index = dat['time']\n    fig, (ax1, ax2) = plt.subplots(2, figsize=(15,8))\n    dat['open'].plot(ax=ax1)\n    ax1.grid()\n    dat['returnsOpenNextMktres10'].plot(ax=ax2)\n    plt.grid()\n    ax1.set_title('Daily Open Values for ' + name_for_title, size=18)\n    ax2.set_title('Adjusted Next 10 Day Return ' + name_for_title, size=18)\n    plt.tight_layout()\n    if show_graph:\n        plt.show()\n    else:\n        return ax1, ax2","76665f0a":"# Check for extreem outliers and take a look at them using check_bad_open\noutlier_dat = market_train[ (market_train['returnsOpenNextMktres10'] > 1000) | (market_train['returnsOpenNextMktres10'] < -1000) ]\noutlier_dat['assetCode'].value_counts()\nmarket_train['time'] = pd.to_datetime(market_train['time'])","ec0baa2b":"check_bad_opens('ATPG.O')","bd3ad3c2":"ax1, ax2 = check_bad_opens('ATPG.O', show_graph=False)\nax1.set_xlim(pd.datetime(2007, 10,1), pd.datetime(2008, 1,1))\nax2.set_xlim(pd.datetime(2007, 10,1), pd.datetime(2008, 1,1))\nplt.tight_layout()\nplt.show()","5fb5e0e0":"## now that it looks like bad open values are driving the bad return values. I will take a look at the distribution of the \n## open values. \n\nplt.hist(market_train['open'].clip(0,400), bins=400)\nplt.title('Two Sigma Open Values for Period of Record')\nplt.show()","06398c7e":"## I can see from the histogram that there are high and low extreem open values. I will assume that opens below 0.05 and \n## above 999 are bad\n\nbad_open_df = market_train[ (market_train['open'] < 0.2) | # or \n                            (market_train['open'] > 900) ]","2704c294":"bad_open_df['assetCode'].value_counts()","2fca9a3e":"## It looks like PCLN and PGN have a lot of bad open values, so I will take a look at them with my graph.\ncheck_bad_opens('PCLN.O')\ncheck_bad_opens('PGN.N')","fe4f8f2b":"## get rid of the values discused above...\nbad_open_df = bad_open_df[ (bad_open_df['assetCode'] != 'PCLN.O') &\n             (bad_open_df['assetCode'] != 'PGN.N')]","8b83f5a6":"%%time\nbad_ix = list()\n\nfor x in bad_open_df.index:\n    bad_date = market_train.loc[x,'time']\n    min_date = bad_date - pd.DateOffset(days=50)\n    max_date = bad_date + pd.DateOffset(days=50)\n    site_name = market_train.loc[x, 'assetCode']\n    bad_data = market_train[ (market_train['time'] > min_date) & \n                             (market_train['time'] < max_date) &\n                             (market_train['assetCode'] == site_name)]\n    bad_ix = bad_ix + list(bad_data.index)","8a1334c4":"data_ = market_train.copy() # since I can't reload the training data I will make a copy\n\ndata_ = data_.drop(axis=0, index=bad_ix)","57f48ca8":"# Summary of raw target variable.\ntarget = data_['returnsOpenNextMktres10']\nprint(f\"Data for the target variable is centered at {target.mean():.3f} with a standard deviation (SD) of \\\n{target.std():.3f}. The mean \\nis now closer to zero, but the SD is still high. Out of {len(target)} total \\\nvalues, the biggest negative target value is {target.min():.0f} and there are {sum(target < -0.5)} values \\\nunder -0.5; the largest posative number is {target.max():.0f} and there are \\\n{sum(market_train['returnsOpenNextMktres10'] > 1)} values over 0.5. The histogram below shows the shape of \\\ntarget variable for values over -0.5 and under 0.5 with the data 'clipped' using pandas clip method at -0.5 and 0.5. The outliers can be seen graphically (with a good \\\nmonitor) at -0.5 and 0.5 on the x-axis.\")\n    \ntarget.clip(-.5,0.5).hist(bins=400, figsize=(7,5))\nplt.title('Target Variable Distribution', size=14)\nplt.xlim(-0.6, 0.6)\nplt.show()","40e2bd41":"# identify the sites which still have high target values and check the minimum outputs\nsites_list = data_[ (data_['returnsOpenNextMktres10'] < -5) |\n                    (data_['returnsOpenNextMktres10'] > 5)]['assetCode'].unique()\n\nmin_open = [data_[data_['assetCode'] == x]['open'].min() for x in sites_list]\n# max_open = [data_[data_['assetCode'] == x]['open'].max() for x in sites_list] # none of the max opens were that high\n\nmin_opens = dict(zip(sites_list, min_open))","893d1d07":"min_opens ","c8264c11":"# will use a dict comprehension to get rid of the unwanted values\nmin_opens = {key:val for (key, val) in min_opens.items() if val < 2}\n_ = min_opens.pop('PGN.N') # this was checked above","31179216":"min_opens","087b84e4":"for asset, min_val in min_opens.items():\n    pass","6ad8172e":"# i didn't use the dict for this, just the list and df index\nbad_ix = list()\n\nfor asset, min_val in min_opens.items():\n    bad_date = data_.loc[(data_['assetCode'] == asset) & (data_['open'] == min_val),'time'][0]\n    min_date = bad_date - pd.DateOffset(days=50)\n    max_date = bad_date + pd.DateOffset(days=50)\n    bad_data = data_[ (data_['time'] > min_date) & \n                             (data_['time'] < max_date) &\n                             (data_['assetCode'] == asset)]\n    bad_ix = bad_ix + list(bad_data.index)","1a069c52":"data_ = data_.drop(axis=0, index=bad_ix, errors='ignore')","81eb91cd":"# Summary of raw target variable.\ntarget = data_['returnsOpenNextMktres10']\nprint(f\"Data for the target variable is centered at {target.mean():.3f} with a standard deviation (SD) of \\\n{target.std():.3f}. The mean \\nis now closer to zero, but the SD is still high. Out of {len(target)} total \\\nvalues, the biggest negative target value is {target.min():.0f} and there are {sum(target < -1)} values \\\nunder -1; the largest posative number is {target.max():.0f} and there are \\\n{sum(market_train['returnsOpenNextMktres10'] > 1)} values over 1. The histogram below shows the shape of \\\ntarget variable for values over -0.5 and under 0.5 with the data 'clipped' using pandas clip method at -0.5 and 0.5. The outliers can be seen graphically (with a good \\\nmonitor) at -0.5 and 0.5 on the x-axis.\")\n    \ntarget.clip(-.5,0.5).hist(bins=400, figsize=(7,5))\nplt.title('Target Variable Distribution', size=14)\nplt.xlim(-0.6, 0.6)\nplt.show()","af1e82e0":"The above graph shows that the 10 day return (target) goes out of range for around a month due to the bad open value. I will try to find all of the bad open values in the next cell. ","73c51c73":"### to be continued...","c3ba218d":"Both of the assets with multiple high and low open values look like they are not bad data. The first 'Booking Holdings' looks like it just had a huge run, and the second looks like a smaller energy asset that has very high volatility. It might be better for training to take the second asset out, but for now I will leave it in. I checked the rest of the values, and they all had visually apparent 'bad open' values.\n\nBelow I will be removing all rows from the training set 50 days before and after a 'bad open'.","ebb0eab5":"# Two Sigma Outlier investigation\n\nThe two sigma dataset includes a fair amount of outliers. I will investigate the validity of the outliers below and document what I find. I will start with the target variable and possibly move on to other variables. Once I post my initial kernal, I will try to build on my work using and siting any useful techniques from other kernals. \n\n\n# Load Librarys and Data","34bb0096":"## Investigation\n\nI would like to find where outliers are, if they are valid readings, and how to remove them and any other bad data I can identify. The easiest way to get rid of the outliers will be to set high and low limits for the target variable to subset the data frame. From the graph above it looks like -0.5 and positive 0.5 are reasonable values to use as limits. \n\nI will start by identifying the largest outliers for the target variable, then I will take a look at them against the current open value as it is the value used to calculate them. I will also take a look at a few other summarizations of the data. I wrote a short function using two matplotlib graphs to illustrate the root cause of the bad target values. Unhide the next cell to view the check_bad_opens function I will use to check the extreme values.\n","ec9d789f":"I will start by looking at the extreem outliers. The figure below illustrates the cause of the extreem values. A bad (close to zero) open value caused the calculated returns to go 'crazy' for a while. The next graph takes a closer look at the periods before and after the bad open value.  "}}