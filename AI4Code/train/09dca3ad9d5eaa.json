{"cell_type":{"bb1e6d35":"code","da100a23":"code","f5fa966d":"code","ef257b13":"code","e2d3c5f4":"code","f5f8ab75":"code","3215f98c":"code","114532ed":"code","130da685":"code","dfe3d3e3":"code","15209469":"code","7caa26bf":"code","20316fb0":"code","aad2b1b4":"code","e5a6a6e2":"code","794ed177":"code","b9019f6f":"code","0e017a18":"code","346a87d5":"code","57bbce31":"code","4b6f1ace":"code","99abc26f":"code","3401d86b":"code","2bf13929":"code","4862b64e":"code","d078e85f":"code","8536eb77":"code","e249ddff":"code","bdeca79e":"code","d65f3ba2":"code","9fba9019":"code","830fee81":"code","5654afeb":"code","199cf126":"code","9821712f":"code","2d45e6f7":"markdown","0affdcea":"markdown","f67fff85":"markdown","f80d7c19":"markdown","1e786021":"markdown","e2740c15":"markdown"},"source":{"bb1e6d35":"#import libraries and datasets\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras import backend as K\nfrom keras.utils import get_custom_objects\nfrom keras.layers import Activation,Dropout\nfrom keras.optimizers import Adam\nsns.set_style('darkgrid')\nplt.rcParams['figure.figsize']=(16, 8.27) #set graphs size to A4 dimensions\nsns.set(font_scale = 1.4)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\ntrain_set = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_set = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n","da100a23":"train_set.info()","f5fa966d":"test_set.info()","ef257b13":"train_set.head(10)","e2d3c5f4":"test_set.head(10)","f5f8ab75":"for feature in train_set.columns:\n    print(feature, round((train_set[feature].isnull().mean()*100),2),'% missing')","3215f98c":"for feature in test_set.columns:\n    print(feature, round((test_set[feature].isnull().mean()*100),2),'% missing')","114532ed":"sns.countplot(train_set['Sex'],edgecolor=\"k\", palette=\"Set1\")","130da685":"sns.countplot(train_set['Pclass'],hue=train_set['Sex'],edgecolor=\"k\", palette=\"Set1\")","dfe3d3e3":"sns.countplot(train_set['Pclass'],hue=train_set['Survived'],edgecolor=\"k\", palette=\"Set1\")","15209469":"sns.countplot(train_set['Embarked'],hue=train_set['Survived'],edgecolor=\"k\", palette=\"Set1\")","7caa26bf":"sns.countplot(train_set['Sex'],hue=train_set['Survived'],edgecolor=\"k\", palette=\"Set1\")","20316fb0":"sns.distplot(train_set['Age'],hist_kws=dict(edgecolor=\"k\", linewidth=2),kde_kws={\"color\": \"#ce0d55\", \"lw\": 2})","aad2b1b4":"sns.distplot(train_set['Fare'],hist_kws=dict(edgecolor=\"k\", linewidth=2),kde_kws={\"color\": \"#ce0d55\", \"lw\": 2})","e5a6a6e2":"# Combine train and test set to achieve a better accuracy score \n# !!This is NOT the proper way in a real-world use case scenario!!\n\ndataset=pd.concat([train_set,test_set])    ","794ed177":"dataset.info() ","b9019f6f":"#sex mapping \ndataset['Sex']=np.where(dataset['Sex']=='male',1,0) #convert Male to 1 and Female to 0","0e017a18":"#Title mapping\ndataset['Title']=dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n#transform all titles to 6 unique categories\nTitle_Dict = { \"Capt\": \"Officer\", \"Col\": \"Officer\", \"Major\": \"Officer\", \"Jonkheer\": \"Royalty\", \"Don\": \"Royalty\",\n                    \"Sir\" : \"Royalty\", \"Dr\": \"Officer\", \"Rev\": \"Officer\", \"the Countess\":\"Royalty\", \"Mme\": \"Mrs\",\n                    \"Mlle\": \"Miss\", \"Ms\": \"Mrs\", \"Mr\" : \"Mr\", \"Mrs\" : \"Mrs\", \"Miss\" : \"Miss\", \"Master\" : \"Master\",\n                    \"Lady\" : \"Royalty\" }\n\ndataset['Title'] = dataset['Title'].map(Title_Dict) #apply transformation to the column","346a87d5":"dataset[dataset['Title'].isnull()] #check if mapping worked properly","57bbce31":"dataset['Title'].fillna('Mrs',inplace=True) #filling NaN values with Mrs because both passengers are ladies at 30's ","4b6f1ace":"dataset['Title'].value_counts() #check frequency of each category","99abc26f":"#encoding categories according to the frequency in dataset (0 most common, 5 rarest) and apply them to dataset\ntitle_labels=dataset['Title'].value_counts().index\ntitle_labels={k:i for i,k in enumerate(title_labels,0)}\ndataset['Title']=dataset['Title'].map(title_labels)","3401d86b":"dataset['Title'].value_counts() #chech if encoding worked properly","2bf13929":"cabin_mapping={'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'T':7} #find cabins unique starting letter and encode them\n\ndataset['Cabin']=dataset['Cabin'].str[:1]\ndataset['Cabin']=dataset['Cabin'].map(cabin_mapping)\n\n#fill nan values of cabin feature, according to the median cabin value for each class.  \ndataset['Cabin'].fillna(dataset.groupby('Pclass')['Cabin'].transform('median'),inplace=True) ","4862b64e":"#fill nan values for age, according to the median value of each category title.\ndataset[\"Age\"].fillna(dataset.groupby(\"Title\")[\"Age\"].transform(\"median\"), inplace=True)","d078e85f":"#fill nan values of Embarked with 'S' because is the mode value for embarked feature\ndataset['Embarked'] = dataset['Embarked'].fillna('S')\n\n#encode the embarked feature according to the frequency of each class.(2 most common, 0 rarest)\nembarked_mapping = {\"S\": 2, \"C\": 1, \"Q\": 0}\ndataset['Embarked']=dataset['Embarked'].map(embarked_mapping)# apply encoding to dataset","8536eb77":"#fill nan values for Fare feature, according to median of fare for each cabin unique value.\ndataset[\"Fare\"].fillna(dataset.groupby(\"Cabin\")[\"Fare\"].transform(\"median\"), inplace=True)","e249ddff":"#Create a new feature FamilySize that includes all members of Parch and SibSp for every passenger \n#(including the passenger too)\ndataset['FamilySize']=dataset['Parch']+dataset['SibSp']+1\ndataset.drop(['SibSp','Parch'],axis=1,inplace=True)","bdeca79e":"dataset['FamilySize'].value_counts()","d65f3ba2":"#apply sqrt transformation to handle outliers.\ndataset['Age']=np.sqrt(dataset['Age'])\ndataset['Fare']=np.sqrt(dataset['Fare'])\ndataset['FamilySize']=np.sqrt(dataset['FamilySize'])","9fba9019":"#drop unuseful features\ndataset.drop(['PassengerId','Name','Ticket'],axis=1,inplace=True)\n\n#split features (X) from target (y)\nX=dataset.drop('Survived',axis=1)\ny=dataset['Survived']\n\n#scaling the X dataframe (features)\nscaler=MinMaxScaler()\nscaled_features=scaler.fit_transform(X)\nscaled_features_X=pd.DataFrame(scaled_features,columns=X.columns)\nscaled_features_X.index=X.index\n\n#concat scaled X features with y target \nfinal_dataset=pd.concat([scaled_features_X,y],axis=1)","830fee81":"X=final_dataset.drop('Survived',axis=1)\ny=final_dataset['Survived']\n\n#split to train and test set\nX_train=X[:891]\nX_test=X[891:]\n\ny_train=y[:891]\ny_test=y[891:]","5654afeb":"class Swish(Activation):\n    \n    def __init__(self, activation, **kwargs):\n        super(Swish, self).__init__(activation, **kwargs)\n        self.__name__ = 'swish'\n\ndef swish(x):\n    return (K.sigmoid(x) * x)\n\nget_custom_objects().update({'swish': Swish(swish)})\n\n\n#build custom Adam optimizer \noptimizer=Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n\n\n# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(units = 64, kernel_initializer = 'normal',activation='swish', input_dim = 8))\n\n#Adding the second hidden layer\nclassifier.add(Dense(units =64, kernel_initializer = 'normal', activation='swish'))\n\nclassifier.add(Dropout(0.4))\n\n# Adding the output layer\nclassifier.add(Dense(units = 1, kernel_initializer = 'normal', activation = 'sigmoid'))\n\n# Compiling the ANN\nclassifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nhistory=classifier.fit(X_train, y_train, batch_size = 32, epochs = 200, validation_split=0.1, shuffle=True)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)\n# each prediction above 0.5 is classified as 1 and the rest as 0\ny_pred = (y_pred > 0.5)\ny_pred= y_pred*1\ny_pred =y_pred.reshape(418)\ny_pred=pd.Series(y_pred).rename('Survived')\n","199cf126":"plt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train set')\nplt.plot(history.history['val_accuracy'], label='validation set')\nplt.legend()\nplt.show()","9821712f":"users_id=test_set['PassengerId']\n\n#concat users id and y_pred to create final submission DataFramet\nsubmission=pd.concat([users_id,y_pred],axis=1)\n\n#export to csv\nsubmission.to_csv('submission.csv',index=False)","2d45e6f7":"# EXPLORATORY DATA ANALYSIS","0affdcea":"# FEATURE ENGINEERING ","f67fff85":"### Let's create an Artificial Neural Network to predict if a passenger has survived or not.","f80d7c19":"#### Now we are going to evaluate the results on the initial test set","1e786021":"# Titanic: Machine Learning from Disaster","e2740c15":"## The Challenge\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc). "}}