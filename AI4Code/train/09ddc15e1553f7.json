{"cell_type":{"8bc6be74":"code","9b2bb69e":"code","0fbca030":"code","02e72d72":"code","5267c599":"code","af4cf9be":"code","44925f51":"code","68fe7495":"code","fe425f78":"markdown","b9bd8e7c":"markdown","6849e69e":"markdown","41d1c57f":"markdown","a6c870e5":"markdown","78e2bcea":"markdown","b90dbf3c":"markdown","c8d79225":"markdown"},"source":{"8bc6be74":"import numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import SubsetRandomSampler\nimport torch.nn.functional as F","9b2bb69e":"class KaggleMNIST(Dataset):\n    \"\"\"A custom dataset to be used with pytorch.\n    Even when pytorch has the MNIST dataset, to participate in the Kaggle\n    competition we better use the Kaggle dataset.\n    This class only loads the train dataset (images + labels).\n    \"\"\"\n    \n    def __init__(self, path):\n        data = np.loadtxt(path + 'train.csv', delimiter=',', skiprows=1, dtype=np.float32)\n        self._digits = torch.from_numpy(data[:, 1:]) \/ 255\n        self._labels = torch.from_numpy(data[:, 0]).type(torch.long)\n        self._size = len(data)\n        print('Training dataset with MNIST digits loaded.')\n        print('  Digits has shape:', self._digits.shape)\n        print('  Labels has shape:', self._labels.shape)\n        \n    def __getitem__(self, idx):\n        return (self._digits[idx], self._labels[idx])\n    \n    def __len__(self):\n        return self._size\n    \nclass KaggleMNIST_test(Dataset):\n    \"\"\"A custom dataset to be used with pytorch.\n    Even when pytorch has the MNIST dataset, to participate in the Kaggle\n    competition we better use the Kaggle dataset.\n    This class only loads the test dataset (images).\n    \"\"\"\n    \n    def __init__(self, path):\n        data = np.loadtxt(path + 'test.csv', delimiter=',', skiprows=1, dtype=np.float32)\n        self._digits = torch.from_numpy(data) \/ 256\n        self._size = len(data)\n        print('Testing dataset with MNIST digits loaded.')\n        print('  Digits has shape:', self._digits.shape)\n        \n    def __getitem__(self, idx):\n        return self._digits[idx]\n    \n    def __len__(self):\n        return self._size","0fbca030":"def create_samplers(size, train_prop):\n    \"\"\"A function that creates 2 subsets from a training dataset, one to be\n    use din training and another to be used in validation.\n    \n    Parameters\n    ----------\n    size : Numeric, integer.\n        The number of elements in the dataset to be split into a train set and\n        a validation set.\n    train_prop : Numeric, float.\n        A number between 0 and 1 that will determine the proportion of elements\n        that will be included in the validation set.\n\n    Returns\n    -------\n    A tuple with 2 SubsetRandomSampler objects, the first to be used with the\n    training DataLoader, and the second with the validation DataLoader.\n    \"\"\"\n    cut_point = int(size * train_prop)\n    shuffled = np.random.permutation(size)\n    train_sampler = SubsetRandomSampler(shuffled[:cut_point])\n    validation_sampler = SubsetRandomSampler(shuffled[cut_point:])\n    return train_sampler, validation_sampler","02e72d72":"class MLP_1_HL_Classification(nn.Module):\n    \"\"\"A simple multilayer perceptron with a single hidden layer, using\n    LeakyReLU as activation function and cross entropy as loss function,\n    to be appklied to classification problems.\n    First attempt at putting everything inot an object, so the model is\n    self contained. Not sure if this is really needed, but I feltI could\n    give it a try. Also, this is my first project with pytorch.\n    \"\"\"\n\n    def __init__(self, input_size, hidden_size, output_size):\n        super().__init__()\n        self._input_size = input_size\n        self._hidden_size = hidden_size\n        self._output_size = output_size\n        self.train_loader = None\n        self.validation_loader = None\n        self._optimizer_fn = torch.optim.Adam\n        self._loss_fn = F.cross_entropy\n        self._net = nn.Sequential(\n            nn.Linear(self._input_size, self._hidden_size),\n            nn.LeakyReLU(),\n            nn.Linear(self._hidden_size, self._output_size))\n        \n    def forward(self, batch):\n        # batch = batch.reshape(-1, self._input_size)\n        return self._net(batch)\n    \n    def define_loaders(self, train, validation):\n        self.train_loader = train\n        self.validation_loader = validation\n            \n    def _accuracy(self, outputs, labels):\n        preds = torch.max(outputs, dim=1)[1]\n        return torch.tensor(torch.sum(preds == labels).item() \/ len(preds))\n                \n    def _validation_with_batch(self, batch):\n        images, labels = batch \n        preds = self(images)\n        loss = self._loss_fn(preds, labels, reduction='sum')\n        acc = self._accuracy(preds, labels) * len(labels)\n        return {'loss': loss, 'accuracy': acc}\n        \n    def _evaluate(self):\n        with torch.no_grad():\n            outputs = [self._validation_with_batch(batch) for batch in self.validation_loader]\n        samples = len(self.validation_loader.sampler.indices)\n        batch_losses = [x['loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).sum() \/ samples\n        batch_accuracies = [x['accuracy'] for x in outputs]\n        epoch_accuracy = torch.stack(batch_accuracies).sum() \/ samples\n        return {'loss': epoch_loss.item(), 'accuracy': epoch_accuracy.item()}\n    \n    def fit(self, epochs, learning_rate):\n        print(f'Training the model for {epochs} epochs with learning rate {learning_rate}.')\n        optim = self._optimizer_fn(self.parameters(), learning_rate)\n        history = []\n        for epoch in range(epochs):\n            # Training Phase \n            for batch in self.train_loader:\n                images, labels = batch \n                loss = self._loss_fn(self(images), labels)\n                loss.backward()\n                optim.step()\n                optim.zero_grad()\n            # Evaluate epoch with validation data set.\n            result = self._evaluate()\n            print(\"Epoch [{}], loss: {:.4f}, accuracy: {:.4f}\".format(epoch, result['loss'], result['accuracy']))\n            history.append(result)\n        return history  \n","5267c599":"train_set = KaggleMNIST('..\/input\/digit-recognizer\/')\ntest_set = KaggleMNIST_test('..\/input\/digit-recognizer\/')","af4cf9be":"batch_size = 80\nhidden_layer = 500\n\ntrain_sampler, validation_sampler = create_samplers(len(train_set), 0.8)\n\nprint(f'Size of training set: {len(train_sampler.indices)}.')\nprint(f'Size of validation set: {len(validation_sampler.indices)}.')\n\ntrain_loader = DataLoader(train_set, batch_size, sampler = train_sampler)\nvalidation_loader = DataLoader(train_set, batch_size, sampler = validation_sampler)","44925f51":"model = MLP_1_HL_Classification(784, hidden_layer, 10)\nmodel.define_loaders(train_loader, validation_loader)\nmodel.fit(10, 0.005)\n\nresult0 = model._evaluate()\nprint(result0)","68fe7495":"predictions = [[idx+1, torch.max(model(point), dim=1)[1].item()] for idx, point in enumerate(DataLoader(test_set))]\nsubmission = pd.DataFrame(predictions, columns=['ImageId', 'Label'])\nsubmission.to_csv(\"submission.csv\", index=False)","fe425f78":"## 5. Further set up","b9bd8e7c":"## 3. The model\nFor this problem, the loss function is cross entropy and the optimization is done with stochastic gradient descent. The hidden layes has a LeakyReLU activation function, but I have not seen it to be better or worse than plain ReLU. Using signoid did perform somewhat worse. I have not tried anything else.\n\nThe model receives the loaders, and accuracy is built in.\n\nValidation is a private method, since it is used during training and the results (evolution of loss and accuracy) are reported to screen and returned by the fit() method in case the user wants to do something them.","6849e69e":"# Intro\nThis my first attempt at working with PyTorch; my goal is to be able to create a dataset, and a simple (1 hidden layer) net for classification purposes. I want everything as self contained as possible. Probably there are more flexible designs, but I am focusing on just solving the MNIST digit classifier.\n\nFirst of all, some useful imports.","41d1c57f":"## 2. A helper function\nThe purpose of this function is to reduce clutter.","a6c870e5":"## 1. Create the dataloaders\nThe hardest part here was getting the right shapes and types. There are 2 objects, one for the training set (which returns data and true label) and one for test set (which only return data).\nSince in this simple model we do not care abut the shape, the data is returned as a vector of length 784.","78e2bcea":"## 7. Submit to competition\nYucks, some ugly code... but it works, and it is the least important code. Do not look here!","b90dbf3c":"## 6. Create the model and train it\nGiven that we know the dataset, input and ouput layer sizes are constants.\n\nThe fit method alloes for consecutive calls modifying the epochs and the learning rates. The ones shown here were found by trial and error. This net seems to have a top accuracy of 97%, and more epochs or bigger hidden layer do not make it better, at least in my attempts. Still, pretty good for my first net.","c8d79225":"## 4. Loading the data"}}