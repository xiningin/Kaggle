{"cell_type":{"95c6880c":"code","a4f869ed":"code","d524c80e":"code","ac1bed2d":"code","a3505b3e":"code","e79be3a6":"code","817045c3":"code","1f09a7c4":"code","b6226a94":"code","e7b239bd":"code","a27c1f9a":"code","71f62e29":"code","36f9e685":"code","14119968":"code","362ab2ba":"code","0326ce7d":"code","a44fb07d":"code","13bd5102":"code","6db86e75":"markdown","e10dc250":"markdown","e26fbdfc":"markdown","6ec74dfd":"markdown","c89a0faf":"markdown","65bf3de1":"markdown","b006b361":"markdown","54b150a0":"markdown","a53766a5":"markdown","ae21870a":"markdown","691e6582":"markdown"},"source":{"95c6880c":"from pandas import read_csv, DataFrame\n\ndata = read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\ndata.head()","a4f869ed":"len(data)","d524c80e":"!pip install nlp","ac1bed2d":"from nlp import load_dataset\n\nextra_data = load_dataset(path='glue', name='mnli')\nmnli = []\nfor sample in extra_data['train']:\n    mnli.append([sample['premise'], sample['hypothesis'], sample['label']])\ndel extra_data\nmnli = DataFrame(mnli, columns=['premise','hypothesis','label'])\nlen(mnli)","a3505b3e":"from tensorflow.distribute.cluster_resolver import TPUClusterResolver\nfrom tensorflow.config import experimental_connect_to_cluster\nfrom tensorflow.tpu.experimental import initialize_tpu_system\nfrom tensorflow.distribute.experimental import TPUStrategy\nfrom tensorflow.distribute import get_strategy\n\ntry:\n    tpu = TPUClusterResolver()\n    experimental_connect_to_cluster(tpu)\n    initialize_tpu_system(tpu)\n    strategy = TPUStrategy(tpu)\n    print('using TPU...')\nexcept ValueError:\n    strategy = get_strategy() # for CPU and single GPU\n    print('not using TPU...')","e79be3a6":"from transformers import AutoTokenizer, TFAutoModel\n\npretrained = 'jplu\/tf-xlm-roberta-large'\ntokenizer = AutoTokenizer.from_pretrained(pretrained)","817045c3":"from tensorflow import data as d\nfrom tensorflow.data.experimental import AUTOTUNE\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# average length of encoded sequences in test set is ~44\n# bigger maximum length might need more than given 16GB of RAM\nmax_len = 256\n\ndef generator(dataset, batch_size):\n        \n    texts = dataset[['premise', 'hypothesis']].values.tolist()\n    encoded = tokenizer.batch_encode_plus(texts, max_length=max_len, padding=True, truncation=True)['input_ids']\n    label = dataset['label'].values\n    \n    # padding with 0\n    data_tensor = d.Dataset.from_tensor_slices((pad_sequences(encoded, padding='post', value=0), label))\n    \n    return data_tensor.shuffle(2048).batch(batch_size).prefetch(AUTOTUNE)","1f09a7c4":"# bigger batch size might need more RAM\ntrain = generator(mnli, 256)\nval = generator(data, 256)","b6226a94":"from gc import collect\n\ndel data, mnli\ncollect()","e7b239bd":"from tensorflow.keras.layers import Input, Dropout, Dense, GlobalAveragePooling1D\nfrom tensorflow.keras import Model\nfrom tensorflow import int32\nfrom tensorflow.keras.metrics import CategoricalAccuracy\n\ndef build(dropout_rate, optimizer, loss):\n    inputs = Input(shape=(max_len,), dtype=int32)\n    layers = TFAutoModel.from_pretrained(pretrained)(inputs)[0]\n#     layers = Dropout(rate=dropout_rate)(layers)\n    layers = GlobalAveragePooling1D()(layers)\n    outputs = Dense(3, activation='softmax')(layers)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.layers[1].trainable = False\n    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n    return model","a27c1f9a":"from tensorflow.keras.optimizers import Adam\n\nwith strategy.scope():\n    dropout_rate = 0\n    optimizer = Adam(lr=1e-5)\n    loss = 'sparse_categorical_crossentropy'\n    model = build(dropout_rate=dropout_rate, optimizer='adam',loss=loss)\n    model.summary()","71f62e29":"# Time limit is 2 hours\n# Theoretically, one could train for infinite time by saving weights after each session then using the final weights for evaluation\nhst = model.fit(train, epochs=20, verbose=1, validation_data=val)","36f9e685":"import matplotlib.pyplot as plt\n\ndef visualize(metric):\n    plt.plot(hst.history[metric])\n    plt.plot(hst.history['val_' + metric])\n    plt.title(metric)\n    plt.ylabel(metric)\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.show()","14119968":"visualize('loss')","362ab2ba":"visualize('accuracy')","0326ce7d":"data_test = read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\ntexts = data_test[['premise', 'hypothesis']].values.tolist()\nencoded = tokenizer.batch_encode_plus(texts, max_length=max_len, padding='max_length', truncation=True)['input_ids']\nencoded_data_test = d.Dataset.from_tensor_slices(pad_sequences(encoded, padding='post', value=0))\nencoded_data_test = encoded_data_test.batch(256)","a44fb07d":"prediction = model.predict(encoded_data_test, verbose=1)\nprediction","13bd5102":"submission = data_test.id.copy().to_frame()\nsubmission['prediction'] = prediction.argmax(1)\nsubmission.to_csv(\"submission.csv\", index = False)","6db86e75":"Notebook for competition: https:\/\/www.kaggle.com\/c\/contradictory-my-dear-watson","e10dc250":"## Predict","e26fbdfc":"pre-trained model from https:\/\/huggingface.co\/jplu\/tf-xlm-roberta-large","6ec74dfd":"## Get extra data","c89a0faf":"## Set up TPU\n","65bf3de1":"## Train","b006b361":"## Load data","54b150a0":"## Convert data into Tensorflow dataset","a53766a5":"MNLI data set: https:\/\/cims.nyu.edu\/~sbowman\/multinli\/","ae21870a":"Given data is not enough to get good result","691e6582":"## Build model"}}