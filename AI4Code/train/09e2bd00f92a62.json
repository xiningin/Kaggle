{"cell_type":{"4a04158a":"code","d1b90166":"code","2abe1e5e":"code","1b078c6d":"code","30de1b34":"code","e1af0bd5":"code","b326fb2e":"code","b92c1291":"code","0a0337aa":"code","c3f12ce4":"code","21645020":"code","b3426a9b":"code","21ed8f6c":"code","92279a38":"code","59c5c7f1":"code","2b999a01":"code","a24440ac":"code","8433db7d":"code","ebaa3b94":"code","549283fc":"code","181ae955":"code","843d1678":"code","d7ee09ff":"code","f54fa59d":"code","7d7dd751":"code","4191c530":"code","8525ee21":"code","d4283801":"code","d791334a":"code","dcc90807":"code","bc156d01":"code","25bbde7c":"code","6c6f667e":"code","cd169f92":"code","f6ddc546":"code","dd1367bf":"code","2223e6cb":"code","48faa39c":"code","5bb85168":"code","6781ce8c":"code","43bf12e0":"markdown","0bb1e7db":"markdown","bec1961b":"markdown","3145a4c6":"markdown","801739c9":"markdown","3f48a443":"markdown","ec9b0ce4":"markdown","c234da50":"markdown","3a059124":"markdown"},"source":{"4a04158a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current dir|ectory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1b90166":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","2abe1e5e":"data = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\", encoding=\"latin1\")\ndata.head()","1b078c6d":"data.shape","30de1b34":"data.info()","e1af0bd5":"data.columns","b326fb2e":"data.isnull().sum()","b92c1291":"data['OriginalTweet'][1]","0a0337aa":"data['OriginalTweet'][4]","c3f12ce4":"data['Sentiment'].value_counts()","21645020":"plt.figure(figsize=(8, 6))\nsns.countplot(data['Sentiment'], data=data)","b3426a9b":"#Keeping the important columns only and removing the rest\ndata = data[['OriginalTweet', 'Sentiment']]","21ed8f6c":"data.head()","92279a38":"#Combining 'Extremely negative and Negative Together into single sentiment\nfor i in ['Extremely Negative', 'Negative']:\n    data.loc[data['Sentiment'] == i, \"Sentiment\"] = \"Negative\"","59c5c7f1":"# Combining 'Extremely Postive and Positive Together' into single sentiment\nfor i in ['Extremely Positive', 'Positive']:\n    data.loc[data['Sentiment'] == i, \"Sentiment\"] = \"Positive\"\n","2b999a01":"data['Sentiment'].value_counts()","a24440ac":"sns.countplot(data['Sentiment'], data=data)","8433db7d":"X = data[\"OriginalTweet\"]\ny = data['Sentiment']","ebaa3b94":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom tensorflow.keras.preprocessing.text import one_hot","549283fc":"message = X.copy()","181ae955":"message.head()","843d1678":"message.shape","d7ee09ff":"## Text cleaning\ncorpus = []\nps = PorterStemmer()\n\nfor i in range(len(message)):\n    \n    #removing everything other than the alphabets\n    review = re.sub('[^a-zA-Z]',\" \", string=str(message[i]))\n    \n    #remove urls\n    text = re.sub(r'http\\S+', \" \", str(message[i]))\n    \n    #remove mentions\n    text = re.sub(r'@\\w+', \" \", str(message[i]))\n    \n    #remove hastags\n    text = re.sub(r'#\\w+', \" \", str(message[i]))\n    \n    #remove the html tags\n    text = re.sub('r<.*?>', \" \", str(message[i]))\n    \n    #lowering the text\n    review = review.lower()\n    \n    #Converting into a list\n    review = review.split()\n    \n    #Removing the stopwords\n    review = [ps.stem(word) for word in review if not word in stopwords.words(\"english\")]\n\n    #Joining the list\n    review = \" \".join(review)\n    \n    corpus.append(review)","f54fa59d":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)","7d7dd751":"voc_size = 5000","4191c530":"# we will do one_hot encoding for the corpus. It will allocate every word an index according to the vocabulary size\n\none_hot = [one_hot(words, voc_size) for words in corpus]","8525ee21":"len(one_hot)","d4283801":"from tensorflow.keras.preprocessing.sequence import pad_sequences","d791334a":"embedded_docs = pad_sequences(one_hot, padding='pre', maxlen= 305)","dcc90807":"embedded_docs","bc156d01":"from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalAveragePooling1D, Input, GlobalMaxPool1D\nfrom tensorflow.keras.models import Sequential\nfrom keras.utils.np_utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam","25bbde7c":"model = Sequential([\n    Embedding(voc_size+1,305, input_length=len(embedded_docs[0])),\n    Dropout(0.5),\n    Bidirectional(LSTM(200, return_sequences=True)),\n    Dropout(0.5),\n    GlobalMaxPool1D(),\n    Dropout(0.5),\n    Dense(3, activation='softmax')\n])\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","6c6f667e":"len(embedded_docs), y.shape","cd169f92":"y = to_categorical(y, 3)\ny","f6ddc546":"# Creating new independent and dependent varibles\nX_final = np.array(embedded_docs)\ny_final = np.array(y)","dd1367bf":"X_final","2223e6cb":"y_final","48faa39c":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X_final,y_final,test_size=0.3,random_state=0)","5bb85168":"history=model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=5,batch_size=64)\n","6781ce8c":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'bo', label ='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label= 'Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label ='Training loss')\nplt.plot(epochs, val_loss, 'b', label= 'Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show","43bf12e0":"### Training the Model","0bb1e7db":"### Visualizing the result","bec1961b":"### Data Cleaning and Preprocessing","3145a4c6":"### Combining Similiar Sentiments","801739c9":"### Exploratory Data Analysis","3f48a443":"## Encoding the dependent variable","ec9b0ce4":"### Model Creation","c234da50":"### Data Preprocessing","3a059124":"#### Import NLP Libraries"}}