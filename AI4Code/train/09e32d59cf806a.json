{"cell_type":{"51fa2ee0":"code","081a9bab":"code","184e9665":"code","69856af6":"code","b96f46fa":"code","35d906dc":"code","a74b5356":"code","dad0487b":"code","464f3bcd":"code","5b35f891":"code","63b8eff8":"code","45a0edb4":"code","c52718e3":"code","09c65880":"code","515d02d8":"code","190a7460":"code","90b817ce":"code","8c23b341":"markdown","4ccfbe50":"markdown","e556b890":"markdown","a2be6f1c":"markdown","c820e1e8":"markdown","8d8422c0":"markdown","66ca66ae":"markdown","038db2f1":"markdown","739e05a8":"markdown","f2faa2c2":"markdown","f27a461b":"markdown","a617c32e":"markdown","d4c7c8dc":"markdown","cefa8a20":"markdown"},"source":{"51fa2ee0":"import nltk\nfrom nltk import FreqDist\nnltk.download('stopwords') # run this one time\n\nimport pandas as pd\npd.set_option(\"display.max_colwidth\", 200)\nimport numpy as np\nimport re\nimport spacy\nimport json\n\nimport gensim\nfrom gensim import corpora\n\n# libraries for visualization\nimport pyLDAvis\nimport pyLDAvis.gensim\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","081a9bab":"df = pd.read_json('..\/input\/Automotive_5.json', lines=True)\ndf.head()","184e9665":"def freq_words(x, terms = 30):\n  all_words = ' '.join([text for text in x])\n  all_words = all_words.split()\n\n  fdist = FreqDist(all_words)\n  words_df = pd.DataFrame({'word':list(fdist.keys()), 'count':list(fdist.values())})\n\n  # selecting top 20 most frequent words\n  d = words_df.nlargest(columns=\"count\", n = terms) \n  plt.figure(figsize=(20,5))\n  ax = sns.barplot(data=d, x= \"word\", y = \"count\")\n  ax.set(ylabel = 'Count')\n  plt.show()","69856af6":"freq_words(df['reviewText'])","b96f46fa":"# remove unwanted characters, numbers and symbols\ndf['reviewText'] = df['reviewText'].str.replace(\"[^a-zA-Z#]\", \" \")","35d906dc":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","a74b5356":"def remove_stopwords(rev):\n    rev_new = \" \".join([i for i in rev if i not in stop_words])\n    return rev_new\n\n# remove short words (length < 3)\ndf['reviewText'] = df['reviewText'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n\n# remove stopwords from the text\nreviews = [remove_stopwords(r.split()) for r in df['reviewText']]\n\n# make entire text lowercase\nreviews = [r.lower() for r in reviews]","dad0487b":"freq_words(reviews, 35)","464f3bcd":"!python -m spacy download en # one time run","5b35f891":"nlp = spacy.load('en', disable=['parser', 'ner'])\n\ndef lemmatization(texts, tags=['NOUN', 'ADJ']): # filter noun and adjective\n       output = []\n       for sent in texts:\n             doc = nlp(\" \".join(sent)) \n             output.append([token.lemma_ for token in doc if token.pos_ in tags])\n       return output","63b8eff8":"tokenized_reviews = pd.Series(reviews).apply(lambda x: x.split())\nprint(tokenized_reviews[1])","45a0edb4":"reviews_2 = lemmatization(tokenized_reviews)\nprint(reviews_2[1]) # print lemmatized review","c52718e3":"reviews_3 = []\nfor i in range(len(reviews_2)):\n    reviews_3.append(' '.join(reviews_2[i]))\n\ndf['reviews'] = reviews_3\n\nfreq_words(df['reviews'], 35)","09c65880":"dictionary = corpora.Dictionary(reviews_2)","515d02d8":"doc_term_matrix = [dictionary.doc2bow(rev) for rev in reviews_2]\n\n# Creating the object for LDA model using gensim library\nLDA = gensim.models.ldamodel.LdaModel\n\n# Build LDA model\nlda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=7, random_state=100,\n                chunksize=1000, passes=50)","190a7460":"lda_model.print_topics()","90b817ce":"# Visualize the topics\npyLDAvis.enable_notebook()\nvis = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)\nvis","8c23b341":"**Topics Visualization**","4ccfbe50":"Most common words are \u2018the\u2019, \u2018and\u2019, \u2018to\u2019, so on and so forth. These words are not so important for our task and they do not tell any story. We\u2019 have to get rid of these kinds of words. Before that let\u2019s remove the punctuations and numbers from our text data.","e556b890":"**Topic Modeling for this task**?\n\nTopic Modeling is a process to automatically identify topics present in a text object and to derive hidden patterns exhibited by a text corpus. Topic Models are very useful for multiple purposes, including:\n*     Document clustering\n*     Organizing large blocks of textual data\n*     Information retrieval from unstructured text\n*     Feature selection\n","a2be6f1c":"To further remove noise from the text we can use lemmatization from the spaCy library. It reduces any given word to its base form thereby reducing multiple forms of a word to a single word.","c820e1e8":"Let\u2019s try to remove the stopwords and short words (<2 letters) from the reviews","8d8422c0":"**Data Preprocessing**\n\nIn this step, we will remove the punctuations, stopwords and normalize the reviews as much as possible. After every preprocessing step, it is a good practice to check the most frequent words in the data. Therefore, let\u2019s define a function that would plot a bar graph of n most frequent words in the data.","66ca66ae":"It seems that now most frequent terms in our data are relevant. We can now go ahead and start building our topic model.","038db2f1":"We have not just lemmatized the words but also filtered only nouns and adjectives. Let\u2019s de-tokenize the lemmatized reviews and plot the most common words.","739e05a8":"The fifth topic Topic 4 has terms like \u2018towel\u2019, \u2018clean\u2019, \u2018wax\u2019, \u2018water\u2019, indicating that the topic is very much related to car-wash.","f2faa2c2":"We can see some improvement here. Terms like \u2018battery\u2019, \u2018price\u2019, \u2018product\u2019, \u2018oil\u2019 have come up which are quite relevant for the Automotive category. However, we still have neutral terms like \u2018the\u2019, \u2018this\u2019, \u2018much\u2019, \u2018they\u2019 which are not that relevant.","f27a461b":"**Building an LDA model**\n\nWe will start by creating the term dictionary of our corpus, where every unique term is assigned an index","a617c32e":"Convert the list of reviews (reviews_2) into a Document Term Matrix using the dictionary prepared above","d4c7c8dc":"![Topic](http:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2018\/10\/topic_model.png)","cefa8a20":"**If you like this please UPVOTE, it help me to keep motivated**"}}