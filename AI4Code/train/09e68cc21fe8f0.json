{"cell_type":{"05f15061":"code","493af0c4":"code","a45ee875":"code","19a3bb52":"code","f8d53488":"code","1b6ed222":"code","33c5f19d":"code","09d3d731":"code","c09e2502":"code","b1d7161d":"code","d68362e9":"code","c3d06055":"code","e8cec2e4":"code","317b86b0":"code","4ec40939":"code","82df5bb4":"code","79a03d28":"code","c96d1b03":"code","d6e305e3":"code","3766ad32":"code","2baa9f3c":"code","f265263a":"code","8872277a":"code","322fe25e":"code","21bf8296":"code","975da99f":"code","c2f52cd5":"code","30832aa1":"code","6e9b7a39":"code","6d52f2a2":"code","ae6ba40f":"code","48d6adc1":"code","e6282547":"code","e87d8273":"code","aeb661d1":"code","d9a3e833":"code","9767eddd":"code","225e6eba":"code","1042427d":"code","58134409":"code","c954f788":"code","623bc6d1":"code","f32b7cc2":"code","2a33f1ec":"markdown","1f7bc7ef":"markdown","718e1ca0":"markdown","1196509c":"markdown","72f96bae":"markdown","9a0bfe79":"markdown","2831f3a7":"markdown","5702e72b":"markdown","1983b93f":"markdown","5bdb7f59":"markdown","5b620d40":"markdown","c898e869":"markdown","9186daa7":"markdown","df6bbdbc":"markdown","fabab725":"markdown","ca919f4b":"markdown","b3e4af01":"markdown","2235df0e":"markdown","58003c39":"markdown","68e48160":"markdown","ee941007":"markdown","9d909f0f":"markdown","ae944af7":"markdown","6fbc213e":"markdown","0dd6cb79":"markdown","9da2a6f7":"markdown","4d1085d0":"markdown","49337765":"markdown","a0285625":"markdown","3c41e2eb":"markdown","c9438772":"markdown","533d30b9":"markdown","96032396":"markdown","f17e6aeb":"markdown","9c07a1c2":"markdown","c2338fff":"markdown","d286750a":"markdown"},"source":{"05f15061":"!pip install opencv-contrib-python\n#!pip uninstall opencv-contrib-python opencv-python opencv-python-headless opencv-contrib-python-headless -y\n","493af0c4":"from IPython.display import HTML\nHTML('<center><iframe width=\"700\" height=\"400\" src=\"https:\/\/www.youtube.com\/embed\/6YXKt0bMBJA\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen><\/iframe><\/center>')","a45ee875":"import os\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport tensorflow as tf\n\nimport cv2\nfrom random import randint\n\nimport numpy as np","19a3bb52":"CLASSES, gems = [], [] # names of classes, count of images for each class\n\nfor root, dirs, files in os.walk('\/kaggle\/input\/gemstones-images'):\n    f = os.path.basename(root)    # get class name - Amethyst, Onyx, etc    \n        \n    if len(files) > 0:\n        gems.append(len(files))\n        if f not in CLASSES:\n            CLASSES.append(f) # add folder name\n    \n    # uncomment this block if you want a text output about each subfolder\n#     count_dirs = 0\n#     for f in dirs:           # count subfolders\n#         count_dirs += 1\n#     depth = root.split(os.sep)\n#     print((len(depth) - 2) * '--'+'>', '{}:\\t {} folders, {} imgs'.format(os.path.basename(root), count_dirs, gems[-1] if gems!=[] else 0)) \n    \ngems_count = len(CLASSES) # 87 = number of classes\nprint('{} classes with {} images in total'.format(len(CLASSES), sum(gems)))\n\nf, ax = plt.subplots(figsize=(15,6))\nif(gems[0])<10:\n    plt.bar(range(gems_count), gems[gems_count:], label = 'Train data')\n    plt.bar(range(gems_count), gems[0:gems_count], label = 'Test data')\nelse:\n    plt.bar(range(gems_count), gems[0:gems_count], label = 'Train data')\n    plt.bar(range(gems_count), gems[gems_count:], label = 'Test data')\nax.grid()\nax.legend(fontsize = 12);","f8d53488":"img_w, img_h = 128, 128    # width and height of image\nhist_w, hist_h = 192, 192    # width and height of image\ntrain_dir = '\/kaggle\/input\/gemstones-images\/train\/'","1b6ed222":"def read_imgs_lbls(_dir):\n    Images, Labels, Hists, Masks, Results = [], [], [], [], []\n    for root, dirs, files in os.walk(_dir):\n        f = os.path.basename(root)  # get class name - Amethyst, Onyx, etc       \n        for file in files:\n            Labels.append(f)\n            try:\n                image = cv2.imread(root+'\/'+file)\n                \n                hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n                original = image.copy()\n                hsv_lower = np.array([0,0,20])\n                hsv_upper = np.array([255,255,255])\n                mask = cv2.inRange(hsv, hsv_lower, hsv_upper)\n                result = cv2.bitwise_and(original, original, mask=mask)\n                \n                image = cv2.resize(image,(int(img_w*1.5), int(img_h*1.5)))       # resize the image (images are different sizes)\n                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB);\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # converts an image from BGR color space to RGB\n                image_rgb[:,:,0] = cv2.equalizeHist(image_rgb[:,:,0])\n                image_output = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)\n\n                Images.append(image)\n                Hists.append(image_output)\n                Masks.append(mask)\n                Results.append(result)\n            except Exception as e:\n                print(e)\n    Images = np.array(Images)\n    Hists = np.array(Hists)\n    Masks = np.array(Masks)\n    Results = np.array(Results)\n    return (Images, Labels, Hists, Masks, Results)","33c5f19d":"def get_class_index(Labels):\n    for i, n in enumerate(Labels):\n        for j, k in enumerate(CLASSES):    # foreach CLASSES\n            if n == k:\n                Labels[i] = j\n    Labels = np.array(Labels)\n    return Labels","09d3d731":"Train_Imgs, Train_Lbls, Train_Hists, Train_Masks, Train_Results = read_imgs_lbls(train_dir)\nTrain_Lbls = get_class_index(Train_Lbls)\nprint('Shape of train images: {}'.format(Train_Imgs.shape))\nprint('Shape of train histograms: {}'.format(Train_Hists.shape))\nprint('Shape of train labels: {}'.format(Train_Lbls.shape))","c09e2502":"dim = 4 #you can change it;  4x4 dimension flat plot\n\nf,ax = plt.subplots(dim,dim) \nf.subplots_adjust(0,0,2,2)\nfor i in range(0,dim):\n    for j in range(0,dim):\n        rnd_number = randint(0,len(Train_Imgs))\n        cl = Train_Lbls[rnd_number]\n        ax[i,j].imshow(Train_Results[rnd_number])\n        ax[i,j].set_title(CLASSES[cl]+': ' + str(cl))\n        ax[i,j].axis('off')","b1d7161d":"dim = 4 #you can change it;  4x4 dimension flat plot\n\nf,ax = plt.subplots(dim,dim) \nf.subplots_adjust(0,0,2,2)\nfor i in range(0,dim):\n    for j in range(0,dim):\n        rnd_number = randint(0,len(Train_Imgs)-1)\n        cl = Train_Lbls[rnd_number]\n        ax[i,j].imshow(Train_Imgs[rnd_number])\n        ax[i,j].set_title(CLASSES[cl]+': ' + str(cl))\n        ax[i,j].axis('off')","d68362e9":"high_thresh, low_thresh = 60, 40\ndef edge_and_cut(img):\n    try:\n        img = cv2.GaussianBlur(img, (7, 7), 0)\n        edges = cv2.Canny(img, high_thresh, low_thresh)\n        #img = remove_shadows(img)\n        \n        if(np.count_nonzero(edges)>edges.size\/10000):           \n            pts = np.argwhere(edges>0)\n            y1,x1 = pts.min(axis=0)\n            y2,x2 = pts.max(axis=0)\n            \n            new_img = img[y1:y2, x1:x2]           # crop the region\n            new_img = cv2.resize(new_img,(img_w, img_h))  # Convert back\n        else:\n            new_img = cv2.resize(img,(img_w, img_h))\n    \n    except Exception as e:\n        print(e)\n        new_img = cv2.resize(img,(img_w, img_h))\n    \n    return new_img","c3d06055":"def remove_background(img):\n    rgb_planes = cv2.split(img)\n\n    result_planes = []\n    result_norm_planes = []\n    for plane in rgb_planes:\n        dilated_img = cv2.dilate(plane, np.ones((3,3), np.uint8), iterations=1)\n        bg_img = cv2.medianBlur(dilated_img, 21)\n        diff_img = 255 - cv2.absdiff(plane, bg_img)\n        norm_img = cv2.normalize(diff_img,None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)\n        result_planes.append(diff_img)\n        result_norm_planes.append(norm_img)\n\n    result = cv2.merge(result_planes)\n    result_norm = cv2.merge(result_norm_planes)\n    \n    return result","e8cec2e4":"\ndef show_cropped(img):\n    emb_img = img.copy()\n    high_thresh, low_thresh = 60, 40\n    edges = cv2.Canny(img, high_thresh, low_thresh)\n    #img = remove_background(img)\n    \n    if(np.count_nonzero(edges)>edges.size\/10000):\n        pts = np.argwhere(edges>0)\n        y1,x1 = pts.min(axis=0)\n        y2,x2 = pts.max(axis=0)\n\n        new_img = img[y1:y2, x1:x2]  \n\n        edge_size = 2 #replace it with bigger size for larger images            \n\n        emb_img[y1-edge_size:y1+edge_size, x1:x2] = [255, 0, 0]\n        emb_img[y2-edge_size:y2+edge_size, x1:x2] = [255, 0, 0]\n        emb_img[y1:y2, x1-edge_size:x1+edge_size] = [255, 0, 0]\n        emb_img[y1:y2, x2-edge_size:x2+edge_size] = [255, 0, 0]\n\n        new_img = cv2.resize(new_img,(img_w, img_h))  # Convert to primary size  \n        \n    else:\n        new_img = cv2.resize(img,(img_w, img_h))\n    \n    fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(10, 10))\n    ax[0].imshow(img, cmap='gray')\n    ax[0].set_title('Original Image', fontsize=14)\n    ax[1].imshow(edges, cmap='gray')\n    ax[1].set_title('Canny Edges', fontsize=14)\n    ax[2].imshow(emb_img, cmap='gray')\n    ax[2].set_title('Bounding Box', fontsize=14)       \n    ax[3].imshow(new_img, cmap='gray')\n    ax[3].set_title('Cropped', fontsize=14)   ","317b86b0":"for x in range(0,3):\n    show_cropped(Train_Results[randint(0,len(Train_Imgs))])","4ec40939":"def crop_images(Imgs):\n    CroppedImages = np.ndarray(shape=(len(Imgs), img_w, img_h, 3), dtype=np.int)\n\n    ind = 0\n    for im in Imgs: \n        x = edge_and_cut(im)\n        CroppedImages[ind] = x\n        ind += 1\n\n    return CroppedImages","82df5bb4":"Train_Imgs = crop_images(Train_Imgs)\nTrain_Results = crop_images(Train_Results)\nprint('Final shape of images in train set: {} '.format(Train_Imgs.shape))\nprint('Final shape of images in train set: {} '.format(Train_Results.shape))","79a03d28":"from sklearn.model_selection import train_test_split\nX1_train, X1_val, y1_train, y1_val = train_test_split(Train_Results, Train_Lbls, shuffle = True, test_size = 0.2, random_state = 42)\nprint('Shape of X_train: {}, y_train: {} '.format(X1_train.shape, y1_train.shape))\nprint('Shape of X_val: {}, y_val: {} '.format(X1_val.shape, y1_val.shape))","c96d1b03":"# X2_train, X2_val, y2_train, y2_val = train_test_split(Train_Imgs, Train_Lbls, shuffle = True, test_size = 0.2, random_state = 42)\n\n# print('Shape of X_train: {}, y_train: {} '.format(X2_train.shape, y2_train.shape))\n# print('Shape of X_val: {}, y_val: {} '.format(X2_val.shape, y2_val.shape))","d6e305e3":"from tensorflow.python.client import device_lib\ndevices = device_lib.list_local_devices()\nprint(devices)","3766ad32":"#TODO: This will probably be where I start editing. \n# remember that the images are 48 x 48\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras import optimizers","2baa9f3c":"filters = 32      # the dimensionality of the output space\nkernel_size = 3   # length of the 2D convolution window\nmax_pool = 2      # size of the max pooling windows","f265263a":"EPOCHS = 40                                  # while testing you can change it\nbatch_size = 64 # number of training samples using in each mini batch during GD (gradient descent) \niter_per_epoch1 = len(X1_train) \/\/ batch_size  # each sample will be passed [iter_per_epoch] times during training\nval_per_epoch1 = len(X1_val) \/\/ batch_size     # each sample will be passed [val_per_epoch] times during validation\n\n# iter_per_epoch2 = len(X2_train) \/\/ batch_size  # each sample will be passed [iter_per_epoch] times during training\n# val_per_epoch2 = len(X2_val) \/\/ batch_size     # each sample will be passed [val_per_epoch] times during validation\nprint(iter_per_epoch1)","8872277a":"model1 = Sequential()\n\n# first layer\nmodel1.add(Conv2D(batch_size, (kernel_size, kernel_size), activation='relu', padding='same', input_shape=(img_w, img_h, 3))) # 32\nmodel1.add(MaxPooling2D((max_pool, max_pool))) #reduce the spatial size of incoming features\n\n# second layer\nmodel1.add(Conv2D(2*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 64\nmodel1.add(MaxPooling2D((max_pool, max_pool))) \nmodel1.add(Dropout(0.2))\n\n# third layer\nmodel1.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\nmodel1.add(MaxPooling2D((max_pool, max_pool))) \nmodel1.add(Dropout(0.2))\n\n# fourth layer\nmodel1.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\nmodel1.add(AveragePooling2D(pool_size= (2, 2), strides= (2, 2))) \nmodel1.add(Dropout(0.2))\n\n# fifth layer\nmodel1.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\nmodel1.add(MaxPooling2D((max_pool, max_pool))) \nmodel1.add(Dropout(0.2))\n\nmodel1.add(Flatten())\nmodel1.add(Dense(4*batch_size, activation='relu'))                                             # 512\nmodel1.add(Dense(87, activation='softmax'))\nmodel1.summary()","322fe25e":"# model2 = Sequential()\n\n# # first layer\n# model2.add(Conv2D(batch_size, (kernel_size, kernel_size), activation='relu', padding='same', input_shape=(img_w, img_h, 3))) # 32\n# model2.add(MaxPooling2D((max_pool, max_pool))) #reduce the spatial size of incoming features\n\n# # second layer\n# model2.add(Conv2D(2*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 64\n# model2.add(MaxPooling2D((max_pool, max_pool))) \n\n# # third layer\n# model2.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\n# model2.add(MaxPooling2D((max_pool, max_pool))) \n\n# # fourth layer\n# model2.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\n# model2.add(AveragePooling2D(pool_size= (2, 2), strides= (2, 2))) \n\n# # fifth layer\n# model2.add(Conv2D(4*batch_size, (kernel_size, kernel_size), activation='relu', padding='same')) # 128\n# model2.add(MaxPooling2D((max_pool, max_pool))) \n\n# model2.add(Flatten())\n# model2.add(Dropout(0.75))\n# model2.add(Dense(16*batch_size, activation='relu'))                                             # 512\n# model2.add(Dense(87, activation='softmax'))\n# model2.summary()","21bf8296":"model1.compile(optimizer=optimizers.Adam(learning_rate=.001),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n# model2.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])","975da99f":"from keras.preprocessing.image import ImageDataGenerator\n\n# TODO: I think I'll want to change here\ntrain_datagen = ImageDataGenerator(              # this is the augmentation configuration used for training\n        rotation_range=25,\n        zoom_range=0.1,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        shear_range=0.2,\n        horizontal_flip=True,\n        vertical_flip=True,\n        channel_shift_range=0.5,\n        )\n\nval_datagen = ImageDataGenerator()                # for val\/testing only rescaling function ","c2f52cd5":"n = randint(0,len(X1_train))\nsamples = np.expand_dims(X1_train[n], 0)\nit = train_datagen.flow(samples, batch_size=batch_size)\ncols = 7\n\nfig, ax = plt.subplots(nrows=1, ncols=cols, figsize=(15, 10))\nax[0].imshow(X1_train[n], cmap='gray')\nax[0].set_title('Original', fontsize=10)\n\nfor i in range(1,cols):\n    batch = it.next()    # generate batch of images \n    image = batch[0].astype('uint32') # convert to unsigned int for viewing\n    ax[i].set_title('augmented {}'.format(i), fontsize=10)\n    ax[i].imshow(image, cmap='gray')","30832aa1":"# n = randint(0,len(X2_train))\n# samples = np.expand_dims(X2_train[n], 0)\n# it = train_datagen.flow(samples, batch_size=batch_size)\n# cols = 7\n\n# fig, ax = plt.subplots(nrows=1, ncols=cols, figsize=(15, 10))\n# ax[0].imshow(X2_train[n], cmap='gray')\n# ax[0].set_title('Original', fontsize=10)\n\n# for i in range(1,cols):\n#     batch = it.next()    # generate batch of images \n#     image = batch[0].astype('uint32') # convert to unsigned int for viewing\n#     ax[i].set_title('augmented {}'.format(i), fontsize=10)\n#     ax[i].imshow(image, cmap='gray')","6e9b7a39":"train_gen1 = train_datagen.flow(X1_train, y1_train, batch_size=batch_size)\nval_gen1 = val_datagen.flow(X1_val, y1_val, batch_size=batch_size)","6d52f2a2":"# train_gen2 = train_datagen.flow(X2_train, y2_train, batch_size=batch_size)\n# val_gen2 = val_datagen.flow(X2_val, y2_val, batch_size=batch_size)","ae6ba40f":"m1 = model1.fit_generator(\n       train_gen1,\n       steps_per_epoch= iter_per_epoch1,\n       epochs=EPOCHS, \n       validation_data = val_gen1,\n       validation_steps = val_per_epoch1,\n       verbose = 1 # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n       )","48d6adc1":"# m2 = model2.fit_generator(\n#        train_gen2,\n#        steps_per_epoch= iter_per_epoch2,\n#        epochs=EPOCHS, \n#        validation_data = val_gen2,\n#        validation_steps = val_per_epoch2,\n#        verbose = 1 # Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n#        )","e6282547":"fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\naxs[0].plot(m1.history['accuracy'])\naxs[0].plot(m1.history['val_accuracy'])\naxs[0].set_title('Model1 accuracy')\naxs[0].legend(['Train', 'Val'], loc='upper left')\n\naxs[1].plot(m1.history['loss'])\naxs[1].plot(m1.history['val_loss'])\naxs[1].set_title('Model1 loss')\naxs[1].legend(['Train', 'Val'], loc='upper left')\n\nfor ax in axs.flat:\n    ax.set(xlabel='Epoch')","e87d8273":"# fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(15,5))\n# axs[0].plot(m2.history['accuracy'])\n# axs[0].plot(m2.history['val_accuracy'])\n# axs[0].set_title('Model2 accuracy')\n# axs[0].legend(['Train', 'Val'], loc='upper left')\n\n# axs[1].plot(m2.history['loss'])\n# axs[1].plot(m2.history['val_loss'])\n# axs[1].set_title('Model2 loss')\n# axs[1].legend(['Train', 'Val'], loc='upper left')\n\n# for ax in axs.flat:\n#     ax.set(xlabel='Epoch')","aeb661d1":"score = model1.evaluate_generator(val_gen1, steps= len(val_gen1))\n\nfor idx, metric in enumerate(model1.metrics_names):\n    print('{}:{}'.format(metric, score[idx]))","d9a3e833":"from sklearn.metrics import confusion_matrix\ny_pre_test=model1.predict(X1_val)\ny_pre_test=np.argmax(y_pre_test,axis=1)\ncm=confusion_matrix(y1_val,y_pre_test)\n\nplt.figure(figsize = (15,15))\nsn.heatmap(cm, annot=True)","9767eddd":"x=(y_pre_test-y1_val!=0).tolist()\nx=[i for i,l in enumerate(x) if l!=False]\n\nfig,ax=plt.subplots(1,5,sharey=False,figsize=(13,13))\nfig.tight_layout()\n\nfor i in range(5):\n    ax[i].imshow(X1_val[x[i]][:,:,1])\n    ax[i].set_xlabel('{}, Pred: {}'.format(CLASSES[y1_val[x[i]]],CLASSES[y_pre_test[x[i]]]))","225e6eba":"red_stones = ['Almandine', 'Garnet Red', 'Hessonite', 'Pyrope', 'Rhodolite']\nred_stones = get_class_index(red_stones)\n\nfig,ax=plt.subplots(1,len(red_stones),sharey=False,figsize=(13,13))\nfig.tight_layout()\n\nfor i in range(len(red_stones)):\n    ax[i].imshow(Train_Imgs[np.where(Train_Lbls==red_stones[i])[0][1]])\n    ax[i].set_xlabel(CLASSES[red_stones[i]])","1042427d":"model1.save('model_gemstones.h5')","58134409":"test_dir = '\/kaggle\/input\/gemstones-images\/test\/'","c954f788":"Test_Imgs, Test_Lbls, Test_Hists, Test_Masks, Test_Results = read_imgs_lbls(test_dir)\nTest_Lbls = get_class_index(Test_Lbls)","623bc6d1":"Test_Results = crop_images(Test_Results)\nprint('shape of images in test set: {} '.format(Test_Results.shape))","f32b7cc2":"f,ax = plt.subplots(5,5) \nf.subplots_adjust(0,0,2,2)\nfor i in range(0,5,1):\n    for j in range(0,5,1):\n        rnd_number = randint(0,len(Test_Results))\n        pred_image = np.array([Test_Results[rnd_number]])\n        pred_class = model1.predict_classes(pred_image)[0]\n        pred_prob = model1.predict(pred_image).reshape(87)\n        act = CLASSES[Test_Lbls[rnd_number]]\n        ax[i,j].imshow(Test_Results[rnd_number])\n        ax[i,j].imshow(pred_image[0])\n        if(CLASSES[pred_class] != CLASSES[Test_Lbls[rnd_number]]):\n            t = '{} [{}]'.format(CLASSES[pred_class], CLASSES[Test_Lbls[rnd_number]])\n            ax[i,j].set_title(t, fontdict={'color': 'darkred'})\n        else:\n            t = '[OK] {}'.format(CLASSES[pred_class]) \n            ax[i,j].set_title(t)\n        ax[i,j].axis('off')","2a33f1ec":"Create two numpy array iterators `train_gen` and `val_gen` and fill them with additional images:","1f7bc7ef":"## 6. Save the model\n* Save weights to reuse them instead of training again. Keras function `save` creates h5 file with weights. Use `new_model.load_weights('model_gemstones.h5')` to reuse it in other models.","718e1ca0":"The model summary shows that there are more than 2M parameters to train and the information about different layers.\n\n## 2. Compile a model\n* Compile the model using `adam` optimizer which is a generalization of stochastic gradient descent (SGD) algo. Provided loss function is `sparse_categorical_crossentropy` as we are doing multiclass classification.   ","1196509c":"## 8. Replace train images with cropped images\n* Create function which calls `edge_and_cut` and replaces `Train_Imgs` numpy array with array of cropped images. Don't forget that images that cannot be cropped will be replced with originals;\n* Make sure the shape of final array is the same: NUMBER OF IMAGES x img_w x img_h x 3 (CHANNELS):","72f96bae":"## 2. Parameters to fit the model\n* **epoch** describes the number of times the algorithm sees the ENTIRE dataset. Each time the algo has seen all samples in the dataset, an epoch has completed.  \n* since one epoch is too big to feed to the memory at once divide it in several smaller **batches**. Batch size is always factor of 2. \n* **Iterations** per epoch = number of passes, each pass using batch size number of examples.   \n\nSo if we have ~2200 (80%) training samples, and batch size is 32, then it will take ~70 iterations to complete 1 epoch.","9a0bfe79":"## 2. Plot test images and model predictions\nPlot image from test folder with a label, class which model predicted, and actual class.","2831f3a7":"for x in range(0,1):\n    remove_shadows(Train_Imgs[randint(0,len(Train_Imgs))])","5702e72b":"## 4. Score the model\n> Accuracy is a metric for evaluating classification models. `Accuracy  = Number of right predictions \/ Total number of predictions`.\n\n* Function `evaluate_generator` evaluates the model on a data generator: `score` is a list of scalars (loss and accuracy).","1983b93f":"# I. Check data \n\nFirst, check the number of files in every gemstone class in folder `\/kaggle\/input\/gemstones-images`. Images are already divided into train (~2,800 images) and test (~400 images) data. Each class in train set contains `27 - 47` images, in test set - `4 - 6` images.\n* create list `CLASSES` which contains the names of 87 classes of gemstones based on folders names;\n* plot the distribution of data;\n* *uncomment block of code if you want a text output about each subfolder*.","5bdb7f59":"Color Histogram Equalization","5b620d40":"## 3. Create function which converts string labels to numbers\n* Convert string labels to a list of numbers using list `CLASSES`. The index will represent label of class, f.e. *Ruby = 0, Amethyst = 24*, etc.\n* when `Labels` list is ready - convert it to Numpy array.","c898e869":"## 7. Show cropped images\n* Function `show_cropped` is kind-of duplicate `edge_and_cut()`: it shows same random examples of Canny algo work: **original image, Canny edges, image with bounding box, cropped image** for better understanding how Canny algo works.","9186daa7":"## 2. Import keras\nKeras is an open-source neural-network library written in Python which is capable of running on top of **TensorFlow**.\nFrom Keras needed:\n* `models` - type of models, import only `Sequential` \n* `layers` - layers corresponding to our model: as it a simple one take only `Conv2D`, `MaxPooling2D` and `AveragePooling2D`\n* `optimizers` - contains back propagation algorithms\n* `ImageDataGenerator` - for image augmenation (there are not so many samples of each class)","df6bbdbc":"Create `Test_Imgs` and `Test_Lbls` absolutely the same as we did with training folder. Convert them to numpy arrays - there are 358 images for test. `Test_Lbls` array will help to check is the model predictions are correct.","fabab725":"Don't judge poor model. Just look at `Almandine`, `Garnet Red`, `Hessonite`, `Pyrope` and `Rhodolite`. Can you distinguish between them?","ca919f4b":"## 5. Confusion matrix   \n> Confusion matrix can be pretty useful when evaluating multiclass classifications.   \n\n* `from sklearn.metrics import confusion_matrix`: Diagonal of matrix should be mostly filled with numbers.","b3e4af01":"## 2. Create function which reads images and class names\n* this function will be also used with test images;\n* read each image from disk using `cv2` and resize it to `img_w*1.5, img_h*1.5`;\n* set `cv2.COLOR_BGR2RGB` option because opencv reads and displays an image as BGR color format instead of RGB color format. Without this option images will be shown in blue hue because `matplotlib` uses RGB to display image;\n* create a list of class names while reading folders - `Amethyst, Onyx, etc`;\n* when `Images` list is ready - convert it to Numpy array;\n* return tuple of 2 elements: Images and corresponding Labels.","2235df0e":"**ALMOST DONE!** \ud83d\ude08   \n\n## 3. Check the accuracy\n\n* plot the accuracy of model against size of epoch (train and val);\n* plot the loss of model against size of epoch (train and val).","58003c39":"* the original image + examples of work of `ImageDataGenerator`: ","68e48160":"## 4. Fill arrays of Images and corresponding Labels with data\n* Create two arrays `Train_Imgs, Train_Lbls` which contain images and corresponding names of classes of gemstones respectively;\n* Convert `Train_Lbls` with strings to list with corresponding numbers;\n* print the dimensions of both numpy arrays: `Train_Imgs` which stores pictures is 4-dimensional: **Number of images x Width of image x Height of image x Channel of image**.","ee941007":"# III. Fit the train generator\n\n## 1. Image augmentation\n> **Image augmentation** is a creation of additional training data based on existing images, for example translation, rotation, flips and zoom.\n\n* As far as there are not so many samples for every class add a train data generator using class `ImageDataGenerator` with augmentation parameters. Using `ImageDataGenerator` class from Keras library create additional images of each gemstone class in the memory.","9d909f0f":"# IV. Evaluate on testing folder\n\n## 1. Get samples from test folder\nCreate test data generator using class `ImageDataGenerator` and validate it providing the test directory `'\/kaggle\/input\/gemstones-images\/test\/'`","ae944af7":"# Preparation steps\n\n# I. Import necessary modules\n\n* `import os`: to read files from disk\n* `import cv2`: OpenCV is an image and video processing library  \n* `import matplotlib.pyplot as plt` and `import seaborn as sn`: to show images and graphs\n* `from random import randint`: to show random images\n* `import numpy as np`: linear algebra; all images will represent numpy-arrays.\n* other modules and their components will be imported later.","6fbc213e":"The accuracy of a model from scratch ~ 60%. Any suggestions on improving a model are taking up \ud83d\ude0a","0dd6cb79":"# Context\nWhat is a gemstone? A gemstone (gem, fine gem, jewel, precious stone, or semi-precious stone) is a piece of mineral crystal which, in cut and polished form, is used to make jewelry or other adornments. Certain rocks (such as lapis lazuli and opal) and occasionally organic materials that are not minerals (such as amber, jet, pearl) are also used for jewelry and are therefore often considered to be gemstones as well (source: [wiki](https:\/\/en.wikipedia.org\/wiki\/Gemstone)).\n\nA few gemstones are used as gems in the crystal or other form in which they are found. Most however, are cut and polished for usage as jewelry. \n\n![](https:\/\/lh3.googleusercontent.com\/proxy\/oKyVDQ5e2whCQYHdmOhN6FopacGApwNz9NxAy2WRToIj3Bhs82Z7vY94GjwPueOCtgTKmgXCOhgHUJVEF6h3JS2EPBeI9Mus3TfIZZa6FhLOW6rqZY1ewMWXVKjYpo6Ixw)\n\nI have already collected the dataset of [gemstones images](https:\/\/www.kaggle.com\/lsind18\/gemstones-images) with 3.200+ images of faceted gems. Gemstones' images are grouped into 87 classes with division into train and test data in ratio ~ 0,9 : 0,1. The images are in various sizes of .jpg format. All gemstones are faceted in various shapes - round, oval, square, rectangle, heart. \n\n## todo:: Build a simple convolutional neural network from scratch over the dataset of [gemstones images](https:\/\/www.kaggle.com\/lsind18\/gemstones-images).\n### <font color='red'>If you like this notebook please upvote! Have fun<\/font> \ud83c\udf1f   \n\n* Before we start the science, watch this short video to know how our beautiful nature forms gemstones:","9da2a6f7":"Orange small bars represent test data and blue bars represent train data. Data is normally distributed.\n\n# II. Prepare training data\n\n## 1. Prepare parameters\n\n* resize processed images to `img_w, img_h` - this option will be used when cropped and as a parameter of neural network; \n* provide train directory path.","4d1085d0":"* there is an overfitting: even though train and val accuracy are pretty close to each other, `val_loss` parameter often 'jumps'.","49337765":"Because of great amount of classes just **plot misclassified gemstones by model**. `numpy.argmax()` function returns the indices of maximum elements along the specific axis inside the array (`axis = 1` - 'horizontally').\n* Create a list of misclassified indexes which will be substitued into validation set `X_val`.  \n* Plot misclassified gemstones.","a0285625":"# III. Prepare for model creation\n## 1. Check devices\n\nUsing `tensorflow` check which devices uses the Kaggle platform.\n\n`XLA_CPU device`: CPU  \n`XLA_GPU device`: Tesla P100-PCIE-16GB (to accelerate computing use GPU mode).  \nXLA stands for *accelerated linear algebra*. It's Tensorflow's relatively optimizing compiler that can further speed up ML models.  \n\nRun this notebook with GPU mode: for example, using image size 190 x 190 and basic architecture of CNN mentioned above every epoch on CPU takes ~3 minutes, on GPU ~ 15 sec.","3c41e2eb":"# Conclusion\n\nThe model tries! Finally it understands the color: some gemstones are really similar.  \n\n\n![Diamonds? really?](https:\/\/cdn.leibish.com\/media\/mediabank\/blue-diamond-scale_1634.c8fbb.jpg)\n\n\n### **Please upvote this kernel if you find it useful** \ud83d\ude4b  \nFeel free to give any suggestions to improve my code.","c9438772":"## 5. Plot images and their labels for preview\n* Using `matplotlib` and `random` show 16 (4x4) random images from the set and their labels (as string and as int number).","533d30b9":"# Build a simple CNN  \nCNN (Convolutional neural network or ConvNet) is a class of deep neural networks, commonly applied to analyzing visual imagery. Here is the simpliest example of CNN with few layers using `Conv2D` - 2D convolution layer (spatial convolution over images) and `MaxPooling2D` - application of a moving window across a 2D input space.\n\n# I. Provide Hyperparameters\nHyperparameters are set before training; they represent the variables which determines the neural network structure and how the it is trained.  \n  \n## 1. Parameters for layers\n* Convolutional layer filter size (`filters`). The number of filters should depend on the complexity of dataset and the depth of neural network. A common setting to start with is [32, 64, 128] for three layers.  \n* `kernel_size` = number of filters  = a small window of pixels at a time (3\u00d73) which will be moved until the entire image is scanned. If images are smaller than 128\u00d7128, work with smaller filters of 1\u00d71;\n* Width and Height of images were already provided. 2D convolutional layers take a three-dimensional input, typically an image with three color channels;\n* `max_pool` = max pooling is the application of a moving window across a 2D input space, where the maximum value within that window is the output: 2x2. ","96032396":"# II. Provide a model\n\n## 1. Architect a model\nThe Sequential model is a linear stack of layers.\n* I use a kind of VGG network architecture:\n\n|   |       Layers       |\n|:-:|:------------------:|\n| 1 |  Conv2D 32 -> Pool |\n| 2 |  Conv2D 64 -> Pool |\n| 3 | Conv2D 128 -> Pool |\n| 4 | Conv2D 128 -> Pool |\n| 5 | Conv2D 128 -> Pool |\n| 6 |        FLAT        |\n| 7 |        Drop        |\n| 8 |      Dense 512     |\n| 9 | Dense len(CLASSES) |\n\n\n1. ADD 5 'blocks': \n   *  Conv2D with hypermarameters mantioned above: `Conv2D(kernel_size, (filters, filters), input_shape=(img_w, img_h, 3))` with activation function for each layer as a Rectified Linear Unit (ReLU): `Activation('relu')`  \n   * MaxPooling2D layer to reduce the spatial size of the incoming features; 2D input space: `MaxPooling2D(pool_size=(max_pool, max_pool))`  \n   * Do the same increading the kernel size: 32 -> 64 -> 128 -> 128 -> 128\n\n2. Flatten the input: transform the multidimensional vector into a single dimensional vector: `Flatten()`\n3. Add dropout layer which randomly sets a certain fraction of its input to 0 and helps to reduce overfitting: `Dropout(0.5)`\n5. Add fully connected layer with 512 nodes and activation function relu: `Dense(512), Activation('relu')`\n6. Provide last fully connected layer which specifies the number of classes of gemstones: **87**. `Softmax` activation function outputs a vector that represents the probability distributions of a list of potential outcomes: `Dense(87, activation='softmax')`   \n\n\n* Print the summary of the model.","f17e6aeb":"## 2. Crop test images","9c07a1c2":"From 4x4 plot above we can see that gemstones are mostly centered; but let's try to crop the edges.\n\n## 6. Crop edges of images using Canny algorithm\n> Canny is a popular edge detection algorithm, which detects the edges of objects present in an image.\n\n* Using `cv2.Canny` find the array representing frame which is the edges how the original picture will be cut;\n* Function `edge_and_cut(img)` receives single image and returns a **cropped image (`new_img`)** of the size `img_w, img_h`;\n* sometimes Canny algo cannot detect edges (f.e. when the object has almost same color as background) so array `edges` will be zero-valued. In this case use original image.","c2338fff":"## 9. Split data into train and validation sets\n* use `sklearn` to split `Train_Imgs`, `Train_Lbls` into train (80%) and validation (20%) sets. **Important!**","d286750a":"## 2. Fit the model\n* get a history object\n* If you see that `val_los` parameter is increasing that is *overfitting*. It happens when your model explains the training data too well, rather than picking up patterns that can help generalize over unseen data."}}