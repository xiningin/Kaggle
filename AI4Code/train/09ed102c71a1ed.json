{"cell_type":{"52617f59":"code","0a8160ab":"code","c684c28b":"code","f10b4133":"code","0b6cef95":"code","a8afef05":"code","1e8f2a7b":"code","e7e053b4":"code","e0b44469":"code","2f30ce3d":"code","2502c9b4":"code","6b1735ce":"code","0bb75d29":"code","f8e173f7":"code","639510bd":"code","aad81208":"code","0ca526af":"code","f867d031":"code","61ef22ba":"code","95bb00b3":"code","d524b3fb":"code","023dd60b":"code","1ac62253":"code","5d969e3b":"code","808e3c8a":"code","198d01e2":"code","2fa8a6dc":"code","651589b2":"code","0130b1ec":"code","34b4d273":"code","4b601d2f":"code","6625784a":"code","ba86d086":"code","4c8bd0ad":"code","448ad40f":"code","8350b47a":"code","aefeab8f":"code","ce679d34":"code","f68e9ed7":"code","178d21f6":"code","8411c79c":"code","71d92ff7":"code","ff2d251e":"code","9839bba9":"code","a1972e7a":"code","adba66cf":"code","a23a4c3d":"code","0aa354aa":"code","c67324c4":"code","b5342b3c":"code","b04b1311":"code","240302e4":"code","8ca6c091":"code","341101e0":"code","2aa4d941":"code","dc514da5":"code","fedf919c":"code","a3e84e9d":"code","b8b7664b":"code","c61e5931":"code","420897ed":"code","6937f38f":"code","cad7c749":"code","84c58db8":"code","e66dadf8":"code","6f39c03b":"code","d0b319ea":"markdown","62a0a301":"markdown","dde54d04":"markdown","ef3749dd":"markdown","15ca2af1":"markdown","303f47aa":"markdown","ba398d8f":"markdown","332d9cb2":"markdown","7bd29817":"markdown","ab8ec5ca":"markdown","d7be8810":"markdown","5e7e9511":"markdown","e6afd5c6":"markdown","1092697e":"markdown","a1605d17":"markdown","6505e2e8":"markdown","aeaf4314":"markdown","7fb0ce60":"markdown","befc2b47":"markdown","66714040":"markdown","512e7fe5":"markdown","40e03532":"markdown","fa77e5ca":"markdown","9941a0c5":"markdown","205c0c98":"markdown","454dcee1":"markdown","886a0534":"markdown","f81bed1e":"markdown","81a70ddd":"markdown","b2f9e389":"markdown","973ad0c7":"markdown","4d8232f7":"markdown","f9bf27c8":"markdown","83beb4ab":"markdown","2912921c":"markdown","1e88ee75":"markdown","a1db747d":"markdown","43c070a2":"markdown","7911e92d":"markdown","8ad78256":"markdown","979d4a24":"markdown","85fa6e10":"markdown","a91aae57":"markdown","fbb5dee2":"markdown","35620604":"markdown","743fc294":"markdown","64315c07":"markdown","c01c3b6f":"markdown","5206aeb8":"markdown","99202a35":"markdown","25d62e13":"markdown","317c3921":"markdown","991ec8a2":"markdown","47a6da6d":"markdown","55ad1303":"markdown","458ed3c8":"markdown","876ad3e4":"markdown","703c8170":"markdown","e2706f5e":"markdown","66ae9524":"markdown","6ee3ef40":"markdown","8539a473":"markdown","5fbbeefe":"markdown","49e2128d":"markdown","ad713baa":"markdown","dfa521c7":"markdown","424b1e0a":"markdown","363e537b":"markdown","ce52435a":"markdown","b2f3682f":"markdown","624cb552":"markdown","2150e799":"markdown","5172aa97":"markdown","69aaaed8":"markdown","e4ce3ded":"markdown","6c071bce":"markdown","aa8ae71f":"markdown","65a0ca50":"markdown","ec6e5bd1":"markdown","613e275a":"markdown","add38555":"markdown","6dbc258c":"markdown","9cb33464":"markdown","be7041ed":"markdown","5739c0a6":"markdown","0e550bdb":"markdown","062f27b8":"markdown","db4ab893":"markdown","02e33621":"markdown","808b8913":"markdown","b0baa4a3":"markdown","971c0022":"markdown","850aee73":"markdown","7e2f2a4d":"markdown"},"source":{"52617f59":"import pandas as pd\nimport numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, roc_auc_score, roc_curve, precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve, accuracy_score\nfrom catboost import CatBoostClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom catboost import Pool, cv\nfrom sklearn.utils import shuffle\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","0a8160ab":"df = pd.read_csv('..\/input\/bank-customer-churn-modeling\/Churn_Modelling.csv')\ndf.head()","c684c28b":"df.columns = df.columns.str.lower()\ndf.columns","f10b4133":"df.columns = ['row_number', 'customer_id', 'surname', 'creditscore', 'geography',\n       'gender', 'age', 'tenure', 'balance', 'num_of_products', 'has_crcard',\n       'isactive_member', 'estimated_salary', 'exited']\ndf.columns","0b6cef95":"print(df.shape)\ndf.info()","a8afef05":"df.dtypes","1e8f2a7b":"df.describe().T","e7e053b4":"df['exited'].value_counts().to_frame()","e0b44469":"print('Percentage of positive marks: {:.2%}'.format(df['exited'].mean()))","2f30ce3d":"df.hist(bins=50, figsize=(20,15), edgecolor='black', linewidth=2)\nplt.show()","2502c9b4":"df.duplicated().sum()","6b1735ce":"df.head()","0bb75d29":"data = df.drop(['row_number', 'customer_id', 'surname'], axis=1).copy()\ndata.head()","f8e173f7":"data['geography'].value_counts()","639510bd":"data['gender'].value_counts()","aad81208":"# OHE of features\ngender_ohe = pd.get_dummies(df[\"gender\"], drop_first=True)\ncountry_ohe = pd.get_dummies(df[\"geography\"], drop_first=True)\n\n# delete catfeatures\ndata.drop([\"gender\", \"geography\"], axis=1, inplace=True)\n\n#concat new sets\ndf_ohe = pd.concat([data, gender_ohe, country_ohe], axis=1)\n\ndf_ohe.head()","0ca526af":"df_ohe.info()","f867d031":"def split_data(data, target_column):\n    return data.drop(columns=[target_column], axis=1), data[target_column]","61ef22ba":"features, target = split_data(df_ohe,'exited')","95bb00b3":"features_df, features_valid, target_df, target_valid = ( \n                                train_test_split(\n                                features, target, test_size=0.20, random_state=42)\n)\n","d524b3fb":"features_train, features_test, target_train, target_test = ( \n                                train_test_split(\n                                features_df, target_df, test_size=0.25, random_state=42)\n)\n","023dd60b":"print('Objects of train:', len(features_train))\nprint('Objects of valid:', len(features_valid))\nprint('Objects of test:', len(features_test))\nprint('Sum of objects:', len(features_train) + len(features_test) + len(features_test))\nprint()\nprint('Objects of original set (check sum):', len(df_ohe))","1ac62253":"numeric = ['creditscore', 'age', 'balance', 'estimated_salary']\nscaler = StandardScaler()\nscaler.fit(features_train[numeric])\npd.options.mode.chained_assignment = None\nfeatures_train[numeric] = scaler.transform(features_train[numeric])\nfeatures_train.head()","5d969e3b":"features_valid[numeric] = scaler.transform(features_valid[numeric])\nfeatures_valid.head()","808e3c8a":"features_test[numeric] = scaler.transform(features_test[numeric])\nfeatures_test.head()","198d01e2":"model = LogisticRegression(random_state=42, solver='liblinear')\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","2fa8a6dc":"model = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced')\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","651589b2":"model = RandomForestClassifier(random_state=42, n_estimators=10)\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","0130b1ec":"model = RandomForestClassifier(random_state=42, n_estimators=10, class_weight='balanced')\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","34b4d273":"model = CatBoostClassifier(verbose=100, random_state=42)\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","4b601d2f":"df['exited'].value_counts().to_frame()","6625784a":"def upsample(features, target, repeat):\n    \n    features_zeros = features[target == 0]\n    features_ones = features[target == 1]\n    target_zeros = target[target == 0]\n    target_ones = target[target == 1]\n    \n    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n    \n    features_upsampled = shuffle(features_upsampled, random_state=12345)\n    target_upsampled = shuffle(target_upsampled, random_state=12345)\n    \n    return features_upsampled, target_upsampled\n\n    \n    \nfeatures_upsampled, target_upsampled = upsample(features_train, target_train, 5)\n\nprint(features_upsampled.shape)\nprint(target_upsampled.shape)","ba86d086":"model = LogisticRegression(random_state=42, solver='liblinear')\nmodel.fit(features_upsampled, target_upsampled)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","4c8bd0ad":"model = RandomForestClassifier(random_state=42, n_estimators=10)\nmodel.fit(features_upsampled, target_upsampled)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","448ad40f":"model = CatBoostClassifier(verbose=100, random_state=42)\nmodel.fit(features_upsampled, target_upsampled)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","8350b47a":"def downsample(features, target, fraction):\n    features_zeros = features[target == 0]\n    features_ones = features[target == 1]\n    target_zeros = target[target == 0]\n    target_ones = target[target == 1]\n\n    features_sample = features_zeros.sample(frac=0.1, random_state=12345)\n    target_sample = target_zeros.sample(frac=0.1, random_state=12345)\n    \n    features_downsampled = pd.concat([features_sample] + [features_ones])\n    target_downsampled = pd.concat([target_sample] + [target_ones])\n    \n    features_downsampled = shuffle(features_downsampled, random_state=12345)\n    target_downsampled = shuffle(target_downsampled, random_state=12345)\n    \n\n    \n    return features_downsampled, target_downsampled\n\nfeatures_downsampled, target_downsampled = downsample(features_train, target_train, 0.1)\n\nprint(features_downsampled.shape)\nprint(target_downsampled.shape)","aefeab8f":"model = LogisticRegression(random_state=42, solver='liblinear')\nmodel.fit(features_downsampled, target_downsampled)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","ce679d34":"model = RandomForestClassifier(random_state=42, n_estimators=10)\nmodel.fit(features_downsampled, target_downsampled)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","f68e9ed7":"model = CatBoostClassifier(verbose=100, random_state=42)\nmodel.fit(features_downsampled, target_downsampled)\npredicted_valid = model.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))\n\n","178d21f6":"model = LogisticRegression(random_state=42, solver='liblinear')\nmodel.fit(features_train, target_train)\nprobabilities_valid = model.predict_proba(features_valid)\nprobabilities_one_valid = probabilities_valid[:, 1]\n\nfor threshold in np.arange(0, 0.95, 0.05):\n    predicted_valid = probabilities_one_valid > threshold\n    precision = precision_score(target_valid, predicted_valid)\n    recall = recall_score(target_valid, predicted_valid)\n    f1 = f1_score(target_valid, predicted_valid)\n    print(\"Threshold = {:.2f} | Precision = {:.3f}, Recall = {:.3f} | F1-score = {:.3f}\".format(\n        threshold, precision, recall, f1))\n\nprecision, recall, thresholds = precision_recall_curve(target_valid, probabilities_valid[:, 1])    \nplt.figure(figsize=(10, 10))\nplt.step(recall, precision, where='post')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('PR curve')\nplt.show() ","8411c79c":"model = RandomForestClassifier(random_state=42, n_estimators=10)\nmodel.fit(features_train, target_train)\nprobabilities_valid = model.predict_proba(features_valid)\nprobabilities_one_valid = probabilities_valid[:, 1]\n\nfor threshold in np.arange(0, 0.95, 0.05):\n    predicted_valid = probabilities_one_valid > threshold\n    precision = precision_score(target_valid, predicted_valid)\n    recall = recall_score(target_valid, predicted_valid)\n    f1 = f1_score(target_valid, predicted_valid)\n    print(\"Threshold = {:.2f} | Precision = {:.3f}, Recall = {:.3f} | F1-score = {:.3f}\".format(\n        threshold, precision, recall, f1))\n\nprecision, recall, thresholds = precision_recall_curve(target_valid, probabilities_valid[:, 1])    \nplt.figure(figsize=(10, 10))\nplt.step(recall, precision, where='post')\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('PR curve')\nplt.show() ","71d92ff7":"par_grid_logist = {\n                   'intercept_scaling': [0.5, 1.0, 1.5],\n                   'class_weight': [None, 'balanced'],\n                   'C': [0.5, 1, 1.5]\n                   }\nmodel = LogisticRegression(solver='liblinear',random_state=42)\n\ngrid_search = GridSearchCV(model, par_grid_logist, cv=5,\n                           scoring='f1')\ngrid_search.fit(features_upsampled, target_upsampled)","ff2d251e":"grid_search.best_params_","9839bba9":"model_lreg = LogisticRegression(C=1.5, class_weight=None, intercept_scaling=0.5,\n                                solver='liblinear', random_state=42\n)\nmodel_lreg.fit(features_upsampled, target_upsampled)\npredicted_valid = model_lreg.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))\n","a1972e7a":"probabilities_valid = model_lreg.predict_proba(features_valid)\nprobabilities_one_valid = probabilities_valid[:, 1]\n\nfpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid) \n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\n\nplt.show()\n\nauc_roc = roc_auc_score (target_valid, probabilities_one_valid)\n\nprint(\"AUC:\", auc_roc)","adba66cf":"par_grid_ensemble = {'n_estimators': [3, 10, 30],\n                     'criterion': ['gini', 'entropy'],\n                     'min_samples_split': range(5, 15)\n                    }\nmodel = RandomForestClassifier(random_state=42)\n\ngrid_search = GridSearchCV(model, par_grid_ensemble, cv=5,\n                           scoring='accuracy'\n                          )\ngrid_search.fit(features_upsampled, target_upsampled)","a23a4c3d":"grid_search.best_params_","0aa354aa":"model_rfc = RandomForestClassifier(random_state=42, criterion='gini', \n                               min_samples_split=5, n_estimators=30\n                              )\nmodel_rfc.fit(features_upsampled, target_upsampled)\npredicted_valid = model_rfc.predict(features_valid)\nprint(\"F1:\", f1_score(target_valid, predicted_valid))","c67324c4":"probabilities_valid = model_rfc.predict_proba(features_valid)\nprobabilities_one_valid = probabilities_valid[:, 1]\n\nfpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid) \n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\nplt.show()\n\nauc_roc = roc_auc_score (target_valid, probabilities_one_valid)\n\nprint(\"AUC:\", auc_roc)","b5342b3c":"model_cat = CatBoostClassifier(\n                           custom_loss=['F1'],\n                           random_seed=42,\n                           logging_level='Silent'\n)","b04b1311":"model_cat.fit(\n          features_train, target_train,\n          eval_set=(features_valid, target_valid)\n\n)","240302e4":"cv_params = model_cat.get_params()\ncv_params.update({\n                 'loss_function': 'Logloss'\n})\ncv_data = cv(\n             Pool(features_train, target_train),\n             cv_params\n)","8ca6c091":"print('F1-score: {}'.format(np.max(cv_data['test-F1-mean'])))","341101e0":"probabilities_valid = model_cat.predict_proba(features_valid)\nprobabilities_one_valid = probabilities_valid[:, 1]\n\nfpr, tpr, thresholds = roc_curve(target_valid, probabilities_one_valid) \n\nplt.figure(figsize=(10, 10))\nplt.plot(fpr, tpr, linestyle='-')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC-curve')\nplt.show()\n\nauc_roc = roc_auc_score (target_valid, probabilities_one_valid)\n\nprint(\"AUC:\", auc_roc)","2aa4d941":"# collect indicators in lists\n\ntable_of_model = []\ntable_of_prec = []\ntable_of_acc = []","dc514da5":"predictions_test = model_lreg.predict(features_test)\ntest_f1 = f1_score(target_test, predictions_test)\ntest_acc = accuracy_score(target_test, predictions_test)\n\nprint(\"Accuracy\")\nprint(\"Test set:\", test_acc)\nprint(\"F1-\u043c\u0435\u0440\u0430\")\nprint(\"Test set:\", test_f1)\n\ntable_of_acc.append(round(test_acc, 2))\ntable_of_prec.append(round(test_f1, 2))\ntable_of_model.append('LogisticRegression')","fedf919c":"predictions_test = model_rfc.predict(features_test)\ntest_f1 = f1_score(target_test, predictions_test)\ntest_acc = accuracy_score(target_test, predictions_test)\n\nprint(\"Accuracy\")\nprint(\"Test set:\", test_acc)\nprint(\"F1-\u043c\u0435\u0440\u0430\")\nprint(\"Test set:\", test_f1)\n\ntable_of_acc.append(round(test_acc, 2))\ntable_of_prec.append(round(test_f1, 2))\ntable_of_model.append('RandomForestClassifier')","a3e84e9d":"model_rfc.feature_importances_","b8b7664b":"features_test.columns","c61e5931":"fi = pd.DataFrame({'name':features_test.columns,'fi':model_rfc.feature_importances_})\nfi.sort_values('fi',ascending=False)","420897ed":"predictions = model_cat.predict(features_test)\ntest_f1 = f1_score(target_test, predictions_test)\ntest_acc = accuracy_score(target_test, predictions_test)\n\nprint(\"Accuracy\")\nprint(\"Test set:\", test_acc)\nprint(\"F1-\u043c\u0435\u0440\u0430\")\nprint(\"Test set:\", test_f1)\n\ntable_of_acc.append(round(test_acc, 2))\ntable_of_prec.append(round(test_f1, 2))\ntable_of_model.append('Catboost')","6937f38f":"model_cat.feature_importances_","cad7c749":"fi_cat = pd.DataFrame({'name':features_test.columns,'fi_cat':model_cat.feature_importances_})\nfi_cat.sort_values('fi_cat',ascending=False)","84c58db8":"table_of_models = (pd.DataFrame({'Model':table_of_model, 'Accuracy':table_of_acc, \n                                'F1 score':table_of_prec}).sort_values(by='F1 score', ascending=False).\n                  reset_index(drop=True))\ntable_of_models['Threshold of testing'] = (\n                   table_of_models['F1 score'].apply(lambda x: 'good model' if x>0.59 else 'bad model')\n)\ntable_of_models","e66dadf8":"target_const = target*0\nacc_const = accuracy_score(target, target_const)\n\n\nprint(\"Accuracy\")\nprint(\"const:\", acc_const)\n","6f39c03b":"fi.sort_values('fi',ascending=False).reset_index(drop=True).head()","d0b319ea":"Let's try to configure Catboost using cross-validation. We will get the basic model, we will check it by the F1-score. Excluding class imbalance","62a0a301":"### Scaling","dde54d04":"Let's compare our models with a constant model: it predicts class \"0\" for any object","ef3749dd":"### Testing Models","15ca2af1":"#### Conclusion","303f47aa":"We have no NaN in our set","ba398d8f":"#### Logistic regression","332d9cb2":"Let's choose upsampling increasing the sample. On it we will train our models and select the hyperparameters. We will not change the threshold or reduce the sample","7bd29817":"The highest indicator is reached at a threshold of 0.2","ab8ec5ca":"### Train Models and Tuning Hyperparameters","d7be8810":"#### Logistic regression","5e7e9511":"Get the grid of parameters and cross-validate using the built-in Pool function","e6afd5c6":"The most important signs to look out for are:\n\n - client's age\n - credit speed\n - expected profit\n - account balance\n - number of products\n \n \n To predict churn, you can use a model based on the Random Forest algorithm","1092697e":"The categorical features `geography` and` gender` must be converted to numerical ones using the direct coding technique, or display (English One-Hot Encoding, OHE). We need quantitative features to be more accurate","a1605d17":"![](https:\/\/i.ibb.co\/yQ3D64X\/face.jpg)\n\n# Customer churn\n\n\n\n*It is necessary to predict whether the client will leave the bank in the near future or not.\nWe have been provided with historical data on customer behavior and termination of agreements with the bank*\n","6505e2e8":"Random Forest and Catboost have been validated. The accuracy of our models is higher than that of the random one. We also looked at ROC-AUC validations, our models performed better.","aeaf4314":"Better now. At this stage, we will not select the hyperparameters, we will move on to the next algorithm","7fb0ce60":"Let's see general information about data in work","befc2b47":"We will train the model on an enlarged sample, check the parameters on a validation sample and evaluate it by the F1-measure, we will not use cross-validation for logistic regression and a random forest.\n\nThe parameters will be selected through `GridSearchCV`. loop and enumeration will not be used","66714040":"Customer churn is the loss of customers, expressed in the absence of purchases or payments over a period of time. Churn rate is extremely important for companies with a subscription and transactional business model that means recurring payments to the company.\n\nWe've previewed our dataset:\n\n- no duplicates found, no need to delete lines\n- in the process of preparing features for analysis - remove the columns `customer_id`,` row_number` and `surname`\n\n\nWe can start preparing the features","512e7fe5":"#### Logistic regression","40e03532":"### Downsampling","fa77e5ca":"### Importing required libraries","9941a0c5":"Above the threshold of 0.59 on the validation set. Let's try on a sample test and see how the model behaves on unfamiliar data","205c0c98":"#### Catboost (bonus)","454dcee1":"For convenience, we will translate the proximity to the classes into the probability of classes (we have two classes - 0 and 1). The probability of class \"1\" is enough for us. By default it is equal to 0.5 - let's try different parameters, for example, up to 0.95","886a0534":"We are faced with the task of classification. In order to improve the forecasting results and facilitate the training of the model, we have transformed the data:\n\n- removed unnecessary features - such as surname, customer id and line number\n- carried out coding of categorical variables\n- carried out scaling of quantitative variables\n- divided the samples in a ratio of 60%: 20%: 20% - training, validation for the selection of hyperparameters and model verification, test - for the final model verification and evaluation\n\nWe tried to train the models on objects with class imbalance. Now let's try to get rid of this problem, select the model hyperparameters.","f81bed1e":"Features: \n\n- `RowNumber` - the index of the row in the data\n- `CustomerId` - unique customer identifier\n- `Surname` - surname\n- `CreditScore` - credit rating\n- `Geography` - country of residence\n- `Gender` - gender\n- `Age` - age\n- `Tenure` - how many years a person has been a client of the bank\n- `Balance` - account balance\n- `NumOfProducts` - the number of bank products used by the client\n- `HasCrCard` - availability of a credit card\n- `IsActiveMember` - client activity\n- `EstimatedSalary` - estimated salary\n\nTarget column:\n\n- `Exited` - the fact of the client's departure","81a70ddd":"## Conclusion","b2f9e389":"## Dealing with imbalances and improving models","973ad0c7":"For a threshold of 0, the completeness is 1 - all answers are positive. At a threshold of 0.85, the model stops giving correct answers. The highest F1 value is observed with a threshold of 0.25","4d8232f7":"We have prepared features. Now we will divide our samples into training, validation for the selection of hyperparameters and test, on which we will test our model. We will not touch the test sample to the end, we will work out the best model on it","f9bf27c8":"#### Random forest","83beb4ab":"We get a validation sample of 20% and divide the remaining 80% again to obtain a test sample. We will conduct training on 60% of the data","2912921c":"### Conclusion","1e88ee75":"#### Random forest","a1db747d":"#### Random Forest","43c070a2":"## General information about data in operation and preinspection","7911e92d":"\nPoor enough indicator. Let's try to specify `class_weight = 'balanced' '","8ad78256":"we need `tenure` to be integre","979d4a24":"#### Random forest","85fa6e10":"#### Catboost","a91aae57":"## Model testing and validation","fbb5dee2":"Classes are not represented in the same way in our problem, let's look again:","35620604":"Below the threshold of 0.59, let's see how the model will behave during testing","743fc294":"Let's try to solve this problem in three ways. We will choose the best one and use it to improve our model.","64315c07":"Let's start with basic logistic regression. We do not indicate the weight of the classes","c01c3b6f":"We observe a slight increase in the metric, close to the one we got by specifying the `class_weight` parameter","5206aeb8":"### Split data set","99202a35":"#### Logistic regression","25d62e13":"### Exploring Data Set","317c3921":"F1 is a fairly high measure, let's look at the results after we select the hyperparameters and test the model on a test sample","991ec8a2":"\nFor convenience, we will display a table of our parameters by model:","47a6da6d":"### Conclusion","55ad1303":"### One-hot Encoding","458ed3c8":"The best result was obtained on the Random forest - 0.61, Catboost takes the second place - but this is without correcting the imbalance problem! Logistic regression could not overcome the F1-score threshold of 0.59\n\nWe also looked at what features are important for classification models:\nage, expected salary, credit rate, balance and number of products - age is in the lead","876ad3e4":"#### Random forest","703c8170":"Scaling features across the entire dataset can lead to a data leak. You only need to train the scaler on the train.\n\nWe will train and then apply to our samples","e2706f5e":"Let's apply a trained scaller to the validation set","66ae9524":"The sample was divided, we can proceed to trial training of the models. In our task, there is a strong class imbalance, which has a bad effect on training the model. Let's look at the results, we will evaluate the model by the F1 measure - it is a good candidate for a formal metric for assessing the quality of the classifier. It reduces to one number two other fundamental metrics: `precision` and` recall`","6ee3ef40":"To do this, let's use a function that performs the following transformations:\n\n- divide the training sample into negative and positive objects\n- randomly discard some of the negative objects\n- taking into account the received data, we will create a new training sample\n- shuffle the data","8539a473":"### Senity test","5fbbeefe":"The indicator has worsened. also now we will not change the hyperparameters, we will return to this after we fix the imbalance problem","49e2128d":"At first glance, we do not observe anomalies in the data, significant outliers. For example, by age, the minimum age is 18 - the maximum is 92, which may be true.\n\nLet's see the target column `exited`","ad713baa":"In the process of feature engeneering for our model, it will be possible to delete three columns - `customer_id`,` row_number` and `surname`, which do not carry the payload in our case","dfa521c7":"The random forest did better in terms of class imbalance. Similar to logistic regression, let's try setting the `class_weight` parameter","424b1e0a":"AUC greater than 0.5, our model is better than random","363e537b":"For our convenience, we will lower the name of the columns and bring them to the serpentine register","ce52435a":"Let's apply a trained scaller to the test set","b2f3682f":"### Trial training of models without considering class imbalance","624cb552":"#### Catboost","2150e799":"We were provided with historical data on customer behavior and termination of agreements with the bank. Based on this data, we formed features for training the model in order to predict customer churn. We have achieved the best results with a model based on the Random Forest algorithm - F1 measure - `0.61`.\n\nBased on the analysis (using the best model as an example):","5172aa97":"Let's remove unnecessary features and form a new date set so as not to overwrite variables","69aaaed8":"### Change threshold","e4ce3ded":"To do this, let's use a function that performs the following transformations:\n\n- divide the training sample into negative and positive objects\n- copy positive objects several times\n- taking into account the received data, we will create a new training sample\n- shuffle the data ","6c071bce":"\nLet's take a look at the data types in our dataframe separately:","aa8ae71f":"### Features engeneering","65a0ca50":"\n#### Logistic regression","ec6e5bd1":"## Research of task","613e275a":"#### Logistic regression","add38555":"There is a class imbalance. We will train the model on the initial data, then we will try to overcome the imbalance and train again. Let's see the results later.\n","6dbc258c":"Let's apply our parameters and see the result:","9cb33464":"We are faced with the task of classification - it is necessary to determine whether the client will leave in the near future or not. Thus, to achieve the goals of this task, I propose to use the algorithms of Logistic Regression, Random Forest and Catboost.\n\nTo evaluate the models, we will use the F1 measure (`F1 score`) (let us apply the good values is > 0.59)\n\nTo evaluate the final model, we use the ROC curve with its area (`ROC-AUC`).\n\nAs we found out, we have an imbalance of classes, accuracy does not suit us.","be7041ed":"Columns were coded. \n\nIt is also necessary to standardize the characteristics, since the quantitative values \u200b\u200bvary greatly. We will not apply standardization to the columns `tenure`,` num_of_products`, `has_crcard`,` isactive_member` and to the target with transformed categorical","5739c0a6":"The AUC also tells us that the model is better, random, and better than logistic regression.","0e550bdb":"#### Catboost","062f27b8":"#### Catboost","db4ab893":"Catboost Shows Better Results Again On Validation Set\n\nLet's try another way - decreasing the sample","02e33621":"`Downsampling` shows worse results than` upsampling` for all three algorithms.\n\nLet's try changing the threshold and see what the metrics will be - this time we'll turn to `recall` and` precision`","808b8913":"### Upsampling","b0baa4a3":"#### Comparison with constant","971c0022":"We got a very good result, let's see how the model will behave on the test set. Model is better than random","850aee73":"#### Random forest","7e2f2a4d":"There is also an improvement here"}}