{"cell_type":{"13b0ad64":"code","564c0ff3":"code","99598fcc":"code","6e9ff262":"code","cf8c9dfa":"code","5c3ebdea":"code","411f3441":"code","fb264a57":"markdown","ff3962fa":"markdown","d3292108":"markdown","bfcfeb8b":"markdown","cafe2cfc":"markdown","65c46e21":"markdown"},"source":{"13b0ad64":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport pandas as pd\nimport json\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport time\nimport numpy as np\ndf = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\", lines=True)\ndf.head()","564c0ff3":"sentences = []\nlabels = []\n\nvocab_size = 1000\nembedding_dim = 16\nmax_length = 16\ntrunc_type = 'post'\npadding_type = 'post'\noov_tok = \"<OOV>\"\ntraining_size = 20000\n","99598fcc":"for index , rows in df.iterrows():\n    my_list = rows.headline\n    my_labels = rows.is_sarcastic\n    \n    sentences.append(my_list)\n    labels.append(my_labels)\n    \n\ntraining_sentences = sentences[0:training_size]\ntesting_sentences = sentences[training_size:]\n\ntraining_labels = labels[0:training_size]\ntesting_labels = labels[training_size:]","6e9ff262":"tokenizer = Tokenizer(num_words = vocab_size ,oov_token = \"<OOV>\")\ntokenizer.fit_on_texts(sentences)\n\nword_index = tokenizer.word_index\n\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen = max_length,padding = padding_type,truncating = trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded= pad_sequences(testing_sequences, maxlen = max_length,padding = padding_type,truncating = trunc_type)\n\ntraining_padded = np.array(training_padded)\ntraining_labels = np.array(training_labels)\ntesting_padded = np.array(testing_padded)\ntesting_labels = np.array(testing_labels)\n","cf8c9dfa":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24,activation='relu'),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n    \n    \n    \n    \n    \n])\n\nmodel.compile(loss = 'binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()\ntime.sleep(0.1)","5c3ebdea":"num_epochs = 29\n\n\nhistory = model.fit(training_padded,training_labels,epochs = num_epochs,validation_data = (testing_padded,testing_labels),verbose = 1)\n\nresult = model.evaluate(testing_padded,testing_labels)\nprint(result)","411f3441":"\ndef plot_graphs(history,string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string,'val_'+string])\n    plt.show()\nplot_graphs(history,\"accuracy\")\nplot_graphs(history,\"loss\")\n","fb264a57":"Here we can see the accuracy is **82.24773%**","ff3962fa":"Declaring all the essential parameters that we are going to use in the following code","d3292108":"Putting all the headlines and Lables to one list and dividing them into testing and training sets respectively","bfcfeb8b":"Loss function is tweaked but accuracy is not that high.","cafe2cfc":"****Importing Tensorflow API's and the data set of course","65c46e21":"Creating a Tokenizer object and fitting it onto sentences and Converting each sentence into a sequence of words using texts_to_sequences method. After that we also use padding to ensure all sentences are of same length."}}