{"cell_type":{"495fe4df":"code","58e2831b":"code","2ccd4274":"code","267d6ce3":"code","9a2bc0f6":"code","c5b15ddb":"code","76692801":"code","5b218499":"code","2b361d10":"code","154a4b51":"code","bbbf98e1":"code","14c25903":"code","ef88112f":"code","0ee6d96a":"code","a55fd44d":"code","de23502b":"code","02e813f3":"code","2f5cd774":"code","2c0dfa25":"code","a2a2d1b4":"code","8b60336b":"code","6f8c5872":"code","1ffec6b7":"code","b9b74e8a":"code","8c5cb824":"code","262bd46f":"code","423acb32":"code","7b96b827":"code","2acb1d1a":"code","ca1c4e21":"markdown","ae0d0f4b":"markdown","9aeca8d5":"markdown","6946dfdd":"markdown","b5610530":"markdown","e0f3ba46":"markdown","d0293da2":"markdown","3cd27c36":"markdown","0a507037":"markdown","c8390961":"markdown","2be74df4":"markdown","15fe9845":"markdown"},"source":{"495fe4df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nimport optuna\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","58e2831b":"train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\")\nsubmssion = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/sample_submission.csv\")\ntrain.shape, test.shape, submssion.shape","2ccd4274":"train.info()","267d6ce3":"train.head()","9a2bc0f6":"train.drop('id',axis=1,inplace=True)\ntest.drop('id',axis=1,inplace=True)","c5b15ddb":"train.describe().T.style.bar(subset=['mean']).background_gradient(subset=['std'], cmap='PuBu').background_gradient(subset=['50%'], cmap='PuBu')","76692801":"test.describe().T.style.bar(subset=['mean']).background_gradient(subset=['std'], cmap='PuBu').background_gradient(subset=['50%'], cmap='PuBu')","5b218499":"sns.distplot(train['target']);","2b361d10":"cat_column = [\"cat\" + str(i) for i in range(10)]\ncont_column = [\"cont\" + str(i) for i in range(14)]","154a4b51":"fig = plt.figure(figsize = (10, 60))\nfor i in range(len(train[cont_column].columns.tolist()[:14])):\n    plt.subplot(20,5,i+1)\n    plt.title(train[cont_column].columns.tolist()[:14][i], size = 10)\n    a = sns.kdeplot(train[train[cont_column].columns.tolist()[:14][i]], shade = True, alpha = 1.0, linewidth = 1.5, facecolor=(1, 1, 1, 0), edgecolor=\".1\")\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.0)\n        \nfig.tight_layout(h_pad = 3)\nplt.show()","bbbf98e1":"fig = plt.figure(figsize = (10, 60))\nfor i in range(len(train[cont_column].columns.tolist()[:14])):\n    plt.subplot(20,5,i+1)\n    \n    plt.title(train[cont_column].columns.tolist()[:14][i], size = 10)\n    a = sns.boxplot(train[train[cont_column].columns.tolist()[:100][i]], linewidth = 1.5)\n    plt.ylabel('')\n    plt.xlabel('')\n    plt.yticks([])\n    for j in ['right', 'left', 'top']:\n        a.spines[j].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.0)\n        \nfig.tight_layout(h_pad = 3)","14c25903":"y = train['target']\ntrain.drop('target',axis=1,inplace=True)","ef88112f":"features = []\nfor feature in train.columns:\n    features.append(feature)\nprint(features)","0ee6d96a":"for col in cat_column:\n    encoder = OrdinalEncoder()\n    train[col] = encoder.fit_transform(np.array(train[col]).reshape(-1, 1))\n    test[col] = encoder.transform(np.array(test[col]).reshape(-1, 1))","a55fd44d":"X = train.copy()","de23502b":"def fit_lgb(trial, x_train, y_train, x_test, y_test):\n    params = {\n        'reg_alpha' : trial.suggest_loguniform('reg_alpha' , 0.1 , 1.0),\n        'reg_lambda' : trial.suggest_loguniform('reg_lambda' , 0.10 , 0.5),\n        'num_leaves' : trial.suggest_int('num_leaves' , 10 , 70),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.03 , 0.10),\n        'max_depth' : trial.suggest_int('max_depth', 1 , 40),\n        'n_estimators' : trial.suggest_int('n_estimators', 100 , 6100),\n        'min_child_weight' : trial.suggest_loguniform('min_child_weight', 0.015 , 0.02),\n        'subsample' : trial.suggest_uniform('subsample' , 0.9 , 1.0), \n        'colsample_bytree' : trial.suggest_loguniform('colsample_bytree', 0.52 , 1),\n        'min_child_samples' : trial.suggest_int('min_child_samples', 76, 80),\n        'metric' : 'rmse',\n        'device_type' : 'gpu',\n    }\n    \n    \n    model = LGBMRegressor(**params, random_state=123)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False)\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","02e813f3":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n    model, log = fit_lgb(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","2f5cd774":"#study = optuna.create_study(direction='maximize')\n#study.optimize(objective, n_trials=50)\n#print(study.best_params)\n#{'reg_alpha': 0.6638611569890519, 'reg_lambda': 0.49693382194626423, 'num_leaves': 64, 'learning_rate': 0.0976839536160219, 'max_depth': 1, 'n_estimators': 3639, 'min_child_weight': 0.01667977024032092, 'subsample': 0.9525467910589746, 'colsample_bytree': 0.7679248147894047, 'min_child_samples': 77}","2c0dfa25":"lgb_params = {\n                'reg_alpha': 0.6638611569890519, 'reg_lambda': 0.49693382194626423,\n                'num_leaves': 64, 'learning_rate': 0.0976839536160219, 'max_depth': 1,\n                'n_estimators': 3639, 'min_child_weight': 0.01667977024032092, 'subsample': 0.9525467910589746,\n                'colsample_bytree': 0.7679248147894047, 'min_child_samples': 77, 'device':'gpu'\n             }","a2a2d1b4":"def cross_val_lgb(X, y, lgb_params):\n    # Setting up fold parameters\n    splits = 10\n    skf = KFold(n_splits=splits, shuffle=True, random_state=42)\n\n    # Creating an array of zeros for storing \"out of fold\" predictions\n    oof_preds = np.zeros((X.shape[0],))\n    preds = 0\n    total_mean_rmse = 0\n\n    # Generating folds and making training and prediction for each of 10 folds\n    for num, (train_idx, valid_idx) in enumerate(skf.split(X)):\n        X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n        y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n\n        model = LGBMRegressor(**lgb_params, random_state = 123)\n        model.fit(X_train, y_train,\n                  verbose=False,\n                  # These three parameters will stop training before a model starts overfitting \n                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                  early_stopping_rounds=400,\n                  )\n\n        # Getting mean test data predictions (i.e. devided by number of splits)\n        preds += model.predict(X_valid) \/ splits\n        \n        # Getting validation data predictions. Each fold model makes predictions on an unseen data.\n        # So in the end it will be completely filled with unseen data predictions.\n        # It will be used to evaluate hyperparameters performance only.\n        oof_preds[valid_idx] = model.predict(X_valid)\n\n        # Getting score for a fold model\n        fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))\n        print(f\"Fold {num} RMSE: {fold_rmse}\")\n\n        # Getting mean score of all fold models (i.e. devided by number of splits)\n        total_mean_rmse += fold_rmse \/ splits\n\n    print(f\"\\nOverall RMSE: {total_mean_rmse}\")\n    return model","8b60336b":"lgb_model = cross_val_lgb(X, y, lgb_params)","6f8c5872":"def fit_xgb(trial, x_train, y_train, x_test, y_test):\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 4, 10),\n        'learning_rate' : trial.suggest_uniform('learning_rate' , 0.03 , 0.10),\n        'n_estimators': trial.suggest_int('n_estimators', 400, 10000, 200),\n        'eta': trial.suggest_float('eta', 0.005, 0.02),\n        'subsample': trial.suggest_discrete_uniform('subsample', 0.2, 1.0, 0.01),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.1, 1.0, 0.01),\n        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-4, 1e4), \n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-4, 1e4),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-4, 1e4),\n        'gamma': trial.suggest_loguniform('gamma', 1e-4, 1e4)\n    } \n    \n    \n    model = XGBRegressor(**params,tree_method='gpu_hist', random_state=123)\n    model.fit(x_train, y_train,eval_set=[(x_test,y_test)], early_stopping_rounds=150, verbose=False, eval_metric=\"rmse\")\n    \n    y_train_pred = model.predict(x_train)\n    \n    y_test_pred = model.predict(x_test)\n    y_train_pred = np.clip(y_train_pred, 0.1, None)\n    y_test_pred = np.clip(y_test_pred, 0.1, None)\n    \n    log = {\n        \"train rmse\": mean_squared_error(y_train, y_train_pred,squared=False),\n        \"valid rmse\": mean_squared_error(y_test, y_test_pred,squared=False)\n    }\n    \n    return model, log","1ffec6b7":"def objective(trial):\n    rmse = 0\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n    model, log = fit_xgb(trial, x_train, y_train, x_test, y_test)\n    rmse += log['valid rmse']\n        \n    return rmse","b9b74e8a":"#study = optuna.create_study(direction='maximize')\n#study.optimize(objective, n_trials=50)\n#print(study.best_params)\n#{'max_depth': 8, 'learning_rate': 0.09539255227145249, 'n_estimators': 2800, 'eta': 0.012516384305174103, 'subsample': 0.77, 'colsample_bytree': 0.65, 'min_child_weight': 0.004379936141888949, 'reg_lambda': 0.008097685589798119, 'reg_alpha': 0.0007012913297733225, 'gamma': 9845.89122772672}","8c5cb824":"xgb_params = {\n    'max_depth': 8, 'learning_rate': 0.09539255227145249,\n    'n_estimators': 2800, 'eta': 0.012516384305174103, 'subsample': 0.77,\n    'colsample_bytree': 0.65, 'min_child_weight': 0.004379936141888949,\n    'reg_lambda': 0.008097685589798119, 'reg_alpha': 0.0007012913297733225, 'gamma': 9845.89122772672,\n    'tree_method':'gpu_hist'\n}","262bd46f":"def cross_val_xgb(X, y, xgb_params):\n    # Setting up fold parameters\n    splits = 10\n    skf = KFold(n_splits=splits, shuffle=True, random_state=42)\n\n    # Creating an array of zeros for storing \"out of fold\" predictions\n    oof_preds = np.zeros((X.shape[0],))\n    preds = 0\n    total_mean_rmse = 0\n\n    # Generating folds and making training and prediction for each of 10 folds\n    for num, (train_idx, valid_idx) in enumerate(skf.split(X)):\n        X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n        y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n\n        model = XGBRegressor(**xgb_params, random_state=123)\n        model.fit(X_train, y_train,\n                  verbose=False,\n                  # These three parameters will stop training before a model starts overfitting \n                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                  eval_metric=\"rmse\",\n                  early_stopping_rounds=100,\n                  )\n\n        # Getting mean test data predictions (i.e. devided by number of splits)\n        preds += model.predict(X_valid) \/ splits\n        \n        # Getting validation data predictions. Each fold model makes predictions on an unseen data.\n        # So in the end it will be completely filled with unseen data predictions.\n        # It will be used to evaluate hyperparameters performance only.\n        oof_preds[valid_idx] = model.predict(X_valid)\n\n        # Getting score for a fold model\n        fold_rmse = np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))\n        print(f\"Fold {num} RMSE: {fold_rmse}\")\n\n        # Getting mean score of all fold models (i.e. devided by number of splits)\n        total_mean_rmse += fold_rmse \/ splits\n\n    print(f\"\\nOverall RMSE: {total_mean_rmse}\")\n    return model","423acb32":"xgb_model = cross_val_xgb(X, y, xgb_params)","7b96b827":"from sklearn.ensemble import VotingRegressor\nfolds = KFold(n_splits = 10, random_state = 2021, shuffle = True)\n\npredictions = np.zeros(len(test))\n\nfor fold, (trn_idx, val_idx) in enumerate(folds.split(X)):\n    print(f\"Fold: {fold}\")\n    X_train, X_val = X.values[trn_idx], X.values[val_idx]\n    y_train, y_val = y.values[trn_idx], y.values[val_idx]\n\n    model = VotingRegressor(\n            estimators = [\n                ('lgbm', LGBMRegressor(**lgb_params, random_state=123)),\n                ('xgb', XGBRegressor(**xgb_params, random_state=123))\n            ],\n            weights = [0.8, 0.2]\n        )\n   \n    model.fit(X_train, y_train)\n    pred = model.predict(X_val)\n    error = mean_squared_error(y_val, pred,squared = False)\n    print(f\" mean_squared_error: {error}\")\n    print(\"-\"*50)\n    \n    predictions += model.predict(test) \/ folds.n_splits ","2acb1d1a":"submssion['target'] = predictions\nsubmssion.to_csv(f'submission.csv',index = False)","ca1c4e21":"# LGBMRegressor","ae0d0f4b":"# Data Preprocessing","9aeca8d5":"# Submission","6946dfdd":"# Model Build with Optuna","b5610530":"Reference1:https:\/\/www.kaggle.com\/maximkazantsev\/30dml-eda-xgboost\n\nReference2:https:\/\/www.kaggle.com\/ranjeetshrivastav\/tps-aug-21-optuna-lgb-xgb-cb","e0f3ba46":"# Data Exploration","d0293da2":"# Data Visualization","3cd27c36":"# XGBRegressor","0a507037":"# VotingRegressor ","c8390961":"# Importing Libraries","2be74df4":"Reference:https:\/\/www.kaggle.com\/dmitryuarov\/falling-below-7-87-voting-cb-xgb-lgbm","15fe9845":"<center><h2>If you like this notebook, support with an upvote<\/h2><\/center>"}}