{"cell_type":{"6619ff42":"code","d84f4e6b":"code","e556ee73":"code","79c59ee8":"code","9c0a0b26":"code","44520b65":"code","6daf68d1":"code","8dc5d15d":"code","85d0e4af":"code","2178651d":"code","ea379f51":"code","64752451":"code","a45efa69":"code","d7facf28":"code","dab1f0af":"code","e521b9f8":"code","dc25e08d":"code","dc163a77":"code","c3abb95f":"code","d54a209a":"code","9ac271b7":"markdown","517c2b02":"markdown","0beee1c2":"markdown","88fb41a7":"markdown","27b2e16a":"markdown","a1b71398":"markdown","29892b87":"markdown","c7b07550":"markdown","7b2f3238":"markdown"},"source":{"6619ff42":"import os\nimport time\nimport warnings\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import classification_report\n\nimport lightgbm as lgb\n\nwarnings.filterwarnings('ignore')","d84f4e6b":"np.random.seed(42)","e556ee73":"class DataExplorer:\n    \"\"\"\n    Reading dataset\n    \"\"\"\n    def __init__(self, filedir, filename):\n        self.data = pd.read_csv(os.path.join(filedir, filename))\n        \n    def grouper_quality(self, row):\n        \"\"\"\n        Enlarging groups of target features\n        \"\"\"\n        quality = row['quality']\n\n        if quality < 5:\n            return 3\n\n        elif quality > 6:\n            return 1\n\n        else:\n            return 2\n        \n    def generalization(self):\n        \"\"\"\n        Greate enlarged groups of target features: third, second and first class wines\n        \"\"\"\n        data = self.data.copy()\n        data['gen_quality'] = self.data.apply(self.grouper_quality, axis=1)\n\n        return data.drop('quality', axis=1)\n    \n    def binomizator(self):\n        \"\"\"\n        Binominaizing target features\n        \"\"\"\n        data = self.data.copy()\n        data['bi_quality'] = self.data.quality.apply(lambda x: 1 if x >= 6 else 0)\n\n        return data.drop('quality', axis=1)\n\n    class Reporter():\n        \"\"\"\n        Collecting perfomance data and bulids report\n        \"\"\"\n        def __init__(self, data, target, features_dict, models, binomial=False):\n            \"\"\"\n            Instances for Reporter\n            \"\"\"\n\n            self.final_report = None\n            self.best_estimator = []\n            self.predictions = []\n            self.data = data\n            self.target = target\n            self.models = models\n            self.binomial = binomial\n            self.score = f1_score\n            self.scoring = 'f1_micro'\n            self.random_state = 42\n            self.features_dict = features_dict\n            self.folds = 5\n\n        def metrics_plot(self, model, model_title, features_valid, target_valid):\n            \"\"\"\n            Displays the PR curve and ROC curve\n            \"\"\"\n\n            probabilities_valid = model.predict_proba(features_valid)\n            precision, recall, thresholds = precision_recall_curve(target_valid, probabilities_valid[:, 1])\n            fpr, tpr, thresholds = roc_curve(target_valid, probabilities_valid[:, 1])\n\n            fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n            fig.subplots_adjust(hspace=0.4, wspace=0.4)\n\n            sns.lineplot(recall, precision, drawstyle='steps-post', ax=ax[0])\n            ax[0].set_xlabel('Recall')\n            ax[0].set_ylabel('Precision')\n            ax[0].set_ylim([0.0, 1.05])\n            ax[0].set_xlim([0.0, 1.0])\n            ax[0].set_title('Precision-Recall Curve ' + model_title)\n\n            sns.lineplot(fpr, tpr, ax=ax[1])\n            ax[1].plot([0, 1], [0, 1], linestyle='--')\n            ax[1].set_xlim(0, 1)\n            ax[1].set_ylim(0, 1)\n            ax[1].set_xlabel('False Positive Rate')\n            ax[1].set_ylabel('True Positive Rate')\n            ax[1].set_title('ROC-curve ' + model_title)\n\n        def auc_roc(self, model, features_valid, target_valid):\n            \"\"\"\n            Calculating ROC-AUC\n            \"\"\"\n\n            probabilities_valid = model.predict_proba(features_valid)\n            probabilities_one_valid = probabilities_valid[:, 1]\n            auc_roc = roc_auc_score(target_valid, probabilities_one_valid)\n\n            return auc_roc\n\n        def grid_search(self, model, param_grid, x_features, y_features):\n            \"\"\"\n            GridSearchCV\n            \"\"\"\n            kfold = KFold(n_splits=self.folds, shuffle=True,\n                          random_state=self.random_state)\n            grid_model = GridSearchCV(model, param_grid=param_grid,\n                                      scoring=self.scoring, cv=kfold,\n                                      verbose=1, n_jobs=-1, )\n            grid_model.fit(x_features, y_features)\n            best_estimator = grid_model.best_estimator_\n            return best_estimator\n\n        def data_spliter(self, features):\n            \"\"\"\n            Splitting data into training and test in a ratio of 60:40\n            \"\"\"\n            x_train, x_test, y_train, y_test = train_test_split(self.data[features], \n                                                                self.data[self.target], \n                                                                train_size=0.6, \n                                                                stratify=self.data[self.target],\n                                                                random_state=self.random_state)\n\n            return x_train, y_train, x_test, y_test\n\n        def reporter(self):\n\n            started = time.time()\n            report = []\n            estimators = []\n            predictions = []\n            score_name = str(self.score).split(' ')[1]\n            models = self.models\n\n            for key in self.features_dict:\n\n                features = self.features_dict[key]\n\n                print('Features set - ', key)\n\n                x_train, y_train, x_test, y_test = self.data_spliter(features)\n                \n                x_train = np.log(x_train, where=x_train>0)\n                x_test = np.log(x_test, where=x_test>0)\n\n                scaler = StandardScaler()\n\n                x_train = scaler.fit_transform(x_train)\n                x_test = scaler.transform(x_test)\n\n                print('Dataset was splitted into training and test in a ratio of 60:40 and scaled using StandardScaler. \\n')\n                print('Shapes: \\n')\n                print('- train ', x_train.shape, y_train.shape)\n                print('- test ', x_test.shape, y_test.shape)\n                print('\\n')\n\n                print('Report: ')\n\n                for model in models:\n                    started_local = time.time()\n                    print('\\n', model[0], '\\n')\n                    grid_search = self.grid_search(model[1], model[2], x_train, y_train)\n                    print(grid_search)\n                    ended_local = time.time()\n                    predicted_test = np.ravel(grid_search.predict(x_test))\n                    test_score = self.score(y_test, predicted_test, average='micro')\n\n                    report.append((model[0], test_score, ended_local-started_local, key))\n                    estimators.append((model[0], grid_search))\n                    predictions.append((model[0], predicted_test))\n                    if self.binomial == True:\n                        self.metrics_plot(grid_search, model[0], x_test, y_test)\n                    print('\\n', 'Classification report for ' + model[0], '\\n\\n', classification_report(y_test, predicted_test))\n\n                print('--------------------------------------------------------------------')\n                print('--------------------------------------------------------------------')\n                print('--------------------------------------------------------------------')\n                print('--------------------------------------------------------------------')\n            self.final_report = pd.DataFrame(report, columns=['model', score_name + '_test', 'seconds_to_fit', 'features_key'])\n            self.best_estimator = pd.DataFrame(estimators, columns=['model', 'grid_params'])\n            self.predictions = pd.DataFrame(predictions, columns=['model', 'test_predictions'])\n            ended = time.time()\n            print('Cross-validation training and parameter search completed in {} sec.'.format(round(ended-started, 2)))","79c59ee8":"explorer = DataExplorer('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/', 'winequality-red.csv')","9c0a0b26":"dict_of_features_combinations = {'all_features': ['fixed acidity', 'volatile acidity', 'citric acid', \n                                                   'residual sugar', 'chlorides', 'free sulfur dioxide', \n                                                   'total sulfur dioxide', 'density', 'pH', 'sulphates', \n                                                   'alcohol'],\n                                 \n                                 'most_important': ['volatile acidity', 'residual sugar', 'sulphates', 'alcohol']}","44520b65":"models_list = []","6daf68d1":"lg = lgb.LGBMClassifier(random_state=42, class_weight='balanced')\n\nparam_grid = {'boosting_type': ['gbdt'],\n              'num_leaves': [10, 20, 30],\n              'num_iterations': [600],\n              'learning_rate': [0.01, 0.0001],\n              'max_depth': [10, 30, 100]}\n\nmodels_list.append(('LGBMClassifier', lg, param_grid))","8dc5d15d":"explorer.data","85d0e4af":"multi_report = explorer.Reporter(explorer.data, 'quality', dict_of_features_combinations, models_list)","2178651d":"multi_report.reporter()","ea379f51":"multi_report.final_report","64752451":"df_three_classed = explorer.generalization()","a45efa69":"df_three_classed","d7facf28":"gen_report = explorer.Reporter(df_three_classed, 'gen_quality', dict_of_features_combinations, models_list)","dab1f0af":"gen_report.reporter()","e521b9f8":"gen_report.final_report","dc25e08d":"df_binomial = explorer.binomizator()","dc163a77":"bi_report = explorer.Reporter(df_binomial, 'bi_quality', dict_of_features_combinations, models_list, binomial=True)","c3abb95f":"bi_report.reporter()","d54a209a":"bi_report.final_report","9ac271b7":"# 3. Declaring DataExplorer class","517c2b02":"#### - LGBMClassifier","0beee1c2":"# 8. Report for binomial classifications","88fb41a7":"# 7. Report for enlarged groups classifications: three classes of wine (1 - best, 3 - worst)","27b2e16a":"# 2. Seeding random","a1b71398":"# 4. Defining features bunch","29892b87":"# 6. Report for multiple classifications","c7b07550":"# 1. Import libraries","7b2f3238":"# 5. Preparing models and parameters for tuning"}}