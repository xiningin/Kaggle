{"cell_type":{"253578c7":"code","56ae9ac6":"code","6cd43118":"code","54a33cdb":"code","06677842":"code","a5462723":"code","137b9354":"code","6fe3d3cb":"code","046fd3cf":"code","413ce049":"code","64ff3e96":"code","29ee25be":"code","48788c93":"code","68f8fe55":"code","b75ccf41":"code","0689dc97":"code","106bfd1c":"code","69838a62":"code","96bcffd7":"code","adda4244":"code","c79e753a":"code","021535a8":"code","1406de98":"code","0b71e328":"code","842400d1":"code","6076a21b":"code","4780b771":"code","b2f40158":"code","40e93cb7":"code","e1a927b2":"code","45185860":"code","db9ecc05":"code","a44a4489":"code","56fa3d85":"code","2dd0da07":"code","9b2c072d":"code","47880551":"code","c03cafb2":"code","bcadff43":"code","3de99056":"code","a945dad3":"code","b5c0aad4":"code","1915e8af":"code","691a25f3":"code","60315073":"code","3906e6b2":"code","27c9f54d":"code","f6323b6d":"code","7f4b59c4":"code","6a2d6028":"code","92e41d44":"code","73887f0a":"code","140cb4e8":"code","a8ef8c64":"code","4fdf317d":"code","9d30a214":"code","fc4758f4":"code","b78eaab3":"code","e51f0b6b":"code","bb981e2b":"code","658e77e0":"code","40ec2b00":"code","2b63501d":"code","1bc9644f":"code","e6fff3be":"code","4106e74f":"code","eeef2dc9":"code","7de2099d":"code","a008ccb4":"code","2a5fa35a":"code","809ede20":"code","b5d6d5c6":"code","cb224b47":"code","caf3cc72":"code","96e2dd28":"code","0505ac93":"code","593a8fa9":"code","708f6227":"code","224a8e9f":"code","56064a86":"code","9bbbf915":"code","26baabf8":"markdown","1a3aad6a":"markdown","c8f6d847":"markdown","4e878799":"markdown","7d561339":"markdown","aff040b0":"markdown","8d9a2805":"markdown","15de8c81":"markdown","7fdf86f0":"markdown","95d1e99d":"markdown","70d4c09a":"markdown","69d88562":"markdown","37cd070b":"markdown","33734166":"markdown","91dbf16a":"markdown","82acda8a":"markdown","fa70acac":"markdown","cedef931":"markdown","f95e26a1":"markdown","add5a304":"markdown","1585a5a8":"markdown","a3fd4ac7":"markdown","19d22066":"markdown","7f8ba981":"markdown","f56f550a":"markdown","7e6b2d72":"markdown","ff0dcb36":"markdown","f475a55b":"markdown","ed471776":"markdown","6ef799a0":"markdown","5f465adb":"markdown","77f33e46":"markdown","c5dcded6":"markdown"},"source":{"253578c7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport os\nimport time\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom math import sqrt\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\n\nimport scipy.stats as stats\nfrom scipy.stats import skew\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import roc_auc_score, auc, roc_curve, recall_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.rcParams['font.sans-serif'] = ['SimHei']  # show chinese\nplt.rcParams['axes.unicode_minus']=False \n\nimport itertools\n\nimport gc","56ae9ac6":"def resumetable(df:pd.DataFrame)-> pd.DataFrame:\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary","6cd43118":"# plot confusion matrix\ndef plot_confusion_matrix(cm:np.array, classes:str, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    Input\n    - cm : \u8ba1\u7b97\u51fa\u7684\u6df7\u6dc6\u77e9\u9635\u7684\u503c\n    - classes : \u6df7\u6dc6\u77e9\u9635\u4e2d\u6bcf\u4e00\u884c\u6bcf\u4e00\u5217\u5bf9\u5e94\u7684\u5217\n    - normalize : True:\u663e\u793a\u767e\u5206\u6bd4, False:\u663e\u793a\u4e2a\u6570\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","54a33cdb":"# plot roc\ndef plot_roc_curve(y_pred, y_pred_score):\n    fpr, tpr, thresholds = roc_curve(y_pred, y_pred_score)\n    roc_auc = auc(fpr, tpr)\n    # Plot ROC\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')\n    plt.xlim([-0.1,1.0])\n    plt.ylim([-0.1,1.01])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","06677842":"train = pd.read_csv(r'..\/input\/rs6-attrition-predict\/train.csv')\ntest = pd.read_csv(r'..\/input\/rs6-attrition-predict\/test.csv')","a5462723":"resumetable(train)","137b9354":"resumetable(test)","6fe3d3cb":"train.groupby('Attrition').count()['user_id']  # Uneven sample distribution","046fd3cf":"object_features = [column for column in train if train[column].dtype == 'object']  # get the non-value features","413ce049":"object_features","64ff3e96":"train[train['Over18'] == 'N']","29ee25be":"train.groupby(['Attrition'])['Over18'].count()","48788c93":"f, ax = plt.subplots(2, 4, figsize=(20, 8))\nplt.xticks(rotation=60) \ntick_spacing = 3    # \u8bbe\u7f6e\u5bc6\u5ea6\uff0c\u6bd4\u5982\u6a2a\u5750\u68079\u4e2a\uff0c\u8bbe\u7f6e\u8fd9\u4e2a\u4e3a3,\u5230\u65f6\u5019\u6a2a\u5750\u6807\u4e0a\u5c31\u663e\u793a 9\/3=3\u4e2a\u6a2a\u5750\u6807\uff0c\nsns.countplot(x='BusinessTravel', hue='Attrition', data=train, ax=ax[0,0])\nsns.countplot(x='Department', hue='Attrition', data=train, ax=ax[0,1])\nsns.countplot(x='EducationField', hue='Attrition', data=train, ax=ax[0,2])\nsns.countplot(x='Gender', hue='Attrition', data=train, ax=ax[0,3])\nsns.countplot(x='JobRole', hue='Attrition', data=train, ax=ax[1,0])\nsns.countplot(x='MaritalStatus', hue='Attrition', data=train, ax=ax[1,1])\nsns.countplot(x='Over18', hue='Attrition', data=train, ax=ax[1,2])\nsns.countplot(x='OverTime', hue='Attrition', data=train, ax=ax[1,3])\n\nplt.subplots_adjust(wspace =0.2, hspace =0.5)#\u8c03\u6574\u5b50\u56fe\u95f4\u8ddd\nax[0,0].set_title('BusinessTravel Feature Analytics')\nax[0,1].set_title('Department Feature Analytics')\nax[0,2].set_title('EducationField Feature Analytics')\nax[0,3].set_title('Gender Feature Analytics')\nax[1,0].set_title('JobRole Feature Analytics')\nax[1,1].set_title('MaritalStatus Feature Analytics')\nax[1,2].set_title('Over18 Feature Analytics')\nax[1,3].set_title('OverTime Feature Analytics')\nf.suptitle('Object Feature Visualize', size=20, y=1.1)\nf.tight_layout()#\u8c03\u6574\u6574\u4f53\u7a7a\u767d\n# todo \u6807\u7b7e\u65cb\u8f6c","68f8fe55":"def extract_features(df, is_train=False):\n    # target\n    if is_train:\n        attrition_dict = {'No':0,'Yes':1}\n        df['Attrition'] = df['Attrition'].map(lambda x: attrition_dict[x])\n    # BusinessTravel\n    businesstravel_dict = {'Non-Travel':0, 'Travel_Rarely':1, 'Travel_Frequently':2}\n    df['BusinessTravel'] = df['BusinessTravel'].map(lambda x: businesstravel_dict[x])\n    # Department\n    department_dict = {'Sales':0, 'Research & Development':1, 'Human Resources':2}\n    df['Department'] = df['Department'].map(lambda x: department_dict[x])\n    # EducationField\n    educationfield_dict = {'Life Sciences':0, 'Medical':1, 'Marketing':2, 'Technical Degree':3, 'Human Resources':4, 'Other':5}\n    df['EducationField'] = df['EducationField'].map(lambda x: educationfield_dict[x])\n    # Gender\n    gender_dict = {'Male':0, 'Female': 1}\n    df['Gender'] = df['Gender'].map(lambda x: gender_dict[x])\n    # JobRole\n    jobrole_dict = {'Sales Executive':0, \n                    'Research Scientist':1, \n                    'Laboratory Technician':2, \n                    'Manufacturing Director':3, \n                    'Healthcare Representative':4,\n                    'Manager':5, \n                    'Sales Representative':6,\n                    'Research Director':7,\n                    'Human Resources':8\n                   }\n    df['JobRole'] = df['JobRole'].map(lambda x: jobrole_dict[x])\n    # MaritalStatus\n    maritalstatus_dict = {'Single':0, 'Married':1, 'Divorced':2}\n    df['MaritalStatus'] = df['MaritalStatus'].map(lambda x: maritalstatus_dict[x])\n    # Over18\n    df = df.drop(['Over18'], axis=1)\n    # OverTime\n    overtime_dict = {'Yes':0, 'No':1}\n    df['OverTime'] = df['OverTime'].map(lambda x: overtime_dict[x])\n    return df","b75ccf41":"train_ex = extract_features(train, True)\ntest_ex = extract_features(test, False)","0689dc97":"target = 'Attrition'  # model target (y)\nfeatures = [x for x in train_ex.columns if x not in ['Attrition', 'uesr_id', 'user_id']]  # delete invaild features \/ model input","106bfd1c":"train_x = train_ex[features]\ntrain_y = train_ex[target]","69838a62":"X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.40, random_state=1729)  # split train and test","96bcffd7":"# clf = ExtraTreesClassifier(random_state=2020)\n# selector = clf.fit(X_train, y_train)","adda4244":"# fs = SelectFromModel(selector, prefit=True)  # select feature from model\n\n# X_train = fs.transform(X_train)\n# X_test = fs.transform(X_test)\n# test = fs.transform(test_ex[features])","c79e753a":"# ## standard data\n# X_train = StandardScaler().fit_transform(X_train)\n# X_test = StandardScaler().fit_transform(X_test)\n# test = StandardScaler().fit_transform(test)","021535a8":"m2_xgb = xgb.XGBClassifier(n_estimators=1200, max_depth=9, seed=2020)\nm2_xgb.fit(X_train, y_train, eval_metric=\"auc\", verbose=False, eval_set=[(X_test, y_test)])\n#  https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\ny_pre = m2_xgb.predict(X_test)\n# calculate the auc score\nprint(\"Roc AUC: \", roc_auc_score(y_test, m2_xgb.predict_proba(X_test)[:,1], average='macro'))\n# Roc AUC:  0.7726366656599214 - test_size=0.40  not-select importance feature\n# Roc AUC:  0.770009060706735  - test_size=0.40  select importance feature\n# Roc AUC:  0.7684385382059802 - test_size=0.40 not-select importance feature but standard data, so not need standard","1406de98":"plot_confusion_matrix(confusion_matrix(y_test, y_pre), classes=['No', 'Yes'])","0b71e328":"plot_roc_curve(y_test, m2_xgb.predict_proba(X_test)[:,1])","842400d1":"probs = m2_xgb.predict_proba(test_ex[features])   # feature don't need standard or normalization","6076a21b":"test_id = test_ex.user_id","4780b771":"submission = pd.DataFrame({\"user_id\":test_id, \"Attrition\": probs[:,1]})","b2f40158":"submission.head()","40e93cb7":"submission.to_csv(f'result-{int(time.time())}.csv', index=False)","e1a927b2":"gc.enable()  # start gc","45185860":"# # these code don't need run\n# scaler = MinMaxScaler()\n# x_train = scaler.fit_transform(x_train)\n# x_test = scaler.fit_transform(x_test)","db9ecc05":"# https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn import svm","a44a4489":"random_state = np.random.RandomState(2020)\nclassifier = OneVsRestClassifier(svm.SVC(kernel='linear', probability=True, random_state=random_state))","56fa3d85":"# X_train, X_test, y_train, y_test\ny_score = classifier.fit(X_train, y_train).decision_function(X_test)","2dd0da07":"plt_roc_curve(y_test, y_score)_roc_curve(y_test, y_score)","9b2c072d":"NFOLDS = 5   # the variable for cross-validation folds\nSEED = 2020","47880551":"kf = KFold(n_splits =NFOLDS, shuffle=True, random_state=SEED)\nkf = StratifiedKFold(n_splits=NFOLDS, random_state=SEED)\n## the difference between *KFold* and *StratifiedKFold*, you can see https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html ","c03cafb2":"class SklearnWrapper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict_proba(x)[:,1]\n\nclass CatboostWrapper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_seed'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict_proba(x)[:,1]\n        \nclass LightGBMWrapper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['feature_fraction_seed'] = seed\n        params['bagging_seed'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict_proba(x)[:,1]\n\n\nclass XgbWrapper(object):\n    def __init__(self, seed=0, params=None):\n        self.param = params\n        self.param['seed'] = seed\n        self.nrounds = params.pop('nrounds', 250)\n\n    def train(self, x_train, y_train):\n#         print(x_train, y_train.value_counts())\n        dtrain = xgb.DMatrix(x_train, label=y_train)\n        self.gbdt = xgb.train(self.param, dtrain, self.nrounds)\n\n    def predict(self, x):\n        return self.gbdt.predict(xgb.DMatrix(x)) # return proba https:\/\/stackoverflow.com\/questions\/58698313\/reading-back-a-saved-lgbmclassifier-model","bcadff43":"def get_oof(clf, ntrain, ntest, x_train, y_train):  # out of fold, which means each step use the k-fold way to cross validate the dataset. \n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n#     for i, (train_index, test_index) in enumerate(kf.split(x_train)):\n    for i, (train_index, test_index) in enumerate(kf.split(x_train, y_train)):\n        if isinstance(x_train, np.ndarray):\n            x_tr = x_train[train_index]\n            y_tr = y_train[train_index]\n            x_te = x_train[test_index]\n        else:\n            x_tr = x_train.iloc[train_index]\n            y_tr = y_train.iloc[train_index]\n            x_te = x_train.iloc[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","3de99056":"et_params = {\n    'n_jobs': 16,\n    'n_estimators': 600,\n    'max_features': 0.5,\n    'max_depth': 12,\n    'min_samples_leaf': 2,\n#     'silent': 1,\n}\n\nrf_params = {\n    'n_jobs': 16,\n    'n_estimators': 600,\n    'max_features': 0.2,\n    'max_depth': 12,\n    'min_samples_leaf': 2,\n#     'silent': 1,\n}\n\nxgb_params = {\n    'booster':'gbtree',\n    'seed': 2020,\n    'colsample_bytree': 0.7,\n    'silent': 1,\n    'subsample': 0.75,\n    'learning_rate': 0.075,\n    'objective': 'binary:logistic',\n    'max_depth': 7,\n    'num_parallel_tree': 1,\n    'min_child_weight': 1,\n    'nrounds': 200\n}\n\ncatboost_params = {\n    'iterations': 600,\n    'learning_rate': 0.5,\n    'depth': 10,\n    'l2_leaf_reg': 40,\n    'bootstrap_type': 'Bernoulli',\n    'subsample': 0.7,\n    'scale_pos_weight': 5,\n    'eval_metric': 'AUC',\n    'od_type': 'Iter',\n    'allow_writing_files': False,\n    'silent': True,\n}\n\nlightgbm_params = {\n    'n_estimators':600,\n    'learning_rate':0.1,\n    'num_leaves':123,\n    'colsample_bytree':0.8,\n    'subsample':0.9,\n    'max_depth':15,\n    'reg_alpha':0.2,\n    'reg_lambda':0.4,\n    'min_split_gain':0.01,\n    'min_child_weight':2,\n    'silent': 1,\n}","a945dad3":"xg = XgbWrapper(seed=SEED, params=xgb_params)\net = SklearnWrapper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nrf = SklearnWrapper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\ncb = CatboostWrapper(clf=CatBoostClassifier, seed = SEED, params=catboost_params)\nlg = LightGBMWrapper(clf=LGBMClassifier, seed = SEED, params = lightgbm_params)","b5c0aad4":"x_train_multi = train_ex[features]\ny_train_multi = train_ex[target]\nx_test = test_ex[features]\nntrain_multi = x_train_multi.shape[0]\nntest = x_test.shape[0]","1915e8af":"# clf, ntrain, ntest, x_train, y_train\nxg_oof_train, xg_oof_test = get_oof(xg, ntrain_multi, ntest, x_train_multi, y_train_multi)\net_oof_train, et_oof_test = get_oof(et, ntrain_multi, ntest, x_train_multi, y_train_multi)\nrf_oof_train, rf_oof_test = get_oof(rf, ntrain_multi, ntest, x_train_multi, y_train_multi)\ncb_oof_train, cb_oof_test = get_oof(cb, ntrain_multi, ntest, x_train_multi, y_train_multi)","691a25f3":"print(\"XG-CV: {}\".format(roc_auc_score(y_train_multi, xg_oof_train)))\nprint(\"ET-CV: {}\".format(roc_auc_score(y_train_multi, et_oof_train)))\nprint(\"RF-CV: {}\".format(roc_auc_score(y_train_multi, rf_oof_train)))\nprint(\"CB-CV: {}\".format(roc_auc_score(y_train_multi, cb_oof_train)))\n# Fisrt Time\n# XG-CV: 0.8016840382461883\n# ET-CV: 0.810093461969162\n# RF-CV: 0.8077299939701955\n# CB-CV: 0.7908357739684727","60315073":"x_train_fin = np.concatenate((xg_oof_train, et_oof_train, rf_oof_train), axis=1)\nx_test_fin = np.concatenate((xg_oof_test, et_oof_test, rf_oof_test), axis=1)\n\nprint(\"{},{}\".format(x_train_fin.shape, x_test_fin.shape))","3906e6b2":"logistic_regression = LogisticRegression()\nlogistic_regression.fit(x_train_fin, y_train_multi)\nresult = pd.DataFrame()\nresult['user_id'] = test_ex['user_id']\nresult['Attrition'] = logistic_regression.predict_proba(x_test_fin)[:,1]","27c9f54d":"result[['user_id', 'Attrition']].to_csv(f'result-{int(time.time())}.csv', index=False, float_format='%.8f')","f6323b6d":"count_class_0, count_class_1 = train_ex.Attrition.value_counts()\ndf_class_0, df_class_1 = train_ex[train_ex[target] == 0], train_ex[train_ex[target] == 1]","7f4b59c4":"df_class_1.sample(5)","6a2d6028":"df_class_0_under = df_class_0.sample(count_class_1)\ndf_train_under = pd.concat([df_class_0_under, df_class_1], axis=0)\nprint('Random under-sampling:')\nprint(df_train_under.Attrition.value_counts())\n\ndf_train_under.Attrition.value_counts().plot(kind='bar', title='Count (target)');","92e41d44":"x_train_under_sampling = df_train_under[features].copy()\ny_train_under_sampling = df_train_under[target]\nntrain_under_sampling = y_train_under_sampling.shape[0]","73887f0a":"# clf, ntrain, ntest, x_train, y_train\nxg_oof_train, xg_oof_test = get_oof(xg, ntrain_under_sampling, ntest, x_train_under_sampling, y_train_under_sampling)\net_oof_train, et_oof_test = get_oof(et, ntrain_under_sampling, ntest, x_train_under_sampling, y_train_under_sampling)\nrf_oof_train, rf_oof_test = get_oof(rf, ntrain_under_sampling, ntest, x_train_under_sampling, y_train_under_sampling)\ncb_oof_train, cb_oof_test = get_oof(cb, ntrain_under_sampling, ntest, x_train_under_sampling, y_train_under_sampling)","140cb4e8":"print(\"XG-CV: {}\".format(roc_auc_score(y_train_under_sampling, xg_oof_train)))\nprint(\"ET-CV: {}\".format(roc_auc_score(y_train_under_sampling, et_oof_train)))\nprint(\"RF-CV: {}\".format(roc_auc_score(y_train_under_sampling, rf_oof_train)))\nprint(\"CB-CV: {}\".format(roc_auc_score(y_train_under_sampling, cb_oof_train)))\n# XG-CV: 0.773228836577637\n# ET-CV: 0.7693809416025351\n# RF-CV: 0.7701731552738798\n# CB-CV: 0.7287799909461297","a8ef8c64":"X_train, X_test, y_train, y_test = train_test_split(x_train_under_sampling, y_train_under_sampling, test_size=0.40, random_state=1729)  # split train and test","4fdf317d":"m2_xgb = xgb.XGBClassifier(n_estimators=1600, max_depth=8, seed=2020)\nm2_xgb.fit(X_train, y_train, eval_metric=\"auc\", verbose=False, eval_set=[(X_test, y_test)])\n#  https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html#sklearn.metrics.roc_auc_score\ny_pre = m2_xgb.predict(X_test)\n# calculate the auc score\nprint(\"Roc AUC: \", roc_auc_score(y_test, m2_xgb.predict_proba(X_test)[:,1], average='macro'))","9d30a214":"df_class_1_over = df_class_1.sample(count_class_0, replace=True)\ndf_train_over = pd.concat([df_class_0, df_class_1_over], axis=0)\n\nprint('Random over-sampling:')\nprint(df_train_over.Attrition.value_counts())\n\ndf_train_over.Attrition.value_counts().plot(kind='bar', title='Count (target)');","fc4758f4":"x_train_over_samping = df_train_over[features]\ny_train_over_samping = df_train_over[target]\nntrain_over_samping = y_train_over_samping.shape[0]","b78eaab3":"gc.enable()","e51f0b6b":"# clf, ntrain, ntest, x_train, y_train\nxg_oof_train, xg_oof_test = get_oof(xg, ntrain_over_samping, ntest, x_train_over_samping, y_train_over_samping)\net_oof_train, et_oof_test = get_oof(et, ntrain_over_samping, ntest, x_train_over_samping, y_train_over_samping)\nrf_oof_train, rf_oof_test = get_oof(rf, ntrain_over_samping, ntest, x_train_over_samping, y_train_over_samping)\ncb_oof_train, cb_oof_test = get_oof(cb, ntrain_over_samping, ntest, x_train_over_samping, y_train_over_samping)","bb981e2b":"print(\"XG-CV: {}\".format(roc_auc_score(y_train_over_samping, xg_oof_train)))\nprint(\"ET-CV: {}\".format(roc_auc_score(y_train_over_samping, et_oof_train)))\nprint(\"RF-CV: {}\".format(roc_auc_score(y_train_over_samping, rf_oof_train)))\nprint(\"CB-CV: {}\".format(roc_auc_score(y_train_over_samping, cb_oof_train)))","658e77e0":"x_train_fin = np.concatenate((xg_oof_train, et_oof_train, rf_oof_train, cb_oof_train), axis=1)\nx_test_fin = np.concatenate((xg_oof_test, et_oof_test, rf_oof_test, cb_oof_test), axis=1)\n\nprint(\"{},{}\".format(x_train_fin.shape, x_test_fin.shape))","40ec2b00":"logistic_regression = LogisticRegression()\nlogistic_regression.fit(x_train_fin, y_train_over_samping)\nresult = pd.DataFrame()\nresult['user_id'] = test_ex['user_id']\nresult['Attrition'] = logistic_regression.predict_proba(x_test_fin)[:,1]","2b63501d":"result[['user_id', 'Attrition']].to_csv(f'result-{int(time.time())}.csv', index=False, float_format='%.8f')","1bc9644f":"from imblearn.over_sampling import SMOTE","e6fff3be":"smote = SMOTE(ratio='minority')\nX_sm, y_sm = smote.fit_sample(train_ex[features], train_ex[target])","4106e74f":"x_test = x_test.values","eeef2dc9":"x_test.shape","7de2099d":"y_sm.shape","a008ccb4":"# clf, ntrain, ntest, x_train, y_train\nxg_oof_train, xg_oof_test = get_oof(xg, X_sm.shape[0], ntest, X_sm, y_sm)\net_oof_train, et_oof_test = get_oof(et, X_sm.shape[0], ntest, X_sm, y_sm)\nrf_oof_train, rf_oof_test = get_oof(rf, X_sm.shape[0], ntest, X_sm, y_sm)\ncb_oof_train, cb_oof_test = get_oof(cb, X_sm.shape[0], ntest, X_sm, y_sm)","2a5fa35a":"print(\"XG-CV: {}\".format(roc_auc_score(y_sm, xg_oof_train)))\nprint(\"ET-CV: {}\".format(roc_auc_score(y_sm, et_oof_train)))\nprint(\"RF-CV: {}\".format(roc_auc_score(y_sm, rf_oof_train)))\nprint(\"CB-CV: {}\".format(roc_auc_score(y_sm, cb_oof_train)))","809ede20":"x_train_fin = np.concatenate((xg_oof_train, et_oof_train, rf_oof_train, cb_oof_train), axis=1)\nx_test_fin = np.concatenate((xg_oof_test, et_oof_test, rf_oof_test, cb_oof_test), axis=1)\n\nprint(\"{},{}\".format(x_train_fin.shape, x_test_fin.shape))","b5d6d5c6":"logistic_regression = LogisticRegression()\nlogistic_regression.fit(x_train_fin, y_train_over_samping)\nresult = pd.DataFrame()\nresult['user_id'] = test_ex['user_id']\nresult['Attrition'] = logistic_regression.predict_proba(x_test_fin)[:,1]","cb224b47":"result[['user_id', 'Attrition']].to_csv(f'result-{int(time.time())}.csv', index=False, float_format='%.8f')","caf3cc72":"from imblearn.combine import SMOTETomek","96e2dd28":"smt = SMOTETomek(ratio='auto')\nX_smt, y_smt = smt.fit_sample(train_ex[features], train_ex[target])","0505ac93":"X_smt.shape","593a8fa9":"xg_oof_train, xg_oof_test = get_oof(xg, X_smt.shape[0], ntest, X_smt, y_smt)\net_oof_train, et_oof_test = get_oof(et, X_smt.shape[0], ntest, X_smt, y_smt)\nrf_oof_train, rf_oof_test = get_oof(rf, X_smt.shape[0], ntest, X_smt, y_smt)\ncb_oof_train, cb_oof_test = get_oof(cb, X_smt.shape[0], ntest, X_smt, y_smt)","708f6227":"print(\"XG-CV: {}\".format(roc_auc_score(y_smt, xg_oof_train)))\nprint(\"ET-CV: {}\".format(roc_auc_score(y_smt, et_oof_train)))\nprint(\"RF-CV: {}\".format(roc_auc_score(y_smt, rf_oof_train)))\nprint(\"CB-CV: {}\".format(roc_auc_score(y_smt, cb_oof_train)))","224a8e9f":"x_train_fin = np.concatenate((xg_oof_train, et_oof_train, rf_oof_train, cb_oof_train), axis=1)\nx_test_fin = np.concatenate((xg_oof_test, et_oof_test, rf_oof_test, cb_oof_test), axis=1)\n\nprint(\"{},{}\".format(x_train_fin.shape, x_test_fin.shape))","56064a86":"logistic_regression = LogisticRegression()\nlogistic_regression.fit(x_train_fin, y_smt)\nresult = pd.DataFrame()\nresult['user_id'] = test_ex['user_id']\nresult['Attrition'] = logistic_regression.predict_proba(x_test_fin)[:,1]","9bbbf915":"result[['user_id', 'Attrition']].to_csv(f'result-{int(time.time())}.csv', index=False, float_format='%.8f')","26baabf8":"#### Importance Feature Selector\n- Don't need to select some importance features from raw featrues, because before selecting the score is $0.85$, and after selecting the score is $0.77$.","1a3aad6a":"### Define Multi-Model","c8f6d847":"### Feature Dealing\n- String label to Value label only","4e878799":"## Import package\n- StratifiedKFold for Cross Validation\n- lightgbm\/xgboost\/catboost\/ExtraTreesClassifier\/RandomForestClassifier which is classifier for prediction\n- StandardScaler which make sample features standard","7d561339":"#### Result save to file","aff040b0":"#### Some Conclusion\n- The probability of predicting Yes to No is too high\n- Further mining features - Feature engineering","8d9a2805":"## Focal Loss\n- [FL for lightgbm](https:\/\/github.com\/jrzaurin\/LightGBM-with-Focal-Loss)\n- [FL for xgboost](https:\/\/github.com\/jhwjhw0123\/Imbalance-XGBoost)\n- [Custom Loss Function for xgboost and lightgbm](https:\/\/www.zhangqibot.com\/post\/ml-custom-loss-lgbm-xgb\/)\n- [Custom Loss Function for catboost](https:\/\/cloud.tencent.com\/developer\/article\/1513081)","15de8c81":"#### Multi-Model","7fdf86f0":"#### Split train and test set\n- The ratio of train set to test set maybe change the final score","95d1e99d":"#### Predict the Testset","70d4c09a":"### Under-Sampling\n- Random under-sampling","69d88562":"#### Single-model","37cd070b":"### Concat the result","33734166":"## Data Pre-processing","91dbf16a":"#### Define and Train Model","82acda8a":"### Result","fa70acac":"### Over-Sampling\n- 0.79561","cedef931":"### Helper Script\n- Statistic features attribute distribution of each columns **resumetable()**\n- Plot Confusion matrix **plot_confusion_matrix()**","f95e26a1":"#### Result Analysis","add5a304":"### LogisticRegression Model predict the results","1585a5a8":"#### Define model input and target","a3fd4ac7":"### Algorithm Parameters","19d22066":"### Visualization Features","7f8ba981":"### Over-sampling followed by under-sampling\n- we will do a combination of over-sampling and under-sampling, using the **SMOTE** and **Tomek links** techniques:","f56f550a":"## Imbalance Data\n- Resamping: Undersampling and Oversampling","7e6b2d72":"### xgboost","ff0dcb36":"### SVM","f475a55b":"### SMOTE(Synthetic Minority Oversampling Technique)\n- Leaderboard:0.82202**","ed471776":"### Each Step of Cross Validation","6ef799a0":"### Cross Validation(CV) Conclusion\n- Catboost algorithm is some week. so not emumate the model for predict","5f465adb":"## Multi-Model","77f33e46":"## Single Model\n- use the xgboost model as the baseline model\n- try to use svm linear kernel","c5dcded6":"#### Standardization Data\n- When using the Tree structrue to class the data, Standardization and Normalization operation is unnecessary. you can see https:\/\/zhuanlan.zhihu.com\/p\/64362722"}}