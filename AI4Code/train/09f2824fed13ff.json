{"cell_type":{"5b67f847":"code","de90e3d8":"code","9f29fbc8":"code","afbccbde":"code","85286fdc":"code","b8d18266":"code","f445355d":"code","a7e59ac6":"code","4e435d36":"code","c89b4da4":"code","63b96932":"code","ccfd0f8c":"code","7abff634":"code","261174a2":"code","6c7338ca":"code","3886527e":"code","67e95d6f":"code","7a41bb46":"code","8ecc6622":"markdown","c72bce93":"markdown","bbb87173":"markdown","862a3e79":"markdown","b59d3be9":"markdown","3b220ab3":"markdown"},"source":{"5b67f847":"import os # this makes better error tracing \nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"","de90e3d8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,LabelEncoder\nfrom sklearn.metrics import mean_squared_error\n","9f29fbc8":"from torch import nn\nimport torch\nfrom torch.nn  import functional as F\nimport torch.optim as  optim \nif torch.cuda.is_available():  \n  dev = \"cuda:0\" \n  print(\"gpu up\")\nelse:  \n  dev = \"cpu\"  \ndevice = torch.device(dev)","afbccbde":"ratings = pd.read_csv(\"\/kaggle\/input\/anime-recommendations-database\/rating.csv\")\nanime = pd.read_csv(\"\/kaggle\/input\/anime-recommendations-database\/anime.csv\")","85286fdc":"ratings","b8d18266":"anime","f445355d":"anime.nunique() # number of unique values in each column","a7e59ac6":"anime.isnull().sum()","4e435d36":"anime.eq(\"Unknown\").sum()","c89b4da4":"\"\"\"\nwe will replace null values from categorical columns with \"NULL\"(like it's a new category) \nand for the rating column we will use the mean\n\"\"\"\nanime.genre.fillna(\"NULL\",inplace=True)\nanime.type.fillna(\"NULL\",inplace=True)\nanime.rating.fillna(anime.rating.mean(),inplace=True)\n","63b96932":"\"\"\"\nthese are the animes that we don't have information about, they are just 3 and\nthey have only 10 combinations(which means they contribute with only 10 samples) with the users but they will produce null values  for other features  \nwe will drop them  \n\"\"\"\nbadAnimes = [x  for x in ratings.anime_id.unique() if x not in anime.anime_id.unique() ] \nbadIndecies = ratings[ratings.anime_id.isin(badAnimes) ].index\nratings.drop(badIndecies,inplace=True)","ccfd0f8c":"ratings.rating.eq(-1).sum() # -1 means the user didn't rate this anime","7abff634":"\"\"\"\non how weird this processing seems, but it had to be like that for memory efficiency\n\n\"\"\"\n\nratingsMean = anime.rating.mean()\nmembersMean = anime.members.mean()\nbase = anime.copy() \n\nbase.drop([\"episodes\",\"name\"],axis=1,inplace=True) #dropped episodes  many null values \n# and will cause a false  relationship as there are many movies (1 episode)\nbase.rename(columns={\"rating\":\"avgRating\"},inplace=True)\n\nbase.genre = base.genre.apply(lambda x: \" \".join(x.split(\" \")).split(\", \")) #to remove unnecessary spaces\ngenres = [] # generating the genres data\nfor i in range(len(base)):\n    for g in base.genre[i]:\n        if g in genres:\n            base[\"genre_\"+g][i] = 1\n        else:\n            base[\"genre_\"+g] = 0\n            base[\"genre_\"+g][i] = 1\n            genres.append(g)\n\n\n\nbase = pd.concat([base,pd.get_dummies(base['type'], prefix='type',dummy_na=True)],axis=1).drop(['type'],axis=1)\n\n\nratings =ratings[[\"rating\",\"user_id\",\"anime_id\"]] # rearranging columns\nbase= pd.merge(ratings,base,how=\"left\",on=\"anime_id\")\nratings = None\n\n\nbase.avgRating.fillna(ratingsMean,inplace=True)\nbase.members.fillna(membersMean,inplace=True)\nbase.drop([\"genre\"],axis=1,inplace=True)\n\n\nbase.fillna(0,inplace=True) # for any \n\n\n\n\n","261174a2":"import random\nrandom.seed(42)\n\n\"\"\"\nwe will set the productionData to the data where the users didn't rate the anime\nand we will use 500000 rows for validation\n\n\"\"\"\n\n\nproductionData= base[base.rating == -1]\nbase.drop(base[base.rating == -1].index,inplace=True)\nbase = base.sample(frac=1).reset_index(drop=True)\nvalidation = base.iloc[:500000]\ntrain = base.iloc[500000:]\nbase = None # for reducing memory usage (we won't need this df again)\n","6c7338ca":"membersEnc = MinMaxScaler()\navgRatingEnc = MinMaxScaler()\n\nanimeEncoder = LabelEncoder()\n\n\"\"\"\nsince the anime ids are not consecutive numbers, we will have to label encode them for the embedding layer \n\"\"\"\n\nanime[\"anime_id\"] = animeEncoder.fit_transform(anime[\"anime_id\"])\n\n\n\ntrain[\"anime_id\"] = animeEncoder.transform(train[\"anime_id\"])\ntrain[\"avgRating\"] = avgRatingEnc.fit_transform(train[\"avgRating\"].to_numpy().reshape(-1,1))[:,0]\ntrain[\"members\"] = membersEnc.fit_transform(train[\"members\"].to_numpy().reshape(-1,1))[:,0]\n\nvalidation[\"anime_id\"] = animeEncoder.transform(validation[\"anime_id\"])\nvalidation[\"avgRating\"] = avgRatingEnc.transform(validation[\"avgRating\"].to_numpy().reshape(-1,1))[:,0]\nvalidation[\"members\"] = membersEnc.transform(validation[\"members\"].to_numpy().reshape(-1,1))[:,0]\n\n\nproductionData[\"anime_id\"] = animeEncoder.transform(productionData[\"anime_id\"])\nproductionData[\"avgRating\"] = avgRatingEnc.transform(productionData[\"avgRating\"].to_numpy().reshape(-1,1))[:,0]\nproductionData[\"members\"] = membersEnc.transform(productionData[\"members\"].to_numpy().reshape(-1,1))[:,0]\n\n","3886527e":"\"\"\"\nfinally the neural network \n\n\"\"\"\n\nclass RecommendationNet(nn.Module):\n    def __init__(self):\n        super(RecommendationNet,self).__init__()\n        self.users = nn.Embedding(73517,100) \n        self.animes = nn.Embedding(12294,100)\n        self.linear1 = nn.Linear(100+100+54,128)\n        self.linear2 = nn.Linear(128,32)\n        self.linear3 = nn.Linear(32,1)\n    def forward(self,x):\n        user = x[:,0].long() # here am selecting the user and anime ids from the input \n        anime = x[:,1].long() \n        otherfeatures = x[:,2:]\n        userVector = self.users(user)\n        animeVector = self.animes(anime)\n#         print(userVector.shape,animeVector.shape,otherfeatures.shape) was used for debugging \n        layer1 = torch.cat((userVector,animeVector,otherfeatures),1)# concatenating vectors\n        layer2 = F.relu(self.linear1(layer1))\n        layer3 = F.relu(self.linear2(layer2))\n        out = torch.sigmoid(self.linear3(layer3)) \n        return out\n    \n\nmyNN = RecommendationNet()\nmyNN.to(device)\n        \n        \n        \n        ","67e95d6f":"optimizer = optim.Adagrad(myNN.parameters(),lr = 0.001)\n\nbatch_size = 128\n\nnpData = train.to_numpy()\nnpData[:,:1] = npData[:,:1]\/10 # scaling the target variable\n# traintrues = np.expm1(npData[:,4].reshape(-1,1)).reshape(-1)\ndef ceil(a,b):\n    return -(-a\/\/b)\n\nn_samples = len(npData)\nbetter_batch_size = ceil(n_samples, ceil(n_samples, batch_size))\n\nfor i in range(10):\n#     preds=[]\n    for i in range(ceil(n_samples, better_batch_size)):\n        batch = npData[i * better_batch_size: (i+1) * better_batch_size]\n        batch = torch.Tensor(batch).to(device)\n        X = batch[:,1:]\n        y = batch[:,:1]\n        myNN.zero_grad()\n#         print(i)\n        pred = myNN(X)\n#         preds.extend(np.expm1(pred.cpu().detach().numpy()).reshape(-1))\n        err = F.mse_loss(pred,y)\n        err.backward()\n        optimizer.step()\n    print(torch.sqrt(err))\n    valpreds = myNN(torch.Tensor(validation.to_numpy()[:,1:]).to(device)).cpu().detach().numpy().reshape(-1)\n    print(np.sqrt(mean_squared_error(validation.rating.to_numpy(),valpreds*10)),\"Validation Error\")\n","7a41bb46":"\"\"\"\nwe can use the productionData df(data where users didn't rate animes) to recommend animes for users\nwe will choose an arbitrary user id like 1 and will choose to show an arbitrary number also \nlike the top 10 animes\n\n\"\"\"\n\nnpRecommend = productionData[productionData.user_id == 1].to_numpy()\nnpRecommend[:,0] = myNN(torch.Tensor(npRecommend[:,1:]).to(device)).to(device).cpu().detach().numpy().reshape(-1)\nindecies =  np.argsort(npRecommend[:,0])[-10:][::-1]\nanime_ids = [npRecommend[i,2] for i in indecies]\n\nrecommendedAnimes = anime[anime.anime_id.isin(anime_ids)]\n    \nrecommendedAnimes\n\n\n\n","8ecc6622":"![the neural network](https:\/\/i.ibb.co\/HXyf49X\/recommendation.png)\n\nthis is the architecture of the neural network","c72bce93":"the anime df has some few null values but dropping these rows might affect the performance greatly as we should take into considerations that we will need the combinations between animes and users","bbb87173":"Anime dataframe  \n\n    anime_id - myanimelist.net's unique id identifying an anime.\n    name - full name of anime.\n    genre - comma separated list of genres for this anime.\n    type - movie, TV, OVA, etc.\n    episodes - how many episodes in this show. (1 if movie).\n    rating - average rating out of 10 for this anime.\n    members - number of community members that are in this anime's\n    \"group\".\n\n\nRatings dataframe \n\n    user_id - non identifiable randomly generated user id.\n    anime_id - the anime that this user has rated.\n    rating - rating out of 10 this user has assigned (-1 if the user watched it but didn't assign a rating).\n    \n      \n(actually this description from the dataset page)","862a3e79":"The goal is to make a deep learning model that will be used in a hybrid recommendation system \n\nembedding layers will help us with that a lot (basically a layer that maps an index to a vector of trainable  weights) \n\nI was inspired by this talk, it's really helpful\nhttps:\/\/www.youtube.com\/watch?v=HG3FDCegKVc","b59d3be9":"* Now we can predict how a specific user will rate a specific anime with some level of confidence ","3b220ab3":"Doing some preprocessing"}}