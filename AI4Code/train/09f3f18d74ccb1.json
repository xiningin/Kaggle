{"cell_type":{"862503f7":"code","a66121ab":"code","ffa8a802":"code","1d15637a":"code","f7c62b5e":"code","a7e0f898":"code","62e0182d":"code","9ed566b8":"code","2a3381c5":"code","567296cf":"code","fa905d8a":"code","002b4a6b":"code","f5250250":"code","fff5b9d8":"code","66456286":"code","d6db7af1":"code","288e2fd3":"code","76af2d0a":"code","3468992f":"markdown","d53e0771":"markdown","981525c3":"markdown","5be524dc":"markdown","b36d105a":"markdown","2113ab9d":"markdown","60eefeaa":"markdown","1b90883f":"markdown","e314f463":"markdown","fd682964":"markdown"},"source":{"862503f7":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline","a66121ab":"import numpy as np \nimport pandas as pd\nfrom IPython.display import display\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport os\nfrom pandas_summary import DataFrameSummary\nfrom matplotlib import pyplot as plt\nimport math\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\nimport re\n\nimport shap\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp, get_dataset, info_plots\n\nimport IPython\nfrom IPython.display import display\nprint(os.listdir(\"..\/input\/\"))","ffa8a802":"train_df = pd.read_csv(\"..\/input\/reducing-memory-size-for-ieee\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/reducing-memory-size-for-ieee\/test.csv\")\ntrain_df.head()","1d15637a":"test_df.head()","f7c62b5e":"import fastai_structured   ## Adding structured.py from fastai v0.7 as a utility script to the kernel\nfastai_structured.train_cats(train_df)\nfastai_structured.apply_cats(test_df, train_df)","a7e0f898":"nas = {}\ndf_trn, y_trn, nas = fastai_structured.proc_df(train_df, 'isFraud', na_dict=nas)   ## Avoid creating NA columns as total cols may not match later\ndf_test, _, _ = fastai_structured.proc_df(test_df, na_dict=nas)\ndf_trn.head()","62e0182d":"df_test.head()","9ed566b8":"del train_df, test_df","2a3381c5":"from imblearn.under_sampling import RandomUnderSampler\n\nran=RandomUnderSampler(return_indices=True) ##intialize to return indices of dropped rows\ndf_trn_sm,y_trn_sm,dropped = ran.fit_sample(df_trn,y_trn)\n\n#print(\"The number of removed indices are \",len(dropped))\n#plot_2d_space(X_rs,y_rs,X,y,'Random under sampling')","567296cf":"train_X, val_X, train_y, val_y = train_test_split(df_trn_sm, y_trn_sm, test_size=0.33, random_state=42)","fa905d8a":"from sklearn.metrics import roc_auc_score\n\ndef print_score(m):\n    res = [roc_auc_score(m.predict(train_X), train_y), roc_auc_score(m.predict(val_X), val_y)]\n    print(res)","002b4a6b":"##To reduce CPU load, and for faster iteration\nfastai_structured.set_rf_samples(200000)\ndel df_trn, y_trn, df_trn_sm, y_trn_sm","f5250250":"%time\nm = RandomForestClassifier(n_estimators=1, min_samples_leaf=5, max_depth = 3) ## Use all CPUs available\nm.fit(train_X, train_y)\n\nprint_score(m)","fff5b9d8":"def draw_tree(t, df, size=10, ratio=0.6, precision=0):\n    \"\"\" Draws a representation of a random forest in IPython.\n    Parameters:\n    -----------\n    t: The tree you wish to draw\n    df: The data used to train the tree. This is used to get the names of the features.\n    \"\"\"\n    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True,\n                      special_characters=True, rotate=True, precision=precision)\n    IPython.display.display(graphviz.Source(re.sub('Tree {',\n       f'Tree {{ size={size}; ratio={ratio}', s)))","66456286":"#draw_tree(m.estimators_[0], train_X, precision=3)","d6db7af1":"%time\nm = RandomForestClassifier(n_estimators=30, min_samples_leaf=20, max_features=0.7, \n                                n_jobs=-1, oob_score=True) ## Use all CPUs available\nm.fit(train_X, train_y)\n\nprint_score(m)","288e2fd3":"## pred = m.predict(df_test)          ## Gets an AUC of ~0.8\npred = m.predict_proba(df_test)[:,1]  ## Gets an AUC of ~0.9\nsubmission = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv')\nsubmission.head()","76af2d0a":"submission['isFraud'] = pred   \nsubmission.to_csv('rf_submission_vf.csv', index=False)","3468992f":"Initially, let's just fit a single decision tree to visualize it properly","d53e0771":"## Submitting Predictions","981525c3":"### Split the data into training and validation sets","5be524dc":"### Defining function to calculate the evaluation metric","b36d105a":"We can now pass this processed data frame to Random Forest Classifier.","2113ab9d":"We'll just use a Random Forest Classifier. For that, we need to convert all columns to numeric type. But there are some categorical variables too.","60eefeaa":"## To handle imbalanced datasets, we'll use [resampling](https:\/\/www.kaggle.com\/shahules\/tackling-class-imbalance)","1b90883f":"- To reduce CPU load, we are using kernel output from this kernel : https:\/\/www.kaggle.com\/mjbahmani\/reducing-memory-size-for-ieee\n- We are using some fastai v0.7 functions for preprocessing etc, hence I've added the structured.py file as a utility script : https:\/\/www.kaggle.com\/priteshshrivastava\/fastai-structured\n- This kernel is focussed on model interpretation using Permutation Feature Importance, Partial Dependence Plots and SHAP values.","e314f463":"We'll replace categories with their numeric codes, handle missing continuous values, and split the dependent variable into a separate variable. Fastai to the rescue again !!","fd682964":"A single decision tree did not perform so badly. You can read more about the gini impurity metric [here](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning#Gini_impurity).\n\nNow, let's bag a collection of trees to create a random forest."}}