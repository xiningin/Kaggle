{"cell_type":{"fce6f48e":"code","8451aed4":"code","ed7bc862":"code","2515ac5a":"code","bc52682e":"code","bf8361c7":"code","b19ccd85":"code","af4dd446":"code","a77ab1de":"code","5c1e4784":"code","f07fdcc5":"code","f0d9948b":"code","59cd5957":"code","430bebb7":"code","4a6980fc":"code","1aad0472":"code","0a6d53cf":"code","c026bb7e":"code","89fc6e62":"code","22c78e88":"code","cd0af0b0":"code","aae74b61":"code","78caa595":"code","e25b6178":"code","3505d462":"code","00b860bb":"code","36c9dcfd":"code","5503d17e":"code","f73ed550":"code","6dba6c5b":"code","96cb8147":"code","fafb8de2":"code","e470802c":"code","69344adf":"code","80d8ebb6":"code","918dcd75":"markdown","28ab7bbc":"markdown","2a16570e":"markdown","9ac460e8":"markdown","ff7a6403":"markdown","6fdbe9d3":"markdown","94a7cbbb":"markdown","a1288941":"markdown","769b0b05":"markdown","8f03d927":"markdown","59eeef6f":"markdown","16f0febf":"markdown","9770439b":"markdown"},"source":{"fce6f48e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\n\n\nimport re\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8451aed4":"df = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')","ed7bc862":"rslt_df = df[(df['toxic'] == 0) & (df['severe_toxic'] == 0) & (df['obscene'] == 0) & (df['threat'] == 0) & (df['insult'] == 0) & (df['identity_hate'] == 0)]\nrslt_df2 = df[(df['toxic'] == 1) & (df['severe_toxic'] == 0) & (df['obscene'] == 0) & (df['threat'] == 0) & (df['insult'] == 0) & (df['identity_hate'] == 0)]\nnew1 = rslt_df[['id', 'comment_text', 'toxic']].iloc[:23000].copy() \nnew2 = rslt_df2[['id', 'comment_text', 'toxic']].iloc[:900].copy()\nnew = pd.concat([new1, new2], ignore_index=True)\nnew.head()","2515ac5a":"from nltk.corpus import stopwords\nmy_stopwords = stopwords.words('english')","bc52682e":"import nltk\ntk=nltk.tokenize.TreebankWordTokenizer()\ncomment_tokens = [tk.tokenize(sent) for sent in new['comment_text']]","bf8361c7":"type(comment_tokens)","b19ccd85":"comment_tokens[0]","af4dd446":"len(comment_tokens)","a77ab1de":"from nltk.corpus import stopwords\nfor i in range(len(comment_tokens)):\n    comment_tokens[i] = [w for w in comment_tokens[i] if w not in stopwords.words('english')]","5c1e4784":"#glove embeddings\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\n\nembeddings_dictionary = dict()\n\nglove_file = open('\/kaggle\/input\/nlpword2vecembeddingspretrained\/glove.6B.100d.txt', encoding = \"utf8\")","f07fdcc5":"for line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = asarray(records[1:], dtype='float32')\n    embeddings_dictionary[word] = vector_dimensions\nglove_file.close()    ","f0d9948b":"print(word)","59cd5957":"print(records)","430bebb7":"print(vector_dimensions)","4a6980fc":"print(embeddings_dictionary['hello'])","1aad0472":"vocab = embeddings_dictionary.keys()","0a6d53cf":"len(vocab)","c026bb7e":"# Let's find the top 7 words that are closest to 'compute'\nu = embeddings_dictionary['compute']\nnorm_u = np.linalg.norm(u)\nsimilarity = []\n\nfor word in embeddings_dictionary.keys():\n    v = embeddings_dictionary[word]\n    cosine = np.dot(u, v)\/norm_u\/np.linalg.norm(v)\n    similarity.append((word, cosine))\nprint(len(similarity))","89fc6e62":"sorted(similarity, key=lambda x: x[1], reverse=True)[:10]","22c78e88":"# ## Now let's do vector algebra.\n# \n# ### First we subtract the vector for `france` from `paris`. This could be imagined as a vector pointing from country to its capital. Then we add the vector of `nepal`. Let's see if it does point to the country's capital\noutput = embeddings_dictionary['paris'] - embeddings_dictionary['france'] + embeddings_dictionary['nepal']\nnorm_out = np.linalg.norm(output)","cd0af0b0":"similarity = []\nfor word in embeddings_dictionary.keys():\n    v = embeddings_dictionary[word]\n    cosine = np.dot(output, v)\/norm_out\/np.linalg.norm(v)\n    similarity.append((word, cosine))\n    \nprint(len(similarity))\n\nsorted(similarity, key=lambda x: x[1], reverse=True)[:7]    ","aae74b61":"documents = []\nfor x in comment_tokens:\n    document = [word for word in x if word in vocab]\n    documents.append(document)\n#now this document have only those words which are present in our model's vocab\ndocuments[1:5]   ","78caa595":"documents[0]","e25b6178":"len(documents)","3505d462":"#checking if there is any empty list inside documents\ncounter = 0\nfor i in range (0,len(documents)):\n    if documents[i] == []:\n        counter += 1\nprint(counter)","00b860bb":"#document embeddings\nlist_v=[]\nfor i in range (0,len(documents)):\n    if documents[i] == []:\n        list_v.append(np.zeros(100,))\n    else:\n        vec = []\n        for j in documents[i]:\n            v = embeddings_dictionary[j]\n            vec.append(v)\n        list_v.append(np.mean(vec, axis=0))","36c9dcfd":"len(documents[i])","5503d17e":"len(list_v[0])","f73ed550":"from collections import Counter\nprint('Original dataset shape before smote %s' % Counter(new['toxic']))\nfrom imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX, y = oversample.fit_resample(list_v, new['toxic'])\nprint('Original dataset shape after smote %s' % Counter(y))","6dba6c5b":"#test-train split\nfrom sklearn.model_selection import train_test_split\nXw_train, Xw_test, yw_train, yw_test = train_test_split(X,y, test_size=0.3, random_state=42)","96cb8147":"from sklearn.linear_model import LogisticRegression\nclf=LogisticRegression(max_iter=1000)\nclf.fit(Xw_train,yw_train)","fafb8de2":"predicted_res=clf.predict(Xw_test)\nfrom sklearn.metrics import accuracy_score\naccuracy=accuracy_score(yw_test,predicted_res)\naccuracy","e470802c":"import numpy as np\n\nz=1.96\ninterval = z * np.sqrt( (0.8244 * (1 - 0.8244)) \/ yw_test.shape[0])\ninterval","69344adf":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\nclf3 = RandomForestClassifier() #Initialize with whatever parameters you want to\n\n# 10-Fold Cross validation\nscores = cross_val_score(clf3,Xw_train,yw_train, cv=5)","80d8ebb6":"y_p3 = clf3.fit(Xw_train, yw_train).predict(Xw_test)\naccuracy = accuracy_score(yw_test, y_p3)\nprint('Accuracy: %f' % accuracy)\n\nimport numpy as np\n\nz=1.96\ninterval = z * np.sqrt( (0.9629 * (1 - 0.9629)) \/ yw_test.shape[0])\ninterval","918dcd75":"> So there were in total this much empty vectors(output of above cell) which were form due to removal of words whch are not present in our pretrained model's vocab, now we will fill those vectors with zeros","28ab7bbc":"## LOGISTIC REGRESSION","2a16570e":"## SMOTE","9ac460e8":"Main concept - \n\nBut how can statistics represent meaning? Let me explain.\n\nOne of the simplest ways is to look at the co-occurrence matrix. A co-occurrence matrix tells us how often a particular pair of words occur together. Each value in a co-occurrence matrix is a count of a pair of words occurring together.\n\nFor example, consider a corpus: \u201cI play cricket, I love cricket and I love football\u201d. The co-occurrence matrix for the corpus looks like this:\n\nco occurrence matrix\n\nNow, we can easily compute the probabilities of a pair of words. Just to keep it simple, let\u2019s focus on the word \u201ccricket\u201d:\n\n![image](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/03\/Screenshot-from-2020-03-14-13-27-54.png)\n\np(cricket\/play)=1\n\np(cricket\/love)=0.5\n\nNext, let\u2019s compute the ratio of probabilities:\n\np(cricket\/play) \/ p(cricket\/love) = 2\n\nAs the ratio > 1, we can infer that the most relevant word to cricket is \u201cplay\u201d as compared to \u201clove\u201d. Similarly, if the ratio is close to 1, then both words are relevant to cricket.\n\nWe are able to derive the relationship between the words using simple statistics. This the idea behind the GloVe pretrained word embedding.\n\nGloVe learns to encode the information of the probability ratio in the form of word vectors.\n\n","ff7a6403":"> ","6fdbe9d3":"![image](https:\/\/nlp.stanford.edu\/projects\/glove\/images\/man_woman.jpg)","94a7cbbb":"for more please refer this link - https:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/pretrained-word-embeddings-nlp\/","a1288941":"## RANDOM FOREST","769b0b05":"> confidence interval [97.05  97.67]","8f03d927":"> So there are 20 words in last document in documents list","59eeef6f":"### Preprocessing\nNow we will Preprocess the data by removing the stopwords","16f0febf":"# GloVe: Global Vectors for Word Representation\nThe basic idea behind the GloVe word embedding is to derive the relationship between the words from Global Statistics","9770439b":"> Confidence interval [80.22  81.48]"}}