{"cell_type":{"c94f7316":"code","0bf7fe0a":"code","5300e529":"code","06b41baa":"code","5cf6f83d":"code","ec834817":"code","f3e311d6":"code","99eced01":"code","36f4510f":"code","89b852eb":"code","7840a546":"code","c92eb1bd":"code","0716c755":"code","47affc51":"code","02bdc0bb":"code","e210e763":"code","4f811130":"code","eae05c06":"code","bb676385":"code","5db3c53b":"code","955a712d":"code","05d09b0c":"code","a6de3fc9":"code","7f117ea5":"code","c4700c00":"code","c0001655":"code","75279c63":"code","99314a42":"code","cc3f55e0":"code","127f6b20":"code","c7d29b6c":"code","6213256b":"code","38165758":"code","a8dfda46":"code","ac5125ad":"code","82deb580":"code","4522a0e6":"code","53f2bdf8":"code","1f31fa66":"code","67dbe25f":"code","1914e781":"code","088705f2":"code","7055afe1":"code","d941c386":"code","829745a5":"code","91a898c1":"code","d6c62533":"code","31225bed":"code","bff4429e":"code","255bf048":"code","6b3e00d1":"code","aeaa0490":"code","609055d3":"markdown","0ec0a16a":"markdown","747efcbf":"markdown","028f1d73":"markdown","0a992d33":"markdown","35b15cbc":"markdown","80c2328b":"markdown","5d0bf021":"markdown","fdd0d42c":"markdown","93f3fc7c":"markdown","dcf2f869":"markdown","eab15b46":"markdown","9a7bb6ea":"markdown","21fa3e71":"markdown","06359e71":"markdown","05465f96":"markdown","8fa65f06":"markdown","0941f621":"markdown","65edbf58":"markdown","576e59df":"markdown","338aea3e":"markdown","d8c0175c":"markdown","90a39492":"markdown","3b3786e3":"markdown","4620c563":"markdown","b7be73ac":"markdown","be01843e":"markdown","72ffd5b5":"markdown","e8c1cadc":"markdown","2c8d2aa3":"markdown","f573ff42":"markdown","ff833a71":"markdown","413294ad":"markdown","4c331fdc":"markdown","865255a8":"markdown","7792f775":"markdown","bdae2274":"markdown","9f28ae86":"markdown","12451e17":"markdown","f3694d14":"markdown"},"source":{"c94f7316":"'''Importing necessary libraries'''\n\n#for data manipulation\nimport pandas as pd\n\n#for mathematical operations\nimport numpy as np\n\n#for data visualization\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n\n#for data preprocessing \nfrom sklearn.preprocessing import LabelEncoder, OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n\n#for modeling\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import cross_val_score , validation_curve\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\n\n#for evaluation\nfrom sklearn.metrics import mean_absolute_error, r2_score, classification_report,confusion_matrix , accuracy_score, f1_score\n\nimport time","0bf7fe0a":"import warnings\nwarnings.simplefilter('ignore')","5300e529":"\ntrain = pd.read_csv(\"..\/input\/house-prices-dataset\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-dataset\/test.csv\")","06b41baa":"train.head()","5cf6f83d":"train.columns\n","ec834817":"#train set info\ntrain.info()","f3e311d6":"train.shape\n","99eced01":"train.describe()","36f4510f":"pd.set_option('display.max_rows', 81)","89b852eb":"\n#this function gives us the number of missing values in each column\ndef basic_details(df):\n    b = pd.DataFrame()\n    b['Missing value'] = df.isnull().sum().sort_values(ascending=False)\n    b['dtype'] = df.dtypes\n    return b\nbasic_details(train)","7840a546":"missing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)","c92eb1bd":"total=train.shape[0]\nmissing=missing\/total\nmissing","0716c755":"import matplotlib.ticker as mtick\nax = missing.plot(kind='bar')\nax.yaxis.set_major_formatter(mtick.PercentFormatter(1.0))\n","47affc51":"#drop duplicate rows (if existing) since they're pretty much useless\ntrain.drop_duplicates(keep='first', inplace=True)","02bdc0bb":"#Alley, MiscFeature, PoolQC and Fence have a lot of NaN values ( more than 80% of the total of values )\ntrain=train.drop(['Id','MiscFeature','PoolQC','Fence','Alley'], axis=1)\ntest=test.drop(['MiscFeature','PoolQC','Fence','Alley'], axis=1)","e210e763":"print((train['PoolArea']==0).count())\ntrain=train.drop('PoolArea', axis=1)\ntest=test.drop('PoolArea', axis=1)","4f811130":"#we'll need id feature for submission, so we'll save it in a variable\ntest_id=test.Id\n","eae05c06":"#we'll start with checking our target's distribution \nsns.distplot(train['SalePrice'])","bb676385":"#Now let's seperate Numerical features and Categorical features, then study each category independantly\nNum_features  = train.select_dtypes(include=['int64','float64']) \nCat_features =train.select_dtypes(include=['object']) \n","5db3c53b":"Num_features.hist(figsize=(16, 20), bins=50, xlabelsize=8, ylabelsize=8);","955a712d":"#let's take for 'example GarageCars'\ntrain.nunique()","05d09b0c":"Num_corr = Num_features.corr()['SalePrice'][:-1]\ngolden_features_list = Num_corr[abs(Num_corr) > 0.5].sort_values(ascending=False)\nprint(\"There is {} strongly correlated values with SalePrice:\\n{}\".format(len(golden_features_list), golden_features_list))","a6de3fc9":"data = pd.concat([train['SalePrice'], train['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0,700000));","7f117ea5":"#process of checking the outliers\nfor i in train['GrLivArea']:\n  if i>4500:\n    print(i)\n    df_new = train.query(\"GrLivArea=={}\".format(i))\n    print(i)\n    print(df_new['SalePrice'])","c4700c00":"data = pd.concat([train['LotArea'], train['SalePrice']], axis=1)\nsns.scatterplot(data=data, x='LotArea', y='SalePrice');","c0001655":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([train['SalePrice'], train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","75279c63":"\nfor c in Cat_features:\n    train[c] = train[c].astype('category')\n    if train[c].isnull().any():\n        train[c] = train[c].cat.add_categories(['NONE'])\n        train[c] = train[c].fillna('NONE')\n","99314a42":"\nfor c in Cat_features:\n    test[c] = test[c].astype('category')\n    if test[c].isnull().any():\n        test[c] = test[c].cat.add_categories(['NONE'])\n        test[c] = test[c].fillna('NONE')","cc3f55e0":"def boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\nf = pd.melt(train, id_vars=['SalePrice'], value_vars= Cat_features)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")","127f6b20":"#seperate target and features\nX=train.drop(\"SalePrice\", axis=1)\ny=train.SalePrice","c7d29b6c":"#Seperate Numerical Columns and Categorical columns \nNum_Col  = X.select_dtypes(include=['int64','float64']).columns \nCat_Col  = X.select_dtypes(include=['object']).columns","6213256b":"from sklearn.impute import SimpleImputer\nmy_imputer= SimpleImputer(strategy='median')\nX[Num_Col] = my_imputer.fit_transform(X[Num_Col])\n\ntest[Num_Col] = my_imputer.transform(test[Num_Col])\n\n","38165758":"Pipeline = ColumnTransformer([\n    ('num', StandardScaler(), Num_Col),\n    ('cat', OrdinalEncoder(),Cat_Col)\n])","a8dfda46":"#train_val split\nx_train,x_val,y_train,y_val = train_test_split(X,y,test_size = 0.1,random_state = 42)","ac5125ad":"test=Pipeline.fit_transform(test)\ntest=pd.DataFrame(test)\ntest","82deb580":"test.info()","4522a0e6":"#Use the pipeline to transform the features\n\nx_train = Pipeline.fit_transform(x_train)\nx_val = Pipeline.transform(x_val) \n","53f2bdf8":"#Define models\nnames = [\"Linear Regression\",\"Random Forest Regressor\"]\nRegressors = [LinearRegression(), RandomForestRegressor() ]","1f31fa66":"for name, Reg in zip(names, Regressors):\n  Reg.fit(x_train, y_train)\n  preds = Reg.predict(x_val)\n  MAE = mean_absolute_error(y_val,preds)\n  R2 = r2_score(y_val,preds)\n  print (name, ' : mean absolute error  :  ', \"%.2f\" %(MAE), '        R2_Score : ', \"%.2f\" %(R2))","67dbe25f":"from sklearn.ensemble import GradientBoostingRegressor\nGBR = GradientBoostingRegressor(n_estimators=100, max_depth=4)","1914e781":"GBR.fit(x_train, y_train)","088705f2":"#accuracy of the model\nprint(GBR.score(x_val, y_val)*100)","7055afe1":"#Initialize my gridsearch parameters\nGrid_par = [\n    {'n_estimators' : [5,10,20,30,50,100] , 'max_features' : [4,5,6]},\n    {'bootstrap': [False], 'n_estimators' : [5,10,20,30,50,100] , 'max_features' : [4,5,6]},\n    {'n_estimators' : [5,10,20,30,50,100] , 'max_features' : [2,3,4],'max_depth' : [10,20,30]},\n    {'bootstrap': [False], 'n_estimators' : [5,10,20,30,50,100] , 'max_features' : [4,5,6],'max_depth' : [10,20,30]}]\n    \nmodel = RandomForestRegressor(n_jobs=-1)","d941c386":"#Randomized Search \nRandSearch = RandomizedSearchCV(estimator=model, param_distributions=Grid_par, cv = 5 ,\n                               scoring='neg_mean_absolute_error', return_train_score=True,n_iter=20)","829745a5":"start = time.time()\nRandSearch.fit(x_train,y_train)\nend = time.time()","91a898c1":"Randresult = RandSearch.cv_results_","d6c62533":"best_rand = RandSearch.best_estimator_","31225bed":"best_rand.fit(x_train,y_train)","bff4429e":"rand_preds = best_rand.predict(x_val)","255bf048":"mean_absolute_error(y_val,rand_preds)","6b3e00d1":"predic=GBR.predict(test)\n'''predic= best_rand.predict(test)'''","aeaa0490":"submit =pd.DataFrame({'Id':test_id, 'SalePrice':predic})\nsubmit.to_csv('submission_file2.csv' ,index=False)","609055d3":"##**Building our Model**","0ec0a16a":"## Categorical Features ","747efcbf":"**describe** method gives some statistics about Numerical features ","028f1d73":"As we can see, 'PoolQC', 'MiscFeature' , 'Alley', 'Fence'  'FireplaceQu' and others have NaN values.\nFor now, we'll drop the 1st four features since most than half (more than 80%) of their values are NaN which makes them uselesss for Prices Prediction","0a992d33":"We always start by visualiazing the distribution of the target feature 'SalePrice' ","35b15cbc":"Overall remarks :\n- There are some features that have a similar distribution to our target feature ( LotFrontage, GarageArea .. )\n- Some plots show very thin and distinct vertical lines even though these plots are supposed to show distribution of Numerical features ( continuous values ) \n   *  These plots correspond to features with low distinct values which makes their behaviour similar to categorical features ","80c2328b":"<font color='red'>2. correlation of these features with our target 'SalePrice'<\/font>","5d0bf021":"## **Data Visualization**","fdd0d42c":"Let's try another model, say for example *GradientBoostingRgressor*","93f3fc7c":"- Presence of two outliers ( area where GrLivArea > 4500 )","dcf2f869":"Handling missing values : We'll replace NaN values in each **Numerical** feature with the median of the values of that specific feature  ","eab15b46":"Now, let's see distribution of all the categorical variables ","9a7bb6ea":"Since, all of 'PoolArea''s values are equal to 0, this feature's presence won't affect the model's performance => Drop","21fa3e71":"##**Data Transformation** \/ **Feature Engineering**\n","06359e71":"ML Models have parameters ( we can not alter them ) and Hyperparameters, which are parameters that we can tweak \/ modify to improve the model's performance \n- And ofc we won't do that manually.\nThere are predefined techniques that search for the hyperparameters's values which are : ","05465f96":"<font color='blue'>- Standardize Numerical Features : <\/font>Here we picked standardization over normalization since there are a lot of outliers that can hold useful information and normalization tends to remove the impact of these outliers\n \n<font color='blue'>- Encode Categorical Features :<\/font> Here we used Ordinal Encoding since One-Hot encoding takes a lot of Memory Space given the fact that we have a lot of categorical features ","8fa65f06":"The target variable is right skewed\/ not normally distributed.\n\nAlso, there are multiple outliers ( they lie above ~500,000.)\nWe will eventually want to get rid of the them to get a normal distribution of the target variable","0941f621":"1st approach :  2  models in one ","65edbf58":"##**Fine Tuning on Random Forest Regressor**","576e59df":"             - GridSearch \n             - RandomizedSearch ","338aea3e":"The goal of this Project is :\ngiven some features of a house ( LotFrontage, LotArea, Alley ... ), the model will predict the Sale Price of the house.\n\nIt's a Regression Problem ( Prediction of a value ), and we'll use Machine Learning to tackle it.","d8c0175c":"- The 'SalePrice' increases with the Overall Quality of the House..Logic ! \n- Presence of outliers ( especially when OverallQual is higher than 5 )","90a39492":"##**Numerical features**\n\n","3b3786e3":"2nd approach : trying a boosting model ( good if the dataset contains outliers )","4620c563":"# **INTRO :**\n\n\nIn this Notebook we'll cover :\n1. Data Preprocessing :\n  *  Handling missing values\n  * Dealing with outliers \n  * Data transformation :\n       - Standardization ( Continuous variables )\n       - Encoding ( Categorical variables )\n  * Feature Engineering \/ Feature Selection \n\n2. Modeling :\n  1. Data Splitting : K-fold Cross Validation\n  2. Trying different Machine Learning models :\n\n    Since it's a regression problem, we'll begin with :\n       - Linear regression\n       - Polynomial regression\n       - Random Forest Regressor (Grid Search to pick best hyperparameters )\n      \n  3. Evaluation \n\n","b7be73ac":"Since it's a regression problem, we calculate error to evaluate the model's performance \nwe picked RMSE as an evaluation metric ","be01843e":"###Correlation Matrix ","72ffd5b5":"Since we have a lot of numerical features, it will take us a considerable amount of time to plot each plot seperately.\nThankfully, there's a function that plots them all in the same grid ","e8c1cadc":"As we can see there are some columns that are highly correlated with the target feature 'SalePrice'.\n","2c8d2aa3":"'GarageCars'  has only 5 distinct values\n","f573ff42":"# **Create-Submission**","ff833a71":" - OverallQual and GrLivArea are highly correlated with SalePrice.\nLet's look into them in more details.","413294ad":"\n\n##  **Import Data**\n","4c331fdc":"To get more insights on the most suitable **Regression Algorithm** for solving this problem (linear, polynomial, ensemble .. ), we need to look at the distributions of the features by plotting them","865255a8":"Here, we'll handle missing values :\n- Non numerical features : Replace them with 'NONE'\n- Numerical features : Impute ","7792f775":"# **Data Exploration**","bdae2274":"There are multiple outliers which can cause overfitting but at the same time outliers can hold valuable informations.\nThat's why we can't decide yet if we should drop them all or not at this stage since we're not certain of their origin.\n\n","9f28ae86":"## **Import Libraries** \n\n\n","12451e17":"<font color='red'>3. Relationship between some of the features ( Scatterplots ! )<\/font>","f3694d14":"<font color='red'> 1. Distribution of the features <\/font>\n"}}