{"cell_type":{"6776cc5b":"code","57ea2d78":"code","4d33f0de":"code","8a3f5ef6":"code","163927d4":"code","22c17737":"code","f87a159e":"code","9d661728":"code","13daab52":"code","cf8635eb":"code","140176e4":"code","a3e2d4d9":"code","ab9a158d":"code","d5a3a469":"code","078fe7bc":"code","61ab315a":"code","bd8d6674":"markdown"},"source":{"6776cc5b":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Model Evaluation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport shap\n\n# Hyperparam Tuning\nfrom skopt import dummy_minimize\nfrom skopt import gp_minimize\n\n# Models used for the analysis\nimport catboost\nfrom catboost import Pool\n\n# Version check\nprint(f'catboost_version: {catboost.__version__}')","57ea2d78":"path = '..\/input\/covid19\/Kaggle_Sirio_Libanes_ICU_Prediction.xlsx'\ndata = pd.read_excel(path)\ndata.shape","4d33f0de":"# Corr matrix\ncorr = data.corr()\n\n# Select only variables with +.20 possitive corr, and -.20 negative corr!\nteste = corr[(corr['ICU'] >= 0.20)|(corr['ICU'] <= -0.20)]\nteste.index.values","8a3f5ef6":"# window var added with the top 62 corr vars\ntop_62_vars = [\"AGE_ABOVE65\", \"ALBUMIN_MEDIAN\", \"ALBUMIN_MEAN\", \"ALBUMIN_MIN\",\n       \"ALBUMIN_MAX\", \"BE_ARTERIAL_MEDIAN\", \"BE_ARTERIAL_MEAN\",\n       \"BE_ARTERIAL_MIN\", \"BE_ARTERIAL_MAX\", \"BE_VENOUS_MEDIAN\",\n       \"BE_VENOUS_MEAN\", \"BE_VENOUS_MIN\", \"BE_VENOUS_MAX\",\n       \"HEMATOCRITE_MEDIAN\", \"HEMATOCRITE_MEAN\", \"HEMATOCRITE_MIN\",\n       \"HEMATOCRITE_MAX\", \"HEMOGLOBIN_MEDIAN\", \"HEMOGLOBIN_MEAN\",\n       \"HEMOGLOBIN_MIN\", \"HEMOGLOBIN_MAX\", \"LACTATE_MEDIAN\", \"LACTATE_MEAN\",\n       \"LACTATE_MIN\", \"LACTATE_MAX\", \"LEUKOCYTES_MEDIAN\", \"LEUKOCYTES_MEAN\",\n       \"LEUKOCYTES_MIN\", \"LEUKOCYTES_MAX\", \"NEUTROPHILES_MEDIAN\",\n       \"NEUTROPHILES_MEAN\", \"NEUTROPHILES_MIN\", \"NEUTROPHILES_MAX\",\n       \"UREA_MEDIAN\", \"UREA_MEAN\", \"UREA_MIN\", \"UREA_MAX\",\n       \"BLOODPRESSURE_DIASTOLIC_MEAN\", \"RESPIRATORY_RATE_MEAN\",\n       \"BLOODPRESSURE_DIASTOLIC_MEDIAN\", \"RESPIRATORY_RATE_MEDIAN\",\n       \"BLOODPRESSURE_DIASTOLIC_MIN\", \"HEART_RATE_MIN\", \"TEMPERATURE_MIN\",\n       \"OXYGEN_SATURATION_MIN\", \"BLOODPRESSURE_SISTOLIC_MAX\", \"HEART_RATE_MAX\",\n       \"RESPIRATORY_RATE_MAX\", \"OXYGEN_SATURATION_MAX\",\n       \"BLOODPRESSURE_DIASTOLIC_DIFF\", \"BLOODPRESSURE_SISTOLIC_DIFF\",\n       \"HEART_RATE_DIFF\", \"RESPIRATORY_RATE_DIFF\", \"TEMPERATURE_DIFF\",\n       \"OXYGEN_SATURATION_DIFF\", \"BLOODPRESSURE_DIASTOLIC_DIFF_REL\",\n       \"BLOODPRESSURE_SISTOLIC_DIFF_REL\", \"HEART_RATE_DIFF_REL\",\n       \"RESPIRATORY_RATE_DIFF_REL\", \"TEMPERATURE_DIFF_REL\",\n       \"OXYGEN_SATURATION_DIFF_REL\", \"ICU\", \"WINDOW\"]\n\ntop_vars = data.loc[:,top_62_vars]","163927d4":"x = top_vars.drop('ICU', axis=1)\ny = top_vars['ICU']\n\nX_train, X_val, y_train, y_val = train_test_split(x, y, test_size=0.2, \n                                                  random_state=0)","22c17737":"# selectin all the features execept for the y variable\nfeatures = [feat for feat in list(top_vars) \n            if feat != 'ICU']\n\ncategorical_features1 = np.where(top_vars[features].dtypes != np.float)[0]\n\nclf = catboost.CatBoostClassifier(\n    iterations=10,\n    random_seed=301,\n    learning_rate=0.3,\n    custom_loss=['AUC', 'Accuracy']\n)\n\nclf.fit(X_train, y_train, \n        cat_features=categorical_features1,\n        eval_set=(X_val, y_val), \n        verbose=1,\n        use_best_model=True, \n        plot=True\n)","f87a159e":"preds = clf.predict(X_val)\nprint(confusion_matrix(y_val, preds))\nprint(accuracy_score(preds, y_val) * 100)","9d661728":"# same for the full data\nfeatures= [feat for feat in list(data) \n            if feat != 'ICU']\n\ncategorical_features = np.where(data[features].dtypes != np.float)[0]\n\n# Select the full raw data for split\nx_f_data = data[features]\ny_f_data = data[['ICU']]\n\nX_f_train, X_f_val, y_f_train, y_f_val = train_test_split(\n                                                  x_f_data, y_f_data, \n                                                  test_size=0.2, \n                                                  random_state=0)","13daab52":"model_f = catboost.CatBoostClassifier(\n    iterations=100,\n    random_seed=42,\n    learning_rate=0.2, \n    custom_loss=['AUC', 'Accuracy']\n)\n\nmodel_f.fit(X_f_train, y_f_train, \n        cat_features=categorical_features,\n        eval_set=(X_f_val, y_f_val), \n        verbose=100,\n        plot=True\n)\nprint('CatBoost model is fitted: ' + str(clf_f.is_fitted()))\nprint('CatBoost model parameters:')\nprint(clf_f.get_params())","cf8635eb":"preds_f = model_f.predict(X_f_val)\nprint(confusion_matrix(y_f_val, preds_f))\nprint(accuracy_score(preds_f, y_f_val) * 100)","140176e4":"# Tip: for model hyper Parameter tuning you can use '?your model',\n# it shows all the params that it uses\n?catboost.CatBoostClassifier","a3e2d4d9":"def train_model(params):\n    learning_rate = params[0]\n    subsample = params[1]\n    min_child_samples = params[2]\n    depth = params[3]\n    \n    print(params, '\\n')\n    \n    model = catboost.CatBoostClassifier(learning_rate=learning_rate, depth=depth,\n                                        min_child_samples=min_child_samples,\n                                        subsample=subsample, random_seed=42,\n                                        custom_loss=['AUC', 'Accuracy'])\n    \n    \n    model.fit(X_f_train, y_f_train,\n             cat_features=categorical_features,\n             eval_set=(X_f_val, y_f_val), \n             verbose=100\n             #plot=True,\n             )\n    \n    predh_f = model.predict(X_f_val)\n    \n    return -accuracy_score(predh_f, y_f_val)\n\n# space of hyperparamtuning\nspace = [(1e-3, 1e-1, 'log-uniform'), # Learning_rate\n         (0.05, 1.0), #Subsample\n         (1, 100),    #Min_child_samples\n         (4, 10)      #Depth\n        ]\n\nresult = dummy_minimize(train_model, space, \n                        random_state=1, verbose=1,\n                        n_calls=10)","ab9a158d":"result.x\n# [0.09871192514273254, 0.935929491371726, 10, 7] - acc(92.47%)","d5a3a469":"# Bayesian Optimization\nresults_gp = gp_minimize(train_model, space, random_state=1, n_calls=15, n_random_starts=5)","078fe7bc":"results_gp.x\n# [0.07431528396574379, 0.8539953708517165, 32, 7] - acc(91.68%)","61ab315a":"shap_values = model_f.get_feature_importance(Pool(X_f_val, label=y_f_val,\n                                                cat_features=categorical_features),\n                                                type=\"ShapValues\")\n\nexpected_value = shap_values[0, 1]\nshap_values = shap_values[:,:-1]\n\nshap.initjs()\nshap.summary_plot(shap_values, X_f_val)","bd8d6674":"<h3>Summary:<\/h3>\n<p>With the top62 vars the catmodel performed: <b>84.67532467532467<\/b><\/p>\n<p>[258  15]<\/p>\n<p>[44   68]<\/p>\n\n<p> and with all the data the model performed: <b>91.42857142857143<\/b> <\/p>\n<p>[259  14]<\/p>\n<p>[19  93]<\/p>\n\n<p> Bayesian Optimazation + full data the model performed: <b>91.68%<\/b><\/p>\n<p> params: [0.07431528396574379, 0.8539953708517165, 32, 7] <\/p>\n<p>[262  11]<\/p>\n<p>[ 21  91]<\/p>\n\n<p> Random search + full data the model performed: <b>92.47%<\/b> <\/p>\n<p> params:[0.09871192514273254, 0.935929491371726, 10, 7] <\/p>\n\n\n<p>\nin this case the random search showed better results, but I believe that running the Baysean optimization for more iterations can generate better results than <b>92.47<\/b>\n<\/p>\n\n"}}