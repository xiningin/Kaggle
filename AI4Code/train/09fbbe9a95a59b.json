{"cell_type":{"2b4397bb":"code","3361aed7":"code","7e60a06a":"code","c20e7c85":"code","376e9f3a":"code","385bb18b":"code","19b7ab57":"code","3ca08711":"code","4cc86306":"code","d1556b02":"code","56fec1aa":"code","a289723a":"code","1d8a9f74":"code","dadcd405":"code","58e7a07a":"code","923125fd":"code","c4ddc6c0":"code","6110d728":"code","709193e8":"code","2f28745f":"code","ecadf281":"code","b62cd717":"code","64b91ff2":"code","d230724a":"code","9852ddab":"code","dccff251":"code","b4ab6ff5":"code","c89adb18":"code","e108f407":"code","61234b8a":"code","3e42d770":"code","6674809c":"code","480564ac":"code","4b52b0d9":"code","7e481342":"code","831cdf58":"code","50f2244e":"code","c8b0080c":"code","3175ec35":"code","332fb249":"code","cc0b05cd":"code","0a46a863":"code","9c06dea1":"markdown","83ae7053":"markdown","3d054891":"markdown","1371e295":"markdown","cc096601":"markdown","ac8181f9":"markdown","c2bf2c24":"markdown","1e173393":"markdown","bf238721":"markdown","a797f98a":"markdown","06c24008":"markdown","ba4c4f94":"markdown","c4d896bf":"markdown","99fd2c5b":"markdown","b8da5512":"markdown","edece2c0":"markdown","bc0510a5":"markdown","721f681d":"markdown","e9dbcb5a":"markdown","3314ebde":"markdown","d39d9df5":"markdown","d81c26fb":"markdown","0085a315":"markdown","69d08fd2":"markdown","2a95a4b2":"markdown","5819976e":"markdown","d784e1f5":"markdown","f6315df9":"markdown","36c6365a":"markdown","3440f755":"markdown","621800e0":"markdown","85110e9b":"markdown","93dc2879":"markdown","8c8f011e":"markdown","7090138c":"markdown"},"source":{"2b4397bb":"import fastai\nprint(\"Fastai version : \" + fastai.__version__)","3361aed7":"import pandas as pd\nimport numpy as np\n\n\nfrom fastai import *\nfrom fastai.tabular.all import *\n\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom IPython.display import Markdown, display\npd.options.mode.chained_assignment = None  # default='warn'","7e60a06a":"test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\n\nprint(f\"train shape: {train.shape} | test shape: {test.shape}\")","c20e7c85":"print(\"% Null distribution\")\n(train.isnull().sum()*100\/train.shape[0]).sort_values(ascending = False).to_frame().T","376e9f3a":"print(\"# Uniques\")\ndisplay(train.nunique().sort_values(ascending = False).to_frame().T)","385bb18b":"test_ext = pd.read_csv('\/kaggle\/input\/titanic-extended\/test.csv')\ntrain_ext = pd.read_csv('..\/input\/titanic-extended\/train.csv')\n\n\nprint(f\"train extended shape: {train_ext.shape} | test extended shape: {test_ext.shape}\")","19b7ab57":"print(\"% Null distribution\")\ndisplay((train_ext.isnull().sum()*100\/train.shape[0]).sort_values(ascending = False).to_frame().T)","3ca08711":"print(\"# Uniques\")\ndisplay(train_ext.nunique().sort_values(ascending = False).to_frame().T)","4cc86306":"req_cols = train_ext.columns.difference(['Body','Cabin' # high null features\n                                         ,'Age','Class','Name','Embarked'# duplicate features\n                                         ,'PassengerId','Name_wiki','WikiId'] # high cardinality features\n                                       ).tolist()\nprint(\"Required features :\", req_cols)","d1556b02":"# filtering train and test on required columns\ntrain_ext_fltr = train_ext[req_cols]\ntest_ext_fltr = test_ext[[col for col in req_cols if col != \"Survived\"]]","56fec1aa":"# categorical features\ncat_feat = train_ext_fltr.select_dtypes(\"O\").columns.tolist()\n\n# continuos features\ncont_feat = train_ext_fltr.columns.difference(cat_feat+[\"Survived\"]).tolist()","a289723a":"print(\"categorical data:\")\ndisplay(train_ext_fltr[cat_feat].head(2))\n\n\nprint(\"continuos data:\")\ndisplay(train_ext_fltr[cont_feat].head(2))","1d8a9f74":"print(\"% Null distribution\")\ndisplay((train_ext_fltr.isnull().sum()*100\/train_ext_fltr.shape[0]).sort_values(ascending = False).to_frame().T)","dadcd405":"# splitting criteria - random split with 20% in validation set\nsplits = RandomSplitter(valid_pct=0.2, seed=1000)(range_of(train_ext_fltr))\n\ntp = TabularPandas(train_ext_fltr, procs=[Categorify, FillMissing,Normalize],\n                   cat_names = cat_feat,\n                   cont_names = cont_feat,\n                   y_names='Survived',\n                   y_block=CategoryBlock(),\n                   splits=splits)","58e7a07a":"tp.show(max_n=3)","923125fd":"print(\"filling strategy:\",tp.fill_strategy)","c4ddc6c0":"# we can see `_na` are added to cat_feat list\nprint(\"we can see that corresponding  _na (bool) features have been added wherever the columns have nulls\")\nprint([col for col in cat_feat if  \"_na\" in col])","6110d728":"# normalize transform\nnorms = tp.procs.normalize","709193e8":"print(\"Mean\/STD for each continuos col\")\ncont_col_stat = pd.DataFrame([norms.means, norms.stds], index = [\"mean\",\"std\"])\ncont_col_stat","2f28745f":"# At index = 299 the value for Age_wiki is 1.528887\ntp.conts.head(1)","ecadf281":"# Mean & Std for Age feature\nage_mean = cont_col_stat[\"Age_wiki\"][\"mean\"]\nage_std = cont_col_stat[\"Age_wiki\"][\"std\"]\n\n# standardizing age_wiki for 299 index\nage_index_299 = train_ext_fltr.loc[299,\"Age_wiki\"]\nage_std = (age_index_299 - age_mean)\/age_std\n\nprint(\"Standard Age_wiki using (X-Xmean\/Xstd) : \",age_std)","b62cd717":"print(\"means calculated for normalizing\")\ndisplay(cont_col_stat.iloc[0].to_frame().T)\n\nprint()\n\nprint(\"means calc manually\")\ndisplay(train_ext[cont_feat].mean().to_frame().T)","64b91ff2":"print(\"lets look at how label-encodings are done for categorical data \\n\")\nlabel_encoding_dict = {cat : tp.procs.categorify[cat].o2i for cat in tp.cat_names}\n\n\nprint(\"label encodings for 'boarded' variable\")\nlabel_encoding_dict['Boarded']","d230724a":"print(\"encoded data\")\ndisplay(tp.items.iloc[[0]])\n\nprint()\n\nprint(\"original data - categorical values\")\nrow = tp.items.iloc[0]\ntp.decode_row(row).to_frame().T","9852ddab":"emb_szs = get_emb_sz(tp)\n\nprint(\"Embedding size associated with each categorical feature:\\n\")\nprint({k:v for k,v in zip(tp.cat_names,emb_szs)})","dccff251":"print(\"# uniques in categorical features\")\ntp[cat_feat].nunique().to_frame().T","b4ab6ff5":"# creating TabDataloaders\ntrn_dl = TabDataLoader(tp.train, bs=64, shuffle=True, drop_last=True)\nval_dl = TabDataLoader(tp.valid, bs=64)","c89adb18":"# creating dataloaders\ndls = DataLoaders(trn_dl, val_dl)","e108f407":"print(\"Dataloaders is just combining the train and validation TabDataloader\")\nprint(dls.train == trn_dl)","61234b8a":"# lets build the NN with 2 layer - [30, 10 Neurons]\nlearn = tabular_learner(dls, layers=[30,10], metrics=accuracy)","3e42d770":"# model architecture\nlearn.summary()","6674809c":"print(\"size of embeddings\")\nprint(learn.embeds)","480564ac":"learn.lr_find()","4b52b0d9":"learn.fit_one_cycle(10, lr_max = 5e-2)","7e481342":"plt.title(\"cyclic learning rate\")\nplt.xlabel(\"training step\")\nplt.ylabel(\"learning rate\")\nplt.plot(range(len(learn.recorder.lrs)),learn.recorder.lrs)","831cdf58":"plt.title(\"Loss trend\")\nplt.xlabel(\"training step\")\nplt.ylabel(\"cross-entropy loss\")\nplt.plot(range(len(learn.recorder.losses)),learn.recorder.losses)","50f2244e":"# checking validation results\nlearn.recorder.show_results(max_n = 3)","c8b0080c":"print(\"This is how the model's architecture looks! \\n\")\nlearn.model","3175ec35":"concat_embed_size = sum([x.weight.shape[1] for x in learn.embeds])  # 163 size\nn_cont_feat = 5 # continuos data\n\nprint(\"total size of concatenated categorical embeddings and continuos features equals ::\",  concat_embed_size + n_cont_feat)","332fb249":"# imputing fare with median (coz training data did not have any missings for Fare)\nfare_med_val = test_ext_fltr.Fare.median()\ntest_ext_fltr.loc[:,'Fare'].fillna(fare_med_val, inplace = True)","cc0b05cd":"t1 = learn.dls.train_ds.new(test_ext_fltr)\nt1.process()\n\ndl = TabDataLoader(t1)\npreds = learn.get_preds(dl=dl)[0].argmax(1).numpy()","0a46a863":"out = pd.DataFrame({'PassengerId':test_ext.PassengerId, 'Survived': preds.astype(int)})\nout.to_csv('submission_1.csv', index=False)\nout.head()","9c06dea1":"# <p style=\"background-color:#535353;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">NN architecture [IMP]<\/p>\n\n- This part is slightly experimentational (basic understanding of `pytorch` is needed) - please bear me for any slight mistakes!\ud83d\ude22\n\n- Honestly, I was `not impressed` with the learner part above, thought I should add this too for `interested users`\n\n---------\n\n**Let's talk more about NN architecture - let's go step by step!**\n\n[refer to NN arch.](#NN)\n\n- **STEP 1 :** At the start we have `7 embeddings` layers (explained earlier) for each of our 7 `categorical features` with respective sizes\n\n- **STEP 2:** We also have a `Dropout` with `p == 0`, not sure why ?\n\n- **STEP 3:** We have a `BatchNorm1d` layer of `size = 5`, why? `common you can guess this!!` (BTW do we need BatchNorm for categorical data in this case?)\n\n- **STEP 4:** Then we have a sequential block with multiple  `[Linear, ReLU, Batchnorm]` layers\n    - I am not going in as to why we need `Batchnorm` & `ReLU`\n    \n    - My only question is **why in_features == 168 for the first initial layer ?**\n    \n    ![image.png](attachment:49eeff26-2412-4989-a3e0-4b2286a96fc4.png)\n    \n    - Let me try: \n\n        ![image.png](attachment:40c91876-9450-4d7a-b970-75e06d7b60e4.png)\n\n        - This is the step where we are **concatenating** the `categorical data embeddings` and `continuos data` [shown above]        \n        \n        - let's validate that : \n            if we are concatenating the categorical data embeddings then our column dimension should be\n            \n            **sum(Embeddings) + 5 (continuos features) == 168** \n            > [The above conition holds true as shown in the code below](#assert)\n    \n- **STEP 5:** After the 168 neuron Linear layer we have 2 more Linear layers giving 2 values for `binary classification` as `out_features` in the final layer","83ae7053":"- **Another fun thing!**\n    - Can someone tell me why aren't the calculated normalizing means matching when we are calculating means manually ?? `Its easy`","3d054891":"- **`Categorical Features`**","1371e295":"# <p style=\"background-color:#535353;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Fastai tabular setup<\/p>\n\n### Fastai has a couple cool features!\n\n> **Handles `categorical data` (no encodings needed)**\n\n> **Handles `missing data`**","cc096601":"- **`Continuos Features`**","ac8181f9":"- **`Dataloaders`**","c2bf2c24":"# <p style=\"background-color:#535353;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Dataloader<\/p>\n\n- Now what's `Dataloader!!`\n\n> PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow you to use pre-loaded datasets as well as your own data. \n\n> **`Dataset stores the samples and their corresponding labels`**, and **`DataLoader wraps an iterable around the Dataset`** to enable easy access to the samples.\n\n\n--------------\n\n- Okay, but we have **TabDataLoader** and **TabularPandas**\n\n<div class=\"alert alert-block alert-success\">  \n\n<b>TabDataLoader<\/b> inherits from <b>Dataloader<\/b> class (is a Dataloader only)\n\n<\/div>\n\n![image.png](attachment:7b6413ca-ca5c-4a7c-b9f6-60e97fd70173.png)\n\n\n\n > Note! : `TabDataLoader` inherits from `TfmdDL` which inturn inherits from `Dataloader` class (pls check source-code)\n\n\n-------------\n\n<div class=\"alert alert-block alert-success\">  \n\n<p>\n<b>TabularPandas<\/b> : It's a <b>Tabular<\/b> object with transforms<br>\n<li><b>Tabular<\/b> is a <b>DataFrame wrapper<\/b> that knows which cols are cont\/cat\/y, and returns rows in __getitem_<\/li>\n<\/p>\n    \n<\/div>\n\n","1e173393":"# <p style=\"background-color:#535353;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Feature Engg.<\/p>\n\n\n<div class=\"alert alert-block alert-success\">  \n<p>\n<li>Fastai comes with powerful feature engineering builtin features like <b> Categorify, Normalize,Fillmissing <\/b> <\/li><br>\n<li>We will only be doing  <b>basic feature engg.<\/b> here coz fastai takes care of it <\/li> \n<\/p>\n<\/div>\n\n\n### Removing the following features \n\n- **High # Nulls**\n    - Body : 804 Nulls\n    - Cabin : 687 Nulls\n    \n\n- **Duplicate feature in `titanic-extended` and `titanic` data**\n    - Age : Age_wiki\n    - Class : Pclass\n    - Name : Name_wiki\n    - Embarked : Boarded\n    \n    \n- **High Cardinality**\n    - Name : 891 distinct values\n    - PassengerId : 891 distinct values\n    - Name_wiki : 889 distinct values\n    - WikiId : 889 distinct values","bf238721":"# <p style=\"background-color:#535353;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Normalize<\/p>\n\n### Fastai normaizes continuos features using `(Xi - Xmean) \/ Xstd`\n\n> **`Let's check how it happens under the hood`**","a797f98a":"<div class=\"alert alert-block alert-success\">  \n\n<p>    \n    \n<li><b>lr_find<\/b> is a real good fastai feature.It lets you find the <b>optimum max_lr<\/b><\/li><br>\n<li>This is very handy if you are using fasti's <b>fit_one_cycle<\/b> <\/li>\n<\/p>    \n<\/div>\n\n> Wanna know more!\n[fit_one_cycle & max_lr explained](#fastai_concept)","06c24008":"### Categorify\n> **Transforms columns that are in your `cat_names` into that categorical type, along with `label encoding` our categorical data**","ba4c4f94":"- **`TabDataloaders`**","c4d896bf":"**Okay, so ideally the first param should follow `embedding_size == # unique categories`**\n\n\n- However, you see the counts is 1 more for `Sex`, `Ticket` and `Age_wiki_na` , why ?\n    - Honestly, even I dont know but let me try!\n    - Ans: The embedding `adds` an additional dimension(category) to handle `potential nulls (#na#)` for columns which have `NOT` seen Nulls \n    \n    Ref : https:\/\/forums.fast.ai\/t\/size-of-matrix-in-colab-learner-not-matching\/72017\/6","99fd2c5b":"### Loss Trend","b8da5512":"### Titanic extended dataset","edece2c0":"# <p style=\"background-color:#696969;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Inference<\/p>\n\n- **Predicting the Test-set**","bc0510a5":"# <p style=\"background-color:#535353;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Dataset Comparison<\/p>\n\n> **Here, we are using dataset which is a bit extension of classic titanic dataset. you can check out this: https:\/\/www.kaggle.com\/pavlofesenko\/titanic-extended**","721f681d":"# <p style=\"background-color:#535353;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Tabular Learner<\/p>\n\n<div class=\"alert alert-block alert-success\">  \n\n<p>    \n<li> <b>Fastai<\/b> uses <b>Tabular learner<\/b> to initialize the <b>Neural Network<\/b> <\/li> <br> \n<li> <b>Tabular learner<\/b> <b>automatically<\/b> create a TabularModel suitable for your data and <b>infer the right loss function<\/b> <\/li>\n<\/p>\n    \n<\/div>\n\n**Learner Params**\n    \n- Loss : `FlattenedLoss of CrossEntropyLoss()` : what's that. common, just google it!\n- Optimizer : `Adam`\n    \n- Incase you are wondering, how `params #` are calc: just use ``` learn.embeds ``` ","e9dbcb5a":"- Lets look at how a row at `index 0` in the data is `label-encoded`, use below code\n- You can see that the `Boarded` variable for value `Cherbourg` is encoded as `2` which can be verified in the above encodings","3314ebde":"### Titanic dataset","d39d9df5":"# <p style=\"background-color:#535353;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">TabularPandas<\/p>\n  \n<div class=\"alert alert-block alert-success\">  \n<b>TabularPandas<\/b> is fastai's datastructure for dataprocessing <b>tabular data<\/b>  \n<\/div>\n\n- We need to add 3 transformation methods for data processing\n    - **Categorify:** for handling categorical data. Fastai trains `embedding vectors` for categorical features\n    - **FillMissing:** \n        - fills missing categorical data with `#na#` value . \n        - Also creates a new `is_na` type of feature for continuos features after imputing them with `mean\/median`\n    - **Normalize:** for normalizing the input data\n    \n    \n- **RandomSplitter** \n    - As the name suggests, It splits the data in train and validation (80 :20)\n\n-------------\n \n> **TabularPandas parameters explained**\n\n![image.png](attachment:05f8d3fa-d77f-4544-bdbc-4e8bdee2c865.png)","d81c26fb":"- `Hint`\n\n```\nt1 = train_ext_fltr.iloc[tp.train.xs.index.tolist()].Age_wiki\nt1.fillna(t1.median()).mean()\n```","0085a315":"# <p style=\"background-color:#535353;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Expectations<\/p>\n\n<div class=\"alert alert-block alert-success\">  \n<li> <b>fastai<\/b> is a <b>deep learning library<\/b> which provides practitioners with high-level components that can <b>quickly<\/b> and easily provide <b>state-of-the-art<\/b> results in standard deep learning domains <\/li>\nref : https:\/\/docs.fast.ai\/\n\n<li>This kernel is focussed on <b>fastai-tabular<\/b> (fastai's implementation for tabular data) <\/li>\n    \n<\/div>\n\n \n- The Notebook caters to develop an **indepth** understanding on the **implementation of fastai-tabular**  \n- There are few concepts that I have mentioned and `explained(briefy) [1cycle-policy, etc]`. I'd recommend going through `fastai course` for deeper understanding on those concepts\n- Though I have been able acheive `top-1.5%` in Titanic. This notebook is at-best a `baseline` as far as good ML practices are concerned\n\n-----------\n\n<div class=\"alert alert-block alert-info\">  \nThis NB is getting <b>forked<\/b> a lot, would request you to <b>upvote<\/b> it too if you like it  <\/div>\n\n- **`Disclaimer:`**  This a NB is mainly focussed on `Fastai tabular implementation` and `detailed explaination` of the framework","69d08fd2":"<a id=\"assert\"><\/a> \n- `Assert if N_cont + N_cat_embed == 168`","2a95a4b2":"- Let's check for one observation!\n    - At `index = 299` the value for Age is `1.528887`\n    - We can see that `Age_wiki` is standardized using `(Xi - Xmean) \/ Xstd`","5819976e":"# <p style=\"background-color:#535353;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Embeddings & Categorify<\/p>\n\n<div class=\"alert alert-block alert-success\">  \n\n<p>    \n<li>When dealing with our <b>categorical data<\/b> ,after the missing-imputation step , <b> Categorify <\/b> encodes your categories into labels using <b> Label encodings <\/b><\/li><br>\n<li>We then create what is called an <b> embedding matrix <\/b> mapped to the encodings . This allows for a higher dimentionality for relationships between the different categorical cardinalities<\/li>\n<\/p>\n    \n<\/div>","d784e1f5":"<a id=\"NN\"><\/a> \n- `Learner architecture` ","f6315df9":"### Learning rate - Cyclic Trend","36c6365a":"# <p style=\"background-color:#535353;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Fillmissing<\/p>\n\n<div class=\"alert alert-block alert-success\">  \n\n<p>    \n<li><b>Fillmissing<\/b> allows us to handle features with <b>missing values<\/b><\/li><br>\n<li><b>Fillmissing<\/b> handles the missing values differently for <b>categorical features<\/b> and <b>continuos features<\/b> as discussed below<\/li>\n<\/p>\n    \n<\/div>\n\n- **Categorical Data** \n\n    - `#na#` is being used for imputing missing values (see `Lifeboat`)\n    \n    ------------------\n- **Continuos Data**    \n    - Fastai uses `Median` as default value for imputing the missing continous values\n    - You can change the `Median` to `Mean` or `fixed_value`\n    - `_na` features are created corresponding to each feature with missing values\n\n    -  <u> code snippet to change imputation strategy <\/u>\n\n    ```\n    fm = FillMissing(fill_strategy=FillStrategy.median)\n    to = TabularPandas(df, fm, cont_names=cont_names)\n    ```","3440f755":"# <p style=\"background-color:#535353;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Importing Libraries<\/p>","621800e0":"- **`Filtering features needed for the final model`**","85110e9b":"# <p style=\"background-color:#535353;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Thanks and Feedback<\/p>","93dc2879":"### Emebddings\n\n**Why we need embeddings - we aleardy have encodings!**\n\n- `Ques`: Wait, why we need embeddings now, our categorical data is handled. Isn't it ?\n- `Answer`: Not really. `Label encoding` the `nominal-data` can be a problem beacause you are `unnecessarily adding order` to the categories which was never there\n\n-------------------\n\n- `Ques` : Why Embeddings , why not 1-hot encodings ?\n- `Ans` : Primarily because Embeddings allow a `better representation (unlike 1-hot encoding)` and are `less sparse` so handling is not an issue\n\n------------------\n\n\n\n<div class=\"alert alert-block alert-success\">  \n\n<p>\n\n<b>Embedding size calculation logic<\/b><br>\nThe rule is to use either a maximum embedding space of 600, or 1.6 times the cardinality raised to the 0.56 , or written out as:<br>\n<li><b> min(600, (1.6 * {var.nunique)}^{0.56})<\/b><\/li>\n<\/p>\n    \n<\/div>    \n    \n- Let's calculate these embedding sizes for our model to take a look-see:","8c8f011e":"<a id=\"fastai_concept\"><\/a> \n\n# <p style=\"background-color:#535353;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">Why LR-max<\/p>\n\n- As you might have noticed while training the model we called **`fit_one_cycle` & NOT `fit`**\n\n\n<div class=\"alert alert-block alert-success\">  \n\n<p>    \n<li><b>Fit_one_cycle<\/b> is an appraoch which uses <b>dynamic learning-rate<\/b> while trainig the model<\/li><br>\n<li>It uses <b>Cyclic learning rate<\/b> with <b>max_lr (0.05)<\/b> which we specify as shown below<\/li> \n<\/p>\n    \n<\/div>\n\n----------------------\n\nI can talk more about `why lr_max` ,`momentum`,`cyclic-lr` or some other `cyclic lr strategy` \n\nMaybe on a different kernel OR refer to below link!\n\n> *read more about `fit_one_cycle` if you don't know :*\nhttps:\/\/sgugger.github.io\/the-1cycle-policy.html\n\n> *Learn about `Cyclic learning rates`* \nhttps:\/\/www.kaggle.com\/isbhargav\/guide-to-pytorch-learning-rate-scheduling","7090138c":"**`Model performance on validation data`**"}}