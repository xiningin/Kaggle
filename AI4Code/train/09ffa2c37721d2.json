{"cell_type":{"392efd2a":"code","71ff8a90":"code","3a42c3ac":"code","9bbe739e":"code","28c68d8e":"code","7a343b59":"code","d043a097":"code","3306577a":"code","9ac53a34":"code","90b2f406":"code","3254fcb5":"code","81c88748":"code","a998daf9":"code","0ee6e257":"code","22bc8a90":"code","bef4cd5b":"code","879b997c":"code","64ee6d65":"code","ba962005":"code","229386a6":"code","56d3a345":"code","6e35f0a4":"code","51cdd78e":"code","9ce081e5":"code","07721f91":"code","f05eddfc":"code","a69bfce3":"code","17844ca6":"code","029d3174":"code","8277446e":"code","5c91faf3":"code","804e5b99":"code","b052ab19":"code","61eb844d":"code","bb2ce8b4":"code","305229fa":"code","a620a140":"code","645a47c8":"code","bbc8de2d":"code","2f04a684":"code","986c6b5a":"code","a193e9c2":"code","538bf13c":"code","3237c14f":"code","c6d16c08":"code","fa983ac4":"code","afcd2197":"code","ddfcd25e":"code","b785b0e3":"code","52a0a2a5":"markdown","e1e2ed8a":"markdown","497c476e":"markdown","3383951e":"markdown","c654cc4d":"markdown","4489cdd8":"markdown","22620f9c":"markdown","262387fd":"markdown","8d93bb70":"markdown"},"source":{"392efd2a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","71ff8a90":"from pandas.io.parsers import read_csv\nfrom sklearn.utils import shuffle\n\nFTRAIN = '..\/input\/training\/training.csv'\nFTEST = '..\/input\/test\/test.csv'\n\n\ndef load(test=False, cols=None):\n    \"\"\"Loads data from FTEST if *test* is True, otherwise from FTRAIN.\n    Pass a list of *cols* if you're only interested in a subset of the\n    target columns.\n    \"\"\"\n    fname = FTEST if test else FTRAIN\n    df = read_csv(os.path.expanduser(fname))  # load pandas dataframe\n\n    # The Image column has pixel values separated by space; convert\n    # the values to numpy arrays:\n    df['Image'] = df['Image'].apply(lambda im: np.fromstring(im, sep=' '))\n\n    if cols:  # get a subset of columns\n        df = df[list(cols) + ['Image']]\n\n    print(df.count())  # prints the number of values for each column\n    df = df.dropna()  # drop all rows that have missing values in them\n\n    X = np.vstack(df['Image'].values) \/ 255.  # scale pixel values to [0, 1]\n    X = X.astype(np.float32)\n\n    if not test:  # only FTRAIN has any target columns\n        y = df[df.columns[:-1]].values\n        y = (y - 48) \/ 48  # scale target coordinates to [-1, 1]\n        X, y = shuffle(X, y, random_state=42)  # shuffle train data\n        y = y.astype(np.float32)\n    else:\n        y = None\n\n    return X, y\n\n\nX, y = load()\nprint(\"X.shape == {}; X.min == {:.3f}; X.max == {:.3f}\".format(\n    X.shape, X.min(), X.max()))\nprint(\"y.shape == {}; y.min == {:.3f}; y.max == {:.3f}\".format(\n    y.shape, y.min(), y.max()))","3a42c3ac":"#  I changed the X dimension structure to have (Nsample, Nrows in frame, N columns in frame, 1) in load2d.\ndef load2d(test=False,cols=None):\n\n    re = load(test, cols)\n    \n    X = re[0].reshape(-1,96,96,1)\n    y = re[1]\n\n    return X, y","9bbe739e":"%%time\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras.optimizers import SGD\nfrom keras.layers import Dropout\n\nmodel = Sequential()\nmodel.add(Dense(128,input_dim=X.shape[1]))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(64))\nmodel.add(Activation('relu'))\nmodel.add(Dense(30))\n\nmodel.summary()","28c68d8e":"sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\nmodel.compile(loss='mean_squared_error', optimizer=sgd)\n\nhist = model.fit(X, y, nb_epoch=50,batch_size=128, validation_split=0.2,verbose=False)","7a343b59":"\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ndef plot_loss(hist,name,plt,RMSE_TF=False):\n    '''\n    RMSE_TF: if True, then RMSE is plotted with original scale \n    '''\n    loss = hist['loss']\n    val_loss = hist['val_loss']\n    if RMSE_TF:\n        loss = np.sqrt(np.array(loss))*48 \n        val_loss = np.sqrt(np.array(val_loss))*48 \n        \n    plt.plot(loss,\"--\",linewidth=3,label=\"train:\"+name)\n    plt.plot(val_loss,linewidth=3,label=\"val:\"+name)\n\nplot_loss(hist.history,\"model 1\",plt)\nplt.legend()\nplt.grid()\nplt.yscale(\"log\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"log loss\")\nplt.show()","d043a097":"X_test, _ = load(test=True)\ny_test = model.predict(X_test)","3306577a":"#converting the images back to 96*96 pixels so i can check the performance of my model on the image dataset\n\ndef plot_sample(x, y, axis):\n    img = x.reshape(96, 96)\n    axis.imshow(img, cmap='gray')\n    axis.scatter(y[0::2] * 48 + 48, y[1::2] * 48 + 48, marker='x', s=10)\n\n\nfig = plt.figure(figsize=(10, 7))\nfig.subplots_adjust(\n    left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n\nfor i in range(16):\n    axis = fig.add_subplot(4, 4, i+1, xticks=[], yticks=[])\n    plot_sample(X_test[i], y_test[i], axis)\n\nplt.show()","9ac53a34":"from keras.models import load_model\n# import h5py\nmodel.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n# del model  # deletes the existing model\n\n# # returns a compiled model\n# # identical to the previous one\nmodel = load_model('my_model.h5')","90b2f406":"X.shape","3254fcb5":"# #Deleting these as i will use a new structure\n\n# del X, y, X_test, y_test","81c88748":"X,y = load2d()","a998daf9":"print(X.shape)\nprint(y.shape)","0ee6e257":"from keras.layers import MaxPooling2D, Conv2D , Flatten, Dropout\nfrom keras.layers.normalization import BatchNormalization","22bc8a90":"def CNN():\n    model2 = Sequential()\n\n    model2.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(96,96,1)))\n    model2.add(Dropout(0.1))\n    model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode=\"valid\"))\n    model2.add(BatchNormalization())\n\n    model2.add(Conv2D(32, 5, 5,activation=\"relu\"))\n    # model.add(Activation(\"relu\"))\n    model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode=\"valid\"))\n    model2.add(Dropout(0.2))\n    model2.add(BatchNormalization())\n\n    model2.add(Conv2D(64, 5, 5,activation=\"relu\"))\n    # model.add(Activation(\"relu\"))\n    model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode=\"valid\"))\n    model2.add(BatchNormalization())\n\n    model2.add(Conv2D(128, 3, 3,activation=\"relu\"))\n    # model.add(Activation(\"relu\"))\n    model2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), border_mode=\"valid\"))\n    model2.add(Dropout(0.4))\n    model2.add(BatchNormalization())\n\n    model2.add(Flatten())\n\n    model2.add(Dense(500, activation=\"relu\"))\n    model2.add(Dropout(0.1))\n\n    model2.add(Dense(128, activation=\"relu\"))\n    model2.add(Dropout(0.1))\n\n    model2.add(Dense(30))\n\n\n    model2.summary()\n    model2.compile(optimizer='adam', \n              loss='mse',\n              metrics=['mae','accuracy'])\n    return(model2)","bef4cd5b":"model2 = CNN()\nhist2 = model2.fit(X, y, nb_epoch=500,batch_size=128, validation_split=0.2,verbose=False)","879b997c":"# print(hist2.history)","64ee6d65":"# hist2.history","ba962005":"# Comparing model1 and model2\nplt.figure(figsize=(4,4))\nplot_loss(hist.history,\"model 1\",plt)\nplt.legend()\nplt.grid()\nplt.yscale(\"log\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.show()\n\nplot_loss(hist2.history,\"model 2\",plt)\nplt.legend()\nplt.grid()\nplt.yscale(\"log\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.show()","229386a6":"sample1,_ = load(test=True)\nsample2,_ = load2d(test=True)\ny_pred1 = model.predict(sample1)\ny_pred2 = model2.predict(sample2)","56d3a345":"#Comparing model1 and model2 on images\n\nfig = plt.figure(figsize=(10,10))\nfig.subplots_adjust(hspace=0.001,wspace=0.001,\n                    left=0,right=1,bottom=0, top=1)\nNpicture = 5\ncount = 1\nfor irow in range(Npicture):\n    ipic = np.random.choice(sample2.shape[0])\n    ax = fig.add_subplot(Npicture, 2, count,xticks=[],yticks=[])        \n    plot_sample(sample1[ipic],y_pred1[ipic],ax)\n    if count < 3:\n        ax.set_title(\"model 1\")\n        \n    count += 1\n    ax = fig.add_subplot(Npicture, 2, count,xticks=[],yticks=[])  \n    plot_sample(sample2[ipic],y_pred2[ipic],ax)\n    if count < 3:\n        ax.set_title(\"model 2\")\n    count += 1\nplt.show()","6e35f0a4":"model2.save('my_model2.h5')  # creates a HDF5 file 'my_model.h5'\n# del model  # deletes the existing model\n\n# # returns a compiled model\n# # identical to the previous one\nmodel2 = load_model('my_model2.h5')","51cdd78e":"# from keras.preprocessing.image.ImageDataGenerator(featurewise_center=False,\n#     samplewise_center=False,\n#     featurewise_std_normalization=False,\n#     samplewise_std_normalization=False,\n#     zca_whitening=False,\n#     rotation_range=0.,\n#     width_shift_range=0.,\n#     height_shift_range=0.,\n#     shear_range=0.,\n#     zoom_range=0.,\n#     channel_shift_range=0.,\n#     fill_mode='nearest',\n#     cval=0.,\n#     horizontal_flip=False,\n#     vertical_flip=False,\n#     dim_ordering='th')","9ce081e5":"## Using ImageDataGenerator to flip the images and flip indices will be used to manually flipping\n## the keypoints on the face\n\nfrom keras.preprocessing.image import ImageDataGenerator\nclass FlippedImageDataGenerator(ImageDataGenerator):\n    flip_indices = [\n        (0, 2), (1, 3),\n        (4, 8), (5, 9), (6, 10), (7, 11),\n        (12, 16), (13, 17), (14, 18), (15, 19),\n        (22, 24), (23, 25),\n        ]\n\n    def next(self):\n        X_batch, y_batch = super(FlippedImageDataGenerator, self).next()\n        batch_size = X_batch.shape[0]\n        indices = np.random.choice(batch_size, batch_size\/2, replace=False)\n        X_batch[indices] = X_batch[indices, :, :, ::-1]\n\n        if y_batch is not None:\n            \n            y_batch[indices, ::2] = y_batch[indices, ::2] * -1\n\n            # left_eye_center_x -> right_eye_center_x \u306e\u3088\u3046\u306b\u30d5\u30ea\u30c3\u30d7\n            for a, b in self.flip_indices:\n                y_batch[indices, a], y_batch[indices, b] = (\n                    y_batch[indices, b], y_batch[indices, a]\n                )\n\n        return X_batch, y_batch","07721f91":"## splitting the data\nfrom sklearn.model_selection import train_test_split\n\nX, y = load2d()\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","f05eddfc":"model3 = CNN()\nflipgen = FlippedImageDataGenerator()\nhist3 = model3.fit_generator(flipgen.flow(X_train, y_train),\n                             samples_per_epoch=X_train.shape[0],\n                             nb_epoch=300,\n                             validation_data=(X_val, y_val))\n\n","a69bfce3":"## Comparing mode1, model2 and model3 using pyplot\nplt.figure(figsize=(8,8))\nplot_loss(hist.history,\"model 1\",plt)\nplt.legend()\nplt.grid()\nplt.yscale(\"log\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.show()\n\nplot_loss(hist2.history,\"model 2\",plt)\nplt.legend()\nplt.grid()\nplt.yscale(\"log\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.show()\n\nplot_loss(hist3.history,\"model 3\",plt)\nplt.legend()\nplt.grid()\nplt.yscale(\"log\")\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.show()","17844ca6":"X_train.shape","029d3174":"model3.save('my_model3.h5')  # creates a HDF5 file 'my_model.h5'\n# del model  # deletes the existing model\n\n# # returns a compiled model\n# # identical to the previous one\nmodel3 = load_model('my_model3.h5')","8277446e":"SPECIALIST_SETTINGS = [\n    dict(\n        columns=(\n            'left_eye_center_x', 'left_eye_center_y',\n            'right_eye_center_x', 'right_eye_center_y',\n            ),\n        flip_indices=((0, 2), (1, 3)),\n        ),\n\n    dict(\n        columns=(\n            'nose_tip_x', 'nose_tip_y',\n            ),\n        flip_indices=(),\n        ),\n\n    dict(\n        columns=(\n            'mouth_left_corner_x', 'mouth_left_corner_y',\n            'mouth_right_corner_x', 'mouth_right_corner_y',\n            'mouth_center_top_lip_x', 'mouth_center_top_lip_y',\n            ),\n        flip_indices=((0, 2), (1, 3)),\n        ),\n\n    dict(\n        columns=(\n            'mouth_center_bottom_lip_x',\n            'mouth_center_bottom_lip_y',\n            ),\n        flip_indices=(),\n        ),\n\n    dict(\n        columns=(\n            'left_eye_inner_corner_x', 'left_eye_inner_corner_y',\n            'right_eye_inner_corner_x', 'right_eye_inner_corner_y',\n            'left_eye_outer_corner_x', 'left_eye_outer_corner_y',\n            'right_eye_outer_corner_x', 'right_eye_outer_corner_y',\n            ),\n        flip_indices=((0, 2), (1, 3), (4, 6), (5, 7)),\n        ),\n\n    dict(\n        columns=(\n            'left_eyebrow_inner_end_x', 'left_eyebrow_inner_end_y',\n            'right_eyebrow_inner_end_x', 'right_eyebrow_inner_end_y',\n            'left_eyebrow_outer_end_x', 'left_eyebrow_outer_end_y',\n            'right_eyebrow_outer_end_x', 'right_eyebrow_outer_end_y',\n            ),\n        flip_indices=((0, 2), (1, 3), (4, 6), (5, 7)),\n        ),\n    ]\n","5c91faf3":"from collections import OrderedDict\n\ndef fit_specialists(freeze=True,\n                    print_every=10,\n                    epochs=100,\n                    prop=0.1,\n                    name_transfer_model=\"my_model3.h5\"):\n    specialists = OrderedDict()\n \n\n    for setting in SPECIALIST_SETTINGS:\n        \n        cols = setting['columns']\n        flip_indices = setting['flip_indices']\n        \n        X, y = load2d(cols=cols)\n        X_train, X_val, y_train, y_val = train_test_split(X, y, \n                                                          test_size=0.2, \n                                                          random_state=42)\n        model4 = load_model(name_transfer_model) \n        if freeze:\n            for layer in model.layers:\n                layer.trainable = False\n            \n        model4.layers.pop() # get rid of output layer\n        model4.outputs = [model4.layers[-1].output]\n        model4.layers[-1].outbound_nodes = []\n        model4.add(Dense(len(cols))) # add new output layer\n\n        model4.compile(loss='mean_squared_error', optimizer=\"adam\")\n        \n        flipgen = FlippedImageDataGenerator()\n        flipgen.flip_indices = setting['flip_indices']\n        print(X_train.shape)\n        print(y_train.shape)\n        print(X_val.shape)\n        print(y_val.shape)\n        hist_final = model4.fit_generator(flipgen.flow(X_train, y_train),\n                                     samples_per_epoch=X_train.shape[0],\n                                     nb_epoch=epochs,\n                                     validation_data=(X_val, y_val))\n        \n        ## print(model.summary()) \n        \n       \n        specialists[cols] = model4\n    return(specialists)\n","804e5b99":"%%time\nspecialists1 = fit_specialists(freeze=True,\n                    print_every=10,\n                    epochs=100,\n                    name_transfer_model=\"my_model3.h5\")","b052ab19":"X_train.shape","61eb844d":"type(specialists1)","bb2ce8b4":"from pandas import DataFrame, concat\n\nX_test,_ = load2d(test=True)\n\n## prediction with model 3\ny_pred3 = model3.predict(X_test)\nlandmark_nm = read_csv(os.path.expanduser(FTRAIN)).columns[:-1].values\ndf_y_pred3 = DataFrame(y_pred3,columns = landmark_nm)\n\n## prediction with specialist model\ndef predict_specialist(specialists1,X_test):\n    y_pred_s = []\n    for columns, value in specialists1.items():\n        smodel = value\n\n        y_pred = smodel.predict(X_test)\n        y_pred = DataFrame(y_pred,columns=columns)\n        y_pred_s.append(y_pred)\n\n    df_y_pred_s = concat(y_pred_s,axis=1)\n    return(df_y_pred_s)\ndf_y_pred_s = predict_specialist(specialists1,X_test)\ny_pred_s = df_y_pred_s.values","305229fa":"FIdLookup = '..\/input\/IdLookupTable.csv'\n\nIdLookup = read_csv(os.path.expanduser(FIdLookup))\n\ndef prepare_submission(y_pred4,filename):\n    '''\n    save a .csv file that can be submitted to kaggle\n    '''\n    ImageId = IdLookup[\"ImageId\"]\n    FeatureName = IdLookup[\"FeatureName\"]\n    RowId = IdLookup[\"RowId\"]\n    \n    submit = []\n    for rowId,irow,landmark in zip(RowId,ImageId,FeatureName):\n        submit.append([rowId,y_pred4[landmark].iloc[irow-1]])\n    \n    submit = DataFrame(submit,columns=[\"RowId\",\"Location\"])\n    ## adjust the scale \n    submit[\"Location\"] = submit[\"Location\"]*48 + 48\n    print(submit.shape)\n#     loc = \"result\/\" + filename + \".csv\"\n    if filename == \"model3\":\n       submit.to_csv(\"model3.csv\",index=False) \n    else:\n        submit.to_csv(\"special.csv\",index=False)\n    \n#     print(\"File is saved at:\" +  loc)\n\nprepare_submission(df_y_pred_s,\"special\")    \nprepare_submission(df_y_pred3,\"model3\")\n\n","a620a140":"# sample3,_ = load2d(test=True)\n# sample_special,_ = load2d(test=True)\n\n# y_pred3 = model3.predict(X_test)\n\n# def predict_specialist_case(specialists1,X_test):\n#     for columns, value in specialists1.items():\n#         smodel = value\n\n#         y_pred_sample = smodel.predict(X_test)\n#         return y_pred_sample\n\n# y_pred_sample = predict_specialist_case(specialists1,X_test)\n\n\n\n","645a47c8":"# fig = plt.figure(figsize=(12, 20))\n# fig.subplots_adjust(hspace=0.001,wspace=0.001,\n#                     left=0,right=1,bottom=0, top=1)\n# Npicture = 7\n# count = 1\n# for irow in range(Npicture):\n#     ipic = np.random.choice(X_test.shape[0])\n#     ax = fig.add_subplot(Npicture, 2, count,xticks=[],yticks=[])        \n#     plot_sample(X_test[ipic],y_pred3[ipic],ax)\n#     if count < 3:\n#         ax.set_title(\"model 3\")\n        \n#     count += 1\n#     ax = fig.add_subplot(Npicture, 2, count,xticks=[],yticks=[])  \n#     plot_sample(X_test[ipic],y_pred_sample[ipic],ax)\n#     if count < 3:\n#         ax.set_title(\"special model\")\n#     count += 1\n# plt.show()","bbc8de2d":"df_y_pred_s = df_y_pred_s[df_y_pred3.columns]\ndf_compare = {}\ndf_compare[\"difference\"] = ((df_y_pred_s - df_y_pred3)**2).mean(axis=1)\ndf_compare[\"RowId\"] = range(df_y_pred_s.shape[0])\ndf_compare = DataFrame(df_compare)\ndf_compare = df_compare.sort_values(\"difference\",ascending=False)","2f04a684":"fig = plt.figure(figsize=(12,35))\n\nNsample = 13\npic_index = df_compare[\"RowId\"].iloc[:Nsample].values\npic_index_good = df_compare[\"RowId\"].iloc[-Nsample:].values\ncount = 1\n\n\nfor ipic_g,ipic in zip(pic_index_good,pic_index):\n    ## good model 3\n    ax = fig.add_subplot(Nsample,4,count,xticks=[],yticks=[])\n    count += 1\n    plot_sample(X_test[ipic_g],y_pred3[ipic_g],ax)\n    ax.set_title(\"Good:model3:pic\"+str(ipic_g))\n    \n    ## good special\n    ax = fig.add_subplot(Nsample,4,count,xticks=[],yticks=[])\n    count += 1\n    plot_sample(X_test[ipic_g],y_pred_s[ipic_g],ax)\n    ax.set_title(\"Good:special:pic\"+str(ipic_g))\n    \n    ## bad model 3\n    ax = fig.add_subplot(Nsample,4,count,xticks=[],yticks=[])\n    count += 1\n    plot_sample(X_test[ipic],y_pred3[ipic],ax)\n    ax.set_title(\"Bad:model3:pic\"+str(ipic))\n    \n    ## bad special\n    ax = fig.add_subplot(Nsample,4,count,xticks=[],yticks=[])\n    count += 1\n    plot_sample(X_test[ipic],y_pred_s[ipic],ax)\n    ax.set_title(\"Bad:special:pic\"+str(ipic))\n\nplt.show()","986c6b5a":"def fit_specialists(freeze=True,\n                    print_every=10,\n                    epochs=100,\n                    prop=0.1,\n                    name_transfer_model=\"my_model2.h5\"):\n    specialists = OrderedDict()\n \n\n    for setting in SPECIALIST_SETTINGS:\n        \n        cols = setting['columns']\n        flip_indices = setting['flip_indices']\n        \n        X, y = load2d(cols=cols)\n        X_train, X_val, y_train, y_val = train_test_split(X, y, \n                                                          test_size=0.2, \n                                                          random_state=42)\n        model4 = load_model(name_transfer_model) \n        if freeze:\n            for layer in model.layers:\n                layer.trainable = False\n            \n        model4.layers.pop() # get rid of output layer\n        model4.outputs = [model4.layers[-1].output]\n        model4.layers[-1].outbound_nodes = []\n        model4.add(Dense(len(cols))) # add new output layer\n\n        model4.compile(loss='mean_squared_error', optimizer=\"adam\")\n        \n        flipgen = FlippedImageDataGenerator()\n        flipgen.flip_indices = setting['flip_indices']\n        \n        print(X_train.shape)\n        print(y_train.shape)\n        print(X_val.shape)\n        print(y_val.shape)\n        \n        hist_final = model4.fit_generator(flipgen.flow(X_train, y_train),\n                                     samples_per_epoch=X_train.shape[0],\n                                     nb_epoch=epochs,\n                                     validation_data=(X_val, y_val))\n        \n        \n       \n        specialists[cols] = model4\n    return(specialists)\n\n","a193e9c2":"%%time\nspecialists2 = fit_specialists(freeze=True,\n                    print_every=10,\n                    epochs=100,\n                    name_transfer_model=\"my_model2.h5\")","538bf13c":"X_test,_ = load2d(test=True)\n\ndef predict_specialist(specialists2,X_test):\n    y_pred_s = []\n    for columns, value in specialists2.items():\n        smodel = value\n\n        y_pred = smodel.predict(X_test)\n        y_pred = DataFrame(y_pred,columns=columns)\n        y_pred_s.append(y_pred)\n\n    df_y_pred_s = concat(y_pred_s,axis=1)\n    return(df_y_pred_s)\ndf_y_pred_s = predict_specialist(specialists2,X_test)\ny_pred_s = df_y_pred_s.values","3237c14f":"## prediction with model 2\ny_pred2 = model2.predict(X_test)\nlandmark_nm = read_csv(os.path.expanduser(FTRAIN)).columns[:-1].values\ndf_y_pred2 = DataFrame(y_pred2,columns = landmark_nm)","c6d16c08":"FIdLookup = '..\/input\/IdLookupTable.csv'\n\nIdLookup = read_csv(os.path.expanduser(FIdLookup))\n\ndef prepare_submission(y_pred2,filename):\n    '''\n    save a .csv file that can be submitted to kaggle\n    '''\n    ImageId = IdLookup[\"ImageId\"]\n    FeatureName = IdLookup[\"FeatureName\"]\n    RowId = IdLookup[\"RowId\"]\n    \n    submit = []\n    for rowId,irow,landmark in zip(RowId,ImageId,FeatureName):\n        submit.append([rowId,y_pred2[landmark].iloc[irow-1]])\n    \n    submit = DataFrame(submit,columns=[\"RowId\",\"Location\"])\n    ## adjust the scale \n    submit[\"Location\"] = submit[\"Location\"]*48 + 48\n    print(submit.shape)\n\n    if filename == \"model2\":\n        submit.to_csv(\"model2.csv\",index=False) \n    else:\n        submit.to_csv(\"special_model2.csv\",index=False)\n    \n\nprepare_submission(df_y_pred_s,\"special_model2\")    \nprepare_submission(df_y_pred2,\"model2\")","fa983ac4":"df_y_pred_s = df_y_pred_s[df_y_pred3.columns]\ndf_compare = {}\ndf_compare[\"difference\"] = ((df_y_pred_s - df_y_pred2)**2).mean(axis=1)\ndf_compare[\"RowId\"] = range(df_y_pred_s.shape[0])\ndf_compare = DataFrame(df_compare)\ndf_compare = df_compare.sort_values(\"difference\",ascending=False)","afcd2197":"fig = plt.figure(figsize=(12,35))\n\nNsample = 13\npic_index = df_compare[\"RowId\"].iloc[:Nsample].values\npic_index_good = df_compare[\"RowId\"].iloc[-Nsample:].values\ncount = 1\n\n\nfor ipic_g,ipic in zip(pic_index_good,pic_index):\n    ## good model 2\n    ax = fig.add_subplot(Nsample,4,count,xticks=[],yticks=[])\n    count += 1\n    plot_sample(X_test[ipic_g],y_pred2[ipic_g],ax)\n    ax.set_title(\"Good:model2:pic\"+str(ipic_g))\n    \n    ## good special\n    ax = fig.add_subplot(Nsample,4,count,xticks=[],yticks=[])\n    count += 1\n    plot_sample(X_test[ipic_g],y_pred_s[ipic_g],ax)\n    ax.set_title(\"Good:special:pic\"+str(ipic_g))\n    \n    ## bad model 2\n    ax = fig.add_subplot(Nsample,4,count,xticks=[],yticks=[])\n    count += 1\n    plot_sample(X_test[ipic],y_pred2[ipic],ax)\n    ax.set_title(\"Bad:model2:pic\"+str(ipic))\n    \n    ## bad special\n    ax = fig.add_subplot(Nsample,4,count,xticks=[],yticks=[])\n    count += 1\n    plot_sample(X_test[ipic],y_pred_s[ipic],ax)\n    ax.set_title(\"Bad:special:pic\"+str(ipic))\n\nplt.show()","ddfcd25e":"# def fit_specialists(freeze=True,\n#                     print_every=10,\n#                     epochs=100,\n#                     prop=0.1,\n#                     name_transfer_model=\"my_model.h5\"):\n#     specialists = OrderedDict()\n \n\n#     for setting in SPECIALIST_SETTINGS:\n        \n#         cols = setting['columns']\n#         flip_indices = setting['flip_indices']\n        \n#         X, y = load2d(cols=cols)\n# #         X.reshape(7049, 96, 96, 1)\n#         X_train, X_val, y_train, y_val = train_test_split(X, y, \n#                                                           test_size=0.2, \n#                                                           random_state=42)\n# #         X_val = np.expand_dims(X_val, axis=0)\n#         model4 = load_model(name_transfer_model) \n#         if freeze:\n#             for layer in model.layers:\n#                 layer.trainable = False\n            \n#         model4.layers.pop() # get rid of output layer\n#         model4.outputs = [model4.layers[-1].output]\n#         model4.layers[-1].outbound_nodes = []\n#         model4.add(Dense(len(cols))) # add new output layer\n\n#         model4.compile(loss='mean_squared_error', optimizer=\"adam\")\n        \n#         flipgen = FlippedImageDataGenerator()\n#         flipgen.flip_indices = setting['flip_indices']\n# #         print(y_val.shape)\n# # #         X_val.reshape(1407,9216)\n# # #         y_val.reshape()\n# #         print(X_train.shape)\n#         print(X_train.shape)\n#         print(y_train.shape)\n#         print(X_val.shape)\n#         print(y_val.shape)\n#         X_train.reshape(5626,9216)\n#         X_val.reshape(1407,9216)\n#         hist_final = model4.fit_generator(flipgen.flow(X_train, y_train),\n#                                      samples_per_epoch=X_train.shape[0],\n#                                      nb_epoch=epochs,\n#                                      validation_data=(X_val, y_val))\n    \n       \n#         specialists[cols] = model4\n#     return(specialists)\n","b785b0e3":"# %%time\n# specialists3 = fit_specialists(freeze=True,\n#                     print_every=10,\n#                     epochs=100,\n#                     name_transfer_model=\"my_model.h5\")","52a0a2a5":"## Training special model with model3","e1e2ed8a":"## Creating submission files for both the models","497c476e":"## Creating submission files for both the models","3383951e":"## Using Image Augmentation\n\n### Image Flipping\n\nUsing augmentation to flip the images but to flip the image i also have to flip the data points:\nLike :\nleft_eye_centre_x       --> right_eye_centre_x,\nleft_eye_center_y       --> right_eye_center_y,\nleft_eye_inner_corner_x --> right_eye_inner_corner_x\n\nBut nose_tip will remain same.\nSo i will flip the remaining data points","c654cc4d":"## Benchmark models","4489cdd8":"## Specialist Setting\n\n### I have divided my dataset into 6 different groups.\n### I will train my model on each of these 6 groups separately.\n### All 6 models contains the same CNN architecture but the final output layer is adjusted for different number of outputs: for example we have a model for left eye and right eye center landmark prediction. As there are x and y coordinates for both eye centers, we have 4 nodes in the output layer of this model.\n","22620f9c":"## Convolutional Neural Network","262387fd":"## A Fully connected model","8d93bb70":"## Training special model with model2"}}