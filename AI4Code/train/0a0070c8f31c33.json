{"cell_type":{"5549c17f":"code","658e1b01":"code","42cdf4e8":"code","15be6e66":"code","966a03b1":"code","72edf4c4":"code","ef90c261":"code","740ce184":"code","cd1841e4":"code","2b45fa96":"code","22c5443e":"code","e5410de4":"code","068f0cf6":"code","0401bb27":"code","df07134c":"code","2c9f931e":"code","2104512b":"code","5517be07":"code","2f0500de":"code","55f6e29b":"code","1f1e32e7":"code","17e5a506":"code","053d6992":"code","bffb85cf":"code","063c62da":"code","94de17ff":"code","e4b1e80f":"code","01b5f78e":"code","0f9f8f63":"code","e1b62f56":"code","60ac5977":"code","6a261070":"code","bb4b6e65":"markdown","df8dd2e5":"markdown","1c90663f":"markdown","612331db":"markdown","2a966166":"markdown","02003ea1":"markdown","ac8d3d67":"markdown","8addcb2a":"markdown","bf1b2c18":"markdown","0b578e89":"markdown","96946185":"markdown"},"source":{"5549c17f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","658e1b01":"# Check train.csv roughly\ndf = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf.head()","42cdf4e8":"# Check there is no unexpected \"y\" field\nprint('count of null value of Survived column : {}'.format(df[df.Survived.isnull()].size))\nprint('-----------------------')\nprint('counts of unique values')\nprint(df.Survived.value_counts())","15be6e66":"# Just check SibSp counts (Just satisfy my interest)\ndf.SibSp.value_counts()","966a03b1":"# Just check Parch counts (Just satisfy my interest)\ndf.Parch.value_counts()","72edf4c4":"# Prepare for one-hot encoding for categorical variable\none_hot_case1 = {\n    \"Pclass\":object,\n    \"Sex\":object,\n    \"Embarked\":object,\n    \"Cabin\":object,\n}","ef90c261":"# Load train.csv again with one_hot_case1 setting\ndf_case1 = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\",\n                dtype=one_hot_case1)","740ce184":"# Divide to y and x features\ny_train = df_case1 .loc[:,[\"Survived\"]]\nX_case1 = df_case1 .iloc[:,2:]\n# Drop unnecessary columns\nX_case1 = X_case1.drop([\"Name\",\"Ticket\"],axis=1)","cd1841e4":"# Check column including null\ndef null_check(df):\n    for col in df.columns:\n        if np.sum(df[col].isnull()) > 0:\n            print(col)","2b45fa96":"null_check(X_case1)","22c5443e":"# One-hot encoding with one_hot_case1 setting\nX_case1 = pd.get_dummies(X_case1,\n              dummy_na=True,\n              columns=one_hot_case1.keys())","e5410de4":"# Set average value against null\nfrom sklearn.impute import SimpleImputer\nimp_case1 = SimpleImputer()\nimp_case1.fit(X_case1)\nX_case1 = pd.DataFrame(imp_case1.transform(X_case1), columns=X_case1.columns)","068f0cf6":"null_check(X_case1)","0401bb27":"# Check number of columns. I think there are too many columns. That's why, I will do feature selection in next step.\nprint(X_case1.shape[1])","df07134c":"# Feature selection\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nselector = RFE(RandomForestClassifier(n_estimators=100, random_state=1),\n               n_features_to_select=20,\n               step=.05)\n\nselector.fit(X_case1,y_train)\n\nX_case1_selected = X_case1.loc[:, X_case1.columns[selector.support_]]\nprint('X_fin_case1 shape:(%i,%i)' % X_case1_selected.shape)\nX_case1_selected.head()","2c9f931e":"# Prepare for one-hot encoding for categorical variable\none_hot_case2 = {\n    \"Pclass\":object,\n    \"Sex\":object,\n}","2104512b":"# Load train.csv again with one_hot_case2 setting\ndf_case2 = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\",\n                dtype=one_hot_case2)","5517be07":"# Drop unnecessary columns\nX_case2 = df_case2.iloc[:,2:]\nX_case2 = X_case2.drop([\"Name\",\"Ticket\", \"Embarked\", \"Cabin\"],axis=1)","2f0500de":"# One-hot encoding with one_hot_case2 setting\nX_case2 = pd.get_dummies(X_case2,\n              dummy_na=True,\n              columns=one_hot_case2.keys())","55f6e29b":"# Set average value against null\nimp_case2 = SimpleImputer()\nimp_case2.fit(X_case2)\nX_case2 = pd.DataFrame(imp_case2.transform(X_case2), columns=X_case2.columns)","1f1e32e7":"null_check(X_case2)","17e5a506":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nimport warnings\nwarnings.filterwarnings('ignore')\n\npl = Pipeline([('scl',StandardScaler()),\n               ('pca',PCA(random_state=1)),\n               ('est',LogisticRegression(solver='liblinear', random_state=1))])\nparam_grid = {'pca__n_components':[None,5,7,10],\n             'est__C':[0.001,0.01,0.1,0.2,0.3,0.5,1.0,10.0,100.0],\n             'est__penalty':['l1', 'l2']}\n\ngs = GridSearchCV(estimator=pl,\n                  param_grid=param_grid,\n                  scoring='roc_auc',\n                  cv=3,\n                  return_train_score=False)\ngs.fit(X_case1, y_train)\nprint('case 1 best score : {}'.format(gs.best_score_))\nprint('case 1 best param : %s' % gs.best_params_)\ngs.fit(X_case1_selected, y_train)\nprint('case 1 selected best score : {}'.format(gs.best_score_))\nprint('case 1 selected best param : %s' % gs.best_params_)\ngs.fit(X_case2, y_train)\nprint('case 2 best score : {}'.format(gs.best_score_))\nprint('case 2 best param : %s' % gs.best_params_)","053d6992":"from sklearn.svm import SVC\n\npl = Pipeline([('scl',StandardScaler()),\n               ('pca',PCA(random_state=1)),\n               ('est',SVC(random_state=1))])\nparam_grid = {'est__gamma':[0.001,0.002,0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1,0.2,0.5,1.0,10.0],\n             'est__kernel':['linear','rbf'],\n             'pca__n_components':[None,5,7,10],}\n\ngs = GridSearchCV(estimator=pl,\n                  param_grid=param_grid,\n                  scoring='roc_auc',\n                  cv=3,\n                  return_train_score=False)\ngs.fit(X_case1, y_train)\nprint('case 1 best score : {}'.format(gs.best_score_))\nprint('case 1 best param : %s' % gs.best_params_)\ngs.fit(X_case1_selected, y_train)\nprint('case 1 selected best score : {}'.format(gs.best_score_))\nprint('case 1 selected best param : %s' % gs.best_params_)\ngs.fit(X_case2, y_train)\nprint('case 2 best score : {}'.format(gs.best_score_))\nprint('case 2 best param : %s' % gs.best_params_)","bffb85cf":"from sklearn.ensemble import RandomForestClassifier\n\npl = Pipeline([('scl',StandardScaler()),\n              # ('pca',PCA(random_state=1)),\n               ('est',RandomForestClassifier(random_state=1))])\nparam_grid = {'est__max_depth':[4,5,6,7,10],\n             'est__n_estimators':[1200,1300,1400]}\n            # 'pca__n_components':[None,5,7,10],}\n\ngs = GridSearchCV(estimator=pl,\n                  param_grid=param_grid,\n                  scoring='roc_auc',\n                  cv=3,\n                  return_train_score=False)\n#gs.fit(X_case1, y_train)\n#print('case 1 best score : {}'.format(gs.best_score_))\n#print('case 1 best param : %s' % gs.best_params_)\ngs.fit(X_case1_selected, y_train)\nprint('case 1 selected best score : {}'.format(gs.best_score_))\nprint('case 1 selected best param : %s' % gs.best_params_)\n#gs.fit(X_case2, y_train)\n#print('case 2 best score : {}'.format(gs.best_score_))\n#print('case 2 best param : %s' % gs.best_params_)","063c62da":"from sklearn.ensemble import GradientBoostingClassifier\n\npl = Pipeline([('scl',StandardScaler()),\n              # ('pca',PCA(random_state=1)),\n               ('est',GradientBoostingClassifier(random_state=1))])\nparam_grid = {'est__max_depth':[1,2,3,4,5],\n             'est__n_estimators':[50,60,70,80,90,100,200,250],\n             'est__learning_rate':[0.09,0.1]}\n            # 'pca__n_components':[None,10],}\n\ngs = GridSearchCV(estimator=pl,\n                  param_grid=param_grid,\n                  scoring='roc_auc',\n                  cv=3,\n                  return_train_score=False)\ngs.fit(X_case1, y_train)\nprint('case 1 best score : {}'.format(gs.best_score_))\nprint('case 1 best param : %s' % gs.best_params_)\ngs.fit(X_case1_selected, y_train)\nprint('case 1 selected best score : {}'.format(gs.best_score_))\nprint('case 1 selected best param : %s' % gs.best_params_)\ngs.fit(X_case2, y_train)\nprint('case 2 best score : {}'.format(gs.best_score_))\nprint('case 2 best param : %s' % gs.best_params_)","94de17ff":"# Load test.csv\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\",\n                     dtype=one_hot_case1)\nID = test_df.loc[:,[\"PassengerId\"]]\n# Remove unnecessary columns\nX_test = test_df.drop([\"Name\",\"Ticket\"],axis=1)","e4b1e80f":"# One-hot encoding with one_hot_case1 setting\nX_test = pd.get_dummies(X_test,\n                       dummy_na=True,\n                       columns=one_hot_case1.keys())","01b5f78e":"# Remove columns which doesn't exist in train data\nX_train_none_df = pd.DataFrame(None,\n                              columns=X_case1.columns,\n                              dtype=float)\nX_test_with_none_train = pd.concat([X_test, X_train_none_df])\nset_columns_x_train = set(X_case1.columns)\nset_columns_x_test = set(X_test.columns)\nX_test_arrange = X_test_with_none_train.drop(list(set_columns_x_test - set_columns_x_train),axis=1)","0f9f8f63":"# Set 0 in empty columns which doesn't exist in test data originally\nX_test_arrange.loc[:,list(set_columns_x_train - set_columns_x_test)] = X_test_arrange.loc[:,list(set_columns_x_train - set_columns_x_test)].fillna(0, axis=1)","e1b62f56":"# Set average value against null\nX_test_arrange = X_test_arrange.reindex(X_case1.columns,axis=1)\nX_test_arrange = pd.DataFrame(imp_case1.transform(X_test_arrange), columns=X_test_arrange.columns)","60ac5977":"# Feature selection by RFE using train data\nX_test_fin = X_test_arrange.loc[:, X_test_arrange.columns[selector.support_]]","6a261070":"# Predict\ngb = GradientBoostingClassifier( max_depth=2, n_estimators=90, random_state=1)\ngb.fit(X_case1_selected, y_train)\npredict_y = pd.DataFrame(gb.predict(X_test_fin), columns=[\"Survived\"])\nID.join(predict_y).to_csv('\/kaggle\/working\/matsukawa2_submission.csv', index=False)","bb4b6e65":"# Check data","df8dd2e5":"# Feature preprocessing\nI will consider of two cases.\nOne is including Embarked and Cabin, the other is excluding them.","1c90663f":"## Case2 Exclude Embarked and Cabin","612331db":"### 1st LogisticRegression","2a966166":"# Grid search\nIn this timing, I will try LogisticRegression, SVC, RandomForestClassifier, GradientBoostingClassifier","02003ea1":"# Conclusion\nAs a result\n* best model is GradientBoostingClassifier(est__max_depth=2, est__n_estimators=90)\n* best data is X_case1_selected. Include Embarked and Cabin and selected by RFE","ac8d3d67":"### 4th GradientBoosting","8addcb2a":"## Case 1 : Include Embarked and Cabin","bf1b2c18":"### 2nd SVM","0b578e89":"### 3rd RandomForester","96946185":"# Predict test data"}}