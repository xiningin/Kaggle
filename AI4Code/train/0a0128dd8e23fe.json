{"cell_type":{"23a2cb85":"code","946f694a":"code","f206fb5c":"code","3fc1ba45":"code","288fb9ed":"code","0667bbfd":"code","9a27015e":"code","855d7185":"code","d0951ff3":"code","4f174f6e":"code","0ff6c9ed":"code","000f9442":"code","9a822749":"code","fbef4b18":"code","e1b2ad54":"code","ec71f169":"code","1040568f":"code","ffe051f5":"code","fd6caf8e":"code","d7f1a0db":"code","88303f7f":"code","c301ed55":"code","4ab80f87":"code","9dca2174":"code","8aafdb76":"code","df78b72d":"code","b34f57ae":"code","7c3ff58e":"markdown","aace68f4":"markdown","5b9fce18":"markdown","352c7e8b":"markdown","53be8cdf":"markdown","dbbfd1bf":"markdown","5afb8002":"markdown","04aae829":"markdown","d2e5adb6":"markdown","314e0a9b":"markdown","c5af9a65":"markdown","f6bc2974":"markdown","9c3b66b6":"markdown","880bbcd1":"markdown","10cba23c":"markdown","9b11f547":"markdown","9942e172":"markdown","b00f124f":"markdown","c22bbd93":"markdown","f8fdf955":"markdown","6f202f8c":"markdown","43f238d7":"markdown","4ed8518d":"markdown"},"source":{"23a2cb85":"import numpy as np\nimport pandas as pd\nimport scipy.special\nimport matplotlib.pyplot as plt\nimport os\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.ensemble import IsolationForest\nfrom xgboost import XGBRegressor","946f694a":"path_in = '..\/input\/house-prices-advanced-regression-techniques\/'\nos.listdir(path_in)","f206fb5c":"train_data = pd.read_csv(path_in+'train.csv', index_col=0)\ntest_data = pd.read_csv(path_in+'test.csv', index_col=0)\nsamp_subm = pd.read_csv(path_in+'sample_submission.csv', index_col=0)","3fc1ba45":"print('number train samples: ', len(train_data.index))\nprint('number test samples: ', len(test_data.index))","288fb9ed":"train_data.head()","0667bbfd":"y_train = train_data['SalePrice'].copy().to_frame()\ndel train_data['SalePrice']\ny_train.head()","9a27015e":"data = pd.concat([train_data, test_data])","855d7185":"no_features = ['Alley', 'PoolQC', 'Fence', 'MiscFeature', 'FireplaceQu']\ndata = data[data.columns.difference(no_features)]","d0951ff3":"data","4f174f6e":"cols_with_missing_values = [col for col in data.columns if data[col].isnull().any()]","0ff6c9ed":"print('# of cols with missing values: ', len(cols_with_missing_values))\nprint('columns with missing data:', cols_with_missing_values)","000f9442":"data.dtypes.value_counts()","9a822749":"data[cols_with_missing_values].dtypes.value_counts()","fbef4b18":"for col in cols_with_missing_values:\n    data[col + '_was_missing'] = data[col].isnull()","e1b2ad54":"for col in cols_with_missing_values:\n    data[col] = data[col].fillna(data[col].value_counts().index[0])","ec71f169":"cols_for_one_hot_encdoing = []\ncols_for_scale = []\nfor col in data.columns:\n    if data[col].dtypes == 'object':\n        cols_for_one_hot_encdoing.append(col)\n    elif data[col].dtypes != 'bool':\n        cols_for_scale.append(col)","1040568f":"data = pd.get_dummies(data, columns = cols_for_one_hot_encdoing, prefix = cols_for_one_hot_encdoing)","ffe051f5":"# for col in cols_for_scale:\n#     data[col] = data[col].astype('float32')\n#     mean = data[col].mean(axis=0)\n#     data[col] -= data[col].mean(axis=0)\n#     std = data[col].std(axis=0)\n#     data[col] \/= data[col].std(axis=0)","fd6caf8e":"X_train = data.loc[train_data.index]\nX_test = data.loc[test_data.index]","d7f1a0db":"# iso = IsolationForest(contamination=0.2)\n# yhat = iso.fit_predict(X_train)\n# mask = yhat != -1\n# X_train, y_train = X_train[mask], y_train[mask]","88303f7f":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state=2020)","c301ed55":"model_XGB = XGBRegressor(objective='reg:squarederror', n_estimators=1000)\nmodel_XGB.fit(X_train, y_train)\ny_val_pred = model_XGB.predict(X_val)\nnp.sqrt(mean_squared_log_error(y_val, y_val_pred))","4ab80f87":"importance = model_XGB.feature_importances_","9dca2174":"fig = plt.figure(figsize=(10, 60))\nx = X_train.columns.values\nplt.barh(x, 100*importance)\nplt.title('Feature Importance', loc='left')\nplt.xlabel('Percentage')\nplt.grid()\nplt.show()","8aafdb76":"y_train_pred = model_XGB.predict(X_train)\ny_val_pred = model_XGB.predict(X_val)\n\nfig, axs = plt.subplots(1, 2, figsize=(22, 6))\nfig.subplots_adjust(hspace = .5, wspace=.5)\naxs = axs.ravel()\naxs[0].plot(y_train, y_train_pred, 'ro')\naxs[0].plot(y_train, y_train, 'blue')\naxs[1].plot(y_val, y_val_pred, 'ro')\naxs[1].plot(y_val, y_val, 'blue')\nfor i in range(2):\n    axs[i].grid()\n    axs[i].set_xlabel('true')\n    axs[i].set_ylabel('pred')\naxs[0].set_title('train')\naxs[1].set_title('val')\nplt.show()","df78b72d":"y_test = model_XGB.predict(X_test).astype(int)","b34f57ae":"output = pd.DataFrame({'Id': samp_subm.index,\n                       'SalePrice': y_test})\noutput.to_csv('submission.csv', index=False)","7c3ff58e":"For more options look here: https:\/\/www.kaggle.com\/drcapa\/titanic-eda-feature-enginneering-xgb <br>\nThis is also a starter code.","aace68f4":"# Split train and val data","5b9fce18":"# Define X_train and X_test","352c7e8b":"Show the number of different datatypes in the data set with missing values.","53be8cdf":"# Load data","dbbfd1bf":"# Input path","5afb8002":"# Concatenate train and test data\nWe recomment to concatenate the train and test data for a better preparation. We have to deal with missing values and encode features. So we don't want to do all operations twice. First we extract the target feature *SalePrice* and then we concatenate. After the preparation we split the data to X_train and X_test based on the *Id*.","04aae829":"# Feature Importance","d2e5adb6":"First we extract als features for the one-hot-encoding and write the names into a list. And we wirte the cols for scaling into another list.","314e0a9b":"# Define Model\nWe use a simple XGB Regressor.","c5af9a65":"The easiest way to hanlde missing values is to say take the most frequent values. So you can deal with both numerical and categorical features.","f6bc2974":"# Handle missing values (necessary)\nThere are a lot of missing values we have to deal with. First we need to know which feature has missing values and which datetype it is. ","9c3b66b6":"\n# Overview","880bbcd1":"# Scale data (advanced)","10cba23c":"## Outlier Dedection","9b11f547":"# Load Libraries","9942e172":"There are many opportunities to deal with missing values. Additionally we could create new features and label the rows with missing values. This we have to do before handling missing values. This information could be helpful later.","b00f124f":"# Intro \nWelcome to the [House Prices](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) competition\n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/5407\/media\/housesbanner.png)\nThis notebook is a starter code for all beginners and easy to understand. We will give an introduction to analysis and feature engineering.\n\nTherefore we focus on\n\n* a simple analysis of the data,\n* create new features,\n* encoding and\n* scale data. \n\nWe use categorical feature encoding techniques, compare\nhttps:\/\/www.kaggle.com\/drcapa\/categorical-feature-encoding-challenge-xgb\n\nThe data set includes a lot of features. The data description ist here: https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data\n\nWe label the **necessary** operations and the operations for **advanced** feature engeneering. So for the first run you can skip the advanced feature engeneering.\n\nFor prediction we use a simple XGB Regressor.\n\n<span style=\"color: royalblue;\">Please vote the notebook up if it helps you. Thank you. <\/span>","c22bbd93":"# Visualization Error","f8fdf955":"# Write output for submission","6f202f8c":"Show the number of different datatypes in the data set.","43f238d7":"# Predict Test Data","4ed8518d":"## Encoding data (necessary)\nThe easiest way to encode non numerical values is to use the one-hot-encoding. <br>\n\nFor a more goal-oriented encoding we recommend another competition: https:\/\/www.kaggle.com\/drcapa\/categorical-feature-encoding-challenge-xgb <br>\nThere are some classes of features we have to encode with different techniques: <br>\n1) categorical features, <br>\n2) binary features, <br>\n3) ordinal features. <br>"}}