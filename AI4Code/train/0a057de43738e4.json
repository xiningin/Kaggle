{"cell_type":{"72f5a867":"code","b3723c4f":"code","355aacc8":"code","f62b131b":"code","e2ad2a6c":"code","2f332816":"code","032d4dea":"code","60060220":"code","428243ac":"code","17744ea7":"code","dfd20782":"code","cde1e6d1":"code","e4cbafd4":"code","c577f3e5":"code","d2e9c088":"code","f67ad4e5":"code","8363570d":"code","91d508a6":"code","918c8d25":"code","8ba3a668":"code","69d2bf74":"code","0863385e":"code","214ed2a3":"code","09f41828":"code","2809d68d":"code","f7d2a1fe":"code","af86419b":"code","f7ff06f1":"code","027a9ab7":"code","cd9e2c77":"code","ccb2997c":"code","5f6fb155":"code","02f06c1b":"code","ae1d8de5":"code","facb2e78":"code","27bf29d5":"code","701d8c79":"code","e968eabd":"markdown","69d3804c":"markdown","e107d5c0":"markdown","9d876a8e":"markdown","493b696d":"markdown","d1a3f5c5":"markdown","217736b3":"markdown","f9e2a511":"markdown","cbd11eea":"markdown","268315aa":"markdown","5fad096d":"markdown","76bbd797":"markdown","9ff6a3b7":"markdown","10081b6c":"markdown","76e0d74c":"markdown","2c3c7c59":"markdown","5bd01093":"markdown"},"source":{"72f5a867":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix, classification_report\nsns.set_style('whitegrid')\nsns.set(font_scale = 1.2)\nfrom catboost import CatBoostClassifier\nfrom catboost import Pool, cv\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.metrics import accuracy_score\n","b3723c4f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","355aacc8":"df_train = pd.read_csv('\/kaggle\/input\/dry-beans-classification-iti-ai-pro-intake01\/train.csv')\ndf_train.describe()","f62b131b":"df_train.describe()","e2ad2a6c":"df_test=pd.read_csv('\/kaggle\/input\/dry-beans-classification-iti-ai-pro-intake01\/test.csv')\ndf_test","2f332816":"data_all = pd.concat((df_train,df_test),ignore_index=True)","032d4dea":"data_all","60060220":"data= data_all[(data_all['y']=='SIRA') | (data_all['y']=='DERMASON')]\ndata.drop('ID',axis=1,inplace=True)","428243ac":"# sns.pairplot(data=data,hue='y',kind='scatter',markers='o')","17744ea7":"sns.set_style('whitegrid')\nfor i in data.columns[:-2]:\n    sns.displot(x=i,data=data,kind='kde',hue='y',fill=True)\n    plt.show()","dfd20782":"# area of ellipse containing the bean\ndata_all['elipse_area']= np.pi* data_all['MajorAxisLength']*data_all['MinorAxisLength']\/4\n\n# perimeter of the ellipse containing the bean\ndata_all['perimeter']= 2*np.pi * np.sqrt(((0.5*data_all['MajorAxisLength'])**2+(0.5*data_all['MinorAxisLength'])**2)\/2)\n\n# difference between ellipse area and bean area\ndata_all['area_diff']= data_all['elipse_area']-data_all['Area']\n\n# difference between ellipse perimeter and bean perimeter\ndata_all['perimeter_diff']= data_all['perimeter']-data_all['Perimeter']\n\n\n\n\n\n\n","cde1e6d1":"# # Extent1\n\n# data_all['Extent1']= data_all['ConvexArea']\/(data_all['MajorAxisLength']*data_all['MinorAxisLength'])\n\n# #Extent2 \n\n# data_all['Extent2'] =data_all['Area']\/(data_all['MajorAxisLength']*data_all['MinorAxisLength'])\n\n\n# #Extent3\n\n# data_all['Extent3'] =data_all['Area']\/ data_all['elipse_area']\n\n\n\n\n# # shape factors\n\n# data_all['f1'] = data_all['ShapeFactor1']*data_all['ShapeFactor2']\n# data_all['f1'] = data_all['ShapeFactor1']*data_all['ShapeFactor3']\n\n\n# data_all['f2'] = data_all['ShapeFactor1']\/data_all['ShapeFactor2']\n# data_all['f1'] = data_all['ShapeFactor1']\/data_all['ShapeFactor3']\n\n\n\n# # equivalent diameter\n\n\n# data_all['Ed']= data_all['Perimeter']\/np.pi\n\n# data_all['com1']= data_all['Ed']\/data_all['MajorAxisLength']\n\n# data_all['com2']= data_all['Ed']\/data_all['MinorAxisLength']\n\n# data_all['a']= 0.25*np.pi* data_all['Ed']**2\n\n# data_all['Extent4'] =data_all['Area']\/ data_all['a']\n\n# data_all['Extent5']= data_all['a']\/(data_all['MajorAxisLength']*data_all['MinorAxisLength'])\n\n# data_all['Extent6']= data_all['Extent4']\/np.log(data_all['Extent5'])\n\n# data_all['com3']= data_all['EquivDiameter']\/data_all['MajorAxisLength']\n\n# data_all['com4']= data_all['EquivDiameter']\/data_all['MinorAxisLength']\n\n\n\n\n\n\n# # Curl\n\n# data_all['fibre_length']= (data_all['Perimeter']- np.sqrt((data_all['perimeter']**2)-16*data_all['Area']))\/4\n\n# data_all['fibre_width'] = data_all['Area']\/data_all['fibre_length']\n\n# # Extent4\n\n# rec_area= data_all['MajorAxisLength']*data_all['MinorAxisLength']\n\n\n# data_all['circle_area']= 0.25*np.pi*data_all['EquivDiameter']**2\n\n# data_all['Extent4'] = data_all['circle_area']\/ data_all['ConvexArea']\n\n# data_all['Extent5'] = data_all['circle_area']\/ rec_area\n\n# data_all['Extent6'] = data_all['Area']\/ data_all['circle_area']\n\n\n# # perimeter \n\n\n# data_all['Extent7']= data_all['perimeter_diff']\/data_all['perimeter']\n\n# data_all['Extent8']= data_all['perimeter_diff']\/data_all['Perimeter']\n\n\n\n\n# area diff\n\n\n# data_all['percent1']=  (rec_area- data_all['Area'])\/rec_area\n\n# data_all['percent2']=  (rec_area- data_all['ConvexArea'])\/rec_area\n\n# data_all['percent3']=  (data_all['elipse_area']- data_all['Area'])\/data_all['elipse_area']\n\n\n\n\n\n\n# data_all=Bin(data_all,'percent1')\n# data_all=Bin(data_all,'percent2')\n# data_all=Bin(data_all,'percent3')\n# data_all=Bin(data_all,'Extent1')\n# data_all=Bin(data_all,'Extent2')\n# data_all=Bin(data_all,'Extent3')\n\n\n\n\n# # Extent9\n# data_all['Extent9'] =data_all['elipse_area']\/(data_all['MajorAxisLength']*data_all['MinorAxisLength'])\n# # Extent10\n# data_all['Extent10'] =data_all['ConvexArea']\/ data_all['elipse_area']\n\n\n\n\n\n\n# def Bin1(df,col):\n#     q1=df[col].quantile(0.1)\n#     q2=df[col].quantile(0.2)\n#     q3=df[col].quantile(0.3)\n#     q4=df[col].quantile(0.4)\n#     q5=df[col].quantile(0.5)\n#     q6=df[col].quantile(0.6)\n#     q7=df[col].quantile(0.7)\n#     q8=df[col].quantile(0.8)\n#     q9=df[col].quantile(0.9)\n#     bins = [-np.inf,q1,q2,q3,q4,q5,q6,q7,q8,q9,np.inf]\n#     category = ['q1', 'q2','q3','q4','q5','q6','q7','q8','q9','q10']\n#     data_all[col+'_cat'] = pd.cut(data_all[col], bins, labels=category).astype(str)\n#     return data_all\n\n# data_all=Bin1(data_all,'ShapeFactor1')\n# data_all=Bin1(data_all,'ShapeFactor2')\n# data_all=Bin1(data_all,'ShapeFactor3')\n# data_all=Bin1(data_all,'ShapeFactor4')\n# data_all=Bin1(data_all,'Compactness')\n# data_all=Bin1(data_all,'roundness')\n# data_all=Bin1(data_all,'Solidity')\n# data_all=Bin1(data_all,'Eccentricity')\n# data_all=Bin1(data_all,'AspectRation')\n# data_all=Bin1(data_all,'EquivDiameter')\n\n","e4cbafd4":"data_all.drop(['ID'],axis=1,inplace=True)","c577f3e5":"train_data= data_all[data_all['y'].notna()]\ntest_data= data_all[data_all['y'].isna()]","d2e9c088":"y= train_data['y'].copy()\ntrain_data.drop(['y'],axis=1,inplace=True)\ntrain_data['y']=y","f67ad4e5":"test_data.drop(['y'],axis=1,inplace=True)","8363570d":"train_data.head()","91d508a6":"test_data.head()","918c8d25":"train_data = train_data.sample(frac=1,random_state=444).reset_index(drop=True)","8ba3a668":"train_count= int(1*train_data.shape[0])","69d2bf74":"X_train = train_data.iloc[:,:-1]\ny_train = train_data.iloc[:,-1]\n","0863385e":"X_train.shape","214ed2a3":"y_train.shape","09f41828":"# cbc = CatBoostClassifier(  \n                            \n#                          loss_function= 'MultiClass',\n\n#                          eval_metric='Accuracy',\n                              \n#                           verbose=False,\n                          \n#                           task_type='CPU',\n    \n#                             random_state=42,    \n    \n#                         )\n\n\n\n\n\n\n# params = { 'learning_rate':[0.0301,0.03001,0.030001], \n#          'border_count':[10,100,200],\n#         'depth': [2,4],\n#         'l2_leaf_reg': [10,20],\n#          'bagging_temperature':[0.3,0.9],\n#          'iterations':[1000,2000,2225,3000],\n#          'random_stength':[10,100,1000]}\n\n\n\n\n# Grid_CBC = GridSearchCV(estimator=cbc, param_grid = params, cv = 3,refit=True, n_jobs=-1)\n\n# Grid_CBC.fit(X_train, y_train)","2809d68d":"# print(\" Results from Grid Search \" )\n# print(\"\\n The best estimator across ALL searched params:\\n\",Grid_CBC.best_estimator_)\n# print(\"\\n The best score across ALL searched params:\\n\",Grid_CBC.best_score_)\n# print(\"\\n The best parameters across ALL searched params:\\n\",Grid_CBC.best_params_)","f7d2a1fe":"model = CatBoostClassifier(\n    \n                            border_count=100,\n    \n                            iterations=2225,\n    \n                            learning_rate=0.03001,\n\n                              depth=4 ,\n\n                           l2_leaf_reg=10,\n    \n    \n    \n                         loss_function= 'MultiClass',\n\n                         eval_metric='Accuracy',\n                              \n                          verbose=False,\n                          \n                          task_type='CPU',\n    \n                            random_state=42,\n        \n                             bagging_temperature=0.9,\n\n                            random_strength= 10,\n    \n        \n)\n\n\nmodel.fit(X_train,y_train,\n          \n        \n          )","af86419b":"fig, ax = plt.subplots(figsize=(10, 10))\nplot_confusion_matrix(model, X_train, y_train,ax=ax,cmap='Blues')  \nplt.xticks(rotation=30)\nplt.grid()\nplt.title('Catboost Confusion Matrix')\nplt.show()  ","f7ff06f1":"data_all['y'].value_counts()","027a9ab7":"train_preds = model.predict(X_train)\nreport_cb = classification_report(y_train, train_preds, output_dict=True)\nreport = pd.DataFrame(report_cb).transpose()\nreport","cd9e2c77":"feature_importance= model.get_feature_importance()*100\/sum(model.get_feature_importance())\nplt.figure(figsize=(16,8))\ngraph=sns.barplot(x=feature_importance,y=X_train.columns)\nplt.legend(X_train.columns,feature_importance)\n# plt.xticks(rotation=90)\nplt.show()","ccb2997c":"model.plot_tree(tree_idx=0)","5f6fb155":"predictions= model.predict(test_data)","02f06c1b":"predictions.shape","ae1d8de5":"predictions","facb2e78":"id=list(df_test['ID'])\npred= pd.DataFrame()\npred['ID']=id\npred['y']=predictions\npred","27bf29d5":"pred['y'].value_counts()","701d8c79":"pred.to_csv('subfinal.csv',index=False)","e968eabd":"### Train data shuffling ","69d3804c":"### Data Visualization","e107d5c0":"### Model with best parameters","9d876a8e":"#### demonstrating the overlap between sira and dermason","493b696d":"### Separating train data from test data","d1a3f5c5":"### Concatenation of train and test data to preprocess them in one shot","217736b3":"### Training the model with best parameters on the whole dataset","f9e2a511":"### Exporting csv submission file","cbd11eea":"### Feature Importance","268315aa":"### Tree Graph","5fad096d":"### Generating predictions","76bbd797":"### Loading train and test data","9ff6a3b7":"### Importing relevant libraries","10081b6c":"### Confusion matrix ","76e0d74c":"### Feature Engineering","2c3c7c59":"### Classification Report","5bd01093":"### Grid Search"}}