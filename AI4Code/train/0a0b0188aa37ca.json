{"cell_type":{"e6ba36ac":"code","fe426acb":"code","6365748d":"code","5bf8dcb0":"code","505deb06":"code","477aa841":"code","21563c05":"code","17975c95":"code","9e8c82d3":"code","43438f3e":"code","f749edeb":"markdown","b5e6d8d4":"markdown","1a386943":"markdown","ae9d0e52":"markdown","280d6491":"markdown","22d80c3a":"markdown","9861881e":"markdown","4df240cd":"markdown","1f7e0c52":"markdown","cc0280a9":"markdown","0e10f746":"markdown"},"source":{"e6ba36ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fe426acb":"from IPython.display import Image\nimport os","6365748d":"Image(\"..\/input\/20200105_233950.jpg\")","5bf8dcb0":"Image(\"..\/input\/20200105_234013.jpg\")","505deb06":"Image(\"..\/input\/20200105_234028.jpg\")","477aa841":"Image(\"..\/input\/20200105_234036.jpg\")","21563c05":"Image(\"..\/input\/20200105_234044.jpg\")","17975c95":"Image(\"..\/input\/20200105_234105.jpg\")","9e8c82d3":"Image(\"..\/input\/20200105_234122.jpg\")","43438f3e":"Image(\"..\/input\/20200105_234149.jpg\")","f749edeb":"- first component is further broken down into two components, the first one coming from w5 and second from w6\n- the first component coming from w5 is actually passed down from what we calculated before (that's why backpropagation is so fast), the only change is that instead of original w5 it's the w5 new that we previously calculated\n\n","b5e6d8d4":"- the update is very small because of random weight initialization\n- as you can see if the weight is not in the range of [0.3, 0.7] for example, then the slope (rate of change is close to 0) and when you pass through multiple layers the update becomes very small\n- thus other activation function of such are relu or leaky relu to mitigate this issue","1a386943":"- calculate and update w6, w7, w8\n- as an example we are going to update w6\n- the reason why we updated w6 is because we need continue to backpropagate to the weight of the input( first layer)\n- as an example we are going to update w1, which is a component of w5 and w6","ae9d0e52":"This is a fully connected neural network. I am demonstrating the forward pass, backpropagation and the weight update process\n\nmatrix multiplication\n\nH1=[x1, x2 ] [w1, w3]T\nH2=[x1, x2 ] [w2, w4]T\n\nx represents input data, it can be a tokenized words, stock price etc.\n\n- the weights are assigned in Keras, they are updated via backpropagation\n- used qudratic cost funtion for simplicity\n- bias = 0 for simiplicty\n- there are two ways of assigning weights in Keras in a dense layer. 'Random_uniform' or 'zeros'\n\nmodel.add(Dense(64, kernel_initializer='random_uniform', bias_initializer='zeros'))\n","280d6491":"- forward pass to get the values of the hidden layer, input the value into the sigmoid fucntion to get the output for hidden layer\n- use the hidden layer output to continue to forward pass to get the value of the output node (y)","22d80c3a":"- now that the backpropagation process is understood, it will become the foundation mechanisms to comprehend more complex neural network architecture","9861881e":"- now we calculate the second Error component w.r.t H out 1, if you look at the setup from page 1, you can see that half of the Error ('half of error' w.r.t H out 1) is coming from w6 which in turn is coming from H out 1\n\n- this is why w6 was previously caculated and updated to w6 new\n","4df240cd":"- just like before the question that we are asking is what is the closet component w.r.t the cost function that we are trying to find the gradient (rate of change of the weight)\n- that component is cloest at H1 because w1 is the closest to it\n- therefore first componet is the derivative of the cost function w.r.t to the contribution of H out 1.\n- second component is the derivative of the H out 1 (after input into the sigmoid) w.r.t the H1 (the value before input into the sigmoid function). We can prove that the sigmoid function in the binary class is y(1-y)\n\n- third component is the derivative of the H1 (the value before input into the sigmoid function) w.r.t to w1, this is a linear function, therefore the derivative of w5 it's itself","1f7e0c52":"- now that we have all 3 components, we can update w1","cc0280a9":"- input the y out value into the cost fucntion, the cost function takes into account both the output y1 and output y2\n- the goal is to minimized the cost funtion for a more accurate prediction\n- in this case output y1 predicts value of 0.6 for a target in reality that is 0 and output y2 predicts value of 0.65 for a target in reality that is 1\n\nbackpropagation step\n\n- find the partial derivative of w5 w.r.t to the Error total ( the quadratic cost fucntion)\n- first component is the derivative of the cost function w.r.t to the contribution of output y1. Here use the chain rule\n- second component is the derivative of the output y1 (after input into the sigmoid) w.r.t the y1 (the value before input into the sigmoid function). We can prove that the sigmoid function in the binary class is y(1-y)\n\n- third component is the derivative of the y1 (the value before input into the sigmoid function) w.r.t to w5, this is a linear function, there for the derivative of w5 it's itself","0e10f746":"- now we can update the original randomly initiated weight of 0.5 with the gradient w5 we just calculated w.r.t Error total\n- now w5 is updated. Keep in mind the learn rate; If the learn rate is too small or too large it can overshoot or become stuck at a local minimum"}}