{"cell_type":{"ea6f0674":"code","99ca6660":"code","31241edf":"code","fc311954":"code","3864b755":"code","9cac9815":"code","c902cc8e":"code","dd747d2d":"code","78ada425":"code","1cacb00f":"code","fc90d0d5":"code","1171333b":"code","c6a12344":"code","23732726":"code","eac2d3a3":"code","af7d070d":"code","17c3a637":"code","0d7b72e5":"code","e526cc73":"code","afa3cc64":"code","0d54ed25":"code","5df0dba3":"code","0f71bb0a":"code","907c01e0":"code","9530beb5":"code","f1f463f5":"code","2bd30822":"code","8c2706b3":"code","467dd82b":"code","d461437a":"code","1cf6ef27":"code","35e205fb":"code","f5187bd0":"code","8306fe26":"code","f1c9be74":"code","747b566d":"code","b906873b":"code","a6e94e38":"code","696de77e":"code","553a2d11":"code","4ba15baa":"code","4537371e":"code","9d08b662":"code","1e58b5f1":"code","92f2a7dd":"code","0386bc03":"code","bf90cea9":"code","43366bf0":"code","926651b7":"code","a1c6924c":"code","2ffce73d":"code","3e294f65":"code","26759018":"code","3efdb7bc":"code","177a1b5b":"code","31fc3d51":"code","70993c8c":"code","0167d69f":"code","a962b978":"code","167a1bf3":"code","4634f4ea":"code","a0a08323":"code","8e565bff":"code","2937f7db":"code","94613a7c":"code","a3057650":"code","22c0b642":"code","8d4882bd":"code","c5599c75":"code","7b184fe9":"code","11e18d44":"code","832ea66d":"code","3a787f1e":"code","19ffe68a":"code","10f6958a":"code","7ca7ec68":"code","216adf55":"code","1df2545a":"code","9fc982c0":"code","c2e6dca2":"code","03700c5d":"code","626c8775":"code","70c4f657":"code","143e57ed":"code","66611f64":"code","e7074edc":"code","ab803091":"code","e284c630":"code","1d37b848":"code","513db76c":"code","354b93d8":"code","4dd4aa07":"code","67183f62":"code","46bd85c9":"code","d0a37ce0":"code","cc6d3c61":"code","fc45ba83":"code","f9addb3d":"code","26010499":"code","7511499c":"code","9f1c61b5":"code","2ffe639c":"code","4bd95715":"code","af7e66b7":"code","4a9b770c":"code","4a9271a1":"code","3109ff2d":"code","e285ba1a":"code","a9409704":"code","c1de504e":"code","d8a1f58e":"code","77dd8398":"code","0c3d404f":"code","84c9b6dd":"code","7f52b23b":"code","41eea02f":"code","226e8f7b":"code","664be9ed":"code","9f3e48f8":"code","8e6f9356":"code","136aadb4":"code","c8cddb49":"code","8cfd50a2":"code","693e42bf":"code","939dcbe7":"code","3759aba0":"code","f9c24d94":"code","29423236":"code","1cd11d5d":"code","015e35bc":"code","b59b2ec4":"code","b866c757":"code","136de473":"markdown","e1bfd050":"markdown","2c143a10":"markdown","9e870ba3":"markdown","db11163e":"markdown","fef6ee3f":"markdown","226d69c4":"markdown","3a79af0c":"markdown","e4e39ea3":"markdown","48e5802a":"markdown","8eb05ffa":"markdown","facc4b49":"markdown","0d9c3ed3":"markdown","34502c14":"markdown","18bd6d27":"markdown","7b0bdda8":"markdown","c54bb56e":"markdown","d61a6920":"markdown","20171200":"markdown"},"source":{"ea6f0674":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","99ca6660":"import pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nprint(\"Setup Complete\")","31241edf":"# Load the data\ntrain_filepath = \"..\/input\/store-sales-time-series-forecasting\/train.csv\"\noil_filepath = \"..\/input\/store-sales-time-series-forecasting\/oil.csv\"\nholidays_filepath = \"..\/input\/store-sales-time-series-forecasting\/holidays_events.csv\"\ntransactions_filepath = \"..\/input\/store-sales-time-series-forecasting\/transactions.csv\"\nstores_filepath = \"..\/input\/store-sales-time-series-forecasting\/stores.csv\"\n\n","fc311954":"# Read the data\ntrain_data = pd.read_csv(train_filepath, index_col=\"id\")\noil_data = pd.read_csv(oil_filepath, index_col=\"date\", parse_dates=True)\nholidays_data = pd.read_csv(holidays_filepath, index_col=\"date\", parse_dates=True)\ntransactions_data = pd.read_csv(transactions_filepath, index_col=\"date\")\n\n","3864b755":"# Convert all 'date' columns into Panda's format:\ntrain_data['date'] = pd.to_datetime(train_data['date'], format = \"%Y-%m-%d\")\noil_data['date'] = pd.to_datetime(oil_data['date'], format = \"%Y-%m-%d\")\nholidays_data['date'] = pd.to_datetime(holidays_data['date'], format = \"%Y-%m-%d\")\ntransactions_data['date'] = pd.to_datetime(transactions_data['date'], format =\"%Y-%m-%d\")","9cac9815":"train_data.head()","c902cc8e":"train_data.tail()","dd747d2d":"oil_data.head()\n","78ada425":"# Display Sales per Product Family\n\nplt.figure(figsize=(16,6))\nplt.title('Sales per product family')\nsns.barplot(x=train_data.sales, y=train_data.family)\nplt.xlabel('Sales')\nplt.ylabel('Product Family')\n\n","1cacb00f":"# Display changes in Oil Price over time\nplt.figure(figsize=(16,6))\nsns.lineplot(data=oil_data.dcoilwtico, label=\"Oil price\")\nplt.title('Ecuador Oil Price')\nplt.xlabel('Date')\nplt.ylabel('Price')","fc90d0d5":"plt.figure(figsize=(16,6))\nsns.scatterplot(x=train_data['onpromotion'], y=train_data['sales'])\nplt.title('Promotion and Sales from 2013 to 2017')\nplt.xlabel('Discount Promotion')\nplt.ylabel('Sales')\n","1171333b":"#Merge Data\ntrain_data_m1 = train_data.merge(oil_data, on = 'date', how = 'left')\ntrain_data_m1","c6a12344":"train_data_m1.head()","23732726":"train_data_m1.tail()","eac2d3a3":"# Daily Oil Price\n# Group the data set by date and oil price per day\noil_price = train_data_m1.groupby('date').dcoilwtico.mean()\noil_price","af7d070d":"# Daily Average Sales\n# grouping data set by date and deriving ave sales per day\nave_sales = train_data_m1.groupby('date').sales.mean()\nave_sales","17c3a637":"#converting data to data frame\nave_sales.to_frame()","0d7b72e5":"#converting data to data frame\noil_price.to_frame()","e526cc73":"# CONCAT to merge columns with similar id\nave_oil_sales = pd.concat([ave_sales, oil_price], axis=1)\nave_oil_sales","afa3cc64":"#filling NaN with values above and below the cell\nave_oil_sales['dcoilwtico'] = ave_oil_sales['dcoilwtico'].fillna(method='ffill')\nave_oil_sales['dcoilwtico'] = ave_oil_sales['dcoilwtico'].fillna(method='bfill')\nave_oil_sales","0d54ed25":"#Ave Sales Diagnostics\n\nimport statsmodels.api as sm\nfrom scipy.stats import norm\nimport pylab\n\nave_sales = norm.rvs(size=1000)\nsm.qqplot(ave_sales, line='45')\npylab.show()\n\nfrom statsmodels.stats.diagnostic import lilliefors\nave_sales = np.random.normal(loc = 20, scale = 5, size=150)\nstatistic,pvalue = lilliefors(ave_sales)\nprint('statistic=%.3f, p=%.3f\\n' %  (statistic, pvalue))\nif pvalue > 0.05:\n    print('Probably Gaussian')\nelse:\n    print('Probably not Gaussian')\n        ","5df0dba3":"oil_price = oil_price.fillna(method='ffill')\noil_price = oil_price.fillna(method='bfill')\noil_price\n","0f71bb0a":"#Oil Price Diagnostics\n\noil_price = norm.rvs(size=1000)\nsm.qqplot(oil_price, line='45')\npylab.show()\n\noil_price = np.random.normal(loc = 20, scale = 5, size=150)\nstatistic,pvalue = lilliefors(oil_price)\nprint('statistic=%.3f, p=%.3f\\n' %  (statistic, pvalue))\nif pvalue > 0.05:\n    print('Probably Gaussian')\nelse:\n    print('Probably not Gaussian')","907c01e0":"# add dummy to holidays\n\nholidays_data['Dummy Holiday'] = '1'\nholidays_data","9530beb5":"#\"groupby\" Holidays_data by Dummy Holiday\n\ndummy_holidays = holidays_data[\"Dummy Holiday\"]\ndummy_holidays","f1f463f5":"dummy_holidays.to_frame()","2bd30822":"final_df = ave_oil_sales.merge(dummy_holidays, on = 'date', how = 'left')\nfinal_df","8c2706b3":"#replace NaN with 0\n\nfinal_df['Dummy Holiday'] = final_df['Dummy Holiday'].fillna(0)\nfinal_df","467dd82b":"# add 'on promotion' to final_df\n# onpromotion has number of products on sale that  day\n\nave_promotion = train_data_m1.groupby('date').onpromotion.mean()\nave_promotion","d461437a":"ave_promotion.to_frame()","1cf6ef27":"#Use merge to add a column of data to the set\n\nfinal_df = final_df.merge(ave_promotion, on = 'date', how = 'left')\nfinal_df","35e205fb":"# Ave_promotion Diagnostics\n\n\nave_promotion = norm.rvs(size=1000)\nsm.qqplot(ave_promotion, line='45')\npylab.show()\n\nave_promotion = np.random.normal(loc = 20, scale = 5, size=150)\nstatistic,pvalue = lilliefors(ave_promotion)\nprint('statistic=%.3f, p=%.3f\\n' %  (statistic, pvalue))\nif pvalue > 0.05:\n    print('Probably Gaussian')\nelse:\n    print('Probably not Gaussian')","f5187bd0":"import numpy as np\nimport pylab\nimport scipy.stats as stats\nimport seaborn as sns\n\nave_promotion = np.random.normal(loc = 20, scale = 5, size = 150)\nax = sns.distplot(ave_promotion)","8306fe26":"oil_price = np.random.normal(loc = 20, scale = 5, size = 150)\nax = sns.distplot(oil_price)","f1c9be74":"ave_sales = np.random.normal(loc = 20, scale = 5, size = 150)\nax = sns.distplot(ave_sales)","747b566d":"#Regression Plot: Overall Avereage Sales and Explanatory Variables\nplt.figure(figsize=(16,6))\nplt.title('Average Sales and Products on Promotion')\nsns.regplot(x=final_df['onpromotion'], y=final_df['sales'])\nplt.xlabel('Average Products on Promotion')\nplt.ylabel('Average Sales')","b906873b":"plt.figure(figsize=(16,6))\nplt.title('Average Sales and Products on Promotion')\nsns.regplot(x=final_df['dcoilwtico'], y=final_df['sales'])\nplt.xlabel('Oil Price')\nplt.ylabel('Average Sales')","a6e94e38":"#Exploring the correlation of numerical features\nplt.figure(figsize=(5,3), dpi=150)\nsns.heatmap(final_df.corr(), annot=True)","696de77e":"#Group by multiple columns and averaging sales\nave_family_sales = train_data_m1.groupby([\"date\",\"family\"]).sales.mean()\nave_family_sales","553a2d11":"ave_family_sales.to_frame()","4ba15baa":"#Extracting Select Data from a DF\n\ngrocery_sales = train_data_m1.loc[train_data_m1.family == 'GROCERY I']\ngrocery_sales","4537371e":"# Average Grocery I Sales, grouped by date\nave_grocery_sales = grocery_sales.groupby('date').sales.mean()\nave_grocery_sales","9d08b662":"ave_grocery_sales.to_frame()","1e58b5f1":"# Average Grocery I Sales, grouped by date\noil_prices = grocery_sales.groupby('date').dcoilwtico.mean()\noil_prices","92f2a7dd":"oil_ave_grocery_sales = pd.concat([ave_grocery_sales, oil_prices], axis = 1)\noil_ave_grocery_sales","0386bc03":"oil_ave_grocery_sales['dcoilwtico'] = oil_ave_grocery_sales['dcoilwtico'].fillna(method='ffill')\noil_ave_grocery_sales['dcoilwtico'] = oil_ave_grocery_sales['dcoilwtico'].fillna(method='bfill')\noil_ave_grocery_sales","bf90cea9":"# Merge holiday dummy data to the set\n\ngrocery_final_df = oil_ave_grocery_sales.merge(dummy_holidays, on = 'date', how = 'left')\ngrocery_final_df","43366bf0":"#replace NaN with 0\n\ngrocery_final_df['Dummy Holiday'] = grocery_final_df['Dummy Holiday'].fillna(0)\ngrocery_final_df","926651b7":"#derive onpromotion under Grocery I\n\nave_grocery_promo = grocery_sales.groupby('date').onpromotion.mean()\nave_grocery_promo","a1c6924c":"grocery_final_df = grocery_final_df.merge(ave_grocery_promo, on = 'date', how = 'left')\ngrocery_final_df\n\n","2ffce73d":"#Regression Plot: Grocery Sales and Explanatory Variables\nplt.figure(figsize=(16,6))\nplt.title('Average Grocery Sales and Products on Promotion')\nsns.regplot(x=grocery_final_df['onpromotion'], y=grocery_final_df['sales'])\nplt.xlabel('Average Products on Promotion')\nplt.ylabel('Average Sales')","3e294f65":"plt.figure(figsize=(16,6))\nplt.title('Average Grocery Sales and Products on Promotion')\nsns.regplot(x=grocery_final_df['dcoilwtico'], y=grocery_final_df['sales'])\nplt.xlabel('Oil Price')\nplt.ylabel('Average Sales')","26759018":"# Ave_grocery_sales Diagnostics\n\n\nave_grocery_sales = norm.rvs(size=1000)\nsm.qqplot(ave_grocery_sales, line='45')\npylab.show()\n\nave_grocery_sales = np.random.normal(loc = 20, scale = 5, size=150)\nstatistic,pvalue = lilliefors(ave_grocery_sales)\nprint('statistic=%.3f, p=%.3f\\n' %  (statistic, pvalue))\nif pvalue > 0.05:\n    print('Probably Gaussian')\nelse:\n    print('Probably not Gaussian')","3efdb7bc":"# ave_grocery_promo Diagnostics\n\n\nave_grocery_promo = norm.rvs(size=1000)\nsm.qqplot(ave_grocery_promo, line='45')\npylab.show()\n\nave_grocery_promo = np.random.normal(loc = 20, scale = 5, size=150)\nstatistic,pvalue = lilliefors(ave_grocery_promo)\nprint('statistic=%.3f, p=%.3f\\n' %  (statistic, pvalue))\nif pvalue > 0.05:\n    print('Probably Gaussian')\nelse:\n    print('Probably not Gaussian')","177a1b5b":"#Exploring the correlation of numerical features\nplt.figure(figsize=(5,3), dpi=150)\nsns.heatmap(grocery_final_df.corr(), annot=True)","31fc3d51":"#Regression OLS -- Average Total Sales and explanatory variables\n\n# Step 1: Create X and Y data matrices\nX = final_df.drop(['sales'], axis = 1)\nY = final_df.drop(['dcoilwtico', 'onpromotion', 'Dummy Holiday'], axis = 1)","70993c8c":"Y","0167d69f":"X","a962b978":"Y = pd.DataFrame(Y)\nY","167a1bf3":"X = pd.DataFrame(X)\nX","4634f4ea":"import statsmodels.api as sm\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std","a0a08323":"X = sm.add_constant(X)\nX","8e565bff":"# print regression result\n# Average Daily Sales\n\nest = sm.OLS(Y.astype(float), X.astype(float)).fit()\nest\nprint(est.summary())","2937f7db":"# OLS Regression for Ave Grocery Sales\n\nY_grocery = grocery_final_df.drop(['dcoilwtico', 'onpromotion', 'Dummy Holiday'], axis = 1)\nY_grocery","94613a7c":"X_grocery = grocery_final_df.drop(['sales'], axis = 1)\nX_grocery","a3057650":"X_grocery = sm.add_constant(X_grocery)\nX_grocery","22c0b642":"#Regression result for Grocery Sales\n\nmodel_g = sm.OLS(Y_grocery.astype(float), X_grocery.astype(float))\nresult = model_g.fit()\nprint(result.summary())","8d4882bd":"# Step 2: Import Library\n\nfrom sklearn.model_selection import train_test_split","c5599c75":"# Step 3: Perfrom 70\/30 data split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)","7b184fe9":"# Step 4: Check data dimension\n\nX_train.shape, Y_train.shape, X_test.shape, Y_test.shape\n","11e18d44":"# Step 4: Import library for Linear Regression Model\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score","832ea66d":"# Random Forest for Regression\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","3a787f1e":"from sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor(n_estimators=100, random_state=42)\nregressor.fit(X_train, Y_train)\ny_pred = regressor.predict(X_test)","19ffe68a":"Y_test","10f6958a":"y_pred","7ca7ec68":"print(y_pred.shape)","216adf55":"from sklearn import metrics\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_test, y_pred))\nprint('R2:', metrics.r2_score(Y_test, y_pred))\nprint('Explained Variance:', metrics.explained_variance_score(Y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, y_pred)))","1df2545a":"Y_test_copy = Y_test\ndf_pred = Y_test_copy\ndf_pred = df_pred.rename(columns={'sales' : 'Actual Values'})\ndf_pred","9fc982c0":"df_pred['Predicted Values'] = y_pred\ndf_pred['% Difference'] = abs(df_pred['Predicted Values'] - df_pred['Actual Values'])\/df_pred['Actual Values']*100\ndf_pred","c2e6dca2":"# LGBM\n\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport lightgbm as ltb","03700c5d":"# Step 3: Perfrom 70\/30 data split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)","626c8775":"# Step 2 - Using LightGBM Regressor and calculating the scores\n\nmodel = ltb.LGBMRegressor()\nmodel.fit(X_train.astype(float), Y_train.astype(float))\nprint(); print(model)\n    \nexpected_y  = Y_test.astype(float)\npredicted_y = model.predict(X_test.astype(float))","70c4f657":"expected_y","143e57ed":"print(predicted_y.shape)\npredicted_y","66611f64":"print(metrics.r2_score(expected_y, predicted_y))\nprint(metrics.mean_squared_log_error(expected_y, predicted_y))","e7074edc":"df_lgbm = expected_y\ndf_lgbm['Predicted Values'] = predicted_y\ndf_lgbm['% Difference'] = abs(df_lgbm['Predicted Values'] - df_lgbm['sales'])\/df_lgbm['sales']*100\n\ndf_lgbm = df_lgbm.rename(columns={'sales' : 'Actual Values'})\ndf_lgbm","ab803091":"# Random Forest for Grocery Sales\n# Step 3: Perfrom 70\/30 data split\n\nX_grocery_train, X_grocery_test, Y_grocery_train, Y_grocery_test = train_test_split(X_grocery, Y_grocery, test_size = 0.2)","e284c630":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_grocery_train = sc.fit_transform(X_grocery_train)\nX_grocery_test = sc.transform(X_grocery_test)","1d37b848":"from sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor(n_estimators=200, random_state=100)\nregressor.fit(X_grocery_train, Y_grocery_train)\ny_grocery_pred = regressor.predict(X_grocery_test)","513db76c":"y_grocery_pred","354b93d8":"print(y_grocery_pred.shape)","4dd4aa07":"Y_grocery_test","67183f62":"from sklearn import metrics\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(Y_grocery_test, y_grocery_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(Y_grocery_test, y_grocery_pred))\nprint('R2:', metrics.r2_score(Y_grocery_test, y_grocery_pred))\nprint('Explained Variance:', metrics.explained_variance_score(Y_grocery_test, y_grocery_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_grocery_test, y_grocery_pred)))","46bd85c9":"df_grf = Y_grocery_test\ndf_grf['Predicted Values'] = y_grocery_pred\ndf_grf['% Difference'] = abs(df_grf['Predicted Values'] - df_grf['sales'])\/df_grf['sales']*100\ndf_grf\n\ndf_grf = df_grf.rename(columns={'sales' : 'Actual Values'})\ndf_grf","d0a37ce0":"# Random Forest for Grocery Sales\n# Step 3: Perfrom 70\/30 data split\n\nX_grocery_train, X_grocery_test, Y_grocery_train, Y_grocery_test = train_test_split(X_grocery, Y_grocery, test_size = 0.2)","cc6d3c61":"model = ltb.LGBMRegressor()\nmodel.fit(X_grocery_train.astype(float), Y_grocery_train.astype(float))\nprint(); print(model)\n    \nexpected_g_y  = Y_grocery_test.astype(float)\npredicted_g_y = model.predict(X_grocery_test.astype(float))","fc45ba83":"expected_g_y","f9addb3d":"predicted_g_y","26010499":"print(metrics.r2_score(expected_g_y, predicted_g_y))\nprint(metrics.mean_squared_log_error(expected_g_y, predicted_g_y))","7511499c":"df_glgbm = expected_g_y\ndf_glgbm['Predicted Values'] = predicted_g_y\ndf_glgbm['% Difference'] = abs(df_glgbm['Predicted Values'] - df_glgbm['sales'])\/df_glgbm['sales']*100\n\n\ndf_glgbm = df_glgbm.rename(columns={'sales' : 'Actual Values'})\ndf_glgbm","9f1c61b5":"# load reshaped transactions_data\n\ntransact_filepath = \"..\/input\/transaction-data-final\/transactions_data_F.csv\"","2ffe639c":"transact_data = pd.read_csv(transact_filepath, index_col=\"Id\")","4bd95715":"# Column Values refer to Store Codes\ntransact_data.head()","af7e66b7":"transact_data['date'] = pd.to_datetime(transact_data['date'], format =\"%Y-%m-%d\")","4a9b770c":"transact_data.mean()","4a9271a1":"transact_data = transact_data.fillna(transact_data.mean())\ntransact_data","3109ff2d":"# Merge the data set to include transactions per Store Number (Columns)\nfinal_df2 = final_df.merge(transact_data, on = \"date\", how = 'left')\nfinal_df2","e285ba1a":"# Set the index to Date\nfinal_df3 = final_df2.set_index('date')\nfinal_df3","a9409704":"# Check how many cells are NaN\nfinal_df3.isnull().sum().sum()","c1de504e":"# Fill NaN with mean values per Column or mean daily no. of transactions per Store location\nfinal_df3 = final_df3.fillna(final_df3.mean())\nfinal_df3","d8a1f58e":"# Check if there are still NaN\nfinal_df3.isnull().sum().sum()","77dd8398":"# Step 1: Create X and Y data matrices\nx_fin = final_df3.drop(['sales'], axis = 1)\ny_fin = final_df3['sales']","0c3d404f":"x_fin","84c9b6dd":"y_fin","7f52b23b":"y_fin = pd.DataFrame(y_fin)\ny_fin","41eea02f":"x_fin = pd.DataFrame(x_fin)\nx_fin","226e8f7b":"# Step 2: Import Library\n\nfrom sklearn.model_selection import train_test_split","664be9ed":"# Step 3: Perfrom 70\/30 data split\n\nx_fin_train, x_fin_test, y_fin_train, y_fin_test = train_test_split(x_fin, y_fin, test_size = 0.3)","9f3e48f8":"# Step 4: Import library for Linear Regression Model\n\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score","8e6f9356":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nx_fin_train = sc.fit_transform(x_fin_train)\nx_fin_test = sc.transform(x_fin_test)","136aadb4":"x_fin_train","c8cddb49":"x_fin_test","8cfd50a2":"from sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor(n_estimators=20, random_state=0)\nregressor.fit(x_fin_train, y_fin_train)\ny_fin_pred = regressor.predict(x_fin_test)","693e42bf":"from sklearn import metrics\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_fin_test, y_fin_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_fin_test, y_fin_pred))\nprint('R2:', metrics.r2_score(y_fin_test, y_fin_pred))\nprint('Explained Variance:', metrics.explained_variance_score(y_fin_test, y_fin_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_fin_test, y_fin_pred)))","939dcbe7":"df_rfall = y_fin_test\ndf_rfall['Predicted Values'] = y_fin_pred\ndf_rfall['% Difference'] = abs(df_rfall['Predicted Values'] - df_rfall['sales'])\/df_rfall['sales']*100\n\n\ndf_rfall = df_rfall.rename(columns={'sales' : 'Actual Values'})\ndf_rfall","3759aba0":"# LGBM\n\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('ggplot')\n\nimport lightgbm as ltb","f9c24d94":"x_f_train, x_f_test, y_f_train, y_f_test = train_test_split(x_fin, y_fin, test_size = 0.3)","29423236":"# Step 2 - Using LightGBM Regressor and calculating the scores\n\nmodel = ltb.LGBMRegressor()\nmodel.fit(x_f_train.astype(float), y_f_train.astype(float))\nprint(); print(model)\n    \nexpected_y  = y_f_test.astype(float)\npredicted_y = model.predict(x_f_test.astype(float))","1cd11d5d":"expected_y","015e35bc":"predicted_y","b59b2ec4":"print(metrics.r2_score(expected_y, predicted_y))\nprint(metrics.mean_squared_log_error(expected_y, predicted_y))","b866c757":"df_lgbmall = expected_y\ndf_lgbmall['Predicted Values'] = predicted_y\ndf_lgbmall['% Difference'] = abs(df_lgbmall['Predicted Values'] - df_lgbmall['sales'])\/df_lgbmall['sales']*100\n\n\ndf_lgbmall = df_lgbmall.rename(columns={'sales' : 'Actual Values'})\ndf_lgbmall","136de473":"**Diagnostics for Grocery Sales, Promotion**","e1bfd050":"# **Sixth: Light GBM with Average Grocery Sales**","2c143a10":"Print the Metrics of Random Forest Regression","9e870ba3":"# **Third: Random Forest Regression with Ave Daily Sales (all products)**","db11163e":"Narrow the analysis on a specific product group - Grocery Sales","fef6ee3f":"# **Fifth: Random Forest Regression with Average Grocery Sales**","226d69c4":"# **Fourth: Light GBM with Ave Sales**","3a79af0c":"*Lilliefors Test for Normality*\n\nThe Lilliefors test is a normality test based on the Kolmogorov\u2013Smirnov test. As all the above methods, this test is used to check if the data come from a normal distribution.\n\nIf the p-value \u2264 0.05, then we reject the null hypothesis i.e. we assume the distribution of our variable is not normal\/gaussian.\nIf the p-value > 0.05, then we fail to reject the null hypothesis i.e. we assume the distribution of our variable is normal\/gaussian.","e4e39ea3":"# **Second: OLS Regression with Average Grocery Sales**","48e5802a":"**Print Metrics of Random Forest Regression**","8eb05ffa":"# **First: OLS Regression with Average Daily Sales (All products)**","facc4b49":"# **Data Preparation with EDAs**","0d9c3ed3":"**Print Metrics of Random Forest Regression**","34502c14":"Prepare relevant data for Regression and Machine Learning.\nAddress NaN cell values.\nAdd dummy variable for holildays.","18bd6d27":"# **Seventh: Random Forest Regression with Average Daily Sales, with daily transactions per store location used in the regression**","7b0bdda8":"# **Sixth: Light GBM Regression with Average Daily Sales, with daily transactions per store location used in the regression**","c54bb56e":"**Diagnostics of Variables**","d61a6920":"The following EDA provides an overview of the data and data relationships ","20171200":"# **Regression Analysis by OLS and Random Forest**"}}