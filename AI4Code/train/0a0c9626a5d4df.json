{"cell_type":{"4e2919ea":"code","8e06cdbf":"code","810ed74e":"code","8b09f000":"code","370dc346":"code","28883dd3":"code","9d0db0b5":"code","dd95a9e5":"code","eb7f4600":"code","9d6c38e2":"code","4ef87776":"code","3c4f2d7f":"code","9465ed23":"code","d8d2d8ba":"code","7b9c3771":"code","bcfaf72d":"code","304a7b45":"code","376203de":"code","49fb7638":"code","e4db03b7":"code","da798ce8":"code","c78b192a":"code","3fc61a44":"code","f04688e6":"code","31b9ef11":"code","3ddf169e":"code","fd39641f":"code","95365495":"code","505f6756":"code","8bc9e792":"code","ace2f8b6":"code","e4dc25d3":"code","d4598b79":"code","5ef2121e":"code","b62f1122":"markdown","f06e9500":"markdown","9cc09dc5":"markdown","93cd1d02":"markdown","0fc2d391":"markdown","a930d2ae":"markdown","1bc74f5f":"markdown","64a17369":"markdown","037b13ed":"markdown","57af1fa7":"markdown","d8a2ccac":"markdown","5b761e8d":"markdown","3afc5c11":"markdown","c8ccf80f":"markdown","d8edafa8":"markdown","13a42af0":"markdown","7443ea0e":"markdown","0319e07b":"markdown","b2118137":"markdown","ff2fd61f":"markdown","184eebde":"markdown","0ab056e7":"markdown","39ab5b68":"markdown","7061017c":"markdown","ad3c5e9d":"markdown","582e1399":"markdown"},"source":{"4e2919ea":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport seaborn as sns\n%matplotlib inline\ndata_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_train.head()\n","8e06cdbf":"data_train.describe()","810ed74e":"data_train = data_train.drop(['PassengerId','Name','Ticket'],axis = 'columns')\n\nprint(data_train.isnull().sum())\nplt.figure(figsize=(20,10))\nsns.heatmap(data_train.isnull())","8b09f000":"data_train = data_train.drop('Cabin',axis='columns')","370dc346":"plt.figure(figsize=(20,15))\nfor i, col in enumerate(data_train.columns):\n    plt.subplot(2,4,i+1)\n    sns.boxplot(x=col,y='Age', data=data_train)","28883dd3":"plt.subplot(1,2,1)\nsns.boxplot('Pclass','SibSp',data=data_train)\nplt.subplot(1,2,2)\nsns.boxplot('Pclass','Parch',data=data_train)","9d0db0b5":"print(data_train[['Age','Pclass']].groupby('Pclass').mean())","dd95a9e5":"# 25 29 37\ndef AgeMissing(age_null):\n    age = age_null['Age'].tolist()\n    pclass = age_null['Pclass'].tolist()\n    \n    for i in range(len(age)):\n        if pd.isnull(age[i]):\n            if pclass[i] == 3:\n                age[i] = 25\n            elif pclass[i] == 2:\n                age[i] = 29\n            else:\n                age[i] = 38\n    return age\n\ndata_train.Age = AgeMissing(data_train[['Age','Pclass']])","eb7f4600":"data_train[\"Embarked\"].value_counts()","9d6c38e2":"data_train[\"Embarked\"] = data_train[\"Embarked\"].fillna('S')\ndata_train.isnull().sum()","4ef87776":"grid = sns.FacetGrid(data_train, col='Survived')\ngrid.map(plt.hist,'Age',bins=15)","3c4f2d7f":"grid = sns.FacetGrid(data_train, col='Survived',row='Pclass')\ngrid.map(plt.hist,'Age',bins=15)","9465ed23":"data_train.head()","d8d2d8ba":"sns.catplot('Sex',kind='count',data=data_train,hue='Survived')\nsns.catplot('Sex',kind='count',data=data_train,hue='Pclass',col='Survived')","7b9c3771":"data_train.head()","bcfaf72d":"data_train = data_train.join(pd.get_dummies(data_train.Embarked,drop_first =True).rename(columns = {'Q':'Embarked_Q',\n                                                      'S':'Embarked_S'}))\ndata_train = data_train.drop('Embarked',axis='columns')\n\ndata_train.Sex = data_train.Sex.apply(lambda x: 1 if x == 'male' else 0)\ndata_train.head()","304a7b45":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nx = data_train.columns.tolist()\ny='Survived'\nx.remove(y)\n\nx_train,x_test,y_train,y_test =  train_test_split(data_train[x],data_train[y],test_size=0.2)","376203de":"lg = LogisticRegression()\nlg.fit(x_train,y_train)\nlg_predicts = lg.predict(x_test)\n\nfrom sklearn.metrics import classification_report,confusion_matrix\nprint('Confusion matrix for Logistic :\\n ',confusion_matrix(y_test,lg_predicts),\"\\n\")\nprint(classification_report(y_test,lg_predicts))","49fb7638":"from sklearn.tree import DecisionTreeClassifier\ntr = DecisionTreeClassifier()\ntr.fit(x_train,y_train)\ntr_predicts = tr.predict(x_test)\n\nprint('Confusion matrix for Decision Tree:\\n ',confusion_matrix(y_test,tr_predicts),\"\\n\")\nprint(classification_report(y_test,tr_predicts))","e4db03b7":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train,y_train)\nrf_predicts = rf.predict(x_test)\n\nprint('Confusion matrix for Random Forest:\\n ',confusion_matrix(y_test,rf_predicts),\"\\n\")\nprint(classification_report(y_test,rf_predicts))","da798ce8":"plt.figure(figsize=(18,8))\nsns.heatmap(data_train.corr(),center=0,annot=True)","c78b192a":"data_train.Age.describe()","3fc61a44":"data_train.Age = data_train.Age.apply(lambda x : 'Age_10' if x<11 else ('Age_20' if x<21 else ('Age_40' if x<41 else 'Age_above_40')))\n#data_train.Age = data_train.Age.apply(lambda x : 1 if x<11 else (2 if x<21 else (3 if x<41 else 0)))\n\ndata_train = data_train.join(pd.get_dummies(data_train.Age,drop_first=True))\ndata_train = data_train.drop('Age',axis='columns')\n","f04688e6":"x = data_train.columns.tolist()\ny='Survived'\nx.remove(y)\n\nx_train,x_test,y_train,y_test =  train_test_split(data_train[x],data_train[y],test_size=0.2)","31b9ef11":"lg = LogisticRegression()\nlg.fit(x_train,y_train)\nlg_predicts = lg.predict(x_test)\n\nprint('Confusion matrix for Logistic :\\n ',confusion_matrix(y_test,lg_predicts),\"\\n\")\nprint(classification_report(y_test,lg_predicts))","3ddf169e":"rf = RandomForestClassifier()\nrf.fit(x_train,y_train)\nrf_predicts = rf.predict(x_test)\n\nprint('Confusion matrix for Random Forest:\\n ',confusion_matrix(y_test,rf_predicts),\"\\n\")\nprint(classification_report(y_test,rf_predicts))","fd39641f":"from sklearn.model_selection import KFold,cross_val_score\nrf = RandomForestClassifier(n_estimators = 100)\n\nx = data_train.columns.tolist()\ny='Survived'\nx.remove(y)\n\nprint(\"Recall - \" , np.mean(cross_val_score(rf,data_train[x],data_train[y],cv=10,scoring='recall')))\nprint(\"precision - \" , np.mean(cross_val_score(rf,data_train[x],data_train[y],cv=10,scoring='precision')))\nprint(\"accuracy - \" , np.mean(cross_val_score(rf,data_train[x],data_train[y],cv=10,scoring='accuracy')))\n\nx = data_train[x]\ny = data_train[y]\n\n#x = pd.DataFrame(scaler.fit_transform(x),columns= x.columns)\n\n\nscores=[]\n\ncv = KFold(n_splits=10, random_state=42, shuffle=False)\nfor train_index, test_index in cv.split(x):\n    x_train, x_test, y_train, y_test = x.loc[train_index], x.loc[test_index], y.loc[train_index], y.loc[test_index]\n    rf.fit(x_train, y_train)\n    scores.append(rf.score(x_test, y_test))\n    \n\nnp.mean(scores)\n","95365495":"data_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndata_test = pd.read_csv('..\/input\/titanic\/test.csv')\npassenger_id = data_test.PassengerId\ndef AgeMissing(age_null):\n    age = age_null['Age'].tolist()\n    pclass = age_null['Pclass'].tolist()\n    \n    for i in range(len(age)):\n        if pd.isnull(age[i]):\n            if pclass[i] == 3:\n                age[i] = 25\n            elif pclass[i] == 2:\n                age[i] = 29\n            else:\n                age[i] = 38\n    return age\n\ndef GetTitle(name):\n    title = re.findall(' ([A-Za-z]+)\\.', name)[0]\n    if title:\n        return title\n    else:\n        return(\"\")\ndef ChangeFeatures(data):\n    data.Age = AgeMissing(data[['Age','Pclass']])\n    data[\"Embarked\"] = data[\"Embarked\"].fillna('S')\n    data['Fare'].fillna(np.mean(data['Fare']))\n    data[\"Embarked\"] = data[\"Embarked\"].map({'S':0,'C':1,'Q':2})\n    data.Sex = data.Sex.apply(lambda x: 1 if x=='male' else 0)\n    \n    # Get the title from the name and arrange them in a numeric categorial format\n    data['Title'] = data.Name.apply(GetTitle)\n    data['Title'] = data['Title'].replace('Ms','Miss')\n    data['Title'] = data['Title'].replace(['Mlle','Mme'],'Mrs')\n    data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', \n                                                 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    data['Title'] = data['Title'].map({'Mr':1,'Miss':2,'Mrs':3,'Master':4,'Rare':5})\n    data['Title'].fillna(0)\n    \n    # adding Parch and SibSp  as a family size and SibSp will be dropped.\n    data['FamilySize'] = data.Parch + data.SibSp\n    \n    # Changing the Fare values to a numeric categorial of a respective ranges\n    bins = [0,7.91,14.45,31,513]\n    labels = [0,1,2,3]\n    data.Fare = pd.cut(data.Fare,bins,labels=labels).astype(int)\n    \n    # Changing the Age values to a numeric categorial of a respective ranges\n    bins = [0,6,16,29,42,55,80]\n    labels = [0,1,2,3,4,5]\n    data.Age = pd.cut(data.Age,bins,labels=labels).astype(int)\n\n    data.drop(['Name','Cabin','PassengerId','Ticket','SibSp'],axis='columns',inplace=True)\n    return(data)\n\ndata_train = ChangeFeatures(data_train)\ndata_test = ChangeFeatures(data_test)","505f6756":"plt.figure(figsize=(20,10))\nsns.heatmap(data_train.corr(),annot=True)","8bc9e792":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm  import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nx = data_train.columns.tolist()\ny = 'Survived'\nx.remove(y)\nknn = KNeighborsClassifier(n_neighbors = 7)\n\nrf = RandomForestClassifier(n_jobs= -1,\n                            n_estimators= 100,\n                            warm_start = True,\n                            max_depth =  6,\n                            min_samples_leaf = 2,\n                            max_features = 'auto')\n\nsvc = SVC(gamma=.1, kernel='rbf', probability=True)\n\ngb = GradientBoostingClassifier(n_estimators = 100,\n                                max_depth = 5,\n                                min_samples_leaf = 2\n                               )\n\n\nprint(\"\\naccuracy RF - \" , np.mean(cross_val_score(rf,data_train[x],data_train[y],cv=10,scoring='accuracy')))\nprint(\"\\naccuracy KNN - \" , np.mean(cross_val_score(knn,data_train[x],data_train[y],cv=10,scoring='accuracy')))\nprint(\"\\naccuracy SVC - \" , np.mean(cross_val_score(svc,data_train[x],data_train[y],cv=10,scoring='accuracy')))\nprint(\"\\naccuracy GB - \" , np.mean(cross_val_score(gb,data_train[x],data_train[y],cv=10,scoring='accuracy')))","ace2f8b6":"# kfold with 5 folds is used for training \nn_folds = 5\ncv = KFold(n_splits=n_folds, random_state=42, shuffle=False)\ndef PerformCV(clf,test):\n    \n    local_test=np.empty((n_folds,len(data_test)))\n    train_labels = np.zeros(len(data_train)) +2 \n    i=0\n    for train_index, test_index in cv.split(data_train[x]):\n        x_train = data_train[x].loc[train_index]\n        x_test = data_train[x].loc[test_index]\n        y_train = data_train[y].loc[train_index]\n        \n        clf.fit(x_train, y_train)\n        train_labels[test_index] = clf.predict(x_test)\n        local_test[i,:] =  clf.predict(test)\n        i += 1\n    return (train_labels,np.mean(local_test,axis=0))\n\n\nrf_train,rf_test = PerformCV(rf,data_test)\nknn_train,knn_test = PerformCV(knn,data_test)\nsvc_train,svc_test = PerformCV(svc,data_test)\ngb_train,gb_test = PerformCV(gb,data_test)","e4dc25d3":"# concatenation\nx_train = pd.DataFrame({'KNN' : knn_train,\n              'RF' : rf_train,\n              'SVC':svc_train,\n              'GB':gb_train})\n\nx_test = pd.DataFrame({'KNN' : knn_test,\n              'RF' : rf_test,\n              'SVC':svc_test,\n              'GB':gb_test})\n\ny_train = data_train[y]","d4598b79":"import xgboost as xgb\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1)\ngbm.fit(x_train, y_train)\npredictions = gbm.predict(x_test)\npredictions","5ef2121e":"final_output = pd.DataFrame({'PassengerId' : passenger_id,\n              'Survived':predictions})\nfinal_output.to_csv(\"titanic_predictions.csv\",index=False)\nfinal_output.head()","b62f1122":"For statcking, all te above models are trained with the training data. Then train predictions and test predictions are concatenated into respective dataframes\nLater the concatenated trained data will be sent XGboost classifer for the second level of training and predictions are made wit the concatenated test data","f06e9500":"The above boxplots doesnot give much information between Parch vs Pclass and SinSp vs Pclass\n\nSo, we will be considering only Pclass to to fill in the missing values of Age.","9cc09dc5":"TIPS:\n* Could improve the model by performing anoter level. \n* By Splitting the numeric categorials using pd.get_dummies may improve the accuracy\n\nBut since we have a small data, it is always recommended to go with simple models\n","93cd1d02":"> **Data Analysis**","0fc2d391":"1. Sex should be changed from categorial to binary => male(1) and female(0) \n2. For Embarked we could use the get_dummies which will split the categorail data into columns with binary","a930d2ae":"Ranges can be defined as\n* 0-10 (based on our insights that children below 10 survived) , 11-20, 21-40 and  >40\n* Use get_dummies to split them","1bc74f5f":"From above we can see that 'Age', 'Cabin' and \"Embarked' have null values. These can be treated by filling them with mean or median or can be ignored. \n1. \"Cabin\" has a lot of null values which is 687 out of 891. It is better to drop the column rather than treating the null values.\n2. 'Age' can be replaced with mean or median but let's see how it is effected by other features and find any other better option to handle these null values.","64a17369":"Passengerid, Name and Ticket columns can be droped as they provide no information for analysis","037b13ed":"** Let's try to improve the accuracy by changing the features and performing ensemble\/statcking classification**","57af1fa7":"> **Data Cleaning**","d8a2ccac":"*Insights*\n* Infants of age <=5 survived\n* Older people above 50 survived\n* more people in the range 20-30 years of age didn't survive \n* Passengers in the ship are largely distributed between 20-40 years.","5b761e8d":"Accuracy seems to be improved after making the feature changes","3afc5c11":"** Observations from the analysis: **\n* Infants of age <=5 and older people above 50 survived with almost 90% probability\n* Most of the people in the range 20-30 years of age didn't survive\n* A large of the passangers from class 1 survived\n* Almost all the females of class 1 and 3 survived\n* Only around 15% of the males form class 3 survived and same with class 2\n* Almost all the females of class 1 and 3 survived\n* Only around 15% of the males form class 3 survived and same with class 2\n\n\nThese insights explain that the authorities first started evacuating the children and older people in the order of the class 3 being low and 1 being high. Later females were evacuated of class 1 and 2. Past this it is hard to say anything about the situations with the available data[](http:\/\/).","c8ccf80f":"*Insights*\n* 1st graph -> more females survived the disaster, almost 60%\n* 2nd graph\n    * Almost all the females of class 1 and 3 survived\n    * Only around 15% of the males form class 3 survived and same with class 2","d8edafa8":"> **First level**","13a42af0":"From above Heatmap we can see the correlations between the columns. None of the fefatures are higly correlated which says that each feature holds a unique value for prediction.","7443ea0e":"*Insights*\n* Very less passengers in class 3 survived**\n* Many Passengers from class 1 survived\n* All children below age 15 survived ","0319e07b":"Heatmap says that none of them are higly correlated. So, all the feature are independent on each other. ","b2118137":"From above Pclass, SibSp and Parch contribute for the Age.","ff2fd61f":"using the above values the Missing Age values are replaced with the help of the Pclass\n\nLet's create afunction to provide the  replaces the missing values with the mean age with respect to he class that passenger belongs to.","184eebde":"> **Second level model**","0ab056e7":"Let's replace the missing 2 values for Embarked with the value with high frequency","39ab5b68":"Let's train and run the model for predictions and see if it improves**","7061017c":"> **Data model**","ad3c5e9d":"RandomForest and LogisticRegression seems to be giving best results","582e1399":"### Introduction to ML using Titanic disaster data\n\nIn this kernel the predictions of the titanic disaster survival will be performed.\n\nMetadata\n* PassengerId -> unique number assigned to each passenger\n* Survived -> this gives the survival stats of the passenger survived(1)  and not survived(0)\n* Pclass -> The class te passenger belongs to. 1 being highest and 3 being lowest class\n* Name -> name of the passenger\n* Sex -> Sex of the passenger\n* Age -> Age of the passenger\n* SibSp -> Provides the information about the passenger's siblings\n* Parch -> Parent or child relationship\n* Ticket -> Ticket details of the passenger\n* Fare -> Fare of the ticket\n* Cabin -> Cabin of the passenger\n* Embarked -> Port of Embarkation\n"}}