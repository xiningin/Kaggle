{"cell_type":{"386d4290":"code","d866df3e":"code","df50ecc3":"code","a3331704":"code","7bf6e11d":"code","14374c11":"code","0bf7d9e8":"code","37519048":"code","ef5b605c":"code","261f82f2":"code","8a31f4ce":"code","9d8e51c6":"code","0dfa3443":"code","a1169686":"code","2a48392a":"code","eb8249e9":"code","98c92f52":"code","ff17d929":"code","0aeb41ff":"code","88ec14a3":"code","3750c17e":"code","35c3561e":"code","87977aad":"code","79f9750e":"code","9e645797":"code","fda95829":"code","7fe1be46":"code","04ba762c":"code","99dd09ee":"code","4fbd5d3d":"code","6c4579a9":"code","6672b49d":"code","05523686":"code","f0343b32":"code","71606df6":"code","5aca1a26":"code","0d7db7fc":"markdown","859b4bd3":"markdown","d7c2a6ba":"markdown","e0a32eb0":"markdown","04b929ea":"markdown","8affcb3c":"markdown","f87be263":"markdown","7210ae72":"markdown","507de913":"markdown","1065b5de":"markdown","2baf596d":"markdown","d898a8eb":"markdown","518a6aa3":"markdown","fb8274be":"markdown","f4082c1e":"markdown","d55dc02a":"markdown","c9b1d08e":"markdown","ee4296d4":"markdown","833243e4":"markdown","f4a42bad":"markdown","a0bc87ad":"markdown","57b3efa2":"markdown","95667adf":"markdown","97aea2b0":"markdown","b8faa6df":"markdown","e00bc37c":"markdown","9fe6af11":"markdown"},"source":{"386d4290":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d866df3e":"# read the data\ndf = pd.read_csv(\"\/kaggle\/input\/rest-rev-toronto\/Rest_rev_toronto.csv\")","df50ecc3":"df.head()","a3331704":"# The Stars distribution \nvalues, counts = np.unique(df['stars'], return_counts=True)\n\nplt.figure()\nplt.bar(values, counts, tick_label=['1','2','3','4','5'])\nplt.title('Distribution of Stars')\nplt.xlabel('Stars')\nplt.ylabel('Number of reviews')\nplt.show()","7bf6e11d":"# Create new column \"Target\" that stored 0's or 1's. 0 being Negative, 1 being Positive\ndf[\"Target\"] = np.where(df[\"stars\"] >= 3, 1, 0)\ndf.head(3)","14374c11":"# Picking the only text and target column\ndf_final = df[['text','Target']]","0bf7d9e8":"df_final.head(5)","37519048":"from wordcloud import WordCloud\nfrom os import path\nfrom PIL import Image\n# Dividing the reviews to positive and negative\npos = df_final[df_final['Target']==1]['text']\nneg = df_final[df_final['Target']==0]['text']","ef5b605c":"# Mask image\npos_mask = np.array(Image.open(\"\/kaggle\/input\/positive\/smile.png\"))\n\ntext_pos = \" \".join(i for i in pos)\n\n# Create a word cloud image\nwc_pos = WordCloud(background_color=\"white\", max_words=100, mask=pos_mask, contour_width=10, contour_color='firebrick')\n\n# Generate a wordcloud\nwc_pos.generate(text_pos)\n\nplt.figure(figsize=(14,7))\nplt.imshow(wc_pos, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","261f82f2":"# Mask image\nneg_mask = np.array(Image.open(\"\/kaggle\/input\/madness\/m.jpg\"))\n                    \n# Create a word cloud image\ntext_neg = \" \".join(i for i in neg)\n\n# Create a word cloud image\nwc_neg = WordCloud(background_color=\"white\", max_words=100, mask=neg_mask, contour_width=25, contour_color='firebrick', max_font_size=500)\n\n# Generate a wordcloud\nwc_neg.generate(text_neg)\n\nplt.figure(figsize=(20,10))\nplt.imshow(wc_neg, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","8a31f4ce":"import re \ndef function_clean(text):\n    text = re.sub(r\"http\\S+\", \"\", text) #removing the URL Http\n    # Removal of mentions\n    text = re.sub(\"@[^\\s]*\", \"\", text)\n    # Removal of hashtags\n    text = re.sub(\"#[^\\s]*\", \"\", text)\n    # Removal of numbers\n    text = re.sub('[0-9]*[+-:]*[0-9]+', '', text)\n    text = re.sub(\"'s\", \"\", text)   \n    return text","9d8e51c6":"# applying the cleaning function to text column\ndf_final['text'] = df_final['text'].apply(lambda text: function_clean(text))","0dfa3443":"#Splitting the data to train and test\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nX = df_final[\"text\"]\ny = df_final[\"Target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)","a1169686":"y.value_counts()","2a48392a":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","eb8249e9":"import string\n#nltk.download('stopwords')\nimport nltk\nfrom nltk.corpus import stopwords \nENGLISH_STOP_WORDS = stopwords.words('english')\n\ndef my_tokenizer(sentence):\n\n    listofwords = sentence.strip().split()          # to remove any space from beginning and the end of text\n    listof_words = []    \n    for word in listofwords:\n        if not word in ENGLISH_STOP_WORDS:\n            lemm_word = WordNetLemmatizer().lemmatize(word)\n            # remove the stop words\n            for punctuation_mark in string.punctuation:\n                word = word.replace(punctuation_mark, '').lower()\n            if len(word)>0:\n                listof_words.append(word)\n    return(listof_words)","98c92f52":"from sklearn.feature_extraction.text import TfidfVectorizer\nvect_1 = TfidfVectorizer(min_df=100,tokenizer=my_tokenizer, stop_words={'english'}, ngram_range=(1,3)).fit(X_train)\nX_train1 = vect_1.transform(X_train)\nX_test1 = vect_1.transform(X_test)","ff17d929":"new_df_words = pd.DataFrame(columns=vect_1.get_feature_names(), data=X_train1.toarray())\nnew_df_words","0aeb41ff":"#counting most repetitive words \nword_counts = np.array(np.sum(X_train1, axis=0)).reshape((-1,))\nwords = np.array(vect_1.get_feature_names())\nwords_df = pd.DataFrame({\"word\":words, \"count\":word_counts})\nwords_df.sort_values(by=\"count\",ascending=False).head(20)","88ec14a3":"from imblearn.over_sampling import SMOTE\n\n#SMOTE the training data\nsm = SMOTE(random_state=1)\nX_bal, y_bal = sm.fit_resample(X_train1, y_train)","3750c17e":"y_bal.value_counts()","35c3561e":"# fitting a logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Fitting Logistic regression to the training set\nlogreg = LogisticRegression(solver='lbfgs',multi_class='auto',random_state=1)\nlogreg.fit(X_bal, y_bal)\n\n# Predicting the test set results\ny_pred_logreg = logreg.predict(X_test1)\n\n# Training score\nprint(f\"Score on training set: {logreg.score(X_train1,y_train)}\")\nprint(f\"Score on test set: {logreg.score(X_test1,y_test)}\")","87977aad":"from sklearn.metrics import classification_report\nprint('The Confusion Matrix')\ncon_mat_lr = confusion_matrix(y_test, y_pred_logreg)\ndf_cm_lr = pd.DataFrame(con_mat_lr, columns = ['Predicted 0','Predicted 1'], index = ['True 0','True 1'])\ndisplay(df_cm_lr)\nprint('The Classification report')\nreport = classification_report(y_test, y_pred_logreg, output_dict=True)\ndf_report = pd.DataFrame(report).transpose()\ndf_report","79f9750e":"from sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\nestimators = [('classifier',LogisticRegression(solver='lbfgs',multi_class='auto',random_state=1))]\npipe = Pipeline(estimators)\n\nparams = {'classifier__C' : [10**j for j in range(-4,4)]}\ngrid_search = GridSearchCV(pipe, param_grid=params,cv=5)","9e645797":"fitted_search = grid_search.fit(X_bal, y_bal)","fda95829":"fitted_search.best_estimator_","7fe1be46":"logreg = LogisticRegression(C=10, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='auto', n_jobs=None,\n                                    penalty='l2', random_state=1,\n                                    solver='lbfgs', tol=0.0001, verbose=0,\n                                    warm_start=False)\nlogreg.fit(X_bal, y_bal)\n\n# Predicting the test set results\ny_pred_logreg = logreg.predict(X_test1)\n\n# Training score\nprint(f\"Score on training set: {logreg.score(X_train1,y_train)}\")\nprint(f\"Score on test set: {logreg.score(X_test1,y_test)}\")\n\n# Creating confusion matrix\/ dataFrame\ncon_mat_lr = confusion_matrix(y_test, y_pred_logreg)\ndf_cm_lr = pd.DataFrame(con_mat_lr, columns = ['Predicted 0','Predicted 1'], index = ['True 0','True 1'])\ndf_cm_lr","04ba762c":"from sklearn.ensemble import RandomForestClassifier\n\nmy_random_forest = RandomForestClassifier(random_state=1)\nmy_random_forest.fit(X_bal, y_bal)\nprint(f\"Random Forest: {my_random_forest.score(X_bal, y_bal)}\")\nprint(f\"Random Forest: {my_random_forest.score(X_test1, y_test)}\")","99dd09ee":"dt_train=[]\ndt_test=[]\ndepth=[]\nfor i in range(1,15):\n    my_random_forest = RandomForestClassifier(n_estimators=i,random_state=1)\n    my_random_forest.fit(X_bal, y_bal)\n    dt_train.append(my_random_forest.score(X_bal, y_bal))\n    dt_test.append(my_random_forest.score(X_test1, y_test))\n    depth.append(i)","4fbd5d3d":"plt.figure()\nplt.title('The test and train accuracy for different estimators')\nplt.plot(dt_train,color='red',label='Train Score')\nplt.plot(dt_test,label=' Test Score')\nplt.legend()\nplt.show()\nprint(depth[np.argmax(dt_test)])","6c4579a9":"my_random_forest = RandomForestClassifier(n_estimators=13,random_state=1)\nmy_random_forest.fit(X_bal, y_bal)\nprint(f\"Random Forest train: {my_random_forest.score(X_bal, y_bal)}\")\nprint(f\"Random Forest test: {my_random_forest.score(X_test1, y_test)}\")","6672b49d":"# Predicting the test set results\ny_pred_rand = my_random_forest.predict(X_test1)","05523686":"# Creating confusion matrix\/ dataFrame\ncon_mat_RF = confusion_matrix(y_test, y_pred_rand)\ndf_cm_RF = pd.DataFrame(con_mat_RF, columns = ['Predicted 0','Predicted 1'], index = ['True 0','True 1'])\ndf_cm_RF","f0343b32":"from xgboost import XGBClassifier\n\nXGB_model = XGBClassifier(random_state=1)\nXGB_model.fit(X_bal, y_bal)\n\nprint(f\"XG Boost train score: {XGB_model.score(X_bal, y_bal)}\")\nprint(f\"XG Boost test score: {XGB_model.score(X_test1, y_test)}\")","71606df6":"input_string = \"The waiting time was really long.\"\nprint(\"\\nPrediction on an input string: \" + input_string)\nprint(\"Logistic Regression model:\",logreg.predict(vect_1.transform([function_clean(input_string)])))\nprint(\"Random Forest model      :\",my_random_forest.predict(vect_1.transform([function_clean(input_string)])))\nprint(\"XGboost model            :\",XGB_model.predict(vect_1.transform([function_clean(input_string)])))","5aca1a26":"input_string = \" people that works here for sure is friendly! :)I do love that big menu book and seems like there are a lot of items to choose from. This is always nice as Vietnamese food is definitely more than pho and more spring rolls.\"\nprint(\"\\nPrediction on an input string: \" + input_string)\nprint(\"Logistic Regression model:\",logreg.predict(vect_1.transform([function_clean(input_string)])))\nprint(\"Random Forest model      :\",my_random_forest.predict(vect_1.transform([function_clean(input_string)])))\nprint(\"XGboost model            :\",XGB_model.predict(vect_1.transform([function_clean(input_string)])))","0d7db7fc":"* ## Negative review word cloud\n****Using word cloud to show the frequency of words in negative reviews or complains.****","859b4bd3":"# (Negative review):\n# \"The waiting time was really long.\"\n<img src=\"https:\/\/cdn.shopify.com\/s\/files\/1\/1061\/1924\/products\/Super_Angry_Face_Emoji_ios10_large.png?v=1571606092\" width=\"200px\" style=\"float:top\" > \n","d7c2a6ba":"# The stars distribution\n","e0a32eb0":"****Now we found the optimal *n_estimators * as 13, the both train and test accuracy are increased. ****","04b929ea":"## 1. Logistic Regression","8affcb3c":"# Modeling\n****As the sentiment-classifier system has many applications from business to social sciences,  the natural language processing and machine learning techniques are used to create the classifiers in order to explore the polarity of the reviews easily. Hence, we analyze the reviews given by the customers for the restaurant with the the implementation of various classification algorithms such as Logistic Regression, Random Forest(RF), XGboost.****","f87be263":"# Sentiment analysis of Yelp restaurant reviews","7210ae72":"* ## Positive review word cloud\n****we can idicate the frequency of occurrence of the word in the Positive reviews or compliments.\nWe can see some interesting words like \u201cquarantined\u201d in positive reviews.****","507de913":"# Let's Smote \n****The only challenge that we\u2019ve faced was about balancing the train dataset in terms of having the equal numbers of positive and negative reviews for our two classes. So we are using SMOTE to balanace our target(class) column.****","1065b5de":"****To find the optimal *n_estimators* parameter, we are going to try various number of *n_estimators* as shown below.****","2baf596d":"##  3. XGboost","d898a8eb":"# Hurray! Here we go. Hope you enjoyed and learned from our project:)","518a6aa3":"# Split the data into train and test set\n\n****In order to train the classifier, we need to devide the dataset into train and test datasets. So we are going to split the reviews by 80:20 for train and test data. Before splitting the data, we create the *function_clean* to clean up reviews.****","fb8274be":"# Testing\n**** In this stage, after creating the different classification models, now we can use them to predict the sentiment of the restaurant reviews.****","f4082c1e":"* ### Grid search (logistic)\n****Let's now try to optimize some hyperparameters. We will start off with the logistic regression by finding out the optimal value for parameter C by applying the Grid search.****","d55dc02a":"# Preprocessing the data\n****Now it is time to preprocess the reviews because all these modifications will directly affect the classifier\u2019s performance. As we are going to use words as features so we can use some text formatting techniques which will help us in feature extraction including removing punctuation marks\/digits ,and also stop-words. In addition, the implementation of lemmatization words using NLTK can be workable to maximize the performance. Tokenization is the last step to break reviews up into words and other meaningful tokens.****","c9b1d08e":"# Introduction\n\n****Restaurants owners always want to know how customers think about the quality of their services in order to improve their business. Yelp users may post their reviews and ratings on restaurants and simply express their thoughts on other reviews. Knowing the user's experiences on the variety of criteria such as food quality, service, ambience, discounts is very important for owners of the businesses.The goal of this project is to be able to automatically classify reviews as positive and negative in order to derive the attitude of a user about the specific restaurants.****","ee4296d4":"![](https:\/\/camo.githubusercontent.com\/ba46f53e744f8b0bb470b8740fce3a1990c151c4\/68747470733a2f2f63646e2d696d616765732d312e6d656469756d2e636f6d2f6d61782f3830302f312a45573776554834584c4353496957416d4a4c765638672e6a706567)\n\n<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\">\n<ul class=\"toc-item\">\n<li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Introduction<\/a><\/span><\/li>\n<li><span><a href=\"#Data\" data-toc-modified-id=\"Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Data<\/a><\/span><\/li>\n<li><span><a href=\"#The-stars-distribution\" data-toc-modified-id=\"The stars distribution-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>The stars distribution<\/a><\/span><\/li>\n<li><span><a href=\"#Word-Cloud\" data-toc-modified-id=\"Word Cloud-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Word Cloud<\/a><\/span>\n<ul>\n    <li><span><a href=\"#Positive-review-word-cloud\" data-toc-modified-id=\"Positive review word cloud\"> <span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Positive review word cloud<\/a><\/span><\/li>\n    <li><span><a href=\"#Negative-review-word-cloud\" data-toc-modified-id=\"Negative review word cloud\"> <span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Negative review word cloud<\/a><\/span><\/li>\n    <\/ul><\/li>\n    <li><span><a href=\"#Split-the-data-into-train-and-test-set\" data-toc-modified-id=\"Split the data into train and test set-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Split the data into train and test set<\/a><\/span><\/li>\n    <li><span><a href=\"#Preprocessing-the-data\" data-toc-modified-id=\"Preprocessing the data-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Preprocessing the data<\/a><\/span><\/li>\n<li><span><a href=\"#Vectorizing-the-text-using-TF_IDF\" data-toc-modified-id=\"Vectorizing the text using TF_IDF-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Vectorizing the text using TF_IDF<\/a><\/span><\/li>\n\n<li><span><a href=\"#Let's-Smote\" data-toc-modified-id=\"Let's Smote-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Let's Smote<\/a><\/span><\/li>\n<li><span><a href=\"#Modeling\" data-toc-modified-id=\" Modeling-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;<\/span>Modeling<\/a><\/span>\n<ul>\n   <li><span><a href=\"#1.-Logistic-Regression\" data-toc-modified-id=\"1. Logistic Regression\"><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>1. Logistic Regression<\/a><\/span>\n       <ul>\n           <li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model Evaluation\"> <span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>Model Evaluation<\/a><\/span><\/li><\/ul>\n   <li><span><a href=\"#2.-Random-Forest\" data-toc-modified-id=\"2. Random Forest\"><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>2. Random Forest<\/a><\/span><\/li>\n   <li><span><a href=\"#3.-XGboost\" data-toc-modified-id=\"3. XGboost\"><span class=\"toc-item-num\">&nbsp;&nbsp;<\/span>3. XGboost<\/a>    <\/span><\/li>\n<\/ul><\/li>\n<li><span><a href=\"#Testing\" data-toc-modified-id=\"Testing-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;<\/span>Testing<\/a><\/span><\/li>\n <\/ul><\/div>","833243e4":"****After grid search we found our optimal parameter and applied in the our model. As a result, the train and test accuracy slightly are increased.****","f4a42bad":"## 2. Random Forest","a0bc87ad":"# Word Cloud\n****As a word cloud is a graphical representation of frequently used words in the texts so we are going to investigate the frequency of words in the reviews.****","57b3efa2":"# Data\n****Yelp Restaurant review dataset will be used to do the sentiment classification using TF-IDF model. Yelp dataset includes five tables. We have used the business and review tables to create another small table that contains information of Toronto restaurants' reviews to ease the computaional process.  ****","95667adf":"#### ****As a result, among the all three models, the Logistic Regression gave us the highest test accuracy score as 0.89 and after that the XGboost with 0.88.****","97aea2b0":"# (Positive review)\n# \"people that works here for sure is friendly! :) I do love that big menu.\"\n   <img src=\"https:\/\/i.pinimg.com\/564x\/ce\/6c\/3c\/ce6c3c20878b5c690fe3916c62d9195b.jpg\" width=\"240px\" style=\"float:center\" > ","b8faa6df":"****In this section, we classify our reviews rating into two classes of 0 (Negative for rating less than 3) and 1(Positive for rating >= 3) by creating new column as the \"Target\" for further processing.****","e00bc37c":"# Vectorizing the text using TF_IDF\n\n****By implementing the sklearn library, we can use TF_IDF vectorizing to find the weighted words that occur more frequently in the document that leads to creation of the bag of words model. So our features will be the words or sequence of words of these reviews. We are going to explore different models with the combinations of n_grams (unigrams,bigrams,trigrams).****","9fe6af11":"## Model Evaluation\n**** As shown above, our model test accuracy is 0.89. In order to evaluate the model further, we can extract some important evaluation metrics from the classification report such as precision, recall, and f1_score. As shown in the following report, as the f1_score is 0.92 for class 1 and 0.76 for class 0, therefore we can conclude this model is better in predicting positive reviews than negative ones.**** "}}