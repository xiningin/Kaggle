{"cell_type":{"0bf9f6fb":"code","f8c0c3d5":"code","e2a28416":"code","58527df4":"code","3d768cc3":"code","668eaaa0":"code","5e93011f":"code","4454a970":"code","bbbbf620":"code","97503722":"code","5762d4b5":"code","2bbb5067":"code","da6f42bf":"code","b4dbe87d":"code","ca08e79b":"code","66a869ad":"code","8d5a6eb8":"code","d79f60ab":"code","b1e9c54b":"code","70961c4d":"code","db615f7d":"code","a8a3742b":"code","6d7cc4c1":"code","e617ff55":"code","b77bbccb":"code","7bacc6bd":"code","74ddc224":"code","faca4578":"code","794671cf":"code","0f2ff3ee":"code","f690977c":"code","b515b53a":"code","b50c6b3c":"code","ab9a203d":"code","412972a7":"code","9448501c":"code","4484f47e":"code","0f7a7ae3":"markdown","4994ed82":"markdown","22ddaee9":"markdown","c2962d24":"markdown","968a2a2d":"markdown","d18b4e5d":"markdown","07d13baa":"markdown","5b8e29ed":"markdown","08d0b04b":"markdown","1aaddc29":"markdown","24a9288b":"markdown","adde80f9":"markdown","19255b04":"markdown","135e1468":"markdown","a5a4b928":"markdown","148be418":"markdown","0e6828aa":"markdown","5ef7dbea":"markdown","aa251033":"markdown","a9011c30":"markdown","c8e7b2b6":"markdown","62edbb11":"markdown","a31f7f6c":"markdown","6d049bb3":"markdown","fc558ec1":"markdown","9e01a578":"markdown","fadebaf9":"markdown","9ebd47e6":"markdown"},"source":{"0bf9f6fb":"# constants \/\/ data\nMIN_CASES_FIT = 1e-5\n\"\"\"threshold to start modeling, as confirmed cases over total population\"\"\"\nMIN_DAYS_FIT = 45\n\"\"\"total days of valid data needed for training\/modeling\"\"\"\nDAYS_TEST = 10\n\"\"\"number of recent days held out for prediction\/testing\"\"\"\nMAX_MISSING_DATA = 0.15\n\"\"\"acceptable portion of missing input data\"\"\"\nDENOM_SZ = 1000\n\"\"\"denominater size when computing rmse, proportion active cases, etc\"\"\"\n\n# constants \/\/ sir-poly\nSIR_POLY_BETA_A_MIN = -10\n\"\"\"lower bound on first order coefficient of sir-poly\"\"\"\n\n# constants \/\/ ML\nRANDOM_SEED = 1234\n\"\"\"fix seed for random number generator for reproducible results\"\"\"\nY_WIN_SZ = 15\n\"\"\"window size when computing smoothed contact rate\"\"\"\nMOBILITY_WIN_SZ = 21\n\"\"\"windows size for rolling mean when replacing missing values\"\"\"\nCROSS_FOLDS = 5\n\"\"\"number of folds to use during cross validation\"\"\"\nPARAMETERS_SVM = [\n    {\n        'kernel': ['rbf'],\n        'C': [0.01, 0.1, 1.0, 10.0],\n        'gamma': ['scale'],\n        'epsilon': [0.001, 0.005, 0.01, 0.1]\n    }\n]\n\"\"\"SVM parameter options to be tried during grid search\"\"\"\nPARAMETERS_RANDOMFOREST = [\n    {\n        'n_estimators': [100, 500, 1000],\n        'criterion': ['mse'],# 'mae'],\n        'min_samples_leaf': [100, 200, 500],\n        'max_features': ['auto', 'sqrt']\n    }\n]\n\"\"\"Random forest options to be tried during grid search\"\"\"\npass","f8c0c3d5":"# lib\nimport os\nimport numpy as np\nimport pandas as pd\nfrom scipy.integrate import odeint\nfrom scipy.optimize import curve_fit\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure, ColumnDataSource\nfrom bokeh.models import BoxAnnotation, Span, HoverTool, CrosshairTool\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\n\n# check input files\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n# inline notebook viz\noutput_notebook()\n\n# set random seed\nnp.random.seed(RANDOM_SEED)","e2a28416":"# load raw datasets\nconfirmed = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/time_series_covid_19_confirmed.csv')\nrecovered = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/time_series_covid_19_recovered.csv')\ndeaths = pd.read_csv('\/kaggle\/input\/novel-corona-virus-2019-dataset\/time_series_covid_19_deaths.csv')\ncountries = pd.read_csv('\/kaggle\/input\/countries-of-the-world\/countries of the world.csv', decimal=',')\nmobility = pd.read_csv('\/kaggle\/input\/covid19-mobility-data\/Global_Mobility_Report.csv')\ntesting = pd.read_csv('\/kaggle\/input\/covid19-owid-data\/owid-covid-data.csv')\n\n# collapse counts by country\nconfirmed.drop(columns=['Province\/State', 'Lat', 'Long'], inplace=True)\nconfirmed = confirmed.groupby('Country\/Region').sum()\n\nrecovered.drop(columns=['Province\/State', 'Lat', 'Long'], inplace=True)\nrecovered = recovered.groupby('Country\/Region').sum()\n\ndeaths.drop(columns=['Province\/State', 'Lat', 'Long'], inplace=True)\ndeaths = deaths.groupby('Country\/Region').sum()\n\n# flip for more traditional dataframe format\nconfirmed = confirmed.transpose()\nconfirmed.index.rename('Date', inplace=True)\nconfirmed.columns.rename('', inplace=True)\n\nrecovered = recovered.transpose()\nrecovered.index.rename('Date', inplace=True)\nrecovered.columns.rename('', inplace=True)\n\ndeaths = deaths.transpose()\ndeaths.index.rename('Date', inplace=True)\ndeaths.columns.rename('', inplace=True)\n\n# normalize dates\nconfirmed.index = pd.to_datetime(confirmed.index)\nrecovered.index = pd.to_datetime(recovered.index)\ndeaths.index = pd.to_datetime(deaths.index)\n\n# reindex country data\ncountries['Country'] = countries['Country'].str.strip()\ncountries.set_index('Country', inplace=True)\n\n# map any missing country names we can\ncountries.rename(\n    index={\n        'Antigua & Barbuda': 'Antigua and Barbuda',\n        'Bahamas, The': 'Bahamas',\n        'Bosnia & Herzegovina': 'Bosnia and Herzegovina',\n        'Cape Verde': 'Cabo Verde',\n        'Central African Rep.': 'Central African Republic',\n        'Czech Republic': 'Czechia',\n        'Taiwan': 'Taiwan*',\n        'Trinidad & Tobago': 'Trinidad and Tobago',\n        # TODO \/\/ more here...\n        'United States': 'US'\n    },\n    inplace=True\n)\n\n# validate matches between datasets\ncount = 0\nfor name in confirmed:\n    if name not in countries.index:\n        count += 1\n        #print('Missing country data on: {}'.format(name))\nprint('Missing country data for {} countries'.format(count))\n        \n# only want country-level data\nmobility = mobility[mobility[\"sub_region_1\"].isnull()]\nmobility.drop(columns=['country_region_code', 'sub_region_1', 'sub_region_2'], inplace=True)\n\n# more usable names\nmobility.rename(\n    columns={\n        'retail_and_recreation_percent_change_from_baseline': 'retail',\n        'grocery_and_pharmacy_percent_change_from_baseline': 'grocery',\n        'parks_percent_change_from_baseline': 'parks',\n        'transit_stations_percent_change_from_baseline': 'transit',\n        'workplaces_percent_change_from_baseline': 'work',\n        'residential_percent_change_from_baseline': 'residential'\n    },\n    inplace=True\n)\n\n# reindex mobility data\nmobility['date'] = pd.to_datetime(mobility['date'])\nmobility.set_index(['country_region', 'date'], inplace=True)\n\n# map any missing mobility country names we can\nmobility.rename(\n    index={\n        'The Bahamas': 'Bahamas',\n        \"C\u00f4te d'Ivoire\": \"Cote d'Ivoire\",\n        'South Korea': 'Korea, South',\n        'Taiwan': 'Taiwan*',\n        # TODO \/\/ more here...\n        'United States': 'US'\n    },\n    inplace=True\n)\n\ncount = 0\nmobility_country_names = set(mobility.index.get_level_values('country_region'))\nfor name in confirmed :\n    if name not in mobility_country_names:\n        count += 1\n        #print('Missing mobility data on: {}'.format(name))\nprint('Missing mobility data for {} countries'.format(count))\n\n# reindex testing data\ntesting['date'] = pd.to_datetime(testing['date'])\ntesting.drop(columns=['total_cases', 'new_cases', 'total_deaths', 'new_deaths'], inplace=True)\ntesting.set_index(['location', 'date'], inplace=True)\n\n# map any missing mobility country names we can\ntesting.rename(\n    index={\n        'Cape Verde': 'Cabo Verde',\n        'Czech Republic': 'Czechia',\n        'South Korea': 'Korea, South',\n        'Taiwan': 'Taiwan*',\n        # TODO \/\/ more here...\n        'United States': 'US'\n    },\n    inplace=True\n)\n\ncount = 0\ntesting_country_names = set(testing.index.get_level_values('location'))\nfor name in confirmed :\n    if name not in testing_country_names:\n        count += 1\n        #print('Missing testing data on: {}'.format(name))\nprint('Missing testing data for {} countries'.format(count))","58527df4":"# preview confirmed cases data\nconfirmed.tail()","3d768cc3":"# preview country data\ncountries.head()","668eaaa0":"# preview mobility data\nmobility.head()","5e93011f":"# preview testing data\ntesting.head()","4454a970":"# examine data sample\nname = 'Italy'\nidx = confirmed[name] > MIN_CASES_FIT * countries.loc[name]['Population']\nsample_confirmed = confirmed[name][idx]\nsample_recovered = recovered[name][idx]\nsample_deaths = deaths[name][idx]\n\n# plot\np = figure(x_axis_type=\"datetime\", title=name, plot_height=400, plot_width=800)\np.xaxis.axis_label = 'Time'\np.yaxis.axis_label = 'Count'\np.line(sample_confirmed.index, sample_confirmed, color='blue', legend_label='confirmed')\np.line(sample_recovered.index, sample_recovered, color='green', legend_label='recovered')\np.line(sample_deaths.index, sample_deaths, color='red', legend_label='deaths')\np.legend.location = \"top_left\"\nshow(p)\n\n# country data\ncountries.loc[name]","bbbbf620":"data = {}\n\nfor name in confirmed:\n    if name not in mobility_country_names:\n        continue\n    if name not in testing_country_names:\n        continue\n    if name not in countries.index:\n        continue\n\n    # select data of interest\n    pop = countries.loc[name]['Population']\n    min_cases = MIN_CASES_FIT * pop\n    idx = confirmed[name] > min_cases\n    idx_dt = confirmed[idx].index\n    \n    # check for sufficient input data\n    idx_overlap = idx_dt.intersection(mobility.loc[name].index)\n    mob_missing_count = mobility.loc[name].loc[idx_overlap]['retail'].isnull().sum()\n    if mob_missing_count > len(idx) * MAX_MISSING_DATA:\n        #print('Insufficient mobility data {} {}'.format(name, mob_missing_count))\n        continue\n        \n    idx_overlap = idx_dt.intersection(testing.loc[name].index)\n    test_missing_count = testing.loc[name].loc[idx_overlap]['new_tests_smoothed'].isnull().sum()\n    if test_missing_count > len(idx) * MAX_MISSING_DATA:\n        #print('Insufficient testing data {} {}'.format(name, test_missing_count))\n        continue\n    \n    # compute SIR values\n    s = pop - confirmed[name][idx]  # susceptible\n    r = recovered[name][idx] + deaths[name][idx]  # recovered\n    i = confirmed[name][idx] - r  # infected\n    t = range(len(s))\n    \n    if len(s) < MIN_DAYS_FIT:\n        continue\n    \n    # format SIR data as numpy array\n    sir = np.zeros((len(s), 3))\n    sir[:, 0] = s\n    sir[:, 1] = i\n    sir[:, 2] = r\n                        \n    # store by country\n    data[name] = {\n        'sir': sir,\n        'dt': s.index\n    }\n\nprint('Prepared SIR data for {} countries'.format(len(data)))","97503722":"# differential equations for SIR\ndef deriv_sir(y, t, n, beta, gamma):\n    s, i, r = y\n    dsdt = -beta * s * i \/ n\n    didt = beta * s * i \/ n - gamma * i\n    drdt = gamma * i\n    return dsdt, didt, drdt\n    \n# compute SIR given candidate beta and gamma\ndef gen_compute_sir(s0, i0, r0, n):\n    def compute_sir(t, beta, gamma):\n        init = (s0, i0, r0)\n        pop = n\n        res = odeint(deriv_sir, init, t, args=(pop, beta, gamma))\n        return res.flatten()\n    return compute_sir","5762d4b5":"# helper function to visualize SIR\ndef plot_sir(\n    name=None, t=None, s=None, i=None, r=None,\n    t_fit=None, s_fit=None, i_fit=None, r_fit=None,\n    t_pred=None, s_pred=None, i_pred=None, r_pred=None\n):\n    p = figure(title=name, x_axis_type=\"datetime\", plot_height=400, plot_width=800)\n\n    # plot actual data\n    src = ColumnDataSource(data={\n        'dt': t,\n        's': s if s is not None else np.full(len(t), np.nan),\n        'i': i if i is not None else np.full(len(t), np.nan),\n        'r': r if r is not None else np.full(len(t), np.nan)\n    })\n    if s is not None:\n        ls = p.line('dt', 's', source=src, color='blue', alpha=0.3, legend_label='susceptible (actual)')\n        p.add_tools(HoverTool(renderers=[ls], tooltips=[('S', '@s{0,0}')], mode='vline'))\n    if i is not None:\n        li = p.line('dt', 'i', source=src, color='red', alpha=0.3, legend_label='infected (actual)')\n        p.add_tools(HoverTool(renderers=[li], tooltips=[('I', '@i{0,0}')], mode='vline'))\n    if r is not None:\n        lr = p.line('dt', 'r', source=src, color='green', alpha=0.3, legend_label='recovered (actual)')\n        p.add_tools(HoverTool(renderers=[lr], tooltips=[('R', '@r{0,0}')], mode='vline'))\n        \n    # plot fit data\n    if t_fit is not None:\n        src_fit = ColumnDataSource(data={\n            'dt': t_fit,\n            's': s_fit if s_fit is not None else np.full(len(t_fit), np.nan),\n            'i': i_fit if i_fit is not None else np.full(len(t_fit), np.nan),\n            'r': r_fit if r_fit is not None else np.full(len(t_fit), np.nan)\n        })\n        if s_fit is not None:\n            lsf = p.line('dt', 's', source=src_fit, color='blue', legend_label='susceptible (fit)')\n            p.add_tools(HoverTool(renderers=[lsf], tooltips=[('S (fit)', '@s{0,0}')], mode='vline'))\n        if i_fit is not None:\n            lif = p.line('dt', 'i', source=src_fit, color='red', legend_label='infected (fit)')\n            p.add_tools(HoverTool(renderers=[lif], tooltips=[('I (fit)', '@i{0,0}')], mode='vline'))\n        if r_fit is not None:\n            lrf = p.line('dt', 'r', source=src_fit, color='green', legend_label='recovered (fit)')\n            p.add_tools(HoverTool(renderers=[lrf], tooltips=[('R (fit)', '@r{0,0}')], mode='vline'))\n            \n    # plot prediction data\n    if t_pred is not None:\n        src_pred = ColumnDataSource(data={\n            'dt': t_pred,\n            's': s_pred if s_pred is not None else np.full(len(t_pred), np.nan),\n            'i': i_pred if i_pred is not None else np.full(len(t_pred), np.nan),\n            'r': r_pred if r_pred is not None else np.full(len(t_pred), np.nan)\n        })\n        if s_pred is not None:\n            lsp = p.line('dt', 's', source=src_pred, color='blue', line_dash=[4, 4], legend_label='susceptible (predicted)')\n            p.add_tools(HoverTool(renderers=[lsp], tooltips=[('S (pred)', '@s{0,0}')], mode='vline'))\n        if i_pred is not None:\n            lip = p.line('dt', 'i', source=src_pred, color='red', line_dash=[4, 4], legend_label='infected (predicted)')\n            p.add_tools(HoverTool(renderers=[lip], tooltips=[('I (pred)', '@i{0,0}')], mode='vline'))\n        if r_pred is not None:\n            lrp = p.line('dt', 'r', source=src_pred, color='green', line_dash=[4, 4], legend_label='recovered (predicted)')\n            p.add_tools(HoverTool(renderers=[lrp], tooltips=[('R (pred)', '@r{0,0}')], mode='vline'))\n\n    # additional annotation\n    p.add_tools(CrosshairTool(dimensions='height', line_alpha=0.3))\n    p.legend.location = \"top_left\"\n    if t_fit is not None and t_pred is not None:\n        t_cutoff = t_fit[-1] + (t_pred[0] - t_fit[-1]) \/ 2.0\n        p.add_layout(\n            Span(location=t_cutoff, dimension='height', line_color='gray', line_dash='dotted')\n        )\n    show(p)","2bbb5067":"# test with dummy values\nt = range(100)\nres = odeint(deriv_sir, (9990, 10, 0), t, args=(10000, 0.4, 0.2))\n\n# plot\nplot_sir(name='Dummy SIR', t=t, s=res[:,0], i=res[:,1], r=res[:,2])","da6f42bf":"for name in ['Italy', 'Japan', 'US']:\n    sir = data[name]['sir']\n    dt = data[name]['dt']\n    s0, i0, r0 = sir[0, :]\n    pop = countries.loc[name]['Population']\n    t = range(sir.shape[0])\n\n    # fit SIR model\n    fx = gen_compute_sir(s0, i0, r0, pop)\n    opt_params, opt_cov = curve_fit(fx, t, sir.flatten(), bounds=(0, 1))\n    beta, gamma = opt_params\n    \n    print('{} (beta: {:.4f}, gamma: {:.4f})'.format(name, beta, gamma))\n   \n    # integrate\n    fit = odeint(deriv_sir, (s0, i0, r0), t, args=(pop, beta, gamma))\n    \n    # plot\n    plot_sir(\n        t=dt, i=sir[:, 1], r=sir[:, 2],\n        t_fit=dt, i_fit=fit[:, 1], r_fit=fit[:, 2]\n    )","b4dbe87d":"# differential equations for SIR w\/ polynomial contact rate\ndef deriv_sir_poly(y, t, n, beta_a, beta_b, gamma):\n    s, i, r = y\n    \n    # compute polynomial contact rate\n    x = i \/ n * DENOM_SZ  # (1000)\n    beta = beta_a * x + beta_b\n\n    dsdt = -beta * s * i \/ n\n    didt = beta * s * i \/ n - gamma * i\n    drdt = gamma * i\n    return dsdt, didt, drdt\n    \n# compute SIR w\/ polynomial contact given candidate values\ndef gen_compute_sir_poly(s0, i0, r0, n):\n    def compute_sir_poly(t, beta_a, beta_b, gamma):\n        init = (s0, i0, r0)\n        pop = n\n        res = odeint(deriv_sir_poly, init, t, args=(pop, beta_a, beta_b, gamma))\n        return res.flatten()\n    return compute_sir_poly","ca08e79b":"for name in ['Italy', 'Japan', 'US']:\n    # select data of interest\n    sir = data[name]['sir']\n    dt = data[name]['dt']\n    s0, i0, r0 = sir[0, :]\n    pop = countries.loc[name]['Population']\n    t = range(sir.shape[0])\n        \n    # fit SIR model\n    fx = gen_compute_sir_poly(s0, i0, r0, pop)\n    opt_params, opt_cov = curve_fit(\n        fx,\n        t,\n        sir.flatten(),\n        bounds=([SIR_POLY_BETA_A_MIN, 0, 0], [0, 1, 1])\n    )\n    beta_a, beta_b, gamma = opt_params\n    \n    print('{} (beta A: {:.4f}, beta B: {:.4f}, gamma: {:.4f})'.format(\n        name, beta_a, beta_b, gamma\n    ))\n        \n    # integrate\n    fit = odeint(\n        deriv_sir_poly,\n        (s0, i0, r0),\n        t,\n        args=(pop, beta_a, beta_b, gamma)\n    )\n    \n    # plot\n    plot_sir(\n        t=dt, i=sir[:, 1], r=sir[:, 2],\n        t_fit=dt, i_fit=fit[:, 1], r_fit=fit[:, 2]\n    )","66a869ad":"# setup\ndebug_countries = ['Italy', 'Japan', 'US']\nrmse_normalized = []\nfor name in data:\n    sir = data[name]['sir']\n    dt = data[name]['dt']\n    pop = countries.loc[name]['Population']\n    \n    # get training split\n    train_sz = int(sir.shape[0] - DAYS_TEST)\n    train_t = range(train_sz)\n    train_data = sir[:train_sz]\n    \n    # fit SIR model against training split\n    s0, i0, r0 = sir[0, :]\n    fx = gen_compute_sir_poly(s0, i0, r0, pop)\n    opt_params, opt_cov = curve_fit(\n        fx,\n        train_t,\n        train_data.flatten(),\n        bounds=([SIR_POLY_BETA_A_MIN, 0, 0], [0, 1, 1])\n    )\n    beta_a, beta_b, gamma = opt_params\n        \n    # integrate across train split\n    fit = odeint(\n        deriv_sir_poly,\n        (s0, i0, r0),\n        train_t,\n        args=(pop, beta_a, beta_b, gamma)\n    )\n\n    # integrate across test split\n    test_t = range(train_sz, sir.shape[0])\n    predicted = odeint(\n        deriv_sir_poly,\n        (sir[train_sz, 0], sir[train_sz, 1], sir[train_sz, 2]),\n        test_t,\n        args=(pop, beta_a, beta_b, gamma)\n    )\n    \n    # score RMSE against normalized values\n    test_data = sir[train_sz:]\n    rmse = mean_squared_error(test_data, predicted, squared=False)\n    rmse_normalized.append(rmse \/ pop * DENOM_SZ)\n    \n    if name not in debug_countries:\n        continue\n        \n    print('{}'.format(name))\n    print('beta A: {:.4f}, beta B: {:.4f}, gamma: {:.4f}'.format(\n        beta_a, beta_b, gamma\n    ))\n    print('RMSE: {:.2f}, RMSE per {}: {:.6f}'.format(\n        rmse, DENOM_SZ, rmse \/ pop * DENOM_SZ\n    ))\n    \n    # plot\n    plot_sir(\n        t=dt, i=sir[:, 1], r=sir[:, 2],\n        t_fit=dt[:train_sz], i_fit=fit[:, 1], r_fit=fit[:, 2],\n        t_pred=dt[train_sz:], i_pred=predicted[:, 1], r_pred=predicted[:, 2]\n    )","8d5a6eb8":"# compute statistics on normalized RMSE across all countries\nsir_poly_ind_error_mean = np.mean(rmse_normalized)\nsir_poly_ind_error_med = np.median(rmse_normalized)\nsir_poly_ind_error_std = np.std(rmse_normalized)\n\nprint('SIR-poly individual country model RMSE per-{}'.format(DENOM_SZ))\nprint('  countries: {}'.format(len(rmse_normalized)))\nprint('  mean: {:.6f}'.format(sir_poly_ind_error_mean))\nprint('  median: {:.6f}'.format(sir_poly_ind_error_med))\nprint('  std: {:.6f}'.format(sir_poly_ind_error_std))","d79f60ab":"# aggregate train and test datasets\ntrain_splits = {}\ntrain_dt = {}\ntest_splits = {}\ntest_dt = {}\nfor name in sorted(data):\n    sir = data[name]['sir']\n    dt = data[name]['dt']    \n    \n    # split training\/test\n    train_sz = int(sir.shape[0] - DAYS_TEST)\n    train_splits[name] = sir[:train_sz]\n    train_dt[name] = dt[:train_sz]\n    test_splits[name] = sir[train_sz:]\n    test_dt[name] = dt[train_sz:]\n\n# stack all training data\ntrain_combined = np.concatenate(\n    [train_splits[c] for c in sorted(train_splits)],\n    axis=0\n)\n\n# define our function to be optimized\ndef compute_sir_poly_all(_, beta_a, beta_b, gamma):\n    results = []\n    for name in sorted(train_splits):\n        init = train_splits[name][0, :]\n        pop = countries.loc[name]['Population']\n        t = range(train_splits[name].shape[0])\n        res = odeint(deriv_sir_poly, init, t, args=(pop, beta_a, beta_b, gamma))\n        results.append(res)\n    results = np.concatenate(results, axis=0)\n    return results.flatten()\n\n# fit unified SIR-poly model\nopt_params, opt_cov = curve_fit(\n    compute_sir_poly_all,\n    [],\n    train_combined.flatten(),\n    bounds=([SIR_POLY_BETA_A_MIN, 0, 0], [0, 1, 1])\n)\nbeta_a, beta_b, gamma = opt_params\nprint('Unified model')\nprint('beta A: {:.4f}, beta B: {:.4f}, gamma: {:.4f}'.format(\n    beta_a, beta_b, gamma\n))","b1e9c54b":"rmse_normalized = []\nfor name in test_splits:\n    pop = countries.loc[name]['Population']\n\n    # integrate across test split\n    test_t = range(test_splits[name].shape[0])\n    predicted = odeint(\n        deriv_sir_poly,\n        test_splits[name][0, :],\n        test_t,\n        args=(pop, beta_a, beta_b, gamma)\n    )\n    \n    # score RMSE against normalized values\n    rmse = mean_squared_error(test_splits[name], predicted, squared=False)\n    rmse_normalized.append(rmse \/ pop * DENOM_SZ)\n    \n    if name not in debug_countries:\n        continue\n        \n    # integrate across train split\n    train_t = range(train_splits[name].shape[0])\n    fit = odeint(\n        deriv_sir_poly,\n        train_splits[name][0, :],\n        train_t,\n        args=(pop, beta_a, beta_b, gamma)\n    )\n        \n    print('{}'.format(name))\n    print('RMSE: {:.2f}, RMSE per {}: {:.6f}'.format(\n        rmse, DENOM_SZ, rmse \/ pop * DENOM_SZ\n    ))\n    \n    # plot\n    plot_sir(\n        t=data[name]['dt'], i=data[name]['sir'][:, 1], r=data[name]['sir'][:, 2],\n        t_fit=train_dt[name], i_fit=fit[:, 1], r_fit=fit[:, 2],\n        t_pred=test_dt[name], i_pred=predicted[:, 1], r_pred=predicted[:, 2]\n    )","70961c4d":"# compute statistics on RMSE per-capita across all countries\nsir_poly_unified_error_mean = np.mean(rmse_normalized)\nsir_poly_unified_error_med = np.median(rmse_normalized)\nsir_poly_unified_error_std = np.std(rmse_normalized)\n\nprint('SIR-poly unified model RMSE per-{}'.format(DENOM_SZ))\nprint('  countries: {}'.format(len(rmse_normalized)))\nprint('  mean: {:.6f}'.format(sir_poly_unified_error_mean))\nprint('  median: {:.6f}'.format(sir_poly_unified_error_med))\nprint('  std: {:.6f}'.format(sir_poly_unified_error_std))","db615f7d":"for name in data:\n    idx = data[name]['dt'][:-1]\n    pop = countries.loc[name]['Population']\n    \n    # new daily cases\n    confirmed_delta = confirmed[name].shift(-1) - confirmed[name]\n    confirmed_delta = confirmed_delta.loc[idx]\n    \n    # S * I\n    denom = data[name]['sir'][:len(idx), 0] * data[name]['sir'][:len(idx), 1]\n    \n    if any(denom == 0):\n        print('skipping {}, small counts causing divide-by-zero'.format(name))\n        continue\n    \n    # growth rate\n    y_raw = pop * confirmed_delta \/ denom\n    data[name]['y_raw'] = y_raw\n\n# preview data\npreview_name = 'Italy'\nsrc = ColumnDataSource(data={\n    'dt': data[preview_name]['dt'][:-1],\n    'y_raw': data[preview_name]['y_raw']\n})\np = figure(title=preview_name, x_axis_type='datetime', plot_height=400, plot_width=800)\np.line('dt', 'y_raw', source=src, legend_label='Contact rate')\nhover = HoverTool(\n    tooltips=[('date', '@dt{%F}'), ('contact rate', '@y_raw{0.0000}' )],\n    formatters={'@dt': 'datetime'},\n    mode='vline'\n)\np.add_tools(hover)\nshow(p)","a8a3742b":"for name in data:\n    if 'y_raw' not in data[name]:\n        continue\n    data[name]['y'] = data[name]['y_raw'].rolling(window=Y_WIN_SZ, min_periods=1, center=True).mean()\n    \n# preview data\nsrc = ColumnDataSource(data={\n    'dt': data[preview_name]['dt'][:-1],\n    'y_raw': data[preview_name]['y_raw'],\n    'y': data[preview_name]['y']\n})\np = figure(title=preview_name, x_axis_type='datetime', plot_height=400, plot_width=800)\np.line('dt', 'y_raw', source=src, legend_label='Contact rate', alpha=0.3)\np.line('dt', 'y', source=src, legend_label='Contact rate smoothed', name='y')\nhover = HoverTool(\n    tooltips=[('date', '@dt{%F}'), ('raw', '@y_raw{0.0000}' ), ('smoothed', '@y{0.0000}' )],\n    formatters={'@dt': 'datetime'},\n    mode='vline', names=['y']\n)\np.add_tools(hover)\nshow(p)","6d7cc4c1":"# clean data\ncols_of_interest = [\n    'Pop. Density (per sq. mi.)',\n    'Coastline (coast\/area ratio)',\n    'Net migration',\n    'Infant mortality (per 1000 births)',\n    'GDP ($ per capita)',\n    'Literacy (%)',\n    'Phones (per 1000)',\n    'Arable (%)',\n    'Crops (%)',\n    'Birthrate',\n    'Deathrate',\n    'Agriculture',\n    'Industry',\n    'Service'\n]\ncountries_cleaned = countries[cols_of_interest].copy()\nfor col in countries_cleaned:\n    median = countries_cleaned[col].median(skipna=True)\n    count_nan = countries_cleaned[col].isnull().sum()\n    countries_cleaned[col].fillna(value=median, inplace=True)\n    print('{}, missing count: {}, filled with median value: {}'.format(col, count_nan, median))\n\ncountries_cleaned.head()","e617ff55":"# pca\npca = PCA(n_components=2, whiten=True)\npca.fit(countries_cleaned)\n\n# want this to sum close to 1.0\nprint('explained variance ratio by component: {}'.format(\n    pca.explained_variance_ratio_\n))","b77bbccb":"# transform countries into their embeddings\ncountry_embeddings = pca.transform(countries_cleaned)\ncountry_embeddings = pd.DataFrame(\n    data=country_embeddings,\n    index=countries_cleaned.index,\n    columns=['C0', 'C1']\n)\n\nfor name in data:\n    data[name]['static'] = country_embeddings.loc[name]\n\ncountry_embeddings.head()","7bacc6bd":"# plot\np = figure(\n    title='Country embeddings', plot_height=600, plot_width=800,\n    tooltips=[('', '@Country (@C0, @C1)')],\n    active_scroll='wheel_zoom'\n)\nsrc = ColumnDataSource(country_embeddings)\np.scatter(x='C0', y='C1', source=src, size=10, fill_alpha=0.3)\nshow(p)","74ddc224":"for name in data:\n    # setup null dataframe for desired input data range\n    idx = data[name]['dt'][:-1]\n    x_mob = pd.DataFrame(index=idx, columns=mobility.columns)\n    \n    # fill in where available\n    mob_sub = mobility.loc[name]\n    idx_intersect = idx.intersection(mob_sub.index)\n    x_mob.loc[idx_intersect] = mob_sub.loc[idx_intersect]\n    \n    # clean up nan with rolling mean\n    x_mob.fillna(x_mob.rolling(MOBILITY_WIN_SZ, min_periods=1, center=True).mean(), inplace=True)\n    #x_mob = x_mob.rolling(MOBILITY_WIN_SZ, min_periods=1, center=True).mean()\n\n    # set any remaining nan to 0\n    x_mob.fillna(0.0, inplace=True)\n    \n    # normalize percentages\n    x_mob \/= 100.0\n\n    data[name]['mobility'] = x_mob\n\n# peek\npreview_name = 'Italy'\ndata[preview_name]['mobility'].head()","faca4578":"# preview data\nsrc = ColumnDataSource(data[preview_name]['mobility'])\np = figure(title=preview_name, x_axis_type='datetime', plot_height=400, plot_width=800)\np.line('Date', 'retail', source=src, legend_label='Retail', color='red')\np.line('Date', 'grocery', source=src, legend_label='Grocery', color='orange')\np.line('Date', 'parks', source=src, legend_label='Parks', color='green')\np.line('Date', 'transit', source=src, legend_label='Transit', color='magenta')\np.line('Date', 'work', source=src, legend_label='Work', color='gray')\np.line('Date', 'residential', source=src, legend_label='Residential', color='blue', name='res')\nhover = HoverTool(\n    tooltips=[\n        ('date', '@Date{%F}'),\n        ('retail', '@retail{0.0000}'),\n        ('grocery', '@grocery{0.0000}'),\n        ('parks', '@parks{0.0000}'),\n        ('transit', '@transit{0.0000}'),\n        ('work', '@work{0.0000}'),\n        ('residential', '@residential{0.0000}')\n    ],\n    formatters={'@Date': 'datetime'},\n    mode='vline', names=['res']\n)\np.add_tools(hover)\nshow(p)","794671cf":"for name in data:\n    idx = data[name]['dt'][:-1]\n    pop = countries.loc[name]['Population']\n    \n    # setup null dataframe for desired input data range\n    x_test = pd.DataFrame(index=idx, columns=['new_tests', 'new_tests_smoothed'])\n    \n    # fill in where available\n    test_sub = testing.loc[name]\n    idx_intersect = idx.intersection(test_sub.index)\n    x_test.loc[idx_intersect] = test_sub.loc[idx_intersect][['new_tests', 'new_tests_smoothed']]\n    \n    # clean up nan with rolling mean\n    x_test.fillna(x_test.rolling(15, min_periods=1, center=True).mean(), inplace=True)\n    \n    total_nan = x_test['new_tests_smoothed'].isnull().sum()\n    if total_nan > 0:\n        print('{}: setting {} remaining nan values to 0'.format(name, total_nan))\n    \n    # set any remaining nan to 0\n    x_test.fillna(0.0, inplace=True)\n    \n    # normalize testing per 1000 people\n    x_test \/= (pop \/ 1000)\n\n    data[name]['testing'] = x_test\n\n# peek\npreview_name = 'Italy'\ndata[preview_name]['testing'].head()","0f2ff3ee":"# preview data\nsrc = ColumnDataSource(data={\n    'dt': data[preview_name]['dt'][:-1],\n    'tests_raw': data[preview_name]['testing']['new_tests'],\n    'tests': data[preview_name]['testing']['new_tests_smoothed']\n})\np = figure(title=preview_name, x_axis_type='datetime', plot_height=400, plot_width=800)\np.line('dt', 'tests_raw', source=src, legend_label='New tests per 1000 (raw)', alpha=0.3)\np.line('dt', 'tests', source=src, legend_label='New tests per 1000 (smoothed)', name='tests')\nhover = HoverTool(\n    tooltips=[('date', '@dt{%F}'), ('raw', '@tests_raw{0.0000}' ), ('smoothed', '@tests{0.0000}' )],\n    formatters={'@dt': 'datetime'},\n    mode='vline', names=['tests']\n)\np.add_tools(hover)\np.legend.location = \"top_left\"\nshow(p)","f690977c":"# define helper function to aggregate input features for a specific sample\ndef get_input_features(country, date):\n    return np.concatenate((\n        data[country]['static'].values,\n        data[country]['mobility'].loc[date].values,\n        [data[country]['testing'].loc[date, 'new_tests_smoothed']]\n    )).astype(np.float64)\n\n# collect data from each country\nX = []\ny = []\nfor name in data:\n    if 'y' not in data[name]:\n        continue\n    idx = data[name]['dt'][:-1]\n    train_sz = int(sir.shape[0] - DAYS_TEST)\n    idx_train = idx[:train_sz]\n    \n    # treat each day as an independent sample\n    for sample_dt in idx_train:\n        X.append(get_input_features(name, sample_dt))\n        y.append(data[name]['y'].loc[sample_dt])\n\nX = np.array(X)\ny = np.array(y)\nprint('Input data shape: {}'.format(X.shape))\nprint('Output label shape: {}'.format(y.shape))\nnp.set_printoptions(precision=2, suppress=True)\nprint('Example I\/O pair:\\n  {}\\n  {}'.format(X[100], y[100]))","b515b53a":"search_svm = GridSearchCV(\n    SVR(),\n    PARAMETERS_SVM,\n    cv=CROSS_FOLDS,\n    refit=True\n)\n\nsearch_svm.fit(X, y)","b50c6b3c":"model_svm = search_svm.best_estimator_\nprint( 'Top SVM model params: {}'.format( search_svm.best_params_ ) )\nprint( 'Top SVM model scores: {}'.format( search_svm.best_score_ ) )","ab9a203d":"search_randomforest = GridSearchCV(\n    RandomForestRegressor(),\n    PARAMETERS_RANDOMFOREST,\n    cv=CROSS_FOLDS,\n    refit=True\n)\n\nsearch_randomforest.fit(X, y)","412972a7":"model_randomforest = search_randomforest.best_estimator_\nprint( 'Top Random Forest model params: {}'.format( search_randomforest.best_params_ ) )\nprint( 'Top Random Forest model scores: {}'.format( search_randomforest.best_score_ ) )","9448501c":"preview_name = 'Italy'\n\n# predict\ny_pred = []\nidx = data[preview_name]['dt'][:-1]\nfor dt in idx:\n    x = get_input_features(preview_name, dt)\n    res = model_svm.predict([x])\n    y_pred.append(res[0])\n\n# preview data vs predictions\nsrc = ColumnDataSource(data={\n    'dt': idx,\n    'y': data[preview_name]['y'],\n    'y_pred': y_pred\n})\np = figure(title=preview_name, x_axis_type='datetime', plot_height=400, plot_width=800)\np.line('dt', 'y', source=src, legend_label='Contact rate smoothed', alpha=0.3)\np.line('dt', 'y_pred', source=src, legend_label='Contact rate predicted', name='y_pred')\ntrain_sz = int(idx.shape[0] - DAYS_TEST)\nt_cutoff = idx[train_sz - 1] + (idx[train_sz] - idx[train_sz - 1]) \/ 2.0\np.add_layout(\n    Span(location=t_cutoff, dimension='height', line_color='gray', line_dash='dotted')\n)\nhover = HoverTool(\n    tooltips=[('date', '@dt{%F}'), ('smoothed', '@y{0.0000}' ), ('predicted', '@y_pred{0.0000}' )],\n    formatters={'@dt': 'datetime'},\n    mode='vline', names=['y_pred']\n)\np.add_tools(hover)\nshow(p)","4484f47e":"# TODO","0f7a7ae3":"Now let's run optimization to fit a few actual datasets to this model.\n\nOur target variables for optimization are **beta** (contact rate) and **gamma** (transition rate)","4994ed82":"# SIR with polynomial contact rate (SIR-poly)\nJust from the above samples, we can quickly see that a **basic SIR model is insufficient** to model the spread of COVID-19. One obvious problem is that **we assume a static contact rate and transition rate** through the entire course of the disease. In practice (and as we have all seen first hand), the spread of this disease has led to large changes in societal behavior, such as social distancing, self isolation, and closure of public spaces. These changes naturally affect the rate of spread, creating a feedback loop.\n\nAs a simple improvement, let's define the **contact rate as a function of active cases** to at least represent some basic change in behavior as the disease spreads. Let $X_i$ be active cases per thousand and $\\beta_i$ be the contact rate at a point in time $i$.\n\n$$ X_i = \\left(\\frac{I_i}{N}\\right) \\times 1000 $$\n$$ \\beta_i = \\beta_a X_i + \\beta_b$$\n","22ddaee9":"## ML FE: Testing rates\n\nWe are going to use the [COVID-19 dataset by OWID](https:\/\/www.kaggle.com\/devinaconley\/covid19-owid-data) for information on daily testing rates across different countries. While certainly incomplete, this gives us a simple indicator to represent the progression of each country's medical preparedness and response.","c2962d24":"Finally, we are going to format this data for easy consumption by an ML model. We will concatenate input features, then stack the data from each country to form a single dataset.","968a2a2d":"## Setup","d18b4e5d":"# ML model training\n\nWe will train and evaluate multiple regression models. For each model, will do a grid search for parameter optimization, with k-fold cross-validation at each possible parameter combination","07d13baa":"## ML model training: Random Forest regressor","5b8e29ed":"## ML FE: Mobility data\n\nWe are going to use the [COVID-19 Google mobility report](https:\/\/www.kaggle.com\/devinaconley\/covid19-mobility-data) dataset as a representation of public activity, social behavior, and the effect of containment\/mitigation policies.","08d0b04b":"Load, normalize and correlate datasets","1aaddc29":"Format SIR data by country for easy re-use across models","24a9288b":"Calculate the overall root mean squared error (RMSE) normalized per-capita, using our unified SIR-poly model","adde80f9":"## SIR-poly: Fitting a unified model\nWhile we do see some predictive capability in the individual country models, there is a quick and obvious **divergence as the country progresses along the epidemic curve**. The training data for a single country is limited to what we have already seen, so **all future predictions are being made in undefined space**.\n\nOur ultimate goal it create a **unified model which can capture data and learnings across all countries** affected by COVID-19. Let's start by aggregating data from various countries and fitting a single SIR-poly model.","19255b04":"Calculate the overall root mean squared error (RMSE) normalized per-capita, using individual country SIR-poly models","135e1468":"## SIR-poly: Evaluation as a predictive model\nSubjectively, we can already see a clear improvement in our ability to fit and reproduce these curves.\n\nLet's now split our data into a train and test set to get some hard numbers on actual predictive capability","a5a4b928":"## ML model training: SVM predicted contact rate\n\nLet's visually compare the smoothed contact rate with our SVM predicted values","148be418":"## ML FE: Static country embedding\nWe are going to use the [Countries of the World](https:\/\/www.kaggle.com\/fernandol\/countries-of-the-world) dataset to represent static country data, such as population density, GDP, mortality, etc.\n\nMany of these features are correlated, and we are also very limited on training data, so we cannot afford to throw all of this data directly into our model input. To handle this, we will use PCA for dimensionality reduction to create a more efficient embedding.","0e6828aa":"# ML problem formulation\n\nAt this point, we have shown the **benefit of modeling a dynamic growth rate for disease forecasting**. Meanwhile, we can also clearly see that a **simple model like SIR-poly is not expressive enough** to handle the diverse variety of country data we are throwing at it. I think we have now made an adequate case to introduce some machine learning techniques. Time to bring on the buzzwords...\n\nWe are going to extend on the idea above in using a traditional SIR model and allowing the coefficient for contact rate to be set dynamically. Now, instead of defining this value with a polynomial function, we will use an **ML regression model to predict contact rate at each time step**.\n\nConveniently, we can compute the contact rate directly from the COVID-19 time series data provided. So our prediction target at each timestep is:\n\n$$ y_i = \\beta_i = \\frac{(confirmed_{i+1} - confirmed_i) N}{S_i I_i} $$\n\n<br>\n\nOur input feature set will be a combination of dynamic features (number of active cases, local temperature, active containment policies, etc.) and static country features (population density, GDP per capita, major industries, etc.) These will be concatenated, normalized, and compressed to form a single numerical feature vector $X_i$ at each timestep. This gives us a very simple goal of learning a function $f$ to predict:\n\n$$ \\hat{y_i} = f(X_i) $$\n\n\n","5ef7dbea":"## ML model training: SVM regressor","aa251033":"# Summary and further work\n\n#### SEIR\n\n#### Additional input features\n\n#### Uncertainty in future input features\n\n#### Containment policies","a9011c30":"# COVID-19 Forecasting: enhanced SIR model\nThis notebook explores multiple approaches for improving a standard SIR model using a dynamically set contact rate\n\n**SIR-poly** defines the contact rate as a function of the currently infected population percentage\n\n**SIR-SVM** defines the contact rate as the prediction target of an SVM regression model","c8e7b2b6":"# SIR with SVM predicted contact rate (SIR-SVM)","62edbb11":"# SIR model\nFit training data to a standard SIR model (susceptible, infectious, recovered)\n\nh\/t https:\/\/scipython.com\/book\/chapter-8-scipy\/additional-examples\/the-sir-epidemic-model\/  \nh\/t https:\/\/www.kaggle.com\/saga21\/covid-global-forecast-sir-model-ml-regressions  ","a31f7f6c":"# ML feature engineering\n\n## ML FE: Prediction targets\n\nFirst let's prepare the actual target prediction values","6d049bb3":"## Explore data","fc558ec1":"## Evaluating unified SIR-poly","9e01a578":"By fitting this combined SIR-poly model, we unfortunately squashed our ability to accurately represent country-specific trends.","fadebaf9":"From examining the scatter plot above, we can clearly see **similar countries grouped together**. This subjective observation, along with the high ratio of total explained variance, should give us confidence that this embedding will be useful for a downstream estimator.","9ebd47e6":"These values seems reasonable, especially relative to the coefficients we calculated when fitting the SIR-poly model above. However, we will assume that the true contact rate is a fairly smooth signal. The noise here likely comes from administrative inefficiences, batch data processing, delayed reporting, etc.\n\nTo account for this, we will do temporal smoothing on the raw calculated values for contact rate."}}