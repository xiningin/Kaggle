{"cell_type":{"6acc6399":"code","53639495":"code","e26fffc7":"code","284d758f":"code","3194a4e8":"code","6ef71e83":"code","63749688":"code","cc70917d":"code","aba27169":"code","6510012a":"code","0f1961bf":"code","cfd7f7ff":"code","d4e4a08b":"code","10a837a2":"code","dfdf019c":"code","f80de695":"code","9e7f0cac":"code","edd581ca":"code","4a1802c6":"code","99c73cd3":"code","28589c1d":"code","84069c64":"code","9bc0ed18":"markdown","453b27fd":"markdown","8c9d774b":"markdown","f73cbadd":"markdown","038634a7":"markdown","e446f8f2":"markdown","77b1db70":"markdown","874c8ae9":"markdown","db74dd8e":"markdown","3352498b":"markdown","9071b3d3":"markdown","de826df5":"markdown","88f085e6":"markdown","d0de2580":"markdown","67d5ea66":"markdown","01ab57a9":"markdown","a927bd3f":"markdown"},"source":{"6acc6399":"!pip3 install -U -qq seaborn ","53639495":"import numpy as np, pandas as pd\nfrom glob import glob\nimport shutil, os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\nimport seaborn as sns","e26fffc7":"dim = '1024'#1024 #512, 256, 'original'\nfold = 1","284d758f":"train_df = pd.read_csv(f'..\/input\/vinbigdata-{dim}-image-dataset\/vinbigdata\/train.csv')\ntrain_df.head()","3194a4e8":"train_df['image_path'] = f'\/kaggle\/input\/vinbigdata-{dim}-image-dataset\/vinbigdata\/train\/'+train_df.image_id+('.png' if dim!='original' else '.jpg')\ntrain_df.head()","6ef71e83":"train_df = train_df[train_df.class_id!=14].reset_index(drop = True)","63749688":"train_df['x_min'] = train_df.apply(lambda row: (row.x_min)\/row.width, axis =1)\ntrain_df['y_min'] = train_df.apply(lambda row: (row.y_min)\/row.height, axis =1)\n\ntrain_df['x_max'] = train_df.apply(lambda row: (row.x_max)\/row.width, axis =1)\ntrain_df['y_max'] = train_df.apply(lambda row: (row.y_max)\/row.height, axis =1)\n\ntrain_df['x_mid'] = train_df.apply(lambda row: (row.x_max+row.x_min)\/2, axis =1)\ntrain_df['y_mid'] = train_df.apply(lambda row: (row.y_max+row.y_min)\/2, axis =1)\n\ntrain_df['w'] = train_df.apply(lambda row: (row.x_max-row.x_min), axis =1)\ntrain_df['h'] = train_df.apply(lambda row: (row.y_max-row.y_min), axis =1)\n\ntrain_df['area'] = train_df['w']*train_df['h']\ntrain_df.head()","cc70917d":"gkf  = GroupKFold(n_splits = 5)\ntrain_df['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(gkf.split(train_df, groups = train_df.image_id.tolist())):\n    train_df.loc[val_idx, 'fold'] = fold\ntrain_df.head()","aba27169":"train_files = []\nval_files   = []\nval_files += list(train_df[train_df.fold==fold].image_path.unique())\ntrain_files += list(train_df[train_df.fold!=fold].image_path.unique())\nlen(train_files), len(val_files)","6510012a":"os.makedirs('\/kaggle\/working\/vinbigdata\/labels\/train', exist_ok = True)\nos.makedirs('\/kaggle\/working\/vinbigdata\/labels\/val', exist_ok = True)\nos.makedirs('\/kaggle\/working\/vinbigdata\/images\/train', exist_ok = True)\nos.makedirs('\/kaggle\/working\/vinbigdata\/images\/val', exist_ok = True)\nlabel_dir = '\/kaggle\/input\/vinbigdata-yolo-labels-dataset\/labels'\nfor file in tqdm(train_files):\n    shutil.copy(file, '\/kaggle\/working\/vinbigdata\/images\/train')\n    filename = file.split('\/')[-1].split('.')[0]\n    shutil.copy(os.path.join(label_dir, filename+'.txt'), '\/kaggle\/working\/vinbigdata\/labels\/train')\n    \nfor file in tqdm(val_files):\n    shutil.copy(file, '\/kaggle\/working\/vinbigdata\/images\/val')\n    filename = file.split('\/')[-1].split('.')[0]\n    shutil.copy(os.path.join(label_dir, filename+'.txt'), '\/kaggle\/working\/vinbigdata\/labels\/val')","0f1961bf":"class_ids, class_names = list(zip(*set(zip(train_df.class_id, train_df.class_name))))\nclasses = list(np.array(class_names)[np.argsort(class_ids)])\nclasses = list(map(lambda x: str(x), classes))\nclasses","cfd7f7ff":"from os import listdir\nfrom os.path import isfile, join\nimport yaml\n\ncwd = '\/kaggle\/working\/'\n\nwith open(join( cwd , 'train.txt'), 'w') as f:\n    for path in glob('\/kaggle\/working\/vinbigdata\/images\/train\/*'):\n        f.write(path+'\\n')\n            \nwith open(join( cwd , 'val.txt'), 'w') as f:\n    for path in glob('\/kaggle\/working\/vinbigdata\/images\/val\/*'):\n        f.write(path+'\\n')\n\ndata = dict(\n    train =  join( cwd , 'train.txt') ,\n    val   =  join( cwd , 'val.txt' ),\n    nc    = 14,\n    names = classes\n    )\n\nwith open(join( cwd , 'vinbigdata.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(join( cwd , 'vinbigdata.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","d4e4a08b":"# https:\/\/www.kaggle.com\/ultralytics\/yolov5\n# !git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n# %cd yolov5\nshutil.copytree('\/kaggle\/input\/yolov5-official-v31-dataset\/yolov5', '\/kaggle\/working\/yolov5')\nos.chdir('\/kaggle\/working\/yolov5')\n# %pip install -qr requirements.txt # install dependencies\n\nimport torch\nfrom IPython.display import Image, clear_output  # to display images\n\nclear_output()\nprint('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))","10a837a2":"# !python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data\/images\/\n# Image(filename='runs\/detect\/exp\/zidane.jpg', width=600)","dfdf019c":"# # !WANDB_MODE=\"dryrun\" python train.py --img 640 --batch 16 --epochs 3 --data coco128.yaml --weights yolov5s.pt --nosave --cache \n!WANDB_MODE=\"dryrun\" python train.py --img 832 --batch 16 --epochs 30 --data \/kaggle\/working\/vinbigdata.yaml --weights yolov5s.pt --cache","f80de695":"import matplotlib.pyplot as plt\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp\/train_batch0.jpg'))\n\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp\/train_batch1.jpg'))\n\nplt.figure(figsize = (15, 15))\nplt.imshow(plt.imread('runs\/train\/exp\/train_batch2.jpg'))","9e7f0cac":"fig, ax = plt.subplots(3, 2, figsize = (2*5,3*5), constrained_layout = True)\nfor row in range(3):\n    ax[row][0].imshow(plt.imread(f'runs\/train\/exp\/test_batch{row}_labels.jpg'))\n    ax[row][0].set_xticks([])\n    ax[row][0].set_yticks([])\n    ax[row][0].set_title(f'runs\/train\/exp\/test_batch{row}_labels.jpg', fontsize = 12)\n    \n    ax[row][1].imshow(plt.imread(f'runs\/train\/exp\/test_batch{row}_pred.jpg'))\n    ax[row][1].set_xticks([])\n    ax[row][1].set_yticks([])\n    ax[row][1].set_title(f'runs\/train\/exp\/test_batch{row}_pred.jpg', fontsize = 12)","edd581ca":"plt.figure(figsize=(30,15))\nplt.axis('off')\nplt.imshow(plt.imread('runs\/train\/exp\/results.png'));","4a1802c6":"plt.figure(figsize=(30,15))\nplt.axis('off')\nplt.imshow(plt.imread('runs\/train\/exp\/confusion_matrix.png'));","99c73cd3":"!python detect.py --weights 'runs\/train\/exp\/weights\/best.pt'\\\n--img 640\\\n--conf 0.15\\\n--iou 0.5\\\n--source \/kaggle\/working\/vinbigdata\/images\/val\\\n--exist-ok","28589c1d":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport numpy as np\nimport random\nimport cv2\nfrom glob import glob\nfrom tqdm import tqdm\n\nfiles = glob('runs\/detect\/exp\/*')\nfor _ in range(3):\n    row = 4\n    col = 4\n    grid_files = random.sample(files, row*col)\n    images     = []\n    for image_path in tqdm(grid_files):\n        img          = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n        images.append(img)\n\n    fig = plt.figure(figsize=(col*5, row*5))\n    grid = ImageGrid(fig, 111,  # similar to subplot(111)\n                     nrows_ncols=(col, row),  # creates 2x2 grid of axes\n                     axes_pad=0.05,  # pad between axes in inch.\n                     )\n\n    for ax, im in zip(grid, images):\n        # Iterating over the grid returns the Axes.\n        ax.imshow(im)\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()","84069c64":"shutil.rmtree('\/kaggle\/working\/vinbigdata')\nshutil.rmtree('runs\/detect')\nfor file in (glob('runs\/train\/exp\/**\/*.png', recursive = True)+glob('runs\/train\/exp\/**\/*.jpg', recursive = True)):\n    os.remove(file)","9bc0ed18":"# Confusion Matrix","453b27fd":"# Pre-Processing","8c9d774b":"# Summary 1) \n\nAccording to Table 1), I fixed the models to yolov5s for quickly check. Larger image resolution leads more mAP. Actually It is obvious to understand, but I think it is worth trying to prove. To get good scores, Original or 1024 sizes are recommended. \n\nTested with these parameters \n> python train.py --img 640 --batch 16 --epochs 30 --data \/kaggle\/working\/vinbigdata.yaml --weights yolov5s.pt --cache\n\n\nTable 1) Fixed Model(yolov5s)\n\n| Model | Image Resolution | fold1 | fold2 | fold3 | fold4 | mean | std |\n|---------- |------ | ------| ------ |------ |----- |----- |  :------: |\n| yolov5s | 1024 -> 832 | 0.1327 | 0.13061 | 0.13486 | 0.13138 | 0.13239 | 0.002 |\n| yolov5s | 1024 -> 640 | 0.13039 | 0.13063 | 0.12988 | 0.13095 | 0.13046 | 0.000 |\n| yolov5s | 512 -> 640 | 0.12395 | 0.12539 | 0.12837 | 0.12611 | 0.12595 | 0.002 |\n| yolov5s | 256 -> 640 | 0.12067 | 0.12402 | 0.11966 | 0.12395 | 0.12208 | 0.002 |  |","f73cbadd":"# Selecting Models\nIn this notebok I'm using `v5s`. To select your prefered model just replace `--cfg models\/yolov5s.yaml --weights yolov5s.pt` with the following command:\n* `v5s` : `--cfg models\/yolov5s.yaml --weights yolov5s.pt`\n* `v5m` : `--cfg models\/yolov5m.yaml --weights yolov5m.pt`\n* `v5l` : `--cfg models\/yolov5l.yaml --weights yolov5l.pt`\n* `v5x` : `--cfg models\/yolov5x.yaml --weights yolov5x.pt`","038634a7":"# Train","e446f8f2":"# YOLOv5 Stuff","77b1db70":"# Get Class Name","874c8ae9":"## Pretrained Checkpoints:\n\n| Model | AP<sup>val<\/sup> | AP<sup>test<\/sup> | AP<sub>50<\/sub> | Speed<sub>GPU<\/sub> | FPS<sub>GPU<\/sub> || params | FLOPS |\n|---------- |------ |------ |------ | -------- | ------| ------ |------  |  :------: |\n| [YOLOv5s](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | 37.0     | 37.0     | 56.2     | **2.4ms** | **416** || 7.5M   | 13.2B\n| [YOLOv5m](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | 44.3     | 44.3     | 63.2     | 3.4ms     | 294     || 21.8M  | 39.4B\n| [YOLOv5l](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | 47.7     | 47.7     | 66.5     | 4.4ms     | 227     || 47.8M  | 88.1B\n| [YOLOv5x](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0)    | **49.2** | **49.2** | **67.7** | 6.9ms     | 145     || 89.0M  | 166.4B\n| | | | | | || |\n| [YOLOv5x](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0) + TTA|**50.8**| **50.8** | **68.9** | 25.5ms    | 39      || 89.0M  | 354.3B\n| | | | | | || |\n| [YOLOv3-SPP](https:\/\/github.com\/ultralytics\/yolov5\/releases\/tag\/v3.0) | 45.6     | 45.5     | 65.2     | 4.5ms     | 222     || 63.0M  | 118.0B","db74dd8e":"# Inference Plot","3352498b":"# Copying Files","9071b3d3":"# 256 vs 512 vs 1024? Which dataset is useful? \n\nThanks for Great codes https:\/\/www.kaggle.com\/awsaf49\/vinbigdata-cxr-ad-yolov5-14-class-train\n\nUsing his notebooks, I did some experiments with Image Resolutions.\nI'm trying to check the effect of image resolution to mAP or etc.\nI wrote my processes in Korean but the result will be written in English :D\n  \n* version 2-5) 512 -> 640 \uc131\ub2a5 \uce21\uc815 - fold 4~1 yolov5s\n* version 6-9) 256 -> 640 \uc131\ub2a5 \uce21\uc84d - fold 4~1 yolov5s\n* version 10-13) 1024 -> 640 \uc131\ub2a5 \uce21\uc815 - fold 4~1 yolov5s\n* version 16-19) 1024 -> 832 \uc131\ub2a5 \uce21\uc815 - fold 4~1 yolov5s","de826df5":"# GT Vs Pred","88f085e6":"# Only 14 Class","d0de2580":"# Split","67d5ea66":"# Batch Image","01ab57a9":"# (Loss, Map) Vs Epoch","a927bd3f":"# Inference"}}