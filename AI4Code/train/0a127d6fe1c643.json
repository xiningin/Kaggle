{"cell_type":{"cfdfd2f8":"code","40940a65":"code","d520d970":"code","6ed90171":"code","fdbe54c7":"code","8e72f76f":"code","04297cc1":"code","f4a8a5c2":"code","f522ca8e":"code","49541970":"code","a9211981":"code","9149b0c6":"code","ead1fa45":"code","c70be03d":"code","2fb77a71":"code","cf2921c1":"code","03c80f57":"code","f0eaa2f6":"code","3f6ba4c9":"code","102fb54c":"code","e1eb58bf":"code","b6eb3924":"code","6149da61":"code","d6537f27":"code","272c9295":"code","40af82ae":"code","2691892a":"code","7d146606":"code","dfa98665":"code","83a9f60f":"code","36c694e3":"code","4769f058":"code","8ed4923a":"code","afa9d0af":"code","a348dd13":"code","8babbd4e":"code","24b62fd9":"code","7af23ee5":"code","e5852b52":"code","0c388ab9":"code","ee79064e":"code","e8aac899":"markdown","f470fdba":"markdown","cff85a6d":"markdown","ce29cdef":"markdown","69f9dfdc":"markdown","ca377a8c":"markdown","287310a7":"markdown","65e0d8dc":"markdown","92cee578":"markdown","1b22eca6":"markdown","ca948999":"markdown","e824ffcb":"markdown","75ff92f3":"markdown","d9abaa08":"markdown","7c20ab36":"markdown","d5982956":"markdown","274420f2":"markdown","91385a3d":"markdown","5d8b9bb4":"markdown","c15bf6a2":"markdown","7f2f8207":"markdown","a1f5bc4f":"markdown","78cd7ee5":"markdown","a54577e3":"markdown","08e14f64":"markdown","9546059d":"markdown","06a54f67":"markdown","5b1b6265":"markdown","3710c515":"markdown"},"source":{"cfdfd2f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","40940a65":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scikitplot as skplt\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold,StratifiedKFold, cross_validate, train_test_split, GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.metrics import roc_auc_score, accuracy_score, classification_report, roc_curve, auc\nfrom sklearn.metrics import plot_confusion_matrix\nfrom imblearn.over_sampling import SMOTE\n\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom keras.wrappers.scikit_learn import KerasClassifier\nimport re\nimport pandas as pd\nfrom sklearn.pipeline import Pipeline\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.layers import Dense, Input, Dropout\nfrom keras import Sequential\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d520d970":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\nprint(df.shape)\ndf.head(3)","6ed90171":"print(df.columns)\ndf.describe()","fdbe54c7":"df.Time = (df.Time - df.Time.mean())\/df.Time.std()\ndf.Amount = (df.Amount - df.Amount.mean())\/df.Amount.std()\n\n#features and targets\nX = df.iloc[:,:-1]\ny = df.iloc[:,-1:]","8e72f76f":"# distribution plots for our principal components\n# shows each PC is approximately normal and centered\n\nf, axs = plt.subplots(7, 4,figsize=(20,20))\naxs= axs.flatten().tolist()\nfor pc in df.columns[1:-2]:\n    ax = axs.pop(0)\n    sns.distplot(df[pc],ax=ax)\n    ax.set_title(pc)\n    \n    ax.tick_params(\n    which='both',\n    bottom='off',\n    left='off',\n    right='off',\n    top='off'\n    )\n    \n    ax.grid(linewidth=0.25)\n    ax.set_xlabel(\"\")\n    ax.spines['left'].set_visible(False)\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    \nplt.show()","04297cc1":"fraud_n = np.sum(df.Class==1)\nlegit_n = df.shape[0] - fraud_n\nprint(\"The total number of fraudulent transactions in our data set is: \" + str(fraud_n))\nprint(\"The total number of legitimate transactions in our data set is: \" + str(df.shape[0] - np.sum(df.Class==1)))\nprint(\"\")\nprint(\"The percentage of frauds in our entire data set is: \" + str(np.round(fraud_n\/df.shape[0],5)*100)+\"%\")","f4a8a5c2":"#simple logistic classifier\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1\/3, random_state=77)\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\nmodel.score(X_test,y_test)","f522ca8e":"X_train.shape","49541970":"print(\"number of correctly classifed frauds in test data: \" + str(\\\n    np.sum((model.predict(X_test)==1)*np.array((y_test==1)).reshape(1,-1))))\nprint(\"total number of frauds in test data: \" + str(np.sum(y_test,axis=0)[0]))\nprint(\"\")\nprint(classification_report(y_test, model.predict(X_test)))\n\ncm=plot_confusion_matrix(model,X_test,y_test,normalize='true',cmap=plt.cm.viridis)\ncm.ax_.set_title(\"Confusion Matrix. 0: Legitimate, 1: Fraud\")\nplt.show()","a9211981":"fpr, tpr, threshold = roc_curve(y_test, model.predict_proba(X_test)[:,1])\nauc_score = auc(fpr,tpr)\nplt.title('ROC Curve for naive classifer')\nplt.plot(fpr, tpr, 'b',label=\"ROC\")\nplt.legend()\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([-0.0011, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","9149b0c6":"#stratified sampling\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1\/3,stratify=y, random_state=77)\n\nprint(\"# of frauds in our test set is: \" + str(np.sum(y_test)[0]))\nprint(\"# of legitimate cases in our test set is: \" + str(y_test.shape[0] - np.sum(y_test)[0]))\nprint(\"the percentage of frauds in our test set is: \" + str(round(100*np.sum(y_test)[0]\/len(y_test),3))+\"%\")\n\nprint(\"\")\n\nprint(\"# of frauds in our training set: \" + str((np.sum(y_train)[0])))\nprint(\"# of legitimate in our training set: \" + str(y_train.shape[0] - (np.sum(y_train)[0])))\nprint(\"the percentage of frauds in our training sample is: \" + str(round(100*np.sum(y_train)[0]\/len(y_train),3))+\"%\")","ead1fa45":"def undersample(X_train,y_train,rs=None):\n    \"\"\"returns *BALANCED* tuple of independent variables\n    and their associated targets from the training set\"\"\"\n    #bootstraps index of 328 legitimate cases\n    legit_index = y_train[y_train.Class == 0].sample(328,replace=True,random_state=rs).index \n    #all 328 fraud indices from training set \n    fraud_index = y_train[y_train['Class']==1].index \n    \n    X_legit = X_train.loc[legit_index]\n    y_legit = y_train.loc[legit_index]\n    X_fraud = X_train.loc[fraud_index]\n    y_fraud = y_train.loc[fraud_index]\n    return (pd.concat([X_legit,X_fraud]),pd.concat([y_legit,y_fraud]))","c70be03d":"#Let's see if our function does what it's supposed to\nU = undersample(X_train,y_train)\nsns.countplot('Class',data=U[1])\nplt.title(\"Distribution of labels within each undersample\")\nplt.show()\n\nprint(\"the ratio of frauds within each undersample is: \" + str(np.sum(U[1])[0]\/U[1].shape[0]))","2fb77a71":"U = undersample(X_train,y_train,rs=77) #defined undersample from training set","cf2921c1":"#LDA:\nLDA_params = {'solver': ['svd','lsqr','eigen'], 'shrinkage': np.arange(0.1,1.01,step=0.01)}\ng_LDA = GridSearchCV(LinearDiscriminantAnalysis(), LDA_params).fit(U[0],U[1])\nLDA = g_LDA.best_estimator_\n\n#QDA:\nQDA_params = {'reg_param':np.arange(0.1,10.1,step=0.1)}\ng_QDA = GridSearchCV(QuadraticDiscriminantAnalysis(), QDA_params).fit(U[0],U[1])\nQDA = g_QDA.best_estimator_\n\n#KNN:\nKNN_params = {\"n_neighbors\": np.arange(1,10,step=1), 'weights': ['uniform','distance'] ,'p': [1,2]}\ng_KNN = GridSearchCV(KNeighborsClassifier(), KNN_params).fit(U[0],U[1])\nKNN = g_KNN.best_estimator_","03c80f57":"#Logistic Regression:\nLR_params = {'penalty': ['l1','l2','elasticnet'], 'C': np.arange(0.01,1,step=0.03),'dual': [False,True],\n             'solver': ['newton-cg', 'lbfgs', 'saga']}\ng_LR = GridSearchCV(LogisticRegression(), LR_params).fit(U[0],U[1])\nLR = g_LR.best_estimator_","f0eaa2f6":"#Decision Tree:\nDT_params = {'criterion': ['gini','entropy'], 'max_depth': [None,5,10,25,50], 'min_samples_split': [2,3,4,5],\n            'min_samples_leaf': [1,2,10,20,25]}\ng_DT = GridSearchCV(DecisionTreeClassifier(), DT_params).fit(U[0],U[1])\nDT = g_DT.best_estimator_\n\n#Random Forest: \nRF_params = {'n_estimators':[100], 'max_depth': [None,5,10], 'min_samples_split': [2,3],\n            'min_samples_leaf': [1,2,3],'max_features': ['sqrt','log2'],'random_state':[77]}\ng_RF = GridSearchCV(RandomForestClassifier(), RF_params).fit(U[0],U[1])\nRF = g_RF.best_estimator_","3f6ba4c9":"#SVM:\nSVM_params = {'C': np.arange(0.01,2,step=0.05), 'kernel': ['rbf', 'poly', 'sigmoid', 'linear'],\n             'gamma': ['scale','auto'],'probability':[True]}\ng_SVM = GridSearchCV(SVC(), SVM_params).fit(U[0],U[1])\nSVM = g_SVM.best_estimator_","102fb54c":"#all our previous classifiers\nclfs = {'Naive Logistic Reg (no undersampling)': model, 'LDA':LDA, 'QDA': QDA, 'KNN':KNN, \n        'Logistic Regression': LR, 'Decision Tree': DT, 'Random Forest': RF, 'SVM':SVM}","e1eb58bf":"#details of our classifiers after grid search\nfor key,clf in clfs.items():\n    print('params for ' +str(key) +\": \" + str(clf))\n    print(\"\")","b6eb3924":"f, axs = plt.subplots(4, 2, figsize=(10,18))\naxs= axs.flatten().tolist()\nfor key,clf in clfs.items():\n    ax = axs.pop(0)\n    cm=plot_confusion_matrix(clf,X_test,y_test,normalize='true',ax=ax,cmap='viridis')\n    ax.set_title(key)\nplt.show()","6149da61":"def plot_rocs(clfs,y_bound=0,x_bound=1,omit=[],title=\"\",size=(8,8)):\n    \"\"\"takes in a dict of classifiers and y axis lower bound.\n    omit is a list of the classifiers key to omit.\n    returns plot of ROC and associated AUC scores\"\"\"\n    plt.figure(figsize=size)\n    for key,clf in clfs.items():\n        if key in omit:\n            continue\n        else:\n            fpr, tpr, threshold = roc_curve(y_test, clf.predict_proba(X_test)[:,1])\n            auc_score = auc(fpr,tpr)\n            plt.title(title)\n            plt.plot(fpr, tpr,label=key+ \" AUC: \"+ str(np.round(auc_score,3)))\n            plt.legend()\n            plt.plot([0, 1], [0, 1],'r--')\n            plt.xlim([0, x_bound])\n            plt.ylim([y_bound, 1.001])\n            plt.ylabel('True Positive Rate')\n            plt.xlabel('False Positive Rate')\n    plt.show()","d6537f27":"plot_rocs(clfs,title = \"ROC for our classifiers\")","272c9295":"plot_rocs(clfs,y_bound=0.8,x_bound=1,size=(10,10), title = \"ROC for our classifiers\")","40af82ae":"#bagging within the undersample\nbagged_clfs = {}\nfor key,clf in clfs.items():\n    if clf==model: #naive model, bag on X_train \n        bagged_clfs[key] = BaggingClassifier(clf,100).fit(X_train,y_train) \n    else: #undersample models, bag on our undersample\n         bagged_clfs[key] = BaggingClassifier(clf,100).fit(U[0],U[1])","2691892a":"conf=plot_confusion_matrix(bagged_clfs[\"Decision Tree\"],X_test,y_test,normalize='true',ax=ax,cmap='viridis')","7d146606":"plot_rocs(bagged_clfs,y_bound=0.8,omit=[\"KNN\"],title= \"ROC for our *BAGGED* classifiers\")","dfa98665":"conf=plot_confusion_matrix(bagged_clfs[\"Decision Tree\"],X_test,y_test,normalize='true',ax=ax,cmap='viridis')\nconf.plot()\nplt.title('bagged Decision Tree')\nplt.show()","83a9f60f":"from xgboost import XGBClassifier","36c694e3":"#training on original \nxgb = XGBClassifier(random_state=77).fit(X_train,y_train)\n#training on the undersample\nxgb_u = XGBClassifier(random_state=77).fit(U[0],U[1])\n\nd = {'XGBOOST (on original train)':xgb,'XGBOOST (on undersample)': xgb_u}","4769f058":"dic = {\n    'XGBOOST (on original train)':xgb,'XGBOOST (on undersample)': xgb_u,\n    'Naive Logistic Reg (no undersampling)': model,'LDA': LDA, 'Random Forest': RF, \n    'bagged DT':bagged_clfs['Decision Tree'] \n      }\nplot_rocs(dic,0.8,title = \"XGBOOST classifier ROC\")","8ed4923a":"f, axs = plt.subplots(1, 2, figsize=(10,6))\naxs= axs.flatten().tolist()\nfor key,clf in d.items():\n    ax = axs.pop(0)\n    cm=plot_confusion_matrix(clf,X_test,y_test,normalize='true',ax=ax,cmap='viridis')\n    ax.set_title(key)\nplt.show()","afa9d0af":"n_inputs = U[0].shape[1]\n#function creates a Keras Neural network based on provided inputs\ndef create_model(dense_layer_sizes, epochs = 10, optimizer=\"adam\", dropout=0.1, \n                 init='uniform', dense_nparams=256, batch_size = 2):\n    model = Sequential()\n    model.add(Dense(dense_nparams, activation='relu', input_shape=(n_inputs,), kernel_initializer=init,)) \n    model.add(Dropout(dropout), )\n    for layer_size in dense_layer_sizes*[dense_nparams]:\n        model.add(Dense(layer_size, activation='relu'))\n        model.add(Dropout(dropout), )\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=[\"accuracy\"])\n    return model","a348dd13":"keras_estimator = KerasClassifier(build_fn=create_model, verbose=1)\nparam_grid = {\n    'dense_layer_sizes' : [1,2,3],\n    'epochs': [50],\n    'dense_nparams': [32, 64],\n    'init': [ 'uniform'], \n    'batch_size':[4],\n    'optimizer':['Adam'],\n    'dropout': [0.2, 0.1,0.05, 0]\n}","8babbd4e":"kfold_splits = 5\ngrid = GridSearchCV(estimator=keras_estimator,  \n                    n_jobs=-1, \n                    verbose=1,\n                    return_train_score=True,\n                    cv=kfold_splits,  #StratifiedKFold(n_splits=kfold_splits, shuffle=True)\n                    param_grid=param_grid,)","24b62fd9":"grid.fit(U[0],U[1])","7af23ee5":"nn_best = grid.best_estimator_\nnn_best.get_params()","e5852b52":"#for confusion matrix\nx = nn_best.predict(X_test)","0c388ab9":"skplt.metrics.plot_confusion_matrix(\n    y_test, \n    x,\n    figsize=(4,4),normalize=True,title='Neural Net Confusion Matrix',cmap='Blues')\nplt.show()","ee79064e":"dic = {\n    'Neural (on undersample)':nn_best,'XGBOOST (on original training)':xgb\n      }\nplot_rocs(dic,0.8,title=\"Neural Net and XGBOOST ROCs\",size=(5,5))","e8aac899":"Note that we have not done hyperparameter tuning for the XGBOOST estimator.\nWe now plot these two classifiers against our best perfoming AUC classifier (LDA), naive logistic regression, our bagged decision tree and our random forest classifier, for comparison.","f470fdba":"At first glance, it may seem that a 99.9 test accuracy is a fantastic result. \nHowever, this simple accuracy measure is extremely misleading when dealing with imbalanced datasets.\nIndeed, since only 0.17% of the data is classified as fraudulent (1), a classifier that predicts 0 every single time would still get 99.83 accuracy!\n\nA look at the recall and ROC curve from our naive classifier explicitly displays the problem:","cff85a6d":"We can zoom in on the region of interest:","ce29cdef":"Amazingly, the (un-tuned) boosting classifier trained on the original training set outperforms all of our previous classifiers in terms of ROC AUC. As discussed earlier, this is because the algorithm is weighing the frauds more heavily while training, despite only representing 0.17% of the training set. \n\nHowever, as before, ROCs and AUCs only paint a partial picture of the perfomance of classifier, which here is highly contingent on the choice of threshold. Indeed, the following confusion matrices show that the boosting classifier (on the training) perfoms relatively poorly in terms of correctly identifying frauds at the 0.5 threshold, with only a 0.81 true positive rate:","69f9dfdc":"To remedy the aforementioned problem, one potential strategy is to use **undersampling**.\n\nThe underlying principle is to train our model using a balanced data set (i.e an equal number of fraudulent and legitimate cases in the training set), so that the classifer will not overfit the majority class. The potential drawback with this approach is that we can lose a great deal of information from the majority class, as we will be taking only a very small fraction of our 284,315 legitimate datapoints in order to balance the training set with the minority class. To remedy this information loss, we will try bagging our undersampled models, as this will give us increased exposure to the majority class.","ca377a8c":"We can now begin undersampling from the training set:\n\n<li> Each undersample will contain all 328 frauds and 328 randomly selected legitimate cases (out of the 189,543) from the training set.\n    <li> We can later bag the classifiers we fit to each undersample in order to mitigate majority class information loss","287310a7":"# 2 - Tackling the imbalanced data issue: Undersampling","65e0d8dc":"# 6 - Concluding Remarks\n\nOur results show that multiple techniques - such as undersampling or boosting - can remedy the problem posed by classification under imbalanced data. By fitting a wide range of different classifiers, we have explored the metrics of model selection in the context of the sensitivity and specificity trade-off. Our optimized neural network fitted on the undersample proved to be the most sensitive classifier, despite running GridSearch over a relatively small set for each of our hyperparameters. However, as emphasised throughout our discussion, finding the \u201cbest\u201d model is not simply an enterprise of comparing metrics such as sensitivity or AUC scores, but ultimately an optimization problem over specified costs - further research into this topic would entail specifying and solving such a problem. Moreover, higher computing power would likely yield even better out-of-sample results from our Neural net and Boosting classifiers, due to the wider hyperparameter searches.","92cee578":"Before finalizing, we now shift our focus on neural networks and deep learning classifiers.","1b22eca6":"# 5 - Neural Nets\n\nIn this last section, we use deep neural networks to fit the undersample. In particular, we implement the model using the Keras library. In order to maximize the test set performance of our network, we first proceed by tuning the following hyperparameters using GridSearch: \n\n<li> Number of hidden layers\n<li> Number of neurons in hidden layers\n<li> Dropout rate for regularization\n\nNote that due to computational power limitations, we only run GridSearch over a relatively small set for each of our hyperparameters.","ca948999":"We can now create multiple balanced undersamples from our training set, train classifiers within these undersamples, and then assess their perfomance in the test set (where there is only 0.17% of frauds).","e824ffcb":"Only a measly 0.17% of our data actually represents frauds (class=1). \n\nIntuitively, training a classifier (say, on 60% of the data) should make it extremely difficult for the classifier to accurately predict frauds, since it has so few examples to learn from. Is this really a problem? Let's fit a naive logistic regression classifier and evaluate further:","75ff92f3":"We will now try to improve our results - and particularly our true positive rate - by using boosting. \nIntuitively, boosting should help with the true positive rate even without undersampling, as it gives more weight while training on erroneously classified examples.\n\nAs such, if we train on our original training set (as we did with the naive logistic regressing), the vast majority of errors will occur on fraudulent cases (since they only represent 0.17% of the data), which will cause the fraudulent cases to receive higher weightage during subsequent training. \n\nWe investigate the perfomance of boosting by training both within the undersample and the original (not balanced) training set.","d9abaa08":"# 4 - Bagging and Boosting\n\n### Bagging\n\nWe begin by investigating our perfomance by bagging each classifier within the undersample.","7c20ab36":"From the above, we note that ROC AUC measures are all relatively very high, with our naive classifier perfoming the worst. Give the high AUC scores, this suggets that threshold selection is particularly important here. However, model selection based on AUC scores is still equivocal: for instance, consider the LDA and Logistic Regression (with undersampling) classifiers.\n\nIndeed, as per our confusion matrices, the LDA classifier has one of the lowest FPR at the 0.5 threshold, correctly classifying almost all of the legitimate transactions in the test set. However, it bolsts a relatively low TPR of 0.82 compared to our other classifiers: The undersample results suggests that the LDA classifies frauds with 9% **less** accuracy than the Logistic Regression. However, the LDA's AUC score is higher than all our classifiers.\n\nOf course, this is because the choice of probability threshold plays an important role in the TPR-FPR trade-off here.  which explains the LDA's high AUC score. However, if threshold optimization is not at play, LDA is clearly not our best \"off-the-shelf\" classifier.","d5982956":"The above confusion matrix shows the network's ability to correctly classify fraudulent transactions: out of all our classifiers, it has the highest sensitivity (0.95) at the 0.5 threshold! Of course, this comes at a somewhat lower specificity, but the slight decrease is beareable given our previous discussion on asymmetric misclassification costs.\n\nLastly, the ROC plot of the neural net against our boosted classifier once again drastically shows the sensitivity-specificity trade-off. While the boosted classifier has the highest AUC out of all our previous classifiers, the neural net would be the clear winner if the costs associated with misclassifications of frauds are very high.","274420f2":"We have approximately 285,000 rows and 30 features. Note that due to confidentiality reasons, the names of the features are removed, to the exception of the \"Time\" and \"Amount\" feature, which represent the time in seconds after the first transaction, and the amount of the transaction, respectively.\n\nFurthermore, as per the providers of the data set, all 28 other features were obtained via PCA reduction, and as such we cannot extrapolate intuitive interpretation from the variables, but this does not stop us from building meaningful classifiers.","91385a3d":"We optimize using GridSearchCV:","5d8b9bb4":"**From the above, we see that our undersampling classifiers perform much better than our naive classifier at the 0.5 threshold level (assuming that frauds are more important to correctly classify).**\n\nIndeed, our true positive rate increases significantly across all classifiers when using undersampling, with our Logistic Regression, SVM and random forest classifier perfoming the best in terms of TPR and FPR trade-off (at the 0.5 threshold). Of course, the optimal TPR-FPR trade-off choice is ultimately determined by the exact costs associated with misclassifications. \n\nWe can take a look at the ROC plot of our classifiers, and their associated AUC scores. This will further give us an idea of how threshold selection plays a role in model selection.","c15bf6a2":"As per the above, our logistic classifer is only classifying 58% of the frauds correctly in the test data, which stems from the fact that our model trained on so few actual examples of fraud.\n\n**Our task now becomes apparent: Given this imbalanced dataset, we want to build a classifier that accurately predicts frauds when it encounters them, while still maintaining high recall for the legitimate transactions.**\n\nThis is especially important when the cost of misclassification is asymmetric, as might be the case here. For instance, classifying a fraudulent transaction as legitimate could cause a customer to incur massive losses, while classifying a legitimate transaction as fraudulent may be relatively less costly. There is thus a striking trade-off between the sensitivity and specificity of our classifier, which ultimately depends on the costs associated with each misclassification error. Since total test accurarcy is not a good metric, we will rely on other measures during our model selection, such as Confusion Matrices and Recall. ","7f2f8207":"So far so good - but what about our response variable?","a1f5bc4f":"As per the above descriptive statistics, our principal components are roughly centered and scaled, whereas the \"Time\" and \"Amount\" features are not. This may impact our classifiers negatively, particularly in the presence of regularization - we thus standardize them before proceeding with the analysis.","78cd7ee5":"# 1 - Exploratory look at the data and problem","a54577e3":"### Boosting","08e14f64":"As per the ROC and AUC scores, bagging within the undersample does little to help: our results are vastly similar to the non-bagged case. However, the highest (and only noticeable) increase in AUC comes from the decision tree classifier.\n\nIndeed, the only real improvement from bagging within the undersample is on the false positive rate of our decision tree classifier, which drops 2%. This makes sense since this is the highest variance classifier from our list.\nOtherwise, bagging within the undersample yields similar results as the no-bagging case for the other classifiers.","9546059d":"In this project we attempt to take a deeper look at how to tackle imbalanced datasets for classification, an ongoing research issue in the field of statistical learning and data mining. In order to do so, we investigate and compare the perfomance of a wide range of classifiers while using known techniques for dealing with imbalanced data sets.\n\nThe following data set contains large amounts of data on credit card transactions, and our goal will be to ultimately discriminate whether incoming transactions are to be classified as legitimate (0) or fraudulent (1). \nLet's start by taking a deeper look at our data and what issues might arise from the imbalanced nature of the data set.","06a54f67":"**Important point:** Before continuing with undersampling, we save a test set from the data and remove it from the training dataset, as we want to avoid using any of the datapoints from the test set while training. \nWe also want this test set to be representative of the population to accurately estimate the perfomance of our classifiers, so we preserve the percentage of frauds to 0.17% - this can easily be done via stratified sampling.","5b1b6265":"# 3 - Fitting multiple classifiers\n\nWe'll start by fitting a variety of classifiers on a single undersample, and assess their relative perfomances.\nWe optimize hyperparameters for each classifier using GridSearch, within the undersample.","3710c515":"We now test these classifiers on the saved test set (where there are only 0.17% frauds, as in the entire dataset)."}}