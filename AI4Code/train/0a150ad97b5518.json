{"cell_type":{"ff029aa1":"code","c05b7d11":"code","af2c0ace":"code","6fe9b455":"code","13abf65a":"code","302d0247":"code","337c6686":"code","27c7d57d":"code","243cda34":"code","1b9081fb":"code","44cd4e70":"code","fa3e14c1":"code","c5e1388a":"markdown","aed8e95c":"markdown","0949ee78":"markdown","7a0a5a05":"markdown","5fe22639":"markdown","4cef4ec1":"markdown","0be4f408":"markdown","e5b654d0":"markdown","2c9d5f6e":"markdown"},"source":{"ff029aa1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c05b7d11":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.utils import np_utils\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\nimport matplotlib.pyplot as plt\nimport seaborn as sns","af2c0ace":"csv = '..\/input\/speech-emotion-dataset\/features.csv'\nFeatures = pd.read_csv(csv)","6fe9b455":"Features.head()","13abf65a":"X = Features.iloc[: ,:-1].values\nY = Features['labels'].values\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=24, shuffle=True)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\n\nx_train = np.expand_dims(x_train, axis=2)\nx_test = np.expand_dims(x_test, axis=2)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","302d0247":"# model=Sequential()\n# model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\n# model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\n# model.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\n# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n# model.add(Dropout(0.2))\n\n# model.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\n# model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\n# model.add(Flatten())\n# model.add(Dense(units=32, activation='relu'))\n# model.add(Dropout(0.3))\n\n# model.add(Dense(units=16, activation='softmax'))\n# model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n\n# model.summary()\n\n\nmodel = Sequential()\nmodel.add(Conv1D(2048, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\nmodel.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel.add(BatchNormalization())\n\nmodel.add(LSTM(256, return_sequences=True))\n\nmodel.add(LSTM(128))\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(16, activation='softmax'))\n\noptimiser = keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=optimiser,\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\nmodel.summary()","337c6686":"# rlrp = ReduceLROnPlateau(monitor='loss', factor=0.4, verbose=0, patience=2, min_lr=0.0000001)\n# history=model.fit(x_train, y_train, batch_size=64, epochs=100, validation_data=(x_test, y_test), callbacks=[rlrp])\n\nhistory = model.fit(x_train, y_train, batch_size=64, epochs=200, validation_data=(x_test, y_test))","27c7d57d":"print(\"Accuracy of our model on test data : \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")","243cda34":"epochs = [i for i in range(200)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()","1b9081fb":"pred_test = model.predict(x_test)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)\n\ndf = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","44cd4e70":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","fa3e14c1":"# model = Sequential()\n# model.add(Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n# model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n# model.add(BatchNormalization())\n\n# model.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))\n# model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n# model.add(BatchNormalization())\n\n# model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n# model.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\n# model.add(BatchNormalization())\n\n# model.add(LSTM(128, return_sequences=True))\n\n# model.add(LSTM(64))\n\n# model.add(Dense(64, activation='relu'))\n# model.add(Dropout(0.5))\n\n# model.add(Dense(32, activation='relu'))\n# model.add(Dropout(0.5))\n\n# model.add(Dense(16, activation='softmax'))\n\n# optimiser = keras.optimizers.Adam(learning_rate=0.0001)\n# model.compile(optimizer=optimiser,\n#                   loss='categorical_crossentropy',\n#                   metrics=['accuracy'])\n\n# model.summary()","c5e1388a":"## Confusion Matrix","aed8e95c":"## One Hot Encoding and Scaling the data","0949ee78":"## Callback","7a0a5a05":"## Data path","5fe22639":"## Looking into the data","4cef4ec1":"## Plotting train and test loss and accuracy","0be4f408":"## Model building","e5b654d0":"## Importing required libraries","2c9d5f6e":"**0 - 19 columns are the extracted features and can be trained to get the desired emotion.**"}}