{"cell_type":{"59f548f1":"code","f75153c6":"code","ea684900":"code","ba68b815":"code","04d33eff":"code","be7c18e1":"code","42e1a8f9":"code","fe260e97":"code","355a11a9":"code","6d609308":"code","cfad60d3":"code","4f33309e":"code","a01089d6":"code","a6e1ae4b":"code","a83a1b05":"code","d9501489":"code","21137638":"code","ea8aff55":"code","1b2b3fe0":"code","3c2d5312":"code","895dfe4c":"code","80fdeab2":"code","83d935b8":"code","701de831":"code","0c75781f":"code","d7dcc93a":"code","00942eaf":"code","0d215bce":"code","3e770344":"code","32773ba8":"code","8c74f593":"code","5574b797":"code","c0000db7":"code","2ea8aa4c":"code","185440bf":"code","25ffd160":"code","2b09127a":"code","fa75baed":"code","d1ba4c21":"code","6eb611ad":"code","2d091d18":"code","775ac04e":"code","2fbda9b4":"code","18c34e7a":"code","49a4ca8e":"code","0e811d52":"code","454a5a7a":"code","fe5d5b55":"code","8655c62d":"code","70e2bf12":"code","eaebda2a":"code","6831097b":"code","6e858451":"code","3a91e794":"code","e93cbc62":"markdown","e8fbba5d":"markdown"},"source":{"59f548f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f75153c6":"import io\nimport random\nimport urllib.request\nimport numpy  as np\nimport pandas as pd\nfrom sklearn import *\nimport  matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.io import arff\nfrom warnings import filterwarnings\nfilterwarnings(action='ignore')\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\n%matplotlib inline","ea684900":"url = \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00266\/seismic-bumps.arff\"","ba68b815":" s = urllib.request.urlopen(url)\n df, meta = arff.loadarff(io.StringIO(s.read().decode('utf-8')))\n","04d33eff":"df = pd.DataFrame(df)","be7c18e1":"df.head()","42e1a8f9":"df.info()","fe260e97":"df.describe().T","355a11a9":"df.tail()","6d609308":"df.dtypes","cfad60d3":"df.groupby([\"energy\",\"maxenergy\"])['class'].count()","4f33309e":"col_use = df.columns[:-1]","a01089d6":"col","a6e1ae4b":"col = []\ndef object_col():\n    for i in df.columns:\n        if(df[i].dtypes == 'object'):\n            print(i,df[i].unique())\n            col.append(i)\n        else:\n            print(\"No object column -->\")\n            ","a83a1b05":"object_col()","d9501489":"col","21137638":"lab = preprocessing.LabelEncoder()","ea8aff55":"df[col] = df[col].apply(lab.fit_transform)","1b2b3fe0":"df","3c2d5312":"def color_negative(val):\n    color= \"red\" if val < 0 else 'blue'\n    return \"color: %s\" % color","895dfe4c":"# to show me each value less then 0 color red and greater then 0 cololr blue\ndf.style.applymap(color_negative)","80fdeab2":"\ndf.plot(figsize=(15,12))\n","83d935b8":"sns.countplot(df['class'])","701de831":"x1,y1 = df.drop('class',axis=1).values, df['class'].values","0c75781f":"sm = SMOTE()","d7dcc93a":"x1, y1 =  sm.fit_resample(x1,y1)","00942eaf":"Counter(y1)","0d215bce":"sns.countplot(y1)","3e770344":"xtrain, xtest, ytrain, ytest = model_selection.train_test_split(x1,y1,test_size=0.30,random_state=42)","32773ba8":"import  scikitplot as sk","8c74f593":"sk.estimators.plot_learning_curve(linear_model.LogisticRegression(),xtrain, ytrain,cv=5)","5574b797":"xtrain = preprocessing.StandardScaler().fit_transform(xtrain)\nxtest = preprocessing.StandardScaler().fit_transform(xtest)","c0000db7":"sk.estimators.plot_learning_curve(linear_model.LogisticRegression(),xtrain, ytrain,cv=5)","2ea8aa4c":"df.head()","185440bf":"\nfig= plt.figure(figsize=(20,4))\nfig.add_subplot(1,2,1)\nsns.distplot(df['genergy'])\nfig.add_subplot(1,2,2)\nsns.boxplot(df['genergy'])","25ffd160":"df.describe().T","2b09127a":"from sklearn.svm import SVC","fa75baed":"linear_model.LogisticRegression?","d1ba4c21":"models = {\n    \"LogisticRgs\":linear_model.LogisticRegression(C=100,solver='liblinear'),\n    \"KNN\": neighbors.KNeighborsClassifier(n_neighbors=2),\n    \"SVM\":SVC( C=1,kernel='linear'),\n    \"RandomFs\":ensemble.RandomForestClassifier(n_estimators=100,criterion='entropy')\n    \n}\n","6eb611ad":"for name,model in models.items():\n    model.fit(xtrain, ytrain)\n    cm = model_selection.cross_val_score(model, xtrain,ytrain,cv=10,scoring=\"accuracy\")\n    print('<========================>')\n    print(f\"StdV:                {cm.std():.3f}%\")\n    print(f\"Accuracy:            {cm.mean():.2f}%  Model:{name}\")\n    print(f\"Acuracy N:           {model.score(xtest, ytest)}\")\n    print(f\"Training Testing N:  {model.score(xtrain, ytrain)}\")\n    sk.metrics.plot_confusion_matrix(ytest,model.predict(xtest),title=name)\n    print( metrics.classification_report(ytest,model.predict(xtest)))","2d091d18":"df.iloc[:,:-1].","775ac04e":"q1 = df.iloc[:,:-1].quantile(0.25)\nq3 = df.iloc[:,:-1].quantile(0.75)\niqr = q3 - q1","2fbda9b4":"data = df[~((df < (q1 - 1.5 * iqr)) |(df > (q3 + 1.5 * iqr))).any(axis=1)]\n","18c34e7a":"data","49a4ca8e":"x2 = data.drop('class',axis=1).values\ny2 = data['class'].values","0e811d52":"Counter(data['class'])","454a5a7a":"Counter(data['class'])","fe5d5b55":"x2,y2 = sm.fit_resample(x2,y2)","8655c62d":"x_train,x_test, y_train,y_test = model_selection.train_test_split(x2,y2, test_size=0.30,random_state=42,stratify=y2)","70e2bf12":"x_train = preprocessing.StandardScaler().fit_transform(x_train)\nx_test = preprocessing.StandardScaler().fit_transform(x_test)","eaebda2a":"Counter(y2)","6831097b":"n = []\nts = []\nss = []\nfor name,model in models.items():\n    model.fit(x_train, y_train)\n    cm = model_selection.cross_val_score(model, x_train,y_train,cv=10,scoring=\"accuracy\")\n    t= model.score(x_train, y_train)\n    s = model.score(x_test, y_test)\n    n.append(name)\n    ts.append(t)\n    ss.append(s)\n    print('<========================>')\n    print(f\"StdV:                {cm.std():.3f}%\")\n    print(f\"Accuracy:            {cm.mean():.2f}%  Model:{name}\")\n    print(f\"Acuracy N:           {s}\")\n    print(f\"Training Testing N:  {t}\")\n    sk.metrics.plot_confusion_matrix(y_test,model.predict(x_test),title=name)\n    print( metrics.classification_report(y_test,model.predict(x_test)))\n    \ndf_frame = pd.DataFrame({\"Infor\":\"Model\",\"Name\":n,\"Accuracy\":ss,\"Training Test\":ts})    \n    ","6e858451":"df_frame","3a91e794":"def color_negative_red(val):\n    color = 'red' if val < 0.90 else 'blue'\n    return 'color: %s' % color\n\ndf_frame[[\"Accuracy\",\"Training Test\"]].style.applymap(color_negative_red)","e93cbc62":"**Chech the Object columns**","e8fbba5d":"**Preprocessing**"}}