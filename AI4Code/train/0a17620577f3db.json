{"cell_type":{"1807ce3e":"code","315307ae":"code","04441b9b":"code","2dd16f2b":"code","75a295f7":"code","33b16b9e":"code","29c45674":"code","ba2813e6":"code","fadd1848":"code","a550f4a2":"code","0e0b126d":"code","bf6067f6":"code","8db64137":"code","188c8492":"code","48cf0c54":"code","24afddad":"code","e981696d":"code","a6f7a371":"code","611053bb":"code","0fec3cda":"code","ecbb6362":"code","577027d2":"code","cbc8b1c0":"code","78480eba":"code","8025d0f0":"code","4891e857":"code","9b587346":"code","878ba096":"code","d262edb4":"code","c2702a8b":"code","186dfa8b":"code","b3296242":"code","77af78b0":"code","34ef3037":"code","db867593":"code","80b539b7":"code","2a1db917":"code","e0f14c90":"code","4909e2b2":"code","fb20fbe5":"code","a16a6939":"code","8896a032":"code","c5a531c8":"code","8f81df13":"code","02b9eb58":"code","72650cda":"code","80f722d7":"code","2837c11d":"code","80d5e732":"code","467786e3":"code","8ca4cb15":"code","e120079b":"code","cad340b8":"code","2b42501b":"code","546405b4":"code","1a3e1f0d":"code","079ff9ea":"code","791c27b7":"code","42efcb90":"code","8dc28310":"code","4d0ca881":"code","6b2b8c7b":"code","461144cb":"code","4798d51d":"code","2c424f84":"code","4992772c":"code","e01de064":"code","fd1d3652":"code","c5df2230":"code","024ee0b8":"code","5caebede":"markdown","c1eba931":"markdown","3ac37416":"markdown","39a2a3a9":"markdown","a757bacd":"markdown","791f56cb":"markdown","2be17a27":"markdown","d122bbe0":"markdown","14cb5625":"markdown","13210bbc":"markdown","71280d0a":"markdown","26ba9375":"markdown","f13daa9e":"markdown","b3d0ed7b":"markdown","4e519de0":"markdown","0672647d":"markdown","bf74805d":"markdown","644a9d1e":"markdown","488097c6":"markdown","981064dd":"markdown","a5fdd596":"markdown","b94b9fbe":"markdown","87397147":"markdown","e98ea675":"markdown","eb84803f":"markdown","16ece88d":"markdown","8a6bd168":"markdown","669c7b2a":"markdown","d5ae579f":"markdown","23a38d61":"markdown","6be81ff9":"markdown","f6654a4d":"markdown","90bfd056":"markdown","997372e2":"markdown","a349bd04":"markdown","506416ab":"markdown","017a3ba8":"markdown","402afabc":"markdown","d43c15c0":"markdown","50f54d7e":"markdown","fe47fb5f":"markdown","b6507c95":"markdown","62a8087e":"markdown","98b1d2d0":"markdown","cc2277ec":"markdown","2ffe3cb7":"markdown","c826731d":"markdown","ab7382c1":"markdown","bd209ead":"markdown","80d8936a":"markdown","107a839c":"markdown","97dd9e57":"markdown","c9ba5b5c":"markdown","7c9b7433":"markdown"},"source":{"1807ce3e":"import matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn\nimport gc","315307ae":"data_partitions_dirpath = '..\/input\/random_split\/random_split'\nprint('Available dataset partitions: ', os.listdir(data_partitions_dirpath))","04441b9b":"def read_all_shards(partition='dev', data_dir=data_partitions_dirpath):\n    shards = []\n    for fn in os.listdir(os.path.join(data_dir, partition)):\n        with open(os.path.join(data_dir, partition, fn)) as f:\n            shards.append(pd.read_csv(f, index_col=None))\n    return pd.concat(shards)\n\ntest = read_all_shards('test')\ndev = read_all_shards('dev')\ntrain = read_all_shards('train')\n\npartitions = {'test': test, 'dev': dev, 'train': train}\nfor name, df in partitions.items():\n    print('Dataset partition \"%s\" has %d sequences' % (name, len(df)))","2dd16f2b":"train.reset_index(inplace=True, drop=True)\ndev.reset_index(inplace=True, drop=True)\ntest.reset_index(inplace=True, drop=True)","75a295f7":"train.groupby('family_id').size().sort_values(ascending=False).head(10)","33b16b9e":"def getTransitionMatForSequence(transitions, default):\n    df = pd.DataFrame(transitions)\n    df['shift'] = df[0].shift(-1)\n    df['count'] = 1\n    trans_mat = df.groupby([0, 'shift']).count().unstack().fillna(0)\n    trans_mat.columns = trans_mat.columns.droplevel()\n    return (default+trans_mat).fillna(0)","29c45674":"def makeHeatmap(df, familyId):\n    mask = df.family_id == familyId\n    famSeqs = df.loc[mask, 'sequence'].reset_index(drop=True)\n    AAs = [aa for aa in 'GALMFWKQESPVICYHRNDTXUBOZ']\n    seqs = famSeqs.apply(lambda seq: [aa for aa in seq])\n    default = pd.DataFrame([[0]*len(AAs)]*len(AAs), columns=AAs, index=AAs, dtype='int64')\n    transMat = default.copy()\n    for seq in seqs: #tqdm(seqs):\n        transMat += getTransitionMatForSequence(seq, default)\n    maskX = transMat.sum(axis=1) != 0\n    maskY = transMat.sum(axis=0) != 0\n    transMat = transMat.loc[maskX, maskY]\n    transMat = transMat.div(transMat.sum(axis=1), axis=0)\n    return transMat","ba2813e6":"seaborn.heatmap(makeHeatmap(dev, 'Methyltransf_25'))","fadd1848":"seaborn.heatmap(makeHeatmap(train, 'Lipase_GDSL_2'), vmax=0.5)","a550f4a2":"for name, partition in partitions.items():\n    partition.groupby('family_id').size().hist(bins=100)\n    plt.title('Distribution of minority family sizes for %s' % name)\n    plt.ylabel('# Families')\n    plt.xlabel('Family size')\n    plt.show()","0e0b126d":"train.groupby('family_id').size().hist(bins=[1,5,10,20,30,40,50])\nplt.title('Distribution of minority family sizes for train')\nplt.ylabel('# Families')\nplt.xlabel('Family size')\nplt.show()","bf6067f6":"train.groupby('family_id').size().hist(bins=[100,120,150,175,200,300,400,500,750,1000])\nplt.title('Distribution of majority family sizes for train')\nplt.ylabel('# Families')\nplt.xlabel('Family size')\nplt.show()","8db64137":"def makeSeqLenPlots(familyId=None):\n    if familyId != None:\n        famSeqsTrain = train.loc[train.family_id == familyId, 'sequence'].reset_index(drop=True)\n        famSeqsDev = dev.loc[dev.family_id == familyId, 'sequence'].reset_index(drop=True)\n        famSeqsTest = test.loc[test.family_id == familyId, 'sequence'].reset_index(drop=True)\n    else:\n        famSeqsTrain = train['sequence']\n        famSeqsDev = dev['sequence']\n        famSeqsTest = test['sequence']\n    # Length of sequence in train data for specific family (optional).\n    trainCharCount = famSeqsTrain.apply(len)\n    devCharCount = famSeqsDev.apply(len)\n    testCharCount = famSeqsTest.apply(len)\n\n    def plot_seq_count(count, data_name):\n        seaborn.distplot(count.values)\n        plt.title(f'Sequence char count: {data_name}')\n        plt.grid(True)\n\n    plt.subplot(1, 3, 1)\n    plot_seq_count(trainCharCount, 'Train')\n\n    plt.subplot(1, 3, 2)\n    plot_seq_count(devCharCount, 'Dev')\n\n    plt.subplot(1, 3, 3)\n    plot_seq_count(testCharCount, 'Test')\n\n    plt.subplots_adjust(right=3.0)\n    plt.show()","188c8492":"makeSeqLenPlots()","48cf0c54":"makeSeqLenPlots('Acetyltransf_7')","24afddad":"print(len(train['family_accession'].apply(lambda x: x.split('.')[0]).unique()))\nprint(len(train['family_id'].unique()))\nprint(len(train['family_accession'].unique()))","e981696d":"len(dev.family_accession.unique())","a6f7a371":"famSize = train.family_accession.value_counts()\n\n# Minimum sample size in training set for a class in dev set - all test labels are in train labels\nfamSize.loc[dev.family_accession.unique()].min()","611053bb":"#taking some families, by default, the 100 most common ones\nfamiliesOfInterest = train.family_accession.value_counts()[:100] #100","0fec3cda":"mask = train.family_accession.isin(familiesOfInterest.index.values)\ntrain = train.loc[mask,:]\n\nmask = dev.family_accession.isin(familiesOfInterest.index.values)\ndev = dev.loc[mask,:]\n\nmask = test.family_accession.isin(familiesOfInterest.index.values)\ntest = test.loc[mask,:]","ecbb6362":"valCounts = pd.concat([pd.DataFrame(train.family_accession.value_counts()[:20]), \n           pd.DataFrame(dev.family_accession.value_counts()[:20]), \n           pd.DataFrame(test.family_accession.value_counts()[:20])], \n          axis=1)\nvalCounts.columns = ['train samples', 'dev samples', 'test samples']\nvalCounts.plot.bar(figsize = (10,7), fontsize = 15, stacked=True)","577027d2":"makeSeqLenPlots()","cbc8b1c0":"print(len(train))\nprint(len(dev))\nprint(len(test))","78480eba":"# undersampling\n# train = train.groupby('family_accession').head(100)","8025d0f0":"!pip install tape_proteins","4891e857":"from tape import ProteinBertModel, TAPETokenizer","9b587346":"import torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","878ba096":"model = ProteinBertModel.from_pretrained('bert-base')\nmodelTAPE = model.to(device)\ntokenizerTAPE = TAPETokenizer(vocab='iupac')  # iupac is the vocab for TAPE models, use unirep for the UniRep model\n\nmodelTAPE.half()  # convert to half precision\nfor layer in modelTAPE.modules():\n    if isinstance(layer, torch.nn.BatchNorm2d):\n        layer.float()","d262edb4":"vocab_size = 25\nmax_length = 512 \ntrunc_type = \"post\"\npadding_type = \"post\"\n\ntrainTAPE = [tokenizerTAPE.encode(w.upper()) for w in train['sequence']]\nvalTAPE = [tokenizerTAPE.encode(w.upper()) for w in dev['sequence']]\ntestTAPE = [tokenizerTAPE.encode(w.upper()) for w in test['sequence']]","c2702a8b":"trainTAPE = [np.array(t[:max_length]) for t in trainTAPE]\nvalTAPE = [np.array(t[:max_length]) for t in valTAPE]\ntestTAPE = [np.array(t[:max_length]) for t in testTAPE]\n\ntrainTAPE = [torch.from_numpy(t) for t in trainTAPE]\nvalTAPE = [torch.from_numpy(t) for t in valTAPE]\ntestTAPE = [torch.from_numpy(t) for t in testTAPE]","186dfa8b":"TAPEtrain = torch.nn.utils.rnn.pad_sequence(trainTAPE, batch_first=False)\nTAPEvalidation = torch.nn.utils.rnn.pad_sequence(valTAPE, batch_first=False)\nTAPEtest = torch.nn.utils.rnn.pad_sequence(testTAPE, batch_first=False)","b3296242":"TAPEtrain = torch.transpose(TAPEtrain,0,1)\nTAPEvalidation = torch.transpose(TAPEvalidation,0,1)\nTAPEtest = torch.transpose(TAPEtest,0,1)","77af78b0":"TAPEvalidation.size()","34ef3037":"from torch.utils.data import DataLoader\nfrom tqdm import tqdm\nbatch_size = 8 #just reduce the batch size further if GPU is out of memory\n\ndataloader_train = DataLoader(\n    TAPEtrain,\n    batch_size=batch_size\n)\n\ndataloader_validation = DataLoader(\n    TAPEvalidation,\n    batch_size=batch_size\n)\n\ndataloader_test = DataLoader(\n    TAPEtest,\n    batch_size=batch_size\n)","db867593":"del trainTAPE, valTAPE, TAPEtrain, TAPEvalidation\ngc.collect()","80b539b7":"torch.cuda.empty_cache() ","2a1db917":"train_labels = train['family_accession'].apply(lambda x: x.split('.')[0])\nvalidation_labels = dev['family_accession'].apply(lambda x: x.split('.')[0])\ntest_labels = test['family_accession'].apply(lambda x: x.split('.')[0])","e0f14c90":"train_seq = train['sequence']\ndev_seq = dev['sequence']\ntest_seq = test['sequence']\n\ndel train, test, dev\ngc.collect()","4909e2b2":"TAPEncoded_train = []\nwith torch.no_grad():\n    for batch in tqdm(dataloader_train):\n        b = batch.to(torch.long)\n        output = modelTAPE(b.to(device))[1] #1 for pooled, 0 for all amino acids in peptide encoded\n        TAPEncoded_train.append(output.cpu().detach().numpy())\n        torch.cuda.empty_cache() \n        del b, output\n        gc.collect()","fb20fbe5":"# import pickle\n# path = \".\/\" + \"TAPEncoded_train_pooled\" + \".pickle\"\n# output = open(path, 'w+b')\n# pickle.dump(TAPEncoded_train, output)\n# output.close()","a16a6939":"TAPEncoded_val = []\nfor batch in tqdm(dataloader_validation):\n    b = batch.to(torch.long)\n    output = modelTAPE(b.to(device))[1]\n    TAPEncoded_val.append(output.cpu().detach().numpy())\n    torch.cuda.empty_cache() \n    del b, output\n    gc.collect()","8896a032":"TAPEncoded_test = []\nfor batch in tqdm(dataloader_test):\n    b = batch.to(torch.long)\n    output = modelTAPE(b.to(device))[1]\n    TAPEncoded_test.append(output.cpu().detach().numpy())\n    torch.cuda.empty_cache() \n    del b, output\n    gc.collect()","c5a531c8":"train_encoded = np.array(TAPEncoded_train[0])\nfor t in tqdm(TAPEncoded_train[1:]):\n    train_encoded = np.concatenate([train_encoded,t], axis=0)","8f81df13":"dev_encoded = np.array(TAPEncoded_val[0])\nfor t in tqdm(TAPEncoded_val[1:]):\n    dev_encoded = np.concatenate([dev_encoded,t], axis=0)","02b9eb58":"test_encoded = np.array(TAPEncoded_test[0])\nfor t in tqdm(TAPEncoded_test[1:]):\n    test_encoded = np.concatenate([test_encoded,t], axis=0)","72650cda":"dev_encoded.shape","80f722d7":"from tensorflow.keras.preprocessing.text import Tokenizer\n\nlabel_tokenizer = Tokenizer(oov_token = -1)\nlabel_tokenizer.fit_on_texts(train_labels)\n\ntraining_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\nvalidation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\ntest_label_seq = np.array(label_tokenizer.texts_to_sequences(test_labels))","2837c11d":"numclass = len(np.unique(training_label_seq))","80d5e732":"#labels should be 1 to num_classes, otherwise loss will be nan: https:\/\/github.com\/keras-team\/keras\/issues\/1244\ntraining_label_seq = training_label_seq-1\nvalidation_label_seq = validation_label_seq-1\ntest_label_seq = test_label_seq-1","467786e3":"del train_labels, validation_labels, test_labels\ngc.collect()","8ca4cb15":"import tensorflow as tf\ninput_x = tf.keras.layers.Input(shape=(768,)) #768 is the embedding dimension\nout = tf.keras.layers.Dense(numclass+1, activation=\"softmax\")(input_x) #if you don't add 1, loss will be nan: https:\/\/github.com\/keras-team\/keras\/issues\/1244\nmodel = tf.keras.Model(inputs=input_x, outputs=out)\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\nmodel.summary()","e120079b":"es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', min_delta=0.01, patience=5)\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(-epoch \/ 100))\nhistory = model.fit(train_encoded, \n                    training_label_seq, \n                    epochs=100, \n                    validation_data=(dev_encoded, validation_label_seq), \n                    verbose=2, \n                    callbacks=[es, lr_schedule])","cad340b8":"preds = model.predict(test_encoded)","2b42501b":"preds = np.argmax(preds,axis=1) ","546405b4":"sum(preds==test_label_seq.T[0])\/len(test_label_seq)*100","1a3e1f0d":"train_sentences = train_seq.apply(lambda seq: [aa for aa in seq])\nvalidation_sentences = dev_seq.apply(lambda seq: [aa for aa in seq])\ntest_sentences = test_seq.apply(lambda seq: [aa for aa in seq])","079ff9ea":"# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n# policy = mixed_precision.Policy('mixed_float16')\n# mixed_precision.set_policy(policy)","791c27b7":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import backend as K\n\nvocab_size = 25\nembedding_dim = vocab_size\nmax_length = 512\ntrunc_type = \"post\"\npadding_type = \"post\"\noov_tok = \"<OOV>\"\ntraining_portion = .8\n\ntokenizer = Tokenizer(oov_token=oov_tok, num_words = vocab_size)\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding=padding_type)\n\nvalidation_sequences = tokenizer.texts_to_sequences(validation_sentences)\nvalidation_padded = pad_sequences(validation_sequences, maxlen=max_length, padding=padding_type)\n\ntest_sequences = tokenizer.texts_to_sequences(test_sentences)\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding=padding_type)","42efcb90":"del train_sentences, train_sequences, validation_sentences, validation_sequences, test_sentences, test_sequences\ngc.collect()","8dc28310":"print(train_padded.shape)\nprint(validation_padded.shape)\nprint(test_padded.shape)","4d0ca881":"def residual_block(x, dil, filters):\n    shortcut = x\n    bn1 = tf.keras.layers.BatchNormalization()(x)\n    a1 = tf.keras.layers.Activation(\"relu\")(bn1)\n    conv1 = tf.keras.layers.Conv1D(filters, 3, dilation_rate = dil, padding=\"same\")(a1) #1100 filters and 31 kernel size in ProtCNN\n    \n    bn2 = tf.keras.layers.BatchNormalization()(conv1)\n    a2 = tf.keras.layers.Activation(\"relu\")(bn2)\n    conv2 = tf.keras.layers.Conv1D(filters, 1, padding=\"same\")(a2)\n    \n    x = tf.keras.layers.Add()([conv2, shortcut])\n    \n    return x","6b2b8c7b":"es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', min_delta=0.01, patience=5)\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-4 * 10**(-epoch \/ 100))","461144cb":"# # detect and init the TPU\n# tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n\n# # instantiate a distribution strategy\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","4798d51d":"# instantiating the model in the strategy scope creates the model on the TPU\n# with tpu_strategy.scope():\ninput_x = tf.keras.layers.Input(shape=(512,))\n#will run out of memory if to_categorical is used, so need to one-hot encode here\nx = tf.keras.layers.Embedding(vocab_size, vocab_size, embeddings_initializer=tf.keras.initializers.Identity(gain=1.0), trainable=False)(input_x)\nx = tf.keras.layers.Permute(dims=[2, 1])(x)\nx = tf.keras.layers.Conv1D(64, 8, padding=\"same\")(x)\nx = residual_block(x, 1, 64)\nx = residual_block(x, 2, 64)\nx = tf.keras.layers.Permute(dims=[2, 1])(x)\nx = tf.keras.layers.Conv1D(64, 3, padding=\"same\")(x)\nx = residual_block(x, 1, 64)\nx = residual_block(x, 2, 64) #4 blocks of these in ProtCNN\nx = tf.keras.layers.Lambda(lambda x: K.expand_dims(x, -1))(x) #will not compile if not defined as lambda\nx = tf.keras.layers.Conv2D(32, (4,4), padding=\"same\")(x)\nx = tf.keras.layers.Activation(\"relu\")(x)\nx = tf.keras.layers.Conv2D(8, (8,8), padding=\"same\")(x)\nx = tf.keras.layers.Activation(\"relu\")(x)\nx = tf.keras.layers.MaxPooling2D((64,64))(x)\n#x = tf.keras.layers.Conv1D(8, 8, padding=\"same\")(x)\n#x = tf.keras.layers.MaxPooling1D(64)(x)\nx = tf.keras.layers.Flatten()(x)\nout = tf.keras.layers.Dense(numclass+1, activation=\"softmax\")(x) #if you don't add 1, loss will be nan: https:\/\/github.com\/keras-team\/keras\/issues\/1244\n\nmodel = tf.keras.Model(inputs=input_x, outputs=out)\noptimizer = tf.keras.optimizers.Adam(lr=1e-5) #0.001 default learning rate was too large and learning was stuck\n\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\nmodel.summary()","2c424f84":"# with tpu_strategy.scope():\n#     input_x = tf.keras.layers.Input(shape=(512,))\n#     x = tf.keras.layers.Embedding(vocab_size, vocab_size, embeddings_initializer=tf.keras.initializers.Identity(gain=1.0), trainable=False)(input_x)\n#     x = tf.keras.layers.LSTM(64, return_sequences=True)(x)\n    \n#     x = tf.keras.layers.LSTM(64)(x)\n#     out = tf.keras.layers.Dense(numclass, activation=\"softmax\")(x)\n#     model2 = tf.keras.Model(inputs=input_x, outputs=out)\n#     model2.compile(loss='sparse_categorical_crossentropy',optimizer=\"adam\",metrics=['accuracy'])\n# model2.summary()","4992772c":"# history2 = model2.fit(train_padded, \n#                       training_label_seq, \n#                       epochs=100, \n#                       validation_data=(validation_padded, validation_label_seq), \n#                       callbacks = [lr_schedule],\n#                       verbose=2)","e01de064":"history = model.fit(train_padded, \n                    training_label_seq, \n                    epochs=100, \n                    validation_data=(validation_padded, validation_label_seq), \n                    callbacks = [es,lr_schedule], \n                    verbose=2)","fd1d3652":"preds = model.predict(test_padded)","c5df2230":"preds = np.argmax(preds,axis=1) ","024ee0b8":"sum(preds==test_label_seq.T[0])\/len(test_label_seq)*100","5caebede":"##### Let's make the architecture similar to ProtCNN's (see paper above) because it outperformed HMMER\n\nCreating the residual blocks with dilution","c1eba931":"##### Let's put a DNN on top to classify (just one layer, so really it's just logistic regression)","3ac37416":"##### Zooming in a little","39a2a3a9":"##### In conclusion we got around 95% accuracy on 100 similarly-sized classes from scratch which is pretty amazing! We used two different methods, and could've done a lot of others, for example:\n- HMMER\n- LSTM\n- k-NN in embedding dimension\n- k-NN with Levenshtein distance\n- BERT embedding for each amino acid (not pooled) and then ProtCNN (ResNet) layer on top instead of just DNN\n- Ensemble of models\n-...","a757bacd":"### Taking families of interest and optional traning size reduction via undersampling\n- Further analysis to check correspondence between family Id, accession and family version number\n- Reduced dataset analysis","791f56cb":"##### And for the test set","2be17a27":"##### Convert to half precision for faster speed","d122bbe0":"##### Not super tricky, i.e. \n- we have all test\/dev labels in train labels\n- every test\/dev family has at least 8 samples in training set","14cb5625":"##### Making predictions for test set","13210bbc":"##### Number of Distinct classes in dev set","71280d0a":"##### Let's see the histogram of sequence lengths in the reduced dataset","26ba9375":"##### Taking the class with the highest probability","f13daa9e":"##### Define batch sizes and dataloaders to we don't run out of GPU memory","b3d0ed7b":"##### Let's try our own method from scratch that doesn't rely on BERT and is quicker and see if we can get similar testing accuracy","4e519de0":"##### Again, we got around 95% accuracy on dev set!\n- Likely could be made better by avoiding the slight overfitting via regularization\/dropout\/model complexity reduction","0672647d":"##### To be able to run on Kaggle, let's reduce the data by considering the 100 most common families only","bf74805d":"### One Hot Encoding + Residual Nets","644a9d1e":"##### Let's pad the sequences","488097c6":"### Loading the data","981064dd":"##### Final set sizes","a5fdd596":"##### Let's try the BERT embedding from this famous paper:\n\n@inproceedings{tape2019,\nauthor = {Rao, Roshan and Bhattacharya, Nicholas and Thomas, Neil and Duan, Yan and Chen, Xi and Canny, John and Abbeel, Pieter and Song, Yun S},\ntitle = {Evaluating Protein Transfer Learning with TAPE},\nbooktitle = {Advances in Neural Information Processing Systems}\nyear = {2019}\n}\n\nhttps:\/\/github.com\/songlab-cal\/tape","b94b9fbe":"##### Let's check out the number of samples in each family","87397147":"##### Only consider up to the maximum length, set to 512 based off the sequence lengths histogram","e98ea675":"### Full Dataset Analysis","eb84803f":"##### Defining labels from zero","16ece88d":"##### We use a learning rate scheduler and early stopping\nThe 95% validation accuracy is quite nice","8a6bd168":"### BERT Embedding","669c7b2a":"##### Let's tokenize the sequences","d5ae579f":"##### For validation (dev)","23a38d61":"##### this is how the transition matrix heatmap looks like for the Methyltransf_25 family from the dev set","6be81ff9":"##### quick look at the most common families","f6654a4d":"##### Final metric: testing accuracy","90bfd056":"##### Let's get the pooled embeddings for each sequence (non-pooled would be more interesting but results in memory error in Kaggle)","997372e2":"##### Relatively complex architecture containing 1D as well as 2D convolutions and skip connection (Residual Net)\n\n- One-hot encoding for the amino acids\n- First 1D Convolution along amino acids to capture if the same amino acid is nearby\n- 1D convolution along ebedding dimension to check which amino acids are recurring close to each other\n- Residual blocks to include possible skips\n- 2D convolution to finish it off to combine resulting embedding dimension along the sequence\n- Pooling and flattening and finally a Dense layer for multiclass classification\n- Note: 2D Conv layer was added for fun, is not in ProtCNN","a349bd04":"##### and for the Lipase_GDSL_2 family from the train set","506416ab":"##### Final dev shape, embedding is in a 768-long vector space","017a3ba8":"##### Let's check the sequence length in the training\/dev\/test set","402afabc":"##### the version numbers are unique and we don't really care about them, so might as well just remove them","d43c15c0":"### Pooled BERT Embedding + Dense Neural Net","50f54d7e":"##### Check if GPU is available","fe47fb5f":"##### Version number is unique for each family and family ID is unique for each family accession","b6507c95":"##### Let's merge the batches for train\/dev\/test","62a8087e":"##### Model architecture - going simple, the hard work should've been done by the BERT encoding","98b1d2d0":"##### Need to transpose to have (sample, length)","cc2277ec":"## Pfam seed random split","2ffe3cb7":"#### Final metric: testing accuracy\n","c826731d":"##### Train\/dev\/test shapes","ab7382c1":"##### If we have a new protein, we don't have the sequence alignment to the family, since we don't know the family, we are trying to find it! So it'd be cheating to use the aligned sequences, hence we just use the sequences ","bd209ead":"##### based on the above, the best method to classify families is likely to be profile HMMs ... however, it is more interesting to create a deep learning model from scratch than running HMMER ... it is also possible for a deep learning model to surpass HMMER actually on Pfam! This was shown by Google research: \nUsing Deep Learning to Annotate the Protein Universe\nMaxwell L. Bileschi, David Belanger, Drew Bryant, Theo Sanderson, Brandon Carter, D. Sculley, Mark A. DePristo, Lucy J. Colwell\nbioRxiv 626507; doi: https:\/\/doi.org\/10.1101\/626507","80d8936a":"##### we should be able to observe a quite distinctive heatmap for each family, although families have been created using more sophisticated methods than can markov chains; they were created using profile HMMs, which models insertions and deletions and are based off the multiple sequence alignment. However, note that Pfam is not as simple as running HMMSearch on a seed sequence: \"Pfam entries are manually annotated with functional information from the literature where available.\"\nPfam: The protein families database in 2021: J. Mistry, S. Chuguransky, L. Williams, M. Qureshi, G.A. Salazar, E.L.L. Sonnhammer, S.C.E. Tosatto, L. Paladin, S. Raj, L.J. Richardson, R.D. Finn, A. Bateman\nNucleic Acids Research (2020) doi: 10.1093\/nar\/gkaa913","107a839c":"##### Let's check the distribution of the 20 most common families in train\/dev\/test","97dd9e57":"##### Use tensorflow's tokenizer and sequence padder","c9ba5b5c":"##### ... and we can also use this function to see the sequence lengths in a specific family","7c9b7433":"##### We could have also reduced the data by undersampling..."}}