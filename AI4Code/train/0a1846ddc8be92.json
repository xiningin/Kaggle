{"cell_type":{"e2b5ed02":"code","6de431c1":"code","c83490d0":"code","7255a1db":"code","5ae32640":"code","5fb02d2f":"code","4fcc8ce7":"code","603c73e6":"code","d14d053e":"code","99eff212":"code","28135c66":"code","5423f9ea":"code","ee134344":"code","2080ed99":"code","8b73e1be":"code","2375d711":"code","670e8b45":"code","856246eb":"code","80c8f209":"code","f6d77b58":"code","d0aa6510":"code","6cf92fef":"code","cfb0bbf7":"code","fd5124fa":"code","e36bf22d":"code","818a7224":"code","d37cb9a6":"code","afcc6166":"code","eb7228bb":"code","e9757cc9":"code","08bf9f85":"code","63f54af8":"code","183757d3":"code","5f0aa9f3":"code","d9c5939c":"code","621ca326":"code","f9e44ed1":"code","34a1dacd":"code","7afe6d25":"code","2f88c84b":"code","476c4943":"code","7b184ca4":"code","9b8ed243":"code","5426206b":"code","9415cc12":"code","b7409979":"code","f7e86c7a":"code","096c8afb":"code","e429d904":"code","dc1b8a7a":"code","db7be730":"code","5452492b":"code","cd7aab55":"code","71f73f2f":"code","5bbd49d7":"code","347c2779":"code","f89c2d35":"code","9b9528f2":"code","bdf9d1b6":"code","56df9ff3":"code","4f1c7dc8":"code","8484a349":"code","513355e1":"markdown","e303fada":"markdown","c70b9fe6":"markdown","67d70776":"markdown","8e5f993b":"markdown","3e6fba7d":"markdown","f96ee475":"markdown","2d0c1859":"markdown","cfc0c2bc":"markdown","3baad58d":"markdown","686dd845":"markdown","d1370473":"markdown","af7a02fc":"markdown","a8e50b27":"markdown","cd577b97":"markdown","cdebc6db":"markdown","e27f84a9":"markdown","ddf978de":"markdown","0935e74e":"markdown","11801137":"markdown","739d3e5b":"markdown","f35c85fe":"markdown","52cfb13d":"markdown","e9a716c5":"markdown","f5a47ebe":"markdown","0b49a5ec":"markdown","9f67e5a3":"markdown","0db700fd":"markdown","12ba0188":"markdown","b6df56a7":"markdown","7425de46":"markdown","fd766c86":"markdown","d3e50b44":"markdown","9c7c4f4c":"markdown","70600823":"markdown","2dda7a74":"markdown","3091efa0":"markdown","36e0970c":"markdown","35127298":"markdown","d37dd9a2":"markdown","2d90db2b":"markdown","9e2a5006":"markdown","c86d9d94":"markdown","0a722052":"markdown","f82bb80a":"markdown","104b2160":"markdown","46f01ab5":"markdown","70a0c800":"markdown","559685f4":"markdown"},"source":{"e2b5ed02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import LinearSVC\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6de431c1":"data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndata_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","c83490d0":"data.head(10)","7255a1db":"y = data[\"Survived\"]\nX = data.copy()\nX_test = data_test.copy()\nX_full = pd.concat([X, X_test])\nn = len(X)\nn_test = len(X_test)\nn_full = n + n_test","5ae32640":"print(f\"Size of X = {X.shape}\")\nprint(f\"Size of X_test = {X_test.shape}\")\nprint(f\"Size of X_full = {X_full.shape}\")\nprint(f\"n = {n}\")\nprint(f\"n_test = {n_test}\")\nprint(f\"n_full = {n_full}\")","5fb02d2f":"X.head()\nX_corr = X.drop(\"PassengerId\", axis=1).corr().abs().unstack().sort_values(ascending = False).reset_index()\nX_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n\nX_corr_nd = X_corr.drop(X_corr[X_corr['Correlation Coefficient'] == 1.0].index)","4fcc8ce7":"plt.figure(figsize=(8, 6))\nsns.heatmap(X.drop(['PassengerId'], axis=1).corr(), annot=True, square=True, cmap='coolwarm', annot_kws={'size': 14}, vmin=-1, vmax=1)","603c73e6":"(X_full.drop([\"Survived\"], axis = 1).isna().sum() \/ n_full ) * 100","d14d053e":"df_median_age = X.groupby(\"Sex\").median().drop([\n    \"PassengerId\",\n    \"Survived\",\n    \"Pclass\",\n    \"SibSp\",\n    \"Parch\",\n    \"Fare\"\n], axis = 1)\n\ndf_median_age.head(10)","99eff212":"df_median_age_class = X.groupby([\"Sex\", \"Pclass\"]).median().drop([\n    \"PassengerId\",\n    \"Survived\",\n    \"SibSp\",\n    \"Parch\",\n    \"Fare\"\n], axis = 1).transpose()\n\ndf_median_age_class.head(15)","28135c66":"\ndef display_age():\n    fig, axs = plt.subplots(ncols=2, nrows=1, figsize=(10, 5))\n\n    ship_classes = df_median_age_class.columns.levels[1]\n    median_age = {\"female\": [],\n               \"male\": []}\n    \n    for i, k in enumerate(median_age.keys()):\n        median_age[k] = [df_median_age_class[k][c][0] for c in ship_classes]\n        sns.barplot(x = ship_classes, y = median_age[k], ax = axs[i])\n        axs[i].set_xlabel(\"{}\".format(\"Pclass\"))\n        axs[i].set_ylabel(\"Median Age for gender {}\".format(k))    \n        \n    plt.show()\n    \ndisplay_age()","5423f9ea":"def fill_na_age(row):\n    row[\"Age\"] = df_median_age_class[row[\"Sex\"]][row[\"Pclass\"]][0] if pd.isnull(row[\"Age\"]) else row[\"Age\"]\n    return row\n\nX_full = X_full.apply(fill_na_age, axis = 1)\nX_full.info()","ee134344":"df_median_fare_class = X.groupby([\"Pclass\"]).median().drop([\n    \"PassengerId\",\n    \"Survived\",\n    \"SibSp\",\n    \"Parch\",\n    \"Age\"\n], axis = 1).transpose()\n\ndf_median_fare_class.head()","2080ed99":"def display_fare():\n    plt.figure(figsize=(8, 6))\n\n    ship_classes = df_median_fare_class.columns\n    median_fare = [df_median_fare_class[c][0] for c in ship_classes]\n    sns.barplot(x = ship_classes, y = median_fare)\n    plt.xlabel(\"{}\".format(\"Pclass\"))\n    plt.ylabel(\"Median Fare\")    \n        \n    plt.show()\n\n\ndisplay_fare()","8b73e1be":"print(X_full[X_full[\"Fare\"].isnull()])","2375d711":"def fill_na_fare(row):\n    row[\"Fare\"] = df_median_fare_class[row[\"Pclass\"]][0] if pd.isnull(row[\"Fare\"]) else row[\"Fare\"]\n    return row\n\nX_full = X_full.apply(fill_na_fare, axis = 1)\nX_full.info()\nprint(X_full[X_full[\"PassengerId\"] == 1044])","670e8b45":"X_full[X_full[\"Embarked\"].isnull()]","856246eb":"embarked_survived = X.groupby([\"Embarked\", \"Survived\"])[\"PassengerId\"].count()\nembarked_survived.head(10)","80c8f209":"def display_embarked_prob():\n    surv_counts = {\n        \"C\": {}, \n        \"Q\": {}, \n        \"S\": {}\n    }\n    \n    classes = set(embarked_survived.index.get_level_values(0))  \n    \n    for c in classes:\n        for survive in range(0, 2):\n            surv_counts[str(c)][survive] = embarked_survived[c][survive]\n            \n    df_surv = pd.DataFrame(surv_counts)\n    surv_percentages = {}\n\n    for col in df_surv.columns:\n        surv_percentages[col] = [(count \/ df_surv[col].sum()) * 100 for count in df_surv[col]]\n        \n    df_survived_percentages = pd.DataFrame(surv_percentages).transpose()\n    class_names = (\"C\", \"Q\", \"S\")\n    bar_count = np.arange(len(class_names))  \n    bar_width = 0.5\n    \n    not_survived = df_survived_percentages[0]\n    survived = df_survived_percentages[1]\n    \n    plt.figure(figsize=(10, 6))\n    plt.bar(bar_count, not_survived, color='#E8B100', edgecolor='white', width=bar_width, label=\"Not Survived\")\n    plt.bar(bar_count, survived, bottom=not_survived, color='#2AB100', edgecolor='white', width=bar_width, label=\"Survived\")\n    \n    plt.xlabel('Embarked', size=15, labelpad=20)\n    plt.ylabel('Survival Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, class_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Survival Percentage by Embarked', size=18, y=1.05)\n    \n    plt.show()\n\ndisplay_embarked_prob()","f6d77b58":"embarked_class_passengers = X.groupby(\"Pclass\")[\"Embarked\"].value_counts()\nprint(embarked_class_passengers)","d0aa6510":"import random\n\ndef fill_na_embarked(row):\n    row_class = row[\"Pclass\"]\n    total_class_passengers = embarked_class_passengers[row_class].sum()\n    class_embarked_dist = embarked_class_passengers[row_class].cumsum() \/ total_class_passengers\n    r = random.uniform(0, 1)\n    \n    if pd.isnull(row[\"Embarked\"]):\n        for port in class_embarked_dist.index:\n            if r <= class_embarked_dist[port]:\n                row[\"Embarked\"] = port\n                break\n    \n    return row\n\nX_full = X_full.apply(fill_na_embarked, axis = 1)\nX_full.info()\nX_full[X_full[\"PassengerId\"].isin([62, 830])]","6cf92fef":"X[\"Cabin\"].sample(10)","cfb0bbf7":"X_full[\"CabinId\"] = X_full[\"Cabin\"].map(lambda cabin: cabin if pd.isnull(cabin) else cabin[0])\nX[\"CabinId\"] = X[\"Cabin\"].map(lambda cabin: cabin if pd.isnull(cabin) else cabin[0])\nX[\"CabinId\"].sample(10)\n\n","fd5124fa":"plt.figure(figsize=(8, 6))\nsns.countplot(x=\"CabinId\", hue='Survived', data=X)\n\nplt.xlabel('{}'.format(\"CabinId\"), size=20, labelpad=15)\nplt.ylabel('Passenger Count', size=20, labelpad=15)    \nplt.tick_params(axis='x', labelsize=20)\nplt.tick_params(axis='y', labelsize=20)\n    \nplt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\nplt.title('Count of Survival in CabinId Feature', size=20, y=1.05)","e36bf22d":"X.groupby([\"CabinId\", \"Pclass\"])[\"PassengerId\"].count()","818a7224":"class_cabin_passengers = X.groupby([\"Pclass\", \"CabinId\"])[\"PassengerId\"].count()\nprint(class_cabin_passengers)","d37cb9a6":"import random\n\ndef fill_na_cabin_id(row):\n    row_class = row[\"Pclass\"]\n    total_class_passengers = class_cabin_passengers[row_class].sum()\n    class_cabin_dist = class_cabin_passengers[row_class].cumsum() \/ total_class_passengers\n    rand = random.uniform(0, 1)\n    \n    if pd.isnull(row[\"CabinId\"]):\n        for cabinId in class_cabin_dist.index:\n            if rand <= class_cabin_dist[cabinId]:\n                row[\"CabinId\"] = cabinId\n                break\n    \n    return row\n\nX_full = X_full.apply(fill_na_cabin_id, axis = 1)\nX_full.info()\nX_full[[\"Pclass\", \"CabinId\"]].sample(10)","afcc6166":"survival_by_gender = X.groupby(\"Sex\").apply(lambda df: sum(df[\"Survived\"] == 1))\nsurvival_by_gender","eb7228bb":"plt.figure(figsize=(8, 6))\nsns.countplot(x=\"Sex\", hue='Survived', data=X)\n\nplt.xlabel('{}'.format(\"Sex\"), size=20, labelpad=15)\nplt.ylabel('Passenger Count', size=20, labelpad=15)    \nplt.tick_params(axis='x', labelsize=20)\nplt.tick_params(axis='y', labelsize=20)\n    \nplt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\nplt.title('Count of Survival in Sex Feature', size=20, y=1.05)","e9757cc9":"survival_by_gender[\"female\"] \/ (survival_by_gender[\"female\"] + survival_by_gender[\"male\"])","08bf9f85":"survival_by_class = X.groupby(\"Pclass\")[\"Survived\"].apply(lambda x: {\"Survived\": sum(x == 1), \"Not Survived\": sum(x == 0)})\nsurvival_by_class","63f54af8":"plt.figure(figsize=(10, 8))\nsns.countplot(x=\"Pclass\", hue='Survived', data=X)\n\nplt.xlabel('{}'.format(\"Pclass\"), size=20, labelpad=15)\nplt.ylabel('Passenger Count', size=20, labelpad=15)    \nplt.tick_params(axis='x', labelsize=20)\nplt.tick_params(axis='y', labelsize=20)\n    \nplt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 18})\nplt.title('Count of Survival in Pclass Feature', size=20, y=1.05)","183757d3":"gender_by_class = X.groupby(\"Pclass\")[\"Sex\"].apply(lambda x: {\"Male\": sum(x == \"male\"), \"Female\": sum(x == \"female\")})\ngender_by_class","5f0aa9f3":"plt.figure(figsize=(10, 8))\nsns.countplot(x=\"Pclass\", hue='Sex', data=X)\n\nplt.xlabel('{}'.format(\"Pclass\"), size=20, labelpad=15)\nplt.ylabel('Passenger Count', size=20, labelpad=15)    \nplt.tick_params(axis='x', labelsize=20)\nplt.tick_params(axis='y', labelsize=20)\n    \nplt.legend(['Male', 'Female'], loc='upper center', prop={'size': 18})\nplt.title('Count of Gender in Pclass Feature', size=20, y=1.05)","d9c5939c":"df_class_survived = X.groupby(['Pclass', \"Survived\", \"Sex\"]).count().drop(columns=[\n    'Age', \n    'SibSp', \n    'Parch', \n    'Fare', \n    'Embarked',\n    'Cabin', \n    'PassengerId', \n    'Ticket']).rename(columns={'Name':'Count'}).transpose()\n\ndf_class_survived.head(15)","621ca326":"def display_gender_by_class_prob(gender):\n    surv_counts = {\n        \"1\": {}, \n        \"2\": {}, \n        \"3\": {}\n    }\n    \n    classes = df_class_survived.columns.levels[0]    \n    \n    for c in classes:\n        for survive in range(0, 2):\n            surv_counts[str(c)][survive] = df_class_survived[c][survive][gender][0]\n            \n    df_surv = pd.DataFrame(surv_counts)\n    surv_percentages = {}\n\n    for col in df_surv.columns:\n        surv_percentages[col] = [(count \/ df_surv[col].sum()) * 100 for count in df_surv[col]]\n        \n    df_survived_percentages = pd.DataFrame(surv_percentages).transpose()\n    class_names = (\"1\", \"2\", \"3\")\n    bar_count = np.arange(len(class_names))  \n    bar_width = 0.5\n    \n    not_survived = df_survived_percentages[0]\n    survived = df_survived_percentages[1]\n    \n    plt.figure(figsize=(10, 6))\n    plt.bar(bar_count, not_survived, color='#E8B100', edgecolor='white', width=bar_width, label=\"Not Survived\")\n    plt.bar(bar_count, survived, bottom=not_survived, color='#2AB100', edgecolor='white', width=bar_width, label=\"Survived\")\n    \n    plt.xlabel('Pclass', size=15, labelpad=20)\n    plt.ylabel('Survival Percentage', size=15, labelpad=20)\n    plt.xticks(bar_count, class_names)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(loc='upper left', bbox_to_anchor=(1, 1), prop={'size': 15})\n    plt.title('Survival Percentage by Class for gender {}'.format(gender), size=18, y=1.05)\n    \n    plt.show()\n\ndisplay_gender_by_class_prob(\"female\")\ndisplay_gender_by_class_prob(\"male\")","f9e44ed1":"numerical_columns = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\ncategorical_columns = [\"Pclass\", \"Embarked\", \"Sex\"]\nfeature_columns = numerical_columns + categorical_columns\n\nX = X[feature_columns]\nX_test = X_test[feature_columns]\nX.head()","34a1dacd":"# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                      train_size=0.7, test_size=0.3,\n                                                      random_state=0)","7afe6d25":"X_train.shape","2f88c84b":"X_train.isna().sum() # How many NA we have per columns. ","476c4943":"X_train.isna().sum() # How many NA we have per columns. ","7b184ca4":"X_train[\"Age\"].median()","9b8ed243":"X_train[\"Age\"].mean()","5426206b":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='mean')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])","9415cc12":"# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_columns),\n        ('cat', categorical_transformer, categorical_columns)\n    ])    ","b7409979":"# Define the model\nmodel = RandomForestClassifier(n_estimators = 150, max_depth = 5, random_state=0)","f7e86c7a":"# Bundle preprocessing and modeling code in a pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', model)\n])","096c8afb":"# Preprocessing of training data, fit model \npipeline.fit(X_train, y_train)","e429d904":"# Preprocessing of validation data, get predictions\npreds = pipeline.predict(X_valid)\npreds","dc1b8a7a":"np.array(y_valid)","db7be730":"accuracy_score(y_valid, preds)","5452492b":"recall_score(y_valid, preds)","cd7aab55":"data_test.head()","71f73f2f":"preds_test = pipeline.predict(X_test)\npreds_test.shape","5bbd49d7":"# Save test predictions to file\noutput = pd.DataFrame({\"PassengerId\": data_test[\"PassengerId\"],\n                       \"Survived\": preds_test})\noutput.head()","347c2779":"output.to_csv('submission.csv', index=False)","f89c2d35":"# Define the linear regression model\nlr_model = LogisticRegression(random_state=0)\nlr_pipeline = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('model', lr_model)\n])\n\n# Preprocessing of training data, fit model \nlr_pipeline.fit(X_train, y_train)\n\nlr_preds = lr_pipeline.predict(X_valid)\n\naccuracy_score(y_valid, lr_preds)\n","9b9528f2":"lr_preds_test = lr_pipeline.predict(X_test)\n# Save test predictions to file\nlr_output = pd.DataFrame({\"PassengerId\": data_test[\"PassengerId\"],\n                       \"Survived\": lr_preds_test})\n\nlr_output.to_csv('lr_submission.csv', index=False)","bdf9d1b6":"# Define the linear regression model\nxgb_model = XGBClassifier(n_estimators = 1000, learning_rate = 0.1, max_depth = 5, random_state=0)\nxgb_pipeline = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('model', xgb_model)\n])\n\n# Preprocessing of training data, fit model \nxgb_pipeline.fit(X_train, y_train)\n\nxgb_preds = xgb_pipeline.predict(X_valid)\n\naccuracy_score(y_valid, xgb_preds)","56df9ff3":"xgb_preds_test = xgb_pipeline.predict(X_test)\n# Save test predictions to file\nxgb_output = pd.DataFrame({\"PassengerId\": data_test[\"PassengerId\"],\n                       \"Survived\": xgb_preds_test})\n\nxgb_output.to_csv('xgb_submission.csv', index=False)","4f1c7dc8":"# Define the linear regression model\nsvm_model = LinearSVC(dual = False, random_state=0)\nsvm_pipeline = Pipeline(steps = [\n    ('preprocessor', preprocessor),\n    ('model', svm_model)\n])\n\n# Preprocessing of training data, fit model \nsvm_pipeline.fit(X_train, y_train)\n\nsvm_preds = svm_pipeline.predict(X_valid)\n\naccuracy_score(y_valid, svm_preds)","8484a349":"svm_preds_test = svm_pipeline.predict(X_test)\n# Save test predictions to file\nsvm_output = pd.DataFrame({\"PassengerId\": data_test[\"PassengerId\"],\n                       \"Survived\": svm_preds_test})\n\nsvm_output.to_csv('svm_submission.csv', index=False)","513355e1":"Split the training data in 80% for training and 20% for validation","e303fada":"### Fare\n\nFare is highly correlated to the class, which makes sense, higher class, more expensive tickets. There is also a correlation with the family size, but not as strong as with the class variable. ","c70b9fe6":"We will use **ColumnTransformer** to modify our columns with the pre-processing steps we just defined","67d70776":"The only passenger with missing is passenger with ```id = 1044```, which was placed in $3^{rd}$ class. ","8e5f993b":"### XGBoost\n\nLet's try the XGBoost model, which is also an ensemble model","3e6fba7d":"The *CabinId* column has no NaN values, and according to the sample above, seems reasonable the cabin assignment according to the passenger class.","f96ee475":"### Logistic Regression\n\nI don't expect that Logistic Regression give us better results than Random Forest, since I understand that Logistic Regression works better for linear relationships, but maybe since there is a strong correlation between Gender and Survival, we can get some good results as well.","2d0c1859":"After submitting the output obtained using the Logistic Regression method we obtained an accuracy of 76.55%. Which is close to our best result of 78.8% obtained using the Random Forest method.","cfc0c2bc":"> ### Correlations\n\nLet see how variables are related to each other. If two variables are correlated we can use that in our advantage to perform a better feature engineering and fill NaN values.","3baad58d":"### Categorical Features\n\nLet's split the data by gender and see how many survivers from each gender ","686dd845":"## Clean Data\n\nNow that we have a better idea of the kind of data we have, we will proceed to clean that data and define some pre-processing steps to do that.\n\n* For imputation of numerical columns, in this case only Age has NaN values, we will use the mean of the values in the Age column to fill those values.\n* For  the Embarked column there are only 2 missing values, so we will fill it with the most repeated value, and then we will use the *OneHotEncoder* to translate that column into integer values that can be interpreted by our model.","d1370473":"Now let see how the classes are distributed according to the embarked port. Accordinf to the list bellow, most passengers embarked on port S (Southampton), we can assign weights to differents ports depending on the class and then randomly assign a port according to the computed weights. For example we have that for class 1, around 59% of passengers embarked on port S, 40% on port C, and 1% on port Q. We can get a random number $r$ between $[0,1]$, and if $r \\leq 0.59$ we assign it to port S, if $r \\leq 0.99$ we assign it to port C, otherwise assign it to port Q. ","af7a02fc":"\n#### Age\n\nWe can fill the age missing values with the median, but not the median of the whole passengers. Instead of that we will analyze the median age for genders and for classes.","a8e50b27":"* Cabins A, B, C there were only passengers of class 1, with cabin C having the most of them. \n* Passengers of class 1 were also placed on cabins D and E, and they are majorith on those cabins.\n* Cabin D is conformed by passengers of classes 1 (29) and 2 (4).\n* Cabin E id conformed by passengers of classes 1 (25), 2 (4), and 3 (3)\n* Cabin F had passengers of classes 2, and 3.\n* Cabin G only had passengers of class 3.\n* Cabin T has only one passenger, and is fron class 1. Perhaps a special guest, why is not located on cabins A, B, C with the rest of class 1 passengers? \n\nWith this data we will assign a passenger with NaN as CabinId, a new CabinId depending on the class. Following the same process when we filled NaN values on the Embarked column, we will create a distribution based on weights for each class in order to assign a new CabinId.   ","cd577b97":"It makes sense that depending on the class, passengers were located in some cabins. Let's see if that is true. ","cdebc6db":"And get the results for our validation data. We printed both, the results from our model and the target value of the validation data. At first glance is difficult to see a difference, which is good, a sign that our model is not doing weird stuff.","e27f84a9":"From the 491 passengers from 3rd class, 24% survived, from 2nd class 47% survived, and from 1st class 63% surivived. Based on these numbers, it seems that being on the 1st class give it better chances for survival. Perhaps there were more males in 3rd class, let'see how many males and females per class were in the ship.","ddf978de":"Both passengers with NaN as Embarked port, have been assigned a port, in this case, port S, if we set those values to NaN and apply the ```fill_na_embarked``` function again we can end up with different values. ","0935e74e":"Both passengers were on first class, and both survived. Also they had the same ticket number, so they should traveled together.","11801137":"Get the target variable to be the *Survived* column, and remove that column from the training data.","739d3e5b":"From the table and the bar plots we can see that men tends to be older than women, and also that higher the class, older the person. So we can define now an strategy to fill the missing values for age based on gender and class.","f35c85fe":"Now let's predict the values from the test data","52cfb13d":"# Titanic: Machine Learning from Disaster\n\nIn this document we will try to come up with a model for the Titanic problem. https:\/\/www.kaggle.com\/c\/titanic\/overview. In few words, the objective is given information about the passengers in the Titanic, our goal is to predict if a passenger surived or not, given some information about a passenger.\n\nThe first step is to load the libraries that we will need for this exercise.","e9a716c5":"Let's see how many NaN values has our training data and validation data.","f5a47ebe":"Now we train the model  ","0b49a5ec":"## Data Distribution\n\nIn order to have a better understanding of the data we will analyze how numeric and non-numeric data is distributed, and how different variables interact with each other.","9f67e5a3":"We can see now that the there are no missing values in the **Fare** column and that the passenger with missing Fare now has a Fare value of $8.05$, which is the median of the fare for passengers in class 3.","0db700fd":"So far it seems that being on 1st clas or 2nd class gives you some advantage for survival, and also being a woman gives you more chances for survival. Next we will show the survival rate for each gender and for each class.","12ba0188":"Create a csv file with the results for submission.","b6df56a7":"#### Cabin\n\nFor the *Cabin* columns, more than 77% of the data is missing, so we are tempted to just ignore this column for our model. But it makes sense that cabin location in the ship could be a survival factor, since cabins located in the lower levels of the ship should had less chanced to survive, since they were the first to flood. So based on this, let's take a look to the few data that we have.","7425de46":"According to the graph bellow, the passengers that embarked in port C (Cherbourg), had more chances to survive.","fd766c86":"There are different models we can train for classification, some that come to my mind are:\n\n* Random Forest\n* Logistic Regression\n* Support Vector Machines\n\nAnd of course we can use more robust models like Neural Networks. Even when I'm familiared with the math of the models just listed, I don'thave too much experience yet, with Kaggle competitions, so we will try some of them and see what we can learn.","d3e50b44":"### Random Forest Classifier\n\nRandom forest is an ensemble method (is model built for more models), and what it does is basically a concensus between multiple decission trees to get a result. We can define some charactericts, we will focus in two for the moment.\n\n* **n_estimators.** Represents the number of trees in our forest.\n* **max_depth.** The maximum depth of a tree. Deeper trees can led us to overfitting, and a small depth can result in underfitting.","9c7c4f4c":"A 82% accuracy and 70% of recall is not bad. ","70600823":"Our training data has 891 rows","2dda7a74":"The results were very interesting, it seems that a female in 1st and 2nd class had good opportunities to survive, more than 90% chance. On the other hand a woman in the 3rd class had 50 - 50 chance to survive. \n\nOn the other hand, men in 1st class apparently had better chances to survive than men in the other classes, with a survival rate of around 34%. ","3091efa0":"68% of the surivers were females, which means that there is a strong correlation between surivival and gender. Now let's see just by curiosity if the people in higher classes had more opportunities to surive or not. Something that I remember from the movie Titanic, is that people in lower classes were placed in the lowest levels of the ship, make it more difficult for them to escape.","36e0970c":"The 77% of the *Cabin* column has NaN values, *Age* is the next with 20% and *Fare* and *Embarked* with less than 1%. We need to define a strategy to fill those NaN values. ","35127298":"### Missing Values\n\nIs important to see if there are NaN values in both, the training set and the test set. For this we will use ```X_full``` and calculate the percentage of NaN values for every column.","d37dd9a2":"## Exploratory Data Analysis (EDA)\n\nLet's take a look to our data. We can notice some few things from just looking at the first 10 passengers. \n\n* There are some NaN values in the *Age* column and in the *Cabin* column. \n* All passengers that survived were females\n* *Ticket* column looks is not related with survival.\n* It looks like *Cabin* has a lot of NaN values, which it can be a candidate for removal","2d90db2b":"## Define Model","9e2a5006":"70% of the passengers on 3rd class were males, which can be the reason of why there were less survivers in 3rd class.","c86d9d94":"1. The accuracy was 77[](http:\/\/).7%, which is similar to our 82%, The ranking in the scoreboard is 2654 of 20321, which take us to the top 13%. Not bad for our first model. To uderstand the importance and fragility of the pre-processing step, Lets use \"median\" to replace Age missing values and \"constant\" for Embarked missing values and got 78.2% of accuracy ","0a722052":"1. Seems that cabin number is composed by a letter and a number, perhaps the letter can mean something, let's compare the relation between cabin letter and survival. But first we will create a new feature called \"CabinId\" that will contain only the initial letter of the cabin number.","f82bb80a":"#### Embarked\n\nAccording to the summary of our data there are two missing values on the **Embarked** column. Let's take a look to those values.","104b2160":"The default value for *n_estimators* is 100, we used cross validation (will not go into this here), to get a better estimation for this parameter and 150 was slighlty better. And a value of *max_depth = 5* makes given the number of features in out data, this value was empirically found.  \n\nWe define our pipeline with the pre-processing and modeling steps. This is useful since we don't have to remember all the pre-processing steps for our validation data or test data, the pipeline will take care of everything for us.","46f01ab5":"## Load Data\n\nRead the training data and test data. The training data contains the **Survived** column, but the test data doesn't, that is because that is exactly the output we need to get from out model. We will predict if passengers in the test data survived or not, and submit the results.","70a0c800":"An accuracy if 80% of the validation set is not bad, but is not better that the one we obtained using the Random Forest Regression. ","559685f4":"Looking at the graph bellow and with our limited data, seems that cabin has indeed influence on the survival. Having cabins B, D, and E better chances to survive than the others. "}}