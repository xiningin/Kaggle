{"cell_type":{"a302ff23":"code","a9c984af":"code","1bcd754f":"code","acdf543a":"code","4ea46cf8":"code","44ac31f5":"code","f57b0b5b":"code","591f90ca":"code","85a5ff98":"code","8218bd3a":"code","9dbee87e":"code","51d5f621":"code","c01d164d":"code","7a6fe940":"code","cf6f36e1":"code","b4871e60":"code","d0de03b9":"code","149f85d3":"code","00afd5c2":"code","ff89b311":"markdown","ded126ce":"markdown","66c3e330":"markdown","0c1918bf":"markdown","f2239c5a":"markdown","caa5d693":"markdown","712c0f0a":"markdown","73bc1ec2":"markdown","204c1c30":"markdown","6488ffe6":"markdown","f5e61611":"markdown","ac4eb7f4":"markdown","f033d1da":"markdown","e592b83b":"markdown","03d73055":"markdown","86b0fa0e":"markdown"},"source":{"a302ff23":"\"\"\"\nTo install a library, run the following command: \n!pip install library_name\n\"\"\"\nimport warnings\nwarnings.filterwarnings('ignore') # to avoid warnings\n\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\"\"\"\nSklearn Libraries\n\"\"\"\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\n\"\"\"\nTransformer Libraries\n\"\"\"\nfrom transformers import BertTokenizer,  AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n\n\"\"\"\nPytorch Libraries\n\"\"\"\nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset","a9c984af":"def show_headline_distribution(sequence_lengths, figsize = (15,8)):\n    \n    # Get the percentage of reviews with length > 512\n    len_512_plus = [rev_len for rev_len in sequence_lengths if rev_len > 512]\n    percent = (len(len_512_plus)\/len(sequence_lengths))*100\n    \n    print(\"Maximum Sequence Length is {}\".format(max(sequence_lengths)))\n    \n    # Configure the plot size\n    plt.figure(figsize = figsize)\n\n    sns.set(style='darkgrid')\n    \n    # Increase information on the figure\n    sns.set(font_scale=1.3)\n    \n    # Plot the result\n    sns.distplot(sequence_lengths, kde = False, rug = False)\n    plt.title('Headlines Lengths Distribution')\n    plt.xlabel('Headlines Length')\n    plt.ylabel('Number of Headlines')\n    \n    \ndef show_random_headlines(total_number, df):\n    \n    # Get the random number of reviews\n    n_reviews = df.sample(total_number)\n    \n    # Print each one of the reviews\n    for val in list(n_reviews.index):\n        print(\"Reviews #\u00b0{}\".format(val))\n        print(\" - Sentiment: {}\".format(df.iloc[val][\"sentiment\"]))\n        print(\" - News Headline: {}\".format(df.iloc[val][\"NewsHeadline\"]))\n        print(\"\")\n        \ndef get_headlines_len(df):\n    \n    headlines_sequence_lengths = []\n    \n    print(\"Encoding in progress...\")\n    for headline in tqdm(df.NewsHeadline):\n        encoded_headline = finbert_tokenizer.encode(headline, \n                                         add_special_tokens = True)\n        \n        # record the length of the encoded review\n        headlines_sequence_lengths.append(len(encoded_headline))\n    print(\"End of Task.\")\n    \n    return headlines_sequence_lengths\n\n\ndef encode_sentiments_values(df):\n    \n    possible_sentiments = df.sentiment.unique()\n    sentiment_dict = {}\n    \n    for index, possible_sentiment in enumerate(possible_sentiments):\n        sentiment_dict[possible_sentiment] = index\n    \n    # Encode all the sentiment values\n    df['label'] = df.sentiment.replace(sentiment_dict)\n    \n    return df, sentiment_dict\n\ndef f1_score_func(preds, labels):\n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(labels_flat, preds_flat, average='weighted')\n\ndef accuracy_per_class(preds, labels):\n    label_dict_inverse = {v: k for k, v in sentiment_dict.items()}\n    \n    preds_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n\n    for label in np.unique(labels_flat):\n        y_preds = preds_flat[labels_flat==label]\n        y_true = labels_flat[labels_flat==label]\n        print(f'Class: {label_dict_inverse[label]}')\n        print(f'Accuracy: {len(y_preds[y_preds==label])}\/{len(y_true)}\\n')\n        \n        \ndef evaluate(dataloader_val):\n\n    model.eval()\n    \n    loss_val_total = 0\n    predictions, true_vals = [], []\n    \n    for batch in dataloader_val:\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }\n\n        with torch.no_grad():        \n            outputs = model(**inputs)\n            \n        loss = outputs[0]\n        logits = outputs[1]\n        loss_val_total += loss.item()\n\n        logits = logits.detach().cpu().numpy()\n        label_ids = inputs['labels'].cpu().numpy()\n        predictions.append(logits)\n        true_vals.append(label_ids)\n    \n    loss_val_avg = loss_val_total\/len(dataloader_val) \n    \n    predictions = np.concatenate(predictions, axis=0)\n    true_vals = np.concatenate(true_vals, axis=0)\n            \n    return loss_val_avg, predictions, true_vals","1bcd754f":"path_to_file = \"..\/input\/financialnewsheadline\/FinancialNewsHeadline.csv\"\nfinancial_data = pd.read_csv(path_to_file, encoding='latin-1', names=['sentiment', 'NewsHeadline'])\n\nfinancial_data.head()","acdf543a":"print(\"Data shape: {}\".format(financial_data.shape))\nprint(\"\\nSentiment distribution: {}\".format(financial_data.sentiment.value_counts()))","4ea46cf8":"# Configure the plot size\nplt.figure(figsize = (15,8))\n\nsns.set(style='darkgrid')\n    \n# Increase information on the figure\nsns.set(font_scale=1.3)\nsns.countplot(x='sentiment', data = financial_data)\nplt.title('News Sentiment Distribution')\nplt.xlabel('News Polarity')\nplt.ylabel('Number of News')","44ac31f5":"# Show 3 random headlines\nshow_random_headlines(5, financial_data)","f57b0b5b":"# Encode the sentiment column\nfinancial_data, sentiment_dict = encode_sentiments_values(financial_data)\n\nfinancial_data.head()","591f90ca":"# Create training and validation data\nX_train, X_val, y_train, y_val = train_test_split(financial_data.index.values, \n                                                  financial_data.label.values, \n                                                  test_size = 0.15, \n                                                  random_state = 2022, \n                                                  stratify = financial_data.label.values)","85a5ff98":"# Create the data type columns\nfinancial_data.loc[X_train, 'data_type'] = 'train'\nfinancial_data.loc[X_val, 'data_type'] = 'val'\n\n# Vizualiez the number of sentiment occurence on each type of data\nfinancial_data.groupby(['sentiment', 'label', 'data_type']).count()","8218bd3a":"# Get the FinBERT Tokenizer\nfinbert_tokenizer = BertTokenizer.from_pretrained(\"ProsusAI\/finbert\", \n                                          do_lower_case=True)","9dbee87e":"headlines_sequence_lengths = get_headlines_len(financial_data)","51d5f621":"'''\n# Show the reviews distribution \nThe overall implementation of this function is in my notebook at end of the article\n'''\nshow_headline_distribution(headlines_sequence_lengths)","c01d164d":"\n# Encode the Training and Validation Data\nencoded_data_train = finbert_tokenizer.batch_encode_plus(\n    financial_data[financial_data.data_type=='train'].NewsHeadline.values, \n    return_tensors='pt',\n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=150 # the maximum lenght observed in the headlines\n)\n\nencoded_data_val = finbert_tokenizer.batch_encode_plus(\n    financial_data[financial_data.data_type=='val'].NewsHeadline.values, \n    return_tensors='pt',\n    add_special_tokens=True, \n    return_attention_mask=True, \n    pad_to_max_length=True, \n    max_length=150 # the maximum lenght observed in the headlines\n)\n\n\ninput_ids_train = encoded_data_train['input_ids']\nattention_masks_train = encoded_data_train['attention_mask']\nlabels_train = torch.tensor(financial_data[financial_data.data_type=='train'].label.values)\n\ninput_ids_val = encoded_data_val['input_ids']\nattention_masks_val = encoded_data_val['attention_mask']\nsentiments_val = torch.tensor(financial_data[financial_data.data_type=='val'].label.values)\n\n\ndataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\ndataset_val = TensorDataset(input_ids_val, attention_masks_val, sentiments_val)","7a6fe940":"len(sentiment_dict)","cf6f36e1":"model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI\/finbert\",\n                                                          num_labels=len(sentiment_dict),\n                                                          output_attentions=False,\n                                                          output_hidden_states=False)","b4871e60":"batch_size = 5\n\ndataloader_train = DataLoader(dataset_train, \n                              sampler=RandomSampler(dataset_train), \n                              batch_size=batch_size)\n\ndataloader_validation = DataLoader(dataset_val, \n                                   sampler=SequentialSampler(dataset_val), \n                                   batch_size=batch_size)","d0de03b9":"optimizer = AdamW(model.parameters(),\n                  lr=1e-5, \n                  eps=1e-8)\n\nepochs = 2\n\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=0,\n                                            num_training_steps=len(dataloader_train)*epochs)","149f85d3":"seed_val = 2022\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\n\nfor epoch in tqdm(range(1, epochs+1)):\n    \n    model.train()\n    \n    loss_train_total = 0\n\n    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n    for batch in progress_bar:\n\n        model.zero_grad()\n        \n        batch = tuple(b.to(device) for b in batch)\n        \n        inputs = {'input_ids':      batch[0],\n                  'attention_mask': batch[1],\n                  'labels':         batch[2],\n                 }       \n\n        outputs = model(**inputs)\n        \n        loss = outputs[0]\n        loss_train_total += loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n        \n        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()\/len(batch))})\n         \n        \n    torch.save(model.state_dict(), f'finetuned_finBERT_epoch_{epoch}.model')\n        \n    tqdm.write(f'\\nEpoch {epoch}')\n    \n    loss_train_avg = loss_train_total\/len(dataloader_train)            \n    tqdm.write(f'Training loss: {loss_train_avg}')\n    \n    val_loss, predictions, true_vals = evaluate(dataloader_validation)\n    val_f1 = f1_score_func(predictions, true_vals)\n    tqdm.write(f'Validation loss: {val_loss}')\n    tqdm.write(f'F1 Score (Weighted): {val_f1}')","00afd5c2":"model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI\/finbert\",\n                                                          num_labels=len(sentiment_dict),\n                                                          output_attentions=False,\n                                                          output_hidden_states=False)\n\nmodel.to(device)\n\nmodel.load_state_dict(torch.load('.\/finetuned_finBERT_epoch_1.model', \n                                 map_location=torch.device('cpu')))\n\n_, predictions, true_vals = evaluate(dataloader_validation)\n\naccuracy_per_class(predictions, true_vals)","ff89b311":"# Financial Text Classification Using FinBERT With Pytorch    \n\nThe code of the training section of this notebook is highly inspired of this course on coursera:   \nhttps:\/\/www.coursera.org\/projects\/sentiment-analysis-bert","ded126ce":"\nEpoch 1   \nTraining loss: 0.4581567718019004   \nValidation loss: 0.3778555450533606   \nF1 Score (Weighted): 0.8753212258177056   \n\n\nEpoch 2   \nTraining loss: 0.2466177608593462   \nValidation loss: 0.43929754253413067   \nF1 Score (Weighted): 0.8823813944083021   \n","66c3e330":"## Model Training","0c1918bf":"## Training & Validation Data Loader","f2239c5a":"## Load the best model for predictions","caa5d693":"## Quick EDA","712c0f0a":"# About the Data set   \n## Load the data","73bc1ec2":"## Tockenization & Data Encoding","204c1c30":"**Observation**: \nThe training loss kept decreasing, while the validation loss kept increasing right after the first epoch. This means that the model starts overfitting from the second epoch, so the right thing to do in this situation is to stop the training after epoch 1. ","6488ffe6":"From the previous observation, the best model is the first one from epoch 1. ","f5e61611":"## Useful Libraries","ac4eb7f4":"Let have a look at the distribution of headlines lenght in order to know the maximum lenght for encoding","f033d1da":"## Optimizer & Scheduler","e592b83b":"## Utility Functions","03d73055":"## Model Configuration","86b0fa0e":"# Data Processing"}}