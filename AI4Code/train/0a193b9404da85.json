{"cell_type":{"2484bba6":"code","2bebda70":"code","cfe542ba":"code","6a34cd2c":"code","4594541f":"code","ea705d91":"code","51538432":"code","4e64023b":"code","288de8c9":"code","d2b9777d":"code","9b5c71c2":"code","11dc57b8":"code","4cc44219":"code","53859da3":"code","f491cc79":"code","183a9b05":"code","0f21e4e8":"code","684edc07":"code","49d3b7d7":"code","c5f54585":"code","590ff183":"code","5c1ba9c1":"code","62ebda89":"code","05bb8df3":"code","5ef577b8":"code","58960f46":"code","e529f40b":"code","223e909a":"code","ddd113ec":"code","a754725e":"code","5e5608c1":"code","345dcf02":"code","e111421f":"code","29a60cee":"code","374830cd":"code","44a04428":"code","31cf6d22":"code","273135dc":"code","2ed229b5":"code","1a68f858":"code","c68bac43":"code","e8445966":"code","906d4a15":"code","4175bff5":"code","e252f61f":"code","bb7119e3":"code","05d87f06":"code","62d36046":"code","441a5663":"code","df9ffccd":"code","22e37f58":"code","8290b688":"code","b3f03802":"code","e79c4615":"code","0d6f0fba":"code","14bdebe3":"code","6087f6fa":"code","9d9c156d":"code","95cab87b":"code","2752cec2":"code","6a1c7f3d":"code","995b3a9c":"code","69a35f3e":"code","ff9c23f3":"code","20d9b33f":"code","aab8740a":"code","e3761ba8":"code","7e36d038":"code","7ce6733f":"code","bf433454":"code","39ac958e":"code","386a9c11":"code","122924f5":"code","49ca0f1a":"code","2fbd8f98":"code","773cf174":"code","9ff9d444":"code","37a1c0be":"code","71b1e903":"code","b05542df":"code","8f79d61f":"code","ca03b640":"code","0dc62b32":"code","9369d5e1":"code","01e6f807":"code","5d8a2586":"code","f3827949":"code","017afa4e":"code","76b08ac7":"code","e0c9a14f":"code","2ed6831b":"code","93a1b2b8":"code","183b2e53":"code","4a5714c5":"code","4fd9a719":"code","22670b78":"code","03906d4b":"code","b310bfef":"code","4833a484":"code","43150604":"code","0603bbe5":"code","1d5554c0":"code","9f285ee3":"code","01fbaed0":"code","cbec6914":"code","6a6a4cb8":"code","ddf52570":"code","1710ff0e":"code","05aaeace":"code","ffb51a29":"code","b971929a":"code","1fd2b0a4":"code","db32a34e":"code","f36e5c02":"code","4d1ce8fb":"code","dacfccd6":"code","f397cded":"code","2fc8c430":"code","2b0a6e65":"code","2125c770":"code","edb141fa":"code","da9b8c08":"code","f267694a":"code","888c6340":"code","cfe768d6":"code","d4b00722":"markdown","9c9579ce":"markdown","f12adb3a":"markdown","67d082ce":"markdown","ee70fb44":"markdown","438e1e53":"markdown","4b572680":"markdown","efe3afa6":"markdown","afb7a457":"markdown","3928bfa0":"markdown","3f96caa9":"markdown","076979ce":"markdown","7f02a48d":"markdown","ea65e8c2":"markdown","bd4cc454":"markdown","dfc172cf":"markdown","88aec656":"markdown","fe76a6df":"markdown","458e1aa9":"markdown","c93b6a3c":"markdown","2fa1f857":"markdown","7ccbd7f8":"markdown","87cbe7ac":"markdown","274a60ac":"markdown"},"source":{"2484bba6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2bebda70":"pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\n#Here we learn that SalePrice is the target column","cfe542ba":"pd.read_fwf(\"..\/input\/house-prices-advanced-regression-techniques\/data_description.txt\")\n","6a34cd2c":"df= pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf.info()","4594541f":"test=pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ntest.info()","ea705d91":"df.describe() # we get all the statistical information of all the numerical data","51538432":"y=df[\"SalePrice\"].values","4e64023b":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,10))\nsns.set_style(\"darkgrid\")\nsns.histplot(data=df, x=\"SalePrice\", bins=50,cbar=True)\n#It seem that majority of the prices are between 100 000 and 250 000","288de8c9":"# Skew and kurtosis for SalePrice \nprint(\"Skewness: %f\" % df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df['SalePrice'].kurt())","d2b9777d":"#Applying log transformation to remove skewness and make target variable normally distributed\ndf['SalePrice'] = np.log1p(df['SalePrice'])","9b5c71c2":"plt.figure(figsize=(12,10))\nsns.set_style(\"darkgrid\")\nsns.histplot(data=df, x=\"SalePrice\", bins=50,cbar=True,color=\"red\")\n#Now it is normally distributed","11dc57b8":"plt.figure(figsize=(12,10))\nsns.boxplot(data=df, x=\"SalePrice\",color=\"green\")\n","4cc44219":"df[\"SalePrice\"].describe() \n#The mean price is 180 921 and standart deviation is 79 442 and its very high","53859da3":"df.corr()[\"SalePrice\"].sort_values(ascending=False)\n#Here we list the correlation between the target and other features from the highest to the negative ones\n#It seem the overall quality has the highest positive correlation with the target","f491cc79":"plt.figure(figsize=(12,10))\nsns.heatmap(df.corr(),cmap=\"jet\",annot=False,linewidths=1,robust=True)","183a9b05":"plt.figure(figsize=(12,10))\nsns.regplot(x=\"OverallQual\", y=\"SalePrice\",data=df)\n#OverallQual has the highest positive correlation with the target ","0f21e4e8":"plt.figure(figsize=(12,10))\nsns.regplot(x=\"KitchenAbvGr\", y=\"SalePrice\",data=df,color=\"red\")\n#KitchenAbvGr has the highest negative correlation with the target","684edc07":"#Visualising numerical predictor variables with Target Variables\ntrain_num = df.select_dtypes(include=['int64','float64'])\nfig,axs= plt.subplots(12,3,figsize=(20,80))\n#adjust horizontal space between plots \nfig.subplots_adjust(hspace=0.6)\nfor i,ax in zip(train_num.columns,axs.flatten()):\n    sns.scatterplot(x=i, y='SalePrice', hue='SalePrice',data=train_num,ax=ax,palette='coolwarm')\n    plt.xlabel(i,fontsize=12)\n    plt.ylabel('SalePrice',fontsize=12)\n    #ax.set_yticks(np.arange(0,900001,100000))\n    ax.set_title('SalePrice'+' - '+str(i),fontweight='bold',size=20)","49d3b7d7":"##Visualising Categorical predictor variables with Target Variables\ncategorical = df.select_dtypes(exclude=['int64','float64'])\ndef facetgrid_boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n    \n\nf = pd.melt(df, id_vars=['SalePrice'], value_vars=sorted(df[categorical.columns]))\ng = sns.FacetGrid(f, col=\"variable\", col_wrap=3, sharex=False, sharey=False, size=5)\ng = g.map(facetgrid_boxplot, \"value\", \"SalePrice\")","c5f54585":"num_cols=df.columns[df.dtypes!= \"object\"]\ncat_cols=df.columns[df.dtypes== \"object\"]\ntest_num_cols=test.columns[test.dtypes!= \"object\"]\ntest_cat_cols=test.columns[test.dtypes== \"object\"]","590ff183":"df[num_cols].isnull().sum().sort_values(ascending=False)\n#We have only 3 numerical columns with missing values","5c1ba9c1":"test[test_num_cols].isnull().sum().sort_values(ascending=False)","62ebda89":"df[num_cols].isnull().sum().sort_values(ascending=False)\/len(df)\n#Their percentage is not so high and we can fill the missing values","05bb8df3":"#Lets begin with the one with highest missing value among numerical columns\ndf[\"LotFrontage\"].value_counts() #There 110 different types","5ef577b8":"df[\"LotFrontage\"].describe()","58960f46":"df[\"LotFrontage\"].fillna(df[\"LotFrontage\"].mean(),inplace=True) # we fill the missing values with the mean of the column\ntest[\"LotFrontage\"].fillna(test[\"LotFrontage\"].mean(),inplace=True)\ndf[\"LotFrontage\"].isnull().sum()","e529f40b":"test[\"LotFrontage\"].isnull().sum()","223e909a":"#Let look at another numerical column with missing value:\ndf[\"GarageYrBlt\"] #These are the years of the garage built: ","ddd113ec":"df[\"GarageYrBlt\"].fillna(df[\"GarageYrBlt\"].median(),inplace=True)\ntest[\"GarageYrBlt\"].fillna(test[\"GarageYrBlt\"].median(),inplace=True)\nprint(df[\"GarageYrBlt\"].isnull().sum())\nprint(test[\"GarageYrBlt\"].isnull().sum())","a754725e":"df[\"MasVnrArea\"].fillna(df[\"MasVnrArea\"].median(), inplace=True)\ntest[\"MasVnrArea\"].fillna(test[\"MasVnrArea\"].median(), inplace=True)\n","5e5608c1":"df[num_cols].isnull().sum().sort_values(ascending=False)\n#Now we do not have any missing value with the numerical columns","345dcf02":"test[test_num_cols].isnull().sum().sort_values(ascending=False) # we still have some missing values, but so import","e111421f":"test[\"BsmtHalfBath\"].fillna(test[\"BsmtHalfBath\"].median(), inplace=True)\ntest[\"BsmtFullBath\"].fillna(test[\"BsmtFullBath\"].median(), inplace=True)\ntest[\"BsmtFinSF1\"].fillna(test[\"BsmtFinSF1\"].mean(), inplace=True)\ntest[\"GarageCars\"].fillna(test[\"GarageCars\"].mean(), inplace=True)\ntest[\"GarageArea\"].fillna(test[\"GarageArea\"].mean(), inplace=True)\ntest[\"TotalBsmtSF\"].fillna(test[\"TotalBsmtSF\"].mean(), inplace=True)\ntest[\"BsmtUnfSF\"].fillna(test[\"BsmtUnfSF\"].mean(), inplace=True)\ntest[\"BsmtFinSF2\"].fillna(test[\"BsmtFinSF2\"].mean(), inplace=True)\ntest[test_num_cols].isnull().sum().sort_values(ascending=False) \n#We have dealt with the missing values in all numerical columns in the test set\n\n","29a60cee":"df[cat_cols].isnull().sum().sort_values(ascending=False)\/len(df[cat_cols])\n#We have 16 categorical columns with missing values","374830cd":"test[test_cat_cols].isnull().sum().sort_values(ascending=False)\/len(test[test_cat_cols])","44a04428":"# We will drop columns which have more than %80 missing values:\ndf.drop([\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\"],axis=1, inplace=True)\ndf.isnull().sum().sort_values(ascending=False)","31cf6d22":"test.drop([\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\"],axis=1, inplace=True)\ntest.isnull().sum().sort_values(ascending=False)","273135dc":"#FireplaceQu  has % over %40 missing value, but it has a couple of categories that we can fill\ndf[\"FireplaceQu\"].value_counts()","2ed229b5":"df[\"FireplaceQu\"].fillna(\"Gd\",inplace=True)\ndf[\"FireplaceQu\"].isnull().sum()","1a68f858":"test[\"FireplaceQu\"].fillna(\"Gd\",inplace=True)\ntest[\"FireplaceQu\"].isnull().sum()","c68bac43":"df[\"GarageType\"].value_counts()","e8445966":"df[\"GarageType\"].fillna(\"Attchd\",inplace=True) #Lets fill the missing values with the most common value\n","906d4a15":"test[\"GarageType\"].fillna(\"Attchd\",inplace=True)","4175bff5":"df[\"GarageFinish\"].fillna(\"Unf\",inplace=True)\ntest[\"GarageFinish\"].fillna(\"Unf\",inplace=True)","e252f61f":"df[\"GarageCond\"].fillna(\"TA\",inplace=True)\ntest[\"GarageCond\"].fillna(\"TA\",inplace=True)","bb7119e3":"df[\"GarageQual\"].fillna(\"TA\",inplace=True)\ntest[\"GarageQual\"].fillna(\"TA\",inplace=True)\n","05d87f06":"df.isnull().sum().sort_values(ascending=False) #The rest of the missing values are so few, so we can just drop raws with missing values","62d36046":"df[\"BsmtFinType2\"].fillna(\"Unf\",inplace=True)","441a5663":"df[\"BsmtExposure\"].fillna(\"No\",inplace=True)","df9ffccd":"df[\"BsmtFinType1\"].fillna(\"Unf\",inplace=True)","22e37f58":"df[\"BsmtQual\"].fillna(\"TA\",inplace=True)","8290b688":"df[\"BsmtCond\"].fillna(\"TA\",inplace=True)","b3f03802":"df[\"MasVnrType\"].fillna(\"None\",inplace=True)","e79c4615":"df[\"Electrical\"].fillna(\"SBrkr\",inplace=True)","0d6f0fba":"df.isnull().sum().sort_values(ascending=False) #Now we do  ot have nay missing value in categorical columns of trainin set","14bdebe3":"test.isnull().sum().sort_values(ascending=False)","6087f6fa":"test[\"BsmtCond\"].fillna(\"TA\",inplace=True)\ntest[\"BsmtQual\"].fillna(\"TA\",inplace=True)\ntest[\"BsmtExposure\"].fillna(\"No\",inplace=True)\ntest[\"BsmtFinType2\"].fillna(\"Unf\",inplace=True)\ntest[\"BsmtFinType1\"].fillna(\"GLQ\",inplace=True)\ntest.isnull().sum().sort_values(ascending=False)","9d9c156d":"test[\"MasVnrType\"].fillna(\"None\",inplace=True)\ntest[\"MSZoning\"].fillna(\"RL\",inplace=True)\ntest[\"Functional\"].fillna(\"No\",inplace=True)\ntest[\"Utilities\"].fillna(\"AllPub\",inplace=True)\ntest[\"Exterior2nd\"].fillna(\"VinylSd\",inplace=True)\ntest.isnull().sum().sort_values(ascending=False)\n#We have still 3 columns with only 1 missing value","95cab87b":"test[\"KitchenQual\"].fillna(\"TA\",inplace=True)\ntest[\"Exterior1st\"].fillna(\"VinylSd\",inplace=True)\ntest[\"SaleType\"].fillna(\"WD\",inplace=True)\ntest.isnull().sum().sort_values(ascending=False)\n#There is no missing value in our test data now.","2752cec2":"# Creating box plots for all numeric columns:\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=df[num_cols] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","6a1c7f3d":"# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n","995b3a9c":"# Finding skewed numerical columns:\nskew_columns = df[num_cols].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_columns[skew_columns > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_columns","69a35f3e":"# Lets normalize skewed features\nfor i in skew_index:\n    df[i] = boxcox1p(df[i], boxcox_normmax(df[i] + 1))","ff9c23f3":"# Let's make sure we handled all the skewed values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=df[skew_index] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)\n#Now all the numerical seems to be normally distributed","20d9b33f":"#Lets do the same operations to the test data:\n# Creating box plots for all numeric columns:\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=test[test_num_cols] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","aab8740a":"# Finding skewed numerical columns:\nskew_columns = test[test_num_cols].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nhigh_skew = skew_columns[skew_columns > 0.5]\nskew_index = high_skew.index\n\nprint(\"There are {} numerical features with Skew > 0.5 :\".format(high_skew.shape[0]))\nskewness = pd.DataFrame({'Skew' :high_skew})\nskew_columns","e3761ba8":"# Lets normalize skewed features\nfor i in skew_index:\n    test[i] = boxcox1p(test[i], boxcox_normmax(test[i] + 1))\n# Let's make sure we handled all the skewed values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=test[skew_index] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)\n#Now all the numerical seems to be normally distributed","7e36d038":"df[\"SalePrice\"]","7ce6733f":"\ncat_cols=df.columns[df.dtypes== \"object\"]\ncat_cols","bf433454":"test_cat_cols = test.columns[test.dtypes==\"object\"]\ntest_cat_cols","39ac958e":"df[cat_cols].head()","386a9c11":"df= pd.get_dummies(df,columns =[\"MSZoning\",\"Street\",\"LotShape\",\"LandContour\",'LandSlope', \n                                'LotConfig','Neighborhood', 'Condition1', 'BldgType', \n                                'RoofStyle',  'MasVnrType', 'ExterQual', \n                                'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n                                'BsmtFinType1', 'HeatingQC', 'CentralAir', \n                                'KitchenQual', 'FireplaceQu', 'GarageType', 'GarageFinish', \n                                'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition'],drop_first =True)\ndf.head() #Now we transform all of the categorical  columns into numerical values","122924f5":"df.drop([\"Exterior2nd\",\"Condition2\",\"BsmtFinType2\",\"Utilities\", 'HouseStyle',\n         'RoofMatl', 'Exterior1st',  'Heating', 'Electrical', 'Functional', \n        'GarageQual'],axis=1,inplace=True) # We drop 3 columns because they have the same information with some columns\ndf.head()","49ca0f1a":"df.info() # Now all the columns are numerical","2fbd8f98":"#Lets do the same operation for test data:\ntest[test_cat_cols].head()","773cf174":"test= pd.get_dummies(test,columns =[\"MSZoning\",\"Street\",\"LotShape\",\"LandContour\",'LandSlope', \n                                    'LotConfig','Neighborhood', 'Condition1', 'BldgType', \n                                    'RoofStyle',  'MasVnrType', 'ExterQual', \n                                    'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n                                    'BsmtFinType1', 'HeatingQC', 'CentralAir', \n                                    'KitchenQual', 'FireplaceQu', 'GarageType', 'GarageFinish', \n                                    'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition'],drop_first =True)\ntest.head() #Now we transform all of the categorical  columns into numerical values","9ff9d444":"test.drop([\"Exterior2nd\",\"Condition2\",\"BsmtFinType2\",\"Utilities\", \n           'HouseStyle', 'RoofMatl', 'Exterior1st',  'Heating', \n           'Electrical', 'Functional', 'GarageQual'],axis=1,inplace=True) # We drop 3 columns because they have the same information with some columns\ntest.head()","37a1c0be":"test.info() # Now all the columns are numerical for test data and both train and test data is ready for ML algorithms","71b1e903":"#Id column is just index, so we need to drop it totally from the dataframe:\ndf.drop(\"Id\",axis=1,inplace=True)\ndf.head()\n","b05542df":"test_id = test[\"Id\"]\ntest_id","8f79d61f":"\ntest.drop(\"Id\",axis=1,inplace=True)\ntest.head()","ca03b640":"X=df.drop(\"SalePrice\",axis=1)\nX = X.values\nX.shape","0dc62b32":"\ny.shape","9369d5e1":"test= test.values\ntest.shape","01e6f807":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train,y_test=train_test_split(X,y, test_size=0.05,random_state=42)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","5d8a2586":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\npredictions1 = model.predict(X_test)\n","f3827949":"from sklearn.metrics import mean_squared_error, mean_absolute_error\nprint(mean_absolute_error(y_test,predictions1))\nprint(mean_squared_error(y_test,predictions1))\nprint(np.sqrt(mean_squared_error(y_test,predictions1)))","017afa4e":"import seaborn as sns\nsns.set_style(\"darkgrid\")\nplt.figure(figsize=(12,10))\nsns.regplot(predictions1,y_test)","76b08ac7":"from sklearn.tree import DecisionTreeRegressor\ndtree = DecisionTreeRegressor()\ndtree.fit(X_train,y_train)\npredictions2 = dtree.predict(X_test)\nprint(mean_absolute_error(y_test,predictions2))\nprint(mean_squared_error(y_test,predictions2))\nprint(np.sqrt(mean_squared_error(y_test,predictions2))) ","e0c9a14f":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = dtree, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))","2ed6831b":"from sklearn.ensemble import RandomForestRegressor\nrandom_forest = RandomForestRegressor()\nrandom_forest.fit(X_train,y_train)\npredictions_rforest= random_forest.predict(X_test)\nprint(mean_absolute_error(y_test,predictions_rforest))\nprint(mean_squared_error(y_test,predictions_rforest))\nprint(np.sqrt(mean_squared_error(y_test,predictions_rforest)))","93a1b2b8":"accuracies = cross_val_score(estimator = random_forest, X = X_train, y = y_train, cv = 10)\nprint(\"Accuracy: {:.2f} %\".format(accuracies.mean()*100))\nprint(\"Standard Deviation: {:.2f} %\".format(accuracies.std()*100))\n#It seems random fores has very good accuracy in test data, is the best to this point","183b2e53":"from sklearn.model_selection import GridSearchCV\nparam_grid = [\n{'n_estimators': [ 100,120,150], 'max_features': [ 12,14,16]},\n{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3,\n4]},\n]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=10,\nscoring='neg_mean_squared_error',\nreturn_train_score=True)\ngrid_search.fit(X_train, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Accuracy: {:.2f} %\".format(-best_accuracy*100))\nprint(\"Best Parameters:\", best_parameters)","4a5714c5":"grid_predictions = grid_search.predict(X_test)\nprint(mean_absolute_error(y_test,grid_predictions))\nprint(mean_squared_error(y_test,grid_predictions))\nprint(np.sqrt(mean_squared_error(y_test,grid_predictions))) #The predictions is very close to Random Forest","4fd9a719":"from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression,Lasso,Ridge,ElasticNet\nfrom sklearn.linear_model import LassoCV,RidgeCV,ElasticNetCV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import GridSearchCV,KFold,RandomizedSearchCV,StratifiedKFold,cross_val_score","22670b78":"kfold= KFold(n_splits=11,random_state=42,shuffle=True) #kfold cross validation","03906d4b":"lightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)","b310bfef":"lightgbm.fit(X_train,y_train)\nlightgbm_predictions = lightgbm.predict(X_test)\nprint(mean_absolute_error(y_test,lightgbm_predictions))\nprint(mean_squared_error(y_test,lightgbm_predictions))\nprint(np.sqrt(mean_squared_error(y_test,lightgbm_predictions))) \n#This is better than both Random Forest and Grid Search Results","4833a484":"from sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.00015))","43150604":"svr.fit(X_train,y_train)\nsvr_predictions= svr.predict(X_test)\nprint(mean_absolute_error(y_test,svr_predictions))\nprint(mean_squared_error(y_test,svr_predictions))\nprint(np.sqrt(mean_squared_error(y_test,svr_predictions)))","0603bbe5":"from xgboost import XGBRegressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)","1d5554c0":"xgboost.fit(X_train, y_train)\nxgboost_predictions =xgboost.predict(X_test)","9f285ee3":"print(mean_absolute_error(y_test,xgboost_predictions))\nprint(mean_squared_error(y_test,xgboost_predictions))\nprint(np.sqrt(mean_squared_error(y_test,xgboost_predictions)))","01fbaed0":"gbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42) ","cbec6914":"gbr.fit(X_train,y_train)\ngbr_predictions = gbr.predict(X_test)\nprint(mean_absolute_error(y_test,gbr_predictions))\nprint(mean_squared_error(y_test,gbr_predictions))\nprint(np.sqrt(mean_squared_error(y_test,gbr_predictions))) ","6a6a4cb8":"alpha_elnet= [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\nl1ratio_elnet = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1000000, alphas=alpha_elnet, \\\n                                                        cv=kfold, l1_ratio=l1ratio_elnet))","ddf52570":"elasticnet.fit(X_train,y_train)\nelastic_predictions = elasticnet.predict(X_test)\nprint(mean_absolute_error(y_test,elastic_predictions))\nprint(mean_squared_error(y_test,elastic_predictions))\nprint(np.sqrt(mean_squared_error(y_test,elastic_predictions)))","1710ff0e":"alphas_lasso = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008] #Best value of alpha parmaters for lasso\nlasso = make_pipeline(RobustScaler(), LassoCV(alphas=alphas_lasso, cv=kfold))","05aaeace":"lasso.fit(X_train,y_train)\nlasso_predictions = lasso.predict(X_test)\nprint(mean_absolute_error(y_test,lasso_predictions))\nprint(mean_squared_error(y_test,lasso_predictions))\nprint(np.sqrt(mean_squared_error(y_test,lasso_predictions)))","ffb51a29":"# Stack up all the models that performs better than the others, optimized using xgboost\nstack_reg = StackingCVRegressor(regressors=(xgboost, lightgbm, random_forest,\n                                            gbr,svr,lasso,elasticnet),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)\nstack_reg.fit(X_train,y_train)\nstack_predictions = stack_reg.predict(X_test)\nprint(mean_absolute_error(y_test,stack_predictions))\nprint(mean_squared_error(y_test,stack_predictions))\nprint(np.sqrt(mean_squared_error(y_test,stack_predictions))) ","b971929a":"# Blend models in order to make the final predictions more robust to overfitting\nblend_predictions=(0.025* elasticnet.predict(X_test)) + \\\n            (0.025 * lasso.predict(X_test)) + \\\n            (0.025 * random_forest.predict(X_test)) + \\\n            (0.025* svr.predict(X_test)) + \\\n            (0.62 * gbr.predict(X_test)) + \\\n            (0.03 * xgboost.predict(X_test)) + \\\n            (0.03 * lightgbm.predict(X_test)) + \\\n            (0.22 * stack_reg.predict(np.array(X_test))) ","1fd2b0a4":"print(mean_absolute_error(y_test,blend_predictions))\nprint(mean_squared_error(y_test,blend_predictions))\nprint(np.sqrt(mean_squared_error(y_test,blend_predictions))) \n#As we can see the blended version of the models outperforms every single model:","db32a34e":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout,BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam","f36e5c02":"\nrs=RobustScaler()","4d1ce8fb":"X_deep_train=rs.fit_transform(X_train)\nX_deep_train","dacfccd6":"X_deep_test =rs.transform(X_test)\nX_deep_test","f397cded":"\nmodel = Sequential()\nmodel.add(Dense(200, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(100, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(50, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(25, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(1))\n# Compile model\nmodel.compile(loss='mean_squared_error', optimizer=\"adam\")\n\n\n\nmodel.fit(np.array(X_train), np.array(y_train), epochs=100, batch_size=10)\n","2fc8c430":"deep_predictions=model.predict(X_test)","2b0a6e65":"print(mean_absolute_error(y_test,deep_predictions))\nprint(mean_squared_error(y_test,deep_predictions))\nprint(np.sqrt(mean_squared_error(y_test,deep_predictions)))","2125c770":"submission_sample = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\nsubmission_sample.shape","edb141fa":"pd.DataFrame(X_train)","da9b8c08":"def latest_predictions(features): \n    return ((0.025* elasticnet.predict(features)) + \\\n            (0.025 * lasso.predict(features)) + \\\n            (0.025 * random_forest.predict(features)) + \\\n            (0.025* svr.predict(features)) + \\\n            (0.62 * gbr.predict(features)) + \\\n            (0.03 * xgboost.predict(features)) + \\\n            (0.03 * lightgbm.predict(features)) + \\\n            (0.22 * stack_reg.predict(np.array(features))))","f267694a":"submission_sample.iloc[:,1] = latest_predictions(test)\nsubmission_sample","888c6340":"q1 = submission_sample['SalePrice'].quantile(0.005)\nq2 = submission_sample['SalePrice'].quantile(0.995)\nsubmission_sample['SalePrice'] = submission_sample['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission_sample['SalePrice'] = submission_sample['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n","cfe768d6":"submission_sample.to_csv(\"new_submission_regression4.csv\", index=False)","d4b00722":"Lets try ANNs:","9c9579ce":"<font color= \"red\">\n2.3. Handling the Text and Categorical Columns:","f12adb3a":"<font color=\"green\">\nLets try XGBoost:","67d082ce":"<font color=\"green\">\nLets try ElasticNetCV:","ee70fb44":"## 2. Prepare the Data Before Applying Machine Learning Algorithms","438e1e53":"boxcox_normmax(x, brack=(-2.0, 2.0), method='pearsonr')\n    Compute optimal Box-Cox transform parameter for input data.\n    \n    Parameters\n    ----------\n    x : array_like\n        Input array.\n    brack : 2-tuple, optional\n        The starting interval for a downhill bracket search with\n        `optimize.brent`.  Note that this is in most cases not critical; the\n        final result is allowed to be outside this bracket.\n    method : str, optional\n        The method to determine the optimal transform parameter (`boxcox`\n        ``lmbda`` parameter). Options are:\n    \n        'pearsonr'  (default)\n            Maximizes the Pearson correlation coefficient between\n            ``y = boxcox(x)`` and the expected values for ``y`` if `x` would be\n            normally-distributed.\n    \n        'mle'\n            Minimizes the log-likelihood `boxcox_llf`.  This is the method used\n            in `boxcox`.\n    \n        'all'\n            Use all optimization methods available, and return all results.\n            Useful to compare different methods.","4b572680":"<font color=\"green\">\nLets try a blending version of best models we have used above:","efe3afa6":"<font color=\"green\">\nLets try LassoCV:","afb7a457":"<font color=\"green\">\nLets use another Other Ensemble and Boosting Models to have a better performance:","3928bfa0":"Utilities, test[\"HouseStyle\"],RoofMatl, Exterior1st, Heating,Electrical,Functional, GarageQual","3f96caa9":"<font color=\"red\">\n2.1. Handling the Missing Values:","076979ce":"Light Gradient Boosting Regressor:\n\nGradient boosting refers to a class of ensemble machine learning algorithms that can be used for classification or regression predictive modeling problems.\n\nEnsembles are constructed from decision tree models. Trees are added one at a time to the ensemble and fit to correct the prediction errors made by prior models. This is a type of ensemble machine learning model referred to as boosting.","7f02a48d":"<font color=\"green\">\nLets use another model:","ea65e8c2":"<font color=\"green\">\nLets use another model:","bd4cc454":"<font color=\"green\">\nLets try StackingCVRegressor:","dfc172cf":"<font color=\"green\">\nLets try GradientBoostingRegressor:","88aec656":"skew(a, axis=0, bias=True, nan_policy='propagate')\n    Compute the sample skewness of a data set.\n    \n    For normally distributed data, the skewness should be about zero. For\n    unimodal continuous distributions, a skewness value greater than zero means\n    that there is more weight in the right tail of the distribution. The\n    function `skewtest` can be used to determine if the skewness value\n    is close enough to zero, statistically speaking.","fe76a6df":"## Exploratory Data Analysis:","458e1aa9":"<font color= \"red\">\n2.2. Fixing Skewed Numerical Columns:","c93b6a3c":"<font color=\"blue\">\nThe SalePrice is skewed to the right.SalePrice is not normally distributed, so we need to adjust it.","2fa1f857":"You could use RobustScaler if you have outliers and want to reduce their influence. However, you might be better off removing the outliers, instead. Use StandardScaler if you need a relatively normal distribution.\nSo we will use RobustScaler with Support Vector Regressor","7ccbd7f8":"## 2.3. Select and Train a Model:","87cbe7ac":"<font color=\"green\">\nLets use another model:","274a60ac":"SalePrice column is our target "}}