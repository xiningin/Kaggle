{"cell_type":{"09a99f40":"code","2323632f":"code","706a8cf1":"code","7be51109":"code","b4874585":"code","767f3558":"code","66d1703d":"code","585af7fa":"code","449c8919":"code","9c80fdce":"code","1f117af0":"code","15577c21":"code","f0feba47":"code","7f3f139b":"code","025488e6":"code","60919902":"code","2fd4b170":"code","7bf8e843":"code","10b68db8":"code","60de543d":"code","03d45c38":"code","e2f47e0a":"code","9d8e16a8":"code","d4241711":"code","33a8c199":"code","362642a7":"code","1d51c177":"markdown","e5231e5b":"markdown","220829ce":"markdown","bed1f644":"markdown","82e1ef13":"markdown","64767329":"markdown","a5abd09a":"markdown","74b13a9c":"markdown","811fa48f":"markdown","162e2ebd":"markdown","d744d6b9":"markdown","cc1145fa":"markdown","22f3d1db":"markdown","824e6518":"markdown","a4944046":"markdown","59dceeb2":"markdown","b7f1d528":"markdown","2af7f25f":"markdown","2dc0be2c":"markdown","7afbb14d":"markdown","afe81d05":"markdown","5a506064":"markdown","6e874696":"markdown","fe9a4637":"markdown","a9e4a559":"markdown","783ee74f":"markdown","4b82d146":"markdown"},"source":{"09a99f40":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing, metrics\n# Fix these imports!\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2323632f":"trainingData = pd.read_csv(\"\/kaggle\/input\/184702-tu-ml-ws-20-congressional-voting\/CongressionalVotingID.shuf.lrn.csv\")\ntestData = pd.read_csv(\"\/kaggle\/input\/184702-tu-ml-ws-20-congressional-voting\/CongressionalVotingID.shuf.tes.csv\")\nshufData = pd.read_csv(\"\/kaggle\/input\/184702-tu-ml-ws-20-congressional-voting\/CongressionalVotingID.shuf.sol.ex.csv\")","706a8cf1":"print(\"Training data: \" + str(trainingData.shape))\nprint(\"Testing data: \" + str(testData.shape))\nprint(\"Shuf data: \" + str(shufData.shape))","7be51109":"#print(trainingData.describe())\n#print(trainingData.dtypes)\n#print(trainingData.head)\n","b4874585":"num_missing = (trainingData == \"unknown\").sum()\nnum_missing","767f3558":"num_missing = (testData == \"unknown\").sum()\nnum_missing","66d1703d":"def label_encoding(data):\n    label_encoder = preprocessing.LabelEncoder()\n\n    for attr in data.select_dtypes(include = \"object\").columns:\n        data[attr] = label_encoder.fit_transform(data[attr].astype(str))\n\n    return data","585af7fa":"trainingData = trainingData.replace('unknown', np.nan)\ntestData = testData.replace('unknown', np.nan)","449c8919":"trainingData_DC = trainingData.copy()\ntestData_DC = testData.copy()\n\n# Drop columns\ntrainingData_DC.dropna(axis = 'columns', inplace=True)\ntestData_DC.dropna(axis = 'columns', inplace=True)\n## Yeah... This does not work\n\nlabel_encoding(trainingData_DC)\nlabel_encoding(testData_DC)\n\nprint(\"Training data after deleting columns: \" + str(trainingData_DC.shape))\nprint(\"Testing data after deleting columns: \" + str(testData_DC.shape))","9c80fdce":"trainingData_DR = trainingData.copy()\ntestData_DR = testData.copy()\n\n# Drop rows\ntrainingData_DR.dropna(inplace=True)\ntestData_DR.dropna(inplace=True)\n\nlabel_encoding(trainingData_DR)\nlabel_encoding(testData_DR)\n\nprint(\"Training data after deleting rows: \" + str(trainingData_DR.shape))\nprint(\"Testing data after deleting rows: \" + str(testData_DR.shape))\n\n# Deletes half of the observations","1f117af0":"trainingData_LAZY = trainingData.copy()\ntestData_LAZY = testData.copy()\n\ntrainingData_LAZY.fillna(2, inplace=True)\ntestData_LAZY.fillna(2, inplace=True)\n\nlabel_encoding(trainingData_LAZY)\nlabel_encoding(testData_LAZY)\n\nprint(\"Training data after lazy impute: \" + str(trainingData_LAZY.shape))\nprint(\"Testing data after lazy impute: \" + str(testData_LAZY.shape))\n","15577c21":"trainingData_MEAN = trainingData.copy()\ntestData_MEAN = testData.copy()\n\n# Impute mean\ntrainingData_MEAN.fillna(trainingData_MEAN.mean(), inplace=True)\ntestData_MEAN.fillna(testData_MEAN.mean(), inplace=True)\n\nlabel_encoding(trainingData_MEAN)\nlabel_encoding(testData_MEAN)\n\nprint(\"Training data after mean impute: \" + str(trainingData_MEAN.shape))\nprint(\"Testing data after mean impute: \" + str(testData_MEAN.shape))","f0feba47":"trainingData_MODE = trainingData.copy()\ntestData_MODE = testData.copy()\n\n# Impute mode\ntrainingData_MODE.fillna(trainingData_MODE.mode(), inplace=True)\ntestData_MODE.fillna(testData_MODE.mode(), inplace=True)\n\nlabel_encoding(trainingData_MODE)\nlabel_encoding(testData_MODE)\n\nprint(\"Training data after mode impute: \" + str(trainingData_MODE.shape))\nprint(\"Testing data after mode impute: \" + str(testData_MODE.shape))","7f3f139b":"# I Think this can be discarded completyly\n\n#X_train = trainingData5.iloc[:, 2:17].values\n#y_train = trainingData5.iloc[:, 1].values\n\n\n#X_test = trainingData5.iloc[:, 2:17].values\n#y_test = trainingData5.iloc[:, 1].values\n\n#from sklearn.preprocessing import StandardScaler\n#from sklearn.ensemble import RandomForestRegressor\n#from sklearn import metrics\n\n#sc = StandardScaler()\n#X_train = sc.fit_transform(X_train)\n#X_test = sc.transform(X_test)\n\n#regressor = RandomForestRegressor(n_estimators=20)\n#regressor.fit(X_train, y_train)\n#y_pred = regressor.predict(X_test)\n\n#print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n#print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n#print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n","025488e6":"def RunSimulation(ImputeMethod\n                  , ipN_estimators = 100\n                  , ipRandom_state = 0\n                  , ipCritirion = \"gini\"\n                  , ipMin_samples_split = 2\n                  , ipMin_samples_leaf = 1\n                  , ipMin_weight_fraction_leaf = 0\n                  , ipMax_features = \"auto\"\n                  , ipBootstrap = True\n                  , ipSplitSize = 0.2\n                 ):\n    # Choose which dataset to run\n    if ImputeMethod == 'LAZY':\n        training_LEARN = trainingData_LAZY.copy()\n        test_LEARN = testData_LAZY.copy()\n    elif ImputeMethod == 'MEAN':\n        training_LEARN = trainingData_MEAN.copy()\n        test_LEARN = testData_MEAN.copy()\n    elif ImputeMethod == 'MODE':\n        training_LEARN = trainingData_MODE.copy()\n        test_LEARN = testData_MODE.copy()\n    elif ImputeMethod == 'DC':\n        training_LEARN = trainingData_DC.copy()\n        test_LEARN = testData_DC.copy()\n    elif ImputeMethod == 'DR':\n        training_LEARN = trainingData_DR.copy()\n        test_LEARN = testData_DR.copy()\n    # Should handle undefined as well, buuut....\n        \n    # Choose parameters for the model\n    pN_estimators = ipN_estimators                         # int\n    pRandom_state = ipRandom_state                         # int\n    pCritirion = ipCritirion                               # or entropy?\n    pMin_samples_split = ipMin_samples_split               # int or float\n    pMin_samples_leaf = ipMin_samples_leaf                 # int\n    pMin_weight_fraction_leaf = ipMin_weight_fraction_leaf # float\n    pMax_features = ipMax_features                         # {\u201cauto\u201d, \u201csqrt\u201d, \u201clog2\u201d}\n    pBootstrap = ipBootstrap                               # bool\n\n    # For test\/train split\n    pSplitSize = ipSplitSize\n\n    # Predictor variables\n    X = training_LEARN.iloc[:, 2:17].values\n\n    # Value to be predicted variables\n    y = training_LEARN.iloc[:, 1].values # Class\n\n    # Split the training into train and test - for proper classificaion, entire training will be used.\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=pSplitSize, random_state=0)\n\n\n    # Scale the data - Not neccesary in binary classification\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n\n\n    # Start running the models\n    ## Create the regressor\n    regressor = RandomForestClassifier(n_estimators = pN_estimators\n                                       , random_state = pRandom_state\n                                       , criterion = pCritirion\n                                       #, max_depth #int or none\n                                       , min_samples_split = pMin_samples_split\n                                       , min_samples_leaf = pMin_samples_leaf\n                                       , min_weight_fraction_leaf = pMin_weight_fraction_leaf\n                                       , max_features = pMax_features\n                                       , bootstrap = pBootstrap\n                                      )\n    ## Fit the model\n    regressor.fit(X_train, y_train)\n    ## Calculate the estimate class\n    y_pred = regressor.predict(X_test)\n\n    print(\"\\n******************************************\")\n    print(\"******************************************\")\n    print(\"** Results for \" + ImputeMethod + \": **\")\n    \n    print(\"Confusion Matrix:\")\n    print(confusion_matrix(y_test,y_pred))\n    print(\"Classification Report:\")\n    print(classification_report(y_test,y_pred))\n    print(\"Accuraccy Score:\")\n    print(accuracy_score(y_test, y_pred))\n\n    # Used parameters\n    print(\"\\n Used Parameters:\")\n    print(\"n_estimators = \" + str(ipN_estimators))\n    print(\"random_state = \" + str(ipRandom_state))\n    print(\"critirion = \" + str(ipCritirion))\n    print(\"min_samples_split = \" + str(ipMin_samples_split))\n    print(\"min_samples_leaf = \" + str(ipMin_samples_leaf))\n    print(\"min_weight_fraction_leaf = \" + str(ipMin_weight_fraction_leaf))\n    print(\"max_features = \" + str(ipMax_features))\n    print(\"bootstrap = \" + str(ipBootstrap))\n    print(\"splitSize = \" + str(ipSplitSize))\n    # See sizes of datasets\n    print(\"\\n Split sizes:\")\n    print(\"X_Train: \" + str(X_train.shape))\n    print(\"y_train: \" + str(y_train.shape))\n\n    print(\"X_test: \" + str(X_test.shape))\n    print(\"y_test: \" + str(y_test.shape))\n    \n    print(\"******************************************\")\n    print(\"******************************************\")\n","60919902":"RunSimulation(\"LAZY\"\n              , ipN_estimators = 100\n              , ipRandom_state = 0\n              , ipCritirion = \"gini\"\n              , ipMin_samples_split = 2\n              , ipMin_samples_leaf = 1\n              , ipMin_weight_fraction_leaf = 0\n              , ipMax_features = \"auto\"\n              , ipBootstrap = True\n              , ipSplitSize = 0.2\n             )","2fd4b170":"RunSimulation(\"MODE\"\n              , ipN_estimators = 100\n              , ipRandom_state = 0\n              , ipCritirion = \"gini\"\n              , ipMin_samples_split = 2\n              , ipMin_samples_leaf = 1\n              , ipMin_weight_fraction_leaf = 0\n              , ipMax_features = \"auto\"\n              , ipBootstrap = True\n              , ipSplitSize = 0.2\n             )","7bf8e843":"RunSimulation(\"MEAN\"\n              , ipN_estimators = 100\n              , ipRandom_state = 0\n              , ipCritirion = \"gini\"\n              , ipMin_samples_split = 2\n              , ipMin_samples_leaf = 1\n              , ipMin_weight_fraction_leaf = 0\n              , ipMax_features = \"auto\"\n              , ipBootstrap = True\n              , ipSplitSize = 0.2\n             )","10b68db8":"RunSimulation(\"DR\"\n              , ipN_estimators = 100\n              , ipRandom_state = 0\n              , ipCritirion = \"gini\"\n              , ipMin_samples_split = 2\n              , ipMin_samples_leaf = 1\n              , ipMin_weight_fraction_leaf = 0\n              , ipMax_features = \"auto\"\n              , ipBootstrap = True\n              , ipSplitSize = 0.2\n             )","60de543d":"#RunSimulation(\"DC\"\n#              , ipN_estimators = 100\n#              , ipRandom_state = 0\n#              , ipCritirion = \"gini\"\n#              , ipMin_samples_split = 2\n#              , ipMin_samples_leaf = 1\n#              , ipMin_weight_fraction_leaf = 0\n#              , ipMax_features = \"auto\"\n#              , ipBootstrap = True\n#              , ipSplitSize = 0.2\n#             )","03d45c38":"RunSimulation(\"LAZY\"\n              , ipN_estimators = 200\n              , ipRandom_state = 0\n              , ipCritirion = \"gini\"\n              , ipMin_samples_split = 2\n              , ipMin_samples_leaf = 1\n              , ipMin_weight_fraction_leaf = 0\n              , ipMax_features = \"auto\"\n              , ipBootstrap = True\n              , ipSplitSize = 0.2\n             )","e2f47e0a":"# Choose which dataset to run\ntraining_LEARN = trainingData_LAZY.copy()\ntest_LEARN = testData_LAZY.copy()\n\n# Choose parameters for the model\npN_estimators = 2000           # int\npRandom_state = 0             # int\npCritirion = \"entropy\"           # or entropy?\npMin_samples_split = 2        # int or float\npMin_samples_leaf = 1         # int\npMin_weight_fraction_leaf = 0 # float\npMax_features = \"auto\"        # {\u201cauto\u201d, \u201csqrt\u201d, \u201clog2\u201d}\npBootstrap = False             # bool\n#Bootstrap is changed\n# And critirion\n\n# Predictor variables\nX_train = training_LEARN.iloc[:, 2:17].values\nX_test = test_LEARN.iloc[:, 1:16].values ##### REMEMBER TO CHANGE THIS INDEX!! CLASS IS IN PLACE 1 _ THIS FUCKED US UP!\n\n# Value to be predicted variables\ny_train = training_LEARN.iloc[:, 1].values # Class\n\n# Scale the data - Not neccesary in binary classification\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n\n# Start running the models\n## Create the regressor\nregressor = RandomForestClassifier(n_estimators = pN_estimators\n                                   , random_state = pRandom_state\n                                   , criterion = pCritirion\n                                   #, max_depth #int or none\n                                   , min_samples_split = pMin_samples_split\n                                   , min_samples_leaf = pMin_samples_leaf\n                                   , min_weight_fraction_leaf = pMin_weight_fraction_leaf\n                                   , max_features = pMax_features\n                                   , bootstrap = pBootstrap\n                                  )\n## Fit the model\nregressor.fit(X_train, y_train)\n## Calculate the estimate class\ny_pred = regressor.predict(X_test)\n\n#y_pred","9d8e16a8":"testData['class'] = y_pred\ntestData['class'].replace({0: 'democrat', 1: 'republican'}, inplace=True)\n# Should use a more fancy decoder \nprint(testData[{'ID', 'class'}])\n\n\ntestData.to_csv('SolutionGroup34v01.csv', index = False, header=True, columns={'ID', 'class'})","d4241711":"print(training_LEARN.dtypes)","33a8c199":"print(test_LEARN.dtypes)\n","362642a7":"# Predictor variables\nX_train = training_LEARN.iloc[:, 2:17].values\nprint(X_train.shape)\n#training_LEARN.iloc[:, 2:17].names\nX_test = test_LEARN.iloc[:, 1:16].values\n\nprint(X_test.shape)\n\n# Value to be predicted variables\n#y_train = training_LEARN.iloc[:, 1].values # Class","1d51c177":"## MEAN Impute","e5231e5b":"### Testing data","220829ce":"# Conclusion\nIt seems that either lazy or Mode impute is the way to go (Mean gives the same result, but feels less intuitive to me)\n\nNext step is to see if we can optimize the parameters for it.\n\n\nAfter some manual trial and error, these values seem to work decently well:","bed1f644":"### Training Data","82e1ef13":"Do more stuff ehre","64767329":"## MODE Impute","a5abd09a":"# Random Forest - Congressional Voting Dataset\nThis is a copy of the workbook stored on kaggle workbooks (https:\/\/www.kaggle.com\/gunnarsjurarsonknudsen\/g34-congressional-voting-first-draft\/edit\/run\/46448902)","74b13a9c":"# Start learning","811fa48f":"## Impute mean\nNot sure if this makes any sense with binary values...","162e2ebd":"## Impute mode\nI assume that this or lazy is the way to go...","d744d6b9":"## Lazy Impute","cc1145fa":"## Lazy Impute\nImpute with a dummy variable not elsewhere found","22f3d1db":"## Define function for testing\nThis is used to choose which imputing method, as well as which parameters we'll choose to run on the final test-dataset to be uploaded to kaggle.","824e6518":"### Drop columns\nThis will not work, as there are missing values in most all datasets","a4944046":"## Delete Rows\nThis should suck","59dceeb2":"# Run the full model","b7f1d528":"## Replacing the unkown","2af7f25f":"# Import the data","2dc0be2c":"# Conclusion\nWe see that we have all binary categorical values for the parameters, which is great for classification. \nHowever, we also see that we have missing values for basicly all columns, and we therefore need to find a method for imputing the values.","7afbb14d":"## Shape:\nThis is the dimensions of the datasets","afe81d05":"# Start the data cleansing","5a506064":"## Define a helper function for encoding\n(This is stolen directly from Michaels github)","6e874696":"## Delete Columns\nThis will not work","fe9a4637":"## Impute in various ways\nWe need various ways to handle the missing values.","a9e4a559":"# Drop rows","783ee74f":"## Missing values","4b82d146":"# Get a feel for the data\nWe'll start by seeing what we are working with:[](http:\/\/)"}}