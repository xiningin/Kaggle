{"cell_type":{"12bd1c79":"code","e1f6fbe3":"code","ae488d2b":"code","6b68434f":"code","f9464d8d":"code","3d011b9a":"code","15f8116a":"code","b90e30e4":"code","193dfd87":"code","38216638":"code","16aafbc4":"code","11b9095d":"code","e94d4c31":"code","c620a560":"code","77d2c2d9":"code","da992f62":"code","c12d5bf2":"code","96e529d6":"code","410679fb":"code","120bcfc3":"code","7e0c9ad4":"code","93381ad9":"code","674791dd":"code","99dff2b6":"code","cc99559b":"code","e91dcd32":"code","461cf93f":"code","5c5495d0":"code","c395d37c":"code","9df9aad5":"code","ff511a4d":"code","0a42b91c":"code","0a238b90":"code","33849c35":"code","f01f4319":"code","9e911b1f":"code","4e0e96a8":"code","26537d01":"code","e272d9e5":"code","4d315181":"code","815ffe69":"code","c102423f":"code","cdb03455":"code","4c23051d":"code","1382babc":"code","2f55c210":"code","6f9cc1ef":"code","7339f16f":"code","7955a74e":"code","a4214209":"code","37615b22":"code","0d6dcd77":"markdown","0e0bb81f":"markdown","342f2945":"markdown","4bd1abad":"markdown","af896f0d":"markdown","5f825f64":"markdown","b2eddd6e":"markdown","0326b473":"markdown","bdfd038d":"markdown","5a6257f5":"markdown","1b93c2fe":"markdown","e1586d27":"markdown","8abf9396":"markdown","710ee2de":"markdown","2fc2971b":"markdown","de3e93e4":"markdown","511a1ccf":"markdown","18b07bb5":"markdown","8a5522d0":"markdown","8438645d":"markdown","4d1e7dbb":"markdown","0c820a53":"markdown","c63ebea7":"markdown","032fe7fb":"markdown","4d9ad5ae":"markdown","cf5f70b8":"markdown"},"source":{"12bd1c79":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e1f6fbe3":"df=pd.read_csv(\"\/kaggle\/input\/house_prices_1.csv\")","ae488d2b":"df.head()","6b68434f":"df.info()","f9464d8d":"print (\"Train data shape:\", df.shape)","3d011b9a":"import matplotlib.pyplot as plt\nplt.style.use(style='ggplot')\nplt.rcParams['figure.figsize'] = (10, 6)","15f8116a":"df.averageprice.describe()","b90e30e4":"df.averagepricesemidetached.describe()","193dfd87":"df.averagepricedetached.describe()","38216638":"df.averagepriceterraced.describe()","16aafbc4":"df.averagepriceflatormaisonette.describe()","11b9095d":"print (\"Skew is:\", df.averageprice.skew())\nplt.hist(df.averageprice, color='black')\nplt.show()","e94d4c31":"target = np.log(df.averageprice)\nprint (\"Skew is:\", target.skew())\nplt.hist(target, color='red')\nplt.show()","c620a560":"print (\"Skew is:\", df.averagepricesemidetached.skew())\nplt.hist(df.averagepricesemidetached, color='orange')\nplt.show()","77d2c2d9":"target = np.log(df.averagepricesemidetached)\nprint (\"Skew is:\", target.skew())\nplt.hist(target, color='black')\nplt.show()","da992f62":"print (\"Skew is:\", df.averagepricedetached.skew())\nplt.hist(df.averagepricedetached, color='orange')\nplt.show()","c12d5bf2":"target = np.log(df.averagepricedetached)\nprint (\"Skew is:\", target.skew())\nplt.hist(target, color='black')\nplt.show()","96e529d6":"print (\"Skew is:\", df.averagepriceterraced.skew())\nplt.hist(df.averagepriceterraced, color='orange')\nplt.show()","410679fb":"target = np.log(df.averagepriceterraced)\nprint (\"Skew is:\", target.skew())\nplt.hist(target, color='black')\nplt.show()","120bcfc3":"print (\"Skew is:\", df.averagepriceflatormaisonette.skew())\nplt.hist(df.averagepriceflatormaisonette, color='orange')\nplt.show()","7e0c9ad4":"target = np.log(df.averagepriceflatormaisonette)\nprint (\"Skew is:\", target.skew())\nplt.hist(target, color='black')\nplt.show()","93381ad9":"numeric_features = df.select_dtypes(include=[np.number])\nnumeric_features.dtypes","674791dd":"corr = numeric_features.corr()\nprint (corr['averageprice'].sort_values(ascending=False)[:5], '\\n')\nprint (corr['averageprice'].sort_values(ascending=False)[-5:])","99dff2b6":"df.averageprice.unique()","cc99559b":"df.averagepricedetached.unique()","e91dcd32":"df.averagepricesemidetached.unique()","461cf93f":"df.averagepriceterraced.unique()","5c5495d0":"df.averagepriceflatormaisonette.unique()","c395d37c":"plt.scatter(x=df['averageprice'], y=target)\nplt.ylabel('averagepriceterraced')\nplt.xlabel('Average price')\nplt.show()","9df9aad5":"plt.scatter(x=df['averagepricesemidetached'], y=target)\nplt.ylabel('averageprice')\nplt.xlabel('Average price Semi detached')\nplt.show()","ff511a4d":"plt.scatter(x=df['averagepricedetached'], y=target)\nplt.ylabel('averageprice')\nplt.xlabel('Average price Detached')\nplt.show()","0a42b91c":"plt.scatter(x=df['averagepriceflatormaisonette'], y=target)\nplt.ylabel('averageprice ')\nplt.xlabel('Average price Flatormaisonette')\nplt.show()","0a238b90":"plt.scatter(x=df['averagepriceflatormaisonette'], y=np.log(df.averageprice))\nplt.xlim(0,200000) # This forces the same scale as before\nplt.ylabel('Average price')\nplt.xlabel('Average price flatormaisonette')\nplt.show()","33849c35":"nulls = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)[:25])\nnulls.columns = ['Null Count']\nnulls.index.name = 'Feature'\nnulls","f01f4319":"print (\"Unique values are:\", df.geocode.unique())","9e911b1f":"categoricals = df.select_dtypes(exclude=[np.number])\ncategoricals.describe()","4e0e96a8":"print (\"Original: \\n\")\nprint (df.geoname.value_counts(), \"\\n\")","26537d01":"condition_pivot = df.pivot_table(index='geoname', values='averageprice', aggfunc=np.median)\ncondition_pivot.plot(kind='bar', color='red')\nplt.xlabel('Name of  the City')\nplt.ylabel('average Sale Price')\nplt.xticks(rotation=0)\nplt.show()\n","e272d9e5":"def encode(x):\n return 1 if x == 'Partial' else 0\ndf['enc_condition'] = df.geoname.apply(encode)\n","4d315181":"condition_pivot = df.pivot_table(index='enc_condition', values='averageprice', aggfunc=np.median)\ncondition_pivot.plot(kind='bar', color='blue')\nplt.xlabel('Encoded Sale Condition')\nplt.ylabel('Average price')\nplt.xticks(rotation=0)\nplt.show()","815ffe69":"data = df.select_dtypes(include=[np.number]).interpolate().dropna()","c102423f":"sum(data.isnull().sum() != 0)","cdb03455":"y = np.log(df.averageprice)\nX = data.drop(['averageprice'], axis=1)","4c23051d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.33)","1382babc":"from sklearn import linear_model\nlr = linear_model.LinearRegression()","2f55c210":"model = lr.fit(X_train, y_train)","6f9cc1ef":"print (\"R^2 is: \\n\", model.score(X_test, y_test))","7339f16f":"predictions = model.predict(X_test)","7955a74e":"from sklearn.metrics import mean_squared_error\nprint ('RMSE is: \\n', mean_squared_error(y_test, predictions))","a4214209":"actual_values = y_test\nplt.scatter(predictions, actual_values, alpha=.7,\n            color='b') #alpha helps to show overlapping data\nplt.xlabel('Predicted Price')\nplt.ylabel('Actual Price')\nplt.title('Linear Regression Model')\nplt.show()","37615b22":"for i in range (-2, 3):\n    alpha = 10**i\n    rm = linear_model.Ridge(alpha=alpha)\n    ridge_model = rm.fit(X_train, y_train)\n    preds_ridge = ridge_model.predict(X_test)\n\n    plt.scatter(preds_ridge, actual_values, alpha=.75, color='green')\n    plt.xlabel('Predicted Price')\n    plt.ylabel('Actual Price')\n    plt.title('Ridge Regularization with alpha = {}'.format(alpha))\n    overlay = 'R^2 is: {}\\nRMSE is: {}'.format(ridge_model.score(X_test, y_test),mean_squared_error(y_test, preds_ridge))\n    plt.annotate(s=overlay,xy=(12.1,10.6),size='x-large')\n    plt.show()","0d6dcd77":"**Now, we want to evaluate the performance of the model.\nEach competition might evaluate the submissions differently.Root-mean-squared-error (RMSE). We\u2019ll also look at The r-squared value. The r-squared value is a measure of how close the data are to the fitted regression line. It takes a value between 0 and 1, 1 meaning that all of the variance in the target is explained by the data. In general, a higher r-squared value means a better fit.**\n\n**The model.score() method returns the r-squared value by default.**","0e0bb81f":"**5.Average price for flatormaisonette**","342f2945":"**Next, we\u2019ll consider rmse. To do so, use the model we have built to make predictions on the test data set.**","4bd1abad":"** The Ridge Regularization model takes a parameter, alpha , which controls the strength of the regularization.\n\nWe\u2019ll experiment by looping through a few different values of alpha, and see how this changes our results**.","af896f0d":"we\u2019ll examine the null or missing values.\n\nWe will create a DataFrame to view the top null columns. Chaining together the dfisnull().sum() methods, we return a Series of the counts of the null values in each column.","5f825f64":" We\u2019ll check for skewness, which is a measure of the shape of the distribution of values.\n\nWhen performing regression, sometimes it makes sense to log-transform the target variable when it is skewed. One reason for this is to improve the linearity of the data.","b2eddd6e":"# Wrangling the non-numeric Features","0326b473":"**df_test_split() returns four objects:\n* 1. X_train is the subset of our features used for training.\n* 1. X_test is the subset which will be our \u2018hold-out\u2019 set \u2013 what we\u2019ll use to test the model.\n* 1. y_train is the target variable SalePrice which corresponds to X_train.\n* 1. y_test is the target variable SalePrice which corresponds to X_test.\nThe first parameter value X denotes the set of predictor data, and y is the target variable. Next, we set random_state=42. This provides for reproducible results, since sci-kit learn\u2019s train_test_split will randomly partition the data. The test_size parameter tells the function what proportion of the data should be in the test partition. \ndf=test,train**","bdfd038d":"# Explore the data and engineer Features","5a6257f5":"4.Average price for terraced","1b93c2fe":"**Interpreting this value is somewhat more intuitive that the r-squared value. The RMSE measures the distance between our predicted values and actual values.**\n\n**We can view this relationship graphically with a scatter plot.**","e1586d27":"# Acquire the data and create our environment","8abf9396":"**1.Averageprice**","710ee2de":"# Begin modelling","2fc2971b":"**2.Average price for semi detached**","de3e93e4":"#  Build a linear model","511a1ccf":"**The model.predict() method will return a list of predictions given a set of predictors. Use model.predict() after fitting the model.**\n**\nThe mean_squared_error function takes two arrays and calculates the rmse.**","18b07bb5":"**This means that our features explain approximately 95% of the variance in our target variable.**","8a5522d0":"The count column indicates the count of non-null observations, while unique counts the number of unique values. top is the most commonly occurring value, with the frequency of the top value shown by freq.\n\nFor many of these features, we might want to use one-hot encoding to make use of the information for modeling.\nOne-hot encoding is a technique which will transform categorical data into numbers so the model can understand whether or not a particular observation falls into one category or another.","8438645d":"3.Average price for detached","4d1e7dbb":"# Handling Null Values\n","0c820a53":"# Evaluate the performance and visualize results","c63ebea7":"**Prepare our data for modeling. We\u2019ll separate the features and the target variable for modeling. We will assign the features to X and the target variable to y. We use np.log() as explained above to transform the y variable for the model. data.drop([features], axis=1) tells pandas which columns we want to exclude. **","032fe7fb":"# To Find SKEW","4d9ad5ae":"**Next, we need to fit the model. First instantiate the model and next fit the model. Model fitting is a procedure that varies for different types of models. Put simply, we are estimating the relationship between our predictors and the target variable so we can make accurate predictions on new data.**\n\n**We fit the model using X_train and y_train, and we\u2019ll score with X_test and y_test. The lr.fit() method will fit the linear regression on the features and target variable that we pass**.","cf5f70b8":"# Transforming and engineering features\n"}}