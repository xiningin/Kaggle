{"cell_type":{"11808688":"code","d91e3dda":"code","88436243":"code","10beff99":"code","3101d40a":"code","c240a5dc":"code","6923cd84":"code","45f9ffc7":"code","8777e302":"code","fe67659c":"code","3127701a":"code","04b27ed6":"code","52165972":"code","5f8fc5f8":"code","1742bb19":"code","5e79108d":"code","06ef946f":"code","7799e71f":"code","340b3e67":"code","bc2fd51e":"code","55effccd":"code","9e5e256e":"code","a785d40d":"code","9e3ccad4":"code","bf29cfb8":"code","fb46e766":"code","b9a60bfc":"code","de1945ae":"code","d4b27438":"code","6e245a71":"code","cc2d7f84":"code","c69df252":"code","0c1c9761":"code","f6e0a762":"code","a2b7ae51":"code","576f0ab1":"code","ef0ecf3d":"code","a4641457":"code","cb62d25c":"code","217a49ec":"code","8c86f8aa":"code","92aa883d":"code","6c60aeea":"code","aa68e326":"code","26a04aa4":"code","0c52b36e":"code","9e54da84":"code","22ad5021":"code","5dd7490a":"code","f63ffb16":"code","01b14199":"code","ecf05e6f":"code","1aa1c8cf":"code","f7b4b315":"markdown","9752f22c":"markdown","e70bdea8":"markdown","8f337171":"markdown","da6c7e43":"markdown","c4a5de83":"markdown","a6fe7581":"markdown","e9c4972b":"markdown","f884a8e0":"markdown","ccbe39cd":"markdown","73fa3a67":"markdown","edd4a718":"markdown","1afb29ce":"markdown","eaefcc89":"markdown","035f7fb3":"markdown","36255522":"markdown","c375045a":"markdown","a64e891e":"markdown","035b0607":"markdown","c76f8192":"markdown","4312a547":"markdown","d45dbb0a":"markdown","1cce2479":"markdown","788fee50":"markdown","ac59397a":"markdown","81795fea":"markdown","cd4a87b3":"markdown","ade748d8":"markdown","1349d0e5":"markdown","bd25d6bb":"markdown","2960b451":"markdown","5bb741ea":"markdown","04225c04":"markdown","6daea232":"markdown","ecc907ae":"markdown","ee6653ce":"markdown","cbe5fa5a":"markdown","179d879b":"markdown","43053d4e":"markdown","f71ac54c":"markdown","85c0b66b":"markdown","2be23c34":"markdown","39ff43bd":"markdown","b66975e1":"markdown","1242825f":"markdown"},"source":{"11808688":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn import ensemble, linear_model, neighbors, tree\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d91e3dda":"# Any results you write to the current directory are saved as output.\n\ndataset = pd.read_csv('..\/input\/titanic\/train.csv')\nX_test_full = pd.read_csv('..\/input\/titanic\/test.csv')\n\ndataset.head()","88436243":"dataset.info()","10beff99":"X_test_full.info()","3101d40a":"dataset.describe()","c240a5dc":"dataset.describe(include=['O'])","6923cd84":"dataset[[\"Pclass\", \"Survived\"]].groupby([\"Pclass\"], as_index=False).mean().sort_values(by='Survived', ascending=False)","45f9ffc7":"dataset[[\"SibSp\", \"Survived\"]].groupby([\"SibSp\"], as_index=False).mean().sort_values(by='Survived', ascending=False)","8777e302":"dataset[[\"Parch\", \"Survived\"]].groupby([\"Parch\"], as_index=False).mean().sort_values(by='Survived', ascending=False)","fe67659c":"g = sns.FacetGrid(dataset, col='Survived', height=5, aspect=1.2)\ng.map(plt.hist, 'Age')","3127701a":"g1 = sns.FacetGrid(dataset, col='Survived', row='Pclass', height=4, aspect=1.3)\ng1.map(plt.hist, 'Age')","04b27ed6":"g2 = sns.FacetGrid(dataset, col='Embarked', height=4, aspect=1.3)\ng2.map(sns.pointplot, 'Pclass', 'Survived','Sex')\nplt.legend()","52165972":"g3 = sns.FacetGrid(dataset, col='Embarked', row='Survived')\ng3.map(sns.barplot, 'Sex', 'Fare')","5f8fc5f8":"dataset = dataset.drop(['Ticket', 'Cabin'], axis=1)\nX_test_full = X_test_full.drop(['Ticket', 'Cabin'], axis=1)","1742bb19":"print(dataset.shape)\nprint(X_test_full.shape)\n#combined","5e79108d":"for data in [dataset, X_test_full]:\n    data['Salution'] = data['Name'].str.extract(' ([A-Z][a-z]+)\\.', expand=False)\n\npd.crosstab(dataset['Salution'], dataset['Sex'])","06ef946f":"pd.crosstab(X_test_full['Salution'], X_test_full['Sex'])","7799e71f":"dataset[dataset['Salution'] == 'Mlle']","340b3e67":"dataset[(dataset['Salution'] == 'Dr') & (dataset['Sex'] == 'female')]","bc2fd51e":"dataset.loc[dataset['PassengerId'] == 797, 'Salution'] = 'Mrs'\ndataset[dataset['PassengerId'] == 797]","55effccd":"dataset[dataset['Salution'].isin(['Lady', 'Countess'])]","9e5e256e":"dataset[dataset['Salution'].isin(['Rev', 'Don', 'Jonkheer'])]","a785d40d":"X_test_full[X_test_full['Salution'] == 'Dona']","9e3ccad4":"for ds in [dataset, X_test_full]:\n    ds['Salution'] = ds['Salution'].replace(['Ms', 'Mlle'], 'Miss')\n    ds['Salution'] = ds['Salution'].replace(['Lady', 'Countess', 'Dona', 'Mme'], 'Mrs')\n    ds['Salution'] = ds['Salution'].replace(['Dr', 'Rev', 'Capt', 'Col', 'Don', 'Dr', 'Jonkheer', 'Major', 'Sir'], 'Mr')","bf29cfb8":"dataset.head()","fb46e766":"dataset[['Salution', 'Survived']].groupby(['Salution']).mean().sort_values(by='Survived')","b9a60bfc":"dataset['Age'].isnull().sum()","de1945ae":"X_test_full['Age'].isnull().sum()","d4b27438":"median_age_1 = pd.DataFrame(dataset.groupby('Salution')['Age'].median()).reset_index()\nmedian_age_1['Age'] = median_age_1['Age'].astype(int)\nmedian_age_1","6e245a71":"median_age_2 = pd.DataFrame(X_test_full.groupby('Salution')['Age'].median()).reset_index()\nmedian_age_2['Age'] = median_age_2['Age'].astype(int)\nmedian_age_2","cc2d7f84":"def replace_na_ages(row, mdns):\n    if(pd.isna(row['Age'])):\n        row['Age'] = mdns[mdns['Salution'] == row['Salution']]['Age'].values[0]\n    return row\n\ndataset = dataset.apply(replace_na_ages, args=(median_age_1,), axis='columns')\nX_test_full = X_test_full.apply(replace_na_ages, args=(median_age_2,), axis='columns')","c69df252":"print(dataset['Age'].isnull().sum())\nprint(X_test_full['Age'].isnull().sum())","0c1c9761":"dataset['AgeBand'] = pd.cut(dataset['Age'], 6)\ndataset[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=False)","f6e0a762":"for dst in [dataset, X_test_full]:\n    dst.loc[ dst['Age'] <= 14, 'Age'] = 0\n    dst.loc[(dst['Age'] > 14) & (dst['Age'] <= 27), 'Age'] = 1\n    dst.loc[(dst['Age'] > 27) & (dst['Age'] <= 40), 'Age'] = 2\n    dst.loc[(dst['Age'] > 40) & (dst['Age'] <= 53), 'Age'] = 3\n    dst.loc[(dst['Age'] > 53) & (dst['Age'] <= 67), 'Age'] = 4\n    dst.loc[ dst['Age'] > 67, 'Age'] = 5\n\ndataset.head()","a2b7ae51":"dataset = dataset.drop(['AgeBand', 'Name'], axis=1)\nX_test_full = X_test_full.drop(['Name'], axis=1)\ndataset.head()","576f0ab1":"for ds in [dataset, X_test_full]:\n    ds['FamilySize'] = ds['SibSp'] + ds['Parch'] + 1\n\ndataset[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","ef0ecf3d":"for ds in [dataset, X_test_full]:\n    ds['IsAlone'] = 0\n    ds.loc[ds['FamilySize'] == 1, 'IsAlone'] = 1\n    \ndataset[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean().sort_values(by='IsAlone', ascending=False)","a4641457":"dataset = dataset.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\nX_test_full = X_test_full.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ndataset.head()","cb62d25c":"for ds in [dataset, X_test_full]:\n    ds['Age*Class'] = ds['Age'] * ds['Pclass']\n    \ndataset.loc[:, ['Age', 'Pclass', 'Age*Class']].head()","217a49ec":"X_test_full['Fare'].fillna(X_test_full['Fare'].dropna().median(), inplace=True)\nX_test_full.head()","8c86f8aa":"dataset['FareBand'] = pd.qcut(dataset['Fare'], 6)\ndataset[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=False)","92aa883d":"for ds in [dataset, X_test_full]:\n    ds.loc[ds['Fare'] <= 7.775, 'Fare'] = 0\n    ds.loc[(ds['Fare'] > 7.775) & (ds['Fare'] <= 8.662), 'Fare'] = 1\n    ds.loc[(ds['Fare'] > 8.662) & (ds['Fare'] <= 14.454), 'Fare'] = 2\n    ds.loc[(ds['Fare'] > 14.454) & (ds['Fare'] <= 26.0), 'Fare'] = 3\n    ds.loc[(ds['Fare'] > 26.0) & (ds['Fare'] <= 52.369), 'Fare'] = 4\n    ds.loc[(ds['Fare'] > 52.369) & (ds['Fare'] <= 512.329), 'Fare'] = 5\n    ds['Fare'] = ds['Fare'].astype(int)\n\ndataset.head()","6c60aeea":"full_y = dataset['Survived']\nfull_X = dataset.drop(['PassengerId', 'FareBand', 'Survived'], axis=1)\nprint(full_X.columns)","aa68e326":"categorical_cols = [cname for cname in full_X.columns if\n                    full_X[cname].nunique() < 10 and \n                    full_X[cname].dtype == \"object\"]\n\nnumerical_cols = [cname for cname in full_X.columns if \n                full_X[cname].dtype in ['int64', 'float64']]\n\n\nmy_cols = categorical_cols + numerical_cols\n\nprint(my_cols)\nfull_X = full_X[my_cols].copy()\nX_test = X_test_full[my_cols].copy()","26a04aa4":"X_train_full, X_valid_full, y_train, y_valid = train_test_split(full_X, full_y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)\nnumerical_transformer = SimpleImputer(strategy='most_frequent')\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ordinal', OrdinalEncoder())])\n\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)])","0c52b36e":"model = ensemble.RandomForestClassifier()\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)])\nparams = {\"model__n_estimators\" : [10, 20, 50, 100]}\nrandom_search = RandomizedSearchCV(clf, param_distributions=params, n_iter=5, scoring='roc_auc', n_jobs=-1, cv=5, verbose=3)\nrandom_search.fit(full_X, full_y)","9e54da84":"random_search.best_params_","22ad5021":"# Here just checking what are the configurable parameters. We can use them later for further optimization.\nmodel.get_params().keys()","5dd7490a":"model = ensemble.RandomForestClassifier(n_estimators=50)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)])\nclf.fit(X_train_full, y_train)\n\npreds = clf.predict(X_valid_full)\n\nprint('Accuracy:', accuracy_score(y_valid, preds))","f63ffb16":"model = ensemble.RandomForestClassifier()\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)])\nparams = {\n    \"model__n_estimators\" : [10, 20, 50, 100, 150, 200],\n    \"model__max_depth\" : [6, 8, 9, 10, 12],\n    \"model__random_state\" : [3, 4],\n    \"model__min_samples_split\" : [50, 75, 100],\n    \"model__max_features\" : [2, 4, 5, 10]\n}\n\nrandom_search = RandomizedSearchCV(clf, param_distributions=params, n_iter=5, scoring='roc_auc', n_jobs=-1, cv=5, verbose=3)\nrandom_search.fit(full_X, full_y)","01b14199":"random_search.best_params_","ecf05e6f":"model = ensemble.RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_split=50, random_state=3, max_features=5)\nclf = Pipeline(steps=[('preprocessor', preprocessor),\n                      ('model', model)])\nclf.fit(X_train_full, y_train)\n\npreds = clf.predict(X_valid_full)\n\nprint('Accuracy:', accuracy_score(y_valid, preds))","1aa1c8cf":"preds_test = clf.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId' : X_test_full['PassengerId'], 'Survived' : preds_test})\n\noutput.to_csv('submission.csv', index=False)","f7b4b315":"## Setting up data transformers\n\nAs stated earlier, instead of converting features or filling missing features manually, we will be using sklearn libraries corresponding to such tasks. Now there are both numerical and categorical features, which are segregrated before applying the methods.","9752f22c":"Yet another improvement: we can check whether a passenger has travelled alone or not by creating a feature 'IsAlone' which is equal to 1 if 'FamilySize' is 1.","e70bdea8":"Observations:\n* For embarked = 'S' or 'Q' higher survival rates are visible for females.\n* For embarked = 'C' however, male survival rates are higher.\n\nThis concludes that features 'Sex' and 'Embarked' are considered as features for classification.","8f337171":"Here the percentage rows indicate quartile values, 25% = 1st quartile, 50% = 2nd quartile (median), 75% = 3rd quartile\n\nFrom this table, we can conclude:\n* Most passengers didn't have any parent or child specified.\n* More than 50% of passengers didn't include any sibling or spouse.\n* More than 50% passengers of class 3.\n* Around 75% of the passengers paid less than 31, however some passengers paid much higher fares.\n\nThis does not include the categorical features, so we give `include=['O']` on parameter.","da6c7e43":"Observation: The dominating salutions were 'Mr', 'Miss', 'Mrs' and 'Master', and there were many fancy salutions as well (like 'Don', 'Rev').\n\nThe referred notebook had converted the fancy salutions to 'Rare' and replaced 'Mlle' with 'Miss'. Here, we are taking a slightly different approach and try to find out the most probable common salution for such cases using other features. This is done to increase the generalisation of the model.\n\nBut before proceeding, we observe the salution values in the test set as well.","c4a5de83":"From the features, the assumption seemed correct, and these are converted to 'Miss' later on.\n\nThere was only one female passenger with salution 'Dr' (may be doctor?). So, we observe the corresponding entry.","a6fe7581":"This shows that class 1 passengers had more survival rates, followed by class 2 passengers and class 3 passengers. True, in actual incident, there were compliants of favouring the upper class passengers.","e9c4972b":"W.r.t no. of parents \/ children, the trend is not clear. Only values 4 and 6 show 0 survival rate.\n\nNow we show the distribution of age values of passengers who survived or not. For this, we show an histogram of the age values, divided by survival values. This is conveniently done using `FacetGrid` from seaborn, and then mapping histogram function from `matplotlib` library.","f884a8e0":"Note: Since the classification model is inside a pipeline, therefore we suffix the parameter with `<model name>__` to indicate that the passed parameter is associated with this model.\n\nNow to get the best possible value of `n_estimators` we use `best_params_`.","ccbe39cd":"Now we remove both the 'AgeBand' and 'Name' features.","73fa3a67":"Observing the 'name' feature, it is found that all such names have a salution (as a form of respect). As the chances of survival was dependent on social class, so finding the salution was found to be useful. For extracting these salutions, we have used regular expressions - each salution was expected to have its first letter capital and a '.' was found at its end. These extracted solutions were stored as a new feature 'salution'.","edd4a718":"### *First things first, without thinking about what technical things we can explore here, let's remember the first thing that will come in most of our's minds when we hear* **Titanic**. Yes, the epic, the James Cameron directed film 'Titanic', released in 1997, starring Leonardo DCaprio as Jack, and Kate Winslet as Rose, which has been acclaimed worldwide for the depiction of the sink of RMS Titanic on 15th April, 1912, the actual incident being the domain of the dataset in this competition.","1afb29ce":"Now we remove the remaining not required features. 'PassengerId' wasn't used elsewhere, so that is removed from training dataset. Then, 'FareBand' was only created at the training dataset, and is removed accordingly. Also, we have separated the independent variables and the target variables ('Survived').","eaefcc89":"Now we observe the mean survival rates with respect to passenger class. For this we use the `groupby()` statement from pandas.","035f7fb3":"Observations:\n* Most passengers under age 20 survived.\n* Passengers within 15 - 30 years didn't survive generally.\n* Oldest passengers (within 70 - 80 age) survived.","36255522":"The person is aged, and the probablity of her being married seemed to be more (though the reverse may be true but this is a simplified assumption). So, her salution is changed to 'Mrs'.","c375045a":"The accuracy came around 0.84 (84%). This is of lesser accuracy than the original. So, from now onwards we will just change parameters and configure our model to get a suitable output for submission. \n\nThanks a lot to the viewers.","a64e891e":"So we see that:\n\n1. The dataset has 891 entries, and no. of attributes are 12.\n\n\n2. Most attributes have no null values, with the exception of 'Age', 'Cabin' and 'Embarked'.\n\n    * It seems that many didn't disclose their age.\n    * Cabin data is obviously incomplete.\n    * 2 of them weren't even assigned their embarkment port (strange!)\n    \n    \n3. 'Age' and 'Fare' are floats, 'Name', 'Sex', 'Ticket', 'Cabin' and 'Embarked' are objects (actually Strings) while others are integers.\n\nNow let's check the test dataset.","035b0607":"## Model creation (hyperparameter optimization)\nFrom the referred notebook, it has already been observed that the models `DecisionTree` and `RandomForest` have given maximum accuracy on classification (86.76%). In model `RandomForest`, there is a parameter called `n_estimators` which specifies the number of decision trees which are used in decision making. In the referred notebook, this parameter was set to 100. However, is it the best possible value? For this, instead of trying other models, we perform a search for the best value of `n_estimators`. \n\nHere we use `RandomizedSearchCV` from scikit-learn. Here we provide a dictionary for possible values of `n_estimators`, and using cross validation, the best value is found out.\n\nAlso, we have enclosed the data transformers and model in a pipeline for faster execution.","c76f8192":"Then, we create a new feature named 'Age * Class' which stores multiplication results of 'Age' and 'Pclass' values. One thing to be observed is that if 'Age' is 0 then this value is 0, so for infants, importance of passenger class is reduced.","4312a547":"Then we have converted the fare values into ordinals based on what fare groups they are lying.","d45dbb0a":"Observation: Higher fare ensures better chances of survival. Again, that is somehow related to the embarkment point (embarked='C' has higher fare values than others). Also, fare bands are recommended for use as features.","1cce2479":"So we see that:\n\n1. The dataset has 418 entries, and no. of attributes are 11 (the target variable 'Survived' is absent as expected).\n\n2. Most attributes have no null values, with the exception of 'Age', 'Cabin' and 'Fare' (instead of 'Embarked' as in training set).\n\n3. No difference in datatypes have been observed.","788fee50":"Observations:\n* Most passengers who didn't survive were of age 20 - 30 and of passenger class 3.\n* Infant passengers mostly survived in case of passenger class 1 and 2, but not in class 3.\n\nSo, 'PClass' is an important feature used in the model.","ac59397a":"Then the gap filling is performed. Use of `apply()` is used, which applies the method for age conversion to each row (especially the ones with missing ages).","81795fea":"So another salution, different from the ones on the training set ('Dona') has appeared. Now we start observing the rows with fancy salutions.\n\nWe start with the salution 'Mlle'.","cd4a87b3":"Now the final task in this case, i.e., conversion is done accordingly.","ade748d8":"Now we try to manipulate the 'Fare' feature. There is one missing value in the test data, so we try to fill that using `fillna()`.","1349d0e5":"With the exception of passengers having no spouse \/ sibling, the survival rates show an overall downward trend.","bd25d6bb":"Now based on the approximation of intervals obtained here, we convert the age values to ordinals based on groups.","2960b451":"## Manipulating data\n\nHere the data has been 'wrangled' (processed), features have been added or deleted based on their utitlity. As seen earlier, for some features, data is absent. The question then arises whether we should remove these features or keep them by guessing \/ imputing the absent values. The earlier observations have helped us to determine some of the features which impact the survival rates, and the features which don't contribute much useful information.\n\nSo from earlier observations, it seemed that 'Cabin' feature is largely incomplete. Also, there is not much need of 'Ticket' feature as it is just an identifier for the issued ticket. Therefore, these 2 features are dropped from both training and testing sets.","5bb741ea":"Then, we find appropriate age groups serving as age bands. Initially, 6 equal age intervals are found using `cut()` of `pandas`. Then we observed the survival rates by these groups.","04225c04":"Now the survival rates are observed w.r.t. the newly obtained feature, i.e., 'Salution'.","6daea232":"What are the attributes of this dataset? What are the types of datas stored in each of them? And how many entries are there in this dataset? All of these questions can be conveninently answered using info().","ecc907ae":"Next, we perform the following:\n\n1. Original training set is now divided into training set and validation set.\n2. For both numerical and categorical features, a SimpleImputer object has been defined. Its task is to fill the missing values (though most have been filled, but to ensure that no missing features are left).\n3. For categorical features, a OrdinalEncoder object has been defined, which performs one hot encoding for the categorical features.","ee6653ce":"Observations:\n\n* Frequency here means the count of occurance of most common value, for example, 'male' value of column 'Sex' is most frequent with a frequency of 577.\n* 'Name' values are all unique, but 'Cabin' values are not, so 2 or more passengers may share a cabin.\n","cbe5fa5a":"Now we remove the features 'SibSp', 'Parch' and 'FamilySize'. The information represented by them is now represented using 'IsAlone'.","179d879b":"Then we create bands for 'Fare' feature. Instead of 4 in the original, we have created 6.","43053d4e":"This is what this Kaggle challenge asks for in this regard.\n\n> On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n> \n> While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n> \n> In this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n> \n\nSo this notebook will largely follow the steps of [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions) by [Manav Sehgal](https:\/\/www.kaggle.com\/startupsci), however some steps will be altered as per requirement.\n\nLet's import some of the required models first.","f71ac54c":"Unlike the referred notebook, we don't convert these titles into ordinals. We use ordinal encoding as provided by `scikit-learn` for this purpose.\n\nWe now try to guess the missing age values, and determine age groups. At first we find the number of null values in both datasets.","85c0b66b":"## Understanding data values and their trends\n\n\nTo get an idea about the column values, we can use `describe()`.","2be23c34":"Then, we find the median ages of passengers grouped by salutions. This helps in better gap filling as it now considers the gender and financial states of passengers.","39ff43bd":"Now, features like no. of parents \/ child and sibling and spouse essentially mean the same thing. Thus, we can replace them with simpler features like 'FamilySize'. Then we check the survival rates w.r.t. feature 'FamilySize'.","b66975e1":"Then the data; watch the training dataset first.","1242825f":"Now we observe some of the other salutions."}}