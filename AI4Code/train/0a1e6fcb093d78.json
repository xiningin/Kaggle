{"cell_type":{"1e690080":"code","bc6a49fd":"code","06d813d3":"code","61004cef":"code","e97f4e4d":"code","2cf6ca9e":"code","3cf76e67":"code","be4ae753":"code","fe8ced40":"code","9c5839fc":"code","1cb7521c":"code","4b8574fd":"code","2704d469":"code","08b4f894":"code","eb6df174":"code","5d0de190":"markdown","e6f15ef3":"markdown","1ac6c56a":"markdown","31f75d27":"markdown","40c7d49a":"markdown","71b79f7e":"markdown","1cd0df49":"markdown","f57e2faa":"markdown","4fb9bf41":"markdown","2b717391":"markdown","697a10e9":"markdown","48a9ece0":"markdown","26b9ad84":"markdown","bb02e8e6":"markdown"},"source":{"1e690080":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bc6a49fd":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nimport matplotlib.pyplot as plt","06d813d3":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","61004cef":"\ndf_train['BsmtFinSF1'].fillna(0, inplace=True)\ndf_train['BsmtFinSF2'].fillna(0, inplace=True)\ndf_train['TotalBsmtSF'].fillna(0, inplace=True)\ndf_train['BsmtUnfSF'].fillna(0, inplace=True)\ndf_train['Electrical'].fillna('FuseA', inplace=True)\ndf_train['KitchenQual'].fillna('TA', inplace=True)\ndf_train['LotFrontage'].fillna(df_train.groupby('1stFlrSF')['LotFrontage'].transform('mean'), inplace=True)\ndf_train['LotFrontage'].interpolate(method='linear', inplace=True)\ndf_train['MasVnrArea'].fillna(df_train.groupby('MasVnrType')['MasVnrArea'].transform('mean'), inplace=True)\ndf_train['MasVnrArea'].interpolate(method='linear', inplace=True)\n\ndf_test['BsmtFinSF1'].fillna(0, inplace=True)\ndf_test['BsmtFinSF2'].fillna(0, inplace=True)\ndf_test['TotalBsmtSF'].fillna(0, inplace=True)\ndf_test['BsmtUnfSF'].fillna(0, inplace=True)\ndf_test['Electrical'].fillna('FuseA', inplace=True)\ndf_test['KitchenQual'].fillna('TA', inplace=True)\ndf_test['LotFrontage'].fillna(df_train.groupby('1stFlrSF')['LotFrontage'].transform('mean'), inplace=True)\ndf_test['LotFrontage'].interpolate(method='linear', inplace=True)\ndf_test['MasVnrArea'].fillna(df_train.groupby('MasVnrType')['MasVnrArea'].transform('mean'), inplace=True)\ndf_test['MasVnrArea'].interpolate(method='linear', inplace=True)\n\n# nas = df_train.isna().sum()[df_train.isna().sum() > 0].index.to_list()\n# df_train.drop(nas, axis=1, inplace=True)\n\ndf_train['Total_sq_ft'] = (df_train['BsmtFinSF1'] + df_train['BsmtFinSF2'] + df_train['1stFlrSF'] +\n                           df_train['2ndFlrSF'] + df_train['TotalBsmtSF'])\ndf_train['SqFtPerRoom'] = df_train['GrLivArea'] \/ (df_train['TotRmsAbvGrd'] + df_train['FullBath'] +\n                                                   df_train['HalfBath'] + df_train['KitchenAbvGr'])\n\ndf_test['Total_sq_ft'] = (df_test['BsmtFinSF1'] + df_test['BsmtFinSF2'] + df_test['1stFlrSF'] +\n                          df_test['2ndFlrSF'] + df_test['TotalBsmtSF'])\ndf_test['SqFtPerRoom'] = df_test['GrLivArea'] \/ (df_test['TotRmsAbvGrd'] + df_test['FullBath'] +\n                                                 df_test['HalfBath'] + df_test['KitchenAbvGr'])\n","e97f4e4d":"df = df_train[['Id', 'LotArea', 'FullBath', 'HalfBath', 'GarageCars',\n               'CentralAir', 'KitchenQual',\n               'Total_sq_ft', 'SqFtPerRoom', 'OverallQual',\n               # 'Street','LandContour','ExterCond','HeatingQC','LotConfig','BldgType','OverallCond',\n               #  'PavedDrive','Utilities',\n               #        'BsmtCond','BsmtFinType1','BsmtFinType2','SaleType','SaleCondition',\n               'SalePrice']]\nprint(df.info())\nlabels = df.dtypes[df.dtypes == 'object'].index.to_list()\nprint(df.isna().sum())\n\ndf = df.drop('Id', axis=1)\ncols = df.drop('SalePrice', axis=1).columns.tolist()\n","2cf6ca9e":"df_enc = pd.get_dummies(df, columns=labels)\n# sns.heatmap(df_enc.corr(), cmap='YlGnBu', annot=True)","3cf76e67":"top_features = df_enc.corr()[['SalePrice']].sort_values(by=['SalePrice'],ascending=False).head(30)\nplt.figure(figsize=(5,10))\nsns.heatmap(top_features,cmap='rainbow',annot=True,annot_kws={\"size\": 16},vmin=-1)","be4ae753":"X = df_enc.drop('SalePrice', axis=1)\ny = df_enc['SalePrice']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=123)\n\nX_test = df_test[cols]","fe8ced40":"rf = RandomForestRegressor(random_state=123)\nrf.fit(X_train, y_train)\nprint(f'The initial model score is {rf.score(X_valid, y_valid):.2f}')\nprint(f'The initial model MSE is {mean_squared_error(y_valid, rf.predict(X_valid), squared=False):.2f}')","9c5839fc":"# Run optimzations\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=6)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num=6)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nrf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid, n_iter=100, cv=3, verbose=2,\n                               random_state=42, n_jobs=-1)  # Fit the random search model\nrf_random.fit(X_train, y_train)\n\nprint(f'The optimal model score is {rf_random.score(X_valid, y_valid):.2f}')\nprint(f'The optimium parameters so far are: {rf_random.best_params_}')","1cb7521c":"param_grid = {'n_estimators': [900, 2500, 3000],\n              'min_samples_split': [2, 5, 7],\n              'max_features': ['sqrt'],\n              'max_depth': [15, 50, 90],\n              'bootstrap': [True]}\n\ngrid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\ngrid_search.fit(X_train, y_train)\nprint(f'New model has a score of {grid_search.score(X_valid, y_valid):.2f}')\n\nprint(grid_search.best_params_)\n\nrf = RandomForestRegressor(random_state=123, **grid_search.best_params_)\nrf.fit(X_train, y_train)\nfinal_score = mean_squared_error(y_valid,rf.predict(X_valid),squared=False)\nprint(f\"The final Random Forest Model has a score of {final_score:.2f}\")\nrf_import = rf.feature_importances_\nRF_pred = rf.predict(X_valid)\nprint(f'The final Random Forest Model has an mse of {mean_}')","4b8574fd":"fig, ax = plt.subplots()\nax = sns.barplot(x=X.columns, y=rf_import)\nplt.xticks(rotation=45)","2704d469":"\n# Trying an XGB Regression model\neval_set = [(X_train, y_train), (X_valid, y_valid)]\neval_metric = ['rmse']\n\ngb = xgb.XGBRegressor(objective='reg:squarederror')\ngb.fit(X_train, y_train, eval_set=eval_set, eval_metric=eval_metric)\nresults = gb.evals_result()\nepochs = len(results['validation_0']['rmse'])\nx_axis = range(0, epochs)\nprint(gb.get_params())\n\nfig, ax = plt.subplots()\nax.plot(x_axis, results['validation_0']['rmse'], label='Train')\nax.plot(x_axis, results['validation_1']['rmse'], label='Test')\nax.legend()\nplt.ylabel('RMSE')\nplt.xlabel('epochs')\nplt.title('XGBoost Error')\nplt.show()","08b4f894":"\nparams = {'max_depth': [1, 3, 5],\n          'learning_rate': [0.001, 0.01, 0.1, .2],\n          'n_estimators': [25, 50, 100],\n          'colsample_bytree': [.1, .7, 1],\n          'gamma': [0, 1]\n          }\n\nxgbr = xgb.XGBRegressor(seed=123)\ncv = GridSearchCV(estimator=xgbr,\n                  param_grid=params,\n                  scoring='neg_root_mean_squared_error',\n                  verbose=1)\ncv.fit(X_train, y_train)\nprint(\"Best parameters:\", cv.best_params_)\nprint(mean_squared_error(y_valid, cv.predict(X_valid), squared=False))\n\ngb_best = xgb.XGBRegressor(seed=123, **cv.best_params_)\ngb_best.fit(X_train, y_train)\ny_pred = gb_best.predict(X_valid)\nprint(mean_squared_error(y_valid, y_pred, squared=False))","eb6df174":"X_results = pd.get_dummies(df_test[cols],columns=labels)\n\nresults = gb_best.predict(X_results)\n\nresults = pd.DataFrame(results,columns=['SalePrice'])\nresults['Id'] = df_test['Id']\nresults = results[['Id','SalePrice']]\nresults.head()\n\nresults.to_csv('submission.csv',index=False)","5d0de190":"Imports","e6f15ef3":"Finish prepping the data for analysis and split the training data","1ac6c56a":"Remove NA Values","31f75d27":"Run a grid search to determine the best  model parameters.","40c7d49a":"Try a random forest regressor to start","71b79f7e":"Since the gradient boost model has the lowest MSE yet (and it's approximately <15% of the average Sale Price). Encode the test data, run the final submission data (df_test). Create a submission file of the results.","1cd0df49":"Run the data through an extreme gradient boost model. Start by creating the regressor, training the model and then visualizing the epoch vs. error\n","f57e2faa":"One Hot Code all the qualitative columns","4fb9bf41":"Plot the most important features for the random forest model","2b717391":"Draw in the data. Keep it split","697a10e9":"Downselect all the data so a few specific and relevant features. Keep the kitchen data in there as kitchens are important when buying or selling a house.","48a9ece0":"Run a random parameter search to get an idea of where the parameters should be","26b9ad84":"Highlight the top features","bb02e8e6":"Using the data from the random grid search, pick some narrower parameters for optimizetion"}}