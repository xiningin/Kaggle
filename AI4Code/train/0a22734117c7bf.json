{"cell_type":{"534d07af":"code","58d1df0f":"code","f2e3dae8":"code","e6130ac2":"code","6a62320f":"code","364ce6a1":"code","35223572":"code","d5a65ce4":"code","c0d9205a":"code","f5512208":"code","aaf16b9d":"code","4dca0b59":"code","31f84dd5":"code","dabeaa01":"code","f727df9c":"code","1e2c0676":"code","c225da2d":"code","1b28af08":"code","ae088e46":"code","5def9edc":"code","a058ba77":"code","c599d2c0":"code","ff36db30":"code","0b743ce8":"code","eaaef5be":"code","e204dde2":"code","bc1c83f9":"code","8a014491":"code","ad705445":"code","ae00b9f2":"code","856173a5":"code","b184b3b9":"markdown","9a8a8bac":"markdown","1b52f1a9":"markdown","8585c404":"markdown","271d10eb":"markdown","34c32354":"markdown","d7ee431b":"markdown","f34c4d3e":"markdown","a4394f90":"markdown","71fc65ae":"markdown","c3543487":"markdown","4bb96cb1":"markdown","b8ee4f66":"markdown","c9142e12":"markdown","3bf88192":"markdown","e9527c23":"markdown","1a6e5297":"markdown","9afee904":"markdown","f7628b25":"markdown","9616e654":"markdown","17923da4":"markdown"},"source":{"534d07af":"# \u74b0\u5883\u306b\u3088\u3063\u3066\u51e6\u7406\u3092\u5909\u3048\u308b\u305f\u3081\u306e\u3082\u306e\nimport sys\nIN_COLAB = 'google.colab' in sys.modules\nIN_KAGGLE = 'kaggle_web_client' in sys.modules\nLOCAL = not (IN_KAGGLE or IN_COLAB)\nprint(f'IN_COLAB:{IN_COLAB}, IN_KAGGLE:{IN_KAGGLE}, LOCAL:{LOCAL}')","58d1df0f":"%%python\nimport numpy as np\nimport pandas as pd\nimport os\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int64)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float64)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\nif not os.path.isfile('.\/train_updated_playerBoxScores.pickle'):\n    # drop playerTwitterFollowers, teamTwitterFollowers from example_test\n    df = pd.read_csv(f\"..\/input\/mlb-player-digital-engagement-forecasting\/train_updated.csv\").dropna(axis=1,how='all')\n    df = df.query(\"date >= 20210501\")\n    daily_data_nested_df_names = df.drop('date', axis = 1).columns.values.tolist()\n    \n    for df_name in daily_data_nested_df_names:\n        date_nested_table = df[['date', df_name]]\n    \n        date_nested_table = (date_nested_table[\n          ~pd.isna(date_nested_table[df_name])\n          ].\n          reset_index(drop = True)\n          )\n    \n        daily_dfs_collection = []\n    \n        for date_index, date_row in date_nested_table.iterrows():\n            daily_df = pd.read_json(date_row[df_name])\n    \n            daily_df['dailyDataDate'] = date_row['date']\n    \n            daily_dfs_collection = daily_dfs_collection + [daily_df]\n    \n        # Concatenate all daily dfs into single df for each row\n        unnested_table = (pd.concat(daily_dfs_collection,\n          ignore_index = True).\n          # Set and reset index to move 'dailyDataDate' to front of df\n          set_index('dailyDataDate').\n          reset_index()\n          )\n        #print(f\"{file}_{df_name}.pickle\")\n        #display(unnested_table.head(3))\n        reduce_mem_usage(unnested_table).to_pickle(f\"train_updated_{df_name}.pickle\")\n        #print('\\n'*2)\n    \n        # Clean up tables and collection of daily data frames for this df\n        del(date_nested_table, daily_dfs_collection, unnested_table)","f2e3dae8":"%%python\nimport numpy as np\nimport pandas as pd\nfrom numpy import mean,std\nfrom scipy.stats import norm\nimport statistics as st\nimport warnings\n\nwarnings.simplefilter('ignore')\n\ndef calc_probs(year, pid, df, temp, patern):\n    to_append=[year, pid,'','','','','','','','','','','','','','','','','','','','','','','','']\n    targets=['target1','target2','target3','target4']\n    z=2\n    for target in targets:\n        target_prob = temp[target].tolist()\n        mean = np.mean(target_prob) # \u5e73\u5747\u5024\n        std = np.std(target_prob) # \u6a19\u6e96\u504f\u5dee\n        median = st.median(target_prob) # \u4e2d\u592e\u5024\n        distribution = norm(mean, std) # \u30ce\u30eb\u30e0\n        min_weight = min(target_prob) # \u6700\u5c0f\u5024\n        max_weight = max(target_prob) # \u6700\u5927\u5024\n        values = list(np.linspace(min_weight, max_weight)) # \u30c7\u30d5\u30a9\u30eb\u30c850\n        probabilities = [distribution.pdf(v) for v in values]\n        max_value = max(probabilities)\n        max_index = probabilities.index(max_value) # \u78ba\u7387\u5bc6\u5ea6\u95a2\u6570\u306e\u9802\u70b9\u306flinspace\u306e\u4f55\u500b\u76ee\u304b\n        to_append[z]=mean # \u5e73\u5747\n        to_append[z+1]=median # \u4e2d\u592e\u5730\n        to_append[z+2]=std # \u6a19\u6e96\u504f\u5dee\n        to_append[z+3]=min_weight # \u6700\u5c0f\u5024\n        to_append[z+4]=max_weight # \u6700\u5927\u5024\n        # \u3088\u304f\u308f\u304b\u3089\u306a\u3044\u306e\u3067\u8907\u6570\u30d1\u30bf\u30fc\u30f3\u7528\u610f\n        # ============================\n        if patern == 1:\n            to_append[z+5]=target_prob[max_index] # \u78ba\u7387\u5bc6\u5ea6\u5909\u6570\u306e\u9802\u70b9\n        elif patern == 2:\n            to_append[z+5]=sorted(target_prob)[max_index] # \u5b9f\u6e2c\u5024\n        elif patern == 3:\n            to_append[z+5]=values[max_index] # \u6b63\u898f\u5206\u5e03\u306e\u4e2d\u5fc3\n        z=z+6\n    df_length = len(df)\n    df.loc[df_length] = to_append\n    return df\n\n# 2021\u5e748\u6708\u4ee5\u964d\u7528\u306e\u30b9\u30bf\u30c3\u30c4\u3092\u4f5c\u308b\ntargets = pd.read_pickle('.\/train_updated_nextDayPlayerEngagement.pickle')\ntargets = targets.query('20210601 <= dailyDataDate')\n\n# CREATE DATAFRAME to store probabilities\ncolumn_names = [\"year\", \"playerId\", \"target1_mean\",\"target1_median\",\"target1_std\",\"target1_min\",\"target1_max\",\"target1_prob\", \n                \"target2_mean\",\"target2_median\",\"target2_std\",\"target2_min\",\"target2_max\",\"target2_prob\", \n                \"target3_mean\",\"target3_median\",\"target3_std\",\"target3_min\",\"target3_max\",\"target3_prob\", \n                \"target4_mean\",\"target4_median\",\"target4_std\",\"target4_min\",\"target4_max\",\"target4_prob\"]\nplayer_target_probs = pd.DataFrame(columns = column_names)\nyear_by_probs = pd.DataFrame(columns = column_names)\n\nyears = [\"2021\"]\ndfs = [targets]\n\nfor year, df in zip(years, dfs):\n    playerId_list = df.playerId.unique().tolist()\n    for pid in playerId_list:\n        temp = df[df['playerId'] == pid]\n        player_target_stats=calc_probs(year, pid, player_target_probs, temp, patern=3)\n\ndf = pd.read_csv('..\/input\/mlb-features\/statsdata.csv')\ndf8 = player_target_stats.copy()\ndf9 = player_target_stats.copy()\ndf10 = player_target_stats.copy()\ndf8['month'] = 8\ndf9['month'] = 9\ndf10['month'] = 10\nplayer_target_stats = pd.concat([df, df8, df9, df10],axis=0).reset_index(drop=True)\nprint(player_target_stats.groupby(['year','month']).size())\nplayer_target_stats.to_csv('player_target_stats.csv', index = False)","e6130ac2":"# Standard library\nimport os, sys, gc, time, warnings, shutil, random\nfrom pathlib import Path\nfrom contextlib import contextmanager\n\n# third party\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb\n\n# \nimport mlb\n\npd.set_option('display.max_rows', 500)\nprint(lgb.__version__)","6a62320f":"class CFG:\n    seed = 29","364ce6a1":"# Seed\ndef set_seed(seed: int = 29):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    \nset_seed(CFG.seed)","35223572":"INPUT_DIR = Path('..\/input')\nUNNESTED_DIR = INPUT_DIR \/ 'mlb-unnested'\n\n# non update files\n# ======================================================================================\n#df_players = pd.read_pickle(UNNESTED_DIR \/ 'players.pickle')\ndf_players = pd.read_csv(INPUT_DIR \/ 'playerscsv\/NEWplayers.csv') # salarydata\ndf_teams = pd.read_pickle(UNNESTED_DIR \/ 'teams.pickle').rename(columns = {'id':'teamId'})\n\n# update files\n# ======================================================================================\ndf_targets = pd.concat([pd.read_pickle(UNNESTED_DIR \/ 'train_nextDayPlayerEngagement.pickle'),\n                        pd.read_pickle('.\/train_updated_nextDayPlayerEngagement.pickle')],axis=0).reset_index(drop=True)\ndf_games = pd.concat([pd.read_pickle(UNNESTED_DIR \/ 'train_games.pickle'),\n                      pd.read_pickle('.\/train_updated_games.pickle')],axis=0).reset_index(drop=True)\ndf_rosters = pd.concat([pd.read_pickle(UNNESTED_DIR \/ 'train_rosters.pickle'),\n                        pd.read_pickle('.\/train_updated_rosters.pickle')],axis=0).reset_index(drop=True)\ndf_scores = pd.concat([pd.read_pickle(UNNESTED_DIR \/ 'train_playerBoxScores.pickle'),\n                       pd.read_pickle('.\/train_updated_playerBoxScores.pickle')],axis=0).reset_index(drop=True)\ndf_team_scores = pd.concat([pd.read_pickle(UNNESTED_DIR \/ 'train_teamBoxScores.pickle'),\n                            pd.read_pickle('.\/train_updated_teamBoxScores.pickle')],axis=0).reset_index(drop=True)\ndf_transactions = pd.concat([pd.read_pickle(UNNESTED_DIR \/ 'train_transactions.pickle'),\n                             pd.read_pickle('.\/train_updated_transactions.pickle')],axis=0).reset_index(drop=True)\ndf_standings = pd.concat([pd.read_pickle(UNNESTED_DIR \/ 'train_standings.pickle'),\n                          pd.read_pickle('.\/train_updated_standings.pickle')],axis=0).reset_index(drop=True)\ndf_awards = pd.concat([pd.read_pickle(UNNESTED_DIR \/ 'train_awards.pickle'),\n                       pd.read_pickle('.\/train_updated_awards.pickle')],axis=0).reset_index(drop=True)\ntwitter_players = pd.concat([pd.read_pickle(UNNESTED_DIR \/ 'train_playerTwitterFollowers.pickle'),\n                             pd.read_pickle('.\/train_updated_playerTwitterFollowers.pickle')],axis=0).reset_index(drop=True)\ntwitter_team = pd.concat([pd.read_pickle(UNNESTED_DIR \/ 'train_teamTwitterFollowers.pickle'),\n                          pd.read_pickle('.\/train_updated_teamTwitterFollowers.pickle')],axis=0).reset_index(drop=True)\n\n# \u30cd\u30b9\u30c8\u3092\u5916\u3059\u3068\u304dnan\u306e\u5834\u5408\u304c\u3042\u308a\u3001\u305d\u306e\u3068\u304d\u306f\u540c\u3058\u5f62\u306edataframe\u3092\u4f5c\u308b\u5fc5\u8981\u304c\u3042\u308b\ncolumns_dict = {'games': df_games.columns,\n                'rosters': df_rosters.columns,\n                'playerBoxScores': df_scores.columns,\n                'teamBoxScores': df_team_scores.columns,\n                'transactions': df_transactions.columns,\n                'standings': df_standings.columns,\n                'awards': df_awards.columns,\n                'playerTwitterFollowers':twitter_players.columns,\n                'teamTwitterFollowers':twitter_team.columns}","d5a65ce4":"# Setting COL list\n# ======================================================================================\n# COL_PLAYERS = ['playerId', 'primaryPositionCode', 'american']\nCOL_PLAYERS = ['playerId', 'primaryPositionName', 'american', 'salary']\nCOL_TEAMS = ['teamId', 'leagueId', 'divisionId']\n\nCOL_ROSTERS = ['dailyDataDate', 'playerId', 'teamId', 'statusCode']\nCOL_STANDINGS = ['dailyDataDate', 'teamId', 'wins', 'losses', 'lastTenWins','lastTenLosses']\nCOL_SCORES = [i for i in df_scores.columns.to_list() if i not in ['gamePk','gameDate','gameTimeUTC','teamId','teamName','playerName','positionName','positionType','jerseyNum', 'battingOrder']]\n\nCOL_STANDINGS = ['wins', 'losses', 'lastTenWins','lastTenLosses']\n\n\ntmp_feature_set = set(COL_PLAYERS + COL_ROSTERS + COL_SCORES + COL_TEAMS + COL_STANDINGS)\ntmp_feature_set.discard('dailyDataDate')\ntmp_feature_set.discard('gameDate')\ntmp_feature_set.discard('playerId')\nCOL_FEATURES = list(tmp_feature_set)\nCOL_TARGETS = ['target1', 'target2', 'target3', 'target4']","c0d9205a":"def FE_team_score(df_team_scores):\n    \"\"\"\n    \u305d\u306e\u65e5\u30c1\u30fc\u30e0\u306e\u52dd\u6557\u3001\u3069\u3046\u3044\u3046\u52dd\u3061\u65b9\u3092\u3057\u305f\u304b\u3069\u3046\u304b\u306e\u7279\u5fb4\u91cf\u3092\u4f5c\u6210\u3059\u308b\n    \"\"\"\n    df_team_scores = df_team_scores.rename(columns={'runsScored':'team_runsScored', 'runsPitching':'team_runsPitching'})\n    # \u52dd\u3061\u8ca0\u3051\n    df_team_scores.loc[df_team_scores['team_runsScored'] > df_team_scores['team_runsPitching'], ['team_win']] = 1\n    # \u5f97\u5931\u70b9\u5dee\n    df_team_scores['team_runsdiff'] = df_team_scores['team_runsScored'] - df_team_scores['team_runsPitching']\n    # \u5b8c\u5c01\u52dd\u3061\n    df_team_scores.loc[(df_team_scores['team_runsScored'] > 0) & (df_team_scores['team_runsPitching'] == 0),['team_shutout_win']] = 1\n    df_team_scores.loc[(df_team_scores['team_runsScored'] == 0) & (df_team_scores['team_runsPitching'] > 0),['team_shutout_lose']] = 1\n    # fillna\n    df_team_scores[['team_win','team_shutout_win','team_shutout_lose']] = df_team_scores[['team_win','team_shutout_win','team_shutout_lose']].fillna(0)\n    # double header\n    df_team_scores = df_team_scores.groupby(['dailyDataDate','teamId']).sum().reset_index()\n    \n    df_team_scores = df_team_scores[['dailyDataDate','teamId'] + COL_TEAMSCORE]\n    return df_team_scores\n","f5512208":"# Players\n# =========================================================================================\ndf_players['american'] = df_players['birthCountry'].apply(lambda x:1 if x == 'USA' else 0)\n\n# playerBoxScores \n# =========================================================================================\nprint(f'scores shape {df_scores.shape}')\ndf_scores = df_scores.groupby(['playerId','dailyDataDate']).sum().reset_index()\nprint(f'marged shape {df_scores.shape}')\n\n# teamBoxScores \n# =========================================================================================\nCOL_TEAMSCORE = ['team_win','team_runsScored','team_runsPitching', 'team_runsdiff','team_shutout_win','team_shutout_lose']\nCOL_FEATURES = COL_FEATURES + COL_TEAMSCORE\nprint(f'team scores shape {df_team_scores.shape}')\ndf_team_scores = FE_team_score(df_team_scores)\nprint(f'team scores shape {df_team_scores.shape}')\n\n# award \n# =========================================================================================\nCOL_AWARDS = ['dailyDataDate', 'playerId','num_of_award']\nCOL_FEATURES = COL_FEATURES + ['num_of_award']\n\ndf_awards = df_awards.groupby([\"dailyDataDate\",\"playerId\"]).size().reset_index()\ndf_awards = df_awards.rename(columns={0: 'num_of_award'})\n\n# transaction\n# =========================================================================================\nCOL_TRANSACTION = ['trade']\nCOL_FEATURES = COL_FEATURES + COL_TRANSACTION\n# 0\u884c\u3067\u3082\u4e00\u5fdc\u52d5\u304f\ndf_transactions = df_transactions.query('typeDesc == \"Trade\"').dropna(subset=['playerId']).reset_index(drop=True)\ndf_transactions = df_transactions[['dailyDataDate','playerId']]\ndf_transactions = df_transactions.drop_duplicates().reset_index(drop=True)\ndf_transactions['trade'] = 1\n\n# twitter\n# =========================================================================================\n# twitter_players = pd.read_pickle(UNNESTED_DIR \/ 'train_playerTwitterFollowers.pickle')\n# twitter_team = pd.read_pickle(UNNESTED_DIR \/ 'train_teamTwitterFollowers.pickle')\n# \n# df_train['yearmonth'] = df_train['dailyDataDate'].astype(str).str[:6].astype(np.int64)\n# twitter_players['yearmonth'] = twitter_players['dailyDataDate'].astype(str).str[:6].astype(np.int64)\n# twitter_team['yearmonth'] = twitter_team['dailyDataDate'].astype(str).str[:6].astype(np.int64)\n# \n# twitter_players = twitter_players.rename(columns={'numberOfFollowers': 'numberOfFollowers_player'})\n# twitter_team = twitter_team.rename(columns={'numberOfFollowers': 'numberOfFollowers_team'})\n# \n# df_train = df_train.merge(twitter_players[['yearmonth', 'playerId','numberOfFollowers_player']], on=['yearmonth', 'playerId'], how='left')\n# df_train = df_train.merge(twitter_team[['yearmonth', 'teamId','numberOfFollowers_team']], on=['yearmonth', 'teamId'], how='left')\n# df_train[['numberOfFollowers_player','numberOfFollowers_team']] = df_train[['numberOfFollowers_player','numberOfFollowers_team']].fillna(-1)\n# df_train = df_train.drop(columns='yearmonth')\n# COL_TWITTER = ['numberOfFollowers_player', 'numberOfFollowers_team']\n# COL_FEATURES = COL_FEATURES + COL_TWITTER\n","aaf16b9d":"%%time\nprint(df_targets.shape)\n# Focus on regular season data\ndf_targets = df_targets.query('20180329 <= dailyDataDate <= 20181001 | \\\n                               20190328 <= dailyDataDate <= 20190929 | \\\n                               20200723 <= dailyDataDate <= 20200927 | \\\n                               20210401 <= dailyDataDate').reset_index(drop=True)\n\nprint(f\"filtered{df_targets.shape}\")\n# Create train dataframe\ndf_train = df_targets.merge(df_players[COL_PLAYERS], on=['playerId'],how='left')\ngc.collect()\nprint(df_train.shape, 'after_players')\nprint('--------------------------------------')\n\ndf_train = df_train.merge(df_rosters[COL_ROSTERS], on=['playerId','dailyDataDate'], how='left')\ngc.collect()\nprint(df_train.shape, 'after_rosters')\nprint('--------------------------------------')\n\ndf_train = df_train.merge(df_scores[COL_SCORES], on=['playerId','dailyDataDate'], how='left')\ngc.collect()\nprint(df_train.shape, 'after_scores')\nprint('--------------------------------------')\n\ndf_train = df_train.merge(df_team_scores[['dailyDataDate','teamId'] + COL_TEAMSCORE], on=['dailyDataDate','teamId'], how= 'left')\ngc.collect()\nprint(df_train.shape, 'after_team_scores')\nprint('--------------------------------------')\n\ndf_train = df_train.merge(df_teams[COL_TEAMS], on=['teamId'], how='left')\ngc.collect()\nprint(df_train.shape, 'after_teams')\nprint('--------------------------------------')\n\ndf_train = df_train.merge(df_standings[['dailyDataDate', 'teamId'] + COL_STANDINGS], on=['dailyDataDate','teamId'], how= 'left')\ngc.collect()\nprint(df_train.shape, 'after_standings')\nprint('--------------------------------------')\n\ndf_train = df_train.merge(df_awards[COL_AWARDS], on=['dailyDataDate', 'playerId'], how='left')\ngc.collect()\nprint(df_train.shape, 'after_awards')\nprint('--------------------------------------')\nprint(df_train.shape)\n\ndf_train = df_train.merge(df_transactions[['dailyDataDate','playerId'] + COL_TRANSACTION], on=['dailyDataDate','playerId'], how='left')\ngc.collect()\nprint(df_train.shape, 'after_transactions')\nprint('--------------------------------------')\nprint(df_train.shape)\n# print object columns\ndf_train.select_dtypes(include=['object']).columns","4dca0b59":"df_train['rbi_teamruns'] = df_train['rbi'] \/ df_train['team_runsScored']\nCOL_FEATURES = COL_FEATURES + ['rbi_teamruns']","31f84dd5":"if os.path.isfile('.\/player_target_stats.csv'):\n    df_stats = pd.read_csv('.\/player_target_stats.csv')\n    df_train['year'] = df_train['dailyDataDate'].astype(str).str[:4].astype(np.int64)\n    df_train['month'] = df_train['dailyDataDate'].astype(str).str[4:6].astype(np.int64)\n    df_train = df_train.merge(df_stats, on=['year','month', 'playerId'], how='left')\n    df_train = df_train.drop(columns=['year', 'month'])\n    df_stats = df_stats.drop(columns=['month'])\nelse:\n    df_stats = pd.read_csv('..\/input\/mlb-features\/player_target_stats_pattern3.csv')\n    df_train['year'] = df_train['dailyDataDate'].astype(str).str[:4].astype(np.int64)\n    df_train = df_train.merge(df_stats, on=['year', 'playerId'], how='left')\n    df_train = df_train.drop(columns='year')\n\nstas_feat = df_stats.columns.to_list()[2:]\nCOL_FEATURES = COL_FEATURES + stas_feat","dabeaa01":"HR_dict = {545361:'HR_Trout', 592450:'HR_Judge', 592885:'HR_Yelich', 660271:'HR_Ohtani', 660670:'HR_Acuna'}\ndef get_HR(df):\n    COL_HR = []\n    HR_list = [pd.DataFrame({'dailyDataDate' : [0,0,0,0,0],\n              'playerId': [545361, 592450, 592885, 660271, 660670],\n              'homeRuns':[0,0,0,0,0]})]\n    for key in HR_dict:\n        df_tmp = df.query(f\"playerId == {key} & homeRuns > 0\")[['dailyDataDate','playerId', 'homeRuns']]\n        HR_list.append(df_tmp)\n        COL_HR.append(HR_dict[key])\n    df_HR = pd.concat(HR_list, axis=0)\n    df_HR = df_HR.groupby(['dailyDataDate','playerId']).sum().reset_index()\n    df_HR = df_HR.pivot(index='dailyDataDate', columns='playerId', values='homeRuns')\n    df_HR = df_HR.rename(columns=HR_dict)\n    return df_HR, COL_HR\ndf_HR, COL_HR = get_HR(df_train.copy())\ndf_train = df_train.merge(df_HR, on=['dailyDataDate'], how='left')\ndf_train[COL_HR] = df_train[COL_HR].fillna(0)\n# \u7279\u5fb4\u91cf\u914d\u5217\u306b\u8ffd\u8a18\nCOL_FEATURES = COL_FEATURES + COL_HR","f727df9c":"# label encoding\nplayer2num = {c: i for i, c in enumerate(df_train['playerId'].unique())}\nposition2num = {c: i for i, c in enumerate(df_train['primaryPositionName'].unique())} # salary\u30c7\u30fc\u30bf\nteamid2num = {c: i for i, c in enumerate(df_train['teamId'].unique())}\nstatus2num = {c: i for i, c in enumerate(df_train['statusCode'].unique())}\nleagueId2num = {c: i for i, c in enumerate(df_train['leagueId'].unique())}\ndivisionId2num = {c: i for i, c in enumerate(df_train['divisionId'].unique())}\n\ndf_train['label_playerId'] = df_train['playerId'].map(player2num)\ndf_train['primaryPositionName'] = df_train['primaryPositionName'].map(position2num)\ndf_train['teamId'] = df_train['teamId'].map(teamid2num)\ndf_train['statusCode'] = df_train['statusCode'].map(status2num)\ndf_train['leagueId'] = df_train['leagueId'].map(leagueId2num)\ndf_train['divisionId'] = df_train['divisionId'].map(divisionId2num)\nCOL_FEATURES = COL_FEATURES + ['label_playerId']","1e2c0676":"df_train.isnull().sum()","c225da2d":"set(df_train.columns).difference(set(COL_FEATURES))","1b28af08":"# save\nimport pickle\ndf_train.to_pickle('df_train.pickle')\nwith open('COL_FEATURES.pickle', mode='wb') as f:\n        pickle.dump(COL_FEATURES,f)","ae088e46":"train_X = df_train.query(\"dailyDataDate > 20210101\")[COL_FEATURES]\ntrain_y = df_train.query(\"dailyDataDate > 20210101\")[COL_TARGETS]\n#train_X = df_train[COL_FEATURES]\n#train_y = df_train[COL_TARGETS]\n\n_index = (df_train['dailyDataDate'] < 20210601)\nX_train = train_X.loc[_index].reset_index(drop=True)\ny_train = train_y.loc[_index].reset_index(drop=True)\nX_valid = train_X.loc[~_index].reset_index(drop=True)\ny_valid = train_y.loc[~_index].reset_index(drop=True)\nprint(X_train.shape, X_valid.shape)","5def9edc":"def fit_lgbm(X_train, y_train, X_valid, y_valid,  params: dict=None, seed=42,verbose=100):\n    oof_pred = np.zeros(len(y_valid), dtype=np.float32)\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid = lgb.Dataset(X_valid, y_valid)\n    params[\"seed\"] = seed\n    model = lgb.train(params,\n                      lgb_train,\n                      #categorical_feature=['statusCode', 'primaryPositionCode'],\n                      valid_sets=[lgb_train, lgb_valid],\n                      verbose_eval=100,\n                      num_boost_round=10000,\n                      early_stopping_rounds=100\n                     )\n    oof_pred = model.predict(X_valid)\n    score = mean_absolute_error(oof_pred, y_valid)\n    print('mae:', score)\n    _ = lgb.plot_importance(model,max_num_features=20,figsize=(10,10))\n    return oof_pred, model, score","a058ba77":"# training lightgbm\nparams1 = {'objective':'mae',\n           'reg_alpha': 0.14947461820098767, \n           'reg_lambda': 0.10185644384043743, \n           'n_estimators': 3633, \n           'learning_rate': 0.08046301304430488, \n           'num_leaves': 674, \n           'feature_fraction': 0.9101240539122566, \n           'bagging_fraction': 0.9884451442950513, \n           'bagging_freq': 8, \n           'min_child_samples': 51}\n\nparams2 = {'objective':'mae',\n           'reg_alpha': 0.1,\n           'reg_lambda': 0.1, \n           'n_estimators': 80,\n           'learning_rate': 0.1,\n           'random_state': 42,\n           \"num_leaves\": 22}\n\nparams3 = {'objective':'mae',\n           'reg_alpha': 0.1,\n           'reg_lambda': 0.1, \n           'n_estimators': 10000,\n           'learning_rate': 0.1,\n           'random_state': 42,\n           \"num_leaves\": 100\n           }\n\nparams4 = {'objective':'mae',\n           'reg_alpha': 0.016468100279441976, \n           'reg_lambda': 0.09128335764019105, \n           'n_estimators': 9868, \n           'learning_rate': 0.10528150510326864, \n           'num_leaves': 157, \n           'feature_fraction': 0.5419185713426886, \n           'bagging_fraction': 0.2637405128936662, \n           'bagging_freq': 19, \n           'min_child_samples': 71}\n\n\n\noof1_1, model1_1, score1_1 = fit_lgbm(\n    X_train, y_train['target1'],\n    X_valid, y_valid['target1'],\n    params1,29\n)\noof1_2, model1_2, score1_2 = fit_lgbm(\n    X_train, y_train['target1'],\n    X_valid, y_valid['target1'],\n    params1,42\n)\noof2_1, model2_1, score2_1 = fit_lgbm(\n    X_train, y_train['target2'],\n    X_valid, y_valid['target2'],\n    params2,29\n)\noof2_2, model2_2, score2_2 = fit_lgbm(\n    X_train, y_train['target2'],\n    X_valid, y_valid['target2'],\n    params2,42\n)\noof3_1, model3_1, score3_1 = fit_lgbm(\n    X_train, y_train['target3'],\n    X_valid, y_valid['target3'],\n    params3,29\n)\noof3_2, model3_2, score3_2 = fit_lgbm(\n    X_train, y_train['target3'],\n    X_valid, y_valid['target3'],\n    params3,42\n)\noof4_1, model4_1, score4_1 = fit_lgbm(\n    X_train, y_train['target4'],\n    X_valid, y_valid['target4'],\n    params4,29\n)\noof4_2, model4_2, score4_2 = fit_lgbm(\n    X_train, y_train['target4'],\n    X_valid, y_valid['target4'],\n    params4,42\n)\nscore1 = (score1_1+score2_1+score3_1+score4_1) \/ 4\nscore2 = (score1_2+score2_2+score3_2+score4_2) \/ 4\nprint(f'score1: {score1}')\nprint(f'score2: {score2}')","c599d2c0":"score1 = mean_absolute_error((oof1_1+oof1_2)\/2, y_valid['target1'])\nscore2 = mean_absolute_error((oof2_1+oof2_2)\/2, y_valid['target2'])\nscore3 = mean_absolute_error((oof3_1+oof3_2)\/2, y_valid['target3'])\nscore4 = mean_absolute_error((oof4_1+oof4_2)\/2, y_valid['target4'])\nscore = (score1+score2+score3+score4) \/ 4\nprint(f'score: {score}')","ff36db30":"def fit_lgbm_all(X_train, y_train, params: dict=None, seed=42, verbose=100):\n    lgb_train = lgb.Dataset(X_train, y_train)\n    params[\"seed\"] = seed\n    model = lgb.train(params,lgb_train)\n    return model","0b743ce8":"train_X = df_train.query(\"dailyDataDate > 20210101\")[COL_FEATURES]\ntrain_y = df_train.query(\"dailyDataDate > 20210101\")[COL_TARGETS]\nprint(train_X.shape, train_y.shape)\nbest_param1 = dict(params1)\nbest_param1['num_iterations'] = int(model1_1.best_iteration*1.1)\nmodel1_1 = fit_lgbm_all(train_X, train_y['target1'], best_param1, 29)\nbest_param1['num_iterations'] = int(model1_2.best_iteration*1.1)\nmodel1_2 = fit_lgbm_all(train_X, train_y['target1'], best_param1, 42)\n\nbest_param2 = dict(params2)\nbest_param2['num_iterations'] = int(model2_1.best_iteration*1.1)\nmodel2_1 = fit_lgbm_all(train_X, train_y['target2'], best_param2, 29)\nbest_param2['num_iterations'] = int(model2_2.best_iteration*1.1)\nmodel2_2 = fit_lgbm_all(train_X, train_y['target2'], best_param2, 42)\n\nbest_param3 = dict(params3)\nbest_param3['num_iterations'] = int(model3_1.best_iteration*1.1)\nmodel3_1 = fit_lgbm_all(train_X, train_y['target3'], best_param3, 29)\nbest_param3['num_iterations'] = int(model3_2.best_iteration*1.1)\nmodel3_2 = fit_lgbm_all(train_X, train_y['target3'], best_param3, 42)\n\nbest_param4 = dict(params4)\nbest_param4['num_iterations'] = int(model4_1.best_iteration*1.1)\nmodel4_1 = fit_lgbm_all(train_X, train_y['target4'], best_param4, 29)\nbest_param4['num_iterations'] = int(model4_2.best_iteration*1.1)\nmodel4_2 = fit_lgbm_all(train_X, train_y['target4'], best_param4, 42)","eaaef5be":"del(train_X, train_y, df_games, df_targets)\ngc.collect()","e204dde2":"import os\nimport warnings\nfrom typing import Optional, Tuple\n\n\nclass Environment:\n    def __init__(self,\n                 data_dir: str,\n                 eval_start_day: int,\n                 eval_end_day: Optional[int],\n                 use_updated: bool,\n                 multiple_days_per_iter: bool):\n        warnings.warn('this is mock module for mlb')\n\n        postfix = '_updated' if use_updated else ''\n        \n        # recommend to replace this with pickle, feather etc to speedup preparing data\n        df_train = pd.read_pickle(os.path.join(data_dir, f'train{postfix}.pkl'))\n\n        players = pd.read_pickle('..\/input\/mlb-unnested\/players.pickle')\n\n        self.players = players[players['playerForTestSetAndFuturePreds'] == True]['playerId'].astype(str)\n        if eval_end_day is not None:\n            self.df_train = df_train.set_index('date').loc[eval_start_day:eval_end_day]\n        else:\n            self.df_train = df_train.set_index('date').loc[eval_start_day:]\n        self.date = self.df_train.index.values\n        self.n_rows = len(self.df_train)\n        self.multiple_days_per_iter = multiple_days_per_iter\n\n        assert self.n_rows > 0, 'no data to emulate'\n\n    def predict(self, df: pd.DataFrame) -> None:\n        # if you want to emulate public LB, store your prediction here and calculate MAE\n        pass\n\n    def iter_test(self) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        if self.multiple_days_per_iter:\n            for i in range(self.n_rows \/\/ 2):\n                date1 = self.date[2 * i]\n                date2 = self.date[2 * i + 1]\n                sample_sub1 = self._make_sample_sub(date1)\n                sample_sub2 = self._make_sample_sub(date2)\n                sample_sub = pd.concat([sample_sub1, sample_sub2]).reset_index(drop=True)\n                df = self.df_train.loc[date1:date2]\n\n                yield df, sample_sub.set_index('date')\n        else:\n            for i in range(self.n_rows):\n                date = self.date[i]\n                sample_sub = self._make_sample_sub(date)\n                df = self.df_train.loc[date:date]\n\n                yield df, sample_sub.set_index('date')\n\n    def _make_sample_sub(self, date: int) -> pd.DataFrame:\n        next_day = (pd.to_datetime(date, format='%Y%m%d') + pd.to_timedelta(1, 'd')).strftime('%Y%m%d')\n        sample_sub = pd.DataFrame()\n        sample_sub['date_playerId'] = next_day + '_' + self.players\n        sample_sub['target1'] = 0\n        sample_sub['target2'] = 0\n        sample_sub['target3'] = 0\n        sample_sub['target4'] = 0\n        sample_sub['date'] = date\n        return sample_sub\n    \nclass MLBEmulator:\n    def __init__(self,\n                 data_dir: str = '..\/input\/mlb-features',\n                 eval_start_day: int = 20210401,\n                 eval_end_day: Optional[int] = 20210430,\n                 use_updated: bool = True,\n                 multiple_days_per_iter: bool = False):\n        self.data_dir = data_dir\n        self.eval_start_day = eval_start_day\n        self.eval_end_day = eval_end_day\n        self.use_updated = use_updated\n        self.multiple_days_per_iter = multiple_days_per_iter\n\n    def make_env(self) -> Environment:\n        return Environment(self.data_dir,\n                           self.eval_start_day,\n                           self.eval_end_day,\n                           self.use_updated,\n                           self.multiple_days_per_iter)","bc1c83f9":"def reduce_mem_usage(df, verbose=False):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int64)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float32)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float64)\n                else:\n                    df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef get_unnested_data(df: pd.DataFrame, sample_prediction_df: pd.DataFrame):\n    # ['games', 'rosters', 'playerBoxScores', 'teamBoxScores', 'transactions', 'standings', 'awards', 'events']\n    #daily_data_nested_df_names = df.drop('date', axis = 1).columns.values.tolist()\n    daily_data_nested_df_names =['games', 'rosters', 'playerBoxScores', 'teamBoxScores','awards','transactions','standings']\n    dfs_dict = {}\n    for df_name in daily_data_nested_df_names:\n        #print(df_name)\n        date_nested_table = df[['date', df_name]]\n        \n        date_nested_table = (date_nested_table[\n            ~pd.isna(date_nested_table[df_name])\n            ].reset_index(drop = True))\n        #Dealing with missing values\n        #print(len(date_nested_table))\n        daily_dfs_collection = []\n        \n        if len(date_nested_table) == 0:\n            daily_df = pd.DataFrame({'dailyDataDate':sample_prediction_df['dailyDataDate'],\n                                     'playerId': sample_prediction_df['playerId']})\n            for col in columns_dict[df_name]:\n                if col in ['dailyDataDate', 'playerId']: continue\n                daily_df[col] = np.nan\n            daily_dfs_collection = daily_dfs_collection + [daily_df]\n        else:\n            for date_index, date_row in date_nested_table.iterrows():\n                daily_df = pd.read_json(date_row[df_name])\n                daily_df['dailyDataDate'] = date_row['date']\n                daily_dfs_collection = daily_dfs_collection + [daily_df]\n            \n        unnested_table = (pd.concat(daily_dfs_collection,\n            ignore_index = True).\n            # Set and reset index to move 'dailyDataDate' to front of df\n            set_index('dailyDataDate').\n            reset_index()\n            )\n        reduce_mem_usage(unnested_table).to_pickle(f\"test_{df_name}.pickle\")\n        dfs_dict[df_name] = reduce_mem_usage(unnested_table)\n        del(date_nested_table, daily_dfs_collection, unnested_table)\n    return dfs_dict","8a014491":"def inference(test_df, sample_prediction_df):\n    dfs_dict = get_unnested_data(test_df, sample_prediction_df)\n    df_test_rosters = dfs_dict['rosters']\n    df_test_games = dfs_dict['games']\n    df_test_scores = dfs_dict['playerBoxScores']\n    df_test_team_scores = dfs_dict['teamBoxScores']\n    df_test_awards = dfs_dict['awards']\n    df_test_transactions = dfs_dict['transactions']\n    df_test_standings = dfs_dict['standings']\n\n    # FE\n    # ==========================================\n    df_test_team_scores = FE_team_score(df_test_team_scores)\n\n    df_test_scores = df_test_scores.groupby(['playerId','dailyDataDate']).sum().reset_index()\n    #df_test_scores = df_test_scores.drop_duplicates(subset=['playerId','dailyDataDate']).reset_index()\n    df_test = sample_prediction_df[['playerId','dailyDataDate']].copy()\n    df_test = df_test.merge(df_players[COL_PLAYERS], on=['playerId'],how='left')\n    df_test = df_test.merge(df_test_rosters[COL_ROSTERS], on=['playerId','dailyDataDate'], how='left')\n    df_test = df_test.merge(df_test_scores[COL_SCORES], on=['playerId','dailyDataDate'], how='left')\n    df_test = df_test.merge(df_test_team_scores[['dailyDataDate','teamId'] + COL_TEAMSCORE], on=['dailyDataDate','teamId'], how= 'left')\n    df_test = df_test.merge(df_teams[COL_TEAMS],on=['teamId'], how='left')\n    \n    #standings\n    if test_df['standings'].iloc[0] == test_df['standings'].iloc[0]: # nan\u3060\u3068else\u306b\u884c\u304f\n        df_test = df_test.merge(df_test_standings[['dailyDataDate', 'teamId'] + COL_STANDINGS], on=['dailyDataDate','teamId'], how= 'left')\n    else:\n        df_test[COL_STANDINGS] = np.nan\n    # awards\n    df_test_awards = df_test_awards.dropna(how='any')\n    if len(df_test_awards) > 0:\n        df_test_awards = df_test_awards.groupby([\"dailyDataDate\",\"playerId\"]).size().reset_index()\n        df_test_awards = df_test_awards.rename(columns={0: 'num_of_award'})\n        df_test = df_test.merge(df_test_awards[COL_AWARDS], on=['dailyDataDate', 'playerId'], how='left')\n    else:\n        df_test['num_of_award'] = np.nan\n        \n    # transaction\n    df_test_transactions = df_test_transactions.query('typeDesc == \"Trade\"').dropna(subset=['playerId']).reset_index(drop=True)\n    if len(df_test_transactions) > 0:\n        df_test_transactions = df_test_transactions[[\"dailyDataDate\",\"playerId\"]]\n        df_test_transactions = df_test_transactions.drop_duplicates().reset_index(drop=True)\n        df_test_transactions['trade'] = 1\n        df_test = df_test.merge(df_test_transactions[[\"dailyDataDate\",\"playerId\"] + COL_TRANSACTION], on=[\"dailyDataDate\",\"playerId\"], how='left')\n    else:\n        df_test['trade'] = 0\n    # rbi\u306e\u5272\u5408\n    df_test['rbi_teamruns'] = df_test['rbi'] \/ df_train['team_runsScored']\n    \n    # \u8a18\u8ff0\u7d71\u8a08\n    if os.path.isfile('.\/player_target_stats.csv'):\n        df_stats = pd.read_csv('.\/player_target_stats.csv')\n        df_test['year'] = df_test['dailyDataDate'].astype(str).str[:4].astype(np.int64)\n        df_test['month'] = df_test['dailyDataDate'].astype(str).str[4:6].astype(np.int64)\n        df_test = df_test.merge(df_stats, on=['year','month', 'playerId'], how='left')\n        df_test = df_test.drop(columns=['year', 'month'])\n    else:\n        df_stats = pd.read_csv('..\/input\/mlb-features\/player_target_stats_pattern3.csv')\n        df_test['year'] = df_test['dailyDataDate'].astype(str).str[:4].astype(np.int64)\n        df_test = df_test.merge(df_stats, on=['year', 'playerId'], how='left')\n        df_test = df_test.drop(columns='year')\n    \n    # HR\n    df_HR, _ = get_HR(df_test.copy())\n    if len(df_HR) > 0:\n        df_test = df_test.merge(df_HR, on=['dailyDataDate'], how='left')\n        df_test[COL_HR] = df_test[COL_HR].fillna(0)\n    else:\n        df_test[COL_HR] = 0\n    \n    # Label Encoding\n    df_test['label_playerId'] = df_test['playerId'].map(player2num)\n    df_test['primaryPositionName'] = df_test['primaryPositionName'].map(position2num)\n    df_test['teamId'] = df_test['teamId'].map(teamid2num)\n    df_test['statusCode'] = df_test['statusCode'].map(status2num)\n    df_test['leagueId'] = df_test['leagueId'].map(leagueId2num)\n    df_test['divisionId'] = df_test['divisionId'].map(divisionId2num)\n    \n    test_X = df_test[COL_FEATURES]\n    # predict\n    pred1_1 = model1_1.predict(test_X)\n    pred2_1 = model2_1.predict(test_X)\n    pred3_1 = model3_1.predict(test_X)\n    pred4_1 = model4_1.predict(test_X)\n    pred1_2 = model1_2.predict(test_X)\n    pred2_2 = model2_2.predict(test_X)\n    pred3_2 = model3_2.predict(test_X)\n    pred4_2 = model4_2.predict(test_X)\n    \n    # merge submission\n    sample_prediction_df['target1'] = np.clip((pred1_1 + pred1_2)\/2, 0, 100)\n    sample_prediction_df['target2'] = np.clip((pred2_1 + pred2_2)\/2, 0, 100)\n    sample_prediction_df['target3'] = np.clip((pred3_1 + pred3_2)\/2, 0, 100)\n    sample_prediction_df['target4'] = np.clip((pred4_1 + pred4_2)\/2, 0, 100)\n    \n    #\u3000\u5927\u8c37\n    if df_test[\"HR_Ohtani\"][0] > 0:\n        sample_prediction_df.loc[sample_prediction_df['playerId'] ==660271, ['target1', 'target2', 'target3', 'target4']] = 100   \n    \n    sample_prediction_df = sample_prediction_df.fillna(0.)\n    del sample_prediction_df['playerId'], sample_prediction_df['dailyDataDate']\n    \n    return sample_prediction_df\n    #env.predict(sample_prediction_df)","ad705445":"emulation_mode = False\n\nif emulation_mode:\n    mlb = MLBEmulator(eval_start_day=20210501, eval_end_day=20210531)\nelse:\n    import mlb\nenv = mlb.make_env()# initialize the environment\n\niter_test = env.iter_test() # iterator which loops over each date in test set\n\nfor (test_df, sample_prediction_df) in iter_test: # make predictions here\n    #sample_prediction_df = sample_prediction_df.reset_index(drop=True)\n    sample_prediction_df = sample_prediction_df.reset_index().rename(columns={'date':'dailyDataDate'})\n    sample_prediction_df['playerId'] = sample_prediction_df['date_playerId']\\\n                                        .map(lambda x: int(x.split('_')[1]))\n    \n    # ==========================================\n    test_df = test_df.reset_index().rename(columns={'index':'date'})\n    \n    sample_prediction_df = inference(test_df, sample_prediction_df)\n    \n    env.predict(sample_prediction_df)","ae00b9f2":"mlb = MLBEmulator(eval_start_day=20210501, eval_end_day=20210531)\nenv = mlb.make_env()# initialize the environment\niter_test = env.iter_test() # iterator which loops over each date in test set\ncol = ['target1_', 'target2_', 'target3_', 'target4_']\noof_preds = []\nscores=0\nfor n, (test_df, sample_prediction_df) in enumerate(iter_test):\n    #sample_prediction_df = sample_prediction_df.reset_index(drop=True)\n    sample_prediction_df = sample_prediction_df.reset_index().rename(columns={'date':'dailyDataDate'})\n    sample_prediction_df['playerId'] = sample_prediction_df['date_playerId']\\\n                                        .map(lambda x: int(x.split('_')[1]))\n    \n    # ==========================================\n    test_df = test_df.reset_index().rename(columns={'index':'date'})\n    sample_prediction_df = inference(test_df, sample_prediction_df)\n    \n    #env.predict(sample_prediction_df)\n    \n    targets = pd.read_json(test_df['nextDayPlayerEngagement'][0])\n    targets.columns = ['engagementMetricsDate', 'playerId'] + col\n    sample_prediction_df['playerId'] = sample_prediction_df['date_playerId']\\\n                                        .map(lambda x: int(x.split('_')[1]))\n    oof_pred = sample_prediction_df.merge(targets[['playerId'] + col], on='playerId', how='left')\n    # mae by day\n    score = mean_absolute_error(oof_pred[['target1', 'target2', 'target3', 'target4']].values, oof_pred[col].values)\n    print(f\"{score}\")\n    scores += score\n    oof_preds.append(oof_pred)\n\noof_df = pd.concat(oof_preds,axis=0).reset_index(drop=True)\nprint('=*'*30)\nprint(f\"score{scores\/len(oof_preds)}\")","856173a5":"# _test_df = [pd.read_csv('..\/input\/mlb-player-digital-engagement-forecasting\/example_test.csv').query(\"date == 20210426\"),\n#             pd.read_csv('..\/input\/mlb-player-digital-engagement-forecasting\/example_test.csv').query(\"date == 20210427\"),\n#             pd.read_csv('..\/input\/mlb-player-digital-engagement-forecasting\/example_test.csv').query(\"date == 20210428\"),\n#             pd.read_csv('..\/input\/mlb-player-digital-engagement-forecasting\/example_test.csv').query(\"date == 20210429\"),\n#             pd.read_csv('..\/input\/mlb-player-digital-engagement-forecasting\/example_test.csv').query(\"date == 20210430\")]\n# sample = [pd.read_csv('..\/input\/mlb-player-digital-engagement-forecasting\/example_sample_submission.csv').query('date == 20210426').set_index('date'),\n#           pd.read_csv('..\/input\/mlb-player-digital-engagement-forecasting\/example_sample_submission.csv').query('date == 20210427').set_index('date'),\n#           pd.read_csv('..\/input\/mlb-player-digital-engagement-forecasting\/example_sample_submission.csv').query('date == 20210428').set_index('date'),\n#           pd.read_csv('..\/input\/mlb-player-digital-engagement-forecasting\/example_sample_submission.csv').query('date == 20210429').set_index('date'),\n#           pd.read_csv('..\/input\/mlb-player-digital-engagement-forecasting\/example_sample_submission.csv').query('date == 20210430').set_index('date')]\n# # nan\u306e\u30c6\u30b9\u30c8\n# _test_df[0].iloc[:,1:] = np.nan\n# pred = []\n# for i in range(5):\n#     test_df = _test_df[i]\n#     sample_prediction_df = sample[i]\n#     #sample_prediction_df = sample_prediction_df.reset_index(drop=True)\n#     sample_prediction_df = sample_prediction_df.reset_index().rename(columns={'date':'dailyDataDate'})\n#     sample_prediction_df['playerId'] = sample_prediction_df['date_playerId']\\\n#                                         .map(lambda x: int(x.split('_')[1]))\n\n#     sample_prediction_df = inference(test_df, sample_prediction_df)\n#     pred.append(sample_prediction_df)\n    \n#     #dfs_dict = get_unnested_data(test_df, sample_prediction_df)\n# sub = pd.concat(pred, axis=0)","b184b3b9":"## CV Split","9a8a8bac":"### \u6253\u70b9\u304c\u30c1\u30fc\u30e0\u306e\u5f97\u70b9\u306e\u4f55\u5272\u304b \/What percentage of the team's score is RBI?","1b52f1a9":"#### about stats\n![\u7121\u984c.png](attachment:de7b72c3-b2bc-4aa6-98b8-77711ca1cc3f.png)  \n1\u6708\u30682\u6708\u30683\u6708\u306e\u30bf\u30fc\u30b2\u30c3\u30c8\u306e\u5024\u306e\u8a18\u8ff0\u7d71\u8a08\u91cf\u30924\u6708\u306b\u7279\u5fb4\u3068\u3057\u3066\u4f7f\u3046  \nUse descriptive statistics of target values for January, February, and March as features in April","8585c404":"## Road","271d10eb":"## Get env","34c32354":"## Config","d7ee431b":"## model","f34c4d3e":"score1.2487188781157268","a4394f90":"## Inference & emulator","71fc65ae":"## Reference\n\nThank you for publishing a great notebook and dataset!\n\n+ @columbia2131 [MLB lightGBM Starter Dataset&Code[en, ja]](https:\/\/www.kaggle.com\/columbia2131\/mlb-lightgbm-starter-dataset-code-en-ja)\n+ @naotaka1128 [Creating Unnested Dataset](https:\/\/www.kaggle.com\/naotaka1128\/creating-unnested-dataset)\n+ @mlconsult [create player descriptive stats dataset](https:\/\/www.kaggle.com\/mlconsult\/create-player-descriptive-stats-dataset)\n+ @kaito510 [Player Salary + MLB lightGBM Starter](https:\/\/www.kaggle.com\/kaito510\/player-salary-mlb-lightgbm-starter)\n+ @kohashi0000 [1.36 simple_LightGBM](https:\/\/www.kaggle.com\/kohashi0000\/1-36-simple-lightgbm)\n+ @somayyehgholami, @mehrankazeminia [[Fork of] LightGBM + CatBoost + ANN 2505f2](https:\/\/www.kaggle.com\/somayyehgholami\/fork-of-lightgbm-catboost-ann-2505f2)\n+ @nyanpn [API Emulator for debugging your code locally](https:\/\/www.kaggle.com\/nyanpn\/api-emulator-for-debugging-your-code-locally)","c3543487":"### \u4eba\u6c17\u9078\u624b\u304c\u30db\u30fc\u30e0\u30e9\u30f3\u3092\u6253\u3063\u305f\u304b\u7279\u5fb4\u91cf \/ Whether a popular player hit a home run","4bb96cb1":"## Unnest updatedfile","b8ee4f66":"## Libraries","c9142e12":"## About this notebook\n\n+ train on 2021 regular season data(use update data\n+ cv on may,2021(test player)1.2833 but this score is leakage\n  + publicLB 1.1133\u3000Why doesn't it match the emulation?\n+ cv on july,2021(include allplayer) 0.7153 this score is not leakage     maybe....","3bf88192":"### aggregate\u3000NaN","e9527c23":" # Utills","1a6e5297":"## The emulator","9afee904":"## train on alldata","f7628b25":"### nan test","9616e654":"### \u8a18\u8ff0\u7d71\u8a08\u91cf\u306e\u8ffd\u52a0 \/ add stats","17923da4":"## Preprocess & Feature engineering"}}