{"cell_type":{"66a679f4":"code","27a07879":"code","422d855a":"code","dccaa71f":"code","df62b074":"code","3b458799":"code","fd0d8114":"code","c521e3da":"code","4b1bdf6e":"code","b284e21f":"code","1fbe4f10":"code","3022c843":"code","368a61ec":"code","10b0fcf4":"code","15c03569":"code","b7238d19":"code","989c9e8d":"code","c109d430":"code","01a2aa08":"code","092a9cf7":"code","6bb804ab":"code","14e38ebd":"code","7b2ef071":"code","bb018547":"code","134ef208":"code","190cc4c6":"code","b21e56f9":"code","c6c89e7e":"code","f98015aa":"code","d60ebf31":"code","d945e55e":"code","5147a775":"code","ba65db90":"code","3b99a3fe":"code","400394e4":"code","11f5f60e":"code","bfaeb75f":"code","8b37e2cb":"code","590bea85":"code","1ec3490a":"code","3f97f93b":"code","6a7bcaa0":"code","60532843":"code","4f20e9c7":"code","6ec13369":"code","fa67e004":"code","cbed00b2":"code","62f0d181":"code","af30f1bd":"code","98d69757":"code","576df72a":"code","5ce396ee":"code","c2efe11c":"code","03a7ffd8":"code","aec3d965":"code","89e72b90":"code","1c19a539":"markdown","4efdf324":"markdown","2701d0a0":"markdown","d0f2310d":"markdown","9bebe84e":"markdown","ea22a61a":"markdown","eeb3c22f":"markdown","3d05507f":"markdown","85a25397":"markdown","a6b94cf7":"markdown","44bcbda9":"markdown","d67a3513":"markdown","7d807732":"markdown","015c2a2d":"markdown","fb72682b":"markdown","9fca17cd":"markdown","312ff7e8":"markdown","f7c13c10":"markdown","08b4bb50":"markdown","c6f9cf35":"markdown","60e0270c":"markdown","4157666c":"markdown","dfbe21db":"markdown","1de01ba9":"markdown","20a3560e":"markdown","89b693cf":"markdown","09b43bb8":"markdown","1b829b72":"markdown","a21be687":"markdown","06312479":"markdown","2e777417":"markdown"},"source":{"66a679f4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style \nstyle.use('ggplot')\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport plotly.offline as py \nfrom plotly.offline import init_notebook_mode, iplot\npy.init_notebook_mode(connected=True) # this code, allow us to work with offline plotly version\nimport plotly.graph_objs as go # it's like \"plt\" of matplot\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nimport gc\n\n\n# Any results you write to the current directory are saved as output.","27a07879":"X_train = pd.read_csv('..\/input\/X_train.csv')\nX_train.head(3)","422d855a":"y_train = pd.read_csv('..\/input\/y_train.csv')\ny_train.head(3)","dccaa71f":"X_test = pd.read_csv('..\/input\/X_test.csv')\nX_test.head(3)","df62b074":"print('Size of Train Data')\nprint('Number of samples are: {0}\\nNumber of features are: {1}'.format(X_train.shape[0], X_train.shape[1]))\n\nprint('\\nSize of Test Data')\nprint('Number of samples are: {0}\\nNumber of features are: {1}'.format(X_test.shape[0], X_test.shape[1]))\n\nprint('\\nSize of Target Data')\nprint('Number of samples are: {0}\\nNumber of features are: {1}'.format(y_train.shape[0], y_train.shape[1]))","3b458799":"X_train.describe()","fd0d8114":"target = y_train['surface'].value_counts().reset_index().rename(columns = {'index' : 'target'})\ntarget","c521e3da":"#sns.countplot(y='surface',data = y_train)\ntrace0 = go.Bar(\n    x = y_train['surface'].value_counts().index,\n    y = y_train['surface'].value_counts().values\n    )\n\ntrace1 = go.Pie(\n    labels = y_train['surface'].value_counts().index,\n    values = y_train['surface'].value_counts().values,\n    domain = {'x':[0.55,1]})\n\ndata = [trace0, trace1]\nlayout = go.Layout(\n    title = 'Frequency Distribution for surface\/target data',\n    xaxis = dict(domain = [0,.50]))\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig)\n","4b1bdf6e":"X_train.isnull().sum()","b284e21f":"X_train['is_duplicate'] = X_train.duplicated()\nX_train['is_duplicate'].value_counts()","1fbe4f10":"X_train = X_train.drop(['is_duplicate'], axis = 1)","3022c843":"X_train_sort = X_train.sort_values(by = ['series_id', 'measurement_number'], ascending = True)\nX_train_sort.head()","368a61ec":"corr = X_train.corr()\ncorr","10b0fcf4":"fig, ax = plt.subplots(1,1, figsize = (15,6))\n\nhm = sns.heatmap(X_train.iloc[:,3:].corr(),\n                ax = ax,\n                cmap = 'coolwarm',\n                annot = True,\n                fmt = '.2f',\n                linewidths = 0.05)\nfig.subplots_adjust(top=0.93)\nfig.suptitle('Orientation, Angular_velocity and Linear_accelaration Correlation Heatmap for Train dataset', \n              fontsize=14, \n              fontweight='bold')","15c03569":"fig, ax = plt.subplots(1,1, figsize = (15,6))\n\nhm = sns.heatmap(X_test.iloc[:,3:].corr(),\n                ax = ax,\n                cmap = 'coolwarm',\n                annot = True,\n                fmt = '.2f',\n                linewidths = 0.05)\nfig.subplots_adjust(top=0.93)\nfig.suptitle('Orientation, Angular_velocity and Linear_accelaration Correlation Heatmap for Test dataset', \n              fontsize=14, \n              fontweight='bold')","b7238d19":"fig = plt.figure(figsize=(15,15))\nax = fig.add_subplot(311)\nax.set_title('Distribution of Orientation_X,Y,Z,W',\n             fontsize=14, \n             fontweight='bold')\nX_train.iloc[:,3:7].boxplot()\nax = fig.add_subplot(312)\nax.set_title('Distribution of Angular_Velocity_X,Y,Z',fontsize=14, \n             fontweight='bold')\nX_train.iloc[:,7:10].boxplot()\nax = fig.add_subplot(313)\nax.set_title('Distribution of linear_accelaration_X,Y,Z',fontsize=14, \n             fontweight='bold')\nX_train.iloc[:,10:13].boxplot()","989c9e8d":"plt.figure(figsize=(26, 16))\nfor i, col in enumerate(X_train.columns[3:]):\n    ax = plt.subplot(3, 4, i + 1)\n    sns.distplot(X_train[col], bins=100, label='train')\n    sns.distplot(X_test[col], bins=100, label='test')\n    ax.legend()   ","c109d430":"df = X_train.merge(y_train, on = 'series_id', how = 'inner')\ntargets = (y_train['surface'].value_counts()).index","01a2aa08":"df.head(3)","092a9cf7":"plt.figure(figsize=(26, 16))\nfor i,col in enumerate(df.columns[3:13]):\n    ax = plt.subplot(3,4,i+1)\n    ax = plt.title(col)\n    for surface in targets:\n        surface_feature = df[df['surface'] == surface]\n        sns.kdeplot(surface_feature[col], label = surface)","6bb804ab":"series_dict = {}\nfor series in (X_train['series_id'].unique()):\n    series_dict[series] = X_train[X_train['series_id'] == series] ","14e38ebd":"# From: Code Snippet For Visualizing Series Id by @shaz13\ndef plotSeries(series_id):\n    style.use('ggplot')\n    plt.figure(figsize=(28, 16))\n    print(y_train[y_train['series_id'] == series_id]['surface'].values[0].title())\n    for i, col in enumerate(series_dict[series_id].columns[3:]):\n        if col.startswith(\"o\"):\n            color = 'red'\n        elif col.startswith(\"a\"):\n            color = 'green'\n        else:\n            color = 'blue'\n        if i >= 7:\n            i+=1\n        plt.subplot(3, 4, i + 1)\n        plt.plot(series_dict[series_id][col], color=color, linewidth=3)\n        plt.title(col)","7b2ef071":"plotSeries(1)","bb018547":"# from @theoviel at https:\/\/www.kaggle.com\/theoviel\/fast-fourier-transform-denoising\ndef filter_signal(signal, threshold=1e3):\n    fourier = rfft(signal)\n    frequencies = rfftfreq(signal.size, d=20e-3\/signal.size)\n    fourier[frequencies > threshold] = 0\n    return irfft(fourier)","134ef208":"# denoise train and test angular_velocity and linear_acceleration data\nX_train_denoised = X_train.copy()\nX_test_denoised = X_test.copy()","190cc4c6":"X_train.head(3)","b21e56f9":"from numpy.fft import *\n\n# train\nfor col in X_train.columns:\n    if col[0:3] == 'ang' or col[0:3] == 'lin':\n        # Apply filter_signal function to the data in each series\n        denoised_data = X_train.groupby(['series_id'])[col].apply(lambda x: filter_signal(x))\n        \n        # Assign the denoised data back to X_train\n        list_denoised_data = []\n        for arr in denoised_data:\n            for val in arr:\n                list_denoised_data.append(val)\n                \n        X_train_denoised[col] = list_denoised_data\n        \n# test\nfor col in X_test.columns:\n    if col[0:3] == 'ang' or col[0:3] == 'lin':\n        # Apply filter_signal function to the data in each series\n        denoised_data = X_test.groupby(['series_id'])[col].apply(lambda x: filter_signal(x))\n        \n        # Assign the denoised data back to X_train\n        list_denoised_data = []\n        for arr in denoised_data:\n            for val in arr:\n                list_denoised_data.append(val)\n                \n        X_test_denoised[col] = list_denoised_data\n        ","c6c89e7e":"series_dict = {}\nfor series in (X_train_denoised['series_id'].unique()):\n    series_dict[series] = X_train_denoised[X_train_denoised['series_id'] == series] ","f98015aa":"plotSeries(1)","d60ebf31":"plt.figure(figsize=(24, 8))\nplt.title('linear_acceleration_X')\nplt.plot(X_train.angular_velocity_Z[128:256], label=\"original\");\nplt.plot(X_train_denoised.angular_velocity_Z[128:256], label=\"denoised\");\nplt.legend()\nplt.show()","d945e55e":"#https:\/\/en.wikipedia.org\/wiki\/Conversion_between_quaternions_and_Euler_angles\n#quaternion to eular\ndef quaternion_to_euler(qx,qy,qz,qw):\n    import math\n    # roll (x-axis rotation)\n    sinr_cosp = +2.0 * (qw * qx + qy + qz)\n    cosr_cosp = +1.0 - 2.0 * (qx * qx + qy * qy)\n    roll = math.atan2(sinr_cosp, cosr_cosp)\n    \n    # pitch (y-axis rotation)\n    sinp = +2.0 * (qw * qy - qz * qx)\n    if(math.fabs(sinp) >= 1):\n        pitch = copysign(M_PI\/2, sinp)\n    else:\n        pitch = math.asin(sinp)\n        \n    # yaw (z-axis rotation)\n    siny_cosp = +2.0 * (qw * qz + qx * qy)\n    cosy_cosp = +1.0 - 2.0 * (qy * qy + qz * qz)\n    yaw = math.atan2(siny_cosp, cosy_cosp)\n    \n    return roll, pitch, yaw","5147a775":"def eular_angle(data):\n    x, y, z, w = data['orientation_X'].tolist(), data['orientation_Y'].tolist(), data['orientation_Z'].tolist(), data['orientation_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    data['euler_x'] = nx\n    data['euler_y'] = ny\n    data['euler_z'] = nz\n    \n    return data","ba65db90":"data = eular_angle(X_train_denoised)\ntest = eular_angle(X_test_denoised)\nprint(data.shape, test.shape)","3b99a3fe":"data.head(3)","400394e4":"def fe_eng1(data):\n    data['total_angular_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 + data['angular_velocity_Z']**2)** 0.5\n    data['total_linear_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 + data['linear_acceleration_Z']**2)**0.5\n    data['total_orientation'] = (data['orientation_X']**2 + data['orientation_Y']**2 + data['orientation_Z']**2)**0.5\n    data['acc_vs_vel'] = data['total_linear_acc'] \/ data['total_angular_vel']\n    data['total_angle'] = (data['euler_x'] ** 2 + data['euler_y'] ** 2 + data['euler_z'] ** 2) ** 5\n    data['angle_vs_acc'] = data['total_angle'] \/ data['total_linear_acc']\n    data['angle_vs_vel'] = data['total_angle'] \/ data['total_angular_vel']\n    return data","11f5f60e":"data = fe_eng1(data)\ntest = fe_eng1(test)\nprint(data.shape, test.shape)","bfaeb75f":"def fe_eng2(data):\n    df = pd.DataFrame()\n    \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        df[col + '_maxtoMin'] = df[col + '_max'] \/ df[col + '_min']\n        #in statistics, the median absolute deviation (MAD) is a robust measure of the variablility of a univariate sample of quantitative data.\n        df[col + '_mad'] = data.groupby(['series_id'])[col].apply(lambda x: np.median(np.abs(np.diff(x))))\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])\/2\n    return df","8b37e2cb":"%%time\ndata = fe_eng2(data)\ntest = fe_eng2(test)\nprint(data.shape, test.shape)","590bea85":"data.head(3)","1ec3490a":"data.fillna(0, inplace = True)\ndata.replace(-np.inf, 0, inplace = True)\ndata.replace(np.inf, 0, inplace = True)\ntest.fillna(0, inplace = True)\ntest.replace(-np.inf, 0, inplace = True)\ntest.replace(np.inf, 0, inplace = True)","3f97f93b":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_train['surface'] = le.fit_transform(y_train['surface'])","6a7bcaa0":"y_train.head()","60532843":"folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=60)\npredicted = np.zeros((test.shape[0],9))\nmeasured= np.zeros((data.shape[0]))\nscore = 0","4f20e9c7":"for times, (trn_idx, val_idx) in enumerate(folds.split(data.values,y_train['surface'].values)):\n    model = RandomForestClassifier(n_estimators=700, n_jobs = -1)\n    #model = RandomForestClassifier(n_estimators=500, max_depth=10, min_samples_split=5, n_jobs=-1)\n    model.fit(data.iloc[trn_idx],y_train['surface'][trn_idx])\n    measured[val_idx] = model.predict(data.iloc[val_idx])\n    predicted += model.predict_proba(test)\/folds.n_splits\n    score += model.score(data.iloc[val_idx],y_train['surface'][val_idx])\n    print(\"Fold: {} score: {}\".format(times,model.score(data.iloc[val_idx],y_train['surface'][val_idx])))\n    \n    gc.collect()","6ec13369":"print('Average score', score \/ folds.n_splits)","fa67e004":"confusion_matrix(measured,y_train['surface'])","cbed00b2":"fig, ax = plt.subplots(1,1,figsize=(12,5))\nsns.heatmap(pd.DataFrame(confusion_matrix(measured,y_train['surface'])),\n            ax = ax,\n            cmap = 'coolwarm',\n            annot = True,\n            fmt = '.2f',\n            linewidths = 0.05)\nfig.subplots_adjust(top=0.93)\nfig.suptitle('Confusion matrix, Actual vs Predicted label Correlation Heatmap', \n              fontsize=14, \n              fontweight='bold')\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')","62f0d181":"importances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis = 0)\nindices = np.argsort(importances)[::-1]","af30f1bd":"feature_importances = pd.DataFrame(importances, index = data.columns, columns = ['importance'])\nfeature_importances.sort_values('importance', ascending = False)\nfeature_importances.head(20)","98d69757":"feature_importances.sort_values('importance', ascending = False).plot(kind = 'bar', \n                         figsize = (35,8), \n                         color = 'r', \n                         yerr=std[indices], \n                        align = 'center')\nplt.xticks(rotation=90)\nplt.show()","576df72a":"feature_importances.sort_values('importance', ascending = False)[:100].plot(kind = 'bar',\n                                                                            figsize = (30,5),\n                                                                            color = 'g', \n                                                                            yerr=std[indices[:100]], \n                                                                            align = 'center')\nplt.xticks(rotation=90)\nplt.show()","5ce396ee":"less_important_features = feature_importances.loc[feature_importances['importance'] < 0.0025]\nprint('There are {0} features their importance value is less then 0.0025'.format(less_important_features.shape[0]))","c2efe11c":"#Remove less important features from train and test set.\nfor i, col in enumerate(less_important_features.index):\n    data = data.drop(columns = [col], axis = 1)\n    test = test.drop(columns = [col], axis = 1)\n    \ndata.shape, test.shape","03a7ffd8":"predicted = np.zeros((test.shape[0],9))\nmeasured= np.zeros((data.shape[0]))\nscore = 0\nfor times, (trn_idx, val_idx) in enumerate(folds.split(data.values,y_train['surface'].values)):\n    model = RandomForestClassifier(n_estimators=700, n_jobs = -1)\n    #model = RandomForestClassifier(n_estimators=500, max_depth=10, min_samples_split=5, n_jobs=-1)\n    model.fit(data.iloc[trn_idx],y_train['surface'][trn_idx])\n    measured[val_idx] = model.predict(data.iloc[val_idx])\n    predicted += model.predict_proba(test)\/folds.n_splits\n    score += model.score(data.iloc[val_idx],y_train['surface'][val_idx])\n    print(\"Fold: {} score: {}\".format(times,model.score(data.iloc[val_idx],y_train['surface'][val_idx])))\n    \n    gc.collect()","aec3d965":"print('Average score', score \/ folds.n_splits)","89e72b90":"submission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['surface'] = le.inverse_transform(predicted.argmax(axis=1))\nsubmission.to_csv('rs_surface_submission6.csv', index=False)\nsubmission.head(10)","1c19a539":"#### Observation:\nNow our data file sample size is same as target sample size. our test file sample size is same as number of requested series_ids.","4efdf324":"### Histogram plot for all features","2701d0a0":"### Feature distribution for each target value (surface)","d0f2310d":"#### Observation: There is no duplicate data","9bebe84e":"Thanks for stopping by. Please upvote if you like my kernel. \nStay Tuned for further Analaysis and model accuracy improvement.","ea22a61a":"### Run ML Model Again","eeb3c22f":"## Feature Enginnering\n\nFeature Enginnering is the process of using domain knowledge of the data to create features that make machine learning algorithms work.\nFeature engineering is fundamental to the application of machine learning, and is both difficult and expensive.\nThe features in your data are important to the predictive models you use and will influence the results you are going to achieve. The quality and quantity of the features will have great influence on whether the model is good or not.\n\n### Euler angles\nThe Euler angles are three angles introduced by Leonhard Euler to describe the orientation of a rigid body with respect to a fixed coordinate system.\n\n### Fast Fourier Transform Denoising","3d05507f":"#### Observation: No missing data","85a25397":"# Descriptive Statistics","a6b94cf7":"**Observation:**\n*     orientation_X and orientation_W are strongly correlated\n*     orientation_Y and orientation_Z are strongly correlated\n*     linear_accelaration_Y and linear_accelaration_Z also has positive correlation\n*     angular_velocity_Y and angular_velocity_Z has negative correlation","44bcbda9":"### Feature Engineering\n* calculate total angular velocity\n* calculate total linear accelearation\n* calculate total orientaion\n* calculate acceleration vs velocity\n* calculate total eular angle","d67a3513":"### Observation:\n*    Angular velocity are normally distributed infect they are symmetrical data distribution\n*    linear_accelaration are normally distributed\/symmetrical distribution but average value is slightly negative for linear_accelaration_Z\n*    X,Y,Z,W orientation data are not symmetrical or bell shaped distributed. \n*         X,Y orientation data are distributed un-even between 1 to -1.\n*         Z,W orientation data are distributed un-even between 1.5 to -1.5\n\nSince orientation data is not linearly distributed, taking log of the orientation data may improve the results.","7d807732":"### Sorting based on series_id and measurement_number","015c2a2d":"Now, let's look at the result:","fb72682b":"### Correlation Matrix","9fca17cd":"**Observation:**\n\nLooks like orientation features are Most important features. we can do further feature engineering around Orientation Feature. Lets remove low importance features and then run the model.","312ff7e8":"Let's say that I want to denoise the signal on angular_velocity and linear_acceleration column","f7c13c10":"## Feature Enginnering\nFeature Enginnering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. Feature engineering is fundamental to the application of machine learning, and is both difficult and expensive. The features in your data are important to the predictive models you use and will influence the results you are going to achieve. The quality and quantity of the features will have great influence on whether the model is good or not.\n\n## Euler angles\nThe Euler angles are three angles introduced by Leonhard Euler to describe the orientation of a rigid body with respect to a fixed coordinate system.","08b4bb50":"### Is there any duplicate data?","c6f9cf35":"**Observation**: There are many outliers in angular_velocity and linear accelaration data","60e0270c":"## Introduction:\nRobots are smart\u2026 by design. To fully understand and properly navigate a task, however, they need input about their environment.\nIn this competition, you\u2019ll help robots recognize the floor surface they\u2019re standing on using data collected from Inertial Measurement Units (IMU sensors).\n\n## About Data: \nCareerCon has collected IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises. \n\n## Objective:\nThe task is to predict which one of the nine floor types (carpet, tiles, concrete) the robot is on using sensor data such as acceleration and velocity. Succeed and you'll help improve the navigation of robots without assistance across many different surfaces, so they won\u2019t fall down on the job.\n","4157666c":"## Run Model:\n#### As this is a multi class classification problem. Lets try Random Forest Classifier algorithm.","dfbe21db":"**Feature Importance**\n\nUnderstanding about important features will help us fine tuning feature enginnering as well accuracy improvement.","1de01ba9":"**Observation:**\n\n*     For hard tile surface we can see little jerk in orientation data.\n*     for orientation_X these data range is approx 0.5 to 1.0, \n*     for orientation_Y these data range is approx -1.0 to -0.5\n*     for orientation_Z these data range is approx -0.12 to -0.8\n*     for orientation_W these data range is approx 0.07 to 0.12 \n*     for angular velocity and linear accelaration data, there is a symmetry around mean in terms of data distribution.\n    ","20a3560e":"If for whatever reason you want to denoise the signal, you can use fast fourier transform. Detailed implementation of how it's done is out of the scope of this kernel. You can learn more about it here: https:\/\/en.wikipedia.org\/wiki\/Fast_Fourier_transform","89b693cf":"Ref:\n\nfeature engg kernel1: https:\/\/www.kaggle.com\/jesucristo\/1-robots-eda-rf-cval-0-73\nkernel 2: https:\/\/www.kaggle.com\/willkoehrsen\/automated-feature-engineering-basics\/notebook\n\nfeature importance: https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html\n\nmedian absolute deviation: https:\/\/en.wikipedia.org\/wiki\/Median_absolute_deviation\nQuaternions and 3rd rotation, explained interactively: https:\/\/www.youtube.com\/watch?v=zjMuIxRvygQ https:\/\/en.wikipedia.org\/wiki\/Conversion_between_quaternions_and_Euler_angles","09b43bb8":"### Box plot of angular_velocity, orientation and linear_accelaration data","1b829b72":"## Preprocessing data\n\n### Is there any missing data?","a21be687":"## Target surface type and their sample count","06312479":"As you can see, our signal become much smoother than before. Here's a closer comparison:","2e777417":"## Train Data Description"}}