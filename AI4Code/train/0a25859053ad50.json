{"cell_type":{"6af87dc4":"code","0fa84250":"code","ab618edc":"code","635c02aa":"code","2f3cacbc":"code","90572283":"code","b82af4b4":"code","f42f86a0":"code","44adb3c1":"code","d125294f":"markdown","ec962b51":"markdown","80f9bde6":"markdown","c4cf9c66":"markdown","3a060515":"markdown","c6717895":"markdown","dd55488e":"markdown","6b9d634b":"markdown"},"source":{"6af87dc4":"import os\nimport sys\nimport tensorflow as tf\nimport subprocess\n\n# If in Colab install tensorflow_cloud\nif \"google.colab\" in sys.modules and not os.environ.get(\n    \"TF_KERAS_RUNNING_REMOTELY\"):\n    subprocess.run(\n        ['python3', '-m', 'pip', 'install', 'tensorflow-cloud', '-q'])\n\nimport tensorflow_cloud as tfc\nprint(tfc.__version__)","0fa84250":"# Set Google Cloud Specific parameters\n\n# TODO: Please set GCP_PROJECT_ID to your own Google Cloud project ID.\nGCP_PROJECT_ID = 'YOUR_PROJECT_ID' #@param {type:\"string\"}\n\n# TODO: set GCS_BUCKET to your own Google Cloud Storage (GCS) bucket.\nGCS_BUCKET = 'YOUR_GCS_BUCKET_NAME' #@param {type:\"string\"}\n\n# DO NOT CHANGE: Currently only the 'us-central1' region is supported.\nREGION = 'us-central1'\n\n# OPTIONAL: You can change the project name to any string.\nJOB_NAME = 'nasnet' #@param {type:\"string\"}\n\n# Setting location were training logs and checkpoints will be stored\nGCS_BASE_PATH = f'gs:\/\/{GCS_BUCKET}\/{JOB_NAME}'\nTENSORBOARD_LOGS_DIR = os.path.join(GCS_BASE_PATH,\"logs\")\nMODEL_CHECKPOINT_DIR = os.path.join(GCS_BASE_PATH,\"checkpoints\")","ab618edc":"# Using tfc.remote() to ensure this code only runs in notebook\nif not tfc.remote():\n\n    # Authentication for Kaggle Notebooks\n    if \"kaggle_secrets\" in sys.modules:\n        from kaggle_secrets import UserSecretsClient\n        UserSecretsClient().set_gcloud_credentials(project=GCP_PROJECT_ID)\n\n    # Authentication for Colab Notebooks\n    if \"google.colab\" in sys.modules:\n        from google.colab import auth\n        auth.authenticate_user()\n        os.environ[\"GOOGLE_CLOUD_PROJECT\"] = GCP_PROJECT_ID","635c02aa":"(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n\n# Setting input specific parameters\n# The model expects input of dimetions of (INPUT_IMG_SIZE, INPUT_IMG_SIZE, 3)\nINPUT_IMG_SIZE = 32\nNUM_CLASSES = 10","2f3cacbc":"from tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow.keras.models import Sequential\n\n\nimg_augmentation = Sequential(\n    [\n        # Resizing input to better match ImageNet size\n        preprocessing.Resizing(256, 256),\n        preprocessing.RandomRotation(factor=0.15),\n        preprocessing.RandomFlip(),\n        preprocessing.RandomContrast(factor=0.1),\n    ],\n    name=\"img_augmentation\",\n)","90572283":"from tensorflow.keras import layers\n\ndef build_model(num_classes, input_image_size):\n    inputs = layers.Input(shape=(input_image_size, input_image_size, 3))\n    x = img_augmentation(inputs)\n\n    model = tf.keras.applications.NASNetMobile(\n        input_shape=None,\n        include_top=False,\n        weights=\"imagenet\",\n        input_tensor=x,\n        pooling=None,\n        classes=num_classes,\n    )\n\n    # Freeze the pretrained weights\n    model.trainable = False\n\n    # We unfreeze the top 20 layers while leaving BatchNorm layers frozen\n    for layer in model.layers[-20:]:\n        if not isinstance(layer, layers.BatchNormalization):\n            layer.trainable = True\n\n    # Rebuild top\n    x = layers.GlobalAveragePooling2D(name=\"avg_pool\")(model.output)\n    x = layers.BatchNormalization()(x)\n\n    x = layers.Dense(128, activation=\"relu\")(x)\n    x = layers.Dense(64, activation=\"relu\")(x)\n    outputs = layers.Dense(num_classes, activation=\"softmax\", name=\"pred\")(x)\n\n    # Compile\n    model = tf.keras.Model(inputs, outputs, name=\"NASNetMobile\")\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-4)\n    model.compile(\n        optimizer=optimizer,\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    return model","b82af4b4":"model = build_model(NUM_CLASSES, INPUT_IMG_SIZE)\n\nif tfc.remote():\n    # Configure Tensorboard logs\n    callbacks=[\n        tf.keras.callbacks.TensorBoard(log_dir=TENSORBOARD_LOGS_DIR),\n        tf.keras.callbacks.ModelCheckpoint(\n            MODEL_CHECKPOINT_DIR,\n            save_best_only=True),\n        tf.keras.callbacks.EarlyStopping(\n            monitor='loss',\n            min_delta =0.001,\n            patience=3)]\n\n    model.fit(x=x_train, y=y_train, epochs=100,\n              validation_split=0.2, callbacks=callbacks)\n\nelse:\n    # Run the training for 1 epoch and a small subset of the data to validate setup\n    model.fit(x=x_train[:100], y=y_train[:100], validation_split=0.2, epochs=1)","f42f86a0":"if not tfc.remote():\n    print('Training on TensorFlow Cloud...')\n\n    # If you are using a custom image you can install modules via requirements\n    # txt file.\n    with open('requirements.txt','w') as f:\n        f.write('tensorflow-cloud==0.1.12\\n')\n\n    # Optional: Some recommended base images. If you provide none the system\n    # will choose one for you.\n    TF_GPU_IMAGE= \"tensorflow\/tensorflow:latest-gpu\"\n    TF_CPU_IMAGE= \"tensorflow\/tensorflow:latest\"\n\n    tfc.run(\n        distribution_strategy='auto',\n        requirements_txt='requirements.txt',\n        docker_config=tfc.DockerConfig(\n            parent_image=TF_GPU_IMAGE,\n            image_build_bucket=GCS_BUCKET\n            ),\n        chief_config=tfc.COMMON_MACHINE_CONFIGS['K80_1X'],\n         worker_config=tfc.COMMON_MACHINE_CONFIGS['K80_1X'],\n         worker_count=3,\n        job_labels={'job': JOB_NAME}\n    )","44adb3c1":"# %load_ext tensorboard\n# %tensorboard --logdir TENSORBOARD_LOGS_DIR","d125294f":"Set project parameters. For Google Cloud Specific parameters refer to [Google Cloud Project Setup Instructions](https:\/\/www.kaggle.com\/nitric\/google-cloud-project-setup-instructions\/).","ec962b51":"# Distributed training NasNet with tensorflow_cloud and Google Cloud\n\nThis example is based on [Image classification via fine-tuning with EfficientNet](https:\/\/keras.io\/examples\/vision\/image_classification_efficientnet_fine_tuning\/) to demonstrate how to train a [NasNetMobile](https:\/\/keras.io\/api\/applications\/nasnet\/#nasnetmobile-function) model using [tensorflow_cloud](https:\/\/github.com\/tensorflow\/cloud) and Google Cloud Platform at scale using distributed training.\n\n<table align=\"left\">\n    <td>\n        <a href=\"https:\/\/colab.research.google.com\/github\/tensorflow\/cloud\/blob\/master\/examples\/distributed_training_nasnet_with_tensorflow_cloud.ipynb\">\n            <img width=\"50\" src=\"https:\/\/cloud.google.com\/ml-engine\/images\/colab-logo-32px.png\" alt=\"Colab logo\">Run in Colab\n        <\/a>\n    <\/td>\n    <td>\n        <a href=\"https:\/\/github.com\/tensorflow\/cloud\/blob\/master\/examples\/distributed_training_nasnet_with_tensorflow_cloud.ipynb\">\n            <img src=\"https:\/\/cloud.google.com\/ml-engine\/images\/github-logo-32px.png\" alt=\"GitHub logo\">View on GitHub\n        <\/a>\n     <\/td>\n    <td>\n        <a href=\"https:\/\/www.kaggle.com\/nitric\/distributed-training-nasnet-with-tensorflow-cloud\">\n            <img width=\"90\" src=\"https:\/\/www.kaggle.com\/static\/images\/site-logo.png\" alt=\"Kaggle logo\">Run in Kaggle\n        <\/a>\n     <\/td>\n<\/table>","80f9bde6":"# Training Results\nWhile the training is in progress you can use Tensorboard to view the results.","c4cf9c66":"## Load the model and prepare for training\nWe will load a NASNetMobile pretrained model (with weights) and unfreeze a few layers for fine tuning the model to better match the dataset.","3a060515":"## Start the remote training\n\nThis step will prepare your code from this notebook for remote execution and starts a distributed training remotely on Google Cloud Platfrom to train the model. Once the job is submitted you can go to the next step to monitor the jobs progress via Tensorboard.\n","c6717895":"## Authenticating the notebook to use your Google Cloud Project\n\nFor Kaggle Notebooks click on \"Add-ons\"->\"Google Cloud SDK\" before running the cell below.","dd55488e":" Add preprocessing layers APIs for image augmentation.","6b9d634b":"## Load and prepare data\nRead raw data and split to train and test data sets."}}