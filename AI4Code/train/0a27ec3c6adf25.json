{"cell_type":{"6267721f":"code","e43461a3":"code","20534e0f":"code","6603b341":"code","b207323a":"code","8dfd964e":"code","c585b8bd":"code","54327916":"code","6ea0b9dd":"code","ddf4eb0e":"code","9eb087f0":"code","25f60bf2":"code","c1d5d33f":"code","59c9fe91":"code","965dbda7":"code","09d003d5":"code","3327a79f":"code","7a468122":"code","a5d925ab":"code","2f05b8c2":"code","f1d5238d":"code","b073cfd3":"code","94a03444":"code","9324c01b":"code","36814e83":"code","0eb570f9":"code","9efda2d4":"code","a832c8dd":"code","17a8d98c":"code","b43b18c2":"code","3b0a0216":"code","7687dcc5":"code","141eec76":"markdown","6ece262f":"markdown","687f2d91":"markdown","ad660dd5":"markdown","ba593f5a":"markdown","25512d76":"markdown","89be23d6":"markdown","9b1e89b6":"markdown","e4d945a6":"markdown","0b3c6996":"markdown","8c1144ae":"markdown","56bac62c":"markdown","2643065a":"markdown","2c815233":"markdown","23b866ad":"markdown","aa41ec90":"markdown","a4e20065":"markdown","7c7344a7":"markdown","7b67d121":"markdown","288295be":"markdown","a8dd022e":"markdown","43fb7201":"markdown","2ebdeb79":"markdown"},"source":{"6267721f":"#Importing few libraries \nimport imageio\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.rcParams[\"figure.figsize\"] = (20,10)","e43461a3":"img = imageio.imread('..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/dog.1.jpg')\nimg.shape","20534e0f":"plt.imshow(img)","6603b341":"def show_rgb(img):\n    for i in range(3):\n        plt.subplot(1,3,i+1)\n        plt.imshow(img[:,:,i], cmap ='gray')","b207323a":"show_rgb(img)","8dfd964e":"#Importing pytorch and numpy\nimport torch\nimport numpy as np\nimport torch.nn.functional as F","c585b8bd":"#Flat input tensor\nx_fc = torch.randn(100,784)\nx_fc","54327916":"#Weights or Features\nW = torch.randn(784, 10)\/np.sqrt(784)\nW.requires_grad_()","6ea0b9dd":"b = torch.zeros(10, requires_grad=True)\nb","ddf4eb0e":"y_pre = torch.matmul(x_fc,W)+b\ny = F.relu(y_pre)\nx_fc.shape, y.shape","9eb087f0":"x_cnn = torch.randn(100,1,28,28)\nx_cnn.shape","25f60bf2":"#Take 16 number of 3x3 filters initialized randomly: Weights\/Filters\nW1 = torch.randn(16, 1, 3, 3)\/np.sqrt(1*3*3)\nW1.requires_grad_()#to set weights for updation during training","c1d5d33f":"#Take a vector of 16x1: Bias Vector\nb1 = torch.zeros(16, requires_grad=True)\nb1","59c9fe91":"conv1_pre = F.conv2d(x_cnn, W1, bias=b1, stride=1, padding=1)\nconv1 = F.relu(conv1_pre)","965dbda7":"x_cnn.shape, conv1.shape","09d003d5":"W2 = torch.randn(32, 16, 3, 3)\/np.sqrt(16*3*3)\nW2.requires_grad_()\n\nb2 = torch.zeros(32, requires_grad=True)\n\nconv2 = F.relu(F.conv2d(conv1, W2, b2, stride=1, padding=1))\nconv2.shape","3327a79f":"M = torch.zeros(4, 3)\n\nM2 = M.view(1,1,12)\nM3 = M.view(-1)\nM4 = M.view(-1,2,3)\nM5 = M.view(2,1,2,3)\n\nprint(\"M:{},\\nM2: {}, \\nM3: {}, \\nM4: {},\\nM5: {}.\" .format(M, M2, M3, M4, M5))","7a468122":"# Reshape flat input image into a 4D batched image input\nx_flat = torch.randn(100, 784)\nx_reshaped = x_flat.view(-1, 1, 28, 28)\n\n# Print input shape\nprint(x_reshaped.shape) #CNN expects 4D input: [batch, channel, height, width]","a5d925ab":"# Flatten convolutional feature maps into a vector\nh_flat = conv2.view(-1, 28*28*32)\n\n# Print output shape\nprint(h_flat.shape) #\"flatten\" a CNN's 4D output to 2D","2f05b8c2":"print(\"Shape of conv2 feature maps before pooling : {0}\".format(conv2.shape))","f1d5238d":"max_pool2 = F.max_pool2d(conv2, kernel_size = 2)\nprint(\"Shape of conv2 feature maps after pooling : {0}\".format(max_pool2.shape))","b073cfd3":"avg_pool2 = F.avg_pool2d(conv2, kernel_size=2)\nprint(\"Shape of conv2 feature maps after avg pooling: {0}\".format(avg_pool2.shape))","94a03444":"feature_map_fig = torch.tensor(np.array([[1,1,2,4], [5,6,7,8], [3,2,1,0], [1,2,3,4]], dtype = np.float32))\nfmap_fig = feature_map_fig.view(1,1,4,4)\nprint(\"Feature map shape pre-pooling: {}\".format(fmap_fig.shape))","9324c01b":"# Maxpool\nmax_pool_fig = F.max_pool2d(fmap_fig, kernel_size=2)\nprint(\"\\nMax pool\")\nprint(\"Shape: {}\".format(max_pool_fig.shape))\nprint(torch.squeeze(max_pool_fig))","36814e83":"# Avgpool\navg_pool_fig = F.avg_pool2d(fmap_fig, kernel_size=2)\nprint(\"\\nAvg pool\")\nprint(\"Shape: {}\".format(avg_pool_fig.shape))\nprint(torch.squeeze(avg_pool_fig))","0eb570f9":"# Since striding is part of the convolution operation, we'll start with the feature maps before the 2nd convolution\nprint(\"Shape of conv1 feature maps: {0}\".format(conv1.shape))","9efda2d4":"# Apply 2nd convolutional layer, with striding of 2\nconv2_strided = F.relu(F.conv2d(conv1, W2, bias=b2, stride=2, padding=1))\n\n# Print output shape\nprint(\"Shape of conv2 feature maps with stride of 2: {0}\".format(conv2_strided.shape))","a832c8dd":"!pip install torchsummary","17a8d98c":"import torch.nn as nn\nfrom torchsummary import summary","b43b18c2":"class Custom_CNN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=5, padding=2) #Convolution Layer\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2) #Convolution Layer\n        self.fc1 = nn.Linear(7*7*64, 256) #Fully Connected Layer\n        self.fc2 = nn.Linear(256, 2) #Fully Connected Layer(Final Output Layer)\n\n    def forward(self, x):\n        #First Layer with Convolution, ReLU and Max Pooling\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, kernel_size=2)\n        \n        #Second Layer with Convolution, ReLU and Max Pooling\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, kernel_size=2)\n        \n        #Fully Connected Layer with activation function\n        x = x.view(-1 , 7*7*64)\n        x = self.fc1(x)\n        x = F.relu(x)\n        \n        #Fully connected layer : Final Output Layer\n        x = self.fc2(x)\n        return x","3b0a0216":"model = Custom_CNN()\nmodel","7687dcc5":"summary(model, (3,28,28))","141eec76":"# Convolutions and Layers\n---\n\nNow let's develop a model that would learn the information of image and would classify them into 'class'. But before we jump directly into developing some 'deep' architecture, we will each part of a convolution neural network.\n\n![](https:\/\/miro.medium.com\/max\/2560\/1*ciDgQEjViWLnCbmX-EeSrA.gif)\n\n## So, what is a Convoution Neural Network?\n> In 1990s Yann LeCun introduced a powerful model that would classifiy handwritten digits with high accuracy(at that time). This popular model is known as LeNet. It had few operations different from the traditional [multi-layered perceptron](https:\/\/www.kaggle.com\/pr2tik1\/multi-layered-perceptron-pytorch). The LeNet model had a **convolution** operation and hence the name \"Convolution Neural Networks\".\n\nThis technique proved to be effective after addition of two more factors:\n\n    1. Availability of better and powerful processors such as GPUs and TPUs.\n    2. Availability of large sets of data for training and testing.\n","6ece262f":"## 2. Convolution Layer\n---\n\nFollowing are two convolution layers that we usually find at initial stage of deep learning CNN architecture. ","687f2d91":"#### Layer 2: Weights and Biases","ad660dd5":"## Feature map: ","ba593f5a":"Now similarly as above we create a second layer as below.","25512d76":"# Custom CNN\n---\n\nFinally we will be creating a custom CNN model. This is combination of all what we had explored till now. Also we will be using a libraryt named torchsummary to look closely into our cutom model later. Follow the comments in the next few cells to understand better.","89be23d6":"### 3.2. Striding","9b1e89b6":"The activation function used here is a ReLU function. A ReLU activation function is a non-linear (**NOTE:** It looks linear but is non-linear for complete range.) More about ReLU [here](https:\/\/machinelearningmastery.com\/rectified-linear-activation-function-for-deep-learning-neural-networks\/).","e4d945a6":"Observe the shape of input and output as a vector of shape 100x784 and 100x10 respectively. The '100' denotes number of vectors\/row and the 784 denotes a flattened image vector from a 28x28 image array. Finally in output the '10' denotes the number of classes.(This is just for example and not related to images in our dataset). ","0b3c6996":"### 3.1. Pooling","8c1144ae":"#### Reshaping \n\nWhile connecting the layers we often need to reshape the arrays. Developing a perfectly working CNN is all about building correct set of dimensions.","56bac62c":"A convolution operation requires \"Filters\" or \"Weights\" same as in a fully connected layer. But these are matrices of different sizes. The filters takes the normalized image arrays ([tensors](https:\/\/pytorch.org\/docs\/stable\/tensors.html)) and returns feature maps as output. This are learned weights that is passed to next layer. ","2643065a":"# END\n\n- What next?\n    - This notebook only introduces the model development part. I will be posting next notebook on training the network. Until then you may check and upvote if you like: \n    \n        1. [PyTorch Logistic Regression](https:\/\/www.kaggle.com\/pr2tik1\/logistic-regression-in-pytorch-from-scratch)\n        2. [PyTorch MLP](https:\/\/www.kaggle.com\/pr2tik1\/multi-layered-perceptron-pytorch)\n        3. [Other Data Science Notebooks](https:\/\/www.kaggle.com\/pr2tik1\/code)\n\n\n- References:\n\n    1. https:\/\/www.coursera.org\/learn\/machine-learning-duke\n    2. http:\/\/d2l.ai\n    3. https:\/\/machinelearningmastery.com \n    4. https:\/\/cs231n.github.io\/convolutional-networks\/\n    5. https:\/\/pytorch.org\/tutorials\/\n    6. All the images are directly rendered from there respective websites. Please visit by accessing them from markdown.\n    \n- Author: Pratik Kumar","2c815233":"Note the dimension of bias is not exactly same to output of matrix multiplication of weights and input vectors. This is because we will taking advantage of something called [broadcasting](https:\/\/machinelearningmastery.com\/broadcasting-with-numpy-arrays\/). ","23b866ad":"# \ud83e\uddd8\u200d\u2642\ufe0f Basics of CNN in PyTorch\n---\n\nConvolutional Neural Networks, do we really understand what goes under the hood? Imagining what actually goes within a deep learning architecture can be tough. That's okay it's normal! As neural networks are still called the *black box*, even though we have moved ahead so much towards powerful-complex models. This notebook is an attempt to understand a very common yet powerful deep learning model of working with image classification, detection, etc., known as **Convolution Neural Network**.\n\n![](https:\/\/miro.medium.com\/max\/3840\/1*oB3S5yHHhvougJkPXuc8og.gif)\n\n\nHere, I have tried to explain each part of the deep learning with CNN architectures. We will be using PyTorch framework. Let's start!","aa41ec90":"# Image\n---\n\nLet's understand more about a colored image. The image of an example from the dataset contains RGB(Red Green Blue) Channels. We will try to look how exactly, just a combination of three primary colors forms a complete image.   \n\n![](https:\/\/miro.medium.com\/max\/2146\/1*icINeO4H7UKe3NlU1fXqlA.jpeg)","a4e20065":"## 1. Fully Conneted Layer\n---","7c7344a7":"# Model\n---\n\nTo summarize a convolution neural network contains: \n\n- Model:\n    - Fully connected Layers\n    - Convolution Layers\n    - Pooling Layers\n    - Normalization Layers\n    - Striding operaiton\n    - Forward and Backward Propagation\n    - Activation Functions\n\n- Hyper-parameters:\n    - Updation Rule\n    - Loss function\n    - Learning Rate","7b67d121":"#### Layer 1: Weights and Biases","288295be":"Fully connected layer has set of weight matrices and biases. These together can be formulated as:\n\n   > Output = activation_function(Weight*Input + Bias)\n   \nGenerally within a CNN these are placed at the end. The input is from the convolution layers and output is number of classes(2 in this case, Dog and Cat).","a8dd022e":"## RGB Channels\n---\nNotice the difference between pixel intensities of the original image splitted into images of each individual channels. ","43fb7201":"# PyTorch Section\n---\n\nPyTorch is one the commonly used frameworks in developing deep learning architectures, training theam and testing the network.","2ebdeb79":"## 3. Pooling and Striding \n---\n\nPooling addresses the problem of sensitivity of the output feature maps from the convolution layers. Follow this [post](https:\/\/machinelearningmastery.com\/pooling-layers-for-convolutional-neural-networks\/) for a brief explanation. Further [Striding](https:\/\/www.google.com\/search?client=opera&q=pooling+and+striding&sourceid=opera&ie=UTF-8&oe=UTF-8) is skipping positions while sliding the convolutional kernel.\n\n![](https:\/\/cdn-media-1.freecodecamp.org\/images\/gb08-2i83P5wPzs3SL-vosNb6Iur5kb5ZH43)\n\nNote how the filter moves on the image array input."}}