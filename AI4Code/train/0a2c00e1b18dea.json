{"cell_type":{"63e7b321":"code","5a3c1b28":"code","3c807118":"code","e15154a5":"code","9fc0dacd":"code","5ffdc546":"code","1e801bde":"code","7dcbbc87":"code","f51c0d22":"code","c1874640":"code","d69bbfd3":"code","ad20f922":"code","8ac45af9":"code","f0a2dae7":"code","e4b46443":"code","0e84e293":"code","85a0a90a":"code","59973a41":"code","93a653a9":"code","086a6555":"code","7834a462":"code","ab11c4ae":"code","7d06b0a8":"code","6aadb46e":"code","e733ca80":"markdown","b110785c":"markdown","bd92ea34":"markdown","26d6b90c":"markdown","0402dc23":"markdown","024de4a3":"markdown","5d2d119f":"markdown","ee7fcd05":"markdown","7801f160":"markdown","fabc2068":"markdown","5cd6cd62":"markdown","fe7953eb":"markdown","a1813350":"markdown","771adf8a":"markdown","86c3f9d8":"markdown","e06411d5":"markdown","756893ff":"markdown","fb21e655":"markdown","a7949dd4":"markdown","c39b1642":"markdown","a37c49e8":"markdown","45daf144":"markdown"},"source":{"63e7b321":"# ! pip3 install transformers","5a3c1b28":"import pandas as pd\nimport numpy as np\nimport pickle\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport copy\nfrom tqdm.notebook import tqdm\nimport gc\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom sklearn.metrics import (\n    accuracy_score, \n    f1_score, \n    classification_report\n)\n\nfrom transformers import (\n    T5Tokenizer, \n    T5Model,\n    T5ForConditionalGeneration,\n    get_linear_schedule_with_warmup\n)\n\nproject_dir = '..\/input\/avjanatahackresearcharticlesmlc\/av_janatahack_data\/'","3c807118":"! nvidia-smi","e15154a5":"train_df = pd.read_csv(project_dir + 'train.csv')\ntrain_df.head()","9fc0dacd":"# preprocessing\ndef clean_abstract(text):\n    text = text.split()\n    text = [x.strip() for x in text]\n    text = [x.replace('\\n', ' ').replace('\\t', ' ') for x in text]\n    text = ' '.join(text)\n    text = re.sub('([.,!?()])', r' \\1 ', text)\n    return text\n    \n\ndef get_texts(df):\n    texts = 'multilabel classification: ' + df['ABSTRACT'].apply(clean_abstract)\n    texts = texts.values.tolist()\n    return texts\n\n\ndef get_labels(df):\n    labels_li = [' '.join(x.lower().split()) for x in df.columns.to_list()[3:]]\n    labels_matrix = np.array([labels_li] * len(df))\n\n    mask = df.iloc[:, 3:].values.astype(bool)\n    labels = []\n    for l, m in zip(labels_matrix, mask):\n        x = l[m]\n        if len(x) > 0:\n            labels.append(' , '.join(x.tolist()) + ' <\/s>')\n        else:\n            labels.append('none <\/s>')\n    return labels\n\ntexts = get_texts(train_df)\nlabels = get_labels(train_df)\n\nfor text, label in zip(texts[:5], labels[:5]):\n    print(f'TEXT -\\t{text}')\n    print(f'LABEL -\\t{label}')\n    print()","5ffdc546":"# no. of samples for each class\ncategories = train_df.columns.to_list()[3:]\nplt.figure(figsize=(6, 4))\n\nax = sns.barplot(categories, train_df.iloc[:, 3:].sum().values)\nplt.ylabel('Number of papers')\nplt.xlabel('Paper type ')\nplt.xticks(rotation=90)\nplt.show()","1e801bde":"# no of samples having multiple labels\nrow_sums = train_df.iloc[:, 3:].sum(axis=1)\nmultilabel_counts = row_sums.value_counts()\n\nplt.figure(figsize=(6, 4))\nax = sns.barplot(multilabel_counts.index, multilabel_counts.values)\nplt.ylabel('Number of papers')\nplt.xlabel('Number of labels')\nplt.show()","7dcbbc87":"# text lengths\ny = [len(t.split()) for t in texts]\nx = range(0, len(y))\nplt.bar(x, y)","f51c0d22":"# label lengths\ny = [len(l.split()) for l in labels]\nx = range(0, len(y))\nplt.bar(x, y)","c1874640":"class Config:\n    def __init__(self):\n        super(Config, self).__init__()\n\n        self.SEED = 42\n        self.MODEL_PATH = 't5-base'\n\n        # data\n        self.TOKENIZER = T5Tokenizer.from_pretrained(self.MODEL_PATH)\n        self.SRC_MAX_LENGTH = 320\n        self.TGT_MAX_LENGTH = 20\n        self.BATCH_SIZE = 16\n        self.VALIDATION_SPLIT = 0.25\n\n        # model\n        self.DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.FULL_FINETUNING = True\n        self.LR = 3e-5\n        self.OPTIMIZER = 'AdamW'\n        self.CRITERION = 'BCEWithLogitsLoss'\n        self.SAVE_BEST_ONLY = True\n        self.N_VALIDATE_DUR_TRAIN = 3\n        self.EPOCHS = 1\n\nconfig = Config()","d69bbfd3":"class T5Dataset(Dataset):\n    def __init__(self, df, indices, set_type=None):\n        super(T5Dataset, self).__init__()\n\n        df = df.iloc[indices]\n        self.texts = get_texts(df)\n        self.set_type = set_type\n        if self.set_type != 'test':\n            self.labels = get_labels(df)\n\n        self.tokenizer = config.TOKENIZER\n        self.src_max_length = config.SRC_MAX_LENGTH\n        self.tgt_max_length = config.TGT_MAX_LENGTH\n\n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, index):\n        src_tokenized = self.tokenizer.encode_plus(\n            self.texts[index], \n            max_length=self.src_max_length,\n            pad_to_max_length=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_token_type_ids=False,\n            return_tensors='pt'\n        )\n        src_input_ids = src_tokenized['input_ids'].squeeze()\n        src_attention_mask = src_tokenized['attention_mask'].squeeze()\n\n        if self.set_type != 'test':\n            tgt_tokenized = self.tokenizer.encode_plus(\n                self.labels[index], \n                max_length=self.tgt_max_length,\n                pad_to_max_length=True,\n                truncation=True,\n                return_attention_mask=True,\n                return_token_type_ids=False,\n                return_tensors='pt'\n            )\n            tgt_input_ids = tgt_tokenized['input_ids'].squeeze()\n            tgt_attention_mask = tgt_tokenized['attention_mask'].squeeze()\n\n            return {\n                'src_input_ids': src_input_ids.long(),\n                'src_attention_mask': src_attention_mask.long(),\n                'tgt_input_ids': tgt_input_ids.long(),\n                'tgt_attention_mask': tgt_attention_mask.long()\n            }\n\n        return {\n            'src_input_ids': src_input_ids.long(),\n            'src_attention_mask': src_attention_mask.long()\n        }","ad20f922":"# train-val split\n\nnp.random.seed(config.SEED)\n\ndataset_size = len(train_df)\nindices = list(range(dataset_size))\nsplit = int(np.floor(config.VALIDATION_SPLIT * dataset_size))\nnp.random.shuffle(indices)\n\ntrain_indices, val_indices = indices[split:], indices[:split]","8ac45af9":"train_data = T5Dataset(train_df, train_indices)\nval_data = T5Dataset(train_df, val_indices)\n\ntrain_dataloader = DataLoader(train_data, batch_size=config.BATCH_SIZE)\nval_dataloader = DataLoader(val_data, batch_size=config.BATCH_SIZE)\n\nb = next(iter(train_dataloader))\nfor k, v in b.items():\n    print(f'{k} shape: {v.shape}')","f0a2dae7":"class T5Model(nn.Module):\n    def __init__(self):\n        super(T5Model, self).__init__()\n\n        self.t5_model = T5ForConditionalGeneration.from_pretrained(config.MODEL_PATH)\n\n    def forward(\n        self,\n        input_ids, \n        attention_mask=None, \n        decoder_input_ids=None, \n        decoder_attention_mask=None, \n        lm_labels=None\n        ):\n\n        return self.t5_model(\n            input_ids,\n            attention_mask=attention_mask,\n            decoder_input_ids=decoder_input_ids,\n            decoder_attention_mask=decoder_attention_mask,\n            lm_labels=lm_labels,\n        )","e4b46443":"device = config.DEVICE\ndevice","0e84e293":"# the get_ohe function converts the decoder generated labels from textual -\n# - format to a one hot encoded form, in order to calculate the micro f1 score\n\ndef get_ohe(x):\n    labels_li = ['_'.join(x.lower().split()) for x in train_df.columns.to_list()[3:]]\n    labels_li_indices = dict()\n    for idx, label in enumerate(labels_li):\n        labels_li_indices[label] = idx\n        \n    y = [labels.split(', ') for labels in x]\n    ohe = []\n    for labels in y:\n        temp = [0] * 6\n        for label in labels:\n            idx = labels_li_indices.get(label, -1)\n            if idx != -1:\n                temp[idx] = 1\n        ohe.append(temp)\n    ohe = np.array(ohe)\n    return ohe","85a0a90a":"def val(model, val_dataloader, criterion):\n    \n    val_loss = 0\n    true, pred = [], []\n    \n    # set model.eval() every time during evaluation\n    model.eval()\n    \n    for step, batch in enumerate(val_dataloader):\n        # unpack the batch contents and push them to the device (cuda or cpu).\n        b_src_input_ids = batch['src_input_ids'].to(device)\n        b_src_attention_mask = batch['src_attention_mask'].to(device)\n    \n        b_tgt_input_ids = batch['tgt_input_ids']\n        lm_labels = b_tgt_input_ids.to(device)\n        lm_labels[lm_labels[:, :] == config.TOKENIZER.pad_token_id] = -100\n\n        b_tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n\n        # using torch.no_grad() during validation\/inference is faster -\n        # - since it does not update gradients.\n        with torch.no_grad():\n            # forward pass\n            outputs = model(\n                input_ids=b_src_input_ids, \n                attention_mask=b_src_attention_mask,\n                lm_labels=lm_labels,\n                decoder_attention_mask=b_tgt_attention_mask)\n            loss = outputs[0]\n\n            val_loss += loss.item()\n\n            # get true \n            for true_id in b_tgt_input_ids:\n                true_decoded = config.TOKENIZER.decode(true_id)\n                true.append(true_decoded)\n\n            # get pred (decoder generated textual label ids)\n            pred_ids = model.t5_model.generate(\n                input_ids=b_src_input_ids, \n                attention_mask=b_src_attention_mask\n            )\n            pred_ids = pred_ids.cpu().numpy()\n            for pred_id in pred_ids:\n                pred_decoded = config.TOKENIZER.decode(pred_id)\n                pred.append(pred_decoded)\n\n    true_ohe = get_ohe(true)\n    pred_ohe = get_ohe(pred)\n\n    avg_val_loss = val_loss \/ len(val_dataloader)\n    print('Val loss:', avg_val_loss)\n    print('Val accuracy:', accuracy_score(true_ohe, pred_ohe))\n\n    val_micro_f1_score = f1_score(true_ohe, pred_ohe, average='micro')\n    print('Val micro f1 score:', val_micro_f1_score)\n    return val_micro_f1_score\n\n\ndef train(\n    model,  \n    train_dataloader, \n    val_dataloader, \n    criterion, \n    optimizer, \n    scheduler, \n    epoch\n    ):\n    \n    # we validate config.N_VALIDATE_DUR_TRAIN times during the training loop\n    nv = config.N_VALIDATE_DUR_TRAIN\n    temp = len(train_dataloader) \/\/ nv\n    temp = temp - (temp % 100)\n    validate_at_steps = [temp * x for x in range(1, nv + 1)]\n    \n    train_loss = 0\n    for step, batch in enumerate(tqdm(train_dataloader, \n                                      desc='Epoch ' + str(epoch))):\n        # set model.eval() every time during training\n        model.train()\n        \n        # unpack the batch contents and push them to the device (cuda or cpu).\n        b_src_input_ids = batch['src_input_ids'].to(device)\n        b_src_attention_mask = batch['src_attention_mask'].to(device)\n    \n        lm_labels = batch['tgt_input_ids'].to(device)\n        lm_labels[lm_labels[:, :] == config.TOKENIZER.pad_token_id] = -100\n\n        b_tgt_attention_mask = batch['tgt_attention_mask'].to(device)\n\n        # clear accumulated gradients\n        optimizer.zero_grad()\n\n        # forward pass\n        outputs = model(input_ids=b_src_input_ids, \n                        attention_mask=b_src_attention_mask,\n                        lm_labels=lm_labels,\n                        decoder_attention_mask=b_tgt_attention_mask)\n        loss = outputs[0]\n        train_loss += loss.item()\n\n        # backward pass\n        loss.backward()\n\n        # update weights\n        optimizer.step()\n        \n        # update scheduler\n        scheduler.step()\n\n        if step in validate_at_steps:\n            print(f'-- Step: {step}')\n            _ = val(model, val_dataloader, criterion)\n    \n    avg_train_loss = train_loss \/ len(train_dataloader)\n    print('Training loss:', avg_train_loss)","59973a41":"def run():\n    # setting a seed ensures reproducible results.\n    # seed may affect the performance too.\n    torch.manual_seed(config.SEED)\n\n    criterion = nn.BCEWithLogitsLoss()\n    \n    # define the parameters to be optmized -\n    # - and add regularization\n    if config.FULL_FINETUNING:\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        optimizer = optim.AdamW(optimizer_parameters, lr=config.LR)\n\n    num_training_steps = len(train_dataloader) * config.EPOCHS\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps\n    )\n\n    max_val_micro_f1_score = float('-inf')\n    for epoch in range(config.EPOCHS):\n        train(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epoch)\n        val_micro_f1_score = val(model, val_dataloader, criterion)\n\n        if config.SAVE_BEST_ONLY:\n            if val_micro_f1_score > max_val_micro_f1_score:\n                best_model = copy.deepcopy(model)\n                best_val_micro_f1_score = val_micro_f1_score\n\n                model_name = 't5_best_model'\n                torch.save(best_model.state_dict(), model_name + '.pt')\n\n                print(f'--- Best Model. Val loss: {max_val_micro_f1_score} -> {val_micro_f1_score}')\n                max_val_micro_f1_score = val_micro_f1_score\n\n    return best_model, best_val_micro_f1_score","93a653a9":"model = T5Model()\nmodel.to(device);","086a6555":"best_model, best_val_micro_f1_score = run()","7834a462":"test_df = pd.read_csv(project_dir + 'test.csv')\ndataset_size = len(test_df)\ntest_indices = list(range(dataset_size))\n\ntest_data = T5Dataset(test_df, test_indices, set_type='test')\ntest_dataloader = DataLoader(test_data, batch_size=config.BATCH_SIZE)","ab11c4ae":"def predict(model):\n    val_loss = 0\n    pred = []\n    model.eval()\n    for step, batch in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n        b_src_input_ids = batch['src_input_ids'].to(device)\n        b_src_attention_mask = batch['src_attention_mask'].to(device)\n\n        with torch.no_grad():\n            # get pred\n            pred_ids = model.t5_model.generate(\n                input_ids=b_src_input_ids, \n                attention_mask=b_src_attention_mask\n            )\n            pred_ids = pred_ids.cpu().numpy()\n            for pred_id in pred_ids:\n                pred_decoded = config.TOKENIZER.decode(pred_id)\n                pred.append(pred_decoded)\n\n    pred_ohe = get_ohe(pred)\n    return pred_ohe\n\npred_ohe = predict(best_model)","7d06b0a8":"sample_submission = pd.read_csv('sample_submission.csv')\nids = sample_submission['ID'].values.reshape(-1, 1)\n\nmerged = np.concatenate((ids, pred_ohe), axis=1)\nsubmission = pd.DataFrame(merged, columns=sample_submission.columns).astype(int)\n\nsubmission","6aadb46e":"submission_fname = f't5_microf1-{round(best_val_micro_f1_score, 4)}.csv'\nsubmission.to_csv(submissions_dir + submission_fname, index=False)","e733ca80":"Load the test dataset, and initialize a DataLoader object for it.","b110785c":"Here we define a Config class, which contains all the fixed parameters & hyperparameters required for **Dataset** creation as well as **Model** training.","bd92ea34":"# Engine","26d6b90c":"# Dataset & Dataloader","0402dc23":"# Config","024de4a3":"Our **T5Dataset** Class takes as input the **dataframe**, **indices** & **set_type**. We calculate the train \/ val set indices beforehand, pass it to **T5Dataset** and slice the dataframe using these indices.","5d2d119f":"Check the GPU configurations","ee7fcd05":"# Exploratory Data Analysis","7801f160":"# Imports","fabc2068":"The entire code is written using **PyTorch**.<br>\nWe'll be using the **transformers** library by [huggingface](https:\/\/github.com\/huggingface\/transformers) as they provide wrappers for multiple Transformer models.","5cd6cd62":"We're using just the abstract text, but concatenating the title text along with it performed better on the leaderboard. \n<br><br>\n\n#### Abstracts\n- Stripping extra whitespaces around the text.\n- Replacing escape characters with whitespace.\n- Padding all punctuations with whitespaces on both sides.\n\n#### Labels\n- Convert the one hot encoded labels to textual format (since T5 only deals with textual data).<br>\ne.g. OHE Labels - [1, 0, 1, 0, 1]<br>T5 Labels - 'computer_science, mathematics, quantitative_finance'\n\n#### Additional Tips\n- Replacing Latex equations with a special token.\n- Data Augmentation. \n\n","fe7953eb":"Our engine consists of the training and validation step functions.","a1813350":"# Data Preprocessing","771adf8a":"Coming to the most interesting part - the model architecture! We'll create a class named **Model**, inherited from **torch.nn.Module**.<br><br>\n\n### Flow\n- We initialize our pretrained T5 model with a Conditional Generation Head.\n- Pass in the src & tgt, input_ids & attention_mask.\n- The model returns the decoder generated output ids (predicted labels in textual format), which we need to decode further using the tokenizer.","86c3f9d8":"# Submission","e06411d5":"Now, we'll create a custom Dataset class inherited from the PyTorch Dataset class. We'll be using the **T5 tokenizer** that returns **input_ids** and **attention_mask**.<br><br>\nThe custom Dataset class will return a dict containing - <br>\n\n- src_input_ids\n- src_attention_mask\n- tgt_input_ids'\n-tgt_attention_mask","756893ff":"From the plot above we can infer that, **320** seems like a good choice for **src MAX_LENGTH** and appx. **15** for **tgt MAX_LENGTH**.<br><br>\n**Convention**\n- src - texts<br>\n- tgt - labels","fb21e655":"Here we'll initialize PyTorch DataLoader objects for the training & validation data.<br>\nThese dataloaders allow us to iterate over them during training, validation or testing and return a batch of the Dataset class outputs.","a7949dd4":"# Model","c39b1642":"### [T5](https:\/\/github.com\/google-research\/text-to-text-transfer-transformer) \n- **Text-To-Text Transfer Transformer**\n- A unified framework that converts every language problem into a text-to-text format.\n- Achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more.\n\n### Multi Class vs Multi Label Classification\n- **Multi Class** - There are multiple categories but each instance is assigned only one, therefore such problems are known as multi-class classification problem.\n- **Multi Label** - There are multiple categories and each instance can be assigned with multiple categories, so these types of problems are known as multi-label classification problem, where we have a set of target labels.","a37c49e8":"### Loss function used<br>\n- **BCEWithLogitsLoss** - Most commonly used loss function for Multi Label Classification tasks. Note that, PyTorch's BCEWithLogitsLoss is numerically stable than BCELoss.\n<br>\n\n### Optimizer used <br>\n- **AdamW** - Commonly used optimizer. Performs better than Adam.\n<br>\n\n### Scheduler used <br>\n- **get_linear_scheduler_with_warmup** from the **transformers** library.\n<br>","45daf144":"# Run"}}