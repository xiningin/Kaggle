{"cell_type":{"62100556":"code","1df12a31":"code","0641e68b":"code","629a1d8f":"code","69950e59":"code","851a639e":"code","a5544a05":"code","843f7c32":"code","b2042d3c":"code","be2300cb":"markdown","f4c7e20c":"markdown","c9f7be74":"markdown","2bd00f82":"markdown","55e9a910":"markdown","6885eb02":"markdown","3fcbd6ab":"markdown","dec7d15d":"markdown","05a141df":"markdown"},"source":{"62100556":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.tsa.ar_model import AutoReg\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename)) # displays file path\n        ","1df12a31":"# gets csv file from kaggle\ndf = pd.read_csv('\/kaggle\/input\/us-military-deaths-by-cause-19802010\/U.S. Military Deaths by cause 1980-2010.csv')\n\n# basic dataset investigation\nprint(df.info())","0641e68b":"namesMap = {'Accident ': 'Accident',\n            'Homicide ': 'Homicide',\n            'Illness ': 'Illness',\n            'Pending ': 'Pending',\n            'Undetermined ': 'Undetermined'} # mappings of column names with extra spaces\n                                            # to better column names\n\n\ndf = df.rename(namesMap, axis=1) # renames dataframe","629a1d8f":"# plots each time series against 'Calendar Year'\nfor columnName in df.columns:\n    if columnName == 'Calendar Year': # we don't care to see 'Calendar Year' plotted against 'Calendar Year'\n        pass\n    else:\n        column = df[columnName]\n        f, ax = plt.subplots()\n        ax.plot(df['Calendar Year'], column)\n        ax.set_title(columnName)\n        ","69950e59":"corr = df.corr() # correlation matrix\n\n# displays in pretty way\nmask = np.triu(np.ones_like(corr, dtype=bool))\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(20, 230, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","851a639e":"# columns that seem to be components of total deaths\ncolumnNames = ['Accident','Illness','Self-Inflicted','Hostile Action',\n               'Homicide', 'Undetermined', 'Terrorist Attack', 'Pending']\n\n# selected data in new dataframes\ncomposedData = df[columnNames]\n\nbottom = np.array([0 for _ in df['Calendar Year']])\ncolors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink', 'tab:gray']\n\nf, ax = plt.subplots()\nfor columnName, color in zip(composedData.columns, colors):\n    column = df[columnName]\n    bottomPlus = bottom + column\n    ax.fill_between(list(df['Calendar Year']), list(bottomPlus), y2=bottom, color=color, label=columnName, alpha=0.85)\n    bottom = bottomPlus\n        \nax.legend(loc='lower left', fontsize=8)\nax.set_title(\"Types of Deaths over Time Count\")\nplt.show()","a5544a05":"# before we attempt to model total deaths from other columns\n# it's worth seeing if we can model total deaths from time--or a lagged version of itself\n\n# function for shifting data streams, used for autocorrelation lagged correlation\ndef shiftSeries(series: pd.Series, lag: int):\n    if lag == 0:\n        return series, series\n    else:\n        return series[lag:], series[:-lag]\n\n# function returns autocorrelation for each lag in lags list \/ range\ndef autoCorrelation(series: pd.Series, lags: list):\n    mean = np.mean(series)\n    sumDevSquared = np.sum((series - mean) ** 2) # sum of squared deviations from mean\n    \n    autoCorrelations = []\n    for lag in lags:\n        seriesOne, seriesTwo = shiftSeries(series, lag) # shifts\n        seriesOneDiff = seriesOne - mean # deviations from mean\n        seriesTwoDiff = seriesTwo - mean # deviations from mean\n        cov = np.sum(seriesOneDiff * seriesTwoDiff) # sum of products of deviations\n        autoCorrelation = cov\/ sumDevSquared # divided by sum of squared deviation to get autocorrelation for lag\n        autoCorrelations.append(autoCorrelation)\n        \n    return autoCorrelations \n\nlags = range(20)\nacf = autoCorrelation(df['Total Deaths'], lags)\n\nf, ax = plt.subplots()\nax.stem(lags, acf)\nax.set_ylabel(\"Auto Correlation\")\nax.set_title(\"Auto Correlation of Total Deaths\")\nax.set_xticks(range(0, 20, 5))\nplt.show()\n\n","843f7c32":"predictor = df['Total Deaths'][:-1].reset_index(drop=True)\ntoPredict = df['Total Deaths'][1:].reset_index(drop=True)\nx = df['Calendar Year'][1:].reset_index(drop=True)\n\nf, ax = plt.subplots()\nax.plot(x, predictor, label='Predictions')\nax.plot(x, toPredict, label='Actual')\nax.legend()\nax.set_title(\"Prior Observation as Prediction for Next\")\nplt.show()","b2042d3c":"def rSquared(actual, predicted):\n    actualMean = np.mean(actual)\n    tss = np.sum((actual - actualMean)**2)\n    rss = np.sum((predicted - actual)**2)\n    return 1 - (rss\/tss)\n\nrsquared = rSquared(toPredict, predictor)\nprint(\"R-Squared\", round(rsquared, 4))","be2300cb":"Because there isn't a lot of noise to this time series, the one year lag model appears to match the data well. We can numerically verify this by calculating the R-Squared.","f4c7e20c":"From the preliminary investigation we did, it looks like the dataset is fairly clean and easy to model. All the values are numeric, and there isn't any missing values. However, there is one problem that is worth addressing now--some of the column names have extra spaces at the end. To fix the column names, I'll map the problematic column names to some new ones in a dictionary. ","c9f7be74":"![Screen Shot 2021-10-23 at 1.43.45 PM.png](attachment:d7e02a59-22c4-42cf-a3a7-0ab6e66a86d0.png)","2bd00f82":"I find it quite shocking that total deaths has strong autocorrelation even with as big of a lag as 5 or 10 years. The strong autocorrelation suggests that time is a strong or perhaps the strongest factor in total deaths. And the lack of any cyclicality suggests that seasonality may not be too large of a factor. \n\nGiven the strong influence of time, we should start by modeling total deaths with a lagged version of itself. ","55e9a910":"There is a great deal of cross-correlation between the columns. Even \"Calendar Year\" exhibits strong (positive and negative) correlation with many of the columns. Now that I think about it, it would make sense for 'Total Deaths' to be simply the sum of some of the other columns--columns like \"Hostile Action\", \"Homicide\" and \"Illness.\" Let's test this hypothesis by plotting the sum of these columns. ","6885eb02":"The \"Total Deaths\" column seems like the most important time series in the database, so I'll focus my modeling efforts there. But first, let's look at each time series. ","3fcbd6ab":"This matches exactly with the line plot of total deaths shown earlier. Because total deaths is additively composed of these columns, it doesn't make sense to model total deaths with the components. We can perfectly predict total deaths by simply adding up the components--there is no need for estimating model parameters. ","dec7d15d":"Turning now to the content of \"Total Deaths,\" several interesting trends are apparent. First and most obviously, the scale of death we're dealing with is tiny, especially compared to horrors like WW1 and WW2. 2500 deaths a year is not much at all. \n\nSecond, the vast majority of deaths are due to Accidents or Illness. Hostile Action has only recently started to increase (recently as in since the 2000s), presumbably due conflicts like Afghanistan. And terrorist attacks, as graphic and shocking as they are, are a truly small proportion of deaths. \n\nAccidents and Illness, while the biggest components of total deaths, seem to be steadily decreasing, indicating that the military is becoming better at reducing these dangers. \n\nLet's now turn to modeling Total Deaths.","05a141df":"By simply using the previous observation as a prediction for the next one, we can explain 80.79% of the variation in \"Total Deaths.\" Because this model is so simple, intuitive, and performs fairly well, I am comfortable with settling on it, even given the fact that there might be more sophisticated methods out there. In particular, I suspect that some form of auto-regressive model will do well. Perhaps information from the other columns can also be incorporated."}}