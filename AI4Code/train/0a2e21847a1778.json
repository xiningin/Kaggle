{"cell_type":{"7a14dde6":"code","cce4a5cd":"code","7c2dc2e4":"code","03f051f7":"code","7dbb9417":"code","1ec0527c":"code","bdd281dd":"code","1459c506":"code","78c57982":"code","5a668a2d":"code","b74afd61":"code","4907ea74":"code","a193db4b":"code","851fe328":"code","00ea7bd6":"code","498e4447":"code","e8a9d998":"code","a2456e9a":"code","880e2983":"code","467c4987":"code","e6ea0591":"code","3bf35f60":"code","fa9b35f1":"code","3180e138":"code","809cc44d":"code","8f00d2bc":"code","96bb4804":"code","0f42e0e1":"code","02f169e7":"code","6ff7fdfb":"code","0865f083":"code","ff6e675e":"code","8b06aaa0":"code","e228838e":"code","79b53286":"code","316fe99d":"code","b7d0bd55":"code","2d5f0ca0":"code","928bd680":"code","17a65517":"code","76733105":"markdown","e5c0a8ec":"markdown","668040f4":"markdown","5180cc70":"markdown","cefd1ac6":"markdown","0367206b":"markdown","576c7812":"markdown","677536ec":"markdown","20eb42fd":"markdown","15d98ebf":"markdown","4ab499bb":"markdown","2f8eed2b":"markdown","7fed63ad":"markdown"},"source":{"7a14dde6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (20,5)\nimport datetime\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range = (0,1))\nfrom sklearn.metrics import mean_squared_error\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cce4a5cd":"df = pd.read_csv(\"\/kaggle\/input\/BIIB_data.csv\")","7c2dc2e4":"df.shape","03f051f7":"df.head()","7dbb9417":"df_stocks = df.iloc[:1000, 3]       #first 300 are to fit the model\ndf_holdout= df.iloc[1000: , 3]      #validation set\ndf_stocks.plot()","1ec0527c":"df_holdout.plot(color='green')","bdd281dd":"df_stocks_diff = df_stocks-df_stocks.shift()\nplt.figure(figsize=(20,8))\nplt.title('Differenced Time Series')\ndf_stocks_diff.plot(color='red')","1459c506":"df_stocks_diff.head()","78c57982":"#first value is NaN so replace it with original time series\ndf_stocks_diff.iloc[0] = df_stocks.iloc[0]","5a668a2d":"df_stocks_diff.head()","b74afd61":"from statsmodels.tsa.stattools import acf, pacf\nlag_acf = acf(df_stocks, nlags=10)    #for 10 lags\nlag_pacf = pacf(df_stocks, nlags=10, method='ols')   #for 10 lags","4907ea74":"plt.figure(figsize=(13,6))\nplt.subplot(121)\nplt.title('ACF')\nplt.plot(lag_acf, color='purple')\nplt.axhline(y=0, linestyle='--', color = 'red')\nplt.axhline(y=-1.96\/np.sqrt(len(df_stocks_diff)), linestyle='--', color='red')\nplt.axhline(y = 1.96\/np.sqrt(len(df_stocks_diff)), linestyle='--', color='red')\n\nplt.subplot(122)\nplt.title('PACF')\nplt.plot(lag_pacf, color='green')\nplt.axhline(y=0, linestyle='--', color='red')\nplt.axhline(y=-1.96\/np.sqrt(len(df_stocks_diff)), linestyle='--', color='red')\nplt.axhline(y = 1.96\/np.sqrt(len(df_stocks_diff)), linestyle='--', color='red')","a193db4b":"from statsmodels.tsa.arima_model import ARIMA\nmodel = ARIMA(df_stocks, order=(2,1,1))\nresults_AR = model.fit(disp=-1)","851fe328":"pred_AR = results_AR.predict(start=len(df_stocks_diff), end=len(df_stocks_diff) + 258)\npred_AR.iloc[0] = pred_AR.iloc[0] + df_stocks.iloc[998]\npred_AR = pred_AR.cumsum()\npred_AR.head()\n\npred_AR.index = df_holdout.index\n\nplt.plot(df_stocks, color='grey')\nplt.plot(df_holdout, color='green')\nplt.plot(pred_AR, color='red')","00ea7bd6":"error = mean_squared_error(df_holdout, pred_AR)\nprint(\"Current Error of our model is \", error)","498e4447":"df = pd.read_csv(\"\/kaggle\/input\/BIIB_data.csv\")\ndf.head()","e8a9d998":"from datetime import datetime","a2456e9a":"def components(x):\n    #function to extract date, month and year as a feature\n    date = datetime.strptime(x, '%m\/%d\/%Y')\n    return (date.day,date.month,date.year)\n\ndf['Day'] = df['date'].apply(lambda x:components(x)[0])\ndf['Month'] = df['date'].apply(lambda x:components(x)[1])\ndf['Year'] = df['date'].apply(lambda x:components(x)[2])","880e2983":"df.head()","467c4987":"#Prepare x and y variable\ny = df['close']\ndf = df.drop(['Name', 'date','close','open','volume','high', 'low'],1)","e6ea0591":"df.head()","3bf35f60":"df.shape","fa9b35f1":"from keras.models import Sequential\nfrom keras.layers import LSTM, Dense\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt","3180e138":"scale = MinMaxScaler()\ndf = scale.fit_transform(df)","809cc44d":"pd.DataFrame(df).head()","8f00d2bc":"df_train = df[:1100,:]\ny_train = np.array(y.iloc[:1100])\ndf_test = df[1100:,:]\ny_test = np.array(y.iloc[1100:])\n","96bb4804":"df_train.shape, df_test.shape","0f42e0e1":"model= Sequential()\nmodel.add(LSTM(250, input_shape=(df_train.shape[1],1)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse')","02f169e7":"df_train= df_train.reshape((df_train.shape[0], df_train.shape[1], 1))\ndf_test = df_test.reshape((df_test.shape[0], df_test.shape[1],1))","6ff7fdfb":"model.fit(df_train, y_train, epochs=400, shuffle=False)","0865f083":"pred_lstm = model.predict(df_test)\ntrain_val = pd.DataFrame(y_train, index = range(1100))\nholdout_predictions = pd.DataFrame(pred_lstm, index = range(1100, 1100+len(pred_lstm)))\nholdout_val = pd.DataFrame(y_test, index = range(1100, 1100+len(pred_lstm)))","ff6e675e":"plt.title('LSTM Model')\nplt.plot(train_val, color='grey')\nplt.plot(holdout_val, color ='green')\nplt.plot(holdout_predictions, color='red')","8b06aaa0":"error = mean_squared_error(holdout_val, pred_lstm)\nprint(error)","e228838e":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV","79b53286":"param_dict = {'n_estimators':[1,5,10]}\nrf = RandomForestRegressor()\ncv = GridSearchCV(rf, param_grid=param_dict, scoring='neg_mean_squared_error')\ndf_train = df[:1100, :]\ny_train = np.array(y.iloc[:1100])\ndf_test = df[1100:, :]\ny_test = np.array(y.iloc[1100:])","316fe99d":"cv.fit(df_train, y_train)","b7d0bd55":"cv.best_score_","2d5f0ca0":"pred_rf = cv.best_estimator_.predict(df_test)\ntrain_val = pd.DataFrame(y_train, index = range(1100))\nholdout_pred = pd.DataFrame(pred_rf, index=range(1100, 1100+len(pred_rf)))\nholdout_values = pd.DataFrame(y_test, index=range(1100,1100+len(pred_rf)))","928bd680":"plt.plot(train_val, color='grey')\nplt.plot(holdout_val, color='green')\nplt.plot(holdout_pred, color='red')","17a65517":"error = mean_squared_error(y_test, pred_rf)\nprint(error)","76733105":"### Plot differenced time series","e5c0a8ec":"# Fitting ARIMA","668040f4":"this looks like a perfect data to modelling. It has few outliers but they can be ignore.","5180cc70":"# Machine Learnign And Deep Learning\n\n#### Now lets make a model using deep learning and machine learning and lets check the error of this \n\nTHe model which gives the better accuracry will be considered as the best one","cefd1ac6":"fir our ARIMA(p=1,d=1,q=0) model. ","0367206b":"### Plot","576c7812":"# Lets use Random Forest ALGO ","677536ec":"# Lets introduce ACF (Autocorrelation plot) and PACF(Partial autocorrelation plot):\n![image.png](attachment:image.png)\nWhere X\u2019 is the mean, k is the lag (number of time steps we look back. k=1 means t-1), N is the total number of points. C is the autocovariance. For a stationary time series, c does not depend on t. It only depends on k or the lag.","20eb42fd":"### REsult","15d98ebf":"### Plot this","4ab499bb":"* Grey --> data on which model is fitted\n* Green --> holdout\/validation data\n* Red --> our model's performance","2f8eed2b":"# DATA \nThe data is actually about the price of wheat","7fed63ad":"### FOR ACF\nFrom lag 0 it decays gradually. The top and bottom grey dashed lines show the significance level. Between these levels, the autocorrelation is attributed to noise and not some actual relation. Therefore q=0\n\n### FOR PACF\nIt cuts off quickly, unlike the gradual decay in ACF. Lag 1 is significantly high, indicating the presence of 1 AR term. Therefore p=1."}}