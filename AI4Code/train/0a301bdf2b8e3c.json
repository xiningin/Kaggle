{"cell_type":{"b5edacba":"code","baad6f57":"code","62fbbd78":"code","06169f71":"code","42ee4aa8":"code","0a27861b":"code","08c23891":"code","33e2bc93":"code","5ac4ece2":"code","0061f941":"code","601c5cab":"code","87e2c403":"code","f675e88e":"code","e8fc6f67":"code","694d86f6":"code","0e9791aa":"code","c4593eb0":"code","e94a2efa":"code","bb8acc84":"code","845ac254":"code","0dad3b60":"code","c7b9146a":"code","6132e13f":"code","f08790a0":"code","0e0d3eb0":"code","9b822cc2":"code","a9fa0d17":"code","d9fbca08":"code","a11270f3":"code","2d591ace":"code","e87fd578":"code","9112a995":"code","593c1943":"code","94ca1bf2":"code","7bc14aa1":"code","4d7e837a":"code","0bf90f74":"code","8b2b2e74":"code","86eab083":"code","8668af00":"code","f8ab9f4a":"code","03204447":"code","ead0b490":"code","7dca5761":"code","76ceba58":"code","410314cb":"code","6b608f5b":"code","4cb1ae64":"code","4ba9f918":"code","48b6d8c2":"code","bf629483":"code","60756812":"code","ddbe6884":"code","ccd5ffa0":"code","449f9171":"code","fefe76ea":"code","3e2b87df":"code","108f0f75":"code","45057007":"code","73aebc07":"code","7bb6b280":"code","d1639b35":"code","057ffdff":"code","e93e679c":"markdown","7955bb79":"markdown","b961e58b":"markdown","51361eee":"markdown","e65145f6":"markdown","a1d02a4f":"markdown","ba568162":"markdown","5c187278":"markdown","716589ba":"markdown","5a7aa8a2":"markdown"},"source":{"b5edacba":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport time\n%matplotlib inline","baad6f57":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))","62fbbd78":"matplotlib.rcParams['figure.figsize'] = (15.0, 8.0)","06169f71":"def load_atp_data(filename='ATP') :\n    if filename != 'ATP' :\n        if str(filename)[-1] == '0' :\n            filename = f'atp_rankings_{filename[-2:]}s.csv'\n        else :\n            filename = f'atp_rankings_current.csv'\n    data_folder = \"..\/input\/atpdata\/\"\n    data = pd.read_csv(data_folder + filename + \".csv\", parse_dates=['tourney_date'])\n    return data\n\ndef clean_atp_data(data) :\n    data_clean = data.copy()\n    data_clean.score = data_clean.score.apply(lambda x: x if str(x)[0].isnumeric() else np.nan)\n    data_clean = data_clean.dropna(axis=0, subset=[\"score\"])\n    data_clean = data_clean[data_clean[\"score\"] != 'W\/O']\n    data_clean = data_clean[data_clean[\"score\"] != ' W\/O']\n    data_clean = data_clean[data_clean[\"score\"] != 'DEF']\n    data_clean = data_clean[data_clean[\"score\"] != 'In Progress']\n    data_clean = data_clean[data_clean[\"score\"] != 'Walkover']\n    \n    data_clean[\"surface\"] = data_clean[\"surface\"].replace('None', np.nan)\n    data_clean['match_id'] = data_clean.apply(lambda x: str(x['tourney_date']) + \n                                              str(x['match_num']).zfill(3) + \n                                              str(x['tourney_id']).split('-')[-1], axis=1)\n    data_clean = data_clean.drop_duplicates(['tourney_id', 'match_num'])\n    \n    return data_clean","42ee4aa8":"def scores_to_list(scores) :\n    scores = scores.split()\n    scores_list = []\n    for i, score in enumerate(scores) :\n#         if score == 'RET' :\n#             score == '0-0'\n        score_list = score.split('-')\n        score_list = [s.split('(')[0] for s in score_list]\n        score_list = [int(s) if s.isnumeric() else 0 for s in score_list]\n        if max(score_list) > 7 :\n            if not ((i + 1) == len(scores) and bool(len(scores) % 2)) :\n                if score_list[0] > score_list[1] :\n                    score_list = [7,6]\n                else :\n                    score_list = [6,7]\n        scores_list.append(score_list)\n    return scores_list","0a27861b":"data = load_atp_data()\ndata = clean_atp_data(data)\ndata['score'] = data['score'].apply(scores_to_list)","08c23891":"data.describe()","33e2bc93":"data.dtypes","5ac4ece2":"data","0061f941":"data.score[:20]","601c5cab":"g = sns.barplot(data.columns, data.isna().sum(axis=0))\ng.set_xticklabels(data.columns, rotation=80)\nplt.show()","87e2c403":"# def get_player_hist(row, data, i=0) :\n#     try :\n#         player_id = row['id']\n#     except :\n#         player_id = row['id2']\n#     match_id = row['match_id']\n#     if match_id[8:11] == '069' :\n#         print(match_id)\n# #     print(match_id[8:11])\n# #     match_date = row['match_date']\n#     sel = data[data['id'] == player_id]\n# #     data.sort_values('match_id')\n#     past_matches = sel[sel['match_id'] < match_id]\n#     return past_matches.sort_values('match_date', ascending=True).values\n\ndef select_features(data) :\n    x = pd.DataFrame()\n    x['surface'] = data['surface'].astype('category')\n    x['match_id'] = data['match_id'].astype('str')\n    x['best_of'] = data['best_of'].astype('category')\n    x['round'] = data['round'].astype('category')\n    x['tourney_level'] = data['tourney_level'].astype('category')\n    x['season'] = data['tourney_date'].dt.strftime('%j').astype('float')\n    d = data[['tourney_date','tourney_id','match_num']].copy()\n    d['match_num_min'] = d[['tourney_id','match_num']].groupby('tourney_id', sort=False).transform(min)\n    d['match_num_max'] = d[['tourney_id','match_num']].groupby('tourney_id', sort=False).transform(max)\n    match_num_scale = lambda x: max(x['match_num'] - x['match_num_min'], 1) \/ max(x['match_num_max'] - x['match_num_min'], 1)\n    x['match_num_norm'] = d.apply(match_num_scale, axis=1)\n    x['match_date'] = d.apply(lambda x: x['tourney_date'] + pd.DateOffset(days=x['match_num'] - x['match_num_min']), axis=1)\n    \n    x2 = x.copy()\n    max_rank = data['loser_rank'].max()\n    \n    x['p1_id'] = data['winner_id'].astype('int')\n    x2['p1_id'] = data['loser_id'].astype('int')\n    x['p1_rank'] = data['winner_rank'].fillna( max_rank ).astype('float')\n    x2['p1_rank'] = data['loser_rank'].fillna( max_rank ).astype('float')\n    x['p1_age']  = data['winner_age'].astype('float')\n    x2['p1_age']  = data['loser_age'].astype('float')\n    x['p1_ht']   = data['winner_ht'].astype('float')\n    x2['p1_ht']   = data['loser_ht'].astype('float')\n    x['p1_hand'] = data['winner_hand'].fillna('U').astype('category')\n    x2['p1_hand'] = data['loser_hand'].fillna('U').astype('category')\n    ioc = pd.concat((data.winner_ioc, data.loser_ioc))\n    to_remove = ioc.value_counts().iloc[20:].index.tolist()\n    x['p1_ioc'] = data['winner_ioc'].replace(to_remove, 'other').astype('category')\n    x2['p1_ioc'] = data['loser_ioc'].replace(to_remove, 'other').astype('category')\n    \n    \n    x2['p2_id'] = data['winner_id'].astype('int')\n    x['p2_id'] = data['loser_id'].astype('int')\n    x2['p2_rank'] = data['winner_rank'].fillna( max_rank ).astype('float')\n    x['p2_rank'] = data['loser_rank'].fillna( max_rank ).astype('float')\n    x2['p2_age']  = data['winner_age'].astype('float')\n    x['p2_age']  = data['loser_age'].astype('float')\n    x2['p2_ht']   = data['winner_ht'].astype('float')\n    x['p2_ht']   = data['loser_ht'].astype('float')\n    x2['p2_hand'] = data['winner_hand'].fillna('U').astype('category')\n    x['p2_hand'] = data['loser_hand'].fillna('U').astype('category')\n    ioc = pd.concat((data.winner_ioc, data.loser_ioc))\n    to_remove = ioc.value_counts().iloc[20:].index.tolist()\n    x2['p2_ioc'] = data['winner_ioc'].replace(to_remove, 'other').astype('category')\n    x['p2_ioc'] = data['loser_ioc'].replace(to_remove, 'other').astype('category')\n    \n    x['label'] = 1\n    x2['label'] = 0\n    x['label'] = x['label'].astype('uint8')\n    x2['label'] = x2['label'].astype('uint8')\n\n    \n    x_duplicated = pd.concat([x,x2]).sort_index()\n    x_duplicated = x_duplicated.drop(['p1_ioc','p2_ioc','p1_hand','p2_hand'], axis=1)\n    \n    idx = np.random.randint(2, size=len(x)).astype('bool')\n    x = x[idx]\n    x2 = x2[~idx]\n\n    x = pd.concat([x,x2]).sort_index()\n    \n    return x, x['label'].values, x_duplicated","f675e88e":"# d = data.iloc[:500]\nd = data.copy()\nX, y, X_dup = select_features(d)\ncols = X.columns","e8fc6f67":"sns.barplot(X.columns, X.isna().sum(axis=0))\nplt.show()","694d86f6":"def plot_feat(data, col, kind='bar', bins=10) :\n    if kind == 'hist' :\n        x = data[[col, 'label']].copy()\n        x[col] = pd.cut(data[col], bins)\n        x.groupby([col, 'label']).size().reset_index() \\\n        .pivot(columns='label', index=col, values=0).fillna(0)\\\n        .plot(kind='bar', stacked=True)\n    else :\n            data[[col, 'label']].groupby([col, 'label']).size().reset_index() \\\n            .pivot(columns='label', index=col, values=0).plot(kind=kind, stacked=True)\n    plt.show()","0e9791aa":"plot_feat(X,'p1_ht')","c4593eb0":"plot_feat(X,'p1_ioc')","e94a2efa":"plot_feat(X,'p1_age', 'hist', 15)","bb8acc84":"plot_feat(X,'p1_hand')","845ac254":"plot_feat(X,'match_num_norm', 'hist', 10)","0dad3b60":"sns.lineplot(list(range(len(X.match_num_norm.unique()))),np.sort(X.match_num_norm.unique()))\nplt.show()","c7b9146a":"idx = 5000\nsns.boxplot(data['tourney_id'].iloc[-idx:], X['match_num_norm'].iloc[-idx:])\nplt.show()","6132e13f":"X.head()","f08790a0":"X.tail()","0e0d3eb0":"sns.scatterplot(X['match_num_norm'], X['p1_age'], hue=X['label'])\nplt.show()","9b822cc2":"X.columns","a9fa0d17":"data.columns","d9fbca08":"cor = X.corr()\nsns.heatmap(cor, annot=True, cmap=plt.cm.Reds)\nplt.show()","a11270f3":"cor_target = abs(cor[\"label\"])\nrelevant_features = cor_target[cor_target>0.01]\nrelevant_features","2d591ace":"from sklearn import preprocessing","e87fd578":"min_max_scaler = preprocessing.MinMaxScaler()\nmin_max_scaler_dup = preprocessing.MinMaxScaler()\n\ncols = X.select_dtypes('float').columns\nx_scaled = min_max_scaler.fit_transform(X[cols])\nX[cols] = x_scaled\n\ncols_dup = X_dup.select_dtypes('float').columns\ncols_dup =  list(set(cols_dup) - set(['match_id','p1_id','p2_id','match_date']))\nx_scaled_dup = min_max_scaler_dup.fit_transform(X_dup[cols_dup])\nX_dup[cols_dup] = x_scaled_dup","9112a995":"X = pd.get_dummies(X, columns=X.select_dtypes('category').columns)\nX = X.fillna(0)\n\nX_dup = pd.get_dummies(X_dup, columns=X_dup.select_dtypes('category').columns)\nX_dup = X_dup.fillna(0)","593c1943":"X_dup = X_dup.drop(['match_id','p2_id'], axis=1)","94ca1bf2":"test_size = .1\ntrain_size = .8\n\ntotal_len = X.shape[0]\nX_split, X_test = X[:-int(total_len*test_size)], X[-int(total_len*test_size):]\n\ntotal_len = X_split.shape[0]\nidx_random = np.random.permutation(total_len)\nidx_cut = int(total_len * train_size)\nidx_train, idx_valid = idx_random[:idx_cut], idx_random[idx_cut:]\nX_train, X_valid = X_split.iloc[idx_train,:], X_split.iloc[idx_valid,:]\n\nlen_train = len(X_train)","7bc14aa1":"# from sklearn.model_selection import train_test_split","4d7e837a":"from sklearn.linear_model import LassoCV","0bf90f74":"reg = LassoCV(random_state=42)\nx = X.drop(['match_id','match_date','label'], axis=1)\nreg.fit(x, y)\nprint(\"Best alpha using built-in LassoCV: %f\" % reg.alpha_)\nprint(\"Best score using built-in LassoCV: %f\" %reg.score(x,y))\ncoef = pd.Series(reg.coef_, index = x.columns)","8b2b2e74":"print(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","86eab083":"imp_coef = coef.sort_values()\nimp_coef = imp_coef[abs(imp_coef) > 0]\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Feature importance using Lasso Model\")","8668af00":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nclf = ExtraTreesClassifier(n_estimators=50)\nclf = clf.fit(x, y)\nclf.feature_importances_  \n\nm = SelectFromModel(clf, prefit=True)\nx_new = m.transform(x)\nx_new.shape","f8ab9f4a":"g = sns.barplot(list(range(len(clf.feature_importances_))), clf.feature_importances_)\ng = g.set_xticklabels(x.columns, rotation=80)","03204447":"from tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\n# from tensorflow.keras.layers import RepeatVector\n# from tensorflow.keras.layers import TimeDistributed\n# from tensorflow.keras.layers import Lambda\n# from tensorflow.keras.layers import InputLayer\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Masking\nfrom tensorflow.keras.utils import plot_model\n# from tensorflow.keras.optimizers import SGD, RMSprop\nfrom tensorflow.keras.callbacks import Callback\nimport datetime","ead0b490":"n_features = X_dup.shape[-1] - 2\n# generator will drop ['match_date','p1_id']\nn_match_features = X.shape[-1] - 5 \n# generator will drop ['match_id','match_date','p1_id','p2_id','label']","7dca5761":"class My_Callback(Callback):\n    def __init__(self,batch_interval, validation_data):\n        metrics = ['loss', 'val_loss', 'acc', 'val_acc']\n        self.metrics = metrics\n        self.batch_interval = batch_interval\n        self.loss = []\n        self.val_loss = []\n        self.acc = []\n        self.val_acc = []\n        self.test_data = validation_data\n\n    def on_train_begin(self,log={}):\n        self.loss = []\n        self.val_loss = []\n        self.acc = []\n        self.val_acc = []\n        self.batch_number = 0\n        self.epoch_number = 0\n\n    def on_epoch_begin(self, epoch, logs={}):\n        return\n\n    def on_batch_end(self,batch,logs={}):\n        self.batch_number +=1\n        \n        if self.batch_number % self.batch_interval == 0:\n            self.epoch_number +=1\n            self.loss.append(logs['loss'])\n            self.acc.append(logs['accuracy'])\n        return\n    \n    def on_epoch_end(self,batch,logs={}):\n        loss, acc = self.model.evaluate_generator(self.test_data, steps=int(1000*.33), verbose=0)\n        self.loss.append(loss)\n        self.val_acc.append(acc)\n        return","76ceba58":"def create_model(n_feat, n_match_feat) :\n    K.clear_session()\n\n    input_p1 = Input(shape=(None, n_feat))\n    input_p2 = Input(shape=(None, n_feat))\n    input_match = Input(shape=([n_match_feat]))\n    \n    masking = Masking(mask_value=0.0)\n    encoder = LSTM(32, activation='sigmoid', return_sequences=False)\n\n    hidden_p1 = Dropout(.4)(encoder(masking(input_p1)))\n    hidden_p2 = Dropout(.4)(encoder(masking(input_p2)))\n    \n    merge_layer = Concatenate()([input_match, hidden_p1, hidden_p2])\n    hidden_merged = Dense(32, activation='relu')(merge_layer)\n    predictions = Dense(1, activation='sigmoid')(hidden_merged)\n    \n    model = Model(inputs=[input_match, input_p1, input_p2], outputs=predictions)\n    \n    model.compile(loss='binary_crossentropy',\n                  optimizer='rmsprop',\n                  metrics=['accuracy'])\n    model.summary()\n    return model","410314cb":"model = create_model(n_features, n_match_features)","6b608f5b":"batch_size = 64\nt_delta = datetime.timedelta(days=600)\n\ndef train_generator(x_matches, x_players, n_feat, n_match_feat, batch_size=1):\n    p1_input_batch = []\n    p1_mask_batch = []\n    p2_input_batch = []\n    p2_mask_batch = []\n    p1_seq_len_batch = []\n    p2_seq_len_batch = []\n    match_input_batch = []\n    label_batch = []\n    count_batch_i = 0\n    p1_max_len = 0\n    p2_max_len = 0\n    while True :\n        for i, x in x_matches.iterrows() :\n            count_batch_i += 1\n            p1_id = x['p1_id']\n            p2_id = x['p2_id']\n            match_date = x['match_date']\n            p1_input = x_players[x_players['p1_id'] == p1_id]\n            p1_input = p1_input[(p1_input['match_date'] < match_date) & \n                            (p1_input['match_date'] > match_date - t_delta)] \\\n                            .drop(['match_date','p1_id'], axis=1).values.tolist()\n            if p1_input == [] :\n                p1_input = [[0]*n_feat]\n            p1_seq_len = len(p1_input)\n            p1_max_len = max(p1_max_len, p1_seq_len)\n            p1_input_batch.append(p1_input)\n            p1_seq_len_batch.append(p1_seq_len)\n            \n            p2_input = x_players[x_players['p1_id'] == p2_id]\n            p2_input = p2_input[(p2_input['match_date'] < match_date) & \n                            (p2_input['match_date'] > match_date - t_delta)] \\\n                            .drop(['match_date','p1_id'], axis=1).values.tolist()\n            if p2_input == [] :\n                p2_input = [[0]*n_feat]\n            p2_seq_len = len(p2_input)\n            p2_max_len = max(p2_max_len, p2_seq_len)\n            p2_input_batch.append(p2_input)\n            p2_seq_len_batch.append(p2_seq_len)\n            \n            label = x['label']\n            match_input = x.drop(['match_id','match_date','p1_id','p2_id','label'])\n            match_input_batch.append(match_input.values)\n            label_batch.append(label)\n            \n            if count_batch_i == batch_size :\n                match_input_batch = np.array(match_input_batch).reshape(-1,n_match_feat)\n#                 p1_mask_batch = np.zeros((batch_size, p1_max_len))\n#                 p2_mask_batch = np.zeros((batch_size, p2_max_len))\n                for s_idx, s_len in enumerate(p1_seq_len_batch) :\n#                     p1_mask_batch[s_idx,:s_len] = True\n                    p1_input_batch[s_idx] = p1_input_batch[s_idx] + [[0]*n_feat] * (p1_max_len - s_len)\n                for s_idx, s_len in enumerate(p2_seq_len_batch) :\n#                     p2_mask_batch[s_idx,:s_len] = True\n                    p2_input_batch[s_idx] = p2_input_batch[s_idx] + [[0]*n_feat] * (p2_max_len - s_len)\n                input_batch = [match_input_batch, \n                               np.array(p1_input_batch).reshape(batch_size,-1,n_feat), \n#                                np.array(p1_mask_batch ).reshape(batch_size,-1,1), \n                               np.array(p2_input_batch).reshape(batch_size,-1,n_feat)\n#                                np.array(p2_mask_batch ).reshape(batch_size,-1,1)\n                              ]\n                return_labels = np.array(label_batch).reshape(batch_size,1)\n    \n                p1_input_batch = []\n                p1_mask_batch = []\n                p2_input_batch = []\n                p2_mask_batch = []\n                match_input_batch = []\n                label_batch = []\n                count_batch_i = 0\n                p1_max_len = 0\n                p2_max_len = 0\n                p1_seq_len_batch = []\n                p2_seq_len_batch = []\n                \n                yield input_batch, return_labels","4cb1ae64":"# history = My_Callback(batch_interval=10, validation_data=train_generator(X_valid)) # 100*len(X_train)\/16\nhistory = model.fit_generator(train_generator(X_train, X_dup, n_features, n_match_features, batch_size), \n                              steps_per_epoch=int(len(X_train)\/batch_size), \n                              validation_data=train_generator(X_valid, X_dup, n_features, n_match_features, batch_size), \n                              validation_steps=int(len(X_valid)\/batch_size), \n                              epochs=2, verbose=1)  # , callbacks=[history]","4ba9f918":"for key in history.history:\n    plt.plot(history.history[key], label=key)\n    plt.legend()\nplt.show()","48b6d8c2":"test_score = model.evaluate_generator(train_generator(X_test, X_dup, n_features, n_match_features, batch_size), \n                                      steps=len(X_test), verbose=1)\ntest_score","bf629483":"def create_model_dense() :\n    K.clear_session()\n    \n    model = Sequential()\n    model.add(Dense(32, input_dim=n_match_features, activation='relu'))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    model.summary()\n    return model\nmodel_dense = create_model_dense()","60756812":"y_train_dense = X_train['label'].values.reshape(-1,1)\ny_valid_dense = X_valid['label'].values.reshape(-1,1)\ny_test_dense = X_test['label'].values.reshape(-1,1)\nX_train_dense = X_train.drop(['match_id','match_date','p1_id','p2_id','label'], axis=1).values.reshape(-1,n_match_features)\nX_valid_dense = X_valid.drop(['match_id','match_date','p1_id','p2_id','label'], axis=1).values.reshape(-1,n_match_features)\nX_test_dense = X_test.drop(['match_id','match_date','p1_id','p2_id','label'], axis=1).values.reshape(-1,n_match_features)\nhistory_dense = model_dense.fit(X_train_dense, y_train_dense, batch_size=batch_size,\n                                validation_data=[X_valid_dense, y_valid_dense], \n                                epochs=4, verbose=1)","ddbe6884":"for key in history_dense.history:\n    plt.plot(history_dense.history[key], label=key)\n    plt.legend()\nplt.show()","ccd5ffa0":"test_score_dense = model_dense.evaluate(X_test_dense, y_test_dense, batch_size=batch_size, verbose=1)\ntest_score_dense","449f9171":"d = data[data['tourney_date'] < '1991-01-01'].copy()\ng = sns.barplot(d.columns, d.isna().sum(axis=0))\ng.set_xticklabels(d.columns, rotation=90)\nprint(f'NA count prior to 1991 : {d[\"l_1stIn\"].isna().sum()}')\nplt.show()","fefe76ea":"d = data[data['tourney_date'] > '1991-01-01'].copy()\ng = sns.barplot(d.columns, d.isna().sum(axis=0))\ng.set_xticklabels(d.columns, rotation=90)\nprint(f'NA count after 1991 : {d[\"l_1stIn\"].isna().sum()}')\nplt.show()","3e2b87df":"d = data[['tourney_date','l_1stIn']].copy()\nd['l_1stIn'] = d['l_1stIn'].isna()\nd['tourney_date'] = pd.cut(d['tourney_date'],60)\nd = d.groupby(['tourney_date']).sum()\ng = sns.barplot(d.index.values.astype('str'), d.l_1stIn.values)\ng.set_xticklabels(d.index.values.astype('str'), rotation=90)\nplt.show()","108f0f75":"def select_all_features(data) :\n    x = pd.DataFrame()\n    x['surface'] = data['surface'].astype('category')\n    x['match_id'] = data['match_id'].astype('str')\n    x['best_of'] = data['best_of'].astype('category')\n    x['round'] = data['round'].astype('category')\n    x['tourney_level'] = data['tourney_level'].astype('category')\n    d = data[['tourney_date','tourney_id','match_num']].copy()\n    date = pd.to_datetime(data['tourney_date'], format='%Y%m%d').copy()\n    x['season'] = date.dt.strftime('%j').astype('float')    \n    d['tourney_date'] = date\n    d['match_num_min'] = d[['tourney_id','match_num']].groupby('tourney_id', sort=False).transform(min)\n    d['match_num_max'] = d[['tourney_id','match_num']].groupby('tourney_id', sort=False).transform(max)\n    match_num_scale = lambda x: max(x['match_num'] - x['match_num_min'], 1) \/ max(x['match_num_max'] - x['match_num_min'], 1)\n    x['match_num_norm'] = d.apply(match_num_scale, axis=1)\n    x['match_date'] = d.apply(lambda x: x['tourney_date'] + pd.DateOffset(days=x['match_num'] - x['match_num_min']), axis=1)\n    \n    x2 = x.copy()\n    max_rank = data['loser_rank'].max()\n\n    x['p1_id'] = data['winner_id'].astype('int')\n    x2['p1_id'] = data['loser_id'].astype('int')\n    x['p1_rank'] = data['winner_rank'].fillna(max_rank).astype('float')\n    x2['p1_rank'] = data['loser_rank'].fillna(max_rank).astype('float')\n    x['p1_age']  = data['winner_age'].astype('float')\n    x2['p1_age']  = data['loser_age'].astype('float')\n    x['p1_ht']   = data['winner_ht'].astype('float')\n    x2['p1_ht']   = data['loser_ht'].astype('float')\n    x['p1_hand'] = data['winner_hand'].fillna('U').astype('category')\n    x2['p1_hand'] = data['loser_hand'].fillna('U').astype('category')\n    ioc = pd.concat((data.winner_ioc, data.loser_ioc))\n    to_remove = ioc.value_counts().iloc[20:].index.tolist()\n    x['p1_ioc'] = data['winner_ioc'].replace(to_remove, 'other').astype('category')\n    x2['p1_ioc'] = data['loser_ioc'].replace(to_remove, 'other').astype('category')\n    \n    x2['p2_id'] = data['winner_id'].astype('int')\n    x['p2_id'] = data['loser_id'].astype('int')\n    x2['p2_rank'] = data['winner_rank'].fillna(max_rank).astype('float')\n    x['p2_rank'] = data['loser_rank'].fillna(max_rank).astype('float')\n    x2['p2_age']  = data['winner_age'].astype('float')\n    x['p2_age']  = data['loser_age'].astype('float')\n    x2['p2_ht']   = data['winner_ht'].astype('float')\n    x['p2_ht']   = data['loser_ht'].astype('float')\n    x2['p2_hand'] = data['winner_hand'].fillna('U').astype('category')\n    x['p2_hand'] = data['loser_hand'].fillna('U').astype('category')\n    ioc = pd.concat((data.winner_ioc, data.loser_ioc))\n    to_remove = ioc.value_counts().iloc[20:].index.tolist()\n    x2['p2_ioc'] = data['winner_ioc'].replace(to_remove, 'other').astype('category')\n    x['p2_ioc'] = data['loser_ioc'].replace(to_remove, 'other').astype('category')\n    \n    cols_to_add = ['_1stIn', '_1stWon', '_2ndWon', '_SvGms', '_ace', '_bpFaced', '_bpSaved', '_df', '_svpt']\n    for c in cols_to_add :\n        x['p1'+c] = data['w'+c].astype('float')\n        x2['p1'+c] = data['l'+c].astype('float')\n        x2['p2'+c] = data['w'+c].astype('float')\n        x['p2'+c] = data['l'+c].astype('float')\n    \n    x['label'] = 1\n    x2['label'] = 0\n    x['label'] = x['label'].astype('uint8')\n    x2['label'] = x2['label'].astype('uint8')\n\n    \n    x_duplicated = pd.concat([x,x2]).sort_index()\n    x_duplicated = x_duplicated.drop(['p1_ioc','p2_ioc','p1_hand','p2_hand'], axis=1)\n    \n    idx = np.random.randint(2, size=len(x)).astype('bool')\n    x = x[idx]\n    x2 = x2[~idx]\n    \n\n    x = pd.concat([x,x2]).sort_index()\n    drop_cols = [p+c for c in cols_to_add for p in ['p1','p2']]\n    x = x.drop(drop_cols, axis=1)\n    \n    return x, x['label'].values, x_duplicated","45057007":"d = data[data['tourney_date'] > datetime.datetime(1991,1,1)].copy()\nX_1991, y_1991, X_dup_1991 = select_all_features(d)\ncols_1991 = X_1991.columns","73aebc07":"min_max_scaler_1991 = preprocessing.MinMaxScaler()\nmin_max_scaler_dup_1991 = preprocessing.MinMaxScaler()\n\ncols = X_1991.select_dtypes('float').columns\nx_scaled_1991 = min_max_scaler_1991.fit_transform(X_1991[cols])\nX_1991[cols] = x_scaled_1991\n\ncols_dup = X_dup_1991.select_dtypes('float').columns\ncols_dup =  list(set(cols_dup) - set(['match_id','p1_id','p2_id','match_date']))\nx_scaled_dup_1991 = min_max_scaler_dup_1991.fit_transform(X_dup_1991[cols_dup])\nX_dup_1991[cols_dup] = x_scaled_dup_1991\n\nX_1991 = pd.get_dummies(X_1991, columns=X_1991.select_dtypes('category').columns)\nX_1991 = X_1991.fillna(0)\n\nX_dup_1991 = pd.get_dummies(X_dup_1991, columns=X_dup_1991.select_dtypes('category').columns)\nX_dup_1991 = X_dup_1991.fillna(0)\n\nX_dup_1991 = X_dup_1991.drop(['match_id','p2_id'], axis=1)\n\ntest_size = .1\ntrain_size = .8\ntotal_len = X_1991.shape[0]\nX_split_1991, X_test_1991 = X_1991[:-int(total_len*test_size)], X_1991[-int(total_len*test_size):]\ntotal_len = X_split_1991.shape[0]\nidx_random = np.random.permutation(total_len)\nidx_cut = int(total_len * train_size)\nidx_train, idx_valid = idx_random[:idx_cut], idx_random[idx_cut:]\nX_train_1991, X_valid_1991 = X_split_1991.iloc[idx_train,:], X_split_1991.iloc[idx_valid,:]","7bb6b280":"n_features_1991 = X_dup_1991.shape[-1] - 2\nn_match_features_1991 = X_1991.shape[-1] - 5\n\nmodel_1991 = create_model(n_features_1991, n_match_features_1991)\n\nhistory_1991 = model_1991.fit_generator(train_generator(X_train_1991, X_dup_1991, n_features_1991, n_match_features_1991, batch_size), \n                                        steps_per_epoch=int(len(X_train_1991)\/batch_size), \n                                        validation_steps=int(len(X_valid_1991)\/batch_size), \n                                        validation_data=train_generator(X_valid_1991, X_dup_1991, \n                                                                        n_features_1991, n_match_features_1991, batch_size), \n                                        epochs=2, verbose=1)  # , callbacks=[history]","d1639b35":"for key in history_1991.history:\n    plt.plot(history_1991.history[key], label=key)\n    plt.legend()\nplt.show()","057ffdff":"test_score_1991 = model_1991.evaluate_generator(train_generator(X_test_1991, X_dup_1991, \n                                                                n_features_1991, n_match_features_1991, batch_size), \n                                                steps=len(X_test_1991), verbose=1)\ntest_score_1991","e93e679c":"# Feature selection\n\n- 1st run. I selected the columns with less then ~ 25% null values.\n\nI've created 2 features :\n - match_num_norm - Mesure how far in the tournement the match occur.\n - season - A player could perform better durring summer or winter.","7955bb79":"# Checking again the correlation","b961e58b":"# LSTM Model","51361eee":"# Some data exploration","e65145f6":"# Tennis Match Betting Prediction\n\nI'm trying to predict the winner of a match based on the current and past matches' information.\n\nI'm using a LSTM layer to encode each player's past matches and results, concatenate with the features of the current match, then pass through a fully connected layer.\n\nI've found the data a bit tricky to handle. The data is complex, without strong correlations and in 2 different formats.\n\nAt first I had lot of dataleak, giving me 99% accuracy. So I selected only a few columns with less NAs and did some normalisation.\n\nAnother issue I had was how to handle the past data of each player. It tourned out to be quite heavy.   \nI tried to process some of it on the fly with a generator, but that slowed even more the training.\n\nI've managed to get around 70% * accuracy after pading and masking the input. So I think I'm getting somewhere.\n\nI selected all features from only the recent rows, but not much improvement.   \nFine-tunning the model and the hyperparameters might improve the results.\n\n** accuracy varied between 66 and 72 % trying different hyperparameters.*","a1d02a4f":"# Normalize data","ba568162":"# Simple dense model without past matches' info","5c187278":"# Import and clean data","716589ba":"# Select features\n- Now instead I'll try using the most recent data that has more information about each match.\n\nI've only used the extra features to encode each player's past data, since those features (first serve, ace, etc..) are not known prior to the match.","5a7aa8a2":"# 2nd LSTM model\nSame model architecture as before, but different input shape."}}