{"cell_type":{"1007ff8a":"code","6532318c":"code","7fae7342":"code","cd831845":"code","f91293f3":"code","0d5e5904":"code","458f3669":"code","4102976d":"code","19a25b8f":"code","2605e944":"code","7e1d5429":"code","af0276f7":"code","9aa71ce1":"code","32cdc5d7":"code","87cb141f":"code","27e6ca12":"code","aefd9721":"code","20c4fe25":"code","aa1f030e":"code","e68503cd":"code","a6093a4a":"code","181d8766":"code","4ac844e3":"code","ab7417b4":"code","68539484":"markdown","1b025efe":"markdown","d042b888":"markdown","22fc50d0":"markdown","1c255631":"markdown","1dc41062":"markdown","2599bb0e":"markdown","953ae793":"markdown","88dc35f3":"markdown","af0fcdcf":"markdown","7407da3d":"markdown","7ae593b5":"markdown","d2a35bc1":"markdown"},"source":{"1007ff8a":"import gc\nimport glob\nimport os\nimport time\n\nimport joblib\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom kaggle.competitions import twosigmanews\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","6532318c":"pd.set_option(\"display.max_columns\", 96)\npd.set_option(\"display.max_rows\", 96)\n\nplt.rcParams['figure.figsize'] = (12, 9)","7fae7342":"env = twosigmanews.make_env()\ntrain_market, train_news = env.get_training_data()\ndays = env.get_prediction_days()","cd831845":"debug = True\nn_debug_samples = 100000","f91293f3":"train_market['time'] = pd.to_datetime(train_market['time'])\ntrain_news['time'] = pd.to_datetime(train_news['time'])\ntrain_news['sourceTimestamp'] = pd.to_datetime(train_news['sourceTimestamp'])\ntrain_news['firstCreated'] = pd.to_datetime(train_news['firstCreated'])\n\ntrain_market = train_market.sort_values('time')\ntrain_news = train_news.sort_values('time')\n\nif debug:\n    print('Debugging.')\n    train_market = train_market.iloc[:n_debug_samples, :]\n    train_news = train_news.iloc[:n_debug_samples, :]","0d5e5904":"train_news = train_news.drop(['headline', 'subjects', 'audiences'], axis=1)","458f3669":"print('Market NaN structure:\\n{}'.format(np.sum(pd.isnull(train_market))))\nprint('\\nNews NaN structure:\\n{}'.format(np.sum(pd.isnull(train_news))))","4102976d":"print(train_news.headlineTag.value_counts())\n\ntrain_news.headlineTag = train_news.headlineTag.replace('', 'NoInfo')\n\nprint('\\nAfter filling in:\\n{}'.format(train_news.headlineTag.value_counts()))","19a25b8f":"def create_time_resolutions(df):\n    \n    df = df.copy()\n    # Basic hour and day:\n    df['dt_hour'] = df.time.dt.floor('h')\n    df['dt_day'] = df.time.dt.floor('d')\n\n    # Week of year, cannot be created in an above way due to not being a fixed frequency (according to pandas docs):\n    df['dt_weekofyear'] = df.apply(\n        lambda x: '{}_{}'.format(x['time'].weekofyear, x['time'].year), axis=1)\n    \n    return df\n\n\ntrain_news = create_time_resolutions(train_news)\ntrain_market = create_time_resolutions(train_market)","2605e944":"# News columns:\nnews_cols_agg_num = ['urgency', 'sentenceCount', 'wordCount',\n                    'firstMentionSentence', 'relevance',\n                   'sentimentClass', 'sentimentNegative',\n                   'sentimentNeutral', 'sentimentPositive',\n                   'sentimentWordCount']\n\nnews_cols_agg_cat = ['sourceId', 'provider', 'headlineTag',\n                    'marketCommentary']\n\n\n# Market columns:\nmarket_cols_agg_num = ['volume', 'close', 'open',\n                      'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',]\n\nmarket_cols_agg_cat = ['assetCode']\n\n\naggs_num = ['mean', 'std']\naggs_cat = ['count']","7e1d5429":"def rename_columns(df):\n    \n    df.columns = pd.Index(['{}{}'.format(\n        c[0], c[1].upper()) for c in df.columns.tolist()])\n    \n    return df","af0276f7":"# Mean aggregates for day for each of the assets:\nnews_asset_day = train_news.groupby(['dt_day', 'assetName'])[news_cols_agg_num].agg(aggs_num).reset_index()\n# Similarly, for each week of year:\nnews_asset_week = train_news.groupby(['dt_weekofyear', 'assetName'])[news_cols_agg_num].agg(aggs_num).reset_index()\n\n# Now, let's incorporate some history.\n# We will shift week by one and create features describing last week state of the asset:\nnews_asset_last_day = train_news.groupby(\n    ['assetName', 'dt_day'])[news_cols_agg_num].agg(aggs_num).groupby(['assetName']).shift(1).reset_index()\nnews_asset_last_week = train_news.groupby(\n    ['assetName', 'dt_weekofyear'])[news_cols_agg_num].agg(aggs_num).groupby(['assetName']).shift(1).reset_index()\n\n\nnews_asset_day = rename_columns(news_asset_day)\nnews_asset_week = rename_columns(news_asset_week)\nnews_asset_last_day = rename_columns(news_asset_last_day)\nnews_asset_last_week = rename_columns(news_asset_last_week)","9aa71ce1":"# Mean aggregates for day for each of the assets:\nmarket_asset_day = train_market.groupby(['dt_day', 'assetName'])[market_cols_agg_num].agg(aggs_num).reset_index()\n# Similarly, for each week of year:\nmarket_asset_week = train_market.groupby(['dt_weekofyear', 'assetName'])[market_cols_agg_num].agg(aggs_num).reset_index()\n\n# Now, let's incorporate some history.\n# We will shift week by one and create features describing last week state of the asset:\nmarket_asset_last_day = train_market.groupby(\n    ['assetName', 'dt_day'])[market_cols_agg_num].agg(aggs_num).groupby(['assetName']).shift(1).reset_index()\nmarket_asset_last_week = train_market.groupby(\n    ['assetName', 'dt_weekofyear'])[market_cols_agg_num].agg(aggs_num).groupby(['assetName']).shift(1).reset_index()\n\n\nmarket_asset_day = rename_columns(market_asset_day)\nmarket_asset_week = rename_columns(market_asset_week)\nmarket_asset_last_day = rename_columns(market_asset_last_day)\nmarket_asset_last_week = rename_columns(market_asset_last_week)","32cdc5d7":"X = train_market.copy()\n\n# Merge news features:\n# At this point we will perform a suboptimal merge on assetName and day:\nX = X.merge(\n    news_asset_day, how='left', on=['assetName', 'dt_day'], suffixes=('', '_news_day'))\nX = X.merge(\n    news_asset_week, how='left', on=['assetName', 'dt_weekofyear'], suffixes=('', '_news_week'))\nX = X.merge(\n    news_asset_last_day, how='left', on=['assetName', 'dt_day'], suffixes=('', '_news_last_day'))\nX = X.merge(\n    news_asset_last_week, how='left', on=['assetName', 'dt_weekofyear'], suffixes=('', '_news_last_week'))\n\n# Merge market features:\nX = X.merge(\n    market_asset_day, how='left', on=['assetName', 'dt_day'], suffixes=('', '_market_day'))\nX = X.merge(\n    market_asset_week, how='left', on=['assetName', 'dt_weekofyear'], suffixes=('', '_market_week'))\nX = X.merge(\n    market_asset_last_day, how='left', on=['assetName', 'dt_day'], suffixes=('', '_market_last_day'))\nX = X.merge(\n    market_asset_last_week, how='left', on=['assetName', 'dt_weekofyear'], suffixes=('', '_market_last_week'))\n\n\n# Encode basic categorical variables:\n# X['assetCode'] = pd.factorize(X['assetCode'])[0]\n# X['assetName'] = pd.factorize(X['assetName'])[0]","87cb141f":"print('New train shape: {}'.format(X.shape))\nprint('\\nNew NaN structure:\\n{}'.format(np.sum(pd.isnull(X))))","27e6ca12":"# Floor all datetime objects to a specified resolution:\n# 'd' - days\n# 'h' - hours\nX['time_split_resolution'] = X.time.dt.floor('d')\n\n# Select unique values based on market set:\ntrain_times_unique = X.time_split_resolution.unique()\n# Split unique dates into 80\/20% training\/validation split:\ntr_times, valid_times = train_test_split(train_times_unique, test_size=0.2, random_state=1337)","aefd9721":"# Create subsets for market and news datasets:\nX_tr = X[X.time_split_resolution.isin(tr_times)]\nX_val = X[X.time_split_resolution.isin(valid_times)]\n\nprint('Dataset shapes: train - {}, valid - {}'.format(X_tr.shape, X_val.shape))","20c4fe25":"def get_input(df_, to_drop):\n    X = df_.drop(to_drop, axis=1)\n    y = (df_.loc[:, 'returnsOpenNextMktres10'] >= 0).values.astype(np.uint8)\n    # y = df_.loc[:, 'returnsOpenNextMktres10'].values\n    r = df_.loc[:, 'returnsOpenNextMktres10'].values\n    u = df_.loc[:, 'universe']\n    d = df_.loc[:, 'time'].dt.date\n    return X, y, r, u, d\n\n\nto_drop = ['returnsOpenNextMktres10',\n          'universe',\n          'time',\n          'assetCode',\n          'assetName',\n          'dt_weekofyear',\n          'dt_hour',\n          'dt_day',\n          'time_split_resolution']","aa1f030e":"X_train, y_train, r_train, u_train, d_train = get_input(X_tr, to_drop)\nX_valid, y_valid, r_valid, u_valid, d_valid = get_input(X_val, to_drop)\n\n# To reduce memory footprint:\nX_train = X_train.astype(np.float32)\nX_valid = X_valid.astype(np.float32)","e68503cd":"train_cols = X_train.columns.tolist()\n\ndtrain = lgb.Dataset(X_train.values, y_train, feature_name=train_cols)\ndvalid = lgb.Dataset(X_valid.values, y_valid,\n                     feature_name=train_cols, reference=dtrain)\n\n\nparams = {'learning_rate': 0.02,\n          'boosting': 'gbdt', \n          'objective': 'binary', \n          'seed': 2018}\n\nlgb_model = lgb.train(params, dtrain, \n                      num_boost_round=1000, \n                      valid_sets=(dvalid,), \n                      valid_names=('valid',), \n                      verbose_eval=50, \n                      early_stopping_rounds=100)","a6093a4a":"y_pred_conf_valid = lgb_model.predict(X_valid) * 2 - 1\ny_pred_conf_valid_binary = (y_pred_conf_valid > 0).astype(np.uint8)\nprint('Valid confidence accuracy: {}'.format(accuracy_score(y_valid, y_pred_conf_valid_binary)))","181d8766":"confidence_valid = y_pred_conf_valid.copy()\nr_valid = r_valid.clip(-1,1)\nx_t_i = confidence_valid * r_valid * u_valid\ndata = {'day' : d_valid, 'x_t_i' : x_t_i}\ndf = pd.DataFrame(data)\nx_t = df.groupby('day').sum().values.flatten()\nmean = np.mean(x_t)\nstd = np.std(x_t)\nscore_valid = mean \/ std\nprint('Score valid: {}'.format(score_valid))","4ac844e3":"n_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\npredicted_confidences = np.array([])\nto_drop_test = [\n          'time',\n          'assetCode',\n          'assetName',\n          'dt_weekofyear',\n          'dt_hour',\n          'dt_day']\n\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n\n    n_days += 1\n    print(n_days, end=' ')\n\n    t = time.time()\n\n    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n    X = market_obs_df.copy()\n    X = create_time_resolutions(X)\n\n    # Merge news features:\n    X = X.merge(\n        news_asset_day, how='left', on=['assetName', 'dt_day'], suffixes=('', '_news_day'))\n    X = X.merge(\n        news_asset_week, how='left', on=['assetName', 'dt_weekofyear'], suffixes=('', '_news_week'))\n    X = X.merge(\n        news_asset_last_day, how='left', on=['assetName', 'dt_day'], suffixes=('', '_news_last_day'))\n    X = X.merge(\n        news_asset_last_week, how='left', on=['assetName', 'dt_weekofyear'], suffixes=('', '_news_last_week'))\n\n    # Merge market features:\n    X = X.merge(\n        market_asset_day, how='left', on=['assetName', 'dt_day'], suffixes=('', '_market_day'))\n    X = X.merge(\n        market_asset_week, how='left', on=['assetName', 'dt_weekofyear'], suffixes=('', '_market_week'))\n    X = X.merge(\n        market_asset_last_day, how='left', on=['assetName', 'dt_day'], suffixes=('', '_market_last_day'))\n    X = X.merge(\n        market_asset_last_week, how='left', on=['assetName', 'dt_weekofyear'], suffixes=('', '_market_last_week'))\n\n    X = X.drop(to_drop_test, axis=1)\n    X = X.astype(np.float32)\n\n    prep_time += time.time() - t\n\n    t = time.time()\n    market_prediction = lgb_model.predict(X) * 2 - 1\n    predicted_confidences = np.concatenate(\n        (predicted_confidences, market_prediction))\n    prediction_time += time.time() - t\n\n    t = time.time()\n    preds = pd.DataFrame(\n        {'assetCode': market_obs_df['assetCode'], 'confidence': market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds, how='left').drop(\n        'confidenceValue', axis=1).fillna(0).rename(columns={'confidence': 'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n\n\nenv.write_submission_file()\ntotal = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')\n","ab7417b4":"plt.hist(predicted_confidences, bins='auto')\nplt.title(\"Test set confidence:\")\nplt.show()","68539484":"## Missing  values:\n\nLet's explore missing values structure of the data:","1b025efe":"## Feature engineering:","d042b888":"# Time-resolution feature engineering\n\nData for this competition is time-series data. In this kind of problems, historical data usually is the best source of features.\nCreating aggregates based on groupby function is a standard method of feature creation. It can be also expanded to the case of time-series, where you first aggregate for a certain period of time and then shift this feature to add information about previous or future state in time. This notebooks explores idea of shifted aggregates based on various time resolutions.\nIt is supposed to provide a working example and can easily be extended to more scenarios - different time resolutions, different number of shifts and feature selection.\n\nCurrently this script uses only a small subset of data to show an example of how aggregates of various features can be created.\nKernels are RAM-constrained, so a full dataset run will cause the kernel to die.\n\nIn a manner similar to creation of aggregates in current point in time, lags\/leads of aggregates can be prepared to incorporate informationa about asset behavior in previous and incoming days or hours, to give an example.\n\n#### Baseline for this script was inspired by [kernel](https:\/\/www.kaggle.com\/christofhenkel\/market-data-nn-baseline) by Dieter.","22fc50d0":"## Remove obsolete (at this point) columns:\n\nSome of the columns require different methods of processing than ones shown in this kernel. At this point, those columns will simply be removed.\nIt it worth taking a closer look at them though!","1c255631":"### Calculate metric score:","1dc41062":"## Datetime processing:\n\nLet's begin with setting columns corresponding to the time dimension to `datetime`, this will enable us to easily derive various information from days, such as day, month, hour.\nThis kind of derived information may serve as a basis for further feature engineering, where new features will capture relationships of varied time resolution.\n\nIn order to ease experimentation, `debug` mode can be set to `True`, which will significantly decrease number of observations, making quick experiments much faster.","2599bb0e":"## Day-based split:\n\nBecause this is time-series data, a random split is not optimal.\nWe will create a split based on unique days in the training market data.\nIt is also worth trying a split based just on hours, with setting `'d'` to `'h'` in the `dt.floor` parameter.","953ae793":"### Check results:\n\nA significant amount of new columns has been created.\nSome of them will also contain a significant amount of NaNs.\n\nIt may be a good idea to filter out columns containing, let's say, more than 80% NaN.","88dc35f3":"### Show distribution of predicted confidence values:","af0fcdcf":"### Calculate accuracy:","7407da3d":"# Floor all datetime objects to a specified resolution:\n# 'd' - days\n# 'h' - hours\ntrain_news['time_split_resolution'] = train_news.time.dt.floor('d')\ntrain_market['time_split_resolution'] = train_market.time.dt.floor('d')\n\n# Select unique values based on market set:\ntrain_times_unique = train_market.time_split_resolution.unique()\n# Split unique dates into 80\/20% training\/validation split:\ntr_times, valid_times = train_test_split(train_times_unique, test_size=0.2, random_state=1337)","7ae593b5":"## Features of different time resolutions:\n\nWe will create a few additional features capturing different time resolutions. These will serve as a basis for lags\/leads and grouping features for different frequencies.","d2a35bc1":"There are some missing values in `train_market` set.\nAt first glance, it seems like `train_news` is all good, with no missing values.\nBut when we take a closer look, `headlineTag` columns consists mostly of empty entries.\nLet's replace empty values with __NoInfo__ category."}}