{"cell_type":{"ac478d0f":"code","952c272c":"code","913f54e8":"code","464ca58b":"code","c211a107":"code","6a9a0548":"code","6c6e759c":"code","7f11dc2b":"code","4dfeb1c7":"code","f158b8b1":"code","b9d6c488":"code","78ad8d6f":"code","9a71fb71":"code","bed8d8ca":"code","7483aa41":"markdown","e814be74":"markdown","085fb75a":"markdown","87cf4720":"markdown","8f5b114c":"markdown","e3903753":"markdown","3a2291b2":"markdown","c38ba3b3":"markdown","6c6d721d":"markdown","1e4f675a":"markdown","dff6962f":"markdown","11f5a776":"markdown","e3f1fe8f":"markdown"},"source":{"ac478d0f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import preprocessing # package containing modules for modelling, data processing, etc.\nfrom sklearn import impute\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import r2_score\nimport seaborn as sns # visualization package #1\nimport matplotlib.pyplot as plt # visualization package #2\n# Configure visualisations\n%matplotlib inline\n","952c272c":"# Import files containing data\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Convert .csv to dataframes\ntrain_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\", index_col=\"Id\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\", index_col=\"Id\")","913f54e8":"# concatenate both train and test sets\n# create extra column to allow a later separation of test and train data sets\ntrain_df['isTrain'] = 1\ntest_df['isTrain'] = 0\ndataset = pd.concat([train_df, test_df], axis=0, sort=False)\n\n# fillna, substitutes nan values with mean values in that feature\ndataset.fillna(dataset.mean(), inplace=True)\n\n# Fill na's of categorical features with simple imputer\nimputer = impute.SimpleImputer(missing_values= np.nan, strategy='constant', fill_value='Empty')\nimputed_df = pd.DataFrame(imputer.fit_transform(dataset), index=dataset.index, columns=dataset.columns, dtype=np.array(dataset.dtypes))\n# convert imputed_df features to those of train_df\n# when creating df - imputed_df - all features are assigned dtype \"object\"\ncolumns = dataset.select_dtypes(include=\"int\").columns\nimputed_df[columns] = imputed_df[columns].astype(int)\ncolumns = dataset.select_dtypes(include=\"float\").columns\nimputed_df[columns] = imputed_df[columns].astype(float)\ncolumns = dataset.select_dtypes(include=\"object\").columns\nimputed_df[columns] = imputed_df[columns].astype(object)\n\n# Define Ordinal encoder to convert categorical features to numerical\nOE_encoder = preprocessing.OrdinalEncoder()\ncat_features = imputed_df.select_dtypes(include=['object']).dtypes.index # create array containing indexes of categorical features\nimputed_df_num = imputed_df.select_dtypes(exclude=['object']).copy() # create dataframe containing only numerical features\nimputed_df_cat = imputed_df[cat_features].copy() # create dataframe containing only categorical features\ncat_feat_encoded = pd.DataFrame(OE_encoder.fit_transform(imputed_df_cat), index=imputed_df_cat.index, columns=cat_features) #create dataframe after fitting and transforming categorical features\ndf_encoded = pd.concat([imputed_df_num, cat_feat_encoded], axis=1) # concatenate numerical and categorical dataframes\n\n# Separate test and train data sets after modifications\ntrain_df2 = df_encoded[df_encoded['isTrain'] == 1].copy()\ntest_df2 = df_encoded[df_encoded['isTrain'] == 0].copy()\n\n# Drop create feature 'isTrain'\ntrain_df2.drop(['isTrain'], axis=1, inplace=True)\ntest_df2.drop(['isTrain', 'SalePrice'], axis=1, inplace=True)\n\n# Set objective feature\ny = train_df2[\"SalePrice\"]\nX = train_df2.drop([\"SalePrice\"], axis=1).copy()\n\n# Split data into train and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Use a linear regression model as baseline\nBaseline_model = LinearRegression()\nBaseline_model.fit(X_train, y_train) #training the algorithm\ny_pred = Baseline_model.predict(X_valid) #make predictions on algorithm\n\n# Use a decision tree model as baseline #1\nBaseline_model_DT = DecisionTreeRegressor(max_depth=10, min_samples_leaf=14, random_state=0) # max_depth and min_samples_leaf selected after performing small side study\nBaseline_model_DT.fit(X_train, y_train) #training the algorithm\ny_pred_DT = Baseline_model_DT.predict(X_valid) #make predictions on algorithm\n\n# Accuracy results of Linear Regressor model to check for overfitting\nprint(\"Linear Regressor - Train set R2 score is: \", round(Baseline_model.score(X_train, y_train),4))\nprint(\"Linear Regressor - Validation set R2 score is: \", round(Baseline_model.score(X_valid, y_valid),4), \"\\n\")\n\n# Accuracy results of Linear Regressor model to check for overfitting\nprint(\"Decision Tree Regressor - Train set R2 score is: \", round(Baseline_model_DT.score(X_train, y_train),4))\nprint(\"Decision Tree Regressor - Validation set R2 score is: \", round(Baseline_model_DT.score(X_valid, y_valid),4), \"\\n\")\n\n# Compute Root Mean Square Error\nprint(\"The RMS log error of Linear Regressor Baseline Model is: \", round(mean_squared_log_error(y_valid, np.absolute(y_pred)),4))\nprint(\"The RMS log error of Decision Tree Regressor Baseline Model is: \", round(mean_squared_log_error(y_valid, y_pred_DT),4), \"\\n\")\n","464ca58b":"# construction of a plot to show differences between original SalePrice and its logarithmic distribution\nplt.figure(figsize=(8,6))\nsns.distplot(y_train)\nsns.distplot(y_valid)","c211a107":"# scatter plot showing the differences between the predicted and real SalePrice \nplt.figure(figsize=(9,6))\nplt.ylabel('Prediction')\nsns.regplot(x=y_valid, y=y_pred, label='Linear Regressor')\nsns.regplot(x=y_valid, y=y_pred_DT, label='Decision Tree Regressor')\nplt.legend()","6a9a0548":"# Side study used to determine best parameters for decision tree regressor\n# create 3 empty dictionaries to contain iteration results\nresults_rmsle = {}\nresults_accuracy_valid = {}\nresults_accuracy_train = {}\n\nfor depth in range(1,11):\n    aux_rmsle = []\n    aux_accuracy_train = []\n    aux_accuracy_valid = []\n    \n    for samples in range(4,22,2):\n        Baseline_model_DT = DecisionTreeRegressor(max_depth=depth, min_samples_leaf=samples, random_state=0)\n        Baseline_model_DT.fit(X_train, y_train) #training the algorithm\n        y_pred_DT = Baseline_model_DT.predict(X_valid) #make predictions on algorithm\n        \n        aux_rmsle.append(round(mean_squared_log_error(y_valid, y_pred_DT),2))\n        aux_accuracy_train.append(round(Baseline_model_DT.score(X_train, y_train),2))\n        aux_accuracy_valid.append(round(Baseline_model_DT.score(X_valid, y_valid),2))\n        \n    results_rmsle[\"depth=\" + str(depth)] = aux_rmsle\n    results_accuracy_train[\"depth=\" + str(depth)] = aux_accuracy_train\n    results_accuracy_valid[\"depth=\" + str(depth)] = aux_accuracy_valid\n\n# convert dictionaries to dataframes\nresults_rmsle_df = pd.DataFrame(results_rmsle, index=range(4,22,2), columns=results_rmsle.keys())\nresults_accuracy_train_df = pd.DataFrame(results_accuracy_train, index=range(4,22,2), columns=results_accuracy_train.keys())\nresults_accuracy_valid_df = pd.DataFrame(results_accuracy_valid, index=range(4,22,2), columns=results_accuracy_valid.keys())\n\n# rename index of dataframes\nresults_rmsle_df.index.names = [\"SamplesPerLeaf\"]\nresults_accuracy_train_df.index.names = [\"SamplesPerLeaf\"]\nresults_accuracy_valid_df.index.names = [\"SamplesPerLeaf\"]\n\n# create 3 plots showing the evolutions of the errors for the different samples per leaf and tree levels\nf, axes = plt.subplots(nrows=1, ncols=3, figsize=(20,6))\naxes[0].set_title('RMSLE Error')\naxes[1].set_title('Accuracy Train Errors')\naxes[2].set_title('Accuracy Validation Errors')\nsns.lineplot(data=results_rmsle_df, dashes=False, ax=axes[0], legend='brief')\nsns.lineplot(data=results_accuracy_train_df, dashes=False, ax=axes[1], legend='brief')\nsns.lineplot(data=results_accuracy_valid_df, dashes=False, ax=axes[2], legend='brief')","6c6e759c":"# store prediction results from Decision Tree Baseline Model\nid = test_df2.index\nresult = Baseline_model_DT.predict(test_df2)\n\noutput = pd.DataFrame( { 'id': id , 'SalePrice': result} )\noutput = output[['id', 'SalePrice']]\n\noutput.to_csv(\"solution.csv\", index = False)\noutput.head(10)","7f11dc2b":"# Use MinMaxScaler module - scales all features between defined feature range (default=(0,1))\nmM_scalerX = preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X_train)\nmM_scalery = preprocessing.MinMaxScaler(feature_range=(0,1)).fit(np.array(y_train).reshape(-1,1))\nX_train_scaled = pd.DataFrame(mM_scalerX.transform(X_train), index=X_train.index, columns=X_train.columns)\ny_train_scaled = pd.DataFrame(mM_scalery.transform(np.array(y_train).reshape(-1,1)), index=y_train.index)\nX_valid_scaled = pd.DataFrame(mM_scalerX.transform(X_valid), index=X_valid.index, columns=X_valid.columns)\ny_valid_scaled = pd.DataFrame(mM_scalery.transform(np.array(y_valid).reshape(-1,1)), index=y_valid.index)\n\n# Use a LinearRegression model - with MinMaxScaler\nScaled_model = LinearRegression()\nScaled_model.fit(X_train_scaled, y_train_scaled) # training the algorithm\ny_pred_scaled = mM_scalery.inverse_transform(np.array(Scaled_model.predict(X_valid_scaled)).reshape(-1,1)) # make predictions on algorithm and invert the scaling made\n\n# Use StandardScaler module - standardize features by removing the mean and scaling to unit variance\nnormalizer_X = preprocessing.StandardScaler().fit(X_train)\nnormalizer_Y = preprocessing.StandardScaler().fit(np.array(y_train).reshape(-1,1))\nX_train_normalized = pd.DataFrame(normalizer_X.transform(X_train), index=X_train.index, columns=X_train.columns)\ny_train_normalized = pd.DataFrame(normalizer_Y.transform(np.array(y_train).reshape(-1,1)), index=y_train.index)\nX_valid_normalized = pd.DataFrame(normalizer_X.transform(X_valid), index=X_valid.index, columns=X_valid.columns)\ny_valid_normalized = pd.DataFrame(normalizer_Y.transform(np.array(y_valid).reshape(-1,1)), index=y_valid.index)\n\n# Use a LinearRegression model - with StandardScaler\nNormalized_model = LinearRegression()\nNormalized_model.fit(X_train_normalized, y_train_normalized) #training the algorithm\ny_pred_normalized = normalizer_Y.inverse_transform(np.array(Normalized_model.predict(X_valid_normalized)).reshape(-1,1)) # make predictions on algorithm and invert the normalization made\n\n# Apply logarithmic transformation on predictions\ny_train_log = np.log(y_train)\ny_valid_log = np.log(y_valid)\n\n# Use a LinearRegression model - with logarithmic transformation\nlog_model = LinearRegression()\nlog_model.fit(X_train, y_train_log) #training the algorithm\ny_pred_log = np.exp(log_model.predict(X_valid)) # make predictions on algorithm and invert the log made\n\n# Use a DecisionTreeRegressor model - with logarithmic transformation\nlog_model_DT = DecisionTreeRegressor(max_depth=10, min_samples_leaf=14, random_state=0)\nlog_model_DT.fit(X_train, y_train_log) #training the algorithm\ny_pred_log_DT = np.exp(log_model_DT.predict(X_valid)) # make predictions on algorithm and invert the log made\n\n# Accuracy results of Linear Regressor model to check for overfitting\nprint(\"Linear Regressor with MinMaxScaler - Train set R2 score is: \", round(Scaled_model.score(X_train_scaled, y_train_scaled),4))\nprint(\"Linear Regressor with MinMaxScaler - Validation set R2 score is: \", round(Scaled_model.score(X_valid_scaled, y_valid_scaled),4), \"\\n\")\n\n# Accuracy results of Linear Regressor model to check for overfitting\nprint(\"Linear Regressor with StandardScaler - Train set R2 score is: \", round(Normalized_model.score(X_train_normalized, y_train_normalized),4))\nprint(\"Linear Regressor with StandardScaler - Validation set R2 score is: \", round(Normalized_model.score(X_valid_normalized, y_valid_normalized),4), \"\\n\")\n\n# Accuracy results of Linear Regressor model to check for overfitting\nprint(\"Linear Regressor with Log Transformation - Train set R2 score is: \", round(log_model.score(X_train, y_train_log),4))\nprint(\"Linear Regressor with Log Transformation - Validation set R2 score is: \", round(log_model.score(X_valid, y_valid_log),4))\n# Accuracy results of Linear Regressor model to check for overfitting\nprint(\"Decision Tree Regressor with Log Transformation - Train set R2 score is: \", round(log_model_DT.score(X_train, y_train_log),4))\nprint(\"Decision Tree Regressor with Log Transformation - Validation set R2 score is: \", round(log_model_DT.score(X_valid, y_valid_log),4), \"\\n\")\n\n# Compute Root Mean Square Error\nprint(\"The RMS log error of Linear Model with MinMaxScaler is: \", round(mean_squared_log_error(y_valid, np.absolute(y_pred_scaled)),4))\nprint(\"The RMS log error of Linear Model with StandardScaler is: \", round(mean_squared_log_error(y_valid, np.absolute(y_pred_normalized)),4))\nprint(\"The RMS log error of Linear Model with log transformation is: \", round(mean_squared_log_error(y_valid, y_pred_log),4))\nprint(\"The RMS log error of DT Model with log transformation is: \", round(mean_squared_log_error(y_valid, y_pred_log_DT),4), \"\\n\")\n\n","4dfeb1c7":"# construction of a plot to show differences between original SalePrice and its logarithmic distribution\nf, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\naxes[0].set_title('SalePrice')\naxes[1].set_title('log(SalePrice)')\nsns.distplot(y_train, ax=axes[0])\nsns.distplot(y_train_log, ax=axes[1])","f158b8b1":"f, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,6))\naxes[0].set_title('Validation SalePrice')\naxes[1].set_title('Predicted e^log(SalePrice)')\nsns.distplot(y_valid, ax=axes[0])\nsns.distplot(y_pred_log, ax=axes[1])","b9d6c488":"train_df2[train_df2['MasVnrArea'] != 0][['TotalBsmtSF', 'GarageArea', '1stFlrSF','MasVnrArea', 'GrLivArea', 'OpenPorchSF', 'WoodDeckSF', 'EnclosedPorch', 'ScreenPorch', '3SsnPorch']].head(20)","78ad8d6f":"train_df2[train_df2['LowQualFinSF'] != 0][['TotalBsmtSF', 'GarageArea', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'MasVnrArea', 'GrLivArea', 'OpenPorchSF', 'WoodDeckSF', 'EnclosedPorch', 'ScreenPorch', '3SsnPorch']].head(20)","9a71fb71":"# Makes sense to build a single feature containing the entire house square footage of the lot\ntrain_df2['TotalInsideSF'] = train_df2['TotalBsmtSF'] + train_df2['GrLivArea'] +train_df2['GarageArea'] + _\n                           train_df2['OpenPorchSF'] + train_df2['WoodDeckSF'] + train_df2['EnclosedPorch'] + _\n                           train_df2['ScreenPorch'] + train_df2['3SsnPorch'] + train_df2['PoolArea']\n        \ntrain_df2['TotalOutsideSF'] = train_df2['LotArea'] - train_df2['TotalBsmtSF']\n\n# Makes sense to build a single feature containing the entire square footage of the lot\ntrain_df2['TotalHomeSqArea'] = train_df2['TotalBsmtSF'] + train_df2['GrLivArea'] + _\n                           train_df2['OpenPorchSF'] + train_df2['WoodDeckSF'] + train_df2['EnclosedPorch'] + _\n                           train_df2['ScreenPorch'] + train_df2['3SsnPorch']\n","bed8d8ca":"# Correlation plot - too confuse to understand relationships\n#f, axis = plt.subplots(figsize=(12, 12))\n#corr = train_df_encoded.corr()\n#sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=axis)\n\n# compute bar plot showing all correlations between features and SalePrice\ncorr_SalePrice = train_df2[train_df2.columns[1:]].corr()['SalePrice'][:].sort_values(ascending=False)\ncorr_SalePrice.drop([\"SalePrice\"], axis=0, inplace=True)\nplt.figure(figsize=(18, 5))\ncorr_SalePrice.plot.bar()","7483aa41":"The `LinearRegressor` model shows some interesting results! Why is there such a discrepancy between train and validation sets accuracy? Are the distributions of the variables inside each set very different from one another? ","e814be74":"# Define a Simple Linear Regressor - Baseline Model\n\nStart by defining a simple linear regressor model to use as baseline. The LabelEncoder module shall be used to encode categorical feaatures. Moreover, the SimpleImputer module imputing median values will be used to fill not-a-number entries.\n","085fb75a":"# House Prices Competition\n\nThe purpose of this competition is to practice different regression modelling techniques and data pre-processing approaches.","87cf4720":"## Import Relevant Packages","8f5b114c":"# Test #3 - Feature Correlation\n\nIn this approach, I will select the most correlated features (above a certain threshold) with the `SalePrice` target feature, and use them to build a model. \nThe goal is to check if by removing the least correlated features, hence reducing the associated noise, this first iteration performs better than the baseline model.\n\n\n\n","e3903753":"As observed above, with the log transformation, the `SalePrice` becomes less skewed and more \"normal\".","3a2291b2":"# Test #2 - Compute New Features","c38ba3b3":"## Possible Paths:\n1. Try to fill null (NA\/nan, etc.) values, with values that make sense for that specific feature. This approach ties to make the most out of every feature available, despite possibly introducing bias to the modelling. \n2. Perform some feature engineering, for instance, aggregating all features containing the square footage of a house as `TotalSqFtg`\n3. Try feature normalization (only relevant in regression type models)\n4. Try something like an `XGBoost` or `LightGBM` or `ElasticNet`\n\nDon't discard variables unless you have a good reason for it. No it is not a good reason to say it's not correlated. Tree algorithms can use the information and are not harmed by including it. A good reason for excluting could be I'm using KNN. The other main reason for discarding variables are if they are correlated with being in the test or training set e.g. in many competitions you goal is to predict out of time. So including time can be very harmfull.","6c6d721d":"## Create Submission file\n\nCreate .csv file with the submissions of the baseline Decision Tree model.","1e4f675a":"As observed, the parameters combination that minimize the errors is:\n* `SamplesPerLeaf` = 14\n* `MaxDepth` = 10","dff6962f":"# Test #1 - Scaling\/Normalization\nUsing `sklearn.preprocessing` two modules, A- `MinMaxScaler` and B-`StandardScaler`, scale\/normalize all variables to check their influence on the linear regressor baseline model predictions.\nAlso, as a further test, apply a logarithmic transformation on predictions and check if results improve.","11f5a776":"## Import data set .csv files and transform to dataframes","e3f1fe8f":"Next, a small study is performed to find good parameters for the Decision Tree Regressor. It is done in the hidden cells."}}