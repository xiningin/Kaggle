{"cell_type":{"be7bebe3":"code","9653434c":"code","235db43c":"code","09df330d":"code","2a3f7a15":"code","b6fe746f":"code","5469f99d":"code","12d769f8":"code","32ebabb0":"code","b5538c4c":"code","d33045f3":"code","e6b0291c":"code","23ca18e5":"code","3c3f3a05":"code","e8639059":"code","237f5cac":"code","03882dff":"code","1504f81d":"code","7e7e655b":"code","4e67502b":"code","93654a90":"code","9524b01a":"code","dfd65811":"code","3503ff37":"code","6d5a8354":"code","aa22054e":"code","6459fbf4":"code","0d79677c":"code","77a4a393":"code","b803ca4e":"code","cc6bdfcb":"code","af380124":"code","bf1ac744":"code","938b0e24":"code","b0682ed5":"code","e0e14a0b":"code","22e2c87a":"code","e1d49e17":"code","614ad156":"code","242446c7":"code","5dc2537a":"code","02d7554a":"code","dd001bc9":"code","b40142b9":"code","e71ddb30":"code","95d17aaa":"code","43f45da8":"code","916dd040":"code","3ad5293e":"code","9dac98df":"code","3f8aaa84":"code","21638aac":"code","acc2b615":"code","3c1e29cc":"code","3f92892c":"code","c529d599":"code","e294c1cc":"code","949fdbd0":"code","77a00b17":"code","1c994abd":"code","7837f132":"code","d1310565":"code","51214bd4":"code","fc65178d":"code","cc21445c":"code","a0d2f180":"code","c41d77e0":"code","f913ae57":"code","e701232d":"code","6e608926":"markdown","3c9e26f6":"markdown","af0ba7fa":"markdown","87335e83":"markdown","c1323724":"markdown","0f90b6e4":"markdown","70349c3e":"markdown","6a5f555b":"markdown","f2747025":"markdown","85fd1273":"markdown","72eebb68":"markdown","a8710117":"markdown","9e18fa43":"markdown","bb98342f":"markdown","5384feb7":"markdown","819f3b05":"markdown","7700acd9":"markdown","210d2368":"markdown"},"source":{"be7bebe3":"# Load Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n!pip install pretrainedmodels\nimport pretrainedmodels\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks import * \n#from fastai.vision.models.cadene_models import *\nfrom fastai.vision.models import *\nfrom fastai.vision.learner import model_meta\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom fastai.torch_core import flatten_model\nfrom fastai.layers import CrossEntropyFlat\nfrom fastai.metrics import error_rate # 1 - accuracy\nfrom sklearn.metrics import f1_score , roc_auc_score\nimport torch\nfrom tqdm import tqdm","9653434c":"%matplotlib inline\n%reload_ext autoreload\n%autoreload 2\nos.environ['FASTAI_TB_CLEAR_FRAMES']=\"1\"\n# Setting the path\npath = \"\/kaggle\/input\/auto-tag-images-of-gala\/dataset\"","235db43c":"def arch_summary(arch):\n    model = arch(False)\n    tot = 0\n    for i, l in enumerate(model.children()):\n        n_layers = len(flatten_model(l))\n        tot += n_layers\n        print(f'({i}) {l.__class__.__name__:<12}: {n_layers:<4}layers (total: {tot})')\n\n\ndef get_groups(model, layer_groups):\n    group_indices = [len(g) for g in layer_groups]\n    curr_i = 0\n    group = []\n    for layer in model:\n        group_indices[curr_i] -= len(flatten_model(layer))\n        group.append(layer.__class__.__name__)\n        if group_indices[curr_i] == 0:\n            curr_i += 1\n            print(f'Group {curr_i}:', group)   \n            group = []","09df330d":"def get_data(valid_pct, img_size, batch_size):\n    tfms = get_transforms()\n    data = ImageDataBunch.from_csv(path,folder='Train Images',valid_pct=0.2,csv_labels='train.csv',ds_tfms=tfms,fn_col='Image',test='Test Images',label_col='Class',size=img_size,bs=batch_size).normalize(imagenet_stats)\n#     if normalise:\n#         data = data.normalise(imagenet_stats)\n    return data\n\ndef train(arch,data,epoch,max_lr=None):\n    learner = cnn_learner(data, arch,metrics=accuracy)\n    learner.model_dir = '\/kaggle\/working\/'\n    if max_lr==None:\n        learner.fit_one_cycle(epoch)\n    else:\n        learner.fit_one_cycle(epoch,max_lr=slice(max_lr))\n    return learner\n\ndef prog_resize(arch,sz,n,k):\n    sz=sz\n    for i in range(n):\n        data = get_data(0.2,sz,batch_size=64)\n        if i>0:\n            load_size=int(sz-k)\n            learner = cnn_learner(data, arch,metrics=[FBeta(),accuracy],  wd=1e-1, opt_func= ranger).to_fp16()#, use_swa=True, callbacks=[SaveModelParams(learner.model)])\n            RLR = ReduceLROnPlateauCallback(learner, monitor='f_beta',patience = 2)\n            SAVEML = SaveModelCallback(learner, every='improvement', monitor='f_beta', name='best')\n            learner.model_dir = '\/kaggle\/working\/'\n            #learner.lr_find()\n            #learner.recorder.plot(suggestion=True)\n            learner.load(f'learner_{load_size}')\n            learner.fit_one_cycle(4,max_lr=slice(1e-4,1e-1),wd=3e-4,moms=(0.99-0.90),callbacks = [RLR, SAVEML, ShowGraph(learner)])\n            #learner.unfreeze()\n            #learner.fit_one_cycle(1, max_lr)\n            learner.save(f'learner_{sz}')\n        else:\n            learner = train(arch, data, epoch=8)\n            learner.model_dir = '\/kaggle\/working\/'\n            learner.save(f'learner_{sz}')\n        sz+=k\n#         params=[]\n#         swa_model_params = [p.data.cpu().numpy() for p in learner.swa_model.parameters()]\n\n#         for p_model1, p_model2, p_model3, p_swa_model in zip(*params, swa_model_params):\n#             # check for equality up to a certain tolerance\n#             print(np.isclose(p_swa_model, np.mean(np.stack([p_model1, p_model2, p_model3]), axis=0)))\n\n    return learner","2a3f7a15":"tfms = get_transforms(flip_vert=True, max_lighting=0.1, max_zoom=1.05, max_warp=0.15)\ndata = ImageDataBunch.from_csv(path,folder='Train Images',valid_pct=0.2,csv_labels='train.csv',ds_tfms=tfms,fn_col='Image',test='Test Images',label_col='Class',size=224).normalize(imagenet_stats)","b6fe746f":"class FocalLoss(nn.Module):\n    def __init__(self, alpha=1., gamma=1.):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n\n    def forward(self, inputs, targets, **kwargs):\n        CE_loss = nn.CrossEntropyLoss(reduction='none')(inputs, targets)\n        pt = torch.exp(-CE_loss)\n        F_loss = self.alpha * ((1-pt)**self.gamma) * CE_loss\n        return F_loss.mean()\n\nloss_func = FocalLoss(gamma=1.)","5469f99d":"class SWA(Callback):\n    def __init__(self, model, swa_model, swa_start):\n        super().__init__()\n        self.model,self.swa_model,self.swa_start=model,swa_model,swa_start\n        \n    def on_train_begin(self):\n        self.epoch = 0\n        self.swa_n = 0\n\n    def on_epoch_end(self, metrics):\n        if (self.epoch + 1) >= self.swa_start:\n            self.update_average_model()\n            self.swa_n += 1\n            \n        self.epoch += 1\n            \n    def update_average_model(self):\n        # update running average of parameters\n        model_params = self.model.parameters()\n        swa_params = self.swa_model.parameters()\n        for model_param, swa_param in zip(model_params, swa_params):\n            swa_param.data *= self.swa_n\n            swa_param.data += model_param.data\n            swa_param.data \/= (self.swa_n + 1)\n\ndef collect_bn_modules(module, bn_modules):\n    if isinstance(module, torch.nn.modules.batchnorm._BatchNorm):\n        bn_modules.append(module)\n\ndef fix_batchnorm(swa_model, train_dl):\n    \"\"\"\n    During training, batch norm layers keep track of a running mean and\n    variance of the previous layer's activations. Because the parameters\n    of the SWA model are computed as the average of other models' parameters,\n    the SWA model never sees the training data itself, and therefore has no\n    opportunity to compute the correct batch norm statistics. Before performing \n    inference with the SWA model, we perform a single pass over the training data\n    to calculate an accurate running mean and variance for each batch norm layer.\n    \"\"\"\n    bn_modules = []\n    swa_model.apply(lambda module: collect_bn_modules(module, bn_modules))\n    \n    if not bn_modules: return\n\n    swa_model.train()\n\n    for module in bn_modules:\n        module.running_mean = torch.zeros_like(module.running_mean)\n        module.running_var = torch.ones_like(module.running_var)\n    \n    momenta = [m.momentum for m in bn_modules]\n\n    inputs_seen = 0\n\n    for (*x,y) in iter(train_dl):        \n        xs = V(x)\n        batch_size = xs[0].size(0)\n\n        momentum = batch_size \/ (inputs_seen + batch_size)\n        for module in bn_modules:\n            module.momentum = momentum\n                            \n        res = swa_model(*xs)        \n        \n        inputs_seen += batch_size\n                \n    for module, momentum in zip(bn_modules, momenta):\n        module.momentum = momentum \n\n\n# callback for storing the params of the model after each epoch\nclass SaveModelParams(Callback):\n    def __init__(self, model):\n        self.model = model\n\n    def on_epoch_end(self, metrics):\n        params.append([p.data.cpu().numpy() for p in self.model.parameters()])","12d769f8":"# params = []\n\n# learn2.fit(lr, 3, use_swa=True, callbacks=[SaveModelParams(learn2.model)])\n\n# # grab the params from the SWA model\n# swa_model_params = [p.data.cpu().numpy() for p in learn2.swa_model.parameters()]\n\n# for p_model1, p_model2, p_model3, p_swa_model in zip(*params, swa_model_params):\n#     # check for equality up to a certain tolerance\n#     print(np.isclose(p_swa_model, np.mean(np.stack([p_model1, p_model2, p_model3]), axis=0)))","32ebabb0":"def senet154(pretrained=False):\n    pretrained = 'imagenet' if pretrained else None\n    model = pretrainedmodels.senet154(pretrained=pretrained)\n    return model\n\n_se_resnet_meta = {'cut': -3, 'split': lambda m: (m[0][3], m[1]) }\nmodel_meta[senet154] = _se_resnet_meta","b5538c4c":"learn = create_cnn(data, senet154, ps=0.5, wd=1e-1, loss_func=loss_func, metrics=[FBeta(),accuracy, auc_roc_score, MultiLabelFbeta(average='weighted')], pretrained=True, model_dir='\/kaggle\/working\/').to_fp16()","d33045f3":"RLR = ReduceLROnPlateauCallback(learn, monitor='f_beta',patience = 2)\nSAVEML = SaveModelCallback(learn, every='improvement', monitor='f_beta', name='best')","e6b0291c":"learn.model_dir='\/kaggle\/working\/'\nlearn.lr_find()\n\nlearn.recorder.plot(suggestion=True)","23ca18e5":"learn.model_dir='\/kaggle\/working\/'\nlearn.fit_one_cycle(2)","3c3f3a05":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F  #(uncomment if needed,but you likely already have it)\n\n#Mish - \"Mish: A Self Regularized Non-Monotonic Neural Activation Function\"\n#https:\/\/arxiv.org\/abs\/1908.08681v1\n#implemented for PyTorch \/ FastAI by lessw2020 \n#github: https:\/\/github.com\/lessw2020\/mish\n\nclass Mish(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        #inlining this saves 1 second per epoch (V100 GPU) vs having a temp x and then returning x(!)\n        return x *( torch.tanh(F.softplus(x)))","e8639059":"def resnext101_32x4d(pretrained=False):\n    pretrained = 'imagenet' if pretrained else None\n    model = pretrainedmodels.resnext101_32x4d(pretrained=pretrained)\n    all_layers = list(model.children())\n    return nn.Sequential(*all_layers[0], *all_layers[1:])","237f5cac":"arch_summary(resnext101_32x4d)","03882dff":"import math\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\nimport itertools as it\n\n\n\nclass Ranger(Optimizer):\n\n    def __init__(self, params, lr=1e-3, alpha=0.5, k=6, N_sma_threshhold=5, betas=(.95,0.999), eps=1e-5, weight_decay=0):\n        #parameter checks\n        if not 0.0 <= alpha <= 1.0:\n            raise ValueError(f'Invalid slow update rate: {alpha}')\n        if not 1 <= k:\n            raise ValueError(f'Invalid lookahead steps: {k}')\n        if not lr > 0:\n            raise ValueError(f'Invalid Learning Rate: {lr}')\n        if not eps > 0:\n            raise ValueError(f'Invalid eps: {eps}')\n\n        #parameter comments:\n        # beta1 (momentum) of .95 seems to work better than .90...\n        #N_sma_threshold of 5 seems better in testing than 4.\n        #In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.\n\n        #prep defaults and init torch.optim base\n        defaults = dict(lr=lr, alpha=alpha, k=k, step_counter=0, betas=betas, N_sma_threshhold=N_sma_threshhold, eps=eps, weight_decay=weight_decay)\n        super().__init__(params,defaults)\n\n        #adjustable threshold\n        self.N_sma_threshhold = N_sma_threshhold\n\n        #now we can get to work...\n        #removed as we now use step from RAdam...no need for duplicate step counting\n        #for group in self.param_groups:\n        #    group[\"step_counter\"] = 0\n            #print(\"group step counter init\")\n\n        #look ahead params\n        self.alpha = alpha\n        self.k = k \n\n        #radam buffer for state\n        self.radam_buffer = [[None,None,None] for ind in range(10)]\n\n        #self.first_run_check=0\n\n        #lookahead weights\n        #9\/2\/19 - lookahead param tensors have been moved to state storage.  \n        #This should resolve issues with load\/save where weights were left in GPU memory from first load, slowing down future runs.\n\n        #self.slow_weights = [[p.clone().detach() for p in group['params']]\n        #                     for group in self.param_groups]\n\n        #don't use grad for lookahead weights\n        #for w in it.chain(*self.slow_weights):\n        #    w.requires_grad = False\n\n    def __setstate__(self, state):\n        print(\"set state called\")\n        super(Ranger, self).__setstate__(state)\n\n\n    def step(self, closure=None):\n        loss = None\n        #note - below is commented out b\/c I have other work that passes back the loss as a float, and thus not a callable closure.  \n        #Uncomment if you need to use the actual closure...\n\n        #if closure is not None:\n            #loss = closure()\n\n        #Evaluate averages and grad, update param tensors\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('Ranger optimizer does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]  #get state dict for this param\n\n                if len(state) == 0:   #if first time to run...init dictionary with our desired entries\n                    #if self.first_run_check==0:\n                        #self.first_run_check=1\n                        #print(\"Initializing slow buffer...should not see this at load from saved model!\")\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n\n                    #look ahead weight storage now in state dict \n                    state['slow_buffer'] = torch.empty_like(p.data)\n                    state['slow_buffer'].copy_(p.data)\n\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                #begin computations \n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                #compute variance mov avg\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                #compute mean moving avg\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n\n\n                buffered = self.radam_buffer[int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 \/ (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t \/ (1 - beta2_t)\n                    buffered[1] = N_sma\n                    if N_sma > self.N_sma_threshhold:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) \/ (N_sma_max - 4) * (N_sma - 2) \/ N_sma * N_sma_max \/ (N_sma_max - 2)) \/ (1 - beta1 ** state['step'])\n                    else:\n                        step_size = 1.0 \/ (1 - beta1 ** state['step'])\n                    buffered[2] = step_size\n\n                if group['weight_decay'] != 0:\n                    p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n\n                if N_sma > self.N_sma_threshhold:\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n                else:\n                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n\n                p.data.copy_(p_data_fp32)\n\n                #integrated look ahead...\n                #we do it at the param level instead of group level\n                if state['step'] % group['k'] == 0:\n                    slow_p = state['slow_buffer'] #get access to slow param tensor\n                    slow_p.add_(self.alpha, p.data - slow_p)  #(fast weights - slow weights) * alpha\n                    p.data.copy_(slow_p)  #copy interpolated weights to RAdam param tensor\n\n        return loss","1504f81d":"ranger = Ranger\nresnext = cnn_learner(data, resnext101_32x4d, pretrained=True, metrics=[FBeta(),accuracy],  ps=0.5, wd=1e-1, loss_func=loss_func, opt_func= ranger,\n                    cut=-2, split_on=lambda m: (m[0][6], m[1])).to_fp16()\nRLR = ReduceLROnPlateauCallback(resnext, monitor='f_beta',patience = 2)\nSAVEML = SaveModelCallback(resnext, every='improvement', monitor='f_beta', name='best')","7e7e655b":"resnext.model_dir='\/kaggle\/working\/'\nresnext.lr_find()\n\nresnext.recorder.plot(suggestion=True)","4e67502b":"resnext.fit_one_cycle(60, 2.75e-2, callbacks = [RLR, SAVEML, ShowGraph(resnext)])","93654a90":"def dpn92(pretrained=False):\n    pretrained = 'imagenet+5k' if pretrained else None\n    model = pretrainedmodels.dpn92(pretrained=pretrained)\n    return nn.Sequential(*list(model.children()))\n\ndpn = cnn_learner(data, dpn92, pretrained=True, metrics=accuracy,\n                    cut=-1, split_on=lambda m: (m[0][0][16], m[1])).to_fp16()\ndpn.fit_one_cycle(60)","9524b01a":"def inceptionresnetv2(pretrained=False):\n    pretrained = 'imagenet' if pretrained else None\n    model = pretrainedmodels.inceptionresnetv2(pretrained=pretrained)\n    return nn.Sequential(*model.children())\n\narch_summary(inceptionresnetv2)\n\ninres = cnn_learner(data, inceptionresnetv2, pretrained=True, metrics=accuracy,\n                    cut=-2, split_on=lambda m: (m[0][11], m[1]))\ninres.fit_one_cycle(8)","dfd65811":"# Needs lot more epochs to train\n# def xception(pretrained=False):\n#     pretrained = 'imagenet' if pretrained else None\n#     model = pretrainedmodels.xception(pretrained=pretrained)\n#     return nn.Sequential(*list(model.children()))\n\n# arch_summary(xception)\n\n# xcept = cnn_learner(data, xception, pretrained=True, metrics=accuracy,\n#                     cut=-1, split_on=lambda m: (m[0][11], m[1]))\n# xcept.fit_one_cycle(16)","3503ff37":"# Needs more epochs \n# def se_resnet50(pretrained=False):\n#     pretrained = 'imagenet' if pretrained else None\n#     model = pretrainedmodels.se_resnet50(pretrained=pretrained)\n#     return model\n# arch_summary(se_resnet50)\n\n# senet = cnn_learner(data, se_resnet50, pretrained=True,metrics=accuracy,\n#                     cut=-2, split_on=lambda m: (m[0][3], m[1]))\n# senet.fit_one_cycle(12)\n\n#get_groups(nn.Sequential(*learn.model[0], *learn.model[1]), learn.layer_groups)","6d5a8354":"# def inceptionv4(pretrained=False):\n#     pretrained = 'imagenet' if pretrained else None\n#     model = pretrainedmodels.inceptionv4(pretrained=pretrained)\n#     all_layers = list(model.children())\n#     return nn.Sequential(*all_layers[0], *all_layers[1:])\n\n# learn = cnn_learner(FakeData(), inceptionv4, pretrained=False,\n#                     cut=-2, split_on=lambda m: (m[0][11], m[1]))\n# get_groups(nn.Sequential(*learn.model[0], *learn.model[1]), learn.layer_groups)\n\n# def inceptionresnetv2(pretrained=False):\n#     pretrained = 'imagenet' if pretrained else None\n#     model = pretrainedmodels.inceptionresnetv2(pretrained=pretrained)\n#     return nn.Sequential(*model.children())\n\n# arch_summary(inceptionresnetv2)\n\n# learn = cnn_learner(FakeData(), inceptionresnetv2, pretrained=False,\n#                     cut=-2, split_on=lambda m: (m[0][9], m[1]))\n# get_groups(nn.Sequential(*learn.model[0], *learn.model[1]), learn.layer_groups)\n\n# def xception(pretrained=False):\n#     pretrained = 'imagenet' if pretrained else None\n#     model = pretrainedmodels.xception(pretrained=pretrained)\n#     return nn.Sequential(*list(model.children()))\n# arch_summary(xception)\n# learn = cnn_learner(FakeData(), xception, pretrained=False,\n#                     cut=-1, split_on=lambda m: (m[0][11], m[1]))\n# get_groups(nn.Sequential(*learn.model[0], *learn.model[1]), learn.layer_groups)\n\n# def identity(x): return x\n\n# def nasnetamobile(pretrained=False):\n#     pretrained = 'imagenet' if pretrained else None\n#     model = pretrainedmodels.nasnetamobile(pretrained=pretrained, num_classes=1000)  \n#     model.logits = identity\n#     return nn.Sequential(model)\n\n# arch_summary(lambda _: nasnetamobile(False)[0])\n# learn = cnn_learner(FakeData(), nasnetamobile, pretrained=False)\n\n\n# def dpn92(pretrained=False):\n#     pretrained = 'imagenet+5k' if pretrained else None\n#     model = pretrainedmodels.dpn92(pretrained=pretrained)\n#     return nn.Sequential(*list(model.children()))\n\n# arch_summary(dpn92)\n\n# arch_summary(lambda _: next(dpn92(False).children()))\n\n# get_groups(nn.Sequential(*learn.model[0][0], *learn.model[1]), learn.layer_groups)\n# learn = cnn_learner(FakeData(), dpn92, pretrained=False,\n#                     cut=-1, split_on=lambda m: (m[0][0][16], m[1]))\n\n\n# def pnasnet5large(pretrained=False):    \n#     pretrained = 'imagenet' if pretrained else None\n#     model = pretrainedmodels.pnasnet5large(pretrained=pretrained, num_classes=1000) \n#     model.logits = identity\n#     return nn.Sequential(model)\n# arch_summary(lambda _: pnasnet5large(False)[0])\n\n# model_meta[pnasnet5large] =  { 'cut': noop, \n#                                'split': lambda m: (list(m[0][0].children())[8], m[1]) }\n\n# All the metas\n\n# Calling\n#model_meta[fn_name] = _model_meta\n\n# _resnext_meta = {'cut': -2, 'split': lambda m: (m[0][6], m[1]) }\n# _se_resnet_meta = {'cut': -2, 'split': lambda m: (m[0][3], m[1]) }\n# _inception_4_meta = { 'cut': -2, 'split': lambda m: (m[0][11], m[1]) }\n# _squeezenet_meta = { 'cut': -1, 'split': lambda m: (m[0][0][7], m[1]) }\n# _xception_meta = { 'cut': -1, 'split': lambda m: (m[0][11], m[1]) }\n# model_meta[nasnetamobile] =  { 'cut': noop, \n#                                'split': lambda m: (list(m[0][0].children())[8], m[1]) }","aa22054e":"# def get_ensemble(nmodels):\n#     ens_model = [] # Empty List of ensemble model, I will store the trained learner object here \n#     learning_rate =[1e-3,1e-3,1e-3,1e-3] # List of learning rate for each model \n#     model_list = [models.resnet50,models.resnet152,models.densenet169,models.densenet201] ##List of Models . You can add resnet ones in the mix\n#     for i in range(nmodels):\n#         print(f'-----Training model: {i+1}--------')\n             \n#         data = get_data(0.2,80,64)\n#         learn_resnet = cnn_learner(data, model_list[i],pretrained=True, metrics=[error_rate, accuracy,AUROC()],\n#                                    model_dir=\"\/tmp\/model\/\")\n#         learn_resnet.fit_one_cycle(6)\n#         print('training for 120x120')\n#         learn_resnet.set_data = get_data(0.2,120,64) # Train the model for imagesize 128\n#         #learn_resnet.lr_find()\n#         #learn_resnet.recorder.plot(suggestion=True)\n#         learn_resnet.fit_one_cycle(4) # using the learning rate for the first model \n        \n#         print('training for 160x160')\n#         learn_resnet.set_data = get_data(0.2,160,64) # Train the model for imagesize 128\n#         #learn_resnet.lr_find()\n#         #learn_resnet.recorder.plot(suggestion=True)\n#         learn_resnet.fit_one_cycle(3) # using the learning rate for the first model \n        \n#         print('training for 200x200')\n#         learn_resnet.set_data = get_data(0.2,200,64) # Train the model for imagesize 128\n#         #learn_resnet.lr_find()\n#         #learn_resnet.recorder.plot(suggestion=True)\n#         learn_resnet.fit_one_cycle(2) # using the learning rate for the first model \n        \n#         print('training for 240x240')\n#         learn_resnet.set_data = get_data(0.2,240,64) #Train the model for imagesize 150\n#         learn_resnet.fit_one_cycle(1)   # using the learning rate assigned for the first model   \n        \n#         learn_resnet.save(f'ensem_model_{i}.weights')\n#         ens_model.append(learn_resnet)\n#         print(f'-----Training of model {i+1} complete----')\n#     return ens_model","6459fbf4":"#ens = get_ensemble(4)","0d79677c":"# ens_test_preds = [] ## Creating a list of predictions \n# for mdl in ens:\n#     preds,_ = mdl.TTA(ds_type=DatasetType.Test)\n#     print(np.array(preds).shape)\n#     ens_test_preds.append(np.array(preds)) ## create a list of prediction numpy arrays . \n    \n# ens_preds = np.mean(ens_test_preds, axis =0) ## Average the prediction from various numpy arrays using numpy mean function\n# test_df.has_cactus = ens_preds[:, 0] ##update the prediction in the test data\n# test_df.head()","77a4a393":"def ensemble_predition(img, ens):\n    #img = open_image(test_path + test_img)\n    preds=[]\n    for i in ens:\n        preds.append(i.predict(img))\n        #densenet_predicition = dense.predict(img)\n        #inc_predicition = inception.predict(img)\n    \n    #ensemble average\n    sum_pred=torch.tensor([0,0,0,0],dtype=torch.float)\n    for i in preds:\n        #print(i[2])\n        sum_pred += i[2] #+ densenet_predicition[2] #+ inc_predicition[2]\n    prediction = sum_pred \/ len(preds)\n    \n    #prediction results\n    predicted_label = torch.argmax(prediction).item()\n    if predicted_label==0:\n        predicted_label='Attire'\n    elif predicted_label==1:\n        predicted_label='Decorationandsignage'\n    elif predicted_label==2:\n        predicted_label='Food'\n    elif predicted_label==3:\n        predicted_label='misc'\n    #predicted_label.map(val)\n    return predicted_label, preds#resnet_predicition[0], densenet_predicition[0]#, inc_predicition[0]","b803ca4e":"res = prog_resize(models.resnet152,64,5,40)","cc6bdfcb":"dense = prog_resize(models.densenet201,64,5,40)","af380124":"def train_resnext(arch,data,epoch,max_lr=None):\n    learner = cnn_learner(data, arch, pretrained=True, metrics=accuracy,\n                    cut=-1, split_on=lambda m: (m[0][0][16], m[1]))\n    learner.model_dir = '\/kaggle\/working\/'\n    if max_lr==None:\n        learner.fit_one_cycle(epoch)\n    else:\n        learner.fit_one_cycle(epoch,max_lr=slice(max_lr))\n    return learner\n\ndef prog_resize_resnext(arch,sz,n,k, max_lr=None):\n    sz=sz\n    for i in range(n):\n        data = get_data(0.2,sz,batch_size=64)\n        if i>0:\n            load_size=int(sz-k)\n            learn = cnn_learner(data, arch, pretrained=True, metrics=accuracy,\n                    cut=-1, split_on=lambda m: (m[0][0][16], m[1]))\n            learner.model_dir = '\/kaggle\/working\/'\n            learner.load(f'learner_{load_size}')\n            learner.fit_one_cycle(4)\n#             learner.unfreeze()\n#             learner.fit_one_cycle(1, max_lr)\n            learner.save(f'learner_{sz}')\n        else:\n            learner = train_resnext(arch, data, epoch=8)\n            learner.model_dir = '\/kaggle\/working\/'\n            learner.save(f'learner_{sz}')\n        sz+=k\n#         params=[]\n#         swa_model_params = [p.data.cpu().numpy() for p in learner.swa_model.parameters()]\n\n#         for p_model1, p_model2, p_model3, p_swa_model in zip(*params, swa_model_params):\n#             # check for equality up to a certain tolerance\n#             print(np.isclose(p_swa_model, np.mean(np.stack([p_model1, p_model2, p_model3]), axis=0)))\n\n    return learner","bf1ac744":"dpn_pr = prog_resize_resnext(dpn92,80,5,40)","938b0e24":"tfms = get_transforms()\ndata = ImageDataBunch.from_csv(path,folder='Train Images',valid_pct=0.02,csv_labels='train.csv',ds_tfms=tfms,fn_col='Image',test='Test Images',label_col='Class',size=220,bs=64)\ndpn = train_dpn(dpn92,data,8)","b0682ed5":"# import pretrainedmodels\n# def inceptionv4(pretrained=False):\n#     pretrained = 'imagenet' if pretrained else None\n#     model = pretrainedmodels.inceptionv4(pretrained=pretrained)\n#     all_layers = list(model.children())\n#     return nn.Sequential(*all_layers[0], *all_layers[1:])\n# tfms = get_transforms()\n# data = ImageDataBunch.from_csv(path,folder='Train Images',valid_pct=0.02,csv_labels='train.csv',ds_tfms=tfms,fn_col='Image',test='Test Images',label_col='Class',size=240)\n# inception = cnn_learner(data, inceptionv4, pretrained=True, metrics=accuracy,\n#                     cut=-2, split_on=lambda m: (m[0][11], m[1]))","e0e14a0b":"#import pretrainedmodels\n# def incres(pretrained=False):\n#     pretrained = 'imagenet' if pretrained else None\n#     model = inceptionresnetv2(num_classes=1000, pretrained=pretrained)\n#     #all_layers = list(model.children())\n#     #return nn.Sequential(*all_layers[0], *all_layers[1:])\n#     return nn.Sequential(model)\n\n# def nasnetalarge(pretrained:bool=False):\n#     pretrained = 'imagenet' if pretrained else None\n#     model = nasnetalarge(num_classes=1000, pretrained=pretrained)\n#     model.logits = noop\n#     return nn.Sequential(model)\n# #model_meta[nasnetamobile] = {'cut': noop, 'split': lambda m: (list(m[0][0].children())[8], m[1])}\n\n# def inceptionresnetv2(pretrained:bool=False):  return get_model('inceptionresnetv2', pretrained, seq=True)\n# def dpn92(pretrained:bool=False):              return get_model('dpn92', pretrained, pname='imagenet+5k', seq=True)\n# def xception_cadene(pretrained=False):         return get_model('xception', pretrained, seq=True)\n# def se_resnet50(pretrained:bool=False):        return get_model('se_resnet50', pretrained)\n# def se_resnet101(pretrained:bool=False):       return get_model('se_resnet101', pretrained)\n# def se_resnext50_32x4d(pretrained:bool=False): return get_model('se_resnext50_32x4d', pretrained)\n# def senet154(pretrained:bool=False):           return get_model('senet154', pretrained)\n\n# model_meta[inceptionresnetv2] = {'cut': -2, 'split': lambda m: (m[0][9],     m[1])}\n# model_meta[dpn92]             = {'cut': -1, 'split': lambda m: (m[0][0][16], m[1])}\n# model_meta[xception_cadene]   = {'cut': -1, 'split': lambda m: (m[0][11],    m[1])}\n# model_meta[senet154]          = {'cut': -3, 'split': lambda m: (m[0][3],     m[1])}\n# _se_resnet_meta               = {'cut': -2, 'split': lambda m: (m[0][3],     m[1])}\n# model_meta[se_resnet50]        = _se_resnet_meta\n# model_meta[se_resnet101]       = _se_resnet_meta\n# model_meta[se_resnext50_32x4d] = _se_resnet_meta\n\n\ntfms = get_transforms()\ndata = ImageDataBunch.from_csv(path,folder='Train Images',valid_pct=0.02,csv_labels='train.csv',ds_tfms=tfms,fn_col='Image',test='Test Images',label_col='Class',size=180,bs=128)\n\n#incres = cnn_learner(data, incres, pretrained=True, metrics=accuracy)#,\n                    #cut=-2, split_on=lambda m: (m[0][11], m[1]))\n#nas = cnn_learner(data, nasnetamobile, pretrained=True, metrics=accuracy)#,cut= 'noop', split_on = lambda m: (list(m[0][0].children())[8], m[1]))","22e2c87a":"#nas.fit_one_cycle(6)","e1d49e17":"from fastai.metrics import error_rate # 1 - accuracy\nfrom sklearn.metrics import f1_score , roc_auc_score","614ad156":"# resnet = cnn_learner(data, models.resnet152, metrics=error_rate)#.mixup()\n# resnet_withMixUp = cnn_learner(data, models.resnet152, metrics=error_rate).mixup()\n# dnet = cnn_learner(data, models.densenet169, metrics=error_rate)#.mixup()\n# dnet_withMixUp = cnn_learner(data, models.densenet169, metrics=error_rate).mixup()\n# #resnext = cnn_learner(data, models.resnext152, metrics=error_rate)\n# defaults.device = torch.device('cuda') # makes sure the gpu is used","242446c7":"interp_res = ClassificationInterpretation.from_learner(res)\ninterp_res.plot_confusion_matrix()","5dc2537a":"interp_dense = ClassificationInterpretation.from_learner(dense)\ninterp_dense.plot_confusion_matrix()","02d7554a":"interp_in = ClassificationInterpretation.from_learner(inception)\ninterp_in.plot_confusion_matrix()","dd001bc9":"rnext.unfreeze() # must be done before calling lr_find\n#res.unfreeze() # must be done before calling lr_find\n#inception.unfreeze() # must be done before calling lr_find","b40142b9":"learn.model_dir = '\/kaggle\/working\/'\nlearn.lr_find()\nlearn.recorder.plot()","e71ddb30":"# dense.lr_find()\n# dense.recorder.plot()","95d17aaa":"# inception.lr_find()\n# inception.recorder.plot()","43f45da8":"# tfms = get_transforms()\n# data = ImageDataBunch.from_csv(path,folder='Train Images',valid_pct=0.02,csv_labels='train.csv',ds_tfms=tfms,fn_col='Image',test='Test Images',label_col='Class',size=240)\n# res = cnn_learner(data, models.resnet152, metrics=accuracy)\n# dense = cnn_learner(data, models.densenet201, metrics=accuracy)\n# res.model_dir = '\/kaggle\/working\/'\n# dense.model_dir = '\/kaggle\/working\/'\n# res.load('reslearner_220')\n# dense.load('denselearner_220')\n# res.fit_one_cycle(2)","916dd040":"rnext.fit_one_cycle(8,max_lr=slice(1e-5, 1e-4),callbacks=[ShowGraph(rnext)])","3ad5293e":"#inception.fit_one_cycle(20,max_lr=slice(1e-5, 5e-3),callbacks=[ShowGraph(inception)])","9dac98df":"res.fit_one_cycle(8,max_lr=slice(1e-5, 6e-5),callbacks=[ShowGraph(res)])","3f8aaa84":"#learn.fit_one_cycle(20, max_lr=slice(3e-6, 6e-4),callbacks=[ShowGraph(learn)])","21638aac":"# from fastai.widgets import *\n\n# ds, idxs = DatasetFormatter().from_toplosses(learn)\n# ImageCleaner(ds, idxs, '\/kaggle\/working\/')","acc2b615":"# interp = ClassificationInterpretation.from_learner(learn)\n# interp.plot_confusion_matrix()","3c1e29cc":"# preds,y = learn.TTA()\n# acc = accuracy(preds, y)\n# print('The validation accuracy is {} %.'.format(acc * 100))","3f92892c":"# #preds,y = learn.TTA()\n# labels = np.argmax(preds, 1)\n# score = f1_score(list(y),list(labels),average='weighted')\n# print('The validation f1 score is {} %.'.format(score * 100))","c529d599":"res_preds,y = res.TTA()\nres_acc = accuracy(res_preds, y)\nprint('The validation accuracy of Resnet is {} %.'.format(res_acc * 100))\n\n#preds,y = learn.TTA()\nres_labels = np.argmax(res_preds, 1)\nres_score = f1_score(list(y),list(res_labels),average='weighted')\nprint('The validation f1 score of Resnet is {} %.'.format(res_score * 100))\n\n# dense_preds,y = dense.TTA()\n# dense_acc = accuracy(dense_preds, y)\n# print('The validation accuracy of Densenet is {} %.'.format(dense_acc * 100))\n\n# #preds,y = learn.TTA()\n# dense_labels = np.argmax(dense_preds, 1)\n# dense_score = f1_score(list(y),list(dense_labels),average='weighted')\n# print('The validation f1 score of Densenet is {} %.'.format(dense_score * 100))\n\n# inception_preds,y = inception.TTA()\n# inception_acc = accuracy(inception_preds, y)\n# print('The validation accuracy of InceptionNet is {} %.'.format(inception_acc * 100))\n\n# #preds,y = learn.TTA()\n# inception_labels = np.argmax(inception_preds, 1)\n# inception_score = f1_score(list(y),list(inception_labels),average='weighted')\n# print('The validation f1 score of InceptionNet is {} %.'.format(inception_score * 100))","e294c1cc":"ens = [res, dense, dpn]","949fdbd0":"test_path = path+'\/Test Images\/'\ndef generateSubmission():\n    submissions = pd.read_csv(path+'\/test.csv')\n    #id_list = list(data.test_ds.x.items)\n    id_list=list(submissions.Image)\n    #predictions, *_ = learn.get_preds(DatasetType.Test,order=True)\n    ensemble_label=[]\n    #inception_label=[]\n    #resnet1_label=[]\n    resnet2_label=[]\n    dense1_label=[]\n    #dense2_label=[]\n    dpn_label = []\n    #resnext_label= []\n    with tqdm(total=len(os.listdir(test_path))) as pbar:\n        for iname in id_list:\n            img=open_image(path+\"\/Test Images\/\"+iname)\n#             resnet50_predicition = resnet50_learner.predict(img)\n#             densenet121_predicition = densenet121_learner.predict(img)\n#             vgg_predicition = vgg_learner.predict(img)\n\n            ##ensemble average\n#             sum_pred = resnet50_predicition[2] + densenet121_predicition[2] + vgg_predicition[2]\n#             prediction = sum_pred \/ 3\n\n            ##prediction results\n#             label.append(torch.argmax(prediction).item())\n            en,preds = ensemble_predition(img,ens)\n            #dpn_pred = dpn.predict(img)\n            ensemble_label.append(en)\n            #resnet1_label.append(res.predict(img)[0])\n            resnet2_label.append(preds[0][0])\n            dense1_label.append(preds[1][0])\n            #dense2_label.append(preds[3][0])\n            dpn_label.append(preds[2][0])\n            #resnext_label.append(resnext.predict(img)[0])\n            #inception_label.append(inception.predict(img)[0])\n            pbar.update(1)\n    submissions = pd.DataFrame({'Image':id_list,'Class':ensemble_label})\n    submissions.to_csv(\"submission_ensemble.csv\",index = False)\n    submissions_d1 = pd.DataFrame({'Image':id_list,'Class':dense1_label})\n    submissions_d1.to_csv(\"submission_dense1.csv\",index = False)\n    #submissions_d2 = pd.DataFrame({'Image':id_list,'Class':dense2_label})\n    #submissions_d2.to_csv(\"submission_dense2.csv\",index = False)\n    submissions_r1 = pd.DataFrame({'Image':id_list,'Class':resnet2_label})\n    submissions_r1.to_csv(\"submission_res1.csv\",index = False)\n    #submissions_r2 = pd.DataFrame({'Image':id_list,'Class':resnet2_label})\n    #submissions_r2.to_csv(\"submission_res2.csv\",index = False)\n    submissions_dpn = pd.DataFrame({'Image':id_list,'Class':dpn_label})\n    submissions_dpn.to_csv(\"submission_dpn.csv\",index = False)\n    #submissions_resnext = pd.DataFrame({'Image':id_list,'Class':resnext_label})\n    #submissions_resnext.to_csv(\"submission_resnext.csv\",index = False)\n#   \n#   \n#     submissions_i = pd.DataFrame({'Image':id_list,'Class':inception_label})\n#     submissions_i.to_csv(\"submission_inception.csv\",index = False)","77a00b17":"generateSubmission()","1c994abd":"# import os\n\n# #to give np array the correct style\n# submission_data = np.array([['dummy', 0]])\n\n# #progress bar\n# with tqdm(total=len(os.listdir(test_path))) as pbar:       \n#     #test all test images\n#     for img in os.listdir(test_path):\n#         label = learn.predict(open_image(test_path+img))\n#         new_np_array = np.array([[img, label]])\n#         submission_data = np.concatenate((submission_data, new_np_array), axis=0)\n#         pbar.update(1)\n\n# #remove dummy\n# submission_data = np.delete(submission_data, 0, 0)\n# submissions = pd.DataFrame(submission_data, columns=['Image','Class'])\n# submissions.to_csv(\"submission.csv\",index = False)","7837f132":"# generateSubmission(learn)","d1310565":"interp.plot_top_losses(9, figsize=(15,15))","51214bd4":"res.model","fc65178d":"import matplotlib.pyplot as plt\n# Visualising Covolution Layers\nlayers = res.model\nlayer_ids = [1,4,7,11,15]\n#plot the filters\nfig,ax = plt.subplots(nrows=1,ncols=5)\nfor i in range(5):\n    ax[i].imshow(layers[layer_ids[i]].get_weights()[0][:,:,:,0][:,:,0],cmap='gray')\n    ax[i].set_title('block'+str(i+1))\n    ax[i].set_xticks([])\n    ax[i].set_yticks([])","cc21445c":"#importing the required modules\nfrom vis.visualization import visualize_activation\nfrom vis.utils import utils\nfrom keras import activations\nfrom keras import applications\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (18,6)\n#creating a VGG16 model using fully connected layers also because then we can \n#visualize the patterns for individual category\nfrom keras.applications import VGG16\nmodel = VGG16(weights='imagenet',include_top=True)\n\n#finding out the layer index using layer name\n#the find_layer_idx function accepts the model and name of layer as parameters and return the index of respective layer\nlayer_idx = utils.find_layer_idx(model,'predictions')\n#changing the activation of the layer to linear\nmodel.layers[layer_idx].activation = activations.linear\n#applying modifications to the model\nmodel = utils.apply_modifications(model)\n#Indian elephant\nimg3 = visualize_activation(model,layer_idx,filter_indices=385,max_iter=5000,verbose=True)\nplt.imshow(img3)","a0d2f180":"import numpy as np\n\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.activations import relu\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\ndef iter_occlusion(image, size=8):\n\n    occlusion = np.full((size * 5, size * 5, 1), [0.5], np.float32)\n    occlusion_center = np.full((size, size, 1), [0.5], np.float32)\n    occlusion_padding = size * 2\n\n    # print('padding...')\n    image_padded = np.pad(image, ( \\\n                        (occlusion_padding, occlusion_padding), (occlusion_padding, occlusion_padding), (0, 0) \\\n                        ), 'constant', constant_values = 0.0)\n\n    for y in range(occlusion_padding, image.shape[0] + occlusion_padding, size):\n\n        for x in range(occlusion_padding, image.shape[1] + occlusion_padding, size):\n            tmp = image_padded.copy()\n\n            tmp[y - occlusion_padding:y + occlusion_center.shape[0] + occlusion_padding, \\\n                x - occlusion_padding:x + occlusion_center.shape[1] + occlusion_padding] \\\n                = occlusion\n\n            tmp[y:y + occlusion_center.shape[0], x:x + occlusion_center.shape[1]] = occlusion_center\n\n            yield x - occlusion_padding, y - occlusion_padding, \\\n                  tmp[occlusion_padding:tmp.shape[0] - occlusion_padding, occlusion_padding:tmp.shape[1] - occlusion_padding]\n\n            \nfrom keras.preprocessing.image import load_img\n# load an image from file\nimage = load_img('car.jpeg', target_size=(224, 224))\nplt.imshow(image)\nplt.title('ORIGINAL IMAGE')\n\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\n# convert the image pixels to a numpy array\nimage = img_to_array(image)\n# reshape data for the model\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n# prepare the image for the VGG model\nimage = preprocess_input(image)\n# predict the probability across all output classes\nyhat = model.predict(image)\ntemp = image[0]\nprint(temp.shape)\nheatmap = np.zeros((224,224))\ncorrect_class = np.argmax(yhat)\nfor n,(x,y,image) in enumerate(iter_occlusion(temp,14)):\n    heatmap[x:x+14,y:y+14] = model.predict(image.reshape((1, image.shape[0], image.shape[1], image.shape[2])))[0][correct_class]\n    print(x,y,n,' - ',image.shape)\nheatmap1 = heatmap\/heatmap.max()\nplt.imshow(heatmap)\n\nimport skimage.io as io\n#creating mask from the standardised heatmap probabilities\nmask = heatmap1 < 0.85\nmask1 = mask *256\nmask = mask.astype(int)\nio.imshow(mask,cmap='gray')\n\nimport cv2\n#read the image\nimage = cv2.imread('car.jpeg')\nimage = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n#resize image to appropriate dimensions\nimage = cv2.resize(image,(224,224))\nmask = mask.astype('uint8')\n#apply the mask to the image\nfinal = cv2.bitwise_and(image,image,mask = mask)\nfinal = cv2.cvtColor(final,cv2.COLOR_BGR2RGB)\n#plot the final image\nplt.imshow(final)","c41d77e0":"# Utility to search for layer index by name. \n# Alternatively we can specify this as -1 since it corresponds to the last layer.\nlayer_idx = utils.find_layer_idx(model, 'predictions')\n\n# Swap softmax with linear\nmodel.layers[layer_idx].activation = activations.linear\nmodel = utils.apply_modifications(model)\n\n#generating saliency map with unguided backprop\ngrads1 = visualize_saliency(model, layer_idx,filter_indices=None,seed_input=image)\n#plotting the unguided saliency map\nplt.imshow(grads1,cmap='jet')","f913ae57":"#generating saliency map with guided backprop\ngrads2 =  visualize_saliency(model, layer_idx,filter_indices=None,seed_input=image,backprop_modifier='guided')\n#plotting the saliency map as heatmap\nplt.imshow(grads2,cmap='jet')","e701232d":"#importing required libraries and functions\nfrom keras.models import Model\n#defining names of layers from which we will take the output\nlayer_names = ['block1_conv1','block2_conv1','block3_conv1','block4_conv2']\noutputs = []\nimage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n#extracting the output and appending to outputs\nfor layer_name in layer_names:\n    intermediate_layer_model = Model(inputs=model.input,outputs=model.get_layer(layer_name).output)\n    intermediate_output = intermediate_layer_model.predict(image)\n    outputs.append(intermediate_output)\n#plotting the outputs\nfig,ax = plt.subplots(nrows=4,ncols=5,figsize=(20,20))\n\nfor i in range(4):\n    for z in range(5):\n        ax[i][z].imshow(outputs[i][0,:,:,z])\n        ax[i][z].set_title(layer_names[i])\n        ax[i][z].set_xticks([])\n        ax[i][z].set_yticks([])\nplt.savefig('layerwise_output.jpg')","6e608926":"# Class Activation Maps (Gradient Weighted)\n","3c9e26f6":"### Defining Custom Loss","af0ba7fa":"#### Example SWA Callback","87335e83":"# Normal Model (Both Cadene and Default) Runs","c1323724":"# Setting Notebook Defaults","0f90b6e4":"# Training Functions","70349c3e":"# InceptionResnet","6a5f555b":"# Finalising The Notebook","f2747025":"#### Probable Usage of SWA","85fd1273":"## Se-Net 154 - with Custom Loss, Precision, MixUp, Weight Decay, and Callbacks","72eebb68":"### Defining Custom Callbacks","a8710117":"\n## ResNext","9e18fa43":"## Dual Path Net","bb98342f":"# Getting The Big Guns out - One by one","5384feb7":"### Data Loaders with Transforms\n\n**Hyperparameters**\n1. Transformation Parameters -\n    1. Flip_vert,\n    2. max_lighting\n    3. max_zoom\n    4. max_warp\n        1. Default values - [\"False\",0.1,1.05,0.15]\n        2. Best - \n2. Data Loader Parameters -\n    1. Valid Pct (Def-0.2)\n    2. Image Size (Def- 140)\n    3. Batch Size\n    4. Normalize (Def-ImagenetStats)","819f3b05":"# Util Functions","7700acd9":"# Saliency Maps","210d2368":"# Layerwise Output"}}