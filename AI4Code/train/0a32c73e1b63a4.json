{"cell_type":{"e29437ed":"code","cbd8e7e5":"code","4146d498":"code","62d1efe7":"code","a1c5280e":"code","19f93b2a":"code","5824e1e0":"code","f4cf64f5":"code","f3279dde":"code","b10dfcef":"code","8366a15a":"code","0e1ef3f8":"code","d0d7a356":"code","5c5b8ed0":"code","63c23485":"code","7c463451":"code","a0e7e213":"code","75128e2c":"code","ee7aa8a0":"code","b2d8e296":"code","faa70130":"code","6fab5372":"code","3978e28f":"code","ce8cabcf":"code","17aab02e":"code","d735f0fa":"code","9f55ec7b":"code","a25bea36":"code","f8c2c099":"code","e0e63d28":"code","eeecf4e5":"code","643229ea":"code","472e5fc7":"code","07e5c8b4":"code","bdd360fa":"code","31a6bca1":"code","b9ded87f":"code","d754fca8":"code","b756c623":"code","7b57a9f5":"code","d601aebb":"code","d9f7f24f":"code","8a2dd122":"code","f5a5c847":"code","28c3ebe7":"code","bca09884":"code","5d5c1040":"code","a04c0e44":"code","86688083":"code","c793f6b5":"code","6423aa39":"code","0027a939":"code","6a1edea9":"code","357e4058":"code","706a7f22":"code","74fa8751":"code","8bbedfd7":"code","cb9e1c22":"code","c3c66327":"code","c66e04ed":"code","55960f8b":"code","f8c2dea4":"code","f409a0ad":"code","a70e6c90":"code","4c990254":"code","929d1ee0":"code","12a60371":"code","a4365560":"code","635992e4":"code","534889b9":"code","a695a25f":"code","b2c0ed10":"code","ead23de0":"code","f0d2fa35":"code","b4889cd0":"code","bf4265bb":"code","f64f26f8":"code","f3fe775d":"code","00310426":"code","fe8d287e":"code","5ac8c130":"code","96dbe40f":"code","aae35923":"code","b8ca582e":"code","b146b3be":"code","b8d94ffe":"code","57b76a72":"code","a25dadf8":"code","897c9734":"code","6f66ea2c":"code","43ae303e":"code","d37fe707":"code","77d05b4e":"code","f834e26c":"code","6a5ad094":"code","5a26e205":"code","396ede50":"code","76f2c7a5":"code","9c2bb2da":"markdown","7156175b":"markdown","30756191":"markdown","24646d4e":"markdown","23c1c698":"markdown","1f32ca83":"markdown","0af4fa94":"markdown","67fd70ad":"markdown","34995c2e":"markdown","3a98124b":"markdown","18486bbc":"markdown","4a90b1d9":"markdown","49cb60d7":"markdown","779b1aa8":"markdown","f040c8dc":"markdown","661a55b6":"markdown","6032a89e":"markdown","55929686":"markdown","7c2fde5b":"markdown","97e8bfe7":"markdown","e27c0e90":"markdown","616e0e37":"markdown","a7902da3":"markdown","7a95fd16":"markdown","82445515":"markdown","8d1d9f11":"markdown"},"source":{"e29437ed":"! pip install dtreeviz imbalanced-learn\n# Dtreeviz helps with decsion tree isualization and \n# imbalanced learn will help create a class-balanced dataset","cbd8e7e5":"import numpy as np \nimport pandas as pd  \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom fastai import *\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeRegressor\n#from dtreeviz.trees import *\nfrom IPython.display import Image\nimport joblib","4146d498":"train_df = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv', low_memory=False)","62d1efe7":"test_df = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv', low_memory=False)","a1c5280e":"train_df.drop(columns=\"id\", inplace = True) # Because the ID column is useless","19f93b2a":"dep_var = \"target\" ","5824e1e0":"import seaborn as sns\ntrain_df.describe().style.background_gradient(axis=1,cmap=sns.light_palette('green', as_cmap=True))","f4cf64f5":"plt.figure(figsize=(5, 30))\nplt.spy(train_df.drop(columns=\"target\").sample(n = 100),  markersize=1 )","f3279dde":"from imblearn.over_sampling import RandomOverSampler, SMOTE\n# from imblearn.under_sampling import NearMiss,RandomUnderSampler\nfrom collections import Counter\nfrom matplotlib import pyplot\nfrom numpy import where","b10dfcef":"y = train_df[\"target\"]","8366a15a":"x = train_df.drop(columns = \"target\")","0e1ef3f8":"counter = Counter(y)\nprint(counter)","d0d7a356":"train_df.target.hist()\n# Clearly there's a massive imbalance. \n# In my other non resampled trials, the valid dataset, even when using  \n# class balanced  split would only have class 2 and 3 in it, \n# and this lead to a lot of confusion\/underfitting. ","5c5b8ed0":"smote = SMOTE() # this initializes the function \n# I recently learned that generally all sklearn functions that start \n# with capital letters need to be initialized before being called","63c23485":"x_resampled, y_resampled = smote.fit_resample(x, y)\n","7c463451":"counter = Counter(y_resampled)\nprint(counter)","a0e7e213":" x_resampled['target'] = y_resampled #adding the y to the x ","75128e2c":"x_.target.value_counts()","ee7aa8a0":"x_.to_csv(\"resampled_train.csv\")","b2d8e296":"x_resampled = pd.read_csv(\"..\/input\/resampled-traincsv\/resampled_train.csv\")","faa70130":"x_resampled.target.hist() #Shows all classes are equal in number","6fab5372":"cont,cat = cont_cat_split(x_resampled, dep_var=dep_var)\n","3978e28f":"cat, cont","ce8cabcf":"splits = RandomSplitter()(range_of(x_resampled))","17aab02e":"to = TabularPandas(x_resampled,cat, cont, y_names=\"target\", splits=splits)\nlen(to.train),len(to.valid)","d735f0fa":"save_pickle('\/kaggle\/working\/to_resampled.pkl',to)","9f55ec7b":"to = load_pickle('\/kaggle\/working\/to_resampled.pkl')","a25bea36":"xs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y","f8c2c099":"m = DecisionTreeRegressor(max_leaf_nodes=6)\nm.fit(xs, y);","e0e63d28":"tree.export_graphviz(m, out_file='tree.dot', feature_names = xs.columns.tolist(),\n           rounded = True, proportion = False, precision = 3, filled = True)\n!dot -Tpng tree.dot -o tree.png \nfrom IPython.display import Image\nImage(filename = 'tree.png')","eeecf4e5":"samp_idx = np.random.permutation(len(y))[:1500]\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='monospace', scale=2, label_fontsize=10,\n        orientation='LR')","643229ea":" m.predict(valid_xs)","472e5fc7":"valid_y.unique()","07e5c8b4":"m.get_n_leaves(), len(xs)","bdd360fa":"xs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y","31a6bca1":"xs.shape","b9ded87f":"def rf(xs, y, n_estimators=1000, max_samples=183991,\n       max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestClassifier(n_jobs=-1, n_estimators=n_estimators,\n        max_samples=max_samples, max_features=max_features,\n        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)\n","d754fca8":"m = rf(xs, y)","b756c623":"m_probs = m.predict_proba(valid_xs)\n","7b57a9f5":"m_probs","d601aebb":"from sklearn.metrics import log_loss\nscore = log_loss(valid_y, m_probs)\nscore\n#That's a good score but maybe overfitted? ","d9f7f24f":"m.oob_score_","8a2dd122":"\nm.score(valid_xs, valid_y)","f5a5c847":"joblib.dump(m, \"\/kaggle\/working\/random_forest_resampled\")","28c3ebe7":"def rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)","bca09884":"fi = rf_feat_importance(m, xs)\nfi[:10]","5d5c1040":"def plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30]);","a04c0e44":"from scipy.cluster import hierarchy as hc\n\ndef cluster_columns(df, figsize=(10,16), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()","86688083":"cluster_columns(xs)\n","c793f6b5":"test_df= pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\")\ntest_df.drop(columns=\"id\", inplace =True)","6423aa39":"test_df+=1","0027a939":"# We need to feed this tree a tabular pandas object because \n# that's what we trained it using. \ndef load_pandas(fname):\n    \"Load in a `TabularPandas` object from `fname`\"\n    distrib_barrier()\n    res = pickle.load(open(fname, 'rb'))\n    return res\n","6a1edea9":"save_pickle('\/kaggle\/working\/to_resampled.pkl',to)","357e4058":"to_load = load_pandas('\/kaggle\/working\/to_resampled.pkl')","706a7f22":"to_new = to_load.train.new(test_df)","74fa8751":"testing_df = to_new.xs","8bbedfd7":"\ntest_preds= m.predict_proba(test_xs)\n","cb9e1c22":"submission= pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\nsubmission.iloc[:, 1:] = test_preds.data\nsubmission.to_csv('rf_resampled_smot.csv', index = False)","c3c66327":"kaggle competitions submit -c tabular-playground-series-may-2021 -f rf_zerodeleted.csv  -m \"RF with zeroes removed\"","c66e04ed":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(m, random_state=31).fit(valid_xs, valid_y)\narry = eli5.show_weights(perm, feature_names = valid_xs.columns.tolist())","55960f8b":"arry","f8c2dea4":"from sklearn.feature_selection import SelectFromModel\nsel = SelectFromModel(perm, threshold=0.0005, prefit=True)\nxs_trans = sel.transform(xs)\n","f409a0ad":"valid_xs_trans = sel.transform(valid_xs)","a70e6c90":"test_xs=  sel.transform(testing_df)","4c990254":"m = rf(xs_trans, y)","929d1ee0":"m.score(xs_trans, y)","12a60371":"m.oob_score_","a4365560":"m.score(valid_xs_trans, valid_y)","635992e4":"train_df_nn= pd.read_csv(\"..\/input\/resampled-traincsv\/resampled_train.csv\", low_memory=False, )","534889b9":"train_df_nn.drop(columns=\"Unnamed: 0\", inplace= True) # Iforgot to pass index =false while saving the file","a695a25f":"train_df_nn.head()","b2c0ed10":"train_df_nn.target.value_counts()","ead23de0":"dep_var = \"target\"","f0d2fa35":"cont_nn,cat_nn = cont_cat_split(train_df_nn, dep_var=dep_var)","b4889cd0":"cat_nn","bf4265bb":"procs = [Categorify, Normalize] ","f64f26f8":"splits = RandomSplitter()(range_of(train_df_nn))","f3fe775d":"to_nn = TabularPandas(train_df_nn, procs, cat_nn, cont_nn,\n                      splits=splits, y_names=dep_var)","00310426":"save_pickle(\"to_nn_resampeled.pkl\", to_nn)","fe8d287e":"to_nn","5ac8c130":"dls = to_nn.dataloaders(1024) \n# a Smaller batch size is probably better but this works\n# because tabular data doesn't take up much juice\n","96dbe40f":"y = to_nn.train.y\ny.min(),y.max()\n#Once we have the range of the Y, we can declare it to the learner","aae35923":"from sklearn.metrics import confusion_matrix, hamming_loss\n# The Hamming loss is the fraction of labels that are incorrectly predicted.\nhammingloss= HammingLoss() \n\n# In multilabel classification, Accuracy computes subset accuracy: the set of\n# labels predicted for a sample must exactly match the corresponding set \n# of labels in y_true.\n\n","b8ca582e":"learn = tabular_learner(dls,  y_range=(0,3),  wd=0.1,  \n                        metrics=[accuracy,hammingloss])\n ","b146b3be":"learn.lr_find()","b8d94ffe":"learn.fit_one_cycle(10, 3e-3)\n# As a rule of thumb, pick a LR that \n# is about 10 times lesser than the lowest LR","57b76a72":"learn.recorder.plot_loss()\n# As you can see, the loss hasn't levelled off,  and there doesnt seem \n#to be any over fitting, so we can run this for longer","a25dadf8":"learn.lr_find()","897c9734":"learn.fit_one_cycle(50, 8e-8) ","6f66ea2c":"learn.recorder.plot_loss()","43ae303e":"learn.lr_find()","d37fe707":"learn.fit_one_cycle(50, 4e-6)","77d05b4e":"learn.recorder.plot_loss()","f834e26c":"# Fastai Has a native confusion matrix method\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)","6a5ad094":"test_dl = dls.test_dl(test_df)","5a26e205":"test_preds, _ = learn.get_preds(dl = test_dl)\ntest_preds.shape","396ede50":"submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')\nsubmission.iloc[:, 1:] = test_preds.data","76f2c7a5":"submission.to_csv('nn_resamp_100ep.csv', index = False)","9c2bb2da":"Let's look at the feature importance","7156175b":"Te Kagge score for this entry was 1.31014 so the random forest actually did better","30756191":"IN learning about resampling, i found[ this article about SMOTE](https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/) very helpful. Here is a link to [the original paper on SMOTE](https:\/\/arxiv.org\/pdf\/1106.1813.pdf). The [imbalanced learn docs](https:\/\/imbalanced-learn.org\/stable\/over_sampling.html#smote-adasyn) are also great","24646d4e":"The  Kaggle score for this entry was : 1.28073","23c1c698":"OK, the valid loss has levelled off, and the train loss is showing no sign of reducing. more would just lead to overfitting.\n\nI have experimented with upto a 1000 ephocs with adjustments of the LR But the score doesn't significantly improve beyond this point.","1f32ca83":"I just finished Fast.ai's [Practical Deep Learning for Coders Part 1](https:\/\/course.fast.ai\/)\nand as a next step am trying to learn a little more via competitions.\nIn this notebook, I'll explore this dataset using methods discovered via the course. \nI have also run a bunch of other beginner-friendly methods and submitted, and I will post a comparison of the different methods.\n\nI first ran this whole thing without fixing the severe class imbalance. Then ran it again after fixing the imbalance. \n\nComments and tips on how to improve on this most welcome\n","0af4fa94":"seeing all the zeros, I added 1 as a constant to the whole dataframe (df+=1), and ran a random forest on that, which gave me my best manual leaderboard score so far 1.11039, my overall best scores are still from AUTOML. But here we go. ","67fd70ad":"## Random forest with resampled data","34995c2e":"Let's look at Permutation Importance. \n\n \nPermutation Importance predicts feature importance by looking at how much the score (accuracy, F1, decreases when a feature is not available.\n\nBut selecting based on this did not make much of a difference and actually worsened by scores, because i think the difference is 0.0002 and lesser for the worst performing ones. \n ","3a98124b":"What a cluster chart shows is how similar the various features are in importance. the earlier the split, the less similar the feature. In this case, none of the features seeem to be too similar as they all split up pretty early. This is useful in random forests because unlike other algorithms, RFs do better if you prune the featuers and remove redundant ones.","18486bbc":"## Basic EDA","4a90b1d9":"I am using a high number of n_estimators because while a lower number can get a good reduction in error rate,  proximity measures do not improve with lower numbers. sadly i havent been able to figure out how to plot them in python. ","49cb60d7":"## Resampling to fix class imbalance","779b1aa8":"### Run the resampled Model on test data","f040c8dc":"OOB score and the score for the tree are roughly the same, this means the's no major stuff missing or some skew in the random forest classifier","661a55b6":"We can see that the accuracy is best for \n`class_2` > `class_1` > `class_4` > `class_3` and that `class_3` is most frequently confused with `class_2`\n\nIf you remember the class imbalance, it was ({'Class_2': 57497, 'Class_3': 21420, 'Class_4': 12593, 'Class_1': 8490})`\nand we would have expected class_1 and class_4 to have worse classification. but SMOTE has helped. \nNow, the next way to improve this would be to maybe resample using other methods, over and under etc. But without knowing more about the data, it's difficult to do other data engineering, at my level.","6032a89e":"Fast.ai's tabular methods have an [automatic categorical data identifier or `cont_cat_split`](https:\/\/docs.fast.ai\/tabular.core.html#cont_cat_split) -- This function works by determining if a column is continuous or categorical based on the cardinality of its values. It's interesting to play around with. ","55929686":"Fast.ai's   `fit one cycle` learner is based on [Leslie Smith's 1cycle policy](https:\/\/arxiv.org\/pdf\/1803.09820.pdf) [Link to paper].  For a more graphical and intuitive explanation check out [Sylvain Gugger's post.](https:\/\/sgugger.github.io\/the-1cycle-policy.html). I find that this leads to much faster improvements than most other methods Ive tried","7c2fde5b":"There are a LOT OF ZEROs in this dataset","97e8bfe7":"Procs  is another fastai method, you can select what kind of data processing you would like the tabular data to go through and it will do it like a pipeiline. \n\n`Categorify`  takes every categorical variable and makes a map from integer to unique categories, then replace the values by the corresponding index.\n`FillMissing` will fill the missing values in the continuous variables by the median of existing values (you can choose a specific value if you prefer)\n`Normalize` will normalize the continuous variables (substract the mean and divide by the std)\n\n","e27c0e90":"These features are so very different from the ones that the decision tree found. \nINterestingly, in my experiments with LGBoost, Tabular NN, XGboost and CatBoost, roughly the same features seem to be the most interestnig.","616e0e37":"## Fastai Tabular Data prep","a7902da3":"Tabular learner creates a NN customized for your data and automatically  picks up most things that need to be otherwise delcared.\n","7a95fd16":"## Decision Tree ","82445515":"## FastAI Tabular NN Trial with resampled data","8d1d9f11":"What's interesting is that these graphs gives us some idea about the feature importance\nas found by decision tree, and they are so very different from the ones found by randomforest \nand by other methods"}}