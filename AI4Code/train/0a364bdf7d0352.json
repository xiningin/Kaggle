{"cell_type":{"65d97dda":"code","7542f8e7":"code","9eb77da0":"code","098492a4":"code","3192a7e2":"code","dbce88f5":"code","e3c3d347":"code","d8fa75bd":"code","0c2d8707":"code","4f54fae6":"code","e7d1a4be":"code","7fb04a19":"code","a7cf5756":"code","df8bc713":"code","2b263802":"code","68581321":"code","0b01f995":"code","16330bf2":"code","a04c7741":"code","1760844f":"code","7ed47fea":"code","76d6fe46":"markdown","e1d7e0ce":"markdown","72b6044a":"markdown","c8e6b0dc":"markdown","b1a02483":"markdown","b6b600b7":"markdown","954e48f7":"markdown","7029637b":"markdown","8d409f43":"markdown","3a3a52f5":"markdown","1110c642":"markdown","d688fefd":"markdown","48e00b90":"markdown","d9e46de0":"markdown","2fee46e3":"markdown","dc773564":"markdown","416443db":"markdown","17fe6bb6":"markdown"},"source":{"65d97dda":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))\n\n\n# Any results you write to the current directory are saved as output.","7542f8e7":"# Lets read the data into a dataframe\ndf = pd.read_csv('..\/input\/creditcard.csv')\ndf.head()","9eb77da0":"# split into label and features\ntarget_label = df['Class']\ntarget_label.value_counts()\nX = df.iloc[:,:-1]\ndf.info() # there are 284806 rows with 31 columns including the target label 'Class' in the provided data.","098492a4":"# Now lets look at the correlationin the data. \n# Prior to that we will split out data into training and testing set.\nfrom sklearn.model_selection import train_test_split \ntrain_X, test_X, train_y, test_y = train_test_split(X,target_label,test_size=0.3,random_state=42)\n# find the correlation between the different variables.\ncorr_mtx = df.corr()\ncorr_mtx\nprint(corr_mtx['Class'].sort_values(ascending = False)) \n# V11 thru V27 are the features which have the most correlation impact on Class, and there are attributes which are negatively \n# correlated as well\n\n    ","3192a7e2":"# Here we will plot a hist of all features to see the spread of data.\n# doing a visual on data helps to further understand the data.\nimport matplotlib.pyplot as plt\nX.hist(figsize=(20,21))\nplt.show()","dbce88f5":"# Separate out Fraud & Non-Fraud Data, and split to get train & test set.\nnon_fraud_data = df[df.Class == 0]\nfraud_data = df[df.Class == 1]\nnp.unique(non_fraud_data.Class)\n\n\nnon_fraud_label = non_fraud_data['Class']\nnon_fraud_X = non_fraud_data.iloc[:,:-1]\nnon_fraud_X.head()\n\nfraud_label = fraud_data['Class']\nfraud_X = fraud_data.iloc[:,:-1]\n\nnon_train_X, non_test_X, non_train_y, non_test_y = train_test_split(non_fraud_X,non_fraud_label,test_size=0.3,random_state=42)\n","e3c3d347":"# Lets look at Kmeans...\n# Lets see whether we can segregate data in Clusters.\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\n\nks = range(1, 6)\ninertias = []\nfor k in ks:\n    # Create a KMeans instance with k clusters: model\n    model = KMeans(n_clusters=k)\n   # Fit model to samples\n    model.fit(non_train_X)\n   # Append the inertia to the list of inertias\n    inertias.append(model.inertia_)\n    \n# Plot ks vs inertias\nplt.plot(ks, inertias, '-o')\nplt.xlabel('number of clusters, k')\nplt.ylabel('inertia')\nplt.xticks(ks)\nplt.show()","d8fa75bd":"# lets use three clusters..\n\nfrom sklearn.cluster import KMeans\n\nmodel = KMeans(n_clusters=3, random_state=42)\nk_labels = model.fit_predict(non_fraud_X,non_fraud_label)\n","0c2d8707":"print('len of fraud', len(fraud_X))\nprint('len of non_fraud_X', len(non_fraud_X))\nprint('len of df', len(df))\n","4f54fae6":"fraud_predict_labels = model.predict(fraud_X)\nnp.unique(fraud_predict_labels)\n\nlen(fraud_predict_labels[fraud_predict_labels < 0])\n\n","e7d1a4be":"# compute outlier_fraction for the model here\noutlier_fraction = len(fraud_X)\/len(df)\nprint(outlier_fraction)\n\nfrom sklearn.ensemble import IsolationForest\nclf = IsolationForest(n_estimators=10, max_samples= len(train_X),contamination = outlier_fraction,n_jobs=5, random_state=42, behaviour ='new')\nclf.fit(train_X)\nscore = clf.decision_function(train_X)\n\ny_pred_train = clf.predict(train_X)\ny_pred_test = clf.predict(test_X)\n","7fb04a19":"# lets try to build confusion matrix and get precision and accuracy scores.\ny_pred_test[y_pred_test == 1] = 0\ny_pred_test[y_pred_test == -1] = 1\nnp.unique(y_pred_test)\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\ncnf_mtrx = confusion_matrix(test_y,y_pred_test)\ncnf_mtrx","a7cf5756":"accuracy_score(train_y,y_pred_train)\n############################################################################################################################\n# Calculate precison & recall. \n# Precision is actual positive prediction\/ total positive prediction\n# Recall is actual positive prediction\/ total actual positives\n############################################################################################################################\nprecision = cnf_mtrx[1,1]\/(cnf_mtrx[1,1]+cnf_mtrx[0,1])\nrecall = cnf_mtrx[1,1]\/(cnf_mtrx[1,1]+cnf_mtrx[1,0])\nprint(\"precision is {0}, and recall is {1}\".format(precision,recall))","df8bc713":"from sklearn.ensemble import AdaBoostClassifier\n\nclf_ada = AdaBoostClassifier(n_estimators=100, random_state=42)\nclf_ada.fit(train_X,train_y) ","2b263802":"test_y_ada_predict = clf_ada.predict(test_X)\n\n# lets build confusion matrix\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\nada_test_cnf_mtrx = confusion_matrix(test_y,test_y_ada_predict)\nada_test_cnf_mtrx","68581321":"##Find precision and recall for AdaBoost Model\nprecision = ada_test_cnf_mtrx[1,1]\/(ada_test_cnf_mtrx[1,1]+ada_test_cnf_mtrx[0,1])\nrecall = ada_test_cnf_mtrx[1,1]\/(ada_test_cnf_mtrx[1,1]+ada_test_cnf_mtrx[1,0])\nprint(\"precision is {0}, and recall is {1}\".format(precision,recall))","0b01f995":"from sklearn.linear_model import LogisticRegression\n\nlg_clf = LogisticRegression(penalty='l2',tol=0.0001,random_state=42)\nlg_clf.fit(train_X,train_y)","16330bf2":"test_y_lg_predict = lg_clf.predict(test_X)\n\n# lets build confusion matrix\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n\nlg_test_cnf_mtrx = confusion_matrix(test_y,test_y_lg_predict)\nlg_test_cnf_mtrx","a04c7741":"##Find precision and recall for Logistic Regression Model\nprecision = lg_test_cnf_mtrx[1,1]\/(lg_test_cnf_mtrx[1,1]+lg_test_cnf_mtrx[0,1])\nrecall = lg_test_cnf_mtrx[1,1]\/(lg_test_cnf_mtrx[1,1]+lg_test_cnf_mtrx[1,0])\nprint(\"precision is {0}, and recall is {1}\".format(precision,recall))","1760844f":"from keras import  backend as K\n\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense,Activation, Flatten, Dropout\nfrom keras.optimizers import Adam,RMSprop, SGD\n\nfrom keras.utils import np_utils\nimport numpy as np\n\nn_cols = train_X.shape[1]\ny_train = np_utils.to_categorical(train_y,2)\ny_test = np_utils.to_categorical(test_y,2)\nprint('shape is',n_cols)\n\nmodel = Sequential()\nmodel.add(Dense(128,activation='relu',input_shape=(n_cols,)))\nmodel.add(Dense(2,activation='softmax'))\nmodel.summary()\nmodel.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\nhistory = model.fit(train_X,y_train,batch_size=1000,epochs=200,verbose='VERBOSE',validation_split = 0.2)\n\n","7ed47fea":"score1 = model.evaluate(train_X,y_train)\nprint('train score',score1[0])\nprint('train accuracy',score1[1])\n\nscore = model.evaluate(test_X,y_test)\nprint('test score',score[0])\nprint('test accuracy',score[1])","76d6fe46":"Lets use a simple Logistic Regression Model and check our precision.\n\n**Logistic Regression**","e1d7e0ce":"Now..Lets do DeepLearning using Keras and TensorFlow as our backend.\nI have built a very simple model with no hidden layers.\nI have 128 neuron for the input layer, and output layer has 2 neurons (0 or 1) with softmax activation..\nI have used to_categorical method to change to binary output for train_y & test_y label array.\nI have used sgd optimizer, and then I run the model...","72b6044a":"A simple Log Regression model has given us a precision of 69% and recall of 60%.\nNow lets dive into Deep Learning Model, and see how further it can take us......","c8e6b0dc":"Only 44 fraud's have been detected by our model, and 93 have been missed whereas 92 have been incorrectly classified as fraud by our model.","b1a02483":"Here I am plotting feature data, and seeing their spread. Histogram will help us to do that. :)","b6b600b7":"Lets look at correlation of features. This will tell us how the features are correlated with each other. \nCorrelation gives us a intution on which variables are important, and have impact on the predicted class.","954e48f7":"Its good to always do df.info() & df.describe()\nThese methods help you to find out whether there are any invalid\/empty data, and how the data is spread as well.","7029637b":"**WOW!!! Blown away....**\ntrain accuracy is 99.82% whereas test accuracy is 99.84%.\n\nI am satisfied with this outcome. Question is, **Are You?**","8d409f43":"**WOW!!** If you were excited about 32% accuracy with IsolationForest, AdaBoost has been able to give us 87% accuracy. We will able to predict 87 out of 100 Fraud cases, and can potentially stop them before happening :)","3a3a52f5":"Lets change our gear to using Supervised Learning. We will use ADABOOST & Logistic Regression, and see where it gets us...\n\n**ADABOOST**","1110c642":"Lets try to find accuracy and precision of our model for the test data.\n\nFirst we have to align both the prediction and the observation. To match with Observation categorical values, we change predictions values to '1', when it is fraud, and '0' when it is not a fraud.\nWe will build a confusion matrix, and then calculate our precision and accuracy values.","d688fefd":"From the above plot of inertia with number of clusters, we can see that the inertial change is smaller post the 3 clusters. We will now build our model for three clusters, and use it for predictions.","48e00b90":"Precision and recall both are around 32%. Although, this may seem low, but the results are fantastic! \nModel is now able to detect 32% of fraud cases.","d9e46de0":"Lets try our first model, and see where it gets us..\n\n**KMEANS**\n\nWe will try with KMeans Clustering algorithm. We will seggregate the data in different clusters, and see whether the fraud gets segregated differently...Also, I will train the model using the non-fraud data (meaning, I will remove the fraud data out, and train the model with non fraud data only). Then we will feed in fraud data, and see how the prediction is..\n\nYou can feed in the entire data set, including Fraud, and see how the model behaves. \n\nIn the below step, I am separating fraud from non-fraud, and creating train and set set for non fraud data.","2fee46e3":"**Lets do an Isolation Forest. Isolation Forest is used to find anamoly.******","dc773564":"KMeans was not really helpful here. \nOfcourse, you can try including fraud data when you are training the model, and see how it works.\nThe predictions has classified them in one of the existing clusters. Not much helpful.\nSo lets move on to other predictor models.","416443db":"**PLEASE UP VOTE ME!!!!**\n\nIn this Kernel, I will take up a use case for identifying anamoly in the data, and thereby being able to predict anamoly in data. I will take a use case of credit card fraud data.\n\nFirstly, we will have a look at our provided data, and glean insights by using data exploratory and visualization tools.\n\nThen we will explore different ways to look for anamolies, and compare & contrast between them. We will start with Unsupervised learning, in that we will develop KMeans Cluster & IsolationForest Model, and use them to predict anamolies. After that, we will use Supervised learning methods, in that we will use gradient boost & Logistic regression to predict anamolies.\n\nFinally, we will dive into deep learning methods. We will build a deep sequential model, and predict anamolies.\n\nThis Kernel is for a beginner to understand how to work through different steps in building a model, and selecting the right one.\n\n**Have Fun!!! - Lijesh Shetty..**","17fe6bb6":"Lets read data from the credit card csv file. \nYou can read more about this data, but essentially the dimension of the data has been reduced by applying PCA (Principal Component Analysis). "}}