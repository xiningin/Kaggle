{"cell_type":{"60c5c736":"code","28082391":"code","2d22ce38":"code","0eec82af":"code","670c03ea":"code","b074f758":"code","3c4e7cdb":"code","19016cc0":"code","240629d8":"code","90b8980d":"code","806fb899":"code","3d2cb489":"code","b4fb74d3":"code","b2b6a5f3":"code","8ad4b22d":"code","20872c83":"code","3994b6cd":"code","e03db152":"code","3fe55686":"code","f6baae00":"code","0947b0ba":"code","fe4f7dd1":"code","48835023":"code","ff3cb2eb":"code","a3fb5674":"code","74c0e5ac":"code","dfb7e19e":"code","96635c3f":"code","a9b2b322":"code","ad8b7d58":"code","ab36a8ab":"code","c61730fb":"code","3353408b":"markdown","e183cd12":"markdown","1251341f":"markdown","5cf8a6db":"markdown","7bb74870":"markdown","9dcc2e7c":"markdown","383df7a4":"markdown","872d3246":"markdown","d5418150":"markdown","f871886a":"markdown","1f969581":"markdown","b1e67d1b":"markdown","920e59df":"markdown","bf09069b":"markdown","35e122ab":"markdown","439a6c63":"markdown","df99faa1":"markdown","2c4e25b8":"markdown","1ba3500d":"markdown","3f71e514":"markdown","570b50db":"markdown","097ed366":"markdown","9956ee1f":"markdown","ff7ff99a":"markdown","d131f643":"markdown","031ed257":"markdown","36d05ba3":"markdown","df7f3c27":"markdown","c114cfef":"markdown","701c57a7":"markdown","9e480f3c":"markdown","65342152":"markdown","1a975134":"markdown","3241fa95":"markdown","21365ede":"markdown","af003f7b":"markdown","74e5ff85":"markdown","24b5b166":"markdown","de8d2299":"markdown","dee79e32":"markdown","9841091f":"markdown"},"source":{"60c5c736":"import pandas as pd # package for high-performance, easy-to-use data structures and data analysis\nimport numpy as np # fundamental package for scientific computing with Python\nimport matplotlib\nimport matplotlib.pyplot as plt # for plotting\nimport seaborn as sns # for making plots with seaborn\ncolor = sns.color_palette()\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\noffline.init_notebook_mode()\n#import cufflinks and offline mode\nimport cufflinks as cf\ncf.go_offline()\n\n# Venn diagram\nfrom matplotlib_venn import venn2\nimport re\nimport nltk\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport string\neng_stopwords = stopwords.words('english')\nimport gc\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD","28082391":"import os\nprint(os.listdir(\"..\/input\/google-quest-challenge\"))","2d22ce38":"print('Reading data...')\ntrain_data = pd.read_csv('..\/input\/google-quest-challenge\/train.csv')\ntest_data = pd.read_csv('..\/input\/google-quest-challenge\/test.csv')\nsample_submission = pd.read_csv('..\/input\/google-quest-challenge\/sample_submission.csv')\nprint('Reading data completed')","0eec82af":"print('Size of train_data', train_data.shape)\nprint('Size of test_data', test_data.shape)\nprint('Size of sample_submission', sample_submission.shape)","670c03ea":"train_data.head()","b074f758":"train_data.columns","3c4e7cdb":"test_data.head()","19016cc0":"test_data.columns","240629d8":"sample_submission.head()","90b8980d":"targets = list(sample_submission.columns[1:])\ntargets","806fb899":"train_data[targets].describe()","3d2cb489":"# checking missing data\ntotal = train_data.isnull().sum().sort_values(ascending = False)\npercent = (train_data.isnull().sum()\/train_data.isnull().count()*100).sort_values(ascending = False)\nmissing_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_train_data.head()","b4fb74d3":"# checking missing data\ntotal = test_data.isnull().sum().sort_values(ascending = False)\npercent = (test_data.isnull().sum()\/test_data.isnull().count()*100).sort_values(ascending = False)\nmissing_test_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_test_data.head()","b2b6a5f3":"temp = train_data[\"host\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\ndf.iplot(kind='pie',labels='labels',values='values', title='Distribution of hosts in Training data')","8ad4b22d":"temp = test_data[\"host\"].value_counts()\ndf = pd.DataFrame({'labels': temp.index,\n                   'values': temp.values\n                  })\ndf.iplot(kind='pie',labels='labels',values='values', title='Distribution of hosts in test data')","20872c83":"temp = train_data[\"category\"].value_counts()\n#print(\"Total number of states : \",len(temp))\ntrace = go.Bar(\n    x = temp.index,\n    y = (temp \/ temp.sum())*100,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of categories in training data in % \",\n    xaxis=dict(\n        title='category',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count in %',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='test')","3994b6cd":"temp = test_data[\"category\"].value_counts()\n#print(\"Total number of states : \",len(temp))\ntrace = go.Bar(\n    x = temp.index,\n    y = (temp \/ temp.sum())*100,\n)\ndata = [trace]\nlayout = go.Layout(\n    title = \"Distribution of categories in test data in % \",\n    xaxis=dict(\n        title='category',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count in %',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='test')","e03db152":"fig, axes = plt.subplots(6, 5, figsize=(18, 15))\naxes = axes.ravel()\nbins = np.linspace(0, 1, 20)\n\nfor i, col in enumerate(targets):\n    ax = axes[i]\n    sns.distplot(train_data[col], label=col, kde=False, bins=bins, ax=ax)\n    # ax.set_title(col)\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 6079])\nplt.tight_layout()\nplt.show()\nplt.close()","3fe55686":"plt.figure(figsize=(23,13))\n\nplt.subplot(321)\nvenn2([set(train_data.question_user_name.unique()), set(test_data.question_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common question_user_name in training and test data\", fontsize=15)\n#plt.show()\n\n#plt.figure(figsize=(15,8))\nplt.subplot(322)\nvenn2([set(train_data.answer_user_name.unique()), set(test_data.answer_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common answer_user_name in training and test data\", fontsize=15)\n#plt.show()\n\n#plt.figure(figsize=(15,8))\nplt.subplot(323)\nvenn2([set(train_data.question_title.unique()), set(test_data.question_title.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common question_title in training and test data\", fontsize=15)\n#plt.show()\n\n#plt.figure(figsize=(15,8))\nplt.subplot(324)\nvenn2([set(train_data.question_user_name.unique()), set(train_data.answer_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common users in both question & answeer in train data\", fontsize=15)\n\n#plt.figure(figsize=(15,8))\nplt.subplot(325)\nvenn2([set(test_data.question_user_name.unique()), set(test_data.answer_user_name.unique())], set_labels = ('Train set', 'Test set') )\nplt.title(\"Common users in both question & answeer in test data\", fontsize=15)\n\nplt.subplots_adjust(wspace = 0.5, hspace = 0.5,\n                    top = 0.9)\nplt.show()","f6baae00":"train_question_title=train_data['question_title'].str.len()\ntest_question_title=test_data['question_title'].str.len()\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,6))\nsns.distplot(train_question_title,ax=ax1,color='blue')\nsns.distplot(test_question_title,ax=ax2,color='green')\nax2.set_title('Distribution for Question Title in test data')\nax1.set_title('Distribution for Question Title in Training data')\nplt.show()","0947b0ba":"train_question_title=train_data['question_body'].str.len()\ntest_question_title=test_data['question_body'].str.len()\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,6))\nsns.distplot(train_question_title,ax=ax1,color='blue')\nsns.distplot(test_question_title,ax=ax2,color='green')\nax2.set_title('Distribution for Question Body in test data')\nax1.set_title('Distribution for Question Body in Training data')\nplt.show()","fe4f7dd1":"train_question_title=train_data['answer'].str.len()\ntest_question_title=test_data['answer'].str.len()\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,6))\nsns.distplot(train_question_title,ax=ax1,color='blue')\nsns.distplot(test_question_title,ax=ax2,color='green')\nax2.set_title('Distribution for Answers in test data')\nax1.set_title('Distribution for Answers in Training data')\nplt.show()","48835023":"# Duplicate Questions\nprint(\"Number of duplicate questions in descending order\")\nprint(\"------------------------------------------------------\")\ntrain_data.groupby('question_title').count()['qa_id'].sort_values(ascending=False).head(25)","ff3cb2eb":"train_data[train_data['question_title'] == 'What is the best introductory Bayesian statistics textbook?']","a3fb5674":"#https:\/\/www.kaggle.com\/urvishp80\/quest-encoding-ensemble\nprint(\"Data cleaning started........\")\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\",\n\"tryin'\":\"trying\"}\n\n\ndef clean_text(text):\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = text.lower().split()\n    stops = set(stopwords.words(\"english\"))\n    text = [w for w in text if not w in stops]    \n    text = \" \".join(text)\n    return(text)\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\ndef replace_typical_misspell(text):\n    mispellings, mispellings_re = _get_mispell(mispell_dict)\n\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\ndef clean_data(df, columns: list):\n    for col in columns:\n        df[col] = df[col].apply(lambda x: clean_text(x.lower()))\n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n\n    return df","74c0e5ac":"columns = ['question_title','question_body','answer']\ntrain_data = clean_data(train_data, columns)\ntest_data = clean_data(test_data, columns)\nprint(\"Data cleaning Done........\")","dfb7e19e":"# training data\nfreq_dist = FreqDist([word for text in train_data['question_title'].str.replace('[^a-za-z0-9^,!.\\\/+-=]',' ') for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Training Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()\n\n# test data\nfreq_dist = FreqDist([word for text in test_data['question_title'] for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Test Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","96635c3f":"# training data\nfreq_dist = FreqDist([word for text in train_data['question_body'].str.replace('[^a-za-z0-9^,!.\\\/+-=]',' ') for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question body (Training Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()\n\n# test data\nfreq_dist = FreqDist([word for text in test_data['question_body'] for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question body (Test Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","a9b2b322":"# training data\nfreq_dist = FreqDist([word for text in train_data['question_title'] for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Training Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()\n\n# test data\nfreq_dist = FreqDist([word for text in test_data['question_title'] for word in text.split()])\nplt.figure(figsize=(20, 7))\nplt.title('Word frequency on question title (Test Data)').set_fontsize(25)\nplt.xlabel('').set_fontsize(25)\nplt.ylabel('').set_fontsize(25)\nfreq_dist.plot(60,cumulative=False)\nplt.show()","ad8b7d58":"# Number of characters in the text\ntrain_data[\"question_title_num_chars\"] = train_data[\"question_title\"].apply(lambda x: len(str(x)))\ntrain_data[\"question_body_num_chars\"] = train_data[\"question_body\"].apply(lambda x: len(str(x)))\ntrain_data[\"answer_num_chars\"] = train_data[\"answer\"].apply(lambda x: len(str(x)))\n\ntest_data[\"question_title_num_chars\"] = test_data[\"question_title\"].apply(lambda x: len(str(x)))\ntest_data[\"question_body_num_chars\"] = test_data[\"question_body\"].apply(lambda x: len(str(x)))\ntest_data[\"answer_num_chars\"] = test_data[\"answer\"].apply(lambda x: len(str(x)))\n\n# Number of words in the text\ntrain_data[\"question_title_num_words\"] = train_data[\"question_title\"].apply(lambda x: len(str(x).split()))\ntrain_data[\"question_body_num_words\"] = train_data[\"question_body\"].apply(lambda x: len(str(x).split()))\ntrain_data[\"answer_num_words\"] = train_data[\"answer\"].apply(lambda x: len(str(x).split()))\n\ntest_data[\"question_title_num_words\"] = test_data[\"question_title\"].apply(lambda x: len(str(x).split()))\ntest_data[\"question_body_num_words\"] = test_data[\"question_body\"].apply(lambda x: len(str(x).split()))\ntest_data[\"answer_num_words\"] = test_data[\"answer\"].apply(lambda x: len(str(x).split()))\n\n# Number of unique words in the text\ntrain_data[\"question_title_num_unique_words\"] = train_data[\"question_title\"].apply(lambda x: len(set(str(x).split())))\ntrain_data[\"question_body_num_unique_words\"] = train_data[\"question_body\"].apply(lambda x: len(set(str(x).split())))\ntrain_data[\"answer_num_unique_words\"] = train_data[\"answer\"].apply(lambda x: len(set(str(x).split())))\n\ntest_data[\"question_title_num_unique_words\"] = test_data[\"question_title\"].apply(lambda x: len(set(str(x).split())))\ntest_data[\"question_body_num_unique_words\"] = test_data[\"question_body\"].apply(lambda x: len(set(str(x).split())))\ntest_data[\"answer_num_unique_words\"] = test_data[\"answer\"].apply(lambda x: len(set(str(x).split())))","ab36a8ab":"tfidf = TfidfVectorizer(ngram_range=(1, 3))\ntsvd = TruncatedSVD(n_components = 128, n_iter=5)\ntfquestion_title = tfidf.fit_transform(train_data[\"question_title\"].values)\ntfquestion_title_test = tfidf.transform(test_data[\"question_title\"].values)\ntfquestion_title = tsvd.fit_transform(tfquestion_title)\ntfquestion_title_test = tsvd.transform(tfquestion_title_test)\n\ntfquestion_body = tfidf.fit_transform(train_data[\"question_body\"].values)\ntfquestion_body_test = tfidf.transform(test_data[\"question_body\"].values)\ntfquestion_body = tsvd.fit_transform(tfquestion_body)\ntfquestion_body_test = tsvd.transform(tfquestion_body_test)\n\ntfanswer = tfidf.fit_transform(train_data[\"answer\"].values)\ntfanswer_test = tfidf.transform(test_data[\"answer\"].values)\ntfanswer = tsvd.fit_transform(tfanswer)\ntfanswer_test = tsvd.transform(tfanswer_test)","c61730fb":"train_data[\"tfquestion_title\"] = list(tfquestion_title)\ntest_data[\"tfquestion_title_test\"] = list(tfquestion_title_test)\n\ntrain_data[\"tfquestion_body\"] = list(tfquestion_body)\ntest_data[\"tfquestion_body_test\"] = list(tfquestion_body_test)\n\ntrain_data[\"tfanswer\"] = list(tfanswer)\ntest_data[\"tfanswer_test\"] = list(tfanswer_test)","3353408b":"## <a id='3-2'> 3.2 Statistical overview of the Data<\/a>","e183cd12":"# <a id='3'>3. Glimpse of Data<\/a>","1251341f":"## <a id='5-8'>5.8 Duplicate Questions Title & Most popular Questions<\/a>","5cf8a6db":"\n# More To Come. Stay Tuned. !!\nIf there are any suggestions\/changes you would like to see in the Kernel please let me know :). Appreciate every ounce of help!\n\n**This notebook will always be a work in progress.** Please leave any comments about further improvements to the notebook! Any feedback or constructive criticism is greatly appreciated!. **If you like it or it helps you , you can upvote and\/or leave a comment :).**\n","7bb74870":"**checking missing data in test_data **","9dcc2e7c":"# <a id='5-6'>5.6 Distribution for Question body<\/a>","383df7a4":"**test_data**","872d3246":"**checking missing data in train_data **","d5418150":"# <a id='5-1'>5.1 Distribution of Host(from which website Question & Answers collected)<\/a>","f871886a":"### <a id='6-3-1'>6.3.1 Text based features<\/a>","1f969581":" # <a id='2'>2. Retrieving the Data<\/a>","b1e67d1b":"# <a id='5-4'>5.4 Venn Diagram(Common Features values in training and test data)<\/a>","920e59df":"## <a id='6-2'>6.2 Word frequency<\/a>","bf09069b":"**sample_submission**","35e122ab":"# <a id='5-3'>5.3 Distribution of Target variables<\/a>","439a6c63":"# <a id='5-5'>5.5 Distribution for Question Title<\/a>","df99faa1":"# <a id='5'>5. Data Exploration<\/a>","2c4e25b8":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/7968\/logos\/thumb76_76.png?t=2017-12-01-22-32-31)","1ba3500d":"# More To Come. Stay Tuned. !!","3f71e514":"Text based features are :\n * Number of characters in the question_title\n * Number of characters in the question_body\n * Number of characters in the answer\n * Number of words in the question_title\n * Number of words in the question_body\n * Number of words in the answer\n * Number of unique words in the question_title\n * Number of unique words in the question_body\n * Number of unique words in the answer","570b50db":"**train_data columns**","097ed366":"## <a id='3-1'>3.1 Overview of tables<\/a>","9956ee1f":"* **A Venn diagram uses overlapping circles or other shapes to illustrate the logical relationships between two or more sets of items. Often, they serve to graphically organize things, highlighting how the items are similar and different.**","ff7ff99a":"\n- <a href='#1'>1. Introduction<\/a>  \n- <a href='#2'>2. Retrieving the Data<\/a>\n     - <a href='#2-1'>2.1 Load libraries<\/a>\n     - <a href='#2-2'>2.2 Read the Data<\/a>\n- <a href='#3'>3. Glimpse of Data<\/a>\n     - <a href='#3-1'>3.1 Overview of tables<\/a>\n     - <a href='#3-2'>3.2 Statistical overview of the Data<\/a>\n- <a href='#4'>4. Check for missing data<\/a>\n- <a href='#5'>5. Data Exploration<\/a>\n    - <a href='#5-1'>5.1 Distribution of Host(from which website Question & Answers collected)<\/a>\n    - <a href='#5-2'>5.2 Distribution of categories<\/a>\n    - <a href='#5-3'>5.3 Distribution of Target variables<\/a>\n    - <a href='#5-4'>5.4 Venn Diagram(Common Features values in training and test data)<\/a>\n    - <a href='#5-5'>5.5 Distribution of Question Title<\/a>\n    - <a href='#5-6'>5.6 Distribution of Question Body<\/a>\n    - <a href='#5-7'>5.7 Distribution of Answers<\/a>\n    - <a href='#5-8'>5.8 Duplicate Questions Title & Most popular Questions<\/a>\n- <a href='#6'>6. Data Preparation & Feature Engineering<\/a>\n    - <a href='#6-1'>6.1 Data Cleaning<\/a>\n    - <a href='#6-2'>6.2 Feature Engineering<\/a>\n        - <a href='#6-2-1'>6.2.1 Text Based Features<\/a>\n        - <a href='#6-2-2'>6.2.2 TF-IDF Features<\/a>","d131f643":"## <a id='6-3'>6.3 Feature Engineering<\/a>","031ed257":" ## <a id='2-1'>2.1 Load libraries<\/a>","36d05ba3":"#### TF-IDF :\n  *  Term Frequency (TF) and Inverse Document Frequency (IDF)\n  *  TF(t) = (Number of times term t appears in a document) \/ (Total number of terms in the document)\n  *  IDF(t) = log_e(Total number of documents \/ Number of documents with term t in it)\n  \n**TF-IDF based features are :**\n\n* Character Level N-Gram TF-IDF of question_title\n* Character Level N-Gram TF-IDF of question_body\n* Character Level N-Gram TF-IDF of answer\n* Word Level N-Gram TF-IDF of question_title\n* Word Level N-Gram TF-IDF of question_body\n* Word Level N-Gram TF-IDF of answer","df7f3c27":"## <a id='6-1'>6.1 Data cleaning<\/a>","c114cfef":"# <a id='5-7'>5.7 Distribution for Answers<\/a>","701c57a7":"# <a id='6'>6. Data Preparation & Feature Engineering<\/a>","9e480f3c":"# <a id='2-2'>2.2 Reading Data<\/a>","65342152":"# <a id='4'> 4 Check for missing data<\/a>","1a975134":"**test_data columns**","3241fa95":"**Most popular questions**","21365ede":"# <a id='5-2'>5.2 Distribution of categories<\/a>","af003f7b":"**train_data**","74e5ff85":"### <a id='6-3-2'>6.3.2 TF-IDF Features<\/a>","24b5b166":"In this competition, you\u2019re challenged to use this new dataset to build predictive algorithms for different subjective aspects of question-answering. The question-answer pairs were gathered from nearly 70 different websites, in a \"common-sense\" fashion. Our raters received minimal guidance and training, and relied largely on their subjective interpretation of the prompts. As such, each prompt was crafted in the most intuitive fashion so that raters could simply use their common-sense to complete the task. By lessening our dependency on complicated and opaque rating guidelines, we hope to increase the re-use value of this data set. What you see is what you get!","de8d2299":"**Target variables**","dee79e32":"# <a id='1'>1. Introduction<\/a>","9841091f":"![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/google-research\/human_computable_dimensions_1.png)"}}