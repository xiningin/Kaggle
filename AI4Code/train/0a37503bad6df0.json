{"cell_type":{"5d36f6d8":"code","51d22805":"code","7fdf0485":"code","c680640b":"code","87d50512":"code","9d4c819e":"code","fda54a2a":"code","abd70617":"code","62c1e411":"code","b8ad6525":"code","19c438f4":"code","408d97a6":"code","842b2c4a":"code","f459daf9":"code","9d04ce87":"code","ac66fa61":"code","0cbbc311":"code","9287110d":"code","d230f164":"code","e0d751c9":"code","0efe6fc1":"code","a314b0ef":"code","dc39fcb5":"code","304bfb3d":"code","f1ebf5df":"code","a669c7b3":"code","567cbe66":"code","33dd5da8":"code","889a01f9":"code","f31160fc":"code","a5cdab52":"code","c834ec00":"code","82876fd6":"code","fa7a5778":"code","b7336c80":"code","3b977996":"code","c3bd5407":"code","e650fdb8":"code","00d869ee":"code","e74c2c45":"code","f149560b":"code","e99d0d48":"code","1e701026":"code","45992e72":"code","ce702dc9":"code","ec130fd4":"code","88b53928":"code","e26d8d19":"code","e3ada5f8":"code","66de7a40":"code","173d73f3":"code","7ea434a4":"code","a946c5ac":"code","576f80a1":"code","9cddb67d":"code","eef57002":"code","ff30f9f6":"markdown","92c3583d":"markdown","bf5a2695":"markdown","f4debe8c":"markdown","5c449392":"markdown","8d3b53c7":"markdown"},"source":{"5d36f6d8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\nfrom numpy import cov\nfrom numpy.linalg import eig\nimport xgboost as xgb\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve\nimport scikitplot as skplt\n\nfrom tqdm import tqdm_notebook\n\npd.set_option('display.max_columns', 500)\nprint(os.listdir('\/kaggle\/input\/ieee-fraud-detection'))\nprint(os.listdir('\/kaggle\/input\/submission-9009'))\n# Any results you write to the current directory are saved as output.","51d22805":"PATH = '\/kaggle\/input\/ieee-fraud-detection'\nPATH1 = '\/kaggle\/input'","7fdf0485":"df_train_tran = pd.read_csv(PATH + '\/train_transaction.csv')\ndf_train_tran.head()","c680640b":"df_train_id = pd.read_csv(PATH + '\/train_identity.csv')","87d50512":"df_train_tran_id = pd.merge(df_train_tran,\n                 df_train_id,\n                 on='TransactionID', \n                 how='inner')\n\ndel df_train_tran, df_train_id\ngc.collect()","9d4c819e":"print(df_train_tran_id.shape)","fda54a2a":"df_train_tran_id['V11'].value_counts(normalize=True)\ndf_train_tran_id['V1'].isna().sum()\/df_train_tran_id.shape[0]","abd70617":"all_cols = list(df_train_tran_id.columns)\nall_null_cols = []\nm = df_train_tran_id.shape[0]\nfor i in range(0,len(all_cols)):\n    if all_cols[i] == 'isFraud':\n        continue\n    percent_na = df_train_tran_id[all_cols[i]].isna().sum()\/m\n    if percent_na > 0.45:\n        print(i)\n        print(all_cols[i])\n        all_null_cols.append(all_cols[i])\n        df_train_tran_id.drop(all_cols[i], axis=1, inplace=True)\n#list(df_train_tran.columns)","62c1e411":"df_train_tran_id[all_cols[2]].isna().sum()\/m","b8ad6525":"len(all_null_cols)","19c438f4":"df_train_tran_id.drop(columns=['TransactionID'], inplace=True)\nall_cols = list(df_train_tran_id.columns)\nprint(len(all_cols))\nprint(all_cols)","408d97a6":"df_train_tran_id.head()","842b2c4a":"gc.collect()","f459daf9":"cat_cols = ['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType','DeviceInfo','id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']\ncat_cols = list(set(cat_cols).difference(all_null_cols))\nn_cat = len(cat_cols)\nprint(n_cat)","9d04ce87":"all_cols = set(df_train_tran_id.columns).difference(set(['TransactionID','isFraud']))\nall_cols = list(set(all_cols).difference(all_null_cols))\ntype(all_cols)\nlen(all_cols)","ac66fa61":"non_cat_cols = list(set(all_cols).difference(set(cat_cols)))\nlen(non_cat_cols)","0cbbc311":"for col in cat_cols:\n    #print(col)\n    imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n    if col in list(df_train_tran_id.columns):\n        imp.fit(df_train_tran_id[[col]])\n        df_train_tran_id[[col]]= imp.transform(df_train_tran_id[[col]])\n        df_train_tran_id[[col]] = LabelEncoder().fit_transform(df_train_tran_id[[col]])","9287110d":"df_train_tran_id.head()","d230f164":"print(len(non_cat_cols))\nfor col in non_cat_cols:\n    print(col)\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n    if col in list(df_train_tran_id.columns):\n        print(len(df_train_tran_id[[col]]=='NAN'))\n        imp.fit(df_train_tran_id[[col]])\n        print('hi')\n        df_train_tran_id[[col]]= imp.transform(df_train_tran_id[[col]])\n        #df_train_tran_id[[col]] = LabelEncoder().fit_transform(df_train_tran_id[[col]])","e0d751c9":"df_train_tran_id.head(10)","0efe6fc1":"'''df_train_tran_id[np.isfinite(df_train_tran_id) == False] = 0\ndf_train_tran_id[np.isnan(df_train_tran_id) == True] = 0\ndf_train_tran_id.fillna()'''","a314b0ef":"#df_train_tran_id.dropna(axis=1)\ndf_train_tran_id = df_train_tran_id.values","dc39fcb5":"print(type(df_train_tran_id))\nprint(df_train_tran_id)","304bfb3d":"train_tran_x = df_train_tran_id[:,1:]\ntrain_tran_y = df_train_tran_id[:,0]\ndel df_train_tran_id\ngc.collect()\nprint(np.shape(train_tran_x))\nprint(np.shape(train_tran_y))","f1ebf5df":"any(train_tran_y==1)","a669c7b3":"for i in range(0,np.shape(train_tran_x)[1]):\n    min1 = np.min(train_tran_x[:,i])\n    max1 = np.max(train_tran_x[:,i])\n    mean = np.mean(train_tran_x[:,i])\n    train_tran_x[:,i] = (train_tran_x[:,i] - min1)\/(max1-min1)\n    #train_tran_x = MinMaxScaler().fit_transform(train_tran_x)","567cbe66":"print(np.shape(train_tran_x))","33dd5da8":"train_tran_x[np.isnan(train_tran_x)] = 0\ntrain_tran_x","889a01f9":"rs = RandomUnderSampler(random_state=42)\nX, y = rs.fit_resample(train_tran_x, train_tran_y)","f31160fc":"print(X.shape)","a5cdab52":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold\n\nxgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n                    silent=True, nthread=1)","c834ec00":"params = {\n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 4, 5]\n        }","82876fd6":"from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\n\nfolds = 3\nparam_comb = 5\n\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=4, cv=skf.split(X,y), verbose=3, random_state=1001 )\n\n# Here we go\n#start_time = timer(None) # timing starts from this point for \"start_time\" variable\nrandom_search.fit(X, y)\n#timer(start_time) # timing ends here for \"start_time\" variable","fa7a5778":"print('\\n All results:')\nprint(random_search.cv_results_)\nprint('\\n Best estimator:')\nprint(random_search.best_estimator_)\nprint('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\nprint(random_search.best_score_ * 2 - 1)\nprint('\\n Best hyperparameters:')\nprint(random_search.best_params_)\nresults = pd.DataFrame(random_search.cv_results_)\nresults.to_csv('xgb-random-grid-search-results-01.csv', index=False)","b7336c80":"del X, y\ngc.collect()","3b977996":"df_test_tran = pd.read_csv(PATH + '\/test_transaction.csv')\ndf_test_id = pd.read_csv(PATH + '\/test_identity.csv')","c3bd5407":"df_test_tran_id = pd.merge(df_test_tran,\n                 df_test_id,\n                 on='TransactionID', \n                 how='inner')\n\n#del df_test_tran, df_test_id\ngc.collect()","e650fdb8":"df_test_tran_id.drop(columns=all_null_cols, axis=1, inplace=True)\n#df_test_tran_id.drop(columns=['isFraud'], axis=1, inplace=True)\nprint(np.shape(df_test_tran_id))","00d869ee":"print(np.shape(df_test_tran_id))\n#df_test_tran_id.drop(columns=all_null_cols, axis=1, inplace=True)","e74c2c45":"df_test_tran_id.head(5)","f149560b":"for col in cat_cols:\n    #print(col)\n    imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n    if col in list(df_test_tran_id.columns):\n        imp.fit(df_test_tran_id[[col]])\n        df_test_tran_id[[col]] = imp.transform(df_test_tran_id[[col]])\n        df_test_tran_id[[col]] = LabelEncoder().fit_transform(df_test_tran_id[[col]])\n        #df_test_tran_id[[col]] = df_test_tran_id[[col]].astype('category')","e99d0d48":"print(len(non_cat_cols))\nfor col in non_cat_cols:\n    #print(col)\n    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n    if col in list(df_test_tran_id.columns):\n        imp.fit(df_test_tran_id[[col]])\n        df_test_tran_id[[col]]= imp.transform(df_test_tran_id[[col]])\n        #df_train_tran[[col]] = LabelEncoder().fit_transform(df_train_tran[[col]])","1e701026":"list(df_test_tran_id.isnull().any())","45992e72":"#df_test_tran = df_test_tran.replace(np.nan, 0)","ce702dc9":"df_test_tran_id.fillna(0, inplace=True)\n#df_test_tran_id.convert_objects(convert_numeric=True)","ec130fd4":"df_test_tran_id.head(10)","88b53928":"df_test_tran_id.head()","e26d8d19":"df_test_tran_id = df_test_tran_id.values","e3ada5f8":"test_tran_x = df_test_tran_id[:,1:]\n#del df_test_tran_id\ngc.collect()\nprint(np.shape(test_tran_x))\ntest_tran_x.astype('int')","66de7a40":"#test_tran_x[np.isnan(test_tran_x)] = 0\n#test_tran_x","173d73f3":"for i in range(0,np.shape(test_tran_x)[1]):\n    min1 = np.min(test_tran_x[:,i])\n    max1 = np.max(test_tran_x[:,i])\n    mean = np.mean(test_tran_x[:,i])\n    if max1 - min1 != 0:\n        test_tran_x[:,i] = (test_tran_x[:,i] - min1)\/(max1-min1)","7ea434a4":"print(test_tran_x)","a946c5ac":"submission = pd.read_csv(PATH + '\/sample_submission.csv')\nsubmission_9009 = pd.read_csv(PATH1 + '\/submission-9009\/ieee_submission_9009.csv')\n#submission['isFraud'] = submission_9009['isFraud']\ny_test = random_search.predict_proba(test_tran_x)","576f80a1":"#y_test = random_search.predict_proba(test_tran_x)\nw1 = 0.6\nw2 = 0.4\nlist_of_tranids_common = list(df_test_tran_id[:,0].astype('int'))\nfor i in range(0,submission_9009.shape[0]):\n    idx = -1\n    old_pred = submission_9009.iloc[i,1]\n    old_idx = submission_9009.iloc[i,0]\n    if old_idx in list_of_tranids_common:\n        idx = list_of_tranids_common.index(old_idx)\n    if idx >= 0:\n        y = y_test[idx,1] # \n        submission_9009.iloc[i,1] = (w1 * y) + (w2 * old_pred)\n    else:\n        submission_9009.iloc[i,1] = old_pred\n    \nsubmission_9009.to_csv(\"submission.csv\", index=False)    ","9cddb67d":"submission_9009.iloc[0,1]","eef57002":"'''\nPlan of Action:-\nAssumption: actual values of w1, w2 are same of all data points.\nLoad the saved params of the model from csv file\nFind w1 and w2 based on 2 predictions on train_tran_x and train_tran_id_x with non-linear regression\n(with only one sigmoid neuron and log-loss)\nThen apply the learned value of w1 and w2 for testing\n'''","ff30f9f6":"# Dropping some columns with more than 50% null values.","92c3583d":"# Scaling the data in df_train_tran","bf5a2695":"# Cleaning df_train_tran","f4debe8c":"Null value imputation for the categorical columns in df_train_tran dataframe","5c449392":"Null value imputation for the non-categorical columns in df_train_tran dataframe","8d3b53c7":"# Under-sampling the data to remove the bias due to skewness"}}