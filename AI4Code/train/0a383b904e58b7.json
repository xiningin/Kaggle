{"cell_type":{"0a357f41":"code","0ad66890":"code","6ad9dfa1":"code","f376256c":"code","398e4c14":"code","dfab60a7":"code","6a2bb4a1":"code","4e1e65da":"code","74d07a6d":"code","7a900ae0":"code","d0a8216c":"code","4e6c41de":"code","c4ba236c":"code","460dd7ec":"code","f2bbe08b":"code","6bd83b90":"code","caa333ad":"code","be661fbf":"code","42382190":"code","25f1b776":"code","e93154b7":"code","7a14a84a":"code","4180bad2":"code","031bc954":"code","15b0e8ad":"markdown","ea14dca7":"markdown","06925d95":"markdown","80a7ac51":"markdown","4eb31c06":"markdown","59d59523":"markdown","ef7372e9":"markdown","f02e3beb":"markdown","fe0f6450":"markdown","6c07da4e":"markdown","2825e574":"markdown","8860fb22":"markdown","acffea36":"markdown","384fe7d4":"markdown","16b56411":"markdown","5ee0c6ff":"markdown","c0ca0a6a":"markdown","3f48f51b":"markdown","90e3681a":"markdown","7237db34":"markdown","c86e2f56":"markdown","89f11707":"markdown","96e47bd4":"markdown","4e9660d4":"markdown","be86e529":"markdown","20ed879a":"markdown","00de17b3":"markdown","67ff123d":"markdown"},"source":{"0a357f41":"#Tensorflow library. Used to implement machine learning models\nimport tensorflow as tf\n#Numpy contains helpful functions for efficient mathematical calculations\nimport numpy as np\n#Dataframe manipulation library\nimport pandas as pd\n#Graph plotting library\nimport matplotlib.pyplot as plt\n%matplotlib inline","0ad66890":"#Loading in the ratings dataset\nimport pandas as pd\nratings_data = pd.read_csv('..\/input\/ml-1m\/ml-1m\/ratings.dat', sep='::', header=None,engine='python')\n#ratings_df.head()\n#Loading in the movies dataset\nmovies_data = pd.read_csv('..\/input\/ml-1m\/ml-1m\/movies.dat', sep='::', header=None, engine='python')\n#movies_df.head()\n\n# !ls ..\/input\/ml-1m\/ml-1m","6ad9dfa1":"# so we can add the columns\nmovies_data.columns = ['MovieID', 'Title', 'Genres']\nratings_data.columns = ['UserID', 'MovieID', 'Rating', 'Timestamp']","f376256c":"movies_data.head()","398e4c14":"ratings_data.head()","dfab60a7":"#lets see the shape of our data\nprint(\"shape of movie dataset: \",movies_data.shape)\nprint(\"shape of rating dataset: \",ratings_data.shape)","6a2bb4a1":"movies_data.tail()","4e1e65da":"movies_data['List Index'] = movies_data.index\nmovies_data.head()","74d07a6d":"#Merging movies_df with ratings_df by MovieID\nmerged_data = movies_data.merge(ratings_data, on='MovieID')\n#Dropping unecessary columns\nmerged_data = merged_data.drop('Timestamp', axis=1).drop('Title', axis=1).drop('Genres', axis=1)\n#Displaying the result\nmerged_data.head()","7a900ae0":"#Group up by UserID\nuserGroup = merged_data.groupby('UserID')\nuserGroup.first().head()","d0a8216c":"#Amount of users used for training\namountOfUsedUsers = 1000\n#Creating the training list\ntrain_X = []\n#For each user in the group\nfor userID, curUser in userGroup:\n    #Create a temp that stores every movie's rating\n    temp = [0]*len(movies_data)\n    #For each movie in curUser's movie list\n    for num, movie in curUser.iterrows():\n        #Divide the rating by 5 and store it\n        temp[movie['List Index']] = movie['Rating']\/5.0\n    #Now add the list of ratings into the training list\n    train_X.append(temp)\n    #Check to see if we finished adding in the amount of users for training\n    if amountOfUsedUsers == 0:\n        break\n    amountOfUsedUsers -= 1","4e6c41de":"hiddenUnits = 20\nvisibleUnits = len(movies_data)\nvb = tf.placeholder(\"float\", [visibleUnits]) #Number of unique movies\nhb = tf.placeholder(\"float\", [hiddenUnits]) #Number of features we're going to learn\nW = tf.placeholder(\"float\", [visibleUnits, hiddenUnits])","c4ba236c":"#Phase 1: Input Processing\nv0 = tf.placeholder(\"float\", [None, visibleUnits])\n_h0= tf.nn.sigmoid(tf.matmul(v0, W) + hb)\nh0 = tf.nn.relu(tf.sign(_h0 - tf.random_uniform(tf.shape(_h0))))\n#Phase 2: Reconstruction\n_v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb) \nv1 = tf.nn.relu(tf.sign(_v1 - tf.random_uniform(tf.shape(_v1))))\nh1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)","460dd7ec":"#Learning rate\nalpha = 1.0\n#Create the gradients\nw_pos_grad = tf.matmul(tf.transpose(v0), h0)\nw_neg_grad = tf.matmul(tf.transpose(v1), h1)\n#Calculate the Contrastive Divergence to maximize\nCD = (w_pos_grad - w_neg_grad) \/ tf.to_float(tf.shape(v0)[0])\n#Create methods to update the weights and biases\nupdate_w = W + alpha * CD\nupdate_vb = vb + alpha * tf.reduce_mean(v0 - v1, 0)\nupdate_hb = hb + alpha * tf.reduce_mean(h0 - h1, 0)","f2bbe08b":"err = v0 - v1\nerr_sum = tf.reduce_mean(err * err)","6bd83b90":"#Current weight\ncur_w = np.zeros([visibleUnits, hiddenUnits], np.float32)\n#Current visible unit biases\ncur_vb = np.zeros([visibleUnits], np.float32)\n#Current hidden unit biases\ncur_hb = np.zeros([hiddenUnits], np.float32)\n#Previous weight\nprv_w = np.zeros([visibleUnits, hiddenUnits], np.float32)\n#Previous visible unit biases\nprv_vb = np.zeros([visibleUnits], np.float32)\n#Previous hidden unit biases\nprv_hb = np.zeros([hiddenUnits], np.float32)\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())","caa333ad":"epochs = 15\nbatchsize = 100\nerrors = []\nfor i in range(epochs):\n    for start, end in zip( range(0, len(train_X), batchsize), range(batchsize, len(train_X), batchsize)):\n        batch = train_X[start:end]\n        cur_w = sess.run(update_w, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n        cur_vb = sess.run(update_vb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n        cur_nb = sess.run(update_hb, feed_dict={v0: batch, W: prv_w, vb: prv_vb, hb: prv_hb})\n        prv_w = cur_w\n        prv_vb = cur_vb\n        prv_hb = cur_nb\n    errors.append(sess.run(err_sum, feed_dict={v0: train_X, W: cur_w, vb: cur_vb, hb: cur_nb}))\n    print (errors[-1])\nplt.plot(errors)\nplt.ylabel('Error')\nplt.xlabel('Epoch')\nplt.show()","be661fbf":"#Selecting the input user\ninputUser = [train_X[75]]","42382190":"#Feeding in the user and reconstructing the input\nhh0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)\nvv1 = tf.nn.sigmoid(tf.matmul(hh0, tf.transpose(W)) + vb)\nfeed = sess.run(hh0, feed_dict={ v0: inputUser, W: prv_w, hb: prv_hb})\nrec = sess.run(vv1, feed_dict={ hh0: feed, W: prv_w, vb: prv_vb})","25f1b776":"scored_movies_df_75 = movies_data\nscored_movies_df_75[\"Recommendation Score\"] = rec[0]\nscored_movies_df_75.sort_values([\"Recommendation Score\"], ascending=False).head(20)","e93154b7":"merged_data.iloc[75]","7a14a84a":"movies_data_75 = merged_data[merged_data['UserID']==215]\nmovies_data_75.head()","4180bad2":"#Merging movies_df with ratings_df by MovieID\nmerged_data_75 = scored_movies_df_75.merge(movies_data_75, on='MovieID', how='outer')\n#Dropping unecessary columns\nmerged_data_75 = merged_data_75.drop('List Index_y', axis=1).drop('UserID', axis=1)","031bc954":"merged_data_75.sort_values([\"Recommendation Score\"], ascending=False).head(20)","15b0e8ad":"# RECOMMENDATION SYSTEM WITH A RESTRICTED BOLTZMANN MACHINE using Tensorflow","ea14dca7":"### Architecture:<br>\nRestricted Boltzmann Machine is a stochastic neural network (that is a network of neurons where each neuron have some random behavior when activated). it is a two-layer neural network. The two layers are fully connected to each other, there are no inner connections inside the layers. The input layer is also called visible layer, the output layer is hidden.Visible layer is typically bigger than the hidden layer, since the visible layer represents input and reconstructions while the hidden layer neurons correspond to features of the input. This means that information flows in both directions during the training and during the usage of the network and that weights are the same in both directions.In RBMs predictions are made differently as in regular feedforward neural networks.<br>\n<img src=\"http:\/\/imonad.com\/rbm\/restricted-boltzmann-machine\/rbm2.png\">","06925d95":"With that, let's merge the ratings dataframe into the movies one so we can have the List Index values in both dataframes. Additionally we're also going to drop the Timestamp, Title and Genres columns since we won't be needing it to make recommendations.","80a7ac51":"Now, we can find all the movies that our mock user has watched before:","4eb31c06":"And set the error function, which in this case will be the Mean Absolute Error Function.","59d59523":"Now we set the RBM training parameters and functions.","ef7372e9":"lets sort it and take a look at the firt 20 rows:","f02e3beb":"### References:\n* https:\/\/en.wikipedia.org\/wiki\/Restricted_Boltzmann_machine  \n* http:\/\/deeplearning.net\/tutorial\/rbm.html  \n* http:\/\/deeplearning4j.org\/restrictedboltzmannmachine.html  \n* http:\/\/imonad.com\/rbm\/restricted-boltzmann-machine\/  \n* [Restricted Boltzmann Machines for Collaborative Filtering](http:\/\/www.cs.utoronto.ca\/~hinton\/absps\/netflixICML.pdf)\n* https:\/\/cognitiveclass.ai\/\n    ","fe0f6450":"As you can see, there are some movies that user has not watched yet and has high score based on our model. So, we can recommend them to user","6c07da4e":"We then move on to creating the visible and hidden layer units and setting their activation functions. In this case, we will be using the tf.sigmoid and tf.relu functions as nonlinear activations since it's what is usually used in RBM's.","2825e574":"So, how to recommend the movies that the user has not watched yet?\n\nLets first find the User ID of our mock user:","8860fb22":"We also have to initialize our variables. Thankfully, NumPy has a handy zeros function for this. We use it like so:","acffea36":"Now we train the RBM with 15 epochs with each epoch using 10 batches with size 100. After training, we print out a graph with the error by epoch.","384fe7d4":"# Boltzmann Machines\nBoltzmann machines are one of the first neural networks capable of learning internal representations and are able to solve, in a specific time period, difficult combinatoric problems. Unlike neural networks like CNN, RNN, etc. Boltzmann Machines are **undirected models**. \n<img  src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/7\/7a\/Boltzmannexamplev1.png\/330px-Boltzmannexamplev1.png\">\n<!--The above graph depicts every undirected edge as a dependency. -->\n\nHere $v_1$, $v_2$, $v_3$, $v_4$ are visible or input nodes and $h_1$, $h_2$, $h_3$ are hidden nodes.\n \nFirstly, we notice that it contains 2 layers : \n1. An input layer \n2. A hidden layer \n \nBut this model doesn't have an output layer and every node is inter-connected without any direction.\n \nWhen I first saw this graph, I thought - \n> \"Why is every input node connected to every other input node?\".\n\nHere I understood that Boltzmann machines are fundamently diffrent from other ML Algorithms. *They don't just expect input data, they also generate data or say generate information from all of these nodes regardless of input or hidden nodes*. For Boltzmann machines these nodes are all same.\n\nTo understand this, lets look at an example that Geoffrey Hinton ( inventor of RBMs) gave:\n\nLook at the picture of a nuclear power plant.<br>\n<img src=\"https:\/\/www.nuclear-power.net\/wp-content\/uploads\/2014\/10\/Nuclear_Plant.gif\"><br>\n\nLets say there are certain parameters that we can measure like temprature inside the containement building, how quickly turbine is moving, pressure inside the pump etc. \nBut at the same time there are certain things that we are not measuring like wind speed or let's say thickness of cooling tower wall etc. These parameters that we can measure (or can't), form a system and that is what Boltzmann machines represent. \n\nSo a Boltzmann machine is a representation of a system where visible\/ input nodes are params that we can measure and hidden units\/nodes are params that we cannot\/don't measure. \n\nBoltzmann machines don't wait for input data in order to change the state of the system, it is capable of generating these values(parameters) on its own. Thats why Boltzman machines are **Generative models** or **Stochastic deep learning models** but not  **Discriminative models**.\n<br><br>\n","16b56411":"as you can see there is no header in both rating and movies file.So our movies_data variable contains a dataframe that stores a movie's unique ID number, title and genres, while our ratings_data variable stores a unique User ID number, a movie's ID that the user has watched, the user's rating to said movie and when the user rated that movie.","5ee0c6ff":"As it is possible to notice, we have 3883 movies, while our ID's vary from 1 to 3952. Due to this, we won't be able to index movies through their ID since we would get memory indexing errors. To amend this, we can create a column that shows what spot in our list that particular movie is in:","c0ca0a6a":"# Models parameter\n\nNext, let's start building our RBM with Tensorflow. We'll begin by first determining the amount of hidden layers and then creating placeholder variables for storing our visible layer biases, hidden layer biases and weights that connect the hidden layer with the visible one. We will be arbitrarily setting the amount of hidden layers to 20. You can freely set this value to any number you want since each neuron in the hidden layer will end up learning a feature.","3f48f51b":"First, lets see what is different betwee discriminative and generative model: \n\n__Discriminative :__Consider a classification problem in which we want to learn to distinguish between Sedan cars (y = 1) and SUV cars (y = 0), based on some features of an cars. Given a training set, an algorithm like logistic regression tries to find a straight line\u2014that is, a decision boundary\u2014that separates the suv and sedan. <br>\n__Generative:__ looking at cars, we can build a model of what Sedan cars look like. Then, looking at SUVs, we can build a separate model of what SUV cars look like. Finally, to classify a new car, we\ncan match the new car against the Sedan model, and match it against the SUV model, to see whether the new car looks more like the SUV or Sedan. \n\nGenerative Models specify a probability distribution over a dataset of input vectors. we can do both supervise and unsupervise tasks with generative models:\n- In an unsupervised task, we try to form a model for P(x), where x is an input vector. \n- In the supervised task, we first form a model for P(x|y), where y is the label for x. For example, if y indicates whether an example is a SUV (0) or a Sedan (1), then p(x|y = 0) models the distribution of SUVs\u2019 features, and p(x|y = 1) models the distribution of Sedans\u2019 features. If we manage to find P(x|y) and P(y), then we can use `bayes rule` to estimate P(y|x), because: p(y|x) = p(x|y)p(y)\/p(x)\n\n\n","90e3681a":"Let's also group up the users by their user IDs and take a look at one of them.","7237db34":"Now, we can start formatting the data into input for the RBM. We're going to store the normalized users ratings into a list of lists called train_X.","c86e2f56":"The datasets we're going to use were acquired by [GroupLens](\"http:\/\/files.grouplens.org\/) and contain movies, users and movie ratings by these users. you can download the dataset from here: [Dataset](http:\/\/files.grouplens.org\/datasets\/movielens\/ml-1m.zip) <br>","89f11707":"in this section we will learn how to use RBM in a Collaborative Filtering based recommendation system.<br>\n__Collaborative Filtering__: Also referred as social filtering, filters information by using the recommendations of other people. It is based on the idea that people who agreed in their evaluation of certain items in the past are likely to agree again in the future.So, This algorithm recommends items by trying to find users that are similar to each other based on their item ratings.","96e47bd4":"# Restricted Boltzmann Machine (RBM):<br>\n<b>RBMs<\/b>  are shallow neural nets that learn to reconstruct data by themselves in an unsupervised fashion. Application of RBMs are  Collaborative Filtering, dimensionality reduction, classification, regression, feature learning, topic modeling ,Recommandation System and even <b>Deep Beleif Networks (DBN) .<\/b><br>\n\nThe energy function for the RBMs is defined as:<br>\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/2596eb7f60f387129fd3bbc075f37898eded353b\"><br>\nor in matrix notation :<br>\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/ef4edf17279787e29bb1a581316d17d70de2072e\"><br>\nwhere a and b are bias  and v is visible node and h is hidden unit node and w is weight between visible and hidden node. As you can see from here is that energy funtion depends on the visible\/input states, hidden states, weights and biases.The training of RBM consists in finding of the parameters for given input values so that the energy reaches a minimum. It can automatically extract __meaningful__ features from a given input.<br>\n\nRestricted Boltzmann Machines are probabilistic. So, probability distributions over hidden and\/or visible vectors are defined in terms of the energy function:<br>\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/cfc031033f9ef72633258c4d8a7bbdbbecfd01ba\"><br>\nHere Z is called the [\u2018partition function\u2019](https:\/\/en.wikipedia.org\/wiki\/Partition_function_(mathematics) that is the summation over all possible pairs of visible and hidden vectors. In other words, just a normalizing constant to ensure the probability distribution sums to 1\n\n","4e9660d4":" So we feed our training data into boltzman machine as the input to help it adjust the weights of the system accordingly so that it actually resemble our system.it learns from what we feed in like how the parameter of our system influence each other. then we can use this model to monitor our system. in our example above boltzman machine understand how the parameters affect each other like what is the effect of wind on temprature etc. because we trained it using good behaviour. once this done our model can monitor the power plant. so the boltzman machine can help us understand what is abnormal behaviour of our system.\n <br><br><br>\n \nThey are named after the [Boltzmann distribution](https:\/\/en.wikipedia.org\/wiki\/Boltzmann_distribution) (also known as <b>Gibbs sampling<\/b>) in statistical mechanics, which is used in their sampling function.In statistical mechanics, the Boltzmann distribution is a probability distribution that gives the probability that a system will be in a certain state as a function of that state\u2019s energy and the temperature of the system. thats why they are also known as <b> Energy based Models(EBM)<\/B>.<br>\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/16fe9eb74e0fe4140463bbb7dfdd64ae83f8c1b0\" >\n<br>\n\nwhere pi is the probability of state i, \u03b5i the energy of state i, k the Boltzmann constant, T the temperature of the system, and M is the number of states accessible to the system.The sum is over all states accessible to the system of interest.<br><br><br>\n","be86e529":"### How does RBMs works :<br>\nSimply, RBM takes the inputs and translates them to a set of numbers that represents them. Then, these numbers can be translated back to reconstruct the inputs. Through several forward and backward passes, the RBM will be trained, and a trained RBM can reveal which features are the most important ones when detecting patterns.After the network is trained we can use it on new unknown data to make classification of the data (this is known as unsupervised learning)\n\nThink RBM as a model that have been trained, and now it can calculate the probability of observing a case (e.g. wet road) given some hidden\/latent values (e.g. raining). That is, the RBM can be viewed as a generative model that assigns a probability to each possible binary state vectors over its visible units (v).\n\nDuring the training process 3 steps are repeated:\n1. __forward pass__: In the forward pass every input is combined with an individual weight and an overall bias. the result goes to hidden layers whose neurons may or may not be active.\n2. __backward pass__: In the backward pass the activated neurons in the hidden layer send the result back to the visible layer, where the input will be reconstructed. during this step data passed backwards is also combined with individual weights and an overall bias. once the information goes to visible\/input layers, input is reconstructed and RBM perform 3rd step.\n3. __Assessing the quality__: it consist of assessing the quality of the reconstruction by comparing it to original data. RBM calculate the error and adjust the weights and bias in order to minimize the error.\n\nadvantage of RBM is that during the learning process or say training process , RBM extract the features from inputs and it automatically decides which features are important or relavant and how to combine them to form patterns.\n\nIn order to train an RBM, we have to maximize the product of probabilities assigned to the training set V (a matrix, where each row of it is treated as a visible vector v):\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/d42e9f5aad5e1a62b11b119c9315236383c1864a\" >\n\n\nOr equivalently, maximize the expected log probability of V:\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/ba0ceed99dca5ff1d21e5ace23f5f2223f19efc0\" >\n\nWe can use stochastic gradient descent to find the optimal weight and consequently minimize the objective function.But When we derive, it give us 2 terms, called positive and negative gradient. The Positive phase increases the probability of training data. The Negative phase decreases the probability of samples generated by the model.The negative phase is hard to compute. So, training a RBM is performed by algorithm known as <b>\"Contrastive Divergence Learning\"<\/b> Contrastive Divergence is actually matrix of values that is computed and used to adjust values of the W matrix. Changing W incrementally leads to training of W values. Then on each step (epoch), W is updated to a new value W' through the equation below: <br>\n    W\u2032 = W + alpha\u2217CD <br>\n    where alpha is learning rate, W is weight matrix.<br><br>\n    \n   <b> how to calculate CD :<\/b><br>\nWe can perform single-step Contrastive Divergence (CD-1) taking the following steps:<br>\nlets define some terms : X is our input, W is weight matrix (number of visible neurons, number of hidden neurons), vb is the bias of visible layer, hb is the bias of hidden layer.\n\n1. Take a training sample from X, compute the probabilities of the hidden units and sample a hidden activation vector h0 from this probability distribution.\n - $\\_h0 = sigmoid(X \\otimes W + hb)$\n - $h0 = sampleProb(h0)$\n2. Compute the [outer product](https:\/\/en.wikipedia.org\/wiki\/Outer_product) of X and h0 and call this the positive gradient.\n - $w\\_pos\\_grad = X \\otimes h0$  (Reconstruction in the first pass)  \n3. From h, reconstruct v1, and then take a sample of the visible units, then resample the hidden activations h1 from this. (**Gibbs sampling step**)\n - $\\_v1 = sigmoid(h0 \\otimes transpose(W) + vb)$\n - $v1 = sample_prob(v1)$  (Sample v given h)\n - $h1 = sigmoid(v1 \\otimes W + hb)$\n4. Compute the outer product of v1 and h1 and call this the negative gradient.\n - $w\\_neg\\_grad = v1 \\otimes h1$  (Reconstruction 1)\n5. Now, CD equals the positive gradient minus the - negative gradient, CD is a matrix of size 784x500. \n - $CD = (w\\_pos\\_grad - w\\_neg\\_grad) \/ datapoints$\n6. Update the weight to be CD times some learning rate\n - $W' = W + alpha*CD$\n7. At the end of the algorithm, the visible nodes will store the value of the sample.\n\n#### What is sampling here (sampleProb)?\nIn forward pass: We randomly set the values of each hi to be 1 with probability $sigmoid(v \\otimes W + hb)$.  \nIn reconstruction: We randomly set the values of each vi to be 1 with probability $ sigmoid(h \\otimes transpose(W) + vb)$.\n","20ed879a":"# Recommendation\nWe can now predict movies that an arbitrarily selected user might like. This can be accomplished by feeding in the user's watched movie preferences into the RBM and then reconstructing the input. The values that the RBM gives us will attempt to estimate the user's preferences for movies that he hasn't watched based on the preferences of the users that the RBM was trained on.","00de17b3":"In the next cell, we merge all the movies that our mock users has watched with the predicted scors based on his historical data:","67ff123d":"We can then list the 20 most recommended movies for our mock user by sorting it by their scores given by our model."}}