{"cell_type":{"3ceba752":"code","763ac9a4":"code","3490c557":"code","14dc0e3a":"code","70ad7ca8":"code","117a3bc6":"code","af8a7b44":"code","19751d90":"code","e29a0ce4":"code","f237b910":"code","94f29f1f":"code","3df30795":"code","0909361b":"code","633193a6":"code","d42dfa5e":"code","c76d5c4a":"code","24b5473f":"code","f7d0b526":"code","e9dd6bbd":"markdown","12b90f15":"markdown","8d718c43":"markdown","9d7df9ea":"markdown","f6d560ea":"markdown","5b4affde":"markdown","e831ae10":"markdown","c1ea45b0":"markdown","8812f809":"markdown"},"source":{"3ceba752":"import os\nimport numpy as np\nimport pandas as pd\nfrom datetime import datetime\nimport random\nimport copy\nimport time\n\n\n#Visualization imports\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# plt.rcParams.update({'font.size': 18})\n# plt.style.use('ggplot')\nsns.set_theme(color_codes=True)\n\n#Preprocessing imports\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\n\n\n#NN imports\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils import data\nfrom torch.utils.data import DataLoader, TensorDataset\n# from torch.utils.tensorboard import SummaryWriter\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","763ac9a4":"train_set = pd.read_csv(\"..\/input\/wids-22-dataset\/train.csv\")\ntest_set = pd.read_csv(\"..\/input\/wids-22-dataset\/test.csv\")\nprint(\"Number of train samples are\",train_set.shape)\nprint(\"Number of test samples are\",test_set.shape)\ncategorical_features = ['State_Factor', 'building_class', 'facility_type']\nnumerical_features=train_set.select_dtypes('number').columns","3490c557":"#Check missing values\nplt.figure(figsize = (25,11))\nsns.heatmap(train_set.isna().values, xticklabels=train_set.columns)\nplt.title(\"Missing values in training Data\", size=20)","14dc0e3a":"#Fill missing values == Data Impute \n#code copied from https:\/\/www.kaggle.com\/shrutisaxena\/wids2022-starter-code\nmissing_columns = [col for col in train_set.columns if train_set[col].isnull().any()]\nmissingvalues_count = train_set.isna().sum()\nmissingValues_df = pd.DataFrame(missingvalues_count.rename('Null Values Count')).loc[missingvalues_count.ne(0)]\nmissingValues_df.style.background_gradient(cmap=\"Pastel1\")\n\ntrain_set['year_built'] = train_set['year_built'].replace(np.nan, 2022)\ntest_set['year_built'] = test_set['year_built'].replace(np.nan, 2022)\nnull_col=['energy_star_rating','direction_max_wind_speed','direction_peak_wind_speed','max_wind_speed','days_with_fog']\nimputer = SimpleImputer()\nimputer.fit(train_set[null_col])\ndata_transformed = imputer.transform(train_set[null_col])\ntrain_set[null_col] = pd.DataFrame(data_transformed)\ntest_data_transformed = imputer.transform(test_set[null_col])\ntest_set[null_col] = pd.DataFrame(test_data_transformed)\n\n#Encode the string features\nle = LabelEncoder()\nfor col in categorical_features:\n    train_set[col] = le.fit_transform(train_set[col])\n    test_set[col] = le.fit_transform(test_set[col])\n    \n#See the dataset\ntrain_set.describe().style.background_gradient()\n","70ad7ca8":"plt.figure(figsize = (25,11))\nsns.heatmap(train_set.isna().values, xticklabels=train_set.columns)\nplt.title(\"Missing values in training Data\", size=20)","117a3bc6":"def seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    print('Done SEEDing: {}'.format(seed))\n\nseed_everything()","af8a7b44":"Y_target = train_set[\"site_eui\"].to_numpy()\ntrain_f = train_set.drop([\"site_eui\",\"id\"],axis=1)\n\ntest_f_id = test_set['id']\ntest_f = test_set.drop([\"id\"],axis=1)\n\ntrain_names = copy.deepcopy(train_f)\nscaler = StandardScaler()\ntrain_f = scaler.fit_transform(train_f)\ntest_f = scaler.transform(test_f)\n\ntrain_f, train_f.shape, Y_target, Y_target.shape\ntest_f, test_f.shape, test_f_id, test_f_id.shape","19751d90":"#Split data into TRAIN and TEST 80:20\nx_train, x_test, y_train, y_test = train_test_split(train_f, Y_target, test_size = 0.2, random_state = 100)\nx_train, x_test.shape, y_train, y_test, y_test.shape","e29a0ce4":"x_train_tensor = torch.Tensor(x_train)\nx_test_tensor = torch.Tensor(x_test)\ny_train_tensor = torch.Tensor(y_train)\ny_test_tensor = torch.Tensor(y_test)\n#Evaluation set for submission\n\neval_target = torch.Tensor(test_f_id.to_numpy())\ny_eval = torch.Tensor(test_f)\n\n#Create Tensor datasets out of numpy dataset \ntrain_dataset = TensorDataset(x_train_tensor, y_train_tensor)\ntest_dataset = TensorDataset(x_test_tensor, y_test_tensor)\neval_dataset = TensorDataset(y_eval, eval_target)\n\n#Create the dataloaders\ntrain_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=2)\ntest_dataloader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\neval_loader = DataLoader(eval_dataset, batch_size=64, shuffle=False, num_workers=2)\n\ntrain_dataloader, test_dataloader, eval_loader","f237b910":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","94f29f1f":"class SimpleMLP(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(SimpleMLP, self).__init__()\n        \n        self.net = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(inplace=True),\n            nn.Linear(hidden_size, 256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 64),\n            nn.ReLU(inplace=True),\n            nn.Linear(64, 1)\n        \n        )\n        \n    def forward(self, x):\n        obs = self.net(x)\n        return obs","3df30795":"n_features = 62\nhidden_size = 300\nmodel = SimpleMLP(n_features, hidden_size)\nmodel.to(device)","0909361b":"# construct an optimizer\nparams = [p for p in model.parameters() if p.requires_grad]\n# optimizer = optim.SGD(params, lr=0.005,\n#                             momentum=0.9, weight_decay=0.0005)\noptimizer = optim.Adam(params, lr=1e-3)\n# and a learning rate scheduler\nlr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n\n# loss_function = nn.MSELoss()\n# loss = F.mse_loss()\nloss_function = nn.L1Loss()","633193a6":"def train_one_epoch(epoch_index, tb_writer):\n    running_loss = 0.\n    last_loss = 0.\n\n    # Here, we use enumerate(training_loader) instead of\n    # iter(training_loader) so that we can track the batch\n    # index and do some intra-epoch reporting\n    for i, data in enumerate(train_dataloader):\n        # Every data instance is an input + label pair\n        inputs, labels = data\n        # Zero your gradients for every batch!\n        optimizer.zero_grad()\n\n        # Make predictions for this batch\n        outputs = model(inputs.to(device))\n        # Compute the loss and its gradients\n        loss = loss_function(outputs.squeeze(1), labels.to(device))\n        loss.backward()\n        # Adjust learning weights\n        optimizer.step()\n\n        # Gather data and report\n        running_loss += loss.item()\n        if i % 800 == 799:\n            last_loss = running_loss \/ 800 # loss per batch\n#             print('Mphka kai vrhka to last loss:{}'.format(last_loss))\n#             print('  batch {} loss: {}'.format(i + 1, last_loss))\n            tb_x = epoch_index * len(train_dataloader) + i + 1\n            tb_writer.add_scalar('Loss\/train', last_loss, tb_x)\n            running_loss = 0.\n\n    \n    return last_loss","d42dfa5e":"from tensorboardX import SummaryWriter\n\n# Initializing in a separate cell so we can easily add more epochs to the same run\ntimestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\nwriter = SummaryWriter('runs\/wids_trainer_{}'.format(timestamp))\nepoch_number = 0\nEPOCHS = 150\nloss_list = []\n\nbest_vloss = 1_000_000.\n\nfor epoch in range(EPOCHS):\n    print('EPOCH {}:'.format(epoch_number + 1))\n    ts = time.time()\n    # Make sure gradient tracking is on, and do a pass over the data\n    model.train(True)\n    avg_loss = train_one_epoch(epoch_number, writer)\n    train_time = time.time() - ts\n    # We don't need gradients on to do reporting\n    model.train(False)\n    model.eval()\n    \n    te = time.time()\n    running_vloss = 0.0\n    for i, vdata in enumerate(test_dataloader):\n        vinputs, vlabels = vdata\n        voutputs = model(vinputs.to(device))\n        vloss = loss_function(voutputs.squeeze(1), vlabels.to(device))\n        running_vloss += vloss\n\n    avg_vloss = running_vloss \/ (i + 1)\n    print('Train Loss {} Validation Loss {}'.format(avg_loss, avg_vloss))\n#     print('I was trained on {} for {} and evaluated for {} seconds'.format(device, round(train_time, 3), round(time.time() - te, 3)))\n    # Log the running loss averaged per batch\n    # for both training and validation\n    writer.add_scalars('Training vs. Validation Loss',\n                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n                    epoch_number + 1)\n    writer.flush()\n    # Track best performance, and save the model's state\n    avg_mse = avg_vloss.detach().cpu().numpy() \/ 1.0\n#     print(avg_mse)\n    if avg_vloss < best_vloss:\n        best_vloss = avg_vloss\n        model_path = 'model_{}_{}'.format(timestamp, epoch_number)\n        torch.save(model.state_dict(), model_path)\n    \n    loss_list.append([avg_loss, avg_mse])\n#     lr_scheduler.step()\n    \n    epoch_number += 1\n    \n\n# print(loss_list)\nmean_losses = pd.DataFrame(loss_list, columns=['Average_Training_Loss', 'Average_Validation_Loss'])\n\nsns.set()\n#Plot the train and validation losses.\n# sns.relplot(x='epoch', y='Average_Validation_Loss',  kind='line', data=mean_losses)\n# sns.relplot(x='epoch', y='Average_Training_Loss',  kind='line', data=mean_losses)\n\nplt.figure(); mean_losses.plot(); plt.legend(loc='best')\n","c76d5c4a":"%reload_ext tensorboard","24b5473f":"%tensorboard --logdir .\/","f7d0b526":"#Evaluation for submission\n\npredictions_eui = []\nmodel.eval()\n\nfor i, vdata in enumerate(eval_loader):\n    vinputs, v_id = vdata\n    voutputs = model(vinputs.to(device))\n    predictions_eui.append(voutputs.detach().cpu().numpy())\n\nout = np.concatenate(predictions_eui).ravel()\n\nprint(len(out), out)\nsubmission = pd.read_csv('..\/input\/wids-22-dataset\/sample_solution.csv')\nsubmission['site_eui'] = out\nsubmission.head()\n\nsubmission.to_csv('submission.csv', index=False)\n","e9dd6bbd":"## 3. Normalize the features & create train - test sets","12b90f15":"## 2. Read the dataset & visualize the features","8d718c43":"## 7. Train routine for 1 epoch ","9d7df9ea":"## Did you found the above resource understandable and usefull ? Give back a comment and\/or vote up!","f6d560ea":"## 4. Create Dataloaders for Pytorch NN training","5b4affde":"# Introduction\n\nIn this notebook we will go through some basic data pre-processing with Python, visualize our dataset with Seaborn and Matplotlib, split it with sklearn, and train a simple Multy Layer Perceptron (MLP) using PyTorch to solve the [WIDS22 Challenge](https:\/\/www.kaggle.com\/c\/widsdatathon2022) prediction problem.\n\n## 1. Libraries to use","e831ae10":"## 8. Train the NN for many epochs \n### Visualize the train and validation losses","c1ea45b0":"## 5. Create the NN\n### A simple convergent architecture","8812f809":"## 6. Construct Optimizer and set the Criterion\n### We use the Mean Absolute Error [MAE](https:\/\/en.wikipedia.org\/wiki\/Mean_absolute_error) or L1 loss."}}