{"cell_type":{"3d067450":"code","104acb09":"code","058f5f7b":"code","151f3aa0":"code","ee0e79ea":"code","a33a6d52":"code","3268f909":"code","1b4b90e2":"code","e1d8d893":"code","d20517a1":"code","4232987c":"code","1922dd48":"code","bbe2bef7":"code","f7601250":"code","1e26f379":"code","9022f813":"code","93f3569c":"code","ad8b5f9d":"code","8e75b261":"code","e8899068":"code","3bada468":"code","a16dc9ed":"code","e850c7e2":"code","d2844e03":"code","41b979d8":"code","8c8ca65a":"code","bb88ce49":"markdown","e2b3124d":"markdown","878bdbc2":"markdown","52a1fcc6":"markdown","f979e220":"markdown","9d378f62":"markdown","603ec667":"markdown","f7d7e330":"markdown","f9bc955b":"markdown","32fe518e":"markdown","74302c41":"markdown","52a412c0":"markdown","4a5350d7":"markdown","471c10d2":"markdown","9d566e3c":"markdown","72c9807d":"markdown","9fe460a1":"markdown","42020a06":"markdown","9a8dc41d":"markdown","999e3dae":"markdown","13d1e362":"markdown","2cea23e9":"markdown","a373d386":"markdown","ae9a6d69":"markdown","01407418":"markdown","85a70f0e":"markdown","3ebb9cc0":"markdown","a912a48d":"markdown","48f384d9":"markdown","4980430e":"markdown"},"source":{"3d067450":"#from google_drive_downloader import GoogleDriveDownloader as gdd\n#gdd.download_file_from_google_drive(file_id='11450lgjhOH3nNDAKaucYwYKgs9h8kdIt', dest_path='..\/input.zip', unzip=True)\n#!mv ..\/vietai-advance-course-retinal-disease-detection ..\/input","104acb09":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","058f5f7b":"import sys\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\n\nfrom fastprogress import master_bar, progress_bar\n\nfrom PIL import Image\nfrom tensorflow.contrib.eager.python import tfe\n\n# Enable Eager Execution\ntf.enable_eager_execution()\ntf.executing_eagerly() ","151f3aa0":"data = pd.read_csv(\"..\/input\/train.csv\")\ndata.head()","ee0e79ea":"for label in data.columns[1:]:\n    print(\"Distribution of\", label)\n    print(data[label].value_counts())","a33a6d52":"LABELS = data.columns[1:]\ndef build_label(row):\n    return \",\".join([LABELS[idx] for idx, val in enumerate(row[1:]) if val == 1])\n        \ndata.apply(lambda x: build_label(x), axis=1).value_counts()","3268f909":"LABELS","1b4b90e2":"train_data, val_data = train_test_split(data, test_size=0.2, random_state=2019)","e1d8d893":"IMAGE_SIZE = 224                              # Image size (224x224)\nIMAGENET_MEAN = [0.485, 0.456, 0.406]         # Mean of ImageNet dataset (used for normalization)\nIMAGENET_STD = [0.229, 0.224, 0.225]          # Std of ImageNet dataset (used for normalization)\nBATCH_SIZE = 64                             \nLEARNING_RATE = 0.001\nLEARNING_RATE_SCHEDULE_FACTOR = 0.1           # Parameter used for reducing learning rate\nLEARNING_RATE_SCHEDULE_PATIENCE = 5           # Parameter used for reducing learning rate\nMAX_EPOCHS = 100                              # Maximum number of training epochs","d20517a1":"def preprocessing_image(image):\n    \"\"\"\n    Preprocess image after resize and augment data with ImageDataGenerator\n    \n    Parameters\n    ----------\n    image: numpy tensor with rank 3\n        image to preprocessing\n    \n    Returns\n    -------\n    numpy tensor with rank 3\n    \"\"\"\n    # TODO: augment more here\n    \n    return image","4232987c":"train_datagen = keras.preprocessing.image.ImageDataGenerator(rescale=1.\/255,\n                                                             featurewise_center=True,\n                                                             featurewise_std_normalization=True,\n                                                             preprocessing_function=preprocessing_image)","1922dd48":"def build_label_list(row):\n    return [LABELS[idx] for idx, val in enumerate(row[1:]) if val == 1]\n        \ntrain_data[\"label\"] = train_data.apply(lambda x: build_label_list(x), axis=1)\nval_data[\"label\"] = val_data.apply(lambda x: build_label_list(x), axis=1)","bbe2bef7":"train_gen = train_datagen.flow_from_dataframe(dataframe=train_data, \n                                        directory=\"..\/input\/train\/train\", \n                                        x_col=\"filename\", \n                                        y_col=\"label\",\n                                        class_mode=\"categorical\",\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE), \n                                        batch_size=BATCH_SIZE)","f7601250":"val_gen = train_datagen.flow_from_dataframe(dataframe=val_data, \n                                        directory=\"..\/input\/train\/train\", \n                                        x_col=\"filename\", \n                                        y_col=\"label\",\n                                        class_mode=\"categorical\",\n                                        shuffle=False,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE), \n                                        batch_size=BATCH_SIZE)","1e26f379":"base_model = keras.applications.ResNet50(input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),\n                                    include_top=False,\n                                    weights='imagenet')\nbase_model.trainable = True\n\nmodel = keras.Sequential([\n  base_model,\n  keras.layers.GlobalAveragePooling2D(),\n  keras.layers.Dense(len(LABELS), activation='sigmoid')\n])\n\n# Print out model summary\nmodel.summary()","9022f813":"import tensorflow.keras.backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","93f3569c":"mcp = keras.callbacks.ModelCheckpoint(\"resnet50.h5\", monitor=\"val_f1\", save_best_only=True, save_weights_only=True, verbose=1,mode='max')\nrlr = keras.callbacks.ReduceLROnPlateau(monitor='val_f1', factor=LEARNING_RATE_SCHEDULE_FACTOR, mode='max', patience=LEARNING_RATE_SCHEDULE_PATIENCE, min_lr=1e-8, verbose=1)\ncallbacks = [mcp, rlr]","ad8b5f9d":"device = '\/cpu:0' if tfe.num_gpus() == 0 else '\/gpu:0'\n\nwith tf.device(device):\n    steps_per_epoch = train_gen.n \/\/ BATCH_SIZE\n    validation_steps = val_gen.n \/\/ BATCH_SIZE\n    \n    model.compile(optimizer=keras.optimizers.Adam(lr=LEARNING_RATE), loss='binary_crossentropy', metrics=[f1])\n\n    # Hu\u1ea5n luy\u1ec7n\n    history = model.fit_generator(train_gen,\n                                  steps_per_epoch=steps_per_epoch,\n                                  epochs= MAX_EPOCHS,\n                                  verbose=1,\n                                  validation_data=val_gen,\n                                  validation_steps=validation_steps,\n                                  callbacks=callbacks)","8e75b261":"test_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\ntest_df.head()","e8899068":"test_gen = train_datagen.flow_from_dataframe(dataframe=test_df,\n                                             directory=\"..\/input\/test\/test\",\n                                             x_col=\"filename\",\n                                             class_mode=None,\n                                             shuffle=False,\n                                             target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                             batch_size=BATCH_SIZE)","3bada468":"model.load_weights(\"resnet50.h5\")","a16dc9ed":"pred = model.predict_generator(test_gen)","e850c7e2":"labels = (train_gen.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\nlabels\nLABELS = list(LABELS)\n\ndef probs2label(probs):\n    ''' Return real index following LABELS\n    '''\n    global LABELS, labels\n    result = \" \".join([str(LABELS.index(labels[idx])) for idx, prob in enumerate(probs) if prob > 0.5])\n    print(result)\n    return result","d2844e03":"#test_df['predicted'] = np.apply_along_axis(probs2label, 1, pred)\nfor idx, row in test_df.iterrows():\n    test_df.loc[idx]['predicted'] = probs2label(pred[idx])","41b979d8":"test_df.head()","8c8ca65a":"test_df.to_csv(\"submission.csv\", index=False)","bb88ce49":"## Define model\nIn the baseline, we use ResNet50 pretrained on ImageNet dataset. The classifier of model would be replaced with a new dense layer to make the output suit the problem.","e2b3124d":"`ImageDataGenerator` only accepts list of strings as label, we need to convert the label in the dataframe following that way","878bdbc2":"# Split the dataset","52a1fcc6":"Read the test data","f979e220":"Create data generator object","9d378f62":"# Data analyzing","603ec667":"# Observations on the dataset\nThe dataset provided is extremely imbalanced. In this baseline model, by simply train the model the original dataset, we will easily get overfitting on the training set and the score on the test set is very low. With the proposed methods below, you will tweak the training process and improve the metric score on the test set:\n- **Image Augmentation**: By augmenting images, we will have more data and make the training set become more regularize. [imgaug](https:\/\/github.com\/aleju\/imgaug) is a very strong augmentation library that you can use in this assignment\n- **Data sampling**: the idea here is to make the distribution between classes in the dataset balance. There are 2 kinds: oversampling and undersampling\n- **Adjust loss function**: the current loss function becomes very small after several epochs. By adding weights, we adjust the loss function to make it suitable for this imbalanced dataset. You can check the [BCEWithLogitsLoss](https:\/\/pytorch.org\/docs\/stable\/nn.html#bcewithlogitsloss) and try applying it.","f7d7e330":"For the data provided, we will split the dataset to 80% for training and 20% for validation","f9bc955b":"In this notebook, we will use Pytorch library to implement and train ResNet50 as a baseline model. With initial weights from ImageNet, we will retrain all layers for this problem.","32fe518e":"If you run on Google Colab, run the code below to download the dataset","74302c41":"## Define F1-score\nKeras recently removed the F1-score metric, we will implement it in the function below:","52a412c0":"Create training generator","4a5350d7":"# Build and train baseline model","471c10d2":"Test data generator","9d566e3c":"Write result to submission file","72c9807d":"As we can see, **opacity**, **normal** and **glaucoma** are diseases that share largest proportions in label distribution. The other diseases or combinations just account for small pieces.","9fe460a1":"## Analyze combination of classes","42020a06":"Import libraries","9a8dc41d":"# Read dataset","999e3dae":"## Implement Dataset loader\nIn Keras, you can use `ImageDataGenerator` to feed image to the model. However, the supported augmentation of `ImageDataGenerator` is not enough, we can add more from another library in this `preprocessing_image` below","13d1e362":"We need to train about 23 millions parameters","2cea23e9":"## Analyze distribution of 0 and 1 for each label","a373d386":"**To simplify the baseline model, the dataset is splited randomly. However, to improve the model, cross-validation techniques can be applied here**","ae9a6d69":"As can be observed, the number of label 0 is much more larger than label 1","01407418":"## Training\nFully training model\n","85a70f0e":"We also need to create validation generator. Different from training generator, we don't shuffle the validation set","3ebb9cc0":"Load best model weights and switch to evaluation mode","a912a48d":"# Inference","48f384d9":"Predict test images","4980430e":"## Define callbacks\nThere are 2 callbacks we need to add to the training:\n- Saving the best model on validation set\n- Reduce learning rate during training"}}