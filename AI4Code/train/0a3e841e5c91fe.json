{"cell_type":{"1db92752":"code","3ea35319":"code","f267dabe":"code","07f261d5":"code","0f761656":"code","cca01185":"code","df9c8328":"code","6146137e":"code","1e199a51":"code","99f49681":"code","7e121301":"code","19f09f9b":"code","1485d3b8":"code","aa1922aa":"code","6db5ec93":"code","f27cf585":"code","1eeea5e0":"code","6b7ab235":"code","00a82f7e":"code","71fc8303":"code","c329aa8e":"code","2c38ed96":"code","07644244":"code","920d95d2":"code","2dfa4c87":"code","b8695819":"code","fdd1025d":"code","bd6e6f2a":"code","dd25d3aa":"code","4a1003a4":"code","f8bfa82b":"code","08b7636c":"code","e47ef506":"code","bd2ff8b1":"code","beb6abfd":"code","0ba1ba26":"code","287d0d95":"code","8c9c5ae4":"code","cdc5df68":"code","6021bd0b":"code","75f8854b":"code","25979896":"code","d67d66f9":"code","d162259d":"code","09aff0ba":"code","7c637d78":"code","92f6b293":"markdown","4437f0d4":"markdown","44a723af":"markdown","64651237":"markdown","893b3b75":"markdown","60112f45":"markdown","bf2a0452":"markdown","89b121cb":"markdown","072aad79":"markdown","683638a8":"markdown","5da3274e":"markdown","0b8c8c12":"markdown","957389d3":"markdown","ebac95df":"markdown","1747de66":"markdown","3c48f9e5":"markdown"},"source":{"1db92752":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3ea35319":"df_train_features = pd.read_csv('..\/input\/train_features.csv')\ndf_train_labels = pd.read_csv(\"..\/input\/train_labels.csv\")\ndf_test_features = pd.read_csv(\"..\/input\/test_features.csv\")\nsample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")","f267dabe":"df_train_labels.columns","07f261d5":"df_train_features.shape","0f761656":"df_test_features.shape","cca01185":"import numpy as np\nmajority_class = df_train_labels['status_group'].mode()[0]\n#print(majority_class)\n\ny_pred = np.full(shape=df_train_labels['status_group'].shape, fill_value=majority_class)","df9c8328":"df_train_labels.status_group.shape, y_pred.shape","6146137e":"all(y_pred==majority_class)","1e199a51":"from sklearn.metrics import accuracy_score \naccuracy_score(df_train_labels['status_group'], y_pred)","99f49681":"df_train_labels['status_group'].value_counts()","7e121301":"df_train_labels['status_group'].value_counts(normalize=True)","19f09f9b":"%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom mlxtend.plotting import plot_decision_regions\n\n\n#let's import the warning before running any sophisticated methods\nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)","1485d3b8":"print(classification_report(df_train_labels['status_group'], y_pred) )","aa1922aa":"#Let's merge train and test\nfull_df = pd.concat([df_train_features, df_test_features])","6db5ec93":"full_df.shape","f27cf585":"#these were the number of rows in the orignal sets\n59400 + 14358","1eeea5e0":"full_df.head()","6b7ab235":"full_df.isna().sum()","00a82f7e":"#data['Native Country'] = data['Native Country'].fillna(data['Native Country'].mode()[0])\nfull_df['funder'] = full_df['funder'].fillna(full_df['funder'].mode()[0])","71fc8303":"full_df['installer'] = full_df['installer'].fillna(full_df['installer'].mode()[0])","c329aa8e":"full_df['subvillage'] = full_df['subvillage'].fillna(full_df['subvillage'].mode()[0])","2c38ed96":"full_df['public_meeting'] = full_df['public_meeting'].fillna(full_df['public_meeting'].mode()[0])","07644244":"full_df['scheme_management'] = full_df['scheme_management'].fillna(full_df['scheme_management'].mode()[0])","920d95d2":"full_df['permit'] = full_df['permit'].fillna(full_df['permit'].mode()[0])","2dfa4c87":"full_df.isna().sum()","b8695819":"full_df = full_df.drop(columns = 'scheme_name')","fdd1025d":"#split the data back\nX_cleaned = full_df[:-14358]\nX_test_cleaned = full_df[-14358:]\ny = df_train_labels['status_group']","bd6e6f2a":"X_cleaned.shape, X_test_cleaned.shape, y.shape\n","dd25d3aa":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_cleaned, y, test_size=0.25, random_state=42, shuffle=True)","4a1003a4":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","f8bfa82b":"X_train.dtypes","08b7636c":"X_train_numeric = X_train.select_dtypes(np.number)","e47ef506":"X_test_numeric = X_test.select_dtypes(np.number)","bd2ff8b1":"#let's see how our model does here\n\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train_numeric, y_train)\ny_pred = model.predict(X_test_numeric)\naccuracy_score(y_test, y_pred)","beb6abfd":"from sklearn.preprocessing import LabelEncoder\ndef dummyEncode(df):\n        columnsToEncode = list(df.select_dtypes(include=['category','object']))\n        le = LabelEncoder()\n        for feature in columnsToEncode:\n            try:\n                df[feature] = le.fit_transform(df[feature])\n            except:\n                print('Error encoding '+feature)\n        return df","0ba1ba26":"#encode our train df that we split from the full_df\ncat_coded_df = dummyEncode(X_cleaned)","287d0d95":"cat_coded_df.head()","8c9c5ae4":"#let's also encode out test set we split from full_df\nX_cleaned_test = dummyEncode(X_test_cleaned)","cdc5df68":"X_cleaned_test.head()","6021bd0b":"#split our train set that we just encoded (and assigned into cat_coded_df)\n#into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(cat_coded_df, y, test_size=0.25, random_state=42, shuffle=True)","75f8854b":"import warnings\nfrom sklearn.exceptions import DataConversionWarning\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)","25979896":"#run multinomial logistic regression\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(solver='newton-cg', multi_class='multinomial')\nmodel.fit(X_train, y_train)","d67d66f9":"y_pred = model.predict(X_test)","d162259d":"accuracy_score(y_test, y_pred)","09aff0ba":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\npipeline = make_pipeline(StandardScaler(), \n                        LogisticRegression(solver='newton-cg', multi_class='multinomial'))\n\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)","7c637d78":"accuracy_score(y_test, y_pred)","92f6b293":"For now, we aren't touching X_test_cleaned dataframe. That's our actual test data. We are only using our training that we split furtherinto training and testing sets","4437f0d4":"As expected, we do not see a great improvement over our baseline model. ","44a723af":"**Spliting the dataset into orignal shape**\n","64651237":"**a Bit of Cleaning**\n\nbefore we run logistic regression, let's do little bit of cleaning of the data","893b3b75":"**Class imbalance**\n\nIt's import to check how the classes are represented. ","60112f45":"**Logistic Regression with encoded categorical features and using StandardScaler**","bf2a0452":"i'll take care of the misisng values in other columns now","89b121cb":"**Inspectt the files**","072aad79":"**Majority Class Baseline**\n\nUsing the mode of the target variable as the majority class baseline \n","683638a8":"StandardScaler doesn't seem to have any impact on the scores. Infact, the score is a little worse now. ","5da3274e":" **classification report**","0b8c8c12":"**Logistic Regression with only Numeric Features**\n\nThe score isn't going to be super exciting but let's give a try before going into more complicated\/sophisticated models","957389d3":"Looking at the above report, it's a pretty bad prediction and recall. ","ebac95df":"**Encoding Catergorical Features using dummyEncoder from sklearn to perform Logistic R**\n\nSince our numeric only is not a significat improvment over the baseline model, let's one hot encode all catergorical without much cleaning. This encoding isn't a great choice for this kind of dataset (with lots of categories of categorical features so in some sense, it's a baseline encoding) as it encodes the catergories in ordianl fashion. ","1747de66":"**Logistic Regression only with Numeric Values**","3c48f9e5":"**Merge Training and Test datasets**\n\nUsing pandas concat function excluding 'axis=1' in the arguements. I'll notedown the last row of my train dataset to make the splitting easier (we want to retain the same train\/test shape and observations as originally given). The last row of our df_train_features dataset is 59394."}}