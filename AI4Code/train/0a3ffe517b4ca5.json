{"cell_type":{"8e9e8041":"code","d82d20bf":"code","4d371b50":"code","0f13e023":"code","edc116ca":"code","05eaf166":"code","e0f78173":"code","97c743e0":"code","b1f35d53":"code","98875c3b":"code","7f185141":"code","6617c868":"code","b2ab446b":"code","a006c886":"code","7421e41c":"code","446ba17a":"code","d6c5bc01":"code","2faa4b6b":"code","130311a8":"code","800f6d29":"code","408fe9a1":"code","ac744926":"markdown","d8b5772f":"markdown","15dd9fde":"markdown","16ac4ca6":"markdown","d7663e1f":"markdown","ab1f0f54":"markdown","176194dc":"markdown","3e7dc418":"markdown","0ef78e5c":"markdown","33ee89e0":"markdown"},"source":{"8e9e8041":"! pip install matplotlib numpy pandas scipy scikit-learn","d82d20bf":"%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","4d371b50":"# Load the CSV into a pandas DataFrame\nhousing = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col=\"Id\")","0f13e023":"# View the `head()` (first few rows and columns) of the DataFrame\nhousing.head()","edc116ca":"# Get meta information on the data: data types, column names, etc\nhousing.info()","05eaf166":"housing.columns[housing.isna().any()].tolist()","e0f78173":"housing.corr()[\"SalePrice\"].sort_values(ascending=False)","97c743e0":"housing.describe()['OverallQual']","b1f35d53":"housing['OverallQual'].value_counts().sort_index()","98875c3b":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(housing, housing[\"OverallQual\"]):\n    strat_train_set = housing.iloc[train_index]\n    strat_test_set = housing.iloc[test_index]","7f185141":"from sklearn.model_selection import train_test_split\n\n# Compare stratification with random sampling\n\nrand_train_set, rand_test_set = train_test_split(housing, test_size=0.2, random_state=42)\n\ndef overall_quality_proprotion(data):\n    return data[\"OverallQual\"].value_counts() \/ len(data)\n\noverall = overall_quality_proprotion(housing)\nstratified = overall_quality_proprotion(strat_test_set)\nrandom = overall_quality_proprotion(rand_test_set)\n\npd.DataFrame({\n    'Overall': overall,\n    'Stratified': stratified,\n    'Random': random,\n    'Strat %error': 100 * stratified \/ overall - 100,\n    'Random %error': 100 * random \/ overall - 100\n}).sort_index()","6617c868":"# Save these sets for next time\n\nstrat_train_set.to_csv('train_set.csv', index=\"Id\")\nstrat_test_set.to_csv('test_set.csv', index=\"Id\")","b2ab446b":"# Load models\n\n# train_set = pd.read_csv('train_set.csv', index_col=0)\n# test_set = pd.read_csv('test_set.csv', index_col=0)\n\ntrain_set = strat_train_set.copy()\ntest_set = strat_test_set.copy()","a006c886":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n# function for comparing different approaches\ndef score_dataset(features, labels):\n    \"\"\"\n    Function for compring different data preparation approaches\n\n    Parameters\n    ----------\n    features : array_like (same len as labels)\n        the features for a given dataset\n    labels   : array_like (same len as features)\n        the labels for a given dataset\n\n    Returns\n    -------\n    loss: float or ndarray of floats\n        a value in a string\n        \n    MAE output is non-negative floating point. The best value is 0.0.\n    \"\"\"\n    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n    model = RandomForestRegressor(n_estimators=100, random_state=42)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    \n    return mean_absolute_error(y_test, preds)","7421e41c":"# For this first round of data clean (removing `null` values), we'll need to remove\n# any categorical data so that we can evalute each null-cleaning approach\nprep_train = train_set.copy().select_dtypes(exclude=['object'])\n\n# Keep track of scores so we can compare them at the end\nscores = pd.DataFrame(columns = [\n    \"approach\",\n    \"score\"\n])","446ba17a":"# Drop rows\ndropped_null_rows = prep_train.dropna()\n\ndropped_null_features = dropped_null_rows.drop(\"SalePrice\", axis=1)\ndropped_null_labels = dropped_null_rows[\"SalePrice\"].copy()\n\nscore = score_dataset(dropped_null_features, dropped_null_labels)\nscores = scores.append({\"approach\": \"drop_rows\", \"score\": score}, ignore_index=True)\n\n\nprint(\"Rows Dropped: \", len(prep_train[prep_train.isna().any(axis=1)]))\nprint(\"Total Rows: \", len(prep_train))\nprint(\"Score: \", score)","d6c5bc01":"# Drop columns\ndropped_null_cols = prep_train.dropna(axis=1)\n\ndropped_null_features = dropped_null_cols.drop(\"SalePrice\", axis=1)\ndropped_null_labels = dropped_null_cols[\"SalePrice\"].copy()\n\nscore = score_dataset(dropped_null_features, dropped_null_labels)\nscores = scores.append({\"approach\": \"drop_cols\", \"score\": score}, ignore_index=True)\n\n\nprint(\"Columns Dropped: \", len(prep_train.columns[prep_train.isna().any()]))\nprint(\"Total Columns: \", len(prep_train.columns))\nprint(\"Score: \", score)","2faa4b6b":"# SimpleImputer\nfrom sklearn.impute import SimpleImputer\n\n# We instantiate it with a strategy, in this case, median\nimputer = SimpleImputer(strategy=\"median\")\n\nimputed_null_values = pd.DataFrame(imputer.fit_transform(prep_train))\n\n# SimpleImputer removed column names...\nimputed_null_values.columns = prep_train.columns\n\nimputed_null_features = imputed_null_values.drop(\"SalePrice\", axis=1)\nimputed_null_labels = imputed_null_values[\"SalePrice\"].copy()\n\nscore = score_dataset(imputed_null_features, imputed_null_labels)\nscores = scores.append({\"approach\": \"median_impute\", \"score\": score}, ignore_index=True)\n\n\nprint(\"Values Imputed: \", prep_train.isna().sum().sum())\nprint(\"Score: \", score)","130311a8":"# SimpleImputer + `_was_null` column\n\nimputer = SimpleImputer(strategy=\"median\")\n\nnull_plus = prep_train.copy()\n\n# Add boolean columns for missing values\nfor col in prep_train.columns[prep_train.isna().any()]:\n     null_plus[col + '_was_missing'] = null_plus[col].isnull()\n\n# Impute in place\nimputed_null_plus = pd.DataFrame(imputer.fit_transform(null_plus))\n\n# SimpleImputer removed column names...\nimputed_null_plus.columns = null_plus.columns\n\nimputed_plus_features = imputed_null_plus.drop(\"SalePrice\", axis=1)\nimputed_plus_labels = imputed_null_plus[\"SalePrice\"].copy()\n\nscore = score_dataset(imputed_plus_features, imputed_plus_labels)\nscores = scores.append({\"approach\": \"median_impute_plus\", \"score\": score}, ignore_index=True)\n\nprint(\"Values Imputed: \", prep_train.isna().sum().sum())\nprint(\"Score: \", score)","800f6d29":"scores.sort_values(by=\"score\")","408fe9a1":"# Get list of categorical variables\n\nobject_features = train_set.select_dtypes('object').columns; object_features","ac744926":"# Split Data\nWithout more information from the data team, I don't know that there's an obvious way to stratify the data, which means that I'm willing to do a simple 80:20 split using `Scikit-learn`'s `train_test_split()`. First, I'm interested to see if there are any strong correlations that I should consider.","d8b5772f":"## Thoughts\nIt appears that `OverallQual`, which the \"Overall material and finish quality\" on a scale from `1` to `10`, is a strong predictor of `SalePrice`, (0.79 correlation). Moreover, the distribution of `OverallQual` is not even across all of the data. I'm inclinded to stratify the train\/test sets along `OverallQual`. \n\nIt is possible that a categorical attribute has a stronger correlation. I'll see if I can revisit this later.","15dd9fde":"## First Glance\n* 1460 rows\n* 81 columns\n* Most columns (43) are of type `object`, read: non-numeric\n* 19 columns have some number of `null` values\n* We're trying to predict `housing[\"SalePrice\"]`","16ac4ca6":"# Data Preparation\n","d7663e1f":"Because there are so few examples for `OverallQual` for `1`, `2`, `9`, and `10`, I'm wondering if it would be better to stratify into larger groupings, say `1`-`3`, `4`-`7`, and `8`-`10`\u2014`poor`, `average`, and `excellent`.","ab1f0f54":"## Dataset Evaluation\nWe can develop a metric to quickly evaluate the effect that each data prep approach has on our ability to predict `SalesPrice`.\nFor example, if we try to drop the categorical features, we can fit a RandomForestRegressor and calculate the Mean Absolute Error. Then do the same for label encoding and one-hot encoding. By fitting a model and calculating the loss, we can determine which approach gives us better results, (the lowest MAE).","176194dc":"# Framing\n## Objective\nIt is your job to predict the sales price for each house. For each Id in the test set, you must predict the value of the SalePrice variable.\n### Output\nThe file should contain a header and have the following format:\n```csv\nId,SalePrice\n1461,169000.1\n1462,187724.1233\n1463,175221\netc.\n```\n## Performance Metric\nSubmissions are evaluated on Root-Mean-Squared-Error (RMSE) between the logarithm of the predicted value and the logarithm of the observed sales price. (Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally.)\n\n","3e7dc418":"# Data Exploration","0ef78e5c":"## Data Cleaning\nMost ML models won't work with `(null)` values. As we've seen already, 19 columns have some number of `null` values.\n\nWe can:\n\nGet rid of rows that are incomplete, i.e. have `(null)` values somewhere. Be sure to also remove the labels for the corresponding rows.\n\n```python\n housing.dropna(subset=[\"MasVnrArea\"])\n```\n\nGet rid of the attribute (column) entirely\n\n```python\nhousing.drop(\"MasVnrArea\", axis=1)\n```\n\nSet the values (to zero, the mean, the median, etc.), a.k.a. _impute_\n\n```python\nmedian = housing[\"MasVnrArea\"].median()\nhousing[\"MasVnrArea\"].fillna(median, inplace=True)\n\n```\nWe'd need to keep this median value to also fill in missing data in our test set, as well as any new data that comes in.\n\nAs an additional step to imputation, we can add an additional column which indicates that a certain value was imputed. We do this to see if the model performs better by knowing about which data was missing.\n\n```python\nhousing[\"MasVnrArea_was_missing\"] = housing[\"MasVnrArea\"].isnull()\nmedian = housing[\"MasVnrArea\"].median()\nhousing[\"MasVnrArea\"].fillna(median, inplace=True)\n```","33ee89e0":"## Categorical Attributes\n\nThe three approches to dealing with categorical values 1) dropping, 2) label encoding, and 3) one-hot encoding will vary in performance, but for our first approach, let's prep all of the categorical attributes the same way, and the evaluate performance with on a Random Forest model."}}