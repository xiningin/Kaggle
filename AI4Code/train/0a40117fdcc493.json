{"cell_type":{"bec5415a":"code","f035a43e":"code","2987a2aa":"code","b4ec4d08":"code","7228246a":"code","0dc86e6d":"code","740309b6":"code","6680af4d":"code","1bd7c7e9":"code","f4fd1ab2":"code","5c2e89e2":"code","2892b54e":"code","c1d5f751":"code","0a247096":"code","fc09a92e":"code","247ffa4c":"code","f07e1773":"code","ac5df33f":"code","e21df35e":"code","4c05dc92":"code","30d0196d":"code","48df43c0":"code","2ac6a5d9":"code","11c71716":"code","0db6a6ab":"code","070a31b0":"code","05da402a":"code","ae36e615":"code","b6b7828f":"code","4ec07ac0":"code","07bafe8e":"code","1f31094a":"code","bc51b2c4":"code","9e8ba760":"code","9b1e2319":"code","1a01fb45":"code","5c4d5292":"code","acf3efde":"code","83053058":"code","e9f9c4f4":"code","03b53df1":"code","d803011e":"code","863410ed":"code","adc55fa2":"code","a415d88e":"code","a52b8f97":"code","ea574f27":"markdown","781ae4cc":"markdown","09417c95":"markdown","f3877258":"markdown","864f81ff":"markdown","9c4caf2e":"markdown","023709b2":"markdown","6a9ced56":"markdown","1b732db1":"markdown","e6f30033":"markdown","bf238ccf":"markdown","4df07688":"markdown","001ed7f9":"markdown","2e952bc6":"markdown","da758d0c":"markdown","f3f5e963":"markdown","f64c8274":"markdown","c66ecd0d":"markdown","516e14fc":"markdown","d15020da":"markdown","2046137c":"markdown","838035d7":"markdown","6b3bf0da":"markdown","db6a822a":"markdown","857c7992":"markdown","2900542c":"markdown","d0e263da":"markdown","b6aa1138":"markdown","b9cf130f":"markdown","38fe11e9":"markdown","ffe64bbf":"markdown","521a9c06":"markdown","fc70986a":"markdown","1010c4b8":"markdown","6c5480cd":"markdown","795b8507":"markdown","7b82d893":"markdown","3a66adb0":"markdown","a1b2fe9c":"markdown","e9a56924":"markdown","860fbbfc":"markdown","070922ce":"markdown","7aca67c9":"markdown","3d050b18":"markdown","637ce1fa":"markdown","02158a3b":"markdown","443fa395":"markdown","533b656a":"markdown","b2b420c4":"markdown","3a4dbcf8":"markdown","0287d2b2":"markdown","f0b2890d":"markdown","881dca3e":"markdown","83664875":"markdown","2cb5caba":"markdown","5b3136c7":"markdown"},"source":{"bec5415a":"import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom keras import layers, models\nfrom keras.utils import to_categorical\nimport tensorflow as tf\nimport datetime, os\nfrom matplotlib.pyplot import figure\nimport missingno\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nimport seaborn as seb\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.gaussian_process.kernels import DotProduct\nfrom sklearn.gaussian_process.kernels import Matern\n\nseed = 42\nnp.random.seed(seed)","f035a43e":"train_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","2987a2aa":"train_data.head(3)","b4ec4d08":"test_data.head(3)","7228246a":"id = test_data[\"PassengerId\"]\n\ntrain_data.drop([\"PassengerId\", \"Ticket\"], axis =1, inplace=True)\ntest_data.drop([\"PassengerId\", \"Ticket\"], axis =1, inplace=True)","0dc86e6d":"for df in [train_data, test_data]:\n    missingno.matrix(df)","740309b6":"#fill in the NaN using the most common value\ntrain_data['Embarked'].fillna(train_data['Embarked'].mode()[0], inplace = True)\n\n#fill in the NaN using the median value\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())\n\n#similar to the previous one\ntrain_data['Age'] = train_data['Age'].fillna(train_data['Age'].median())\ntest_data['Age'] = test_data['Age'].fillna(test_data['Age'].median())\n\n#and here let the absence of the cabin number be characterized by the letter 'O'\ntrain_data['Cabin'] = train_data['Cabin'].fillna('O')\ntest_data['Cabin'] = test_data['Cabin'].fillna('O')","6680af4d":"for df in [train_data, test_data]:\n    missingno.matrix(df)","1bd7c7e9":"figure(num=None, figsize=(15, 4), dpi=100)\nseb.boxplot(x=\"Fare\",  data=train_data)","f4fd1ab2":"train_data = train_data[train_data['Fare'] <= 300]","5c2e89e2":"train_data = train_data.reset_index(drop = True)","2892b54e":"train_data['Sex'].value_counts()","c1d5f751":"test_data['Sex'].value_counts()","0a247096":"train_data['Sex'] = train_data['Sex'].replace({'male': 0, 'female': 1})\ntest_data['Sex'] = test_data['Sex'].replace({'male': 0, 'female': 1})","fc09a92e":"train_data['Embarked'] = train_data['Embarked'].replace({'S': 1, 'C': 2, 'Q': 3, 'O' : 0})\ntest_data['Embarked'] = test_data['Embarked'].replace({'S': 1, 'C': 2, 'Q': 3, 'O' : 0})","247ffa4c":"train_data = pd.get_dummies(train_data, columns = ['Pclass', 'Sex', 'Embarked'])\ntest_data = pd.get_dummies(test_data, columns = ['Pclass', 'Sex', 'Embarked'])","f07e1773":"train_data['Family_members'] = train_data['Parch'] + train_data['SibSp']\ntest_data['Family_members'] = test_data['Parch'] + test_data['SibSp']","ac5df33f":"train_data['Cabin'] = train_data['Cabin'].apply(lambda x: x[0])\ntest_data['Cabin'] = test_data['Cabin'].apply(lambda x: x[0])","e21df35e":"le = LabelEncoder()\ntrain_data['Cabin'] = le.fit_transform(train_data['Cabin'])\ntest_data['Cabin'] = le.fit_transform(test_data['Cabin'])","4c05dc92":"train_data['Cabin'].value_counts()","30d0196d":"#divide the set of different ages into 4 parts\nage_bins_train = np.linspace(np.min(train_data['Age']), np.max(train_data['Age']), 4)\n\n#set the names of these parts (classes)\nage_classes = [1,2,3]\n\n#creating a new column\ntrain_data['Age_classes'] = pd.cut(train_data['Age'], bins = age_bins_train, labels = age_classes, include_lowest = True)\n\n#same with the test dataset\nage_bins_test = np.linspace(np.min(test_data['Age']), np.max(test_data['Age']), 4)\ntest_data['Age_classes'] = pd.cut(test_data['Age'], bins = age_bins_test, labels = age_classes, include_lowest = True)","48df43c0":"fare_bins_train = np.linspace(np.min(train_data['Fare']), np.max(train_data['Fare']), 5)\nfare_classes = [1,2,3,4]\ntrain_data['Fare_classes'] = pd.cut(train_data['Fare'], bins = fare_bins_train, labels =fare_classes, include_lowest = True)\n\nfare_bins_test = np.linspace(np.min(test_data['Fare']), np.max(test_data['Fare']), 5)\ntest_data['Fare_classes'] = pd.cut(test_data['Fare'], bins = fare_bins_test, labels =fare_classes, include_lowest = True)","2ac6a5d9":"#extracting the title from the name\ntrain_data['Name'] = train_data['Name'].apply(lambda x: x.split('.')[0].split(',')[1][1:])\n\ntrain_data['Name'].value_counts()","11c71716":"#creating a separate class for rare titles\ntrain_data['Name'] = train_data['Name'].replace(['Lady', 'Ms', 'Mme', 'the Countess', 'Mlle', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n#encoding titles\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ntrain_data['Name'] = train_data['Name'].map(title_mapping)\n\n#just in case there are missing values, we will replace them with zeros\ntrain_data['Name'] = train_data['Name'].fillna(0)","0db6a6ab":"train_data['Name'].value_counts()","070a31b0":"test_data['Name'] = test_data['Name'].apply(lambda x: x.split('.')[0].split(',')[1][1:])\ntest_data['Name'] = test_data['Name'].replace(['Lady', 'Ms', 'Mme', 'the Countess', 'Mlle', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntest_data['Name'] = test_data['Name'].map(title_mapping)\ntest_data['Name'] = test_data['Name'].fillna(0)","05da402a":"train_data.head()","ae36e615":"scaler = StandardScaler()\n\n#selecting specific columns from the dataset\nunscaled_train = train_data[[\"Age\", \"Fare\", \"Cabin\", \"SibSp\", \"Parch\", \"Family_members\", \"Name\", \"Age_classes\", \"Fare_classes\"]]\n\n#scaling\nscaled_train = scaler.fit_transform(unscaled_train)\n\n#in the previous step, we got a numpy array, so we convert it back to a DataFrame\nscaled_train = pd.DataFrame(scaled_train, columns = [\"Age\", \"Fare\", \"Cabin\", \"SibSp\", \"Parch\", \"Family_members\", \"Name\", \"Age_classes\", \"Fare_classes\"])\n\n#creating a dataset without the attributes that we scaled\ntrain_data_scaling = train_data.drop([\"Age\", \"Fare\", \"Cabin\", \"SibSp\", \"Parch\", \"Family_members\", \"Name\", \"Age_classes\", \"Fare_classes\"], axis = 1)\n\n#putting it all together\ntrain_data_scaled = pd.concat([scaled_train, train_data_scaling], axis=1, sort=False)\ntrain_data = train_data_scaled","b6b7828f":"train_data.head(3)","4ec07ac0":"scaler = StandardScaler()\nunscaled_test = test_data[[\"Age\", \"Fare\", \"Cabin\", \"SibSp\", \"Parch\", \"Family_members\", \"Name\", \"Age_classes\", \"Fare_classes\"]]\nscaled_test = scaler.fit_transform(unscaled_test)\nscaled_test = pd.DataFrame(scaled_test, columns = [\"Age\", \"Fare\", \"Cabin\", \"SibSp\", \"Parch\", \"Family_members\", \"Name\", \"Age_classes\", \"Fare_classes\"])\ntest_data_scaling = test_data.drop([\"Age\", \"Fare\", \"Cabin\", \"SibSp\", \"Parch\", \"Family_members\", \"Name\", \"Age_classes\", \"Fare_classes\"], axis = 1)\ntest_data_scaled = pd.concat([scaled_test, test_data_scaling], axis=1, sort=False)\ntest_data = test_data_scaled","07bafe8e":"#throwing overboard doppelgangers\ntrain_data.drop_duplicates(inplace = True)\n\n#restoring the order of the indexes\ntrain_data = train_data.reset_index(drop = True)","1f31094a":"train_labels = train_data['Survived']\ntrain_data.drop('Survived', axis = 1, inplace = True)","bc51b2c4":"train_data.head(3)","9e8ba760":"test_data.head(3)","9b1e2319":"#type coercion\ntrain_data = train_data.astype('float32')\n\n#just in case, we'll leave DataFrame-copy\ntrain_data_df = train_data\n\n#DataFrame to numpy array\ntrain_data = train_data.to_numpy()\n\n#similarly\ntest_data = test_data.astype('float32')\ntest_data_df = test_data\ntest_data = test_data.to_numpy()\n\ntrain_labels = train_labels.to_numpy()","1a01fb45":"#initializing the classifier\nforest = ExtraTreesClassifier(n_estimators=250,\n                              random_state=seed)\nforest.fit(train_data, train_labels.reshape(-1,))\n\n#output a sorted list of important parameters and build a graph\nimportances = forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\nprint(\"Feature ranking:\")\n\nfor f in range(train_data.shape[1]):\n    print(\"%d. '%s' (%f)\" % (f + 1, train_data_df.columns[indices[f]], importances[indices[f]]))\n\nfigure(num=None, figsize=(8, 6), dpi=100)\nplt.title(\"Feature importances\")\nplt.bar(range(train_data.shape[1]), importances[indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(train_data.shape[1]), indices)\nplt.xlim([-1, train_data.shape[1]])\nplt.show()","5c4d5292":"#removing unnecessary columns from both sets\ntrain_data_df.drop([\"Embarked_1\", \"Embarked_2\", \"Embarked_3\", \"Fare_classes\", 'Parch'], axis =1, inplace=True)\ntest_data_df.drop([\"Embarked_1\", \"Embarked_2\", \"Embarked_3\", \"Fare_classes\", 'Parch'], axis =1, inplace=True)\n\n#getting the final datasets\ntrain_data = train_data_df.to_numpy()\ntest_data = test_data_df.to_numpy()","acf3efde":"#model names\nnames = [\"Nearest Neighbors\", \n         \"Linear SVM\", \n         \"RBF SVM\", \n         \"Gaussian Process\",\n         \"Decision Tree\", \n         \"Random Forest\", \n         \"AdaBoost\",\n         \"Naive Bayes\"\n         ]\n\n#initialize models\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    GaussianProcessClassifier(1.0 * RBF(1.0)),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    AdaBoostClassifier(),\n    GaussianNB()\n    ]\n\n#output the accuracy of each model on cross-validation with 5 folds\nfor name, clf in zip(names, classifiers):\n    print(\"%s : %.2f%%\" % (name, np.mean(cross_val_score(clf, train_data, train_labels.reshape(-1,), cv=5))*100))","83053058":"gp_param = {\n    'kernel':[DotProduct(i) for i in [2,3,5]] + [Matern(i) for i in [2,3,5]]  + [RBF(i) for i in [2,3,5]],\n    'max_iter_predict' : [30, 50, 80]\n    }\n\n\ngp = GaussianProcessClassifier(random_state=seed)\ngp_clf = GridSearchCV(gp, gp_param, n_jobs=-1, verbose=1)\ngp_clf.fit(train_data, train_labels)","e9f9c4f4":"#show best parameters\ngp_best_params = gp_clf.best_params_    \nprint('Best params : ', gp_best_params)\ngp_best = gp_clf.best_estimator_","03b53df1":"print(np.mean(cross_val_score(gp_best, train_data, train_labels.reshape(-1,), cv=5)))","d803011e":"#a simple function that returns a network trained on N epochs\ndef get_network(N):\n    all_accuracy_history = []\n    all_val_accuracy_history = []\n    num_epochs = N\n\n    #perform cross-validation with five folds\n    skf = StratifiedKFold(n_splits = 5, random_state = seed, shuffle = True)\n\n    for train_index, val_index in skf.split(train_data, train_labels):\n        X_train, X_val = train_data[train_index], train_data[val_index]\n        y_train, y_val = train_labels[train_index], train_labels[val_index]\n\n        network = models.Sequential()\n        network.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n        network.add(layers.Dense(64, activation='relu'))\n        network.add(layers.Dense(1, activation='sigmoid'))\n        network.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n        history = network.fit(X_train, y_train, epochs = num_epochs, validation_data = (X_val, y_val), batch_size = 4, verbose = 0)\n        history_dict = history.history\n        all_accuracy_history.append(history_dict['accuracy'])\n        all_val_accuracy_history.append(history_dict['val_accuracy'])\n\n    avg_accuracy_per_epoch = [np.mean([x[i] for x in all_accuracy_history]) for i in range(num_epochs)]\n    avg_val_accuracy_per_epoch = [np.mean([x[i] for x in all_val_accuracy_history]) for i in range(num_epochs)]\n\n    #plotting how the accuracy of our model has changed over time to track overfitting\n    figure(num=None, figsize=(8, 6), dpi=100)\n    plt.plot(range(1, num_epochs + 1), avg_accuracy_per_epoch, 'b', label='train')\n    plt.plot(range(1, num_epochs + 1), avg_val_accuracy_per_epoch, 'r', label='validation')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n\n    print(\"Average accuracy: %.2f%%\" % (np.mean([np.mean(fold) for fold in all_val_accuracy_history])*100))\n\n    return network","863410ed":"network = get_network(20)","adc55fa2":"network = get_network(10)","a415d88e":"#getting predictions on the test set\nresult = network.predict(test_data)\n\n#let's take a look at the predictions\nresult[:5]","a52b8f97":"#convert to binary form\nresult =(result > 0.5)\n\n#we create the final answer and output it to the \"answer.csv\" file\nanswer = np.c_[id, result]\nanswer = answer.astype('int64')\nanswer = pd.DataFrame(answer, columns=[\"PassengerId\", \"Survived\"])\nanswer.to_csv(\"answer.csv\", index=False)","ea574f27":"Apparently, the landing location, fare classes, and 'Parch' have almost no effect on the model's predictions. So let's remove them from our dataset so that they don't negatively affect the accuracy of predictions","781ae4cc":"Well, it's time to build new features!\n\nFirst, let's count the family members for each passenger.","09417c95":"Apparently, our network begins to overfit somewhere around the 10th epoch. Let's train our network again, only this time on 10 epochs. Accuracy should improve","f3877258":"Good! The first step is done!\nHowever, now let's take a look at the 'Fare' column. Something tells me there are outliers here. Let's build a boxplot for this parameter.","864f81ff":"<a id=\"section-3-3\"><\/a>\n### Scaling features and duplicates deleting ","9c4caf2e":"Now we just need to get the predictions on the test set and save the result in the \"answer.csv\" file\"","023709b2":"Let's train the ExtraTreesClassifier and see which features it finds least useful","6a9ced56":"Not bad! However, I suggest not to stop there and try to build a neural network for our task. Maybe it will give us a better result","1b732db1":"Let the number of epochs be equal to 20","e6f30033":"Let's look again at the matrix of missing values in both datasets","bf238ccf":"In conclusion, we will throw out of the training set the passengers-doubles (ha-ha, another joke of mine)\n\n","4df07688":"# Exploring the Titanic Dataset\n\n   by M.Shcherbakov\n   \n   27 September 2020","001ed7f9":"<a id=\"section-6\"><\/a>\n# Step 6. Modeling","2e952bc6":"Let's train different classifiers and take the five most common models. Than train them, and see which ones show the highest accuracy in cross-validation","da758d0c":"<a id=\"section-3-1\"><\/a>\n### Encoding","f3f5e963":"'Sex' column contains only two different values: 'male' and 'female'. Encode them with '0' and '1'","f64c8274":"Let's add another feature. We will divide the set of passenger ages into 4 parts (classes). And create a new column that correlates the passenger's age with the corresponding class","c66ecd0d":"Finally, convert all values to the 'float32' type and convert Data Frame to numpy array so that the algorithms can work with our data properly","516e14fc":"Now let's look at the missing values","d15020da":"<a id=\"section-2-2\"><\/a>\n### Outliers","2046137c":"<a id=\"section-4\"><\/a>\n# Step 4. Final Data Preparation","838035d7":"We won't need passenger indexes for training, so we'll remove them. However, we will save the indexes from the test data to a separate variable. We will need them to create a file with the final results.\nAdditionally, we will remove the \"Ticket\" column, since it is unlikely that the name of the ticket has any effect on survival","6b3bf0da":"Loading the training and testing datasets","db6a822a":"<a id=\"section-2-1\"><\/a>\n### Missing values","857c7992":"<a id=\"section-2\"><\/a>\n# Step 2. Check for null-values and outliers","2900542c":"First, import all the necessary libraries, and set the seed for the (pseudo-)random number generator","d0e263da":"We do the same with the test data:","b6aa1138":"Of course, we also need to check the other parameters for outliers, but I will omit this so as not to make the notebook too long. Just trust me that there are no outliers in the other columns (I checked)","b9cf130f":"Here's what we have:","38fe11e9":"Wow! There are pretty many of them. But this is not a problem. We can easily deal with them now.","ffe64bbf":"After we mercilessly threw a few people overboard (ha-ha), the order of the indexes was broken, so let's put everything back in its place","521a9c06":"Similarly, we will divide 'Fare' into classes","fc70986a":"<a id=\"section-7\"><\/a>\n# Step 7. Conclusion","1010c4b8":"Repeat the same for the test data","6c5480cd":"Let's use the Keras framework and write a simple function that returns a standard three-layer network for binary classification, trained on N epochs. At the exit, it will give the probability that the passenger survived.","795b8507":"Disorder! One of the values flew somewhere in the sky. But never mind, we can easily get rid of it by simply dropping values less than 300 (lousy outliers...)","7b82d893":"Next, instead of the full cabin number, we will only use its first letter. It probably means a deck on a ship, or something like that.","3a66adb0":"To make our algorithm work correctly, let's scale the values so that they are all in the same range. Apply StandardScaler to 9 of the 13 columns. 'Survived' is a target feature, so we don't touch it. But with 'Pclass', 'Sex' and 'Embarked' I'll figure it out in the next step","a1b2fe9c":"<a id=\"section-3\"><\/a>\n# Step 3. Feature Engineering","e9a56924":"Let's have a look at our data","860fbbfc":"Let's take a last look at our passengers (come on, don't cry...)","070922ce":"To make our algorithm work correctly, we will encode the cab names using the Label Encoder","7aca67c9":"<a id=\"section-5\"><\/a>\n# Step 5. Feature Importance","3d050b18":"Here is the result:","637ce1fa":"We will do the same with 'Embarked'","02158a3b":"This is the result:","443fa395":"Use the get_dummies method to perform one-hot-encoding of the remaining categorical parameters","533b656a":"Let's take the \"Gaussian Process\" model, which proved to be excellent, and select the best hyperparameters for it using a Grid Search","b2b420c4":"Voila! All that remains is to remove the target attribute from the training set and place it in a separate variable","3a4dbcf8":"<a id=\"section-3-2\"><\/a>\n### Creating New Features","0287d2b2":"<a id=\"section-1\"><\/a>\n# Step 1. Load and split data","f0b2890d":"That's right, the accuracy has increased! Moreover, it is even larger than the \"Gaussian Process\" that we built earlier. Therefore, this neural network will be our final model.","881dca3e":"It turns out that there are quite a lot of titles. However, since many of them are extremely rare, we will combine them into a separate 'Rare' class, and then encode them using numbers from 1 to 5","83664875":"**Thank you to everyone who has read up to this point!**\n\n**And UPVOTE please, if you enjoyed reading this!**","2cb5caba":"Now let's look at the names of the passengers. I suggest taking only the titles that precede the name, and discarding everything else.","5b3136c7":"* [Step 1. Load and split data](#section-1)\n* [Step 2. Check for null-values and outliers](#section-2)\n    - [Missing values](#section-2-1)\n    - [Outliers](#section-2-2)\n* [Step 3. Feature Engineering](#section-3)\n    - [Encoding](#section-3-1)\n    - [Creating New Features](#section-3-2)\n    - [Scaling features and duplicates deleting](#section-3-3)\n* [Step 4. Final Data Preparation](#section-4)\n* [Step 5. Feature Importance](#section-5)\n* [Step 6. Modeling](#section-6)\n* [Step 7. Conclusion](#section-7)\n"}}