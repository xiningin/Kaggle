{"cell_type":{"e11c98d3":"code","34f53e18":"code","9ce0fb75":"code","12454dc8":"code","d8ffb684":"code","9d7d1e26":"code","856572b9":"code","a1c81507":"code","015b03a1":"code","0f7362a3":"code","fe379dab":"code","578a81e7":"code","621ee147":"code","96ded9c5":"code","902b5ee8":"code","9a4a92f1":"code","3dd3180c":"code","b9d813e9":"code","3938ad32":"code","44ceda52":"code","0260a186":"code","c2bd6ab0":"code","4360a553":"code","d3d34f9e":"code","2f5b03aa":"code","7b99f8a4":"code","1a0210b8":"code","fae32c07":"code","310174b9":"code","486931b6":"code","b192d762":"code","9d608c20":"code","4e7f4bb2":"markdown","3524f740":"markdown","a33af364":"markdown","532ef402":"markdown","748673c3":"markdown","843816a8":"markdown","3d0d3dae":"markdown","960ec62f":"markdown","76dd4c33":"markdown","613ed9e9":"markdown","300fa729":"markdown","ca48c318":"markdown","713728d5":"markdown","77805235":"markdown","48b62145":"markdown","14e9762c":"markdown","dad0941f":"markdown","81e936e1":"markdown","8f61f786":"markdown","abda3a5e":"markdown","1b74a4d9":"markdown","9dbad9cc":"markdown","2d64e315":"markdown","21c1c4f0":"markdown","93b9da30":"markdown"},"source":{"e11c98d3":"# https:\/\/huggingface.co\/docs\/transformers\/custom_datasets#tok_ner","34f53e18":"from sklearn.model_selection import train_test_split\nimport numpy as np\nimport pandas as pd\nimport os\nimport re\nimport torch\nfrom transformers import AutoTokenizer\nfrom datasets import Dataset\nfrom transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\nfrom transformers import AutoModelForSequenceClassification\nfrom datasets import Dataset, load_metric, DatasetDict\nimport nltk\nimport numpy as np\nfrom tqdm.auto import tqdm","9ce0fb75":"ISLOCAL = False\nTOKENIZER_NAME = \"distilbert-base-uncased\" #\"bert-base-uncased\" # \"allenai\/longformer-base-4096\" # \"distilbert-base-uncased\"\nUSE_SMALL_DATASET = False #If you want to train on whole dataset\nUSE_LOCAL_DATASET = True\nBATCH_SIZE = 8\nEPOCHS = 4\n\nCHECKPOINT = \"checkpoint-125\"\nINFER_ONLY = True\n\nDATASET_PATH = \"raw_dataset\"\nTOKENIZED_PATH = \"{}_tokenized\".format(TOKENIZER_NAME)\nTOKENIZER_PATH = \"{}_er\".format(TOKENIZER_NAME)\nMODEL_PATH = \"{}init_model\".format(TOKENIZER_NAME)\nTRAINOUT_PATH = \"{}-finetuned\".format(TOKENIZER_NAME)\n\n#SHOULD_I_TRAIN_MODEL = False #if only inference is needed\nSMALL_DATASET_SIZE = 50\n\nif ISLOCAL == True:\n    train_directory = \".\/train\"\n    test_directory = \".\/test\"\n    main_directory = \".\/\"\n    output_directory = \".\/output\"\nelse:\n    train_directory = \"..\/input\/feedback-prize-2021\/train\"\n    test_directory = \"..\/input\/feedback-prize-2021\/test\"\n    main_directory = \"..\/input\/feedback-prize-2021\/\"\n    output_directory = \"..\/input\/distillibertmodelner\/output\"","12454dc8":"def read_train_file(currid = \"423A1CA112E2\", curr_dir = train_directory):\n    with open(os.path.join(curr_dir, \"{}.txt\".format(currid)), \"r\") as f:\n        filetext = f.read()\n        \n    return filetext","d8ffb684":"train = pd.read_csv(os.path.join(main_directory, \"train.csv\"))\nss = pd.read_csv(os.path.join(main_directory,'sample_submission.csv'))","9d7d1e26":"train['discourse_start'] = train['discourse_start'].astype(int)\ntrain['discourse_end'] = train['discourse_end'].astype(int)\n\ndiscourse_labels = {x:i for i,x in enumerate(train['discourse_type'].unique())}","856572b9":"ss.head()","a1c81507":"def add_ner_start_ends(df):\n    '''\n    Process the predictionstring and return the very first and the very last index of the words for the current row.\n    This may be helpful in tokenizing for the NER tasks using numpy later.\n    df should be in the same format as train.csv\n    '''\n    ret = []\n\n    for i in tqdm(df.itertuples(), total = len(df)):\n        word_start = getattr(i, \"predictionstring\").split()[0]\n        word_end = getattr(i, \"predictionstring\").split()[-1]\n        word_start = int(word_start)\n        word_end = int(word_end)\n        \n        assert word_end >= word_start\n        \n        ret.append([int(word_start), int(word_end)])\n\n    ret = pd.DataFrame(ret)\n    ret.columns = ['word_start', 'word_end']\n    df = pd.concat([df, ret], axis = 1)\n    return df\n\n\ndef add_ner_label_ids(df):\n    d_labels = df['discourse_type'].unique()\n\n    df['label_b'] = 0\n    df['label_e'] = 0\n\n    for i, x in tqdm(enumerate(df['discourse_type'].unique())):\n        #print(i, \" --> \", x)\n        df.loc[ df['discourse_type'] == x, 'label_b'] = i * 2 + 1\n        df.loc[ df['discourse_type'] == x, 'label_e'] = i * 2 + 2\n\n    return df\n\ndef labels_to_discourse(df):\n    from collections import defaultdict\n    labels_to_discourse = defaultdict(str)\n    labels_to_discourse[0] = 'no-label'\n    \n    for i, x in tqdm(enumerate(df['discourse_type'].unique())):\n        labels_to_discourse[i * 2 + 1] = x\n        labels_to_discourse[i * 2 + 2] = x\n\n    return labels_to_discourse\n","015b03a1":"def prepare_dataset(df, direc = train_directory, limit = None):\n    ret = []\n    print(\"Total records to be processed are \", len(df['id'].unique()))\n    \n    \n    count = 0\n    ids = 0\n    for x in tqdm( df['id'].unique(), total =  len(df['id'].unique()) ):\n        count += 1\n        ids += 1\n        if limit is not None:\n            if count >= limit:\n                break\n        ft = read_train_file(x) #, # direc)\n        text_splits = ft.split()\n        #row = {\"id\" : x}\n        row = {\"id\" : ids}\n        \n        # now we need to update the \"ner_tags\"\n        result = np.zeros( len(text_splits), dtype = \"uint16\" )\n        for j, y in enumerate(df.loc[ df['id'] == x ].itertuples()):\n            #print(j, getattr(y, \"discourse_type\"))\n            result[ getattr(y, \"word_start\") ] = getattr(y, \"label_b\")\n            result[ getattr(y, \"word_start\") + 1 : getattr(y, \"word_end\") ] = getattr(y, \"label_e\")\n        row.update( {\"ner_tags\" : result})\n        row.update( {\"tokens\" : text_splits})\n\n        ret.append(row.copy())\n    return ret\n\ndef prepare_prediction(df, direc = test_directory):\n    ret = []\n    for x in tqdm( df['id'].unique(), total = len(df['id'].unique() )):\n        row = {\"tokens\": read_train_file(x, test_directory).split()}\n        ret.append(row.copy())\n        \n    return pd.DataFrame(ret)","0f7362a3":"if USE_LOCAL_DATASET == False:\n    train = add_ner_start_ends(train)\n    train = add_ner_label_ids(train)\n    ret = prepare_dataset(train)\n    \n\n    #ret[0]\nlabels_to_d = labels_to_discourse(train)","fe379dab":"if USE_LOCAL_DATASET == False:\n    df_train = pd.DataFrame(ret)\n\n    del(ret)","578a81e7":"if USE_LOCAL_DATASET == False:\n    X_train, X_test = train_test_split(df_train, random_state = 91, train_size = 0.7)\n\n    X_test, X_valid = train_test_split(X_test, random_state = 91, train_size = 0.5)\n\n    del(df_train)","621ee147":"if USE_LOCAL_DATASET == False:\n    train_set = Dataset.from_pandas(X_train)\n    test_set = Dataset.from_pandas(X_test)\n    valid_set = Dataset.from_pandas(X_valid)","96ded9c5":"if USE_LOCAL_DATASET == False:\n    raw_datasets = DatasetDict( {\n        'train' : train_set,\n        'test': test_set,\n        'validation': valid_set\n    })","902b5ee8":"paths = os.path.join(output_directory, DATASET_PATH)\n\nimport datasets\n\nif USE_LOCAL_DATASET == False:\n    print(\"Saving dataset to disk. \", paths)\n    \n    raw_datasets.save_to_disk( paths )\n    \nelse:\n    assert os.path.exists(paths), \"The dataset local path does not exist. Please retrain the notebook with appropriate switches.\"\n    raw_datasets = datasets.load_from_disk( paths )","9a4a92f1":"from transformers import LongformerTokenizerFast, BertTokenizer, AutoTokenizer\npaths = os.path.join(output_directory, TOKENIZER_PATH)\n\nif USE_LOCAL_DATASET == False:\n    tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, add_prefix_space=True)\n    tokenizer.save_pretrained(paths)\nelse:\n    assert os.path.exists(paths), \"Tokenizer local path is not found. Please recheck configuration of the notebook\"\n    tokenizer = AutoTokenizer.from_pretrained(paths)","3dd3180c":"def tokenize_predictions(examples):\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n    return tokenized_inputs\n\ndef tokenize_and_align_labels(examples, is_train = True):\n    \n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n\n    if is_train == False:\n        tokenized_inputs[\"labels\"] = examples[\"tokens\"]\n        return tokenized_inputs\n    \n    labels = []\n    for i, label in enumerate(examples[f\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:                            # Set the special tokens to -100.\n            if word_idx is None:\n                label_ids.append(-100)\n            elif word_idx != previous_word_idx:              # Only label the first token of a given word.\n                label_ids.append(label[word_idx])\n\n        labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs\n\n#tokenized_examples = examples.map(tokenize_and_align_labels, batched=True)\n","b9d813e9":"paths = os.path.join(output_directory, TOKENIZED_PATH)\n\nimport datasets\n\nif USE_LOCAL_DATASET == False:\n    tokenized_datasets = raw_datasets.map(tokenize_and_align_labels, batched=True)\n    print(\"Saving tokenized dataset to disk. \", paths)\n    \n    tokenized_datasets.save_to_disk( paths )\n    \nelse:\n    assert os.path.exists(paths), \"The dataset local path does not exist. Please retrain the notebook with appropriate switches.\"\n    tokenized_datasets = datasets.load_from_disk( paths )","3938ad32":"if USE_SMALL_DATASET:\n    print(\"CAUTION: Using a small subset of data as per the switches. \")\n    small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed = 91).select(range(SMALL_DATASET_SIZE))\n    small_eval_dataset = tokenized_datasets[\"validation\"].shuffle(seed = 91).select(range(SMALL_DATASET_SIZE))\n    small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed = 91).select(range(SMALL_DATASET_SIZE))\nelse:\n    small_train_dataset = tokenized_datasets[\"train\"]\n    small_eval_dataset = tokenized_datasets[\"validation\"]\n    small_test_dataset = tokenized_datasets[\"test\"]","44ceda52":"print(\"Total target unique labels are : \" , len(discourse_labels) * 2 + 1)","0260a186":"from transformers import DataCollatorForTokenClassification\ndata_collator = DataCollatorForTokenClassification(tokenizer)","c2bd6ab0":"from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer","4360a553":"paths = os.path.join(output_directory, MODEL_PATH)\n\nimport datasets\n\nif USE_LOCAL_DATASET == False:\n    model = AutoModelForTokenClassification.from_pretrained(TOKENIZER_NAME, \n                                                        num_labels=len(discourse_labels) * 2 + 1) #total labels are twice the discourse types and 1 for NONE token\n    print(\"Saving model to disk. \", paths)\n    \n    model.save_pretrained( paths )\nelif CHECKPOINT is not None:\n    if ISLOCAL:\n        paths = os.path.join( os.path.join(main_directory, TRAINOUT_PATH), CHECKPOINT)\n    else:\n        paths = os.path.join( os.path.join(os.path.split(output_directory)[0], TRAINOUT_PATH), CHECKPOINT) #stupid coding\n    model = AutoModelForTokenClassification.from_pretrained(paths)\n    \nelse:\n    assert os.path.exists(paths), \"The model local path does not exist. Please retrain the notebook with appropriate switches.\"\n    model = AutoModelForTokenClassification.from_pretrained(paths)","d3d34f9e":"print(\"I shall train on dataset of size: \", len(small_train_dataset))","2f5b03aa":"training_args = TrainingArguments(\n    output_dir = TRAINOUT_PATH,\n    logging_steps = min(500, len(small_train_dataset)),\n    save_strategy = \"epoch\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size = BATCH_SIZE,\n    per_device_eval_batch_size = BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    weight_decay=0.01,\n    save_total_limit = 3,\n    fp16 = True,\n    fp16_full_eval = True,\n    report_to = \"none\",\n)","7b99f8a4":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset = small_train_dataset, \n    eval_dataset = small_eval_dataset,\n    data_collator = data_collator,\n    tokenizer = tokenizer,\n)","1a0210b8":"if INFER_ONLY == False:\n    trainer.train()","fae32c07":"def get_predictions_on_eval(eval_set = small_eval_dataset):\n    predictions = trainer.predict(eval_set)\n    preds = np.argmax( predictions.predictions, axis = -1)\n    return preds\n\n\ndef get_predictions_on_submission_file(df_ss):\n    '''\n        df_ss should be the loaded sample_submission.csv file\n    '''\n    ss_pred = Dataset.from_pandas( prepare_prediction(df_ss) )\n    tokenized_preds = ss_pred.map(tokenize_predictions, batched = True)\n    predictions = trainer.predict( tokenized_preds )\n    return np.argmax( predictions.predictions, axis = -1)\n    \npreds = get_predictions_on_submission_file(ss)\n","310174b9":"from itertools import groupby\nfrom operator import itemgetter\n\ndef get_groups(data, min_length = 2):\n    # https:\/\/stackoverflow.com\/questions\/2154249\/identify-groups-of-continuous-numbers-in-a-list\n    ranges =[]\n\n    for k,g in groupby(enumerate(data),lambda x:x[0]-x[1]):\n        group = (map(itemgetter(1),g))\n        group = list(map(int,group))\n        if len(group) >= min_length:\n            ranges.append(group)\n    return ranges\n\n\n\n# First we change the Lead-I and Lead-B tokens to simply Lead and so on ...\ndef get_processed_submission_file(preds, min_length = 5):\n    # CAUTION. WE pass and modify the preds variable here. Wont be able to reuse afterwards\n    for i in range(2, 15, 2):\n        preds[preds == i] = i - 1\n    \n    ret = []\n    for i, j in tqdm(labels_to_d.items()):\n        if i % 2 == 0:\n            continue #there should be no even number token identifiers anymore. (apart from the 0)\n        print(\"Processing predictions of type : \", labels_to_d[i])\n        for ind, a in enumerate(preds):\n            tk = (a == i)\n            all_groups = get_groups(np.argwhere(tk) , min_length = min_length)\n            if len(all_groups) > 0: # we have some groups found for the current discourse type\n                for group in all_groups:\n                    pred_str = str(group).replace(\",\", \"\").replace(\"[\", \"\").replace(\"]\", \"\")\n                    pred_categ = i\n                    pred_categ_label = labels_to_d[i]\n                    pred_len = len(group)\n                    ret.append({\"predictionstring\" : pred_str,\n                               \"pred_categ\" : pred_categ,\n                               \"class\": pred_categ_label,\n                               \"pred_len\" : pred_len,\n                               \"pred_index\" : ind})\n    return ret","486931b6":"ret = get_processed_submission_file(preds)\nret = pd.DataFrame(ret)\n\nret['id'] = ret['pred_index'].map( ss['id'].to_dict() ) #map the FILE IDs using submission file","b192d762":"keep_cols = [\"id\", \"class\", \"predictionstring\"]\nret = ret[keep_cols]","9d608c20":"ret = ret.sort_values(by = ['id'])\nret.to_csv(\"submission.csv\", index = False)","4e7f4bb2":"# Start of work\n\n* Prepare the train dataframe and return the tokenized + ner_tags version\n* Expected time to run = 7 minutes","3524f740":"# Introduction\n\nThis notebook is intended to serve as a simple starter book for Transformer's Trainer API. It is intended to be as simple as possible and close to the original HuggingFace Transformers example @ https:\/\/huggingface.co\/docs\/transformers\/custom_datasets#tok_ner\n\n* Model used = Distillibert uncased\n    * This means that the outputs will be clipped to 512 as the model processes 512 tokens only.\n    * The 512 is the input sequence limit which was present in the BERT family of models\n    * Indeed, if you want to be in the top on leadership board, this model should be replaced OR a rolling window data processing should be done\n* Train for 3 Epochs. This model will overfit as you train more","a33af364":"# Switches & Constants\n\n* **ISLOCAL** can control if you want to run the same notebook in your local environment. (It merely switches the names of the directories between my local environment and Kaggle)\n    * Set to False to run in Kaggle\n* **TOKENIZER_NAME** should be the base name of the model you are interested in (HuggingFace Models https:\/\/huggingface.co\/models?pipeline_tag=text-classification )\n* **USE_SMALL_DATASET** can be used to verify small changes in code. It will only train the notebook for total size of SMALL_DATASET_SIZE constant (default = 50)\n* **USE_LOCAL_DATASET** = True will look for the tokenized dataset + tokenizer + model all in the local directory. This option **must** be used when submitting to Kaggle. But when using local resources, you can disable this option.\n* **BATCH_SIZE** depends on you machine memory. In Kaggle environment with Distillibert, batch size of 8 works fine and I suspect it can be increased to 16\/32 as well.\n* **EPOCHS** is your choice. But viewing the metrics, after 3\/4 Epochs, the model seems to start overfit severely.","532ef402":"# Tokenized Dataset Checkpoint\n\nAs always, we will either tokenize the dataset and align labels **OR** save the currently processed dataset to disk","748673c3":"# Helper Function 02\n\n## **add_ner_start_ends** \n* This function takes a dataframe as input. (train dataframe)\n* This function then returns the 1st position of prediction string and the last position of prediction string concatenated in the same dataframe\n* This is useful as a final output in pandas dataframe is easier to process \/ visualize\n\n\n## **add_ner_label_ids**\n* We will need labels for Beginning-Lead, Inside-Lead, Beginning-Claim, Inside-Claim etc. So, this function simply adds numbers to the pandas dataframe for each row on what is the numeric label for beginning and inside for this particular category\n\n## **labels_to_discourse**\n* Return the labels against category as a dictionary","843816a8":"# Submit (Finally!)","3d0d3dae":"# Training Arguments\n\n* Output directory to save the model\n* Logging steps to report training loss. (If using small datasets, the logging may never be done as default value is 500 steps. So we use the formula specified)\n* Batch sizes \/ learning rates are upto your liking","960ec62f":"# Model Checkpoint\n\n* We can load the model from tag using Internet OR\n* we can load the model using our last checkpoint after training OR\n* we can load the downloaded untrained model from local storage\n\nSwitches to control are USE_LOCAL_DATASET & CHECKPOINT where CHECKPOINT should be the name of checkpoint after training model. (like checkpoint-100 etc.)","76dd4c33":"# That's all folks.","613ed9e9":"## Split the dataset\n\n* Split data into 3 sets for local debugging \/ troubleshooting","300fa729":"# Verify,\nthat all is locked and loaded","ca48c318":"## **prepare_dataset**\n* This function will return a tokenized dataset after reading a processed pandas dataframe.\n* If input was \n    * Input : \"This is a text which is also a claim. But this other text is Evidence\"\n    * Output : { \"tokens\": [\"This\", \"is\", \"a\", \"text\", \"which\", \"is\", \"also\", \"a\", \"claim.\", \"But\", \"this\", \"other\", \"text\", \"is\", \"Evidence\"],\n                \"ner_tags\":[1     , 2   , 2  ,  2    ,       2,    2,       2, 2 , 2       ,   3  ,   4   ,   4    , 4     ,   4,     4      ] }\n    * where, we assume that ner_tag == 1 for Lead-Beginning, 2 for Lead-Inside, 3 for Evidence-Beginning, 4 for Evidence-Inside\n    \n## **prepare_prediction**\n* Simple function to load the submission text files and return their tokens (separated strings)","713728d5":"# Caution\n\n* If you set the **USE_SMALL_DATASET** switch to ON, then it will take a shuffled training sample of small size to train the model\n* Useful to check if after any changes the notebook runs fine end to end.\n","77805235":"## Convert to transformers dataset\n\n* https:\/\/huggingface.co\/docs\/datasets\/","48b62145":"# Train the Model\n\n* If the switch INFER_ONLY is true, then we wont train but only generate predictions","14e9762c":"# Tokenizer Checkpoint\n\nEither we will load the tokenizer using local storage **OR** we will save the tokenizer to disk after downloading from Internet.\n\nThis is based on setting of the **USE_LOCAL_DATASET** switch","dad0941f":"# Data Cleanup\n\nIn case some integer processing is needed, it is better to change discourse start and end to numeric types.","81e936e1":"# Process Predictions\n\nAfter predictions are done, we need to split the predictions by the Categories. \nFor example,\n\nIf we have 1 Lead and 2 Claims in the same file, then we need to split them accordingly.\n\n## **get_groups** \nfunction finds consecutive groups in a list https:\/\/stackoverflow.com\/questions\/2154249\/identify-groups-of-continuous-numbers-in-a-list\n\n\n## **get_processed_submission_file** \nfunction returns the processed entries to be submitted for the competition.\n* It will only include an entry if its minimum length (min consecutive length) is atleast min_length (default is 5 words)\n\n","8f61f786":"# Helper Function\n\n* Function to read a text file and return contents as a string","abda3a5e":"# Cleanup\n\nCleanup before submission. Assign file IDs and etc.","1b74a4d9":"### Read CSV Files\n\nRead the train and submission files\n\n* train will be used for the training loop\n* ss will be used for generating the predictions","9dbad9cc":"# Helper Functions 03\n\n## **tokenize_predictions**\n* This applies the loaded tokenizer to the Submission Text file dataset. (No NER Processing is needed)\n\n## **tokenize_and_align_labels**\n* This is taken from HuggingFace resource. This function takes into account the addition of special tokens to the text and then aligns the NER tags which we had assigned.\n* For example,\n    * We may have assigned NER tags = [ 0, 1, 2 , 2 , 2 , 2 , 0]\n    * After addition of special tokens like [CLS] etc., the position of these NER tags may need to be changed as well.\n    * This is taken care of within this tokenize function","2d64e315":"# Dataset Checkpoint\n\nEither we will load the dataset using local storage OR we will save the currently created dataset on disk.\n\nThis is based on the switch settings of **USE_LOCAL_DATASET**","21c1c4f0":"## Data collator\n\n* Data collator makes sure that the sequences have same length. It can utilize padding to achieve this effect\n* Check resource from huggingface highlighted in the beginning of notebook","93b9da30":"# Inference Helpers\n\n## **get_predictions_on_eval**\n* Assumes that the trainer has the target model loaded and outputs the predictions of NER tags.\n\n## **get_predictions_on_submission_file**\n* Same as above but for the submission file dataframe"}}