{"cell_type":{"bcb7ec9d":"code","134ea014":"code","d1303d4b":"code","353a2d4c":"code","c212a5e8":"code","8b5d2add":"code","49682e73":"code","1ef2dfd2":"code","2145d190":"code","a1462ea2":"code","accc620c":"code","739fb2ef":"code","f8b7e7d4":"code","dce67146":"code","e99a9778":"code","0c85a08f":"code","de22f450":"code","0be8873d":"code","230b3161":"code","48f02d35":"markdown","051fe502":"markdown","de51c378":"markdown","dafc86e6":"markdown","7fde12df":"markdown","990a9802":"markdown","f021bfeb":"markdown"},"source":{"bcb7ec9d":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom keras import layers\nimport keras\n \n%matplotlib inline\ntf.random.set_seed(42)","134ea014":"df = pd.read_csv('..\/input\/mobile-health\/mhealth_raw_data.csv')\ndf","d1303d4b":"from sklearn.utils import resample\n \ndf_majority = df[df.Activity==0]\ndf_minorities = df[df.Activity!=0]\n \ndf_majority_downsampled = resample(df_majority,n_samples=30000, random_state=42)\ndf = pd.concat([df_majority_downsampled, df_minorities])\ndf.Activity.value_counts()","353a2d4c":"#Dropping feature have data outside 98% confidence interval\ndf1 = df.copy()\n\nfor feature in df1.columns[:-2]:\n  lower_range = np.quantile(df[feature],0.01)\n  upper_range = np.quantile(df[feature],0.99)\n  print(feature,'range:',lower_range,'to',upper_range)\n\n  df1 = df1.drop(df1[(df1[feature]>upper_range) | (df1[feature]<lower_range)].index, axis=0)\n  print('shape',df1.shape)","c212a5e8":"label_map = {\n    0: 'Nothing',\n    1: 'Standing still',  \n    2: 'Sitting and relaxing', \n    3: 'Lying down',  \n    4: 'Walking',  \n    5: 'Climbing stairs',  \n    6: 'Waist bends forward',\n    7: 'Frontal elevation of arms', \n    8: 'Knees bending (crouching)', \n    9: 'Cycling', \n    10: 'Jogging', \n    11: 'Running', \n    12: 'Jump front & back' \n}","8b5d2add":"#spliting data into train and test set\ntrain = df1[(df1['subject'] != 'subject10') & (df1['subject'] != 'subject9')]\ntest = df1.drop(train.index, axis=0)\ntrain.shape,test.shape","49682e73":"X_train = train.drop(['Activity','subject'],axis=1)\ny_train = train['Activity']\nX_test = test.drop(['Activity','subject'],axis=1)\ny_test = test['Activity']\nX_train.shape,y_train.shape,X_test.shape,y_test.shape","1ef2dfd2":"from scipy import stats\n\n#function to create time series datset for seuence modeling\ndef create_dataset(X, y, time_steps, step=1):\n    Xs, ys = [], []\n    for i in range(0, len(X) - time_steps, step):\n        x = X.iloc[i:(i + time_steps)].values\n        labels = y.iloc[i: i + time_steps]\n        Xs.append(x)\n        ys.append(stats.mode(labels)[0][0])\n    return np.array(Xs), np.array(ys).reshape(-1, 1)","2145d190":"X_train,y_train = create_dataset(X_train, y_train, 100, step=50)\nX_train.shape, y_train.shape","a1462ea2":"X_test,y_test = create_dataset(X_test, y_test, 100, step=50)\nX_test.shape, y_test.shape","accc620c":"model = keras.Sequential()\nmodel.add(layers.Input(shape=[100,12]))\nmodel.add(layers.Conv1D(filters=32, kernel_size=3, padding=\"same\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.ReLU())\nmodel.add(layers.Conv1D(filters=64, kernel_size=3, padding=\"same\"))\nmodel.add(layers.BatchNormalization())\nmodel.add(layers.ReLU())\nmodel.add(layers.MaxPool1D(2))\nmodel.add(layers.LSTM(64))\nmodel.add(layers.Dense(units=128, activation='relu'))\nmodel.add(layers.Dense(13, activation='softmax'))\nmodel.summary()","739fb2ef":"tf.keras.utils.plot_model(model, show_shapes=True)","f8b7e7d4":"callbacks = [keras.callbacks.ModelCheckpoint(\"mhealth_best.h5\", save_best_only=True, monitor=\"val_loss\"),\n             keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=50, verbose=1)]\n \nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"sparse_categorical_accuracy\"],)\n \nmodel_history = model.fit(X_train,y_train, epochs= 10, validation_data=(X_test,y_test), callbacks=callbacks)","dce67146":"train_loss = model_history.history['loss']\nval_loss = model_history.history['val_loss']\ntrain_accuracy = model_history.history['sparse_categorical_accuracy']\nval_accuracy = model_history.history['val_sparse_categorical_accuracy']\n\nplt.figure(figsize=(12,6))\n\nplt.subplot(1,2,1)\nplt.plot(train_loss, 'r', label='Training loss')\nplt.plot(val_loss, 'b', label='Validation loss')\nplt.title('Training and Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss Value')\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.plot(train_accuracy, 'r', label='Training Accuracy')\nplt.plot(val_accuracy, 'b', label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()","e99a9778":"model = keras.models.load_model('.\/mhealth_best.h5')\n\ntrain_loss, train_acc = model.evaluate(X_train,y_train)\ntest_loss, test_acc = model.evaluate(X_test,y_test)\n\nprint(\"Train accuracy\", round(train_acc*100, 2),'%')\nprint(\"Train loss\", train_loss)\nprint(\"Test accuracy\", round(test_acc*100, 2),'%')\nprint(\"Test loss\", test_loss)","0c85a08f":"pred = model.predict(X_test)\npred = np.argmax(pred, axis = 1)\npred = pred.reshape(-1,1)","de22f450":"pred.shape,y_test.shape","0be8873d":"from sklearn.metrics import confusion_matrix, classification_report\n \nprint(classification_report(y_test,pred))\nprint('*'*50)\nprint(confusion_matrix(y_test,pred))","230b3161":"plt.figure(figsize=(12,8))\nconf_matrix = confusion_matrix(y_test,pred)\nsns.heatmap(conf_matrix, xticklabels= label_map.values(), yticklabels= label_map.values(), annot=True, fmt=\"d\")\nplt.show()","48f02d35":"# Model Evaluation","051fe502":"Follow this ****[notebook](https:\/\/www.kaggle.com\/gaurav2022\/eda-dataviz-resample\/)**** for EDA,data visualization and to know how I came to particular decision to prepare data.","de51c378":"If you have learn something new, Kindly upvote to help community :)\n\nYou can follow this ****[github repo](https:\/\/github.com\/G0rav\/Human_Activity_Recognition)**** for future advancments. ","dafc86e6":"# Modeling Data Preparation\n\nFollow this ****[notebook](https:\/\/www.kaggle.com\/gaurav2022\/eda-dataviz-resample\/)**** to see how I select time frame for model data preparation.","7fde12df":"# Model building","990a9802":"# Importing and preparing data","f021bfeb":"Model performs really well on almost all activities but confuses between sitting and lying down. This would be solved by doing feature engineering, that will be the future task. "}}