{"cell_type":{"4bdcc256":"code","e2c570d6":"code","ff859b0d":"code","1fa4311c":"code","468f9626":"code","882e12d7":"code","3866aa78":"code","16fb8a66":"code","2df4ff33":"code","c7ac5b47":"code","a7e285aa":"code","212695c7":"code","bf1eccc6":"code","6d621e22":"code","e20b9725":"markdown","b4d9490c":"markdown","d3277e52":"markdown","455921fe":"markdown","528d2b6e":"markdown","0387fb98":"markdown","4471f8da":"markdown","a8ea2b35":"markdown","30e0517a":"markdown"},"source":{"4bdcc256":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e2c570d6":"from keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.optimizers import RMSprop\nfrom keras.utils import np_utils","ff859b0d":"np.random.seed(100) #for reproducibility..,\nbatch_size = 128 # number of images that we use in each epoch\nnb_classes = 10 # one class for each digit(0 -- 9)\nnb_epoch = 20 # number of times we train the whole data","1fa4311c":"(X_train, Y_train),(X_test, Y_test) = mnist.load_data()","468f9626":"X_train = X_train.reshape(60000, 784) #60,000 digit images\nX_test = X_test.reshape(10000, 784)","882e12d7":"X_train = X_train.astype('float32')\nX_test = X_test.astype('float32')","3866aa78":"X_train = (X_train- np.mean(X_train))\/np.std(X_train)\nX_test = (X_test- np.mean(X_test))\/np.std(X_test)","16fb8a66":"print(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n","2df4ff33":"Y_train = np_utils.to_categorical(Y_train, nb_classes)\nY_test = np_utils.to_categorical(Y_test, nb_classes)\n","c7ac5b47":"model = Sequential()\nmodel.add(Dense(512, input_shape=(784, )))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(120))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\n\n","a7e285aa":"rms = RMSprop()","212695c7":"model.compile(loss='categorical_crossentropy', optimizer = rms, metrics = [\"accuracy\"])","bf1eccc6":"# now we train our model for real!!!","6d621e22":"model.fit(X_train, Y_train,\nbatch_size = batch_size, nb_epoch = nb_epoch,\nverbose=2,\nvalidation_data = (X_test, Y_test))","e20b9725":"# now we convert class vectors to binary class metrics ","b4d9490c":"# load data","d3277e52":"# we flatten data , because mlp doesen't use the 2d strusture of data","455921fe":"# now we define our model()","528d2b6e":"# now we compile our network and add metrics for knowing accuracy","0387fb98":"# now its time to use an optimizer function to optimize loss function","4471f8da":"we convert train and test data to float32","a8ea2b35":"# display the number of train and test samples present in the data","30e0517a":"# now we have to normalized the data (by Z- SCORE)\n\n"}}