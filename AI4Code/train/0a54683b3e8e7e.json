{"cell_type":{"e0a333e8":"code","e3700dc6":"code","808cf708":"code","bff11824":"code","2568a70a":"code","718d055e":"code","e7ae1aee":"code","9a6576ff":"code","5e58262f":"code","86872684":"code","5dbdcb83":"code","8334c156":"code","27ac0fb7":"code","eb13efe9":"code","d7753b27":"code","49d865ca":"code","0504e7ab":"code","10862d30":"code","397978cc":"code","97e98644":"code","eecad627":"code","427f99c4":"code","bb65d42e":"code","77c99925":"code","284e6434":"code","07cccd59":"code","72381d2b":"code","0e961eca":"code","b99b0215":"code","2e8bb028":"code","54435705":"code","27298a48":"code","08f22d11":"code","b12d1d9a":"code","724bc6fe":"code","92645604":"code","68ba8587":"code","f70d1364":"code","64ded5d1":"code","10055ef3":"code","ba63feae":"code","e9624311":"code","78a85fdd":"code","45e6ea0b":"code","87b99aec":"code","8e280ed6":"code","061254d1":"code","6e80fcdf":"code","1d7f6ba1":"code","e0628d34":"code","5a7ee32f":"code","be32ea24":"code","97c0ad76":"code","52a30ea3":"code","f158a2ef":"code","4bb8c44a":"code","fc412f32":"code","269827c5":"markdown","31ed6237":"markdown","2a55cda8":"markdown","3b1286c0":"markdown","1914776d":"markdown","87256de2":"markdown","b80f08b4":"markdown","dfdcc5ae":"markdown","50d0bec5":"markdown","d762859a":"markdown"},"source":{"e0a333e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e3700dc6":"### Importing Required Libraries","808cf708":"import pandas as pd\nimport numpy as np\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection\n\n%matplotlib inline","bff11824":"train = pd.read_csv('\/kaggle\/input\/bigmart-sales-data\/Train.csv')\ntrain.head()","2568a70a":"train.info()","718d055e":"train.describe()  ## Univariate statistical analysis","e7ae1aee":"# To check for null values\ntrain.isnull().sum()","9a6576ff":"#to check wether outlet identifier should be used for modelling or not\ntrain.Outlet_Identifier.nunique()\n# As there are very few unique values we should consider it for modelling","5e58262f":"#similarly for Item Identifier\ntrain.Item_Identifier.nunique()\n#It should be ignored","86872684":"train.shape","5dbdcb83":"#defining numerical columns and categorical columns\nnum_cols = ['Item_Weight','Item_Visibility','Item_MRP',]\ncat_cols = ['Item_Type','Item_Fat_Content','Outlet_Identifier','Outlet_Size','Outlet_Location_Type','Outlet_Type','Outlet_Establishment_Year']\ntarget = 'Item_Outlet_Sales'","8334c156":"# 1:\ntrain['Item_Weight'] = train['Item_Weight'].fillna(train['Item_Weight'].mean())","27ac0fb7":"# 2 : Using Apply method\n\"\"\"weights = {}\nfor key,df in train.groupby(['Item_Type','Item_Fat_Content']):\n    \n    weights[key] = df['Item_Weight'].mean()\n    \nprint(weights)\n\ntrain['Item_Weight'] = train.apply(lambda x:weights[(x['Item_Type'],x['Item_Fat_Content'])] \n                                   if np.isnan(x['Item_Weight']) else x['Item_Weight'] ,axis = 1)\"\"\"","eb13efe9":"train['Outlet_Size'] = train['Outlet_Size'].fillna(train['Outlet_Size'].mode().values[0])","d7753b27":"train.isnull().sum()\n# Now there are no null values","49d865ca":"#finding the categories in categorical columns\ndef cat_cols_info(df,col):\n    print(\"Unique categories in {}\".format(col))\n    print(df[col].unique())\n    print(\"Distribution of categories: \\n\")\n    print(df[col].value_counts())\n    print('\\n')","0504e7ab":"for col in cat_cols:\n    cat_cols_info(train,col)","10862d30":"# observation #\n# category Item_fat_content has values such as low fat, lf  or LOW FAT which means the same, hence replacing them #\n\"\"\"\ntrain.Item_Fat_Content.replace('Low Fat','LF',inplace = True)\ntrain.Item_Fat_Content.replace('low fat','LF',inplace = True)\ntrain.Item_Fat_Content.replace('reg','Regular',inplace = True)\n\"\"\"","397978cc":"# 1) Numeric vs Numeric\nsns.pairplot(train[num_cols+[target]])","97e98644":"## Categorical w.r.t target\nfor cat_col in cat_cols:\n    fig = plt.figure(figsize=(15,4))\n    ax = fig.add_subplot(1,1,1)\n    j = 0\n    for key,df in train.groupby([cat_col]):\n        sns.kdeplot(df[target], label = key)\n        ax.set_xlabel(target)\n        ax.set_ylabel(\"Frequency\")\n        ax.legend(loc=\"best\")\n        ax.set_title('Frequency Distribution of {}'.format(col), fontsize = 10)\n        j = j + 1","eecad627":"# Categorical univariate\nfig = plt.figure(figsize = (15,50))\n\nj = 1\nfor cat_col in cat_cols:\n    ax = fig.add_subplot(len(cat_cols),1,j)\n    sns.countplot(x = cat_col,\n                  data = train,\n                  ax = ax)\n    ax.set_xlabel(cat_col)\n    ax.set_ylabel(\"Frequency\")\n    ax.set_title('Frequency Distribution for individual classes in {}'.format(cat_col), fontsize = 10)\n    j = j + 1","427f99c4":"# categorical vs numerical\nfor cat_col in cat_cols:\n    fig = plt.figure(figsize=(15,50))\n    j = 1\n    for num_col in num_cols:\n        ax = fig.add_subplot(len(cat_cols),len(num_cols),j)\n        sns.boxplot(y = num_col,\n                   x = cat_col,\n                   data = train,\n                   ax = ax)\n        ax.set_xlabel(cat_col)\n        ax.set_ylabel(num_col)\n        ax.set_title('Distribution of {} with respect to {}'.format(num_col,cat_col), fontsize = 10)\n        j = j + 1","bb65d42e":"# Finding coefficient of correlation\ntrain_corr = train[num_cols].corr()\ntrain_corr.head()","77c99925":"# plotting coefficient of correlation\nsns.heatmap(train_corr, annot= True, cmap='coolwarm_r')\nplt.show()","284e6434":"def handle_outliers(df,var,target,tol):\n    var_data = df[var].sort_values().values\n    q25, q75 = np.percentile(var_data, 25), np.percentile(var_data, 75)\n    \n    print('Outliers handling for {}'.format(var))\n    print('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n    iqr = q75 - q25\n    print('IQR {}'.format(iqr))\n    \n    cut_off = iqr * tol\n    lower, upper = q25 - cut_off, q75 + cut_off\n    print('Cut Off: {}'.format(cut_off))\n    print('{} Lower: {}'.format(var,lower))\n    print('{} Upper: {}'.format(var,upper))\n    \n    outliers = [x for x in var_data if x < lower or x > upper]\n\n    print('Number of Outliers in feature {}: {}'.format(var,len(outliers)))\n\n    print('{} outliers:{}'.format(var,outliers))\n\n    print('----' * 25)\n    print('\\n')\n    print('\\n')\n        \n    return list(df[(df[var] > upper) | (df[var] < lower)].index)","07cccd59":"outliers = []\nfor num_col in num_cols:\n    outliers.extend(handle_outliers(train,num_col,target,1.5))\noutliers = list(set(outliers))\nprint(len(outliers))","72381d2b":"#dropping the outliers\ntrain = train.drop(outliers)","0e961eca":"train.shape","b99b0215":"train = train[num_cols + cat_cols + [target]]","2e8bb028":"train.head()","54435705":"train = pd.get_dummies(train,columns=cat_cols,drop_first=True)\ntrain.head()","27298a48":"train_data,test_data = train_test_split(train, test_size = .2, random_state = 101)","08f22d11":"X_train = train_data.iloc[:,:-1]\nX_test = test_data.iloc[:,:-1]\n\ny_train = train_data.iloc[:,-1]\ny_test = test_data.iloc[:,-1]","b12d1d9a":"X_train.shape, X_test.shape","724bc6fe":"y_train.shape, y_test.shape","92645604":"sc = StandardScaler()\nsc.fit(X_train[num_cols])\nX_train[num_cols] = sc.transform(X_train[num_cols])","68ba8587":"X_train[num_cols].head()","f70d1364":"X_test[num_cols] = sc.transform(X_test[num_cols])","64ded5d1":"X_train.shape, X_test.shape","10055ef3":"train_df = pd.concat([X_train,y_train],axis = 1)\ntrain_df.to_csv('preprocessed_train.csv',index = False)\n\ntest_df = pd.concat([X_test,y_test], axis = 1)\ntest_df.to_csv('preprocessed_test.csv',index = False)","ba63feae":"train = pd.read_csv('preprocessed_train.csv')\ntest = pd.read_csv('preprocessed_test.csv')","e9624311":"train.head()","78a85fdd":"X_train = train.drop(['Item_Outlet_Sales'], axis = 1).values\nX_test = test.drop(['Item_Outlet_Sales'], axis = 1).values","45e6ea0b":"y_train = train['Item_Outlet_Sales'].values\ny_test = test['Item_Outlet_Sales'].values","87b99aec":"\nmodel_dict = {\"Linear Regression\": linear_model.LinearRegression(),\n            \"SGDRegressor\" : linear_model.SGDRegressor() }","8e280ed6":"for key,regressor in model_dict.items():\n    regressor.fit(X_train,y_train)\n    y_pred = regressor.predict(X_test)\n    print('The evalution scores for: ',regressor.__class__.__name__, 'are:')\n    mse = metrics.mean_squared_error(y_test,y_pred)\n    rmse = mse ** 0.5\n    mae = metrics.mean_absolute_error(y_test,y_pred)\n    mdae = metrics.median_absolute_error(y_test,y_pred)\n    print('MSE :', mse)\n    print('RMSE: ', rmse)\n    print('MAE: ', mae)\n    print('MDAE: ', mdae)\n    print('\\n')","061254d1":"import os\nfrom keras.utils import plot_model\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input,Dense,Flatten,Dropout\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import Callback","6e80fcdf":"model = Sequential()","1d7f6ba1":"model.add(Dense(200, input_shape=(X_train.shape[1],), kernel_initializer='glorot_normal', activation='relu'))\nmodel.add(Dense(100, kernel_initializer='glorot_normal', activation='relu'))\nmodel.add(Dense(50, kernel_initializer='glorot_normal', activation='relu'))\nmodel.add(Dense(1))","e0628d34":"model.compile(optimizer='adam', loss='mean_absolute_error', metrics=['mse'])","5a7ee32f":"print(model.summary())","be32ea24":"X_train.shape","97c0ad76":"(200 * 43) + 200","52a30ea3":"history = model.fit(X_train,\n                    y_train,\n                    batch_size=200,\n                    epochs=100,\n                    verbose=1,\n#                    callbacks=None,\n                    validation_data=(X_test,y_test)).history","f158a2ef":"res_df = pd.DataFrame(history)\nres_df.head()","4bb8c44a":"# Plot training vs validation MAE\nplt.plot(res_df['loss'],label=\"Training\")\nplt.plot(res_df['val_loss'],label=\"Validation\")\nplt.legend(loc='best')\nplt.xlabel('Epochs')\nplt.ylabel('MAE')\nplt.title('Training vs Validation MAE')","fc412f32":"# Plot training vs validation MSE\nplt.plot(res_df['mse'],label=\"Training\")\nplt.plot(res_df['val_mse'],label=\"Validation\")\nplt.legend(loc='best')\nplt.xlabel('Epochs')\nplt.ylabel('SE')\nplt.title('Training vs Validation MSE')","269827c5":"wi * xi + bias","31ed6237":"### Scaling for standard normal distribution\n  $$ z = (x-mean)\/std $$","2a55cda8":"## Artificial Neural Networks - Keras","3b1286c0":"# Data Preprocessing","1914776d":"### Splitting the data","87256de2":"# Data Visulization","b80f08b4":"*** Removing null values from cat cols which is Outlet size, we can use mode ***","dfdcc5ae":"### Handling Outliers using IQR method","50d0bec5":"# Data Exploration","d762859a":"### Removing null Values\n*** Item_Weight which is a numerical column has null values which can be treated in various ways ***\n1) As there is not much difference b\/w mean and median we can replace null with mean values.\n2) using Apply method"}}