{"cell_type":{"2f786141":"code","afb0bb41":"code","04099cba":"code","1c8860e5":"code","a90a9842":"code","4ff18523":"code","1371e601":"code","c98d6777":"code","405d89cb":"code","8f1554c7":"code","7de52838":"code","68f3f818":"code","fed17fe5":"code","ef218859":"code","2cfae0b9":"code","ca8f1bbf":"code","88781ff7":"code","c428b06d":"code","b5dc4409":"code","ed5d228f":"code","2df4a8e3":"code","15fcf48e":"code","dc33a478":"code","7dcc0b30":"code","d36f39a4":"code","4c90929d":"code","2c0febd3":"code","c219ae78":"code","5abea85e":"code","0cc60408":"code","95b64ec2":"code","291843ed":"code","b32b4f24":"code","70f64006":"code","5065b5bf":"code","818a3e34":"code","14094129":"code","67413d2f":"code","10d5a8fa":"markdown","1a38f72e":"markdown","15a4f32f":"markdown","873381be":"markdown","b61df5e5":"markdown","dac45d5f":"markdown","19b84899":"markdown","9051b614":"markdown","4c499cc6":"markdown","7649951e":"markdown","60296266":"markdown","e3ddad83":"markdown","e1cfffcf":"markdown","fc401b1f":"markdown","f3db4275":"markdown","e4204468":"markdown","ede481fd":"markdown","cb18c29c":"markdown","b00a684e":"markdown","496abaf4":"markdown","996b287f":"markdown","bf4d052a":"markdown","b3d5b791":"markdown","061be7aa":"markdown","a807eadd":"markdown","a4e19642":"markdown","0d6e2a80":"markdown","72912c9e":"markdown","58c0c7bd":"markdown","f3acc535":"markdown","a533bc02":"markdown","0b9171e6":"markdown","e1720888":"markdown","c140bf6e":"markdown","ec33fb8b":"markdown","9ba058e2":"markdown","d49d351b":"markdown"},"source":{"2f786141":"# import all libraries and dependencies for dataframe\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom datetime import datetime, timedelta\n\n# import all libraries and dependencies for data visualization\npd.options.display.float_format='{:.4f}'.format\nplt.rcParams['figure.figsize'] = [8,8]\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', -1) \nsns.set(style='darkgrid')\nimport matplotlib.ticker as ticker\nimport matplotlib.ticker as plticker\n\n# import all libraries and dependencies for machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.base import TransformerMixin\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nimport statsmodels.api as sm\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","afb0bb41":"# Local file path. Please change the file path accordingly\n\n# Read training data\ntrain = pd.read_csv(\"..\/input\/hospital-charges-simple-lr\/insurance_training.csv\")\n# train.head()\n\n# Read test data\ntest = pd.read_csv(\"..\/input\/hospital-charges-simple-lr\/insurance_test.csv\")\ntest.head()","04099cba":"train.shape","1c8860e5":"train.info()","a90a9842":"train.describe()","4ff18523":"# lets drop Unnamed: 0 and bmi\ntrain = train.drop(['Unnamed: 0','bmi'],axis=1)","1371e601":"# Calculating the Missing Values % contribution in DF\n\ndf_null = train.isna().mean().round(4) * 100\n\ndf_null.sort_values(ascending=False).head()","c98d6777":"train.dtypes","405d89cb":"# Outlier Analysis of target variable with maximum amount of Inconsistency\n\noutliers = ['charges']\nplt.rcParams['figure.figsize'] = [8,8]\nsns.boxplot(data=train[outliers], orient=\"v\", palette=\"Set1\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Charges Range\", fontweight = 'bold')\nplt.xlabel(\"Continuous Variable\", fontweight = 'bold')","8f1554c7":"# checking for duplicates\n\ntrain.loc[train.duplicated()]","7de52838":"train.head()","68f3f818":"# Visualizing the smoker counts\n\nplt.rcParams['figure.figsize'] = [6,6]\nax=train['smoker'].value_counts().plot(kind='bar',stacked=True, colormap = 'Set1')\nax.title.set_text('Smoker or Non-Smoker')\nplt.xlabel(\"Smoker Status\",fontweight = 'bold')\nplt.ylabel(\"Count of Smoker\/Non-Smoker\",fontweight = 'bold')","fed17fe5":"# Visualizing the count of males\/females \n\nplt.rcParams['figure.figsize'] = [6,6]\nax=train['sex'].value_counts().plot(kind='bar',stacked=True, colormap = 'Set1')\nax.title.set_text('Sex')\nplt.xlabel(\"Sex\",fontweight = 'bold')\nplt.ylabel(\"Count of Males\/Females\",fontweight = 'bold')","ef218859":"# Visualizing the count of BMI groups available\n\nplt.rcParams['figure.figsize'] = [6,6]\nax=train['BMI_group'].value_counts().plot(kind='bar',stacked=True, colormap = 'Set1')\nax.title.set_text('BMI group')\nplt.xlabel(\"BMI group'\",fontweight = 'bold')\nplt.ylabel(\"Count of BMI group'\",fontweight = 'bold')","2cfae0b9":"plt.figure(figsize=(8,8))\n\nplt.title('Charges Distribution Plot')\nsns.distplot(train['charges'])","ca8f1bbf":"plt.figure(figsize=(20,8))\nwarnings.filterwarnings(\"ignore\")\nsns.pairplot(train, x_vars=['age', 'sex','children','smoker','region','BMI_group'], y_vars='charges',size=6, aspect=1, kind='scatter')\nplt.show()","88781ff7":"# Let's check the correlation coefficients to see which variables are highly correlated\n\nplt.figure(figsize = (8, 8))\ndf_corr = train.corr()\nax = sns.heatmap(df_corr, annot=True, cmap=\"RdYlGn\") \nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","c428b06d":"train.head()","b5dc4409":"col_rel = ['age','smoker','BMI_group']","ed5d228f":"# Scatter Plot of independent variables vs dependent variables\n\nfig,axes = plt.subplots(1,3,figsize=(18,6))\nfor seg,col in enumerate(col_rel):\n    x,y = seg\/\/3,seg%3\n    an=sns.scatterplot(x=col, y='charges' ,data=train, ax=axes[y])\n    plt.setp(an.get_xticklabels(), rotation=45)\n   \nplt.subplots_adjust(hspace=0.5)","2df4a8e3":"y_train = train.pop('charges')\nX_train = train","15fcf48e":"# Adding a constant variable and Build a first fitted model\nimport statsmodels.api as sm  \nX_train_c = sm.add_constant(X_train)\nlm = sm.OLS(y_train,X_train_c).fit()\n\n#Summary of linear model\nprint(lm.summary())","dc33a478":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nX = X_train\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","7dcc0b30":"# Dropping highly correlated variables and insignificant variables\n\nX_train_1 = X_train.drop('region', 1,)\n\n# Adding a constant variable and Build a second fitted model\n\nX_train_1c = sm.add_constant(X_train_1)\nlm1 = sm.OLS(y_train, X_train_1c).fit()\n\n#Summary of linear model\nprint(lm1.summary())","d36f39a4":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nX = X_train_1\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","4c90929d":"# Dropping highly correlated variables and insignificant variables\n\nX_train_2 = X_train_1.drop('sex', 1,)\n\n# Adding a constant variable and Build a third fitted model\n\nX_train_2c = sm.add_constant(X_train_2)\nlm2 = sm.OLS(y_train, X_train_2c).fit()\n\n#Summary of linear model\nprint(lm2.summary())","2c0febd3":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nX = X_train_2\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c219ae78":"# Dropping highly correlated variables and insignificant variables\n\nX_train_3 = X_train_2.drop('BMI_group', 1,)\n\n# Adding a constant variable and Build a fourth fitted model\n\nX_train_3c = sm.add_constant(X_train_3)\nlm3 = sm.OLS(y_train, X_train_3c).fit()\n\n#Summary of linear model\nprint(lm3.summary())","5abea85e":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nX = X_train_3\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","0cc60408":"# Dropping highly correlated variables and insignificant variables\n\nX_train_4 = X_train_3.drop('children', 1,)\n\n# Adding a constant variable and Build a fifth fitted model\n\nX_train_4c = sm.add_constant(X_train_4)\nlm4 = sm.OLS(y_train, X_train_4c).fit()\n\n#Summary of linear model\nprint(lm4.summary())","95b64ec2":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nX = X_train_4\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","291843ed":"# Dropping highly correlated variables and insignificant variables\n\nX_train_5 = X_train_4.drop('age', 1,)\n\n# Adding a constant variable and Build a sixth fitted model\n\nX_train_5c = sm.add_constant(X_train_5)\nlm5 = sm.OLS(y_train, X_train_5c).fit()\n\n#Summary of linear model\nprint(lm5.summary())","b32b4f24":"# Predicting the price of training set.\ny_train_charges = lm4.predict(X_train_4c)","70f64006":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_charges), bins = 20)\nfig.suptitle('Error Terms Analysis', fontsize = 20)                   \nplt.xlabel('Errors', fontsize = 18)","5065b5bf":"X_test = test","818a3e34":"# Adding constant\nX_test_1 = sm.add_constant(X_test)\n\nX_test_new = X_test_1[X_train_4c.columns]","14094129":"# Making predictions using the final model\n\ny_pred = lm4.predict(X_test_new)\ny_pred = pd.DataFrame(y_pred)\ny_pred = y_pred.rename(columns={0:'pred charges'})","67413d2f":"merge = pd.concat([X_test,y_pred],axis=1)","10d5a8fa":"### Checking VIF\n\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating `VIF` is:\n\n### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","1a38f72e":"### Dropping the variable and updating the model","15a4f32f":"#### Insights:\n- More than 800 people are non-smoker.\n- Around 200 people are smoker.","873381be":"- Lets build model with `lm4` which has basically 2 predictor variables.","b61df5e5":"- There is a significant drop after dropping age. So, lets not drop `age`","dac45d5f":"*Dropping `sex` beacuse its `p-value` is `0.875` and we want p-value less than 0.05 and hence rebuilding the model*","19b84899":"Let's see scatterplot for few correlated variables  vs `Charges`.","9051b614":"- Here, residuals are following normal distribution with a mean 0. Looks good","4c499cc6":"We generally want a VIF that is less than 5 and it look good.","7649951e":"*Dropping `region` beacuse its `p-value` is `0.879` and we want p-value less than 0.05 and hence rebuilding the model*","60296266":"We need to do some basic cleansing activity in order to feed our model the correct data.","e3ddad83":"#### If this Kernel helped you in any way, some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated","e1cfffcf":"## Step 2: Cleaning the Data","fc401b1f":"## Simple Linear Regression\n\nIn this notebook, we'll build a linear regression model to predict `Healthcare charges billed by their Insurance provider` using an appropriate predictor variable.","f3db4275":"- Looking at the p-values, it looks like some of the variables aren't really significant (in the presence of other variables)<br>\nand we need to drop it","e4204468":"#### Visualizing the distribution of charges","ede481fd":"#### Insights: \n- There are some price ranges above 50000 which can be termed as outliers but lets not remove it rather we will use standarization scaling.","cb18c29c":"### If this Kernel helped you in any way, some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated","b00a684e":"* Lets drop `age` and see if there is any significant drop in adj R squared.*","496abaf4":"*Dropping `BMI_group` beacuse its `p-value` is `0.357` and we want p-value less than 0.05 and hence rebuilding the model*","996b287f":"#### Dividing test set into X_test and y_test","bf4d052a":"#### Equation of Line to predict the Hospital charges","b3d5b791":"#### Insights:\n- `Smoker` seems to be high correlated with `charges`\n- `Age` and `bmi` seems to be moderately correlated with `charges`","061be7aa":"- The plots seems to be right skewed, the patients were charged majorly less than 15000 .\n","a807eadd":"- There is no significant drop in adj R squared.So, we will proceed with dropping `children`","a4e19642":"#### This kernel is based on the assignment by IIITB collaborated with Upgrad.","0d6e2a80":"#### Understanding the dataframe","72912c9e":"#### Model Conclusions:\n- R-squared and Adjusted R-squared - 0.713 and 0.712 - 70% variance explained.\n- F-stats and Prob(F-stats) (overall model fit) - 1323 and 1.12e-289(approx. 0.0) - Model fit is significant and explained 70%<br> variance is just not by chance.\n- p-values - p-values for all the coefficients seem to be less than the significance level of 0.05. - meaning that all the <br>predictors are statistically significant.","58c0c7bd":"## Step 4: Building a linear model using Bottom down approach","f3acc535":"## Step 1: Reading and Understanding the Data","a533bc02":"## Step 3: Visualising the Data\n\n- Here we will identify if some predictors directly have a strong association with the outcome variable `Charges`","0b9171e6":"### Dividing into X and Y sets for the model building","e1720888":"## Step 6: Making Predictions Using the Final Model\n\nNow that we have fitted the model and checked the normality of error terms, it's time to go ahead and make predictions using the `lm4` model.","c140bf6e":"## Step 5: Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of it.","ec33fb8b":"* Lets drop `children` and see if there is any significant drop in adj R squared. ","9ba058e2":"### $ charges = -2387.3678 +  276  \\times  age  + 2.369e+04  \\times  smoker $","d49d351b":"#### Visualising Numeric Variables\n\nPairplot of all the numeric variables"}}