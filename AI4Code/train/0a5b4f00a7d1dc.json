{"cell_type":{"9a353d26":"code","890f73e3":"code","f7f2c7e0":"code","d4f2bb14":"code","b17a136b":"code","de4c10cb":"code","ae9633fb":"code","66a381e8":"code","95ce8eb9":"code","818b6b6c":"code","27d7cec6":"code","cd0ecfa0":"code","9bf4bd64":"code","dfa32234":"code","b1c6246c":"code","9c96a98b":"code","2414c7cd":"code","79300771":"code","e956a0b5":"code","48d7a162":"code","8bef2bd8":"code","9b791851":"code","ee108ee9":"code","36fc6379":"code","2428b923":"code","beaf0238":"code","2fbb4140":"code","0582af35":"code","a2c23b84":"code","72f0e1b6":"code","4fd22998":"code","ad83e571":"code","31ee68b9":"markdown","ff3edc46":"markdown","0998a46e":"markdown","9368aef0":"markdown","bb4e10da":"markdown","dc82b930":"markdown","1d3e1d13":"markdown","b6546462":"markdown"},"source":{"9a353d26":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport IPython.display as ipd\nimport math\nfrom pathlib import Path\nimport urllib\nimport scipy, matplotlib.pyplot as plt, sklearn, urllib, IPython.display as ipd\nimport librosa, librosa.display\nfrom sklearn.metrics import accuracy_score\n\nimport os","890f73e3":"csv = pd.read_csv('\/kaggle\/input\/urbansound8k\/UrbanSound8K.csv')\ncsv","f7f2c7e0":"#retorna o caminho completo de cada arquivo do CSV. \ndef path_class(filename):\n    #cria um filtro com a linha onde o arquivo est\u00e1. \n    excerpt = csv[csv['slice_file_name'] == filename]\n    \n    #cria o path completo\n    path_name = os.path.join('\/kaggle\/input\/urbansound8k\/', 'fold'+str(excerpt.fold.values[0]), filename)\n    return path_name, excerpt['class'].values[0]","d4f2bb14":"#retorna o audio e informa\u00e7\u00f5es do mesmo. \ndef to_dataset (df, fold=[]):\n    # df = dataframe a ser percorrido.\n    # fold = qual\/quais folds ler. \n    audio = []\n    audio_signals = []\n    label= []\n    labels=[]\n    paths=[]   \n    sampling_rate=[]\n    librosa_sampling_rate = []\n    \n    # quando fold nulo, significa para ler tudo.         \n    if fold != []:\n        #filtra somentes os folds que foram enviados.  \n        filter_fold = df.fold.isin(fold)\n        df = df[filter_fold]\n\n    ###Descomentar para compilar mais rapido.     \n    #df = df.head(100)   \n\n    #para cada fold, pega todos os arquivos de dentro.            \n    for i in (df.fold.unique()):\n        #filtra o cada fold em cada itera\u00e7\u00e3o\n        filter_slice =  df['fold']==i\n        dt_fold = df[filter_slice]\n        \n        #Itera\u00e7\u00e3o para ler os arquivos da pasta fold da vez.\n        for p in dt_fold['slice_file_name']:\n            # Librosa j\u00e1 converte os dois canais para um canal e normaliza os dados entre 1 e -1. \n            audio,librosa_sampling_rate  = librosa.load('\/kaggle\/input\/urbansound8k\/fold'+str(i)+'\/' + p)\n            audio_signals.append(audio)\n            sampling_rate.append(librosa_sampling_rate)\n            \n            #busca as classes e paths.\n            path, label = path_class(p)\n            paths.append(path)\n            labels.append(label)\n\n    \n    print('Reading...')    \n    \n    #audio = cont\u00e9m os arquivos de audio\n    #paths = cont\u00e9m o caminho completo do arquivo.\n    #labels = contem as classifica\u00e7\u00f5es, \n    #librosa_sampling_rate = cont\u00e9m o sampling rate, que pode ser visto adiante. \n    return audio_signals,paths,labels,sampling_rate","b17a136b":"#Cria os dados de treino.\n#Descomentar para rodar mais r\u00e1pidp\n#Train, Train_Path, Train_Label,Train_S_Rate = to_dataset(csv,[1,2])\nTrain, Train_Path, Train_Label,Train_S_Rate = to_dataset(csv,[1,2,3,4,5,6,7,8,9])\n\nprint(\"Train set size: \" + str(len(Train)))\n\n#Cria os dados de teste\ntest, test_path, test_label,test_S_Rate  = to_dataset(csv,[10])\nprint(\"Test set size: \" + str(len(test)))","de4c10cb":"# Para anlisar cada classe, primeiro \u00e9 necess\u00e1rio extrair cada uma e criar um dataframe. \ndt_class = pd.DataFrame()\ndt_fold = (csv[csv['fold']==1])\nfor i in (dt_fold.classID.unique()):\n    dt_class = dt_class.append(dt_fold[dt_fold['classID']==i].head(1))    \ndt_class","ae9633fb":"#Envia o dataframe com as 10 classes para recuperar os audios e demais informa\u00e7\u00f5es. \nsample, sample_path, sample_label, sample_S_Rate = to_dataset(dt_class)","66a381e8":"sample_label","95ce8eb9":"#Leitura de um wav de exemplo\nimport struct\nipd.Audio(sample_path[0])\n# encontrei arquivos de audio que nao tem dois canais. Embora a biblioteca deva resolver, \n# seria interessante fazer esse tratamento com wav.\n# nesses mesmo samples, alguns arquivos nao podem ser ouvidos pelo ipd.audio. \nprint('simple rate:')\nprint(sample_S_Rate)\n\n# Librosa converte o sampling rate para 22050 no processo. ","818b6b6c":"#Plotando com Librosa (WAVEPLOT)\nplt.figure(figsize=(15, 6))\nfor i, x in enumerate(sample):\n    plt.subplot(4, 3, i+1)\n    plt.title(label = sample_label[i])\n    librosa.display.waveplot(x[:10000])\n    #plt.ylim(-1, 1)\n\n# \u00c9 possivel notar algumas diferen\u00e7as na onda. \n# Se ","27d7cec6":"#Plotando com Librosa (Fourier Transform)\n#converte tempo em frequencia. \n\nplt.figure(figsize=(10, 5))\nfor i, x in enumerate(sample):\n    plt.subplot(4, 3, i+1)\n    D = librosa.amplitude_to_db(np.abs(librosa.stft(sample[i])), ref=np.max)\n    librosa.display.specshow(D, y_axis='linear')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(sample_label[i])\n    \n# Fica mais claro as diferen\u00e7as. ","cd0ecfa0":"#exemplo de plot de cada classe. Usando Mel-Frequency Cepstral Coefficients (MFCC).\n#Define a fun\u00e7\u00e3o vista em aula para captura de feature. Realiza a media. \ndef extract_features(signal):\n    return  librosa.feature.mfcc(y=signal, n_mfcc = 40)\n\n# Cria um array com o MFCC de cada classe. \ndt_features = ([extract_features(x) for x in sample])\nprint(len(dt_features) )\n\n# Cria um plot para cada classe.  \nplt.figure(figsize=(8,8))\nfor i, x in enumerate(sample):\n    plt.subplot(4, 3, i+1)\n    librosa.display.specshow(dt_features[i], sr=sample_S_Rate[i], x_axis='time')\n    plt.colorbar(format='%+2.0f dB');\n    plt.title(sample_label[i])","9bf4bd64":"#exemplo de plot de cada classe. Usando Mel Spectrogram\ndef extract_features(signal):\n    S = librosa.feature.melspectrogram(signal, n_fft=2048,hop_length=512, n_mels=128)\n    return librosa.power_to_db(S, ref=np.max)\n\n# Cria um array com o MFCC de cada classe. \ndt_features = ([extract_features(x) for x in sample])\n#print(len(dt_features) )\n\n# Cria um plot para cada classe.  \nplt.figure(figsize=(8,8))\nfor i, x in enumerate(sample):\n    plt.subplot(4, 3, i+1)\n    librosa.display.specshow(dt_features[i], hop_length=512, \n                         x_axis='time', y_axis='mel');\n    plt.colorbar(format='%+2.0f dB');\n    plt.title(sample_label[i])","dfa32234":"#### !!!!! vou comentar esse trecho, pois na fun\u00e7\u00e3o a seguir usarei o vetor sem fazer a media. \n\n#Extraindo as features para efetivamente Treinar (redefinindo a fun\u00e7\u00e3o para treino)\n#def extract_features(signal):                \n#    return  (\n#        #librosa.feature.zero_crossing_rate(signal).mean(),\n#        librosa.feature.mfcc(signal)\n#    )\n#converte para array para poder criar um dataframe mais a frente. \n#Train_label_np = np.array(Train_Label)\n\n#le a primeira feature\n#data = ([extract_features(x) for x in Train])\n#data\n\n\n#alguns autores nao fazem a media, deixam a coluna feature com o vetor inteiro... qual a diferen\u00e7a?\n\n\n","b1c6246c":"#Train2 = Train+test#\n#Train_Label2 = Train_Label + test_label","9c96a98b":"#remover ao implementar o cross validation\nTrain = Train+test\nTrain_Label = Train_Label+test_label","2414c7cd":"def extract_features(signal):\n    mfccs = librosa.feature.mfcc(y=signal,  n_mfcc=40)\n    mfccs_processed = np.mean(mfccs.T,axis=0)\n     \n    return mfccs_processed     \n\nfeatures = []\n# Iterate through each sound file and extract the features \nfor x in Train:\n    features.append(extract_features(x))    \n","79300771":"#normalizar features\n#scaler = sklearn.preprocessing.MinMaxScaler(feature_range=(-1, 1))\n#training_features = scaler.fit_transform(data)\n#print(training_features.min(axis=0))\n#print(training_features.max(axis=0))\n#training_features\n\n#Embora eu tenha feira o scalar, nao vou usar para nada. Vou treinar com a feature no valor integral. ","e956a0b5":"#converte em um pandas df\ndf = pd.DataFrame({'feature':features, 'label':Train_Label})\ndf","48d7a162":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom keras.optimizers import Adam\nfrom keras.utils import np_utils\nfrom sklearn import metrics \nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical","8bef2bd8":"# Como os datasets est\u00e3o como list, vou converter pra array para poder fazer o train_test_split. \nX = np.array(df.feature.tolist())\ny = np.array(df.label.tolist())\n\n# Como eu trouxe os labels da colunas de texto, vou fazer um encoder para mudar para numerico. \nle = LabelEncoder()\nyy = to_categorical(le.fit_transform(y)) \n\n\n","9b791851":"import pickle\nd = dict(zip(le.classes_, le.transform(le.classes_)))\n\nfilename = 'dict'\npickle.dump(d, open(filename, 'wb'))","ee108ee9":"# separa\u00e7\u00e3o do dataset\nfrom sklearn.model_selection import train_test_split \n\nx_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)\n","36fc6379":"yy.shape[1]","2428b923":"# \nnum_labels = yy.shape[1]\n\n# Usar um sSequential do Keras simples. Os parametros s\u00e3o os mais comuns. 3 camadas. \n\n# O input ser\u00e1 40, devido ao n_mfcc=40 que usei antes. \n# Refer\u00eancia https:\/\/keras.io\/getting-started\/sequential-model-guide\/\n\ndef build_model(input_shape=(40,)):\n    model = Sequential()\n    # Uma primeira camada\n    model.add(Dense(256,  activation='relu'))\n    \n    #Dropout para reduziro o overfitting\n    model.add(Dropout(0.5))\n    \n    #Camada intermedi\u00e1ria para completar o modelo\n    # Relu por ser a mais usada e com boa performance. \n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(0.5))    \n    \n    # como deve ter uma saida para cada classe, eu fa\u00e7o de acordo com o num_labels\n    model.add(Dense(num_labels))\n    \n    #Softmax por ser mais de duas classes\n    #https:\/\/missinglink.ai\/guides\/neural-network-concepts\/7-types-neural-network-activation-functions-right\/    \n    model.add(Activation('softmax'))\n    # Compila o modelo\n    # Da lista de metricas, a unica que me parece fazer sentido \u00e9 a acur\u00e1cia \n    # Fonte: https:\/\/keras.io\/metrics\/\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n    return model\n\n#Cria o modelo\nmodel = build_model()","beaf0238":"#Compila o modelo. \n# Descomentar se precisar recompilar\n#model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n\n# Pr\u00e9 Avalia o modelo.  \nscore = model.evaluate(x_test, y_test, verbose=1)\naccuracy = 100*score[1]\nprint(\"Pre-training accuracy: %.4f%%\" % accuracy) \n\n# Mesmo baixa, o valor do treino final fica melhor. ","2fbb4140":"model.summary()","0582af35":"# Usando o early stopping para evitar perder tempo. \n# Usando checkpoint para nao perder o melhor resultado. \nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint \nmc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=20)\n\n\nfrom datetime import datetime \nnum_epochs = 300\nnum_batch_size = 32\nmodel.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_split=0.33, verbose=1,callbacks=[es, mc])","a2c23b84":"# Usando a metrica de acur\u00e1cia que \u00e9 a mais comum. \nscore = model.evaluate(x_train, y_train, verbose=0)\nprint(\"Training Accuracy: {0:.2%}\".format(score[1]))\nscore = model.evaluate(x_test, y_test, verbose=0)\nprint(\"Testing Accuracy: {0:.2%}\".format(score[1]))","72f0e1b6":"score","4fd22998":"#Export do modelo para usar no proximo algoritmo. (Al\u00e9m do best_model, claro) \nimport pickle\n# save the model to disk\nfilename = 'keras_audio_sequential.sav'\npickle.dump(model, open(filename, 'wb'))\n\n","ad83e571":"x_train.shape","31ee68b9":"### Roteiro do Trabalho <br>\n1. Import das bibliotecas <br>\n1. Leitura do Dataset <br>\n1.1 Treino e Teste\n1. Captura das Features <br>\n1.1 Amostra das Classes <br>\n1.2 Plot das features\n1. Cria\u00e7\u00e3o dos datasets de treino e teste <br>\n1. Cria\u00e7\u00e3o do modelo <br> \n1. Teste de Predi\u00e7\u00e3o <br>","ff3edc46":"## Import das Bibliotecas","0998a46e":"### Extra\u00e7\u00e3o das Features","9368aef0":"> Como n\u00e3o \u00e9 necess\u00e1rio implementar o cross validation pelos paths dos folds, vou usar o train tests split. Por isso, o trecho abaixo coloca o fold separado pra teste pra dentro do de Treino. ","bb4e10da":"## Leitura do dataset","dc82b930":"## An\u00e1lise\n\n> ### Preparacao do dataset de exemplo","1d3e1d13":"## Criando o modelo\n\n### Vou usar a biblioteca mais simples que encontrei. Keras","b6546462":"### Conclus\u00e3o\n\nAp\u00f3s ler v\u00e1rias fontes, compilando as ideias de cada uma e entendendo quase todos o funcionamento, foi poss\u00edvel criar um modelo que consegue identificar com quase 87% de acerto, os sons que ser\u00e3o testados na parte 2. \n\nEntendendo como a biblioteca Librosa funciona e como facilita a maioria das opera\u00e7\u00f5es, pude ver o comportamento do audio e graficamente como s\u00e3o diferentes. \n\nAlguns desafios que surgiram foi plotar de forma que fique mais claro o que elas representam. No momento consegui plotar de forma ainda um pouco atrapalhada. Mas \u00e9 vis\u00edvel como os arquivos de audio s\u00e3o diferentes nas imagens. \n\nGerar o modelo, ap\u00f3s testar combina\u00e7\u00f5es de quantidade de unidades, modos de ativa\u00e7\u00e3o e diferentes m\u00e9tricas, pareceu a melhor estrat\u00e9gia uma rede simples. Conforme visto em aula em diversos artigos. \n\nConsegui gerar um modelo que ainda n\u00e3o est\u00e1 perfeito e pode melhorar, principalmente na configura\u00e7\u00e3o das camadas, mas que vai conseguir acertar mais da metade da analise na parte 2 do trabalho. O Early Stopping parou na epoch 106. \n\nForam usados e gerados os seguintes arquivos:\n\nInputs:\n - urbansound8k\nOutputs:\n - best_model.h5 (modelo do checkpoint)\n - keras_audio_sequential.sav (modelo completo)\n - dict (dicion\u00e1rio das classes)\n "}}