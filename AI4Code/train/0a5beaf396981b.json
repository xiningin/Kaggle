{"cell_type":{"a4c8cc3c":"code","64e3e447":"code","e178af9a":"code","38073696":"code","07bfeb2c":"code","7abdef12":"code","6e06e944":"markdown","63f8dd54":"markdown","6d770fbc":"markdown","345e0341":"markdown","c6cec01b":"markdown","847cb769":"markdown","50f47bbf":"markdown"},"source":{"a4c8cc3c":"import numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing \nfrom sklearn import model_selection\nfrom sklearn.preprocessing import OrdinalEncoder \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.metrics import mean_squared_error \nfrom xgboost import XGBRegressor \nimport optuna","64e3e447":"# Load the training data\ndf = pd.read_csv(\"..\/input\/zisun-257\/train_folds_zisun_rs_257.csv\")\ndf_test=pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission=pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")","e178af9a":"useful_features=[c for c in df.columns if c not in (\"id\",\"target\",\"kfold\")]\nobject_cols=[col for col in useful_features if 'cat' in col]\nnumerical_cols =[col for col in useful_features if 'cont' in col]\ndf_test=df_test[useful_features]\n","38073696":"final_preds=[]\nfor fold in range(6):\n    xtrain=df[df.kfold !=fold].reset_index(drop=True)\n    xvalid=df[df.kfold ==fold].reset_index(drop=True)\n    xtest=df_test.copy()\n    ytrain=xtrain.target\n    yvalid=xvalid.target\n    xtrain=xtrain[useful_features]\n    xvalid=xvalid[useful_features]\n    \n    ordinal_encoder=OrdinalEncoder()\n\n    xtrain[object_cols]=ordinal_encoder.fit_transform(xtrain[object_cols])\n    xvalid[object_cols]=ordinal_encoder.transform(xvalid[object_cols])\n    xtest[object_cols]=ordinal_encoder.transform(xtest[object_cols])\n    mn=preprocessing.MinMaxScaler()\n    xtrain[numerical_cols]=mn.fit_transform(xtrain[numerical_cols])\n    xvalid[numerical_cols]=mn.transform(xvalid[numerical_cols])\n    xtest[numerical_cols]=mn.transform(xtest[numerical_cols])\n    model=XGBRegressor(n_estimators=9000,\n                       learning_rate =.08385934473366072,\n                       \n                       reg_lambda=37.8076574314643,\n                       \n                       reg_alpha =29.659533081597356,\n                       \n                       subsample=.5703331709353955,\n                       \n                       colsample_bytree =.10694310196501851,\n                      \n                       max_depth =2,\n                       \n                       booster='gbtree',\n                       \n                       random_state=40,\n                      )\n    model.fit(xtrain,ytrain)\n    preds_valid=model.predict(xvalid)\n    test_preds=model.predict(xtest)\n    final_preds.append(test_preds)\n    print(fold,mean_squared_error(yvalid,preds_valid,squared=False))","07bfeb2c":"preds=np.mean(np.column_stack(final_preds),axis=1)","7abdef12":"sample_submission.target=preds\nsample_submission.to_csv(\"submission.csv\",index=False)","6e06e944":"# Step 6: Keep Learning!\n\nIf you're not sure what to do next, you can begin by trying out more model types!\n1. If you took the **[Intermediate Machine Learning](https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning)** course, then you learned about **[XGBoost](https:\/\/www.kaggle.com\/alexisbcook\/xgboost)**.  Try training a model with XGBoost, to improve over the performance you got here.\n\n2. Take the time to learn about **Light GBM (LGBM)**, which is similar to XGBoost, since they both use gradient boosting to iteratively add decision trees to an ensemble.  In case you're not sure how to get started, **[here's a notebook](https:\/\/www.kaggle.com\/svyatoslavsokolov\/tps-feb-2021-lgbm-simple-version)** that trains a model on a similar dataset.","63f8dd54":"# Step 2: Load the data\n\nNext, we'll load the training and test data.  \n\nWe set `index_col=0` in the code cell below to use the `id` column to index the DataFrame.  (*If you're not sure how this works, try temporarily removing `index_col=0` and see how it changes the result.*)","6d770fbc":"# Step 3: Prepare the data\n\nNext, we'll need to handle the categorical columns (`cat0`, `cat1`, ... `cat9`).  \n\nIn the **[Categorical Variables lesson](https:\/\/www.kaggle.com\/alexisbcook\/categorical-variables)** in the Intermediate Machine Learning course, you learned several different ways to encode categorical variables in a dataset.  In this notebook, we'll use ordinal encoding and save our encoded features as new variables `X` and `X_test`.","345e0341":"Once you have run the code cell above, follow the instructions below to submit to the competition:\n1. Begin by clicking on the **Save Version** button in the top right corner of the window.  This will generate a pop-up window.  \n2. Ensure that the **Save and Run All** option is selected, and then click on the **Save** button.\n3. This generates a window in the bottom left corner of the notebook.  After it has finished running, click on the number to the right of the **Save Version** button.  This pulls up a list of versions on the right of the screen.  Click on the ellipsis **(...)** to the right of the most recent version, and select **Open in Viewer**.  This brings you into view mode of the same page. You will need to scroll down to get back to these instructions.\n4. Click on the **Output** tab on the right of the screen.  Then, click on the file you would like to submit, and click on the **Submit** button to submit your results to the leaderboard.\n\nYou have now successfully submitted to the competition!\n\nIf you want to keep working to improve your performance, select the **Edit** button in the top right of the screen. Then you can change your code and repeat the process. There's a lot of room to improve, and you will climb up the leaderboard as you work.","c6cec01b":"# Step 4: Train a model\n\nNow that the data is prepared, the next step is to train a model.  \n\nIf you took the **[Intro to Machine Learning](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning)** courses, then you learned about **[Random Forests](https:\/\/www.kaggle.com\/dansbecker\/random-forests)**.  In the code cell below, we fit a random forest model to the data.","847cb769":"In the code cell above, we set `squared=False` to get the root mean squared error (RMSE) on the validation data.\n\n# Step 5: Submit to the competition\n\nWe'll begin by using the trained model to generate predictions, which we'll save to a CSV file.","50f47bbf":"The next code cell separates the target (which we assign to `y`) from the training features (which we assign to `features`)."}}