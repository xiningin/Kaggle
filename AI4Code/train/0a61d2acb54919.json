{"cell_type":{"8c2e2080":"code","ad449b59":"code","b50939b2":"code","26021db5":"code","13cac87d":"code","98d956ae":"code","bda9d8a1":"code","459489da":"code","4dbfece7":"code","e6a9d63a":"code","2193e469":"code","3b96bb66":"code","883e23ac":"markdown","874c8525":"markdown","afa74cbd":"markdown","ccb0c017":"markdown","7ab2dd9f":"markdown","d8522a0c":"markdown"},"source":{"8c2e2080":"import numpy as np\nimport pandas as pd\n\nfrom numpy import asarray\nfrom pandas import read_csv\nfrom pandas import DataFrame\nfrom pandas import concat\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Flatten, Conv1D\nfrom tensorflow.keras.layers import MaxPooling1D, Dense\n\nimport warnings\nwarnings.filterwarnings('ignore')","ad449b59":"path = '..\/input\/daily-total-female-births-in-california-1959\/daily-total-female-births-CA.csv'\ndf = pd.read_csv(path, header=0, index_col=0)\ndf.head()","b50939b2":"def series_to_supervised(df, n_in=1, n_out=1, dropnan=True):\n    n_vars = df.shape[1]\n    cols = list()\n    \n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        \n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n\n    agg = concat(cols, axis=1)\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg.values","26021db5":"data = series_to_supervised(df, n_in=6)\n\n# print 2 of the datas\ndata[0], data[1]","13cac87d":"def random_forest(train, test):\n    train = asarray(train)\n    X_train, y_train = train[:, :-1], train[:, -1]\n\n    model = RandomForestRegressor(n_estimators=3000)\n    model.fit(X_train, y_train)\n\n    yhat = model.predict([test])\n    return yhat[0]","98d956ae":"def xgboost(train, test):\n    train = asarray(train)\n    X_train, y_train = train[:, :-1], train[:, -1]\n    \n    model = XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n    model.fit(X_train, y_train)\n    \n    y_hat = model.predict(asarray([test]))\n    return y_hat[0]","bda9d8a1":"def cnn(train, test):\n    n_in, n_out = 6, 1\n    train = asarray(train)\n    \n    X_train, y_train = train[:, :-1], train[:, -1]\n    \n    n_features = X_train.shape[1]\n    X_train = X_train.reshape(len(X_train), n_in, 1)\n    \n    model = Sequential()\n    model.add(Conv1D(filters=64, kernel_size=2, activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(Dense(50, activation='relu'))\n    model.add(Dense(n_out))\n    model.compile(optimizer='adam', loss='mse')\n    model.fit(X_train, y_train, epochs=1000, verbose=0)\n    \n    y_hat = model.predict(test.reshape(1, len(test), 1))\n    return y_hat[0][0]","459489da":"def get_models():\n    models = []\n    models.append(('random_forest', random_forest))\n    models.append(('xgboost', xgboost))\n    models.append(('cnn', cnn))\n    return models","4dbfece7":"def train_test_split(data, n_test):\n    return data[:-n_test, :], data[-n_test:, :]\n\ndef training(data, n_test, n_models):\n    preds = [list() for a in range(n_models)]\n    train, test = train_test_split(data, n_test)\n\n    history = [x for x in train]\n    for i in range(len(test)):\n        X_test, y_test = test[i, :-1], test[i, -1]\n        models = get_models()\n        for j, (name, model) in enumerate(models):\n            y_hat = model(history, X_test)\n            preds[j].append(y_hat)\n            if(i % 2 == 0):\n                print('i:{:3d}, Model:{:15s}, Expected:{:.1f}, Predicted:{:.1f}'\n                      .format(i, name, y_test, y_hat))\n            \n        history.append(test[i])\n    \n    errors = [list() for a in range(n_models)]\n    for i, error in enumerate(errors):\n        errors[i] = mean_absolute_error(test[:, -1], preds[i])\n    return errors, test[:, -1], preds","e6a9d63a":"mae, y, y_hat = training(data, 12, 3)","2193e469":"print('Random Forest MAE: %.3f' % mae[0])\nprint('XGBoost MAE: %.3f' % mae[1])\nprint('CNN MAE: %.3f' % mae[2])","3b96bb66":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 5))\naxes[0].set_title(\"Random Forest\")\naxes[0].plot(y, label='Expected')\naxes[0].plot(y_hat[0], label='Predicted')\naxes[0].legend()\naxes[1].set_title(\"XGBoost\")\naxes[1].plot(y, label='Expected')\naxes[1].plot(y_hat[1], label='Predicted')\naxes[1].legend()\naxes[2].set_title(\"CNN\")\naxes[2].plot(y, label='Expected')\naxes[2].plot(y_hat[2], label='Predicted')\naxes[2].legend()\n#fig.tight_layout()","883e23ac":"# Training","874c8525":"<pre>\n         _\n       .' '.\n  __  \/     \\   _\n \/.-;|  \/'._|_.'#`\\   Predict <b>California Births<\/b>\n||   |  |  _       |     Models: - Random Forest\n\\\\__\/|  \\.' ;'-.__\/              - XGBoost\n '--' \\     \/                    - CNN\n       '._.'\n<\/pre>","afa74cbd":"# Retrieve Data","ccb0c017":"# Analysis","7ab2dd9f":"# Models","d8522a0c":"# Series to Supervised Learning"}}