{"cell_type":{"76787925":"code","371257c9":"code","1f2d81f5":"code","e5953fcf":"code","90279f6a":"code","5fad70b0":"code","41a08658":"code","ca4775c7":"code","a56e0bf8":"code","f85b2321":"code","561918af":"code","5c9d14b9":"code","8c48c4f9":"code","b64e8891":"code","3facf019":"code","72fef682":"code","77015e10":"code","cd3e768f":"code","f4b2e388":"code","dace0949":"code","65bdf481":"code","a83b4aa7":"code","20da8a13":"code","ff87c4a4":"code","c3aeefd8":"code","8755bb3e":"code","654fec30":"code","22a67e8c":"code","385e632c":"code","e79ec7de":"code","16cfa61a":"code","30259bd6":"code","3549fe52":"code","552c2829":"code","2173cbbc":"markdown","70d3ad50":"markdown","03a7a04e":"markdown","bb22827e":"markdown","2eb8737d":"markdown","7fe6b6c8":"markdown","fb9ceef4":"markdown","34f27063":"markdown"},"source":{"76787925":"!pip install icevision[all]\n!pip install torch_optimizer\n!pip install wandb -U","371257c9":"import os\nimport functools\nimport numpy as np\nimport pandas as pd\nimport torch_optimizer as optim\nimport torchvision.transforms as T\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom icevision.all import *\nfrom tqdm.contrib.concurrent import process_map\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint, Callback\nfrom kaggle_secrets import UserSecretsClient\nfrom torch.distributions.beta import Beta\nfrom operator import itemgetter\nfrom PIL import Image\nfrom icevision.metrics import Metric\nfrom scipy.optimize import linear_sum_assignment","1f2d81f5":"user_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"wandb-key\")\n\n!wandb login $wandb_key","e5953fcf":"pl.seed_everything(42)","90279f6a":"FRAME_RANGE = 4\nVALID_PERCENT = 0.2\nSIZE = (256, 256)\nCLASSES_NUM = 2\nIMPACT_CLASS = 2","5fad70b0":"path = Path('\/kaggle\/input\/nfl-impact-detection')\ntrain_video_path = path\/'train'","41a08658":"video_labels = pd.read_csv(path\/'train_labels.csv').fillna(0)\nvideo_labels.head(2)","ca4775c7":"video_labels_with_impact = video_labels[video_labels['impact'] > 0]\nfor index, row in tqdm(video_labels_with_impact.iterrows(), total=len(video_labels_with_impact)):\n    frames = np.arange(-FRAME_RANGE, FRAME_RANGE + 1) + row.frame\n    indexes = video_labels.query('video == @row.video and frame in @frames and label == @row.label').index\n    video_labels.loc[indexes, 'impact'] = 1\nvideo_labels['image_name'] = video_labels['video'].str.replace('.mp4', '') + '_' + video_labels['frame'].astype(str) + '.png'\nvideo_labels = video_labels[video_labels.groupby('image_name')['impact'].transform('sum') > 0].reset_index(drop=True)\nvideo_labels['impact'] = video_labels['impact'].astype(int) + 1\nvideo_labels.head()","a56e0bf8":"video_names = np.random.permutation(video_labels.video.unique())\nvalid_video_len = int(len(video_names) * VALID_PERCENT)\nvideo_valid = video_names[:valid_video_len]\nvideo_train = video_names[valid_video_len:]\nimages_valid = video_labels[video_labels.video.isin(video_valid)].image_name.unique()\nimages_train = video_labels[video_labels.video.isin(video_train)].image_name.unique()","f85b2321":"def make_images(video_name, video_dir, video_labels, out_dir, only_with_impact=True, impact_cls=IMPACT_CLASS):\n    vidcap = cv2.VideoCapture(str(video_dir\/video_name))\n    frame = 0\n    while True:\n        read, img = vidcap.read()\n        if not read:\n            break\n        frame += 1\n        if only_with_impact:\n            query_str = 'video == @video_name and frame == @frame and impact == @impact_cls'\n            boxes = video_labels.query(query_str)\n            if len(boxes) == 0:\n                continue\n        image_path = f'{out_dir}\/{video_name}'.replace('.mp4', f'_{frame}.png')\n        _ = cv2.imwrite(image_path, img)","561918af":"train_images_path = Path('\/kaggle\/working\/train_images')\ntrain_images_path.mkdir()","5c9d14b9":"make_images_part = functools.partial(make_images, video_dir=train_video_path, video_labels=video_labels, out_dir=train_images_path)\nprocess_map(make_images_part, os.listdir(train_video_path), max_workers=2);","8c48c4f9":"len(os.listdir(train_images_path))","b64e8891":"class HelmetParser(parsers.FasterRCNN, parsers.FilepathMixin, parsers.SizeMixin):\n    def __init__(self, df, source):\n        self.df = df\n        self.source = source\n\n    def __iter__(self):\n        yield from self.df.itertuples()\n\n    def __len__(self):\n        return len(self.df)\n\n    def imageid(self, o) -> Hashable:\n        return o.image_name\n\n    def filepath(self, o) -> Union[str, Path]:\n        return self.source\/o.image_name\n\n    def image_width_height(self, o) -> Tuple[int, int]:\n        return get_image_size(self.filepath(o))\n\n    def labels(self, o) -> List[int]:\n        return [o.impact]\n\n    def bboxes(self, o) -> List[BBox]:\n        return [BBox.from_xywh(o.left, o.top, o.width, o.height)]","3facf019":"parser = HelmetParser(video_labels, train_images_path)\ndata_splitter = FixedSplitter([images_train, images_valid])","72fef682":"train_rs, valid_rs = parser.parse(data_splitter=data_splitter, autofix=True)","77015e10":"show_records(train_rs[:1], display_label=True, figsize=(10, 10), ncols=1)","cd3e768f":"train_tfms = tfms.A.Adapter([tfms.A.HorizontalFlip(p=0.5),\n                             tfms.A.RGBShift(), tfms.A.RandomBrightnessContrast(),\n                             tfms.A.Blur(blur_limit=(1, 3), p=0.5),\n                             tfms.A.OneOrOther(tfms.A.RandomSizedBBoxSafeCrop(*SIZE), tfms.A.Resize(*SIZE), p=0.5),\n                             tfms.A.Normalize()])\nvalid_tfms = tfms.A.Adapter([tfms.A.Resize(*SIZE), tfms.A.Normalize()])","f4b2e388":"train_ds = Dataset(train_rs, train_tfms)\nvalid_ds = Dataset(valid_rs, valid_tfms)","dace0949":"samples = [train_ds[0] for _ in range(6)]\nshow_samples(samples, ncols=3, denormalize_fn=denormalize_imagenet, display_label=False, figsize=(30,30))","65bdf481":"samples = [valid_ds[0] for _ in range(3)]\nshow_samples(samples, ncols=3, denormalize_fn=denormalize_imagenet, display_label=False, figsize=(30,30))","a83b4aa7":"train_dl = efficientdet.train_dl(train_ds, batch_size=32, num_workers=2, shuffle=True)\nvalid_dl = efficientdet.valid_dl(valid_ds, batch_size=32, num_workers=2, shuffle=False)","20da8a13":"batch, samples = next(iter(train_dl))\nshow_samples(samples[:6], ncols=3, denormalize_fn=denormalize_imagenet, display_label=False, figsize=(30,30))","ff87c4a4":"model = efficientdet.model(model_name=\"tf_efficientdet_d3\", num_classes=CLASSES_NUM, img_size=SIZE)","c3aeefd8":"class MixUp:\n    def __init__(self, alpha=20.0, min_w=0.4, max_w=0.6):\n        self.distrib = Beta(tensor(alpha), tensor(alpha))\n        self.min_w = min_w\n        self.max_w = max_w\n    \n    def __call__(self, batch):\n        x, y = batch\n        batch_size = x.shape[0]\n        device = x.device\n        self.lam = self.distrib.sample((batch_size,)).squeeze().to(device)\n        self.lam = torch.clip(self.lam, self.min_w, self.max_w)\n        self.shuffle = torch.randperm(batch_size, device=device)\n        classes = y['cls']\n        bbox = y['bbox']\n        dims = len(x.shape)\n        return (torch.lerp(x, x[self.shuffle], self.unsqueeze(self.lam, dims - 1, -1)),\n                {\n                  'bbox': list(map(torch.cat, zip(bbox, itemgetter(*self.shuffle)(bbox)))),\n                  'cls': list(map(torch.cat, zip(classes, itemgetter(*self.shuffle)(classes))))\n                }\n               )\n    \n    def unsqueeze(self, x, n, dim):\n        for _ in range(n):\n            x = x.unsqueeze(dim)\n        return x","8755bb3e":"class F1Metric(Metric):\n    def __init__(self, detection_threshold):\n        self._records, self._preds = [], []\n        self.detection_threshold = detection_threshold\n\n    def _reset(self):\n        self._records.clear()\n        self._preds.clear()\n\n    def accumulate(self, records, preds):\n        self._records.extend(records)\n        self._preds.extend(preds)\n\n    def finalize(self) -> Dict[str, float]:\n        gt_boxes = []\n        for s in self._records:\n            gt_boxes.append(list(map(lambda b: np.array(b.xyxy), \n                                     np.array(s[\"bboxes\"])[np.array(s[\"labels\"]) == IMPACT_CLASS])))\n        pred_boxes = []\n        for p in self._preds:\n            pred_boxes.append(list(map(lambda b: np.array(b.xyxy),\n                                       np.array(p[\"bboxes\"])[\n                                           (np.array(p[\"scores\"]) >= self.detection_threshold)\n                                           & (np.array(p[\"labels\"]) == IMPACT_CLASS)\n                                       ]\n                                      )))\n        \n        tps, fps, fns = [], [], []\n        for i in range(len(gt_boxes)):\n            tp, fp, fn = self.precision_calc(gt_boxes[i], pred_boxes[i])\n            tps.append(tp)\n            fps.append(fp)\n            fns.append(fn)\n\n        tp = np.sum(tps)\n        fp = np.sum(fps)\n        fn = np.sum(fns)\n        precision = tp \/ (tp + fp + 1e-6)\n        recall =  tp \/ (tp + fn + 1e-6)\n        f1_score = 2 * (precision*recall ) \/(precision + recall + 1e-6)\n        \n        self._reset()\n        return {\"f1\": f1_score}\n    \n    @property\n    def name(self) -> str:\n        return self.__class__.__name__ + str(self.detection_threshold)\n    \n    def iou(self, bbox1, bbox2):\n        bbox1 = list(map(float, bbox1))\n        bbox2 = list(map(float, bbox2))\n\n        (x0_1, y0_1, x1_1, y1_1) = bbox1\n        (x0_2, y0_2, x1_2, y1_2) = bbox2\n\n        # get the overlap rectangle\n        overlap_x0 = max(x0_1, x0_2)\n        overlap_y0 = max(y0_1, y0_2)\n        overlap_x1 = min(x1_1, x1_2)\n        overlap_y1 = min(y1_1, y1_2)\n\n        # check if there is an overlap\n        if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:\n            return 0\n\n        # if yes, calculate the ratio of the overlap to each ROI size and the unified size\n        size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)\n        size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)\n        size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)\n        size_union = size_1 + size_2 - size_intersection\n\n        return size_intersection \/ size_union\n    \n    def precision_calc(self, gt_boxes, pred_boxes):\n        cost_matix = np.ones((len(gt_boxes), len(pred_boxes)))\n        for i, box1 in enumerate(gt_boxes):\n            for j, box2 in enumerate(pred_boxes):\n                iou_score = self.iou(box1, box2)\n\n                if iou_score < 0.35:\n                    continue\n                else:\n                    cost_matix[i,j]=0\n\n        row_ind, col_ind = linear_sum_assignment(cost_matix)\n        fn = len(gt_boxes) - row_ind.shape[0]\n        fp = len(pred_boxes) - col_ind.shape[0]\n        tp = 0\n        for i, j in zip(row_ind, col_ind):\n            if cost_matix[i,j] == 0:\n                tp += 1\n            else:\n                fp += 1\n                fn += 1\n        return tp, fp, fn","654fec30":"class LightModel(efficientdet.lightning.ModelAdapter):\n    def __init__(self, lr, epochs, dl_len, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.mixup = MixUp()\n        self.save_hyperparameters('lr', 'epochs', 'dl_len')\n        \n    def training_step(self, batch, batch_idx):\n        self.mixuped = self.mixup(batch[0])\n        return super().training_step((self.mixuped, batch[1]), batch_idx)\n    \n    def configure_optimizers(self):\n        optimizer =  optim.RAdam(self.parameters(), lr=self.hparams.lr, weight_decay=0.1)\n        scheduler = {\n            'scheduler': torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n                                                                    self.hparams.dl_len * self.hparams.epochs,\n                                                                    self.hparams.lr \/ 100),\n            'interval': 'step',\n            'frequency': 1,\n        }\n        return [optimizer], [scheduler]","22a67e8c":"metrics = [F1Metric(0.3)]\nlight_model = LightModel(1e-2, 5, len(train_dl), model, metrics=metrics)","385e632c":"wandb_logger = WandbLogger(name='effdet_d3', project='NFL', log_model=True)","e79ec7de":"lr_monitor = LearningRateMonitor(logging_interval='step')\n\ntrainer = pl.Trainer(max_epochs=5, gpus=1, precision=16,\n                     callbacks=[lr_monitor],\n                     logger=wandb_logger,\n                     log_every_n_steps=1,\n                     flush_logs_every_n_steps=10,\n                     auto_lr_find=False\n                    )","16cfa61a":"# lr_finder = trainer.tuner.lr_find(light_model, train_dl, valid_dl)\n# fig = lr_finder.plot(suggest=True)\n# fig.show()","30259bd6":"trainer.fit(light_model, train_dl, valid_dl)","3549fe52":"imgs = light_model.mixuped[0].permute(0, 2, 3, 1).cpu().numpy()\npx.imshow(denormalize_imagenet(imgs[:3]), facet_col=0)","552c2829":"!rm -rf \/kaggle\/working\/train_images\/*","2173cbbc":"Sample train batch:","70d3ad50":"For metric implementation [this notebook](https:\/\/www.kaggle.com\/nvnnghia\/evaluation-metrics) was used.","03a7a04e":"Sample from validation dataset:","bb22827e":"Create images using frames with impact labels:","2eb8737d":"This notebook uses [IceVision](https:\/\/github.com\/airctic\/icevision) object detection library.\n\nFor final submission efficientdet_d5 model was trained on 512x512 image size and also pretrained on provided separate image dataset.\n\n[Inference notebook](https:\/\/www.kaggle.com\/nikitautin\/35th-place-efficientdet-inference)","7fe6b6c8":"Mixup augmentation samples:","fb9ceef4":"Sample from train dataset:","34f27063":"Set impact label for helmets at range of 4 frames from labeled impact:"}}