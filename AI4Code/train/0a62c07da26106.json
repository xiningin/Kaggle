{"cell_type":{"6d6890dc":"code","be4c8b6d":"code","bca14f31":"code","422e31d1":"code","b9660490":"code","fa1aed49":"code","42ea68b1":"code","0432647e":"code","bdae73b7":"code","2a2d5d58":"code","1a6c4085":"code","1fe27c77":"markdown","3eb8d00f":"markdown","f086b91e":"markdown","14c6d9ec":"markdown","d88f894b":"markdown","decfad6d":"markdown"},"source":{"6d6890dc":"import os\n\nimport math, random\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nplt.rc('figure', figsize=(20, 5))\n\nimport torch\nimport torchaudio\nfrom torchaudio import transforms\n\nimport librosa\nimport librosa.display","be4c8b6d":"sample_audio = '..\/input\/cat-meow-classification\/dataset\/dataset\/B_ANI01_MC_FN_SIM01_101.wav'","bca14f31":"# Helper Functions to display Waveform and Spectrogram\ndef get_waveform(samples=None,sample_rate=None, wav_file=False, audiofile=None):\n    # display Waveform\n    if wav_file == True:\n        samples, sample_rate = librosa.load(audiofile, sr=None)\n        \n    librosa.display.waveplot(samples, sr=sample_rate)\n\n    \ndef get_spectrogram(samples=None,sample_rate=None, wav_file=False, audiofile=None):\n    # Display Mel Spectrogram\n    if wav_file == True:\n        samples, sample_rate = librosa.load(audiofile, sr=None)\n        \n    sgram = librosa.stft(samples)\n    sgram_mag, _ = librosa.magphase(sgram)\n    mel_scale_sgram = librosa.feature.melspectrogram(S=sgram_mag, sr=sample_rate, n_mels=512)\n    mel_sgram = librosa.amplitude_to_db(mel_scale_sgram, ref=np.min)\n    librosa.display.specshow(mel_sgram, sr=sample_rate, x_axis='time',y_axis='mel')\n    plt.colorbar(format='%+2.0f db')","422e31d1":"get_waveform(wav_file=True,audiofile=sample_audio)","b9660490":"get_spectrogram(wav_file=True,audiofile=sample_audio)","fa1aed49":"class AudioHelper():\n    # Load an audo file\n    def open(audio_file:str):\n        signal, sample_rate = torchaudio.load(audio_file)\n        \n        return signal, sample_rate\n    \n    # changes the channel of audio\n    # in this dataset, all samples are single channels, no need to rechannel\n    def rechannel(audio, desired_channels=1):\n        signal, sample_rate = audio\n        \n        if signal.shape[0] == desired_channels:\n            return audio\n        \n        if desired_channel == 1:\n            resig = signal[:1, :]\n        \n        else:\n            resig = torch.cat([signal, signal])\n        \n        return resig, sample_rate\n    \n    # Standardize Sampling rate\n    # in this dataset, all samples are sampled at 8000 Hz, no need to resample\n    def resample(audio, desired_samplerate=8000):\n        signal, sample_rate = audio\n        \n        if sample_rate == desired_samplerate:\n            return audio\n        \n        num_channels = signal.shape[0]\n        # resample the first channel\n        resig = torchaudio.transforms.Resample(sample_rate, desired_samplerate)(signal[:1,:])\n        # Resample the second channel\n        if num_channels > 1:\n            resampled_second = torchaudio.transforms.Resample(sample_rate, desired_samplerate)(signal[:1,:])\n            resig = torch.cat([resig, resampled_second])\n            \n        return resig, desired_samplerate\n    \n    def resize(audio, max_time):\n        signal, sample_rate = audio\n        num_rows, signal_length = signal.shape\n        max_length = sample_rate\/\/1000 * max_time\n        \n        if signal_length >= max_length :\n            signal = signal[:, :max_length]\n        \n        elif signal_length < max_length :\n            # padding starting and ending of signal\n            pad_begin_length = random.randint(0, max_length - signal_length)\n            pad_end_length = max_length - signal_length - pad_begin_length\n            \n            # Pad with zeros\n            pad_begin = torch.zeros((num_rows, pad_begin_length))\n            pad_end = torch.zeros((num_rows, pad_end_length))\n            \n            signal = torch.cat((pad_begin, signal, pad_end), 1)\n        \n        return signal, sample_rate\n    \n    def time_shift(audio, shift_limit):\n        signal,sample_rate = audio\n        _, signal_length = signal.shape\n        \n        shift_amt = int(random.random() * shift_limit * signal_length)\n        \n        return signal.roll(shift_amt), sample_rate\n    \n    def spectro_gram(audio, n_mels=256, n_fft=1024, hop_len=None):\n        signal,sample_rate = audio\n        top_db = 130\n\n        # spec has shape [channel, n_mels, time], where channel is mono, stereo etc\n        spec = transforms.MelSpectrogram(sample_rate, n_fft=n_fft, hop_length=hop_len, n_mels=n_mels)(signal)\n\n        # Convert to decibels\n        spec = transforms.AmplitudeToDB(top_db=top_db)(spec)\n        \n        return spec\n    \n    def spectro_augment(spec, max_mask_pct=0.1, n_freq_masks=1, n_time_masks=1):\n        _, n_mels, n_steps = spec.shape\n        mask_value = spec.mean()\n        aug_spec = spec\n\n        freq_mask_param = max_mask_pct * n_mels\n        for _ in range(n_freq_masks):\n              aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec, mask_value)\n\n        time_mask_param = max_mask_pct * n_steps\n        for _ in range(n_time_masks):\n              aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec, mask_value)\n\n        return aug_spec\n    ","42ea68b1":"# Helper Function to convert from torch.tensor to numpy.ndarray\ndef get_numpyarray(arr:torch.tensor)->np.array:\n    return torch.squeeze(arr).detach().cpu().numpy()","0432647e":"Audio = AudioHelper.open(sample_audio)\nresized_sample, resized_sr = AudioHelper.resize(Audio, 4000) # 4000 ms = 4s\nget_waveform(get_numpyarray(resized_sample), resized_sr)","bdae73b7":"get_spectrogram(get_numpyarray(resized_sample), resized_sr)","2a2d5d58":"shifted_sample, shifted_sr = AudioHelper.time_shift(AudioHelper.resize(Audio, 4000), 500)# 1000 ms = 1seconds\nget_waveform(get_numpyarray(shifted_sample), shifted_sr)","1a6c4085":"get_spectrogram(get_numpyarray(shifted_sample), shifted_sr)","1fe27c77":"# Resizing Audio","3eb8d00f":"# Pre Processing","f086b91e":"# Frequency Masking\n![Original](https:\/\/download.pytorch.org\/torchaudio\/doc-assets\/specaugment_freq_masking1.png)\n![Masked](https:\/\/download.pytorch.org\/torchaudio\/doc-assets\/specaugment_freq_masking2.png)","14c6d9ec":"# Time Shifting","d88f894b":"# Augmentation for Audio\nJust like for image, Augmenting our audio samples help make our model more robust to disturbances and help it generalize better. Augmentation for Audio is different than for images due to inherent temporal nature of audio, so augmentations like Horizontal Flip, Skewing, Rotation, etc.. fundamentally alter the audio. We have to be very careful while performing augmentation. \n\nAudio Augmentation can be broadly be broken down into\n1. Raw Audio Augmentation\n2. Spectrogram Augmentation (SpecAugment)\nLet us see what each of them entail\n\n## Raw Audio Augmentation\n1. Time Shift - Shift audio left or right by a random amount\n2. Pitch Shift - Randomly modify the frequency of parts of the sound\n3. Time Stretch - Randomly slow down or speed up the sound\n4. Add Noise - Add some random values to the sound\n\n## Spectrogram Augmentation\n1. Frequency Masking - Randomly mask out a range of consecutive frequencies by adding horizontal bars on the spectrogram.\n2. Time Mask - Randomly block out ranges of time from the spectrogram by using vertical bars.","decfad6d":"# Time Masking\n![Original Waveform](https:\/\/download.pytorch.org\/torchaudio\/doc-assets\/specaugment_time_masking1.png)\n![Time Masked](https:\/\/download.pytorch.org\/torchaudio\/doc-assets\/specaugment_time_masking2.png)"}}