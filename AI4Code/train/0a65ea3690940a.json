{"cell_type":{"9381906f":"code","b96933ce":"code","0b433be0":"code","adb679d0":"code","abb4cd3f":"code","4b6d594a":"code","a387bd3f":"code","facee3d8":"code","e85ab8d6":"code","1852e91d":"code","28cdd5e3":"code","f75c1204":"code","891c219d":"code","5e411dd6":"code","bebc40ac":"code","b3ed4264":"code","72d33ab9":"code","4796122e":"code","65835d25":"code","4db28fc4":"code","6e58bfba":"code","cedf6ffb":"code","a3acb225":"code","09857bb2":"code","36296da4":"code","a2579d97":"code","8d9efb35":"code","24b3de1a":"code","83c705f9":"markdown","31fdb0ea":"markdown","0f48150b":"markdown","c05c34bd":"markdown","16ae428e":"markdown","d0c3664b":"markdown","37ccc4a3":"markdown","bd699449":"markdown","d73d8e42":"markdown","ac1b0149":"markdown","0066695a":"markdown","06128374":"markdown","eb4b04a6":"markdown","f8348e01":"markdown","d6f052a5":"markdown","93cc365c":"markdown","8b3a319a":"markdown","555fa1fa":"markdown","35dfbbba":"markdown"},"source":{"9381906f":"!pip install -q ..\/input\/ftfywhl602\/ftfy-6.0.2-py2.py3-none-any.whl","b96933ce":"!pip install -q ..\/input\/docoptwhl062\/docopt-0.6.2-py2.py3-none-any.whl","0b433be0":"!pip install -q ..\/input\/num2wordswhl0510\/num2words-0.5.10-py2.py3-none-any.whl","adb679d0":"import re\nimport nltk\nimport ftfy\nimport pandas as pd\n\nfrom string import punctuation\nfrom num2words import num2words\nfrom urllib.parse import unquote\nfrom string import punctuation\nfrom nltk.stem import WordNetLemmatizer\nfrom gensim import corpora, models, similarities\nfrom nltk import word_tokenize, sent_tokenize\nfrom wordsegment import load, segment\n\nlemmatizer = WordNetLemmatizer()\nload()","abb4cd3f":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")","4b6d594a":"# For convenience, combine the two\ntrain_df['is_train'] = True\ntest_df['is_train'] = False\n\ndf = pd.concat([train_df, test_df], axis=0)","a387bd3f":"df['excerpt'] = df['excerpt'].apply(lambda x: str(x).lower().replace('\\\\', '').replace('_', ' '))","facee3d8":"df['excerpt'] = df['excerpt'].apply(lambda x: ftfy.fix_text(x))","e85ab8d6":"df['excerpt'][df['excerpt'].str.contains(\"  \")]","1852e91d":"def remove_multiple_spaces(text):\n    text = re.sub('\\s+',  ' ', text)\n    return text\n\ndf['excerpt'] = df['excerpt'].apply(lambda x: remove_multiple_spaces(x))","28cdd5e3":"def decontraction(phrase):\n    # specific\n    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n\ndf['excerpt'] = df['excerpt'].apply(lambda x: decontraction(x))","f75c1204":"def remove_punct(text):\n    new_punct = re.sub('\\ |\\!|\\?', '', punctuation)\n    table=str.maketrans('', '', new_punct)\n    return text.translate(table)\n\ndf['excerpt'] = df['excerpt'].apply(lambda x: remove_punct(x))","891c219d":"def lemma(text):\n    words = word_tokenize(text)\n    return ' '.join([lemmatizer.lemmatize(w, pos='v') for w in words])\n\ndf['excerpt'] = df['excerpt'].apply(lambda x: lemma(x))","5e411dd6":"df['excerpt'] = df['excerpt'].apply(lambda x: re.sub(r\"(\\d+)\", lambda s: num2words(int(s.group(0))), x))","bebc40ac":"df[df['is_train']].iloc[:, :-1].to_csv(\"train.csv\", index=False)\ndf[~df['is_train']].iloc[:, :-3].to_csv(\"test.csv\", index=False)","b3ed4264":"# Takes as input the tweet dataframe, dictionary, corpus and dimensions for the tweets and returns \n# a new dataframe with each tweet characterized by the new lower dimensional features\n# also returns the topics if desired\ndef latent_semantic_analysis(df, dictionary, corpus_tfidf, dimensions, return_topics = False, n_topics = 10, n_words = 10):\n    # Create a lsi wrapper around the tfidf wrapper\n    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=dimensions, power_iters=10)\n    corpus_lsi = lsi[corpus_tfidf]\n    \n    #create the features for a new dataframe\n    features = []\n    for doc in corpus_lsi:\n        features.append(remove_doc_label(doc))\n        \n    # Create a new dataframe with the features\n    df_features = pd.DataFrame(data = features)\n    \n    #return the new features dataframe devoid of columns that contain nothing\n    if return_topics:\n        return (df_features.fillna(0), lsi.print_topics(n_topics, num_words = n_words), lsi)\n    else:\n        return df_features.fillna(0)","72d33ab9":"# Makes the gensim dictionary and corpus\ndef make_dictionary_and_corpus(df):\n    # The tokenized and stemmed data form our texts database \n    texts = df.copy()\n    \n    # Check how frequently a given word appears and remove it if only one occurrence\n    frequency = defaultdict(int)\n    for text in texts:\n        for token in text:\n            frequency[token] += 1\n    texts = [[token for token in text if frequency[token] > 1] for text in texts]\n    \n    # Create a gensim dictionary\n    dictionary = corpora.Dictionary(texts)\n    \n    # Create a new texts of only the ones I will analyze\n    texts = df.copy()    \n    \n    # Create the bag of words corpus\n    corpus = [dictionary.doc2bow(text) for text in texts]\n    #corpus = [token_word2vec_map(text, frequency) for text in texts]\n    \n    # Create a tfidf wrapper and convert the corpus to a tfidf format\n    tfidf = models.TfidfModel(corpus)\n    corpus_tfidf = tfidf[corpus]\n    \n    # Return a tuple with the dictionary and corpus\n    return (dictionary, corpus_tfidf, corpus, tfidf)\n\n#clean the features for use in dataframe\ndef remove_doc_label(doc):\n    cleaned_doc = []\n    for element in doc:\n        cleaned_doc.append(element[1])\n    return cleaned_doc","4796122e":"from nltk.corpus import stopwords\nfrom collections import defaultdict","65835d25":"df['excerpt_tokenized'] = df['excerpt'].apply(\n    lambda excerpt: [word for word in word_tokenize(excerpt) if word not in stopwords.words('english')]\n)","4db28fc4":"# Generate the dictionary and the corpus for our tweets\ntext_col = \"excerpt_tokenized\"\ndictionary, corpus_tfidf, corpus_bow, tfidf = make_dictionary_and_corpus(df[text_col])","6e58bfba":"dimensions = 43\ndf_lsi_features, topics, lsi = latent_semantic_analysis(df, dictionary, corpus_tfidf, dimensions, True, 15, 20)","cedf6ffb":"for topic in topics:\n    print(\"Topic %d:\" % topic[0])\n    print(topic[1] + \"\\n\")","a3acb225":"import matplotlib.pyplot as plt","09857bb2":"#set the two topics\nfeature_0 = 0\nfeature_1 = 1\nfeature_2 = 9\n\n# Extract the data for plotting\nfeature_0 = df_lsi_features[feature_0]\nfeature_1 = df_lsi_features[feature_1]\nfeature_2 = df_lsi_features[feature_2]\n\n# Things to plot\nplt.scatter(feature_0, feature_1, c=\"b\", s=40, alpha=0.3, linewidths=0.0, label = \"More readable\")\nplt.scatter(feature_0, feature_2, c=\"r\", s=40, alpha=0.3, linewidths=0.0, label = \"Less readable\")\n\n#backround grid details\naxes = plt.gca()\naxes.grid(b = True, which = 'both', axis = 'both', color = 'gray', linestyle = '-', alpha = 0.5, linewidth = 0.5) \n# axes.set_axis_bgcolor('white')  \n\n#font scpecifications\ntitle_font = {'family' : 'arial', 'color'  : 'black', 'weight' : 'heavy','size': 20}\naxis_label_font = {'family' : 'arial', 'color'  : 'black', 'weight' : 'normal','size': 20}\n\n#figure size and tick style\nplt.rcParams[\"figure.figsize\"] = [6,6]\nplt.rc('axes',edgecolor='black',linewidth=1)\nplt.tick_params(which='both', axis='both', color='black', length=4, width=0.5)\nplt.rcParams['xtick.direction'] = 'in'\nplt.rcParams['ytick.direction'] = 'in'\n\n#axis range and labels (also specify if log or not)\nplt.xlim(0.0, 0.4)\n#plt.xscale('log')\nplt.ylim(-0.3, 0.3)\nplt.xlabel(r'Origin topic', y=3, fontsize=20, fontdict = axis_label_font)\nplt.ylabel(r'Target topic', fontsize=20, fontdict = axis_label_font)\n\n#title and axis labels\nplt.tick_params(axis='both', labelsize=20)\nplt.title('Features', y=1.05, fontdict = title_font)\n\n#legend details\nlegend = plt.legend(shadow = True, frameon = True, fancybox = False, ncol = 1, fontsize = 15, loc = 'lower left')\nframe = legend.get_frame()\n#frame.set_width(100)\nframe.set_facecolor('white')\nframe.set_edgecolor('black')\n    \nplt.show()","36296da4":"!pip install -q \/kaggle\/input\/coleridge-packages\/seqeval-1.2.2-py3-none-any.whl\n!pip install -q simpletransformers==0.51.0 --no-index --find-links=file:\/\/\/kaggle\/input\/simpletransformers\/simpletransformers-0.51.0\n\nfrom sklearn.model_selection import train_test_split\nfrom simpletransformers.classification import ClassificationModel, ClassificationArgs","a2579d97":"train = df[df['is_train']][['excerpt', 'target']].copy()\ntrain.columns = ['text', 'labels']\ntrain_df, valid_df = train_test_split(train, test_size=0.01, random_state=42)","8d9efb35":"model_args = ClassificationArgs()\nmodel_args.max_seq_length = 300\nmodel_args.num_train_epochs = 5\nmodel_args.regression = True\nmodel_args.no_save = True\nmodel_args.save_model_every_epoch = False\nmodel_args.save_steps = -1\n\nmodel = ClassificationModel(\n    \"roberta\",\n    \"..\/input\/robertalarge\",\n    num_labels=1,\n    args=model_args\n)\n\nmodel.train_model(train_df)\n\nresult, model_outputs, wrong_predictions = model.eval_model(valid_df)\nprint(result)","24b3de1a":"test = df[~df['is_train']][['id', 'excerpt']].copy()\ntest.columns = ['id', 'text']\n\npredictions, _ = model.predict(test['text'].values)\ntest['target'] = predictions\n\ntest[['id', 'target']].to_csv('submission.csv', index=False)","83c705f9":"Convert to lower case. This is the most common of transformation and is pretty self-explanatory","31fdb0ea":"Save back the datasets","0f48150b":"### Upvote if it was remotely helpful! More to come :)","c05c34bd":"### Latent semantic analysis. \n\nLet's look at how well our dataset has been separated, based on abstract, high level topics using LDA analysis","16ae428e":"Finally, any numbers should be replaced with their letter equivalents.","d0c3664b":"Contractions. The ability to replace contractions like \"don't\" with \"do not\" and \"we've\" with \"we have\" can help us reveal tokens\/features that are hidden in such a way. Normally, I would use the [pycontractions](https:\/\/pypi.org\/project\/pycontractions\/) library, but it does not support python 3.7. Regardless, the way to use this goes as follows:\n\n```\nfrom pycontractions import Contractions\n\ncont = Contractions(api_key=\"text8\")\ndf['excerpt'].apply(lambda x: list(cont.expand_texts([x]))[0])\n```\n\nFor the purposes of this demo, we can write our own function like so:","37ccc4a3":"**Workflow**\nCreate a dictionary and corpus\n  * Dictionary: use words from all of the tweets for more tokens\n  * Corpus: composed of the tweets with the greatest certainty in their classification.","bd699449":"### \u2699 Basic text pre-processing and new dataset generation\nIn this notebook, we will use the most common methods of text pre-processing to reduce the number of features across the train and test sets and homogenise the input. ","d73d8e42":"**Plot topics**\n\nTopic 0 and topic 1 seem to be easily readable, as they contain easier words like old, man and mother, while topic 9 a lot less so, as it includes words such as bacteria, DNA and species. Let's verify this with a plot..","ac1b0149":"In case there is unicode text in the input, this will fix inconsistencies and glitches in it, such as mojibake (text that was decoded in the wrong encoding).","0066695a":"### Modelling","06128374":"The following were copied in from https:\/\/www.kaggle.com\/hengzheng\/simpletransformers-regression-starter-less-code. Many thanks @hengzheng ","eb4b04a6":"Indeed, the topics match our intuition based on what tokens they contain! The readable topics are more spread on the x and y axis in comparison with non-readable topics, which remained \"clumped\" near the origin. With more topics and text augmentations applied to enrich the input dataset, good separation is possible.","f8348e01":"### Transformations","d6f052a5":"Words are typically inflected (e.g., letters suffixed, affixed, etc.) to express their forms (e.g., plural, tense, etc.). Dog -> Dogs is an example of inflection. Usually, words must be compared in their native forms for effective text matching.\n\nLemmatization is one method used to convert a word to a non-inflected form, i.e. reduce a word to its most native form.\n\nIt uses a simple mechanism that removes or modifies inflections to form the root word, but the root word may not be a valid word in the language. Also, it removes or modifies the inflections to form the root word, but the root word needs to be a valid word in the language.","93cc365c":"Punctuation should also be irrelevant in terms of modelling for readability difficulty. Remove them","8b3a319a":"Double spaces are erroneous in our context. Remove them","555fa1fa":"Boilerpate follows","35dfbbba":"**Check out the \"topics\"**\n\n  1. Print out the top 15 topics with the top 20 tokens they are composed of\n  2. Plot some topics against each other with colors to indicate class"}}