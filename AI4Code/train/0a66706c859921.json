{"cell_type":{"2415a234":"code","2a88bc8c":"code","910fbde3":"code","a94034cc":"code","ed98558d":"code","e2ce7fdd":"code","48261e4d":"code","d571869a":"code","aa5c56b3":"code","3a8008d0":"code","439ac087":"code","c8c916ac":"code","9ddbb0d9":"code","7a9b9b13":"code","c8b6325c":"code","110cbd6b":"code","caf8d7fa":"code","f25d88e8":"code","d27e1094":"code","b33f7102":"code","72d847e7":"code","2946ea56":"code","3f369ba2":"code","052078df":"code","6ba10520":"code","470ef0c3":"code","cbf9d5db":"code","6d3a781f":"code","5d1cc4cb":"code","8eab99f4":"code","bbeadc9a":"code","19d91d71":"code","590618a5":"code","18bcde6a":"code","8419c930":"code","f83c8b11":"code","0b83a748":"code","083435cc":"code","9699fcda":"code","0a28b176":"code","4866748a":"code","c83217c4":"code","c9800e0f":"code","b81db1fa":"code","12befe71":"code","7dde7688":"code","264115c8":"code","7682a071":"code","d099bcf4":"code","74073dd2":"code","2af8cf2d":"code","1252de6b":"code","05387059":"code","9e388c7a":"code","f75d0d68":"code","0b99d14f":"code","0d0262e5":"code","741e6659":"code","8a01367f":"code","7e37750f":"code","43cf10b2":"code","5046cdc5":"code","3940593c":"code","df7eabb5":"code","30547a17":"code","a9d50e08":"code","983b97d2":"code","20e3d1d7":"code","1d9146a7":"code","29b54115":"code","1ada53da":"code","5d155e47":"code","60a96dd8":"code","e06c8317":"code","1f9d758a":"code","f30256b9":"code","489e98b5":"code","8a696bca":"code","2e189443":"code","f7927cdc":"code","1e2b8226":"code","ae37d3cd":"code","629e2b99":"code","4a38a638":"code","920323de":"code","fa7608bd":"markdown","322a70af":"markdown","fb3e845d":"markdown","5f63e367":"markdown","e4911e92":"markdown","54a15b51":"markdown","e3248f80":"markdown","e1581dc4":"markdown","f36b2cfb":"markdown","891fe741":"markdown","a4edea22":"markdown","6cdf6928":"markdown","8fd487cf":"markdown","0c880848":"markdown","79996d35":"markdown","82a1a280":"markdown","53029b44":"markdown","a57c764f":"markdown","992a3319":"markdown","d0f477d3":"markdown","2db2c781":"markdown","e15e1798":"markdown","e81c9a86":"markdown","9a36c583":"markdown","480829f1":"markdown","4b14c4f3":"markdown","c2751807":"markdown","9fafbfa5":"markdown","528080a9":"markdown","89f68928":"markdown","ee760136":"markdown","8984beaa":"markdown","41fd9997":"markdown","a94551ab":"markdown","b9ae4bac":"markdown","e83f8228":"markdown","042e5816":"markdown","79f14f68":"markdown","6f2f4a5e":"markdown","e42d2a35":"markdown","bfb3c41d":"markdown","5ee8540a":"markdown","606dc957":"markdown","1294dc2d":"markdown","16803a22":"markdown","6e3f91b0":"markdown","329474ea":"markdown","455f4c8d":"markdown"},"source":{"2415a234":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# pandas\nimport pandas as pd\nfrom pandas import Series,DataFrame\n\n# numpy, matplotlib, seaborn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\n%matplotlib inline\n\nfrom IPython.display import display\n\n# remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2a88bc8c":"train = pd.read_csv('..\/input\/train.csv',index_col='Id')\ntest  = pd.read_csv('..\/input\/test.csv',index_col='Id')\n","910fbde3":"print(train.shape)\ndisplay(train.head(1))\n\nprint(test.shape)\ndisplay(test.head(1))","a94034cc":"import matplotlib.pyplot as plt\nplt.style.use(style='ggplot')\nplt.rcParams['figure.figsize'] = (10, 6)","ed98558d":"train.head(5)","e2ce7fdd":"train.SalePrice.describe()","48261e4d":"print (\"Skew is:\", train.SalePrice.skew())\nplt.hist(train.SalePrice, color='blue')\nplt.show()","d571869a":"target = np.log(train.SalePrice)\nprint (\"Skew is:\", target.skew())\nplt.hist(target, color='blue')\nplt.show()","aa5c56b3":"numeric_features = train.select_dtypes(include=[np.number])\nnumeric_features.dtypes","3a8008d0":"corr = numeric_features.corr()\n\nprint (corr['SalePrice'].sort_values(ascending=False)[1:11], '\\n')\nprint (corr['SalePrice'].sort_values(ascending=False)[-10:])","439ac087":"# How many unique features are there?\n\ntrain.OverallQual.unique()","c8c916ac":"\n#Define a function which can pivot and plot the intended aggregate function \ndef pivotandplot(data,variable,onVariable,aggfunc):\n    pivot_var = data.pivot_table(index=variable,\n                                  values=onVariable, aggfunc=aggfunc)\n    pivot_var.plot(kind='bar', color='blue')\n    plt.xlabel(variable)\n    plt.ylabel(onVariable)\n    plt.xticks(rotation=0)\n    plt.show()\n    ","9ddbb0d9":"pivotandplot(train,'OverallQual','SalePrice',np.median)","7a9b9b13":"# It is a continous variable and hence lets look at the relationship of GrLivArea with SalePrice using a Regression plot\n\n_ = sns.regplot(train['GrLivArea'], train['SalePrice'])","c8b6325c":"train=train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n_ = sns.regplot(train['GrLivArea'], train['SalePrice'])","110cbd6b":"_ = sns.regplot(train['GarageArea'], train['SalePrice'])","caf8d7fa":"train = train[train['GarageArea'] < 1200]\n_ = sns.regplot(train['GarageArea'], train['SalePrice'])","f25d88e8":"# Let us first create a DF for log transformation of SalePrice\ntrain['log_SalePrice']=np.log(train['SalePrice']+1)\nsaleprices=train[['SalePrice','log_SalePrice']]\n\nsaleprices.head(5)","d27e1094":"train=train.drop(columns=['SalePrice','log_SalePrice'])","b33f7102":"print(train.shape)\nprint(test.shape)","72d847e7":"all_data = pd.concat((train, test))\nprint(all_data.shape)\nall_data.head(5)","2946ea56":"null_data = pd.DataFrame(all_data.isnull().sum().sort_values(ascending=False))[:50]\n\nnull_data.columns = ['Null Count']\nnull_data.index.name = 'Feature'\nnull_data","3f369ba2":"(null_data\/len(all_data)) * 100","052078df":"print (\"Unique values are:\", train.MiscFeature.unique())","6ba10520":"for col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType', 'MSSubClass'):\n    all_data[col] = all_data[col].fillna('None')","470ef0c3":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n    all_data[col] = all_data[col].fillna(0)","cbf9d5db":"for col in ('MSZoning', 'Electrical', 'KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType', 'Functional', 'Utilities'):\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])","6d3a781f":"_=sns.regplot(train['LotFrontage'],saleprices['SalePrice'])","5d1cc4cb":"all_data['LotFrontage'] = all_data.groupby('Neighborhood')['LotFrontage'].apply(lambda x: x.fillna(x.median()))","8eab99f4":"figure, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\nfigure.set_size_inches(14,10)\n_ = sns.regplot(train['TotalBsmtSF'], saleprices['SalePrice'], ax=ax1)\n_ =sns.regplot(train['1stFlrSF'], saleprices['SalePrice'], ax=ax2)\n_ = sns.regplot(train['2ndFlrSF'], saleprices['SalePrice'], ax=ax3)\n_ = sns.regplot(train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF'], saleprices['SalePrice'], ax=ax4)","bbeadc9a":"#Impute the entire data set\nall_data['TotalSF']=all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\n#Let's add two new variables for No nd floor and no basement\nall_data['No2ndFlr']=(all_data['2ndFlrSF']==0)\nall_data['NoBsmt']=(all_data['TotalBsmtSF']==0)","19d91d71":"figure, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2)\nfigure.set_size_inches(14,10)\n_ = sns.barplot(train['BsmtFullBath'], saleprices['SalePrice'], ax=ax1)\n_ = sns.barplot(train['FullBath'], saleprices['SalePrice'], ax=ax2)\n_ = sns.barplot(train['BsmtHalfBath'], saleprices['SalePrice'], ax=ax3)\n_ = sns.barplot(train['BsmtFullBath'] + train['FullBath'] + train['BsmtHalfBath'] + train['HalfBath'], saleprices['SalePrice'], ax=ax4)","590618a5":"all_data['TotalBath']=all_data['BsmtFullBath'] + all_data['FullBath'] + all_data['BsmtHalfBath'] + all_data['HalfBath']","18bcde6a":"figure, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3)\nfigure.set_size_inches(18,8)\n_ = sns.regplot(train['YearBuilt'], saleprices['SalePrice'], ax=ax1)\n_ = sns.regplot(train['YearRemodAdd'], saleprices['SalePrice'], ax=ax2)\n_ = sns.regplot((train['YearBuilt']+train['YearRemodAdd'])\/2, saleprices['SalePrice'], ax=ax3)","8419c930":"all_data['YrBltAndRemod']=all_data['YearBuilt']+all_data['YearRemodAdd']","f83c8b11":"# Deleting dominating features over 97%\nall_data=all_data.drop(columns=['Street','Utilities','Condition2','RoofMatl','Heating'])\n","0b83a748":"# treat some numeric values as str which is actually a categorical data\nall_data['MSSubClass']=all_data['MSSubClass'].astype(str)\nall_data['MoSold']=all_data['MoSold'].astype(str)\nall_data['YrSold']=all_data['YrSold'].astype(str)","083435cc":"# I found these features might look better without 0 data. (just like the column '2ndFlrSF' above.)\nall_data['NoLowQual']=(all_data['LowQualFinSF']==0)\nall_data['NoOpenPorch']=(all_data['OpenPorchSF']==0)\nall_data['NoWoodDeck']=(all_data['WoodDeckSF']==0)\nall_data['NoGarage']=(all_data['GarageArea']==0)\nall_data=all_data.drop(columns=['PoolArea','PoolQC']) # most of the houses has no pools. \nall_data=all_data.drop(columns=['MiscVal','MiscFeature']) # most of the houses has no misc feature.","9699fcda":"Basement = ['BsmtCond', 'BsmtExposure', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFinType1', 'BsmtFinType2', 'BsmtQual', 'BsmtUnfSF','TotalBsmtSF']\nBsmt=all_data[Basement]\nBsmt.head()","0a28b176":"Bsmt['BsmtCond'].unique()","4866748a":"from sklearn.preprocessing import LabelEncoder","c83217c4":"Bsmt.head()","c9800e0f":"Bsmt['BsmtCond'].unique()","b81db1fa":"# Replacing Categorical values to numbers\ncond_encoder = LabelEncoder()\nBsmt['BsmtCond']=cond_encoder.fit_transform(Bsmt['BsmtCond'])\n\nexposure_encoder = LabelEncoder()\nBsmt['BsmtExposure'] = exposure_encoder.fit_transform(Bsmt['BsmtExposure'])\n\nfinTyp1_encoder = LabelEncoder()\nBsmt['BsmtFinType1'] = finTyp1_encoder.fit_transform(Bsmt['BsmtFinType1'])\n\nfinTyp2_encoder = LabelEncoder()\nBsmt['BsmtFinType2'] = finTyp2_encoder.fit_transform(Bsmt['BsmtFinType2'])\n\nqual_encoder = LabelEncoder()\nBsmt['BsmtQual'] = qual_encoder.fit_transform(Bsmt['BsmtQual'])","12befe71":"Bsmt.head()","7dde7688":"Bsmt['BsmtScore']= Bsmt['BsmtQual']  * Bsmt['BsmtCond'] * Bsmt['TotalBsmtSF']\nall_data['BsmtScore']=Bsmt['BsmtScore']","264115c8":"Bsmt['BsmtFin'] = (Bsmt['BsmtFinSF1'] * Bsmt['BsmtFinType1']) + (Bsmt['BsmtFinSF2'] * Bsmt['BsmtFinType2'])\nall_data['BsmtFinScore']=Bsmt['BsmtFin']\nall_data['BsmtDNF']=(all_data['BsmtFinScore']==0)","7682a071":"lot=['LotFrontage', 'LotArea','LotConfig','LotShape']\nLot=all_data[lot]","d099bcf4":"\nLot.head()","74073dd2":"garage=['GarageArea','GarageCars','GarageCond','GarageFinish','GarageQual','GarageType','GarageYrBlt']\nGarage=all_data[garage]","2af8cf2d":"Garage.head()","1252de6b":"garcond_encoder = LabelEncoder()\nGarage['GarageCond'] = garcond_encoder.fit_transform(Garage['GarageCond'])\n\ngarfin_encoder = LabelEncoder()\nGarage['GarageFinish'] = garfin_encoder.fit_transform(Garage['GarageFinish'])\n\ngarqual_encoder = LabelEncoder()\nGarage['GarageQual'] = garqual_encoder.fit_transform(Garage['GarageQual'])\n\ngartyp_encoder = LabelEncoder()\nGarage['GarageType'] = gartyp_encoder.fit_transform(Garage['GarageType'])\n\n\n","05387059":"Garage.head()","9e388c7a":"Garage['GarageScore']=(Garage['GarageArea']) * (Garage['GarageCars']) * (Garage['GarageFinish'])*(Garage['GarageQual']) *(Garage['GarageType'])\nall_data['GarageScore']=Garage['GarageScore']","f75d0d68":"all_data.head()","0b99d14f":"non_numeric=all_data.select_dtypes(exclude=[np.number, bool])\nnon_numeric.head()","0d0262e5":"\ndef onehot(col_list):\n    global all_data\n    while len(col_list) !=0:\n        col=col_list.pop(0)\n        data_encoded=pd.get_dummies(all_data[col], prefix=col)\n        all_data=pd.merge(all_data, data_encoded, on='Id')\n        all_data=all_data.drop(columns=col)\n    print(all_data.shape)","741e6659":"onehot(list(non_numeric))","8a01367f":"def log_transform(col_list):\n    transformed_col=[]\n    while len(col_list)!=0:\n        col=col_list.pop(0)\n        if all_data[col].skew() > 0.5:\n            all_data[col]=np.log(all_data[col]+1)\n            transformed_col.append(col)\n        else:\n            pass\n    print(f\"{len(transformed_col)} features had been tranformed\")\n    print(all_data.shape)","7e37750f":"numeric=all_data.select_dtypes(include=np.number)\nlog_transform(list(numeric))","43cf10b2":"print(train.shape)\nprint(test.shape)","5046cdc5":"train=all_data[:len(train)]\ntest=all_data[len(train):]\n\n# re-Set the train & test data for ML","3940593c":"print(train.shape)\nprint(test.shape)\n\n# OK. I'm ready","df7eabb5":"# loading pakages for model. \nfrom sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn import linear_model, model_selection, ensemble, preprocessing\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor, GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.model_selection import GridSearchCV, KFold, cross_val_score\nfrom sklearn.neighbors import KNeighborsRegressor\nimport xgboost as xgb\n\n#Evaluation Metrics\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error, r2_score,mean_absolute_error","30547a17":"def rmse(predict, actual):\n    score =mean_squared_error(Ytrain, y_pred)**0.5\n    return score\nrmse_score = make_scorer(rmse)\nrmse_score","a9d50e08":"feature_names=list(all_data)\nXtrain=train[feature_names]\nXtest=test[feature_names]\nYtrain=saleprices['log_SalePrice']","983b97d2":"def score(model):\n    score = cross_val_score(model, Xtrain, Ytrain, cv=5, scoring=rmse_score).mean()\n    return score","20e3d1d7":"scores ={}","1d9146a7":"lr_model = LinearRegression(n_jobs=-1)\nlr_model.fit(Xtrain,Ytrain)\n\n#accuracies = cross_val_score(estimator=lr_model,\n                         #   X=Xtrain,\n                         #   y=Ytrain,\n                          #  cv=5,\n                          #  verbose=1)\n\ny_pred = lr_model.predict(Xtrain)\n\nprint('')\nprint('####### Linear Regression #######')\nmeanCV = score(lr_model)\nprint('Mean CV Score : %.4f' % meanCV)\n\n\nmse = mean_squared_error(Ytrain,y_pred)\nmae = mean_absolute_error(Ytrain, y_pred)\nrmse = mean_squared_error(Ytrain, y_pred)**0.5\nr2 = r2_score(Ytrain, y_pred)\nscores.update({'OLS':[meanCV,mse,mae,rmse,r2]})\n\nprint('')\nprint('MSE(RSS)    : %0.4f ' % mse)\nprint('MAE         : %0.4f ' % mae)\nprint('RMSE        : %0.4f ' % rmse)\nprint('R2          : %0.4f ' % r2)","29b54115":"model_lasso = Lasso(random_state=42,alpha=0.00035)\nlr_lasso = make_pipeline(RobustScaler(), model_lasso)\nlr_lasso.fit(Xtrain,Ytrain)\n\n\n\ny_pred = lr_lasso.predict(Xtrain)\n\nprint('')\nprint('####### Lasso Regression #######')\nmeanCV = score(lr_lasso)\nprint('Mean CV Score : %.4f' % meanCV)\n\n\nmse = mean_squared_error(Ytrain,y_pred)\nmae = mean_absolute_error(Ytrain, y_pred)\nrmse = mean_squared_error(Ytrain, y_pred)**0.5\nr2 = r2_score(Ytrain, y_pred)\nscores.update({'Lasso':[meanCV,mse,mae,rmse,r2]})\n\nprint('')\nprint('MSE(RSS)    : %0.4f ' % mse)\nprint('MAE         : %0.4f ' % mae)\nprint('RMSE        : %0.4f ' % rmse)\nprint('R2          : %0.4f ' % r2)","1ada53da":"lr_ridge = make_pipeline(RobustScaler(), Ridge(random_state=42,alpha=0.002))\nlr_ridge.fit(Xtrain,Ytrain)\n\n\n\ny_pred = lr_ridge.predict(Xtrain)\n\nprint('')\nprint('####### Ridge Regression #######')\nmeanCV = score(lr_ridge)\nprint('Mean CV Score : %.4f' % meanCV)\n\n\nmse = mean_squared_error(Ytrain,y_pred)\nmae = mean_absolute_error(Ytrain, y_pred)\nrmse = mean_squared_error(Ytrain, y_pred)**0.5\nr2 = r2_score(Ytrain, y_pred)\nscores.update({'Ridge':[meanCV,mse,mae,rmse,r2]})\n\nprint('')\nprint('MSE(RSS)    : %0.4f ' % mse)\nprint('MAE         : %0.4f ' % mae)\nprint('RMSE        : %0.4f ' % rmse)\nprint('R2          : %0.4f ' % r2)","5d155e47":"lr_elasticnet = make_pipeline(RobustScaler(),ElasticNet(alpha=0.02, l1_ratio=0.7,random_state=42))\nlr_elasticnet.fit(Xtrain,Ytrain)\n\n\n\ny_pred = lr_elasticnet.predict(Xtrain)\n\nprint('')\nprint('####### ElasticNet Regression #######')\nmeanCV = score(lr_elasticnet)\nprint('Mean CV Score : %.4f' % meanCV)\n\nmse = mean_squared_error(Ytrain,y_pred)\nmae = mean_absolute_error(Ytrain, y_pred)\nrmse = mean_squared_error(Ytrain, y_pred)**0.5\nr2 = r2_score(Ytrain, y_pred)\nscores.update({'ElasticNet':[meanCV,mse,mae,rmse,r2]})\n\nprint('')\nprint('MSE(RSS)    : %0.4f ' % mse)\nprint('MAE         : %0.4f ' % mae)\nprint('RMSE        : %0.4f ' % rmse)\nprint('R2          : %0.4f ' % r2)","60a96dd8":"knn = make_pipeline(RobustScaler(),KNeighborsRegressor())\nknn.fit(Xtrain,Ytrain)\n\n\n\ny_pred = knn.predict(Xtrain)\n\nprint('')\nprint('####### KNN Regression #######')\nmeanCV = score(knn)\nprint('Mean CV Score : %.4f' % meanCV)\n\n\nmse = mean_squared_error(Ytrain,y_pred)\nmae = mean_absolute_error(Ytrain, y_pred)\nrmse = mean_squared_error(Ytrain, y_pred)**0.5\nr2 = r2_score(Ytrain, y_pred)\nscores.update({'KNN':[meanCV,mse,mae,rmse,r2]})\n\nprint('')\nprint('MSE(RSS)    : %0.4f ' % mse)\nprint('MAE         : %0.4f ' % mae)\nprint('RMSE        : %0.4f ' % rmse)\nprint('R2          : %0.4f ' % r2)","e06c8317":"model_GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =42)\n\nmodel_GBoost.fit(Xtrain,Ytrain)\ny_pred = model_GBoost.predict(Xtrain)\n\nprint('')\nprint('####### GradientBoosting Regression #######')\nmeanCV = score(model_GBoost)\nprint('Mean CV Score : %.4f' % meanCV)\n\n\nmse = mean_squared_error(Ytrain,y_pred)\nmae = mean_absolute_error(Ytrain, y_pred)\nrmse = mean_squared_error(Ytrain, y_pred)**0.5\nr2 = r2_score(Ytrain, y_pred)\nscores.update({'GradientBoosting':[meanCV,mse,mae,rmse,r2]})\n\nprint('')\nprint('MSE(RSS)    : %0.4f ' % mse)\nprint('MAE         : %0.4f ' % mae)\nprint('RMSE        : %0.4f ' % rmse)\nprint('R2          : %0.4f ' % r2)","1f9d758a":"forest_reg = RandomForestRegressor(random_state=42)\nforest_reg.fit(Xtrain, Ytrain)\n\nforest_reg.fit(Xtrain,Ytrain)\ny_pred = forest_reg.predict(Xtrain)\n\nprint('')\nprint('####### RandomForest Regression #######')\nmeanCV = score(forest_reg)\nprint('Mean CV Score : %.4f' % meanCV)\n\n\nmse = mean_squared_error(Ytrain,y_pred)\nmae = mean_absolute_error(Ytrain, y_pred)\nrmse = mean_squared_error(Ytrain, y_pred)**0.5\nr2 = r2_score(Ytrain, y_pred)\nscores.update({'RandomForest':[meanCV,mse,mae,rmse,r2]})\n\nprint('')\nprint('MSE(RSS)    : %0.4f ' % mse)\nprint('MAE         : %0.4f ' % mae)\nprint('RMSE        : %0.4f ' % rmse)\nprint('R2          : %0.4f ' % r2)","f30256b9":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n  \n    {'n_estimators': [70,100], 'max_features': [150]},\n   \n    {'bootstrap': [True], 'n_estimators': [70,100], 'max_features': [150]},\n  ]\n\nforest_reg = RandomForestRegressor(random_state=42)\n# train across 5 folds, that's a total of (12+6)*5=90 rounds of training \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5,\n                           scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(Xtrain, Ytrain)\n\nprint('')\nprint('####### GridSearch RF Regression #######')\nmeanCV = score(grid_search)\nprint('Mean CV Score : %.4f' % meanCV)\n\n\nmse = mean_squared_error(Ytrain,y_pred)\nmae = mean_absolute_error(Ytrain, y_pred)\nrmse = mean_squared_error(Ytrain, y_pred)**0.5\nr2 = r2_score(Ytrain, y_pred)\nscores.update({'GridSearchRF':[meanCV,mse,mae,rmse,r2]})\n\nprint('')\nprint('MSE(RSS)    : %0.4f ' % mse)\nprint('MAE         : %0.4f ' % mae)\nprint('RMSE        : %0.4f ' % rmse)\nprint('R2          : %0.4f ' % r2)","489e98b5":"grid_search.best_estimator_","8a696bca":"scores_list =[]\nfor k,v in scores.items():\n    temp_lst =[]\n    temp_lst.append(k)\n    temp_lst.extend(v)\n    scores_list.append(temp_lst)","2e189443":"scores_df =pd.DataFrame(scores_list,columns=['Model','CV_Mean_Score','MSE(RSS)','MAE','RMSE','R2Squared'])","f7927cdc":"scores_df.sort_values(['CV_Mean_Score'])","1e2b8226":"_ =sns.scatterplot(x='Model',y='CV_Mean_Score',data=scores_df,style='Model')","ae37d3cd":"Lasso_Predictions=np.exp(lr_lasso.predict(Xtest))-1\n\nGBoost_Predictions=np.exp(model_GBoost.predict(Xtest))-1\n\nKNN_Predictions=np.exp(knn.predict(Xtest))-1\n\nGridSearch_Predictions = np.exp(grid_search.best_estimator_.predict(Xtest))-1","629e2b99":"submission=pd.read_csv('..\/input\/sample_submission.csv')","4a38a638":"submission['SalePrice'] = Lasso_Predictions\nsubmission.to_csv('Lasso.csv',index=False)\n\nsubmission['SalePrice'] = GBoost_Predictions\nsubmission.to_csv('GBoost.csv',index=False)\n\nsubmission['SalePrice'] = KNN_Predictions\nsubmission.to_csv('KNN.csv',index=False)\n\nsubmission['SalePrice'] = GridSearch_Predictions\nsubmission.to_csv('GidSearch.csv',index=False)","920323de":"coef_df = pd.DataFrame(list(zip(train.columns, model_lasso.coef_)),columns=['Feature','Coefficient'])\ncoef_df","fa7608bd":"LotFrontage: Linear feet of street connected to property\n\nLotArea: Lot size in square feet\n\nLotShape: General shape of property\n\n       Reg  Regular \n       IR1  Slightly irregular\n       IR2  Moderately Irregular\n       IR3  Irregular\n\nLotConfig: Lot configuration\n\n       Inside   Inside lot\n       Corner   Corner lot\n       CulDSac  Cul-de-sac\n       FR2  Frontage on 2 sides of property\n       FR3  Frontage on 3 sides of property\n","322a70af":"## Grid Search for finding best params for RandomForest","fb3e845d":"# Explore Data & Wrangle Data","5f63e367":"# Modeling","e4911e92":"\nBsmtQual: Evaluates the height of the basement\n\n       Ex\tExcellent (100+ inches)\t\n       Gd\tGood (90-99 inches)\n       TA\tTypical (80-89 inches)\n       Fa\tFair (70-79 inches)\n       Po\tPoor (<70 inches\n       NA\tNo Basement\n\t\t\nBsmtCond: Evaluates the general condition of the basement\n\n       Ex\tExcellent\n       Gd\tGood\n       TA\tTypical - slight dampness allowed\n       Fa\tFair - dampness or some cracking or settling\n       Po\tPoor - Severe cracking, settling, or wetness\n       NA\tNo Basement\n\t\nBsmtExposure: Refers to walkout or garden level walls\n\n       Gd\tGood Exposure\n       Av\tAverage Exposure (split levels or foyers typically score average or above)\t\n       Mn\tMimimum Exposure\n       No\tNo Exposure\n       NA\tNo Basement\n\t\nBsmtFinType1: Rating of basement finished area\n\n       GLQ\tGood Living Quarters\n       ALQ\tAverage Living Quarters\n       BLQ\tBelow Average Living Quarters\t\n       Rec\tAverage Rec Room\n       LwQ\tLow Quality\n       Unf\tUnfinshed\n       NA\tNo Basement\n\t\t\nBsmtFinSF1: Type 1 finished square feet\n\nBsmtFinType2: Rating of basement finished area (if multiple types)\n\n       GLQ\tGood Living Quarters\n       ALQ\tAverage Living Quarters\n       BLQ\tBelow Average Living Quarters\t\n       Rec\tAverage Rec Room\n       LwQ\tLow Quality\n       Unf\tUnfinshed\n       NA\tNo Basement\n\nBsmtFinSF2: Type 2 finished square feet\n\nBsmtUnfSF: Unfinished square feet of basement area\n\nTotalBsmtSF: Total square feet of basement area","54a15b51":"**The  BsmtFullBath ,FullBath, BsmtHalfBath can be combined for a TotalBath similar to TotalSF**","e3248f80":"## Coefficients of the Model","e1581dc4":"## Check the top 10 and Bottom 10 correlated and non-corelate features respectively","f36b2cfb":"## Ridge Regression","891fe741":"**Some Outliers after GarageArea of 1200**\n**Remove the outliers**","a4edea22":"## Analyze the highly correlated features","6cdf6928":"**Create a dictionary to keep track of scores for each model and compare later**","8fd487cf":"**By taking log of SalePrice the skew reduces to a larger extent**","0c880848":"## RandomForest Regressor","79996d35":"#### Impute Categorical data for missing values and relace by 'None'****","82a1a280":"**Current Winning model looks to be GradientBoosting  on cross validation data**","53029b44":"# Data Preprocessesing","a57c764f":"## KNN Regression","992a3319":"** The options in Miscellenous features explain if there is Shed or Second Garage or some TenC **","d0f477d3":"**Since Lasso is wininng in predictions here are the coefficients**","2db2c781":"### Impute the Data for missing values","e15e1798":"## ElasticNet Regression","e81c9a86":"**Note Even though a model wins on CV score it may not be best predictor. I could not validate data on test as true Y's are not available in kaggle. So the only way would be to predict on good models and try submitting scores for them**","9a36c583":"## Simple Linear Regression","480829f1":"## New Features","4b14c4f3":"### Analyze Garage Area ","c2751807":"### Analyze Numeric features for correlations with Target variable","9fafbfa5":"* **TotalBsmtSF - Total Basement Square Feet  **\n* **1stFlrSF - First Floor Square Feet **\n* **2ndFlrSF - Second Floor Square Feet **\n\n**All the above three feature define area of the house and we can easily combine these to form TotalSF - Total Area in square feet**\n","528080a9":"* ** 99% of Pool Quality Data is missing.In the case of PoolQC, the column refers to Pool Quality. Pool quality is NaN when PoolArea is 0, or there is no pool.**\n* ** Similar is case for Garage column **\n\n** But what are the 96% missing Miscelleanous features ?**\n\n** Let's find out **","89f68928":"### Analyze Overall Quality (OverallQual) - highest correlation with SalePrice","ee760136":"#### Impute the numerical features and replace with a value of zero","8984beaa":"## Lasso Regression","41fd9997":"### Group the similar featurtes  related to a House Feature and analyze","a94551ab":"**Looks much better now**","b9ae4bac":"**We had log transformed the Ytrain and hence it is essential to transform it back to original by taking an exponential of model predictions**","e83f8228":"**The regression plot clearly indicates that rices increase with increase in GrLivArea however there are some outliers  where for GrLivArea above 4000 the prices are below 20k**\n**Let's Remove the outliers**","042e5816":"**Let's find the percentage of missing values for the features**","79f14f68":"## Predictions","6f2f4a5e":"## GradientBoosting Regression","e42d2a35":"* **Before imputing the categorical values it is very important that we impute it on entire (Dev + Test) **\n* ** This is of outmost importance since some of the categories might be missing in the test data  which will create problems in OneHotEncoding later when we run models on test data **\n","bfb3c41d":"### Analyze the GrLivArea: Above grade (ground) living area square feet - Second highest correlation with SalePrice","5ee8540a":"### Merge train and test data","606dc957":"# Load Data","1294dc2d":"**LotFrontage seems like an important feature let's look at the relation with SalePrice**","16803a22":"## Transform the target variable","6e3f91b0":"**The median SalePrice shows and increasing trend with increase in quality rating**","329474ea":"**Even though OverallQual is a numeric feature but it has a rating from 1-10**\n\n**Let's check how the Overall Quality rating affects the median price of the house**","455f4c8d":"** Impute the LotFrontage with Median values**"}}