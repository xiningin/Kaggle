{"cell_type":{"0c518e5b":"code","8959c1f8":"code","341472d2":"code","e2089e18":"code","cce6ee00":"code","723f19c0":"code","0c6612eb":"code","e1946215":"code","b68adbc6":"code","4f529520":"code","cd4fe160":"code","a7575ff7":"code","7dd5ca20":"code","7ef6d953":"code","79eacbd0":"code","510c829e":"code","ef3da87b":"code","f4c80cdb":"code","11c98dd9":"code","7aa00765":"code","5f93c9b2":"code","057bef96":"code","e4fe5735":"code","12b382d2":"code","2a42ddbf":"code","93068d28":"code","ca5141a7":"code","acef2dc9":"code","a232467b":"code","2743d567":"code","e342deb5":"code","7a9d4a24":"code","87ff3ec5":"code","573e1f66":"code","7ed1f76d":"code","de3c63cc":"code","ef37ec8a":"code","f61828fc":"code","0d20d224":"code","8fbfc6ea":"code","90894f34":"code","c9a47b4e":"code","3f9c46b5":"code","2d7ca033":"code","ce4d6409":"code","8963632b":"code","6ff9719f":"markdown"},"source":{"0c518e5b":"!pip install dataprep","8959c1f8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nfrom collections import Counter\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom dataprep.eda import *\nfrom dataprep.eda import plot\nfrom dataprep.eda import plot_correlation\nfrom dataprep.eda import plot_missing\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.preprocessing.text import *\nfrom tensorflow.keras.preprocessing.sequence import *\nfrom tensorflow.keras.models import *\nimport tensorflow.keras.backend as k\nfrom tensorflow.keras.optimizers import *\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import *\nfrom keras.models import Sequential \n\n#For training model\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, GRU\nfrom tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\ngpu_devices=tf.config.experimental.list_physical_devices(\"GPU\")\nfor device in gpu_devices:\n    tf.config.experimental.set_memory_growth(device,True)\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\nfrom keras.layers import Dense\nfrom keras.layers import SimpleRNN\nfrom keras.layers import Dropout\nfrom nltk.corpus import *\nfrom nltk.stem import *\nimport string\nfrom sklearn.preprocessing import *\nfrom tqdm import tqdm","341472d2":"train=pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest=pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsubmission=pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","e2089e18":"train.head()","cce6ee00":"train.info()","723f19c0":"train.describe().T","0c6612eb":"# Generate the Profiling Report\n\nimport pandas_profiling as pp\nprofile = pp.ProfileReport(\n    train,html={\"style\": {\"full_width\": True}}, sort=\"None\"\n)","e1946215":"profile.to_widgets()","b68adbc6":"profile","4f529520":"plot(train)","cd4fe160":"plot_correlation(train)","a7575ff7":"plot(test)","7dd5ca20":"for col in train.columns:\n    print(f\"{col}: {len(train[col].unique())}\")","7ef6d953":"train['license']","79eacbd0":"train['license'].value_counts()","510c829e":"plt.figure(figsize=(35, 5))\nsns.countplot(data= train, x= 'license', saturation=0.2, color=\"r\")\nplt.title('Types of License')\nplt.show();","ef3da87b":"fig, ax = plt.subplots(1,2,figsize=(12,7))\nsns.histplot(train['target'], kde= True, ax=ax[0])\nsns.histplot(train['standard_error'], kde= True, ax=ax[1])\nax[0].set_title(\"Target Distribution\")\nax[1].set_title(\"Standard Error Distribution\")\nplt.show();","f4c80cdb":"print(train.target.describe())\nprint('_____________________________')\nprint(train.standard_error.describe())","11c98dd9":"# Extract all url's\nurl_list = train['url_legal'].dropna().apply(lambda x : re.findall('https?:\/\/([A-Za-z_0-9.-]+).*',x)[0])\nurl_list = [url for url in url_list]\nurl_list[:10]\n# count url's and sort them descending order \nurls_counts = Counter(url_list)\nurls_counts_sorted = sorted(urls_counts.items(), key=lambda pair: pair[1], reverse=True)\nurls_counts_df = pd.DataFrame(urls_counts_sorted, columns=['sites', 'counts'])\nurls_counts_df","7aa00765":"site = urls_counts_df['sites'].head(20)\ncount = urls_counts_df['counts'].head(20)\n \n# Figure Size\nfig, ax = plt.subplots(figsize =(16, 9))\n \n# Horizontal Bar Plot\nax.barh(site, count)\n \n# Remove axes splines\nfor s in ['top', 'bottom', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n \n\n# Remove x, y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n \n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad = 5)\nax.yaxis.set_tick_params(pad = 10)\n \n# Add x, y gridlines\nax.grid(b = True, color ='grey',\n        linestyle ='-.', linewidth = 0.5,\n        alpha = 0.2)\n \n# Show top values\nax.invert_yaxis()\n \n# Add annotation to bars\nfor i in ax.patches:\n    plt.text(i.get_width()+0.2, i.get_y()+0.5,\n             str(round((i.get_width()), 2)),\n             fontsize = 10, fontweight ='bold',\n             color ='grey')\n \n# Add Plot Title\nax.set_title('Unique Sites count',\n             loc ='left', )\n \n# Add Text watermark\nfig.text(0.9, 0.15, 'kritanjalijain', fontsize = 12,\n         color ='grey', ha ='right', va ='bottom',\n         alpha = 0.7)\n \n# Show Plot\nplt.show()","5f93c9b2":"print(\"First example from train dataset: \\n\")\nprint(train.excerpt[0])","057bef96":"# Top 2 excerpts with lowest target\n\nmin_targets = sorted(train['target'])[:2]\nfor min_target in min_targets:\n    print(\"Target:\", train[train['target'] == min_target].iloc[0,4])\n    print(train[train['target'] == min_target].iloc[0,3])\n    print(\"-\" * 100)\n","e4fe5735":"# Top 2 excerpts with highest target\n\nmax_targets = sorted(train['target'])[-2:]\nfor max_target in max_targets:\n    print(\"Target:\", train[train['target'] == max_target].iloc[0,4])\n    print(train[train['target'] == max_target].iloc[0,3])\n    print(\"-\" * 100)","12b382d2":"from nltk.stem import PorterStemmer\ndef excerpt_to_words(excerpt):\n    ''' Convert excerpt text into a sequence of words '''\n    \n    # convert to lowercase\n    text = excerpt.lower()\n    # remove non letters\n    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n    # tokenize\n    words = text.split()\n    # remove stopwords\n    words = [w for w in words if w not in stopwords.words(\"english\")]\n    # apply stemming\n    words = [PorterStemmer().stem(w) for w in words]\n    # return list\n    return words\n\nwords_list = excerpt_to_words(''.join(sents for sents in train['excerpt']))\ns=words_list[:10]\n\n\nwords_list_freq = Counter(words_list)\nwords_list_freq_sorted = sorted(words_list_freq.items(), key=lambda pair: pair[1], reverse=True)\n\nwords_list_freq_sorted_df = pd.DataFrame(words_list_freq_sorted, columns=['words', 'counts'])[:30]\nwords_list_freq_sorted_df.head() ","2a42ddbf":"print(\"\\nOriginal excerpt ->\", train['excerpt'][0])\nprint(\"\\nProcessed excerpt ->\", excerpt_to_words(train['excerpt'][0]))","93068d28":"X = list(map(excerpt_to_words, train['excerpt']))","ca5141a7":"word = words_list_freq_sorted_df['words'].head(20)\ncount = words_list_freq_sorted_df['counts'].head(20)\n \n# Figure Size\nfig, ax = plt.subplots(figsize =(16, 9))\n \n# Horizontal Bar Plot\nax.barh(word, count)\n \n# Remove axes splines\nfor s in ['top', 'bottom', 'left', 'right']:\n    ax.spines[s].set_visible(False)\n \n\n# Remove x, y Ticks\nax.xaxis.set_ticks_position('none')\nax.yaxis.set_ticks_position('none')\n \n# Add padding between axes and labels\nax.xaxis.set_tick_params(pad = 5)\nax.yaxis.set_tick_params(pad = 10)\n \n# Add x, y gridlines\nax.grid(b = True, color ='grey',\n        linestyle ='-.', linewidth = 0.5,\n        alpha = 0.2)\n \n# Show top values\nax.invert_yaxis()\n \n# Add annotation to bars\nfor i in ax.patches:\n    plt.text(i.get_width()+0.2, i.get_y()+0.5,\n             str(round((i.get_width()), 2)),\n             fontsize = 10, fontweight ='bold',\n             color ='grey')\n \n# Add Plot Title\nax.set_title('Top 20 frequent words and no. of times they occured',\n             loc ='left', )\n \n# Add Text watermark\nfig.text(0.9, 0.15, 'kritanjalijain', fontsize = 12,\n         color ='grey', ha ='right', va ='bottom',\n         alpha = 0.7)\n \n# Show Plot\nplt.show()","acef2dc9":"targets=np.array(train['target'])\nexcerpt_text=np.array(train['excerpt'])","a232467b":"targets","2743d567":"excerpt_text","e342deb5":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train.excerpt)  \nvocab_size = len(tokenizer.word_index) + 1 \nmax_length = 200","7a9d4a24":"sequences_train = tokenizer.texts_to_sequences(excerpt_text) \n#sequences_test = tokenizer.texts_to_sequences(test_data.excerpt) \n\nX_train = pad_sequences(sequences_train, maxlen=max_length, padding='post')\n#X_test = pad_sequences(sequences_test, maxlen=max_length, padding='post')\n\ny_train = train.target.values\n#y_test = test_data.target.values","87ff3ec5":"y_train","573e1f66":"from tqdm import tqdm\nembedding_vector = {}\nf = open('..\/input\/glove-version840b300d\/glove.840B.300d.txt')\nfor line in tqdm(f):\n    value = line.split(' ')\n    word = value[0]\n    coef = np.array(value[1:],dtype = 'float32')\n    embedding_vector[word] = coef","7ed1f76d":"embedding_matrix = np.zeros((vocab_size,300))\nfor word,i in tqdm(tokenizer.word_index.items()):\n    embedding_value = embedding_vector.get(word)\n    if embedding_value is not None:\n        embedding_matrix[i] = embedding_value","de3c63cc":"embedding_matrix.shape","ef37ec8a":" input_length =embedding_matrix.shape[1]","f61828fc":"embid_dim = 300\nlstm_out = 256\nmodel = keras.Sequential()\nmodel.add(Embedding(vocab_size, embid_dim, input_length =max_length, weights = [embedding_matrix] , trainable = False))\nmodel.add(Bidirectional(GRU(lstm_out, dropout=0.2)))\nmodel.add(Dense(128, activation = 'tanh'))\nmodel.add(Dense(128, activation = 'tanh'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(30, activation = 'tanh'))\nmodel.add(Dense(30, activation = 'tanh'))\nmodel.add(Dense(1, activation = 'linear'))\nmodel.summary()","0d20d224":"from tensorflow.keras.utils import plot_model\nfrom IPython.display import Image\nplot_model(model, to_file='convnet.png', show_shapes=True,show_layer_names=True)\nImage(filename='convnet.png')","8fbfc6ea":"# Callbacks\nlrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 3,verbose = 1,factor = 0.50, min_lr = 1e-7)\n\nmcp = ModelCheckpoint('model_RUN.h5',save_freq='epoch', verbose=1)\n\nes = EarlyStopping(verbose=1, patience=3)   \n\nmodel.compile(loss=tf.keras.losses.MeanSquaredError(),optimizer=Adam(learning_rate=1e-05))      ","90894f34":"%time\nhistory=model.fit(X_train,y_train,epochs=100,verbose=1,callbacks=[lrd,mcp,es])","c9a47b4e":"history.history.keys()","3f9c46b5":"import matplotlib.pyplot as plt\nloss = history.history['loss']\nepochs = range(1, len(loss) + 1)\nplt.figure(figsize=(10,5))\n\nplt.plot(epochs, loss, 's', color='C3', label='Training loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.grid()\nplt.show()","2d7ca033":"excerpt_test=np.array(test['excerpt'])\ntest.head()","ce4d6409":"sequences_test = tokenizer.texts_to_sequences(excerpt_test) \n\nX_test = pad_sequences(sequences_test, maxlen=max_length, padding='post')\n\nprediction_testdata = model.predict(X_test)\n\ntest[\"target\"]= prediction_testdata\ntest.head()","8963632b":"submission[\"target\"]=prediction_testdata\nsubmission.to_csv('submission.csv',index=False)","6ff9719f":"\n\n<h1 style='background-color:Gray; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;' > CommonLit Readability Prize With EDA+GRU+glove.840B.300d <\/h1>\n\n## What is GloVe?\n##### GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n\n\n\n<img src=\"https:\/\/dz2cdn2.dzone.com\/storage\/article-thumb\/11629773-thumb.jpg\" width=\"800px\">\n\n\n\n\n<h1 style='background-color:Gray; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;' > Gated Recurrent Units  <\/h1>\n\n## What is GRU? \n##### The GRU is the newer generation of Recurrent Neural networks and is pretty similar to an LSTM. GRU's got rid of the cell state and used the hidden state to transfer information. It also only has two gates, a reset gate and update gate\n\n<img src=\"https:\/\/www.researchgate.net\/publication\/334385520\/figure\/fig1\/AS:779310663229447@1562813549841\/Structure-of-a-GRU-cell.ppm\" width=\"800px\">\n\n\n## Data Description\n\n### Files\n* train.csv - the training set\n* test.csv - the test set\n* sample_submission.csv - a sample submission file in the correct format\n\n### Columns\n* id - unique ID for excerpt\n* url_legal - URL of source - this is blank in the test set.\n* license - license of source material - this is blank in the test set.\n* excerpt - text to predict reading ease of\n* target - reading ease\n* standard_error - measure of spread of scores among multiple raters for each excerpt. Not included for test data.\n\n\n### Dataset link \n\n\n#### [Here](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize)"}}