{"cell_type":{"301ddabf":"code","dff67576":"code","9663cb4e":"code","1172266c":"code","2e2efc08":"code","89017750":"code","4477a64d":"code","a659b5a9":"code","8f553704":"code","5287b567":"code","24e8b4e8":"code","80047c53":"code","65f73ad6":"code","3d574d72":"code","da1b6d9c":"code","afae773a":"code","64f002f6":"code","db80d115":"code","39a79780":"code","5e537155":"code","87a799cb":"code","ecd2031a":"code","0380a02a":"code","da15fb0a":"code","8c50b579":"code","35518a4f":"code","f347d583":"code","6244882c":"code","653dce43":"code","d10b7413":"code","b5c1ca2e":"markdown","58524de8":"markdown","ee841070":"markdown","8ca484a8":"markdown","4517b807":"markdown","2839dc4d":"markdown","4c508578":"markdown","3aec7825":"markdown","21d5cbbc":"markdown","6289b0f2":"markdown"},"source":{"301ddabf":"# Imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.compose import ColumnTransformer","dff67576":"train = pd.read_csv('..\/input\/train.csv') # Uncomment for Kaggle Notebook\n# train = pd.read_csv('train.csv')\ntrain.head()","9663cb4e":"train.info()","1172266c":"train.hist(figsize=(15, 20))","2e2efc08":"f, ax = plt.subplots(figsize=(12, 8))\nsns.boxplot(x=train['SalePrice'], y=train['Neighborhood'], width=0.5)","89017750":"def impute_and_scale_values(df_in):\n    numerical_cols = []\n    df = df_in.copy()\n\n    for col in df:\n        if (df[col].dtype == 'int64') or (df[col].dtype == 'float64'):\n            df[col].fillna(0, inplace=True) # fill missing numerical values with 0\n            numerical_cols.append(col)\n        else:\n            df[col].fillna('None', inplace=True) # fill missing attribute values with 'None'\n            \n    if 'Id' in numerical_cols:\n        numerical_cols.remove('Id')\n    \n    for col in numerical_cols:\n        scaler = StandardScaler()\n        df[col] = scaler.fit_transform(df[col].values.reshape(-1,1))\n    \n    return df","4477a64d":"train_imputed = impute_and_scale_values(train)","a659b5a9":"def encode_data(df):\n    # Label Encoding\n    label_encoding_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive']\n    # One-hot Encoding\n    one_hot_encoding_cols = ['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', 'CentralAir', 'Electrical', 'GarageType', 'Fence', 'SaleType', 'SaleCondition']\n    # String Encoding\n    string_encoding_cols = ['MSSubClass', 'MoSold', 'YrSold']\n\n    # Convert String Encoding cols to type string:\n    for col in string_encoding_cols:\n        if col in df:\n            df[col] = df[col].astype(str)\n\n    # Run label encoding\n    for col in label_encoding_cols:\n        if col in df:\n            lbl = LabelEncoder() \n            lbl.fit(list(df[col].values)) \n            df[col] = lbl.transform(list(df[col].values))\n    \n    # One-hot encode the one-hot-encoding and string-encoding cols\n    df_encoded = pd.get_dummies(df)\n    \n    return df_encoded","8f553704":"train_encoded = encode_data(train_imputed)","5287b567":"train_encoded.info()","24e8b4e8":"corr_mat = train_encoded.corr()\nk = 20 # number of features to select\nfeatures_selected = corr_mat.nlargest(20, 'SalePrice')['SalePrice'].index","80047c53":"sns.set(font_scale=1.25)\nplt.subplots(figsize=(20,15))\nsns.heatmap(train_encoded[features_selected].corr(), cbar=True, annot=True, square=False, fmt='.2f', annot_kws={'size': 15})\nplt.show()","65f73ad6":"def select_features(df):\n    df_selected = df.copy()\n    f_s = features_selected.drop('SalePrice')\n    for col in f_s:\n        if col not in df_selected:\n            df_selected[col] = np.zeros(len(df_selected))\n            print('Adding column: ', col)\n    return df_selected[f_s]","3d574d72":"train_selected = select_features(train_encoded)","da1b6d9c":"from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor","afae773a":"#X = train_selected.drop(['SalePrice'], axis=1)\ny = train['SalePrice']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","64f002f6":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","db80d115":"# Test out different models\nmodels = [RandomForestRegressor(), svm.SVR(), LinearRegression(), DecisionTreeRegressor()]\n\nfor clf in models:\n    scores = -1*cross_val_score(clf, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')\n    print(\"Model: {}, \\n Scores: {}\".format(clf, scores.mean()))","39a79780":"# This will take a while to run\nsearch_params = {\n    'n_estimators': range(20, 160, 20),\n    'criterion': ['mse', 'mae'],\n    'max_depth': [2, 3, 5, None],\n    'min_samples_split': range(2, 5, 1),\n    'min_samples_leaf': range(2, 5, 1),\n}\nsearch = GridSearchCV(RandomForestRegressor(), search_params, cv=3)\nsearch.fit(X_train, y_train)","5e537155":"print(search.best_params_)\nprint(search.best_score_)","87a799cb":"clf = RandomForestRegressor(criterion='mae', max_depth=None, min_samples_leaf=3, min_samples_split=3, n_estimators=80)","ecd2031a":"# Data pipeline\ndef data_pipeline(df):\n    df_imputed = impute_and_scale_values(df)\n    df_encoded = encode_data(df_imputed)\n    df_selected = select_features(df_encoded)\n    return df_selected","0380a02a":"test = pd.read_csv('..\/input\/test.csv') # import test data - uncomment for Kaggle\n#test = pd.read_csv('test.csv') # import test data local\n#train = pd.read_csv('train.csv')","da15fb0a":"X = train.drop(labels=['SalePrice', 'Id'], axis=1)\ny = train['SalePrice']\nX_train_prepped = data_pipeline(X)\nX_train, X_test, y_train, y_test = train_test_split(X_train_prepped, y)","8c50b579":"X_final = data_pipeline(test)","35518a4f":"clf.fit(X_train, y_train)","f347d583":"clf_score = cross_val_score(clf, X_train, y_train, cv=5, scoring='r2')\nprint(\"Scores: {}, ScoresAve: {}\".format(clf_score, clf_score.mean()))","6244882c":"pred_final = pd.Series(clf.predict(X_final), name=\"SalePrice\").astype(float)","653dce43":"results = pd.concat([test['Id'], pred_final], axis=1)","d10b7413":"results.to_csv(\"housing_prediction.csv\", index=False)","b5c1ca2e":"### We'll need to do some more data prepation to correctly represent each feature\n\n42 Categorical Features, 35 Numerical\n\n**1. Label Encoding:** \nSome of the categorical features are relational and need to be label encoded (for example, the external quality ('ExterQual') has values of ['Ex', 'Gd', 'TA', 'Fa', and 'Po']. These should be converted as 1-5.).\n\n**2. One-hot Encoding:** \nOther categorical features are non-relational and need to be one-hot encoded (for example, lot configuration ('LotConfig'), which has values of ['Inside', 'Corner', 'CulDSac', 'FR2', and 'FR3'] and cannot be easily ranked in terms of preference.).\n\n**3. String Encoding:** \nLastly, there are some numeric features that should not be treated as numeric (for example 'MSSubClass', which uses numeric values to differentiate between different types of dwellings. A 30 represents a 1-story that was built in 1945 or earlier, which is not necessarily worse than a 120 1-story Planned Unit Development). These need to be converted to strings and then one-hot encoded.","58524de8":"## 2.0 Feature Selection\n\nCheck correlation between each feature and the output ('SalePrice'), and select the top 20 features\n\n","ee841070":"## 6.0 Submission Prep","8ca484a8":"## 1.0 Data Exploration","4517b807":"## 4.0 Tuning","2839dc4d":"Best params were found to be:\n\n* criterion: 'mae'\n* max_depth: None\n* min_samples_leaf: 3\n* min_samples_split: 3\n* n_estimators: 80","4c508578":"## 5.0 Data Pipeline\n\nLet's put all of this together into a model pipeline.","3aec7825":"# Housing Prices Kernel - Location, Location, Location!\n\n### It has been said that there are three things that determine the price of a house: location, location, and location. This kernel \"Ames\" to reinforce or refute that claim (pun intended). \n\nAnecdotally, there is validity to this claim. A 2,000 sq ft house in the middle of Oklahoma will sell for \\$ 130,000, but put that same house in Malibu, California and it will sell for \\$ 2.2 million. Likewise, the proximity to a major street, or being up on a hill, or being lakefront all have influence on the price of a house.\n\n**This housing dataset is from Ames, Iowa**. Ames is in the middle of Iowa. Points of interest in Ames:\n* **Ames is home to Iowa State University,** which has an undergraduate enrollment of around 30,000 students. Proximity to the college campus will likely influence housing prices, as will the overall rentability of the house.\n* The estimated population of Ames in 2010 was 58,965, which includes ISU students. Therefore, **about half the population of Ames is college students**, and about half of the properties will be bought and sold for renting, as opposed to permanent residence.\n* There are two golf courses, Coldwater Golf Links and Veenker Memorial Golf Course. The Grove Apartments are the only residential properties adjacent to Coldwater, and Veenker has no residential properties adjacent.\n* Ames overall is flat, with no major lakes.","21d5cbbc":"Highest performing model was found to be RandomForestRegressor (lowest mean absolute error)","6289b0f2":"## 3.0 Modelling\n\nNow with our dimensionality reduced, let's try out some models"}}