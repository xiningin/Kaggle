{"cell_type":{"77fc08b1":"code","1f83ec69":"code","259096a5":"code","9e600dff":"code","e7bfd49c":"code","5ee0bfdd":"code","017f9bd8":"code","556e4331":"code","fafdf591":"code","5e86fb55":"code","4f155746":"code","e4ee2db7":"code","0c9a9af0":"code","0ae53fc6":"code","702a97cd":"code","c659f319":"code","a2ce10c5":"code","c4e12ae2":"code","fd577910":"code","dc1d076a":"code","80aa53ce":"code","ba68113e":"code","2ae56a2c":"code","0969f5f9":"code","9a20dfe8":"markdown","71a34183":"markdown","f1c657cc":"markdown","5593be6e":"markdown","05bc5734":"markdown","5b7109d6":"markdown","396145f7":"markdown","7d7c8c65":"markdown","7c11dd5d":"markdown","b2e0d50e":"markdown","119fc09b":"markdown","5f370ff4":"markdown","aa36f9e5":"markdown","34d65749":"markdown","49fa4226":"markdown","0b9c698b":"markdown","967069b9":"markdown","7808d224":"markdown","88d0812e":"markdown","fd0ddc56":"markdown","c3d65fd5":"markdown","484fb744":"markdown","daa0cf78":"markdown","5f35eaaa":"markdown","87a041b6":"markdown","0778be25":"markdown","f209cfa0":"markdown","69a94161":"markdown","942a36d4":"markdown","17a3b547":"markdown"},"source":{"77fc08b1":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns","1f83ec69":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import linear_model\nfrom sklearn import tree\nfrom sklearn.linear_model import LinearRegression","259096a5":"house_train=pd.read_csv('..\/input\/atividade-regressao-PMR3508\/train.csv', na_values='?')\nhouse_train.set_index('Id',inplace=True)\n\nhouse_test=pd.read_csv('..\/input\/atividade-regressao-PMR3508\/test.csv', na_values='?')\nhouse_test.set_index('Id',inplace=True)\n\nprint(house_train.shape)\nprint(house_test.shape)","9e600dff":"house_train.sample(5)","e7bfd49c":"house_test.sample(5)","5ee0bfdd":"house_train.isnull().sum().sort_values(ascending = False)","017f9bd8":"house_test.isnull().sum().sort_values(ascending = False)","556e4331":"house_train.info()","fafdf591":"house_train.describe()","5e86fb55":"house_train[['median_age','total_rooms','total_bedrooms','population','households','median_income','median_house_value']].hist(bins=50, figsize=(8,8), histtype='bar')\nplt.show()","4f155746":"sns.pairplot(data=house_train, x_vars=['median_age','total_rooms','total_bedrooms','population','households','median_income'], y_vars= 'median_house_value')","e4ee2db7":"house_train.plot(kind = 'scatter', x = 'longitude', y = 'latitude', figsize = (12,12), c = 'median_house_value', cmap = plt.get_cmap('jet'), colorbar = True)","0c9a9af0":"house_test.info()","0ae53fc6":"house_test.describe()","702a97cd":"x = house_train[['median_age','total_rooms','total_bedrooms','population','households','median_income']]\ny = house_train['median_house_value']\n\nSEED = 158020\nnp.random.seed(SEED)\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25)\n  \nscaler = StandardScaler()\nscaler.fit(xtrain)\nxtrain = scaler.transform(xtrain)\nxtest = scaler.transform(xtest)\n\nmodel = LinearRegression()\nmodel.fit(xtrain,ytrain)\n\ncross_val_score(model, xtrain, ytrain, cv=10).mean()","c659f319":"x = house_train[['median_age','total_rooms','total_bedrooms','population','households','median_income']]\ny = house_train['median_house_value']\n\nSEED = 158020\nnp.random.seed(SEED)\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25)\n  \nscaler = StandardScaler()\nscaler.fit(xtrain)\n\nxtrain = scaler.transform(xtrain)\nxtest = scaler.transform(xtest)\n\nmodel = tree.DecisionTreeClassifier()\nmodel.fit(xtrain,ytrain)\n\ncross_val_score(model, xtrain, ytrain, cv=10).mean()","a2ce10c5":"x = house_train[['median_age','total_rooms','total_bedrooms','population','households','median_income']]\ny = house_train['median_house_value']\n\nSEED = 158020\nnp.random.seed(SEED)\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25)\n  \nscaler = StandardScaler()\nscaler.fit(xtrain)\nxtrain = scaler.transform(xtrain)\nxtest = scaler.transform(xtest)\n\nmodel = linear_model.BayesianRidge()\nmodel.fit(xtrain,ytrain)\n\ncross_val_score(model, xtrain, ytrain, cv=10).mean()","c4e12ae2":"x = house_train[['median_age','total_rooms','total_bedrooms','population','households','median_income']]\ny = house_train['median_house_value']\n\nSEED = 158020\nnp.random.seed(SEED)\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25)\n  \nscaler = StandardScaler()\nscaler.fit(xtrain)\nxtrain = scaler.transform(xtrain)\nxtest = scaler.transform(xtest)\n\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(xtrain,ytrain)\ncross_val_score(knn, xtrain, ytrain, cv=10).mean()","fd577910":"x = house_train[['median_age','total_rooms','total_bedrooms','population','households','median_income']]\ny = house_train['median_house_value']\n\nSEED = 158020\nnp.random.seed(SEED)\nxtrain = x\nytrain = y\n\nscaler = StandardScaler()\nscaler.fit(xtrain)\nxtrain = scaler.transform(xtrain)\n\nmodel = LinearRegression()\nmodel.fit(xtrain,ytrain)","dc1d076a":"test = house_test[['median_age','total_rooms','total_bedrooms','population','households','median_income']]\nscaler = StandardScaler()\nscaler.fit(test)\ntest = scaler.transform(test)\n\nlen(house_test)","80aa53ce":"predict = model.predict(test)\nlen(predict)","ba68113e":"predict","2ae56a2c":"submission = pd.DataFrame()\nsubmission[0] = house_test.index\nsubmission[1] = predict\nsubmission.columns = ['Id','median_house_value']\nsubmission['median_house_value'].value_counts()","0969f5f9":"submission.to_csv('my_submission.csv',index = False)","9a20dfe8":"These are the libraries that will be used for the preprocessing and model creating part of this project","71a34183":"And finally, we submit our newly created DataFrame as \"submission.csv\"","f1c657cc":"The difference in the column number, visualized in the previous paragrah, can now be explained. The \"house_test\" dataframe is missing it's \"missing_house_value\" column, meaning that this we'll be what our model will predict.","5593be6e":"## 3. Treating missing data\nIt's pretty common to have a dataset with missing data, that's why we'll analyse this facet of our dataframes.","05bc5734":"## 4. Data Analysis\nAs introduced in the chapter 2 of this notebook, before we create any machine learning model, we first need to understand how each variable impact it's label. With that in mind, we are going to use \"matplotlib\" and \"seaborn\" as a way to achieve this goal.","5b7109d6":"These scatter plots help us understand the relations of each variable with the label.","396145f7":"These are the libraries that we will be using for the data analysis section of the notebook","7d7c8c65":"# PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es\nAuthor: Gabriel Pinheiro\n\nDate:29\/10\/2020","7c11dd5d":"### 4.2 Testing data\nNow, let's analyse our testing data","b2e0d50e":"With the \".info()\" function, we can see that the training data is filled with \"floats\" and \"ints\" (Only numbers).","119fc09b":"It also seems that testing dataset is free of missing values. This means that we can now advance to the data analysis chapter.","5f370ff4":"This is what our model predicted for our testing data. \n\nNow let's create a new DataFrame to populate with our new predictions\n","aa36f9e5":"## 5. Prediction\nFinally, we can focus on creating our machine leaning models to predict the median house value.","34d65749":"Taking a look at the histograms for our columns, we can see that most of them have a high density region. The \"median_age\" column is one that don't really follow the previous statement, given it's flat shape.","49fa4226":"It seems that our training dataframe has no missing values, let's take a look at the testing dataframe.","0b9c698b":"## 2. Data import\nIn this chapter, we'll be taking a look at our datasets. It was provided a training and a testing data with the respective names of: \"train_csv\" and \"test.csv\".\nWith this, we'll try to understand how the each variable impacts it's label.","967069b9":"For each model created, we will be normalizing them with a \"StandardScaler\".","7808d224":"## 6. Submition\nThe last chapter of this notebook is aimed to the submition of our predictions created by the linear regression model.","88d0812e":"With the \".describe()\" function, we can see how each of the variables fluctuates, giving us a better understanding of their nature.","fd0ddc56":"As we can see by the last plot, longitude and latitude seem to be a great indicative of what's the medium value of a house in california.\nLet's try to correlate this with a map of the state.\n![mapa-california.jpg](attachment:mapa-california.jpg)\nIt seems that being near a beach and\/or being in a big city, really increase the value of the house. So we can stipulate that a house in Los Angeles near Venice beach is more expansive than a house in Bakersfield.","c3d65fd5":"### 5.4 KNN","484fb744":"### 5.1 Linear Regression","daa0cf78":"### 5.2 Decision Tree","5f35eaaa":"## 1. Importing Libraries","87a041b6":"### 5.3 Bayesian regression","0778be25":"By these tests we can see that a linear regression is a much better model for this problem if compared to KNN and a Decision tree.","f209cfa0":"In this notebook, we'll be taking a look at the California Housing dataset. With that, we are going to create a prediction model based on Linear Regression. We'll also compare our base model with other machine learning technics (KNN, Decision Tree, ...).","69a94161":"### 4.1 Training data\nLet's start our analysis with the training data.","942a36d4":"First of all, we need to import these datasets and transform them into dataframes with pandas. It's interesting to take a look at the shape of both datasets. The \"house_train\" dataframe (\"train.csv\") has both more columns and more lines if compared to the \"house_test\" dataframe (\"test.csv\").","17a3b547":"By the \".info()\" and \".describe()\" functions we can see that the nature of the testing data is really similar to the training data, meaning that we can accurately create a prediction model for the testing data based on the training data."}}