{"cell_type":{"e613a4f8":"code","3b1a70d4":"code","434f5cef":"code","e7b7adf9":"code","d5a7ed2d":"code","bb1d9e9b":"code","0bb6420a":"code","22b0f57e":"code","d944a7ea":"code","25827744":"code","6a8815ef":"code","4668b776":"code","9cc70de0":"code","42c68b45":"code","e2de439a":"code","14b7b985":"code","1f619f71":"code","50025307":"code","8753e652":"code","5572b10f":"code","a79660fe":"code","44787bfa":"code","f87e379a":"code","0e8a7714":"code","7660042f":"code","a6b3f58b":"markdown","04e216a0":"markdown","58d66d66":"markdown","b04539e1":"markdown","ef33e74d":"markdown","f032f603":"markdown","a7adfec9":"markdown","7b1e33e9":"markdown","e11c63da":"markdown","e5c0a22b":"markdown","ec4e6c60":"markdown","e13a0925":"markdown"},"source":{"e613a4f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score,plot_confusion_matrix,classification_report, matthews_corrcoef\nfrom sklearn.metrics import recall_score,precision_score,f1_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","3b1a70d4":"data = pd.read_csv(\"\/kaggle\/input\/fetal-health-classification\/fetal_health.csv\")","434f5cef":"print(data.columns)\ndata.head()","e7b7adf9":"for col in data.columns:\n    print(col, data[col].nunique())","d5a7ed2d":"# these three have the least unique values -- can be considered as categorical\nprint(data['fetal_health'].unique())\nprint(data['histogram_tendency'].unique())\nprint(data['severe_decelerations'].unique())","bb1d9e9b":"data.describe()","0bb6420a":"#checking for null values\ndata.info()","22b0f57e":"#checking for class Imbalance\nsns.histplot(data['fetal_health'])","d944a7ea":"data[\"fetal_health\"].value_counts()","25827744":"total = data[\"fetal_health\"].sum()\nnormal = total - 471\nsuspect = total - 1831\npathological = total - 1950\n\npie_fetal_health = plt.pie([normal, suspect, pathological], labels=[\"Normal\", \"Suspect\", \"Pathological\"], colors = [\"#5F9EA0\", \"#B0E0E6\", \"#ADD8E6\"],explode=[0.01,0.01,0.01], autopct=\"%1.0f%%\")\nplt.title('Pie chart of Fetal Heath', fontsize = 15)","6a8815ef":"plt.figure(figsize=(25,25))\ni=1\nfor col in data.columns:\n    plt.subplot(6,4,i)\n    sns.boxplot(x = 'fetal_health', y = col, data = data)\n    plt.tight_layout()\n    plt.title(col,fontsize=18)\n    i+=1","4668b776":"plt.figure(figsize=(15,15))\nsns.heatmap(data.corr(), square=\"True\", annot=True, cmap= \"coolwarm\")\nplt.show()","9cc70de0":"X, y = data.drop(['fetal_health'], axis=1), data['fetal_health']","42c68b45":"## Checking for LDA variations to see the separability\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis(n_components=2)\n\nlda_data_mod = lda.fit_transform(X,y)\nlda_data = pd.DataFrame(data=lda_data_mod, columns=['C1', 'C2'])\n\nlda_data = pd.concat([lda_data, y], axis=1)\nsns.lmplot(x='C1', y='C2', data=lda_data, hue='fetal_health', fit_reg=False)\nlda.explained_variance_ratio_","e2de439a":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","14b7b985":"from sklearn.svm import SVC\nsvc = SVC(gamma='scale')\nsvc.fit(X_train, y_train)\nsvc_score = svc.score(X_test, y_test)\nprint('accuracy is: ', svc_score*100)\ny_pred = svc.predict(X_test)\n\nprint(\"Accuracy score: {}\".format(accuracy_score(y_test,y_pred)))\nprint(\"Classification report:\\n {}\".format(classification_report(y_test,y_pred)))\nprint(\"Matthew Correlation Coefficient Score: {}\".format(matthews_corrcoef(y_test,y_pred)))\nplot_confusion_matrix(svc, X_test, y_test, cmap='Blues')","1f619f71":"from sklearn.ensemble import RandomForestClassifier\ntrees = [140,150,160,180]\nfor i in trees:\n    clf = RandomForestClassifier(n_estimators=i, criterion=\"gini\", max_depth= 5, random_state=0)\n    clf.fit(X_train, y_train)\n    preds = clf.predict(X_test)\n    acc_sc = accuracy_score(y_test, preds, normalize=True)\n    f1_sc = f1_score(y_test, preds, labels=[1,2,3], pos_label=1, average=None, zero_division='warn')\n    print(\"accuracy \", acc_sc)\n    print(\"f1_score \", f1_sc)","50025307":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nparameters = { \n    'n_estimators': [140,150,160,180],\n    'max_features': ['auto'],\n    'max_depth' : [4,6,8],\n    'criterion' :['gini', 'entropy'],\n    'n_jobs':[-1,None]\n}\n\n#Fitting the trainingset to find parameters with best accuracy\n\nCV_rfc = GridSearchCV(estimator=RandomForestClassifier(), param_grid=parameters, cv= 5)\nCV_rfc.fit(X_train, y_train)\n\nprint('Best params are: ', CV_rfc.best_params_)\n\nRF_model = RandomForestClassifier(**CV_rfc.best_params_)\nRF_model.fit(X_train, y_train)\n#Testing the Model on test set\npredictions=RF_model.predict(X_test)\n\nprint(\"Accuracy score: {}\".format(accuracy_score(y_test,predictions)))\nprint(\"Classification report:\\n {}\".format(classification_report(y_test,predictions)))\nprint(\"Matthew Correlation Coefficient Score: {}\".format(matthews_corrcoef(y_test,predictions)))\nplot_confusion_matrix(CV_rfc, X_test, y_test, cmap='Blues')","8753e652":"from sklearn.neighbors import KNeighborsClassifier\n\nneighbors = [1,3,5,7,9]\n\nfor i in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train, y_train)\n    \n    predsk = knn.predict(X_test)\n    \n    acc_sc_knn = accuracy_score(y_test, predsk, normalize=True)\n    f1_sc = f1_score(y_test, predsk, labels=[1,2,3], pos_label=1, average=None, zero_division='warn')\n    print(\"accuracy \", acc_sc)\n    print(\"f1_score \", f1_sc)","5572b10f":"# in xbg the classes must start from 0\ny_ = y[:]-1\nX1_train, X1_test, y1_train, y1_test = train_test_split(X, y_, test_size=0.25, random_state=42)\n\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(learning_rate = 0.1, max_depth = 5, n_estimators = 275, use_label_encoder=False, verbosity=0)\nxgb.fit(X1_train, y1_train)\ny1_pred = xgb.predict(X1_test)\n\nprint(\"Accuracy score: {}\".format(accuracy_score(y1_test,y1_pred)))\nprint(\"Classification report:\\n {}\".format(classification_report(y1_test,y1_pred)))\nprint(\"Matthew Correlation Coefficient Score: {}\".format(matthews_corrcoef(y1_test,y1_pred)))\nplot_confusion_matrix(xgb, X1_test, y1_test, cmap='Blues')","a79660fe":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier(subsample=0.9, max_features=0.7, n_estimators=150, random_state=42)\ngbc.fit(X_train, y_train)\nstochastic_boost_score = gbc.score(X_test, y_test)\nprint('accuracy is: ', stochastic_boost_score*100)\ny_pred = gbc.predict(X_test)\n\nprint(\"Accuracy score: {}\".format(accuracy_score(y_test,y_pred)))\nprint(\"Classification report:\\n {}\".format(classification_report(y_test,y_pred)))\nprint(\"Matthew Correlation Coefficient Score: {}\".format(matthews_corrcoef(y_test,y_pred)))\nplot_confusion_matrix(gbc, X_test, y_test, cmap='Blues')","44787bfa":"# converting to a binary class row\ndef convertBinary(row):\n    if row['fetal_health'] == 1:\n        return 1\n    else:\n        return 2\ndata['binary_y'] = data.apply(convertBinary, axis=1)\ndata.head()","f87e379a":"# splitting data for the sub cases\nXb, yb = data.drop(['binary_y'], axis=1), data['binary_y']\nXb_train, Xb_test, yb_train, yb_test = train_test_split(Xb, yb, test_size=0.25, random_state=42)\n\ndata2 = pd.concat([Xb_train, yb_train], axis=1)\ndata2 = data2[data2['binary_y'] == 2]\n\ndata3 = pd.concat([Xb_test, yb_test], axis=1)\ndata3 = data3[data3['binary_y'] == 2]\n\nXbb_train = data2.drop(['fetal_health', 'binary_y'], axis=1)\nybb_train = data2.fetal_health\n\nXbb_test = data3.drop(['fetal_health', 'binary_y'], axis=1)\nybb_test = data3.fetal_health\n\nXb_train = Xb_train.drop(['fetal_health'], axis=1)\nXb_test = Xb_test.drop(['fetal_health'], axis=1)","0e8a7714":"gbc2 = GradientBoostingClassifier(loss='deviance',subsample=0.9, max_features=0.7, n_estimators=150, learning_rate=0.4, max_depth=4, random_state=42)\ngbc2.fit(Xb_train, yb_train)\nstochastic_boost_score2 = gbc2.score(Xb_test, yb_test)\n# print('accuracy is: ', stochastic_boost_score2*100)\nyb_pred = gbc2.predict(Xb_test)\n\nprint(\"Accuracy score: {}\".format(accuracy_score(yb_test,yb_pred)))\nprint(\"Classification report:\\n {}\".format(classification_report(yb_test,yb_pred)))\nprint(\"Matthew Correlation Coefficient Score: {}\".format(matthews_corrcoef(yb_test,yb_pred)))\nplot_confusion_matrix(gbc2, Xb_test, yb_test, cmap='Blues')","7660042f":"gbc3 = GradientBoostingClassifier(loss='deviance',subsample=0.9, max_features='auto', n_estimators=200, learning_rate=0.2, max_depth=4, random_state=42)\ngbc3.fit(Xbb_train, ybb_train)\nstochastic_boost_score3 = gbc3.score(Xbb_test, ybb_test)\nprint('accuracy is: ', stochastic_boost_score3*100)\nybb_pred = gbc3.predict(Xbb_test)\n\nprint(\"Accuracy score: {}\".format(accuracy_score(ybb_test,ybb_pred)))\nprint(\"Classification report:\\n {}\".format(classification_report(ybb_test,ybb_pred)))\nprint(\"Matthew Correlation Coefficient Score: {}\".format(matthews_corrcoef(ybb_test,ybb_pred)))\nplot_confusion_matrix(gbc3, Xbb_test, ybb_test, cmap='Blues')","a6b3f58b":"> ### RainForest Classifier","04e216a0":"> ### \"This requires stronger classifiers\"","58d66d66":"# EDA","b04539e1":"#### * Case 1: direct classification, letting epochs handle the class imbalance\n#### * Case 2: dividing the problem into two sub problems, first classify 1 vs (2,3) and then 2 vs 3 --> handling class imbalance","ef33e74d":"> ### Grid Search for RainForest Classifier","f032f603":"#### Data Overview and Understanding it\n","a7adfec9":"> ### Final F1_score of case 2 -- 98, 93 x 99 = 92.07, 93 x 97 = 90.21 \n> ### There's slight increase in class 2's F1_score and decrease in class 3's. So we can say, there's not much of a difference.","7b1e33e9":"> ### Support Vector Classification","e11c63da":"> ### Most of the box plots shows that normal (class 1) and suspect (class 2) are highly overlapping","e5c0a22b":"Comparing models -- SVC, Rain Forest Classifier, Stochastic Gradient Boosting","ec4e6c60":"> ### Stochastic Gradient Boosting","e13a0925":"## Stochastic Gradient Boosting performed the best in separation, So for case 2, Only Stochastic Gradient Boosting will be used"}}