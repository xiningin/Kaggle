{"cell_type":{"57c2e4c8":"code","eb4a3bec":"code","b3732b69":"code","c1e9384e":"code","a3a8007e":"code","30689722":"code","7ba24758":"code","6b83a2c3":"code","836b45bf":"code","77841da9":"code","f6e30e94":"code","d7413ccb":"code","eea53580":"code","d1b8066d":"code","a37051d8":"code","c225f9d3":"code","39a24412":"code","7b9a8fe0":"code","436fc11b":"code","d6661ee6":"code","a0f0570a":"code","433f2f8f":"code","f59156ac":"code","9d2440f3":"code","ea652935":"code","7e1ab536":"code","c630ffd7":"code","7a4c5bc9":"code","3ae60f52":"code","38cf0091":"code","14bf2108":"code","bc625c3c":"code","748c0b47":"code","603d9861":"code","881acb88":"code","05f1fea3":"code","a714a207":"code","45a78701":"code","aad50d01":"code","fa026f1d":"code","4e0b09f9":"code","8ea07757":"code","5993a272":"code","8b7272d5":"code","1f044347":"code","7261244d":"code","42bb77c3":"code","bf8a3a0d":"code","76d19432":"code","fac58f98":"code","67a16541":"markdown","fcf1a3d1":"markdown","3fc268d6":"markdown","2daeb388":"markdown","65585f94":"markdown","2c8df7d1":"markdown","fdf98c86":"markdown","dce95f1c":"markdown","66cfed4f":"markdown","37d06019":"markdown","e6b3b4d9":"markdown","f651589d":"markdown","ec5e8e09":"markdown","8eb8516b":"markdown","b9f8f6ac":"markdown","e140b25f":"markdown","b2bf11be":"markdown","5cd15056":"markdown","6438fdbf":"markdown","2752dafc":"markdown","cf3b100f":"markdown","215ae00d":"markdown","fcb64130":"markdown"},"source":{"57c2e4c8":"import tensorflow as tf\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D, BatchNormalization\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nimport numpy as np\nimport pandas as pd\nimport os \nimport re\n\nfrom nltk.stem import SnowballStemmer\nfrom nltk.stem import WordNetLemmatizer\n\n\nfrom wordcloud import WordCloud\nfrom PIL import Image\n\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn')\n%matplotlib inline\n\nprint(\"Tensorflow Version\",tf.__version__)","eb4a3bec":"import nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords","b3732b69":"!pip install pyspellchecker\nfrom spellchecker import SpellChecker","c1e9384e":"df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf.head()","a3a8007e":"print(\"Train data size : {}\".format(df.shape))","30689722":"df.info()","7ba24758":"YlGnBu_palette = sns.color_palette(\"YlGnBu\", 10)\nsns.palplot(YlGnBu_palette)","6b83a2c3":"fig,ax = plt.subplots(1,2,figsize=(12,6))\ndf['target'].value_counts().plot.pie(explode=[0, 0.1], autopct='%1.1f%%', ax=ax[0], shadow=True, colors=[YlGnBu_palette[2],YlGnBu_palette[3]])\nax[0].set_title('Pie plot - target')\nsns.countplot('target', data=df, ax=ax[1], palette=[YlGnBu_palette[5],YlGnBu_palette[6]])\nax[1].patch.set_alpha(0)\nax[1].set_title('Count plot - target')\n\nfig.text(0.13,0.95,\"Plot the target data percent\", fontweight=\"bold\", fontfamily='serif', fontsize=20)\nplt.show()\n#target == 0 : negative(fake)\n#tarege == 1 : postive(real)","836b45bf":"keyword_unique = df['keyword'].unique()\nprint(keyword_unique[:20])","77841da9":"fig,ax = plt.subplots(1,1,figsize=(16,6))\nsns.countplot('keyword',data=df,ax=ax,order=df['keyword'].value_counts().index, palette=sns.color_palette(\"YlGnBu\", 15))\nax.set_xlim(0,15)\nax.patch.set_alpha(0)\nfig.text(0.13,0.92,\"Count distribution by keyword in Tweets\", fontweight=\"bold\", fontfamily='serif', fontsize=20)\nplt.show()","f6e30e94":"def get_length(text):\n    return len(text)","d7413ccb":"df['length'] = df['text'].apply(get_length)","eea53580":"YlGnBu_palette","d1b8066d":"fig, ax = plt.subplots(1,1, figsize=(18,5))\nplt.hist(df[df['target'] == 0]['length'], alpha = 0.4, bins=150, label = 'Not', color=YlGnBu_palette[3])\nplt.hist(df[df['target'] == 1]['length'], alpha = 0.4, bins=150, label = 'Real', color=YlGnBu_palette[6])\nplt.xlabel('length')\nplt.ylabel('numbers')\nplt.legend(loc='upper right')\nplt.xlim(0,150)\nax.set_ylabel('')\nax.patch.set_alpha(0)\nfig.text(0.13,0.8,\"Count distribution by length in Tweets\", fontweight=\"bold\", fontfamily='serif', fontsize=25)\nplt.show()","a37051d8":"df.isnull().sum()","c225f9d3":"df['keyword'].fillna(\"\", inplace = True)\ndf['location'].fillna(\"\",inplace = True)\ndf.isnull().sum()","39a24412":"stop_words = stopwords.words('english')\nstemmer = SnowballStemmer('english')\nlemmatizer = WordNetLemmatizer()","7b9a8fe0":"def preprocess(text,stem=False):\n    text = text.lower()  # lowercase\n\n    text = re.sub(r'[!]+', '!', text)\n    text = re.sub(r'[?]+', '?', text)\n    text = re.sub(r'[.]+', '.', text)\n    text = re.sub(r\"'\", \"\", text)\n    text = re.sub('\\s+', ' ', text).strip()  # Remove and double spaces\n    text = re.sub(r'&amp;?', r'and', text)  # replace & -> and\n    text = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", text)  # Remove URLs\n    # remove some puncts (except . ! # ?)\n    text = re.sub(r'[:\"$%&\\*+,-\/:;<=>@\\\\^_`{|}~]+', '', text)\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'EMOJI', text)\n    \n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(SnowballStemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)","436fc11b":"spell = SpellChecker()\ndef correct_spell(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)","d6661ee6":"text = \"corect\"\ncorrect_spell(text)","a0f0570a":"df.text = df.text.apply(lambda x: preprocess(x))\n#df.text = df.text.apply(lambda x: correct_spell(x))","433f2f8f":"mask_dir = np.array(Image.open('..\/input\/masksforwordclouds\/twitter_mask3.jpg'))","f59156ac":"fig, axes = plt.subplots(1,2, figsize=(18,9))\ntarget_list = np.unique(df['target'])\n\nfor i, target in zip(range(2), target_list):\n    wc = WordCloud(background_color=\"white\", max_words = 2000, width = 1600, height = 800, mask=mask_dir, colormap=\"Blues\").generate(\" \".join(df[df['target']==target]['text']))\n    \n    if target == 0:\n        axes[i].text(0.5,1, \"Fake text\", fontweight=\"bold\", fontfamily='serif', fontsize=17)\n    else:\n        axes[i].text(0.5,1, \"Reak text\", fontweight=\"bold\", fontfamily='serif', fontsize=17)\n    axes[i].patch.set_alpha(0)\n    axes[i].axis('off')\n    axes[i].imshow(wc)\n\nfig.text(0.1, 0.9,\"WordCloud by target per text in Tweets\", fontweight=\"bold\", fontfamily='serif', fontsize=22)\nplt.show()","9d2440f3":"df.head(10)","ea652935":"df.drop(['keyword','id','location','length'],axis=1,inplace=True)\ndf.head(10)","7e1ab536":"train_data, test_data = train_test_split(df,test_size = 0.2)\nprint(\"Train Data size : \",len(train_data))\nprint(\"Test Data size : \",len(test_data))","c630ffd7":"train_data.head(10)","7a4c5bc9":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data.text)\nword_index = tokenizer.word_index\nvocab_size = len(word_index) + 1\nprint(\"Vocab size : \",vocab_size)","3ae60f52":"x_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text), maxlen = 30)\nx_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text), maxlen = 30)\n\n\nprint(\"Training X Shape:\",x_train.shape)\nprint(\"Testing X Shape:\",x_test.shape)","38cf0091":"y_train = train_data.target\ny_test = test_data.target\n\nprint(\"y_train shape:\", y_train.shape)\nprint(\"y_test shape:\", y_test.shape)\nprint(y_train,y_test)","14bf2108":"GLOVE_EMB = '\/kaggle\/input\/glove6b\/glove.6B.300d.txt'\nEMBEDDING_DIM = 300\nLR = 1e-3\nBATCH_SIZE = 512","bc625c3c":"embeddings_index = {}\n\nf = open(GLOVE_EMB)\nfor line in f:\n    values = line.split()\n    word = value = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(embeddings_index))","748c0b47":"embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","603d9861":"embedding_layer = tf.keras.layers.Embedding(vocab_size,\n                                            EMBEDDING_DIM,\n                                            weights=[embedding_matrix],                                            \n                                            input_length=30,\n                                           trainable=False)","881acb88":"inputs = Input(shape=(30,),dtype = 'int32')\n\nembedding = embedding_layer(inputs)\nnet = SpatialDropout1D(0.2)(embedding)\nnet = Bidirectional(LSTM(64,dropout=0.2, recurrent_dropout=0.2))(net)\nnet = Dense(1,activation = 'sigmoid')(net)\n\noutputs = net\nmodel = tf.keras.Model(inputs,outputs)","05f1fea3":"model.compile(optimizer = Adam(learning_rate = LR),\n             loss = 'binary_crossentropy',\n             metrics = ['accuracy'])","a714a207":"callback = ReduceLROnPlateau(factor=0.1,\n                    min_lr = 0.01,\n                    monitor = 'val_loss',\n                    verbose = 1)","45a78701":"history = model.fit(x_train,\n                    y_train,\n                    batch_size=512,\n                    epochs=10,\n                    validation_data=(x_test, y_test),\n                    callbacks = [callback])","aad50d01":"\nf,ax = plt.subplots(2,1,figsize=(20,15))\n\nax[0].plot(history.history['accuracy'])\nax[0].plot(history.history['val_accuracy'])\nax[0].set_title('Model Accuracy')\nax[0].set_ylabel('Accuracy')\nax[0].set_xlabel('Epochs')\nax[0].legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n\n\nax[1].plot(history.history['loss'])\nax[1].plot(history.history['val_loss'])\nax[1].set_title('Model Loss')\nax[1].set_ylabel('Loss')\nax[1].set_xlabel('Epochs')\nax[1].legend(['train', 'val'], loc = 'upper left')\n\nf.tight_layout()\nplt.show()","fa026f1d":"sample = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","4e0b09f9":"sample.head()","8ea07757":"test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","5993a272":"test.head()","8b7272d5":"test.drop(['keyword','id','location',],axis=1,inplace=True)\ntest.head(10)","1f044347":"test.text = test.text.apply(lambda x: preprocess(x))","7261244d":"test.shape","42bb77c3":"test = pad_sequences(tokenizer.texts_to_sequences(test.text), maxlen = 30)\n\nprint(\"test Shape:\",test.shape)","bf8a3a0d":"prediction = model.predict(test)\nprediction = list(1 if x>0.5 else 0 for x in list(prediction))\nprediction[:10]","76d19432":"sample['target'] = prediction\nsample['target'].value_counts()","fac58f98":"sample.to_csv('submission.csv',index=False)","67a16541":"# 1. Import & Install libray\n* Import basic & Enginnering libray\n* Install pyspellchecker \/ stopwords list","fcf1a3d1":"#### \u2714\ufe0f This notebook will use this palettes.","3fc268d6":"### 6-1) Glove & LSTM Modeling","2daeb388":"### 3-3) Number of alphabets by sentence \/ Number of words by sentence","65585f94":"### 6-2) Training","2c8df7d1":"### 5-5) Text data tokenization","fdf98c86":"### 5-2) Use SpellChecker libray","dce95f1c":"### 6-3) Plot model's loss \/ accuray values","66cfed4f":"### 3-1) Plot the target data percent","37d06019":"# 2. Check out my data\n* Check Shape \/ Info","e6b3b4d9":"### 5-3) Plot Word Cloud\n* fake target\n* real target","f651589d":"# 7. Submission\n* Submit the predictions","ec5e8e09":"##### reference \n* https:\/\/www.kaggle.com\/mnavaidd\/tweet-classification-using-lstm-bert\/notebook?select=glove.6B.zip\n* https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\n\n###  If this notebook is useful for your kaggling, \"UPVOTE\" for it \ud83d\udc40\n#### THX to Reading My Notebook\ud83c\udf08","8eb8516b":"### 5-6) Text data to fit on Glove Embedding","b9f8f6ac":"***\n\n## My workflow\n#### 1. Import & Install libray\n* Import basic & Enginnering libray\n* Install pyspellchecker \/ stopwords list\n\n#### 2. Check out my data\n* Check Shape \/ Info\n\n#### 3. Exploratory Data Analysis(EDA) with Visualization [Before Preprocessing]\n* Plot the target data percent\n* Plot the Keyword per Count\n* Number of alphabets by sentence \/ Number of words by sentence\n\n#### 4. Prepocessing Data\n* Null value processing\n\n#### 5. Feature Enginnering \n* Text data to nomalization\n* Use SpellChecker libray\n* Plot WordClouds\n* Drop unuseful columns\n* Text data tokenization\n* Text data to fit on Glove Embedding\n\n#### 6. Modeling\n* Glove & LSTM Modeling\n* Training\n* Plot model's loss \/ accuray values\n\n#### 7. Submission\n* Submit the predictions","e140b25f":"# 3. Exploratory Data Analysis(EDA) with Visualization [Before Preprocessing]\n* Plot the target data percent\n* Plot the Keyword per Count\n* Number of alphabets by sentence \/ Number of words by sentence","b2bf11be":"# 6. Modeling\n* Glove & LSTM Modeling\n* Training\n* Plot model's loss \/ accuray values","5cd15056":"### 5-4) Drop unuseful columns","6438fdbf":"![Unknown.png](attachment:22d596b3-5aa1-433d-81f4-e16d1ee27c29.png)\n\n# Natural Language Processing with Disaster Tweets\n\n## Overview\n\nThis Notebook will be completed in two main ways.<br\/>\nFirst, find and visualize useful data or meaningful relationships within the data.<br\/>\nSecond, select a model based on the visualization of the previous process. Transform or refine the data into the appropriate form for the model to be used.<br\/><br\/>\n\nThis competition is about predicting positive and negative through data.<br\/>\nThat is why we need to go through a careful data preprocessing process.\n\n#### My opinion :\nTo provide a variety of information to the deep learning model, we will combine the columns 'keyword', 'location' and 'text' of the data.","2752dafc":"# 4. Prepocessing Data\n* Null value processing","cf3b100f":"### 5-1) Text data to nomalization","215ae00d":"# 5. Feature Enginnering \n* Text data to nomalization\n* Use SpellChecker libray\n* Plot WordClouds\n* Drop unuseful columns\n* Text data tokenization\n* Text data to fit on Glove Embedding","fcb64130":"### 3-2) Plot the Keyword per Count"}}