{"cell_type":{"318b26dd":"code","edda6c80":"code","de84e210":"code","0d8f30e1":"code","8cd87609":"code","b85e8310":"code","c347fd98":"code","12803f58":"code","49d238a9":"code","ccb3e303":"code","f9b62be5":"code","751fef04":"code","eb0fb81a":"code","81940886":"code","e1423f2c":"code","acc2068a":"code","75550a30":"code","55490bd1":"code","615eb212":"code","35d61773":"code","86e79583":"code","4416d69a":"code","82f73b4c":"code","27b3dff1":"code","dfc9323e":"code","99fa3256":"code","953b4e6e":"code","d1eeb3d4":"code","71f1a8c1":"code","be1b683e":"code","cc2d8ea4":"code","1ea0f9b6":"code","ea77774d":"code","0957c51f":"code","dfa64c79":"code","bf124099":"code","5aed6fd5":"code","d379f574":"code","818cba8a":"code","11b08072":"code","ee29c618":"code","06ec0e32":"code","35fe48a7":"code","95479530":"code","08a80428":"code","d24a2274":"code","4cce7e55":"code","27ad75f9":"code","77d6c3a8":"code","87147bd1":"markdown","95a38b74":"markdown","0c8b69e8":"markdown","75c25593":"markdown","782d7cab":"markdown","c4cde946":"markdown","fa7855d8":"markdown","6cf542e9":"markdown","ef6291af":"markdown","4a414195":"markdown","c35d626d":"markdown","59d4c519":"markdown","037257de":"markdown","bbec2ff5":"markdown","742477a6":"markdown","d66338db":"markdown","ab649bd5":"markdown","f62cfb63":"markdown","ebaa53b6":"markdown","a979597e":"markdown","25937299":"markdown","6ea7d32b":"markdown","dd8bb22f":"markdown","ff9921d2":"markdown","dc02b60c":"markdown","1ca2904c":"markdown","7ee9f622":"markdown","fddc25db":"markdown","14171180":"markdown","2ffb2a7f":"markdown","21fc4543":"markdown","52283a2c":"markdown","3be16f4c":"markdown","f7a5103c":"markdown","7a6618df":"markdown","1c1cc16f":"markdown","18729ea6":"markdown","18af242f":"markdown"},"source":{"318b26dd":"!pip install openpyxl\n!pip install dgl==0.6a210210","edda6c80":"import dgl\nfrom dgl.dataloading.pytorch import GraphDataLoader\nfrom dgl.data import DGLDataset\nfrom dgl.nn.pytorch import GraphConv, GATConv, GatedGraphConv, DotGatConv\nfrom dgl.nn import AvgPooling, MaxPooling\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport numpy as np \nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \n\nimport operator \n\nimport pandas as pd\nimport pickle\n\nimport string\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import TSNE\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold\nimport scipy.sparse as sp\n\nfrom tqdm import tqdm\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import CrossEntropyLoss\nimport torch.nn as nn\n\n\nfrom wordcloud import WordCloud\nimport wandb\nimport warnings\n\n\nwarnings.filterwarnings('ignore')\n\nsns.set_theme()\nsns.set_context(\"talk\")\n\nstopwords=set(stopwords.words('english'))\nlemmatizer=WordNetLemmatizer()\n\nseed = 42","de84e210":"def get_label_dist(df, top_k = 10, reverse = False):\n    \"\"\"\n    Plots barplot of count of labels in the dataset\n\n        Parameters:\n            df (pandas DataFrame): pandas DataFrame with 'target' as the label column\n            top_k (int): plot top_k labels accorinding to the count. Default 10\n            reverse (bool): whether to plot top_k most prevalent or least prevalent. Default False\n            \n        Returns:\n            Nothing\n    \"\"\"\n    \n    \n    \n    val_count = df['target'].value_counts(normalize = True)\n    \n    if not reverse:\n        for ind in val_count.index[:top_k]:\n            print(f'{ind} : {idx2label[ind]}')\n    else:\n        for ind in val_count.index[-top_k:]:\n            print(f'{ind} : {idx2label[ind]}')\n            \n    plt.figure(figsize = (20,6))\n    if reverse:\n        sns.barplot(x = list(map(str, val_count.index[-top_k:])), y = val_count.values[-top_k:], palette = 'Blues_r')\n        plt.title(f'Distribution of normalized count of bottom {top_k} labels')\n    else:\n        sns.barplot(x = list(map(str, val_count.index[:top_k])), y = val_count.values[:top_k], palette = 'Blues_r')\n        plt.title(f'Distribution of normalized count of top {top_k} labels')\n    plt.xticks()\n    \n    \n    \ndef filter_text(text):\n\n    \"\"\"\n    Returns the lowercase of input text by removing punctuations, stopwords\n\n        Parameters:\n            text (str): input string\n            \n        Returns:\n            string\n    \"\"\"\n    \n    tokenized_words=word_tokenize(text)\n    filtered_words=[word.strip().strip('.').lower() for word in tokenized_words if ((word.lower() not in string.punctuation) &\n                                                                 (word.lower() not in stopwords))]\n    \n    # I trained a GNN on lemmatized text, the overall 5-fold avg metric scores on all the metrics were lower on lemmatized text compared to non-lemmatized. \n#     stemmed_words=[lemmatizer.lemmatize(word) for word in filtered_words]\n    return ' '.join(filtered_words)\n\n\ndef remove_words_nums(text):\n\n    \"\"\"\n    Returns the input text by removing names of months, the word 'company' and numerics\n\n        Parameters:\n            text (str): input string\n            \n        Returns:\n            string\n    \"\"\"\n    \n    remove_words = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'company']\n    \n    filtered_text = []\n    for w in text.split():\n        if (not w.isnumeric() and w not in remove_words):\n            filtered_text.append(w)\n\n    return ' '.join(filtered_text)\n\n\ndef plot_textlen_dist(df, label_list, unique = True):\n    \"\"\"\n    Plots histogram of number of words in each sample for given list of labels\n\n        Parameters:\n            df (pandas DataFrame): pandas DataFrame with 'target' as the label column\n            label_list (list): list of labels to plot\n            unique (bool): whether to count only unique words. Default True\n            \n        Returns:\n            Nothing\n    \"\"\"\n    \n    assert isinstance(label_list, list), 'label_list must be a list'\n    assert all(x in labels for x in label_list), f\"The industry must belong to one of {', '.join(labels)}\"\n    \n    if unique:\n        col = 'unique_text_len'\n        title = 'Distribution of number of unique words in description'\n    else:\n        col = 'text_len'\n        title = 'Distribution of number of words in description'\n        \n    plt.figure(figsize = (10,6))\n    \n    for ind in label_list:\n        sns.distplot(df[df['target'] == label2idx[ind]][col], label = ind, kde = False, norm_hist = True)\n        plt.legend()\n        \n    plt.title(title)\n    plt.xlabel('Text Length')\n    \n\ndef plot_wordcloud(text, label, max_font_size=40, max_words=50):\n    \n    \"\"\"\n    Plots wordcloud of the input text\n\n        Parameters:\n            text (str): input string\n            label (str): industry tag. Used in title\n            max_font_size (int): maximum font size of the most frequent word. Default 40\n            max_words (int): maximum words to show in the wordcloud. Default 50\n            \n        Returns:\n            Nothing\n    \"\"\"\n    \n    plt.figure(figsize=(10,5))\n    wordcloud=WordCloud(max_font_size=max_font_size,max_words=max_words,random_state=seed)\n    plot=wordcloud.generate(text)\n    plt.title('Industry Tag: ' + label, fontsize = 25)\n    plt.imshow(plot)\n    plt.axis('off')\n    plt.show()\n\n\ndef get_text(traindf, industry):\n\n    \"\"\"\n    Returns string containing all text from input industry tag\n\n        Parameters:\n            traindf (pandas DataFrame): pandas DataFrame with 'target' as the label column and 'text' as the description column\n            industry (str): industry tag for using text\n            \n        Returns:\n            string \n    \"\"\"\n    \n    assert industry in labels, f\"The industry must belong to one of {', '.join(labels)}\"\n    \n    target=traindf[traindf['target']==label2idx[label]].reset_index(drop=True)\n\n    target_text=' ' \n\n    for i in range(target.shape[0]):\n        target_text+=target.text[i]\n        \n    return target_text\n\n\ndef plot_top_ngrams(traindf, industry, vectorizer = 'count', ngrams=(1,1),top=10,max_features=10000):\n    \n    \"\"\"\n    Plots barplot of count of top ngrams occurred in the description for given industry\n\n        Parameters:\n            traindf (pandas DataFrame): pandas DataFrame with 'target' as the label column and 'text' as the description column\n            industry (str): industry tag to plot\n            vectorizer (str): which vectorizer to use. Can be one of 'count' and 'tfidf'. Default 'count'\n            ngrams (tuple\/int): ngrams to consider. Default (1,1)\n            top (int): how many top ngrams to show in the plot. Default 10\n            max_features (int): maximum ngrams to form. \n            \n        Returns:\n            Nothing\n    \"\"\"\n    \n    assert vectorizer in ['count', 'tfidf'], \"vectorizer must be one of 'count' and 'tfidf'\"\n    assert industry in labels, f\"The industry must belong to one of {', '.join(labels)}\"\n    \n    text = traindf[traindf['target']==label2idx[industry]]['text']\n    \n    vector_dict = {'count': CountVectorizer, 'tfidf': TfidfVectorizer}\n    \n    cv=vector_dict[vectorizer](ngram_range=ngrams,max_features=max_features)\n    trans_text=cv.fit_transform(text)\n    col_sum=trans_text.sum(axis=0)\n    word_index=[(word,col_sum[0,idx]) for word,idx in cv.vocabulary_.items()]\n    sorted_word_index=sorted(word_index,key=lambda x:x[1],reverse=True)\n    top_words_index=sorted_word_index[:top]\n    top_words=[element[0] for element in top_words_index]\n    counts=[element[1] for element in top_words_index]\n\n    plt.figure(figsize=(20,6))\n    sns.barplot(x = top_words, y = counts, palette = 'Blues_r')\n    plt.title('Industry Tag: ' + industry)\n    plt.xticks(rotation = 45)\n\n\ndef get_tsne_array(traindf, vectorizer = 'count', ngrams = (1,3)):\n    \n    \"\"\"\n    Returns array of shape (len(traindf), 2). The array is obtained by operating 'count' of 'tfidf' vectorizer over each text, \n    followed by TruncatedSVD to reduce the number of columns to 50 and then finally, applying TSNE to obtain 2 columns. \n\n        Parameters:\n            traindf (pandas DataFrame): pandas DataFrame with 'text' as the description column\n            vectorizer (str): which vectorizer to use. Can be one of 'count' and 'tfidf'. Default 'count'\n            ngrams (tuple\/int): ngrams to consider. Default (1,1)\n\n        Returns:\n            Array of shape (len(traindf), 2)\n    \"\"\"\n    \n    assert vectorizer in ['count', 'tfidf'], \"vectorizer must be one of 'count' and 'tfidf'\"\n    vector_dict = {'count': CountVectorizer, 'tfidf': TfidfVectorizer}\n    \n    cv=vector_dict[vectorizer](ngram_range=ngrams,analyzer='word')\n    train_X_cv=cv.fit_transform(traindf['text'].values)\n    \n    tsvd=TruncatedSVD(n_components=50,random_state=seed)\n    train_X_svd=tsvd.fit_transform(train_X_cv)\n\n    tsne=TSNE(n_components=2,random_state=seed)\n    train_X_tsne=tsne.fit_transform(train_X_svd)\n    \n    return train_X_tsne\n\n\ndef plot_scatter(train_X_tsne, traindf, label_list):\n    \n    \"\"\"\n    Plots scatterplot of the first two features of the train_X_tsne array. \n\n        Parameters:\n            train_X_tsne (array): Array of shape (len(traindf), n) where n >= 2 \n            traindf (pandas DataFrame): pandas DataFrame with 'target' as the label column\n            label_list (list): list of labels to plot\n            \n        Returns:\n            Nothing\n    \"\"\"\n    \n    assert isinstance(label_list, list), 'label_list must be a list'\n    assert all(x in labels for x in label_list), f\"The industry must belong to one of {', '.join(labels)}\"\n    \n    \n    indices= traindf[traindf['target'].isin([label2idx[x] for x in label_list])].index\n    label_series = traindf[traindf['target'].isin([label2idx[x] for x in label_list])]['target'].values\n   \n    tsne_array = train_X_tsne[indices, :]\n    \n    df=pd.DataFrame()\n    df['tsne1']=pd.Series(tsne_array[:,0])\n    df['tsne2']=pd.Series(tsne_array[:,1])\n    df['target']=pd.Series(label_series)\n    \n    fig, ax = plt.subplots(figsize=(15,6))\n    scatter = sns.scatterplot(df['tsne1'],df['tsne2'],hue=df['target'], ax = ax, palette = 'tab10')\n    move_legend(ax, idx2label)\n    plt.xlabel('TSNE Axis 1')\n    plt.ylabel('TSNE Axis 2')\n    plt.show()\n    \n    \ndef move_legend(ax, idx2label):\n\n    \"\"\"\n    Adds legend to a plot\n\n        Parameters:\n            ax (Axes): Axes object \n            idx2label (dict): dictionary with ids as keys and labels as values\n            \n        Returns:\n            Nothing\n    \"\"\"\n    \n    \n    old_legend = ax.legend_\n    handles = old_legend.legendHandles\n    labels = [idx2label[int(t.get_text())] for t in old_legend.get_texts()]\n\n    title = old_legend.get_title().get_text()\n    ax.legend(handles, labels, title=title)","0d8f30e1":"traindf = pd.read_excel('..\/input\/hr-string\/train.xlsx')\ntraindf.columns = ['company', 'text', 'target']\n\n\nprint('Checking and removing rows with na values in the dataset \\n')\nprint(traindf.isna().sum())\ntraindf = traindf.dropna(axis = 0).reset_index(drop = True)\nprint('*'*50)\nprint('After processing \\n')\nprint(traindf.isna().sum())","8cd87609":"traindf.head()","b85e8310":"labels = traindf['target'].unique()\nprint('Number of unique industry tags: ', len(labels))\nlabel2idx = {l:i for i,l in enumerate(sorted(labels))}\nidx2label = {v:k for k,v in label2idx.items()}\n\ntraindf['target'] = traindf['target'].apply(lambda x: label2idx[x])","c347fd98":"get_label_dist(traindf, top_k = 10, reverse = False)","12803f58":"get_label_dist(traindf, top_k = 10, reverse = True)","49d238a9":"traindf['text'] = traindf['text'].apply(lambda x: filter_text(x))\ntraindf['text_len'] = traindf['text'].apply(lambda x: len(x.split()))\ntraindf['unique_text_len'] = traindf['text'].apply(lambda x: len(set(x.split())))","ccb3e303":"print('Max of mean text length: ', traindf.groupby(by='target')['text_len'].mean().max())\nprint('Tag of max of mean text length: ', idx2label[traindf.groupby(by='target')['text_len'].mean().argmax()])\n\nprint('Min of mean text length: ', traindf.groupby(by='target')['text_len'].mean().min())\nprint('Tag of min of mean text length: ', idx2label[traindf.groupby(by='target')['text_len'].mean().argmin()])","f9b62be5":"print(traindf['text_len'].describe())","751fef04":"plot_textlen_dist(traindf, ['Oil & Gas Storage & Transportation', 'Asset Management & Custody Banks'], unique = False)","eb0fb81a":"plot_textlen_dist(traindf, ['Oil & Gas Storage & Transportation', 'Asset Management & Custody Banks'], unique = True)","81940886":"plt.figure(figsize = (20,6))\n\nplt.subplot(1,2,1)\nsns.kdeplot(traindf.groupby(by='target')['text_len'].mean())\nplt.xlabel('Text Length')\nplt.title('Distribution of labelwise mean of text length')\n\nplt.subplot(1,2,2)\nsns.kdeplot(traindf.groupby(by='target')['unique_text_len'].mean())\nplt.xlabel('Text Length')\nplt.title('Distribution of labelwise mean of unique text length')\n\nplt.show()","e1423f2c":"label = 'Biotechnology'\n\ntarget_text = get_text(traindf, label)\nplot_wordcloud(target_text, label)\n\nlabel = 'Advertising'\ntarget_text = get_text(traindf, label)\nplot_wordcloud(target_text, label)\n\nlabel = 'Casinos & Gaming'\ntarget_text = get_text(traindf, label)\nplot_wordcloud(target_text, label)\n\nlabel = 'Health Care Facilities'\ntarget_text = get_text(traindf, label)\nplot_wordcloud(target_text, label)\n\nlabel = 'Movies & Entertainment'\ntarget_text = get_text(traindf, label)\nplot_wordcloud(target_text, label)\n\nlabel = 'Restaurants'\ntarget_text = get_text(traindf, label)\nplot_wordcloud(target_text, label)","acc2068a":"label = 'Biotechnology'\nplot_top_ngrams(traindf, label, vectorizer = 'tfidf', ngrams = (2,2), top = 20)\n\nlabel = 'Advertising'\nplot_top_ngrams(traindf, label, vectorizer = 'tfidf', ngrams = (2,2), top = 20)\n\nlabel = 'Casinos & Gaming'\nplot_top_ngrams(traindf, label, vectorizer = 'tfidf', ngrams = (2,2), top = 20)\n\nlabel = 'Health Care Facilities'\nplot_top_ngrams(traindf, label, vectorizer = 'tfidf', ngrams = (2,2), top = 20)\n\nlabel = 'Movies & Entertainment'\nplot_top_ngrams(traindf, label, vectorizer = 'tfidf', ngrams = (2,2), top = 20)\n\nlabel = 'Restaurants'\nplot_top_ngrams(traindf, label, vectorizer = 'tfidf', ngrams = (2,2), top = 20)","75550a30":"traindf['text'] = traindf['text'].apply(remove_words_nums)","55490bd1":"label = 'Biotechnology'\nplot_top_ngrams(traindf, label, vectorizer = 'tfidf', ngrams = (2,2), top = 20)\n\nlabel = 'Advertising'\nplot_top_ngrams(traindf, label, vectorizer = 'tfidf', ngrams = (2,2), top = 20)\n\nlabel = 'Casinos & Gaming'\nplot_top_ngrams(traindf, label, vectorizer = 'tfidf', ngrams = (2,2), top = 20)\n\nlabel = 'Health Care Facilities'\nplot_top_ngrams(traindf, label, vectorizer = 'tfidf', ngrams = (2,2), top = 20)\n\nlabel = 'Movies & Entertainment'\nplot_top_ngrams(traindf, label, vectorizer = 'tfidf', ngrams = (2,2), top = 20)\n\nlabel = 'Restaurants'\nplot_top_ngrams(traindf, label, vectorizer = 'tfidf', ngrams = (2,2), top = 20)","615eb212":"label = 'Biotechnology'\n\ntarget_text = get_text(traindf, label)\nplot_wordcloud(target_text, label)\n\nlabel = 'Advertising'\ntarget_text = get_text(traindf, label)\nplot_wordcloud(target_text, label)\n\nlabel = 'Casinos & Gaming'\ntarget_text = get_text(traindf, label)\nplot_wordcloud(target_text, label)\n\nlabel = 'Health Care Facilities'\ntarget_text = get_text(traindf, label)\nplot_wordcloud(target_text, label)\n\nlabel = 'Movies & Entertainment'\ntarget_text = get_text(traindf, label)\nplot_wordcloud(target_text, label)\n\nlabel = 'Restaurants'\ntarget_text = get_text(traindf, label)\nplot_wordcloud(target_text, label)","35d61773":"train_tsne = get_tsne_array(traindf, vectorizer = 'tfidf')","86e79583":"label_list = ['Advertising', 'Consumer Finance', 'Pharmaceuticals']   # 3 very different industry tags\nplot_scatter(train_tsne, traindf, label_list)\n\nlabel_list = ['Casinos & Gaming', 'Life Sciences Tools & Services', 'Regional Banks']   # 3 very different industry tags\nplot_scatter(train_tsne, traindf, label_list)\n\nlabel_list = ['Construction & Engineering', 'Hotels, Resorts & Cruise Lines', 'Semiconductors']   # 3 very different industry tags\nplot_scatter(train_tsne, traindf, label_list)","4416d69a":"label_list = ['Health Care Equipment', 'Health Care Facilities', 'Health Care Technology'] # 3 closely related industry tags\nplot_scatter(train_tsne, traindf, label_list)\n\nlabel_list = ['Electric Utilities', 'Electronic Equipment & Instruments', 'Electrical Components & Equipment'] # 3 closely related industry tags\nplot_scatter(train_tsne, traindf, label_list)\n\nlabel_list = ['Oil & Gas Equipment & Services', 'Oil & Gas Exploration & Production', 'Oil & Gas Refining & Marketing'] # 3 closely related industry tags\nplot_scatter(train_tsne, traindf, label_list)","82f73b4c":"def load_embeddings(path):\n    with open(path,'rb') as f:\n        emb_arr = pickle.load(f)\n    return emb_arr\n\n\ndef check_coverage(vocab,embeddings_index):\n    \"\"\"\n    Returns list of tuples. The first element of each tuple specifies the word present in the description but not in the embeddings  and the second element \n    specifies the count of that word in the descriptions. The tuples are sorted in the descending order of their count. \n\n        Parameters:\n            vocab (dict): Dictionary with keys as words and values as their count of occurence\n            embeddings_index (dict): Dictionary with keys as words and values as their embeddings\n            \n        Returns:\n            List of tuples\n    \"\"\"\n    a = {}\n    oov = {}\n    k = 0\n    i = 0\n    for word in tqdm(vocab):\n        try:\n            a[word] = embeddings_index[word]\n            k += vocab[word]\n        except:\n\n            oov[word] = vocab[word]\n            i += vocab[word]\n            pass\n\n    print('Found embeddings for {:.2%} of vocab'.format(len(a) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(k \/ (k + i)))\n    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n\n    return sorted_x\n\ndef build_vocab(sentences, verbose =  True):\n    \"\"\"\n    Returns dictionary with keys as words in the sentences and values as their count of occurence\n\n        Parameters:\n            sentences (list of list): List of lists of descriptions\n            verbose (bool): whether to show the progress bar\n            \n        Returns:\n            dictionary\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\n","27b3dff1":"GLOVE_EMBEDDING_PATH = '..\/input\/glove6b50dtxt\/glove.6B.50d.txt'\n\n\n\nword_embeddings = {}\n\nwith open(GLOVE_EMBEDDING_PATH, 'r') as f:\n    for line in f.readlines():\n        data = line.split()\n        word_embeddings[str(data[0])] = list(map(float,data[1:]))","dfc9323e":"vocab = build_vocab(list(traindf['text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,word_embeddings)\noov[:10]","99fa3256":"traindf['text'] = traindf['text'].apply(lambda x: ' '.join(x.split('-')))","953b4e6e":"vocab = build_vocab(list(traindf['text'].apply(lambda x:x.split())))\noov = check_coverage(vocab,word_embeddings)\noov[:10]","d1eeb3d4":"def build_graph(start, end, truncate = False, weighted_graph = True):\n    \"\"\"\n    Returns list of adjacency matrix and list of node matrix \n\n        Parameters:\n            start (int): start index of list\n            end (int): end index of list\n            truncate (bool): whether to truncate the text\n            weighted_graph (bool): whether to use word pair count as the weight in adjacency matrix or just 1.0\n            \n        Returns:\n            list of adjacency matrices, list of node matrices\n    \n    \n    \"\"\"\n    x_adj = []\n    x_feature = []\n    doc_len_list = []\n    vocab_set = set()\n\n    for i in tqdm(range(start, end)):\n\n        doc_words = shuffle_doc_words_list[i].split()\n        if truncate:\n            doc_words = doc_words[:MAX_TRUNC_LEN]\n        doc_len = len(doc_words)\n\n        doc_vocab = list(set(doc_words))\n        doc_nodes = len(doc_vocab)\n\n        doc_len_list.append(doc_nodes)\n        vocab_set.update(doc_vocab)\n\n        doc_word_id_map = {}\n        for j in range(doc_nodes):\n            doc_word_id_map[doc_vocab[j]] = j\n\n        # sliding windows\n        windows = []\n        if doc_len <= window_size:\n            windows.append(doc_words)\n        else:\n            for j in range(doc_len - window_size + 1):\n                window = doc_words[j: j + window_size]\n                windows.append(window)\n\n        word_pair_count = {}\n        for window in windows:\n            for p in range(1, len(window)):\n                for q in range(0, p):\n                    word_p = window[p]\n                    word_p_id = word_id_map[word_p]\n                    word_q = window[q]\n                    word_q_id = word_id_map[word_q]\n                    if word_p_id == word_q_id:\n                        continue\n                    word_pair_key = (word_p_id, word_q_id)\n                    # word co-occurrences as weights\n                    if word_pair_key in word_pair_count:\n                        word_pair_count[word_pair_key] += 1.\n                    else:\n                        word_pair_count[word_pair_key] = 1.\n                    # bi-direction\n                    word_pair_key = (word_q_id, word_p_id)\n                    if word_pair_key in word_pair_count:\n                        word_pair_count[word_pair_key] += 1.\n                    else:\n                        word_pair_count[word_pair_key] = 1.\n    \n        row = []\n        col = []\n        weight = []\n        features = []\n\n        for key in word_pair_count:\n            p = key[0]\n            q = key[1]\n            row.append(doc_word_id_map[vocab[p]])\n            col.append(doc_word_id_map[vocab[q]])\n            weight.append(word_pair_count[key] if weighted_graph else 1.)\n        adj = sp.csr_matrix((weight, (row, col)), shape=(doc_nodes, doc_nodes))\n    \n        for k, v in sorted(doc_word_id_map.items(), key=lambda x: x[1]):\n            features.append(word_embeddings[k] if k in word_embeddings else oov[k])\n\n        x_adj.append(adj)\n        x_feature.append(features)\n\n    \n    return x_adj, x_feature\n\n\n\n","71f1a8c1":"class args:\n    max_epochs = 20\n    lr = 1e-3\n    batch_size = 64\n    embedding_dim = 50\n    hidden_dim = 50\n    num_heads = 8   # used for attention model\n    n_folds = 5\n    window_size = 3","be1b683e":"testdf = pd.read_excel('..\/input\/hr-string\/test.xlsx')\ntestdf.columns = ['company', 'text']\n\ntestdf['text'] = testdf['text'].apply(lambda x: filter_text(x))\ntestdf['text'] = testdf['text'].apply(lambda x: ' '.join(x.split('-')))\ntestdf['text'] = testdf['text'].apply(remove_words_nums)","cc2d8ea4":"    \nword_embeddings_dim = args.embedding_dim\n\nshuffle_doc_words_list = list(traindf['text'].values) + list(testdf['text'].values)\n\nword_set = set()\n\nfor doc_words in shuffle_doc_words_list:\n    words = doc_words.split()\n    word_set.update(words)\n    \nvocab = list(word_set)\nvocab_size = len(vocab)\n\nword_id_map = {}\nfor i in range(vocab_size):\n    word_id_map[vocab[i]] = i\n    \noov = {}\nfor v in vocab:\n    oov[v] = np.random.uniform(-0.1, 0.1, word_embeddings_dim)\n    \n\nwindow_size = args.window_size\n\nprint('building graphs for training')\nx_adj, x_feature = build_graph(start=0, end=len(traindf), weighted_graph = True)","1ea0f9b6":"def check_train_test_word_overlap():\n    \n    \"\"\"\n    This function prints the fraction of unique words in test set that also occur in train set. \n            \n        Returns:\n            Nothing\n    \n    \n    \"\"\"\n    train_unique_words = []\n\n    for text in traindf['text']:\n        train_unique_words.extend(text.split())\n\n    train_unique_words = list(set(train_unique_words))\n\n    testdf = pd.read_excel('..\/input\/hr-string\/test.xlsx')\n    testdf.columns = ['company', 'text']\n\n    test_unique_words = []\n\n    for text in testdf['text']:\n        test_unique_words.extend(text.split())\n\n    test_unique_words = list(set(test_unique_words))\n\n    overlap = [x for x in test_unique_words if x in train_unique_words]\n    print('Intersection of words: ', np.round(len(overlap)\/len(test_unique_words), 3))\n    \ndef plot_results(hist_list):\n\n    \"\"\"\n    Plots evaluation curves\n    \n        Parameters:\n            hist (dict): dictionary with metric names as keys and their epochwise values in list as values\n            \n        Returns:\n            Nothing\n            \n    \"\"\"\n        \n    \n    plt.figure(figsize = (20, 12))\n    \n    for i, (metric_name, metric) in enumerate(hist_list[0].items()):\n        plt.subplot(2,2,i+1)\n        train = [x[0] for hist in hist_list for x in hist[metric_name]]\n        sns.lineplot(x = np.arange(len(metric)*args.n_folds), y = train, label = 'train')\n        val = [x[1] for hist in hist_list for x in hist[metric_name]]\n        sns.lineplot(x = np.arange(len(metric)*args.n_folds), y = val, label = 'val')\n        sns.despine(right = True, top = True)\n        plt.title(metric_name)\n        \n        if i > 1:\n            plt.xlabel('Epochs')\n        ","ea77774d":"\nskf = StratifiedKFold(n_splits = args.n_folds, random_state = 42)\ntraindf['fold'] = -1\n\nfor idx, (_, val_idx) in enumerate(skf.split(traindf, traindf['target'])):\n    traindf.loc[val_idx, 'fold'] = idx","0957c51f":"for fold in range(args.n_folds):\n    print(traindf[traindf['fold']==fold]['target'].value_counts(normalize = True))","dfa64c79":"\nclass GraphDataset(DGLDataset):\n    \"\"\"\n    A dataset class \n\n    ...\n\n    Attributes\n    ----------\n    x_adj (list): list of scipy sparse adjacency matrices\n    \n    x_feature (list): list of node matrices\n\n    targets (list): list of industry tags\n\n\n    \"\"\"\n    def __init__(self, x_adj, x_feature, targets = None):\n        \n        self.adj_matrix = x_adj\n        self.node_matrix = x_feature\n        self.targets = targets\n\n        \n    def __len__(self):\n        return len(self.adj_matrix)\n    \n    def __getitem__(self, idx):\n        \n        \"\"\"\n        Returns a Graph and tensor of label\n        \n        \"\"\"\n        \n        scipy_adj = self.adj_matrix[idx]\n        G = dgl.from_scipy(scipy_adj)\n#         feat = torch.zeros((len(self.node_matrix[idx]), 50))\n#         self.n = self.node_matrix[idx]\n#         for item in self.n:\n#             feat[int(item[0])] = torch.tensor(item[1], dtype = torch.float)\n            \n#         G.ndata['feat'] = feat\n        G.ndata['feat'] = torch.stack([torch.tensor(x, dtype = torch.float) for x in self.node_matrix[idx]])\n        \n        \n        if self.targets is not None:\n            label = self.targets[idx]\n            \n            return G, torch.tensor(label, dtype = torch.long)\n        \n        return G\n    \n    \nimport torch.nn.functional as F\n\n\n# Graph Neural Network with normal Convolutional Layers\nclass Classifier(nn.Module):\n    def __init__(self, in_dim, hidden_dim, n_classes):\n        super(Classifier, self).__init__()\n        self.conv1 = GraphConv(in_dim, hidden_dim)\n        self.conv2 = GraphConv(hidden_dim, hidden_dim)\n        self.avgpooling = AvgPooling()\n        self.classify = nn.Linear(hidden_dim, n_classes)\n\n    def forward(self, g, h):\n        # Apply graph convolution and activation.\n        h = F.relu(self.conv1(g, h))\n        h = F.relu(self.conv2(g, h))\n        h = self.avgpooling(g, h)\n        \n        return self.classify(h)\n        \n    \n\n# Graph Neural Network with Attention Layers where the node features are concatenated for attention    \nclass GATClassifier(nn.Module):\n    def __init__(self, in_dim, hidden_dim, num_heads, n_classes):\n        super(GATClassifier, self).__init__()\n        self.hid_dim = hidden_dim\n        self.gat1 = GATConv(in_dim, hidden_dim, num_heads)\n        self.gat2 = GATConv(hidden_dim*num_heads, hidden_dim, 1)\n        self.avgpooling = AvgPooling()\n        self.drop = nn.Dropout(p = 0.3)\n#         self.maxpooling = MaxPooling()\n        self.classify = nn.Linear(hidden_dim, n_classes)\n\n    def forward(self, g, h):\n        # Apply graph convolution and activation.\n        bs = h.shape[0]\n        h = F.relu(self.gat1(g, h))\n        h = h.reshape(bs, -1)\n        h = F.relu(self.gat2(g, h))\n        h = h.reshape(bs, -1)\n        h = self.drop(h)\n        h = self.avgpooling(g, h)\n#         hmax = self.maxpooling(g, h)\n#         h = torch.cat([havg, hmax], 1)\n        \n        return self.classify(h)\n    \n    \n# Graph Neural Network with Attention Layers where a dot product is performed between node features  \nclass GATDotClassifier(nn.Module):\n    def __init__(self, in_dim, hidden_dim, num_heads, n_classes):\n        super(GATDotClassifier, self).__init__()\n        self.hid_dim = hidden_dim\n        self.gat1 = DotGatConv(in_dim, hidden_dim, num_heads)\n        self.gat2 = DotGatConv(hidden_dim*num_heads, hidden_dim, 1)\n        self.avgpooling = AvgPooling()\n        self.classify = nn.Linear(hidden_dim, n_classes)\n\n    def forward(self, g, h):\n        # Apply graph convolution and activation.\n        bs = h.shape[0]\n        h = F.relu(self.gat1(g, h))\n        h = h.reshape(bs, -1)\n        h = F.relu(self.gat2(g, h))\n        h = h.reshape(bs, -1)\n        h = self.avgpooling(g, h)\n        \n        return self.classify(h)\n    ","bf124099":"wandb.init(project = 'gnn')","5aed6fd5":"\n\ndef train_fold(args, adj_list, node_list, fold = 0):\n\n    \"\"\"\n    Returns dictionary with loss, f1, auc and mrr as the keys and list containing their epochwise scores as values.\n    This function trains and validates a model over a fold of dataset\n\n        Parameters:\n            args (class): Class containing variables specifying values necessary for training model\n            adj_list (list): list of adjacency matrices\n            node_list (list): list of node matrices\n            fold (int): fold to validate model on. \n            \n        Returns:\n            dictionary\n    \n    \n    \"\"\"\n    \n    train_idx = list(traindf[traindf['fold']!=fold].index)\n    val_idx = list(traindf[traindf['fold']==fold].index)\n    \n    print('Num train samples ', len(train_idx))\n    print('Num val samples ', len(val_idx))\n\n    num_classes = traindf['target'].nunique()\n    \n    train = traindf[traindf['fold']!=fold].reset_index(drop = True)\n    val = traindf[traindf['fold']==fold].reset_index(drop = True)\n    \n    train_adj_list, val_adj_list = [adj_list[i] for i in train_idx], [adj_list[i] for i in val_idx]\n    train_node_list, val_node_list = [node_list[i] for i in train_idx], [node_list[i] for i in val_idx]\n    train_label_list, val_label_list = train['target'].values, val['target'].values\n#     weights = torch.tensor(1\/pd.Series(train_label_list).value_counts().sort_index().values, dtype = torch.float)\n    \n    traindataset = GraphDataset(train_adj_list, train_node_list, train_label_list)\n    valdataset = GraphDataset(val_adj_list, val_node_list, val_label_list)\n    \n    trainloader = GraphDataLoader(traindataset, batch_size = args.batch_size, shuffle = True)\n    valloader = GraphDataLoader(valdataset, batch_size = args.batch_size, shuffle = False)\n    \n    model = GATClassifier(args.embedding_dim, args.hidden_dim, args.num_heads, num_classes)\n    criterion = CrossEntropyLoss()   # weight = weights\n    optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\n    scheduler = None\n    \n    best_val_mrr = 0\n    \n    loss = []\n    f1 = []\n    auc = []\n    mrr = []\n    \n    for idx in range(args.max_epochs):\n        print(f'Epoch {idx + 1}\/{args.max_epochs}')\n        \n        train_loss, train_f1, train_auc, train_mrr = train_one_epoch(trainloader, model, criterion, optimizer, scheduler, num_classes)\n        val_loss, val_f1, val_auc, val_mrr = validate(valloader, model, criterion, num_classes)\n        \n        log_results(train_loss, train_f1, train_auc, train_mrr, val_loss, val_f1, val_auc, val_mrr, idx)\n        \n        loss.append((train_loss, val_loss))\n        f1.append((train_f1, val_f1))\n        auc.append((train_auc, val_auc))\n        mrr.append((train_mrr, val_mrr))\n        \n        if val_mrr > best_val_mrr:\n            torch.save(model.state_dict(), f'fold-{fold}.pt')\n            best_val_mrr = val_mrr\n            \n    return {'loss': loss, 'f1': f1, 'auc': auc, 'mrr': mrr}","d379f574":"def train_one_epoch(trainloader, model, criterion, optimizer, scheduler, num_classes):\n\n    \"\"\"\n    Returns training loss, f1, roc_auc and mrr scores over 1 epoch\n    This function trains model for 1 epoch\n\n        Parameters:\n            trainloader (DataLoader\/Iterable): dataloader that yields a batch for training\n            model (nn.Module): model used for training\n            criterion (nn.Module): loss function\n            optimizer (Optimizer): used to optimize the loss function\n            scheduler (Scheduler): used to change the learning rate over epochs\n            num_classes (int): number of classes\n            \n        Returns:\n            loss, f1, roc_auc and mrr floats\n    \n    \n    \"\"\"\n    train_loss = 0\n    train_f1 = 0\n    train_auc = 0\n    \n    all_labels = []\n    all_logits = []\n    \n    total = len(trainloader)\n    model.train()\n    for idx, (G, label) in tqdm(enumerate(trainloader), total = total):\n                \n        h = G.ndata['feat'].float()\n        logit = model(G, h)\n        loss = criterion(logit, label)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if scheduler:\n            scheduler.step()\n        \n        label_numpy = label.detach().cpu().numpy()\n        logit_numpy = logit.softmax(-1).detach().cpu().numpy()\n        \n        train_loss += loss.item()\/total\n        train_f1 += sklearn.metrics.f1_score(label_numpy, logit_numpy.argmax(-1), average = 'micro')\/total\n        \n        all_labels.append(label_numpy)\n        all_logits.append(logit_numpy)\n    \n    all_labels = np.concatenate(all_labels)\n    all_logits = np.concatenate(all_logits)\n    \n    one_hot_labels= np.zeros((len(all_labels), num_classes))\n    one_hot_labels[np.arange(len(all_labels)), all_labels] = 1.0\n    \n    train_auc = sklearn.metrics.roc_auc_score(all_labels, all_logits, multi_class = 'ovo', labels = np.array([int(i) for i in range(num_classes)]))\n    train_mrr = sklearn.metrics.label_ranking_average_precision_score(one_hot_labels, all_logits)\n    \n    return train_loss, train_f1, train_auc, train_mrr\n\n\ndef validate(valloader, model, criterion, num_classes):\n\n    \"\"\"\n    Returns validation loss, f1, roc_auc and mrr scores over 1 epoch\n    This function validates the model \n\n        Parameters:\n            valloader (DataLoader\/Iterable): dataloader that yields a batch for validating\n            model (nn.Module): model to be used for validation\n            criterion (nn.Module): loss function\n            num_classes (int): number of classes\n            \n        Returns:\n            loss, f1, roc_auc and mrr floats\n    \n    \n    \"\"\"\n    \n    val_loss = 0\n    val_f1 = 0\n    val_auc = 0\n    \n    all_labels = []\n    all_logits = []\n    \n    total = len(valloader)\n    model.eval()\n    \n    with torch.no_grad():\n        for idx, (G, label) in tqdm(enumerate(valloader), total = total):\n\n            h = G.ndata['feat'].float()\n            logit = model(G, h)\n            loss = criterion(logit, label)\n\n            label_numpy = label.detach().cpu().numpy()\n            logit_numpy = logit.softmax(-1).detach().cpu().numpy()\n\n            val_loss += loss.item()\/total\n            val_f1 += sklearn.metrics.f1_score(label_numpy, logit_numpy.argmax(-1), average = 'micro')\/total\n\n        \n            all_labels.append(label_numpy)\n            all_logits.append(logit_numpy)\n\n        all_labels = np.concatenate(all_labels)\n        all_logits = np.concatenate(all_logits)\n        \n        print((idx2label[lab], idx2label[log.argmax(-1)]) for (lab, log) in zip(all_labels, all_logits))\n        \n        one_hot_labels= np.zeros((len(all_labels), num_classes))\n        one_hot_labels[np.arange(len(all_labels)), all_labels] = 1.0\n        \n        val_auc = sklearn.metrics.roc_auc_score(all_labels, all_logits, multi_class = 'ovo', labels = np.array([int(i) for i in range(num_classes)]))\n        val_mrr = sklearn.metrics.label_ranking_average_precision_score(one_hot_labels, all_logits)\n        \n    \n    return val_loss, val_f1, val_auc, val_mrr\n\n\ndef log_results(train_loss, train_f1, train_auc, train_mrr, val_loss, val_f1, val_auc, val_mrr, idx):\n\n    \"\"\"\n    This function logs all the metric values to wandb project\n\n        Parameters:\n            ints\/floats of values to be logged by wandb logger\n            \n        Returns:\n            Nothing\n    \n    \n    \"\"\"\n    \n    metric_dict = {'train_loss': train_loss, 'train_f1': train_f1, 'train_auc': train_auc, 'train_mrr': train_mrr,\n                  'val_loss': val_loss, 'val_f1': val_f1, 'val_auc': val_auc, 'val_mrr': val_mrr, 'epoch': idx}\n    \n    wandb.log(metric_dict)","818cba8a":"result_list = []\nfor i in range(args.n_folds):\n    result = train_fold(args, x_adj, x_feature, fold = i)\n    result_list.append(result)","11b08072":"metrics = ['f1', 'auc', 'mrr']\n\nfor m in metrics:\n    maxi = 0\n    for idx, l in enumerate(result_list):\n        maxi += max(l[m], key = lambda x: x[1])[1] \/ args.n_folds\n    \n    print(f'Average of max validation {m} over all folds: ', np.round(maxi, 3))","ee29c618":"plot_results(result_list)","06ec0e32":"def test(args, n_classes):\n\n    \"\"\"\n    Returns test dataframe with 'preds_list' and 'preds' as two new columns. 'preds_list' has a list for each description with \n    predictions sorted in descending order of their softmax score. It can be used for test mrr evaluation. 'preds' column has the first entry\n    of the 'preds_list' list for each sample. \n\n        Parameters:\n            args (class): Class containing variables specifying values necessary for training model\n            n_classes (int): number of classes\n            \n        Returns:\n            dataframe\n    \n    \n    \"\"\"\n\n    num_classes = n_classes\n    window_size = args.window_size\n\n    print('building graphs for training')\n    x_adj, x_feature = build_graph(start=len(traindf), end=len(traindf) + len(testdf), weighted_graph = True)\n    \n    testdataset = GraphDataset(x_adj, x_feature)\n    testloader = GraphDataLoader(testdataset, batch_size = args.batch_size, shuffle = False)\n    \n    model = GATClassifier(args.embedding_dim, args.hidden_dim, args.num_heads, num_classes)\n    model_list = load_models(model, args.n_folds)\n    \n    pred_list = []\n    \n    with torch.no_grad():\n        for idx, G in enumerate(tqdm(testloader)):\n            h = G.ndata['feat'].float()\n            logits = 0\n            for mod in model_list:\n                log = mod(G, h)\n                # blending of logits from all 5 models. This helps in getting more robust predictions. \n                logits += log.softmax(-1) \/ args.n_folds\n\n            pred_soft = logits.detach().cpu().numpy()\n            pred_list.append(pred_soft)\n\n        preds = np.concatenate(pred_list)\n    \n    tags = []\n\n    for sample in preds:\n        sample = sample.argsort(-1)[ : :-1]\n        x  = [idx2label[i] for i in sample]\n        tags.append(f'{x}')\n    \n    \n    testdf['preds_list'] = tags\n    \n    preds = preds.argmax(-1)\n    preds = [idx2label[i] for i in preds]\n    \n    testdf['preds'] = preds\n    \n    return testdf","35fe48a7":"def load_models(model, folds):\n\n    \"\"\"\n    Returns list of models loaded with pre-trained wegihts.\n        Parameters:\n            model (nn.Module): Class of model\n            folds (int): number of folds. The number of models saved is equal to number of folds used for training. \n            \n        Returns:\n            list \n    \n    \n    \"\"\"\n    model_list = []\n    \n    for i in range(folds):\n        print('Loading weights')\n        model.load_state_dict(torch.load(f'\/kaggle\/working\/fold-{i}.pt'))\n        model.eval()\n        model_list.append(model)\n        \n    return model_list","95479530":"df = test(args, 62)","08a80428":"check_train_test_word_overlap()","d24a2274":"df.to_csv('submission.csv', index = False)","4cce7e55":"df.head()","27ad75f9":"subdf = pd.read_csv('.\/submission.csv')\none_predlist = eval(subdf['preds_list'][0])","77d6c3a8":"subdf.head()","87147bd1":"Now, I create mapping from labels to ids and ids to labels. This will facilitate easy conversion from one space to other. Then, I convert all labels to ids. We see that there are 62 unique industry tags.","95a38b74":"Now I apply tfidf or count vectorizer over the text and then apply TruncatedSVD first and then TSNE to get two feature columns per sample. Then, I plot the data points labelwise. TruncatedSVD is faster than TSNE, that is why, I first applied TruncatedSVD to reduce the number of columns obtained from either tfidf or count vectorizer. Intuitively, points belonging to similar tags should lie close to each other and those having very different tags should lie far. We see the same behaviour in the below plots. ","0c8b69e8":"# Make predictions on test set","75c25593":"# Build graphs","782d7cab":"Use wandb to track different metrics. You can see all the logged curves over here: https:\/\/wandb.ai\/vbagal\/gnn?workspace=user-vbagal","c4cde946":"# Install Libraries","fa7855d8":"I plot labelwise normalized distribution of number of words and number of unique words in the next two plots. Even though the 'Oil & Gas Storage & Transportation' has longer texts on average than 'Asset Management & Custody Banks', the average of number of unique words is more or less same. ","6cf542e9":"**This number is quite low which means that the words in the test dataset description are significantly different from those in the train dataset. Indeed, there is a significant distribution shift.**","ef6291af":"Now, I plot the distribution of labelwise mean of text length and unique text length. We see that most of the labels have mean text length around 82.","4a414195":"I have arranged the notebook in the following way:\n1. Install necessary libraries and import them.\n2. Define some functions for EDA\n3. Preprocess text and do the EDA\n4. Load Embeddings and check coverage\n5. Build graphs for each sample\n6. Define Dataset and Models\n7. Define functions for training and validating\n8. Make stratified splits and train models on different splits. Log results in real time using wandb\n9. Plot the evaluation curves\n10. Make predictions on test set by blending the predictions of saved models\n\n\nI tried improving the method at 3 levels. \n1. **Input level**: Trained a model on unweighted and weighted graphs. Weighted graphs gave better results. So, continued with them.\n2. **Model level**: Implemented Graph Convolutional NN and Graph Attention NN with different readout functions like maxpooling, avgpooling and combination of them. Graph Attention NN with dropout and avgpooling as the readout function gave best metric scores across all folds\n3. **Output level**: Used weighted and unweighted Cross Entropy Loss. Weights were assigned to each class as the inverse of their frequency. Unweighted gave better results\n\n\n**Link for comparison between different methods**: https:\/\/wandb.ai\/vbagal\/gnn?workspace=user-vbagal\n\n\n**1. Average of max validation f1 over all folds:  0.609**\n**2. Average of max validation auc over all folds:  0.962**\n**3. Average of max validation mrr over all folds:  0.734**","c35d626d":"We see that the clusters overlap because the 3 industries are very similar. We also spot that some points lie very far away from clusters. Such points might be difficult to correctly classify.","59d4c519":"Check the overlap of words in the glove embeddings and the words in the descriptions. We see that only around 65% of unique words in the descriptions have the glove embeddings. But those 65% words constitute areound 95% of total words in the description. In the oov, we see that many words have hyphen in between them. Let us remove that and check the overlap again. ","037257de":"Now I check the count of top_k ngrams for some industry tags. This will allow us to spot co-occurences. While building graph, an edge will be created between co-occuring words. You can use Tfidf or Count vectorizer and also visualize different ngrams. \n\nWe see that the names of months and dates occur many times. Also, the word 'company' occurs many times. I don't think these tokens provide much info. So, we will remove it with further processing.","bbec2ff5":"Following Zhang et al., 'Every Document Owns Its Structure: Inductive Text Classification via Graph Neural Networks', ACL 2020, I construct per sample graph by representing unique words as vertices and co-occurrences between words as edges. The co-occurrences describe the relationship of words that occur within a fixed size sliding window (length 3 at default) and they are undirected in the graph","742477a6":"Args for building graphs and training model","d66338db":"We can clearly see 3 different clusters because the 3 industries are very different. We find some intermingling between the 'Advertising' points and 'Consumer Finance' points. Such samples might be difficult to classify.","ab649bd5":"Here, we see that 135 is the max text length. I think we can use the whole text for building graphs as 135 is not that big. But, if it becomes computationally heavy to train with max text length of 135, we can truncate down to around 95 which will include around 75% of full length samples. ","f62cfb63":"# Import Libraries","ebaa53b6":"I plot the wordcloud again","a979597e":"Now lets check the distribution of count of labels. For that, I plot the normalized count of top 10 and bottom 10 labels. The top_k argument allows to plot the count of the 'k' most prevalent industry tags and 'reverse' argument. The printed values indicate the id to label mapping for the 10 labels. The xaxis ticks are ids because the labels were quite big and it wasn't aesthetic. \n\nFrom this plot, we see that 'Regional Banks' and 'Real Estate Operating Companies' are the most and least common tags constituting around 9% and 0.8% of total labels respectively. So, the data is highly imbalanced. While training our model, using weighted loss function with inverse of class frequency as weights might help. ","25937299":"To get one prediction list, use the following sample code.","6ea7d32b":"Train the model on all 5 folds","dd8bb22f":"I check the maximum and minimum of the labelwise mean text length and their industry tags. I use these tags to plot their distributions just to check if there is large difference in them. ","ff9921d2":"# Exploratory Data Analysis","dc02b60c":"# Preprocessing for Glove Embeddings","1ca2904c":"Here I load the train dataset and remove rows with na values","7ee9f622":"Now we definitely see more informative co-occurences than before due to our preprocessing. ","fddc25db":"Now, I filter the text by removing stopwords and punctuations. I also apply stemmer on the text. Then I calculate num of words and num of unique words in each text. ","14171180":"Create a list of unique words in the whole dataset. Create a word to id dictionary and then a dictionary for out of vocabulary words as keys and their word embeddings following uniform distribution from -0.1 to 0.1","2ffb2a7f":"# Make Splits and Train","21fc4543":"Create a stratified 5 fold split for similar quantity of samples of each labels in all the folds. Print the value counts of each target in all folds just for seeing whether they are similar. Random state is very important because we want to compare different models on the same splits.","52283a2c":"Here, I try to see which words are labelwise most common. In case of biotechnology, words like product, candidate, company, therapeutic, cancer, clinical trial, etc. are most common. While choosing pre-trained embeddings, we should try to maximize the overlap between the words in text and those in the embedding matrix. Jargon words like receptor, inhibitor, therapeutic might not be present in the embedding matrix.","3be16f4c":"# EDA Functions","f7a5103c":"Load glove embeddings of 50 dimnesions","7a6618df":"Now the coverage is around 71%. That is a significant improvement! This will allow us to use pre-trained embeddings for more words in the text. I don't see a straightforward way of increasing this coverage further. Now, lets build a graph. ","1c1cc16f":"Define functions for fold training, 1 epoch training and validation","18729ea6":"For interactive comparison between different runs, please see this link: https:\/\/wandb.ai\/vbagal\/gnn?workspace=user-vbagal","18af242f":"Create the dataset and model classes"}}