{"cell_type":{"3b69a60c":"code","4b3853cc":"code","c3426970":"code","753014cf":"code","a9e2616e":"code","fe53900a":"code","988e2f9b":"code","d354caec":"code","c16c99e3":"code","adf13735":"code","0479dc0f":"code","c5d702d0":"code","f2ff6201":"code","89abc584":"code","78378cc1":"code","e9941933":"code","034c920f":"code","f7168bc2":"code","d8d4def4":"code","2204b5f1":"code","49c1bc69":"code","e36ce819":"code","08a38001":"code","d1e4d9f4":"code","751246c6":"code","01bfb31b":"code","5e7395d1":"code","ddaa858d":"code","b8911525":"code","df6d8a53":"code","64b71b3b":"code","73869df7":"code","31e7b02d":"code","9c223fc5":"code","1a68869a":"code","c7317cde":"code","1ecda653":"code","1419be13":"code","0a1b63d6":"code","9805c403":"code","689018e9":"code","14fe4280":"code","d0c85f9b":"code","3a0be618":"code","40574389":"code","bb36eb44":"code","8ff5ceed":"code","fff3ad25":"code","1d75621a":"code","64875fd5":"code","88a6e83f":"code","45a57ddc":"code","e4b006b7":"code","4c38feb4":"code","01ce29e1":"code","7d7595c5":"code","28801a84":"code","51e5157a":"code","789f0190":"code","f6cf2c12":"code","e9e35819":"code","ef5e41fa":"code","477bba9d":"code","bd07d905":"code","4c6eb6a3":"code","fad6619d":"code","f3f6e300":"code","1a2d9def":"code","0fd925c7":"code","4afb926c":"code","cc72d818":"code","f570373f":"code","51890b18":"markdown","a4b54873":"markdown","3c813b2e":"markdown","e62e7ad3":"markdown","af4701c2":"markdown","6ee0d5ca":"markdown","e873edae":"markdown","3238d56b":"markdown","8a33d175":"markdown","bf00d714":"markdown","067d2274":"markdown","240e5270":"markdown","23b62bb4":"markdown","90f69f43":"markdown","a4382b27":"markdown","03c4b0bb":"markdown","cf6004ec":"markdown","79226527":"markdown","ece63a7c":"markdown","65cfe097":"markdown","f57fff59":"markdown","4285cfb1":"markdown","3d2ff0e0":"markdown","6f031a1c":"markdown","db1d0c63":"markdown","e530c0d0":"markdown","2b334d61":"markdown","f2c156b0":"markdown","3bdd1ba7":"markdown"},"source":{"3b69a60c":"import string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\nfrom fastai.text import *\nfrom fastai.callbacks import *\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport os\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")","4b3853cc":"def random_colours(number_of_colors):\n    '''\n    Simple function for random colours generation.\n    Input:\n        number_of_colors - integer value indicating the number of colours which are going to be generated.\n    Output:\n        Color in the following format: ['#E86DA4'] .\n    '''\n    colors = []\n    for i in range(number_of_colors):\n        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n    return colors","c3426970":"train = pd.read_csv('..\/input\/60k-stack-overflow-questions-with-quality-rate\/data.csv')","753014cf":"train.info()","a9e2616e":"train.head()","fe53900a":"temp = train.sample(frac=0.5).groupby('Y').count()['Body'].reset_index().sort_values(by='Body',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","988e2f9b":"fig = go.Figure(go.Funnelarea(\n    text =temp.Y,\n    values = temp.Body,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Question Quality Distribution\"}\n    ))\nfig.show()","d354caec":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","c16c99e3":"results_jaccard=[]\n\nfor ind,row in train.iterrows():\n    sentence1 = row.Title\n    sentence2 = row.Body\n\n    jaccard_score = jaccard(sentence1,sentence2)\n    results_jaccard.append([sentence1,sentence2,jaccard_score])","adf13735":"jaccard = pd.DataFrame(results_jaccard,columns=[\"Title\",\"Body\",\"jaccard_score\"])\ntrain = train.merge(jaccard,how='outer')","0479dc0f":"train['Num_words_body'] = train['Body'].apply(lambda x:len(str(x).split())) #Number Of words in Selected Text\ntrain['Num_words_title'] = train['Title'].apply(lambda x:len(str(x).split())) #Number Of words in main text\ntrain['difference_in_words'] = abs(train['Num_words_body'] - train['Num_words_title']) #Difference in Number of words text and Selected Text","c5d702d0":"train.head()","f2ff6201":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train['Num_words_body'], shade=True, color=\"r\").set_title('Kernel Distribution of Number Of words')\np1=sns.kdeplot(train['Num_words_title'], shade=True, color=\"b\")\nplt.xlim(0,300)","89abc584":"train.Y.unique()","78378cc1":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['Y']=='HQ']['difference_in_words'], shade=True, color=\"b\").set_title('Kernel Distribution of Difference in Number Of words')\np2=sns.kdeplot(train[train['Y']=='LQ_CLOSE']['difference_in_words'], shade=True, color=\"r\")\np2=sns.kdeplot(train[train['Y']=='LQ_EDIT']['difference_in_words'], shade=True, color=\"g\")\nplt.legend(labels=['HQ','LQ_CLOSE','LQ_EDIT'])\nplt.xlim(-20,500)","e9941933":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['Y']=='HQ']['jaccard_score'], shade=True,).set_title('KDE of Jaccard Scores across different Quality Question')\np2=sns.kdeplot(train[train['Y']=='LQ_CLOSE']['jaccard_score'], shade=True, )\np3=sns.kdeplot(train[train['Y']=='LQ_EDIT']['jaccard_score'], shade=True, )\nplt.legend(labels=['HQ','LQ_CLOSE','LQ_EDIT'])\nplt.xlim(-0.05,0.4)","034c920f":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","f7168bc2":"train['Title'] = train['Title'].apply(lambda x:clean_text(x))\ntrain['Body'] = train['Body'].apply(lambda x:clean_text(x))","d8d4def4":"train.head()","2204b5f1":"train['temp_list'] = train['Body'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","49c1bc69":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Body', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","e36ce819":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words in the body')\nfig.show()","08a38001":"def remove_stopword(x):\n    return [y for y in x if y not in stopwords.words('english')]","d1e4d9f4":"train['temp_list1'] = train['Title'].apply(lambda x:str(x).split()) #List of words in every row for text\ntrain['temp_list1'] = train['temp_list1'].apply(lambda x:remove_stopword(x)) #Removing Stopwords","751246c6":"top = Counter([item for sublist in train['temp_list1'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","01bfb31b":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Title', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","5e7395d1":"hq = train[train['Y']=='HQ']\nlq_edit = train[train['Y']=='LQ_EDIT']\nlq_close = train[train['Y']=='LQ_CLOSE']","ddaa858d":"#MosT common HQ words\ntop = Counter([item for sublist in hq['temp_list'] for item in sublist])\ntemp_p = pd.DataFrame(top.most_common(20))\ntemp_p.columns = ['Common_words','count']\ntemp_p.style.background_gradient(cmap='Greens')","b8911525":"fig = px.bar(temp_p, x=\"count\", y=\"Common_words\", title='Most Commmon HQ words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","df6d8a53":"#MosT common lq_edit words\ntop = Counter([item for sublist in lq_edit['temp_list'] for item in sublist])\ntemp_n = pd.DataFrame(top.most_common(20))\ntemp_n = temp_n.iloc[1:,:]\ntemp_n.columns = ['Common_words','count']\ntemp_n.style.background_gradient(cmap='Reds')","64b71b3b":"fig = px.treemap(temp_n, path=['Common_words'], values='count',title='Tree Of Most Common LQ_EDIT Words')\nfig.show()","73869df7":"#MosT common lq_close words\ntop = Counter([item for sublist in lq_close['temp_list'] for item in sublist])\ntemp_n = pd.DataFrame(top.most_common(20))\ntemp_n = temp_n.loc[1:,:]\ntemp_n.columns = ['Common_words','count']\ntemp_n.style.background_gradient(cmap='Reds')","31e7b02d":"fig = px.bar(temp_n, x=\"count\", y=\"Common_words\", title='Most Commmon LQ_CLOSE words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","9c223fc5":"fig = px.treemap(temp_n, path=['Common_words'], values='count',title='Tree Of Most LQ_CLOSE Words')\nfig.show()","1a68869a":"raw_text = [word for word_list in train['temp_list1'] for word in word_list]","c7317cde":"def words_unique(segment,numwords,raw_words):\n    '''\n    Input:\n        segment - Segment category (ex. 'HQ,LQ_EDIT');\n        numwords - how many specific words do you want to see in the final result; \n        raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n\n    '''\n    allother = []\n    for item in train[train.Y != segment]['temp_list1']:\n        for word in item:\n            allother .append(word)\n    allother  = list(set(allother ))\n    \n    specificnonly = [x for x in raw_text if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in train[train.Y == segment]['temp_list1']:\n        for word in item:\n            mycounter[word] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n    \n    return Unique_words","1ecda653":"Unique_P= words_unique('HQ', 10, raw_text)\nprint(\"The top 10 unique words in HQ are:\")\nUnique_P.style.background_gradient(cmap='Greens')","1419be13":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_P['count'], labels=Unique_P.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique HQ Words')\nplt.show()","0a1b63d6":"Unique_lqedit = words_unique('LQ_EDIT', 10, raw_text)\nprint(\"The top 10 unique words in LQ_EDIT are:\")\nUnique_lqedit.style.background_gradient(cmap='Reds')","9805c403":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.rcParams['text.color'] = 'black'\nplt.pie(Unique_lqedit['count'], labels=Unique_lqedit.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique LQ_EDIT Words')\nplt.show()","689018e9":"Unique_N= words_unique('LQ_CLOSE', 10, raw_text)\nprint(\"The top 10 unique words in LQ_CLOSE are:\")\nUnique_N.style.background_gradient(cmap='Oranges')","14fe4280":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_N['count'], labels=Unique_N.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique LQ_CLOSE Words')\nplt.show()","d0c85f9b":"def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(20.0,8.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'u', \"im\"}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \nd = '\/kaggle\/input\/masks-for-wordclouds\/'","3a0be618":"plot_wordcloud(hq.Body,color='white',max_font_size=100,title_size=30,title=\"WordCloud of HQ Questions\")","40574389":"plot_wordcloud(lq_edit.Body,color='white',max_font_size=100,title_size=30,title=\"WordCloud of LQ_EDIT Questions\")","bb36eb44":"plot_wordcloud(lq_close.Body,color='white',max_font_size=100,title_size=30,title=\"WordCloud of LQ_CLOSE Questions\")","8ff5ceed":"df = train[['Title','Body','Y']].copy()\ndf.head()","fff3ad25":"path = Path('\/kaggle\/input\/')","1d75621a":"data_lm = (TextList.from_df(df, path, cols=['Title','Body'] ) # Create A text list for model\n                   .split_by_rand_pct(0.2)  # how to split data, 80% train, 20% validation\n                   .label_for_lm() # label according to a language model\n                   .databunch(bs=64)) # create a databunch","64875fd5":"data_lm.save('\/kaggle\/working\/data_lm.pkl')","88a6e83f":"data_lm.show_batch(rows=5)","45a57ddc":"learn = language_model_learner(data_lm,AWD_LSTM,drop_mult=0.4,\n                               metrics=[accuracy,Perplexity()],model_dir='\/kaggle\/working\/').to_fp16()","e4b006b7":"callbacks = SaveModelCallback(learn, monitor=\"perplexity\", mode=\"min\", name=\"best_lang_model\")","4c38feb4":"lr = 5e-02\nmoms = (0.8, 0.7)\nwd = 0.1","01ce29e1":"learn.fit_one_cycle(8, slice(lr), moms=moms, wd=wd, callbacks=[callbacks])","7d7595c5":"learn.load('best_lang_model');","28801a84":"txt = 'the question is very simple'\n[learn.predict(txt,n_words=30,temperature=0.5) for i in range(5)]","51e5157a":"learn.save_encoder('ftenc')","789f0190":"learn = None\ngc.collect()","f6cf2c12":"data_cls = (TextList.from_df(df, path, cols=['Title','Body'], vocab=data_lm.vocab)\n            # Creating a textlist for lang model df--> dataframe , cols = Columns of df you want to include in classifier model , vocab=we will use same vacab we use to create a language model\n                    .split_by_rand_pct(0.2,seed=64)\n            #   will take 20% of text as validation set\n                    .label_from_df(cols='Y')\n            # label the classifier from dataframe cols= target columns name\n                    .databunch(bs=128))\n            # creates a databunch","e9e35819":"data_cls.show_batch(rows=5)","ef5e41fa":"clf = text_classifier_learner(data_cls, AWD_LSTM, metrics=[accuracy], drop_mult=0.3,model_dir='\/kaggle\/working\/').to_fp16()\nclf.load_encoder('\/kaggle\/working\/ftenc');","477bba9d":"clf.summary()","bd07d905":"gc.collect()","4c6eb6a3":"cb = SaveModelCallback(clf, monitor=\"accuracy\", mode=\"max\", name=\"best_clf\")","fad6619d":"clf.unfreeze()\nclf.fit_one_cycle(8, 1e-2 ,moms=(0.8,0.7), callbacks=[cb])","f3f6e300":"clf.load('best_clf');","1a2d9def":"interp = TextClassificationInterpretation.from_learner(clf)","0fd925c7":"interp.show_intrinsic_attention(\"why are java optionals immutable\")","4afb926c":"interp.show_intrinsic_attention(\"why ternary operator in swift is so picky\")","cc72d818":"interp.plot_confusion_matrix(figsize=(5,5))","f570373f":"interp.show_top_losses(10)","51890b18":"## It's Time For WordClouds\n\nWe will be building wordclouds in the following order:\n\n* WordCloud of HQ Questions\n* WordCloud of LQ_EDIT Questions\n* WordCloud of LQ_CLOSE Questions","a4b54873":"* Top 3 Question words are Regarding **C, python, and error**","3c813b2e":"<h2> <span style=\"color:Red\">I hope you Liked my kernel. An upvote is a gesture of appreciation and encouragement, to keep improving my efforts ,be kind to show one.<\/h2>","e62e7ad3":"* Below a Helper Function that generates random colors","af4701c2":"## Generating Meta Features\n\n* Difference In Number Of words of title and body\n* Jaccard Similarity Scores between title and body\n\nFor what who don't know what Jaccard Similarity is : https:\/\/www.geeksforgeeks.org\/find-the-jaccard-index-and-jaccard-distance-between-the-two-given-sets\/\n","6ee0d5ca":"## HQ Questions","e873edae":"* Let's look at the distribution of Meta-Features","3238d56b":"### Training the language model","8a33d175":"# About this Notebook\n\nIn this kernel, I will briefly explain the structure of dataset.I will generate and analyze metafeatures. Then, I will visualize the dataset using Matplotlib, seaborn and Plotly to gain as much insight as I can . Also I will approach this problem as an NLP Classification problem to build a model\n\nIn case you are just starting with NLP here is a guide to Approach almost any NLP Problem by Grandmaster [**@Abhishek Thakur**](https:\/\/www.slideshare.net\/abhishekkrthakur\/approaching-almost-any-nlp-problem)\n\n**<span style=\"color:Red\">If you find this kernel useful, Please Upvote it , it motivates me to write more Quality content**","bf00d714":"## Language Model for questions","067d2274":"# Most Common words in Title\n\nLet's also look at the most common words in Title","240e5270":"* **Of course question body will have more words than the title**","23b62bb4":"**Now It will be more interesting to see the differnce in number of words and jaccard_scores across different Segment**","90f69f43":"## Now Time to Build a Classifier","a4382b27":"## Let's Look at Unique Words in each Segment\n\nWe will look at unique words in each segment in the Following Order:\n* HQ\n* LQ_EDIT\n* LQ_CLOSE","03c4b0bb":"# Most common words Question Quality Wise\n\nLet's look at the most common words in different question qualities","cf6004ec":"# EDA","79226527":"**By Looking at the Unique Words of each segment ,we now have much more clarity about the data,these unique words are very strong determiners of segment of questions**","ece63a7c":"## EDA of Conclusion\n* Target distribution is almost identical for all 3 categories\n* `LQ_EDIT` questions have less difference in num of words between **Body** and **Title**.\n","65cfe097":"* Distribution in 50% of data","f57fff59":"# Modeling the Problem as NLP Text Classification Task\n\n\n\n**Text classification is the process of assigning tags or categories to text according to its content. \nIt's one of the fundamental tasks in Natural Language Processing (NLP) with broad applications such as sentiment analysis, topic labeling, spam detection, and intent detection.**\n\n* We will use a AWD_LSTM arch.\n* First we will build a language model that better understands questions language.\n* Then using the language model we will build a classifier","4285cfb1":"* We can see words like **i,to , a, and, the,is** are common in all three segments .","3d2ff0e0":"* We have 60k rows 6 columns","6f031a1c":"## Lets see our top losses","db1d0c63":"## Cleaning the Corpus\n\nNow Before We Dive into extracting information out of words in title and body,let's first clean the data","e530c0d0":"## Most Common words in our Body","2b334d61":"## Classfier Interpretation","f2c156b0":"**OOPS!** While we cleaned our dataset we didnt remove the stop words and hence we can see one of the most common words is 'to'. \nLet's try again after removing the stopwords.","3bdd1ba7":"## Classifier Model Summary"}}