{"cell_type":{"ed037a6f":"code","5c6c23b9":"code","e2d6e53e":"code","56635e81":"code","397cc60b":"code","203123a8":"code","0e92a270":"code","bd0779a1":"code","3a8173b1":"code","6e19e023":"code","589cc88c":"code","100196aa":"code","56f630ee":"code","b530c965":"code","3202aab8":"code","2ff882f7":"code","94d30f4c":"code","dfb1639a":"code","8293064e":"code","dc017f22":"code","7baab558":"code","e7eb8647":"code","a7e3f3b4":"code","90d42fcb":"code","68c612d0":"code","ab7ec932":"code","0111552a":"code","9e004483":"code","63d84bf8":"code","fa959c7e":"code","021d709b":"code","eb7492e8":"code","17a36e2d":"code","bec46cad":"code","097b12c3":"code","9e726b46":"code","6bfe4fb2":"code","b61c731f":"code","4305ea84":"code","ee966ccd":"code","cbfa046d":"code","f8c8357d":"code","797ac87b":"code","e42c4159":"code","f6deb0b0":"code","a905560f":"code","508cb9f0":"code","b757be21":"code","7dfa5afb":"markdown","ed15ccc1":"markdown","5f7ad9a8":"markdown","84d79b15":"markdown","e981efed":"markdown","fdcbaa70":"markdown","9b84f93c":"markdown","15a048d7":"markdown","3d6f2bbb":"markdown","f27e6218":"markdown","c1484cd3":"markdown","6b792ea5":"markdown","6a7468aa":"markdown","e3b96d16":"markdown","4815bace":"markdown","b0ddd095":"markdown","2aa025de":"markdown","310d1a4d":"markdown"},"source":{"ed037a6f":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom matplotlib.pylab import rcParams\nrcParams['figure.figsize'] = 15,15\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import train_test_split,StratifiedShuffleSplit,StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score,accuracy_score ,confusion_matrix\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.preprocessing import RobustScaler\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom tqdm.notebook import tqdm ,tnrange","5c6c23b9":"train_data = pd.read_csv('\/kaggle\/input\/avcrosssell\/train.csv')\n\nprint(train_data.shape)\n\ntrain_data.head()","e2d6e53e":"test_data = pd.read_csv('\/kaggle\/input\/avcrosssell\/test.csv')\n\nprint(test_data.shape)\n\ntest_data.head()","56635e81":"x = train_data[~train_data.iloc[:,1:].duplicated(keep = 'first')]\n\n#reemove confusing ids\n\ntrain_data = train_data[~train_data.id.isin(x[x.iloc[:,1:-1].duplicated(keep = False)].id)]","397cc60b":"def nullColumns(train_data):\n    \n    list_of_nullcolumns =[]\n    \n    for column in train_data.columns:\n        \n        total= train_data[column].isna().sum()\n        \n        try:\n            \n            if total !=0:\n                \n                print('Total Na values is {0} for column {1}' .format(total, column))\n                \n                list_of_nullcolumns.append(column)\n        \n        except:\n            \n            print(column,\"-----\",total)\n    \n    print('\\n')\n    \n    return list_of_nullcolumns\n\n\ndef percentMissingFeature(data):\n    \n    data_na = (data.isnull().sum() \/ len(data)) * 100\n    \n    data_na = data_na.drop(data_na[data_na == 0].index).sort_values(ascending=False)[:30]\n    \n    missing_data = pd.DataFrame({'Missing Ratio' :data_na})\n    \n    return data_na\n\n\ndef plotMissingFeature(data_na):\n    \n    f, ax = plt.subplots(figsize=(15, 12))\n    \n    plt.xticks(rotation='90')\n    \n    if(data_na.empty ==False):\n        \n        sns.barplot(x=data_na.index, y=data_na)\n        \n        plt.xlabel('Features', fontsize=15)\n        \n        plt.ylabel('Percent of missing values', fontsize=15)\n        \n        plt.title('Percent missing data by feature', fontsize=15)","203123a8":"print('train data')\n\nprint(nullColumns(train_data))\n\nprint(percentMissingFeature(train_data))\n\nprint('\\n')\n\nprint('test_data')\n\nprint(nullColumns(test_data))\n\nprint(percentMissingFeature(test_data))","0e92a270":"train_data.describe()","bd0779a1":"response = train_data.loc[:,\"Response\"].value_counts().rename('Count')\nplt.xlabel(\"Response\")\nplt.ylabel('Count')\nsns.barplot(response.index , response.values).set_title('Response')","3a8173b1":"response","6e19e023":"sns.distplot(train_data['Vintage'])","589cc88c":"sns.distplot(train_data['Annual_Premium'])","100196aa":"sns.distplot(test_data['Annual_Premium'])","56f630ee":"sns.distplot(train_data['Age'])","b530c965":"sns.distplot(test_data['Age'])","3202aab8":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n\nf, axes = plt.subplots(2, 1, figsize=(15, 15))\n\nmale = train_data[train_data['Gender'] =='Male'][\"Response\"].value_counts().rename('Count')\n\nfemale = train_data[train_data['Gender'] =='Female'][\"Response\"].value_counts().rename('Count')\n\nsns.barplot(male.index,male,  color=\"b\", ax=axes[0]).set_title('Gender : Male')\n\nsns.barplot(female.index,female,   color=\"r\", ax=axes[1]).set_title('Gender : Female')\n\nplt.setp(axes, yticks = np.arange(0,50000,10000))\n\nfor ax in f.axes:\n    \n    plt.sca(ax)\n    \n    plt.xticks(rotation=0)\n\nplt.tight_layout()","2ff882f7":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n\nf, axes = plt.subplots(2, 1, figsize=(15, 15))\n\ndl0 = train_data[train_data['Driving_License'] ==0][\"Response\"].value_counts().rename('Count')\n\ndl1 = train_data[train_data['Driving_License'] ==1][\"Response\"].value_counts().rename('Count')\n\nsns.barplot(dl0.index,dl0,  color=\"b\", ax=axes[0]).set_title('Driving_License : No')\n\nsns.barplot(dl1.index,dl1,   color=\"r\", ax=axes[1]).set_title('Driving_License : Yes')\n\nplt.setp(axes, yticks = np.arange(0,200000,1000))\n\nfor ax in f.axes:\n    \n    plt.sca(ax)\n    \n    plt.xticks(rotation=0)\n\nplt.tight_layout()","94d30f4c":"sns.set(style=\"white\", palette=\"muted\", color_codes=True)\n\nf, axes = plt.subplots(2, 1, figsize=(15, 15))\n\npi0 = train_data[train_data['Previously_Insured'] ==0][\"Response\"].value_counts().rename('Count')\n\npi1 = train_data[train_data['Previously_Insured'] ==1][\"Response\"].value_counts().rename('Count')\n\nsns.barplot(dl0.index,dl0,  color=\"b\", ax=axes[0]).set_title('Previously_Insured : No')\n\nsns.barplot(dl1.index,dl1,   color=\"r\", ax=axes[1]).set_title('Previously_Insured : Yes')\n\nplt.setp(axes, yticks = np.arange(0,50000,5000))\n\nfor ax in f.axes:\n    \n    plt.sca(ax)\n    \n    plt.xticks(rotation=0)\n\nplt.tight_layout()","dfb1639a":"train_data['Policy_Region'] = train_data['Policy_Sales_Channel'].astype(str)+'_'+train_data['Region_Code'].astype(str)\n\ntest_data['Policy_Region'] = test_data['Policy_Sales_Channel'].astype(str)+'_'+test_data['Region_Code'].astype(str)\n\ntrain_data['Vehicle_Age_License'] = train_data['Vehicle_Age'].astype(str)+'_'+train_data['Driving_License'].astype(str)\n\ntest_data['Vehicle_Age_License'] = test_data['Vehicle_Age'].astype(str)+'_'+test_data['Driving_License'].astype(str)\n","8293064e":"cat_features = ['Gender','Driving_License','Region_Code','Previously_Insured',\n                'Vehicle_Damage','Policy_Sales_Channel','Policy_Region',\n                'Vehicle_Age','Vintage','Annual_Premium','Vehicle_Age_License']\n\ncont_features = ['Age']\n\nlabel = 'Response'","dc017f22":"def encode_cat_cols(train, test, cat_cols): #target\n\n    train_df = train_data.copy()\n    \n    test_df = test_data.copy()\n    \n    # Making a dictionary to store all the labelencoders for categroical columns to transform them later.\n    \n    le_dict = {}\n\n    for col in cat_cols:\n        \n        if col!= 'Vehicle_Age':\n        \n            le = LabelEncoder()\n\n            le.fit(train_df[col].unique().tolist() + test_df[col].unique().tolist())\n\n            train_df[col] = le.transform(train_df[[col]])\n\n            test_df[col] = le.transform(test_df[[col]])\n\n            le_dict[col] = le\n        \n    train_df['Vehicle_Age'] = train_df['Vehicle_Age'].map({'< 1 Year':1,'1-2 Year':2,'> 2 Years':3})\n    \n    test_df['Vehicle_Age'] = test_df['Vehicle_Age'].map({'< 1 Year':1,'1-2 Year':2,'> 2 Years':3})\n\n    le = LabelEncoder()\n    \n    train_df[label] = le.fit_transform(train_df[[label]])\n    \n    le_dict[label] = le\n    \n    \n    return train_df, test_df, le_dict","7baab558":"train_df, test_df, le_dict = encode_cat_cols(train_data,test_data,cat_features)","e7eb8647":"train_df = train_df[~train_df.Policy_Sales_Channel.isin(list(set(train_df.Policy_Sales_Channel)-set(test_df.Policy_Sales_Channel)))]\n\n#test_df.loc[(test_df.Policy_Sales_Channel.isin(list(set(test_df.Policy_Sales_Channel) - set(train_df.Policy_Sales_Channel)))),'Policy_Sales_Channel'] = -1\n\ntest_df.loc[(test_df.Policy_Sales_Channel==137),'Policy_Sales_Channel'] = -1\n\ntest_df.loc[(test_df.Policy_Sales_Channel==136),'Policy_Sales_Channel'] = -1","a7e3f3b4":"#Used only for XgBoost and LightGBM\n#test_df.loc[(test_df.Annual_Premium.isin(list(set(test_df.Annual_Premium) - set(train_df.Annual_Premium)))),'Annual_Premium'] = -1","90d42fcb":"train_df['train'] = 1\n\ntest_df['train'] = 0\n\ncombined_data = pd.concat([train_df,test_df],axis =0).reset_index(drop = True).copy()","68c612d0":"premium_discretizer = KBinsDiscretizer(n_bins=8, encode='ordinal', strategy='quantile')\n\ncombined_data['Premium_Bins'] =premium_discretizer.fit_transform(combined_data['Annual_Premium'].values.reshape(-1,1)).astype(int)\n\nage_discretizer = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='quantile')\n\ncombined_data['Age_Bins'] =age_discretizer.fit_transform(combined_data['Age'].values.reshape(-1,1)).astype(int)","ab7ec932":"sns.boxplot(combined_data[combined_data['train']==1]['Response'],combined_data[combined_data['train']==1]['Age'])","0111552a":"# Age Bin demarcates two classses better(compare the follwing graph with the above one)\nsns.boxplot(combined_data[combined_data['train']==1]['Response'],combined_data[combined_data['train']==1]['Age_Bins'])","9e004483":"sns.boxplot(combined_data[combined_data['train']==1]['Response'],combined_data[combined_data['train']==1]['Annual_Premium'])","63d84bf8":"# the same can be seen after binning annual premium\nsns.boxplot(combined_data[combined_data['train']==1]['Response'],combined_data[combined_data['train']==1]['Premium_Bins'])","fa959c7e":"gender_counts = combined_data['Gender'].value_counts().to_dict()\n\ncombined_data['Gender_Counts'] = combined_data['Gender'].map(gender_counts)\n\nregion_counts = combined_data['Region_Code'].value_counts().to_dict()\n\ncombined_data['Region_counts'] = combined_data['Region_Code'].map(region_counts)\n\nvehicle_age_counts = combined_data['Vehicle_Age'].value_counts().to_dict()\n\ncombined_data['Vehicle_Age_Counts'] = combined_data['Vehicle_Age'].map(vehicle_age_counts)","021d709b":"combined_data['Nunq_Policy_Per_Region'] = combined_data.groupby('Region_Code')['Policy_Sales_Channel'].transform('nunique') \n\ncombined_data['SDev_Annual_Premium_Per_Region_Code_int'] = combined_data.groupby('Region_Code')['Annual_Premium'].transform('std').fillna(-1) \n\ncombined_data['Nunq_Region_Per_Premium'] = combined_data.groupby('Annual_Premium')['Region_Code'].transform('nunique')\n\n# 1230.45 can be split into \u201c1230\u201d and \u201c45\u201d. LGBM cannot see these pieces on its own, you need to split them.\ncombined_data['SDev_Annual_Premium_Per_Region_Code_dec'] = combined_data['SDev_Annual_Premium_Per_Region_Code_int'] %1\n\ncombined_data['SDev_Annual_Premium_Per_Region_Code_int'] =combined_data['SDev_Annual_Premium_Per_Region_Code_int'].astype(int)\n\n\ncombined_data['Avg_Policy_Region_Age'] = combined_data.groupby(['Policy_Region'])['Age'].transform('mean')\n\ncombined_data['Avg_Policy_Region_Premium'] = combined_data.groupby(['Policy_Region'])['Annual_Premium'].transform('mean') \n\ncombined_data['Avg_Region_Premium'] = combined_data.groupby(['Region_Code'])['Annual_Premium'].transform('mean')\n\ncombined_data['Nunq_Premium_Region'] = combined_data.groupby(['Annual_Premium'])['Region_Code'].transform('nunique')","eb7492e8":"#combined_data['f4'] = combined_data.groupby(['Vehicle_Age'])['Annual_Premium'].transform('max')-combined_data.groupby(['Vehicle_Age'])['Annual_Premium'].transform('min')\n#combined_data['f1'] =combined_data['Annual_Premium'] \/(combined_data['Vintage']\/365)\n\n\n#combined_data['f2'] = combined_data.groupby(['Premium_Bins'])['Annual_Premium'].transform('max')-combined_data.groupby(['Premium_Bins'])['Annual_Premium'].transform('min')\n#combined_data['f2'] = combined_data.groupby(['Annual_Premium'])['Vintage'].transform('nunique')","17a36e2d":"train_df = combined_data[combined_data['train']==1]\n\ntest_df = combined_data[combined_data['train']==0]","bec46cad":"# Remove duplicate rows---> More trustworthy CV\ncols = ['Gender', 'Age', 'Driving_License', 'Region_Code',\n       'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage',\n        'Annual_Premium','Policy_Sales_Channel', 'Vintage']\n\ntrain_df = train_df[~train_df.loc[:,cols].duplicated(keep = 'first')].reset_index(drop=True)","097b12c3":"target = train_df['Response']\n\ntrain_df = train_df.drop(columns =['train','id','Response'])\n\ntest_df = test_df.drop(columns=['train','id','Response'])","9e726b46":"test_size = 0.34\n\ntrain_df","6bfe4fb2":"def feature_importance(model, X_train):\n\n    fI = model.feature_importances_\n    \n    print(fI)\n    \n    names = X_train.columns.values\n    \n    ticks = [i for i in range(len(names))]\n    \n    plt.bar(ticks, fI)\n    \n    plt.xticks(ticks, names,rotation = 90)\n    \n    plt.show()","b61c731f":"%%time\n##LightGBM\n\ncat_features = ['Driving_License','Gender','Region_Code','Previously_Insured','Vehicle_Damage',\n                'Policy_Sales_Channel','Policy_Region','Vehicle_Age','Vintage',\n                'Annual_Premium','Vehicle_Age_License','Premium_Bins']\n\ncont_features = ['Age','Age_Bins']\n\nprobs_lgb = np.zeros(shape=(len(test_df),))\n\nscores = []\n\navg_loss = []\n\nseeds = [1]\n\nfor seed in tnrange(len(seeds)):\n    \n    print(' ')\n    \n    print('#'*100)\n    \n    print('Seed',seeds[seed])\n\n    X_train_cv,y_train_cv = train_df.copy(), target.copy()\n\n    sssf = StratifiedShuffleSplit(n_splits=5, test_size = test_size ,random_state=seed)\n    \n    for i, (idxT, idxV) in enumerate(sssf.split(X_train_cv, y_train_cv)):\n\n        print('Fold',i)\n\n        print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n\n        clf = lgb.LGBMClassifier(boosting_type='gbdt',\n                                 n_estimators=10000,\n                                 max_depth=10,\n                                 learning_rate=0.02,\n                                 subsample=0.9,\n                                 colsample_bytree=0.4,\n                                 objective ='binary',\n                                 random_state = 1,\n                                 importance_type='gain',\n                                 reg_alpha=2,\n                                 reg_lambda=2\n                                 #cat_features=cat_features\n                                )        \n        \n        h = clf.fit(X_train_cv.iloc[idxT], y_train_cv.iloc[idxT], \n                    eval_set=[(X_train_cv.iloc[idxV],y_train_cv.iloc[idxV])],\n                    verbose=100,eval_metric=['binary_logloss','auc'],\n                    early_stopping_rounds=100)\n        probs_oof = clf.predict_proba(X_train_cv.iloc[idxV])[:,1]\n        \n        probs_lgb +=clf.predict_proba(test_df)[:,1]\n        \n        roc = roc_auc_score(y_train_cv.iloc[idxV],probs_oof)\n\n        scores.append(roc)\n\n        avg_loss.append(clf.best_score_['valid_0']['binary_logloss'])\n\n        print ('LGB Val OOF AUC=',roc)\n\n        print('#'*100)\n\n        if i==0:\n            feature_importance(clf,X_train_cv)\n\nprint(\"Log Loss Stats {0:.8f},{1:.8f}\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))\n\nprint('%.8f (%.8f)' % (np.array(scores).mean(), np.array(scores).std()))","4305ea84":"%%time\n\n##XGBM\n\nprobs_xgb = np.zeros(shape=(len(test_df),))\n\nscores = []\n\navg_loss = []\n\nX_train_cv,y_train_cv = train_df.copy(), target.copy()\n\nseeds = [1]\n\nfor seed in tnrange(len(seeds)):\n    \n    print(' ')\n    \n    print('#'*100)\n    \n    print('Seed',seeds[seed])\n    \n    sssf = StratifiedShuffleSplit(n_splits=5, test_size = test_size ,random_state=seed)\n    \n    for i, (idxT, idxV) in enumerate(sssf.split(X_train_cv, y_train_cv)):\n\n        print('Fold',i)\n\n        print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n\n        clf = xgb.XGBClassifier(n_estimators=1000,\n                                max_depth=6,\n                                learning_rate=0.04,\n                                subsample=0.9,\n                                colsample_bytree=0.35,\n                                objective = 'binary:logistic',\n                                random_state = 1\n                               )        \n\n\n        h = clf.fit(X_train_cv.iloc[idxT], y_train_cv.iloc[idxT], \n                    eval_set=[(X_train_cv.iloc[idxV],y_train_cv.iloc[idxV])],\n                    verbose=100,eval_metric=['auc','logloss'],\n                    early_stopping_rounds=50)\n        \n        probs_oof = clf.predict_proba(X_train_cv.iloc[idxV])[:,1]\n        \n        probs_xgb +=clf.predict_proba(test_df)[:,1]\n\n        roc = roc_auc_score(y_train_cv.iloc[idxV],probs_oof)\n\n        scores.append(roc)\n        \n        avg_loss.append(clf.best_score)\n\n        print ('XGB Val OOF AUC=',roc)\n\n        print('#'*100)\n\n        if i==0:\n            \n            feature_importance(clf,X_train_cv)\n            \nprint(\"Log Loss Stats {0:.5f},{1:.5f}\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))\n\nprint('%.6f (%.6f)' % (np.array(scores).mean(), np.array(scores).std()))","ee966ccd":"%%time\n\n##CatBoost\n\ncat_features = ['Driving_License','Gender','Region_Code','Previously_Insured','Vehicle_Damage',\n                'Policy_Sales_Channel','Policy_Region','Vehicle_Age','Vintage','Annual_Premium',\n                'Vehicle_Age_License','Premium_Bins']\n\ncont_features = ['Age','Age_Bins']\n\nprobs_cb = np.zeros(shape=(len(test_df),))\n\nscores = []\n\navg_loss = []\n\nX_train_cv,y_train_cv = train_df.copy(), target.copy()\n\nseeds = [1]\n\nfor seed in tnrange(len(seeds)):\n    \n    print(' ')\n    \n    print('#'*100)\n    \n    print('Seed',seeds[seed])\n    \n    sssf = StratifiedShuffleSplit(n_splits=5, test_size = test_size ,random_state=seed)\n    \n    for i, (idxT, idxV) in enumerate(sssf.split(X_train_cv, y_train_cv)):\n\n        print('Fold',i)\n\n        print(' rows of train =',len(idxT),'rows of holdout =',len(idxV))\n\n        clf = CatBoostClassifier(iterations=10000,\n                                learning_rate=0.02,\n                                random_strength=0.1,\n                                depth=8,\n                                loss_function='Logloss',\n                                eval_metric='Logloss',\n                                leaf_estimation_method='Newton',\n                                random_state = 1,\n                                cat_features =cat_features,\n                                subsample = 0.9,\n                                rsm = 0.8\n                                )    \n\n        h = clf.fit(X_train_cv.iloc[idxT], y_train_cv.iloc[idxT],\n                    eval_set=[(X_train_cv.iloc[idxV],y_train_cv.iloc[idxV])],\n                   early_stopping_rounds=50,verbose = 100)\n\n        probs_oof = clf.predict_proba(X_train_cv.iloc[idxV])[:,1]\n        \n        probs_cb +=clf.predict_proba(test_df)[:,1]\n        \n        roc = roc_auc_score(y_train_cv.iloc[idxV],probs_oof)\n\n        scores.append(roc)\n\n        print ('CatBoost Val OOF AUC=',roc)\n\n        avg_loss.append(clf.best_score_['validation']['Logloss'])\n\n        if i==0:\n            \n            feature_importance(clf,X_train_cv)\n\n        print('#'*100)\n\nprint(\"Log Loss Stats {0:.8f},{1:.8f}\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))\n\nprint('%.8f (%.8f)' % (np.array(scores).mean(), np.array(scores).std()))","cbfa046d":"#!pip uninstall -q fastai -y\n\n#!pip install -q \/kaggle\/input\/fast-v2-offline\/dataclasses-0.6-py3-none-any.whl\n\n#!pip install -q \/kaggle\/input\/fast-v2-offline\/torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl\n\n#!pip install -q \/kaggle\/input\/fast-v2-offline\/torchvision-0.7.0-cp37-cp37m-manylinux1_x86_64.whl\n\n#!pip install -q \/kaggle\/input\/fast-v2-offline\/fastcore-1.0.1-py3-none-any.whl\n\n#!pip install -q \/kaggle\/input\/fast-v2-offline\/fastai-2.0.8-py3-none-any.whl\n\n#import fastai\n\n#print('fastai version :', fastai.__version__)\n\n#from fastai import *\n\n#from fastai.layers import *\n\n#from fastai.tabular.all import *\n\n#import torch.nn.functional as F\n\n#import torch\n\n#import torch.nn as nn\n\n#import torch.optim as optim\n\n#from torch.utils.data import Dataset, DataLoader","f8c8357d":"\"\"\"\ndef categorical_features(df,cat_features,uuid_col):\n    \n    cat_features_subdf = []\n    \n    for col in df.columns:\n        \n        if col in cat_features:\n            \n            cat_features_subdf.append(col)\n            \n    return cat_features_subdf\n\n\ndef continuous_features(df,uuid_col,cat_features_subdf):\n    \n    cont_features_subdf = []\n    \n    for col in df.columns:\n        \n        if col not in cat_features_subdf:\n            \n            cont_features_subdf.append(col)\n            \n    return cont_features_subdf\n    \n\"\"\"","797ac87b":"\"\"\"\"\nclass FocalLoss(nn.Module):\n    \n    def __init__(self, alpha=4, gamma=2, logits=False, reduction = 'mean'):\n        \n        super(FocalLoss, self).__init__()\n       \n        self.alpha = alpha\n        \n        self.gamma = gamma\n        \n        self.logits = logits\n        \n        self.reduction = reduction\n\n    def forward(self, inputs, targets):\n\n        targets =targets.type_as(inputs)\n        \n        if self.logits:\n            \n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n        \n        else:\n            \n            BCE_loss = F.binary_cross_entropy(inputs, targets, reduction='none')\n        \n        pt = torch.exp(-BCE_loss)\n        \n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduction is None:\n            \n            return F_loss\n        \n        else:\n            \n            return torch.mean(F_loss)\n            \n\"\"\"","e42c4159":"\"\"\"\n%%time\n\nscores = []\n\navg_loss = []\n\ncat_features = ['Gender','Driving_License','Region_Code','Previously_Insured','Vehicle_Damage','Policy_Sales_Channel','Policy_Region','Vehicle_Age','Vintage']\n\ncont_features = ['Age','Annual_Premium',]\n\n#custom sklearn metric\nroc =  skm_to_fastai(roc_auc_score,axis=0)\n\n#np.random.seed(10)\n\nprobs = np.zeros(shape=(len(test_df)))\n\n#seeds = list(np.random.randint(5,500,1))\n\nseeds = [1]\n\nfor seed in seeds:\n    \n    print(' ')\n\n    print('#'*100)\n\n    print('Seed',seed)\n\n    df = train_df.copy().sample(frac = 1.0,axis =1,random_state = seed)\n\n    cat_features_subdf= categorical_features(df,cat_features,'')\n\n    cont_features_subdf = continuous_features(df,'',cat_features_subdf)\n\n    df = pd.concat([df, target], axis=1)\n\n    sssf = StratifiedShuffleSplit(n_splits=1, test_size = 0.35 ,random_state=seed)\n\n    procs = [FillMissing,Categorify, Normalize]\n\n    for i, (idxT, idxV) in enumerate(sssf.split(train_df, target)):\n\n\n        data = TabularDataLoaders.from_df(df, procs=procs,\n                                              cont_names=cont_features_subdf, cat_names=cat_features_subdf,\n                                              y_names='Response', valid_idx=idxV, bs=256, val_bs=256)\n\n\n        learn = tabular_learner(data, y_range=(0,1), layers=[256,128,64],loss_func=BCELossFlat(),metrics= [roc])\n\n        #learn.lr_find()\n        \n        learn.fit_one_cycle(5, 1e-3)\n            \n        val_loss, val_roc = learn.validate()\n            \n        scores.append(val_roc)\n\n        avg_loss.append(val_loss)\n        \n        test_dl = learn.dls.test_dl(test_df)\n        \n        sub = learn.get_preds(dl=test_dl)\n        \n        probs += sub[0].numpy().ravel()\n            \n        print('#'*100)\n\nprint(\"Log Loss Stats {0:.5f},{1:.5f}\".format(np.array(avg_loss).mean(), np.array(avg_loss).std()))\n\nprint('%.6f (%.6f)' % (np.array(scores).mean(), np.array(scores).std()))\n\n\"\"\"","f6deb0b0":"p1 =probs_lgb\/5\n\np2 = probs_cb\/5\n\np3 = probs_xgb\/5","a905560f":"submission = pd.read_csv('..\/input\/avcrosssell\/sample_submission.csv')\n\nsubmission['Response'] =  0.7*p2+0.3*p3","508cb9f0":"submission.to_csv('submission.csv',index =False)\n\nsubmission.head()","b757be21":"np.save('lgb.npy',p1)\nnp.save('cb.npy',p2)\nnp.save('xgb.npy',p3)","7dfa5afb":"# Blending","ed15ccc1":"# Feature Binning","5f7ad9a8":"# Differentiation based on how each algorithm handles Missing values\n\n* **Catboost** has two modes for processing missing values, \u201cMin\u201d and \u201cMax\u201d.\n    * In \u201cMin\u201d, missing values are processed as the minimum value for a feature (they are given a value that is less than all existing values)\n    * This way, it is guaranteed that a split that separates missing values from all other values is considered when selecting splits\n    * \u201cMax\u201d works exactly the same as \u201cMin\u201d, only with maximum values\n\n\n\n* In **LightGBM** and **XGBoost** missing values will be allocated to the side that reduces the loss in each split","84d79b15":"# CV CatBoost","e981efed":"## CV FastAI--->Poor Performance","fdcbaa70":"# Feature Aggregations","9b84f93c":"\n# Differentiation based on how each algorithm grows leaves\n\n\n\n![](https:\/\/www.riskified.com\/wp-content\/uploads\/2019\/11\/inner-image-trees-1-1024x386.png)\n\n* **Catboost** grows a balanced tree.\n\n\n* **LightGBM** uses leaf-wise (best-first) tree growth\n    * It chooses to grow the leaf that minimizes the loss, allowing a growth of an imbalanced tree\n    * Because it doesn\u2019t grow level-wise, but leaf-wise, overfitting can  happen when data is small\n    * It is therefore important to control the tree depth\n\n\n* **XGboost** splits up to the specified max_depth hyperparameter\n    * It then starts pruning the tree backwards and removes splits beyond which there is no positive gain\n    * It uses this approach since sometimes a split of no loss reduction may be followed by a split with loss reduction","15a048d7":"# Feature Engineering","3d6f2bbb":"# Sampling Strategy : Stratified Shuffle Split\n![](https:\/\/i.stack.imgur.com\/AGv9B.png)","f27e6218":"# Concat Features","c1484cd3":"# Differentiation based on how each algorithm Splits feature\n\n**Catboost** \n* Uses oblivious decision trees\n* Before learning, the possible values of each feature are divided into buckets delimited by threshold values, creating feature-split pairs\n* Example for such pairs are: (age, <5), (age, 5-10), (age, >10) and so on. In each level of an oblivious tree, the feature-split pair that brings to the lowest loss (according to a penalty function) is selected and is used for all the level\u2019s nodes.\n\n**LightGBM** \n* Uses gradient-based one-side sampling (GOSS) that selects the split using all the instances with large gradients (i.e., large error) and a random sample of instances with small gradients\n* In order to keep the same data distribution when computing the information gain, GOSS introduces a constant multiplier for the data instances with small gradients\n* GOSS,thus achieves a good balance between increasing speed by reducing the number of data instances and keeping the accuracy for learned decision trees.\n\n**XGboost** \n* Several methods for selecting the best split\n* For example, a histogram-based algorithm that buckets continuous features into discrete bins and uses these bins to find the split value in each node\n* This method is faster than the exact greedy algorithm, which linearly enumerates all the possible splits for continuous features, but it is slower compared to GOSS that is used by LightGBM\n","6b792ea5":"# CV LightGBM","6a7468aa":"# CV XGBOOST","e3b96d16":"# Feature Counts","4815bace":"# Differentiation based on how each algorithm handle categorical feature\n\n* **Catboost** uses a combination of one-hot encoding and an advanced mean encoding\n    * For features with low number of categories, it uses one-hot encoding\n    * The maximum number of categories for one-hot encoding can be controlled by the one_hot_max_size parameter\n    * For the remaining categorical columns, CatBoost uses a method of encoding similar to mean encoding but with an additional mechanism aimed at reducing overfitting\n    * Using CatBoost\u2019s categorical encoding comes with a downside of a slower model\n\n* **LightGBM** splits categorical features by partitioning their categories into 2 subsets\n    * The basic idea is to sort the categories according to the training objective at each split\n    * This method does not necessarily improve the LightGBM model. It has comparable (and sometimes worse) performance than other methods \n\n* **XGBoost**\ndoesn\u2019t have an inbuilt method for categorical features. Encoding (one-hot, target encoding, etc.) should be performed by the user.","b0ddd095":"# Algorithms Used\n\n* **LightGBM**\n\n* **XgBoost**---> used for ensemling\n\n* **CatBoost**---> used for ensemling\n\n* **NN via FastAi**---> Poor CV","2aa025de":"# Differentiation based on how each algorithm calculates feature importance\n\n* **Catboost** has two methods\n    * The first is \u201cPredictionValuesChange\u201d. For each feature, PredictionValuesChange shows how much, on average, the prediction changes if the feature value changes. A feature would have a greater importance when a change in the feature value causes a big change in the predicted value\n    * The second method is \u201cLossFunctionChange\u201d. This type of feature importance can be used for any model, but is particularly useful for ranking models. For each feature the value represents the difference between the loss value of the model with this feature and without it. \n\n\n\n* **LightGBM** and **XGBoost** have two similar methods\n    * The first is \u201cGain\u201d which is the improvement in accuracy (or total gain) brought by a feature to the branches it is on.\n    * The second method has a different name in each package: \u201csplit\u201d (LightGBM) and \u201cFrequency\u201d\/\u201dWeight\u201d (XGBoost). This method calculates the relative number of times a particular feature occurs in all splits of the model\u2019s trees. This method can be biased by categorical features with a large number of categories. ","310d1a4d":"# What Worked?\n\n* Hyperparameter tuning\n* Feature Engineering such as concatenation,aggregation, binning\n* Data cleaning steps and most pertinent of them were removing duplicates[more reliable CV] and confusing rows\n* Xgboost and CatBoost\n* Treating Annual Premium and Vintage as categorical features in catboost\n* Blending CatBoost and XgBoost **[Blended probablities from 125 CB Models and 125 XgB models]**\n\n\n# What didn't Work?\n\n* NN and LGBM\n* Class balancing via oversampler,undersampler,TomkeLinks etc and via hardcoding class weights parameter in individual algorithm\n"}}