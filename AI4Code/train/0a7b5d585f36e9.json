{"cell_type":{"2ff4a9c8":"code","f2166679":"code","dd7f3768":"code","6192e755":"code","8b0b7cdb":"code","e661507a":"code","4977b689":"code","4c5230cf":"code","d20fb8a1":"code","1c7a5ed3":"code","d146b640":"code","a954e95a":"code","1c68e58f":"code","e42af887":"code","39fd75c9":"code","d99ca541":"code","1958b86e":"code","b7a5b589":"code","982897e7":"code","43784fb1":"code","3ef0e810":"markdown","bb501530":"markdown","208438aa":"markdown","ebed1ae5":"markdown","0c6b95ab":"markdown","91052319":"markdown","5debbd39":"markdown","0056da50":"markdown"},"source":{"2ff4a9c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport nltk\nimport sklearn\nimport xgboost as xgb\nimport time\nimport re\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f2166679":"df = pd.read_json('..\/input\/Sarcasm_Headlines_Dataset.json',lines=True)","dd7f3768":"df.head()","6192e755":"#Remove Un-necessary punctuations","8b0b7cdb":"df['headline_new'] = df['headline'].apply(lambda x: re.sub('[^a-zA-Z]','  ',x))","e661507a":"# Remove the un-necessary noise from the headlines","4977b689":"noise_list = nltk.corpus.stopwords.words('english')","4c5230cf":"df['headline_new'] = df['headline_new'].apply(lambda x: [i for i in x.split() if i not in noise_list])","d20fb8a1":"from nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()","1c7a5ed3":"df['headline_new'] = df['headline_new'].apply(lambda x: [lem.lemmatize(i,'v') for i in x])","d146b640":"# Converting all the texts into lower case","a954e95a":"df['headline_new'] = df['headline_new'].apply(lambda x: [i.lower() for i in x])","1c68e58f":"# Now, Let us compare both the headlines ","e42af887":"df[['headline','headline_new']].head(10)","39fd75c9":"df['headline_new'] = df['headline_new'].apply(lambda x: ' '.join(x))","d99ca541":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 1400)\nx = cv.fit_transform(df['headline_new']).toarray()\ny = df['is_sarcastic'].values","1958b86e":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state = 0)","b7a5b589":"x_train.shape","982897e7":"from sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)\n\n#Classifier using Random Forest Classifier\n#from sklearn.ensemble import RandomForestClassifier\n#classifier = RandomForestClassifier(n_estimators=100)\n#classifier.fit(x_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(x_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy score=\",accuracy_score(y_test,y_pred)*100)","43784fb1":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n\n#Creating the L=4 layer ANN\nclassifier = Sequential()\nclassifier.add(Dense(output_dim=700,kernel_initializer = 'uniform',activation='relu',input_dim=1400))\nclassifier.add(Dense(output_dim=350,kernel_initializer = 'uniform',activation='relu'))\nclassifier.add(Dense(output_dim =175, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(output_dim =80, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(output_dim =40, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(output_dim =20, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(output_dim =10, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(output_dim = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\n\n#Compile\nclassifier.compile(optimizer='adam',loss ='binary_crossentropy',metrics=['accuracy'])\nclassifier.fit(x_train,y_train,epochs=8,batch_size=10)\n\ny_pred = classifier.predict(x_test)\ny_pred = (y_pred > 0.5)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_pred,y_test))","3ef0e810":"### The most common **lexicon normalization** practices are :\n\n#### ** Stemming**:  Stemming is a rudimentary rule-based process of stripping the suffixes (\u201cing\u201d, \u201cly\u201d, \u201ces\u201d, \u201cs\u201d etc) from a word.\n#### ** Lemmatization**: Lemmatization, on the other hand, is an organized & step by step procedure of obtaining the root form of the word, it makes use of vocabulary (dictionary importance of words) and morphological analysis (word structure and grammar relations).\n\nPlease refer to the excellent blog by Shivam for more details : [https:\/\/www.analyticsvidhya.com\/blog\/2017\/01\/ultimate-guide-to-understand-implement-natural-language-processing-codes-in-python\/](http:\/\/)","bb501530":"## Let's do some text pre-processing which includes the following : \n\n    Noise Removal\n    Lexicon Normalization\n    Object Standardization","208438aa":"### Noise Removal ","ebed1ae5":"### Checking the first 5 records of the dataset","0c6b95ab":"### Lexicon Normalization\n\nAnother type of textual noise is about the multiple representations exhibited by single word.\n\nFor example \u2013 \u201cplay\u201d, \u201cplayer\u201d, \u201cplayed\u201d, \u201cplays\u201d and \u201cplaying\u201d are the different variations of the word \u2013 \u201cplay\u201d, Though they mean different but contextually all are similar. The step converts all the disparities of a word into their normalized form (also known as lemma).\n\n### Importance : \nNormalization is a pivotal step for feature engineering with text as it converts the high dimensional features (N different features) to the low dimensional space (1 feature), which is an ideal ask for any ML model.","91052319":"### Import the dataset","5debbd39":"#### I would proceed with Lemmatization and would try to reduce to verbs to their roots","0056da50":"#### Joining the split words together again"}}