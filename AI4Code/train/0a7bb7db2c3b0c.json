{"cell_type":{"8d5c9d87":"code","5d50848e":"code","0b4ee731":"code","95239215":"code","219cd200":"code","aa4a3daa":"code","d4da99b5":"code","6ba5a0c6":"code","31bb38eb":"code","4276f0b5":"code","bc6a18a3":"code","23e86b4e":"code","426f3ff5":"code","64b62761":"code","d2dac02a":"code","5e35e92e":"code","d08688fe":"code","cb785a39":"code","ddbd6178":"code","efc13f42":"code","0811875a":"code","d3c29cc2":"code","754bb68b":"code","bb94e036":"code","59fe47a4":"code","9b347522":"code","37dad7af":"code","13a1743a":"code","526c3a3c":"code","d7e46297":"code","51dc6145":"code","76ec2f7c":"code","d156b9c7":"code","8cdf5fd1":"code","5779b498":"code","09bdbea2":"code","037c3b34":"code","b471f51d":"code","3ee5087e":"code","ca76cd42":"code","2f3226ee":"code","4edaa7a4":"code","67088583":"code","669b2c7f":"code","a9a4a384":"code","db83c2f5":"code","8605ae3d":"code","51c6c390":"code","7c6d738c":"code","b3cbc917":"code","ce48dcc0":"code","d2f1c903":"code","1a5e8920":"code","1290ff46":"code","72886179":"code","3aef3f13":"code","fb3a4249":"code","d4ab5578":"code","8560aa86":"code","d56c64b3":"code","bf101278":"code","aeffaaa0":"code","4fe72d4a":"code","e0cbbf30":"code","c9f6eae7":"code","67f9097e":"code","660c4a4e":"code","7132d4d8":"code","4aea33c6":"code","4c6d0195":"code","be3f57bd":"code","cab255c6":"code","28c95ec9":"code","29d1e2db":"code","e5da2a5b":"code","1b4940a1":"code","5b362abf":"code","c96a0443":"code","f2ea62df":"code","efbde3d7":"code","62c0d00d":"code","794a1ad1":"code","34b6efe3":"code","8c8789db":"code","c0c2480c":"code","2b3088b3":"markdown","7c7a6c86":"markdown","adeb110b":"markdown","899a063f":"markdown","744a9e17":"markdown","5490bd45":"markdown","4704ddb3":"markdown","cd5ce363":"markdown","8eca5af6":"markdown","f1a13cdc":"markdown","24f1f91b":"markdown","ca75fef0":"markdown","64ce3781":"markdown","c3fba1a6":"markdown","4117284c":"markdown","d7d499d4":"markdown","309bfb75":"markdown","b7c01f0a":"markdown","1fc5e8c4":"markdown","a2c6515c":"markdown","203990ee":"markdown","7c00065f":"markdown","6de24a0a":"markdown","4089ad13":"markdown","ef60067d":"markdown","82a4c089":"markdown","e3d6d873":"markdown","56bdf830":"markdown","6cda4ebc":"markdown","2f07c9fe":"markdown","b3474c9d":"markdown","f32af98b":"markdown","83f1a3d7":"markdown","26748b5d":"markdown","758150a2":"markdown","a416a481":"markdown","0ac4c363":"markdown"},"source":{"8d5c9d87":"#First we are importing the libraries we are going smeelingly use. Later on, we will import other libraries if they are necessary\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nimport glob\nimport json\nimport re\nimport string\nimport networkx as nx\nimport gc\n\nfrom IPython.utils import io\n#with io.capture_output() as captured:\n\n # !pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\n\nfrom pprint import pprint\nfrom copy import deepcopy\n\nfrom collections import  Counter\n","5d50848e":"from tqdm.notebook import tqdm","0b4ee731":"#NATURAL LANGUAGE PROCESSING\n\n#1. NLTK\n\n\n!pip install nltk\nimport nltk as nlp\n#nltk.download()\nfrom nltk import tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\n\n\n#from rake_nltk import Rake\n","95239215":"#2. SPACY\n\n#!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\n","219cd200":"#!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_bionlp13cg_md-0.2.4.tar.gz ","aa4a3daa":"!pip install scispacy","d4da99b5":"import scispacy\n#import en_ner_bionlp13cg_md\n#import en_core_sci_lg\n\nfrom spacy import displacy\nfrom scispacy.abbreviation import AbbreviationDetector\nfrom scispacy.umls_linking import UmlsEntityLinker\n\n#STOPWORDS IN ENGLISH SPACY\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\n#TOKENIZER\nfrom spacy.tokens import Span\n\n#GENSIM\n\nimport gensim\n\n# SPACY MODEL FOR LDA visualization\n\nimport pyLDAvis\nimport pyLDAvis.gensim\n\n#TENSOR FLOWS\nimport tensorflow_hub as hub\nimport tensorflow as tf\n\n#MATCHER\nfrom spacy.matcher import Matcher ","6ba5a0c6":"#SKLEARN\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n#from sklearn.cluster import DBSCAN","31bb38eb":"#os.listdir('..\/input\/CORD-19-research-challenge\/')\n#with open('..\/input\/CORD-19-research-challenge\/metadata.readme', 'r') as f:\n#   data = f.read()\n#    print(data)","4276f0b5":"os.listdir( '\/kaggle\/input\/all-cleaned')","bc6a18a3":"biorxiv_clean = pd.read_csv('\/kaggle\/input\/all-cleaned\/biorxiv_clean.csv')","23e86b4e":"biorxiv_clean.head(2)","426f3ff5":"biorxiv_clean.text[0]","64b62761":"path=\"..\/input\/all-cleaned\/biorxiv_clean.csv\"\nall_sources=pd.read_csv(path)\n\nall_sources.isna().sum()\n","d2dac02a":"def prepare_similarity(vectors):\n    similarity=cosine_similarity(vectors)\n    return similarity\n\ndef get_top_similar(sentence, sentence_list, similarity_matrix, topN):\n    # find the index of sentence in list\n    index = sentence_list.index(sentence)\n    # get the corresponding row in similarity matrix\n    similarity_row = np.array(similarity_matrix[index, :])\n    # get the indices of top similar\n    indices = similarity_row.argsort()[-topN:][::-1]\n    return [(i,sentence_list[i]) for i in indices]\n\nmodule_url = \"..\/input\/universalsentenceencoderlarge4\" \n# Import the Universal Sentence Encoder's TF Hub module\nembed = hub.load(module_url)\n\ntitles=all_sources['title'].fillna(\"Unknown\")\nembed_vectors=embed(titles[:100].values)['outputs'].numpy()\nsentence_list=titles.values.tolist()\nsentence=titles.iloc[5]\nprint(\"Find similar research papers for :\")\nprint(sentence)\n","5e35e92e":"similarity_matrix=prepare_similarity(embed_vectors)\nsimilar=get_top_similar(sentence,sentence_list,similarity_matrix,6)\n","d08688fe":"for sentence in similar:\n    print(sentence)\nprint(\"\\n\")","cb785a39":"del embed_vectors,sentence_list,similarity_matrix\ngc.collect()\n","ddbd6178":"path=\"..\/input\/all-cleaned\/\"\n","efc13f42":"clean_comm=pd.read_csv(path+\"clean_comm_use.csv\",nrows=5000)\nclean_comm['source']='clean_comm'\n\n\nbiox = pd.read_csv(path+\"biorxiv_clean.csv\")\nbiox['source']='biorx'\n\nall_articles=pd.concat([biox,clean_comm])\n\ndel biox,clean_comm\ngc.collect()\n\n","0811875a":"all_articles.shape\n","d3c29cc2":"#TASK AND SUBTASKS OF THE GENETICS ASPECTS\n\ntasks= [\"What is known about the virus genetics, origin and evolution\", \n        \"Real-time tracking of whole genomes\",\n        \"Access to geographic and temporal diverse sample sets to understand geographic distribution and genomic differences\",\n        \"Evidence of whether farmers are infected,\",\n        \"Surveillance of mixed wildlife- livestock farms for SARS-CoV-2 \",\n        \"Experimental infections to test host range for this pathogen\",\n        \"Animal host(s) and any evidence of continued spill-over to humans\",\n        \"Socioeconomic and behavioral risk factors for this spill-over\",\n        \" Sustainable risk reduction strategies\"]","754bb68b":"task_df=pd.DataFrame({'title':tasks,'source':'task'})","bb94e036":"task_df.head()","59fe47a4":"all_articles=pd.concat([all_articles,task_df])\nall_articles.fillna(\"Unknown\",inplace=True)","9b347522":"sentence_list=all_articles.title.values.tolist()\nembed_vectors=embed(sentence_list)['outputs'].numpy()\nsimilarity_matrix=prepare_similarity(embed_vectors)\n\nsentence= [\"What is known about the virus genetics, origin and evolution\"]\n\nsimilar=get_top_similar(sentence,sentence_list,similarity_matrix,10)\n\nfor sent in similar:\n    print(sent[1])\n\n","37dad7af":"for sent in similar:\n    print(sent[1])\n","13a1743a":"#Clean and store abstracts from related articles.\n\nind,title=list(map(list,zip(*similar)))\ntitles=[]\ntexts=[]\nfor i in ind:\n    titles.append(all_articles.iloc[i]['title'])\n    texts.append(all_articles.iloc[i]['abstract'])\n\nimport re\ndef clean(txt):\n    txt=re.sub(r'\\n','',txt)\n    txt=re.sub(r'\\([^()]*\\)','',txt)\n    txt=re.sub(r'https?:\\S+\\sdoi','',txt)\n    return txt\n\ntexts=list(map(clean,texts))\ntext_list=' '.join(texts)\n#text_list=word_tokenize(text_list)\n\n","526c3a3c":"!pip install pytextrank\n","d7e46297":"import logging\nimport pytextrank\nimport spacy\nimport sys\n\nnlp = spacy.load(\"en_core_web_sm\")\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogger = logging.getLogger(\"PyTR\")\n\n#add PyTextRank into the spaCy pipeline\n\ntr = pytextrank.TextRank(logger=None)\nnlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)\n","51dc6145":"#parse the document\n\ndoc = nlp(text_list)\n\nprint(\"pipeline\", nlp.pipe_names)\nprint(\"elapsed time: {} ms\".format(tr.elapsed_time))\n\n\n#examine the top-ranked phrases in the document\n","76ec2f7c":"#keywords\n\nfor phrase in doc._.phrases[:10]:\n    print(\"{}\".format(phrase.text))\n#print(phrase.chunks)\n","d156b9c7":"\nsentence= \" genomes\",\n\nsimilar=get_top_similar(sentence,sentence_list,similarity_matrix,10)\n\nfor sent in similar:\n    print(sent[1])","8cdf5fd1":"#Clean and store abstracts from related articles.\n\nind,title=list(map(list,zip(*similar)))\ntitles=[]\ntexts=[]\nfor i in ind:\n    titles.append(all_articles.iloc[i]['title'])\n    texts.append(all_articles.iloc[i]['abstract'])\n\nimport re\ndef clean(txt):\n    txt=re.sub(r'\\n','',txt)\n    txt=re.sub(r'\\([^()]*\\)','',txt)\n    txt=re.sub(r'https?:\\S+\\sdoi','',txt)\n    return txt\n\ntexts=list(map(clean,texts))\ntext_list=' '.join(texts)\n#text_list=word_tokenize(text_list)\n","5779b498":"nlp = spacy.load(\"en_core_web_sm\")\n\nlogging.basicConfig(stream=sys.stdout, level=logging.DEBUG)\nlogger = logging.getLogger(\"PyTR\")\n\n#add PyTextRank into the spaCy pipeline\n\ntr = pytextrank.TextRank(logger=None)\nnlp.add_pipe(tr.PipelineComponent, name=\"textrank\", last=True)","09bdbea2":"#parse the document\n\ndoc = nlp(text_list)\n\nprint(\"pipeline\", nlp.pipe_names)\nprint(\"elapsed time: {} ms\".format(tr.elapsed_time))\n\n\n#examine the top-ranked phrases in the document","037c3b34":"#keywords\n\nfor phrase in doc._.phrases[:10]:\n    print(\"{}\".format(phrase.text))\n#print(phrase.chunks)\n","b471f51d":"sentence= \"What do we know about virus genetics, origin, and evolution\"\n\nsimilar=get_top_similar(sentence,sentence_list,similarity_matrix,15)\n\nind,title=list(map(list,zip(*similar)))\ntitles=[]\ntexts=[]\nfor i in ind:\n    titles.append(all_articles.iloc[i]['title'])\n    texts.append(all_articles.iloc[i]['abstract'])\n    \ntexts=list(map(clean,texts))\ntext_list=' '.join(texts)","3ee5087e":"df = prepare_df(text_list)\ndraw_kg(df,c1='blue',c2='pink',c3='green')","ca76cd42":"similar=get_top_similar(sentence,sentence_list,similarity_matrix,15)\n\nind,title=list(map(list,zip(*similar)))\ntitles=[]\ntexts=[]\nfor i in ind:\n    titles.append(all_articles.iloc[i]['title'])\n    texts.append(all_articles.iloc[i]['abstract'])\n    \ntexts=list(map(clean,texts))\ntext_list=' '.join(texts)","2f3226ee":"sentence= \"Evidence of whether farmers are infected\"\n\nsimilar=get_top_similar(sentence,sentence_list,similarity_matrix,15)\n\nind,title=list(map(list,zip(*similar)))\ntitles=[]\ntexts=[]\nfor i in ind:\n    titles.append(all_articles.iloc[i]['title'])\n    texts.append(all_articles.iloc[i]['abstract'])\n    \ntexts=list(map(clean,texts))\ntext_list=' '.join(texts)","4edaa7a4":"sentence= \"Surveillance of mixed wildlife- livestock farms for SARS-CoV-2\"\n\nsimilar=get_top_similar(sentence,sentence_list,similarity_matrix,15)\n\nind,title=list(map(list,zip(*similar)))\ntitles=[]\ntexts=[]\nfor i in ind:\n    titles.append(all_articles.iloc[i]['title'])\n    texts.append(all_articles.iloc[i]['abstract'])\n    \ntexts=list(map(clean,texts))\ntext_list=' '.join(texts)","67088583":"sentence= \"Experimental infections to test host range for this pathogen\"\n\nsimilar=get_top_similar(sentence,sentence_list,similarity_matrix,15)\n\nind,title=list(map(list,zip(*similar)))\ntitles=[]\ntexts=[]\nfor i in ind:\n    titles.append(all_articles.iloc[i]['title'])\n    texts.append(all_articles.iloc[i]['abstract'])\n    \ntexts=list(map(clean,texts))\ntext_list=' '.join(texts","669b2c7f":"sentence= \"Animal host(s) and any evidence of continued spill-over to humans\"\n\nsimilar=get_top_similar(sentence,sentence_list,similarity_matrix,15)\n\nind,title=list(map(list,zip(*similar)))\ntitles=[]\ntexts=[]\nfor i in ind:\n    titles.append(all_articles.iloc[i]['title'])\n    texts.append(all_articles.iloc[i]['abstract'])\n    \ntexts=list(map(clean,texts))\ntext_list=' '.join(texts","a9a4a384":"sentence= \"Socioeconomic and behavioral risk factors for this spill-over\"\nsimilar=get_top_similar(sentence,sentence_list,similarity_matrix,15)\n\nind,title=list(map(list,zip(*similar)))\ntitles=[]\ntexts=[]\nfor i in ind:\n    titles.append(all_articles.iloc[i]['title'])\n    texts.append(all_articles.iloc[i]['abstract'])\n    \ntexts=list(map(clean,texts))\ntext_list=' '.join(texts)\n\ndf = prepare_df(text_list)\ndraw_kg(df,c1='blue',c2='pink',c3='green')","db83c2f5":"sentence= \"Sustainable risk reduction strategies\"\nsimilar=get_top_similar(sentence,sentence_list,similarity_matrix,15)\n\nind,title=list(map(list,zip(*similar)))\ntitles=[]\ntexts=[]\nfor i in ind:\n    titles.append(all_articles.iloc[i]['title'])\n    texts.append(all_articles.iloc[i]['abstract'])\n    \ntexts=list(map(clean,texts))\ntext_list=' '.join(texts)\n\ndf = prepare_df(text_list)\ndraw_kg(df,c1='blue',c2='pink',c3='green')","8605ae3d":"import re\n\nfile = open('wordlist.txt', 'r')\n\nfor line in file.readlines():\n    if re.search('^there$', line, re.I):\n        print (line)","51c6c390":"nlp = en_ner_bionlp13cg_md.load() #LOAD MODEL","7c6d738c":"sentence_list=[]\nsentence_list_without_col = []\nID_list = []\nent_type_all_check = []\nent_type_all= []\n\nfor i in tqdm(range(len(abstractID[:100]))):\n    ID = list(abstractID[i].keys())[0]\n    text = list(abstractID[i].values())[0]\n    a = tokenize.sent_tokenize(text) # Split Sentence\n    \n    for sent in a:\n        print_flag = False\n        doc = nlp(sent)\n        count_label =0\n        ent_type = ''\n        check_dupl =[]\n        sent_withcol = sent\n        sent_withoutcol = sent\n        \n        for x in doc.ents:\n            if x.text not in check_dupl:\n                sent_withcol = sent_withcol.replace(x.text,f\"\\033[1;40m{x.text}\\033[1;31;40m ({x.label_}) \\033[00m\")\n                sent_withoutcol = sent_withoutcol.replace(x.text,f\"{x.text} *{x.label_}*\")\n                check_dupl.append(x.text)\n                print_flag =True\n\n            if x.label_ not in ent_type:\n                ent_type+= f'{x.label_}, '\n                ent_type_all_check.append(x.label_)\n                \n        if print_flag== True:\n            sentence_list.append('* '+sent_withcol)\n            sentence_list_without_col.append('* '+sent)\n            ent_type_all.append(ent_type)\n            ID_list.append(ID)","b3cbc917":"len(ID_list),len(sentence_list),len(sentence_list_without_col),len(ent_type_all)","ce48dcc0":"pd_all = pd.DataFrame({'Paper_ID':ID_list,'Sentence_col':sentence_list,'Sentence_wo_col':sentence_list_without_col,'NER_Key':ent_type_all})","d2f1c903":"type_list = pd.Series(ent_type_all_check).unique().tolist() # Entity type","1a5e8920":"def f(Paper_ID='all',Show_Many='100', Entity='GENETICS', Show_original=False):\n    pd_all2 = pd_all.copy()[:int(Show_Many)]\n    if Paper_ID != 'all':\n        pd_all2 = pd_all[pd_all.Paper_ID==Paper_ID].copy()[:int(Show_Many)]\n    pd_all2 = pd_all2[pd_all2['NER_Key'].str.contains(Entity)].reset_index(drop=True)\n    for i in range(len(pd_all2)):\n        if Show_original==True:\n            print (pd_all2['Sentence_wo_col'][i])  \n        else:\n            print (pd_all2['Sentence_col'][i])  ","1290ff46":"interact(f, Paper_ID='all',Show_Many='100',Entity=type_list, Original = False);","72886179":"df = biorxiv_clean\ndf = df.abstract.dropna()\ndata = df.values.tolist()","3aef3f13":"%%time\n#ref : https:\/\/gist.github.com\/gaurav5430\/8d7810495ec3f914ffb151458f352c60\n\n'''import tensorflow_hub as hub\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef prepare_similarity(vectors):\n    similarity=cosine_similarity(vectors)\n    return similarity\n\ndef get_top_similar(sentence, sentence_list, similarity_matrix, topN):\n    # find the index of sentence in list\n    index = sentence_list.index(sentence)\n    # get the corresponding row in similarity matrix\n    similarity_row = np.array(similarity_matrix[index, :])\n    # get the indices of top similar\n    indices = similarity_row.argsort()[-topN:][::-1]\n    return [sentence_list[i] for i in indices]\n\n\nmeta=pd.read_csv(\"..\/input\/CORD-19-research-challenge\/metadata.csv\")\nmodule_url = \"..\/input\/universalsentenceencoderlarge4\" \nembed = hub.load(module_url)\n\n\n# Creating an empty Dataframe with column names only\nsimsentence = pd.DataFrame()\n\ntitles=meta['title'].fillna(\"Unknown\")\nembed_vectors=embed(titles[:5000].values)['outputs'].numpy()\nsentence_list=titles.values.tolist()\nfor i in range(5):\n\n    sentences=titles.iloc[i]\n    #print(\">>>>>>>>>>>>Using title Find similar research papers for :\",sentences, \"<<<<<<<<<<<<\")\n\n    similarity_matrix=prepare_similarity(embed_vectors)\n    similar=get_top_similar(sentences,sentence_list,similarity_matrix,6)\n    for sentence in similar:\n        #print(sentence)\n        simsentence = simsentence.append({'sentence': sentences, 'similar': sentence}, ignore_index=True)\n        #print(\"\\n\") '''","fb3a4249":"simsentence = pd.read_csv('..\/input\/simsentence\/simsentence.csv')","d4ab5578":"def sent_to_words(sentences):\n    for sentence in sentences:\n        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  \n\ndata_words = list(sent_to_words(data))\n\nprint(data_words[:1])","8560aa86":"# Build the bigram and trigram models\nbigram = gensim.models.Phrases(data_words, min_count=5, threshold=20) # higher threshold fewer phrases.\ntrigram = gensim.models.Phrases(bigram[data_words], threshold=20)  \n\n# Faster way to get a sentence clubbed as a trigram\/bigram\nbigram_mod = gensim.models.phrases.Phraser(bigram)\ntrigram_mod = gensim.models.phrases.Phraser(trigram)\n\n# See trigram example\nprint(trigram_mod[bigram_mod[data_words[1]]])","d56c64b3":"\n#https:\/\/github.com\/cjriggio\/classifying_medical_innovation\ndef remove_stopwords(texts):\n    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n\ndef make_bigrams(texts):\n    return [bigram_mod[doc] for doc in texts]\n\ndef make_trigrams(texts):\n    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n\ndef lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_out = []\n    for sent in texts:\n        doc = nlp(\" \".join(sent)) \n        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_out","bf101278":"# Remove Stop Words\ndata_words_nostops = remove_stopwords(data_words)\n\n# Form Bigrams\ndata_words_bigrams = make_bigrams(data_words_nostops)\n\n# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n# python3 -m spacy download en\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Do lemmatization keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n\nprint(data_lemmatized[1])","aeffaaa0":"print(data_lemmatized[:1])","4fe72d4a":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n# View\nprint(corpus[:1])","e0cbbf30":"[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]","c9f6eae7":"type(biorxiv_clean.csv.abstract.dropna().tolist())","67f9097e":"from gensim.summarization.summarizer import summarize\nsummarize(clean_noncomm_use.abstract.dropna().to_string())","660c4a4e":"import numpy as np\nimport pandas as pd\nimport nltk\nnltk.download('punkt') # one time execution\nimport re","7132d4d8":"#ref : https:\/\/www.analyticsvidhya.com\/blog\/2018\/11\/introduction-text-summarization-textrank-python\/\nfrom nltk.tokenize import sent_tokenize\nsentences = []\nfor s in clean_noncomm_use.abstract.dropna():\n    sentences.append(sent_tokenize(s))\n\nsentences = [y for x in sentences for y in x] # flatten list","4aea33c6":"sentences[:3]","4c6d0195":"# Extract word vectors\nword_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","be3f57bd":"len(word_embeddings)","cab255c6":"# remove punctuations, numbers and special characters\nclean_sentences = pd.Series(sentences).str.replace(\"[^a-zA-Z]\", \" \")\n\n# make alphabets lowercase\nclean_sentences = [s.lower() for s in clean_sentences]","28c95ec9":"def remove_stopwords(sen):\n    sen_new = \" \".join([i for i in sen if i not in stop_words])\n    return sen_new","29d1e2db":"# remove stopwords from the sentences\nclean_sentences = [remove_stopwords(r.split()) for r in clean_sentences]","e5da2a5b":"# Extract word vectors\nword_embeddings = {}\nf = open('glove.6B.100d.txt', encoding='utf-8')\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    word_embeddings[word] = coefs\nf.close()","1b4940a1":"sentence_vectors = []\nfor i in clean_sentences:\n    if len(i) != 0:\n        v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])\/(len(i.split())+0.001)\n    else:\n        v = np.zeros((100,))\n    sentence_vectors.append(v)","5b362abf":"# similarity matrix\nsim_mat = np.zeros([len(sentences), len(sentences)])","c96a0443":"import torch\ntorch.cuda.is_available()","f2ea62df":"%%time\nfrom sklearn.metrics.pairwise import cosine_similarity\nfor i in range(1000):\n    for j in range(1000):\n        if i != j:\n            sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]\n            #print(sim_mat[i][j])","efbde3d7":"df1 = biorxiv_clean\ndf1 = df1.abstract.dropna()\ndf1abstracts = df.values.tolist()\n\nlen(df1abstracts)","62c0d00d":"T5_PATH = 't5-base'\nt5_model = T5ForConditionalGeneration.from_pretrained(T5_PATH, output_past=True)\nt5_tokenizer = T5Tokenizer.from_pretrained(T5_PATH)","794a1ad1":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndef t5_summarize(input_text, num_beams=4, num_words=80):\n    #input_text = str(input_text).replace('\\n', '')\n    input_text = ' '.join(input_text.split())\n    input_tokenized = t5_tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n    summary_task = torch.tensor([[21603, 10]]).to(device)\n    input_tokenized = torch.cat([summary_task, input_tokenized], dim=-1).to(device)\n    summary_ids = t5_model.generate(input_tokenized,\n                                    num_beams=int(num_beams),\n                                    no_repeat_ngram_size=3,\n                                    length_penalty=2.0,\n                                    min_length=30,\n                                    max_length=int(num_words),\n                                    early_stopping=True)\n    output = [t5_tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids]\n    return output[0]","34b6efe3":"%%time\nfor i in range(20):\n    try:\n        print('BioArvix paper  ',i + 1, \" : \\n\" )\n        print(t5_summarize(df1abstracts[i]))\n        print('............................................................................\\n\\n\\n\\n')\n    except:\n        print('paper ',i+1 ,\" has LONG ABSTRACT\\n\\n\")","8c8789db":"simsentence.head(5)","c0c2480c":"\"\"\"\nThis is a simple application for sentence embeddings: semantic search\nWe have a corpus with various sentences. Then, for a given query sentence,\nwe want to find the most similar sentence in this corpus.\nThis script outputs for various queries the top 5 most similar sentences in the corpus.\n\"\"\"\n# taken from : https:\/\/github.com\/UKPLab\/sentence-transformers\/blob\/master\/examples\/application_semantic_search.py\nfrom sentence_transformers import SentenceTransformer\nimport scipy.spatial\n\nembedder = SentenceTransformer('bert-base-nli-mean-tokens')\n\n# Corpus with example sentences\ncorpus = df.values.tolist()\ncorpus_embeddings = embedder.encode(corpus)\n\n# Query sentences:\nqueries = ['Range of incubation periods for the disease in humans','risk factors of covid-19','cure for covid-19', 'antiviral covid-19 success treatment','Does smoking or pre-existing pulmonary disease increase risk of COVID-19?', 'risk of fatality among symptomatic hospitalized patients']\nquery_embeddings = embedder.encode(queries)\n\n# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\nclosest_n = 5\nfor query, query_embedding in zip(queries, query_embeddings):\n    distances = scipy.spatial.distance.cdist([query_embedding], corpus_embeddings, \"cosine\")[0]\n\n    results = zip(range(len(distances)), distances)\n    results = sorted(results, key=lambda x: x[1])\n\n    print(\"\\n\\n======================\\n\\n\")\n    print(\"Query:\", query)\n    print(\"\\nTop 5 most similar sentences in corpus:\")\n\n    for idx, distance in results[0:closest_n]:\n        print(corpus[idx].strip(), \"(Score: %.4f)\" % (1-distance))","2b3088b3":"***************************************************\n\n","7c7a6c86":"AUTOMATIC SUMMARIZATION","adeb110b":"There are three big models for the automatic summariation methos. We are going to focus on the last one, the T5-BERT, as it was suggered in * this* notebook: https:\/\/www.kaggle.com\/mobassir\/mining-covid-19-scientific-papers\/comments. The other two are: PageRank Algorithm & BERT algorightm. I will, eventually","899a063f":"****************\n","744a9e17":"Find similar research papers using universalsentenceencoderlarge4","5490bd45":"Define functions for stopwords, bigrams, trigrams and lemmatization","4704ddb3":"Summariing first 20 papers abstract of bioarvix","cd5ce363":"Create Dictionary,Corpus and Document Frequency","8eca5af6":"***********************\n","f1a13cdc":"NER\n","24f1f91b":"************************\n","ca75fef0":"******************************\n","64ce3781":"**********************\n","c3fba1a6":"WE ARE GOING TO FOCCUS ON BIORXIV subset and on the abstract part\n","4117284c":"This project on covid is being adaptated based on previous notebooks and similar ones that have been found on kaggle as well as our own previous developments whose hints can be found on my profiles (acutally updating some of them). References can be found within the notbook. ","d7d499d4":"Now we are going to define the tasks","309bfb75":"Due to slightly modifications on the CORD-19 challenge database, we are still adapting the code, which is aimed to give us the results we are looking for but some parts are in need of changes. \n","b7c01f0a":"Now let's save this new dataframe as csv file for possible further research","1fc5e8c4":"REMOVE STOPWORDS","a2c6515c":"***********************\n","203990ee":"Vector Representation of Sentences","7c00065f":"Now we have it for the first task, we are going to do it for all the other tasks. We copy the model and change teh sentence for each task. ","6de24a0a":"Human readable format of corpus (term-frequency)","4089ad13":"checking everythin is ok with the first paper\n","ef60067d":"The function below converts sentences to words using gensim","82a4c089":"Build the bigram and trigram models using gensim","e3d6d873":"Download GloVe Word Embeddings","56bdf830":"KEYWORD EXTRACTION. ","6cda4ebc":"Summarization Task using T5 model","2f07c9fe":"Keyword extraction","b3474c9d":"*Now we are going to answer the questions: *","f32af98b":"The first step would be to concatenate all the text contained in the articles\nThen split the text into individual sentences\nIn the next step, we will find vector representation (word embeddings) for each and every sentence\nSimilarities between sentence vectors are then calculated and stored in a matrix\nThe similarity matrix is then converted into a graph, with sentences as vertices and similarity scores as edges, for sentence rank calculation\nFinally, a certain number of top-ranked sentences form the final summary","83f1a3d7":"Similarity Matrix preparation","26748b5d":"MATCHING THE KEYWORDS WITH THE DOCUMENTS\n","758150a2":"![](http:\/\/)","a416a481":"CONVERT ABSTRACT TO LIST","0ac4c363":"VISUALIZATION\n\nHere is when we decide to define the Entinty as GENETICS"}}