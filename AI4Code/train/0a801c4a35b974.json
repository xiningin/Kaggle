{"cell_type":{"5dde1cbb":"code","f1eb0b95":"code","7d4ddbd8":"code","fcb5ef7d":"code","66ee1872":"code","68a582ba":"code","3e78839c":"code","8b126bf1":"code","080a03b9":"code","2fb8b7c3":"code","7979c20c":"code","babeb0e6":"code","6774c1a9":"code","4afde906":"code","d93908cb":"code","cb105e69":"code","8c2881c9":"code","9f1f1233":"code","03b4660f":"code","94dcbc95":"code","12605984":"code","3cc60fe5":"code","4e6bdd52":"code","83907613":"code","9c037d05":"code","e27dcadf":"code","8d3698b0":"code","bc8e3227":"code","771ff4dd":"code","1282555a":"code","791f6ad4":"code","797edd14":"code","5f28769b":"code","a3ee5e76":"code","bdf6a660":"code","9c34d550":"code","cccaddce":"code","6177a57f":"code","8f7b8e82":"code","66745bd9":"code","572f5b0d":"code","814bc8ae":"markdown","e7266ddd":"markdown","ca5875da":"markdown","db593aab":"markdown","aba7f26a":"markdown","f7f5699d":"markdown","d103ca6a":"markdown","99d57517":"markdown","9ef32f3f":"markdown","1ad508b7":"markdown","204f65a9":"markdown","98082dff":"markdown","a72ca979":"markdown","c9af870e":"markdown","0ae0cd8d":"markdown","0ddc1b84":"markdown"},"source":{"5dde1cbb":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f1eb0b95":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings(\"ignore\")","7d4ddbd8":"df = pd.read_csv('\/kaggle\/input\/heart-failure-prediction\/heart.csv')\ndf.shape","fcb5ef7d":"df.head()","66ee1872":"df.dtypes","68a582ba":"df.info()","3e78839c":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap=\"OrRd_r\")","8b126bf1":"categorical_cols= df.select_dtypes(include=['object'])\nprint(f'The dataset contains {len(categorical_cols.columns.tolist())} categorical columns')\nfor cols in categorical_cols.columns:\n    print(cols,':', len(categorical_cols[cols].unique()),'labels')","080a03b9":"#dividing the dataset into train and test sets\nfrom sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df,test_size=0.2,random_state= 1234)","2fb8b7c3":"import plotly.graph_objects as go\n\nnight_colors = ['#F9B1B8',  '#EE4355',  '#B60618','#820815']\nlabels = [x for x in train.ChestPainType.value_counts().index]\nvalues = train.ChestPainType.value_counts()\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3,pull=[0, 0, 0.06, 0])])\n\nfig.update_layout(\n    title_text=\"Chest pain type \")\nfig.update_traces(marker=dict(colors=night_colors))\nfig.show()","7979c20c":"import plotly.figure_factory as ff\nfrom plotly.offline import iplot\nfig = ff.create_distplot([train.Age],['Age'],bin_size=1)\nfig.update_traces(marker=dict(color='#F9B1B8'))\niplot(fig, filename='Basic Distplot')","babeb0e6":"# Add histogram data\nx1 = train[\"RestingBP\"]\nx2 = train[\"Cholesterol\"]\nx3 = train[\"MaxHR\"]\n\n# Group data together\nhist_data = [x1, x2, x3]\n\ngroup_labels = ['RestingBP', 'Cholesterol', 'MaxHR']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels, bin_size=10,colors=night_colors)\n\nfig.show()","6774c1a9":"trace0 = go.Box(y=train[\"Age\"],name=\"Age\")\ntrace1 = go.Box(y=train[\"RestingBP\"],name=\"RestingBP\")\ntrace2 = go.Box(y=train[\"Cholesterol\"],name=\"Cholesterol\")\ntrace3 = go.Box(y=train[\"MaxHR\"],name=\"MaxHR\")\ndata = [trace0, trace1, trace2,trace3]\nlayout = go.Layout(\n    title='Interests',\n    xaxis=dict(title='Interests'),\n    yaxis=dict(title='Number of Person')\n)\nfig = go.Figure(data=data, layout=layout)\nfig.update_traces(marker=dict(color=\"red\"))\niplot(fig)\n","4afde906":"# prepare data\ndata = train.loc[:,['RestingBP', 'Cholesterol', 'MaxHR']]\ndata[\"index\"] = np.arange(1,len(data)+1)\n# scatter matrix\nfig = ff.create_scatterplotmatrix(data, diag='box', index='index',colormap='Portland',\n                                  height=700, width=700)\niplot(fig)","d93908cb":"train['Sex'] = np.where(train['Sex'] == \"F\", 0, 1)\ntrain['ExerciseAngina'] = np.where(train['ExerciseAngina'] == \"N\", 0, 1)\ntest['Sex'] = np.where(test['Sex'] == \"F\", 0, 1)\ntest['ExerciseAngina'] = np.where(test['ExerciseAngina'] == \"N\", 0, 1)","cb105e69":"train.head()","8c2881c9":"train=pd.get_dummies(train)\ntest=pd.get_dummies(test)","9f1f1233":"train.head()","03b4660f":"test.head()","94dcbc95":"print(train.shape)\nprint(test.shape)","12605984":"x_train=train.drop(['HeartDisease'],1)\nx_test=test.drop(['HeartDisease'],1)\n\ny_train=train['HeartDisease']\ny_test=test['HeartDisease']","3cc60fe5":"print(x_train.shape)\nprint(x_test.shape)","4e6bdd52":"sns.countplot(y_train,palette='OrRd')","83907613":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#libraries for model evaluation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report","9c037d05":"#logistic regression\nlr = LogisticRegression(max_iter=20000,penalty='l2')\nmodel1=lr.fit(x_train, y_train)\nprint(\"train accuracy:\",model1.score(x_train, y_train),\"\\n\",\"test accuracy:\",model1.score(x_test,y_test))\nlrpred = lr.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for logistic regression\")\nprint(classification_report(lrpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for logistic regression\")\ndisplr = plot_confusion_matrix(lr, x_test, y_test,cmap=plt.cm.OrRd , values_format='d')","e27dcadf":"#linear discriminant analysis\nlda = LinearDiscriminantAnalysis()\nmodel2=lda.fit(x_train, y_train)\nprint(\"train accuracy:\",model2.score(x_train, y_train),\"\\n\",\"test accuracy:\",model2.score(x_test,y_test))\n\nldapred = lda.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for linear discriminant analysis\")\nprint(classification_report(ldapred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for linear discriminant analysis\")\ndisplr = plot_confusion_matrix(lda, x_test, y_test ,cmap=plt.cm.OrRd , values_format='d')","8d3698b0":"#decision tree classifier\ndt=DecisionTreeClassifier()\nmodel3=dt.fit(x_train, y_train)\nprint(\"train accuracy:\",model3.score(x_train, y_train),\"\\n\",\"test accuracy:\",model3.score(x_test,y_test))\n\ndtpred = dt.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for decision tree classifier\")\nprint(classification_report(dtpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for decision tree classifier\")\ndisplr = plot_confusion_matrix(dt, x_test, y_test ,cmap=plt.cm.OrRd , values_format='d')","bc8e3227":"#random forest classifier\nrf=RandomForestClassifier()\nmodel4=rf.fit(x_train, y_train)\nprint(\"train accuracy:\",model4.score(x_train, y_train),\"\\n\",\"test accuracy:\",model4.score(x_test,y_test))\n\nrfpred = rf.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for random forest classifier\")\nprint(classification_report(rfpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for random forest classifier\")\ndisplr = plot_confusion_matrix(rf, x_test, y_test ,cmap=plt.cm.OrRd , values_format='d')","771ff4dd":"#bagging classifier\nbg=BaggingClassifier()\nmodel5=bg.fit(x_train, y_train)\nprint(\"train accuracy:\",model5.score(x_train, y_train),\"\\n\",\"test accuracy:\",model5.score(x_test,y_test))\n\nbgpred = bg.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for bagging classifier\")\nprint(classification_report(bgpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for bagging classifier\")\ndisplr = plot_confusion_matrix(bg, x_test, y_test ,cmap=plt.cm.OrRd , values_format='d')","1282555a":"# gradient boost classifier \ngbm=GradientBoostingClassifier()\nmodel6=gbm.fit(x_train, y_train)\nprint(\"train accuracy:\",model6.score(x_train, y_train),\"\\n\",\"test accuracy:\",model6.score(x_test,y_test))\n\ngbmpred = gbm.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for gradient boosting classifier\")\nprint(classification_report(gbmpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for gradient boosting classifier\")\ndisplr = plot_confusion_matrix(gbm, x_test, y_test ,cmap=plt.cm.OrRd , values_format='d')","791f6ad4":"# adaboost classifier \nada=AdaBoostClassifier()\nmodel7=ada.fit(x_train, y_train)\nprint(\"train accuracy:\",model7.score(x_train, y_train),\"\\n\",\"test accuracy:\",model7.score(x_test,y_test))\n\nadapred = ada.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for adaboost classifier\")\nprint(classification_report(adapred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for adaboost classifier\")\ndisplr = plot_confusion_matrix(ada, x_test, y_test ,cmap=plt.cm.OrRd , values_format='d')","797edd14":"# extreme gradient boost classifier\nxgb = XGBClassifier()\nmodel8=xgb.fit(x_train.values, y_train)\nprint(\"train accuracy:\",model8.score(x_train, y_train),\"\\n\",\"test accuracy:\",model8.score(x_test,y_test))\n\nxgbpred = xgb.predict(x_test.values)\nprint(\"\\n\")\nprint(\"classification report for extreme gradient boosting classifier\")\nprint(classification_report(xgbpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for extreme gradient boosting classifier\")\ndisplr = plot_confusion_matrix(xgb, x_test.values, y_test ,cmap=plt.cm.OrRd , values_format='d')","5f28769b":"# extra tree classifier\nextree = ExtraTreesClassifier()\nmodel9=extree.fit(x_train, y_train)\nprint(\"train accuracy:\",model9.score(x_train, y_train),\"\\n\",\"test accuracy:\",model9.score(x_test,y_test))\n\nextpred = extree.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for extra tree classifier\")\nprint(classification_report(extpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for extra tree classifier\")\ndisplr = plot_confusion_matrix(extree, x_test, y_test ,cmap=plt.cm.OrRd , values_format='d')","a3ee5e76":"# voting classifer\nfrom sklearn.ensemble import VotingClassifier\nclf1 = GradientBoostingClassifier()\nclf2 = RandomForestClassifier()\n\nvc = VotingClassifier(estimators=[('ext', clf1),('gbm', clf2)], voting='soft')\nmodel10=vc.fit(x_train, y_train)\nprint(\"train accuracy:\",model10.score(x_train, y_train),\"\\n\",\"test accuracy:\",model10.score(x_test,y_test))\n\nvcpred = vc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for voting classifier\")\nprint(classification_report(vcpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for voting classifier\")\ndisplr = plot_confusion_matrix(vc, x_test, y_test ,cmap=plt.cm.OrRd, values_format='d')","bdf6a660":"# stacking classifier \nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nestimators = [('rf', RandomForestClassifier(random_state=5)),('ext', make_pipeline(StandardScaler(),GradientBoostingClassifier(random_state=42)))]\nsc= StackingClassifier( estimators=estimators)\n\nmodel11=sc.fit(x_train, y_train)\nprint(\"train accuracy:\",model11.score(x_train, y_train),\"\\n\",\"test accuracy:\",model11.score(x_test,y_test))\n\nscpred = sc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for voting classifier\")\nprint(classification_report(scpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for voting classifier\")\ndisplr = plot_confusion_matrix(sc, x_test, y_test ,cmap=plt.cm.OrRd , values_format='d')","9c34d550":"from catboost import CatBoostClassifier\n\ncc = CatBoostClassifier(silent=True )\nmodel12=cc.fit(x_train, y_train)\nprint(\"train accuracy:\",model12.score(x_train, y_train),\"\\n\",\"test accuracy:\",model12.score(x_test,y_test))\n\nccpred = cc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for extra tree classifier\")\nprint(classification_report(ccpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for extra tree classifier\")\ndisplr = plot_confusion_matrix(cc, x_test, y_test ,cmap=plt.cm.OrRd , values_format='d')\n","cccaddce":"import catboost as cb\nimport optuna\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","6177a57f":"df['Sex'] = np.where(df['Sex'] == \"F\", 0, 1)\ndf['ExerciseAngina'] = np.where(df['ExerciseAngina'] == \"N\", 0, 1)\ndf=pd.get_dummies(df)\nX=df.drop(['HeartDisease'],1)\ny=df['HeartDisease']\ndef hyper_para_tuning(trial):\n    train_x, valid_x, train_y, valid_y = train_test_split(X,y, test_size=0.2)\n\n    param = {\n        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.01, 0.1),\n        \"depth\": trial.suggest_int(\"depth\", 1, 12),\n        \"boosting_type\": trial.suggest_categorical(\"boosting_type\", [\"Ordered\", \"Plain\"]),\n        \"bootstrap_type\": trial.suggest_categorical(\n            \"bootstrap_type\", [\"Bayesian\", \"Bernoulli\", \"MVS\"]\n        ),\n        \"used_ram_limit\": \"3gb\",\n    }\n\n    if param[\"bootstrap_type\"] == \"Bayesian\":\n        param[\"bagging_temperature\"] = trial.suggest_float(\"bagging_temperature\", 0, 10)\n    elif param[\"bootstrap_type\"] == \"Bernoulli\":\n        param[\"subsample\"] = trial.suggest_float(\"subsample\", 0.1, 1)\n\n    cbm = cb.CatBoostClassifier(**param)\n\n    cbm.fit(train_x, train_y, eval_set=[(valid_x, valid_y)], verbose=0, early_stopping_rounds=100)\n\n    pred = cbm.predict(valid_x)\n    pred_labels = np.rint(pred)\n    accuracy = accuracy_score(valid_y, pred_labels)\n    return accuracy","8f7b8e82":"hyper_para_vals = optuna.create_study(direction=\"maximize\")\nhyper_para_vals.optimize(hyper_para_tuning, n_trials=300, timeout=600)","66745bd9":"print(\"Number of finished trials: {}\".format(len(hyper_para_vals.trials)))\n\nprint(\"Best trial:\")\ntrial = hyper_para_vals.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","572f5b0d":"c = CatBoostClassifier(silent=True,objective='Logloss',colsample_bylevel=0.07668035398982363, depth=5, boosting_type='Ordered',bootstrap_type='Bernoulli',subsample=0.994709550179773)\nmodel13=c.fit(x_train, y_train)\nprint(\"train accuracy:\",model13.score(x_train, y_train),\"\\n\",\"test accuracy:\",model13.score(x_test,y_test))\n\ncpred = c.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for extra tree classifier\")\nprint(classification_report(cpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for extra tree classifier\")\ndisplr = plot_confusion_matrix(c, x_test, y_test ,cmap=plt.cm.OrRd , values_format='d')\n","814bc8ae":"<p style=\"background-color:#FC8695;color:black;font-size:22px;text-align:center;border-radius:10px 10px;font-weight:bold;\">Heart failure prediction \u2764\ud83d\ude91\ud83c\udfe5 <\/p>\n<center><img src=\"https:\/\/github.com\/Isharaneranjana\/kaggle_gif\/blob\/main\/HALLOWEEN.gif?raw=true\"><\/center>\n","e7266ddd":"<center><img src=\"https:\/\/media.giphy.com\/media\/MXiWqZBY45qiJ818nX\/giphy.gif\" style=\"width:480px;height:280px;\"><\/center>\n","ca5875da":"## <p style=\"background-color:#FC8695;color:black;font-size:20px;text-align:center;border-radius:10px 10px;\">\ud83e\ude7aDescription of the dataset <\/p>\n\n<font size=\"4\">This dataset has created by combining different datasets already available independently but not combined before. In this dataset, 5 heart datasets are combined over 11 common features which makes it the largest heart disease dataset available so far for research purposes.<\/font>","db593aab":"## <p style=\"background-color:#FC8695;color:black;font-size:20px;text-align:center;border-radius:10px 10px;\"> \ud83c\udfafObjectives <\/p>\n\n<font size=\"4\">People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help. hence the main objective is <span style=\"color:red;\">to predict if some one is at high risk of being diagnoised as a heart patient.<\/span><\/font>\n\n<font size=\"4\">When a person has risk factors, their doctor can refer them to a cardiologist for further testing. These tests include a coronary calcium scan(CT) scan that takes pictures of the arteries to check for calcified plaque in the arteries, and a CT angiogram, which uses X-rays to provide detailed pictures of the heart and the blood vessels to look for disease. source: <a href=\"https:\/\/www.montefiorenyack.org\/highland\/press\/early-detection-of-heart-disease-can-keep-you-healthy\">click here<\/a> <\/font>","aba7f26a":"<font size=\"4\"> Hyper parameter tuning for catboost classifier <\/font>","f7f5699d":"<font size=\"4\">Here's a brief version of the data description file.<\/font>\n\n* Age: age of the patient [years]\n* Sex: sex of the patient [M: Male, F: Female]\n* ChestPainType: chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n* RestingBP: resting blood pressure [mm Hg]\n* Cholesterol: serum cholesterol [mm\/dl]\n* FastingBS: fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n* RestingECG: resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n* MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n* ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n* Oldpeak: oldpeak = ST [Numeric value measured in depression]\n* ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n* HeartDisease: output class [1: heart disease, 0: Normal]","d103ca6a":"## <p style=\"background-color:#FC8695;color:black;font-size:20px;text-align:center;border-radius:10px 10px;\">\ud83e\ude78 Model fitting <\/p>","99d57517":"## <p style=\"background-color:#FC8695;color:black;font-size:20px;text-align:center;border-radius:10px 10px;\"> \ud83c\udfeeIntroduction <\/p>\n\n<font size=\"4\">In this modern era people are very busy and working hard in order to satisfying their materialistic needs and not able to spend time for themselves which leads to physical stress and mental disorder. There are also reports that heart suffer because of global pandemic corona virus. Inflammation of the heart muscle can be caused by corona virus. Thus heart disease is very common now a day\u2019s particularly in urban areas because of excess mental stress due to corona virus. As a result Heart disease has become one of the most important factors for death of men and women in the so called material world (Sahoo,2020). <\/font>\n\n<font size=\"4\">Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease. Both linear and machine learning models are used to predict heart failure based on various data as inputs. <\/font>","9ef32f3f":"<font size=\"4\"> There are no missing values in this dataset. <\/font>","1ad508b7":"## <p style=\"background-color:#FC8695;color:black;font-size:20px;text-align:center;border-radius:10px 10px;\">\ud83d\udc68\u200d\u2695\ufe0f Main findings of descriptive analysis <\/p>","204f65a9":"### \ud83c\udf88 Importing... the dataset","98082dff":"## <p style=\"background-color:#FC8695;color:black;font-size:20px;text-align:center;border-radius:10px 10px;\">\ud83d\udc89 Hyper parameter tuning <\/p>","a72ca979":"<center><img src=\"https:\/\/media.giphy.com\/media\/3o6MboQRbk4w8msepW\/giphy.gif\" style=\"width:480px;height:300px;\"><\/center>\n","c9af870e":"<font size=\"4\" color=\"#9A0716\"><b>Highly appreciate your questions or feedback related to this notebook.Stay fit and healthy. THANK YOU \ud83d\ude0a <\/b><\/font>","0ae0cd8d":"## <p style=\"background-color:#FC8695;color:black;font-size:20px;text-align:center;border-radius:10px 10px;\">\ud83d\udc8e Results <\/p>\n\n<font size=\"4\"> This hyper parameter tuning did not outperform the model with deafault hyper parameters. This may be due to the fact that our hyper parameter space is quite small. more exhaustive search may provide higher performance. <\/font>\n\n<font size=\"4\"> From the above fitted models <span style=\"color:red;\">CatBoost classifier<\/span> has the highest classification accuracy of <span style=\"color:red;\">90.76% <\/span>.<\/font>","0ddc1b84":"<font size=\"4\"> From the above fitted models <span style=\"color:red;\">CatBoost classifier<\/span> has the highest classification accuracy of <span style=\"color:red;\">90.76% <\/span>. we can improve the accuracy with hyper parameter tuning for these models also. <\/font>"}}