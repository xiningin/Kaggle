{"cell_type":{"ff3f4a32":"code","97e8285f":"code","6f44c700":"code","78633d1d":"code","275c0ae6":"code","c65afe30":"code","085b9c34":"code","e7e988ad":"code","ee0322cf":"code","e5412464":"code","64d82003":"code","da748b7f":"code","c08034ef":"code","fb898274":"code","188412a7":"code","0cb8ed07":"code","20975deb":"code","bb523022":"code","d6481869":"code","44b6da97":"code","916d7bed":"code","51693713":"code","102cab1d":"code","f3527f4e":"code","43177f24":"code","520854a7":"code","7a7031f4":"code","b3bf8679":"code","71e17e74":"code","f9a2519f":"code","5729d3c3":"code","de69e7a8":"code","88318293":"code","3006accd":"code","39725f5d":"code","b59c3fba":"code","a868c1d9":"code","293fdbe6":"code","ace3c178":"code","984ad8cb":"code","9cce20b9":"markdown","412f411f":"markdown","482ae030":"markdown","b204dd2b":"markdown","960a0a9c":"markdown","055dc700":"markdown","6a55cf48":"markdown","9ebee401":"markdown","140e851c":"markdown","0edf6a38":"markdown","32e3bff1":"markdown","031b7ce6":"markdown","c9d57771":"markdown","567bd68a":"markdown","5f5792f2":"markdown","d8edc0f3":"markdown","cdd58753":"markdown","f6aab3f4":"markdown","25652cab":"markdown","e28cb8a0":"markdown","b2e274ad":"markdown","86e88a3a":"markdown","462da2ff":"markdown","f9abc84a":"markdown","137f95c5":"markdown","a933b014":"markdown","719bf302":"markdown"},"source":{"ff3f4a32":"#first, load standard packages:\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as ss\nimport math as mt\nimport seaborn as sns\nimport sklearn.datasets\nfrom matplotlib import pyplot as plt\nimport re\nimport datetime as dt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder","97e8285f":"#store our training and test data into train\/test respectively:\ntrain = pd.read_csv('..\/input\/take-home-task-data\/training_data.csv')\ntest = pd.read_csv('..\/input\/take-home-task-data\/test_data.csv')\ntrain.head()","6f44c700":"#Creating our first function to check for data quality. \n#There are many things we can check for, but the principles I will be revolving my finds around include checking for duplicate rows\/non-unique rows, unusual entries, and NaN values.\ndef quality_check(df):\n    #Before I begin, I will quickly create two variables - the number of rows and columns in our original dataset. I will be using them in the following calculations:\n    rows = df.shape[0]\n    cols = df.shape[1]\n    \n    #1st simple check is to see if there exists any duplicate rows:\n    print('There are {} duplicate rows. This is {}.'.format(df[df.duplicated()].shape[0],['unexpected','expected'][df[df.duplicated()].shape[0] == 0]) + '\\n')\n\n    #2nd check is to see if the unique columns are in fact, unique.\n    #In our dataset, it is reasonable to have customers who have multiple orders in our dataset (therefore, customer_id will not be unique).\n    #However, I expect order_number to be. Therefore, I will quickly create a check below:\n    print('There are {} unique order numbers out of {} rows. This is {}.'.format(df['order_number'].nunique(),rows,['unexpected','expected'][rows == df['order_number'].nunique()]) + '\\n')\n\n    #check unique types of final_payment_status. We expect only Paid, Unpaid, Cancelled, Refunded or Other:\n    print('The following are the unusual payment statuses and how many: \\n' + str([str(i) + ' : ' + str(df[df['final_payment_status'] == i].shape[0]) for i in df['final_payment_status'].unique()]) + '\\n')\n\n\n    #Check for null\/NaN values. I'll be printing a list of columns with the number of entries that are null:\n    print('The following are the number of null values in the dataset by column: \\n' + str([df.columns[i] + ' : ' + str(df.isnull().sum()[i]) for i in range(cols) if df.isnull().sum()[i] > 0]) + '\\n')\n\n    #Check is for unusual entries. For example: custom_meal_selection should only have Yes and No. I'll return the unusual entries with how many rows have this entry:\n    print('The following are the unusual entries for custom meal selections and how many: \\n' + str([str(i) + ' : ' + str(df[df['custom_meal_selection'] == i].shape[0]) for i in df['custom_meal_selection'].unique() if i not in ['Yes','No']]) + '\\n')\n\n    #For other variables that we are not told to use feature engineering on (these include:\n        #payment_method,\n        #state)\n    #We'll be checking the entries for these values and see if it's valuable to add feature engineering based on these columns:\n    print('The following are the entries for payment methods and how many: \\n' + str([str(i) + ' : ' + str(df[df['payment_method'] == i].shape[0]) for i in df['payment_method'].unique()])+ '\\n')\n    print('The following are the entries for states and how many: \\n' + str([str(i) + ' : ' + str(df[df['state'] == i].shape[0]) for i in df['state'].unique()])+ '\\n')\n\n    #Finally, we'll check how many engagement scores are not > 0:\n    print('There are {} invalid engagement scores (not strictly greater than 0). \\n'.format(str(df[df['engagement_score'] <= 0].shape[0])))","78633d1d":"#An intial run of the function to check data quality.\nquality_check(train)","275c0ae6":"#Creating our second function to clean the data.\n#This function will focus on two things: accounting for rows with 'corrupted' entries, as well as insignificant\/Zero-Variance Predictors.\ndef data_clean(df):\n    #If there exist duplicates, drop them.\n    if df[df.duplicated()].shape[0] != 0:\n        df.drop_duplicates(inplace = True)\n\n    #Dropping rows with invalid engagement_scores:\n    df.drop(df[df['engagement_score'] <= 0].index, inplace = True)\n\n    #Dropping columns stated from assumptions and decisions section:\n    df.drop('customer_id', axis=1,inplace=True)\n    df.drop('order_number', axis=1,inplace=True)\n    df.drop('delivery_week', axis=1,inplace=True)\n    df.drop('state', axis=1,inplace=True)\n    return(df)","c65afe30":"#data copy\ntrain_clean = train.copy()\n#use data_clean function\ntrain_clean = data_clean(train_clean)\ntrain_clean.head()","085b9c34":"def EDA1(df):\n    plt.figure(figsize=[16,16])\n    #first plot will be a simple plot of the time_to_pay variable showing all unique values and their frequency\n    plt.subplot(231)\n    plt.hist(df['time_to_pay'],np.linspace(0, 1000, 100), log=True, color = \"skyblue\", edgecolor =\"black\")\n    plt.title('Number of unique Times to Pay') \n    plt.xlabel('Times to Pay') \n    plt.ylabel('Count')\n\n    #second plot will be a simple plot of the final_payment_status variable, grouping by their category and plotting their frequency\n    plt.subplot(232)\n    plt.hist(df['final_payment_status'].sort_values(), bins=np.arange(6) - 0.5, log=True, edgecolor =\"black\")\n    plt.title('Number of Final Payment Statuses') \n    plt.xlabel('Final Payment Status') \n    plt.ylabel('Count')\n    plt.xticks(range(5))\n    for i,v in enumerate(list(df.groupby('final_payment_status').final_payment_status.count())):\n        plt.text(i,v,str(v), ha='center', va='bottom')\n\n    #the last plot for good measure will be time_to_pay but hue'd by final_payment_status\n    plt.subplot(233)\n    plt.hist(df[df['final_payment_status']=='Paid']['time_to_pay'], np.linspace(0, 1000, 100), alpha = 0.5, label='Paid', log=True)\n    plt.hist(df[df['final_payment_status']=='Refunded']['time_to_pay'], np.linspace(0, 1000, 100), alpha = 0.5, label='Refunded')\n    plt.hist(df[df['final_payment_status']=='Unpaid']['time_to_pay'], np.linspace(0, 1000, 100), alpha = 0.5, label='Unpaid')\n    plt.hist(df[df['final_payment_status']=='Cancelled']['time_to_pay'], np.linspace(0, 1000, 100), alpha = 0.5, label='Cancelled')\n    plt.hist(df[df['final_payment_status']=='Other']['time_to_pay'], np.linspace(0, 1000, 100), alpha = 0.5, label='Other')\n    plt.legend(loc='upper right')\n    plt.title('Number of unique Times to Pay by Final Payment Status') \n    plt.xlabel('Times to Pay') \n    plt.ylabel('Count')\n    print(\"With a positively-skewed distribution of Times to Pay for all categories by Final Payment Status, there is not much insight to be drawn from this variable. We'll draw the same if not greater information from Final Payment Status. \\nFurthermore with most empty\/NaN time_to_pays having a status of either Unpaid, Cancelled or Other in only a few rows, I conclude that Times to Pay is not a good indicator for a customer's payment status and will not be a final variable in the model.\")","e7e988ad":"EDA1(train_clean)","ee0322cf":"#Sort engagement_scores into engagement_categories by quartile ranges:\n    #1 represents the 0-25% quartile range\n    #2 represents the 25-50% quartile range\n    #3 represents the 50-75% quartile range\n    #4 represents the 75-100% quartile range\n    #These will be our buckets:\ndef categorize_engagements(df):\n    describe = df.describe().copy()\n    ebin = [describe.loc['min']['engagement_score'],describe.loc['25%']['engagement_score'],describe.loc['50%']['engagement_score'],describe.loc['75%']['engagement_score'],describe.loc['max']['engagement_score']]\n    ecategory = [1,2,3,4]\n    df['engagement_category'] = pd.cut(df['engagement_score'],ebin,labels = ecategory, include_lowest=True)\n    return(df)","e5412464":"#The following function calls upon categorize_engagements to bucket \ndef EDA2(df):\n    categorize_engagements(df)\n    df_pivot=df.groupby(['engagement_category','final_payment_status']).size().to_frame('count').reset_index()\n    df_pivot.pivot(index='final_payment_status',columns='engagement_category',values='count')\n    sns.barplot(data=df_pivot, x='final_payment_status', y='count', hue='engagement_category',palette='viridis')\n    plt.title('customers sorted by engagement category')\n    print(\"A clear trend appears where the likelihood of highly engaged customers to be in the 'Paid' payment status increases, while the likeihood of unengaged customers to be in the 'Cancelled' or 'Unpaid' payment status increases.\")\n    return df_pivot.pivot(index='final_payment_status',columns='engagement_category',values='count')","64d82003":"EDA2(train_clean)","da748b7f":"def EDA3_1(df):\n    #call upon the first function that buckets the customers by engagement:\n    categorize_engagements(df)\n\n    #perform a pivot to wrangle our data to find the relationship between final_payment_status + num_prior_orders_unpaid_sum, hued by engagement_category:\n    df_pivot=df.groupby(['engagement_category','final_payment_status']).num_prior_orders_unpaid.sum().to_frame('num_prior_orders_unpaid_sum').reset_index()\n    df_pivot.pivot(index='final_payment_status',columns='engagement_category',values='num_prior_orders_unpaid_sum')\n\n    #plot graph\n    sns.barplot(data=df_pivot, x='final_payment_status', y='num_prior_orders_unpaid_sum', hue='engagement_category',palette='viridis')\n    plt.title('customers sorted by num_prior_orders_unpaid')\n    print(\"A trend appears where the total number of prior orders unpaid by engagement category is different for each payment status. Because more unengaged customers fall in the Unpaid category, it is natural that they will have a higher total of number of prior orders unpaid. This is also true for highly engaged customers, who have more number of prior orders unpaid because there are more highly engaged customers who fall in the Paid category.\")\n    return df_pivot.pivot(index='final_payment_status',columns='engagement_category',values='num_prior_orders_unpaid_sum')","c08034ef":"EDA3_1(train_clean)","fb898274":"def EDA3_2(df):\n    #iterate through Yes, No and Unknown. Pivot logic is similar to EDA3_1:\n    for i in df['custom_meal_selection'].unique():\n        plt.figure(figsize=[8,4])\n        df_pivot = df[df['custom_meal_selection'] == i].groupby(['engagement_category','final_payment_status']).custom_meal_selection.count().to_frame(i + '_count').reset_index()\n        df_pivot.pivot(index='final_payment_status',columns='engagement_category',values= i +'_count')\n        sns.barplot(data=df_pivot, x='final_payment_status', y=i +'_count', hue='engagement_category',palette='viridis')\n        plt.title('customers with custom meal selection = ' + i +'_count')\n        plt.show()\n        \n    print(\"A clear trend we can see is that customers who tend to have custom_meal_selections tend to not only be highly engaged, but also belong in the 'Paid' status.\")\n    print(\"While the same trend can be see for customers without custom_meal_selections, there are also a higher number of customers who belong in the 'Unpaid' status.\")\n    print(\"Finally, customers who have unknown custom_meal_selections generally belong in the 'Cancelled' status.\")\n    print(\"My initial assumption was that Unknown statuses were corrupt and could be rewritten as No's later on in our Model. However, I am now more inclined to create a binary variable for Yes\/No vs Unknown.\")","188412a7":"EDA3_2(train_clean)","0cb8ed07":"def EDA3_3(df):\n    #iterate through all channels. Code is very similar to the function EDA3_2\n    for i in df['channel'].unique():\n        plt.figure(figsize=[8,4])\n        df_pivot = df[df['channel'] == i].groupby(['engagement_category','final_payment_status']).custom_meal_selection.count().to_frame(i + '_count').reset_index()\n        df_pivot.pivot(index='final_payment_status',columns='engagement_category',values= i +'_count')\n        sns.barplot(data=df_pivot, x='final_payment_status', y=i +'_count', hue='engagement_category',palette='viridis')\n        plt.title('customers with channel = ' + i +'_count')\n        plt.show()\n        \n    print(\"The distribution of customers in channels by final payment statuses is largely the same throughout all channels. I cannot draw any concrete insights if more of a status is relative to their marketing channel.\")\n    print(\"The biggest insight we can draw from this is that most customers come from the F1 and F2 marketing channel, which validates using is_f as a binary variable for f1\/f2 vs everything else.\")","20975deb":"EDA3_3(train_clean)","bb523022":"#We'll call this function the required feature engineering, which performs feature engineering on variables explicitly stated in the task:\ndef required_feature_engineering(df):\n    #1st feature engineering involves changing avg_recipe_rating into a binary variable:\n    df = df.fillna(value = {'avg_recipe_rating':0})\n    df['ever_rated_recipe'] = np.where(df['avg_recipe_rating'] == 0,0,1)\n\n    #2nd feature engineering involves changing channel into a binary variable as well:\n    df['is_f'] = np.where(df['channel'].isin(['F1','F2']),1,0)\n\n    #Finally, we'll be altering our y-variable into a binary variable as either Paid or Unpaid:\n    df['Paid'] = np.where(df['final_payment_status'].isin(['Paid','Cancelled','Other','Refunded']),1,0)\n\n    #We can now drop the old variables as they will not have any use to us anymore:\n    df.drop('avg_recipe_rating', axis=1,inplace=True)\n    df.drop('channel', axis=1,inplace=True)\n    df.drop('final_payment_status', axis=1,inplace=True)\n    return(df)","d6481869":"train_clean = required_feature_engineering(train_clean)","44b6da97":"def custom_feature_engineering(df):\n    #transform payment_method \n    label = LabelEncoder()\n    df['payment_method']=label.fit_transform(df['payment_method'])\n    df.payment_method = df.payment_method.astype('category')\n\n    #From our EDA, my assumption that custom_meal_selection's unknowns would closely relate to the distribution of no custom_meal_selections.\n    #However after our EDA, 'Yes' and 'No' are more closely related. Therefore, we'll create a new binary variable for (Yes\/No) vs Unknown:\n    df['not_unknown'] = np.where(df['custom_meal_selection'].isin(['Yes','No']),1,0)\n\n    #We'll also change delivery_date to a binary_variable is_weekend:\n    #create the date from delivery_date then use dayofweek to check if it is a weekend or not.\n    dates = pd.to_datetime({\"year\": df['delivery_date'].astype(str).str[:4].astype(int)\n    , \"month\": df['delivery_date'].astype(str).str[4:6].astype(int)\n    , \"day\": df['delivery_date'].astype(str).str[6:8].astype(int)})\n    df[\"Is Weekend\"] = np.where(dates.dt.dayofweek > 4,1,0)\n    \n    #We'll be dropping delivery_date, custom_meal_selection, time_to_pay and engagement_score.\n    df.drop('delivery_date', axis=1,inplace=True)\n    df.drop('custom_meal_selection', axis=1,inplace=True)\n    df.drop('time_to_pay', axis=1,inplace=True)\n    df.drop('engagement_score', axis=1,inplace=True)\n    #df.drop('payment_method', axis=1,inplace=True)\n    \n    #Finally, we'll be scaling our data because most of our entries are now binary, but variables such as engagement_scores are very large in comparison. Therefore, we will scale the data so columns now have mean of 0 and stdev of 1.\n    #We don't want to standardize our y-variable. We will extract it, standardize the rest and finally add it back:\n    y = df['Paid']\n    x = df.drop('Paid',axis=1)\n    sc = StandardScaler()\n    df2 = sc.fit_transform(x)\n    df= pd.DataFrame(df2, columns = list(x.columns))   \n    df['Paid'] = np.array(y)\n    return(df)","916d7bed":"train_clean = custom_feature_engineering(train_clean)","51693713":"train_clean","102cab1d":"quality_check(test)","f3527f4e":"test_clean = test.copy()\ntest_clean = data_clean(test_clean)","43177f24":"test_clean.head()","520854a7":"EDA1(test_clean)","7a7031f4":"EDA2(test_clean)","b3bf8679":"EDA3_1(test_clean)","71e17e74":"EDA3_2(test_clean)","f9a2519f":"EDA3_3(test_clean)","5729d3c3":"test_clean = required_feature_engineering(test_clean)","de69e7a8":"test_clean = custom_feature_engineering(test_clean)","88318293":"test_clean","3006accd":"#load necessary regression packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","39725f5d":"#set our training and test predictors:\ny_train=train_clean['Paid']\ny_test=test_clean['Paid']","b59c3fba":"#set our training and test dataset:\nX_train=train_clean.drop('Paid',axis=1)\nX_test=test_clean.drop('Paid',axis=1)","a868c1d9":"# instantiate the model (using the default parameters):\nlogreg = LogisticRegression()\n\n# fit the model with data:\nlogreg.fit(X_train,y_train)\n\n#output our predicted y_values after training and putting in our test data.\ny_pred=logreg.predict(X_test)","293fdbe6":"#print our accuracy score:\nprint(accuracy_score(y_pred,y_test))","ace3c178":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","984ad8cb":"#summary of our logistic regression model:\nimport statsmodels.api as sm\nlogit_model=sm.Logit(y_test,X_test)\nresult=logit_model.fit()\nprint(result.summary2())","9cce20b9":"<i style=\"font-size:20px\">Custom Feature Engineering<\/i>\n\nFollowing my initial assumptions, we will now look to further transform variables we haven't dealt with.\n\n<p>1. <b>I will be label-encoding payment_method<\/b>. This variable is largely uninterpretable having V,W,X,Y & Z as entries, but once understood - this could serve as an important variable.<\/p>\n\n<p>2. As mentioned above, <b>I will now create the binary variable is_unknown to replace the variable custom_meal_selection<\/b>.<\/p>\n\n<p>3. <b>I will also be converting delivery_date to the binary variable is_weekend<\/b>.<\/p>\n\n<p>4. <b>Engagement_score is no longer needed and will be dropped<\/b> since we already have engagement_category which was derived from engagement_score. Keeping both would simply overfit our future model.<\/p>\n\n<p>5. Like point 4, <b>I will drop all unnecessary variables after creating these new ones<\/b>.<\/p>\n\n<p>6. After creating so many new binary variables, <b>I will have to standardize my columns so future model choices will be able to deal with a variety of variables with extremely different proportions.<\/b><\/p>","412f411f":"<u style=\"font-size:20px\"><i>Part Two: SQL<\/i><\/u>\n<p> 1. \n    A list of unique <i><b>order_numbers<\/b><\/i> that failed payment with delivery dates between\n2020-02-08 and 2020-02-12 (inclusive) in Australia.<\/p>\n<p> 2. For each order that failed payment: <\/p>\n\n<p style=\"margin-left: 40px\">a. If the order was eventually paid, the number of days between the <b>first<\/b> failed\npayment and it being paid for by the customer<\/p>\n\n<p style=\"margin-left: 40px\">b. If the order was never paid, put <i><b>never_paid<\/b><\/i><\/p>\n\n<p> 3. <i><b>customer_id, name, phone number<\/b><\/i> and <i><b>order_number<\/b><\/i> for the most recent order (by delivery date) per customer","482ae030":"<u style=\"font-size:20px\"><i>Part Two Reponses<\/i><\/u>\n\nThe following answers are in SQL Server\/T-SQL syntax:\n\n<b>1.<\/b> \n<p style=\"margin-left: 20px\">select distinct f.order number <\/p>\n<p style=\"margin-left: 20px\">from<\/p>\n<p style=\"margin-left: 40px\">order_payment_history o<\/p>\n<p style=\"margin-left: 40px\">left join orders f on<\/p>\n<p style=\"margin-left: 60px\">o.order_number = f.order_number<\/p>\n<p style=\"margin-left: 20px\">where<\/p>\n<p style=\"margin-left: 40px\">f.country = 'Australia'<\/p>\n<p style=\"margin-left: 40px\">and o.status = 'payment_failed'<\/p>\n<p style=\"margin-left: 40px\">and cast(f.delivery_date as date) between '2020-02-08' and '2020-02-13' <\/p>\n\n<b>2.<\/b> \n<p style=\"margin-left: 20px\">with failed_order_numbers as (<\/p>\n<p style=\"margin-left: 40px\">select order_number<\/p>\n<p style=\"margin-left: 40px\">,time<\/p>\n<p style=\"margin-left: 40px\">,[rw] = row_number() over (partition by order_number order by time)<\/p>\n<p style=\"margin-left: 40px\">from <\/p>\n<p style=\"margin-left: 60px\">order_payment_history <\/p>\n<p style=\"margin-left: 40px\">where<\/p>\n<p style=\"margin-left: 60px\">status = 'payment_failed')<\/p>\n<p style=\"margin-left: 20px\">,first_fail as (<\/p>\n<p style=\"margin-left: 40px\">select order_number<\/p>\n<p style=\"margin-left: 40px\">,time<\/p>\n<p style=\"margin-left: 40px\">from <\/p>\n<p style=\"margin-left: 60px\">failed_order_numbers <\/p>\n<p style=\"margin-left: 40px\">where<\/p>\n<p style=\"margin-left: 60px\">[rw] = 1)<\/p>\n<p style=\"margin-left: 20px\">,paid_or_not as (<\/p>\n<p style=\"margin-left: 40px\">select order_number<\/p>\n<p style=\"margin-left: 40px\">,time<\/p>\n<p style=\"margin-left: 40px\">from <\/p>\n<p style=\"margin-left: 60px\">order_payment_history <\/p>\n<p style=\"margin-left: 40px\">where<\/p>\n<p style=\"margin-left: 60px\">status = 'order_paid')<\/p>\n<p style=\"margin-left: 20px\">select [result] = <\/p>\n<p style=\"margin-left: 20px\">case when p.time is null then 'never_paid' else datediff(day, f.time, p.time) end<\/p>\n<p style=\"margin-left: 20px\">from <\/p>\n<p style=\"margin-left: 40px\">failed_order_numbers f<\/p>\n<p style=\"margin-left: 40px\">left join paid_or_not p on<\/p>\n<p style=\"margin-left: 60px\">f.order_number = p.order_number<\/p>\n\n<b>3.<\/b> \n<p style=\"margin-left: 20px\">with sort_orders as (<\/p>\n<p style=\"margin-left: 40px\">select c.customer_id<\/p>\n<p style=\"margin-left: 40px\">,c.name<\/p>\n<p style=\"margin-left: 40px\">,[phone_number] = c.phone<\/p>\n<p style=\"margin-left: 40px\">,f.order_number<\/p>\n<p style=\"margin-left: 40px\">,[rw] = row_number() over (partition by o.customer_id order by f.delivery_date desc)<\/p>\n<p style=\"margin-left: 40px\">from <\/p>\n<p style=\"margin-left: 60px\">orders f<\/p>\n<p style=\"margin-left: 60px\">left join customers c on<\/p>\n<p style=\"margin-left: 80px\">f.customer_id = c.customer_id<\/p>\n<p style=\"margin-left: 20px\">select customer_id<\/p>\n<p style=\"margin-left: 40px\">,name<\/p>\n<p style=\"margin-left: 40px\">,phone_number<\/p>\n<p style=\"margin-left: 40px\">,order_number<\/p>\n<p style=\"margin-left: 20px\">from sort_orders<\/p>\n<p style=\"margin-left: 20px\">where<\/p>\n<p style=\"margin-left: 40px\">[rw] = 1)<\/p>\n\n\n\n\n\n\n\n\n\n","b204dd2b":"We will first build our first function to do an initial quality check. \n\nBy containing our work in a function, it allows for greater collaboration between other Hellofresh Data Scientists who are likely to utilise these functions and increase their productivity.","960a0a9c":"<u style=\"font-size:20px\"><i>Part One Reponses<\/i><\/u>\n\n<b>1.<\/b>\n    \n<p style=\"margin-left: 20px\">The Goal for our Model is to accurately decide for the logistics team whether a specific customer with a failed payment should be shipped a box within at least a few hours.<\/p>\n\n<b>2.<\/b> \n    \n<p style=\"margin-left: 20px\">Shipping orders that fail payments to customers who are likely to fulfill future payments is profitable, but shipping orders to customers who aren't likely tois not profitable.<\/p> \n\n<p style=\"margin-left: 20px\">Handling the problem of involuntary customer churning by shipping orders that fail payments can potentially take a customer who is somewhat satsified with the Hellofresh service into just dissatisfied enough that they become voluntary churn, losing not only lifetime value of that customer but also the contribution to the budget for acquiring more customers.<\/p> \n \n<p style=\"margin-left: 20px\">In my opinion: although involuntary customer churning can be a pain point for subscription businesses, an approach that successfully recovers failed payments from a customer's human errors (such as credit card expiry) and is also effortless on their end to rectify can have a positive effect on churn and retention rates.<\/p> ","055dc700":"<i style=\"font-size:20px\">Required Feature Engineering<\/i>\n\nWe will begin using feature engineering to transform\/manipulated some variables into something much more usable in our final model.\n\nAfter our EDA, I have new assumptions:\n\n<p>1. With most customers being brought in from F1\/F2 marketing channels, <b>it validates using is_f as a binary variable<\/b>.<\/p>\n\n<p>2. <b>Time_to_pay will be a variable I will drop<\/b>, as explained in my first EDA function.<\/p>\n\n<p>3. I initally thought custom_meal_selection being strictly Yes or No meant that Unknown was likely to be No's that were corrupted. But after our EDA, <b>it's clear that Yes\/No's are more closely distributed compared to Unknown<\/b> if analyzing by final_payment_status.<\/p>\n\n<p>4. With our new variable engagement_category and our EDA, we should expect that <b>highly engaged customers have a higher likelihood of fulfilling their future payments vs unengaged customers<\/b>.","6a55cf48":"Now, I'll simply run all the reusable functions in my test data:","9ebee401":"The above information reveals to use the coefficients, standard error, z-score, p-values and confidence intervals of our variables.\n\nThe most important metrics are the p-values, or the $P > |z|$ values. \n\nAs a rule of thumb, if our p-values are < 0.05, then we can reject the Null Hypothesis that the variable against the p-value has no input in the regression in our target variable.\n\nOut of all of our variables, we can see that the strongest variables are <b>num_prior_orders_unpaid<\/b>, <b>engagement_category<\/b> + <b>not_unknown<\/b> which was feature engineered from custom_meal_selection.\n\nOne of our weakest variables was <b>is_weekend<\/b> (although we stil failed to reject the Null Hypothesis for num_prior_orders_failed, ever_rated_recipe and is_f, so keep that in mind).\n\n<p style=\"margin-left: 40px\">-  A reasonable explanation for is_weekend being so low is that our test data only spanned across one week between February & March 2020. With the lack of public holidays, or any seaonality coming into play, this variable was not useful and would be dropped in future model creations.","140e851c":"We will now create our second function to some an initial cleaning of our data.","0edf6a38":"<u style=\"font-size:20px\"><i>Part Three: Python<\/i><\/u>\n\nThis section should be completed using Python, preferably with Jupyter notebooks.\nKey assessment criteria:\n<p style=\"margin-left: 40px\">\u25cf Code structure and readability<\/p>\n<p style=\"margin-left: 40px\">\u25cf Code reusability (use of functions)<\/p>\n<p style=\"margin-left: 40px\">\u25cf Thought process explained through comments or text blocks<\/p>\n<p style=\"margin-left: 40px\">\u25cf Insights extracted<\/p>\n\nUsing the data from training_data.csv write Python code to:\n<p style=\"margin-left: 40px\">1. Perform data cleaning and check data accuracy. These checks will be applied to the\ntest data in question 5 so use functions to make the code reusable.<\/p>\n<p style=\"margin-left: 80px\">a. Write a function to check data quality<\/p>\n<p style=\"margin-left: 80px\">b. Write a function to clean the data<\/p>\n\n\n<p style=\"margin-left: 40px\">2. Write functions to build charts and perform EDA (exploratory data analysis). Provide\ncomments for all charts.<\/p>\n<p style=\"margin-left: 80px\">a. Produce one chart to examine the target variable and one to examine\ntime_to_pay.<\/p>\n<p style=\"margin-left: 80px\">b. Produce a chart to examine the relationship between engagement_score\nand the target variable (hint: it might be helpful to bucket customers based on\nthe engagement_score).<\/p>\n<p style=\"margin-left: 80px\">c. Produce separate charts to examine the relationship between the target\nvariable and the following features: num_prior_orders_unpaid,\ncustom_meal_selection, channel. The insights might differ between\ncustomers with low\/high engagement scores - make sure you account for this.<\/p>\n<p style=\"margin-left: 40px\">3. Write a function to perform feature engineering:<\/p>\n<p style=\"margin-left: 80px\">a. Create an ever_rated_recipe binary feature based on the\navg_recipe_rating column<\/p>\n<p style=\"margin-left: 80px\">b. Create an is_f binary feature based on the channel (include f1 and f2)<\/p>\n<p style=\"margin-left: 40px\">4. Train an ML model to predict whether orders will be paid or unpaid. Use the customer\nfeatures provided. Note:<\/p>\n\n<p style=\"margin-left: 40px\">\u25cf Treat orders with payment status of Cancelled, Refunded or Other as Paid<\/p>\n\n<p style=\"margin-left: 40px\">\u25cf Include features that you engineered in question 3.<\/p>\n\n<p style=\"margin-left: 40px\">\u25cf Use functions (or other methods of reusable code) to prepare your data for the\nmodel.<\/p>\n\n<p style=\"margin-left: 40px\"> Focus on model evaluation. Evaluate model performance and discuss the results.<\/p>\n\nUsing the data from test_data.csv write Python code to:\n<p style=\"margin-left: 40px\">5. Make predictions using the model built in Q4:\n<p style=\"margin-left: 80px\">a. Check and clean the data (reuse functions from Q1)\n<p style=\"margin-left: 80px\">b. Transform the data (reuse functions from Q4)\n<p style=\"margin-left: 80px\">c. Make predictions\n<p style=\"margin-left: 80px\">d. Evaluate model performance and discuss the results.\n","32e3bff1":"<u style=\"font-size:20px\"><i>Assumptions and Decisions on attributes after an initial meet and greet of our dataset:<\/i><\/u>\n\n\n<p>It is important for the builder of a Machine Learning Model to state all assumptions, since these will lead to justified decisions in how the data is wrangled.<\/p>\n\n<p>1. <b>I'll be dropping customer_id and order_id<\/b> as unique IDs have no prediction value in our model.<\/p>\n\n<p>2. Delivery_Date as a variable is not very useful on its on as an input in our model. However, <b>I am considering transforming it into a binary variable is_weekend<\/b> (1 if it is a Saturday\/Sunday, 0 for other days of the week), as the weekday\/weekend disparity has proven time and time again to affect a customer's buying patterns. By extension, seasonality also affect the customer, but our training data is enclosed within a month, so we won't be using Seasonality as a variable.<\/p>\n\n<p>3. Delivery_week is not only an attribute that can be derived from delivery_date, but it tends towards being a zero variance predictor (only having 1 entry across all observations). <b>Therefore, we'll be dropping this as well.<\/b><\/p>\n\n<p>4. <b>Final_Payment_Status will be our target variable.<\/b> Furthermore, we'll be altering this variable in our feature engineering, where we we'll only have 'Paid'\/'Unpaid' (or 1\/0) as our y-variable (changing 'Cancelled', 'Refunded' + 'Other' to 'Paid').<\/p>\n\n<p>5. There are quite a few NaN Values for time_to_pay. But considering it will be part of our EDA later, I'll leave it as it is until EDA. What I'm expecting is that the NaN values can either:<\/p> \n<p style=\"margin-left: 20px\">- find a legitmate reason for NaN for some instances<\/p>\n<p style=\"margin-left: 20px\">- be replaced with something simple such as median<\/p>\n<p style=\"margin-left: 20px\">- or dive deeper into KNN Input <\/p>\n\n<p>6. <b>We could potentially do some one-hot encoding\/label-encoding on payment_method<\/b> (although not explicitly required in our feature engineering task).<\/p>\n\n<p>7. Similarly to time_to_pay, the 'Unknown' value is an invalid entry for custom_meal_selection (should only be 'Yes' or 'No'). However because it will also be part of our EDA later, I'll leave it as it is. <b>What I'm hoping for is that 'No' and 'Unknown' will be similar by engagement_score<\/b>, and I can make a more valid assumption to replace 'Unknown' values with 'No'.<\/p>\n\n<p>8. State is largely uninterpretable and is corrupted with &lt;error&gt; entries as well as some number entries. <b>Therefore, I'll be dropping this variable.<\/b><\/p>\n\n<p>9. Channel will later be transformed into a binary variable is_f (with f1 and f2 as 1, and everything else as 0).<\/p>\n\n<p>10. A criteria we're given is that engagement_score must be greater than 0. Given our data quality check shows a low number of observations with invalid engagement_scores, <b>we'll simply drop these low number of invalid rows.<\/b><\/p>\n\n<p>11. Luckily num_prior_orders_failed & num_prior_orders_unpaid are continuous non-null entries and require minimal cleaning.<\/p>\n\n<p>12. Avg_recipe_rating is a culprit for having an extraordinarily large number of NaNs. However since it will be transformed into a binary variable ever_rated_recipe, this will be dealt with later in our feature engineering steps.<\/p>","031b7ce6":"<i style=\"font-size:20px\">EDA 2<\/i>\n\n\nBefore I build our second EDA function, I will build a preliminary function to bucket customers based on their engagement_score. \n\nThe simplest method would be to bucket them via their <b>Interquartile Ranges<\/b>:\n\nAt the end of the function's resolution, we will have a new column called engagement_category which is either a value of 1,2,3 or 4 representing their quartile ranges.","c9d57771":"Now analyzing final_payment_status vs custom_meal_selection:","567bd68a":"<u style=\"font-size:20px\"><i>Part Three Reponses<\/i><\/u>\n","5f5792f2":"We will now build our second function that calls on the previous function to analyse the a customer's final_payment_status by their engagement_category buckets:","d8edc0f3":"We will copy the training data in 'train_clean' to wrangle with.","cdd58753":"The goal of building this Machine Learning model is to assist the logistics team in making a quick and accurate decision on whether to ship orders to customers who failed payments. As previously mentioned, shipping orders to customers who are likely to fulfill future payments is profitable to Hellofresh, but the opposite is detrimental.\n\nThis model's main business goal is to alleviate a likely pain point of customer churn in the Hellofresh's subscription-based business model. Business cases have shown that minimizing customer churn can have a significant increase in profit.","f6aab3f4":"<u style=\"font-size:20px\"><i>Running Functions for Test Data<\/i><\/u>\n","25652cab":"<i style=\"font-size:20px\">EDA 3<\/i>\n\n\nThe third EDA is to analyse the relationship between our final_payment_status and a few other features. The first one will be num_prior_orders_unpaid:","e28cb8a0":"Returning the confusion matrix (by row) of True Positives, False Negatives, False Positives & True Negatives:\n\nTo summarize the confusion matrix, model returned a True Negative or Positive 4694 times, while returning a False Negative or Positive 458 times.\n\nThis is where we get our 92.77% accuracy:\n \nAccuracy score $= \\frac{19+4576}{19+4576+344+13} = 92.79\\%$","b2e274ad":"<u style=\"font-size:20px\"><i>Part One: Problem Solving<\/i><\/u>\n\n<p>1. The CEO is very interested in your idea to make a model, and would like you to\nsummarise your goal. Write a problem statement: i.e. summarise the challenge you\nare trying to solve in one sentence\/question.\nTry to make your problem statement SMART - specific, measurable, achievable,\nrealistic and time-bound.<\/p>\n    \n\n<p>2. The CEO also wants to know: Do you think shipping orders that fail payments is\nprofitable? Briefly explain your opinion.<\/p>","86e88a3a":"Our accuracy score was a solid <b>92.79%<\/b>.\n\nThis means that more than 9 times out of 10, we would not only quickly but accurately inform the logistics team of a decision that would be profitable to Hellofresh in minimizing customer churn.","462da2ff":"Throughout this model, we'll be testing these functions on our training data. We will use these functions at the end on our test data.","f9abc84a":"<u style=\"font-size:20px\"><i>Conclusion<\/i><\/u>\n\n<p>1. A combination of:\n\n<p style=\"margin-left: 40px\">- A customer's track record of number of prior orders unpaid,<\/p>\n\n<p style=\"margin-left: 40px\">- How engaged the customer is and what quartile bucket they are in,<\/p>\n\n<p style=\"margin-left: 40px\">- And if a customer's custom meal selections is Unknown or not<\/p>\n\nmaximises the likelihood of a customer fulfilling future payments if we ship orders on failed payments.<\/p>\n\n<p>2. Code reusability in creating functions largely helped us in checking any new dataset's quality as well as its wrangling, EDA and feature engineering process.<\/p>\n\n<p>3. Many insights were drawn and had to be rethought throughout the model as more and more insights were drawn.<\/p>\n\n<p>4. Logistic Regression was a strong choice in model selection as we reached an accuracy score of over 92%.<\/p>\n\n\n","137f95c5":"<i style=\"font-size:20px\">EDA 1<\/i>\n\nOur first EDA will be to explore our target variable (final_payment_status) against time_to_pay.\n\nAgain, the EDA will be contained in a function for future use in our test data.","a933b014":"<u style=\"font-size:20px\"><i>Model Choice<\/i><\/u>\n\nIf we were to optimise our accuracy, one would spend time running almost all Machine Learning Models and then choosing the best one based on accuracy. But for computational simplicity, we will just choose one particular Machine Learning model. \n\nThe most obvious choice would be <b>logistic regression<\/b> and that's what I will choose. Not only is it simple to use, but it is a great choice for our wrangled dataset as we have binary outcomes from 0 to 1.\n","719bf302":"<u style=\"font-size:20px\"><i>Exploratory Data Analysis<\/i><\/u>"}}