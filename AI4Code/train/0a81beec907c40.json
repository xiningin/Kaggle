{"cell_type":{"5f85b1c7":"code","3ca9e198":"code","73442556":"code","8475a6ed":"code","507d327f":"code","5df86c7c":"code","44174e95":"code","f90d6803":"code","356ff55f":"code","80357165":"code","88b002d1":"code","1b9ac75e":"code","efca4f69":"code","1893e592":"code","3d0bb5b6":"code","d5cb6a70":"markdown"},"source":{"5f85b1c7":"import numpy as np\nimport pandas as pd\nimport gc\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data\nfrom sklearn.preprocessing import StandardScaler","3ca9e198":"train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv').drop('id', axis=1)\ntest = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv').drop('id', axis=1)\nss = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","73442556":"train.head()","8475a6ed":"X = train.drop(\"target\", axis=1).copy()\ny = train[\"target\"].copy()\nX_test = test.copy()\n\ndel train\ngc.collect()\ndel test\ngc.collect()","507d327f":"scaler = StandardScaler()\n\nX = pd.DataFrame(columns=X.columns, data=scaler.fit_transform(X))\nX_test = pd.DataFrame(columns=X_test.columns, data=scaler.transform(X_test))","5df86c7c":"class CustomDataset:\n    \n    def __init__(self, data, target=None):\n        self.data = data\n        self.target = target\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \n        if self.target is not None:\n            current_sample = self.data.values[idx]\n            current_target = self.target.values[idx]\n            \n            return torch.tensor(current_sample, dtype= torch.float), torch.tensor(current_target, dtype= torch.float)\n        else:\n            current_sample = self.data.values[idx]\n            return torch.tensor(current_sample, dtype= torch.float)","44174e95":"class NNModel(nn.Module):\n    \n    def __init__(self, features, activation = F.relu):\n        super(NNModel, self).__init__()\n        \n        \"\"\"Number of input is no of features(100)\"\"\"\n        self.layer_1 = nn.Linear(features, 264)\n        self.batchnorm1 = nn.BatchNorm1d(264)\n        self.layer_2 = nn.Linear(264, 128)\n        self.batchnorm2 = nn.BatchNorm1d(128)\n        self.layer_3 = nn.Linear(128, 64)\n        self.batchnorm3 = nn.BatchNorm1d(64)\n        self.layer_4 = nn.Linear(64,32)\n        self.batchnorm4 = nn.BatchNorm1d(32)\n        self.layer_out = nn.Linear(32,1)\n        self.flatten = nn.Flatten()\n        self.activation = activation\n        \n    def forward(self, x):\n        x = self.flatten(x)\n        x = self.batchnorm1(self.activation(self.layer_1(x)))\n        x = self.batchnorm2(self.activation(self.layer_2(x)))\n        x = self.batchnorm3(self.activation(self.layer_3(x)))\n        x = self.batchnorm4(self.activation(self.layer_4(x)))\n        x = torch.sigmoid(self.layer_out(x))\n        \n        return torch.squeeze(x, dim=1)","f90d6803":"def initialize_weights(self):\n    for m in self.modules():\n        if isinstance(m, nn.Linear):\n            nn.init.kaiming_uniform_(m.weight.data, nonlinearity=\"relu\")\n            \n            if m.bias is not None:\n                nn.init.constant_(m.bias.data, 0)\n        \n        elif isinstance(m, nn.BatchNorm1d):\n            nn.init.constant_(m.weight.data, 1)\n            nn.init.constant_(m.bias.data, 0)","356ff55f":"device = \"cuda\" if torch.cuda.is_available() else 'cpu'\nBATCH_SIZE = 1024\nfeatures = X.columns","80357165":"def train_model(dataloader, model, criterion, optimizer):\n    size = len(dataloader.dataset)\n    model.train()\n    batches = len(dataloader)\n    train_loss = 0\n    \n    for batch_idx, (X,y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n        \n        scores = model(X)\n        loss = criterion(scores, y)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        loss = loss.item()\n        train_loss += loss\n        \n    train_loss_avg = train_loss\/batches\n    print(f\"avg. train loss: {train_loss_avg}\")\n    return train_loss_avg","88b002d1":"def val_model(dataloader, model, criterion):\n    \n    size= len(dataloader.dataset)\n    batches= len(dataloader)\n    model.eval()\n    test_loss= 0\n\n    with torch.no_grad():\n        for X, y in (dataloader):\n            X, y= X.to(device), y.to(device)\n      \n            scores= model(X)\n            test_loss += criterion(scores, y)\n\n    test_loss \/= batches\n    print(f\"avg test loss : {test_loss}\")\n    return test_loss","1b9ac75e":"def predict_model(dataloader, model):\n    model.eval()\n    y_pred= np.array([])\n    \n    with torch.no_grad():\n        for X in dataloader:\n            X = X.to(device)\n            \n            preds= model(X)\n            preds= preds.flatten().cpu().numpy()\n            \n            y_pred= np.concatenate((y_pred, preds))\n            \n    return y_pred","efca4f69":"from sklearn.model_selection import StratifiedKFold\n\nKFold = StratifiedKFold(n_splits=5, random_state=2021, shuffle=True)\nEPOCHS = 100\ncv_scores = []\npredictions = np.zeros(X_test.shape[0])\n\nfor fold, (train_idx, val_idx) in enumerate(KFold.split(X, y)):\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n    \n    train_dataset = CustomDataset(data=X_train, target=y_train)\n    val_dataset = CustomDataset(data=X_val, target=y_val)\n    \n    train_loader = data.DataLoader(train_dataset, batch_size = BATCH_SIZE)\n    val_loader = data.DataLoader(val_dataset, batch_size = BATCH_SIZE)\n    \n    model = NNModel(features=len(features), activation=F.hardswish).to(device)\n    model.apply(initialize_weights)\n    \n    criterion = nn.BCELoss()\n    criterion.to(device)\n    \n    optimizer= optim.Adam(model.parameters(), lr= 0.001)\n    scheduler= optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n                                                    factor= 0.5,\n                                                    patience= 10,\n                                                    verbose= True)\n    best_valid_loss = float('inf')\n    \n    avg_train_losses = []\n    avg_val_losses = []\n    \n    print(10*\"::\", f\"Fold={fold+1}\", 10*\"::\")\n    \n    for t in range(EPOCHS):\n        print(f\"Epoch: {t+1}\")\n        train_loss = train_model(train_loader, model, criterion, optimizer)\n        val_loss = val_model(val_loader, model, criterion)\n        \n        avg_train_losses.append(train_loss)\n        avg_val_losses.append(val_loss)\n        \n        if (val_loss < best_valid_loss):\n            best_valid_loss= val_loss\n            ofilename = 'TPS%d.pth' % fold\n            torch.save(model.state_dict(),  ofilename)\n        \n        scheduler.step(val_loss)\n        \n    cv_scores.append(best_valid_loss)\n    \n    test_dataset = CustomDataset(data = X_test, target = None)\n    test_loader = data.DataLoader(test_dataset, batch_size = BATCH_SIZE)\n                       \n    model.load_state_dict(torch.load('TPS%d.pth' % fold, map_location=device))\n    predictions += (predict_model(test_loader, model)\/5)","1893e592":"ss['target'] = predictions\nss.to_csv('submission.csv',index = False)","3d0bb5b6":"ss.head()","d5cb6a70":"# Reference\n\n* https:\/\/www.kaggle.com\/sagarikajadon\/simple-lstm-pytorch"}}