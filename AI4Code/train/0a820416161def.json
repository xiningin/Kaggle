{"cell_type":{"1b33bd46":"code","3602727d":"code","a6a202a9":"code","5b88f00b":"code","5ccd0b94":"code","edd57db5":"code","da37cec4":"code","b98004a2":"code","a349eac1":"code","1790375a":"code","9268da4a":"code","97f32375":"code","9a7111f7":"code","218bc49b":"code","62f8b310":"code","4edd31cf":"code","03a8a576":"code","02fa1647":"code","446fa80d":"code","16d294d5":"code","527d5948":"code","051e0caf":"code","85961e5f":"code","3aff8261":"code","0924b7ec":"code","2ee7127c":"code","b52dd31f":"code","bbaf6ea9":"code","e106424d":"code","bb442bb6":"code","c23cd1b9":"code","d8e4bd4f":"code","af68a21f":"code","42241aa0":"code","953430e5":"code","b37cdb8a":"code","cf249999":"code","40e4f666":"code","ead729a1":"code","2ce1f75c":"code","9e1b4430":"code","ca1c790a":"code","0fce1fb1":"code","d8217419":"code","a7685c21":"code","b801c8ed":"code","650ac648":"code","4d4136f1":"code","48aa0229":"code","4bd1445a":"markdown","1840d29d":"markdown","5e4f454e":"markdown","77faf7a7":"markdown","3fe82018":"markdown","0edec86c":"markdown","546b4f43":"markdown","fb5ac05b":"markdown","802de35e":"markdown","62ddde8a":"markdown","86d43173":"markdown","175bb81a":"markdown","869f0de6":"markdown","0f9b8230":"markdown","f6dbf090":"markdown","da996816":"markdown","a27edc50":"markdown","411ef869":"markdown","29132c1e":"markdown","b35bc012":"markdown","28e26c37":"markdown","710a614a":"markdown","8570213c":"markdown","94ab36b0":"markdown","ac5ceb4d":"markdown","28c7856c":"markdown","2edb2023":"markdown","b9052207":"markdown","f3d7345f":"markdown","c1d5e193":"markdown","d1738c47":"markdown","12038e2f":"markdown","b1979cd9":"markdown","6444f782":"markdown","4b0ecafe":"markdown","e50ec2c6":"markdown","e78769e5":"markdown","c3c92293":"markdown","06a7d181":"markdown","ccee0e07":"markdown","6b0fea69":"markdown","4c373edb":"markdown","7118505c":"markdown","197e2baa":"markdown","e5649166":"markdown","e4945ae4":"markdown","a328613e":"markdown","7a54989b":"markdown","e8ee4bdd":"markdown","67c7fd7b":"markdown","e312cd44":"markdown","42c21eb6":"markdown","63e699d0":"markdown","8fcb4421":"markdown","e7e5263c":"markdown","20e6e424":"markdown","155c8f76":"markdown","a9855814":"markdown"},"source":{"1b33bd46":"\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nlabels = ['spaCy\/GiNZA simple_cnn (org)', 'spaCy\/GiNZA simple_cnn (fast_text)', 'spaCy\/GiNZA bow', 'Rasa NLU', 'bert-japanese']\ndata = [0.92, 0.93, 0.94, 0.93, 0.96]\n\nx = np.arange(len(labels))\nwidth = 0.6\n\nfig, ax = plt.subplots(figsize=(10,5))\n\nrect = ax.bar(x, data, width)\nax.set_xticks(x)\nax.set_xticklabels(labels)\n\nlabels = ax.get_xticklabels()\nplt.setp(labels, rotation=80, fontsize=10);\nplt.ylabel('F1 score')\nplt.title('Livedoor News Corpus Text Classification Result Comparison')\n\ndef numlabel(rects):\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                   xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                   xytext=(0, 3),\n                   textcoords=\"offset points\",\n                   ha='center', va='bottom')\nnumlabel(rect)      \nplt.show()","3602727d":"%%time\n# about 1m20s\n!pip install ginza > \/dev\/null 2>&1","a6a202a9":"%%time\n# about 20s\n!pip install -U ginza ja-ginza  > \/dev\/null 2>&1","5b88f00b":"%%time\n# about 20s\n!python -m spacy download ja_core_news_sm > \/dev\/null 2>&1","5ccd0b94":"# after pip install, then reload for GiNZA \/ ja_core_news_sm \nimport pkg_resources, imp\nimp.reload(pkg_resources)","edd57db5":"import sys\nimport spacy\nfrom spacy.lang.ja import Japanese\nfrom spacy import displacy\n\nimport numpy as np\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport ginza # \u52d5\u4f5c\u30e2\u30fc\u30c9\u5207\u66ff\u306e\u305f\u3081\n\nprint('python version:', sys.version)\nprint('spaCy version:', spacy.__version__)","da37cec4":"!pip freeze | grep ginza","b98004a2":"nlp = Japanese()\ndoc = nlp('\u6771\u4eac\u90fd\u6e2f\u533a\u516d\u672c\u67281\u4e01\u76ee\u304b\u3089\u306e\u304a\u306f\u3088\u3046\u3054\u3056\u3044\u307e\u3059\uff01')\nfor token in doc:\n    print(token.text)\ndisplacy.render(doc, style='dep', options={'compact':True, 'distance': 90}) # \u4f9d\u5b58\u56f3\u304c\u8868\u793a\u3055\u308c\u306a\u3044\ndisplacy.render(doc, style='ent') # \u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u304c\u8868\u793a\u3055\u308c\u306a\u3044","a349eac1":"print([t.text for t in doc])\nprint([t.lemma_ for t in doc])\nprint([t.pos_ for t in doc])","1790375a":"pd.DataFrame(\n    [\n        [t.text for t in doc], # \u30c6\u30ad\u30b9\u30c8\n        [t.lemma_ for t in doc], # \u30ec\u30f3\u30de(\u57fa\u672c\u5f62)\n        [t.pos_ for t in doc], # \u54c1\u8a5e\n        [t.tag_ for t in doc], # \u54c1\u8a5e\u8a73\u7d30\n        [t.dep_ for t in doc], # \u69cb\u6587\u5f93\u5c5e\u95a2\u4fc2\n        [t.shape_ for t in doc], # \u6b63\u66f8\u6cd5\u306e\u7279\u5fb4(x:\u6587\u5b57,d:\u6570\u5024)\n        [t.is_alpha for t in doc], # \u6587\u5b57\u304b\u3069\u3046\u304b\n        [t.is_digit for t in doc], # \u6570\u5b57\u304b\u3069\u3046\u304b\n        [t.is_stop for t in doc] # \u30b9\u30c8\u30c3\u30d7\u30ea\u30b9\u30c8\u306e\u4e00\u90e8\u304b\u3069\u3046\u304b\n    ],\n    #columns=[token.i for token in doc], # \u540c\u3058\u6570\u5b57\u306a\u306e\u3067\u7121\u304f\u3066\u3082\u826f\u3044\n    index=['Text', '\u30ec\u30f3\u30de(\u57fa\u672c\u5f62)', '\u54c1\u8a5e', '\u54c1\u8a5e\u8a73\u7d30', '\u4f9d\u5b58\u95a2\u4fc2', '\u6b63\u66f8\u6cd5(x:\u6587\u5b57\/d:\u6570\u5b57)', \n           '\u6587\u5b57\u304b\uff1f', '\u6570\u5b57\u304b\uff1f', 'stop word\u304b\uff1f']\n)","9268da4a":"nlp = spacy.load('ja_core_news_sm')\ndoc = nlp('\u6771\u4eac\u90fd\u6e2f\u533a\u516d\u672c\u67281\u4e01\u76ee\u304b\u3089\u306e\u304a\u306f\u3088\u3046\u3054\u3056\u3044\u307e\u3059\uff01')\nfor token in doc:\n    print(token.text)\ndisplacy.render(doc, style='dep', options={'compact':True, 'distance': 90})\n#displacy.render(doc, style='dep')\ndisplacy.render(doc, style='ent')","97f32375":"spacy.explain('GPE')","9a7111f7":"pd.DataFrame(\n    [\n        [t.text for t in doc], # \u30c6\u30ad\u30b9\u30c8\n        [t.lemma_ for t in doc], # \u30ec\u30f3\u30de(\u57fa\u672c\u5f62)\n        [t.pos_ for t in doc], # \u54c1\u8a5e\n        [t.tag_ for t in doc], # \u54c1\u8a5e\u8a73\u7d30\n        [t.dep_ for t in doc], # \u69cb\u6587\u5f93\u5c5e\u95a2\u4fc2\n        [t.shape_ for t in doc], # \u6b63\u66f8\u6cd5\u306e\u7279\u5fb4(x:\u6587\u5b57,d:\u6570\u5024)\n        [t.is_alpha for t in doc], # \u6587\u5b57\u304b\u3069\u3046\u304b\n        [t.is_digit for t in doc], # \u6570\u5b57\u304b\u3069\u3046\u304b\n        [t.is_stop for t in doc] # \u30b9\u30c8\u30c3\u30d7\u30ea\u30b9\u30c8\u306e\u4e00\u90e8\u304b\u3069\u3046\u304b\n    ],\n    #columns=[token.i for token in doc], # \u540c\u3058\u6570\u5b57\u306a\u306e\u3067\u7121\u304f\u3066\u3082\u826f\u3044\n    index=['Text', '\u30ec\u30f3\u30de(\u57fa\u672c\u5f62)', '\u54c1\u8a5e', '\u54c1\u8a5e\u8a73\u7d30', '\u4f9d\u5b58\u95a2\u4fc2', '\u6b63\u66f8\u6cd5(x:\u6587\u5b57\/d:\u6570\u5b57)', \n   '\u6587\u5b57\u304b\uff1f', '\u6570\u5b57\u304b\uff1f', 'stop word\u304b\uff1f']\n\n)","218bc49b":"nlp = spacy.load('ja_ginza')\nfor mode in ['A', 'B', 'C']:\n    print('-'*5, '\u52d5\u4f5c\u30e2\u30fc\u30c9:', mode, '(\u30c7\u30d5\u30a9\u30eb\u30c8)' if mode=='C' else '')\n    ginza.set_split_mode(nlp, mode) # \u30e2\u30fc\u30c9\u5207\u66ff\n    doc = nlp('\u6771\u4eac\u90fd\u6e2f\u533a\u516d\u672c\u67281\u4e01\u76ee\u304b\u3089\u306e\u304a\u306f\u3088\u3046\u3054\u3056\u3044\u307e\u3059\uff01')\n    for token in doc:\n        print(token.text)","62f8b310":"nlp = spacy.load('ja_ginza')\ndoc = nlp('\u6771\u4eac\u90fd\u6e2f\u533a\u516d\u672c\u6728\uff11\u4e01\u76ee\u304b\u3089\u306e\u304a\u306f\u3088\u3046\u3054\u3056\u3044\u307e\u3059\uff01')\nfor token in doc:\n    print(token.text)\ndisplacy.render(doc, style='dep', options={'compact':True, 'distance': 90})\ndisplacy.render(doc, style='ent')","4edd31cf":"spacy.explain('GPE_OTHER')","03a8a576":"pd.DataFrame(\n    [\n        [t.text for t in doc], # \u30c6\u30ad\u30b9\u30c8\n        [t.lemma_ for t in doc], # \u30ec\u30f3\u30de(\u57fa\u672c\u5f62)\n        [t.pos_ for t in doc], # \u54c1\u8a5e\n        [t.tag_ for t in doc], # \u54c1\u8a5e\u8a73\u7d30\n        [t.dep_ for t in doc], # \u69cb\u6587\u5f93\u5c5e\u95a2\u4fc2\n        [t.shape_ for t in doc], # \u6b63\u66f8\u6cd5\u306e\u7279\u5fb4(x:\u6587\u5b57,d:\u6570\u5024)\n        [t.is_alpha for t in doc], # \u6587\u5b57\u304b\u3069\u3046\u304b\n        [t.is_digit for t in doc], # \u6570\u5b57\u304b\u3069\u3046\u304b\n        [t.is_stop for t in doc] # \u30b9\u30c8\u30c3\u30d7\u30ea\u30b9\u30c8\u306e\u4e00\u90e8\u304b\u3069\u3046\u304b\n    ],\n    #columns=[token.i for token in doc], # \u540c\u3058\u6570\u5b57\u306a\u306e\u3067\u7121\u304f\u3066\u3082\u826f\u3044\n    index=['Text', '\u30ec\u30f3\u30de(\u57fa\u672c\u5f62)', '\u54c1\u8a5e', '\u54c1\u8a5e\u8a73\u7d30', '\u4f9d\u5b58\u95a2\u4fc2', '\u6b63\u66f8\u6cd5(x:\u6587\u5b57\/d:\u6570\u5b57)', \n           '\u6587\u5b57\u304b\uff1f', '\u6570\u5b57\u304b\uff1f', 'stop word\u304b\uff1f']\n)","02fa1647":"nlp = spacy.load('ja_ginza')\ndoc = nlp(\n    '\u73fe\u5728\u306f\u65b0\u5bbf\u533a\u3067\u3059\u304c\u3001\u90fd\u5e81\u304c\u6709\u697d\u753a\u306b\u3042\u3063\u305f\u6642\u4ee3\u306e\u6771\u4eac\u90fd\u306e\u770c\u5e81\u6240\u5728\u5730\u306f\u3069\u3053\u3067\u3057\u305f\u304b\uff1f'\n    '\u6b63\u89e3\u306f\u6771\u4eac\u3067\u3057\u305f\u3002\u65e7\u6771\u4eac\u5e02\u8857\u5730\u533a\u3067\u3042\u308b\u73fe\u6771\u4eac\u7279\u522523\u533a\u57df\u306e\u4e8b\u3092\u300c\u6771\u4eac\u300d\u3068\u547c\u79f0\u3057\u3066\u3044\u307e\u3059\u3002'\n)\ndisplay(\n    pd.DataFrame(\n        [\n            [e.text for e in doc.ents],\n            [e.start_char for e in doc.ents],\n            [e.end_char for e in doc.ents],\n            [e.label_ for e in doc.ents]\n        ],\n        index=['Text', '\u958b\u59cb\u6587\u5b57\u756a\u53f7', '\u7d42\u4e86\u6587\u5b57\u756a\u53f7', '\u30e9\u30d9\u30eb' ]\n    )\n)\ndisplacy.render(doc, style='ent')","446fa80d":"#nlp = Japanese() # \u3067\u304d\u306a\u3044\n#nlp = spacy.load('ja_core_news_sm')\nnlp = spacy.load('ja_ginza')\ndoc = nlp('\u6700\u521d\u306e\u6587\u3002\u7b2c\u4e8c\u306e\u6587\u3002\u7b2c\u4e09\u306e\u6587')\n\n# \u6587\u5883\u754c\u89e3\u6790\nfor sent in doc.sents:\n    print(sent)","16d294d5":"# sent\u306b\u306f\u6700\u5f8c\u306e\u30bb\u30f3\u30c6\u30f3\u30b9\u304c\u5165\u3063\u3066\u3044\u308b\nfor t in sent:\n    print(t.text)","527d5948":"# sent\u306b\u306f\u6700\u5f8c\u306e\u30bb\u30f3\u30c6\u30f3\u30b9\u304c\u5165\u3063\u3066\u3044\u308b\nfor e in sent.ents:\n    print(e.text, e.label_)","051e0caf":"nlp = spacy.load('ja_ginza')\ndoc = nlp(\"\u767d\u3044\u72ac\u306f\u3068\u3066\u3082\u3084\u304b\u307e\u3057\u3044\u3002\u9ed2\u3044\u732b\u304c\u3051\u305f\u305f\u307e\u3057\u304f\u9cf4\u3044\u305f\u3002\")\n\nprint('-'*5, 'token')\nfor token in doc:\n    print(token.lemma_, token.dep_, token.head.lemma_)\nprint('-'*5, 'noun_chunks')\nfor chunk in doc.noun_chunks:\n    print(chunk.lemma_, chunk.root.dep_, chunk.root.head.lemma_)\n    if chunk.root.dep_ == 'nsubj':\n        print(chunk.lemma_, '=', chunk.root.head.lemma_)","85961e5f":"nlp = spacy.load('ja_ginza')\n#nlp = spacy.load('ja_core_news_sm')\ntokens = nlp('\u685c \u6885 \u72ac \u308f\u30fc\u3044')\n\nfor token in tokens:\n    print(token.text, token.has_vector, token.vector_norm, token.is_oov)","3aff8261":"tokens = nlp('\u685c')\ntokens.vector # tokens[0].vector \u3067\u3082\u540c\u3058","0924b7ec":"len(tokens.vector)","2ee7127c":"nlp = spacy.load('ja_ginza')\ntokens = nlp('\u685c \u6885 \u72ac ')\n\nfor token1 in tokens:\n    for token2 in tokens:\n        print(token1.text, token2.text, token1.similarity(token2))","b52dd31f":"nlp = spacy.load('ja_ginza')\ndoc = nlp('\u685c\u304c\u54b2\u304f\u3002\u6885\u304c\u54b2\u304f\u3002\u72ac\u304c\u5420\u3048\u308b\u3002')\n\nfor sent1 in doc.sents:\n    for sent2 in doc.sents:\n        print(sent1.text, sent2.text, sent1.similarity(sent2))","bbaf6ea9":"nlp = spacy.load(\"ja_ginza\")\ndoc = nlp('\u846c\u9001\u306e\u30d5\u30ea\u30fc\u30ec\u30f3\u306f\u52c7\u8005\u304c\u9b54\u738b\u3092\u5012\u3057\u305f\u3042\u3068\u6570\u5341\u5e74\u5f8c\u306e\u7269\u8a9e\u3067\u3059\u3002')\ndisplacy.render(doc, style=\"ent\")","e106424d":"from spacy.tokens import Span\n\nnlp = spacy.load(\"ja_ginza\")\ndoc = nlp(\"\u846c\u9001\u306e\u3075\u308a\u30fc\u308c\u3093\u306f\u52c7\u8005\u304c\u9b54\u738b\u3092\u5012\u3057\u305f\u3042\u3068\u6570\u5341\u5e74\u5f8c\u306e\u7269\u8a9e\u3067\u3059\u3002\")\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)\nprint('-'*5)\ndoc = nlp(\"\u846c\u9001\u306e\u3075\u308a\u30fc\u308c\u3093\u306f\u52c7\u8005\u304c\u9b54\u738b\u3092\u5012\u3057\u305f\u3042\u3068\u6570\u5341\u5e74\u5f8c\u306e\u7269\u8a9e\u3067\u3059\u3002\u3002\")\ndoc.ents = [Span(doc, 0, 4, label=\"MANGA\")] + list(doc.ents) # token\u756a\u53f7 0 \uff5e 3\nfor ent in doc.ents:\n    print(ent.text, ent.start_char, ent.end_char, ent.label_)\ndisplacy.render(doc, style=\"ent\")","bb442bb6":"# GINZA5\u3067\u306f\u52d5\u4f5c\u304c\u3053\u3068\u306a\u308b\u3088\u3046\u3060\n'''\nfrom spacy.pipeline import EntityRuler \n\nnlp = spacy.load(\"ja_ginza\")\npatterns = [{\"label\": \"BOOK\", \"pattern\": \"\u846c\u9001\u306e\u30d5\u30ea\u30fc\u30ec\u30f3\"}]\n\nruler = EntityRuler(nlp)\nruler.add_patterns(patterns)\nnlp.add_pipe(ruler) \n\ndoc = nlp(\"\u846c\u9001\u306e\u30d5\u30ea\u30fc\u30ec\u30f3\u306f\u52c7\u8005\u304c\u9b54\u738b\u3092\u5012\u3057\u305f\u3042\u3068\u6570\u5341\u5e74\u5f8c\u306e\u7269\u8a9e\u3067\u3059\u3002\")\n\ndisplacy.render(doc, style=\"ent\")\n\nfor ent in doc.ents:\n    print(ent.text, ent.has_vector, ent.vector_norm)\n'''","c23cd1b9":"nlp = spacy.load(\"ja_ginza\")\ndoc = nlp('\u846c\u9001\u306e\u30d5\u30ea\u30fc\u30ec\u30f3')\nfor token in doc:\n    print(token.text, token.has_vector, token.vector_norm)","d8e4bd4f":"# for \u4e3b\u6210\u5206\u89e3\u6790\nfrom sklearn.decomposition import PCA\n\n#text2vector\nLABEL = 0 # q_list\u5404\u8981\u7d20\u306e\u30ea\u30b9\u30c8\u4e2d\u30010\u756a\u76ee\u304c\u30e9\u30d9\u30eb\nQSTR = 1 # q_list\u5404\u8981\u7d20\u306e\u30ea\u30b9\u30c8\u4e2d\u30011\u756a\u76ee\u304c\u30d9\u30af\u30bf\u30fc\nq_list = [\n    ['2p:Skytree', '\u6771\u4eac\u30bf\u30ef\u30fc\u306e\u9ad8\u3055\u306f333m\u3067\u3059\u304c\u3001\u30b9\u30ab\u30a4\u30c4\u30ea\u30fc\u306e\u9ad8\u3055\u306f\uff1f'],\n    ['2p:Light-bulb', '\u84c4\u97f3\u6a5f\u306e\u767a\u660e\u8005\u306f\u30a8\u30b8\u30bd\u30f3\u3067\u3059\u304c\u3001\u96fb\u7403\u306e\u767a\u660e\u8005\u306f\u3060\u308c\u3067\u3057\u3087\u3046\uff1f'],\n    ['Kamakura', '\u938c\u5009\u5e55\u5e9c\u304c\u6210\u7acb\u3057\u305f\u306e\u306f\u4f55\u5e74\u3067\u3057\u3087\u3046\uff1f'],\n    ['Togarashi', '\u5510\u8f9b\u5b50\u306e\u539f\u7523\u56fd\u306f\u3069\u3053\u3067\u3057\u3087\u3046\uff1f'],\n    ['2p:Muromachi', '\u938c\u5009\u5e55\u5e9c\u3092\u4f5c\u3063\u305f\u306e\u306f\u6e90\u306e\u983c\u671d\u3067\u3059\u304c\u3001\u5ba4\u753a\u5e55\u5e9c\u3092\u4f5c\u3063\u305f\u306e\u306f\u3060\u308c\u3067\u3057\u3087\u3046\uff1f'],\n    ['Curry', '\u30ab\u30ec\u30fc\u306e\u4e3b\u306a\u30b9\u30d1\u30a4\u30b9\u306f\u306a\u3093\u3067\u3057\u3087\u3046\uff1f'],\n    ['Independence', '\u30a2\u30e1\u30ea\u30ab\u72ec\u7acb\u6226\u4e89\u306f\u4f55\u5e74\u3067\u3059\u304b\uff1f'], \n    ['Choco', '\u30c1\u30e7\u30b3\u30ec\u30fc\u30c8\u3092\u767a\u660e\u3057\u305f\u306e\u306f\u8ab0\u3067\u3059\u304b\uff1f'],\n    ['Fuji', '\u65e5\u672c\u30671\u756a\u9ad8\u3044\u5c71\u306f\u4f55\u3067\u3057\u3087\u3046\uff1f'],\n    ['2p:Fuji', '\u65e5\u672c\u30671\u756a\u9ad8\u3044\u5c71\u306f\u5bcc\u58eb\u5c71\u3067\u3059\u304c\u3001\u4e16\u754c\u30671\u756a\u9ad8\u3044\u5c71\u306f\u4f55\u3067\u3057\u3087\u3046\uff1f'],\n    ['2p:Biwako', '\u65e5\u672c\u30671\u756a\u5927\u304d\u3044\u6e56\u306f\u7435\u7436\u6e56\u3067\u3059\u304c\u3001\u4e16\u754c\u30671\u756a\u5927\u304d\u3044\u6e56\u306f\u4f55\u3067\u3057\u3087\u3046\uff1f'],\n    ['2p:Monet', '\u7761\u84ee\u306e\u4f5c\u8005\u306f\u30e2\u30cd\u3067\u3059\u304c\u3001\u30e2\u30ca\u30ea\u30b6\u306e\u4f5c\u8005\u306f\u8ab0\u3067\u3057\u3087\u3046\uff1f'],\n    ['Desuga', '\u300c\u3067\u3059\u304c\u300d\u304c\u3042\u308b\u3068\u5fc5\u305a\u30d1\u30e9\u30ec\u30eb\u554f\u984c\u306b\u306a\u308a\u307e\u3059\u304b\uff1f'],\n    ['2p:Desuga', '\u300c\u3067\u3059\u304c\u300d\u3092\u542b\u3080\u554f\u984c\u306f\u591a\u3044\u3067\u3059\u304c\u3001\u5fc5\u305a\u30d1\u30e9\u30ec\u30eb\u554f\u984c\u306b\u306a\u308a\u307e\u3059\u304b\uff1f']\n]\n\n#PCA(\u4e3b\u6210\u5206\u5206\u6790)\nvectors = np.vstack([nlp(q_list[v][QSTR]).vector for v in range(len(q_list))])\npca = PCA(n_components=2).fit(vectors)\nt = pca.fit_transform(vectors)\npc_ratio = pca.explained_variance_ratio_\n\n#plot\nplt.figure(figsize=(10,10))\nplt.scatter(t[:,0], t[:,1]) # PC1 ... t[:,0], PC2 ... t[:,1] \n\ni = 0\nfor txt in [q_list[v][LABEL] for v in range(len(q_list))]:\n    c = 'red' if '2p:' in txt else 'black'\n    plt.text(t[i,0]+0.01, t[i,1]+0.01, txt, size=12, color = c) # \u3061\u3087\u3063\u3068\u305a\u3089\u3057\u305f\u4f4d\u7f6e\u306b\u8868\u793a\n    i += 1\n\nplt.hlines(0, min(t[:,0]), max(t[:,0]), linestyle='dashed', linewidth=1, color='gray')\nplt.vlines(0, min(t[:,1]), max(t[:,1]), linestyle='dashed', linewidth=1, color='gray')\nplt.xlabel('PC1 ('+str(round(pc_ratio[0]*100,2))+'%)')\nplt.ylabel('PC2 ('+str(round(pc_ratio[1]*100,2))+'%)')\nplt.tight_layout()\nplt.show()","af68a21f":"doc = nlp(q_list[11][QSTR])\npd.DataFrame(\n    data=[\n        [t.text for t in doc], # \u30c6\u30ad\u30b9\u30c8\n        [t.lemma_ for t in doc], # \u30ec\u30f3\u30de(\u57fa\u672c\u5f62)\n        [t.pos_ for t in doc], # \u54c1\u8a5e\n        [t.tag_ for t in doc], # \u54c1\u8a5e\u8a73\u7d30\n        [t.dep_ for t in doc], # \u69cb\u6587\u5f93\u5c5e\u95a2\u4fc2\n    ],\n    #columns=[token.i for token in doc], # \u540c\u3058\u6570\u5b57\u306a\u306e\u3067\u7121\u304f\u3066\u3082\u826f\u3044\n    index=['Text', '\u30ec\u30f3\u30de(\u57fa\u672c\u5f62)', '\u54c1\u8a5e', '\u54c1\u8a5e\u8a73\u7d30', '\u4f9d\u5b58\u95a2\u4fc2']\n)","42241aa0":"doc = nlp(q_list[12][QSTR])\npd.DataFrame(\n    [\n        [t.text for t in doc], # \u30c6\u30ad\u30b9\u30c8\n        [t.lemma_ for t in doc], # \u30ec\u30f3\u30de(\u57fa\u672c\u5f62)\n        [t.pos_ for t in doc], # \u54c1\u8a5e\n        [t.tag_ for t in doc], # \u54c1\u8a5e\u8a73\u7d30\n        [t.dep_ for t in doc], # \u69cb\u6587\u5f93\u5c5e\u95a2\u4fc2\n    ],\n    #columns=[token.i for token in doc], # \u540c\u3058\u6570\u5b57\u306a\u306e\u3067\u7121\u304f\u3066\u3082\u826f\u3044\n    index=['Text', '\u30ec\u30f3\u30de(\u57fa\u672c\u5f62)', '\u54c1\u8a5e', '\u54c1\u8a5e\u8a73\u7d30', '\u4f9d\u5b58\u95a2\u4fc2']\n)","953430e5":"doc = nlp(q_list[13][QSTR])\npd.DataFrame(\n    [\n        [t.text for t in doc], # \u30c6\u30ad\u30b9\u30c8\n        [t.lemma_ for t in doc], # \u30ec\u30f3\u30de(\u57fa\u672c\u5f62)\n        [t.pos_ for t in doc], # \u54c1\u8a5e\n        [t.tag_ for t in doc], # \u54c1\u8a5e\u8a73\u7d30\n        [t.dep_ for t in doc], # \u69cb\u6587\u5f93\u5c5e\u95a2\u4fc2\n    ],\n    #columns=[token.i for token in doc], # \u540c\u3058\u6570\u5b57\u306a\u306e\u3067\u7121\u304f\u3066\u3082\u826f\u3044\n    index=['Text', '\u30ec\u30f3\u30de(\u57fa\u672c\u5f62)', '\u54c1\u8a5e', '\u54c1\u8a5e\u8a73\u7d30', '\u4f9d\u5b58\u95a2\u4fc2']\n)","b37cdb8a":"for l, qstr in q_list:\n    para = False\n    doc = nlp(qstr)\n    for token in doc:\n        if token.text == '\u3067\u3059':\n            i = token.i\n            next_token = doc[i+1]\n            if next_token.text == '\u304c' and next_token.dep_ == 'mark':\n                para = True\n    if para:\n        print('\u30d1\u30e9\u30ec\u30eb\u554f\u984c: ', qstr)\n    else:\n        print('\u3075\u3064\u3046\u306e\u554f\u984c: ', qstr)","cf249999":"%%time\n# 30s\u304f\u3089\u3044\n# for \u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\n!pip install bs4 > \/dev\/null 2>&1\n# for \u6587\u7ae0\u62bd\u51fa\u8981\u7d04\n!pip install sumy > \/dev\/null 2>&1\n!pip install tinysegmenter  > \/dev\/null 2>&1","40e4f666":"# for \u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\nfrom urllib import request\nfrom bs4 import BeautifulSoup\nimport bs4\n\n# url\u306b\u8981\u7d04\u5bfe\u8c61\u3068\u3059\u308b\u66f8\u7c4d\u306eURL\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n#title = '\u3060\u3057\u306e\u53d6\u308a\u65b9 \u5317\u5927\u8def\u9b6f\u5c71\u4eba'\n#url = 'https:\/\/www.aozora.gr.jp\/cards\/001403\/files\/49986_37674.html'\ntitle = '\u6cb3\u8c5a\u306f\u6bd2\u9b5a\u304b \u5317\u5927\u8def\u9b6f\u5c71\u4eba'\nurl = 'https:\/\/www.aozora.gr.jp\/cards\/001403\/files\/50001_37885.html'\n#title = '\u300e\u543e\u8f29\u306f\u732b\u3067\u3042\u308b\u300f\u4e0a\u7bc7\u81ea\u5e8f \u590f\u76ee\u6f31\u77f3'\n#url = 'https:\/\/www.aozora.gr.jp\/cards\/000148\/files\/47148_32217.html'\n\nhtml = request.urlopen(url)\nsoup = BeautifulSoup(html, 'html.parser')\nbody = soup.select('.main_text')","ead729a1":"body","2ce1f75c":"text = ''\nfor b in body[0]:\n    if type(b) == bs4.element.NavigableString:\n        text += b\n        continue\n    # \u30eb\u30d3\u306e\u5834\u5408\u3001\u30d5\u30ea\u30ac\u30ca\u306f\u5bfe\u8c61\u306b\u305b\u305a\u306b\u3001\u6f22\u5b57\u306e\u307f\u4f7f\u7528\u3057\u307e\u3059\u3002\n    text += ''.join([e.text for e in b.find_all('rb')])","9e1b4430":"text","ca1c790a":"# \\u3000 \u306f\u5168\u89d2\u30b9\u30da\u30fc\u30b9\ntext = text.replace('\\n','').replace('\\r','').replace('\\u3000','')","0fce1fb1":"text","d8217419":"# GINZA\u304c\u65b0\u3057\u304f\u306a\u3063\u3066'\u2026'\u306e\u52d5\u4f5c\u304c\u5909\u308f\u3063\u305f\u3089\u3057\u3044\u3002\u539f\u6587\u304b\u3089\u9664\u53bb\ntext = text.replace('\u2026', '')\n\ncorpus = []\ncorpus_lemma = []\noriginals = []\ndoc = nlp(text)\nfor s in doc.sents:\n    originals.append(s.text)\n    tokens = []\n    tokens_lemma = []\n    for t in s:\n        # \u7279\u5b9a\u306e\u6587\u5b57\u304c\u90aa\u9b54\u3059\u308b\u5834\u5408\u306b\u306f\u3053\u3053\u3067\u9664\u53bb\u3059\u308b\n        if (t.text in ['\u300c','\u300d','\u3010','\u3011']):\n            continue\n        tokens.append(t.text)\n        tokens_lemma.append(t.lemma_)\n\n    corpus.append(' '.join(tokens))\n    corpus_lemma.append(' '.join(tokens_lemma))\n\n\nprint(len(originals))\nprint(len(corpus))\nprint(len(corpus_lemma))","a7685c21":"originals[-5:]","b801c8ed":"corpus[-5:]","650ac648":"corpus_lemma[-5:]","4d4136f1":"from sumy.parsers.plaintext import PlaintextParser\nfrom sumy.nlp.tokenizers import Tokenizer\nfrom sumy.summarizers.lex_rank import LexRankSummarizer\n\ndef summarize(originals, corpus, lines):\n    # \u9577\u3055\u304c\u9055\u3063\u3066\u3044\u305f\u3089\u30a8\u30e9\u30fc\n    assert len(originals) == len(corpus), 'original\u3068corpus\u306e\u9577\u3055\u304c\u9055\u3044\u307e\u3059'\n\n    # \u9023\u7d50\u3057\u305fcorpus\u3092\u518d\u5ea6tinysegmenter\u3067\u30c8\u30fc\u30af\u30ca\u30a4\u30ba\u3055\u305b\u308b\n    parser = PlaintextParser.from_string(''.join(corpus), Tokenizer('japanese'))\n    summarizer = LexRankSummarizer()\n    #summarizer.stop_words = ['\u300d', '\u300c']  # \u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9# sentencres_count\u306b\u8981\u7d04\u5f8c\u306e\u6587\u306e\u6570\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\n    summary = summarizer(document=parser.document, sentences_count=lines)\n    \n\n    # corpus\u3092\u30ad\u30fc\u3068\u3057\u3066\u5143\u306e\u6587\u3092\u53c2\u7167\u3001\u305d\u308c\u3089\u3092\u63a5\u7d9a\u3057\u3066return\n    summary_str = []\n    if originals == corpus :\n        for sentence in summary:\n            summary_str.append(sentence.__str__())\n    else:\n        for sentence in summary:\n            #print(sentence.__str__())\n            summary_str.append(originals[corpus.index(sentence.__str__())])\n\n    return summary_str","48aa0229":"# sentencres_count\u306b\u8981\u7d04\u5f8c\u306e\u6587\u306e\u6570\u3092\u6307\u5b9a\u3057\u307e\u3059\u3002\nLINES = 3\n\nprint(title)\nprint('----- \u539f\u6587\u306e\u307e\u307e\u62bd\u51fa\u8981\u7d04 -----')\nsummary_lines = summarize(originals, originals, LINES)\nfor s in summary_lines:\n    print(s)\n\nprint('----- spaCy\/GINZA\u3067\u5206\u304b\u3061\u66f8\u304d(\u30c8\u30fc\u30af\u30f3)\u5316\u3001\u30ec\u30f3\u30de\u5909\u63db\u306a\u3057\u3067\u62bd\u51fa\u8981\u7d04 -----')\nsummary_lines = summarize(originals, corpus, LINES)\nfor s in summary_lines:\n    print(s)\n\nprint('----- spaCy\/GINZA\u3067\u5206\u304b\u3061\u66f8\u304d(\u30c8\u30fc\u30af\u30f3)\u5316\u3001\u30ec\u30f3\u30de\u5909\u63db\u3042\u308a\u3067\u62bd\u51fa\u8981\u7d04 -----')\nsummary_lines = summarize(originals, corpus_lemma, LINES)\nfor s in summary_lines:\n    print(s)","4bd1445a":"\u300c\u846c\u9001\u300d:0\u3001\u300c\u306e\u300d:1.8\u3001\u300c\u30d5\u30ea\u30fc\u30ec\u30f3\u300d:0\u304c\u5e73\u5747\u5316\u3055\u308c\u3066\u30010.6\u306b\u306a\u3063\u3066\u3044\u308b\u3088\u3046\u306b\u898b\u3048\u308b\u3002\n\n\u9069\u5207\u306a\u30d9\u30af\u30c8\u30eb\u3092\u8ffd\u52a0\u3059\u308b\u306b\u306f\u3053\u308c\u3089\u306eurl\u3092\u53c2\u8003\u306b\u3057\u3066\u5b66\u7fd2\u3055\u305b\u308b\u5fc5\u8981\u304c\u3042\u308b\u3002\n* [spaCy + GiNZA\u3092\u4f7f\u3063\u3066\u56fa\u6709\u8868\u73fe\u62bd\u51fa\u3068\u30ab\u30b9\u30bf\u30e0\u30e2\u30c7\u30eb\u306e\u5b66\u7fd2\u3092\u3057\u3066\u307f\u308b](https:\/\/tech.mof-mof.co.jp\/blog\/spacy-ner\/)\n* [spaCy \/ GiNZA \u3092 Python \u3067\u4f7f\u3063\u3066\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u5206\u6790\u3092\u884c\u3046](https:\/\/qiita.com\/567000\/items\/798caa06af0985618a0d)\n\n\u307e\u305f[spaCy 101: Everything you need to know \u548c\u8a33](https:\/\/qiita.com\/miorgash\/items\/0eda4adcc8d9ecd143e6)\u3082\u7d20\u6674\u3089\u3057\u3044\u3067\u3059\u3002","1840d29d":"## sumy\u306b\u3088\u308b\u8981\u7d04 \/ Summary by using 'sumy'","5e4f454e":"## \u72ec\u81ea\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u8ffd\u52a0\u65b9\u6cd5\uff12 \/ How to add your own entity 2","77faf7a7":"# \u4fc2\u308a\u53d7\u3051\u89e3\u6790 \/ Dependency analysis","3fe82018":"\u4fc2\u308a\u53d7\u3051\u89e3\u6790\u3067\u306f\u3001token\u3060\u3068\u7d30\u304b\u304f\u5206\u5272\u3057\u3059\u304e\u308b\u306e\u3067\u3001noun_chunks\u3092\u4f7f\u3046\u3068\u826f\u3044\u3002\n\nIn the dependency analysis, if it is token, it is divided too finely, so it is good to use noun_chunks.","0edec86c":"# What is spaCy\n\n\u300cspaCy\u300d\u306f\u3001Python\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002\u5927\u91cf\u306e\u30c6\u30ad\u30b9\u30c8\u306e\u51e6\u7406\u304a\u3088\u3073\u8a00\u8a9e\u7406\u89e3\u3092\u884c\u3046\u30a2\u30d7\u30ea\u306e\u69cb\u7bc9\u306b\u5f79\u7acb\u3061\u307e\u3059\u3002\u300c\u60c5\u5831\u62bd\u51fa\u300d\u300c\u81ea\u7136\u8a00\u8a9e\u7406\u89e3\u300d\u300c\u6df1\u5c64\u5b66\u7fd2\u306e\u30c6\u30ad\u30b9\u30c8\u306e\u524d\u51e6\u7406\u300d\u306b\u4f7f\u7528\u3067\u304d\u307e\u3059\u3002\n\nspaCy\u306e\u6a5f\u80fd\uff1a\n* \u30c8\u30fc\u30af\u30f3\u5316 : \u30c6\u30ad\u30b9\u30c8\u3092\u5358\u8a9e\u3084\u53e5\u8aad\u70b9\u306b\u5206\u5272\u3002\n* \u54c1\u8a5e\uff08POS: Part-of-speech\uff09\u306e\u30bf\u30b0\u4ed8\u3051 : \u5358\u8a9e\u7a2e\u5225\uff08\u52d5\u8a5e\u3084\u540d\u8a5e\u306a\u3069\uff09\u3092\u30c8\u30fc\u30af\u30f3\u306b\u5272\u308a\u5f53\u3066\u3002\n* \u4f9d\u5b58\u95a2\u4fc2\u306e\u89e3\u6790 : \u69cb\u6587\u4f9d\u5b58\u95a2\u4fc2\uff08\u500b\u3005\u306e\u30c8\u30fc\u30af\u30f3\u9593\u306e\u95a2\u4fc2\uff09\u30e9\u30d9\u30eb\u3092\u30c8\u30fc\u30af\u30f3\u306b\u5272\u308a\u5f53\u3066\u3002\n* \u30ec\u30f3\u30de\u5316 : \u5358\u8a9e\u306e\u57fa\u672c\u5f62\u306e\u5272\u308a\u5f53\u3066\u3002\n* \u6587\u5883\u754c\u691c\u51fa\uff08SBD : Sentence Boundary Detection\uff09: \u6587\u306e\u5883\u754c\u3092\u691c\u51fa\u3002\n* \u540d\u524d\u4ed8\u304d\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u306e\u8a8d\u8b58\uff08NER : Named Entity Recognition\uff09 : \u4eba\u3001\u4f1a\u793e\u3001\u5834\u6240\u306a\u3069\u3001\u540d\u524d\u4ed8\u304d\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\uff08\u5b9f\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\uff09\u306b\u30e9\u30d9\u30eb\u3092\u4ed8\u52a0\u3002\n* \u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u30ea\u30f3\u30af\uff08EL: Entity Linking\uff09 : \u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u3092\u77e5\u8b58\u30d9\u30fc\u30b9\u306e\u4e00\u610f\u306e\u8b58\u5225\u5b50\u306b\u5909\u63db\u3002\n*  \u985e\u4f3c\u5ea6 : \u5358\u8a9e\u3001\u30c6\u30ad\u30b9\u30c8\u30b9\u30d1\u30f3\u3001\u30c6\u30ad\u30b9\u30c8\u306e\u6bd4\u8f03\u3001\u304a\u3088\u3073\u985e\u4f3c\u5ea6\u306e\u8a08\u7b97\u3002\n*  \u30c6\u30ad\u30b9\u30c8\u5206\u985e : \u30c6\u30ad\u30b9\u30c8\u5168\u4f53\u307e\u305f\u306f\u4e00\u90e8\u306b\u30ab\u30c6\u30b4\u30ea\uff08\u307e\u305f\u306f\u30e9\u30d9\u30eb\uff09\u3092\u5272\u308a\u5f53\u3066\u3002\n* \u30eb\u30fc\u30eb\u30d9\u30fc\u30b9\u306e\u30de\u30c3\u30c1\u30f3\u30b0 : \u6b63\u898f\u8868\u73fe\u3068\u540c\u69d8\u306b\u3001\u30c6\u30ad\u30b9\u30c8\u3068\u8a00\u8a9e\u30a2\u30ce\u30c6\u30fc\u30b7\u30e7\u30f3\u306b\u57fa\u3065\u3044\u3066\u30c8\u30fc\u30af\u30f3\u306e\u30b7\u30fc\u30b1\u30f3\u30b9\u3092\u691c\u7d22\u3002\n* \u8a13\u7df4 : \u7d71\u8a08\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u306e\u66f4\u65b0\u3068\u6539\u5584\u3002\n* \u30b7\u30ea\u30a2\u30e9\u30a4\u30bc\u30fc\u30b7\u30e7\u30f3 : \u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u30d5\u30a1\u30a4\u30eb\uff08\u307e\u305f\u306f\u30d0\u30a4\u30c8\u6587\u5b57\u5217\uff09\u306b\u4fdd\u5b58\u3002\n\n\"SpaCy\" is Python's natural language processing library. Useful for building apps that process large amounts of text and understand languages. It can be used for \"information extraction\", \"natural language understanding\", and \"preprocessing of deep learning texts\".\n\nFunctions of spaCy\n* Tokenization: Split text into words and punctuation.\n* Part-of-speech (POS) tagging: Assign word types (verbs, nouns, etc.) to tokens.\n* Dependency parsing: Assign syntax dependency (relationships between individual tokens) labels to tokens.\n* Lemma: Assignment of basic forms of words.\n* Sentence Boundary Detection (SBD): Detects sentence boundaries.\n* Named Entity Recognition (NER): Labels named entities (real objects) such as people, companies, and locations.\n* Entity Linking (EL): Converts an entity into a unique knowledge base identifier.\n* Similarity: Word, text span, text comparison, and similarity calculation.\n* Text classification: Assign a category (or label) to all or part of the text.\n* Rule-based matching: Search for sequences of tokens based on text and language annotations, similar to regular expressions.\n* Training: Update and improve predictions in statistical models.\n* Serialization: Save the object to a file (or byte string).","546b4f43":"## ja_core_news\nja_core_news\u306b\u306f\u4e09\u3064\u306e\u30e2\u30c7\u30eb\u304c\u3042\u308a\u307e\u3059\u3002\n\nThere are three models in ja_core_news.\n\n|\u30e2\u30c7\u30eb\u540d|\u30b5\u30a4\u30ba|\u30b3\u30e1\u30f3\u30c8|\n|:---|---:|:---|\n|ja_core_news_sm|12MB|0 keys, 0 unique vectors (0 dimensions)|\n|ja_core_news_md|41MB|480k keys, 20k unique vectors (300 dimensions)|\n|ja_core_news_lg|531MB|480k keys, 480k unique vectors (300 dimensions)|\n\nvector\u306f\u985e\u4f3c\u5ea6\u5206\u6790\u306b\u4f7f\u7528\u3057\u307e\u3059\u3002\n\nvector is used for similarity analysis.","fb5ac05b":"ja_ginza\u306e\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u306f\u300c\u6771\u4eac\u90fd\u6e2f\u533a\u516d\u672c\u6728\u300d\u307e\u3067\u3067\u3059\u3002\n\nThe entity of ja_ginza is up to \"Roppongi, Minato-ku, Tokyo\".","802de35e":"token\u306e\u30d7\u30ed\u30d1\u30c6\u30a3\uff08\u4e00\u90e8\uff09\uff1a\n\n\u6700\u5f8c\u304c'_'\u3067\u7d42\u308f\u3063\u3066\u3044\u308b\u3068\u6587\u5b57\u5217\u3002\u305d\u308c\u4ee5\u5916\u306f\u6570\u5024\u304bTrue\/False\u3002\n\n* token.i : \u30c8\u30fc\u30af\u30f3\u756a\u53f7\u3002\n* token.text : \u30c6\u30ad\u30b9\u30c8\u3002\n* token.lemma_ : \u30ec\u30f3\u30de\u3002\n* token.pos_ : \u54c1\u8a5e\u3002\n* token.tag_ : \u54c1\u8a5e\u8a73\u7d30\u3002\n* token.dep_ : \u69cb\u6587\u5f93\u5c5e\u95a2\u4fc2\n* token.shape_ : \u6b63\u66f8\u6cd5\u306e\u7279\u5fb4(x:\u6587\u5b57,d:\u6570\u5024)\n* is_alpha : \u6587\u5b57\u304b\u3069\u3046\u304b\n* is_digit : \u6570\u5b57\u304b\u3069\u3046\u304b\n* is_stop : \u30b9\u30c8\u30c3\u30d7\u30ef\u30fc\u30c9\u30ea\u30b9\u30c8\u306e\u4e00\u90e8\u304b\uff1f\n\n\u8a73\u7d30\u306f[\u3053\u3061\u3089](https:\/\/github.com\/megagonlabs\/ginza\/blob\/develop\/ginza\/__init__.py)\u3092\u53c2\u7167\u3002\n\ntoken properties (partial):\n\nA string that ends with a'_'. Other than that, it is a numerical value or True \/ False.\n\n* token.i: Token number.\n* token.text: Text.\n* token.lemma_: Lemma.\n* token.pos_: Part of speech.\n* token.tag_: Part of speech details.\n* token.dep_: Syntax Dependency\n* token.shape_: Orthographic features (x: letters, d: numbers)\n* is_alpha: Whether it is a character\n* is_digit: Whether it is a number\n* is_stop: Is it part of the stopword list?\n\nSee [here](https:\/\/github.com\/megagonlabs\/ginza\/blob\/develop\/ginza\/__init__.py) for details.","62ddde8a":"doc.sents\u3067\u30bb\u30f3\u30c6\u30f3\u30b9\uff08\u6587\uff09\u5358\u4f4d\u3067\u53d6\u308a\u51fa\u305b\u308b\u3002\n\nYou can retrieve it in sentence units with doc.sents.","86d43173":"\u6587\u66f8\u8981\u7d04\u624b\u6cd5\u306f\u62bd\u51fa\u578b\u3068\u751f\u6210\u578b\u306b\u5206\u3051\u3089\u308b\u304cLexRank\u306f\u62bd\u51fa\u578b\u3002\n\u6587\u7ae0\u3092node\u3001\u6587\u7ae0\u9593\u306e\u985e\u4f3c\u5ea6\u3092edge\u3067\u8868\u73fe\u3057\u305f\u7121\u5411\u30b0\u30e9\u30d5\u3092\u4f5c\u6210\u3002\n\u5404\u6587\u7ae0\u3092TF-IDF\u3092\u7528\u3044\u3066\u7279\u5fb4\u30d9\u30af\u30c8\u30eb\u3068\u3057\u3066\u8868\u73fe\u3057\u3001\u30b3\u30b5\u30a4\u30f3\u985e\u4f3c\u5ea6\u3092\u7528\u3044\u3066\n\u6587\u7ae0\u9593(node\u9593)\u306e\u985e\u4f3c\u5ea6\u3092\u8a08\u7b97\u3002\n\u985e\u4f3c\u5ea6\u306b\u57fa\u3065\u3044\u3066\u3001\u591a\u304f\u306e\u6587\u7ae0node\u3068\u985e\u4f3c\u5ea6\u304c\u9ad8\u3044\u5834\u5408\u3001\u305d\u306e\u6587\u7ae0\u306f\u91cd\u8981\u3067\u3042\u308b\u3068\u5224\u65ad\u3057\u3066\u3001\n\u305d\u308c\u3092\u4ee3\u8868\u7684\u306a\u6587\u7ae0\u3068\u3057\u3066\u8981\u7d04\u3068\u3057\u3066\u629c\u304d\u51fa\u3059\u3002PageRank\u624b\u6cd5\u306b\u985e\u4f3c\u3059\u308b\u8003\u3048\u65b9\u3002\n\nDocument summarization methods are divided into extraction type and generation type, but LexRank is extraction type.\nCreate an undirected graph that expresses sentences with nodes and similarities between sentences with edges.\nEach sentence is expressed as a feature vector using TF-IDF, and cosine similarity is used.\nCalculate the similarity between sentences (between nodes).\nBased on the similarity, if the similarity is high with many sentence nodes, it is judged that the sentence is important,\nExtract it as a summary as a representative sentence. A concept similar to the PageRank method.","175bb81a":"Definition of part of speech:\n\n1. NOUN: Noun\n  * Nouns-common nouns (except those used as VERB, ADJ) (Example: Bread)\n2. PROPN: proper noun\n  * Nouns-proper nouns (eg Hokkaido)\n3. VERB: Verb\n  * Verbs (except those that are non-independent) (Example: Eat)\n  * Business card + service that can be changed and has a verb ending (example:'meal')\n4. ADJ: adjective\n  * Adjectives (except those that are non-independent) (Example: Large)\n  * Shape words (eg rich)\n  * Adnominal adjectives (excluding DET) (Example: Large)\n  * Noun-When a shape is possible and the shape has a flexion (eg'free')\n5. ADV: Adverb\n  * Adverbs (eg slowly)\n6. INTJ: Interjection\n  * Interjection (Example: Ah)\n7. PRON: Pronoun\n  * Pronoun (eg me)\n8. NUM: Numeral\n  * Nouns-Numerals (Example: 5)\n9. AUX: Auxiliary verb\n  * Auxiliary verb (example: ta)\n  * Non-independent verbs \/ adjectives (eg,'is', hard to eat')\n10. CONJ: Conjunction\n  * Conjunctions or particles-Conjunctions used as coordinated particles (eg and)\n11. SCONJ: Subordinating conjunction\n  * Conjunctions \/ Particles-Conjunctions (excluding those that become CONJ) (Example:)\n  * Quasifield particles (eg, go'no')\n12. DET: Determiner\n  * Part of the adnominal adjective (eg this, that, that, what)\n13. ADP: Preposition\n  * Particles-Case particles (eg)\n  * Sub-particles (example: only)\n  * Particles (eg, just)\n14. PART: Affix\n  * Particles-Final particles (eg'what time')\n  * Suffix (eg depth'depth')\n15. PUNCT: Punctuation\n  * Auxiliary symbols-punctuation marks \/ commas \/ open parentheses \/ closed parentheses\n16. SYM: Symbol\n  * Symbols \/ auxiliary symbols other than PUNCT\n17. X: Other\n  * Blank","869f0de6":"Token dependency analysis properties:\n\n* token.dep_: Syntax dependency.\n* token.head: Token of the syntactic parent.\n* token.children: Syntax child token.\n* token.lefts: The token county on the left in the syntax.\n* token.rights: The token county on the right syntactically.","0f9b8230":"spaCy\u65e5\u672c\u8a9e\u306b\u306f\u3001Japanese()\u3001ja_core_news, ja_ginza\u304c\u3042\u308a\u307e\u3059\u3002\n\n\u57fa\u672c\u7684\u306a\u4f7f\u3044\u65b9\uff1a\n\n1. \u307e\u305a nlp \u3092\u8a2d\u5b9a\u3057\u307e\u3059\u3002Japanese()\u3084\u3001ja_core_news, ja_ginza\u306e\u30ed\u30fc\u30c9\u3002\n1. doc = nlp(\u89e3\u6790\u3059\u308b\u6587) \u3067\u89e3\u6790\u7d50\u679c\u3092 doc \u306b\u5165\u308c\u307e\u3059\u3002\n1. for token in doc: \u306a\u3069\u3067\u5404token\u3092\u53c2\u7167\u3067\u304d\u307e\u3059\u3002\n1. token.text(\u305d\u306e\u8a00\u8449\u81ea\u4f53\u306e\u6587\u5b57\u5217), token.lemma_(\u305d\u306e\u8a00\u8449\u306e\u57fa\u672c\u5f62) \u306a\u3069\u306e\u30d7\u30ed\u30d1\u30c6\u30a3\u3092\u4f7f\u3063\u3066\u89e3\u6790\u3067\u304d\u307e\u3059\u3002\n1. displacy.render() \u3092\u4f7f\u3063\u3066\u56f3\u793a\u3067\u304d\u307e\u3059\u3002\n\n\u5177\u4f53\u7684\u306b\u898b\u3066\u3044\u304d\u307e\u3057\u3087\u3046\u3002\n\nspaCy Japanese includes Japanese (), ja_core_news, ja_ginza.\n\nBasic usage:\n\n1. First, set nlp. Loading Japanese (), ja_core_news, ja_ginza.\n1. Put the analysis result in doc with doc = nlp (statement to analyze).\n1. You can refer to each token with for token in doc :.\n1. It can be parsed using properties such as token.text (the lexical string of the word itself) and token.lemma_ (the basic form of the word).\n1. It can be illustrated using displacy.render ().\n\nLet's look at it concretely.\n\n\n## Japanese()\nJapanese()\u3067\u306f\u30c8\u30fc\u30af\u30f3\u5316\u3057\u3066\u304f\u308c\u307e\u3059\u304c\u3001\u4f9d\u5b58\u95a2\u4fc2\u3084\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u306f\u89e3\u6790\u3057\u3066\u304f\u308c\u307e\u305b\u3093\uff08\u56f3\u793a\u3067\u304d\u307e\u305b\u3093\uff09\u3002\n\nJapanese () will tokenize it, but it will not parse dependencies or entities (not shown).","f6dbf090":"\"Funeral\": 0, \"no\": 1.8, \"Freeren\": 0 are averaged to 0.6.\n\nIn order to add an appropriate vector, it is necessary to learn by referring to these urls.\n* [Try named entity recognition and custom model learning using spaCy + GiNZA](https:\/\/tech.mof-mof.co.jp\/blog\/spacy-ner\/)\n* [Perform entity analysis using spaCy \/ GiNZA in Python](https:\/\/qiita.com\/567000\/items\/798caa06af0985618a0d)\n\nAlso, [spaCy 101: Everything you need to know Japanese translation](https:\/\/qiita.com\/miorgash\/items\/0eda4adcc8d9ecd143e6) is also wonderful.","da996816":"\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u904e\u7a0b\u304c\u898b\u305f\u3044\u5834\u5408\u306b\u306f '> \/dev\/null 2>&1' \u3092\u3068\u3063\u3066\u304f\u3060\u3055\u3044\u3002\n\nIf you want to see the installation process, take '>\/dev\/null 2>&1'.","a27edc50":"# \u6700\u5f8c\u306b \/ Finally\n\u6700\u5f8c\u306f\u65e5\u672c\u8a9eBERT, Transformer\u306b\u3044\u304f\u3068\u601d\u3044\u307e\u3059\u304c\u3001\u524d\u51e6\u7406\u3001\u5f8c\u51e6\u7406\u3001\u5c0f\u56de\u308a\u306e\u5fc5\u8981\u306a\u51e6\u7406\u306a\u3069\u3067\u4eca\u5f8c\u3082 spaCy\/GiNZA\u304c\u5fc5\u8981\u3068\u306a\u308b\u30b1\u30fc\u30b9\u304c\u591a\u3044\u3068\u60f3\u50cf\u3057\u3066\u3044\u307e\u3059\u3002\n\nAt the end, I think I will go to Japanese BERT, Transformer, but I imagine that there are many cases where spaCy \/ GiNZA will be required in the future for pre-processing, post-processing, processing that requires a small turn, etc.","411ef869":"\u300c\u3067\u3059\u304c\u300d\u3092\u542b\u3093\u3067\u3044\u308b\u304c\u30d1\u30e9\u30ec\u30eb\u554f\u984c\u3067\u306f\u7121\u3044\uff1a\n\nContains \"\u3067\u3059\u304c\" but not a parallel problem:","29132c1e":"# \u306f\u3058\u3081\u306b \/ Introduction\n\n\u65e5\u672c\u8a9eNLP(\u81ea\u7136\u8a00\u8a9e\u51e6\u7406)\u3092\u52c9\u5f37\u3059\u308b\u306b\u3042\u305f\u308a\u3001BERT\u3084Transformer, Reformer\u306b\u624b\u3092\u51fa\u3057\u305f\u304f\u306a\u308b\u304c\u3001\u305d\u306e\u524d\u306bspaCy\/GiNZA\u3067\u57fa\u672c\u3092\u7406\u89e3\u3057\u3066\u304a\u3044\u305f\u65b9\u304c\u826f\u3044\u3002BERT\u306a\u3069\u306b\u304f\u3089\u3079\u308b\u3068\u65ad\u7136\u51e6\u7406\u6642\u9593\u30fb\u5fc5\u8981\u306a\u30de\u30b7\u30f3\u30b9\u30da\u30c3\u30af\u304c\u4f4e\u3044\u306e\u3067\u3001\u69d8\u3005\u306a\u30c8\u30e9\u30a4\uff06\u30a8\u30e9\u30fc\u306b\u306f\u6709\u5229\u306a\u306f\u305a\u3002\u611f\u89e6\u3092\u3064\u304b\u3093\u3067\u304b\u3089BERT, Transformer\u3092\u4f7f\u3044\u305f\u3044\u3057\u3001spaCy\/GiNZA\u3067\u306e\u7d4c\u9a13\u306fNLP\u5168\u822c\u3067\u3001\u69d8\u3005\u306a\u524d\u51e6\u7406\u30fb\u5f8c\u51e6\u7406\u306b\u3082\u5f79\u306b\u7acb\u3064\u3068\u4fe1\u3058\u3066\u3044\u307e\u3059\u3002\n\nWhen studying Japanese NLP (Natural Language Processing), you will want to start working on BERT, Transformer, and Reformer, but before that, you should understand the basics with spaCy \/ GiNZA. Compared to BERT etc., the processing time and required machine specifications are much lower, so it should be advantageous for various trials and errors. I want to use BERT and Transformer after getting a feel for it, and I believe that my experience with spaCy \/ GiNZA is useful for various pre-processing and post-processing in NLP in general.\n\n\u307e\u305f[\u306f\u3058\u3081\u3066\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406 \u7b2c4\u56de spaCy\/GiNZA \u3092\u7528\u3044\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406](https:\/\/www.ogis-ri.co.jp\/otc\/hiroba\/technical\/similar-document-search\/part4.html)\u306b\u3088\u308b\u3068\u3001spaCy\/GiNZA bow \u3067\u3082bert-japanese\u3068\u905c\u8272\u306a\u3044\u7d50\u679c\u304c\u3067\u308b\u30b1\u30fc\u30b9\u3082\u3042\u308b\u3088\u3046\u3067\u3059\u3002\u3046\u307e\u304f\u4f7f\u3044\u5206\u3051\u3066\u3044\u304d\u305f\u3044\u3067\u3059\u306d\u3002\n\nIn addition, [First Natural Language Processing 4th Natural Language Processing Using spaCy \/ GiNZA](https:\/\/www.ogis-ri.co.jp\/otc\/hiroba\/technical\/similar-document-search\/part4.html ), It seems that there are cases where even spaCy \/ GiNZA bow produces results comparable to bert-japanese. I want to use them properly.\n\n\u512a\u79c0\u306a\u5148\u4eba\u305f\u3061\u306e\u77e5\u898b\u3092\u53c2\u8003\u306b\u3055\u305b\u3066\u3082\u3089\u3044\u3001\u5177\u4f53\u4f8b\u3092\u591a\u304f\u307e\u3068\u3081\u305f\u3064\u3082\u308a\u3067\u3059\u3002NLP\u521d\u5fc3\u8005\u306e\u5f79\u306b\u7acb\u3066\u308c\u3070\u3046\u308c\u3057\u3044\u3067\u3059\u3002\n\nI intend to use the knowledge of excellent ancestors as a reference and summarize many concrete examples. I hope it helps NLP beginners.","b35bc012":"\u300c\u685c\u304c\u54b2\u304f\u3002\u300d\u3068\u300c\u6885\u304c\u54b2\u304f\u3002\u300d\u306f\u76f8\u5f53\u4f3c\u3066\u3044\u308b\u3002\n\n\"Sakura blooms\" and \"Plum blooms.\" Are quite similar.","28e26c37":"# \u9752\u7a7a\u6587\u5eab\u306e\u62bd\u51fa\u8981\u7d04 \/ Extraction summary of Aozora Bunko\n\n[sumy\u3092\u4f7f\u3063\u3066\u9752\u7a7a\u6587\u5eab\u3092\u8981\u7d04\u3057\u3066\u307f\u308b](https:\/\/qiita.com\/hideki\/items\/5e9892094ae786d2ad6c)\u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\u3002\n\n\u9752\u7a7a\u6587\u5eab\u3092web\u30b9\u30af\u30ec\u30a4\u30d4\u30f3\u30b0\u624b\u6cd5\u3067\u8aad\u307f\u8fbc\u3093\u3067\u8981\u7d04\u3057\u3066\u307f\u308b\u3002\n\u9752\u7a7a\u6587\u5eab\u306e\u6587\u7ae0\u672c\u4f53\u304b\u3089\u4f59\u8a08\u306a\u6539\u884c\u3001\u5168\u89d2\u6587\u5b57\u3092\u53d6\u308a\u9664\u304d\u3001spaCy\/GiNZA\u3067\u6587\u5358\u4f4d\u306b\u5206\u5272\u3057\u3066\u30ea\u30b9\u30c8\u5316\u3059\u308b\u3002\nsumy\u306f\u51fa\u73fe\u5358\u8a9e\u306b\u57fa\u3065\u3044\u305f\u985e\u4f3c\u5ea6\u304b\u3089\u8981\u7d04\u3059\u308b\u306e\u3067\u3001\u30ec\u30f3\u30de\u5316\uff08\u57fa\u672c\u5f62\u5316\uff09\u3059\u308b\u3053\u3068\u3067\u7cbe\u5ea6\u304c\u3042\u304c\u308b\u3002\n\u6700\u5f8c\u306b\u3001\u539f\u6587\u306e\u307e\u307e\u3001\u30c8\u30fc\u30af\u30f3\u5316\u307e\u3067\u3001\u30c8\u30fc\u30af\u30f3\u5316\uff0b\u30ec\u30f3\u30de\u5316\u3067\u306esumy\u8981\u7d04\u3092\u6bd4\u8f03\u3059\u308b\u3002\n\nI referred to [Aozora Bunko is summarized using sumy](https:\/\/qiita.com\/hideki\/items\/5e9892094ae786d2ad6c).\n\nLet's read Aozora Bunko by web scraping method and summarize it.\nRemove unnecessary line breaks and double-byte characters from the main text of Aozora Bunko, and use spaCy \/ GiNZA to divide the text into text units and list them.\nSince sumy is summarized from the similarity based on the appearing words, the accuracy can be improved by lemma (basic form).\nFinally, the original text, up to tokenization, is compared with sumy summaries in tokenization + lemmaization.","710a614a":"\u6b8b\u5ff5\u306a\u304c\u3089ja_ginza\u306e\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u540d\u306f\u8aac\u660e\u3057\u3066\u304f\u308c\u306a\u3044\u3088\u3046\u3067\u3059\u3002\n\nUnfortunately the entity name of ja_ginza doesn't seem to explain.","8570213c":"\u8a66\u3057\u306b\u300c\u685c\u300d\u30d9\u30af\u30bf\u30fc\u3092\u8868\u793a\u3057\u3066\u307f\u305f\u3044\u5834\u5408\u306b\u306f\u3001\u4e0b\u8a18\u306e\u30b3\u30fc\u30c8\u306e\u30b3\u30e1\u30f3\u30c8\u5916\u3057\u3066\u5b9f\u884c\u3057\u3066\u304f\u3060\u3055\u3044\u3002\n\nIf you want to display the \"Sakura\" vector as a trial, please uncomment the coat below.","94ab36b0":"\u300c\u3067\u3059\u304c\u300d\u3092\u542b\u3080\u30d1\u30e9\u30ec\u30eb\u554f\u984c\uff1a\n\nParallel problems including \"\u3067\u3059\u304c\":","ac5ceb4d":"\u30d1\u30e9\u30ec\u30eb\u554f\u984c\uff1a","28c7856c":"token\u306e\u4fc2\u308a\u53d7\u3051\u89e3\u6790\u306e\u30d7\u30ed\u30d1\u30c6\u30a3\uff1a\n\n* token.dep_ : \u69cb\u6587\u5f93\u5c5e\u95a2\u4fc2\u3002\n* token.head: \u69cb\u6587\u4e0a\u306e\u89aa\u306e\u30c8\u30fc\u30af\u30f3\u3002\n* token.children : \u69cb\u6587\u4e0a\u306e\u5b50\u306e\u30c8\u30fc\u30af\u30f3\u3002\n* token.lefts : \u69cb\u6587\u4e0a\u306e\u5de6\u306e\u30c8\u30fc\u30af\u30f3\u90e1\u3002\n* token.rights : \u69cb\u6587\u4e0a\u306e\u53f3\u306e\u30c8\u30fc\u30af\u30f3\u90e1\u3002","2edb2023":"\u3055\u3059\u304c\u306b\u300c\u308f\u30fc\u3044\u300d\u306f\u30d9\u30af\u30c8\u30eb\u6301\u3063\u3066\u3044\u306a\u3044\u3002\u3053\u306e\u5834\u5408\u306eoov\u306f\u8a9e\u5f59\u306b\u542b\u307e\u308c\u308b\u304b\u3092\u793a\u3057\u3066\u3044\u308b\u306f\u305a\u3060\u304c\u3001\u3088\u304f\u308f\u304b\u3089\u306a\u3044\u3002\n\nAs expected, \"Wow\" does not have a vector. The oov in this case should indicate if it is included in the vocabulary, but I'm not sure.","b9052207":"## ja_ginza\n\nGINZA\u306e\u30c8\u30fc\u30af\u30f3\u5316\u306b\u306f3\u30e2\u30fc\u30c9\u3042\u308a\u307e\u3059\u3002\nginza.set_split_mode(nlp, 'C')\n\u306e\u3088\u3046\u306b\u8a2d\u5b9a\u53ef\u80fd\u3067\u3059\u3002\u30e2\u30fc\u30c9'C'\u304c\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u3059\u3002\n\nThere are 3 modes for tokenizing GINZA.\nginza.set_split_mode (nlp,'C')\nIt can be set as. Mode'C'is the default.","f3d7345f":"# \u30c6\u30ad\u30b9\u30c8\u985e\u4f3c\u5ea6\u6bd4\u8f03\u306e\u4e3b\u6210\u5206\u5206\u6790 \/ Principal component analysis of text similarity comparison","c1d5e193":"Definition of syntactic dependencies:\n\n* acl: Noun clause modifier\n* advcl: Adverb clause modifier\n* advmod: Adverb modifier\n* amod: adjective modifier\n* appos: apposition\n* aux: auxiliary verb\n* case: Case display\n* cc: Coordinate conjunction\n* ccomp: Supplement\n* clf: classifier\n* compound: compound noun\n* conj: classical compound\n* cop: Concatenator\n* csubj: Main part\n* dep: unknown dependency\n* det: determiner\n* discourse: discourse element\n* dislocated: transposed\n* expl: lie\n* fixed: Fixed multi-word expression\n* flat: apposition plural word expression\n* goes with: 1 word split expression\n* iobj: Joint object\n* list: List representation\n* mark: conjunction\n* nmod: Noun modifier\n* nsubj: Subject noun\n* nummod: Numeral modifier\n* obj: object\n* obl: Italic noun\n* orphan: Independent relationship\n* parataxis: parallel\n* punct: Punctuation\n* reparandum: Word expressions that are not recognized as words\n* root: root\n* vocative: Vocalization\n* xcomp: Complement","d1738c47":"\u6b74\u53f2\u3001\u5730\u7406\u3001\u98df\u3079\u7269\u306f\u5206\u985e\u3067\u304d\u3066\u3044\u308b\u3088\u3046\u3060\u3002\uff12\u30d1\u30e9\u30ec\u30eb\u554f\u984c\u5224\u5b9a\u306f\u4e0b\u8a18\u306e\u3088\u3046\u306b\u30c8\u30fc\u30af\u30f3\u3001\u4f9d\u5b58\u95a2\u4fc2\u3067\u5224\u65ad\u3057\u305f\u65b9\u304c\u65e9\u305d\u3046\u3002\n\nHistory, geography, and food seem to be categorized. 2 Parallel problem judgment seems to be faster if it is judged by tokens and dependencies as shown below.","12038e2f":"# What is GiNZA\n\n\u300cGiNZA\u300d\u306f\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u306a\u65e5\u672c\u8a9e\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u30e9\u30a4\u30d6\u30e9\u30ea\u3067\u3059\u3002\u300cspaCy\u300d\u3092\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u3068\u3057\u3066\u5229\u7528\u3057\u3066\u304a\u308a\u3001\u30c8\u30fc\u30af\u30f3\u5316\u51e6\u7406\u306b\u30aa\u30fc\u30d7\u30f3\u30bd\u30fc\u30b9\u306a\u5f62\u614b\u7d20\u89e3\u6790\u5668\u300cSudachiPy\u300d\u304c\u4f7f\u308f\u308c\u3066\u3044\u307e\u3059\u3002\u4e3b\u306a\u6a5f\u80fd\u306f\u4e0b\u8a18\uff1a\n* \u30bb\u30f3\u30c6\u30f3\u30b9\uff08\u6587\uff09\u5883\u754c\u89e3\u6790\n* \u5f62\u614b\u7d20\u89e3\u6790\n* \u4fc2\u308a\u53d7\u3051\u89e3\u6790\n* \u56fa\u6709\u8868\u73fe(\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3)\u62bd\u51fa\n* \u6587\u7bc0\u62bd\u51fa\n\n\"GiNZA\" is an open source Japanese natural language processing library. \"SpaCy\" is used as a framework, and the open source morphological analyzer \"SudachiPy\" is used for tokenization processing. The main functions are as follows:\n* Sentence boundary analysis\n* Morphological analysis\n* Dependency analysis\n* Named entity (entity) extraction\n* Phrase extraction","b1979cd9":"\u30c8\u30fc\u30af\u30f3\u3068\u300c\u6771\u4eac\u90fd\u6e2f\u533a\u516d\u672c\u67281\u4e01\u76ee\u300d\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u304c\u7570\u306a\u308b\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002\nGPE\u306e\u610f\u5473\u306f\u4e0b\u8a18\u3067\u8abf\u3079\u3089\u308c\u307e\u3059\u3002\n\nPlease note that the token and the \"1-chome, Roppongi, Minato-ku, Tokyo\" entity are different.\nThe meaning of GPE can be found below.","6444f782":"\u3053\u306e\u4e2d\u3067\u306f\u685c\u3068\u6885\u304c\u4f3c\u3066\u3044\u308b\u3002\n\nAmong them, cherry blossoms and plums are similar.","4b0ecafe":"\u72ec\u81ea\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u3092\u8ffd\u52a0\u3059\u308b\u306b\u306f\u4e0b\u8a18\u306e\u3088\u3046\u306b\u3059\u308b\u3002\n\nTo add your own entity:","e50ec2c6":"\u54c1\u8a5e\u306e\u5b9a\u7fa9\uff1a\n\n1. NOUN : \u540d\u8a5e\n  * \u540d\u8a5e-\u666e\u901a\u540d\u8a5e (\u4f46\u3057VERB,ADJ\u3068\u3057\u3066\u4f7f\u308f\u308c\u308b\u3082\u306e\u3092\u9664\u304f) (\u4f8b: \u30d1\u30f3) \n2. PROPN : \u56fa\u6709\u540d\u8a5e\n  * \u540d\u8a5e-\u56fa\u6709\u540d\u8a5e (\u4f8b: \u5317\u6d77\u9053)\n3. VERB : \u52d5\u8a5e\n  * \u52d5\u8a5e(\u4f46\u3057\u975e\u81ea\u7acb\u3068\u306a\u308b\u3082\u306e\u3092\u9664\u304f) (\u4f8b: \u98df\u3079\u308b)\n  * \u540d\u523a+\u30b5\u5909\u53ef\u80fd\u3067\u52d5\u8a5e\u306e\u8a9e\u5c3e\u304c\u4ed8\u3044\u305f\u3082\u306e (\u4f8b: '\u98df\u4e8b'\u3059\u308b)\n4. ADJ : \u5f62\u5bb9\u8a5e\n  * \u5f62\u5bb9\u8a5e(\u4f46\u3057\u975e\u81ea\u7acb\u3068\u306a\u308b\u3082\u306e\u3092\u9664\u304f) (\u4f8b: \u5927\u304d\u3044)\n  * \u5f62\u72b6\u8a5e (\u4f8b: \u8c4a\u304b)\n  * \u9023\u4f53\u8a5e(\u4f46\u3057DET\u3092\u9664\u304f) (\u4f8b: \u5927\u304d\u306a)\n  * \u540d\u8a5e-\u5f62\u72b6\u8a5e\u53ef\u80fd\u3067\u5f62\u72b6\u8a5e\u306e\u8a9e\u5c3e\u304c\u4ed8\u304f\u5834\u5408 (\u4f8b: '\u81ea\u7531'\u306a)\n5. ADV : \u526f\u8a5e\n  * \u526f\u8a5e (\u4f8b: \u3086\u3063\u304f\u308a)\n6. INTJ : \u9593\u6295\u8a5e\n  * \u9593\u6295\u8a5e (\u4f8b: \u3042\u3063)\n7. PRON : \u4ee3\u540d\u8a5e\n  * \u4ee3\u540d\u8a5e (\u4f8b: \u79c1)\n8. NUM : \u6570\u8a5e\n  * \u540d\u8a5e-\u6570\u8a5e (\u4f8b: 5)\n9. AUX : \u52a9\u52d5\u8a5e\n  * \u52a9\u52d5\u8a5e (\u4f8b: \u305f)\n  * \u52d5\u8a5e\/\u5f62\u5bb9\u8a5e\u306e\u3046\u3061\u975e\u81ea\u7acb\u306e\u3082\u306e (\u4f8b: \u3057\u3066'\u3044\u308b', \u98df\u3079'\u306b\u304f\u3044\u2019)\n10. CONJ : \u63a5\u7d9a\u8a5e\n  * \u63a5\u7d9a\u8a5e\u307e\u305f\u306f\u52a9\u8a5e-\u63a5\u7d9a\u52a9\u8a5e\u306e\u3046\u3061\u3001\u7b49\u4f4d\u63a5 \u7d9a\u8a5e\u3068\u3057\u3066\u7528\u3044\u308b\u3082\u306e (\u4f8b: \u3068)\n11. SCONJ : \u5f93\u5c5e\u63a5\u7d9a\u8a5e\n  * \u63a5\u7d9a\u8a5e\u30fb\u52a9\u8a5e-\u63a5\u7d9a\u52a9\u8a5e(CONJ\u3068\u306a\u308b\u3082\u306e\u3092\u9664\u304f) (\u4f8b: \u3066)\n  * \u6e96\u4f53\u52a9\u8a5e (\u4f8b: \u884c\u304f'\u306e'\u304c)\n12. DET : \u9650\u5b9a\u8a5e\n  * \u9023\u4f53\u8a5e\u306e\u4e00\u90e8 (\u4f8b: \u3053\u306e, \u305d\u306e, \u3042\u3093\u306a, \u3069\u3093\u306a)\n13. ADP : \u63a5\u7f6e\u8a5e\n  * \u52a9\u8a5e-\u683c\u52a9\u8a5e (\u4f8b: \u304c)\n  * \u526f\u52a9\u8a5e (\u4f8b: \u3057\u304b)\n  * \u4fc2\u52a9\u8a5e (\u4f8b: \u3053\u305d)\n14. PART : \u63a5\u8f9e\n  * \u52a9\u8a5e-\u7d42\u52a9\u8a5e (\u4f8b: \u4f55\u6642\u3067\u3059'\u304b')\n  * \u63a5\u5c3e\u8f9e (\u4f8b: \u6df1'\u3055')\n15. PUNCT : \u53e5\u8aad\u70b9\n  * \u88dc\u52a9\u8a18\u53f7-\u53e5\u70b9\/\u8aad\u70b9\/\u62ec\u5f27\u958b\/\u62ec\u5f27\u9589\n16. SYM : \u8a18\u53f7\n  * \u8a18\u53f7\u30fb\u88dc\u52a9\u8a18\u53f7\u306e\u3046\u3061PUNCT\u4ee5\u5916\u306e\u3082\u306e\n17. X : \u305d\u306e\u4ed6\n  * \u7a7a\u767d","e78769e5":"\u72ec\u81ea\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u306f\u8ffd\u52a0\u3067\u304d\u305f\u304c\u3001\u9069\u5207\u306a\u30d9\u30af\u30c8\u30eb\u306f\u8a2d\u5b9a\u3055\u308c\u3066\u3044\u306a\u3044\u3088\u3046\u3060\u3002\n\nI was able to add my own entity, but it doesn't seem to have the proper vector set.","c3c92293":"## \u72ec\u81ea\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u8ffd\u52a0\u65b9\u6cd5\uff11 \/ How to add your own entity 1","06a7d181":"# \u30a4\u30f3\u30dd\u30fc\u30c8","ccee0e07":"\u300c\u3067\u3059\u300d\u306e\u4f9d\u5b58\u95a2\u4fc2\u306fcop\u3068aux\u306e\u5834\u5408\u304c\u3042\u308b\u3002\n\u300c\u3067\u3059\u300d\uff0b\u300c\u304c\u300d(\u4f9d\u5b58\u95a2\u4fc2:mark)\u306e\u6761\u4ef6\u3067\u5224\u5225\u3067\u304d\u305d\u3046\u3002","6b0fea69":"\u5358\u8a9e\u540c\u58eb\u306e\u985e\u4f3c\u5ea6 \/ Word-to-word similarity:\uff1a","4c373edb":"# \u72ec\u81ea\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\uff08\u72ec\u81ea\u56fa\u6709\u8868\u73fe\uff09\u306e\u8ffd\u52a0 \/ Add unique entity (named entity)","7118505c":"\u69cb\u6587\u5f93\u5c5e\u95a2\u4fc2\u306e\u5b9a\u7fa9\uff1a\n\n* acl : \u540d\u8a5e\u306e\u7bc0\u4fee\u98fe\u5b50\n* advcl : \u526f\u8a5e\u7bc0\u4fee\u98fe\u5b50\n* advmod : \u526f\u8a5e\u4fee\u98fe\u5b50\n* amod : \u5f62\u5bb9\u8a5e\u4fee\u98fe\u5b50\n* appos : \u540c\u683c\n* aux : \u52a9\u52d5\u8a5e\n* case : \u683c\u8868\u793a\n* cc : \u7b49\u4f4d\u63a5\u7d9a\u8a5e\n* ccomp : \u88dc\u6587\n* clf : \u985e\u5225\u8a5e\n* compound : \u8907\u5408\u540d\u8a5e\n* conj : \u7d50\u5408\u8a5e\n* cop : \u9023\u7d50\u8a5e\n* csubj : \u4e3b\u90e8\n* dep : \u4e0d\u660e\u306a\u4f9d\u5b58\u95a2\u4fc2\n* det : \u9650\u5b9a\u8a5e\n* discourse : \u8ac7\u8a71\u8981\u7d20\n* dislocated : \u8ee2\u7f6e\n* expl : \u5618\u8f9e\n* fixed : \u56fa\u5b9a\u8907\u6570\u5358\u8a9e\u8868\u73fe\n* flat : \u540c\u683c\u8907\u6570\u5358\u8a9e\u8868\u73fe\n* goeswith : 1\u5358\u8a9e\u5206\u5272\u8868\u73fe\n* iobj : \u95a2\u7bc0\u76ee\u7684\u8a9e\n* list : \u30ea\u30b9\u30c8\u8868\u73fe\n* mark : \u63a5\u7d9a\u8a5e\n* nmod : \u540d\u8a5e\u4fee\u98fe\u5b50\n* nsubj : \u4e3b\u8a9e\u540d\u8a5e\n* nummod : \u6570\u8a5e\u4fee\u98fe\u5b50\n* obj : \u76ee\u7684\u8a9e\n* obl : \u659c\u683c\u540d\u8a5e\n* orphan : \u72ec\u7acb\u95a2\u4fc2\n* parataxis : \u4e26\u5217\n* punct : \u53e5\u8aad\u70b9\n* reparandum : \u5358\u8a9e\u3068\u3057\u3066\u8a8d\u8b58\u3055\u308c\u306a\u3044\u5358\u8a9e\u8868\u73fe\n* root : \u30eb\u30fc\u30c8\n* vocative : \u767a\u58f0\u95a2\u4fc2\n* xcomp : \u88dc\u4f53","197e2baa":"\u6587\u540c\u58eb\u306e\u985e\u4f3c\u5ea6 \/ Similarity between sentences\uff1a","e5649166":"# \u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3068\u985e\u4f3c\u5ea6\u8a08\u7b97 \/ Word vector and similarity calculation\n\n\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306f word2vec \u306e\u3088\u3046\u306a\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3092\u7528\u3044\u3066\u751f\u6210\u304c\u53ef\u80fd\u3067\u3059\u3002\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3064\u304d\u30e2\u30c7\u30eb\u3067\u306f Token.vector \u3092\u4f7f\u3063\u3066\u30d9\u30af\u30c8\u30eb\u64cd\u4f5c\u3067\u304d\u307e\u3059\uff0e Doc.vector \u3068 Span.vector \u306e\u30d9\u30af\u30c8\u30eb\u306f\uff0c\u5404\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306e\u5e73\u5747\u304c\u7528\u3044\u3089\u308c\u308b\u3066\u3044\u307e\u3059\u3002\n\n* Token.ext: \u3082\u3068\u306e\u30c6\u30ad\u30b9\u30c8\n* Token.has vector: \u30c8\u30fc\u30af\u30f3\u304c\u30d9\u30af\u30c8\u30eb\u3092\u6301\u3063\u3066\u3044\u308b\u304b\n* Token.vector norm: \u30c8\u30fc\u30af\u30f3\u30d9\u30af\u30c8\u30eb\u306e[ L2 \u6b63\u5247\u5316\uff08\u539f\u70b9\u304b\u3089\u306e\u30e6\u30fc\u30af\u30ea\u30c3\u30c9\u8ddd\u96e2\uff1f\uff09](https:\/\/qiita.com\/g-k\/items\/d3124eb00cb166f5b575#:~:text=%E3%81%93%E3%81%A1%E3%82%89%E3%81%AB%E6%AD%A3%E5%89%87%E5%8C%96%E3%82%92,%E3%82%92%E8%A1%8C%E3%81%86%E5%BF%85%E8%A6%81%E3%81%8C%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99%E3%80%82&text=%E9%87%8D%E3%81%BFw%E3%81%8C%E6%90%8D%E5%A4%B1%E9%96%A2%E6%95%B0,%E3%81%8C%E3%82%88%E3%81%8F%E7%94%A8%E3%81%84%E3%82%89%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82)\n* Token.is_oov: \u8a9e\u5f59\u306b\u767b\u9332\u304c\u3042\u308b\u304b\u5426\u304b\n\nWord vectors can be generated using algorithms like word2vec. In the model with word vector, you can manipulate the vector using Token.vector. For the Doc.vector and Span.vector vectors, the average of each word vector is used.\n\n* Token.ext: Original text\n* Token.has vector: Does the token have a vector?\n* Token.vector norm: [L2 regularization of token vector (Euclidean distance from origin?)](https:\/\/qiita.com\/g-k\/items\/d3124eb00cb166f5b575#:~:text=%E3%81%93%E3%81%A1%E3%82%89%E3%81%AB%E6%AD%A3%E5%89%87%E5%8C%96%E3%82%92,%E3%82%92%E8%A1%8C%E3%81%86%E5%BF%85%E8%A6%81%E3%81%8C%E3%81%82%E3%82%8A%E3%81%BE%E3%81%99%E3%80%82&text=%E9%87%8D%E3%81%BFw%E3%81%8C%E6%90%8D%E5%A4%B1%E9%96%A2%E6%95%B0,%E3%81%8C%E3%82%88%E3%81%8F%E7%94%A8%E3%81%84%E3%82%89%E3%82%8C%E3%81%BE%E3%81%99%E3%80%82)\n* Token.is_oov: Whether or not the vocabulary is registered","e4945ae4":"# spaCy\/GiNZa install","a328613e":"# \u30c8\u30fc\u30af\u30f3\u5316\uff08\u5f62\u614b\u7d20\u89e3\u6790\uff09\/ Tokenization (morphological analysis)","7a54989b":"[[For beginners] Language analysis using the natural language processing tool \"GiNZA\" (from morphological analysis to vectorization)](https:\/\/qiita.com\/cove_ht\/items\/63ffdd8ff237d4845566) was used as a reference.\n\nBased on the quiz question, compare the similarity of texts and analyze the principal components. The 2 parallel problem has a '2p:' at the beginning of the label.","e8ee4bdd":"\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u306f[word2vec](https:\/\/qiita.com\/g-k\/items\/69afa87c73654af49d36)\u3067\u751f\u6210\u3067\u304d\u307e\u3059\u3002\n\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3092\u5dee\u3057\u66ff\u3048\u308b\u65b9\u6cd5\u306f [\u306f\u3058\u3081\u3066\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406 \u7b2c4\u56de spaCy\/GiNZA \u3092\u7528\u3044\u305f\u81ea\u7136\u8a00\u8a9e\u51e6\u7406](https:\/\/www.ogis-ri.co.jp\/otc\/hiroba\/technical\/similar-document-search\/part4.html)\n\u3067\u7d39\u4ecb\u3055\u308c\u3066\u3044\u307e\u3059\u3002\n\nThe word vector can be generated with [word2vec](https:\/\/qiita.com\/g-k\/items\/69afa87c73654af49d36)).\n\nHow to replace the word vector is [First natural language processing 4th Natural language processing using spaCy\/GiNZA](https:\/\/www.ogis-ri.co.jp\/otc\/hiroba\/technical\/similar-document-search\/part4.html).","67c7fd7b":"\u4eca\u5f8c\u306fGINZA\u306e\u30e2\u30fc\u30c9C(\u30c7\u30d5\u30a9\u30eb\u30c8)\u3092\u4f7f\u3063\u3066\u3044\u304d\u307e\u3059\u3002\n\nFrom now on, I will use GINZA mode C (default).","e312cd44":"[\u3010\u521d\u5fc3\u8005\u5411\u3051\u3011\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u30c4\u30fc\u30eb\u300cGiNZA\u300d\u3092\u7528\u3044\u305f\u8a00\u8a9e\u89e3\u6790\uff08\u5f62\u614b\u7d20\u89e3\u6790\u304b\u3089\u30d9\u30af\u30c8\u30eb\u5316\u307e\u3067\uff09](https:\/\/qiita.com\/cove_ht\/items\/63ffdd8ff237d4845566)\u3092\u53c2\u8003\u306b\u3057\u307e\u3057\u305f\u3002\n\n\u30af\u30a4\u30ba\u554f\u984c\u3092\u3082\u3068\u306b\u30c6\u30ad\u30b9\u30c8\u306e\u985e\u4f3c\u5ea6\u6bd4\u8f03\u3001\u4e3b\u6210\u5206\u5206\u6790\u3092\u3057\u3066\u307f\u308b\u30022\u30d1\u30e9\u30ec\u30eb\u554f\u984c\u306f\u30e9\u30d9\u30eb\u306e\u6700\u521d\u306b'2p:'\u3092\u3064\u3051\u3066\u3044\u308b\u3002\n","42c21eb6":"**GINZA 5.0 \u3067\u52d5\u4f5c\u304c\u5909\u308f\u3063\u305f\u3088\u3046\u3067\u3059\u3002\u3053\u306e\u7ae0\u306f\u98db\u3070\u3057\u3066\u304f\u3060\u3055\u3044\u3002**\n\n**It seems that the behavior has changed in GINZA 5.0. Skip this chapter.**","63e699d0":"## \u5206\u6790\u7528\u30b3\u30fc\u30d1\u30b9\u306e\u6e96\u5099 \/ Preparation of corpus for analysis\nGiNZA\u3067\u5206\u304b\u3061\u66f8\u304d(\u30c8\u30fc\u30af\u30f3)\u5316\u3057\u307e\u3059\u3002\u305d\u306e\u5f8c\u3001\u6d3b\u7528\u306a\u3069\u3067\u8868\u8a18\u304c\u7570\u306a\u308b\u3068\u9055\u3046\u5358\u8a9e\u3068\u3057\u3066\u8a08\u7b97\u3055\u308c\u3066\u3057\u307e\u3046\u305f\u3081\u3001\u5206\u6790\u7528\u30b3\u30fc\u30d1\u30b9\u3067\u306f\u30ec\u30f3\u30de\u5316\u3057\u307e\u3059\n\nUse GiNZA to divide (token). After that, if the notation is different due to utilization etc., it will be calculated as a different word, so it will be a lemma in the analysis corpus","8fcb4421":"\u30bb\u30f3\u30c6\u30f3\u30b9\uff08\u6587\uff09\u306e\u4e2d\u306b\u3082token\u3068ents\uff08\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\uff09\u304c\u3042\u308b\u3002\n\nThere are also tokens and ents in sentences.","e7e5263c":"# \u53c2\u8003 \/ reference\n\n## \u5f62\u614b\u7d20\u89e3\u6790\u306bjanome\u3092\u4f7f\u3063\u3066livedoor\u30cb\u30e5\u30fc\u30b9\u30b3\u30fc\u30d1\u30b9\u3092\u4e3b\u6210\u5206\u5206\u6790\n## Principal component analysis of livedoor news corpus using janome for morphological analysis\n\n[Livedoor\u30cb\u30e5\u30fc\u30b9\u30b3\u30fc\u30d1\u30b9\u3067\u4e3b\u6210\u5206\u5206\u6790 - \u524d\u6e96\u5099 -](https:\/\/qiita.com\/torahirod\/items\/2d8644ee6bb67de68039)\n\n[Livedoor\u30cb\u30e5\u30fc\u30b9\u30b3\u30fc\u30d1\u30b9\u3067\u4e3b\u6210\u5206\u5206\u6790 - \u5b9f\u8df5 - ](https:\/\/qiita.com\/torahirod\/items\/18abf694f574eb4a8ea3)\n\n![livedoor\u30cb\u30e5\u30fc\u30b9\u30b3\u30fc\u30d1\u30b9\u4e3b\u6210\u5206\u5206\u6790](https:\/\/camo.qiitausercontent.com\/181d8edfe6a71b9043546937a8f538ae9686d010\/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3439303532312f63646337386130622d666334652d353962392d356433322d6235626336613261343861662e706e67)\n\n![\u56e0\u5b50\u8ca0\u8377\u91cf\u3092\u78ba\u8a8d](https:\/\/camo.qiitausercontent.com\/5d19783e64961cff6ae48b084238b6783b62e8a7\/68747470733a2f2f71696974612d696d6167652d73746f72652e73332e61702d6e6f727468656173742d312e616d617a6f6e6177732e636f6d2f302f3439303532312f64376430666630632d643738392d643165622d633938372d6161366566383161333361342e706e67)\n\n## spaCy\/GiNZA\u3092\u4f7f\u3063\u305fLivedoor\u30cb\u30e5\u30fc\u30b9\u30b3\u30fc\u30d1\u30b9\u306e\u30c6\u30ad\u30b9\u30c8\u30de\u30a4\u30cb\u30f3\u30b0\n## Livedoor News Corpus text mining with spaCy \/ GiNZA\n\n[\u306f\u3058\u3081\u3066\u306e\u81ea\u7136\u8a00\u8a9e\u51e6\u7406 \u7b2c6\u56de OSS \u306b\u3088\u308b\u30c6\u30ad\u30b9\u30c8\u30de\u30a4\u30cb\u30f3\u30b0](https:\/\/www.ogis-ri.co.jp\/otc\/hiroba\/technical\/similar-document-search\/part6.html)\n\n![](https:\/\/www.ogis-ri.co.jp\/otc\/hiroba\/technical\/similar-document-search\/img\/pic201912-101.png)\n![](https:\/\/www.ogis-ri.co.jp\/otc\/hiroba\/technical\/similar-document-search\/img\/pic201912-103.png)\n![](https:\/\/www.ogis-ri.co.jp\/otc\/hiroba\/technical\/similar-document-search\/img\/pic201912-105.png)\n![](https:\/\/www.ogis-ri.co.jp\/otc\/hiroba\/technical\/similar-document-search\/img\/pic201912-106.png)\n![](https:\/\/www.ogis-ri.co.jp\/otc\/hiroba\/technical\/similar-document-search\/img\/pic201912-109.png)","20e6e424":"The \"\u3067\u3059\" dependency may be cop and aux.\nIt seems that it can be determined by the condition of \"\u3067\u3059\" + \"\u304c\" (dependency: mark).","155c8f76":"\u30a8\u30f3\u30c6\u30a3\u30c6\u30a3\u306f doc.ents \u3067\u53d6\u308a\u51fa\u305b\u307e\u3059\u3002\u4f55\u6587\u5b57\u76ee\u304b\u3089\u4f55\u6587\u5b57\u76ee\u307e\u3067\u304c\u5bfe\u5fdc\u3057\u3066\u3044\u308b\u304b\u3082\u308f\u304b\u308a\u307e\u3059\u3002\n\nEntity can be retrieved with doc.ents. You can also see what character to what character corresponds.","a9855814":"* # \u30bb\u30f3\u30c6\u30f3\u30b9\uff08\u6587\uff09\u533a\u5207\u308a\u89e3\u6790 \/ Sentence delimited analysis"}}