{"cell_type":{"5283844d":"code","c3028d73":"code","fbe4def9":"code","67b88f3a":"code","76280846":"code","8593c303":"code","d46473d5":"code","e3dbf54b":"code","e1651501":"code","f7597aee":"code","3d0796aa":"code","b6027464":"code","989f13dd":"code","52769dea":"code","803891ed":"code","5bb93fa7":"code","622041ed":"code","aaaf7057":"code","814d9369":"code","adf863c8":"code","7f271cb6":"code","fb02df16":"code","e41221ad":"code","659a36c6":"code","bed6e698":"code","0cbfdda0":"code","352d3dca":"code","5d51841d":"code","41cf0b3f":"code","d2cc5023":"code","034c9471":"code","e15b7fc4":"code","a41926f1":"code","b3da7640":"code","eb54dcc5":"code","9e5c1f18":"code","c3458ac0":"code","9e46df31":"code","e5203b32":"code","14b6afae":"code","3e2a6e18":"code","6ef94166":"code","8d8dbdd4":"code","d3d91432":"code","915bdf85":"code","2e9ed156":"code","a0b00722":"code","53a9f277":"code","23fd12b4":"code","f9ed28a8":"code","c46ab0dd":"code","98c6f706":"code","30a3641d":"code","58904da8":"code","f03b0b79":"code","831dd005":"markdown","d315bf7e":"markdown","f2795c03":"markdown","89e9d369":"markdown","9af15acc":"markdown","7cb799dd":"markdown","8ffef425":"markdown","95e474ee":"markdown","06c947bc":"markdown","425bf660":"markdown","91a7efed":"markdown","6492dd29":"markdown","636ff1e6":"markdown","f1a1bc62":"markdown","33dc835f":"markdown","a6cbcd58":"markdown","9ee202c0":"markdown","55920fa4":"markdown","7bc4b1a3":"markdown","3d390762":"markdown","0d0f5107":"markdown","1f7000ec":"markdown","2b972f7d":"markdown","0518f142":"markdown","564de41a":"markdown","4b793cbe":"markdown","8bfcff22":"markdown","e38391c7":"markdown","5f1c38d0":"markdown","4be5ad41":"markdown","8dce940e":"markdown","505e575f":"markdown","c406f4ab":"markdown","75b6d27b":"markdown","bfbbf5d5":"markdown","95318b4f":"markdown","e80a57f7":"markdown","46ad96a0":"markdown","ce470a60":"markdown","02d4fd9a":"markdown","1ab17d68":"markdown","3aee61b2":"markdown","ac61c139":"markdown","cb84a44e":"markdown","c0b102f2":"markdown","9df4e65e":"markdown","f8891b1f":"markdown","660adcd1":"markdown","9f8e3482":"markdown","329bbd29":"markdown","0881937f":"markdown","8ec3e507":"markdown","bcabd6a6":"markdown","9a0fef35":"markdown","f121fa2c":"markdown","51396c5a":"markdown","7e3ef18b":"markdown","51f5e4b2":"markdown","4ace852d":"markdown","78d4ec50":"markdown","b11a848f":"markdown","0d100c16":"markdown","87ad0525":"markdown","576eb62f":"markdown","18f1d806":"markdown"},"source":{"5283844d":"#importing the libraries\n\n#Data Processing Libraries\nimport numpy as np\nimport pandas as pd\n\n#Data Vizualization Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Pretty display for notebooks\n%matplotlib inline\n\n# Machine Learning Library\nfrom sklearn.preprocessing import LabelEncoder # Encode Categorical Variable to Numerical Variable\nfrom sklearn.preprocessing import Imputer # Imputer Class to replace missing values\nfrom sklearn.metrics import confusion_matrix # Library for model evaluation\nfrom sklearn.metrics import accuracy_score # Library for model evaluation\nfrom sklearn.model_selection import train_test_split # Library to split datset into test and train\n\nfrom sklearn.linear_model  import LogisticRegression # Logistic Regression Classifier\nfrom sklearn.linear_model import SGDClassifier # Stochastic Gradient Descent Classifier\nfrom sklearn.tree import DecisionTreeClassifier # Decision Tree Classifier\nfrom sklearn.ensemble  import RandomForestClassifier # Random Forest Classifier\nfrom sklearn.neighbors import KNeighborsClassifier # K Nearest neighbors Classifier\nfrom sklearn.naive_bayes import GaussianNB #Naive Bayes Classifier\nfrom sklearn.svm import SVC #Support vector Machine Classifier\nfrom sklearn.ensemble import AdaBoostClassifier # Ada Boost Classifier\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\n\n#Ignoring the warnings\nimport warnings\nwarnings.filterwarnings('ignore')","c3028d73":"# Read .csv file from location and load into pandas DataFrame\ndatset_churn = pd.read_csv('..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","fbe4def9":"# Keeping a backup of original datset.Always a good practice\ndatset_churn_copy = datset_churn.copy()","67b88f3a":"datset_churn.shape  # output = (rows, columns)","76280846":"# Getting the column names\ndatset_churn.columns.values","8593c303":"# Renaming the 3 columns.\ndatset_churn = datset_churn.rename(columns={'customerID' : 'CustomerID' , 'gender': 'Gender', 'tenure':'Tenure'})\nprint(datset_churn.columns.values)","d46473d5":"datset_churn.info()","e3dbf54b":"datset_churn.head()","e1651501":"datset_churn['TotalChargesNum']=pd.to_numeric(datset_churn['TotalCharges'])","f7597aee":"#Identifying the rows containing missing data\nmissing_value_row = list(datset_churn[datset_churn['TotalCharges'] == \" \"].index)\nprint('Missing Value Rows-->', missing_value_row , '\\nTotal rows-->', len(missing_value_row))","3d0796aa":"# Replacing the spaces with 0\nfor missing_row in missing_value_row :\n    datset_churn['TotalCharges'][missing_row] = 0","b6027464":"# Let's try to convert it back to Numeric\ndatset_churn['TotalCharges']=pd.to_numeric(datset_churn['TotalCharges'])","989f13dd":"datset_churn.info()","52769dea":"datset_churn.head() # This will print first 5 rows in pandas dataset.","803891ed":"datset_churn.describe(include=['O'])","5bb93fa7":"#Creating the list of columns\ndatset_churn_column = list(datset_churn.columns)\n\n#Removing numerical columns & CustomerID\ndatset_churn_column.remove('CustomerID')\ndatset_churn_column.remove('SeniorCitizen')\ndatset_churn_column.remove('Tenure')\ndatset_churn_column.remove('MonthlyCharges')\ndatset_churn_column.remove('TotalCharges')\n\n# Printing Unique values in each categorical column\nfor col in datset_churn_column:\n    print(col, \"-\", datset_churn[col].unique())","622041ed":"datset_churn.describe()","aaaf7057":"print(\"Assess missing values in dataset\")\ntotal = datset_churn.isnull().sum().sort_values(ascending=False)\npercent = (datset_churn.isnull().sum()\/datset_churn.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nprint(missing_data)","814d9369":"datset_churn[['MonthlyCharges','Tenure','TotalCharges']].head()","adf863c8":"#Identifying the rows containing 0 value in Total Charges\nzero_value_row = list(datset_churn[datset_churn['TotalCharges'] == 0].index)\nprint('0 Value Rows-->', missing_value_row , '\\nTotal rows-->', len(missing_value_row))","7f271cb6":"# Replacing the spaces with 0\nfor zero_row in zero_value_row :\n    datset_churn['TotalCharges'][zero_row] = datset_churn['Tenure'][zero_row] * datset_churn['MonthlyCharges'][zero_row]","fb02df16":"#Validating the data\nfor zero_row in zero_value_row :\n    print( datset_churn['MonthlyCharges'][zero_row],datset_churn['Tenure'][zero_row],datset_churn['TotalCharges'][zero_row])","e41221ad":"# Getting the list of all columns\ncolumns_hist = list(datset_churn.columns)\n\n#Removing the Numerical Variables\ncolumns_hist.remove('CustomerID')\ncolumns_hist.remove('SeniorCitizen')\ncolumns_hist.remove('Tenure')\ncolumns_hist.remove('MonthlyCharges')\ncolumns_hist.remove('TotalCharges')\n\n#Creating Column into 4X4 matrix to display 16 bar charts in 4X4 form:\ncolumns_hist_nparray = np.array(columns_hist)\ncolumns_hist_nparray = np.reshape(columns_hist_nparray, (4,4)) # reshaping the columns into 4X4 matrix","659a36c6":"# Plotting the bar chart\nrows = 4 ; columns = 4\nf, axes = plt.subplots(rows, columns, figsize=(20, 20))\nprint('Univariate Analysis of each categorical Variables')\nfor row in range(rows):\n    for column in range(columns):\n        sns.countplot(datset_churn[columns_hist_nparray[row][column]], palette = \"Set1\", ax = axes[row, column])","bed6e698":"print('Univariate Analysis of each numerical Variables')\nf, axes = plt.subplots(2, 3, figsize=(20,10))\n#Charting the histogram\ndatset_churn[\"Tenure\"].plot.hist(color='DarkBlue', alpha=0.7, bins=50, title='Tenure',ax=axes[0, 0])\ndatset_churn[\"MonthlyCharges\"].plot.hist(color='DarkBlue', alpha=0.7, bins=50, title='MonthlyCharges',ax=axes[0, 1])\ndatset_churn[\"TotalCharges\"].plot.hist(color='DarkBlue', alpha=0.7, bins=50, title='TotalCharges',ax=axes[0, 2])\n\n#Charting the density plot\nsns.distplot( datset_churn[\"Tenure\"] , kde=True, rug=False, color=\"skyblue\", ax=axes[1, 0])\nsns.distplot( datset_churn[\"MonthlyCharges\"] , kde=True, rug=False, color=\"olive\", ax=axes[1, 1])\nsns.distplot( datset_churn[\"TotalCharges\"] , kde=True, rug=False, color=\"gold\", ax=axes[1, 2])","0cbfdda0":"sns.countplot(datset_churn['SeniorCitizen'], palette = \"Set1\")","352d3dca":"f, axes = plt.subplots(1, 3, figsize=(15,5))\nsns.boxplot(x=datset_churn[\"Tenure\"], orient=\"v\", color=\"olive\",ax=axes[0])\nsns.boxplot(x=datset_churn[\"MonthlyCharges\"], orient=\"v\", color=\"gold\",ax=axes[1])\nsns.boxplot(x=datset_churn[\"TotalCharges\"] , orient=\"v\", color=\"skyblue\",ax=axes[2])","5d51841d":"# Converting the categorical variable to numerical variable\ndatset_churn['Churn_Num'] = datset_churn['Churn'].map( {'Yes': 1, 'No': 0} ).astype(int)","41cf0b3f":"# Validating the mappaing\ndatset_churn[['Churn','Churn_Num']].head()","d2cc5023":"# Plotting Tenure Column with Churn\n# Churn_num indicates customer who left the company. 0 indicates customer who stayed.\nfighist = sns.FacetGrid(datset_churn, col='Churn_Num')\nfighist.map(plt.hist, 'Tenure', bins=20) ","034c9471":"# Plotting MonthlyCharges Column with Churn\n# Churn_num indicates customer who left the company. 0 indicates customer who stayed.\nfighist = sns.FacetGrid(datset_churn, col='Churn_Num')\nfighist.map(plt.hist, 'MonthlyCharges', bins=20)","e15b7fc4":"# Plotting TotalCharges Column with Churn\n# Churn_num indicates customer who left the company. 0 indicates customer who stayed.\nfighist = sns.FacetGrid(datset_churn, col='Churn_Num')\nfighist.map(plt.hist, 'TotalCharges', bins=20)","a41926f1":"col_list = columns_hist\ncol_list.remove('Churn')\nfor col in col_list:\n    if col == 'PaymentMethod':\n        aspect_ratio = 2.0\n    else:\n        aspect_ratio = 0.8\n        \n    plot_cat_data = sns.catplot(x=col, col='Churn_Num', data = datset_churn, kind='count', height=4, aspect=aspect_ratio)","b3da7640":"# Creating tenure band and co-relation with Churn\ndatset_churn['TenureRange'] = pd.cut(datset_churn['Tenure'], 5)\ndatset_churn[['TenureRange', 'Churn_Num']].groupby(['TenureRange'], as_index=False).mean().sort_values(by='TenureRange', ascending=True)\n\n# Replacing Age band with ordinals based on these bands\ndatset_churn.loc[ datset_churn['Tenure'] <= 8, 'TenureCat'] = 0\ndatset_churn.loc[(datset_churn['Tenure'] > 8) & (datset_churn['Tenure'] <= 15), 'TenureCat'] = 1\ndatset_churn.loc[(datset_churn['Tenure'] > 15) & (datset_churn['Tenure'] <= 30), 'TenureCat'] = 2\ndatset_churn.loc[(datset_churn['Tenure'] > 30) & (datset_churn['Tenure'] <= 45 ), 'TenureCat'] = 3\ndatset_churn.loc[(datset_churn['Tenure'] > 45) & (datset_churn['Tenure'] <= 60 ), 'TenureCat'] = 4\ndatset_churn.loc[ datset_churn['Tenure'] > 60, 'TenureCat'] = 5\n\ndatset_churn[['Tenure','TenureRange','TenureCat']].head(10)","eb54dcc5":"# Creating MonthlyCharges Band and co-relation with Churn\ndatset_churn['MonthlyChargesRange'] = pd.cut(datset_churn['MonthlyCharges'], 5)\ndatset_churn[['MonthlyChargesRange', 'Churn_Num']].groupby(['MonthlyChargesRange'], as_index=False).mean().sort_values(by='MonthlyChargesRange', ascending=True)\n\n# Replacing Age band with ordinals based on these bands\ndatset_churn.loc[ datset_churn['MonthlyCharges'] <= 20, 'MonthlyChargesCat'] = 0\ndatset_churn.loc[(datset_churn['MonthlyCharges'] > 20) & (datset_churn['MonthlyCharges'] <= 40), 'MonthlyChargesCat'] = 1\ndatset_churn.loc[(datset_churn['MonthlyCharges'] > 40) & (datset_churn['MonthlyCharges'] <= 60), 'MonthlyChargesCat'] = 2\ndatset_churn.loc[(datset_churn['MonthlyCharges'] > 60) & (datset_churn['MonthlyCharges'] <= 80 ), 'MonthlyChargesCat'] = 3\ndatset_churn.loc[(datset_churn['MonthlyCharges'] > 80) & (datset_churn['MonthlyCharges'] <= 100 ), 'MonthlyChargesCat'] = 4\ndatset_churn.loc[ datset_churn['MonthlyCharges'] > 100, 'MonthlyChargesCat'] = 5\n\n#Checking the categories\ndatset_churn[['MonthlyCharges','MonthlyChargesRange','MonthlyChargesCat']].head(10)","9e5c1f18":"#Creating a new column for family. If a customer has dependant or Partner, I am considering it as family .\nlist_family = []\nfor rows in range(len(datset_churn['Partner'])):\n    if ((datset_churn['Partner'][rows] == 'No') and (datset_churn['Dependents'][rows] == 'No')):\n        list_family.append('No')\n    else:\n        list_family.append('Yes')\ndatset_churn['Family'] = list_family\n#print(datset_churn[['Partner', 'Dependents', 'Family' ]].head(10))\n\n#Creating a new column for Online Services (Online Security & Online Backup) . If a customer has Online Security or Online Backup services\n#then , I am considering it as \"Yes\" else \"No\"\nlist_online_services = []\nfor rows_os in range(len(datset_churn['OnlineSecurity'])):\n    if ((datset_churn['OnlineSecurity'][rows_os] == 'No') and (datset_churn['OnlineBackup'][rows_os] == 'No')):\n        list_online_services.append('No')\n    else:\n        list_online_services.append('Yes')\ndatset_churn['OnlineServices'] = list_online_services\n\n#print(datset_churn[['OnlineSecurity', 'OnlineBackup', 'OnlineServices' ]].head(10))\n \n#Creating a new column for Streaming Services (StreamingTV & StreamingMovies) . If a customer has StreamingTV or StreamingMovies\n#then , I am considering it as \"Yes\" else \"No\"\nlist_streaming_services = []\nfor rows_stv in range(len(datset_churn['StreamingTV'])):\n    if ((datset_churn['StreamingTV'][rows_stv] == 'No') and (datset_churn['StreamingMovies'][rows_stv] == 'No')):\n        list_streaming_services.append('No')\n    else:\n        list_streaming_services.append('Yes')\ndatset_churn['StreamingServices'] = list_streaming_services\n\n#print(datset_churn[['StreamingTV', 'StreamingMovies', 'StreamingServices' ]].head(10))\n\nplot_cat_data = sns.catplot(x='Family', col='Churn_Num', data = datset_churn, kind='count', height=4, aspect=0.8)\nplot_cat_data = sns.catplot(x='OnlineServices', col='Churn_Num', data = datset_churn, kind='count', height=4, aspect=0.8)\nplot_cat_data = sns.catplot(x='StreamingServices', col='Churn_Num', data = datset_churn, kind='count', height=4, aspect=0.8)","c3458ac0":"datset_churn.info()","9e46df31":"#Converting Gender column to numeric value\n#datset_churn['Gender'].unique() # Print unique values in the column\ndatset_churn['Gender_Num'] = datset_churn['Gender'].map( {'Female': 1, 'Male': 0} ).astype(int) #Map Categorical to Numerical Values\ndatset_churn[['Gender','Gender_Num']].head(2) # Test the mapping","e5203b32":"# For Partner & Dependant , we created Family Column . Converting Family column to numeric value\n#datset_churn['Family'].unique() # Print unique values in the column\ndatset_churn['Family_Num'] = datset_churn['Family'].map( {'Yes': 1, 'No': 0} ).astype(int) #Map Categorical to Numerical Values\ndatset_churn[['Family','Family_Num']].head(2) # Test the mapping","14b6afae":"datset_churn['PhoneService_Num'] = datset_churn['PhoneService'].map( {'Yes': 1, 'No': 0} ).astype(int)\ndatset_churn['MultipleLines_Num'] = datset_churn['MultipleLines'].map( {'No': 0, 'Yes': 1, 'No phone service':2} ).astype(int)\ndatset_churn['InternetService_Num'] = datset_churn['InternetService'].map( {'DSL': 0, 'Fiber optic': 1, 'No':2} ).astype(int)\ndatset_churn['OnlineServices_Num'] = datset_churn['OnlineServices'].map( {'Yes': 1, 'No': 0} ).astype(int)\n\ndatset_churn['DeviceProtection_Num'] = datset_churn['DeviceProtection'].map( {'No': 0, 'Yes': 1, 'No internet service':2} ).astype(int)\ndatset_churn['StreamingServices_Num'] = datset_churn['StreamingServices'].map( {'Yes': 1, 'No': 0} ).astype(int)\ndatset_churn['TechSupport_Num'] = datset_churn['TechSupport'].map( {'No': 0, 'Yes': 1, 'No internet service':2} ).astype(int)\ndatset_churn['Contract_Num'] = datset_churn['Contract'].map( {'Month-to-month': 0, 'One year': 1, 'Two year': 2} ).astype(int)\ndatset_churn['PaperlessBilling_Num'] = datset_churn['PaperlessBilling'].map( {'Yes': 1, 'No': 0} ).astype(int)\ndatset_churn['PaymentMethod_Num'] = datset_churn['PaymentMethod'].map( {'Electronic check': 0, 'Mailed check': 1, 'Bank transfer (automatic)': 2 , 'Credit card (automatic)' : 3} ).astype(int)","3e2a6e18":"datset_churn.info()","6ef94166":"# Take a copy of dataset\ndatset_churn_copy = datset_churn.copy()","8d8dbdd4":"#Dropping the Categorical columns and keeping their equivalent numeric column\ncolumns_to_drop = ['Gender', 'Partner', 'Dependents', 'Tenure', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'TotalCharges', 'Churn', 'Family', 'OnlineServices', 'StreamingServices']\ndatset_churn = datset_churn.drop(columns_to_drop, axis=1)\n\n#Re-arranging the columns as per origial dataset\ndatset_churn = datset_churn[['CustomerID', 'Gender_Num', 'SeniorCitizen', 'Family_Num', 'TenureCat', 'PhoneService_Num', 'MultipleLines_Num', 'InternetService_Num', 'OnlineServices_Num', 'DeviceProtection_Num', 'TechSupport_Num', 'StreamingServices_Num', 'Contract_Num', 'PaperlessBilling_Num', 'PaymentMethod_Num', 'MonthlyChargesCat', 'Churn_Num']]\ndatset_churn = datset_churn.rename(columns={'Gender_Num' : 'Gender', \n                             'Family_Num' : 'Family',\n                             'PhoneService_Num' : 'PhoneService',\n                             'MultipleLines_Num': 'MultipleLines', \n                             'InternetService_Num' : 'InternetService', \n                             'OnlineServices_Num' : 'OnlineServices', \n                             'DeviceProtection_Num' : 'DeviceProtection',\n                             'TechSupport_Num' : 'TechSupport', \n                             'StreamingServices_Num' : 'StreamingServices', \n                             'Contract_Num' : 'Contract', \n                             'PaperlessBilling_Num' : 'PaperlessBilling', \n                             'PaymentMethod_Num' : 'PaymentMethod', \n                             'MonthlyCharges' : 'MonthlyCharges', \n                             'Churn_Num' :  'Churn' })\ndatset_churn.info()","d3d91432":"datset_churn.head(10) # Taking a quick look into the new data","915bdf85":"X = datset_churn.iloc[:,1:16].values # Feature Variable\ny = datset_churn.iloc[:,16].values # Target Variable\n\n#Dividing data into test & train splitting 70% data for training anf 30% for test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)\nprint('There are {} samples in the training set and {} samples in the test set'.format(X_train.shape[0], X_test.shape[0]))","2e9ed156":"#Creating function for Confusion Matrix , Precsion, Recall and F1 Score\ndef plot_confusion_matrix(classifier, y_test, y_pred_test):\n    cm = confusion_matrix(y_test, y_pred_test)\n    \n    print(\"\\n\",classifier,\"\\n\")\n    plt.clf()\n    plt.imshow(cm, interpolation='nearest', cmap='RdBu')\n    classNames = ['Churn-No','Churn-Yes']\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    tick_marks = np.arange(len(classNames))\n    plt.xticks(tick_marks, classNames, rotation=45)\n    plt.yticks(tick_marks, classNames)\n    s = [['TN','FP'], ['FN', 'TP']]\n    \n    for i in range(2):\n        for j in range(2):\n            plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]), \n                     horizontalalignment='center', color='White')\n    \n    plt.show()\n        \n    tn, fp, fn, tp = cm.ravel()\n\n    recall = tp \/ (tp + fn)\n    precision = tp \/ (tp + fp)\n    F1 = 2*recall*precision\/(recall+precision)\n\n    print('Recall={0:0.3f}'.format(recall),'\\nPrecision={0:0.3f}'.format(precision))\n    print('F1={0:0.3f}'.format(F1))\n    return;","a0b00722":"from sklearn.metrics import average_precision_score, precision_recall_curve\ndef plot_prec_rec_curve(classifier, y_test, y_pred_score):\n    precision, recall, _ = precision_recall_curve(y_test, y_pred_score)\n    average_precision = average_precision_score(y_test, y_pred_score)\n\n    print('Average precision-recall score: {0:0.3f}'.format(\n          average_precision))\n\n    plt.plot(recall, precision, label='area = %0.3f' % average_precision, color=\"green\")\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('Recall')\n    plt.ylabel('Precision')\n    plt.title('Precision Recall Curve')\n    plt.legend(loc=\"best\")\n    plt.show()","53a9f277":"# Making a list of all classifiers\nclassifier_model = [LogisticRegression(),KNeighborsClassifier(),GaussianNB(),SVC(),DecisionTreeClassifier(),RandomForestClassifier(), SGDClassifier(), AdaBoostClassifier()]\n\n# Creating empty list to store the performance details\nclassifier_model_list= []\nclassifier_accuracy_test = []\nclassifier_accuracy_train = []\nf1score = []\nprecisionscore = []\nrecallscore = []\navg_pre_rec_score = []\ncv_score = []\n\nfor classifier_list in classifier_model:\n    classifier = classifier_list\n \n    # Fitting the training set into classification model\n    classifier.fit(X_train,y_train)\n    \n    # Predicting the output on test datset\n    y_pred_test = classifier.predict(X_test)    \n    score_test = accuracy_score(y_test, y_pred_test)\n    \n    # Predicting the output on training datset\n    y_pred_train = classifier.predict(X_train) \n    score_train = accuracy_score(y_train, y_pred_train)\n    \n    # Cross Validation Score on training test\n    scores = cross_val_score(classifier, X_train,y_train, cv=10)\n    cv_score.append(scores.mean())\n    \n    #Keeping the model and accuracy score into a list\n    classifier_model_list.append(classifier_list.__class__.__name__)\n    classifier_accuracy_test.append(round(score_test,4))\n    classifier_accuracy_train.append(round(score_train,4))\n    \n    #Precision, Recall and F1 score\n    f1score.append(f1_score(y_test, y_pred_test))\n    precisionscore.append(precision_score(y_test, y_pred_test))\n    recallscore.append(recall_score(y_test, y_pred_test))\n    \n    #Calculating Average Precision Recall Score\n    try:\n        y_pred_score = classifier.decision_function(X_test)\n    except:\n        y_pred_score = classifier.predict_proba(X_test)[:,1]\n    \n    from sklearn.metrics import average_precision_score\n    average_precision = average_precision_score(y_test, y_pred_score)\n    avg_pre_rec_score.append(average_precision)\n    \n    \n    #Confusion Matrix\n    plot_confusion_matrix(classifier_list.__class__.__name__, y_test, y_pred_test)\n    plot_prec_rec_curve(classifier_list.__class__.__name__, y_test, y_pred_score)","23fd12b4":"#Creating pandas dataframe with Model and corresponding accuracy\n#accuracy_df = pd.DataFrame({'Model':classifier_model_list , 'Test Accuracy':classifier_accuracy_test, 'Train Accuracy' :classifier_accuracy_train , 'Precision':precisionscore, 'Recall':recallscore ,'F1 Score':f1score},index=None)\naccuracy_df = pd.DataFrame({'Model':classifier_model_list , 'Cross Val Score':cv_score, 'Test Accuracy' :classifier_accuracy_train , 'Precision':precisionscore, 'Recall':recallscore ,'Avg Precision Recall':avg_pre_rec_score ,'F1 Score':f1score})\n\n# Calculating Average Accuracy = (Test + Train)\/2\naccuracy_df['Average_Accuracy'] =  (accuracy_df['Cross Val Score'] + accuracy_df['Test Accuracy'] )\/ 2\n\n#Arranging the Columns\nprint(\"\\n*------------------------------    CLASSIFICATION MODEL PERFORMANCE EVALUATION      ---------------------*\\n\")\naccuracy_df = accuracy_df[['Model','Cross Val Score', 'Test Accuracy', 'Average_Accuracy','Precision', 'Recall','Avg Precision Recall','F1 Score']]  # This will arrange the columns in the order we want\n\n#Sorting the Columns based on Average Accuracy\naccuracy_df.sort_values('Average_Accuracy', axis=0, ascending=False, inplace=True) # Sorting the data with highest accuracy in the top\naccuracy_df\n#accuracy_df.transpose()","f9ed28a8":"from sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import fbeta_score, accuracy_score\nfrom sklearn.linear_model  import LogisticRegression # Logistic Regression Classifier\n\n#Logistic Regression Classifier\nclf = LogisticRegression()\n\n#Hyperparameters\nparameters = {'C':np.logspace(0, 4, 10), \n              'penalty' : ['l1', 'l2']\n             }\n\n# Make an fbeta_score scoring object\nscorer = make_scorer(fbeta_score,beta=0.5)\n\n# Perform grid search on the classifier using 'scorer' as the scoring method\ngrid_obj = GridSearchCV(clf, parameters,scorer)\n\n# Fit the grid search object to the training data and find the optimal parameters\ngrid_fit = grid_obj.fit(X_train,y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# View best hyperparameters\n#print(grid_srchfit.best_params_)\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)\n\n# Report the before-and-afterscores\nprint (\"Unoptimized model\\n------\")\nprint (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\nprint (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))\nprint (\"\\nOptimized Model\\n------\")\nprint (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint (\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))\nprint (best_clf)","c46ab0dd":"# TODO: Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import fbeta_score, accuracy_score\n\n# TODO: Initialize the classifier\nclf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\n\n# TODO: Create the parameters list you wish to tune\nparameters = {'n_estimators':[50, 120], \n              'learning_rate':[0.1, 0.5, 1.],\n              'base_estimator__min_samples_split' : np.arange(2, 8, 2),\n              'base_estimator__max_depth' : np.arange(1, 4, 1)\n             }\n\n# TODO: Make an fbeta_score scoring object\nscorer = make_scorer(fbeta_score,beta=0.5)\n\n# TODO: Perform grid search on the classifier using 'scorer' as the scoring method\ngrid_obj = GridSearchCV(clf, parameters,scorer)\n\n# TODO: Fit the grid search object to the training data and find the optimal parameters\ngrid_fit = grid_obj.fit(X_train,y_train)\n\n# Get the estimator\nbest_clf = grid_fit.best_estimator_\n\n# Make predictions using the unoptimized and model\npredictions = (clf.fit(X_train, y_train)).predict(X_test)\nbest_predictions = best_clf.predict(X_test)\n\n# Report the before-and-afterscores\nprint (\"Unoptimized model\\n------\")\nprint (\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\nprint (\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))\nprint (\"\\nOptimized Model\\n------\")\nprint (\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\nprint (\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))\nprint (best_clf)","98c6f706":"# Feature Importance for Adaboost\nfrom sklearn.feature_selection import RFE\nfeatures = list(datset_churn.columns[1:16])\n\n# Feature Importance for AdaBoostClassifier\nadboost_cls = AdaBoostClassifier()\nadboost_cls .fit(X_train, y_train)\nfeature_imp_adboost = np.round(adboost_cls.feature_importances_, 5)\n\nfeature_imp_df = pd.DataFrame({'Features' :features, 'Adaboost_Score': feature_imp_adboost})\nfeature_imp_df.sort_values('Adaboost_Score', axis=0, ascending=False, inplace=True)\nprint(feature_imp_df)","30a3641d":"dataset_churn_new = datset_churn[['MonthlyChargesCat', 'TenureCat', 'Contract', 'InternetService', 'MultipleLines', 'PaymentMethod', 'Churn']]\nX_new = dataset_churn_new.iloc[:,:-1].values # Feature Variable\ny_new = dataset_churn_new.iloc[:,-1].values # Target Variable\n\n#Dividing data into test & train splitting 80% data for training and 20% for test\nX_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_new , y_new, test_size=0.20)\nprint('There are {} samples in the training set and {} samples in the test set'.format(X_train_new.shape[0], X_test_new.shape[0]))","58904da8":"#Adaboost Classifier , filled the hyperparameter from the Grid Search\nclassifier = AdaBoostClassifier(algorithm='SAMME.R',\n          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n            max_features=None, max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best'),\n          learning_rate=0.5, n_estimators=120, random_state=None)\n \n# Fitting the training set into classification model\nclassifier.fit(X_train_new, y_train_new)\n    \n# Predicting the output on test datset\ny_pred_new = classifier.predict(X_test_new)    \n\ntry:\n    y_pred_new_score = classifier.decision_function(X_test_new)\nexcept:\n    y_pred_new_score = classifier.predict_proba(X_test_new)[:,1]\n    \n#Confusion Matrix and Precision Recall Curve\nplot_confusion_matrix('Adaboost Classifier', y_test_new, y_pred_new)\nplot_prec_rec_curve('Adaboost Classifier', y_test_new, y_pred_new_score)","f03b0b79":"#Logistic Regression , filled the hyperparameter from the Grid Search\nclassifier_logreg = LogisticRegression(C=2.7825594022071245, class_weight=None, dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n          solver='liblinear', tol=0.0001, verbose=0, warm_start=False)\n \n# Fitting the training set into classification model\nclassifier_logreg.fit(X_train_new, y_train_new)\n    \n# Predicting the output on test datset\ny_pred_new = classifier_logreg.predict(X_test_new)    \n\ntry:\n    y_pred_new_score = classifier_logreg.decision_function(X_test_new)\nexcept:\n    y_pred_new_score = classifier_logreg.predict_proba(X_test_new)[:,1]\n    \n#Confusion Matrix and Precision Recall Curve\nplot_confusion_matrix('Logistic Regression', y_test_new, y_pred_new)\nplot_prec_rec_curve('Logistic Regression', y_test_new, y_pred_new_score)","831dd005":"#### Observations\n\n1. Since our dataset class is imbalanced. Churn \"Yes\" is almost 3 times as \"No', Accuracy is not the right measure and we have to consider Precision, Recall and F1 Score for further evaluation and improvement of model\n    \n    1.1 Precision: A measure of a classifiers exactness.A low precision can also indicate a large number of False Positives.\n    \n    1.2 Recall: A measure of a classifiers completeness.A low recall indicates many False Negatives.\n    \n    1.3 F1 Score (or F-score): A weighted average or Harmonic Mean of precision and recall.\n\n2. Logistic Regression (AUC = 0.65) and Adaboost model (AUC = 0.65) looks promising. Let's try to improve the model","d315bf7e":"#### Let's get the column names\/information","f2795c03":"### Converting the object variable into Numeric variable.","89e9d369":"#### Creating bands for numerical variables - Tenure & Monthly Charges","9af15acc":"#### Summary Statistics of Numeric variable.","7cb799dd":"#### Which features are numerical?\nSeniorCitizen, Tenure, MonthlyCharges, TotalCharges\n\nContinous - Tenure, MonthlyCharges, TotalCharges\n\nDiscrete  - SeniorCitizen\n\n#### Which features are categorical?\nPhoneService, MultipleLines, InternetService, OnlineSecurity, OnlineBackup DeviceProtection, TechSupport, StreamingTV, StreamingMovies, Contract, PaperlessBilling, PaymentMethod, gender, Partner, Dependents\n\n#### Which features are Nominal or Ordinal ?\nOrdinal data (variables with a meaningful order) - No.\n\nNominal data (categories that have no meaningful order) - All Columns.\n\n#### Which features are mixed data types?\nNone","8ffef425":"## Feature Importance\n### In this section , we will run scikit learn feature importances to evaluate which columns are being give higher weights","95e474ee":"Observation - Seems like we dont have outliers !","06c947bc":"#### Observation\n - Customers with family are less likely to Churn\n - Customers not opted for online services (online backup or security) have slightly higher chances of churn\n - Customer opted for Streaming Services seems to have slightly higher chances of churn","425bf660":"### Taking first look into data","91a7efed":"#### Observations:\n1. We have 4 numeric variables\n2. Tenure can vary from 0 months to 72 months. This is how long customer is with Telco.\n3. Total Charges = Monthly Charges * Tenure\n4. Looking at the count column, all columns have count as 7043 . TotalCharges have count of 7032, a differece of 11 records. These are missing records","6492dd29":"#### Observations :\n1. We have almost equal genders in our dataset\n2. Almost 50% have partners\n3. Around 30% have dependants\n4. 85% of the customers have phone service\n5. Around 40% customers have multiple lines\n6. People prefer Fiber Optics over DSL for Internet\n7. Around 30% have taken online security.Majority of Customer don't have Online security or backup\n8. Close 35% prefer device protection\n9. Majority of Customer don't have Tech Support\n10. Around 37% have registered for Streaming TV & MOvie\n11. Contract - Majority of customers are subscribed for Month to Month contract (55%)\n12. Majority of customers have opted Paperless billing\n13. Majority of customers pay eletronic check. 43 % prefer Automatic payment (Bank Transfer and Credit Card)\n14. Target Variable - \"Churn\" - We have unbalanced distribution (Yes - Approx 1800 ; No - Approx 5000). So Churn positive is 25% (Approx).","636ff1e6":"## Preparing Columns for Classification","f1a1bc62":"Observation -   TotalCharges is object variable. By column name, we can guess it as amount field. \n### Let's take a look into the data","33dc835f":"Hope you enjoyed the kernel. Thank You!\nJagannath Banerjee |https:\/\/jagannathbanerjee.com | https:\/\/www.linkedin.com\/in\/jagannath-banerjee\/ | Aug 2018","a6cbcd58":"#### Let's Check for Outliers using Box Plot  for Tenure, MonthlyCharges, TotalCharges","9ee202c0":"### Vizualizing the Numeric variables\n#### Histogram to see data distribution of Quantitative Variables(SeniorCitizen, tenure, MonthlyCharges, TotalCharges)\n","55920fa4":"#### Find out total rows and columns or shapes of the dataset","7bc4b1a3":"## Bivariate Analysis\nCorrelating the features with Target column (Churn)\nLet us start by understanding correlations between numerical features and our solution goal (Churn).\n\nA histogram chart is useful for analyzing continous numerical variables like tenure , Monthly Charges and Total Charges where banding or ranges will help identify useful patterns. The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. This helps us answer questions relating to specific bands.","3d390762":"#### Defining function for Precision Recall Curve","0d0f5107":"### Basic Overview of dataset","1f7000ec":"### Assess missing values in dataset","2b972f7d":"### Who is Telco ?\nTelco Systems is market-leading solutions enable service providers to create and operate high quality, service assured, carrier-grade, intelligent networks. They bring over 40 years of experience to the design and development of advanced, high-performance telecom network communications solutions. \n\nTelco provide the capabilities for service differentiation that enable new forms of revenue production, maximizing network profitability. Service providers, large and small, depend on our consistent delivery of advanced solutions, enabling them to stay ahead of the capacity crunch while keeping total cost of ownership to a minimum.\n\n(Refrence - http:\/\/www.telco.com\/index.php?page=company-profile)","0518f142":"### Understanding the summary statistics , central tendency and dispersion of dataset","564de41a":"#### Observations:\n\n1. Customer who left the Telco are mostly customers within 1st month (600+) and churn steady declines with time.\n2. If customer can be retained between 10-20 months, there are high chances, customer will stay very long. Churn decreases over time\n3. Customer at 72 month tenure, mostly stayed (Churn=0).\n\n#### Decisions:\n\n1. We should definitely use 'Tenure' column in our model training.\n2. We should band 'Tenure'","4b793cbe":"No Errors! Let's view the column details","8bfcff22":"Observation - \n1. TotalCharges is left aligned. Numeric variables should be right aligned\n2. It's defines as object variable. We will convert it to numeric variable","e38391c7":"## Get Data\n\n1. Source of data  - https:\/\/www.kaggle.com\/blastchar\/telco-customer-churn\n2. Space - 955 KB\n3. Legal Obligations - Free Dataset","5f1c38d0":"### Business Objective\n\nEvery retailer is concerned about high customer churn rate. Churn rate the number of customers who drop out of the buying cycle. It could be non renewal of a loyalty program, or unhappy customers going in search of a better service. One of the key things for the busines to run is loyal customeers , meaning minimize the churn rate.\n\n#### Business objective of this exercise :\n1. Analyze customer data to understand reason for churn  and who could be the next potential customer to leave the company\n2. What contributes to the higher churn rate of customer and what could be some of the probable solution to address the same.\n\n#### What type of problem is it ?\nSupervised Machine Learning -  Classfication problem\n\n#### How should performance be measured ?\nModel performance of at least 70% is expected\n\n#### Assumptions made :\n1. The sample data is correct represetation of the entire population and is randomly selected\n2. The columns in the dataset are exhaustive list of features that determine churn rate","4be5ad41":"### Vizualizing the Categorical variables with bar chart","8dce940e":"### Converting the Object\/Categorical Variable to Numerical Variable","505e575f":"#### Barchart for the Gender (0 is No , 1 is Yes)","c406f4ab":"### Grid Search for Adaboost Classifier and running with optimized hyperparameters","75b6d27b":"Observations:\nBoth Logistic Regression & Adaboost Classifier gives us final F score of 0.65  and accuracy of 0.80 post grid search.","bfbbf5d5":"# TELCO CUSTOMER CHURN\n### Focused customer retention programs","95318b4f":"#### Defining function for Confusion Matrix , Precision, Recall and F1 Score","e80a57f7":"Target Variable or The variable we want to predict is 'Churn'\n\nFeature Variable  - All Other columns (First 20 columnns)\n\n1. Customers who left within the last month \u2013  'Churn'\n2. Customer Services \u2013 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup' 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies'\n3. Customer Account information \u2013 'Tenure', 'Contract', 'PaperlessBilling', 'PaymentMethod', 'MonthlyCharges', 'TotalCharges'\n4. Customer Personal Information \u2013 'Gender', 'SeniorCitizen', 'Partner', 'Dependents'","46ad96a0":"### Master Classification Engine","ce470a60":"#### Summary Statistics of Object\/Categorical variable. ","02d4fd9a":"#### Observations:\n1. Tenure:\n\n    1.1. Not a normal distribution. Bi-Modal distribution (having 2 peaks) which means data is concentrated across two different groups\n    \n    1.2 We have major chunk of customers in 0-1 month period. Lot of them might be cutomers who tried the service and left or liked the service and continued\n    \n    1.3. Between 10 months to 65 months, we can see flat distribution of data.\n    \n    1.4. We have lot customers in 69-72 months range. They are the loyal customers\n   \n   \n 2. Monthly Charges  - \n \n     2.1. Not a normal distribution.Close to Bi-Modal distribution\n     \n     2.2. Majority of customers are paying $18 to $20 dollars. Must be the service charge for basic service. Majority of customers are subscribed to basic package.\n     \n     2.3. Between $70-$100 dollars, we  have quite a number of customers. They might be the ones subscribed for multiple services.\n     \n     \n 3. Total Charges - \n \n    3.1. Data is positively skewed.\n    \n    3.2. Majority of the population have spent close to $1,100 dollars\n    \n    3.3. Cutomers have spent upto $9,000  dollars \n","1ab17d68":"Interesting ! So Tenure is 0 for the cstomers with TotalCharges as 0, that initially had a space. We will leave it as it and move ahead.","3aee61b2":"There are 7043 rows and 21 columns including the target\/output variable.","ac61c139":"### CLASSIFICATION MODEL PERFORMANCE EVALUATION","cb84a44e":"### Now we will create the dataset with top 5 columns and run Adaboost classifier to see if there is any improvement in performance","c0b102f2":"### Content of the dataset\nEach row represents a customer, each column contains customer\u2019s attributes described on the column Metadata.\n\nThe data set includes information about:\n\n1. Customers who left within the last month \u2013 the column is called Churn\n2. Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n3. Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n4. Demographic info about customers \u2013 gender, age range, and if they have partners and dependents","9df4e65e":"\nObservation - No missing values. But remember, we replaced columns with 0 values. We need data to be filled into that columns. Now let's find a way.","f8891b1f":"#### Creating new derived columns for Categorical variables","660adcd1":"#### Observation :\n1. Majority of customers are in 18 to 20 range and they didn't leave\n2. Customer Leaving are mostly in the bannd of 75-100  who have opted for multiple services.\n\n#### Decisions :\n1. We will use 'MonthlyCharges' column in our model training.\n2. We should band 'MonthlyCharges'","9f8e3482":"## Conclusion\n1.  Adaboost classifier performed well with Precision Recall Curve -  0.65\n2. MonthlyChargesCat, TenureCat, Contract, InternetService, MultipleLines, PaymentMethod are leading contributing to churn\n\nModel can be further improved using the strategies discussed in next paragragh.\n\n### Handling Imbalaced Dataset :\n\n1. Increasing the number of instances of the minority class (This case Churn = 'Yes') . We need more data with Churn Class as \"Yes\".\n2. Decreasing the number of instances of majority class\n3. Random Under-Sampling\n4. Random Over-Sampling\n5. Cluster-Based Over Sampling\n6. Synthetic Minority Over-sampling Technique(SMOTE)\n\nDetailed explanation - https:\/\/www.analyticsvidhya.com\/blog\/2017\/03\/imbalanced-classification-problem\/","329bbd29":"## Classification\n### We will run all classifiers to have an initial look at the performance","0881937f":"#### Observation:\nIt's difficult to conclude anything using this column. Total charges are Tenure * MonthlyCharges . Tenur might me high and Monthly charges may be low and vice-versa. Data is positively skewed.\n\n#### Decision\nWe will not use this column\n","8ec3e507":"Observation -   'customerID' , 'gender' and 'tenure' in lowercase. We will rename to convert the first letter to uppercase","bcabd6a6":"### Now we will delete the non-required rows and prepare the dataset for classification","9a0fef35":"16% of customers are senior citizens","f121fa2c":"TotalCharges looks like close to product of MonthlyCharges & Tenure , although not exact but close. We will replace the 0 values with \n( MonthlyCharges * Tenure)","51396c5a":"## Improving our Model: Model Tuning\n### Grid Search for Logistic Regression Classifier and running with optimized hyperparameters","7e3ef18b":"#### Observations :\n1. Gender, Partner, Dependents, PhoneService, PaperlessBilling and Churn - They have 2 unique categories - Yes\/No and for gender Male\/Female\n2. MultipleLines, InternetService, OnlineSecurity, OnlineBackup, DeviceProtection, TechSupport, StreamingTV, StreamingMovies, Contract - They have 3 unique categories\n3. Payment Method -  4 unique categories or 4 methods by which customer pays for their service","51f5e4b2":"### Now we will use the Categorical variables and their relationships with Churn\n\n#### We will use Seaborn Categorical Plot","4ace852d":"## Import Libraries","78d4ec50":"#### Observation:","b11a848f":"Observation - Column has \" \" . Let's find the columns containing spaces.","0d100c16":"## Univariate Analysis","87ad0525":"1. Overall - Contract, Monthly Charges, Tenure and Payment Method and Internet Service are leading columns contributing to churn. They consitute 60% weight from Mean_Feature_Importance\n\n2. Gender has no impact on Churn","576eb62f":"### Creating new feature from existing set of columns using the above observations","18f1d806":"#### Observations (Churn_Num - 1 is \"Yes\" ; 0 is \"No\") :\n1. 'Gender'  : Difficult to determine Churn using this field. Counts are almost same  in either category\n2. 'Partner' : Customer with partner have lower chance of leaving\n3. 'Dependents' :Customer with dependants have lower chance of leaving. We will merge Partner & Dependant Columns as 1 column\n4. 'PhoneService' & 'MultipleLines' : We will merge these columns into PhoneLines - Single & Multiple and determine\n5. 'InternetService' : Customer with Fiber Optic Interner Service have higher chances of leaving\n6. 'OnlineSecurity' & 'OnlineBackup' : We will merge these columns for better visibility\n7. 'DeviceProtection' : Customers without device protection have likely higher chances of leaving\n8. 'TechSupport' - Customer not opting for TechSupport have higher chances of leaving \n9. 'StreamingTV', 'StreamingMovies' - We will merge these columns into streaming and check again\n10. 'Contract' - Month to Month customers have likely higher chances to leave\n11. 'PaperlessBilling' - Customers with paperless billing have higher chances of leaving\n12. 'PaymentMethod' - People paying with electronic check have higher chances of leaving"}}