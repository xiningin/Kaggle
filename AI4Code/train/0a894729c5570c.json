{"cell_type":{"43f7870e":"code","39dbe275":"code","577657f8":"code","4c1e07b5":"code","9278c7aa":"code","e8682e31":"code","029a28bb":"code","2093a77c":"code","b6ef0f03":"code","d30abd36":"code","408a4f43":"code","bb5560fd":"code","58c8acd6":"code","d1c8589d":"code","66d5417c":"code","6590545f":"code","bf8c95fd":"code","fc323a45":"code","83349624":"code","8899bc09":"code","b7b207cb":"code","55ed5009":"code","c2213ce2":"code","dba45b50":"code","ebac59bf":"code","e03c6fef":"code","9b76a7d7":"code","e20ed65d":"code","afe94537":"code","6f48b3bc":"code","8138109b":"code","05be600b":"code","5d97ca5e":"code","db022f46":"code","4a7f01e6":"code","c2579ce6":"code","7dbea53d":"code","16a7b0cb":"code","5713aa78":"code","4042f753":"code","61fe6beb":"code","707e01a3":"code","dc42b8ab":"code","633c11aa":"code","737e70dc":"code","b605b821":"code","d91aad3d":"code","4218c898":"markdown","cd6fb3b5":"markdown","32e336d3":"markdown","08295fe0":"markdown","c6d3eeb1":"markdown","c9acee73":"markdown","16917537":"markdown","8c6c1c94":"markdown","53c5cfd2":"markdown","96e10fb4":"markdown","31c4f4cc":"markdown","13be24e8":"markdown","3c1aa5b8":"markdown","2e032678":"markdown","4b78f3fc":"markdown","a9e42541":"markdown","83005597":"markdown","6574ffb3":"markdown","cda844b5":"markdown","8f40aa31":"markdown","afdb06d5":"markdown","eea97b20":"markdown","1875bf29":"markdown","1f989334":"markdown"},"source":{"43f7870e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\n\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nos.listdir('..\/input')\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM, Dropout, Bidirectional\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence","39dbe275":"brainwave_df = pd.read_csv('..\/input\/emotions.csv')","577657f8":"brainwave_df.head()","4c1e07b5":"brainwave_df.shape","9278c7aa":"brainwave_df.describe()","e8682e31":"plt.figure()\nsns.countplot(x=brainwave_df.label, color='blue')\nplt.title('Emotional sentiment class distribution')\nplt.ylabel('Class Counts')\nplt.xlabel('Class Label')\nplt.xticks(rotation='vertical');","029a28bb":"label_df = brainwave_df['label']\nbrainwave_df.drop('label', axis = 1, inplace=True)","2093a77c":"correlations = brainwave_df.corr(method='pearson')\ncorrelations","b6ef0f03":"skew = brainwave_df.skew()\nskew","d30abd36":"%%time\n\npl_random_forest = Pipeline(steps=[('random_forest', RandomForestClassifier())])\nscores = cross_val_score(pl_random_forest, brainwave_df, label_df, cv=10,scoring='accuracy')\nprint('Accuracy for RandomForest : ', scores.mean())","408a4f43":"%%time\n\npl_log_reg = Pipeline(steps=[('scaler',StandardScaler()),\n                             ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=200))])\nscores = cross_val_score(pl_log_reg, brainwave_df, label_df, cv=10,scoring='accuracy')\nprint('Accuracy for Logistic Regression: ', scores.mean())","bb5560fd":"scaler = StandardScaler()\nscaled_df = scaler.fit_transform(brainwave_df)\npca = PCA(n_components = 30)\npca_vectors = pca.fit_transform(scaled_df)\nfor index, var in enumerate(pca.explained_variance_ratio_):\n    print(\"Explained Variance ratio by Principal Component \", (index+1), \" : \", var)\n","58c8acd6":"plt.figure()\nplt.plot(pca.explained_variance_ratio_)\nplt.xticks(rotation='vertical')","d1c8589d":"plt.figure(figsize=(25,8))\nsns.scatterplot(x=pca_vectors[:, 0], y=pca_vectors[:, 1], hue=label_df)\nplt.title('Principal Components vs Class distribution', fontsize=16)\nplt.ylabel('Principal Component 2', fontsize=16)\nplt.xlabel('Principal Component 1', fontsize=16)\nplt.xticks(rotation='vertical');","66d5417c":"%%time\npl_log_reg_pca = Pipeline(steps=[('scaler',StandardScaler()),\n                             ('pca', PCA(n_components = 2)),\n                             ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=200))])\nscores = cross_val_score(pl_log_reg_pca, brainwave_df, label_df, cv=10,scoring='accuracy')\nprint('Accuracy for Logistic Regression with 2 Principal Components: ', scores.mean())","6590545f":"%%time\n\npl_log_reg_pca_10 = Pipeline(steps=[('scaler',StandardScaler()),\n                             ('pca', PCA(n_components = 10)),\n                             ('log_reg', LogisticRegression(multi_class='multinomial', solver='saga', max_iter=200))])\nscores = cross_val_score(pl_log_reg_pca_10, brainwave_df, label_df, cv=10,scoring='accuracy')\nprint('Accuracy for Logistic Regression with 10 Principal Components: ', scores.mean())","bf8c95fd":"%%time\n\npl_mlp = Pipeline(steps=[('scaler',StandardScaler()),\n                             ('mlp_ann', MLPClassifier(hidden_layer_sizes=(1275, 637)))])\nscores = cross_val_score(pl_mlp, brainwave_df, label_df, cv=10,scoring='accuracy')\nprint('Accuracy for ANN : ', scores.mean())","fc323a45":"%%time\n\npl_svm = Pipeline(steps=[('scaler',StandardScaler()),\n                             ('pl_svm', LinearSVC())])\nscores = cross_val_score(pl_svm, brainwave_df, label_df, cv=10,scoring='accuracy')\nprint('Accuracy for Linear SVM : ', scores.mean())","83349624":"%%time\npl_xgb = Pipeline(steps=\n                  [('xgboost', xgb.XGBClassifier(objective='multi:softmax'))])\nscores = cross_val_score(pl_xgb, brainwave_df, label_df, cv=10)\nprint('Accuracy for XGBoost Classifier : ', scores.mean())","8899bc09":"# np.array(brainwave_df).shape\nX = np.array(brainwave_df)\n# X = np.reshape(X, (X.shape[0], 1, X.shape[1]))\nY = np.array(label_df)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y)\nX_train.shape, X_test.shape, Y_train.shape, Y_test.shape","b7b207cb":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_train = np.resize(X_train, (X_train.shape[0], 1, X_train.shape[1]))\nX_test = np.resize(X_test, (X_test.shape[0], 1, X_test.shape[1]))","55ed5009":"label_enc = LabelEncoder()\nY_train = label_enc.fit_transform(Y_train)\nY_test = label_enc.transform(Y_test)\nY_train.shape, Y_test.shape","c2213ce2":"model = Sequential()\nmodel.add(LSTM(120, activation='relu', input_shape=(1, 2548)))\nmodel.add(Dense(1))\nmodel.compile(optimizer='adam', loss='mse', metrics=['accuracy'])","dba45b50":"history = model.fit(X_train, Y_train, epochs=100, validation_split=0.2, verbose=1)","ebac59bf":"scores = model.evaluate(X_test, Y_test, verbose=0)\nscores","e03c6fef":"lstmdrop = Sequential()\nlstmdrop.add(LSTM(100))\nlstmdrop.add(Dropout(0.2))\nlstmdrop.add(Dense(1, activation='sigmoid'))\nlstmdrop.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","9b76a7d7":"history_lstm = lstmdrop.fit(X_train, Y_train, epochs=100, validation_split=0.2, verbose=1)","e20ed65d":"scores_dropout = lstmdrop.evaluate(X_test, Y_test)\nscores_dropout","afe94537":"lstm_stack = Sequential()\nlstm_stack.add(LSTM(200, activation='relu', return_sequences=True, input_shape=(1, 2548)))\nlstm_stack.add(LSTM(100, activation='relu', return_sequences=True))\nlstm_stack.add(LSTM(50, activation='relu', return_sequences=True))\nlstm_stack.add(LSTM(25, activation='relu'))\nlstm_stack.add(Dense(20, activation='relu'))\nlstm_stack.add(Dense(10, activation='relu'))\nlstm_stack.add(Dense(1, activation='sigmoid'))\nlstm_stack.compile(optimizer='adam', loss='mse', metrics=['accuracy'])","6f48b3bc":"history_stacked = lstm_stack.fit(X_train, Y_train, epochs=100, validation_split=0.2, verbose=1)","8138109b":"scores_stacked = lstm_stack.evaluate(X_test, Y_test)\nscores_stacked","05be600b":"conv = Sequential()\nconv.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nconv.add(MaxPooling1D(pool_size=2, padding='same'))\nconv.add(LSTM(100))\nconv.add(Dense(1, activation='sigmoid'))\nconv.compile(optimizer='adam', loss='mse', metrics=['accuracy'])","5d97ca5e":"history_conv = conv.fit(X_train, Y_train, epochs=100, validation_split=0.2, verbose=1)","db022f46":"scores_conv = conv.evaluate(X_test, Y_test)\nscores_conv","4a7f01e6":"conv_stack = Sequential()\nconv_stack.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nconv_stack.add(MaxPooling1D(pool_size=2, padding='same'))\nconv_stack.add(LSTM(100, activation='relu', return_sequences=True))\nconv_stack.add(LSTM(50, activation='relu', return_sequences=True))\nconv_stack.add(LSTM(25, activation='relu'))\nconv_stack.add(Dense(20, activation='relu'))\nconv_stack.add(Dense(10, activation='relu'))\nconv_stack.add(Dense(1, activation='sigmoid'))\nconv_stack.compile(optimizer='adam', loss='mse', metrics=['accuracy'])","c2579ce6":"history_conv_stack = conv_stack.fit(X_train, Y_train, epochs=100, validation_split=0.2, verbose=1)","7dbea53d":"scores_conv_stack = conv_stack.evaluate(X_test, Y_test)\nscores_conv_stack","16a7b0cb":"history","5713aa78":"bi = Sequential()\nbi.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(1, 2548)))\nbi.add(Dense(1))\nbi.compile(optimizer='adam', loss='mse', metrics=['accuracy'])","4042f753":"history_bi = bi.fit(X_train, Y_train, epochs=100, validation_split=0.2, verbose=1)","61fe6beb":"scores_bi = bi.evaluate(X_test, Y_test)\nscores_bi","707e01a3":"print(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('LSTM Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('LSTM Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","dc42b8ab":"print(history_lstm.history.keys())\n# summarize history for accuracy\nplt.plot(history_lstm.history['acc'])\nplt.plot(history_lstm.history['val_acc'])\nplt.title('LSTM\/Dropout Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_lstm.history['loss'])\nplt.plot(history_lstm.history['val_loss'])\nplt.title('LSTM\/Dropout Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","633c11aa":"print(history_stacked.history.keys())\n# summarize history for accuracy\nplt.plot(history_stacked.history['acc'])\nplt.plot(history_stacked.history['val_acc'])\nplt.title('Stacked LSTM Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_stacked.history['loss'])\nplt.plot(history_stacked.history['val_loss'])\nplt.title('Stacked LSTM Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","737e70dc":"print(history_conv.history.keys())\n# summarize history for accuracy\nplt.plot(history_conv.history['acc'])\nplt.plot(history_conv.history['val_acc'])\nplt.title('Convolutions + LSTM Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_conv.history['loss'])\nplt.plot(history_conv.history['val_loss'])\nplt.title('Convolutions + LSTM Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","b605b821":"print(history_conv_stack.history.keys())\n# summarize history for accuracy\nplt.plot(history_conv_stack.history['acc'])\nplt.plot(history_conv_stack.history['val_acc'])\nplt.title('Convolutions + Stacked LSTM Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_conv_stack.history['loss'])\nplt.plot(history_conv_stack.history['val_loss'])\nplt.title('Convolutions + Stacked LSTM Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","d91aad3d":"print(history_bi.history.keys())\n# summarize history for accuracy\nplt.plot(history_bi.history['acc'])\nplt.plot(history_bi.history['val_acc'])\nplt.title('Bidirectional LSTM Accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_bi.history['loss'])\nplt.plot(history_bi.history['val_loss'])\nplt.title('Bidirectional LSTM Loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","4218c898":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Take all 10 PCs<\/h2>","cd6fb3b5":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Artificial Neural Network Classifier (ANN)<\/h2>\n\nAn ANN classifier is non-linear with automatic feature engineering and dimensional reduction techniques. `MLPClassifier` in scikit-learn works as an ANN. But here also, basic scaling is required for the data.[](http:\/\/)","32e336d3":"# LSTM with Dropout","08295fe0":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">RandomForest Classifier<\/h2>\n\n`RandomForest` is a tree & bagging approach-based ensemble classifier. It will automatically reduce the number of features by its probabilistic entropy calculation approach.","c6d3eeb1":"# BiLSTM","c9acee73":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Dimensions of Data","16917537":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Import Packages<\/h2>","8c6c1c94":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Peek of Data<\/h2>","53c5cfd2":"# CONV + LSTM","96e10fb4":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Principal Component Analysis (PCA)<\/h2>\n\nPCA can transform original low level variables to a higher dimensional space and thus reduce the number of required variables. All co-linear variables get clubbed together. ","31c4f4cc":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\"> A comparison of different classifiers\u2019 accuracy & performance for high-dimensional data<\/h2>","13be24e8":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Logistic Regression Classifier<\/h2>\n\n`Logistic Regression` is a linear classifier and works in same way as linear regression.","3c1aa5b8":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Logistic Regression classifier with these two PCs<\/h2>","2e032678":"# Simple LSTM","4b78f3fc":"# Stacked LSTM","a9e42541":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Correlation Between Attributes<\/h2>\nCorrelation refers to the relationship between two variables and how they may or may not change together.\n\nThe most common method for calculating correlation is [Pearson\u2019s Correlation Coefficient](https:\/\/en.wikipedia.org\/wiki\/Pearson_product-moment_correlation_coefficient), that assumes a normal distribution of the attributes involved. A correlation of -1 or 1 shows a full negative or positive correlation respectively. Whereas a value of 0 shows no correlation at all.","83005597":"# Histories","6574ffb3":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Class Distribution<\/h2>","cda844b5":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Linear Support Vector Machines Classifier (SVM)<\/h2>","8f40aa31":"# Stacked LSTM + Conv ","afdb06d5":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Skew of Univariate Distributions<\/h2>","eea97b20":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Problem formulation<\/h2>\n\nThe **EEG Brainwave Dataset** contains electronic brainwave signals from an EEG headset and is in temporal format.\n\nThe challenge is: **Can we predict emotional sentiment from brainwave readings?**","1875bf29":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Extreme Gradient Boosting Classifier (XGBoost)<\/h2>\n\nXGBoost is a boosted tree based ensemble classifier. Like \u2018RandomForest\u2019, it will also automatically reduce the feature set. ","1f989334":"<h2 style=\"text-align:center; color:#546545;text-shadow: 2px 2px 4px #000000;\">Descriptive Statistics<\/h2>"}}