{"cell_type":{"c0d25f45":"code","23c898be":"code","b98bcd4b":"code","a0ff56e4":"code","826f324c":"code","9165a7ee":"code","476c545f":"code","15550222":"code","ecff58d4":"code","3afbf7ff":"code","ec250547":"code","93d8815a":"code","3f09b3d2":"code","524c47e6":"code","df4d8469":"code","86cb66d3":"code","ba0a1f94":"code","54dee57e":"code","deab22f2":"code","13306ceb":"code","cd41d06f":"code","42e13d88":"code","6d338966":"code","4f5fa339":"code","0a1d798f":"code","da598046":"code","7f689c28":"code","3023cfc7":"code","a2d813b1":"code","658bd902":"code","80188fed":"code","67026898":"code","10b48d08":"code","6c197d70":"code","e5fe78a5":"code","465bd330":"markdown","715e1819":"markdown","2509a5ca":"markdown","520b5535":"markdown","d1b8d98b":"markdown","7ad71af2":"markdown","bab15016":"markdown","e81ed407":"markdown","6474e70f":"markdown","1e00abc0":"markdown","eecdbc94":"markdown","b61c5bea":"markdown","db95ad30":"markdown","47fb41f9":"markdown","7efd0fd7":"markdown","d24ee531":"markdown","f7ee8cfd":"markdown","b15daff5":"markdown","bd33baf9":"markdown","7197e8db":"markdown","bb822e3e":"markdown","2cacaa12":"markdown","e824dacc":"markdown","902a3d70":"markdown","3390b941":"markdown","9ae983e0":"markdown","c3ddf7ad":"markdown","0f707a21":"markdown","456f6304":"markdown","55275791":"markdown"},"source":{"c0d25f45":"!pip install nlp\n!pip install datasets\nimport tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom wordcloud import WordCloud\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport nlp\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import LSTM\nfrom keras.models import Sequential\nfrom keras.layers import Embedding\nfrom keras.layers import Flatten\nfrom keras.layers import Bidirectional\nfrom keras.callbacks import EarlyStopping\nfrom keras.layers import GlobalAvgPool1D\nimport random\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","23c898be":"# Importing the dataset\ndata = nlp.load_dataset('emotion')\n\n# Converting the train, validation and test datasets into DataFrame format\ntrain = pd.DataFrame(data['train'])\nvalidation = pd.DataFrame(data['validation'])\ntest = pd.DataFrame(data['test'])","b98bcd4b":"# Train dataset\ntrain.head(10)","a0ff56e4":"# Let's check the unique labels of the dataset\ntrain['label'].unique()","826f324c":"train['length_of_text'] = [len(i.split(' ')) for i in train['text']]\n\nfig = px.histogram(train['length_of_text'], marginal='box',\n                   labels={\"value\": \"Length of the Text\"})\n\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Distribution of the Length of the Texts',\n                  title_x=0.5, title_font=dict(size=22))\nfig.show()","9165a7ee":"fig = px.histogram(train['length_of_text'], marginal='box',\n                   labels={\"value\": \"Length of the Text\"},\n                   color=train['label'])\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Distribution of the Length of the Texts by Emotions',\n                  title_x=0.5, title_font=dict(size=22))\nfig.show()","476c545f":"fig = px.histogram(train, x='label', color='label')\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Distribution of the Labels',\n                  title_x=0.5, title_font=dict(size=22))\nfig.show()","15550222":"FreqOfWords = train['text'].str.split(expand=True).stack().value_counts()\nFreqOfWords_top200 = FreqOfWords[:200]\n\nfig = px.treemap(FreqOfWords_top200, path=[FreqOfWords_top200.index], values=0)\nfig.update_layout(title_text='Frequency of the Words in the Train Dataset',\n                  title_x=0.5, title_font=dict(size=22)\n                  )\nfig.update_traces(textinfo=\"label+value\")\nfig.show()","ecff58d4":"def tokenization(inputs):\n    return word_tokenize(inputs) #REFERENCE[1]\n\n\ntrain['text_tokenized'] = train['text'].apply(tokenization)\nvalidation['text_tokenized'] = validation['text'].apply(tokenization)","3afbf7ff":"train.head()","ec250547":"stop_words = set(stopwords.words('english'))\n\ndef stopwords_remove(inputs):\n    return [item for item in inputs if item not in stop_words]\n\ntrain['text_stop'] = train['text_tokenized'].apply(stopwords_remove)\nvalidation['text_stop'] = validation['text_tokenized'].apply(stopwords_remove)\n\ntrain.head()","93d8815a":"lemmatizer = WordNetLemmatizer()\n\ndef lemmatization(inputs):\n    return [lemmatizer.lemmatize(word=x, pos='v') for x in inputs]\n\ntrain['text_lemmatized'] = train['text_stop'].apply(lemmatization)\nvalidation['text_lemmatized'] = validation['text_stop'].apply(lemmatization)\n\ntrain.head()","3f09b3d2":"train['text_cleaned'] = train['text_lemmatized'].str.join(' ')\nvalidation['text_cleaned'] = validation['text_lemmatized'].str.join(' ')\n\ntrain.head() # Final form of the dataset","524c47e6":"WordCloud = WordCloud(max_words=100,\n                      random_state=30,\n                      collocations=True).generate(str((train['text_cleaned'])))\n\nplt.figure(figsize=(15, 8))\nplt.imshow(WordCloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","df4d8469":"num_words = 10000\ntokenizer = Tokenizer(num_words=num_words, oov_token='<OOV>')\ntokenizer.fit_on_texts(train['text_cleaned'])\n\nword_index = tokenizer.word_index \n# print(word_index) ","86cb66d3":"Tokenized_train = tokenizer.texts_to_sequences(train['text_cleaned'])\nTokenized_val = tokenizer.texts_to_sequences(validation['text_cleaned'])","ba0a1f94":"print('Non-tokenized Version: ', train['text_cleaned'][0])\nprint('Tokenized Version: ', tokenizer.texts_to_sequences([train['text_cleaned'][0]]))\nprint('--'*50)\nprint('Non-tokenized Version: ', train['text_cleaned'][10])\nprint('Tokenized Version: ', tokenizer.texts_to_sequences([train['text_cleaned'][10]]))\nprint('--'*50)\nprint('Non-tokenized Version: ', train['text'][100])\nprint('Tokenized Version: ', tokenizer.texts_to_sequences([train['text_cleaned'][100]]))","54dee57e":"maxlen = 40\nPadded_train = pad_sequences(Tokenized_train, maxlen=maxlen, padding='pre')\nPadded_val = pad_sequences(Tokenized_val, maxlen=maxlen, padding='pre')\n\nprint('Non-padded Version: ', tokenizer.texts_to_sequences([train['text_cleaned'][0]]))\nprint('Padded Version: ', Padded_train[0])\nprint('--'*50)\nprint('Non-padded Version: ', tokenizer.texts_to_sequences([train['text_cleaned'][10]]))\nprint('Padded Version: ', Padded_train[10])\n","deab22f2":"model = Sequential()\n\nmodel.add(Embedding(num_words, 16, input_length=maxlen))\nmodel.add(GlobalAvgPool1D())\n\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, activation='relu'))\nmodel.add(Dropout(0.3))\n\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, activation='relu', return_sequences=True))\nmodel.add(Dropout(0.3))\n\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(40, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(6, activation='softmax'))\n\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","13306ceb":"# Replacing the string labels with integers\nlabel_ = {\"sadness\": 0, \"joy\": 1, \"love\": 2, \"anger\": 3, \"fear\": 4, \"surprise\": 5}\ntrain['label'] = train['label'].replace(label_)\nvalidation['label'] = validation['label'].replace(label_)\n\ntrain.head()","cd41d06f":"early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='auto', patience=5,\n                                                 restore_best_weights=True)\n\nepochs = 100\nhist = model.fit(Padded_train, train['label'], epochs=epochs,\n                 validation_data=(Padded_val, validation['label']), \n                 callbacks=[early_stopping])","42e13d88":"plt.figure(figsize=(15, 8))\nplt.plot(hist.history['loss'], label='Train Loss')\nplt.plot(hist.history['val_loss'], label='Validation Loss')\nplt.title('Train and Validation Loss Graphs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()","6d338966":"test['text_tokenized'] = test['text'].apply(tokenization)\ntest['text_stop'] = test['text_tokenized'].apply(stopwords_remove)\ntest['text_lemmatized'] = test['text_stop'].apply(lemmatization)\ntest['text_cleaned'] = test['text_lemmatized'].str.join(' ')\n\nTokenized_test = tokenizer.texts_to_sequences(test['text_cleaned'])\nPadded_test = pad_sequences(Tokenized_test, maxlen=maxlen, padding='pre')\n\ntest['label'] = test['label'].replace(label_)\n\ntest_evaluate = model.evaluate(Padded_test, test['label'])","4f5fa339":"test.head()","0a1d798f":"def make_predictions(text_input):\n    text_input = str(text_input)\n    text_input = tokenization(text_input)\n    text_input = stopwords_remove(text_input)\n    text_input = lemmatization(text_input)\n    text_input = ' '.join(text_input)\n    text_input = tokenizer.texts_to_sequences([text_input])\n    text_input = pad_sequences(text_input, maxlen=maxlen, padding='pre')\n    text_input = np.argmax(model.predict(text_input))\n    \n    if text_input == 0:\n        print('Predicted Emotion: Sadness')\n    elif text_input == 1:\n        print('Predicted Emotion: Joy')\n    elif text_input == 2:\n        print('Predicted Emotion: Love')\n    elif text_input == 3:\n        print('Predicted Emotion: Anger')\n    elif text_input == 4:\n        print('Predicted Emotion: Fear')\n    else:\n        print('Predicted Emotion: Surprise')\n    return text_input\n\nlabel_ = {0: \"Sadness\", 1: \"Joy\", 2: \"Love\", 3: \"Anger\", 4: \"Fear\", 5: \"Surprise\"}\ntest['label'] = test['label'].replace(label_)\n\n# Randomly chosen Test Dataset data points\ni = random.randint(0, len(test) - 1)\n\nprint('Test Text:', test['text'][i])\nprint(' ')\nprint('Actual Emotion:', test['label'][i])\nmake_predictions(test['text'][i])\nprint('-'*50)\nprint('Test Text:', test['text'][i+1])\nprint(' ')\nprint('Actual Emotion:', test['label'][i+1])\nmake_predictions(test['text'][i+1])","da598046":"from sklearn.metrics import confusion_matrix\n\nlabel_ = {\"Sadness\": 0, \"Joy\": 1, \"Love\": 2, \"Anger\": 3, \"Fear\": 4, \"Surprise\": 5}\ntest['label'] = test['label'].replace(label_)\n\npred = model.predict_classes(Padded_test)\nplt.figure(figsize=(15, 8))\nconf_mat = confusion_matrix(test['label'].values, pred)\nconf_mat = pd.DataFrame(conf_mat, columns=np.unique(test['label']), index=np.unique(pred))\nconf_mat.index.name = 'Actual'\nconf_mat.columns.name = 'Predicted'\nsns.heatmap(conf_mat, annot=True, fmt='g')\nplt.title('Confusion Matrix of the Test Data', fontsize=14)\nplt.show()","7f689c28":"make_predictions('No one told you when to run, you missed the starting gun')","3023cfc7":"make_predictions(\"I just asked one question to confirm his request, and my boss bit my head off.\")","a2d813b1":"make_predictions(\"She\u2019s flying high after the successful product launch.\")","658bd902":"make_predictions(\"I\u2019m going to have the first meeting with a big client tomorrow, and I\u2019m feeling butterflies in my stomach\")","80188fed":"make_predictions(\"Sometimes the people who appear to be the most confident are actually afraid of their own shadows.\")","67026898":"make_predictions(\"I'm really impressed that Ashley can speak 7 languages, whereas I only speak one!\")","10b48d08":"make_predictions(\"Grandpa was very proud of me when I got a promotion at work. He took me out to dinner to celebrate.\")","6c197d70":"make_predictions(\"We are delighted that you will be coming to visit us. It will be so nice to have you here.\")","e5fe78a5":"make_predictions(\"I am anxious to hear back about the job interview I had on Friday. I hope I get the job!\")","465bd330":"# Joining Tokens into Sentences\n","715e1819":"**The length of the data points is distributed between 4 to 46. The outliers start from 48 words.**","2509a5ca":"**According to the first 5 rows of the train dataset, it is obvious that we achieved our goal.**","520b5535":"# Training the Model\n","d1b8d98b":"# Tokenizing with NLTK\n","7ad71af2":"# Making Predictions in the Test Data\n","bab15016":"**According to graph above, the most frequent words include stopwords such as \"i\", \"and\", \"to\", etc. For the further steps, I will remove them.**","e81ed407":"# Preparing the Test Data\n","6474e70f":"# Train and Validation Loss Graphs\n","1e00abc0":"# References\nhttps:\/\/medium.com\/analytics-vidhya\/text-preprocessing-for-nlp-natural-language-processing-beginners-to-master-fd82dfecf95\n\nhttps:\/\/www.coursera.org\/learn\/tweet-emotion-tensorflow\n\nhttps:\/\/www.kaggle.com\/rmisra\/news-headlines-dataset-for-sarcasm-detection\n\nhttps:\/\/www.geeksforgeeks.org\/python-lemmatization-with-nltk\/","eecdbc94":"# Tokenizing with Tensorflow\n","b61c5bea":"**By using tokenization, I split each data point into words. Tokenization is one of the key steps for NLP applications.**","db95ad30":"# Creating the Model\n","47fb41f9":"# <center>HuggingFace Emotion Recognition using DNN","7efd0fd7":"# Distribution of the Length of the Texts by Emotions\n","d24ee531":"# WordCloud of the Cleaned Dataset\n","f7ee8cfd":"**Some Tokenziation Examples from the Dataset**","b15daff5":"# Importing the Libraries","bd33baf9":"# Lemmatization","7197e8db":"# Importing the Dataset\n\n**Detailed dataset info: https:\/\/huggingface.co\/datasets\/emotion**","bb822e3e":"# Stopwords Removal\n","2cacaa12":"**Hello everyone,**\n\n**This notebook aims to classify the 5 different emotions (sadness, anger, love, surprise, fear, joy) using Deep Neural Networks (DNN).**\n\n**To accomplish our aim, I will visualize the dataset for the first step. And then, I will clear the dataset using NLP techniques. For the final step, I will perform LSTM machine learning algorithm to classify the 5 different emotions.**\n\n**This is my first NLP project and, I am open to your feedback and suggestions, feel free to comment your feedback and suggestions on the comment section or contact me.**\n\n**Thank you and let's get started!**","e824dacc":"# Frequency of the Words in the Train Dataset\n","902a3d70":"# Having Fun with the Model","3390b941":"# Confusion Matrix of the Test Data","9ae983e0":"# Distribution of the Length of the Texts","c3ddf7ad":"# Padding","0f707a21":"**As we have seen from the Frequency of the Words in the Train Dataset visualization, the most frequent words were the English stopwords such as \"i\", \"you\", \"their\", \"to\", etc. In this step, we will remove these words from the entire dataset by using the NLTK library.**","456f6304":"# Distribution of the Labels\n","55275791":"**Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word [2].** "}}