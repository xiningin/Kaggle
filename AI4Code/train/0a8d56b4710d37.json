{"cell_type":{"340e3e84":"code","c4bdf932":"code","33d302de":"code","f2c550b2":"code","31a812eb":"code","b70dd93e":"code","d996d746":"code","b27c795a":"code","c5897aa8":"code","75ceccea":"code","64965787":"code","6c2b423f":"code","52441284":"code","63eb80ed":"code","d308f397":"code","e35e1ce0":"code","dc5a0f98":"code","cc6f91cf":"code","9e23fad0":"code","10dc4ba6":"code","7d64b615":"code","442d0406":"code","aab80947":"code","9582ee89":"code","3396cc89":"code","2471d83c":"code","0506374b":"code","2f1a8da0":"markdown","e1a670d6":"markdown","b54b22f7":"markdown","2ade844b":"markdown","278f307d":"markdown","d9641c11":"markdown","7ee58fc0":"markdown","27c2d73d":"markdown","342c0b6d":"markdown","3384641d":"markdown","578b6142":"markdown","d458aa77":"markdown","eec0c1a7":"markdown","35a8cc18":"markdown","2d16df43":"markdown","6d0ba10e":"markdown","a631a180":"markdown","412bd0bb":"markdown","4ed4253e":"markdown"},"source":{"340e3e84":"!rm -rf utils.py\n!wget https:\/\/raw.githubusercontent.com\/sevenfx\/fastai_audio\/master\/notebooks\/utils.py","c4bdf932":"%matplotlib inline\nimport os\nfrom pathlib import Path\nfrom IPython.display import Audio\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom utils import read_file, transform_path","33d302de":"!rm -rf free-spoken-digit-dataset-master master.zip\n!wget https:\/\/github.com\/Jakobovski\/free-spoken-digit-dataset\/archive\/master.zip\n!unzip -q master.zip\n!rm -rf master.zip\n!ls","f2c550b2":"AUDIO_DIR = Path('free-spoken-digit-dataset-master\/recordings')\nIMG_DIR = Path('imgs')\n!mkdir {IMG_DIR} -p","31a812eb":"fnames = os.listdir(str(AUDIO_DIR))\nlen(fnames), fnames[:5]","b70dd93e":"fn = fnames[94]\nprint(fn)\nAudio(str(AUDIO_DIR\/fn))","d996d746":"# ??read_file","b27c795a":"x, sr = read_file(fn, AUDIO_DIR)\nx.shape, sr, x.dtype","c5897aa8":"def log_mel_spec_tfm(fname, src_path, dst_path):\n    x, sample_rate = read_file(fname, src_path)\n    \n    n_fft = 1024\n    hop_length = 256\n    n_mels = 40\n    fmin = 20\n    fmax = sample_rate \/ 2 \n    \n    mel_spec_power = librosa.feature.melspectrogram(x, sr=sample_rate, n_fft=n_fft, \n                                                    hop_length=hop_length, \n                                                    n_mels=n_mels, power=2.0, \n                                                    fmin=fmin, fmax=fmax)\n    mel_spec_db = librosa.power_to_db(mel_spec_power, ref=np.max)\n    dst_fname = dst_path \/ (fname[:-4] + '.png')\n    plt.imsave(dst_fname, mel_spec_db)","75ceccea":"log_mel_spec_tfm(fn, AUDIO_DIR, IMG_DIR)\nimg = plt.imread(str(IMG_DIR\/(fn[:-4] + '.png')))\nplt.imshow(img, origin='lower');","64965787":"transform_path(AUDIO_DIR, IMG_DIR, log_mel_spec_tfm, fnames=fnames, delete=True)","6c2b423f":"os.listdir(str(IMG_DIR))[:10]","52441284":"import fastai\nfastai.__version__","63eb80ed":"from fastai.vision import *","d308f397":"digit_pattern = r'(\\d+)_\\w+_\\d+.png$'","e35e1ce0":"data = (ImageList.from_folder(IMG_DIR)\n        .split_by_rand_pct(0.2)\n        #.split_by_valid_func(lambda fname: 'nicolas' in str(fname))\n        .label_from_re(digit_pattern)\n        .transform(size=(128,64))\n        .databunch())\ndata.c, data.classes","dc5a0f98":"# Shape of batch\nxs, ys = data.one_batch()\nxs.shape, ys.shape","cc6f91cf":"# Stats\nxs.min(), xs.max(), xs.mean(), xs.std()","9e23fad0":"# Sample batch\ndata.show_batch(4, figsize=(5,9), hide_axis=False)","10dc4ba6":"learn = cnn_learner(data, models.resnet18, metrics=accuracy)","7d64b615":"learn.fit_one_cycle(4)","442d0406":"learn.unfreeze()\nlearn.fit_one_cycle(4)","aab80947":"interp = ClassificationInterpretation.from_learner(learn)","9582ee89":"interp.plot_confusion_matrix(figsize=(10, 10), dpi=60)","3396cc89":"# Clean up (Kaggle)\n# !rm -rf {AUDIO_DIR}\n# !rm -rf {IMG_DIR}","2471d83c":"import jovian","0506374b":"jovian.commit()","2f1a8da0":"Now we're ready to define and train the model. We'll use the ResNet18 architecture.","e1a670d6":"We can import the necessary modules and functions","b54b22f7":"Now we can apply the `log_mel_spec_tfm` transformation to the entire dataset.","2ade844b":"It's looking pretty good. Let's look at the confusion matrix.","278f307d":"# Audio Classification using FastAI\n\nUse the 'Clone' button if you want to run this notebook on a local\/cloud machine, or use the 'Run' button to run it on BinderHub or Kaggle.\n\nSource: https:\/\/github.com\/sevenfx\/fastai_audio","d9641c11":"We can use the `read_file` helper function to read the audio file and do some preprocessing","7ee58fc0":"Let's see how many recordings we have, and some sample files.","27c2d73d":"The label can be extracted using a regular expression, and for the validation set, we'll pick all the recordings of one of the speakers.","342c0b6d":"Let's unfreeze and train some more.","3384641d":"As before we can play the recording using the `Audio` widget.","578b6142":"## Image Classifier","d458aa77":"Next, let's define a function to convert it into a mel spectrogram, and save the resulting image as a PNG file.","eec0c1a7":"We start by finetuning the classification layers.","35a8cc18":"## Save & Commit","2d16df43":"To begin, we download a bunch of [utility functions](https:\/\/github.com\/sevenfx\/fastai_audio\/blob\/master\/notebooks\/utils.py) for I\/O and conversion of audio files to spectrogram images.","6d0ba10e":"Let's see what a batch of data looks liks.","a631a180":"Here's an example audio file convertered to PNG.","412bd0bb":"Next, let's download the data. We'll use the free spoken digits database: https:\/\/github.com\/Jakobovski\/free-spoken-digit-dataset","4ed4253e":"From this point onwards, we use a standard FastAI CNN image classifier."}}