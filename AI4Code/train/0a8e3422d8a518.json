{"cell_type":{"e8ba5f45":"code","504a7ffa":"code","2d242da2":"code","ef045f16":"code","912178a7":"code","173829e6":"code","35dca815":"code","64397a39":"code","08f0e77d":"code","bb84a1b6":"code","1024eb6f":"code","7112efbd":"code","ce841b36":"code","ea1aaa3e":"code","4d13f3a0":"markdown","59774be9":"markdown","a412907a":"markdown","426e4cbe":"markdown","472df93c":"markdown","c96a1c93":"markdown","e7f8b12c":"markdown","daa9d3e7":"markdown","98309a37":"markdown","16b8642b":"markdown","4667e36a":"markdown","a902a1a1":"markdown","fac4df40":"markdown","eafe11a0":"markdown","5282e88d":"markdown","80212a22":"markdown","d5d1fe19":"markdown"},"source":{"e8ba5f45":"import os\nimport sys\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom scipy.spatial import distance\nfrom matplotlib import pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV, KFold, train_test_split\n\n\nwarnings.simplefilter('ignore')\nrandom_seed = 1","504a7ffa":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        path = (os.path.join(dirname, filename))\ncl = [\n    'relative_compactness', 'surface_area', 'wall_area', 'roof_area',\n    'overall_height', 'orientation', 'glazing_area',\n    'glazing_area_distribution', 'heating_load', 'cooling_load'\n]\ndata = pd.read_csv(path,\n                   names=cl,\n                   header=None)\n\ndata = data.drop(index=0, inplace=False)\nX = data.drop(['heating_load', 'cooling_load'], axis=1).values\nprint('X shape:', X.shape)","2d242da2":"y_cooling = ((data['cooling_load']).values).reshape(-1, 1)\ny_heating = ((data['heating_load']).values).reshape(-1, 1)\n\nscaler = StandardScaler()\ny_cooling = scaler.fit_transform(y_cooling)\ny_heating = scaler.fit_transform(y_heating)\n\n# Normalize the outputs to remove the relationship and bring all ranges of the data.\n\nx_train, x_test, y_train_h, y_test_h = train_test_split(X,\n                                                        y_heating,\n                                                        test_size=0.3,\n                                                        random_state=random_seed)\n\nx_train, x_test, y_train_c, y_test_c = train_test_split(X,\n                                                        y_cooling,\n                                                        test_size=0.3,\n                                                        random_state=random_seed)","ef045f16":"model = GradientBoostingRegressor(learning_rate=0.1,\n                                  subsample=1,\n                                  max_features=\"sqrt\",\n                                  loss='ls',\n                                  n_estimators=100,\n                                  max_depth=3,\n                                  random_state=random_seed)\n\nmodel_c = model.fit(x_train, y_train_c)\nmodel_h = model.fit(x_train, y_train_h)\npred_c = model_c.predict(x_test)\npred_h = model_h.predict(x_test)\n\ny_test = np.column_stack([y_test_h, y_test_c])\npred = np.column_stack([pred_h, pred_c])","912178a7":"output_errors = np.average((y_test - pred)**2, axis=0)\nrmse = np.sqrt(output_errors)\nprint('RMSE for both outputs:', rmse)","173829e6":"np.mean(np.sqrt(np.power(y_test - pred, 2).sum(axis=1)))","35dca815":"def ProgressBar(percent, barLen=20):\n    sys.stdout.write(\"\\r\")\n    progress = \"\"\n    for i in range(barLen):\n        if i < int(barLen * percent):\n            progress += \"=\"\n        else:\n            progress += \" \"\n    sys.stdout.write(\"[ %s ] %.2f%%\" % (progress, percent * 100))\n    sys.stdout.flush()\n\n\ndef gridsearch(X, y, model, grid,\n               random_state=None,\n               n_cv_general=5,\n               n_cv_intrain=5):\n\n    pred = np.zeros_like(np.squeeze(y))\n\n    kfold_gen = KFold(n_splits=n_cv_general,\n                      random_state=random_state,\n                      shuffle=True)\n\n    for cv_i, (train_index, test_index) in enumerate(kfold_gen.split(X, y)):\n        x_train, x_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n\n        kfold = KFold(n_splits=n_cv_intrain,\n                      random_state=random_state,\n                      shuffle=True)\n\n        grid_search = GridSearchCV(model, grid,\n                                   cv=kfold,\n                                   scoring='neg_root_mean_squared_error',\n                                   refit=True,\n                                   return_train_score=False,\n                                   )\n\n        grid_search.fit(x_train, y_train)\n\n        pred[test_index] = grid_search.predict(x_test)\n\n        ProgressBar(cv_i\/abs((n_cv_general)-1), barLen=n_cv_general)\n\n    return pred","64397a39":"model = GradientBoostingRegressor(learning_rate=0.1,\n                                  subsample=1,\n                                  max_features=\"sqrt\",\n                                  loss='ls',\n                                  n_estimators=100,\n                                  max_depth=3,\n                                  random_state=random_seed)\n\nparam_grid = {\"max_depth\": [2, 5, 10, 20],\n              \"learning_rate\": [0.025, 0.05, 0.1, 0.5, 1],\n              \"subsample\": [0.75, 0.5, 1]}\n\npred_c = gridsearch(X=X, y=y_cooling,\n                    model=model,\n                    grid=param_grid,\n                    random_state=random_seed)\n\npred_h = gridsearch(X=X, y=y_heating,\n                    model=model,\n                    grid=param_grid,\n                    random_state=random_seed)\n\ny_test = np.column_stack([y_heating, y_cooling])\npred = np.column_stack([pred_h, pred_c])","08f0e77d":"output_errors = np.average((y_test - pred)**2, axis=0)\nrmse_gbdt = np.sqrt(output_errors)\nprint('RMSE for both outputs:', rmse_gbdt)","bb84a1b6":"np.mean(np.sqrt(np.power(y_test - pred, 2).sum(axis=1)))","1024eb6f":"y = ((data[['cooling_load', 'heating_load']])).values\ny = scaler.fit_transform(y)\n\nparam_grid = {\"n_jobs\": [1, 2]}\n\nMulti = MultiOutputRegressor(model)\n\npred_Multi = gridsearch(X=X, y=y,\n                        model=Multi,\n                        grid=param_grid,\n                        random_state=random_seed)","7112efbd":"output_errors = np.average((y - pred_Multi)**2, axis=0)\nrmse_multi = np.sqrt(output_errors)\nprint('RMSE for both outputs:', rmse_multi)","ce841b36":"def plot(titles, value, label):\n    bar = plt.bar(titles, value, label=label)\n    plt.title('single output model')\n    plt.xlabel('Output')\n    plt.ylabel('RMSE')\n    plt.legend()\n    \n    for i in bar:\n        height = i.get_height()\n        plt.annotate('{:.02f}'.format(height),\n              xy=(i.get_x() + i.get_width() \/ 2, height),\n              xytext=(0, 3), # 3 points vertical offset\n              textcoords=\"offset points\",\n              ha='center', va='bottom')","ea1aaa3e":"titles=['heating', 'cooling']\nplot(titles, rmse_multi, label='MultiOutPutModel')\nplot(titles, rmse_gbdt, label='SingleModel')","4d13f3a0":"#### [Back to top](#index)","59774be9":"### Split the dataset and preprocess the inputs and outputs","a412907a":"## Euclidean distance","426e4cbe":"#### [Back to top](#index)","472df93c":"### RMSE","c96a1c93":"<a id='Comparison'><\/a>\n# Comparison","e7f8b12c":"## Euclidean distance","daa9d3e7":"<a id='2nd_modeling'><\/a>\n\n# 2st Model selection\n\nThe difference between the 2nd part and the previous is that in this one I tunned the hyperparameters and used the cross-validation to split the data.","98309a37":"<a id='Training2'><\/a>\n## Training","16b8642b":"<a id='Evaluation2'><\/a>\n## Evaluation\n\n### RMSE","4667e36a":"<a id='Evaluation1'><\/a>\n# Model evaluation\n\n## RMSE","a902a1a1":"<a id='model1'> <\/a>\n\n# 1st Model selection\n\nTrain the model with train_test split.\n\nIn the following, I trained the model for both outputs (cooling and heating) separately.","fac4df40":"<a id='index'><\/a>\n# Table of contents\n\n* [Dataset](#dataset)\n* [1st Model selection](#model1)\n    * [Training](#Training1)\n    * [Evaluation](#Evaluation1)\n* [2nd Model selection](#2nd_modeling)\n    * [Training](#Training2)\n    * [Evaluation](#Evaluation2)\n* [MultiOutputRegressor](#MultiOutputRegressor)\n* [Comparison](#Comparison)\n\n### Author: [Seyedsaman Emami](https:\/\/github.com\/samanemami)\n\nIf you want to have this method or use the outputs of the notebook, you can fork the Notebook as following (copy and Edit Kernel).\n\n<img src=\"https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1101107%2F8187a9b84c9dde4921900f794c6c6ff9%2FScreenshot%202020-06-28%20at%201.51.53%20AM.png?generation=1593289404499991&alt=media\" alt=\"Copyandedit\" width=\"300\" height=\"300\" class=\"center\">\n\nI tried to keep everything as simple as possible, if you have any doubts or questions, please feel free to ask in the comments.","eafe11a0":"# Author: Seyedsaman Emami\n\n| **Author** | **GitHub** | \n| :-- | :-- | \n| Seyedsaman Emami | [GitHub profile](https:\/\/github.com\/samanemami) |\n\n<hr>\n\n## Abouth this NoteBook\n\nIn the following notebook, I considered a multivariate regression problem with two different regression target labels.\n\nThe model for the prediction is the Gradient Boosting Regressor.\n\nI trained the model in two different approaches in terms of data splitting (train\/test split and cross-validation).\n\nI considered the tunning h-params as well.\n\nThe focus of this notebook is to evaluate the model performance with other metrics and approaches. So, I considered the Euclidean distance between two targets for the performance report.In the following notebook, I considered a multivariate regression problem with two different regression target labels.\n\nThe model for the prediction is the Gradient Boosting Regressor.\n\nI trained the model in two different approaches in terms of data splitting (train\/test split and cross-validation).\n\nI considered the tunning h-params as well.\n\nThe focus of this notebook is to evaluate the model performance with other metrics and approaches. So, I considered the Euclidean distance between two targets for the performance report.","5282e88d":"<a id='Training1'><\/a>\n## Training the model","80212a22":"<a id='MultiOutputRegressor'><\/a>\n# MultiOutputRegressor\nfitting one regressor per target in one model","d5d1fe19":"<a id='dataset'><\/a>\n\n# Dataset"}}