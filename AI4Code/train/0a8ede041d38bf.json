{"cell_type":{"73758195":"code","fca6cbc6":"code","8b2c18eb":"code","ef8eabdd":"code","21b29d45":"code","28f79da1":"code","23d430dd":"code","003d4465":"code","646bedf9":"code","627a23ad":"code","47ac9792":"code","1843258f":"code","badc700d":"code","4556436e":"code","cb251447":"code","9508cb57":"code","9d526e06":"code","6684eb46":"code","2cf8cd67":"code","c1ee652c":"code","28361894":"code","c78c8602":"code","0b7e5ba2":"code","449ad49c":"code","e60ef830":"code","c120b2cc":"code","40fde879":"code","4fa488de":"code","87b6aa4a":"code","6c903356":"code","81d77eaa":"code","30000e40":"code","4bb1776d":"code","5588bc99":"code","b6deaed6":"code","8e549138":"code","f178ecc9":"code","e7c4d7d1":"code","2ebcce60":"markdown","6201ce65":"markdown","a931babc":"markdown","27e51a30":"markdown","1382c3ed":"markdown","09a4500b":"markdown","8bfc1e17":"markdown","5652e8e2":"markdown","6d5a204a":"markdown","b0094012":"markdown","d119352c":"markdown","bb283c6b":"markdown","a3e6a51a":"markdown","9074988a":"markdown","6739cc56":"markdown","606996d5":"markdown","29d69853":"markdown","532d3453":"markdown","7f5a430a":"markdown","65fd5de1":"markdown"},"source":{"73758195":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nimport io\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport sklearn.ensemble as skens","fca6cbc6":"# #read in files\n# train = files.upload()\n# test = files.upload()\n\n# Store data in a Pandas Dataframe\n# df_train_csv = pd.read_csv(io.BytesIO(train['train.csv']))\n# df_test_csv = pd.read_csv(io.BytesIO(test['test.csv']))\n\ndf_train_csv = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test_csv = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\n#drop ID column\ndf_train = df_train_csv.drop(['Id'], axis=1)\ndf_test =  df_test_csv.drop(['Id'], axis=1)","8b2c18eb":"#Check the distribution of salesprice\nsns.distplot(df_train['SalePrice']).set_title('Sale Price Distribution Training Dataset');","ef8eabdd":"#Remove the skew of salesprice by taking the log of the salesprice\ndf_train['logSalePrice'] = np.log(df_train['SalePrice'])\ndf_train['GrLivArea'] = np.log(df_train['GrLivArea']) # temp, remove\n\ntarget = df_train['SalePrice']\ntarget_log = df_train['logSalePrice']\nsns.distplot(df_train['logSalePrice']).set_title('Log Sale Price Distribution Training Dataset');","21b29d45":"#combine train and test data\ndf = df_train_csv.append(df_test_csv, ignore_index = True, sort=False)","28f79da1":"#check the shape of dataframe\ndf.shape","23d430dd":"#drop entire column if missing more than 10 percent of values\npercent_missing = df.isnull().sum() * 100 \/ len(df)\nmissing_value_df = pd.DataFrame({'column_name': df.columns,\n                                 'percent_missing': percent_missing})\ndropped_columns = []\nfor row in missing_value_df.values:\n    if row[1] > 10 and row[0] != 'SalePrice':\n        dropped_columns.append(row[0])\n# dropped_columns.delete('SalePrice')\ndf = df.drop(dropped_columns,axis=1)","003d4465":"#checking number of columns after dropping columns with many NaN values\ndf.shape","646bedf9":"#checking for NaN values in remaining columns\nna = df.isnull().sum()\nna[na>0]","627a23ad":"# Converting the numerical columns that should be categorical into type objects\ndf['MSSubClass'] = df['MSSubClass'].astype(str)\ndf['YrSold'] = df['YrSold'].astype(str)\ndf['MoSold'] = df['MoSold'].astype(str)","47ac9792":"df.dtypes.to_frame().reset_index()[0].value_counts()","1843258f":"#separate into continuous and categorical columns\ncontinuous_cols = []\nfor col in df.columns.values:\n    if df[col].dtype != 'object':\n        continuous_cols.append(col)\ncontinuous_cols\ndf_cont = df[continuous_cols]\ndf_cat = df.drop(continuous_cols,axis=1)","badc700d":"#impute missing values with median \nimp_mean = SimpleImputer(missing_values=np.nan, strategy='median')\nimputed_cont = imp_mean.fit_transform(df_cont)\n#convert output into df\nnewlst=[]\nfor array in imputed_cont:\n  newlst.append(array)\nimputed_df= pd.DataFrame(newlst)\n#add column names\nlabels = df_cont.columns.tolist()\nimputed_df.columns = labels\ndf_cont = imputed_df","4556436e":"#impute missing values with most frequent value\nfrom sklearn.impute import SimpleImputer\ndf_cat.fillna(0, inplace=True)\nimp_mean = SimpleImputer(missing_values=0, strategy='most_frequent')\nimp_mean.fit(df_cat)\nimputed_cat = imp_mean.transform(df_cat)\n#'get column labels\nlst = []\nfor array in imputed_cat:\n    lst.append(array)\nimputed_df = pd.DataFrame(lst)\nlabels = df_cat.columns.tolist()\nimputed_df.columns = labels\n# Make a copy without dummies for viz after random forest\ndf_cat_original = df_cat.copy()","cb251447":"#manual hot encoding\ndummies = pd.get_dummies(imputed_df)\ndf_cat = dummies","9508cb57":"# joining the two dataframes\ndf_cleaned = df_cont.join(df_cat)","9d526e06":"# adding the log prices to the training data frame\ndf_train = df_cleaned.iloc[:len(df_train)]\ndf_train = df_train.join(target_log)","6684eb46":"# splitting the test and train data\ndf_test = df_cleaned.iloc[len(df_train):]\n\nX_train = df_train.drop(['SalePrice','logSalePrice'], axis=1)\ny_train = df_train['logSalePrice']\n\nX_test = df_test.drop(['SalePrice'], axis=1)","2cf8cd67":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\nX = X_train\ny = y_train\nX_train_t, X_test_t, y_train_t, y_test_t = train_test_split(X, y, test_size=0.25, random_state=42)","c1ee652c":"def train_model(params, X_train, y_train):\n    rf_model = RandomForestRegressor(**params)\n    rf_model.fit(X_train, y_train)\n    return rf_model","28361894":"def viz_results(model, X_test, y_test = None, score = None):\n    score = round(model.score(X_test, y_test)*100,2) if score else \"unknown\"\n    predicted_labels = model.predict(X_test)\n    unlog_lables = np.exp(predicted_labels)\n    title = f'Predicted Sale Price Distribution (score: {score} %)'\n    sns.distplot(unlog_lables).set_title(title);","c78c8602":"def viz_feature_importance(model, df_train, n_features):\n    feat_importance = model.feature_importances_\n    df_feat_importance = pd.DataFrame({'Feature Importance':feat_importance},\n                index=df_train.columns[:len(df_train.columns)])\n\n    df_feat_imp_sorted = df_feat_importance.sort_values(by='Feature Importance', ascending=False)\n    df_feat_imp_sorted.iloc[:n_features].plot(kind='barh')","0b7e5ba2":"rf_model = train_model({'n_estimators': 5, 'max_depth': 10}, X_train_t, y_train_t)\nviz_results(rf_model, X_test_t, y_test_t, score = True)","449ad49c":"viz_feature_importance(rf_model, X, 10)","e60ef830":"from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# grid of possible hyperparameters\nparam_grid = {\n              'n_estimators': [200, 1200, 1500, 2000],\n              'bootstrap': [False],  \n              'max_features': [70],\n              'max_depth': [15, 30, 40, 50, 100], \n              'min_samples_split': [2, 5],\n              'min_samples_leaf': [1, 5]\n             }\nrf = RandomForestRegressor()\n\n# We chose two options for finding the best hyperparameters: Grid Search and Random Search\n# running the Grid Search\n# clf_search = GridSearchCV(rf, \n#                                      param_grid, \n#                                      cv=3, \n#                                      verbose=10, \n#                                      n_jobs = -1,\n#                                      )\n# clf_search.fit(X_train_t, y_train_t)\n# best_params_search = clf_search.best_params_\n\n# TODO(vanya): make the params continious, np.instead of a list\n# running the Randomized Search\nclf_rand = RandomizedSearchCV(estimator = rf, \n                               param_distributions = param_grid, \n                               n_iter = 5, \n                               cv = 3, \n                               verbose=10, \n                               random_state=42, \n                               n_jobs = -1)\nclf_rand.fit(X_train_t, y_train_t)\nclf_rand.best_estimator_\nbest_params_rand = clf_rand.best_params_\n\n# print(clf_search.best_estimator_)\n# print(best_params_search)\n# print(best_params_rand)","c120b2cc":"best_params = best_params_rand\nbest_params","40fde879":"rf_model = train_model(best_params, X_train_t, y_train_t)\nrf_model.score(X_test_t, y_test_t)\nviz_results(rf_model,X_test_t, y_test_t, score=True)","4fa488de":"# Perform K-Fold CV\nscores = cross_val_score(rf_model, X, y, cv=5)\nprint(f'Accuracy: {scores.mean():.2f} (+\/- {scores.std()*2:.2f})')","87b6aa4a":"rf_model = train_model(best_params, X_train, y_train)\nviz_results(rf_model,X_test)\n\npredicted_labels = rf_model.predict(X_test)\nunlog_lables = np.exp(predicted_labels)","6c903356":"viz_feature_importance(rf_model, X_test, 10)","81d77eaa":"unlog_lables = pd.Series(unlog_lables)\nsubmission = df_test_csv['Id']\nsubmission = submission.to_frame()\nsubmission['SalePrice'] = unlog_lables\nsubmission.to_csv('submission.csv', index=False)","30000e40":"# Cut prices into 4 buckets by quartile (low, middle 1, middle 2, high)\ndf_cat_original['Price_Cat'] = pd.qcut(df_train['SalePrice'], 4, labels=[\"low\", \"middle1\", \"middle2\", \"high\"])\n\n# Drop houses in the middle price range\ndf_cat_cleaned = df_cat_original[(df_cat_original['Price_Cat'] =='low') | (df_cat_original['Price_Cat'] =='high' ) ]\n\n# Drop any categorical variables with only one category\n# Variables with only one category would be a meaningless predictor\nnunique = df_cat_cleaned.nunique()\ndf_cat_cleaned = df_cat_cleaned.drop( columns = nunique[nunique<=1].index.values )","4bb1776d":"df_cat_cleaned.head()","5588bc99":"from matplotlib import rc\nfrom statsmodels.graphics.mosaicplot import mosaic\nfrom scipy.stats import chi2_contingency","b6deaed6":"# Draw a mosaic plot and return chi-square test result (p-value) for a given categorical variable\ndef association (col):\n  # Draw Mosaic plot\n  # fig, ax = plt.subplots(figsize = (15, 10))\n  # props = lambda key: {'color': 'orange' if 'low' in key else 'lightblue'}\n  # labels = lambda key: '\\n'.join(key) if ('low' in key) or ('high' in key) else ''\n\n  # m = mosaic(df_cat_cleaned, [col, 'Price_Cat'], title = \"{0} v.s. Price\".format(col),\\\n  #           ax = ax, axes_label = False, horizontal = True, \\\n  #           properties = props, labelizer=labels)\n  # fig.show()\n  \n  # Make Crosstab and return p-value from the chi-square test\n  ct = pd.crosstab(df_cat_cleaned[col], df_cat_cleaned['Price_Cat'], margins = True)\n  chi2, p, dof, ex = chi2_contingency(ct)\n  print(col + ' v.s. Price'+ '     '+\"p-value: \"+\"{0:.4f}\".format(round(p,4)))\n  return p","8e549138":"# Loop through all the columns except for price_cat using the association function defined earlier\n# Store the pvalues into dictionary called pvalue. Key: Column name, Value: p-value\npvalues = {}\nfor col in df_cat_cleaned.columns.values[:-1]:\n  pvalues[col] = association (col)","f178ecc9":"\n# Return the categorical values with the smallest p-value\nmin(pvalues,key=pvalues.get)","e7c4d7d1":"# Extract the categorical variables with p-value greater than 0.05\n# P-value that's greater than 0.05 are insignificant\n\n{ key:value for (key,value) in pvalues.items() if value > 0.05}","2ebcce60":"**Future Work that we are considering to improve the accuracy:**\n* Working on further preparing the data before running the model\n* Running a random search on hyperparameters from a large range instead of a list of descrete values\n* Trying different models to predict the sell price[[](http:\/\/)](http:\/\/)","6201ce65":"## Steps to take\n1. Make price categorical: high-price houses, low-price, and middle range\n2. Make paired (high and low prices) stacked bar graphs for each categorical variable to see if the results are uniform\n3. Run chi-square test to see if the difference is statistically significant\n4. What categorical variables have the smallest p-value\n5. Interpret the result\n","a931babc":"# Build a Random Forest\n","27e51a30":"**Step 4: Cleaning data**","1382c3ed":"# Introduction","09a4500b":"**Step 7: Combine Categorical and Continuous Datasets**","8bfc1e17":"# Data Preprocessing","5652e8e2":"## Steps to take\n1. Split the training data into the _test and _train\n2. train the Random Forest on the _test\n3. measure the accuracy\n4. Tune the huperparameters\n5. Train the Random Forest on the original Train set with selected hyper parameters\n6. Look at the feature importance\n7. Predict the prices based on the original Test dataset","6d5a204a":"This noteboook is the fruit of our final project in [SI 370](https:\/\/www.si.umich.edu\/programs\/courses\/370) class (*Undergrad Data Exploration* taught at the University of Michigan School of Information)\n\nReference Notebooks that we used to get started: \n* https:\/\/www.kaggle.com\/dfitzgerald3\/randomforestregressor\n* https:\/\/www.kaggle.com\/goldens\/house-prices-on-the-top-with-a-simple-model\n* https:\/\/www.kaggle.com\/masumrumi\/a-detailed-regression-guide-with-house-pricing","b0094012":"**Step 2: Read In Data**","d119352c":"**Step 3: Remove Skew**","bb283c6b":"**Step 5: Preprocessing for Continuous Variables**","a3e6a51a":"Running the predictions on the 'test.csv' that has gone through data preprocessing","9074988a":"### Hyperparameters tweaking","6739cc56":"# Accounting for the categorical variables\n","606996d5":"Preparing the predicted value for submition on Kaggle","29d69853":"## Steps to take\n1. Import modules \n2. Read in data \n3. Remove skew by taking log\n4. Cleaning data by dropping columns with more than 10% missing values\n5. Impute missing values for continuous variables\n6. Impute missing values for categorical variables\n7. Combine categorical and continuous datasets after imputation","532d3453":"**Step 6: Preprocessing for Categorical Variables**\n\n","7f5a430a":"Traininig the model with the found 'best' parameters","65fd5de1":"**Step 1: Importing**"}}