{"cell_type":{"96996041":"code","f05ae88f":"code","03028820":"code","237f078d":"code","e8c7e730":"code","439f6379":"code","84f609d1":"code","457b2ad9":"code","daa56e1d":"code","8200ac78":"code","07c44b20":"code","2e5fb62b":"code","f74c5654":"code","1be1df6e":"code","61402c50":"code","97baea38":"code","64410ae1":"code","42a26795":"code","627ea63b":"code","5b4c7404":"code","650adaf2":"code","b8d50503":"code","80090812":"code","3ac009f7":"code","064a1676":"code","97d2bd7d":"code","eb55f5fb":"code","4371906a":"code","d309cc11":"code","a9a89e89":"code","36550f9f":"code","2d639045":"code","4c38f153":"code","efca42c2":"code","3e9d478a":"code","a409a12c":"code","3c0fceaf":"code","94fad7be":"code","37a52f56":"code","57089091":"code","4cd2ca2b":"code","81036419":"code","5115c5e3":"code","7bd698b0":"code","ff6e8481":"code","ef731693":"code","7a4e24ba":"code","fa0d19c9":"code","de262385":"code","2042d65b":"code","fcbd2f12":"code","f1c1023d":"code","33efe065":"code","6b651e9a":"code","a485a37f":"code","b1309d2c":"code","1a19fff2":"code","ada003ba":"code","b0798535":"code","f325b5a9":"code","dc200fec":"code","7353d211":"code","2681d8bf":"code","1f2426d6":"code","71633e72":"code","147450cd":"code","6d5d9f17":"code","c9f1c55d":"code","4bdadf57":"markdown","e96072fa":"markdown","18ddab45":"markdown","04296b19":"markdown","3c3e8459":"markdown","a8f6bf59":"markdown","c963fbae":"markdown","f17ed9c4":"markdown","d933c8c1":"markdown","65edaa0a":"markdown","cac54a13":"markdown","0ddea050":"markdown","510149f0":"markdown","f75030f0":"markdown","3ab14982":"markdown","49b68583":"markdown","e6df7bb2":"markdown","23bfc8b8":"markdown","91c4205c":"markdown","2d94be1d":"markdown"},"source":{"96996041":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%config InlineBackend.figure_format= 'retina'\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f05ae88f":"# load data\ndf = pd.read_csv('..\/input\/telecom-users-dataset\/telecom_users.csv')","03028820":"df.head()","237f078d":"df.info()","e8c7e730":"df = df.drop(['Unnamed: 0', 'customerID'], axis = 1)","439f6379":"df.head()","84f609d1":"df.shape","457b2ad9":"df['Churn'].value_counts()","daa56e1d":"sns.set_style('whitegrid')\nsns.countplot(df['Churn'])","8200ac78":"# Checking dublicate rows in the dataset. There are total of 16 rows of dublicate data\ndf.duplicated(subset = None, keep = 'first').sum()","07c44b20":"# Descipsion of duplicate dataset\nduplicate_rows = df[df.duplicated()].copy()\nduplicate_rows.groupby('gender').describe()","2e5fb62b":"df.isnull().sum().sort_values(ascending = False)","f74c5654":"# Handling duplicate rows\ndf = df.drop_duplicates(keep = 'first')","1be1df6e":"df.head()","61402c50":"df.shape","97baea38":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'gender', data = df)","64410ae1":"df.columns","42a26795":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'SeniorCitizen', data = df)","627ea63b":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'Partner', data = df)","5b4c7404":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'Dependents', data = df)","650adaf2":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'PhoneService', data = df)","b8d50503":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'MultipleLines', data = df)","80090812":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'InternetService', data = df)","3ac009f7":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'OnlineSecurity', data = df)","064a1676":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'OnlineBackup', data = df)","97d2bd7d":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'DeviceProtection', data = df)","eb55f5fb":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'TechSupport', data = df)","4371906a":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'StreamingTV', data = df)","d309cc11":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'StreamingMovies', data = df)","a9a89e89":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'Contract', data = df)","36550f9f":"plt.figure(figsize=(10,6))\nsns.countplot(x = 'Churn', hue = 'PaperlessBilling', data = df)","2d639045":"plt.figure(figsize=(12,8))\nsns.countplot(x = 'Churn', hue = 'PaymentMethod', data = df)","4c38f153":"plt.figure(figsize=(10,6))\nsns.lineplot(x = 'Churn', y = 'tenure', data = df)","efca42c2":"plt.figure(figsize=(10,6))\nsns.lineplot(x = 'Churn', y = 'MonthlyCharges', data = df)","3e9d478a":"plt.figure(figsize=(10,6))\nsns.lineplot(x = 'Churn', y = 'TotalCharges', data = df)","a409a12c":"df['gender'] = pd.factorize(df['gender'])[0]\ndf['Partner'] = pd.factorize(df['Partner'])[0]\ndf['Dependents'] = pd.factorize(df['Dependents'])[0]\ndf['PhoneService'] = pd.factorize(df['PhoneService'])[0]\ndf['PaperlessBilling'] = pd.factorize(df['PaperlessBilling'])[0]\ndf['Churn'] = pd.factorize(df['Churn'])[0]","3c0fceaf":"df.head()","94fad7be":"df = pd.get_dummies(data = df, columns=['MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod'])","37a52f56":"df.head()","57089091":"df.shape","4cd2ca2b":"pd.DataFrame([df.mean(), df.median(), df.std(), df.var()], index = ['Mean', 'Median', 'Std. Deviation', 'Variance'])","81036419":"cols = df.columns\nnum_cols = df._get_numeric_data().columns\nnum_cols","5115c5e3":"list(set(cols) - set(num_cols))","7bd698b0":"# Changing the totalcharge column into numeric.\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'], downcast = 'float', errors='coerce') # It converts any string value inside the column to NaN","ff6e8481":"df.info()","ef731693":"(df.isnull().sum()*100 \/ len(df)).round(2).sort_values(ascending = False)","7a4e24ba":"df_new = df.dropna(how = 'any').copy()","fa0d19c9":"df_new.isnull().values.any()","de262385":"df_new.info()","2042d65b":"X = df_new.drop('Churn', axis = 1)\ny = df_new['Churn']","fcbd2f12":"print('Shape of X(Predictors): {}'.format(X.shape))\nprint('Shape of Y(Target): {}'.format(y.shape))","f1c1023d":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=10, shuffle=True)\n\nprint('Number of X-train dataset: ', X_train.shape)\nprint('Number of y-train dataset: ', y_train.shape)\nprint('Number of X-test dataset: ', X_test.shape)\nprint('Number of y-test dataset: ', y_test.shape)","33efe065":"print('Before OverSampling of Training dataset of value 0: {}'.format(sum(y_train == 0)))\nprint('Before OverSampling of Training dataset of value 1: {}'.format(sum(y_train == 1)))","6b651e9a":"# importing SMOTE library\nfrom imblearn.over_sampling import SMOTE","a485a37f":"sm = SMOTE(random_state=2)\nX_train_sm, y_train_sm = sm.fit_resample(X_train, y_train.ravel())\n\nprint('Number of X-train dataset after oversampling: ', X_train_sm.shape)\nprint('Number of y-train dataset after oversampling: ', y_train_sm.shape)\n\nprint('After OverSampling of Training dataset of value 0: {}'.format(sum(y_train_sm == 0)))\nprint('After OverSampling of Training dataset of value 1: {}'.format(sum(y_train_sm == 1)))","b1309d2c":"from sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","1a19fff2":"# Creation of Pipelines without PCA\npipeline_lr = Pipeline([('scaler1', StandardScaler()), \n                       ('lr_classifier', LogisticRegression(random_state = 101))])\n\npipeline_dt = Pipeline([('dt_classifier', DecisionTreeClassifier(random_state= 101))])\n\npipeline_rf = Pipeline([('rf_classifier', RandomForestClassifier(random_state = 101))])","ada003ba":"# Creation of Pipelines with PCA\npipeline_pca_lr = Pipeline([('scaler1', StandardScaler()), \n                        ('pca1', PCA(n_components = 2)), \n                       ('lr_classifier', LogisticRegression(random_state = 101))])\n\npipeline_pca_dt = Pipeline([('scaler2', StandardScaler()), \n                        ('pca2', PCA(n_components = 2)), \n                       ('dt_classifier', DecisionTreeClassifier(random_state = 101))])\n\npipeline_pca_rf = Pipeline([('scaler3', StandardScaler()), \n                        ('pca3', PCA(n_components = 2)), \n                       ('rf_classifier', RandomForestClassifier(random_state = 101))])","b0798535":"# list of pipelines\npipelines_list = [pipeline_lr, pipeline_dt, pipeline_rf, pipeline_pca_lr, pipeline_pca_dt, pipeline_pca_rf]","f325b5a9":"# fit pipelines\nfor pipe in pipelines_list:\n    pipe.fit(X_train_sm, y_train_sm)","dc200fec":"# dictionary of pipelines and classifier for further use\npipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'Random Forest', 3: 'PCA with Logistic Regression', 4: 'PCA with Decision Tree', 5: 'PCA with Random Forest'}","7353d211":"for i, model in enumerate(pipelines_list):\n    print('{} training accuracy: {}'.format(pipe_dict[i], model.score(X_train_sm, y_train_sm).round(4)))","2681d8bf":"for i, model in enumerate(pipelines_list):\n    print('{} testing accuracy: {}'.format(pipe_dict[i], model.score(X_test, y_test).round(4)))","1f2426d6":"best_accuracy = 0.0\nbest_classifier = 0\nbest_pipeline = ''\n\nfor i, model in enumerate(pipelines_list):\n    if model.score(X_test, y_test) > best_accuracy:\n        best_accuracy = model.score(X_test, y_test)\n        best_pipeline = model\n        best_classifier = i\nprint('Classifier yielding best accuracy in test dataset: {}'.format(pipe_dict[best_classifier]))","71633e72":"pipeline_lr.fit(X_train_sm, y_train_sm)\ny_pred_lr = pipeline_lr.predict(X_test)","147450cd":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score","6d5d9f17":"print(classification_report(y_test, y_pred_lr))","c9f1c55d":"print(confusion_matrix(y_test, y_pred_lr))","4bdadf57":"# **Pipeline Creation without PCA**\n1. Process data using standard scaler\n2. Apply classifier\n\n# **Pipelines Creation with PCA**\n1. Process data using standard scaler\n2. Apply PCA\n3. Apply classifier","e96072fa":"Lets see if there are any string data types in the dataframe","18ddab45":"There is presence of unbalanced dataset in categorical varibale Churn.","04296b19":"# **Information generated from EDA**\n\n\n**High Churn rate**\n1. Telecom users who don't have dependents\n2. Telecom users who have fiber-optics internet\n3. Telecom users who don't have online security, online backup, device protection, tech support\n4. Telecom users who have month-to-month contract\n5. Telecom users who have paper billing system\n6. Telecom users who have electronic check payment system\n7. Telecom users who have low tenure\n8. Telecom users who have high monthly charges and high total charges","3c3e8459":"# Modeling","a8f6bf59":"Now, lets use the 'one hot encoding' technique for the categorical columns which have more than 2 values.","c963fbae":"# Train\/ Test Execution Information","f17ed9c4":"The TotalCharge column is not in numerical type. Lets change it to numeric","d933c8c1":"\n# Pre-Processing of Data","65edaa0a":"Now, lets convert the categorical datasets to 0 and 1. Here, pd.factorize() is used to those categorical columns whose values are in two categories. E.g: YES or NO, Male or Female","cac54a13":"# EDA","0ddea050":"There are no null values in the dataset.","510149f0":"# Data Visualization","f75030f0":"As we are going to predict the Churn patterns of telecom users, lets explore the relationship between various columns with Churn column","3ab14982":"As, we can see the data types in the columns are of int64, float64, object. The columns 'Unnamed: 0' and 'customerID' doesn't provide any signifiant help in prediction so we can drop those columns","49b68583":"# Selection of ML Model\nIt was found that the Logistic Regression performs with better accuracy.","e6df7bb2":"As, the null values are less than 5%. So, we can delete the null values safely as per rule of thumb.","23bfc8b8":"# Data ","91c4205c":"So, no more null values","2d94be1d":"As, the dataset is in unbalanced nature, so lets use SMOTE algorithm to handle it"}}