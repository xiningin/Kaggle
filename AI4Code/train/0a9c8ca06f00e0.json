{"cell_type":{"9dae971e":"code","e062d608":"code","5fe8d8f8":"code","f9e4448c":"code","0018cc20":"code","877d5f2b":"code","b4487164":"code","bd2aa52b":"code","d25452e4":"code","2d777c22":"code","009dfe3d":"code","88acf5cf":"code","5176428c":"code","172c56dc":"code","5f4dd4bb":"code","f1dc94cc":"markdown","2302dd87":"markdown","4e2d0a22":"markdown","eff464d7":"markdown","d52858bc":"markdown","296ed0d1":"markdown","07d344ea":"markdown","5dd48326":"markdown","c45ea908":"markdown","dbe32220":"markdown","492ff862":"markdown","d5bf1c36":"markdown"},"source":{"9dae971e":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nimport sklearn\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold\nimport keras_tuner as kt","e062d608":"train = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")\ntest = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/sample_submission.csv\")","5fe8d8f8":"train.head()","f9e4448c":"train[\"Pawpularity\"].hist()","0018cc20":"batch_size = 128 # Batch Size\ntrain_on_fold = None # Which fold to train, None to train on all folds.\ntabular_columns = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory', 'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur']","877d5f2b":"def rmse(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean((y_true -  y_pred) ** 2))","b4487164":"def build_model(hp):\n    inputs = tf.keras.layers.Input((len(tabular_columns)))\n    width = hp.Choice('width', [8, 16, 32, 64])\n    depth = hp.Choice('depth', [3, 6, 9, 12])\n    activation = \"relu\"\n    dropout = hp.Choice('dropout', [0.0, 0.1, 0.2])\n    use_batch_norm = hp.Choice('use_batch_norm', [True, False])\n    # Whether to use MSE or RMSE is worth to have a try\n    loss_function = hp.Choice(\"loss_function\", [\"mse\", \"rmse\"])\n    kernel_regularizer = hp.Choice(\"kernel_regularizer\", [\"none\", \"l1\", \"l2\", \"l1_l2\"])\n    acutal_kernel_regularizer = None\n    if kernel_regularizer == \"l1\":\n        acutal_kernel_regularizer = keras.regularizers.l1()\n    if kernel_regularizer == \"l2\":\n        acutal_kernel_regularizer = keras.regularizers.l2()\n    if kernel_regularizer == \"l1_l2\":\n        acutal_kernel_regularizer = keras.regularizers.l1_l2()\n    for i in range(depth):\n        if i == 0:\n            x = inputs\n           \n        x = keras.layers.Dense(\n            width, \n            activation=activation,\n            kernel_regularizer=acutal_kernel_regularizer\n        )(x)\n        if (i + 1) % 3 == 0:\n            if dropout > 0:\n                x = keras.layers.Dropout(dropout)(x)\n            if use_batch_norm:\n                x = keras.layers.BatchNormalization()(x)\n            x = keras.layers.Concatenate()([x, inputs])\n    output = keras.layers.Dense(1, activation=\"relu\")(x)\n    model = keras.Model(inputs=inputs, outputs=output)\n    adam = keras.optimizers.Adam(learning_rate=hp.Float(\"learing_rate\", 1e-5, 5e-3))\n    loss = \"mse\" if loss_function == \"mse\" else rmse\n    model.compile(loss=loss, optimizer=adam, metrics=[\"mae\", rmse])\n    return model","bd2aa52b":"def preprocess(x, y):\n    x = tf.cast(x, tf.float32)\n    y = tf.cast(y, tf.float32)\n    print(x, y)\n    return x, y","d25452e4":"tf.keras.backend.clear_session()\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\nfor index, (train_indices, val_indices) in enumerate(kfold.split(train)):\n    if train_on_fold is not None and train_on_fold != index:\n        continue\n    train_features = train.loc[train_indices, tabular_columns]\n    train_targets = train.loc[train_indices, [\"Pawpularity\"]]\n    val_features = train.loc[val_indices, tabular_columns]\n    val_targets = train.loc[val_indices, [\"Pawpularity\"]]\n    checkpoint_path = \"model_%d.h5\"%(index)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_path, \n        save_best_only=True\n    )\n    early_stop = tf.keras.callbacks.EarlyStopping(\n        min_delta=1e-4, \n        patience=10\n    )\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        factor=0.3,\n        patience=2, \n        min_lr=1e-7\n    )\n    callbacks = [early_stop, checkpoint, reduce_lr]\n    print(train_features.shape, train_targets.shape)\n    print(val_features.shape, val_targets.shape)\n    train_ds = tf.data.Dataset.from_tensor_slices((train_features, train_targets)).map(preprocess).shuffle(512).batch(batch_size).cache().prefetch(2)\n    val_ds = tf.data.Dataset.from_tensor_slices((val_features, val_targets)).map(preprocess).batch(batch_size).cache().prefetch(2)\n    tuner = kt.RandomSearch(\n        build_model,\n        objective=kt.Objective(\"val_rmse\", direction=\"min\"),\n        max_trials=100,\n        directory=\"directory\"\n    )\n    tuner.search(train_ds, validation_data=val_ds, epochs=10, callbacks=callbacks)\n    break","2d777c22":"best_model = tuner.get_best_models()[0]\nkeras.utils.plot_model(best_model, show_shapes=True)","009dfe3d":"\"\"\"\n {'width': 64,\n 'depth': 6,\n 'dropout': 0.1,\n 'use_batch_norm': 0,\n 'loss_function': 'mse',\n 'kernel_regularizer': 'none',\n 'learing_rate': 0.0038644865099609653}\n\"\"\"\nbest_hp = tuner.get_best_hyperparameters()[0]\nbest_hp.get_config()[\"values\"]","88acf5cf":"models = []\nfor index, (train_indices, val_indices) in enumerate(kfold.split(train)):\n    if train_on_fold is not None and train_on_fold != index:\n        continue\n    train_features = train.loc[train_indices, tabular_columns]\n    train_targets = train.loc[train_indices, [\"Pawpularity\"]]\n    val_features = train.loc[val_indices, tabular_columns]\n    val_targets = train.loc[val_indices, [\"Pawpularity\"]]\n    checkpoint_path = \"model_%d.h5\"%(index)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(\n        checkpoint_path, \n        save_best_only=True\n    )\n    early_stop = tf.keras.callbacks.EarlyStopping(\n        min_delta=1e-4, \n        patience=10\n    )\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n        factor=0.3,\n        patience=2, \n        min_lr=1e-7\n    )\n    callbacks = [early_stop, checkpoint, reduce_lr]\n    print(train_features.shape, train_targets.shape)\n    print(val_features.shape, val_targets.shape)\n    train_ds = tf.data.Dataset.from_tensor_slices((train_features, train_targets)).map(preprocess).shuffle(512).batch(batch_size).cache().prefetch(2)\n    val_ds = tf.data.Dataset.from_tensor_slices((val_features, val_targets)).map(preprocess).batch(batch_size).cache().prefetch(2)\n    model = tuner.hypermodel.build(best_hp)\n    history = model.fit(train_ds, epochs=50, validation_data=val_ds, callbacks=callbacks)\n    model.load_weights(checkpoint_path)\n    models.append(model)","5176428c":"def preprocess_test_data(x):\n    x = tf.cast(x, tf.float32)\n    print(x)\n    return x","172c56dc":"test_ds = tf.data.Dataset.from_tensor_slices((test[tabular_columns])).map(preprocess_test_data).batch(batch_size).prefetch(2)","5f4dd4bb":"total_results = []\nfor model in models:\n    total_results.append(model.predict(test_ds).reshape(-1))\nresults = np.mean(total_results, axis=0).reshape(-1)\nsample_submission[\"Pawpularity\"] = results\nsample_submission.to_csv(\"submission.csv\", index=False)","f1dc94cc":"### HyperParmeter Tuning","2302dd87":"# Pawpularity Model with DNN on Meta Data\n\n## Table of Contents\n- Summary\n- Set up\n- Import datasets\n- Data Preprocessing\n- Model Development\n- Model Evaluation\n- Submission\n\n\n## Summary\nIn this notebook, I am going use KeraTuner to find the best Model that fits only on Meta Data. So that I can improve total Performance of Model that use both Image Data and Meta Data. Some of my notebooks that use result can be found in:\n* [Pawpularity with EfficientNet: [Training]](https:\/\/www.kaggle.com\/lonnieqin\/pawpularity-with-efficientnet-training)\n* [Pawpularity with EfficientNet: [Inference]](https:\/\/www.kaggle.com\/lonnieqin\/pawpularity-with-efficientnet-inference)\n* [TensorFlow multi-input Pet Pawpularity Model](https:\/\/www.kaggle.com\/lonnieqin\/tensorflow-multi-input-pet-pawpularity-model)\n\n## Set up","4e2d0a22":"### Best Architecture","eff464d7":"### Preprocess function","d52858bc":"## Data Preprocessing","296ed0d1":"## Submission","07d344ea":"## Import datasets","5dd48326":"### RMSE loss function","c45ea908":"### Best parameters","dbe32220":"## Model Development","492ff862":"### The Model","d5bf1c36":"### Keep Trainnig with best Parameters"}}