{"cell_type":{"4a137024":"code","f9c03387":"code","aae93282":"code","680c2529":"code","56965161":"code","9f4ec718":"code","5c4918db":"code","8f94c2c1":"code","fbe8257b":"code","94f30e8d":"code","e1fdaa22":"code","0dd46b31":"code","a47de808":"code","9c6128f3":"code","75758dcf":"code","535d9b83":"code","4da16b9f":"code","17e984ac":"code","0da40fd2":"code","62e88042":"code","db0b552b":"code","99830ac3":"code","82d90c45":"code","20890f9d":"code","354d8d2a":"code","03f359cb":"code","7904d4b6":"code","f651a074":"code","96a24aea":"code","c5cb5b59":"code","63bfa358":"code","f3398f7b":"code","e51b92f6":"code","851a1b17":"code","e858743e":"code","fd759440":"code","2edefd6e":"code","6ba8c6ec":"code","c40e84f4":"code","a25d0fe6":"code","e8643fd8":"code","932321a1":"code","b46f1830":"code","772ddcb5":"code","aade15e9":"code","6dd823a8":"code","c8f5d37b":"code","dd167cf2":"code","94632d00":"code","77877aed":"code","8d1a6f1c":"code","e299e837":"code","4a2b3ecc":"code","0ff9c358":"code","83fddba2":"markdown","6eeb0980":"markdown","02dc2ea0":"markdown","2daebf90":"markdown","41eb0daa":"markdown","4f771935":"markdown","030f49cf":"markdown","13dae10c":"markdown","65968030":"markdown","bbbb0092":"markdown","ecc2caae":"markdown","c35ccc2d":"markdown","f00fe8a8":"markdown","2a89e36a":"markdown","cb2318b9":"markdown","b047f5bb":"markdown","ef19ab9f":"markdown","051200e2":"markdown","85096681":"markdown","89d49c81":"markdown","921051c2":"markdown","f586731f":"markdown","43e060f1":"markdown","b207dc08":"markdown","7dfb8aa8":"markdown","dca445e4":"markdown","9f475a67":"markdown","96af19c3":"markdown","de1e8cdd":"markdown","0db678a2":"markdown","3396a0d3":"markdown","6cb4a2d8":"markdown","20ad92bc":"markdown","cb94ae70":"markdown","40a48fc6":"markdown","2a790d0a":"markdown","d35630c9":"markdown","099b633b":"markdown","f755d563":"markdown","faa1b4b9":"markdown","f2051b2c":"markdown","3d1c5920":"markdown","d9907983":"markdown","3d3c51f9":"markdown","74937595":"markdown","e119648a":"markdown","fc7b4b40":"markdown","53bbd7b7":"markdown","568fff15":"markdown","d3f8bddb":"markdown","5e06a867":"markdown","7c69bc69":"markdown","5c7a408e":"markdown","c1ac95d6":"markdown","26837a3d":"markdown","0b11bcdd":"markdown"},"source":{"4a137024":"import pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\n\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\nfrom keras.optimizers import Adam","f9c03387":"df_train= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","aae93282":"print('Training Set Shape = {}'.format(df_train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(df_train.memory_usage().sum() \/ 1024**2))\nprint('Test Set Shape = {}'.format(df_test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(df_test.memory_usage().sum() \/ 1024**2))\n\ndf_train.head()\nprint(df_train.columns)\nprint(df_train['text'].values[0])\nprint(\"=\"*50)\nprint(df_test['text'].values[150])\nprint(\"=\"*50)","680c2529":"# extracting the number of examples of each class\nReal_len = df_train[df_train['target'] == 1].shape[0]\nNot_len = df_train[df_train['target'] == 0].shape[0]\n# bar plot of the 3 classes\nplt.rcParams['figure.figsize'] = (7, 5)\nplt.bar(10,Real_len,3, label=\"Disastrous tweets counts\", color='red')\nplt.bar(15,Not_len,3, label=\"Non-Disastrous tweets counts\", color='green')\nplt.legend()\nplt.ylabel('Number of examples')\nplt.title('Propertion of examples')\nplt.show()","56965161":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=df_train[df_train['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=df_train[df_train['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","9f4ec718":"def create_corpus(target): # creating corpus of all the words in our dataset\n    corpus=[]\n    for x in df_train[df_train['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","5c4918db":"corpus=create_corpus(0)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop_0=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:20]\n\n####################################################################\n\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop_1=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:20]","8f94c2c1":"plt.rcParams['figure.figsize'] = (18.0, 6.0)\nx0,y0=zip(*top_0)\nx1,y1=zip(*top_1)\nplt.bar(x0,y0, color=['green'], label = \"Non-disaster\")                # indicates non-disaster tweets\nplt.bar(x1,y1, color=['red'], label = \"Disaster\")              # indicates disaster tweets\nplt.legend()","fbe8257b":"corpus = create_corpus(1)\nplt.figure(figsize=(16,5))\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x).set_title('Words most frequently occuring in Non-disastrous tweets')","94f30e8d":"corpus = create_corpus(0)\nplt.figure(figsize=(16,5))\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x).set_title('Words most frequently occuring in Disastrous tweets')","e1fdaa22":"df=pd.concat([df_train,df_test])       \ndf.shape","0dd46b31":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","a47de808":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","9c6128f3":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","75758dcf":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","535d9b83":"df['text']=df['text'].apply(lambda x : remove_URL(x))\ndf['text']=df['text'].apply(lambda x : remove_html(x))\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))\ndf['text']=df['text'].apply(lambda x : remove_punct(x))","4da16b9f":"#creating seperate corpus for both disastrous & non-disastrous tweets\ncorpus_new1 = create_corpus(1)\ncorpus_new0 = create_corpus(0)","17e984ac":"plt.figure(figsize=(12,8))\nword_cloud1 = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_new1[:50]))\n\nword_cloud0 = WordCloud(\n                          background_color='black',\n                          max_font_size = 80\n                         ).generate(\" \".join(corpus_new0[:50]))\n","0da40fd2":"fig, ax = plt.subplots(1,2, figsize=(20, 5))\nax[0].imshow(word_cloud1)\nax[0].set_title('Most frequently appearing words in disastrous tweets',fontsize = 20)\nax[0].axis(\"off\")\n\nax[1].imshow(word_cloud0)\nax[1].set_title('Most frequently appearing words in non-disastrous tweets',fontsize = 20)\nax[1].axis(\"off\")\n\nplt.show()","62e88042":"def create_corpus_new(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet)]\n        corpus.append(words)\n    return corpus   ","db0b552b":"corpus=create_corpus_new(df)","99830ac3":"embedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","82d90c45":"MAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","20890f9d":"tweet_pad[0][0:]","354d8d2a":"word_index=tokenizer_obj.word_index\nprint('Total number of unique words in corpus(full datset):',len(word_index))","03f359cb":"num_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\nfor word,i in tqdm(word_index.items()):\n    if i < num_words:\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec","7904d4b6":"model=Sequential()\n\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=3e-4)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])","f651a074":"model.summary()","96a24aea":"train=tweet_pad[:df_train.shape[0]]\ntest=tweet_pad[df_train.shape[0]:]","c5cb5b59":"train.shape","63bfa358":"X_train,X_cv,y_train,y_cv=train_test_split(train,df_train[\"target\"],test_size=0.1)\nprint('Shape of train  ->',X_train.shape)\nprint(\"Shape of Validation ->\",X_cv.shape)","f3398f7b":"%%time\n# Recomended 10-20 epochs\nhistory=model.fit(X_train,y_train,batch_size=64,epochs=10,validation_data=(X_cv,y_cv),verbose=2)","e51b92f6":"train_pred_GloVe = model.predict(X_train)\ntrain_pred_GloVe_int = train_pred_GloVe.round().astype('int')\n\ncv_pred_GloVe = model.predict(X_cv)\ncv_pred_GloVe_int = cv_pred_GloVe.round().astype('int')\n\ntest_pred_GloVe = model.predict(test)\ntest_pred_GloVe_int = test_pred_GloVe.round().astype('int')\n","851a1b17":"# Showing Confusion Matrix\ndef plot_cm(y_true, y_pred, title, figsize):\n    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n    cm_sum = np.sum(cm, axis=1, keepdims=True)\n    cm_perc = cm \/ cm_sum.astype(float) * 100\n    annot = np.empty_like(cm).astype(str)\n    nrows, ncols = cm.shape\n    for i in range(nrows):\n        for j in range(ncols):\n            c = cm[i, j]\n            p = cm_perc[i, j]\n            if i == j:\n                s = cm_sum[i]\n                annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n            elif c == 0:\n                annot[i, j] = ''\n            else:\n                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n    cm.index.name = 'Actual'\n    cm.columns.name = 'Predicted'\n    fig, ax = plt.subplots(figsize=figsize)\n    plt.title(title)\n    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)","e858743e":"# Showing Confusion Matrix for LSTM+GloVe model on cv dataset\nplot_cm(cv_pred_GloVe_int, y_cv, 'Confusion matrix for LSTM+GloVe model on cv dataset', figsize=(7,7))","fd759440":"submission['target'] = test_pred_GloVe_int\nsubmission.head(10)\n\nsubmission.to_csv(\"submission.csv\", index=False, header=True)\nsubmission.head()","2edefd6e":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\n\n\n# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py\n    \nimport tokenization","6ba8c6ec":"# Thanks to https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","c40e84f4":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    \n    if Dropout_num == 0:\n        # Without Dropout\n        out = Dense(1, activation='sigmoid')(clf_output)\n    else:\n        # With Dropout(Dropout_num), Dropout_num > 0\n        x = Dropout(Dropout_num)(clf_output)\n        out = Dense(1, activation='sigmoid')(x)\n\n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","a25d0fe6":"# Load BERT from the Tensorflow Hub\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","e8643fd8":"# Load CSV files containing training data\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","932321a1":"df=pd.concat([train,test])       \ndf.shape","b46f1830":"df['text']=df['text'].apply(lambda x : remove_URL(x))\ndf['text']=df['text'].apply(lambda x : remove_html(x))\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))\ndf['text']=df['text'].apply(lambda x : remove_punct(x))","772ddcb5":"train = df[:train.shape[0]]\ntest = df[train.shape[0]:]","aade15e9":"X_train_BERT,X_cv_BERT,y_train_BERT,y_cv_BERT=train_test_split(train,train['target'].values,test_size=0.2)\nprint('Shape of train set ->',X_train_BERT.shape)\nprint(\"Shape of Validation set ->\",X_cv_BERT.shape)\nprint(\"Shape of test set ->\",test.shape)","6dd823a8":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","c8f5d37b":"# Thanks to https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\ntrain_input = bert_encode(X_train_BERT.text.values, tokenizer, max_len=160)\ncv_input = bert_encode(X_cv_BERT.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = y_train_BERT\ncv_labels = y_cv_BERT","dd167cf2":"# Thanks to https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\nDropout_num = 0\nmodel_BERT = build_model(bert_layer, max_len=160)\nmodel_BERT.summary()","94632d00":"# Thanks to https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\ncheckpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', save_best_only=True)\n\ntrain_history = model_BERT.fit(\n    train_input, train_labels,\n    validation_split = 0.1,\n    epochs = 3, # recomended 3-5 epochs\n    callbacks=[checkpoint],\n    batch_size = 16\n)","77877aed":"# Thanks to https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\n# Prediction by BERT model with my tuning\nmodel_BERT.load_weights('model_BERT.h5')\ntrain_pred_BERT = model_BERT.predict(train_input)\ntrain_pred_BERT_int = train_pred_BERT.round().astype('int')\n\ncv_pred_BERT = model_BERT.predict(cv_input)\ncv_pred_BERT_int = cv_pred_BERT.round().astype('int')","8d1a6f1c":"test_pred_BERT = model_BERT.predict(test_input)\ntest_pred_BERT_int = test_pred_BERT.round().astype('int')","e299e837":"submission['target'] = test_pred_BERT_int\nsubmission.head(10)","4a2b3ecc":"submission.to_csv(\"submission.csv\", index=False, header=True)","0ff9c358":"plot_cm(cv_pred_BERT_int, cv_labels, 'Confusion matrix for BERT model on cv dataset', figsize=(7,7))","83fddba2":"# 6. EDA","6eeb0980":"*Encoding the text into tokens, masks, and segment flags which are taken as input to BERT model*","02dc2ea0":"*Removing URLs from tweets*","2daebf90":"# 9. Predicting train,cv,test outputs from the LSTM model","41eb0daa":"Romoving Emojis from tweets","4f771935":"*Concatenating train & test data to apply cleaning on whole dataset similar to what we done earlier*","030f49cf":"*Representation of first word in our corpus. Note that the size of tweet_pad[0][0:] is 50 dimension as MAX_LEN defined above is 50.*","13dae10c":"*Creating a new corpus*","65968030":"*Please note that we are providing embedding_matrix as embedding_initializer in embedding layer of this LSTM model.\nEmbedding layer in Keras also provides a way to vectorize the words. Instead of providing embedding_matrix specifically if you leave it as it is then it will perform word embedding on its own.*\n\n*People prefer to use trained word embedding models because they have been trained on a very large dataset and models like GloVe, W2V, BERT are some of the most advanced and accurate word embedding techniques out there.* ","bbbb0092":"# 3. References of some other notebooks used in this project\n\n* https:\/\/www.kaggle.com\/vbmokin\/nlp-eda-bag-of-words-tf-idf-glove-bert\n\n* https:\/\/www.kaggle.com\/xhlulu\/disaster-nlp-keras-bert-using-tfhub\n\n* https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\n\n* https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove","ecc2caae":"# 13. Comparing LSTM with GloVe vs BERT model","c35ccc2d":"* > ## Checking common words in both types of tweets","f00fe8a8":"# 1. General Advice\n\n* Ensure that the Internet option is turned \"ON\" in kaggle to download libraries,datasets,etc.\n \n* Ensure that you have enabled GPU accelerator for your notebook at the time of running your notebook for faster performance. You can enable it by clicking on the arrow appearing on the right of \"Save Version\" button. \n\n\n\n","2a89e36a":"# \u2714\ufe0fPLEASE GIVE THIS NOTEBOOK AN UPVOTE IF YOU LIKED IT!!!","cb2318b9":"*Fitting the model with the train dataset keeping cv dataset as validation_data*","b047f5bb":"# 11. Applying BERT","ef19ab9f":"* ## Checking stop words for disastrous & Non-disastrous tweets","051200e2":"*Importing pretrained GloVe vector representation of words*\n\n\n*Please note that we are creating an embedding dictionary which will store each word representation in it. One more thing to note here is that these words are not the words from our dataset instead these are the words stored in the glove model *","85096681":"# 8. Building an LSTM Model with GloVe results","89d49c81":"* ## Printing WordCloud","921051c2":"> ### CONCLUSION : Similar argument can be made for most frequently appearing words in disastrous tweets. Data cleaning is required in the whole dataset.\n\n","f586731f":"*Splitting train & test datasets. Please note that df stores all the dataset so we are splitting df into train and test*","43e060f1":"> ## RESULTS : Our model is able to accurately classify ~82% of disastrous tweets as disastrous and ~79 of non-disastrous and non-disastrous.","b207dc08":"> ### CONCLUSION : Both types of tweets have stop word \"the\" appearing most number of times. Stop words \"on\", \"by\", \"at\",  \"from\", \"are\", \"after\", \"as\" doesn't appear in non-disasterous tweets.But the count of these words are not enough to qualify them as a differentiating feature. In summary we cannot find much from here also.","7dfb8aa8":"> ### CONCLUSION : Both disastrous & non-disastrous tweets are almost equally probable to occur. So there is no problem of imbalanced dataset.\n\n*What if we had imbalanced dataset : We then had to implement techniques like upsampling, creating synthetic points(Data Augmentation),etc. Downsampling majority class is generally not recommended because it results into loss of dataset which ultimately results into loss of information.*","dca445e4":"# 5. Loading the dataset","9f475a67":"# 2. Some important study material links related to this project\n\nhttp:\/\/jalammar.github.io\/illustrated-bert\/\n\nhttp:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n\nhttps:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/","96af19c3":"*Removing punctuations from tweets*","de1e8cdd":"# 12. Testing the BERT model on train & test dataset","0db678a2":"*Please note that df here includes train as well as test dataset.Therefore we have to split this dataset into train and test before applying BERT model in it.*","3396a0d3":"*Load tokenizer from the bert layer*","6cb4a2d8":"*Building BERT model with self tuning the parameters*","20ad92bc":"*Storing model's output for data in submission file*","cb94ae70":"# 10. Plotting Confusion Matrix","40a48fc6":"* ## Check for the presence of imbalanced dataset","2a790d0a":"> ## RESULTS : We are getting ~82% accuracy on cv dataset(unseen by model)","d35630c9":"# Table of Content\n1.  General Advice\n1.  Some important study material links related to this project\n1.  References of some other notebooks used in this project\n1.  Importing relevant Libraries\n1.  Loading the dataset\n1.  EDA\n     *      Check for the presence of imbalanced dataset.\n     *      Checking number of words in disastrous & non-disastrous tweets\n     *      Checking stop words for disastrous & Non-disastrous tweets\n     *      Checking common words in both types of tweets\n     *      Cleaning the dataset\n     *      Printing WordCloud\n1.  Word Embedding using GloVe  \n1.  Building an LSTM Model with GloVe results\n1.  Predicting train,cv,test outputs from the LSTM model\n1.  Plotting Confusion Matrix\n1.  Applying BERT\n1.  Testing the BERT model on train & test dataset\n1.  Comparing LSTM with GloVe vs BERT model","099b633b":"> ### CONCLUSION : Words like \"-\",\"...\" are appearing in most of the sentences of non-disastrous tweets so we will eliminate such words appearing in the sentences because they don't add any information about the tweet.\n\n*But why are we removing unnecessary words? Why can't we just leave them as it is? : Some of the un-necessary words are occuring in almost every sentence so it will occupy so many space and doesn't make any sense to keep that word which is not adding any differentiating value for the classification*","f755d563":"* ## Cleaning the dataset","faa1b4b9":"* ## Checking number of words in disatrous & non-disastrous tweets[](http:\/\/)","f2051b2c":"*Splitting train further into train and cv datasets*","3d1c5920":"> ## CONCLUSION - Words like \"Shelter\", \"evacuation\", \"Forest\", \"Reason\" appear a lot in disastrous tweets as we can see from the word cloud. Words like \"wonderful\", \"lovely\" and some words very irrelevant to a disastrous tweet appear in non-disastrous tweets, as expected. \n\n*Why are we drawing Word Clouds : Generally we draw word clouds to have an idea about the most common words appearing in the document(tweets in this case). In reference to this dataset, it also gives an idea about the words people are most commonly using in a disastrous & non-disastrous tweets. *","d9907983":"*Concatenating train & test data to apply cleaning on whole dataset*","3d3c51f9":"*Performing padding to ensure that each word vector representation is of same length because models coming up require inputs to be of same length.*","74937595":"# 7. Word Embedding using GloVe","e119648a":"*Here we are creating embedding matrix which will store vector representation of each word in our corpus. Note that this is the first time we are creating vector representation of words appearing in our dataset(corpus).embedding_matrix[i] gives the vector representaion of ith word in our dataset. \nPlease note that we are creating a 100 dimension vector representation of each word here.*","fc7b4b40":"## Printing Confusion Matrix","53bbd7b7":"# 4. Importing relevant Libraries","568fff15":"## Cleaning the data","d3f8bddb":"*Removing HTML tags from tweets*","5e06a867":"> ## CONCLUSION : ~86% disastrous tweets are correctly predicted & ~85% non-disastrous are also correctly predicted. Whereas in LSTM+GloVe model this percentage was ~82% & ~79% respectively. So we have significantly improved from LSTM+GloVe model to BERT model.","7c69bc69":"*I would strongly recommend you to refer to the external reference links provided in the starting of this kernal. Following lines of codes will be very clear after that.*","5c7a408e":"*Applying all the data cleaning methods*","c1ac95d6":"Please note that we are again loading the dataset","26837a3d":"*Further splitting train set further into train and cv sets*","0b11bcdd":"> ### CONCLUSION - We performed this check to see if number of characters appearing in disastrous tweets is very different from the number of characters appearing in non-disastrous tweets but ~120-140 character sentences are very common between disastrous & non-disastrous tweets. And the overall distribution also looks very similar so character count per sentence feature is not so important to classify between disastrous & non-disastrous tweets"}}