{"cell_type":{"d4e92f98":"code","feec6379":"code","cea7e9b3":"code","f59ef58e":"code","cb08525f":"code","85476afb":"code","6c70c6c2":"code","c16dbe1b":"code","aada005a":"code","aeaf62d2":"code","1047e33b":"code","54d44400":"markdown","5d42a611":"markdown","1ca01cd9":"markdown","4167f651":"markdown","400eda82":"markdown","a51939d5":"markdown","10023b8b":"markdown","9d10aab4":"markdown","6ae2d9ea":"markdown","f804d254":"markdown","354e4294":"markdown","19f88b25":"markdown","6cd38c44":"markdown","d84656d5":"markdown","450ff5e3":"markdown"},"source":{"d4e92f98":"import pandas as pd\nimport numpy as np\n\ndata = pd.read_csv(\"\/kaggle\/input\/neuromuscular-monitoring-data\/Neuromuscular_monitoring_data.csv\")\ndata.info()","feec6379":"# Plot TOF observations per patient\n# First two plots illustrate the progress of a surgery without TOF outliers\n# In the last two plots, the outliers are emphasized with red marks\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\ndef plotOutliersAndNormalObservations():\n\n    # These patients are specifically selected to demonstrate anomalous and non-anomalous \n    # observations during neuromuscular monitoring\n    patientsPK = [17, 4, 58, 93]\n    \n    for patient in patientsPK: \n        tempDf = data.loc[data[\"PK\"] == patient]\n        tempDf = tempDf.reset_index(drop=True)\n        plt.figure()\n        sns.lineplot(data=tempDf['TOF'])\n        sns.scatterplot(data = tempDf.loc[(tempDf[\"Outlier\"] == 1), [\"TOF\"]], palette=['red'])\n        plt.xlabel('Time (minutes)')\n        plt.legend([],[], frameon=False)\n        \nplotOutliersAndNormalObservations()","cea7e9b3":"from sklearn import preprocessing\n\n# Create train dataset with patients from the retrospective study\nX_all_train = data.loc[data[\"typeStudy\"] == 'R']\ny_all_train = X_all_train['Outlier'].values\nX_all_train = X_all_train.drop([\"PK\", \"typeStudy\", \"Outlier\"], axis=1)\n\n# Create test dataset with patients from the prospective study\nX_all_test = data.loc[data[\"typeStudy\"] == 'P']\ny_all_test = X_all_test['Outlier'].values\nX_all_test = X_all_test.drop([\"PK\", \"typeStudy\", \"Outlier\"], axis=1)\n\n# Since we dropped several columns in the training set, we have to reset the indices of training datasets\nX_all_train = X_all_train.reset_index(drop=True)\nX_all_test  = X_all_test .reset_index(drop=True)\n\n# Standard scaling of the training datasets\nX_all_train = preprocessing.scale(X_all_train)\nX_all_test = preprocessing.scale(X_all_test)","f59ef58e":"# Hyperparameter finetuning for the cost-sensitive logistic regression model\n# In case of multi-class classification, see https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV\n\ndef hyperParameterOptimalizationCSLR(X, y):\n\n    # Specify the classifier\n    clf = LogisticRegression(solver=\"lbfgs\", random_state=42, max_iter=500)\n\n    # The different parameters to perform random search on:\n    param_grid = {'C': [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0],\n                'class_weight' : [{0:100,1:1}, {0:10,1:1}, {0:1,1:1}, {0:1,1:10}, {0:1,1:100}, 'balanced']}\n\n    \n    # We can assign multiple scores to evaluate the process of hyperparameter tuning if we like:\n    # scores = ['accuracy', 'precision', 'recall', 'f1']\n    scores = ['roc_auc', 'f1']\n\n    # Perform seperate Random Search for every score that we have defined\n    for score in scores:\n\n        random_search = RandomizedSearchCV(estimator=clf,\n                        param_distributions=param_grid,\n                        scoring=score, \n                        cv=5,\n                        n_jobs=-1,\n                        random_state = 42)\n\n        print(\"\\n############### Tuning hyper-parameters for %s\" % score)\n\n        random_search.fit(X, y)\n\n        print(\"\\nBest parameters set found on development set:\\n\")\n        print(random_search.best_params_)\n\n        print(\"\\nCorresponding Score: \", random_search.best_score_)\n        print(\"\\Scores on train dataset:\")\n        means = random_search.cv_results_['mean_test_score']\n        stds = random_search.cv_results_['std_test_score']\n        for mean, std, params in zip(means, stds, random_search.cv_results_['params']):\n            print(\"%0.3f (+\/-%0.03f) for %r\"\n                % (mean, std * 2, params))\n\n        # Pick best estimator based on best F1 score\n        if(score == 'f1'):\n            clf = random_search.best_estimator_\n            print(\"\\nBest Estimator:\\n\", clf)\n            return clf\n\nfullCLF = hyperParameterOptimalizationCSLR(X_all_train, y_all_train)","cb08525f":"# Train optimized model on the full training dataset\n\nfrom sklearn.model_selection import cross_val_score\n\ndef trainModelAfterOptimization(clf, X_train, y_train):\n\n    # Evaluate the classifier on the full training set with best parameters, applying stratified k-fold cross validation\n\n    print(\"\\n################## Evaluate classifier on the entire training set:\\n\")\n\n    scoring = ['f1', 'roc_auc', 'precision', 'recall']\n    \n    for score in scoring:\n        scores = cross_val_score(estimator=clf,\n                                X=X_train,\n                                y=y_train,\n                                cv=5,\n                                n_jobs=-1,\n                                scoring=score)\n        \n        print('Average and std. dec CV sets -', score,': %.3f +\/- %.3f' % (np.mean(scores), np.std(scores)))\n\ntrainModelAfterOptimization(fullCLF, X_all_train, y_all_train)","85476afb":"# Bootstrap method to obtain confidence intervals of our performance metrics\n\nfrom mlxtend.evaluate import bootstrap_point632_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, roc_curve, auc\n\ndef bootstrap_estimate_and_ci(estimator, X, y, random_seed=0, method='.632', alpha=0.05, n_splits=50):\n\n    print(\"\\n----------------  Bootstrap Estimates of performance metrics\\n\")\n\n    scoring = [f1_score, roc_auc_score, precision_score, recall_score]\n\n    for score in scoring:\n\n        scores = bootstrap_point632_score(estimator, X, y, scoring_func=score, \n                                        n_splits=n_splits, random_seed=random_seed, \n                                        method=method)\n        estimate = np.mean(scores)\n        lower_bound = np.percentile(scores, 100*(alpha\/2))\n        upper_bound = np.percentile(scores, 100*(1-alpha\/2))\n        stderr = np.std(scores)\n        \n        printScore = str(score).split(\" at\", 1)[0].split(\" \", 1)[1]\n        print(printScore, '---', f\"estimate: {estimate:.2f}, confidence interval: [{lower_bound:.2f}, {upper_bound:.2f}], \"f\"standard error: {stderr:.2f}\")\n\nbootstrap_estimate_and_ci(fullCLF, X_all_train, y_all_train)\n","6c70c6c2":"# Plot learning curves \nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(clf, X_train, y_train, score, plot_y_limits, plot_y_ticks):\n\n    train_sizes, train_scores, test_scores =\\\n            learning_curve(estimator=clf,\n                           X=X_train,\n                           y=y_train,\n                           train_sizes=np.linspace(0.1, 1.0, 10),\n                           cv=5,\n                           n_jobs=1,\n                           scoring=score)\n\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n\n    plt.plot(train_sizes, train_mean,\n            color='blue', marker='o',\n            markersize=5, label='Training score')\n\n    plt.plot(train_sizes, test_mean,\n            color='green', linestyle='--',\n            marker='s', markersize=5,\n            label='Cross-validation score')\n\n    plt.grid()\n\n    # Change following settings depending on dataset and score:\n    plt.ylim(plot_y_limits)\n    plt.yticks(plot_y_ticks)\n    plt.legend(loc='lower right', fontsize='large')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Plot learning curve for F1-score\n\n# Specify the y-axis limits for the learning curve plot, related to F1-score    \nplot_y_limits = [0.0, 1.0]\n# Specify the y-axis ticks for the learning curve plot, related to F1-score   \nplot_y_ticks = [0.2, 0.4, 0.6, 0.8, 1.0]\nplot_learning_curve(fullCLF , X_all_train, y_all_train, 'f1', plot_y_limits, plot_y_ticks)\n\n\n# Plot learning curve for ROC-AUC score\n\n# Specify the y-axis limits for the learning curve plot, related to ROC_AUC score    \nplot_y_limits = [0.7, 1.0]\n# Specify the y-axis ticks for the learning curve plot, related to ROC_AUC score   \nplot_y_ticks = [0.8, 0.9, 1.0]\nplot_learning_curve(fullCLF , X_all_train, y_all_train, 'roc_auc', plot_y_limits, plot_y_ticks)","c16dbe1b":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc, precision_score, recall_score, f1_score\n\n# Plot the confusion matrix \ndef plotConfusionMatrix(y_true, y_pred):\n    confmat = confusion_matrix(y_true=y_true, y_pred=y_pred)\n    fig, ax = plt.subplots(figsize=(2.5, 2.5))\n    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n    for i in range(confmat.shape[0]):\n        for j in range(confmat.shape[1]):\n            ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n\n    plt.xlabel('predicted label')\n    plt.ylabel('true label')\n    plt.tight_layout()\n    print(\"----> Confusion Matrix:\")\n    plt.show()\n    \n    \n### Plot the ROC curve as performance evaluator\ndef plotROCGraph(y_test, y_score):\n    fpr, tpr, threshold = metrics.roc_curve(y_test, y_score)\n    roc_auc = metrics.auc(fpr, tpr)\n    print('ROC-AUC score: ', roc_auc)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True-positive rate')\n    plt.xlabel('False-positive rate')\n    print(\"\\n----> ROC graph:\")\n    plt.show()\n    \n# Calculate Recall, Precision and F1-score \ndef getRecallPrecisionF1Score(y_true, y_pred):\n    print(\"----> Performance metrics:\\n\")\n    print('Precision: %.3f' % precision_score(y_true=y_true, y_pred=y_pred))\n    print('Recall: %.3f' % recall_score(y_true=y_true, y_pred=y_pred))\n    print('F1: %.3f' % f1_score(y_true=y_true, y_pred=y_pred))\n    \n\n# Test model on test set, and visualize performance\ndef testModel(clf, X_train, X_test, y_train, y_test):\n\n    print(\"\\n\\n---------------- Testing classifier:\\n\")\n\n    y_pred = clf.predict(X_test)\n    y_score = clf.fit(X_train, y_train).decision_function(X_test)\n\n    # Get Performance Evaluation Metrics\n    plotConfusionMatrix(y_test, y_pred)\n    getRecallPrecisionF1Score(y_test, y_pred)\n    plotROCGraph(y_test, y_score)\n\ntestModel(fullCLF, X_all_train, X_all_test, y_all_train, y_all_test)\n","aada005a":"# Bootstrapping on test data\nbootstrap_estimate_and_ci(fullCLF, X_all_test, y_all_test)\n","aeaf62d2":"# Train Extra Trees Classifier with Randomized Search to determine feature importance\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\ndef RandomForest(X, y):\n    \n    # Specify the classifier\n    clf = ExtraTreesClassifier(random_state=42)\n\n    # For additional info on the hyperparameters grid below,\n    # see https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n    param_grid = {\n     'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n     'max_features': ['auto', 'sqrt'],\n     'min_samples_leaf': [1, 2, 4],\n     'min_samples_split': [2, 5, 10],\n     'max_leaf_nodes': [2, 3, 4, 5, 6],\n     'bootstrap': [True, False],\n     'n_estimators': [100, 200, 300, 1000]\n    }\n\n    # We opt for the F1-score as a performance metric for our binary classifier.\n    # In case of multiclass classification, see https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html\n    scores = ['f1']\n\n    # Different performance metrics  can also be used to select our most optimal random forest classifier\n    # scores = ['accuracy', 'precision', 'recall', 'roc_auc]\n\n    # Perform seperate random search (in case multiple scores have been selected)\n    for score in scores:\n\n        random_search = RandomizedSearchCV(estimator=clf,\n                        param_distributions=param_grid,\n                        scoring=score, \n                        cv=5,\n                        n_jobs=-1,\n                        random_state = 42)\n\n\n        random_search.fit(X, y)\n\n        # Pick best estimator based on a specific score (in case of multiple scores)\n        # if(score == 'f1'):\n\n        return random_search.best_estimator_\n","1047e33b":"# Display features sorted by importance\n\nclfRandomForest = RandomForest(X_all_train, y_all_train)\n\ndataTemp = data.drop(columns=[\"PK\", \"typeStudy\", \"Outlier\"]) \n\nprint(\"\\n-------- Features sorted by importance:\")\n\ndictionaryFeatures = {}\n\nfor name, score in zip(dataTemp.columns, clfRandomForest.feature_importances_):\n        dictionaryFeatures[name] = np.round(score, 2)\n\ndict(sorted(dictionaryFeatures.items(), key=lambda item: item[1], reverse=True))       ","54d44400":"## 3. Outliers\/Anomalies\n\nOutliers or anomalies can be classified into three different categories (1,12), and depending on their nature this will influence the type of machine-learning model and learning strategy that you will adopt:\n\n1.\t**Point anomalies**. If an individual data instance can be considered as anomalous with respect to the rest of data, then the instance is termed as a point anomaly. A good example of point anomalies can be found in credit card fraud detection. A transaction for which the amount spent is very high compared to the normal range of expenditure for that person will be a point anomaly (considering only one feature, being amount spent).\n\n2.\t**Contextual anomalies**. If a data instance is anomalous in a specific context (but not otherwise), then it is termed as a contextual anomaly \u2013 also referred to as conditional anomaly. For instance, the amount of rainfall in a certain month could be considered as average for the whole year, but highly exceptional for that particular month. \n\n3.\t**Collective anomalies**. If a collection of related data instances is anomalous with respect to the entire data set, it is termed as a collective anomaly. The individual data instances in a collective anomaly may not be anomalies by themselves, but their occurrence together as a collection is anomalous. For example, a sequence of actions (e.g., ssh, buffer-overflow, ftp) on a web server could mark a typical web-based attack by a remote machine. Each of these individual actions themselves are not considered anomalous, but their combination or collection makes it an anomaly. \n\n\nIn our case, outliers are defined as TOF values that are considered an inaccurate represenatation of a patient's neuromuscular status, and can thus be seen as point anomalies. As mentioned, TOF values are a patient's muscular response to an electrical stimulation pattern. Thus, when a patient has just received a (substantial) dose of a neuromuscular blocking agent (e.g. Rocoronium), then we expect a very low TOF value (<0.1). On the other hand, when a certain time period has passed (depending on the dose), then we expect TOF to rise, and eventually reach \u00b11.0. \n\nThe evaluation for outliers of the TOF observations was performed (manually) by a team of anesthesiologists at UZ Brussels, who performed the evaluation independently from one another. The classification of outliers was afterwards compared and corresponded to an 98% agreement on the labelling of TOF outliers. In total, 203 of the 21,891 observations were identified as outliers. These anomalies were detected in the TOF observations of 76 patients.\n\nTo illustrate how these outliers migth look, let's take a look at the TOF values of several patients. The TOF outliers in these plots have been given a red marking.","5d42a611":"### Bootstrapping and confidence intervals\n\nIn the code below, we adopt a bootstrap method to estimate the performance metrics of our classifier on a hypothetically larger training dataset and construct their corresponding confidence intervals.\n\nBriefly, bootstrapping allows us to use our existing dataset to create new bootstrap datasets by drawing from the original dataset with replacement, i.e., data points can appear more than once in a bootstrap generated dataset. In the code below, we adopt the .632 bootstrap method, first introduced by (13).\n\nThe main goal of adopting bootstrapping is to arrive at confidence intervals for our performance metrics. Calculating a confidence interval will give us a range of values that\u2019s likely to include the specific performance metric with a certain degree of confidence. Hence, confidence intervals allow us to gain a better understanding of the variability our estimate is likely to have. For instance, say that we obtain a very wide confidence interval for our F1-score, then we know that the actual or real F1-score could vary substantially with our obtained F1-score result. So, in summary, confidence intervals allow us to estimate the variability of our performance metrics and tell us how confident we can be about these results if we want to generalize them to larger datasets. \n","1ca01cd9":"## 1. Dataset\n\nAll data for the present study has been collected during a prospective open-label bi-centric clinical trial (Clinical Trial Identifier: NCT04518761) taking place at the University Hospital of Brussels (Belgium). Patient identifiers have been anonymized, and the trial was conducted in accordance with the established protocol after approval by the Medical Ethical Committee of the hospital (approved 27th of May 2020). It followed current Good Clinical Practice guidelines and applicable law(s), as well as adhered to the applicable CONSORT guidelines. \n\nThe dataset includes 136 patients, with a total of 21,891 intraoperative observations. In total, 85 patients were added retrospectively and 51 prospectively.\n\nLet us begin by importing the dataset and the numpy and pandas libaries. Also, let's have a look at the features in the dataset. \n","4167f651":"### Bootstrapping the test data \n\nNow let us again bootstrap the testdata, so we can generate estimates and confidence intervals in addition with the obtained results from our test data. The results and confidence intervals again seem to indicate that our ROC-AUC score may be slighly overestimated if we would generalize our model to a larger dataset. On the other hand, bootstrapping assings a higher F1-score than we obtained on the test data, indicating that we will probably obtain a better score when we would generalize our model to a larger population. ","400eda82":"## 7. Model testing\n\nIn the code below, we are going to evaluate the performance of our model on the test dataset, which consists of all our prospectively collected patients. As you can see in the code below, several supportive functions have also been added to aid with the visualization of the performance of our model (e.g., confusion matrix, ROC-AUC graph).\n","a51939d5":"## 9. Discussion\n\nWhile our team recognizes that the current model is built upon a limited dataset and that further development is necessary in order to arrive at a clinically deployable outlier detection algorithm, the performance of the CSLR classifier and the related engineered features are promising for a possible clinical application. \n\nVarious research efforts (11,14,15) have highlighted that neuromuscular monitoring is still sub optimally and reluctantly adopted by practicing anaesthesiologists, among others, due to a low perceived usefulness and reliability of monitoring devices. Even when effectively available on request, perceived unreliability due to artifactual recordings has been shown to be a substantial barrier and a technical hindrance towards consistent monitoring adoption (11). \n\nThe anchoring of such outlier analysis to the clinical context of neuromuscular blockade monitoring is yet to be done practically and prospectively. Our team anticipates that studies and developments such as these will improve both neuromuscular monitoring adoption and reduce clinical errors when embedded into anaesthesia monitors. **We hope by sharing our efforts in the open-source space, that this will eventually aid others in achieving improved clinical practices and arrive at improved patient wellbeing.**  ","10023b8b":"# Cost-Sensitive Logistic Regression for Outlier Detection\n\n### <i>Authors: Michael Verdonck, Hugo Carvalho<i>\n\n## Goal of this notebook\n\nThis notebook aims to provide an introduction into the detection of anomalies. Outlier analysis or anomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior (1). It is an important field within machine-learning that has been researched and implemented within diverse research areas and application domains.  Many outlier analysis techniques have been applied for specific domains, while other are more generic.\n\nIn this notebook, we adopt a generic outlier analysis technique \u2013 i.e., cost-sensitive logistic regression \u2013 and demonstrate how we tune, train, and test this machine-learning algorithm. The domain in which we apply anomaly detection is neuromuscular monitoring, a subdomain within anesthesia. Additionally, this notebook is accompanied by a public dataset on neuromuscular monitoring. We hope this notebook and dataset will benefit data engineers, clinicians and researchers.\n\n## Domain: Anesthesia and Neuromuscular monitoring \n\nEach year, in 60% of all surgical procedures taking place under general anesthesia worldwide, neuromuscular-blocking drugs (NMDs) are administered to patients by their attending anesthesiologists (2). Given that approximately 234,4 million surgical procedures are being performed worldwide on an annual basis, this equates to the exposure of 140,64 million patients to NMDs per year. Commonly referred to as \u201ccurares\u201d, these drugs serve an ultimate pharmacodynamic purpose: the paralyzation of skeletal muscle fibers and so impeding (in)voluntary muscular contractions. This molecular effect translates into a multitude of both surgical and anesthetic benefits: it facilitates airway manipulations such as endotracheal intubation; eases protective lung ventilation strategies; enhances surgical conditions; among others (3). \n\nSince NMDs paralyze both primary and secondary breathing muscles, the close perioperative monitoring of their clinical effect is paramount in order to assure that patients are only allowed to awake from anesthesia when their muscular breathing capacity is deemed acceptable by international anesthetic standards. As shown by recent meta-analyses and prospective multicenter analyses (4,5), not doing so has the deleterious potential to significantly increase post-operative pulmonary complications, as well as its accompanying healthcare costs. \n\nTo measure the effects of NMDs, anesthesiologists turn to **neuromuscular monitoring**, a technique that involves the electrical stimulation of a motor nerve and monitoring the response of the muscle supplied by that nerve (6). The electrical stimulation pattern resulting from neuromuscular monitoring is the Train of Four (TOF). A TOF minimum of 0.9 is internationally used as the cut-off benchmark to green-flag an extubation attempt of a patient that has received NMDs (7). The diagnosis of a TOFR under 0.9 on an extubated patient is referred to as residual neuromuscular block. \n\nUnfortunately, **the incidence of residual neuromuscular block is still unacceptably high** and estimated at 20- 40% (8). In spite of the recent international academic and medical expert emphasis on the widespread necessity of objective neuromuscular monitoring (7,9), its use remains suboptimal. The effective reduction of such high rates is primordial, as residual neuromuscular block is responsible for an increased incidence of postoperative pulmonary complications (5).\n\nArtifactual recordings of TOF monitoring devices have been recorded in various clinical studies (10) and despite the documentation of such aberrancy, a definite physiological explanation for the variability is lacking. Such **unpredictable and unexplainable erroneous measurements have been reported to result in a reluctant adoption of neuromuscular monitoring devices** and can a great deal of early cynicism on the part of the clinical providers (11).\n\nThe authors of this notebook are part of a research group at the University Hospital Brussels that aims to improve neuromuscular monitoring techniques by adopting AI and machine-learning techniques. One aspect where we aim to make neuromuscular monitoring safer is by identifying outlier during the intraoperative monitoring of patients that received NMDs.\n\n### Summary\n\nIn this notebook, we will develop a machine-learning model to identify anomalous observations during the monitoring of patients that have been given muscular paralysis drugs. Our long-term purpose is to filter out anomalies, so anesthesiologists have reliable monitoring data when supporting surgery.  ","9d10aab4":"## 6. Model Training\n\nNow that we have fine-tuned our hyperparameters, we are going to re-train our chosen estimator on the entire training set with stratified k-fold cross validation. This additional training round will (hopefully) result in better optimized model parameters.\n\nAs for evaluation metrics, we are now going to also include precision and recall, as to get a better understanding of how our model performs on the training data. ","6ae2d9ea":"The confidence intervals obtained through bootstrapping are reasonably wide, where also the estimates of our performance metrics are rather similar to the performance metrics obtained from our train dataset. The largest discrepancy however can be observed with the ROC-AUC score. While the ROC-AUC on our training set is approximately 0.9, the bootstrap estimate equals only 0.82, with a corresponding confidence interval of [0.79, 0.86]. This indicates that probably the ROC-AUC of our training data is overexaggerated.","f804d254":"## 5. Cost-sensitive logistic regression and hyperparameter tuning\n\nSince outliers are rare instances in the data, there is a class imbalance where the distribution between the normal observations and outliers is significantly skewed. To overcome the issue of class imbalance, we adopted a cost-sensitive learning technique, where the objective function of the classification algorithm is modified in order to weight the classification errors in a differential way for the normal and the less frequent class. \n\nThis refers concretely to Cost-Sensitive Logistic Regression (CSLR), where a class weighting configuration is used to influence the amount of logistic regression coefficients that are updated during training. The weighting penalizes the model less for errors made on instances from the normal class, while maintaining a larger penalty for errors made on instances from the rare class. The result is a version of logistic regression that performs better on imbalanced classification tasks (12).\n\n### Hyperparameter tuning\n\nIn the code below, we are tuning for two hyperparameters: the regularization parameter 'C' and the 'class_weight' hyperparameter that regulates the penalty for errors made on rare class instances. \n\nAs an example of the 'class_weight' hyperparameter, if our class distribution of the training dataset is a 1:50 ratio for the rare class to the majority class, then the inversion of this ratio could be used with 1 for the majority class and 50 for the rare class: {0:50,1:1}. \n\nNote that the 'balanced' argument given to 'class_weight' adopts the values of y to automatically adjust weights inversely proportional to class frequencies in the input data. For more information on this topic, see: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html. \n\nAdditionally, we have adopted random search as our optimization method, where we picked the best estimator based on the F1-score. As for an additional evaluation criteria, we have also printed out the result of the ROC-AUC score. We adopted the default 5-fold stratified cross validation technique to perform the hyperparameter tuning.   \n","354e4294":"## 10. References\n\n1. \tChandola V, Banerjee A, Kumar V. Anomaly detection: A survey. ACM Comput Surv. 2009 Jul 1;41(3):1\u201358. \n\n2. \tWeiser TG, Regenbogen SE, Thompson KD, Haynes AB, Lipsitz SR, Berry WR, et al. An estimation of the global volume of surgery: a modelling strategy based on available data. The Lancet. 2008 Jul;372(9633):139\u201344. \n\n3. \tKang W-S, Oh C-S, Rhee KY, Kang MH, Kim T-H, Lee SH, et al. Deep neuromuscular blockade during spinal surgery reduces intra-operative blood loss: A randomised clinical trial. Eur J Anaesthesiol. 2020 Mar;37(3):187\u201395. \n\n4. \tCarvalho H, Verdonck M, Cools W, Geerts L, Forget P, Poelaert J. Forty years of neuromuscular monitoring and postoperative residual curarisation: a meta-analysis and evaluation of confidence in network meta-analysis. Br J Anaesth. 2020; \n\n5. \tKirmeier E, Eriksson LI, Lewald H, Jonsson Fagerlund M, Hoeft A, Hollmann M, et al. Post-anaesthesia pulmonary complications after use of muscle relaxants (POPULAR): a multicentre, prospective observational study. Lancet Respir Med. 2019 Feb;7(2):129\u201340. \n\n6. \tNaguib M, Brull SJ, Johnson KB. Conceptual and technical insights into the basis of neuromuscular monitoring. Anaesthesia. 2017 Jan;72:16\u201337. \n\n7. \tNaguib M, Brull SJ, Kopman AF, Hunter JM, F\u00fclesdi B, Arkes HR, et al. Consensus Statement on Perioperative Use of Neuromuscular Monitoring: Anesth Analg. 2018 Jul;127(1):71\u201380. \n\n8. \tMurphy GS, Kopman AF. To Reverse or Not to Reverse? Anesthesiology. 2016;125(4):611\u2013614. \n\n9. \tBrull SJ, Naguib M, Miller RD. Residual Neuromuscular Block: Rediscovering the Obvious: Anesth Analg. 2008 Jul;107(1):11\u20134. \n\n10. \tSuzuki T, Fukano N, Kitajima O, Saeki S, Ogawa S. Normalization of acceleromyographic train-of-four ratio by baseline value for detecting residual neuromuscular block. Br J Anaesth. 2006 Jan;96(1):44\u20137. \n\n11. \tTodd MM, Hindman BJ, King BJ. The Implementation of Quantitative Electromyographic Neuromuscular Monitoring in an Academic Anesthesia Department: Anesth Analg. 2014 Aug;119(2):323\u201331. \n\n12. \tAggarwal CC. Outlier analysis. In: Data mining. Springer; 2015. p. 237\u2013263. \n\n13. \tEfron B. Estimating the error rate of a prediction rule: improvement on cross-validation. J Am Stat Assoc. 1983;78(382):316\u2013331. \n\n14. \tBaillard C, Clec\u2019h C, Catineau J, Salhi F, Gehan G, Cupa M, et al. Postoperative residual neuromuscular block: a survey of management. Br J Anaesth. 2005 Nov;95(5):622\u20136. \n\n15. \tNaguib M, Kopman AF, Lien CA, Hunter JM, Lopez A, Brull SJ. A Survey of Current Management of Neuromuscular Block in the United States and Europe: Anesth Analg. 2010 Jul;111(1):110\u20139. \n","19f88b25":"## 4. Train- and testset\n\nAs mentioned above, our dataset included patients that were added retrospectively and prospectively. For the training of our models, we will use retrospectively collected patients only. Test data will then only include patients that were added prospectively. The table below gives some additional information on our retrospective and prospective datasets:\n\n|| **Prospective** | **Retrospective** | **Total** |\n| --- | --- | --- | --- | \n| Number of patients | 51 | 85 | 136 |\n| Sex, female | 34 | 49 | 83 |\n| Age, years (mean \u00b1 Stand. Dev.)| 52.39 \u00b1 15.91 | 53.74 \u00b1 22.87 | 53.95 \u00b1 20.03 |\n| Number of TOF observations| 8,058 | 13,833 | 21,891 |\n| Number of TOF outliers| 58 | 145 | 203 |\n\nIn the code below, we split our dataframe into the train- and testset accordingly. Additionally, we preprocess our data by performing standard scaling on the trianing datasets. ","6cd38c44":"## 8. Feature importance \n\nAs to better grasp which features influence the detection of anomalies in our dataset, we are going to train a random forest and adopt this classifier to create a ranking of features based on their importance. \n\nAs the results depict, the 5 most important features are engineered features: (1) delta_TOF, (2) delta_TOF_relative, (3) EMA_TOF_Long, (4) SMA_TOF and (5) EMA_TOF_Short. These results confirm that our efforts in engineering our own features was not in vain for aiding the detection of anomalies.  \n","d84656d5":"### Learning curves \n\nIn the code below, we plot the learning curves during model training and validation, for both the F1-score and ROC-AUC performance metrics. Learning curves allow us to assess if our models suffer from high variance (i.e. overfitting) or bias (i.e. underfitting) by plotting the model training and validation performances as functions of the training set size. \n\nAs we can observe from our plots, there seems to be a good bias-variance trade-off, indicating that our model is not overfitting nor underfitting the training data.","450ff5e3":"## 2. Features\n\nThe features of our dataset can be differentiated into **four different categories**: *drug-related features, patient-related features, TOF-related features, and engineered features*. While the data belonging to the first three categories were obtained through pre- and intraoperative monitoring and measuring, the category of engineered features consists of a set of features that were specifically engineered to capture additional information that could aid in the identification of outliers. Note, the 'PK' column refers to the primaryKey identifier of every patient. \n\n| **Features** | **Description** |\n| --- | --- | \n| **Drug Related Features**  | \n| ExpSevo | Expiratory sevoflurane (value unit: %; concentration of the gas in the expired air of the patient) | \n| InspSevo | Inspiratory sevoflurane (value unit: %; concentration of the gas in the expired air of the patient) | \n| Esmeron | Rocoronium (value unit: mg) | \n| Bridion | Sugammadex, reversal agent of Esmeron (value unit: mg) | \n| **TOF-related features**  | \n| TOF | Train-Of-Four | \n| T1 | First twitch of Train-Of-Four response | \n| Count | Number of twitches of Train-Of-Four response | \n| **Patient-related features**  | \n| Temp | Patient temperature monitored during surgery (value unit: degrees Celsius) | \n| Age | Age of patient (value unit: years) | \n| Sex | Sex of patient (male = 0, female = 1)| \n| BMI | Body mass index of patient (value unit: kg\/m2) | \n| **Engineered features**  | \n| ratio_Esmeron_Time | Ratio between first dose of Esmeron given and the number of minutes that have passed since first dose | \n| ratio_Bridion_Time | Ratio between first dose of Bridion given and the number of minutes that have passed since first dose | \n| ratio_ExpSevo_Time | Ratio between first dose of ExpSevo given and the number of minutes that have passed since first dose | \n| ratio_InspSevo_Time | Ratio between first dose of InspSevo given and the number of minutes that have passed since first dose | \n| SMA_TOF | Simple moving average of TOF values  (window of 3) | \n| EMA_TOF_Short | Exponential moving average of TOF, short window (window of 3) | \n| EMA_TOF_Long | Exponential Moving Average of TOF, long window (window of 10) | \n| delta_TOF | Difference between a TOF value and the previous observed TOF value (t-1) | \n| ratio_TOF | Ratio of a TOF observation and the arithmetic mean of all observed TOF values | \n\n"}}