{"cell_type":{"757a2482":"code","67b4a254":"code","ae71fcc5":"code","8638dc49":"code","497197db":"code","1082fb5d":"code","8e5b97c6":"code","244a01f2":"code","b8079371":"code","4cb78c96":"code","907d095e":"code","f651588c":"code","d483275f":"code","6b72cd01":"code","69b5e503":"code","b146488f":"code","fa8e57fc":"code","9fd82840":"code","baed5833":"code","5dcc2269":"code","fb509013":"code","075c872d":"code","a7be61b5":"code","4ad192cc":"markdown","6aa37838":"markdown","b8f7e0d9":"markdown","6b3bc0ba":"markdown"},"source":{"757a2482":"import numpy as np \nimport pandas as pd \n","67b4a254":"data1 = pd.read_csv(\"..\/input\/SW_EpisodeIV.txt\",delim_whitespace = True,header = 0,escapechar='\\\\')\ndata2 = pd.read_csv(\"..\/input\/SW_EpisodeV.txt\",delim_whitespace = True,header = 0,escapechar='\\\\')\ndata3 = pd.read_csv(\"..\/input\/SW_EpisodeVI.txt\",delim_whitespace = True,header = 0,escapechar='\\\\')\ndata = pd.concat([data1,data2,data3],axis = 0)\ndata.head()","ae71fcc5":"import re\nimport nltk\nfrom nltk.corpus import stopwords #To Remove the StopWords like \"the\",\"in\" ect\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer #lemmatize the word for example \"studies\" and \"studying\" will be converted to \"study\"","8638dc49":"print(PorterStemmer().stem(\"trouble\"))\nprint(PorterStemmer().stem(\"troubling\"))\nprint(PorterStemmer().stem(\"troubled\"))","497197db":"def unigram(data):\n    text = \" \".join(data)\n    CleanedText = re.sub(r'[^a-zA-Z]',\" \",text)\n    CleanedText = \" \".join([WordNetLemmatizer().lemmatize(word) for word in nltk.word_tokenize(CleanedText) if word not in stopwords.words(\"english\") and len(word) > 3])\n    return CleanedText","1082fb5d":"CleanedText = unigram(data['dialogue'])","8e5b97c6":"from wordcloud import WordCloud\n%matplotlib inline\nimport matplotlib.pyplot as plt\nwordcloud = WordCloud(random_state=21).generate(CleanedText)\nplt.figure(figsize = (30,15))\nplt.imshow(wordcloud,interpolation = 'bilinear')\nplt.axis(\"off\")\nplt.show()","244a01f2":"def ngrams(data,n):\n    text = \" \".join(data)\n    text1 = text.lower()\n    text2 = re.sub(r'[^a-zA-Z]',\" \",text1)\n    text3 = \" \".join([WordNetLemmatizer().lemmatize(word) for word in nltk.word_tokenize(text2) if word not in stopwords.words(\"english\") and len(word) > 2])\n    words = nltk.word_tokenize(text3)\n    ngram = list(nltk.ngrams(words,n))\n    return ngram","b8079371":"ngram = ngrams(data['dialogue'],2)\nngram[1:10]","4cb78c96":"\"_\".join(ngram[0])","907d095e":"for i in range(0,len(ngram)):\n    ngram[i] = \"_\".join(ngram[i])","f651588c":"Bigram_Freq = nltk.FreqDist(ngram)","d483275f":"bigram_wordcloud = WordCloud(random_state = 21).generate_from_frequencies(Bigram_Freq)\nplt.figure(figsize = (50,25))\nplt.imshow(bigram_wordcloud,interpolation = 'bilinear')\nplt.axis(\"off\")\nplt.show()","6b72cd01":"ngram = ngrams(data['dialogue'],3)","69b5e503":"for i in range(0,len(ngram)):\n    ngram[i] = \"_\".join(ngram[i])","b146488f":"Trigram_Freq = nltk.FreqDist(ngram)","fa8e57fc":"trigram_wordcloud = WordCloud(random_state = 21).generate_from_frequencies(Trigram_Freq)\nplt.figure(figsize = (50,25))\nplt.imshow(trigram_wordcloud,interpolation = 'bilinear')\nplt.axis(\"off\")\nplt.show()","9fd82840":"lda_data = []\nfor i in range(0,len(data)):\n    lda_data.append(data.iloc[i,]['dialogue'])\n    ","baed5833":"import string\nexclude = set(string.punctuation)\ndef clean_doc(doc):\n    stop_free = \" \".join([i for i in doc.lower().split() if i not in stopwords.words(\"english\")])\n    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n    normalized = \" \".join([WordNetLemmatizer().lemmatize(word) for word in nltk.word_tokenize(punc_free)])\n    return normalized","5dcc2269":"doc_clean = [clean_doc(doc).split() for doc in lda_data] \ndoc_clean[0]","fb509013":"import gensim\nfrom gensim import corpora\n#Creating the term dictionary of our courpus, where every unique term is assigned an index\ndictionary = corpora.Dictionary(doc_clean)\n#Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above\ndtm = [dictionary.doc2bow(doc) for doc in doc_clean]","075c872d":"# Creating the object for LDA model using gensim library\nLda = gensim.models.ldamodel.LdaModel\n#num_topics = number of topics you want to extract from the corpus\nldamodel = Lda(dtm, num_topics=5, id2word = dictionary, passes=50)","a7be61b5":"print(ldamodel.print_topics(num_topics=5, num_words=5))","4ad192cc":"Disadvantages of using Stemming","6aa37838":"We have to combine the two words for better visualisation","b8f7e0d9":"**Cleaning the Data**\nStep 1 : We will join the data\nStep 2 : we will  remove the punctuations,numbers which will not provide any information\nStep 3 : We will remove the words whose len is less than 3 and also the stopwords","6b3bc0ba":"We Will Load the data"}}