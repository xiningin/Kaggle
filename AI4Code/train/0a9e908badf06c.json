{"cell_type":{"dd96e3ca":"code","aa2ceeb1":"code","e4487a60":"code","eefc26a3":"code","f5d61ae7":"code","6577ce69":"code","880fecbf":"code","4869ad9c":"code","c84ba730":"code","e8c6f312":"code","6792f6f1":"code","302bb41a":"code","d63eb8b4":"code","10ad6518":"code","06b625a4":"code","b62a70d1":"code","5af4b4ca":"code","8a0f2b01":"code","908c7495":"code","654af8d5":"code","c937a8dc":"code","d2981ef4":"code","8fd6d76e":"code","d3b48c6f":"code","2e22b50c":"code","f1975b18":"code","06174d1b":"code","9ecdabf0":"code","619eb610":"code","9254e076":"code","f02a9758":"code","ccc5de43":"code","53727fcc":"code","df174a79":"code","649ffaab":"code","58c82405":"code","6cb036e7":"code","69106d51":"code","b16ee6b9":"code","e49cfd2c":"code","7e59993f":"code","6ee4e512":"code","901122a0":"code","6cb10831":"code","62b6cf60":"code","55c7228b":"code","0a61f3e1":"code","f5f1984e":"code","09d1bd7f":"code","f0bad914":"code","de7c0a61":"code","be93399a":"code","bc644d1e":"code","fe87dfd7":"code","4dde95f8":"code","461a161c":"code","e1534391":"code","4e2788af":"code","b1eb71ba":"code","c05c619c":"code","5e583e7a":"code","15de407b":"code","03f7273a":"code","86df2cc2":"code","77906b1e":"code","d7da68b7":"code","00bdbc4e":"code","3785c907":"code","cb4f3e1b":"code","124012ce":"code","2b08fc85":"code","9c705018":"code","2beddf06":"code","0de432df":"code","14f140bd":"code","f7c3ca2b":"code","403a96d7":"code","53c1af10":"code","cfefbb82":"code","43c834f5":"code","a3112480":"code","7384cef4":"code","e57b5a24":"markdown","60f375fa":"markdown","1f4f9cf8":"markdown","1d9e78e1":"markdown","bb413753":"markdown","af16d42c":"markdown","4c964666":"markdown","66ab13ec":"markdown","b6800f91":"markdown","ec99b575":"markdown","af5830e1":"markdown","a4e30e3f":"markdown","1e07ab03":"markdown","3b46c3c0":"markdown","6e593b30":"markdown","db95e684":"markdown","1dfa969e":"markdown","5181aa22":"markdown","ea01d6b1":"markdown"},"source":{"dd96e3ca":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","aa2ceeb1":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\n\nimport pickle\nimport time\nfrom bs4 import BeautifulSoup\n\nfrom string import punctuation\n\nimport re\nimport os\nimport tqdm\n\nimport pickle\nimport sqlite3\nimport warnings\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport xgboost as xgb\nimport tensorflow as tf\nfrom sklearn import metrics\n\nfrom datetime import datetime\nfrom bs4 import BeautifulSoup\nfrom wordcloud import WordCloud\n\n\nfrom gensim.models import Word2Vec\nfrom itertools import combinations\n\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nfrom nltk.tokenize import ToktokTokenizer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.corpus import words\nfrom nltk.stem import SnowballStemmer\nfrom nltk.tokenize import sent_tokenize\n\nfrom scipy.sparse import coo_matrix, hstack\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom skmultilearn.problem_transform import BinaryRelevance\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import f1_score,precision_score,recall_score,hamming_loss\n\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import PassiveAggressiveClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import model_selection\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import hamming_loss\nfrom sklearn.cluster import KMeans\nwarnings.filterwarnings(\"ignore\")\nstemmer = SnowballStemmer('english')\n\n%autosave 120\n\n\nimport logging\n\nfrom scipy.sparse import hstack\n\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('bmh')\n%matplotlib inline","e4487a60":"movies=pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/movie.csv')\ngen_scores = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/genome_scores.csv')\ngen_tags=pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/genome_tags.csv')\nlink = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/link.csv')\ntags = pd.read_csv('\/kaggle\/input\/movielens-20m-dataset\/tag.csv')\nmeta_data = pd.read_csv('\/kaggle\/input\/meta-data2csv\/mata_data2.csv')","eefc26a3":"movies.head()","f5d61ae7":"gen_scores.head()","6577ce69":"gen_tags.head()","880fecbf":"tags.head()","4869ad9c":"meta_data.head()","c84ba730":"#Dropping unnecessary column\nmeta_data = meta_data.drop(['Unnamed: 0'], axis=1)","e8c6f312":"combined_df=pd.merge(gen_scores, gen_tags, on=\"tagId\", how=\"left\")","6792f6f1":"combined_df=pd.merge(combined_df, meta_data, on=\"movieId\", how=\"left\")","302bb41a":"combined_df.head()","d63eb8b4":"#dropping unnecessary data\ncombined_df = combined_df.drop(['tmdbId'], axis=1)","10ad6518":"combined_df.head()","06b625a4":"df1 = combined_df[['movieId','overview','tag','relevance']]","b62a70d1":"df1.head()","5af4b4ca":"df1 = df1.dropna()","8a0f2b01":"#keeping rows with relevance > 0.7\ndf1 = df1[df1[\"relevance\"] > 0.7]\n","908c7495":"df1['tag'] = df1['tag'].astype(str)","654af8d5":"df2 = df1.groupby(\"movieId\")['tag'].apply(lambda tags: ', '.join(tags))","c937a8dc":"df2.head()","d2981ef4":"df2.reset_index()","8fd6d76e":"df_tags= pd.DataFrame({'movieId':df2.index, 'tags':df2.values})","d3b48c6f":"df_tags.head(5)","2e22b50c":"#dropping genres from movies data\nmovies.drop(columns=['genres'], inplace=True)","f1975b18":"#merging df_tags with movies data\nfinal_df = movies.merge(df_tags, on='movieId')","06174d1b":"final_df.head()","9ecdabf0":"#dropping unnecessary data from meta_data\nmeta_data.drop(columns=['imdbId', 'tmdbId'], inplace=True)","619eb610":"final_df = final_df.merge(meta_data,on='movieId')","9254e076":"final_df.head()\n","f02a9758":"#dropping all rows with any null value\nfinal_df.dropna()","ccc5de43":"final_df.head()","53727fcc":"final_df.isnull().sum()","df174a79":"!pip install scikit-multilearn\n","649ffaab":"#extracting columns for modelling purpose\ndata = final_df[['overview','tags']]","58c82405":"#creating data copy\ndata_copy = data.copy()","6cb036e7":"data.head()","69106d51":"#splitting tags into a list\ndata['tags'] = data['tags'].apply(lambda x: x.split(', '))","b16ee6b9":"data.head()","e49cfd2c":"#creating single list of all possible tags in data\nall_tags = [item for sublist in data['tags'].values for item in sublist]","7e59993f":"len(all_tags)","6ee4e512":"#extracting unique tags from all_tags\nmy_set = set(all_tags)\nunique_tags = list(my_set)\nlen(unique_tags)","901122a0":"flat_list = [item for sublist in data['tags'].values for item in sublist]\n\nkeywords = nltk.FreqDist(flat_list)\n\nkeywords = nltk.FreqDist(keywords)","6cb10831":"fig, ax = plt.subplots(figsize=(15, 10))\nkeywords.plot(100, cumulative=False)","62b6cf60":"#Removing orginal and mentor from flat_list\nflat_list2 = [] \ntwo_most_common = ['original','mentor']\n[flat_list2.append(x) for x in flat_list if x not in two_most_common]","55c7228b":"keywords2 = nltk.FreqDist(flat_list2)\n\nkeywords2 = nltk.FreqDist(keywords2)","0a61f3e1":"fig, ax = plt.subplots(figsize=(15, 10))\nkeywords2.plot(100, cumulative=False)","f5f1984e":"#storing most common 1000 words and creating a list\nfrequencies_words = keywords2.most_common(1000)\ntags_features = [word[0] for word in frequencies_words]","09d1bd7f":"len(tags_features)","f0bad914":"print(tags_features)","de7c0a61":"#defining function to ensure that tags are choosen only from tags_feature list\ndef most_common(tags):\n    tags_filtered = []\n    for i in range(0, len(tags)):\n        if tags[i] in tags_features:\n            tags_filtered.append(tags[i])\n    return tags_filtered","be93399a":"data['tags'] = data['tags'].apply(lambda x: most_common(x))\ndata['tags'] = data['tags'].apply(lambda x: x if len(x)>0 else None)","bc644d1e":"data.head()","fe87dfd7":"data.shape","4dde95f8":"data.dropna(subset=['tags'], inplace=True)","461a161c":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"can not \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub(r\"\\'\\n\", \" \", text)\n    text = re.sub(r\"\\'\\xa0\", \" \", text)\n    text = re.sub('\\s+', ' ', text)\n    text = text.strip(' ')\n    return text","e1534391":"data['overview'] = data['overview'].apply(lambda x: clean_text(x))","4e2788af":"token=ToktokTokenizer()","b1eb71ba":"punctuation","c05c619c":"punct = '!\"#$%&\\'()*+,.\/:;<=>?@[\\\\]^_`{|}~'","5e583e7a":"def strip_list_noempty(mylist):\n    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n    return [item for item in newlist if item != '']","15de407b":"def clean_punct(text): \n    words=token.tokenize(text)\n    punctuation_filtered = []\n    regex = re.compile('[%s]' % re.escape(punct))\n    remove_punctuation = str.maketrans(' ', ' ', punct)\n    for w in words:\n        if w in tags_features:\n            punctuation_filtered.append(w)\n        else:\n            punctuation_filtered.append(regex.sub('', w))\n  \n    filtered_list = strip_list_noempty(punctuation_filtered)\n        \n    return ' '.join(map(str, filtered_list))","03f7273a":"data['overview'] = data['overview'].apply(lambda x: clean_punct(x)) ","86df2cc2":"data.head()","77906b1e":"lemma=WordNetLemmatizer()\nstop_words = set(stopwords.words(\"english\"))","d7da68b7":"def lemitizeWords(text):\n    words=token.tokenize(text)\n    listLemma=[]\n    for w in words:\n        x=lemma.lemmatize(w, pos=\"v\")\n        listLemma.append(x)\n    return ' '.join(map(str, listLemma))\n\ndef stopWordsRemove(text):\n    \n    stop_words = set(stopwords.words(\"english\"))\n    \n    words=token.tokenize(text)\n    \n    filtered = [w for w in words if not w in stop_words]\n    \n    return ' '.join(map(str, filtered))","00bdbc4e":"data['overview'] = data['overview'].apply(lambda x: lemitizeWords(x)) \ndata['overview'] = data['overview'].apply(lambda x: stopWordsRemove(x))","3785c907":"text = data['overview']","cb4f3e1b":"vectorizer_train = TfidfVectorizer(analyzer = 'word',\n                                       min_df=0.0,\n                                       max_df = 1.0,\n                                       strip_accents = None,\n                                       encoding = 'utf-8', \n                                       preprocessor=None,\n                                       token_pattern=r\"(?u)\\S\\S+\", # Need to repeat token pattern\n                                       max_features=1000)","124012ce":"#fitting tfidf to text\nTF_IDF_matrix = vectorizer_train.fit_transform(text)","2b08fc85":"X1 = data['overview']\ny = data['tags']","9c705018":"multilabel_binarizer = MultiLabelBinarizer()\ny = multilabel_binarizer.fit_transform(y)","2beddf06":"X_train, X_test, y_train, y_test = train_test_split(X1, y, test_size = 0.2, random_state = 0) # Do 80\/20 split","0de432df":"vectorizer_X1 = TfidfVectorizer(analyzer = 'word',\n                                       min_df=0.0,\n                                       max_df = 1.0,\n                                       strip_accents = None,\n                                       encoding = 'utf-8', \n                                       preprocessor=None,\n                                       token_pattern=r\"(?u)\\S\\S+\",\n                                       max_features=1000)\n","14f140bd":"#fitting tfidf to x_train and x_test\nx_train = vectorizer_X1.fit_transform(X_train)\nx_test = vectorizer_X1.fit_transform(X_test)","f7c3ca2b":"def avg_jacard(y_true,y_pred):\n    '''\n    see https:\/\/en.wikipedia.org\/wiki\/Multi-label_classification#Statistics_and_evaluation_metrics\n    '''\n    jacard = np.minimum(y_true,y_pred).sum(axis=1) \/ np.maximum(y_true,y_pred).sum(axis=1)\n    \n    return jacard.mean()*100\n\ndef print_score(y_pred, clf):\n    print(\"Clf: \", clf.__class__.__name__)\n    print(\"Jacard score: {}\".format(avg_jacard(y_test, y_pred)))\n    print(\"Hamming loss: {}\".format(hamming_loss(y_pred, y_test)*100))\n    print(\"---\")    ","403a96d7":"dummy = DummyClassifier()\nsgd = SGDClassifier()\nlr = LogisticRegression()\nmn = MultinomialNB()\nsvc = LinearSVC()\nperceptron = Perceptron()\npac = PassiveAggressiveClassifier()\n\nfor classifier in [dummy, sgd, lr, mn, svc, perceptron, pac]:\n    clf = OneVsRestClassifier(classifier)\n    clf.fit(x_train, y_train)\n    y_pred = clf.predict(x_test)\n    print_score(y_pred, classifier)","53c1af10":"#MLP_Classifier\nmlpc = MLPClassifier()\nmlpc.fit(x_train, y_train)\n\ny_pred = mlpc.predict(x_test)\n\nprint_score(y_pred, mlpc)","cfefbb82":"#rfc = RandomForestClassifier()\n#rfc.fit(x_train, y_train)\n\n#y_pred = rfc.predict(x_test)\n\n#print_score(y_pred, rfc)","43c834f5":"#Confusion Matrix on y_test and y_pred\nfor i in range(y_train.shape[1]):\n    print(multilabel_binarizer.classes_[i])\n    print(confusion_matrix(y_test[:,i], y_pred[:,i]))\n    print(\"\")","a3112480":"#defining predict function on mlp classifier\ndef predict(m):\n    m = stopWordsRemove(m)\n    m_vec = vectorizer_X1.transform([m])\n    pred_prob = mlpc.predict_proba(m_vec)\n    t = 0.3\n    predp = (pred_prob >= t).astype(int)\n    #m_pred = classifier.predict(m_vec)\n    return multilabel_binarizer.inverse_transform(predp)","7384cef4":"for i in range(10):\n    k = X_test.sample(1).index[0]\n    print(\"overview: \", data_copy['overview'][k],\n          \"\\nPredicted tags: \", predict(X_test[k])),\n    print(\"Actual tags: \",data['tags'][k], \"\\n\")","e57b5a24":"# Importing Libraries","60f375fa":"# Loading Data","1f4f9cf8":"# Predicting Test Set","1d9e78e1":"# **Evaluating performance**","bb413753":"**Applying MultiLabelBinarizer to dependent Variable**","af16d42c":"Among all the classifiers MLP classifier has performed the best.","4c964666":"# Analysing Data","66ab13ec":"# Training Model ","b6800f91":"# lemmatising Data and removing stopwords","ec99b575":"# Applying TfidfVectorizer","af5830e1":"# Data Cleaning","a4e30e3f":"**Splitting data into train and test**","1e07ab03":"Clearly, now we have data that will not overfit the model","3b46c3c0":"we can see orginal and mentor is the most common and will result in overfitting the data.","6e593b30":"# Merging Data","db95e684":"# Data Visualisation","1dfa969e":"**Removing Punctuation**","5181aa22":"# Checking different Classifiers","ea01d6b1":"**Tokenising Data**"}}