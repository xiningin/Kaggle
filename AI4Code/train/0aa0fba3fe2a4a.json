{"cell_type":{"17af19fd":"code","a868a1d5":"code","961ab0bb":"code","77debaee":"code","7a05b045":"code","8581f836":"code","07ce274c":"code","6ad769cc":"code","324c62ae":"code","8fb5b24b":"code","5173b80e":"code","2fc05082":"code","12d60add":"code","91f366be":"code","e3641922":"code","b32b0320":"code","eb4aa1c1":"code","5ef2be7d":"code","8f3f5157":"code","af084eab":"code","da12abcc":"code","b6ecd898":"code","289ba103":"code","c3c3c3f4":"code","84a1c655":"markdown","e37ab9bf":"markdown","bc48b7cf":"markdown","c7e2e5ee":"markdown","38f9e50d":"markdown","95317d52":"markdown","b5e90d12":"markdown","e0f7de7e":"markdown","b45d4923":"markdown","4f7536f0":"markdown","a8606fb9":"markdown","104ab39f":"markdown","6d8f60e4":"markdown","8c6e6d09":"markdown","5a8a9d21":"markdown","997bca92":"markdown","d8eb4872":"markdown","22e8ab70":"markdown","c814d1fa":"markdown","40b2dc05":"markdown","14554e9f":"markdown","0b4654ab":"markdown","1d2aff7c":"markdown"},"source":{"17af19fd":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LinearRegression\nfrom numpy.polynomial.polynomial import polyfit","a868a1d5":"raw_data = pd.read_csv(\"..\/input\/college-basketball-dataset\/cbb.csv\")\nraw_data.columns","961ab0bb":"data_subset = raw_data.copy()\ndata_subset.drop(['ADJOE','ADJDE','BARTHAG','ADJ_T','WAB','POSTSEASON','SEED'],axis=1,inplace=True)","77debaee":"data_subset['Win%'] = data_subset['W']\/data_subset['G']\n\ndata_subset.drop(['W','G'],axis=1,inplace=True)","7a05b045":"data_subset.corr()['Win%'].sort_values()[:-1] #This removes Win%, which would otherwise be 100% correlated with itself","8581f836":"data_subset.plot.scatter(x = 'EFG_O',y='Win%');","07ce274c":"data_subset.plot.scatter(x = 'FTR',y='Win%');","6ad769cc":"data_subset.groupby(['CONF'])['Win%'].mean().sort_values(ascending=False).plot(kind='bar', figsize = (10,7));","324c62ae":"data_subset[data_subset['Win%'] == data_subset['Win%'].min()]","8fb5b24b":"data_subset[data_subset['Win%'] == data_subset['Win%'].max()]","5173b80e":"data_subset['YEAR'] = data_subset['YEAR'].astype(str)\n\ndummy_df = pd.get_dummies(data_subset)","2fc05082":"standard_df = pd.DataFrame(StandardScaler().fit_transform(dummy_df), columns = dummy_df.columns)\n\nstandard_df = standard_df.drop('Win%',axis=1)\n\npca = PCA(n_components=3)\n\npca_df = pd.DataFrame(pca.fit_transform(standard_df))\n\npca_df.columns = ['Feature1','Feature2','Feature3']\n\npca_df.head()","12d60add":"abs(pd.Series(pca.components_[0],index = standard_df.columns)).sort_values(ascending=False)[:5]","91f366be":"abs(pd.Series(pca.components_[1],index = standard_df.columns)).sort_values(ascending=False)[:5]","e3641922":"abs(pd.Series(pca.components_[2],index = standard_df.columns)).sort_values(ascending=False)[:5]","b32b0320":"regmodel = LinearRegression()\n\nregmodel.fit(X = pca_df, y = dummy_df['Win%'])\n\noutputs = regmodel.predict(pca_df)","eb4aa1c1":"# Fit with polyfit\nb, m = polyfit(dummy_df['Win%'], outputs, 1)\n\nplt.plot(dummy_df['Win%'], outputs, '.', alpha = 0.4)\nplt.plot(dummy_df['Win%'], b + m * dummy_df['Win%'], '-')\nplt.show()","5ef2be7d":"results_df = data_subset[['TEAM','YEAR','Win%']].copy()\n\nresults_df['prediction'] = outputs\n\nresults_df['difference'] = results_df['Win%'] - results_df['prediction']","8f3f5157":"results_df.sort_values('difference')[:5]","af084eab":"results_df.sort_values('difference',ascending=False)[:5]","da12abcc":"data_subset[data_subset.index == 367]","b6ecd898":"EFG_O_view = data_subset.groupby(round(data_subset['EFG_O'],0))['Win%'].mean()\nEFG_D_view = data_subset.groupby(round(data_subset['EFG_D'],0))['Win%'].mean()\nprint(EFG_O_view[EFG_O_view.index == 49])\nprint(EFG_D_view[EFG_D_view.index == 55])","289ba103":"data_subset[data_subset.index == 292]","c3c3c3f4":"print(EFG_O_view[EFG_O_view.index == 48])\nprint(EFG_D_view[EFG_D_view.index == 57])","84a1c655":"We'll use good-old fashioned Linear Regression to make predictions from our principal components.","e37ab9bf":"# What makes certain Colleges better than others at Basketball?\n\nThis dataset provided to us by Andrew Sundberg is fantastically prepared for us, so there's little we need to do in terms of data cleaning.\n\nHowever, while finding ensuring a clean and robust dataset is a large part of a data scientists role, another just-as-important part is the art of answering a relevant question.\n\nWe could use the dataset to find lots of historical trivia, but while it might be good to know, it's not something that I would consider particularly relevant. Instead, we'll be trying to find out which features correlate with success.\n\nIn short, what makes good College Basketball teams good?","bc48b7cf":"A visual comparison of the two highest and lowest scoring features shows the clear correlation present in one and not the other. However, despite this, there is still clear variance remains in the EFG_O graph, meaning that it alone cannot explain team success","c7e2e5ee":"These are some pretty big variations! Let's look at some individual cases","38f9e50d":"Meanwhile, the 2016 Oklahoma State team should have had a win percentage between 35%-45%, but only barely scraped into the 30% mark. Perhaps playing in the Big 12 means that normally good performance isn't always good enough","95317d52":"# Load and Process Data\n\nWe load the data and look at the column names - the Kaggle dataset description contains good metadata about what each one means","b5e90d12":"Their performances given their Effective Field Goal performance on Offense and Defence mean that the 2018 Portland State team should have had a Win% well below 50%, yet they managed to win 64.5% of their games!","e0f7de7e":"Given teams don't play in the same conferences, it is reasonable to expect this to have a significant effect on performance too. The B12 and ACC are the highest, the MEAC and SWAC are the lowest","b45d4923":"# Notable Results\n\nGiven our parameters, we can see which teams over-or-underperformed their seasons results, and by how much, and see how we correlate with the BARTHAG statistics","4f7536f0":"We also want to create our target column - Win percentage. Once we have it, we don't need to the two original columns any more","a8606fb9":"We can then plot a graph of predicted v actual Win % for each team in each season","104ab39f":"The first component is broadly connected to Offensive Statistics","6d8f60e4":"For this analysis I want to estimate performance from raw Basketball statistics (such as Effective Field Goal Percentage), and not estimated statistics such as BARTHAG. We drop these from the dataset before continuing","8c6e6d09":"And the final seems linked to Conference choice","5a8a9d21":"Finally, we can also use the data to find the best and worst seasons of all time:\n\nIn 2015, San Jose State and Grambling State lost all their conference games, while in the same year Kentucy recorded a 97.4% win rate. A quick look at the columns for shooting statistics shows a very clear story about what differentiates good teams from the bad","997bca92":"# Final Analysis","d8eb4872":"For Principal Components Analysis to be reliable, the data must first be Standardised. SKLearn comes with useful tools to do this for us, and we can apply the PCA model to our new dataset.","22e8ab70":"# Import Libraries\n\nAll the libraries used in this notebook a fairly standard - sometimes you don't need state of the art tech to do what you need","c814d1fa":"Find the top 5 underperformers and the top 5 overperformers","40b2dc05":"# Feature Selection\n\nWe have a lot of features to make our analysis from, but some of them (such as the shooting figures) are highly similar. It would make more sense if we found underlying features that describe these various similar ones.\n\nThankfully, a technique called Principal Components Analysis allows us to do just this, by finding the combinations of features that explain the most variance of the target column.","14554e9f":"# Conclusions\n\nThis notebook is not complete, and there will be more to come. However, what should be clear from it so far is the large amount of variance present in sports. It's what keeps us watching. Indeed, even though we know which features are mostly correlated with success, and which are not, there are still many cases where the top-level figures don't tell the whole story! Maybe your next notebook will help illuminate the reasons why...","0b4654ab":"# Exploratory Analysis\n\nWe can see that shooting categories are most highly correlated with Win%. There is also a clear order - Effective Field Goal % is first, followed by 2 point %, followed by 3 point %. Free-Throw rate is the relevant statistic with the lowest correlation of all.","1d2aff7c":"The second component seems more connected to Defensive Statistics"}}