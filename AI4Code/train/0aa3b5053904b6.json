{"cell_type":{"0151cf5b":"code","e5d3f2d9":"code","2916879b":"code","5d55e013":"code","00267e49":"code","951e439e":"code","f9ec611f":"code","13de1cc8":"code","e922ba4a":"code","23518579":"code","8f2f2446":"code","893403ee":"code","cba933cf":"code","f582ddb5":"code","4c35bf2b":"code","6963747b":"code","6515ddf8":"code","be6e0050":"code","96002cfe":"code","8c5277e6":"code","bcc46214":"code","64918a04":"code","fcc3a65c":"code","8afb7c3d":"code","0c6e9b37":"code","d9c5947f":"markdown","6dffff20":"markdown","72f09f1f":"markdown","fdc288c7":"markdown","35fae901":"markdown","57ee878f":"markdown","659e47fd":"markdown","5d30d1f5":"markdown","d12d3a20":"markdown","faa278c9":"markdown","e342208a":"markdown","3057de74":"markdown","691ce749":"markdown","35643fb1":"markdown","f0991b8d":"markdown","8455c583":"markdown"},"source":{"0151cf5b":"!pip install pyspark","e5d3f2d9":"import pyspark\nfrom pyspark.sql.functions import *\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.feature import StringIndexer\nfrom pyspark.sql import SparkSession\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nspark = SparkSession.builder.appName('Class').getOrCreate()","2916879b":"#header = True informs Spark that we have a header row in our table\n#inferSchema = True tells Spark to automatically guess the data types for each field\n\nwalmart_df = spark.read.csv('..\/input\/walmart-stock-price\/walmart_stock.csv',header=True,inferSchema=True)","5d55e013":"# Creates (or replaces) a temporary view of the table on memory to run SQL queries in this Spark session\n\nwalmart_df.createOrReplaceTempView(\"walmart\")","00267e49":"walmart_df.show(5)","951e439e":"walmart_df.printSchema()","f9ec611f":"walmart_df.describe().show()","13de1cc8":"## Use the output of describe method to create a dataframe\n\nfrom pyspark.sql.functions import format_number # Actually don't need to import as we have import * in the first cell\n\nsummary = walmart_df.describe()\nsummary.select(summary['summary'],\n              format_number(summary['Open'].cast('float'), 2).alias('Open'),\n              format_number(summary['High'].cast('float'), 2).alias('High'),\n              format_number(summary['Low'].cast('float'), 2).alias('Low'),\n              format_number(summary['Close'].cast('float'), 2).alias('Close'),\n              format_number(summary['Volume'].cast('float'), 2).alias('Volume')).show()","e922ba4a":"walmart_hv = walmart_df.withColumn('HV Ratio', walmart_df['High']\/walmart_df['Volume']).select('HV Ratio')\nwalmart_hv.show(10)","23518579":"#Method 1\n\nwalmart_df.orderBy('High', ascending = False).select(['Date']).show(1)","8f2f2446":"#Method 2\n\nwalmart_df.orderBy(walmart_df['High'].desc()).head()[0] # Extract the first column from the 'orderby' method","893403ee":"#Method 3\n\nspark.sql(\"SELECT date FROM walmart ORDER BY high DESC LIMIT 1\").show()","cba933cf":"#Method 1\n\nwalmart_df.agg({'Close' : 'mean'}).show()","f582ddb5":"#Method 2\n\nwalmart_df.select(mean('Close')).show()","4c35bf2b":"#Method 3\n\nspark.sql(\"SELECT ROUND(AVG(close), 2) FROM walmart\").show()","6963747b":"#Method 1\n\nwalmart_df.filter('Close < 70').count()","6515ddf8":"#Method 2\n\nspark.sql(\"SELECT COUNT(date) FROM walmart WHERE close < 70\").show()","be6e0050":"walmart_df.filter('High > 80').count() * 100\/walmart_df.count()","96002cfe":"walmart_df.corr('High', 'Volume')","8c5277e6":"year_df = walmart_df.withColumn('Year', year(walmart_df['Date']))","bcc46214":"# Method 1\n\nyear_df.groupBy('Year').max()['Year', 'max(High)'].orderBy('Year').show()","64918a04":"# Method 2\n\nyear_df.groupBy('Year').agg({'High':'max'}).orderBy('Year').show()","fcc3a65c":"month_df = walmart_df.withColumn('Month', month(walmart_df['Date']))","8afb7c3d":"# Method 1\n\nmonth_df.groupBy('Month').mean()['Month', 'avg(Close)'].orderBy('Month').show() # select only month & average close","0c6e9b37":"# Method 2\n\nmonth_df.groupBy('Month').agg({'Close': 'mean'}).orderBy('Month').show()","d9c5947f":"# Uncovering Insights From Dataframe","6dffff20":"# Loading the walmart_stock.csv","72f09f1f":"## What percentage of the time was the High greater than 80 USD ?\n\nIn other words, `(Number of High Days>80) \/ (Total Days in the dataframe)`","fdc288c7":"Let's see the **first 5 rows** and print the dataframe **schema**","35fae901":"## What is the correlation between High and Volume?","57ee878f":"# Apache Spark Dataframe Practice\n\nIn this notebook, we are going to get some insights on stock market data in Spark environment. We'll be using`walmart_stock.csv` file as our dataset to analyse the data.","659e47fd":"# Install and Create an Apache Spark Session","5d30d1f5":"# Creating A New Column\n\nWe are interested in the ratio of High Price vs Volume of stock traded for a particular day. Therefore, we have to create a new column called **HV Ratio** on a new dataframe derived from `walmart_df`","d12d3a20":"**Annddd, we're done! Congratulations**","faa278c9":"# Finding Early Insights and Pre-Processing","e342208a":"## What is the average of the Close field?","3057de74":"## What is the average Close for each Calendar Month i.e. close price for Jan, Feb, Mar, etc.?\n","691ce749":"## How many days was the Close lower than 70 USD?","35643fb1":"## What day had the Peak High in Price?","f0991b8d":"## What is the max High per year?\n\nWe can use `from pyspark.sql.functions import (dayofmonth, hour, dayofyear, month, year, weekofyear, format_number, date_format)` if we do not import it at the start of our session\n\nFirst, we have to extract the Year\/Month from `Date` column in `walmart_df` Dataframe and save them in a new Dataframe","8455c583":"Seems that we have too many decimal points in our data measurements. Let's try to make them in 2 decimal points using `format_number()` function to increase their readability."}}