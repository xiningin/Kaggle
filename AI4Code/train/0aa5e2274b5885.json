{"cell_type":{"58a05fb0":"code","177fd34e":"code","8f90072f":"code","7a99c574":"code","a5f52d01":"code","9c671f2c":"code","9753d8ec":"code","87c44fa5":"code","65145b0f":"code","4805fbc6":"code","eb24b91a":"code","df58bccf":"code","596c179b":"code","347aa7ee":"code","79ec7ac8":"code","354625d0":"code","646e3a23":"code","deac3e18":"code","a0e42d69":"code","f2ddf8a8":"code","1324c79c":"code","09284031":"code","b32010cf":"code","9d3cfbdc":"code","926b8686":"code","e022dd61":"code","168fc5f6":"code","269a2297":"code","64338893":"code","0c4a3591":"code","ed5d1da6":"code","b6a0900d":"code","3b65fd92":"code","baaba3bd":"code","e94814c2":"code","181b3a96":"code","cd69ef84":"code","0a090b0e":"code","e64b4c2d":"code","f99c5f13":"code","4901e933":"code","43b7d2ff":"code","09e0d469":"code","955add54":"code","1ab61822":"code","af4afee3":"code","78421ea4":"code","5e309210":"code","af870835":"code","9d8ce8af":"code","2661fabb":"code","acc4b5b3":"code","4f68c278":"code","29faa0bd":"code","9c0b597f":"markdown","f7bc7a83":"markdown","4154d73d":"markdown","02c2de4b":"markdown","4604cee5":"markdown","727ee832":"markdown","d6c60adc":"markdown","2f8eb902":"markdown","29b2edc9":"markdown","fa1acc93":"markdown","f46e36f3":"markdown","3bb31b1f":"markdown","922948d9":"markdown","4c83af31":"markdown","e8e76360":"markdown","5087fa61":"markdown","9d876efe":"markdown","c584f7cc":"markdown","8551c9c8":"markdown","b33d04d8":"markdown","864c0ae7":"markdown","d3ca70c5":"markdown","2ac59085":"markdown","7a9c577d":"markdown","aabf395a":"markdown"},"source":{"58a05fb0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","177fd34e":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')","8f90072f":"train = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)","7a99c574":"numerical = train.select_dtypes(exclude='object').columns\ncategorical = train.select_dtypes(include='object').columns","a5f52d01":"def SmoothTarget(train, test, features, weight):\n    \n    mean_target = train['target'].mean()\n    \n    for col in features:\n        agg = train.groupby(col)['target'].agg(['count', 'mean'])\n        count = agg['count']\n        mean = agg['mean']\n        \n        smooth = (count*mean + weight*mean_target) \/ (count+weight)\n        \n        train[col] = train[col].map(smooth)\n        test[col] = test[col].map(smooth)\n    \n    return train, test","9c671f2c":"train_se, test_se = train.copy(), test.copy()","9753d8ec":"train_se, test_se = SmoothTarget(train_se, test_se, categorical, 10)","87c44fa5":"from category_encoders import LeaveOneOutEncoder","65145b0f":"train_loo, test_loo = train.copy(), test.copy()","4805fbc6":"for col in categorical:\n    loo = LeaveOneOutEncoder()\n    loo.fit(train_loo[col], train_loo['target'])\n    train_loo[col] = loo.transform(train_loo[col])\n    test_loo[col] = loo.transform(test_loo[col])","eb24b91a":"target = train['target']\n\nfor dataframe in (train, train_se, train_loo):\n    dataframe = dataframe.drop('target', axis=1, inplace=True)","df58bccf":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import auc, roc_auc_score, roc_curve, plot_roc_curve","596c179b":"def KFoldROC(X, y, test_set, model, params, folds, eval_set_bool):\n\n    train_pred = np.zeros(len(train.index))\n    test_pred = np.zeros(len(test.index))\n    \n    roc_score = []\n    \n    \n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=21)\n\n    for train_idx, test_idx in skf.split(X,y):\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        clf = model(**params)\n        \n        if eval_set_bool == True:          \n            clf.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=200, verbose=False)\n        else:\n            clf.fit(X_train, y_train)\n\n        train_pred[test_idx] = clf.predict_proba(X_test)[:, 1]\n        \n        test_pred += clf.predict_proba(test_set)[:, 1] \/ folds\n        \n        score = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n        roc_score.append(score)\n    \n    overall_roc = roc_auc_score(target, train_pred)\n    \n    return clf, train_pred, test_pred, np.mean(roc_score), overall_roc","347aa7ee":"from xgboost import XGBClassifier","79ec7ac8":"xgb_params = {\n    'tree_method' : 'gpu_hist',\n    'eval_metric' : 'auc',\n    'verbosity' : 0,\n    'learning_rate': 0.011,\n     'n_estimators': 13278,\n     'max_depth': 21,\n     'reg_alpha': 7.369502726375538,\n     'gamma': 0.6911623139352171,\n     'reg_lambda': 4.4405272244246765,\n     'subsample': 0.8558774777122383,\n     'colsample_bytree': 0.17259675946606295,\n     'min_child_weight': 2.1918267231776003\n}","354625d0":"xgb, train_pred_xgb, test_pred_xgb, roc_xgb, overall_roc_xgb = KFoldROC(\n    train_loo, target, test_loo, XGBClassifier, xgb_params, 5, eval_set_bool=True)","646e3a23":"print(roc_xgb)","deac3e18":"print(overall_roc_xgb)","a0e42d69":"from lightgbm import LGBMClassifier","f2ddf8a8":"lgb_params = {\n 'learning_rate' : 0.03,\n 'metric' : 'auc',\n 'n_estimators': 8511,\n 'num_leaves': 205,\n 'max_depth': 10,\n 'reg_alpha': 8.337753037902587,\n 'reg_lambda': 2.778797190184823,\n 'subsample': 0.593175849495612,\n 'colsample_bytree' : 0.4228037476166183,\n 'min_child_samples': 1592}","1324c79c":"light, train_pred_light, test_pred_light, roc_light, overall_roc_light = KFoldROC(\n    train_se, target, test_se, LGBMClassifier, lgb_params, 5, eval_set_bool=True)","09284031":"print(roc_light)","b32010cf":"print(overall_roc_light)","9d3cfbdc":"from sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import RidgeClassifier","926b8686":"ridge_params = {\n    'base_estimator' : RidgeClassifier(),\n    'cv' : 5   \n}","e022dd61":"ridge, train_pred_ridge, test_pred_ridge, roc_ridge, overall_roc_ridge = KFoldROC(\n    train_loo, target, test_loo, CalibratedClassifierCV, ridge_params, 5, eval_set_bool=False)","168fc5f6":"print(roc_ridge)","269a2297":"print(overall_roc_ridge)","64338893":"from catboost import CatBoostClassifier","0c4a3591":"cat_params = {\n    'cat_features' : categorical,\n    'task_type' : 'GPU',\n    'grow_policy' : 'Depthwise',\n    'loss_function' : 'Logloss',\n    'eval_metric' : 'AUC',\n    'metric_period' : 500,\n    'learning_rate': 0.01,\n    'max_depth': 15,\n    'l2_leaf_reg': 2.998072993047546,\n    'num_boost_round': 5535,\n    'min_data_in_leaf': 296,\n    'bagging_temperature': 1.8002809995267188,\n    'penalties_coefficient': 3.2585922042596422\n}","ed5d1da6":"cb, train_pred_cb, test_pred_cb, roc_cb, overall_roc_cb = KFoldROC(\n    train, target, test, CatBoostClassifier, cat_params, 5, eval_set_bool=True)","b6a0900d":"print(roc_cb)","3b65fd92":"print(overall_roc_cb)","baaba3bd":"train_predictions = pd.DataFrame(\n    [train_pred_xgb, train_pred_light, train_pred_ridge, train_pred_cb, target]).transpose()\n\ntrain_predictions.columns = ['XGB', 'LightGBM', 'Ridge', 'CatBoost', 'target']","e94814c2":"test_predictions = pd.DataFrame(\n    [test_pred_xgb, test_pred_light, test_pred_ridge, test_pred_cb]).transpose()\n\ntest_predictions.columns = ['XGB', 'LightGBM', 'Ridge', 'CatBoost']","181b3a96":"train_predictions.to_csv('train_predictions.csv', index=False)","cd69ef84":"test_predictions.to_csv('test_predictions.csv', index=False)","0a090b0e":"average_pred = (train_pred_xgb + train_pred_light + train_pred_ridge + train_pred_cb) \/ 4","e64b4c2d":"print(roc_auc_score(target, average_pred))","f99c5f13":"average_pred_test = (test_pred_xgb + test_pred_light + test_pred_ridge + test_pred_cb) \/ 4","4901e933":"x = 0.33930655\ny = 0.34311931\nz = 0.31757414","43b7d2ff":"w_avg_pred = train_pred_xgb*x + train_pred_light*y + train_pred_cb*z","09e0d469":"print(roc_auc_score(target, w_avg_pred))","955add54":"w_avg_pred_test = test_pred_xgb*x + test_pred_light*y + test_pred_cb*z","1ab61822":"X_2 = train_predictions.drop('target', axis=1)\ny_2 = train_predictions['target']","af4afee3":"def KFoldROC_L2(X, y, model, folds):\n\n    train_pred = np.zeros(len(train.index))\n    test_pred = np.zeros(len(test.index))\n    \n    roc_score = []\n    \n    \n    skf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=21)\n\n    for train_idx, test_idx in skf.split(X,y):\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n        clf = CalibratedClassifierCV(model, cv=5)\n        clf.fit(X_train, y_train)\n\n        train_pred[test_idx] = clf.predict_proba(X_test)[:, 1]\n        \n        test_pred += clf.predict_proba(test_predictions)[:, 1] \/ folds\n        \n        score = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n        roc_score.append(score)\n    \n    overall_roc = roc_auc_score(target, train_pred)\n    \n    return clf, train_pred, test_pred, np.mean(roc_score), overall_roc","78421ea4":"L2_clf, train_pred_L2, test_pred_L2, roc_L2, overall_roc_L2 = KFoldROC_L2(X_2, y_2, RidgeClassifier(), 10)","5e309210":"print(roc_L2)","af870835":"print(overall_roc_L2)","9d8ce8af":"sub_id = sub['id']","2661fabb":"sub_avg = pd.DataFrame(data=[sub_id, average_pred_test]).transpose()\nsub_avg.columns = ['id', 'target']\nsub_avg['id'] = sub['id'].astype('int64')","acc4b5b3":"sub_wavg = pd.DataFrame(data=[sub_id, w_avg_pred_test]).transpose()\nsub_wavg.columns = ['id', 'target']\nsub_wavg['id'] = sub['id'].astype('int64')","4f68c278":"sub_l2 = pd.DataFrame(data=[sub_id, test_pred_L2]).transpose()\nsub_l2.columns = ['id', 'target']\nsub_l2['id'] = sub['id'].astype('int64')","29faa0bd":"sub_avg.to_csv('Submission Average.csv', index=False)\nsub_wavg.to_csv('Submission Weighted Average.csv', index=False)\nsub_l2.to_csv('Submission Level 2.csv', index=False)","9c0b597f":"Now let's produce submissions for our Level 2 Model, Average and Weighted Average models.","f7bc7a83":"Since the code above takes a while to run, I save the train and test predictions to csv files, so that I can work with them in a new notebook specifically dedicated to ensembling methods (stacking of course, but also averaging or weighted averaging).","4154d73d":"### CatBoost Classifier","02c2de4b":"## Submissions","4604cee5":"Again, after many tests, categorical data transformed using LeaveOneOut encoding gave me better results with XGBoost, so I'll use this method for my XGBoost model.","727ee832":"## Ensemble Predictions","d6c60adc":"### Leave-One-Out Encoding","2f8eb902":"### Smooth Target Encoding","29b2edc9":"### Cross Validation Function","fa1acc93":"## Data Preprocessing","f46e36f3":"## Creating Level 1 models","3bb31b1f":"# TPS March 21 - Stacking Ensemble","922948d9":"### Getting predictions","4c83af31":"## Level 2 Classifier","e8e76360":"From the different tests I've made, I've found that smooth target encoding works best with LightGBM. So I'll transform categorical data using this method, and then feed that into my LightGBM model.\n\nIf you want to learn more about smooth target encoding, I recommend this great link which explains clearly how it works:\nhttps:\/\/maxhalford.github.io\/blog\/target-encoding\/","5087fa61":"### LGBM Classifier","9d876efe":"### XGB Classifier","c584f7cc":"## Imports","8551c9c8":"### Averaging","b33d04d8":"### Calibrated Ridge Classifier","864c0ae7":"### Weighted Average","d3ca70c5":"Weights were obtained using scipy.optimize on a different notebook. From the many tests I've made, the RidgeClassifier model only decreased the overall score, so I didn't include it in this prediction.","2ac59085":"Hello everyone. Let me share with you my approach to March 21 competition. It's a simple ensemble of four models (XGBoost, LightGBM, CatBoost and RidgeClassifier), whose individual predictions were then trained on a meta-classifier.\n\nAll individual models hyperparameters were obtained using Optuna.\n\nHuge thanks to all the participants who published awesome notebooks during the competition; these helped me to learn a lot about different topics such as stratified k-fold, and obviously stacking.\n\nSpecial thanks to Craig Thomas for his notebook which I've been greatly inspired by:\nhttps:\/\/www.kaggle.com\/craigmthomas\/tps-mar-2021-stacked-starter","7a9c577d":"Finally, let's create a meta-classifier into which we'll feed our previous' models predictions.","aabf395a":"I will be training a calibrated version of RidgeClassifier, which doesn't accept fit parameters such as eval_set. This is why I've added the eval_set_bool parameter to this cross-validation function. Depending on the model I'm training, it just allows me to specify whether the model accepts the eval_set parameter or not."}}