{"cell_type":{"a629bdf2":"code","53ec3183":"code","e139fa02":"code","2e1104df":"code","dd24f3a0":"code","f8ce318b":"code","fe50d8e5":"code","4ad9f331":"code","c0dc92ea":"code","5577d7b7":"code","cf129476":"code","c89f56d0":"code","3d368e87":"code","2069c945":"code","952f1208":"code","42a2f60a":"code","f1f71016":"code","932920e7":"code","b17044f3":"code","5ab4f464":"markdown","b5f4c7e1":"markdown","9549f4d7":"markdown","89ad3f9c":"markdown","0ccc8026":"markdown"},"source":{"a629bdf2":"import sys\nsys.path.append('\/kaggle\/input\/torch-utils\/TorchUtils')\n!pip install -r \/kaggle\/input\/torch-utils\/TorchUtils\/requirements.txt\n!pip install \/kaggle\/input\/torch-utils\/TorchUtils","53ec3183":"import torch_utils as tu\n\nSEED = 20\ntu.tools.seed_everything(SEED, deterministic=False) ","e139fa02":"CLASSES = 176\nFOLD = 5\nBATCH_SIZE = 128\nACCUMULATE = 1\nLR = 0.01\nEPOCH = 50\nDECAY_SCALE = 20.0\nMIXUP = 0","2e1104df":"import pandas as pd\n\ntrain_df = pd.read_csv('\/kaggle\/input\/classify-leaves\/train.csv')","dd24f3a0":"train_df.head()","f8ce318b":"from sklearn.model_selection import StratifiedKFold\n\nsfolder = StratifiedKFold(n_splits=FOLD,random_state=SEED,shuffle=True)\ntr_folds = []\nval_folds = []\nfor train_idx, val_idx in sfolder.split(train_df.image, train_df.label):\n    tr_folds.append(train_idx)\n    val_folds.append(val_idx)\n    print(len(train_idx), len(val_idx))","fe50d8e5":"import albumentations\nfrom albumentations import pytorch as AT\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\n\ntrain_transform = albumentations.Compose([\n    albumentations.RandomRotate90(p=0.5),\n    albumentations.Transpose(p=0.5),\n    albumentations.Flip(p=0.5),\n    albumentations.HorizontalFlip(p=0.5),\n    albumentations.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.0625, rotate_limit=45, border_mode=1, p=0.5),\n    tu.randAugment(),\n    albumentations.Normalize(),\n    AT.ToTensorV2(),\n])\n    \ntest_transform = albumentations.Compose([\n    albumentations.Normalize(),\n    AT.ToTensorV2(),\n])\n\nclass LeavesDataset(Dataset):\n    \n    def __init__(self, df, label_encoder, data_path='.\/data', transform = train_transform): \n        self.df = df \n        self.data_path = data_path\n        self.transform = transform\n        self.df.label = self.df.label.apply(lambda x: label_encoder[x])\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, idx):\n        img_path, label = self.df.image[idx], self.df.label[idx]\n        img_path = os.path.join(self.data_path, img_path)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = self.transform(image = img)['image']\n        return img, label","4ad9f331":"from torch.optim.lr_scheduler import CosineAnnealingLR\nscaler = torch.cuda.amp.GradScaler() # for AMP training","c0dc92ea":"def train_model(epoch, verbose=False):\n    model_conv.train()         \n    avg_loss = 0.\n    optimizer.zero_grad()\n    if verbose:\n        bar = tqdm(total=len(train_loader))\n    mixup_fn = tu.Mixup(prob=MIXUP, switch_prob=0.0, onehot=True, label_smoothing=0.05, num_classes=CLASSES)\n    for idx, (imgs, labels) in enumerate(train_loader):\n        imgs_train, labels_train = imgs.float().cuda(), labels.cuda()\n        if MIXUP:\n            imgs_train, labels_train = mixup_fn(imgs_train, labels_train)\n        with torch.cuda.amp.autocast():\n            output_train = model_conv(imgs_train)\n            loss = criterion(output_train, labels_train)\n        scaler.scale(loss).backward()\n        if ((idx+1)%ACCUMULATE==0): # Gradient Accumulate\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            scheduler.step()\n        avg_loss += loss.item() \/ len(train_loader) \n        if verbose:\n            bar.update(1)\n    if verbose:\n        bar.close()\n    return avg_loss\n\ndef test_model():    \n    avg_val_loss = 0.\n    model_conv.eval()\n    y_true_val = np.zeros(len(valset))\n    y_pred_val = np.zeros((len(valset), CLASSES))\n    with torch.no_grad():\n        for idx, (imgs, labels) in enumerate(val_loader):\n            imgs_vaild, labels_vaild = imgs.float().cuda(), labels.cuda()\n            output_test = model_conv(imgs_vaild)\n            avg_val_loss += (criterion_test(output_test, labels_vaild).item() \/ len(val_loader)) \n            a = labels_vaild.detach().cpu().numpy().astype(np.int)\n            b = softmax(output_test.detach().cpu().numpy(), axis=1)\n\n            y_true_val[idx*BATCH_SIZE:idx*BATCH_SIZE+b.shape[0]] = a\n            y_pred_val[idx*BATCH_SIZE:idx*BATCH_SIZE+b.shape[0]] = b\n            \n    metric_val = sum(np.argmax(y_pred_val, axis=1) == y_true_val) \/ len(y_true_val)\n    return avg_val_loss, metric_val","5577d7b7":"def train(fold):\n    best_avg_loss = 100.0\n    best_acc = 0.0\n\n    avg_val_loss, avg_val_acc = test_model()\n    print('pretrain val loss %.4f precision %.4f'%(avg_val_loss, avg_val_acc))       \n\n    ### training\n    for epoch in range(EPOCH):   \n        print('lr:', optimizer.param_groups[0]['lr']) \n        np.random.seed(SEED+EPOCH*999)\n        start_time = time.time()\n        avg_loss = train_model(epoch)\n        avg_val_loss, avg_val_acc = test_model()\n        elapsed_time = time.time() - start_time \n        print('Epoch {}\/{} \\t train_loss={:.4f} \\t val_loss={:.4f} \\t val_precision={:.4f} \\t time={:.2f}s'.format(\n            epoch + 1, EPOCH, avg_loss, avg_val_loss, avg_val_acc, elapsed_time))\n\n        if avg_val_loss < best_avg_loss:\n            best_avg_loss = avg_val_loss\n\n        if avg_val_acc > best_acc:\n            best_acc = avg_val_acc\n            torch.save(model_conv.state_dict(), '.\/model-best' + str(fold) + '.pth')\n            print('model saved!')\n\n        print('=================================') \n\n    print('best loss:', best_avg_loss)\n    print('best precision:', best_acc)\n    return best_avg_loss, best_acc","cf129476":"from efficientnet_pytorch import EfficientNet\nimport numpy as np\nimport cv2\nfrom scipy.special import softmax\nimport time\n\ncv_losses = []\ncv_metrics = []\n\nfor fold in range(FOLD):\n    print('\\n ********** Fold %d **********\\n'%fold)\n\n    labels = train_df.label.unique()\n    label_encoder = {}\n    for idx, name in enumerate(labels):\n        label_encoder.update({name:idx})\n    \n    trainset = LeavesDataset(train_df.iloc[tr_folds[fold]].reset_index(), label_encoder, base_dir, train_transform)\n    train_loader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, num_workers=4, shuffle=True, drop_last=True, worker_init_fn=tu.tools.worker_init_fn)\n    \n    valset = LeavesDataset(train_df.iloc[val_folds[fold]].reset_index(), label_encoder, base_dir, test_transform)\n    val_loader = torch.utils.data.DataLoader(valset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n\n    model_conv = EfficientNet.from_pretrained('efficientnet-b1', num_classes=CLASSES)\n    model_conv.cuda()\n\n    optimizer = tu.RangerLars(model_conv.parameters(), lr=LR, weight_decay=1e-4)\n\n    if MIXUP:\n        criterion = tu.SoftTargetCrossEntropy()\n    else:\n        criterion = tu.LabelSmoothingCrossEntropy()\n        \n    criterion_test = nn.CrossEntropyLoss()\n\n    T = len(train_loader)\/\/ACCUMULATE * EPOCH # cycle\n    scheduler = CosineAnnealingLR(optimizer, T_max=T, eta_min=LR\/DECAY_SCALE)\n    \n    val_loss, val_acc = train(fold)\n    \n    cv_losses.append(val_loss)\n    cv_metrics.append(val_acc)\n    torch.cuda.empty_cache()\n\ncv_loss = sum(cv_losses) \/ FOLD\ncv_acc = sum(cv_metrics) \/ FOLD\nprint('CV loss:%.6f  CV precision:%.6f'%(cv_loss, cv_acc))","c89f56d0":"class LeavesDatasets(Dataset):\n    def __init__(self, df, data_path='.\/data', transform = train_transform): \n        self.df = df \n        self.data_path = data_path\n        self.transform = transform\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, idx):\n        img_path = self.df.image[idx]\n        img_path = os.path.join(self.data_path, img_path)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = self.transform(image = img)['image']\n        return img","3d368e87":"test_df = pd.read_csv( '\/kaggle\/input\/classify-leaves\/test.csv')\ntest_df.head()","2069c945":"testset = LeavesDatasets(test_df, base_dir, test_transform)\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)","952f1208":"import tqdm.notebook as tqdm\nfrom efficientnet_pytorch import EfficientNet\nimport numpy as np\nimport cv2\nfrom scipy.special import softmax\nimport time\n\ntst_preds_all = []\n\nfor fold in range(FOLD):\n    model = EfficientNet.from_pretrained('efficientnet-b1', num_classes=CLASSES)\n    model.cuda()\n    \n    model.load_state_dict(torch.load('.\/model-best' + str(fold) + '.pth'))\n    \n    with torch.no_grad():\n        model.eval()\n        image_preds_all = []\n        \n        for idx, imgs in enumerate(test_loader):\n            imgs = imgs.cuda()\n            image_preds = model(imgs)\n            image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]      \n        \n        image_preds_all = np.concatenate(image_preds_all, axis=0)\n\n    tst_preds_all.append(image_preds_all)\n\n    del model\n    torch.cuda.empty_cache()","42a2f60a":"avg_tst = np.mean(tst_preds_all, axis=0)\ntest_df['label'] = np.argmax(avg_tst, axis=1)\ntest_df.head()","f1f71016":"labels = train_df.label.unique()\nlabel_encoder = {}\nfor idx, name in enumerate(labels):\n    label_encoder.update({idx:name})\n    \ntest_df['label'] = test_df['label'].apply(lambda x: label_encoder[x])","932920e7":"test_df.head()","b17044f3":"test_df.to_csv(\".\/result.csv\", index=False)","5ab4f464":"# step4\uff1a\u6a21\u578b\u9a8c\u8bc1","b5f4c7e1":"# step2: \u52a0\u8f7d\u6570\u636e","9549f4d7":"\u7b2c\u4e00\u6b21\u53c2\u52a0kaggle\u7684\u6bd4\u8d5b\uff0c\u9009\u7528\u7684\u662fEfficientNet-b1\u7684\u6a21\u578b\uff0c\u975e\u5e38\u611f\u8c22seefun\u63d0\u4f9b\u7684\u4e00\u4e9btrick\uff0c\u5728\u672a\u4f7f\u7528\u6570\u636e\u589e\u5f3a\u30015\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3001mixup\u3001labelsmoothing\u3001CosineAnnealingLR\u5f97\u5230\u7684\u7ed3\u679c\u662f0.96340, \u672c\u4ee3\u7801\u662f\u5728seefun\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4fee\u6539\n\n\u6bd4\u8d5b\u7ed3\u679c\u4e3a\uff1a\n+ private learnboard\uff1a0.98590\n+ public leaderboard\uff1a0.98363","89ad3f9c":"# step3: \u5f00\u59cb\u8bad\u7ec3\u6a21\u578b","0ccc8026":"# step1: \u5b89\u88c5\u76f8\u5173\u4f9d\u8d56\n\n+ \u9996\u5148\u9700\u8981\u6839\u636e https:\/\/github.com\/seefun\/TorchUtils \u5b89\u88c5torch_utils\u5de5\u5177\u5305\n+ \u4f7f\u7528\u7684\u662falbumentations\u6570\u636e\u589e\u5f3a\u5e93\n+ \u4f7f\u7528StratifiedKFold 5\u6298\u4ea4\u53c9\u9a8c\u8bc1\n+ \u5b89\u88c5efficientnet_pytorch\n+ \u52a0\u8f7d\u5176\u4ed6\u4f9d\u8d56\u9879\u7b49"}}