{"cell_type":{"e3903e5d":"code","7f1c246a":"code","26a95144":"code","68dde89a":"code","3819dedb":"code","951264fe":"code","bd238e7d":"code","b22b740e":"code","78688d9e":"code","13159526":"code","708244ad":"code","8f9f0975":"code","89d1a9fc":"code","477ad496":"code","708e2516":"code","3a01056e":"code","0464b609":"code","dbeb2074":"code","3795c0df":"code","d350c115":"code","8133e0a7":"code","2a204302":"code","0911981f":"code","20857984":"code","073898ab":"code","bd9987cd":"code","92de43d0":"code","63431cc1":"code","3ef6ba5f":"code","b792e6aa":"code","48006d9e":"code","9baa06c0":"code","21954733":"code","d1a85b48":"code","c1a3bc5f":"code","e04db6f1":"markdown","66fac92b":"markdown","d07d29c9":"markdown","2a5c76ee":"markdown","102df2c5":"markdown","7d6b2267":"markdown","0053e5a3":"markdown","842c01b3":"markdown","6aa055f1":"markdown","d6b4dd59":"markdown","06359368":"markdown","60cc18bf":"markdown","007d71cd":"markdown","6088e5d2":"markdown","2532776a":"markdown","f3675b04":"markdown","17dae68f":"markdown"},"source":{"e3903e5d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport re\nimport nltk\n\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords.words('english')\n\nyorumlar = pd.read_csv('..\/input\/restaurant-reviews\/Restaurant_Reviews.csv')\nyorumlar.head()","7f1c246a":"#import re #regular expression\n#Preprocessing (\u00d6ni\u015fleme) -- SparceMatrix-StopWords-Stemmer\nderlem = []\nfor i in range(len(yorumlar)):\n    yorum = re.sub('[^a-zA-Z]',' ',yorumlar['Review'][i])\n    yorum = yorum.lower()\n    yorum = yorum.split()\n    yorum = [ps.stem(kelime) for kelime in yorum if not kelime in set(stopwords.words('english'))] #stopwords.words('turkish')\n    yorum = ' '.join(yorum) #vektor i\u00e7in list de\u011fil string hali gerekli\n    derlem.append(yorum)\n    \nderlem[0:5]","26a95144":"#Feautre Extraction ( \u00d6znitelik \u00c7\u0131kar\u0131m\u0131) -- CountVectorizer\n#Bag of Words (BOW)\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = 2000) #kel\u0131melerden en cok kullan\u0131lan kac tane al\u0131ns\u0131n \nX = cv.fit_transform(derlem).toarray() # ba\u011f\u0131ms\u0131z de\u011fi\u015fken\ny = yorumlar.iloc[:,1].values # ba\u011f\u0131ml\u0131 de\u011fi\u015fken\nprint(X[0:5])\nprint(y[0:5])","68dde89a":"import seaborn as sns\nimport matplotlib.pyplot as plt\n# visualize number of digits classes\nplt.figure(figsize=(15,7))\nsns.countplot(y)\nplt.title(\"Liked\")","3819dedb":"from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(X_train,y_train)\n\ny_pred = gnb.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\n#print(cm)\nprint(\"score: \",gnb.score(X_test,y_test))\n\n# %% cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","951264fe":"import pandas as pd\nimport re\nimport nltk # natural language tool kit\nnltk.download(\"stopwords\")      # corpus diye bir kalsore indiriliyor\nfrom nltk.corpus import stopwords  # sonra ben corpus klasorunden import ediyorum\n\nimport nltk as nlp\nlemma = nlp.WordNetLemmatizer()\n\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\n\n# %% import twitter data\ndata = pd.read_csv(r\"..\/input\/twitter-user-gender-classification\/gender-classifier-DFE-791531.csv\",encoding = \"latin1\")\ndata.head()","bd238e7d":"data = pd.concat([data.gender,data.description],axis=1)\nprint(data.info())","b22b740e":"data.dropna(axis = 0,inplace = True)\ndata.gender = [1 if each == \"female\" else 0 for each in data.gender]\nprint(data.head())\nprint(data.info())","78688d9e":"#%% clening data \ndescription_list = []\nfor description in data.description:\n    # regular expression RE mesela \"[^a-zA-Z]\"\n    description = re.sub(\"[^a-zA-Z]\",\" \",description)\n    description = description.lower()   # buyuk harftan kucuk harfe cevirme\n    # description = description.split()\n    # split yerine tokenizer kullanabiliriz\n    # split kullan\u0131rsak \"shouldn't \" gibi kelimeler \"should\" ve \"not\" diye ikiye ayr\u0131lmaz ama word_tokenize() kullanirsak ayrilir\n    description = nltk.word_tokenize(description)\n    # stopwords (irrelavent words) gereksiz kelimeler\n    description = [ word for word in description if not word in set(stopwords.words(\"english\"))]\n    # lemmatazation loved => love   gitmeyecegim = > git\n    lemma = nlp.WordNetLemmatizer()\n    description = [ lemma.lemmatize(word) for word in description] #[ ps.stem(word) for word in description]\n    description = \" \".join(description)  #vektor i\u00e7in list de\u011fil string hali gerekli\n    description_list.append(description)\ndescription_list[0:5]","13159526":"# %% bag of words\nfrom sklearn.feature_extraction.text import CountVectorizer # bag of words yaratmak icin kullandigim metot\nmax_features = 5000 #kel\u0131melerden en cok kullan\u0131lan kac tane al\u0131ns\u0131n\n\ncount_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\nsparce_matrix = count_vectorizer.fit_transform(description_list).toarray()  # x ba\u011f\u0131ms\u0131z de\u011fi\u015fken\nprint(\"en sik kullanilan {} kelimeden baz\u0131lar\u0131: {}\".format(max_features,count_vectorizer.get_feature_names()[0:5]))\n\ny = data.iloc[:,0].values   # male or female classes\nx = sparce_matrix","708244ad":"import seaborn as sns\nimport matplotlib.pyplot as plt\n# visualize number of digits classes\nplt.figure(figsize=(15,7))\nsns.countplot(y)\nplt.title(\"male or female classes\")","8f9f0975":"# train test split\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 0)\n\n# %% naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\n#%% prediction\ny_pred = nb.predict(x_test)\n\nprint(\"accuracy: \",nb.score(y_pred.reshape(-1,1),y_test))\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\n\n# %% cm visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","89d1a9fc":"# NLTK-Tokenize\nfrom nltk.tokenize import sent_tokenize, word_tokenize\ntext = \"Alan Mathison Turing was an English computer scientist, mathematician, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general purpose computer. Turing is widely considered to be the father of theoretical computer science and artificial intelligence.\"","477ad496":"text.split()[0:10]","708e2516":"word_tokenize(text)[0:10] #kel\u0131me tokenlest\u0131rme","3a01056e":"sent_tokenize(text)[0:10] #cumle tokenlest\u0131rme","0464b609":"# NLTK-StopWords\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\ntext = 'Faz\u0131l Say is a Turkish pianist and composer who was born in Ankara, described recently as \"not merely a pianist of genius; but undoubtedly he will be one of the great artists of the twenty-first century\".'\n\nstopwords = stopwords.words('english')\nprint(stopwords[0:10])","dbeb2074":"words = word_tokenize(text)\nfiltered_words = []\nfor word in words:\n    if word not in stopwords:\n        filtered_words.append(word)\n        \nfiltered_words[0:10]","3795c0df":"# NLTK-Stemmer\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\n\nwords = ['drive', 'driving', 'driver', 'drives', 'drove', 'cats', 'children']\n#words = ['Boyunluk', 'Boynu', 'Boylar', 'Boyun', 'Boy']\n\nfor w in words:\n    print(ps.stem(w))\n    \n#kelimenin sonundaki ekler kesiliyor","d350c115":"# NLTK-Part of Speech Tagging\nimport nltk\n\ntext = 'Friedrich Wilhelm Nietzsche was a German philosopher, cultural critic, composer, poet, philologist, and a Latin and Greek scholar whose work has exerted a profound influence on Western philosophy and modern intellectual history. He began his career as a classical philologist before turning to philosophy. He became the youngest ever to hold the Chair of Classical Philology at the University of Basel in 1869 at the age of 24. Nietzsche resigned in 1879 due to health problems that plagued him most of his life; he completed much of his core writing in the following decade. In 1889 at age 44, he suffered a collapse and afterward, a complete loss of his mental faculties. He lived his remaining years in the care of his mother until her death in 1897 and then with his sister Elisabeth F\u00f6rster-Nietzsche. Nietzsche died in 1900.'\ntokenized = nltk.word_tokenize(text)\ntokenized[0:10]","8133e0a7":"\"\"\"\nCC     coordinating conjunction\nCD     cardinal digit\nDT     determiner\nEX     existential there (like: \"there is\" ... think of it like \"there exists\")\nFW     foreign word\nIN     preposition\/subordinating conjunction\nJJ     adjective 'big'\nJJR    adjective, comparative 'bigger'\nJJS    adjective, superlative 'biggest'\nLS     list marker 1)\nMD     modal could, will\nNN     noun, singular 'desk'\nNNS    noun plural 'desks'\nNNP    proper noun, singular 'Harrison'\nNNPS   proper noun, plural 'Americans'\nPDT    predeterminer 'all the kids'\nPOS    possessive ending parent's\nPRP    personal pronoun I, he, she\nPRP$   possessive pronoun my, his, hers\nRB     adverb very, silently,\nRBR    adverb, comparative better\nRBS    adverb, superlative best\nRP     particle give up\nTO     to go 'to' the store.\nUH     interjection errrrrrrrm\nVB     verb, base form take\nVBD    verb, past tense took\nVBG    verb, gerund\/present participle taking\nVBN    verb, past participle taken\nVBP    verb, sing. present, non-3d take\nVBZ    verb, 3rd person sing. present takes\nWDT    wh-determiner which\nWP     wh-pronoun who, what\nWP$    possessive wh-pronoun whose\nWRB    wh-abverb where, when\n\"\"\"","2a204302":"nltk.pos_tag(tokenized)[0:10]","0911981f":"# NLTK-named entitiy recognition\n\nimport nltk\ntext = \"Steve Jobs was an American entrepreneur and business magnate. He was the chairman, chief executive officer (CEO), and a co-founder of Apple Inc., chairman and majority shareholder of Pixar, a member of The Walt Disney Company's board of directors following its acquisition of Pixar, and the founder, chairman, and CEO of NeXT. Jobs is widely recognized as a pioneer of the microcomputer revolution of the 1970s and 1980s, along with Apple co-founder Steve Wozniak. \"\ntokenized = nltk.word_tokenize(text)\nprint(tokenized[0:10])","20857984":"tagged = nltk.pos_tag(tokenized)\nprint(tagged[0:10])","073898ab":"named_ent = nltk.ne_chunk(tagged)\nprint(named_ent[0:10])\n#named_ent.draw()","bd9987cd":"\"\"\"\nNE T\u00fcr\u00fc         \t\u00d6rnek\nORGANIZATION    \tGeorgia-Pacific Corp., WHO\nPERSON          \tEddy Bonte, President Obama\nLOCATION        \tMurray River, Mount Everest\nDATE            \tJune, 2008-06-29\nTIME            \ttwo fifty a m, 1:30 p.m.\nMONEY           \t175 million Canadian Dollars, GBP 10.40\nPERCENT         \ttwenty pct, 18.75 %\nFACILITY        \tWashington Monument, Stonehenge\nGPE             \tSouth East Asia, Midlothian\n\"\"\"","92de43d0":"# NLTK-Lemmatizing\nfrom nltk.stem import WordNetLemmatizer\nlem = WordNetLemmatizer()\n\nwords = ['drive', 'driving', 'driver', 'drives', 'drove', 'cats', 'children']\nfor w in words:\n    print(lem.lemmatize(w))\n#kelimenin s\u00f6zl\u00fckteki k\u00f6k\u00fcne iniliyor (morfolojik)\n\nprint(lem.lemmatize('drove', 'v'))","63431cc1":"#Word2Vec\nimport numpy as np\n#numpy vektor ve matr\u0131sler uzer\u0131ne \u0131slem yapmam\u0131z\u0131 sa\u011flar\nfrom gensim.models import Word2Vec\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\n# Word2Vec corpus \u00fczerinde t\u00fcm kelimelerin \u00fczerinden ge\u00e7\n# her kelimenin etraf\u0131ndaki kelimeleri tahmin et\n# iki kelime birbirne ne kadar s\u0131k bulunuyorsa vekt\u00f6re yans\u0131t\nf = open('..\/input\/hurriyet\/hurriyet.txt', 'r', encoding='utf8')\ntext = f.read()\nt_list = text.split('\\n')\n\ncorpus = []\n\nfor cumle in t_list:\n    corpus.append(cumle.split())\nprint(corpus[:10])","3ef6ba5f":"model = Word2Vec(corpus, size=100, window=5, min_count=5, sg=1) #genelde size 5-300 uzunlugunda vektor olustururuz #gozet\u0131ms\u0131z ogrenme yontem\u0131\n#size 100 uzunlugunda vektor\n#window sol ve sagda bak\u0131lacak kel\u0131me say\u0131s\u0131\n#sg 1 ise skip-gram kullan\u0131lacak, default olarak cbow kullan\u0131l\u0131yor.\n#m\u0131n_count kel\u0131me en az kac kere gec\u0131yorsa al \nmodel.wv['ankara']\n# wv word vektorun k\u0131salt\u0131lm\u0131s\u0131\n#kelime vekt\u00f6r\u00fc, word vector, word embedding, embedding hep ayn\u0131 \u015feyi ifade ediyor","b792e6aa":"print(model.wv.most_similar('almanya'))\n#yazd\u0131g\u0131n\u0131z kel\u0131me kel\u0131me haznes\u0131nde yoksa hata al\u0131rs\u0131n\u0131z.","48006d9e":"#model.save('word2vec.model')\n#model = Word2Vec.load('word2vec.model')","9baa06c0":"def closestwords_tsneplot(model, word):\n    '''\n     bu s\u0131n\u0131f ver\u0131len model\u0131 \u0131le kel\u0131mey\u0131 al\u0131r\n     kel\u0131meye en yak\u0131n kel\u0131meler\u0131n vektorler\u0131n\u0131 b\u0131r d\u0131z\u0131ye atar\u0131z\n     TSNE \u0131le bu vektorler\u0131 graf\u0131ge donustururuz\n    '''\n    word_vectors = np.empty((0,100)) # en yak\u0131n olanlar\u0131 l\u0131steyi haz\u0131rlad\u0131k\n    word_labels = [word] #kel\u0131mey\u0131 d\u0131z\u0131 hal\u0131ne get\u0131rd\u0131k\n    \n    close_words = model.wv.most_similar(word) #yak\u0131n olan kel\u0131meler bulundu\n    \n    word_vectors = np.append(word_vectors, np.array([model.wv[word]]), axis=0) #gelen kel\u0131men\u0131n vektoru eklend\u0131\n    \n    for w, _ in close_words: #w kelimen\u0131n kend\u0131s\u0131 d\u0131ger\u0131 \u0131se yak\u0131nl\u0131k oran\u0131\n        word_labels.append(w)\n        word_vectors = np.append(word_vectors, np.array([model.wv[w]]), axis=0) #yak\u0131n kel\u0131meler\u0131n vektorler\u0131 de eklend\u0131\n        #boylece  gelen kel\u0131me ve yak\u0131n kel\u0131meler word_labels, bunlar\u0131n vektorler\u0131 ise word_vectors akta\n        \n    tsne = TSNE(random_state=0) #kel\u0131mele\u0131 graf\u0131ge doken kutuphane\n    Y = tsne.fit_transform(word_vectors)\n    \n    x_coords = Y[:, 0]\n    y_coords = Y[:, 1]\n    \n    plt.scatter(x_coords, y_coords)\n    \n    for label, x, y in zip(word_labels, x_coords, y_coords):\n        plt.annotate(label, xy=(x, y), xytext=(5, -2), textcoords='offset points')\n        \n    plt.show()","21954733":"closestwords_tsneplot(model, 'almanya')","d1a85b48":"def read_data(file_name):\n    with open(file_name,'r') as f:\n        word_vocab = set() # not using list to avoid duplicate entry\n        word2vector = {}\n        for line in f:\n            line_ = line.strip() #Remove white space\n            words_Vec = line_.split()\n            word_vocab.add(words_Vec[0])\n            word2vector[words_Vec[0]] = np.array(words_Vec[1:],dtype=float)\n    print(\"Total Words in DataSet:\",len(word_vocab))\n    return word_vocab,word2vector","c1a3bc5f":"from gensim.scripts.glove2word2vec import glove2word2vec\nfrom gensim.models import KeyedVectors # global vectors for word representations (Glove 2014)\n'''\nglove_input =  read_data('..\/input\/glove6b100dtxt\/glove.6B.100d.txt')\nword2vec_output =  read_data('..\/input\/glove-vec\/glove.6B.100d.word2vec') #word2vec yaz\u0131yor ama glove kullan\u0131lacak word2vec yazmas\u0131n\u0131n amac\u0131 gens\u0131m yuklemey\u0131 kolaylast\u0131rma\nglove2word2vec(glove_input, word2vec_output)\n\n\nmodel = KeyedVectors.load_word2vec_format(word2vec_output, binary=False)\nmodel['istanbul']\nmodel.most_similar('ankara') #bu yontem word2vec vard\u0131 ama glove 6 m\u0131lyar kel\u0131me \u0131c\u0131nde daha basar\u0131l\u0131\nmodel.most_similar(positive=['woman', 'king'], negative=['man'], topn=2)\n#topn demek sadece n tane goster\nmodel.most_similar(positive=['berlin', 'turkey'], negative=['ankara'], topn=1)\nmodel.most_similar(positive=['teach', 'doctor'], negative=['treat'], topn=1)\n\n\n'''\n","e04db6f1":"## Word2Vec","66fac92b":"## Clening data ","d07d29c9":"## CountVectorizer\n### Bag of Words (BOW)","2a5c76ee":"## NLTK-Stemmer","102df2c5":"## NLTK-StopWords","7d6b2267":"## Bag of Words","0053e5a3":"\n<a id=\"3.\"><\/a> \n# 3.NLTK - Word2Vec(SkipGram,CBOW) - Glove","842c01b3":"## Glove","6aa055f1":"\n<a id=\"2.\"><\/a> \n# 2.Reg.Exp.- Lemmatization - Bag of Words","d6b4dd59":"## Train-Test Split","06359368":"## NLTK-Lemmatizing","60cc18bf":"## NLTK-Part of Speech Tagging","007d71cd":"## NLTK-Tokenize","6088e5d2":"## Regular Expression\n### SparceMatrix-StopWords-Stemmer","2532776a":"It is the kernel that I have tried and compiled from the courses of [\u0130brahim Cebeci](https:\/\/www.udemy.com\/user\/ibrahim-cebeci-2\/) (Language of the courses is Turkish: [Do\u011fal Dil \u0130\u015fleme A-Z\u2122: (NLP)](https:\/\/www.udemy.com\/course\/dogal-dil-isleme\/)), which is has more than 6 courses on Udemy.\n\n# Content\n\n\n#### [1.StopWords - Stemmer - Count Vectorizer](#1.)\n#### [2.Reg.Exp.- Lemmatization - Bag of Words](#2.)\n#### [3.NLTK - Word2Vec(SkipGram,CBOW) - Glove](#3.)\n\n![](https:\/\/iili.io\/JGUWve.png)","f3675b04":"## NLTK-named entitiy recognition","17dae68f":"\n<a id=\"1.\"><\/a> \n# 1.StopWords - Stemmer - Count Vectorizer"}}