{"cell_type":{"e392a0e7":"code","4e5d8e49":"code","70bf86e7":"code","78f949a5":"code","5b70a48a":"code","00f38b99":"code","afcbf537":"code","395be6f0":"code","3d0b5b6e":"code","64dad015":"code","f0160f08":"code","44b7746d":"code","4b88f5f5":"code","0eeaef2c":"code","5a67d63e":"code","c3abdd3b":"code","1d532c1a":"code","6941fd1d":"code","61f3bdd7":"code","642f2718":"code","ec2f287b":"code","5ff11463":"code","75c37961":"code","d94cbcb2":"code","1d7e7689":"code","18a8cb27":"code","6b748e5e":"code","5eee36f0":"code","11881bfc":"code","a5eb8765":"code","8990ce31":"code","7e76c01d":"markdown","b7eca167":"markdown","f0634a69":"markdown"},"source":{"e392a0e7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\n\n%matplotlib inline","4e5d8e49":"df=pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf\n","70bf86e7":"df[\"Outcome\"].value_counts().plot(kind='bar')","78f949a5":"df1=df.copy()","5b70a48a":"for i in df.columns:\n    plt.figure()\n    plt.title(i)\n    plt.hist(df[i],bins=30)\n    plt.show()","00f38b99":"df[\"ins\"]=np.where(df[\"Insulin\"]==0,1,0)\nprint(df.groupby(\"ins\")[\"Outcome\"].mean())\ndf[\"sk\"]=np.where(df[\"SkinThickness\"]==0,1,0)\nprint(df.groupby(\"sk\")[\"Outcome\"].mean())","afcbf537":"df[\"Glucose\"]=np.where(df[\"Glucose\"]==0,df[\"Glucose\"].mean(),df[\"Glucose\"])\ndf[\"BloodPressure\"]=np.where(df[\"BloodPressure\"]==0,df[\"BloodPressure\"].mean(),df[\"BloodPressure\"])\ndf[\"BMI\"]=np.where(df[\"BMI\"]==0,df[\"BMI\"].mean(),df[\"BMI\"])\n","395be6f0":"for i in df.columns:\n    plt.figure()\n    plt.title(i)\n    plt.hist(df[i],bins=30)\n    plt.show()","3d0b5b6e":"print(df1.groupby(\"Outcome\")[\"SkinThickness\"].mean())\ndf1=df[df[\"SkinThickness\"]!=0]\na=df1.groupby(\"Outcome\")[\"SkinThickness\"].mean()[0]\nprint(a)\nb=df1.groupby(\"Outcome\")[\"SkinThickness\"].mean()[1]\ndf1=df[df[\"Insulin\"]!=0]\nc=df1.groupby(\"Outcome\")[\"Insulin\"].mean()[0]\n\nd=df1.groupby(\"Outcome\")[\"Insulin\"].mean()[1]\n","64dad015":"\n#df.loc[(df['First_name'] == 'Bill') | (df['First_name'] == 'Emma'), 'name_match'] = 'Match'  \ndf.loc[(df[\"SkinThickness\"]==0 )&( df[\"Outcome\"]==1),\"SkinThickness\"]=b\ndf.loc[(df[\"SkinThickness\"]==0 )&( df[\"Outcome\"]==0),\"SkinThickness\"]=a\ndf.loc[(df[\"SkinThickness\"]==0 )&( df[\"Outcome\"]==1),\"SkinThickness\"]=d\ndf.loc[(df[\"SkinThickness\"]==0 )&( df[\"Outcome\"]==0),\"SkinThickness\"]=c\n","f0160f08":"df","44b7746d":"for i in df.columns:\n    plt.figure()\n    plt.title(i)\n    plt.hist(df[i],bins=30)\n    plt.show()","4b88f5f5":"sns.pairplot(df,hue=\"Outcome\")","0eeaef2c":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(),cmap=\"RdYlGn\",annot=True)","5a67d63e":"for fet in df.columns:\n    plt.figure()\n    sns.boxplot(df[fet])\n    plt.show()","c3abdd3b":"Q1 = df.quantile(0.25)\nQ3 = df.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)\ndf = df[~((df < (Q1 - 3 * IQR)) |(df > (Q3 + 3 * IQR))).any(axis=1)]","1d532c1a":"for fet in df.columns:\n    plt.figure()\n    sns.boxplot(df[fet])\n    plt.show()","6941fd1d":"a=['DiabetesPedigreeFunction', 'Age', 'Insulin','Pregnancies']\n","61f3bdd7":"def diagnostic_plots(df, variable):\n    plt.figure(figsize=(15,6))\n    plt.subplot(1, 2, 1)\n    df[variable].hist(bins=20)\n    plt.subplot(1, 2, 2)\n    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n    plt.title(variable)\n    plt.show()\nfor i in df.columns:\n    diagnostic_plots(df, i)","642f2718":"for i in a:\n    df[i]=np.log(df[i]+1)\n    diagnostic_plots(df,i)","ec2f287b":"x=df.drop(\"Outcome\",axis=1)\ny=df[\"Outcome\"]\nfrom sklearn.preprocessing import MinMaxScaler \nscl=MinMaxScaler()\nx=scl.fit_transform(df.drop(\"Outcome\",axis=1))","5ff11463":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score \nclf1=LogisticRegression()\ncross_val_score(clf1,x,y,cv=20).mean()\n","75c37961":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score,precision_score,recall_score\nfrom sklearn.metrics import f1_score\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=42)\nfrom sklearn.model_selection import GridSearchCV\n\nfor i in [0.1,0.5,1,2,3,5,10,50,100]:\n    clf2 = LogisticRegression(C=i)\n#grid_clf_acc = GridSearchCV(clf1, param_grid = grid_values,scoring = {'Accuracy': 'accuracy'})\n    clf2.fit(x_train, y_train)\n    print(str(i) +\"- c value\")\n    \n\n#Predict values based on new parameters\n\n#\n\n#print(list(zip(y_test,y_score)))\n    y_pred=clf2.predict(x_test)\n    print(\"confusion_matrix - \"+str(confusion_matrix(y_test,y_pred)))\n    print(\"accuracy_score - \"+str(accuracy_score(y_test, y_pred)))\n    print(\"f1_score - \"+str(f1_score(y_test, y_pred,average=None)))\n    print(\"______________________________________________________\")","d94cbcb2":"df.hist()","1d7e7689":"df","18a8cb27":"for fet in df.columns:\n    plt.figure()\n    sns.boxplot(df[fet])\n    plt.show()","6b748e5e":"x=df.drop(\"Outcome\",axis=1)\ny=df[\"Outcome\"]\nfrom sklearn.neighbors import KNeighborsClassifier as knn\nfor i in [3,5,7,9,11,15,21]: \n    clf2=knn(n_neighbors=i)\n    print(cross_val_score(clf2,x,y,cv=10).mean())","5eee36f0":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nx=df.drop(\"Outcome\",axis=1)\ny=df[\"Outcome\"]\nclf3=RandomForestClassifier()\nx_train,x_test,y_train,y_test=train_test_split(x,y,random_state=42)\n#clf.fit(x,y)\ncross_val_score(clf3,x,y,cv=10).mean()","11881bfc":"\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","a5eb8765":"rf = RandomForestClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(x, y)\nrf_random.best_params_","8990ce31":"rf = RandomForestClassifier(n_estimators= 1600,\n min_samples_split= 2,\n min_samples_leaf= 4,\n max_features= 'sqrt',\n max_depth= 10,\n bootstrap= True)\ncross_val_score(rf,x,y,cv=10).mean()","7e76c01d":"\n# K N N","b7eca167":"# Logistic Regression","f0634a69":"# Random Forest"}}