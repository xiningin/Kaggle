{"cell_type":{"6fe1c527":"code","016f78ec":"code","f304bfc2":"code","3b1a9b78":"code","55035877":"code","0e4e0689":"code","9c56efbd":"code","bfbd472d":"code","73064ce9":"code","95d35193":"code","e9adbbcf":"code","ced516ca":"code","12712654":"code","f56f08ac":"code","ecf72e94":"code","3a8e94e6":"code","ccafd399":"code","6431bf78":"code","9e0a3921":"code","82d0dd72":"code","a88e3d63":"code","66dfb326":"code","35ea5eb3":"code","2fa11c8e":"code","d5d38a7e":"code","1f0ed154":"code","61266556":"code","2cd3f8e2":"code","96d356de":"code","3c080442":"code","fa42862a":"code","6e33c6df":"code","fdb2f2b6":"code","3ffcf110":"code","875f673f":"code","6c69d0fc":"code","1b119db1":"code","d71f9491":"code","37ff2a69":"code","bf02ece0":"code","981bcf85":"code","e2286964":"code","4119adcf":"code","3af0a005":"code","a42a4048":"code","93096f61":"code","476d00e0":"code","4c13ce4c":"code","c066f9e4":"code","1333b373":"code","5ab07aa8":"code","96b4d46d":"code","773b9258":"code","21ab6e4b":"code","6b9e4fad":"code","8c97e835":"code","4bc38242":"code","e186159c":"code","a15006f2":"code","2536883a":"markdown","56a63b99":"markdown","17063496":"markdown","6fe98ef3":"markdown","c7d92832":"markdown","0ae89ca5":"markdown","b1c8ce0f":"markdown","2f541fa1":"markdown","a74eaf47":"markdown","501af819":"markdown","1eb45c0c":"markdown","aa324d25":"markdown","65cbe30d":"markdown","8caa8465":"markdown","82ce6c5e":"markdown","c3406d4b":"markdown","4c561fee":"markdown"},"source":{"6fe1c527":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","016f78ec":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pandas import plotting\n\n#plotly \nimport plotly.offline as py\nimport plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly import tools\ninit_notebook_mode(connected=True)\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import preprocessing\nfrom sklearn import neighbors\nfrom sklearn.metrics import confusion_matrix,classification_report,precision_score\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\nplt.style.use('fivethirtyeight')","f304bfc2":"df=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","3b1a9b78":"df.columns","55035877":"col=['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']","0e4e0689":"df.columns","9c56efbd":"X_train=df[col]\nX_train.shape","bfbd472d":"# using sklearn variancethreshold to find constant features\n\nfrom sklearn.feature_selection import VarianceThreshold\nsel = VarianceThreshold(threshold=0)\nsel.fit(X_train) ","73064ce9":"# get_support is a boolean vector that indicates which features are retained\n# if we sum over get_support, we get the number of features that are not constant\nsum(sel.get_support())","95d35193":"# alternate way of finding non-constant features\nlen(X_train.columns[sel.get_support()])","e9adbbcf":"# print the constant features\nprint(\n    len([\n        x for x in X_train.columns\n        if x not in X_train.columns[sel.get_support()]\n    ]))\n\n[x for x in X_train.columns if x not in X_train.columns[sel.get_support()]]","ced516ca":"# we can then drop these columns from the train and test sets\nX_train = sel.transform(X_train)","12712654":"# check the shape of training and test set\n\nX_train.shape","f56f08ac":"df=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","ecf72e94":"X_train=df[col]\nX_train.shape","3a8e94e6":"sel = VarianceThreshold(threshold=0.01)  # 0.1 indicates 99% of observations approximately\n\nsel.fit(X_train)","ccafd399":"# get_support is a boolean vector that indicates which features \n# are retained. If we sum over get_support, we get the number\n# of features that are not quasi-constant\nsum(sel.get_support())","6431bf78":"# alternative way of doing the above operation:\nlen(X_train.columns[sel.get_support()])","9e0a3921":"# finally we can print the quasi-constant features\nprint(\n    len([\n        x for x in X_train.columns\n        if x not in X_train.columns[sel.get_support()]\n    ]))\n\n[x for x in X_train.columns if x not in X_train.columns[sel.get_support()]]","82d0dd72":"# percentage of observations showing each of the different values\nX_train['smoothness_mean'].value_counts() \/ np.float(len(X_train))","a88e3d63":"# we can then remove the features from training and test set\nX_train = sel.transform(X_train)","66dfb326":"# check the shape of training and test set\nX_train.shape","35ea5eb3":"from sklearn.datasets import load_iris\nfrom sklearn.feature_selection import SelectKBest, chi2\n","2fa11c8e":"df=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.shape","d5d38a7e":"X=df[col]\ny=df['diagnosis']","1f0ed154":"X.shape","61266556":"# select the two best features\nX_new = SelectKBest(chi2, k=2).fit_transform(X, y)\nX_new.shape","2cd3f8e2":"# select the two best features\nX_new1 = SelectKBest(chi2, k=10).fit_transform(X, y)\nX_new1.shape","96d356de":"from sklearn.datasets import load_digits\nfrom sklearn.feature_selection import SelectPercentile, chi2","3c080442":"X=df[col]\ny=df['diagnosis']","fa42862a":"X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)\nX_new.shape","6e33c6df":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif","fdb2f2b6":"X=df[col]\ny=df['diagnosis']","3ffcf110":"X.values","875f673f":"y.values","6c69d0fc":"# Select Features With Best ANOVA F-Values\n\n# Create an SelectKBest object to select features with two best ANOVA F-Values\nfvalue_selector = SelectKBest(f_classif, k=2)\n\n# Apply the SelectKBest object to the features and target\nX_kbest = fvalue_selector.fit_transform(X, y)","1b119db1":"print('Original number of features:', X.shape[1])\nprint('Reduced number of features:', X_kbest.shape[1])","d71f9491":"# Select Features With Best ANOVA F-Values\n\n# Create an SelectKBest object to select features with 10 best ANOVA F-Values\nfvalue_selector = SelectKBest(f_classif, k=10)\n\n# Apply the SelectKBest object to the features and target\nX_kbest = fvalue_selector.fit_transform(X, y)","37ff2a69":"print('Original number of features:', X.shape[1])\nprint('Reduced number of features:', X_kbest.shape[1])","bf02ece0":"df=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","981bcf85":"# step forward feature selection\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS","e2286964":"diagnosis={'M':1, 'B':0}\ndf['diagnosis']=[diagnosis[x] for x in df['diagnosis']]","4119adcf":"df=df.drop('Unnamed: 32', axis=1)\n","3af0a005":"df.columns","a42a4048":"# In practice, feature selection should be done after data pre-processing,\n# so ideally, all the categorical variables are encoded into numbers,\n# and then you can assess how deterministic they are of the target\n\n# here for simplicity I will use only numerical variables\n# select numerical columns:\n\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_vars = list(df.select_dtypes(include=numerics).columns)\ndata = df[numerical_vars]\ndata.shape","93096f61":"data.columns","476d00e0":"X=data[col]\ny=data['diagnosis']","4c13ce4c":"# separate train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3,random_state=0)\n\nX_train.shape, X_test.shape","c066f9e4":"# find and remove correlated features\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr\n\ncorr_features = correlation(X_train, 0.8)\nprint('correlated features: ', len(set(corr_features)) )","1333b373":"# removed correlated  features\nX_train.drop(labels=corr_features, axis=1, inplace=True)\nX_test.drop(labels=corr_features, axis=1, inplace=True)\n\nX_train.shape, X_test.shape","5ab07aa8":"# step forward feature selection\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\n\nsfs1 = SFS(RandomForestRegressor(), \n           k_features=10, \n           forward=True, \n           floating=False, \n           verbose=2,\n           scoring='r2',\n           cv=3)\n\nsfs1 = sfs1.fit(np.array(X_train), y_train)","96b4d46d":"sfs1.k_feature_idx_","773b9258":"X_train.columns[list(sfs1.k_feature_idx_)]","21ab6e4b":"# step backward feature elimination\n\nsfs1 = SFS(RandomForestRegressor(), \n           k_features=10, \n           forward=False, \n           floating=False, \n           verbose=2,\n           scoring='r2',\n           cv=3)\n\nsfs1 = sfs1.fit(np.array(X_train), y_train)","6b9e4fad":"sfs1.k_feature_idx_","8c97e835":"X_train.columns[list(sfs1.k_feature_idx_)]","4bc38242":"# step backward feature elimination\n\nsfs1 = SFS(RandomForestRegressor(), \n           k_features=12, \n           forward=False, \n           floating=False, \n           verbose=2,\n           scoring='r2',\n           cv=3)\n\nsfs1 = sfs1.fit(np.array(X_train), y_train)","e186159c":"sfs1.k_feature_idx_","a15006f2":"X_train.columns[list(sfs1.k_feature_idx_)]","2536883a":"* We can see that only 7 features lie on the top 10 percentile and hence we select them accordingly.","56a63b99":"* We can see how by removing constant features, we managed to reduced the feature space quite a bit.\n* As we dont have any constant feature ","17063496":"# Backward Elimination \n* In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n\n* The procedure starts with the full set of attributes. At each step, it removes the worst attribute remaining in the set.\n\n","6fe98ef3":"# SelectPercentile\n* Select features according to a percentile of the highest scores.","c7d92832":"# Forward Selection Method","0ae89ca5":"* We can see that 16 columns \/ variables are almost constant. This means that 107 variables show predominantly one value for ~99% the observations of the training set.","b1c8ce0f":"* We can see that there are 0 columns \/ variables that are constant. This means that 51 variables show the same value, just one value, for all the observations of the training set.","2f541fa1":"# Using variance threshold from sklearn\nVariance threshold from sklearn is a simple baseline approach to feature selection. It removes all features which variance doesn\u2019t meet some threshold. By default, it removes all zero-variance features, i.e., features that have the same value in all samples.","a74eaf47":"# ANOVA F-value For Feature Selection\n* Compute the ANOVA F-value for the provided sample.\n\n* If the features are categorical, we will calculate a chi-square statistic between each feature and the target vector. However, if the features are quantitative, we will compute the ANOVA F-value between each feature and the target vector.\n\n* The F-value scores examine if, when we group the numerical feature by the target vector, the means for each group are significantly different.","501af819":"# Remove quasi-constant features\n* Quasi-constant features are those that show the same value for the great majority of the observations of the dataset. In general, these features provide little if any information that allows a machine learning model to discriminate or predict a target. \n* So we should be careful when removing these type of features. Identifying and removing quasi-constant features, is an easy first step towards feature selection and more easily interpretable machine learning models.\n","1eb45c0c":"* Thus, we have selected the 10 best features from the iris dataset.","aa324d25":"* We can see that forward feature selection results in the above columns being selected from all the given columns.\n* 10 features are selected","65cbe30d":"#  SelectKBest\n* This method select features according to the k highest scores.\n\n* For instance, we can perform a chi-square test to the samples to retrieve only the two best features from iris dataset as follows:","8caa8465":"* So, backward feature elimination results in the following columns being selected.","82ce6c5e":"* Thus, we have selected the two best features from the iris dataset.","c3406d4b":"* We can see that > 99% of the observations show one value, 0. Therefore, this feature is almost constant.","4c561fee":"# Univariate selection methods\n* Univariate feature selection methods works by selecting the best features based on univariate statistical tests like ANOVA. It can be seen as a preprocessing step to an estimator.\n* Scikit-learn exposes feature selection routines as objects that implement the transform method.\n* The methods based on F-test estimate the degree of linear dependency between two random variables\n* They assume a linear relationship between the feature and the target. These methods also assume that the variables follow a Gaussian distribution.\n* There are 4 methods that fall under this category :-\n\n        1 SelectKBest\n        2 SelectPercentile\n        3 SelectFpr, SelectFdr, or family wise error SelectFwe\n        4 GenericUnivariateSelection\n* Here, I will limit the discussion to SelectKBest and SelectPercentile, because these two are most commonly used in practice."}}