{"cell_type":{"ab45f925":"code","e5e154fd":"code","85c9bac6":"code","f570f99c":"code","01fd22d7":"code","8eb83b51":"code","16d303eb":"code","60dec541":"code","2b87ec3b":"code","cb3dfd77":"code","67c044b9":"code","9a055b69":"code","a8e937be":"code","6c2abcfe":"code","4c353c44":"code","aa542b4c":"code","ac9a6344":"code","1fef4267":"code","a5d59e20":"code","ec5bcf29":"code","1eb1019f":"code","3a63db9e":"code","0c21ef7f":"code","9aa0eaa1":"code","caa15abe":"code","8f0bb50a":"code","d0e933ec":"code","5c8bc8d0":"code","ffea958b":"code","6a829236":"code","01d1895a":"code","524dc1e8":"code","623b2818":"code","75c24121":"code","08b4d1e8":"code","11f5b69b":"code","427d4e43":"code","98a42206":"markdown","2c87938a":"markdown","a27b62d7":"markdown","cda4cb56":"markdown","bbc3913f":"markdown","959b2730":"markdown","f8a32eb6":"markdown","89850207":"markdown","f1bcf718":"markdown","e11c45c7":"markdown","df922496":"markdown","0ff1bde1":"markdown","7da5e48a":"markdown","20e709e4":"markdown","10b81178":"markdown","2e9b2080":"markdown","60404062":"markdown","36953cc4":"markdown","a7de7e32":"markdown","29c576ef":"markdown","3c132adf":"markdown","46f4cd35":"markdown","17c83631":"markdown","49f7e0b6":"markdown","d9b111a2":"markdown","15ac06f2":"markdown","81f3a407":"markdown","66ede021":"markdown","8d568dc3":"markdown","6b7ef9ac":"markdown"},"source":{"ab45f925":"## basic libraries\nimport os\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline\n\n## libraries for feature engineering\nfrom sklearn.preprocessing import LabelEncoder\n","e5e154fd":"path = \"..\/input\/tabular-playground-series-jan-2022\"\ntdf = pd.read_csv(path + \"\/train.csv\")\nTdf = pd.read_csv(path + \"\/test.csv\")\nsdf = pd.read_csv(path +\"\/sample_submission.csv\")","85c9bac6":"print(f\"train:{tdf.shape} \\ntest:{Tdf.shape}\\ntest-train ratio: {(Tdf.shape[0])\/(tdf.shape[0])}\")","f570f99c":"tdf.head()","01fd22d7":"Tdf.head()","8eb83b51":"tdf.num_sold.describe()","16d303eb":"print(\"total predictions :\", tdf.num_sold.count())\nprint(\"\\ntotal unique predictions:\",tdf.num_sold.nunique())\nprint(\"\\nnull_predictions: \" ,tdf.num_sold.isnull().sum())","60dec541":"sns.lineplot(y=tdf['num_sold'],x=tdf['date'] ) # liquidity \nplt.show()\nsns.histplot(data=tdf ,x= \"num_sold\" ) # distribution \nplt.show()\nsns.boxplot(data=tdf, x= \"num_sold\") # variance","2b87ec3b":"tdf.head(3)","cb3dfd77":"tdf.info()","67c044b9":"def time_splitter(df,date): #here df = data and date= given time feature \n    df[date]         = pd.to_datetime(df[date])\n    df['year']       = df[date].dt.year\n    df['month']      = df[date].dt.month\n    df['day']        = df[date].dt.day\n    df['week']       = df[date].dt.isocalendar().week\n    df['weekday']    = df[date].dt.weekday\n    df['weekend']    = (df[date].dt.weekday>4).astype(int)\n    df['day_of_yr']  = df[date].dt.dayofyear\n    df['quarter']    = df[date].dt.quarter\n    return df\n\n# time splitting for training data\ntdf=time_splitter(tdf,'date')\ntdf.loc[300:310]","9a055b69":"# time splitting for test data\nTdf=time_splitter(Tdf,'date')\nTdf.head()","a8e937be":"tdf=tdf.drop(['row_id','date'],axis=1)","6c2abcfe":"\ncol=list(tdf.columns)  \ncol","4c353c44":"print(f\"values with None:\\n{tdf[tdf.values==None].count()} \\n\\nvalue with null or empty:\\\n     \\n{tdf.isnull().nunique()}\")","aa542b4c":"sns.set_style('darkgrid')\nsns.pairplot(data=tdf ,height=5 , corner=True ,diag_kind='kde')","ac9a6344":"cols = ['country','store','product']\n[print(f\"\\nTotal Sales by {i}  :\\n{tdf[i].value_counts()}\") for i in cols]","1fef4267":"from collections import defaultdict\nd= defaultdict(LabelEncoder)         # trick to do encoding and decoding vary easily \ud83d\ude01\n \nx = tdf[['country' ,'store','product','year']]\n\nx=x.apply(lambda x: d[x.name].fit_transform(x))\nx.head()\ntdf[['country' ,'store','product','year']]=x\ntdf","a5d59e20":"# for inverting the x to it's original data if needed\nx=x.apply(lambda x: d[x.name].inverse_transform(x))\nx.head()","ec5bcf29":"  tdf.describe()","1eb1019f":"from sklearn.covariance import EllipticEnvelope\nX= tdf[\"num_sold\"].values\nX=X.reshape((13149,2)) # 13149 = len(X)\/2 since X need to be 2D array\n# print(X)\nenvlp = EllipticEnvelope(contamination = 0.02 ,random_state=10)  # contamination == % of data outside ellipse\npred = envlp.fit_predict(X)\n# Extract outliers\noutlier_index = np.where(pred<=-1)\noutlier_values = X[outlier_index]\n\n# Plot the data\nsns.set(rc={'figure.figsize':(20,12)})\nsns.set_style('darkgrid')\nsns.scatterplot(x=X[:,0], y=X[:,1] )\nsns.scatterplot(x=outlier_values[:,0], \n                y=outlier_values[:,1], color='r')\nplt.title(\"Elliptic Envelope Outlier Detection\", fontsize=15, pad=15)\n\n#plt.savefig(\"Elliptic Envelope Detection.png\", dpi=80)\n\n# outlier count \nprint(\"total outliers\" ,outlier_values.size)","3a63db9e":"tdf['num_sold'].describe()","0c21ef7f":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nfrom sklearn.model_selection import train_test_split , cross_val_score\nfrom sklearn.preprocessing import LabelEncoder","9aa0eaa1":"\nX=tdf.drop(['num_sold'],axis=1)\ny=tdf[\"num_sold\"]\nX['week']=X['week'].astype('int')","caa15abe":"X_train , X_val ,y_train , y_val = train_test_split(X,y,test_size=0.24983 , random_state =47)","8f0bb50a":"def SMAPE(y_true,y_pred):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    smape=0\n    if(len(y_true)==len(y_pred)):\n        smape = (100\/len(y_true)) * np.sum(2* np.abs(y_pred-y_true)\/(np.abs(y_true)+np.abs(y_pred)))\n    else:\n        return\n    return(smape)","d0e933ec":"dic_model = {\"RandomForest\":RandomForestRegressor(n_estimators=100),\n             'GradientBoosting':GradientBoostingRegressor(n_estimators=100),\n            'XGradientBoosting':xgb.XGBRegressor(n_estimators=100),\n            'CatBoostRegressor':cb.CatBoostRegressor(n_estimators=100),\n            'LightGBM': lgb.LGBMRegressor(n_estimators=100)}\n\nfor i in dic_model:\n    #Training\n    print(\"Training with \",i+\" Algorithm....\")\n    print()\n    model = dic_model[i].fit(X_train,y_train)\n    \n    #Predicting\n    print(\"Predicting with \",i+\" Model....\")\n    print()\n    prediction = model.predict(X_val)\n    \n    # Using SMAPE for predicting models\n    print(\"SMAPE of \",i+\" Model is \",SMAPE(y_val,prediction))\n    print(\"------------------------------------------------------------------\")","5c8bc8d0":"# model.compile(tf.keras.optimizers.Adam(learning_rate=0.0001,beta_1=0.99),\n#              loss='hinge',\n#              metrics=['accuracy'])","ffea958b":" # 'min_data_in_leaf': 186 'l2_leaf_reg': 0.018327921341491783,'rsm': 0.7531284161762606, \nparams = { 'max_bin': 376, \n          'learning_rate': 1.299687, 'n_estimators': 4560, 'max_depth': 500, \n          'random_state': 47}\nmodel_cat = lgb.LGBMRegressor(**params)\nmodel_cat.fit(X,y,verbose=20)","6a829236":" prediction = model_cat.predict(X_val)\n    \n    # Using SMAPE for predicting models\nprint(\"SMAPE of \",i+\" Model is \",SMAPE(y_val,prediction))\nprint(\"------------------------------------------------------------------\")","01d1895a":"from collections import defaultdict\nc= defaultdict(LabelEncoder)         # trick to do encoding and decoding vary easily \ud83d\ude01\n \nx = Tdf[['country' ,'store','product' ,'year']]\n\nx=x.apply(lambda x: c[x.name].fit_transform(x))\nx.head()\nTdf[['country' ,'store','product' ,'year']]=x\nTdf","524dc1e8":"X_test =Tdf.drop(['row_id','date'],axis=1)\nX_test['week']=X_test['week'].astype('int')","623b2818":"pred=model_cat.predict(X_test)","75c24121":"pred.shape","08b4d1e8":"sub=pd.DataFrame()\nsub[\"row_id\"]=Tdf[\"row_id\"]\nsub[\"num_sold\"]=np.ceil(pred)","11f5b69b":"sub","427d4e43":"sub.to_csv('submission.csv' , index=False)","98a42206":"ok now we got data all in numerical form let's do some more visualizations and analysis","2c87938a":"- - - - - -","a27b62d7":"for beginners above line of code may look difficult to understand \ud83d\ude05 \nbut it's simple \n1. first I used `f\" {} {}\"`  to write code in just one print function \n2. then to break the lines I used `\\` at end of line after **value with null or empty:`\\`** \ud83d\udc48\n3. inside {} I used two functions \n   * `tdf[tdf.values==None].count()` comapres values of data with `None` data type since nunique can't detect this data type as null value \n   * `tdf.nunique(axis=0 , dropna =False)`  compares null values like `[] ,\" \" , NaN ` etc \n\n> outputs for values with none ==0 => there is no None value<br>\n> outputs for null values ==1 => isnull() writes true or false as nunique of isnull is 1 which means either all isnull values are True (entire data is Null)  or all isnull values are false ( there is no null value in the data)","cda4cb56":"# \ud83c\udf40WELCOME To My Notebook \nThis is my 2022's first Notebook... \n\n**HAPPY NEW YEAR TO ALL** \ud83c\udf89\ud83c\udf89\ud83c\udf8a\n\n\nI will try to explore and explain the notebook as friendly as I can, I can guarantee  you that it will be very interesting  to read this notebook \ud83d\ude02","bbc3913f":"total data is symmetric ... this is doing of the team who arranged this competition , we don't get symmetric data in general\ud83d\ude09","959b2730":"let's do some interesting outlier analysis for this new year \ud83d\ude02\ud83d\udca5","f8a32eb6":"ok we have very less data , which is always a bad thing for us to create models \ud83e\udd23","89850207":"soka \ud83d\ude2e ,as we can see there are `4 features` which are `objects`and `2` are `int64` , and if we see tdf.head output then we can see 3 of the 4 objects are `categorical:`  `country` , `store` ,`product` and 1 of them is `time` , time could be categorical , ordinal, continuous .. anything [refer here for understanding time data type](https:\/\/statisticalanalysisconsulting.com\/is-time-nominal-ordinal-interval-or-ratio-is-it-categorical-or-continuous\/) most of the time we drop the date in the datasets but in this problem  `date` is the main feature ","f1bcf718":"suppose we want to see how much % of data is outside the regionr as per our recommended outlier region we can use boxplot but there is something more good for it as follows\n\nlet's see we keep data with 98% inlier and 2 % outlier ==>  contamination = 0.02","e11c45c7":"##  \ud83d\udca0 Handling Categorical features","df922496":"references [lambda](https:\/\/towardsdatascience.com\/lambda-functions-with-practical-examples-in-python-45934f3653a8)\n[LabelEncoder](https:\/\/scikit-learn.org\/stable\/modules\/preprocessing_targets.html) [trick to use encoding](https:\/\/stackoverflow.com\/questions\/24458645\/label-encoding-across-multiple-columns-in-scikit-learn)","0ff1bde1":"## \u2b55 creating new time features\n\nhere we try to break the timeline into different parts  like `year , month , week , days , weekdays , quarter , Hours , Minutes , seconds` etc to understand how the predictor `y` ( no. of sales) is changing with respect to these different timelines\n","7da5e48a":"### \ud83d\udca2outlier analysis","20e709e4":"# \ud83d\udd05 MODEL ","10b81178":"so conclusion is there is no null value in this dataset everthing looks great and easy\ud83d\ude02","2e9b2080":"## \ud83d\udca0Determining the predictor Y and it's behaviour\nlet's find the label `Y` from the data and  see what exactly we have to predict","60404062":"the only outlier here is `num_sold`  which we are not goint to fix as it will help us to underfit the data","36953cc4":"#### 1.Eliptic Envelope Algorithm for outlier detection","a7de7e32":"# \ud83d\udd05 Basic EDA","29c576ef":"# \ud83d\udd05  Feature Engineering For Time Series Data ","3c132adf":"## \ud83d\udca0 importing DATA","46f4cd35":"As soon as you hear the name `time series` or`forecasting model` then change your attitude towards that problem , because this problem need to be handled in very unique manner, we can't apply basic supervised learning models directly to predict the future,\n\nlet's understand what time series means :\n\n**Time Series Prediction**  \n* The data which follows the periodicity with respect to time  is `Time Series Data` ,\n* Generally Data looks like `Sine waves` or like `stock market curves` but repeating the curves after some interval\n* We want to understant how the trends of predictor `y`  ( in our case Y is no. of sales ) changes with respect to time \n* and we want to predict the future trends of predictor `y` ( in simple we want to predict the future sales , future stock prices , future product evaluations and many more ...)\n\n\nI tried to discuss few simple , easily understandable + effective techniques to solve this problem\nlet's dive into the data science of the future prediction \ud83c\udfca\u200d\u2640\ufe0f\ud83c\udfca\u200d\u2640\ufe0f\n\n","17c83631":"**Understanding The Problem** all of us have read the description of problem statement and it says ,\n>There are two (fictitious) independent store chains selling Kaggle merchandise that want to become the official outlet for all things Kaggle. We've decided to see if the Kaggle community could help us figure out which of the store chains would have the best sales going forward. So, we've collected some data and are asking you to build `forecasting models` to help us decide.","49f7e0b6":"### \ud83d\udca2null value analysis ","d9b111a2":"So, here the feature named `num_sold` is the predictor `y` for this problem, let's see what this `num_sold` has inside it","15ac06f2":"since our data has huge classes we will use label encoding ","81f3a407":"`fig 1` this fig is computationally heavy to plot as we have not seperated `date` with respect to specific `year` , `month` and `day` but for first glance we can see our predictor `y` is following periodicity which clearly indicates this could be the problem of time series prediction even if it is not specified in compitition\n\n`fig2` \ud83d\ude2e chi-squared curve or skewed normal distribution... looks great curve, <br>\n`Note:`  predictor `y` is distributed under chi_square distribution, which clearly indicates under normal conditions  we should be using categorical ML Models like multiclass  classification , clustering or softmax or svm  .....`but` this is not a supervised learning problem it's problem of future prediction which comes under `time series predictions`  so our entire methodology to solve the problem changes ...<br>\n\n`fig3` it shows that predictor `y` is having some outliers which are indications of high sales on some festival or some special ocassion","66ede021":"## \ud83d\udca0 Importing libraries","8d568dc3":"#### \ud83d\udcdaPN : 2\n\ud83d\udcda[PN : 1](https:\/\/www.kaggle.com\/sarabhian\/gbr-extremely-beginner-level-guide-1)","6b7ef9ac":"this above plot looks scary isn't it ? \ud83d\udc79\ud83d\udc79 most of the people consider pairplot as nonsense plot because people won't understand it\ud83e\udd23\ud83e\udd23, but let me tell you the insights that I am getting from this plot\ndouble click of graph and see each graph carefully to check the below given insights<br>\n( considering `num_sold` =`ns` for decreasing tyoing load \ud83d\ude01)<br>\n1. first column of plot:\n\n * `quarter` vs `ns` : in 2nd and 4th quarter we had to more sales\n * `day_of_year` vs `ns` : for days 50:150 and for days 350:365 we had high sales\n * `weekend` vs `ns` : we had little high sales on weekends (as expected)\n * `weekday` vs `ns` : --||--\n * `day` vs `ns` : at end of month we have high sales\n * `month` vs `ns` : 4th ,5th and 12th month had more sales\n * `year` vs `ns` : 2017 and 2018 had more sales ( I will say sales are increasing each year)\n * `index` vs `ns` : indicates num_sold is chi-square distributed which is expected curve for sales \n <br><br>\n similarly I can tell each plots meaning but I am tired of writing so you guys decide on your own \ud83d\ude02"}}