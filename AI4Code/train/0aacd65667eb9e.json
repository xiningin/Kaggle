{"cell_type":{"8bb5d936":"code","7aa9b983":"code","8ec0bb43":"code","2add618b":"code","2128af7e":"code","833a9cda":"code","69074083":"code","23753613":"code","a84243b0":"code","6d35c2fc":"code","416d4a62":"code","ac2122ee":"code","3b5b7a7b":"code","ee8ed726":"code","32492687":"code","0c08b82b":"code","b097c27c":"code","3cf837eb":"code","7beb220f":"code","1f31cde4":"code","fa2cfb8a":"code","2ab54369":"code","611b5b62":"code","1a1bf498":"markdown","01f21a3b":"markdown","b78ad112":"markdown","c2f121f2":"markdown"},"source":{"8bb5d936":"import numpy as np\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport shap\nfrom sklearn import preprocessing","7aa9b983":"plt.style.use('seaborn')\nplt.rcParams['figure.figsize'] = 12, 7","8ec0bb43":"# Make environment and get data\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()\n(market_train_df, news_train_df) = env.get_training_data()","2add618b":"# https:\/\/www.kaggle.com\/rspadim\/parse-ric-stock-code-exchange-asset\nmarket_train_df['assetCode_asset'] = market_train_df['assetCode']\nmarket_train_df['assetCode_exchange'] = market_train_df['assetCode']\ntmp_map_a, tmp_map_b = {}, {}\nfor i in market_train_df['assetCode'].unique():\n    a,b = i.split('.')\n    tmp_map_a[i] = a\n    tmp_map_b[i] = b\nmarket_train_df['assetCode'] = market_train_df['assetCode'].astype('category')\nmarket_train_df['assetCode_asset'] = market_train_df['assetCode_asset'].map(tmp_map_a).astype('category')\nmarket_train_df['assetCode_exchange'] = market_train_df['assetCode_exchange'].map(tmp_map_b).astype('category')\nprint(market_train_df.dtypes)","2128af7e":"# Dropping assetName just to focus exclusively on one categorical variable\nmarket_train_df.drop('assetName', axis=1, inplace=True)","833a9cda":"def make_test_train(df, split=0.80):\n    # Label encode the assetCode feature\n    X = df[df.universe==1]\n    le = preprocessing.LabelEncoder()\n    X = X.assign(assetCode = le.fit_transform(X.assetCode))\n    X = X.assign(assetCode_asset = le.fit_transform(X.assetCode_asset))\n    X = X.assign(assetCode_exchange = le.fit_transform(X.assetCode_exchange))\n    \n    \n    # split test and train\n    train_ct = int(X.shape[0]*split)\n    y_train, y_test = X['returnsOpenNextMktres10'][:train_ct], X['returnsOpenNextMktres10'][train_ct:]\n    X = X.drop(['time', 'returnsOpenNextMktres10'], axis=1)\n    X_train, X_test = X.iloc[:train_ct,], X.iloc[train_ct:,]\n    return X, X_train, X_test, y_train, y_test","69074083":"# Make the encoding and split\nX, X_train, X_test, y_train, y_test = make_test_train(market_train_df)","23753613":"def make_lgb(X_train, X_test, y_train, y_test, categorical_cols = ['assetCode', 'assetCode_asset', 'assetCode_exchange']):\n    # Set up LightGBM data structures\n    train_cols = X_train.columns.tolist()\n    dtrain = lgb.Dataset(X_train.values, y_train, feature_name=train_cols, categorical_feature=categorical_cols)\n    dvalid = lgb.Dataset(X_test.values, y_test, feature_name=train_cols, categorical_feature=categorical_cols)\n    print('train cols:', train_cols)\n    print('categorical_feature:', dtrain.categorical_feature)\n    return dtrain, dvalid","a84243b0":"# Set up the LightGBM data structures\ndtrain, dvalid = make_lgb(X_train, X_test, y_train, y_test)","6d35c2fc":"# Set up the LightGBM params\nlgb_params = dict(\n    objective='regression_l1', learning_rate=0.1, num_leaves=127, max_depth=-1, bagging_fraction=0.75,\n    bagging_freq=2, feature_fraction=0.5, lambda_l1=1.0, seed=1015\n)","416d4a62":"# Fit and predict\nevals_result = {}\nm = lgb.train(\n    lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), \n    verbose_eval=25, early_stopping_rounds=20, evals_result=evals_result,\n    #categorical_feature=categorical_feature\n)","ac2122ee":"# Plot reported feature importance\nlgb.plot_importance(m);","3b5b7a7b":"lgb.plot_importance(m, importance_type='gain');","ee8ed726":"shap_explainer = shap.TreeExplainer(m)","32492687":"%%time\nsample = X.sample(frac=0.50, random_state=100)\nshap_values = shap_explainer.shap_values(sample)","0c08b82b":"%%time\nshap.summary_plot(shap_values, sample)","b097c27c":"# Make the encoding and split\nX, X_train, X_test, y_train, y_test = make_test_train(market_train_df)\nX.drop(['assetCode_asset', 'assetCode'], axis=1, inplace=True)\nX_train.drop(['assetCode_asset', 'assetCode'], axis=1, inplace=True)\nX_test.drop(['assetCode_asset', 'assetCode'], axis=1, inplace=True)\n# Set up the LightGBM data structures\ndtrain, dvalid = make_lgb(X_train, X_test, y_train, y_test, categorical_cols = ['assetCode_exchange'])","3cf837eb":"# Fit and predict\nevals_result = {}\nm = lgb.train(\n    lgb_params, dtrain, num_boost_round=1000, valid_sets=(dvalid,), valid_names=('valid',), \n    verbose_eval=25, early_stopping_rounds=20, evals_result=evals_result,\n    #categorical_feature=categorical_feature\n)","7beb220f":"# Plot reported feature importance\nlgb.plot_importance(m);","1f31cde4":"lgb.plot_importance(m, importance_type='gain');","fa2cfb8a":"shap_explainer = shap.TreeExplainer(m)","2ab54369":"%%time\nsample = X.sample(frac=0.50, random_state=100)\nshap_values = shap_explainer.shap_values(sample)","611b5b62":"%%time\nshap.summary_plot(shap_values, sample)","1a1bf498":"# The fallacy of encoding assetCode\nBy @marketneutral\n\nThere have been a few kernel shares of gradient-boosted decision trees (\"GBDT\"; e.g., `lightgbm`) applied directly to the data \"as is\" in this competition. The results of this show that `assetCode` (and `assetName`), as a categorical variable, is a substantially important feature. Does this make sense? If you simply know the ticker symbol should that add predictive power to the model? On the face of it, it seems implausible and that the use of `assetCode` rather is simply leaking future information and producing an overfit model. I investigate this idea in this kernel.\n\n## Minimal Reproduction\nFirst, let's do a minimal reproduction. There is no effort here to do parameter tuning, or to create a great model per se. Here we just want to fit the bare minimum GBDT model and see what features the model thinks are important. We just want to reproduce in a minimal way that the model will find `assetCode` and `assetName` to be very important.","01f21a3b":"FROM: \n    \n    https:\/\/www.kaggle.com\/marketneutral\/the-fallacy-of-encoding-assetcode\n    \n    https:\/\/www.kaggle.com\/rspadim\/parse-ric-stock-code-exchange-asset\n","b78ad112":"So indeed,  consistent with other public kernels, the model believes `assetCode` is a valuable feature. \n\nLet's remove it","c2f121f2":"Let's see what SHAP thinks. Per the [GitHub repo](https:\/\/github.com\/slundberg\/shap), **SHAP** (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model. SHAP connects game theory with local explanations, uniting several previous methods [1-7] and representing the only possible consistent and locally accurate additive feature attribution method based on expectations (see the [SHAP NIPS paper](http:\/\/papers.nips.cc\/paper\/7062-a-unified-approach-to-interpreting-model-predictions) for details)."}}