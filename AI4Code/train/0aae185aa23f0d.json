{"cell_type":{"bfe90da4":"code","880d831c":"code","a0403046":"code","287fb4ee":"code","ffdea9ae":"code","32614f97":"code","db2014d8":"code","47964360":"code","4648e9f1":"code","f38f8097":"code","9adf5fd5":"code","805dadf4":"code","d88a2361":"code","d4d007bd":"markdown","b76756b8":"markdown","b524b7bb":"markdown","9fee62dc":"markdown"},"source":{"bfe90da4":"#Data Manipulation\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n#Data Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\n#OS interaction\nimport os","880d831c":"path = \"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\"\nheartDF = pd.read_csv(path)\nheartDF.head()","a0403046":"#Check if there are null values\nheartDF.isnull().sum()","287fb4ee":"from sklearn.tree import DecisionTreeClassifier,export_graphviz\nfrom graphviz import render,Source\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\ny = heartDF['DEATH_EVENT']\nX = heartDF.drop(columns=['DEATH_EVENT'])\n\naccuracy_dict = {}\n\nx_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\ndtc = DecisionTreeClassifier(random_state=0)\ndtc.fit(x_train,y_train)\ny_pred = dtc.predict(x_test)\naccuracy = accuracy_score(y_test,y_pred)\naccuracy_dict[\"DecisionTreeClassifier\"] = accuracy\nprint(\"Accuracy:\",accuracy)\ndotfile = open(\"dtc.dot\", 'w')\nexport_graphviz(dtc, out_file = dotfile, feature_names = x_train.columns)\ndotfile.close()","ffdea9ae":"print(\"Decision path for Decision Tree Classifier\")\nrender('dot', 'png', 'dtc.dot')\nSource.from_file(\"dtc.dot\")","32614f97":"from sklearn.feature_selection import VarianceThreshold\nvarDF = pd.DataFrame(X.var(),columns=['Feature Variance'])\nvarDF","db2014d8":"thres = VarianceThreshold(threshold=0.2)\nhigh_var = thres.fit_transform(X)\nprint(\"Columns deleted:\",X.columns[~thres.get_support()])\nx_train = x_train[ X.columns[(thres.get_support())]]\nx_test = x_test[X.columns[(thres.get_support())]]\npd.DataFrame(high_var,columns= X.columns[(thres.get_support())])","47964360":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(x_train,y_train)\ny_pred = rfc.predict(x_test)\naccuracy = accuracy_score(y_test,y_pred)\nprint(\"Accuracy:\",accuracy)\n\naccuracy_dict[\"RandomForestClassifier\"] = accuracy","4648e9f1":"from xgboost import XGBClassifier\n\nxgb = XGBClassifier()\nxgb.fit(x_train,y_train)\ny_pred = xgb.predict(x_test)\naccuracy = accuracy_score(y_test,y_pred)\nprint(\"Accuracy:\",accuracy)\n\naccuracy_dict[\"XGBoost Classifier\"] = accuracy","f38f8097":"from sklearn.ensemble import AdaBoostClassifier\n\nabc = AdaBoostClassifier(n_estimators=50)\nabc.fit(x_train,y_train)\ny_pred = abc.predict(x_test)\naccuracy = accuracy_score(y_test,y_pred)\nprint(\"Accuracy:\",accuracy)\n\naccuracy_dict[\"AdaBoost Classifier\"] = accuracy","9adf5fd5":"accuracy_dict","805dadf4":"for k,v in accuracy_dict.items():\n    if(v == max(accuracy_dict.values())):\n        model_chosen = k\n\nif model_chosen == \"DecisionTreeClassifier\":\n    model = dtc\nelif model_chosen == \"RandomForestClassifier\":\n    model = rfc\nelif model_chosen == \"XGBoost Classifier\":\n    model = xgb\nelse:\n    model = ada","d88a2361":"print(\"Best model for this classification:\", model_chosen)\nprint(\"Accuracy:\",accuracy_dict[model_chosen])","d4d007bd":"# Applying simple Decision Tree for classification","b76756b8":"# Boosting\n* With XGBoost and AdaBoost","b524b7bb":"# Data Cleaning\n\n* Check for features with low variance and remove those features\n* There is no need for feature scaling for the bagging and boosting algorithms below","9fee62dc":"# Bagging\n* With Random Forrest Classifier"}}