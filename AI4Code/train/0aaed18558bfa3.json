{"cell_type":{"4ea5cb37":"code","e23787c9":"code","3a9e03f0":"code","7b1f80a5":"code","1732b5e3":"code","083a6e55":"code","cbcec349":"code","0d173786":"code","3c9ff4d6":"code","331bebd3":"code","ade4d61e":"code","89a9ea56":"code","24c5db8f":"code","6b36fdac":"code","e7e0fcf8":"code","45061f67":"markdown","513db001":"markdown","ac87f1de":"markdown","de5cfd07":"markdown","4b3cb36e":"markdown","a2086eaa":"markdown","8092e7ce":"markdown","25aad443":"markdown","9591bdb3":"markdown","b8959186":"markdown","89b6a4a2":"markdown","757210d7":"markdown","0d141875":"markdown"},"source":{"4ea5cb37":"import torch\nfrom torchvision import transforms\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport json\nimport matplotlib\nimport requests\nfrom io import BytesIO\nimport time\nimport os\nfrom torchvision.utils import make_grid, save_image\nfrom PIL import Image","e23787c9":"labels = {\n    0: 'Mitochondria',\n    1: 'Nuclear bodies',\n    2: 'Nucleoli',\n    3: 'Golgi apparatus',\n    4: 'Nucleoplasm',\n    5: 'Nucleoli fibrillar center',\n    6: 'Cytosol',\n    7: 'Plasma membrane',\n    8: 'Centrosome',\n    9: 'Nuclear speckles'\n}","3a9e03f0":"def decode_target(target, text_labels=False, threshold=0.5):\n    result = []\n    for i, x in enumerate(target):\n        if (x >= threshold):\n            if text_labels:\n                result.append(labels[i] + \"(\" + str(i) + \")\")\n            else:\n                result.append(str(i))\n    return ' '.join(result)","7b1f80a5":"def predict_single(image):\n    xb = image.unsqueeze(0)\n    preds = model(xb)\n    prediction = preds[0]\n    result = decode_target(prediction,text_labels=True)\n    return result","1732b5e3":"def F_score(output, label, threshold=0.5, beta=1):\n    prob = output > threshold\n    label = label > threshold\n\n    TP = (prob & label).sum(1).float()\n    TN = ((~prob) & (~label)).sum(1).float()\n    FP = (prob & (~label)).sum(1).float()\n    FN = ((~prob) & label).sum(1).float()\n\n    precision = torch.mean(TP \/ (TP + FP + 1e-12))\n    recall = torch.mean(TP \/ (TP + FN + 1e-12))\n    F2 = (1 + beta**2) * precision * recall \/ (beta**2 * precision + recall + 1e-12)\n    return F2.mean(0)\n\nclass MultilabelImageClassificationBase(nn.Module):\n    def training_step(self, batch):\n        images, targets = batch \n        out = self(images)                      \n        loss = F.binary_cross_entropy(out, targets)      \n        return loss\n    \n    def validation_step(self, batch):\n        images, targets = batch \n        out = self(images)                           # Generate predictions\n        loss = F.binary_cross_entropy(out, targets)  # Calculate loss\n        score = F_score(out, targets)\n        return {'val_loss': loss.detach(), 'val_score': score.detach() }\n        \n    def validation_epoch_end(self, outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n        batch_scores = [x['val_score'] for x in outputs]\n        epoch_score = torch.stack(batch_scores).mean()      # Combine accuracies\n        return {'val_loss': epoch_loss.item(), 'val_score': epoch_score.item()}\n    \n    def epoch_end(self, epoch, result):\n        print(\"Epoch [{}], train_loss: {:.4f}, val_loss: {:.4f}, val_score: {:.4f}\".format(\n            epoch, result['train_loss'], result['val_loss'], result['val_score']))\n        \nclass ProteinCnnModel2(MultilabelImageClassificationBase):\n    def __init__(self):\n        super().__init__()\n        # Use a pretrained model\n        self.network = models.resnet152(pretrained=True)\n        # Replace last layer\n        num_ftrs = self.network.fc.in_features\n        self.network.fc = nn.Linear(num_ftrs, 10)\n    \n    def forward(self, xb):\n        return torch.sigmoid(self.network(xb))","083a6e55":"# Install gradcam\n\n!pip install pytorch-gradcam","cbcec349":"import gradcam\nfrom gradcam.utils import visualize_cam\nfrom gradcam.gradcam import GradCAMpp\nimport numpy as np","0d173786":"# Transform test image\n\nnorm_vals = ([0.0793, 0.0530, 0.0545], [0.1290, 0.0886, 0.1376])\n\ntrans = transforms.Compose([\n    transforms.Resize(256), \n    transforms.ToTensor(), \n    transforms.Normalize(*norm_vals),\n])","3c9ff4d6":"# Function used to create activation map\n\ndef activation_map(img_path):\n    \n    img = Image.open(img_path)\n    \n    print(predict_single(trans(img)))\n\n    torch_img = transforms.Compose([ transforms.Resize((224)),transforms.ToTensor()])(img).to('cpu')\n\n    normed_torch_img = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(torch_img)[None]\n    \n    model.eval()\n\n    model_conf = dict(model_type='resnet', arch=model, layer_name='layer4')\n\n    mask_pp, _ = GradCAMpp.from_config(**model_conf)(normed_torch_img)\n    \n    _ , map = visualize_cam(mask_pp, torch_img)\n\n    map_image = make_grid([torch_img.cpu() , map])\n    \n    return transforms.ToPILImage()(map_image)","331bebd3":"# Upload your model to kernel and load the path.\nmodel_path = '..\/input\/stratified-sampling-and-normalization\/protien_model.pth'","ade4d61e":"# Setup model\n\nmodel = torch.load(model_path, map_location='cpu').network\nmodel.eval()","89a9ea56":"import glob\n\n# Load test image directory\ntest_img = glob.glob('..\/input\/jovian-pytorch-z2g\/Human protein atlas\/test\/*')","24c5db8f":"# Visualize output \n\nactivation_map(test_img[110])","6b36fdac":"activation_map(test_img[112])","e7e0fcf8":"activation_map(test_img[12])","45061f67":"                                      \n                                            Low                                 High\n\n![image.png](attachment:image.png)","513db001":"### Visulaize model's prediction","ac87f1de":"\n>\nRamprasaath R. Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, Dhruv Batra\nWe propose a technique for producing \"visual explanations\" for decisions from a large class of CNN-based models, making them more transparent. Our approach - Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept, flowing into the final convolutional layer to produce a coarse localization map highlighting important regions in the image for predicting the concept. Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers, (2) CNNs used for structured outputs, (3) CNNs used in tasks with multimodal inputs or reinforcement learning, without any architectural changes or re-training. We combine Grad-CAM with fine-grained visualizations to create a high-resolution class-discriminative visualization and apply it to off-the-shelf image classification, captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into their failure modes, (b) are robust to adversarial images, (c) outperform previous methods on localization, (d) are more faithful to the underlying model and (e) help achieve generalization by identifying dataset bias. For captioning and VQA, we show that even non-attention based models can localize inputs. We devise a way to identify important neurons through Grad-CAM and combine it with neuron names to provide textual explanations for model decisions. Finally, we design and conduct human studies to measure if Grad-CAM helps users establish appropriate trust in predictions from models and show that Grad-CAM helps untrained users successfully discern a 'stronger' nodel from a 'weaker' one even when both make identical predictions. Our code is available at this https URL, along with a demo at this http URL, and a video at this http URL.\n>\n\nhttps:\/\/arxiv.org\/abs\/1610.02391","de5cfd07":" A technique for generating class activation maps using the global average pooling (GAP) in CNNs. A class activation map for a particular category indicates the discriminative image regions used by the CNN to identify that category. The procedure for generating these maps is illustrated as follows:\n\n![image_png](http:\/\/cnnlocalization.csail.mit.edu\/framework.jpg)\n\nClass activation maps could be used to intepret the prediction decision made by the CNN. The left image below shows the class activation map of top 5 predictions respectively, you can see that the CNN is triggered by different semantic regions of the image for different predictions. The right image below shows the CNN learns to localize the common visual patterns for the same object class.\n\n![image.png](attachment:image.png)\n\n\nFurthermore, the deep features from our networks could be used for generic localization, with newly trained SVM's weights to generate the class activation map, then you could get class-specific saliency map for free. Check the paper for the detail or check the supplementary materials for more visualization.","4b3cb36e":"### You could observe that, in red areas model activates. So the we could visualize how our model works.","a2086eaa":"### Import helper functions","8092e7ce":"### NOTE:- This activation_map function supports only pretrained resnet protien model (created by given funtion on competition starter notebooks), but works on any version of resnet (resnet18, resnet34, resnet50, res101, resnet152).","25aad443":"## More Examples","9591bdb3":"### **Class activation maps used to visualize the places where the model activates more. It helps you to determine how to improve your model.**","b8959186":"### Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization using class activation maps","89b6a4a2":"## Class Activation Maps","757210d7":"# Copy and Edit the kernel -> Load your model -> Visualize your model performance !!!","0d141875":"Source: http:\/\/cnnlocalization.csail.mit.edu\/"}}