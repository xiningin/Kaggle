{"cell_type":{"1625dd53":"code","24946395":"code","319c0969":"code","28e4f2ec":"code","97001db0":"code","33d3b90f":"code","7d11f51f":"code","95342c06":"code","744f9cb1":"code","eb616ab5":"code","821ce7ac":"code","72d851bb":"code","e157200a":"code","ea259708":"code","7c088610":"code","b30e5ad6":"code","8db369a3":"code","e4769f87":"code","19f874b8":"code","2d777f5f":"code","839b7e7f":"code","9255b589":"code","2372936d":"code","2b9819f8":"code","3cc5c258":"code","6ddab132":"code","8b2b5cee":"code","a435b5c6":"code","96baa5b6":"code","71d23685":"code","64734f95":"code","f546c923":"code","d47f8809":"code","da487073":"code","dc6d203a":"code","e56ddbcd":"code","1a21aafb":"code","5126502e":"code","755ee051":"code","8de46cb4":"code","24e33b9d":"code","80ec9b65":"code","086f1132":"code","d880e89b":"code","977cdcef":"code","ed0d28e8":"code","f5c84deb":"code","101e1088":"code","e493430f":"code","c1f0fd01":"code","0da45f59":"code","b4e37919":"code","c9d342c9":"code","155cb33b":"code","5c9a453e":"code","c355382b":"code","0efd2e36":"code","0032ddd7":"code","ea627c71":"code","ccd58e42":"markdown","9016c1af":"markdown","bc610bde":"markdown","8571feb1":"markdown","ba7a4afd":"markdown","af410306":"markdown","12969e9e":"markdown","d9b3b29d":"markdown","2ce13a64":"markdown","88eb0a1f":"markdown","9fa6f3ac":"markdown"},"source":{"1625dd53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport openslide\nimport os\nimport cv2\nimport PIL\nfrom IPython.display import Image, display\nfrom keras.applications.vgg16 import VGG16,preprocess_input\n# Plotly for the interactive viewer (see last section)\nimport plotly.graph_objs as go\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential, Model,load_model\nfrom keras.applications.vgg16 import VGG16,preprocess_input\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.preprocessing.image import ImageDataGenerator,load_img, img_to_array\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten,BatchNormalization,Activation\nfrom keras.layers import GlobalMaxPooling2D\nfrom keras.models import Model\nfrom keras.optimizers import Adam, SGD, RMSprop\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers.normalization import BatchNormalization\nimport gc\nimport matplotlib.pyplot as plt\n%inline matplotlib\nimport skimage.io\nfrom sklearn.model_selection import KFold\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport tensorflow as tf\nfrom tensorflow.python.keras import backend as K\nsess = K.get_session()","24946395":"from keras.callbacks.callbacks import ReduceLROnPlateau, EarlyStopping\nimport tensorflow as tf\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import load_model\nfrom keras.applications.xception import Xception\nimport time\nimport seaborn as sns\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc","319c0969":"\nfrom numpy.random import seed\nseed(1)","28e4f2ec":"train=pd.read_csv('\/kaggle\/input\/prostate-cancer-grade-assessment\/train.csv')\ntrain.head()\ntrain_copy = train.copy()","97001db0":"img=openslide.OpenSlide('\/kaggle\/input\/prostate-cancer-grade-assessment\/train_images\/2fd1c7dc4a0f3a546a59717d8e9d28c3.tiff')\ndisplay(img.get_thumbnail(size=(512,512)))","33d3b90f":"img.dimensions","7d11f51f":"patch = img.read_region((18500,4100), 0, (256, 256))\n\n# Display the image\ndisplay(patch)\n# Close the opened slide after use\nimg.close()","95342c06":"train['isup_grade'].value_counts()","744f9cb1":"train.head()","eb616ab5":"labels=[]\ndata=[]\ndata_dir='\/kaggle\/input\/panda-resized-train-data-512x512\/train_images\/train_images\/'\nfor i in range(train.shape[0]):\n    data.append(data_dir + train['image_id'].iloc[i]+'.png')\n    labels.append(train['isup_grade'].iloc[i])\ndf=pd.DataFrame(data)\ndf.columns=['images']\ndf['isup_grade']=labels","821ce7ac":"df.head()","72d851bb":"X_train, X_val, y_train, y_val = train_test_split(df['images'],df['isup_grade'], test_size=0.1, random_state=42)\n","e157200a":"train=pd.DataFrame(X_train)\ntrain.columns=['images']\ntrain['isup_grade']=y_train\n\nvalidation=pd.DataFrame(X_val)\nvalidation.columns=['images']\nvalidation['isup_grade']=y_val\n\ntrain['isup_grade']=train['isup_grade'].astype(str)\nvalidation['isup_grade']=validation['isup_grade'].astype(str)","ea259708":"#Image data generator generates varied images with the input data for better prediction for the models\n#shear is avoided since the cancerous cells look like sheared normal cells","7c088610":"train_datagen = ImageDataGenerator(rescale=1.\/255,rotation_range=40,\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    #zoom_range=[0.8, 1.2],        \n    horizontal_flip=True, vertical_flip = True,\n    brightness_range=[0.9, 1.1],\n    width_shift_range=1.0,\n    height_shift_range=1.0)#,\n    #validation_split=0.1)\n\n\nval_datagen=train_datagen = ImageDataGenerator(rescale=1.\/255)\ntrain_generator = train_datagen.flow_from_dataframe(\n    train,\n    x_col='images',\n    y_col='isup_grade',\n    target_size=(224, 224),\n    batch_size=32,\n    shuffle = True,\n    class_mode='categorical')\n\nvalidation_generator = val_datagen.flow_from_dataframe(\n    validation,\n    x_col='images',\n    y_col='isup_grade',\n    target_size=(224, 224),\n    batch_size=32,\n    class_mode='categorical')","b30e5ad6":"filenames = validation_generator.filenames\nnb_samples = len(filenames)\n\n","8db369a3":"#y_true = validation_generator.classes\n\n","e4769f87":"#y_true","19f874b8":"#VGG16 model","2d777f5f":"def vgg16_model( num_classes=None):\n\n    model = VGG16(weights='\/kaggle\/input\/keras-pretrained-models\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5', include_top=False, input_shape=(224, 224, 3))\n    #x=Dropout(0.2)(model.output)\n    x=Flatten()(model.output)\n    #x =Dense(32, activation = 'relu')(x)\n    output=Dense(num_classes,activation='softmax')(x)\n    model=Model(model.input,output)\n    return model\n\n#vgg_conv=vgg16_model(6)","839b7e7f":"#VGG19 model-- Uncomment last line to run the model","9255b589":"from keras.applications.vgg19 import VGG19\ndef vgg19_model(num_classes = None):\n    #vgg19_weights = '..\/input\/vgg19\/vgg19_weights_tf_dim_ordering_tf_kernels.h5'\n    model = VGG19(weights='imagenet',include_top=False, input_shape=(224, 224, 3))\n    #model = VGG19(weights='\/input\/vgg19\/vgg19_weights_tf_dim_ordering_tf_kernels.h5', include_top=False, input_shape=(224, 224, 3))\n    x=Dropout(0.3)(model.output)\n    x=Flatten()(x)\n    x =Dense(32, activation = 'relu')(x)\n    x =Dropout(0.2)(x)\n    output=Dense(num_classes,activation='softmax')(x)\n    model=Model(model.input,output)\n    return model\n#vgg19_conv = vgg19_model(6)","2372936d":"#InceptionV3 model- Uncomment last line to run the model","2b9819f8":"from keras.applications.inception_v3 import InceptionV3\ndef InceptionV3_model(num_classes = None):\n    InceptionV3_weights = '..\/input\/inceptionv3\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    #model = ResNet50(weights='imagenet', include_top = False, input_shape = (224,224,3))\n    model = InceptionV3(weights= InceptionV3_weights, include_top=False, input_shape=(224, 224, 3))\n    #model = InceptionV3(weights='imagenet', include_top = False, input_shape = (224,224,3))\n    x=Dropout(0.3)(model.output)\n    #x=Flatten()(model.output)\n    #x =Dense(64, activation = 'relu')(model.output)\n    #x = tf.compat.v1.keras.layers.GlobalAveragePooling2D()(model.output)\n    x=Flatten()(x)\n    #x =Dropout(0.2)(x)\n    x =Dense(32, activation = 'relu')(x)\n    x =Dropout(0.2)(x)\n    output=Dense(num_classes,activation='softmax')(x)\n    model=Model(model.input,output)\n    return model\nInceptionV3_conv = InceptionV3_model(6)","3cc5c258":"#Resnet50 model- Uncomment last line to run the model","6ddab132":"#!pip install tensorflow==2.1.0","8b2b5cee":"from keras.applications.resnet50 import ResNet50\ndef ResNet50_model(num_classes = None):\n    ResNet_weights = '..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    #model = ResNet50(weights='imagenet', include_top = False, input_shape = (224,224,3))\n    model = ResNet50(weights= ResNet_weights, include_top=False, input_shape=(224, 224, 3))\n    #x=Dropout(0.2)(model.output)\n    #x = GlobalAveragePooling2D()(model.output)\n    x=Flatten()(model.output)\n    #x =Dropout(0.2)(x)\n    x =Dense(16, activation = 'relu')(x)\n    x =Dropout(0.2)(x)\n    output=Dense(num_classes,activation='softmax')(x)\n    model=Model(model.input,output)\n    return model\n#ResNet50_conv = ResNet50_model(6)","a435b5c6":"from keras.applications import InceptionResNetV2\ndef InceptionResnet_model(num_classes = None):\n    InceptionResnet_weights = '..\/input\/inceptionresnetv2\/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5'\n    #model = inception_resnet_v2(weights='imagenet', include_top = False, input_shape = (224,224,3))\n    model = InceptionResNetV2(weights= InceptionResnet_weights, include_top=False, input_shape=(224, 224, 3))\n    #x=Dropout(0.2)(model.output)\n    #x = GlobalAveragePooling2D()(model.output)\n    x=Flatten()(model.output)\n    #x =Dropout(0.2)(x)\n    x =Dense(16, activation = 'relu')(x)\n    x =Dropout(0.2)(x)\n    output=Dense(num_classes,activation='softmax')(x)\n    model=Model(model.input,output)\n    return model\n#InceptionResnet_conv = InceptionResnet_model(6)","96baa5b6":"#Uncomment any one to run the corresponding summary","71d23685":"#vgg_conv.summary()","64734f95":"#vgg19_conv.summary()","f546c923":"\nInceptionV3_conv.summary()","d47f8809":"#ResNet50_conv.summary()","da487073":"#InceptionResnet_conv.summary()","dc6d203a":"#Including batch Normalization","e56ddbcd":"def freeze(model):\n    # freeze layers before 99\n    for layer in model.layers[:99]:\n        layer.trainable = False\n        if layer.name.startswith('batch_normalization'):\n            layer.trainable = True\n        if layer.name.endswith('bn'):\n            layer.trainable = True\n\n    # unfreeze layers after 99\n    for layer in model.layers[99:]:\n        layer.trainable = True\n        \n\n        \n        \n        ","1a21aafb":"#freeze(vgg16_conv)\n#freeze(vgg19_conv)\n#freeze(InceptionV3_conv)\n#freeze(ResNet50_conv)","5126502e":"#y_pred = np.argmax(InceptionV3_conv.predict_generator(validation_generator, steps= len(validation_generator)), axis=1)","755ee051":"#kappa score is the metric we use since two types of evaluation is done on the data set \n\n#The kappa statistic.\n#According to Cohen's original article, \n#values \u2264 0 as indicating no agreement and 0.01\u20130.20 as none to slight,\n#0.21\u20130.40 as fair, 0.41\u2013 0.60 as moderate,\n#0.61\u20130.80 as substantial, \n#0.81\u20131.00 as almost perfect agreement.","8de46cb4":"def kappa_score(y_true, y_pred):\n    \n    y_true=tf.math.argmax(y_true)\n    y_pred=tf.math.argmax(y_pred)\n    #return tf.compat.v1.py_func(cohen_kappa_score,(y_true, y_pred),tf.double)\n    return tf.compat.v1.py_func(cohen_kappa_score,(y_true,y_pred),tf.double)\n    #return (y_true,y_pred)","24e33b9d":"import numpy as np\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\n\nimport keras.backend as K\nimport tensorflow as tf\n\n\ndef kappa_keras(y_true, y_pred):\n\n    y_true = K.cast(K.argmax(y_true, axis=-1), dtype='int32')\n    y_pred = K.cast(K.argmax(y_pred, axis=-1), dtype='int32')\n    #print(y_true)\n    #print(y_pred)\n    # Figure out normalized expected values\n    min_rating = K.minimum(K.min(y_true), K.min(y_pred))\n    max_rating = K.maximum(K.max(y_true), K.max(y_pred))\n\n    # shift the values so that the lowest value is 0\n    # (to support scales that include negative values)\n    y_true = K.map_fn(lambda y: y - min_rating, y_true, dtype='int32')\n    y_pred = K.map_fn(lambda y: y - min_rating, y_pred, dtype='int32')\n\n    # Build the observed\/confusion matrix\n    num_ratings = max_rating - min_rating + 1\n    observed = tf.math.confusion_matrix(y_true, y_pred,\n                                num_classes=num_ratings)\n    num_scored_items = K.shape(y_true)[0]\n\n    weights = K.expand_dims(K.arange(num_ratings), axis=-1) - K.expand_dims(K.arange(num_ratings), axis=0)\n    weights = K.cast(K.pow(weights, 2), dtype='float64')\n\n    hist_true = tf.math.bincount(y_true, minlength=num_ratings)\n    hist_true = hist_true[:num_ratings] \/ num_scored_items\n    hist_pred = tf.math.bincount(y_pred, minlength=num_ratings)\n    hist_pred = hist_pred[:num_ratings] \/ num_scored_items\n    expected = K.dot(K.expand_dims(hist_true, axis=-1), K.expand_dims(hist_pred, axis=0))\n\n    # Normalize observed array\n    observed = observed \/ num_scored_items\n\n    # If all weights are zero, that means no disagreements matter.\n    score = tf.where(K.any(K.not_equal(weights, 0)), \n                     K.sum(weights * observed) \/ K.sum(weights * expected), \n                     0)\n    \n    return 1. - score\n\nif __name__ == '__main__':\n    y_true = np.array([2, 0, 2, 2, 0, 1])\n    y_pred = np.array([0, 0, 2, 2, 0, 2])\n    # Testing Keras implementation of QWK\n    \n    # Calculating QWK score with scikit-learn\n   \n    skl_score = cohen_kappa_score(y_true, y_pred, weights='quadratic')\n    \n    # Keras implementation of QWK work with one hot encoding labels and predictions (also it works with softmax probabilities)\n    # Converting arrays to one hot encoded representation\n    shape = (y_true.shape[0], np.maximum(y_true.max(), y_pred.max()) + 1)\n\n    y_true_ohe = np.zeros(shape)\n    y_true_ohe[np.arange(shape[0]), y_true] = 1\n\n    y_pred_ohe = np.zeros(shape)\n    y_pred_ohe[np.arange(shape[0]), y_pred] = 1\n    \n    # Calculating QWK score with Keras\n    with tf.compat.v1.Session() as sess:\n        keras_score = kappa_keras(y_true_ohe, y_pred_ohe).eval()\n    \n    #print('Scikit-learn score: {:.03}, Keras score: {:.03}'.format(skl_score, keras_score))\n    ","80ec9b65":"opt = SGD(lr= 0.0005, momentum=0.9,decay=1e-4)\n#vgg_conv.compile(loss='binary_crossentropy',optimizer=opt,metrics=[kappa_keras, 'accuracy'])\n#vgg19_conv.compile(loss='binary_crossentropy',optimizer=opt,metrics=[kappa_keras, 'accuracy'])\nInceptionV3_conv.compile(loss='binary_crossentropy',optimizer=opt,metrics=[kappa_score,kappa_keras,'accuracy'])\n#ResNet50_conv.compile(loss='binary_crossentropy',optimizer=opt,metrics=[kappa_score,kappa_keras, 'accuracy'])\n#InceptionResnet_conv.compile(loss='binary_crossentropy',optimizer=opt,metrics=[kappa_score,kappa_keras, 'accuracy'])","086f1132":"nb_epochs = 4\nbatch_size=16\nnb_train_steps = train.shape[0]\/\/batch_size\nnb_val_steps=validation.shape[0]\/\/batch_size\n#nb_train_steps = 128\n#nb_val_steps = 64\nprint(\"Number of training and validation steps: {} and {}\".format(nb_train_steps,nb_val_steps))","d880e89b":"#vgg_conv.fit_generator( train_generator,steps_per_epoch=nb_train_steps,epochs=10,validation_data=validation_generator,\n#validation_steps=nb_val_steps),\n","977cdcef":"#generator to activate the augmentation.\ncallbacks = [ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1, factor=0.5),\n             EarlyStopping(monitor='val_loss', patience=3),\n             ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]","ed0d28e8":"#tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\".\/logs\")","f5c84deb":"#vgg19_conv.fit_generator( train_generator,steps_per_epoch=nb_train_steps,epochs=5,validation_data=validation_generator,validation_steps=nb_val_steps,callbacks = callbacks)","101e1088":"history =InceptionV3_conv.fit_generator( train_generator,steps_per_epoch=nb_train_steps,epochs=nb_epochs,validation_data=validation_generator,\nvalidation_steps=nb_val_steps, callbacks = callbacks)","e493430f":"#ResNet50_conv.fit_generator( train_generator,steps_per_epoch=nb_train_steps,epochs=20,validation_data=validation_generator,\n#validation_steps=nb_val_steps, callbacks = callbacks)","c1f0fd01":"#history = InceptionResnet_conv.fit_generator( train_generator,steps_per_epoch=nb_train_steps,epochs=2,validation_data=validation_generator,\n#validation_steps=nb_val_steps, callbacks = callbacks)","0da45f59":"def show_history(history):\n    fig, ax = plt.subplots(1, 3, figsize=(15,5))\n    ax[0].set_title('loss')\n    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    ax[1].set_title('kappa_keras')\n    ax[1].plot(history.epoch, history.history[\"kappa_keras\"], label=\"Train Quadratic Kappa score\")\n    ax[1].plot(history.epoch, history.history[\"val_kappa_keras\"], label=\"Validation Quadratic Kappa score\")\n    ax[2].set_title('accuracy')\n    ax[2].plot(history.epoch, history.history[\"accuracy\"], label=\"Train acc\")\n    ax[2].plot(history.epoch, history.history[\"val_accuracy\"], label=\"Validation accuracy\")\n    ax[0].legend()\n    ax[1].legend()\n    ax[2].legend()","b4e37919":"show_history(history)","c9d342c9":"#vgg_baseline = Model.save('VGG16_Baseline.h5')  # creates a HDF5 file 'my_model.h5'\nInceptionV3_baseline =InceptionV3_conv.save('InceptionV3_Baseline.h5')  # creates a HDF5 file 'my_model.h5'\n#Resnet50_baseline =ResNet50_conv.save('ResNet50_Baseline.h5')  # creates a HDF5 file 'my_model.h5'\n#InceptionResnet_baseline =InceptionResnet_conv.save('InceptionResnet_Baseline.h5')  # creates a HDF5 file 'my_model.h5'","155cb33b":"#vgg_baseline_weights = Model.save_weights('vgg_baseline_weights.h5')\nInceptionV3_weights =InceptionV3_conv.save_weights('InceptionV3_weights.h5')  # creates a HDF5 file 'my_model.h5'\n#Resnet50_weights =ResNet50_conv.save_weights('Resnet50_weights.h5')  # creates a HDF5 file 'my_model.h5'\n#InceptionResnet_weights =InceptionResnet_conv.save_weights('InceptionResnet_weights.h5')  # creates a HDF5 file 'my_model.h5'","5c9a453e":"os.listdir('.')","c355382b":"InceptionV3_conv.load_weights(\"best_model.h5\")\n#InceptionResnet_conv.load_weights(\"best_model.h5\")","0efd2e36":"def predict_submission(df, path, passes=1):\n    \n    df[\"image_path\"] = [path+image_id+\".tiff\" for image_id in df[\"image_id\"]]\n    df[\"isup_grade\"] = 0\n    \n    for idx, row in df.iterrows():\n        prediction_per_pass = []\n        for i in range(passes):\n            model_input = np.array([get_random_samples(row.image_path)\/255.])\n            input_image1 = model_input[:,0,:,:]\n            input_image2 = model_input[:,1,:,:]\n            input_image3 = model_input[:,2,:,:]\n\n            prediction = InceptionResnet_conv.predict([input_image1,input_image2,input_image3])\n            prediction_per_pass.append(np.argmax(prediction))\n            \n        df.at[idx,\"isup_grade\"] = np.mean(prediction_per_pass)\n    df = df.drop('image_path', 1)\n    return df[[\"image_id\",\"isup_grade\"]]","0032ddd7":"# submission code from https:\/\/www.kaggle.com\/frlemarchand\/high-res-samples-into-multi-input-cnn-keras\ndef predict_submission(df, path):\n    \n    df[\"image_path\"] = [path+image_id+\".tiff\" for image_id in df[\"image_id\"]]\n    df[\"isup_grade\"] = 0\n    predictions = []\n    for idx, row in df.iterrows():\n        print(row.image_path)\n        img=skimage.io.imread(str(row.image_path))\n        img = cv2.resize(img, (224,224))\n        img = cv2.resize(img, (224,224))\n        img = img.astype(np.float32)\/255.\n        img=np.reshape(img,(1,224,224,3))\n       \n    \n        prediction=InceptionResnet_conv.predict(img)\n        predictions.append(np.argmax(prediction))\n            \n    df[\"isup_grade\"] = predictions\n    df = df.drop('image_path', 1)\n    return df[[\"image_id\",\"isup_grade\"]]","ea627c71":"test_path = \"..\/input\/prostate-cancer-grade-assessment\/test_images\/\"\nsubmission_df = pd.read_csv(\"..\/input\/prostate-cancer-grade-assessment\/sample_submission.csv\")\n\nif os.path.exists(test_path):\n    test_df = pd.read_csv(\"..\/input\/prostate-cancer-grade-assessment\/test.csv\")\n    submission_df = predict_submission(test_df, test_path)\nelse:\n    print('submission csv not found')\n\n    \ndel InceptionV3_conv\nsubmission_df.to_csv('submission.csv', index=False)\nsubmission_df.head()","ccd58e42":"![](https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/PANDA\/Screen%20Shot%202020-04-08%20at%202.03.53%20PM.png)\n\n\n\nThe most common (blue outline, Gleason pattern 3) and second most common (red outline, Gleason pattern 4) cancer growth patterns present in the biopsy dictate the Gleason score (3+4 for this biopsy), which in turn is converted into an ISUP grade (2 for this biopsy) following guidelines of the International Society of Urological Pathology. Biopsies not containing cancer are represented by an ISUP grade of 0 in this challenge.\n\nLet's also look at the class distribution.","9016c1af":"Let's take a look at the architecture.","bc610bde":"These images are pretty big in size for training I'll be using the resized 512x512 dataset uploaded by @xhlulu.","8571feb1":"Hyperparameter Tuning ","ba7a4afd":"We are asked to predict ISUP grade(scale 0-5) for each prostate biopsy image, which is in turn derived from the gleason score(three categories 3,4,5).Before diving deeper into the competition let's try to understand these first.\n\n## Gleason Score:-\nSince prostate tumors are often made up of cancerous cells that have different grades, two grades are assigned for each patient.  A primary grade is given to describe the cells that make up the largest area of the tumor and a secondary grade is given to describe the cells of the next largest area.  For instance, if the Gleason Score is written as 3+4=7, it means most of the tumor is grade 3 and the next largest section of the tumor is grade 4, together they make up the total Gleason Score.  If the cancer is almost entirely made up of cells with the same score, the grade for that area is counted twice to calculated the total Gleason Score. \n\nThe samples are made up of glandular tissue and connective tissue. The glands are hollow structures, which can be seen as white \u201choles\u201d or branched cavities in the WSI. The appearance of the glands forms the basis of the Gleason grading system.\n\n![](https:\/\/www.prostateconditions.org\/images\/about\/murtagh7e_c114_f04-2.png)\n\n\nThis is how Gleason score is decided from(3-5), 5 being the most severe. To understand more [read this](https:\/\/www.kaggle.com\/c\/prostate-cancer-grade-assessment\/overview\/additional-resources).\n\n\n## ISUP grade\nAccording to current guidelines by the International Society of Urological Pathology (ISUP), the Gleason scores are summarized into an ISUP grade on a scale from 1 to 5 according to the following rule:\n\n* Gleason score 6 = ISUP grade 1\n* Gleason score 7 (3 + 4) = ISUP grade 2\n* Gleason score 7 (4 + 3) = ISUP grade 3\n* Gleason score 8 = ISUP grade 4\n* Gleason score 9-10 = ISUP grade 5.\n\nAn example as provided on the official page of the competition can make us understand better.","af410306":"Inference\n","12969e9e":"The classes are imbalanced, more severe cases are underrepresented.\n\nLet's start preparing the images for training.","d9b3b29d":"Data-\n* train.csv:- image_id,karlonski\/radhound, gleason and ISUP grade.\n* test.csv:-  image_ids and data_provider.\n* train_images:- A folder containing biopsy images in a large multilevel .tiff file, some of them will be used for testing).\n* train_label_masks:-Folder with segmentation masks showing which parts of the image led to the ISUP grade. Not all training images have label masks, and there may be false positives or false negatives in the label masks. These masks may assist with the development of strategies for selecting useful image subsamples and the values might depend also on the data provider.","2ce13a64":"VGG16 Baseline","88eb0a1f":"\nLet's take a look at one of the biopsy.","9fa6f3ac":"So the basic preprocessing I've done is:-\n* Normalizing the images.\n* Reshape the images to be of shape 224,224,3\n* Basic image augmentation like rotation, flipping etc."}}