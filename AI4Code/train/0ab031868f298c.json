{"cell_type":{"fab1b0f3":"code","a6bedeae":"code","b025a1b8":"code","e7839b00":"code","efd2afd3":"code","f5b8bcb2":"code","7a9d82b9":"code","eea40030":"code","55555df4":"code","d9f58199":"code","5ee20317":"markdown","51a66c97":"markdown","f36a21f0":"markdown","d0b8f442":"markdown","979acddb":"markdown","4fb50842":"markdown","0a03811e":"markdown","0e3b084a":"markdown"},"source":{"fab1b0f3":"#@title Installation\n!nvidia-smi -L\n!pip install rudalle==0.0.1rc1 > \/dev\/null","a6bedeae":"#@title Imports\nfrom rudalle.pipelines import generate_images, show, super_resolution, cherry_pick_by_clip\nfrom rudalle import get_rudalle_model, get_tokenizer, get_vae, get_realesrgan, get_ruclip\nfrom rudalle.utils import seed_everything","b025a1b8":"# %%time\ndevice = 'cuda'\ndalle = get_rudalle_model('Malevich', pretrained=True, fp16=True, device=device)\n# %%time\ntry:\n    realesrgan, tokenizer, ruclip, ruclip_processor\nexcept NameError:\n    realesrgan = get_realesrgan('x4', device=device)\n    tokenizer = get_tokenizer()\n    vae = get_vae().to(device)\n    ruclip, ruclip_processor = get_ruclip('ruclip-vit-base-patch32-v5')\n    ruclip = ruclip.to(device)","e7839b00":"import torch\nimport torchvision\nimport transformers\nimport more_itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nfrom rudalle import utils\n\n\ndef generate_images(text, tokenizer, dalle, vae, top_k, top_p, images_num, temperature=1.0, bs=8, seed=None,\n                    use_cache=True):\n    # TODO docstring\n    if seed is not None:\n        utils.seed_everything(seed)\n\n    vocab_size = dalle.get_param('vocab_size')\n    text_seq_length = dalle.get_param('text_seq_length')\n    image_seq_length = dalle.get_param('image_seq_length')\n    total_seq_length = dalle.get_param('total_seq_length')\n    device = dalle.get_param('device')\n\n    text = text.lower().strip()\n    input_ids = tokenizer.encode_text(text, text_seq_length=text_seq_length)\n    pil_images, scores = [], []\n    for chunk in more_itertools.chunked(range(images_num), bs):\n        chunk_bs = len(chunk)\n        with torch.no_grad():\n            attention_mask = torch.tril(torch.ones((chunk_bs, 1, total_seq_length, total_seq_length), device=device))\n            out = input_ids.unsqueeze(0).repeat(chunk_bs, 1).to(device)\n            # out = input_ids.unsqueeze(0).repeat(chunk_bs, total_seq_length \/\/ len(input_ids)).to(device)\n            has_cache = False\n            sample_scores = []\n            for i in tqdm(range(len(input_ids), total_seq_length)):\n                logits, has_cache = dalle(out[:, :i], attention_mask,\n                                          has_cache=has_cache, use_cache=use_cache, return_loss=False)\n                logits = logits[:, -1, vocab_size:]\n                logits \/= temperature\n                filtered_logits = transformers.top_k_top_p_filtering(logits, top_k=top_k, top_p=top_p)\n                probs = torch.nn.functional.softmax(filtered_logits, dim=-1)\n                sample = torch.multinomial(probs, 1)\n                sample_scores.append(probs[torch.arange(probs.size(0)), sample.transpose(0, 1)])\n                out = torch.cat((out, sample), dim=-1)\n                # out[:, i:i+1] = sample\n            codebooks = out[:, -image_seq_length:]\n            images = vae.decode(codebooks)\n            pil_images += utils.torch_tensors_to_pil_list(images)\n            scores += torch.cat(sample_scores).sum(0).detach().cpu().numpy().tolist()\n    return pil_images, scores\n\n\ndef show(pil_images, nrow=4):\n    imgs = torchvision.utils.make_grid(utils.pil_list_to_torch_tensors(pil_images), nrow=nrow)\n    if not isinstance(imgs, list):\n        imgs = [imgs.cpu()]\n    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(14, 14))\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = torchvision.transforms.functional.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n    fig.show()\n    plt.show()","efd2afd3":"import math\nimport torch\nimport torch.nn.functional as F\nfrom rudalle.dalle.utils import divide, split_tensor_along_last_dim\n\n\n@torch.jit.script\ndef gelu_impl(x):\n    \"\"\"OpenAI's gelu implementation.\"\"\"\n    return 0.5 * x * (1.0 + torch.tanh(0.7978845608028654 * x * (1.0 + 0.044715 * x * x)))\n\n\ndef gelu(x):\n    return gelu_impl(x)\n\n\ndef dalle_layer_forward(self, hidden_states, ltor_mask, has_cache, use_cache):\n    # hidden_states: [b, s, h]\n    # ltor_mask: [1, 1, s, s]\n\n    # Layer norm at the begining of the transformer layer.\n    # output = hidden_states\n    # att_has_cache, mlp_has_cache = True, True\n    layernorm_output = self.input_layernorm(hidden_states)\n\n    # Self attention.\n    attention_output, att_has_cache = self.attention(\n        layernorm_output, ltor_mask, has_cache=has_cache, use_cache=use_cache)  # if False else layernorm_output, True\n\n    if self.cogview_sandwich_layernorm:\n        attention_output = self.before_first_addition_layernorm(\n            attention_output, has_cache=has_cache, use_cache=use_cache)\n\n    # Residual connection.\n    layernorm_input = hidden_states + attention_output\n\n    # Layer norm post the self attention.\n    layernorm_output = self.post_attention_layernorm(\n        layernorm_input, has_cache=has_cache, use_cache=use_cache)\n\n    # MLP.\n    # mlp_has_cache = True\n    mlp_output, mlp_has_cache = self.mlp(\n        layernorm_output, has_cache=has_cache, use_cache=use_cache\n        )  # if False else layernorm_output, True\n\n    if self.cogview_sandwich_layernorm:\n        mlp_output = self.before_second_addition_layernorm(\n            mlp_output, has_cache=has_cache, use_cache=use_cache)\n\n    # Second residual connection.\n    output = layernorm_input + mlp_output\n\n    return output, att_has_cache and mlp_has_cache\n\n\n# def patch_full(self, func_name)\n#     orig = getattr(self, func_name)\n#     def patched(x):\n#         return orig(x)\n#     setattr(self, func_name, patched)\n\n\n# About 1.3x speedup. Query\/key\/value cat is surprisingly fast.\ndef dalle_sa_forward(self, hidden_states, ltor_mask, has_cache=False, use_cache=False,):\n    # hidden_states: [b, s, h]\n    # ltor_mask: [1, 1, s, s]\n    # Attention heads. [b, s, hp]\n    \n    def calculate_attention_scores(query_layer, key_layer, ltor_mask):\n        key_t = key_layer.transpose(-1, -2)\n        if self.cogview_pb_relax:\n            attention_scores = torch.matmul(\n                query_layer \/ math.sqrt(self.hidden_size_per_attention_head),\n                key_t\n            )\n        else:\n            attention_scores = torch.matmul(query_layer, key_t) \/ math.sqrt(self.hidden_size_per_attention_head)\n        # print(attention_scores.shape, ltor_mask.shape)\n        ltor_mask = ltor_mask[:, :, -attention_scores.shape[-2]:]\n        # print(attention_scores.shape, ltor_mask.shape)\n        attention_scores = torch.mul(attention_scores, ltor_mask) - 10000.0 * (1.0 - ltor_mask)\n        if self.cogview_pb_relax:\n            # normalize attention scores. Should not affect resulting softmax value\n            alpha = 32\n            attention_scores_scaled = attention_scores \/ alpha\n            attention_scores_scaled_maxes, _ = attention_scores_scaled.detach().view(\n                [attention_scores.size(0), attention_scores.size(1), -1]\n            ).max(dim=-1)  # max per head per sample\n            attention_scores_scaled_maxes = attention_scores_scaled_maxes.unsqueeze(-1).unsqueeze(-1).expand(\n                [-1, -1, attention_scores.size(2), attention_scores.size(3)]\n            )  # expand to [b, np, s, s]\n            attention_scores = (attention_scores_scaled - attention_scores_scaled_maxes) * alpha\n        return attention_scores\n    \n    t = hidden_states.shape[-2]\n    if has_cache and use_cache:\n        mixed_x_layer = self.query_key_value(hidden_states[:, self.past_output.shape[-2]:, :])\n    else:\n        mixed_x_layer = self.query_key_value(hidden_states)\n\n    (mixed_query_layer,\n        mixed_key_layer,\n        mixed_value_layer) = split_tensor_along_last_dim(mixed_x_layer, 3)\n\n    query_layer = self._transpose_for_scores(mixed_query_layer)\n    key_layer = self._transpose_for_scores(mixed_key_layer)\n    value_layer = self._transpose_for_scores(mixed_value_layer)\n\n    if use_cache and has_cache:\n        value_layer = torch.cat((self.past_value, value_layer), dim=-2)\n        key_layer = torch.cat((self.past_key, key_layer), dim=-2)\n    attention_scores = calculate_attention_scores(\n        query_layer=query_layer, key_layer=key_layer, ltor_mask=ltor_mask\n    )\n\n    if use_cache:\n        # self.past_query = query_layer\n        self.past_key = key_layer\n        self.past_value = value_layer\n    else:\n        has_cache = False\n\n    if use_cache and has_cache:\n        attention_scores = attention_scores[..., -1:, :]\n        # value_layer = value_layer[..., -1:, :]\n    \n    # Attention probabilities. [b, np, s, s]\n    attention_probs = torch.nn.Softmax(dim=-1)(attention_scores)\n\n    # This is actually dropping out entire tokens to attend to, which might\n    # seem a bit unusual, but is taken from the original Transformer paper.\n    attention_probs = self.attention_dropout(attention_probs)\n\n    # Context layer.\n    # [b, np, s, hn]\n    context_layer = torch.matmul(attention_probs, value_layer)\n\n    # [b, s, np, hn]\n    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n\n    new_context_layer_shape = context_layer.size()[:-2] + (self.hidden_size,)\n    # [b, s, hp]\n    context_layer = context_layer.view(*new_context_layer_shape)\n\n    # Output. [b, s, h]\n    output = self.dense(context_layer)\n\n    # print(output.shape)\n    if use_cache:\n        # Can be simplified, but I didn't for readability's sake\n        if has_cache:\n            output = torch.cat((self.past_output, output), dim=-2)\n            self.past_output = output\n        else:\n            self.past_output = output\n        has_cache = True \n    # print(0, output.shape, t)\n    output = self.output_dropout(output)\n    # output = hidden_states\n    return output, has_cache\n\n\n# This version of MLP .forward() is better in theory \n# But in practice it's the same\n# Uncomment and swap their names to turn on\n# def dalle_mlp_forward_(self, hidden_states, has_cache=False, use_cache=False):\n#     t = hidden_states.shape[-2]\n#     try:\n#         dont_alloc = self.past_x.shape[-2] >= t\n#     except AttributeError:\n#         dont_alloc = False\n#     if has_cache and use_cache and dont_alloc:\n#         hidden_states = hidden_states[:, -1:]\n\n#     # [b, s, 4hp]\n#     x = self.dense_h_to_4h(hidden_states)\n#     x = gelu(x)\n#     # [b, s, h]\n#     x = self.dense_4h_to_h(x)\n#     if use_cache:\n#         if has_cache and dont_alloc:\n#             self.past_x[:, t-1:t] = x\n#             x = self.past_x[:, :t]\n#         else:\n#             # print(\"alloc\")\n#             allocate_every = 512\n#             n = -(-t \/\/ allocate_every) * allocate_every  # allocate every 512\n#             # print(n)\n#             self.past_x = x.clone().repeat((1, -(-n\/\/t), 1))  # hack\n#         has_cache = True\n#     else:\n#         has_cache = False\n#     output = self.dropout(x)\n#     return output, has_cache\n\n\ndef dalle_mlp_forward(self, hidden_states, has_cache=False, use_cache=False):\n    if has_cache and use_cache:\n        hidden_states = hidden_states[:, self.past_x.shape[1]:]\n\n    # [b, s, 4hp]\n    x = self.dense_h_to_4h(hidden_states)\n    x = gelu(x)\n    # [b, s, h]\n    x = self.dense_4h_to_h(x)\n    if use_cache:\n        # Can be simplified, but I didn't for readability's sake\n        if has_cache:\n            x = torch.cat((self.past_x, x), dim=-2)\n            self.past_x = x\n        else:\n            self.past_x = x\n        has_cache = True\n    else:\n        has_cache = False\n    output = self.dropout(x)\n    return output, has_cache\n\n\n# Speeds up like 6 seconds.\ndef ln_forward(self, input, has_cache=False, use_cache=False):\n    if has_cache and use_cache:\n        input = input[:, self.past_output.shape[1]:]\n    \n    output = F.layer_norm(\n        input, self.normalized_shape, self.weight, self.bias, self.eps)\n    \n    if use_cache:\n        # Can be simplified, but I didn't for readability's sake\n        if has_cache:\n            output = torch.cat((self.past_output, output), dim=1)\n            self.past_output = output\n        else:\n            self.past_output = output\n        has_cache = True\n    else:\n        has_cache = False\n    return output  # , has_cache","f5b8bcb2":"import inspect\nfrom functools import partial\n\n\nfor layer in dalle.module.transformer.layers:\n    layer.forward = partial(dalle_layer_forward, layer)\n    layer.mlp.forward = partial(dalle_mlp_forward, layer.mlp)\n    layer.attention.past_attentions = None\n    layer.attention.past_query = None\n    layer.attention.forward = partial(dalle_sa_forward, layer.attention)\n    for ln in [layer.input_layernorm,\n               layer.before_first_addition_layernorm,\n               layer.post_attention_layernorm,\n               layer.before_second_addition_layernorm]:\n        # print(inspect.getsource(ln.forward))\n        ln.forward = partial(ln_forward, ln)","7a9d82b9":"#@title ruDALLE generation\nfrom tqdm.auto import tqdm\n\n#@markdown Use Google Translate to get a prompt in Russian.\ntext = '\u043a\u0440\u0435\u0441\u043b\u043e \u0432 \u0444\u043e\u0440\u043c\u0435 \u0430\u0432\u043e\u043a\u0430\u0434\u043e' #@param {type:\"string\"}\n#@markdown Number of resolutions. Lower this to get faster generations at the cost of fidelity (will produce lower quality pictures)\nnum_resolutions = 7 #@param {type:\"integer\"}\n\nseed_everything(42)\n\npil_images = []\nscores = []\n#@markdown The triplets of TopK, TopP and Images_Num control the number of tokens to be considered by probability and count ((0.98, 1024, ...) is a good starting point)\n#@markdown Images_Num is just the number of images to be generated with the same parameters.\nfor top_k, top_p, images_num in tqdm([\n    # (2048, 0.995, 3),\n    # (1536, 0.99, 3),\n    # (64, 0.92, 1),\n    (1024, 0.98, 3),\n    (1024, 0.98, 3),\n    (512, 0.97, 3),\n    (384, 0.96, 3),\n    (256, 0.95, 3),\n    (128, 0.95, 3), \n    (64, 0.92, 1)\n][::-1][:num_resolutions]):\n    _pil_images, _scores = generate_images(text, tokenizer, dalle, vae, top_k=top_k, images_num=images_num, top_p=top_p)\n    show([pil_image for pil_image, score in sorted(zip(_pil_images, _scores), key=lambda x: -x[1])], 6)\n    pil_images += _pil_images\n    scores += _scores","eea40030":"show([pil_image for pil_image, score in sorted(zip(pil_images, scores), key=lambda x: -x[1])], 6)","55555df4":"top_images, clip_scores = cherry_pick_by_clip(pil_images, text, ruclip, ruclip_processor, device=device, count=1)\nshow(top_images, 1)","d9f58199":"sr_images = super_resolution(top_images, realesrgan)\nshow(sr_images, 1)","5ee20317":"## Creating models","51a66c97":"## super resolution","f36a21f0":"## Monkey patch","d0b8f442":"# Optimized ruDALLE v2.0\nContains minor patches like caching and enabling `ipywidgets` for `tqdm`. By @nev#4905","979acddb":"# That's it!","4fb50842":"### auto-cherry-pick by ruCLIP","0a03811e":"## Installation and imports","0e3b084a":"## Generation by ruDALLE"}}