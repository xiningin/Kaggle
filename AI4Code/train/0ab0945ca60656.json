{"cell_type":{"a943b75b":"code","908505bc":"code","4de68e30":"code","bdf61ff5":"code","692484db":"code","6145900e":"code","efbe4a20":"code","98c87e80":"code","a6ecbe38":"code","3dd050aa":"code","8ceeb0f9":"code","e723b226":"code","60f3ba59":"code","f6fb9a5a":"code","7c394655":"code","b1c5f974":"code","f161ad91":"code","84c6b3a8":"markdown","1902b686":"markdown","63c30f09":"markdown","d4251a3c":"markdown","31a72850":"markdown","376fefa5":"markdown","b35473d0":"markdown","b4a3520b":"markdown","4a80a162":"markdown","d56a65ec":"markdown","13866265":"markdown","2636f692":"markdown","e2c89df8":"markdown","0a708ecb":"markdown","186864c4":"markdown","ff7b0181":"markdown","9df49315":"markdown","932817f1":"markdown","98abd7fe":"markdown","7eb479d6":"markdown","0e3774ea":"markdown"},"source":{"a943b75b":"import tensorflow as tf\nfrom tensorflow import keras\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\n\n# List of sentences to tokenize\nsentences = [\n    'I love my mom',\n    'I love my dad',\n    'You love me!',\n    'Do you think my dad is cool?'\n]\n\n# intializing a tokenizer that can index\n# num_words is the maximum number words that can be kept \n# tokenizer will automatically help in choosing most frequent words\ntokenizer = Tokenizer(num_words = 100)\n# fitting the sentences to using created tokenizer object\ntokenizer.fit_on_texts(sentences)\n# the full list of words is available as the tokenizer's word index\nword_index = tokenizer.word_index\n\n# Print the indexed dictionary \nprint(\"\\nWord Index = \", word_index)\n","908505bc":"# Create sequences of tokens that represent each sentence\nsequences = tokenizer.texts_to_sequences(sentences)\nprint(\"\\nWord Index = \", word_index)\nprint(\"\\nSequences = \", sequences)","4de68e30":"# New sentences with words that the tokenizer wasn't fit to\nnew_sentences = [\n    'I love my girlfriend',\n    'My dog is cool'\n]\n\n# Create sequences of tokens\nnew_seq = tokenizer.texts_to_sequences(new_sentences)\nprint(\"\\nNew Sequence = \", new_seq)","bdf61ff5":"# Apply tokenizer\ntokenizer = Tokenizer(num_words = 100, oov_token = '<OOV>')\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\n\n# New sentences with words that the tokenizer wasn't fit to\nnew_sentences = [\n    'I love my girlfriend',\n    'My dog is cool'\n]\n\n# Create sequences of tokens representing each sentence\nnew_seq = tokenizer.texts_to_sequences(new_sentences)\nprint(\"\\nWord Index = \", word_index)\nprint(\"\\nNew Sequence = \", new_seq)","692484db":"# Import pad_sequence method\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Pass sequences to pad_sequence method \n# padded = pad_sequences(sequences, truncating = 'post', maxlen = 5)\npadded = pad_sequences(sequences)\n\n# Print the original and padded sequences\nprint(\"\\nSequences = \", sequences)\nprint(\"\\nPadded Sequences = \", padded)","6145900e":"# Import json library to handle json files\nimport json\n\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# Declare variables\nvocab_size = 10000\nembedding_dim = 16\nmax_length = 100\ntrunc_type = 'post'\npadding_type = 'post'\noov_tok = \"<OOV>\"\ntraining_size = 20000","efbe4a20":"# Open the dataset\ndf = pd.read_json(\"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines=True)\ndf.head(5)","98c87e80":"# Create subsets for sentences and labels\nsentences = df.headline\nlabels = df.is_sarcastic\n# Create training subsets\ntraining_sentences = sentences[0:training_size]\ntraining_labels = labels[0:training_size]\n# Create testing subsets\ntesting_sentences  = sentences[training_size:]\ntesting_labels = labels[training_size:]","a6ecbe38":"# Initialize tokenizer\ntokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n# Fitting the training sentences\ntokenizer.fit_on_texts(training_sentences)\n# Create a words dictionary\nword_index = tokenizer.word_index\n","3dd050aa":"# Create sequences of tokens that represent each sentence \ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)","8ceeb0f9":"# Create padding sequences for training sentences \ntraining_padded = pad_sequences(training_sequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)\n# Create padding sequences for testing sentences \ntesting_padded = pad_sequences(testing_sequences, maxlen = max_length, padding = padding_type, truncating = trunc_type)","e723b226":"# Convert the sets to array to get it to work with TensorFlow 2\ntraining_padded = np.array(training_padded)\ntraining_labels = np.array(training_labels)\n\ntesting_padded = np.array(testing_padded)\ntesting_labels = np.array(testing_labels)","60f3ba59":"# Reference https:\/\/www.kaggle.com\/galvaowesley\/nlp-tensorflow-predicting-sarcastic-sentences\n# Create a model\nmodel = tf.keras.Sequential([\n    # Embedding layer for NN                         \n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),    \n    # Global Average pooling is similar to adding up vectors\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(64, activation = 'relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(1, activation = 'sigmoid')                             \n])\n# Modelo compile for binary classification\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])","f6fb9a5a":"# Model summary\nfrom tensorflow.keras.utils import plot_model\nplot_model(model, to_file='model_plot4a.png', show_shapes=True, show_layer_names=True)","7c394655":"# Train the model\nepoch_num = 3\nhistory = model.fit(training_padded, \n                    training_labels, \n                    epochs = epoch_num, \n                    validation_data = (testing_padded, testing_labels), verbose = 2)","b1c5f974":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","f161ad91":"# New sentences\nnew_sentences = [\n    \"Well, what a surprise, bitch\", # sarcastic\n    \"Have a nice day, my dear\", # not sarcastic\n    \"Nice perfume. How long did you marinate in it?\", # sarcastic\n    \"granny starting to fear spider in the garden might be real\", # sarcastic\n    \"game of thrones season finale showing this sunday night\" # not sarcastic\n]\n# Sequencing\nsequences = tokenizer.texts_to_sequences(new_sentences)\n# Padding\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n# Predicting\nprint(np.round(model.predict(padded),3))\n","84c6b3a8":"### Sequencing","1902b686":"### Embedding","63c30f09":"# Preprocessing techniques","d4251a3c":"## Tokenization\n\nThe tokenization is a technique to represent the words in a way that a computer can process them. Thus, a tokenized sentence, or list of sentences, can be train a Neural Network later. ","31a72850":"## Training\n","376fefa5":"Note that the sentence `I love my girlfriend` has 4 words, and the respective sequence` [3,1,2] `has only 3 numbers. This is because the tokenizer does not recognize the new word `girlfriend`. The same is true for the second sentence.\n\nTo work around this problem, we will use the parameter `oov_token = '<OOV>'` in the function `Tokenizer ()`. This generates an ** Out of Vocabulary ** token when an unrecognized token occurs in the vocabulary.","b35473d0":"\n\n---\n\n# Training a model to recognize sentiment in text\n\n\n","b4a3520b":"In this section we'll train a model to recognize if a sentence is sarcastic or not, based on the [dataset](https:\/\/www.kaggle.com\/rmisra\/news-headlines-dataset-for-sarcasm-detection\/home) of [Rishabh Misra](https:\/\/rishabhmisra.github.io\/).\n\nThe dataset is structured as json. The content has the following organization:\n\n**Content**\n\nEach record consists of three attributes:\n\n- `is_sarcastic`: 1 if the record is sarcastic otherwise 0\n\n- `headline`: the headline of the news article\n\n- `article_link`: link to the original news article. Useful in collecting supplementary data\n","4a80a162":"## Spliting dataset\n\nLet's split dataset in two subsets. One to train the model and other to test the trained model. The train subset size is 20000.  ","d56a65ec":"## Sequecing\n\nIn this step we'll sequence the setences. Once the words are properly indexed and tokenized, the setences can be represented by a sequence of numbers ordered by their respective words. In other words, we'll create sequences of tokens that represent each sentence.","13866265":"<a href=\"https:\/\/colab.research.google.com\/github\/galvaowesley\/MachineLearning_Learning\/blob\/master\/DeepLearning\/TensorFlow\/Intro_to_NLP_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","2636f692":"### Padding","e2c89df8":"## Padding\n\nSo far, we have seen that the strings have different lengths. But what if we need to train a neural network, how to handle with sequences of different lengths?\n\nFor this, we will use the padding technique, that pads the sequence with zeros, if necessary. \n\nFirst, import `pad_sequence` method. \n\n\n","0a708ecb":"Note that each number of sequence corresponds to a token. For example, the sequence `[4, 2, 3, 7]` is related to the sentence `I love my mom`, because:\n- `i`: 4\n- `love` : 2\n- `my` : 3\n- `mom` : 7 ","186864c4":"## Importing and loading dataset","ff7b0181":"## Preprocessing\n\nIn this section we will run the preprocessing techniques already seen. ","9df49315":"## Predicting sentences","932817f1":"### Evaluation","98abd7fe":"Natural Language Processing with TensorFlow\n===\n\n**WORKING IN PROGRESS!**\n\nWesley Galv\u00e3o \n\nAug, 2020\n\nThis notebook is based on the videos serie of [TensorFlow channel](https:\/\/www.youtube.com\/playlist?list=PLQY2H8rRoyvzDbLUZkbudP-MFQZwNmU4S)","7eb479d6":"### Tokenization","0e3774ea":"## Unseen words\n\nLet's try to sequence a new list of sentences where there are words that the already fitted tokenizer is not familiarized with. What can happen?\n"}}