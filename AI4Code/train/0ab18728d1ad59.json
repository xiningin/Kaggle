{"cell_type":{"2497778b":"code","69dad33c":"code","bcc03e96":"code","13101453":"code","ce2ce952":"code","c0a99659":"code","8d06b4d3":"code","1e9bfb64":"code","1790b859":"code","04e45187":"code","2a5bfbac":"code","661f3a5f":"code","34b62853":"code","9d1e2969":"code","be867ea0":"markdown","a4cd2281":"markdown","de9fb22d":"markdown","aec2465f":"markdown","f7fa1b08":"markdown","da287505":"markdown","548d579e":"markdown","42e0bb14":"markdown","1ae971ff":"markdown","b8cbd608":"markdown","5dbeb962":"markdown","76b6f676":"markdown"},"source":{"2497778b":"import time\nimport copy\nimport torch\nimport pandas as pd\nimport numpy as np\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Subset","69dad33c":"if torch.cuda.is_available():\n    !nvidia-smi\n\ndev = torch.device(\"cuda\")","bcc03e96":"class DigitDataset(Dataset):\n    # must define 3 func\n    def __init__(self, path, transform=None):\n        self.df = pd.read_csv(path)\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx): \n        image = self.df.iloc[idx, 1:].values.reshape(28, 28).astype(np.uint8)\n        label = self.df.iloc[idx, 0].astype(np.uint8)\n        \n        image = self.transform(image)\n        return image, label","13101453":"transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5,), std=(0.5,)) \n    ])\n\ndigit_dataset = DigitDataset(\"..\/input\/digit-recognizer\/train.csv\", transform=transform)","ce2ce952":"random_seed = 33\ndataset, dataset_size = {}, {}\n\ntrain_idx, val_idx = train_test_split(list(range(len(digit_dataset))), test_size=0.2, random_state=random_seed)\ndataset['train'] = Subset(digit_dataset, train_idx)\ndataset['valid'] = Subset(digit_dataset, val_idx)\n\ndataset_size['train'] = len(dataset['train'])\ndataset_size['valid'] = len(dataset['valid'])\n\nprint('train_data size:', dataset_size['train'])\nprint('valid_data_size:', dataset_size['valid'])","c0a99659":"dataloaders, batch_num = {}, {}\nbatch_size = 200\n\ndataloaders['train'] = DataLoader(dataset['train'],\n                                  batch_size=batch_size, shuffle=True,\n                                  num_workers=8)\ndataloaders['valid'] = DataLoader(dataset['valid'],\n                                  batch_size=batch_size, shuffle=True,\n                                  num_workers=8)\n\nbatch_num['train'], batch_num['valid'] = len(dataloaders['train']), len(dataloaders['valid'])\nprint('batch_size : {},  train\/valid : {} \/ {}' \n      .format(batch_size, batch_num['train'], batch_num['valid']))","8d06b4d3":"import matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n\ndef imshow(data):\n    # data is normalized, need to restore\n    img = (data + 1) \/ 2\n    fig, axes = plt.subplots(1, 8, figsize=(8, 1))\n    \n    # draw 8 figures\n    for i, ax in enumerate(axes.flat):\n        ax.imshow(np.array(img)[i].squeeze(), cmap='gray')\n        ax.set_xticks([])\n        ax.set_yticks([])\n    plt.show()\n\nprint('real digit data')\nfor data, _ in dataloaders['train']:\n    imshow(data[0:16, :, :])\n    break","1e9bfb64":"# Generator G\nclass Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.layers = nn.Sequential(\n              nn.Linear(100, 256),\n              nn.ReLU(),\n              nn.Dropout(0.1),\n              nn.Linear(256, 256),\n              nn.ReLU(),\n              nn.Dropout(0.1),\n              nn.Linear(256, 28*28),\n              nn.Tanh())\n\n    def forward(self, inputs):\n        return self.layers(inputs)\n    \n# Discriminator D\nclass Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(28*28, 256),\n            nn.LeakyReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, 256),\n            nn.LeakyReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, 1),\n            nn.Sigmoid())\n\n    def forward(self, inputs):\n        inputs = inputs.view(-1, 28*28)\n        return self.layers(inputs)","1790b859":"# train stage\ndef training(D, G, optimizer_D, optimizer_G):\n    D.train()\n    G.train()\n\n    average_loss_D = 0.0\n    average_loss_G = 0.0\n\n    for data_real, _ in dataloaders['train']:\n        data_real = data_real.to(dev)\n        \n        # ground truth of real data\n        target_real = torch.ones(batch_size, 1).to(dev)\n        \n        # ground truth of fake(generated) data\n        target_fake = torch.zeros(batch_size, 1).to(dev)\n        \n        # random noise\n        z = torch.randn((batch_size, 100)).to(dev)\n\n        # training Discriminator\n        optimizer_D.zero_grad()\n        \n        # make prediction about real\/generated data\n        predict_real = D(data_real)\n        predict_fake = D(G(z))\n        \n        # calculate loss and update D\n        loss_D = criterion(predict_real, target_real) \\\n               + criterion(predict_fake, target_fake)\n        loss_D.backward()\n        optimizer_D.step()\n\n        # training Generator\n        optimizer_G.zero_grad()\n        \n        # generate fake data to fool D\n        fake = G(z)\n        predict_fake = D(fake)\n        \n        # calculate loss and update G\n        loss_G = criterion(predict_fake, target_real)\n        loss_G.backward()\n        optimizer_G.step()\n\n        average_loss_D += loss_D.item()\n        average_loss_G += loss_G.item()\n\n    return average_loss_D \/ batch_num['train'], average_loss_G \/ batch_num['train']","04e45187":"# evaluation stage\ndef evaluation(D, G, epoch):\n    D.eval()\n    G.eval()\n\n    prob_r2r = 0.0\n    prob_f2r = 0.0\n\n    for data_real, _ in dataloaders['valid']:\n        data_real = data_real.to(dev)\n        \n        # probility of predicting real data to real(not fooled rate)\n        prob_r2r += torch.sum(D(data_real)).item()\n        \n        # generate fake data\n        fake = G(torch.randn((batch_size, 100)).to(dev))\n        # probility of predicting fake data to real(fooled rate)\n        prob_f2r += torch.sum(D(fake)).item()\n\n    # visualize generated data every 50 epochs \n    if epoch % 50 == 0:\n        imshow(fake[0:8].view(-1, 28, 28).cpu())\n\n    return prob_r2r \/ dataset_size['valid'], prob_f2r \/ dataset_size['valid']","2a5bfbac":"G = Generator().to(dev)\nD = Discriminator().to(dev)\n\ncriterion = nn.BCELoss()\n\noptimizer_G = Adam(G.parameters(), lr=0.0003)\noptimizer_D = Adam(D.parameters(), lr=0.0003)","661f3a5f":"prob_real_to_real = []\nprob_fake_to_real = []\n\nbest_f2r = 0.0\nbest_generator = None\n\nsince = time.time()\n\nfor epoch in range(300):\n    # train stage\n    loss_D, loss_G = training(D, G, optimizer_D, optimizer_G)\n    \n    # evaluation stage\n    with torch.autograd.no_grad():\n        r2r, f2r = evaluation(D, G, epoch)\n        prob_real_to_real.append(r2r)\n        prob_fake_to_real.append(f2r)\n        \n        # best GAN model update\n        if (epoch > 150) and (f2r > best_f2r):\n            best_generator = copy.deepcopy(G.state_dict())\n            best_f2r = f2r\n            print('[epoch {}]: Best Generator saved! | {:.0f}m elapsed'\n                  .format(epoch, (time.time() - since) \/\/ 60))\n\n    # print status\n    if epoch % 10 == 0:  \n        print('[epoch {}] Discriminator loss: {:.4f} | Generator loss: {:.4f}'\n          .format(epoch + 1, loss_D, loss_G))\n        print('predict real to real: {:.2f}% | predict fake to real: {:.2f}%'\n              .format(r2r * 100, f2r * 100))        ","34b62853":"best_G = Generator().to(dev)\nbest_G.load_state_dict(best_generator)\nbest_fake = best_G(torch.randn((batch_size, 100)).to(dev))\n\nwith torch.autograd.no_grad():\n    for i in range(0, 63, 8):\n        imshow(best_fake[i:i+8].view(-1, 28, 28).cpu())\n        \nprint('Discriminator {:.1f}% predicts generated(fake) data as real'.format(best_f2r * 100))","9d1e2969":"plt.plot(prob_real_to_real, label='predict real data as real')\nplt.plot(prob_fake_to_real, label='predict fake data as real')\nplt.legend()","be867ea0":"[I. Introduction](.\/#Introduction)\n\n[II. Implementation](.\/#Implementation)\n\n[III. Learning](.\/#Learning)\n\n[IV. Conclusion](.\/#Conclusion)","a4cd2281":"## Data preparation\n\nCreate custom DigitDataset class to read .csv file\n\n- csv file -> dataset -> dataloader","de9fb22d":"# Introduction\n\n## How GANs work\n\nGAN has two models:\n\n- **Generator**(G): Generate plausible output example, given random noise as input.\n- **Discriminator**(D): Discern the generator's plausible example from real example.\n\nTraining objective:\n\n- **Generator**: Minimize the probability of D predicting generated image as fake.\n- **Discriminator**: Maximize the probability of D correctly predicting each image as \u201creal\u201d or \u201cfake\u201d.","aec2465f":"### Learning","f7fa1b08":"### References\n\n- [Paper]()\n- [Google Developers Overview]()\n\n\n# Code","da287505":"## Meaning of GAN\n\n**G**enerative **A**dversarial **N**etworks\n\n**Generative**: They create new data instances that resemble training data.\n\n**Adversarial**: The generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator.\n\n**Network**: Both the generator and the discriminator are neural networks.\n","548d579e":"Running on CUDA","42e0bb14":"### Result\n\n- fake data from trained generator","1ae971ff":"## Conclusion\n\n- GAN is based on the minmax\/zero-sum game. So if one wins the other loses.\n- A stable GAN will reach Nash equlibrium and have around 50% probability to predict correctly","b8cbd608":"### Train\/Evaluation stage","5dbeb962":"## Implementation\n\n[What is Generator?](\/#Introduction)\n\n1. Random data is selected for use as input to the **G** and **G** produces fake data.\n2. **D** make predictions for the real and fake data.\n3. **D** is updated from how correct or incorrect those predictions were.\n\n\n4. Once again make random noise and and **G** produces fake data.\n5. **D** make predictions for the fake data.\n6. **G** is updated from how incorrect those predictions were.","76b6f676":"![](https:\/\/cdn-media-1.freecodecamp.org\/images\/m41LtQVUf3uk5IOYlHLpPazxI3pWDwG8VEvU)\n\nby [Thalles Silva](https:\/\/www.freecodecamp.org\/news\/an-intuitive-introduction-to-generative-adversarial-networks-gans-7a2264a81394\/)"}}