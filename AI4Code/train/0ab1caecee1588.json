{"cell_type":{"a0d07a63":"code","b7fe86ac":"code","491c798e":"code","e90777fd":"code","b3e2c5f4":"code","5201e3d1":"code","80e4906d":"code","fad91f3a":"code","cdd472c4":"code","4e92e027":"code","ee8d289e":"code","3802bca2":"code","713972ac":"code","343c8050":"code","a4db3631":"code","9138624d":"code","56eb08b3":"code","fbdadc3b":"code","dd820e58":"code","9130a7f9":"code","4e443891":"code","fa5121a4":"code","178957fe":"code","138429b0":"code","79572e55":"code","a5a74c26":"code","538cc246":"code","78ee805b":"code","37a49c6b":"code","f7d43659":"code","10991bd4":"code","ec007069":"code","13978433":"code","3923ab5c":"code","ea08e0d2":"code","ef702dd0":"code","716b3c24":"code","cad7157d":"code","fec84292":"code","a083dc6c":"code","517193e1":"code","387282c7":"code","6efb4cf3":"code","87bbb30e":"code","f3b8ddf9":"code","0a37bd83":"code","10a8977a":"code","9ed0e1de":"code","c381d8b7":"code","ddb78a46":"code","716a3fa9":"code","180cdacc":"code","d5159d83":"code","a2ff6e75":"code","93f4a0a6":"code","0fbe2ad9":"code","cdb393ad":"code","ea7ac0d8":"code","4656b8d9":"code","622d9405":"code","5d343afd":"code","44ae308e":"code","97e819a9":"code","dcd9b0a7":"code","d42b0a3f":"code","a6f88c39":"code","01920743":"code","2c07f373":"code","4bc21e2e":"code","155b1af1":"code","50eba7ee":"code","ad89c529":"code","760edc3d":"code","4aadee0f":"code","d4a2a648":"code","d4c4ed41":"code","7154091b":"code","075627c3":"code","65f11856":"code","3b001ad9":"code","c086d470":"code","c3f4bf30":"code","a1588f73":"code","f4c1e688":"code","badefc6b":"code","3241ce0a":"code","b63da14e":"code","0e2776c2":"code","28fb92a3":"code","db6f4727":"code","ae2b0801":"code","a00f64f3":"code","28838d35":"code","634535b5":"code","7ffc6416":"code","075aa858":"code","1e2c50df":"code","21552a36":"code","1b8fdab7":"code","e71dfe01":"code","9aae5ef1":"code","bd80de6b":"code","82e86265":"code","d1b8f40e":"code","4aa9bdff":"code","0f5dc916":"code","38ec2010":"code","2a85517e":"code","b24ba88d":"code","39b9174a":"code","df5c0196":"code","916e09f2":"code","65a3c2d5":"code","03ee9780":"code","266dbe05":"code","34bb9dc1":"code","d3a6a006":"code","ed121fdf":"code","fe608938":"code","0b4e856b":"code","b841e69a":"markdown","0d4ac97f":"markdown","7c73ecfe":"markdown","927e870f":"markdown","6236c16b":"markdown","6bb86512":"markdown","bad6656c":"markdown","8a620567":"markdown","684fd325":"markdown","f45d3fdb":"markdown","3f2905cf":"markdown","9be29324":"markdown","e0395dff":"markdown","0c44dbfd":"markdown","1b43307d":"markdown","77628cf9":"markdown","4a9ec734":"markdown","1b05d659":"markdown","d3eaf49b":"markdown","153aa640":"markdown","067c87c4":"markdown","d1b83f02":"markdown","ac18524c":"markdown","3490849f":"markdown","bcaaec40":"markdown","422e4117":"markdown","f1ef9d49":"markdown","4fc528d7":"markdown","381db30a":"markdown","f0497095":"markdown","d2dbea30":"markdown","ac0019d4":"markdown","98400115":"markdown","abd105d4":"markdown","ebf60166":"markdown","03ec2909":"markdown","99aa90ec":"markdown","62bacec4":"markdown","21c3242e":"markdown","856e9ffb":"markdown","7aa4bf54":"markdown","4a94ef71":"markdown","d611be2f":"markdown","c6402807":"markdown","15842db0":"markdown","f904b845":"markdown","8c801fb8":"markdown","a618b852":"markdown","1b7ce4f6":"markdown","4ef50ec8":"markdown","27016fa2":"markdown","2122178b":"markdown","248d9a36":"markdown","42a90bc2":"markdown","3001d8f7":"markdown","5763d870":"markdown","4f897cf0":"markdown","09b3671d":"markdown","6f5d76a3":"markdown","5dd3abf7":"markdown","5ba83ddd":"markdown","bb643242":"markdown","0cdaf88a":"markdown","0c7f756c":"markdown","a99ce8fd":"markdown","105863d1":"markdown","dba655fe":"markdown","461bcb20":"markdown","782a81ef":"markdown","b599feed":"markdown","3a38f0af":"markdown","b630b974":"markdown","972bf1ed":"markdown","db2afc81":"markdown","30e75027":"markdown","0af43f73":"markdown","5b894f54":"markdown","d73b9e81":"markdown","4a81b44e":"markdown","5fe0f587":"markdown","900dfbdc":"markdown","d091b1ff":"markdown","b72467be":"markdown","e7278536":"markdown","756f17b8":"markdown","115ad774":"markdown","34fda0e8":"markdown","eee24d13":"markdown","4a20da85":"markdown","b79a96c5":"markdown","b5d5fa03":"markdown","94311cfc":"markdown"},"source":{"a0d07a63":"import numpy as np\nimport pandas as pd\nimport missingno as msno\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom pandas.core.common import SettingWithCopyWarning\nfrom pandas_profiling import ProfileReport\nfrom pathlib import Path\nfrom scipy.stats import probplot, chi2_contingency, chi2\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_val_predict\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, OrdinalEncoder\nfrom sklearn.metrics import ConfusionMatrixDisplay, classification_report, roc_curve, roc_auc_score\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier, ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.inspection import permutation_importance\nimport scikitplot as skplt\nfrom yellowbrick.model_selection import FeatureImportances\nimport scipy.stats as stats\nimport joblib\nimport os\n%matplotlib inline\n","b7fe86ac":"cc_data_full_data = pd.read_csv('..\/input\/credit-card-approval-prediction\/application_record.csv')\ncredit_status = pd.read_csv('..\/input\/credit-card-approval-prediction\/credit_record.csv')","491c798e":"begin_month=pd.DataFrame(credit_status.groupby(['ID'])['MONTHS_BALANCE'].agg(min))\nbegin_month=begin_month.rename(columns={'MONTHS_BALANCE':'Account age'})\ncc_data_full_data=pd.merge(cc_data_full_data,begin_month,how='left',on='ID')\ncredit_status['dep_value'] = None\ncredit_status['dep_value'][credit_status['STATUS'] =='2']='Yes'\ncredit_status['dep_value'][credit_status['STATUS'] =='3']='Yes'\ncredit_status['dep_value'][credit_status['STATUS'] =='4']='Yes'\ncredit_status['dep_value'][credit_status['STATUS'] =='5']='Yes'\ncpunt=credit_status.groupby('ID').count()\ncpunt['dep_value'][cpunt['dep_value'] > 0]='Yes'\ncpunt['dep_value'][cpunt['dep_value'] == 0]='No'\ncpunt = cpunt[['dep_value']]\ncc_data_full_data = pd.merge(cc_data_full_data,cpunt,how='inner',on='ID')\ncc_data_full_data['Is high risk']=cc_data_full_data['dep_value']\ncc_data_full_data.loc[cc_data_full_data['Is high risk']=='Yes','Is high risk']=1\ncc_data_full_data.loc[cc_data_full_data['Is high risk']=='No','Is high risk']=0\ncc_data_full_data.drop('dep_value',axis=1,inplace=True)\nwarnings.simplefilter(action='always', category=SettingWithCopyWarning)","e90777fd":"# rename the features to a more readable feature names\ncc_data_full_data = cc_data_full_data.rename(columns={\n    'CODE_GENDER':'Gender',\n    'FLAG_OWN_CAR':'Has a car',\n    'FLAG_OWN_REALTY':'Has a property',\n    'CNT_CHILDREN':'Children count',\n    'AMT_INCOME_TOTAL':'Income',\n    'NAME_INCOME_TYPE':'Employment status',\n    'NAME_EDUCATION_TYPE':'Education level',\n    'NAME_FAMILY_STATUS':'Marital status',\n    'NAME_HOUSING_TYPE':'Dwelling',\n    'DAYS_BIRTH':'Age',\n    'DAYS_EMPLOYED': 'Employment length',\n    'FLAG_MOBIL': 'Has a mobile phone',\n    'FLAG_WORK_PHONE': 'Has a work phone',\n    'FLAG_PHONE': 'Has a phone',\n    'FLAG_EMAIL': 'Has an email',\n    'OCCUPATION_TYPE': 'Job title',\n    'CNT_FAM_MEMBERS': 'Family member count',\n    'Account age': 'Account age'\n    })","b3e2c5f4":"# split the data into train and test\ndef data_split(df, test_size):\n    train_df, test_df = train_test_split(df, test_size=test_size, random_state=42)\n    return train_df.reset_index(drop=True), test_df.reset_index(drop=True)","5201e3d1":"cc_train_original, cc_test_original = data_split(cc_data_full_data, 0.2)","80e4906d":"cc_train_original.shape","fad91f3a":"cc_test_original.shape","cdd472c4":"# creating a copy of the dataset so that the original stays untouched\ncc_train_copy = cc_train_original.copy()\ncc_test_copy = cc_test_original.copy()","4e92e027":"cc_data_full_data.head()","ee8d289e":"cc_data_full_data.info()","3802bca2":"cc_data_full_data.describe()","713972ac":"msno.matrix(cc_data_full_data)\nplt.show()","343c8050":"msno.bar(cc_data_full_data)\nplt.show()","a4db3631":"#Function that will return the value count and frequency of each observation within a feature\ndef value_cnt_norm_cal(df,feature):\n    ftr_value_cnt = df[feature].value_counts()\n    ftr_value_cnt_norm = df[feature].value_counts(normalize=True) * 100\n    ftr_value_cnt_concat = pd.concat([ftr_value_cnt, ftr_value_cnt_norm], axis=1)\n    ftr_value_cnt_concat.columns = ['Count', 'Frequency (%)']\n    return ftr_value_cnt_concat","9138624d":"# function to create display general information about the feature\ndef gen_info_feat(df,feature):\n    if feature == 'Age':\n        # change the feature to be express in positive numbers days\n        print('Description:\\n{}'.format((np.abs(df[feature])\/365.25).describe()))\n        print('*'*50)\n        print('Object type:{}'.format(df[feature].dtype))\n    elif feature == 'Employment length':\n        # select only the rows where the rows are negative to ignore whose who have retired or unemployed\n        employment_len_no_ret = cc_train_copy['Employment length'][cc_train_copy['Employment length'] < 0]\n        employment_len_no_ret_yrs = np.abs(employment_len_no_ret)\/365.25\n        print('Description:\\n{}'.format((employment_len_no_ret_yrs).describe()))\n        print('*'*50)\n        print('Object type:{}'.format(employment_len_no_ret.dtype))\n    elif feature ==  'Account age':\n        # change the account age to a positive number of months\n        print('Description:\\n{}'.format((np.abs(df[feature])).describe()))\n        print('*'*50)\n        print('Object type:{}'.format(df[feature].dtype))\n    else:\n        print('Description:\\n{}'.format(df[feature].describe()))\n        print('*'*50)\n        print('Object type:\\n{}'.format(df[feature].dtype))\n        print('*'*50)\n        value_cnt = value_cnt_norm_cal(df,feature)\n        print('Value count:\\n{}'.format(value_cnt))\n","56eb08b3":"# function to create a pie chart plot\ndef create_pie_plot(df,feature):\n    if feature ==  'Dwelling' or 'Education level':\n        ratio_size = value_cnt_norm_cal(df, feature)\n        ratio_size_len = len(ratio_size.index)\n        ratio_list = []\n        for i in range(ratio_size_len):\n            ratio_list.append(ratio_size.iloc[i]['Frequency (%)'])\n        fig, ax = plt.subplots(figsize=(10,10))\n        # %1.2f%% display decimals in the pie chart with 2 decimal places\n        plt.pie(ratio_list, startangle=90, wedgeprops={'edgecolor' :'black'})\n        plt.title('Pie chart of {}'.format(feature))\n        plt.legend(loc='best',labels=ratio_size.index)\n        plt.axis('equal')\n        return plt.show()\n    else :\n        ratio_size = value_cnt_norm_cal(df, feature)\n        ratio_size_len = len(ratio_size.index)\n        ratio_list = []\n        for i in range(ratio_size_len):\n            ratio_list.append(ratio_size.iloc[i]['Frequency (%)'])\n        fig, ax = plt.subplots(figsize=(10,10))\n        # %1.2f%% display decimals in the pie chart with 2 decimal places\n        plt.pie(ratio_list, labels=ratio_size.index, autopct='%1.2f%%', startangle=90, wedgeprops={'edgecolor' :'black'})\n        plt.title('Pie chart of {}'.format(feature))\n        plt.legend(loc='best')\n        plt.axis('equal')\n        return plt.show()","fbdadc3b":"# function to create a bar chart plot\ndef create_bar_plot(df,feature):\n    if feature == 'Marital status' or 'Dwelling' or 'Job title' or 'Employment status' or 'Education level':\n        fig, ax = plt.subplots(figsize=(6,10))\n        sns.barplot(x=value_cnt_norm_cal(df,feature).index,y=value_cnt_norm_cal(df,feature).values[:,0])\n        ax.set_xticklabels(labels=value_cnt_norm_cal(df,feature).index,rotation=45,ha='right')\n        plt.xlabel('{}'.format(feature))\n        plt.ylabel('Count')\n        plt.title('{} count'.format(feature))\n        return plt.show()\n    else :\n        fig, ax = plt.subplots(figsize=(6,10))\n        sns.barplot(x=value_cnt_norm_cal(df,feature).index,y=value_cnt_norm_cal(df,feature).values[:,0])\n        plt.xlabel('{}'.format(feature))\n        plt.ylabel('Count')\n        plt.title('{} count'.format(feature))\n        return plt.show()","dd820e58":"# function to create a box plot\ndef create_box_plot(df,feature):\n    if feature == 'Age':\n        fig, ax = plt.subplots(figsize=(2,8))\n        # change the feature to be express in positive numbers days\n        sns.boxplot(y=np.abs(df[feature])\/365.25)\n        plt.title('{} distribution(Boxplot)'.format(feature))\n        return plt.show()\n    elif feature == 'Children count':\n        fig, ax = plt.subplots(figsize=(2,8))\n        sns.boxplot(y=df[feature])\n        plt.title('{} distribution(Boxplot)'.format(feature))\n        plt.yticks(np.arange(0,df[feature].max(),1))\n        return plt.show()\n    elif feature ==  'Employment length':\n        fig, ax = plt.subplots(figsize=(2,8))\n        employment_len_no_ret = cc_train_copy['Employment length'][cc_train_copy['Employment length'] < 0]\n        # employement length in days is a negative number so we need to change it to positive and change it to days\n        employment_len_no_ret_yrs = np.abs(employment_len_no_ret)\/365.25\n        sns.boxplot(y=employment_len_no_ret_yrs)\n        plt.title('{} distribution(Boxplot)'.format(feature))\n        plt.yticks(np.arange(0,employment_len_no_ret_yrs.max(),2))\n        return plt.show()\n    elif feature ==  'Income':\n        fig, ax = plt.subplots(figsize=(2,8))\n        sns.boxplot(y=df[feature])\n        plt.title('{} distribution(Boxplot)'.format(feature))\n        # suppress scientific notation\n        ax.get_yaxis().set_major_formatter(\n            matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n        return plt.show()\n    elif feature ==  'Account age':\n        fig, ax = plt.subplots(figsize=(2,8))\n        sns.boxplot(y=np.abs(df[feature]))\n        plt.title('{} distribution(Boxplot)'.format(feature))\n        return plt.show()\n    else :\n        fig, ax = plt.subplots(figsize=(2,8))\n        sns.boxplot(y=df[feature])\n        plt.title('{} distribution(Boxplot)'.format(feature))\n        return plt.show()","9130a7f9":"# function to create a histogram plot\ndef create_hist_plot(df,feature, the_bins=50):\n    if feature == 'Age':\n        fig, ax = plt.subplots(figsize=(18,10))\n        # change the feature to be express in positive numbers days\n        sns.histplot(np.abs(df[feature])\/365.25,bins=the_bins,kde=True)\n        plt.title('{} distribution'.format(feature))\n        return plt.show()\n    elif feature == 'Income':\n        fig, ax = plt.subplots(figsize=(18,10))\n        sns.histplot(df[feature],bins=the_bins,kde=True)\n        # suppress scientific notation\n        ax.get_xaxis().set_major_formatter(\n            matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n        plt.title('{} distribution'.format(feature))\n        return plt.show()\n    elif feature ==  'Employment length':\n        employment_len_no_ret = cc_train_copy['Employment length'][cc_train_copy['Employment length'] < 0]\n        # change the feature to be express in positive numbers days\n        employment_len_no_ret_yrs = np.abs(employment_len_no_ret)\/365.25\n        fig, ax = plt.subplots(figsize=(18,10))\n        sns.histplot(employment_len_no_ret_yrs,bins=the_bins,kde=True)\n        plt.title('{} distribution'.format(feature))\n        return plt.show()\n    elif feature ==  'Account age':\n        fig, ax = plt.subplots(figsize=(18,10))\n        sns.histplot(np.abs(df[feature]),bins=the_bins,kde=True)\n        plt.title('{} distribution'.format(feature))\n        return plt.show()\n    else :\n        fig, ax = plt.subplots(figsize=(18,10))\n        sns.histplot(df[feature],bins=the_bins,kde=True)\n        plt.title('{} distribution'.format(feature))\n        return plt.show()","4e443891":"# High risk vs low risk applicants compared on a box plot\ndef low_high_risk_box_plot(df,feature):\n    if feature == 'Age':\n        print(np.abs(df.groupby('Is high risk')[feature].mean()\/365.25))\n        fig, ax = plt.subplots(figsize=(5,8))\n        sns.boxplot(y=np.abs(df[feature])\/365.25,x=df['Is high risk'])\n        plt.xticks(ticks=[0,1],labels=['no','yes'])\n        plt.title('High risk individuals grouped by age')\n        return plt.show()\n    elif feature == 'Income':\n        print(np.abs(df.groupby('Is high risk')[feature].mean()))\n        fig, ax = plt.subplots(figsize=(5,8))\n        sns.boxplot(y=np.abs(df[feature]),x=df['Is high risk'])\n        plt.xticks(ticks=[0,1],labels=['no','yes'])\n        # suppress scientific notation\n        ax.get_yaxis().set_major_formatter(\n            matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n        plt.title('High risk individuals grouped by {}'.format(feature))\n        return plt.show()\n    elif feature == 'Employment length':\n        #checking is an applicant is high risk or not (for those who have negative employment length aka the employed ones)\n        employment_no_ret = cc_train_copy['Employment length'][cc_train_copy['Employment length'] <0]\n        employment_no_ret_idx = employment_no_ret.index\n        employment_len_no_ret_yrs = np.abs(employment_no_ret)\/365.25\n        employment_no_ret_df = cc_train_copy.iloc[employment_no_ret_idx][['Employment length','Is high risk']]\n        employment_no_ret_is_high_risk = employment_no_ret_df.groupby('Is high risk')['Employment length'].mean()\n        # compare the age of high risk individuals with the age of low risk individuals (those who are employed)\n        print(np.abs(employment_no_ret_is_high_risk)\/365.25)\n        fig, ax = plt.subplots(figsize=(5,8))\n        sns.boxplot(y=employment_len_no_ret_yrs,x=df['Is high risk'])\n        plt.xticks(ticks=[0,1],labels=['no','yes'])\n        plt.title('High vs low risk individuals grouped by {}'.format(feature))\n        return plt.show()\n    else :\n        print(np.abs(df.groupby('Is high risk')[feature].mean()))\n        fig, ax = plt.subplots(figsize=(5,8))\n        sns.boxplot(y=np.abs(df[feature]),x=df['Is high risk'])\n        plt.xticks(ticks=[0,1],labels=['no','yes'])\n        plt.title('High risk individuals grouped by {}'.format(feature))\n        return plt.show()","fa5121a4":"# High risk vs low risk applicants compared on a bar plot\ndef low_high_risk_bar_plot(df,feature):\n    is_high_risk_grp = df.groupby(feature)['Is high risk'].sum()\n    is_high_risk_grp_srt = is_high_risk_grp.sort_values(ascending=False)\n    print(dict(is_high_risk_grp_srt))\n    fig, ax = plt.subplots(figsize=(6,10))\n    sns.barplot(x=is_high_risk_grp_srt.index,y=is_high_risk_grp_srt.values)\n    ax.set_xticklabels(labels=is_high_risk_grp_srt.index,rotation=45, ha='right')\n    plt.ylabel('Count')\n    plt.title('High risk applicants count grouped by {}'.format(feature))\n    return plt.show()","178957fe":"cc_train_copy.shape","138429b0":"gen_info_feat(cc_train_copy,'Gender')","79572e55":"create_bar_plot(cc_train_copy,'Gender')","a5a74c26":"create_pie_plot(cc_train_copy,'Gender')","538cc246":"gen_info_feat(cc_train_copy,'Age')","78ee805b":"create_box_plot(cc_train_copy,'Age')","37a49c6b":"create_hist_plot(cc_train_copy,'Age')","f7d43659":"low_high_risk_box_plot(cc_train_copy,'Age')","10991bd4":"gen_info_feat(cc_train_copy,'Marital status')","ec007069":"create_pie_plot(cc_train_copy,'Marital status')","13978433":"create_bar_plot(cc_train_copy,'Marital status')","3923ab5c":"low_high_risk_bar_plot(cc_train_copy,'Marital status')","ea08e0d2":"gen_info_feat(cc_train_copy,'Family member count')","ef702dd0":"create_box_plot(cc_train_copy,'Family member count')","716b3c24":"create_bar_plot(cc_train_copy,'Family member count')","cad7157d":"gen_info_feat(cc_train_copy,'Children count')","fec84292":"create_box_plot(cc_train_copy,'Children count')","a083dc6c":"create_bar_plot(cc_train_copy,'Children count')","517193e1":"gen_info_feat(cc_train_copy,'Dwelling')","387282c7":"create_pie_plot(cc_train_copy,'Dwelling')","6efb4cf3":"create_bar_plot(cc_train_copy,'Dwelling')","87bbb30e":"pd.set_option('display.float_format', lambda x: '%.2f' % x)\ngen_info_feat(cc_train_copy,'Income')","f3b8ddf9":"create_box_plot(cc_train_copy,'Income')","0a37bd83":"create_hist_plot(cc_train_copy,'Income')","10a8977a":"low_high_risk_box_plot(cc_train_copy,'Income')","9ed0e1de":"gen_info_feat(cc_train_copy,'Job title')","c381d8b7":"job_title_nan_count = cc_train_copy['Job title'].isna().sum()\njob_title_nan_count","ddb78a46":"rows_total_count = cc_train_copy.shape[0]","716a3fa9":"print('The percentage of missing rows is {:.2f} %'.format(job_title_nan_count * 100 \/ rows_total_count))","180cdacc":"create_bar_plot(cc_train_copy,'Job title')","d5159d83":"gen_info_feat(cc_train_copy,'Employment status')","a2ff6e75":"create_bar_plot(cc_train_copy,'Employment status')","93f4a0a6":"create_pie_plot(cc_train_copy,'Employment status')","0fbe2ad9":"gen_info_feat(cc_train_copy,'Education level')","cdb393ad":"create_pie_plot(cc_train_copy,'Education level')","ea7ac0d8":"create_bar_plot(cc_train_copy,'Education level')","4656b8d9":"gen_info_feat(cc_train_copy,'Employment length')","622d9405":"create_box_plot(cc_train_copy,'Employment length')","5d343afd":"create_hist_plot(cc_train_copy,'Employment length')","44ae308e":"# distribution of employment length for high vs low risk applicants\n# Here 0 means No and 1 means Yes\nlow_high_risk_box_plot(cc_train_copy,'Employment length')","97e819a9":"gen_info_feat(cc_train_copy,'Has a car')","dcd9b0a7":"create_bar_plot(cc_train_copy,'Has a car')","d42b0a3f":"create_pie_plot(cc_train_copy,'Has a car')","a6f88c39":"gen_info_feat(cc_train_copy,'Has a property')","01920743":"create_bar_plot(cc_train_copy,'Has a property')","2c07f373":"create_pie_plot(cc_train_copy,'Has a property')","4bc21e2e":"gen_info_feat(cc_train_copy,'Has a work phone')","155b1af1":"create_bar_plot(cc_train_copy,'Has a work phone')","50eba7ee":"create_pie_plot(cc_train_copy,'Has a work phone')","ad89c529":"gen_info_feat(cc_train_copy,'Has a mobile phone')","760edc3d":"create_pie_plot(cc_train_copy,'Has a mobile phone')","4aadee0f":"gen_info_feat(cc_train_copy,'Has a phone')","d4a2a648":"create_bar_plot(cc_train_copy,'Has a phone')","d4c4ed41":"create_pie_plot(cc_train_copy,'Has a phone')","7154091b":"gen_info_feat(cc_train_copy,'Has an email')","075627c3":"create_bar_plot(cc_train_copy,'Has an email')","65f11856":"create_pie_plot(cc_train_copy,'Has an email')","3b001ad9":"gen_info_feat(cc_train_copy,'Account age')","c086d470":"create_box_plot(cc_train_copy,'Account age')","c3f4bf30":"create_hist_plot(cc_train_copy,'Account age', the_bins=30)","a1588f73":"low_high_risk_box_plot(cc_train_copy,'Account age')","f4c1e688":"gen_info_feat(cc_train_copy,'Is high risk')","badefc6b":"create_bar_plot(cc_train_copy,'Is high risk')","3241ce0a":"create_pie_plot(cc_train_copy,'Is high risk')","b63da14e":"sns.pairplot(cc_train_copy[cc_train_copy['Employment length'] < 0].drop(['ID','Has a mobile phone', 'Has a work phone', 'Has a phone', 'Has an email','Is high risk'],axis=1),corner=True)\nplt.show()","0e2776c2":"sns.regplot(x='Children count',y='Family member count',data=cc_train_copy,line_kws={'color': 'red'})\nplt.show()","28fb92a3":"y_age = np.abs(cc_train_copy['Age'])\/365.25\nsns.jointplot(np.abs(cc_train_copy['Account age']),y_age, kind=\"hex\", height=12)\nplt.yticks(np.arange(20, y_age.max(), 5))\nplt.xticks(np.arange(0, 65, 5))\nplt.ylabel('Age')\nplt.show()","db6f4727":"x_employ_length = np.abs(cc_train_copy[cc_train_copy['Employment length'] < 0]['Employment length'])\/365.25\nfig, ax = plt.subplots(figsize=(12,8))\nsns.scatterplot(x_employ_length,y_age,alpha=.05)\n# change the frequency of the x-axis and y-axis labels\nplt.xticks(np.arange(0, x_employ_length.max(), 2.5))\nplt.yticks(np.arange(20, y_age.max(), 5))\nplt.show()","ae2b0801":"# change the datatype of target feature to int\nis_high_risk_int = cc_train_copy['Is high risk'].astype('int32')","a00f64f3":"# correlation analysis with heatmap, after dropping the has a mobile phone with the target feature as int\ncc_train_copy_corr_no_mobile = pd.concat([cc_train_copy.drop(['Has a mobile phone','Is high risk'], axis=1),is_high_risk_int],axis=1).corr()\n# Get the lower triangle of the correlation matrix\n# Generate a mask for the upper triangle\nmask = np.zeros_like(cc_train_copy_corr_no_mobile, dtype='bool')\nmask[np.triu_indices_from(mask)] = True\n# Set up the matplotlib figure\nfig, ax = plt.subplots(figsize=(18,10))\n# seaborn heatmap\nsns.heatmap(cc_train_copy_corr_no_mobile, annot=True, cmap='flare',mask=mask, linewidths=.5)\n# plot the heatmap\nplt.show()","28838d35":"fig, axes = plt.subplots(4,2,figsize=(15,20),dpi=180)\nfig.tight_layout(pad=5.0)\ncat_features = ['Gender', 'Has a car', 'Has a property', 'Employment status', 'Education level', 'Marital status', 'Dwelling', 'Job title']\nfor cat_ft_count, ax in enumerate(axes):\n    for row_count in range(4):\n        for feat_count in range(2):\n            sns.boxplot(ax=axes[row_count,feat_count],x=cc_train_copy[cat_features[cat_ft_count]],y=np.abs(cc_train_copy['Age'])\/365.25)\n            axes[row_count,feat_count].set_title(cat_features[cat_ft_count] + \" vs age\")\n            plt.sca(axes[row_count,feat_count])\n            plt.xticks(rotation=45,ha='right')\n            plt.ylabel('Age')\n            cat_ft_count += 1\n    break","634535b5":"fig, axes = plt.subplots(4,2,figsize=(15,20),dpi=180)\nfig.tight_layout(pad=5.0)\n\nfor cat_ft_count, ax in enumerate(axes):\n    for row_count in range(4):\n        for feat_count in range(2):\n            sns.boxplot(ax=axes[row_count,feat_count],x=cc_train_copy[cat_features[cat_ft_count]],y=np.abs(cc_train_copy[cc_train_copy['Employment length'] < 0]['Employment length'])\/365.25)\n            axes[row_count,feat_count].set_title(cat_features[cat_ft_count] + \" vs employment length\")\n            plt.sca(axes[row_count,feat_count])\n            plt.ylabel('Employment length')\n            plt.xticks(rotation=45,ha='right')\n            cat_ft_count += 1\n    break","7ffc6416":"def chi_func(feature):\n    # selection row with high risk\n    high_risk_ft = cc_train_copy[cc_train_copy['Is high risk'] == 1][feature]\n    high_risk_ft_ct = pd.crosstab(index=high_risk_ft, columns=['Count']).rename_axis(None, axis=1)\n    # drop the index feature name\n    high_risk_ft_ct.index.name = None\n    # observe values\n    obs = high_risk_ft_ct\n    print('Observed values:\\n')\n    print(obs)\n    print('\\n')\n    # expected values\n    print(obs.index)\n    exp = pd.DataFrame([obs['Count'].sum()\/len(obs)] * len(obs.index),columns=['Count'], index=obs.index)\n    print('Expected values:\\n')\n    print(exp)\n    print('\\n')\n    # chi-square test\n    chi_squared_stat = (((obs-exp)**2)\/exp).sum()\n    print('Chi-square:\\n')\n    print(chi_squared_stat[0])\n    print('\\n')\n    #critical value\n    crit = stats.chi2.ppf(q = 0.95, df = len(obs) - 1)\n    print('Critical value:\\n')\n    print(crit)\n    print('\\n')\n    # p-value\n    p_value = 1 - stats.chi2.cdf(x = chi_squared_stat, df = len(obs) - 1)\n    print('P-value:\\n')\n    print(p_value)\n    print('\\n')\n    if chi_squared_stat[0] >= crit:\n        print('Reject the null hypothesis')\n    elif chi_squared_stat[0] <= crit:\n        print('Fail to reject the null hypothesis')","075aa858":"cat_ft = ['Gender', 'Has a car', 'Has a property', 'Employment status', 'Education level', 'Marital status', 'Dwelling', 'Job title']\nfor ft in cat_ft:\n    print('\\n\\n**** {} ****\\n'.format(ft))\n    chi_func(ft)","1e2c50df":"class OutlierRemover(BaseEstimator, TransformerMixin):\n    def __init__(self,feat_with_outliers = ['Family member count','Income', 'Employment length']):\n        self.feat_with_outliers = feat_with_outliers\n    def fit(self,df):\n        return self\n    def transform(self,df):\n        if (set(self.feat_with_outliers).issubset(df.columns)):\n            # 25% quantile\n            Q1 = df[self.feat_with_outliers].quantile(.25)\n            # 75% quantile\n            Q3 = df[self.feat_with_outliers].quantile(.75)\n            IQR = Q3 - Q1\n            # keep the data within 1.5 IQR\n            df = df[~((df[self.feat_with_outliers] < (Q1 - 1.5 * IQR)) |(df[self.feat_with_outliers] > (Q3 + 1.5 * IQR))).any(axis=1)]\n            return df\n        else:\n            print(\"One or more features are not in the dataframe\")\n            return df","21552a36":"class DropFeatures(BaseEstimator,TransformerMixin):\n    def __init__(self,feature_to_drop = ['ID','Has a mobile phone','Children count','Job title']):\n        self.feature_to_drop = feature_to_drop\n    def fit(self,df):\n        return self\n    def transform(self,df):\n        if (set(self.feature_to_drop).issubset(df.columns)):\n            df.drop(self.feature_to_drop,axis=1,inplace=True)\n            return df\n        else:\n            print(\"One or more features are not in the dataframe\")\n            return df","1b8fdab7":"class TimeConversionHandler(BaseEstimator, TransformerMixin):\n    def __init__(self, feat_with_days = ['Employment length', 'Age'], feat_with_months = ['Account age']):\n        self.feat_with_days = feat_with_days\n        self.feat_with_months = feat_with_months\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        if (set(self.feat_with_days).issubset(X.columns)) & (set(self.feat_with_months).issubset(X.columns)):\n            # convert days to absolute value\n            X[['Employment length','Age']] = np.abs(X[['Employment length','Age']])\n            # convert months to absolute value\n            X['Account age'] = np.abs(X['Account age'])\n            return X\n        else:\n            print(\"One or more features are not in the dataframe\")\n            return X","e71dfe01":"class RetireeHandler(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self, df):\n        return self\n    def transform(self, df):\n        if 'Employment length' in df.columns:\n            # select rows with employment length is 365243 which corresponds to retirees\n            df_ret_idx = df['Employment length'][df['Employment length'] == 365243].index\n            # change 365243 to 0\n            df.loc[df_ret_idx,'Employment length'] = 0\n            return df\n        else:\n            print(\"Employment length is not in the dataframe\")\n            return df","9aae5ef1":"class SkewnessHandler(BaseEstimator, TransformerMixin):\n    def __init__(self,feat_with_skewness=['Income','Age']):\n        self.feat_with_skewness = feat_with_skewness\n    def fit(self,df):\n        return self\n    def transform(self,df):\n        if (set(self.feat_with_skewness).issubset(df.columns)):\n            # Handle skewness with cubic root transformation\n            df[self.feat_with_skewness] = np.cbrt(df[self.feat_with_skewness])\n            return df\n        else:\n            print(\"One or more features are not in the dataframe\")\n            return df","bd80de6b":"class BinningNumToYN(BaseEstimator, TransformerMixin):\n    def __init__(self,feat_with_num_enc=['Has a work phone','Has a phone','Has an email']):\n        self.feat_with_num_enc = feat_with_num_enc\n    def fit(self,df):\n        return self\n    def transform(self,df):\n        if (set(self.feat_with_num_enc).issubset(df.columns)):\n            # Change 0 to N and 1 to Y for all the features in feat_with_num_enc\n            for ft in self.feat_with_num_enc:\n                df[ft] = df[ft].map({1:'Y',0:'N'})\n            return df\n        else:\n            print(\"One or more features are not in the dataframe\")\n            return df","82e86265":"class OneHotWithFeatNames(BaseEstimator,TransformerMixin):\n    def __init__(self,one_hot_enc_ft = ['Gender', 'Marital status', 'Dwelling', 'Employment status', 'Has a car', 'Has a property', 'Has a work phone', 'Has a phone', 'Has an email']):\n        self.one_hot_enc_ft = one_hot_enc_ft\n    def fit(self,df):\n        return self\n    def transform(self,df):\n        if (set(self.one_hot_enc_ft).issubset(df.columns)):\n            # function to one hot encode the features in one_hot_enc_ft\n            def one_hot_enc(df,one_hot_enc_ft):\n                one_hot_enc = OneHotEncoder()\n                one_hot_enc.fit(df[one_hot_enc_ft])\n                # get the result of the one hot encoding columns names\n                feat_names_one_hot_enc = one_hot_enc.get_feature_names_out(one_hot_enc_ft)\n                # change the array of the one hot encoding to a dataframe with the column names\n                df = pd.DataFrame(one_hot_enc.transform(df[self.one_hot_enc_ft]).toarray(),columns=feat_names_one_hot_enc,index=df.index)\n                return df\n            # function to concatenat the one hot encoded features with the rest of features that were not encoded\n            def concat_with_rest(df,one_hot_enc_df,one_hot_enc_ft):\n                # get the rest of the features\n                rest_of_features = [ft for ft in df.columns if ft not in one_hot_enc_ft]\n                # concatenate the rest of the features with the one hot encoded features\n                df_concat = pd.concat([one_hot_enc_df, df[rest_of_features]],axis=1)\n                return df_concat\n            # one hot encoded dataframe\n            one_hot_enc_df = one_hot_enc(df,self.one_hot_enc_ft)\n            # returns the concatenated dataframe\n            full_df_one_hot_enc = concat_with_rest(df,one_hot_enc_df,self.one_hot_enc_ft)\n            return full_df_one_hot_enc\n        else:\n            print(\"One or more features are not in the dataframe\")\n            return df","d1b8f40e":"class OrdinalFeatNames(BaseEstimator,TransformerMixin):\n    def __init__(self,ordinal_enc_ft = ['Education level']):\n        self.ordinal_enc_ft = ordinal_enc_ft\n    def fit(self,df):\n        return self\n    def transform(self,df):\n        if 'Education level' in df.columns:\n            ordinal_enc = OrdinalEncoder()\n            df[self.ordinal_enc_ft] = ordinal_enc.fit_transform(df[self.ordinal_enc_ft])\n            return df\n        else:\n            print(\"Education level is not in the dataframe\")\n            return df","4aa9bdff":"class MinMaxWithFeatNames(BaseEstimator,TransformerMixin):\n    def __init__(self,min_max_scaler_ft = ['Age', 'Income', 'Account age', 'Employment length']):\n        self.min_max_scaler_ft = min_max_scaler_ft\n    def fit(self,df):\n        return self\n    def transform(self,df):\n        if (set(self.min_max_scaler_ft).issubset(df.columns)):\n            min_max_enc = MinMaxScaler()\n            df[self.min_max_scaler_ft] = min_max_enc.fit_transform(df[self.min_max_scaler_ft])\n            return df\n        else:\n            print(\"One or more features are not in the dataframe\")\n            return df","0f5dc916":"class OversampleSMOTE(BaseEstimator,TransformerMixin):\n    def __init__(self):\n        pass\n    def fit(self,df):\n        return self\n    def transform(self,df):\n        if 'Is high risk' in df.columns:\n            # SMOTE function to oversample the minority class to fix the imbalance data\n            smote = SMOTE()\n            X_bal, y_bal = smote.fit_resample(df.loc[:, df.columns != 'Is high risk'],df['Is high risk'].astype('int64'))\n            df_bal = pd.concat([pd.DataFrame(X_bal),pd.DataFrame(y_bal)],axis=1)\n            return df_bal\n        else:\n            print(\"Is high risk is not in the dataframe\")\n            return df","38ec2010":"def full_pipeline(df):\n    min_max_scaler_ft = ['Age', 'Income', 'Account age', 'Employment length']\n\n    pipeline = Pipeline([\n        ('outlier_remover', OutlierRemover()),\n        ('feature_dropper', DropFeatures()),\n        ('time_conversion_handler', TimeConversionHandler()),\n        ('retiree_handler', RetireeHandler()),\n        ('skewness_handler', SkewnessHandler()),\n        ('binning_num_to_yn', BinningNumToYN()),\n        ('one_hot_with_feat_names', OneHotWithFeatNames()),\n        ('ordinal_feat_names', OrdinalFeatNames()),\n        ('min_max_with_feat_names', MinMaxWithFeatNames()),\n        ('oversample_smote', OversampleSMOTE())\n    ])\n    df_pipe_prep = pipeline.fit_transform(df)\n    return df_pipe_prep","2a85517e":"pd.options.mode.chained_assignment = None  # Hide the copy warning\ncc_train_prep = full_pipeline(cc_train_copy)","b24ba88d":"cc_train_prep.shape","39b9174a":"cc_train_prep.head()","df5c0196":"# split the train data into X and y (target)\nX_cc_train_prep, y_cc_train_prep = cc_train_prep.loc[:, cc_train_prep.columns != 'Is high risk'], cc_train_prep['Is high risk'].astype('int64')","916e09f2":"classifiers = {\n    'sgd':SGDClassifier(random_state=42),\n    'logistic_regression':LogisticRegression(random_state=42,max_iter=1000),\n#     'support_vector_machine':SVC(random_state=42,probability=True),\n    'decision_tree':DecisionTreeClassifier(random_state=42),\n    'random_forest':RandomForestClassifier(random_state=42),\n    'gaussian_naive_bayes':GaussianNB(),\n    'k_nearest_neighbors':KNeighborsClassifier(),\n    'gradient_boosting':GradientBoostingClassifier(random_state=42),\n    'linear_discriminant_analysis':LinearDiscriminantAnalysis(),\n    'bagging':BaggingClassifier(random_state=42),\n    'neural_network':MLPClassifier(random_state=42,max_iter=1000),\n    'adaboost':AdaBoostClassifier(random_state=42),\n    'extra_trees':ExtraTreesClassifier(random_state=42),\n    }","65a3c2d5":"def feat_importance_plot(model, model_name):\n    if model_name not in ['support_vector_machine','gaussian_naive_bayes','k_nearest_neighbors','bagging','neural_network']:\n        # change xtick font size\n        plt.rcParams['xtick.labelsize'] = 12\n        plt.rcParams['ytick.labelsize'] = 12\n        # top 10 most predictive features\n        top_10_feat = FeatureImportances(model, relative=False, topn=10)\n        # top 10 least predictive features\n        bottom_10_feat = FeatureImportances(model, relative=False, topn=-10)\n        #change the figure size\n        plt.figure(figsize=(10, 4))\n        #change x label font size\n        plt.xlabel('xlabel', fontsize=14)\n        # Fit to get the feature importances\n        top_10_feat.fit(X_cc_train_prep, y_cc_train_prep)\n        # show the plot\n        top_10_feat.show()\n        print('\\n')\n        plt.figure(figsize=(10, 4))\n        plt.xlabel('xlabel', fontsize=14)\n        # Fit to get the feature importances\n        bottom_10_feat.fit(X_cc_train_prep, y_cc_train_prep)\n        # show the plot\n        bottom_10_feat.show()\n        print('\\n')\n    else:\n        print('No feature importance for {0}'.format(model_name))\n        print('\\n')","03ee9780":"def y_prediction_func(model_trn):\n    # check if y_train_copy_pred exists, if not create it\n    y_cc_train_pred_path = Path('saved_models\/{0}\/y_train_copy_pred_{0}.sav'.format(model_name))\n    try:\n        y_cc_train_pred_path.resolve(strict=True)\n    except FileNotFoundError:\n        #cross validation prediction with kfold = 10\n        y_cc_train_pred = cross_val_predict(model_trn,X_cc_train_prep,y_cc_train_prep,cv=10,n_jobs=-1)\n        #save the predictions\n        joblib.dump(y_cc_train_pred,y_cc_train_pred_path)\n        return y_cc_train_pred\n    else:\n        # if it exist load the predictions\n        y_cc_train_pred = joblib.load(y_cc_train_pred_path)\n        return y_cc_train_pred","266dbe05":"def confusion_matrix_func(model_name):\n    fig, ax = plt.subplots(figsize=(8,8))\n    #plot confusion matrix\n    conf_matrix = ConfusionMatrixDisplay.from_predictions(y_cc_train_prep,y_prediction_func(model_trn),ax=ax, cmap='Blues',values_format='d')\n    # remove the grid\n    plt.grid(b=None)\n    # increase the font size of the x and y labels\n    plt.xlabel('Predicted label', fontsize=14)\n    plt.ylabel('True label', fontsize=14)\n    #give a title to the plot using the model name\n    plt.title('Confusion Matrix', fontsize=14)\n    #show the plot\n    plt.show()\n    print('\\n')","34bb9dc1":"def roc_curve_func(model_trn,model_name):\n    # check if y probabilities file exists, if not create it\n    y_proba_path = Path('saved_models\/{0}\/y_cc_train_proba_{0}.sav'.format(model_name))\n    try:\n        y_proba_path.resolve(strict=True)\n    except FileNotFoundError:\n        y_cc_train_proba = model_trn.predict_proba(X_cc_train_prep)\n        joblib.dump(y_cc_train_proba,y_proba_path)\n    else:\n        # if path exist load the y probabilities file\n        y_cc_train_proba = joblib.load(y_proba_path)\n    skplt.metrics.plot_roc_curve(y_cc_train_prep, y_cc_train_proba, title = 'ROC curve for {0}'.format(model_name), cmap='cool',figsize=(8,6), text_fontsize='large')\n    #remove the gride\n    plt.grid(b=None)\n    plt.show()\n    print('\\n')","d3a6a006":"def score_func(model_trn, model_name):\n    # check if score file exists, if not create it\n    class_report_path = Path('saved_models\/{0}\/class_report_{0}.sav'.format(model_name))\n    try:\n        class_report_path.resolve(strict=True)\n    except FileNotFoundError:\n        # calculate the scores of the model\n        class_report = classification_report(y_cc_train_prep,y_prediction_func(model_trn))\n        print(class_report)\n        # save the scores\n        joblib.dump(class_report,class_report_path)\n    else:\n        # if it exist load the scores\n        class_report = joblib.load(class_report_path)\n        print(class_report)","ed121fdf":"def train_model(model,model_name):\n    # check if the model file exist and if not create, train and save it\n    model_file_path = Path('saved_models\/{0}\/{0}_model.sav'.format(model_name))\n    try:\n        model_file_path.resolve(strict=True)\n    except FileNotFoundError:\n        if model_name == 'sgd':\n            # for sgd, loss = 'hinge' does not have a predict_proba method. Therefore, we use a calibrated model\n            calibrated_model = CalibratedClassifierCV(model, cv=10, method='sigmoid')\n            model_trn = calibrated_model.fit(X_cc_train_prep,y_cc_train_prep)\n        else:\n            model_trn = model.fit(X_cc_train_prep,y_cc_train_prep)\n        joblib.dump(model_trn,model_file_path)\n        # plot the most and least predictive features\n        return model_trn\n    else:\n        # if path exist load the model\n        model_trn = joblib.load(model_file_path)\n        # plot the most and least predictive features\n        return model_trn","fe608938":"def folder_check():\n    # check if the folder for saving the model exists, if not create it\n    if not os.path.exists('saved_models\/{}'.format(model_name)):\n        os.makedirs('saved_models\/{}'.format(model_name))\n","0b4e856b":"for model_name,model in classifiers.items():\n    # title formatting\n    print('\\n')\n    print('\\n')\n    print('  {}  '.center(50,'-').format(model_name))\n    print('\\n')\n    # check if the folder for saving the model exists, if not create it\n    folder_check()\n    # train the model\n    model_trn = train_model(model,model_name)\n    # print the scores from the classification report\n    score_func(model_trn, model_name)\n    # plot the ROC curve\n    roc_curve_func(model_trn,model_name)\n    # plot the confusion matrix\n    confusion_matrix_func(model_name)\n    # print the most and least predictive features\n    feat_importance_plot(model,model_name)","b841e69a":"## 2.3 Univariate analysis","0d4ac97f":"Interpretation:\n- More than 90% of applicants don\u2019t have an email\n\nNote: Here 0 is no and 1 is yes","7c73ecfe":"### 2.3.5 Children count","927e870f":"Interpretation:\n- The more children a person has, the larger the family member count.","6236c16b":"## 2.4 Bivariate analysis","6bb86512":"### 3.2.1 Outliers handling","bad6656c":"### 3.4.4 Binning","8a620567":"##### 2.4.1.1.3 Employment length vs age (numerical vs numerical feature comparison)","684fd325":"Interpretation:\n- Most applicants don\u2019t have a phone (probably a home phone)\n\nNote: Here 0 is no and 1 is yes","f45d3fdb":"### 2.3.2 Age","3f2905cf":"Null hypothesis: the feature's categories have no effect on the target variable.\nAlternate hypothesis: one(or more) of the feature categories has a significant effect on the target variable.","9be29324":"### 3.4.2 Retiree handling (in the employment length feature)","e0395dff":"### 2.3.15 Has a mobile phone","0c44dbfd":"* bivariate analysis with target variable","1b43307d":"### 2.3.8 Job title","77628cf9":"### 2.4.3 Categorical vs categorical features (Chi-square test)","4a9ec734":"Interpretation:\n- There is no feature that is correlated with the target feature\n- Family member count is highly correlated with children count as previously discussed\n- Age has some positive correlation with the family member count and children count. The older a person is, the most likely he\/she will have a larger family.\n- Another positive correlation is having a phone and having a work phone.\n- The final positive correlation is between the age and work phone. The younger someone is the less likely he\/she will have a work phone.\n- We also have a negative correlation between the employment length and the age as previously seen.","1b05d659":"Interpretation:\n- Almost every applicants live in house or apartment","d3eaf49b":"Interpretation:\n-\tThe most prolific Job title is laborers by far\n-\tWe have 30.95% if missing data","153aa640":"##### 2.4.1.1.2 Account age vs age (numerical vs numerical feature comparison)","067c87c4":"Interpretation:\n- Female applicants are older than their male counterpart.\n- Those who don't own a car tend to be older.\n- Those who own a property tend to be older than those who don't.\n- Of course, the pensioners are older that those who are working (We also see that some have pensioned at a young age, those are outliers).\n- It is also interesting to see that those who hold an academic degree are younger in general than the other groups.\n- Obviously, the widows tend to be much older. We also see some outliers in their 30's as well.\n- With no surprise, those who live with parent tend to be younger. We also see some outlier as well.\n- Lastly, who work as cleaning staff tend to be older while those who work in IT tend to be younger.","d1b83f02":"#### 2.4.2.1 Age vs the rest of categorical features","ac18524c":"### 2.3.18 Account age","3490849f":"### 2.3.10 Education level","bcaaec40":"Interpretation:\n- We have more female applicants than male (67% vs 32%)","422e4117":"## 2.2 Functions used to explore each feature","f1ef9d49":"Interpretation:\n- Most applicant don\u2019t own a car","4fc528d7":"# 0. import the necessary packages","381db30a":"### 2.3.9 Employment status","f0497095":"Interpretation:\n-\tAs previously discussed most applicant don\u2019t have a child\n-\tAlso here we have 6 outliers, probably the same from the family member count","d2dbea30":"Interpretation:\n- Most of the applicants are between 20 and 45 years old and have an account that is less than 25 months old.","ac0019d4":"Interpretation:\n-\tMost of applicant have been working for 5 years\n-\tWe have quite a number of outliers who have been working for more than 20 years+\n-\tThe employment length is positively skewed\n-\tThose who are high risk have a low employment length of 5 versus 7 years for the low risk\n","98400115":"* bivariate analysis with target variable","abd105d4":"Interpretation:\n- This scatterplot shows that the age of the applicants is correlated with the length of the employment.\nThe reason why it is shaped like a reversed triangle, it is because the age of the applicants increase with the length of the employment. You can't have an employment length > than the age.","ebf60166":"### 3.4.8 Oversampling with SMOTE","03ec2909":"### 3.4.3 Skewness handling","99aa90ec":"### 2.3.6 Dwelling type","62bacec4":"### 2.3.16 Has a phone","21c3242e":"## 1.4 Split the data into training and test sets, creating a copy of the datasets","856e9ffb":"Interpretation:\n- The youngest applicant is 21 years old while the oldest is 68 years old. with the average of 43.7 and median of 42.6(outliers insensitive)\n- Age feature is not normally distributed, it is slightly positively skew\n- There is no difference between the average age of high and low risk applicants","7aa4bf54":"## 3.2 Data Cleaning","4a94ef71":"ID:\n* Drop the feature\n\nGender:\n* One hot encoding\n\nAge:\n* Min-max scaling\n* Fix skewness\n* Abs value and div 365.25\n\nMarital status:\n* One hot encoding\n\nFamily member count\n* Fix outliers\n\nChildren count\n* Fix outliers\n* Drop feature\n\nDwelling type\n* One hot encoding\n\nIncome\n* Remove outliers\n* Fix skewness\n* Min-max scaling\n\nJob title\n* One hot encoding\n* Impute missing values\n\nEmployment status:\n* One hot encoding\n\nEducation level:\n* Ordinal encoding\n\nEmployment length:\n* Remove outliers\n* Min-max scaling\n* Abs value and div 365.25\n* change days of employments of retirees to 0\n\nHas a car:\n* Change it numerical\n* One-hot encoding\n\nHas a property:\n* Change it numerical\n* One-hot encoding\n\nHas a mobile phone:\n* Drop feature\n\nHas a work phone:\n* One-hot encoding\n\nHas a phone:\n* One-hot encoding\n\nHas an email:\n* One-hot encoding\n\nAccount age:\n* Min-max scaling\n* Change to abs value\n\nIs high risk(Target):\n* balance the data with SMOTE","d611be2f":"## 2.5 Business findings from the EDA","c6402807":"### 3.4.7 Min-Max scaling (with feature's names)","15842db0":"### 2.3.7 Income","f904b845":"Interpretation:\n- Most applicants are employed","8c801fb8":"Interpretation:\n-   Most applicants are two in their household, this is also confirmed with the fact that most don\u2019t have a child (more on this in a bit)\n-   We also have 6 outliers, 2 of them are extreme with 20 and 15 members in their household\n","a618b852":"### 2.3.3 Marital status","1b7ce4f6":"Interpretation:\n- All the applicants without exception have a mobile phone\n\nNote: Here 0 is no and 1 is yes","4ef50ec8":"### 3.3.1 Drop features","27016fa2":"#### 2.4.1.2 Correlation analysis","2122178b":"## 1.1 import csv file","248d9a36":"## 3.5 Data Preprocessing","42a90bc2":"# 3. Prepare the data","3001d8f7":"## 4.1 Quick model training","5763d870":"## 3.1 Transform to be done on each feature","4f897cf0":"### 3.4.6 Ordinal encoding (with feature's names)","09b3671d":"* bivariate analysis with target variable","6f5d76a3":"Interpretation:\n- More than \u00be of applicants don\u2019t have a work phone\n\nNote: Here 0 is no and 1 is yes","5dd3abf7":"## 2.1 Quick glance at the data","5ba83ddd":"##### 2.4.1.1.1 Family member count vs children count (numerical vs numerical feature comparison)","bb643242":"### 2.3.17 Has an email","0cdaf88a":"### 2.3.14 Has a work phone","0c7f756c":"# 2. Explore the Data","a99ce8fd":"### 2.4.2 Numerical vs categorical features (ANOVA)","105863d1":"Interpretation:\n- Most applicants are married\n- Even though we have a higher number of applicants who are separated than those who are widow, it seems like widow applicants are high risk than those who are separated.","dba655fe":"- ***Typical profile of an applicant is: a Female in her early 40\u2019s, married with a partner and no child. She has been employed for 5 years with a salary of 157500. She has completed her secondary education. She does not own a car but owns a property (a house\/ apartment). Her account is 26 months old.***\n- ***Age and income do not have any effects on the target variable***\n- ***Those who are flagged as bad client, tend to have a shorter employment length and older accounts. They also constitute less than 2% of total applicants.***\n- ***Most applicants are 20 to 45 years old and have an account that is 25 months old or less.***","461bcb20":"### 2.3.11 Employment length","782a81ef":"# 4. Short-list promising models","b599feed":"Interpretation:\n- A vast majority of applicants are low risk applicants.\n- We have a very imbalance data.\n\nNote: Here 0 is no and 1 is yes\n","3a38f0af":"Interpretation:\n-\tMost accounts are 26 months old\n-\tAccount age is not normally distributed, it is positively skewed\n-\tThe accounts that have been flagged as high risk are on average 34 months old vs 26 months old for old account","b630b974":"## 3.3 Feature selection","972bf1ed":"Interpretation:\n-\tThe average income is 186890 but this amount accounts for outliers. If we ignore the outlier most people make 157500\n-\tWe have 3 applicants who makes more than 1000000\n-\tThis feature is positively skewed\n-\tHigh risk and low risk applicants have roughly similar income","db2afc81":"#### 2.4.1.1 Scatter plots","30e75027":"## 1.2 creating the target variable","0af43f73":"### 2.3.13 Has a property","5b894f54":"* bivariate analysis with target variable","d73b9e81":"Interpretation:\n- Most applicants own a property","4a81b44e":"### 2.3.4 Family member count","5fe0f587":"### 3.4.5 One hot encoding (with feature's name)","900dfbdc":"Interpretation:\n-\tThe majority of applicants have completed their secondary degree, \u00bc completed their higher education","d091b1ff":"### 2.3.12 Has a car","b72467be":"#### 2.4.2.2 Income vs the rest of categorical features","e7278536":"# 1. Get the data","756f17b8":"Interpretation:\n- We can see a positive linear correlation between the family member and the children count. This makes sense, the more the children someone have, the larger the family member count. This is a multicollinearity problem. Meaning that the features are highly correlated. We will need to drop one of them.\n- Another interesting trend is the Employment length and age. This also makes sense, the longer the employee has been working, the older they are.","115ad774":"## 3.4 Feature engineering","34fda0e8":"### 3.4.1 Time conversion\n","eee24d13":"### 2.4.1 Numerical vs numerical features (Correlation & scatter plots)","4a20da85":"### 2.3.1. Gender","b79a96c5":"### 2.3.19 Is high risk (target variable)","b5d5fa03":"Interpretation:\n- State employed applicant tend to have been employed longer than the rest.\n- Those who work in the medical field, have been employed longer than the rest.","94311cfc":"## 1.3 Rename features"}}