{"cell_type":{"f4f4125f":"code","f2ae79c8":"code","551ff951":"code","f7e246fd":"code","6da4af95":"code","a650e2fe":"code","db636d36":"code","4f59ac1e":"code","d7aad608":"code","fa29893d":"code","d100933d":"code","cb20bb22":"code","cfe25366":"code","0431241b":"code","a9f5e725":"code","55b97c8e":"code","59b71126":"code","132fe4b3":"code","8c1eb9b7":"code","9885f93b":"code","1a4c362b":"code","675c3381":"markdown","691afaaa":"markdown","d688beaa":"markdown","b9c502d9":"markdown","42d35a60":"markdown","3be273e0":"markdown"},"source":{"f4f4125f":"num_words=50000\nmax_len=64\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport operator\nimport tensorflow as tf\nimport keras.backend as K\nfrom keras.engine.topology import Layer\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, concatenate, Dense, Flatten, Embedding, GRU, CuDNNGRU, LSTM, CuDNNLSTM, SpatialDropout1D, Dropout, Bidirectional, Conv1D, Activation, GlobalMaxPooling1D, GlobalAveragePooling1D, MaxPooling1D, RepeatVector, Permute\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.python.keras.optimizers import Adam\nfrom tensorflow.keras import utils\nfrom keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\ntqdm.pandas()\nimport matplotlib.pyplot as plt\nimport tracemalloc\n%matplotlib inline \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f2ae79c8":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)","551ff951":"def build_vocab(sentences, verbose =  True):\n    \"\"\"\n    :param sentences: list of list of words\n    :return: dictionary of words and their count\n    \"\"\"\n    vocab = {}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] += 1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\ndef clean_text(x):\n\n    x = str(x)\n    for punct in \"\/-'\":\n        x = x.replace(punct, ' ')\n    for punct in '&':\n        x = x.replace(punct, f' {punct} ')\n    for punct in '?!.,\"#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~' + '\u201c\u201d\u2019':\n        x = x.replace(punct, '')\n    \n    return x\ndef clean_numbers(x):\n\n    x = re.sub('[0-9]{5,}', ' huge number ', x)\n    x = re.sub('[0-9]{4}', ' year ', x)\n    x = re.sub('[0-9]{3}', ' number ', x)\n    x = re.sub('[0-9]{2}', ' number ', x)\n    return x\n\ndef clean_more(x):\n    x=re.sub('\\s+', ' ', x).strip()\n    regex = re.compile('[^a-zA-Z] ')\n    #First parameter is the replacement, second parameter is your input string\n    return regex.sub('', x)\n    \n\n\n\n\ndef _get_mispell(mispell_dict):\n    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n    return mispell_dict, mispell_re\n\n\nmispell_dict = {'colour':'color',\n                'centre':'center',\n                'didnt':'did not',\n                'doesnt':'does not',\n                'isnt':'is not',\n                'shouldnt':'should not',\n                'favourite':'favorite',\n                'travelling':'traveling',\n                'counselling':'counseling',\n                'theatre':'theater',\n                'cancelled':'canceled',\n                'labour':'labor',\n                'organisation':'organization',\n                'citicise':'criticize',\n                }\nmispellings, mispellings_re = _get_mispell(mispell_dict)\n\ndef replace_typical_misspell(text):\n    def replace(match):\n        return mispellings[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\ntrain[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: clean_text(x))\ntrain[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\ntrain[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: clean_more(x))\ntrain[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: x.lower())\ntrain[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\nsentences = train[\"question_text\"].apply(lambda x: x.split())\n#to_remove = ['a','to','of','and']\n#sentences = [[word.lower() for word in sentence if not word.lower() in to_remove] for sentence in tqdm(sentences)]\n\n\ntest[\"question_text\"] = test[\"question_text\"].progress_apply(lambda x: clean_text(x))\ntest[\"question_text\"] = test[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\ntest[\"question_text\"] = test[\"question_text\"].progress_apply(lambda x: clean_more(x))\ntest[\"question_text\"] = test[\"question_text\"].progress_apply(lambda x: x.lower())\ntest[\"question_text\"] = test[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\ntsentences = test[\"question_text\"].apply(lambda x: x.split())\n#to_remove = ['a','to','of','and']\n#tsentences = [[word.lower() for word in sentence if not word.lower() in to_remove] for sentence in tqdm(tsentences)]\n\nvocab = build_vocab(list(sentences)+list(tsentences))\ndef wordindex(vocab,n):\n  word_index={}\n  sorted_v = sorted(vocab.items(), key=operator.itemgetter(1))[::-1]\n  for i in range(n-3):\n    word_index[sorted_v[i][0]]=i+3\n  return(word_index)\n\nword_index=wordindex(vocab,num_words)\n\ndef zif(word):\n  ans=2\n  if (word in word_index):\n    ans=word_index[word]\n  return ans\n\nx_train = [[zif(word) for word in sentence] for sentence in tqdm(sentences)]\nx_train=pad_sequences(x_train, maxlen=max_len)\n\n\n\nx_test = [[zif(word) for word in sentence] for sentence in tqdm(tsentences)]\nx_test=pad_sequences(x_test, maxlen=max_len)\nprint({k: vocab[k] for k in list(vocab)[:5]})","f7e246fd":"def wordindex(vocab,n):\n  word_index={}\n  sorted_v = sorted(vocab.items(), key=operator.itemgetter(1))[::-1]\n  for i in range(n-3):\n    word_index[sorted_v[i][0]]=i+3\n  return(word_index)\n\nword_index=wordindex(vocab,num_words)\n\ndef zif(word):\n  ans=2\n  if (word in word_index):\n    ans=word_index[word]\n  return ans\n\nx_train = [[zif(word) for word in sentence] for sentence in tqdm(sentences)]\nx_train=pad_sequences(x_train, maxlen=max_len)\n\n\n\nx_test = [[zif(word) for word in sentence] for sentence in tqdm(tsentences)]\nx_test=pad_sequences(x_test, maxlen=max_len)\n\ny_train=train['target']","6da4af95":"del train\ndel sentences\ndel tsentences","a650e2fe":"embedding_matrix_Glove = np.zeros((num_words, 300))\nwith open('..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt', 'r') as f:\n    for line in tqdm(f):\n        values = line.split()\n        word = values[0]\n        if (word in word_index):\n            try:\n                word_vector = np.asarray(values[1:], dtype='float32')        \n            except ValueError:\n                pass  # do nothing!\n            else:\n                embedding_matrix_Glove[word_index[word]] = word_vector","db636d36":"embedding_matrix_Wiki = np.zeros((num_words, 300))\nwith open('..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec', 'r') as f:\n    for line in tqdm(f):\n        values = line.split()\n        word = values[0]\n        if (word in word_index):\n            try:\n                word_vector = np.asarray(values[1:], dtype='float32')        \n            except ValueError:\n                pass  # do nothing!\n            else:\n                embedding_matrix_Wiki[word_index[word]] = word_vector","4f59ac1e":"len(y_train)","d7aad608":"def f1(y_true, y_pred):\n    y_pred = K.round(y_pred+0.15)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)","fa29893d":"class Netn:  \n    def __init__(self,em):\n        tweet_input = Input(shape=(max_len,), dtype='int32')\n        tweet_encoder = Embedding(num_words, 300, input_length=max_len,\n                          weights=[em], trainable=False)(tweet_input)\n        X = SpatialDropout1D(0.1)(tweet_encoder)\n        X = Bidirectional(CuDNNGRU(64, return_sequences=True))(X)\n        activations = CuDNNGRU(64, return_sequences=True)(X)\n        # compute importance for each step\n        attention = Dense(1, activation='tanh')(activations)\n        attention = Flatten()(attention)\n        attention = Activation('softmax')(attention)\n        attention = RepeatVector(64)(attention)\n        attention = Permute([2, 1])(attention)\n        attention = Dropout(0.5)(attention)\n        X = concatenate([activations, attention])\n        x=CuDNNGRU(64, return_sequences=True)(X)\n        a=GlobalMaxPooling1D()(x)\n        b=GlobalAveragePooling1D()(x) \n        x=concatenate([a,b])\n        x=Dense(64, activation='relu')(x)\n        x=Dropout(0.4)(x)\n        output = Dense(1, activation='sigmoid')(x)\n        self.model = Model(inputs=[tweet_input], outputs=[output])\n        self.model.summary()\n    \n    def unfreeze(self):\n        self.model.layers[1].trainable = True\n  \n    def fit(self,b1,b2,epp,bs,**data):\n        self.model.compile(**data)\n        \n        x_t=x_train[b1:b2]\n        y_t=y_train[b1:b2]\n        if (epp>0):\n            filepath='tmp.hd5'\n            cp=ModelCheckpoint(filepath, monitor=\"val_f1\",verbose=1, save_best_only=True,mode='max')\n            history=self.model.fit(x_t, \n                    y_t, \n                    epochs=epp,\n                    batch_size=bs,\n                    callbacks=[cp],\n                    validation_split=0.1)\n            self.model.load_weights(filepath)\n            plt.plot(history.history['f1'], label='f1 train')\n            plt.plot(history.history['val_f1'], label='f1 val')\n            plt.xlabel('epoche')\n            plt.ylabel('f1')\n            plt.legend()\n            plt.show()\n            ansz=history.history['val_f1']\n            mx=max(ansz)\n            print('val f1 is maximal {} on a step {}'.format(mx,ansz.index(mx)+1))\n\n  \n\n    def predvec(self,x_test):\n        return(self.model.predict(x_test))    \n","d100933d":"m1=Netn(embedding_matrix_Wiki)\n","cb20bb22":"m1.fit(0,600000,3,2000,loss='binary_crossentropy', metrics=['accuracy', f1],\n              optimizer=Adam(lr=1e-3))\nm1.fit(0,1250000,5,2000,loss='binary_crossentropy', metrics=['accuracy', f1],\n              optimizer=Adam(lr=3e-4))\nm1.unfreeze()\nm1.fit(0,1250000,14,2000,loss='binary_crossentropy', metrics=['accuracy', f1],\n              optimizer=Adam(lr=3e-5))","cfe25366":"m2=Netn(embedding_matrix_Glove)","0431241b":"m2.fit(0,600000,3,2000,loss='binary_crossentropy', metrics=['accuracy', f1],\n              optimizer=Adam(lr=1e-3))\nm2.fit(0,1250000,5,2000,loss='binary_crossentropy', metrics=['accuracy', f1],\n              optimizer=Adam(lr=3e-4))\nm2.unfreeze()\nm2.fit(0,1250000,14,2000,loss='binary_crossentropy', metrics=['accuracy', f1],\n              optimizer=Adam(lr=3e-5))","a9f5e725":"lval=1250000\ny_v=y_train.tolist()[lval:]\nx_v=x_train[lval:]\n","55b97c8e":"u1=m1.predvec(x_v)\nu2=m2.predvec(x_v)","59b71126":"def qf1(t,vv):\n    tp=0\n    fp=0\n    fn=0\n    yt=y_train.tolist()\n    for i in range(len(vv)):\n        if vv[-i]>t:\n            if yt[-i]==1:\n                tp+=1\n            else:\n                fp+=1\n        else:\n            if yt[-i]==1:\n                fn+=1\n    return (2*tp\/(2*tp+fn+fp))\n\ndef best(vv):\n    cc=0.35\n    a=0\n    for i in range(10):\n        r= qf1(cc+0.01*i,vv)\n        if (r>a):\n            ii=i\n            a=r\n    a=0\n    for i in range(10):\n        r= qf1(cc+0.01*(ii-1)+0.002*i,vv)\n        if (r>a):\n            iii=i\n            a=r\n    \n    print(a)\n    return(cc+0.01*(ii-1)+0.002*iii)\n    ","132fe4b3":"bt=best((u1+u2)\/2)\nprint(bt)","8c1eb9b7":"def ans(v,tr):\n    res=np.zeros(len(v))\n    for i in range(len(v)):\n        if v[i]>tr:\n            res[i]=1\n    return res","9885f93b":"v1=m1.predvec(x_test)\nv2=m2.predvec(x_test)\nvv=ans((v1+v2)\/2,bt)","1a4c362b":"out = np.column_stack((test['qid'].values,vv))\nnp.savetxt('submission.csv', out, header=\"qid,prediction\", \n            comments=\"\", fmt=\"%s,%d\")","675c3381":"**Best threshold**","691afaaa":"This is my first public kernel here. Thanks to other kagglers for showing the way how to do all this. Special thanks to\n\nhttps:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings for data preparation\n\nhttps:\/\/www.kaggle.com\/guglielmocamporese\/macro-f1-score-keras for f1 metric\n\nhttps:\/\/stackoverflow.com\/questions\/42918446\/how-to-add-an-attention-mechanism-in-keras\/44387553 for the attention layer\n","d688beaa":"**Committung**","b9c502d9":"**Embeddings**","42d35a60":"**Loading and Preprocessing text**","3be273e0":"**NET**"}}