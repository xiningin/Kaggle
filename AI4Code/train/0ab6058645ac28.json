{"cell_type":{"f577a71a":"code","b81a3164":"code","65ade381":"code","58b480c8":"code","cd3904da":"code","47fdad56":"code","5db7c23d":"code","9adcd40b":"code","3d871b68":"code","a48e875f":"code","e7723718":"code","8a456cf4":"code","eea66637":"code","7940a265":"code","275549ea":"code","b9e4bc6e":"code","a3e88d79":"code","f23bbe84":"code","f69fa81f":"code","f89b96e2":"code","b30fea9f":"code","11a713f4":"code","8ea43d30":"code","46585c3c":"code","8be0aee3":"code","04ad30f8":"code","9cb4a830":"code","bf6382d6":"code","e394d64e":"code","6764fd5c":"code","ccfb41d0":"code","bfea0571":"code","762b1db4":"code","1988e74a":"code","9879cbfd":"code","830e37f5":"code","afcde768":"code","b9615ae5":"markdown","8cfc1cf8":"markdown","e48767ff":"markdown","2831269d":"markdown","b8e50185":"markdown","719fad4f":"markdown","747563c0":"markdown","82d0be67":"markdown","048a9cea":"markdown","07d7c8b6":"markdown","638d085c":"markdown"},"source":{"f577a71a":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# visualization\nimport seaborn as sns\n%config InlineBackend.figure_format = 'retina' #set 'png' here when working on notebook\nsns.set_context(\"talk\")\nsns.set_palette('husl')\nsns.set()\n\n# Pre-processing\nfrom sklearn.preprocessing import (StandardScaler, MinMaxScaler, Normalizer, \n                                      MaxAbsScaler, RobustScaler, PowerTransformer)\nfrom sklearn.preprocessing import PolynomialFeatures \nfrom sklearn.experimental import enable_iterative_imputer  # to import IterativeImputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nfrom sklearn.utils import resample\n\n# Dimensionallity Reduction and Feature Extraction\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Post-analysis\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\n# stats\nfrom scipy.stats import skew\nfrom scipy.stats.stats import pearsonr","b81a3164":"test = pd.read_csv('test.csv')\ntrain = pd.read_csv('train.csv')\n\nall_data = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                      test.loc[:,'MSSubClass':'SaleCondition']))\n\nall_data.head(3)","65ade381":"y = train['SalePrice']\nsns.distplot(y, kde=False);","58b480c8":"y = np.log1p(train['SalePrice'])\n\n#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])","cd3904da":"# one hot encoder\nall_data = pd.get_dummies(all_data)\n\n# impute date\nall_data = all_data.fillna(all_data.mean())","47fdad56":"X_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]","5db7c23d":"from sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, ElasticNet, ElasticNetCV\nfrom sklearn.model_selection import cross_val_score\n\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","9adcd40b":"model_ridge = Ridge()\n\nalphas = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50]\ncv_ridge = [rmse_cv(Ridge(alpha = alpha)).mean() for alpha in alphas]\n\ncv_ridge = pd.Series(cv_ridge, index = alphas)\ncv_ridge.plot(title = \"Validation\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmse\");\nprint(cv_ridge.min())","3d871b68":"model_ridge.fit(X_train, y)\npreds_ridge = np.expm1(model_ridge.predict(X_train))\n\n# Plot residuals\nplt.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":model_ridge.predict(X_train), \"true\":y})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")","a48e875f":"model_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\nrmse_cv(model_lasso).mean()","e7723718":"coef = pd.Series(model_lasso.coef_, index = X_train.columns)\n\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","8a456cf4":"imp_coef = pd.concat([coef.sort_values().head(10), coef.sort_values().tail(10)])\n\nplt.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\");","eea66637":"#plot residuals\nplt.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":model_lasso.predict(X_train), \"true\":y})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")","7940a265":"model_elas = ElasticNetCV(alphas = [1, 0.1, 0.001, 0.0005]).fit(X_train, y)\nrmse_cv(model_elas).mean()","275549ea":"coef = pd.Series(model_elas.coef_, index = X_train.columns)\nprint(\"ElasticNet picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","b9e4bc6e":"imp_coef = pd.concat([coef.sort_values().head(10), coef.sort_values().tail(10)])\n\nplt.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the ElasticNet Model\");","a3e88d79":"#plot residuals\nplt.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":model_elas.predict(X_train), \"true\":y})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\");","f23bbe84":"import xgboost\n\ndtrain = xgboost.DMatrix(X_train, label = y)\ndtest = xgboost.DMatrix(X_test)\n\nparams = {\"max_depth\":2, \"eta\":0.1}\nmodel = xgboost.cv(params, dtrain,  num_boost_round=500, early_stopping_rounds=100)","f69fa81f":"model.loc[30:,[\"test-rmse-mean\", \"train-rmse-mean\"]].plot()","f89b96e2":"model_xgb = xgboost.XGBRegressor(n_estimators=360, max_depth=2, learning_rate=0.1) #the params were tuned using xgb.cv\nmodel_xgb.fit(X_train, y)\nrmse_cv(model_xgb).mean()","b30fea9f":"#plot residuals\nplt.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":model_xgb.predict(X_train), \"true\":y})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")","11a713f4":"xgb_preds = np.expm1(model_xgb.predict(X_test))\nlasso_preds = np.expm1(model_lasso.predict(X_test))","8ea43d30":"predictions = pd.DataFrame({\"xgb\":xgb_preds, \"lasso\":lasso_preds})\npredictions.plot(x = \"xgb\", y = \"lasso\", kind = \"scatter\")","46585c3c":"solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":lasso_preds})\nsolution.to_csv(\"submission.csv\", index = False)","8be0aee3":"from keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.regularizers import l1\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n","04ad30f8":"#X_train = StandardScaler().fit_transform(X_train)","9cb4a830":"X_tr, X_val, y_tr, y_val = train_test_split(X_train, y, random_state = 3)\nX_tr = StandardScaler().fit_transform(X_tr)\nX_val = StandardScaler().fit_transform(X_val)","bf6382d6":"model = Sequential()\nmodel.add(Dense(32, activation='relu', input_dim=X_tr.shape[1]))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","e394d64e":"model.compile(optimizer='sgd',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])","6764fd5c":"history = model.fit(X_tr, y_tr,\n          batch_size=32, epochs=50,\n          validation_data=(X_val, y_val)\n                   )","ccfb41d0":"# accuracy\nmodel.evaluate(X_val, y_val)[1]","bfea0571":"# set up figure\nf = plt.figure(figsize=(12,5))\nf.add_subplot(1,2, 1)\n\n# plot accuracy as a function of epoch\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\n\n# plot loss as a function of epoch\nf.add_subplot(1,2, 2)\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\nplt.show(block=True)","762b1db4":"model_2 = Sequential([\n    Dense(1000, activation='relu', input_dim=X_tr.shape[1]),\n    Dense(1000, activation='relu'),\n    Dense(1000, activation='relu'),\n    Dense(1000, activation='relu'),\n    Dense(1, activation='sigmoid'),\n])\n\nmodel_2.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nhistory_2 = model_2.fit(X_tr, y_tr,\n          batch_size=32, epochs=50,\n          validation_data=(X_val, y_val))","1988e74a":"# set up figure\nf = plt.figure(figsize=(12,5))\nf.add_subplot(1,2, 1)\n\n# plot accuracy as a function of epoch\nplt.plot(history2.history['accuracy'])\nplt.plot(history2.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\n\n# plot loss as a function of epoch\nf.add_subplot(1,2, 2)\nplt.plot(history2.history['loss'])\nplt.plot(history2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\nplt.show(block=True)","9879cbfd":"from keras.layers import Dropout\nfrom keras import regularizers\n\nmodel_3 = Sequential([\n    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01), input_dim=X_tr.shape[1]),\n    Dropout(0.3),\n    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n    Dropout(0.3),\n    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n    Dropout(0.3),\n    Dense(1000, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n    Dropout(0.3),\n    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01)),\n])","830e37f5":"model_3.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nhistory3 = model_3.fit(X_tr, y_tr,\n          batch_size=32, epochs=50,\n          validation_data=(X_val, y_val))","afcde768":"# set up figure\nf = plt.figure(figsize=(12,5))\nf.add_subplot(1,2, 1)\n\n# plot accuracy as a function of epoch\nplt.plot(history3.history['accuracy'])\nplt.plot(history3.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\n\n# plot loss as a function of epoch\nf.add_subplot(1,2, 2)\nplt.plot(history3.history['loss'])\nplt.plot(history3.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['training', 'validation'], loc='best')\nplt.show(block=True)","b9615ae5":"## **Regression Models with Regularization**\n\nRegularization is a very useful method to handle collinearity, filter out noise from data, and eventually prevent overfitting. The concept behind regularization is to introduce additional information (bias) to penalize extreme parameter weights.","8cfc1cf8":"Standardization cannot be done before the partitioning and it cannot be done over one-hot encoded columns.","e48767ff":"## Preprocess\n\n**It is a good idea to plot the data. Skewed-distributions are not easy to work with, so transforming them with logarithm is a good solution.**","2831269d":"# **House Prices: Advanced Regression Techniques**\n\n**Predict sales prices and practice feature engineering, RFs, and gradient boosting**\n\nThis notebook is a fusion of https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models and https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset, but mostly the first one\n___","b8e50185":"**==============================================================================**\n## Neural Networks","719fad4f":"### **XGBoost**","747563c0":"## Output File","82d0be67":"The distribution of pricess is important. The predicted values should have a similar distribution as the values in the train sample.","048a9cea":"### **ElasticNet (L1 and L2 penalty)**\n\nElasticNet is a compromise between Ridge and Lasso regression. It has a L1 penalty to generate sparsity and a L2 penalty to overcome some of the limitations of Lasso, such as the number of variables (Lasso can't select more features than it has observations, but it's not the case here anyway). The result is a regularization with weighted L1 and L2.","07d7c8b6":"### **Lasso (L1 penalty)**\n\nL1 penalized models use the square of the weights by the sum of the absolute value of the weights. In contrast to L2 regularization, L1 regularization yields sparse feature vectors : most feature weights will be zero. Sparsity can be useful in practice if we have a high dimensional dataset with many features that are irrelevant.\n\nLasso has feature selection, which is nice.","638d085c":"### **Ridge** (L2 penalty)\n\nL2 penalized model are those who simply add the squared sum of the weights to our cost function.\n\nThe main tuning parameter for the Ridge model is alpha - a regularization parameter that measures how flexible our model is. The higher the regularization the less prone our model will be to overfit. However it will also lose flexibility and might not capture all of the signal in the data.\n\nIt is good to make a fine tunning of alpha after fitting the first time.\n"}}