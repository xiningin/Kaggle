{"cell_type":{"0bf568d1":"code","3ce080ff":"code","f80dc36a":"code","57e04cf6":"code","45da9eb2":"code","0a12e47b":"code","02f9fd7a":"code","3a171c8b":"code","de8dc056":"code","19e495be":"code","9a677f7f":"code","5b02a4ec":"code","e74ce8e5":"code","3e548d99":"code","6f8e71b3":"code","504583cf":"code","02b6ccb7":"code","49cbd0ad":"code","1013ed6c":"code","c47baa97":"code","ca193c40":"code","bea89992":"code","8deadf25":"code","9b04c5f2":"code","8071a0b0":"code","6c5caa80":"code","156c80c2":"code","e7800576":"code","7ad8c06d":"code","b6c67b9a":"code","ffad7cec":"code","68aa4404":"code","ea70bb54":"code","80fbf755":"code","eb494ddc":"code","29eb6dcc":"code","c79a0944":"code","d8ccab12":"code","8a97f3ea":"code","8b0cafc4":"code","df53b3a4":"code","f6ca43fe":"code","12d59c86":"code","927fd6d9":"code","09a45558":"code","15a3653a":"code","c9928ae8":"code","9a0a6aad":"code","4526d5dc":"markdown","aa80042f":"markdown","f4140633":"markdown","d11387f7":"markdown","9366bcdc":"markdown","cf73c78c":"markdown","b831dfcb":"markdown","5a8881fd":"markdown","8b7b59b1":"markdown","8bef1b52":"markdown","a92343a6":"markdown","6b9514e6":"markdown","3eeddc74":"markdown","0fa0b1c1":"markdown","bcb09e18":"markdown","071e1f2a":"markdown","51c2fe55":"markdown","cd41ccdc":"markdown","6dfd2aa2":"markdown","df8a2a6f":"markdown","1589614b":"markdown","3ffff4fb":"markdown","0f600be8":"markdown","7e7af94b":"markdown","8c352df9":"markdown","1e5a7237":"markdown","890ff444":"markdown","c1da2c80":"markdown","db492517":"markdown","bee18630":"markdown","860578a9":"markdown","12749fc2":"markdown","6578c05e":"markdown","0deb813f":"markdown","1e479c3e":"markdown","2da4431b":"markdown","e8b911eb":"markdown","de21966b":"markdown","5785f561":"markdown","70deeafe":"markdown","dffced31":"markdown","172e9a42":"markdown","056dc939":"markdown","5dff2317":"markdown","36059a57":"markdown","573b9551":"markdown","9185b0e1":"markdown","1179f6e6":"markdown","9116e78e":"markdown","2929b307":"markdown","5e51ce75":"markdown","60828a2a":"markdown","5bcc966a":"markdown","86efb18f":"markdown","0ab7ae9a":"markdown","df52a533":"markdown","4502b9e3":"markdown","f7463a76":"markdown"},"source":{"0bf568d1":"!pip install torchdata\n!pip install imblearn","3ce080ff":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as image\n\nfrom pprint import pprint\n\n%matplotlib inline","f80dc36a":"# below variables are established to be used throughout the notebook without being subjected to any change\nBATCH_SIZE     = 64\nHEIGHT = WIDTH = 48\nCHANNELS       =  3\n\nEMOTIONS       = ['surprise', 'fear', 'angry', 'neutral', 'sad', 'disgust', 'happy']","57e04cf6":"import os\nimport re\n\nimport numpy as np\nimport pandas as pd\n\nimport cv2\nimport PIL.Image as Image\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import TensorDataset\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\n\nfrom torchvision import transforms","45da9eb2":"paths   = []\nlabels  = []\n\nbase = f\"..\/input\/fer2013\/\"\n\nif os.path.exists(path=base) and os.path.isdir(base):\n    for level_1_dir in os.listdir(path=base): # train\/test\n        level_1_path = f\"{level_1_dir}\"\n        if os.path.exists(path=base + level_1_path) and os.path.isdir(base + level_1_path):\n            for level_2_dir in os.listdir(path=base + level_1_path): # 7 emotions\n                level_2_path =f\"{level_1_path}\/{level_2_dir}\"\n                if os.path.exists(path=base + level_2_path) and os.path.isdir(base + level_2_path):\n                    for file in os.listdir(path=base + level_2_path): # files\n                        file_path = f\"{level_2_path}\/{file}\"\n                        if os.path.isfile(base + file_path):\n                            paths.append(file_path)\n                            labels.append(level_2_dir)\n\ndf = pd.DataFrame({'path': [base + path for path in paths], 'label': labels})","0a12e47b":"# create a new column to store the images as ndarray\ndf['img_as_matrix'] = df['path'].apply(lambda path: cv2.imread(path))\n\n# view firts 5 rows of the newly created dataframe\ndf.head()","02f9fd7a":"# set plotting options\nfig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, _)) = plt.subplots(2, 4, figsize=(10, 5))\n_.set_visible(False)\n\nfor index, label in enumerate(df['label'].unique()):\n    img = df[df['label'] == label]['img_as_matrix'].iloc[0]\n    \n    exec(f\"ax{index + 1}.imshow(img)\")\n    exec(f\"ax{index + 1}.set_title(label.title())\")\n    \nfig.tight_layout()","3a171c8b":"# setplotting options\nfig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, _)) = plt.subplots(2, 4, figsize=(10, 5))\n_.set_visible(False)\n\nfor index, label in enumerate(df['label'].unique()):\n    avg_img = np.stack(df[df['label'] == label]['img_as_matrix']).mean(axis=0).astype(np.uint8)\n    \n    exec(f\"ax{index + 1}.imshow(avg_img)\")\n    exec(f\"ax{index + 1}.set_title(label.title())\")\n    \nfig.tight_layout()","de8dc056":"# set plotting options - equalized images for more transparent view\nfig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, _)) = plt.subplots(2, 4, figsize=(10, 5))\n_.set_visible(False)\n\nfor index, label in enumerate(df['label'].unique()):\n    avg_img   = np.stack(df[df['label'] == label]['img_as_matrix']).mean(axis=0).astype(np.uint8)\n    \n    R, G, B   = cv2.split(avg_img)\n\n    output1_R = cv2.equalizeHist(R)\n    output1_G = cv2.equalizeHist(G)\n    output1_B = cv2.equalizeHist(B)\n\n    equ = cv2.merge((output1_R, output1_G, output1_B))\n    \n    exec(f\"ax{index + 1}.imshow(equ)\")\n    exec(f\"ax{index + 1}.set_title(label.title())\")\n    \nfig.tight_layout()","19e495be":"plt.title('Value Counts per Emotion')\nplt.barh(y = df.label.value_counts().index, width = df.label.value_counts().values)","9a677f7f":"fig, ((ax1, ax2, ax3, ax4), (ax5, ax6, ax7, ax8)) = plt.subplots(2, 4, figsize=(10, 5))\n\nfor index, img in enumerate(df['img_as_matrix']):\n    # calculate mean value from RGB channels and flatten to 1D array\n    img_1_chan = img.mean(axis=2).flatten().astype(np.uint8)\n    \n    # gett same \n    equ = cv2.equalizeHist(img_1_chan)\n    \n    exec(f\"ax{index + 1}.hist(img_1_chan, bins=100, range=(0, 255))\")\n    exec(f\"ax{index + 1}.hist(equ, bins=100, range=(0, 255))\")\n    \n    if index >= 7:\n        break","5b02a4ec":"# store the average image per emotion\naverage_image_per_label = {}\n\n# compute the average image per label\nfor index, label in enumerate(df['label'].unique()):\n    avg_img = np.stack(df[df['label'] == label]['img_as_matrix']).mean(axis=0).astype(np.int8)\n    \n    # store the result\n    average_image_per_label[label] = avg_img","e74ce8e5":"# store deltas between each image and its label's average\ndeltas = []\n\nfor i in range(len(df)):\n    label = df.loc[i, 'label']\n    # find the delta between each image and its label's average\n    delta = np.absolute(average_image_per_label[label] - df.loc[i, 'img_as_matrix'].astype(np.int8))\n    deltas.append(delta)\n    \ndf['delta_vs_avg_image'] = deltas","3e548d99":"# find the mean error across all channels\ndf['mean_error'] = df['delta_vs_avg_image'].apply(lambda x: x.mean())","6f8e71b3":"# find an average mean error per group\nmean_errors_per_label = df.groupby('label')['mean_error'].mean()\n\n# store standard deviations\nerror_stds = []\n\nfor i in range(len(df)):\n    label = df.loc[i, 'label']\n    # compute variance between mean error of each image and average error for label\n    var = abs(df.loc[i, 'mean_error'] - mean_errors_per_label[label])**2\n    # compute standard deviation from variance\n    std = np.sqrt(var)\n    # store the result\n    error_stds.append(std)\n    \ndf['error_std'] = error_stds","504583cf":"df.head()","02b6ccb7":"# compute standard deviation boundaries\nmin_std = df['error_std'].min()\nmax_std = df['error_std'].max()\n\n\n# treat the image as anomalous if its mean error is below or above standard deviation boundaries.\ndef get_anomalous_point(mean_error: float, min_std: float, max_std: float): return mean_error < min_std or mean_error > max_std\n    \n# mark images as anomalous or not\ndf['check_for_anomalies'] = df['mean_error'].apply(lambda err: get_anomalous_point(err, min_std, max_std))","49cbd0ad":"# retrieve indices of the anomalous images to drop\nimage_indices_to_drop = list(df[df['check_for_anomalies'] == True].index)\n\n# drop anomalous image indices,reset index, and update dataframe\ndf = df.drop(image_indices_to_drop).reset_index(drop=True)","1013ed6c":"# print planned to be dropped indices\nprint(f\"How many images gets dropped: {len(image_indices_to_drop)}\\nSome samples: {image_indices_to_drop[:20]}\\n\")\n\nprint(\"Printing first 5 rows from dataframe\")\ndf.head()","c47baa97":"# view total number of records in dataset\nprint(f'Length of original dataset is: {len(df)}')\n\n# decrease the dataset to %N samples per each category to solve the memory allocation issue\nN_percent       = 0.3\n\n# aggregating storage dataframe\ndf_agg          = pd.DataFrame(data=[], columns=df.columns)\n\n# per each label, select the first %N samples and append to the aggregating dataframe\nfor lbl in np.unique(labels):\n    len_to_keep = int(N_percent*len(df[df.label == lbl]))\n    subset_df   = df[df.label == lbl][:len_to_keep]\n    df_agg      = pd.concat([df_agg, subset_df])\n\n# reset index and drop the resulting index column\ndf_agg          = df_agg.reset_index().drop('index', axis=1)\n\n# view total number of records in the reduced dataset\nprint(f'Length of reduced dataset is: {len(df_agg)}')","ca193c40":"# view the resulting df\ndf_agg.head()","bea89992":"# initialize the SMOTE model\nsmote = SMOTE(random_state=62)\n\n# assign labels to y\ny = df_agg['label']\n\n# X is a list of read-in images\nX = np.stack(df_agg['img_as_matrix'])\n\n# get all dimensions of the resulting X\nn_samples, height, width, n_channels = [X.shape[index] for index in range(4)]\n\nprint(f\"Shape of X before reshape: {X.shape}\")\n\n# reshape X because SMOTE accepts only (n_samples, n_channels*height*weight)-type data\nX_reshaped = X.reshape(n_samples, n_channels*height*width)\n\nprint(f\"Shape of X before reshape: {X.shape}\")","8deadf25":"# perform re-sampling on modified X given y\nX_smote, y_smote = smote.fit_resample(X_reshaped, y)","9b04c5f2":"print(f'Before re-sampling, the amount of images:  {len(X):6}')\nprint(f'After re-sampling, the amount of images:{len(X_smote):10}')","8071a0b0":"# view the resulting balanced data(modified distribution)\nplt.title('Value Counts per Emotion After Re-sampling')\nplt.barh(y = y_smote.value_counts().index, width = y_smote.value_counts().values)","6c5caa80":"# initializing label encoder\nlabel_encoder = preprocessing.LabelEncoder()\n\n# convert X and y to Tensors\nX_smote = torch.Tensor(X_smote) # features\ny_smote = label_encoder.fit_transform(y_smote) # targets\ntargets = torch.as_tensor(y_smote)\n\n# split data into train, valid and test\nX_train, X_trial, y_train, y_trial = train_test_split(X_smote, targets, test_size=0.20, random_state=42)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.20, random_state=42)","156c80c2":"# print initial shape of the datasets\nprint(f\"Initial shape of the X datasets along with y:\\n\\\n    {X_train.shape}, {y_train.shape}\\n\\\n    {X_valid.shape}, {y_valid.shape}\\n\\\n    {X_trial.shape}, {y_trial.shape}\\n\")\n\n# reshape to remove the requirements of SMOTE that do not suit standard model training\nX_train = X_train.reshape((X_train.shape[0], 3, HEIGHT, WIDTH))\nX_valid = X_valid.reshape((X_valid.shape[0], 3, HEIGHT, WIDTH))\nX_trial = X_trial.reshape((X_trial.shape[0], 3, HEIGHT, WIDTH))\n\n# print latest shape of the datasets\nprint(f\"Latest shape of the X datasets along with y:\\n\\\n    {X_train.shape}, {y_train.shape}\\n\\\n    {X_valid.shape}, {y_valid.shape}\\n\\\n    {X_trial.shape}, {y_trial.shape}\\n\")","e7800576":"# create train, test and validation datasets for the data loader\ntrain_ds = TensorDataset(X_train,y_train)\nvalid_ds = TensorDataset(X_valid,y_valid)\ntrial_ds = TensorDataset(X_trial,y_trial)","7ad8c06d":"# add data transformations: (1) convert to tensor format, and (2) normalize the images\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(degrees=180),\n])\n\n# apply transformations to the train set\ntrain_ds.transform = transform\n\n# apply transformations to the validation set\nvalid_ds.transform = transform\n\n# apply transformations to the test set\ntrial_ds.transform = transform\n\n# create a dataloader for each subset of data\ntrainloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\nvalidloader = DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\ntrialloader = DataLoader(trial_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)","b6c67b9a":"# count number of times a color appears\ncolor_counts = {}\n\nfor img in df_agg.loc[:, 'img_as_matrix']:\n    # flatten the image into 1 channel\n    img_1_channel = img.mean(axis=2).flatten().astype(np.uint8)\n    # count each color occurrence\n    for color in img_1_channel:\n        try:\n            color_counts[color] += 1\n        except:\n            color_counts[color] = 1","ffad7cec":"# view pixel distribution across the original dataset\nplt.bar(color_counts.keys(), color_counts.values(), color='b')","68aa4404":"import torchvision\nimport torchvision.transforms as transforms\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n\nimport torch\n\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization","ea70bb54":"def build_classifier(optimizer=\"adam\"):\n    \"\"\"\n    The model is based on 4 convolutional layers.\"\"\"\n    cnn4 = Sequential()\n    cnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 3)))\n    cnn4.add(BatchNormalization())\n\n    cnn4.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n    cnn4.add(BatchNormalization())\n    cnn4.add(MaxPooling2D(pool_size=(2, 2)))\n    cnn4.add(Dropout(0.25))\n\n    cnn4.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n    cnn4.add(BatchNormalization())\n    cnn4.add(Dropout(0.25))\n\n    cnn4.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n    cnn4.add(BatchNormalization())\n    cnn4.add(MaxPooling2D(pool_size=(2, 2)))\n    cnn4.add(Dropout(0.25))\n\n    cnn4.add(Flatten())\n\n    cnn4.add(Dense(512, activation='relu'))\n    cnn4.add(BatchNormalization())\n    cnn4.add(Dropout(0.5))\n\n    cnn4.add(Dense(128, activation='relu'))\n    cnn4.add(BatchNormalization())\n    cnn4.add(Dropout(0.5))\n\n    cnn4.add(Dense(len(EMOTIONS), activation='softmax'))\n\n    cnn4.compile(loss='categorical_crossentropy',\n                 optimizer=optimizer,\n                 metrics=[\"accuracy\"])\n    \n    return cnn4\n\n# establish estimator\nestimator = KerasClassifier(build_fn=build_classifier, verbose=0)","80fbf755":"# choose the hyperparameter values to try\n# reference: https:\/\/machinelearningmastery.com\/grid-search-hyperparameters-deep-learning-models-python-keras\/\nparams = {\n    \"batch_size\" : [25, 32, 64],        # this is for measuring the impact of batch size on loss\n    \"epochs\"     : [1,   2,  3],        # how deep do we want to go\n    \"optimizer\"  : [\"adam\", \"rmsprop\"], # choose between different optimizers to test convergence\n}","eb494ddc":"# ref: https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring\n# ref: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\ngrid = GridSearchCV(estimator=estimator, param_grid=params, cv=5, scoring=\"accuracy\")","29eb6dcc":"# fit the grid on our train data and print the best scores\n# ISSUE: https:\/\/stackoverflow.com\/questions\/53806892\/i-cant-add-optimizer-parameter-in-gridsearch\ngrid.fit(X_train, y_train)\nprint(\"Best: %f using %s\" % (grid.best_score_, grid.best_params_))","c79a0944":"# compute predictions on the holdout\nyhat_probs = grid.predict(X_trial, verbose=0)","d8ccab12":"# evaluate how well the model did on the holdout set\ngrid.evaluate(X_trial, y_trial, verbose=0)","8a97f3ea":"# predict crisp classes for holdout set\nyhat_classes = model.predict_classes(X_trial, verbose=0)\n\n# calculate precision score\nprecision_score(y_trial, yhat_classes)\n\n# calculate recall score\nrecall_score(y_trial, yhat_classes)","8b0cafc4":"# compute the confusion matrix\nmetrics.confusion_matrix(y_true, y_pred)","df53b3a4":"# instantiate Resnet50 model\nmodel = torchvision.models.resnet50(pretrained=True)\n\n# replace the last fully connected layer to suit the classification problem\nmodel.fc = torch.nn.Sequential(\n    torch.nn.Linear(\n        in_features  = 2048,\n        out_features = len(np.unique(df_agg['label']))\n    ),\n    torch.nn.Softmax(dim=1)\n)\n\n# view the model structure \nmodel","f6ca43fe":"# freeze \/ unfreeze all layers by toggling the below\n# setting to True because we want to use pre-trained weights\nfor param in model.parameters():\n    param.requires_grad = True\n    \n# freeze \/ unfreeze just the fully connected layer\nfor name, param in model.named_parameters():\n    if name.startswith('fc'):\n        param.requires_grad = True","12d59c86":"# update only those parameters that are unfrozen and store them in a list\nparams_to_update = []\n\nfor param in model.parameters():\n    if param.requires_grad == True:\n        params_to_update.append(param)\n\n# instantiate an SGD optimizer for the chosen parameters\noptimizer = optim.SGD(params_to_update, lr=0.1)","927fd6d9":"# negative log likelihood useful for multiclass problems\nloss_criterion = nn.NLLLoss()","09a45558":"# choose the device to train on\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# send the model to chosen device\nmodel.to(device);","15a3653a":"def compute_running_loss(loader, model):\n    running_loss = 0\n    \n    for images, labels in loader: # processing one batch at a time\n        \n        # send the batch inputs and targets to the selected device\n        images      = images.to(device)\n        labels      = labels.to(device)\n        \n        predictions = model(images) # predict labels\n        \n        loss        = loss_criterion(predictions, labels) # calculate the loss\n        \n        # BACK PROPAGATION OF LOSS to generate updated weights\n        optimizer.zero_grad() # pytorch accumulates gradients from previous backwards\n                              # passes by default -- we want to zero them out;\n                              # you can read online why they have this implementation choice\n                \n        loss.backward()       # compute gradients by using the predictions' grad_fn\n                              # that was passed to loss_criterion() above -- this is how it\n                              # knows what model parameters need updating eventually\n                              # (this is confusing IMO and not obvious to those used to OOP)\n                    \n        optimizer.step()      # using gradients just calculated for model parameters, \n                              # update the weights via the optimizer (which was init with those \n                              # model parameters)\n        \n        running_loss += loss.item()\n    \n    return running_loss","c9928ae8":"def calculate_and_print_accuracy_score(name, loader):\n    # Calculate accuracy score on loader\n    \"\"\"\n    The below code is written to answer the below questions:\n    - Feed in the entire test dataset to the model, to make predictions? \n    - Could you write code to do this, and measure your model performance?\n    \"\"\"\n    results = []\n\n    for tensor_image_batch in iter(loader):\n        tensor_images, labels = tensor_image_batch\n        \n        # send inputs and targets to device\n        tensor_images         = tensor_images.to(device)\n        labels                = labels.to(device)\n        \n        for tensor_image, label in zip(tensor_images, labels):\n            prediction        = model(tensor_image.unsqueeze(0).cuda()).detach().cpu().numpy()\n            max_prob          = max(list(np.array(prediction)[0]))\n            predicted_label   = list(prediction[0]).index(max_prob)\n\n            results.append(label == predicted_label)\n\n    print(f\"{name.capitalize()} accuracy score: {sum(results)\/len(results) * 100}\")","9a0a6aad":"# storing all running losses\ntrain_losses    = []\nvalidate_losses = []\n\n# Train the network here\nepochs = range(1, 1 + 1)\n\nfor epoch in epochs:\n    print(\"In epoch\", epoch)\n\n    model.train()\n    train_average_running_loss = compute_running_loss(trainloader, model)\/len(trainloader)\n    train_losses.append(train_average_running_loss)\n    print(f\"Train loss: {train_average_running_loss}\")\n    calculate_and_print_accuracy_score('train', trainloader)\n\n    model.eval()\n    validate_average_running_loss = compute_running_loss(validloader, model)\/len(validloader)\n    validate_losses.append(validate_average_running_loss)\n    print(f\"Validation loss: {validate_average_running_loss}\")\n    calculate_and_print_accuracy_score('validation', validloader)\n\n    print()","4526d5dc":"**Task 3:** Correct explanation generalization from such a holdout split.\n> For the train \/ test split the following percentages were chosen: 80%-20%. Of the 80% coming from the train set, 20% is the validation set. Since the data is already balanced, the choice of a random split is a reasonable one.","aa80042f":"**Task 13**: Separate your training data into features and labels.","f4140633":"# Emotion Recognition Using Deep Learning","d11387f7":"**Task 19:** Calculate accuracy, precision and recall on the holdout dataset. Discuss which metric you think is most meaningful for this dataset, and why\n\n> *Precision* and *Recall*, both are very important in the content. But in any case if you desire to have high precision then you sacrifice recall and vice versa. In the particular usage, precision is slightly over weigting since having any false negative does not have a high cost contrast to sick patients example. However, in the particular dataset, we are looking for getting as much correct answer as possible and therefore, *Accuracy* is the correct metric to go with.","9366bcdc":"**Task 52:** Take a look at each of the items in all classes individually. What aspects of the item (such as backgrounds) might be influencing the decision-making of the model, besides the salient parts themselves?.","cf73c78c":"**Task 33:** Choose and instantiate an optimizer. Discuss your choice.\n\n>`SGD` (Stochastic Gradient Descent) was chosen as an optimizer to enable faster training since it approximates true gradient descent by considering only random samples of the dataset, instead of the whole. Also, since our dataset is large, SGD can converge faster than other types of optimizers.","b831dfcb":"**Task 17:** Set up a gridsearchCV with 5-fold cross validation (scikit-learn) or equivalent in PyTorch. Discuss what accuracy metric you chose and why.\n    \n    In our task our main desire it to find the image labeling correct most of the time, but if we fail it is not  critical. Therefore, most of the time the accuracy is a write metric.\n> 1. F1 Score would be suitable sfor the case, since it tries to find the balance between precision and recall.\n2. Recall can also be suitable for the case, since it answers the question \"how many relevant items has been choosen?\"","5a8881fd":"**Task 20:** Discuss how the model performance on holdout compares to the model performance during training. Do you think your model will generalize well? Why or why not?","8b7b59b1":"**Task 14:** Discuss and implement how you will handle any dataset imbalance.\n\nThere are a number of ways to deal with data imbalance:\n> 1. *Over-sampling the under-represented class*. However, given the big discrepancy in the counts between such classes as `surprise` and `happy`, this method might cause the model to overfit.\n2. *Under-sampling the over-represented class*. However, this can result in a loss of big amounts of useful data.\n3. ***SMOTE (Synthetic Minority Oversampling Technique)***. This method selects examples of data in the feature space (a data point and its nearest neighbor), and then creates a synthetic data point in-between them. In essence, this is an augmentation technique to add varying copies of existing data. We chose to go with this option since we do not want to lose data and do not want to experience the effects of over-fitting.","8bef1b52":"### Notes\n1. For visualization purposes **test** key word has been replaced with **trial** in variable naming to keep character numbers equal to `5`","a92343a6":"**Task 49:** Generate a dataset of just three items, one for each class, and show your model correctly labels them. (display each item in your notebook, pass it to your model, and then print the prediction).","6b9514e6":"**Task 6 and 7:** Handle any missing data. For imagery datasets, discuss what images you might drop and why. The holdout dataset also contains missing data\/bad images. Discuss how you handled this in your holdout.\n> 1. There are images that are far off from the way an average image looks like in an emotion category. To solve this, we find an average image per emotion class, compute the mean error for each image and find whether the mean error is within standard deviation limits. If the mean error is below or above the standard deviation boundaries, then we consider the image as anomalous and drop it.\n2. We perform this for the whole dataset, before splitting it into train \/ holdout.\n3. In the future, we plan to apply augmentations to images to increase their quality \/ contrast. For example, adding salt and pepper noise to the train dataset will help to train the model to predict more or less accurately on the bad holdout images.","3eeddc74":"**Task 46:** Make a list of reasons why your model may have under-performed.","0fa0b1c1":"**Task 32:** Did you use batch normalization in the step above? Why or why not?","bcb09e18":"**Task 51:** Generate a dataset from your original dataset where 20% of the classes in one class are mis-labelled as the remaining two classes. How do you think your model performance will be impacted? Re-train your model on this test dataset, and discuss your results.","071e1f2a":"### Import Libraries","51c2fe55":"**Task 3:** For imagery datasets, provide the \"average image\" for each class.\n\n\n> In this section, an average image per emotion class is displayed. The average image is computed by adding together the array representations of each image in the emotion sub-folder and dividing that sum by the total number of images in the sub-folder to find the mean.\n\n*The resulting average images per emotion class are then displayed.*","cd41ccdc":"# General Setups\n### Install Libraries","6dfd2aa2":"The dataloaders are set up in the cell above for all three data subsets: train, validation and test. `BATCH_SIZE` was chosen to be 64, which is rather small, however, it means the model can start learning better since the number of parameters it needs to update per epoch is now higher. `shuffle` is set to `True` so that at each epoch the data gets re-shuffled which helps avoid under- or over-fitting. `drop_last` is set to `True` to avoid cases where a batch contains just one sample image.","df8a2a6f":"**Task 57:** Discussion of saliency mapping results.","1589614b":"**Task 9**: If you are using images\/text, discuss whether you are performing classification or regression on your dataset and why (instead of the other one).\n\n> We are performing a classification on our dataset and the reason is that we have non-continuous values as our y-labels (in this instance, emotion classes). Regression could have been used if we were trying to predict a continuous value but here the prediction target is discrete.","3ffff4fb":"### Set Up Constants","0f600be8":"# MILESTONE - II\n### Import Libraries","7e7af94b":"**Task 26:** Correctly set up DataLoaders for the three folders (train, validation, holdout). Discuss what options you chose for these loaders, and why (including batch size, shuffling, and dropping last).","8c352df9":"**Task 45:** Graph training versus validation loss using matplotlib.pyplot (or other). Was your model overfitting, underfitting, or neither?","1e5a7237":"**Task 11**: For imagery\/text: Show a histogram of the distribution of pixels or word embeddings across your dataset.","890ff444":"**Task 1:** Load data correctly and show contents in a cell.\n\n> 1. Database is already divided into test and train in the emotion named folders but it is required to have label data to start working. While doing this, all the data has been combined under unique set which will be seperated later according to the **20\/80** rule.\n2. A sample of each emotion is illustrated by looping through each emotion sub-folder and looking up the first image to plot as an example. Each example image is annotated with the emotion label assigned to it.","c1da2c80":"**Task 31:** Did you use dropout in the step above? Why or why not?","db492517":"**Task 34:** Choose and instantiate a loss function. Discuss your choice.\n> `Negative log-likelihood` is used as the loss criterion, due to the reason that we're dealing with a multi-class problem + it helps with handling small probabilities by the computer.","bee18630":"**Task 24:** Define a list of image transformations to be used during training, passing them to transforms.Compose(). Discuss why you think these transformations might help.","860578a9":"**Task 10:** Give an example of an ordinal feature that you've seen used by others, when it should have been treated as a categorical.\n\n> Example: `color` feature (red, green, blue). People might assume there's a color order, but in fact this is a categorical feature, since there's no particular ordering (and if there is, it's a rather complex one). We can treat different pixel intensities within `green` as an ordinal feature, but not when it comes to differing colors such as `green` vs `red`.","12749fc2":"**Task 25:** Repeat the step above for test and validation transformations.","6578c05e":"**Task 48:** Graph training versus validation accuracy using matplotlib.pyplot (or other). Score your model on its predictions on the holdout. Discuss why you think your results will or will not generalize.","0deb813f":"**Task 47:** Make a list of ways you could improve your model performance (you don't have to implement these unless you wan to).","1e479c3e":"**Task 9**: If you are using images\/text, discuss whether you are performing classification or regression on your dataset and why (instead of the other one).\n\n> We are performing a classification on our dataset and the reason is that we have non-continuous values as our y-labels (in this instance, emotion classes). Regression could have been used if we were trying to predict a continuous value but here the prediction target is discrete.","2da4431b":"**Task 27:** Instantiate any pre-trained model. Discuss why you chose it amongst the others.\n\nA ResNet50 model was chosen for training due to the following reasons:\n1. it does not sacrifice network depth for the sake of reducing the vanishing gradient problem, and in fact\n2. it solves the vanishing gradient problem with \"shortcut skip connections\" - identity maps (residual blocks). \n  \n*Note that the last fully connected layer is replaced to suit our classification problem, i.e. the number of labels. `Softmax` activation function is used because we are dealing with a multi-class problem.*\n\n*Dropout* was not used in the last fully connected layer because we want the model to learn the features as intricately as possible, and if we drop a layer, our training loss can end up being higher and lead to underfitting. Also, batch normalization is used throughout the model structure which already regularizes our model, so adding a dropout will double-penalize the training.\n\n*BatchNormalization* is usually used between a convolutional layer and an activation function, for regularization purposes, to make sure that an input is distributed around approximately the same mean and standard deviation amongst layers. For the purpose of regularization (i.e. to make sure the model does not overfit), it's already quite prevalent in the standard resnet50 model, therefore, we don't see a need to add it in at this stage.","e8b911eb":"**Task 16:** Define a grid to tune at least three different hyperparameters with at least two different values each. Discuss why you think these parameter values might be useful for this dataset.","de21966b":"**Task 23:** Next, repeat training and tuning on the same data with a third model, dissimilar from the other two. Do you need to do any additional feature cleaning or scaling here? Why or why not?","5785f561":"**Task 18:** Train your model using grid search, and report the best performing hyperparameters.","70deeafe":"# MILESTONE - I\n### Import Libraries","dffced31":"**Task 54:** If you noted some potential biases in the modeling\/dataset above, discuss how you could help mitigate these biases (you don't need to implement, just discuss). If you didn't note any biases in this dataset, discuss what biases there could have been, and how the dataset designers might have helped mitigate them.","172e9a42":"**Task 8:** Discuss (and implement if applicable) whether or not you need to scale\/normalize your features, and which ones, if any.\n\n> 1. Scaling is important to ensure that all images are of the same size and hence, can be accepted by the model. Our image dataset is already well-scaled, with a fixed height and weight parameters as defined at the beginning of this notebook.\n2. Normalization in our image dataset is needed so that each image has similar pixel distribution. This way we reduce the skewness in our dataset.\n\nWe will use `torchvision.transforms` to apply scaling\/normalization techniques.\n\n> 1. `.ToTensor()`: converts an image from array to tensor format and adjusts pixel intensities to be between 0 and 1. This is essentially a pixel normalization technique.   \n2. `.Normalize()`: normalizes the image to be between -1 and 1 (given the provided 0.5 input parameter values). This helps reduce skewness and hence, contributes to faster training.   \n3. `.RandomHorizontalFlip()` and `.RandomRotation()`: this creates synthetic augmented images which can help the model make correct predictions regardless of image position. ","056dc939":"**Task 15:** Instantiate a model of your choosing.","5dff2317":"**Task 50:** Generate three datasets of our inputs, where each has only two of the classes. What do you predict the performance should be for three binary classifiers trained on these three datasets? Re-train your model on these three datasets, and discuss your results.","36059a57":"**Task 28:** Write code to freeze\/unfreeze the pretrained model layers.","573b9551":"**Task 21:** Generate a confusion matrix and discuss your results.\n> In confusion matrix the desired output is to have all zero except diagonal.","9185b0e1":"**Task 29:** Replace the head of the model with sequential layer(s) to predict our three classes.","1179f6e6":"**Task 55:** Correctly train your model without pre-training (and discussion how this affects performance)","9116e78e":"**Tasks Combined: 36, 37, 38, 39, 40, 41, 42, 43, 44**\n1. Correctly set up your model to train over 20 epochs.\n2. Correctly set up your model to use your batches for training.\n3. Correctly make predictions with your model (the predictions can be wrong).\n4. Correctly call your loss function and back-propagate its results.\n5. Use the optimizer correctly to update weights\/gradients.\n6. Correctly record training losses for each epoch.\n7. Correctly set up validation at each epoch.\n8. Correctly record validation losses for each epoch.\n9. Correctly record training and validation accuracies for each epoch\n**Note:** *The occurence of the task description is relative to the about index sequence.*","2929b307":"**Group Members:**\n\n- Aydin Bagiyev (abagiyev@gwu.edu)\n- Narmin Jamalova (Aydin's awesome!!! lovely girlfriend! njamalova54@gwu.edu)","5e51ce75":"**Task 2:** Holdout dataset split as specified.","60828a2a":"**Task 53:** Is the data biased in any way that could impact your results? Why or why not?","5bcc966a":"**Task 22:** Train and tune another type of model on your training dataset. Using the best performing hyperparameters, test this model on your holdout. How did it perform, compared to your earlier model? Do you think your results will generalize?","86efb18f":"**Task 5:** Discussion of how the dataset distribution can\/will affect your modeling.\n\n> Depending on how over- or under-represented some emotions are, the model can learn a skewed pattern and hence, not generalize well enough when it comes to test data. For instance, in the cell below, we're plotting the number of images per emotion class to understand the underlying distribution. As can be observed, the representation of different classes is dramatically different from others: e.g., `disgust` is under-represented, while `happy` is over-represented. This means that our model can be skewed in its predictions due to such discrepancies and hence, imbalance will have to be handled.","0ab7ae9a":"**Task 12**: What customized dataset augmentation you would use (not required to implement) for images?\n\n> 1. improving the contrast on the images (adaptive equalization)\n2. applying flipping, rotations to stabilize model performance regardless of angle\n3. fill in the gaps on the image to the nearest pixel","df52a533":"**Task 56:** Correctly implement saliency maps for all images. If doing NLP, discuss feature importances or other metric.","4502b9e3":"**Task 30:** What activation function did you use in the step above? Why?","f7463a76":"**Task 35:** Write code that places the model on the GPU, if it exists, otherwise using the CPU."}}