{"cell_type":{"b1912dcb":"code","d3299e33":"code","aef916f5":"code","b0662918":"code","5931f636":"code","b52d9c47":"code","f1d4de1a":"code","5be5fcf8":"code","8d8515dc":"code","16807193":"code","dea2709c":"code","fdd62ede":"code","04f4e840":"code","cd82b6de":"code","bcfbc57d":"code","966dfb2c":"code","3181046c":"code","5dc84c75":"code","69f0b514":"code","9a82432d":"code","57ce25bd":"markdown","33dfafc0":"markdown","e1ade655":"markdown","a9bbbb2f":"markdown","b11a60be":"markdown","7c5b6bcb":"markdown","bb166478":"markdown","68d2c9bd":"markdown","2a247169":"markdown","98f9a576":"markdown","9ffe0358":"markdown","da0121cb":"markdown","bbb8a018":"markdown","1b317132":"markdown","7939c2fc":"markdown","ed8d621c":"markdown","d8ff2c0d":"markdown","1bc06c4b":"markdown","a8dbb5b0":"markdown"},"source":{"b1912dcb":"import numpy as np\nimport pandas as pd \nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d3299e33":"from cycler import cycler\n\n\nraw_light_palette = [\n    (0, 122, 255), # Blue\n    (255, 149, 0), # Orange\n    (52, 199, 89), # Green\n    (255, 59, 48), # Red\n    (175, 82, 222),# Purple\n    (255, 45, 85), # Pink\n    (88, 86, 214), # Indigo\n    (90, 200, 250),# Teal\n    (255, 204, 0)  # Yellow\n]\n\nraw_dark_palette = [\n    (10, 132, 255), # Blue\n    (255, 159, 10), # Orange\n    (48, 209, 88),  # Green\n    (255, 69, 58),  # Red\n    (191, 90, 242), # Purple\n    (94, 92, 230),  # Indigo\n    (255, 55, 95),  # Pink\n    (100, 210, 255),# Teal\n    (255, 214, 10)  # Yellow\n]\n\nraw_gray_light_palette = [\n    (142, 142, 147),# Gray\n    (174, 174, 178),# Gray (2)\n    (199, 199, 204),# Gray (3)\n    (209, 209, 214),# Gray (4)\n    (229, 229, 234),# Gray (5)\n    (242, 242, 247),# Gray (6)\n]\n\nraw_gray_dark_palette = [\n    (142, 142, 147),# Gray\n    (99, 99, 102),  # Gray (2)\n    (72, 72, 74),   # Gray (3)\n    (58, 58, 60),   # Gray (4)\n    (44, 44, 46),   # Gray (5)\n    (28, 28, 39),   # Gray (6)\n]\n\n\nlight_palette = np.array(raw_light_palette)\/255\ndark_palette = np.array(raw_dark_palette)\/255\ngray_light_palette = np.array(raw_gray_light_palette)\/255\ngray_dark_palette = np.array(raw_gray_dark_palette)\/255\n\nmpl.rcParams['axes.prop_cycle'] = cycler('color',dark_palette)\nmpl.rcParams['figure.facecolor']  = gray_dark_palette[-2]\nmpl.rcParams['figure.edgecolor']  = gray_dark_palette[-2]\nmpl.rcParams['axes.facecolor'] =  gray_dark_palette[-2]\n\nwhite_color = gray_light_palette[-2]\nmpl.rcParams['text.color'] = white_color\nmpl.rcParams['axes.labelcolor'] = white_color\nmpl.rcParams['axes.edgecolor'] = white_color\nmpl.rcParams['xtick.color'] = white_color\nmpl.rcParams['ytick.color'] = white_color\n\nmpl.rcParams['figure.dpi'] = 200\n\nmpl.rcParams['axes.spines.top'] = False\nmpl.rcParams['axes.spines.right'] = False","aef916f5":"sns.palplot(dark_palette)","b0662918":"train = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-may-2021\/sample_submission.csv')","5931f636":"print(train.shape)\nprint(test.shape)","b52d9c47":"train.head()","f1d4de1a":"train = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)","5be5fcf8":"fig, ax = plt.subplots()\nsns.countplot(x='target', data=train, order=sorted(train['target'].unique()), ax=ax)\nax.set_ylim(0, 63000)\nax.set_title('Target Distribution', weight='bold')\nplt.show()","8d8515dc":"train.describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","16807193":"test.describe().T.style.bar(subset=['mean'], color='#205ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')","dea2709c":"def diff_color(x):\n    color = 'red' if x<0 else ('green' if x > 0 else 'black')\n    return f'color: {color}'\n\n(train.describe() - test.describe())[test.columns].T.iloc[:,1:].style\\\n        .bar(subset=['mean', 'std'], align='mid', color=['#d65f5f', '#5fba7d'])\\\n        .applymap(diff_color, subset=['min', 'max'])","fdd62ede":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n\ny = np.array([train[f'feature_{i}'].nunique() for i in range(50)])\ny2 = np.array([test[f'feature_{i}'].nunique() for i in range(50)])\ncomp = y-y2\n\n\nax.bar(range(50), y2, alpha=0.7, color=gray_dark_palette[0], label='Test Dataset')\n# Thanks to @rahulchauhan3j to fix typo\nax.bar(range(50),  comp*(comp>0), bottom=y2, color=dark_palette[2], alpha=0.7, label='Train > Test')\nax.bar(range(50), comp*(comp<0), bottom=y2-comp*(comp<0), color=dark_palette[3], alpha=0.7, label='Train < Test')\n\nax.set_yticks(range(0, 80, 5))\nax.margins(0.02)\nax.grid(axis='y', linestyle='--', zorder=5)\nax.set_title('# of Features Unique Values (Train\/Test)', loc='left', fontweight='bold')\nax.legend()\nplt.show()","04f4e840":"fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n\ny = [train[f'feature_{i}'].nunique() for i in range(50)]\n\nfor ax in axes:\n    ax.bar(range(50), y, zorder=10)\n    ax.set_yticks(range(0, 80, 5))\n    ax.margins(0.02)\n    ax.grid(axis='y', linestyle='--', zorder=5)\n    y.sort()\n\naxes[0].set_title('# of Features Unique Values (Raw)', loc='left', fontweight='bold')\naxes[1].set_title('# of Features Unique Values (Sorted)', loc='left', fontweight='bold')\n\nplt.show()","cd82b6de":"fig, axes = plt.subplots(13, 4, figsize=(10, 16))\n\ntarget_order = sorted(train['target'].unique())\nfor idx, ax in zip(range(50), axes.flatten()):\n    cnt = train[f'feature_{idx}'].value_counts().sort_index()\n    sns.kdeplot(x=f'feature_{idx}', \n                hue='target', hue_order=target_order,\n                data=train,\n                alpha=0.5, \n                linewidth=0.6, fill=True,\n                legend=False,\n                ax=ax)\n    \n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.spines['left'].set_visible(False)\n    cnt = len(train[f'feature_{idx}'].unique())\n    ax.set_title(f'Feature_{idx}({cnt})', loc='right', weight='bold', fontsize=11)\n\naxes.flatten()[-1].axis('off')    \naxes.flatten()[-2].axis('off')\n\nfig.supxlabel('Distribution by class (by feature)', ha='center', fontweight='bold')\n\nfig.tight_layout()\nplt.show()","bcfbc57d":"zero_data = ((train.iloc[:,:50]==0).sum() \/ len(train) * 100)[::-1]\nfig, ax = plt.subplots(1,1,figsize=(10, 19))\n\nax.barh(zero_data.index, 100, color='#dadada', height=0.6)\nbarh = ax.barh(zero_data.index, zero_data, color=light_palette[1], height=0.6)\nax.bar_label(barh, fmt='%.01f %%', color='black')\nax.spines[['left', 'bottom']].set_visible(False)\n\nax.set_xticks([])\n\nax.set_title('# of Zeros (by feature)', loc='center', fontweight='bold', fontsize=15)    \nplt.show()","966dfb2c":"fig, axes = plt.subplots(13, 4, figsize=(10, 16))\n\ntarget_order = sorted(train['target'].unique())\nmean = train.groupby('target').mean().sort_index()\nstd = train.groupby('target').std().sort_index()\n\nfor idx, ax in zip(range(50), axes.flatten()):\n    ax.bar(mean[f'feature_{idx}'].index, mean[f'feature_{idx}'], \n           color=dark_palette[:4], width=0.6)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_xlabel('')\n    ax.set_ylabel('')\n    ax.margins(0.1)\n    ax.spines['left'].set_visible(False)\n    ax.set_title(f'Feature_{idx}', loc='right', weight='bold', fontsize=11)\n\naxes.flatten()[-1].axis('off')    \naxes.flatten()[-2].axis('off')\n\nfig.supxlabel('Average by class (by feature)', ha='center', fontweight='bold')\n\nfig.tight_layout()\nplt.show()","3181046c":"label_dict = {val:idx for idx, val in enumerate(sorted(train['target'].unique()))}\ntrain['target'] = train['target'].map(label_dict)","5dc84c75":"fig, ax = plt.subplots(figsize=(9 , 9))\n\ncorr = train.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\n\nsns.heatmap(corr,\n        square=True, center=0, linewidth=0.2,\n        cmap=sns.diverging_palette(240, 10, as_cmap=True),\n        mask=mask, ax=ax) \n\nax.set_title('Feature Correlation', loc='left', fontweight='bold')\nplt.show()","69f0b514":"from umap import UMAP\n\ntrain_sub = train.sample(10000, random_state=0)\ntarget = train_sub['target']\numap = UMAP(random_state=0)\ndr = umap.fit_transform(train_sub.iloc[:,:-1], target)","9a82432d":"fig = plt.figure(figsize=(12, 12))\ngs = fig.add_gridspec(5, 4)\nax = fig.add_subplot(gs[:-1,:])\n\nsub_axes = [None] * 4\nfor idx in range(4): \n    sub_axes[idx] = fig.add_subplot(gs[-1,idx])\n\nfor idx in range(4):\n    ax.scatter(x=dr[:,0][target==idx], y=dr[:,1][target==idx],\n              s=10, alpha=0.2\n              )\n\n    for j in range(4):\n        sub_axes[j].scatter(x=dr[:,0][target==idx], y=dr[:,1][target==idx],\n                              s=10, alpha = 0.4 if idx==j else 0.008, color = (dark_palette[j%9]) if idx==j else white_color,\n                            zorder=(idx==j)\n                           )\n        \n    \n    sub_axes[idx].set_xticks([])\n    sub_axes[idx].set_yticks([])\n    sub_axes[idx].set_xlabel('')\n    sub_axes[idx].set_ylabel('')\n    sub_axes[idx].set_title(f'Class_{idx+1}')\n    sub_axes[idx].spines['right'].set_visible(True)\n    sub_axes[idx].spines['top'].set_visible(True)\n\nax.set_title('Dimenstion Reduction (UMAP)', fontweight='bold', fontfamily='serif', fontsize=20, loc='left')   \n    \nax.set_xticks([])\nax.set_yticks([])\nax.set_xlabel('')\nax.set_ylabel('')\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\n\nfig.tight_layout()\nplt.show()","57ce25bd":"## Distribution Check","33dfafc0":"## Import Library & Default Setting\n\n","e1ade655":"It can range from a fairly small number of features to a large number of features.\n\nIn the case of the features that are few here, I think you can think of them as catechical features.","a9bbbb2f":"## Correlation","b11a60be":"## EDA with the Table\n\nFirst, let's look at the statistical values for each feature.\n\n### Train data","7c5b6bcb":"## Dataset Check","bb166478":"### Summary\n\n- No missing data\n- The scale of the mean and deviation varies.\n- The median is mostly 0, and there are 2 columns with median 1.\n- The representative statistics of train and test are almost similar.\n\n- There is no significant difference between the mean and the deviation.\n- There are cases where the minimum value and the maximum value are different, which means that the feature range of the test may be different in the train.","68d2c9bd":"- reference : https:\/\/www.kaggle.com\/subinium\/dark-mode-visualization-apple-version","2a247169":"- The dimensionality reduction has yielded quite meaningful results. I think you can get good results when modeling using this.\n- Especially, since Class_2, which is an orange, occupies the largest number, we expect it to produce basic performance even if it can be classified well.\n\n> It must be Kaggle's gift. It looks like fireworks.","98f9a576":"### Comparison of statistics of train and test.","9ffe0358":"All features are left skewed. And many features are 0, around 80%.\n\nHandling this well may be a key.\n\nThere are obvious differences in mean when each feature is grouped by class. \n\nHowever, the deviation is also that large. (Normalization can also yield significant deviations)","da0121cb":"## Dimension Reduction with Sampling\n\nLet's look at meaningful clustering through dimension reduction.\n\nSince there is a lot of data, we will use it by sampling only 10%.\n\nI tried reducing the dimensions with UMAP.","bbb8a018":"## Target Distribution","1b317132":"The id value is meaningless, so I will leave it out in advance.","7939c2fc":"### If the content is helpful, please upvote. :)","ed8d621c":"All data in the dataset are integers. Then, a rough look at the number of each integer is as follows.","d8ff2c0d":"Most are weak correlations with a correlation of 0.02 or less.","1bc06c4b":"### Test data","a8dbb5b0":"## [TPS-Apr] Categorical EDA\n\nThis competition is a classification problem that classifies **4 classes** using a total of **50 integer features.**\n\nI think the core of this competition is how to proceed with the **ensemble** based on the **encoding method of integer features**.\n\n**The results of the dimension reduction in the back are interesting, so be sure to take a look.**\n\n\n- Import Library & Default Setting\n- Dataset Check\n- Target Distribution\n- EDA with the Table\n- Distribution Check\n- # of Zeros\n- Correlation\n- Dimension Reduction"}}