{"cell_type":{"475d8b6d":"code","b6c51573":"code","216f3d1f":"code","e6445903":"code","d1931841":"code","f1079a02":"code","d02dcc80":"code","a3d098db":"code","64b0c4ee":"markdown","bf930c50":"markdown","5934f7cb":"markdown","a1ffc528":"markdown","6dae766e":"markdown","aa178d6e":"markdown"},"source":{"475d8b6d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n%load_ext autoreload\n%autoreload 2\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.options.display.float_format = '{:,.5f}'.format\n\nfrom pandas_profiling import ProfileReport # library for automatic EDA\n\nfrom time import time\nfrom IPython.display import display\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn import datasets\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b6c51573":"# Importing the data and displaying some rows\ndf = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\").drop('Id', axis=1)\n\nX = df.drop(['Species'], axis=1)\ny = pd.Series(LabelEncoder().fit_transform(df['Species']))\nX","216f3d1f":"report = ProfileReport(df)","e6445903":"display(report)","d1931841":"g = sns.pairplot(df, hue='Species', markers='x')\ng = g.map_upper(plt.scatter)\ng = g.map_lower(sns.kdeplot)","f1079a02":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    random_state=1, test_size=0.33,\n    shuffle=True, stratify=y\n)\n\n# Create prediction pipeline\n# - Logistic Regression will converge faster if the features are scaled to the same range\nscaler = StandardScaler() \nclf = LogisticRegression(random_state=1)\n\npipeline = make_pipeline(scaler, clf)\n\n# Fit the model, predict train and test sets\npipeline.fit(X_train, y_train)\ny_pred_train = pipeline.predict(X_train)\ny_pred_test = pipeline.predict(X_test)\n\n# Calculate scores, display results\ndisplay(\n    f'Train accuracy: {accuracy_score(y_train, y_pred_train)}',\n    f'Test accuracy: {accuracy_score(y_test, y_pred_test)}', \n)\n","d02dcc80":"from sklearn.model_selection import learning_curve\n\ntrain_sizes, train_scores, test_scores = learning_curve(\n    pipeline, X, y, train_sizes=np.arange(10, 110, 10),\n    random_state=1, shuffle=True, scoring='accuracy'\n)\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nsns.lineplot(x=train_sizes, y=train_scores_mean, label='train accuracy', ax=ax)\nsns.lineplot(x=train_sizes, y=test_scores_mean, label='test accuracy', ax=ax)","a3d098db":"from sklearn.model_selection import RepeatedStratifiedKFold, cross_validate\n\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=10, random_state=1)\n\nres = cross_validate(pipeline, X, y, scoring='accuracy', return_train_score=True, cv=cv, n_jobs=-1)\n\ntrain_score = np.mean(res['train_score'])\ntest_score = np.mean(res['test_score'])\n\ndisplay(\n    f'Mean train accuracy: {train_score:.2f}',\n    f'Mean test accuracy: {test_score:.2f}', \n)","64b0c4ee":"## Import dependencies","bf930c50":"Accuracy of 0.98 looks like a pretty decent result and it's not clear whether existing data can be separated more precisely out of sample. Let's draw learning curves and see how train and test scores change based on training set size.","5934f7cb":"## Load data","a1ffc528":"Test score got a bit worse, but is still good. That confirms that linear model is indeed capable of separating the classes.","6dae766e":"## Logistic regression\n\nThe 0-class is linearly separable from the other two classes. The 1 and 2 classes are not perfectly separable with a straight line. Visually estimating from the plot above, linear classifier should give us close to perfect, but not perfect, performance. Let's say our baseline expectation would be 95% accuracy. ","aa178d6e":"Learning curves look decent as well. There doesn't seem to be much variance or bias.\n\nHowever, the results above are measured on one specific subset of data. We might've got lucky. To validate the result, let's run repeated k-fold cross validation and calculate mean accuracy between folds and shuffles."}}