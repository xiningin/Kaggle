{"cell_type":{"dd10e8b9":"code","1eb4d82c":"code","f49be566":"code","b0f07d89":"code","29b37abb":"code","a6e0b590":"code","76a92bdf":"code","fe9c3b45":"code","b227a300":"code","41f7f486":"code","8471f896":"code","46949d95":"code","8972a0aa":"code","53c7ed54":"markdown","abe87b84":"markdown","f1315186":"markdown","e22452c8":"markdown","96a0b6e0":"markdown","c7c6711a":"markdown","121b2c9e":"markdown","1dca2ee1":"markdown","c738e1a3":"markdown","21d1bbac":"markdown","7707c8d9":"markdown","0de66735":"markdown","d7b401f3":"markdown","efcd6b13":"markdown","ec15f50e":"markdown","10356980":"markdown","55508df6":"markdown","11f5d771":"markdown"},"source":{"dd10e8b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n# All Import Statements Defined Here\n# Note: Do not add to this list.\n# All the dependencies you need, can be installed by running .\n# ----------------\nimport sys\nassert sys.version_info[0]==3\nassert sys.version_info[1] >= 5\n\nimport pprint\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [10, 5]\nimport nltk\nnltk.download('reuters')\nfrom nltk.corpus import reuters\nimport numpy as np\nimport random\nimport scipy as sp\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import PCA\n\nSTART_TOKEN = '<START>'\nEND_TOKEN = '<END>'\n\nnp.random.seed(0)\nrandom.seed(0)\n# ----------------\n\ndef read_corpus(category=\"crude\"):\n    \"\"\" Read files from the specified Reuter's category.\n        Params:\n            category (string): category name\n        Return:\n            list of lists, with words from each of the processed files\n    \"\"\"\n    files = reuters.fileids(category)\n    return [[START_TOKEN] + [w.lower() for w in list(reuters.words(f))] + [END_TOKEN] for f in files]\n\nreuters_corpus = read_corpus()\n\n\n# Answer to Question 1.1: Implement distinct_words [code] (2 points)\n#===============================================================================================================\n\ndef distinct_words(corpus):\n    \"\"\" Determine a list of distinct words for the corpus.\n        Params:\n            corpus (list of list of strings): corpus of documents\n        Return:\n            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n            num_corpus_words (integer): number of distinct words across the corpus\n    \"\"\"\n    corpus_words = []\n    num_corpus_words = -1\n\n    # ------------------\n    # Write your implementation here.\n    # Flattening the corpus\n    split_corpus = [y for word in corpus for y in word]\n    # Adding words from list if not present in corpus_words(corpus_words is the output or unique word list)\n    for word in split_corpus:\n        if word not in corpus_words:\n            corpus_words.append(word)\n            # Print list\n    corpus_words = sorted(corpus_words)\n    num_corpus_words = len(corpus_words)\n    # ------------------\n\n    return corpus_words, num_corpus_words\n\n# The Answer to Question 1.1 ends here\n#===============================================================================================================\n\n# ---------------------\n# Run this sanity check\n# Note that this not an exhaustive check for correctness.\n# ---------------------\n\n# Define toy corpus\ntest_corpus = [\"START All that glitters isn't gold END\".split(\" \"), \"START All's well that ends well END\".split(\" \")]\ntest_corpus_words, num_corpus_words = distinct_words(test_corpus)\n\n# Correct answers\nans_test_corpus_words = sorted(list(set([\"START\", \"All\", \"ends\", \"that\", \"gold\", \"All's\", \"glitters\", \"isn't\", \"well\", \"END\"])))\nans_num_corpus_words = len(ans_test_corpus_words)\n\n# Test correct number of words\nassert(num_corpus_words == ans_num_corpus_words), \"Incorrect number of distinct words. Correct: {}. Yours: {}\".format(ans_num_corpus_words, num_corpus_words)\n\n# Test correct words\nassert (test_corpus_words == ans_test_corpus_words), \"Incorrect corpus_words.\\nCorrect: {}\\nYours:   {}\".format(str(ans_test_corpus_words), str(test_corpus_words))\n\n# Print Success\nprint (\"-\" * 80)\nprint(\"Passed All Tests!\")\nprint (\"-\" * 80)\n\n#=====================================================================================================================\ndef compute_co_occurrence_matrix(corpus, window_size=4):\n    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n\n        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n              number of co-occurring words.\n\n              For example, if we take the document \"START All that glitters is not gold END\" with window size of 4,\n              \"All\" will co-occur with \"START\", \"that\", \"glitters\", \"is\", and \"not\".\n\n        Params:\n            corpus (list of list of strings): corpus of documents\n            window_size (int): size of context window\n        Return:\n            M (numpy matrix of shape (number of corpus words, number of corpus words)):\n                Co-occurence matrix of word counts.\n                The ordering of the words in the rows\/columns should be the same as the ordering of the words given by the distinct_words function.\n            word2Ind (dict): dictionary that maps word to index (i.e. row\/column number) for matrix M.\n    \"\"\"\n    words, num_words = distinct_words(corpus)\n    M = None\n    word2Ind = {}\n\n    # ------------------\n    # Write your implementation here.\n    for i in range(num_words):\n        word2Ind[words[i]] = i\n    M = np.zeros((num_words, num_words))\n    for line in corpus:\n        for i in range(len(line)):\n            target = line[i]\n            target_index = word2Ind[target]\n            \n            left = max(i - window_size, 0)\n            right = min(i + window_size, len(line) - 1)\n            for j in range(left, i):\n                window_word = line[j]\n                M[target_index][word2Ind[window_word]] += 1\n                M[word2Ind[window_word]][target_index] += 1\n\n\n\n    # ------------------\n\n    return M, word2Ind\n\n\n\n# ---------------------\n# Run this sanity check\n# Note that this is not an exhaustive check for correctness.\n# ---------------------\n\n# Define toy corpus and get student's co-occurrence matrix\ntest_corpus = [\"START All that glitters isn't gold END\".split(\" \"), \"START All's well that ends well END\".split(\" \")]\nM_test, word2Ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\n\n# Correct M and word2Ind\nM_test_ans = np.array(\n    [[0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,],\n     [0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,],\n     [0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,],\n     [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,],\n     [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,],\n     [0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,],\n     [0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,],\n     [0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,],\n     [1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,],\n     [0., 1., 1., 0., 1., 0., 0., 0., 1., 0.,]]\n)\nword2Ind_ans = {'All': 0, \"All's\": 1, 'END': 2, 'START': 3, 'ends': 4, 'glitters': 5, 'gold': 6, \"isn't\": 7, 'that': 8, 'well': 9}\n\n# Test correct word2Ind\nassert (word2Ind_ans == word2Ind_test), \"Your word2Ind is incorrect:\\nCorrect: {}\\nYours: {}\".format(word2Ind_ans, word2Ind_test)\n\n# Test correct M shape\nassert (M_test.shape == M_test_ans.shape), \"M matrix has incorrect shape.\\nCorrect: {}\\nYours: {}\".format(M_test.shape, M_test_ans.shape)\n\n# Test correct M values\nfor w1 in word2Ind_ans.keys():\n    idx1 = word2Ind_ans[w1]\n    for w2 in word2Ind_ans.keys():\n        idx2 = word2Ind_ans[w2]\n        student = M_test[idx1, idx2]\n        correct = M_test_ans[idx1, idx2]\n        if student != correct:\n            print(\"Correct M:\")\n            print(M_test_ans)\n            print(\"Your M: \")\n            print(M_test)\n            raise AssertionError(\"Incorrect count at index ({}, {})=({}, {}) in matrix M. Yours has {} but should have {}.\".format(idx1, idx2, w1, w2, student, correct))\n\n# Print Success\nprint (\"-\" * 80)\nprint(\"Passed All Tests for Q1.2!\")\nprint (\"-\" * 80)\n\n#=================================================================================================================================\n#Question 1.3: Implement reduce_to_k_dim [code] (1 point)\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.random_projection import sparse_random_matrix\n\n\ndef reduce_to_k_dim(M, k=2):\n    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n            - http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.TruncatedSVD.html\n\n        Params:\n            M (numpy matrix of shape (number of corpus words, number of corpus words)): co-occurence matrix of word counts\n            k (int): embedding size of each word after dimension reduction\n        Return:\n            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n                    In terms of the SVD from math class, this actually returns U * S\n    \"\"\"\n    n_iters = 10  # Use this parameter in your call to `TruncatedSVD`\n    M_reduced = None\n    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n\n    # ------------------\n    # Write your implementation here.\n    from sklearn.decomposition import TruncatedSVD\n    svd = TruncatedSVD(n_components=k, n_iter=n_iters)\n    svd.fit(M)\n    M_reduced = svd.transform(M)\n\n\n\n    # ------------------\n\n    print(\"Done.\")\n    return M_reduced\n\n#==========================================================================================================================\n\n# ---------------------\n# Run this sanity check\n# Note that this not an exhaustive check for correctness\n# In fact we only check that your M_reduced has the right dimensions.\n# ---------------------\n\n# Define toy corpus and run student code\ntest_corpus = [\"START All that glitters isn't gold END\".split(\" \"), \"START All's well that ends well END\".split(\" \")]\nM_test, word2Ind_test = compute_co_occurrence_matrix(test_corpus, window_size=1)\nM_test_reduced = reduce_to_k_dim(M_test, k=2)\n\n# Test proper dimensions\nassert (M_test_reduced.shape[0] == 10), \"M_reduced has {} rows; should have {}\".format(M_test_reduced.shape[0], 10)\nassert (M_test_reduced.shape[1] == 2), \"M_reduced has {} columns; should have {}\".format(M_test_reduced.shape[1], 2)\n\n# Print Success\nprint (\"-\" * 80)\nprint(\"Passed All Tests for Q1.3!\")\nprint (\"-\" * 80)\n\n\n#Q1.3 Ends here\n#===========================================================================================================================\n\n\n#======================================================================================================================\n#Question 1.4: Implement plot_embeddings [code] (1 point)\ndef plot_embeddings(M_reduced, word2Ind, words):\n    \"\"\" Plot in a scatterplot the embeddings of the words specified in the list \"words\".\n        NOTE: do not plot all the words listed in M_reduced \/ word2Ind.\n        Include a label next to each point.\n\n        Params:\n            M_reduced (numpy matrix of shape (number of unique words in the corpus , k)): matrix of k-dimensioal word embeddings\n            word2Ind (dict): dictionary that maps word to indices for matrix M\n            words (list of strings): words whose embeddings we want to visualize\n    \"\"\"\n    \n    # ------------------\n    # Write your implementation here.\n    words_index = [word2Ind[word] for word in words]\n    print(\"Word Index = \", words_index)\n    x_coords = [M_reduced[word_index][0] for word_index in words_index]\n    y_coords = [M_reduced[word_index][1] for word_index in words_index]\n    \n    for i, word in enumerate(words):\n        x = x_coords[i]\n        y = y_coords[i]\n        plt.scatter(x, y, marker = 'x', color = 'red')\n        plt.text(x + 0.0003, y + 0.0003, word, fontsize = 9)\n        \n    \n\n    # ------------------\n\n    # ---------------------\n    # Run this sanity check\n    # Note that this not an exhaustive check for correctness.\n    # The plot produced should look like the \"test solution plot\" depicted below.\n    # ---------------------\n\n#The Answer ends here\n#============================================================================================================================\n\n\n\n","1eb4d82c":"#Q1.5\n# -----------------------------\n# Run This Cell to Produce Your Plot\n# ------------------------------\nreuters_corpus = read_corpus()\nM_co_occurrence, word2Ind_co_occurrence = compute_co_occurrence_matrix(reuters_corpus)\nM_reduced_co_occurrence = reduce_to_k_dim(M_co_occurrence, k=2)\n\n# Rescale (normalize) the rows to make them each of unit-length\nM_lengths = np.linalg.norm(M_reduced_co_occurrence, axis=1)\nM_normalized = M_reduced_co_occurrence \/ M_lengths[:, np.newaxis] # broadcasting\n\nwords = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']\nplot_embeddings(M_normalized, word2Ind_co_occurrence, words)\n","f49be566":"\n\n\n#============================================================================================================================\n\n#Part 2 begins\ndef load_word2vec():\n    \"\"\" Load Word2Vec Vectors\n        Return:\n            wv_from_bin: All 3 million embeddings, each lengh 300\n    \"\"\"\n    from gensim.models import Word2Vec\n    import gensim\n  \n    print(\"gensim imported\")\n    # Load Google's pre-trained Word2Vec model.\n\n\n    wv_from_bin = gensim.models.KeyedVectors.load_word2vec_format(\"..\/input\/GoogleNews-vectors-negative300.bin\", binary=True)\n    print(\"model trained\")\n\n    vocab = list(wv_from_bin.vocab.keys())\n    print(\"Loaded vocab size %i\" % len(vocab))\n    return wv_from_bin\n\n# -----------------------------------\n# Run Cell to Load Word Vectors\n# Note: This may take several minutes\n# -----------------------------------\nwv_from_bin = load_word2vec()\n","b0f07d89":"def get_matrix_of_vectors(wv_from_bin, required_words=['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']):\n    \"\"\" Put the word2vec vectors into a matrix M.\n        Param:\n            wv_from_bin: KeyedVectors object; the 3 million word2vec vectors loaded from file\n        Return:\n            M: numpy matrix shape (num words, 300) containing the vectors\n            word2Ind: dictionary mapping each word to its row number in M\n    \"\"\"\n    import random\n    words = list(wv_from_bin.vocab.keys())\n    print(\"Shuffling words ...\")\n    random.shuffle(words)\n    words = words[:10000]\n    print(\"Putting %i words into word2Ind and matrix M...\" % len(words))\n    word2Ind = {}\n    M = []\n    curInd = 0\n    for w in words:\n        try:\n            M.append(wv_from_bin.word_vec(w))\n            word2Ind[w] = curInd\n            curInd += 1\n        except KeyError:\n            continue\n    for w in required_words:\n        try:\n            M.append(wv_from_bin.word_vec(w))\n            word2Ind[w] = curInd\n            curInd += 1\n        except KeyError:\n            continue\n    M = np.stack(M)\n    print(\"Done.\")\n    return M, word2Ind","29b37abb":"# -----------------------------------------------------------------\n# Run Cell to Reduce 300-Dimensinal Word Embeddings to k Dimensions\n# Note: This may take several minutes\n# -----------------------------------------------------------------\nM, word2Ind = get_matrix_of_vectors(wv_from_bin)\nM_reduced = reduce_to_k_dim(M, k=2)","a6e0b590":"words = ['barrels', 'bpd', 'ecuador', 'energy', 'industry', 'kuwait', 'oil', 'output', 'petroleum', 'venezuela']\nplot_embeddings(M_reduced, word2Ind, words)","76a92bdf":"# ------------------\n# Write your polysemous word exploration code here.\n\nwv_from_bin.most_similar(\"lie\")\n\n# ------------------","fe9c3b45":"# ------------------\n# Write your synonym & antonym exploration code here.\n\nw1 = \"angel\"\nw2 = \"saint\"\nw3 = \"demon\"\nw1_w2_dist = wv_from_bin.distance(w1, w2)\nw1_w3_dist = wv_from_bin.distance(w1, w3)\n\nprint(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2, w1_w2_dist))\nprint(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w1_w3_dist))\n\n# ------------------","b227a300":"# Run this cell to answer the analogy -- man : king :: woman : x\npprint.pprint(wv_from_bin.most_similar(positive=['woman', 'king'], negative=['man']))","41f7f486":"# ------------------\n# Write your analogy exploration code here.\n\npprint.pprint(wv_from_bin.most_similar(positive=['father','woman'], negative=['man']))\n\n# ------------------","8471f896":"# ------------------\n# Write your incorrect analogy exploration code here.\n\npprint.pprint(wv_from_bin.most_similar(positive=['apple','orange'], negative=['red']))\n\n# ------------------","46949d95":"# Run this cell\n# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be\n# most dissimilar from.\npprint.pprint(wv_from_bin.most_similar(positive=['woman', 'boss'], negative=['man']))\nprint()\npprint.pprint(wv_from_bin.most_similar(positive=['man', 'boss'], negative=['woman']))","8972a0aa":"# ------------------\n# Write your bias exploration code here.\n\npprint.pprint(wv_from_bin.most_similar(positive=['doctor','female'], negative=['male']))\nprint()\npprint.pprint(wv_from_bin.most_similar(positive=['nurse','male'], negative=['female']))\n\n# ------------------","53c7ed54":"man:father::woman:?","abe87b84":"**Question 2.6: Guided Analysis of Bias in Word Vectors [written] (1 point)\u00b6**\nIt's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit to our word embeddings.\n\nRun the cell below, to examine (a) which terms are most similar to \"woman\" and \"boss\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"boss\" and most dissimilar to \"woman\". What do you find in the top 10?","f1315186":"**Question 2.2: Polysemous Words (2 points) [code + written]\n**","e22452c8":"**Solving Analogies with Word Vectors**\nWord2Vec vectors have been shown to sometimes exhibit the ability to solve analogies.\n\nAs an example, for the analogy \"man : king :: woman : x\", what is x?\n\nIn the cell below, we show you how to use word vectors to find x. The most_similar function finds words that are most similar to the words in the positive list and most dissimilar from the words in the negative list. The answer to the analogy will be the word ranked most similar (largest numerical value).\n\nNote: Further Documentation on the most_similar function can be found within the GenSim documentation.","96a0b6e0":"**Question 2.4: Finding Analogies [code + written] (2 Points)**\nFind an example of analogy that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy is complicated, explain why the analogy holds in one or two sentences.\n\nNote: You may have to try many analogies to find one that works!","c7c6711a":"**Question 2.1: Word2Vec Plot Analysis [written] (4 points)\n**","121b2c9e":"Please state the polysemous word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous words you tried didn't work?  \nI discovered lie which had \"sit\" and \"perjure_yourself\". So, it has close enough words to mean be prone and to be false.  \nI believe the other words which didn't work (list below) were because one usage was so prevelant that the other usage didn't figure in the top 10 words with low cosine value.  \n\n\nPolysemous words I tried which didn't work -  \n**Current** meaning recent and electricity but the model only showed the words which mean \"recent\" or unrelated words.  \n**Pine** meaning a type of tree and hope, but the model only showed the words which mean \"tree\".  \n**Bank** meaning a financial institution and river bank or reliable, but the model only showed words for \"Financial Institution\".  \n**Count** meaning figuring out number of items or reliable or a noble, but the model only showed words for \"figuring out the number of items\".  \n**Fair** meaning reasonable or just and white but the model only showed output for \"reasonable\".  \n","1dca2ee1":"What clusters together in 2-dimensional embedding space? What doesn't cluster together that you might think should have? How is the plot different from the one generated earlier from the co-occurrence matrix?\n\n**Answer:** Intererstingly, industry and energy seem to have clustered close to each other. I would have expected kuwait to be much closer to ecuasor and venezula, but it is further away. Finally, the error in the previous co-occurence matrix of oil and petroleum being apart has been rectified. They are much closer now.","c738e1a3":"**Question 2.7: Independent Analysis of Bias in Word Vectors [code + written] (2 points)\u00b6**\nUse the most_similar function to find another case where some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover.","21d1bbac":"**Question 2.5: Incorrect Analogy [code + written] (1 point)**\nFind an example of analogy that does not hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the (incorrect) value of b according to the word vectors.","7707c8d9":"Top 10 words similar to woman boss: bosses, manageress, exec, Manageress, receptionist, Jane_Danson, Fiz_Jennie_McAlpine, Coronation_Street_actress, supremo, coworker\nTop 10 words similar to man boss: supremo, MOTHERWELL_boss, CARETAKER_boss, Bully_Wee_boss, YEOVIL_Town_boss,head_honco, manager_Stan_Ternent, Viv_Busby, striker_Gabby_Agbonlahor, BARNSLEY_boos","0de66735":"Using the code above we can see that gyanecologist was mentioned as an occupation for females but not for males. Surprisingly, I expected the algorithm to have bias and show nurse for females but not for males. As you can see though the bias does not exist.","d7b401f3":"apple:red::orange:orange\nI expected apple-red+orange = orange\nHowever, the answer doesn't work perhaps because the two words are same.\nThe results I have got are apples, fruit, strawberry, watermelon. ","efcd6b13":"**Reducing dimensionality of Word2Vec Word Embeddings\n**","ec15f50e":"**Question 2.3: Synonyms & Antonyms (2 points) [code + written]**\nWhen considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n\nFind three words (w1,w2,w3) where w1 and w2 are synonyms and w1 and w3 are antonyms, but Cosine Distance(w1,w3) < Cosine Distance(w1,w2). For example, w1=\"happy\" is closer to w3=\"sad\" than to w2=\"cheerful\".\n\nOnce you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n\nYou should use the the wv_from_bin.distance(w1, w2) function here in order to compute the cosine distance between two words. Please see the GenSim documentation for further assistance.","10356980":"Event though the cosine distance between Angel and Saint is slightly more than the cosine distance between Angel and demon, it is counter intuititive. The reason this happens is because the angel and demon would be rated on similarly on several parameters\/dimensions such as supernatural, biblical reference and powers. However, they would be exact opposite in one dimension benevolence. Therefore, it makes sense that they would have lower cosine distance.","55508df6":"**Part 2**","11f5d771":"Q. What clusters together in 2-dimensional embedding space?\nEcuador and Venezula are clubbed together. This makes sense as the two are countries in South America. Their embeddings are bound to be similar in several dimensions.\n\nQ. What doesn't cluster together that you might think should have?\nOil and petroleum. The two are liquid fuels. I use the terms as synonyms. Hence, it is surprising to see them so far apart on the lot.\n"}}