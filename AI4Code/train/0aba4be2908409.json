{"cell_type":{"a54b05f0":"code","5226c166":"code","99bc735f":"code","4c156c81":"code","63f305f7":"code","5ea53129":"code","d2728810":"code","8d117a5c":"code","fc5db929":"code","6751ed3b":"code","ee1b7bac":"code","64b6cdfd":"code","16253e96":"code","efadfa04":"code","95cbe152":"code","82237e12":"code","79162207":"code","c7bd9b44":"code","785d44a0":"code","e889e69a":"code","efc14426":"code","01332f25":"code","0d205304":"code","aa99d16a":"code","51c841c1":"code","073332f7":"code","da1fc2f2":"code","f73a2083":"code","f28ff3b5":"code","2bf3e5da":"code","ae7e4a93":"code","3a52c24e":"code","fd2ac724":"code","dc5b6245":"code","005b7416":"code","a906a0aa":"code","0d7738c1":"code","1065d056":"code","3445be54":"code","ad9d0359":"code","cc41ab0b":"code","c0364284":"code","e82cd7f1":"code","83f8fb64":"code","70f61061":"code","06ab88f8":"code","e9dc9626":"code","269d9834":"code","262aacf3":"code","964680be":"code","8c6222a0":"code","fe0f2d4d":"code","770b8278":"code","0d705c86":"markdown","14e862a3":"markdown","01f6fb54":"markdown","b07121e5":"markdown","b2dc70f1":"markdown","d8837a52":"markdown","ea27bc47":"markdown","ecf71000":"markdown","b4e7363a":"markdown","65568b03":"markdown","492f791b":"markdown","e53ee819":"markdown","5a528897":"markdown","76a50676":"markdown","03247545":"markdown"},"source":{"a54b05f0":"import tensorflow as tf\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport itertools\nimport re\n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nimport gensim\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n%matplotlib inline","5226c166":"data_dir = \"\/kaggle\/input\/ykc-2nd\/\"\ntrain = pd.read_csv(data_dir+\"train.csv\", index_col=0)\ntest = pd.read_csv(data_dir+\"test.csv\", index_col=0)\nsub = pd.read_csv(data_dir+\"sample_submission.csv\", index_col=0)\n\n# merge train and test\ndf = pd.concat([train, test])\n\n# map words to list\n# \u8a18\u53f7\u3084's\u306a\u3069\u3001\u3044\u3089\u306a\u3044\u90e8\u5206\u3092\u53d6\u308a\u9664\u3044\u3066\u3044\u308b\ndf[\"product_name\"] = df[\"product_name\"].map(lambda w: list(\n    filter(None, list(filter(None, \n                             re.sub(\"\\'\", \"\", \n                                    re.sub(r\"-|\/|\\\\|\\|\\\"|\\:|\\;|\\@|\\^\", \" \", \n                                    re.sub(r\"[0-9]+|\\+|\\&|\\?|!|%|\\.|,|\\)|\\(|\\'s|\u00ae|\u2122|#|~|\u00a9|\",\n                                           \"\", w.lower().replace(\"\\xa0\", \" \")))).split(\" \"))))))\n\n# get train dataframe\ndf_train = df[~df.department_id.isna()]\ndf_test = df[df.department_id.isna()]","99bc735f":"n_department = 21\nn_split = 5","4c156c81":"df","63f305f7":"# for tf-idf\ndocs = []\nfor i in range(n_department):\n    names = itertools.chain.from_iterable(df_train[df_train[\"department_id\"] == i][\"product_name\"].tolist())\n    docs.append(\" \".join([n for n in names]))\n    \n\nvectorizer = TfidfVectorizer(max_df=0.9)\nX = vectorizer.fit_transform(docs)\nwords = vectorizer.get_feature_names()\n\nword_to_index = dict(zip(words, np.arange(len(words))))\nX = X.toarray()","5ea53129":"def calculate_score(word_list: list):\n    \"\"\"\n    convert word list to score \n    \n    Args:\n        word_list: list of string. word list\n\n    Returns:\n        np.ndarray: averaged score of the words. shape = [n_columns]\n    \"\"\"\n    result = np.zeros([n_department])\n    n = 0\n    for w in word_list:\n        try:\n            result += X[:, word_to_index[w]]\n            n += 1\n        except KeyError as e:\n            pass\n    return result \/ n","d2728810":"tf_idf = df[\"product_name\"].map(calculate_score)\n\n# \u6b63\u898f\u5316\u3068\u3001\u306a\u305c\u304bnan\u304c\u3044\u308b\u306e\u3067\u3001nan\u30920\u306b\u3059\u308b(\u304a\u305d\u3089\u304f\u672a\u77e5\u8a9e?)\ntf_idf = np.nan_to_num(np.array(tf_idf.tolist()))\nmax_value, min_value = tf_idf.max(), tf_idf.min()\ntf_idf = (tf_idf - min_value) \/ (max_value - min_value)\n\n# \u305d\u308c\u305e\u308c\u306e\u30b9\u30b3\u30a2\u3092\u30ab\u30e9\u30e0\u306b\u3059\u308b\ndf = df.merge(pd.DataFrame(tf_idf.tolist(), columns=[\"dept_\"+str(i) for i in range(n_department)]), left_index=True, right_index=True)\n\n\ndf = df.reset_index()","8d117a5c":"## \u8a13\u7df4\u6e08\u307f\u306e\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\uff0cproduct_name\u306b\u542b\u307e\u308c\u308b\u5358\u8a9e\u3092\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3057\u3066\u5e73\u5747\u3092\u53d6\u308b\u3053\u3068\u3067\uff0c\u5404product_id\u306b\u5bfe\u3057\u3066\u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u3059\u308b\n\n## gensim\u3067vec\u304b\u3089\u8aad\u307f\u8fbc\u3080\u5834\u5408\uff08\uff15\u5206\u3050\u3089\u3044\u304b\u304b\u308b\uff09\nmodel_ft = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/ykc-2nd\/wiki-news-300d-1M.vec\/wiki-news-300d-1M.vec')","fc5db929":"# \u3088\u304f\u308f\u304b\u3089\u306a\u3044\u304c\u3001\u305d\u308c\u3089\u3057\u3044\u30e9\u30a4\u30d6\u30e9\u30ea\u3092\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3057\u305f\n# \u3064\u306a\u304c\u3063\u305f\u5358\u8a9e\u3092\u5206\u89e3\u3059\u308b\u30e9\u30a4\u30d6\u30e9\u30ea\n# https:\/\/pypi.org\/project\/hyphenate\/\n# pip install hyphenate\n\n!pip install hyphenate","6751ed3b":"from hyphenate import hyphenate_word","ee1b7bac":"\nfrom collections import defaultdict\nunused_words = defaultdict(int)\ndef to_vec(x, model_ft):\n    v = np.zeros(model_ft.vector_size)\n    for w in x:\n        try:\n            v += model_ft[w] ## \u5358\u8a9e\u304c\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u306evocab\u306b\u3042\u3063\u305f\u3089\n        except:\n            # \u5358\u8a9e\u3092\u5206\u89e3\u3067\u304d\u305f\u3089\u5206\u89e3\n            hw = hyphenate_word(w)\n            # \u5206\u89e3\u3057\u305f\u5358\u8a9e\u306b\u5bfe\u3057\u3066\u540c\u3058\u51e6\u7406\n            for w2 in hw:\n                try:\n                    v += model_ft[w2]\n                except:\n                    unused_words[w2] += 1\n    v = v \/ (np.sqrt(np.sum(v ** 2)) + 1e-16) ## \u9577\u3055\u30921\u306b\u6b63\u898f\u5316\n    return v\n\ndef to_vec_max(x, model_ft):\n    v = [np.zeros(model_ft.vector_size), np.zeros(model_ft.vector_size)]\n    for w in x:    \n        try:\n            v.append(model_ft[w])\n        except:\n            hw = hyphenate_word(w)\n            for w2 in hw:\n                try:\n                    v.append(model_ft[w2])\n                except:\n                    pass\n    v = np.max(v, axis=0)\n    v = v \/ (np.sqrt(np.sum(v ** 2)) + 1e-16) ## \u9577\u3055\u30921\u306b\u6b63\u898f\u5316\n    return v\n\n# SWEM_mean\nvecs = df[\"product_name\"].apply(lambda x : to_vec(x, model_ft))\nvecs = np.vstack(vecs)\nvecs = np.nan_to_num(vecs)\n\n# SWEM_max_pooling\nvecs_max = df[\"product_name\"].apply(lambda x : to_vec_max(x, model_ft))\nvecs_max = np.vstack(vecs_max)\nvecs_max = np.nan_to_num(vecs_max)\n\n\nfasttext_pretrain_cols = [f\"fasttext_pretrain_vec{k}\" for k in range(vecs.shape[1])]\nfasttext_pretrain_cols_max = [f\"fasttext_pretrain_vec_max{k}\" for k in range(vecs.shape[1])]\n\n# merge dataframes\nvec_df = pd.DataFrame(vecs, columns=fasttext_pretrain_cols)\nvec_max_df = pd.DataFrame(vecs_max, columns=fasttext_pretrain_cols_max)\ndf = pd.concat([df, vec_df], axis = 1)\ndf = pd.concat([df, vec_max_df], axis = 1)\n\ndf.head()","64b6cdfd":"df_train = df[~df.department_id.isna()]\ndf_test = df[df.department_id.isna()]","16253e96":"np.random.seed(42)\n\n# fasttext\u7cfb\u306e\u5165\u3063\u305f\u30ab\u30e9\u30e0\ntrainCols = fasttext_pretrain_cols + fasttext_pretrain_cols_max + \\\n            [\"order_rate\", \"order_dow_mode\", \"order_hour_of_day_mode\"]\n\n# tf-idf\u7cfb\u306e\u30ab\u30e9\u30e0\ntrainCols2 = [\"dept_\"+str(i) for i in range(n_department)] + [\"order_rate\", \"order_dow_mode\", \"order_hour_of_day_mode\"]\n\n# NN\u7528\u306etf-idf\u7cfb\nnn_cols = [\"dept_\"+str(i) for i in range(n_department)]\n\n# NN\u7528\u306etf-idf + fasttext\nnn_cols2 = fasttext_pretrain_cols + fasttext_pretrain_cols_max + [\"dept_\"+str(i) for i in range(n_department)]\n\n# \u305d\u308c\u305e\u308cX_train\u3068\u3044\u3046\u540d\u524d\u3067\u53d6\u308a\u51fa\u3059\nX_train = df_train[trainCols]\nX2_train = df_train[trainCols2]\nXnn_train = df_train[nn_cols]\nXnn2_train = df_train[nn_cols2]\n\n# \u6b63\u89e3\u306e\u30ab\u30e9\u30e0\nY_train = df_train[\"department_id\"]\n\n# NN\u7528\u306eOneHot\u306e\u6b63\u89e3\u30c7\u30fc\u30bf\nYnn_train = pd.get_dummies(df_train, columns=[\"department_id\"])[[f\"department_id_{i}.0\" for i in range(21)]]\n\n# validation\u3068train\u306e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u30e9\u30f3\u30c0\u30e0\u306b\u6c7a\u3081\u308b\nvalidInds = np.random.choice(X_train.index.values, 4000, replace = False)\ntrainInds = np.setdiff1d(X_train.index.values, validInds)","efadfa04":"params = {\n    'lambda_l1': 0.0,\n    'lambda_l2': 0.0,\n    'num_leaves': 200,\n    'feature_fraction': 0.9840000000000001,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'min_child_samples': 20,\n    'task': 'train',\n    'boosting_type': 'dart',\n    'objective': 'multiclass',\n    'num_class': 21,\n    'metric': 'multi_error',\n    'verbose': 1,\n    'learning_rate': 0.1,\n    'num_iterations': 2000,\n    'max_depth': 7\n}","95cbe152":"import lightgbm as lgb\n\nlgb_train = lgb.Dataset(X_train.iloc[trainInds], Y_train.iloc[trainInds])\n\n# valid data\nlgb_val = lgb.Dataset(X_train.iloc[validInds], Y_train.iloc[validInds])\n\n# train LGBM model\nbest_params, history = {}, []\nlgb_model = lgb.train(params, lgb_train, \n                  valid_sets=lgb_val,\n                  verbose_eval=10)","82237e12":"## predict on valid\npred_val_lgb = lgb_model.predict(X_train.iloc[validInds])\n\n## evaluate\nscore = {\n    \"logloss\"  : log_loss(Y_train.iloc[validInds], pred_val_lgb),\n    \"f1_micro\" : f1_score(Y_train.iloc[validInds], np.argmax(pred_val_lgb, axis = 1), average = \"micro\")}\n\n## predict on test\npred_test_lgb = lgb_model.predict(df_test[trainCols])","79162207":"score","c7bd9b44":"params = {\n    'lambda_l1': 0.0,\n    'lambda_l2': 0.0,\n    'num_leaves': 111,\n    'feature_fraction': 0.9840000000000001,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'min_child_samples': 20,\n    'task': 'train',\n    'boosting_type': 'dart',\n    'objective': 'multiclass',\n    'num_class': 21,\n    'metric': 'multi_error',\n    'verbose': 1,\n    'learning_rate': 0.05,\n    'num_iterations': 1000\n}","785d44a0":"lgb_train2 = lgb.Dataset(X2_train.iloc[trainInds], Y_train.iloc[trainInds])\n\n# valid data\nlgb_val2 = lgb.Dataset(X2_train.iloc[validInds], Y_train.iloc[validInds])\n\n# train LGBM model\nbest_params, history = {}, []\nlgb_model2 = lgb.train(params, lgb_train2, \n                  valid_sets=lgb_val2,\n                  verbose_eval=10)","e889e69a":"## predict on valid\npred_val_lgb2 = lgb_model2.predict(X2_train.iloc[validInds])\n\n## evaluate\nscore = {\n    \"logloss\"  : log_loss(Y_train.iloc[validInds], pred_val_lgb2),\n    \"f1_micro\" : f1_score(Y_train.iloc[validInds], np.argmax(pred_val_lgb2, axis = 1), average = \"micro\")}\n\n## predict on test\npred_test_lgb2 = lgb_model2.predict(df_test[trainCols2])","efc14426":"score","01332f25":"from tensorflow.keras import regularizers\nnn_model1 = tf.keras.Sequential([\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dense(21, activation='softmax')\n])","0d205304":"nn_model1.compile(optimizer=tf.optimizers.Adam(lr=0.001), \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# early stopping\nes_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')","aa99d16a":"history = nn_model1.fit(Xnn2_train.iloc[trainInds], Ynn_train.iloc[trainInds], validation_data=(Xnn2_train.iloc[validInds], Ynn_train.iloc[validInds]),  epochs=100, batch_size=16, verbose=2,  callbacks=[es_cb])","51c841c1":"pred_val_nn1 = nn_model1.predict(Xnn2_train.iloc[validInds])\npred_test_nn1 = nn_model1.predict(df_test[nn_cols2])","073332f7":"log_loss(Y_train.iloc[validInds], pred_val_nn1), f1_score(Y_train.iloc[validInds], np.argmax(pred_val_nn1, axis = 1), average = \"micro\")","da1fc2f2":"from tensorflow.keras import regularizers\nnn_model = tf.keras.Sequential([\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(21, activation='softmax')\n])","f73a2083":"nn_model.compile(optimizer=tf.optimizers.Adam(lr=0.001), \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# early stopping\nes_cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')","f28ff3b5":"nn_cols = [\"dept_\"+str(i) for i in range(n_department)]\n\nXnn_train = df_train[nn_cols]\nY_train = df_train[\"department_id\"]\n","2bf3e5da":"history = nn_model.fit(Xnn_train.iloc[trainInds], Ynn_train.iloc[trainInds], validation_data=(Xnn_train.iloc[validInds], Ynn_train.iloc[validInds]),  epochs=100, batch_size=16, verbose=2,  callbacks=[es_cb])","ae7e4a93":"# \u5b66\u7fd2\u66f2\u7dda\nplt.plot(history.history['accuracy'], label=\"train\")\nplt.plot(history.history['val_accuracy'], label=\"validation\")\nplt.legend()\nplt.show()","3a52c24e":"# Loss\u306e\u5b66\u7fd2\u66f2\u7dda\nplt.plot(history.history['loss'], label=\"train\")\nplt.plot(history.history['val_loss'], label=\"validation\")\nplt.legend()\nplt.show()","fd2ac724":"# \u4e88\u6e2c\u3092\u884c\u3046\npred_val_nn = nn_model.predict(Xnn_train.iloc[validInds])\nXnn_test = df_test[nn_cols]\npred_test_nn = nn_model.predict(Xnn_test)","dc5b6245":"log_loss(Y_train.iloc[validInds], pred_val_nn), f1_score(Y_train.iloc[validInds], np.argmax(pred_val_nn, axis = 1), average = \"micro\")","005b7416":"rfc = RandomForestClassifier(random_state=0, n_estimators=1000, n_jobs=-1, verbose=1)\nrfc.fit(Xnn_train.iloc[trainInds], Y_train.iloc[trainInds])","a906a0aa":"pred_val_rfc = rfc.predict_proba(Xnn_train.iloc[validInds])\npred_test_rfc = rfc.predict_proba(Xnn_test)","0d7738c1":"log_loss(Y_train.iloc[validInds], pred_val_rfc), f1_score(Y_train.iloc[validInds], np.argmax(pred_val_rfc, axis = 1), average = \"micro\")","1065d056":"rfc2 = RandomForestClassifier(random_state=0, n_estimators=1000, n_jobs=-1, verbose=1)\nrfc2.fit(X_train.iloc[trainInds], Y_train.iloc[trainInds])","3445be54":"pred_val_rfc2 = rfc2.predict_proba(X_train.iloc[validInds])\npred_test_rfc2 = rfc2.predict_proba(df_test[trainCols])\nlog_loss(Y_train.iloc[validInds], pred_val_rfc2), f1_score(Y_train.iloc[validInds], np.argmax(pred_val_rfc2, axis = 1), average = \"micro\")","ad9d0359":"svc_clf = make_pipeline(StandardScaler(), SVC(gamma='auto', probability=True))\nsvc_clf.fit(Xnn_train.iloc[trainInds],  Y_train.iloc[trainInds])","cc41ab0b":"pred_val_svc = svc_clf.predict_proba(Xnn_train.iloc[validInds])\npred_test_svc = svc_clf.predict_proba(Xnn_test)","c0364284":"log_loss(Y_train.iloc[validInds], pred_val_svc), f1_score(Y_train.iloc[validInds], np.argmax(pred_val_svc, axis = 1), average = \"micro\")","e82cd7f1":"svc_clf2 = make_pipeline(StandardScaler(), SVC(gamma='auto', probability=True))\nsvc_clf2.fit(X_train.iloc[trainInds],  Y_train.iloc[trainInds])","83f8fb64":"pred_val_svc2 = svc_clf2.predict_proba(X_train.iloc[validInds])","70f61061":"pred_test_svc2 = svc_clf2.predict_proba(df_test[trainCols])\nlog_loss(Y_train.iloc[validInds], pred_val_svc2), f1_score(Y_train.iloc[validInds], np.argmax(pred_val_svc2, axis = 1), average = \"micro\")","06ab88f8":"len(validInds), len(trainInds)","e9dc9626":"# \u307e\u305a\u306f\u3001\u4e88\u6e2c\u5024(probability)\u3092\u4e00\u3064\u306b\u304f\u3063\u3064\u3051\u308b\nval_preds_all =  np.hstack([pred_val_lgb2,  pred_val_lgb,  pred_val_nn,  pred_val_nn1,   pred_val_svc,  pred_val_svc2,  pred_val_rfc,  pred_val_rfc2])\ntest_preds_all = np.hstack([pred_test_lgb2, pred_test_lgb, pred_test_nn, pred_test_nn1, pred_test_svc, pred_test_svc2, pred_test_rfc, pred_test_rfc2])","269d9834":"# Logistic Regression\u306e\u5b66\u7fd2\u3002\u3053\u3053\u3067\u306fOne-vs-Rest\u306e\u5b66\u7fd2\u306b\u306a\u3063\u3066\u3044\u308b\u3002\nclf = LogisticRegression(multi_class=\"ovr\").fit(val_preds_all, Y_train.iloc[validInds])","262aacf3":"# \u30c6\u30b9\u30c8d\u2212\u5f97\u305f\u306b\u5bfe\u3059\u308b\u4e88\u6e2c\u7d50\u679c\nprediction = clf.predict(test_preds_all)\nprediction","964680be":"# \u4e88\u6e2c\u7d50\u679c\u3092\u63d0\u51fa\u30c7\u30fc\u30bf\u306b\u307e\u3068\u3081\u308b\nsub[\"department_id\"] = prediction.astype(int)\nsub.to_csv(\"submission.csv\", index = False)\nsub.head()","8c6222a0":"sub.to_csv('submission.csv')","fe0f2d4d":"df.to_csv(\"dataframe_all.csv\")\nnp.savetxt(\"val_preds_all.csv\", val_preds_all, delimiter=\",\")\nnp.savetxt(\"test_preds_all.csv\", test_preds_all, delimiter=\",\")\nnp.save(\"trainInds\", trainInds)\nnp.save(\"validInds\", validInds)","770b8278":"df_test.to_csv(\"dataframe_test.csv\")","0d705c86":"## lgb for tf-idf\ntf-idf\u306e\u30ab\u30e9\u30e0\u3060\u3051\u4f7f\u3063\u3066\u5b66\u7fd2\u3057\u3066\u3044\u304f","14e862a3":"# Split train and validation\ntrain\u30c7\u30fc\u30bf\u3068\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u3092\u5206\u3051\u308b\n\u307e\u305f\u3001\u30ab\u30e9\u30e0\u306b\u3064\u3044\u3066\u3082\u3001\u30e2\u30c7\u30eb\u3067\u4f7f\u3044\u5206\u3051\u308b","01f6fb54":"# LightGBM\nLightGBM\u3067\u5b66\u7fd2\u3092\u884c\u3046\u3002\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u6c7a\u3081\u6253\u3061\n## LightGBM for fasttext\nfasttext\u306e\u30c7\u30fc\u30bf\u3092\u4f7f\u3063\u3066\u5b66\u7fd2\u3059\u308b","b07121e5":"## tf-idf\u306e\u307f\u306eNN","b2dc70f1":"# Neural network\n\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u3092fasttext+tf-idf, tf-idf\u306e\u307f\u306e\uff12\u3064\u306e\u30e2\u30c7\u30eb\u3067\u5b66\u7fd2\u3059\u308b\u3002\n\u904e\u5b66\u7fd2\u9632\u6b62\u3067DropOut\u306f\u5f37\u3081\u3002  \n\n## fasttext + tf-idf","d8837a52":"# SVM\nSVM\u3092\u4f7f\u3063\u3066\u5b66\u7fd2\u3059\u308b\u3002\u4eca\u56de\u306fscikit-learn\u306eSVC\u3092\u4f7f\u3046\u3002  \n\u30d1\u30e9\u30e1\u30fc\u30bf\u306f\u30c7\u30d5\u30a9\u30eb\u30c8\u3067\u3084\u308b\u3002(\u672c\u5f53\u306f\u8abf\u6574\u3057\u305f\u3044\uff09  \n\u5b66\u7fd2\u6642\u9593\u9577\u3081\u306a\u306e\u3067\u6ce8\u610f\n\n## tf-idf\u306e\u307f","ea27bc47":"## Randomforest with fasttext feature","ecf71000":"Split train and test","b4e7363a":"# Stacking\n- fasttext\n- tf-idf  \n\n\u3092\u5b66\u7fd2\u3057\u305f\u3001\u30e2\u30c7\u30eb\u305f\u3061\n- RandomForest\n- SVM\n- Neural Network\n- LightGBM  \n  \n\u3092\u30b9\u30bf\u30c3\u30ad\u30f3\u30b0\u3057\u3066\u3001\u6700\u7d42\u7684\u306a\u30b9\u30b3\u30a2\u3092LogisticRegression\u3067\u4e88\u6e2c\u3059\u308b\u3002  \n\u3053\u3053\u3067\u306f\u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\u30c7\u30fc\u30bf\u306e\u307f\u3092\u4f7f\u3046\u3002","65568b03":"# Load datasets","492f791b":"# tf-idf\ntf-idf\u306b\u3088\u308ascores\u3092\u4f5c\u308b\u3002","e53ee819":"## fasttext\u30d0\u30fc\u30b8\u30e7\u30f3","5a528897":"# RandomForest\nstacking\u306e\u305f\u3081\u306b\u3001RF\u3067\u3082\u5b66\u7fd2\u3055\u305b\u308b\u3002\n\u5b66\u7fd2\u30c7\u30fc\u30bf\u306fnn\u3068\u540c\u3058\u30d0\u30fc\u30b8\u30e7\u30f3\u3068\u3001fasttext\u306elightGBM\u306e\u30d0\u30fc\u30b8\u30e7\u30f3\u3092\u3084\u3063\u305f\n\n## tf-idf\u306e\u307f","76a50676":"# define parameters","03247545":"# FastText scores"}}