{"cell_type":{"4cfb5e39":"code","098a6d03":"code","e67cb84f":"code","0b324dc3":"code","392b0adc":"code","2ed7bb3b":"code","3e01f152":"code","1810da45":"code","650cdddc":"code","0211dbaa":"code","c736cd8d":"code","ff7d2e17":"code","b82fe036":"code","7e68bc90":"code","adc62d16":"code","9e0ad812":"code","3a7494df":"code","3452b3ca":"markdown","6c3ba281":"markdown","255fc659":"markdown","640144c7":"markdown","b97bb894":"markdown","38cfe588":"markdown","71ec4dc2":"markdown","2c4ad3fb":"markdown","0ad4c99d":"markdown"},"source":{"4cfb5e39":"import os, re, timeit, json, warnings, random\n\nimport numpy as np \nimport pandas as pd \nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom PIL import Image\nfrom matplotlib.image import imread\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Tensorflow version \" + tf.__version__)","098a6d03":"device_name = tf.test.gpu_device_name()\nif \"GPU\" not in device_name:\n    print(\"GPU device not found\")\nprint('Found GPU at: {}'.format(device_name))","e67cb84f":"# hyperparameters\nIMAGE_SIZE = 28\nEPOCHS = 25\nBATCH_SIZE = 32\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# storing data directory path\ndata = \"..\/input\/gtsrb-german-traffic-sign\"\n# reading Train.csv\ntrain_csv = pd.read_csv('..\/input\/gtsrb-german-traffic-sign\/Train.csv')\n# looking at the first 5 rows of Train.csv\ntrain_csv.head()","0b324dc3":"# reading Test.csv\ntest_csv = pd.read_csv('..\/input\/gtsrb-german-traffic-sign\/Test.csv')\n# looking at the first 5 rows of Test.csv\ntest_csv.head()","392b0adc":"# identifying number of unique target labels in the train dataset\nCLASSES = train_csv[\"ClassId\"].nunique()\nprint(\"Number of unique classes: \", train_csv[\"ClassId\"].nunique())\n\n# visualising how many images of each class exist in the train dataset\nplt.figure(figsize=(20,10))\ntrain_csv[\"ClassId\"].value_counts(sort=True).plot.bar()","2ed7bb3b":"# creating a list of train and test images path\ntrain_img_path = data+'\/'+train_csv[\"Path\"].values\ntest_img_path = data+'\/'+test_csv[\"Path\"].values\n\n# a function that will plot a grid of random 9 images and accepts image path\ndef plot_img(img_path):\n    plt.figure(figsize=(15,15))\n    for i in range(1,10):\n        plt.subplot(3,3,i)\n        random_img_path = random.choice(img_path)\n        rand_img = imread(random_img_path)\n        plt.imshow(rand_img)\n        plt.grid(b=None)\n        plt.xlabel(rand_img.shape[1], fontsize = 20)#width of image\n        plt.ylabel(rand_img.shape[0], fontsize = 20)#height of image\n\n# visualising 9 train images\nplot_img(train_img_path)\nplt.suptitle('Train Images')\nplt.show()\n# visualising 9 test images\nplot_img(test_img_path)\nplt.suptitle('Test Images')\nplt.show()","3e01f152":"def img_to_array(data_path, csv):\n    data = []\n    labels= csv.ClassId\n    for i in range(len(data_path)):\n        image = tf.io.read_file(data_path[i])\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.cast(image, tf.float32)\/256.0\n        image = tf.image.resize(image, [IMAGE_SIZE, IMAGE_SIZE])\n        # adding image to arrays\n        data.append(image)\n        \n    return data, labels\n\ndata, labels = img_to_array(train_img_path, train_csv)\n\n# changing the list to a numpy array\nX = np.array(data)\ny = np.array(labels)","1810da45":"# before splitting\nprint(\"----Before Splitting----\")\nprint(\"X.shape\", X.shape)\nprint(\"y.shape\", y.shape)\n\n# splitting the train data into train and validation\nX_train, X_val, y_train, y_val = train_test_split( X, y, test_size=0.20, random_state=42, shuffle=True)\n\n# checking the shape of train and validation data after splitting\nprint(\"----After Splitting----\")\nprint(\"X_train.shape\", X_train.shape)\nprint(\"X_valid.shape\", X_val.shape)\nprint(\"y_train.shape\", y_train.shape)\nprint(\"y_valid.shape\", y_val.shape)\n\n# one-hot encoding the target labels\ny_train = tf.keras.utils.to_categorical(y_train, CLASSES)\ny_val = tf.keras.utils.to_categorical(y_val, CLASSES)\n# checking the shape of train and validation data after one-hot encoding\nprint(\"----After 1-hot encoding----\")\nprint(\"y_train.shape\", y_train.shape)\nprint(\"y_valid.shape\", y_val.shape)","650cdddc":"# using Cyclic Learning Rate\n! git clone https:\/\/github.com\/bckenstler\/CLR.git","0211dbaa":"from keras.callbacks import *\nfrom CLR.clr_callback import *\n\n# using the triangular learning rate policy\nclr_triangular = CyclicLR(mode='triangular')","c736cd8d":"learning_rate_reduction = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor = 'val_loss', \n    patience = 3, \n    verbose = 1, \n    factor = 0.5, \n    min_lr = 1e-6\n)","ff7d2e17":"# applying random transformations to the images\naugment = tf.keras.preprocessing.image.ImageDataGenerator(\n    rotation_range=10,\n    zoom_range=0.15,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    shear_range=0.15,\n    horizontal_flip=False,\n    vertical_flip=False,\n    fill_mode=\"nearest\")\n\n# defining model-resnet50\nresnet_model = tf.keras.applications.ResNet50(\n    weights='imagenet',\n    include_top=False\n)\n\nx = resnet_model.output\nx = tf.keras.layers.GlobalAveragePooling2D()(x)\nx = tf.keras.layers.Dense(512,activation='relu')(x)\npredictions = tf.keras.layers.Dense(\n    CLASSES,\n    activation='softmax'\n)(x)\n\nresnet50_model = tf.keras.models.Model(\n    inputs= resnet_model.input, \n    outputs=predictions\n)\n\nresnet50_model.compile(\n    loss='categorical_crossentropy', \n    optimizer=tf.keras.optimizers.SGD(lr=1e-4, momentum=0.9),\n    metrics=['accuracy']\n)\n\nresnet50_model.summary()","b82fe036":"startTime = timeit.default_timer()\nhistory_lrr = resnet50_model.fit(\n    augment.flow(X_train, y_train, batch_size=BATCH_SIZE),\n    epochs=50, \n    validation_data=(X_val, y_val),\n    callbacks = [learning_rate_reduction]\n)\nelapsedTime = timeit.default_timer() - startTime\nprint(\"Time taken for the Network to train : \",elapsedTime)","7e68bc90":"#Display of the accuracy and the loss values\nplt.figure(0)\nplt.plot(history_lrr.history['accuracy'], label='training accuracy')\nplt.plot(history_lrr.history['val_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history_lrr.history['loss'], label='training loss')\nplt.plot(history_lrr.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()","adc62d16":"startTime = timeit.default_timer()\nhistory_clr = resnet50_model.fit(\n    augment.flow(X_train, y_train, batch_size=BATCH_SIZE),\n    epochs=50, \n    validation_data=(X_val, y_val),\n    callbacks = [clr_triangular]\n)\nelapsedTime = timeit.default_timer() - startTime\nprint(\"Time taken for the Network to train : \",elapsedTime)","9e0ad812":"#Display of the accuracy and the loss values\nplt.figure(0)\nplt.plot(history_clr.history['accuracy'], label='training accuracy')\nplt.plot(history_clr.history['val_accuracy'], label='val accuracy')\nplt.title('Accuracy')\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.legend()\n\nplt.figure(1)\nplt.plot(history_clr.history['loss'], label='training loss')\nplt.plot(history_clr.history['val_loss'], label='val loss')\nplt.title('Loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()","3a7494df":"X_test, y_test = img_to_array(test_img_path, test_csv)\n\n# changing the list to a numpy array\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n# predictions\nprediction = resnet50_model.predict(X_test).argmax(axis=1)\n\n# accuracy\nacc = accuracy_score(y_test, prediction)\nprint(\"Accuracy: \", acc)","3452b3ca":"# GPU Detection","6c3ba281":"# Augmenting and Modeling","255fc659":"# Loading Dataset","640144c7":"# Callbacks","b97bb894":"# Importing Libraries","38cfe588":"## Using `clr` (Cyclic Learnig Rate) as callback","71ec4dc2":"## Using `learning_rate_reduction` as callback","2c4ad3fb":"# Testing the Model","0ad4c99d":"# Configuring and Visualising"}}