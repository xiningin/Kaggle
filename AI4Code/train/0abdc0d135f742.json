{"cell_type":{"4045f388":"code","be0291fd":"code","1b4d5006":"code","850ebac4":"code","b2aa7391":"code","2c8e5cf4":"code","719b3d31":"code","a577d691":"code","1bae3f1f":"code","759deaee":"code","23be602f":"code","5828e58b":"code","fcbb1214":"code","6310690b":"code","54ab4732":"code","626e9aa7":"code","c1e2f0c4":"markdown"},"source":{"4045f388":"DATA_PATH = '\/kaggle\/input\/shakespeare-text\/text.txt'","be0291fd":"import numpy as np\nimport torch.nn as nn\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom torch.nn import functional as F\nfrom tqdm import tqdm_notebook\nfrom collections import Counter ","1b4d5006":"fp = open(DATA_PATH,'r')\ntxt = fp.read()","850ebac4":"txt = txt.replace('$','')\ntxt = txt.replace('&','')","b2aa7391":"print(f'total number of characters in corpus: {len(txt)}')\nprint(f'total number of unique characters in corpus: {len(set(txt))}')","2c8e5cf4":"unique_txt = set(txt)","719b3d31":"character_count = Counter(txt)\n#character_count","a577d691":"char2idx = {char: key for key,char in enumerate(sorted(unique_txt))}\n\nidx2char = [c for c in sorted(unique_txt)]","1bae3f1f":"batch_size = 64\nseq_size = 100\ncoded_text = [char2idx[c] for c in txt]\nn_vocab = len(unique_txt)","759deaee":"def batch_generate(text,batch_size= batch_size,seq_size = seq_size):\n    #print(f'vocab_size: {len(char2idx)}')    \n    total_batches = int(len(coded_text)\/(batch_size*seq_size))\n    input_txt = text[:total_batches*batch_size*seq_size]\n    output_txt = np.zeros_like(input_txt)\n    output_txt[:-1] = input_txt[1:]\n    output_txt[-1] = input_txt[0]\n    input_txt = np.reshape(input_txt,(batch_size,-1))\n    output_txt = np.reshape(output_txt,(batch_size,-1))\n    return input_txt,output_txt","23be602f":"def get_batches(data,target,batch_size,seq_size):\n    num_batches = np.prod(data.shape) \/\/ (seq_size * batch_size)\n    for i in range(0, num_batches * seq_size, seq_size):\n        yield data[:, i:i+seq_size], target[:, i:i+seq_size]","5828e58b":"class RNNModule(nn.Module):\n    def __init__(self, n_vocab, seq_size, embedding_size, lstm_size):\n        super(RNNModule, self).__init__()\n        self.seq_size = seq_size\n        self.lstm_size = lstm_size\n        self.embedding = nn.Embedding(n_vocab, embedding_size)\n        self.lstm = nn.LSTM(embedding_size,\n                            lstm_size,\n                            batch_first=True)\n        self.dense = nn.Linear(lstm_size, n_vocab)\n        \n    def forward(self, x, prev_state):\n        embed = self.embedding(x)\n        output, state = self.lstm(embed, prev_state)\n        logits = self.dense(output)\n\n        return logits, state\n    \n    def zero_state(self, batch_size):\n        return (torch.zeros(1, batch_size, self.lstm_size),\n                torch.zeros(1, batch_size, self.lstm_size))","fcbb1214":"def get_loss_and_train_op(net, lr=0.001):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n    return criterion, optimizer","6310690b":"def main():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    net = RNNModule(n_vocab,seq_size,embedding_size=14,lstm_size=128).to(device)\n    criterion, optimizer = get_loss_and_train_op(net, 0.01)\n    iteration = 0\n    data,target = batch_generate(coded_text)\n    for e in tqdm_notebook(range(10)):\n        batches = get_batches(data,target,batch_size,seq_size)\n        state_h,state_c = net.zero_state(batch_size)\n        state_h = state_h.to(device)\n        state_c = state_c.to(device)\n        for x,y in batches:\n            iteration+=1\n            net.train()\n            optimizer.zero_grad()\n            x = torch.tensor(x).to(device)\n            y = torch.tensor(y).to(device)\n            logits, (state_h, state_c) = net(x, (state_h, state_c))\n            loss = criterion(logits.transpose(1, 2), y)\n            state_h = state_h.detach()\n            state_c = state_c.detach()\n            loss_value = loss.item()\n            loss.backward()\n            _ = torch.nn.utils.clip_grad_norm_(\n                net.parameters(), 5)\n            optimizer.step()\n            \n            if iteration % 100 == 0:\n                print('Epoch: {}\/{}'.format(e, 200),\n                      'Iteration: {}'.format(iteration),\n                      'Loss: {}'.format(loss_value))\n                \n            if iteration%2000 ==0:\n                predict(device,net,['I',' ','a','m',' '],n_vocab,char2idx,idx2char)\n                torch.save(net.state_dict(),\n                           'model-{}.pth'.format(iteration))","54ab4732":"def predict(device, net, words, n_vocab, vocab_to_int, int_to_vocab, top_k=5):\n    net.eval()\n    state_h, state_c = net.zero_state(1)\n    state_h = state_h.to(device)\n    state_c = state_c.to(device)\n    for w in words:\n        ix = torch.tensor([[vocab_to_int[w]]]).to(device)\n        output, (state_h, state_c) = net(ix, (state_h, state_c))\n    \n    _, top_ix = torch.topk(output[0], k=top_k)\n    choices = top_ix.tolist()\n    choice = np.random.choice(choices[0])\n\n    words.append(int_to_vocab[choice])\n    \n    for _ in range(100):\n        ix = torch.tensor([[choice]]).to(device)\n        output, (state_h, state_c) = net(ix, (state_h, state_c))\n\n        _, top_ix = torch.topk(output[0], k=top_k)\n        choices = top_ix.tolist()\n        choice = np.random.choice(choices[0])\n        words.append(int_to_vocab[choice])\n\n    print(''.join(words))","626e9aa7":"main()","c1e2f0c4":"### Character Level Text Generation\n### Please UPVOTE if you like this kernel"}}