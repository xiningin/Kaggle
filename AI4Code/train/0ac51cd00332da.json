{"cell_type":{"a58d08f0":"code","68c5e70b":"code","60b77230":"code","30a14cda":"code","58df6642":"code","11bcfc00":"code","b4d5b667":"code","b9f144a0":"code","eff7dda0":"code","f8454f85":"code","84a25134":"code","b5f6f69c":"code","2449f1a8":"code","7a74714b":"code","b2a3ed12":"code","c87cb45d":"code","460bf8b8":"code","9fa8d983":"code","d7239b8a":"code","1d4d2ec8":"code","1bbe3e82":"code","0d6185c2":"code","769b9ffc":"markdown","56c22f24":"markdown","92d8d297":"markdown","701dc050":"markdown","dead362c":"markdown","76de0199":"markdown","0409eb37":"markdown","48fbe42f":"markdown","e46da9a4":"markdown","a83425ea":"markdown","278df538":"markdown"},"source":{"a58d08f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import mean_squared_error as rmse\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport collections\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import ElasticNet, Lasso\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","68c5e70b":"data = pd.read_csv(\"..\/input\/jjoijl\/prices.csv\")\ntemp = data.head(3)\ntemp.style.background_gradient(cmap=\"Oranges\")","60b77230":"print(data.info(3))\ntemp = data.describe()\ntemp.style.background_gradient(cmap=\"ocean_r\")","30a14cda":"X = data.drop([\"price\"],axis=1)\ny = data[\"price\"]","58df6642":"y_1 = np.log(y)  # Price columns ","11bcfc00":"sns.set_style(\"darkgrid\")\nfig, axarr = plt.subplots(1, 2, figsize=(28,12))\nsns.histplot(data[\"price\"], bins=100,kde=True,ax=axarr[0])\naxarr[0].set_title(\"Price Distribution\", color=\"#f60000\", size=20)\naxarr[0].set_ylabel('Count', fontsize = 20, color=\"#2d242e\")\naxarr[0].set_xlabel('Price', fontsize = 20, color=\"#2d242e\")\n\n\nsns.histplot(y_1, bins=80, kde=True, ax=axarr[1])\naxarr[1].set_title(\"Price distribution after log transformation\", color=\"#f60000\", size=20)\naxarr[1].set_ylabel('Count', fontsize = 20, color=\"#2d242e\")\naxarr[1].set_xlabel('Log_price', fontsize = 20, color=\"#2d242e\")\nfig.show()","b4d5b667":"print(\"Skewness: %f\" % y.skew())\nprint(\"Kurtosis: %f\" % y.kurt())\nprint(\"*\"*50)\nprint(\"Skewness after log transformation: %f\" % y_1.skew())\nprint(\"Kurtosis after log transformation: %f\" % y_1.kurt())","b9f144a0":"corr = data.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.set(style=\"dark\")\nf, ax = plt.subplots(figsize=(18, 12))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.4, center=0,\n            square=True, linewidths=.8, cbar_kws={\"shrink\": .5}, annot=True ,fmt='.1%')","eff7dda0":"plt.figure(figsize=(10,10))\ncols = ['lot_area', 'living_area', 'num_bedrooms', 'num_bathrooms', \n        'year_built']\nsns.pairplot(data[cols], height = 2.5)\nplt.savefig(\"squares.png\")\nplt.show();","f8454f85":"col = ['lot_area', 'living_area', 'year_built']\nplt.style.use(\"ggplot\")\nfig, axs = plt.subplots(ncols = 1, nrows = 3, figsize=(8,24))\nplt.subplots_adjust(right=1.5)\nfor i, feature in enumerate(col, 1):\n    plt.subplot(3,1,i)\n    sns.regplot(x = \"price\", y= feature, data = data)\n    plt.xlabel('{}'.format(feature), size=15)\n    \n    for j in range(2):\n        plt.tick_params(axis='x', labelsize=12)\n        plt.tick_params(axis='y', labelsize=12)\nplt.show()","84a25134":"def detect_outliers(df,features):\n    outlier_indices = []\n    \n    for c in features:\n        #1st quartile\n        Q1=np.percentile(df[c],25)\n        #3st quartile\n        Q3=np.percentile(df[c],75)\n        #IQR \n        IQR=Q3-Q1\n        #outlier step\n        outlier_step=IQR*1.5\n        #detect outlier\n        outlier_list_col=df[(df[c] < Q1-outlier_step) | (df[c] > Q3 + outlier_step)].index\n        #store indeces\n        outlier_indices.extend(outlier_list_col)\n    outlier_indices = collections.Counter(outlier_indices)\n    multiple_outliers = list(i for i , v in outlier_indices.items() if v > 2)\n        \n    return multiple_outliers\n\noutliers_index = detect_outliers(data,['price', 'lot_area', 'living_area', 'num_floors', 'num_bedrooms',\n       'num_bathrooms', 'waterfront', 'year_built', 'year_renovated'])\nprint(len(outliers_index))","b5f6f69c":"data = data.drop(outliers_index)\ndata = pd.DataFrame(data)\n\nX = data.drop([\"price\"],axis=1)\ny = np.log(data[\"price\"])","2449f1a8":"fig, ax = plt.subplots(2, 2, figsize=(18,12))\nax[0,0].set_title(\"Before log Transformation\")\nsns.distplot(data[\"price\"], fit=norm, ax= ax[0,0])\nstats.probplot(data[\"price\"], plot=ax[1,0])\n\nax[0,1].set_title(\"After log Transformation\")\nsns.distplot(y_1, fit=norm, ax= ax[0,1])\nstats.probplot(y_1, plot=ax[1,1])\nplt.show()","7a74714b":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, random_state=4)\nn_folds = 5\n\n\"\"\"root-mean-square error (RMSE) is a frequently used measure of the differences between \nvalues (sample or population values) predicted by a model or an estimator and the values observed. \"\"\"\n\ndef rmsle_cv(model):\n    kfold  = KFold(n_folds, shuffle=True, random_state=1).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kfold))\n    return(rmse)","b2a3ed12":"\"\"\"\n#Lasso\ngrid_lasso={'alpha' : [0.05,0.005, 0.0005, 0.00005],\n            'max_iter': [100,500,1000]}\nlasso=Lasso()\nlasso_cv=GridSearchCV(lasso, grid_lasso, cv=5)\nlasso_cv.fit(X_train,y_train)\nprint(\"best hpyerparameters :\",lasso_cv.best_params_)\nprint(\"Score :\",lasso_cv.best_score_)\n\n#Enet\ngrid_enet={'alpha' : [0.05,0.005, 0.0005, 0.00005],\n          'l1_ratio': np.linspace(0.1,0.9, 8)}\nenet=ElasticNet()\nenet_cv=GridSearchCV(enet, grid_enet, cv=5)\nenet_cv.fit(X_train,y_train)\nprint(\"best hpyerparameters :\",enet_cv.best_params_)\nprint(\"Score :\",enet_cv.best_score_)\n\n#XGBregressor\ngrid_xgb={'reg_lambda' : np.linspace(0.1,1,9),\n          'reg_alpha' :  np.linspace(0.1,0.6,10),\n          'min_child_weight' : np.linspace(1,4,9),\n          'gamma' : np.linspace(0.01,1,10),\n          'subsample':[0.3,0.4,0.5]}\nxgb=XGBRegressor()\nxgb_cv=GridSearchCV(xgb, grid_xgb, cv=5)\nxgb_cv.fit(X_train,y_train)\nprint(\"best hpyerparameters :\",xgb_cv.best_params_)\nprint(\"Score :\",xgb_cv.best_score_)\n\"\"\"","c87cb45d":"lasso = make_pipeline(StandardScaler(), Lasso(alpha =0.0005,\n                                              max_iter =100,\n                                              random_state=1))\n\nENet = make_pipeline(StandardScaler(), ElasticNet(alpha=0.0005,\n                                                  l1_ratio=0.1,\n                                                  random_state=1))\n\nmodel_xgb = XGBRegressor(gamma=0.045, \n                         learning_rate=0.05, max_depth=3, \n                         min_child_weight=1.7, n_estimators=2000,\n                         reg_alpha=0.4, reg_lambda=0.8,\n                         subsample=0.5,\n                         random_state =1)\n\nlin_reg = make_pipeline(StandardScaler(), LinearRegression())","460bf8b8":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.6f} (rmse) \\n\".format(score.mean()))\n\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.5f} (rmse) \\n\".format(score.mean()))\n\nscore = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.6f} (rmse) \\n\".format(score.mean()))\n\nscore = rmsle_cv(lin_reg)\nprint(\"Lineer Reg. score: {:.6f} (rmse) \\n\".format(score.mean()))","9fa8d983":"from datetime import datetime\nK = 5\nkfold = KFold(n_splits=K, shuffle=True, random_state=1)","d7239b8a":"models = {'LassoCV': lasso, \n          'ElasticNetCV': ENet,\n          'XGBoostRegressor': model_xgb,\n          'LineerRegression': lin_reg}\n\npredictions = {}\nscores = {}\ndef cv_rmse(model, X=X_train, y=y_train):    \n    return np.sqrt(-cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=kfold))\n\nfor name, model in models.items():\n    \n    model.fit(X_train, y_train)\n    predictions[name] = np.expm1(model.predict(X_test))\n    \n    score = cv_rmse(model, X=X_test, y=y_test)\n    scores[name] = (score.mean(), score.std())\n    \n    print(' {} Mean RMSE: {:.6f} \/ Std: {:.6f}\\n'.format(name, scores[name][0], scores[name][1]))","1d4d2ec8":"fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(14, 24))\nplt.subplots_adjust(top=1, right=1.5)\n\nfor i, model in enumerate(models, 1):\n    plt.subplot(2, 2, i)\n    plt.scatter(predictions[model], np.expm1(y_test))\n    plt.plot([210000, 2050000], [0, 3300000], '--m', linewidth=3)\n    plt.plot([0, 2700000], [0, 2700000], '--m', linewidth=1)\n    \n\n    plt.xlabel('{} Predictions (y_pred)'.format(model), size=20)\n    plt.ylabel('Real Values (y_train)', size=20)\n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    plt.title('{} Predictions vs Real Values'.format(model), size=25)\n\nplt.show()","1bbe3e82":"def blend_predict(X):\n    return ((0.2  * ENet.predict(X)) + \n            (0.2 * lasso.predict(X)) +\n            (0.5 * model_xgb.predict(X)) +\n            (0.1  * lin_reg.predict(X)))\n\nblended_score = rmse(y_train, blend_predict(X_train))\nprint('Blended RMSE: {}'.format(blended_score))","0d6185c2":"df = pd.DataFrame({'Regression Models': [\"lasso\", \"Elastic_net\", \"Xgboost\",\"LineerRegression\"],\n                   'Raw Scores':[0.5230, 0.5250, 0.37537, 0.5423],\n                   'After Optimizations': [0.349099,0.34910 ,0.341628, 0.352849],\n                   'With Blend Method':[0.098138,0.098138,0.098138,0.098138]})\n\ndetas = df.set_index('Regression Models')\ndetas.style.background_gradient(cmap=\"Oranges\")","769b9ffc":"\n<span style=\"color:#FA7066; font-size:35px;\">   Tricks 4  \"Find outlier and handle.\" <\/span>\n1. We want to use outliers for some projects like spam detect, fraud detection or disease diagnosis. But this case we dont want it. \n2. regression models is very sensetive the outliers.\n3. For this data outliers reduce the our scores.\n\n<span style=\"color:#FA7066; font-size:35px;\">   Tricks 5  \"IQR * \"X\" \" <\/span>\n1. x can be 1.5 or 3 generally. But for large ranges it makes more sense to use 3.\n2. In this case (IQR * 1.5) find \"470\" row. But with (IQR * 3) we find only \"82\" row. This differences affects the score a lot.","56c22f24":"<span style=\"color:#FA7066; font-size:35px;\">  Trick 9 \"Blend the models\" <\/span>\n1. Many times all models predict same value for some parts. But complicated part can be different for each model when models predictions are blended, models can work more effective ","92d8d297":"<span style=\"color:crimson; font-size:35px;\">   Tricks 1 \"Missing value operations\"<\/span>\n1. But for this dataset we have not any missing value.","701dc050":"<span style=\"color:#FA7066; font-size:35px;\">  Trick 3 \"Observe the data with statistical values and different type of visualizations\" <\/span>","dead362c":"<span style=\"color:crimson; font-size:35px;\">  Trick 8  \"Scaler type\"\" <\/span>\n1. There are many different scaler algorithm. You can implement the best scaler method according your data. For example;\n* Robust Scaler works similarly to Normalization. It may give better results on data with outliers.\n* StandardScaler removes the mean and scales the data to unit variance. However, the outliers have an influence when computing the empirical mean and standard deviation which shrink the range of the feature values. StandardScaler therefore cannot guarantee balanced feature scales in the presence of outliers.","76de0199":"<span style=\"color:#FA7066; font-size:35px;\">  Trick 7 \"Parameters optimization\" <\/span>\n1. Models need to be optimized. You can increase your score effectively with this way.","0409eb37":"# compare the models scores ","48fbe42f":"# With this notebook \u0131 will build very basic regression models. But scores will increase with very basic tricks. You can add more complex algorithm to reach more high scores\n# such as ;\n1. Stacking Algorithm,\n2. Feature Engineering.\n3. more complex models\n4. deep statistical.\n","e46da9a4":"<span style=\"color:#FA7066; font-size:35px;\">  Trick 6 \"Use cross validation\" <\/span>","a83425ea":"<span style=\"color:#FA7066; font-size:35px;\">  Tricks 2 \"Transformation\" <\/span>\n1. for a good regression models we have some assumptions like normality.\n2. You can see the saleprice distributions. This is not a normal distribution. Make it normal.\n3. You can use some transformations methods which are log, box cox and Arcsine transformation. (the link is in the references block)\n4. You can check this proses with visualization and skewness, kurtosis values.","278df538":"# references\n1. https:\/\/en.wikipedia.org\/wiki\/Root-mean-square_deviation\n2. https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python (Awesome Notebook-1)\n3. https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard (awesome notebook-2)\n4. https:\/\/www.kaggle.com\/gunesevitan\/house-prices-advanced-stacking-tutorial (awesome notebook-3)\n5. https:\/\/pro.arcgis.com\/en\/pro-app\/latest\/help\/analysis\/geostatistical-analyst\/box-cox-arcsine-and-log-transformations.htm\n6. https:\/\/www.youtube.com\/watch?v=nk2CQITm_eo&t=1387s&ab_channel=StatQuestwithJoshStarmer\n7. https:\/\/machinelearningmastery.com\/stacking-ensemble-machine-learning-with-python\/ (about stacking)\n8. http:\/\/www.sthda.com\/english\/articles\/40-regression-analysis\/167-simple-linear-regression-in-r\/"}}