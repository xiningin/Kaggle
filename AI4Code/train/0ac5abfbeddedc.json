{"cell_type":{"1c578664":"code","52ff9747":"code","9019846b":"code","2ea0ff48":"code","05b9a226":"code","12a96f05":"code","b734c528":"markdown","4a2fd471":"markdown","14fcd3cf":"markdown","a463ed2a":"markdown","e18f626a":"markdown","40319c8c":"markdown","1bdea677":"markdown","74347246":"markdown"},"source":{"1c578664":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev","52ff9747":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport torch\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\n\nimport pytorch_lightning as pl\n\nimport warnings\nwarnings.simplefilter('ignore')","9019846b":"class Config:\n    file_path = '..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv'\n    lr = 1e-5\n    max_len = 64\n    train_bs = 64\n    valid_bs = 32\n    train_pcent = 0.99\n    num_workers = 8\n    bert_model = 'bert-base-uncased'\n    tokenizer = transformers.BertTokenizer.from_pretrained(bert_model, do_lower_case=True)","2ea0ff48":"class BertData(Dataset):\n    def __init__(self, review, target):\n        self.review = review\n        self.target = target\n        self.tokenizer = Config.tokenizer\n        self.max_len = Config.max_len\n\n    def __len__(self):\n        return len(self.review)\n    \n    def __getitem__(self, idx):\n        review = str(self.review[idx])\n        review = ' '.join(review.split())\n\n        inputs = self.tokenizer.encode_plus(\n            review,\n            None,\n            add_special_tokens = True,\n            max_length = Config.max_len,\n            pad_to_max_length=True,\n            truncation='longest_first'\n        )\n\n        ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n        targets = torch.tensor(self.target[idx], dtype=torch.float)\n\n        return {'ids': ids,\n                'mask': mask,\n                'token_type_ids': token_type_ids,\n                'targets': targets\n                }","05b9a226":"class BERTModel(pl.LightningModule):\n    def __init__(self) -> None:\n        super(BERTModel, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(Config.bert_model)\n        self.drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 1)\n        self.all_targets = []\n        self.train_loss_fn = nn.BCEWithLogitsLoss()\n        self.valid_loss_fn = nn.BCEWithLogitsLoss()\n    \n    def forward(self, ids, mask, token_type_ids) -> torch.Tensor:\n        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n        output = self.drop(output)\n        output = self.out(output)\n        return output\n    \n    def prepare_data(self) -> None:\n        # Load the data, encode, shuffle and split it\n        data = pd.read_csv(Config.file_path, encoding='latin-1', names=['target', 'id', 'date', 'query', 'username', 'text'])\n        data = data[['target', 'text']]\n        data['target'] = data['target'].map({4: 1, 0: 0})\n        data = data.sample(frac=1).reset_index(drop=True)\n        \n        nb_training_samples = int(Config.train_pcent * len(data))\n\n        self.train_data = data[:nb_training_samples]\n        self.valid_data = data[nb_training_samples:]\n\n        # Make Training and Validation Datasets\n        self.training_set = BertData(\n            review=self.train_data['text'].values,\n            target=self.train_data['target'].values\n        )\n\n        self.validation_set = BertData(\n            review=self.valid_data['text'].values,\n            target=self.valid_data['target'].values\n        )\n\n    def train_dataloader(self):\n        train_loader = DataLoader(\n            self.training_set,\n            batch_size=Config.train_bs,\n            shuffle=False,\n            num_workers=Config.num_workers,\n        )\n        return train_loader\n\n    def val_dataloader(self):\n        val_loader = DataLoader(\n            self.validation_set,\n            batch_size=Config.valid_bs,\n            shuffle=False,\n            num_workers=Config.num_workers,\n        )\n        return val_loader\n    \n    def training_step(self, batch, batch_idx):\n        ids = batch['ids'].long()\n        mask = batch['mask'].long()\n        token_type_ids = batch['token_type_ids'].long()\n        targets = batch['targets'].float()\n\n        outputs = self(ids=ids, mask=mask, token_type_ids=token_type_ids)\n\n        train_loss = self.train_loss_fn(outputs, targets.view(-1, 1))\n        return {'loss': train_loss}\n    \n    def validation_step(self, batch, batch_idx):\n        ids = batch['ids'].long()\n        mask = batch['mask'].long()\n        token_type_ids = batch['token_type_ids'].long()\n        targets = batch['targets'].float()\n\n        outputs = self(ids=ids, mask=mask, token_type_ids=token_type_ids)\n\n        self.all_targets.extend(targets.cpu().detach().numpy().tolist())\n        \n        valid_loss = self.valid_loss_fn(outputs, targets.view(-1, 1))\n        return {'val_loss': valid_loss}\n    \n    def configure_optimizers(self):\n        param_optimizer = list(self.named_parameters())\n        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n        optimizer_parameters = [\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.001,\n            },\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        return transformers.AdamW(optimizer_parameters, lr=Config.lr)","12a96f05":"# Run the training loop\nmodel = BERTModel()\ntrainer = pl.Trainer(max_epochs=1, tpu_cores=1)\ntrainer.fit(model)","b734c528":"Now just run the code with either GPU (`gpus=1`) or TPU (`tpu_cores=1`)\n\nI am running the code only for 1 training and validation epoch(s) because the data is huge just like the model and it takes about **2 hours** to train a single epoch (just the training, not even the validation) on TPU.\n\nThe same one epoch on Kaggle's GPU takes **2.5 hours** (so TPU saves you good 30 minutes!).\n\nIf you have some some Cloud Platform Credits of sustained training on more then one GPUs \/ TPU Cores, feel free to increase the number of epochs.","4a2fd471":"Declare a Config class to define important variables and parameters.","14fcd3cf":"# Sentiment Classification using BERT and PyTorch Lightning on TPU!\n\n![](https:\/\/miro.medium.com\/max\/876\/0*ViwaI3Vvbnd-CJSQ.png)\n\n\nHello kind person! Welcome to my notebook on how to do Sentiment Analysis using BERT and PyTorch Lightning while also using the Power of TPUs!\n\n#### Feel free to fork my Notebook and don't forget to leave a vote!","a463ed2a":"I hope you like this notebook, don't forget to leave a Vote!","e18f626a":"### `BertModel` is our main class and we are inheriting `pl.LightningModule` and then doing the following:\n\n* Defining the Model: In the `__init__()` and `forward()` function we are defining the model architecture and the model forward pass logic.\n\n* Preparing Data: In the `prepare_data()` function we are reading, shuffling, splitting and converting the data into Dataset form.\n\n* Training and Validation Dataloaders: In these functions (`train_dataloader()` and `val_dataloader()`) we are taking the train and val datasets and returning them in a DataLoader.\n\n* Training and Validation Steps: In these functions (`training_step()` and `validation_step()`) we are defining the training and validation logic to be done on respective data for one epoch. Backward and Optimizer logic doesn't need to be included.\n\n* Configuring Optimizer: This function will return the optimizer (which we would want to optimize the model on). For BERT Base Uncased model, we won't be optimizing all parameters but only some of them.","40319c8c":"This is our Custom Dataset class to be used for BERT task.","1bdea677":"Below are a few points that will help you understand how the `__getitem__()` function work:\n\n* We first make sure that the review is one big string.\n* Then we send our `review` to the tokenizeer to have it encoded. We also include special tokens and padding.\n* The tokenizer returns a dictionary with 3 values:\n    * `input_ids`: These are token indices (encoded review)\n    * `attention_mask`: This mask indicates which tokens are to be attended (think of paying special attention)\n    * `token_type_ids`: These will be used to indicate where a sentence ends and where one starts ('CLS' and 'SEP' ones)\n    \n    \n* Now we are going to convert the above variables along with `target` variable (which is the sentiment) to Tensors and return them in a dictionary.","74347246":"Installing PyTorch XLA Frameworks so that we can work with TPUs."}}