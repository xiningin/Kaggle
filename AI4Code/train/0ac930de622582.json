{"cell_type":{"c97cf63d":"code","53613124":"code","60591242":"code","024eb5c7":"code","7a9d5e74":"code","9831280b":"code","afbf3fb4":"code","4da9b0ef":"code","c75b0676":"code","f4bd5788":"code","62ee88b1":"code","fba4c4eb":"code","3d2c224f":"code","380d50f2":"code","19262b3a":"code","7cdfb452":"code","37f6eef0":"code","9cd85bf0":"code","5d641af6":"code","73f0270b":"code","d47051ba":"code","f4c739c2":"code","52ee3a21":"code","3ae4a764":"code","fa41b76d":"code","7ad56408":"markdown"},"source":{"c97cf63d":"!pip install tensorflow_addons","53613124":"import tensorflow as tf\nfrom tensorflow.keras.layers import LSTM, Dropout, Dense, GRU\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.models import Sequential\nfrom sklearn.preprocessing import Normalizer, MinMaxScaler, LabelEncoder\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom tensorflow.keras import metrics\nfrom sklearn.utils import shuffle\nfrom tensorflow.keras.layers import *\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import losses, models, optimizers\n\nSEED = 321\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","60591242":"df_train = pd.read_csv('..\/input\/covid19-global-forecasting-week-5\/train.csv')\ndf_test = pd.read_csv('..\/input\/covid19-global-forecasting-week-5\/test.csv')\n\n#Public LB eval\n#test_date_min = df_test['Date'].min()\n#test_date_max = df_test['Date'].max()\n#df_train = df_train[df_train['Date']<test_date_min]","024eb5c7":"df_train['Date'].max()","7a9d5e74":"from datetime import date\n\nd0 = date(2020,1,23)\nd1 = date(2020,5,10)\ndelta = d1 - d0\nnumber = delta.days+1\nnumber","9831280b":"\n\ntrain_data = df_train.drop(\n    ['Id', 'County', 'Province_State', 'Country_Region'], axis=1)\n#test_data = df_test.drop(\n#    ['County', 'Province_State', 'Country_Region'], axis=1)\ntrain_data.set_index('Date', inplace=True)\n#test_data.set_index('Date', inplace=True)\n\ntrain_confirm = train_data[train_data['Target'] == 'ConfirmedCases']\ntrain_confirm = train_confirm.drop(['Target'], axis = 1)\ntrain_confirm['TargetValue'] = np.where(train_confirm['TargetValue'] <=0, 0, train_confirm['TargetValue'])\n#print(train_confirm)\n\nX = train_confirm.iloc[:, 0: 4].to_numpy()\nY = train_data.iloc[:, 4: 5].to_numpy()\n\n# MinMaxScaling\n\nsc_pop = MinMaxScaler(feature_range=(0, 1))\nsc_tg = MinMaxScaler(feature_range=(0, 1))\nX[:, 0:1] = sc_pop.fit_transform(X[:, 0:1])\nX[:, 2:3] = sc_tg.fit_transform(X[:, 2:3])\n\n\nX = X.reshape(-1,number,3)\nprint(X.shape)\n#print(df_train.dtypes)\n#print(X)\n\ndef multivariate_data(dataset, target, start_index, end_index, time_step) :\n\tdata=list()\n\tlabel =list()\n\n\tstart_index = start_index + time_step\n\tfor i in range(start_index, end_index) :\n\t\tindices = range(i-time_step, i)\n\t\tdata.append(dataset[indices])\n\t\tlabel.append(target[i])\n\n\treturn np.array(data), np.array(label)\n\ntime_step = 40\npartition = number-4-time_step\nX_train, Y_train = multivariate_data(X[0,:,:], X[0,:,2], 0, number, time_step)\n\nfor i in range(1,3463) :\n\tX_dummy, Y_dummy = multivariate_data(X[i,:,:], X[i,:,2], 0, number, time_step)\n\tX_train = np.concatenate((X_train, X_dummy), axis = 0)\n\tY_train = np.concatenate((Y_train, Y_dummy), axis = 0)\n\nprint(X_train.shape)\nprint(Y_train.shape)\n","afbf3fb4":"def swishE(x):\n   beta = 1.75 #1, 1.5 or 2\n   return beta * x * tf.keras.backend.sigmoid(x)\n\ndef swish(x):\n    return x * tf.keras.backend.sigmoid(x)\n\ndef phrishII(x):\n    return x*tf.keras.backend.tanh(1.75 * x * tf.keras.backend.sigmoid(x))\ndef phrishI(x):\n    return x*tf.keras.backend.tanh(x * tf.keras.backend.sigmoid(x))\n\ndef mish(x):\n    return x*tf.keras.backend.tanh(tf.keras.backend.softplus(x))\n\ndef gelu_new(x):\n    \"\"\"Gaussian Error Linear Unit.\n    This is a smoother version of the RELU.\n    Original paper: https:\/\/arxiv.org\/abs\/1606.08415\n    Args:\n        x: float Tensor to perform activation.\n    Returns:\n        `x` with the GELU activation applied.\n    \"\"\"\n    cdf = 0.5 * (1.0 + tf.tanh(\n        (np.sqrt(2 \/ np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n    return x * cdf","4da9b0ef":"del regressor, checkpoint, es, ReduceLROnPlateau,regr1","c75b0676":"def regr1():\n        \n    inp = Input(shape = (X_train.shape[1], 3))\n    x = tf.keras.layers.Bidirectional(LSTM(units = 32, return_sequences = True,input_shape = (X_train.shape[1], 3)))(inp)\n    x = Activation(mish)(x)\n    x = Dropout(0.3)(x)\n    x = tf.keras.layers.Bidirectional(LSTM(units = 32, return_sequences = True))(x)\n    x = Activation(mish)(x)\n    x = Dropout(0.3)(x)\n    x = tf.keras.layers.Bidirectional(GRU(units = 32))(x)\n    x = Activation(mish)(x)\n    x = Dropout(0.3)(x)\n    out = Dense(1, activation = swish, name = 'out')(x)\n    \n    model = models.Model(inputs = inp, outputs = out)\n    \n    #opt1 = tf.keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)#'RMSprop'\n    opt = 'RMSprop'\n    #opt2 = tfa.optimizers.AdamW(lr=0.001,weight_decay=5.5e-6)\n    #opt = tfa.optimizers.Lookahead(tfa.optimizers.AdamW(lr=0.001,weight_decay=0.006), sync_period=5, slow_step_size=0.5)\n    #opt = tfa.optimizers.Lookahead(opt1, sync_period=5, slow_step_size=0.5)\n    #opt = tfa.optimizers.SWA(opt)\n\n    model.compile(optimizer = opt, loss = 'mean_squared_error')\n    return model\n\nregressor = regr1()\nregressor.summary()\ntf.keras.utils.plot_model(\n    regressor, to_file='model1.png', show_shapes=True, show_layer_names=True,\n    rankdir='TB', expand_nested=True, dpi=96\n)","f4bd5788":"checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model_1_.h5\".format(i), monitor='val_loss', verbose=1, save_best_only=True,save_weights_only=True, mode='min')\nes = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\nReduceLROnPlateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',mode='min', patience=2, verbose=1, factor=0.5, min_lr=0.00001)\nregressor.fit(X_train, Y_train, epochs = 100, batch_size = 64, validation_split=0.05, shuffle=True,callbacks=[checkpoint,es])\nregressor.load_weights(\"model_1_.h5\")\n","62ee88b1":"test_data = df_test.drop(\n    ['ForecastId','County', 'Province_State', 'Country_Region'], axis=1)\ntest_data.set_index('Date', inplace=True)\ntest_confirm = test_data[test_data['Target'] == 'ConfirmedCases']\ntest_confirm = test_confirm.drop(['Target'], axis = 1)\n\nx_test = test_confirm.iloc[:, 0: 4].values\n(a,b) = x_test.shape\nx_test[:, 0:1] = sc_pop.fit_transform(x_test[:, 0:1])\n\nX_modify = np.zeros(shape = (a,b+1))\nX_modify[:,:-1] = x_test\nX_modify = X_modify.reshape(-1,45,3)\n\nX_t1 = X_modify[0,:,:]\nX_t2 = X[0,:,:]\nX_t3 = np.concatenate((X_t2,X_t1), axis = 0)\nst_index = number - time_step\nX_t3 = X_t3[st_index:160,:]\nX_test1, Y_d1 = multivariate_data(X_t3, X_t3[:,2], 0, time_step + 45, time_step)\nprint(X_test1.shape)\n\nfor i in range (1,3463) :\n\tx_t1 = X_modify[i,:,:]\n\tX_t2 = X[i,:,:]\n\tX_t3 = np.concatenate((X_t2,X_t1), axis = 0)\n\tX_t3 = X_t3[st_index:160,:]\n\tX_test_dummy, Y_d1 = multivariate_data(X_t3, X_t3[:,2], 0, time_step + 45, time_step)\n\tX_test1 = np.concatenate((X_test1, X_test_dummy), axis =0)\n\npredicted_test = regressor.predict(X_test1)\npredicted_test = sc_tg.inverse_transform(predicted_test)\nprint(predicted_test)","fba4c4eb":"pred_test_flat = predicted_test.flatten()\npred_test_flat = pred_test_flat.astype(int)","3d2c224f":"pred_test_flat.shape","380d50f2":"df = pd.DataFrame(pred_test_flat)\nmy_list = [*range(2,935011,6)]\ndf['Id_1'] = my_list\ndf.rename(columns = {0 : 'Predicted_Results'}, inplace = True)\ndf['Predicted_Results'] = np.where(df['Predicted_Results'] <0, 0, df['Predicted_Results'])\ndf","19262b3a":"s1 = df['Predicted_Results']\nl1 = s1.tolist()\nl2 = [i*2 for i in l1]\nl1 = [i*0.95 for i in l2]\nl2 = [round(i) for i in l1]\ndf2 = pd.DataFrame(l2)\n\nmy_list = [*range(3,935011,6)]\ndf2['Id_1'] = my_list\ndf2.rename(columns = {0 : 'Predicted_Results'}, inplace = True)\n\ns1 = df['Predicted_Results']\nl1 = s1.tolist()\nl2 = [i*2 for i in l1]\nl1 = [i*0.05 for i in l2]\nl2 = [round(i) for i in l1]\ndf3 = pd.DataFrame(l2)\n\nmy_list = [*range(1,935011,6)]\ndf3['Id_1'] = my_list\ndf3.rename(columns = {0 : 'Predicted_Results'}, inplace = True)\n\nresult_confirmed_cases = pd.concat([df, df2, df3])\nresult_confirmed_cases.sort_values(by = ['Id_1'], inplace = True)\nresult_confirmed_cases","7cdfb452":"train_confirm1 = train_data[train_data['Target'] == 'Fatalities']\ntrain_confirm1 = train_confirm1.drop(['Target'], axis = 1)\ntrain_confirm1['TargetValue'] = np.where(train_confirm1['TargetValue'] <=0, 0, train_confirm1['TargetValue'])\n#print(train_confirm)\n\nX1 = train_confirm1.iloc[:, 0: 4].to_numpy()\n\n# MinMaxScaling\n\nX1[:, 0:1] = sc_pop.fit_transform(X1[:, 0:1])\nX1[:, 2:3] = sc_tg.fit_transform(X1[:, 2:3])\n\n\nX1 = X1.reshape(-1,number,3)\nprint(X1.shape)\n#print(df_train.dtypes)\n#print(X)\n\ndef multivariate_data(dataset, target, start_index, end_index, time_step) :\n\tdata=list()\n\tlabel =list()\n\n\tstart_index = start_index + time_step\n\tfor i in range(start_index, end_index) :\n\t\tindices = range(i-time_step, i)\n\t\tdata.append(dataset[indices])\n\t\tlabel.append(target[i])\n\n\treturn np.array(data), np.array(label)\n\nX_train1, Y_train1 = multivariate_data(X1[0,:,:], X1[0,:,2], 0, number, time_step)\n\nfor i in range(1,3463) :\n\tX_dummy, Y_dummy = multivariate_data(X1[i,:,:], X1[i,:,2], 0, number, time_step)\n\tX_train1 = np.concatenate((X_train1, X_dummy), axis = 0)\n\tY_train1 = np.concatenate((Y_train1, Y_dummy), axis = 0)\n\nprint(X_train1.shape)\nprint(Y_train1.shape)\n","37f6eef0":"del regr1,regressor, checkpoint, es, ReduceLROnPlateau","9cd85bf0":"def regr2():\n        \n    inp = Input(shape = (X_train.shape[1], 3))\n    x = tf.keras.layers.Bidirectional(LSTM(units = 32, return_sequences = True,input_shape = (X_train.shape[1], 3)))(inp)\n    x = Activation(mish)(x)\n    x = Dropout(0.3)(x)\n    x = tf.keras.layers.Bidirectional(LSTM(units = 32, return_sequences = True))(x)\n    x = Activation(mish)(x)\n    x = Dropout(0.3)(x)\n    x = tf.keras.layers.Bidirectional(GRU(units = 32))(x)\n    x = Activation(mish)(x)\n    x = Dropout(0.3)(x)\n    out = Dense(1, activation = swish, name = 'out')(x)\n    \n    model = models.Model(inputs = inp, outputs = out)\n    \n    #opt1 = tf.keras.optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-08, decay=0.0)#'RMSprop'\n    opt = 'RMSprop'\n    #opt2 = tfa.optimizers.AdamW(lr=0.001,weight_decay=5.5e-6)\n    #opt = tfa.optimizers.Lookahead(tfa.optimizers.AdamW(lr=0.001,weight_decay=0.006), sync_period=5, slow_step_size=0.5)\n    #opt = tfa.optimizers.Lookahead(opt1, sync_period=5, slow_step_size=0.5)\n    #opt = tfa.optimizers.SWA(opt)\n\n    model.compile(optimizer = opt, loss = 'mean_squared_error')\n    return model\n\nregressor1 = regr2()\nregressor1.summary()\ntf.keras.utils.plot_model(\n    regressor1, to_file='model2.png', show_shapes=True, show_layer_names=True,\n    rankdir='TB', expand_nested=True, dpi=96\n)","5d641af6":"checkpoint = tf.keras.callbacks.ModelCheckpoint(\"model_2_.h5\".format(i), monitor='val_loss', verbose=1, save_best_only=True,save_weights_only=True, mode='min')\nes = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\nReduceLROnPlateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',mode='min', patience=2, verbose=1, factor=0.5, min_lr=0.00001)\nregressor1.fit(X_train, Y_train, epochs = 100, batch_size = 64, validation_split=0.05, shuffle=True,callbacks=[checkpoint,es])\nregressor1.load_weights(\"model_2_.h5\")\n","73f0270b":"test_data = df_test.drop(\n    ['ForecastId','County', 'Province_State', 'Country_Region'], axis=1)\ntest_data.set_index('Date', inplace=True)\ntest_confirm = test_data[test_data['Target'] == 'Fatalities']\ntest_confirm = test_confirm.drop(['Target'], axis = 1)\n\nx_test = test_confirm.iloc[:, 0: 4].values\n(a,b) = x_test.shape\nx_test[:, 0:1] = sc_pop.fit_transform(x_test[:, 0:1])\n\nX_modify = np.zeros(shape = (a,b+1))\nX_modify[:,:-1] = x_test\nX_modify = X_modify.reshape(-1,45,3)\n\nX_t1 = X_modify[0,:,:]\nX_t2 = X[0,:,:]\nX_t3 = np.concatenate((X_t2,X_t1), axis = 0)\nst_index = number - time_step\nX_t3 = X_t3[st_index:160,:]\nX_test1, Y_d1 = multivariate_data(X_t3, X_t3[:,2], 0, time_step + 45, time_step)\n\nfor i in range (1,3463) :\n\tx_t1 = X_modify[i,:,:]\n\tX_t2 = X[i,:,:]\n\tX_t3 = np.concatenate((X_t2,X_t1), axis = 0)\n\tX_t3 = X_t3[st_index:160,:]\n\tX_test_dummy, Y_d1 = multivariate_data(X_t3, X_t3[:,2], 0, time_step + 45, time_step)\n\tX_test1 = np.concatenate((X_test1, X_test_dummy), axis =0)\n\npredicted_test = regressor1.predict(X_test1)\npredicted_test = sc_tg.inverse_transform(predicted_test)\nprint(predicted_test)","d47051ba":"pred_test_flat = predicted_test.flatten()\npred_test_flat = pred_test_flat.astype(int)\ndf4 = pd.DataFrame(pred_test_flat)\nmy_list = [*range(5,935011,6)]\ndf4['Id_1'] = my_list\ndf4.rename(columns = {0 : 'Predicted_Results'}, inplace = True)\ndf4['Predicted_Results'] = np.where(df4['Predicted_Results'] <0, 0, df4['Predicted_Results'])\ndf4","f4c739c2":"s1 = df4['Predicted_Results']\nl1 = s1.tolist()\nl2 = [i*2 for i in l1]\nl1 = [i*0.95 for i in l2]\nl2 = [round(i) for i in l1]\ndf5 = pd.DataFrame(l2)\n\nmy_list = [*range(6,935011,6)]\ndf5['Id_1'] = my_list\ndf5.rename(columns = {0 : 'Predicted_Results'}, inplace = True)\n\ns1 = df4['Predicted_Results']\nl1 = s1.tolist()\nl2 = [i*2 for i in l1]\nl1 = [i*0.05 for i in l2]\nl2 = [round(i) for i in l1]\ndf6 = pd.DataFrame(l2)\n\nmy_list = [*range(4,935011,6)]\ndf6['Id_1'] = my_list\ndf6.rename(columns = {0 : 'Predicted_Results'}, inplace = True)\n\nresult_fatalities = pd.concat([df4, df5, df6])\nresult_fatalities.sort_values(by = ['Id_1'], inplace = True)\nresult_fatalities","52ee3a21":"result_total = pd.concat([result_confirmed_cases, result_fatalities])\nresult_total.sort_values(by = ['Id_1'], inplace = True)\nresult_total","3ae4a764":"df_submission = pd.read_csv('..\/input\/covid19-global-forecasting-week-5\/submission.csv')\nmy_list = [*range(1,935011)]\ndf_submission['Id'] = my_list \nfinal_result = pd.merge(df_submission,result_total, left_on = 'Id', right_on ='Id_1', how = 'inner')\nfinal_result.drop(['TargetValue','Id','Id_1'], axis =1, inplace = True)\nfinal_result.rename({'Predicted_Results' : 'TargetValue'}, axis = 1, inplace = True)\nfinal_result","fa41b76d":"final_result.to_csv('submission.csv', index = False)","7ad56408":"Based on this kernel https:\/\/www.kaggle.com\/avirl3364\/lstm-covid19, credit to @avirl3364.\nI changed it to Bi-LSTM with Mish and Swish and some other minor modifications.\nIt's also config. to be tested on public LB, with no leak.\n"}}