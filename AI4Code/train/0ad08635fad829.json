{"cell_type":{"44cc55f1":"code","a1e5e115":"code","a82f6bf0":"code","c31d1fb8":"code","4ba90b85":"code","90ec9c59":"code","d19606ea":"code","0db5de1b":"code","76ea1c91":"code","5edf04c1":"code","d5b3bbb0":"code","d05e2c34":"code","260d0cfb":"code","1f7a0e8a":"code","3a7cd9e9":"code","0f64e730":"code","82296ec2":"code","8a1436ee":"code","3a033541":"code","f32a2935":"code","b99fb5dd":"code","bd65cdf3":"code","0ebeb599":"code","c6ffa96c":"code","0d5b633f":"code","37d4cf4b":"code","dd87a331":"code","6c7067d2":"markdown","430439bf":"markdown","c9c19498":"markdown","58392cfd":"markdown","ca36a688":"markdown","fc17fd9a":"markdown","50b18105":"markdown","90615ec4":"markdown","c7cad925":"markdown"},"source":{"44cc55f1":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport graphviz\n\nfrom sklearn import preprocessing,model_selection\nimport itertools\n\nimport xgboost as xgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a1e5e115":"import os\nprint(os.listdir(\"\/kaggle\/input\/GiveMeSomeCredit\"))","a82f6bf0":"train_df = pd.read_csv('\/kaggle\/input\/GiveMeSomeCredit\/cs-training.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/GiveMeSomeCredit\/cs-test.csv')\nprint (\"training dataset shape is {}\".format(train_df.shape))\nprint (\"testing dataset shape is {}\".format(test_df.shape))","c31d1fb8":"train_df.head()","4ba90b85":"col_names = train_df.columns.values\ncol_names[0] = 'ID' ## rename first column to ID\ntrain_df.columns = col_names ## assign new column name to training dataset\ntest_df.columns = col_names ## assign new column name to testing dataset","90ec9c59":"train_df.head()","d19606ea":"test_df.head()","0db5de1b":"print(train_df.dtypes)","76ea1c91":"print(test_df.dtypes)","5edf04c1":"train_df.isnull().sum()","d5b3bbb0":"test_df.isnull().sum()","d05e2c34":"# remove ID, target variable Dlqin2yrs and variables with missing values\nfeature_list=list(train_df.columns.values)\nremove_list = ['ID','SeriousDlqin2yrs','MonthlyIncome','NumberOfDependents']\nfor each in remove_list:\n    feature_list.remove(each)\n\nfor each in feature_list:\n    sns.distplot(train_df[each])\n    plt.show()","260d0cfb":"print (train_df.columns.values)","1f7a0e8a":"log_trans_list = train_df.columns.values[[2,4,5,8,9,10]]\nlog_trans_list\nfor each in log_trans_list:\n    train_df[each] = np.log(1+train_df[each].values)","3a7cd9e9":"for each in feature_list:\n    sns.distplot(train_df[each])\n    plt.show()","0f64e730":"partial_train_df = train_df[['MonthlyIncome','NumberOfDependents']]\n#partial_train_df.dropna(how='any')\npartial_train_df = partial_train_df.dropna(how='any')\n\nsns.distplot(partial_train_df['MonthlyIncome'])\nplt.show()\nsns.distplot(partial_train_df['NumberOfDependents'])\nplt.show()","82296ec2":"partial_train_df['MonthlyIncome'] = np.log(1+partial_train_df['MonthlyIncome'].values)\npartial_train_df['NumberOfDependents'] = np.log(1+partial_train_df['NumberOfDependents'].values)\n\n\nsns.distplot(partial_train_df['MonthlyIncome'])\nplt.show()\nsns.distplot(partial_train_df['NumberOfDependents'])\nplt.show()","8a1436ee":"train_df['MonthlyIncome'] = np.log(1+train_df['MonthlyIncome'].values)\ntrain_df['NumberOfDependents'] = np.log(1+train_df['NumberOfDependents'].values)","3a033541":"grouped_df = train_df.groupby('age')\ndlinq_age = grouped_df['SeriousDlqin2yrs'].aggregate([np.mean,'count']).reset_index()\nprint(dlinq_age)\ndlinq_age.columns =['age','DlqinFreq','count']\nsns.regplot(x='age',y='DlqinFreq',data=dlinq_age)\nplt.show()","f32a2935":"## remove outlier\ntrain_df = train_df[train_df['age'] != 0]\ntrain_df = train_df[train_df['age'] !=99]\ntrain_df = train_df[train_df['age'] !=101]\ngrouped_df = train_df.groupby('age')\ndlinq_age = grouped_df['SeriousDlqin2yrs'].aggregate([np.mean,'count']).reset_index()\ndlinq_age.columns =['age','DlqinFreq','count']\nsns.regplot(x='age',y='DlqinFreq',data=dlinq_age)\nplt.show()\n\n## create new features\ntrain_df['age_sqr'] = train_df['age'].values^2 \n## apply the same operation on testing set\ntest_df['age_sqr'] = test_df['age'].values^2","b99fb5dd":"train_y = train_df['SeriousDlqin2yrs']\n#'RevolvingUtilizationOfUnsecuredLines'\ntrain_X = train_df.drop(['SeriousDlqin2yrs','ID'],axis=1,inplace=False)\ntest_X = test_df.drop(['SeriousDlqin2yrs','ID'],axis=1,inplace=False)\nprint(type(train_y))\n\nskf = model_selection.StratifiedKFold(n_splits=5,random_state=100)\nxgb_params = {\n'eta':0.03,\n'max_depth':4,\n'sub_sample':0.9,\n'colsample_bytree':0.5,\n'objective':'binary:logistic',\n'eval_metric':'auc',\n'silent':0\n}\n\nprint(train_X.shape)\nprint(train_X.columns)\nprint(test_X.shape)","bd65cdf3":"best_iteration =[]\nbest_score= []\ntraining_score = []\nfor train_ind,val_ind in skf.split(train_X,train_y):\n    #print (set(train_y))\n    #print (type(train_y))\n    X_train,X_val = train_X.iloc[train_ind,],train_X.iloc[val_ind,]\n    y_train,y_val = train_y.iloc[train_ind],train_y.iloc[val_ind]\n    #print (set(train_y))\n    #print (max(train_ind),min(train_ind),max(val_ind),min(val_ind))\n    #print (train_ind,val_ind)\n    #print(set(y_train))\n    dtrain = xgb.DMatrix(X_train,y_train,feature_names = X_train.columns)\n    dval = xgb.DMatrix(X_val,y_val,feature_names = X_val.columns)\n    model = xgb.train(xgb_params,dtrain,num_boost_round=1000,\n                      evals=[(dtrain,'train'),(dval,'val')],verbose_eval=True,early_stopping_rounds=30)\n    best_iteration.append(model.attributes()['best_iteration'])\n    best_score.append(model.attributes()['best_score'])\n    # training_score.append(model.attributes()['best_msg'].split()[1][-8:])\n    xgb.plot_importance(model)\n    plt.show()","0ebeb599":"def xgbCV(eta=[0.05],max_depth=[6],sub_sample=[0.9],colsample_bytree=[0.9]):\n    train_y = train_df['SeriousDlqin2yrs'] # label for training data\n    train_X = train_df.drop(['SeriousDlqin2yrs','ID'],axis=1,inplace=False) # feature for training data\n    test_X = test_df.drop(['SeriousDlqin2yrs','ID'],axis=1,inplace=False) # feature for testing data\n    skf = model_selection.StratifiedKFold(n_splits=5,random_state=100) # stratified sampling\n    train_performance ={} \n    val_performance={}\n    for each_param in itertools.product(eta,max_depth,sub_sample,colsample_bytree): # iterative over each combination in parameter space\n        xgb_params = {\n                    'eta':each_param[0],\n                    'max_depth':each_param[1],\n                    'sub_sample':each_param[2],\n                    'colsample_bytree':each_param[3],\n                    'objective':'binary:logistic',\n                    'eval_metric':'auc',\n                    'silent':0\n                    }\n        best_iteration =[]\n        best_score=[]\n        training_score=[]\n        for train_ind,val_ind in skf.split(train_X,train_y): # five fold stratified cross validation\n            X_train,X_val = train_X.iloc[train_ind,],train_X.iloc[val_ind,] # train X and train y\n            y_train,y_val = train_y.iloc[train_ind],train_y.iloc[val_ind] # validation X and validation y\n            dtrain = xgb.DMatrix(X_train,y_train,feature_names = X_train.columns) # convert into DMatrix (xgb library data structure)\n            dval = xgb.DMatrix(X_val,y_val,feature_names = X_val.columns) # convert into DMatrix (xgb library data structure)\n            model = xgb.train(xgb_params,dtrain,num_boost_round=1000, \n                              evals=[(dtrain,'train'),(dval,'val')],verbose_eval=False,early_stopping_rounds=30) # train the model\n            best_iteration.append(model.attributes()['best_iteration']) # best iteration regarding AUC in valid set\n            best_score.append(model.attributes()['best_score']) # best score regarding AUC in valid set\n            training_score.append(model.attributes()['best_msg'].split()[1][10:]) # best score regarding AUC in training set\n        valid_mean = (np.asarray(best_score).astype(np.float).mean()) # mean AUC in valid set\n        train_mean = (np.asarray(training_score).astype(np.float).mean()) # mean AUC in training set\n        val_performance[each_param] =  train_mean\n        train_performance[each_param] =  valid_mean\n        print (\"Parameters are {}. Training performance is {:.4f}. Validation performance is {:.4f}\".format(each_param,train_mean,valid_mean))\n    return (train_performance,val_performance)\n#xgbCV(eta=[0.01,0.02,0.03,0.04,0.05],max_depth=[4,6,8,10],colsample_bytree=[0.3,0.5,0.7,0.9]) \nxgbCV(eta=[0.04],max_depth=[4],colsample_bytree=[0.5])","c6ffa96c":"print(train_X.columns)\nany(train_X.columns == test_X.columns)","0d5b633f":"train = xgb.DMatrix(train_X,train_y,feature_names=train_X.columns)\ntest = xgb.DMatrix(test_X,feature_names=test_X.columns)\nxgb_params = {\n                    'eta':0.03,\n                    'max_depth':4,\n                    'sub_sample':0.9,\n                    'colsample_bytree':0.5,\n                    'objective':'binary:logistic',\n                    'eval_metric':'auc',\n                    'silent':0\n                    }\n\nfinal_model = xgb.train(xgb_params,train,num_boost_round=500)\nypred = final_model.predict(test)","37d4cf4b":"xgb.plot_importance(final_model)\nplt.show()","dd87a331":"SUB_1 = pd.DataFrame({'Id':test_df.ID.values,'Probability':ypred})\nSUB_1.to_csv('Submission.csv',index=False)\nSUB_1.head()","6c7067d2":"Post transformation looks better than before. I will keep log transformation on both at this time.","430439bf":"Split the data","c9c19498":"Distribution of following features are highly skewed.\n\n* RevolvingUtilizationOfUnsecuredLines\n* NumberOfTime30-59DaysPastDueNotWorse\n* DebtRatio\n* NumberOfTimes30DaysLate\n* NumberRealEstateLoansOrLines\n* NumberOfTime60-89DaysPastDueNotWorse\n\nTake a log transformation to see if distribution can be less skewed.","58392cfd":"monthlyIncome is highly skewed. let us take log transformation on both then check their distribution again","ca36a688":"From the plot above, we can see:\n\n* DlinFreq is negatively associated with age in general\n* age of 0,99 and 101 looks like outliers\n* DlinFreq looks like a quardratic function of age. Put a higher order of age maybe helpful\n\nRemove outlier in age and create new feature $age^2$","fc17fd9a":"Check column type","50b18105":"Check distribution of each features to see outlier\n\n\"MonthlyIncome\" and \"NumberOfDependents\" are removed here as they have nan values","90615ec4":"Distribution after log transformation","c7cad925":"The distribution after transformation is much less skewed. We may able to put them into machine learning algorithm later.\n\nRemove nan values in \"MonthlyIncome\" and \"NumberOfDependents\" to check their distribution"}}