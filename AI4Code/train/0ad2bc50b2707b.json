{"cell_type":{"798e2e6e":"code","ccc09e75":"code","64d96547":"code","aa00c795":"code","54a67678":"code","6830dedb":"code","b73879a1":"code","a99ac1c6":"code","226ac6dd":"code","de9bab58":"code","0071bc7f":"code","9be4be8b":"code","d4c03222":"code","cad427f6":"code","f8c82e3f":"code","1377a025":"code","ebabb8df":"code","bb66e11a":"code","5866d561":"code","2c4a7dce":"code","5ce0d583":"code","30912dca":"code","86277547":"markdown","938396c3":"markdown","f6b1b7b3":"markdown","b1fb44e9":"markdown","7a4e1dbc":"markdown","ed7967ad":"markdown","27cebf70":"markdown","7f9ea23b":"markdown","ad738673":"markdown","e79e428d":"markdown","85fadf94":"markdown","be9b0b9f":"markdown","8a24af94":"markdown","b3c1295e":"markdown","e58f4c08":"markdown","d1640a35":"markdown","ed1398ca":"markdown","738dcc78":"markdown","c207fafc":"markdown","65e349d2":"markdown","6a0224d1":"markdown"},"source":{"798e2e6e":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.ticker as ticker\n\nfrom IPython.core.debugger import set_trace #https:\/\/docs.python.org\/3\/library\/pdb.html\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import RobustScaler, PowerTransformer\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\n%matplotlib inline \n#to allow plots to render in the notebook\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.rcParams.update({'font.size': 18})","ccc09e75":"class AlcoholStudyDataPreparer:\n    \n    def __init__(self):\n        \n        pass\n    \n    def read_in_data_from(self, path):\n        self.data= pd.read_csv(path)\n        \n    def data_length_checker(self):\n        print(\"There are \", str(self.data.shape[1]), \" columns in the data\")\n        \n    def move_alcohol_related_variables_to_end_of_column_list(self, data):\n        self.data = data\n        self.data_length_checker()\n        print(\"Moving alcohol related variables to the end of the list\")\n        alcohol_related_variables = [\"binge_drinker\", \"heavy_drinker\", \"Dalc\", \"Walc\"]\n        cols = self.data.columns.tolist()\n        print(\"Removed extra index column resulting from csv import\")\n        cols = set(cols) - set(alcohol_related_variables) - set([\"Unnamed: 0\"])\n        cols = list(cols) + alcohol_related_variables\n        self.data = data[cols]\n        self.data_length_checker()\n        \n    def ready_data_for_experiments(self, path):\n        self.read_in_data_from(path)\n        self.move_alcohol_related_variables_to_end_of_column_list(self.data)\n        print(self.data.shape)\n        return self\n    \n    def plot_dependent_variable(self):\n        data = self.data\n        f, ax = plt.subplots(1,1)\n        sns.countplot(\"heavy_drinker\", data=data, ax=ax)\n        f.suptitle(\"Distribution of heavy drinkers\")\n        print(data.groupby(data[\"heavy_drinker\"])[\"heavy_drinker\"].count())\n        \nclass Experiment:\n    \n    def __init__(self):\n        \n        pass\n    \n    def run_experiment(self, data, y_col_name, estimator, evaluation):\n        self.data = data\n        model = ModelFitter(data)\n        self.X_train, self.X_test, self.y_train, self.y_test = model.get_test_train_splits(y_col_name)\n        fit_model = model.fit_model_and_evaluate(estimator, evaluation)\n        Evaluation().plot_learning_curve(self.data, estimator)\n        return fit_model\n        \n    \nclass ModelFitter:\n    \n    def __init__(self, data):\n        self.data = data\n        pass\n    \n    def separate_X_from_y(self, y_col_name):\n        self.X = self.data.iloc[:, :-4]\n        self.y = self.data.loc[:, y_col_name].values.reshape(-1, )\n        return self.X, self.y\n    \n    def divide_into_training_and_test(self, train_size):\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            self.X, self.y, random_state=0, train_size=train_size, test_size=(1-train_size)\n        )\n        return self.X_train, self.X_test, self.y_train, self.y_test\n    \n    def get_test_train_splits(self, y_col_name, train_size=0.7):\n        self.separate_X_from_y(y_col_name)\n        return self.divide_into_training_and_test(train_size)\n    \n    def fit_model(self, estimator, X_train, y_train):\n        self.model = estimator(X_train, y_train)\n        fit_model = self.model.fit(X_train, y_train)\n        return fit_model\n    \n    def fit_model_and_evaluate(self, estimator, evaluation):\n        fitted_model = self.fit_model(estimator, self.X_train, self.y_train)\n        evaluation(fitted_model, self)\n        return fitted_model\n    \nclass Evaluation:\n    \n    def print_accuracy_score(self, model, data):\n        print(\"Accuracy is: \" + str(model.score(data.X_train, data.y_train)))\n        \n    def print_confusion_matrix(self, model, data, figsize=(20,8), fontsize=14, model_name=\"\"):\n        #tweaked from shyapal5 https:\/\/gist.github.com\/shaypal5\/94c53d765083101efc0240d776a23823\n        class_names = [\"normal_drinker\", \"heavy_drinker\"]\n        \n        f, (ax1, ax2) = plt.subplots(1,2, figsize=figsize)\n        \n        def __create_single_heatmap(X, y, name, ax):\n            y_pred = model.predict(X)\n            conf_matrix = confusion_matrix(y, y_pred)\n            df_cm = pd.DataFrame(\n                conf_matrix, index=class_names, columns=class_names\n            )\n            try:\n                heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", ax=ax)\n            except ValueError:\n                raise ValueError(\"Confusion matrix values must be integers\")\n            heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n            heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n            heatmap.set_title((\"Confusion matrix for \" + name + \" set\"))\n            #plt.ylabel('True label')\n            heatmap.set_xlabel('Predicted label')\n            \n        __create_single_heatmap(data.X_train, data.y_train, \"train\", ax1)\n        __create_single_heatmap(data.X_test, data.y_test, \"test\", ax2)\n        ax1.set_ylabel(\"True Label\")\n        ax2.set_ylabel(\"\")\n    \n    def __compute_auc(self, y, y_pred):\n        fpr, tpr, thresholds = roc_curve(y, y_pred)\n        return auc(fpr, tpr)\n    \n    def __compute_confusion_matrix(self, y, y_pred):\n        return confusion_matrix(y, y_pred)\n    \n    def __calculate_metric(self, fitted_model, X, y):\n        y_pred = fitted_model.predict(X)\n        return self.metric(self, y, y_pred) #if I were passing in a custom metric, would it be tricky to pass in the Evaluation in stance from the caller?\n        \n    def __get_train_and_test_metric_for_train_size(self, train_size):\n        model = ModelFitter(self.data)\n        #TODO: find a way to get rid of this magic string\n        self.X_train, self.X_test, self.y_train, self.y_test = model.get_test_train_splits(\"heavy_drinker\", train_size)\n        fitted_model = model.fit_model(self.estimator, self.X_train, self.y_train)\n        train_metric = self.__calculate_metric(fitted_model, self.X_train, self.y_train)\n        test_metric = self.__calculate_metric(fitted_model, self.X_test, self.y_test)\n        return (train_size, train_metric, test_metric)\n    \n    def __get_learning_curve_data(self):\n        metrics = [self.__get_train_and_test_metric_for_train_size(train_size) for train_size in self.train_sizes]\n        train_df = pd.DataFrame(data=[(metric[0], metric[1], \"train\") for metric in metrics])\n        test_df = pd.DataFrame(data=[(metric[0], metric[2], \"test\") for metric in metrics])        \n        metrics = pd.concat([train_df, test_df])\n        metrics.columns = [\"train_size\", \"auc\", \"split\"]\n\n        return metrics\n    \n    def plot_learning_curve(self, data, estimator, train_sizes=[0.4, 0.5, 0.6, 0.7, 0.8, 0.9], metric=__compute_auc):\n        \n        #plots the learning_curves\n        self.metric = metric\n        self.data = data\n        self.train_sizes = train_sizes\n        self.estimator = estimator\n        lc_data = self.__get_learning_curve_data()\n        fig, ax = plt.subplots(1,1)\n        fig.suptitle(\"Learning Curve\")\n        sns.pointplot(\"train_size\", \"auc\", hue=\"split\", data=lc_data, ax=ax)\n        return lc_data\n\nclass PCAInvestigator:\n    \n    def __init__(self, pca, data):\n        self.pca = pca\n        self.data = data\n        self.create_pca_df()\n        self.transformed = self.transform_to_pca_components()\n        pass\n\n    def create_pca_df(self):\n        components = self.pca.components_\n        self.component_labels = [(\"PCA\" + str(i+1)) for i, val in enumerate(components)]\n        df = pd.DataFrame(data=components, columns=self.data.X_train.columns).transpose()\n        df.columns = self.component_labels\n        self.df = df\n        return df\n    \n    def get_n_largest_features_for_component(self, component_name, n):\n        \n        abs_col = str(component_name + \"_abs\")\n        self.df[abs_col] = self.df[component_name].abs() #for ranking purposes, we need the absolute value of the coeff\n        components = self.df.nlargest(n, abs_col)[[component_name, abs_col]]\n        df = pd.DataFrame(data=components).reset_index() #unhooks the feature names from the index\n        df[\"rank\"] = df[abs_col].rank(ascending=False)\n        df = df.loc[:, [\"index\", component_name, \"rank\"]] #after we have rank, we no longer need abs_col\n        df.columns=[\"feature_name_\" + component_name, \"variance_explained_\" + component_name, \"rank\"]\n        return df\n    \n    def get_n_largest_features_for_pca(self, n):\n        \n        n_largest_features = [self.get_n_largest_features_for_component(label, n) for label in self.component_labels]\n        \n        if len(n_largest_features) > 1:\n            length = len(self.component_labels[:-1]); \n            i = 0;\n            \n            def merge_feature_dfs(df, dfs, labels, length, i):\n                i = i + 1\n                combined_df = pd.merge(df, dfs[i], on=\"rank\")\n                if (i >= length):\n                    return combined_df\n                else:\n                    return merge_feature_dfs(combined_df, dfs, labels, length, i)\n            \n            self.largest_features_per_factor = merge_feature_dfs(n_largest_features[0], n_largest_features, self.component_labels, length, i).set_index(\"rank\")\n        else:\n            self.largest_features_per_factor = n_largest_features[0]\n        return self.largest_features_per_factor\n    \n    \n    def transform_to_pca_components(self):\n        \n        pca_results = self.pca.transform(self.data.X_train)\n        pca_results = pd.DataFrame(data=pca_results)\n        pca_results.columns = self.component_labels\n        pca_results[\"heavy_drinker\"] = self.data.y_train\n        return pca_results\n    \n    def plot_pca_components(self):\n        \n        f, ax = plt.subplots(1,1, figsize = (10,7))\n        f.suptitle(\"Heavy Drinker instances across PCA1 vs PCA2\")\n        sns.scatterplot(\"PCA1\", \"PCA2\", hue=\"heavy_drinker\", data=self.transformed);\n        \n    def plot_bar_chart_of_component_features(self, component_name):\n        #these lines are scoping the columns to be selected\n        feature_name = \"feature_name_\" + component_name\n        variance_explained = \"variance_explained_\" + component_name\n        columns = [feature_name, variance_explained]\n        \n        f, ax = plt.subplots(1,1, figsize = (10,7))\n        f.suptitle(\"Variance explained for features in component: \" + component_name)\n        barplot = sns.barplot(feature_name, variance_explained, data=self.largest_features_per_factor[columns], ax=ax)\n        barplot.xaxis.set_ticklabels(barplot.xaxis.get_ticklabels(), rotation=45, ha='right')\n        \n    def plot_distplot_of_component_features(self, component_name):\n        \n        f, ax = plt.subplots(1,1, figsize = (10,7))\n        f.suptitle(\"Distribution of \" + component_name)\n        sns.distplot(a=self.transformed[component_name])\n        \n    def plot_pca_component(self, y):\n        \n        f, ax = plt.subplots(1,1, figsize = (10,7))\n        f.suptitle(\"Distribution of heavy drinkers across \" + y)\n        sns.swarmplot(x=\"heavy_drinker\", y=y, data=self.transformed);\n        ","64d96547":"asdp = AlcoholStudyDataPreparer().ready_data_for_experiments('..\/input\/mathportugeseyouthalcoholstudy\/student_math_por_formatted.csv')","aa00c795":"asdp.plot_dependent_variable()","54a67678":"#exp1_classifier = DummyClassifier(strategy=\"stratified\", random_state=0)\ndef exp1_classifier(X_train, y_train):\n    return DummyClassifier(strategy=\"stratified\", random_state=0)\n\nexp1_eval = Evaluation().print_confusion_matrix\nexp1 = Experiment()\ndummy = exp1.run_experiment(asdp.data, \"heavy_drinker\", exp1_classifier,exp1_eval)\nprint(\"accuracy score: \" + str(dummy.score(exp1.X_test, exp1.y_test)))","6830dedb":"def exp2_classifier(X_train, y_train):\n    \n    estimators = [('robust_scaler', RobustScaler()), \n                  ('reduce_dim', PCA(random_state=0)), \n                  ('normalise_pca', PowerTransformer()), #I applied this as the distribution of the PCA factors were skew\n                  ('clf', LogisticRegression(random_state=0, solver=\"liblinear\"))] \n                #solver specified here to suppress warnings, it doesn't seem to effect gridSearch\n    pipe = Pipeline(estimators)\n    pipe.fit(X_train, y_train)\n    \n    param_grid = dict(reduce_dim__n_components=[1, 2, 3, 4, 5, 10],\n                    clf__solver=[\"liblinear\", \"lbfgs\"],\n                     clf__class_weight=[\"balanced\", None])\n    return GridSearchCV(pipe, param_grid=param_grid, cv=3, scoring=\"recall\", iid=False)\n\nexp2_eval = Evaluation().print_confusion_matrix\nexp2 = Experiment()\nlogit_grid = exp2.run_experiment(asdp.data, \"heavy_drinker\", exp2_classifier, exp2_eval);\n\n#TODO: link this kernel here: https:\/\/www.kaggle.com\/willkoehrsen\/intro-to-model-tuning-grid-and-random-search#354172","b73879a1":"def exp3_classifier(X_train, y_train):\n    \n    estimators = [('robust_scaler', RobustScaler()), \n                  ('reduce_dim', PCA(random_state=0)), \n                  ('normalise_pca', PowerTransformer()), #I applied this as the distribution of the PCA factors were skew\n                  ('clf', LinearDiscriminantAnalysis())] \n                #solver specified here to suppress warnings, it doesn't seem to effect gridSearch\n    pipe = Pipeline(estimators)\n    pipe.fit(X_train, y_train)\n    \n    param_grid = dict(reduce_dim__n_components=[1, 2, 3, 4, 5, 10])\n    return GridSearchCV(pipe, param_grid=param_grid, cv=3, scoring=\"recall\", iid=False)\n\nexp3_eval = Evaluation().print_confusion_matrix\nexp3 = Experiment()\ngbdt_grid = exp3.run_experiment(asdp.data, \"heavy_drinker\", exp3_classifier, exp3_eval);","a99ac1c6":"def exp3_classifier(X_train, y_train):\n    \n    estimators = [('robust_scaler', RobustScaler()), \n                  ('reduce_dim', PCA(random_state=0)), \n                  ('normalise_pca', PowerTransformer()), #I applied this as the distribution of the PCA factors were skew\n                  ('clf', GradientBoostingClassifier(random_state=0))] \n                #solver specified here to suppress warnings, it doesn't seem to effect gridSearch\n    pipe = Pipeline(estimators)\n    pipe.fit(X_train, y_train)\n    \n    param_grid = dict(reduce_dim__n_components=[1, 2, 3, 4, 5, 10])\n    return GridSearchCV(pipe, param_grid=param_grid, cv=3, scoring=\"recall\", iid=False)\n\nexp3_eval = Evaluation().print_confusion_matrix\nexp3 = Experiment()\ngbdt_grid = exp3.run_experiment(asdp.data, \"heavy_drinker\", exp3_classifier, exp3_eval);","226ac6dd":"def exp4_classifier(X_train, y_train):\n    \n    return GradientBoostingClassifier(random_state=0)\n\nexp4_eval = Evaluation().print_confusion_matrix\nexp4 = Experiment()\ngbdt_grid = exp4.run_experiment(asdp.data, \"heavy_drinker\", exp4_classifier, exp4_eval);\n\n","de9bab58":"def exp5_classifier(X_train, y_train):\n    \n    clf = GradientBoostingClassifier(random_state=0).fit(X_train, y_train)\n    \n    param_grid = dict(loss=[\"deviance\", \"exponential\"],\n                     learning_rate=[0.1,0.2,0.5,0.8],\n                     n_estimators=[50, 100, 150],\n                    max_depth=[1,3,5],\n                      max_features=[\"sqrt\", \"log2\", 3] #using the rule of 3s\n                     )\n    \n    return GridSearchCV(clf, param_grid=param_grid, cv=3, scoring=\"recall\", iid=False) \n\nexp5_eval = Evaluation().print_confusion_matrix\nexp5 = Experiment()\ngbdt_grid = exp5.run_experiment(asdp.data, \"heavy_drinker\", exp5_classifier, exp5_eval);","0071bc7f":"def exp6_classifier(X_train, y_train):\n    \n    return RandomForestClassifier(random_state=0, n_estimators=100)\n\nexp6_eval = Evaluation().print_confusion_matrix\nexp6 = Experiment()\ngbdt_grid = exp6.run_experiment(asdp.data, \"heavy_drinker\", exp6_classifier, exp6_eval);","9be4be8b":"def exp7_classifier(X_train, y_train):\n    \n    estimators = [('robust_scaler', RobustScaler()), \n                  ('reduce_dim', PCA(random_state=0)), \n                  ('normalise_pca', PowerTransformer()), #I applied this as the distribution of the PCA factors were skew\n                  ('clf', RandomForestClassifier(random_state=0, n_estimators=100))] \n                #solver specified here to suppress warnings, it doesn't seem to effect gridSearch\n    pipe = Pipeline(estimators)\n    pipe.fit(X_train, y_train)\n    \n    param_grid = dict(reduce_dim__n_components=[1, 2, 3, 4, 5, 10])\n    return GridSearchCV(pipe, param_grid=param_grid, cv=3, scoring=\"recall\", iid=False)\n\nexp7_eval = Evaluation().print_confusion_matrix\nexp7 = Experiment()\nexp7_grid = exp7.run_experiment(asdp.data, \"heavy_drinker\", exp7_classifier, exp7_eval);","d4c03222":"pca_investigation = PCAInvestigator(logit_grid.best_estimator_.steps[1][1], exp2)\npca_investigation.get_n_largest_features_for_pca(10)","cad427f6":"pca_investigation.plot_bar_chart_of_component_features(\"PCA1\")","f8c82e3f":"pca_investigation.plot_distplot_of_component_features(\"PCA1\")","1377a025":"pca_investigation.plot_pca_component(y=\"PCA1\")","ebabb8df":"pca_investigation.plot_bar_chart_of_component_features(\"PCA2\")","bb66e11a":"pca_investigation.plot_distplot_of_component_features(\"PCA2\")","5866d561":"pca_investigation.plot_pca_component(y=\"PCA2\")","2c4a7dce":"pca_investigation.plot_bar_chart_of_component_features(\"PCA3\")","5ce0d583":"pca_investigation.plot_distplot_of_component_features(\"PCA3\")","30912dca":"pca_investigation.plot_pca_component(y=\"PCA3\")","86277547":"### Performance: Surprisingly Poor\n\nRunning with the same pipeline as the logistic regression resulted in anomalously poor peformance. Interestingly, removing the PCA step to the pipeline resulted in better performance, even in the test set. However, doing so violates the assumption of minimum collinearity thus this approach should probably not be considered.\n\n### Learning Curve\n\nThe learning curve is difficult to inpterpet here and perhaps should be discarded along with the performance of this model.\n\nGiven that *any* improvement seen in the LDA model was trivial compared to logistic regression, the anomalies seen in this learning curve won't be investigated further (but is included in this report to show the reader the diversity of methods employed).\n\n## Gradient Boosted Decision Trees\n\n3 models will be investigated here and [discussed collectively at the end of the section](#GBDT_Conclusion).\n\n### Using the same pipe \/ GridSearch params (where applicable)","938396c3":"### Using no transformation pipeline nor GridSearch","f6b1b7b3":"##### PCA Analysis Conclusion\n\nThe feature space appears to be reduced to \n* PCA1: conscientiousness and academic achievement\n* PCA2: health and family relationship quality\n* PCA3: extracurricular activities and responsibilities\n\nThe second and third features mentioned appear very similar in terms of their distribution and are particularly noisey when compared with PCA1.","b1fb44e9":"##### PCA3 Analysis\n\nThis factor is difficult to interpret in ioslation, let alone in the context of PCA1 and PCA2. The top five components relate to an emphasis on extracurricular activities.","7a4e1dbc":"Using the grid search CV, it was found that the pipeline worked best when the feature space was reduced to 3 features.\n##### PCA 1 Analysis","ed7967ad":"### With GridSearchCV","27cebf70":"The figure above shows that the classes are imbalanced; thus accuracy may not be a suitable measure.\n\nTake, for example, an accuracy of 95%. Since the heavy drinkers make up less than 5% of the sample, a classifier could fail to detect all of the heavy drinkers in the sample and still achieve a 95% accuracy.\n\nThe baseline model below will make this more concrete.\n\nEffectiveness of this model would be \n* finding all the heavy drinkers so that they can be targeted for any interventions.\n* the risk of classifying normal drinkers as heavy_drinkers is acceptable but should be minimised.\n* the risk of classigying heavy_drinkers as normal_drinkers is not acceptable.\n\n# Modelling\n## Baseline performance: Stratified Dummy Classifier","7f9ea23b":"The improvement over baseline is trivial and outclassed by logistic regression. However, it could be that the dimensionality reduction is oversimplifying the space for the model. In the next step, the model will have the opportunity to search the entire feature space by removing the preprocessing pipeline.","ad738673":"### Performance: Promising\n\nThe logistic regression appears promising. Note, however, that recall is still not perfect as there are still 3 cases of heavy_drinkers getting classified as normal drinkers. \n\n### Learning Curve\n\nThe test performance is climbing with increased training size which suggests that more training data might be useful.\n\nSince there are relatively few instances in the target class, Linear Discriminant Analysis might be useful to apply\n\n## Linear Discriminant Analysis","e79e428d":"## Random Forest Classifier\n\n2 models will be investigated and will be [discussed collectively at the end of the section](#RF_Conclusion)\n### Without GridSearchCV","85fadf94":"### Lots of room for improvement\nThe results above illustrate the inappropriateness of using accuracy as a performance metric: The accuracy score is 93%. However, as can be seen from the confusion matrix, only one heavy_drinker was predicted correctly. What's worse, nine normal drinkers were incorrectly classified as heavy drinkers (somewhat acceptable) and nine heavy drinkers were incorrectly classified as normal drinkers (not acceptable).\n\nA dummy classifier is meant to give us only a baseline of recall performance. Hopefully, things will only improve from here!\n\n#### Learning Curves\n\nAs noted in the [previous exploration](https:\/\/www.kaggle.com\/joeycorea\/academicsuccessalcoholconsumptioneda), the small sample size is a concern. Thus, learning curves will be plotted and interpreted to see if there is a case for requesting more data. The learning curves will be plotted using the area under the ROC curve metric.\n<a id=\"logistic-regression\"><\/a>\n## Logistic Regression Approach\n### Preprocessing\n1. Remove collinearity by applying PCA.\n2. Investigate factors for linear relationship with heavy_drinker variable.\n3. Apply transformations as required to remove outliers","be9b0b9f":"The barplot below suggests PCA1 emphasises academic performance and possibly conscientiousness","8a24af94":"[back to executive summary](https:\/\/www.kaggle.com\/joeycorea\/academicsuccessalcoholconsumptionexecutivesummary)\n# Model of heavy alcohol consumption among high school students\n\nThis section of the analysis will attempt to construct a model to predict which students are at risk of heavy drinking behaviour.\nYou may recall that a student is labelled a heavy drinker when they have\n\n* a high level (Walc = 4-5) of weekend alcohol consumption accompanied by \n* a high level (Dalc = 4-5) of weekday alcohol consumption.\n\n<a id=\"executive_summary\"><\/a>\n# Executive Summary\nThe target variable was highly unbalanced with the positive class (heavy drinkers) making up less than 5% of the population. For this reason, recall was chosen as the performance metric rather than accuracy. That is, performance should be weighted towards identifying heavy drinkers even if it means incorrectly classifying some normal drinkers as heavy drinkers.\n\nThe most effective model proved to be a Principle Component Analysis (PCA) followed by logistic regression. The PCA reduced the 50 feature space to three features and achieved the most impressive recall in finding the heavy drinkers but also incorrectly classifying many non-drinkers as heavy drinkers.\n\nIn an attempt to detect any subtle signals, both the maths and Portugese classes were combined in the same analysis. If more data could be obtained, these should be considered separately. The shape of the learning curves on both simple and complex models suggest that more data would help amplify the subtle signals and perhaps smoothen out the noise found in this analysis.\n\n\n# Uninteresting Helper Code\n[Fast forward to the meat](#reading_in_file)","b3c1295e":"# Metric Selection","e58f4c08":"<a id=\"a_telling_swarm\"><\/a>\nFrom the plot below, it is a surprise that the model was able to pick the heavy drinkers from the crowd: For PCA 1, the heavy drinkers somewhat evenly distributed among the normal drinkers. It should be noted, however, in the test set, 113 normal drinkers were classified as heavy drinkers. Depending on the investment in any interventions on suspected heavy drinkers, this may be unacceptable.\n\n[back to executive summary](#executive_summary)","d1640a35":"<a id=\"reading_in_file\"><\/a>\n# Reading in file and checking columns","ed1398ca":"<a id=\"GBDT_Conclusion\"><\/a>\n### Gradient Boosted Decision Tree Suite Conclusion\n\n#### Performance: Relatively Poor\n\nOverall, the suite of GBDT models a far inferior to logistic regression in recall performance. Indeed, there was very little difference between the different GBDT iterations applied above.\n\n#### Learning Curves\n\nThe following interpretations of the curves suggest that more training data could allow the model to better classify normal from heavy drinkers:\n\n* the upward trend of test performance with increasing training set size\n* in all cases, it appeared like the models were overfitting the training data","738dcc78":"# Conclusion\n\nDespite the poor performance of most of the models implemented above, there is reason to believe that the investigation can be improved with more training data. This is suggested by:\n\n* the upward trend between test performance and training set size on most learning curves.\n* the tendency for complex models like GBDT and Random Forest to overfit the training data.\n\nThe finding that the relatively simplistic logistic regression could recall many of the heavy drinkers in the test set was promising. Part of this procedure was reducing the feature space from 50 features to 3. These features seemed to correspond to:\n\n* conscientiousness and academic achievement\n* health and family relationship quality\n* extracurricular activities and responsibilities\n\nHowever, the interpretations of these are fairly loose as the mapping of the features to the components was noisey (especially for the second and third features).\n[back to executive summary](https:\/\/www.kaggle.com\/joeycorea\/academicsuccessalcoholconsumptionexecutivesummary)","c207fafc":"### Using GridSearchCV\nSee the param_grid variable below for the hyperparameters searched over.","65e349d2":"<a id=\"RF_Conclusion\"><\/a>\n### Random Forest Suite Conclusion\n\n#### Performance\n\nOverall, the suite of Random Forest models a far inferior to logistic regression in recall performance. Like the GBDT suite, there was very little difference between the different Random Forest iterations applied above.\n\n#### Learning Curves\n\nThe suggestions from the learning curves were mixed:\n* random forest alone showed an upward trend of test performance with training set size.\n* random forest with dimensionality reduction and GridSearchCV showed a relationship between test performance and training size that was too noisey to interpret.\n* in both cases, it appeared like the models were overfitting the training data\n\n# Deeper Dive into Dimensionality Reduction Analysis: 3 Components\n\nLogistic Regression combined with dimensionality reduction to 3 components provided the best result out of all the models studied above. The following section will take a closer look at these components.","6a0224d1":"##### PCA2 Analysis\n\nThis component is difficult to interpret. It might point to health (high absences) or family issues (low famrel)."}}