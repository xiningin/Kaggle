{"cell_type":{"53b789ba":"code","17f64773":"code","8aacb7a5":"code","91cd9f46":"code","2a0dbf14":"code","73bb1632":"code","0c6a4a35":"code","5c1a0f91":"code","05f7c17a":"code","e0769c49":"code","6c65b364":"code","4747fabf":"code","fc96b588":"code","bd33794e":"code","349970d7":"code","afc8e8d9":"code","29f99338":"code","a91e3a8a":"code","e53065f3":"code","7a184eda":"code","05ed406b":"code","81bf6829":"code","71aed8f6":"code","2071d785":"code","639f8b13":"code","9d3c7b34":"code","1dd40066":"code","c215cd4c":"code","8dcf8635":"code","42f2818f":"code","a1f29fdf":"code","3be42dd0":"code","7c4f7aa8":"code","f713f514":"code","944df8a4":"code","7e02fd25":"code","c6523cbe":"code","ff0b7bcf":"code","f00c16b3":"code","f390c723":"code","bacda372":"code","a3acfb61":"code","dbece7d9":"code","45280232":"code","6ee50c5d":"code","e959654d":"code","33b31c7d":"code","9ec9c3c9":"code","ab9176e0":"code","d30b7412":"code","84aa394a":"code","83e911d5":"code","c699d2ce":"code","127b26c7":"code","eeb38ad5":"code","540016e6":"code","657ff18c":"code","636e088c":"code","b37959c4":"code","fff20936":"code","47bf6db6":"code","47b23529":"code","7cc7b8bb":"code","4e1c7f89":"code","76fb0df3":"code","a3e79060":"code","aa6d9e47":"code","061f49c9":"code","593740ec":"code","59482400":"code","ca541ebf":"markdown","222955d8":"markdown","09148ed9":"markdown","841f0c1b":"markdown","ec561c10":"markdown","bfa1f07e":"markdown","baf2d15e":"markdown","e6816a71":"markdown","1bf1bbcf":"markdown","19167074":"markdown","e0a3fcf4":"markdown","8aa47ac2":"markdown","7e8b3a4f":"markdown","fdc80364":"markdown","97c9b0a3":"markdown","892be9ce":"markdown","86a575a5":"markdown","cbbf19f6":"markdown","cdda1a81":"markdown","3b68b933":"markdown","a70bdedf":"markdown","02463d20":"markdown","23838828":"markdown","af5312dd":"markdown","476e7d88":"markdown","01c7e18c":"markdown","4cbb9301":"markdown","04c9675f":"markdown","eef87910":"markdown","5e8d8260":"markdown","8b1dc31b":"markdown","506076bd":"markdown","26c7061f":"markdown","b216920f":"markdown","b2594be7":"markdown","4f95c5d5":"markdown","61868322":"markdown","6539ea93":"markdown","2fdf43fe":"markdown","a8695e7d":"markdown","4540f42a":"markdown","70a9582a":"markdown","db9b3776":"markdown","517c9539":"markdown","9adca027":"markdown","ed8bce05":"markdown","bf79c073":"markdown","58770bd8":"markdown","41809a6d":"markdown"},"source":{"53b789ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/favorita-grocery-sales-forecasting'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","17f64773":"!pip install py7zr","8aacb7a5":"import py7zr\nfrom subprocess import check_output\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/favorita-grocery-sales-forecasting'):\n    for filename in filenames:\n        archive = py7zr.SevenZipFile(os.path.join(dirname, filename), mode='r')\n        archive.extractall(path=\"\/kaggle\/working\")\n        archive.close()\n\nprint(check_output([\"ls\", \"..\/working\"]).decode(\"utf8\"))","91cd9f46":"# Importing the relevant libraries\nimport IPython.display\nimport json\nimport pandas as pd\nimport seaborn as sns\nimport squarify\n%matplotlib inline\nimport missingno as msno\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport numpy as np\nfrom matplotlib import pyplot as plt\ncolor = sns.color_palette()\n\n# D3 modules\nfrom IPython.core.display import display, HTML, Javascript\nfrom string import Template","2a0dbf14":"items = pd.read_csv(\"..\/working\/items.csv\")\nholiday_events = pd.read_csv(\"..\/working\/holidays_events.csv\", parse_dates=['date'])\nstores = pd.read_csv(\"..\/working\/stores.csv\")\noil = pd.read_csv(\"..\/working\/oil.csv\", parse_dates=['date'])\ntransactions = pd.read_csv(\"..\/working\/transactions.csv\", parse_dates=['date'])\n# the full training data's output: \"125,497,040 rows | 6 columns\"\n#Therefore I will only load approx 5% of the data just to get a rough idea of what is in store for us.\ntrain = pd.read_csv(\"..\/working\/train.csv\", nrows=6000000  , parse_dates=['date'])\ntrain_large = pd.read_csv('..\/working\/train.csv', skiprows = 115000000, names = train.columns, parse_dates = ['date'])\n","73bb1632":"train.head()","0c6a4a35":"print(\"Nulls in Oil columns: {0} => {1}\".format(oil.columns.values,oil.isnull().any().values))\nprint(\"=\"*70)\nprint(\"Nulls in holiday_events columns: {0} => {1}\".format(holiday_events.columns.values,holiday_events.isnull().any().values))\nprint(\"=\"*70)\nprint(\"Nulls in stores columns: {0} => {1}\".format(stores.columns.values,stores.isnull().any().values))\nprint(\"=\"*70)\nprint(\"Nulls in transactions columns: {0} => {1}\".format(transactions.columns.values,transactions.isnull().any().values))","5c1a0f91":"oil.head(3)","05f7c17a":" trace = go.Scatter(\n     name='Oil prices',\n     x=oil['date'],\n     y=oil['dcoilwtico'].dropna(),\n     mode='lines',\n     line=dict(color='rgb(20, 15, 200, 0.8)'),\n     #fillcolor='rgba(68, 68, 68, 0.3)',\n     fillcolor='rgba(0, 0, 216, 0.3)',\n     fill='tonexty' )\n\n data = [trace]\n\n layout = go.Layout(\n     yaxis=dict(title='Daily Oil price'),\n     title='Daily oil prices from Jan 2013 till July 2017',\n     showlegend = False)\n fig = go.Figure(data=data, layout=layout)\n py.iplot(fig, filename='pandas-time-series-error-bars')","e0769c49":"stores.head(3)","6c65b364":"temp=transactions.groupby(['store_nbr']).agg({'date':[np.min,np.max]}).reset_index()\ntemp['store_age']=temp['date']['amax']-temp['date']['amin']\ntemp['open_year']=temp['date']['amin'].dt.year\ndata=temp['open_year'].value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(data.index,data.values, alpha=0.8, color=color[0])\nplt.ylabel('Stores', fontsize=12)\nplt.xlabel('Store opening Year', fontsize=12)\nplt.title('When were the stores started?', fontsize=15)\nplt.xticks(rotation='vertical')\nplt.show()","4747fabf":"fig = plt.figure(figsize=(25, 21))\nmarrimeko=stores.city.value_counts().to_frame()\nax = fig.add_subplot(111, aspect=\"equal\")\nax = squarify.plot(sizes=marrimeko['city'].values,label=marrimeko.index,\n              color=sns.color_palette('cubehelix_r', 28), alpha=1)\nax.set_xticks([])\nax.set_yticks([])\nfig=plt.gcf()\nfig.set_size_inches(40,25)\nplt.title(\"Treemap of store counts across different cities\", fontsize=18)\nplt.show();","fc96b588":"fig = plt.figure(figsize=(25, 21))\nmarrimeko=stores.state.value_counts().to_frame()\nax = fig.add_subplot(111, aspect=\"equal\")\nax = squarify.plot(sizes=marrimeko['state'].values,label=marrimeko.index,\n              color=sns.color_palette('viridis_r', 28), alpha=1)\nax.set_xticks([])\nax.set_yticks([])\nfig=plt.gcf()\nfig.set_size_inches(40,25)\nplt.title(\"Treemap of store counts across different States\", fontsize=18)\nplt.show()","bd33794e":"stores.state.unique()","349970d7":"neworder = [23, 24, 26, 36, 41, 15, 29, 31, 32, 34, 39, \n            53, 4, 37, 40, 43, 8, 10, 19, 20, 33, 38, 13, \n            21, 2, 6, 7, 3, 22, 25, 27, 28, 30, 35, 42, 44, \n            48, 51, 16, 0, 1, 5, 52, 45, 46, 47, 49, 9, 11, 12, 14, 18, 17, 50]","afc8e8d9":"# Finally plot the seaborn heatmap\nplt.style.use('dark_background')\nplt.figure(figsize=(15,12))\nstore_pivot = stores.dropna().pivot(\"store_nbr\",\"cluster\", \"store_nbr\")\nax = sns.heatmap(store_pivot, cmap='jet', annot=True, linewidths=0, linecolor='white')\nplt.title('Store numbers and the clusters they are assigned to')","29f99338":" plt.style.use('seaborn-white')\n nbr_cluster = stores.groupby(['store_nbr','cluster']).size()\n nbr_cluster.unstack().iloc[neworder].plot(kind='bar',stacked=True, colormap= 'tab20', figsize=(13,11),  grid=False)\n plt.title('Store numbers and the clusters they are assigned to', fontsize=14)\n plt.ylabel('')\n plt.xlabel('Store number')\n plt.show()","a91e3a8a":"plt.style.use('seaborn-white')\ntype_cluster = stores.groupby(['type','cluster']).size()\ntype_cluster.unstack().plot(kind='bar',stacked=True, colormap= 'PuBu', figsize=(13,11),  grid=False)\nplt.title('Stacked Barplot of Store types and their cluster distribution', fontsize=18)\nplt.ylabel('Count of clusters in a particular store type', fontsize=16)\nplt.xlabel('Store type', fontsize=16)\nplt.show()","e53065f3":"plt.style.use('seaborn-white')\ncity_cluster = stores.groupby(['city','type']).store_nbr.size()\ncity_cluster.unstack().plot(kind='bar',stacked=True, colormap= 'viridis', figsize=(13,11),  grid=False)\nplt.title('Stacked Barplot of Store types opened for each city')\nplt.ylabel('Count of stores for a particular city')\nplt.show()","7a184eda":"holiday_events.head(3)","05ed406b":"#does the transactions peak at holiday events?\nplt.figure(figsize=(12,12))\nplt.plot(transactions.rolling(window=30,center=False).mean(),label='Rolling Mean');\nplt.plot(transactions.rolling(window=30,center=False).std(),label='Rolling sd');\nplt.legend();","81bf6829":"plt.style.use('seaborn-white')\nholiday_local_type = holiday_events.groupby(['locale_name', 'type']).size()\nholiday_local_type.unstack().plot(kind='bar',stacked=True, colormap= 'magma_r', figsize=(12,10),  grid=False)\nplt.title('Stacked Barplot of locale name against event type')\nplt.ylabel('Count of entries')\nplt.show()","71aed8f6":" with pd.option_context('display.max_rows', None, 'display.max_columns', 3):\n     print(holiday_events[['type','description']].apply(pd.Series.value_counts))","2071d785":"# Prepping the json file\nholiday_json = {\n\"name\": \"flare\",\n\"children\": [\n{\n\"name\": \"Additional\",\n\"children\":[\n{\"name\": \"Batalla de Pichincha\",       \"size\": 5.0},\n{\"name\": \"Cantonizacion de Cayambe\",   \"size\": 6.0},\n{\"name\": \"Cantonizacion de El Carmen\", \"size\": 6.0},\n{\"name\": \"Cantonizacion de Guaranda\",  \"size\": 6.0},\n{\"name\": \"Cantonizacion de Latacunga\", \"size\": 6.0},\n{\"name\": \"Cantonizacion de Libertad\",  \"size\": 6.0},\n{\"name\": \"Cantonizacion de Quevedo\",   \"size\": 6.0},\n{\"name\": \"Cantonizacion de Riobamba\",  \"size\": 6.0},\n{\"name\": \"Cantonizacion de Salinas\",   \"size\": 6.0},\n{\"name\": \"Cantonizacion del Puyo\",     \"size\": 6.0},\n{\"name\": \"Carnaval\",                   \"size\": 0.0},\n{\"name\": \"Dia de Difuntos\",            \"size\": 6.0},\n{\"name\": \"Dia de la Madre\",            \"size\": 5.0},\n{\"name\": \"Dia de la Madre-1\",          \"size\": 5.0},\n{\"name\": \"Dia del Trabajo\",             \"size\": 5.0},\n{\"name\": \"Fundacion de Guayaquil\",    \"size\": 5.0},\n{\"name\": \"Fundacion de Guayaquil-1\",  \"size\": 5.0},\n{\"name\": \"Fundacion de Quito\",        \"size\": 6.0},\n{\"name\": \"Fundacion de Quito-1\",      \"size\": 6.0},\n{\"name\": \"Navidad+1                                      \", \"size\": 6.0},\n{\"name\": \"Navidad-1                                      \", \"size\": 6.0},\n{\"name\": \"Navidad-2                                      \", \"size\": 6.0},\n{\"name\": \"Navidad-3                                      \", \"size\": 6.0},\n{\"name\": \"Navidad-4                                      \", \"size\": 6.0},\n]\n},\n{\n\"name\":  \"Holiday\",\n\"children\":[\n{\"name\": \"Fundacion de Ambato\",       \"size\": 6.0},\n{\"name\": \"Fundacion de Cuenca\",       \"size\": 7.0},\n{\"name\": \"Fundacion de Esmeraldas\",   \"size\": 6.0},\n{\"name\": \"Fundacion de Ibarra\",       \"size\": 7.0},\n{\"name\": \"Fundacion de Loja\",         \"size\": 6.0},\n{\"name\": \"Fundacion de Machala\",      \"size\": 6.0},\n{\"name\": \"Fundacion de Manta\",        \"size\": 6.0},\n{\"name\": \"Fundacion de Riobamba\",     \"size\": 6.0},\n{\"name\": \"Fundacion de Santo Domingo\", \"size\": 6.0}\n]\n},\n{\n\"name\": \"Event\",\n\"children\": [\n{\"name\": \"Inauguracion Mundial de futbol Brasil          \", \"size\": 1.0},\n{\"name\": \"Independencia de Ambato                        \", \"size\": 6.0},\n{\"name\": \"Independencia de Cuenca                        \", \"size\": 6.0},\n{\"name\": \"Independencia de Guaranda                      \", \"size\": 6.0},\n{\"name\": \"Independencia de Guayaquil                     \", \"size\": 6.0},\n{\"name\": \"Independencia de Latacunga                     \", \"size\": 6.0},\n{\"name\": \"Mundial de futbol Brasil: Cuartos de Final     \", \"size\": 2.0},\n{\"name\": \"Mundial de futbol Brasil: Ecuador-Francia      \", \"size\": 1.0},\n{\"name\": \"Mundial de futbol Brasil: Ecuador-Honduras     \", \"size\": 1.0},\n{\"name\": \"Mundial de futbol Brasil: Ecuador-Suiza        \", \"size\": 1.0},\n{\"name\": \"Mundial de futbol Brasil: Final                \", \"size\": 1.0},\n{\"name\": \"Mundial de futbol Brasil: Octavos de Final     \", \"size\": 4.0},\n{\"name\": \"Mundial de futbol Brasil: Semifinales          \", \"size\": 2.0},\n{\"name\": \"Mundial de futbol Brasil: Tercer y cuarto lugar\", \"size\": 1.0},\n{\"name\": \"Navidad                                        \", \"size\": 6.0},\n{\"name\": \"Primer Grito de Independencia                  \", \"size\": 6.0},\n{\"name\": \"Primer dia del ano                             \", \"size\": 5.0},\n{\"name\": \"Primer dia del ano-1                           \", \"size\": 5.0},\n{\"name\": \"Black Friday\",               \"size\": 3.0},\n{\"name\": \"Cyber Monday\",               \"size\": 3.0},\n{\"name\": \"Provincializacion Santa Elena                  \", \"size\": 6.0},\n{\"name\": \"Provincializacion de Cotopaxi                  \", \"size\": 6.0},\n{\"name\": \"Provincializacion de Imbabura                  \", \"size\": 6.0},\n{\"name\": \"Provincializacion de Santo Domingo             \", \"size\": 6.0},\n{\"name\": \"Terremoto Manabi                               \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+1                             \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+10                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+11                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+12                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+13                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+14                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+15                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+16                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+17                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+18                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+19                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+2                             \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+20                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+21                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+22                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+23                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+24                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+25                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+26                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+27                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+28                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+29                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+3                             \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+30                            \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+4                             \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+5                             \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+6                             \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+7                             \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+8                             \", \"size\": 1.0},\n{\"name\": \"Terremoto Manabi+9                             \", \"size\": 1.0}\n]\n},\n{\n\"name\": \"Transfer\",\n\"children\":[\n{\"name\": \"Traslado Batalla de Pichincha         \", \"size\": 2.0},\n{\"name\": \"Traslado Fundacion de Guayaquil       \", \"size\": 1.0},\n{\"name\": \"Traslado Fundacion de Quito           \", \"size\": 1.0},\n{\"name\": \"Traslado Independencia de Guayaquil   \", \"size\": 3.0},\n{\"name\": \"Traslado Primer Grito de Independencia\", \"size\": 2.0},\n{\"name\": \"Traslado Primer dia del ano           \", \"size\": 1.0},\n{\"name\": \"Viernes Santo                         \", \"size\": 5.0}\n]\n},\n    {\n\"name\": \"Bridge\",\n\"children\":[\n{\"name\": \"Puente Dia de Difuntos                         \", \"size\": 1.0},\n{\"name\": \"Puente Navidad                                 \", \"size\": 2.0},\n{\"name\": \"Puente Primer dia del ano                      \", \"size\": 2.0},\n]\n},\n{\n\"name\": \"Work Day\",\n\"children\":[\n    {\"name\": \"Recupero puente Navidad\", \"size\": 2.0},\n    {\"name\": \"ecupero puente primer dia del ano\", \"size\": 2.0},\n    {\"name\": \"Recupero Puente Navidad\", \"size\": 2.0},\n    {\"name\": \"Recupero Puente Primer dia del ano\", \"size\": 2.0},\n    {\"name\": \"Recupero Puente Dia de Difuntos\", \"size\": 2.0}\n]\n}\n] \n} ","639f8b13":"# dumping the holiday_events data into a json file\nwith open('output.json', 'w') as outfile:  \n    json.dump(holiday_json, outfile)\npd.read_json('output.json').head()\n\n#Embedding the html string\nhtml_string = \"\"\"\n<!DOCTYPE html>\n<meta charset=\"utf-8\">\n<style>\n\n.node {\n  cursor: pointer;\n}\n\n.node:hover {\n  stroke: #000;\n  stroke-width: 1.5px;\n}\n\n.node--leaf {\n  fill: white;\n}\n\n.label {\n  font: 11px \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n  text-anchor: middle;\n  text-shadow: 0 1px 0 #fff, 1px 0 0 #fff, -1px 0 0 #fff, 0 -1px 0 #fff;\n}\n\n.label,\n.node--root,\n.node--leaf {\n  pointer-events: none;\n}\n\n<\/style>\n<svg width=\"760\" height=\"760\"><\/svg>\n\"\"\"","9d3c7b34":"# Finally embed the D3.js to produce the circular treemap\njs_string=\"\"\"\n require.config({\n    paths: {\n        d3: \"https:\/\/d3js.org\/d3.v4.min\"\n     }\n });\n\n  require([\"d3\"], function(d3) {\n\n   console.log(d3);\n\nvar svg = d3.select(\"svg\"),\n    margin = 20,\n    diameter = +svg.attr(\"width\"),\n    g = svg.append(\"g\").attr(\"transform\", \"translate(\" + diameter \/ 2 + \",\" + diameter \/ 2 + \")\");\n\nvar color = d3.scaleSequential(d3.interpolateViridis)\n    .domain([-4, 4]);\n\nvar pack = d3.pack()\n    .size([diameter - margin, diameter - margin])\n    .padding(2);\n\nd3.json(\"output.json\", function(error, root) {\n  if (error) throw error;\n\n  root = d3.hierarchy(root)\n      .sum(function(d) { return d.size; })\n      .sort(function(a, b) { return b.value - a.value; });\n\n  var focus = root,\n      nodes = pack(root).descendants(),\n      view;\n\n  var circle = g.selectAll(\"circle\")\n    .data(nodes)\n    .enter().append(\"circle\")\n      .attr(\"class\", function(d) { return d.parent ? d.children ? \"node\" : \"node node--leaf\" : \"node node--root\"; })\n      .style(\"fill\", function(d) { return d.children ? color(d.depth) : null; })\n      .on(\"click\", function(d) { if (focus !== d) zoom(d), d3.event.stopPropagation(); });\n\n  var text = g.selectAll(\"text\")\n    .data(nodes)\n    .enter().append(\"text\")\n      .attr(\"class\", \"label\")\n      .style(\"fill-opacity\", function(d) { return d.parent === root ? 1 : 0; })\n      .style(\"display\", function(d) { return d.parent === root ? \"inline\" : \"none\"; })\n      .text(function(d) { return d.data.name; });\n\n  var node = g.selectAll(\"circle,text\");\n\n  svg\n      .style(\"background\", color(-1))\n      .on(\"click\", function() { zoom(root); });\n\n  zoomTo([root.x, root.y, root.r * 2 + margin]);\n\n  function zoom(d) {\n    var focus0 = focus; focus = d;\n\n    var transition = d3.transition()\n        .duration(d3.event.altKey ? 7500 : 750)\n        .tween(\"zoom\", function(d) {\n          var i = d3.interpolateZoom(view, [focus.x, focus.y, focus.r * 2 + margin]);\n          return function(t) { zoomTo(i(t)); };\n        });\n\n    transition.selectAll(\"text\")\n      .filter(function(d) { return d.parent === focus || this.style.display === \"inline\"; })\n        .style(\"fill-opacity\", function(d) { return d.parent === focus ? 1 : 0; })\n        .on(\"start\", function(d) { if (d.parent === focus) this.style.display = \"inline\"; })\n        .on(\"end\", function(d) { if (d.parent !== focus) this.style.display = \"none\"; });\n  }\n\n  function zoomTo(v) {\n    var k = diameter \/ v[2]; view = v;\n    node.attr(\"transform\", function(d) { return \"translate(\" + (d.x - v[0]) * k + \",\" + (d.y - v[1]) * k + \")\"; });\n    circle.attr(\"r\", function(d) { return d.r * k; });\n  }\n});\n  });\n \"\"\"","1dd40066":"h = display(HTML(html_string))\nj = IPython.display.Javascript(js_string)\nIPython.display.display_javascript(j)","c215cd4c":"holiday_events.type.unique()","8dcf8635":"transactions.head(3)","42f2818f":"plt.style.use('seaborn-white')\nplt.figure(figsize=(13,11))\nplt.plot(transactions.date.values, transactions.transactions.values, color='darkblue')\nplt.ylim(-50, 10000)\nplt.title(\"Distribution of transactions per day from 2013 till 2017\")\nplt.ylabel('transactions per day', fontsize= 16)\nplt.xlabel('Date', fontsize= 16)\nplt.show()","a1f29fdf":"#transactions\n# month over month sales\ntransactions['date']=pd.to_datetime(transactions['date'])\ntemp=transactions.groupby(['date']).aggregate({'store_nbr':'count','transactions':np.sum})\ntemp=temp.reset_index()\ntemp_2013=temp[temp['date'].dt.year==2013].reset_index(drop=True)\ntemp_2014=temp[temp['date'].dt.year==2014].reset_index(drop=True)\ntemp_2015=temp[temp['date'].dt.year==2015].reset_index(drop=True)\ntemp_2016=temp[temp['date'].dt.year==2016].reset_index(drop=True)\ntemp_2017=temp[temp['date'].dt.year==2017].reset_index(drop=True)\n\nsns.set(style=\"whitegrid\", color_codes=True)\nplt.figure(figsize=(15,14))\nplt.subplot(211)\nplt.plot(temp_2013['date'],temp_2013.iloc[:,1],label=\"2013\")\nplt.plot(temp_2014['date'],temp_2014.iloc[:,1],label=\"2014\")\nplt.plot(temp_2015['date'],temp_2015.iloc[:,1],label=\"2015\")\nplt.plot(temp_2016['date'],temp_2016.iloc[:,1],label=\"2016\")\nplt.plot(temp_2017['date'],temp_2017.iloc[:,1],label=\"2017\")\nplt.ylabel('Number of stores open', fontsize=12)\nplt.xlabel('Time', fontsize=12)\nplt.title('Number of stores open', fontsize=15)\nplt.xticks(rotation='vertical')\nplt.legend(['2013', '2014', '2015', '2016'], loc='lower right')\n\nplt.subplot(212)\nplt.plot(temp_2013.index,temp_2013.iloc[:,1],label=\"2013\")\nplt.plot(temp_2014.index,temp_2014.iloc[:,1],label=\"2014\")\nplt.plot(temp_2015.index,temp_2015.iloc[:,1],label=\"2015\")\nplt.plot(temp_2016.index,temp_2016.iloc[:,1],label=\"2016\")\nplt.plot(temp_2017.index,temp_2017.iloc[:,1],label=\"2017\")\n\n\nplt.ylabel('Number of stores open', fontsize=12)\nplt.xlabel('Day of year', fontsize=12)\nplt.title('Number of stores open', fontsize=15)\nplt.xticks(rotation='vertical')\nplt.legend(['2013', '2014', '2015', '2016'], loc='lower right')\nplt.show()","3be42dd0":"ts=transactions.loc[transactions['store_nbr']==47,['date','transactions']].set_index('date')\nts=ts.transactions.astype('float')\nplt.figure(figsize=(12,12))\nplt.title('Daily transactions in store #47')\nplt.xlabel('time')\nplt.ylabel('Number of transactions')\nplt.plot(ts);","7c4f7aa8":"items.head()","f713f514":"x, y = (list(x) for x in zip(*sorted(zip(items.family.value_counts().index, \n                                         items.family.value_counts().values), \n                                        reverse = False)))\ntrace2 = go.Bar(\n    y=items.family.value_counts().values,\n    x=items.family.value_counts().index,\n    marker=dict(\n        color=items.family.value_counts().values,\n        colorscale = 'Portland',\n        reversescale = False\n    ),\n    orientation='v',\n)\n\nlayout = dict(\n    title='Counts of items per family category',\n     width = 800, height = 800,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')","944df8a4":"x, y = (list(x) for x in zip(*sorted(zip(items['class'].value_counts().index, \n                                         items['class'].value_counts().values), \n                                        reverse = False)))\ntrace2 = go.Bar(\n    x=items['class'].value_counts().index,\n    y=items['class'].value_counts().values,\n    marker=dict(\n        color=items['class'].value_counts().values,\n        colorscale = 'Jet',\n        reversescale = True\n    ),\n    orientation='v',\n)\n\nlayout = dict(\n    title='Number of items attributed to a particular item class',\n     width = 800, height = 1400,\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True\n    ))\n\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')","7e02fd25":"plt.style.use('seaborn-white')\nfam_perishable = items.groupby(['family', 'perishable']).size()\nfam_perishable.unstack().plot(kind='bar',stacked=True, colormap= 'coolwarm', figsize=(12,10),  grid=False)\nplt.title('Stacked Barplot of locale name against event type')\nplt.ylabel('Count of entries')\nplt.show()","c6523cbe":"train.head()","ff0b7bcf":"plt.style.use('seaborn-deep')\nplt.figure(figsize=(13,11))\nplt.plot(train.date.values, train.unit_sales)\nplt.ylim(-50, 10000)\nplt.ylabel('transactions per day')\nplt.xlabel('Date')\nplt.show()","f00c16b3":"sale_day_item_level= pd.read_csv(\"..\/input\/memory-optimization-data-manipulation\/sale_day_item_level.csv\")\nsale_day_store_level= pd.read_csv(\"..\/input\/memory-optimization-data-manipulation\/sale_day_store_level.csv\")\nsale_store_item_level= pd.read_csv(\"..\/input\/memory-optimization-data-manipulation\/sale_store_item_level.csv\")","f390c723":"#Creating store level metrics\nsale_store_level=sale_day_store_level.groupby(['store_nbr'],as_index=False)['store_sales','item_variety'].agg(['sum'])\n\n# Here the group by gives a multiindex , removing that\nsale_store_level.columns = sale_store_level.columns.droplevel(1)\nsale_store_level=sale_store_level.reset_index()\nsale_store_level.head()","bacda372":"#Creating item level metrics\nsale_item_level=sale_day_item_level.groupby(['item_nbr'],as_index=False)['item_sales'].agg(['sum'])\n\nsale_item_level=sale_item_level.reset_index()\nsale_item_level.head()","a3acfb61":"# Sorting by sales\ntemp=sale_store_level.sort_values('store_sales',ascending=False).reset_index(drop=True)\ntemp=temp.set_index('store_nbr').head(10)\n\nplt.figure(figsize=(12,8))\nsns.barplot(temp.index,temp.store_sales, alpha=0.6, color='blue')\nplt.ylabel('Overall Sales', fontsize=12)\nplt.xlabel('Store Number', fontsize=12)\nplt.title('Top Stores by Overall sale', fontsize=15)\n# plt.xticks(rotation='vertical')\nplt.show()","dbece7d9":"# Sorting by sales\ntemp1=sale_item_level.sort_values('sum',ascending=False).reset_index(drop=True)\ntemp1=temp1.set_index('item_nbr').head(10)\nplt.figure(figsize=(12,8))\nx=temp1.index.values\ny=temp1['sum'].values\nsns.barplot(x,y, alpha=0.6, color='purple')\nplt.ylabel('Overall Sales', fontsize=12)\nplt.xlabel('Store Number', fontsize=12)\nplt.title('Top Items by Overall sale', fontsize=15)\nplt.show()","45280232":"#YOY sales\ntemp=sale_day_store_level.groupby('Year')['store_sales'].sum()\nplt.figure(figsize=(13,4))\nsns.pointplot(temp.index,temp.values, alpha=0.8)\nplt.ylabel('Overall Sales', fontsize=12)\nplt.xlabel('Year', fontsize=12)\nplt.title('Sale Year Over Year', fontsize=15)\nplt.xticks(rotation='vertical')\n\nplt.show()","6ee50c5d":"# month over month sales\ntemp=sale_day_store_level.groupby(['Year','Month']).aggregate({'store_sales':np.sum,'Year':np.min,'Month':np.min})\ntemp=temp.reset_index(drop=True)\nsns.set(style=\"whitegrid\", color_codes=True)\n# temp\nplt.figure(figsize=(15,8))\nplt.plot(range(1,13),temp.iloc[0:12,0],label=\"2013\")\nplt.plot(range(1,13),temp.iloc[12:24,0],label=\"2014\")\nplt.plot(range(1,13),temp.iloc[24:36,0],label=\"2015\")\nplt.plot(range(1,13),temp.iloc[36:48,0],label=\"2015\")\nplt.ylabel('Overall Sales', fontsize=12)\nplt.xlabel('Month', fontsize=12)\nplt.title('Monthly sales variation', fontsize=15)\nplt.xticks(rotation='vertical')\nplt.legend(['2013', '2014', '2015', '2016'], loc='upper left')\nplt.show()","e959654d":"#Count of stores in different types and clusters\nplt.figure(figsize=(15,12))\n#row col plotnumber - 121\nplt.subplot(221)\n# Count of stores for each type \ntemp = stores['cluster'].value_counts()\n#plot\nsns.barplot(temp.index,temp.values,color=color[5])\nplt.ylabel('Count of stores', fontsize=12)\nplt.xlabel('Cluster', fontsize=12)\nplt.title('Store distribution across cluster', fontsize=15)\n\nplt.subplot(222)\n# Count of stores for each type \ntemp = stores['type'].value_counts()\n#plot\nsns.barplot(temp.index,temp.values,color=color[7])\nplt.ylabel('Count of stores', fontsize=12)\nplt.xlabel('Type of store', fontsize=12)\nplt.title('Store distribution across store types', fontsize=15)\n\nplt.subplot(223)\n# Count of stores for each type \ntemp = stores['state'].value_counts()\n#plot\nsns.barplot(temp.index,temp.values,color=color[8])\nplt.ylabel('Count of stores', fontsize=12)\nplt.xlabel('state', fontsize=12)\nplt.title('Store distribution across states', fontsize=15)\nplt.xticks(rotation='vertical')\n\nplt.subplot(224)\n# Count of stores for each type \ntemp = stores['city'].value_counts()\n#plot\nsns.barplot(temp.index,temp.values,color=color[9])\nplt.ylabel('Count of stores', fontsize=12)\nplt.xlabel('City', fontsize=12)\nplt.title('Store distribution across cities', fontsize=15)\nplt.xticks(rotation='vertical')\nplt.show()","33b31c7d":"sale_store_level=sale_store_level.iloc[:,0:2]\n#print(sale_store_level)\nmerge=pd.merge(sale_store_level,stores,how='left',on='store_nbr')\n#temp\n\n#Sale of stores in different types and clusters\nplt.figure(figsize=(15,12))\n#row col plotnumber - 121\nplt.subplot(221)\n# Sale of stores for each type \ntemp = merge.groupby(['cluster'])['store_sales'].sum()\n#plot\nsns.barplot(temp.index,temp.values,color=color[5])\nplt.ylabel('Sales', fontsize=12)\nplt.xlabel('Cluster', fontsize=12)\nplt.title('Cumulative sales across store clusters', fontsize=15)\n\nplt.subplot(222)\n# sale of stores for each type \ntemp = merge.groupby(['type'])['store_sales'].sum()\n#plot\nsns.barplot(temp.index,temp.values,color=color[7])\nplt.ylabel('sales', fontsize=12)\nplt.xlabel('Type of store', fontsize=12)\nplt.title('Cumulative sales across store types', fontsize=15)\n\nplt.subplot(223)\n# sale of stores for each type \ntemp = merge.groupby(['state'])['store_sales'].sum()\n#plot\nsns.barplot(temp.index,temp.values,color=color[8])\nplt.ylabel('sales', fontsize=12)\nplt.xlabel('state', fontsize=12)\nplt.title('Cumulative sales across states', fontsize=15)\nplt.xticks(rotation='vertical')\n\nplt.subplot(224)\n# sale of stores for city\ntemp = merge.groupby(['city'])['store_sales'].sum()\n#plot\nsns.barplot(temp.index,temp.values,color=color[9])\nplt.ylabel('sales', fontsize=12)\nplt.xlabel('City', fontsize=12)\nplt.title('Cumulative sales across cities', fontsize=15)\nplt.xticks(rotation='vertical')\nplt.show()","9ec9c3c9":"sale_store_level=sale_store_level.iloc[:,0:2]\nmerge=pd.merge(sale_store_level,stores,how='left',on='store_nbr')\n\nplt.figure(figsize=(15,12))\n#row col plotnumber - 121\nplt.subplot(221)\n#plot\nsns.boxplot(x='cluster', y=\"store_sales\", data=merge)\nplt.ylabel('Sales', fontsize=12)\nplt.xlabel('Cluster', fontsize=12)\nplt.title('Variation across store clusters', fontsize=15)\n\nplt.subplot(222)\n# sale of stores for each type \nsns.boxplot(x='type', y=\"store_sales\", data=merge)\nplt.ylabel('sales', fontsize=12)\nplt.xlabel('Type of store', fontsize=12)\nplt.title('Variation across store types', fontsize=15)\n\nplt.subplot(223)\n# sale of stores for each type \nsns.boxplot(x='state', y=\"store_sales\", data=merge)\nplt.ylabel('sales', fontsize=12)\nplt.xlabel('state', fontsize=12)\nplt.title('Variation across states', fontsize=15)\nplt.xticks(rotation='vertical')\n\nplt.subplot(224)\n# sale of stores for city\nsns.boxplot(x='city', y=\"store_sales\", data=merge)\nplt.ylabel('sales', fontsize=12)\nplt.xlabel('City', fontsize=12)\nplt.title('Variation across cities', fontsize=15)\nplt.xticks(rotation='vertical')\nplt.show()","ab9176e0":"store_items=pd.merge(sale_store_item_level,items,on='item_nbr')\nstore_items=pd.merge(store_items,stores,on='store_nbr')\nstore_items['item_sales']=store_items['item_sales']\n\n#item\n# top selling items by store type\ntop_items_by_type=store_items.groupby(['type','item_nbr'])['item_sales'].sum()\ntop_items_by_type=top_items_by_type.reset_index().sort_values(['type','item_sales'],ascending=[True,False])\n\n#get top 5\ntop_items_by_type=top_items_by_type.groupby(['type']).head(5)\n\n\n#class\n# top selling item class by store type\ntop_class_by_type=store_items.groupby(['type','class'])['item_sales'].sum()\ntop_class_by_type=top_class_by_type.reset_index().sort_values(['type','item_sales'],ascending=[True,False])\n\n#get top 5\ntop_class_by_type=top_class_by_type.groupby(['type']).head(5)\n\n\n#family\n# top selling item family by store type\ntop_family_by_type=store_items.groupby(['type','family'])['item_sales'].sum()\ntop_family_by_type=top_family_by_type.reset_index().sort_values(['type','item_sales'],ascending=[True,False])\n\n#get top 5\ntop_family_by_type=top_family_by_type.groupby(['type']).head(5)","d30b7412":"top_family_by_type=store_items.groupby(['type','family'])['item_sales'].sum()\ntop_family_by_type=top_family_by_type.reset_index().sort_values(['type','item_sales'],ascending=[True,False])\nx=top_family_by_type.pivot(index='family',columns='type')\ncm = sns.light_palette(\"orange\", as_cmap=True)\nx = x.style.background_gradient(cmap=cm)\nx","84aa394a":"top_items_by_type=store_items.groupby(['type','item_nbr'])['item_sales'].sum()\ntop_items_by_type=top_items_by_type.reset_index().sort_values(['type','item_sales'],ascending=[True,False])\ntop_items_by_type=top_items_by_type.groupby(['item_nbr']).head(20)\n#print(top_items_by_type)\nx=top_items_by_type.pivot(index='item_nbr',columns='type')\nx['total']=x.sum(axis=1)\nx=x.sort_values('total',ascending=False)\ndel(x['total'])\nx=x.head(30)\ncm = sns.light_palette(\"green\", as_cmap=True)\nx = x.style.background_gradient(cmap=cm,axis=1)\nx","83e911d5":"import datetime as dt\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass prepare_data(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        print(\"prepare_data -> init\")\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        train_stores = X[0].merge(X[1], right_on = 'store_nbr', left_on='store_nbr')\n        train_stores_oil = train_stores.merge(X[2], right_on='date', left_on='date')\n        train_stores_oil_items = train_stores_oil.merge(X[3], right_on = 'item_nbr', left_on = 'item_nbr')\n        train_stores_oil_items_transactions = train_stores_oil_items.merge(X[4], right_on = ['date', 'store_nbr'], left_on = ['date', 'store_nbr'])\n        train_stores_oil_items_transactions_hol = train_stores_oil_items_transactions.merge(X[5], right_on = 'date', left_on = 'date')\n        \n        data_df = train_stores_oil_items_transactions_hol.copy(deep = True)\n        \n        # change the bool to int\n        data_df['onpromotion'] = data_df['onpromotion'].astype(int)\n        data_df['transferred'] = data_df['transferred'].astype(int)\n\n        # change the names\n        data_df.rename(columns={'type_x': 'st_type', 'type_y': 'hol_type'}, inplace=True)\n\n        # drop the id\n        data_df.drop(['id'], axis=1, inplace=True)\n        \n        print(data_df.head())\n        \n        # handle date\n        data_df['date'] = pd.to_datetime(data_df['date'])\n        data_df['date'] = data_df['date'].map(dt.datetime.toordinal)\n                \n        return data_df","c699d2ce":"# split dataframe into numerical values, categorical values and date\nclass split_data(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        print(\"split_data -> init\")\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        # Get columns for each type         \n        df_ = X.drop(['date'], axis = 1)\n        cols = df_.columns\n        num_cols = df_._get_numeric_data().columns\n        cat_cols = list(set(cols) - set(num_cols))\n        \n        data_num_df = X[num_cols]\n        data_cat_df = X[cat_cols]\n        data_date_df = X['date']\n        \n        return data_num_df, data_cat_df, data_date_df","127b26c7":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\n\nclass process_data(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        print(\"process_data -> init\")\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        ### numerical data\n        # impute nulls in numerical attributes\n        imputer = SimpleImputer(strategy=\"mean\", copy=\"true\")\n        num_imp = imputer.fit_transform(X[0])\n        data_num_df = pd.DataFrame(num_imp, columns=X[0].columns, index=X[0].index)\n        \n        # apply standard scaling\n        scaler = StandardScaler()\n        scaler.fit(data_num_df)\n        num_scaled = scaler.transform(data_num_df)\n        data_num_df = pd.DataFrame(num_scaled, columns=X[0].columns, index=X[0].index)\n        \n        ### categorical data\n        # one hot encoder\n        cat_encoder = OneHotEncoder(sparse=False)\n        data_cat_1hot = cat_encoder.fit_transform(X[1])\n        \n        # convert it to datafram with n*99 where n number of rows and 99 is no. of categories\n        data_cat_df = pd.DataFrame(data_cat_1hot, columns=cat_encoder.get_feature_names()) #, index=X[1].index)\n                \n        return data_num_df, data_cat_df, X[2]","eeb38ad5":"class join_df(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        print(\"join_df -> init\")\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X, y=None):\n        ### numerical data\n        data_df = X[0].join(X[1])\n        data_df = data_df.join(X[2])\n        \n        return data_df","540016e6":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\n\npipe_processing = Pipeline([\n        ('prepare_data', prepare_data()),\n        ('split_data', split_data()),\n        ('process_data', process_data()),\n        ('join_data', join_df())\n    ])\n\n# our prepared data\ndata_df = pipe_processing.fit_transform([train_large, stores, oil, items, transactions, holiday_events])\n\n# split it according to our feature engineering\nX = data_df.drop(['unit_sales', 'transactions'], axis=1)\nY = data_df[['unit_sales', 'transactions']]","657ff18c":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)","636e088c":"from sklearn.linear_model import LinearRegression,SGDRegressor,ElasticNet,Ridge\nfrom sklearn.svm import SVC\nfrom sklearn import linear_model\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\n\n\ndef checkModelPerformane(model):\n    model.fit(x_train.values, y_train.values)\n    \n    pred = model.predict(x_test.values)\n    \n    print(\"mean_squared_error: \",np.sqrt(mean_squared_error(y_test.values, pred))) \n    print(\"mean_absolute_error: \", np.sqrt(mean_absolute_error(y_test.values, pred)))","b37959c4":"print(\"LinearRegression\")\ncheckModelPerformane(LinearRegression())","fff20936":"print(\"lasso regression \")\ncheckModelPerformane(linear_model.Lasso(alpha=0.1))","47bf6db6":"print(\"ElasticNet regression \")\ncheckModelPerformane(ElasticNet())","47b23529":"print(\"Ridge regression \")\ncheckModelPerformane(Ridge(alpha=1.0))","7cc7b8bb":"print(\"Random Forest\")\ncheckModelPerformane(RandomForestRegressor(random_state=42)) ","4e1c7f89":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},]\n\nforest_reg = RandomForestRegressor(random_state=42)\n \ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(x_train.values, y_train.values)","76fb0df3":"grid_search.best_params_","a3e79060":"grid_search.best_estimator_","aa6d9e47":"!head test.csv","061f49c9":"final_model = grid_search.best_estimator_\n\n# load and process data\ntest = pd.read_csv(\"..\/working\/test.csv\", parse_dates=['date'])\n\npipe_processing2 = Pipeline([\n        ('split_data', split_data()),\n        ('process_data', process_data()),\n        ('join_data', join_df())\n    ])\n\ntest_df = pipe_processing2.fit_transform(test)\n\n","593740ec":"test_df\n","59482400":"# final_predictions = final_model.predict(test_x)","ca541ebf":"# Model fine tuning","222955d8":"### Custom transform\n\n1. Fill missing data in numerical attributes\n2. apply standard scaler to numerical attributes\n3. Convert categorical data into numerical","09148ed9":"# Further Analysis","841f0c1b":"Further to make EDA easier, we used data which rolled up the sales to different levels\n\n - Day-Store level\n - Day-Item level\n - Store level\n - Item level\n - Day level","ec561c10":"There seems to be certain local holidays where some of the stores are closed. But there is no consistent pattern of holidays where stores are closed","bfa1f07e":"## Stores Data","baf2d15e":"**Selected features as inputs to the model**\n\ndate, holiday.type, holidaye.locale, holiday.locale_name, holiday_transfered, store_nbr, store.city, store.state, store.type, store.cluster, transactions, item_nbr, item.family, item.class, on_promotion, perishable, dcoilwtico.\n\n**Selected features as outputs of the model**\n\ntransactions per store, unit_sales per item","e6816a71":"Our store numbers will now be arranged against their corresponding shop clusters, allowing us to see whether there are any obvious trends or links in the data. To do so, I'll use the groupby and pivot statements to group our stores Python dataframe based on the values \"store nbr\" and \"cluster.\" Then I'll unstack the grouping by pivoting on the level of store nbr index labels, yielding a DataFrame with a new level of columns that are the store clusters whose inner-most level pertain to the pivoted store nbr index labels. This method is widely used in Python to create stacked barplots, but because we just have unique store nbr numbers, we'll just receive barplots of store numbers arranged by their relevant clusters.","1bf1bbcf":"The chain established itselft in Quito in 1952 (We knew this from Wikipedia), so let's pick a shop in Quito as a starting point, as the brand is well established there. Let us pick #47 and plot the corresponding transactions time series. With a well established store, we can predict that the time series will be almost stationary. High seasonality is expected too, as people consume more during celebration periods.","19167074":"### D3.js visualization library","e0a3fcf4":"### Store #47","8aa47ac2":"### takes files in, outputs a complete dataframe","7e8b3a4f":"As we can see from the plot, the top 3 family categories are the GROCERY I, BEVERAGES and CLEANING categories.","fdc80364":"### Store Distrubution","97c9b0a3":"## Training Data","892be9ce":"### Lasso regression","86a575a5":"### yet another transform","cbbf19f6":" The only missing data occurs in the oil data file, which provides the historical daily price for oil.","cdda1a81":"### Generic function for modelling and testing","3b68b933":"### Sale distribution","a70bdedf":"### Sale variation","02463d20":"## Holiday Events Data","23838828":"# Test model on test set","af5312dd":"### Generate test and training data\n","476e7d88":"**Here we analyze the data and select the features for our model to be trained on.**","01c7e18c":"The bigger yearly periodic spike in transactions seem to occur at the end of the year in December. Perhaps this is due to some sort of Christmas sale\/discount that Corporacion Favorita holds every December.","4cbb9301":"we can look at the distribution of clusters based on the store type to see if we can identify any apparent relationship between types and the way the company has decided to cluster the particular store.","04c9675f":"## Items data","eef87910":"Guayaquil and Quito are two cities that stand out in terms of the range of retail kinds available. These are unsurprising given that Quito is Ecuador's capital and Guayaquil is the country's largest and most populated metropolis. As a result, one might expect Corporacion Favorita to target these major cities with the most diverse store types, as evidenced by the highest counts of store nbrs attributed to those two cities.","5e8d8260":"Random forest model has the lowest error, thus we are going to use it and fine tune it.","8b1dc31b":"5 Stores were opened in 2015 and 1 each in 2014 and 2017","506076bd":"The distribution of sale across the store types for the top 30 items have been shown below\nThe darker the color gradient the more the store type has contributed to the sale of items in that class\n","26c7061f":"From visualising the store numbers side-by-side based on the clustering, we can identify certain patterns. \nFor example clusters 3, 6, 10 and 15 are the most common store clusters based off the fact that there are more store_nbrs attributed to them then the others \nwhile on the other end of the spectrum, we have clusters 5 and 17 which are only related to the stores 44 and 51 respectively.","b216920f":"From January 2013 to July 2017, this graph demonstrates that the daily oil price has been on a declining trend. Whereas the price of oil began 2013 by rising and even breaking the $100 barrier for a few months in 2013, the price of oil began to plummet in the middle of 2014, resulting in a significant decline in the price of oil. This trend appears to be true based on some quick open-source research (i.e. Googling), as oil prices were relatively stable from 2010 to mid-2014, after which they drastically fell (due to a confluence of factors including weak demand due to poor economic growth and surging alternative crude oil sources such as shale\/tar sands).","b2594be7":"### Custom transform for splitting the data","4f95c5d5":"## Explore and prepare the data","61868322":"### find out the best parameters for our model","6539ea93":"### Ridge regression","2fdf43fe":"### Random forests","a8695e7d":"## Transactions data","4540f42a":"# Modelling and testing","70a9582a":"### Grid search","db9b3776":"### Linear regression","517c9539":"## Oil Data","9adca027":"# feature engineering","ed8bce05":"### ElasticNet regression","bf79c073":"**Train**\nid, date, store_nbr, item_nbr, unit_scale, on_promotion\n\n**Items**\nitem_nbr, family, class, perishable\n\n**Holidays_events**\ndate, type, locale, locale_name, description, transferred\n\n**Stores**\nstore_nbr, city, state, type, cluster\n\n**Oil**\ndate, dcoilwtico\n\n**Transactions**\ndate, store_nbr, transactions","58770bd8":"# DATA pipeline","41809a6d":"# EDA"}}