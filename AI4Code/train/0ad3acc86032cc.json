{"cell_type":{"6e58cad5":"code","248e3e39":"code","411aba18":"code","2e5d9049":"code","b5771569":"code","314001ef":"code","873b41ec":"code","e935038b":"code","7f4cbe49":"code","9ce750d5":"code","12f0f6e0":"code","3a7a91b1":"code","28b05b21":"code","db4a800c":"code","3434634e":"code","2d0b221a":"code","1567b9e0":"code","5dcca51c":"code","fc4b6760":"code","f8f81c4f":"code","cb7633bc":"code","b2eaf263":"code","2c1a04f1":"code","9ee9f8ce":"code","a91f2abf":"code","2057880a":"code","b336162d":"code","b26c9446":"code","431c5d10":"code","2cd8f962":"code","8d13dc16":"code","db6efc9b":"code","68e449aa":"code","a8fb8f70":"code","088b087a":"code","12a54e39":"code","2e15d075":"code","2bbdb695":"markdown","9211ba3c":"markdown","9ea92010":"markdown","a0400770":"markdown","0afb39af":"markdown","f6c9499b":"markdown","4eca075d":"markdown","5a84fd8e":"markdown","80e7ad0a":"markdown","12557221":"markdown","96f13eeb":"markdown","6d3e9a61":"markdown","0159023c":"markdown","53412262":"markdown","9d5c314f":"markdown","d7dc1582":"markdown","9cecf9e6":"markdown","2872cbda":"markdown","0379f056":"markdown"},"source":{"6e58cad5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom IPython.display import display, Image","248e3e39":"df = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\n","411aba18":"df.head()","2e5d9049":"df.info()","b5771569":"df.describe()","314001ef":"sns.pairplot(df)","873b41ec":"sns.countplot(x='quality',data=df)","e935038b":"sns.boxplot('quality','fixed acidity',data=df)","7f4cbe49":"sns.boxplot('quality', 'volatile acidity', data = df)","9ce750d5":"sns.boxplot('quality', 'citric acid', data = df)","12f0f6e0":"sns.boxplot('quality', 'residual sugar', data = df)","3a7a91b1":"sns.boxplot('quality', 'chlorides', data = df)","28b05b21":"sns.boxplot('quality', 'free sulfur dioxide', data = df)","db4a800c":"sns.boxplot('quality', 'total sulfur dioxide', data = df)","3434634e":"sns.boxplot('quality', 'density', data = df)","2d0b221a":"sns.boxplot('quality', 'pH', data = df)","1567b9e0":"sns.boxplot('quality', 'sulphates', data = df)","5dcca51c":"sns.boxplot('quality', 'alcohol', data = df)","fc4b6760":"#New column is created as NewQuality. It has the values 1,2, and 3. \n#1 - Bad quality\n#2 - Average quality\n#3 - Excellent quality\n#Split the dataset \n#1,2,3 - Bad quality\n#4,5,6,7 - Average quality\n#8,9,10 - Excellent quality\n\n#Create an empty list called NewQuality\nNewQuality = []\nfor i in df['quality']:\n    if i >= 1 and i <= 3:\n        NewQuality.append('1')\n    elif i >= 4 and i <= 7:\n        NewQuality.append('2')\n    elif i >= 8 and i <= 10:\n        NewQuality.append('3')\ndf['NewQuality'] = NewQuality","f8f81c4f":"x = df.iloc[:,:11]\ny = df['NewQuality']","cb7633bc":"x.head()","b2eaf263":"y.head()","2c1a04f1":"#Train and Test splitting of data \nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","9ee9f8ce":"#Applying Standard scaling to get optimized result\nsc = StandardScaler()","a91f2abf":"X_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","2057880a":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","b336162d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)","b26c9446":"#print confusion matrix and accuracy score\nlr_conf_matrix = confusion_matrix(y_test, lr_predict)\nlr_acc_score = accuracy_score(y_test, lr_predict)\nprint(lr_conf_matrix)\nprint(lr_acc_score*100)","431c5d10":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(X_train,y_train)\ndt_predict = dt.predict(X_test)","2cd8f962":"#print confusion matrix and accuracy score\ndt_conf_matrix = confusion_matrix(y_test, dt_predict)\ndt_acc_score = accuracy_score(y_test, dt_predict)\nprint(dt_conf_matrix)\nprint(dt_acc_score*100)","8d13dc16":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train,y_train)\nnb_predict=nb.predict(X_test)","db6efc9b":"#print confusion matrix and accuracy score\nnb_conf_matrix = confusion_matrix(y_test, nb_predict)\nnb_acc_score = accuracy_score(y_test, nb_predict)\nprint(nb_conf_matrix)\nprint(nb_acc_score*100)","68e449aa":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nrf_predict=rf.predict(X_test)","a8fb8f70":"#print confusion matrix and accuracy score\nrf_conf_matrix = confusion_matrix(y_test, rf_predict)\nrf_acc_score = accuracy_score(y_test, rf_predict)\nprint(rf_conf_matrix)\nprint(rf_acc_score*100)","088b087a":"from sklearn.svm import SVC","12a54e39":"#we shall use the rbf kernel first and check the accuracy\nlin_svc = SVC()\nlin_svc.fit(X_train, y_train)\nlin_svc=rf.predict(X_test)","2e15d075":"#print confusion matrix and accuracy score\nlin_svc_conf_matrix = confusion_matrix(y_test, rf_predict)\nlin_svc_acc_score = accuracy_score(y_test, rf_predict)\nprint(lin_svc_conf_matrix)\nprint(lin_svc_acc_score*100)","2bbdb695":"### 5. SVM classifier\n\nSupport Vector Machine (SVM) is a supervised machine learning algorithm capable of performing classification, regression and even outlier detection.","9211ba3c":"### Let's begin!","9ea92010":"#### Accuracy of the logistic regression model is 98.125%","a0400770":"## I hope this kernal is helpful for you. Your UPVOTE means alot to me!!","0afb39af":"\n### What is a classifier?\n* A classifier is a machine learning model that is used to discriminate different objects based on certain features.\n* Machine Learning Classifiers can be used to predict. Given example data (measurements), the algorithm can predict the class the data belongs to.\n* Start with training data. Training data is fed to the classification algorithm. After training the classification algorithm (the fitting function), you can make predictions.\n\n\n\n","f6c9499b":"### We are going to use the Red Wine Quality dataset.\nThis datasets is related to red variants of the Portuguese \"Vinho Verde\" wine.","4eca075d":"## Thank you for opening this notebook!!","5a84fd8e":"![image.png](attachment:image.png)","80e7ad0a":"#### Accuracy of random forest is 97.8125%","12557221":"#### Accuracy of the Decision tree is 98.125%","96f13eeb":"#### Accuracy of SVM classifier is 97.8125%","6d3e9a61":"## Thank you !!","0159023c":"### 4. Random Forest \n\n#### A Random Forest is an ensemble technique capable of performing both regression and classification tasks. The basic idea behind this is to combine multiple decision trees in determining the final output rather than relying on individual decision trees.","53412262":"### 2. Decision tree\n#### Decision tree is the most powerful and popular tool for classification and prediction. Decision-tree algorithm falls under the category of supervised learning algorithms. It works for both continuous as well as categorical output variables.\n\n","9d5c314f":"#### Accuracy of the Naive Bayes is 95.3125%","d7dc1582":"### 1. Logistic Regression\n#### Here the output variable can have more than two possible discrete outputs. Given below is implementation of the Logistic regression to make predictions on the dataset.\n","9cecf9e6":"### Main objective of this kernel is to learn logistic regression, decision tree, naive bayes, random forest and SVM classifier.","2872cbda":"### Training and testing data is used to perform machine learning algorithm","0379f056":"### 3. Naive Bayes\n#### Principle of Naive Bayes Classifier: A Naive Bayes classifier is a probabilistic machine learning model that\u2019s used for classification task.It is based on the Bayes theorem. Bayes Theorem: P(A\/B) = P(B\/A)*P(A)\/P(B)\n#### Types of Naive Bayes Classifier:\n\n* Multinomial Naive Bayes:\nThis is mostly used for document classification problem, i.e whether a document belongs to the category of sports, politics, technology etc. The features\/predictors used by the classifier are the frequency of the words present in the document.\n\n* Bernoulli Naive Bayes:\nThis is similar to the multinomial naive bayes but the predictors are boolean variables. The parameters that we use to predict the class variable take up only values yes or no, for example if a word occurs in the text or not.\n\n* Gaussian Naive Bayes:\nWhen the predictors take up a continuous value, we assume that these values are sampled from a gaussian distribution."}}