{"cell_type":{"6eca98c1":"code","7bb21c45":"code","77a3caad":"code","d570a0eb":"code","f51b82ec":"code","e3ec60fe":"code","7e8cd663":"code","e6eecc88":"code","dbc04140":"code","01702cd9":"code","f62213d2":"code","abc0fd4c":"code","4a74d68d":"code","ec31a67c":"code","7f02d018":"code","cd876cdf":"code","7ece1984":"code","5fd76123":"code","b4619bc0":"code","c30febf9":"code","7a289eca":"code","9b024873":"code","28605c2a":"code","058a21b5":"code","7169be0c":"code","75e85d72":"code","d039b72b":"code","0aa3cfdf":"code","2dc6b33f":"code","f5c67a7c":"code","4e45b0ed":"code","e2b82ea1":"code","2c9e362a":"code","2f3a52be":"code","fdb08993":"code","d3c8fd34":"code","9719129e":"code","76f56903":"code","16283ad3":"code","90243b40":"markdown","e7b2e064":"markdown","e401c142":"markdown","dea6c3dc":"markdown","1cdda422":"markdown","246f123b":"markdown"},"source":{"6eca98c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7bb21c45":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","77a3caad":"train_Y = train_df['Survived']\ntrain_df.drop('Survived',axis = 1,inplace = True)\n","d570a0eb":"train_X = train_df","f51b82ec":"test_X = test_df","e3ec60fe":"train_X","7e8cd663":"# PassengerId , Name, Ticket , Cabin\ntrain_X.drop(['PassengerId','Name','Ticket','Cabin'],axis = 1,inplace = True)\n","e6eecc88":"test_passengerId = test_X['PassengerId']\ntest_X.drop(['PassengerId','Name','Ticket','Cabin'],axis = 1,inplace = True)","dbc04140":"test_X","01702cd9":"train_X.isna().sum()","f62213d2":"#Calucating the mean of the age in entire dataset\n\nmean_age = train_X['Age'].mean()\nmode_embarked = train_X['Embarked'].mode()","abc0fd4c":"#Filling the mean age in Nan and None values\ntrain_X['Age'].fillna(mean_age,axis = 0,inplace = True)","4a74d68d":"train_X['Embarked'].fillna(mode_embarked[0],axis = 0, inplace = True)\n","ec31a67c":"#Checking whether there are any missing values in test data\n\ntest_X.isna().sum()","7f02d018":"# Apply same mean to the test data and same mode in embarked incase if there are any missing values in test data\ntest_X['Age'].fillna(mean_age,axis = 0,inplace = True)\ntest_X['Embarked'].fillna(mode_embarked[0],axis = 0, inplace = True)","cd876cdf":"# As there is missing value in Feature \"Fare\" in test data.  Filling it with the mean \nmean_fare = test_X['Fare'].mean()\ntest_X['Fare'].fillna(mean_fare,axis = 0,inplace = True)","7ece1984":"#Check whether any missing values again\ntest_X.isna().sum()","5fd76123":"train_X['Sex'].replace(to_replace= ['male','female'], value = [0,1],inplace = True)","b4619bc0":"test_X['Sex'].replace(to_replace= ['male','female'], value = [0,1],inplace = True)","c30febf9":"train_X = pd.get_dummies(train_X)","7a289eca":"test_X = pd.get_dummies(test_X)","9b024873":"train_X","28605c2a":"#from sklearn.preprocessing import RobustScaler\n#scaler = RobustScaler()\n#train_X = scaler.fit_transform(train_X)","058a21b5":"#test_X = scaler.transform(test_X)","7169be0c":"#test_X","75e85d72":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(train_X,train_Y)\npred_y = rfc.predict(test_X)","d039b72b":"# GridSearchCV to find optimal n_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_depth': range(2, 20, 5)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(train_X, train_Y)","0aa3cfdf":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","2dc6b33f":"pred_y = rf.best_estimator_.predict(test_X)","f5c67a7c":"'''from sklearn.svm import SVC\nfrom sklearn.model_selection import KFold , cross_val_score,GridSearchCV\nkfold = KFold(n_splits = 5, shuffle = True,random_state = 0)\nmodel = SVC(C = 2,kernel = 'rbf')\nvalidation = cross_val_score(model,train_X,train_Y,cv = kfold,scoring = 'accuracy')'''\n","4e45b0ed":"#validation.mean()","e2b82ea1":"#model.fit(train_X,train_Y)","2c9e362a":"#pred_y = model.predict(test_X)","2f3a52be":"\"\"\"from sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.svm import LinearSVC\n\npipe = Pipeline(steps=[\n                       ('classifier', LogisticRegression(solver='liblinear', max_iter=2000))])\"\"\"","fdb08993":"\"\"\"param_distributions = [\n              {\n               \n                'classifier': [LogisticRegression(solver='liblinear', max_iter=2000, C = 5.0)],\n                'classifier__penalty': ['l2', 'l1']\n              },\n              {\n               \n                'classifier': [KNeighborsClassifier()],\n                'classifier__n_neighbors': range(1,20),\n                'classifier__p': [1, 2, 3, 4]\n              }\n              \n            ]\n\nrandom_search_cv = RandomizedSearchCV(pipe, param_distributions=param_distributions, n_iter=70, scoring='accuracy', refit=True, cv=5, random_state=0) \nrandom_search_cv.fit(train_X, train_Y)\nprint(random_search_cv.best_params_)\"\"\"","d3c8fd34":"#random_search_cv.best_score_","9719129e":"#best_classifier = random_search_cv.best_estimator_\n#predicted_y = best_classifier.predict(test_X)","76f56903":"#predicted_y","16283ad3":"output = pd.DataFrame({'PassengerId': test_passengerId, 'Survived': pred_y})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","90243b40":"> **Removing columns that won't help in our model**","e7b2e064":"# Converting the categorical columns into numerical columns","e401c142":"**Reading the .csv file from the given directory to the DATAFRAMES**","dea6c3dc":"**Feature Selectiona and Feature Engineering**","1cdda422":"Lets find the missing values","246f123b":"# Filling the missing values"}}