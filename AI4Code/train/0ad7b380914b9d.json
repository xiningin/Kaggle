{"cell_type":{"17bfbc00":"code","1e8c7269":"code","658a6722":"code","76c2f33b":"code","60efa4ff":"code","38ea8905":"code","643949a9":"code","ae22cae0":"code","85d9162f":"code","0426bc1c":"code","a1d45fff":"code","1b7a3562":"code","0d2d3b4b":"code","e97a2b03":"code","59f99761":"code","1fb72dc3":"code","b3638ad8":"code","2306acb2":"code","9d785ac0":"code","77873bde":"code","cd510691":"code","985dc3c3":"code","8378229c":"code","9343b444":"code","6be36d34":"markdown","e5b4b6f0":"markdown","429a448c":"markdown","74d67e3a":"markdown","55f78685":"markdown","2c929235":"markdown","0c9b92d9":"markdown","2f0b13d5":"markdown","ac5c58f6":"markdown","a4805b29":"markdown","1e3cfefc":"markdown","cbabae53":"markdown","a863844f":"markdown","47e07020":"markdown","16fe47cb":"markdown","2e566c51":"markdown","6cda3498":"markdown","31a8903d":"markdown","11c12ae4":"markdown","a61fa8cd":"markdown","ea630b52":"markdown","01df5851":"markdown"},"source":{"17bfbc00":"# Read in data\nimport pandas as pd\n\nwater_quality = pd.read_csv('..\/input\/water-potability\/water_potability.csv')\nwater_quality.info()","1e8c7269":"water_quality[\"Potability\"] = water_quality[\"Potability\"].astype(\"category\")\ntype(water_quality.iloc[:,1].values[1])","658a6722":"water_quality.describe()","76c2f33b":"water_quality.isna().sum().values","60efa4ff":"water_quality.loc[water_quality[\"Potability\"] == 1].describe()","38ea8905":"water_quality.loc[water_quality[\"Potability\"] == 0].describe()","643949a9":"import matplotlib.pyplot as plt\npotability = water_quality[\"Potability\"].hist()\npotability.set_title(\"Potability\")\nwater_quality.hist(bins = 50, figsize = (20, 10))\nplt.show\nplt.suptitle(\"Water Quality Distribution plots\", fontsize = 25)","ae22cae0":"import matplotlib.pyplot as plt\n\nfig, axs = plt.subplots(nrows = 3, ncols = 3, figsize = (20, 10))\nfig.suptitle(\"Water Quality Box plots\", fontsize = 25)\nfig.subplots_adjust(hspace=0.5)\ncounter = 0\n\nfor row_idx in range(3):\n    for col_idx in range(3):\n        axs[row_idx, col_idx].boxplot(water_quality.iloc[:,counter].dropna(), vert = False)\n        axs[row_idx, col_idx].set_title(water_quality.columns[counter])\n        counter += 1\n","85d9162f":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.utils import resample\nfrom sklearn.utils import shuffle\n\n# Data stratification\nsplit = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 42)\nfor train_index, test_index in split.split(water_quality, water_quality[\"Potability\"]):\n    strat_train_set = water_quality.loc[train_index]\n    strat_test_set = water_quality.loc[test_index]\n    \n# Upsampling of data\nportable = strat_train_set.loc[strat_train_set[\"Potability\"] == 1]\nnot_portable  = strat_train_set.loc[strat_train_set[\"Potability\"] == 0]\nportable = resample(portable, replace = True, n_samples = len(not_portable), random_state = 42)\nstrat_train_set = pd.concat([portable, not_portable])\nstrat_train_set = shuffle(strat_train_set, random_state = 42)\n\nstrat_train_set[\"Potability\"].hist()\nplt.title(\"Training data: Potability\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Count\")","0426bc1c":"strat_test_set[\"Potability\"].value_counts()\/len(strat_test_set)","a1d45fff":"water_quality[\"Potability\"].value_counts()\/len(water_quality)","1b7a3562":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# Create pipeline\npipeline = Pipeline([\n    (\"iterative_imputer\", SimpleImputer(strategy = \"mean\")),\n    (\"std_scaler\", StandardScaler())\n])\n\n# predictors\ntraining_pred = pipeline.fit_transform(strat_train_set.drop(\"Potability\", axis = 1))\n\n# explanatory\ntraining_labels = strat_train_set[\"Potability\"].values\n\n\n# Fit and transform data through pipeline \ntraining_pred = pd.DataFrame(training_pred)\ntraining_pred\n","0d2d3b4b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\nfrom sklearn import metrics\n\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Means:\", scores.mean())\n    print(\"Standard Deviation:\", scores.std())\n\nlogistic_model = LogisticRegression(solver = \"liblinear\", random_state = 42)\nlogistic_model.fit(training_pred, training_labels)\nlog_scores = cross_val_score(logistic_model, \n                             training_pred, \n                             training_labels,\n                             scoring = \"accuracy\", cv = 10)\ndisplay_scores(log_scores)","e97a2b03":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state = 42)\ndt.fit(training_pred, training_labels)\ndt_scores = cross_val_score(dt, \n                           training_pred,\n                           training_labels,\n                           scoring = \"accuracy\", cv = 10)\ndisplay_scores(dt_scores)","59f99761":"from sklearn.ensemble import RandomForestClassifier\n\nrf_untuned = RandomForestClassifier(random_state = 42)\nrf_untuned.fit(training_pred, training_labels)\nrf_scores = cross_val_score(rf_untuned, training_pred, training_labels,\n                               scoring = \"accuracy\")\ndisplay_scores(rf_scores)","1fb72dc3":"model_scores = {\"mean_cv_accuracy\":[np.mean(log_scores),\n                               np.mean(dt_scores),\n                               np.mean(rf_scores)]}\nmodel_scores = pd.DataFrame(data = model_scores, index = [\"log\", \"dt\", \"rf\"])\nmodel_scores","b3638ad8":"# The following lines of code were adapted from \n\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Create the random grid\nrandom_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n               'criterion': ['gini', 'entropy'],\n               'max_features': ['auto', 'sqrt'],\n               'max_depth': max_depth,\n               'min_samples_split': [2, 5, 10],\n               'min_samples_leaf': [1, 2, 4],\n               'bootstrap': [True, False],\n               'random_state' : [42]}\nrf = RandomForestClassifier(random_state = 42)\nrf_random = RandomizedSearchCV(estimator = rf, \n                               param_distributions = random_grid, \n                               n_iter = 100, cv = 3, \n                               verbose=2, n_jobs = -1, random_state = 42)\nrf_random.fit(training_pred, training_labels)","2306acb2":"rf_random.best_params_","9d785ac0":"# Gridsearch for best parameter.\n# import joblib\n\nfrom sklearn.model_selection import GridSearchCV\n\nparam_1 = {\n    \"n_estimators\" : [int(x) for x in np.linspace(start = 1600, stop = 2000, num = 5)],\n    \"min_samples_split\" : [5],\n    \"min_samples_leaf\" : [1],\n    \"max_features\" : ['sqrt'],\n    \"max_depth\" : [int(x) for x in np.linspace(start = 50, stop = 100, num = 5)],\n    \"criterion\" : ['gini'],\n    \"bootstrap\" : [False],\n    \"random_state\" : [42]\n}\n\nparam_grid = [param_1]\nrf = RandomForestClassifier(random_state = 42)\nrf_grid_search = GridSearchCV(rf, param_grid, cv = 5, scoring = \"accuracy\",\n                             return_train_score = True, n_jobs = -1, verbose = 2)\nrf_grid_search.fit(training_pred, training_labels)\n\nrf_grid_search.best_estimator_.get_params()\n\n# Export best estimator into pkl file: Uncomment next line to export model into a file\n# joblib.dump(rf_grid_search.best_estimator_, \"random_forest_best_estimator.pkl\")","77873bde":"# Gridsearch for best parameter.\nimport joblib\nfrom sklearn.model_selection import GridSearchCV\nparam_2 = {\n    \"n_estimators\" : [int(x) for x in np.linspace(start = 1300, stop = 1600, num = 5)],\n    \"min_samples_split\" : [5],\n    \"min_samples_leaf\" : [1],\n    \"max_features\" : ['sqrt'],\n    \"max_depth\" : [int(x) for x in np.linspace(start = 30, stop = 55, num = 5)],\n    \"criterion\" : ['gini'],\n    \"bootstrap\" : [False],\n    \"random_state\" : [42]\n}\n\nparam_grid = [param_2]\nrf = RandomForestClassifier(random_state = 42)\nrf_grid_search = GridSearchCV(rf, param_grid, cv = 5, scoring = \"accuracy\",\n                             return_train_score = True, n_jobs = -1, verbose = 2)\nrf_grid_search.fit(training_pred, training_labels)\n\nrf_grid_search.best_estimator_.get_params()\n\n# Export best estimator into pkl file\n#joblib.dump(rf_grid_search.best_estimator_, \"random_forest_best_estimator.pkl\")","cd510691":"#rf_best = joblib.load(\"random_forest_best_estimator.pkl\")\nrf_best = rf_grid_search.best_estimator_\nrf_best.fit(training_pred, training_labels)\nforest_scores = cross_val_score(rf_best, training_pred, \n                                training_labels,\n                               scoring = \"accuracy\")\ndisplay_scores(forest_scores)","985dc3c3":"pd.DataFrame([{'Untuned_Rf':rf_scores.mean(),\n               'Tuned_Rf':forest_scores.mean()}], \n             index = [\"Average Scores\"])","8378229c":"# Transform test data on fitted values of training data.\ntest_features = pipeline.transform(strat_test_set.drop(\"Potability\", axis = 1))\ntest_labels = strat_test_set.loc[:,\"Potability\"]\n\nmodel_list = {\"DecisionTree\":dt,\n              \"Log Reg\":logistic_model,\n             \"RF\": rf_untuned,\n             \"Tuned_RF\": rf_best}\n\n# Function to create ROC plot of multiple models.\ndef plot_multi_roc(model, features, labels):\n    \n    from sklearn.metrics import roc_curve, roc_auc_score\n    from matplotlib import pyplot as plt\n    \n    if type(model) != dict:\n        raise NameError(\"Not Valid Dict\")\n        \n    for key in model_list:\n        model = model_list[key]\n        predictions = model.predict_proba(features)[:,1]\n        auc = roc_auc_score(labels, predictions)\n        fig_label = \"%s AUC=%.3f\" % (key, auc)\n        fpr, tpr, threshld = roc_curve(labels, predictions)\n        plt.plot(fpr, tpr, label = fig_label)\n    \n    plt.plot([0,1],[0,1], linestyle = '--')\n    plt.legend()\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    \nplot_multi_roc(model_list, test_features, test_labels)","9343b444":"def draw_confusion_matrix(model, features, labels, threshold = 0.5):\n    \n    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, precision_score, recall_score\n    \n    predicted_prob = model.predict_proba(features)\n    predicted_prob = pd.DataFrame(predicted_prob)\n    predicted_prob[\"model_classification\"] = np.where(predicted_prob.loc[:,1] > threshold, 1, 0)\n    cm = confusion_matrix(labels, predicted_prob[\"model_classification\"])\n    disp = ConfusionMatrixDisplay(confusion_matrix = cm,\n                             display_labels = model.classes_)\n    \n    tn, fp, fn, tp = confusion_matrix(labels, \n                                  predicted_prob[\"model_classification\"]).ravel()\n    disp.plot()\n    print(\"True Negative: %s\\nFalse Positives: %s\\nFalse Negative: %s\\nTrue Positives: %s\" %(tn, fp, fn, tp))\n    print(\"Sensitivity: %.4f\\nSpecificity: %.4f\\nPrecision: %.4f\" %((tp\/(tp+fn)),(tn\/(tn+fp)),(tp\/(tp+fp))))\n    print(\"F1 Score: %.4f \" %f1_score(labels, predicted_prob[\"model_classification\"]))\n\ndraw_confusion_matrix(rf_best, test_features, test_labels, threshold = 0.5)","6be36d34":"## Logistic Regression","e5b4b6f0":"## Transformation pipeline\n\n1. Multiple missing value imputation.\n2. Standard scaling of data.\n\nTransformation pipeline is created to standardise data transformations for all data being used to train the models. Specifically, the data will be fitted according to training data set values. Fitted information will be used to transform testing data set in order to compared performace between models.","429a448c":"#### Second Grid Search","74d67e3a":"# Data Prep for ML\n## Splitting datasets into train and test set\n\n* Stratify the dataset to split into training and testing set.\n* Training dataset is up-sampled to address the imbalance in the potability dataset. This should address potential bias in the model as a result of imbalanced training dataset.","55f78685":"Hyperparameter tuning of Random forest classifiers resulted in marginal gains in performance when compared to default hyperparameters. AUC values of 0.654 and 0.658 were reported for default Random Forest Model and tuned Random Forest Model respectively. Perhaps a different classifier model may be better suited for this task. Any feedback on how I could better tune the Random Forest model would be appreciated.","2c929235":"Training was initially stratified then upsampled to fix unbalanced Potability data entries. Test data set was not upsampled to preserve data quality and was stratified as according to original data set. A 60\/40 percent split for non-potable vs potable entries exists in the data. ","0c9b92d9":"Randomforest hyperparameter tuning resulted in a small increase in accuracy by 1.2% through K-fold cross validation of training dataset. A final reported accuracy of 85.7% is evident for the best performing tuned Randomforest model. ","2f0b13d5":"# Model Validation (ROC & Confusion Matrix) - Test Set\n\nModel validation is validated on stratified test data that has been transformed on the fitted values of training data. This results in the multiple imputation of missing values in the test set with the means of each category from the training data. It will also result in the scaling of test set to the fitted values evaluated in the training data. Model performance is evaluated by plotting the Receiver Operating Characteristic (ROC) curves and confusion matrix of the model predictions.\n","ac5c58f6":"# Introduction\n\nThe access to safe drinking water is an important aspect of health, sanitation and hygiene. According to the World Health Organisation, some 2.2 billion people around the world do not have access to safe drinking water<sup>1<\/sup>. The investment in water supply and sanitation is important for public heath in preventing the spread of diseases and can yield a net economic benefit. Therefore, the development of technologies that can effectively identify safe drinking water will facilitate the deployment of solutions that will alleviate water shortages. This project outlined here will use Machine Learning Algorithms to predict the water potability of water bodies based on labelled data from surveryed water quality metrics.   \n\n<sup>1<\/sup> <sub>https:\/\/www.who.int\/news\/item\/18-06-2019-1-in-3-people-globally-do-not-have-access-to-safe-drinking-water-unicef-who<\/sub>\n\n# Dataset\n\nThe [`water_potability.csv`] dataset contains 10 different water quality metrics for 3276 different water bodies expressed as floating point values. More details on the dataset can be found on [Kaggle](https:\/\/www.kaggle.com\/adityakadiwal\/water-potability). \n\n#### 1. pH value:\n\nPH is an important parameter in evaluating the acid\u2013base balance of water. It is also the indicator of acidic or alkaline condition of water status.  WHO has recommended maximum permissible limit of pH from 6.5 to 8.5. The current investigation ranges were 6.52\u20136.83 which are in the range of WHO standards.\n\n#### 2. Hardness:\n\nHardness is mainly caused by calcium and magnesium salts. These salts are dissolved from geologic deposits through which water travels. The length of time water is in contact with hardness producing material helps determine how much hardness there is in raw water. Hardness was originally defined as the capacity of water to precipitate soap caused by Calcium and Magnesium.\n\n#### 3. Solids (Total dissolved solids - TDS):\n\nWater has the ability to dissolve a wide range of inorganic and some organic minerals or salts such as potassium, calcium, sodium, bicarbonates, chlorides, magnesium, sulfates etc. These minerals produced un-wanted taste and diluted color in appearance of water. This is the important parameter for the use of water. The water with high TDS value indicates that water is highly mineralized. Desirable limit for TDS is 500 mg\/l and maximum limit is 1000 mg\/l which prescribed for drinking purpose.\n\n#### 4. Chloramines:\n\nChlorine and chloramine are the major disinfectants used in public water systems. Chloramines are most commonly formed when ammonia is added to chlorine to treat drinking water. Chlorine levels up to 4 milligrams per liter (mg\/L or 4 parts per million (ppm)) are considered safe in drinking water.\n\n#### 5. Sulfate:\n\nSulfates are naturally occurring substances that are found in minerals, soil, and rocks. They are present in ambient air, groundwater, plants, and food. The principal commercial use of sulfate is in the chemical industry. Sulfate concentration in seawater is about 2,700 milligrams per liter (mg\/L). It ranges from 3 to 30 mg\/L in most freshwater supplies, although much higher concentrations (1000 mg\/L) are found in some geographic locations.\n\n#### 6. Conductivity:\n\nPure water is not a good conductor of electric current rather\u2019s a good insulator. Increase in ions concentration enhances the electrical conductivity of water. Generally, the amount of dissolved solids in water determines the electrical conductivity. Electrical conductivity (EC) actually measures the ionic process of a solution that enables it to transmit current. According to WHO standards, EC value should not exceeded 400 \u03bcS\/cm.\n\n#### 7. Organic_carbon:\n\nTotal Organic Carbon (TOC) in source waters comes from decaying natural organic matter (NOM) as well as synthetic sources. TOC is a measure of the total amount of carbon in organic compounds in pure water. According to US EPA < 2 mg\/L as TOC in treated \/ drinking water, and < 4 mg\/Lit in source water which is use for treatment.\n\n#### 8. Trihalomethanes:\n\nTHMs are chemicals which may be found in water treated with chlorine. The concentration of THMs in drinking water varies according to the level of organic material in the water, the amount of chlorine required to treat the water, and the temperature of the water that is being treated. THM levels up to 80 ppm is considered safe in drinking water.\n\n#### 9. Turbidity:\n\nThe turbidity of water depends on the quantity of solid matter present in the suspended state. It is a measure of light emitting properties of water and the test is used to indicate the quality of waste discharge with respect to colloidal matter. The mean turbidity value obtained for Wondo Genet Campus (0.98 NTU) is lower than the WHO recommended value of 5.00 NTU.\n\n#### 10. Potability:\n\nIndicates if water is safe for human consumption where 1 means Potable and 0 means Not potable.\n\n# Task\n\nThe task is to create a machine learning model to determine if the sampled water body is fit for human consumption.\n\n# Strategy\n\n1. Exploratory Data Analysis (EDA)\n\n2. Data Preparation for Machine Learning.\n    * Data stratification into training and test set.\n    * Upsampling of training set.\n    * Building transformation pipeline - dealing with missing values using multiple mean imputation and standard scaling of dataset. \n\n3. Model Building\n    * Logistic Regression  \n    * Decision Tree Classifier\n    * Random Forest Classifier\n\n4. Random Forest hyper parameter tuning\n    * Randomised Search Cross Validation\n    * Grid Search Cross Validation \n\n5. Model comparison\n    * ROC Curves and accuracy\n    ","a4805b29":"## Data distribution","1e3cfefc":"# Randomforest classifier","cbabae53":"# Decision Trees","a863844f":"# Conclusions\n\nThe tuned Random Forest algorithm was the best performing model used to predict water potability. An AUC value of 0.658, sensitivity of 27.3%, precision of 73.7%, specificity of 93.8% and an F1 score of 0.39 was reported. In the context of water potability, false positives (predicting water is potable when it is not) will have greater consequence than false negatives (predicting water is not-potable when it is). Therefore, precision scores will have greater weight than sensitivity. Nonetheless, the model falsely predicts a significant proportion (73%) of drinkable water as not potable and will require further tuning. Any feedback on this work will be appreciated! ","47e07020":"A second grid search is required as `n_estimators` for the best estimator lies on the bottom range of the defined grid search value.","16fe47cb":"# Model Building","2e566c51":"## Random forest hyperparameter tuning by Grid Search\n\nRandomised search resulted in parameters where:\n\n* 'n_estimators': 1800,\n* 'min_samples_split': 5,\n* 'min_samples_leaf': 1,\n* 'max_features': 'sqrt',\n* 'max_depth': 70,\n* 'criterion': 'gini',\n* 'bootstrap': False\n \nBest parameters from randomised search will feed a more intensive GridSearch for the best performing model ranging at the values defined in the randomised searched parameters.","6cda3498":"Data in the solids are right skewed while sulfate are slightly left skewed. Overall, the data is still relatively normally distributed. The histogram of potability indicated an unbalanced dataset which may bias the model during training.","31a8903d":"#### First Grid Search","11c12ae4":"Counts are different indicating there are 491, 781, 162 NA's in `ph`, `Sulfate` and `Trihalomethanes` respectively.","a61fa8cd":"## Random forest hyperparameter tuning by Randomised Search\n\nThe following lines of code were adapted from https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74","ea630b52":"# Data Exploration\n\nMost columns for each data type is expressed as `float64` value with exceptions of \"Potability\" which is expressed as `int64`. As evident from the count values, pH, Sulfates and Trihalomethanes contain `NAN` values which were imputed with the mean of each column's values. ","01df5851":"#### Cross validation of tuned random forest model"}}