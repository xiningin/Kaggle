{"cell_type":{"34eba53e":"code","d154a330":"code","bd0c962e":"code","f8b8706a":"code","b9603bc6":"code","4179f3ff":"code","68646ed7":"code","bc041eb8":"code","f76a8275":"code","d66a0f6e":"code","a1df7f1b":"code","d356329c":"code","46443357":"code","a90b6bf8":"code","7fada7c3":"code","82811880":"code","8fd4ce76":"code","5f925acf":"code","0099642f":"code","8bc6800e":"code","bd807331":"code","064c347c":"code","d66fb882":"code","090ca53f":"code","31686dcf":"code","5c71f488":"code","b3b3c963":"code","57edbc92":"code","a67e5464":"markdown","6c5a8a8c":"markdown","2d3f3025":"markdown","ffb68898":"markdown","c62e14ec":"markdown","fe578c5f":"markdown","b1c2f405":"markdown","4f4c40bf":"markdown","46f6130a":"markdown","6120288b":"markdown","9054bf93":"markdown","dc79dc14":"markdown","791bfa31":"markdown","a2ed3589":"markdown","1d109bb5":"markdown","f482aaf7":"markdown","c708745e":"markdown","06aabd65":"markdown","76f4ce94":"markdown","3317daa3":"markdown","30087b5d":"markdown","e8849df0":"markdown","75dbdddb":"markdown","b62a2641":"markdown","fd405aff":"markdown"},"source":{"34eba53e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn as sk\nimport cufflinks as cf\nimport plotly.offline\nimport pandas_profiling\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix,accuracy_score,roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport cufflinks as cf\nimport warnings\n\n\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\nwarnings.filterwarnings('ignore')","d154a330":"AdultDf = pd.read_csv('..\/input\/adult-census-income\/adult.csv')\nAdultDf.head()","bd0c962e":"profile = pandas_profiling.ProfileReport(AdultDf, title='Pandas Profiling Report for training dataset', html={'style':{'full_width':True}})","f8b8706a":"profile.to_notebook_iframe()","b9603bc6":"AdultDf = AdultDf.replace('?', np.nan)\nAdultDf = AdultDf.dropna()\n\nAdultDf = AdultDf[AdultDf.age != 90]\nAdultDf = AdultDf[AdultDf['capital.gain'] != 99999]\n\nAdultDf.head()","4179f3ff":"IncomeDist = pd.crosstab(AdultDf.age, AdultDf.income)\nIncomeDist.iplot(kind ='bar', barmode = 'stack', xTitle = 'Age', yTitle = 'Num of Individuals', title = 'Distribution of Income between Ages', theme = 'white')","68646ed7":"AdultDf.iplot(kind ='histogram', column ='education.num', color ='orange', xTitle = 'Years Spent in Education', yTitle = 'Num of Individuals', title = 'Distribution of Income Levels', theme = 'white', bargap = 0.05)","bc041eb8":"EduIncome = pd.crosstab(AdultDf.education, AdultDf.income)\nEducationLevels = {\"Preschool\":0, \"1st-4th\":1, \"5th-6th\":2, \"7th-8th\":3, \"9th\":4, \"10th\":5, \"11th\":6, \"12th\":7, \"HS-grad\":8, \"Some-college\":9, \"Assoc-voc\":10, \"Assoc-acdm\":11, \"Bachelors\":12, \"Masters\":13, \"Prof-school\":14, \"Doctorate\":15}\nEduIncome = EduIncome.sort_values(by=['education'], key=lambda x: x.map(EducationLevels))\nEduIncome.iplot(kind = 'bar', barmode = 'stack', xTitle = 'Education Levels', yTitle = 'Num of Individuals', title = 'Distibution of Education Levels', theme = 'white')","f76a8275":"IncomeDist = pd.crosstab(AdultDf['relationship'], AdultDf.income)\nIncomeDist.iplot(kind ='bar', barmode = 'stack', xTitle = 'Relationship Status', yTitle = 'Num of Individuals', title = 'Distribution of Income between Relationship Status', theme = 'white')","d66a0f6e":"MarAge = pd.crosstab(AdultDf.age, AdultDf.relationship)\nMarAge.iplot(kind = 'bar', barmode = 'stack', xTitle = 'Age', yTitle = 'Num of Individuals', title = 'Distribution of Relationships between Ages', theme = 'white')","a1df7f1b":"SexIncome = pd.crosstab(AdultDf.sex, AdultDf.income)\nSexIncome.iplot(kind = 'bar', barmode = 'stack', xTitle = 'Sex', yTitle = 'Num of Individuals', title = 'Distribution of Income between Sex', theme = 'white')","d356329c":"OccuInc = pd.crosstab(AdultDf.occupation , AdultDf.income)\nOccuInc = OccuInc.sort_values('<=50K', ascending = False)\nOccuInc.iplot(kind = 'bar', barmode = 'stack', theme ='white', xTitle = 'Occupation', yTitle = 'Num of Individuals', title = 'Distribution of Income between Occupations')","46443357":"AdultDfTarget = AdultDf.copy()\ntarget = AdultDf['income'].unique()\nInts = {name: n for n, name in enumerate(target)}\nAdultDf['target'] = AdultDf['income'].replace(Ints)\nAdultDf.head()","a90b6bf8":"HeadDrop = ['capital.gain','capital.loss','education','fnlwgt','income']\nAdultDf.drop(HeadDrop, inplace = True, axis = 1)\nAdultDf.head()","7fada7c3":"AdultDf['native.country'] = AdultDf['native.country'].replace(['Mexico', 'Greece', 'Vietnam', 'China', 'Taiwan',\n       'India', 'Philippines', 'Trinadad&Tobago', 'Canada', 'South',\n       'Holand-Netherlands', 'Puerto-Rico', 'Poland', 'Iran', 'England',\n       'Germany', 'Italy', 'Japan', 'Hong', 'Honduras', 'Cuba', 'Ireland',\n       'Cambodia', 'Peru', 'Nicaragua', 'Dominican-Republic', 'Haiti',\n       'Hungary', 'Columbia', 'Guatemala', 'El-Salvador', 'Jamaica',\n       'Ecuador', 'France', 'Yugoslavia', 'Portugal', 'Laos', 'Thailand',\n       'Outlying-US(Guam-USVI-etc)', 'Scotland'], 'NonUS')\n\nAdultDf['native.country'].unique()","82811880":"categorical_columns = AdultDf.select_dtypes(exclude=np.number).columns\nBinarisedDf = pd.get_dummies(data=AdultDf, prefix=categorical_columns, drop_first=True)\n\nAdultDf = BinarisedDf\nAdultDf.head()","8fd4ce76":"X = AdultDf.drop('target', axis=1)\ny = AdultDf['target']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\nDtClas = DecisionTreeClassifier()\nDtClas.fit(X_train, y_train)\n\nDtPred = DtClas.predict(X_test)\n\nprint(classification_report(y_test, DtPred))\nprint(\"Accuracy :\",accuracy_score(y_test, DtPred))\n","5f925acf":"LogReg = LogisticRegression(C = 0.05, max_iter = 1000)\nLrMod = LogReg.fit(X_train, y_train)\nLrPred = LrMod.predict(X_test)\n\nprint(classification_report(y_test, LrPred))\nprint(\"Accuracy :\",accuracy_score(y_test, LrPred))","0099642f":"HypeParamLogReg = {'penalty':['l1', 'l2', 'elasticnet'], 'C':[0.001, 0.01, 0.1, 1, 10, 100], 'l1_ratio':[0.001, 0.01, 0.1]}\nLogRegGrid = GridSearchCV(LogisticRegression(), param_grid=HypeParamLogReg, verbose=3)\nLogRegGrid.fit(X, y)","8bc6800e":"print(\"Best Params : \",LogRegGrid.best_params_)\nprint(\"Accuracy : \",LogRegGrid.best_score_)","bd807331":"LogRegAdj = LogisticRegression(C = 0.1, l1_ratio = 0.001, penalty = 'l2', max_iter = 1000)\nLrModAdj = LogRegAdj.fit(X_train, y_train)\nLrPredAdj = LrModAdj.predict(X_test)\n\nprint(classification_report(y_test, LrPredAdj))\nprint(\"Accuracy :\",accuracy_score(y_test, LrPredAdj))","064c347c":"RandomForest = RandomForestClassifier(n_estimators=500,max_features=5)\nRandomForest.fit(X_train, y_train)\nRfPred = RandomForest.predict(X_test)\n\nprint(classification_report(y_test, RfPred))\nprint(\"Accuracy :\",accuracy_score(y_test, RfPred))","d66fb882":"HypeParamRf = {'criterion':['gini', 'entropy'], 'max_depth':[2, 5, 8, 11], 'n_estimators':[200, 300, 400, 500]}\nRfGrid = GridSearchCV(RandomForestClassifier(), param_grid=HypeParamRf, verbose=3)\n\nRfGrid.fit(X, y)","090ca53f":"print(\"Best Params : \",RfGrid.best_params_)\nprint(\"Accuracy : \",RfGrid.best_score_)","31686dcf":"GradBoost = XGBClassifier(learning_rate = 0.35, n_estimator = 200)\nGbMod = GradBoost.fit(X_train, y_train)\nGbPred = GbMod.predict(X_test)","5c71f488":"print(classification_report(y_test, GbPred))\nprint(\"Accuracy :\",accuracy_score(y_test, GbPred))","b3b3c963":"classifiers = [GradBoost,RandomForest,LogReg]\n\nfolds = StratifiedKFold(n_splits=3, shuffle=True, random_state=11)\n\nscores_dict = {}\n\nfor train_index, valid_index in folds.split(X_train, y_train):\n    X_train_fold, X_valid_fold = X.iloc[train_index], X.iloc[valid_index]\n    y_train_fold, y_valid_fold = y.iloc[train_index], y.iloc[valid_index]\n    \n    for classifier in classifiers:\n        name = classifier.__class__.__name__\n        classifier.fit(X_train_fold, y_train_fold)\n        training_predictions = classifier.predict_proba(X_valid_fold)\n        scores = roc_auc_score(y_valid_fold, training_predictions[:, 1])\n        if name in scores_dict:\n            scores_dict[name] += scores\n        else:\n            scores_dict[name] = scores\n\n\nfor classifier in scores_dict:\n    scores_dict[classifier] = scores_dict[classifier]\/folds.n_splits","57edbc92":"print(\"Accuracy :\",scores_dict)","a67e5464":"**Importing file and brief analysis of the datasets content**","6c5a8a8c":"# Relationship Status:\n\nThe relationship attribute was identified in the exploration as providing insight into the solution to the classification problem, further exploration of the attribute and its relationship between income confirmed this analysis. There is a clear increased proportion of higher income earners amongst those who are 'Married' either Husband or Wife, when compared to those who are unmarried, Own-Child, or Other-Relative. This may potentially be due to the increased age of those married when compared to those unmarried or classified as an 'Own-Child'.","2d3f3025":"the profiling report identified a few issues that needed to be addressed before continued exploration, these included removal of outliers and null values which used the sentinel value '?'.","ffb68898":"# Random Forests:\nAn initial 'baseline' Random Forest classifier was created to identify a rough estimate before conducting hyperparameter tuning","c62e14ec":"Due to the highly skewed nature of the 'native.country' with the disproportionate majority of individuals located in the United States, it was decided to collate all the Non-United States countries into a single value. This will assist with dimensionality reduction when the dataset is binarised down the line.","fe578c5f":"Binarising the dataset for use in model development and training.","b1c2f405":"# Age:\nThe distribution of incomes between ages shows a clear positive skew with regard to age, and an increase in the proportion of individuals earning over $50k per year between the ages of 30 and 55. This conclusion is not unsurprising as it follows the expected pattern of adult career progression with younger individuals likely to either studying or starting their careers. Those above 55 are likely to be starting to progress towards retirement, which is associated with lower expected levels of income due to reduced working hours.","4f4c40bf":"Hyperparameter Tuning utilising 'GridSearchCV'","46f6130a":"# Gender:\n\nA quick visualisation of the Sex attribute with respect to the proportion of income distributions between each shows the disparity between incomes between the genders. this clear distinction provides a useful attribute for assistance in the classification task.","6120288b":"# Occupation:\n\nOccupation another attribute that was identified as having potential to be a useful attribute for use within the classification task due to its correlation with the income attribute. There is a increase in the proportion of those earning over $50K with respect to the 'Prof-Speciality' and 'Exec-Managerial' categories when compared to the other categories within the attribute.","9054bf93":"The first step in this process is to convert the Income attribute the target attribute into 0 and 1 for implementation in model training.","dc79dc14":"# Education:\nThe removal of one of these attributed would be beneficial to analysis as it acts to reduce the overall dimensionality of the dataset. The initial exploration through Pandas Profiling utilising 'Cram\u00e9rs V' showed that the two attributes were highly correlated. Further analysis of these attributes and their relationship to income was conducted to confirm this conclusion.","791bfa31":"# Model Development and Training\nThe Model was split into a training and test set with the target column removed for the test set. An initial Decision Tree Classifier was implemented to show a 'baseline' for model training. Further hyperparameter tuning and development should aim to improve upon this initial classification methodology.","a2ed3589":"# Continued Exploration & Visulisations\n\nThis notebook uses cufflinks which integrates with Plotly, feel free to explore the visualisations with the inbuilt, interactive tools provided by the package.","1d109bb5":"# Logistic Regression:","f482aaf7":"**Hyperparamater tuning** of the Random Forest Classifer utilising 'GridSearchCV'","c708745e":"# Using folds to further improve accuracy","06aabd65":"A number of attributes were identified as redundant. This was either due to the attribute providing no meaningful insight into the classification task, not being understandable from the analyst\u2019s perspective, or already describing a feature that was also described by another attribute. Overall this should reduce the dimensionality of the dataset.","76f4ce94":"# Gradient Boosted Descent:","3317daa3":"# Exploritory Analysis\n\n**Pandas Profiling** was implemented to provide quick and efficient initial exploratory analysis of the dataset. The Profiling tool shows initial attribute distributions, identifies outliers and highlights correlations between data that can be further explored in the EDA process. \n\nPandas Profiling is a quite new and exciting package for EDA utilising Pandas, with consistent continued version releases it is a tool that is only going to improve. To find out more visit the [Pandas Profiling GitHub:](https:\/\/github.com\/pandas-profiling\/pandas-profiling)","30087b5d":"Adjusted Logistic Regression ","e8849df0":"'Baseline' Logistic Regression","75dbdddb":"Upon research for model implementation a hyperparameter training methodology was identified and implemented, the code is drawn from [Andrew Lukyanenko's](https:\/\/www.kaggle.com\/artgor\/bayesian-optimization-for-robots) implementation of this technique. the use of folds effectively combines a number of successful models together ultimately working together to produce a improved single classification model.","b62a2641":"# Feature Engineering and Model Development","fd405aff":"# **Predicting An Individual Incomes via Census Data**\n\nThis is my fist Kaggle Notebook, it is a further exploration on top of introductory studies to the fundamentals of analytics and machine learning models. The initial section will explain the data mining problem at hand and the process taken to provide a solution to this problem. The second section will provide an exploration of the attributes with in the dataset, identifying key attributes that show promise in being able to assist with the classification task. The final section will involve the creation, development & results of the classification methodologies used to solve the task."}}