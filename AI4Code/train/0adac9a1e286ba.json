{"cell_type":{"efa6e398":"code","618e7aab":"code","330e59fb":"code","5e2da45c":"code","8961dbee":"code","6212f730":"code","b59823a2":"code","23d64105":"code","3c47d172":"code","9d2da6eb":"code","d73e88fd":"code","415ff51b":"code","c016b78c":"code","8318303e":"code","5d24b057":"code","db2610f6":"code","f5745644":"code","ee47d0ac":"code","e25d1ee6":"code","2b5a1b5f":"code","dca7cbe1":"code","fa3c15d7":"code","c192361e":"code","58195be5":"code","8d1beafe":"code","c7dd02f8":"code","1f619b38":"code","d56074d6":"code","b9f0758c":"code","62de63fd":"markdown","23bdf32f":"markdown","b19f19ea":"markdown","96d5a8ad":"markdown","81e82bbb":"markdown","bf8cd170":"markdown","18766c9b":"markdown","1cf216ba":"markdown","51a73d7e":"markdown","ec49f733":"markdown","aac31468":"markdown","0f61d7e9":"markdown","958c5774":"markdown","c7a43913":"markdown","3964aff7":"markdown","ed5c499e":"markdown","33297b17":"markdown","0829fde5":"markdown","a5e0bd9e":"markdown","0c9bdec3":"markdown"},"source":{"efa6e398":"# import packages\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import ShuffleSplit, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nimport plotly.express as px\nfrom sklearn.decomposition import PCA\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.metrics import silhouette_samples, silhouette_score, homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score, fowlkes_mallows_score\nfrom scipy.cluster import hierarchy \nfrom scipy.spatial import distance_matrix \nfrom sklearn.cluster import DBSCAN \nimport matplotlib.cm as cm","618e7aab":"# read data\ndata = pd.read_csv(\"..\/input\/absenteeism-at-work-uci-ml-repositiory\/Absenteeism_at_work.csv\",\n                        sep=';', index_col=\"ID\")\ndata.head()","330e59fb":"# data types\ndata.info()","5e2da45c":"# min and max values\ndata.describe()","8961dbee":"# divide columns to numerical and categorical ones\ncategorical_atts = ['Reason for absence','Month of absence','Day of the week',\n                     'Seasons','Disciplinary failure', 'Education', 'Social drinker',\n                     'Social smoker', 'Pet', 'Son']\nnumerical_atts = data.drop(columns=categorical_atts).columns.to_list()\n\n# update types of categorical columns \n#for cat in categorical_atts:\n#    data[cat] = data[cat].astype('category')","6212f730":"# number of unique values in categorical atts\ndata[categorical_atts].nunique()","b59823a2":"# plot distribution of \"reason for absence\" column\nsns.set_style(\"whitegrid\")\nsns.catplot(data=data, x='Reason for absence', kind= 'count',size=4, aspect=4, palette='muted')","23d64105":"# plot distribution of numerical attributes\nnum_df = data[numerical_atts]\nplt.figure(figsize=(20,20))\nfor i in range(1, 11):\n    plt.subplot(4, 3, i)\n    sns.distplot(num_df[num_df.columns[i-1]],bins=14)","3c47d172":"# correlation matrix\nnum_corr = num_df.corr()\nplt.figure(figsize=(10,6))\nsns.heatmap(num_corr, annot=True, fmt=\".3f\",vmin=-1, vmax=1, linewidths=.5, cmap = sns.color_palette(\"RdBu\", 100))\nplt.yticks(rotation=0)\nplt.show()","9d2da6eb":"# display boxlots\nplt.figure(figsize=(8,8))\nsns.boxplot(x=\"variable\", y=\"value\", data=pd.melt(num_df))\nplt.xticks(rotation=90)\nplt.show()","d73e88fd":"# Check for outliers using boxplots and drop them\nfor num_att in numerical_atts:\n    # Getting 75 and 25 percentile of variable \"i\"\n    Q3, Q1 = np.percentile(data[num_att], [75,25])\n    MEAN = data[num_att].mean()\n    \n    # Calculating Interquartile range\n    IQR = Q3 - Q1\n    \n    # Calculating upper extream and lower extream\n    minimum = Q1 - (IQR*1.5)\n    maximum = Q3 + (IQR*1.5)\n    \n    # Replacing all the outliers value to Mean\n    data_clean=data.drop(data.loc[data[num_att]< minimum,num_att].index) \n    data_clean=data.drop(data.loc[data[num_att]> maximum,num_att].index) ","415ff51b":"# size reduced to 150 from 740\nlen(data_clean)","c016b78c":"# transform data\nnumeric_transformer = MinMaxScaler()\n#categorical_transformer = OneHotEncoder(sparse=False, handle_unknown='ignore')\n\n# exclude target attr,bute\nnumerical_atts = [x for x in numerical_atts if x not in ['Absenteeism time in hours']]\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numerical_atts),\n        ('cat', 'passthrough', categorical_atts)\n    ], remainder='passthrough')\ndata_pp = preprocessor.fit_transform(data_clean)","8318303e":"# X, y splitting (we dont need train test splitting since this model is just for feature importances)\ny_original = data_pp[:,-1].reshape(-1, 1)\ny = np.round(MinMaxScaler((0,2)).fit_transform(y_original)).ravel()\nX = np.delete(data_pp,-1,1) \n\n# cross-validation with 10 splits\ncross_val = ShuffleSplit(n_splits=10, random_state = 42)\n\n# define model\nrf = RandomForestClassifier(random_state = 0,max_features=None,n_jobs=-1)\n\n# parameters \nparameters = {  \n                \"n_estimators\":[1000],\n                #'max_depth': list(range(1,11)),\n                #\"criterion\": [\"gini\",\"entropy\"],\n                #\"criterion\": [\"mse\",\"mae\"],\n                #\"class_weight\": [None, \"balanced\"],\n                #\"max_features\":[\"auto\", None, \"log2\"],\n                }\n\n# grid search for parameters\ngrid = GridSearchCV(estimator=rf, param_grid=parameters, cv=cross_val, n_jobs=-1)#multithreading; all cores are used\ngrid.fit(X,y)\n\n# print best scores\nprint(\"The best parameters are %s with a score of %0.4f\"\n      % (grid.best_params_, grid.best_score_))","5d24b057":"# pie-chart \natts = numerical_atts+categorical_atts\nplt.figure(figsize=(10,10))\nplt.pie(grid.best_estimator_.feature_importances_, labels=atts, shadow=True, startangle=90) ","db2610f6":"# permutation importance with RF classifier\natts_arr = np.array(atts)\nresult = permutation_importance(grid, X, y, n_repeats=10,\n                                random_state=42, n_jobs=-1)\nsorted_idx = result.importances_mean.argsort()\n\nfig, ax = plt.subplots()\nax.boxplot(result.importances[sorted_idx].T,\n           vert=False, labels=atts_arr[sorted_idx])\nax.set_title(\"Permutation Importances (train set)\")\nfig.tight_layout()\nplt.show()\n\nfor i in result.importances_mean.argsort()[::-1]:\n     print(f\"{atts_arr[i]:<8} \"\n           f\"{result.importances_mean[i]:.3f}\"\n           f\" +\/- {result.importances_std[i]:.3f}\")","f5745644":"# select important attributes\nselected_atts = ['Reason for absence', 'Day of the week', 'Work load Average\/day','Body mass index','Hit target', \n                     'Seasons', 'Month of absence', 'Transportation expense', 'Age']\n\nselected_atts_idx=np.where(np.isin(atts,selected_atts))[0]\nX_selected = X[:,selected_atts_idx]","ee47d0ac":"# run k-means for range of 10 clusters then analyse with elbow method\nclusters = []\n\nfor i in range(1, 11):\n    km = KMeans(n_clusters=i).fit(X_selected)\n    clusters.append(km.inertia_)\n    \nfig, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(x=list(range(1, 11)), y=clusters, ax=ax)\nax.set_title('Searching for Elbow')\nax.set_xlabel('Clusters')\nax.set_ylabel('Inertia')\n\n# Annotate arrow\nax.annotate('Possible Elbow Point', xy=(4, clusters[3]), xytext=(4, clusters[3]+1000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nax.annotate('Possible Elbow Point', xy=(3,clusters[2] ), xytext=(3, clusters[2]+1000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nax.annotate('Possible Elbow Point', xy=(2,clusters[1] ), xytext=(2, clusters[1]+1000), xycoords='data',          \n             arrowprops=dict(arrowstyle='->', connectionstyle='arc3', color='blue', lw=2))\n\nplt.show()","e25d1ee6":"# 2 clusters\nkm2 = KMeans(n_clusters=2).fit(X_selected)\n\n# plot using tsne \nX_embedded  = TSNE(n_components=3, perplexity=10, random_state=24).fit_transform( X_selected )\ncolors = np.array([x for x in 'bgrcmykbgrcmykbgrcmykbgrcmyk'])\n\nfig = px.scatter_3d(x=X_embedded [:,0],y=X_embedded [:,1],z=X_embedded [:,2],color=colors[km2.labels_])\nfig.show()","2b5a1b5f":"# 3 clusters\nkm3 = KMeans(n_clusters=3).fit(X_selected)\n\n# plot using tsne \nX_embedded  = TSNE(n_components=3, perplexity=10,random_state=24).fit_transform( X_selected )\n\nfig = px.scatter_3d(x=X_embedded [:,0],y=X_embedded [:,1],z=X_embedded [:,2],color=colors[km3.labels_])\nfig.show()","dca7cbe1":"# 4 clusters\nkm4 = KMeans(n_clusters=4).fit(X_selected)\n\n# plot using tsne \nX_embedded  = TSNE(n_components=3, perplexity=10,random_state=24).fit_transform( X_selected )\n\nfig = px.scatter_3d(x=X_embedded [:,0],y=X_embedded [:,1],z=X_embedded [:,2],color=colors[km4.labels_])\nfig.show()","fa3c15d7":"# PCA for reducing dimensions to 3\npca = PCA(n_components=3)\npca_results = pca.fit_transform(X_selected)\n\n# running k-means on resuts of pca\nkm_pca = KMeans(n_clusters=3).fit(pca_results)\nfig = px.scatter_3d(x=pca_results [:,0],y=pca_results [:,1],z=pca_results [:,2],color=colors[km_pca.labels_])\nfig.show()","c192361e":"# define a function to plot silhouette values of clusters\ndef silhouette_plot(X, y, n_clusters, ax=None):\n    if ax is None:\n        ax = plt.gca()\n\n    # Compute the silhouette scores for each sample\n    silhouette_avg = silhouette_score(X, y)\n    sample_silhouette_values = silhouette_samples(X, y)\n\n    y_lower = padding = 2\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        ith_cluster_silhouette_values = sample_silhouette_values[y == i]\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ n_clusters)\n        ax.fill_betweenx(np.arange(y_lower, y_upper),\n                         0,\n                         ith_cluster_silhouette_values,\n                         facecolor=color,\n                         edgecolor=color,\n                         alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i + 1))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + padding\n\n    ax.set_xlabel(\"The silhouette coefficient values\")\n    ax.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhoutte score of all the values\n    ax.axvline(x=silhouette_avg, c='r', alpha=0.8, lw=0.8, ls='-')\n    ax.annotate('Average',\n                xytext=(silhouette_avg, y_lower * 1.025),\n                xy=(0, 0),\n                ha='center',\n                alpha=0.8,\n                c='r')\n\n    ax.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax.set_xticks([0, 0.2, 0.4, 0.6, 0.8, 1])\n    ax.set_ylim(0, y_upper + 1)\n    ax.set_xlim(-0.075, 1.0)\n    return ax","58195be5":"# plot silhouette coefficients\nplt.figure(figsize=(20,30))\nfor i in range(2,11):  \n    agnes = AgglomerativeClustering(n_clusters=i, linkage='average')\n    agnes_labels = agnes.fit_predict(X_selected)\n    plt.subplot(5,2,i-1)\n    silhouette_plot(X_selected,agnes_labels,i)\n","8d1beafe":"# 6 clusters\nagnes6 = AgglomerativeClustering(n_clusters=6, linkage='average').fit(X_selected)\n\n# plot using tsne \nX_embedded  = TSNE(n_components=3, perplexity=10,random_state=24).fit_transform( X_selected )\n\nfig = px.scatter_3d(x=X_embedded [:,0],y=X_embedded [:,1],z=X_embedded [:,2],color=colors[agnes6.labels_])\nfig.show()","c7dd02f8":"# there's no inertia in attribute of AgglomerativeClustering class so we use scipy's distance matrix\ndist = distance_matrix(X_selected, X_selected)\n\n# dendrogram\nZ = hierarchy.linkage(dist, 'average')\nplt.figure(figsize=(18, 15))\ndendro = hierarchy.dendrogram(Z, leaf_rotation=0, leaf_font_size=12, orientation='right')","1f619b38":"# search for best parameters by using silhouette_score\nscore_list=[]\nfor eps in np.arange(0.5,20,0.5):\n    for min_samples in range(3,20):\n        db = DBSCAN(eps=eps, min_samples=min_samples).fit(X_selected)\n        labels = db.labels_\n        n = len(np.unique(labels))\n        if n>1:\n            score=silhouette_score(X_selected, labels)\n            score_list.append((score,(eps,min_samples)))\n     \nbiggest_score = sorted(score_list)[-1]  \nbest_eps, best_min_spamles = biggest_score[1]\nbest_eps, best_min_spamles","d56074d6":"# best model for DBSCAN\ndb_best = DBSCAN(eps=best_eps, min_samples=best_min_spamles).fit(X_selected)\n\n# plot using tsne \nX_embedded  = TSNE(n_components=3, perplexity=10,random_state=24).fit_transform( X_selected )\n\nfig = px.scatter_3d(x=X_embedded [:,0],y=X_embedded [:,1],z=X_embedded [:,2],color=colors[db_best.labels_])\nfig.show()","b9f0758c":"# print metric for chosen models\nmodels = [km4, km_pca, agnes6, db_best] \nnames = [\"K-MEANS:\",\"PCA + K-MEANS:\",\"AGNES:\", \"DBSCAN:\"]\n\nfor i, model in enumerate(models):\n    labels = model.labels_\n    n = len(np.unique(labels))\n    y = np.round(MinMaxScaler((0,n)).fit_transform(y_original)).ravel()\n    # Number of clusters in labels, ignoring noise if present.\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n    print(names[i])\n    print('Estimated number of clusters: %d' % n_clusters_)\n    print('Estimated number of noise points: %d' % n_noise_)\n    print(\"Homogeneity: %0.3f\" % homogeneity_score(y, labels))\n    print(\"Completeness: %0.3f\" % completeness_score(y, labels))\n    print(\"V-measure: %0.3f\" % v_measure_score(y, labels))\n    print(\"Adjusted Rand Index: %0.3f\"\n          % adjusted_rand_score(y, labels))\n    print(\"Adjusted Mutual Information: %0.3f\"\n          % adjusted_mutual_info_score(y, labels))\n    print(\"Fowlkes-Mallows score: %0.3f\"\n          % fowlkes_mallows_score(y, labels))\n    print(\"Silhouette Coefficient: %0.3f\"\n          % silhouette_score(X, labels))\n    print(\"\\n\")","62de63fd":"**2. w Permutation importance:**\n\nsee https:\/\/scikit-learn.org\/stable\/modules\/permutation_importance.html#permutation-importance","23bdf32f":"**2. AGNES:**\n\nThere's no inertia (Sum of squared distances of samples to their closest cluster center) attribute of AgglomerativeClustering class so we used silhouette coefficient (best:1, worst:-1) to select cluster number of AGNES.\n\nsee https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.silhouette_score.html#sklearn.metrics.silhouette_score \n\nfor AGNES see https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.AgglomerativeClustering.html#sklearn.cluster.AgglomerativeClustering","b19f19ea":"Only metric doesn't reqire ground-truth labels is Silhouette Coefficient. For it, DBSCAN takes ahead, then comes PCA+KMEANS and followed by K-MEANS and AGNES. Its results are correlated with Fowlkes-Mallows score altough they require different inputs with only difference of swapping AGNES and K-MEANS.\n\nBut from the perspective of Homogeneity and Completeness principles, AGNES performs best. Followings are PCA+KMEANS, K-MEANS and DBSCAN. The results are similar for Mutual Information.\n\nAnd finally for Rand Index AGNES performs best again. Followings are K-MEANS, PCA+KMEANS and DBSCAN. ","96d5a8ad":"# EVALUATION\nWe will compare best of 3 models by using 8 metrics:\n* Estimated number of clusters\n* Estimated number of noise points\n* Homogeneity: For perfect clustering, each cluster contains only members of a single class.\n* Completeness: For perfect clustering, all members of a given class are assigned to the same cluster.\n* V-measure: Harmonic mean of Homogeneity and Completeness.\n* Adjusted Rand Index: Given the knowledge of the ground truth class, the adjusted Rand index is a function that measures the similarity of the two assignments, ignoring permutations and with chance normalization:\n* Adjusted Mutual Information: Given the knowledge of the ground truth class, the Mutual Information is a function that measures the agreement of the two assignments, ignoring permutations.\n* Fowlkes-Mallows score: Geometric mean of precision and recall.\n* Silhouette Coefficient: If the ground truth labels are not known, the Silhouette Coefficient is calculated using the mean intra-cluster distance (a) and the mean nearest-cluster distance (b) for each sample by (b - a) \/ max(a, b)\n\nsee https:\/\/scikit-learn.org\/stable\/modules\/clustering.html#clustering-performance-evaluation","81e82bbb":"# OUTLIER DETECTION","bf8cd170":"# ANALYSING DATA","18766c9b":"**3. DBSCAN:**\n\neps: The maximum distance between two samples for one to be considered as in the neighborhood of the other (default: 0.5).\n\nmin_samples: The number of samples (or total weight) in a neighborhood for a point to be considered as a core point. This includes the point itself (default: 5).\n\nsee https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.DBSCAN.html","1cf216ba":"It seems like 4 clusters work best. Let's try the same process again with PCA + K-means.\n\nFor PCA see https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html","51a73d7e":"**1. w Random Forest importances:**\n\nTrain a RF model to see feature importances. It's ok, we are already have target attribute in the dataset.","ec49f733":"# FEATURE SELECTION","aac31468":"Let's plot the complete dendrogram to see the possible clusterings.","0f61d7e9":"Let's try possible elbow points by creating plots. We can use t-SNE for visualizing 9D data on 3D space.\n\nsee https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html","958c5774":"**1. K-means:**\n\n\"inertia_\" attribute provides sum of squared distances of samples to their closest cluster center. Wen can plot it and decide on the parameter \"n_clusters\" by using elbow method.\n\nsee https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.KMeans.html","c7a43913":"We choosed 9 attributes based on Permutaion importances w RF","3964aff7":"We lost most of valuable information about data with PCA so, resulting graph seems incomplete. Using k-means on original 7-dimensional data then plotting with t-sne gives better results.","ed5c499e":"# Absenteeism at Work - Clustering\nCSE4063 - Data Mining Project-2 part-2\n\n**Absenteeism at work Data Set**  \nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/Absenteeism+at+work","33297b17":"It seems like 3, 6 or 9 are best amongst them (cluster with euqal sizes and significant silhouette value). We choosed to work with n_clusters = 3.","0829fde5":"# NORMALIZATION","a5e0bd9e":"Half of the attributes seem close so it's hard to decide how to do elimination. Thus we tried another method called Permutaion importance.","0c9bdec3":"# CLUSTERING"}}