{"cell_type":{"7424c16a":"code","c0d74379":"code","ec41fb13":"code","4b9c07af":"code","423862e5":"code","eed45e1d":"code","ad6e049f":"code","f04c8405":"code","48c217c6":"code","8c5657e1":"code","266124a6":"code","ccbd334d":"code","cef4b2e6":"code","0701722f":"code","65ece4f3":"code","1b87a60b":"code","758b0b34":"code","5aa97fa1":"code","2771e69f":"code","64c7ee5d":"code","e5c137fd":"code","f6f73b30":"code","3acabc75":"code","f8f6b7c3":"code","720d6e29":"code","f4040fca":"code","a2f4f9cb":"code","9f119999":"code","1292c01d":"code","4066cf8d":"code","fbd7ac17":"code","06a0cbfa":"markdown","8af67231":"markdown","504b8d07":"markdown"},"source":{"7424c16a":"#From the discussion board (https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/291823)\n\n#!pip install scikit-learn-intelex\n#from sklearnex import patch_sklearn\n#patch_sklearn()","c0d74379":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n\n#from functools import partial\n#import optuna\n#import warnings\n#warnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n        \nimport gc\nfrom scipy import stats","ec41fb13":"df_train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv',index_col='Id')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv',index_col =\"Id\")","4b9c07af":"#Use this notebook to make my pseudolabels file https:\/\/www.kaggle.com\/remekkinas\/tps-12-pseudolabels-for-classification-tutorial\/notebook\n\npseudo_df = pd.read_csv('..\/input\/tbsdexxgbclassifierprediction\/tps12-pseudolabels2 (1).csv',index_col =\"Id\")\n\nnew_df_train = pd.concat([df_train,pseudo_df],axis =0)\nnew_df_train.reset_index(drop=True)","423862e5":"del df_train,pseudo_df","eed45e1d":"# from the discussion board (https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/291844)\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","ad6e049f":"#Soil_Type7 and SoilType15 has only zero values. Need to delete those two columns.\n\nnew_df_train = new_df_train.drop(['Soil_Type7','Soil_Type15'],axis=1)\ndf_test = df_test.drop(['Soil_Type7','Soil_Type15'],axis=1)\n","f04c8405":"# Cover_Type 5 was only one sample in this data.  \nnew_df_train = new_df_train[new_df_train.Cover_Type != 5]","48c217c6":"targets = new_df_train.Cover_Type\nfeatures = new_df_train.drop(['Cover_Type'],axis=1)\n\n","8c5657e1":"del new_df_train","266124a6":"#Extra feature engineering from the discussion board https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293373\n\nfeatures[\"Aspect\"][features[\"Aspect\"] <0] +=360\nfeatures[\"Aspect\"][features[\"Aspect\"] >359]-=360\n\ndf_test[\"Aspect\"][df_test[\"Aspect\"] <0] +=360\ndf_test[\"Aspect\"][df_test[\"Aspect\"] >359] -=360\n\n\nfeatures.loc[features[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ndf_test.loc[df_test[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\nfeatures.loc[features[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ndf_test.loc[df_test[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\nfeatures.loc[features[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ndf_test.loc[df_test[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\nfeatures.loc[features[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ndf_test.loc[df_test[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\nfeatures.loc[features[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ndf_test.loc[df_test[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\nfeatures.loc[features[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ndf_test.loc[df_test[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n\n","ccbd334d":"feature_list = features.columns\n#extra feature engineering from the discussion board. \n#sum of soil_type and wilderness_ares https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/292823\n\nsoil_features = [x for x in feature_list if x.startswith(\"Soil_Type\")]\nfeatures['soil_type_count'] = features[soil_features].sum(axis=1)\ndf_test['soil_type_count'] =df_test[soil_features].sum(axis=1)\n\nwilderness_features= [x for x in feature_list if x.startswith('Wilderness')]\nfeatures['wilderness_area_count']=features[wilderness_features].sum(axis=1)\ndf_test['wilderness_area_count'] = df_test[wilderness_features].sum(axis=1)","cef4b2e6":"features[soil_features]= features[soil_features].astype('bool')\n\nfeatures[wilderness_features] = features[wilderness_features].astype('bool')","0701722f":"#some more features engineering\n# from this discussion https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293612\n\n\nfeatures['Euclidean_Distance_to_Hydrology'] =  ((features['Horizontal_Distance_To_Hydrology']).astype(np.int32)**2 + (features['Vertical_Distance_To_Hydrology']).astype(np.int32)**2)**0.5\n\n\nfeatures['Manhattan_Distance_to_Hydrology'] = np.abs(features['Horizontal_Distance_To_Hydrology']) + np.abs(features['Vertical_Distance_To_Hydrology'])\n\n\ndf_test['Euclidean_Distance_to_Hydrology'] =  ((df_test['Horizontal_Distance_To_Hydrology']).astype(np.int32)**2 + (df_test['Vertical_Distance_To_Hydrology']).astype(np.int32)**2)**0.5\n\ndf_test['Manhattan_Distance_to_Hydrology'] = np.abs(df_test['Horizontal_Distance_To_Hydrology']) + np.abs(df_test['Vertical_Distance_To_Hydrology'])","65ece4f3":"features[\"9am*noon\"] = (features[\"Hillshade_9am\"] *features[\"Hillshade_Noon\"])\nfeatures[\"9am*3pm\"] = features[\"Hillshade_9am\"] *features[\"Hillshade_3pm\"]\nfeatures[\"3pm*noon\"] = features[\"Hillshade_3pm\"] *features[\"Hillshade_Noon\"]\n\n\ndf_test[\"9am*noon\"] = df_test[\"Hillshade_9am\"] *df_test[\"Hillshade_Noon\"]\ndf_test[\"9am*3pm\"] = df_test[\"Hillshade_9am\"] *df_test[\"Hillshade_3pm\"]\ndf_test[\"3pm*noon\"] = df_test[\"Hillshade_3pm\"] *df_test[\"Hillshade_Noon\"]","1b87a60b":"#referred to https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293612\n\nfeatures[\"EVDtH\"] = features.Elevation-features.Vertical_Distance_To_Hydrology\ndf_test[\"EVDtH\"] = df_test.Elevation-df_test.Vertical_Distance_To_Hydrology\n\n\nfeatures[\"EHDtH\"] = features.Elevation - (features.Horizontal_Distance_To_Hydrology) * 0.2\ndf_test[\"EHDtH\"] = df_test.Elevation - (df_test.Horizontal_Distance_To_Hydrology) * 0.2\n\n\n","758b0b34":"from sklearn import preprocessing\nscaler = preprocessing.MinMaxScaler()\n#scaler = preprocessing.StandardScaler()\n\nnumeric_features = features.columns[:10].to_list()\nfor i in features.columns[-9:].to_list():\n    numeric_features.append(i)\n\nfeatures[numeric_features] = scaler.fit_transform(features[numeric_features])\ndf_test[numeric_features] = scaler.transform(df_test[numeric_features])","5aa97fa1":"encoder = LabelEncoder()\ntargets[:] = encoder.fit_transform(targets[:])","2771e69f":"features = reduce_mem_usage(features)\ndf_test = reduce_mem_usage(df_test)","64c7ee5d":"#from sklearn.model_selection import train_test_split\n#train_X,val_X,train_y,val_y = train_test_split(features,targets,random_state=1,test_size=0.1)\n#del features,targets\n","e5c137fd":"del scaler","f6f73b30":"from sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\n\ndef objective(trial,X,y, name='xgb'):\n    params = param = {\n        'objective':'multi:softmax',\n        'tree_method':'gpu_hist',  \n        'lambda': trial.suggest_loguniform(\n            'lambda', 1e-3, 10.0\n        ),\n        'alpha': trial.suggest_loguniform(\n            'alpha', 1e-3, 10.0\n        ),\n        'colsample_bytree': trial.suggest_categorical(\n            'colsample_bytree', [0.5,0.6,0.7,0.8,0.9,1.0]\n        ),\n        'subsample': trial.suggest_categorical(\n            'subsample', [0.6,0.7,0.8,1.0]\n        ),\n        'learning_rate': trial.suggest_categorical(\n            'learning_rate', [0.25,0.3,0.03,0.1,0.09, 0.01]\n        ),\n        'n_estimators': trial.suggest_categorical(\n            \"n_estimators\", [150, 200, 300, 3000]\n        ),\n        'max_depth': trial.suggest_categorical(\n            'max_depth', [4,5,7,9,11,13,15,17]\n        ),\n        'random_state': 42,\n        'min_child_weight': trial.suggest_int(\n            'min_child_weight', 1, 300\n        ),\n        'eval_metric':'auc',\n        #'num_of_classes':4,\n        \n    }\n\n    model =  XGBClassifier(**params)\n    model.fit(train_X,train_y,eval_set=[(val_X,val_y)],early_stopping_rounds=50,verbose=False)\n\n\n    train_score = np.round(accuracy_score(train_y, model.predict(train_X)), 5)\n    test_score = np.round(accuracy_score(val_y, model.predict(val_X)), 5)\n                  \n    print(f'TRAIN ROC : {train_score} || TEST ROC : {test_score}')\n                  \n    return test_score","3acabc75":"\n#%%time\n#optimize = partial(objective,X=train_X,y=train_y)\n\n#study_lgbm = optuna.create_study(direction ='maximize')\n#study_lgbm.optimize(optimize,n_trials=30)\n","f8f6b7c3":"#print(f\"\\tBest value (rmse): {study_lgbm.best_value:.5f}\")\n#print(f\"\\tBest params:\")\n\n#for key, value in study_lgbm.best_params.items():\n#   print(f\"\\t\\t{key}: {value}\")\n\n\n#Trial 10 finished with value: 0.9693 and parameters: {'lambda': 0.07892668879722128, 'alpha': 5.967921942315042, 'colsample_bytree': 0.7, \n#'subsample': 1.0, 'learning_rate': 0.09, 'n_estimators': 300, 'max_depth': 15, 'min_child_weight': 64}. Best is trial 10 with value: 0.9693.","720d6e29":"params = {'lambda': 0.07892668879722128, 'alpha': 5.967921942315042, 'colsample_bytree': 0.7, \n'subsample': 1.0, 'learning_rate': 0.09, 'n_estimators': 300, 'max_depth': 15, 'min_child_weight': 64}","f4040fca":"from sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\npreds = []\n\nkf = StratifiedKFold(n_splits=3,random_state=48,shuffle=True)\nacc =[]\nn=0 \n\nfor trn_idx, test_idx in kf.split(features, targets):\n    X_tr, X_val = features.iloc[trn_idx], features.iloc[test_idx]\n    y_tr,y_val= targets.iloc[trn_idx] , targets.iloc[test_idx]\n    \n    model = XGBClassifier(**params,objective= 'multi:softmax', tree_method='gpu_hist')\n    model.fit(X_tr,y_tr,eval_set = [(X_val,y_val)],early_stopping_rounds =100,verbose =False)\n    \n    preds.append(model.predict(df_test))\n    acc.append(accuracy_score(y_val,model.predict(X_val)))\n    \n    \n    print(f\"fold: {n+1} , accuracy: {round(acc[n]*100,3)}\")\n    n+=1\n    \n    del X_tr,X_val,y_tr,y_val\n    gc.collect()\n    \n    \n","a2f4f9cb":"print(f\"the mean Accuracy is : {round(np.mean(acc)*100,3)} \")\n\n","9f119999":"predictions = stats.mode(preds)[0][0]\npredictions = encoder.inverse_transform(predictions)","1292c01d":"predictions","4066cf8d":"index = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\nindex['Cover_Type'] = predictions\nindex.to_csv('submission.csv',index=False)","fbd7ac17":"index","06a0cbfa":"# Making Model and Predict \n","8af67231":"# 1. Data Explanatory Analysis and Cleaning\n\nThis is my third time to join Tabular Play Ground Series. I did not put much time for data explanatory analysis in the last two competitions. I will put some more effort on it this time to do effective feature engineering later on. To do so, I referred to [Machine Learning Explainability](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability) course on Kaggle. \n","504b8d07":"# Data Description\n\nFor this competition, you will be predicting a categorical target based on a number of feature columns given in the data. The data is synthetically generated by a GAN that was trained on a the data from the Forest Cover Type Prediction. This dataset is (a) much larger, and (b) may or may not have the same relationship to the target as the original data.\n\nPlease refer to this data page for a detailed explanation of the features.\n\nFiles\n* train.csv - the training data with the target Cover_Type column\n* test.csv - the test set; you will be predicting the Cover_Type for each row in this file (the target integer class)\n* sample_submission.csv - a sample submission file in the correct format\n\n\n*******From the competition data page.***************\n"}}