{"cell_type":{"9916f10d":"code","4b59e792":"code","14008b3c":"code","ae57ee7c":"code","4918b4f8":"code","834b181c":"code","e4ebc8ad":"code","f4c03ff3":"code","21f11adc":"code","0c1fc36f":"code","a224cf46":"code","53e94f01":"code","63af3cf3":"code","f4001d58":"code","2c7989fe":"code","0b828151":"code","032ff372":"code","daa6a3bd":"code","8457d9ce":"code","3e7c09d2":"code","5966f262":"code","e4ca8083":"code","8b08dacb":"code","9c7304f1":"code","04269c61":"code","411d50a9":"code","559792c9":"code","7e9d1557":"code","22a378cc":"code","cd189156":"markdown","073d52df":"markdown","91ff71fd":"markdown","6eec9b18":"markdown","b78ba1b5":"markdown","9c39ef44":"markdown","d1f8ab38":"markdown","a1ffc979":"markdown"},"source":{"9916f10d":"# Importing libraries\n# Importando bibliotecas\n\nimport pandas as pd\nimport numpy as np","4b59e792":"# Reading Data\n# Lendo os dados\n\ndf = pd.read_csv('..\/input\/simulated-ecommerce-data\/dataset_produtos_para_clusterizacao.csv')\n\ndf = df.astype(str)\n\ndf.head()","14008b3c":"# Extracting the paid value from the string\n# Extraindo o valor pago da string\ndf['valor_pago'] = df['produto'].str.strip().str[-6:].astype('float')\n\n# Creating an sale ID based on the date and client ID\n# Criando uma ID da venda com o date e ID do cliente\nimport re\ndf['id_venda'] = (df['date'].astype(str) + df['id_cliente'].astype(str)).map(lambda x: re.sub(r'\\W+', '', x))\n","ae57ee7c":"# Looking at the unique values\n# Olhando valores \u00fanicos\ndf['produto'].unique()","4918b4f8":"# Creating a category variable for each option with the paid value\n# Criando uma vari\u00e1vel categorica para cada op\u00e7\u00e3o de compra com o valor pago\ndf_categorias = pd.DataFrame()\ndf_categorias['pulseira_m'] = df['produto'].str.contains(\"Pulseira M\").astype(int)*df['valor_pago']\ndf_categorias['sapato_m'] = df['produto'].str.contains(\"Sapato M\").astype(int)*df['valor_pago']\ndf_categorias['camiseta_m'] = df['produto'].str.contains(\"Camiseta M\").astype(int)*df['valor_pago']\ndf_categorias['bota_f'] = df['produto'].str.contains(\"Bota F\").astype(int)*df['valor_pago']\ndf_categorias['sapato_f'] = df['produto'].str.contains(\"Sapato F\").astype(int)*df['valor_pago']\ndf_categorias['cinto_f'] = df['produto'].str.contains(\"Cinto F\").astype(int)*df['valor_pago']\ndf_categorias['bota_m'] = df['produto'].str.contains(\"Bota M\").astype(int)*df['valor_pago']\ndf_categorias['camiseta_f'] = df['produto'].str.contains(\"Camiseta F\").astype(int)*df['valor_pago']\ndf_categorias['moletom_m'] = df['produto'].str.contains(\"Moletom M\").astype(int)*df['valor_pago']\ndf_categorias['vestido_f'] = df['produto'].str.contains(\"Vestido F\").astype(int)*df['valor_pago']\n","834b181c":"# Looking at the unique values\n# Olhando valores \u00fanicos\ndf['pagamento'].unique()","e4ebc8ad":"# Creating a category variable for each option with the paid value\n# Criando uma vari\u00e1vel categorica para cada op\u00e7\u00e3o de compra com o valor pago\ndf_categorias['valor_pix'] = df['pagamento'].str.contains(\"Pix\").astype(int)*df['valor_pago']\ndf_categorias['valor_credito_av'] = df['pagamento'].str.contains(\"Cr\u00e9dito a vista\").astype(int)*df['valor_pago']\ndf_categorias['valor_credito_sj'] = df['pagamento'].str.contains(\"Cr\u00e9dito parcelado sem juros\").astype(int)*df['valor_pago']\ndf_categorias['valor_credito_cj'] = df['pagamento'].str.contains(\"Cr\u00e9dito parcelado com juros\").astype(int)*df['valor_pago']","f4c03ff3":"# Concatenating both datasets \n# Unindo os datasets\ndf = pd.concat([df,df_categorias], axis=1)\ndf.head()","21f11adc":"# Getting the total value for each client in each category\n# Obtendo o valor total para cada cliente em cada categoria\ndf_soma = df.groupby('id_cliente').sum()","0c1fc36f":"# Calculating the amount of unique products for each client\n# Calculando o total de produtos \u00fanicos por cliente\ndf_produtos_unicos = df.groupby(['id_cliente']).nunique()[['produto']]\n\ndf_produtos_unicos.rename(columns={'produto': 'mix_produtos'}, inplace = True)","a224cf46":"# Calculating the total amount of products for each client\n# Calculando o total de produtos comprado por cliente\nds_produtos_total = df.groupby(['id_cliente']).count()[['produto']]\n\nds_produtos_total.rename(columns={'produto': 'total_produtos'}, inplace = True)","53e94f01":"# Calculating the buy frequency for each client\n# Calculando a frequencia de compra de cada cliente\ndf_quantidade_tickets = df.groupby(['id_cliente']).nunique()[['id_venda']]\n\ndf_quantidade_tickets.rename(columns={'id_venda': 'frequencia'}, inplace = True)","63af3cf3":"# Putting everything togheter\n# Unindo tudo em um \u00fanico dataset\ndf = df_soma.merge(df_produtos_unicos, on = 'id_cliente')\n\ndf = df.merge(ds_produtos_total, on = 'id_cliente')\n\ndf = df.merge(df_quantidade_tickets, on = 'id_cliente')\n\n# Calculating the average ticket\n# Calculando o ticket m\u00e9dio\ndf['ticket_medio'] = df['valor_pago'] \/ df['frequencia']\n\ndf.head()","f4001d58":"from sklearn.preprocessing import MinMaxScaler\n\ndf_normalizado = MinMaxScaler().fit_transform(df)\ndf_normalizado = pd.DataFrame(df_normalizado)\n\ndf_normalizado.head()","2c7989fe":"from sklearn.decomposition import PCA\n\n# Testing the amount of componentes to explain 95% of the variance\n# Testando quantidade de componentes que expliquem ao menos 95% da vari\u00e2ncia\nfor i in range(1,df_normalizado.shape[1]):\n  pca = PCA(n_components=i)\n  principalComponents = pca.fit_transform(df_normalizado)\n  variancia = pca.explained_variance_ratio_.sum()\n\n  if  variancia > 0.95:\n    break;\n\nprint(\"Componentes: \", pca.n_components_)\nprint(\"Variance: \", pca.explained_variance_ratio_.sum())\n\ndf_pca = pd.DataFrame(data = principalComponents)","0b828151":"df_pca.head()","032ff372":"# Looking at a 2D plot of the first two principal componentes\n# Olhando um plot 2D dos dois principais componentes\n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Component 1', fontsize = 15)\nax.set_ylabel('Component 2', fontsize = 15)\nax.set_title('Visualization of two components', fontsize = 20)\nax.scatter(df_pca[0],\n           df_pca[1],\n           s = 50,\n           marker = \".\",\n           alpha = 0.6)\nax.grid()","daa6a3bd":"# Looking at a 3D plot\n# Olhando um gr\u00e1fico 3D\nimport plotly.express as px\n\nfig = px.scatter_3d(df_pca, x=0, y=1, z=2)\nfig.show()","8457d9ce":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.metrics import calinski_harabasz_score\nfrom sklearn.metrics import davies_bouldin_score\n\nsilhouette_coefficients = []\ncalinski_coefficients = []\ndavies_coefficients = []\ndistorsions = []\n\nx = range(2,20)\nfor k in x:\n  kmeans = KMeans(n_clusters=k, random_state = 1)\n  kmeans.fit(df_pca)\n\n  silhueta = silhouette_score(df_pca, kmeans.labels_)\n  silhouette_coefficients.append(silhueta)\n\n  calinski = calinski_harabasz_score(df_pca, kmeans.labels_)\n  calinski_coefficients.append(calinski)\n\n  davies = davies_bouldin_score(df_pca, kmeans.labels_)\n  davies_coefficients.append(davies)\n\n  distorcao = kmeans.inertia_\n  distorsions.append(distorcao)\n\n  print(\"Number of clusters: {:1}  Silhouette: {:1.4}  Elbow: {:1.4}  Calinski: {:1.4}  Davies: {:1.4}\".format(k, silhueta,distorcao, calinski, davies))","3e7c09d2":"# Looking at the scores in a plot\n# Olhando os scores em um gr\u00e1fico\nfig, axs = plt.subplots(2, 2,figsize=(10,10))\n\n# x \u00e9 o vetor com os clusters para testar no la\u00e7o for\n\naxs[0, 0].plot(x, silhouette_coefficients)\naxs[0, 0].set_title('Silhouette')\naxs[0, 1].plot(x, distorsions, 'tab:orange')\naxs[0, 1].set_title('Elbow')\naxs[1, 0].plot(x, calinski_coefficients, 'tab:green')\naxs[1, 0].set_title('Calinski Harabasz')\naxs[1, 1].plot(x, davies_coefficients, 'tab:red')\naxs[1, 1].set_title('Davies Bouldin')\n\nfor ax in axs.flat:\n    ax.set(xlabel='Clusters', ylabel='Score', xticks = x)\n\nfor ax in axs.flat:\n    ax.grid()\n\nplt.show()","5966f262":"kmeans = KMeans(n_clusters=4, random_state = 123)\nkmeans.fit(df_pca)\n\ndf_cluster = pd.DataFrame(data = kmeans.labels_\n             , columns = ['Cluster']) \n\ndf_cluster = df_cluster.set_index(df.index)\n\ndf_feature_cluster = df.merge(df_cluster, on = 'id_cliente')","e4ca8083":"# Printing the clusters in the PCA plot\n# Adicionando os clusters ao gr\u00e1fico dos componentes\ndf_pca_cluster = df_pca.set_index(df.index).merge(df_cluster, on = 'id_cliente')\n\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Component 1', fontsize = 15)\nax.set_ylabel('Component 2', fontsize = 15)\nax.set_title('Clusters', fontsize = 20)\nax.scatter(df_pca_cluster[0],\n           df_pca_cluster[1],\n           c = df_pca_cluster['Cluster'],\n           s = 50,\n           marker = \".\",\n           alpha = 0.6)\n\nax.grid()\n\n# Remember that this are simulated data and not real costumers profiles\n# Lembre-se que s\u00e3o dados simulados e n\u00e3o dados de consumidores reais","8b08dacb":"# Looking at a 3D plot\n# Olhando um gr\u00e1fico 3D\nimport plotly.express as px\n\nfig = px.scatter_3d(df_pca_cluster, x=0, y=1, z=2, color='Cluster')\nfig.show()\n\n# Looks way better\n# Melhor de visualizar","9c7304f1":"df_id_cluster = df_feature_cluster[['Cluster']]","04269c61":"# Average of all columns by cluster\n# M\u00e9dia de todas as colunas por cluster\ndf_feature_cluster.groupby(by = 'Cluster').mean()","411d50a9":"# Amount of clients in each cluster\n# Quantidade de clientes em cada cluster\ntamanho_clusters = df_feature_cluster.reset_index().groupby(by = 'Cluster').count()[['id_cliente']]\n\ntotal_cliente_cluster = df_feature_cluster.reset_index().groupby(['Cluster']).agg({'id_cliente':'nunique'}).id_cliente\n\ntamanho_clusters","559792c9":"# Total amount of clients\n# Quantidade total de clientes\ntotal_clientes = df_feature_cluster.reset_index().nunique()['id_cliente']\ntotal_clientes","7e9d1557":"# Percentage of clients that pays in each category,\n# it's more than 100 because each client can use more than one method\n\n# Porcentagem de clientes que pagou em cada categoria\n# N\u00e3o soma 100% pois cada cliente pode ter utilizado mais de uma categoria\n\n\npd.options.mode.chained_assignment = None  # default='warn' Just to remove the warning message\n\ndf_pagamento = df_feature_cluster[['Cluster', 'valor_pix', 'valor_credito_av', 'valor_credito_sj', 'valor_credito_cj']].set_index('Cluster')\n\ndf_pagamento[df_pagamento != 0] = 1\n\ndf_pagamento = df_pagamento.reset_index().groupby(['Cluster']).sum()\n\ndf_pagamento = df_pagamento.div(total_cliente_cluster, axis=0)*100\n\ndf_pagamento","22a378cc":"# Percentage of clients that buys in each product category,\n# it's more than 100 because each client can use more than one method\n\n# Porcentagem de clientes que comprou em cada categoria de produto\n# N\u00e3o soma 100% pois cada cliente pode ter utilizado mais de uma categoria\n\ndf_categorias = df_feature_cluster[['Cluster','pulseira_m','sapato_m','camiseta_m','bota_f','sapato_f','cinto_f','bota_m','camiseta_f','moletom_m','vestido_f']].set_index('Cluster')\n\ndf_categorias[df_categorias != 0] = 1\n\ndf_categorias = df_categorias.reset_index().groupby(['Cluster']).sum()\n\ndf_categorias = df_categorias.div(total_cliente_cluster, axis=0)*100\n\ndf_categorias","cd189156":"## Principal component analysis\n\nDimensionality reduction for clustering","073d52df":"## Feature engineering","91ff71fd":"## Clustering\n\nUsing K-Means","6eec9b18":"### Clustering with the best amount of clusters\n\n4 clusters looks like a good number  \n4 clusters parece ser um n\u00famero adequado an\u00e1lisando a PCA e os gr\u00e1ficos\n\nRemember that this are simulated data and not real costumers profiles  \nLembre-se que s\u00e3o dados simulados e n\u00e3o dados de consumidores reais","b78ba1b5":"### Testing the best amount of clusters","9c39ef44":"## Standarization\n\nUsing MinMaxScaler, you can test with other methods\n\nUsando MinMaxScaler, voc\u00ea pode testar com outros m\u00e9todos","d1f8ab38":"## Studying the clusters","a1ffc979":"# Client segmentation with e-commerce data\n# Segmenta\u00e7\u00e3o de clientes com dados de e-commerce\n\nThis notebook will present a client segmentation using the dataset with simulated e-commerce data.\n\nEste notebook ir\u00e1 apresentar uma segmenta\u00e7\u00e3o de clientes utilizando um dataset com dados simulados de um e-commerce.\n\n\nComments are in English and Portuguese\nOs coment\u00e1rios estar\u00e3o em ingl\u00eas e portugues."}}