{"cell_type":{"3db42dd8":"code","a606bfb0":"code","bfc05594":"code","d7aec646":"code","af9517df":"code","b098a395":"code","a36841df":"code","ab812eb5":"code","5e4ae599":"markdown","a0d8c403":"markdown","2b08a340":"markdown","75e85c5e":"markdown","aa9cf2e0":"markdown","cd8bb6c6":"markdown"},"source":{"3db42dd8":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt","a606bfb0":"var1 = np.random.rand(10_000)\npart1 = np.random.rand(5000)\npart2 = np.random.rand(5000) + 4\n\nvar2 = np.hstack([part1,part2])\ndf = pd.concat([pd.Series(var1), pd.Series(var2)], axis=1)\ndf.hist()\nplt.tight_layout()","bfc05594":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nvar1_standard = scaler.fit_transform(var1.reshape((-1,1)))\nvar2_standard = scaler.fit_transform(var2.reshape((-1,1)))\n\nfig, ax1 = plt.subplots() \ndf_std = pd.concat([pd.Series(var1_standard.flatten()), pd.Series(var2_standard.flatten())], axis=1)\ndf_std.columns = ['variable 1 standardized', 'variable 2 standardized']\ndf_std.plot.scatter(x='variable 1 standardized', y='variable 2 standardized', ax=ax1);","d7aec646":"print(f'The biggest possible manhatten distance in standardized variable 1: {np.abs(var1_standard.min() - var1_standard.max())}')\nprint(f'The biggest possible manhatten distance in standardized variable 1: {np.abs(var2_standard.min() - var2_standard.max())}')","af9517df":"var1 = np.random.rand(10_000)\npart1 = np.random.rand(100)\npart2 = np.random.rand(9800) + 0.5\npart3 = np.random.rand(100) + 1\n\nvar2 = np.hstack([part1,part2, part3])\ndf = pd.concat([pd.Series(var1), pd.Series(var2)], axis=1)\ndf.hist()\nplt.tight_layout()","b098a395":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nvar1_standard = scaler.fit_transform(var1.reshape((-1,1)))\nvar2_standard = scaler.fit_transform(var2.reshape((-1,1)))\n\nfig, ax1 = plt.subplots() \ndf_std = pd.concat([pd.Series(var1_standard.flatten()), pd.Series(var2_standard.flatten())], axis=1)\ndf_std.columns = ['variable 1 standardized', 'variable 2 standardized']\ndf_std.plot.scatter(x='variable 1 standardized', y='variable 2 standardized', ax=ax1);","a36841df":"print(f'The biggest possible manhatten distance in standardized variable 1: {np.abs(var1_standard.min() - var1_standard.max())}')\nprint(f'The biggest possible manhatten distance in standardized variable 1: {np.abs(var2_standard.min() - var2_standard.max())}')","ab812eb5":"from sklearn.preprocessing import MinMaxScaler\nscaler_range = MinMaxScaler()\nvar1_range = scaler_range.fit_transform(var1.reshape((-1,1))) \nvar2_range = scaler_range.fit_transform(var2.reshape((-1,1)))\n\ndf_range = pd.concat([pd.Series(var1_range.flatten()), pd.Series(var2_range.flatten())], axis=1)\ndf_range.columns = ['variable 1 range scaled', 'variable 2 range scaled']\ndf_range.plot.scatter(x='variable 1 range scaled', y='variable 2 range scaled');","5e4ae599":"## Even worse: outliers dramatically worsen the situation","a0d8c403":"# Results\n* Standardization doesn't ensure the data to be exactly on the same scale\n* Discarding extreme outliers vastly improves the results\n\nNevertheless, standard scaling is a very powerful method and it outperforms different feature engineering methods in most cases. However, you shouldn't rely on it being the best method for all of your projects. E.g. try performing range scaling after standardization, or range scaling on its own as well.","2b08a340":"After performing range scaling, all variables are on the same scale (between 0 and 1 by default). This might be beneficial for distance based methods.","75e85c5e":"As we can see, the difference between the minimum value and the maximum value of variable 1 vastly differs that of variable 2.","aa9cf2e0":"Distance based machine learning algorithms might struggle with this.\n\nWhat about range scaling?","cd8bb6c6":"# Range scaling vs Standardization\nHi all,\nin this very short notebook I want to show you why standardization doesn't always force your variables to be on the same scale."}}