{"cell_type":{"b5f8cd1d":"code","9445b7ae":"code","11910fb4":"code","921b5477":"code","de7b3185":"code","13e777da":"code","a755b801":"code","9aaabfa4":"code","340fd606":"code","417c5e75":"code","224e026e":"code","c902fac0":"code","eb9151ea":"code","fba9a649":"code","5b5fde5b":"markdown","f0dcfeee":"markdown","5706b768":"markdown","cd5460d3":"markdown","eb9db879":"markdown","2909fd55":"markdown","cc3c5b32":"markdown","a7f78e0a":"markdown","7ec5e9a2":"markdown","99994fd7":"markdown","993194c6":"markdown","29b0cec8":"markdown","2bfbceb8":"markdown"},"source":{"b5f8cd1d":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt","9445b7ae":"\nx_train = pd.read_csv('..\/input\/train.csv', dtype='uint8', header=0)\n#x_test = pd.read_csv('..\/input\/test.csv', dtype='uint8', header=0)  # Not used","11910fb4":"x_train.shape","921b5477":"x_train.head()","de7b3185":"y_train = x_train.label\nx_train.drop('label', axis=1, inplace=True)","13e777da":"y_train.head()","a755b801":"# Scale values to be friendlier with our Sigmoid activation\nscaler = preprocessing.MinMaxScaler()\nx_train = pd.DataFrame(scaler.fit_transform(x_train))","9aaabfa4":"x_train.describe()","340fd606":"lb = preprocessing.LabelBinarizer()\nlb.fit(y_train)\ny_train = lb.transform(y_train)","417c5e75":"# let's check our vector shapes\nx_train.shape, y_train.shape","224e026e":"# Hidden layer activation function (good as our inputs are b\/w 0 and 1)\ndef sigmoid(x):\n    \"\"\"\n    Sigmoid function\n    \"\"\"\n    return 1.0 \/ (1.0 + np.exp(-x))\n\n# Output layer activation function\ndef softmax(z):\n    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n    return np.exp(z) \/ np.sum(np.exp(z), axis=1, keepdims=True)\n\n# Using log loss\ndef loss_fn(y_true, y_pred, eps=1e-16):\n    \"\"\"\n    Loss function we would like to optimize (minimize)\n    We are using Logarithmic Loss (Categorical Cross Entropy)\n    http:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#log-loss\n    \"\"\"\n    y_pred = np.maximum(y_pred,eps)\n    y_pred = np.minimum(y_pred,(1-eps))  # Preventing inf\n    return -(np.sum(y_true * np.log(y_pred)) + np.sum((1-y_true)*np.log(1-y_pred)))\/len(y_true)\n\n# Setup processor for two layer NN\ndef forward_pass(W_1, W_2, x, y):\n    \"\"\"\n    Does a forward computation of the neural network\n    Also produces the gradient of the log loss function\n    \"\"\"\n    # First, compute the new predictions `y_pred`\n    z_2 = np.dot(x, W_1)  # ILF1  (Induced Local Field to layer 2)\n    a_2 = sigmoid(z_2)\n    z_3 = np.dot(a_2, W_2)  # ILF2\n    y_pred = softmax(z_3)\n    \n    # Now compute the gradient: BackProp\n    J_z_3_grad = -y + y_pred\n    J_W_2_grad = np.dot(a_2.T, J_z_3_grad)\n    a_2_z_2_grad = sigmoid(z_2)*(1-sigmoid(z_2))\n    J_W_1_grad = (np.dot(J_z_3_grad, W_2.T)*a_2_z_2_grad).T.dot(x).T\n    gradient = (J_W_1_grad, J_W_2_grad)\n    \n    # return\n    return y_pred, gradient\n\n\ndef plot_loss_accuracy(loss_vals, accuracies, header):\n    fig = plt.figure(figsize=(16, 8))\n    fig.suptitle('Log Loss and Accuracy over {} iterations'.format(header))\n    \n    ax = fig.add_subplot(1, 2, 1)\n    ax.plot(loss_vals)\n    ax.grid(True)\n    ax.set(xlabel='iterations', title='Log Loss')\n    \n    ax = fig.add_subplot(1, 2, 2)\n    ax.plot(accuracies)\n    ax.grid(True)\n    ax.set(xlabel='iterations', title='Accuracy');","c902fac0":"def train_nn(W_1, W_2, eta, num_iter, x, y, x_test, y_test, eps=0.1):\n    loss_vals, accuracies = [], []\n    loss_vals_test, accuracies_test = [], []\n    y_pred = None\n    l_prev = 100  # To control eta\n    for i in range(num_iter):\n        ### Do a forward computation, and get the gradient\n        y_pred, (g_w_1, g_w_2) = forward_pass(W_1, W_2, x, y)\n                \n        ## Update the weight matrices using Gradient Descent\n        W_1 = W_1 - eta*g_w_1\n        W_2 = W_2 - eta*g_w_2\n\n        ### Compute the loss and accuracy\n        l = loss_fn(y, y_pred)\n#         print(y,y_pred,l)\n        \n        # Adjust eta to help convergence\n        if l >= l_prev:\n            eta = eta * 0.9\n#             print(\"eta updated to: \", eta)\n        l_prev = l\n        match = np.sum(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))\n        acc = match\/len(y)\n        \n        loss_vals.append(l)\n        accuracies.append(acc)\n        \n        test_acc, test_l = test_nn(x_test, y_test, W_1, W_2)\n        loss_vals_test.append(test_l)\n        accuracies_test.append(test_acc)\n\n        ## Print the loss and accuracy for every 200th iteration\n        if i%200 == 0:\n            print(f'Epoch = {i}')\n            print(\"Loss={}, Acc={}\".format(l, acc))\n            print(\"Validation Loss={}, Validation Acc={}\".format(test_l, test_acc))\n            \n        if l <= eps:  # Epsilon threshold\n            print(\"Epoch: {}. Breaking as desired acc ({}) achieved..\".format(i, acc))\n            break\n        \n    plot_loss_accuracy(loss_vals, accuracies, \"train\")\n    plot_loss_accuracy(loss_vals_test, accuracies_test, \"test\")\n    return y_pred, W_1, W_2\n\ndef test_nn(x, y, W_1, W_2):\n    z_2 = np.dot(x, W_1)  # ILF1  (Induced Local Field to layer 2)\n    a_2 = sigmoid(z_2)\n    z_3 = np.dot(a_2, W_2)\n    y_pred = softmax(z_3)\n    \n    # Calculate loss\n    l = loss_fn(y, y_pred)\n    # Calculate acc\n    match = np.sum(np.argmax(y_pred, axis=1) == np.argmax(y, axis=1))\n    acc = match\/len(y)\n    return (acc, l)\n\ndef predict(x, W_1, W_2):\n    z_2 = np.dot(x, W_1)  # ILF1  (Induced Local Field to layer 2)\n    a_2 = sigmoid(z_2)\n    z_3 = np.dot(a_2, W_2)  # ILF2\n    y_pred = softmax(z_3)\n    \n    return np.argmax(y_pred, axis=1)","eb9151ea":"#### Initialize the network parameters\n\n# Split into train\/validation\nX_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train, test_size=0.10)\n\nnp.random.seed(1241)\ninput_units = X_train.shape[1]\nh1_units = 128       # Hidden layer 1 units\nout_units = y_train.shape[1]\nn = len(X_train)           # No. of training samples to use in training\n\nW_1 = np.random.uniform(-1,1,size = (input_units,h1_units))  # Include bias\nW_2 = np.random.uniform(-1,1,size = (h1_units,out_units))\nnum_iter = 1000  # Epochs\neta = .0001\n\ny_pred, W_1, W_2 = train_nn(W_1, W_2, eta, num_iter, X_train[:n], Y_train[:n], X_test[:n], Y_test[:n])\n","fba9a649":"from sklearn.metrics import accuracy_score\ny_pred = predict(X_test, W_1, W_2)\naccuracy_score(np.argmax(Y_test, axis=1), y_pred)","5b5fde5b":"We will start by importing the required packages and the data.","f0dcfeee":"# The ANN\n\n'forward_pass()' performs the forward pass with Backpropagation using Gradient Descent. <br \/>\nBrackpropagation is a loss optimization (error minimization) technique that uses Gradient Descent (fine steps in the negative direction of the gradient\/slope of the error function), to train\/tune the best weights (coefficents) for the function variables (features). \n\nI know, pithy statement with quite a few parentheses. But fret not, here is a good [resource](https:\/\/medium.com\/datathings\/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e) to understand or brush up on the concept if the water looks murky!","5706b768":"Note that I have reduced the number of neurons in the hidden layer to 128. Using 256 neurons yielded 96% accuracy but with longer training time. ","cd5460d3":"### Setting up the Neural Network\n\nHere, I will be using a two-layer NN with Sigmoid activation for the hidden layer units, and Softmax for output. <br \/> \nHere is another [resource](http:\/\/dataaspirant.com\/2017\/03\/07\/difference-between-softmax-function-and-sigmoid-function\/) to understand about these choices.\n\nSimply put, Softmax generates probabilistic output (much like Scikit-learn's predict.proba()) for our multi-class labels.","eb9db879":"## Breaking the Magician's Code!\n### MNIST Digit Recognizer using raw Deep Learning (No ML libs used).\nThis is a good place to understand the ground-up setup of ANNs using BackProp (Gradient Descent), and how things work under-the-hood to train your network. <br \/>\nIf you have any suggestions or questions, please feel free to leave in comments. ","2909fd55":"**Memory Optimization: **Using uint8 takes 4 times less memory as we know pixel vales are b\/w [0,255]. <br \/>\nPS: Although the dataframe will be converted back to float64, during scaling later however, this is still a good technique to use if you need to optimize memory consumption while loading data, among other approaches.","cc3c5b32":"There are 784 (flattened 28x28) pixel values (0-255) for each of the 42000 training digits.\n\nLet's extract the label column.","a7f78e0a":"**One-hot encoding of labels: ** It is important to binarize the labels as we will use a 10 unit ouput layer to predict the digits from 0 to 9.","7ec5e9a2":"Hope this simplified outline helped you get the gist of how things really work under-the-hood in a Deep Neural Network. If it did, please don't forget to vote.","99994fd7":"## Finally, set up the ANN parameters and initialize the training","993194c6":"## Preparing ANN for training","29b0cec8":"Here's a sneak peek at the data.","2bfbceb8":"**Scaling** the pixel values to [0, 1] in the training data makes it easier for our activation functions to converge (using Gradient Descent) faster, by bringing them within the output range."}}