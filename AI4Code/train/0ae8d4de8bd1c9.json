{"cell_type":{"e41931ce":"code","feb41e0c":"code","020d896a":"code","23b5fd96":"code","4b82a13e":"code","f0589748":"code","19b9faf9":"code","3a313ad4":"code","0fbd4dfb":"code","14ad7a00":"code","54e14039":"code","6365381f":"code","b21f7a56":"code","5dbb4a8f":"code","15eaff84":"code","99dc8a63":"code","79703054":"code","9ae1ee1a":"code","5f3ec8e8":"code","f7ab5c63":"code","62ca91d1":"code","b517778f":"code","200e3245":"code","587e1482":"code","d444d387":"code","e3cfc846":"code","1485d612":"code","f5203104":"code","d4062399":"code","e9f6e9bb":"code","edfe384e":"code","05df2c11":"code","9cc0c2e2":"code","3ba0f842":"code","af55da2d":"code","7fb53dc6":"code","21b6a751":"code","9741038b":"code","573eabf8":"code","b6e5f53c":"code","9ebb6839":"code","331caaf9":"code","2db8296d":"code","ee035ff5":"code","0cbcad73":"code","be87e798":"code","86d8281c":"code","86b30850":"code","d0111353":"code","6a79bb82":"code","1b17742f":"code","908bc652":"code","8ad70aec":"code","8101a237":"code","d88831ff":"code","5569df8a":"code","4d572afd":"code","dc100e00":"markdown","fede142d":"markdown","92f10600":"markdown","6100d7e2":"markdown","d02606c6":"markdown","40d6ecde":"markdown","35db2cb6":"markdown","636a5070":"markdown","e64c3e49":"markdown","42a2c30d":"markdown","0bb7aff8":"markdown","07c6bd27":"markdown","34de5404":"markdown","c02a4017":"markdown","c09769ae":"markdown","0e3ad049":"markdown","c137a166":"markdown","cfc13ac5":"markdown","781d6e69":"markdown","b28ee4e5":"markdown"},"source":{"e41931ce":"!pip install seaborn --upgrade","feb41e0c":"import pandas as pd\nimport nest_asyncio             \nimport matplotlib.pyplot as plt\n\n#Reading data and models\nimport glob                     \nimport os\nimport pickle\n\nimport numpy as np\nimport datetime as dt\nimport seaborn as sns\n\n#cleaning\nimport re\nfrom nltk.tokenize import WordPunctTokenizer\nfrom nltk.corpus import stopwords             \n\n# Sentiment Analysis\nfrom textblob import TextBlob\n\n#word cloud\nfrom wordcloud import WordCloud","020d896a":"# Set pallette and theme for visualisation\nsns.set_theme()\npal = {\"FNB\":'c', \"Standard\":\"b\",\"ABSA\":\"r\",\"Nedbank\":\"g\",\"Capitec\":\"grey\"}","23b5fd96":"tweets_df= pd.concat(map(pd.read_csv, glob.glob(os.path.join('', \"..\/input\/twitter-scrape-of-the-top-5-banks-in-south-africa\/full*.csv\"))))","4b82a13e":"tweets_df.shape","f0589748":"file = r\"..\/input\/twitter-sentiment-analysis-custom-model\/pipemodel_model.pickle\"\npipe = pickle.load(open(file, 'rb'))","19b9faf9":"tweets_df = tweets_df.drop(['Unnamed: 0','created_at', 'user_id_str', 'link', 'urls', 'photos', 'video',\n       'thumbnail', 'retweet','nreplies', 'nretweets', 'quote_url', 'near', 'geo', 'source', 'user_rt_id', 'user_rt',\n       'retweet_id', 'reply_to', 'retweet_date', 'translate', 'trans_src',\n       'trans_dest'],axis = 1)","3a313ad4":"tweets_df.head(2)","0fbd4dfb":"tweets_df.info()","14ad7a00":"tweets_df[\"language\"].unique()","54e14039":"# remove all rows where language is not english or undefined\ntweets_df = tweets_df[tweets_df[\"language\"].isin([ 'und', 'en'])]","6365381f":"# remove rows where username is in bank_search\ntweets_df = tweets_df[ ~tweets_df[\"username\"].str.lower().str.contains('fnb|standardbank|nedbank|absa|capitec',regex = True)]","b21f7a56":"tweets_df[\"base_tweet\"] = tweets_df[\"base_tweet\"].astype(str)\ntweets_df[\"cleaned_tweet\"] = tweets_df[\"cleaned_tweet\"].astype(str)","5dbb4a8f":"#Drop duplicated tweets \ntweets_df = tweets_df.drop_duplicates(keep=\"first\")","15eaff84":"#set date as a Datetime and sort\ntweets_df[\"date\"] = pd.to_datetime(tweets_df[\"date\"])\ntweets_df = tweets_df.sort_values(\"date\")","99dc8a63":"# reset the index for visualisation due to dropped values\ntweets_df.reset_index(inplace=True, drop = True )","79703054":"tweets_df.shape","9ae1ee1a":"def Sentiment(val):\n    if val == -1:\n        return \"Negative\"\n    elif val == 0:\n        return \"Neutral\"\n    elif val == 1:\n        return \"Postive\"\n    else:\n        return \"Other\"","5f3ec8e8":"tweets_df[\"sentiment\"] = pipe.predict(tweets_df[\"cleaned_tweet\"])\ntweets_df[\"sentiment\"] = tweets_df[\"sentiment\"].apply(Sentiment)","f7ab5c63":"tweets_df.shape","62ca91d1":"probabilities = pipe.predict_proba(tweets_df[\"cleaned_tweet\"])\nprobabilities = pd.DataFrame(probabilities)","b517778f":"tweets_df[\"polarity\"]= probabilities[0]*-1+probabilities[1]*0+probabilities[2]*1","200e3245":"tweets_df.shape","587e1482":"tweets_df[\"Bank\"] = \"\"","d444d387":"def create_bank_col(df):\n    #print (df[1]) \n    bank = df[1]\n    if \"fnb\" in df[0].lower():\n        bank =  bank+\"FNB;\"\n    if \"nedbank\" in df[0].lower():\n        bank =  bank+\"Nedbank;\"\n    if \"absa\" in df[0].lower():\n        bank =  bank+\"ABSA;\"\n    if \"standard\" in df[0].lower():\n        bank =  bank+\"Standard;\"\n    if \"capitec\" in df[0].lower():\n        bank =  bank+\"Capitec;\"\n    return bank ","e3cfc846":"tweets_df[\"Bank\"] = tweets_df[[\"base_tweet\",\"Bank\"]].apply(create_bank_col, axis =1 )","1485d612":"tweets_df.head()","f5203104":"tweets_df.shape","d4062399":"# stop this warning as the chaining works as intended\npd.options.mode.chained_assignment = None ","e9f6e9bb":"#Create bank Dataframes \nStandard_df = tweets_df[tweets_df[\"Bank\"].str.contains(\"Standard\")]\nStandard_df[\"Bank\"]= \"Standard\"\n\nFNB_df = tweets_df[tweets_df[\"Bank\"].str.contains(\"FNB\")]\nFNB_df[\"Bank\"]= \"FNB\"\n\nNedbank_df = tweets_df[tweets_df[\"Bank\"].str.contains(\"Nedbank\")]\nNedbank_df[\"Bank\"]= \"Nedbank\"\n\nABSA_df = tweets_df[tweets_df[\"Bank\"].str.contains(\"ABSA\")]\nABSA_df[\"Bank\"]= \"ABSA\"\n\nCap_df = tweets_df[tweets_df[\"Bank\"].str.contains(\"Capitec\")]\nCap_df[\"Bank\"]= \"Capitec\"","edfe384e":"Final_df = pd.concat([ABSA_df,Standard_df,Nedbank_df,ABSA_df,Cap_df, FNB_df ], axis =0)","05df2c11":"Final_df.shape","9cc0c2e2":"#Drop duplicated tweets \nFinal_df = Final_df.drop_duplicates(keep=\"first\")\nFinal_df.reset_index(inplace=True, drop = True )","3ba0f842":"Final_df.shape","af55da2d":"Final_df.head()","7fb53dc6":"#install additional libraries for visualisation \nfrom collections import Counter\n\nimport cufflinks as cf\nfrom plotly.offline import init_notebook_mode #, plot, iplot, download_plotlyjs\ninit_notebook_mode(connected = True)\ncf.go_offline()\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots","21b6a751":"Final_df.head()","9741038b":"fig, ax = plt.subplots(1,2, figsize= (15,7))\n\nsns.countplot(ax = ax[0], x= Final_df[\"Bank\"], palette= pal)\nax[0].set_title(\"Count of tweets\")\n\nsns.barplot(data =Final_df, x = \"Bank\" ,y = \"nlikes\",estimator=np.sum,ci=None, palette=pal)\nax[1].set_title(\"Count of likes\")\n\nplt.tight_layout()\nplt.show()","573eabf8":"display(Final_df[[\"base_tweet\",\"Bank\"]].groupby([\"Bank\"]).count().transpose())\n\nplt.figure(figsize=(10,5))\nsns.countplot(data=Final_df, x=\"sentiment\", order = Final_df[\"sentiment\"].value_counts(dropna= False).index,hue=\"Bank\", palette= pal )\nplt.title(\"Count of tweets by sentiment\",fontsize =15)\n\nplt.tight_layout()\nplt.legend(loc=\"upper right\")\nplt.show()","b6e5f53c":"\"\"\"fig1 = sns.displot(data = Final_df, x=\"polarity\",\n                   col=\"Bank\",\n                   col_wrap= 3, \n                   hue=\"Bank\", \n                   legend=False, \n                   palette= pal,\n                   kde = True,\n                   bins =30)\nfig1.fig.suptitle(\"Distribution of Sentiment scores(polarity)\",fontsize =15 )\n\nplt.tight_layout()\nplt.show()\"\"\"","9ebb6839":"#get all hashtags as list\ndef hashlist(df):\n    hashlist = []\n    for i in df['hashtags']:\n        #use ast.literal if you are importing CSV files otherwise just use 'i'\n        hashlist.extend(i)\n    return hashlist","331caaf9":"#Create hashtag dataframes\nhash_Absa= pd.DataFrame(Counter(hashlist(ABSA_df)).items()).sort_values(1,ascending=False)\nhash_NedBank= pd.DataFrame(Counter(hashlist(Nedbank_df)).items()).sort_values(1,ascending=False)\nhash_StdBank= pd.DataFrame(Counter(hashlist(Standard_df)).items()).sort_values(1,ascending=False)\nhash_FNB= pd.DataFrame(Counter(hashlist(FNB_df)).items()).sort_values(1,ascending=False)\nhash_Cap= pd.DataFrame(Counter(hashlist(Cap_df)).items()).sort_values(1,ascending=False)","2db8296d":"fig, ax = plt.subplots(2, 2,figsize=(15, 10))\n\nplt.suptitle(\"Top 5 hashtags per bank\")\n\n# ABSA\nax[0,0].bar(hash_Absa[0].head(), hash_Absa[1].head(), color = \"r\")\nax[0,0].set_title(\"ABSA\")\nax[0,0].xaxis.set_tick_params(rotation=45, size = 15)\n\nax[0,1].bar(hash_NedBank[0].head(), hash_NedBank[1].head(), color = \"g\")\nax[0,1].set_title(\"Nedbank\")\nax[0,1].xaxis.set_tick_params(rotation=45, size = 15)\n\nax[1,0].bar(hash_StdBank[0].head(), hash_StdBank[1].head(), color = \"b\")\nax[1,0].set_title(\"Standard Bank\")\nax[1,0].xaxis.set_tick_params(rotation=45, size = 15)\n\nax[1,1].bar(hash_FNB[0].head(), hash_FNB[1].head(), color = \"c\")\nax[1,1].set_title(\"FNB\")\nax[1,1].xaxis.set_tick_params(rotation=45, size = 15)\n\nplt.tight_layout()\nplt.show()","ee035ff5":"tweetString_a = \" \".join(list(ABSA_df[\"cleaned_tweet\"])).lower()\ntweetString_n = \" \".join(list(Nedbank_df[\"cleaned_tweet\"])).lower()\ntweetString_s = \" \".join(list(Standard_df[\"cleaned_tweet\"])).lower()\ntweetString_f = \" \".join(list(FNB_df[\"cleaned_tweet\"])).lower()","0cbcad73":"#remove Bank name and set wordcloud\n\ntweetString_a = re.sub(r\"absa|bank\",\"\",tweetString_a)\nwordcloud_a = WordCloud(\n                background_color ='white', \n                min_font_size = 5).generate(tweetString_a)\n\ntweetString_n = re.sub(r\"NedBankSA|Nedbank|nedbank|bank\",\"\",tweetString_n)   \nwordcloud_n = WordCloud( \n                background_color ='white', \n                min_font_size = 5).generate(tweetString_n)\n\ntweetString_s = re.sub(r\"standardbankza|standard bank|bank\",\"\",tweetString_s)     \nwordcloud_s = WordCloud( \n                background_color ='white', \n                min_font_size = 5).generate(tweetString_s)\n\ntweetString_f = re.sub(r\"FNB|fnb|bank\",\"\",tweetString_f)\nwordcloud_f = WordCloud( \n                background_color ='white', \n                min_font_size = 5).generate(tweetString_f)","be87e798":"fig, ax = plt.subplots(2,2,figsize=(20, 10),sharey=True)\n\nax[0,0].imshow(wordcloud_s)\nax[0,1].imshow(wordcloud_f)\nax[1,0].imshow(wordcloud_n)\nax[1,1].imshow(wordcloud_a)\n\nax[0,0].axis(\"off\")\nax[0,1].axis(\"off\")\nax[1,0].axis(\"off\")\nax[1,1].axis(\"off\")\n\nax[0,0].set_title(\"StandardBank\")\nax[0,1].set_title(\"FNB\")\nax[1,0].set_title(\"Nedbank\")\nax[1,1].set_title(\"ABSA\")\n\nplt.tight_layout() \nplt.show()","86d8281c":"# Overall mean sentiment by bank\nplt.figure(figsize=(10,5))\nplt.title(\"Overall mean Sentiment by Bank\")\nsns.barplot(data = Final_df, x= \"Bank\", y = \"polarity\", palette=pal, ci=False)\nplt.show()","86b30850":"Final_df = Final_df.sort_values(\"date\").set_index(\"date\")\nABSA_df = ABSA_df.sort_values(\"date\").set_index(\"date\")\nNedbank_df = Nedbank_df.sort_values(\"date\").set_index(\"date\")\nStandard_df = Standard_df.sort_values(\"date\").set_index(\"date\")\nFNB_df = FNB_df.sort_values(\"date\").set_index(\"date\")\nCap_df = Cap_df.sort_values(\"date\").set_index(\"date\")","d0111353":"year_group = Final_df.groupby([\"Bank\",Final_df.index.year])[\"polarity\"].mean()","6a79bb82":"plt.figure(figsize = (20,8))\nplt.title(\"Average sentiment by year\",fontsize= 20)\nsns.barplot(x= year_group.index.get_level_values(\"date\"), y =year_group.values, hue = year_group.index.get_level_values(\"Bank\"))","1b17742f":"plt.figure(figsize = (20,8))\nsns.lineplot(data = FNB_df.groupby(FNB_df.index.year)[\"polarity\"].mean(), color = \"c\", label = \"FNB\")\nsns.lineplot(data = Nedbank_df.groupby(Nedbank_df.index.year)[\"polarity\"].mean(), color = \"g\", label = \"Nedbank\")\nsns.lineplot(data = ABSA_df.groupby(ABSA_df.index.year)[\"polarity\"].mean(), color = \"r\", label = \"ABSA\")\nsns.lineplot(data = Standard_df.groupby(Standard_df.index.year)[\"polarity\"].mean(), color = \"b\", label = \"StdBank\")\nsns.lineplot(data = Cap_df.groupby(Cap_df.index.year)[\"polarity\"].mean(), color = \"grey\", label = \"Capitec\")\nplt.title(\"Sentiment by year\", fontsize = 20)\nplt.show()","908bc652":"plt.figure(figsize = (20,8))\nsns.lineplot(data = FNB_df.groupby(FNB_df.index.month_name())[\"polarity\"].mean(), color = \"c\", label = \"FNB\")\nsns.lineplot(data = Nedbank_df.groupby(Nedbank_df.index.month_name())[\"polarity\"].mean(), color = \"g\", label = \"Nedbank\")\nsns.lineplot(data = ABSA_df.groupby(ABSA_df.index.month_name())[\"polarity\"].mean(), color = \"r\", label = \"ABSA\")\nsns.lineplot(data = Standard_df.groupby(Standard_df.index.month_name())[\"polarity\"].mean(), color = \"b\", label = \"StdBank\")\nsns.lineplot(data = Cap_df.groupby(Cap_df.index.month_name())[\"polarity\"].mean(), color = \"grey\", label = \"Capitec\")\nplt.title(\"Sentiment by month\", fontsize = 15)\nplt.show()","8ad70aec":"plt.figure(figsize = (20,8))\n\nplt.subplot(1,2,1)\nplt.title(\"Sentiment by day\", fontsize = 15)\nsns.lineplot(data = FNB_df.groupby(FNB_df.index.day_name())[\"polarity\"].mean(), color = \"c\", label = \"FNB\", sort=False)\nsns.lineplot(data = Nedbank_df.groupby(Nedbank_df.index.day_name())[\"polarity\"].mean(), color = \"g\", label = \"Nedbank\", sort=False)\nsns.lineplot(data = ABSA_df.groupby(ABSA_df.index.day_name())[\"polarity\"].mean(), color = \"r\", label = \"ABSA\", sort=False)\nsns.lineplot(data = Standard_df.groupby(Standard_df.index.day_name())[\"polarity\"].mean(), color = \"b\", label = \"StdBank\", sort=False)\nsns.lineplot(data = Cap_df.groupby(Cap_df.index.day_name())[\"polarity\"].mean(), color = \"grey\", label = \"Capitec\", sort=False)\n\nplt.subplot(1,2,2)\nplt.title(\"Sentiment by hour\", fontsize = 15)\nsns.lineplot(data = FNB_df.groupby(FNB_df.index.hour)[\"polarity\"].mean(), color = \"c\", label = \"FNB\")\nsns.lineplot(data = Nedbank_df.groupby(Nedbank_df.index.hour)[\"polarity\"].mean(), color = \"g\", label = \"Nedbank\")\nsns.lineplot(data = ABSA_df.groupby(ABSA_df.index.hour)[\"polarity\"].mean(), color = \"r\", label = \"ABSA\")\nsns.lineplot(data = Standard_df.groupby(Standard_df.index.hour)[\"polarity\"].mean(), color = \"b\", label = \"StdBank\")\nsns.lineplot(data = Cap_df.groupby(Cap_df.index.hour)[\"polarity\"].mean(), color = \"grey\", label = \"Capitec\")\n\nplt.show()","8101a237":"#Std Bank\nStandard_df['mean'] = Standard_df['polarity'].expanding().mean()\nStandard_df['rolling'] = Standard_df['polarity'].rolling(\"7d\").mean()\n\n#FNB\nFNB_df['mean'] = FNB_df['polarity'].expanding().mean()\nFNB_df['rolling'] = FNB_df['polarity'].rolling(\"7d\").mean()\n\n#Nebank\nNedbank_df['mean'] = Nedbank_df['polarity'].expanding().mean()\nNedbank_df['rolling'] = Nedbank_df['polarity'].rolling(\"7d\").mean()\n\n#ABSA\nABSA_df['mean'] = ABSA_df['polarity'].expanding().mean()\nABSA_df['rolling'] = ABSA_df['polarity'].rolling(\"7d\").mean()\n\n#Total\nCap_df['mean'] = Cap_df['polarity'].expanding().mean()\nCap_df['rolling'] = Cap_df['polarity'].rolling(\"7d\").mean()","d88831ff":"fig = go.Figure()\nfig.add_scatter(x=FNB_df.index, y=FNB_df[\"rolling\"], name=\"FNB\", mode='lines',line_color=\"#19D3F3\")\nfig.add_scatter(x=Standard_df.index, y=Standard_df[\"rolling\"], name=\"Standard Bank\", mode='lines',line_color=\"blue\")\nfig.add_scatter(x=ABSA_df.index, y=ABSA_df[\"rolling\"], name=\"ABSA\", mode='lines',line_color=\"red\")\nfig.add_scatter(x=Nedbank_df.index, y=Nedbank_df[\"rolling\"], name=\"Nedbank\", mode='lines',line_color=\"green\")\nfig.add_scatter(x=Cap_df.index, y=Cap_df[\"rolling\"], name=\"Capitec\", mode='lines',line_color=\"grey\")\nfig.update_layout(\n    template = \"seaborn\",\n    title=\"Rolling 7 day Sentiment (polarity)\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"7 day rolling polarity\",\n    yaxis_range = [-0.1,0.4],\n    legend_title=\"Banks\",\n    font=dict(size=12),\n    autosize=False,\n    width=1000,\n    height=600,\n    margin=dict(l=10,r=10, b=50,t=50, pad=4)\n)","5569df8a":"# functions to create our graph\ndef trace_rolling_creation(df,gname, glinecolor):\n    return fig.add_trace(\n        go.Scatter(\n            x= df.index, \n            y=df[\"rolling\"], \n            name=gname,  \n            mode='lines',\n            line_color=glinecolor),\n        secondary_y=False\n)\n\ndef trace_count_creation(df,gname, glinecolor):\n    return fig.add_trace(\n        go.Scatter(\n            x= df.index, \n            y=df[\"polarity\"].rolling('7d').count(), \n            name=gname,  \n            fill='tozeroy',line_color=glinecolor), \n        secondary_y=True\n)","4d572afd":"# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# Add traces\ntrace_rolling_creation(ABSA_df, \"ABSA\", '#DC0E1A')\ntrace_rolling_creation(Nedbank_df, \"Nedbank\", '#078a4d')\ntrace_rolling_creation(Standard_df, \"StdBank\", '#054db3')\ntrace_rolling_creation(FNB_df, \"FNB\", '#19D3F3')\ntrace_rolling_creation(Cap_df, \"Capitec\", '#808080')\n\ntrace_count_creation(ABSA_df, \"ABSA\", 'rgb(220, 14, 26)')\ntrace_count_creation(Nedbank_df, \"NedBank\", 'rgb(7, 138, 77)')\ntrace_count_creation(Standard_df, \"Std Bank\", 'rgb(5, 77, 179)')\ntrace_count_creation(FNB_df, \"FNB\", 'rgb(25, 211, 243)')\ntrace_count_creation(Cap_df, \"Capitec\", 'rgb(128,128,128)')\n\n# set figure layout\nfig.update_layout(\n    template = \"seaborn\",\n    title_text=\"Rolling 7d Sentiment vs Count of tweets\",\n    legend_title=\"Banks\",\n    font=dict(size=12),\n    autosize=False,\n    width=1000,\n    height=600,\n    margin=dict (l=10,r=10,b=50,t=50, pad=2)\n)\n\n# Set x-axis title\nfig.update_xaxes(title_text=\"Date\")\n\n# Set y-axes titles\nfig.update_yaxes(title_text=\"Rolling\",range = [-0.1,0.4], secondary_y=False)\nfig.update_yaxes(title_text=\"Count\",range = [0,40000], secondary_y=True)\n\nfig.show()","dc100e00":"# Hashtag analysis ","fede142d":"### Create rolling Mean \/ Expanding ","92f10600":"#### Create an interactive plot","6100d7e2":"# Prediction\nUse pipe model and obtain sentiment of cleaned tweets. Model does the below to tweet\n* TFIDFvectoriser \n* Trained XGBoost prediction","d02606c6":"#### Create seperate dataframes\nCreate a dataframe for each bank and merge them to create a full list of tweets per bank","40d6ecde":"### Reset index & drop Duplicates","35db2cb6":"### Set Base and cleaned tweet to string","636a5070":"### Bank Indicator\n***Note*** Some tweets can reference multiple banks i.e. \"Standard bank is the best Capitec is the worst\"\n* We need to look at each tweet and identify all banks the tweet references\n* For visualisation we will need to duplicate each tweet for each bank it references ","e64c3e49":"### Remove unnecessary rows \n* Remove tweets from Bank owned accounts i.e. FNBSA\n* Remove duplicates where tweet, bank and date are the same \n* Reindex dataframe","42a2c30d":"#### Language removal \n\nAlthough the language tag doesnt seem to get it right 100% of the time, we will drop these rows that arent english but keep undefined:\n* und = undefined --- this will also include tweets with only hashtags so we will keep this\n* en = english ","0bb7aff8":"## Analysis and visualisation","07c6bd27":"# Sentiment overtime comparison ","34de5404":"# Word Cloud","c02a4017":"![Twitter_bank_image.PNG](attachment:c89e00f6-c557-432c-bbf3-2dbbbb7a802e.PNG)","c09769ae":"# Obtain data and model\n* Data is obtained using Twint on AWS EC2 server, process can be found on my [github](https:\/\/github.com\/Slyth3\/Sentiment-Analysis-RSA-Banks-AWS)\n* Model is a [pretrained pipe model](https:\/\/www.kaggle.com\/slythe\/twitter-sentiment-analysis-custom-model) on labelled tweets \n   - tfidfvectorizer\n   - XGBoost","0e3ad049":"# Rolling plots","c137a166":"#### Set Date as index for Datetime Visuals","cfc13ac5":"# Preprocessing","781d6e69":"# Sentiment Analysis of South Africa's Top 5 Financial Insitutions \nIdentify and explore the sentiment of tweets referencing the top 5 banks in South Africa, namely:\n1. Standard Bank \n1. First National Bank (FNB) \n1. Capitec Bnak \n1. Nedbank\n1. ABSA Bank \n\n\n### Cleaning \n* Data contains base tweets (raw) and cleaned tweet. Cleaning process can be found in POC [notebook](https:\/\/www.kaggle.com\/slythe\/sentiment-analysis-with-twint-textblob-poc) \n\n### Current process \n* Apply [custom created pipeline (tfidf,xgboost) model](https:\/\/www.kaggle.com\/slythe\/twitter-sentiment-analysis-custom-model\/) to predict Tweet Sentiment.\n* Explore sentiment results using matplotlib and Seaborn\n\n[Data](..\/input\/twitter-scrape-of-the-top-5-banks-in-south-africa) obtained by Twint Scraping (POC notebook can be found [here](https:\/\/www.kaggle.com\/slythe\/sentiment-analysis-with-twint-textblob-poc) )\n\n### Project 2:\nCompare results to the Customer Satifaction Index (CSI) and determine if the CSI is a correct reflection of the consumer sentiment","b28ee4e5":"Determine the weighted prediction probability \\\ni.e. create a probability within range of -1 -> 1"}}