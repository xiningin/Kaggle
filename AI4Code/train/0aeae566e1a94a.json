{"cell_type":{"f3799ed7":"code","6bbbe79c":"code","0086f817":"code","74ce9620":"code","3668e700":"code","9801203d":"code","c2b9cb40":"code","c6ddcbc9":"code","66aac62e":"code","d432d9e8":"code","5bfbac5a":"code","6dbbdfd0":"code","05212bb8":"code","05019343":"code","34092933":"code","30ff2747":"code","d91e9c20":"code","6936adab":"code","90330329":"code","be2367d4":"code","264aa142":"code","451a7861":"code","fcc4b138":"code","8eac70a7":"code","05d8cb1d":"code","bbaedb34":"code","1bafb1f0":"code","f353ce83":"code","2df9a5c0":"code","d1e84609":"code","a11ce8aa":"code","c0404e39":"code","139b1e90":"code","b822d677":"code","620fd386":"code","cf95f666":"code","b2725a75":"code","549d514a":"code","b6e56fca":"code","914c637a":"code","13698111":"code","413116db":"code","84291ee7":"code","9ace42f0":"code","e6ff31b5":"code","8e15220a":"code","873788bd":"code","b39c13ad":"code","786d8555":"code","0e4e7a75":"code","58cd0ec8":"code","9dd19514":"code","ede12998":"code","0148415d":"code","e9be46bf":"code","eb051e30":"code","0ecda690":"code","7173468d":"code","eb0bd09a":"code","6e6ce40c":"code","3da11ddf":"code","36adfc49":"code","d4046e1a":"code","9096cee8":"code","cca16c79":"code","137d98fa":"code","cd53128d":"code","fa2e529a":"code","44ad7a0d":"code","1b5c59f3":"code","f1d50edf":"code","06cf18f7":"code","bc7def0a":"code","425291ff":"code","73310d5e":"code","62f65d03":"code","bf5d4050":"code","5b2eb21d":"code","f59b0749":"code","1b2ef047":"code","19003112":"code","21eeddae":"markdown","df086932":"markdown","6adb8f22":"markdown","f0521b6b":"markdown","3bdc2f62":"markdown","d6428590":"markdown","8a8fcbd2":"markdown","ec2aca62":"markdown","ae9776be":"markdown","cc711007":"markdown","410a462b":"markdown","0c2f6afa":"markdown","608ccb10":"markdown","eb1c2f25":"markdown","2ed9f75c":"markdown","29b66769":"markdown","540c2473":"markdown","b93316ea":"markdown","7af21818":"markdown","7445f64d":"markdown","3f9c8aab":"markdown","e7fccff1":"markdown","b015fb25":"markdown","2458a59d":"markdown","0b02d467":"markdown","c6e46b71":"markdown","9f24dcc0":"markdown","c31f4b64":"markdown","47343b4f":"markdown","01ec3558":"markdown","8b971541":"markdown","046b4c1f":"markdown","fb45cce3":"markdown","479fc6ad":"markdown","bcc20295":"markdown","9e230995":"markdown","22cc999b":"markdown","6c72ee38":"markdown"},"source":{"f3799ed7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6bbbe79c":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt \nimport os\nimport plotly_express as px\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.patches as mpatches\nfrom wordcloud import WordCloud\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.model_selection import GridSearchCV\nfrom nltk.tokenize import TweetTokenizer , word_tokenize\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , GRU,Input,TimeDistributed , Embedding,Bidirectional,\\\nBatchNormalization,Dropout,LSTM ,GlobalAveragePooling1D\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras.losses import sparse_categorical_crossentropy ,categorical_crossentropy\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import cross_val_score ,KFold\nimport numpy as np \nfrom sklearn.metrics import roc_auc_score, accuracy_score ,auc , roc_curve","0086f817":"root=\"\/kaggle\/input\/nlp-getting-started\/\"\ntrain_df=pd.read_csv(os.path.join(root,\"train.csv\"))\ntest_df=pd.read_csv(os.path.join(root,\"test.csv\"))\ndf_sub=pd.read_csv(os.path.join(root,\"sample_submission.csv\"))","74ce9620":"# Take a look at the datas \ntrain_df.head()","3668e700":"# info about training datas\ntrain_df.info()","9801203d":"# info about test datas\ntest_df.info()","c2b9cb40":"#Some statistics on the datas.\ntrain_df.describe()","c6ddcbc9":"fig=px.pie(train_df,names=\"target\",title=\"Target Disributions\")\nfig.update_layout(title={\"x\":0.475,\"y\":0.9,\"xanchor\":\"center\",\"yanchor\":\"top\"})\nfig.show()","66aac62e":"nb=len(train_df['location'].unique())\nprint(\"Tweets come from {} differents locations \".format(nb))","d432d9e8":"top_20=pd.DataFrame(train_df['location'].value_counts()[:20]).reset_index(drop=False)\ntop_20.rename(columns={'index':\"Locations\",\"location\":\"Occurences\"},inplace=True)","5bfbac5a":"fig=px.bar(top_20,x=\"Locations\",y=\"Occurences\",orientation='v',color=\"Occurences\",title=\\\n          \"Top 20 most used Countries \")\nfig.update_layout(title={\"x\":0.475,\"y\":0.9,\"xanchor\":\"center\",\"yanchor\":\"top\"})\nfig.show()","6dbbdfd0":"df=train_df[train_df['location'].notna()]","05212bb8":"lb=LabelEncoder()\ndf['location']=lb.fit_transform(df['location'])","05019343":"X=df['location']\nY=df['target']\n\n#T_sne implementation\n\nX_reduced=TSNE(n_components=2,random_state=42).fit_transform(X.values.reshape(-1,1))","34092933":"f,ax=plt.subplots(1,1,figsize=(12,8))\nblue_patch = mpatches.Patch(color='#0A0AFF' ,label='Not Fake')\nred_patch = mpatches.Patch(color='#AF0000' ,label='Fake')\n\nplt.scatter(X_reduced[:,0],X_reduced[:,1],c=(Y==1),label=[\"Not Fake\"],cmap=\"coolwarm\"\\\n            )\nplt.scatter(X_reduced[:,0],X_reduced[:,1],c=(Y==0),label=[\"Fake\"],cmap=\"coolwarm\"\\\n           )\nplt.grid(True)\nplt.legend(handles=[blue_patch,red_patch])\nf.suptitle('Clusters using Dimmensionality Reduction',fontsize=14)\nplt.title(\"t-SNE\")","30ff2747":"# most used word for disaster tweet\ndf=train_df[train_df['target']==1]\ntop_20=pd.DataFrame(df['keyword'].value_counts()[:20]).reset_index(drop=False)\ntop_20.columns=['keywords','occurences']","d91e9c20":"fig=px.bar(top_20,x=\"keywords\",y=\"occurences\",color='occurences',title=\\\n          \"Top 20 most used keyword for disaster tweets\")\nfig.update_layout(title={\"x\":0.475,\"y\":0.9,\"xanchor\":\"center\",\"yanchor\":\\\n                        \"top\"})\nfig.show()","6936adab":"df=train_df[train_df[\"keyword\"].notna()]","90330329":"le=LabelEncoder()\ndf['keyword']=le.fit_transform(df['keyword'])","be2367d4":"X=df['keyword']\nY=df['target']\n#T-SNE implementation\nX_reduced=TSNE(n_components=2,random_state=41).fit_transform(\\\n                                                             X.values.reshape(-1,1))","264aa142":"f,ax=plt.subplots(1,1,figsize=(12,8))\nblue_patch=mpatches.Patch(color='#0A0AFF',label='Not Fake')\nred_patch=mpatches.Patch(color='#AF0000',label='Fake')\n\nf.suptitle(\"Clusters using Dimensionnality Reduction \")\n\nax.scatter(X_reduced[:,0],X_reduced[:,1],c=(Y==1),label=\"Not Fake\",\\\n          cmap=\"coolwarm\")\nax.scatter(X_reduced[:,0],X_reduced[:,1],c=(Y==0),label=\"Kake\",\\\n          cmap=\"coolwarm\")\nax.grid(True)\nax.legend(handles=[blue_patch,red_patch])\nplt.title(\"T-SNE\")","451a7861":"tx=\" \".join(df['text'].values.tolist())","fcc4b138":"fig,ax1=plt.subplots(1,1,figsize=(12,16))\nwordcl=WordCloud().generate(tx)\nplt.imshow(wordcl,interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Most used words for real disaster tweets\",fontsize=16)\nplt.show()","8eac70a7":"tx=train_df['text'].values.tolist()","05d8cb1d":"count_vectoriser=CountVectorizer(stop_words=\"english\",max_df=0.95,min_df=2)\ntf=count_vectoriser.fit_transform(tx)","bbaedb34":"#GridSearch to choice the number of topics to extract\nparams={'n_components':[5,10,20,50]}\nlda=LatentDirichletAllocation()\nmodel=GridSearchCV(lda,params)\nmodel.fit(tf)","1bafb1f0":"print(\"Best Model's Params :{}\".format(model.best_params_))","f353ce83":"model=model.best_estimator_","2df9a5c0":"nb_topics=5\ndef display_topic(md,nb_topics,features,no_words):\n    for topic_idx ,topic in enumerate(md.components_):\n        print(\"Topic {} :\".format(topic_idx))\n        print(\" \".join(features[j] for j in topic.argsort()[-no_words-1:-1]))\n        ","d1e84609":"# Display the most 5 represented topic in our corpus\nfeatures=count_vectoriser.get_feature_names()\nno_words=10\ndisplay_topic(model,nb_topics,features,no_words)","a11ce8aa":"def tokenizer(x):\n    token = TweetTokenizer().tokenize(x)\n    return token\ndef stem(x):\n    stemmer = PorterStemmer()\n    res = [stemmer.stem(elt) for elt in x]\n    return res ","c0404e39":"# Compute the frequency apparition of each word in our corpus\nstop_words = Counter([x for row in train_df['text'].values for x in row.split(\" \")])","139b1e90":"# We consider the 100 most used words as common words.\ncommon_words = list(zip(*stop_words.most_common(100)))[0]","b822d677":"common_words = set(common_words)","620fd386":"common_words.update(nltk.corpus.stopwords.words(\"english\"))","cf95f666":"training=train_df['text'].values.tolist()\ntest=test_df['text'].values.tolist()","b2725a75":"countvectorizer=TfidfVectorizer(tokenizer=tokenizer,\\\n                               max_df=0.95,min_df=2)","549d514a":"X_tr=countvectorizer.fit_transform(training)\nX_pr=countvectorizer.fit_transform(test)","b6e56fca":"Y_tr=train_df[\"target\"].values","914c637a":"xtr,xts,ytr,yts=train_test_split(X_tr,Y_tr,test_size=0.2,stratify=Y_tr)","13698111":"#class_weights=class_weight.compute_class_weight(\"balanced\",\\\n                                                #np.unique(Y_tr),Y_tr)","413116db":"estimator=MultinomialNB()\nestimator.fit(xtr,ytr)","84291ee7":"# estimate performance accuracy\nsc=estimator.score(xts,yts)\nprint(\"The score accuracy of the Naive bayes estimator is : {}\".\\\n      format(sc))","9ace42f0":"def preprocessing(x):\n    \"\"\"\n    - Lowercase the sentence. \n    - Change 't to 'not'.\n    - Remove @name\n    - Remove all stop words unless 'can' and 'not'.\n    - Remove all punctuation and special charecter except '!'\n    - Remove trailing white space.\n    \"\"\"\n    x = x.lower()\n    x = re.sub(r\"(@.?)[\\s]\",\" \",x)\n    x = re.sub(r\"\\'t\",\"not\",x)\n    x= \" \".join([word for word in x.split(\" \") if word not in common_words or word in (\"can\",\"not\")])\n    x= re.sub(r\"([\\?\\\\\\,\\:\\;\\.\\'\\\"\\(\\)])\",\" \",x)\n    x= re.sub(r\"\\s+\",\" \",x).strip()\n    return x\n    ","e6ff31b5":"# Apply our preprocessing treatment to our training datas.\ntraining = [preprocessing(text) for text in train_df['text'].values]","8e15220a":"# Define TF_IDF trasnformer with the new subtility to take en consideration ngram from 1 to 3.\ntf_idf = TfidfVectorizer(max_df=0.95,min_df=2)","873788bd":"X = tf_idf.fit_transform(train_df['text'].values)","b39c13ad":"xtr,xts,ytr,yts = train_test_split(X,Y_tr,test_size=0.2,stratify=Y_tr)","786d8555":"def get_auc_cv(model):\n    kf = KFold(5,shuffle=True,)\n    auc = cross_val_score(model,xtr,ytr,cv=kf,scoring=\"roc_auc\")\n    \n    return auc.mean()","0e4e7a75":"res = pd.Series([get_auc_cv(MultinomialNB(i)) for i in np.arange(1,10,0.1)],index=np.arange(1,10,0.1))","58cd0ec8":"best_alpha = res.idxmax()\nprint(\"Best_alpha : {}\".format(best_alpha))\n\nplt.plot(res)\nplt.title(\"AUC vs.Alpha\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"AUC\")\nplt.show()","9dd19514":"def evaluate_roc(probs,y_true):\n    \"\"\" \n    - Print AUC and accuracy score\n    - plot Roc curve.\n    @params: probs(np.array):an array of predicted probabilities with shape (len(y_true,2)).\n    @params: y_true : an array of the true value with shape (len(y_true),)\n    \"\"\"\n    prob = probs[:,1]\n    fpr,tpr,threshold = roc_curve(y_true,prob)\n    AUC = auc(fpr,tpr)\n    print(\"AUC:{}\".format(AUC))\n    \n    # Get the accuracy score.\n    ypred = np.where(prob>0.5,1,0)\n    accuracy = accuracy_score(y_true,ypred)\n    print(\"The accuracy score: {}\".format(accuracy))\n    \n    # Plot the roc curve \n    plt.plot(fpr,tpr,'b',label='AUC')\n    plt.plot([0,1],[0,1],'r--')\n    plt.xlim([0,1])\n    plt.ylim([0,1])\n    plt.xlabel(\"False positive rate\")\n    plt.ylabel(\"True positive rate\")\n    plt.legend(loc=\"best\")\n    plt.title(\"Receiver Operating Characteristic\")\n    plt.show()","ede12998":"model = MultinomialNB(alpha=best_alpha) # define Naives bayes with the best hyperparameter.\nmodel.fit(xtr,ytr)\nprobs = model.predict_proba(xts)","0148415d":"evaluate_roc(probs,yts)","e9be46bf":"# Mandatory functions to prepare datas\nvoc_size = 20000\nmax_len=100\ndef tokenize(x):\n    tk=Tokenizer(char_level=False)\n    tk.fit_on_texts(x)\n    return tk.texts_to_sequences(x),tk\ndef pad(x,max_len=None):\n    if max_len is None:\n        l=[len(s) for s in x]\n        max_len=max(l)\n    return pad_sequences(x,maxlen=max_len,padding='pre')\n       \ndef preprocessing(x):\n    seq_x,tk=tokenize(x)\n    return  pad(seq_x),tk","eb051e30":"# Define the model\ndef model(input_shape,size,emb_size):\n    lr=1e-3\n    md= Sequential()\n    md.add(Embedding(size,emb_size,input_length=input_shape[1]))\n    md.add(BatchNormalization())\n    md.add(Bidirectional(GRU(50,return_sequences=True),\\\n                         input_shape = (emb_size,1)))\n    \n    md.add(BatchNormalization())\n    md.add(Dropout(0.3))\n    md.add(TimeDistributed(Dense(50,activation=\"softmax\")))\n    md.add(BatchNormalization())\n    md.add(Dropout(0.1))\n    md.add(Dense(1,activation=\"sigmoid\"))\n    md.compile(loss=sparse_categorical_crossentropy,optimizer=Adam(lr),metrics=[\"accuracy\"])\n    return md","0ecda690":"#Prepare datas\ninp=train_df['text'].tolist()\nX,tk=preprocessing(inp)","7173468d":"#Reshape datas\nX=X.reshape(-1,X.shape[1],1)\ninput_shape=X.shape","eb0bd09a":"#Corpus size\nsize=len(tk.word_index)","6e6ce40c":"#Build Model\nmodel= model(input_shape,size+1,50)","3da11ddf":"Y=train_df[\"target\"].values","36adfc49":"model.fit(X,Y,batch_size=32,shuffle=True,validation_split=0.1,epochs=10)","d4046e1a":"!pip install GPUtil","9096cee8":"from transformers import BertForSequenceClassification,BertTokenizer,AdamW,\\\nget_linear_schedule_with_warmup,BertModel\nimport torch \nfrom torch.utils.data import DataLoader,TensorDataset,SequentialSampler\nfrom GPUtil import showUtilization as gpu_usage\nfrom numba import cuda\n","cca16c79":"def free_gpu_cache():\n    print(\"Initial GPU Usage\")\n    gpu_usage()                             \n\n    torch.cuda.empty_cache()\n\n    cuda.select_device(0)\n    cuda.close()\n    cuda.select_device(0)\n\n    print(\"GPU Usage after emptying the cache\")\n    gpu_usage()","137d98fa":"def preprocessing(text):\n    \"\"\" \n    - Remove entity mention @\n    - Remove some special characters (exple: #)\n    - Replace 't by not.\n    \n    @param text(str) : an introduced text to be processed.\n    @param text(str) : a processed text.\n    \"\"\"\n    # Remove '@'\n    text = re.sub(r\"(@.*?)[\\s]\",\" \",text)\n    # Remove '#'\n    text = re.sub(r\"(#.*?)[\\s]\",\" \",text)\n    # Remove trailing whitespace\n    text = re.sub(r\"\\s+\",\" \",text).strip()\n    \n    return text","cd53128d":"# Load the Bert Tokenizer \ntokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=True)","fa2e529a":"def preprocessing_for_bert(data,Max_len):\n    \"\"\"Perform required preprocessing step for pretrained BERT.\n    @param data(np.array) : Array of texts to be processed.\n    \n    @ return inp_ids(torch.Tensor): Tensor of token ids to be fed to the model.\n    @ return mask_attention(torch.Tensor): Tensor of indices specifying which tokens \n                                           should be attended by the model\n    \n    \"\"\"\n    \n    # Create empty lists to store outputs.\n    \n    inp_ids = []\n    mask_attention = []\n    \n    for sent in data :\n        \n        encoded_sent = tokenizer.encode_plus(text=preprocessing(sent),\n                                            add_special_tokens=True,\n                                            padding='max_length',\n                                            truncation=True,\n                                            max_length=Max_len)\n        inp_ids.append(encoded_sent.get('input_ids'))\n        mask_attention.append(encoded_sent.get('attention_mask'))\n    \n    # convert list to tensors\n    inp_ids = torch.tensor(inp_ids)\n    mask_attention = torch.tensor(mask_attention)  \n    \n    return inp_ids,mask_attention","44ad7a0d":"#Concatenate train and test data.\ndata = np.concatenate([train_df[\"text\"],test_df[\"text\"]],axis=0)\n#Encode our concatenated data.\nencoded_tweets = [tokenizer.encode(sent,add_special_tokens=True) for sent in data]\n# Find the maximum length.\nMax_len = max([len(sent) for sent in encoded_tweets])\nprint('Max_length: ',Max_len)","1b5c59f3":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"There are {} GPU(s) available\".format(torch.cuda.device_count()))\n    print(\"Device name:\",torch.cuda.get_device_name(0))\nelse:\n    print(\"No GPU available,using CPU instead\")\n    device = torch.device(\"cpu\")","f1d50edf":"import torch.nn as nn \n\nclass BertClassifier(nn.Module):\n    \"\"\" Bert Model for classification Tasks.\"\"\"\n    def __init__(self,freeze_bert=False):\n        super(BertClassifier,self).__init__()\n    \n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n    \n        #Specify hidden size of bert, hidden size of our classifier,and numbers of labels.\n        D_in, H ,D_out = 768,50,2\n    \n        self.classifier = nn.Sequential(nn.Linear(D_in,H),\n                                   nn.ReLU(),\n                                   nn.Dropout(0.3),\n                                   nn.Linear(H,D_out))\n    \n        if freeze_bert :\n           for param in self.bert.parameters():\n              param.requires_grad = False \n    def forward(self,input_id,attention_mask):\n        \"\"\" Feed input to Bert and the classifier to compute output.\n            \n            @param input_id(torch tensor): an input tensor with shape (batch_size,Max_len)\n            @param attention_mask (torch tensor) : an input tensor with shape (batch_size,Max_len)\n            @param output (torch tensor): an output tensor with shape (batch_size,num_labels)\n        \n        \"\"\"\n        \n        hidden_states=self.bert(input_id,attention_mask)\n        hidden_states = hidden_states[0][:,0,:]\n        \n        output = self.classifier(hidden_states)\n        \n        return output\n    \n    ","06cf18f7":"def initialize_model(traindataloader,epochs=4):\n    \"\"\" Initialize the Bert Model, the optimizer and the learning rate scheduler.\"\"\"\n    # Instantiate Bert classifier.\n    bertmodel = BertClassifier(freeze_bert=False)\n    if torch.cuda.device_count() > 1:\n        bertmodel = nn.DataParallel(bertmodel)\n    bertmodel.to(device)\n    #Create the optimizer\n    optimizer = AdamW(bertmodel.parameters(),\n                     lr=5e-5, # default value\n                     eps=1e-8) # default value\n    # Total number of training steps\n    total_steps = len(traindataloader)*epochs\n    \n    # Set up the learning rate Scheduler.\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_training_steps=total_steps,\n                                               num_warmup_steps=0)\n    \n    return bertmodel,optimizer,scheduler","bc7def0a":"x_tr,x_ts,y_tr,y_ts = train_test_split(training,Y_tr,test_size=0.2,stratify=Y_tr)","425291ff":"# compute inputs_ids and mask_attention tensors for training datas.\ntr_ids,tr_mask = preprocessing_for_bert(x_tr,Max_len)\n# compute inputs_ids and mask_attention tensors for test datas.\nval_ids,val_mask = preprocessing_for_bert(x_ts,Max_len)\n# Convert train label vector to torch tensor.\ntr_labels = torch.tensor(y_tr)\nval_labels = torch.tensor(y_ts)\n\n# For fine tunning bert , the authors recommend a batch size of 16 or 32.\nbatch_size =32\n\n#Create the Dataloader for our training set.\ntrain_data = TensorDataset(tr_ids,tr_mask,tr_labels)\ntr_sampler = SequentialSampler(train_data)\ntr_dataloader = DataLoader(train_data,sampler=tr_sampler,batch_size=batch_size)\n\n#Create the Dataloader for our validation set.\nval_data = TensorDataset(val_ids,val_mask,val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data,sampler=val_sampler,batch_size=batch_size)","73310d5e":"import time \n#define Loss function\nloss_fn= nn.CrossEntropyLoss()\n\ndef train(model,optim,scheduler,tr_data,val_data,evaluation=True,epochs=4):\n    \"\"\" Training the Bert Classifier\"\"\"\n    \n    #Start training loop.\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        #==============================================\n        #                 Training\n        #==============================================\n        \n        model.train()\n        \n        # Mesure the elapsed time of each epoch.\n        t0_epoch,t0_batch = time.time(),time.time()\n        # Iinitialize valiation tracked values.\n        val_loss,val_accuracy = None,None\n        # Reset tracking variables at the beginning of each epoch.\n        loss_batch,loss_total, batch_counts= 0,0,0\n        for i,data in enumerate(tr_data):\n            batch_counts +=1\n            #Load input datas to device\n            inp,mask,lab = tuple(t.to(device) for t in data)\n            \n            #Zero out any previously calculated gradients.\n            model.zero_grad()\n            #Perform a forward pass.This will return logits.\n            output = model(inp,mask)\n            # Compute loss and accumulate the loss values. \n            loss = loss_fn(output,lab)\n            loss_batch += loss.item()\n            loss_total += loss.item()\n            \n            #Perform a backward pass to calculate gradients.\n            loss.backward()\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            \n            # update weights and the learning rate \n            optim.step() \n            scheduler.step()\n            optim.zero_grad()\n            \n            # print the loss values and the elapsed time for each 20 steps.\n                                           \n            if (i % 20 ==0 and i!=0) or (i == len(tr_data)-1):\n                print(\"Epochs : {} | Batch :{} | Batch_loss :{} | Total_loss: - | Val_loss : - | Val_Accuracy: - | elapsed_time : {}\".format(epoch_i,(i+1)\/\/20,\\\n                                                                         loss_batch\/batch_counts,time.time()-t0_batch))\n                \n                #Reset tracking variables values on the end of each batch.\n                t0_batch = time.time()\n                batch_counts = 0\n                loss_batch = 0\n            del(inp)\n            del(mask)\n            del(lab)\n            del(output)\n        print(\"-\"*120)\n        if evaluation :\n           val_loss,val_accuracy = validation(model,val_data)\n        # print the loss values and the elapsed time for each epoch.\n        print(\"Epochs : {} | Batch : - | Batch_loss : - | Toal_loss : {} | Val_loss : {} | Val_Accuracy : {} | elapsed_time : {}\".format(epoch_i,loss_total\/len(tr_data),val_loss,val_accuracy,time.time()-\\\n                                                 t0_epoch))\n        print(\"-\"*120)\n        # Reset tracking variables values at the end of each epoch.\n        t0_epoch = 0\n        loss_total = 0\n    print(\"Training complete!\")\ndef validation(model,val_data):\n    \"\"\" Mesure the model'performances on our validation set.\"\"\"\n    \n    # Put the model into the evaluation mode.The dropout layers are disabled during\n    # validation test\n    model.eval()\n    val_loss = 0\n    val_accuracy = 0\n    for data in val_data : \n        # Load data to device                                  \n        inp , msk ,labels= (t.to(device) for t in data)\n        # Compute logits\n        with torch.no_grad():\n           out = model(inp,msk)\n        # Compute the loss values.                                   \n        loss = loss_fn(out,labels)\n        val_loss += loss.item()\n        \n        # Get the predictions.\n        _ ,predictions = torch.max(out,dim=1)\n                                           \n        # Calculate the accuracy score.\n        accuracy = torch.sum(predictions==labels)\n        val_accuracy += accuracy.item()\n       \n                                           \n    val_loss = val_loss \/ len(val_data)\n    accuracy = val_accuracy \/ len(y_ts)                                 \n    \n    return val_loss,accuracy\n        \n                                          \n            \n        ","62f65d03":"free_gpu_cache()","bf5d4050":"# Now , let's training our model.\nBertclassifier,optimizer,scheduler = initialize_model(tr_dataloader,epochs=2)","5b2eb21d":"train(Bertclassifier,optimizer,scheduler,tr_dataloader,val_dataloader,evaluation=True,epochs=2)","f59b0749":"from torch.nn.functional import softmax\ndef bert_predict_proba(model,val_dataloader):\n    \"\"\" Perform a forward pass on the introduced model to predict probabilities\"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during \n    # the test time.\n    model.eval()\n    \n    all_predictions = []\n    \n    for datas in val_dataloader:\n        # Load the datas to device.\n        inp_id,msk_att,lab = tuple(t.to(device) for t in datas)\n        \n        # Compute logits\n        with torch.no_grad():\n            output = model(inp_id,msk_att)\n        \n        all_predictions.append(output)\n        \n    # Concatenate logits from each batch.\n    all_logits = torch.cat(all_predictions,dim=0)\n    \n    # Apply softmax to calculate probabilities\n    probs = softmax(all_logits,dim=1).cpu().numpy()\n    \n    \n    return probs   \n    \n        \n            ","1b2ef047":"# Compute the predicted probabilities on the test set \nprobabilities = bert_predict_proba(Bertclassifier,val_dataloader)","19003112":"# Evaluate the Bertclassifier\nevaluate_roc(probabilities,y_ts)","21eeddae":"<h3> Tweets Location <\/h3>","df086932":"<h4> Create BertClassifier","6adb8f22":"#### Top 20 KeyWords","f0521b6b":"<h3> text analysis <\/h3>","3bdc2f62":"<h4>Tokenization and Input Formatting:","d6428590":"To fine-tunne our Bert Classifier, we need to create an optimizer.The authors recommend following hyper-parametrs:\n\n   <ul>\n    <li> batch_size : 16 or 32 <\/li>\n    <li> optimizer : Adam <\/li>\n    <li> Learning rate : 5e-5,3e-5,2e-5\n    <li> Number of epochs : 2,3\n   <\/ul>","8a8fcbd2":"<h2 id=\"#exploration\"> Datas explorations <\/h2>","ec2aca62":"<font color=redblue> <b>the above t-SNE clustering shows that fake and real tweets can come from any locations. So the location can not be an important features to classify the tweets. <\/font>","ae9776be":"<h3> Let's Ckeck Target Didtribution<\/h3>","cc711007":"<h2 id=\"Modeling\"> Modeling <\/h2>","410a462b":"<h2> Table of Content <\/h2>\n<ul> <li> <a href=\"#preperation\"> Preparation <\/a> <\/li>\n    <li> <a href=\"#importdata\"> Load the datas <\/a> <\/li>\n    <li> <a href=\"#look\"> First Look at the data<\/a> <\/li>\n    <li> <a href=\"#exploration\"> Datas explorations<\/a><\/li>\n    <li> <a href=\"#Modeling\"> Modeling <\/a> <\/li>\n    <\/ul>\n<hr>","0c2f6afa":"<h2 id=\"#look\">First Look at the data","608ccb10":"> Hereunder, we will perform some slight preprocessing on our text removing entity mentions and special character. The level of processing here is much less than in previous approach because BERT was trained with the entire sentence.","eb1c2f25":"<h3> Naives Bayes: Baseline<\/h3>","2ed9f75c":"<h4> Create Pytorch DataLoader","29b66769":"<h4> Set up GPU for training :","540c2473":"<h3> Naives Bayes: fine tunnned","b93316ea":"<h4> Optimizer & Learning rate Scheduler:","7af21818":"<font color=redblue> The Gread search to select the best hyperparameter for our model , had enhanced slightly the performance of our model. We should mention that neither the processing of datas as detailed in the function named preprocessing, nor the the n_gram with range 1 to 3, had positive effect on the performance. <\/font> ","7445f64d":"<h2 id=","3f9c8aab":"<h2 id=importdata>Load the datas<\/h2>","e7fccff1":"<h4> Training Loop:","b015fb25":"<h2 id=\"","2458a59d":"<h3> Top 20 Locations <\/h3>","0b02d467":"#### Check For Datas Imbalance","c6e46b71":"<h3> Text Modeling Topic <\/h3>","9f24dcc0":"<font color=red> <b> ! PLease upvote to carry on , if you like it. <\/font>","c31f4b64":"<h4> Import Required Librairies:","47343b4f":"Before tokenizing,we need to specify the maximum length of our sentences","01ec3558":"<font color=redblue> The RNN model don't give better result than naivebayes. Indeed, he barely gives result near to the result of the random estimator. Therefore, The RNN model don't adapted to such binair classification. <\/font>","8b971541":"<h3>RNN Model <\/h3>","046b4c1f":"<h4> Validation on Validation Set :","fb45cce3":"<h3> KeyWords <\/h3>","479fc6ad":"<h3> Fine-Tunning Bert","bcc20295":"In order to apply the pre-trained BERT, we must use the tokenizer provided by the library. This is because (1) the model has a specific, fixed vocabulary and (2) the BERT tokenizer has a particular way of handling out-of-vocabulary words.\n\nIn addition, we are required to add special tokens to the start and end of each sentence, pad & truncate all sentences to a single constant length, and explicitly specify what are padding tokens with the \"attention mask\".\n\nThe encode_plus method of BERT tokenizer will:\n\n(1) split our text into tokens,\n\n(2) add the special [CLS] and [SEP] tokens, and\n\n(3) convert these tokens into indexes of the tokenizer vocabulary,\n\n(4) pad or truncate sentences to max length, and\n\n(5) create attention mask.","9e230995":"<font color=redblue> The T-SNE clustering above, shows a distingushed clusters for real disaster tweets and fake disaster tweets, which confirm that there is distingushed keywords for each of these cluster.","22cc999b":"==> It obviously, that we should handle the missing values of the variables location and keyword.","6c72ee38":"<h2 id=\"preparation\">Preparation<\/h2>"}}