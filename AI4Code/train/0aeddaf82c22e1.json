{"cell_type":{"37209ea4":"code","978b3b86":"code","a727b0b2":"code","cce4d745":"code","f5c55d3d":"code","1754e148":"code","452b3163":"code","9ee178a8":"code","03d02bb6":"code","1e2f518b":"code","ee508309":"code","66887712":"code","4ae5094d":"code","de2d27aa":"code","f4be6712":"code","b0217c4e":"code","a533039a":"code","a5c7b811":"code","81febe6a":"code","55eef2e7":"code","e2a73cda":"code","36ff1d8b":"code","da66fea5":"code","a150c60e":"code","7374b775":"code","521a16c4":"code","bedf5661":"code","e70c1553":"code","38e34b2f":"code","a6b0f7c6":"code","bed8728f":"code","e399cd52":"code","508a8fa7":"code","f8c6f32f":"code","a3e6eb3d":"code","c23d73fa":"code","c22bff70":"code","1a937ced":"code","bbdd6564":"code","3d4ea2e1":"code","3425909a":"code","1ee2b895":"code","96500a25":"code","5ab61e2d":"code","1249ddbb":"code","807c696c":"code","7c49867d":"code","b39ff0f3":"code","48b80715":"code","4de55c28":"markdown","4f71a0f9":"markdown","0a7e84cc":"markdown","1a41fae5":"markdown","5be91102":"markdown"},"source":{"37209ea4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","978b3b86":"train_data=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain_data.head()","a727b0b2":"test_data=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest_data.head()","cce4d745":"sns.countplot(train_data['target'])\nplt.title('Count for Zeros:'+str(train_data.target.value_counts()[0])+'\\n'+\n         'Count for Ones:'+str(train_data.target.value_counts()[1]))\nplt.show()","f5c55d3d":"len_sent=[]\nfor i in range(len(train_data['text'])):\n    len_sent.append(len(train_data['text'][i].split(' ')))","1754e148":"plt.figure(figsize=(12,6))\nsns.countplot(len_sent)\nplt.xlabel(\"Word lengths:\")\nplt.ylabel('Counts:')\nplt.title('Train Data \\n Max length='+str(max(len_sent)))\nplt.show()","452b3163":"train_data.drop(['keyword','location'],axis=1,inplace=True)\ntest_data.drop(['keyword','location'],axis=1,inplace=True)","9ee178a8":"sent=''\nfor i in range(len(train_data)):\n    sent=sent+train_data['text'][i]","03d02bb6":"from wordcloud import WordCloud\nword_cloud2 = WordCloud(collocations = False, background_color = 'white').generate(sent)\nplt.figure(figsize=(10,10))\nplt.imshow(word_cloud2, interpolation='bilinear')\n\nplt.axis(\"off\")\n\nplt.show()","1e2f518b":"import nltk\nfrom nltk.corpus import stopwords\nimport re","ee508309":"stop_words=set(stopwords.words('english'))\n\ndef text_cleaner(text):\n    newString=text.lower()\n    #remove hyperlinks\n    newString=re.sub(r'(https|http)?:\\\/\\\/(\\w|\\.|\\\/|\\?|\\=|\\&|\\%)*\\b', '', newString)\n     #removing text inside ()\n    newString = re.sub(r'\\([^)]*\\)', '', newString)\n    #removing text inside []\n    newString = re.sub(r'\\{[^)]*\\}', '', newString)\n    #fetching alphabetic characters\n    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n    #removing stop words\n    tokens = [w for w in newString.split() if not w in stop_words] \n    long_words=[]\n    for i in tokens:\n        #removing short words\n        if len(i)>=4:                                                 \n            long_words.append(i)   \n    return (\" \".join(long_words)).strip()","66887712":"cleaned_text_train=[]\nfor i in train_data['text']:\n    cleaned_text_train.append(text_cleaner(i))","4ae5094d":"print(\"Before cleaning:\\n\")\nprint(train_data['text'][0]+\"\\n\")\nprint(\"After cleaning:\\n\")\nprint(cleaned_text_train[0])","de2d27aa":"sent_1=''\nfor i in range(len(cleaned_text_train)):\n    sent_1=sent_1+cleaned_text_train[i]\n    \nfrom wordcloud import WordCloud\nword_cloud2 = WordCloud(collocations = False, background_color = 'white').generate(sent_1)\nplt.figure(figsize=(10,10))\nplt.imshow(word_cloud2, interpolation='bilinear')\n\nplt.axis(\"off\")\n\nplt.show()    ","f4be6712":"len_0=[]\nfor i in range(len(cleaned_text_train)):\n    if len(cleaned_text_train[i])==0:\n        len_0.append(i)\nlen_0        ","b0217c4e":"cleaned_text_test=[]\nfor i in test_data['text']:\n    cleaned_text_test.append(text_cleaner(i))","a533039a":"len_0=[]\nfor i in range(len(cleaned_text_test)):\n    if len(cleaned_text_test[i])==0:\n        len_0.append(i)\nlen_0    ","a5c7b811":"len_sent_train=[]\nfor i in range(len(cleaned_text_train)):\n    len_sent_train.append(len(cleaned_text_train[i].split(' ')))","81febe6a":"plt.figure(figsize=(12,6))\nsns.countplot(len_sent_train)\nplt.xlabel(\"Word lengths:\")\nplt.ylabel('Counts:')\nplt.title('Train Data \\n Max length='+str(max(len_sent_train)))\nplt.show()","55eef2e7":"len_sent_test=[]\nfor i in range(len(cleaned_text_test)):\n    len_sent_test.append(len(cleaned_text_test[i].split(' ')))","e2a73cda":"plt.figure(figsize=(12,6))\nsns.countplot(len_sent_test)\nplt.xlabel(\"Word lengths:\")\nplt.ylabel('Counts:')\nplt.title('Train Data \\n Max length='+str(max(len_sent_test)))\nplt.show()","36ff1d8b":"from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val=train_test_split(cleaned_text_train,train_data['target'],test_size=0.3,random_state=40)\nprint(len(X_train),len(y_train))\nprint(len(X_val),len(y_val))","da66fea5":"max_len=20","a150c60e":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\ntokenizer=Tokenizer(oov_token='<OOV>')\ntokenizer.fit_on_texts(X_train)\nX_train=tokenizer.texts_to_sequences(X_train)\nX_val=tokenizer.texts_to_sequences(X_val)\nX_test=tokenizer.texts_to_sequences(cleaned_text_test)\nX_train=pad_sequences(X_train,maxlen=max_len,padding='post')\nX_val=pad_sequences(X_val,maxlen=max_len,padding='post')\nX_test=pad_sequences(X_test,maxlen=max_len,padding='post')","7374b775":"vocab=len(tokenizer.word_index)+1\nprint(\"Vocab Size\",vocab)","521a16c4":"from keras.utils.np_utils import to_categorical\ny_train=to_categorical(y_train,num_classes=2)\ny_val=to_categorical(y_val,num_classes=2)","bedf5661":"print(y_train.shape)\nprint(y_val.shape)","e70c1553":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding,GRU, LSTM, RNN\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport keras.backend as K\nK.clear_session()\n\nmodel=Sequential()\nmodel.add(Embedding(vocab,100,input_length=max_len,trainable=True,mask_zero=True))\nmodel.add(LSTM(300,dropout=0.1,recurrent_dropout=0.2))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(2,activation='softmax'))\nmodel.summary()","38e34b2f":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])","a6b0f7c6":"history=model.fit(x=np.array(X_train),y=np.array(y_train),batch_size=1200,epochs=30,\n          validation_data=(np.array(X_val),np.array(y_val)))","bed8728f":"plt.plot(history.history['val_loss'],'r',label='val_loss')\nplt.plot(history.history['loss'],'b',label='train_loss')\nplt.legend()","e399cd52":"plt.plot(history.history['val_acc'],'r',label='val_acc')\nplt.plot(history.history['acc'],'b',label='train_acc')\nplt.legend()","508a8fa7":"model2=Sequential()\nmodel2.add(Embedding(vocab,100,input_length=max_len,trainable=True,mask_zero=True))\nmodel2.add(GRU(300,dropout=0.1,recurrent_dropout=0.2))\nmodel2.add(Dense(64,activation='relu'))\nmodel2.add(Dense(2,activation='softmax'))\nmodel2.summary()","f8c6f32f":"model2.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])","a3e6eb3d":"history2=model2.fit(x=np.array(X_train),y=np.array(y_train),batch_size=1200,epochs=30,\n          validation_data=(np.array(X_val),np.array(y_val)))","c23d73fa":"plt.plot(history2.history['val_loss'],'r',label='val_loss')\nplt.plot(history2.history['loss'],'b',label='train_loss')\nplt.legend()","c22bff70":"plt.plot(history2.history['val_acc'],'r',label='val_acc')\nplt.plot(history2.history['acc'],'b',label='train_acc')\nplt.legend()","1a937ced":"model3=Sequential()\nmodel3.add(Embedding(vocab,100,input_length=max_len,trainable=True,mask_zero=True))\nmodel3.add(LSTM(300,dropout=0.1,recurrent_dropout=0.2))\nmodel3.add(Dense(64,activation='relu'))\nmodel3.add(Dense(2,activation='softmax'))\nmodel3.summary()","bbdd6564":"model3.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])","3d4ea2e1":"history3=model3.fit(x=np.array(X_train),y=np.array(y_train),batch_size=1200,epochs=30,\n          validation_data=(np.array(X_val),np.array(y_val)))","3425909a":"plt.plot(history3.history['val_loss'],'r',label='val_loss')\nplt.plot(history3.history['loss'],'b',label='train_loss')\nplt.legend()","1ee2b895":"plt.plot(history3.history['val_acc'],'r',label='val_acc')\nplt.plot(history3.history['acc'],'b',label='train_acc')\nplt.legend()","96500a25":"model4=Sequential()\nmodel4.add(Embedding(vocab,100,input_length=max_len,trainable=True,mask_zero=True))\nmodel4.add(LSTM(300,dropout=0.1,recurrent_dropout=0.2,return_sequences=True))\nmodel4.add(LSTM(100,dropout=0.1,recurrent_dropout=0.2,return_sequences=True))\nmodel4.add(LSTM(50,dropout=0.1,recurrent_dropout=0.2))\nmodel4.add(Dense(64,activation='relu'))\nmodel4.add(Dense(2,activation='softmax'))\nmodel4.summary()","5ab61e2d":"model4.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['acc'])\nhistory4=model4.fit(x=np.array(X_train),y=np.array(y_train),batch_size=120,epochs=30,\n          validation_data=(np.array(X_val),np.array(y_val)))","1249ddbb":"plt.plot(history4.history['val_loss'],'r',label='val_loss')\nplt.plot(history4.history['loss'],'b',label='train_loss')\nplt.legend()","807c696c":"plt.plot(history3.history['val_acc'],'r',label='val_acc')\nplt.plot(history3.history['acc'],'b',label='train_acc')\nplt.legend()","7c49867d":"predict=model4.predict(X_test)","b39ff0f3":"predict_final=[0 if i[0]>=0.5 else 1 for i in predict]\ntest_data['target']=predict_final","48b80715":"submission=test_data[['id','target']]\nsubmission.to_csv('Submission.csv',index=False)","4de55c28":"**Data Cleaning**\n\nSince there are a lot of words in the wordcloud that have no significant meaning so we will remove unnecessary words and other things from the corpus, allowing better training of the models. ","4f71a0f9":"**EDA**","0a7e84cc":"**Model Training**\n1. Splitting the dataset into train, validation data\n2. Tokeninzing the train,validation and test data\n3. Padding the tokens\n4. Creating four different models RNN, LSTM, GRU and Stacked LSTM","1a41fae5":"**As one can observe the wordcloud has a lot better corpus with words having meaning. Thus the cleaning part was successful.**","5be91102":"**Loading Dataset**"}}