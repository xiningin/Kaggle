{"cell_type":{"af6fb8cd":"code","24daa6c8":"code","61eabf5b":"code","1b06a7c6":"code","9b6d1f71":"code","34723783":"markdown","8dd6944a":"markdown","08fa959a":"markdown","7ca01d5e":"markdown","4fdb4d90":"markdown","d92ff868":"markdown"},"source":{"af6fb8cd":"import pandas as pd\ndataset = pd.read_csv(\"..\/input\/clickbait-dataset\/clickbait_data.csv\")\ndataset.head()","24daa6c8":"X = dataset['headline']\ny = dataset['clickbait']","61eabf5b":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n\n# Define the feature extractor\n# Limit the number of features so we don't end up having too many\nvec = TfidfVectorizer(max_features = 2**12)\n\n# Define a gradient boosting classifier\ngb_clf = GradientBoostingClassifier(n_estimators = 10_000,\n                                    n_iter_no_change = 250,\n                                    validation_fraction = 0.3)\n\n# Define a random forest classifier\nrf_clf = RandomForestClassifier(n_estimators = 1_000)\n\n\n# Put them into a pipeline so we don't need to manually transform the text\n# into features before classifying. Pipelines handle the process automatically!\nfrom sklearn.pipeline import make_pipeline\n\ntfidf_gb_pipeline = make_pipeline(vec, gb_clf)\ntfidf_rf_pipeline = make_pipeline(vec, rf_clf)","1b06a7c6":"# Measure each model's performance using cross validation accuracy score ...\n\nfrom sklearn.model_selection import cross_val_score\ngb_scores = cross_val_score(tfidf_gb_pipeline, X, y)\nrf_scores = cross_val_score(tfidf_rf_pipeline, X, y)\n\n# ... and put into dataframe for nicer display ...\n# (note: one fold per row)\ncv_scores = pd.DataFrame({\n    'Gradient Boosting': gb_scores,\n    'Random Forest': rf_scores\n})\n\n# ... add mean accuracy for each model ...\ncv_scores.loc['mean',] = cv_scores.mean(axis = 0)\n\n# ... and show the scores!\ncv_scores","9b6d1f71":"# Fit the models ...\ntfidf_gb_pipeline.fit(X, y)\ntfidf_rf_pipeline.fit(X, y)\n\n# ... and save the feature extractor and the model for future use!\nfrom pickle import dump\n\nwith open('tfidfvectorizer.pkl', 'wb') as f:\n    dump(vec, f)\n\nwith open('gradientboostingclf.pkl', 'wb') as f:\n    dump(gb_clf, f)\n    \nwith open('randomforestclf.pkl', 'wb') as f:\n    dump(rf_clf, f)","34723783":"# Closing\n\nThere we go, creating simple models to classify if a title\/headline is clickbait.\n\nYou are free to use the outputs of this notebook! It can be used as pretrained model in other classification problems in the nearby domains, or better, a baseline to improve your own more complicated model.\n\nI hope this has been useful and fun. Happy data sciencing and feel free to upvote and fork!","8dd6944a":"# Training and saving models","08fa959a":"# Introduction\n\nThis notebook will build several models to classify clickbaitiness of a headline\/title. At the end, the models will be saved\/pickled so that it can be reusable as a pretrained model in another project.\n\nI try to keep the content of this notebook as lightweight as possible. Enjoy!","7ca01d5e":"# Cross-validate models\n\nTo know how good our model performs with unseen data, we can use cross-validation. This step right here benefits greatly from pipelining the (feature extractor, classifier) together.","4fdb4d90":"# Define models","d92ff868":"# "}}