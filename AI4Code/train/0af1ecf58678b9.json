{"cell_type":{"3a33dfb5":"code","03f104fa":"code","5361f342":"code","6b012ca8":"code","a5b670d6":"code","021fc10b":"code","7e8fef4e":"code","7b58bbcb":"code","d45f0409":"code","8051c616":"code","3018af98":"code","9c5e20d3":"code","b306735c":"code","0aa8391c":"code","d5c95387":"code","6c91c649":"code","052fbc9a":"code","6d413a27":"code","5538ab3e":"code","d66da5f5":"code","7bd7de54":"code","193cf9c4":"code","d14511cc":"code","724b60b1":"code","08fb31aa":"code","236da546":"code","047d2355":"code","dafcb801":"code","2ed8271d":"code","11f71364":"code","eda4be75":"code","c1eeee1e":"code","d421f69a":"code","56a7f1a6":"code","f8c04708":"code","7e54abe9":"code","7afd19be":"code","431ad6a8":"code","b5ffc7a6":"code","07dc2fa4":"code","3393ecea":"code","ae267445":"code","445528ee":"code","28df079e":"code","3cabc891":"code","595f363e":"code","1667d62d":"code","873872f7":"code","1da47c53":"code","83ff09d4":"code","391bcdb6":"code","7dfbcb45":"code","65d39eee":"code","cd803c88":"code","7fb448d3":"code","be65fa3a":"code","c5477044":"code","12db3fab":"code","5d81a200":"code","1a35780a":"code","c5dd93de":"code","14a3802e":"code","f899983b":"code","eb72e56c":"code","65feaa3f":"code","df5de248":"code","e6e14fff":"code","cd4a3bbe":"code","a92002d8":"code","db87ea4b":"code","facd3ab3":"code","942271e1":"code","ff45dad8":"code","7e3cbdf1":"code","c4a67868":"code","11efcc41":"code","bc6be33b":"code","870b9cb0":"code","27a26bef":"code","7bfad60c":"code","34e21b7f":"code","7a63eae7":"code","e2c694b9":"code","ed351ea9":"code","36633bba":"code","f41b9ed8":"code","e4b9ecfe":"code","8711409e":"code","c2851741":"code","9e209048":"code","f13694ba":"code","5a710117":"code","9e3f0674":"code","1690116d":"code","f330be0b":"code","1924f571":"code","21b687d2":"code","a4685e70":"code","400c5fd2":"code","680bb855":"code","afeef18b":"code","9791e68c":"code","cc299aa9":"code","d9f0d7a8":"code","c3f54968":"code","57613383":"code","0811df9a":"markdown","bb53a8f5":"markdown","a6cbe04c":"markdown","66e40f96":"markdown","3330cdb3":"markdown","f2f28a6d":"markdown","d925ac2e":"markdown","e8e8974f":"markdown","111441f5":"markdown","02f178d9":"markdown","63a85dcd":"markdown","1cb0e336":"markdown","427ab230":"markdown","18f870d7":"markdown","94eaa3f4":"markdown","c446d7c6":"markdown","6ce23d37":"markdown","f74fbe91":"markdown","fa9f87fa":"markdown","ea5511a0":"markdown","b9d3ce22":"markdown","4384814f":"markdown","055a2e73":"markdown","887b7483":"markdown","28f787be":"markdown","308ff465":"markdown","71049264":"markdown","f2f615a8":"markdown","82150389":"markdown","10c7bfba":"markdown","4d976de3":"markdown","c67016c0":"markdown","a266ac24":"markdown","6e0e0396":"markdown","37317705":"markdown","37aa42a3":"markdown","ba7d9afb":"markdown","04204a10":"markdown","763089e9":"markdown","6323fd87":"markdown"},"source":{"3a33dfb5":"# common imports\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# pandas imports\nfrom pandas.plotting import scatter_matrix\n\n# machine learning imports\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.dummy import DummyRegressor\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn import metrics\n\n# display setup\npd.set_option(\"display.max_columns\", None) # the None parameter displays unlimited columns\nsns.set(style=\"whitegrid\") # for plots","03f104fa":"# read the csv file\ndf = pd.read_csv(\"..\/input\/udemy-courses\/udemy_courses.csv\")","5361f342":"# display the first 5 rows for a quick look\ndf.head()","6b012ca8":"# DataFrame shape (rows, columns)\n# understand the amount of data we are working with\ndf.shape","a5b670d6":"# description of data\ndf.info()","021fc10b":"# check if there are null values\ndf.isna().sum()","7e8fef4e":"# summary of the numerical attributes\ndf.describe()","7b58bbcb":"# a histogram plot for each numerical attribute\ndf.drop(\"is_paid\", axis=1).hist(bins=30, figsize=(20,15))\nplt.tight_layout()\nplt.show()","d45f0409":"# use sklearn train_test_split function to split the data\n# the random state parameter ensures that data will be shuffled and split the same way in each run\ntrain_set, test_set = train_test_split(df, test_size=0.20, random_state=42)","8051c616":"print(\"Number of instances in training set: \", len(train_set))\nprint(\"Number of instances in testing set: \", len(test_set))","3018af98":"# deep copy of the training set\ndf2 = train_set.copy()","9c5e20d3":"df2.head(2)","b306735c":"# method creates a correlations matrix\ncorr_matrix = df2.corr()","0aa8391c":"# looking at attributes correlation with num_subscribers feature\ncorr_matrix[\"num_subscribers\"].sort_values(ascending=False)","d5c95387":"# a histogram plot for attributes with a high correlation\n\nattributes = [\"num_subscribers\", \"num_reviews\", \"num_lectures\", \"content_duration\", \"course_id\"]\n\nscatter_matrix(df2[attributes], figsize=(12,8))\nplt.tight_layout()\nplt.show()","6c91c649":"# scatter plot of the strongest correlation in the corr matrix\n# the alpha is set to show the distribution more clearly\ndf2.plot(kind=\"scatter\", x=\"num_reviews\", y=\"num_subscribers\", alpha=0.1,\n         color='b', figsize=(10,5))\nplt.title(\"Reviews and Subscribers Correlation\", size=20)\nplt.xlabel(\"num_reviews\", size=15)\nplt.ylabel(\"num_subscribers\", size=15)\nplt.tight_layout()\nplt.show()","052fbc9a":"print(\"Number of unique course IDs:\", df2[\"course_id\"].nunique())\nprint(\"Length of DataFrame:\", len(df2))","6d413a27":"# check if number of unique urls\n# should be individual for each instance\ndf2[\"url\"].nunique()","5538ab3e":"# show duplicated listings\ndf2[df2.duplicated(\"course_id\")]","d66da5f5":"# remove duplicated listings\ndf2.drop_duplicates(inplace=True)","7bd7de54":"# examine changes\ndf2.shape","193cf9c4":"# evaluate current values in column\ndf2[\"is_paid\"].head(10)","d14511cc":"# use encoder to convert \"is_paid\" column to binary outcome\nordinal_encoder = OrdinalEncoder(dtype=int)\ndf2[\"is_paid\"] = ordinal_encoder.fit_transform(df2[[\"is_paid\"]])","724b60b1":"# evaluate changes\ndf2[\"is_paid\"].head(10)","08fb31aa":"# 0 is False, 1 is True\nordinal_encoder.categories_","236da546":"# count number of instances for each outcome\ndf2[\"is_paid\"].value_counts()","047d2355":"# use groupby for price attribute\nprice_values = df2.groupby(\"price\")","dafcb801":"# check if number of free courses matches when the price is 0\nprice_values_0 = price_values.get_group(0)\nprice_values_0.shape","2ed8271d":"# plot of free and paid courses\nplt.figure(figsize=(10,5))\nsns.countplot(x=df2[\"is_paid\"])\nplt.title(\"Free and Paid Courses\", size=20)\nplt.xlabel(\"is_paid\", size = 15)\nplt.ylabel(\"count\", size=15)\nplt.tight_layout()\nplt.show()","11f71364":"# course price values sorted by prices\ndf2[\"price\"].value_counts().sort_index()","eda4be75":"# top ten course price values sorted by value counts\nprices_top10 = df2[\"price\"].value_counts().sort_values(ascending=False).head(10)","c1eeee1e":"# calculate percentage of instances per price in data\nprices_percent_in_data = []\nnum_subscribed = []\n\nfor i in range(len(prices_top10.index)):\n    prices_percent_in_data.append(round((prices_top10.values[i]\/len(df2))*100,2))\n    num_subscribed.append(price_values.get_group(prices_top10.index[i])[\"num_subscribers\"].sum())","d421f69a":"# create a DataFrame with the results\nprices_top10_dict = {\"price\": prices_top10.index, \"number_of_instances\": prices_top10.values,\n                     \"% of data\": prices_percent_in_data, \"num_subscribers\": num_subscribed}\nprices_top10_df = pd.DataFrame(prices_top10_dict, index=range(1,11))\nprices_top10_df","56a7f1a6":"# plot of top 10 common prices by amount of subscribers\nplt.figure(figsize=(10,5))\nsns.barplot(x=prices_top10_df[\"price\"], y=prices_top10_df[\"num_subscribers\"])\nplt.xlabel(\"price\", size=15)\nplt.ylabel(\"num_subscribers\\n(millions)\", size=15)\nplt.title(\"Top 10 Common Prices by Subscribers\", size=20)\nplt.tight_layout()\nplt.show()","f8c04708":"# plot of content duration by free or paid course\nplt.figure(figsize=(10,5))\nsns.scatterplot(x=df2[\"content_duration\"], y=df2[\"is_paid\"], alpha=0.1)\nplt.title(\"Content Duration by Type of Course Payment\", size=20)\nplt.xlabel(\"content_duration\", size=15)\nplt.ylabel(\"is_paid\", size=15)\nplt.tight_layout()\nplt.show()","7e54abe9":"# count number of instances\nlevel_values = df2[\"level\"].value_counts()\nlevel_values","7afd19be":"# count number of instances\nsubject_values = df2[\"subject\"].value_counts()\nsubject_values","431ad6a8":"# pie plot of course levels and subjects in data\nfig, ax = plt.subplots(1,2, figsize=(10,5))\nax[0].pie(level_values, startangle=180, labels=level_values.index, autopct=\"%1.1f%%\")\nax[0].set_title(\"Course Levels\", size=20)\nax[1].pie(subject_values, startangle=180, labels=subject_values.index, autopct=\"%1.1f%%\")\nax[1].set_title(\"Course Subjects\", size=20)\nplt.tight_layout()\nplt.show()","b5ffc7a6":"# scatter plot of price by course level\nplt.figure(figsize=(10,5))\nsns.scatterplot(y=df2[\"level\"], x=df2[\"price\"], alpha=0.1)\nplt.title(\"Price by Course Level\", size=20)\nplt.xlabel(\"price\", size=15)\nplt.ylabel(\"level\", size=15)\nplt.tight_layout()\nplt.show()","07dc2fa4":"# plot subject by number of subscribers and level\n# the black bars represent the error\nplt.figure(figsize=(10,5))\nsns.barplot(x=df2[\"subject\"], y=df2[\"num_subscribers\"], hue=df2[\"level\"])\nplt.title(\"Subject by Number of Subscribers and Level\", size=20)\nplt.xlabel(\"subject\", size=15)\nplt.ylabel(\"num_subscribers\", size=15)\nplt.tight_layout()\nplt.show()","3393ecea":"# examine current shape\ndf2.shape","ae267445":"# every course has a unique URL\ndf2[\"url\"].nunique()","445528ee":"# some courses have an identical title\ndf2[\"course_title\"].nunique()","28df079e":"# find duplicated instances\n# false marks all duplicates as true\ntitle_df = df2[df2.duplicated(\"course_title\", keep=False)].copy()\n# show duplicated titles\ntitle_df[\"course_title\"].unique()","3cabc891":"# examine number of unique subscribers values\ntitle_df[\"num_subscribers\"].nunique()","595f363e":"# groupy course title\ntitle = title_df.groupby(\"course_title\")","1667d62d":"# examining one of the duplicated courses\n# the courses have the same name and different values for some features\ntitle.get_group(\"Acoustic Blues Guitar Lessons\")","873872f7":"# clean copy of training set\ndf3 = train_set.copy()","1da47c53":"df3.shape","83ff09d4":"# remove duplicated instances\ndf3.drop_duplicates(\"course_id\", inplace=True)","391bcdb6":"# evaluate changes\ndf3.shape","7dfbcb45":"# separate predictors from target values\n\n# drop creates a copy without changing the training set\nX_train = df3.drop(\"num_subscribers\", axis=1)\n\n# create a deep copy of the target values\ny_train = df3[\"num_subscribers\"].copy()","65d39eee":"# list of numerical features\nnum_features = [\"price\", \"num_reviews\", \"num_lectures\", \"content_duration\"]\n\n# list of level feature categories\nlevels = [\"All Levels\", \"Beginner Level\", \"Intermediate Level\", \"Expert Level\"]\n\n# column transformer:\n# features generated by each transformer will be concatenated to form a single feature space\n# columns of the original feature matrix that are not specified are dropped\nfull_pipeline = ColumnTransformer([\n\n# MinMaxScaler normalizes data (rescales between 0-1)\n    (\"num\", MinMaxScaler(), num_features),\n\n# OrdinalEncoder converts categories to integers according to order specified in list\n    (\"level\", OrdinalEncoder(categories=[levels]), [\"level\"]),\n\n# OrdinalEncoder converts True and False values to integers\n# True=1, False=0\n    (\"is_paid\", OrdinalEncoder(dtype=int), [\"is_paid\"]),\n\n# OneHotEncoder converts categories to a binary dummy array\n    (\"subject\", OneHotEncoder(handle_unknown=\"ignore\"), [\"subject\"])\n])","cd803c88":"features = num_features+[\"level\", \"is_paid\", \"subject\"]\n\n# transform training data using pipeline\nX_train_prepared = full_pipeline.fit_transform(X_train)\nX_tr_testing = full_pipeline.transform(X_train)","7fb448d3":"# function prints scores, mean and std\ndef display_scores(scores):\n    print(\"Scores:\", scores)\n    print(\"Mean:\", scores.mean())\n    print(\"Standard deviation:\", scores.std())\n\n# function prints evaluation metrics\ndef display_evaluation(actual, pred):\n    mse = metrics.mean_squared_error(actual, pred)\n    print(\"Mean Squared Error:\", mse)\n    print(\"Root Mean Squared Error:\", np.sqrt(mse))","be65fa3a":"# instantiate model\nlr = LinearRegression()","c5477044":"# fit the training data\nlr.fit(X_train_prepared, y_train)","12db3fab":"# predict using training data\nlr_pred = lr.predict(X_tr_testing)","5d81a200":"# test on a few instances from training data\nsome_data = X_train.iloc[:10]\nsome_labels = y_train.iloc[:10]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lr.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))","1a35780a":"# use function to show results\ndisplay_evaluation(y_train, lr_pred)","c5dd93de":"# 10 fold cross validation\nlr_scores = cross_val_score(lr, X_train_prepared, y_train, cv=10, scoring=\"neg_mean_squared_error\", )\n\n# scoring function returns a negative value for MSE (need to add the minus)\nlr_rmse_scores = np.sqrt(-lr_scores)\ndisplay_scores(lr_rmse_scores)","14a3802e":"# estimate prediction using cross validation\nlr_pred = cross_val_predict(lr, X_tr_testing, y_train, cv=10)","f899983b":"# test on a few instances from training data\nsome_data = X_train.iloc[:10]\nsome_labels = y_train.iloc[:10]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", lr.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))","eb72e56c":"# use function to show results\ndisplay_evaluation(y_train, lr_pred)","65feaa3f":"# instantiate model\nrfr = RandomForestRegressor(random_state=42)","df5de248":"# fit the training data\nrfr.fit(X_train_prepared, y_train)","e6e14fff":"# predict using training data\nrfr_pred = rfr.predict(X_tr_testing)","cd4a3bbe":"# use function to show results\ndisplay_evaluation(y_train, rfr_pred)","a92002d8":"# max features default is sqrt (number of features selected per split)\n# bootstrap default is true (resampling data true)\n# n estimators default is 100 (number of decision trees)\n# parameters for grid search\nparam_grid = {\"n_estimators\": [10,50,100,500], \"max_features\":[2,4,8], \"bootstrap\": [True, False]}","db87ea4b":"# instantiate grid search\ngrid_search = GridSearchCV(rfr, param_grid, cv=5, scoring=\"neg_mean_squared_error\", return_train_score=True)","facd3ab3":"# fit to the training data\ngrid_search.fit(X_train_prepared, y_train)","942271e1":"# show the best score\nnp.sqrt(-grid_search.best_score_)","ff45dad8":"# show the best parameters\ngrid_search.best_estimator_","7e3cbdf1":"# show results for each iteration\ncvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","c4a67868":"# instantiate model\nrfr = grid_search.best_estimator_\nrfr","11efcc41":"# test on a few instances from training data\nsome_data = X_train.iloc[:10]\nsome_labels = y_train.iloc[:10]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions:\", rfr.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))","bc6be33b":"# predict using training data\nrfr_pred_2 = rfr.predict(X_tr_testing)","870b9cb0":"# use function to show results\ndisplay_evaluation(y_train, rfr_pred_2)","27a26bef":"level_encoder = full_pipeline.named_transformers_[\"level\"]\nlevel_encoder_attribs = list(level_encoder.categories_[0])\n\nsubject_encoder = full_pipeline.named_transformers_[\"subject\"]\nsubject_encoder_attribs = list(subject_encoder.categories_[0])\n\nfeatures_sub = num_features+level_encoder_attribs+[\"is_paid\"]+subject_encoder_attribs","7bfad60c":"# pair the feature names with the results from grid search\nfeature_importance = grid_search.best_estimator_.feature_importances_\nsorted(zip(feature_importance,features_sub), reverse=True)","34e21b7f":"# column transformer with numerical attributes only\nfull_pipeline_2 = ColumnTransformer([\n    (\"num\", MinMaxScaler(), num_features),\n])","7a63eae7":"X_train_prepared_2 = full_pipeline_2.fit_transform(X_train)\nX_tr_testing_2 = full_pipeline_2.transform(X_train)","e2c694b9":"# instantiate model\nrfr = RandomForestRegressor(random_state=42)","ed351ea9":"# fit the training data\nrfr.fit(X_train_prepared_2, y_train)","36633bba":"# test on a few instances from training data\nsome_data = X_train.iloc[:10]\nsome_labels = y_train.iloc[:10]\nsome_data_prepared = full_pipeline_2.transform(some_data)\nprint(\"Predictions:\", rfr.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))","f41b9ed8":"# predict using training data\nrfr_pred_3 = rfr.predict(X_tr_testing_2)","e4b9ecfe":"# use function to show results\ndisplay_evaluation(y_train, rfr_pred_3)","8711409e":"# parameters for grid search\nparam_grid_2 = {\"n_estimators\": [10,50,100,500], \"max_features\":[2,3,4], \"bootstrap\": [True, False]}","c2851741":"# instantiate grid search\ngrid_search_2 = GridSearchCV(rfr, param_grid_2, cv=5, scoring=\"neg_mean_squared_error\", return_train_score=True)","9e209048":"# fit the training data\ngrid_search_2.fit(X_train_prepared_2, y_train)","f13694ba":"# show the best score\nnp.sqrt(-grid_search.best_score_)","5a710117":"# show the best parameters\ngrid_search_2.best_estimator_","9e3f0674":"# show results for each iteration\ncvres = grid_search_2.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","1690116d":"# instantiate model\nrfr_2 = grid_search_2.best_estimator_\nrfr_2","f330be0b":"# test on a few instances from training data\nsome_data = X_train.iloc[:10]\nsome_labels = y_train.iloc[:10]\nsome_data_prepared = full_pipeline_2.transform(some_data)\nprint(\"Predictions:\", rfr_2.predict(some_data_prepared))\nprint(\"Labels:\", list(some_labels))","1924f571":"# predict using training data\nrfr_pred_4 = rfr_2.predict(X_tr_testing_2)","21b687d2":"# use function to show results\ndisplay_evaluation(y_train, rfr_pred_4)","a4685e70":"# instantiate dummy regressor\n# predicts the mean for each instance\ndummy = DummyRegressor(strategy=\"mean\")","400c5fd2":"# fit the training set\ndummy.fit(X_train_prepared_2, y_train)","680bb855":"# predict using dummy regressor\ndummy_pred = dummy.predict(X_train_prepared_2)","afeef18b":"# use function to show results\ndisplay_evaluation(y_train, dummy_pred)","9791e68c":"# separate test set predictors and labels\nX_test = test_set.drop(\"num_subscribers\", axis=1)\ny_test = test_set[\"num_subscribers\"].copy()","cc299aa9":"final_model = grid_search_2.best_estimator_\nfinal_model","d9f0d7a8":"# transform test set\nX_test_prep = full_pipeline_2.transform(X_test)","c3f54968":"# predict test set\nfinal_predictions = final_model.predict(X_test_prep)","57613383":"# evaluate predictions\ndisplay_evaluation(y_test, final_predictions)","0811df9a":"> ### Observations:\n>> * The duplicated courses have different parameters such as is_paid or published_timestamp.\n>> Maybe the course provides the first lessons free of charge, or they added new content.\n>> * These instances can be kept as they are likely to have various values (i.e. each\n>> value in the num_subscribers column is unique).","bb53a8f5":"> ### Assessing Price Features","a6cbe04c":"#### Model 3: Random Forest Regressor","66e40f96":"# 4. Training and Evaluating Models","3330cdb3":"> ### Overview:\n>> ####  Removing the categorical features even slightly improved the score.\n>> * The RMSE with all features was approximately 2551.\n>> * The RMSE with only the numerical features was approximately 2520.\n>> * The model is substantially better than the dummy regressor.","f2f28a6d":"<h1 style=\"text-align: center;\">EDA of Udemy Courses and ML to Predict Subscribers<\/h1>","d925ac2e":"#### Model 1: Linear Regression","e8e8974f":"> # Objective\n> ## Predicting the number of subscribers for a course.\n>> ### Chosen Feature:\n>> #### *num_subscribers* column\n>>> The column represents how many people have subscribed to each course.\n>>> ### Motive:\n>>> Predicting the number of people subscribed to a course, course popularity.","111441f5":"#### Model 2: Random Forest Regressor","02f178d9":"# 3. Data Cleaning","63a85dcd":"> ### Correlations with num_subscribers Attribute- Overview:\n>> The strongest positive correlations (0.1 or more) are:\n>> * num_reviews\n>> * num_lectures\n>> * content_duration\n>>\n>> The strongest negative correlations (-0.1 or less) are:\n>> * course_id\n>> * is_paid","1cb0e336":"> Initial observations from the histograms:\n>> 1. Most course durations are between 0-5 hours.\n>> 2. There are usually around 1-50 lectures per course.\n>> 3. Courses tend to have few reviews. There are probably a handful of courses\n>> with a large amount of reviews since the X axis goes up to 25000 while over 3000\n>> instances are represented in the first bin.\n>> 4. The majority of courses are in the same range of subscribers. The instances farther up\n>> the scale were probably more successful or perhaps courses on a trending topic.\n>> 5. Assuming the prices are in USD, the range is between 0-250 dollars.\n>> The plot shows the most common price roughly $25.","427ab230":"> The Random Forest Regressor model is based on many decision trees.\n> A decision tree is a non-linear model built by constructing many linear boundaries.\n> The random forest model samples random points and subsets of features when training.\n> Then, the predictions are made by averaging the predictions made by each decision tree.","18f870d7":"##### Cross Validation for Linear Regression Model","94eaa3f4":"> #### Resources:\n> 1. Udemy Courses Dataset <a href=\"https:\/\/www.kaggle.com\/andrewmvd\/udemy-courses\"\n> title=\"Kaggle\">link<\/a>\n> 2. Regression Evaluation Metrics Article <a href=\"https:\/\/medium.com\/analytics-vidhya\/mae-mse-rmse\n> -coefficient-of-determination-adjusted-r-squared-which-metric-is-better-cd0326a5697e\" title=\"medium\">link<\/a>\n> 3. Random Forest Article <a href=\"https:\/\/towardsdatascience.com\/an-implementation-and-\n> explanation-of-the-random-forest-in-python-77bf308a9b76\" title=\"towardsdatascience\">link<\/a>","c446d7c6":"#### Grid Search Cross Validation 2","6ce23d37":"#### Model 5: Random Forest Regressor","f74fbe91":"> ## Exploring Attribute Combinations","fa9f87fa":"> Next, lets train a model without the parameters that have less than 0.05 feature importance\n> and compare the model performances.\n>\n> In this case, all categorical features will be removed.","ea5511a0":"> ### Removing the Following Columns:\n> The reason for removing these columns is for the model to generalize better.\n> Furthermore, these columns have a unique value for each instance (i.e. URL, course ID) which\n> does not provide information the model can learn from to predict on new data.\n>> * course_id\n>> * course_title\n>> * url\n>> * published_timestamp","b9d3ce22":"> ### Analyzing Additional Columns","4384814f":"> ### Splitting the Data:\n>> Before further analysis let's split the data into a training set and a testing set.\n>> This will ensure avoidance of bias that could occur from learning the data as a whole.","055a2e73":"> The Linear Regression model computes a weighted sum of the input features, and a constant which\n> is the bias\/intercept term. As the name implies, this is in fact a linear function.","887b7483":"# 1. Getting the Data","28f787be":"### Any feedback, suggestions, questions? Leave a comment below!\n### Upvote if you liked this notebook, learned something new or found it useful!","308ff465":"> Chosen evaluation metric:\n>\n> The root-mean-square error (RMSE) is the standard deviation of the prediction error.\n> It is the differences between the predicted and actual values, and shows how much they are\n> spread out.","71049264":"> ### Researching Level and Subject Features","f2f615a8":"> Since there is a unique value for almost every course ID, the correlation was probably\n> coincidental.","82150389":"#### Grid Search Cross Validation 1","10c7bfba":"#### Feature Importance","4d976de3":"> ### Observations:\n>> * As speculated earlier in the initial observations, $20 is the most common price for a course.\n>> * The number of listings with the price $0 matches the number of instances that were\n>> labeled \"False\" in the is_paid column.\n>> * The prices listed tend to increase by 5 dollars until they reach the maximum price\n>> which is $200.\n>> * Amongst the 10 most common prices in the data, most are subscribed to the free courses.\n>> * Content duration is longer for paid courses.","c67016c0":"> ### Observations:\n>> * All Levels is the most common level, representing over 50%.\n>> * Web Development is the most common subject, and Business Finance is second with\n>> approximately a 1% differential.\n>> * Price variations according to the level of the course also show that Expert is\n>> the least common level in the data. It is also the only level that does not\n>> provide free courses. The other levels are dispersed more frequently\n>> throughout the line.\n>> * Web Development courses are significantly higher in subscribers than the other subjects.\n>> Since Business Finance falls shortly behind in content, it is likely that people are more\n>> interested in studying Web Development courses.","a266ac24":"> ### Overview:\n>> * The course ID is unique for each course.\n>> * This column should be removed when training a model in order to generalize better.","6e0e0396":"#### Dummy Regressor\n> The dummy regressor serves as an indication and comparison for model performance.","37317705":"> ### Examining Course ID Feature","37aa42a3":"> As shown above, there are no missing values which is excellent!\n>\n> ##### *It is vital to understand the features we are working with.*\n> ### Features in the DataFrame:\n>> 1. course_id: Course identification number\n>> 2. course_title: Title of course\n>> 3. url: Course URL\n>> 4. is_paid: True if the course costs money, false if the course is free\n>> 5. price: Price of course\n>> 6. num_subscribers: Number of subscribers for the course\n>> 7. num_lectures: Number of lectures in the course\n>> 8. level: Difficulty level of the course\n>> 9. content_duration: Duration of all course materials\n>> 10. published_timestamp: Course publication date\n>> 11. subject: Subject of course","ba7d9afb":"> The Random Forest Regressor model performed better than the linear regression model,\n> even after cross validation. The next step is to find the hyperparameters\n> that provide the best results.\n>\n> For this task we can use grid search cv. The grid search works by trying all parameter\n> combinations from the ones instantiated, then shows the best combination according to\n> the highest score.","04204a10":"# 5. Evaluating the Test Set","763089e9":"# 2. Understanding and Visualizing the Data\n> ##### *The motivation for this section is to gain more insights*","6323fd87":"#### Model 4: Random Forest Regressor"}}