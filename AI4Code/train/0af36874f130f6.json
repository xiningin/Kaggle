{"cell_type":{"cd128489":"code","1772109d":"code","db3621e4":"code","352e1e59":"code","7b50c965":"code","5cd054d7":"code","db156dbb":"code","ff3ac12a":"code","12a54493":"code","b56186f3":"code","6676c5a7":"code","77346d75":"code","a266f873":"code","2ea63d37":"code","ed8abb97":"code","d01980c0":"code","757baec9":"code","ecc2d044":"code","fba34192":"code","4a2cca12":"code","adda89ae":"code","5fa85a39":"code","e772a563":"code","21b8e18c":"code","017bf0b2":"code","c3cb099a":"code","3ec28c53":"code","c40f9a23":"code","274e4683":"code","52764dcb":"code","fcdfaa90":"code","6f93539c":"code","9b94065f":"code","ebad4897":"markdown","92694f31":"markdown","3b4e7514":"markdown","31bdaf61":"markdown","c79d92fb":"markdown","a9354bba":"markdown","6ce9e55c":"markdown","780579b0":"markdown","597ee396":"markdown","46244555":"markdown","e96c7cee":"markdown","56e80471":"markdown","1971aacd":"markdown","0bd22f90":"markdown","7eb43381":"markdown","8c773bed":"markdown","56de4c9c":"markdown","312034ba":"markdown","0f68e722":"markdown","9f7ee1f7":"markdown","753d6fd4":"markdown","8c86bf48":"markdown","e2b86e14":"markdown","3a168dee":"markdown","75f7e88d":"markdown","a2a4be30":"markdown","3eabf301":"markdown","662ab88c":"markdown","8d11fab6":"markdown","8efb69fa":"markdown"},"source":{"cd128489":"# Importing required libraries \n# Keras\nimport keras\nfrom keras import regularizers\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential, Model, model_from_json\nfrom keras.layers import Dense, Embedding, LSTM\nfrom keras.layers import Input, Flatten, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.utils import np_utils, to_categorical\nfrom keras.callbacks import ModelCheckpoint\n\n# sklearn\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# Other  \nimport librosa\nimport librosa.display\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom matplotlib.pyplot import specgram\nimport pandas as pd\nimport seaborn as sns\nimport glob \nimport os\nfrom tqdm import tqdm\nimport pickle\nimport IPython.display as ipd  # To play sound in the notebook","1772109d":"#########################\n# Augmentation methods\n#########################\ndef noise(data):\n    \"\"\"\n    Adding White Noise.\n    \"\"\"\n    # you can take any distribution from https:\/\/docs.scipy.org\/doc\/numpy-1.13.0\/reference\/routines.random.html\n    noise_amp = 0.05*np.random.uniform()*np.amax(data)   # more noise reduce the value to 0.5\n    data = data.astype('float64') + noise_amp * np.random.normal(size=data.shape[0])\n    return data\n    \ndef shift(data):\n    \"\"\"\n    Random Shifting.\n    \"\"\"\n    s_range = int(np.random.uniform(low=-5, high = 5)*1000)  #default at 500\n    return np.roll(data, s_range)\n    \ndef stretch(data, rate=0.8):\n    \"\"\"\n    Streching the Sound. Note that this expands the dataset slightly\n    \"\"\"\n    data = librosa.effects.time_stretch(data, rate)\n    return data\n    \ndef pitch(data, sample_rate):\n    \"\"\"\n    Pitch Tuning.\n    \"\"\"\n    bins_per_octave = 12\n    pitch_pm = 2\n    pitch_change =  pitch_pm * 2*(np.random.uniform())   \n    data = librosa.effects.pitch_shift(data.astype('float64'), \n                                      sample_rate, n_steps=pitch_change, \n                                      bins_per_octave=bins_per_octave)\n    return data\n    \ndef dyn_change(data):\n    \"\"\"\n    Random Value Change.\n    \"\"\"\n    dyn_change = np.random.uniform(low=-0.5 ,high=7)  # default low = 1.5, high = 3\n    return (data * dyn_change)\n    \ndef speedNpitch(data):\n    \"\"\"\n    peed and Pitch Tuning.\n    \"\"\"\n    # you can change low and high here\n    length_change = np.random.uniform(low=0.8, high = 1)\n    speed_fac = 1.2  \/ length_change # try changing 1.0 to 2.0 ... =D\n    tmp = np.interp(np.arange(0,len(data),speed_fac),np.arange(0,len(data)),data)\n    minlen = min(data.shape[0], tmp.shape[0])\n    data *= 0\n    data[0:minlen] = tmp[0:minlen]\n    return data\n\n####################################\n# the confusion matrix heat map plot\n####################################\ndef print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n    \n    Arguments\n    ---------\n    confusion_matrix: numpy.ndarray\n        The numpy.ndarray object returned from a call to sklearn.metrics.confusion_matrix. \n        Similarly constructed ndarrays can also be used.\n    class_names: list\n        An ordered list of class names, in the order they index the given confusion matrix.\n    figsize: tuple\n        A 2-long tuple, the first value determining the horizontal size of the ouputted figure,\n        the second determining the vertical size. Defaults to (10,7).\n    fontsize: int\n        Font size for axes labels. Defaults to 14.\n        \n    Returns\n    -------\n    matplotlib.figure.Figure\n        The resulting confusion matrix figure\n    \"\"\"\n    df_cm = pd.DataFrame(\n        confusion_matrix, index=class_names, columns=class_names, \n    )\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n        \n    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","db3621e4":"# Use one audio file in previous parts again\nfname = '\/kaggle\/input\/surrey-audiovisual-expressed-emotion-savee\/ALL\/JK_f11.wav'  \ndata, sampling_rate = librosa.load(fname)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)\n\n# Paly it again to refresh our memory\nipd.Audio(data, rate=sampling_rate)","352e1e59":"x = noise(data)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(x, sr=sampling_rate)\nipd.Audio(x, rate=sampling_rate)","7b50c965":"x = shift(data)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(x, sr=sampling_rate)\nipd.Audio(x, rate=sampling_rate)","5cd054d7":"x = stretch(data)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(x, sr=sampling_rate)\nipd.Audio(x, rate=sampling_rate)","db156dbb":"x = pitch(data, sampling_rate)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(x, sr=sampling_rate)\nipd.Audio(x, rate=sampling_rate)","ff3ac12a":"x = dyn_change(data)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(x, sr=sampling_rate)\nipd.Audio(x, rate=sampling_rate)","12a54493":"x = speedNpitch(data)\nplt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(x, sr=sampling_rate)\nipd.Audio(x, rate=sampling_rate)","b56186f3":"# lets pick up the meta-data that we got from our first part of the Kernel\n\n#ref = pd.read_csv(\"\/kaggle\/input\/data-path\/Data_path.csv\")\nref = pd.read_csv(\"\/kaggle\/input\/audio-emotion-part-1-explore-data\/Data_path.csv\")\n\nref.head()","6676c5a7":"# Note this takes a couple of minutes (~16 mins) as we're iterating over 4 datasets, and with augmentation  \ndf = pd.DataFrame(columns=['feature'])\ndf_noise = pd.DataFrame(columns=['feature'])\ndf_speedpitch = pd.DataFrame(columns=['feature'])\ncnt = 0\n\n# loop feature extraction over the entire dataset\nfor i in tqdm(ref.path):\n    \n    # first load the audio \n    X, sample_rate = librosa.load(i\n                                  , res_type='kaiser_fast'\n                                  ,duration=2.5\n                                  ,sr=44100\n                                  ,offset=0.5\n                                 )\n\n    # take mfcc and mean as the feature. Could do min and max etc as well. \n    mfccs = np.mean(librosa.feature.mfcc(y=X, \n                                        sr=np.array(sample_rate), \n                                        n_mfcc=13),\n                    axis=0)\n    \n    df.loc[cnt] = [mfccs]   \n\n    # random shifting (omit for now)\n    # Stretch\n    # pitch (omit for now)\n    # dyn change\n    \n    # noise \n    aug = noise(X)\n    aug = np.mean(librosa.feature.mfcc(y=aug, \n                                    sr=np.array(sample_rate), \n                                    n_mfcc=13),    \n                  axis=0)\n    df_noise.loc[cnt] = [aug]\n\n    # speed pitch\n    aug = speedNpitch(X)\n    aug = np.mean(librosa.feature.mfcc(y=aug, \n                                    sr=np.array(sample_rate), \n                                    n_mfcc=13),    \n                  axis=0)\n    df_speedpitch.loc[cnt] = [aug]   \n\n    cnt += 1\n\ndf.head()","77346d75":"# save it\ndf.to_pickle('my4EmotionDatabases-with-augmentation')","a266f873":"# load it\ndf = pd.read_pickle('my4EmotionDatabases-with-augmentation')\ndf.head()","2ea63d37":"# combine \ndf = pd.concat([ref,pd.DataFrame(df['feature'].values.tolist())],axis=1)\ndf_noise = pd.concat([ref,pd.DataFrame(df_noise['feature'].values.tolist())],axis=1)\ndf_speedpitch = pd.concat([ref,pd.DataFrame(df_speedpitch['feature'].values.tolist())],axis=1)\nprint(df.shape,df_noise.shape,df_speedpitch.shape)","ed8abb97":"df = pd.concat([df,df_noise,df_speedpitch],axis=0,sort=False)\ndf=df.fillna(0)\ndel df_noise, df_speedpitch\n\ndf.head()","d01980c0":"# Split between train and test \nX_train, X_test, y_train, y_test = train_test_split(df.drop(['path','labels','source'],axis=1)\n                                                    , df.labels\n                                                    , test_size=0.25\n                                                    , shuffle=True\n                                                    , random_state=42\n                                                   )\n\n# Lets see how the data present itself before normalisation \nX_train[150:160]","757baec9":"# Lts do data normalization \nmean = np.mean(X_train, axis=0)\nstd = np.std(X_train, axis=0)\n\nX_train = (X_train - mean)\/std\nX_test = (X_test - mean)\/std\n\n# Check the dataset now \nX_train[150:160]","ecc2d044":"# Lets few preparation steps to get it into the correct format for Keras \nX_train = np.array(X_train)\ny_train = np.array(y_train)\nX_test = np.array(X_test)\ny_test = np.array(y_test)\n\n# one hot encode the target \nlb = LabelEncoder()\ny_train = np_utils.to_categorical(lb.fit_transform(y_train))\ny_test = np_utils.to_categorical(lb.fit_transform(y_test))\n\nprint(X_train.shape)\nprint(lb.classes_)\n\n# Pickel the lb object for future use \nfilename = 'labels'\noutfile = open(filename,'wb')\npickle.dump(lb,outfile)\noutfile.close()","fba34192":"X_train = np.expand_dims(X_train, axis=2)\nX_test = np.expand_dims(X_test, axis=2)\nX_train.shape","4a2cca12":"# New model\nmodel = Sequential()\nmodel.add(Conv1D(256, 8, padding='same',input_shape=(X_train.shape[1],1)))  # X_train.shape[1] = No. of Columns\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(256, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(128, 8, padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=(8)))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv1D(64, 8, padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Flatten())\nmodel.add(Dense(14)) # Target class number\nmodel.add(Activation('softmax'))\nopt = keras.optimizers.RMSprop(lr=0.00001, decay=1e-6)\nmodel.summary()","adda89ae":"model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])\nmodel_history=model.fit(X_train, y_train, batch_size=16, epochs=150, validation_data=(X_test, y_test),verbose=2)","5fa85a39":"plt.plot(model_history.history['loss'])\nplt.plot(model_history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","e772a563":"# Save model and weights\nmodel_name = 'Emotion_Model_aug.h5'\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\n\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Save model and weights at %s ' % model_path)\n\n# Save the model to disk\nmodel_json = model.to_json()\nwith open(\"model_json_aug.json\", \"w\") as json_file:\n    json_file.write(model_json)","21b8e18c":"# loading json and model architecture \njson_file = open('model_json_aug.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n\n# load weights into new model\nloaded_model.load_weights(\"saved_models\/Emotion_Model_aug.h5\")\nprint(\"Loaded model from disk\")\n \n# Keras optimiser\nopt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)\nloaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\nscore = loaded_model.evaluate(X_test, y_test, verbose=0)\nprint(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))","017bf0b2":"preds = loaded_model.predict(X_test, \n                         batch_size=16, \n                         verbose=1)\n\npreds=preds.argmax(axis=1)\npreds","c3cb099a":"# predictions \npreds = preds.astype(int).flatten()\npreds = (lb.inverse_transform((preds)))\npreds = pd.DataFrame({'predictedvalues': preds})\n\n# Actual labels\nactual=y_test.argmax(axis=1)\nactual = actual.astype(int).flatten()\nactual = (lb.inverse_transform((actual)))\nactual = pd.DataFrame({'actualvalues': actual})\n\n# Lets combined both of them into a single dataframe\nfinaldf = actual.join(preds)\nfinaldf[170:180]","3ec28c53":"# Write out the predictions to disk\nfinaldf.to_csv('Predictions.csv', index=False)\nfinaldf.groupby('predictedvalues').count()","c40f9a23":"# Get the predictions file \nfinaldf = pd.read_csv(\"Predictions.csv\")\nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \n\n# Confusion matrix \nc = confusion_matrix(finaldf.actualvalues, finaldf.predictedvalues)\nprint(accuracy_score(finaldf.actualvalues, finaldf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","274e4683":"# Classification report \nclasses = finaldf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(finaldf.actualvalues, finaldf.predictedvalues, target_names=classes))","52764dcb":"modidf = finaldf\nmodidf['actualvalues'] = finaldf.actualvalues.replace({'female_angry':'female'\n                                       , 'female_disgust':'female'\n                                       , 'female_fear':'female'\n                                       , 'female_happy':'female'\n                                       , 'female_sad':'female'\n                                       , 'female_surprise':'female'\n                                       , 'female_neutral':'female'\n                                       , 'male_angry':'male'\n                                       , 'male_fear':'male'\n                                       , 'male_happy':'male'\n                                       , 'male_sad':'male'\n                                       , 'male_surprise':'male'\n                                       , 'male_neutral':'male'\n                                       , 'male_disgust':'male'\n                                      })\n\nmodidf['predictedvalues'] = finaldf.predictedvalues.replace({'female_angry':'female'\n                                       , 'female_disgust':'female'\n                                       , 'female_fear':'female'\n                                       , 'female_happy':'female'\n                                       , 'female_sad':'female'\n                                       , 'female_surprise':'female'\n                                       , 'female_neutral':'female'\n                                       , 'male_angry':'male'\n                                       , 'male_fear':'male'\n                                       , 'male_happy':'male'\n                                       , 'male_sad':'male'\n                                       , 'male_surprise':'male'\n                                       , 'male_neutral':'male'\n                                       , 'male_disgust':'male'\n                                      })\n\nclasses = modidf.actualvalues.unique()  \nclasses.sort() \n\n# Confusion matrix \nc = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\nprint(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","fcdfaa90":"# Classification report \nclasses = modidf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))","6f93539c":"modidf = pd.read_csv(\"Predictions.csv\")\nmodidf['actualvalues'] = modidf.actualvalues.replace({'female_angry':'angry'\n                                       , 'female_disgust':'disgust'\n                                       , 'female_fear':'fear'\n                                       , 'female_happy':'happy'\n                                       , 'female_sad':'sad'\n                                       , 'female_surprise':'surprise'\n                                       , 'female_neutral':'neutral'\n                                       , 'male_angry':'angry'\n                                       , 'male_fear':'fear'\n                                       , 'male_happy':'happy'\n                                       , 'male_sad':'sad'\n                                       , 'male_surprise':'surprise'\n                                       , 'male_neutral':'neutral'\n                                       , 'male_disgust':'disgust'\n                                      })\n\nmodidf['predictedvalues'] = modidf.predictedvalues.replace({'female_angry':'angry'\n                                       , 'female_disgust':'disgust'\n                                       , 'female_fear':'fear'\n                                       , 'female_happy':'happy'\n                                       , 'female_sad':'sad'\n                                       , 'female_surprise':'surprise'\n                                       , 'female_neutral':'neutral'\n                                       , 'male_angry':'angry'\n                                       , 'male_fear':'fear'\n                                       , 'male_happy':'happy'\n                                       , 'male_sad':'sad'\n                                       , 'male_surprise':'surprise'\n                                       , 'male_neutral':'neutral'\n                                       , 'male_disgust':'disgust'\n                                      })\n\nclasses = modidf.actualvalues.unique() \nclasses.sort() \n\n# Confusion matrix \nc = confusion_matrix(modidf.actualvalues, modidf.predictedvalues)\nprint(accuracy_score(modidf.actualvalues, modidf.predictedvalues))\nprint_confusion_matrix(c, class_names = classes)","9b94065f":"# Classification report \nclasses = modidf.actualvalues.unique()\nclasses.sort()    \nprint(classification_report(modidf.actualvalues, modidf.predictedvalues, target_names=classes))","ebad4897":"---------------\n#### Gender accuracy result \nif you notice, that the gender classification is more accurate. So lets group them up and measure the accuracy again?","92694f31":"<a id=\"modelling\"><\/a>\n## 3. Modelling\nWe're going to use the same model architecture that was in part 3 so we can compare apples with apples...thou a note of caution that the model build will take close to an hour even with GPU","3b4e7514":"Honestly, I'm not exactly sure how this pitch augmentation works. so I'll need to do more reading on this. But safe to say it's another way of augmenting the data. If you listen to it, you can hear the difference. \n\n<a id=\"dynamic\"><\/a>\n### Dynamic change\nDynamic change.... ","31bdaf61":"Then append the labels to it before we run the accuracy measure...","c79d92fb":"<a id=\"static\"><\/a>\n### Static noise \nThe first augmentation method we'll do is to add static noise in the background. Here's how it sounds...","a9354bba":"# <center>Audio Emotion Recognition<\/center>\n## <center>Part 5 - Data augmentation<\/center>\n#### <center> 7th September 2019 <\/center> \n#### <center> 19th August 2019, Eu Jin Lok <\/center> \n#### <center> 24th March 2021, Yuan-Fu Liao <\/center> ","6ce9e55c":"Great so the shape of all the 3 datasets are the same. Thats expected. So our new dataset once stacked on top, will be 3 times the original size, which is handy since Deep Learning needs alot of data","780579b0":"So how well have we done? Have we made a significant improvement? \n\n#### Emotion by gender accuracy  \nSo lets visualise how well we have done for the Emotion by Gender model","597ee396":"<a id=\"explore\"><\/a>\n## 1. Explore augmentation method\nSo before we go full scale application of the augmentation methods, lets take one audio file and run it through all the different types to get a feel for how they work. From there we'll then take a few forward for our model training and hopefully it improves our accuracy.\n\nWe'll start with the orginal audio file untouched...","46244555":"## Introduction \nContinuing where we left off in [Part 3](https:\/\/www.kaggle.com\/yfliao\/audio-emotion-part-3-baseline-model) where we built a simple baseline model, now we're going to take the next level up and build in some data augmentation methods. I did some reading and desk reseach and there's a few good articles around on this:\n\n- [Edward Ma](https:\/\/medium.com\/@makcedward\/data-augmentation-for-audio-76912b01fdf6)\n- [Qishen Ha](https:\/\/www.kaggle.com\/haqishen\/augmentation-methods-for-audio)\n- [Reza Chu](https:\/\/towardsdatascience.com\/speech-emotion-recognition-with-convolution-neural-network-1e6bb7130ce3)\n\nThanks to the various authors above, I've incorporated their methods into this notebook here. We'll go ahead and test a few methods for the authors, then implement some using the same 1D CNN model we used before so we can compare apples with apples exactly how much the accuracy contribution came from the data augmentation methods. \n\n1. [Explore augmentation methods](#explore)\n    - [Static noise](#static)\n    - [Shift](#shift)\n    - [Stretch](#stretch)\n    - [Pitch](#pitch)\n    - [Dynamic change](#dynamic)\n    - [Speed and pitch](#speed)\n2. [Data preparation and processing](#data)\n3. [Modelling](#modelling)\n4. [Model serialisation](#serialise)\n5. [Model validation](#validation)\n6. [Final thoughts](#final)\n\nMost importantly, I want to thank the 4 authors for their excellent dataset, without it, writing this notebook could not have been possible. The original source of the dataset links are below:\n- [TESS](https:\/\/tspace.library.utoronto.ca\/handle\/1807\/24487)\n- [CREMA-D](https:\/\/github.com\/CheyneyComputerScience\/CREMA-D)\n- [SAVEE](http:\/\/kahlan.eps.surrey.ac.uk\/savee\/Database.html)\n- [RAVDESS](https:\/\/zenodo.org\/record\/1188976#.XYP8CSgzaUk)\n- [RAVDESS_Kaggle](https:\/\/www.kaggle.com\/uwrfkaggler\/ravdess-emotional-speech-audio)","e96c7cee":"With data augmentation we get 80.8% compared to 80.4% without augmentation. So not much difference when differentiating by gender. ","56e80471":"<a id=\"final\"><\/a>\n## 6. Final thoughts \nSo its quite clear that data augmentation does help improve the accuracy albeit slightly. Note that we only introduced 2 augmentation methods. Perhaps if we include more it may make it more accurate. But there comes to a point where we have to consider the trade off between speed and accuracy. \n\nNext section we're going to look at implementing more advance modelling techniques. And of course, we'll be bringing forward this 2 augmentation methods that we just do today","1971aacd":"We now have 48.41%. Yes it is at 150 epochs but even at 100 it would yeild rougly the same results as per illustrated on the plot above. So now lets run it through the test set.","0bd22f90":"<a id=\"serialise\"><\/a>\n## 4. Model serialisation\nSo we serialise the model...","7eb43381":"----------------------------\nSo its not very noticable but what I've done there is move the audio randomly to either the left or right direction, within the fix audio duration. So if you compare this to the original plot, you can see the same audio wave pattern, except there's a tiny bit of delay before the speaker starts speaking. \n\n<a id=\"stretch\"><\/a>\n### Stretch \nNow we go to stretch, my most favourite augmentation method ","8c773bed":"So now after stacking them, we'll go ahead and split the dataset as per the usual...","56de4c9c":"The accuracy for the gender by emotions in Part 3 is 43%, and we got 48%, which is great. A small but not unexpected uplift given we only introduce 2 augmentation methods. ","312034ba":"Notice that I've increased the number of epochs to 150 in this notebook. Now if you recall in Part 3, we set to 100 epochs and the logloss plot shows that it has reached full potential at a logloss of about 1.6 after about 50 epochs, and further epochs doesn't really make it more accuracy anymore. With data augmentation however, it seems to indicate that it hasn't quite plateau yet and could still get better. \n\nAnd so that's why i've increased the number of epochs so it can achieve its full potential. Originally when I set to 100 epochs, the plot indicated it hasn't plateau. So i reran at 150 and you can see cleary now that around 100 epochs diminishing returns sets in. I'm going to keep the final model at 150 instead of 100. ","0f68e722":"Right so ~13 mins for completion, so its definitely longer, almost double the amount of time with the 2 augmentation methods in the loop. Instead of the one data object that we get out, we now get 2 other ones, one for each of the 2 augmentation methods we've used. \n\nWe'll need to stack them on top of each other to make a larger dataset. But before we do so, we need to make sure the number of columns for the 3 datasets are the same. So lets check it out before we stack them \n\n_NOTE: If using the stretch augmentation, the audio duration becomes artifically longer and thus the number of columns will be different from original. So some padding will need to be done to the original dataset_","9f7ee1f7":"----------------------------\nYes I know what you are thinking. It's exactly the same as the original. Yes true, but if you look at the frequency, the wave hits higher frequency notes compared to the original where the min is around -1 and the max is around 1. The min and max of this audio is -6 and 6 respestively. Not exactly sure how useful this is, I'll need to do some more reading.\n\n<a id=\"speed\"><\/a>\n### Speed and pitch\nLast but not least, speed and pitch...","753d6fd4":"... and then we normalise our dataset","8c86bf48":"I really like this augmentation method. It dramatically alters the audio in many ways. It compresses the audio wave but keeping the audio duration the same. If you listen to it, the effect is opposite of the stretch augmentation method. An angry person when applied this augmentation method, to the human ear, will really alter the emotion interpretation of this audio. Not sure if this is counter productive to the algorithm, but lets try it. Another potential, downside is that there will be silence in the later part of the audio.  ","e2b86e14":"This one is one of the more dramatic augmentation methods. The method literally stretches the audio. So the duration is longer, but the audio wave gets strecthed too. Thus introducing and effect that sounds like a slow motion sound. If you look at the audio wave itself, you'll notice that compared to the orginal audio, the strected audio seems to hit a higher frequency note. Thus creating a more diverse data for augmentation. Pretty nifty eh? It does introduce abit of a challenge in the data prep stage cause it lengthens the audio duration. Something to consider especially when doing a 2D CNN. \n \n <a id=\"pitch\"><\/a>\n### Pitch\nI believe, this method accentuates the high pitch notes, by... normalising it sort of. ","3a168dee":"#### Emotion accuracy\nWe'll now ignore the gender part and just super group them into the 7 core emotions. Lets see what we get...","75f7e88d":"54% compared to 49% without data augmentation. ","a2a4be30":"<a id=\"data\"><\/a>\n## 2. Data preparation and processing\nI'm going to use just 2 augmentation method which is the noise, and the speed & pitch method. Just for demo purposes. So now lets apply it across all our audio data. We'll use the same data processing steps we followed in [part 3](https:\/\/www.kaggle.com\/ejlok1\/audio-emotion-part-3-baseline-model), except we add in the augmentation process in the loop. We are going to use the noise and speed pitch augmentation method\n\nActually I have also added one extra enhancement to the code, which is a progress bar using the tqdm library. Given that we're adding a few extra processing steps, it would be worthwhile to know how long we've progressed...","3eabf301":"Lets write the predictions out into a file for re-use","662ab88c":"<a id=\"validation\"><\/a>\n## 5. Model validation\nSo if you recall in part 3, the model accuracy at 100 epochs was at 43.80%... ","8d11fab6":"... make the dataset keras compatible","8efb69fa":"-------------------------\nThat's sounds like a good augmentation, not too much static such that it doesn't obfuscate the signal too much. \n\n<a id=\"shift\"><\/a>\n### Shift\nNext method is shift..."}}