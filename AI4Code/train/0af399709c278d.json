{"cell_type":{"b1420947":"code","11682dfb":"code","64cc91cf":"code","20565547":"code","c9d0d91a":"code","391d63f4":"code","33565576":"code","d35ce3e5":"code","49624287":"code","320c4238":"code","56ebc640":"code","44496f5f":"code","554b038b":"code","958dedec":"code","fa535129":"code","6cd9aa53":"code","13c77c9f":"code","a446879d":"code","686f05da":"code","fa5671ed":"code","bf37081e":"code","a3ea4580":"code","d272710b":"code","d6d09e61":"code","e71751b5":"code","0a0d881a":"code","04cefbed":"code","5fee6465":"code","b720baff":"code","c6ed3908":"code","4ae6c9b9":"code","c22ce5d5":"code","7360d42a":"code","7e6e4579":"code","d8eb9502":"code","d190fcd9":"code","71d56927":"code","91e5076f":"code","b92ea74a":"code","e11c5524":"code","69fa9762":"code","66947ba1":"markdown","febb2e0d":"markdown","d318f120":"markdown","77052858":"markdown","332f8c01":"markdown","7ec06007":"markdown","ed2ef7a5":"markdown","2e5b85af":"markdown","c9bc8314":"markdown","4bf47982":"markdown","a5a2d152":"markdown","f6fdedf4":"markdown","bb60727c":"markdown","daa97069":"markdown","8d7af748":"markdown","771e6db1":"markdown","f820220e":"markdown","50b23fa5":"markdown","372feed9":"markdown","201454ae":"markdown","2ae86e6c":"markdown","7b6a3813":"markdown"},"source":{"b1420947":"# Supressing the warning messages\nimport warnings\nwarnings.filterwarnings('ignore')","11682dfb":"# Reading the dataset\nimport pandas as pd\nimport numpy as np\nBikeRentData=pd.read_csv('..\/input\/bikerentdata\/BikeRentData.csv', encoding='latin')\nprint('Shape before deleting duplicate values:',BikeRentData.shape)\n\n# Removing duplicate rows if any\nBikeRentData=BikeRentData.drop_duplicates()\nprint('Shape After deleting duplicate values:', BikeRentData.shape)\n\n# Printing sample data\n# Start observing the Quantitative\/Categorical\/Qualitative variables\nBikeRentData.head(10)","64cc91cf":"%matplotlib inline\n# Creating Bar chart as the Target variable is Continuous\nBikeRentData['cnt'].hist()","20565547":"# Looking at sample rows in the data\nBikeRentData.head()","c9d0d91a":"# Observing the summarized information of data\n# Data types, Missing values based on number of non-null values Vs total rows etc.\n# Remove those variables from data which have too many missing values (Missing Values > 30%)\n# Remove Qualitative variables which cannot be used in Machine Learning\nBikeRentData.info()","391d63f4":"# Looking at the descriptive statistics of the data\nBikeRentData.describe(include='all')","33565576":"UselessColumns = ['yr',]\nBikeRentData = BikeRentData.drop(UselessColumns,axis=1)\nBikeRentData.head()","d35ce3e5":"# Finging unique values for each column\n# TO understand which column is categorical and which one is Continuous\n# Typically if the numer of unique values are < 20 then the variable is likely to be a category otherwise continuous\nBikeRentData.nunique()","49624287":"# Plotting multiple bar charts at once for categorical variables\n# Since there is no default function which can plot bar charts for multiple columns at once\n# we are defining our own function for the same\n\ndef PlotBarCharts(inpData, colsToPlot):\n    %matplotlib inline\n    \n    import matplotlib.pyplot as plt\n    \n    # Generating multiple subplots\n    fig, subPlot=plt.subplots(nrows=1, ncols=len(colsToPlot), figsize=(20,5))\n    fig.suptitle('Bar charts of: '+ str(colsToPlot))\n\n    for colName, plotNumber in zip(colsToPlot, range(len(colsToPlot))):\n        inpData.groupby(colName).size().plot(kind='bar',ax=subPlot[plotNumber])","320c4238":"#####################################################################\n# Calling the function\nPlotBarCharts(inpData=BikeRentData, colsToPlot=['holiday', 'workingday', 'weathersit','hr','season' , 'mnth','weekday'])","56ebc640":"# Plotting histograms of multiple columns together\nBikeRentData.hist(['temp','atemp','hum','windspeed','registered'], figsize=(18,10))","44496f5f":"# Finding how many missing values are there for each column\nBikeRentData.isnull().sum()","554b038b":"ContinuousCols=['temp','atemp','hum','windspeed','registered']\n\n# Plotting scatter chart for each predictor vs the target variable\nfor predictor in ContinuousCols:\n    BikeRentData.plot.scatter(x=predictor, y='cnt', figsize=(10,5), title=predictor+\" VS \"+ 'cnt')","958dedec":"# Calculating correlation matrix\nContinuousCols=['cnt','temp','atemp','hum','windspeed','registered']\n\n# Creating the correlation matrix\nCorrelationData=BikeRentData[ContinuousCols].corr()\nCorrelationData","fa535129":"# Filtering only those columns where absolute correlation > 0.5 with Target Variable\n# reduce the 0.5 threshold if no variable is selected\nCorrelationData['cnt'][abs(CorrelationData['cnt']) > 0.5 ]","6cd9aa53":"# Box plots for Categorical Target Variable \"Price\" and continuous predictors\nCategoricalColsList=['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n\nimport matplotlib.pyplot as plt\nfig, PlotCanvas=plt.subplots(nrows=1, ncols=len(CategoricalColsList), figsize=(18,5))\n\n# Creating box plots for each continuous predictor against the Target Variable \"Price\"\nfor PredictorCol , i in zip(CategoricalColsList, range(len(CategoricalColsList))):\n    BikeRentData.boxplot(column='cnt', by=PredictorCol, figsize=(5,5), vert=True, ax=PlotCanvas[i])","13c77c9f":"# Defining a function to find the statistical relationship with all the categorical variables\ndef FunctionAnova(inpData, TargetVariable, CategoricalPredictorList):\n    from scipy.stats import f_oneway\n\n    # Creating an empty list of final selected predictors\n    SelectedPredictors=[]\n    \n    print('##### ANOVA Results ##### \\n')\n    for predictor in CategoricalPredictorList:\n        CategoryGroupLists=inpData.groupby(predictor)[TargetVariable].apply(list)\n        AnovaResults = f_oneway(*CategoryGroupLists)\n        \n        # If the ANOVA P-Value is <0.05, that means we reject H0\n        if (AnovaResults[1] < 0.05):\n            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n            SelectedPredictors.append(predictor)\n        else:\n            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', AnovaResults[1])\n    \n    return(SelectedPredictors)","a446879d":"# Calling the function to check which categorical variables are correlated with target\n# Calling the function to check which categorical variables are correlated with target\nCategoricalPredictorList=['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\nFunctionAnova(inpData=BikeRentData, \n              TargetVariable='cnt', \n              CategoricalPredictorList=CategoricalPredictorList)","686f05da":"SelectedColumns=['registered','season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit']\n\n# Selecting final columns\nDataForML=BikeRentData[SelectedColumns]\nDataForML.head()","fa5671ed":"# Saving this final data for reference during deployment\nDataForML.to_pickle('DataForML.pkl')","bf37081e":"# Treating all the nominal variables at once using dummy variables\nDataForML_Numeric=pd.get_dummies(DataForML)\n\n# Adding Target Variable to the data\nDataForML_Numeric['cnt']=BikeRentData['cnt']\n\n# Printing sample rows\nDataForML_Numeric.head()","a3ea4580":"# Printing all the column names for our reference\nDataForML_Numeric.columns","d272710b":"# Separate Target Variable and Predictor Variables\nTargetVariable='cnt'\nPredictors=['registered', 'season', 'mnth', 'hr', 'holiday',\n       'weekday', 'workingday', 'weathersit']\n\nX=DataForML_Numeric[Predictors].values\ny=DataForML_Numeric[TargetVariable].values\n\n# Split the data into training and testing set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=428)","d6d09e61":"### Sandardization of data ###\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n# Choose either standardization or Normalization\n# On this data Min Max Normalization produced better results\n\n# Choose between standardization and MinMAx normalization\n#PredictorScaler=StandardScaler()\nPredictorScaler=MinMaxScaler()\n\n# Storing the fit object for later reference\nPredictorScalerFit=PredictorScaler.fit(X)\n\n# Generating the standardized values of X\nX=PredictorScalerFit.transform(X)\n\n# Split the data into training and testing set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","e71751b5":"# Sanity check for the sampled data\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","0a0d881a":"# Multiple Linear Regression\nfrom sklearn.linear_model import LinearRegression\nRegModel = LinearRegression()\n\n# Printing all the parameters of Linear regression\nprint(RegModel)\n\n# Creating the model on Training Data\nLREG=RegModel.fit(X_train,y_train)\nprediction=LREG.predict(X_test)\n\n# Taking the standardized values to original scale\n\n\nfrom sklearn import metrics\n# Measuring Goodness of fit in Training data\nprint('R2 Value:',metrics.r2_score(y_train, LREG.predict(X_train)))\n\n###########################################################################\nprint('\\n##### Model Validation and Accuracy Calculations ##########')\n\n# Printing some sample values of prediction\nTestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)\nTestingDataResults[TargetVariable]=y_test\nTestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)\n\n# Printing sample prediction values\nprint(TestingDataResults[[TargetVariable,'Predicted'+TargetVariable]].head())\n\n# Calculating the error for each row\nTestingDataResults['APE']=100 * ((abs(\n  TestingDataResults['cnt']-TestingDataResults['Predictedcnt']))\/TestingDataResults['cnt'])\n\nMAPE=np.mean(TestingDataResults['APE'])\nMedianMAPE=np.median(TestingDataResults['APE'])\n\nAccuracy =100 - MAPE\nMedianAccuracy=100- MedianMAPE\nprint('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier\nprint('Median Accuracy on test data:', MedianAccuracy)\n\n\n# Defining a custom function to calculate accuracy\n# Make sure there are no zeros in the Target variable if you are using MAPE\ndef Accuracy_Score(orig,pred):\n    MAPE = np.mean(100 * (np.abs(orig-pred)\/orig))\n    #print('#'*70,'Accuracy:', 100-MAPE)\n    return(100-MAPE)\n\n# Custom Scoring MAPE calculation\nfrom sklearn.metrics import make_scorer\ncustom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))","04cefbed":"# Decision Trees (Multiple if-else statements!)\nfrom sklearn.tree import DecisionTreeRegressor\nRegModel = DecisionTreeRegressor(max_depth=8,criterion='mse')\n# Good Range of Max_depth = 2 to 20\n\n# Printing all the parameters of Decision Tree\nprint(RegModel)\n\n# Creating the model on Training Data\nDT=RegModel.fit(X_train,y_train)\nprediction=DT.predict(X_test)\n\nfrom sklearn import metrics\n# Measuring Goodness of fit in Training data\nprint('R2 Value:',metrics.r2_score(y_train, DT.predict(X_train)))\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(DT.feature_importances_, index=Predictors)\nfeature_importances.nlargest(10).plot(kind='barh')\n\n###########################################################################\nprint('\\n##### Model Validation and Accuracy Calculations ##########')\n\n# Printing some sample values of prediction\nTestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)\nTestingDataResults[TargetVariable]=y_test\nTestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)\n\n# Printing sample prediction values\nprint(TestingDataResults[[TargetVariable,'Predicted'+TargetVariable]].head())\n\n# Calculating the error for each row\nTestingDataResults['APE']=100 * ((abs(\n  TestingDataResults['cnt']-TestingDataResults['Predictedcnt']))\/TestingDataResults['cnt'])\n\nMAPE=np.mean(TestingDataResults['APE'])\nMedianMAPE=np.median(TestingDataResults['APE'])\n\nAccuracy =100 - MAPE\nMedianAccuracy=100- MedianMAPE\nprint('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier\nprint('Median Accuracy on test data:', MedianAccuracy)\n\n\n# Defining a custom function to calculate accuracy\n# Make sure there are no zeros in the Target variable if you are using MAPE\ndef Accuracy_Score(orig,pred):\n    MAPE = np.mean(100 * (np.abs(orig-pred)\/orig))\n    #print('#'*70,'Accuracy:', 100-MAPE)\n    return(100-MAPE)\n\n# Custom Scoring MAPE calculation\nfrom sklearn.metrics import make_scorer\ncustom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))","5fee6465":"# Random Forest (Bagging of multiple Decision Trees)\nfrom sklearn.ensemble import RandomForestRegressor\nRegModel = RandomForestRegressor(max_depth=10, n_estimators=100,criterion='mse')\n# Good range for max_depth: 2-10 and n_estimators: 100-1000\n\n# Printing all the parameters of Random Forest\nprint(RegModel)\n\n# Creating the model on Training Data\nRF=RegModel.fit(X_train,y_train)\nprediction=RF.predict(X_test)\n\nfrom sklearn import metrics\n# Measuring Goodness of fit in Training data\nprint('R2 Value:',metrics.r2_score(y_train, RF.predict(X_train)))\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(RF.feature_importances_, index=Predictors)\nfeature_importances.nlargest(10).plot(kind='barh')\n\n###########################################################################\nprint('\\n##### Model Validation and Accuracy Calculations ##########')\n\n# Printing some sample values of prediction\nTestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)\nTestingDataResults[TargetVariable]=y_test\nTestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)\n\n# Printing sample prediction values\nprint(TestingDataResults[[TargetVariable,'Predicted'+TargetVariable]].head())\n\n# Calculating the error for each row\nTestingDataResults['APE']=100 * ((abs(\n  TestingDataResults['cnt']-TestingDataResults['Predictedcnt']))\/TestingDataResults['cnt'])\n\nMAPE=np.mean(TestingDataResults['APE'])\nMedianMAPE=np.median(TestingDataResults['APE'])\n\nAccuracy =100 - MAPE\nMedianAccuracy=100- MedianMAPE\nprint('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier\nprint('Median Accuracy on test data:', MedianAccuracy)\n\n\n# Defining a custom function to calculate accuracy\n# Make sure there are no zeros in the Target variable if you are using MAPE\ndef Accuracy_Score(orig,pred):\n    MAPE = np.mean(100 * (np.abs(orig-pred)\/orig))\n    #print('#'*70,'Accuracy:', 100-MAPE)\n    return(100-MAPE)\n\n# Custom Scoring MAPE calculation\nfrom sklearn.metrics import make_scorer\ncustom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))","b720baff":"# Adaboost (Boosting of multiple Decision Trees)\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Choosing Decision Tree with 1 level as the weak learner\nDTR=DecisionTreeRegressor(max_depth=10)\nRegModel = AdaBoostRegressor(n_estimators=100, base_estimator=DTR ,learning_rate=0.04)\n\n# Printing all the parameters of Adaboost\nprint(RegModel)\n\n# Creating the model on Training Data\nAB=RegModel.fit(X_train,y_train)\nprediction=AB.predict(X_test)\n\nfrom sklearn import metrics\n# Measuring Goodness of fit in Training data\nprint('R2 Value:',metrics.r2_score(y_train, AB.predict(X_train)))\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(AB.feature_importances_, index=Predictors)\nfeature_importances.nlargest(10).plot(kind='barh')\n\n###########################################################################\nprint('\\n##### Model Validation and Accuracy Calculations ##########')\n\n# Printing some sample values of prediction\nTestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)\nTestingDataResults[TargetVariable]=y_test\nTestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)\n\n# Printing sample prediction values\nprint(TestingDataResults[[TargetVariable,'Predicted'+TargetVariable]].head())\n\n# Calculating the error for each row\nTestingDataResults['APE']=100 * ((abs(\n  TestingDataResults['cnt']-TestingDataResults['Predictedcnt']))\/TestingDataResults['cnt'])\n\nMAPE=np.mean(TestingDataResults['APE'])\nMedianMAPE=np.median(TestingDataResults['APE'])\n\nAccuracy =100 - MAPE\nMedianAccuracy=100- MedianMAPE\nprint('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier\nprint('Median Accuracy on test data:', MedianAccuracy)\n\n\n# Defining a custom function to calculate accuracy\n# Make sure there are no zeros in the Target variable if you are using MAPE\ndef Accuracy_Score(orig,pred):\n    MAPE = np.mean(100 * (np.abs(orig-pred)\/orig))\n    #print('#'*70,'Accuracy:', 100-MAPE)\n    return(100-MAPE)\n\n# Custom Scoring MAPE calculation\nfrom sklearn.metrics import make_scorer\ncustom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))","c6ed3908":"# Xtreme Gradient Boosting (XGBoost)\nfrom xgboost import XGBRegressor\nRegModel=XGBRegressor(max_depth=10, \n                      learning_rate=0.1, \n                      n_estimators=100, \n                      objective='reg:linear', \n                      booster='gbtree')\n\n# Printing all the parameters of XGBoost\nprint(RegModel)\n\n# Creating the model on Training Data\nXGB=RegModel.fit(X_train,y_train)\nprediction=XGB.predict(X_test)\n\nfrom sklearn import metrics\n# Measuring Goodness of fit in Training data\nprint('R2 Value:',metrics.r2_score(y_train, XGB.predict(X_train)))\n\n# Plotting the feature importance for Top 10 most important columns\n%matplotlib inline\nfeature_importances = pd.Series(XGB.feature_importances_, index=Predictors)\nfeature_importances.nlargest(10).plot(kind='barh')\n###########################################################################\nprint('\\n##### Model Validation and Accuracy Calculations ##########')\n\n# Printing some sample values of prediction\nTestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)\nTestingDataResults[TargetVariable]=y_test\nTestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)\n\n# Printing sample prediction values\nprint(TestingDataResults[[TargetVariable,'Predicted'+TargetVariable]].head())\n\n# Calculating the error for each row\nTestingDataResults['APE']=100 * ((abs(\n  TestingDataResults['cnt']-TestingDataResults['Predictedcnt']))\/TestingDataResults['cnt'])\n\n\nMAPE=np.mean(TestingDataResults['APE'])\nMedianMAPE=np.median(TestingDataResults['APE'])\n\nAccuracy =100 - MAPE\nMedianAccuracy=100- MedianMAPE\nprint('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier\nprint('Median Accuracy on test data:', MedianAccuracy)\n\n\n# Defining a custom function to calculate accuracy\n# Make sure there are no zeros in the Target variable if you are using MAPE\ndef Accuracy_Score(orig,pred):\n    MAPE = np.mean(100 * (np.abs(orig-pred)\/orig))\n    #print('#'*70,'Accuracy:', 100-MAPE)\n    return(100-MAPE)\n\n# Custom Scoring MAPE calculation\nfrom sklearn.metrics import make_scorer\ncustom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))","4ae6c9b9":"# K-Nearest Neighbor(KNN)\nfrom sklearn.neighbors import KNeighborsRegressor\nRegModel = KNeighborsRegressor(n_neighbors=2)\n\n# Printing all the parameters of KNN\nprint(RegModel)\n\n# Creating the model on Training Data\nKNN=RegModel.fit(X_train,y_train)\nprediction=KNN.predict(X_test)\n\nfrom sklearn import metrics\n# Measuring Goodness of fit in Training data\nprint('R2 Value:',metrics.r2_score(y_train, KNN.predict(X_train)))\n\n# Plotting the feature importance for Top 10 most important columns\n# The variable importance chart is not available for KNN\n\n###########################################################################\nprint('\\n##### Model Validation and Accuracy Calculations ##########')\n\n# Printing some sample values of prediction\nTestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)\nTestingDataResults[TargetVariable]=y_test\nTestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)\n\n# Printing sample prediction values\nprint(TestingDataResults[[TargetVariable,'Predicted'+TargetVariable]].head())\n\n# Calculating the error for each row\nTestingDataResults['APE']=100 * ((abs(\n  TestingDataResults['cnt']-TestingDataResults['Predictedcnt']))\/TestingDataResults['cnt'])\n\nMAPE=np.mean(TestingDataResults['APE'])\nMedianMAPE=np.median(TestingDataResults['APE'])\n\nAccuracy =100 - MAPE\nMedianAccuracy=100- MedianMAPE\nprint('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier\nprint('Median Accuracy on test data:', MedianAccuracy)\n\n# Defining a custom function to calculate accuracy\n# Make sure there are no zeros in the Target variable if you are using MAPE\ndef Accuracy_Score(orig,pred):\n    MAPE = np.mean(100 * (np.abs(orig-pred)\/orig))\n    #print('#'*70,'Accuracy:', 100-MAPE)\n    return(100-MAPE)\n\n# Custom Scoring MAPE calculation\nfrom sklearn.metrics import make_scorer\ncustom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)\n\n# Importing cross validation function from sklearn\nfrom sklearn.model_selection import cross_val_score\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))","c22ce5d5":"# Separate Target Variable and Predictor Variables\nTargetVariable='cnt'\n\n# Selecting the final set of predictors for the deployment\n# Based on the variable importance charts of multiple algorithms above\nPredictors=['registered', 'mnth', 'hr', 'weekday']\n\nX=DataForML_Numeric[Predictors].values\ny=DataForML_Numeric[TargetVariable].values\n\n### Sandardization of data ###\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n# Choose either standardization or Normalization\n# On this data Min Max Normalization produced better results\n\n# Choose between standardization and MinMAx normalization\n#PredictorScaler=StandardScaler()\nPredictorScaler=MinMaxScaler()\n\n# Storing the fit object for later reference\nPredictorScalerFit=PredictorScaler.fit(X)\n\n# Generating the standardized values of X\nX=PredictorScalerFit.transform(X)\n\nprint(X.shape)\nprint(y.shape)","7360d42a":"from sklearn.model_selection import cross_val_score\n\n# Using final hyperparameters\n# Xtreme Gradient Boosting (XGBoost)\nfrom xgboost import XGBRegressor\nRegModel=XGBRegressor(max_depth=10, \n                      learning_rate=0.1, \n                      n_estimators=100, \n                      objective='reg:linear', \n                      booster='gbtree')\n\n# Running 10-Fold Cross validation on a given algorithm\n# Passing full data X and y because the K-fold will split the data and automatically choose train\/test\nAccuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)\nprint('\\nAccuracy values for 10-fold Cross Validation:\\n',Accuracy_Values)\nprint('\\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))","7e6e4579":"# Training the model on 100% Data available\nFinal_XGB_Model=RegModel.fit(X,y)","d8eb9502":"import pickle\nimport os\n\n# Saving the Python objects as serialized files can be done using pickle library\n# Here let us save the Final model\nwith open('Final_XGB_Model.pkl', 'wb') as fileWriteStream:\n    pickle.dump(Final_XGB_Model, fileWriteStream)\n    # Don't forget to close the filestream!\n    fileWriteStream.close()\n    \nprint('pickle file of Predictive Model is saved at Location:',os.getcwd())","d190fcd9":"# This Function can be called from any from any front end tool\/website\ndef FunctionPredictResult(InputData):\n    import pandas as pd\n    Num_Inputs=InputData.shape[0]\n    \n    # Making sure the input data has same columns as it was used for training the model\n    # Also, if standardization\/normalization was done, then same must be done for new input\n    \n    # Appending the new data with the Training data\n    DataForML=pd.read_pickle('DataForML.pkl')\n    InputData=InputData.append(DataForML)\n    \n    # Generating dummy variables for rest of the nominal variables\n    InputData=pd.get_dummies(InputData)\n            \n    # Maintaining the same order of columns as it was during the model training\n    Predictors=['registered', 'mnth', 'hr', 'weekday']\n    \n    # Generating the input values to the model\n    X=InputData[Predictors].values[0:Num_Inputs]\n    \n    # Generating the standardized values of X since it was done while model training also\n    X=PredictorScalerFit.transform(X)\n    \n    # Loading the Function from pickle file\n    import pickle\n    with open('Final_XGB_Model.pkl', 'rb') as fileReadStream:\n        PredictionModel=pickle.load(fileReadStream)\n        # Don't forget to close the filestream!\n        fileReadStream.close()\n            \n    # Gencnt Predictions\n    Prediction=PredictionModel.predict(X)\n    PredictionResult=pd.DataFrame(Prediction, columns=['Prediction'])\n    return(round(PredictionResult))","71d56927":"# Calling the function for some loan applications\nNewSampleData=pd.DataFrame(\ndata=[[32,1,1,6],\n     [32,1,1,4]],\ncolumns=['registered', 'mnth', 'hr', 'weekday'])\n\nprint(NewSampleData)\n\n# Calling the Function for prediction\nFunctionPredictResult(InputData= NewSampleData)","91e5076f":"def FunctionGeneratePrediction(inp_registered, inp_mnth, inp_hr, inp_weekday):\n    \n    # Creating a data frame for the model input\n    SampleInputData=pd.DataFrame(\n     data=[[inp_registered, inp_mnth, inp_hr, inp_weekday]],\n     columns=['registered', 'mnth', 'hr', 'weekday'])\n\n    # Calling the function defined above using the input parameters\n    Predictions=FunctionPredictResult(InputData= SampleInputData)\n\n    # Returning the predictions\n    return(Predictions.to_json())\n\n# Function call\nFunctionGeneratePrediction(  inp_registered=32,\n                             inp_mnth =1,\n                             inp_hr=1,\n                             inp_weekday=6\n                             )","b92ea74a":"from flask import Flask, request, jsonify\nimport pickle\nimport pandas as pd\nimport numpy","e11c5524":"app = Flask(__name__)\n\n@app.route('\/prediction_api', methods=[\"GET\"])\ndef prediction_api():\n    try:\n        # Getting the paramters from API call\n        registered_value = float(request.args.get('registered'))\n        mnth_value=float(request.args.get('mnth'))\n        hr_value=float(request.args.get('hr'))\n        weekday_value=float(request.args.get('weekday'))\n                \n        # Calling the funtion to get predictions\n        prediction_from_api=FunctionGeneratePrediction(\n                                                     inp_registered=registered_value,\n                                                     inp_mnth =mnth_value,\n                                                     inp_hr=hr_value,\n                                                     inp_weekday=weekday_value\n                                                )\n\n        return (prediction_from_api)\n    \n    except Exception as e:\n        return('Something is not right!:'+str(e))","69fa9762":"import os\nif __name__ ==\"__main__\":\n    \n    # Hosting the API in localhost\n    app.run(host='127.0.0.1', port=8080, threaded=True, debug=True, use_reloader=False)\n    # Interrupt kernel to stop the API","66947ba1":"## Converting the nominal variable to numeric using get_dummies()","febb2e0d":"# Decision Trees","d318f120":"# Basic Data Exploration","77052858":"# Relationship exploration: Categorical Vs Continuous -- Box Plots\nWhen the target variable is Continuous and the predictor variable is Categorical we analyze the relation using Boxplots and measure the strength of relation using Anova test","332f8c01":"# Starting the API engine","7ec06007":"# Deployment of the Model","ed2ef7a5":"# XGBoost","2e5b85af":"# Adaboost","c9bc8314":"# Multiple Linear Regression","4bf47982":"# Standardization\/Normalization of data\nYou can choose not to run this step if you want to compare the resultant accuracy of this transformation with the accuracy of raw data. \n\nHowever, if you are using KNN or Neural Networks, then this step becomes necessary.","a5a2d152":"# Selecting final predictors for Machine Learning\nBased on the above tests, selecting the final columns for machine learning","f6fdedf4":"# KNN","bb60727c":"### Visualize distribution of all the Continuous Predictor variables in the data using histograms\nBased on the Basic Data Exploration, Three continuous predictor variables 'ApplicantIncome', 'CoapplicantIncome',and 'LoanAmount'.","daa97069":"# Feature Selection","8d7af748":"# Creating Flask API","771e6db1":"# Visual Exploratory Data Analysis","f820220e":"# Looking at the distribution of Target variable","50b23fa5":"# Function for predictions API\n","372feed9":"# Random Forest","201454ae":"Data description\u00b6\nThe business meaning of each column in the data is as below\n\nseason: The current season (1:winter, 2:spring, 3:summer, 4:fall)\n\nyr: year (0: 2011, 1:2012)\n\nmnth: month ( 1 to 12)\n\nhr: hour of the day (0 to 23)\n\nholiday: weather day is holiday or not\n\nweekday: day of the week\n\nworkingday: if day is neither weekend nor holiday is 1, otherwise is 0\n\nweathersit: The Weather forecast for the day\n\n1: Clear, Few clouds, Partly cloudy, Partly cloudy\n\n2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n\n3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n\n4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog\n\ntemp: Normalized temperature in Celsius.\n\natemp: Normalized feeling temperature in Celsius.\n\nhum: Normalized humidity. The values are divided to 100 (max)\n\nwindspeed: Normalized wind speed. The values are divided to 67 (max)\n\ncasual: count of casual users\n\nregistered: count of registered users\n\ncnt: count of total rental bikes including both casual and registered\n","2ae86e6c":"I am choosing XGBOOST as the final model since it is producing the best accuracy on this data.","7b6a3813":"# Machine Learning: Splitting the data into Training and Testing sample\nWe dont use the full data for creating the model. Some data is randomly selected and kept aside for checking how good the model is. This is known as Testing Data and the remaining data is called Training data on which the model is built. Typically 70% of data is used as Training data and the rest 30% is used as Tesing data."}}