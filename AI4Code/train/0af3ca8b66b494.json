{"cell_type":{"74874e41":"code","9ca851f2":"code","d14b855e":"code","3248b899":"code","a2c09be0":"code","729f2b42":"code","af4ae3ad":"code","b7c5ac20":"code","afa1c014":"code","8d17e90c":"code","fff29a62":"code","6cd3950a":"code","b59df5bc":"code","d1d959d4":"code","f12bdc8c":"code","42b720e7":"markdown"},"source":{"74874e41":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport h5py\nimport matplotlib.pyplot as plt\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# Any results you write to the current directory are saved as output.","9ca851f2":"def load_data():\n    train_dataset = h5py.File('\/kaggle\/input\/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n    \n    test_dataset = h5py.File('\/kaggle\/input\/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n    \n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes","d14b855e":"def initialize_parameters(layers_dims_vector):\n    no_of_layers = len(layers_dims_vector)\n    parameters={}\n    for l in range(1,no_of_layers):\n        parameters['W'+str(l)]=np.random.randn(layers_dims_vector[l],layers_dims_vector[l-1])*0.01\n        parameters['b'+str(l)]=np.zeros((layers_dims_vector[l],1))\n    return parameters","3248b899":"def sigmoid(z):\n    sigmoid = 1\/(1 + np.exp(-z)) \n    return sigmoid,z\n\ndef relu(z):\n    A = np.maximum(0,z)\n    assert(A.shape == z.shape)\n    return A,z","a2c09be0":"def linear_forward(W,b,A):\n    Z = np.dot(W,A)+b\n    cache = (A,W,b)\n    return Z,cache\n        ","729f2b42":"def linear_activation_forward(A_prev,W,b,activation):\n    if(activation==\"relu\"):\n        Z,linear_cache = linear_forward(W,b,A_prev)\n        A,activation_cache = relu(Z)\n    elif(activation==\"sigmoid\"):\n        Z,linear_cache = linear_forward(W,b,A_prev)\n        A,activation_cache = sigmoid(Z)\n    cache = (linear_cache,activation_cache)\n    return A,cache\n    ","af4ae3ad":"def forward_prop(parameters,X):\n    caches = []\n    A = X\n    L = len(parameters) \/\/ 2\n    print(L)\n    cache={}\n    for l in range(1,L):\n        A_prev = A\n        A, cache = linear_activation_forward(A_prev,parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],activation=\"relu\")\n        caches.append(cache)\n        \n     # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n    ### START CODE HERE ### (\u2248 2 lines of code)\n    AL, cache = linear_activation_forward(A,parameters[\"W\"+str(L)],parameters[\"b\"+str(L)],activation=\"sigmoid\")\n    caches.append(cache)\n    return AL,caches\n            \n    ","b7c5ac20":"def compute_cost(AL,Y):\n    m=Y.shape[1]\n    cost = (-1 \/ m) * np.sum(np.multiply(Y, np.log(AL)) + np.multiply(1 - Y, np.log(1 - AL)))\n    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n    assert(cost.shape == ())\n    \n    return cost","afa1c014":"def linear_backward(dZ,cache):\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    ### START CODE HERE ### (\u2248 3 lines of code)\n    dW = 1\/m*np.dot(dZ,A_prev.T)\n    db = 1\/m*np.sum(dZ,axis=1,keepdims=True)\n    dA_prev = np.dot(W.T,dZ)\n    ### END CODE HERE ###\n    \n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    \n    return dA_prev, dW, db","8d17e90c":"def sigmoid_backward(dA,cache):\n    Z = cache\n    \n    s = 1\/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)\n    \n    assert (dZ.shape == Z.shape)\n    \n    return dZ\n\ndef relu_backward(dA,cache):\n    Z = cache\n    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n    dZ[Z <= 0] = 0\n    \n    \n    \n    return dZ\n    ","fff29a62":"def linear_activation_backward(dA,cache,activation):\n    linear_cache,activation_cache = cache\n    if(activation==\"sigmoid\"):\n        dZ = sigmoid_backward(dA,activation_cache)\n        dA_prev,dW,db=linear_backward(dZ,linear_cache)\n    elif(activation == \"relu\"):\n        dZ = relu_backward(dA,activation_cache)\n        dA_prev,dW,db=linear_backward(dZ,linear_cache)\n    \n    return dA_prev,dW,db   ","6cd3950a":"def back_prop(AL,Y,caches):\n    grads = {}\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    L = len(caches)\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    current_cache = caches[L-1]\n    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n    ### END CODE HERE ###\n    \n    # Loop from l=L-2 to l=0\n    for l in reversed(range(L-1)):\n        # lth layer: (RELU -> LINEAR) gradients.\n        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n        ### START CODE HERE ### (approx. 5 lines)\n        current_cache = caches[l]\n        #print(current_cache)\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation = \"relu\")\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n        ### END CODE HERE ###\n\n    return grads\n\n    \n    ","b59df5bc":"def update_parameters(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using gradient descent\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients, output of L_model_backward\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n                  parameters[\"W\" + str(l)] = ... \n                  parameters[\"b\" + str(l)] = ...\n    \"\"\"\n    \n    L = len(parameters) \/\/ 2 # number of layers in the neural network\n\n    # Update rule for each parameter. Use a for loop.\n    ### START CODE HERE ### (\u2248 3 lines of code)\n    for l in range(L):\n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n    ### END CODE HERE ###\n    return parameters","d1d959d4":"def l_layer_nn(no_of_iterations,learning_rate):\n    train_x_orig, train_y, test_x_orig, test_y, classes = load_data()\n    train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n    test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n\n    train_x = train_x_flatten\/255\n    test_x = test_x_flatten\/255\n    X = train_x\n    Y= train_y\n    no_of_units = [12288,20,7,5,1]\n\n    parameters = initialize_parameters(no_of_units)\n    #print(parameters)\n    costs = []\n    for i in range(0,no_of_iterations):\n        AL,caches = forward_prop(parameters,X)\n        #print(\"The forward Prop is:\"+str(AL.shape))\n        cost = compute_cost(AL,Y)\n        grads = back_prop(AL,Y,caches)\n        parameters = update_parameters(parameters,grads,learning_rate)\n        print(\"Cost after iteration \"+str(i)+\" is:\"+str(cost))\n        costs.append(cost)\n        \n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n        \n    \n    ","f12bdc8c":"l_layer_nn(3000,0.0075)","42b720e7":"The Data is being loaded as the input is imn h5 file"}}