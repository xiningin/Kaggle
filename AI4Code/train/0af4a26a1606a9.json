{"cell_type":{"a1d5fdda":"code","e3a84c89":"code","2232939f":"code","e3dfa08f":"code","a41d79d7":"code","6689ce81":"code","430a4814":"code","ebd75202":"code","d6090477":"code","ad8c7cf2":"code","c38c3f6e":"code","646407d9":"code","da0975df":"code","6aa78c17":"code","a20a1372":"code","1adc1b7c":"code","a11eb0d1":"code","426614a9":"markdown","5e5b0056":"markdown","e9382365":"markdown","d0375464":"markdown","08580b83":"markdown"},"source":{"a1d5fdda":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e3a84c89":"import matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import NuSVR\nfrom sklearn.metrics import mean_absolute_error","2232939f":"train = pd.read_csv('..\/input\/train.csv', dtype={'acoustic_data': np.int16, 'time_to_failure': np.float64})","e3dfa08f":"train.head()","a41d79d7":"# pandas doesn't show us all the decimals\npd.options.display.precision = 15","6689ce81":"# much better!\ntrain.head()","430a4814":"# Create a training file with simple derived features\n\nrows = 150_000\nsegments = int(np.floor(train.shape[0] \/ rows))\n\nX_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['ave', 'std', 'max', 'min'])\ny_train = pd.DataFrame(index=range(segments), dtype=np.float64,\n                       columns=['time_to_failure'])\n\nfor segment in tqdm(range(segments)):\n    seg = train.iloc[segment*rows:segment*rows+rows]\n    x = seg['acoustic_data'].values\n    y = seg['time_to_failure'].values[-1]\n    \n    y_train.loc[segment, 'time_to_failure'] = y\n    \n    X_train.loc[segment, 'ave'] = x.mean()\n    X_train.loc[segment, 'std'] = x.std()\n    X_train.loc[segment, 'max'] = x.max()\n    X_train.loc[segment, 'min'] = x.min()","ebd75202":"X_train.head()","d6090477":"y_train.head()","ad8c7cf2":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)","c38c3f6e":"svm = NuSVR()\nsvm.fit(X_train_scaled, y_train.values.flatten())\ny_pred = svm.predict(X_train_scaled)","646407d9":"plt.figure(figsize=(6, 6))\nplt.scatter(y_train.values.flatten(), y_pred)\nplt.xlim(0, 20)\nplt.ylim(0, 20)\nplt.xlabel('actual', fontsize=12)\nplt.ylabel('predicted', fontsize=12)\nplt.plot([(0, 0), (20, 20)], [(0, 0), (20, 20)])\nplt.show()","da0975df":"score = mean_absolute_error(y_train.values.flatten(), y_pred)\nprint(f'Score: {score:0.3f}')","6aa78c17":"submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id')","a20a1372":"X_test = pd.DataFrame(columns=X_train.columns, dtype=np.float64, index=submission.index)\n","1adc1b7c":"for seg_id in X_test.index:\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    \n    x = seg['acoustic_data'].values\n    \n    X_test.loc[seg_id, 'ave'] = x.mean()\n    X_test.loc[seg_id, 'std'] = x.std()\n    X_test.loc[seg_id, 'max'] = x.max()\n    X_test.loc[seg_id, 'min'] = x.min()","a11eb0d1":"X_test_scaled = scaler.transform(X_test)\nsubmission['time_to_failure'] = svm.predict(X_test_scaled)\nsubmission.to_csv('submission.csv')","426614a9":"Now lets look at how to handle the test data. There are a bunch of different files and we need to go through all of them and predict!","5e5b0056":"So what we have here is 2 columns. The first column is the signal that we will use for prediction and the second column is the `time_to_failure`. It goes down in equal increments and I expect it eventually will reach 0 when there is an earth quake and then back up to count down to the next earth quake. ","e9382365":"# Introduction\nThis is literally just a forked notebook of the original \"Basic Feature Benchmark\" but I added some comments that I thought may be helpful for others.","d0375464":"Below we break up the training data frame into groups of 150,000 rows. We collect the 150,000 signals and set the target as the `time_to_failure` of the last signal. However, some of the signals may be from previous earthquakes if the 150,000 rows has `time_to_failure` equal to zero in it, but oh well! We just use the 150,000 signals to help predict the time to quake target.","08580b83":"> Our features are the average, standard deviation, max and min of the 150,000 signals. Now we standardize these rows using a Z-score and then throw them into a SVM machine learning model."}}