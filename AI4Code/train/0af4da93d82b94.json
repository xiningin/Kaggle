{"cell_type":{"ffc6d3c5":"code","154d2f58":"code","f8399800":"code","a89735c2":"code","562fa559":"code","1ab843fb":"code","ea7f3d8a":"code","b55978fb":"code","3d9c6d6a":"code","408a5912":"code","43ad3ecf":"code","1e4b19a3":"code","0ffc7186":"code","bb51ab38":"code","6433f1b0":"code","023438b4":"code","5b5d61e8":"code","09ec6294":"code","e5426c03":"code","38081f86":"code","04527186":"code","bdf8ee91":"code","6116f7e0":"code","7d47801e":"code","e74c8757":"code","7d63a1a8":"code","6b9e8cd5":"code","6b7173fa":"code","be943800":"code","e5d997bb":"code","a3a01703":"code","8692c746":"code","d7ff5f19":"code","6ab555f4":"code","815fa435":"code","2cf34228":"code","f399f172":"code","655f5eda":"code","86c4e51a":"code","cd458136":"code","5dfcf299":"code","19fd4b92":"code","2fd38309":"code","8281ae48":"code","3cbe315d":"code","c7f33ca6":"code","d98e1381":"code","f95a4be0":"code","ef504a79":"code","b99efde3":"code","583ec870":"code","0a569a1d":"code","f3d3e71e":"code","7cb5ddfb":"markdown","184b27ca":"markdown","cbf8c1e4":"markdown","497a111e":"markdown","ad7e383e":"markdown","c3b3e340":"markdown","ab7126cb":"markdown","9780aff4":"markdown","79287bfb":"markdown","84b7f384":"markdown","14c6ae95":"markdown","b931109a":"markdown","ca0a5481":"markdown","a08e2fb7":"markdown","ba25632c":"markdown"},"source":{"ffc6d3c5":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn as sk\n","154d2f58":"df=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","f8399800":"df.info","a89735c2":"#lets drop the unwanted columns, id & unknown are not required\ndrop=['id','Unnamed: 32']\nfor i in drop:\n    df=df.drop(i, axis=1)","562fa559":"df.describe()","1ab843fb":"df.shape","ea7f3d8a":"# Diagnosis is the only object type column. We will decode it to 0 & 1. O 0s Benign & 1 is Malign\n\ncategory={'B':0,'M':1}\ndf['diagnosis']=df['diagnosis'].map(category)","b55978fb":"df.head()","3d9c6d6a":"df['diagnosis'].dtype","408a5912":"df.isnull().sum() #there are no null values","43ad3ecf":"df.dtypes","1e4b19a3":"df['diagnosis'].value_counts()","0ffc7186":"#plotting categorical variable\nplt.figure(figsize=(10,8))\ndf['diagnosis'].value_counts().plot(kind='bar',colormap='Blues_r')\nplt.title(f\"Plotting the diagnosis\")\nplt.show()","bb51ab38":"col=df.columns\ncol.tolist()","6433f1b0":"col=col.drop('diagnosis')","023438b4":"col","5b5d61e8":"plt.figure(figsize=(20,20))\nplt.tight_layout()\nfor key, i in enumerate(col):\n    plt.subplot(6,5,key+1)\n    g= sns.distplot(df[i],label='Skewness:{:.2f}'.format(df[i].skew()),kde=True)\n    plt.legend(loc='best')\n    plt.title(f\"Plotting the {i}\")\n    plt.tight_layout()","09ec6294":"plt.figure(figsize=(10,8));\nfor key, i in enumerate(col):\n    g=sns.displot(data=df, x=i, hue=\"diagnosis\", kind=\"kde\")\n    plt.suptitle(f\"Plotting the {i}\")\n    plt.tight_layout()","e5426c03":"df_corr=df.corr()\nplt.figure(figsize=(18,12))\nsns.heatmap(df_corr, annot=True)","38081f86":"#correlation with target variable\nplt.figure(figsize=(12,8))\ndf_corr['diagnosis'].sort_values(ascending=True)[:-1].plot(kind=\"barh\")\nplt.title(\"Correlation with target variable\")\nplt.xlabel(\"Correlation\")\nplt.tight_layout()\nplt.show()","04527186":"#before we begin the PCA, we need to scale the data.\n#split the dataset\nX=df.drop(\"diagnosis\",axis=1)\ny=df[\"diagnosis\"]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","bdf8ee91":"#performing scaling\nfrom sklearn.preprocessing import StandardScaler\nstd=StandardScaler()\nX_train_std=std.fit_transform(X_train)\nX_test_std=std.fit_transform(X_test)","6116f7e0":"#import PCA module\nfrom sklearn.decomposition import PCA\n\npca=PCA(n_components=X_train_std.shape[1])\npca_data=pca.fit_transform(X_train_std)\n\npercent_var_explained = pca.explained_variance_\/(np.sum(pca.explained_variance_))\ncumm_var_explained = np.cumsum(percent_var_explained)\n\nplt.plot(cumm_var_explained)\nplt.grid()\nplt.xlabel(\"n_components\")\nplt.ylabel(\"% variance explained\")\nplt.show()","7d47801e":"cumm_var_explained","e74c8757":"pca.explained_variance_","7d63a1a8":"sum(pca.explained_variance_ratio_)","6b9e8cd5":"pca = PCA(n_components=17)\npca_train_data = pca.fit_transform(X_train_std)\npca_test_data = pca.transform(X_test_std)","6b7173fa":"#correlation after PCA\n\ndf_train_pca = pd.DataFrame(pca_train_data)\ndf_train_pca[\"diagnosis\"] = y_train\n\ncorr = df_train_pca.corr()\nplt.figure(figsize=(18,12))\n\nsns.heatmap(corr, annot = True, vmin=-1, vmax=1, cmap=\"YlGnBu\", linewidths=.5)\nplt.grid(b=True, color='#f68c1f', alpha=0.1)\nplt.show()","be943800":"def pca_dec(data, n):\n  pca = PCA(n)\n  X_dec = pca.fit_transform(data)\n  return X_dec, pca\n\n#Decomposing the train set:\npca_train_results, pca_train = pca_dec(X_train_std, 17)\n\n#Decomposing the test set:\npca_test_results, pca_test = pca_dec(X_test_std, 17)\n\n#Creating a table with the explained variance ratio\nnames_pcas = [f\"PCA Component {i}\" for i in range(1, 18, 1)]\nscree = pd.DataFrame(list(zip(names_pcas, pca_train.explained_variance_ratio_)), columns=[\"Component\", \"Explained Variance Ratio\"])\nprint(scree)","e5d997bb":"#Sorting the values of the first principal component by how large each one is\ndf1 = pd.DataFrame({'PCA':pca_train.components_[0], 'Variable Names':list(X_train.columns)})\ndf1 = df1.sort_values('PCA', ascending=False)\n\n#Sorting the absolute values of the first principal component by magnitude\ndf2 = pd.DataFrame(df1)\ndf2['PCA']=df2['PCA'].apply(np.absolute)\ndf2 = df2.sort_values('PCA', ascending=False)\n\ndf2.head(17)","a3a01703":"import xgboost as xgb\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score","8692c746":"xgb_classifier=xgb.XGBClassifier(random_state=42)","d7ff5f19":"# fit the model (using train & test data from original Dataset)\nxgb_classifier.fit(X_train_std, y_train)","6ab555f4":"# make predictions for test data\ny_pred= xgb_classifier.predict(X_test_std)\npredictions = [round(value) for value in y_pred]","815fa435":"# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","2cf34228":"cm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot= True, fmt = 'd')","f399f172":"#defining training & test data for PCA component.\nX_train_pca= pd.DataFrame(pca_train_data)\nX_test_pca=pd.DataFrame(pca_test_data)","655f5eda":"X_train_pca.shape","86c4e51a":"X_test_pca.shape","cd458136":"#fit the model\n\nxgb_classifier.fit(X_train_pca, y_train)","5dfcf299":"# make predictions for test data\ny_pred_1 = xgb_classifier.predict(X_test_pca)\npredictions = [round(value) for value in y_pred_1]","19fd4b92":"# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","2fd38309":"cm = confusion_matrix(y_test, y_pred_1)\nsns.heatmap(cm, annot= True, fmt = 'd')","8281ae48":"#plotting again the original correlation heatmap\ndf_corr=df.corr()\nplt.figure(figsize=(18,12))\nsns.heatmap(df_corr, annot=True)","3cbe315d":"drop_col=['radius_mean', 'perimeter_mean', 'texture_mean','area_mean', 'compactness_mean', 'concave points_mean',\n'texture_worst',  'concave points_worst', 'perimeter_worst', 'area_worst', 'compactness_worst', \n'area_se', 'radius_se', 'perimeter_se']\n\nnew_df=df.drop(drop_col,axis=1)","c7f33ca6":"new_df.shape","d98e1381":"new_df_corr=new_df.corr()\nplt.figure(figsize=(18,12))\nsns.heatmap(new_df_corr, annot=True)","f95a4be0":"#splitting the new_df\nX_new=new_df.drop(\"diagnosis\",axis=1)\ny_new=new_df[\"diagnosis\"]\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_new, y_new, test_size=0.3)","ef504a79":"std=StandardScaler()\nX_train2_std=std.fit_transform(X_train2)\nX_test2_std=std.fit_transform(X_test2)","b99efde3":"xgb_classifier.fit(X_train2_std, y_train2)","583ec870":"# make predictions for test data\ny_pred_2 = xgb_classifier.predict(X_test2_std)\npredictions = [round(value) for value in y_pred_2]","0a569a1d":"# evaluate predictions\naccuracy = accuracy_score(y_test2, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","f3d3e71e":"cm = confusion_matrix(y_test2, y_pred_2)\nsns.heatmap(cm, annot= True, fmt = 'd')","7cb5ddfb":"##### We have total  of 569 rows & 31 columns to be analysed.\n#### Our target variable is diagnosis, M represents Malign & B represents Benign. \n","184b27ca":"The total variance for data captured for 1st PCA is 0.43, 1st two PCA is 0.63, 1st three  PCA is 0.72 and for first 17 PCA is 0.9916.\n\nFor the individual variance captured the variance of data captured by 1st PCA is 1.303, for 2nd PCA is 6.14522007e+00 , 3rd PCA is 2.72676520e+00, and the last PCA is 1.31517117e-04.\n\n***Since 99.16% of the total variance is captured by the 1st 17 PCA itself, we take only 17 components of PCA and compute a correlation heatmap to overserve the multicollinearity.***","cbf8c1e4":"\n\nHence by reducing the dimensionality of the data using PCA, the variance is preserved by approx 99% and multicollinearity of the data is removed.","497a111e":"#### a. original Dataframe","ad7e383e":"we see multicolinearity in the heatmap, we will examine highly correlated variables.\n\n\nIf we observe; radius_worst, perimeter_worst & area_worst are has correaletion of 0.9 with radius mean. Similarly, if we look at the correlation of **_worst** features with that of **_mean** & **_se** features, the correlation is quite high (between 0.7-.95) which evidently signify that the \"worst\" columns are the subset of the \"mean\" columns which is nothing but both contain similar type of data. \n\n\nMulticollinearity undermines the significance of the independent variables, it is important to treat them before we build the model. \n\n\nWe will use PCA to handle the multicollinearity problem, **Principal Component Analysis(PCA)** is a common feature extraction technique in data science that employs matrix factorization to reduce the dimensionality of data into lower space.\n\n### Dealing with Multicollinearity & feature selection Using PCA\n\n**PCA** technique is particularly useful in processing data where multi-colinearity exists between the features\/variables. PCA can be used when the dimensions of the input features are high.\n\n","c3b3e340":"Now that we know which are the important features that has the most impact, we move on to building the model.\n\nWe will try the accuracy using multiple models on the raw training data & the pca dataframe df_train_pca. Just to undertstand if PCA has helped in increasing the accuracy.\n\n\n### 1. XGBoost Algorithm","ab7126cb":"### Univariate Analysis","9780aff4":"#### 2. PCA Dataframe","79287bfb":"#### You see most of the distributions are left skewed, we need normal distribution. we will look into it later. first lets look into correlation between the variables and specifically with target variable.","84b7f384":"We have to standardize the data before implementing PCA. This is absolutely necessary because PCA calculates a new projection of our data on a new axis using the standard deviation of our data. PCA gives more weight to variables that have higher variances than variables with low variances, so it is important to normalize the data on the same scale to get a reasonable covariance.","14c6ae95":"As we observed, the accuracy using PCA dataframe is 93% whereas for the original dataframe is 96%. This signifies the use of PCA or in other words the use of PCA to reduce the multicollinearity has done no good to accuracy.\n\n#### Now lets try other way, by dropping the highly correlated features and check the accuracy score.\n\nGoing back to where we performed correlation. We had identified the highly correlated features. Lets plot the correlation heatmap again for our understanding.","b931109a":"### Bivariate Analysis","ca0a5481":"The features which are highly correlated, which means they have almost same data.","a08e2fb7":"df.head()","ba25632c":"### c. Dropping the highly correlated features"}}