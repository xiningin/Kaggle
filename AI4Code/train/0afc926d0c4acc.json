{"cell_type":{"a135a13a":"code","b5b49f78":"code","c20bc089":"code","6b7628a6":"code","e82d77df":"code","b54bcf3a":"code","fe8de0af":"code","620f4c29":"code","08893345":"code","eb8e61a8":"code","0b8682c0":"code","bbea2f31":"code","6c7a0534":"code","d8d53899":"code","28c89c33":"markdown","b468286b":"markdown","30e520f0":"markdown","1a20ab36":"markdown","46adc404":"markdown","01f6311b":"markdown","606dc18b":"markdown","afbe2ee8":"markdown","a30d1c4f":"markdown","cba9fe21":"markdown"},"source":{"a135a13a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nfrom tqdm import tqdm_notebook\nimport pickle\nimport gc\nfrom sklearn.model_selection import KFold\n\nimport os\nimport operator\nimport random\nfrom multiprocessing import Pool\nfrom gensim.models import KeyedVectors\nimport re\nfrom tqdm import tqdm\nfrom collections import defaultdict\nimport json\nimport dask.dataframe as ddf\nimport platform\nfrom torch.utils import data\nfrom keras.preprocessing import text, sequence\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\nimport time\nfrom sklearn import metrics\nfrom keras.preprocessing.sequence import pad_sequences","b5b49f78":"tqdm.pandas()\n\n\ndef set_seed(seed: int = 0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef df_parallelize_run(df: pd.DataFrame(), func, npartitions=os.cpu_count()):\n    if platform.system() == 'Windows':\n        dask_dataframe = ddf.from_pandas(df, npartitions=os.cpu_count())\n        result = dask_dataframe.map_partitions(func, meta=df)\n        df = result.compute()\n    elif platform.system() == 'Linux':\n        df_split = np.array_split(df, npartitions)\n        pool = Pool(npartitions)\n        df = pd.concat(pool.map(func, df_split))\n        pool.close()\n        pool.join()\n\n    else:\n        print('No idea what to do with your OS :(')\n\n    return df\n\n\ndef load_embed(filepath: str):\n    \"\"\"\n    Load embeddings.\n\n    :param filepath: path to the embeddings\n    :return:\n    \"\"\"\n    def get_coefs(word, *arr):\n        return word, np.asarray(arr, dtype='float32')\n    if '.pkl' in filepath:\n        with open(filepath,'rb') as f:\n            return pickle.load(f)\n    if 'news' in filepath:\n        embeddings_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(filepath) if len(o) > 100)\n    elif '.bin' in filepath:\n        embeddings_index = KeyedVectors.load_word2vec_format(filepath, binary=True)\n    else:\n        embeddings_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(filepath, encoding='utf-8'))\n\n    return embeddings_index\n\n\ndef build_vocab(texts: pd.Series()) -> dict:\n    \"\"\"\n    Creates a vocabulary of the text, which can be used to check text coverage.\n\n    :param texts: pandas series with text.\n    :return: dictionary with words and their counts\n    \"\"\"\n    # sentences = texts.progress_apply(lambda x: x.split()).values\n    vocab = defaultdict(lambda: 0)\n    for sentence in texts.values:\n        for word in str(sentence).split():\n            vocab[word] += 1\n\n    return vocab\n\n\ndef check_coverage(vocab: dict, embeddings_index) -> list:\n    \"\"\"\n    Check word coverage of embedding. Returns words which aren't in embeddings_index\n\n    :param vocab: Dictionary with words and their counts.\n    :param embeddings_index: embedding index\n    :return: list of tuples with unknown words and their count\n    \"\"\"\n    known_words = {}\n    unknown_words = {}\n    nb_known_words = 0\n    nb_unknown_words = 0\n    for word in vocab.keys():\n        if word in embeddings_index:\n            known_words[word] = embeddings_index[word]\n            nb_known_words += vocab[word]\n        elif word.capitalize() in embeddings_index:\n            known_words[word] = embeddings_index[word.capitalize()]\n            nb_known_words += vocab[word]\n        elif word.lower() in embeddings_index:\n            known_words[word] = embeddings_index[word.lower()]\n            nb_known_words += vocab[word]\n        elif word.upper() in embeddings_index:\n            known_words[word] = embeddings_index[word.upper()]\n            nb_known_words += vocab[word]\n        else:\n            unknown_words[word] = vocab[word]\n            nb_unknown_words += vocab[word]\n\n    vocab_rate = len(known_words) \/ len(vocab)\n    print(f'Found embeddings for {vocab_rate:.2%} of vocab')\n\n    text_rate = nb_known_words \/ (nb_known_words + nb_unknown_words)\n    print(f'Found embeddings for {text_rate:.2%} of all text')\n    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n\n    return unknown_words\n\n\ndef vocab_check_coverage(df: pd.DataFrame(), *args) -> list:\n    \"\"\"\n    Calculate word coverage for the passed dataframe and embeddings.\n    Can do it for one or several embeddings.\n\n    :param df: dataframe for which coverage rate will be calculated.\n    :param args: one or several embeddings\n    :return: list of dicts with out of vocab rate and words\n    \"\"\"\n\n    oovs = []\n    vocab = build_vocab(df['comment_text'])\n\n    for emb in args:\n        oov = check_coverage(vocab, emb)\n        oov = {\"oov_rate\": len(oov) \/ len(vocab), 'oov_words': oov}\n        oovs.append(oov)\n\n    return oovs\n\n\ndef remove_space(text: str, spaces: list, only_clean: bool = True) -> str:\n    \"\"\"\n    Remove extra spaces and ending space if any.\n\n    :param text: text to clean\n    :param text: spaces\n    :param only_clean: simply clean texts or also replace texts\n    :return: cleaned text\n    \"\"\"\n    if not only_clean:\n        for space in spaces:\n            text = text.replace(space, ' ')\n\n    text = text.strip()\n    text = re.sub('\\s+', ' ', text)\n\n    return text\n\n\ndef replace_words(text: str, mapping: dict) -> str:\n    \"\"\"\n    Replaces unusual punctuation with normal.\n\n    :param text: text to clean\n    :param mapping: dict with mapping\n    :return: cleaned text\n    \"\"\"\n    for word in mapping:\n        if word in text:\n            text = text.replace(word, mapping[word])\n\n    return text\n\n\ndef clean_number(text: str) -> str:\n    \"\"\"\n    Cleans numbers.\n\n    :param text: text to clean\n    :return: cleaned text\n    \"\"\"\n    text = re.sub(r'(\\d+)([a-zA-Z])', '\\g<1> \\g<2>', text)\n    text = re.sub(r'(\\d+) (th|st|nd|rd) ', '\\g<1>\\g<2> ', text)\n    text = re.sub(r'(\\d+),(\\d+)', '\\g<1>\\g<2>', text)\n    text = re.sub(r'(\\d+),', '\\g<1>', text)\n    text = re.sub(r'(\\d+)(e)(\\d+)', '\\g<1> \\g<3>', text)\n\n    return text\n\n\ndef spacing_punctuation(text: str, punctuation: str) -> str:\n    \"\"\"\n    Add space before and after punctuation and symbols.\n\n    :param text: text to clean\n    :param punctuation: string with symbols\n    :return: cleaned text\n    \"\"\"\n    for punc in punctuation:\n        if punc in text:\n            text = text.replace(punc, f' {punc} ')\n\n    return text\n\n\ndef fixing_with_regex(text) -> str:\n    \"\"\"\n    Additional fixing of words.\n\n    :param text: text to clean\n    :return: cleaned text\n    \"\"\"\n\n    mis_connect_list = ['\\b(W|w)hat\\b', '\\b(W|w)hy\\b', '(H|h)ow\\b', '(W|w)hich\\b', '(W|w)here\\b', '(W|w)ill\\b']\n    mis_connect_re = re.compile('(%s)' % '|'.join(mis_connect_list))\n\n    text = re.sub(r\" (W|w)hat+(s)*[A|a]*(p)+ \", \" WhatsApp \", text)\n    text = re.sub(r\" (W|w)hat\\S \", \" What \", text)\n    text = re.sub(r\" \\S(W|w)hat \", \" What \", text)\n    text = re.sub(r\" (W|w)hy\\S \", \" Why \", text)\n    text = re.sub(r\" \\S(W|w)hy \", \" Why \", text)\n    text = re.sub(r\" (H|h)ow\\S \", \" How \", text)\n    text = re.sub(r\" \\S(H|h)ow \", \" How \", text)\n    text = re.sub(r\" (W|w)hich\\S \", \" Which \", text)\n    text = re.sub(r\" \\S(W|w)hich \", \" Which \", text)\n    text = re.sub(r\" (W|w)here\\S \", \" Where \", text)\n    text = re.sub(r\" \\S(W|w)here \", \" Where \", text)\n    text = mis_connect_re.sub(r\" \\1 \", text)\n    text = text.replace(\"What sApp\", ' WhatsApp ')\n\n    # Clean repeated letters.\n    text = re.sub(r\"(I|i)(I|i)+ng\", \"ing\", text)\n    text = re.sub(r\"(-+|\\.+)\", \" \", text)\n\n    text = re.sub(r'[\\x00-\\x1f\\x7f-\\x9f\\xad]', '', text)\n    text = re.sub(r'(\\d+)(e)(\\d+)', r'\\g<1> \\g<3>', text)  # is a dup from above cell...\n    text = re.sub(r\"(-+|\\.+)\\s?\", \"  \", text)\n    text = re.sub(\"\\s\\s+\", \" \", text)\n    text = re.sub(r'\u1d35+', '', text)\n\n    text = re.sub(r\"(W|w)on(\\'|\\\u2019)t \", \"will not \", text)\n    text = re.sub(r\"(C|c)an(\\'|\\\u2019)t \", \"can not \", text)\n    text = re.sub(r\"(Y|y)(\\'|\\\u2019)all \", \"you all \", text)\n    text = re.sub(r\"(Y|y)a(\\'|\\\u2019)ll \", \"you all \", text)\n\n    text = re.sub(r\"(I|i)(\\'|\\\u2019)m \", \"i am \", text)\n    text = re.sub(r\"(A|a)in(\\'|\\\u2019)t \", \"is not \", text)\n    text = re.sub(r\"n(\\'|\\\u2019)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\\u2019)re \", \" are \", text)\n    text = re.sub(r\"(\\'|\\\u2019)s \", \" is \", text)\n    text = re.sub(r\"(\\'|\\\u2019)d \", \" would \", text)\n    text = re.sub(r\"(\\'|\\\u2019)ll \", \" will \", text)\n    text = re.sub(r\"(\\'|\\\u2019)t \", \" not \", text)\n    text = re.sub(r\"(\\'|\\\u2019)ve \", \" have \", text)\n\n    text = re.sub(\n        r'(by|been|and|are|for|it|TV|already|justhow|some|had|is|will|would|should|shall|must|can|his|here|there|them|these|their|has|have|the|be|that|not|was|he|just|they|who)(how)',\n        '\\g<1> \\g<2>', text)\n\n    return text\n\n\ndef load_preprocessing_data() -> dict:\n    \"\"\"\n    Loads dict with various mappings and strings for cleaning.\n\n    :return:\n    \"\"\"\n\n    if os.path.exists('..\/input\/jigsaw-public-files\/mapping_dict.json'):\n        path = '..\/input\/jigsaw-public-files\/mapping_dict.json'\n    else:\n        path = '..\/input\/mapping_dict.json'\n        \n    with open(path, 'r') as f:\n        mapping_dict = json.load(f)\n\n    # combine several dicts into one\n    replace_dict = {**mapping_dict['contraction_mapping'],\n                    **mapping_dict['mispell_dict'],\n                    **mapping_dict['special_punc_mappings'],\n                    **mapping_dict['rare_words_mapping'],\n                    **mapping_dict['bad_case_words'],\n                    **mapping_dict['mis_spell_mapping']}\n\n    mapping_dict = {'spaces': mapping_dict['spaces'],\n                    'punctuation': mapping_dict['punctuation'],\n                    'words_to_replace': replace_dict}\n\n    return mapping_dict\n\n\ndef preprocess(text: str) -> str:\n    \"\"\"\n    Apply all preprocessing.\n\n    :param text: text to clean.\n    :return: cleaned text\n    \"\"\"\n\n    text = remove_space(text, mapping_dict['spaces'], only_clean=False)\n    text = clean_number(text)\n    text = spacing_punctuation(text, mapping_dict['punctuation'])\n    text = fixing_with_regex(text)\n    text = replace_words(text, mapping_dict['words_to_replace'])\n\n    for punct in \"\/-'\":\n        if punct in text:\n            text = text.replace(punct, ' ')\n\n    text = clean_number(text)\n    text = remove_space(text, mapping_dict['spaces'])\n\n    return text\n\n\ndef text_clean_wrapper(df):\n    df[\"comment_text\"] = df[\"comment_text\"].apply(preprocess).astype(str)\n    return df\n\nmapping_dict = load_preprocessing_data()\n\n\ndef build_matrix(word_index, path: str, embed_size: int):\n    embedding_index = load_embed(path)\n    embedding_matrix = np.zeros((len(word_index) + 1, embed_size))\n    unknown_words = []\n\n    for word, i in word_index.items():\n        try:\n            embedding_matrix[i] = embedding_index[word]\n        except KeyError:\n            unknown_words.append(word)\n\n    return embedding_matrix, unknown_words\n\n\n# class TextDataset(data.Dataset):\n#     def __init__(self, text, lens, y=None):\n#         self.text = text\n#         self.lens = lens\n#         self.y = y\n\n#     def __len__(self):\n#         return len(self.lens)\n\n#     def __getitem__(self, idx):\n#         if self.y is None:\n#             return self.text[idx], self.lens[idx]\n#         return self.text[idx], self.lens[idx], self.y[idx]\n\n\n# class Collator(object):\n#     def __init__(self, test: bool = False, max_length: int = 220):\n#         self.test = test\n#         self.max_length = max_length\n\n#     def __call__(self, batch):\n\n#         if self.test:\n#             texts, lens = zip(*batch)\n#         else:\n#             texts, lens, target = zip(*batch)\n\n#         lens = np.array(lens)\n#         max_batch_len = min(max(lens), self.max_length)\n#         max_batch_len = self.max_length\n#         texts = torch.tensor(sequence.pad_sequences(texts, maxlen=max_batch_len), dtype=torch.long).cuda()\n\n#         if self.test:\n#             return texts\n\n#         return texts, torch.tensor(target, dtype=torch.float32).cuda()\n\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\n\ndef train_model(model, x_train, y_train, x_val, y_val, test_loader, loss_fn, lr=0.001,\n                batch_size=512, n_epochs=4,\n                enable_checkpoint_ensemble=False, validate=False):\n    param_lrs = [{'params': param, 'lr': lr} for param in model.parameters()]\n    optimizer = torch.optim.Adam(param_lrs, lr=lr)\n\n    scheduler = CyclicLR(optimizer, base_lr=0.001, max_lr=0.003,\n                         step_size=300, mode='exp_range', gamma=0.99994)\n\n    train = torch.utils.data.TensorDataset(x_train, y_train)\n    valid = torch.utils.data.TensorDataset(x_val, y_val)\n    \n    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n\n    # all_test_preds = []\n    checkpoint_weights = [2 ** epoch for epoch in range(n_epochs)]\n\n    for epoch in range(n_epochs):\n        start_time = time.time()\n\n        model.train()\n        avg_loss = 0.\n\n        for step, (seq_batch, y_batch) in enumerate(train_loader):\n            y_pred = model(seq_batch)\n            scheduler.batch_step()\n            loss = loss_fn(y_pred, y_batch)\n\n            optimizer.zero_grad()\n            loss.backward()\n\n            optimizer.step()\n            avg_loss += loss.item() \/ len(train_loader)\n\n        model.eval()\n        test_preds = np.zeros((len(test_loader.dataset)))\n\n        val_loss = 0\n        if validate:\n\n            valid_preds = np.zeros((len(valid_loader.dataset)))\n            for i, (seq_batch, y_batch) in enumerate(valid_loader):\n                y_pred = model(seq_batch).detach()\n                val_loss += loss_fn(y_pred, y_batch).item() \/ len(valid_loader)\n                valid_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n\n        for i, seq_batch in enumerate(test_loader):\n            y_pred = model(seq_batch.long().cuda()).detach()\n\n            test_preds[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n\n        # all_test_preds.append(test_preds)\n        elapsed_time = time.time() - start_time\n        print(\n            f'Epoch {epoch + 1}\/{n_epochs} \\t loss={avg_loss:.4f} val_loss={val_loss:.4f} \\t time={elapsed_time:.2f}s')\n\n#     if enable_checkpoint_ensemble:\n#         prediction = np.average(all_test_preds, weights=checkpoint_weights, axis=0)\n#     else:\n#         prediction = all_test_preds[-1]\n\n    results_dict = {}\n    results_dict['test_preds'] = test_preds\n    if validate:\n        results_dict['oof'] = valid_preds\n\n    return results_dict\n\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)  # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\n\nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, embedding_matrix_small, max_features: int = 120000, lstm_units: int = 128,\n                 dense_hidden_units: int = 128):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n\n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n\n        self.embedding1 = nn.Embedding(max_features, 30)\n        self.embedding1.weight = nn.Parameter(torch.tensor(embedding_matrix_small, dtype=torch.float32))\n\n        self.lstm1 = nn.LSTM(embed_size, lstm_units, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(lstm_units * 2, lstm_units, bidirectional=True, batch_first=True)\n\n        self.lstm1s = nn.LSTM(30, int(lstm_units \/ 8), bidirectional=True, batch_first=True)\n        self.lstm2s = nn.LSTM(int(lstm_units \/ 4), int(lstm_units \/ 8), bidirectional=True, batch_first=True)\n\n        self.linear1 = nn.Linear(576, dense_hidden_units)\n        self.linear2 = nn.Linear(576, dense_hidden_units)\n\n        self.linear_out = nn.Linear(832, 1)\n        self.linear_aux_out = nn.Linear(832, 6)\n\n    def forward(self, x):\n        h_embedding = self.embedding(x)\n        h_embedding = self.embedding_dropout(h_embedding)\n\n        embedding_small = self.embedding1(x)\n        embedding_small = self.embedding_dropout(embedding_small)\n\n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n\n        h_lstm1s, _ = self.lstm1s(embedding_small)\n        h_lstm2s, _ = self.lstm2s(h_lstm1s)\n\n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n\n        # global average pooling\n        avg_pools = torch.mean(h_lstm2s, 1)\n        # global max pooling\n        max_pools, _ = torch.max(h_lstm2s, 1)\n\n        h_conc = torch.cat((max_pool, avg_pool, max_pools, avg_pools), 1)\n        h_conc_linear1 = F.relu(self.linear1(h_conc))\n        h_conc_linear2 = F.relu(self.linear2(h_conc))\n        hidden = torch.cat((h_conc, h_conc_linear1, h_conc_linear2), 1)\n\n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n\n        return out\n\n\n# def make_loaders(data: pd.DataFrame(), data_lens: list, target: pd.DataFrame() = None, test: bool=False):\n\n#     collate = Collator(test)\n#     if test:\n#         dataset = TextDataset(data, data_lens)\n#     else:\n#         dataset = TextDataset(data, data_lens, target)\n#     loader = torch.utils.data.DataLoader(dataset, batch_size=512, shuffle=True, collate_fn=collate)\n\n#     return loader\n\n\ndef train_on_folds(X_train, x_train_lens, final_y_train, test_loader, x_test_lens, splits, embedding_matrix, embedding_matrix_small, n_epochs=2, validate=False, debug=False):\n    if validate:\n        scores = []\n\n    test_preds = np.zeros((len(test_loader.dataset), len(splits)))\n    train_oof = np.zeros((len(X_train), 1))\n\n    for i, (train_idx, valid_idx) in enumerate(splits):\n        # for debugging purposes, to make things faster\n        if debug:\n            train_idx = train_idx[:1000]\n\n        x_train_fold = torch.tensor(X_train[train_idx], dtype=torch.long).cuda()\n        y_train_fold = torch.tensor(final_y_train[train_idx], dtype=torch.float32).cuda()\n        x_val_fold = torch.tensor(X_train[valid_idx], dtype=torch.long).cuda()\n        y_val_fold = torch.tensor(final_y_train[valid_idx], dtype=torch.float32).cuda()\n\n        print(f'Fold {i + 1}')\n\n        set_seed(42 + i)\n        model = NeuralNet(embedding_matrix, embedding_matrix_small)\n        loss_fn = nn.BCEWithLogitsLoss(reduction='mean')\n        model.cuda()\n\n        results_dict = train_model(model,\n                                   x_train_fold, \n                                   y_train_fold, \n                                   x_val_fold, \n                                   y_val_fold, test_loader, loss_fn=loss_fn, n_epochs=n_epochs, validate=True)\n\n        if validate:\n            train_oof[valid_idx] = results_dict['oof'].reshape(-1, 1)\n            print(metrics.roc_auc_score(final_y_train[valid_idx][:,0], train_oof[valid_idx]))\n            \n        test_preds[:, i] = results_dict['test_preds']\n\n    return test_preds\n\n\nclass CyclicLR(object):\n    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n                 step_size=2000, factor=0.6, min_lr=1e-4, mode='triangular', gamma=1.,\n                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n\n        if not isinstance(optimizer, torch.optim.Optimizer):\n            raise TypeError('{} is not an Optimizer'.format(\n                type(optimizer).__name__))\n        self.optimizer = optimizer\n\n        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n            if len(base_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} base_lr, got {}\".format(\n                    len(optimizer.param_groups), len(base_lr)))\n            self.base_lrs = list(base_lr)\n        else:\n            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n\n        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n            if len(max_lr) != len(optimizer.param_groups):\n                raise ValueError(\"expected {} max_lr, got {}\".format(\n                    len(optimizer.param_groups), len(max_lr)))\n            self.max_lrs = list(max_lr)\n        else:\n            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n\n        self.step_size = step_size\n\n        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n                and scale_fn is None:\n            raise ValueError('mode is invalid and scale_fn is None')\n\n        self.mode = mode\n        self.gamma = gamma\n\n        if scale_fn is None:\n            if self.mode == 'triangular':\n                self.scale_fn = self._triangular_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = self._triangular2_scale_fn\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = self._exp_range_scale_fn\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n\n        self.batch_step(last_batch_iteration + 1)\n        self.last_batch_iteration = last_batch_iteration\n\n        self.last_loss = np.inf\n        self.min_lr = min_lr\n        self.factor = factor\n\n    def batch_step(self, batch_iteration=None):\n        if batch_iteration is None:\n            batch_iteration = self.last_batch_iteration + 1\n        self.last_batch_iteration = batch_iteration\n        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n            param_group['lr'] = lr\n\n    def step(self, loss):\n        if loss > self.last_loss:\n            self.base_lrs = [max(lr * self.factor, self.min_lr) for lr in self.base_lrs]\n            self.max_lrs = [max(lr * self.factor, self.min_lr) for lr in self.max_lrs]\n\n    def _triangular_scale_fn(self, x):\n        return 1.\n\n    def _triangular2_scale_fn(self, x):\n        return 1 \/ (2. ** (x - 1))\n\n    def _exp_range_scale_fn(self, x):\n        return self.gamma ** (x)\n\n    def get_lr(self):\n        step_size = float(self.step_size)\n        cycle = np.floor(1 + self.last_batch_iteration \/ (2 * step_size))\n        x = np.abs(self.last_batch_iteration \/ step_size - 2 * cycle + 1)\n\n        lrs = []\n        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n        for param_group, base_lr, max_lr in param_lrs:\n            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n            if self.scale_mode == 'cycle':\n                lr = base_lr + base_height * self.scale_fn(cycle)\n            else:\n                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n            lrs.append(lr)\n        return lrs\n","c20bc089":"# setting parameters.\nset_seed(42)\n\ncrawl_embedding_path = '..\/input\/pickled-crawl300d2m-for-kernel-competitions\/crawl-300d-2M.pkl'\nglove_embedding_path = '..\/input\/pickled-glove840b300d-for-10sec-loading\/glove.840B.300d.pkl'\nnum_models = 2\nlstm_units = 128\ndense_hidden_units = 4 * lstm_units\nmax_len = 220\nembed_size = 300\nmax_features = 120000","6b7628a6":"%%time\nload = True\nif not load:\n    train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\n    test = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')\n    train = df_parallelize_run(train, text_clean_wrapper)\n    test = df_parallelize_run(test, text_clean_wrapper)\n    train.to_csv('processed_train.csv', index=False)\n    test.to_csv('processed_test.csv', index=False)\nelse:\n    train = pd.read_csv('..\/input\/jigsaw-public-files\/train.csv')\n    test = pd.read_csv('..\/input\/jigsaw-public-files\/test.csv')\n    # after processing some of the texts are emply\n    train['comment_text'] = train['comment_text'].fillna('')\n    test['comment_text'] = test['comment_text'].fillna('')","e82d77df":"%%time\nglove_embed = load_embed(glove_embedding_path)\noovs = vocab_check_coverage(train, glove_embed)","b54bcf3a":"del glove_embed\nprint(oovs[0]['oov_words'][:20])","fe8de0af":"%%time\nif not load:\n    tokenizer = text.Tokenizer(lower=False, num_words=max_features)\n    tokenizer.fit_on_texts(list(train['comment_text']) + list(test['comment_text']))\n    \n    # by default tokenizer keeps all words, I leave only top max_features\n    sorted_by_word_count = sorted(tokenizer.word_counts.items(), key=lambda x: x[1], reverse=True)\n    tokenizer.word_index = {}\n    i = 0\n    for word,count in sorted_by_word_count:\n        if i == max_features:\n            break\n        tokenizer.word_index[word] = i + 1    # <= because tokenizer is 1 indexed\n        i += 1\n    \n    with open(f'tokenizer_{max_features}.pickle', 'wb') as f:\n        pickle.dump(tokenizer, f)\nelse:\n    with open(f'..\/input\/jigsaw-public-files\/tokenizer_{max_features}.pickle', 'rb') as f:\n        tokenizer = pickle.load(f)\n    \nX_train = tokenizer.texts_to_sequences(train['comment_text'])\nX_test = tokenizer.texts_to_sequences(test['comment_text'])\nx_train_lens = [len(i) for i in X_train]\nx_test_lens  = [len(i) for i in X_test]","620f4c29":"y_train = np.where(train['target'] >= 0.5, 1, 0)\ny_aux_train = train[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']]\nfinal_y_train = np.hstack([y_train[:, np.newaxis], y_aux_train])","08893345":"%%time\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, crawl_embedding_path, embed_size)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nglove_matrix, unknown_words_glove = build_matrix(tokenizer.word_index, glove_embedding_path, embed_size)\nprint('n unknown words (glove): ', len(unknown_words_glove))\n\nembedding_matrix = crawl_matrix * 0.5 +  glove_matrix * 0.5\n\ndel crawl_matrix\ndel glove_matrix\ngc.collect()","eb8e61a8":"embedding_matrix_small = np.zeros((embedding_matrix.shape[0], 30))","0b8682c0":"# splits for training\nsplits = list(KFold(n_splits=5, shuffle=True, random_state=42).split(X_train, final_y_train))","bbea2f31":"X_train_padded = pad_sequences(X_train, maxlen = max_len)\nX_test_padded = pad_sequences(X_test, maxlen = max_len)\nbatch_size = 512\ntest_loader = torch.utils.data.DataLoader(X_test_padded, batch_size=batch_size, shuffle=False)","6c7a0534":"test_preds = train_on_folds(X_train_padded, x_train_lens, final_y_train, test_loader, x_test_lens,\n                            splits, embedding_matrix, embedding_matrix_small, n_epochs=1, validate=False, debug=False)","d8d53899":"submission = pd.DataFrame.from_dict({\n    'id': test['id'],\n    'prediction': test_preds.mean(1)\n})\n\nsubmission.to_csv('submission.csv', index=False)","28c89c33":"### Tokenizing","b468286b":"## Loading data\nI have saved the processed data to the dataset, so I can load it from there.","30e520f0":"## Training model on folds","1a20ab36":"## Checking vocab coverage","46adc404":"## Loading embeddings","01f6311b":"Creating a small embedding, which will be trainable.","606dc18b":"Most of out of vocab words are names, so I suppose there is nothing to do about them.","afbe2ee8":"### All the functions used in this kernel are in the hidden cell below","a30d1c4f":"## General information\n\nIn this kernel I wanted to create a full cycle of processing text and training model in Pytorch.\n\nThis code is based on ideas from several kernels with my changes and improvements when possible, I want to acknowledge these great works:\n\n* https:\/\/www.kaggle.com\/adityaecdrid\/public-version-text-cleaning-vocab-65\/\n* https:\/\/www.kaggle.com\/bminixhofer\/simple-lstm-pytorch-version\n* https:\/\/www.kaggle.com\/authman\/simple-lstm-pytorch-with-batch-loading\n* https:\/\/www.kaggle.com\/gpreda\/jigsaw-fast-compact-solution\n\nIf I missed someone - write to me, I'll add it.\n\nSo my kernel and script contains the following:\n\n* preprocessing texts. Mostly based on adityaecdrid code with some changes. I have a json file with mappings here: https:\/\/www.kaggle.com\/artgor\/jigsaw-public-files\n* fast loading embeddings from pickled files\n* to be done - text dataset with collating for dynamic length change\n* neural net with two embeddings. The first embedding is fasttext and glove embeddings multiplied by weights. The second one is a small trainable embedding. The idea is that this embedding could get some important information while training model\n* training model on folds\n* to be done - competition metric calculation\n* to be done - weighting loss\n* to be done - saving model while training on folds","cba9fe21":"Importing libraries"}}