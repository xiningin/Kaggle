{"cell_type":{"fce0397f":"code","efa4d9d6":"code","1adbfbbc":"code","8e56df1a":"code","95d11ee2":"code","8b1240ad":"code","f09cc21c":"code","0cb30cf3":"code","e0ca6f96":"code","ae54ecd8":"code","aa5523db":"code","e4e52eea":"code","ec8a9d61":"code","3eab5d1c":"code","20a904be":"code","64b39091":"code","f8d8a225":"code","6d2a49fc":"code","afc626d5":"code","0d02beb7":"code","11665310":"code","d1f231e1":"code","67bd32f4":"code","cb11354e":"code","d6a784ad":"code","111c6f35":"code","71a73fc5":"code","7ef9fc2f":"code","a705d403":"code","053c0a40":"code","583c093e":"code","65df7219":"code","6dd0346e":"code","bef6bb35":"code","53e104c4":"code","7665a74b":"code","694be213":"code","413ac3a2":"code","0c9f4149":"code","b400e703":"code","99dc102e":"code","255714d1":"code","04efc9bd":"code","ad01d594":"code","b97fb46c":"code","a4cc4306":"code","35dc9226":"code","5fb62487":"code","b7d4f804":"code","292ddedd":"code","8b419b06":"code","e3ee0c43":"code","199ee4f8":"code","66d9ae71":"markdown","dd875b02":"markdown","72ec583f":"markdown","f7d65018":"markdown","77b74f4a":"markdown","2f4c9251":"markdown","8114754a":"markdown","eeb0dbe7":"markdown","b00a23af":"markdown","00e21f7c":"markdown","3a9ac397":"markdown","8ecb5fb9":"markdown","1f31c05d":"markdown","f49c4668":"markdown","8ab1454e":"markdown","dda5e9aa":"markdown","83273f1d":"markdown","df04c9f0":"markdown","5d4f3c1a":"markdown","c04e3b9e":"markdown","1b7e51be":"markdown","a83a0e7e":"markdown","edbe3f16":"markdown","4c06bac4":"markdown","c2509a71":"markdown","27e9b4af":"markdown","182ff4b0":"markdown","ef4057da":"markdown","6961463c":"markdown"},"source":{"fce0397f":"from IPython.display import Image\nImage(\"..\/input\/images\/RNN.png\")","efa4d9d6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN,Embedding","1adbfbbc":"n = 1000\ntrain_point = int(0.8*n)\n\nt = np.arange(0,n)\nX = np.sin((0.02*t)+2*np.random.rand(n)) #Multiplications done to make the graph \"clean\"","8e56df1a":"df = pd.DataFrame(X)\ndf.head(3)","95d11ee2":"plt.plot(df)","8b1240ad":"step = 4","f09cc21c":"values = df.values\ntrain,test = values[0:train_point,:], values[train_point:n,:]\n#train.shape = (800,1)","0cb30cf3":"step = 4\n# add step elements into train and test\ntest = np.append(test,np.repeat(test[-1,],step))\ntrain = np.append(train,np.repeat(train[-1,],step))\ntrain.shape # = (804,1)","e0ca6f96":"# convert into dataset matrix\ndef convertToMatrix(data, step):\n    X, Y =[], []\n    for i in range(len(data)-step):\n        d=i+step  \n        X.append(data[i:d,])\n        Y.append(data[d,])\n    return np.array(X), np.array(Y)","ae54ecd8":"trainX,trainY =convertToMatrix(train,step)\nprint(trainX.shape)\ntestX,testY =convertToMatrix(test,step)","aa5523db":"trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\ntestX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\ntrainX.shape","e4e52eea":"trainY.shape","ec8a9d61":"#(batch_size, timesteps, input_dim)\ndef RNN():\n    model = Sequential()\n    model.add(SimpleRNN(units=32, input_shape=(1,step), activation=\"relu\"))\n    model.add(Dense(8,activation=\"relu\"))\n    model.add(Dense(1))\n    model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n    return model","3eab5d1c":"#print(trainX.shape,trainy.shape)\nmodel = RNN()\nmodel.fit(trainX,trainY, epochs=100, batch_size=16, verbose=0)","20a904be":"trainPredict = model.predict(trainX)\ntestPredict= model.predict(testX)\npredicted=np.concatenate((trainPredict,testPredict),axis=0)","64b39091":"trainScore = model.evaluate(trainX, trainY, verbose=0)\nprint(trainScore)","f8d8a225":"index = df.index.values\nplt.plot(index,df)\nplt.plot(index,predicted)\nplt.axvline(df.index[train_point], c=\"lightgreen\",lw=2)\nplt.show() ","6d2a49fc":"#dayofweek = [\"Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\"]\ndayofweek = [1,2,3,4,5,6,7]\n#dinner = [\"Pizza\",\"Burguer\",\"Pancake\",\"Spaghetti\",\"Meat\",\"Chicken\",\"Sausage\"]\ndinner = [1,2,3,4,5,6,7]\n\n#Repeating for greater dataset\ndayofweek = np.array(144*dayofweek) # = 1008 days\ndinner = np.array(144*dinner)","afc626d5":"# Time-step\ntstep = 2\n\n# train = ~80% | test = ~20%\ntp = 800\n\ntrainX, trainY = dayofweek[0:tp], dinner[0:tp]\ntestX, testY = dayofweek[tp:], dinner[tp:]","0d02beb7":"testX.shape","11665310":"def makematrix(dataX, dataY, tstep):\n    X, Y = [], []\n    for i in range(len(dataX)-tstep):\n        X.append(dataX[i:i+tstep,])\n        Y.append(dataY[i+tstep])\n    return np.array(X), np.array(Y)","d1f231e1":"trainX, trainY = makematrix(trainX, trainY, tstep)\ntestX, testY = makematrix(testX, testY, tstep)","67bd32f4":"testX.shape","cb11354e":"trainX = np.reshape(trainX, (trainX.shape[0], 1, tstep))\ntestX = np.reshape(testX, (testX.shape[0], 1, tstep))","d6a784ad":"def graph(hist):\n    # Plots Loss Line.\n    plt.plot(hist.history['loss'])\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    \n    lss = str(hist.history['loss'][-1])\n    plt.title(str('Loss='+lss))\n    plt.show()","111c6f35":"def RNN():\n    model = Sequential()\n    import keras\n    keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=5)\n    model.add(SimpleRNN(units=32, input_shape=(1,tstep), kernel_initializer=\"normal\",activation=\"relu\"))\n    model.add(Dense(1))\n    keras.optimizers.Adam(decay=1e-6,learning_rate=0.0001, beta_1=0.9, beta_2=0.999, amsgrad=False )\n    model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n    return model","71a73fc5":"model = RNN()\nhist = model.fit(trainX, trainY, epochs=350, batch_size=32, verbose=0)","7ef9fc2f":"trainPredict = model.predict(trainX)\ntestPredict = model.predict(testX)\npredicted = np.concatenate((trainPredict,testPredict),axis=0)","a705d403":"trainScore = model.evaluate(trainX, trainY, verbose=0)\nprint(trainScore)","053c0a40":"graph(hist)","583c093e":"seq = np.array([[[1, 2]],[[4, 5]],[[7, 1]]])\n# NextDay: [Wednesday],[Saturday],[Tuesday]\n# Returns: Pancake, Chicken, Burguer\n#           3.0      6.0      2.0\n\nfor i in model.predict(seq):\n    if round(i[0]) == 1:\n        print('\\n',round(i[0],3))\n        print(\"Pizza\")\n    elif round(i[0]) == 2:\n        print('\\n',round(i[0],3))\n        print(\"Burguer\")\n    elif round(i[0]) == 3:\n        print('\\n',round(i[0],3))\n        print(\"Pancake\")\n    elif round(i[0]) == 4:\n        print('\\n',round(i[0],3))\n        print(\"Spaghetti\")\n    elif round(i[0]) == 5:\n        print('\\n',round(i[0],3))\n        print(\"Meat\")\n    elif round(i[0]) == 6:\n        print('\\n',round(i[0],3))\n        print(\"Chicken\")\n    else:\n        print(\"Sausage\")\n","65df7219":"Image(\"..\/input\/images\/LSTM.png\",width=400, height=400)","6dd0346e":"Image(\"..\/input\/images\/LSTM2.png\", width=550, height=550)","bef6bb35":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()","53e104c4":"btcdf = pd.read_csv('..\/input\/bitcoin-historical-price\/Bitcoin_Historical_Price.csv')","7665a74b":"btcdf['Date'] = pd.to_datetime(btcdf['Date'])\nbtcdf.head(3)","694be213":"btcdf['Close'] = btcdf['Close']\/btcdf['Close'].max()","413ac3a2":"group = btcdf.groupby('Date')","0c9f4149":"Real_Price = group['Close'].mean()","b400e703":"Real_Price[:3]","99dc102e":"Real_Price.shape","255714d1":"split_point = 1400\nstep = 30\nX_train = Real_Price[:split_point]\ny_train = Real_Price[step:split_point+step] # 30 days after train\nX_test = Real_Price[split_point:-step] # Values until the 30th last value\ny_test = Real_Price[split_point+step:] # 30 days after test","04efc9bd":"def makematrix(dataX, dataY, step):\n    X, Y = [], []\n    for i in range(len(dataX)-step):\n        X.append(dataX[i:i+step,])\n        Y.append(dataY[i+step])\n    return np.array(X), np.array(Y)","ad01d594":"X_train, y_train = makematrix(X_train, y_train, step)\nX_test, y_test = makematrix(X_test, y_test, step)","b97fb46c":"X_train.shape","a4cc4306":"X_train = np.reshape(X_train, (X_train.shape[0], 1, step))\nX_test = np.reshape(X_test, (X_test.shape[0], 1, step))","35dc9226":"X_train.shape","5fb62487":"#X_train = sc.fit_transform(X_train)\n#X_test = sc.fit_transform(X_test)","b7d4f804":"from keras.layers import LSTM","292ddedd":"def BTC_RNN():\n    model = Sequential()\n    model.add(LSTM(units=64, activation=\"sigmoid\", input_shape=(1,step)))\n    model.add(Dense(units=1))\n    import keras\n    keras.optimizers.Adam(decay=1e-6,learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False )\n    model.compile(loss=\"mean_squared_error\",optimizer=\"adam\")\n    return model","8b419b06":"model = BTC_RNN()\nhist1 = model.fit(X_train, y_train, batch_size=32, epochs=5)","e3ee0c43":"graph(hist1)","199ee4f8":"X = y_test\ny = model.predict(X_test)\nplt.figure(figsize=(20,4))\nplt.plot(X,color='orange', label='Real BTC Prices')\nplt.plot(y,color='b', label = 'Predicted BTC Prices')\nplt.legend(loc=2, prop={'size': 12})\nplt.title('BTC Price Prediction', fontsize=20)\nplt.xlabel('Time', fontsize=15)\nplt.ylabel('BTC Price(USD)', fontsize=15)\nplt.show()","66d9ae71":"**Train\/Test Split**","dd875b02":"#### Defining the Model.  \n**Our model is a Recurrent Neural Network with 32 recurrent units and 1 hidden layer with 8 neurons  \nRelu is used as the activation function.  \nMean Squared Error is used as Loss Function with Adam optimizer.**","72ec583f":"## Recurrent Neural Networks","f7d65018":"**Generating sequential dataset**","77b74f4a":"## Imports and Setup","2f4c9251":"**LSTMs are an special kind of RNNs.  \nLSTMs can memorize past terms better than RNN**","8114754a":"**For RNN our input need to be the same shape. When we make a step it brokes the last numbers like:**  \ndata = 1-10 | step = 4  \nx = 1 y = 5  \nx = 2 y = 6  \n...  \nx = 10 y = 14 (14 not in data)","eeb0dbe7":"## Long Short-Term Memory","b00a23af":"## Important Note: The BTC Results are poor. That happens because I'm using an simple Neural Network with an simple input data (only the values of a few previous days). The objective of this BTC work is only to see how to code an RNN(LSTM).","00e21f7c":"**Split ok**","3a9ac397":"**Dataset Creation**","8ecb5fb9":"**As I made in the ANN Notebook, first I'll find some RNN work to take as a base, so I can learn in the process and apply it form myself later.**","1f31c05d":"**NOTES:**  \n   - **I'll use [This Work](https:\/\/www.datatechnotes.com\/2018\/12\/rnn-example-with-keras-simplernn-in.html) from datatechnotes to learn how to code an RNN**\n       - **All merits of this first work must be given to the author.**\n   - **After this, to apply what I've learned, I'll try to make on my own a RNN predict the next dinner. This is an example that I saw when I was learning about RNNs.**\n   - **Last, I'll try to make on my own a _BTC Price Predict_ applying LSTMs.**","f49c4668":"### Vin\u00edcius Rodrigues Ferraz","8ab1454e":"**Remembering**  \n**dayofweek = \n    [\"Monday, Tuesday, Wednesday, Thursday, Friday, Saturday, Sunday\"]  \ndayofweek = \n    [----1----,-----2----,-------3-----,------4-----,----5----,----6-----,----7----]**\n\n**dinner = \n    [\"Pizza\",\"Burguer\",\"Pancake\",\"Spaghetti\",\"Meat\",\"Chicken\",\"Sausage\"]  \ndinner =\n    [----1---,-----2-----,------3-----,------4------,----5----,----6-----,-----7-----]**","dda5e9aa":"**Visualizing the data**","83273f1d":"1772 values - I'll split at 1400 = ~80%","df04c9f0":"**Everything is OK!  \nReshaping to 3 dimensional**","5d4f3c1a":"**Since RNNs can treat sequence problems let's supose this case:  \n My roomate always make the same dinner, based on the day of week.  \n     Monday    - Pizza  \n     Tuesday   - Burguer  \n     Wednesday - Pancake  \n     Thursday  - spaghetti  \n     Friday    - Meat  \n     Saturday  - Chicken  \n     Sunday    - Sausage**","c04e3b9e":"**I'll create a graph function to plot future models**","1b7e51be":"**Forgetting: The Irrelevant Informations are forgetted by the Forget Gate.  \nPossibilities: All output possibilities, given the input.  \nIgnoring: Words to be ignored in actual LSTM cell  \nSelection: Selected words, given the input**\n<br><\/br><br><\/br>\n**First, the LSTM select the possibilities giving P possible words (Possibilities).  \nThem some of them (let's say the I ignored words) will be ignored by the Ignoring.   \nNon-ignored words will pass (Filtered possibilities).  \nSelected words from the forgetting will be removed(forgetted) from the Filtered Possibilities, and new words (Non-forgetted) will be: 1- collected by the memory 2- moved forward (both called Collected Possibilities).  \nFinally, LSTM will \"\"merge\"\" Selected Words with Collected Possibilities giving prediction. Also this prediction comes to the next LSTM cell as part of the input**","a83a0e7e":"**Time-step will be 2, so, given the previous 2 days, the RNN will discover what's the next food  \nGiven Monday(1) and Tuesday(2) RNN have to return Pancake(3)**","edbe3f16":"**RNNs require 3 dimensional inputs, so we have to reshape it**","4c06bac4":"**Fixing dimensions**","c2509a71":"**Now we will make the Train and the Test Data**\n****\n**RNN requires sequential Data but also requires a step**","27e9b4af":"**OK! It works!  \nNow, I will apply it on another sample.**","182ff4b0":"### It works!\n****\n****\n****\n**OK! Now I'll try to predict BTC prices, based on both these previous works  \nBut this time I'll use LSTM**\n<br><br>\n**For sure the results won't be too close to the real value, since I'm using simple inputs and an simple Neural Network.**","ef4057da":"**RNNs are Neural Networks that allows previous outputs to be used as inputs (inputs and outputs have the same shape), while having hidden states.  \nRNNs are used to deal with sequential\/temporal informations, since each epoch in RNN is an time step.**","6961463c":"**To make the work easy, I'll set numbers instead of strings**"}}