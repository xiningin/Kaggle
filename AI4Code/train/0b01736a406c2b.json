{"cell_type":{"e9348f68":"code","51df11b7":"code","ac865c49":"code","281b0510":"code","a5f6293f":"code","e4290ae9":"code","aa86d871":"code","ab9b274e":"code","4da9936e":"code","3ece52fe":"code","838632b2":"code","41871199":"code","f7882333":"code","82038660":"code","1d6f99fb":"code","71bfeb36":"code","9ac1493f":"code","5806d9e1":"code","d57fe945":"code","763e076b":"code","7e8adb96":"code","1aa11291":"code","78d1bbc5":"code","c5fc95ee":"code","972c802b":"code","93348013":"code","3ed7f5fe":"code","29b0f6ac":"code","fee637bd":"code","2846e5b7":"code","b6af2121":"code","fb45ed25":"code","c806ae5d":"code","fccd9f9d":"code","f9949578":"code","fa97a430":"code","56e83e3c":"code","e1b8e459":"code","cd8c2ba2":"code","5d0983d6":"code","b3f3f323":"code","df530765":"code","1918a7f5":"markdown","e386e3ca":"markdown","78605d30":"markdown","1dfa4720":"markdown","eb6096bc":"markdown","c594f11b":"markdown","ab488e53":"markdown","0ae3d043":"markdown","a32e2f26":"markdown","b9f5c94b":"markdown","de69173a":"markdown","52f80cef":"markdown","0f6c57b9":"markdown","7a4ab29a":"markdown","8d363974":"markdown","8d3f92c8":"markdown","517c109e":"markdown"},"source":{"e9348f68":"!pip install deepctr","51df11b7":"# To store the data\nimport pandas as pd\n\n# To do linear algebra\nimport numpy as np\n\n# To create plots\nimport matplotlib.pyplot as plt\n\n# # To create interactive plots\n# from plotly.offline import init_notebook_mode, plot, iplot, download_plotlyjs\n# import plotly as py\n# import plotly.graph_objs as go\n# # init_notebook_mode(connected=True)\n# To operator files\nimport os\n# To shift lists\nfrom collections import deque\n\n# To compute similarities between vectors\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n# To use recommender systems\nimport surprise as sp\nfrom surprise.model_selection import cross_validate\n\n# To create deep learning models\nfrom keras.layers import Input, Embedding, Reshape, Dot, Concatenate, Dense, Dropout\nfrom keras.models import Model\n\n# To create sparse matrices\nfrom scipy.sparse import coo_matrix\n\n# To light fm\nfrom lightfm import LightFM\nfrom lightfm.evaluation import precision_at_k\n\n# To deepctr\nfrom deepctr.inputs import SparseFeat, DenseFeat, get_feature_names\nfrom deepctr.models import DeepFM, xDeepFM, DCN, DIN, DSIN, DIEN\n\n# To stack sparse matrices\nfrom scipy.sparse import vstack","ac865c49":"# \u52a0\u8f7dnetflix-prize-data\u6570\u636e\u96c6\nos.listdir('..\/input\/netflix-prize-data\/')\n# qualifying.txt:\u8981\u63d0\u4ea4\u7684\u9884\u6d4b\u6587\u4ef6\n# MovieID1:\n# CustomerID11,Date11\n# CustomerID12,Date12\n# -> \n# MovieID1:\n# Rating11\n# Rating12\n\n# probe.txt: \u548cqualifying.txt\u6587\u4ef6\u7c7b\u4f3c\uff0c\u4e0e\u4e4b\u4e0d\u540c\u7684\u662f\u6ca1\u6709Date\u5217\n\n# movie_titles.txt : \u7535\u5f71\u4fe1\u606f\uff0c\u6570\u636e\u683c\u5f0f\u4e3aMovieId, YearOfRelease, Title\n# combined_data_1\/2\/3\/4.txt \uff1a \u8bad\u7ec3\u96c6\uff0c \u6570\u636e\u683c\u5f0f\u4e3aCustomerID(user), Rating, Date","281b0510":"movie_netflix = pd.read_csv('..\/input\/netflix-prize-data\/movie_titles.csv', \n                           encoding = 'ISO-8859-1', \n                           header = None, \n                           names = ['Id', 'Year', 'Name']).set_index('Id')\n\nprint('Shape Movie-Titles:\\t{} \\n Contains {} items'.format(movie_netflix.shape, movie_netflix.shape[0]))\nmovie_netflix.sample(5)","a5f6293f":"# \u52a0\u8f7dthe-movies-dataset\u6570\u636e\u96c6\n# os.listdir('..\/input\/the-movies-dataset')\n# movies_metadata.csv: \u7535\u5f71\u5143\u6587\u4ef6\uff0c\u6bcf\u4e2a\u7535\u5f71\u5171\u8ba124\u4e2a\u7279\u5f81\n# keywords.csv: id-keyword\uff0c\u6bcf\u4e2a\u7535\u5f71\u5bf9\u5e94\u4e00\u4e2a\u5173\u952e\u8bcd\n# credits.csv: id-cast-crew\uff0c\u6bcf\u4e2a\u7535\u5f71\u5bf9\u5e94\u6444\u5236\u7ec4\u548c\u6f14\u5458\u4fe1\u606f\n# links.csv: id-imdbid-tmdbid\uff0c\u4e0d\u540c\u7535\u5f71\u5e73\u53f0\u5bf9\u540c\u4e00\u90e8\u7535\u5f71\u7684\u4e0d\u7528\u6807\u8bc6\n# ratings_small.csv : \u8bc4\u5206\u6570\u636e\uff0cuserId-movieId-rating-timestamp","e4290ae9":"# low_memory=False\u5173\u952e\u8bcd\n# low_memory=False \u53c2\u6570\u8bbe\u7f6e\u540e\uff0cpandas\u4f1a\u4e00\u6b21\u6027\u8bfb\u53d6csv\u4e2d\u7684\u6240\u6709\u6570\u636e\uff0c\u7136\u540e\u5bf9\u5b57\u6bb5\u7684\u6570\u636e\u7c7b\u578b\u8fdb\u884c\u552f\u4e00\u7684\u4e00\u6b21\u731c\u6d4b\u3002\u8fd9\u6837\u5c31\u4e0d\u4f1a\u5bfc\u81f4\u540c\u4e00\u5b57\u6bb5\u7684Mixed types\u95ee\u9898\u4e86\u3002\n# \u4f46\u662f\u8fd9\u79cd\u65b9\u5f0f\u771f\u7684\u975e\u5e38\u4e0d\u597d\uff0c\u4e00\u65e6csv\u6587\u4ef6\u8fc7\u5927\uff0c\u5c31\u4f1a\u5185\u5b58\u6ea2\u51fa\uff1b\n# movie_metadata = pd.read_csv('..\/input\/the-movies-dataset\/movies_metadata.csv', low_memory=False)[['original_title', 'id', 'release_date', 'vote_count']].set_index('id')\n# # \u79fb\u9664\u6295\u7968\u6b21\u6570\u5c0f\u4e8e10\u7684\u6837\u672c\n# movie_metadata = movie_metadata[movie_metadata['vote_count']>10].drop('vote_count', axis=1)\n\n# print('Shape Movie-Metadata:\\t{}\\n Contains {} items'.format(movie_metadata.shape, movie_metadata.shape[0]))\n# movie_metadata.sample(5)","aa86d871":"# \u52a0\u8f7dmovielens20m\u6570\u636e\u96c6\n# os.listdir('..\/input\/movielens-20m-dataset\/')\n# tag.csv: userId-movieId-tag-timestamp\n# rating.csv: userId-movieId-rating-timestamp\n# movie.csv: movieId-title-genres\n# link.csv: moiveId-imdbId-tmbdId\n# genome_scores.csv: movieId-tagId-relevance\n# genome_tags.csv: tagId-tag","ab9b274e":"# movie_movielens = pd.read_csv('..\/input\/movielens-20m-dataset\/movie.csv').set_index('movieId')\n# print('Shape MovieLens-movice:\\t{}\\n Contains {} items'.format(movie_movielens.shape, movie_movielens.shape[0]))\n# movie_movielens.head(5)","4da9936e":"# Load single data-file \n# combined_data_1 = pd.read_csv('..\/input\/netflix-prize-data\/combined_data_1.txt', header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n# combined_data_2 = pd.read_csv('..\/input\/netflix-prize-data\/combined_data_2.txt', header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n# combined_data_3 = pd.read_csv('..\/input\/netflix-prize-data\/combined_data_3.txt', header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n# combined_data_4 = pd.read_csv('..\/input\/netflix-prize-data\/combined_data_4.txt', header=None, names=['User', 'Rating', 'Date'], usecols=[0, 1, 2])\n# df_raw = pd.cocat([combined_data_1, combined_data_2, combined_data_3, combined_data_4], axis=0).reset_index()\n# \u9274\u4e8enetflix-prize-data\u4e2d\u5b58\u5728\ndf_raw = pd.read_csv('..\/input\/netflix-prize-data\/combined_data_1.txt', header=None, names=['userId', 'rating', 'Date'], usecols=[0, 1, 2])\nprint('Shape Raw Data:\\t{}'.format(df_raw.shape))\n\n# Find empty rows to slice dataframe for each movie\n# \u7f16\u7801\u601d\u8def\u662f\u5148\u627e\u51fa\u7f3a\u5931\u503c\u7684\u7d22\u5f15\uff0c\u7136\u540e\u904d\u5386\u8fc7\u6ee4\u6389\u7d22\u5f15\u503c\ntmp_movies = df_raw[df_raw['rating'].isna()]['userId'].reset_index()\nmovie_indices = [[index, int(movie[:-1])] for index, movie in tmp_movies.values] # drop ':'\n\n# Shift the movie_indices by one to get start and endpoints of all movies\nshifted_movie_indices = deque(movie_indices)\nshifted_movie_indices.rotate(-1)  # the first element turn to the last element.\n\n\n# Gather all dataframes\nuser_data = []\n\n# Iterate over all movies\nfor [df_id_1, movie_id], [df_id_2, next_movie_id] in zip(movie_indices, shifted_movie_indices):\n    \n    # Check if it is the last movie in the file\n    if df_id_1<df_id_2:\n        tmp_df = df_raw.loc[df_id_1+1:df_id_2-1].copy()\n    else:\n        tmp_df = df_raw.loc[df_id_1+1:].copy()\n        \n    # Create movie_id column\n    tmp_df['movieId'] = movie_id\n    \n    # Append dataframe to list\n    user_data.append(tmp_df)\n\n# Combine all dataframes\nnetflix_prize_User = pd.concat(user_data)\ndel user_data, df_raw, tmp_movies, tmp_df, shifted_movie_indices, movie_indices, df_id_1, movie_id, df_id_2, next_movie_id\nprint('Shape User-Ratings:\\t{}'.format(netflix_prize_User.shape))\nnetflix_prize_User.sample(5)","3ece52fe":"# movie_dataset_User = pd.read_csv('..\/input\/the-movies-dataset\/ratings.csv', low_memory=False)\n# print('Shape User-Ratings:\\t{}'.format(movie_dataset_User.shape))\n# movie_dataset_User.head(5)","838632b2":"# movielens_movie_User = pd.read_csv('..\/input\/movielens-20m-dataset\/rating.csv')\n# print('Shape MovieLens-movice:\\t{}'.format(movielens_movie_User.shape))\n# movielens_movie_User.head(5)","41871199":"def filter_user_item(user_item_rating, min_nb_item_ratings=300, min_nb_user_ratings=200):\n    filter_items = (user_item_rating['movieId'].value_counts() > min_nb_item_ratings)\n    filter_items = filter_items[filter_items].index.tolist()\n    \n    filter_users = (user_item_rating['userId'].value_counts() > min_nb_user_ratings)\n    filter_users = filter_users[filter_users].index.tolist()\n    filter_ret = user_item_rating[(user_item_rating['movieId'].isin(filter_items)) & (user_item_rating['userId'].isin(filter_users))]\n    print('Shape User-Ratings unfiltered:\\t{}'.format(user_item_rating.shape))\n    print('Shape User-Ratings filtered:\\t{}'.format(filter_ret.shape))\n    return filter_ret","f7882333":"# netflix_prize_User\nfiltered_netflix_prize_User = filter_user_item(netflix_prize_User)\n# filtered_movie_dataset_User = filter_user_item(movie_dataset_User)\n# filtered_movielens_movie_User = filter_user_item(movielens_movie_User)","82038660":"del netflix_prize_User#, movie_dataset_User, movielens_movie_User","1d6f99fb":"def get_train_test(filtered_user_item, test_size=0.5):\n    X_train, X_test, _, _ = train_test_split(filtered_user_item.reset_index(), filtered_user_item['movieId'].values, test_size=test_size, random_state=2020, stratify=filtered_user_item['movieId'].values)\n    return X_train, X_test","71bfeb36":"# train_data1, test_data1 = get_train_test(filtered_movie_dataset_User)\n# movieId1 = train_data1.movieId\n# userId1 = train_data1.userId\n# train_data2, test_data2 = get_train_test(filtered_movielens_movie_User)\n# movieId2 = train_data2.movieId\n# userId2 = train_data2.userId\ntrain_data3, test_data3 = get_train_test(filtered_netflix_prize_User)\nmovieId3 = train_data3.movieId\nuserId3 = train_data3.userId\n# del filtered_movie_dataset_User, filtered_movielens_movie_User, filtered_netflix_prize_User\n# del filtered_netflix_prize_User","9ac1493f":"def get_user_item_rating_mat(data):\n    return data.pivot_table(index='userId', columns='movieId', values='rating')","5806d9e1":"# train_data1 = get_user_item_rating_mat(train_data1)\n# train_data2 = get_user_item_rating_mat(train_data2)\nmatrix_train_data3 = get_user_item_rating_mat(train_data3)\n# train_data1.sample(4), train_data2.sample(4), train_data3.sample(4)\nmatrix_train_data3.head(5)","d57fe945":"# train_data1.to_csv('train_data1.csv', index=False, header=None)\n# train_data2.to_csv('train_data2.csv', index=False, header=None)\n# train_data3.to_csv('train_data3.csv', index=False, header=None)","763e076b":"# del train_data1, train_data2, train_data3\n# del train_data3","7e8adb96":"def mean_rating(train, test):\n    # 0\uff1a\u8868\u793a\u6cbf\u7740\u6bcf\u4e00\u5217\u6216\u884c\u6807\u7b7e\/\u7d22\u5f15\u503c\u5411\u4e0b\u6267\u884c\u65b9\u6cd5\n    # 1\uff1a\u8868\u793a\u6cbf\u7740\u6bcf\u4e00\u884c\u6216\u5217\u6807\u7b7e\/\u7d22\u5f15\u503c\u5411\u53f3\u6267\u884c\u65b9\u6cd5\n    ratings_mean = train.mean(axis=0).rename('rating_mean')\n    df_pred = test.set_index('movieId').join(ratings_mean)[['rating', 'rating_mean']]\n#     df_pred.fillna(df_pred.mean(), inplace=True)\n    rmse = np.sqrt(mean_squared_error(y_true=df_pred['rating'], y_pred=df_pred['rating_mean']))\n    print(\"mean rating's rmse is {}\".format(rmse))\n    return rmse","1aa11291":"# train_data3 = pd.read_csv('.\/train_data3.csv',header=None)\n# train_data3.head(5)","78d1bbc5":"# train_data3.index = userId3\n# train_data3.columns = movieId3 ","c5fc95ee":"# train_data3 = pd.read_csv('.\/train_data3.csv', header=None, index_col=userId3.values, names=movieId3.values)\n# mean_rating_data1 = mean_rating(train_data3, test_data3)\n# del train_data3","972c802b":"# train_data1 = pd.read_csv('.\/train_data1.csv')\n# mean_rating_data1 = mean_rating(train_data1, test_data1)\n# del train_data1\n# train_data2 = pd.read_csv('.\/train_data2.csv')\n# mean_rating_data2 = mean_rating(train_data2, test_data2)\n# del train_data2\n# train_data3 = pd.read_csv('.\/train_data3.csv')\n# mean_rating_data3 = mean_rating(train_data3, test_data3)\n# del train_data3\nmean_rating_data3 = mean_rating(matrix_train_data3, test_data3)","93348013":"def weighted_mean_rating(train, test, m=1000):\n    C = train.stack().mean()  # \u4e00\u4e2a\u6d6e\u70b9\u6570\n    \"\"\"\n    \u6570\u636e\u683c\u5f0f\u5982\u4e0b\uff1a\n    userId1:\n    movieId11, rating\n    movieId12, rating\n    userId2:\n    movieId21, rating\n    movieId22, rating\n    \"\"\"\n    R = train.mean(axis=0).values # movie\u4e2a\u6570\u7684\u4e00\u4e2aarray\uff0c\u6bcf\u4e2a\u503c\u4e3arating\u7684\u5e73\u5747\u503c\n    v = train.count().values # movie\u4e2a\u6570\u7684\u4e00\u4e2aarray\uff0c\u6bcf\u4e2a\u503c\u4e3auser\u7684\u4e2a\u6570\n    weighted_score = (v\/ (v+m) *R) + (m\/ (v+m) *C)\n    df_prediction = test.set_index('movieId').join(pd.DataFrame(weighted_score, index=train.columns, columns=['prediction']))[['rating', 'prediction']]\n    y_true = df_prediction['rating']\n    y_pred = df_prediction['prediction']\n    rmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\n    print('weighted mean rating\"s rmse is {}'.format(rmse))\n    return rmse","3ed7f5fe":"weighted_mean_rating_data3 = weighted_mean_rating(matrix_train_data3, test_data3, 50)","29b0f6ac":"def cosine_u2u_similarity(train, test, n_recommendation=100):\n    train_imputed = train.T.fillna(train.mean(axis=1)).T  # \u5229\u7528\u5747\u503c\u8fdb\u884c\u586b\u5145NaN\n    similarity = cosine_similarity(train_imputed.values)  # \u8ba1\u7b97\u7528\u6237\u4e4b\u95f4\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\n    similarity -= np.eye(similarity.shape[0]) # \u51cf\u53bb\u81ea\u8eab\u76f8\u4f3c\u5ea6\n    \n    prediction = []\n    userId_idx_mapping = {userId:idx for idx, userId in enumerate(train_imputed.index)}\n    for userId in test.userId.unique():\n        similarity_user_index = np.argsort(similarity[userId_idx_mapping[userId]])[::-1]\n        similarity_user_score = np.sort(similarity[userId_idx_mapping[userId]])[::-1]\n        for movieId in test[test.userId == userId].movieId.values:\n            \n            score = (train_imputed.iloc[similarity_user_index[:n_recommendation]][movieId] * similarity_user_score[:n_recommendation]).values.sum() \/ similarity_user_score[:n_recommendation].sum()\n            prediction.append([userId, movieId, score])\n    \n    # Create prediction DataFrame\n    df_pred = pd.DataFrame(prediction, columns=['userId', 'movieId', 'prediction']).set_index(['userId', 'movieId'])\n    df_pred = test.set_index(['userId', 'movieId']).join(df_pred)\n\n\n    # Get labels and predictions\n    y_true = df_pred['rating'].values\n    y_pred = df_pred['prediction'].values\n\n    # Compute RMSE\n    rmse = np.sqrt(mean_squared_error(y_true=y_true, y_pred=y_pred))\n    print(\"consine_u2u_similarity's rmse is {}\".format(rmse))\n    return rmse","fee637bd":"cosine_u2u_similarity_data3 = cosine_u2u_similarity(matrix_train_data3, test_data3, n_recommendation=100)","2846e5b7":"def matrix_factorization_dot(train, test, embedding_size=50):\n    userId_idx_mapping = {userId:idx for idx, userId in enumerate(train.userId.unique())}\n    movieId_idx_mapping = {movieId:idx for idx, movieId in enumerate(train.movieId.unique())}\n    # \u548creset_index\u51fd\u6570\u4e00\u6837\uff0c\u4e3a\u4e86\u65b9\u4fbfNN\u6a21\u578b\u7684\u8f93\u5165\uff08\u4e3b\u8981\u4f53\u73b0\u5728Batch\u7684\u83b7\u53d6\u4e0a\uff09\n    train_user_data = train.userId.map(userId_idx_mapping)\n    train_movie_data = train.movieId.map(movieId_idx_mapping)\n    \n    test_user_data = test.userId.map(userId_idx_mapping)\n    test_movie_data = test.movieId.map(movieId_idx_mapping)\n    \n    nb_users = len(userId_idx_mapping)\n    nb_movies = len(movieId_idx_mapping)\n    \n    \n    # \u521b\u5efa\u6a21\u578b\n    # \u5b9a\u4e49\u8f93\u5165\uff0c\u7ef4\u5ea6\n    userId_input = Input(shape=[1], name='user')\n    movieId_input = Input(shape=[1], name='movie')\n    # \u521b\u5efaembedding\u5c42\n    user_embedding = Embedding(\n        output_dim=embedding_size,\n        input_dim=nb_users,\n        input_length=1,\n        name='user_embedding'\n    )(userId_input)\n    \n    movie_embedding = Embedding(\n        output_dim=embedding_size,\n        input_dim=nb_movies,\n        input_length=1,\n        name='movie_embedding'\n    )(movieId_input)\n    # Reshape the embedding layers\n    user_vector = Reshape([embedding_size])(user_embedding)\n    movie_vector = Reshape([embedding_size])(movie_embedding)\n\n    # Compute dot-product of reshaped embedding layers as prediction\n    y = Dot(1, normalize=False)([user_vector, movie_vector])\n\n    # Setup model\n    model = Model(inputs=[userId_input, movieId_input], outputs=y)\n    model.compile(loss='mse', optimizer='adam')\n\n\n    # Fit model\n    model.fit([train_user_data, train_movie_data],\n              train.rating,\n              batch_size=256, \n              epochs=10,\n              validation_split=0.4,\n              shuffle=True)\n\n    # Test model\n    y_pred = model.predict([test_user_data, test_movie_data])\n    y_true = test.rating.values\n\n    #  Compute RMSE\n    rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\n    print('\\n\\nTesting Result With Keras Matrix-Factorization: {:.4f} RMSE'.format(rmse))\n    return rmse","b6af2121":"matrix_factorization_dot_train3 = matrix_factorization_dot(train_data3, test_data3)","fb45ed25":"def matrix_factorization_dnn(train, test, nb_user_embedding=20, nb_movie_embedding=40):\n    userId_idx_mapping = {userId:idx for idx, userId in enumerate(train.userId.unique())}\n    movieId_idx_mapping = {movieId:idx for idx, movieId in enumerate(train.movieId.unique())}\n    \n    # Create correctly mapped train- & testset\n    train_user_data = train.userId.map(userId_idx_mapping)\n    train_movie_data = train.movieId.map(movieId_idx_mapping)\n\n    test_user_data = test.userId.map(userId_idx_mapping)\n    test_movie_data = test.movieId.map(movieId_idx_mapping)\n    \n    nb_users = len(userId_idx_mapping)\n    nb_movies = len(movieId_idx_mapping)\n    ##### Create model\n    # Set input layers\n    userId_input = Input(shape=[1], name='user')\n    movieId_input = Input(shape=[1], name='movie')\n\n  \n    \n    # Create embedding layers for users and movies\n    user_embedding = Embedding(output_dim=nb_user_embedding, \n                               input_dim=nb_users,\n                               input_length=1, \n                               name='user_embedding')(userId_input)\n    movie_embedding = Embedding(output_dim=nb_movie_embedding, \n                                input_dim=nb_movies,\n                                input_length=1, \n                                name='item_embedding')(movieId_input)\n\n    # Reshape the embedding layers\n    user_vector = Reshape([nb_user_embedding])(user_embedding)\n    movie_vector = Reshape([nb_movie_embedding])(movie_embedding)\n\n    # Concatenate the reshaped embedding layers\n    concat = Concatenate()([user_vector, movie_vector])\n\n    # Combine with dense layers\n    dense = Dense(256)(concat)\n    y = Dense(1)(dense)\n\n    # Setup model\n    model = Model(inputs=[userId_input, movieId_input], outputs=y)\n    model.compile(loss='mse', optimizer='adam')\n\n\n    # Fit model\n    model.fit([train_user_data, train_movie_data],\n              train.rating,\n              batch_size=256, \n              epochs=5,\n              validation_split=0.5,\n              shuffle=True)\n\n    # Test model\n    y_pred = model.predict([test_user_data, test_movie_data])\n    y_true = test.rating.values\n\n    #  Compute RMSE\n    rmse = np.sqrt(mean_squared_error(y_pred=y_pred, y_true=y_true))\n    print('\\n\\nTesting Result With Keras Deep Learning: {:.4f} RMSE'.format(rmse))\n    return rmse","c806ae5d":"matrix_factorization_dnn_train3 = matrix_factorization_dnn(train_data3, test_data3)","fccd9f9d":"def surprise_library(data):\n    # Load dataset into surprise specific data-structure\n    sampled_data = sp.Dataset.load_from_df(data[['userId', 'movieId', 'rating']].sample(20000), sp.Reader())\n\n    benchmark = []\n    # Iterate over all algorithms\n    for algorithm in [sp.SVD(), sp.SVDpp(), sp.SlopeOne(), sp.NMF(), sp.NormalPredictor(), sp.KNNBaseline(), sp.KNNBasic(), sp.KNNWithMeans(), sp.KNNWithZScore(), sp.BaselineOnly(), sp.CoClustering()]:\n        # Perform cross validation\n        results = cross_validate(algorithm, sampled_data, measures=['RMSE', 'MAE'], cv=3, verbose=False)\n\n        # Get results & append algorithm name\n        tmp = pd.DataFrame.from_dict(results).mean(axis=0)\n        tmp = tmp.append(pd.Series([str(algorithm).split(' ')[0].split('.')[-1]], index=['Algorithm']))\n\n        # Store data\n        benchmark.append(tmp)\n    return benchmark","f9949578":"surprise_train3 = surprise_library(filtered_netflix_prize_User)","fa97a430":"surprise_train3 = pd.DataFrame(surprise_train3).set_index('Algorithm')","56e83e3c":"# surprise_train3['test_rmse'].tolist()","e1b8e459":"def lightfm_library(train, test):\n    # Create user- & movie-id mapping\n    user_id_mapping = {id:i for i, id in enumerate(train['userId'].unique())}\n    movie_id_mapping = {id:i for i, id in enumerate(train['movieId'].unique())}\n    \n    # Create correctly mapped train- & testset\n    train_user_data = train['userId'].map(user_id_mapping)\n    train_movie_data = train['movieId'].map(movie_id_mapping)\n\n    test_user_data = test['userId'].map(user_id_mapping)\n    test_movie_data = test['movieId'].map(movie_id_mapping)\n\n\n    # Create sparse matrix from ratings\n    shape = (len(user_id_mapping), len(movie_id_mapping))\n    train_matrix = coo_matrix((train['rating'].values, (train_user_data.astype(int), train_movie_data.astype(int))), shape=shape)\n    test_matrix = coo_matrix((test['rating'].values, (test_user_data.astype(int), test_movie_data.astype(int))), shape=shape)\n\n\n    # Instantiate and train the model\n    model = LightFM(loss='warp', no_components=20)\n    model.fit(train_matrix, epochs=10, num_threads=2)\n\n\n    # Evaluate the trained model\n    k = 20\n    precision_score = precision_at_k(model, test_matrix, k=k).mean()\n#     print('Train precision at k={}:\\t{:.4f}'.format(k, precision_at_k(model, train_matrix, k=k).mean()))\n    print('Test precision at k={}:\\t\\t{:.4f}'.format(k, precision_score))\n    return precision_score","cd8c2ba2":"lightfm_train3 = lightfm_library(train_data3, test_data3)","5d0983d6":"## DeepFM\ndef deepfm_algo(data):\n\n    sparse_features = [\"movieId\", \"userId\"]\n    target = ['rating']\n    for feat in sparse_features:\n            lbe = LabelEncoder()\n            data[feat] = lbe.fit_transform(data[feat])\n    \n    fixlen_feature_columns = [SparseFeat(feat, data[feat].nunique(), embedding_dim=4)\n                              for feat in sparse_features]\n    \n    linear_feature_columns = fixlen_feature_columns\n    dnn_feature_columns = fixlen_feature_columns\n    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n    \n    train, test = train_test_split(data, test_size=0.5)\n    train_model_input = {name:train[name].values for name in feature_names}\n    test_model_input = {name:test[name].values for name in feature_names}\n\n    # 4.Define Model,train,predict and evaluate\n    model = DeepFM(linear_feature_columns, dnn_feature_columns, task='regression')\n    model.compile(\"adam\", \"mse\", metrics=['mse'], )\n    \n    history = model.fit(train_model_input, train[target].values,\n                        batch_size=256, epochs=5, verbose=2, validation_split=0.5, )\n    pred_ans = model.predict(test_model_input, batch_size=256)\n    rmse = np.sqrt(mean_squared_error(test[target].values, pred_ans))\n    print(\"test RMSE\", rmse)\n    return rmse","b3f3f323":"deepfm_algor_train3 = deepfm_algo(filtered_netflix_prize_User)","df530765":"ret_rmse = [mean_rating_data3, weighted_mean_rating_data3, cosine_u2u_similarity_data3, matrix_factorization_dot_train3, matrix_factorization_dnn_train3, lightfm_train3, deepfm_algor_train3] + surprise_train3['test_rmse'].tolist() \nret_rmse_name = ['mean_rating', 'weighted', 'cosine_u2u_similarity', 'mf_dot', 'mf_dnn', 'lightfm', 'deepfm'] + surprise_train3.index.tolist()\nfigure, ax = plt.subplots(figsize=(16,4))\nprint(ret_rmse)\nplt.bar(range(len(ret_rmse)), ret_rmse, tick_label=ret_rmse_name)\nfor tick in ax.get_xticklabels():\n    tick.set_rotation(90)\nplt.title('Different RMSE in Dataset by RS algorithm')\nplt.show()","1918a7f5":"### <a id=7.2>7.2. [Weighted Mean Rating](https:\/\/www.quora.com\/How-does-IMDbs-rating-system-work)<\/a>\n\u501f\u52a9\u8d1d\u53f6\u65af\u4f30\u8ba1\uff08the Bayesian estimate\uff09\uff0c\u6743\u91cd\u8bc4\u5206\u516c\u5f0f\u5982\u4e0b\uff1a\n$$(WR) = \\frac{v}{v+m} \\times R + \\frac{m}{v+m} \\times C$$\n\u5176\u4e2d\uff0c$R$\u4e3a\u7535\u5f71\u7684\u5e73\u5747\u503c\uff0c$v$\u4e3a\u7535\u5f71\u7684\u6295\u7968\u6570\u91cf\uff0c$m$\u4e3aTop250\u7684\u6700\u4f4e\u7968\u6570\uff08\u5f53\u524d\u503c\u4e3a25000\uff09\uff0c$C$\u4e3a\u6574\u4e2a\u6570\u636e\u96c6\u7684\u5e73\u5747\u7968\u6570\uff08\u5f53\u524d\u4e3a7.0\uff09\u3002","e386e3ca":"## <a id=5>5. \u521b\u5efa\u8bad\u7ec3\u548c\u6d4b\u8bd5\u96c6<\/a>\n\u521b\u5efa\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u7684\u76ee\u7684\u5728\u4e8e\u4f7f\u7528\u63a8\u8350\u7cfb\u7edf\u6d4b\u8bc4\u6307\u6807\u8fdb\u884c\u9a8c\u8bc1\u6a21\u578b\u7684\u6027\u80fd\uff0c\u9274\u4e8erating\u662f\u4e00\u4e2a\u8fde\u7eed\u503c\uff0c\u53ef\u4ee5\u91c7\u7528RMSE\u5ea6\u91cf\u65b9\u5f0f\uff0c\u5373\n$$RMSE(root\\ square\\ error)=\\sqrt{\\frac{\\sum (y_i-z_i)^2}{N}}$$\n\u5176\u4e2d$y_i$\u8868\u793a\u771f\u5b9e\u503c\uff0c$z_i$\u8868\u793a\u9a8c\u8bc1\u503c\u3002","78605d30":"# \u5982\u4f55\u4f7f\u7528\u63a8\u8350\u5de5\u5177\u4ee5\u53ca\u63a8\u8350\u7b97\u6cd5\u8fdb\u884c\u63a8\u8350\n\n**\u5e38\u7528\u7684\u63a8\u8350\u7b97\u6cd5\u5305\u62ec\u57fa\u4e8eSVD\u5bb6\u65cf\u7684\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\uff0c\u57fa\u4e8eFM\uff0cDeepFM\u7b49\u7684\u4e8c\u9636\u4ea4\u4e92\u6a21\u578b**\n\n\u8fd9\u4e2anotebook\u5c3d\u53ef\u80fd\u7684\u5b9e\u73b0\u8fd9\u4e9b\u57fa\u672c\u65b9\u6cd5\u5e76\u5229\u7528RMSE\u5bf9\u7b97\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002\n\n\u6b64notebook\u662f\u4f9d\u636e\u5982\u4e0b\u7684notebooks\u8fdb\u884c\u4e86\u4fee\u6539:\n**[morrisb](https:\/\/www.kaggle.com\/morrisb\/how-to-recommend-anything-deep-recommender)**,**[siavrez](https:\/\/www.kaggle.com\/siavrez\/deepfm-model)**\n***\n+ [1. \u52a0\u8f7d\u5e93\u6587\u4ef6](#1)<br>\n+ [2. \u52a0\u8f7dItem\u6587\u4ef6](#2)<br>\n+ [3. \u52a0\u8f7dUser\u6587\u4ef6](#3)<br>\n+ [4. \u8fc7\u6ee4\u7a00\u758f\u7684User\u548cItem](#4)<br>\n+ [5. \u521b\u5efa\u8bad\u7ec3\u548c\u6d4b\u8bd5\u96c6](#5)<br>\n+ [6. \u8f6c\u6362User-Ratings\u5230User-Item-Rating-Matrix](#6)<br>\n+ [7. \u63a8\u8350\u7b97\u6cd5\u5f15\u64ce](#7)<br>\n + [7.1. Mean Rating](#7.1)<br>\n + [7.2. Weighted Mean Rating](#7.2)<br>\n + [7.3. Cosine User-User Similarity](#7.3)<br>\n + [7.4. Matrix Factorisation With Keras And Gradient Descent](#7.5)<br>\n + [7.5. Deep Learning With Keras](#7.6)<br>\n+ [8. Exploring Python Libraries](#8)<br>\n + [8.1. Surprise Library](#8.1)<br>\n + [8.2. Lightfm Library](#8.2)<br>\n + [8.3. Deepctr Library](#8.3)<br>\n+ [9. Conclusion](#9)<br>\n***\n## <a id=1>1. \u52a0\u8f7d\u5e93\u6587\u4ef6<\/a>","1dfa4720":"### <a id=7.6>7.6. Deep Learning With Keras<\/a>\n\u6dfb\u52a0\u6a21\u578b\u6df1\u5ea6\uff0c\u4f7f\u7528DNN\u62df\u5408user-item-rating\u77e9\u9635\u503c\uff0c\u8fd9\u91cc\u4ec5\u6dfb\u52a0\u4e86\u4e00\u4e2a\u5168\u8fde\u63a5\u5c42\uff08dense\uff09\uff0c\u4f7f\u7528\u77e9\u9635\u62fc\u63a5\u4f5c\u4e3amodel\u7684\u8f93\u5165\u3002","eb6096bc":"## <a id=3>3. \u52a0\u8f7dUser\u6587\u4ef6<\/a>\n\u5176\u4e2d\u6bcf\u6761user\u6837\u672c\uff0c\u90fd\u7c7b\u4f3c\u5173\u8054\u7b97\u6cd5\u4e2d\u7684transaction\u3002\u7edf\u4e00User-Item-Rating\u7684columns\u4e3auserId-itemId-rating\u3002","c594f11b":"## <a id=7>7. \u63a8\u8350\u5f15\u64ce<\/a>\n### <a id=7.1>7.1. Mean Rating<\/a>\n\u4f7f\u7528Mean Rating\u4f5c\u4e3a\u6700\u7ec8\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u8fd9\u6837\u7684\u7ed3\u679c\u4f1a\u5bfc\u81f4rating\u5177\u6709\u504f\u5411\u6027\uff0c\u6536\u89c6\u7387\u8f83\u9ad8\u7684\uff08\u5373\u6bcf\u5217\u4e2dNaN\u7684\u503c\u8f83\u5c11\uff09\u4f1a\u53d7\u5230\u5f71\u54cd\uff0c\u4f7f\u5f97\u5176rating\u504f\u4f4e\uff0c\u8fdb\u4e00\u6b65\u8ba9rating\u7ed3\u679c\u504f\u5411\u4e8e\u6536\u89c6\u7387\u8f83\u4f4e\u7684rating\u3002","ab488e53":"### <a id=8.3>8.3. DeepCTR<\/a>\n[DeepCTR](https:\/\/github.com\/shenweichen\/DeepCTR)\u662f\u4e00\u4e2a\u57fa\u4e8e\u6df1\u5ea6\u7684CTR\u9884\u6d4b\u5e93\u3002","0ae3d043":"\u7531\u4e0a\u53ef\u77e5\uff0c\u5176\u4e2duser-item-rating-matrix\u4e2d\u6709\u5927\u91cf\u7684NaN\u503c\uff0c\u5bf9\u4e8ePureSVD\u7684\u8f93\u5165\u662f\u4e0d\u5408\u6cd5\u7684\uff0c\u56e0\u6b64\u82e5\u4f7f\u7528PureSVD\u7b97\u6cd5\u7684\u8bdd\uff0c\u9700\u8981\u5bf9\u77e9\u9635\u4e2d\u7684NaN\u503c\u8fdb\u884c\u586b\u5145\u3002","a32e2f26":"### <a id=7.3>7.3. Cosine User-User Similarity<\/a>\n\u5229\u7528\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u7528\u6237\u5411\u91cf\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\uff0c\u7136\u540e\u5229\u7528\u8fd9\u4e2a\u76f8\u4f3c\u5ea6\u4f5c\u4e3a\u4e00\u4e2a\u7535\u5f71\u8bc4\u5206\u6743\u91cd\u548c\u5f53\u524d\u7535\u5f71\u7684\u8bc4\u5206\u505a\u52a0\u6743\u76f8\u4e58\u3002\n$$score=\\frac{\\sum cosine_{ij} rating_{ij}}{\\sum cosine_{ij}}$$\n\u9700\u8981\u6ce8\u610f\u76841\uff09\u7f29\u653e\u56e0\u5b50\uff1b2\uff09\u548c\u4e4b\u524d\u4e24\u79cd\u7b97\u6cd5\u76f8\u6bd4\u66f4\u52a0\u7ec6\u5316\uff0c\u7ec6\u5316\u81f3userId\uff1b3\uff09\u8d85\u53c2\u6570\u76f8\u4f3c\u5ea6\u6392\u540dtop-n\u3002","b9f5c94b":"## <a id=9>9. \u603b\u7ed3<\/a>\nOther **python recommender libraries** are:\n+ [implicit](https:\/\/github.com\/benfred\/implicit)\n+ [spotlight](https:\/\/github.com\/maciejkula\/spotlight)\n+ [turicreate](https:\/\/github.com\/apple\/turicreate\/blob\/master\/README.md)\n+ [mrec](https:\/\/github.com\/Mendeley\/mrec)\n+ [recsys](https:\/\/github.com\/ocelma\/python-recsys)\n+ [crab](http:\/\/muricoca.github.io\/crab\/)","de69173a":"### <a id=7.4>7.4. Matrix Factorization With Keras And Gradient Descent<\/a>\n\u9274\u4e8euser-item-rating\u662f\u9ad8\u7ef4\u4e14\u7a00\u758f\u7684\u77e9\u9635\uff0c\u56e0\u6b64\u53ef\u4ee5\u7528embedding\u5f62\u5f0f\u8868\u793amovieId\u548cuserId\uff0c\u7136\u540e\u4f7f\u7528Dot\u64cd\u4f5c\u53bb\u62df\u5408\u8fd9\u4e2auser-item-rating\u77e9\u9635\u3002","52f80cef":"## <a id=2>2. \u52a0\u8f7dItem\u6587\u4ef6<\/a>","0f6c57b9":"## <a id=8>8. Exploring Python Libraries<\/a>\n### <a id=8.1>8.1. Surprise Library<\/a>\n[surprise library](http:\/\/surpriselib.com\/) \u662f\u4e3a\u63a8\u8350\u7cfb\u7edf\u800c\u6784\u5efa\u7684\u4e00\u4e2a\u5e93\uff0c\u6709\u5f88\u591a\u5185\u7f6e\u7b97\u6cd5\u3002\n\n[SVD](https:\/\/surprise.readthedocs.io\/en\/stable\/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD)\uff0c\u5176\u9884\u6d4brating\u4e3a$\\bar{r}_{ui}=\\mu+b_u+b_i+q_i^Tp_u$\uff0c\u82e5$u$\u662f\u672a\u77e5\u7684\uff0c\u5219\u504f\u7f6e$b_u$\u548c\u56e0\u5b50$p_u$\u5047\u8bbe\u4e3a0\uff0c\u540c\u7406\u5bf9\u4e8eitem $i$\u7684$b_i$\u548c$q_i$\u3002\u6b64\u65f6\u7684\u8bc4\u4f30\u51fd\u6570\u5982\u4e0b:\n$$\\sum_{r_{ui}\\in R_{train}}(r_{ui}-\\bar{ui})^2 + \\lambda(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2)$$\n\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u8fdb\u884c\u53c2\u6570\u5b66\u4e60\uff0c\n$$b_u \\gets b_u + \\alpha(e_{ui} - \\lambda b_u)$$\n$$b_i \\gets b_i + \\alpha(e_{ui} - \\lambda b_i)$$\n$$p_u \\gets p_u + \\alpha(e_{ui} \\cdot q_i - \\lambda p_u)$$\n$$q_i \\gets q_i + \\lambda (e_{ui} \\cdot p_u - \\lambda q_i)$$\n\u5176\u4e2d$e_{ui}=r_{ui}-\\bar{r}_{ui}$\u3002\n\n[SVD++](https:\/\/surprise.readthedocs.io\/en\/stable\/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVDpp)\u5176\u4e2d\u9884\u6d4brating\u4e3a$\\bar{r}_{ui}=\\mu + b_u + b_i + q_i^T(p_u + |I_u|^{-\\frac{1}{2}}\\sum_{j \\in I_u}y_j)$\uff0c\u5176\u4e2d$y_j$\u662f\u4e00\u7ec4\u9690\u5f0f\u56e0\u5b50\uff0c\u4e3b\u8981\u63cf\u8ff0\u4e86user $u$\u5bf9item $j$\u7684\u8bc4\u4ef7\u7684\u4e8b\u5b9e\uff0c\u548c\u8bc4\u4ef7\u7684rating\u65e0\u5173\u3002\n\n[Slope One](https:\/\/surprise.readthedocs.io\/en\/stable\/slope_one.html)\u5176\u4e2d\u9884\u6d4b\u7684rating\u4e3a$\\bar{r_{ui}}=\\mu_u + \\frac{1}{|R_i(u)|} \\sum_{j \\in R_i(u)} dev(i,j)$\uff0c\u5176\u4e2d$R_i(u)$\u662fitem\u7684\u96c6\u5408\uff0c\u5b83\u662f\u6309\u7167user $u$\u7684\uff0c\u5e76\u4e14\u8fd9\u4e2a\u96c6\u5408user $j$\u540c\u6837\u8bc4\u4ef7\u8fc7\uff0c$dev(i,j)$\u88ab\u5b9a\u4e49\u4e3a$dev(i,j) = \\frac{1}{U_{ij}} \\sum_{u \\in U_{ij}} r_{ui}-r_{uj}$\u3002\n\n[NMF](https:\/\/surprise.readthedocs.io\/en\/stable\/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.NMF)\u5176\u4e2d\u9884\u6d4brating\u4e3a$\\bar{r}_{ui}=q_i^Tp_u$\uff0c\u540c\u6837\u4f7f\u7528\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff0c\u5176\u4e2ditem\u548cuser\u7684\u9690\u56e0\u5b50$f$\u66f4\u65b0\u5982\u4e0b\uff1a\n$$p_{uf} \\gets p_{uf} \\cdot \\frac{\\sum_{i \\in I_u}q_{if} \\cdot r_{ui}}{\\sum_{i \\in I_u} q_{if} \\cdot \\bar{r}_{ui} + \\lambda_u |I_u| p_{uf}}$$\n$$q_{if} \\gets q_{if} \\cdot \\frac{\\sum_{u \\in U_i}p_{uf} \\cdot r_{ui}}{\\sum_{u \\in U_i}p_{uf} \\cdot \\bar{r}_{ui} + \\lambda_i |U_i| q_{if}}$$\n\u5176\u4e2d$\\lambda_u$\u548c$\\lambda_i$\u662f\u8d85\u53c2\u6570\uff0c\u4e14\u6b64\u7b97\u6cd5\u9ad8\u5ea6\u4f9d\u8d56\u521d\u59cb\u5316\u503c\u3002\n\n[NormalPredictor](https:\/\/surprise.readthedocs.io\/en\/stable\/basic_algorithms.html#surprise.prediction_algorithms.random_pred.NormalPredictor)\u5176\u4e2d\u9884\u6d4brating\u57fa\u4e8e\u6b63\u5219\u5316\u7684\u5047\u8bbe\u4e0a\u5373\n$$\\begin{split}\\hat{\\mu} &= \\frac{1}{|R_{train}|} \\sum_{r_{ui} \\in R_{train}}\nr_{ui}\\\\\\\\        \\hat{\\sigma} &= \\sqrt{\\sum_{r_{ui} \\in R_{train}}\n\\frac{(r_{ui} - \\hat{\\mu})^2}{|R_{train}|}}\\end{split}$$\n \n[KNNBasic](https:\/\/surprise.readthedocs.io\/en\/stable\/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBasic) \u5176\u4e2drating\u7684\u9884\u6d4b\u57fa\u4e8eKNN\u7684\u76f8\u4f3c\u6027\u3002\n$$\\hat{r}_{ui} = \\frac{\n\\sum\\limits_{v \\in N^k_i(u)} \\text{sim}(u, v) \\cdot r_{vi}}\n{\\sum\\limits_{v \\in N^k_i(u)} \\text{sim}(u, v)}$$\n\n[KNNWithMeans](https:\/\/surprise.readthedocs.io\/en\/stable\/knn_inspired.html#surprise.prediction_algorithms.knns.KNNWithMeans)\u5728KNNBasic\u7684\u57fa\u7840\u4e0a\u6dfb\u52a0\u4e86\u5747\u503c\u3002\n$$\\hat{r}_{ui} = \\mu_u + \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n\\text{sim}(u, v) \\cdot (r_{vi} - \\mu_v)} {\\sum\\limits_{v \\in\nN^k_i(u)} \\text{sim}(u, v)}$$\n\n[KNNWithZScore](https:\/\/surprise.readthedocs.io\/en\/stable\/knn_inspired.html#surprise.prediction_algorithms.knns.KNNWithZScore)\u548cKNNWithMeans\u76f8\u6bd4\uff0c\u5bf9\u6bcf\u4e2auser\u7684rating\u505a\u4e86$z$-score\u5904\u7406\u3002\n$$\n\\hat{r}_{ui} = \\mu_u + \\sigma_u \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n\\text{sim}(u, v) \\cdot (r_{vi} - \\mu_v) \/ \\sigma_v} {\\sum\\limits_{v\n\\in N^k_i(u)} \\text{sim}(u, v)}\n$$\n\n[KNNBaseline](https:\/\/surprise.readthedocs.io\/en\/stable\/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBaseline)\n$$\n\\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n\\text{sim}(u, v) \\cdot (r_{vi} - b_{vi})} {\\sum\\limits_{v \\in\nN^k_i(u)} \\text{sim}(u, v)}\n$$\n\n[BaselineOnly](https:\/\/surprise.readthedocs.io\/en\/stable\/basic_algorithms.html)\u8bc4\u4f30rating\u4e3a$\\bar{r}_{ui}=b_{ui} =\\mu + b_u + b_i$\uff0c\u5f53$u$\u672a\u77e5\u7684\u65f6\u5019\uff0c$b_u$\u5047\u8bbe\u4e3a0\u3002\n\n[CoClustering](https:\/\/surprise.readthedocs.io\/en\/stable\/co_clustering.html#surprise.prediction_algorithms.co_clustering.CoClustering)\n$$\n\\hat{r}_{ui} = \\overline{C_{ui}} + (\\mu_u - \\overline{C_u}) + (\\mu_i - \\overline{C_i})\n$$","7a4ab29a":"## <a id=4>4. \u8fc7\u6ee4\u7a00\u758f\u7684User\u548cItem<\/a>\n\u5bf9\u4e8euser\uff0c\u8fc7\u6ee4\u5176\u4e0e\u8bc4\u5206\u7cfb\u7edf\u4ea4\u4e92\u8f83\u5c11\u7684user\uff0c\u5373\u8bc4\u5206\u7684item\u6570\u91cf\u8f83\u5c11\uff1b\u5bf9\u4e8eitem\uff0c\u5176\u88abuser\u8bc4\u8fc7\u5206\u7684\u6b21\u6570\u8f83\u5c11\u3002\uff08\u5176\u4e3b\u8981\u76ee\u7684\u662f\u4e3a\u4e86\u65b9\u4fbf\u6d4b\u8bd5\uff0c\u5728\u5b9e\u9a8c\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u5e94\u8be5\u5bf9\u7a00\u758f\u7684user\u548citem\u505a\u7279\u6b8a\u5904\u7406\uff0c\u5982\u4f7f\u7528LR\u6a21\u578b\uff0c\u6df1\u5ea6\u6a21\u578b\u7b49\uff09","8d363974":"## <a id=6>6. \u8f6c\u6362User-Ratings\u5230User-Item-Rating-Matrix<\/a>\n\u8f6c\u6362\u77e9\u9635\u4f7f\u5f97DataFrame\u662f\u4ee5userId\u4e3aindex\uff0citemId\u4e3acolumns\uff0c\u5176\u4e2d\u77e9\u9635\u4e2d\u6bcf\u4e2a\u503c\u5bf9\u5e94rating\uff08\u5373\u8bc4\u5206\uff09\u3002","8d3f92c8":"\u5176\u4e2d$m$\u662f\u4e00\u4e2a\u8d85\u53c2\u6570\uff0c\u901a\u8fc7\u8c03\u8282$m$\u6765\u6539\u53d8\u6574\u4f53\u5168\u90e8\u8bc4\u5206\u548c\u6bcf\u4e2a\u7535\u5f71\u8bc4\u5206\u7684\u6bd4\u91cd\u3002","517c109e":"### <a id=8.2>8.2. Lightfm Library<\/a>\n[lightfm librariy](https:\/\/github.com\/lyst\/lightfm)\u91cd\u70b9\u5173\u6ce8\u5177\u6709\u663e\u5f0f\u548c\u9690\u5f0f\u7684\u77e9\u9635\u5206\u89e3\uff0c\u6b64\u5916\u53ef\u4ee5\u5229\u7528item\u7b49\u5143\u4fe1\u606f\u6765\u8fbe\u5230\u57fa\u4e8e\u5185\u5bb9\u63a8\u8350\u548c\u534f\u540c\u63a8\u8350\u5171\u540c\u4f5c\u7528\u7684\u6df7\u5408\u6a21\u578b\uff0c\u4ece\u800c\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u51cf\u5c11\u4e86\u51b7\u542f\u52a8\u7684\u95ee\u9898\u3002"}}