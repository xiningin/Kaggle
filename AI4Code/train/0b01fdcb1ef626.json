{"cell_type":{"773c05dd":"code","8ac45c2d":"code","f0ac8fe4":"code","0ce078c4":"code","0caf38fa":"code","4bfbff90":"code","e076fd5e":"code","fd682d1c":"code","0f4dd6f1":"code","48c8383a":"code","ebd08bf6":"code","ad5a3a08":"code","eaf4d9d8":"code","2d849c29":"code","87705b3f":"code","a3abdf88":"code","50a83dfb":"code","805cb51c":"code","d717b600":"code","711944f9":"code","0720808d":"code","d121ad91":"code","ceeb3dfa":"code","31a09c74":"code","ef787557":"code","9bd3e191":"code","680e316f":"code","cdc88e26":"code","8028b259":"code","fe6b51e6":"code","80228e4c":"code","8d541748":"code","48b3d177":"code","1f4b5035":"code","17eb29dc":"code","c009a758":"code","d19eeced":"code","1eceace0":"code","fc954335":"code","d848c186":"code","6cc6ba33":"code","92cc18e0":"code","6fcba2c5":"code","26dd194b":"code","21dea661":"code","a4435cd9":"code","76f58487":"code","0522f9a5":"code","c19269eb":"code","d8ccf0e4":"code","d66b2636":"code","2dda173c":"code","46053154":"code","c34d65d8":"code","3976d452":"code","9dc8c61d":"code","0557edeb":"code","10d9f342":"code","8139b223":"code","1a826762":"code","ba0d585b":"code","220f0dcb":"code","4432f5bd":"code","5985032e":"code","f58a7a6d":"markdown","ef7e42b2":"markdown","0a39fbb5":"markdown","37fc7634":"markdown","f9fa2acc":"markdown","ee652b04":"markdown","068d53c4":"markdown","104db8d5":"markdown","86bfacaf":"markdown","496eb104":"markdown"},"source":{"773c05dd":"import numpy as np\nimport matplotlib as plt \nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pylab as plt\n\n%matplotlib inline\n\n#to display all columns we use the below code\npd.set_option('display.max_columns', None)","8ac45c2d":"pip install openpyxl","f0ac8fe4":"#Since data is in form of excel file we have to use pandas read_excel to load the data\ndf = pd.read_excel('\/kaggle\/input\/flight-fare-prediction-mh\/Data_Train.xlsx')","0ce078c4":"df.head()","0caf38fa":"#check whether there is null value in the dataset\ndf.isnull().sum().max()","4bfbff90":"#there is only one null value so we drop\ndf.dropna(inplace=True)","e076fd5e":"#the size of the dataset\ndf.shape","fd682d1c":"#now let's analyze, understand and clean our data\ndf['Airline'].unique()","0f4dd6f1":"df['Airline'].value_counts()","48c8383a":"plt.rcParams[\"figure.figsize\"] = (15,7)\nsns.boxplot( df['Airline'], df['Price'])\nplt.xticks(rotation=45)","ebd08bf6":"# From graph we can see that Jet Airways Business have the highest Price. \n# Apart from Jet Airways Business almost all are having similar median \n# since Jet Airways Business is an outlier so we drop these row\ndf = df.loc[df[\"Airline\"] != 'Jet Airways Business']","ad5a3a08":"df.shape","eaf4d9d8":"df[\"Journey_day\"] = pd.to_datetime(df.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ndf[\"Journey_month\"] = pd.to_datetime(df[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month","2d849c29":"# since it's a one year dataset(2019) we don't need to extract for a year from Date_of_Journey column","87705b3f":"df.head()","a3abdf88":"# Since we have converted Date_of_Journey column into integers, Now we can drop as it is of no use.\n\ndf.drop([\"Date_of_Journey\"], axis = 1, inplace = True)","50a83dfb":"df['Source'].unique()","805cb51c":"df['Source'].value_counts()","d717b600":"df['Destination'].unique()","711944f9":"# as we can see from the dataset we recognize one thing that source, destination, total stops are related with route\n# so we don't need column 'route' the rest express these column so we drop these column also ","0720808d":"df.drop('Route',axis=1, inplace=True)","d121ad91":"df.head()","ceeb3dfa":"# Similar to Date_of_Journey we can extract values from Dep_Time\n\n# Extracting Hours\ndf[\"Dep_hour\"] = pd.to_datetime(df[\"Dep_Time\"]).dt.hour\n\n# Extracting Minutes\ndf[\"Dep_min\"] = pd.to_datetime(df[\"Dep_Time\"]).dt.minute\n\n# Now we can drop Dep_Time as it is of no use\ndf.drop([\"Dep_Time\"], axis = 1, inplace = True)","31a09c74":"# we will do similar thing for Arrival_Time also\n\n# Extracting Hours\ndf[\"Arrival_hour\"] = pd.to_datetime(df.Arrival_Time).dt.hour\n\n# Extracting Minutes\ndf[\"Arrival_min\"] = pd.to_datetime(df.Arrival_Time).dt.minute\n\n# Now we can drop Arrival_Time as it is of no use\ndf.drop([\"Arrival_Time\"], axis = 1, inplace = True)","ef787557":"df.head()","9bd3e191":"#let's analyze & clean 'Duration'\ndf['Duration'].unique()","680e316f":"# Assigning and converting Duration column into list\nduration = list(df[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration","cdc88e26":"# Adding duration_hours and duration_mins list to train_data dataframe\n\ndf[\"Duration_hours\"] = duration_hours\ndf[\"Duration_mins\"] = duration_mins","8028b259":"df.drop([\"Duration\"], axis = 1, inplace = True)","fe6b51e6":"df.head()","80228e4c":"df['Total_Stops'].unique()","8d541748":"df['Total_Stops'] = df['Total_Stops'].replace({'non-stop':0,'1 stop':1,'2 stops':2,'3 stops':3,'4 stops':4})","48b3d177":"df['Total_Stops'].unique()","1f4b5035":"df['Additional_Info'].head()","17eb29dc":"# Additional_Info contains almost 80% no_info\n#we drop 'Additional_Info' column\ndf.drop('Additional_Info',axis=1, inplace=True)","c009a758":"#let's analyze our dependent variable\nsns.distplot(df['Price'])","d19eeced":"#let's remove some outlier's from the dependent column\ndf['Price'].describe()","1eceace0":"len(df[df['Price']>20000])","fc954335":"df.shape","d848c186":"df = df[df['Price']<20000]","6cc6ba33":"df.shape","92cc18e0":"sns.distplot(df['Price'])","6fcba2c5":"Airline = pd.get_dummies(df['Airline'],drop_first=True)\nSource = pd.get_dummies(df['Source'], drop_first= True)\nDestination = pd.get_dummies(df['Destination'], drop_first = True)\ndf = pd.concat([df, Airline, Source, Destination], axis = 1)","26dd194b":"df.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)","21dea661":"df.head()","a4435cd9":"x = df.drop('Price', axis=1)\ny= df['Price']","76f58487":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.model_selection import RandomizedSearchCV","0522f9a5":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","c19269eb":"rf = RandomForestRegressor()\nrf.fit(x_train,y_train)","d8ccf0e4":"y_pred = rf.predict(x_test)","d66b2636":"rf.score(x_test,y_test)","2dda173c":"plt.scatter(y_test, y_pred, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","46053154":"from sklearn import metrics","c34d65d8":"print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","3976d452":"#Choose following method for hyperparameter tuning\n#    RandomizedSearchCV --> Fast\n#    GridSearchCV\n#Assign hyperparameters in form of dictionery\n#Fit the model\n#Check best paramters and best score","9dc8c61d":"#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","0557edeb":"# Create the random grid\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","10d9f342":"# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2,random_state=42,n_jobs=1)","8139b223":"rf_random.fit(x_train, y_train)","1a826762":"rf_random.best_params_","ba0d585b":"prediction = rf_random.predict(x_test)","220f0dcb":"plt.figure(figsize = (8,8))\nplt.scatter(y_test, prediction, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","4432f5bd":"print('MAE:', metrics.mean_absolute_error(y_test, prediction))\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))","5985032e":"#Save the model to reuse it again\nimport pickle\n# open a file, where you ant to store the data\nfile = open('rf.pkl', 'wb')\n\n# dump information to that file\npickle.dump(rf, file)","f58a7a6d":"#### FEATURE SCALING","ef7e42b2":"#### Let's Handle Our Categorical Data","0a39fbb5":"##### on these project i'm using non-linear ml algoritms so we don't need to scale it","37fc7634":"#### we do the samething for the test data except the fitting part","f9fa2acc":"#### Fitting model using Random Forest","ee652b04":"From description we can see that Date_of_Journey is a object data type,\\ Therefore, \nwe have to convert this datatype into timestamp so as to use this column properly for prediction\n\nFor this we require pandas to_datetime to convert object data type to datetime dtype.\n\n**.dt.day method will extract only day of that date**\\ **.dt.month method will extract only month of that date**","068d53c4":"### Hyperparameter Tuning","104db8d5":"### Build a Model ","86bfacaf":"The Machine Learning algorithms that require the feature scaling are mostly KNN (K-Nearest Neighbours), Neural Networks, \nLinear Regression, and Logistic Regression.\n\nThe machine learning algorithms that do not require feature scaling is mostly non-linear ML algorithms such as Decision trees, \nRandom Forest, AdaBoost, Na\u00efve Bayes, etc.","496eb104":"#### Outlier Removal"}}