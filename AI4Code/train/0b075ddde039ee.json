{"cell_type":{"2272335c":"code","e4e34837":"code","65bd2b1d":"code","6d6a8ecc":"code","287bcfa2":"code","b36a9c26":"code","c66835e3":"code","bbde8f61":"code","63ff48a5":"code","a78a6878":"code","82be1273":"code","84c0600b":"code","744874e4":"code","21ff9aa0":"code","c4ad7f76":"code","e2014662":"code","c15b6fa9":"code","4f48356a":"code","85eda77f":"code","a0b75819":"code","e38900ad":"code","864d0233":"code","271fb3f6":"code","48e22019":"code","277adab8":"code","3501eaf1":"code","31f9f294":"code","74b18ca0":"code","36efa118":"code","92b22b36":"code","17bb69a5":"code","d3dc3ff2":"code","3eeeaa94":"code","ad38a2cc":"code","60f09964":"code","cc25fa7e":"code","165f69bc":"code","157d244a":"code","0948b32e":"code","27f9fc80":"code","574518da":"code","cc53c162":"code","0aa484f3":"code","6eb7c157":"code","58178d99":"code","db5c9f03":"code","b7699250":"code","bc35f023":"markdown","972bb784":"markdown","cef5fa99":"markdown","9b67f135":"markdown","7d193656":"markdown","2b00e1f8":"markdown","617770eb":"markdown","5264220b":"markdown"},"source":{"2272335c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport os\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS","e4e34837":"df_yout = pd.read_csv(\"..\/input\/youtube-new\/USvideos.csv\")","65bd2b1d":"print(df_yout.shape)\nprint(df_yout.nunique())","6d6a8ecc":"df_yout.head(n=2)","287bcfa2":"df_yout = df_yout.drop_duplicates(subset=['video_id','trending_date'], keep='last', inplace=False) #duble drop\n\ndf_yout['trending_times'] = np.nan #compute trending_times\nfor v_id in df_yout['video_id'].unique():\n    trending_times = sum(df_yout['video_id'] == v_id)\n    df_yout.loc[(df_yout[\"video_id\"] == v_id),\"trending_times\"] = trending_times\n\ndf_youtube = df_yout.drop_duplicates(subset='video_id', keep='last', inplace=False) #drop","b36a9c26":"df_yout = df_yout.drop_duplicates(subset='video_id', keep='last', inplace=False) #drop","c66835e3":"df_yout.info","bbde8f61":"df_yout['category_name'] = np.nan\n\ndf_yout.loc[(df_yout[\"category_id\"] == 1),\"category_name\"] = 'Film and Animation'\ndf_yout.loc[(df_yout[\"category_id\"] == 2),\"category_name\"] = 'Cars and Vehicles'\ndf_yout.loc[(df_yout[\"category_id\"] == 10),\"category_name\"] = 'Music'\ndf_yout.loc[(df_yout[\"category_id\"] == 15),\"category_name\"] = 'Pets and Animals'\ndf_yout.loc[(df_yout[\"category_id\"] == 17),\"category_name\"] = 'Sport'\ndf_yout.loc[(df_yout[\"category_id\"] == 19),\"category_name\"] = 'Travel and Events'\ndf_yout.loc[(df_yout[\"category_id\"] == 20),\"category_name\"] = 'Gaming'\ndf_yout.loc[(df_yout[\"category_id\"] == 22),\"category_name\"] = 'People and Blogs'\ndf_yout.loc[(df_yout[\"category_id\"] == 23),\"category_name\"] = 'Comedy'\ndf_yout.loc[(df_yout[\"category_id\"] == 24),\"category_name\"] = 'Entertainment'\ndf_yout.loc[(df_yout[\"category_id\"] == 25),\"category_name\"] = 'News and Politics'\ndf_yout.loc[(df_yout[\"category_id\"] == 26),\"category_name\"] = 'How to and Style'\ndf_yout.loc[(df_yout[\"category_id\"] == 27),\"category_name\"] = 'Education'\ndf_yout.loc[(df_yout[\"category_id\"] == 28),\"category_name\"] = 'Science and Technology'\ndf_yout.loc[(df_yout[\"category_id\"] == 29),\"category_name\"] = 'Non Profits and Activism'\ndf_yout.loc[(df_yout[\"category_id\"] == 25),\"category_name\"] = 'News & Politics'","63ff48a5":"df_yout['likes_log'] = np.log(df_yout['likes'] + 1)\ndf_yout['views_log'] = np.log(df_yout['views'] + 1)\ndf_yout['dislikes_log'] = np.log(df_yout['dislikes'] + 1)\ndf_yout['comment_log'] = np.log(df_yout['comment_count'] + 1)","a78a6878":"plt.figure(figsize = (14,9))\n\ng = sns.boxplot(x='category_name', y='views_log', data=df_yout, palette=\"Set3\")\ng.set_xticklabels(g.get_xticklabels(),rotation=45)\ng.set_title(\"Views Distribuition by Category Names\", fontsize=20)\ng.set_xlabel(\"\", fontsize=15)\ng.set_ylabel(\"Views(log)\", fontsize=15)\n\nplt.show()","82be1273":"df_yout['publish_date']=df_yout['publish_time'].apply(lambda x : x.split('T')[0])","84c0600b":"df_yout['weekday']=pd.to_datetime(df_yout['publish_date']).dt.weekday_name\ndf_yout['weekday']","744874e4":"plt.figure(figsize = (14,9))\n\ng = sns.boxplot(x='weekday', y='views_log', data=df_yout, palette=\"Set3\")\ng.set_xticklabels(g.get_xticklabels())\ng.set_title(\"Views Distribuition by Publish Date\", fontsize=20)\ng.set_xlabel(\"\", fontsize=15)\ng.set_ylabel(\"Views(log)\", fontsize=15)\n\nplt.show()","21ff9aa0":"df_yout['weekday']=pd.to_datetime(df_yout['publish_date']).dt.weekday","c4ad7f76":"df_yout['publish_time']=df_yout['publish_time'].apply(lambda x : x.split('T')[1])","e2014662":"df_yout['publish_hour']=df_yout['publish_time'].apply(lambda x : int(x[0:2]))","c15b6fa9":"plt.figure(figsize = (14,9))\n\ng = sns.boxplot(x='publish_hour', y='views_log', data=df_yout, palette=\"Set3\")\ng.set_xticklabels(g.get_xticklabels())\ng.set_title(\"Views Distribuition by Publish time\", fontsize=20)\ng.set_xlabel(\"Publish time(hour)\", fontsize=15)\ng.set_ylabel(\"Views(log)\", fontsize=15)\n\nplt.show()","4f48356a":"df_yout['description'].unique()","85eda77f":"df_yout['title_len']=df_yout['title'].apply(lambda x : len(x))\ndf_yout['description_len']=df_yout['description'].apply(lambda  x: len(str(x)))\ndf_yout['tags_number']=df_yout['tags'].apply(lambda x : len(x.split(\"|\")))","a0b75819":"# df_yout[['title_len','description_len','tags_number']].nunique()\nplt.figure(figsize = (12,10))\nplt.title('Length of Title vs. Number of Views', size= 30)\nplt.xlabel('length of tiltle')\nplt.ylabel('number of views(log)')\nplt.scatter(df_yout['title_len'], df_yout['views_log'], marker='o', alpha = .3)\nplt.show()","e38900ad":"plt.figure(figsize = (12,10))\nplt.title('Length of Description vs. Number of Views', size= 30)\nplt.xlabel('length of description')\nplt.ylabel('number of views(log)')\nplt.scatter(df_yout['description_len'], df_yout['views_log'], marker='o', alpha = .3)\nplt.show()","864d0233":"plt.figure(figsize = (12,10))\nplt.title('Number of Tags vs. Number of Views', size= 30)\nplt.xlabel('number of tags')\nplt.ylabel('number of views(log)')\nplt.scatter(df_yout['tags_number'], df_yout['views_log'], marker='o', alpha = .3)\nplt.show()","271fb3f6":"#  df_yout[['views_log','likes_log','dislikes_log','comment_log','trending_times','publish_hour','weekday','title_len','category_id','tags_number','description_len']]","48e22019":"pearsoncorr = df_yout[['views_log','likes_log','dislikes_log','comment_log','trending_times','publish_hour','weekday','title_len','category_id','tags_number','description_len']].corr(method='pearson')","277adab8":"# pearsoncorr = df_yout[['views','likes','dislikes','comment_count','trending_times']].corr(method='pearson')\n# pearsoncorr","3501eaf1":"plt.figure(figsize = (18,15))\nplt.title('Pearson Correlation of Features', size= 30)\nsns.set(font_scale=1.8) \nsns.heatmap(pearsoncorr,linewidths=0.1,vmax = 1, square = True,cmap = plt.cm.RdBu, linecolor='white',annot_kws={'size':15}, annot=True,)\nplt.show()\n","31f9f294":"from sklearn import feature_extraction, linear_model, model_selection, preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\ndata_X = df_yout[['publish_hour','weekday','category_id']]\ndata_y = df_yout['trending_times']\n# data_X = np.array(data_X)\n# data_X = data_X.reshape(-1,1)\nfeat_enc = OneHotEncoder()\nfeat_enc.fit(data_X)\ndata_X = feat_enc.transform(data_X)\ndata_X = pd.DataFrame(data_X.todense())\n\ntags = df_yout['tags']\nn = len(tags)\ncount_vectorizer = feature_extraction.text.CountVectorizer(stop_words=\"english\",analyzer='word', max_df=1.0, min_df=50\/n+0.0001, max_features=None)\ntags_vectors = count_vectorizer.fit_transform(tags)\ntags_vectors = pd.DataFrame(tags_vectors.todense())\n# test_vectors = count_vectorizer.transform(X_test)\n# print(len(count_vectorizer.get_feature_names()))\n# print(count_vectorizer.get_feature_names())\n\n# tags_vectors = np.array(tags_vectors)\n# tags_vectors.todense()\ndata_X = pd.concat([data_X, tags_vectors],axis = 1, sort=False)\ndescription_len =  df_yout['description_len'].reset_index(drop=True) \ndescription_len\ndata_X = pd.concat([data_X, description_len],axis = 1, sort=False)\ndata_X\n\nX_train, X_test, y_train, y_test = train_test_split(data_X, data_y, train_size=0.9)\n\nX_train","74b18ca0":"np.arange(1,len(df_yout['description_len']))","36efa118":"# tags = X_train['tags']\n# tags_test = X_test['tags']","92b22b36":"from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n# count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', ngram_range=(1, 1), max_df=0.50, min_df=1, max_features=None)\n\n#2 times\ncount_vectorizer = feature_extraction.text.CountVectorizer(stop_words=\"english\",analyzer='word', max_df=1.0, min_df=1\/n+0.0001, max_features=None)\ntrain_vectors = count_vectorizer.fit_transform(tags)\ntest_vectors = count_vectorizer.transform(tags)\nprint(len(count_vectorizer.get_feature_names()))\nprint(count_vectorizer.get_feature_names())\n# train_vectors = train_vectors.toarray()\n# test_vectors = test_vectors.toarray()","17bb69a5":"#10 times\ncount_vectorizer = feature_extraction.text.CountVectorizer(stop_words=\"english\",analyzer='word', max_df=1.0, min_df=10\/n+0.0001, max_features=None)\ntrain_vectors = count_vectorizer.fit_transform(tags)\ntest_vectors = count_vectorizer.transform(tags)\nprint(len(count_vectorizer.get_feature_names()))\nprint(count_vectorizer.get_feature_names())","d3dc3ff2":"#50 times\ncount_vectorizer = feature_extraction.text.CountVectorizer(stop_words=\"english\",analyzer='word', max_df=1.0, min_df=50\/n+0.0001, max_features=None)\ntrain_vectors = count_vectorizer.fit_transform(tags)\ntest_vectors = count_vectorizer.transform(tags)\nprint(len(count_vectorizer.get_feature_names()))\nprint(count_vectorizer.get_feature_names())","3eeeaa94":"#100 times\ncount_vectorizer = feature_extraction.text.CountVectorizer(stop_words=\"english\",analyzer='word', max_df=1.0, min_df=100\/n+0.0001, max_features=None)\ntrain_vectors = count_vectorizer.fit_transform(tags)\ntest_vectors = count_vectorizer.transform(tags)\nprint(len(count_vectorizer.get_feature_names()))\nprint(count_vectorizer.get_feature_names())","ad38a2cc":"#150 times\ncount_vectorizer = feature_extraction.text.CountVectorizer(stop_words=\"english\",analyzer='word', max_df=1.0, min_df=150\/n+0.0001, max_features=None)\ntrain_vectors = count_vectorizer.fit_transform(tags)\ntest_vectors = count_vectorizer.transform(tags)\nprint(len(count_vectorizer.get_feature_names()))\nprint(count_vectorizer.get_feature_names())","60f09964":"#200 times\ncount_vectorizer = feature_extraction.text.CountVectorizer(stop_words=\"english\",analyzer='word', max_df=1.0, min_df=200\/n+0.0001, max_features=None)\ntrain_vectors = count_vectorizer.fit_transform(tags)\ntest_vectors = count_vectorizer.transform(tags)\nprint(len(count_vectorizer.get_feature_names()))\nprint(count_vectorizer.get_feature_names())","cc25fa7e":"#500 times\ncount_vectorizer = feature_extraction.text.CountVectorizer(stop_words=\"english\",analyzer='word', max_df=1.0, min_df=500\/n+0.0001, max_features=None)\ntrain_vectors = count_vectorizer.fit_transform(tags)\ntest_vectors = count_vectorizer.transform(tags)\nprint(len(count_vectorizer.get_feature_names()))\nprint(count_vectorizer.get_feature_names())","165f69bc":"# X_train = X_train.to_numpy()\n# X_train = X_train.reshape(-1,1)\n# type(X_train)","157d244a":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nreg = LinearRegression()\n# cross_val_score(reg, train_vectors, y_train, cv=3)\nprint(cross_val_score(reg, X_train, y_train, cv=3))\n# result = reg.predict(test_vectors)\n# print(mean_squared_error(result,y_test)**0.5\/np.mean(y_test))\n# print(reg.score(test_vectors, y_test))","0948b32e":"from sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nreg = MLPRegressor()\n# cross_val_score(reg, train_vectors, y_train, cv=3)\nprint(cross_val_score(reg, X_train, y_train, cv=3))","27f9fc80":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\n\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=6)\n\nregr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=6),\n                          n_estimators=50)\n\nregr_1.fit(X_train, y_train)\nregr_2.fit(X_train, y_train)\n\n# Predict\n# y_1 = regr_1.predict(X)\n# y_2 = regr_2.predict(X)\nprint(regr_1.score(X_test, y_test))\nprint(regr_2.score(X_test, y_test))\nprint(cross_val_score(regr_1, X_train, y_train, cv=3))","574518da":"#150 times\ncount_vectorizer = feature_extraction.text.CountVectorizer(stop_words=\"english\",analyzer='word', max_df=1.0, min_df=150\/n+0.0001, max_features=None)\ntrain_vectors = count_vectorizer.fit_transform(tags)\ntest_vectors = count_vectorizer.transform(tags)\nprint(len(count_vectorizer.get_feature_names()))\nprint(count_vectorizer.get_feature_names())","cc53c162":"reg = LinearRegression()\nreg.fit(train_vectors, data_y)\nd = {'coef': reg.coef_, 'tag':count_vectorizer.get_feature_names()}\ndf = pd.DataFrame(data=d)\ndf.sort_values(by=['coef'])","0aa484f3":"# df_youtube = pd.read_csv(\"..\/input\/youtube-new\/USvideos.csv\")\n# print(df_youtube.shape)\n# print(df_youtube.nunique())","6eb7c157":"# df_youtube = df_youtube.drop_duplicates(subset=['video_id','trending_date'], keep='last', inplace=False)\n# print(df_youtube.shape)\n# print(df_youtube.nunique())","58178d99":"# df_youtube['trending_times'] = np.nan\n# n=0\n# for v_id in df_youtube['video_id'].unique():\n#     trending_times = sum(df_youtube['video_id'] == v_id)\n#     n+=1\n#     print(n)\n#     df_youtube.loc[(df_youtube[\"video_id\"] == v_id),\"trending_times\"] = trending_times\n#     df_youtube.loc[df_youtube['video_id'] == v_id][\"trending_times\"]=trending_times\n# df_youtube.loc[df_youtube['video_id'] == 'ooyjaVdt-jA']","db5c9f03":"# df_youtube = df_youtube.drop_duplicates(subset='video_id', keep='last', inplace=False)","b7699250":"# print(df_youtube.shape)\n# print(df_youtube.nunique())","bc35f023":"<H1>pearson correlation","972bb784":"<H1>Experiments","cef5fa99":"<H1>number of words","9b67f135":"<H1>Vectorlize","7d193656":"<H1>time","2b00e1f8":"<H1>grop_duplicates","617770eb":"<H1>category","5264220b":"<H1>date"}}