{"cell_type":{"97a0ff28":"code","f45f102b":"code","c3109aae":"code","855588c3":"code","33f99f37":"code","82d83bb5":"code","0d061d92":"markdown","3925bbb6":"markdown","809f2af5":"markdown","ee150f0d":"markdown","1739224b":"markdown","6c9eee80":"markdown","19e12669":"markdown","3c631c20":"markdown","92bddb33":"markdown"},"source":{"97a0ff28":"['answered_correctly_u_avg',\n 'answered_correctly_u_count',\n 'answered_correctly_uq_count',\n 'elapsed_time_u_avg_xiuzheng',\n 'explanation_u_avg_xiuzheng',\n 'part_correctly_q_mean',###\u7ebf\u4e0b\n 'part_elapsed_time_mean',###\u7ebf\u4e0b\n 'part_had_explanation_mean',###\u7ebf\u4e0b\n 'part_user_count',\n 'part_user_mean',\n 'prior_question_elapsed_time',###\u539f\u59cb\n 'prior_question_had_explanation',###\u539f\u59cb\n 'question_correct_rate_last_20_mean',\n 'question_correctly_q_count',###\u7ebf\u4e0b\n 'question_correctly_q_mean',###\u7ebf\u4e0b\n 'question_elapsed_time_mean',###\u7ebf\u4e0b\n 'question_had_explanation_mean',###\u7ebf\u4e0b\n 'tag_acc_count',###\u7ebf\u4e0b\n 'tag_acc_max',###\u7ebf\u4e0b\n 'tag_acc_min',###\u7ebf\u4e0b\n 'tags_lsi',###\u7ebf\u4e0b\n 'task_container_id',###\u539f\u59cb\n 'timestamp',###\u539f\u59cb\n 'timestamp_u_correct_recency_1',\n 'timestamp_u_diff_1_2',\n 'timestamp_u_diff_2_3',\n 'timestamp_u_diff_3_end',\n 'timestamp_u_incorrect_recency_1',\n 'user_tag_acc_count',\n 'user_tag_acc_max',\n 'user_tag_acc_min']","f45f102b":"### question_elapsed_time_mean and question_had_explanation_mean\ntemp=all_data.groupby(\"question_id\")[[\"current_question_elapsed_time\",'current_question_had_explanation']].mean()\ntemp.columns=['question_elapsed_time_mean','question_had_explanation_mean']\nque_fea_group_1=temp\ndel temp\ngc.collect()\n\n###  question_correctly_q_count and question_correctly_q_mean\ntemp=all_data.groupby(\"question_id\")['answered_correctly'].agg([(\"question_correctly_q_count\",\"count\"),(\"question_correctly_q_mean\",\"mean\")])\nque_fea_group_2=temp\ndel temp\ngc.collect()\n\n### part_elapsed_time_mean\u3001part_had_explanation_mean and part_correctly_q_mean\ntemp=all_data.groupby(\"part\")[[\"current_question_elapsed_time\",'current_question_had_explanation','answered_correctly']].mean()\ntemp.columns=['part_elapsed_time_mean','part_had_explanation_mean','part_correctly_q_mean']\npart_fea_group=temp\ndel temp\ngc.collect()\n\n","c3109aae":"### TSVD decomposition\nfrom time import time \nimport pandas as pd\nimport warnings\nfrom time import time \nimport warnings\nfrom gensim import corpora,similarities,models\nimport pandas as pd\n\nwarnings.filterwarnings(action='ignore',category=UserWarning,module='gensim')\ntime1=time()\nfrom gensim import corpora,similarities,models\n# test_data_all=pd.read_pickle(\"query_all.pickle\")\n\nall_data['tags'].fillna(\"-1\",inplace=True)\n\nclass mycorpus(object):\n    def __iter__(self):\n        for index,doc in enumerate(all_data['tags']):\n            if index%5000000==0:\n                print(index)\n            yield doc.split(' ')\ncorp = mycorpus()\n# dictionary.add_documents(corp)\ndictionary=corpora.Dictionary(corp)\ntime2=time()\nprint(time2-time1)\n\n\n\n# test_data_all=pd.read_pickle(\"\/home\/kesci\/work\/query_all.pickle\")\n\nprint(111523155656)\nclass MyCorpus(object):\n    def __init__(self):\n        print('MyCorpus')\n    def __iter__(self):\n        for index,doc in enumerate(all_data['tags']):\n            if index%5000000==0:\n                print(index)\n            yield dictionary.doc2bow(doc.split(' '))\ncorpus = MyCorpus()\n# corpus = [dictionary.doc2bow(text) for text in self.text]\ntfidf_model = models.TfidfModel(corpus, id2word=dictionary)\n# corpus_tfidf = tfidf_model[corpus]\n\ntfidf_model.save(\"model\/tfidf.model\")\ndictionary.save(\"model\/dictionary.model\")\n\n\nfrom time import time \nimport warnings\ntime1=time()\nwarnings.filterwarnings(action='ignore',category=UserWarning,module='gensim')\nfrom gensim import corpora,similarities,models\nimport pandas as pd\n# test_data_all=pd.read_pickle(\"\/home\/kesci\/work\/query_all.pickle\")\ncorpus_tfidf = tfidf_model[corpus]\nprint(111523155656)\nclass MyCorpus(object):\n    def __init__(self):\n        print('MyCorpus')\n    def __iter__(self):\n        for index,doc in enumerate(corpus_tfidf):\n            if index%1000000==0:\n                print(index)\n            yield doc\ncorpus3 = MyCorpus()\n# corpus = [dictionary.doc2bow(text) for text in self.text]\nlsi_model = models.LsiModel(corpus3, id2word=dictionary,chunksize=2500000,num_topics=50)\nlsi_model.save(\"lsi_model\/lsi_all_query.lsi\")\ntime2=time()\nprint((time2-time1)\/60)\n# corpus_tfidf = tfidf_model[corpus]\n\ndef get_arg_max(single_list):\n    max_index=0\n    max_num=single_list[0][1]\n    for index in range(len(single_list)-1):\n        if max_num<single_list[index+1][1]:\n            max_num=single_list[index+1][1]\n            max_index=index+1\n    return max_index\nlsi_model=models.LsiModel.load(\"model\/lsi_model\/lsi_all_query_20.lsi\")\ndictionary=corpora.Dictionary.load(\"model\/dictionary.model\")\ntfidf_model=corpora.Dictionary.load(\"model\/tfidf.model\")\nall_data_lsi=[]\nfor text in tqdm(question['tags']):\n    single_row_text=dictionary.doc2bow(text.split(' '))\n    single_row_lsi_list=list(lsi_model[tfidf_model[single_row_text]])\n    if len(single_row_lsi_list)==0:\n        all_data_lsi.append(0)\n        continue\n    single_row_lsi=get_arg_max(single_row_lsi_list)\n    all_data_lsi.append(single_row_lsi)\nquestion['tags_lsi']=all_data_lsi\nquestion.to_csv(\"temp\/question_lsi20.csv\",index=None)\nquestion.head(50)","855588c3":"#  'answered_correctly_u_avg',\n#  'answered_correctly_u_count',\n#  'answered_correctly_uq_count',\n\n#  'elapsed_time_u_avg_xiuzheng',\n#  'explanation_u_avg_xiuzheng',\n\n#  'part_user_count',\n#  'part_user_mean',\n\n#  'question_correct_rate_last_20_mean',\n\n#  'timestamp_u_correct_recency_1',\n#  'timestamp_u_incorrect_recency_1',\n\n#  'timestamp_u_diff_1_2',\n#  'timestamp_u_diff_2_3',\n#  'timestamp_u_diff_3_end',\n\n#  'user_tag_acc_count',\n#  'user_tag_acc_max',\n#  'user_tag_acc_min'\nanswered_correctly_u_count_dict = defaultdict(int)\nanswered_correctly_u_sum_dict = defaultdict(int)\nanswered_correctly_uq_dict = defaultdict(lambda: defaultdict(int))\n\nelapsed_time_u_sum_dict = defaultdict(int)\nexplanation_u_sum_dict = defaultdict(int)\nquestion_u_count_dict = defaultdict(int)\nquestion_u_last_bundle_count_dict = defaultdict(int)\n\npart_user_count_dict = defaultdict(lambda: defaultdict(int))\npart_user_sum_dict = defaultdict(lambda: defaultdict(int))\n\nquestion_correct_last_20_count_dict = defaultdict(int)\nquestion_correct_last_20_sum_dict = defaultdict(int)\nquestion_correct_last_20_all_dict = defaultdict(list)\n\ntimestamp_u_correct_dict = defaultdict(list)\ntimestamp_u_incorrect_dict = defaultdict(list)\n\ntimestamp_u_dict = defaultdict(list)\n\nuser_tag_acc_count_dict = defaultdict(lambda: defaultdict(int))\nuser_tag_acc_sum_dict = defaultdict(lambda: defaultdict(int))\n# -----------------------------------------------------------------------\ndef add_features(update=True):\n    # Client features\n    answered_correctly_u_avg = np.zeros(len(all_data), dtype = np.float32)\n    answered_correctly_u_count = np.zeros(len(all_data), dtype = np.float32)\n    answered_correctly_uq_count = np.zeros(len(all_data), dtype = np.int32)\n\n    elapsed_time_u_avg = np.zeros(len(all_data), dtype = np.float32)\n    explanation_u_avg = np.zeros(len(all_data), dtype = np.float32)\n    \n    part_user_count = np.zeros(len(all_data), dtype = np.float32)\n    part_user_mean = np.zeros(len(all_data), dtype = np.float32)\n    \n    question_correct_rate_last_20_sum = np.zeros(len(all_data), dtype = np.float32)\n    \n    timestamp_u_correct_recency_1 = np.zeros(len(all_data), dtype = np.float32)\n    timestamp_u_incorrect_recency_1 = np.zeros(len(all_data), dtype = np.float32)\n    \n    timestamp_u_diff_1 = np.zeros(len(all_data), dtype = np.float32)\n    timestamp_u_diff_2 = np.zeros(len(all_data), dtype = np.float32)\n    timestamp_u_diff_3 = np.zeros(len(all_data), dtype = np.float32)\n    \n    user_tag_acc_count = np.zeros(len(all_data), dtype = np.float32)\n    user_tag_acc_max = np.zeros(len(all_data), dtype = np.float32)\n    user_tag_acc_min = np.zeros(len(all_data), dtype = np.float32)\n    \n    list_last_user_task_table=[]####\u5b9a\u4e49\u6570\u7ec4 \u7528\u6765\u4fdd\u5b58\u65e7\u7ec4\u7684\u4fe1\u606f\n    list_last_user_task_table_un_back=[]####\u5b9a\u4e49\u6570\u7ec4 \u7528\u6765\u4fdd\u5b58\u65e7\u7ec4\u7684\u4fe1\u606f\n#     for num, row in enumerate(tqdm(all_data[['user_id', 'answered_correctly', 'content_id', 'prior_question_elapsed_time', 'prior_question_had_explanation', 'timestamp','task_container_id']].values)):\n    flag_current_task=0\n    all_data_temp=all_data[['user_id',\"task_container_id\", 'content_id', 'answered_correctly', 'prior_question_elapsed_time', 'prior_question_had_explanation', 'timestamp','part',\"tags\"]].values\n    for num in tqdm(range(len(all_data))):\n        row=all_data_temp[num]\n        if num+1!=len(all_data):\n            row2=all_data_temp[num+1]\n        else:\n            row2=[-100 for i in range(len(row))]\n        \n        \n        ####*********  elapsed_time_u_avg_xiuzheng\u548cexplanation_u_avg_xiuzheng\n        if row[6]!=0:##\u5982\u679c\u65f6\u95f4\u6233\u4e0d\u662f0\u7684\u65f6\u5019\n            if flag_current_task==0:\n                question_u_count_dict[row[0]]+=question_u_last_bundle_count_dict[row[0]]\n                elapsed_time_u_sum_dict[row[0]]+=row[4]*question_u_last_bundle_count_dict[row[0]]\n                explanation_u_sum_dict[row[0]]+=row[5]*question_u_last_bundle_count_dict[row[0]]\n            elapsed_time_u_avg[num]= elapsed_time_u_sum_dict[row[0]]\/question_u_count_dict[row[0]]\n            explanation_u_avg[num] = explanation_u_sum_dict[row[0]]\/question_u_count_dict[row[0]]\n            ###\u2465\u53ea\u9700\u8981\u5f53\u524d\u7ec4\u7684prior\uff08\u4e5f\u5c31\u662f\u4e0a\u4e00\u7ec4\u7684\u5e73\u5747\u65f6\u95f4\u6216\u8005\u662f\u5426\u89e3\u7b54\uff09\uff0c\u5c31\u53ef\u4ee5\u8ba1\u7b97\u4e86\n        else:##\u65f6\u95f4\u6233\u4e3a0\u7684\u65f6\u5019\uff0c\u80af\u5b9a\u662f\u4e0d\u77e5\u9053\u5f53\u524d\u7ec4\u7684\u7528\u65f6\u548c\u89e3\u7b54\u60c5\u51b5\u7684\n            elapsed_time_u_avg[num]=np.nan\n            explanation_u_avg[num] = np.nan\n        flag_current_task=1\n        \n        ###\u2460\u6c42\u8fd9\u4e2a\u7279\u5f81\uff0c\u9700\u8981\u4e0d\u65ad\u7684\u8bb0\u5f55\u4e0a\u4e00\u7ec4\u4e00\u5171\u6709\u591a\u5c11\u9053\u9898\uff0c\u5230\u6700\u540e\u7528    \uff08\u4e0d\u65ad\u7d2f\u52a0\uff08\u6bcf\u7ec4\u591a\u5c11\u9053\u9898*\u6bcf\u9053\u9898\u5e73\u5747\u65f6\u95f4\uff09\uff09\/\u603b\u505a\u9898\u6b21\u6570\n        ###\u2461\u9700\u8981\u628a\u8bb0\u5f55\u8fd9\u7ec4\u6709\u591a\u5c11\u9053\u9898\u653e\u5728\u540e\u9762\u8ba1\u7b97\uff0c\u5728\u524d\u9762\u8ba1\u7b97\u5e73\u5747\u65f6\u95f4\u5e76\u4e14\u586b\u5145\u5230\u7279\u5f81\u6570\u7ec4\u91cc\n        list_last_user_task_table_un_back.append([row[0]])###\u6ca1\u6362\u4eba\u6362\u7ec4\u7684\u65f6\u5019\uff0c\u5148\u4e0d\u65ad\u4fdd\u5b58\u65e7\u7ec4\u7684\u4fe1\u606f,\u5e76\u4e14\u5728\u6362\u4eba\u6362\u7ec4\u7684\u65f6\u5019\u4e5f\u8981\u4fdd\u5b58\uff0c\u4ee5\u9632\u90a3\u6b21\u4fe1\u606f\u6ca1\u88ab\u7528\u5230\n        if row[0]!=row2[0] or row[1]!=row2[1]:###\u6362\u4e86\u4e00\u4e2atask\n            flag_current_task=0\n            question_u_last_bundle_count_dict[row[0]]=len(list_last_user_task_table_un_back)\n            list_last_user_task_table_un_back=[]###\u5728\u5373\u5c06\u6362task\u7684\u65f6\u5019\uff0c\u628a\u65e7\u7ec4\u9700\u8981\u6362\u6210\u65b0\u7ec4\uff08\u66f4\u6362\u6210\u65b0\u7ec4\u4e4b\u524d\uff0c\u9700\u8981\u5148\u628a\u65e7\u7ec4\u7684\u4fe1\u606f\u5728\u4e0a\u9762\u7528\u5b8c\uff09\n            \n        ####*********\n        \n        ####*********   answered_correctly_u_avg\u3001answered_correctly_u_count\u548canswered_correctly_uq_count\n        if answered_correctly_u_count_dict[row[0]] != 0:\n            answered_correctly_u_avg[num] = answered_correctly_u_sum_dict[row[0]] \/ answered_correctly_u_count_dict[row[0]]\n            answered_correctly_u_count[num] = answered_correctly_u_count_dict[row[0]]\n        else:\n            answered_correctly_u_avg[num] = 0.67\n            answered_correctly_u_count[num] = 0\n\n        answered_correctly_uq_count[num] = answered_correctly_uq_dict[row[0]][row[2]]\n        ####*********\n        \n        ####*********   part_user_count\u548cpart_user_mean\n        if part_user_count_dict[row[0]][row[7]]==0:\n            part_user_count[num] = 0\n            part_user_mean[num] = 0.67\n        else:\n            part_user_count[num] = part_user_count_dict[row[0]][row[7]]\n            part_user_mean[num] = part_user_sum_dict[row[0]][row[7]]\/part_user_count_dict[row[0]][row[7]]\n        ####*********\n        \n        ####*********   question_correct_rate_last_20_mean\n#         question_correct_rate_last_20_sum[num]=question_correct_last_20_sum_dict[row[0]]\n        ####*********\n        \n        \n        ####*********   timestamp_u_correct_recency_1\uff0ctimestamp_u_incorrect_recency_1\n        if len(timestamp_u_correct_dict[row[0]]) == 0:\n            timestamp_u_correct_recency_1[num] = np.nan\n        elif len(timestamp_u_correct_dict[row[0]]) == 1:\n            timestamp_u_correct_recency_1[num] = row[6] - timestamp_u_correct_dict[row[0]][0]\n            \n        if len(timestamp_u_incorrect_dict[row[0]]) == 0:\n            timestamp_u_incorrect_recency_1[num] = np.nan\n        elif len(timestamp_u_incorrect_dict[row[0]]) == 1:\n            timestamp_u_incorrect_recency_1[num] = row[6] - timestamp_u_incorrect_dict[row[0]][0]\n        ####*********\n\n        ####*********   timestamp_u_diff_1_2\uff0ctimestamp_u_diff_2_3\uff0ctimestamp_u_diff_3_end\n        if len(timestamp_u_dict[row[0]]) == 0:\n            timestamp_u_diff_1[num] = np.nan\n            timestamp_u_diff_2[num] = np.nan\n            timestamp_u_diff_3[num] = np.nan\n        elif len(timestamp_u_dict[row[0]]) == 1:\n            timestamp_u_diff_1[num] = row[6] - timestamp_u_dict[row[0]][0]\n            timestamp_u_diff_2[num] = np.nan\n            timestamp_u_diff_3[num] = np.nan\n        elif len(timestamp_u_dict[row[0]]) == 2:\n            timestamp_u_diff_1[num] = row[6] - timestamp_u_dict[row[0]][1]\n            timestamp_u_diff_2[num] = timestamp_u_dict[row[0]][1] - timestamp_u_dict[row[0]][0]\n            timestamp_u_diff_3[num] = np.nan\n        elif len(timestamp_u_dict[row[0]]) == 3:\n            timestamp_u_diff_1[num] = row[6] - timestamp_u_dict[row[0]][2]\n            timestamp_u_diff_2[num] = timestamp_u_dict[row[0]][2] - timestamp_u_dict[row[0]][1]\n            timestamp_u_diff_3[num] = timestamp_u_dict[row[0]][1] - timestamp_u_dict[row[0]][0]\n\n        ####*********\n        \n        ####*********   user_tag_acc_count\uff0cuser_tag_acc_max\uff0cuser_tag_acc_min\n        if pd.isnull(row[8]):\n            user_tag_acc_count[num]=np.nan\n            user_tag_acc_max[num] = np.nan\n            user_tag_acc_min[num] = np.nan\n            continue\n        else:\n            tag_list_un_back=row[8].split()\n            row_all_tag_sum=0\n            row_all_tag_count=0\n            row_max_tag_mean=-1###\u5c3d\u91cf\u641e\u5c0f\n            row_min_tag_mean=1000###\u5c3d\u91cf\u641e\u5927\n\n            for single_tag in tag_list_un_back:\n                ###\u5148\u505a\u9700\u8981\u66f4\u65b0\u7684###\n                single_tag_sum=user_tag_acc_sum_dict[row[0]][single_tag]\n                single_tag_count=user_tag_acc_count_dict[row[0]][single_tag]\n                row_all_tag_sum+=single_tag_sum\n                row_all_tag_count+=single_tag_count\n                if single_tag_count==0:\n                    single_tag_mean=0.67\n                else:\n                    single_tag_mean=single_tag_sum\/single_tag_count\n                row_max_tag_mean=max(single_tag_mean,row_max_tag_mean)\n                row_min_tag_mean=min(single_tag_mean,row_min_tag_mean)\n            if row_all_tag_count==0:\n                user_tag_acc_count[num]=0\n                user_tag_acc_max[num] = 0.67\n                user_tag_acc_min[num] = 0.67\n            else:\n                user_tag_acc_count[num]=row_all_tag_count\n                user_tag_acc_max[num] = row_max_tag_mean\n                user_tag_acc_min[num] = row_min_tag_mean\n        ####*********\n        \n\n        \n\n\n        if update:\n            answered_correctly_u_count_dict[row[0]] += 1\n            answered_correctly_u_sum_dict[row[0]] += row[3]\n            answered_correctly_uq_dict[row[0]][row[2]] += 1\n            part_user_count_dict[row[0]][row[7]] += 1\n            part_user_sum_dict[row[0]][row[7]] += row[3]\n#             if question_correct_last_20_count_dict[row[0]]+1<=20:\n#                 question_correct_last_20_count_dict[row[0]]+=1\n#                 question_correct_last_20_sum_dict[row[0]]+=row[3]\n#                 question_correct_last_20_all_dict[row[0]].append(row[3])\n#             else:\n#                 question_correct_last_20_sum_dict[row[0]]+=row[3]\n#                 question_correct_last_20_sum_dict[row[0]]-=question_correct_last_20_all_dict[row[0]][-1]\n#                 question_correct_last_20_all_dict[row[0]].pop(0)\n#                 question_correct_last_20_all_dict[row[0]].append(row[3])\n            \n            tag_list=row[8].split()\n            for single_tag in tag_list:\n                ######\u66f4\u65b0\u4e00\u4e0b user-tag\n                user_tag_acc_count_dict[row[0]][single_tag] += 1\n                user_tag_acc_sum_dict[row[0]][single_tag] += row[3]\n            \n            #'user_id',\"task_container_id\", 'content_id', 'answered_correctly', 'prior_question_elapsed_time', 'prior_question_had_explanation', 'timestamp','part'\n            list_last_user_task_table.append([row[0],row[1],row[2],row[3],row[4],row[5],row[6],row[7]])###\u6ca1\u6362\u4eba\u6362\u7ec4\u7684\u65f6\u5019\uff0c\u5148\u4e0d\u65ad\u4fdd\u5b58\u65e7\u7ec4\u7684\u4fe1\u606f,\u5e76\u4e14\u5728\u6362\u4eba\u6362\u7ec4\u7684\u65f6\u5019\u4e5f\u8981\u4fdd\u5b58\uff0c\u4ee5\u9632\u90a3\u6b21\u4fe1\u606f\u6ca1\u88ab\u7528\u5230\n            if row[0]!=row2[0] or row[1]!=row2[1]:###\u6362\u4e86\u4e00\u4e2atask\n                \n                if len(timestamp_u_dict[row[0]]) == 3:\n                    timestamp_u_dict[row[0]].pop(0)\n                    timestamp_u_dict[row[0]].append(row[6])\n                else:\n                    timestamp_u_dict[row[0]].append(row[6])\n                \n                ####\u7531\u4e8ebundle\u4e0b\u9762\u5305\u542b\u5f88\u591aquestion\uff0c\u6bcf\u4e2aquestion\u90fd\u6709\u4e00\u4e2acorrect\uff0c\u6240\u4ee5\u9700\u8981\u7528\u5217\u8868\u5b58\u50a8\u201c\u65e7\u7684\u4e00\u6574\u4e2a\u7ec4\u201d\u7684correct \n                for single_row_last_user_task_table in list_last_user_task_table:\n                    if single_row_last_user_task_table[3]==1:\n                        if len(timestamp_u_correct_dict[row[0]]) == 1:###\u8fd9\u91cc\uff0c\u5c31\u4f7f\u7528row[0]\u5c31\u884c\uff0c\u56e0\u4e3alist_last_user_task_que_timestamp\u91cc\u5168\u90fd\u662f\u5f53\u524duser-task\u7684\u4fe1\u606f\uff0c\u800c\u975e\u4e0b\u4e00\u4e2auser-task\u7684\u4fe1\u606f\n                            timestamp_u_correct_dict[row[0]].pop(0)\n                            timestamp_u_correct_dict[row[0]].append(single_row_last_user_task_table[6])\n                        else:\n                            timestamp_u_correct_dict[row[0]].append(single_row_last_user_task_table[6])\n                    else:\n                        if len(timestamp_u_incorrect_dict[row[0]]) == 1:###\u8fd9\u91cc\uff0c\u5c31\u4f7f\u7528row[0]\u5c31\u884c\uff0c\u56e0\u4e3alist_last_user_task_que_timestamp\u91cc\u5168\u90fd\u662f\u5f53\u524duser-task\u7684\u4fe1\u606f\uff0c\u800c\u975e\u4e0b\u4e00\u4e2auser-task\u7684\u4fe1\u606f\n                            timestamp_u_incorrect_dict[row[0]].pop(0)\n                            timestamp_u_incorrect_dict[row[0]].append(single_row_last_user_task_table[6])\n                        else:\n                            timestamp_u_incorrect_dict[row[0]].append(single_row_last_user_task_table[6])\n                list_last_user_task_table=[]###\u5728\u5373\u5c06\u6362task\u7684\u65f6\u5019\uff0c\u628a\u65e7\u7ec4\u9700\u8981\u6362\u6210\u65b0\u7ec4\uff08\u66f4\u6362\u6210\u65b0\u7ec4\u4e4b\u524d\uff0c\u9700\u8981\u5148\u628a\u65e7\u7ec4\u7684\u4fe1\u606f\u5728\u4e0a\u9762\u7528\u5b8c\uff09\n\n#     all_data['answered_correctly_u_avg']=answered_correctly_u_avg\n#     all_data['answered_correctly_u_count']=answered_correctly_u_count\n#     all_data['answered_correctly_uq_count']=answered_correctly_uq_count\n#     all_data['elapsed_time_u_avg_xiuzheng']=elapsed_time_u_avg\n#     all_data['explanation_u_avg_xiuzheng']=explanation_u_avg\n#     all_data['part_user_count']=part_user_count\n#     all_data['part_user_mean']=part_user_mean\n#     all_data['timestamp_u_correct_recency_1']=timestamp_u_correct_recency_1\n#     all_data['timestamp_u_incorrect_recency_1']=timestamp_u_incorrect_recency_1\n#     all_data['timestamp_u_diff_1_2']=timestamp_u_diff_1\n#     all_data['timestamp_u_diff_2_3']=timestamp_u_diff_2\n#     all_data['timestamp_u_diff_3_end']=timestamp_u_diff_3\n#     all_data['part_user_count']=part_user_count\n#     all_data['part_user_mean']=part_user_mean\n#     all_data['user_tag_acc_count']=user_tag_acc_count\n#     all_data['user_tag_acc_max']=user_tag_acc_max\n#     all_data['user_tag_acc_min']=user_tag_acc_min\n    \n       \nadd_features()\n\n","33f99f37":"model = lgb.LGBMClassifier(num_leaves=300,\n                        max_depth=15,\n                        learning_rate=0.1,\n                        subsample=0.8,\n                        feature_fraction=0.8,\n                        random_state=2020,\n                        n_estimators=200\n                        )\nlgb_model = model.fit(train_X[fea_list], \n                    train_Y,\n                    eval_names=['train', 'valid'],\n                    eval_set=[(train_X[fea_list], train_Y), (valid_X[fea_list], valid_Y)],\n                    verbose=10,\n                    eval_metric='auc',\n                    early_stopping_rounds=10,\n                     categorical_feature=['tags_lsi'])###                    categorical_feature=cate_feat\n","82d83bb5":"feature_importance = lgb_model.feature_importances_\nfeature_importance = pd.DataFrame({'Features': fea_list, 'Importance': feature_importance}).sort_values('Importance', ascending = False)\n\nfig = plt.figure(figsize = (10, 10))\nfig.suptitle('Feature Importance', fontsize = 20)\nplt.tick_params(axis = 'x', labelsize = 12)\nplt.tick_params(axis = 'y', labelsize = 12)\nplt.xlabel('Importance', fontsize = 15)\nplt.ylabel('Features', fontsize = 15)\nsns.barplot(x = feature_importance['Importance'], y = feature_importance['Features'], orient = 'h')\nplt.show()\n","0d061d92":"# introduction\n\nval:0.788 lb:No time to submit\n\nThanks to ragnar for this great script https:\/\/www.kaggle.com\/ragnar123\/riiid-model-lgbm  \nBased on the notebook mentioned above, I did the following:\n- Fixed some features that could cause overfitting, such as the \"time difference between the current timestamp and the last time you made a mistake\" feature.The original authors used answered_correctly under the same bundle for the frequency features, but in the test data, the answered_correctly under the current bundle is not known.\n- Fixed some statistical features made using \"prior_question_elapsed_time\" and \"prior_question_had_explanation\".These two features mean \"when was the last bundle\" and \"Whether was the last bundle solved\", but the original authors did not align them to the previous bundle.\n- The user attributes and title attributes are described from more angles, such as \"user-que\", \"user-part\", \"user-tags\", \"part\", \"tags\" (e.g., the highest correct rate of the tag under a question)...\n- The Gensim library is used to TSVD the Tags matrix(Direct use of SKLearn'S TSVD can cause memory leaks, while the GenSIM library can stream the TSVD decomposition model training in batches), indexing the highest numeric column in each row as the topic of tags.The number of topics tried 20, 50, 75.There was no significant improvement in the number of topics from 75 to 50.The number of topics 50 is 0.0005 higher than the number of topics 20, but in order to prevent overfitting, I finally chose 20 topics.  It is worth mentioning that after describing the tags and user-tags attributes, the tags can be improved to a certain extent (0.001). On this basis, TSVD can improve the tags to a certain extent (0.0015).\n\nThe following directions can be improved:\n- Adding a description of a \"user-lecture\" can be considered in the following directions, such as how many lectures the user has seen; when the user was working on a particular lecture, the current time difference between the last time a lecture with the same tag was viewed; and how often the lecture was viewed.I think these characteristics will lead to a very big improvement\n\nUnfortunately, due to the problem of laboratory work, I only participated in the competition for less than a week, so that I didn't even have time to move to the online.This plan after my correction, I think the probability has been high will not overfit, I hope to give you some inspiration!Once again, I wish you all good results.  \n***If this notebook is helpful to you, I hope you can do drop an upvote,thank you!***","3925bbb6":"# update\n\nThe comments section has a description of the features associated with tags and some thoughts and solutions to the overfitting problem.  \nPlease translate it by yourself for the time being. I will summarize those features soon.I'm so sorry. I'm so busy that I have to use Chinese for the time being.\n\n\u8bc4\u8bba\u533a\u6709\u5173\u4e8etags\u76f8\u5173\u7279\u5f81\u7684\u8bf4\u660e\uff0c\u5e76\u4e14\u6709\u4e00\u4e9b\u5173\u4e8e\u8fc7\u62df\u5408\u95ee\u9898\u7684\u601d\u8003\u4e0e\u89e3\u51b3\u529e\u6cd5\u3002  \n\u9ebb\u70e6\u5404\u4f4d\u670b\u53cb\u5148\u81ea\u884c\u7ffb\u8bd1\u4e00\u4e0b\uff0c\u6211\u4e0d\u4e45\u540e\u4f1a\u6574\u7406\u90a3\u4e9b\u7279\u5f81\u3002\u592a\u62b1\u6b49\u4e86\uff0c\u592a\u5fd9\u4e86\u5c31\u53ea\u80fd\u6682\u65f6\u5148\u7528\u4e2d\u6587\u5566\u3002  ","809f2af5":"\u4ecb\u7ecd\uff1a\n\u5f88\u611f\u8c22\u8fd9\u4e2a\u4f5c\u8005\u7684\u5f00\u6e90\u8d21\u732e\uff1ahttps:\/\/www.kaggle.com\/ragnar123\/riiid-model-lgbm  \n\u6211\u5728\u8fd9\u4e2a\u4f5c\u8005\u7684\u5de5\u4f5c\u4e0a\u505a\u4e86\u4e00\u4e9b\u6539\u8fdb\uff0c\u5177\u4f53\u5982\u4e0b\uff1a  \n- \u4fee\u6b63\u4e86\u4e00\u4e9b\u53ef\u80fd\u4f1a\u9020\u6210\u8fc7\u62df\u5408\u7684\u7279\u5f81\uff0c\u4f8b\u5982\u201c\u5f53\u524d\u65f6\u95f4\u6233\u548c\u4e0a\u4e00\u6b21\u505a\u9519\u9898\u76ee\u7684\u65f6\u95f4\u5dee\u201d\u7279\u5f81\u3002\u539f\u4f5c\u8005\u5728\u505a\u9891\u7387\u7279\u5f81\u65f6\uff0c\u4f7f\u7528\u5230\u4e86\u540c\u4e00\u4e2abundle\u4e0b\u7684answered_correctly\uff0c\u4f46\u6d4b\u8bd5\u6570\u636e\u4e2d\uff0c\u5f53\u524dbundle\u4e0b\u7684answered_correctly\u662f\u4e0d\u77e5\u9053\u7684\u3002\n- \u4fee\u6b63\u4e86\u4f7f\u7528\u201cprior_question_elapsed_time\u201d\u548c\u201cprior_question_had_explanation\u201d\u6240\u505a\u7684\u4e00\u4e9b\u7edf\u8ba1\u7279\u5f81\u3002\u8fd9\u4e24\u4e2a\u7279\u5f81\u7684\u610f\u4e49\u662f\u201c\u4e0a\u4e00\u4e2abundle\u6240\u7528\u65f6\u95f4\u201d\u548c\u201c\u4e0a\u4e00\u4e2abundle\u662f\u5426\u88ab\u89e3\u7b54\u201d\uff0c\u4f46\u662f\u539f\u4f5c\u8005\u5e76\u6ca1\u6709\u5c06\u4ed6\u4eec\u5bf9\u9f50\u5230\u4e0a\u4e00\u4e2abundle\u3002\n- \u4ece\u66f4\u591a\u7684\u89d2\u5ea6\u6765\u5bf9\u7528\u6237\u5c5e\u6027\u548c\u6807\u9898\u5c5e\u6027\u8fdb\u884c\u63cf\u8ff0\uff0c\u5982\u201cuser-que\u201d\u3001\u201cuser-part\u201d\u3001\u201cuser-tags\u201d\u3001\u201cpart\u201d\u3001\u201ctags\u201d(\u4f8b\u5982\u5728\u4e00\u4e2a\u95ee\u9898\u4e0b\u6b63\u786e\u7387\u6700\u9ad8\u7684\u6807\u7b7e\u7684\u6b63\u786e\u7387)\u2026\u2026\n- \u4f7f\u7528gensim\u5e93\u6765\u5bf9tags\u77e9\u9635\u8fdb\u884ctsvd\u5206\u89e3(\u76f4\u63a5\u4f7f\u7528sklearn\u7684tsvd\u4f1a\u9020\u6210\u5185\u5b58\u6cc4\u6f0f\uff0c\u800cgensim\u5e93\u53ef\u4ee5\u6d41\u5f0f\u7684\u5206\u6279\u6b21\u8fdb\u884ctsvd\u5206\u89e3\u6a21\u578b\u7684\u8bad\u7ec3)\uff0c\u53d6\u6bcf\u4e00\u884c\u6570\u503c\u6700\u9ad8\u7684\u90a3\u4e00\u5217\u7684\u7d22\u5f15\u6765\u4f5c\u4e3atags\u7684\u4e3b\u9898\u3002\u4e3b\u9898\u6570\u5c1d\u8bd5\u8fc720\u300150\u300175\u3002\u4e3b\u9898\u657075\u8f83\u4e3b\u9898\u657050\u76f8\u6bd4\u6ca1\u6709\u663e\u8457\u63d0\u5347\u3002\u4e3b\u9898\u657050\u8f83\u4e3b\u9898\u657020\u67090.0005\u7684\u63d0\u5347\uff0c\u4f46\u662f\u4e3a\u4e86\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u6211\u6700\u7ec8\u9009\u62e9\u4e8620\u4e2a\u4e3b\u9898\u6570\u3002\n- \u503c\u5f97\u4e00\u63d0\u7684\u662f\uff0c\u5bf9tags\u4ee5\u53causer-tags\u7684\u5c5e\u6027\u8fdb\u884c\u63cf\u7ed8\u540e\u80fd\u5f97\u5230\u4e00\u5b9a\u63d0\u5347\uff080.001\uff09\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5bf9tags\u8fdb\u884ctsvd\u53c8\u80fd\u591f\u83b7\u5f97\u4e00\u5b9a\u7684\u63d0\u5347\uff080.0015\uff09\n\n\u672a\u6765\u7684\u4e00\u4e9b\u53ef\u4ee5\u6539\u8fdb\u7684\u65b9\u5411\uff1a\n- \u52a0\u5165\u5bf9\u201cuser-lecture\u201d\u7684\u63cf\u8ff0\uff0c\u53ef\u4ee5\u4ece\u4ee5\u4e0b\u65b9\u5411\u8fdb\u884c\u8003\u8651\uff0c\u4f8b\u5982\u7528\u6237\u770b\u8fc7\u591a\u5c11\u4e2alecture\u3001\u7528\u6237\u5728\u505a\u67d0\u4e2a\u9898\u65f6\u3001\u4e0a\u4e00\u6b21\u89c2\u770b\u5177\u6709\u76f8\u540ctag\u7684\u8bb2\u5ea7\u7684\u8ddd\u79bb\u5f53\u524d\u7684\u65f6\u95f4\u5dee\u3001\u89c2\u770b\u8bb2\u5ea7\u7684\u9891\u7387\u3002\u6211\u8ba4\u4e3a\u8fd9\u4e9b\u7279\u5f81\u4f1a\u5e26\u6765\u975e\u5e38\u5927\u7684\u63d0\u5347\n\n> ### \u4e00\u4e9b\u5410\u69fd\n\u5f88\u9057\u61be\u7684\u662f\u56e0\u4e3a\u5b9e\u9a8c\u5ba4\u5de5\u4f5c\u7684\u95ee\u9898\uff0criiid\u6211\u53ea\u73a9\u4e86\u4e0d\u5230\u4e00\u661f\u671f\uff0c\u4e00\u76f4\u662f\u5728\u7ebf\u4e0b\u505a\uff08\u56e0\u4e3a\u7ebf\u4e0a\u592a\u6162\u4e86\uff09\u3002\u9e3d\u4e86\u597d\u4e45\uff0c\u56de\u6765\u628a\u6211\u7ebf\u4e0b\u5206\u6570\u5bf9\u7167\u7ebf\u4e0a\u6392\u884c\u699c\u4e00\u770b\u5df2\u7ecf\u60e8\u4e0d\u5fcd\u7779\uff0c\u6628\u665a\u60f3\u628a\u8fd9\u4e2a\u8fc1\u5230\u7ebf\u4e0a\u53bb\u6765\u7740\uff0c\u7d22\u6027\u4e0d\u641e\u4e86\u3002   \n\u8fd9\u5957\u65b9\u6848\u7ecf\u8fc7\u6211\u7684\u4fee\u6b63\u540e\uff0c\u6211\u8ba4\u4e3a\u5df2\u7ecf\u5927\u6982\u7387\u4e0d\u4f1a\u8fc7\u62df\u5408\u4e86\uff08\u4e5f\u5c31\u662f\u8bf4\u4f30\u8ba1\u5982\u679c\u80fd\u590d\u73b0\u5b8c\uff0clb\u5e94\u8be5\u4f1a\u6bd4788\u8981\u9ad8\uff09\uff0c\u5e0c\u671b\u80fd\u7ed9\u5927\u5bb6\u5e26\u6765\u4e00\u4e9b\u7075\u611f\uff01\u518d\u6b21\u795d\u5404\u4f4d\u90fd\u80fd\u53d6\u5f97\u597d\u6210\u7ee9\u3002\uff08\u6ce8\uff1a\u840c\u65b0\u7b2c\u4e00\u6b21\u800dkaggle\u505a\u5f00\u6e90\uff0c\u5e0c\u671b\u8001\u54e5\u4eec\u4e0d\u8981\u4e0b\u6b21\u4e00\u5b9a\u5566\uff0c\u7ed9\u4e2a\u8d5e\u5427(*^\u25bd^*)\uff0c\u62dc\u6258\u4e86\u3002\u6709\u9700\u8981\u6211\u5b9e\u9a8c\u5177\u4f53\u6570\u636e\u7684\u53ef\u4ee5\u5728\u8bc4\u8bba\u533a\u7ba1\u6211\u8981\uff0c\u8fd8\u6709\u5df2\u7ecf\u505a\u597d\u7684tsvd\u5206\u89e3\u8868\uff08\u53ef\u4ee5\u76f4\u63a5merge\u5230\u6570\u636e\u96c6\u4e0a\uff09\uff0c\u8fd8\u6709\u5df2\u7ecf\u505a\u597d\u7684\u7279\u5f81\uff08\u5b58\u6210\u4e86hdf5\u4e86\uff09\u3002\u5c0f\u5f1f\u77e5\u65e0\u4e0d\u8a00~\uff09","ee150f0d":"# model\nWhen num_leaves are increased and max_depth is controlled within a certain range, the training speed and the result accuracy can be improved under large data volume (it has been verified on this data set, the effect is improved by about 0.001, and the training time is shortened by 1\/3).","1739224b":"# Features you can do offline (upload to notebook, then merge)","6c9eee80":"# ***Note that this code cannot be run directly and only provides the feature generation method and a list of model parameters***","19e12669":"# Features that need to be constantly updated","3c631c20":"![fea_importance](http:\/\/pic.downk.cc\/item\/5feae6fc3ffa7d37b359cf3b.jpg)","92bddb33":"# Feature list"}}