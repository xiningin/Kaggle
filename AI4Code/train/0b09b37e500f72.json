{"cell_type":{"bc5311b2":"code","6aafa5e8":"code","104c5be9":"code","5f6c712d":"code","38b7c4b7":"code","95660b2d":"code","47ea3aeb":"code","ba6fd5da":"code","6f339050":"code","d4b902e5":"code","498ba56e":"code","b8808216":"code","4464724b":"code","1bf94a37":"code","8ed85409":"code","bab3697f":"code","2a90ca69":"code","546768de":"code","4aa07fd8":"code","bacdf287":"code","10397eb3":"code","8d8c405e":"code","465010d7":"code","faad9629":"code","267f436a":"code","27eec43f":"code","146d6fae":"code","7fffb920":"code","3e5ab441":"code","ef05f047":"code","485bb0ef":"code","bd58b4dc":"code","94f55e45":"code","67b1b32a":"code","4bc6c8eb":"code","a8826169":"code","2adeffb5":"code","79bd4652":"code","be7d3ee8":"code","67e1ca30":"code","35b693ff":"code","7755434f":"code","13dbeff7":"code","eda88fec":"code","6a1e166e":"code","a3109744":"code","b7485e0f":"code","278b096a":"code","738ba428":"code","fa976968":"code","359c0ea8":"code","a861ce47":"code","c0d1324e":"markdown","2991486d":"markdown","59558859":"markdown","b6518cd9":"markdown","c39b435e":"markdown","e3f55e27":"markdown","a3cbebd7":"markdown","4689b62e":"markdown","054f6248":"markdown","5e88ebb0":"markdown","48608b82":"markdown","e2adb84c":"markdown","44096577":"markdown","29b6a253":"markdown","88c0ff47":"markdown","ba2631d0":"markdown","df63a4b8":"markdown","e73a8020":"markdown","f6dfa501":"markdown","6d381ba6":"markdown","fbcc8a33":"markdown","172946be":"markdown","3cef136d":"markdown","d8366007":"markdown","ccb87516":"markdown","2750b0cb":"markdown","8708d380":"markdown","da8818c4":"markdown","a27fd4d7":"markdown","b3c10538":"markdown","5f8df18a":"markdown","4111fe35":"markdown","fbf1c2e8":"markdown","e4cd9ee0":"markdown","7af25f0c":"markdown","49c4c5b2":"markdown","7cee278f":"markdown","ef1e6c90":"markdown","588bd2b4":"markdown"},"source":{"bc5311b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6aafa5e8":"# Basics\nimport numpy as np \nimport pandas as pd\n\n# Plotting\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport squarify\nimport seaborn as sns\n\n# Model stuff\nfrom sklearn.feature_extraction import DictVectorizer as DV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, mean_squared_error\n\n# Appearence\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom colorama import Fore","104c5be9":"train_csv = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\nprint(train_csv.info(verbose = True))","5f6c712d":"train_csv.head()","38b7c4b7":"test_csv = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/test.csv')\nprint(test_csv.info(verbose = True))","95660b2d":"target_data = train_csv['target']\n\nplt.figure(figsize=(9,9))\nplt.title('Target Boxplot', size = 14)\nsns.boxplot(data = target_data, color = 'royalblue')","47ea3aeb":"\"\"\"the max value = 10.3, but 25% quartile is ~6.8 so we have a few count of outliers in our data\"\"\"\ntarget_data.describe()","ba6fd5da":"\"\"\"lower boundary\"\"\"\nlower_whisker = plt.boxplot(target_data)['whiskers'][0].get_ydata()[1]\nprint(sum(1 for i in target_data if i <= (lower_whisker - 0.1)), 'value(s) is(are) below lower whisker') # 0.1 is the \"tolerance\" in case the test data will be shifted\n\n\"\"\"higher boundary\"\"\"\nhigher_whisker = plt.boxplot(target_data)['whiskers'][1].get_ydata()[1]\nprint(sum(1 for i in target_data if i >= (higher_whisker + 0.1)), 'value(s) is(are) above the upper whisker')","6f339050":"\"\"\"creating list with indexes\"\"\"\nindex_list = []\nfor value in target_data:\n    if value <= (lower_whisker - 0.1) or value >= (higher_whisker + 0.1):\n        index_list.append(list(target_data).index(value))\n\n\"\"\"rows deletion\"\"\"\ntrain_csv.drop(index_list, axis=0, inplace = True)","d4b902e5":"plt.figure(figsize=(9,9))\nplt.title('Target Boxplot', size = 14)\nsns.boxplot(data = train_csv['target'], color = 'royalblue')","498ba56e":"def treemap(DataFrame = train_csv):\n    f, axes = plt.subplots(nrows = 3, ncols= 3, figsize = (24,16))\n    \n    for i in range(train_csv.select_dtypes(include='object').shape[1] - 1):\n        \n        # Set labels & size for treemap\n        df = DataFrame.groupby('cat{}'.format(i)).size().reset_index(name = 'counts')\n        labels = df.apply( lambda x: str(x[0]) + '\\n (' + str(x[1]) + ')', axis = 1)\n        size = df['counts'].values\n        \n        \n        colors = [plt.cm.coolwarm(i\/float(len(labels))) for i in range(len(labels))]\n        squarify.plot(sizes=size, label=labels, color = colors, alpha = 0.8, ax = axes[i\/\/3, i%3])\n        \n        # Decorate\n        axes[i\/\/3, i%3].set_title('Treemap of Cat{}'.format(i))\n        axes[i\/\/3, i%3].axis('off')","b8808216":"treemap()","4464724b":"sns.catplot(x=\"cat6\", kind=\"count\", palette=\"coolwarm\", data=train_csv)\nplt.title('cat6 in train', size = 13.5)\n\nsns.catplot(x=\"cat6\", kind=\"count\", palette=\"coolwarm\", data=test_csv)\nplt.title('cat6 in test', size = 13.5)","1bf94a37":"# Select cont data only\ncont_data = train_csv.select_dtypes(include =['float64', 'int64'])\n\nnum_rows, num_cols = 4, 4\nf, axes = plt.subplots(nrows = num_rows, ncols= num_cols, figsize = (24,16))\n\nfor i, col_name in enumerate(cont_data):\n    \n    sns.kdeplot(cont_data[col_name], fill=True, color = 'royalblue',\n                   alpha=.5, linewidth=0, ax = axes[i \/\/ num_rows, i % num_cols])\n\nf.delaxes(axes[0, 0])\nf.delaxes(axes[3, 3])\nplt.tight_layout()\nplt.show()","8ed85409":"# plot heatmap\nplt.figure(figsize=(15,15))\nmask = np.triu(cont_data.corr())\nsns.heatmap(cont_data.corr(), annot=True, mask = mask, fmt=\".2f\", cmap='coolwarm',\n            cbar_kws={\"shrink\": .8}, vmin=0, vmax=1)\n# yticks\nplt.yticks(rotation=0)\nplt.show()","bab3697f":"def modify_df(df):\n    df['cat4'] = df['cat4'].apply(lambda x: x if x == 'B' else 'Z')\n    df['cat5'] = df['cat5'].apply(lambda x: x if x in ['B', 'D'] else 'Z')\n    df['cat6'] = df['cat6'].apply(lambda x: x if x == 'A' else 'Z')\n    df['cat7'] = df['cat7'].apply(lambda x: x if x in ['E', 'D'] else 'Z')\n    df['cat8'] = df['cat8'].apply(lambda x: x if x in ['E', 'C', 'G', 'A'] else 'Z')\n    \n    df['cont001'] = df['cont8'] * df['cont0']\n    df['cont002'] = df['cont9'] * df['cont0']\n    df['cont003'] = df['cont9'] * df['cont5']\n    df['cont004'] = df['cont8'] * df['cont5']\n    df['cont005'] = df['cont2'] * df['cont4']\n    df['cont006'] = df['cont1'] * df['cont3']\n    df['cont007'] = df['cont13'] * df['cont1']\n\n    return df\n\nmod_train_csv = modify_df(train_csv.copy())\nmod_test_csv = modify_df(test_csv.copy())\n\ntreemap(DataFrame = mod_train_csv)","2a90ca69":"feature_cols = mod_train_csv.drop(['id', 'target'], axis=1).columns\n\nX_cat = mod_train_csv[feature_cols].select_dtypes(include = 'object')\nX_cont = mod_train_csv[feature_cols].select_dtypes(exclude = 'object')\ny = mod_train_csv['target']","546768de":"feature_cols = mod_test_csv.drop(['id'], axis=1).columns\n\nX_cat_TEST = mod_test_csv[feature_cols].select_dtypes(include = 'object')\nX_cont_TEST = mod_test_csv[feature_cols].select_dtypes(exclude = 'object')","4aa07fd8":"encoder = DV(sparse = False)\nX_cat_oh = encoder.fit_transform(X_cat.T.to_dict().values())\n\n\"\"\"merging data\"\"\"\nX = np.hstack((X_cont, X_cat_oh))","bacdf287":"X_cat_oh_TEST = encoder.fit_transform(X_cat_TEST.T.to_dict().values())\n\n\"\"\"merging data\"\"\"\nX_TEST = np.hstack((X_cont_TEST, X_cat_oh_TEST))","10397eb3":"(X_train, \n X_test, \n y_train, y_test) = train_test_split(X, y, \n                                     test_size=0.2, \n                                     random_state=0,\n                                     )","8d8c405e":"# Import library\nfrom sklearn import linear_model\n\n# Model set\nestimator1 = linear_model.SGDRegressor(random_state = 42)\n\n\"\"\"Grid Search\"\"\"\n\nparameters_grid = {\n    'penalty' : ['l1', 'l2', 'elasticnet'],\n    'alpha' : np.linspace(0.0001, 0.001, num = 5),\n}\n\ncv = model_selection.ShuffleSplit(n_splits = 5, test_size = 0.2, random_state = 42)\n\ngrid_cv_LM = model_selection.GridSearchCV(estimator1, parameters_grid, scoring = 'neg_root_mean_squared_error', cv = cv)","465010d7":"%%time\ngrid_cv_LM.fit(X_train, y_train)\n\npredictions_LM = grid_cv_LM.best_estimator_.predict(X_test)\n\nscore_rmse_LM = (mean_squared_error(y_test, predictions_LM))**0.5\nprint(Fore.GREEN + 'Base Linear SGDRegressor RMSE: {}'.format(score_rmse_LM))","faad9629":"# Import library\nfrom sklearn.ensemble import RandomForestRegressor\n\nestimator_RF = RandomForestRegressor(n_jobs=-1, random_state=42)","267f436a":"%%time\nestimator_RF.fit(X_train, y_train)\n\n# Test\npredictions_RF = estimator_RF.predict(X_test)\n\nscore_rmse_RF = (mean_squared_error(y_test, predictions_RF))**0.5\nprint(Fore.GREEN + 'RandomForest RMSE: {}'.format(score_rmse_RF))","27eec43f":"%%time\nfrom xgboost import XGBRegressor\nXGB_default = XGBRegressor(random_state=42, tree_method='gpu_hist');\n\nXGB_default.fit(X_train, y_train);","146d6fae":"predictions_XGB = XGB_default.predict(X_test)\n\nscore_rmse_XGB = (mean_squared_error(y_test, predictions_XGB))**0.5\nprint(Fore.GREEN + 'Base XGBoost RMSE: {}'.format(score_rmse_XGB))","7fffb920":"\"\"\"best params\"\"\"\n\nxgb_params = {\n    'booster':'gbtree',\n    'n_estimators':10000,\n    'max_depth':7, \n    'eta':0.01,\n    'gamma':1.8,\n    'objective':'reg:squarederror',\n    'verbosity':0,\n    'subsample':0.85,\n    'colsample_bytree':0.4,\n    'lambda':2.7,\n    'alpha':6,\n    'scale_pos_weight':1,\n    'objective':'reg:squarederror',\n    'eval_metric':'rmse',\n    'seed': 42,\n    'tree_method':'gpu_hist',\n    'gpu_id':0\n}","3e5ab441":"%%time\n\nXGB_tune = XGBRegressor(**xgb_params);\nXGB_tune.fit(X_train, y_train);","ef05f047":"\"\"\"predict\"\"\"\n\npredictions_XGB = XGB_tune.predict(X_test)\n\nscore_rmse_XGB = (mean_squared_error(y_test, predictions_XGB))**0.5\nprint(Fore.GREEN + 'Tune XGB RMSE: {}'.format(score_rmse_XGB))","485bb0ef":"# import library\nfrom sklearn.neighbors import KNeighborsRegressor\n\nestimator_kNN = KNeighborsRegressor()\nestimator_kNN.fit(X_train, y_train)","bd58b4dc":"\"\"\"predict\"\"\"\n\npredictions_kNN = estimator_kNN.predict(X_test)\n\nscore_rmse_kNN = (mean_squared_error(y_test, predictions_kNN))**0.5\nprint(Fore.GREEN + 'Base kNN RMSE: {}'.format(score_rmse_kNN))","94f55e45":"from lightgbm import LGBMRegressor\nimport lightgbm as lgb\n\nbest_params = {\n    'reg_lambda': 0.015979956459638782,\n    'reg_alpha': 9.103977313355028,\n    'colsample_bytree': 0.3,\n    'subsample': 1.0,\n    'learning_rate': 0.009,\n    'n_estimators': 3000,\n    'max_depth': 15,\n    'min_child_samples': 142,\n    'num_leaves': 84,\n    'random_state': 42, \n    'device': 'gpu',\n}\n\n# Instantiate model with 100 decision trees\nestimator_LGBM = LGBMRegressor(**best_params)\n\nestimator_LGBM.fit(X_train, y_train)\n\n# Use the forest's predict method on the test data\npredictions_LGBM = estimator_LGBM.predict(X_test)\n\nscore_rmse_LGBM = (mean_squared_error(y_test, predictions_LGBM))**0.5\nprint(Fore.GREEN + 'Tuned LGBM RMSE: {}'.format(score_rmse_LGBM))","67b1b32a":"results = {'score': [score_rmse_LM, score_rmse_RF, score_rmse_XGB, score_rmse_kNN, score_rmse_LGBM], 'model': ['Linear', 'RF', 'XGB', 'kNN', 'LGBM']}\nprint(Fore.WHITE + 'Results:\\n')\nfor i in range(5):\n    print(Fore.GREEN + list(results.values())[1][i], '\\t', list(results.values())[0][i], '\\n')\n    \nprint(Fore.WHITE + 'LGBM is the best model')","4bc6c8eb":"estimator_LGBM.fit(X, y)\n\npredictions = estimator_LGBM.predict(X_TEST)","a8826169":"XGB_tune.fit(X, y)\n\npredictions2 = XGB_tune.predict(X_TEST)","2adeffb5":"my_submission = pd.DataFrame({'id': test_csv.id, 'target': predictions})\n\nmy_submission.to_csv('submission.csv', index=False)","79bd4652":"my_submission2 = pd.DataFrame({'id': test_csv.id, 'target': predictions2})\n\nmy_submission2.to_csv('submission.csv', index=False)","be7d3ee8":"train_csv['cat6'] = train_csv['cat6'].apply(lambda x: x if x in ['A', 'B', 'C', 'D', 'E', 'I', 'H']  else 'A')\nsns.catplot(x=\"cat6\", kind=\"count\", palette=\"coolwarm\", data=train_csv)\nplt.title('cat6 in train', size = 13.5)","67e1ca30":"feature_cols = train_csv.drop(['id', 'target'], axis=1).columns\n\nX_cat = train_csv[feature_cols].select_dtypes(include = 'object')\nX_cont = train_csv[feature_cols].select_dtypes(exclude = 'object')\ny = train_csv['target']","35b693ff":"feature_cols = test_csv.drop(['id'], axis=1).columns\n\nX_cat_TEST = test_csv[feature_cols].select_dtypes(include = 'object')\nX_cont_TEST = test_csv[feature_cols].select_dtypes(exclude = 'object')","7755434f":"encoder = DV(sparse = False)\nX_cat_oh = encoder.fit_transform(X_cat.T.to_dict().values())\n\n\"\"\"merging data\"\"\"\nX = np.hstack((X_cont, X_cat_oh))","13dbeff7":"X_cat_oh_TEST = encoder.fit_transform(X_cat_TEST.T.to_dict().values())\n\n\"\"\"merging data\"\"\"\nX_TEST = np.hstack((X_cont_TEST, X_cat_oh_TEST))","eda88fec":"(X_train, \n X_test, \n y_train, y_test) = train_test_split(X, y, \n                                     test_size=0.2, \n                                     random_state=0,\n                                     )","6a1e166e":"\"\"\"best params\"\"\"\n\nxgb_params = {\n    'booster':'gbtree',\n    'n_estimators':10000,\n    'max_depth':7, \n    'eta':0.01,\n    'gamma':1.8,\n    'objective':'reg:squarederror',\n    'verbosity':0,\n    'subsample':0.85,\n    'colsample_bytree':0.4,\n    'lambda':2.7,\n    'alpha':6,\n    'scale_pos_weight':1,\n    'objective':'reg:squarederror',\n    'eval_metric':'rmse',\n    'seed': 42,\n    'tree_method':'gpu_hist',\n    'gpu_id':0\n}\n\nXGB_tune1 = XGBRegressor(**xgb_params);\nXGB_tune1.fit(X_train, y_train);\n\n\"\"\"predict\"\"\"\n\npredictions_XGB1 = XGB_tune1.predict(X_test)\n\nscore_rmse_XGB1 = (mean_squared_error(y_test, predictions_XGB1))**0.5\nprint(Fore.GREEN + 'Tune XGB RMSE: {}'.format(score_rmse_XGB1))","a3109744":"best_params = {\n    'reg_lambda': 0.015979956459638782,\n    'reg_alpha': 9.103977313355028,\n    'colsample_bytree': 0.3,\n    'subsample': 1.0,\n    'learning_rate': 0.009,\n    'n_estimators': 3000,\n    'max_depth': 15,\n    'min_child_samples': 142,\n    'num_leaves': 84,\n    'random_state': 42, \n    'device': 'gpu',\n}\n\n# Instantiate model with 100 decision trees\nestimator_LGBM1 = LGBMRegressor(**best_params)\n\nestimator_LGBM1.fit(X_train, y_train)\n\n# Use the forest's predict method on the test data\npredictions_LGBM1 = estimator_LGBM1.predict(X_test)\n\nscore_rmse_LGBM1 = (mean_squared_error(y_test, predictions_LGBM1))**0.5\nprint(Fore.GREEN + 'Tuned LGBM RMSE: {}'.format(score_rmse_LGBM1))","b7485e0f":"estimator_LGBM1.fit(X, y)\n\npredictions3 = estimator_LGBM1.predict(X_TEST)","278b096a":"my_submission3 = pd.DataFrame({'id': test_csv.id, 'target': predictions3})\n\nmy_submission3.to_csv('submission3.csv', index=False)","738ba428":"my_submission3","fa976968":"estimator_LGBM1.fit(X_train, y_train)","359c0ea8":"predictions4 = estimator_LGBM1.predict(X_TEST)","a861ce47":"my_submission4 = pd.DataFrame({'id': test_csv.id, 'target': predictions4})\n\nmy_submission4.to_csv('submission4.csv', index=False)","c0d1324e":"## Categorial  \nLets create a treemap of our categorial data","2991486d":"## Test.csv","59558859":"# kNN \ud83d\udc6f\u200d","b6518cd9":"# Train Test Split \ud83e\ude93\nFor checking our models we have to split our data ","c39b435e":"## Train O-H-E","e3f55e27":"## Test O-H-E","a3cbebd7":"## XGB","4689b62e":"## Test","054f6248":"## Train test split","5e88ebb0":"## Test","48608b82":"# LGMB\nwanna try LGBM model with optimize params that I found on https:\/\/www.kaggle.com\/andreshg\/tps-feb-a-complete-study#5.-Optimized-LGBM-CrossValidated-%F0%9F%A7%AE","e2adb84c":"As we see there is no null data and all continuous features are normalized","44096577":"## Correlation between features","29b6a253":"# Linear Model \ud83d\udcc9","88c0ff47":"# XGBoost Parameter Tuning \ud83d\udeb5\ud83c\udffb","ba2631d0":"**Hello there!**  \n  This is my not only first competition on Kaggle, but the first off-course assignment. My goal is to test some methods, that I'v learned so far, and also to try data visualization techniques.  \n  So I would be very glad to get some feedback ","df63a4b8":"**Lets count how many outliers are there**","e73a8020":"## Target  \nWe can try to visualize target data with boxplot. That would show us such characteristics as percentiles, min (max) values and the set of extreme values ","f6dfa501":"## LGMB","6d381ba6":"## Train","fbcc8a33":"# Train on full data & Submit \ud83d\udd29","172946be":"# Random Forest \ud83c\udf33","3cef136d":"## Train","d8366007":"# Task Detail \n\n## Goal\nFor this competition, you will be predicting a **continuous target** based on a number of feature columns given in the data. All of the feature columns, **cat0 - cat9 are categorical**, and the feature columns **cont0 - cont13 are continuous**.  \n  \nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the amount of an insurance claim. Although the features are **anonymized**, they have properties relating to real-world features.  \n  \n  ## Metric\nSubmissions are scored on the root mean squared error. RMSE is defined as:\n$$\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}$$\n\nwhere  is the predicted value,  is the original value, and  is the number of rows in the test data.","ccb87516":"# XGBoost \ud83c\udfc3\ud83c\udffb","2750b0cb":"**258** values are only **0.086%** of all data, so we can delete these rows in our DataFrame","8708d380":"As we can see, there is no high correlation between variables","da8818c4":"# One-Hot-Encoding \ud83d\udcdf","a27fd4d7":"# First Look On The Data  \n## Train.csv","b3c10538":"## Continuous","5f8df18a":"## Test","4111fe35":"# Importing Libraries","fbf1c2e8":"As we can see, there are some extreme points in our dataset. These points may correspond to reality, in the case of a *large number of them*.  \nBut if there are only few of them, it would be better to **get rid of them**, because the submissions are scored on the root mean squared error, which is very sensitive to outliers in the data","e4cd9ee0":"# Feature Modify \ud83e\uddf0","7af25f0c":"## Train","49c4c5b2":"# Data Visualization \ud83d\udcca","7cee278f":"# Another way to Feature Modify","ef1e6c90":"As we can see, there is no much complexity in categorial features. We have  \n* 5 columns, where one value is dominating others\n* 3 columns, where two values are dominating others  \n* 1 column with with a wide variability  \n  \n**Another moment that we need to notice is that the** `number of cat features in test != number of cat features in train` ","588bd2b4":"# Prepare Submission File \ud83d\udcdd"}}