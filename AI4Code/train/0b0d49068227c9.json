{"cell_type":{"26486ae9":"code","fd1be63d":"code","e8237bed":"code","58204392":"code","0c1cf9d0":"code","eb9f3807":"code","097940ab":"code","226bf4de":"code","28257035":"code","2f6d9050":"code","33f46280":"code","a73b3e6c":"code","76dffc68":"code","fc7fcdbb":"code","ab20a115":"code","4e35a477":"code","5a1803ef":"code","542b90de":"code","17709431":"code","2abe90b0":"code","d6523efc":"code","cd9efb52":"code","de5f229b":"code","dcac6c2e":"code","e3e3caa2":"code","2aeb66f4":"code","e52218df":"code","f9c759b4":"code","62d2a12c":"code","d0f8bb6c":"code","831531ef":"code","63f46b9e":"code","046e9545":"code","86022dc3":"code","c80f5954":"code","00554630":"code","ca393b5b":"code","6b721c04":"code","e0d79747":"code","3e0a26f6":"code","08557a06":"code","51106e89":"code","d613cb42":"code","a1bb565d":"code","cbf8c7b9":"code","ad3616a4":"code","3fe5ed1e":"code","e0d75c45":"code","172ab67d":"code","cecf930d":"code","3c990b47":"code","d548f165":"code","704c5cca":"code","53451f43":"code","f2f2eaec":"code","66150e68":"code","299b7b9e":"code","9b2f96c6":"code","ae3de445":"code","577c6486":"code","87f1539f":"code","c455e963":"code","8001bfca":"code","f7a477df":"code","677baa3a":"code","08bb2611":"code","268ad085":"code","8f1d3558":"code","d420cb0b":"code","6c9d97aa":"code","f15a70da":"code","9250e47a":"code","4564fc48":"code","51393132":"code","d8bb7a26":"code","f337a540":"code","184d5c02":"code","8e9a7641":"code","97098891":"code","3a848696":"markdown","eb07994b":"markdown","10efe9cd":"markdown","fe479913":"markdown","54569e06":"markdown","697c32a1":"markdown","2c278094":"markdown","a372eae5":"markdown","4490abd4":"markdown","a90b663b":"markdown","6d08592f":"markdown","5e8e742c":"markdown","75870ae5":"markdown","132fbd47":"markdown","1fdd3130":"markdown","f3a7eae2":"markdown","5e9ae3a5":"markdown","0b5a9a59":"markdown","1f45f199":"markdown","750c0ab0":"markdown","1995de46":"markdown","5c0ae812":"markdown","a9f582f0":"markdown","fdfa4f3b":"markdown","eb31cf33":"markdown","4649eb9d":"markdown","3dd71c4a":"markdown","0aebec1d":"markdown","c7564c62":"markdown","939b4b14":"markdown","aa257744":"markdown","16b529c1":"markdown","86274e7b":"markdown","f2623bb8":"markdown","562cfbcd":"markdown","806ba61a":"markdown","c9c9104d":"markdown","8a291c57":"markdown","95765cba":"markdown","dd2c2ebe":"markdown","9cc075d8":"markdown","6fff20a2":"markdown","a665f5e5":"markdown","5dd5d0a3":"markdown","fcb8dd5a":"markdown","49476152":"markdown","76120b1f":"markdown","370222f0":"markdown","f3251426":"markdown","006bca76":"markdown","22f54392":"markdown","3b6f15fc":"markdown","13fb08b6":"markdown","8f00948c":"markdown","c8f5a943":"markdown","a1fd4ca2":"markdown","bc10cb0e":"markdown","01901e19":"markdown","0e8f168b":"markdown","72494c93":"markdown","f3890a8c":"markdown","1558a979":"markdown","fcf5ba31":"markdown","20b5a156":"markdown","3aeb9bd6":"markdown","d7b7a0a7":"markdown","afad3c52":"markdown","f95d5624":"markdown","57d33fb2":"markdown","da1b3fd3":"markdown","d89b49ab":"markdown","6906eea9":"markdown","8aaa4958":"markdown","bf03cadc":"markdown","7a3ad1bb":"markdown","6d28e583":"markdown","7499e369":"markdown","6865aa17":"markdown","32170d0a":"markdown","062a118d":"markdown","6cc250df":"markdown","ed443abd":"markdown","deb292be":"markdown","bca1e5c1":"markdown","bd93432d":"markdown","9ded4c22":"markdown","4e29287e":"markdown","d5dd31bc":"markdown","bb5bfd43":"markdown","b2f624e1":"markdown","a4c005b8":"markdown","ab149eaa":"markdown","a1664fcf":"markdown","659bbf69":"markdown","d2b807d0":"markdown","8c9c09d6":"markdown","4f705a6b":"markdown","f5b8773e":"markdown","7923b422":"markdown","27c54ac7":"markdown","e3b2edb1":"markdown","6bc0fb02":"markdown","f637620a":"markdown","2fe5977b":"markdown","e6872e32":"markdown","37dcd840":"markdown","adb7d9b7":"markdown","5564a5ae":"markdown","31212d94":"markdown","f638c9bd":"markdown","edef110a":"markdown","257e7666":"markdown","cd5659a7":"markdown","d634ef5f":"markdown","f795e32b":"markdown","57f20ebc":"markdown","d0a5f07c":"markdown","706c0c24":"markdown","54819bb4":"markdown","727824e2":"markdown","14ada488":"markdown","50bab8e2":"markdown","8919429d":"markdown","87cec22e":"markdown","4f5af16c":"markdown","4ec0236d":"markdown","5a98db85":"markdown","c60e71b9":"markdown"},"source":{"26486ae9":"#Data handling Imports\nimport pandas as pd \nimport numpy as np\n\n#Visualizing Data Imports\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n%matplotlib inline\n\n\n#Notebook arrange Imports\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#Encoding Imports\nfrom sklearn.preprocessing import LabelEncoder\n\n#Feature Selection Imports\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import GenericUnivariateSelect\n\n#Sampling Imports\nfrom sklearn.model_selection import KFold\n\n#Scaling Imports\nfrom sklearn.preprocessing import StandardScaler\n\n#Modeling Imports\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Pre-processing Imports\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import randint as sp_randint\n\n\n#Accuracy Validation Imports\nfrom sklearn import metrics\nfrom sklearn.metrics import auc, roc_curve, f1_score, accuracy_score,precision_recall_curve,\\\nconfusion_matrix, classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.model_selection import cross_val_score\n\n#Model Interpretation Imports\nimport eli5, shap\n\n\n#Other required libraries\nimport time\nstart = time. time()","fd1be63d":"#Loading dataset from UCI repository using URL\n# url = \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/mushroom\/agaricus-lepiota.data\"\n\ncolumn_names = ['class','cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor', 'gill-attachment',\n                'gill-spacing','gill-size', 'gill-color', 'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n                'stalk-surface-below-ring', 'stalk-color-above-ring', 'stalk-color-below-ring','veil-type',\n                'veil-color','ring-number','ring-type','spore-print-color','population','habitat' ]\n\ndf = pd.read_csv('..\/input\/mushroom-uci-dataset\/agaricus-lepiota.data',names=column_names)\n\n# Loading the dataset into a DataFrame df\n#df = pd.read_csv('agaricus-lepiota.data', names=column_names)\n\n#Print out the first 5 rows.\ndf.head()","e8237bed":"my_dict ={'Class': 'edible=e, poisonous=p','*------------------------------- Features related to CAP':'------------------------------- *', 'Cap-shape': 'bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s', 'Cap-surface': 'fibrous=f, grooves=g, scaly=y, smooth=s', 'Cap-color': 'brown=n, buff=b, cinnamon=c, gray=g, green=r, pink=p, purple=u, red=e, white=w, yellow=y',  '*------------------------------- Features related to GILL':'------------------------------- *','Gill-attachment': 'attached=a, descending=d, free=f, notched=n', 'Gill-spacing': 'close=c, crowded=w, distant=d', 'Gill-size': 'broad=b, narrow=n', 'Gill-color': 'black=k, brown=n, buff=b, chocolate=h, gray=g, green=r, orange=o, pink=p, purple=u, red=e, white=w, yellow=y', '*------------------------------- Features related to STALK':'------------------------------- *', 'Stalk-shape': 'enlarging=e, tapering=t', 'Stalk-root': 'bulbous=b, club=c, cup=u, equal=e, rhizomorphs=z, rooted=r, missing=?', 'Stalk-surface-above-ring': 'fibrous=f, scaly=y, silky=k, smooth=s', 'Stalk-surface-below-ring': 'fibrous=f, scaly=y, silky=k, smooth=s',  'Stalk-color-above-ring': 'brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y', 'Stalk-color-below-ring': 'brown=n, buff=b, cinnamon=c, gray=g, orange=o, pink=p, red=e, white=w, yellow=y', '*------------------------------- Features related to VEIL':'------------------------------- *', 'Veil-type': 'partial=p, universal=u',  'Veil-color': 'brown=n, orange=o, white=w, yellow=y',  'Ring-number': 'none=n, one=o, two=t',  'Ring-type': 'cobwebby=c, evanescent=e, flaring=f, large=l, none=n, pendant=p, sheathing=s, zone=z', '*------------------------------- Features related to MISC':'------------------------------- *', 'Bruises': 'bruises=t,no=f', 'Odor': 'almond=a,anise=l, creosote=c, fishy=y, foul=f, musty=m, none=n, pungent=p, spicy=s', 'Spore-print-color': 'black=k, brown=n, buff=b, chocolate=h, green=r, orange=o, purple=u, white=w, yellow=y', 'Population': 'abundant=a, clustered=c, numerous=n, scattered=s, several=v, solitary=y', 'Habitat': 'grasses=g, leaves=l, meadows=m, paths=p, urban=u, waste=w, woods=d'}\n\ndf_attr = pd.DataFrame(list(my_dict.items()),columns = ['Attribute Information','Values'])\n\ndf_attr.style.hide_index().set_table_styles([dict(selector='th', props=[('text-align', 'left')])]).set_properties(**{'text-align': 'left'})","58204392":"#Check if there is duplicate data in the dataset or not\nduplicates = df.duplicated().sum()\ncount = df.shape[0]\nprint(f'There are {duplicates} duplicate rows in {count} rows')","0c1cf9d0":"#Checking the null,NAN values in the dataframe and its sum\nprint(f'Null values in dataset {df.isnull().sum().sum()}')\nprint(f'NAN Values in dataset {df.isna().sum().sum()}')","eb9f3807":"#print the summary of the dataset which gives a description of the data.\ndf.info()","097940ab":"#Checking uniques values in status column\ndf['class'].unique()","226bf4de":"#print the shape of the DataFrame\nrows,cols = df.shape\nprint(f'There are {rows} rows and {cols} columns in the dataset.\\n')\n\n#Find total edible Mushrooms(e)\nmush = df[df['class']=='e'].count()[\"class\"]\navg = mush\/rows*100\n\nprint(f'Total number of Mushrooms that are Edible: {mush}')\nprint(f'Percentage of Edible Mushrooms in the dataset are {avg:.3g}%\\n')\n\n#Find total Poisonous Mushrooms(p)\nmush = df[df['class']=='p'].count()[\"class\"]\navg = mush\/rows*100\n\nprint(f'Total number of Mushrooms that are Poisonous: {mush}')\nprint(f'Percentage of Poisonous Mushrooms in the dataset are {avg:.3g}%')\n","28257035":"#Copiying actual dataframe to another for EDA- Visualisation\ndf_eda = df.copy() \n\n#Changing all the column data with its expansion and keeping it in dict which will be used for visualisation to find the relations b\/w TGT variable\n\nclasses = dict(p='Poisonous', e='Edible')\ncap_shape = dict(b='bell', c='conical', x='convex', f='flat', k='knobbed', s='sunken')\ncap_surface = dict(f='fibrous', g='grooves', y='scaly', s='smooth')\ncap_color = dict(n='brown', b='buff', c='cinnamon', g='gray', r='green', p='pink', u='purple', e='red', w='white', y='yellow')\ngill_attachment = dict(a='attached', d='descending', f='free', n='notched')\ngill_spacing = dict(c='close', w='crowded', d='distant')\ngill_size = dict(b='broad', n='narrow')\ngill_color = dict(n='brown', b='buff', g='gray', r='green', p='pink', u='purple', o='orange',h='chocolate', k='black', e='red', w='white', y='yellow')\nstalk_shape = dict(e='enlarging',t='tapering')\nstalk_root = {'b':'bulbous','c':'club','u':'cup','e':'equal','z':'rhizomorphs','r':'rooted', '?':'missing'}\nstalk_surface_above_ring = dict(f='fibrous',y='scaly',k='silky',s='smooth')\nstalk_surface_below_ring = dict(f='fibrous',y='scaly',k='silky',s='smooth')\nstalk_color_above_ring = dict(n='brown', b='buff', c='cinnamon', g='gray', o='orange', p='pink', e='red',w='white', y='yellow')\nstalk_color_below_ring = dict(n='brown', b='buff', c='cinnamon', g='gray', o='orange', p='pink', e='red',w='white', y='yellow')\nveil_color = dict(n='brown', o='orange', w='white', y='yellow')\nring_number = dict(n='None', o='One', t='Two')\nring_type = dict(c='cobwebby', e='evanescent', f='flaring', l='large', n='none', p='pendant', s='sheathing', z='zone')\nbruises = dict(t='True', f='False')\nodor = dict(a='almond', l='anise', c='creosote', y='fishy', f='foul', m='musty', n='none', p='pungent', s='spicy')\nspore_print_color = dict(k='black', n='brown', b='buff', h='chocoloate', r='green', o='Orange', u='Purple',w='White', y='Yellow')\npopulation = dict(a='abundant', c='clustered', n='numerous', s='scattered', v='several', y='solitary')\nhabitat = dict(g='grasses', l='leaves', m='meadows', p='paths', u='urban', w='waste', d='woods')\n\n#Mapping the dict to the new dataframe with all the abbreviations\ndf_eda['class'] = df_eda['class'].map(classes)\n\ndf_eda['cap-shape'] = df_eda['cap-shape'].map(cap_shape)\ndf_eda['cap-surface'] = df_eda['cap-surface'].map(cap_surface)\ndf_eda['cap-color'] = df_eda['cap-color'].map(cap_color)\n\ndf_eda['gill-attachment'] = df_eda['gill-attachment'].map(gill_attachment)\ndf_eda['gill-spacing'] = df_eda['gill-spacing'].map(gill_spacing)\ndf_eda['gill-size'] = df_eda['gill-size'].map(gill_size)\ndf_eda['gill-color'] = df_eda['gill-color'].map(gill_color)\n\ndf_eda['stalk-shape'] = df_eda['stalk-shape'].map(stalk_shape)\ndf_eda['stalk-root'] = df_eda['stalk-root'].map(stalk_root)\ndf_eda['stalk-surface-above-ring'] = df_eda['stalk-surface-above-ring'].map(stalk_surface_above_ring)\ndf_eda['stalk-surface-below-ring'] = df_eda['stalk-surface-below-ring'].map(stalk_surface_below_ring)\ndf_eda['stalk-color-above-ring'] = df_eda['stalk-color-above-ring'].map(stalk_color_above_ring)\ndf_eda['stalk-color-below-ring'] = df_eda['stalk-color-below-ring'].map(stalk_color_below_ring)\n\ndf_eda['veil-color'] = df_eda['veil-color'].map(veil_color)\ndf_eda['ring-number'] = df_eda['ring-number'].map(ring_number)\ndf_eda['ring-type'] = df_eda['ring-type'].map(ring_type)\n\ndf_eda['bruises'] = df_eda['bruises'].map(bruises)\ndf_eda['odor'] = df_eda['odor'].map(odor)\ndf_eda['spore-print-color'] = df_eda['spore-print-color'].map(spore_print_color)\ndf_eda['population'] = df_eda['population'].map(population)\ndf_eda['habitat'] = df_eda['habitat'].map(habitat)\n\n#Checking result\ndf_eda.head()\n","2f6d9050":"#The number of mushrooms classwise - e=Edible, p=Poisonous\nplt.rcParams['figure.figsize']=15,5\nplt.subplot(121)\nplt.title('Mushroom Class Type Count', fontsize=10)\ns = sb.countplot(x = \"class\", data = df_eda, alpha=0.7)\nfor p in s.patches:\n    s.annotate(format(p.get_height(), '.1f'), \n               (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                ha = 'center', va = 'center', \n                xytext = (0, 4), \n                textcoords = 'offset points')\n\n\nax = plt.subplot(122)\nmush_classpie = df_eda['class'].value_counts()\nmush_size = mush_classpie.values.tolist()\nmush_types = mush_classpie.axes[0].tolist()\nmush_labels = 'Edible', 'Poisonous'\ncolors = ['#EAFFD0', '#F38181']\nplt.title('Mushroom Class Type Percentange', fontsize=10)\npatches, texts, autotexts = plt.pie(mush_size, labels=mush_labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=150)\nfor text,autotext in zip(texts,autotexts):\n    text.set_fontsize(14)\n    autotext.set_fontsize(14)\n\nplt.axis('equal')  \nplt.show()","33f46280":"#Visualizing Data cap-color wise count. Set bar colour as cap-color. ;)\n#Auto-labels the number of mushrooms for each bar color.\ndef label(bars,fontsize=9):\n    \"\"\" Displaying the count of each colour on top of the bar \"\"\"\n    for bar in bars:\n        height = bar.get_height()\n        ax.text(bar.get_x() + bar.get_width()\/2., 1*height,'%d' % int(height),\n                ha='center', va='bottom',fontsize=fontsize)\n\n#Visualizing Data cap-color wise count\nplt.rcParams['figure.figsize']=20,5\nax = plt.subplot(121)\ncap_colors = df_eda['cap-color'].value_counts()\nm_height = cap_colors.values.tolist()\n#Row labels\ncap_colors.axes\n#Converts index to list\ncap_color_labels = cap_colors.axes[0].tolist()\n#the x locations for the groups\nind = np.arange(10)\n#Setting width of the bars\nwidth = 0.7        \ncolors = ['brown','gray','red','yellow','#f8f8ff','#F0DC82','pink','#D22D1E','#C000C5','g']\nmushroom_bars = ax.bar(ind, m_height , width, color=colors)\n#Setting labels, title and axes ticks\nax.set_xlabel('---Cap Color---',fontsize=10)\nax.set_ylabel('---Quantity---',fontsize=10)\nax.set_title('Quantity Cap-Color Wise',fontsize=12)\nax.set_xticks(ind)\nax.set_xticklabels(('Brown', 'Gray','Red','Yellow','White','Buff','Pink','Cinnamon','Purple','Green'), fontsize = 10, rotation=45)\nlabel(mushroom_bars)\n\n\n\nax = plt.subplot(122)\npoisonous_cc = [] #Poisonous color cap list\nedible_cc = []    #Edible color cap list\nfor capColor in cap_color_labels:\n    size = len(df_eda[df_eda['cap-color'] == capColor].index)\n    edibles = len(df_eda[(df_eda['cap-color'] == capColor) & (df_eda['class'] == 'Edible')].index)\n    edible_cc.append(edibles)\n    poisonous_cc.append(size-edibles)\n                        \nwidth = 0.4\n# fig, ax = plt.subplots(figsize=(7,4))\nedible_bars = ax.bar(ind, edible_cc , width, color='#EAFFD0')\npoison_bars = ax.bar(ind+width, poisonous_cc , width, color='#F38181')\n\n#Add some text for labels, title and axes ticks\nax.set_xlabel('---Cap Color---',fontsize=10)\nax.set_ylabel('---Quantity---',fontsize=10)\nax.set_title('Mushrooms Cap Color & Class',fontsize=12)\nax.set_xticks(ind + width \/ 2) #Positioning on the x axis\nax.set_xticklabels(('Brown', 'Gray','Red','Yellow','White','Buff','Pink','Cinnamon','Purple','Green'), fontsize = 10, rotation=45)\nax.legend((edible_bars,poison_bars),('Edible','Poisonous'),fontsize=10)\nlabel(edible_bars, 10)\nlabel(poison_bars, 10)\n# plt.show()\n\n\nplt.show()","a73b3e6c":"#The other 2 cap related attributes using its own unique categories and the ratio of each contains class [e=Edible, p=Poisonous]\nf, axes = plt.subplots(2,1, figsize=(20,5), sharey = True) \nnum_col = ['cap-surface','cap-shape']\nfor i,col in enumerate(num_col):\n    axes[i] = plt.subplot(1,2,i+1)\n    s = sb.countplot(x=col, data = df_eda, hue='class', alpha=0.7)\n    s.legend(loc=\"upper right\", prop={'size': 10})\n    for p in s.patches:\n        s.annotate(format(p.get_height(), '.0f'), \n        (p.get_x() + p.get_width() \/ 2., p.get_height()), \n        ha = 'center', va = 'center', \n        xytext = (0, 5), \n        textcoords = 'offset points')\n\nplt.show()","76dffc68":"#Checking other 4 attributes using histplot along with the ratio of each contains class [e=Edible, p=Poisonous] \nf, axes = plt.subplots(4,1, figsize=(20,15), sharey = True) \nnum_col = ['gill-attachment', 'gill-spacing', 'gill-size', 'gill-color']\nfor i,col in enumerate(num_col):\n    plt.subplot(2,2,i+1)\n    plt.xticks(rotation=45)\n    s = sb.countplot(x=col, data = df_eda, hue='class', palette=\"pastel\")\n    for p in s.patches:\n        s.annotate(format(p.get_height(), '.0f'), \n        (p.get_x() + p.get_width() \/ 2., p.get_height()), \n        ha = 'center', va = 'center', \n        xytext = (0, 5), fontsize=8.5,\n        textcoords = 'offset points')\n\nplt.show()","fc7fcdbb":"#Checking 4 out of 6 stalk-attributes using souble pie chart along with the ratio of each contains class [e=Edible, p=Poisonous] \n#Which will give class wise count & the relative ratio of their own unique values\nf, axes = plt.subplots(4,1, figsize=(17,12), sharey = True) \nnum_col = ['stalk-shape', 'stalk-root', 'stalk-surface-above-ring', 'stalk-surface-below-ring']\n\nfor j,col in enumerate(num_col):\n    cols = df_eda[col].value_counts()\n    pop_size = cols.values.tolist()\n    pop_types = cols.axes[0].tolist()\n    poisonous_pop = [] #Poisonous population type list\n    edible_pop = []    #Edible population type list\n    for pop in pop_types: \n        size = len(df_eda[df_eda[col] == pop].index)\n        edibles = len(df_eda[(df_eda[col] == pop) & (df_eda['class'] == 'Edible')].index)\n        edible_pop.append(edibles)\n        poisonous_pop.append(size-edibles)\n    combine_ed_poi = []\n    for i in range(0,len(edible_pop)):\n        combine_ed_poi.append(edible_pop[i])\n        combine_ed_poi.append(poisonous_pop[i])\n\n    #Double pie chart.\n    plt.subplot(2,2,j+1)\n    plt.title(col)\n    #Outer Pie Chart\n    patches1, texts1 = plt.pie(combine_ed_poi,radius = 4.5,labels= combine_ed_poi,\n                                    colors=['#C4F6F5','#F6EEC4'], shadow=True, labeldistance= 1.1)\n    for i in range(0,len(texts1)):\n        if(i%2==0):\n            texts1[i].set_color('blue')\n        else:\n            texts1[i].set_color('red')\n    for aut in texts1:\n        aut.set_fontsize(9)\n    #Inner Pie Chart\n    patches2, texts2, autotexts2 = plt.pie(pop_size, radius = 3.5,\n            autopct='%1.2f%%', shadow=True, labeldistance= 4.2)\n    for aut in autotexts2:\n        aut.set_fontsize(10)\n        aut.set_horizontalalignment('center')\n    #Set 2 Legends to the plot.\n    first_legend   = plt.legend(patches1, ['Edible','Poisonous'], loc=\"upper left\", fontsize=10)\n    second_ledgend = plt.legend(patches2, pop_types, loc=\"best\",fontsize=8)\n    plt.gca().add_artist(first_legend)\n    plt.axis('equal')\nplt.show()","ab20a115":"#The other 2 stalk related attributes using its own unique categories and the ratio of each contains class [e=Edible, p=Poisonous]\nf, axes = plt.subplots(2,1, figsize=(20,5), sharey = True) \nnum_col = ['stalk-color-above-ring', 'stalk-color-below-ring']\nfor i,col in enumerate(num_col):\n    axes[i] = plt.subplot(1,2,i+1)\n    plt.xticks(rotation=45)\n    s = sb.countplot(x=col, data = df_eda, hue='class', alpha=0.7, palette='gist_rainbow_r')\n    s.legend(loc=\"upper right\", prop={'size': 10})\n    for p in s.patches:\n        s.annotate(format(p.get_height(), '.0f'), \n        (p.get_x() + p.get_width() \/ 2., p.get_height()), \n        ha = 'center', va = 'center', \n        xytext = (0, 5), \n        textcoords = 'offset points', fontsize=8.5)\n\nplt.show()","4e35a477":"f, axes = plt.subplots(3,1, figsize=(20,5), sharey = True) \nnum_col = ['veil-color', 'ring-number', 'ring-type']\nfor i,col in enumerate(num_col):\n    axes[i] = plt.subplot(1,3,i+1)\n    s = sb.countplot(x=col, data = df_eda, hue='class', alpha=0.7, palette='Set1')\n    s.legend(loc=\"upper right\", prop={'size': 10})\n    plt.xticks(rotation=45)\n    for p in s.patches:\n        s.annotate(format(p.get_height(), '.0f'), \n        (p.get_x() + p.get_width() \/ 2., p.get_height()), \n        ha = 'center', va = 'center', \n        xytext = (0, 5), \n        textcoords = 'offset points')\n\nplt.show()","5a1803ef":"#Checking 4 out of 6 stalk-attributes using souble pie chart along with the ratio of each contains class [e=Edible, p=Poisonous] \n#Which will give class wise count & the relative ratio of their own unique values\nf, axes = plt.subplots(3,1, figsize=(20,12), sharey = True) \nnum_col = ['bruises', 'population', 'habitat']\n\nfor j,col in enumerate(num_col):\n    cols = df_eda[col].value_counts()\n    pop_size = cols.values.tolist()\n    pop_types = cols.axes[0].tolist()\n    poisonous_pop = [] #Poisonous population type list\n    edible_pop = []    #Edible population type list\n    for pop in pop_types: \n        size = len(df_eda[df_eda[col] == pop].index)\n        edibles = len(df_eda[(df_eda[col] == pop) & (df_eda['class'] == 'Edible')].index)\n        edible_pop.append(edibles)\n        poisonous_pop.append(size-edibles)\n    combine_ed_poi = []\n    for i in range(0,len(edible_pop)):\n        combine_ed_poi.append(edible_pop[i])\n        combine_ed_poi.append(poisonous_pop[i])\n\n    #Double pie chart.\n    plt.subplot(2,2,j+1)\n    plt.title(col)\n    #Outer Pie Chart\n    patches1, texts1 = plt.pie(combine_ed_poi,radius = 4.5,labels= combine_ed_poi,\n                                    colors=['#C4F6F5','#F6EEC4'], shadow=True, labeldistance= 1.1)\n    for i in range(0,len(texts1)):\n        if(i%2==0):\n            texts1[i].set_color('blue')\n        else:\n            texts1[i].set_color('red')\n    for aut in texts1:\n        aut.set_fontsize(9)\n    #Inner Pie Chart\n    patches2, texts2, autotexts2 = plt.pie(pop_size, radius = 3.5,\n            autopct='%1.2f%%', shadow=True, labeldistance= 4.2)\n    for aut in autotexts2:\n        aut.set_fontsize(10)\n        aut.set_horizontalalignment('center')\n    #Set 2 Legends to the plot.\n    first_legend   = plt.legend(patches1, ['Edible','Poisonous'], loc=\"upper left\", fontsize=10)\n    second_ledgend = plt.legend(patches2, pop_types, loc=\"best\",fontsize=8)\n    plt.gca().add_artist(first_legend)\n    plt.axis('equal')\nplt.show()","542b90de":"f, axes = plt.subplots(2,1, figsize=(20,5), sharey = True) \nnum_col = ['odor', 'spore-print-color']\nfor i,col in enumerate(num_col):\n    axes[i] = plt.subplot(1,2,i+1)\n    s = sb.countplot(x=col, data = df_eda, hue='class', alpha=0.7, palette='hot')\n    s.legend(loc=\"upper right\", prop={'size': 10})\n    plt.xticks(rotation=45)\n    for p in s.patches:\n        s.annotate(format(p.get_height(), '.0f'), \n        (p.get_x() + p.get_width() \/ 2., p.get_height()), \n        ha = 'center', va = 'center', \n        xytext = (0, 5), \n        textcoords = 'offset points')\n\nplt.show()","17709431":"#print details statistical summary\ndf.describe()","2abe90b0":"#Count Of unique values of each columns\ndf.nunique().sort_values()","d6523efc":"del df['veil-type']","cd9efb52":"# Taking the feature columns in a list\nfeat_cols = ['cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n       'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n       'stalk-surface-below-ring', 'stalk-color-above-ring',\n       'stalk-color-below-ring', 'veil-color', 'ring-number', 'ring-type',\n       'spore-print-color', 'population', 'habitat']\nlabel_encoder = LabelEncoder()\n\ndf_1 = df.apply(label_encoder.fit_transform)\n\nX = df_1[feat_cols]\nY = df_1['class']\n\nprint('X structure:',X.shape)\nprint('Y structure:',Y.shape)","de5f229b":"#correlation with top features\n\nsb.set(font_scale=1)\n\n# Set background color of corr matrix to White\nsb.set_style(\"white\")\n\n# Compute the correlation matrix without target variable\ncorr = df_1.iloc[:,1:].corr()\ntop_features = corr.index\n\n#corr = df.apply(lambda x : pd.factorize(x)[0]).corr(method='pearson', min_periods=1)\ncorr = df_1[top_features].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(corr)\n\n# Set up the matplotlib figure\nplt.figure(figsize=(17,17))\n\n# Draw the heatmap with the mask and correct aspect ratio\nsb.heatmap(corr, mask=mask, cmap=\"inferno\", annot=True, fmt = '.2g', vmax=1,vmin = -1,center=0,\n           square=True, cbar_kws={\"shrink\": 0.5})","dcac6c2e":"# Apply SelectKBest Algorithm using chi2 score function\nkbest_features = SelectKBest(score_func=chi2, k=20)\n\nord_features = kbest_features.fit(X, Y)\ndf_scores = pd.DataFrame(ord_features.scores_, columns=[\"Score\"])\ndf_columns = pd.DataFrame(X.columns)\nk_features = pd.concat([df_columns, df_scores], axis=1)\nk_features.columns=['Features','Score']\nk_features","e3e3caa2":"k_features.nlargest(15, 'Score').style.hide_index()","2aeb66f4":"# Lets try an ensemble model to find the top features in our dataset\nmodel = ExtraTreesClassifier()\nmodel.fit(X, Y)\nprint(model.feature_importances_)","e52218df":"# Finding the top ranked fetures and plotting\nranked_features = pd.Series(model.feature_importances_, index = X.columns)\nplt.figure(figsize=(12, 7))\nranked_features.nlargest(15).plot(kind='barh', color='darkgreen',  width =0.9 , linewidth = 0.1)\nplt.show()","f9c759b4":"mutual_info = mutual_info_classif(X, Y)\nmutual_data = pd.Series(mutual_info, index = X.columns)\nmutual_data.sort_values(ascending=False)","62d2a12c":"#trans = GenericUnivariateSelect(score_func=mutual_info_classif, mode='k_best', param=15)\ntrans = GenericUnivariateSelect(score_func = mutual_info_classif, mode='percentile', param = 70)\ntrans_feat = trans.fit_transform(X, Y)\ncolumns_ = df_1.iloc[:, 1:].columns[trans.get_support()].values\n\n# X_feature as tranformed top feature variables\nX_feature = pd.DataFrame(trans_feat, columns=columns_)\n\n# Y_label with only target variable\nY_label = Y\n\nX_feature.columns","d0f8bb6c":"print(\"We started with {0} features in dataset, but retained only {1} of them!\"\n      .format(X.shape[1], X_feature.shape[1]))","831531ef":"skf = KFold(n_splits=5)\nfor train_index,test_index in skf.split(X_feature, Y_label):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X_feature.loc[train_index],X_feature.loc[test_index]\n    Y_train, Y_test = Y_label.loc[train_index],Y_label.loc[test_index]","63f46b9e":"# Apply standardization on numerical features\nscale = StandardScaler()\n\n# fit on training data column\nscale.fit(X_train)\n\n# transform the training and test data\nX_train = scale.transform(X_train)\nX_test = scale.transform(X_test)\n\n#Total no. of rows in the features set\nprint(f'Total no. of Train Features data: {X_train.shape[0]}')\nprint(f'Total no. of Test Features data: {X_test.shape[0]}')\n\n#Total no. of rows in the label set\nprint(f'Total no. of Train Label data: {Y_train.shape[0]}')\nprint(f'Total no. of Test Label data: {Y_test.shape[0]}')","046e9545":"# Dataframe to store Model Performance Results\nperf_cols = ['Model','Train Accuracy','Test Accuracy','F1-Score','Recall','Precision', 'AUC']\ndf_perf = pd.DataFrame(columns = perf_cols)\n\n# DataFrame for Confusion matrix\nconf_mat_cols = ['Model','False Negatives','False Positives','True Negatives','True Positives']\ndf_cm = pd.DataFrame(columns = conf_mat_cols)\n\n# DataFrame for cross validation scores\ncv_cols = ['Model','Best_Cross_Val_Score']\ndf_cv = pd.DataFrame(columns = cv_cols)\n\n# DataFrame for Hyper Parameter\nhyp_cols = ['Model','Test Accuracy','F1-Score','Recall','Precision', 'AUC']\ndf_hyp = pd.DataFrame(columns = hyp_cols)","86022dc3":"# Function to print Confusion Matrix and Classification Report\ndef model_perf(model, Y_test, Y_predict, algorithm, df_cm, df_perf, df_hyp, ax=None, plot = True, hyp = False):\n        \n    # confusion_matrix\n    conf_mat = confusion_matrix(Y_test, Y_predict)\n    \n    # get accuracy of model\n    acc_score = accuracy_score(Y_test, Y_predict)\n    print(f\"Accuracy of {algorithm} for Test data is {acc_score*100}\\n\")\n\n    # get F1-score of model\n    f1score = f1_score(Y_test, Y_predict) \n    print(f\"F1-score of {algorithm} for Test data is {f1score*100}\\n\")\n    \n    # get the classification report\n    class_report = classification_report(Y_test, Y_predict)\n    print(f\"Classification report for {algorithm} is: \\n {class_report}\")\n    \n    # AUC Calculations - false positive rates, true positive rates and thresholds\n    fpr, tpr, thresholds = metrics.roc_curve(Y_test, Y_predict, pos_label=1)\n    \n    #area_under_curve\n    roc_auc = round(metrics.auc(fpr, tpr)*100,2)\n    print(f\"AUC for {algorithm}: {roc_auc}\\n\")\n    \n    if not hyp:\n        #Train Accuracy score\n        train_acc = round(model.score(X_train,Y_train) * 100,2)\n    \n        #Test Accuracy score\n        test_acc = round(model.score(X_test,Y_test) * 100,2)\n    \n        precision,recall,fscore,support = precision_recall_fscore_support(Y_test,Y_predict)\n    \n        #Appending into the dataframe\n        df_perf = df_perf.append({'Model' : algorithm,'Train Accuracy' : train_acc,'Test Accuracy' : test_acc,\n                              'F1-Score' : fscore[1],'Recall' : recall[1], 'Precision' : precision[1], 'AUC' : roc_auc}, ignore_index=True)\n    \n        df_cm = df_cm.append({\"Model\" : algorithm, \"True Positives\" : conf_mat[1][1], \"True Negatives\" : conf_mat[0][0],\n                          \"False Positives\" : conf_mat[0][1], \"False Negatives\" : conf_mat[1][0]}, ignore_index=True, sort=False)\n    else:\n        precision,recall,fscore,support = precision_recall_fscore_support(Y_test,Y_predict)\n        \n        df_hyp = df_hyp.append({'Model' : algorithm, 'Test Accuracy' : acc_score,\n                              'F1-Score' : fscore[1],'Recall' : recall[1], 'Precision' : precision[1], 'AUC' : roc_auc}, ignore_index=True)\n    \n        \n##########################################--PLOT---###########################################\n    if plot:\n        def conf_plot1(ax = None):\n            if ax is None:\n                fig, ax = plt.subplots()\n            # For label annotations in confusion_matrix\n            label_names = ['True -ve','False +ve','False -ve','True +ve']\n            label_counts = ['{0:0.0f}'.format(value) for value in conf_mat.flatten()]\n            labels = [f'{v1}\\n{v2}' for v1, v2 in zip(label_names,label_counts)]\n            labels = np.asarray(labels).reshape(2,2)\n\n            # Draw heatmap using confusion matrix\n            sb.heatmap(conf_mat, cmap = 'YlGnBu_r', annot=labels, fmt='')\n            ax.set_xlabel('Actual Values')\n            ax.set_ylabel('Predicted Values')\n            #ax.show()\n\n        #Line plot for ROC curve using fpr and tpr value\n        def roc_plot2(ax = None):\n            if ax is None:\n                fig, ax = plt.subplots()\n            ax.plot(fpr, tpr, color='red', label = 'AUC = %0.3f' % roc_auc)  \n            ax.set_title('Receiver Operating Characteristic (ROC)')    \n            ax.legend(loc = 'lower right')\n            ax.plot([0, 1], [0, 1],linestyle='--') #Intersection line\n            ax.set_xlabel('False Negative Rate')\n            ax.set_ylabel('True Positive Rate')\n            ax.set_xlim([0,1])\n            ax.set_ylim([0,1])\n            ax.set_xticks([i for i in np.arange(0,1.1,0.1)])\n            ax.set_yticks([i for i in np.arange(0,1.1,0.1)])\n        \n        prec, rec, thres = precision_recall_curve(Y_test, Y_predict)\n        prec, rec, thres = list(prec), list(rec), list(thres)\n        prec.pop()\n        rec.pop()\n            \n        def rec_plot3(ax = None):\n            if ax is None:\n                fig, ax = plt.subplots()\n            #Plot Precision-Recall curve\n            fig, axis = (None, ax) if ax else plt.subplots()\n            axis_twin = axis.twinx()\n\n            #Threshold vs Precision\n            sb.lineplot(x = thres, y = prec, label='Precision', ax=axis)\n            axis.set_xlabel('Threshold')\n            axis.set_ylabel('Precision')\n            axis.legend(loc='lower left')\n\n            #Threshold vs Recall\n            sb.lineplot(x = thres, y = rec, color='limegreen', label='Recall', ax=axis_twin)\n            axis_twin.set_ylabel('Recall')\n            axis_twin.set_ylim(0, 1)\n            axis_twin.legend(bbox_to_anchor=(0.32, 0.20),loc='lower right')\n\n            axis.set_xlim(0, 1)\n            axis.set_ylim(0, 1)\n            axis.set_title('Precision Vs Recall')\n        \n                    \n        fig = plt.figure(figsize = (15,4))\n        ax1 = fig.add_subplot(1,3,1)\n        conf_plot1(ax1)\n        ax2 = fig.add_subplot(1,3,2)\n        roc_plot2(ax2)\n        axis = fig.add_subplot(1,3,3)\n        rec_plot3(axis)\n    \n    \n    return df_cm, df_perf, df_hyp","c80f5954":"log_reg = LogisticRegression()\nlog_reg.fit(X_train, Y_train)\nY_predict = log_reg.predict(X_train)\nprint(\"Train Data Model Accuracy: {0:.3f}\".format(metrics.accuracy_score(Y_train, Y_predict)*100))\n\nY_predict = log_reg.predict(X_test)","00554630":"#Draw Model Performace, Confusion Matrix and Classification Report for Logistic Regression\ndf_cm,df_perf,df_hyp = model_perf(log_reg,Y_test,Y_predict,\"Logistic Regression\",df_cm,df_perf,df_hyp, plot = True)","ca393b5b":"lr_score = cross_val_score(LogisticRegression(), X_train, Y_train, cv= 20, scoring = 'accuracy')\nprint(\"Accuracy for logistic regression using Cross Validation: %0.3f (+\/- %0.3f)\" \n      % (lr_score.mean(), lr_score.std() * 2))\n\ndf_cv = df_cv.append({'Model':'LogisticRegression','Best_Cross_Val_Score':\"%0.2f (+\/- %0.2f)\" \n                      % ((lr_score.mean(), lr_score.std() * 2))},ignore_index=True, sort=False)","6b721c04":"estimator = LogisticRegression()\nparameter = {'C':[0.001,.009,0.01,.09,1,5,10,25]}\ncv = 5\nscoring = 'accuracy'\n\n# Evaluation the model performance using the function for hyperparameter with required inputs\ngrid_CV = GridSearchCV(estimator, param_grid = parameter, cv = cv, verbose = True, scoring = scoring)\n\ngrid_result = grid_CV.fit(X_train, Y_train)","e0d79747":"# Print out the best_estimator for this algorithm\nprint(grid_result.best_estimator_)\n\nY_predict = grid_result.best_estimator_.predict(X_test)\n\n#Draw Confusion Matrix and Classification Report for Logistic Regression\ndf_cm, df_perf, df_hyp = model_perf(estimator, Y_test, Y_predict, \"Logistic Regression with Hyperparameter\",df_cm, \n                            df_perf, df_hyp, plot = False, hyp = True)","3e0a26f6":"#Choosing K value\nerror_rate = []\n\nfor i in range(1,50):\n    KNN_class = KNeighborsClassifier(n_neighbors = i) \n    KNN_class.fit(X_train, Y_train)\n    Y_predict = KNN_class.predict(X_test)\n    #appending error rate that is not equal to test dataset\n    error_rate.append(np.mean(Y_predict != Y_test))\n    \n#Plotting the plot to check the K value from graph\nplt.figure(figsize = (12,6))\nplt.plot(range(1,50), error_rate, color = 'blue', linestyle = 'dashed', marker = '*', markerfacecolor = 'red', markersize = 10)\nplt.title('Error Value vs K-Value')\nplt.xlabel('K-Value')\nplt.ylabel('Error Value')   ","08557a06":"KNN_class = KNeighborsClassifier(n_neighbors =5)\n\nKNN_class.fit(X_train, Y_train)\n\nY_predict = KNN_class.predict(X_train)\n\nprint(\"Train Data Model Accuracy: {0:.4f}\".format(metrics.accuracy_score(Y_train, Y_predict)*100))\n\nY_predict = KNN_class.predict(X_test)","51106e89":"#Draw Model Performace, Confusion Matrix and Classification Report for KNN\ndf_cm, df_perf, df_hyp = model_perf(KNN_class, Y_test, Y_predict, \"KNN\", df_cm, df_perf, df_hyp, plot = True)","d613cb42":"knn_score = cross_val_score(KNeighborsClassifier(), X_train, Y_train, cv= 5,verbose = True, scoring = 'accuracy')\nprint(\"Accuracy for K-Neighbors Classifier using Cross Validation: %0.3f (+\/- %0.3f)\" \n      % (knn_score.mean(), knn_score.std() * 2))\n\ndf_cv = df_cv.append({'Model':'KNeighborsClassifier','Best_Cross_Val_Score':\"%0.2f (+\/- %0.2f)\" \n                      % ((knn_score.mean(), knn_score.std() * 2))},ignore_index=True, sort=False)","a1bb565d":"# Setting up the parameters for KNeighborsClassifier\n\nestimator = KNeighborsClassifier()\nleaf_size = list(range(1,10,2))\nn_neighbors = list(range(1,10))\nweights = ['uniform', 'distance']\np=[1,2]\nparameter = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, weights=weights, p=p)\n\ncv = 5\nscoring = 'accuracy'\n\n# Evaluation the model performance using the function for hyperparameter with required inputs\ngrid_CV = GridSearchCV(estimator, param_grid = parameter, cv = cv, verbose = True, scoring = scoring, n_jobs=10)\n\ngrid_result = grid_CV.fit(X_train, Y_train)","cbf8c7b9":"# Print out the best_estimator for this algorithm\nprint(grid_result.best_estimator_)\n\nY_predict = grid_result.best_estimator_.predict(X_test)","ad3616a4":"#Draw Confusion Matrix and Classification Report for KNeighbors Classifier\ndf_cm, df_perf, df_hyp = model_perf(estimator,Y_test,Y_predict, \"K Neighbors with Hyperparameter\", \n                            df_cm, df_perf,df_hyp, plot = False, hyp = True)","3fe5ed1e":"D_tree = DecisionTreeClassifier()\n\nD_tree.fit(X_train, Y_train)\n\nY_predict = D_tree.predict(X_train)\n\nprint(\"Train Data Model Accuracy: {0:.3f}\".format(metrics.accuracy_score(Y_train, Y_predict)*100))\n\nY_predict = D_tree.predict(X_test)","e0d75c45":"#Draw Model Performace, Confusion Matrix and Classification Report for Decision Tree\ndf_cm, df_perf, df_hyp = model_perf(D_tree, Y_test, Y_predict,\"Decision Tree\", df_cm, df_perf, df_hyp, plot = True)","172ab67d":"dt_score = cross_val_score(DecisionTreeClassifier(), X_train, Y_train, cv= 10,verbose = True, scoring = 'accuracy')\nprint(\"Accuracy for Decision Tree Classifier using Cross Validation: %0.3f (+\/- %0.3f)\" \n      % (dt_score.mean(), dt_score.std() * 2))\n\ndf_cv = df_cv.append({'Model':'DecisionTreeClassifier','Best_Cross_Val_Score':\"%0.2f (+\/- %0.2f)\" \n                      % ((dt_score.mean(), dt_score.std() * 2))},ignore_index=True, sort=False)","cecf930d":"SVM = SVC(kernel='linear')\n\nSVM.fit(X_train, Y_train)\n\nY_predict = SVM.predict(X_train)\n\nprint(\"Train Data Model Accuracy: {0:.3f}\".format(metrics.accuracy_score(Y_train, Y_predict)*100))\n\nY_predict = SVM.predict(X_test)","3c990b47":"#Draw Model Performace, Confusion Matrix and Classification Report for SVM\ndf_cm, df_perf, df_hyp = model_perf(SVM, Y_test, Y_predict, \"SVM\", df_cm, df_perf, df_hyp, plot = True)","d548f165":"svm_score = cross_val_score(SVC(kernel='linear'), X_train, Y_train, cv= 15,verbose = True, scoring = 'accuracy')\nprint(\"Accuracy for SVM using Cross Validation: %0.3f (+\/- %0.3f)\" \n      % (svm_score.mean(), svm_score.std() * 2))\n\ndf_cv = df_cv.append({'Model':'SVM','Best_Cross_Val_Score':\"%0.2f (+\/- %0.2f)\" \n                      % ((svm_score.mean(), svm_score.std() * 2))},ignore_index=True, sort=False)","704c5cca":"# Setting up the parameters for Suport Vector Machine\n\nestimator = SVC()\nkernel = ['poly', 'rbf', 'sigmoid']\nC = [50, 10, 1.0, 0.1, 0.01] #uniform(1, 10)\ngamma = ['scale','auto']\nparameter = dict(kernel=kernel, C=C, gamma=gamma)\n\ncv = 5\nscoring = 'accuracy'\n\n# Evaluation the model performance using the function for hyperparameter with required inputs\ngrid_CV = GridSearchCV(estimator, param_grid = parameter, cv = cv, verbose = True, scoring = scoring, n_jobs=10)\n\ngrid_result = grid_CV.fit(X_train, Y_train)","53451f43":"# Print out the best_estimator for this algorithm\nprint(grid_result.best_estimator_)\nY_predict = grid_result.best_estimator_.predict(X_test)","f2f2eaec":"#Draw Confusion Matrix and Classification Report for SVC\ndf_cm, df_perf, df_hyp = model_perf(estimator,Y_test,Y_predict, \"SVM with Hyperparameter\", \n                            df_cm, df_perf, df_hyp, plot = False, hyp = True)","66150e68":"GNB = GaussianNB()\n\nGNB.fit(X_train, Y_train)\n\nY_predict = GNB.predict(X_train)\n\nprint(\"Train Data Model Accuracy: {0:.3f}\".format(metrics.accuracy_score(Y_train, Y_predict)*100))\n\nY_predict = GNB.predict(X_test)","299b7b9e":"#Draw Model Performace, Confusion Matrix and Classification Report for Naive Bayes\ndf_cm, df_perf, df_hyp = model_perf(GNB, Y_test, Y_predict, \"Naive Bayes\", df_cm, df_perf, df_hyp, plot = True)","9b2f96c6":"gnb_score = cross_val_score(GaussianNB(), X_train, Y_train, cv= 10, verbose = True, scoring = 'accuracy')\nprint(\"Accuracy for GaussianNB using Cross Validation: %0.3f (+\/- %0.3f)\" \n      % (gnb_score.mean(), gnb_score.std() * 2))\n\ndf_cv = df_cv.append({'Model':'GaussianNB','Best_Cross_Val_Score':\"%0.2f (+\/- %0.2f)\" \n                      % ((gnb_score.mean(), gnb_score.std() * 2))},ignore_index=True, sort=False)","ae3de445":"XGB = XGBClassifier()\n\nXGB.fit(X_train, Y_train)\n\nY_predict = XGB.predict(X_train)\n\nprint(\"Train Data Model Accuracy: {0:.3f}\".format(metrics.accuracy_score(Y_train, Y_predict)*100))\n\nY_predict = XGB.predict(X_test)","577c6486":"#Draw Model Performace, Confusion Matrix and Classification Report for XGBoost\ndf_cm, df_perf, df_hyp = model_perf(XGB, Y_test, Y_predict, \"XGBoost\", df_cm, df_perf, df_hyp, plot = True)","87f1539f":"xgb_score = cross_val_score(XGBClassifier(), X_train, Y_train, cv = 8, verbose = True, scoring = 'accuracy')\nprint(\"Accuracy for XGB Classifier using Cross Validation: %0.3f (+\/- %0.3f)\" \n      % (xgb_score.mean(), xgb_score.std() * 2))\n\ndf_cv = df_cv.append({'Model':'XGBClassifier','Best_Cross_Val_Score':\"%0.2f (+\/- %0.2f)\" \n                      % ((xgb_score.mean(), xgb_score.std() * 2))},ignore_index=True, sort=False)","c455e963":"forest_class = RandomForestClassifier(n_estimators = 10, random_state=5, max_features=10)\n\nforest_class.fit(X_train, Y_train)\n\nY_predict = forest_class.predict(X_train)\n\nprint(\"Train Data Model Accuracy: {0:.3f}\".format(metrics.accuracy_score(Y_train, Y_predict)*100))\n\nY_predict = forest_class.predict(X_test)","8001bfca":"#Draw Model Performace, Confusion Matrix and Classification Report for Random Forest\ndf_cm,df_perf,df_hyp = model_perf(forest_class,Y_test,Y_predict, \"Random Forest\",df_cm,df_perf,df_hyp, plot = True)","f7a477df":"forest_score = cross_val_score(RandomForestClassifier(), X_train, Y_train, cv= 12, verbose = True, scoring = 'accuracy')\nprint(\"Accuracy for Random Forest Classifier using Cross Validation: %0.3f (+\/- %0.3f)\" \n      % (forest_score.mean(), forest_score.std() * 2))\n\ndf_cv = df_cv.append({'Model':'RandomForestClassifier','Best_Cross_Val_Score':\"%0.2f (+\/- %0.2f)\" \n                      % ((forest_score.mean(), forest_score.std() * 2))},ignore_index=True, sort=False)","677baa3a":"print('Confusion Matrix of all the models:')\ndf_cm","08bb2611":"print('Performance metrics of all the models:')\ndf_perf.round(2)","268ad085":"print('Cross Validation Scores of all the models:')\ndf_cv","8f1d3558":"print('Performance metrics of all the models with Hyperparameter:')\ndf_hyp.round(2)","d420cb0b":"# pip install eli5\n#pip install shap","6c9d97aa":"# Using train test split for ELI5 model interpretations\nX_train_ex,X_test_ex,Y_train_ex,Y_test_ex=train_test_split(X_feature,Y_label,test_size=0.20,\n                                                               random_state=42, shuffle=True)","f15a70da":"# Prediction explainer is done with XGB Classifier Model\n\n# Fitting teh model with splitted train and test data\nXGB.fit(X_train_ex, Y_train_ex)\n\n# Predicting the data with train data\nY_predict_ex = XGB.predict(X_train_ex)\n\n# Converting into Series to show the prediction \nY_predict_ex = pd.Series(Y_predict_ex)","9250e47a":"# Importing model prediction package\nimport eli5, shap","4564fc48":"# Using tree explainer fom shap\nexplainer = shap.TreeExplainer(XGB)\n\n#Trying with sample test dat\nshap_values = explainer.shap_values(X_test_ex)\nexpected_value = explainer.expected_value\n#shap_interaction_values = explainer.shap_interaction_values(features)\n\nselect = range(20)\nfeatures = X_test_ex.iloc[select]\nfeatures_display = X_feature.loc[features.index]\n\nprint('Expected Value:', explainer.expected_value)\n\n#Printing the output values in a dataframe format\npd.DataFrame(shap_values)","51393132":"# Summary chart using density scatter plot of SHAP values \nshap.summary_plot(shap_values,X_test_ex, plot_type=\"violin\")","d8bb7a26":"p = 0.5  # Probability 0.5\nnew_base_value = np.log(p \/ (1 - p))  # the logit function\nshap.decision_plot(expected_value, shap_values, features_display, link='logit', new_base_value=new_base_value)","f337a540":"# Using eli5 show weight method to get the average gain of the feature when it is used in ensemble models.\neli5.show_weights(XGB.get_booster())","184d5c02":"# Checking for random index location for model prediction, for Y_label =1)\nprint('Actual Label:', Y_test_ex.iloc[1])\nprint('Predicted Label:', Y_predict_ex.iloc[1])\n\neli5.show_prediction(XGB.get_booster(), X_test_ex.iloc[1], show_feature_values=True)","8e9a7641":"# Checking for random index location for model prediction, for Y_label =0)\nprint('Actual Label:', Y_test_ex.iloc[3])\nprint('Predicted Label:', Y_predict_ex.iloc[3])\n\neli5.show_prediction(XGB.get_booster(), X_test_ex.iloc[3], show_feature_values=True)","97098891":"end = time. time()\nmin = (end - start)\/\/60\nprint(f'Total time taken to complete the execution :{min} minute(s)')","3a848696":"-------Self Predict-------","eb07994b":"### [6.4] Performance Matrix [Model With Hyperparameter]","10efe9cd":"### [3.1] Irrelevant Features","fe479913":"From the above result, we can conclude that the most influential features for edible mushrooms are <b>gill-color, odour, population, gill-size, spore-print-color, habitat.<\/b>","54569e06":"> <b>class<\/b> can be used as target variable here.","697c32a1":"#### [5.2.1] Pre-Processing & Development","2c278094":"-------Self Predict-------","a372eae5":"Lets do a confusion matrix analysis analisys on this Algorithm and collect the Performance Data into a DataFrame which can be analysed all together at the end.","4490abd4":"Lets take each features and check how it related to target variable (class).","a90b663b":"Lets do a confusion matrix analysis analisys on this Algorithm and collect the Performance Data into a DataFrame ","6d08592f":"From the above result, we can conclude that the most influential features for edible mushrooms are <b>gill-size, gill-color, odour, spore-print-color, habitat.<\/b>","5e8e742c":"-------Self Predict-------","75870ae5":"![mushroom%20diagram.png](attachment:mushroom%20diagram.png)","132fbd47":"#### [1.2.1] Duplicate Values","1fdd3130":"-------Self Predict-------","f3a7eae2":"#### [5.1.2] Validation","5e9ae3a5":"### [5.2] K Neighbors Classifier","0b5a9a59":"Shap is a unified approach to explain the output of any machine learning model which assigns each feature an importance value for a particular prediction.","1f45f199":"#### [3.5.1]  ExtraTressClassifier","750c0ab0":"###  [1.1] Attribute Information","1995de46":"### [6.3] Cross Validation Scores","5c0ae812":"#### [5.6.1] Pre-Processing & Development","a9f582f0":"As we have got less accuracy let's develop model using hyperparameter tuning.","fdfa4f3b":"#### [5.4.3] Model Using Hyperparameter","eb31cf33":"It estimates mutual information for a discrete target variable between two random variables which is a non-negative value, \nwhich measures the mutual information between a matrix containing a set of feature vectors and the target.\n\nIt is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.","4649eb9d":"-------Using Cross Validation-------","3dd71c4a":"## [4] Data Sampling","0aebec1d":"SHAP values are relative to the model\u2019s expected value like a linear model\u2019s effects are relative to the intercept. * The y-axis lists the model\u2019s features. By default, the features are ordered by descending importance. \nThe importance is calculated over the observations plotted. \n\nMoving from the bottom of the plot to the top, SHAP values for each feature are added to the model\u2019s base value. This shows how each feature contributes to the overall prediction.","c7564c62":"#### Let's find the relations for the Veil Features","939b4b14":"-------Using Cross Validation-------","aa257744":"Function for Model Performance, Confusion Matrix and Classification Report with Plots","16b529c1":"### [3.3] Correlation Analysis","86274e7b":"#### [5.5.2] Validation","f2623bb8":"#### [5.1.1] Pre-Processing & Development","562cfbcd":"### [7.1] Using Shap","806ba61a":"Checking the descriptive statistics,to determine useful feature.","c9c9104d":"From the above statistics, the column <b>veil-type<\/b> has only one unique value in whole data, so it is better to remove it as it wont impact in any performance.","8a291c57":"#### [5.3.2] Validation","95765cba":"Select KBest removes all but the  highest scoring features.\n\nSelect Percentile removes all but a user-specified highest scoring percentage of features.","dd2c2ebe":"#### [3.5.3]  GenericUnivariateSelect","9cc075d8":"### [7.2] Using Eli5","6fff20a2":"#### [5.2.3] Model Using Hyperparameter","a665f5e5":"#### [5.2.2] Validation","5dd5d0a3":"From the above plot, various trial and error values has been used between the range 0-10, finally <b>K value 5<\/b> may give better results.","fcb8dd5a":"## [6] Result","49476152":"#### [1.2.2] Null Values","76120b1f":"This technique gives you a score for each feature of your data,the higher the score more relevant it is.","370222f0":"#### [5.4.1] Pre-Processing & Development","f3251426":"## [1] Data Preparation","006bca76":"Now the data can be used to find relation between the features and helps in deciding whether it is edible or poisonous ones.","22f54392":"Lets do a confusion matrix analysis analisys on this Algorithm and collect the Performance Data into a DataFrame\n","3b6f15fc":"XGBoost model enables us to view feature importances based on Weights, Gain, Coverage.","13fb08b6":"#### [5.6.2] Validation","8f00948c":"## [2] Exploratory Data-analysis","c8f5a943":"The Prediction using eli5 can be defined as the sum of the feature contributions and  the BIAS (i.e. the mean given by the topmost region that covers the entire training set)","a1fd4ca2":"#### Let's find the relations for the Cap Features","bc10cb0e":"### [5.6] XGBoost","01901e19":"## [7] Model Interpretation","0e8f168b":"For <b>Bruises<\/b>, The mushroom is edible if it has has bruises, and vice versa.\n\nIn terms of <b>Odor<\/b>, It's edible if the mushroom has no smell, or the smell of Almond or Anise. If the mushroom has the smell of Pungent, foul, creosote, fishy, spicy, or musty, then it's poisonous.\n\nFor <b>Spore Print Color<\/b>,If the color is black or brown, then it's most likely to be edible. If the color is chocoloate or white, then it's most likely to be poisonous.\n\nFor the <b>colors<\/b> purple, orange, yellow, buff, or green, we don't have enough records to build a verdit.\n\nFrom <b>Population<\/b>, it is clear that if it's numerous or abundant, then it's edible nad if it's scattered or solitary, then it's  likely to be edible. If it is several, then it's poisonous.\n\nFor <b>Habitat<\/b> It's most likely to be edible, if the habitat of the mushroom is grasses, woods, or waste. It is poisonous, if the habitat of the mushroom is paths, leaves, or urban.","72494c93":"## [5] Modeling","f3890a8c":"-------Using Cross Validation-------","1558a979":"### [6.1] Confusion Matrix","fcf5ba31":"As we have got less accuracy let's develop model using hyperparameter tuning.","20b5a156":"For <b>Stalk Root<\/b>, if the root is equal, club, or rooted, then it is edible.\n\nFor <b>Stalk Surface<\/b>, Mushrooms are edible or safe if it has smooth stalk surface then other parameters.\n\nFor <b>Stalk Color<\/b>, edible color for stalk is white, gray and orange.The most poisonous colors is pink, buff, and brown.","3aeb9bd6":"## [3] Feature Selection","d7b7a0a7":"-------Using Cross Validation-------","afad3c52":"### [5.7] Random Forest Classifier","f95d5624":"#### Let's find the relations for the Stalk Features","57d33fb2":"Using eli5, showing weights for each feature depicting how influential it might have been in contributing towards the final prediction across all decision trees.","da1b3fd3":"### [5.4] Support Vector Machines (SVM)","d89b49ab":"-**############################################################*-------*###############################################################**-","6906eea9":"#### [5.1.3] Model Using Hyperparameter","8aaa4958":"### [3.4] SelectKBest Feature","bf03cadc":"Column <b>class<\/b> has binary value p and e with dtype=object which denotes that whether the Mushroom is edible(e) or poisonous(p).","7a3ad1bb":"It works by selecting the best features based on univariate statistical tests and allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator.","6d28e583":"#### [5.7.1] Pre-Processing & Development","7499e369":"-------Predicting when a Mushroom is  edible (e\/1)------","6865aa17":"#### [5.4.2] Validation","32170d0a":"-------Self Predict-------","062a118d":"-------Self Predict-------","6cc250df":"Lets do a confusion matrix analysis analisys on this Algorithm and collect the Performance Data into a DataFrame","ed443abd":"Since the collected data shows the mushroom features, let me make them crisp and clear for you, to be able to understand the big picture.\nWe will start by an image, showing the most important features for mushroom:","deb292be":"Logistic Regression has much better accuracy for Training and Testing datasets. The difference is also not that high.","bca1e5c1":"For <b>Cap Shape<\/b>, most bell shaped mushrooms are edible and all the sunken mushrooms are edible, since our data has only fewer samples.In knobbed mushroom, lot of them looks poisonous and no records for Conical mushrooms.\n\nFor <b>Cap Surface<\/b>, Smooth and Scaly surface are looks more poisonous than the edible ones.\nMost of Fibrous surface mushrooms are safe and no records for Grooves mushrooms.\n\nFor <b>Cap Color<\/b>, mushrooms with brown, white and gray cap color looks more safe.\nThe Yellow, Red and buff cap colors are poisonous among others in the dataset and due to less amount of data cinnamon, Purple and Green looks like edible ones.","bd93432d":"### [1.2] Data Cleaning","9ded4c22":"Let us examine individual data-point predictions with the help of eli5 and shap explainer. The categorical_features parameter lets it know which features are categorical (in this case, all of them). The categorical names parameter gives a string representation of each categorical feature's numerical value, as we saw before.","4e29287e":"#### [3.5.2]  Mutual_info_classif","d5dd31bc":"#### [5.3.1] Pre-Processing & Development","bb5bfd43":"-------Self Predict-------","b2f624e1":"For <b>Gill Attachment<\/b>, if it is Attached, then safe, but due to less amount of data it's hard to predict.\nIf it's Free, then it is difficult to tell as it doesn't add up much information.\n\nFor <b>Gill Spacing<\/b>, if it's Crowded, then it's mostly edible.\nIt would be much safer, if you found mushroom with Close Gill Spacing, and didn't eat it.\n\nFor <b>Gill Size<\/b>, if mushroom is Narrow, then it's poisonous and if it is Broad, then mostly safe to eat.\n\nFor <b>Gill Color<\/b>, the most poisonous color for mushroom is Buff which has to be avoided. The most safer color is Brown, White, Black, and Purple which are likely to be edible. Remaining colors are hard to conclude as Orange, Red, and Yellow have only very few data.","a4c005b8":"Here we use TreeExplainer SHAP implementation integrated into XGBoost and trying to estimate with the sample test data.","ab149eaa":"-------Using Cross Validation-------","a1664fcf":"### [3.2] Label Encoding","659bbf69":"Plotting the summary chart using  density scatter plot of SHAP values for each feature to identify how much impact each feature has on the model output for individuals in the test set.","d2b807d0":"KNN has the best performance in terms of Testing Accuracy, AUC and Precision compared to logistic regression.\n\nLogistic Regression seems to have a good balance of Performance. But the AUC is slightly on the lower side. More data could actually boost Logistic Regression performance.","8c9c09d6":"-------Predicting when a Mushroom is poisonous (p\/0)------","4f705a6b":"### [4.1] KFold Train\/Test Split","f5b8773e":"### [6.2] Performance Matrix","7923b422":"Using decision plot for more details about features","27c54ac7":"As we have got less accuracy let's develop model using hyperparameter tuning.","e3b2edb1":"#### Prediction with XGBoost model","6bc0fb02":"Lets do a confusion matrix analysis analisys on this Algorithm and collect the Performance Data into a DataFrame which can be analysed all together at the end.","f637620a":"There is no null and NAN values in the dataset","2fe5977b":"Data set looks like a balanced one with equal amount of Mushroom Classes.","e6872e32":"The <b>veil-color<\/b> feature doesn't tell us much information as orange and brown are edible, but has only a fewer records and no values for yellow in the dataset.\n\nFor <b>ring-number<\/b>, if the mushroom has two rings, the it is edible one and vice versa.\n\nIn <b>ring-type<\/b>, if the mushroom has ring-type as pendant or flaring, then it's good one where as if the ring type is evanescent or large, then it's likely to be poisonous one.","37dcd840":"### [4.2] Feature Scaling","adb7d9b7":"### [5.5] Naive Bayes","5564a5ae":"With very less amount of data, the dataset looks like imbalanced, so it is better to consider recall, precision and Fscore instead of accuracy.\nKNN is usually a good choice for Medical Classification problems. But with more Data, Logical Regression performance can be improved.\n\nIn Standard models, <b>Random Forest<\/b>, <b>Decision Tree<\/b>, <b>XG Boost<\/b> has overall Great Performance on accuracy, fscore, Recall and Precision. It has the highest AUC amongst all other algorithms and TP and TN counts are higher compared to others.\n\nAfter Hyperparameter Tuning, <b>Logistic Regression, KNN and SVM <\/b>has shown very good accuracy score.","31212d94":"Lets do a confusion matrix analysis analisys on this Algorithm and collect the Performance Data into a DataFrame ","f638c9bd":"### [5.3] Decision Tree Classifier","edef110a":"-------Using Cross Validation-------","257e7666":"### [3.5] Important Features ","cd5659a7":"Dataframe to store Model Performance Results","d634ef5f":"#### [5.7.2] Validation","f795e32b":"#### [5.5.1] Pre-Processing & Development","57f20ebc":"### [5.1] Logistic Regression ","d0a5f07c":"### [2.1] Visulization using Dummy DataFrame","706c0c24":"Lets do a confusion matrix analysis analisys on this Algorithm and collect the Performance Data into a DataFrame ","54819bb4":"#### Let's find the relations for the Gill Features","727824e2":"KNN Algorithm seems to have a good Accuracy for Training and Testing Dataset","14ada488":"Finding right value for K using error rate","50bab8e2":"Random Forest has great Performance on accuracy, fit and Recall. It has the highest AUC.","8919429d":"-------Using Cross Validation-------","87cec22e":"It is noted that the <b>odor, gill-size, gill-color, spore-print-color, population,gill spacing, and stalk root <\/b> has greater impact in \npredicting the mushroom value whether it is edible or poisonous.\n\nIt is clear that large values of <b>gill size, odor, gill-color, spore-print-color <\/b> do indeed increase the prediction, and vice versa. ","4f5af16c":"The dataset is full of string values. All these strings has to be converted to numerical values inorder to make the machine learning algorithm to perform better. Thus we perform label encoding on the data.","4ec0236d":"# Mushroom Classification","5a98db85":"<b>Origin:<\/b> Mushroom records drawn from The Audubon Society Field Guide to North American Mushrooms (1981). G. H. Lincoff (Pres.), New York: Alfred A. Knopf.\n\nMushrooms described in terms of physical characteristics, Classification: <b>Poisonous or Edible<\/b>\n\nThe dataset was obtained from UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Mushroom\n\nBelow Mushroom diagram from https:\/\/jb004.k12.sd.us\/my%20website%20info\/BIOLOGY%202\/FUNGUS%20KINGDOM\/Mushroom%20Diagram%202.htm","c60e71b9":"#### Let's find the relations for the Misc Features"}}