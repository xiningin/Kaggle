{"cell_type":{"581a2ed9":"code","67943a46":"code","a032c6e7":"code","e36e1456":"code","d762427a":"code","867028a2":"code","9ba8b127":"code","375cf54a":"code","1833fe14":"code","090cb010":"code","f77d16e4":"code","cc7adf7c":"code","4c34aeac":"code","baf5ff52":"code","4f4eb121":"code","6fae2969":"code","0498db34":"code","eefe98a1":"code","c45d585e":"code","44bce7d4":"code","7fbb71b4":"code","5262dd4f":"markdown","bbec8e50":"markdown","fe3c8994":"markdown","e7a11b8b":"markdown","8e9b9aad":"markdown","c26af09d":"markdown","d762f00f":"markdown","239382bc":"markdown","75aade13":"markdown","03ab8059":"markdown","deea1a3c":"markdown","69f9f721":"markdown","6a240937":"markdown","a14863d7":"markdown","1e7f937b":"markdown","882e6f52":"markdown","a55fa2a4":"markdown","2c9d576f":"markdown","ef1dd614":"markdown","9d20d9db":"markdown","b9e528b9":"markdown","57929e5d":"markdown","c058274d":"markdown","8437d103":"markdown","280b934a":"markdown","8f792126":"markdown"},"source":{"581a2ed9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67943a46":"#Importing Libraries\nimport tensorflow\nimport h5py\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom tensorflow.keras.layers import Dense, Reshape, BatchNormalization, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import regularizers, optimizers\nimport math\n\n%matplotlib inline\ntensorflow.__version__","a032c6e7":"# open the file as readonly\nh5f = h5py.File('\/kaggle\/input\/street-view-house-nos-h5-file\/SVHN_single_grey1.h5','r')\nh5f.keys()","e36e1456":"# load the already splited train, validation and test data\nX_train = h5f['X_train'][:]\ny_train = h5f['y_train'][:]\n\nX_val = h5f['X_val'][:]\ny_val = h5f['y_val'][:]\n\nX_test = h5f['X_test'][:]\ny_test = h5f['y_test'][:]\n","d762427a":"print(f'Size of X_train is {X_train.shape}')\nprint(f'Size of y_train is {y_train.shape}\\n')\n\nprint(f'Size of X_val is {X_val.shape}')\nprint(f'Size of y_val is {y_val.shape}\\n')\n\nprint(f'Size of X_test is {X_test.shape}')\nprint(f'Size of y_test is {y_test.shape}')","867028a2":"plt.imshow(X_train[1],cmap='gray')\nprint(f'Label for the image is {y_train[1]}')","9ba8b127":"# visualizing the first 10 images in the dataset and their labels\n%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 1))\nfor i in range(10):\n    plt.subplot(1, 10, i+1)\n    plt.imshow(X_train[i], cmap=\"gray\")\n    plt.axis('off')\n    print('label for each of the below image: %s' % ((y_train[i])))\nplt.show()\n","375cf54a":"# RESHAPE 2D - 32*32 into 1D - 1024\nX_train = X_train.reshape(42000, 32*32)\nX_val= X_val.reshape(X_val.shape[0], 32*32)\nX_test = X_test.reshape(X_test.shape[0],32*32)\n\nprint(f'Shape of X_train is {X_train.shape}')\nprint(f'Shape of X_val is {X_val.shape}')\nprint(f'Shape of X_test is {X_test.shape}')","1833fe14":"print(f'Min value for Train = {X_train.min()},Validation ={X_val.min()}, Test = {X_test.min()} ')\nprint(f'Min value for Train = {X_train.max()},Validation ={X_val.max()}, Test = {X_test.max()} ')","090cb010":"print('Before Normalization')\nprint(f'Min value is {X_train.min()}')\nprint(f'Max value is {X_train.max()}\\n')\nmaxVal=X_train.max()\nX_train = X_train\/maxVal\nX_val= X_val\/maxVal\nX_test = X_test\/maxVal\n\nprint('After Normalization')\nprint(f'Min value is {X_train.min()}')\nprint(f'Max value is {X_train.max()}')","f77d16e4":"print(f'Sample value before one hot encode {y_train[0]}\\n')\ny_train = tensorflow.keras.utils.to_categorical(y_train,num_classes=10)\ny_val= tensorflow.keras.utils.to_categorical(y_val,num_classes=10)\ny_test= tensorflow.keras.utils.to_categorical(y_test, num_classes=10)\nprint(f'Sample value after one hot encode {y_train[0]}')","cc7adf7c":"#cross check if we did all right\nplt.figure(figsize=(10,1))\nfor i in range(10):\n    plt.subplot(1,10,i+1)\n    plt.imshow(X_train[i].reshape(32,32),cmap='gray')\n    print(f'Label for image at index {i+1} is {np.argmax(y_train[0:10][i])}')","4c34aeac":"def model(iterations, lr, Lambda, verb=0, eval_test=False):\n    scores=[]\n    learning_rate=lr\n    hidden_nodes=256\n    output_nodes=10\n    iterations=iterations\n    # For early stopping of model.\n    callbacks=tensorflow.keras.callbacks.EarlyStopping(monitor='loss', patience=5)\n    #model\n    model = Sequential()\n    model.add(Dense(500, input_shape=(1024,), activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(hidden_nodes,activation='relu'))\n    model.add(Dense(hidden_nodes,activation='relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(hidden_nodes,activation='relu'))\n    model.add(Dense(hidden_nodes,activation='relu'))\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(hidden_nodes,activation='relu'))\n    model.add(Dense(hidden_nodes,activation='relu'))\n    model.add(Dense(output_nodes, activation='softmax', kernel_regularizer=regularizers.l2(Lambda)))\n    # adam optmizer with custom learning rate\n    adam= optimizers.Adam(lr=learning_rate)\n    #Compile the model\n    model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n    \n    #Fit the model\n    model.fit(X_train,y_train, validation_data=(X_val,y_val),epochs=iterations,\n              batch_size=500, verbose=verb, callbacks=[callbacks])\n    \n    if eval_test == True:\n        score = model.evaluate(X_train,y_train, verbose=0)\n        scores.append(score)\n        score = model.evaluate(X_val,y_val, verbose=0)\n        scores.append(score)\n        score = model.evaluate(X_test,y_test, verbose=0)\n        scores.append(score)\n        return scores\n    else:\n        score = model.evaluate(X_val,y_val, verbose=(verb+1)%2)\n        return score","baf5ff52":"iterations = 1\nlr=0.0001\nLambda=0\nscore=model(iterations, lr, Lambda)\nprint(f'\\nLoss is {score[0]} and Accuracy is {score[1]}')","4f4eb121":"iterations = 1\nlr=1e3\nLambda=0\nscore=model(iterations, lr, Lambda)\nprint(f'\\nLoss is {score[0]} and Accuracy is {score[1]}')","6fae2969":"#let's narrow down our search a bit\niterations = 50\nlr=1e-4\nLambda=1e-7\nscore=model(iterations, lr, Lambda)\nprint(f'Loss is {score[0]} and Accuracy is {score[1]}')","0498db34":"iterations = 10\nlr=2\nLambda=1e-2\nscore=model(iterations, lr, Lambda)\nprint(f'Loss is {score[0]} and Accuracy is {score[1]}')","eefe98a1":"import math\nresults =[]\nfor i in range(10):\n    lr=math.pow(10, np.random.uniform(-4.0,1.0))\n    Lambda = math.pow(10, np.random.uniform(-7,-2))\n    iterations = 30\n    score=model(iterations, lr, Lambda)\n    result=f'Loss is {score[0]} and Accuracy is {score[1]} with learing rate {lr} and Lambda {Lambda}\\n'\n    print(result)\n    results.append(result)","c45d585e":"import math\nresults =[]\nfor i in range(20):\n    lr=math.pow(10, np.random.uniform(-4.0,-2.0))\n    Lambda = math.pow(10, np.random.uniform(-5,-3))\n    iterations = 50\n    score=model(iterations, lr, Lambda)\n    result=f'Loss is {score[0]} and Accuracy is {score[1]} with learing rate {lr} and Lambda {Lambda}\\n'\n    print(result)\n    results.append([result,[score[0],score[1],lr,Lambda]])","44bce7d4":"lr= 0.001798\nLambda= 0.000878\niterations = 100 #Since we have used early stopping so it will halt \neval_test= True\nscores = model(iterations, lr, Lambda,verb=0, eval_test=True)","7fbb71b4":"print(f'Training Dataset Loss is {scores[0][0]} Accuracy is {scores[0][1]}\\n')\nprint(f'Validation Dataset Loss is {scores[1][0]} Accuracy is {scores[1][1]}\\n')\nprint(f'Test Dataset Loss is {scores[2][0]} Accuracy is {scores[2][1]}\\n')","5262dd4f":"#### Trying using random value in a certain range","bbec8e50":"#### Reading Data","fe3c8994":"### <font color='red'>Step 2 <\/font>Reshape and normalize the train and test features","e7a11b8b":"* Normalize the values","8e9b9aad":"#### Lets Narrow it down more","c26af09d":"## <font color='Green'>Street View Housing Number Digit Recognition Using Neural Net<\/font>\n\n## The Problem Description:\nRecognizing multi-digit numbers in photographs captured at street level is an important component of modernday map making. A classic example of a corpus of such street-level photographs is Google\u2019s Street View\nimagery comprised of hundreds of millions of geo-located 360-degree panoramic images. The ability to\nautomatically transcribe an address number from a geo-located patch of pixels and associate the transcribed\nnumber with a known street address helps pinpoint, with a high degree of accuracy, the location of the building\nit represents. More broadly, recognizing numbers in photographs is a problem of interest to the optical\ncharacter recognition community. While OCR on constrained domains like document processing is well\nstudied, arbitrary multi-character text recognition in photographs is still highly challenging. This difficulty arises\ndue to the wide variability in the visual appearance of text in the wild on account of a large range of fonts,\ncolours, styles, orientations, and character arrangements. The recognition problem is further complicated by\nenvironmental factors such as lighting, shadows, specularities, and occlusions as well as by image acquisition\nfactors such as resolution, motion, and focus blurs. In this project, we will use the dataset with images centred\naround a single digit (many of the images do contain some distractors at the sides). Although we are taking a\nsample of the data which is simpler, it is more complex than MNIST because of the distractors.\n## Dataset\nSVHN is a real-world image dataset for developing machine learning and object recognition algorithms with the\nminimal requirement on data formatting but comes from a significantly harder, unsolved, real-world problem\n(recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google\nStreet View images.","d762f00f":"- Loss is pretty resonable and so is the accuracy which is probability of being a certain digit among 10 classes is 10%(equal for all classes).\n- Loss is here calculated via the softmax and crossentropy which is basically -y.ln(0.10) .","239382bc":"### Let's Peek Data","75aade13":"* thus we can conclude there are lots of noise in data images","03ab8059":"- Loss is too high as Learning rate=2 is too high for model.","deea1a3c":"### <font color='red'>Step 4 <\/font>Model Creations\n### Define the model architecture using TensorFlow with a flatten layer followed by dense layers with activation as ReLu and softmax","69f9f721":"- We can clearly see that we have a partial view of digit 3, that is Noise","6a240937":"#### Importing Libraries","a14863d7":"#### Lets try Increasing learning Rate, that should increase our loss, this will act as cross validation","1e7f937b":"- The training dataset(X_train) has 42k records on which we can train upon of matrix size of 32x32 i.e. image size of 32x32.\n- The test dataset(X_test) has 18k records each record being 32x32 in size.\n- y_train, y_test contain label for the given image matrix.\n","882e6f52":"### <font color='red'>Step 1 <\/font>Import neccessary libaries and read the data from the h5py file and understand the train\/test splits","a55fa2a4":"* With random hit, we get 80% accuracy that seems quite right, let's try few more experiments","2c9d576f":"* Loss Exploded thus a great sign that we are moving in correct direction\n* So our Model is behaving right","ef1dd614":"* Re-shape","9d20d9db":"### <font color='red'>Step 4 <\/font>Hyperparameter Tunning\n- We can start with coarse values and then tune the model with fine values.","b9e528b9":"* We get good results in range \n1. Learning Rate = \"0.008 to 0.002\" \n2. Lambda = 1e-3 to 1e-5","57929e5d":"### THE END!","c058274d":"### Lets Check Few more Images","8437d103":"#### Values lr = 0.001798 and Lambda = 0.000878 seem a good fit. Let's train this model deep for that.[](http:\/\/)","280b934a":"### <font color='red'>Step 3 <\/font>One hot encode the labels for train and test data ","8f792126":"#### Let us try with very low learning rate and zero regularization."}}