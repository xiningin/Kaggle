{"cell_type":{"96c7efe6":"code","afb5d610":"code","9168e1f6":"code","8111993c":"code","649f8a18":"code","b0455b8c":"code","3c434126":"code","b00754ed":"code","826c37de":"code","543b67f3":"code","75f57179":"code","b91a7e79":"code","8327ff87":"code","3b92e1d5":"code","fa442b30":"code","72145915":"code","b68c149a":"code","c19c6dcd":"code","b7791085":"code","8f5777a0":"code","dceef970":"code","1d6b0a41":"code","208ca455":"code","432fd7c7":"code","f5c84b35":"code","dfc3dd47":"code","658564a6":"code","6195ae05":"code","ae46900f":"code","cd0cf57b":"code","42fc5b34":"code","07220b21":"code","5130f5c2":"code","f0549e35":"code","2d6c6c85":"code","4fd8d785":"code","e1edf2f3":"code","b925766e":"code","8556ac41":"code","28beb8a8":"code","e5ba33e7":"code","9d9807c2":"code","e4e46340":"code","e7d24867":"code","082656ff":"code","65dd6e1a":"code","56e19c5d":"code","fbc3596b":"code","3e3e9c2b":"code","53565054":"code","296e107d":"code","3e215634":"code","ad36a2a7":"code","0d637634":"code","6f7da75c":"code","b7e5931a":"code","090fa798":"code","cd6dd24f":"code","d7f6d199":"code","6706e1d7":"code","4b8cd77f":"code","12061f06":"code","4f7aa49d":"code","d9d592a0":"code","c53bc964":"code","46530e24":"code","4577ceb3":"code","f35f8f67":"code","258145f8":"code","32e12d52":"code","c3f22db5":"code","25bac52f":"code","11faa7bd":"code","6dcca41e":"code","4988ff6c":"code","9b44cda6":"code","58ffa4ed":"code","16bc9a27":"code","99566480":"code","36704b06":"code","c2c9fc98":"code","41138452":"code","231caa48":"code","a36453f3":"code","d45ef276":"code","0de0b50d":"code","a0b80c90":"code","5b8b36f4":"code","e080b8fe":"code","09b4a1a4":"code","08d0055a":"code","185c040c":"code","f0b5917b":"code","2ba0699b":"code","1a458f9c":"code","64ccdb9a":"code","5090969b":"code","58b26d81":"code","85c1cd3c":"code","c1869f3c":"code","9f5941e4":"code","b617fcfe":"code","c9f9301b":"code","c3df4547":"code","8dbc6dc7":"code","daf89f6f":"code","ce6eca6e":"code","fd62810a":"code","92fa6d6a":"code","a7c8ab65":"code","afda29a1":"code","6e8f22a8":"code","e1c40158":"code","67c995d7":"code","9be27663":"code","848408af":"code","4c309415":"code","6c1aa272":"code","75c4b462":"code","378bf91a":"code","022e02e4":"code","994f3f1d":"code","68d7e96f":"code","c4566432":"code","be150395":"code","201f1d65":"code","4bdfbb23":"code","f20bfd16":"code","d94806e9":"code","acfa689f":"code","440fe21b":"code","20cfd442":"code","a6ed3aa9":"code","ecc91128":"code","d24eae76":"code","7f2059d9":"code","94a9af83":"code","3967fb63":"code","0c6f6aee":"code","0203a8fe":"code","161d9b57":"code","a94a8744":"code","3d0af4e9":"code","94a1c297":"code","be0e9e1d":"code","d626df33":"code","c6fc4feb":"code","46a6a330":"code","e72cc7e2":"code","0af78224":"code","cc568c72":"code","40408d39":"code","8c463bf6":"code","b3243eff":"code","7d0236e3":"code","e69c9db2":"code","dc7f5989":"code","4924366e":"code","5c918fcb":"code","643d13e1":"code","45d611f5":"code","d599fa1d":"code","c4e7ce6d":"code","c7d15347":"code","f56766a5":"code","bde91ba0":"code","aca6d9f7":"code","d6e7f802":"code","01eb1e8b":"code","62772f93":"code","9ad66da9":"code","2f472e03":"code","095a48e0":"code","1abdb8ce":"code","f8667500":"code","41cfc811":"code","9023c0bd":"code","196ec0ab":"code","240cb1f8":"code","6feb1879":"code","c2dce66b":"code","7423f13a":"code","1995f53b":"code","4bd713a3":"code","84b94aa9":"code","27862e5d":"code","2def9596":"code","1aca80c1":"code","d7ac16fd":"code","81ba26f6":"code","e7ab722f":"code","daad6794":"code","879e589c":"code","f4c3603a":"code","2a70f19e":"code","b050ba18":"code","f8120aab":"code","0e4094ef":"code","f7901892":"code","1032b2d7":"code","5fdec8ba":"code","76aaf025":"code","1c4863ad":"code","a03363ce":"code","832a4eda":"code","e16b9e21":"code","4a7506e5":"code","e0bf2a42":"code","4d7126c0":"code","612fae26":"code","9acb9d98":"code","52a8b84b":"code","0dfffae4":"code","c123110c":"code","6a919e9a":"code","3423eaf8":"code","a7bc9970":"code","b04a4208":"code","448712de":"code","ba69fa3f":"code","044b91e0":"code","8f63f46e":"code","5333eeb5":"code","c5adc501":"code","8bef0fcd":"code","32c1abb6":"code","cc648175":"code","3267d985":"code","53b41adf":"code","18105e42":"code","c6d89481":"code","1441200d":"code","678792b9":"code","20d829aa":"code","3e9fefb1":"code","953d5a41":"code","46d9b29d":"code","b94ce9bb":"code","e4690c10":"code","126706a2":"code","a718071b":"code","312124d5":"code","caf21f9f":"code","f8189184":"code","c07c21ad":"code","3247d0de":"code","8b39af7e":"code","757b3ce8":"code","6e108771":"markdown","1e2d5be5":"markdown","cf119449":"markdown","3d700afc":"markdown","f6cfed24":"markdown","be2afd38":"markdown","af10ecbb":"markdown","c47c0f03":"markdown","1268593e":"markdown","0a7e564e":"markdown","44aa03df":"markdown","600252f9":"markdown","475416b3":"markdown","fb72ae9f":"markdown","07892309":"markdown","7a4e4917":"markdown","478358ba":"markdown","7dac9654":"markdown","72215d03":"markdown","8a2669e0":"markdown","feccd4c1":"markdown","9efc60b4":"markdown","ba5a6c13":"markdown","8adca974":"markdown","999e5c0c":"markdown","6ae6975e":"markdown","534265b0":"markdown","f79aed60":"markdown","dc8780e2":"markdown","d477c22c":"markdown","77cf155e":"markdown","0832aac9":"markdown","3146f014":"markdown","5c1aa65c":"markdown","74088460":"markdown","487970d5":"markdown","e36ce15f":"markdown","96289c34":"markdown","3b0afe74":"markdown","93354e5b":"markdown","d9d1d7a9":"markdown","2dff56c0":"markdown","52f74768":"markdown","d010c671":"markdown","6726c1ed":"markdown","f3bd1c8e":"markdown","6b23fd8e":"markdown","0ea8fed5":"markdown","a376ea16":"markdown","12d88657":"markdown","5368f669":"markdown","25e872bf":"markdown","51025cc3":"markdown","cf3beaec":"markdown","98497b0a":"markdown","6cbff9da":"markdown","a6d73099":"markdown","c15e2099":"markdown","b9491dcc":"markdown","bbf206b3":"markdown","11fa2e37":"markdown","5f87e554":"markdown","1f4306f9":"markdown","215daa76":"markdown","b327b325":"markdown","857c8a5a":"markdown","b919643b":"markdown","300ee384":"markdown","4a6fa4c8":"markdown","4db415c3":"markdown","804f0f78":"markdown","f2c5d5d3":"markdown","3232cfd3":"markdown","e2256826":"markdown","678101f3":"markdown","6472ddc1":"markdown","6f87b574":"markdown","b8533350":"markdown","088fb80f":"markdown","218f0d32":"markdown","87698aea":"markdown","3de4d5bf":"markdown","2bac41bc":"markdown","26f1f3f2":"markdown","5dd923f3":"markdown","0c1c1bde":"markdown","860d23c2":"markdown","d2fffae6":"markdown","35b98124":"markdown","691915cb":"markdown","90bec486":"markdown","12689fbf":"markdown","b19a8c1a":"markdown","1e7cf299":"markdown","4c98dc6c":"markdown","fc2801cf":"markdown"},"source":{"96c7efe6":"import nltk","afb5d610":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport scipy.stats as stats\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nimport missingno as msno \n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB, BernoulliNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\n\nnltk.download(\"punkt\")\nnltk.download('stopwords')\nnltk.download('wordnet')\nimport nltk\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom collections import Counter\nfrom wordcloud import WordCloud \nfrom sklearn.metrics import plot_confusion_matrix, classification_report, f1_score, recall_score\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\n\nfrom yellowbrick.classifier import PrecisionRecallCurve\n\nimport nltk\nnltk.download(\"punkt\")\nnltk.download('stopwords')\nnltk.download('wordnet')\n\n# Importing plotly and cufflinks in offline mode\nimport cufflinks as cf\nimport plotly.express as px\nimport plotly.offline\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)\n\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.warn(\"this will not show\")\n\n# Figure&Display options\n%matplotlib inline\nfig, ax = plt.subplots()\n# fig.set_size_inches(10, 6)\nplt.rcParams[\"figure.figsize\"] = (12, 8)  # the size of A4 paper use (11.7, 8.27)\npd.set_option('max_colwidth', 200)\npd.set_option('display.max_rows', 1000)\npd.set_option('display.max_columns', 200)\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\n# !pip install termcolor\nimport colorama\nfrom colorama import Fore, Style  # maakes strings colored\nfrom termcolor import colored\n\nimport ipywidgets\nfrom ipywidgets import interact\n\n# !pip install -U pandas-profiling --user\n# !pip install https:\/\/github.com\/pandas-profiling\/pandas-profiling\/archive\/master.zip\nimport pandas_profiling\nfrom pandas_profiling.report.presentation.flavours.html.templates import create_html_assets","9168e1f6":"###############################################################################\n\ndef missing_values(df):\n    missing_number = df.isnull().sum().sort_values(ascending=False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n    missing_values = pd.concat([missing_number, missing_percent], axis=1, keys=['Missing_Number', 'Missing_Percent'])\n    return missing_values[missing_values['Missing_Number']>0]\n\n###############################################################################\n\ndef first_looking(df):\n    print(colored(\"Shape:\", attrs=['bold']), df.shape,'\\n', \n          colored('-'*79, 'red', attrs=['bold']),\n          colored(\"\\nInfo:\\n\", attrs=['bold']), sep='')\n    print(df.info(), '\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"Number of Uniques:\\n\", attrs=['bold']), df.nunique(),'\\n',\n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"Missing Values:\\n\", attrs=['bold']), missing_values(df),'\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n    print(colored(\"All Columns:\", attrs=['bold']), list(df.columns),'\\n', \n          colored('-'*79, 'red', attrs=['bold']), sep='')\n\n    df.columns= df.columns.str.lower().str.replace('&', '_').str.replace(' ', '_')\n\n    print(colored(\"Columns after rename:\", attrs=['bold']), list(df.columns),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n    \n        \ndef multicolinearity_control(df):\n    feature =[]\n    collinear=[]\n    for col in df.corr().columns:\n        for i in df.corr().index:\n            if (abs(df.corr()[col][i])> .9 and abs(df.corr()[col][i]) < 1):\n                    feature.append(col)\n                    collinear.append(i)\n                    print(colored(f\"Multicolinearity alert in between:{col} - {i}\", \n                                  \"red\", attrs=['bold']), df.shape,'\\n',\n                                  colored('-'*79, 'red', attrs=['bold']), sep='')\n\ndef duplicate_values(df):\n    print(colored(\"Duplicate check...\", attrs=['bold']), sep='')\n    duplicate_values = df.duplicated(subset=None, keep='first').sum()\n    if duplicate_values > 0:\n        df.drop_duplicates(keep='first', inplace=True)\n        print(duplicate_values, colored(\"Duplicates were dropped!\"),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n    else:\n        print(colored(\"There are no duplicates\"),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')     \n        \ndef drop_columns(df, drop_columns):\n    if drop_columns !=[]:\n        df.drop(drop_columns, axis=1, inplace=True)\n        print(drop_columns, 'were dropped')\n    else:\n        print(colored('We will now check the missing values and if necessary will drop realted columns!', attrs=['bold']),'\\n',\n              colored('-'*79, 'red', attrs=['bold']), sep='')\n        \ndef drop_null(df, limit):\n    print('Shape:', df.shape)\n    for i in df.isnull().sum().index:\n        if (df.isnull().sum()[i]\/df.shape[0]*100)>limit:\n            print(df.isnull().sum()[i], 'percent of', i ,'null and were dropped')\n            df.drop(i, axis=1, inplace=True)\n            print('new shape:', df.shape)       \n    print('New shape after missing value control:', df.shape)\n    \n###############################################################################\n\n# To view summary information about the column\n\ndef first_look(col):\n    print(\"column name    : \", col)\n    print(\"--------------------------------\")\n    print(\"per_of_nulls   : \", \"%\", round(df[col].isnull().sum()\/df.shape[0]*100, 2))\n    print(\"num_of_nulls   : \", df[col].isnull().sum())\n    print(\"num_of_uniques : \", df[col].nunique())\n    print(df[col].value_counts(dropna = False))\n    \n###############################################################################","8111993c":"df0 = pd.read_csv(\"..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\")\ndf = df0.copy()\ndf.head()","649f8a18":"df.profile_report()","b0455b8c":"first_looking(df)","3c434126":"df.head(1)","b00754ed":"df.sample(3)","826c37de":"df.shape","543b67f3":"df.drop(\"unnamed:_0\", axis=1, inplace=True)\ndf.head(1)","75f57179":"df.describe().T.style.background_gradient(subset=['mean','std','50%','count'], cmap='RdPu').format('{:.2f}')","b91a7e79":"df.describe(include=object).T","8327ff87":"# to find how many unique values numerical features have\n\nfor col in df.select_dtypes(include=[np.number]).columns:\n  print(colored(f\"{col}\", 'green', attrs=['bold']), f\"feature has\", colored(f\"{df[col].nunique()}\", 'green', attrs=['bold']), f\"unique values.\")","3b92e1d5":"# to find how many unique values object features have\n\nfor col in df.select_dtypes(include=\"object\").columns:\n  print(colored(f\"{col}\", 'green', attrs=['bold']), f\"feature has\", colored(f\"{df[col].nunique()}\", 'green', attrs=['bold']), f\"unique values.\")","fa442b30":"plt.figure(figsize=(14, 10))\n\n# Getting the Upper Triangle of the co-relation matrix\nmatrix = np.triu(df.corr())\n\n# using the upper triangle matrix as mask \nsns.heatmap(df.corr(), annot=True, cmap = sns.cubehelix_palette(8), mask=matrix)\n\nplt.xticks(rotation=45);","72145915":"df.columns","b68c149a":"df.head(2)","c19c6dcd":"df[\"recommended_ind\"].value_counts()","b7791085":"df[\"recommended_ind\"].value_counts()","8f5777a0":"first_look(\"recommended_ind\")","dceef970":"df[\"recommended_ind\"].describe().T","1d6b0a41":"sns.countplot(x = df.recommended_ind, data = df)\nplt.title('Customer Recommendation Distribution', fontsize=30)\nplt.xlabel(\"Recommendation Label\", fontsize=24)\nplt.ylabel(\"The Number of Recommendations\", fontsize=24)\n\nfor index,value in enumerate(df.recommended_ind.value_counts().sort_values()):\n     plt.text(index, value, f\"{value}\", ha=\"center\", va=\"bottom\", fontsize = 13);","208ca455":"plt.figure(figsize=(8, 8))\n\nexplode = [0, 0.1]\nplt.pie(df['recommended_ind'].value_counts(), explode=explode, autopct='%1.1f%%', shadow=True, startangle=140)\nplt.legend(labels=['1', '0'])\nplt.title('Customer Recommendation Distribution', fontsize=20)\nplt.axis('off');","432fd7c7":"sns.swarmplot(y=\"age\", x=\"rating\", hue=\"recommended_ind\", data=df, palette=\"husl\");","f5c84b35":"sns.swarmplot(y=\"age\", x=\"division_name\", hue=\"recommended_ind\", data=df, palette=\"husl\");","dfc3dd47":"sns.swarmplot(y=\"age\", x=\"department_name\", hue=\"recommended_ind\", data=df, palette=\"husl\");","658564a6":"sns.swarmplot(y=\"age\", x=\"class_name\", hue=\"recommended_ind\", data=df, palette=\"husl\");","6195ae05":"df[\"rating\"].value_counts()","ae46900f":"first_look(\"rating\")","cd0cf57b":"df[\"rating\"].describe().T","42fc5b34":"sns.countplot(x = df.rating, data = df)\nplt.title('Customer Rating Distribution', fontsize=30)\nplt.xlabel(\"Rating Label\", fontsize=24)\nplt.ylabel(\"The Number of Rating\", fontsize=24)\n\nfor index,value in enumerate(df.rating.value_counts().sort_values()):\n     plt.text(index, value, f\"{value}\", ha=\"center\", va=\"bottom\", fontsize = 13);","07220b21":"plt.figure(figsize=(8, 8))\n\nexplode = [0.1, 0, 0, 0, 0.1]\nplt.pie(df['rating'].value_counts(), explode=explode, autopct='%1.1f%%', shadow=True, startangle=140)\nplt.legend(labels=['1', '2', '3', '4','5'])\nplt.title('Customer Rating Distribution', fontsize=20)\nplt.axis('off');","5130f5c2":"df[\"age\"].value_counts()","f0549e35":"first_look(\"age\")","2d6c6c85":"df[\"age\"].describe().T","4fd8d785":"plt.figure(figsize = (20, 8))\nplt.title('Customer Age Distribution', fontsize=30)\nplt.xlabel(\"Age\", fontsize=24)\nplt.ylabel(\"The Number of Customer Age\", fontsize=18)\n\nsns.histplot(df, x='age', kde = True, bins = 50);","e1edf2f3":"fig_dims = (30, 10)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.barplot(data = df, x = \"age\", y = \"recommended_ind\", ax=ax)\n\nplt.title('Customer Age Distribution By Recommendations', fontsize=30)\nplt.xlabel(\"Age\", fontsize=24)\nplt.ylabel(\"Customer Recommendation Ratio\", fontsize=18)\nplt.xticks(rotation = 45);","b925766e":"fig = px.histogram(df['age'], color=df['recommended_ind'],\n                   labels={'value': 'Age', 'color': 'Recommended'}, \n                   color_discrete_map={0: \"magenta\", 1: \"MediumPurple\"},\n                   marginal='box')\nfig.update_traces(marker=dict(line=dict(color='#000000', width=2)))\nfig.update_layout(title_text='Distribution of the Age and Recommendation',\n                  title_x=0.5, title_font=dict(size=20))\nfig.update_layout(barmode='overlay')\nfig.show()","8556ac41":"df[\"positive_feedback_count\"].value_counts()","28beb8a8":"first_look(\"positive_feedback_count\")","e5ba33e7":"df[\"positive_feedback_count\"].describe().T","9d9807c2":"plt.figure(figsize = (20, 8))\nplt.title('Customer Positive Feedback Distribution', fontsize=20)\nplt.xlabel(\"Customer Positive Feedback\", fontsize=24)\nplt.ylabel(\"The Number of Customer Positive Feedback\", fontsize=18)\n\nsns.histplot(df, x='positive_feedback_count', kde = True, bins = 50);","e4e46340":"fig_dims = (30, 10)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.barplot(data = df, x = \"positive_feedback_count\", y = \"age\", ax=ax)\n\nplt.xlabel(\"Customer Positive Feedback\", fontsize=24)\nplt.ylabel(\"Age\", fontsize=24)\n\nplt.title('Customer Positive Feedback Distribution By Age', fontsize=30)\nplt.xticks(rotation = 45);","e7d24867":"df.columns","082656ff":"df[\"division_name\"].value_counts()","65dd6e1a":"first_look(\"division_name\")","56e19c5d":"df[\"division_name\"].describe().T","fbc3596b":"g = sns.catplot( x='division_name',\n             kind=\"count\", \n             data=df,\n             height=5,\n             aspect=2)\n\nplt.title('Division Distribution', fontsize=24)\nplt.xlabel(\"Division Name\", fontsize=24)\nplt.ylabel(\"The Number of Divisions\", fontsize=20)\n\nax = g.facet_axis(0, 0)\nfor p in ax.patches:\n    ax.text(p.get_x() + 0.28, \n            p.get_height() * 1.025, \n            '{0:.0f}'.format(p.get_height()), \n            color='black', rotation='horizontal', size='large')\n\nplt.show()","3e3e9c2b":"plt.figure(figsize=(8, 8))\n\nexplode = [0.1, 0.1, 0]\nplt.pie(df['division_name'].value_counts(), explode=explode, autopct='%1.1f%%', shadow=True, startangle=140)\nplt.legend(labels=['1', '2', '3'])\nplt.title('Division Distribution', fontsize=20)\nplt.axis('off');","53565054":"g = sns.catplot(data = df, x =\"division_name\", hue = \"recommended_ind\", kind='count', height=5, aspect=2, legend_out=False)\n\nplt.title('Division Distribution By Recommendation', fontsize=24)\nplt.xlabel(\"Division Name By Recommendation\", fontsize=20)\nplt.ylabel(\"The Number of Divisions\", fontsize=20)\nplt.legend(title='Recommendation Indicator', loc='upper left', labels=['Not Recomnended', 'Recomnended'])\n\nax = g.facet_axis(0, 0)\nfor p in ax.patches:\n    ax.text(p.get_x() + 0.12, \n            p.get_height() * 1.025, \n            '{0:.0f}'.format(p.get_height()), \n            color='black', rotation='horizontal', size='large')\n\nplt.show()","296e107d":"g = sns.catplot(data = df, x =\"rating\", hue = \"division_name\", kind='count', height=5, aspect=2, legend_out=False)\n\nplt.title('Rating Distribution By Division', fontsize=24)\nplt.xlabel(\"Ratings By Division\", fontsize=20)\nplt.ylabel(\"The Number of Ratings\", fontsize=20)\nplt.legend(title='Division Name', loc='upper left', labels=['Intimates', 'General', 'General Petite'])\n\nax = g.facet_axis(0, 0)\nfor p in ax.patches:\n    ax.text(p.get_x() + 0.04, \n            p.get_height() * 1.025, \n            '{0:.0f}'.format(p.get_height()), \n            color='black', rotation='horizontal', size='large')\n\nplt.show()","3e215634":"df[\"department_name\"].value_counts()","ad36a2a7":"first_look(\"department_name\")","0d637634":"df[\"department_name\"].describe().T","6f7da75c":"g = sns.catplot(data = df, x =\"department_name\", kind='count', height=5, aspect=2)\n\nplt.title('Department Distribution', fontsize=26)\nplt.xlabel(\"Department Name\", fontsize=20)\nplt.ylabel(\"The Number of Departments\", fontsize=20)\n\nax = g.facet_axis(0, 0)\nfor p in ax.patches:\n    ax.text(p.get_x() + 0.28, \n            p.get_height() * 1.025, \n            '{0:.0f}'.format(p.get_height()), \n            color='black', rotation='horizontal', size='large')\n\nplt.show()","b7e5931a":"plt.figure(figsize=(8, 8))\n\nexplode = [0.1, 0, 0, 0, 0, 0]\nplt.pie(df['department_name'].value_counts(), explode=explode, autopct='%1.1f%%', shadow=True, startangle=140)\nplt.legend(labels=['Tops', 'Dresses', 'Bottoms', 'Intimate', 'Jackets', 'Trend'])\nplt.title('Department Distribution', fontsize=20)\nplt.axis('off');","090fa798":"g = sns.catplot(data = df, x =\"department_name\", hue = \"recommended_ind\", kind='count', height=7, aspect=2.5, legend_out=False)\n\nplt.title('Department Distribution By Recommendation', fontsize=26)\nplt.xlabel(\"Department Name\", fontsize=20)\nplt.ylabel(\"The Number of Recommendations\", fontsize=20)\nplt.legend(title='Recommendation Indicator', loc='upper left', labels=['Not Recomnended', 'Recomnended'], fontsize='x-large', title_fontsize='24')\n\nax = g.facet_axis(0, 0)\nfor p in ax.patches:\n    ax.text(p.get_x() + 0.12, \n            p.get_height() * 1.025, \n            '{0:.0f}'.format(p.get_height()), \n            color='black', rotation='horizontal', size='large')\n\nplt.show()","cd6dd24f":"g = sns.catplot(data = df, x =\"rating\", hue = \"department_name\", kind='count', height=10, aspect=2.5, legend_out=False)\n\nplt.title('Department Distribution By Recommendation', fontsize=26)\nplt.xlabel(\"Department Name\", fontsize=20)\nplt.ylabel(\"The Number of Recommendations\", fontsize=20)\nplt.legend(title='Department Name', loc='upper left', labels=['Intimates', 'Dresses', 'Bottoms', 'Tops', 'Jackets', 'Trend'], fontsize='x-large', title_fontsize='24')\nplt.figure(figsize=(15, 8))\n\nax = g.facet_axis(0, 0)\nfor p in ax.patches:\n    ax.text(p.get_x() + 0.025, \n            p.get_height() * 1.025, \n            '{0:.0f}'.format(p.get_height()), \n            color='black', rotation='horizontal', size='large')\n\nplt.show()","d7f6d199":"df[\"class_name\"].value_counts()","6706e1d7":"first_look(\"class_name\")","4b8cd77f":"df[\"class_name\"].describe().T","12061f06":"plt.title('Product Class Distribution', fontsize=25)\ndf[\"class_name\"].value_counts().plot(kind=\"pie\", autopct='%1.1f%%', figsize=(16, 16));","4f7aa49d":"g = sns.catplot(data = df, x =\"department_name\", hue = \"rating\", kind='count', height=10, aspect=2.5)\n\nsns.set(rc = {'figure.figsize':(30, 12)})\nplt.title('Department Distribution By Rating', fontsize=30)\nplt.xlabel(\"Department Name\", fontsize=24)\nplt.ylabel(\"The Number of Ratings\", fontsize=24)\nplt.legend(title='Rating Label', loc='upper left', labels=['1', '2', '3', '4', '5'], fontsize='x-large', title_fontsize='24')\n\nax = g.facet_axis(0, 0)\nfor p in ax.patches:\n    ax.text(p.get_x() + 0.02, \n            p.get_height() * 1.025, \n            '{0:.0f}'.format(p.get_height()), \n            color='black', rotation='horizontal', size='large', fontsize = 18)\n\nplt.show()","d9d592a0":"FreqOfWords = df['review_text'].str.split(expand=True).stack().value_counts()\nFreqOfWords_top200 = FreqOfWords[:200]\n\nfig = px.treemap(FreqOfWords_top200, path=[FreqOfWords_top200.index], values=0, width=1000, height=600)\nfig.update_layout(title_text='Top Frequent 200 Words in the Dataset (Before Cleaning)',\n                  title_x=0.5, title_font=dict(size=20)\n                  )\nfig.update_traces(textinfo=\"label+value\")\nfig.show()","c53bc964":"df_cat = df[['division_name', 'department_name', 'class_name', \"recommended_ind\"]]\ndf_cat[\"recommended_ind\"] = df_cat[\"recommended_ind\"].apply(lambda x: \"Recommended\" if x>=1 else \"Not Recommended\")\ndf_cat.rename({'division_name': 'Division Name', 'department_name': 'Department Name', 'class_name': 'Class Name', 'recommended_ind': 'Recommendation Indicator'}, axis=1, inplace=True)\ndf_cat","46530e24":"df_num = df[['age', 'rating', 'positive_feedback_count', 'recommended_ind']]\ndf_num[\"recommended_ind\"] = df_num[\"recommended_ind\"].apply(lambda x: \"Recommended\" if x>=1 else \"Not Recommended\")\ndf_num.rename({'age': 'Age', 'rating': 'Rating', 'positive_feedback_count': 'Positive Feedback', 'recommended_ind': 'Recommendation Indicator'}, axis=1, inplace=True)\ndf_num","4577ceb3":"for i, col in enumerate(df_cat.columns):\n    xtab = pd.crosstab(df_cat[col], df_cat[\"Recommendation Indicator\"], normalize=True)\n    print(colored('-'*55, 'red', attrs=['bold']), sep='')\n    print(xtab*100)","f35f8f67":"for i, col in enumerate(df_num.columns):\n    xtab = pd.crosstab(df_num[col], df_num[\"Recommendation Indicator\"], normalize=True)\n    print(colored('-'*55, 'red', attrs=['bold']), sep='')\n    print(xtab*100)","258145f8":"df.columns","32e12d52":"df.drop(['clothing_id', 'age', 'title', 'rating',\n       'positive_feedback_count', 'division_name',\n       'department_name', 'class_name'], axis=1, inplace=True)","c3f22db5":"df.head(3)","25bac52f":"df['review_text'].isnull().value_counts()","11faa7bd":"df['recommended_ind'].isnull().value_counts()","6dcca41e":"df.info()","4988ff6c":"df = df.dropna()","9b44cda6":"df.info()","58ffa4ed":"df['review_text'].isnull().value_counts()","16bc9a27":"df['recommended_ind'].isnull().value_counts()","99566480":"missing_values(df)","36704b06":"blanks = []  # start with an empty list\n\nfor rv in df.itertuples(): # iterate over the DataFrame\n    if type(rv)==str and rv.isspace(): # avoid NaN values and test 'review' for whitespace\n        blanks.append(i)\nblanks","c2c9fc98":"df[\"review_text\"].str.isspace().sum()","41138452":"df[df[\"review_text\"].str.isspace() == True].index","231caa48":"def cleaning_fsa(data):\n    \n    import re\n    #1. Remove Puncs\n    # \\w typically matches [A-Za-z0-9_]\n    text = re.sub('[^\\w\\s]','', data)\n         \n    #2. Tokenize\n    text_tokens = word_tokenize(text.lower()) \n    \n    #3. Remove numbers\n    tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n    \n    #4. Removing Stopwords\n    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n    \n    #5. lemma\n    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n    \n    #joining\n    return \" \".join(text_cleaned)","a36453f3":"stop_words = stopwords.words('english')","d45ef276":"text = cleaning_fsa(str(df[\"review_text\"]))\ntext","0de0b50d":"df[\"review_text\"] = df[\"review_text\"].apply(cleaning_fsa)\ndf[\"review_text\"].head()","a0b80c90":"df.head(3)","5b8b36f4":"\" \".join(df[\"review_text\"]).split()","e080b8fe":"word_values = pd.Series(\" \".join(df[\"review_text\"]).split()).value_counts()\nword_values","09b4a1a4":"rare_words = word_values[word_values <= 2]\nrare_words","08d0055a":"rare_words.value_counts()","185c040c":"len(rare_words)","f0b5917b":"rare_words.index","2ba0699b":"df[\"review_text\"] = df[\"review_text\"].apply(lambda x: \" \".join([i for i in x.split() if i not in rare_words.index]))\ndf[\"review_text\"].head()","1a458f9c":"df.info()","64ccdb9a":"df.head(3)","5090969b":"df.columns","58b26d81":"df[df[\"recommended_ind\"] == 0]","85c1cd3c":"df[df[\"recommended_ind\"] == 1]","c1869f3c":"df[\"recommended_ind\"].value_counts()","9f5941e4":"\" \".join(df[\"review_text\"]).split()","b617fcfe":"neg_words = \" \".join(df[df[\"recommended_ind\"] == 0].review_text).split()\nneg_words ","c9f9301b":"pos_words =\" \".join(df[df[\"recommended_ind\"] == 1].review_text).split()\npos_words","c3df4547":"review_text = df[\"review_text\"]","8dbc6dc7":"all_words = \" \".join(review_text)","daf89f6f":"all_words[:100]","ce6eca6e":"from wordcloud import WordCloud \n\nwordcloud = WordCloud(background_color=\"white\", max_words =250).generate(all_words)\n\nplt.figure(figsize = (13, 13))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","fd62810a":"wordcloud = WordCloud(background_color=\"white\", max_words =250, colormap='gist_heat').generate(str(neg_words))\n\nplt.figure(figsize = (13, 13))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","92fa6d6a":"wordcloud = WordCloud(background_color=\"white\", max_words =250, colormap='cool').generate(str(pos_words))\n\nplt.figure(figsize = (13, 13))\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.show()","a7c8ab65":"df.head()","afda29a1":"from sklearn.model_selection import train_test_split","6e8f22a8":"X = df[\"review_text\"]\ny= df[\"recommended_ind\"]","e1c40158":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=101)","67c995d7":"from sklearn.feature_extraction.text import CountVectorizer","9be27663":"vectorizer = CountVectorizer()\n\nX_train_count = vectorizer.fit_transform(X_train)\nX_test_count = vectorizer.transform(X_test)","848408af":"type(X_train_count)","4c309415":"X_train_count.toarray()","6c1aa272":"vectorizer.get_feature_names()","75c4b462":"pd.DataFrame(X_train_count.toarray(), columns = vectorizer.get_feature_names())","378bf91a":"from sklearn.feature_extraction.text import TfidfVectorizer","022e02e4":"tf_idf_vectorizer = TfidfVectorizer()\n\nX_train_tf_idf = tf_idf_vectorizer.fit_transform(X_train)\nX_test_tf_idf = tf_idf_vectorizer.transform(X_test)","994f3f1d":"X_train_tf_idf.toarray()","68d7e96f":"pd.DataFrame(X_train_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names())","c4566432":"from sklearn.metrics import confusion_matrix,classification_report, f1_score, recall_score, accuracy_score, precision_score","be150395":"def eval(model, X_train, X_test):\n    y_pred = model.predict(X_test)\n    y_pred_train = model.predict(X_train)\n    \n    print(\"Test_Set\")\n    print(classification_report(y_test, y_pred))\n    print(\"Train_Set\")\n    print(classification_report(y_train, y_pred_train))\n    fig, ax = plt.subplots(figsize=(8, 8))\n    \n    plot_confusion_matrix(model, X_test, y_test, ax=ax)","201f1d65":"from sklearn.linear_model import LogisticRegression\n\nlog = LogisticRegression(C =0.6, max_iter=1000, class_weight= \"balanced\", random_state=101)\nlog.fit(X_train_count,y_train)","4bdfbb23":"print(\"LOG MODEL\")\n\neval(log, X_train_count, X_test_count)","f20bfd16":"from sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\n\ncustom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label=0),\n                 'recall-0': make_scorer(recall_score, pos_label=0),\n                 'f1-0': make_scorer(f1_score, pos_label=0),\n                 'precision-1': make_scorer(precision_score, pos_label=1),\n                 'recall-1': make_scorer(recall_score, pos_label=1),\n                 'f1-1': make_scorer(f1_score, pos_label=1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = LogisticRegression(C =0.6, max_iter=1000, class_weight= \"balanced\", random_state=101)\n    scores = cross_val_score(model, X_train_count, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        log_count_rec = scores\n    elif i == \"f1-1\":\n        log_count_f1 = scores\n    print(f\" {i:20} score for count : {scores}\\n\")","d94806e9":"from yellowbrick.classifier import PrecisionRecallCurve\nviz = PrecisionRecallCurve(\n                            LogisticRegression(C =0.6, max_iter=1000, class_weight= \"balanced\", random_state=101),\n                            classes=log.classes_,\n                            per_class=True,\n                            cmap=\"Set1\"\n                           )\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_facecolor('yellow')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","acfa689f":"log_AP_count = viz.score_","440fe21b":"log = LogisticRegression(C=0.1, max_iter=1000, random_state=101, class_weight=\"balanced\")\n\nlog.fit(X_train_tf_idf,y_train)","20cfd442":"print(\"LOG MODEL\")\n\neval(log, X_train_tf_idf, X_test_tf_idf)","a6ed3aa9":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label=0),\n                 'recall-0': make_scorer(recall_score, pos_label=0),\n                 'f1-0': make_scorer(f1_score, pos_label=0),\n                 'precision-1': make_scorer(precision_score, pos_label=1),\n                 'recall-1': make_scorer(recall_score, pos_label=1),\n                 'f1-1': make_scorer(f1_score, pos_label=1)\n                 }\n\nfor i, j in custom_scorer.items():\n    LogisticRegression(C=0.1, max_iter=1000, random_state=101, class_weight=\"balanced\")\n    scores = cross_val_score(model, X_train_tf_idf, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        log_tfidf_rec = scores\n    elif i == \"f1-1\":\n        log_tfidf_f1 = scores\n    print(f\" {i:20} score for tfidf : {scores}\\n\")","ecc91128":"viz = PrecisionRecallCurve(\n                            LogisticRegression(C=0.1, max_iter=1000, random_state=101, class_weight=\"balanced\"),\n                            classes=log.classes_,\n                            per_class=True,\n                            cmap=\"Set1\"\n                           )\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_facecolor('yellow')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","d24eae76":"log_AP_tfidf = viz.score_","7f2059d9":"from sklearn.naive_bayes import MultinomialNB, BernoulliNB # BernoulliNB for binary model","94a9af83":"nb = MultinomialNB()\nnb.fit(X_train_count, y_train)","3967fb63":"print(\"NB MODEL\")\n\neval(nb, X_train_count, X_test_count)","0c6f6aee":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label=0),\n                 'recall-0': make_scorer(recall_score, pos_label=0),\n                 'f1-0': make_scorer(f1_score, pos_label=0),\n                 'precision-1': make_scorer(precision_score, pos_label=1),\n                 'recall-1': make_scorer(recall_score, pos_label=1),\n                 'f1-1': make_scorer(f1_score, pos_label=1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = MultinomialNB()\n    scores = cross_val_score(model, X_train_count, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        nb_count_rec = scores\n    elif i == \"f1-1\":\n        nb_count_f1 = scores\n    print(f\" {i:20} score for count : {scores}\\n\")","0203a8fe":"viz = PrecisionRecallCurve(\n                            MultinomialNB(),\n                            classes=nb.classes_,\n                            per_class=True,\n                            cmap=\"Set1\"\n                           )\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_facecolor('yellow')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","161d9b57":"nb_AP_count = viz.score_","a94a8744":"from sklearn.naive_bayes import MultinomialNB, BernoulliNB\nnb = MultinomialNB()\nnb.fit(X_train_tf_idf, y_train)","3d0af4e9":"print(\"NB MODEL\")\neval(nb, X_train_tf_idf, X_test_tf_idf)","94a1c297":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label=0),\n                 'recall-0': make_scorer(recall_score, pos_label=0),\n                 'f1-0': make_scorer(f1_score, pos_label=0),\n                 'precision-1': make_scorer(precision_score, pos_label=1),\n                 'recall-1': make_scorer(recall_score, pos_label=1),\n                 'f1-1': make_scorer(f1_score, pos_label=1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = BernoulliNB()\n    scores = cross_val_score(model, X_train_tf_idf, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        nb_tfidf_rec = scores\n    elif i == \"f1-1\":\n        nb_tfidf_f1 = scores\n    print(f\" {i:20} score for tfidf : {scores}\\n\")","be0e9e1d":"from yellowbrick.classifier import PrecisionRecallCurve\n\nviz = PrecisionRecallCurve(\n                            MultinomialNB(),\n                            classes=nb.classes_,\n                            per_class=True,\n                            cmap=\"Set1\"\n                           )\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_facecolor('yellow')\n\nviz.fit(X_train_tf_idf, y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","d626df33":"nb_AP_tfidf = viz.score_","c6fc4feb":"from sklearn.svm import LinearSVC\n\nsvc = LinearSVC(C=0.01, class_weight=\"balanced\", random_state=101)\nsvc.fit(X_train_count,y_train)","46a6a330":"print(\"SVC MODEL\")\n\neval(svc, X_train_count, X_test_count)","e72cc7e2":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label=0),\n                 'recall-0': make_scorer(recall_score, pos_label=0),\n                 'f1-0': make_scorer(f1_score, pos_label=0),\n                 'precision-1': make_scorer(precision_score, pos_label=1),\n                 'recall-1': make_scorer(recall_score, pos_label=1),\n                 'f1-1': make_scorer(f1_score, pos_label=1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = LinearSVC(C=0.01, class_weight=\"balanced\", random_state=101)\n    scores = cross_val_score(model, X_train_count, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        svc_count_rec = scores\n    elif i == \"f1-1\":\n        svc_count_f1 = scores\n    print(f\" {i:20} score for count : {scores}\\n\")","0af78224":"viz = PrecisionRecallCurve(\n                            LinearSVC(C=0.01, class_weight=\"balanced\", random_state=101),\n                            classes=svc.classes_,\n                            per_class=True,\n                            cmap=\"Set1\"\n                           )\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_facecolor('yellow')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","cc568c72":"svc_AP_count = viz.score_","40408d39":"svc = LinearSVC(C=0.01, class_weight=\"balanced\", random_state=101)\n\nsvc.fit(X_train_tf_idf, y_train)","8c463bf6":"print(\"SVC MODEL\")\n\neval(svc, X_train_tf_idf, X_test_tf_idf)","b3243eff":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label=0),\n                 'recall-0': make_scorer(recall_score, pos_label=0),\n                 'f1-0': make_scorer(f1_score, pos_label=0),\n                 'precision-1': make_scorer(precision_score, pos_label=1),\n                 'recall-1': make_scorer(recall_score, pos_label=1),\n                 'f1-1': make_scorer(f1_score, pos_label=1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = LinearSVC(C=0.01, class_weight=\"balanced\", random_state=101)\n    scores = cross_val_score(model, X_train_tf_idf, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        svc_tfidf_rec = scores\n    elif i == \"f1-1\":\n        svc_tfidf_f1 = scores\n    print(f\" {i:20} score for tfidf : {scores}\\n\")","7d0236e3":"viz = PrecisionRecallCurve(\n                            LinearSVC(C=0.01, class_weight=\"balanced\", random_state=101),\n                            classes=svc.classes_,\n                            per_class=True,\n                            cmap=\"Set1\"\n                           )\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_facecolor('yellow')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","e69c9db2":"svc_AP_tfidf = viz.score_","dc7f5989":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(200, max_depth = 12, random_state = 42, n_jobs = -1, class_weight=\"balanced\")\nrf.fit(X_train_count, y_train)","4924366e":"print(\"RF MODEL\")\n\neval(rf, X_train_count, X_test_count)","5c918fcb":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label=0),\n                 'recall-0': make_scorer(recall_score, pos_label=0),\n                 'f1-0': make_scorer(f1_score, pos_label=0),\n                 'precision-1': make_scorer(precision_score, pos_label=1),\n                 'recall-1': make_scorer(recall_score, pos_label=1),\n                 'f1-1': make_scorer(f1_score, pos_label=1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = RandomForestClassifier(200, max_depth = 12, random_state = 42, n_jobs = -1, class_weight=\"balanced\")\n    scores = cross_val_score(model, X_train_count, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        rf_count_rec = scores\n    elif i == \"f1-1\":\n        rf_count_f1 = scores\n    print(f\" {i:20} score for count : {scores}\\n\")","643d13e1":"viz = PrecisionRecallCurve(\n                            RandomForestClassifier(200, max_depth = 12, random_state = 42, n_jobs = -1, class_weight=\"balanced\"),\n                            classes=rf.classes_,\n                            per_class=True,\n                            cmap=\"Set1\"\n                           )\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_facecolor('yellow')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","45d611f5":"rf_AP_count = viz.score_","d599fa1d":"rf = RandomForestClassifier(200, max_depth = 10, random_state = 42, n_jobs = -1, class_weight=\"balanced\")\n\nrf.fit(X_train_tf_idf, y_train)","c4e7ce6d":"print(\"RF MODEL\")\n\neval(rf, X_train_tf_idf, X_test_tf_idf)","c7d15347":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label=0),\n                 'recall-0': make_scorer(recall_score, pos_label=0),\n                 'f1-0': make_scorer(f1_score, pos_label=0),\n                 'precision-1': make_scorer(precision_score, pos_label=1),\n                 'recall-1': make_scorer(recall_score, pos_label=1),\n                 'f1-1': make_scorer(f1_score, pos_label=1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = RandomForestClassifier(200, max_depth = 10, random_state = 42, n_jobs = -1, class_weight=\"balanced\")\n    scores = cross_val_score(model, X_train_tf_idf, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        rf_tfidf_rec = scores\n    elif i == \"f1-1\":\n        rf_tfidf_f1 = scores\n    print(f\" {i:20} score for tfidf : {scores}\\n\")","f56766a5":"viz = PrecisionRecallCurve(\n                            RandomForestClassifier(200, max_depth = 10, random_state = 42, n_jobs = -1, class_weight=\"balanced\"),\n                            classes=rf.classes_,\n                            per_class=True,\n                            cmap=\"Set1\"\n                           )\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_facecolor('yellow')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","bde91ba0":"rf_AP_tfidf = viz.score_","aca6d9f7":"from sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier(n_estimators= 500, random_state = 42)\nada.fit(X_train_count, y_train)","d6e7f802":"print(\"Ada MODEL\")\n\neval(ada, X_train_count, X_test_count)","01eb1e8b":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label=0),\n                 'recall-0': make_scorer(recall_score, pos_label=0),\n                 'f1-0': make_scorer(f1_score, pos_label=0),\n                 'precision-1': make_scorer(precision_score, pos_label=1),\n                 'recall-1': make_scorer(recall_score, pos_label=1),\n                 'f1-1': make_scorer(f1_score, pos_label=1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model = AdaBoostClassifier(n_estimators= 500, random_state = 42)\n    scores = cross_val_score(model, X_train_count, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        ada_count_rec = scores\n    elif i == \"f1-1\":\n        ada_count_f1 = scores\n    print(f\" {i:20} score for count : {scores}\\n\")","62772f93":"viz = PrecisionRecallCurve(\n                            AdaBoostClassifier(n_estimators= 500, random_state = 42),\n                            classes=ada.classes_,\n                            per_class=True,\n                            cmap=\"Set1\"\n                           )\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_facecolor('yellow')\n\nviz.fit(X_train_count,y_train)\nviz.score(X_test_count, y_test)\nviz.show();","9ad66da9":"ada_AP_count = viz.score_","2f472e03":"ada = AdaBoostClassifier(n_estimators= 500, random_state = 42)\n\nada.fit(X_train_tf_idf, y_train)","095a48e0":"print(\"Ada MODEL\")\n\neval(ada, X_train_tf_idf, X_test_tf_idf)","1abdb8ce":"custom_scorer = {'accuracy': make_scorer(accuracy_score),\n                 'precision-0': make_scorer(precision_score, pos_label=0),\n                 'recall-0': make_scorer(recall_score, pos_label=0),\n                 'f1-0': make_scorer(f1_score, pos_label=0),\n                 'precision-1': make_scorer(precision_score, pos_label=1),\n                 'recall-1': make_scorer(recall_score, pos_label=1),\n                 'f1-1': make_scorer(f1_score, pos_label=1)\n                 }\n\nfor i, j in custom_scorer.items():\n    model =AdaBoostClassifier(n_estimators= 500, random_state = 42)\n    scores = cross_val_score(model, X_train_tf_idf, y_train, cv = 10, scoring = j).mean()\n    if i == \"recall-1\":\n        ada_tfidf_rec = scores\n    elif i == \"f1-1\":\n        ada_tfidf_f1 = scores\n    print(f\" {i:20} score for tfidf : {scores}\\n\")","f8667500":"viz = PrecisionRecallCurve(\n                            AdaBoostClassifier(n_estimators= 500, random_state = 42),\n                            classes=ada.classes_,\n                            per_class=True,\n                            cmap=\"Set1\"\n                           )\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.set_facecolor('yellow')\n\nviz.fit(X_train_tf_idf,y_train)\nviz.score(X_test_tf_idf, y_test)\nviz.show();","41cfc811":"ada_AP_tfidf = viz.score_","9023c0bd":"import numpy as np\nimport pandas as pd\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GRU, Embedding\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","196ec0ab":"df0 = pd.read_csv('..\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv')\ndf_dl = df0.copy()\ndf_dl.head()","240cb1f8":"df_dl = df_dl[[\"Review Text\",\"Recommended IND\"]]\ndf_dl.head()","6feb1879":"df_dl.shape","c2dce66b":"df_dl.dropna(inplace = True)","7423f13a":"df_dl.shape","1995f53b":"X = df_dl['Review Text'].values\ny = df_dl['Recommended IND'].values","4bd713a3":"num_words = 10000 \n# We have defined the most frequent 10000 repeated words in corpus for tokenizing. We ignore the rest.\n\ntokenizer = Tokenizer(num_words=num_words) \n# The default values of \"filters\" are '!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n'. \n# If you also want to filters the numbers, then just \"1234567890\" at the end. ","84b94aa9":"tokenizer.fit_on_texts(X)","27862e5d":"tokenizer.word_index","2def9596":"len(tokenizer.word_index) ","1aca80c1":"X_num_tokens = tokenizer.texts_to_sequences(X)","d7ac16fd":"num_tokens = [len(tokens) for tokens in X_num_tokens]\nnum_tokens = np.array(num_tokens)","81ba26f6":"np.array(X_num_tokens)","e7ab722f":"X[105]","daad6794":"print(X_num_tokens[105])","879e589c":"# tokenizer.word_index[\"The\"]\n# This code will give you an error since \"The\" which is not among the most repeated 10000 words was excluded while tokenizing","f4c3603a":"tokenizer.word_index[\"shirt\"]","2a70f19e":"tokenizer.word_index[\"exactly\"]","b050ba18":"num_tokens.mean()","f8120aab":"num_tokens.max() ","0e4094ef":"num_tokens.argmax()","f7901892":"X[16263]","1032b2d7":"len(X[16263])","5fdec8ba":"num_tokens.argmin()","76aaf025":"X[820]","1c4863ad":"len(X[820])","a03363ce":"len(X_num_tokens[105])","832a4eda":"np.array(X_num_tokens[105])","e16b9e21":"len(X_num_tokens[106])","4a7506e5":"np.array(X_num_tokens[106])","e0bf2a42":"num_tokens = [len(tokens) for tokens in X_num_tokens]\n\nnum_tokens = np.array(num_tokens)","4d7126c0":"num_tokens","612fae26":"max_tokens = 103","9acb9d98":"sum(num_tokens < max_tokens) \/ len(num_tokens)","52a8b84b":"sum(num_tokens < max_tokens) # the number of documents which have 103 or less tokens","0dfffae4":"len(num_tokens)  # total number of all documents in corpus which is constrained by num_words as 20000","c123110c":"X_pad = pad_sequences(X_num_tokens, maxlen=max_tokens)","6a919e9a":"X_pad.shape","3423eaf8":"X_pad[105]","a7bc9970":"X_pad[106]","b04a4208":"from sklearn.model_selection import train_test_split","448712de":"X_train, X_test, y_train, y_test = train_test_split(X_pad, y, test_size=0.2, stratify=y, random_state=101)  \n\n# we have been using stratify to prevent imbalance.","ba69fa3f":"model = Sequential()","044b91e0":"embedding_size = 100","8f63f46e":"model.add(Embedding(input_dim=num_words,        \n                    output_dim=embedding_size,                                       \n                    input_length=max_tokens,    \n                    name='embedding_layer')) ","5333eeb5":"model.add(GRU(units=48, return_sequences=True))  \nmodel.add(GRU(units=24, return_sequences=True)) \nmodel.add(GRU(units=12)) \nmodel.add(Dense(1, activation='sigmoid'))  ","c5adc501":"optimizer = Adam(learning_rate=0.006)","8bef0fcd":"model.compile(loss='binary_crossentropy',\n              optimizer=optimizer,\n              metrics=['Recall'])","32c1abb6":"model.summary() ","cc648175":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stop = EarlyStopping(monitor=\"val_loss\", mode=\"auto\", \n                           verbose=1, patience = 10, restore_best_weights=True)","3267d985":"pd.Series(y_train).value_counts(normalize=True) ","53b41adf":"weights = {0:82, 1:18}","18105e42":"model.fit(X_train, y_train, epochs=30, batch_size=256, class_weight=weights,\n         validation_data=(X_test, y_test), callbacks=[early_stop])","c6d89481":"model.save('NLP_Sentiment_Analysis_Project')","1441200d":"model_loss = pd.DataFrame(model.history.history)\nmodel_loss.head()","678792b9":"model_loss.plot();","20d829aa":"model.evaluate(X_train, y_train)","3e9fefb1":"model.evaluate(X_test, y_test)","953d5a41":"from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, roc_auc_score\n\ny_train_pred = (model.predict(X_train) >= 0.5).astype(\"int32\")  \n\nprint(confusion_matrix(y_train, y_train_pred))\nprint(\"-------------------------------------------------------\")\nprint(classification_report(y_train, y_train_pred))","46d9b29d":"y_pred = (model.predict(X_test) >= 0.5).astype(\"int32\")\n\nprint(confusion_matrix(y_test, y_pred))\nprint(\"-------------------------------------------------------\")\nprint(classification_report(y_test, y_pred))","b94ce9bb":"from sklearn.metrics import precision_recall_curve, average_precision_score\n\ny_pred_proba = model.predict(X_test)\nprecision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba)\n\n# plt.plot([1, 0], [0, 1],'k--')\nplt.plot(precision, recall)\nplt.xlabel('precision')\nplt.ylabel('recall')\nplt.title('Precision Recall Curve')\nplt.show()","e4690c10":"from sklearn.metrics import precision_recall_curve, average_precision_score, recall_score\n\nDL_AP = average_precision_score(y_test, y_pred_proba)\nDL_f1 = f1_score(y_test, y_pred)\nDL_rec = recall_score(y_test, y_pred)","126706a2":"review1 = \"Love this dress\"\nreview2 = \"Absolutely wonderful. silky and sexy and comfortable\"\nreview3 = \"i initially ordered the petite small (my usual size) but i found this to be outrageously small. so small in fact that i could not zip it up!\"\nreview4 = \"I love, love, love this jumpsuit. it's fun, flirty, and fabulous! every time i wear it, i get nothing but great compliments!\"\nreview5 = 'This shirt is very flattering to all due to the adjustable front tie. it is the perfect length to wear with leggings and it is sleeveless so it pairs well with any cardigan. love this shirt!!!'\nreview6 = 'I love tracy reese dresses, but this one is not for the very petite. i am just under 5 feet tall and usually wear a 0p in this brand. this dress was very pretty out of the package but its a lot of dress.'\nreview7 = 'I love this dress. i usually get an xs but it runs a little snug in bust so i ordered up a size. very flattering and feminine with the usual retailer flair for style.'\nreview8 = 'Dress runs small esp where the zipper area runs. i ordered the sp which typically fits me and it was very tight! the material on the top looks and feels very cheap that even just pulling on it will cause it to rip the fabric. pretty disappointed as it was going to be my christmas dress this year! needless to say it will be going back.'\nreview9 =  \"if you are at least average height or taller, this may look good on you.\"\nreview10 = \"sadly will be returning, but i'm sure i will find something to exchange it for!\"\nreview11 = \"Cute little dress fits tts. it is a little high waisted. good length for my 5'9 height. i like the dress, i'm just not in love with it. i dont think it looks or feels cheap. it appears just as pictured.\"\nreview12 = 'Loved the material, but i didnt really look at how long the dress was before i purchased both a large and a medium. im 5\\'5\" and there was atleast 5\" of material at my feet. the gaps in the front are much wider than they look. felt like the dress just fell flat. both were returned. im usually a large and the med fit better. 36d 30 in jeans'\nreview13 = \"I have been waiting for this sweater coat to ship for weeks and i was so excited for it to arrive. this coat is not true to size and made me look short and squat.\"\nreview14 = 'Very comfortable, material is good, cut out on sleeves flattering'\nreviews = [review1, review2, review3, review4, review5, review6, review7, review8, review9, review10, review11, review12, review13, review14]","a718071b":"tokens = tokenizer.texts_to_sequences(reviews) ","312124d5":"tokens_pad = pad_sequences(tokens, maxlen=max_tokens)\ntokens_pad.shape","caf21f9f":"mod_pred = model.predict(tokens_pad)","f8189184":"mod_pred","c07c21ad":"df_pred = pd.DataFrame(mod_pred, index=reviews)\ndf_pred.rename(columns={0: 'Pred_Proba'}, inplace=True)","3247d0de":"df_pred[\"Predicted_Feedbaack\"] = df_pred[\"Pred_Proba\"].apply(lambda x: \"Recommended\" if x>=0.5 else \"Not Recommended\")","8b39af7e":"df_pred","757b3ce8":"compare = pd.DataFrame({\"Model\": [\"NaiveBayes_count\", \"LogReg_count\", \"SVM_count\", \"Random Forest_count\", \n                                  \"AdaBoost_count\", \"NaiveBayes_tfidf\", \"LogReg_tfidf\", \"SVM_tfidf\", \n                                  \"Random Forest_tfidf\", \"AdaBoost_tfidf\", \"DL\"],\n                        \n                        \"F1_Score\": [nb_count_f1, log_count_f1, svc_count_f1,\n                                             rf_count_f1, ada_count_f1, nb_tfidf_f1, log_tfidf_f1,\n                                             svc_tfidf_f1, rf_tfidf_f1, ada_tfidf_f1, DL_f1],\n                        \n                        \"Recall_Score\": [nb_count_rec, log_count_rec, svc_count_rec, \n                                                   rf_count_rec, ada_count_rec, \n                                                  nb_tfidf_rec, log_tfidf_rec, svc_tfidf_rec, \n                                                  rf_tfidf_rec, ada_tfidf_rec, DL_rec],\n                        \n                        \"Average_Precision_Score\": [nb_AP_count, log_AP_count, svc_AP_count, rf_AP_count,\n                                                   ada_AP_count, nb_AP_tfidf, log_AP_tfidf, svc_AP_tfidf,\n                                                   rf_AP_tfidf, ada_AP_tfidf, DL_AP]})\n\ndef labels(ax):\n                        \n    for p in ax.patches:\n        width = p.get_width()                        # get bar length\n        ax.text(width,                               # set the text at 1 unit right of the bar\n                p.get_y() + p.get_height() \/ 2,      # get Y coordinate + X coordinate \/ 2\n                '{:1.3f}'.format(width),             # set variable to display, 2 decimals\n                ha = 'left',                         # horizontal alignment\n                va = 'center')                       # vertical alignment\n    \nplt.figure(figsize=(15,30))\nplt.subplot(311)\ncompare = compare.sort_values(by=\"Recall_Score\", ascending=False)\nax=sns.barplot(x=\"Recall_Score\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\n\nplt.subplot(312)\ncompare = compare.sort_values(by=\"F1_Score\", ascending=False)\nax=sns.barplot(x=\"F1_Score\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\n\n\nplt.subplot(313)\ncompare = compare.sort_values(by=\"Average_Precision_Score\", ascending=False)\nax=sns.barplot(x=\"Average_Precision_Score\", y=\"Model\", data=compare, palette=\"Blues_d\")\nlabels(ax)\nplt.show();","6e108771":"**SPECIAL NOTE: Even if we do NOT need to implement this kind of a detailed EDA process for NLP since we are just interested in the columns of \"Review Text\" and \"Recommended IND\";nevertheless, we will implement a detailed EDA to make the reader be more familiar with the dataset.**   ","1e2d5be5":"**Let's create Word Cloud for most common words in recommended not recommended reviews separately.**","cf119449":"<a id=\"9.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">9.2 Naive Bayes<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","3d700afc":"<a id=\"5.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.1 Feature Selection<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","f6cfed24":"**Let's create DataFrame for visually a better understanding.**","be2afd38":"<a id=\"9.3.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>9.3.a Support Vector Machine (SVM) With Count Vectorizor<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","af10ecbb":"<a id=\"4\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">4) EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","c47c0f03":"**Now it's time to train all models using TFIDF and Count vectorizer data.**","1268593e":"**The Examination of \"class_name\" Variable**\n\n- \"class_name\" is a Categorical variable of the product class name.","0a7e564e":"<a id=\"12\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">12) COMPARING THE MODELS<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","44aa03df":"**Let's convert reviews above to numeric by tokenizing.**","600252f9":"Now we'll create a Word Clouds for reviews, representing most common words in each target class.\n\nWord Cloud is a data visualization technique used for representing text data in which the size of each word indicates its frequency or importance. Significant textual data points can be highlighted using a word cloud.\n\nWe are expected to create separate word clouds for positive and negative reviews. As such we can qualify a review as positive or negative, by looking at its recommended status.\n\nWe can follow the steps below:\n\n- Detect Reviews\n- Collect Words \n- Create Word Cloud ","475416b3":"<a id=\"9.2.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>9.2.a Naive Bayes With Count Vectorizor<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","fb72ae9f":"<a id=\"5\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">5) FEATURE SELECTION & DATA CLEANING<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\nFrom now on, the DataFrame we will work with should contains two columns: **\"Review Text\"** and **\"Recommended IND\"**. We can do the missing value detection operations from now on.","07892309":"<a id=\"7.3\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">7.3 Creating of Word Cloud<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","7a4e4917":"<a id=\"10.8\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">10.8 Model Evaluation<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","478358ba":"**In the next step, we will create a numerical feature vector for each document:**","7dac9654":"Text is the most unstructured form of all the available data, therefore various types of noise are present in it. This means that the data is not readily analyzable without any pre-processing. The entire process of cleaning and standardization of text, making it noise-free and ready for analysis is known as **text preprocessing**.\n\nThe **three key steps** of text preprocessing:\n\n- **Tokenization:**\nThis step is one of the top priorities when it comes to working on text mining. Tokenization is essentially splitting a phrase, sentence, paragraph, or an entire text document into smaller units, such as individual words or terms. Each of these smaller units are called tokens.\n\n- **Noise Removal:**\nAny piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\nFor example \u2013 language stopwords (commonly used words of a language \u2013 is, am, the, of, in etc), URLs or links, upper and lower case differentiation, punctuations and industry specific words. This step deals with removal of all types of noisy entities present in the text.\n\n\n- **Lexicon Normalization:**\nAnother type of textual noise is about the multiple representations exhibited by single word.\nFor example \u2013 \u201cplay\u201d, \u201cplayer\u201d, \u201cplayed\u201d, \u201cplays\u201d and \u201cplaying\u201d are the different variations of the word \u2013 \u201cplay\u201d. Though they mean different things, contextually they all are similar. This step converts all the disparities of a word into their normalized form (also known as lemma). \nThere are two methods of lexicon normalisation; **[Stemming or Lemmatization](https:\/\/www.guru99.com\/stemming-lemmatization-python-nltk.html)**. Lemmatization is recommended for this case, because Lemmatization as this will return the root form of each word (rather than just stripping suffixes, which is stemming).\n\nAs the first step change text to tokens and convertion all of the words to lower case.  Next remove punctuation, bad characters, numbers and stop words. The second step is aimed to normalization them throught the Lemmatization method. \n\n\n***Note:*** *Use the functions of the ***[nltk Library](https:\/\/www.guru99.com\/nltk-tutorial.html)*** for all the above operations.*\n\n","72215d03":"![image-2.png](attachment:image-2.png)\n\n**Image credit:** [Freepik](https:\/\/www.freepik.com\/free-photo\/clothes-hang_1240043.htm#page=1&position=0&from_view=detail#&position=0&from_view=detail#position=0)","8a2669e0":"<a id=\"7\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">7) WORLDCLOUD - REPETITION OF WORDS<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","feccd4c1":"**1) Clothing ID:** Integer Categorical variable that refers to the specific piece being reviewed.\n\n**2) Age:** Positive Integer variable of the reviewers age.\n\n**3) Title:** String variable for the title of the review.\n\n**4) Review Text:** String variable for the review body.\n\n**5) Rating:** Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n\n**6) Recommended IND:** Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n\n**7) Positive Feedback Count:** Positive Integer documenting the number of other customers who found this review positive.\n\n**8) Division Name:** Categorical name of the product high level division.\n\n**9) Department Name:** Categorical name of the product department name.\n\n**10) Class Name:** Categorical name of the product class name.","9efc60b4":"**The Examination of \"rating\" Variable**\n\n- \"rating\" is a Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.","ba5a6c13":"<a id=\"2.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">2.1 Context<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\nOutfits are an important element of a female character which emphasizes her attractiveness and creates the individual image. Clothing is a self-expression of a woman, her way of life. \n\nIn this context, the basic goal of this project is to predict whether customers, especially assumed as women, recommend the product they purchased using the information in their *Review Text*. Especially, it should be noted that the expectation in this project is to use only the \"Review Text\" variable and neglect the other ones. Of course, if you want, you can work on other variables individually.\n\nThe data is a collection of 22641 Rows and 10 column variables. Each row includes a written comment as well as additional customer information. Also each row corresponds to a customer review, and includes the variables. Because this is real commercial data, it has been anonymized, and references to the company in the review text and body have been replaced with \"retailer\".","8adca974":"<a id=\"9.5.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>9.5.b Ada Boosting With TF-IDF Vectorizer<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","999e5c0c":"<a id=\"1\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">1) LIBRARIES NEEDED IN THE STUDY<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","6ae6975e":"**The Examination of \"recommended_ind\" Variable**\n\n- \"recommended_ind\" is a binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.","534265b0":"# <p style=\"background-color:#9452a5;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">SENTIMENT ANALYSIS OF WOMEN'S CLOTHES REVIEWS<\/p>","f79aed60":"**We can use one of the following user defined functions for Tokenization, Noise Removal and Lexicon Normalization. Both do the same job. We prefered to implement the first one. It's depends on you which one you prefer.**","dc8780e2":"<a id=\"10.6\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">10.6 Train | Set & Split<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","d477c22c":"**Let's predict the sentiment of our reviews.**","77cf155e":"- https:\/\/www.kaggle.com\/andrewmvd\/heart-failure-clinical-data\n- https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_csv.html\n- https:\/\/www.researchgate.net\/publication\/49814836_Problematic_standard_errors_and_confidence_intervals_for_skewness_and_kurtosis\n- https:\/\/www.researchgate.net\/publication\/304577646_Young_consumers'_intention_towards_buying_green_products_in_a_developing_nation_Extending_the_theory_of_planned_behavior\n- https:\/\/imaging.mrc-cbu.cam.ac.uk\/statswiki\/FAQ\/Simon\n- https:\/\/www.researchgate.net\/publication\/263372601_Resistance_motivations_trust_and_intention_to_use_mobile_financial_services\n- https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\n- https:\/\/machinelearningmastery.com\/power-transforms-with-scikit-learn\/\n- https:\/\/en.wikipedia.org\/wiki\/Dummy_variable_(statistics) \n- https:\/\/www.displayr.com\/what-are-dummy-variables\/ \n- https:\/\/stattrek.com\/multiple-regression\/dummy-variables.aspx \n- https:\/\/www.statisticshowto.com\/dummy-variables\/\n- https:\/\/en.wikipedia.org\/wiki\/Feature_scaling\n- https:\/\/www.dataschool.io\/comparing-supervised-learning-algorithms\/\n- https:\/\/machinelearningmastery.com\/handle-missing-data-python\/\n- https:\/\/www.kaggle.com\/kaanboke\/the-most-used-methods-to-deal-with-missing-values\n- https:\/\/www.kaggle.com\/karnikakapoor\/fetal-health-classification\n- https:\/\/www.kaggle.com\/karnikakapoor\/heart-failure-prediction-ann\n- https:\/\/www.kaggle.com\/kaanboke\/feature-selection-the-most-common-methods-to-know\n- https:\/\/www.kaggle.com\/kaanboke\/the-most-common-evaluation-metrics-a-gentle-intro\n- https:\/\/www.kaggle.com\/kaanboke\/beginner-friendly-end-to-end-ml-project-enjoy","0832aac9":"<a id=\"10.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">10.2 Creating Word Index<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","3146f014":"<a id=\"5.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">5.2 Detecting & Handling With Missing Values<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","5c1aa65c":"**Let's pad the tokenized reviews.**","74088460":"def cleaning(data):\n    \n    #1. Tokenize\n    text_tokens = word_tokenize(data.replace(\"'\", \"\").lower()) \n     # ayra\u00e7lar\u0131 kald\u0131r\u0131yorum ki (can't gibi) olumsuzluk i\u00e7eren kelimeler i\u00e7inde ayra\u00e7 i\u00e7erdi\u011fi i\u00e7in stopwords lerde silinmesin.\n        \n    #2. Remove Puncs\n    tokens_without_punc = [w for w in text_tokens if w.isalpha()]  # noktalama i\u015faretlerinden temizleme\n    \n    #3. Removing Stopwords\n    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]  # stopword lerden temizleme\n    # tokenleri teker teker al stopwords ler i\u00e7inde yoksa oldu\u011fu gibi yaz, yani stopwords ise yazma.\n    \n    #4. lemma\n    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n    \n    #joining\n    return \" \".join(text_cleaned)","487970d5":"**Before diving into modelling, we will create a User-Defined-Function for comparing models at the end.**","e36ce15f":"**The Examination of \"division_name\" Variable**\n\n- \"division_name\" is Categorical name of the product high level division.","96289c34":"<a id=\"8.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.2 Vectorization<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","3b0afe74":"# remove all punctuation\nimport string\ndf1.reviewText = df1.reviewText.str.translate(str.maketrans('', '', string.punctuation))","93354e5b":"<a id=\"2\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">2) Data<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","d9d1d7a9":"<a id=\"10.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">10.1 Tokenization<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","2dff56c0":"<a id=\"9.4.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>9.4.a Random Forest With Count Vectorizer<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","52f74768":"<a id=\"4.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.2 - The Examination of Target Variable<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","d010c671":"<a id=\"11\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">11) PREDICTION<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","6726c1ed":"<a id=\"10.4\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">10.4 The Determination of Maximum Number of Tokens<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","f3bd1c8e":"<a id=\"1.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">1.1 User Defined Functions<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \n**We have defined some useful user defined functions.**","6b23fd8e":"**In this project we have used sentiment analysis to determine whether the product is recommended or not. We have used different machine learning algorithms to get more accurate predictions and deep learning algorithm for comparing it with machine learning models. The following classification algorithms have been used: Logistic Regression, Naive Bayes, Support Vector Machine (SVM), Random Forest and Ada Boosting. The dataset comes from Woman Clothing Review that can be find at [Kaggle Website](https:\/\/www.kaggle.com\/nicapotato\/womens-ecommerce-clothing-reviews).**\n\n**In general, when we compare the models it's hard to decide which model can be picked up among the ones that have been sharing top 5 since their scores are very close to each other. However, Ada Boosting, Naive Bayes, Deep Learning, SVM and LR's scores interchangeably look like better than other models' scores. There is no simple answer to the question of which one is better; each work better in different data sets and conditions. Each modelling algorithm has some pros and cons to each other. So we could select one of these algorithms in consistent with what we need, accuracy or precision.** \n\n**NOTE: You can reach a satisfactory article at [Here](http:\/\/www.emis.de\/journals\/RCE\/V35\/v35n2a03.pdf) for a better understanding of which one, LR or SVM, can be prefered in which conditions and needs.** ","0ea8fed5":"<a id=\"9.1.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>9.1.b Logistic Regression With TF-IDF Vectorizer<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","a376ea16":"- Kline, R.B. (2011). Principles and practice of structural equation modeling (5th ed., pp. 3-427). New York:The Guilford Press.\n- Edwards, A. (1976). An introduction to linear regression and correlation. W. H. Freeman\n- Everitt, B. S.; Skrondal, A. (2010), The Cambridge Dictionary of Statistics, Cambridge University Press.\n- https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1\n- https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W\n- https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1\n- https:\/\/www.amazon.com\/Introduction-Machine-Learning-Python-Scientists\/dp\/1449369413\n- Neural Networks from Scratch in Python (by Kinsley \u00a7 Kukiela) [external link text](https:\/\/nnfs.io\/)\n- Practical Statistics for Data Scientists (by Bruce & Gedeck) [external link text](https:\/\/www.amazon.com\/Practical-Statistics-Data-Scientists-Essential\/dp\/149207294X\/ref=sr_1_1?dchild=1&keywords=Practical+Statistics+for+Data+Scientists&qid=1627662007&sr=8-1)\n- Applications of Deep Neural Networks(by Jeff Heaton) [external link text](https:\/\/arxiv.org\/abs\/2009.05673)\n- Applied Predictive Modeling (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Applied-Predictive-Modeling-Max-Kuhn\/dp\/1461468485\/ref=pd_sbs_3\/141-4288971-3747365?pd_rd_w=AOIS7&pf_rd_p=3676f086-9496-4fd7-8490-77cf7f43f846&pf_rd_r=MCCHJXWK39VD6VW7RVAR&pd_rd_r=4ffcd1ea-44b9-4f33-b9b3-dc02ee159662&pd_rd_wg=nU1Ex&pd_rd_i=1461468485&psc=1:)\n- Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow (by Aur\u00e9lien G\u00e9ron) [external link text](https:\/\/www.amazon.com\/Hands-Machine-Learning-Scikit-Learn-TensorFlow\/dp\/1492032646\/ref=sr_1_1?crid=2GV554Q2EKD1E&dchild=1&keywords=hands-on+machine+learning+with+scikit-learn%2C+keras%2C+and+tensorflow&qid=1627628294&s=books&sprefix=hands%2Cstripbooks-intl-ship%2C309&sr=1-1)\n- Master Machine Learning Algorithms (by Brownlee, ML algorithms are very well explained ) [external link text](https:\/\/machinelearningmastery.com\/master-machine-learning-algorithms\/)\n- Python Feature Engineering Cookbook (by Galli) [external link text](https:\/\/www.amazon.com\/Python-Feature-Engineering-Cookbook-transforming\/dp\/1789806313\/ref=sr_1_1?dchild=1&keywords=feature+engineering+cookbook&qid=1627628487&s=books&sr=1-1)\n- Feature Engineering Made Easy (by Ozdemir & Susarla) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Made-Easy-Identify-ebook\/dp\/B077N6MK5W)\n- Feature Engineering and Selection (by Kuhn & Johnson) [external link text](https:\/\/www.amazon.com\/Feature-Engineering-Selection-Chapman-Science\/dp\/1032090855\/ref=sr_1_1?crid=19T9G95E1W7VJ&dchild=1&keywords=feature+engineering+and+selection+kuhn&qid=1628050948&sprefix=feature+engineering+and+%2Cdigital-text%2C293&sr=8-1)\n- Imbalanced Classification with Python(by Brownlee) [external link text](https:\/\/machinelearningmastery.com\/imbalanced-classification-with-python\/)","12d88657":"**The Examination of \"positive_feedback_count\" Variable**\n\n- \"positive_feedback_count\" is Positive Integer documenting the number of other customers who found this review positive.","5368f669":"**For later parts of the analysis, we will drop unnecassary columns for NLP.**","25e872bf":"<a id=\"8\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">8) SENTIMENT CLASSIFICATION WITH MACHINE LEARNING & DEEP LEARNING<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","51025cc3":"<a id=\"9.3.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>9.3.b Support Vector Machine (SVM) With TF-IDF Vectorizer<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","cf3beaec":"<a id=\"3\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">3) ANALYSIS<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","98497b0a":"<a id=\"7.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">7.1 The Detection of Positive and Negative Reviews<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","6cbff9da":"<a id=\"toc\"><\/a>\n\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">TABLE OF CONTENTS<p>\n\n* [   PREFACE](#0)\n* [1) LIBRARIES NEEDED IN THE STUDY](#1)\n    * [1.1 User Defined Functions](#1.1)\n* [2) DATA](#2)\n    * [2.1 Context](#2.1)\n    * [2.2 About the Features](#2.2) \n    * [2.3 What the Problem is](#2.3) \n    * [2.4 Target Variable](#2.3) \n* [3) ANALYSIS](#3)\n    * [3.1) Reading the Data](#3)\n* [4) EXPLORATORY DATA ANALYSIS (EDA) & VISUALIZATION](#4)\n    * [4.1 A General Looking at the Data](#4.1)\n    * [4.2 - The Examination of Target Variable](#4.2)\n    * [4.3 - The Examination of Other Features](#4.3)\n* [5) FEATURE SELECTION & DATA CLEANING](#5)    \n    * [5.1 Feature Selection](#5.1)\n    * [5.2 Detecting & Handling With Missing Vaalues](#5.2)     \n* [6) TEXT MINING](#6)\n    * [6.1 Tokenization, Noise Removal & Lexicon Normalization](#6.1)\n    * [6.2 Handling With Rare Words](#6.2)    \n* [7) WORLDCLOUD - REPETITION OF WORDS](#7)    \n    * [7.1 The Detection of Positive and Negative Reviews](#7.1)\n    * [7.2 The Collection of Positive and Negative Words](#7.2)\n    * [7.3 Creating of Word Cloud](#7.3)\n* [8) SENTIMENT CLASSIFICATION WITH MACHINE LEARNING & DEEP LEARNING](#8)    \n    * [8.1 Train | Test & Split](#8.1)\n    * [8.2 Vectorization](#8.2)\n        * [8.2.a Count Vectorization](#8.2.a)\n        * [8.2.b TF-IDF Vectorization](#8.2.b)\n* [9) MACHINE LEARNING MODELLING](#9)    \n    * [9.1 Logistic Regression](#9.1)\n        * [9.1.a Logistic Regression With Count Vectorizer](#9.1.a)\n        * [9.1.b Logistic Regression With TF-IDF Vectorizer](#9.1.b)\n    * [9.2 Naive Bayes](#9.2)\n        * [9.2.a Naive Bayes With Count Vectorizer](#9.2.a)\n        * [9.2.b Naive Bayes With TF-IDF Vectorizer](#9.2.b)\n    * [9.3 Support Vector Machine (SVM)](#9.3)\n        * [9.3.a Support Vector Machine (SVM) With Count Vectorizer](#9.3.a)\n        * [9.3.b Support Vector Machine (SVM) With TF-IDF Vectorizer](#9.3.b)\n    * [9.4 Random Forest](#9.4)\n        * [9.4.a Random Forest With Count Vectorizer](#9.4.a)\n        * [9.4.b Random Forest With TF-IDF Vectorizer](#9.4.b)\n    * [9.5 Ada Boosting](#9.5)\n        * [9.5.a Ada Boosting With Count Vectorizer](#9.5.a)\n        * [9.5.b Ada Boosting With TF-IDF Vectorizer](#9.5.b)\n* [10) DEEP LEARNING MODELLING](#10)    \n    * [10.1 Tokenization](#10.1)\n    * [10.2 Creating Word Index](#10.2)   \n    * [10.3 Converting Tokens To Numeric](#10.3)\n    * [10.4 The Determination of Maximum Number of Tokens](#10.4)   \n    * [10.5 Fixing Token Counts of All documents (Pad Sequences)](#10.5)\n    * [10.6 Train | Set & Split](#10.6)   \n    * [10.7 Modeling](#10.7)\n    * [10.8 Model Evaluation](#10.8)   \n* [11) PREDICTION](#11) \n* [12) THE COMPARISON OF MODELS](#12) \n* [13) CONLUSION](#13)\n* [14) REFERENCES](#14)\n* [15) FURTHER READINGS](#15)","a6d73099":"<a id=\"4.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.1 - A General Looking at the Data<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n    \nPandas profiling is an open source Python module with which we can quickly do an exploratory data analysis with just a few lines of code. Similarly, for EDA, profile_report() is One-Line Magical Code creating reports in the interactive HTML format which is quite easy to understand and analyze the data. In short, at the first hand, what pandas profiling does is to save us all the work of visualizing and understanding the distribution of each variable.\n\n**For a better understanding and more information, please refer to [Source 1](https:\/\/www.analyticsvidhya.com\/blog\/2021\/06\/generate-reports-using-pandas-profiling-deploy-using-streamlit\/) & [Source 2](https:\/\/towardsdatascience.com\/exploratory-data-analysis-with-pandas-profiling-de3aae2ddff3)**","c15e2099":"<a id=\"8.2.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>8.2.b TF-IDF Vectorization<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","b9491dcc":"<a id=\"9.2.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>9.2.b Naive Bayes With TF-IDF Vectorizer<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","bbf206b3":"<a id=\"9.3\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">9.3 Support Vector Machine (SVM)<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","11fa2e37":"<a id=\"4.3\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">4.3 - The Examination of Other Features<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","5f87e554":"**Let's Check Proportion of Target Class Variable:**\n\nThe target class variable is imbalanced, where \"Recommended\" values are more dominating then \"Not Recommendation\".","1f4306f9":"**As you noticed that the target variable,** \"recommended_ind\"**, in this study is imblanced so this researcher will concentrate Recall score on evaluating the results rather than Accuracy score used in the evaluation of balanced data.**","215daa76":"<a id=\"10.3\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">10.3 Converting Tokens To Numeric<p>\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","b327b325":"<a id=\"9.4.b\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>9.4.b Random Forest With TF-IDF Vectorizer<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","857c8a5a":"<a id=\"9.5.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>9.5.a Ada Boosting With TF-IDF Vectorizer<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","b919643b":"<a id=\"9.4\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">9.4 Random Forest<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","300ee384":"<a id=\"10.7\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">10.7 Modeling<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","4a6fa4c8":"<a id=\"6.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.2 Handling With Rare Words<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","4db415c3":"<a id=\"10\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">10) DEEP LEARNING MODELLING<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","804f0f78":"<a id=\"10.5\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">10.5 Fixing Token Counts of All documents (Pad Sequences)<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","f2c5d5d3":" **In the proces above, we have prefered to rename the column names.**","3232cfd3":"<a id=\"15\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">15) FURTHER READINGS<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","e2256826":"**Let's visually compare the models' F1 Scores, Recall Scores and Average Precision Score.**","678101f3":"<a id=\"7.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">7.2 The Collection of Positive and Negative Words<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","6472ddc1":"**The Examination of \"age\" Variable**\n\n- \"age\" is a Positive Integer variable of the reviewers age.","6f87b574":"Before moving on to modeling, as data preprocessing steps WE will need to perform **[vectorization](https:\/\/machinelearningmastery.com\/prepare-text-data-machine-learning-scikit-learn\/)** and **train-test split**. \nBut you will perform the vectorization for the first time.\n\nMachine learning algorithms most often take numeric feature vectors as input. Thus, when working with text documents, we need a way to convert each document into a numeric vector. This process is known as text vectorization. Commonly used vectorization approach that we will use here is to represent each text as a vector of word counts.\n\nAt this moment, we have our review text column as a token (which has no punctuations and stopwords). We can use Scikit-learn\u2019s CountVectorizer to convert the text collection into a matrix of token counts. We can imagine this resulting matrix as a 2-D matrix, where each row is a unique word, and each column is a review.\n\n**For Deep learning model, we will implement embedding layer for all words.** \n\nAfter performing data preprocessing, we will build your models using following classification algorithms:\n\n- Logistic Regression,\n- Naive Bayes,\n- Support Vector Machine,\n- Random Forest,\n- Ada Boosting\n- Deep Learning Model.","b8533350":"<a id=\"9.5\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">9.5 Ada Boosting<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","088fb80f":"**To run machine learning algorithms we need to convert text files into numerical feature vectors. We will use bag of words model for our analysis.**\n\n**Let's first we split the data into train and test sets:**","218f0d32":"<a id=\"0\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">PREFACE<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\nWelcome to the \"***Sentiment Analysis and Classification Project***\".\n\nThis project will focus on using Natural Language Processing (NLP) techniques to find broad trends in the written thoughts of the customers. \nThe goal in this project is to predict whether customers recommend the product they purchased using the information in their review text.\n\nOne of the challenges in this project is to extract useful information from the **\"Review Text\"** variable using text mining techniques. The other challenge is that we need to convert text files into numeric feature vectors to run machine learning algorithms.\n\nAt the end of this project, you will be familiar in the context of NLP with (A) what any beginner in Machine Learning and Deep Learning can do as much as possible for a better understanding with the given dataset not only by examining its various aspects by a detailed EDA process but also visualising it and (B) how to build sentiment classification models using **Machine Learning algorithms** (Logistic Regression, Naive Bayes, Support Vector Machine, Random Forest and Ada Boosting) and **Deep Learning algorithms.**\n\nBefore diving into the project, please take a close look at <a href=\"#2.2\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">the features<\/a> for a better understanding of the dataset.\n\n- ***NOTE:*** *This project assumes that you already know the basics of coding in Python and are familiar with the theory behind the algorithms mentioned above as well as NLP techniques.*\n\nThe dataset comes from Woman Clothing Review that can be find at [Kaggle Website](https:\/\/www.kaggle.com\/nicapotato\/womens-ecommerce-clothing-reviews)","87698aea":"**Sometimes we cannot detect missing values if they consist of empty (blank) string such as \" \". In this situation we can use the following syntax.** ","3de4d5bf":"<a id=\"2.2\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">2.2 About the Features<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","2bac41bc":"<a id=\"6\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">6) TEXT MINING<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","26f1f3f2":"<a id=\"13\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">13) CONCLUSION<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","5dd923f3":"<a id=\"8.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">8.1 Train | Test & Split<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","0c1c1bde":"<a id=\"8.2.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>8.2.a Count Vectorization<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","860d23c2":"<a id=\"9.1.a\"><\/a>\n<font color=\"lightseagreen\" size=+0.5><b>9.1.a Logistic Regression With Count Vectorizor<\/b><\/font>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","d2fffae6":"**This part is the preparation for padding.**","35b98124":"<a id=\"9\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">9) MACHINE LEARNING MODELLING<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","691915cb":"<a id=\"6.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">6.1 Tokenization, Noise Removal & Lexicon Normalization<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","90bec486":"<a id=\"9.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">9.1 Logistic Regression<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","12689fbf":"### Collect Words (positive and negative separately)","b19a8c1a":"<a id=\"3.1\"><\/a>\n### <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:LEFT; border-radius:10px 10px;\">3.1 Reading the Data<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**How to read and assign the dataset as df. [Visit Here](https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.read_csv.html) (You can define it as what you want instead of df)**","1e7cf299":"**The Examination of \"department_name\" Variable**\n\n- \"department_name\" is a Categorical variable of the product department name.","4c98dc6c":"<a id=\"14\"><\/a>\n## <p style=\"background-color:#9452a5; font-family:newtimeroman; color:#FFF9ED; font-size:150%; text-align:center; border-radius:10px 10px;\">14) REFERANCES<p>\n\n<a href=\"#toc\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" \nstyle=\"color:blue; background-color:#dfa8e4\" data-toggle=\"popover\">Table of Contents<\/a>","fc2801cf":"**Let's create categorical and numerical sets for the examination of crosstab information.**"}}