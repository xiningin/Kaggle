{"cell_type":{"8a10602e":"code","c406f41d":"code","d021c685":"code","50ec34a1":"code","fffad85c":"code","aadc2dd6":"code","984b9c82":"code","1f333484":"code","b66be381":"code","f7af5778":"code","24c98426":"code","6439a1bb":"code","9272c5b2":"code","58291fb4":"code","5a617f21":"markdown","06398b02":"markdown"},"source":{"8a10602e":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\n\nimport gc\n\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import activations,callbacks\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import initializers\n\nfrom keras.models import Model\n","c406f41d":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv\")\nsubmission = submission.set_index('id')\n","d021c685":"targets = pd.get_dummies(train['target'])","50ec34a1":"def custom_metric(y_true, y_pred):\n    y_pred = K.clip(y_pred, 1e-15, 1-1e-15)\n    loss = K.mean(cce(y_true, y_pred))\n    return loss\n\ncce = tf.keras.losses.CategoricalCrossentropy()\n\nes = tf.keras.callbacks.EarlyStopping(\n    monitor='val_custom_metric', min_delta=1e-05, patience=5, verbose=0,\n    mode='min', baseline=None, restore_best_weights=True)\n\nplateau = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='val_custom_metric', factor=0.7, patience=2, verbose=0,\n    mode='min')","fffad85c":"def model_NN_by_row():\n    model_NN_by_row_inputs = layers.Input(shape = (75,))\n    embed = layers.Embedding(354, 7)(model_NN_by_row_inputs)\n    embed = layers.Flatten()(embed)\n    hidden = layers.Dropout(0.3)(embed)\n    hidden = tfa.layers.WeightNormalization(layers.Dense(units=64, activation='selu', kernel_initializer=\"lecun_normal\"))(hidden)\n    output = layers.Dropout(0.3)(layers.Concatenate()([embed, hidden]))\n    output = tfa.layers.WeightNormalization(layers.Dense(units=64, activation='relu',kernel_initializer=\"lecun_normal\"))(output) \n    output = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden, output]))  \n    output = tfa.layers.WeightNormalization(layers.Dense(units=64, activation='elu',kernel_initializer=\"lecun_normal\"))(output)\n    model_NN_by_row_outputs = layers.Dense(9, activation='softmax',kernel_initializer=\"lecun_normal\")(output)\n    \n    model = Model(model_NN_by_row_inputs,model_NN_by_row_outputs)\n    \n    return model","aadc2dd6":"def model_NN_by_column():\n    model_NN_by_column_inputs = layers.Input(shape = (75,))\n    b = layers.Reshape((-1,1))( model_NN_by_column_inputs)\n    b = layers.Embedding(354, 7)(b)\n    embed = layers.Flatten()(b)\n    hidden = layers.Dropout(0.3)(embed)\n    hidden = tfa.layers.WeightNormalization(layers.Dense(units=32, activation='selu', kernel_initializer=\"lecun_normal\"))(hidden)\n    output = layers.Dropout(0.3)(layers.Concatenate()([embed, hidden]))\n    output = tfa.layers.WeightNormalization(layers.Dense(units=32, activation='relu',kernel_initializer=\"lecun_normal\"))(output) \n    output = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden, output]))\n    output = tfa.layers.WeightNormalization(layers.Dense(units=32, activation='elu',kernel_initializer=\"lecun_normal\"))(output)\n    model_NN_by_column_outputs = layers.Dense(9, activation='softmax',kernel_initializer=\"lecun_normal\")(output)\n    \n    model = Model (model_NN_by_column_inputs,model_NN_by_column_outputs)\n    \n    return model","984b9c82":"def conv_model():\n\n    conv_inputs = layers.Input(shape = (75))\n    embed = layers.Embedding (input_dim=354, output_dim=7,\n                              embeddings_regularizer='l2')(conv_inputs)\n    embed = layers.Conv1D(12,1,activation='relu',)(embed)        \n    embed = layers.Flatten()(embed)\n    hidden = layers.Dropout(0.3)(embed)\n    hidden = tfa.layers.WeightNormalization(layers.Dense(units=32, activation='selu', kernel_initializer=\"lecun_normal\"))(hidden)\n    output = layers.Dropout(0.3)(layers.Concatenate()([embed, hidden]))\n    output = tfa.layers.WeightNormalization(layers.Dense(units=32, activation='relu',kernel_initializer=\"lecun_normal\"))(output) \n    output = layers.Dropout(0.4)(layers.Concatenate()([embed, hidden, output]))\n    output = tfa.layers.WeightNormalization(layers.Dense(units=32, activation='elu',kernel_initializer=\"lecun_normal\"))(output)\n    conv_outputs = layers.Dense(9, activation='softmax',kernel_initializer=\"lecun_normal\")(output)\n    \n    model = Model(conv_inputs,conv_outputs)\n    \n    return model","1f333484":"oof_NN_a = np.zeros((train.shape[0],9))\npred_NN_a = np.zeros((test.shape[0],9))\n\noof_NN_h = np.zeros((train.shape[0],9))\npred_NN_h = np.zeros((test.shape[0],9))\n\noof_NN_v = np.zeros((train.shape[0],9))\npred_NN_v = np.zeros((test.shape[0],9))\n\nNN_h_train_preds = []\nNN_h_test_preds = []\n\nNN_v_train_preds = []\nNN_v_test_preds = []\n\nNN_a_train_preds = []\nNN_a_test_preds = []\n\nN_FOLDS = 10\nSEED = 2021\nEPOCH = 50\n\n\nskf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (tr_idx, ts_idx) in enumerate(skf.split(train,train.iloc[:,-1])):\n    print(f\"\\n ====== TRAINING FOLD {fold} =======\\n\")\n\n    X_train = train.iloc[:,1:-1].iloc[tr_idx]\n    y_train = targets.iloc[tr_idx]\n    X_test = train.iloc[:,1:-1].iloc[ts_idx]\n    y_test = targets.iloc[ts_idx]\n\n    K.clear_session()\n\n    #================= NN CONV MODEL training =========\n    \n    print(\"\\n-----Convolution model Training----\\n\")\n\n    model_conv = conv_model()\n\n    model_conv.compile(loss='categorical_crossentropy', \n                            optimizer = keras.optimizers.Adam(learning_rate=2e-4), \n                            metrics=custom_metric)\n    model_conv.fit(X_train, y_train,\n              batch_size = 256, epochs = EPOCH,\n              validation_data=(X_test, y_test),\n              callbacks=[es, plateau],\n              verbose = 0)\n   \n    #============== Convolution Model prediction ==========\n\n    pred_a = model_conv.predict(X_test) \n    oof_NN_a[ts_idx] += pred_a \n    score_NN_a = log_loss(y_test, pred_a)\n    print(f\"\\nFOLD {fold} Score convolution model: {score_NN_a}\\n\")\n    pred_NN_a += model_conv.predict(test.iloc[:,1:]) \/ N_FOLDS \n\n \n    #================= By column MODELS training ==================\n\n    print(\"\\n-----By column NN model Training----\\n\")\n\n    model_by_col = model_NN_by_column()\n\n    model_by_col.compile(loss='categorical_crossentropy', \n                         optimizer = keras.optimizers.Adam(learning_rate=2e-4), \n                         metrics=custom_metric)\n\n    model_by_col.fit(X_train,y_train,\n            validation_data=(X_test,y_test),\n            epochs=EPOCH,\n            verbose=0,\n            batch_size = 256,\n            callbacks=[es,plateau])\n    \n    #============= By column NN Model prediction =============\n    \n    pred_col = model_by_col.predict(X_test)\n    oof_NN_v[ts_idx] = pred_col \n    score_NN_v = log_loss(y_test,  pred_col)\n    print(f\"\\nFOLD {fold} Score by column embedding model: {score_NN_v}\\n\")\n    pred_NN_v += model_by_col.predict(test.iloc[:,1:]) \/ N_FOLDS \n    \n   \n    #================= By row MODELS training ==================\n    \n    print(\"\\n-----By row NN model Training----\\n\")\n\n    model_by_r = model_NN_by_row()\n\n    model_by_r.compile(loss='categorical_crossentropy', \n                        optimizer = keras.optimizers.Adam(learning_rate=2e-4), \n                        metrics=custom_metric)\n\n    model_by_r.fit(X_train,y_train,\n            validation_data=(X_test,y_test),\n            epochs=EPOCH,\n            verbose=0,\n            batch_size = 256,\n            callbacks=[es,plateau])\n    \n     #============= By row NN Model prediction =============\n\n    pred_row = model_by_r.predict(X_test)\n    oof_NN_h[ts_idx] = pred_row \n    score_NN_h = log_loss(y_test, pred_row)\n    print(f\"\\nFOLD {fold} Score by row embedding model: {score_NN_h}\\n\")\n    pred_NN_h += model_by_r.predict(test.iloc[:,1:]) \/ N_FOLDS  \n    \n    # =========PREPROCESSING FOR FUTURE OPTIMIZATION===========\n    \n    NN_a_train_preds.append(oof_NN_a[ts_idx])\n    NN_a_test_preds.append(model_conv.predict(test.iloc[:,1:]))\n\n    NN_h_train_preds.append(oof_NN_h[ts_idx])\n    NN_h_test_preds.append(model_by_r.predict(test.iloc[:,1:]))\n\n    NN_v_train_preds.append(oof_NN_v[ts_idx])\n    NN_v_test_preds.append(model_by_col.predict(test.iloc[:,1:]))\n    #___________________________________________________________\n    \nscore_NN_h = log_loss(targets, oof_NN_h)\nprint(f\"\\n=== FINAL SCORE BY ROW EMBEDDING MODEL : {score_NN_h}===\\n\") \n\nscore_NN_v = log_loss(targets, oof_NN_v)\nprint(f\"\\n=== FINAL SCORE BY COLUMN EMBEDDING MODEL : {score_NN_v}===\\n\")\n    \nscore_a = log_loss(targets, oof_NN_a)\nprint(f\"\\n=== FINAL SCORE CONVOLUTION MODEL : {score_a}===\\n\") ","b66be381":"# From Ryan Barretto notebook :\ntrain_features = train.drop(['target', 'id'], axis=1).values\ntest_features = test.drop('id', axis=1).values\ntarget = train['target'].values","f7af5778":"def class_to_num(classes):\n    return [int(word[-1]) for word in classes]\n\ndef num_to_class(nums):\n    return ['Class_' + str(num) for num in nums]\n\nlabels = np.array(class_to_num(target))\n\nlabels = pd.DataFrame(labels)","24c98426":"y_valids = []\nfor fold, (train_index, test_index) in enumerate(skf.split(train_features, labels)):\n        \n    y_valid = labels.iloc[test_index]\n    y_valids.append(y_valid)","6439a1bb":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom scipy.optimize import minimize\nscores = []\nweights = []\nfor y, NN_v_pred, NN_h_pred, NN_a_pred in zip(y_valids, \n                                        NN_v_train_preds, \n                                        NN_h_train_preds, \n                                        NN_a_train_preds,\n                                        ):\n    preds = []\n    preds.append(NN_v_pred)\n    preds.append(NN_h_pred)\n    preds.append(NN_a_pred)\n    \n    def log_weight_loss(weights):\n        weighted_pred = (weights[0]*preds[0]) + (weights[1]*preds[1]) + (weights[2]*preds[2])\n        return log_loss(y, weighted_pred)\n    starting_values = [0.4]*len(preds) \n    cons = ({'type':'eq','fun':lambda w: 1-sum(w)})\n    bounds = [(0,1)]*len(preds) \n    res = minimize(log_weight_loss, starting_values, method='Nelder-Mead', bounds=bounds, constraints=cons)\n    \n    weights.append(res['x'])\n    print(res['fun'])\n    scores.append(res['fun'])","9272c5b2":"folds = N_FOLDS\nfinal_weights = sum(weights)\/folds\nweighted_preds = np.array((final_weights[0] * sum(np.array(NN_v_test_preds)\/folds))\n                           +(final_weights[1] * sum(np.array(NN_h_test_preds)\/folds))\n                           +(final_weights[2] * sum(np.array(NN_a_test_preds)\/folds)))","58291fb4":"submission[['Class_1', 'Class_2', 'Class_3', 'Class_4','Class_5','Class_6','Class_7','Class_8','Class_9']] = weighted_preds\nsubmission.to_csv('submission_NN_blending4.csv')","5a617f21":"![image.png](attachment:cba89395-3416-462e-b417-6405cb9a55d9.png)","06398b02":"<h3> Adapted from Alexander Ryzhkov python translation of Oscar Villarreal Escamilla Notebook"}}