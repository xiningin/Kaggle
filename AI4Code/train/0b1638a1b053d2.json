{"cell_type":{"e5e64654":"code","c608de2d":"code","1fb90f5c":"code","89ba8bd4":"code","a23f494c":"code","f2342507":"code","d4619334":"code","2e23703b":"code","3b98126c":"code","3327874f":"code","c74d8099":"code","42a2bdd0":"code","2bb73932":"code","0d115809":"code","5becc6ab":"code","fdf6ca58":"code","648e6de5":"code","f42dea2a":"code","65643cea":"code","7d5b701f":"code","8d4ef5e0":"code","57e93ba1":"code","aa995b58":"code","5848b19d":"code","aff0111a":"code","3c9034b4":"code","fda5db2e":"code","a3257ccd":"code","d9c7895d":"code","089c7d70":"code","71d30329":"code","d5e00756":"code","7ecc9468":"code","a96631ca":"code","cd8abf89":"code","43a1cf06":"code","d27c7051":"code","b636d8c7":"code","1c83944d":"code","764d855b":"code","0d0b9071":"code","3292b58a":"code","474f4dd9":"code","dbaff7cc":"code","6f92a920":"code","10747ba2":"code","1b2bcb8a":"code","86d4256b":"code","307f1071":"code","54f7f637":"code","6438a4a7":"code","60893171":"code","f8128a36":"code","0c6bf6f4":"code","c8f6a474":"code","0abf148a":"markdown","50210d71":"markdown","640fb090":"markdown","3d1da4b8":"markdown","fabbaaf2":"markdown","c7a9bd0f":"markdown","463506d9":"markdown","1287ff29":"markdown","50b84b19":"markdown","8e65cea2":"markdown","74dd8283":"markdown","8e6c702b":"markdown","5134c486":"markdown","cb114b03":"markdown","cd30b5af":"markdown","6d92979c":"markdown"},"source":{"e5e64654":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c608de2d":"main_df = pd.read_csv('\/kaggle\/input\/stock-sentiment-analysis\/Stock_Dataa.csv', encoding=\"ISO-8859-1\")\ndf = main_df.copy()","1fb90f5c":"# Top 5 rows of the dataset\ndf.head()","89ba8bd4":"# Reading some important libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas_profiling as pp\nimport warnings\nwarnings.filterwarnings('ignore')","a23f494c":"pp.ProfileReport(df)","f2342507":"# Dimesion of dataset\ndf.shape","d4619334":"df.isna().any()","2e23703b":"df.info()","3b98126c":"# Getting the number  of missing value in column named Top23\nprint(df[\"Top23\"].isna().sum())","3327874f":"# Getting the number  of missing value in column named Top24\nprint(df[\"Top24\"].isna().sum())","c74d8099":"# Getting the number  of missing value in column named Top25\nprint(df[\"Top25\"].isna().sum())","42a2bdd0":"# Dropping the missing values\ndf.dropna(inplace=True)\nprint(df.shape)","2bb73932":"# Visualizing number of 0 and 1\nplt.figure(figsize=(5, 5))\nsns.countplot(x = 'Label', data=df)\nplt.show()","0d115809":"# Getting Min and max data present in dataset\ndf['Date'].min(),  df['Date'].max()","5becc6ab":"# dataset below 01\/01\/2015 will be considered as training dataset\ntrain = df[df['Date'] < '20150101'] \n\n# and the rest will be taken as testing dataset\ntest = df[df['Date'] > '20141231'] ","fdf6ca58":"# Taking data from training set from column 2 to 27, we are actually ignoring here date and class label\ndata = train.iloc[:, 2:27]\n\n\n# apart from a-z and A-Z replace everything else with blank space\ndata.replace(\"[^a-zA-Z]\", \" \", regex=True, inplace = True)\n\n\n# Making column name  as 1, 2, 3,... for simplicity\nlist1 = [i for i in range(25)]\nnew_Index = [str(i) for i in list1]\ndata.columns = new_Index\ndata.head(3)","648e6de5":"# Converting headlines to lower case to avoid any misunderstanding by model\n\nfor index in new_Index:\n    data[index] = data[index].str.lower() \ndata.head(2)","f42dea2a":"# Joining are the sentences present in a row to form a paragraph\nheadlines = []\nfor row in range(0, len(data.index)):\n    headlines.append(' '.join(str(x) for x in data.iloc[row, 0:25]))","65643cea":"# paragraph present at index 0\nhd = headlines[0]\nhd","7d5b701f":"# Importing Wordcloud for visualization\nfrom wordcloud import WordCloud\n# Importing stopwords to remove those english word which will not add any value to our model\nfrom nltk.corpus import stopwords","8d4ef5e0":"# Generating Wordcloud for visualization\n# Note :  In this the word which are present most frequently will get printed in more bigger form\ntext = hd\nwordcloud = WordCloud(width = 2000, height = 1000, random_state=42, background_color='black', collocations=False, stopwords = stopwords.words('english')).generate(text)\nplt.figure(figsize=(20, 30))\n# Display image\nplt.imshow(wordcloud) \nplt.axis(\"off\")\nplt.show()","57e93ba1":"# Import Counter and word_tokenizer\nfrom collections import Counter \nfrom nltk.tokenize import word_tokenize","aa995b58":"# Plotting a countplot to see which word is present most frequently ( here we are printing top 7 word which are present most frequently )\nspt = word_tokenize(hd)\nspt1 = [word for word in spt if not word in stopwords.words('english')]\n\nCounter = Counter(spt1)\nfreq = Counter.most_common(7)\n\nn_groups = len(freq)\ncnt = [x[1] for x in freq ]\nwrd = [x[0] for x in freq]\n\nfig, ax = plt.subplots()\n\nindex = np.arange(n_groups)\n\nrects1 = plt.bar(index, cnt, 0.3,\n                 alpha=0.5,\n                 color='r',\n                 label='word count')\n\n\nplt.xlabel(' Word ')\nplt.ylabel('Count')\nplt.title('Most Frequent Word in a Paragraph')\nplt.xticks(index  , wrd)\nplt.legend()\n\nplt.tight_layout()\nplt.show()","5848b19d":"# Paragraph at index 1\nhd1 = headlines[1]\nhd1","aff0111a":"# Generating Wordcloud for visualization\n# Note :  In this the word which are present most frequently will get printed in more bigger form\ntext = hd1\nwordcloud = WordCloud(width = 2000, height = 1000, random_state=42, background_color='black', collocations=False, stopwords = stopwords.words('english')).generate(text)\nplt.figure(figsize=(20, 30))\n# Display image\nplt.imshow(wordcloud) \n# No axis \nplt.axis(\"off\")\nplt.show()","3c9034b4":"# Printing the length of headlines\nlen(headlines)","fda5db2e":"# Importing Bag of Word and Random Forest\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier","a3257ccd":"# Count vectorizer will convert these sentences into vectors\ncountvector= CountVectorizer(ngram_range=(2,2))\ntraindataset = countvector.fit_transform(headlines)","d9c7895d":"# Training model\nrandom_classifier= RandomForestClassifier(n_estimators=200,criterion='entropy', random_state=42)\nrandom_classifier.fit(traindataset,train['Label'])","089c7d70":"# Testing Model\ntest_transform=[]\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset= countvector.transform(test_transform)\npredictions= random_classifier.predict(test_dataset)","71d30329":"# Importing some evaluation metrics for classification problem\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report","d5e00756":"# Confusion Matrix\nmatrix= confusion_matrix(test[\"Label\"],predictions)\nsns.heatmap(matrix\/np.sum(matrix), annot = True, fmt=  '0.2%', cmap = 'Reds')","7ecc9468":"# Model Accuracy\nscore1 = accuracy_score(test[\"Label\"],predictions)\nscore1 = round(score1, 4)*100\nprint(score1)","a96631ca":"# Classification Report of the model\nprint(classification_report(test['Label'],predictions))","cd8abf89":"# Import TF-IDF \n\nfrom sklearn.feature_extraction.text import TfidfVectorizer","43a1cf06":"# TF-IDF will convert these sentences into vectors\n\ntfidf= TfidfVectorizer(ngram_range=(2,2))\ntraindataset = tfidf.fit_transform(headlines)","d27c7051":"# Implementing RandomForestClassifier on training dataset\n\nrandom_classifier= RandomForestClassifier(n_estimators=200,criterion='entropy', random_state=42)\nrandom_classifier.fit(traindataset ,train['Label'])","b636d8c7":"# Testing model \n\ntest_transform=[]\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset = tfidf.transform(test_transform)\npredictions= random_classifier.predict(test_dataset)","1c83944d":"# Confusion Matrix\nmatrix= confusion_matrix(test[\"Label\"],predictions)\nsns.heatmap(matrix\/np.sum(matrix), annot = True, fmt=  '0.2%', cmap = 'Reds')","764d855b":"# Accuracy Score\n\nscore2 = accuracy_score(test[\"Label\"],predictions)\nscore2 = round(score2 , 4)*100\nprint(score2)","0d0b9071":"# Classification Report\n\nprint(classification_report(test['Label'],predictions))","3292b58a":"# Importing Naive Bayes\n\nfrom sklearn.naive_bayes import MultinomialNB\nnaive = MultinomialNB()","474f4dd9":"# using Bag of Word\ncountvector= CountVectorizer(ngram_range=(2,2))\ntraindataset= countvector.fit_transform(headlines)","dbaff7cc":"# Training model\nnaive.fit(traindataset,train['Label'])","6f92a920":"# Testing model\n\ntest_transform=[]\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset= countvector.transform(test_transform)\npredictions= naive.predict(test_dataset)","10747ba2":"# Confusion matrix\n\nmatrix= confusion_matrix(test[\"Label\"],predictions)\nsns.heatmap(matrix\/np.sum(matrix), annot = True, fmt=  '0.2%', cmap = 'Reds')","1b2bcb8a":"# Accuracy Score of the model\n\nscore3 = accuracy_score(test[\"Label\"],predictions)\nscore3 = round(score3, 4)*100\nprint(score3)","86d4256b":"# Classification report of the model\n\nprint(classification_report(test['Label'],predictions))","307f1071":"# Using TF-IDF\n\ntraindataset= tfidf.fit_transform(headlines)","54f7f637":"# Trainging dataset\n\nnaive.fit(traindataset,train['Label'])","6438a4a7":"# Testing dataset\n\ntest_transform=[]\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset= countvector.transform(test_transform)\npredictions= naive.predict(test_dataset)","60893171":"# Confusion Matrix\n\nmatrix= confusion_matrix(test[\"Label\"],predictions)\nsns.heatmap(matrix\/np.sum(matrix), annot = True, fmt=  '0.2%', cmap = 'Reds')","f8128a36":"# Accuracy score of the model\n\nscore4 = accuracy_score(test[\"Label\"],predictions)\nscore4 = round(score4, 4)*100\nprint(score4)","0c6bf6f4":"# Classification Report of the Model\n\nreport= classification_report(test['Label'],predictions)\nprint(report)","c8f6a474":"models = pd.DataFrame({\n    'Model':['Random Forest (Bow)', 'Random Forest (TF-IDF)', ' Naive Bayes (BoW) ', 'Naive Bayes (TF-IDF)'],\n    'Accuracy_score' : [score1, score1, score3, score4]\n})\nmodels\nsns.barplot(x='Accuracy_score', y='Model', data=models)\n\nmodels.sort_values(by='Accuracy_score', ascending=False)","0abf148a":"#### USING NAIVE BAYES CLASSIFIER WITH TF-IDF VECTORIZER","50210d71":"<center> <img src=\"https:\/\/s3.envato.com\/files\/298511433\/stockMarketConcept_Preview.jpg\"> <\/center>","640fb090":"## Naive Bayes Classifier","3d1da4b8":"There are few missing data in \"Top23\", \"Top24\" and \"Top24\"","fabbaaf2":"#### Comparing Accuracy of Different Models","c7a9bd0f":"####  Random Forest Classifier with Bag of Words ","463506d9":"* Conclusion :- Model have achieved an Accuracy of 85.5% .","1287ff29":"#### Naive Bayes Classifier using Bag of Word","50b84b19":"## Random Forest","8e65cea2":"##### Two vectorization method used here are \n* Bag of Words (BoW)\n* TF-IDF","74dd8283":"In above cell we are reading the dataset and then making a copy of that dataset because it is always considered as a good practice to work on copy of dataset.","8e6c702b":"* Exploring Data manually","5134c486":"# <center>Stock Sentiment Analysis <\/center>","cb114b03":"* creating a automatic overview of data using Pandas-Profiling","cd30b5af":"Above cell shows that we have 4101 rows and 27 columns","6d92979c":"#### Random Forest Classifier using TF-IDF vectorizer"}}