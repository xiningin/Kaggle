{"cell_type":{"c38ecbb0":"code","91e00d09":"code","c38ee988":"code","6b2b19fc":"code","d8cfe8f1":"code","d31532a1":"code","9a20da18":"code","1900851b":"code","73d18d7c":"code","d1985dfd":"code","b961d822":"code","a2cd9339":"code","d2bbcd9e":"code","53c990d4":"code","a0fae891":"code","ab0afe7f":"code","3ef1a1c3":"code","74670a6c":"code","171828a8":"code","9e35d985":"markdown","b984fa3a":"markdown","9ad2bfc4":"markdown","ccaee731":"markdown","b72dae41":"markdown","8a6419eb":"markdown","0681af60":"markdown","e5ec7b65":"markdown","e17d2ad7":"markdown","d6bd92a5":"markdown"},"source":{"c38ecbb0":"!pip install -q sumeval==0.2.2\n!pip install -q nlpaug==1.1.3\n!pip install -q simpletransformers==0.60.9","91e00d09":"import gc\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nfrom wordcloud import WordCloud\nfrom sklearn.model_selection import train_test_split\n\nimport nlpaug.augmenter.word as naw\nfrom sumeval.metrics.rouge import RougeCalculator\n\nimport torch\nfrom simpletransformers.t5 import T5Model, T5Args\n\nprint('Pytorch version: %s'  % torch.__version__)","c38ee988":"warnings.simplefilter('ignore')\npd.set_option('display.max_colwidth', 10000)\ncuda =  torch.cuda.is_available()","6b2b19fc":"df = pd.read_csv('..\/input\/news-summary\/news_summary.csv', encoding='ISO-8859-1').dropna().reset_index(drop=True)\nmore_df = pd.read_csv('..\/input\/news-summary\/news_summary_more.csv', encoding='ISO-8859-1')","d8cfe8f1":"display(df.head(1))\ndisplay(more_df.head(1))","d31532a1":"df['headlines_length'] = [len(df['headlines'][i]) for i in range(len(df))]\ndf['text_length'] = [len(df['text'][i]) for i in range(len(df))]\ndf['ctext_length'] = [len(df['ctext'][i]) for i in range(len(df))]\n\nmore_df['headlines_length'] = [len(more_df['headlines'][i]) for i in range(len(more_df))]\nmore_df['text_length'] = [len(more_df['text'][i]) for i in range(len(more_df))]","9a20da18":"print('df headlines length:\\n', df['headlines_length'].describe())\nprint()\nprint('more_df headlines length:\\n', more_df['headlines_length'].describe())","1900851b":"print('df text length:\\n', df['text_length'].describe())\nprint()\nprint('df ctext length:\\n', df['ctext_length'].describe())\nprint()\nprint('more_df text length:\\n', more_df['text_length'].describe())","73d18d7c":"df = df.drop(['author', 'date', 'read_more', 'ctext',\n              'headlines_length', 'text_length', 'ctext_length'], axis=1)\nmore_df = more_df.drop(['headlines_length', 'text_length'], axis=1)\ndf = pd.concat([df, more_df]).reset_index(drop=True)","d1985dfd":"# https:\/\/www.kaggle.com\/arthurtok\/spooky-nlp-and-topic-modelling-tutorial\n\nall_words = df['text'].str.split(expand=True).unstack().value_counts()\n \ndata = [go.Bar(\n            x = all_words.index.values[2:50],\n            y = all_words.values[2:50],\n            marker= dict(colorscale='Jet',\n                         color = all_words.values[2:100]\n                        ),\n            text='Word counts'\n    )]\n\nlayout = go.Layout(\n    title='Top 50 Word frequencies in the dataset'\n)\n\nfig = go.Figure(data=data, layout=layout)\n\nfig.show()","b961d822":"wc = WordCloud(width=900, height=600)\n\nwc.generate(','.join(df['headlines']))\nplt.figure(figsize=(18,13))\nplt.imshow(wc)\nplt.axis('off')\nplt.title('headlines word cloud', fontdict={'fontsize': 20})\n\nwc.generate(','.join(df['text']))\nplt.figure(figsize=(18,13))\nplt.imshow(wc)\nplt.axis('off')\nplt.title('text word cloud', fontdict={'fontsize': 20})\n\nplt.show()","a2cd9339":"df = df.rename(columns={'text': 'input_text', 'headlines': 'target_text'}).reindex(columns=['input_text', 'target_text'])\ndf['prefix'] = ''\n\ntrain, test = train_test_split(df, test_size=0.2, random_state=42)\ntrain, valid = train_test_split(train, test_size=0.2, random_state=42)","d2bbcd9e":"aug = naw.SynonymAug(aug_src='wordnet')\naugmented_text = aug.augment(list(train['input_text'].head(1)))\nprint(\"Original:\")\nprint(','.join(train.head(1)['input_text'].values))\nprint()\nprint(\"Augmented Text:\")\nprint(','.join(augmented_text))","53c990d4":"train = pd.concat([\n    train,\n    pd.DataFrame({'input_text': naw.SynonymAug(aug_src='wordnet').augment(list(train['input_text'])),\n                  'target_text': list(train['target_text']),\n                  'prefix': ''}),\n                  ])","a0fae891":"train_params = {\n    'max_seq_length': 512,\n    'max_length': 128,\n    'train_batch_size': 8,\n    'eval_batch_size': 8,\n    'num_train_epochs': 2,\n    'evaluate_during_training': True,\n    'evaluate_during_training_steps': 10000,\n    'use_multiprocessing': False,\n    'fp16': False,\n    'save_steps': -1,\n    'save_eval_checkpoints': False,\n    'save_model_every_epoch': False,\n    'no_cache': True,\n    'reprocess_input_data': True,\n    'overwrite_output_dir': True,\n    'preprocess_inputs': False,\n    'num_return_sequences': 1 \n}\n\nmodel = T5Model('t5', 't5-small', args=train_params, use_cuda=cuda)\nmodel.train_model(train, eval_data=valid)\ngc.collect()","ab0afe7f":"pred_params = {\n        'max_seq_length': 512,\n        'use_multiprocessed_decoding': False\n        }\n\nmodel = T5Model('t5', 'outputs\/best_model', args=pred_params, use_cuda=cuda) \npred = model.predict(list(test['input_text']))","3ef1a1c3":"random.sample(pred, 5)","74670a6c":"rouge = RougeCalculator(stopwords=True, lang=\"en\")\n\ndef rouge_calc(preds, targets):\n    rouge_1 = [rouge.rouge_n(summary=preds[i],references=targets[i],n=1) for i in range(len(preds))]\n    rouge_2 = [rouge.rouge_n(summary=preds[i],references=targets[i],n=2) for i in range(len(preds))]\n    rouge_l = [rouge.rouge_l(summary=preds[i],references=targets[i]) for i in range(len(preds))]\n\n    return {\"Rouge_1\": np.array(rouge_1).mean(),\n            \"Rouge_2\": np.array(rouge_2).mean(),\n            \"Rouge_L\": np.array(rouge_l).mean()}","171828a8":"rouge_calc(pred, list(test['target_text']))","9e35d985":"### Training","b984fa3a":"## EDA <a class=\"anchor\" id=\"EDA\"><\/a>","9ad2bfc4":"## Evaluation of the model <a class=\"anchor\" id=\"Evaluation-of-the-model\"><\/a>\n\n<img src=\"https:\/\/raw.githubusercontent.com\/chakki-works\/sumeval\/master\/doc\/top.png\" style=\"height: 150px; width: 600px;  object-position: 0px;\"\/>\n\nEvaluate the model performance with the [sumeval]'s Rouge score.\n\n[sumeval]: https:\/\/github.com\/chakki-works\/sumeval\n\nRouge1: Evaluate the generated text in units of bi-grams.\n\nRouge2: Evaluate the generated text in units of uni-grams.\n\nRougeL: Evaluate the match of the generated text sequence.","ccaee731":"## Import libraries <a class=\"anchor\" id=\"Import-libraries\"><\/a>","b72dae41":"### Predict","8a6419eb":"After applying data augmentation, we will combine them with the original train data.","0681af60":"## Build the model <a class=\"anchor\" id=\"Build-the-model\"><\/a>\n\n<img src=\"https:\/\/repository-images.githubusercontent.com\/212747520\/6ef26800-0982-11ea-8476-80e5c7b4d3c4\" style=\"height: 250px; width: 500px;  object-position: 0px;\"\/>\n\nWe will use simple transformers to build the model.\n\n* [Document]\n\n[Document]: https:\/\/simpletransformers.ai\/\n\n* [Github]\n\n[Github]: https:\/\/github.com\/ThilinaRajapakse\/simpletransformers","e5ec7b65":"## Data Augmentation <a class=\"anchor\" id=\"Data-Augmentation\"><\/a>\n\n<img src=\"https:\/\/github.com\/makcedward\/nlpaug\/blob\/master\/res\/logo_small.png?raw=true\" style=\"height: 300px; width: 300px;  object-position: 0px;\"\/>\n\nUse nlpaug to apply data augmentation.\n\n[Document]\n\n[Document]: https:\/\/nlpaug.readthedocs.io\/en\/latest\/\n\n[Github]\n\n[Github]: https:\/\/github.com\/makcedward\/nlpaug","e17d2ad7":"# Text summarization with Simple Transformers T5\n\nIn this notebook, we implement a news article summarization task with T5, \n\nusing the news summary dataset published by [Kondalarao Vonteru].\n\n[Kondalarao Vonteru]: https:\/\/www.kaggle.com\/sunnysai12345","d6bd92a5":"## Content\n\n* [Import libraries](#Import-libraries)\n\n* [EDA](#EDA)\n\n* [Data Augmentation](#Data-Augmentation)\n\n* [Build the model](#Build-the-model)\n\n* [Evaluation of the model](#Evaluation-of-the-model)"}}