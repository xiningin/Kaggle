{"cell_type":{"a4cb2bc1":"code","a7453445":"code","45eac8d8":"code","54e94a49":"code","38206f5d":"code","24770d43":"code","e53fc0f9":"code","13bf1f6b":"code","bbd38be6":"code","78594b4e":"code","221f3484":"code","1eb48047":"code","4bb0683e":"code","9febeab7":"code","6da89161":"code","92bd757e":"code","0e8d236a":"code","3076300c":"code","8fd98d11":"code","0a849f12":"code","0ff24cdd":"code","3e1b479b":"code","8ab8c4e7":"code","093c7e1b":"code","f2c01a8e":"code","5db3aa39":"code","79628205":"code","d33bcfca":"code","fca0ced7":"code","4e9438a7":"code","522c55ae":"code","7dfd6510":"code","1b81c7fa":"code","3a884ce6":"code","d22f34cd":"code","53d99bf5":"code","c85e846c":"code","43ddfbc5":"code","1df7315b":"code","d33fd920":"code","8f63de52":"code","1e40193f":"code","ea874893":"code","c1294d37":"code","0b8e3e06":"code","171ae9f7":"code","1e9948d8":"code","709e16c8":"code","2944f7fb":"code","f9c6b89a":"code","852cbfd5":"code","4eab437a":"code","7303df54":"code","89342b64":"code","6eb81fc6":"code","bac90356":"code","8ec3df45":"code","fb8dd518":"code","2624d014":"code","11faa971":"code","db14caf6":"code","5755206a":"code","25a2e0f9":"code","32c7f6b1":"code","bf811156":"code","7cad77ef":"code","d51b55c9":"code","93a67960":"code","75220863":"code","4cf37ce9":"code","db928d83":"code","6fad002d":"code","38f10312":"code","f2fd0e13":"code","b951cc28":"code","08c83c93":"code","7037373e":"code","b108dcf2":"code","72b260c9":"code","a82839ac":"code","21fe8274":"code","9a8f0497":"code","d9ef0f07":"code","11096d33":"code","48de05fd":"code","f6655472":"code","fa302116":"code","1487f21f":"code","fba89f08":"code","a79aeb17":"code","7aaf24d9":"code","e7cbb815":"code","40ee9d2c":"code","07f6be11":"code","d8d319d9":"code","8087e743":"code","fb908af7":"code","d4820f46":"code","b9e47d1c":"code","476ab5f9":"code","e7efff3a":"code","e73fcdd6":"code","59a11ca1":"code","241ffb59":"code","7274fb17":"code","4c70a185":"code","3c9e0e02":"code","0d088d38":"code","378ae7a9":"code","22ea3e81":"code","20e64d91":"code","1ad6c17b":"markdown","934498ea":"markdown","740c3d90":"markdown","42401531":"markdown","74d70622":"markdown","6d66e941":"markdown","498f8c20":"markdown","cd9630f0":"markdown","66f03674":"markdown","449e29cd":"markdown","7462d140":"markdown","6c368541":"markdown","9b9811f1":"markdown"},"source":{"a4cb2bc1":"import numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import LabelEncoder\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a7453445":"print(os.listdir(\"..\/input\/\"))","45eac8d8":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nMAX_EVALS = 5","54e94a49":"features=pd.read_csv('..\/input\/application_train.csv')\nfeatures=features.sample(n=16000,random_state=42)\nprint(features.shape)","38206f5d":"features.dtypes.value_counts()","24770d43":"features = features.select_dtypes('number')\nlabels = np.array(features['TARGET'])\nfeatures = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 6000, random_state = 50)","e53fc0f9":"print(\"Training features shape: \", train_features.shape)\nprint(\"Testing features shape: \", test_features.shape)","13bf1f6b":"train_set = lgb.Dataset(data = train_features, label = train_labels)\ntest_set = lgb.Dataset(data = test_features, label = test_labels)","bbd38be6":"model = lgb.LGBMClassifier()\ndefault_params = model.get_params()\n\n# Remove the number of estimators because we set this to 10000 in the cv call\ndel default_params['n_estimators']\n\n# Cross validation with early stopping\ncv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = 5, seed = 42)","78594b4e":"print(max(cv_results['auc-mean']))\nprint(len(cv_results['auc-mean']))","221f3484":"from sklearn.metrics import roc_auc_score","1eb48047":"model.n_estimators = len(cv_results['auc-mean'])\n# Train and make predicions with model\nmodel.fit(train_features, train_labels)\npreds = model.predict_proba(test_features)[:, 1]\nbaseline_auc = roc_auc_score(test_labels, preds)\nprint('The model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))","4bb0683e":"def objective(hyperparameters, iteration):\n    \n    if 'n_estimators' in hyperparameters.keys():\n        del hyperparameters['n_estimators']   \n\n    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold =5, \n                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n\n    score = cv_results['auc-mean'][-1]\n    estimators = len(cv_results['auc-mean'])\n    hyperparameters['n_estimators'] = estimators     \n    return [score, hyperparameters, iteration]","9febeab7":"score, params, iteration = objective(default_params, 1)\n\nprint('The cross-validation ROC AUC was {:.5f}.'.format(score))","6da89161":"model = lgb.LGBMModel()\nmodel.get_params()","92bd757e":"param_grid = {\n    'boosting_type': ['gbdt', 'goss', 'dart'],\n    'num_leaves': list(range(20, 150)),\n    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n    'subsample_for_bin': list(range(20000, 300000, 20000)),\n    'min_child_samples': list(range(20, 500, 5)),\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n    'subsample': list(np.linspace(0.5, 1, 100)),\n    'is_unbalance': [True, False]\n}","0e8d236a":"import random\n\nrandom.seed(50)\n\n# Randomly sample a boosting type\nboosting_type = random.sample(param_grid['boosting_type'], 1)[0]\n\n# Set subsample depending on boosting type\nsubsample = 1.0 if boosting_type == 'goss' else random.sample(param_grid['subsample'], 1)[0]\n\nprint('Boosting type: ', boosting_type)\nprint('Subsample ratio: ', subsample)","3076300c":"random_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n\ngrid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))","8fd98d11":"import itertools\n\ndef grid_search(param_grid, max_evals = MAX_EVALS):\n    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n    keys, values = zip(*param_grid.items())    \n    i = 0\n    for v in itertools.product(*values):\n        hyperparameters = dict(zip(keys, v))\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n        eval_results = objective(hyperparameters, i)       \n        results.loc[i, :] = eval_results\n        i += 1\n        if i > MAX_EVALS:\n            break\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)    \n    return results    ","0a849f12":"grid_results = grid_search(param_grid)\nprint('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\nimport pprint\npprint.pprint(grid_results.loc[0, 'params'])","0ff24cdd":"grid_search_params = grid_results.loc[0, 'params']\nmodel = lgb.LGBMClassifier(**grid_search_params, random_state=42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))","3e1b479b":"random.seed(50)\n\n# Randomly sample from dictionary\nrandom_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n# Deal with subsample ratio\nrandom_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']\n\nrandom_params","8ab8c4e7":"def random_search(param_grid, max_evals = MAX_EVALS):\n    \"\"\"Random search for hyperparameter optimization\"\"\"\n    \n    # Dataframe for results\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                                  index = list(range(MAX_EVALS)))\n    \n    # Keep searching until reach max evaluations\n    for i in range(MAX_EVALS):\n        \n        # Choose random hyperparameters\n        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n\n        # Evaluate randomly selected hyperparameters\n        eval_results = objective(hyperparameters, i)\n        \n        results.loc[i, :] = eval_results\n    \n    # Sort with best score on top\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)\n    return results ","093c7e1b":"random_results = random_search(param_grid)\n\nprint('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\n\nimport pprint\npprint.pprint(random_results.loc[0, 'params'])","f2c01a8e":"random_search_params = random_results.loc[0, 'params']\n\n# Create, train, test model\nmodel = lgb.LGBMClassifier(**random_search_params, random_state = 42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from random search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))","5db3aa39":"train = pd.read_csv('..\/input\/application_train.csv')\ntest = pd.read_csv('..\/input\/application_test.csv')\n\n# Extract the test ids and train labels\ntest_ids = test['SK_ID_CURR']\ntrain_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1, ))\n\ntrain = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\ntest = test.drop(columns = ['SK_ID_CURR'])\n\nprint('Training shape: ', train.shape)\nprint('Testing shape: ', test.shape)","79628205":"le = LabelEncoder()\nle_count = 0\nfor col in train:\n    if train[col].dtype == 'object':\n        if len(list(train[col].unique())) <= 2:\n            le.fit(train[col])\n            train[col] = le.transform(train[col])\n            test[col] = le.transform(test[col])\n            le_count += 1\nprint('%d columns were label encoded.' % le_count)","d33bcfca":"train = pd.get_dummies(train)\ntest=pd.get_dummies(test)\nprint(train.shape,test.shape)","fca0ced7":"train, test = train.align(test, join = 'inner', axis = 1)\n\n# Add the target back in\n#train['TARGET'] = train_labels\n\nprint('Training Features shape: ', train.shape)\nprint('Testing Features shape: ', test.shape)","4e9438a7":"train_set = lgb.Dataset(train, label = train_labels)\n\nhyperparameters = dict(**random_results.loc[0, 'params'])\ndel hyperparameters['n_estimators']\n\n# Cross validation with n_folds and early stopping\ncv_results = lgb.cv(hyperparameters, train_set,\n                    num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = 5)","522c55ae":"model = lgb.LGBMClassifier(n_estimators = len(cv_results['auc-mean']), **hyperparameters)\nmodel.fit(train, train_labels)\n                        \n# Predictions on the test data\npreds = model.predict_proba(test)[:, 1]","7dfd6510":"submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': preds})\nsubmission.to_csv('submission_simple_features_random.csv', index = False)","1b81c7fa":"app_train = pd.read_csv('..\/input\/application_train.csv')\nprint('Training data shape: ', app_train.shape)\napp_train.head()","3a884ce6":"app_test = pd.read_csv('..\/input\/application_test.csv')\nprint('Testing data shape: ', app_test.shape)\napp_test.head()","d22f34cd":"app_train['TARGET'].value_counts()","53d99bf5":"plt.figure(figsize=(10,5))\nsns.set(style=\"whitegrid\",font_scale=1)\ng=sns.distplot(app_train['TARGET'],kde=False,hist_kws={\"alpha\": 1, \"color\": \"#DA1A32\"})\nplt.title('Distribution of target (1:default, 0:no default)',size=15)\nplt.show()","c85e846c":"def missing_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        return mis_val_table_ren_columns","43ddfbc5":"missing_values = missing_values_table(app_train)\nmissing_values.head(10)","1df7315b":"# Number of each type of column\napp_train.dtypes.value_counts()","d33fd920":"# Number of unique classes in each object column\napp_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","8f63de52":"le = LabelEncoder()\nle_count = 0\nfor col in app_train:\n    if app_train[col].dtype == 'object':\n        if len(list(app_train[col].unique())) <= 2:\n            le.fit(app_train[col])\n            app_train[col] = le.transform(app_train[col])\n            app_test[col] = le.transform(app_test[col])\n            le_count += 1\nprint('%d columns were label encoded.' % le_count)","1e40193f":"app_train = pd.get_dummies(app_train)\napp_test = pd.get_dummies(app_test)\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","ea874893":"train_labels_last=app_train['TARGET']","c1294d37":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n\n# Add the target back in\napp_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Testing Features shape: ', app_test.shape)","0b8e3e06":"(app_train['DAYS_BIRTH'] \/ -365).describe()","171ae9f7":"app_train['DAYS_EMPLOYED'].describe()","1e9948d8":"plt.hist(app_train['DAYS_EMPLOYED'],color=\"#DA1A32\")\nplt.title('Ditribution of employed days')\nplt.show()","709e16c8":"anom = app_train[app_train['DAYS_EMPLOYED'] == 365243]\nnon_anom = app_train[app_train['DAYS_EMPLOYED'] != 365243]\nprint('The non-anomalies default on %0.2f%% of loans' % (100 * non_anom['TARGET'].mean()))\nprint('The anomalies default on %0.2f%% of loans' % (100 * anom['TARGET'].mean()))\nprint('There are %d anomalous days of employment' % len(anom))","2944f7fb":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\napp_train['DAYS_EMPLOYED'].plot.hist(title = 'Ditribution of employed days',color=\"#DA1A32\")\nplt.xlabel('Days')","f9c6b89a":"app_test['DAYS_EMPLOYED_ANOM'] = app_test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\nprint('There are %d anomalies in the test data out of %d entries' % (app_test[\"DAYS_EMPLOYED_ANOM\"].sum(), len(app_test)))","852cbfd5":"# Find correlations with the target and sort\ncorrelations = app_train.corr()['TARGET'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', correlations.tail(15))\nprint('\\nMost Negative Correlations:\\n', correlations.head(15))","4eab437a":"app_train['DAYS_BIRTH'] = abs(app_train['DAYS_BIRTH'])","7303df54":"# Extract the EXT_SOURCE variables and show correlations\next_data = app_train[['TARGET', 'EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\next_data_corrs = ext_data.corr()\next_data_corrs","89342b64":"plt.figure(figsize = (8, 6))\n# Heatmap of correlations\nsns.heatmap(ext_data_corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)\nplt.title('Correlation Heatmap');","6eb81fc6":"manual_features=app_train[['SK_ID_CURR','AMT_CREDIT','AMT_ANNUITY','AMT_INCOME_TOTAL',\n                           'DAYS_BIRTH','DAYS_EMPLOYED']]\nmanual_features_test=app_test[['SK_ID_CURR','AMT_CREDIT','AMT_ANNUITY','AMT_INCOME_TOTAL',\n                           'DAYS_BIRTH','DAYS_EMPLOYED']]","bac90356":"cols=list(manual_features.columns)\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\nmanual_features = imputer.fit_transform(manual_features)\nmanual_features_test = imputer.transform(manual_features_test)","8ec3df45":"manual_features=pd.DataFrame(manual_features,columns=cols)\nmanual_features_test=pd.DataFrame(manual_features_test,columns=cols)","fb8dd518":"manual_features['CREDIT_INCOME_PERCENT'] = manual_features['AMT_CREDIT'] \/manual_features['AMT_INCOME_TOTAL']\nmanual_features['ANNUITY_INCOME_PERCENT'] = manual_features['AMT_ANNUITY'] \/ manual_features['AMT_INCOME_TOTAL']\nmanual_features['CREDIT_TERM'] = manual_features['AMT_ANNUITY'] \/ manual_features['AMT_CREDIT']\nmanual_features['DAYS_EMPLOYED_PERCENT'] = manual_features['DAYS_EMPLOYED'] \/ manual_features['DAYS_BIRTH']","2624d014":"manual_features_test['CREDIT_INCOME_PERCENT'] = manual_features_test['AMT_CREDIT'] \/ manual_features_test['AMT_INCOME_TOTAL']\nmanual_features_test['ANNUITY_INCOME_PERCENT'] = manual_features_test['AMT_ANNUITY'] \/ manual_features_test['AMT_INCOME_TOTAL']\nmanual_features_test['CREDIT_TERM'] = manual_features_test['AMT_ANNUITY'] \/ manual_features_test['AMT_CREDIT']\nmanual_features_test['DAYS_EMPLOYED_PERCENT'] = manual_features_test['DAYS_EMPLOYED'] \/ manual_features_test['DAYS_BIRTH']","11faa971":"poly_features = app_train[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH', 'TARGET']]\npoly_features_test = app_test[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH']]\n\nfrom sklearn.preprocessing import Imputer\nimputer = Imputer(strategy = 'median')\n\npoly_target = poly_features['TARGET']\npoly_features = poly_features.drop(columns = ['TARGET'])\n\npoly_features = imputer.fit_transform(poly_features)\npoly_features_test = imputer.transform(poly_features_test)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n                                \npoly_transformer = PolynomialFeatures(degree = 3)","db14caf6":"poly_transformer.fit(poly_features)\n\npoly_features = poly_transformer.transform(poly_features)\npoly_features_test = poly_transformer.transform(poly_features_test)\nprint('Polynomial Features shape: ', poly_features.shape)","5755206a":"poly_transformer.get_feature_names(input_features = ['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3', 'DAYS_BIRTH'])","25a2e0f9":"poly_features = pd.DataFrame(poly_features, \n                             columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                           'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\npoly_features['TARGET'] = poly_target\n\npoly_corrs = poly_features.corr()['TARGET'].sort_values()\n\nprint(poly_corrs.head(10))\nprint(poly_corrs.tail(5))","32c7f6b1":"poly_features_test = pd.DataFrame(poly_features_test, \n                                  columns = poly_transformer.get_feature_names(['EXT_SOURCE_1', 'EXT_SOURCE_2', \n                                                                                'EXT_SOURCE_3', 'DAYS_BIRTH']))\n\n\npoly_features['SK_ID_CURR'] = app_train['SK_ID_CURR']\napp_train_poly_1 = app_train.merge(poly_features, on = 'SK_ID_CURR', how = 'left')\napp_train_poly_3=app_train_poly_1.merge(manual_features,on = 'SK_ID_CURR', how = 'left')\n\npoly_features_test['SK_ID_CURR'] = app_test['SK_ID_CURR']\napp_test_poly_2 = app_test.merge(poly_features_test, on = 'SK_ID_CURR', how = 'left')\napp_test_poly_4=app_test_poly_2.merge(manual_features_test,on = 'SK_ID_CURR', how = 'left')\n\napp_train_poly, app_test_poly = app_train_poly_3.align(app_test_poly_4, join = 'inner', axis = 1)\n\nprint('Training data with polynomial features shape: ', app_train_poly.shape)\nprint('Testing data with polynomial features shape:  ', app_test_poly.shape)","bf811156":"app_train_poly=app_train_poly_3.T.drop_duplicates().T","7cad77ef":"app_train_poly, app_test_poly = app_train_poly.align(app_test_poly, join = 'inner', axis = 1)","d51b55c9":"app_train_poly.head()","93a67960":"app_train_poly.isnull().sum()","75220863":"col_2=list(app_train_poly.columns)","4cf37ce9":"print(app_train_poly.shape,app_test_poly.shape)","db928d83":"from sklearn.preprocessing import Imputer\n\nif 'TARGET' in app_train_poly:\n    train = app_train_poly.drop(columns = ['TARGET'])\nelse:\n    train = app_train_poly.copy()\n\nfeatures = list(train.columns)\n\ntest = app_test_poly.copy()\n\nimputer = Imputer(strategy = 'median')\n\nimputer.fit(train)\n\ntrain = imputer.transform(train)\ntest = imputer.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","6fad002d":"train=pd.DataFrame(train,columns=col_2)\ntest=pd.DataFrame(test,columns=col_2)","38f10312":"train['TARGET']=app_train['TARGET']","f2fd0e13":"train.head()","b951cc28":"#train_last=train.copy()\n#test_last=test.copy()","08c83c93":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nMAX_EVALS = 5","7037373e":"features=train.sample(n=16000,random_state=42)","b108dcf2":"print(features.shape)","72b260c9":"labels = np.array(features['TARGET'])\nfeatures = features.drop(columns = ['TARGET', 'SK_ID_CURR'])\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 6000, random_state = 50)","a82839ac":"print(\"Training features shape: \", train_features.shape)\nprint(\"Testing features shape: \", test_features.shape)","21fe8274":"train_set = lgb.Dataset(data = train_features, label = train_labels)\ntest_set = lgb.Dataset(data = test_features, label = test_labels)","9a8f0497":"model = lgb.LGBMClassifier()\ndefault_params = model.get_params()\n\n# Remove the number of estimators because we set this to 10000 in the cv call\ndel default_params['n_estimators']\n\n# Cross validation with early stopping\ncv_results = lgb.cv(default_params, train_set, num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = 5, seed = 42)","d9ef0f07":"print(max(cv_results['auc-mean']))","11096d33":"print(len(cv_results['auc-mean']))","48de05fd":"from sklearn.metrics import roc_auc_score","f6655472":"model.n_estimators = len(cv_results['auc-mean'])\n# Train and make predicions with model\nmodel.fit(train_features, train_labels)\npreds = model.predict_proba(test_features)[:, 1]\nbaseline_auc = roc_auc_score(test_labels, preds)\nprint('The baseline model scores {:.5f} ROC AUC on the test set.'.format(baseline_auc))","fa302116":"def objective(hyperparameters, iteration):\n    \"\"\"Objective function for grid and random search. Returns\n       the cross validation score from a set of hyperparameters.\"\"\"\n    \n    # Number of estimators will be found using early stopping\n    if 'n_estimators' in hyperparameters.keys():\n        del hyperparameters['n_estimators']   \n     # Perform n_folds cross validation\n    cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = 10000, nfold =5, \n                        early_stopping_rounds = 100, metrics = 'auc', seed = 42)\n    # results to retun\n    score = cv_results['auc-mean'][-1]\n    estimators = len(cv_results['auc-mean'])\n    hyperparameters['n_estimators'] = estimators     \n    return [score, hyperparameters, iteration]","1487f21f":"score, params, iteration = objective(default_params, 1)\n\nprint('The cross-validation ROC AUC was {:.5f}.'.format(score))","fba89f08":"model = lgb.LGBMModel()\nmodel.get_params()","a79aeb17":"param_grid = {\n    'boosting_type': ['gbdt', 'goss', 'dart'],\n    'num_leaves': list(range(20, 150)),\n    'learning_rate': list(np.logspace(np.log10(0.005), np.log10(0.5), base = 10, num = 1000)),\n    'subsample_for_bin': list(range(20000, 300000, 20000)),\n    'min_child_samples': list(range(20, 500, 5)),\n    'reg_alpha': list(np.linspace(0, 1)),\n    'reg_lambda': list(np.linspace(0, 1)),\n    'colsample_bytree': list(np.linspace(0.6, 1, 10)),\n    'subsample': list(np.linspace(0.5, 1, 100)),\n    'is_unbalance': [True, False]\n}","7aaf24d9":"import random\n\nrandom.seed(50)\n\n# Randomly sample a boosting type\nboosting_type = random.sample(param_grid['boosting_type'], 1)[0]\n\n# Set subsample depending on boosting type\nsubsample = 1.0 if boosting_type == 'goss' else random.sample(param_grid['subsample'], 1)[0]\n\nprint('Boosting type: ', boosting_type)\nprint('Subsample ratio: ', subsample)","e7cbb815":"random_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n\ngrid_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))","40ee9d2c":"import itertools\n\ndef grid_search(param_grid, max_evals = MAX_EVALS):\n    \"\"\"Grid search algorithm (with limit on max evals)\"\"\"\n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                              index = list(range(MAX_EVALS)))\n    keys, values = zip(*param_grid.items())    \n    i = 0\n    for v in itertools.product(*values):\n        hyperparameters = dict(zip(keys, v))\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n        eval_results = objective(hyperparameters, i)       \n        results.loc[i, :] = eval_results\n        i += 1\n        if i > MAX_EVALS:\n            break\n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)    \n    return results    ","07f6be11":"grid_results = grid_search(param_grid)\nprint('The best validation score was {:.5f}'.format(grid_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\nimport pprint\npprint.pprint(grid_results.loc[0, 'params'])","d8d319d9":"grid_search_params = grid_results.loc[0, 'params']\nmodel = lgb.LGBMClassifier(**grid_search_params, random_state=42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from grid search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))","8087e743":"random.seed(50)\nrandom_params = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\nrandom_params['subsample'] = 1.0 if random_params['boosting_type'] == 'goss' else random_params['subsample']","fb908af7":"def random_search(param_grid, max_evals = MAX_EVALS):\n   \n    results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n                                 index = list(range(MAX_EVALS)))\n    for i in range(MAX_EVALS):\n        hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n        hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n        eval_results = objective(hyperparameters, i)\n        results.loc[i, :] = eval_results\n        \n    results.sort_values('score', ascending = False, inplace = True)\n    results.reset_index(inplace = True)\n    return results ","d4820f46":"random_results = random_search(param_grid)\n\nprint('The best validation score was {:.5f}'.format(random_results.loc[0, 'score']))\nprint('\\nThe best hyperparameters were:')\n\nimport pprint\npprint.pprint(random_results.loc[0, 'params'])","b9e47d1c":"random_search_params = random_results.loc[0, 'params']\n\nmodel = lgb.LGBMClassifier(**random_search_params, random_state = 42)\nmodel.fit(train_features, train_labels)\n\npreds = model.predict_proba(test_features)[:, 1]\n\nprint('The best model from random search scores {:.5f} ROC AUC on the test set.'.format(roc_auc_score(test_labels, preds)))","476ab5f9":"#train = pd.read_csv('..\/input\/application_train.csv')\n#test = pd.read_csv('..\/input\/application_test.csv')\n\n# Extract the test ids and train labels\n#test_ids = test['SK_ID_CURR']\n#train_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1, ))\n\n#train = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\n#test = test.drop(columns = ['SK_ID_CURR'])\n\n#print('Training shape: ', train.shape)\n#print('Testing shape: ', test.shape)","e7efff3a":"#le = LabelEncoder()\n#le_count = 0\n#for col in train:\n#    if train[col].dtype == 'object':\n#        if len(list(train[col].unique())) <= 2:\n#            le.fit(train[col])\n#            train[col] = le.transform(train[col])\n#            test[col] = le.transform(test[col])\n#            le_count += 1\n#print('%d columns were label encoded.' % le_count)","e73fcdd6":"#train = pd.get_dummies(train)\n#test=pd.get_dummies(test)\n#print(train.shape,test.shape)","59a11ca1":"\n# Align the training and testing data, keep only columns present in both dataframes\n#train, test = train.align(test, join = 'inner', axis = 1)\n\n# Add the target back in\n#train['TARGET'] = train_labels\n\n#print('Training Features shape: ', train.shape)\n#print('Testing Features shape: ', test.shape)","241ffb59":"#train=train.select_dtypes('number')\n#test=test.select_dtypes('number')","7274fb17":"train_set = lgb.Dataset(train, label = train_labels_last)\n\nhyperparameters = dict(**random_results.loc[0, 'params'])\ndel hyperparameters['n_estimators']\n\n# Cross validation with n_folds and early stopping\ncv_results = lgb.cv(hyperparameters, train_set,\n                    num_boost_round = 10000, early_stopping_rounds = 100, \n                    metrics = 'auc', nfold = 5)\nprint(max(cv_results['auc-mean']))","4c70a185":"test['SK_ID_CURR'].describe()","3c9e0e02":"test_ids = test['SK_ID_CURR']","0d088d38":"train=train.drop(columns=['TARGET','SK_ID_CURR'])\ntest=test.drop(columns='SK_ID_CURR')","378ae7a9":"print(train.shape,test.shape)","22ea3e81":"model = lgb.LGBMClassifier(n_estimators = len(cv_results['auc-mean']), **hyperparameters)\nmodel.fit(train, train_labels_last)\n                        \n# Predictions on the test data\npreds = model.predict_proba(test)[:, 1]\n#auc1=roc_auc_score(y_test, preds)\n#print(auc1)","20e64d91":"#submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': preds})\n#submission['SK_ID_CURR']=submission['SK_ID_CURR'].astype('int32')\n#submission.to_csv('submission_simple_features_random.csv', index = False)","1ad6c17b":"## LightGBM - Second Step","934498ea":"## Random search","740c3d90":"## Grid search","42401531":"<font size=\"4\">Polynomial features & Imputation of missing values. <\/font>","74d70622":"**350000 days of employment seems wrong.**","6d66e941":"## Column Types","498f8c20":"## Basic LightGBM - First Step","cd9630f0":"## Feature engineering","66f03674":"## LightGBM","449e29cd":"<font size=\"4\">Manual features. <\/font>","7462d140":"**Replace it with nan.**","6c368541":"## About missing values","9b9811f1":"## Correlations"}}