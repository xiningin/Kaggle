{"cell_type":{"f2b2c8ee":"code","aaa10681":"code","6e9cfbb8":"code","206d8c3f":"code","896b0acd":"code","e95300cb":"code","b20e9597":"code","4527b38a":"code","6df8c38a":"code","1ed780f3":"code","7c14688a":"code","6db3d648":"code","182f6f6a":"code","d9789b33":"code","90cbcc02":"code","9bd55f54":"code","2c197a5d":"code","f7f1d2e7":"code","07d80591":"code","03f4f6c3":"code","a7e8aef3":"code","1c6a33c6":"code","0e25a8d4":"code","a84b510c":"code","26247de4":"code","b3592f87":"code","ac8c6008":"code","d9e0e52c":"markdown","cf72fbca":"markdown","4e4dc768":"markdown","85883cd8":"markdown","e3153a6e":"markdown","85e50c85":"markdown","3b109f3f":"markdown","9b387626":"markdown","d060151f":"markdown","c0cad143":"markdown","0928602b":"markdown","ab7d9f59":"markdown","97324151":"markdown","9bf4f673":"markdown","08e569d3":"markdown","61e58fb5":"markdown","c69d5aff":"markdown","d57597ec":"markdown","179e775a":"markdown","54b9e8bc":"markdown","9e87c67c":"markdown","bfaacb03":"markdown","99de0298":"markdown","ebf6b30b":"markdown","8f7fbcfd":"markdown","5aa71ed3":"markdown"},"source":{"f2b2c8ee":"import numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","aaa10681":"data = pd.read_csv('\/kaggle\/input\/bank-customer-churn-modeling\/Churn_Modelling.csv')\ndata.head()","6e9cfbb8":"del data['RowNumber'], data['CustomerId'], data['Surname']","206d8c3f":"gender_dummies = pd.get_dummies(data['Gender'])\ncountry_dummies = pd.get_dummies(data['Geography'])\ndata = pd.concat([data, gender_dummies, country_dummies], axis=1)","896b0acd":"del data['Gender'], data['Geography']","e95300cb":"data.head()","b20e9597":"data.shape","4527b38a":"data.isna().sum()","6df8c38a":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\n\n\ny = data['Exited']\nx = data.drop(['Exited'], axis = 1)\n\ndef train_validate_test_split(df, train_percent=.6, validate_percent=.2, seed=None):\n    np.random.seed(seed)\n    perm = np.random.permutation(df.index)\n    m = len(df.index)\n    train_end = int(train_percent * m)\n    validate_end = int(validate_percent * m) + train_end\n    train = df.iloc[perm[:train_end]]\n    validate = df.iloc[perm[train_end:validate_end]]\n    test = df.iloc[perm[validate_end:]]\n    return train, validate, test\n\ndata_train, data_valid, data_test = train_validate_test_split(data, seed=12345)\n\nx_train = data_train.drop(['Exited'], axis = 1)\ny_train = data_train['Exited']\n\nx_valid = data_valid.drop(['Exited'], axis = 1)\ny_valid = data_valid['Exited']\n\nx_test = data_test.drop(['Exited'], axis = 1)\ny_test = data_test['Exited']","1ed780f3":"model = DecisionTreeClassifier(random_state=12345)\nmodel.fit(x_train, y_train)","7c14688a":"from sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","6db3d648":"result = model.predict(x_valid)\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","182f6f6a":"data['Exited'].value_counts()","d9789b33":"7963\/2037","90cbcc02":"from sklearn.linear_model import LogisticRegression\n\nlinear_model = LogisticRegression(random_state=12345)\nlinear_model.fit(x_train, y_train)\nresult = linear_model.predict(x_valid)\n\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = linear_model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","9bd55f54":"from sklearn.ensemble import RandomForestClassifier\n\nforest_model = RandomForestClassifier(random_state=12345)\nforest_model.fit(x_train, y_train)\nresult = forest_model.predict(x_valid)\n\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = forest_model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","2c197a5d":"best_estim = None\nbest_depth = None\nbest_f1_score = None\nfor estimators in range (5, 121, 10):\n    for depth in range (5, 26, 5):\n        forest_model = RandomForestClassifier(random_state=12345, max_depth=depth, n_estimators=estimators)\n        forest_model.fit(x_train, y_train)\n        result = forest_model.predict(x_valid)\n        score = f1_score(result, y_valid)\n        if best_f1_score is None:\n            best_f1_score = score\n            best_estim = estimators\n            best_depth = depth\n        elif best_f1_score < score:\n            best_f1_score = score\n            best_estim = estimators\n            best_depth = depth\n        print(\"{0} estim, {1} depth, {2} score\".format(estimators, depth, score))","f7f1d2e7":"print(\"Estimators - {0}, Depth - {1}, F1 Score - {2}\".format(best_estim, best_depth, best_f1_score))","07d80591":"forest_model = RandomForestClassifier(random_state=12345, class_weight='balanced', n_estimators=best_estim, max_depth=best_depth)\nforest_model.fit(x_train, y_train)\nresult = forest_model.predict(x_valid)\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = forest_model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","03f4f6c3":"def upsample(features, target, repeat):\n    target_zeros = target[target == 0]\n    target_ones = target[target == 1]\n    \n    features_zeros = features[target == 0]\n    features_ones = features[target == 1]\n    \n    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n\n    return features_upsampled, target_upsampled\n\nx_upsampled, y_upsampled = upsample(x_train, y_train, 4)","a7e8aef3":"forest_model = RandomForestClassifier(random_state=12345, n_estimators=best_estim, max_depth=best_depth,class_weight='balanced')\nforest_model.fit(x_upsampled, y_upsampled)\nresult = forest_model.predict(x_valid)\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = forest_model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","1c6a33c6":"forest_model = RandomForestClassifier(random_state=12345, n_estimators=best_estim, max_depth=best_depth)\nforest_model.fit(x_upsampled, y_upsampled)\nresult = forest_model.predict(x_valid)\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = forest_model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","0e25a8d4":"from sklearn.utils import shuffle\n\ndef downsample(features, target, fraction):\n    features_zeros = features[target == 0]\n    features_ones = features[target == 1]\n    target_zeros = target[target == 0]\n    target_ones = target[target == 1]\n\n    features_downsampled = pd.concat(\n        [features_zeros.sample(frac=fraction, random_state=12345)] + [features_ones])\n    target_downsampled = pd.concat(\n        [target_zeros.sample(frac=fraction, random_state=12345)] + [target_ones])\n    \n    features_downsampled, target_downsampled = shuffle(\n        features_downsampled, target_downsampled, random_state=12345)\n    \n    return features_downsampled, target_downsampled\n\nx_downsampled, y_downsampled = downsample(x_train, y_train, 0.25)","a84b510c":"forest_model = RandomForestClassifier(random_state=12345, n_estimators=best_estim, max_depth=best_depth, class_weight='balanced')\nforest_model.fit(x_downsampled, y_downsampled)\nresult = forest_model.predict(x_valid)\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = forest_model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","26247de4":"forest_model = RandomForestClassifier(random_state=12345, n_estimators=best_estim, max_depth=best_depth)\nforest_model.fit(x_downsampled, y_downsampled)\nresult = forest_model.predict(x_valid)\nprint(\"F1 Score {0}\".format(f1_score(y_valid, result)))\nprobs = forest_model.predict_proba(x_valid)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_valid, probs)))\nfpr, tpr, thresholds = roc_curve(y_valid, probs)\nplot_roc_curve(fpr, tpr)","b3592f87":"x_upsampled, y_upsampled = upsample(x_train, y_train, 4)\n\nforest_model = RandomForestClassifier(random_state=12345, n_estimators=best_estim, max_depth=best_depth)\nforest_model.fit(x_upsampled, y_upsampled)\nresult = forest_model.predict(x_test)\nf1_score(result, y_test)","ac8c6008":"from sklearn.metrics import roc_curve, roc_auc_score\nimport matplotlib.pyplot as plt\n\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n\nprobs = forest_model.predict_proba(x_test)\nprobs = probs[:, 1]\n\nprint(\"AUC Score {0}\".format(roc_auc_score(y_test, probs)))\nfpr, tpr, thresholds = roc_curve(y_test, probs)\nplot_roc_curve(fpr, tpr)","d9e0e52c":"We got an unsatisfactory F1 metric and a good AUC score. We will study columns and other metrics for this dataset.","cf72fbca":"It turned out worse, which does not reach our goal, we will stop at Upsampling without balance.","4e4dc768":"Here it\u2019s already better than a decisive tree, while we stop the choice on this model, try to twist its arguments. Also note that the AUC score has grown significantly, we will choose this model for improvement.","85883cd8":"### Conclusion","e3153a6e":"Let's clear the table of already unnecessary columns.","85e50c85":"Data has been studied and prepared for research. Dummy columns are created for categorical variables, you can proceed to the next step.","3b109f3f":"The discrepancy is almost 4 times !!!","9b387626":"### Conclusion","d060151f":"Without balance, it turned out better. Let's try to do Downsampling.","c0cad143":"A big discrepancy between those who left the program and those remaining, you can try to fix this.","0928602b":"Build a model with an F1 measure greater than 0.59.","ab7d9f59":"It became a little better, based on the last step, we will try to do upsampling of some classes.","97324151":"First, try to play around with the arguments and model type.","9bf4f673":"**Gender** and **Geography** are categorical variables. We will generate dummy columns for them.","08e569d3":"## Data Preparations","61e58fb5":"## Task Research","c69d5aff":"### Conclusion","d57597ec":"First, we delete the columns that are unnecessary for our task.","179e775a":"## Unbalance","54b9e8bc":"It would be nice if there was a unit, but it will do! The result is pretty good, I think we are satisfied. The result is much higher than 0.5, which means that our classifier works well and we are on the right track.","9e87c67c":"Already reached 0.547, not bad! For the task you need 0.59, let's try to fix it with the balance of classes.","bfaacb03":"Let's try the decision tree first.","99de0298":"For the entire project, a model was constructed that satisfies the conditions of the assignment, other metrics were also used to validate the result. We looked at several variants of models for the problem, opted for the forest as the most accurate model and brought the F1 metric slightly above 0.59 using the Upsampling technique.","ebf6b30b":"## Task","8f7fbcfd":"We got a slightly better metric, try without balance.","5aa71ed3":"This model didn\u2019t say anything at all, even though the AUC score is pretty good"}}