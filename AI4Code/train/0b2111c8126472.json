{"cell_type":{"567aa63a":"code","a243719d":"code","39fdb542":"code","064231ad":"code","375c2fba":"code","088de27e":"code","6c8551f3":"code","1478afaa":"code","a0eec8b9":"code","80568aff":"code","adea6d76":"code","de79d0fe":"code","3e28d1a6":"code","300d4365":"code","d319f53c":"code","4db29a7f":"code","822e6f0c":"code","0c8753bb":"code","03e34182":"code","c795cd8a":"code","40703631":"code","cb265d68":"code","6f633b9f":"code","6664a28f":"code","ef10b431":"code","b25d5744":"code","046116f7":"code","0cff4427":"code","6c0e81bb":"code","a4b190ae":"code","6c46f359":"code","ffb28357":"code","b5144e70":"code","d8158487":"markdown","05e67d91":"markdown","413c5a70":"markdown","10c435ae":"markdown","558ad85e":"markdown","08acadaa":"markdown","279fe336":"markdown","d712b552":"markdown","dde96215":"markdown","18e4c712":"markdown","84dc2b51":"markdown","39cb0512":"markdown","a38165e8":"markdown"},"source":{"567aa63a":"!wget -O train.parquet https:\/\/www.dropbox.com\/s\/j3jupvnmi4xelwz\/train.parquet?dl=1","a243719d":"!wget -O test.parquet https:\/\/www.dropbox.com\/s\/95jwpl5bs7o8i7g\/test.parquet?dl=1","39fdb542":"!ls \/kaggle\/working","064231ad":"!pip install xgboost","375c2fba":"# imports necessarios\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier","088de27e":"#work_dir = \"\/content\"\n\n#leitura dos dados de entrada\n#df_train_bruto = pd.read_parquet(work_dir +\"\/train.parquet\", engine=\"pyarrow\")\n#df_test_bruto = pd.read_parquet(work_dir +\"\/test.parquet\", engine=\"pyarrow\")\n\ndf_train_bruto = pd.read_parquet('.\/train.parquet', engine=\"pyarrow\")\ndf_test_bruto = pd.read_parquet('.\/test.parquet', engine=\"pyarrow\")\n\nidentificador = df_test_bruto['ID_CANDIDATO']","6c8551f3":"## Junta os datasets de treino e teste para obter o mesmo numero de features durante a convers\u00e3o de categorias\ntrain_len = len(df_train_bruto) # guarda o tamanho do train para divis\u00e3o do dataset depois\ndataset =  pd.concat(objs=[df_train_bruto, df_test_bruto], axis=0).reset_index(drop=True)","1478afaa":"# An\u00e1lise dos datasets\nlen(df_train_bruto)","a0eec8b9":"len(df_test_bruto)","80568aff":"len(dataset)","adea6d76":"dataset.isnull().sum()","de79d0fe":"dataset.info()","3e28d1a6":"dataset.head()","300d4365":"from collections import Counter\nCounter(dataset['NR_IDADE_DATA_POSSE'])","d319f53c":"# Explorar NR_IDADE_DATA_POSSE vs ELEITO\ng = sns.barplot(x=\"NR_IDADE_DATA_POSSE\",y=\"ELEITO\",data=dataset)\ng = g.set_ylabel(\"ELEITO\")","4db29a7f":"# Categorizando a feature NR_IDADE_DATA_POSSE\nage_med = dataset[\"NR_IDADE_DATA_POSSE\"].median()\ndataset[\"NR_IDADE_DATA_POSSE\"] = dataset[\"NR_IDADE_DATA_POSSE\"].fillna(age_med)\ndataset[\"NR_IDADE_DATA_POSSE\"] = dataset[\"NR_IDADE_DATA_POSSE\"].astype(int)\ndataset[\"NR_IDADE_DATA_POSSE\"] = dataset[\"NR_IDADE_DATA_POSSE\"].map(lambda s: 0 if 18 <= s <= 30 else s)\ndataset[\"NR_IDADE_DATA_POSSE\"] = dataset[\"NR_IDADE_DATA_POSSE\"].map(lambda s: 1 if 31 <= s <= 50 else s)\ndataset[\"NR_IDADE_DATA_POSSE\"] = dataset[\"NR_IDADE_DATA_POSSE\"].map(lambda s: 2 if 51 <= s <= 70 else s)\ndataset[\"NR_IDADE_DATA_POSSE\"] = dataset[\"NR_IDADE_DATA_POSSE\"].map(lambda s: 3 if 71 <= s else s)\nCounter(dataset['NR_IDADE_DATA_POSSE'])","822e6f0c":"Counter(dataset['CD_GENERO'])","0c8753bb":"Counter(dataset['CD_GRAU_INSTRUCAO'])","03e34182":"# Tratando os dados nao divulgaveis\ndataset['CD_GENERO'] = dataset['CD_GENERO'].map(lambda s: 2 if s == -4 else s)\ndataset['CD_GRAU_INSTRUCAO'] = dataset['CD_GRAU_INSTRUCAO'].map(lambda s: 8 if s == -4 else s)","c795cd8a":"import math\nindex_NaN_bens = []\nfor i in dataset[\"TOTAL_BENS\"].index:\n  if math.isnan(dataset[\"TOTAL_BENS\"].loc[i]):\n    index_NaN_bens.append(i)\n\nlen(index_NaN_bens)","40703631":"df_out = dataset.drop(index_NaN_bens, axis = 0).reset_index(drop=True)\nmean = df_out[\"TOTAL_BENS\"].mean()\ndf_out = df_out[df_out[\"TOTAL_BENS\"]<mean]\ndf_out.head()","cb265d68":"df_out[\"TOTAL_BENS\"].mean()","6f633b9f":"# Tratando os valores nulos da coluna TOTAL_BENS\n\nfor i in index_NaN_bens :\n    #media dos valores dos bens sem os outliers\n    bens_media = 234690\n    bens_pred = dataset[\"TOTAL_BENS\"][((dataset['CD_COR_RACA'] == dataset.iloc[i][\"CD_COR_RACA\"]) & (dataset['CD_GRAU_INSTRUCAO'] == dataset.iloc[i][\"CD_GRAU_INSTRUCAO\"]) & (dataset['CD_CARGO'] == dataset.iloc[i][\"CD_CARGO\"]))].median()\n    #verifica se TOTAL_BENS \u00e9 NaN, ou seja, caso tenha encontrado outros \u00edndices e feito a m\u00e9dia, atribui esse valor no dataset. Caso contr\u00e1rio atribui a m\u00e9dia do dataset original\n    if not np.isnan(bens_pred) :\n        dataset[\"TOTAL_BENS\"].iloc[i] = bens_pred\n    else :\n        dataset[\"TOTAL_BENS\"].iloc[i] = bens_media","6664a28f":"print(dataset.isnull().sum())","ef10b431":"# remo\u00e7\u00e3o das colunas 'DS_COMPOSICAO_COLIGACAO' e 'NM_MUNICIPIO_NASCIMENTO'\ndataset = dataset.drop(['DS_COMPOSICAO_COLIGACAO', 'NM_MUNICIPIO_NASCIMENTO'], axis=1)","b25d5744":"# transformar colunas categ\u00f3ricas em num\u00e9ricas\ncolunas_categoricas = ['SG_UF','TP_AGREMIACAO','SG_UF_NASCIMENTO','ST_REELEICAO','ST_DECLARAR_BENS']\ndataset = pd.get_dummies(dataset, columns=colunas_categoricas)\ndataset.head()","046116f7":"print(dataset.shape)","0cff4427":"from imblearn.over_sampling import RandomOverSampler \n\ntreino = dataset[:train_len]\nteste = dataset[train_len:]\nteste.drop(labels=[\"ELEITO\"],axis = 1,inplace=True)\n\nskf = StratifiedKFold(n_splits=5, random_state=420, shuffle=True)\nX = treino.drop('ELEITO', axis=1).values\ny = treino['ELEITO'].values\nmedia_treino =[]\nmedia_teste = []\nmedia_validacao =[]\n\nros = RandomOverSampler(random_state=420)","6c0e81bb":"contador = 1\nfor indice_treino, indice_validacao in skf.split(X, y):    \n    X_treino = X[indice_treino]\n    y_treino = y[indice_treino]    \n    X_validacao = X[indice_validacao]\n    y_validacao = y[indice_validacao]\n\n    X_treino, y_treino = ros.fit_resample(X_treino, y_treino)\n    print('Resampled dataset shape %s' % Counter(y_treino))\n\n    modelo = XGBClassifier(random_state=420, n_estimators=2000, n_jobs=4)\n    modelo.fit(X_treino, y_treino, eval_set=[(X_validacao, y_validacao)], eval_metric=\"auc\", early_stopping_rounds=200, verbose=True)    \n    \n    y_pred = modelo.predict_proba(X_treino)    \n    y_pred = y_pred[:,1]\n    score_treino = roc_auc_score(y_treino, y_pred)  \n    print(\"Treino n\u00famero {} : {}\".format(contador, score_treino))\n\n    y_validacao_pred = modelo.predict_proba(X_validacao)\n    y_validacao_pred = y_validacao_pred[:,1]\n    score_validacao = roc_auc_score(y_validacao, y_validacao_pred)\n    print(\"Validacao n\u00famero {} : {} \\n\".format(contador, score_validacao))\n\n    contador += 1\n\n    X_teste = teste.values\n    y_pred_teste = modelo.predict_proba(X_teste)\n    y_pred_teste = y_pred_teste[:, 1]  \n\n    media_treino.append(score_treino)  \n    media_validacao.append(score_validacao)\n    media_teste.append(y_pred_teste)\n\nprint(\"Media de todos treinos {}:\".format(np.mean(media_treino)))\nprint(\"Media de todas valida\u00e7\u00f5es {}:\".format(np.mean(media_validacao)))","a4b190ae":"mediafinal_pred = np.mean(media_teste, axis=0)","6c46f359":"output = pd.concat([identificador, pd.DataFrame(mediafinal_pred, columns=['ELEITO'])], axis=1)\noutput","ffb28357":"#output.to_csv(work_dir +\"\/ouput_sklearn.csv\", index=False)\noutput.to_csv('ouput_sklearn.csv', index=False)","b5144e70":"#from google.colab import files\n#files.download('ouput_sklearn.csv')","d8158487":"O Dataframe *output* \u00e9 escrito no formato CSV para gerar a sa\u00edda do algoritmo de aprendizado de m\u00e1quina constru\u00eddo neste notebook.","05e67d91":"As colunas \"SG_UF\", \"TP_AGREMIACAO\", \"DS_COMPOSICAO_COLIGACAO\", \"SG_UF_NASCIMENTO\", \"NM_MUNICIPIO_NASCIMENTO\", \"ST_REELEICAO\" e \"ST_DECLARAR_BENS\" s\u00e3o transformadas para valores num\u00e9ricos utilizando a fun\u00e7\u00e3o [get_dummies](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.23.4\/generated\/pandas.get_dummies.html) do Pandas. Essa Engenharia de Features \u00e9 importante para que o classificador funcione corretamente. Desenvolvi um [notebook](https:\/\/colab.research.google.com\/drive\/1SzJ_GpFjRq6UmH1d5H1cfMAGXb4YxX7K) que mostra atrav\u00e9s de exemplos o que esta fun\u00e7\u00e3o *get_dummies* faz.","413c5a70":"## Preparando o ambiente\n\nO c\u00f3digo abaixo adiciona a **raiz** do projeto, que cont\u00e9m c\u00f3digos e dados necess\u00e1rios para o \"Hands on\".","10c435ae":"As vari\u00e1veis *NR_IDADE_DATA_POSSE* e *TOTAL_BENS* foram removidas. Ir\u00e1 ficar como **exerc\u00edcio** para voc\u00ea inclu\u00ed-las no conjunto de features. O artigo neste [link](https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b) apresenta algumas estrat\u00e9gias para lidar com vari\u00e1veis num\u00e9ricas. Veja que essas duas vari\u00e1veis podem ser discretizadas e transformadas em v\u00e1rias categorias. Por exemplo, para a feature *NR_IDADE_DATA_POSSE* voc\u00ea pode criar faixas et\u00e1rias.\n\nA vari\u00e1vel 'ID_CANDIDATO' tamb\u00e9m ser\u00e1 removida para n\u00e3o dar overfiting no modelo. Ou seja, ele vai ter uma acur\u00e1cia \u00f3tima nos dados de treinamento que n\u00e3o se reflete nos dados de teste.","558ad85e":"## Leitura dos dados\n\nO trecho de c\u00f3digo abaixo cria uma vari\u00e1vel *work_dir*, que ir\u00e1 apontar para o caminho no sistema de arquivos onde est\u00e3o os dados de entrada e onde a sa\u00edda ser\u00e1 escrita. Como os dados de entrada est\u00e3o no formato Parquet, o Pandas ir\u00e1 utilizar o motor de leitura Pyarrow para conseguir ler este formato de dados e aumentar a performance de leitura e transforma\u00e7\u00f5es no DataFrame.","08acadaa":"Os valores nulos das colunas num\u00e9ricas s\u00e3o substitu\u00eddos por zero para n\u00e3o gerar exce\u00e7\u00f5es no treinamento do modelo de aprendizado de m\u00e1quina.","279fe336":"## Classifica\u00e7\u00e3o utilizando Pandas e Scikit-learn\n\nNeste notebook iremos fazer a predizer os candidatos eleitos utilizando a biblioteca [Scikit-learn](https:\/\/scikit-learn.org\/) e o Pandas. Iremos desenvolver, neste notebook, um modelo capaz de predizer se o candidato foi eleito ou n\u00e3o, ou seja, uma tarefa de classifica\u00e7\u00e3o bin\u00e1ria.","d712b552":"O esquema \u00e9 apresentado na linha abaixo, para que possamos visualizar o modelo de dados que iremos trabalhar.","dde96215":"## Engenharia de Features\n\nEngenharia de Features \u00e9 o processo de usar o conhecimento de dom\u00ednio sobre os dados para criar *features* que fazem os algoritmos de aprendizado de m\u00e1quina funcionar da forma que esperamos. Iremos aplicar [Engenharia de Features](https:\/\/www.kaggle.com\/sudalairajkumar\/feature-engineering-validation-strategy) nos Dataframes de treinamento e teste. ","18e4c712":"## Considera\u00e7\u00f5es Finais\n\nAgora \u00e9 com **voc\u00ea**! Ainda existe muito espa\u00e7o para melhoria na acur\u00e1cia do modelo que desenvolvemos at\u00e9 agora. Utilize o material complementar abaixo para modificar este notebook e construir um algoritmo melhor.\n\n- [Curso de Aprendizado de M\u00e1quina de Stanford com Andrew Ng](https:\/\/www.coursera.org\/learn\/machine-learning)\n- [M\u00e3os \u00e0 Obra: Aprendizado de M\u00e1quina com Scikit-Learn & TensorFlow](https:\/\/www.amazon.com.br\/M%C3%A3os-Obra-Aprendizado-Scikit-Learn-TensorFlow\/dp\/8550803812)\n- [Introduction to Machine Learning with Python](https:\/\/www.amazon.com.br\/Introduction-Machine-Learning-Andreas-Mueller\/dp\/1449369413)\n- [Data Science do Zero](https:\/\/www.amazon.com.br\/Data-Science-zero-Joel-Grus\/dp\/857608998X)\n- [Customer Churn Classification Using Predictive Machine Learning Models](https:\/\/towardsdatascience.com\/customer-churn-classification-using-predictive-machine-learning-models-ab7ba165bf56)","84dc2b51":"Iremos remover tamb\u00e9m as colunas discretas \"DS_COMPOSICAO_COLIGACAO\" e \"NM_MUNICIPIO_NASCIMENTO\". Voc\u00ea acha que vale a pena adicion\u00e1-las? No artigo [Categorical Data](https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-2-categorical-data-f54324193e63), o autor discorre sobre v\u00e1rias estrat\u00e9gias para trabalhar com dados categ\u00f3ricos.","39cb0512":"## Treinamento do modelo, predi\u00e7\u00e3o dos dados e gerando arquivo csv para submiss\u00e3o\n","a38165e8":"A sa\u00edda do modelo \u00e9 salvo em um arquivo csv, contendo as colunas \"ID_CANDIDATO\" e \"ELEITO\". Estas colunas ser\u00e3o utilizadas para avaliar a acur\u00e1cia do modelo. Por isso, o resultado da predi\u00e7\u00e3o em *y_test* \u00e9 adicionada em uma nova coluna (ELEITO) do DataFrame *df_test_bruto*."}}