{"cell_type":{"54e0b9e8":"code","f811dc22":"code","4ad34419":"code","70cc3bc2":"code","1788dd4d":"code","e37f7927":"code","2ba8b4ec":"code","996ec040":"code","7bcaadb9":"code","665f795a":"code","17bcc22c":"code","b9b5c3df":"code","e4b18806":"code","8ef45f5e":"code","11cad114":"code","8931b1ed":"code","b1875339":"code","fe077dda":"code","258b38fb":"code","12f8f1cf":"code","dbb23d2b":"code","0d32c362":"code","68c581bf":"code","2232267c":"code","b7914f35":"code","ed335f62":"code","01864592":"code","1d568930":"code","2333e789":"code","48c74229":"code","be2f4909":"code","8b71409e":"code","02dd1060":"code","5d3c4a84":"code","0467ad7e":"code","b8eb26d1":"code","a71a32c1":"code","387805aa":"code","aacd2631":"code","71f6411c":"code","7c9b632f":"code","6e91048d":"code","dc71125a":"code","27ab69ad":"code","d08828cf":"code","6f4cbe14":"code","b40dbf60":"markdown","d06acc5c":"markdown","cf098fca":"markdown","1847c75b":"markdown","6c8ade86":"markdown"},"source":{"54e0b9e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f811dc22":"\"\"\"\nImporting the necessary package\n\"\"\"\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split \nfrom IPython.display import clear_output\nfrom time import sleep\nimport os","4ad34419":"!ls ..\/input\/facial-keypoints-detection\/\n\nprint(\"\\nExtracting .zip dataset files to working directory ...\")\n!unzip -u ..\/input\/facial-keypoints-detection\/test.zip\n!unzip -u ..\/input\/facial-keypoints-detection\/training.zip\n\nprint(\"\\nCurrent working directory:\")\n!pwd\nprint(\"\\nContents of working directory:\")\n!ls","70cc3bc2":"%%time\n\ntrain_csv = 'training.csv'\ntest_csv = 'test.csv'\nidlookup_file = '..\/input\/facial-keypoints-detection\/IdLookupTable.csv'\ntrain = pd.read_csv(train_csv)\ntest = pd.read_csv(test_csv)\nidlookup_data = pd.read_csv(idlookup_file)","1788dd4d":"train.head().T","e37f7927":"train.info()","2ba8b4ec":"print(\"Total number of images in train : {}\".format(len(train)))","996ec040":"train.isnull().sum()","7bcaadb9":"train.fillna(method = 'ffill',inplace = True)","665f795a":"train.isnull().sum()","17bcc22c":"train.columns","b9b5c3df":"test.isnull().sum()","e4b18806":"def load_images(image_data):\n    images = []\n    for idx, sample in image_data.iterrows():\n        image = np.array(sample['Image'].split(' '), dtype=int)\n        image = np.reshape(image, (96,96,1))\n        images.append(image)\n    images = np.array(images)\/255.\n    return images\n\ndef load_keypoints(keypoint_data):\n    keypoint_data = keypoint_data.drop('Image',axis = 1)\n    keypoint_features = []\n    for idx, sample_keypoints in keypoint_data.iterrows():\n        keypoint_features.append(sample_keypoints)\n    keypoint_features = np.array(keypoint_features, dtype = 'float')\n    return keypoint_features\n\ndef plot_sample(image, keypoint, axis, title):\n    image = image.reshape(96,96)\n    axis.imshow(image, cmap='gray')\n    axis.scatter(keypoint[0::2], keypoint[1::2], marker='x', s=20)\n    plt.title(title)","8ef45f5e":"sample_image_index = 10\n\nclean_train_images = load_images(train)\nprint(\"Shape of clean_train_images: {}\".format(np.shape(clean_train_images)))\nclean_train_keypoints = load_keypoints(train)\nprint(\"Shape of clean_train_keypoints: {}\".format(np.shape(clean_train_keypoints)))\ntest_images = load_images(test)\nprint(\"Shape of test_images: {}\".format(np.shape(test_images)))\n\ntrain_images = clean_train_images\ntrain_keypoints = clean_train_keypoints\nfig, axis = plt.subplots()\nplot_sample(clean_train_images[sample_image_index], clean_train_keypoints[sample_image_index], axis, \"Sample image & keypoints\")","11cad114":"import tensorflow.keras as keras\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input,LeakyReLU, Conv2D,Flatten, BatchNormalization, Dense, Dropout, GlobalAveragePooling2D,MaxPool2D\nfrom tensorflow.keras import optimizers\nimport tensorflow as tf\nfrom keras.utils import np_utils\nfrom keras import applications\nfrom keras.layers import concatenate\nimport time\nfrom skimage.transform import resize","8931b1ed":"# #VGG-16 with batch norm and dropout rate = 0.2\n# model = Sequential()\n# model.add(Conv2D(input_shape=(96,96,1),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n# model.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n# model.add(Dropout(0.2))\n# model.add(BatchNormalization())\n# model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n# model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n# model.add(Dropout(0.2))\n# model.add(BatchNormalization())\n# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n# model.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n# model.add(Dropout(0.4))\n# model.add(BatchNormalization())\n# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n# model.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n# model.add(BatchNormalization())\n# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n# model.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n# model.add(GlobalAveragePooling2D())\n# model.add(BatchNormalization())\n\n# # Input dimensions: (None, 3, 3, 512)\n# model.add(Flatten())\n# model.add(Dense(512,activation='relu'))\n# model.add(Dropout(0.1))\n# model.add(Dense(30))\n# model.summary()","b1875339":"# model = Sequential()\n\n# model.add(Conv2D(32, (3,3), padding='same', use_bias=True, input_shape=(96,96,1),activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Conv2D(32, (3,3), padding='same', use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Conv2D(32, (3,3), padding='same', use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(MaxPool2D(pool_size=(2, 2)))\n# # model.add(Dropout(0.1))\n\n# model.add(Conv2D(64, (3,3), padding='same', use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Conv2D(64, (3,3), padding='same', use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Conv2D(64, (3,3), padding='same', use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(MaxPool2D(pool_size=(2, 2)))\n\n# model.add(Conv2D(96, (3,3), padding='same', use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Conv2D(96, (3,3), padding='same', use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Conv2D(96, (3,3), padding='same', use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(MaxPool2D(pool_size=(2, 2)))\n# # model.add(Dropout(0.1))\n\n# model.add(Conv2D(128, (3,3),padding='same', use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Conv2D(128, (3,3),padding='same', use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Conv2D(128, (3,3),padding='same', use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(MaxPool2D(pool_size=(2, 2)))\n\n# model.add(Conv2D(256, (3,3),padding='same',use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Conv2D(256, (3,3),padding='same',use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Conv2D(256, (3,3),padding='same',use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(MaxPool2D(pool_size=(2, 2)))\n# # model.add(Dropout(0.1))\n\n# model.add(Conv2D(512, (3,3), padding='same', use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Conv2D(512, (3,3), padding='same', use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n# model.add(Conv2D(512, (3,3), padding='same', use_bias=True,activation=\"relu\"))\n# model.add(BatchNormalization())\n\n\n# model.add(Flatten())\n# model.add(Dense(512,activation='relu'))\n# model.add(Dropout(0.1))\n# model.add(Dense(30))\n# model.summary()","fe077dda":"# BEST MODEL AS NOW \n###########################################\n# model = Sequential()\n\n# model.add(Conv2D(32, (3,3), padding='same', use_bias=True, input_shape=(96,96,1)))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n# model.add(Conv2D(32, (3,3), padding='same', use_bias=True))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n# model.add(Conv2D(32, (3,3), padding='same', use_bias=True))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n# model.add(MaxPool2D(pool_size=(2, 2)))\n# # model.add(Dropout(0.1))\n\n# model.add(Conv2D(64, (3,3), padding='same', use_bias=True))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n# model.add(Conv2D(64, (3,3), padding='same', use_bias=True))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n# model.add(Conv2D(64, (3,3), padding='same', use_bias=True))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n# model.add(MaxPool2D(pool_size=(2, 2)))\n\n# model.add(Conv2D(96, (3,3), padding='same', use_bias=True))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n# model.add(Conv2D(96, (3,3), padding='same', use_bias=True))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n# model.add(Conv2D(96, (3,3), padding='same', use_bias=True))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n# model.add(MaxPool2D(pool_size=(2, 2)))\n# # model.add(Dropout(0.1))\n\n# model.add(Conv2D(128, (3,3),padding='same', use_bias=True))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n# model.add(Conv2D(128, (3,3),padding='same', use_bias=True))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n# model.add(MaxPool2D(pool_size=(2, 2)))\n\n# model.add(Conv2D(256, (3,3),padding='same',use_bias=True))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n# model.add(Conv2D(256, (3,3),padding='same',use_bias=True))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n# model.add(MaxPool2D(pool_size=(2, 2)))\n# # model.add(Dropout(0.1))\n\n# model.add(Conv2D(512, (3,3), padding='same', use_bias=True))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n# model.add(Conv2D(512, (3,3), padding='same', use_bias=True))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(BatchNormalization(momentum=0.8))\n\n\n\n# model.add(Flatten())\n# model.add(Dense(512))\n# model.add(LeakyReLU(alpha=0.2))\n# model.add(Dropout(0.1))\n# model.add(Dense(30))\n# model.summary()","258b38fb":"model = Sequential()\n\nmodel.add(Conv2D(32, (5,5), padding='same', use_bias=True, input_shape=(96,96,1)))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Conv2D(32, (5,5), padding='same', use_bias=True))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Conv2D(32, (5,5), padding='same', use_bias=True))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n# model.add(Dropout(0.1))\n\nmodel.add(Conv2D(64, (5,5), padding='same', use_bias=True))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Conv2D(64, (5,5), padding='same', use_bias=True))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Conv2D(64, (5,5), padding='same', use_bias=True))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(96, (5,5), padding='same', use_bias=True))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Conv2D(96, (5,5), padding='same', use_bias=True))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Conv2D(96, (5,5), padding='same', use_bias=True))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n# model.add(Dropout(0.1))\n\nmodel.add(Conv2D(128, (5,5),padding='same', use_bias=True))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Conv2D(128, (5,5),padding='same', use_bias=True))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(256, (5,5),padding='same',use_bias=True))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Conv2D(256, (5,5),padding='same',use_bias=True))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\n# model.add(Dropout(0.1))\n\nmodel.add(Conv2D(512, (5,5), padding='same', use_bias=True))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\nmodel.add(Conv2D(512, (5,5), padding='same', use_bias=True))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(BatchNormalization(momentum=0.8))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(LeakyReLU(alpha=0.2))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(30))\nmodel.summary()","12f8f1cf":"# (trainX, testX, trainY, testY) = train_test_split(train_images, train_keypoints,\n# \ttest_size=0.2, random_state=42)\n# trainX.shape, testX.shape","dbb23d2b":"# #pyimage source\n# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n# aug = ImageDataGenerator(\n# \t\trotation_range=20,\n# \t\tzoom_range=0.15,\n# \t\twidth_shift_range=0.2,\n# \t\theight_shift_range=0.2,\n# \t\tshear_range=0.15,\n# \t\thorizontal_flip=True,\n# \t\tfill_mode=\"nearest\")\n\n# aug.fit(train_images)\n\n# X_batch, y_batch = aug.flow(train_images, train_keypoints, batch_size=32)","0d32c362":"from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom keras.optimizers import Adam\n\n# model = load_model('best_modelV3_1.hdf5') # Uncomment it and start the training from where you left\n\n# Define necessary callbacks\ncheckpointer = ModelCheckpoint(filepath = 'best_modelV8.hdf5', monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n# adam=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n\n# Compile the model\nmodel.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=['mae', 'acc'])\n\n# Train the model\nhistory = model.fit(train_images, train_keypoints, epochs=500, batch_size=256, validation_split=0.1, callbacks=[checkpointer])\n\n","68c581bf":"# #originally\n\n# from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n# from keras.optimizers import Adam\n\n# # model = load_model('best_modelV3_1.hdf5') # Uncomment it and start the training from where you left\n\n# # Define necessary callbacks\n# checkpointer = ModelCheckpoint(filepath = 'best_modelV7.hdf5', monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n# # adam=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n\n# # Compile the model\n# model.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=['mae', 'acc'])\n\n# # Train the model\n# history = model.fit(train_images, train_keypoints, epochs=50, batch_size=256, validation_split=0.1, callbacks=[checkpointer])\n\n","2232267c":"try:\n    plt.plot(history.history['mae'])\n    plt.plot(history.history['val_mae'])\n    plt.title('Mean Absolute Error vs Epoch')\n    plt.ylabel('Mean Absolute Error')\n    plt.xlabel('Epochs')\n    plt.legend(['train', 'validation'], loc='upper right')\n    plt.show()\n    # summarize history for accuracy\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('Accuracy vs Epoch')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Loss vs Epoch')\n    plt.ylabel('Loss')\n    plt.xlabel('Epochs')\n    plt.legend(['train', 'validation'], loc='upper left')\n    plt.show()\nexcept:\n    print(\"One of the metrics used for plotting graphs is missing! See 'model.compile()'s `metrics` argument.\")\n","b7914f35":"from keras.models import load_model\n# import h5py\nmodel.save('best_modelV5x5_V1.hdf5')  # creates a HDF5 file 'best_modelV3.hdf5'\n# del model  # deletes the existing model\n\n# # returns a compiled model\n# # identical to the previous one\n","ed335f62":"%%time\n\nfrom keras.models import load_model \nmodel = load_model('best_modelV5x5_V1.hdf5')\ntest_preds = model.predict(test_images)","01864592":"fig = plt.figure(figsize=(20,16))\nfor i in range(10):\n    axis = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[])\n    plot_sample(test_images[i], test_preds[i], axis, \"\")\nplt.show()","1d568930":"feature_names = list(idlookup_data['FeatureName'])\nimage_ids = list(idlookup_data['ImageId']-1)\nrow_ids = list(idlookup_data['RowId'])\n\nfeature_list = []\nfor feature in feature_names:\n    feature_list.append(feature_names.index(feature))\n    \npredictions = []\nfor x,y in zip(image_ids, feature_list):\n    predictions.append(test_preds[x][y])\n    \nrow_ids = pd.Series(row_ids, name = 'RowId')\nlocations = pd.Series(predictions, name = 'Location')\nlocations = locations.clip(0.0,96.0)\nsubmission_result = pd.concat([row_ids,locations],axis = 1)\nsubmission_result.to_csv('charlin_version_5X5.csv',index = False)","2333e789":"submission_result","48c74229":"#pyimage source\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\naug = ImageDataGenerator(\n\t\trotation_range=20,\n\t\tzoom_range=0.15,\n\t\twidth_shift_range=0.2,\n\t\theight_shift_range=0.2,\n\t\tshear_range=0.15,\n\t\thorizontal_flip=True,\n\t\tfill_mode=\"nearest\")","be2f4909":"trainX[:,:,:,0].shape\ntrainX=np.array([trainX[:,:,:,0],trainX[:,:,:,0],trainX[:,:,:,0]])\ntrainX.shape","8b71409e":"testX[:,:,:,0].shape\ntestX=np.array([testX[:,:,:,0],testX[:,:,:,0],testX[:,:,:,0]])\ntestX.shape","02dd1060":"trainX=np.swapaxes(trainX,0,1)\ntrainX=np.swapaxes(trainX,1,2)\ntrainX=np.swapaxes(trainX,2,3)\ntrainX.shape","5d3c4a84":"testX=np.swapaxes(testX,0,1)\ntestX=np.swapaxes(testX,1,2)\ntestX=np.swapaxes(testX,2,3)\ntestX.shape","0467ad7e":"# #Resnet Version\n# from keras.layers import Input\n# img_input = Input(shape=(96,96,1))\n# img_conc = tf.keras.layers.Concatenate()([img_input, img_input, img_input])   \n\n\n# base_model = applications.resnet50.ResNet50(weights= None, include_top=False, input_shape= (96,96,3))\n# base_model.trainable = False\n# base_model.summary()\n\n# model=Sequential()\n# model.add(Dense(512,activation='relu',input_shape=(2048,)))\n# model.add(Dense(256,activation='relu'))\n# model.add(Dropout(0.1))\n# model.add(Dense(128,activation='relu'))\n# model.add(Dropout(0.1))\n# model.add(Dense(48,activation='relu'))\n# model.add(Dropout(0.1))\n# model.add(Dense(30))\n# model.summary()\n# model.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=['mae', 'acc'])\n\n# final_model = Sequential([base_model, model])\n# final_model.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=['mae', 'acc'])\n# final_model.summary()","b8eb26d1":"# precomputed_train = base_model.predict(trainX, batch_size=256, verbose=1)\n# precomputed_train.shape","a71a32c1":"# history = model.fit(\n#                     x=aug.flow(precomputed_train, trainY,batch_size=256, \n#                                validation_data=[precomputed_val, testY],\n#                     steps_per_epoch=len(trainX) \/\/ 256,\n#                     epochs=10, callbacks=[checkpointer]))","387805aa":"# precomputed_val = base_model.predict(testX,batch_size=256, verbose=1)\n# precomputed_val.shape","aacd2631":"# from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n# from keras.optimizers import Adam\n\n# # model = load_model('best_modelV4.hdf5') # Uncomment it and start the training from where you left\n\n# # Define necessary callbacks\n# checkpointer = ModelCheckpoint(filepath = 'best_modelV5.hdf5', monitor='val_mae', verbose=1, save_best_only=True, mode='min')\n# # adam=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n\n# # Compile the model\n# model.compile(optimizer=\"adam\", loss='mean_squared_error', metrics=['mae', 'acc'])\n# # Train the model\n# history = model.fit(precomputed_train, trainY, epochs=10, batch_size=256, validation_data=(precomputed_val, testY),callbacks=[checkpointer])\n","71f6411c":"# history = model.fit(\n#                     x=aug.flow(precomputed_train, trainY,batch_size=256, \n#                                validation_data=[precomputed_val, testY],\n#                     steps_per_epoch=len(trainX) \/\/ 256,\n#                     epochs=10, callbacks=[checkpointer]))","7c9b632f":"# try:\n#     plt.plot(history.history['mae'])\n#     plt.plot(history.history['val_mae'])\n#     plt.title('Mean Absolute Error vs Epoch')\n#     plt.ylabel('Mean Absolute Error')\n#     plt.xlabel('Epochs')\n#     plt.legend(['train', 'validation'], loc='upper right')\n#     plt.show()\n#     # summarize history for accuracy\n#     plt.plot(history.history['acc'])\n#     plt.plot(history.history['val_acc'])\n#     plt.title('Accuracy vs Epoch')\n#     plt.ylabel('Accuracy')\n#     plt.xlabel('Epochs')\n#     plt.legend(['train', 'validation'], loc='upper left')\n#     plt.show()\n#     # summarize history for loss\n#     plt.plot(history.history['loss'])\n#     plt.plot(history.history['val_loss'])\n#     plt.title('Loss vs Epoch')\n#     plt.ylabel('Loss')\n#     plt.xlabel('Epochs')\n#     plt.legend(['train', 'validation'], loc='upper left')\n#     plt.show()\n# except:\n#     print(\"One of the metrics used for plotting graphs is missing! See 'model.compile()'s `metrics` argument.\")\n","6e91048d":"# from keras.models import load_model\n# # import h5py\n# model.save('best_modelV5.hdf5')  # creates a HDF5 file 'best_modelV3.hdf5'\n# # del model  # deletes the existing model\n\n# # # returns a compiled model\n# # # identical to the previous one\n","dc71125a":"# %%time\n\n# from keras.models import load_model \n# model = load_model('best_modelV5.hdf5')\n# test_preds = model.predict(test_images)","27ab69ad":"# fig = plt.figure(figsize=(20,16))\n# for i in range(10):\n#     axis = fig.add_subplot(4, 5, i+1, xticks=[], yticks=[])\n#     plot_sample(test_images[i], test_preds[i], axis, \"\")\n# plt.show()","d08828cf":"# feature_names = list(idlookup_data['FeatureName'])\n# image_ids = list(idlookup_data['ImageId']-1)\n# row_ids = list(idlookup_data['RowId'])\n\n# feature_list = []\n# for feature in feature_names:\n#     feature_list.append(feature_names.index(feature))\n    \n# predictions = []\n# for x,y in zip(image_ids, feature_list):\n#     predictions.append(test_preds[x][y])\n    \n# row_ids = pd.Series(row_ids, name = 'RowId')\n# locations = pd.Series(predictions, name = 'Location')\n# locations = locations.clip(0.0,96.0)\n# submission_result = pd.concat([row_ids,locations],axis = 1)\n# submission_result.to_csv('charlin_version2_6.csv',index = False)","6f4cbe14":"# submission_result","b40dbf60":"# **IDentifying null**","d06acc5c":"# ***Reading the input file*** ","cf098fca":"Data Augmentation","1847c75b":"# **Model Building**","6c8ade86":"# **EDA**"}}