{"cell_type":{"3d27b2b3":"code","7b23c2c7":"code","7826d293":"code","977b6609":"code","c7f7b346":"code","a07ea316":"code","3b9beaa3":"code","2a5e97d7":"code","6b7a768c":"code","b9838ed1":"code","716a012c":"code","7c2b09cd":"code","f839608e":"code","308f941e":"code","94f62de7":"code","8741ec6d":"code","f488508d":"code","a6a5da63":"code","91c4cb20":"code","b19e83da":"code","8b2a6ecc":"markdown","0ece2fdf":"markdown","5b833a56":"markdown","3e94586b":"markdown","4dbe7d2e":"markdown","47e6255b":"markdown"},"source":{"3d27b2b3":"import pandas as pd\nimport numpy as np\nimport gc\nimport graphviz\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport lightgbm as lgb\nimport xgboost as xgb\nimport sklearn\nimport sys\nfrom datetime import datetime\nprint(sklearn.__version__,pd.__version__, np.__version__, lgb.__version__, xgb.__version__)\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","7b23c2c7":"dtypestrain = {}\ndtypestrain['ID_code'] = 'category'\ndtypestrain['target'] = 'int8'\nfor i in range(0,200):\n    dtypestrain['var_' + str(i)] = 'float32'\n    \ndtypestest = {}\ndtypestest['ID_code'] = 'category'\nfor i in range(0,200):\n    dtypestest['var_' + str(i)] = 'float32'","7826d293":"%%time\ntrain = pd.read_csv('..\/input\/train.csv', dtype=dtypestrain)\ntest = pd.read_csv('..\/input\/test.csv', dtype=dtypestest)","977b6609":"X_train, X_val, y_train, y_val = train_test_split(train.drop(['ID_code','target'],axis=1), train['target'], test_size=0.3, shuffle=True)","c7f7b346":"LGBGBDT_PARAM = {\n    'random_state' : 1981,\n    'n_estimators' : 2000,\n    'learning_rate': 0.1,\n    'num_leaves': 16,\n    'max_depth': 4,\n    'metric' : ['auc','binary_logloss'],\n    'boosting_type' : 'gbdt',\n    'objective' : 'binary',\n    'reg_alpha' : 2.03,\n    'reg_lambda' : 4.7,\n    'feature_fraction' : 0.8, #colsample_bytree\n    'feature_fraction_seed' : 1981, \n    'max_bins' : 100,\n    'min_split_gain': 0.0148,\n    'min_child_weight' : 7.835, #min_sum_hessian_in_leaf \n    'min_data_in_leaf' : 1000, #min_child_samples\n    'random_state' : 1981, # Updated from 'seed'\n    'subsample' : .912, #also known as Bagging fraction!\n    'subsample_freq' : 200, # also known as bagging frequency!\n    'boost_from_average' : False,\n    'verbose_eval' : 50,\n    'is_unbalance' : True,\n    #'scale_pos_weight' : 10.1,\n    }\n\nLGBGBDT = lgb.LGBMClassifier( **LGBGBDT_PARAM,\n                             n_jobs=4, #Kaggle 4 cores 4 threads\n                            silent=-1,\n                            )","a07ea316":"%%time\nLGBGBDT_FIT = LGBGBDT.fit(X_train, y_train,eval_set=[(X_val,y_val)], eval_metric= ['auc','binary_logloss'], early_stopping_rounds=100, verbose=50)","3b9beaa3":"XGBOOST_PARAM = {\n    'random_state' : 1981,\n    'n_estimators' : 1000, #very slow with 2000!\n    'learning_rate': 0.15,\n    'num_leaves': 36,\n    'max_depth': 6,\n    'metric' : ['auc'],\n    'boosting_type' : 'gbdt',\n    #'drop_rate' : 0.2,    ##only for DART\n    #'max_drop' : 100,    ##only for DART\n    #'objective' : 'binary',\n    'reg_alpha' : 2.03,\n    'reg_lambda' : 4.7,\n    'feature_fraction' : 0.8, #colsample_bytree\n    'feature_fraction_seed' : 1981, \n    'max_bins' : 100,\n    'min_split_gain': 0.0148,\n    'min_child_weight' : 7.835, #min_sum_hessian_in_leaf \n    'min_data_in_leaf' : 1000, #min_child_samples\n    'random_state' : 1981, # Updated from 'seed'\n    'subsample' : .912, #also known as Bagging fraction!\n    'subsample_freq' : 200, # also known as bagging frequency!\n    'boost_from_average' : False,\n    #'verbose_eval' : 5,\n    'is_unbalance' : True,\n    #'scale_pos_weight' : 10,\n    }\n\nXGBGBDT = xgb.XGBClassifier(**XGBOOST_PARAM,\n                            tree_method = 'gpu_hist',\n                            #n_jobs =4,\n                            silent=0,\n                            )","2a5e97d7":"%%time\nXGBGBDT_FIT = XGBGBDT.fit(X_train, y_train,\n                      eval_set=[(X_train, y_train), (X_val, y_val)],\n                        eval_metric='auc',\n                          early_stopping_rounds=100,\n                        verbose=25\n                     )","6b7a768c":"#Produces a JSON model dump for LightGBM\nmodel_lgb = LGBGBDT.booster_.dump_model()\n\n#if you have a LGBM model saves as a JSON file you can use the following\n#model_lgb = json.load(open('MY JSON LIGHTGBM MODEL.json', 'r'))","b9838ed1":"def get_splits_gain(tree_num=0, parent=-1, tree=None, lev=0, node_name=None, split_gain=None, reclimit=50000):\n    '''\n    Function to recusively walk thru a single decision tree (only LIGHTGBM for now) and extract GAIN values and Feature interactions. \n    Since it uses YIELD the user of the function needs to walk through the function in a for loop to extract values. \n    ---Arguments---\n    tree_num : The number of the tree node to analyze used only in output.\n    parent : DO NOT PASS A VALUE, it used by the function for recusion to keep track of the interactions.\n    tree : A single decision tree as a DICT. Required.\n    lev : DO NOT PASS A VALUE, it used by the function for recusion to keep track of the level of the node\/interaction.\n    node_name : DO NOT PASS A VALUE, it used by the function for recusion to keep track of the interactions.\n    split_gain : DO NOT PASS A VALUE, it used by the function for recusion to keep track of the gain values.\n    inter : DO NOT PASS A VALUE, it used by the function for recusion to keep track of the interactions.\n    reclimit: this sets the max recusive limit higher incase the model is very deep. USe with caution, I have no idea on how the system beaves with very large values!\n    \n    ---YIELD---\n    A single line per recursion:\n    tree_num : tree number\n    tag : 'split_feature', the tag\/key for which the value is being extracted for the split.\n    old_parent : The actual parent for the column that is splitted on, for the first node of the tree it is '-1' by default.\n    parent : The child node under the old_parent. Note: for the first node the value is passed here\n    lev : The depth\/Level of the node, for the first node the level is 1.\n    node_name : The node from where the info was extracted.\n    split_gain : the gain value at that level\n    '''\n    sys.setrecursionlimit(reclimit)\n    if tree == None:\n        raise Exception('No tree present to analyze!')\n    for k, v in tree.items():\n        if type(v) != dict and k in ['split_feature']:\n            old_parent = parent\n            parent = v\n            tag = k\n            yield tree_num, tag, old_parent, parent, lev, node_name, split_gain\n        elif isinstance(v, dict):\n            if v.get('split_gain') == None:\n                continue\n            else:\n                tree = v\n                lev_inc = lev + 1\n                node_name = k\n                split_gain = v['split_gain']\n                for result in get_splits_gain(tree_num, parent, tree, lev_inc, node_name, split_gain):\n                    yield result\n        else:\n            continue\n            \n#Creates a feature dictionary based on the features present in the LGBM model\ndef lgbm_create_feat_dict(model):\n    feat_dict = dict(enumerate(model['feature_names']))\n    feat_dict[-1] = 'base'\n    return feat_dict\n\ndef analyze_model(model):\n    '''\n    Take a JSON dump of LGBM model, calls the recursive function to analyse all trees in the model, interprets feature index\/names and returns a dataframe with teh model analysis and a feature interactions\n    ---Arguments---\n    model :  LGBM JSON model\n    ---Returns---\n    tree_info_df : pandas DF with model summarized and feature interactions.\n    '''\n    tree_info = []\n    for j in range(0,len(model['tree_info'])):\n        for i in get_splits_gain(tree_num=j, tree=model['tree_info'][j]):\n            tree_info.append(list(i))\n    tree_info_df = pd.DataFrame(tree_info, columns=['TreeNo','Type','ParentFeature', 'SplitOnfeature','Level','TreePos','Gain'])\n    lgbm_feat_dict = lgbm_create_feat_dict(model_lgb)\n    tree_info_df['ParentFeature'].replace(lgbm_feat_dict, inplace=True)\n    tree_info_df['SplitOnfeature'].replace(lgbm_feat_dict, inplace=True)\n    tree_info_df['Interactions'] = tree_info_df['ParentFeature'].map(str) + ' - ' + tree_info_df['SplitOnfeature'].map(str)\n    return tree_info_df","716a012c":"lgb_df = analyze_model(model_lgb)\nlgb_df= round(lgb_df, 2)\nlgb_df.head()","7c2b09cd":"#Produce some calculations for easier plotting\nlgb_inter_calc = lgb_df.groupby('Interactions')['Gain'].agg(['count','sum','min','max','mean','std']).sort_values(by='sum', ascending=False).reset_index('Interactions').fillna(0)\nlgb_inter_calc = round(lgb_inter_calc, 2) #if i dont round sns.barplot fails due to too large a precision.\n#Created 2 datasets as i see that BASE (the first node of the tree) has a very hight gains and thus dilutes the interactions\nlgb_inter_calc_nobase = lgb_inter_calc[lgb_inter_calc['Interactions'].str.contains('base')==False]","f839608e":"lgb_inter_calc.head()","308f941e":"lgb_inter_calc_nobase.head()","94f62de7":"gc.collect()","8741ec6d":"data = lgb_inter_calc_nobase.sort_values('sum', ascending=False).iloc[0:75].reset_index(drop=True)","f488508d":"def plot_feat_interaction(data=None):\n    plt.figure(figsize=(20, 14))\n    ax = plt.subplot(121)\n    sns.barplot(x='sum', y='Interactions', data=data.sort_values('sum', ascending=False), ax=ax)\n    ax.set_title('Total Gain for Feature Interaction', fontweight='bold', fontsize=14)\n    # Plot Gain importances\n    ax = plt.subplot(122)\n    sns.barplot(x='count', y='Interactions', data=data.sort_values('sum', ascending=False), ax=ax)\n    ax.set_title('No. of times Feature interacted', fontweight='bold', fontsize=14)\n    plt.tight_layout()\n\nplot_feat_interaction(data)","a6a5da63":"# Function to take an list and a dictionary and replacate the order of the list. Needed for syncronizing XGBOOST feature importance extraction with LIGHTGBM\ndef order_dict_bylist(order=None, unordered_dict=None):\n    '''\n    Function to order a dict by keys, based on list or predefined dict.\n    ---Arguments---\n    order: a list of values in a desired order\n    unordered_dict: the dict to be sorted based on key values.\n    \n    ---Returns---\n    newdict : the dict unordered_dict in desired order\n    '''\n    if order == None or (isinstance(order, list) or isinstance(order, dict)) == False:\n        print('No ordered list or dict provided')\n        return None\n    #Create ordered dict to perform and easy sort\n    elif isinstance(order, list):\n        order = list(X_train.columns.values)\n        i = 0\n        orderdict = {}\n        for k in order:\n            orderdict[k] = i\n            i += 1\n        order = orderdict\n    #Replace values in the dict\n    newdict = {}\n    for k, _ in order.items():\n        v = unordered_dict[k]\n        newdict[k] = v\n    del order, orderdict\n    return newdict","91c4cb20":"def plot_feature_imp_gain(features=list(X_train.columns), models=[LGBGBDT], feature_count=50, plot_all=True, return_df = False):\n\n    '''\n    Plotting function that plots the Split(weight) and Gain importances bar plots and summary with meaned values for all models.\n    ---Arguments---\n    features : list of features ( ordered)\n    models: list of models\n    feature_count :  number of features to plot for the individual models. will not affect the final plot\n    plot_all : BOOL value to plot all the model plots,  if False, this is the only plot with mean values is printed.\n    return_df : Bool to returns a pandas DF for further analysis.\n    \n    ---Returns---\n    x : pandas df returned if return_df = True.\n    '''\n    \n    x = pd.DataFrame()\n    \n    for model in models:\n        scores_df = pd.DataFrame()\n        scores_df['feature'] = features\n        model_name = str(model).split('(')[0]\n        if 'XGB' in str(model):\n            name =  model_name + ' : ' + str(model.get_params(deep=False)['boosting_type'])\n            scores_df['model'] = name\n            xgbdictweight = model.get_booster().get_score(importance_type='weight')\n            xgbdictgain = model.get_booster().get_score(importance_type='total_gain')           \n            xgbdictweight  = order_dict_bylist(order=features, unordered_dict=xgbdictweight)\n            xgbdictgain  = order_dict_bylist(order=features, unordered_dict=xgbdictgain)\n            scores_df['split_score'] = list(xgbdictweight.values())\n            scores_df['gain_score'] = list(xgbdictgain.values())           \n            del xgbdictweight, xgbdictgain\n        else:\n            name = model_name + ' : ' + str(model.get_params(deep=False)['boosting_type'])\n            scores_df['model'] = name\n            scores_df['split_score'] = model.booster_.feature_importance(importance_type='split')\n            scores_df['gain_score'] = model.booster_.feature_importance(importance_type='gain')\n\n        x = pd.concat([scores_df,x])\n\n        if plot_all == True:\n            plt.figure(figsize=(20, 10))\n            ax = plt.subplot(121)\n            sns.barplot(x='split_score', y='feature', data=scores_df.sort_values('split_score', ascending=False).iloc[0:feature_count], ax=ax)\n            ax.set_title('Feature scores wrt split importances - ' + name, fontweight='bold', fontsize=14)\n            # Plot Gain importances\n            ax = plt.subplot(122)\n            sns.barplot(x='gain_score', y='feature', data=scores_df.sort_values('gain_score', ascending=False).iloc[0:feature_count], ax=ax)\n            ax.set_title('Feature scores wrt gain importances - ' + name, fontweight='bold', fontsize=14)\n            plt.tight_layout()\n        else:\n            continue\n    if len(models) > 1:\n        plt.figure(figsize=(20, 25))\n        ax = plt.subplot(121)\n        sns.barplot(x='split_score', y='feature', data=x.sort_values('split_score', ascending=False), ax=ax)\n        ax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n        # Plot Gain importances\n        ax = plt.subplot(122)\n        sns.barplot(x='gain_score', y='feature', data=x.sort_values('gain_score', ascending=False), ax=ax)\n        ax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\n        plt.tight_layout()\n    if return_df == True:\n        return x.reset_index(drop=True)\nscores_df = plot_feature_imp_gain(models=[XGBGBDT, LGBGBDT], plot_all=True, return_df=True)\n","b19e83da":"#Just to see how the dataset looks\nscores_df[scores_df['feature']=='var_81']","8b2a6ecc":"## 4. Visualize the default split and gains for all models\n\nI also created a plotting function that will plot all models (LighGBM and XGBoost) only for now and also return a pandas DF for future validation.","0ece2fdf":"## 2. Analyze LighGBM Model\n**This is where the magic happens!!**\n\n--Take the model JSON dump and call the function \"*analyze_model(model_lgb)*\", this returns a Pandas DF with all the details\n\nUnhide to see all the fun stuff!","5b833a56":"## 3. Visualize the feature interaction","3e94586b":"## 1. Load data and fit some models","4dbe7d2e":"# **------Feature Interactions and a deeper model understanding------**\n\nI resently came upon a coursera course that mentioned to look for feature interactions on how they split and consider paying closer attention to them.\n\nThe objective of this kernel is explore that idea and intuitively explore Feature interactions for decision tree based models!*\n\n## Kernel Layout:\n1. Load data and fit some models\n1. Analyze the LightGBM models\n1. Visualize Feature interactions\n1. Visualize base lightGBM and XGboost splits and gains\n\nSure you can visualize and plot graphs using LightGBM and XGBoost inbuilt plot function, but looking at model with 100+ trees......way too many graphs, and the basic Plot importances captures number of splits and not the the actual interactions.\n\n### Possiblities....lest see...\n1) Use the output to produce  new feature.\n2) Filter feature for specific model to get a biased output for a specific classification and then blend or bag it with similar models.\n\n*For now I have only been able to manage LightGBM. I plan to add some Scikit-Learn models too as soo as I have time.\nXGBOOST - I would like to add XGBoost models, but they do not have a JSON output and contains only a weird text objects, which is kinda hard to convert. They have added a new function trees_to_dataframe() in v0.82 but it gives the model out in a flattened (and non sequential) pandas DataFrame. So its not possible to know feature interaction without the level\/node relations :(. I will try in a later version to decode their output!\nCATBOOST - if anyone can provide an explination on how the visualize a JSON model output for \"OBLIVIOUS\" TREES (I love the name!), I would be happy to write the functions.\n","47e6255b":"2. **Fit an XGBoost classifier model**"}}