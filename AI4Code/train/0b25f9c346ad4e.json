{"cell_type":{"8e58278b":"code","025e0829":"code","3f24a15c":"code","e150292f":"code","6ac9a899":"code","51656399":"code","498c82c4":"code","ffbe2e72":"code","5d986ae9":"code","8729ca97":"code","163baf05":"code","6eb11999":"code","63b8b598":"code","1431c560":"code","c7f1b288":"code","8a9eb11c":"code","39f38d22":"code","b2454ac6":"code","88943af9":"code","2b73644b":"code","2d623298":"markdown","a7b270c7":"markdown"},"source":{"8e58278b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","025e0829":"df = pd.read_csv('\/kaggle\/input\/twitter-airline-sentiment\/Tweets.csv')\n\ndf.head()","3f24a15c":"import re\n\ndf.text=df.text.apply(lambda x : x.lower())\ndf.text = df.text.apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\ndf.text.head()\n","e150292f":"\ndf.airline_sentiment=df.airline_sentiment.map({'neutral' : 1, 'positive':1,'negative':0})\ndf.airline_sentiment[0:10]\n","6ac9a899":"from nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport nltk\ndef cleanText(text):\n    \n    lemma = WordNetLemmatizer()\n    stp = stopwords.words('english')\n    \n    # This means remove everything except alphabetical and numerical characters\n    text = re.sub(\"[^a-zA-Z0-9]\",\" \",text)\n    \n    text = text.lower()\n    \n    # This mean split sentences by words (\"I am good\" => [\"I\",\"am\",\"good\"])\n    text = nltk.word_tokenize(text)\n    \n    # Lemmatizers convert words to their base form using dictionaries (going => go, bees => be , dog => dog)\n    text = [lemma.lemmatize(word) for word in text]\n    \n    # We should remove stopwords, stopwords are the words that has no special meaning such as I,You,Me,Was\n    text = [word for word in text if word not in stp]\n    \n    # Everything is ready, now we just need join the elements of lists ([\"feel\",\"good\"] => \"feel good\")\n    text = \" \".join(text)\n    \n    return text\n","51656399":"clean_text=[]\nfor w in df.text:\n    clean_text.append(cleanText(w))\n    ","498c82c4":"x=np.array(clean_text)\ny=df.airline_sentiment\ny=np.array(y)","ffbe2e72":"print(x.shape,y.shape)","5d986ae9":"from sklearn.model_selection import train_test_split\n#y = df[['pos_neg','pos_neu','neu_neg']]\n\n#train_vectors = vectorizer.fit_transform(x).toarray()\nX_train, X_test , Y_train, Y_test = train_test_split(x,y, test_size=0.2, random_state=50)","8729ca97":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\ntrain_vectors = vectorizer.fit_transform(X_train)\ntest_vectors = vectorizer.transform(X_test)\nprint(train_vectors.shape, test_vectors.shape)","163baf05":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB().fit(train_vectors, Y_train)","6eb11999":"from  sklearn.metrics  import accuracy_score\npredicted = clf.predict(test_vectors)\nprint(accuracy_score(Y_test,predicted))\n","63b8b598":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nVocab_size= 500\noov_tok='<OOV>'\n\ntokenizer=Tokenizer(num_words=Vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\nword_index=tokenizer.word_index\n\nx_train_sequnces=tokenizer.texts_to_sequences(X_train)\n\ntokenizer.fit_on_texts(X_test)\nx_test_sequnces=tokenizer.texts_to_sequences(X_test)\nx_test_sequnces[0]","1431c560":"max_length = 100\ntrunc_type='post'\npadding_type='post'\nx_train_padded=pad_sequences(x_train_sequnces,maxlen=max_length,truncating=trunc_type,padding=padding_type)\n\nx_test_padded=pad_sequences(x_test_sequnces,maxlen=max_length,truncating=trunc_type,padding=padding_type)\n","c7f1b288":"\nmodel=tf.keras.Sequential([\n    tf.keras.layers.Embedding(1000,15,input_length=max_length),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')    \n])\n\nmodel.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\nmodel.summary()","8a9eb11c":"history=model.fit(x_train_padded,Y_train,epochs=20,validation_data=(x_test_padded,Y_test))","39f38d22":"import matplotlib.pyplot as plt\n\nacc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\n\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs_range=range(1,len(acc)+1)\n\nplt.plot(epochs_range, acc, 'bo', label='Training Accuracy')\nplt.plot(epochs_range, val_acc, 'b' ,label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs_range, loss,'bo', label='Training Loss')\nplt.plot(epochs_range, val_loss,'b', label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","b2454ac6":"embedding_dim=128\nmodel_multiple_bidi_lstm = tf.keras.Sequential([\n    tf.keras.layers.Embedding(3000, embedding_dim, input_length=x_train_padded.shape[1]),\n    tf.keras.layers.SpatialDropout1D(0.4),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(embedding_dim,dropout=0.2,recurrent_dropout=0.2)),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel_multiple_bidi_lstm.summary()","88943af9":"model_multiple_bidi_lstm.compile(optimizer='RMSprop',loss='binary_crossentropy',metrics=['accuracy'])\nhistory=model_multiple_bidi_lstm.fit(x_train_padded,Y_train,epochs=20,steps_per_epoch=100,validation_data=(x_test_padded,Y_test))","2b73644b":"acc=history.history['accuracy']\nval_acc=history.history['val_accuracy']\n\nloss=history.history['loss']\nval_loss=history.history['val_loss']\n\nepochs_range=range(1,len(acc)+1)\n\nplt.plot(epochs_range, acc, 'bo', label='Training Accuracy')\nplt.plot(epochs_range, val_acc, 'b' ,label='Validation Accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs_range, loss,'bo', label='Training Loss')\nplt.plot(epochs_range, val_loss,'b', label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","2d623298":"**Need to resduce over fitting**","a7b270c7":"**WE need text and sentiment for analysis, hence clean the data**"}}