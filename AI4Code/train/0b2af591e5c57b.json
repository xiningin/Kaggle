{"cell_type":{"c95a0e5c":"code","a80ae8c3":"code","a2f63bbb":"code","abc9496a":"code","9d393981":"code","09f96012":"code","ee1f9e30":"code","22938c80":"code","088accbf":"code","fd82e6ca":"code","fcc08190":"code","345b73c5":"code","db730830":"code","b5f720d1":"code","af7dddd4":"code","1dc2d324":"code","8adb073a":"code","5450bba8":"code","4ec6c4a3":"code","5d58cf1b":"code","9fe5df62":"code","328b113a":"code","6074f427":"code","66005d5d":"code","16d1dadf":"code","9fccef1e":"code","9913bf74":"code","d4759eb9":"code","44a47dfc":"code","9fe053d7":"code","ea69e913":"markdown","bbcff342":"markdown","4af90503":"markdown","679f4374":"markdown","194a711c":"markdown","edb5a46a":"markdown","6459b2bf":"markdown","8be7b8b6":"markdown","9f093590":"markdown","41882838":"markdown","76e9330f":"markdown","b73386b9":"markdown","b10bc458":"markdown","2ceaa750":"markdown","c9983655":"markdown","eb105d5d":"markdown"},"source":{"c95a0e5c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport cv2\nfrom tqdm import tqdm\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a80ae8c3":"path = '..\/input\/flickr8k\/Flickr_Data\/Flickr_Data'\ntext_path = path+'\/Flickr_TextData'\nimages_path = path+'\/Images'\ncaptions_path = text_path+'\/Flickr8k.token.txt'\ntrain_path = text_path+'\/Flickr_8k.trainImages.txt'\nval_path = text_path+'\/Flickr_8k.devImages.txt'\ntest_path = text_path+'\/Flickr_8k.testImages.txt'\nimages_dir = os.listdir(path+'\/Images')\n","a2f63bbb":"def show_image(img_path):\n    k = cv2.imread(path+'\/Images\/'+img_path)\n    k = cv2.cvtColor(k,cv2.COLOR_BGR2RGB)\n    plt.axis('off')\n    plt.imshow(k)","abc9496a":"captions = open(captions_path, 'r').read().split(\"\\n\")\nx_train = open(train_path, 'r').read().split(\"\\n\")\nx_val = open(val_path, 'r').read().split(\"\\n\")\nx_test = open(test_path, 'r').read().split(\"\\n\")","9d393981":"tokens = {}\n\nfor ix in range(len(captions)-1):\n    temp = captions[ix].split(\"#\")\n    if temp[0] in tokens:\n        tokens[temp[0]].append(temp[1][2:])\n    else:\n        tokens[temp[0]] = [temp[1][2:]]\n","09f96012":"temp = captions[10].split(\"#\")\nfrom IPython.display import Image, display\nz = Image(filename=images_path+'\/'+temp[0])\ndisplay(z)\n\nfor ix in range(len(tokens[temp[0]])):\n    print(tokens[temp[0]][ix])","ee1f9e30":"start = '<start> '\nend = ' <end>'\ncap_train = []\nfor img in x_train:\n    if img == '':\n        continue\n    caption = [start + text + end for text in tokens[img]]\n    cap_train +=  caption\ncap_val = []\n\nfor img in x_val:\n    if img == '':\n        continue\n    caption = [start + text + end for text in tokens[img]]\n    cap_val +=  caption\ncap_test = []\n\nfor img in x_test:\n    if img == '':\n        continue\n    caption = [start + text + end for text in tokens[img]]\n    cap_test +=  caption\n    ","22938c80":"all_captions = cap_train +cap_val + cap_test\nall_captions[:5]","088accbf":"from collections import Counter\nsent = [k.split(' ') for k in all_captions]\nsentences = [y for x in sent for y in x]\n\ncount_words = Counter(sentences)","fd82e6ca":"min_threshold = 0\nwords = [w for w in count_words.keys() if count_words[w] > min_threshold]\n#unk_words = [w for w in count_words.keys() if count_words[w] < min_threshold]\nword_map = {k: v + 1 for v, k in enumerate(words)}\nword_map['<unk>'] = len(word_map) + 3\nword_map['<start>'] = len(word_map) + 1\nword_map['<end>'] = len(word_map) + 2\nword_map['<pad>'] = 0","fcc08190":"def word2ind(x):\n    if x in word_map.keys():\n        return word_map[x]\n    else:\n        return word_map['<unk>']","345b73c5":"max_len = 40\ntokenized_dataset = {}\nsplits = ['train','val','test']\ncaption_split = [cap_train, cap_val,cap_test]\nfor i,split in enumerate(splits):\n    token_caption = []\n    token_caption_len = []\n    for cap in caption_split[i]:\n        s = list(map(word2ind,cap.split(' ')))\n        token_caption.append(list(s) + [word_map['<pad>']]*(max_len-len(s)))\n        token_caption_len.append(len(cap.split(' ')))\n    tokenized_dataset[split] = [token_caption,token_caption_len]\n","db730830":"import torch\nfrom torch.utils.data import Dataset","b5f720d1":"def readImg(x):\n    img = cv2.imread(x)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (256,256))\n    return img ","af7dddd4":"class CaptionDataset(Dataset):\n    def __init__(self,img_dir,img_list,captions,split,transform=None):\n        self.img_dir = img_dir\n        self.img_list = img_list\n        self.split = split\n        self.captions = captions[0]\n        self.captions_length = captions[1]\n        self.transform = transform\n        self.cpi = 5\n        \n    def __len__(self):\n        return len(self.captions)\n    \n    def __getitem__(self, i):\n        # Remember, the Nth caption corresponds to the (N \/\/ captions_per_image)th image\n        img = readImg(self.img_dir+'\/'+self.img_list[i\/\/self.cpi])\n        img = torch.FloatTensor(img \/ 255.)\n#         img = img\/255.0\n        if self.transform is not None:\n            img = img.permute((2,0,1))\n            img = self.transform(img)\n\n        caption = torch.LongTensor(self.captions[i])\n\n        caplen = torch.LongTensor([self.captions_length[i]])\n\n        if self.split is 'TRAIN':\n            return img, caption, caplen\n        else:\n            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n            all_captions = torch.LongTensor(\n                self.captions[((i \/\/ self.cpi) * self.cpi):(((i \/\/ self.cpi) * self.cpi) + self.cpi)])\n            return img, caption, caplen, all_captions","1dc2d324":"import torchvision.transforms as transforms\nfrom torch.utils.data import TensorDataset, DataLoader, Dataset\n\n\ntransforms_train = transforms.Compose([\n#     transforms.ToPILImage(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                            std=[0.229, 0.224, 0.225])\n])\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                     std=[0.229, 0.224, 0.225])\ntrain_data = CaptionDataset(images_path, x_train, tokenized_dataset['train'],'TRAIN',transform=transforms.Compose([normalize]))\nval_data = CaptionDataset(images_path, x_val, tokenized_dataset['val'],'VAL',transform=transforms.Compose([normalize]))\ntest_data = CaptionDataset(images_path, x_test, tokenized_dataset['test'],'TEST',transform=transforms.Compose([normalize]))","8adb073a":"batch = 64\ntrain_loader = DataLoader(train_data,batch_size=batch, shuffle=True, pin_memory=True)\nval_loader = DataLoader(val_data,batch_size=batch, shuffle=True, pin_memory=True)\ntest_loader = DataLoader(test_data,batch_size=1, shuffle=True, pin_memory=True)","5450bba8":"from torch import nn\nimport torchvision\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","4ec6c4a3":"class Encoder(nn.Module):\n    \n    def __init__(self,encoded_image_size=14,fine_tune=True):\n        super(Encoder,self).__init__()\n        self.enc_size = encoded_image_size\n        resnet = torchvision.models.resnet101(pretrained=True)\n#         We remove the linear and pool layers at the end since we are not doing classification\n        modules = list(resnet.children())[:-2]\n        self.model = nn.Sequential(*modules)\n        self.pool = nn.AdaptiveAvgPool2d(self.enc_size)\n        \n        self.fine_tune(fine_tune)\n    \n    def forward(self,x):\n        bp = self.model(x) # (batch,2048,img\/32,img\/32)\n        ap = self.pool(bp) # (batch, 2048,enc_img_size,enc_img_size)\n        out = ap.permute(0,2,3,1) #(batch,enc_img_size,enc_img_size,2048)\n        return out\n    \n    def fine_tune(self,fine_tune=True):\n        for p in self.model.parameters():\n            p.requires_grad = False\n#         If we fine tune then we only do with conv layers through blocks 2 to 4\n        for c in list(self.model.children())[5:]:\n            for p in c.parameters():\n                p.requires_grad = True","5d58cf1b":"class Attention(nn.Module):\n    def __init__(self,encoder_dim,decoder_dim,attention_dim):\n        \"\"\"\n        encoder_dim : size of encoded images\n        decoder_dim : size of decoder RNNs\n        attention_dim : size of the attention network\n        \"\"\"\n        super(Attention,self).__init__()\n        self.encoder_att = nn.Linear(encoder_dim,attention_dim)\n        self.decoder_att = nn.Linear(decoder_dim,attention_dim)\n        self.full_att = nn.Linear(attention_dim,1)#linear layer to calculate the value to be softmaxed\n        self.relu = nn.ReLU()\n        self.softmax = nn.Softmax(dim=1)# softmax layer to calculate the weights\n        \n    def forward(self,encoder_out,decoder_hidden):\n        \"\"\"\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n        :return: attention weighted encoding, weights\n        \"\"\"\n        att1 = self.encoder_att(encoder_out) \n        att2 = self.decoder_att(decoder_hidden)\n        att_full = self.full_att(att1+att2.unsqueeze(1)).squeeze(2) #(batch,num_pixels)\n        alpha = self.softmax(att_full) #(batch,num_pixels)\n        attention_weighted_encoding = (encoder_out* alpha.unsqueeze(2)).sum(dim=1) #(batch_size,encoder_dim)\n        \n        return attention_weighted_encoding, alpha\n        \n        ","9fe5df62":"class Decoder(nn.Module):\n    def __init__(self,attention_dim,embed_dim, decoder_dim, vocab_size,encoder_dim = 2048, dropout = 0.5):\n        super(Decoder,self).__init__()\n        self.encoder_dim = encoder_dim\n        self.decoder_dim = decoder_dim\n        self.attention_dim = attention_dim\n        self.embed_dim = embed_dim\n        self.vocab_size = vocab_size\n#         self.dropout = dropout\n        \n        self.attention = Attention(encoder_dim,decoder_dim,attention_dim)\n        \n        self.embedding = nn.Embedding(vocab_size,embed_dim) #embedding layer\n        self.dropout = nn.Dropout(p=dropout)\n        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n        self.init_h = nn.Linear(encoder_dim, decoder_dim) #linear layer to find initial hidden layer in LSTM\n        self.init_c = nn.Linear(encoder_dim, decoder_dim) #linear layer to find initial cell layer in LSTM\n        self.f_beta = nn.Linear(decoder_dim, encoder_dim) #linear layer to find create a sigmoid-activated gate\n        \n        self.sigmoid = nn.Sigmoid()\n        self.fc = nn.Linear(decoder_dim, vocab_size) #linear layer to find scores over vocabulary\n        self.init_weights()\n        \n    def init_weights(self):\n        \"\"\"\n        Initialization over uniform distribution\n        \"\"\"\n        self.embedding.weight.data.uniform_(-0.1,0.1)\n        self.fc.bias.data.fill_(0)\n        self.fc.weight.data.uniform_(-0.1,0.1)\n\n    def load_pretrained_embeddings(self,embedding):\n        \"\"\"\n        Loads pretrained embeddings\n        \"\"\"\n        self.embedding.weight = nn.Parameter(embedding)\n    def fine_tune_embeddings(self,fine_tune=True):\n        \"\"\"\n        Unless using pretrained embeddings, keep it true\n        \"\"\"\n        for p in self.embedding.parameters():\n            p.requires_grad=fine_tune\n            \n    def init_hidden_state(self,encoder_out):\n        \"\"\"\n        Creates initial hidden and cell state of the LSTM based on the encoded images.\n        :encoder_out : encoded images, a tensor of dimension(batch_size, num_of_pixels,encoder_dim)\n        :return hidden and cell state\n        \"\"\"\n        mean_encoder_out = encoder_out.mean(dim=1)\n        h = self.init_h(mean_encoder_out) #(batch_size,decoder_dim) output\n        c = self.init_c(mean_encoder_out)\n        return h,c\n    \n    def forward(self, encoder_out, encoded_captions, caption_lengths):\n        \"\"\"\n        Forward propagation.\n        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n        \"\"\"\n\n        batch_size = encoder_out.size(0)\n        encoder_dim = encoder_out.size(-1)\n        vocab_size = self.vocab_size\n\n        # Flatten image\n        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        # Sort input data by decreasing lengths; why? apparent below\n        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n        encoder_out = encoder_out[sort_ind]\n        encoded_captions = encoded_captions[sort_ind]\n\n        # Embedding\n        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n\n        # Initialize LSTM state\n        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n\n        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n        # So, decoding lengths are actual lengths - 1\n        decode_lengths = (caption_lengths - 1).tolist()\n\n        # Create tensors to hold word predicion scores and alphas\n        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n\n        # At each time-step, decode by\n        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n        for t in range(max(decode_lengths)):\n            batch_size_t = sum([l > t for l in decode_lengths])\n            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n                                                                h[:batch_size_t])\n            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n            attention_weighted_encoding = gate * attention_weighted_encoding\n            h, c = self.decode_step(\n                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n            predictions[:batch_size_t, t, :] = preds\n            alphas[:batch_size_t, t, :] = alpha\n\n        return predictions, encoded_captions, decode_lengths, alphas, sort_ind\n            \n","328b113a":"def init_embeddings(embeddings):\n    \"\"\"\n    Fills embeddings with uniform information\n    \"\"\"\n    bias = np.sqrt(3.0\/embeddings.size(1))\n    torch.nn.init.uniform_(embeddings,-bias,bias)\n\ndef load_embeddings(emb_file,word_map):\n    \"\"\"\n    Creates an embedding tensor for the specified word map, for loading into the model.\n    :param emb_file: file containing embeddings (stored in GloVe format)\n    :param word_map: word map\n    :return: embeddings in the same order as the words in the word map, dimension of embeddings\n    \"\"\"\n    with open(emb_file,'r') as f:\n        emb_dim = len(f.readline().split(' ')) - 1\n    vocab = set(word_map.keys())\n    \n    embeddings = torch.FloatTensor(len(vocab), emb_dim)\n    init_embeddings(embeddings)\n    \n    print(\"Loading embeddings\")\n    for line in open(emb_file,'r'):\n        line = line.split(' ')\n        \n        emb_words = line[0]\n        embedding = list(map(lambda t: float(t), filter(lambda n: n and not n.isspace(), line[1:])))\n        \n        if emb_file not in vocab:\n            continue\n        embeddings[word_map[emb_word]] = torch.FloatTensor(embedding)\n    return embeddings, emb_dim\n\ndef clip_gradient(optimizer, grad_clip):\n    \"\"\"\n    Clips gradients during backprop\n    \"\"\"\n    for group in optimizer.param_groups:\n        for param in group['params']:\n            if param.grad is not None:\n                param.grad.data.clamp_(-grad_clip,grad_clip)\ndef save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n                    bleu4, is_best):\n    \"\"\"\n    Saves model checkpoint.\n    :param data_name: base name of processed dataset\n    :param epoch: epoch number\n    :param epochs_since_improvement: number of epochs since last improvement in BLEU-4 score\n    :param encoder: encoder model\n    :param decoder: decoder model\n    :param encoder_optimizer: optimizer to update encoder's weights, if fine-tuning\n    :param decoder_optimizer: optimizer to update decoder's weights\n    :param bleu4: validation BLEU-4 score for this epoch\n    :param is_best: is this checkpoint the best so far?\n    \"\"\"\n    state = {'epoch': epoch,\n             'epochs_since_improvement': epochs_since_improvement,\n             'bleu-4': bleu4,\n             'encoder_state_dict': encoder.state_dict(),\n             'decoder_state_dict': decoder.state_dict(),\n             'encoder_optimizer_state_dict': encoder_optimizer.state_dict() if encoder_optimizer is not None else None,\n             'decoder_optimizer_state_dict': decoder_optimizer.state_dict() if decoder_optimizer is not None else None}\n    filename = 'checkpoint_' + data_name + '.pth.tar'\n    torch.save(state, filename)\n    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n    if is_best:\n        torch.save(state, 'BEST_' + filename)\n\n\nclass AverageMeter(object):\n    \"\"\"\n    Keeps track of most recent, average, sum, and count of a metric.\n    \"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\ndef adjust_learning_rate(optimizer, shrink_factor):\n    \"\"\"\n    Shrinks learning rate by a specified factor.\n    :param optimizer: optimizer whose learning rate must be shrunk.\n    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n    \"\"\"\n\n    print(\"\\nDECAYING learning rate.\")\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = param_group['lr'] * shrink_factor\n    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n\n\ndef accuracy(scores, targets, k):\n    \"\"\"\n    Computes top-k accuracy, from predicted and true labels.\n    :param scores: scores from the model\n    :param targets: true labels\n    :param k: k in top-k accuracy\n    :return: top-k accuracy\n    \"\"\"\n\n    batch_size = targets.size(0)\n    _, ind = scores.topk(k, 1, True, True)\n    correct = ind.eq(targets.view(-1, 1).expand_as(ind))\n    correct_total = correct.view(-1).float().sum()  # 0D tensor\n    return correct_total.item() * (100.0 \/ batch_size)    ","6074f427":"# Model Parameters\nemb_dim = 512\nattention_dim = 512\ndecoder_dim = 512\ndropout = 0.5\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# cudnn.benchmark = True #set to True if inputs to model are fixed size\n\n#Training Parameters\nstart_epoch = 0\nepochs = 32\nepochs_since_improvement = 0\nbatch_size = 64\nencoder_lr = 1e-4\ndecoder_lr = 4e-4\ngrad_clip = 5.0\nalpha_c = 1.0 #will be used in doubly attention loss part\nbest_bleu4 = 0\nprint_freq = 100\nfine_tune_encoder = False\ncheckpoint = None","66005d5d":"decoder = Decoder(attention_dim=attention_dim,\n                 embed_dim = emb_dim,\n                 decoder_dim=decoder_dim,\n                 vocab_size = len(word_map)+2,\n                 dropout=dropout)\noptim_decoder = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n                                             lr=decoder_lr)\nencoder = Encoder()\nencoder.fine_tune(fine_tune_encoder)\noptim_encoder = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n                                             lr=encoder_lr) if fine_tune_encoder else None\nif checkpoint:\n    checkpoint = torch.load(checkpoint)\n    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n    optim_decoder.load_state_dict(checkpoint['decoder_optimizer_state_dict'])\n    optim_encoder.load_state_dict(checkpoint['encoder_optimizer_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1\n    epochs_since_improvement = checkpoint['epochs_since_improvement']\n    best_bleu4 = checkpoint['bleu-4']\n    if fine_tune_encoder and encoder:\n        encoder.fine_tune(fine_tune_encoder)\n        optim_encoder = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n                                             lr=encoder_lr)\ndecoder.to(device)\nencoder.to(device)\ncriterion = nn.CrossEntropyLoss().to(device)","16d1dadf":"from torch.nn.utils.rnn import pack_padded_sequence\nimport time\ndef train(train_loader,encoder, decoder, criterion, optim_encoder, optim_decoder,epoch):\n    decoder.train()\n    encoder.train()\n    \n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    top5acc = AverageMeter()\n    \n    start = time.time()\n    \n    for i , (imgs,caps,caplens) in enumerate(train_loader):\n        data_time.update(time.time()-start)\n        imgs = imgs.to(device)\n        caps = caps.to(device)\n        caplens = caplens.to(device)\n        \n        #Forward Prop\n        \n        imgs = encoder(imgs)\n        scores,cap_sorted, decode_lengths, alphas, sort_ind = decoder(imgs,caps,caplens)\n        \n        targets = cap_sorted[:,1:]\n        \n        # Remove timesteps that we didn't decode at, or are pads\n        # pack_padded_sequence is an easy trick to do this\n        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n        \n        loss = criterion(scores,targets)\n        \n        #Add doubly stochastic attention regularization\n        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n        \n        #Backprop\n        optim_decoder.zero_grad()\n        if optim_encoder:\n            optim_encoder.zero_grad()\n        loss.backward()\n        \n        if grad_clip:\n            clip_gradient(optim_decoder,grad_clip)\n            if optim_encoder:\n                clip_gradient(optim_encoder,grad_clip)\n        \n        #Update weights\n        optim_decoder.step()\n        if optim_encoder:\n            optim_encoder.step()\n        \n        top5 = accuracy(scores, targets, 5)\n        losses.update(loss.item(), sum(decode_lengths))\n        top5acc.update(top5, sum(decode_lengths))\n        batch_time.update(time.time() - start)\n\n        start = time.time()\n\n        # Print status\n        if i % print_freq == 0:\n            print('Epoch: [{0}][{1}\/{2}]\\t'\n                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n                                                                          batch_time=batch_time,\n                                                                          data_time=data_time, loss=losses,\n                                                                          top5=top5acc))","9fccef1e":"from nltk.translate.bleu_score import corpus_bleu\ndef validate(val_loader, encoder,decoder, criterion):\n    decoder.eval()\n    if encoder:\n        encoder.eval()\n    batch_time = AverageMeter()\n    losses = AverageMeter()\n    top5accs = AverageMeter()\n\n    start = time.time()\n\n    references = list()  # references (true captions) for calculating BLEU-4 score\n    hypotheses = list()  # hypotheses (predictions)\n\n    with torch.no_grad():\n        # Batches\n        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n\n            # Move to device, if available\n            imgs = imgs.to(device)\n            caps = caps.to(device)\n            caplens = caplens.to(device)\n\n            # Forward prop.\n            if encoder is not None:\n                imgs = encoder(imgs)\n            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n\n            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n            targets = caps_sorted[:, 1:]\n\n            # Remove timesteps that we didn't decode at, or are pads\n            # pack_padded_sequence is an easy trick to do this\n            scores_copy = scores.clone()\n            scores  = pack_padded_sequence(scores, decode_lengths, batch_first=True).data\n            targets  = pack_padded_sequence(targets, decode_lengths, batch_first=True).data\n\n            # Calculate loss\n            loss = criterion(scores, targets)\n\n            # Add doubly stochastic attention regularization\n            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n\n            # Keep track of metrics\n            losses.update(loss.item(), sum(decode_lengths))\n            top5 = accuracy(scores, targets, 5)\n            top5accs.update(top5, sum(decode_lengths))\n            batch_time.update(time.time() - start)\n\n            start = time.time()\n\n            if i % print_freq == 0:\n                print('Validation: [{0}\/{1}]\\t'\n                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n                                                                                loss=losses, top5=top5accs))\n\n            # Store references (true captions), and hypothesis (prediction) for each image\n            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n\n            # References\n            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n            for j in range(allcaps.shape[0]):\n                img_caps = allcaps[j].tolist()\n                img_captions = list(\n                    map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n                        img_caps))  # remove <start> and pads\n                references.append(img_captions)\n\n            # Hypotheses\n            _, preds = torch.max(scores_copy, dim=2)\n            preds = preds.tolist()\n            temp_preds = list()\n            for j, p in enumerate(preds):\n                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n            preds = temp_preds\n            hypotheses.extend(preds)\n\n            assert len(references) == len(hypotheses)\n\n        # Calculate BLEU-4 scores\n        bleu4 = corpus_bleu(references, hypotheses)\n\n        print(\n            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n                loss=losses,\n                top5=top5accs,\n                bleu=bleu4*100))\n\n    return bleu4","9913bf74":"from tqdm import tqdm\nfor epoch in tqdm(range(start_epoch,epochs)):\n    if epochs_since_improvement > 8:\n        break\n    if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n            adjust_learning_rate(optim_decoder, 0.8)\n            if fine_tune_encoder:\n                adjust_learning_rate(optim_encoder, 0.8)\n    train(train_loader,encoder,decoder,criterion,optim_encoder,optim_decoder,epoch)\n    recent_bleu4 = validate(val_loader,encoder,decoder,criterion)\n    is_best = recent_bleu4 > best_bleu4\n    if recent_bleu4 > best_bleu4:\n        best_bleu4 = recent_bleu4\n        epochs_since_improvement = 0\n    else:\n        epochs_since_improvement += 1\n        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n    save_checkpoint('caption_model', epoch, epochs_since_improvement, encoder, decoder, optim_encoder,\n                        optim_decoder, recent_bleu4, is_best)","d4759eb9":"import torch.nn.functional as F\ndef evaluate(beam_size,loader):\n    \"\"\"\n    Evaluation\n    :param beam_size: beam size at which to generate captions for evaluation\n    :return: BLEU-4 score\n    \"\"\"\n    # DataLoader\n    \n\n    # TODO: Batched Beam Search\n    # Therefore, do not use a batch_size greater than 1 - IMPORTANT!\n\n    # Lists to store references (true captions), and hypothesis (prediction) for each image\n    # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n    # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n    references = list()\n    hypotheses = list()\n\n    # For each image\n    for i, (image, caps, caplens, allcaps) in enumerate(\n            tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n\n        k = beam_size\n\n        # Move to GPU device, if available\n        image = image.to(device)  # (1, 3, 256, 256)\n\n        # Encode\n        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n        enc_image_size = encoder_out.size(1)\n        encoder_dim = encoder_out.size(3)\n\n        # Flatten encoding\n        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n        num_pixels = encoder_out.size(1)\n\n        # We'll treat the problem as having a batch size of k\n        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n\n        # Tensor to store top k previous words at each step; now they're just <start>\n        k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n\n        # Tensor to store top k sequences; now they're just <start>\n        seqs = k_prev_words  # (k, 1)\n\n        # Tensor to store top k sequences' scores; now they're just 0\n        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n\n        # Lists to store completed sequences and scores\n        complete_seqs = list()\n        complete_seqs_scores = list()\n\n        # Start decoding\n        step = 1\n        h, c = decoder.init_hidden_state(encoder_out)\n\n        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n        while True:\n\n            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n\n            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n\n            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n            awe = gate * awe\n\n            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n\n            scores = decoder.fc(h)  # (s, vocab_size)\n            scores = F.log_softmax(scores, dim=1)\n\n            # Add\n            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n\n            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n            if step == 1:\n                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n            else:\n                # Unroll and find top scores, and their unrolled indices\n                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n\n            # Convert unrolled indices to actual indices of scores\n            vocab_size = len(word_map)+2\n            prev_word_inds = top_k_words \/ vocab_size  # (s)\n            next_word_inds = top_k_words % vocab_size  # (s)\n\n            # Add new words to sequences\n            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n\n            # Which sequences are incomplete (didn't reach <end>)?\n            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n                               next_word != word_map['<end>']]\n            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n\n            # Set aside complete sequences\n            if len(complete_inds) > 0:\n                complete_seqs.extend(seqs[complete_inds].tolist())\n                complete_seqs_scores.extend(top_k_scores[complete_inds])\n            k -= len(complete_inds)  # reduce beam length accordingly\n\n            # Proceed with incomplete sequences\n            if k == 0:\n                break\n            seqs = seqs[incomplete_inds]\n            h = h[prev_word_inds[incomplete_inds]]\n            c = c[prev_word_inds[incomplete_inds]]\n            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n\n            # Break if things have been going on too long\n            if step > 50:\n                break\n            step += 1\n        try:\n            \n            i = complete_seqs_scores.index(max(complete_seqs_scores))\n            seq = complete_seqs[i]\n        except:\n            i=0\n\n        # References\n        img_caps = allcaps[0].tolist()\n        img_captions = list(\n            map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}],\n                img_caps))  # remove <start> and pads\n        references.append(img_captions)\n\n        # Hypotheses\n        hypotheses.append([w for w in seq if w not in {word_map['<start>'], word_map['<end>'], word_map['<pad>']}])\n\n        assert len(references) == len(hypotheses)\n\n    # Calculate BLEU-4 scores\n    bleu4 = corpus_bleu(references, hypotheses)\n\n    return bleu4\n","44a47dfc":"beam_size=1\nprint(\"\\nBLEU-4 score @ beam size of %d is %.4f.\" % (beam_size, evaluate(beam_size,test_loader)*100))","9fe053d7":"print(\"\\nBLEU-4 score @ beam size of %d is %.4f.\" % (3, evaluate(3,test_loader)*100))","ea69e913":"### Training Loop","bbcff342":"Token train is the list of captions that will be used for training","4af90503":"### Creating Util functions\nWe create some utility functions such as loading pretrained word embeddings, computing measures, etc.","679f4374":"### Decoder\n* Once the Encoder generates the encoded image, we transform the encoding to create the initial hidden state h (and cell state C) for the LSTM Decoder.\n* At each decode step,  \n    the encoded image and the previous hidden state is used to generate weights for each pixel in the Attention network.  \n    the previously generated word and the weighted average of the encoding are fed to the LSTM Decoder to generate the next word.  \n\n![](https:\/\/raw.githubusercontent.com\/sgrvinod\/a-PyTorch-Tutorial-to-Image-Captioning\/master\/img\/model.png)","194a711c":"Our decoder consists of a long list of LSTMs. The Decoder's job is to look at the encoded image and generate a caption word by word.  \nIn a typical setting without Attention, you could simply average the encoded image across all pixels. You could then feed this, with or without a linear transformation, into the Decoder as its first hidden state and generate the caption. Each predicted word is used to generate the next word.  \n![Without Attention](https:\/\/github.com\/sgrvinod\/a-PyTorch-Tutorial-to-Image-Captioning\/raw\/master\/img\/decoder_no_att.png)","edb5a46a":"### Training the Model\nWe now set the variables and with all our pieces intact we can start training","6459b2bf":"## Creating Pytorch Dataset","8be7b8b6":"### Attention\nThis part is the one which tells where the decoder should look before predicting any word. We use soft attention where the pixels add upto 1.  This could be interpreted as the probability that the given pixel is the place to look at to generate the word.\n![](https:\/\/github.com\/sgrvinod\/a-PyTorch-Tutorial-to-Image-Captioning\/raw\/master\/img\/weights.png)","9f093590":"# Image Captioning\nWe will be emulating \"Show Attend and Tell\" on Flickr 8k dataset. We will be following the [tutorial](https:\/\/github.com\/sgrvinod\/a-PyTorch-Tutorial-to-Image-Captioning). Note: We will add proper explanations at each step to make it a self contained guide.","41882838":"## Model\n### Encoder\nWe now define the model architecture. In very crude terms it is a encoder decoder architecture where encoder will encode the image features and the decoder will use those features to extract the caption. \nThe encoder returns a tensor of size `2048x14x14`. This is then the input to our decoder based LSTM.\n![Encoder](https:\/\/raw.githubusercontent.com\/sgrvinod\/a-PyTorch-Tutorial-to-Image-Captioning\/master\/img\/encoder.png)","76e9330f":"### Validation Loop","b73386b9":"### Creating Vocabulary","b10bc458":"We complete show attend and tell with this. Special thanks to [sagar vinod](https:\/\/github.com\/sgrvinod\/a-PyTorch-Tutorial-to-Image-Captioning) for this amazing tutorial. Will be improved upon with Glove and better embeddings ","2ceaa750":"## Preparing Caption Dataset\nWe need to create a vocabulary for all the words that are present in the image caption dataset. Once we have that the next step is to create our caption vectors. To do that we first choose\n+ Length of the caption\n+ Frequency of the words  \n\nWe want to pad the sentences upto the max length (213) with `<pad>` token. \nOnce that is done, we decide the threshold of the frequency of the words. If the word has lower count than some threshold it is replaced by `<unk>` keyword. Therefore each caption is supposed to look like -   \n`<start> A little girl is sitting in <unk> front of a large <unk> rainbow . <end> <pad> <pad> <pad> ....` where we assume frequency of `front` and `painted` is less than the threshold.  \nOnce we have done that the next step is to tokenize the vectors. This is nothing but just substituting the words with some number. So `little` could be represented by 43. Note keep `<pad>` as 0 for implementation purposes  \nTherefore, captions fed to the model must be an Int tensor of dimension N, L where L is the padded length.\ncaption lengths fed to the model must be an Int tensor of dimension N.\n\n### Observations\n+ Over 6% captions have length > 100. For the purpose of learning we dont tinker with that\n+ Over 42% words have a frequency of 1. Technically we should set a threshold but the score could be severely affected if we use `<unk>`. Thus we dont make those changes","c9983655":"With setting, the word takes the image into account as well. Thus instead of simple average we take weighted average focussing on the word in question.\n![Attention](https:\/\/raw.githubusercontent.com\/sgrvinod\/a-PyTorch-Tutorial-to-Image-Captioning\/master\/img\/decoder_att.png)","eb105d5d":"Our dataset consisits of Images and text. Each Image has 5 captions associated with it in the form of imageid caption. Now we need to do some major data processing. Firstly our "}}