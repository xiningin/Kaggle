{"cell_type":{"c525f39e":"code","231269d4":"code","919d583c":"code","a85f7887":"code","6372fcae":"code","390ce9a8":"code","43556a45":"code","bcc1c675":"code","2af572e9":"code","a946c8bf":"code","cedf42e2":"code","92a7edad":"code","c25919f0":"code","b0636913":"code","57dfe9df":"code","f59b43de":"code","850747b3":"code","8deea99c":"code","3b8189c3":"code","98521984":"code","23663039":"code","3b61e989":"code","6dc1e045":"code","1336aaa8":"code","1aad7595":"code","08c37dae":"code","d3ecc143":"code","0bf5b24b":"code","64ccda58":"code","30fd107a":"code","86813d3b":"code","56853777":"code","f89fdf3d":"code","cb80561e":"code","86ab15a4":"code","2bca9e7c":"code","90e48b18":"code","e4161534":"code","490d838f":"code","e2babe81":"code","4ca4f806":"code","64848de0":"code","90940d28":"code","99cf2033":"code","3035bb30":"code","6acc79fb":"code","0ed7b673":"code","9acf4ff8":"code","bad24286":"code","f36331ec":"markdown","f749aec0":"markdown","e19bd851":"markdown","df157652":"markdown","d0230ff4":"markdown","677dc59d":"markdown","e7f20368":"markdown","c94bf0cc":"markdown","c2f20b75":"markdown","7c888927":"markdown","48c1b4bf":"markdown","6a0cef4e":"markdown","9c3d5869":"markdown","80e6b77c":"markdown","da998a53":"markdown","08a1c701":"markdown","2455fa87":"markdown","55443684":"markdown","2cd11d9e":"markdown","5623dbff":"markdown","af65caa0":"markdown","cb9fab76":"markdown","b2b1e0cd":"markdown","33c1900e":"markdown","12c56aa8":"markdown","f5c9bfc5":"markdown","09eee45f":"markdown","db900ab6":"markdown","6591db69":"markdown","e173e040":"markdown","21335dec":"markdown","35af84fc":"markdown","7ceba28b":"markdown","6330c2e1":"markdown","f2ddc83d":"markdown","c2a33af0":"markdown","e9c91903":"markdown","c4b63adf":"markdown","c1203ea2":"markdown"},"source":{"c525f39e":"from numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\nimport math\nimport pydicom\nimport pylab\nimport os\nimport pickle\n\nfrom sklearn.model_selection import train_test_split\nfrom skimage.transform import resize\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# Don't Show Warning Messages\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport gc; gc.enable()\n","231269d4":"os.listdir('..\/input')","919d583c":"# load the pickled dataframes\n\ndf_train = pickle.load(open('..\/input\/python-generators-to-reduce-ram-usage-part-1\/dftrain.pickle','rb'))\ndf_test = pickle.load(open('..\/input\/python-generators-to-reduce-ram-usage-part-1\/dftest.pickle','rb'))\n\n\nprint(df_train.shape)\nprint(df_test.shape)","a85f7887":"# Source: https:\/\/www.kaggle.com\/peterchang77\/exploratory-data-analysis\n\ndef parse_data(df):\n    \"\"\"\n    Method to read a CSV file (Pandas dataframe) and parse the \n    data into the following nested dictionary:\n\n      parsed = {\n        \n        'patientId-00': {\n            'dicom': path\/to\/dicom\/file,\n            'label': either 0 or 1 for normal or pnuemonia, \n            'boxes': list of box(es)\n        },\n        'patientId-01': {\n            'dicom': path\/to\/dicom\/file,\n            'label': either 0 or 1 for normal or pnuemonia, \n            'boxes': list of box(es)\n        }, ...\n\n      }\n\n    \"\"\"\n    # --- Define lambda to extract coords in list [y, x, height, width]\n    extract_box = lambda row: [row['y'], row['x'], row['height'], row['width']]\n\n    parsed = {}\n    for n, row in df.iterrows():\n        # --- Initialize patient entry into parsed \n        pid = row['patientId']\n        if pid not in parsed:\n            parsed[pid] = {\n                'dicom': '..\/input\/stage_1_train_images\/%s.dcm' % pid,\n                'label': row['Target'],\n                'boxes': []}\n\n        # --- Add box if opacity is present\n        if parsed[pid]['label'] == 1:\n            parsed[pid]['boxes'].append(extract_box(row))\n\n    return parsed","6372fcae":"# define a function to output a row containing all box info incl. confidence scores\n\ndef create_bounding_rows(df_train):\n    \n    \"\"\"\n    Takes each patientId and creates a row of combined bounding boxes and \n    also includes their confidence scores. All patientId's are \n    included in one matrix.\n    This fuction is based on a max of 4 bounding boxes per patientId.\n    Output: Numpy matrix of shape(len(df_train), 20) \n    \"\"\"\n    \n    # read in the dataframe that will be parsed by the function parse_data(df)\n    df_boxes = \\\n    pd.read_csv('..\/input\/rsna-pneumonia-detection-challenge\/stage_1_train_labels.csv')\n\n    \n    # set the length depending on how many bounding boxes we want the model to output\n    length = 20\n    \n    h = np.ones(20)\n    k = np.zeros(20)\n\n    # create an empty numpy matrix matching the size of the output matrix\n    y = np.zeros((len(df_train),length))\n\n    # run the function\n    # this must be here because this must be run each time this script is run or\n    # the resulting matrix will have errors.\n    parsed = parse_data(df_boxes)\n\n\n    for i in range(0,len(df_train)):\n\n        # get the patientId\n        patientId = df_train.loc[i, 'patientId']\n\n        # extract the bounding boxes for a particular patient\n        box = parsed[patientId]['boxes']\n        if len(box) == 0:\n\n            # the first row becomes a dummy row of ones this must be deleted later\n            # k is an array of zeros\n            h = np.vstack((h,k))\n\n        if len(box) != 0:\n\n\n            # insert 1 as the first entry in each bounding box\n            # the 1 represents confidence for that bounding box\n            a=[]\n            for i in range(0,len(box)):\n                box[i].insert(0,1)\n                a = a + box[i]\n\n            # calculate how much padding to add\n            b = length - len(a)\n\n            # pad the list because not all lists have 4 bounding boxes\n            # we want all lists to have the same length\n            for i in range(0,b):\n                a.insert(len(a),0)\n\n            # reshape to horizontal because the above code makes the list vertical\n            a = np.array(a).reshape(1,length)\n            \n            # stack\n            h = np.vstack((h,a))\n\n    # delete the first row because we added this row just to make the code run\n    h = np.delete(h, 0, axis=0)\n    \n    return h\n\n\n# call the function\nbox_rows = create_bounding_rows(df_train)","390ce9a8":"# concat box_rows with df_y\n\n# put box_rows in a dataframe\ndf_y = pd.DataFrame(box_rows)\n\n# rename the columns in df_box_rows\nnew_names = ['conf_1', 'x_1', 'y_1', 'width_1', 'height_1',\n           'conf_2', 'x_2', 'y_2', 'width_2', 'height_2',\n           'conf_3', 'x_3', 'y_3', 'width_3', 'height_3',\n           'conf_4', 'x_4', 'y_4', 'width_4', 'height_4']\n\ndf_y.columns = new_names\n\n# Let's choose only the first two bounding boxes for each sample\ndf_y = df_y[['conf_1', 'x_1', 'y_1', 'width_1', 'height_1',\n           'conf_2', 'x_2', 'y_2', 'width_2', 'height_2']]\n\n# add the patientId column to df_y\ndf_y['patientId'] = df_train['patientId']","43556a45":"df_y.shape","bcc1c675":"df_y.head(2)","2af572e9":"# shuffle df_y\nfrom sklearn.utils import shuffle\n\ndf_y = shuffle(df_y)\n","a946c8bf":"df_train_images, df_val_images = train_test_split(df_y, test_size=0.20,\n                                                   random_state=5)\n\nprint(df_train_images.shape)\nprint(type(df_train_images))\nprint(df_val_images.shape)\nprint(type(df_val_images))","cedf42e2":"# Reset the index of df_train_images and df_val_images.\n\n# We do this because we are going to loop through these dataframes in the next step so\n# we need the index to be sequential, starting from 0.\n\ndf_train_images.reset_index(inplace=True)\n\ndf_val_images.reset_index(inplace=True)","92a7edad":"# Create a version without any unnecessary columns.\n\ndf_train = df_train_images.drop(['index', 'patientId'], axis=1)\n\ndf_val = df_val_images.drop(['index','patientId'], axis=1)\n\n# check that we have only 10 columns\nprint(df_train.shape)\nprint(df_val.shape)","c25919f0":"df_train_images.head(1)","b0636913":"df_val_images.head(1)","57dfe9df":"df_train.head(1)","f59b43de":"df_val.head(1)","850747b3":"# We have 20547 train images and 5137 validation images.","8deea99c":"def train_generator(df_train_images, df_train, batch_size, num_rows, num_cols):\n    \n    '''\n    Input: Dataframes, df_train_images and df_train\n    \n    Outputs one batch (X_train, y_train) on each iteration of the for loop.\n    \n    X_train:\n    Reads images from a folder, converts the images to a numpy array \n    with shape: (batch_size, num_rows, num_cols, 1)\n    \n    y_train:\n    Takes data from a pandas dataframe. Converts the data into a numpy array\n    with shape (batch_size, num_rows, num_cols, 1)\n    \n    '''\n    \n    \n    while True: \n\n        batch = []\n        k = 0\n\n\n        # note that we are rounding down.\n        num_batches = math.ceil(df_train_images.shape[0]\/batch_size)\n\n        # create an empty numpy array matching the number of images\n        image_array = np.zeros((batch_size,num_rows,num_cols))\n\n\n\n        # this loop runs only once each time the next() function is called.\n        for i in range(0,num_batches): # 20547 rows in train_images. we are using only 20000 of them\n\n            if i < num_batches-1:\n\n                # [1] Create X_train\n\n                # carve out 1000 rows of the 'patientId' column\n                batch = list(df_train_images['patientId'][k:(i+1)*batch_size])\n\n                #for patientId in batch:\n                for j in range(0,len(batch)):\n                    patientId = batch[j]\n\n\n                    path = \\\n                '..\/input\/rsna-pneumonia-detection-challenge\/stage_1_train_images\/%s.dcm' % patientId\n\n                    dcm_data = pydicom.read_file(path)\n\n                    # get the image as a numpy array\n                    image = dcm_data.pixel_array\n\n                    # resize the image\n                    small_image = resize(image,(num_rows,num_cols))\n\n                    # add the image to the empty numpy array\n                    image_array[j,:,:] = small_image\n\n                # reshape the array and normalize\n                X_train = image_array.reshape(batch_size,num_rows,num_cols,1)\/255\n\n                # [2] Create y_train\n\n                # note: Here we use df_train instead of df_train_images\n                # because we don't want the output to have the patientId column.\n\n                # carve out 1000 rows\n                y_train = df_train[k:(i+1)*batch_size]\n\n                # convert to a numpy array\n                y_train = y_train.values\n\n            # to cater for the last batch i.e. the fractional part\n            if i == num_batches-1: \n\n                batch_size_fractional = df_train.shape[0] - (batch_size*(num_batches-1)) # -1\n\n                # create an empty numpy array matching the number of images\n                image_array = np.zeros((batch_size_fractional,num_rows,num_cols))\n\n                # select rows from the tail of df_test upwards\n                batch1 = list(df_train_images['patientId'][-batch_size_fractional:]) #1000\n\n                #for patientId in batch:\n                for j in range(0,len(batch1)):\n                    patientId = batch1[j]\n\n                    path = \\\n            '..\/input\/rsna-pneumonia-detection-challenge\/stage_1_train_images\/%s.dcm' % patientId\n\n                    dcm_data = pydicom.read_file(path)\n\n                    # get the image as a numpy array\n                    image = dcm_data.pixel_array\n\n                    # resize the image\n                    small_image = resize(image,(num_rows,num_cols))\n\n                    # add the image to the empty numpy array\n                    image_array[j,:,:] = small_image\n\n                # reshape the array and normalize\n                X_train = image_array.reshape(batch_size_fractional,num_rows,num_cols,1)\/255\n\n                # [2] Create y_train\n\n                # note: Here we use df_val instead of df_val_images\n                # because we don't want the output to have the patientId column.\n\n                # carve out 1000 rows\n                y_train = df_train[-batch_size_fractional:]\n\n                # convert to a numpy array\n                y_train = y_train.values\n\n\n            k = k + batch_size\n\n            # For testing the generator so we can see how many batches it outputs\n            # by calling next(). Uncomment the next line for testing.\n            #print(i)\n\n            # Keras requires a tuple in the form (inputs,targets)\n            yield (X_train.astype(np.float32), y_train)\n            \n    \n","3b8189c3":"def val_generator(df_val_images, df_val, batch_size, num_rows, num_cols):\n    \n    '''\n    Input: Dataframes, df_val_images and df_val\n    \n    Outputs one batch (X_val, y_val) on each iteration of the for loop.\n    \n    X_val:\n    Reads images from a folder, converts the images to a numpy array \n    with shape: (batch_size, num_rows, num_cols, 1)\n    \n    y_val:\n    Takes data from a pandas dataframe. Converts the data into a numpy array\n    with shape (batch_size, num_rows, num_cols, 1)\n    \n    '''\n    \n    \n    while True: \n\n        batch = []\n        k = 0\n\n        # note that we are rounding up.\n        num_batches = math.ceil(df_val_images.shape[0]\/batch_size)\n\n        # Create an empty numpy array that matches the batch size.\n        image_array = np.zeros((batch_size,num_rows,num_cols))\n\n\n         # this loop runs only once each time the next() function is called.\n        for i in range(0,num_batches): \n            \n            if i < num_batches-1:\n\n                # [1] Create X_train\n\n                # carve out a batch of rows of the 'patientId' column\n                batch = list(df_val_images['patientId'][k:(i+1)*batch_size])\n\n                #for patientId in batch:\n                for j in range(0,len(batch)):\n                    patientId = batch[j]\n\n                    path = \\\n            '..\/input\/rsna-pneumonia-detection-challenge\/stage_1_train_images\/%s.dcm' % patientId\n\n                    dcm_data = pydicom.read_file(path)\n\n                    # get the image as a numpy array\n                    image = dcm_data.pixel_array\n\n                    # resize the image\n                    small_image = resize(image,(num_rows,num_cols))\n\n                    # add the image to the empty numpy array\n                    image_array[j,:,:] = small_image\n\n                # reshape the array and normalize\n                X_val = image_array.reshape(batch_size,num_rows,num_cols,1)\/255\n\n                # [2] Create y_train\n\n                # note: Here we use df_val instead of df_val_images\n                # because we don't want the output to have the patientId column.\n\n                # carve out 1000 rows\n                y_val = df_val[k:(i+1)*batch_size]\n\n                # convert to a numpy array\n                y_val = y_val.values\n\n             # to cater for the last batch i.e. the fractional part\n            if i == num_batches-1: \n\n                batch_size_fractional = df_val.shape[0] - (batch_size*(num_batches-1)) \n\n                # create an empty numpy array matching the number of images\n                image_array = np.zeros((batch_size_fractional,num_rows,num_cols))\n\n                # select rows from the tail of df_test upwards\n                batch1 = list(df_val_images['patientId'][-batch_size_fractional:]) \n\n                #for patientId in batch:\n                for j in range(0,len(batch1)):\n                    patientId = batch1[j]\n\n                    path = \\\n            '..\/input\/rsna-pneumonia-detection-challenge\/stage_1_train_images\/%s.dcm' % patientId\n\n                    dcm_data = pydicom.read_file(path)\n\n                    # get the image as a numpy array\n                    image = dcm_data.pixel_array\n\n                    # resize the image\n                    small_image = resize(image,(num_rows,num_cols))\n\n                    # add the image to the empty numpy array\n                    image_array[j,:,:] = small_image\n\n                # reshape the array and normalize\n                X_val = image_array.reshape(batch_size_fractional,num_rows,num_cols,1)\/255\n\n                # [2] Create y_train\n\n                # note: Here we use df_val instead of df_val_images\n                # because we don't want the output to have the patientId column.\n\n                # carve out a batch of rows\n                y_val = df_val[-batch_size_fractional:]\n\n                # convert to a numpy array\n                y_val = y_val.values\n\n\n            k = k + batch_size\n\n            # For testing the generator so we can see how many batches it outputs\n            # by calling next().\n            #print(i)\n\n            # Keras requires a tuple in the form (inputs,targets)\n            yield (X_val.astype(np.float32), y_val)\n            \n           \n    ","98521984":"df_test.head(1)","23663039":"df_test.shape","3b61e989":"# There are 1000 rows in df_test i.e. 1000 test images","6dc1e045":"def test_generator(df_test, batch_size, num_rows, num_cols):\n    \n    \"\"\"\n    Input: Dataframe df_test.\n    \n    Outputs one batch (X_test) on each iteration of the for loop.\n    \n    X_test:\n    Reads images from a folder, converts the images to a numpy array \n    with shape: (batch_size, num_rows, num_cols, 1)\n    \n    \"\"\"\n\n    batch = []\n    k = 0\n    \n    # note that we are rounding up.\n    num_batches = math.ceil(df_test.shape[0]\/batch_size)\n\n    # create an empty numpy array matching the number of images\n    image_array = np.zeros((batch_size,num_rows,num_cols))\n    \n    # this loop runs only once each time the next() function is called.\n    for i in range(0,num_batches):\n        \n        if i < num_batches-1:\n        \n            # [1] Create X_test\n\n            # carve out a batch of rows of the 'patientId' column\n            batch = list(df_test['patientId'][k:(i+1)*batch_size]) #1000\n\n            #for patientId in batch:\n            for j in range(0,len(batch)):\n                patientId = batch[j]\n\n                path = \\\n        '..\/input\/rsna-pneumonia-detection-challenge\/stage_1_test_images\/%s.dcm' % patientId\n\n                dcm_data = pydicom.read_file(path)\n\n                # get the image as a numpy array\n                image = dcm_data.pixel_array\n\n                # resize the image\n                small_image = resize(image,(num_rows,num_cols))\n\n                # add the image to the empty numpy array\n                image_array[j,:,:] = small_image\n\n            # reshape the array and normalize\n            X_test = image_array.reshape(batch_size,num_rows,num_cols,1)\/255\n            \n        # to cater for the last batch i.e. the fractional part\n        if i == num_batches-1: \n            \n            batch_size_fractional = df_test.shape[0] - (batch_size*(num_batches - 1))\n            \n            # create an empty numpy array matching the number of images\n            image_array = np.zeros((batch_size_fractional,num_rows,num_cols))\n            \n            # select rows from the tail of df_test upwards\n            batch = list(df_test['patientId'][-batch_size_fractional:]) #1000\n\n            \n            for j in range(0,len(batch)):\n                patientId = batch[j]\n\n                path = \\\n        '..\/input\/rsna-pneumonia-detection-challenge\/stage_1_test_images\/%s.dcm' % patientId\n\n                dcm_data = pydicom.read_file(path)\n\n                # get the image as a numpy array\n                image = dcm_data.pixel_array\n\n                # resize the image\n                small_image = resize(image,(num_rows,num_cols))\n\n                # add the image to the empty numpy array\n                image_array[j,:,:] = small_image\n\n            # reshape the array and normalize\n            X_test = image_array.reshape(batch_size_fractional,num_rows,num_cols,1)\/255\n            \n        \n        # For testing the generator so we can see how many batches it outputs\n        # by calling next(). Uncomment the next line for testing.\n        #print(i)\n        \n        k = k + batch_size\n        \n        # Keras requires a tuple in the form (inputs,targets)\n        yield (X_test.astype(np.float32))\n    ","1336aaa8":"# train_generator\n\n#train_gen = \\\n#train_generator(df_train_images, df_train, batch_size=50, num_rows=500, num_cols=500)\n\n#val_gen = \\\n#val_generator(df_val_images, df_val, batch_size=10, num_rows=500, num_cols=500)\n\n#test_gen = \\\n #test_generator(df_test, batch_size=1000, num_rows=500, num_cols=500)","1aad7595":"# Note: Each time this notebook cell is run, the generators will output only one batch.\n\n# If the generators are working correctly, the following shapes should be output:\n# X_train (10,500,500,1)\n# y_train (10,10)\n# X_val (10,500,500,1)\n# y_val (10,10)\n# X_test(10,500,500,1)\n\n# tuple unpacking\n#X_train, y_train = next(train_gen)\n#X_val, y_val = next(val_gen)\n#X_test = next(test_gen)\n\n#print(X_train.shape)\n#print(X_train.dtype)\n#print(y_train.shape)\n#print(X_val.shape)\n#print(X_val.dtype)\n#print(y_val.shape)\n#print(X_test.shape)\n#print(X_test.dtype)","08c37dae":"# Check the train_generator()\n#train_gen = \\\n#train_generator(df_train_images, df_train, batch_size=5000,, num_rows=500, num_cols=500)","d3ecc143":"#X_train, y_train = next(train_gen)\n\n#print(X_train.shape)\n#print(y_train.shape)","0bf5b24b":"# Check the val_generator()\n#val_gen = \\\n#val_generator(df_val_images, df_val, batch_size=2000, num_rows=500, num_cols=500)","64ccda58":"#X_val, y_val = next(val_gen)\n\n#print(X_val.shape)\n#print(y_val.shape) ","30fd107a":"# check test_generator\n#test_gen = \\\n#test_generator(df_test, batch_size=300, num_rows=500, num_cols=500)","86813d3b":"# Uncomment the print() function in test_generator() before running this cell. \n# Each time this cell is run the output should increment by 1. \n# The last number to be output should be 4.\n\n# Remember to re-quote the print() function after this test.\n\n# With 1000 test samples, a batch size of 300, and image size of 500x500\n# these are the shapes we should get with each iteration:\n\n# 0: (300,500,500,1)\n# 1: (300,500,500,1)\n# 2: (300,500,500,1)\n# 3: (100,500,500,1)\n# 4: Error\n\n# With 1000 samples and a batch_size of 300 the generator should only run for 4 loops.\n# Run this cell 5 times. On the fifth time you should get a \"StopIteration\" error.\n\n#X_test = next(test_gen)\n\n#X_test.shape","56853777":"# get the number of train and val images\n\nprint(df_train.shape)\nprint(df_val.shape)\nprint(df_test.shape)","f89fdf3d":"########################\n# INPUTS\n\n# Set the batch sizes:\n\ntrain_batch_size = 10\nval_batch_size = 10\ntest_batch_size = 1\n\n# Set the image size:\n\nnum_rows = 1024\nnum_cols = 1024\n\n#########################\n\n# train_generator\ntrain_gen = \\\ntrain_generator(df_train_images, df_train, train_batch_size, num_rows, num_cols)\n\nnum_train_samples = df_train.shape[0]\n\nnum_train_batches = math.ceil(num_train_samples\/train_batch_size) # round down\n\n\n# val_generator\nval_gen = \\\nval_generator(df_val_images, df_val, val_batch_size, num_rows, num_cols)\n\nnum_val_samples = df_val.shape[0]\n\nnum_val_batches = math.ceil(num_val_samples\/val_batch_size) # round down\n\n# test_generator\ntest_gen = \\\ntest_generator(df_test, test_batch_size, num_rows, num_cols)\n\nnum_test_samples = df_test.shape[0]\n\nnum_test_batches = math.ceil(num_test_samples\/test_batch_size) # round up\n","cb80561e":"from keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dense, Dropout, Flatten\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu',\n                        input_shape=(num_rows, num_cols, 1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(10, activation='linear'))\n\n\nmodel.summary()","86ab15a4":"# compile the model\nAdam_opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\nmodel.compile(optimizer=Adam_opt, loss='mse')","2bca9e7c":"# Notes: \n# To reduce RAM use it's best to keep 'max_queue_size' small.\n# The test and val generators run infinitely therefore we must set \n# steps_per_epoch=num_train_batches and validation_steps=num_val_batches so \n# that fit_generator() knows when to stop an epoch and to ensure that the model sees\n# the same batch only once.\n\nfilepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\ncallbacks_list = [checkpoint]\n\n\nhistory = model.fit_generator(generator=train_gen, \n                        steps_per_epoch=num_train_batches, \n                        epochs=3, \n                        verbose=1, \n                        callbacks=callbacks_list, \n                        validation_data=val_gen,\n                        validation_steps=num_val_batches, \n                        class_weight=None, \n                        max_queue_size=2, \n                        workers=4,\n                        use_multiprocessing=True, \n                        shuffle=False, \n                        initial_epoch=0)\n","90e48b18":"import matplotlib.pyplot as plt\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\nplt.legend()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","e4161534":"# Initialize the test generator\n# Note: Put the intilization in the same cell as the prediction step because the \n# test generator was not designed to run infinitely. We want the prediction process\n# to always start at the first batch and run only once.\n\n# I keep the test_batch_size=1 just to ensure that nothing strange happens.\n\ntest_gen = \\\ntest_generator(df_test, test_batch_size, num_rows, num_cols)\n\nmodel.load_weights(filepath = 'model.h5')\npredictions = model.predict_generator(test_gen, \n                                      steps=num_test_batches, \n                                      max_queue_size=1, \n                                      workers=1, \n                                      use_multiprocessing=False, \n                                      verbose=1)","490d838f":"predictions.shape","e2babe81":"predictions[1]","4ca4f806":"# put the predictions into a dataframe\ndf_preds = pd.DataFrame(predictions)\n\n# add column names\nnew_names = ['conf_1', 'x_1', 'y_1', 'width_1', 'height_1',\n       'conf_2', 'x_2', 'y_2', 'width_2', 'height_2']\n\ndf_preds.columns = new_names\n\n# add the patientId column\ndf_preds['patientId'] = df_test['patientId']\n\n# add the PredictionString column\ndf_preds['PredictionString'] = 0","64848de0":"df_preds.head()","90940d28":"# Version 2: Changes were made. See comments below.\n\ndef process_preds(df):\n    \n    limit = 0.5\n    \n    conf_1 = 0\n    conf_2 = 0\n    conf_3 = 0\n    conf_4 = 0\n    \n    string_1 = ''\n    string_2 = ''\n    string_3 = ''\n    string_4 = ''\n    \n    \n    for i in range(0,len(df)):\n        \n        #get the conf scores\n        conf_1 = df.loc[i,'conf_1'] # revised in Version 2\n        conf_2 = df.loc[i,'conf_2'] # revised in Version 2\n        \n        if conf_1 >= limit:\n            string_1 = \\\n            str(conf_1) + ' ' + str(round(df.loc[i,'x_1']))+ ' ' + \\\n            str(round(df.loc[i,'y_1']))+ ' ' + str(round(df.loc[i,'width_1']))+ ' ' + str(round(df.loc[i,'height_1']))\n\n        if conf_2 >= limit:\n            string_2 = \\\n            str(conf_2) + ' ' + str(round(df.loc[i,'x_2']))+ ' ' + \\\n            str(round(df.loc[i,'y_2']))+ ' ' + str(round(df.loc[i,'width_2']))+ ' ' + str(round(df.loc[i,'height_2']))\n\n        df.loc[i,'PredictionString']  = \\\n        string_1 + ' ' + string_2 \n\n    df_submission = df[['patientId', 'PredictionString']]\n    \n    return df_submission\n\n# call the function\ndf_submission = process_preds(df_preds)","99cf2033":"df_submission.head()","3035bb30":"\nID = df_preds['patientId']\npreds = df_preds['PredictionString']\n\nsubmission = pd.DataFrame({'patientId':ID, \n                           'PredictionString':preds, \n                          }).set_index('patientId')\n\nsubmission.to_csv('pneu_keras_model.csv', columns=['PredictionString']) ","6acc79fb":"# This is a simple example of a generator.\ndef my_generator():\n    for i in range(0,3):\n        yield print(i)\n\nmy_gen = my_generator()\n","0ed7b673":"# If you run this cell 3 times  you will notice that the output increases by 1 each time.\n# On the 4th iteration there will be a 'StopIteration'.\n\nout_put = next(my_gen)\nout_put","9acf4ff8":"# source: @Liquid_Fire\n#https:\/\/stackoverflow.com\/questions\/3704918\/\n    #python-way-to-restart-a-for-loop-similar-to-continue-for-while-loops\n\n# To use fit_generator() in keras our generator needs to loop infinitely.\n# This is how to do that:\n\ndef my_generator():\n\n   \n    while True: \n        \n        for i in range(0,4):\n            \n            yield i\n        \n        \n\ninfinity_gen = my_generator()\n","bad24286":"# if you run this cell you will see that a 'StopIteration' never happens.\nout_put = next(infinity_gen)\nout_put","f36331ec":"### Create the Data Generators","f749aec0":"Excellent kernel by @Peter and friends: <br>\nhttps:\/\/www.kaggle.com\/peterchang77\/exploratory-data-analysis\n\nKeras info on fit_generator() that explains what the input format needs to be: <br>\nhttps:\/\/keras.io\/models\/sequential\/\n\nBlog post on using generators with keras: <br>\nhttps:\/\/adriannunez.github.io\/generators-in-keras\/\n\nBlog post on using keras for regression: <br>\nhttps:\/\/machinelearningmastery.com\/how-to-make-classification-and-regression-predictions-for-deep-learning-models-in-keras\/\n\nKeras fit_generator() issue: <br>\nhttps:\/\/github.com\/keras-team\/keras\/issues\/3675","e19bd851":"### See  the Results","df157652":"### Set up the Model Architecture","d0230ff4":"#### Are the output shapes correct?","677dc59d":"We will use the 'mse' loss function because this is a regression task. \n","e7f20368":"### Make a Prediction","c94bf0cc":"### APPENDIX","c2f20b75":"### MODELING ","7c888927":"### Initialize the generators","48c1b4bf":"### [2] Validation Generator","6a0cef4e":"### Check the Generators","9c3d5869":"Unlike a normal function, the ouput from a generator does not stay in memory permanently. Therefore, it can be used to handle large amounts of image data when only a limited amount of memory is available.\n","80e6b77c":"In this kernel we used a generator to read batches of X_train images from a folder and also read y_train info from a dataframe. The generator then outputs a batch in the form of a tuple (X_train, y_train).","da998a53":"### 1. What is a python  generator?","08a1c701":"1. Pre-process the data.<br>\nThis has been done in a seperate kernel. We will use the output of that kernel.\n2. Create the output dataframe, df_y\n3. Train test split<br>\n4. Build the Generators for the training data, validation data and the prediction data.\n5. Create the model architecture, train the model and make predictions.\n6. Post process the predictions to get them into submission format.\n7. Create the submission csv file.","2455fa87":"The train and val generators should loop infinitely through the batches i.e. without a \"StopIteration\" exception being raised. \n\nThe test generator should reach \"StopIteration\" after the last batch.\n\nThe size of the last batch will be smaller than the other batches.","55443684":"### Load the pre processed data","2cd11d9e":"<hr>","5623dbff":"The model input will be the images as 2D numpy arrays.\n\nThe size of each image is 1024x1024. Each image will  be read from the folder and converted into a 1024x1024 numpy array. Because there are 25,684 unique images, trying to read them all into a single numpy array will exceed the 14GB of RAM that is available on kaggle kernels (when GPU is on). \n\nTo solve this problem we will only read a batch of 10 images at a time into memory. Then we will feed this batch to the model. Once the model has eaten up a batch, that batch will disapper from memory. To achieve this memory saving batch effect we will create something called a Generator. When feeding the model we will use fit_generator() and predict_generator() instead of the usual fit() and predict() methods. \n\nNote that  keras needs an input that has the following shape: <br>\n(num_samples, image_size, image_size, num_channels). \n\nThe input batch will have the shape (10, 1024, 1024, 1). Also, in order to use fit_generator() the training and validation generators that we build must be able to loop infinitely. The appendix  has an example of how to do this - as well as quick explanation of what python generators are.\n","af65caa0":"We will treat this as a regression problem.\n\nTo keep this example simple, we will set up the model to predict a max of 2 bounding boxes.\n\nThe model will need to predict an output consisting of 10 columns.\n\n**This is the list of output columns:**\n\npredictions = <br>\n['conf_1', 'x_1', 'y_1', 'width_1', 'height_1', <br>\n'conf_2', 'x_2', 'y_2', 'width_2', 'height_2'] \n\n**Key:**\n\n* 'conf_1' = binary, bounding box confidence score\n* 'x_1' = x coordinate (top left corner of bounding box)\n* 'y_1' = y coordinate (top left corner of bounding box)\n* 'width_1' = width of bounding box\n* 'height_1' = height of bounding box\n","cb9fab76":"In this section I ran a few checks to see if the generators were performing as expected or if there were errors in the code. To save memory I've commented out this code.","b2b1e0cd":"### What will be the loss function?","33c1900e":"### [1] Train Generator","12c56aa8":"### What steps will we follow?","f5c9bfc5":"### Create the submission csv file","09eee45f":"**Summary**\n\nPython generators can be used to handle large amounts of data when only a relatively small amount of RAM is available.\n\nThis kernel uses all 25,684 full size images to train and validate a simple keras cnn. The code is able to run in a kaggle kernel (with GPU) without exceeding the 14GB RAM limit.\n\nTo achieve this I used custom python generators with a batch size of 10.\n\nUsing Generators may be helpful to those who want to take part in this interesting competition but can't because their code keeps crashing due to limited memory.\n\nThe pickled input data used in this notebook was created in Part 1. A simple explanation of how a generator works is included in the appendix section.\n<hr>","db900ab6":"### Plot the Loss Curves","6591db69":"### Process the Predictions","e173e040":"### What will be y, the target?","21335dec":"### [3] Test Generator","35af84fc":"If you find Generators a bit confusing, I've included a simple explanation in the Appendix.","7ceba28b":"### How to make a generator run infinitely?","6330c2e1":"### What will be X, the model input?","f2ddc83d":"#### Are the generators running the correct number of times?","c2a33af0":"<hr>\n\nThank you for reading.","e9c91903":"### Let's start building df_y","c4b63adf":"### 2. Resources\n\nThese are some resources that I found helpful:","c1203ea2":"### Train_Test_Split"}}