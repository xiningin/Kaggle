{"cell_type":{"6b01ff87":"code","8ac26de3":"code","c3a42384":"code","440c4b50":"code","b1201e65":"code","e7dd9ee4":"code","818ab852":"code","1b256dce":"code","153daa99":"code","e128d4e2":"code","1bb11fee":"code","6a74e2fa":"code","434adbf2":"code","3b0cc39e":"code","421c4276":"code","701eedc9":"code","ebb356ac":"code","bd88af0e":"code","488614e1":"code","27805854":"code","3f60e380":"code","69d50e8b":"code","25b3baf8":"code","7a6fc175":"code","7ccae84c":"code","cab7485e":"code","10905656":"markdown"},"source":{"6b01ff87":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8ac26de3":"import warnings\nwarnings.filterwarnings('ignore')","c3a42384":"import os\nimport gc\ngc.enable()\nimport time\nimport random\nimport warnings\n\nimport feather\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn import impute\nfrom sklearn import metrics\nfrom sklearn import ensemble\nfrom sklearn import linear_model\nfrom sklearn import decomposition\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\n\nwarnings.filterwarnings('ignore')\n\nSEED = 42\nnp.random.seed(SEED)\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\n\nsns.set_style(\"darkgrid\")\nmpl.rcParams['figure.dpi'] = 600\n%matplotlib inline\n","440c4b50":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras import callbacks\n\nimport tensorflow as tf\nimport random\nimport os\n\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer, MinMaxScaler\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nimport time\nfrom sklearn import model_selection","b1201e65":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport math","e7dd9ee4":"TRN_PATH  = '\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv'\nTST_PATH  = '\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv'\nSUB_PATH  = '\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv'","818ab852":"SEED = 42\ndef seed_everything(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\nseed_everything(SEED)","1b256dce":"# Read the dataset from the specified train_path...\ntrn = pd.read_csv(TRN_PATH)\n\n# Read the dataset from the specified train_path...\ntst = pd.read_csv(TST_PATH)\n\n# Read the dataset from the specified train_path...\nsub = pd.read_csv(SUB_PATH)","153daa99":"trn.info()","e128d4e2":"trn.head()","1bb11fee":"trn.describe()","6a74e2fa":"TARGET = 'target'\nFEATURES = [col for col in trn.columns if col not in ['id', TARGET]]","434adbf2":"# Remove the id and the targets from the train and test datasets...\ntrain = trn.drop(['id', 'target'], axis = 1)\ntest  = tst.drop(['id',], axis = 1)\n\n# Extract the target variable from the dataset...\ntarget = trn['target']","3b0cc39e":"def create_stat_features(df):\n    '''Creates multiple statistical features.'''\n    df['f_mean'] = df.mean(axis=1)\n    df['f_std']  = df.std(axis=1)\n    df['f_skew'] = df.skew(axis=1)\n    df['f_max']  = df.min(axis=1)\n    df['f_min']  = df.max(axis=1)\n    df['f_var']  = df.var(axis=1)\n    df['f_med']  = df.median(axis=1)\n    df['f_mad']  = df.mad(axis=1)\n\n    \n    return df\n\ntrain = create_stat_features(train)\ntest = create_stat_features(test)","421c4276":"scaler = MinMaxScaler(feature_range=(0, 1))\n\nfor col in FEATURES:\n    trn[col] = scaler.fit_transform(trn[col].to_numpy().reshape(-1,1))\n    tst[col] = scaler.transform(tst[col].to_numpy().reshape(-1,1))\n    \nX = trn[FEATURES].to_numpy().astype(np.float32)\nY = trn[TARGET].to_numpy().astype(np.float32)\nX_test = tst[FEATURES].to_numpy().astype(np.float32)\n\n#del train_df, test_df\n#gc.collect()","701eedc9":"def create_model_inputs():\n    inputs = {}\n    for feature_name in FEATURES:\n        inputs[feature_name] = layers.Input(\n            name=feature_name, shape=(), dtype=tf.float32\n        )\n    return inputs","ebb356ac":"def encode_inputs(inputs, encoding_size):\n    encoded_features = []\n    for i in range(inputs.shape[1]):\n        encoded_feature = tf.expand_dims(inputs[:, i], -1)\n        encoded_feature = layers.Dense(units=encoding_size)(encoded_feature)\n        encoded_features.append(encoded_feature)\n    return encoded_features   ","bd88af0e":"# Creates the units to be used in the model...\nclass GatedLinearUnit(layers.Layer):\n    def __init__(self, units):\n        super(GatedLinearUnit, self).__init__()\n        self.linear = layers.Dense(units)\n        self.sigmoid = layers.Dense(units, activation=\"sigmoid\")\n\n    def call(self, inputs):\n        return self.linear(inputs) * self.sigmoid(inputs)","488614e1":"class GatedResidualNetwork(layers.Layer):\n    def __init__(self, units, dropout_rate):\n        super(GatedResidualNetwork, self).__init__()\n        self.units = units\n        self.elu_dense = layers.Dense(units, activation=\"elu\")\n        self.linear_dense = layers.Dense(units)\n        self.dropout = layers.Dropout(dropout_rate)\n        self.gated_linear_unit = GatedLinearUnit(units)\n        self.layer_norm = layers.LayerNormalization()\n        self.project = layers.Dense(units)\n\n    def call(self, inputs):\n        x = self.elu_dense(inputs)\n        x = self.linear_dense(x)\n        x = self.dropout(x)\n        if inputs.shape[-1] != self.units:\n            inputs = self.project(inputs)\n        x = inputs + self.gated_linear_unit(x)\n        x = self.layer_norm(x)\n        return x","27805854":"class VariableSelection(layers.Layer):\n    def __init__(self, num_features, units, dropout_rate):\n        super(VariableSelection, self).__init__()\n        self.grns = list()\n        # Create a GRN for each feature independently\n        for idx in range(num_features):\n            grn = GatedResidualNetwork(units, dropout_rate)\n            self.grns.append(grn)\n        # Create a GRN for the concatenation of all the features\n        self.grn_concat = GatedResidualNetwork(units, dropout_rate)\n        self.softmax = layers.Dense(units=num_features, activation=\"softmax\")\n\n    def call(self, inputs):\n        v = layers.concatenate(inputs)\n        v = self.grn_concat(v)\n        v = tf.expand_dims(self.softmax(v), axis=-1)\n\n        x = []\n        for idx, input in enumerate(inputs):\n            x.append(self.grns[idx](input))\n        x = tf.stack(x, axis=1)\n\n        outputs = tf.squeeze(tf.matmul(v, x, transpose_a=True), axis=1)\n        return outputs","3f60e380":"def create_model(encoding_size, dropout_rate=0.15):\n    inputs = layers.Input(len(FEATURES))\n    feature_list = encode_inputs(inputs, encoding_size)\n    num_features = len(feature_list)\n\n    features = VariableSelection(num_features, encoding_size, dropout_rate)(\n        feature_list\n    )\n\n    outputs = layers.Dense(units=1, activation=\"sigmoid\")(features)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model","69d50e8b":"def format_time(seconds):\n    \"\"\"\n    Formates time in human readable form\n\n    Args:\n        seconds: seconds passed in a process\n    Return:\n        formatted string in form of MM:SS or HH:MM:SS\n    \"\"\"\n    h = int(seconds \/\/ 3600)\n    m = int((seconds % 3600) \/\/ 60)\n    s = int(seconds % 60)\n    result = ''\n    _h = ('0' + str(h)) if h < 10 else str(h)\n    result += (_h + ' hr ') if h > 0 else ''\n    _m = ('0' + str(m)) if m < 10 else str(m)\n    result += (_m + ' min ') if m > 0 else ''\n    _s = ('0' + str(s)) if s < 10 else str(s)\n    result += (_s + ' sec')\n    return result","25b3baf8":"from collections import defaultdict\nimport tensorflow as tf\n\noof_df = defaultdict(lambda : [])\ntest_df = defaultdict(lambda : np.zeros((X_test.shape[0])))\n\nN_FOLDS = 5\nENCODING_SIZE = 32\nEPOCHS = 250\nVERBOSE = 1\nBATCH_SIZE = 2048\nstart = time.time()\n\nskfolds = model_selection.StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n\nfor fold, (t, v) in enumerate(skfolds.split(X, Y)):\n    x_train, x_val = X[t], X[v]\n    y_train, y_val = Y[t], Y[v]\n    \n    oof_df[TARGET].extend(y_val)\n    print(f\"\\n{'-'*15} FOLD-{fold} {'-'*15}\")\n    \n    tic = time.time()\n    \n#     clf = tf.keras.Sequential([\n#         tf.keras.layers.Input(len(FEATURES)),\n#         tf.keras.layers.Dense(256, activation='swish'),\n#         tf.keras.layers.Dropout(0.3),\n#         tf.keras.layers.Dense(128, activation='swish'),\n#         tf.keras.layers.Dropout(0.2),\n#         tf.keras.layers.Dense(64, activation='swish'),\n#         tf.keras.layers.Dropout(0.2),\n#         tf.keras.layers.Dense(32, activation='swish'),\n#         tf.keras.layers.Dropout(0.2),\n#         tf.keras.layers.Dense(16, activation='swish'),\n#         tf.keras.layers.Dropout(0.2),\n#         tf.keras.layers.Dense(1, activation='sigmoid'),\n#     ])\n    clf = create_model(ENCODING_SIZE)\n    \n    clf.compile(loss='binary_crossentropy', \n                optimizer='adam', \n                metrics=[tf.keras.metrics.AUC(name='auc'), 'acc'])\n    \n    lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.25, \n                               patience=4, verbose=VERBOSE)\n\n    es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=15, \n                       verbose=VERBOSE, mode=\"min\", \n                       restore_best_weights=True)\n    \n    clf.fit(x_train, y_train, \n            epochs=EPOCHS, batch_size=BATCH_SIZE,\n            validation_data=(x_val, y_val),\n            validation_batch_size=len(y_val),\n            callbacks=[es, lr],\n            shuffle=True,\n            verbose=VERBOSE)\n    \n    preds = np.squeeze(clf.predict(x_val, batch_size=len(y_val)))\n    oof_df[f'nn'].extend(preds)\n    test_df[f'nn'] += (np.squeeze(clf.predict(X_test, batch_size=BATCH_SIZE) \/ N_FOLDS))\n\n    score = metrics.roc_auc_score(y_val, preds)\n    print(f\"MODEL: nn\\tSCORE: {score}\\tTIME: {format_time(time.time()-tic)}\")\n\n    del clf\n    gc.collect()\n        \n    del x_train, x_val, y_train, y_val\n    gc.collect()\n        \noof_df = pd.DataFrame(oof_df)\ntest_df = pd.DataFrame(test_df)\n\nprint()\nprint(f'TOTAL TIME: {format_time(time.time() - start)}')","7a6fc175":"score = metrics.roc_auc_score(oof_df[TARGET], oof_df['nn'])\nprint(f'Overall ROC AUC of: {score}')","7ccae84c":"# Overall ROC AUC of: 0.7488767791332664\n# Overall ROC AUC of: 0.7504944878993653\n# Overall ROC AUC of: 0.7552784685463579","cab7485e":"submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\nsubmission[TARGET] = test_df['nn']\nsubmission.to_csv('submission.csv', index=False)\n\nsubmission.head()","10905656":"# Classification with Gated Residual and Variable Selection Networks\nDescription: Using Gated Residual and Variable Selection Networks for income level prediction.<\/br>"}}