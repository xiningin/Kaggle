{"cell_type":{"929a4735":"code","f2448d60":"code","2650c665":"code","7ee45a63":"code","69289d05":"code","1b75bce1":"code","9fd3143c":"code","7d6e8a75":"code","9e4875f0":"code","4591c5da":"code","6678e59b":"code","57f1a2b9":"code","a492127d":"code","a6c25cf4":"code","ce6333cf":"code","94d7a661":"code","976f930b":"code","b408ddf5":"code","3a2a48e9":"code","f7f2bebd":"code","0f3bf1c7":"markdown","99d90701":"markdown","f0d5058d":"markdown","749261b1":"markdown","573c358f":"markdown","01a79223":"markdown","3ab914b1":"markdown","06e8004f":"markdown","e34fd429":"markdown","43e23d35":"markdown","c4ab486d":"markdown","89113aa7":"markdown","369b6e52":"markdown","8e2efc48":"markdown","60618ae6":"markdown","5c1e440f":"markdown","474ac05f":"markdown","3697a9f0":"markdown","55893e95":"markdown","0355c581":"markdown","c43a4291":"markdown"},"source":{"929a4735":"import numpy as np # linear algebra\nimport matplotlib.pyplot as plt # plot displays\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport operator\nimport matplotlib.collections as collections\nimport math\nimport joblib\nimport json\n\nfrom kkc_analysis_utils import *\n\nfrom nltk.corpus import stopwords as nltk_stopwords # english stopword from ntlk corpus\nfrom wordcloud import WordCloud, STOPWORDS # wordcloud library\nfrom PIL import Image # loading image as wordcloud mask\n\nFONT_SIZE = 20\nFONT_SIZE_TITLE = 25\nFONT_SIZE_LEGEND = 15\n__STOPWORDS = set(nltk_stopwords.words('english')) \n__STOPWORDS |= STOPWORDS\n__STOPWORDS |= set(load_english_stopwords('..\/input\/english-stopwords\/english_stopwords.txt'))","f2448d60":"with open('..\/input\/kingkiller-chronicles-dataset\/characters.json', 'r') as characters_file:\n    kkc_characters = json.load(characters_file)\n\nkkc_df = joblib.load(\"..\/input\/kingkiller-chronicles-dataset\/kkc.jbl\")  # Loading the dataframe from the serialized file","2650c665":"kkc_df.head()","7ee45a63":"kkc_df['chapter_content_wordcount'] = \\\n    kkc_df['chapter_content'].map(lambda content: len(split_word(content)))\nkkc_df['chapter_content_sentence_count'] = \\\n    kkc_df['chapter_content'].map(lambda content: len(split_sentence(content)))\n\nbook_notw = kkc_df.loc[kkc_df['book_name'] == 'name_of_the_wind']\nbook_wmf = kkc_df.loc[kkc_df['book_name'] == 'wise_mans_fear']\n\nnotw_wordcount = book_notw['chapter_content_wordcount'].sum()\nnotw_sentence_count = book_notw['chapter_content_sentence_count'].sum()\nwmf_wordcount = book_wmf['chapter_content_wordcount'].sum()\nwmf_sentence_count = book_wmf['chapter_content_sentence_count'].sum()\n\nprint(\"Name of the Wind: {} words\".format(notw_wordcount))\nprint(\"Name of the Wind: {} sentences\".format(notw_sentence_count))\nprint(\"Name of the Wind: Average wordcount per sentence: {} \".format(\n    notw_wordcount \/ notw_sentence_count\n))\nprint(\"Wise man's fear: {} words\".format(wmf_wordcount))\nprint(\"Wise man's fear: {} sentences\".format(wmf_sentence_count))\nprint(\"Wise man's fear: Average wordcount per sentence: {} \".format(\n    wmf_wordcount \/ wmf_sentence_count\n))","69289d05":"def wordcount_per_chapter(book_df, subplot, title=''):\n\n    wordcounts = book_df['chapter_content_wordcount'].values\n    sentence_counts = book_df['chapter_content_sentence_count'].values\n        \n    sp = subplot\n    sp2 = sp.twinx()\n\n    sp.set_xlabel(\"Chapter index\", fontsize=FONT_SIZE)\n    sp.set_ylabel(\"Wordcount\", fontsize=FONT_SIZE)\n    sp2.set_ylabel(\"Sentence Count\", fontsize=FONT_SIZE)\n\n    sp.set_title(title, fontsize=FONT_SIZE_TITLE)\n    sp.grid(True)\n    \n    plot_wordcount, = sp.plot(\n        np.arange(len(wordcounts)),\n        wordcounts,\n        'r',\n        label=\"wordcount per chapter\"\n    )\n    plot_sentence_count, = sp2.plot(\n        np.arange(len(sentence_counts)), \n        sentence_counts,\n        'b',\n        label=\"sentence count per chapter\"\n    )\n    \n    sp2.legend(handles = (plot_wordcount, plot_sentence_count), fontsize=FONT_SIZE)\n    \n    \nfig, axs = plt.subplots(nrows=2, ncols=1, figsize=(30,10))   \n\n# Computing plot for each book\nwordcount_per_chapter(book_notw, axs[0], \"Name of the wind sentence\/wordcount evolution\")\nwordcount_per_chapter(book_wmf, axs[1], \"Wise man's fear sentence\/wordcount evolution\")\n\nplt.tight_layout()\nplt.show()","1b75bce1":"def generate_wordcloud(book_df, color_func=None, lexicon=None):\n\n    mask = np.array(get_wordcloud_mask())\n    book_text = book_df['chapter_content'].sum()\n    \n    stopwords = __STOPWORDS\n    stopwords.add('said')\n\n    if lexicon:\n        for word in split_word(book_text):\n            if word.lower() not in lexicon:\n                stopwords.add(word)\n\n    wordcloud = WordCloud(width=512,\n                            height=512,\n                            scale=5,\n                            margin=-2,\n                            background_color='white',\n                            mask=mask,\n                            stopwords=stopwords,\n                            prefer_horizontal=1.0,\n                            max_font_size=150,\n                            max_words=300,\n                            color_func=color_func,\n                            min_font_size=15\n                        ).generate(book_text)\n\n    return wordcloud\n\nplt.figure(figsize=(25, 10))\nplt.imshow(generate_wordcloud(kkc_df))\nplt.axis('off')\nplt.title('Kingkiller Chronicle Wordcloud', fontsize=FONT_SIZE_TITLE)\nplt.show()","9fd3143c":"# We define this function here as it will be useful again during this analysis\ndef draw_histo_plot(hist, subplot, colors=None, title='', x_title='', box_display=False):\n   \n    sorted_hist = sorted(hist.items(), key=operator.itemgetter(1))\n    axis_words, axis_values = map(list, zip(*[(w, v) for w, v in sorted_hist[-8:]]))\n\n    subplot.barh(np.arange(len(axis_words)), axis_values, color=colors)\n\n    subplot.set_yticks(np.arange(len(axis_words)))\n    subplot.set_yticklabels(axis_words, fontsize=FONT_SIZE)\n    subplot.set_xlabel(x_title, fontsize=FONT_SIZE)\n    subplot.set_title(title, fontsize=FONT_SIZE)\n    subplot.set_axisbelow(True)\n    subplot.grid(True, which='both', linewidth='0.75', linestyle='--', color='lightgrey')\n\n    if box_display:\n        box = dict(boxstyle='square', facecolor='white', pad=0.2)\n        for index, value in enumerate(axis_values):\n            subplot.text(\n                value - subplot.get_xlim()[1] * 0.06,\n                index - 0.1,\n                str(value),\n                size= FONT_SIZE,\n                bbox=box\n        )\n\nhist = {}\nfor word in split_word(kkc_df['chapter_content'].sum()):\n    word = word.strip().lower().replace('\u2019s', '')\n    \n    if word not in hist:\n        hist[word] = 1\n    else:\n        hist[word] += 1\n        \nfig, ax = plt.subplots(1, 1, figsize=(15, 8))\ndraw_histo_plot(hist,\n                subplot=ax,\n                colors=['#596575', '#68778c'],\n                title='Word occurences (including stopwords)',\n                x_title='Occurences')","7d6e8a75":"total_word_count = notw_wordcount + wmf_wordcount\nsorted_hist = sorted(hist.items(), key=operator.itemgetter(1), reverse= True)\n\nx = np.arange(0, len(sorted_hist))\ny = [word[1] \/ total_word_count for word in sorted_hist]\n\nfig, ax = plt.subplots(1, 1, figsize=(15, 8))\nax.plot(x, y)\nax.plot([0, len(sorted_hist)], [y[0], y[-1]], color='g')\nax.grid()\nax.set_title('Word occrence \/ total wordcount, decreasing order', fontsize=FONT_SIZE)\nax.set_ylabel('Words occrences \/ total wordcount', fontsize=FONT_SIZE)\n\nplt.show()","9e4875f0":"print('Total KKC series wordcount: {}'.format(total_word_count))\nprint('80% of total wordcount: {}'.format(int(total_word_count * 0.8)))\nmost_frequent_words_occurences = np.array([int(word[1]) for word in sorted_hist[:int(len(sorted_hist) * 0.2)]])\nprint('Sum of the 20% most frequent word occurences: {}'.format(most_frequent_words_occurences.sum()))\nprint('Most frequent word occurences \/ total kkc series wordcount: {}'.format(most_frequent_words_occurences.sum() \/ total_word_count))","4591c5da":"color_character_name_notw = \\\n    lambda *args, **kwargs: (30, 194, 219) if args[0] in kkc_characters else (209, 94, 15)\ncolor_character_name_wmf = \\\n    lambda *args, **kwargs: (209, 94, 50) if args[0] in kkc_characters else (30, 194, 219)\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(25,15))\n\naxs[0].imshow(generate_wordcloud(book_notw, color_func=color_character_name_notw))\naxs[0].axis('off')\naxs[0].set_title('Name of the Wind wordcloud', fontsize=FONT_SIZE_TITLE)\n\naxs[1].imshow(generate_wordcloud(book_wmf, color_func=color_character_name_wmf))\naxs[1].axis('off')\naxs[1].set_title('Wise Man\\'s Fear wordcloud', fontsize=FONT_SIZE_TITLE)\n\nplt.show()","6678e59b":"hist = {name : 0 for name in kkc_characters}\n\nfor word in split_word(kkc_df['chapter_content'].sum()):\n    if word in kkc_characters:\n        hist[word] += 1     \n\nfig, ax = plt.subplots(1, 1, figsize=(15, 8))\ndraw_histo_plot(hist,\n                subplot=ax,\n                colors=['#379e92', '#41bfb1'],\n                title='Name occurences per character',\n                box_display=True,\n                x_title='Occurences')\n\nplt.show()","57f1a2b9":"fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n\n# Here we get the 13 more common names from the histogram\nn_most_frequent_names = dict(sorted(hist.items(), key=lambda x: x[1])[-13:])\n# Then calculate what percentage represents the rest of the names\nothers_list = [occ for name, occ in hist.items() if name not in n_most_frequent_names.keys()]\nothers_count = sum(others_list)\n# Finally we degine the axes needed for the following plot\nlabels, sizes = map(list, zip(*[(name, count) for name, count in n_most_frequent_names.items()]))\n\n# And we don't forget to append the other_count to the axes!\nlabels.append('Others')\nsizes.append(others_count)\n\nax.pie(sizes, labels=labels, autopct='%1.1f%%', textprops={'size': FONT_SIZE}, radius=1.5)\n\nplt.show()","a492127d":"def generate_n_grams(book_df, n = 1):\n\n    n_grams_hist = {}\n    sentences = split_sentence(book_df['chapter_content'].sum())\n\n    for sentence in sentences:\n        word_list = split_word(sentence)\n        n_grams_list = zip(*[word_list[i:] for i in range(n)])\n\n        for n_gram in n_grams_list:\n            key = ' '.join(n_gram)\n\n            if key not in n_grams_hist:\n                n_grams_hist[key] = 1\n            else:\n                n_grams_hist[key] += 1\n\n    return n_grams_hist            \n\n\nfig, axs = plt.subplots(nrows=4, ncols=2, figsize=(20,20))\nn_gram_count = 3\n\nfor i in range(4):\n        notw_gram = generate_n_grams(book_notw, n_gram_count)\n        wmf_gram = generate_n_grams(book_wmf, n_gram_count)\n    \n        len_n_gram = len(list(notw_gram.keys())[0].split(' '))\n    \n        draw_histo_plot(notw_gram,\n                        subplot=axs[i][0],\n                        colors=['#fdc183', '#f19c41'],\n                        title='Most common {}-gram'.format(str(len_n_gram)),\n                        x_title='Occurences')\n        \n        draw_histo_plot(wmf_gram,\n                        subplot=axs[i][1],\n                        colors=['#8abfa3', '#90d9c5'],\n                        title='Most common {}-gram'.format(str(len_n_gram)),\n                        x_title='Occurences')\n        \n        n_gram_count += 1\n        \nplt.tight_layout()\nplt.show()","a6c25cf4":"senticnet4_sentiment_lexicon = \\\n    load_senticnet4_lexicon('..\/input\/sentiment-datasets\/senticnet4.txt')\nnrc_sentiment_lexicon = \\\n    load_emotion_lexicon('..\/input\/sentiment-datasets\/NRC-Hashtag-Emotion-Lexicon-v0.2.txt')\npos_neg_lexicon = load_pos_neg_lexicons('..\/input\/sentiment-datasets')\n\nsentiment_colors = {\n    'joy': '#2caee7',\n    'anger': '#ffe333',\n    'surprise': '#1eede7',\n    'disgust': '#ed1e1e',\n    'anticipation': '#1eda2d',\n    'sadness': '#8c29f5',\n    'fear': '#8c9db0',\n    'trust': '#2349e5'\n}","ce6333cf":"def wordcloud_color_pos_neg(*args, **kwargs):\n    if args[0].lower() in pos_neg_lexicon:\n        if float(pos_neg_lexicon[args[0].lower()]) >= 0:\n            return (27,100,240)\n        else:\n            return (228,0,27)\n\n    return 'black'\n\nwordcloud_img = generate_wordcloud(kkc_df,\n                                   lexicon=pos_neg_lexicon,\n                                   color_func=wordcloud_color_pos_neg)\n\nplt.figure(figsize=(25, 10))\nplt.imshow(wordcloud_img)\nplt.axis('off')\nplt.title('Kingkiller Chronicle Positive\/Negative Wordcloud', fontsize=FONT_SIZE_TITLE)\n\nplt.show()","94d7a661":"hist = {}\nfor word in split_word(kkc_df['chapter_content'].sum()):\n    word = word.strip().lower().replace('\u2019s', '')\n\n    if word not in pos_neg_lexicon or word not in senticnet4_sentiment_lexicon:\n        continue\n\n    score =  senticnet4_sentiment_lexicon.get(word, 0)\n    score = float(score)\n    \n    if word not in hist:\n        hist[word] = score\n    else:\n        hist[word] += score\n\nfig, ax = plt.subplots(figsize=(25, 12))\n\nsorted_hist = sorted(hist.items(), key=operator.itemgetter(1))\n# Select the 8 first and 8 last\nfinal_hist = sorted_hist[:8] + sorted_hist[-8:]\npos_neg_colors = ['#bf4141'] * 8 + ['#41bfb1'] * 8\naxis_words, axis_values = map(list, zip(*[(w, v) for w, v in final_hist]))\n\nax.barh(np.arange(len(axis_words)), axis_values, color=pos_neg_colors)\n\nax.set_yticks(np.arange(len(axis_words)))\nax.set_yticklabels(axis_words, fontsize=FONT_SIZE)\nax.set_xlabel('Occurences * positive\/negative score', fontsize=FONT_SIZE)\nax.set_title('Top words positive\/negative score', fontsize=FONT_SIZE)\nax.set_axisbelow(True)\nax.grid(True, which='both', linewidth='0.75', linestyle='--', color='lightgrey')\nax.axvline(0, color='black')\n\nplt.show()","976f930b":"def score_evolution(chapter):\n    chapter['pmi_positive_score'] = 0\n    chapter['pmi_negative_score'] = 0\n    pmi_avg_sum = 0\n    nb_sentiment_word = 0\n\n    for w in split_word(chapter['chapter_content']):\n\n        word = w.strip().lower().replace('\u2019s', '')\n        \n        if (word not in senticnet4_sentiment_lexicon and word not in pos_neg_lexicon):\n            continue\n\n        score = senticnet4_sentiment_lexicon.get(word, 0)\n        score = float(score)\n        \n        if score >= 0:\n            chapter['pmi_positive_score'] += score\n        else:\n            chapter['pmi_negative_score'] += score\n\n        pmi_avg_sum += score\n        nb_sentiment_word += 1\n\n    chapter['pmi_average_score'] = pmi_avg_sum \/ nb_sentiment_word\n        \n    return chapter\n\n\ndef draw_score_evolution(book_df, book_title):\n\n    fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(25, 12))\n    \n    pmi_lst_pos = book_df['pmi_positive_score'].values\n    pmi_lst_neg = book_df['pmi_negative_score'].values\n    pmi_lst_avg = book_df['pmi_average_score'].values\n    \n    axs[0].fill_between(\n        np.arange(len(pmi_lst_pos)),\n        pmi_lst_pos,\n        color='green',\n        alpha=0.2\n    )\n    positive_word_score_sum, = axs[0].plot(\n        np.arange(len(pmi_lst_pos)),\n        pmi_lst_pos,\n        color='green',\n        label='scores sum of positive words'\n    )\n    \n    axs[0].fill_between(\n        np.arange(len(pmi_lst_neg)),\n        [abs(value) for value in pmi_lst_neg],\n        color='red',\n        alpha=0.2\n    )\n    negative_word_score_sum, = axs[0].plot(\n        np.arange(len(pmi_lst_neg)),\n        [abs(value) for value in pmi_lst_neg], \n        color='red',\n        label='scores sum of negative words'\n    )\n   \n    axs[0].grid(True)\n    axs[0].set_title(\n        'Sum of words scores per chapter ({})'.format(book_title),\n        fontsize=FONT_SIZE_TITLE\n    )\n    axs[0].set_xlabel('Chapter', fontsize=FONT_SIZE)\n    axs[0].set_ylabel('Word Score Sum', fontsize=FONT_SIZE)\n    axs[0].set_xticks(np.arange(len(pmi_lst_avg) - 1), minor = True)\n    axs[0].legend(handles=(positive_word_score_sum, negative_word_score_sum), fontsize=FONT_SIZE)\n    axs[0].set_xlim(0, len(pmi_lst_avg) - 1)\n\n    axs[1].grid(True)\n    avg_plt = axs[1].fill_between(\n        np.arange(len(pmi_lst_avg)),\n        pmi_lst_avg,\n        label='Average score per chapter'\n    )\n    axs[1].set_title(\n        'Average word score per chapter ({})'.format(book_title),\n        fontsize=FONT_SIZE_TITLE\n    )\n    axs[1].set_xlabel('Chapter', fontsize=FONT_SIZE)\n    axs[1].set_ylabel('Average word score', fontsize=FONT_SIZE)\n    axs[1].legend(handles=[avg_plt], fontsize=FONT_SIZE)\n    axs[1].set_xlim(0, len(pmi_lst_avg) - 1)\n\n    plt.tight_layout()\n    plt.show()\n\n\nbook_notw = book_notw.apply(score_evolution, axis=1)\nbook_wmf = book_wmf.apply(score_evolution, axis=1)\n\ndraw_score_evolution(book_notw, book_title='Name of the wind')\ndraw_score_evolution(book_wmf, book_title=\"Wise man's fear\")","b408ddf5":"frequent_word_emotion = {}\nfor w in split_word(kkc_df['chapter_content'].sum()):\n    word = w.lower()\n\n    if word not in nrc_sentiment_lexicon or word in __STOPWORDS:\n        continue\n\n    word_emotion_dict = nrc_sentiment_lexicon[word]\n\n    for emotion in word_emotion_dict:\n        #Init empty dict if emotion not in frequent_word_emotion\n        if emotion not in frequent_word_emotion:\n            frequent_word_emotion[emotion] = {}\n\n        word_emotion_score = word_emotion_dict[emotion]\n\n        if word not in frequent_word_emotion[emotion]:\n            frequent_word_emotion[emotion][word] = word_emotion_score\n        else:\n            frequent_word_emotion[emotion][word] += word_emotion_score\n\nnb_cols = 2\nnb_rows = 4\nhisto_count = 0\n\nfig, axs = plt.subplots(nrows=nb_rows, ncols=nb_cols, figsize=(15, 15))\nfig.text(\n    0.5,\n    -0.05,\n    'Occurences * Word association score with emotion',\n    ha='center',\n    fontsize=FONT_SIZE\n)\n\nfor emotion in frequent_word_emotion:\n\n    draw_histo_plot(frequent_word_emotion[emotion],\n                    subplot=axs[histo_count \/\/ nb_cols][histo_count % nb_cols],\n                    colors=sentiment_colors[emotion],\n                    title=emotion)\n\n    histo_count += 1\n\nplt.tight_layout()\nplt.show()","3a2a48e9":"def sentiments_evolution(chapter):\n    for sentiment in sentiment_colors.keys():\n        chapter['{}_wordcount'.format(sentiment)] = 0\n            \n    for word in split_word(chapter['chapter_content']):\n        # Get all sentiment associated with this word\n        if word in nrc_sentiment_lexicon:\n            \n            for sentiment in sentiment_colors.keys():\n                if sentiment in nrc_sentiment_lexicon[word]:\n                    chapter['{}_wordcount'.format(sentiment)] += 1\n                    \n    return chapter\n\n\ndef draw_sentiments_evolution(subplot, book_df, book_title, legend=False):\n    plt_handles = []\n    \n    for sentiment in sentiment_colors.keys():\n        handle, = subplot.plot(np.arange(len(book_df['{}_wordcount'.format(sentiment)])),\n                               book_df['{}_wordcount'.format(sentiment)],\n                               color=sentiment_colors[sentiment],\n                               label=sentiment)\n        plt_handles.append(handle)\n    \n    subplot.grid(True)\n    subplot.set_title(\n        'Sum of words scores per chapter ({})'.format(book_title),\n        fontsize=FONT_SIZE_TITLE\n    )\n    subplot.set_xlabel('Chapter', fontsize=FONT_SIZE)\n    subplot.set_ylabel('Word Score Sum', fontsize=FONT_SIZE)\n    subplot.set_xticks(np.arange(len(book_df)), minor=True)\n    \n    box = subplot.get_position()\n\n    if legend:\n        subplot.legend(\n            loc='upper center',\n            bbox_to_anchor=(0.5, -0.15),\n            fancybox=True,\n            shadow=True,\n            ncol=len(plt_handles),\n            handles = tuple(plt_handles),\n            fontsize=FONT_SIZE\n        )\n    \nbook_notw = book_notw.apply(sentiments_evolution, axis=1)\nbook_wmf =  book_wmf.apply(sentiments_evolution, axis=1)\n\nfig, axs = plt.subplots(nrows=2, ncols=1, figsize=(25,13))\n\ndraw_sentiments_evolution(axs[0], book_notw, 'Name of the wind')\ndraw_sentiments_evolution(axs[1], book_wmf, \"Wise man's fear\", legend=True)\n\nplt.tight_layout()\nplt.show()","f7f2bebd":"histo_sentiment = {}\n\nfor w in split_word(kkc_df['chapter_content'].sum()):\n    word = w.lower()\n\n    if word in nrc_sentiment_lexicon:\n        for sentiment in nrc_sentiment_lexicon[word]:\n\n            if sentiment not in histo_sentiment:\n                histo_sentiment[sentiment] = 1\n            else:\n                histo_sentiment[sentiment] += 1\n\n                \nfig, ax = plt.subplots(figsize=(15, 8))\n\naxis_words, axis_values = map(list, zip(*[(w, v) for w, v in histo_sentiment.items()]))\ncolors = [sentiment_colors[sentiment] for sentiment in axis_words]\n\nrects = plt.bar(np.arange(len(axis_values)), axis_values, width=0.5, color=colors)\n\nplt.xticks(np.arange(len(axis_words)), axis_words, fontsize=FONT_SIZE)\nplt.yticks(np.arange(0, 180000, 20000))\nplt.ylim([0, 170000])\nplt.xlabel('Sentiment', fontsize=FONT_SIZE)\nplt.ylabel('Word occurences', fontsize=FONT_SIZE)\nplt.title('Sentiment wordcount', fontsize=FONT_SIZE)\nplt.grid(True, which='both', linewidth='0.75', linestyle='--', color='lightgrey')\n\nfor rect in rects:\n    height = rect.get_height()\n    ax.text(\n        rect.get_x() + rect.get_width() \/ 2.,\n        1.02 * height,\n        height,\n        ha='center',\n        va='bottom',\n        fontsize=13\n    )\n\nplt.tight_layout()\nplt.show()","0f3bf1c7":"Using pandas DataFrame object will allow us to easily manipulate the text, and optimize computation. Moreover, vectorized programming improves the code readability.\n\n# 5. Data Analysis\n## 5.1 Text mining\n\nTo begin with, here is some general information about the books.\n* The Name of the wind is a 722-page book.\n* The Wise man's fear is a 1120-page book.\n\nAccording to our splits functions:","99d90701":"We can now clearly see that in **The Name of the Wind** the names *Denna*, *Kvothe* and *Chronicler* stand out. In **The Wise man's fear** *Tempi*, *Felurian* and *Vashet* appear at the top as well. In both books, *Denna* remains one of the most frequently cited names. To get a more detailed comparison, let's now compute a histogram of the occurences of names in the whole book series.","f0d5058d":"It's a bit surprising that the word \"*ambrose*\" appears among the *anger* words, it might be present in the NRC lexicon as it's based on tweets.\nSeeing these words gives us a overview of the impact of sentiments. Now let's compare these sentiment scores to each another, for each chapter.","749261b1":"So now we might be interested in knowing what has the most impact on a chapter, or even a book. Let's compute a score of positivity\/negativity for each chapter, and plot its evolution over the book series. The score computation is achieved using two lexicons:\n* The senticnet4 lexicon (*https:\/\/www.aclweb.org\/anthology\/C16-1251\/*)\n* A custom lexicon of positive and negative words.","573c358f":"Thanks to this first graph, we can see that the four most frequent words in the whole book series are **the**, **I**, **a** and **to**. Knowing that KKC series contains **654 470 words** (*see previous results*), and according to Zipf's law, here is the word occurence distribution:","01a79223":"## 4.2 Utils functions\n\nIn this section we are going to define basic functions used in this kernel.\n\n* `split_word(text)` Split string into an array of its words.\n* `split_sentence(text)` Split string into an array of sentences.\n* `load_pos_neg_lexicons(filename)` Use to load the positive-negative-words lexicon.\n* `load_senticnet4_lexicon(filename)` Use to load the senticnet4 lexicon.\n* `load_emotion_lexicon(filename)` Use to load the NRC sentiment lexicon.\n\n## 4.3 Data imports","3ab914b1":"So much surprises in these books! *Surprise* appears to be the predominant sentiment, closely followed by *anger* and *disgust*.\n\nIt has been a real pleasure to make this post! Thank you for reading and if you liked it, please upvote it.","06e8004f":"Here we can see that **disgust**, **surprise** and **anger** are the three most present lexical fields according to this analysis.\nFinally, let's count the total number of words for each sentiment:","e34fd429":"## 5.2 Zipf's law and Pareto principle","43e23d35":"To better understand these numbers, let's draw the sentence\/wordcount by chapter evolution.","c4ab486d":"As we can see, the word distribution is less leveled on the second book. The difference is much more evident on the small chapters.\nSpeaking about repartition, what are the most common words ? To find out, let's draw a wordcloud.","89113aa7":"It's easy to see that most of the chapters are mainly positive, but we get to see some chapters where the positive and negative score sums are close to each other. This usually happens on shorter ones.\n\n##\u00a05.7 Sentiment Analysis\n\nNow that we've seen the relation between positivity and negativity, let's do an in-depth analysis of sentiments. Firstly, we will display the most common words per sentiment. To be more precise, we will multiply the word occurences by their sentiment affiliation score. *(Reminder: a word may be bound to multiple sentiment, but its bonding strength score will not be the same for each sentiment which it is linked to)*\n\nTo achieve this analysis we will use the **NRC Emotion Lexicon** (http:\/\/sentiment.nrc.ca\/lexicons-for-research\/).","369b6e52":"It's nice to see what are the most frequent positive and negative words, now let's get more details about it. What are the most impacting words regarding the global positivity\/negativity score:","8e2efc48":"## 5.3 Characters\n\nOn the precedent wordcloud, we can easily identify names of known characters. First, let's split this into two wordclouds corresponding to the two books in the series, then we will highlight the names of the characters and see which ones come out the most.","60618ae6":"The Pareto's principle states that, *for many events, roughly 80% of the effects come from 20% of the causes*. In our studies it means that 80% of the text should only be made of the 20% most frequent words. In our studies, according to the previous computed number, it's acceptably right! Indeed the 20% most frequent words represent more than **93**% of the total words. (*more about pareto principle [here](https:\/\/en.wikipedia.org\/wiki\/Pareto_principle)*)","5c1e440f":"# Kingkiller Chronicle books analysis\n \n![alt text](https:\/\/i.imgur.com\/ZHKAhgf.png \"notw banner\")\n# 1. Summary\n \n1. Summary\n2. Introduction\n3. Datasets\n4. Project Settings\n    1. Library imports\n    2. Utils functions\n    3. Data imports\n5. Data Analysis\n    1. Text Mining\n    2. Characters\n    3. N-grams\n    4. Positive\/Negative Analysis\n    5. Sentiment Analysis\n\n# 2. Introduction\n\nHi! In this kernel we are going to analyze the book series **Kingkiller Chronicle** from **Patrick Rothfuss**. This analysis will first cover a text mining analysis, using wordclouds, n-grams and sentence\/wordcounts. Then we will conduct a more in-depth analysis of sentiments and emotions repartition.\n\n# 3. Datasets\n\nThis kernel includes multiple datasets. Firstly, here is a list of datasets that will help us to do the analysis:\n* `english_stopwords` contains an non-exhaustive list of english stopwords.\n* `sentiment_analysis` is a custom dataset containing known lexicons:\n    * `NRC-Emotion-Lexicon` https:\/\/saifmohammad.com\/WebPages\/NRC-Emotion-Lexicon.htm\n    * `Senticnet4` https:\/\/www.aclweb.org\/anthology\/C16-1251\/\n    * Custom positive\/negative words lexicons.\n\nSecondly, the `kingkiller_chronicle_dataset` includes the data on which is based this analysis. There is two files in this dataset. The first one is a **JSON** list of all the characters present in the books.\nThe second one is a serialized pandas dataframe, used to store each book's chapter (see *Data imports* section for more details).\n\nThis last dataset remains private in order to comply with the non-distribution clause.\n>\"*The scanning, uploading and distribution of these books via the Internet or any other means without the permission of the publisher is illegal*\"\n\n# 4. Project Settings\n## 4.1 Library imports","474ac05f":"Here we see \"*The Name of the Wind*\" in the 5-grams from both books. In the n-grams from *The Wise Man's Fear*, we can notice multiple occurrences of the 2-gram \"*deep breath*\", directly echoing to the name of the wind.\n\n## 5.6 Positive\/Negative Analysis","3697a9f0":"Let\u2019s address the topic of opinion mining or sentiment analysis. First of all, let's draw another wordcloud of the most frequent positive and negative words.","55893e95":"As expected, *Denna* and *Kvothe* come first. What is suprising is that *Tempi* is at 6th position while he's only mentioned in the second book.\n\n## 5.4 N-grams\n\nTo finish the text mining part of this analysis, let's take a look at the **N-grams**. As a reminder, a n-gram is a contiguous sequence of n items from a given sample of text.\nWe will therefore discover here which sub-sentences are used by the author. We are going to compute up to 6-gram as it might be more relevant than the lowest ones, which are essentially linking expression.\nFollowing this kernel color code, on the left are the n-gram from **The Name of the Wind**, and on the right the ones from **The Wise Man's Fear**.","0355c581":"We can clearly see that the world distribution **isn't following the Zipf's law**, as it should be closer to the green line. (*more about zipf's law [here](https:\/\/en.wikipedia.org\/wiki\/Zipf%27s_law)*)","c43a4291":"As defined in the Dataset section, we will use a pandas dataframe object to manage our book data. Within the dataframe, each line represents a chapter. For each one them, the dataframe is initialized with 3 columns: chapter_title, chapter_content and book_name. In this way, we will be able to carry out a chapter-by-chapter analysis, and to compare evolutions over the whole book series."}}