{"cell_type":{"e9f15bee":"code","d726dabe":"code","2a3a0a5e":"code","002d7379":"code","5b237e7b":"code","3fa33acd":"code","2761bb86":"code","d9064ee3":"code","bfec71c6":"code","268f0b92":"code","e0d329ed":"code","a72e1021":"code","160ab327":"code","219364aa":"code","76168139":"code","42419737":"code","4a208166":"code","84dcc879":"code","e562b03e":"code","ed8204ba":"code","92836d72":"code","b72c44c1":"code","e3fb94e0":"code","4538ea68":"code","dab64256":"code","196518c2":"code","ffeb5455":"code","5693b877":"code","041b7132":"code","a6a5df50":"code","786c4170":"code","01cce9d9":"code","146fdb9c":"code","acdc1056":"code","dbe41bc2":"code","219920f6":"code","83c0c9f0":"code","fb50f6c8":"code","7f0873dc":"code","d29d74ae":"code","4036325d":"code","c646b96e":"code","f0518a2e":"code","79572cce":"code","3820b322":"code","89979cee":"code","4635360e":"code","cc090acf":"code","fb10a840":"code","8a9bb5a4":"code","0046953c":"code","ba929c62":"code","95f9f92f":"code","84109c43":"code","c7c7a5d6":"code","f24a47ba":"code","d99f2bf8":"code","3826a7bd":"code","94891b15":"code","7b8f573a":"code","9325af5c":"code","0b9997fd":"code","a321c1f1":"code","f4c8eac6":"code","3cac5b89":"code","2b4706fb":"code","f19fd5ca":"code","5c7e3748":"code","0dc78ada":"code","dae9d359":"code","fbe3c602":"markdown","d396e071":"markdown","be9cd590":"markdown","4f3f0cc5":"markdown","c776a817":"markdown","be3dce53":"markdown","155929bb":"markdown","93111f18":"markdown","5c32155a":"markdown","1ecf56af":"markdown","ccc2756f":"markdown","7ce6069b":"markdown","82850207":"markdown","8beb4396":"markdown","8a1fe430":"markdown","a10bd7f8":"markdown","54c48538":"markdown","a1c91edb":"markdown","ab488ebe":"markdown","f9e53441":"markdown","2223c2cc":"markdown","40963d4a":"markdown","16a567b6":"markdown","3888bd3c":"markdown","f8f84577":"markdown","d3b0e662":"markdown","6012bbd1":"markdown","7159109e":"markdown","94a78416":"markdown","3990d66a":"markdown","d8664977":"markdown","c486c57e":"markdown","dced7adc":"markdown","025cd6f7":"markdown","783af22d":"markdown","3956a40b":"markdown","9b410e73":"markdown","f73aba3e":"markdown","ffdbf628":"markdown","bc2d1c7b":"markdown","658d7bae":"markdown"},"source":{"e9f15bee":"#Really need these\nimport pandas as pd \nimport numpy as np\nfrom numpy import *\n\n\n#Handy for debugging\nimport gc\nimport time\nimport warnings\nimport os\n\n#Date stuff\nfrom datetime import datetime\nfrom datetime import timedelta\n\n#Do some statistics\nfrom scipy.misc import imread\nfrom scipy import sparse\nimport scipy.stats as ss\nimport math\n\n#Nice graphing tools\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.offline as py\nimport plotly.tools as tls\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n#Machine learning tools\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.utils.validation import check_X_y, check_is_fitted\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom scipy import sparse\n\n## Keras for deep learning\nimport keras\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers import Bidirectional\nfrom keras.models import Sequential\nfrom keras import regularizers\nfrom keras import optimizers\n\n## Performance measures\nfrom sklearn.metrics import mean_squared_error","d726dabe":"#Merge info\ndef mergeData(df):\n    features =pd.read_csv('..\/input\/walmart\/features.csv')\n    storesdata =pd.read_csv('..\/input\/walmart\/stores.csv')\n    df = pd.merge(df, features, on=['Store','Date','IsHoliday'],\n                  how='inner')\n    df = pd.merge(df, storesdata, on=['Store'],\n                  how='inner')\n    return df\n\n#http:\/\/scikit-learn.org\/stable\/auto_examples\/plot_cv_predict.html\ndef plot_prediction(predicted,true,desciption):\n    fig, ax = plt.subplots()\n    ax.scatter(true, predicted, edgecolors=(0, 0, 0))\n    ax.plot([true.min(), true.max()], [true.min(), true.max()], 'k--', lw=4)\n    ax.set_xlabel('Measured')\n    ax.set_ylabel('Predicted by '+desciption)\n    ax.plot([-30,30], [0,0], 'k-')   \n    ax.plot([0,0], [-30,30], 'k-')\n    plt.show()\ndef binary(movement):\n    \"\"\"\n    Converts percent change to a binary 1 or 0, where 1 is an increase and 0 is a decrease\/no change\n    \n    \"\"\"\n    #Empty arrays where a 1 represents an increase in price and a 0 represents a decrease in price\n    direction = np.empty(movement.shape[0])\n    #If the change in price is greater than zero, store it as a 1\n    #If the change in price is less than zero, store it as a 0\n    for i in range(movement.shape[0]):\n        if movement[i] > 0:\n            direction[i] = 1\n        else:\n            direction[i]= 0\n    return direction\n\ndef scatterplots(feature, label):\n    x = feature\n    y = df['Weekly_Sales']\n    plt.scatter(x, y)\n    plt.ylabel('sales')\n    plt.xlabel(label)\n    plt.show()","2a3a0a5e":"print('Reading data...')\nprint(os.listdir('..\/input\/walmart\/'))\nprint(os.listdir('..\/input\/walmarts'))","002d7379":"dataSource = 1\nif dataSource==1:\n    train = mergeData(pd.read_csv('..\/input\/walmart\/train.csv'))\n    test = mergeData(pd.read_csv('..\/input\/walmart\/test.csv'))\n    train['Split'] = 'Train'\n    test['Split'] = 'Test'\n    test.head()\nelse: \n    train = pd.read_csv('..\/input\/walmart\/train.csv')\n    test = pd.read_csv('..\/input\/walmart\/test.csv')\n    train['Split'] = 'Train'\n    test['Split'] = 'Test'\n    test.head()","5b237e7b":"\ntest.head(1)","3fa33acd":"t_len = len(train) # Get number of training examples\ndf = pd.concat([train,test],axis=0) # Join train and test\ndf.tail() # Get an overview of the data","2761bb86":"df.describe()","d9064ee3":"df.columns","bfec71c6":"df['Temperature'] = (df['Temperature'] - 32) * 5\/9","268f0b92":"# Code from https:\/\/seaborn.pydata.org\/examples\/many_pairwise_correlations.html\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","e0d329ed":"%matplotlib inline\n\nheaders = list(df)\nlabels = headers\nscatterplots(df['Fuel_Price'], 'Fuel_Price')\nscatterplots(df['Size'], 'Size')\n#scatterplots(df['Temperature'], 'Temperature')\n#scatterplots(df['Unemployment'], 'Unemployment')\nscatterplots(df['IsHoliday'], 'IsHoliday')\nscatterplots(df['Type'], 'Type')\n","a72e1021":"df.loc[df['Weekly_Sales'] >300000]","160ab327":"df.loc[df['Weekly_Sales'] >240000,\"Date\"].value_counts()","219364aa":"df.isnull().sum()","76168139":"df = df.assign(md1_present = df.MarkDown1.notnull())\ndf = df.assign(md2_present = df.MarkDown2.notnull())\ndf = df.assign(md3_present = df.MarkDown3.notnull())\ndf = df.assign(md4_present = df.MarkDown4.notnull())\ndf = df.assign(md5_present = df.MarkDown5.notnull())","42419737":"md1_present = df.MarkDown1.notnull()\nprint(md1_present)","4a208166":"df.fillna(0, inplace=True)","84dcc879":"df.isnull().sum()","e562b03e":"# Make sure we can later recognize what a dummy once belonged to\ndf['Type'] = 'Type_' + df['Type'].map(str)\ndf['Store'] = 'Store_' + df['Store'].map(str)\ndf['Dept'] = 'Dept_' + df['Dept'].map(str)\ndf['IsHoliday'] = 'IsHoliday_' + df['IsHoliday'].map(str)","ed8204ba":"df['Type']","92836d72":"# Create dummies\ntype_dummies = pd.get_dummies(df['Type'])\nstore_dummies = pd.get_dummies(df['Store'])\ndept_dummies = pd.get_dummies(df['Dept'])\nholiday_dummies = pd.get_dummies(df['IsHoliday'])","b72c44c1":"df['DateType'] = [datetime.strptime(date, '%Y-%m-%d').date() for date in df['Date'].astype(str).values.tolist()]\ndf['Month'] = [date.month for date in df['DateType']]\ndf['Month'] = 'Month_' + df['Month'].map(str)\nMonth_dummies = pd.get_dummies(df['Month'] )","e3fb94e0":"df['Black_Friday'] = np.where((df['DateType']==datetime(2010, 11, 26).date()) | (df['DateType']==datetime(2011, 11, 25).date()), 'yes', 'no')\ndf['Pre_christmas'] = np.where((df['DateType']==datetime(2010, 12, 23).date()) | (df['DateType']==datetime(2010, 12, 24).date()) | (df['DateType']==datetime(2011, 12, 23).date()) | (df['DateType']==datetime(2011, 12, 24).date()), 'yes', 'no')\ndf['Black_Friday'] = 'Black_Friday_' + df['Black_Friday'].map(str)\ndf['Pre_christmas'] = 'Pre_christmas_' + df['Pre_christmas'].map(str)\nBlack_Friday_dummies = pd.get_dummies(df['Black_Friday'] )\nPre_christmas_dummies = pd.get_dummies(df['Pre_christmas'] )","4538ea68":"# Add dummies\n# We will actually skip some of these\n#df = pd.concat([df,type_dummies,store_dummies,dept_dummies,holiday_dummies,Pre_christmas_dummies,Black_Friday_dummies,Month_dummies],axis=1)\n\ndf = pd.concat([df,holiday_dummies,Pre_christmas_dummies,Black_Friday_dummies],axis=1)","dab64256":"df.columns.values\n","196518c2":"# Get dataframe with averages per store and department\nmedians = pd.DataFrame({'Median Sales' :df.loc[df['Split']=='Train'].groupby(by=['Type','Dept','Store','Month','IsHoliday'])['Weekly_Sales'].median()}).reset_index()\n","ffeb5455":"# Merge by type, store, department and month\ndf = df.merge(medians, how = 'outer', on = ['Type','Dept','Store','Month','IsHoliday'])\n","5693b877":"# Fill NA\ndf['Median Sales'].fillna(df['Median Sales'].loc[df['Split']=='Train'].median(), inplace=True) \n\n# Create a key for easy access\n\ndf['Key'] = df['Type'].map(str)+df['Dept'].map(str)+df['Store'].map(str)+df['Date'].map(str)+df['IsHoliday'].map(str)\n","041b7132":"df.columns\n","a6a5df50":"# Attach variable of last weeks time\ndf['DateLagged'] = df['DateType']- timedelta(days=7)\ndf.head()","786c4170":"# Make a sorted dataframe. This will allow us to find lagged variables much faster!\nsorted_df = df.sort_values(['Store', 'Dept','DateType'], ascending=[1, 1,1])\nsorted_df = sorted_df.reset_index(drop=True) # Reinitialize the row indices for the loop to work","01cce9d9":"sorted_df.head(1)\n","146fdb9c":"df.head(1)","acdc1056":"sorted_df=pd.read_csv('..\/input\/walmarts\/output.csv')","dbe41bc2":"sorted_df.head(1)","219920f6":"df.rename(columns={\"DateType\": \"Date\"})\nsorted_df[['Dept', 'Store','Date','LaggedSales','LaggedAvailable']].head(1)","83c0c9f0":"intersect=sorted_df[['Dept', 'Store','Date','LaggedSales','LaggedAvailable']]","fb50f6c8":"df = df.merge(intersect, how = 'inner', on = ['Dept', 'Store','Date'])","7f0873dc":"df['Sales_dif'] = df['Median Sales'] - df['LaggedSales']\ndf[['Dept', 'Store','DateType','LaggedSales','Weekly_Sales','Median Sales']].head()","d29d74ae":"switch= 1\n\nif(switch):\n    df_backup = df\nelse:\n    df=df_backup\n    display(df_backup.head())","4036325d":"df['Unemployment'] = (df['Unemployment'] - df['Unemployment'].mean())\/(df['Unemployment'].std())\ndf['Temperature'] = (df['Temperature'] - df['Temperature'].mean())\/(df['Temperature'].std())\ndf['Fuel_Price'] = (df['Fuel_Price'] - df['Fuel_Price'].mean())\/(df['Fuel_Price'].std())\ndf['CPI'] = (df['CPI'] - df['CPI'].mean())\/(df['CPI'].std())\ndf['MarkDown1'] = (df['MarkDown1'] - df['MarkDown1'].mean())\/(df['MarkDown1'].std())\ndf['MarkDown2'] = (df['MarkDown2'] - df['MarkDown2'].mean())\/(df['MarkDown2'].std())\ndf['MarkDown3'] = (df['MarkDown3'] - df['MarkDown3'].mean())\/(df['MarkDown3'].std())\ndf['MarkDown4'] = (df['MarkDown4'] - df['MarkDown4'].mean())\/(df['MarkDown4'].std())\ndf['MarkDown5'] = (df['MarkDown5'] - df['MarkDown5'].mean())\/(df['MarkDown5'].std())\ndf['LaggedSales']= (df['LaggedSales'] - df['LaggedSales'].mean())\/(df['LaggedSales'].std())","c646b96e":"df['Difference'] = df['Median Sales'] - df['Weekly_Sales']","f0518a2e":"df.head()","79572cce":"# Code from https:\/\/seaborn.pydata.org\/examples\/many_pairwise_correlations.html\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","3820b322":"selector = [\n    #'Month',\n    'CPI',\n    'Fuel_Price',\n    'MarkDown1',\n    'MarkDown2',\n    'MarkDown3',\n    'MarkDown4',\n    'MarkDown5',\n    'Size',\n    'Temperature',\n    'Unemployment',\n    \n    \n    \n    'md1_present',\n    'md2_present',\n    'md3_present',\n    'md4_present',\n    'md5_present',\n\n    'IsHoliday_False',\n    'IsHoliday_True',\n    'Pre_christmas_no',\n    'Pre_christmas_yes',\n    'Black_Friday_no',\n    'Black_Friday_yes',    \n    'LaggedSales',\n    'Sales_dif',\n    'LaggedAvailable'\n    ]\ndisplay(df[selector].describe())\ndisplay(df[selector].head())","89979cee":"train = df.loc[df['Split']=='Train']\ntest = df.loc[df['Split']=='Test']\ntest.head()\nprint(len(test))","4635360e":"# Set seed for reproducability \nnp.random.seed(42)\nX_train, X_dev, y_train, y_dev = train_test_split(train[selector], train['Difference'], test_size=0.2, random_state=42)\nprint(X_dev.shape)\nprint(y_dev.shape)","cc090acf":"neural = False\nif neural:\n    # Sequential model\n    \n    adam_regularized = Sequential()\n\n    # First hidden layer now regularized\n    adam_regularized.add(Dense(32,activation='relu',\n                    input_dim=X_train.shape[1],\n                    kernel_regularizer = regularizers.l2(0.01)))\n\n    # Second hidden layer now regularized\n    adam_regularized.add(Dense(16,activation='relu',\n                       kernel_regularizer = regularizers.l2(0.01)))\n\n    # Output layer stayed sigmoid\n    adam_regularized.add(Dense(1,activation='linear'))\n\n    # Setup adam optimizer\n    adam_optimizer=keras.optimizers.Adam(lr=0.01,\n                    beta_1=0.9, \n                    beta_2=0.999, \n                    epsilon=1e-08)\n\n    # Compile the model\n    adam_regularized.compile(optimizer=adam_optimizer,\n                  loss='mean_absolute_error',\n                  metrics=['acc'])\n\n    # Train\n    history_adam_regularized=adam_regularized.fit(X_train, y_train,validation_data=(X_dev, y_dev), # Train on training set\n                                 epochs=10, # We will train over 1,000 epochs\n                                 batch_size=2048, # Batch size \n                                 ) # Suppress Keras output\n    adam_regularized.evaluate(x=X_dev,y=y_dev)\n\n    # Plot network\n    plt.plot(history_adam_regularized.history['loss'], label='Adam Regularized')\n    plt.xlabel('Epochs')\n    plt.ylabel('loss')\n    plt.legend()\n    plt.show()\n    y_pred_neural = adam_regularized.predict(X_dev)","fb10a840":"y_train.shape","8a9bb5a4":"#Random forest model specification\nregr = RandomForestRegressor(n_estimators=20, criterion='mse', max_depth=None, \n                      min_samples_split=2, min_samples_leaf=1, \n                      min_weight_fraction_leaf=0.0, max_features='auto', \n                      max_leaf_nodes=None, min_impurity_decrease=0.0, \n                      min_impurity_split=None, bootstrap=True, \n                      oob_score=False, n_jobs=1, random_state=None, \n                      verbose=2, warm_start=False)\n\n#Train on data\nregr.fit(X_train, y_train.ravel())","0046953c":"y_pred_random = regr.predict(X_dev)\n\ny_dev = y_dev.to_frame()","ba929c62":"# Transform forest predictions to observe direction of change\ndirection_true1= binary(y_dev.values)\ndirection_predict = binary(y_pred_random)\n\n## show confusion matrix random forest\ncnf_matrix = confusion_matrix(direction_true1, direction_predict)\n\nfig, ax = plt.subplots(1)\nax = sns.heatmap(cnf_matrix, ax=ax, cmap=plt.cm.Greens, annot=True)\n#ax.set_xticklabels(abbreviation)\n#ax.set_yticklabels(abbreviation)\nplt.title('Confusion matrix of random forest predictions')\nplt.ylabel('True category')\nplt.xlabel('Predicted category')\nplt.show();","95f9f92f":"y_pred_random","84109c43":"y_dev['Predicted'] = y_pred_random\ndf_out = pd.merge(train,y_dev[['Predicted']],how = 'left',left_index = True, right_index = True,suffixes=['_True','_Pred'])\ndf_out = df_out[~pd.isnull(df_out['Predicted'])]\ndf_out.head()","c7c7a5d6":"df_out['prediction'] = df_out['Median Sales']-df_out['Predicted']\nplot_prediction(df_out['Weekly_Sales'],df_out['prediction'],\"Random Forest\")\nplot_prediction(y_pred_random,y_dev['Difference'].values,\"Random Forest\")","f24a47ba":"df_out.head()","d99f2bf8":"df.to_csv(\"sampleoutpput.csv\")","3826a7bd":"print(\"Medians: \"+str(sum(abs(df_out['Difference']))\/df_out.shape[0]))\nprint(\"Random Forest: \"+str(sum(abs(df_out['Weekly_Sales']-df_out['prediction']))\/df_out.shape[0]))","94891b15":"#Random forest model specification. Set n_estimators lower for faster performance\nrf_model = RandomForestRegressor(n_estimators=80, criterion='mse', max_depth=None, \n                      min_samples_split=2, min_samples_leaf=1, \n                      min_weight_fraction_leaf=0.0, max_features='auto', \n                      max_leaf_nodes=None, min_impurity_decrease=0.0, \n                      min_impurity_split=None, bootstrap=True, \n                      oob_score=False, n_jobs=1, random_state=None, \n                      verbose=0, warm_start=False)\n\n#Train on data\nrf_model.fit(train[selector], train['Difference'])","7b8f573a":"#Use if large model skipped\n#rf_model = regr","9325af5c":"test[selector]","0b9997fd":"final_y_prediction = adam_regularized.predict(test[selector])","a321c1f1":"print(test.head())","f4c8eac6":"len(test[selector].index.values)","3cac5b89":"testfile = pd.concat([test.reset_index(drop=True), pd.DataFrame(final_y_prediction)], axis=1)\ntestfile['prediction'] = testfile['Median Sales']-testfile[0]\ntestfile.head()","2b4706fb":"final_y_prediction[:,0]","f19fd5ca":"submission = pd.DataFrame({'id':pd.Series([''.join(list(filter(str.isdigit, x))) for x in testfile['Store']]).map(str) + '_' +\n                           pd.Series([''.join(list(filter(str.isdigit, x))) for x in testfile['Dept']]).map(str)  + '_' +\n                           testfile['Date'].map(str),\n                          'Weekly_Sales':final_y_prediction[:,0]})\nsubmission.head()","5c7e3748":"submission.to_csv('submission.csv',index=False)","0dc78ada":"len(submission.index.values)","dae9d359":"115064 rows","fbe3c602":"### Random Forest\n\nTrain on random forest","d396e071":"## Prepare functions\nI initialize my functions in the beginning of the script to make the whole seem cleaner..","be9cd590":"Let's get a clearer image of what our data actually looks like with the describe function. This will give use summary statistics of our numerical variables.","4f3f0cc5":"## Forecasting sales\n\nAfter we have created our model, we can predict things with it on the test set","c776a817":"### Select variables to include in model\n\nIn this section, we can change the variables we ultimately want to include in our model training. ","be3dce53":"### Remove redundant items\n\nWe will take the store average in the available data as one of its properties","155929bb":"## Model selection\n\nAs usual, let's start off with all our imports.","93111f18":"From this plot, we notice that type C stores have fewer sales in general and holidays clearly show more sales.Although no further relationships appear evident from this analysis, there appears to be some outliers in our data. Let's take a bit of a closer look at these.","5c32155a":"Next, let's look at 'special dates'. One variable for Christmas, one for black friday. We have to manually look up the dates of black friday if we want to extrapolate our data to other years, but for now we know: 26 - 11 - 2010 and 25 - 11 - 2011.","1ecf56af":"### Split data into training and test sets\n\nNow we can split train test again and of course remove the trivial weekly sales data from the test set.","ccc2756f":"There are two competitions that have more or less the same data. Choose which competition to participate in.\n[One](https:\/\/www.kaggle.com\/c\/walmart-recruiting-store-sales-forecasting) or [two](https:\/\/www.kaggle.com\/c\/walmart-sales-forecasting). All comments are based on number two.","7ce6069b":"Identify data sources","82850207":"## Understanding the problem and defining a success metric\n\nThe problem is quite straightforward. Data from Walmart stores accross the US is given, and it is up to us to forecast their weekly sales. The data is already split into a training and a test set, and we want to fit a model to the training data that is able to forecast those weeks sales as accurately as possible. In fact, our metric of interest will be the [Mean Absolute Error](https:\/\/en.wikipedia.org\/wiki\/Mean_absolute_error). \n\nThe metric is not very complicated. The further away from the actual outcome our forecast is, the harder it will be punished. Optimally, we exactly predict the weekly sales. This of course is highly unlikely, but we must try to get as close as possible. The base case of our model will be a simple linear regression baseline, which gave a MSE of \n\n","8beb4396":"## Summary of results and approach\n\nWork in Progress:\n\nAt writing, our internal competition at Bletchley has ended. Interestingly, the winning group had a different approach then would be expected from an AI\/Machine Learning bootcamp. Their forecasts were based simply on a median of the weekly sales grouped by the Type of Store, Store & Department number, Month and Holiday dummy. \n\nTherefore, in my next approach, the goal will be to improve their results with the help of Neural Networks and other machine learning methods. In fact, the median will be computed similarly to how the winning group did, and a new variable, the difference to the median, will be computed. This difference will be the new dependent variable and will be estimated based on new holiday dummies, markdowns and info on lagged sales data if available.\n\n**Final result: MAE dropped from 2200 to 1800.\n**","8a1fe430":"Check submission one more time","a10bd7f8":"### Lagged Variables\n\nWe will take a lagged variable of our store's previous weeks sales. To do so, we will first add a column with a one week lagged date, sort the data, and then match the lagged sales with the initial dataframe using the department and store number.\n\nWe begin by adding a column with a one week lag.","54c48538":"In order to efficiently modify our data, we merge the two datasets for now. We also keep track of the length of our training set so we know how to split it later.","a1c91edb":"Now we create the submission. Once you run the kernel you can download the submission from its outputs and upload it to the Kaggle InClass competition page.","ab488ebe":"Most of what we see in the correlation table is of little surprise. Discounts are correlated and higher unemployment means lower Consumer Price Index. More interestingly, it appears that higher department numbers have higher sales. Maybe because they are newer? Also, larger stores generate more sales, discounts generally generate higher sales values and larger unemployment result in a bit fewer sales. Unfortunately, there appears to be little relationship between holidays, temperatures or fuelprices with our weekly sales.\n\nNext up, let's plot some of these relationships to get a clearer image.","f9e53441":"### Model evaluation\n \n To evaluate the model, we will look at MAE and accuracy in terms of the number of times it correctly estimated an upward or downward deviation from the median.\n","2223c2cc":"\n\n## Scrub the data and engineer features\n\n### Missing values\n\nWe will start with filling in any blank values. There seem to be some missing values in the data. We have to make sure to deal with them before feeding anything into the network.","40963d4a":"## Load and explore data\nBefore we do anything, lets import some packages.","16a567b6":"Let's have a look at our data set before running our actual models.","3888bd3c":"Now, let's change the variable to be forecasted to the difference from the median. Afterward, we can drop the weekly sales.","f8f84577":"### Scale Variables\n\nTo make the job of our models easier in the next phase, we normalize our continous data. This is also called feature scaling.","d3b0e662":"Next, we create a sorted dataframe.","6012bbd1":"Although there is not a large variety of variables, we can definitely work with this. In the next section, we will clean the data set, engineer some new features and add dummy variables. For now, let's try to find any obvious relations between our variables to get a feeling for the data. We begin with a correlation matrix.\n","7159109e":"We will do a bit of very basic feature engineering here by creating a feature which indicates whether a certain markdown was active at all.","94a78416":"### Adam optimizer with regularization\n\nIn our next model, we will stick with the relu activator, but replace the momentum with an Adam optimizer. Adaptive momumtum estimator uses exponentially weighted averages of the gradients to optimize its momentum.  However, since this method is known to overfit the model because of its fast decent, we will make use of a regulizer to avoid overfitting. The l2 regulizer adds the sum of absolute values of the weights to the loss function, thus discouraging large weights that overemphasize single observations.","3990d66a":"Looks good! Let's train on our full data set to get the maximum amount of information in our model.","d8664977":"It appears to be quite obvious. The end of November sees a lot of exceptionally large sales. This special day, better known as Black friday, causes sales to be on fire, and undoubtedly a dummy variable should be created for this day. Also, Christmas, appears here and there. Since it is not considered holiday, we will also make a dummy for this day. Let's see if we should consider some other special days as well.","c486c57e":"### Dummy variables: Dates\n\nFrom our earlier analysis, it has turned out that the date may be our best friend. As a general rule, it is a good start to already distinguish between different months in our model. This will create 12 dummy variables; one for each month.","dced7adc":"Except for a handful spurious other dates, it appears that the two days before Christmas and Black Friday will do the job.","025cd6f7":"# The Walmart challenge: Modelling weekly sales\nIn this notebook, we use data from Walmart to forecast their weekly sales. ","783af22d":"> ### Store median\n\nWe will take the store median in the available data as one of its properties","3956a40b":"Since we are in the Netherlands, and we don't understand Fahrenheit, let's do a quick change there.","9b410e73":"### Dummy variables: Categorical Data\n\nNow we have to create some dummy variebles for categorical data.","f73aba3e":"We can probably safely fill all missing values with zero. For the markdowns this means that there was no markdown. For the weekly sales, the missing values are the ones we have to predict, so it does not really matter what we fill in there.","ffdbf628":"Loop over its rows and check at each step if the previous week's sales are available. If not, fill with store and department average, which we retrieved before.","bc2d1c7b":"### Test - dev\n\nUsually, model performance can be evaluated on the out-of-sample test set. However, since that data is not available, it may be wise to split our training set one more time in order to be able to test out of sample performance. Let's give up 20% of our training set for this sanity check development set.","658d7bae":"Now, merge this new info with our existing dataset."}}