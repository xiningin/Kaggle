{"cell_type":{"5572345c":"code","749fe3f0":"code","e7589012":"code","5470adae":"code","d4000564":"code","677e5b80":"code","6461987d":"code","421c60e2":"code","d89471db":"code","241327e9":"code","f2fe6a17":"code","f6d7a421":"code","e78c064c":"code","45777470":"code","5e903465":"code","546b3a54":"code","f0d17f79":"code","53fe11f5":"code","761bd9c9":"code","bb37fef7":"code","4ece76c6":"code","0d1282df":"code","3861016f":"code","1851e4c5":"code","7f740633":"markdown","847b28dc":"markdown","6352dc4e":"markdown","d9691db9":"markdown","62283e3c":"markdown","4433de42":"markdown","17b233cd":"markdown","60228590":"markdown","8046ddc4":"markdown","a6d91a9c":"markdown","1f0a14c4":"markdown","617cd0e5":"markdown","f1752231":"markdown","9ede5940":"markdown","59a7ac64":"markdown","f33acb49":"markdown","35b4ff8c":"markdown","4012a3dc":"markdown","c9af2505":"markdown","fdf8cb2c":"markdown","d4c69cec":"markdown"},"source":{"5572345c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nfrom sklearn.model_selection import train_test_split\n\n# Read the data\nX = pd.read_csv('..\/input\/udemy-courses\/udemy_courses.csv', index_col='course_id')","749fe3f0":"import time\nimport datetime\n\nX = pd.read_csv('..\/input\/udemy-courses\/udemy_courses.csv', index_col='course_id')\n\n# helper functions\ndef string_to_datetime(date_string):\n    return datetime.datetime.strptime(date_string,\"%Y-%m-%dT%H:%M:%SZ\")\n\ndef elapsed_time(time_from, time_to):\n    return string_to_datetime(time_from) - string_to_datetime(time_to)\n\n# to calculate time on the platform I use the dataset's date of upload\n# for simplicity, I rounded the time to years\nfn = lambda row: elapsed_time(\"2020-05-10T00:00:00Z\", row.published_timestamp).days\/365\ncol = X.apply(fn, axis=1)\nX = X.assign(time_on_platform=col.values)\n\nX[\"time_on_platform\"].describe()","e7589012":"import matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\n\npp = sns.pairplot(data=X,\n                  y_vars=['num_subscribers'],\n                  x_vars=['time_on_platform', 'price', 'num_lectures', 'content_duration', 'subject'])\n","5470adae":"X = X.sort_values(by=['num_subscribers'], ascending=False)\nX","d4000564":"# get top 10% of the set\n# df = X.head(int(len(X)*(10\/100)))\n\n# X = X.sort_values(by=['time_on_platform'], ascending=False)\n# df = X.head(int(len(X)*(15\/100)))\n\ndf0 = X[X['time_on_platform'].between(X['time_on_platform'].min(), 4)]\ndf1 = X[X['time_on_platform'].between(4, 5)]\ndf2 = X[X['time_on_platform'].between(5, 6)]\ndf3 = X[X['time_on_platform'].between(6, 7)]\ndf4 = X[X['time_on_platform'].between(7,8)]\ndf = X[X['time_on_platform'] >= 8]\n\nplt.plot(df0[\"time_on_platform\"], df0[\"num_subscribers\"],linestyle=\"\",marker=\"o\")\nplt.plot(df1[\"time_on_platform\"], df1[\"num_subscribers\"],linestyle=\"\",marker=\"o\")\nplt.plot(df2[\"time_on_platform\"], df2[\"num_subscribers\"],linestyle=\"\",marker=\"o\")\nplt.plot(df3[\"time_on_platform\"], df3[\"num_subscribers\"],linestyle=\"\",marker=\"o\")\nplt.plot(df4[\"time_on_platform\"], df4[\"num_subscribers\"],linestyle=\"\",marker=\"o\")\nplt.plot(df[\"time_on_platform\"], df[\"num_subscribers\"],linestyle=\"\",marker=\"o\")\nplt.xlabel(\"time_on_platform\")\nplt.ylabel(\"num_subscribers\")\n","677e5b80":"df0 = X[X['time_on_platform'].between(X['time_on_platform'].min(), 4)]\ndf1 = X[X['time_on_platform'].between(4, 6)]\ndf2 = X[X['time_on_platform'].between(6, 7)]\ndf = X[X['time_on_platform'] >= 7]\n\n# draw all subsets side by side\nfig, axs = plt.subplots(2, 3)\nfig.set_size_inches(18.5, 10.5)\n\ndataframes = [df0, df1, df2, df]\ndf_to_draw_index = 0;\nfor r in range(2):\n    for c in range(2):\n        d = dataframes[df_to_draw_index]\n        axs[r, c].plot(d[\"time_on_platform\"], d[\"num_subscribers\"],linestyle=\"\",marker=\"o\")\n        axs[r, c].set_title('Df ' + str(df_to_draw_index))\n        df_to_draw_index += 1\n        \nfor ax in axs.flat:\n    ax.set(xlabel='years_on_platform', ylabel='num_subscribers')","6461987d":"import seaborn as sns\n\nX[\"published_timestamp\"]=pd.to_datetime(X[\"published_timestamp\"])\nX[\"Year\"]=X[\"published_timestamp\"].dt.year\n\nyear = X.groupby([\"Year\"])[\"course_title\"].count()\nyear = pd.DataFrame(year)\nyear.rename(columns = {\"course_title\":\"Total_Courses\"},inplace = True)\nyear.reset_index(inplace=True)\n\nX[\"Month\"]=X[\"published_timestamp\"].dt.month\nmonth = X.groupby([\"Month\"])[\"course_title\"].count()\nmonth = pd.DataFrame(month)\nmonth.rename(columns = {\"course_title\":\"Total_Courses\"},inplace = True)\nmonth.reset_index(inplace=True)\n\nsns.barplot(x=\"Year\",y=\"Total_Courses\",data = year)\nplt.show()\nsns.barplot(x=\"Month\",y=\"Total_Courses\",data = month)\nplt.show()\nX.columns\n\ndf0 = X[X['time_on_platform'].between(X['time_on_platform'].min(), 4)]\ndf1 = X[X['time_on_platform'].between(4, 6)]\ndf2 = X[X['time_on_platform'].between(6, 7)]\ndf = X[X['time_on_platform'] >= 7]\n","421c60e2":"# fn = lambda row: string_to_datetime(row.published_timestamp).month\n# col = X.apply(fn, axis=1)\n# X = X.assign(published_month=col.values)\n\n# X[\"published_month\"].describe()","d89471db":"categorical_columns = ['course_title', 'level', 'subject']\nnumerical_columns = ['price', 'num_lectures', 'content_duration', 'time_on_platform', 'Month']\nfeatures = categorical_columns + numerical_columns\n\n# Select target value\ny = X.num_subscribers\nX_full = X[features]\n# Break off test set from training data\nX_train, X_test, y_train, y_test = train_test_split(X_full, y, train_size=0.8, test_size=0.2, random_state=0)\n\ny.describe()","241327e9":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\n\n# no missing values in any of the columns, so no need for imputing\n\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\nnumerical_transformer = StandardScaler()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"categorical\", categorical_transformer, categorical_columns),\n        (\"numerical\", numerical_transformer, numerical_columns),\n    ]\n)","f2fe6a17":"decission_tree = Pipeline(\n    steps=[(\"preprocessor\", preprocessor), (\"regressor\", DecisionTreeRegressor())]\n)\n\ndecission_tree.fit(X_train, y_train)\n\npredictions_tree = decission_tree.predict(X_test)\nmae = mean_absolute_error(y_test, predictions_tree)\n\nprint(\"decission tree mae: \" + str(mae))","f6d7a421":"from sklearn.linear_model import LinearRegression\n\nlinear = Pipeline(\n    steps=[(\"preprocessor\", preprocessor), (\"linear\", LinearRegression())]\n)\n\nlinear.fit(X_train, y_train)\n\nlinear_pred = linear.predict(X_test)\nmae = mean_absolute_error(y_test, linear_pred)\n\nprint(\"Linear Regression mae: \" + str(mae))","e78c064c":"from sklearn import svm\n\nsvr = Pipeline(\n    steps=[(\"preprocessor\", preprocessor), (\"svr\", svm.SVR())]\n)\n\nsvr.fit(X_train, y_train)\n\nsvr_pred = svr.predict(X_test)\nmae = mean_absolute_error(y_test, svr_pred)\n\nprint(\"SVR mae: \" + str(mae))","45777470":"from sklearn.base import clone\nfrom sklearn.base import BaseEstimator\n\n\nclass OrdinalClassifier():\n    \n    def __init__(self, clf):\n        self.clf = clf\n        self.clfs = {}\n    \n    def fit(self, X, y):\n        self.unique_class = np.sort(np.unique(y))\n        if self.unique_class.shape[0] > 2:\n            for i in range(self.unique_class.shape[0]-1):\n                # for each k - 1 ordinal value we fit a binary classification problem\n                binary_y = (y > self.unique_class[i]).astype(np.uint8)\n                clf = clone(self.clf)\n                clf.fit(X, binary_y)\n                self.clfs[i] = clf\n    \n    def predict_proba(self, X):\n        clfs_predict = {k:self.clfs[k].predict_proba(X) for k in self.clfs}\n        predicted = []\n        for i,y in enumerate(self.unique_class):\n            if i == 0:\n                # V1 = 1 - Pr(y > V1)\n                predicted.append(1 - clfs_predict[y][:,1])\n                \n            elif y in clfs_predict:\n                # Vi = Pr(y > Vi-1) - Pr(y > Vi)\n                 predicted.append(clfs_predict[y-1][:,1] - clfs_predict[y][:,1])\n            else:\n                # Vk = Pr(y > Vk-1)\n                predicted.append(clfs_predict[y-1][:,1])\n        return np.vstack(predicted).T\n    \n    def predict(self, X):\n        return np.argmax(self.predict_proba(X), axis=1)","5e903465":"categorical_columns = ['level', 'subject'] #'course_title'\nnumerical_columns = ['price', 'num_lectures', 'content_duration', 'time_on_platform', 'Month']\nfeatures = numerical_columns + categorical_columns\n\ndef classify(value):\n    if 0 <= value <= 100: return 0\n    elif 100 <= value <= 1000: return 1\n    elif 1000 <= value <= 2500: return 2\n#     elif 2000 <= value <= 5000: return 3\n#     elif 5000 <= value <= 10000: return 4\n    elif value > 2500: return 3\n\nfn = lambda row: classify(row.num_subscribers)\ncol = X.apply(fn, axis=1)\nX = X.assign(sub_class=col.values)\n# Select target value\ny = X.sub_class\nX_full = X[features]\n# Break off test set from training data\nX_train, X_test, y_train, y_test = train_test_split(X_full, y, train_size=0.8, test_size=0.2, random_state=0)\n","546b3a54":"data = X.sub_class.value_counts().reset_index()\nsns.barplot(x=\"index\", y=\"sub_class\", data = data)","f0d17f79":"# from sklearn.svm import SVC\n# from sklearn.tree import DecisionTreeClassifier\n\n# clf = Pipeline(\n#     steps=[(\"preprocessor\", preprocessor), (\"clf\", OrdinalClassifier(DecisionTreeClassifier(max_depth=3)))]\n# )\n\n# clf.fit(X_train, y_train)\n\n# oc_pred = clf.predict(X_test)\n\n# print(oc_pred)\n\n# # print(df0.sub_class)\n# # count = 0\n# # for i in range(290):\n# # #     print(\"true: \" + str(y_test[i]) + \", pred: \" + str(oc_pred[i]))\n# #     if y_test[i] == oc_pred[i]: count+=1\n        \n# # print(count\/290)","53fe11f5":"# baseline model\n\nfrom sklearn.dummy import DummyClassifier\ndummy_clf = DummyClassifier(strategy=\"most_frequent\")\n\ndummy_clf.fit(X, y)\ndummy_clf.predict(X)\ndummy_clf.score(X, y)","761bd9c9":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\ncategorical_transformer = OrdinalEncoder()\nnumerical_transformer = StandardScaler()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"categorical\", categorical_transformer, categorical_columns),\n        (\"numerical\", numerical_transformer, numerical_columns),\n        \n    ],remainder='passthrough'\n)\n\ndtree_model = Pipeline(\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", DecisionTreeClassifier(max_depth = 300))]\n)\n\n\ndtree_model.fit(X_train, y_train)\nfig, ax = plt.subplots(figsize=(10, 10))\nplot_confusion_matrix(dtree_model, X_test, y_test, ax=ax)  \nplt.show()","bb37fef7":"X_train.head()\n# , X_test, y_train, y_test","4ece76c6":"import optuna\nfrom sklearn.metrics import accuracy_score\n\ndef objective(trial):\n\n    max_depth = trial.suggest_int(\"max_depth\", 2, 612)\n    min_samples_split = trial.suggest_int(\"min_samples_split\", 2, 612)\n    max_leaf_nodes = int(trial.suggest_int(\"max_leaf_nodes\", 2, 612))\n\n    criterion = trial.suggest_categorical(\"criterion\", [\"gini\", \"entropy\"])\n\n    DTC = DecisionTreeClassifier(min_samples_split = min_samples_split, \n                                max_leaf_nodes = max_leaf_nodes,\n                                criterion = criterion)\n\n    \n    preprocessor = ColumnTransformer(\n    transformers=[\n        (\"categorical\", categorical_transformer, categorical_columns),\n        (\"numerical\", numerical_transformer, numerical_columns),\n        \n    ],remainder='passthrough'\n    )\n\n    DTC = Pipeline(\n        steps=[(\"preprocessor\", preprocessor), (\"classifier\", DecisionTreeClassifier(max_depth = 300))]\n    )\n    \n    DTC.fit(X_train, y_train)\n\n    return 1.0 - accuracy_score(y_test, DTC.predict(X_test))\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials = 500)","0d1282df":"print(study.best_params)\nprint(1.0 - study.best_value)","3861016f":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OrdinalEncoder\n\n# Read the data\nX = pd.read_csv('..\/input\/udemy-courses\/udemy_courses.csv', index_col='course_id')\n\ncategorical_columns = ['level']\nnumerical_columns = ['price', 'num_lectures', 'content_duration', 'num_subscribers']\nfeatures = categorical_columns+ numerical_columns\n\n# Select target value\ny = X.subject\n\nX = X[features]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=0)\n\ncategorical_transformer = OrdinalEncoder()\nnumerical_transformer = StandardScaler()\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"categorical\", categorical_transformer, categorical_columns),\n        (\"numerical\", numerical_transformer, numerical_columns),\n        \n    ],remainder='passthrough'\n)\n\ndtree_model = Pipeline(\n    steps=[(\"preprocessor\", preprocessor), (\"classifier\", DecisionTreeClassifier(max_depth = 300))]\n)\n\n\ndtree_model.fit(X_train, y_train)\nfig, ax = plt.subplots(figsize=(10, 10))\nplot_confusion_matrix(dtree_model, X_test, y_test, ax=ax)  \nplt.show()\n","1851e4c5":"from sklearn import tree\n\n# text_representation = tree.export_text(dtree_model[\"classifier\"])\n# print(text_representation)\nfig = plt.figure(figsize=(50,50))\n_ = tree.plot_tree(dtree_model[\"classifier\"], \n                   feature_names=numerical_columns + dtree_model[\"preprocessor\"].transformers_[0][2],  \n                   class_names=[\"Web Development\", \"Business Finance\", \"Graphic Design\", \"Musical Instruments\"],\n                   filled=True,\n                   max_depth=3\n                  )","7f740633":"The most popular courses tend to be either free or very expensive. Plots with the number of lectures and content duration show that most popular courses are shorter, and have fewer number of content hours. When it comes to subjects - the recipe for being popular is to make a course on Web Development.","847b28dc":"The cell below creates new column **time_on_platform** which is defined as the days elapsed since the upload of the course to info collection (last updated date in the dataset's description). I do so because all the other information were true about the course at that point in time. It is important when I relate courses' age to any other feature.","6352dc4e":"# # Predicting course popularity in time\n\nAt first, I made a predictive model that outputted the number of subscribers for a given course, but ... I started to think about what aim will a person have in mind when using the model. \n\nHaving a model say: \"Your course will have 2500 subscribers!\" won't be very useful except boosting your ego. Often, numbers without context are of no use. If you wanted to get an answer to the question \"How many subscribers will my course have?\" you'll probably also want a context, for example, timeframe, like 2500 subscribers in the first year from publishing your course. The information allows you to allocate less\/more resources for updating the content, spend less\/more on marketing, etc. In other words, gives you tangible benefits.\n\nFirstly, I will test if the number of subscribers depends on the time the course is on the platform.","d9691db9":"To check if the course popularity depends on course age, I will check\n\n* The most popular courses (and what makes them popular)\n\n* The newest courses\n\n* The oldest courses\n\nAnd then investigate the number of subscribers for courses by the time on the platform, in those groups.","62283e3c":"For the classification algoithms, it is important to have balanced classes. Check class balance in year categories that I've created.","4433de42":"First, I started off with a baseline model to compare my models to. I chose Dummy Classifier with the best performing strategy - in this case `most_frequent`.","17b233cd":"The most numerous dataframe is the one containging the newest courses.","60228590":"# # Ordinal classification\n\nBased on: https:\/\/towardsdatascience.com\/simple-trick-to-train-an-ordinal-regression-with-any-classifier-6911183d2a3c","8046ddc4":"At fisrt glance, I can tell that top most popular courses are Beginner Level\/All levels courses on Web Development, and are on the platform form five to seven years, while the least popular are usually paid and on Graphic Design and Musical Instruments.","a6d91a9c":"It looks like the courses based on time on the platform could be separated into four classes:\n\n- min to 4\n\n- 4 to 6\n\n- 6 to 7\n\n- 7 to 8\n\n- 8 +\n\nWhich are outliers and should they be excluded from the analysis? Is it possible to tell which range contains the most successful courses? To test that, I've plotted the four classes separately.\n\nNow, let's check the performance of models in the subsets defined by 'course age'.","1f0a14c4":"The courses in the dataset are on the platform form ~ 2 to 8 years, 75% are 5 years or shorter. I've plotted a few pairplots to look for simple relations between chosen variables from the set.","617cd0e5":"# # Time of publishing the course\n\nI've experimented the impact of the 'age' of a course on its popularity. Now, I'm interested whether the month of publishing the course also matters. For that I added the column denoting the month of publishing.\n\ntodo: month and number of subscribers, month trend in subsets","f1752231":"Below, I plotted the number of subscribers for courses by their age (in years).","9ede5940":"**Which columns to exclude and why?**\n\n* num_reviews - it is directly connected to the number of subscribers and this information is not availible at the time of publishing the course (data leakage)\n* url - cannot be chosen by the course author, shouldn't be used in prediction\n* is_paid - carries a less complete yet simmilar information in comparison to price column\n* published_timestamp - is replaced by time_on_platform\n* ? title - all unique","59a7ac64":"# Predicting udemy course popularity","f33acb49":"Hyperparameter optimization using optuna.","35b4ff8c":"# # Classification","4012a3dc":"[ ] How to include course title?\n \n - error: can't handle unknown\n\nWhy Ordinal Encoder? https:\/\/towardsdatascience.com\/one-hot-encoding-is-making-your-tree-based-ensembles-worse-heres-why-d64b282b5769\n\nOne hot encoder with categorical variables produces very sparse trees:\n\n![image.png](attachment:07d2bdeb-e3fb-4601-85c3-c4a6d149c975.png)!\n\n","c9af2505":"# Scratch pad","fdf8cb2c":"Hi! This notebook is still a work in progress. If you are interested in the process, and what I've accomplished so far... keep on reading ;)\n\nIn this notebook, I will analyze the dataset and check how to effectively predict the course popularity (defined as the number of subscribers).","d4c69cec":"# # Regression"}}