{"cell_type":{"13df0789":"code","5d1e5f2c":"code","e7106602":"code","ef527cee":"code","154ae24a":"code","d8506476":"code","3d335daa":"code","25cb4df7":"code","3079cee7":"code","51c32209":"code","dd8aad6c":"code","45e3891a":"code","cc9ff578":"code","c0e877dd":"code","4a653745":"code","c672bbf4":"code","9de4a4fc":"code","3f3d878a":"code","3f709e06":"code","7cc6a151":"code","f4143e7c":"code","ec8ce512":"markdown","58f67b3f":"markdown","c2f44385":"markdown","6dd0802e":"markdown","6e144fae":"markdown","2c3668b8":"markdown","80463b33":"markdown","9052c597":"markdown","a3bc4cfb":"markdown","a7886b40":"markdown","fea87b6e":"markdown","18aedcc3":"markdown","0f02d009":"markdown","4aada973":"markdown","ee4d2b86":"markdown","9beda466":"markdown","10cc0666":"markdown"},"source":{"13df0789":"import torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nimport warnings #Turn off warning\nwarnings.filterwarnings(\"ignore\")\nfrom IPython.display import Image #For image","5d1e5f2c":"#Only work with train set\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\", dtype=np.float32,nrows=10000)\n\n#Reduce these sample to 2000 for study purpose (200 image for each number)\n#Note that we are not using GPU here so the notebook will break if have more than 4000 sample. \ndf=pd.DataFrame({})\n\nfor i in range(10):\n    df=pd.concat([df,train[train.label==i].head(100)])\n\nprint(df.shape)\ndf.sample(5)","e7106602":"index=2\nsample_pic=df.iloc[index,1:].values\nplt.imshow(sample_pic.reshape(28,28),cmap='gray')","ef527cee":"# Prepare Dataset\n\n# split data into features(pixels) and labels(numbers from 0 to 9)\nX_temp = df.loc[:,df.columns != \"label\"].values\/255  # Will significantly increase your score\nX=(X_temp-X_temp.mean())\/(X_temp.std()+ 1e-8) #Highly reccomended\ny = df.label.values\n\n# train test split. Size of train data is 80% and size of test data is 20%. \nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.1,random_state = 420) \n\n# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\nX_train_torch = torch.from_numpy(X_train)\ny_train_torch = torch.from_numpy(y_train).type(torch.LongTensor) # data type is long\n\n# create feature and targets tensor for test set.\nX_test_torch = torch.from_numpy(X_test)\ny_test_torch = torch.from_numpy(y_test).type(torch.LongTensor) # data type is long","154ae24a":"Image(filename = \"..\/input\/neural-network\/MLP.png\")","d8506476":"from sklearn.neural_network import MLPClassifier","3d335daa":"mlp=MLPClassifier()\nmlp.fit(X_train,y_train)\npredict=mlp.predict(X_test)\nscore=(predict==y_test).sum()\/len(y_test)","25cb4df7":"print(score)","3079cee7":"import torch\nimport torch.nn as nn\nfrom torchvision import transforms, models\nfrom torch.autograd import Variable\nfrom torch import nn, optim\nimport torch.nn.functional as F","51c32209":"# smaller batches means that the number of parameter updates per epoch is greater. \nbatch_size = 100 \nnum_epochs = 200\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(X_train_torch,y_train_torch) #This is like panda DataFrame\ntest = torch.utils.data.TensorDataset(X_test_torch,y_test_torch)\n\n# data loader\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)\n","dd8aad6c":"class Classifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 5 Hidden Layer Network\n        self.fc1 = nn.Linear(28*28, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 128)\n        self.fc4 = nn.Linear(128, 64)\n        self.fc5 = nn.Linear(64, 10)\n        \n        # Dropout module with 0.2 probbability\n        self.dropout = nn.Dropout(p=0.2) #This is optional\n        # Add softmax on output layer\n        self.softmax = F.softmax\n        \n    def forward(self, x): #max(0,x)\n        x = self.dropout(F.relu(self.fc1(x))) #rectified linear unit (ReLU)\n        x = self.dropout(F.tanh(self.fc2(x)))\n        x = self.dropout(F.relu(self.fc3(x)))\n        x = self.dropout(F.relu(self.fc4(x)))\n        \n        x = self.softmax(self.fc5(x),dim=1)\n        \n        return x","45e3891a":"# Instantiate our model\nmodel = Classifier()\n# Define our loss function\ncriterion = nn.NLLLoss()\n# Define the optimier\noptimizer = optim.Adam(model.parameters(), lr=0.0015)\n","cc9ff578":"num_display=10\ndisplay_epoch=np.append(np.arange(0,num_epochs,int(num_epochs\/num_display)), num_epochs-1)\n\nfor epoch in range(num_epochs):\n    for images, labels in train_loader: # #Images=Total num image\/batch_size\n        \n        optimizer.zero_grad() # Prevent accumulation of gradients\n        # Make predictions\n        prediction = model(images.float()) #pass set of images to our Classifier(), stored as x and return prediction\n        loss = criterion(prediction, labels)#pass in loss function define above\n        \n        #backprop\n        loss.backward() #The neural net store everystep of the calculation. This simply reserves the previous gradient\n        optimizer.step() #... Does the update on weight and bias\n        \n        #################Report####################\n    with torch.no_grad():\n        model.eval()\n        if epoch in display_epoch:\n            \n            outputs = model(X_test_torch)\n            prediction=torch.max(outputs.data,1)[1]\n\n            accuracy = np.array((np.array(prediction) == y_test)).sum()\/100\n            print('Epoch: {}   Accuracy: {} %'.format(epoch+1, round(accuracy,3)))\n\n","c0e877dd":"Image(filename = \"..\/input\/neural-network\/CNN.jpeg\")","4a653745":"Image(filename = \"..\/input\/neural-network\/kernel.gif\")","c672bbf4":"Image(filename = \"..\/input\/neural-network\/maxpool_animation.gif\")","9de4a4fc":"# Import Libraries\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable","3f3d878a":"# Create CNN Model\nclass CNNModel(nn.Module):\n    def __init__(self):\n        super(CNNModel, self).__init__()\n        \n        # Convolution 1. DimIn 28*28, Dimout 24*24\n        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=1)\n        self.relu1 = nn.ReLU()\n        \n        # Max pool 1. DimIn 24 * 24, DimOut 12*12\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n     \n        # Convolution 2. DimIn 12*12, DimOut 10*10\n        self.cnn2 = nn.Conv2d(in_channels=6, out_channels=18, kernel_size=5, stride=1, padding=1)\n        self.relu2 = nn.ReLU()\n        \n        # Convolution 3 DimIn 10*10, DimOut 8*8 \n        self.cnn3 = nn.Conv2d(in_channels=18, out_channels=54, kernel_size=5, stride=1, padding=1)\n        self.relu3 = nn.ReLU()\n        \n        # Max pool 3: DimIn 8*8 , DimOut 4*4\n        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n        \n        # Fully connected 1\n        self.fc1 = nn.Linear(54 * 4 * 4, 27*2) \n        \n        # Fully connected 2\n        self.fc2 = nn.Linear(27*2, 10)\n        \n        #Dropout function\n        self.dropout = nn.Dropout(p=0.2) #This is optional\n        # Add softmax on output layer\n        self.softmax = F.softmax\n        \n    def forward(self, x):\n        # Convolution 1\n        x = self.cnn1(x)\n        x = self.relu1(x)\n        \n        # Max pool 1\n        x = self.maxpool1(x)\n        \n        # Convolution 2 \n        x = self.cnn2(x)\n        x = self.relu2(x)\n        \n        # Convolution 3 \n        x = self.cnn3(x)\n        x = self.relu3(x)\n        \n        # Max pool 3\n        x = self.maxpool3(x)\n        x = x.view(x.size(0), -1) #Flatten it before going to FC layer\n\n        # Linear function (readout)\n        x = self.dropout(F.relu(self.fc1(x)))\n        \n        # x = self.fc1(x)\n        x = self.softmax(self.fc2(x), dim=1)\n        \n        return x","3f709e06":"# batch_size, epoch and iteration\nbatch_size = 100\nnum_epochs = 200\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(X_train_torch,y_train_torch)\ntest = torch.utils.data.TensorDataset(X_test_torch,y_test_torch)\n\n# data loader\ntrain_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)","7cc6a151":"# Create CNN\nmodel = CNNModel()\n\n# Cross Entropy Loss \ncriterion = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.1\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","f4143e7c":"# CNN model training\nnum_display=10\ndisplay_epoch=np.append(np.arange(0,num_epochs,int(num_epochs\/num_display)), num_epochs-1)\n\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        \n        train = Variable(images.view(batch_size,1,28,28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = criterion(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n    with torch.no_grad():\n        model.eval()\n        if epoch in display_epoch:\n            X_test_resize = Variable(X_test_torch.view(X_test_torch.shape[0],1,28,28))\n            outputs = model(X_test_resize)\n            prediction=torch.max(outputs.data,1)[1]\n            accuracy = np.array((prediction == y_test_torch)).sum()\/100\n            print('Epoch: {}   Accuracy: {} %'.format(epoch+1, round(accuracy,3)))","ec8ce512":"# MLP with Pytorch","58f67b3f":"# Conclusion\n\nObserve that we have significant improvement using convolutional neural network. However, it takes a little longer to train the CNN","c2f44385":"# Introduction\n\nAfter reading this blog you will \n* Have a solid understanding of neural network architecture in Python language\n* Learn a few tricks that can improve your neural network performance\n\nMy code was written in a \"simple version\" so that beginner can easily follow and understand the concept. After that, feel free to copy and customize it.","6dd0802e":"Some gifs that present the convolution filter and maxpool concept...","6e144fae":"#### Future work\nHypertuning using gridsearch or random gridsearch. Do something like this:\n```\nfrom sklearn.model_selection import GridSearchCV\nparam_dist = {\"hidden_layer_sizes\": [(100,50,20,),(250,100,50,25),(500,250,125,63,30,15)],\n              \"activation\":['relu','tanh'],\n              \"alpha\":[0.0001,0.001,0.1,0.5],\n              \"learning_rate_init\" :[0.05,0.1]\n              }\n\ngrid_search = GridSearchCV(mlp, param_grid=param_dist, cv=3, iid=False)\n```\n**Warning** this may break your computer","2c3668b8":"# References\n\nFor concept behind neural network, I highly recommend you to read the series of [Convolutional Neural Networks for Visual Recognition](http:\/\/cs231n.github.io\/) lectures from Standford!","80463b33":"In this example, I will use 3 cnn layers and 2 fully connected layers (fc). The fully connected layers are very similar to MLP layers.","9052c597":"**Note:** that when we define the network, we should keep track of the dimension of our convolved feature, one way to do it is using the formula:\n\n$$n_{new}=n_{previous}+2p-k+1$$\n\nwhere $n \\times n$ is the images dimension, $p$ is padding, and $k$ is kernel size.\n\nFor the pooling step, the new image reduces $k$ times where $k$ is the kernel size of the pool filter. ","a3bc4cfb":"# Import Data","a7886b40":"Have fun doing what you are doing!","fea87b6e":"## Further work","18aedcc3":"The neural network is a black box. You throw data in the box and wait until you see the magical output without even knowing what happened. However, there are a few steps that you can do to improve your outcome. \n\n* Create a small sample and play with it first.\n* Use the convoluted neural network for image classification problem\n* Always normalize and scale your input. The discussion about it [here](https:\/\/stackoverflow.com\/questions\/4674623\/why-do-we-have-to-normalize-the-input-for-an-artificial-neural-network).\n* Initialize the [weight](https:\/\/stackoverflow.com\/questions\/49433936\/how-to-initialize-weights-in-pytorch). \n* Increase your batch size or number of epochs. This is only recommended if you have a good graphics card\n* Check if your neural network is overfitting so that you can add some regularization\n* The more data the better. You can expand your data set by creating new data! For this, read more about [GAN](https:\/\/stackoverflow.com\/questions\/49433936\/how-to-initialize-weights-in-pytorch)\n\nThe important key here is **time**. It might take 10 days to train a network for a person but might take others a few hours. All of the techniques above help the model converge faster hence take less time to train. You can also speed up the process by using GPU, either on a cloud or private computer. The step to activate your GPU can be found in this [discussion](https:\/\/discuss.pytorch.org\/t\/solved-make-sure-that-pytorch-using-gpu-to-compute\/4870).\n\n","0f02d009":"**Note that:**\n\n* `model.eval()` will notify all your layers that you are in eval mode, that way, batchnorm or dropout layers will not be applied on test set\n* `torch.no_grad()` impacts the autograd engine and deactivate it. It will reduce memory usage and speed up computations but you won\u2019t be able to backprop (which you don\u2019t want in an eval script).","4aada973":" \n\n#### Things that can be tweaked:\n* **batch size**: Number of images feed in NN = (total number of image)\/(batch size)\n\nExample: num_images = 10, batch_size=3, images input for the neural network are: 4,4, and 2. (the smallest number is the last). It is a good idea to have num images divisible by the batch size.\n\n* **Epoch**: number of time NN train all the data.  \nMore about the batch size and Epoch [here](https:\/\/machinelearningmastery.com\/difference-between-a-batch-and-an-epoch)\n\n* **Number of hidden layers** (5 in this case)\n\n* **Number of nodes for each layer** starting from the first hidden layer to the output layers `(512,256,128,64,10)`. They can be any integers, as long as the last one is equal to the number of unique labels.\n\n* **Activation function**: `ReLu, tanh, sigmpoid, leakyRelu,cSoftPlus`... Can have different one at each layer\n\n* **Brute force regularization**: dropout, batch_norm... This is optional and can be added to any layers that you want. The purpose of this is to adding some noise to reduce overfitting. A short explanation for dropout:\n\nDuring training, randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution. Each channel will be zeroed out independently on every forward call.\n\n* **Function at the last layers (classifier)**: `softmax, log_softmax`,... rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1. There are other options for the regression problem.\n\n* **criterion**: `NLLLoss(),CrossEntropyLoss, KLDivLoss`(continuos) calculate the error between output and target. \n\nAll of the above can be found in [here](https:\/\/pytorch.org\/docs\/stable\/nn.html)\n\n* **optimizer**: Adam, ASGD, SGD... different method to update Weights and Bias values. More about [this](https:\/\/pytorch.org\/docs\/stable\/optim.html)\n\n#### Note: in this example, we only deal with the gray image.","ee4d2b86":"A Convolutional Neural Network (CNN) is comprised of one or more convolutional layers (often with a subsampling step) and then followed by one or more fully connected layers as in a standard multilayer neural network. The architecture of a CNN is designed to take advantage of the 2D structure of an input image (or other 2D input such as a speech signal or image).","9beda466":"# Convolutioncal Neural Network (CNN)","10cc0666":"# Multilayer Perceptron (MLP) with sklearn\n\nThe multilayer perceptron (MLP) is a feed-forward, supervised learning network with up to two hidden layers. The MLP network is a function of one or more predictors (also called inputs or independent variables) that minimizes the prediction error of one or more target variables (also called outputs)."}}