{"cell_type":{"2efeb2bd":"code","b138c8a5":"code","dd130a72":"code","058ff0a4":"code","a9af53f9":"code","aa03fe2c":"code","30b30cf2":"code","e8c13970":"code","963c0828":"code","add7ad5f":"code","4c3f6dfc":"code","ce57abfb":"code","e17821bf":"code","eede7ffa":"code","282d4dd8":"code","26f94a03":"code","be7417ed":"code","51589dd8":"code","21314afd":"code","0a8a14bb":"code","3c299358":"code","ee515ee0":"code","1d0f2921":"code","61341c99":"code","bc4cd3ea":"code","d4e1b12d":"code","9e52ed32":"code","9a600fc5":"markdown","d68514a7":"markdown","0c07f2a5":"markdown","19f90e5d":"markdown","a1a1610e":"markdown","68dfad28":"markdown","12cfe68b":"markdown","b0862880":"markdown","f8909ee0":"markdown","024dd086":"markdown","f08f1c4c":"markdown","8faf913a":"markdown","e6d86461":"markdown","932fe7a0":"markdown","169f77af":"markdown","1f37ec6d":"markdown","59c258e2":"markdown","76b7426d":"markdown","eaffb421":"markdown","1168557e":"markdown"},"source":{"2efeb2bd":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b138c8a5":"# Load Data\ndf = pd.read_csv('..\/input\/creditcard.csv')","dd130a72":"print(\"Samples and Features in Dataset : \",df.shape)\ndf.head()","058ff0a4":"# Data Quality Check\nprint(\"Percentage of Null Data : \")\nprint(\"No null values in data set !!\")\n(df.isna()).sum()\/len(df)","a9af53f9":"# Look at the distribution of Class Variable\nprint(\"Is highly unbalanced  class distribution, as most of the data is of negative class\")\nprint('-'*70)\ndf['Class'].value_counts().plot.bar()","aa03fe2c":"# Look at distribution of other variables\nfig = plt.figure(figsize=(20,24))\nloc = 1\nfor i in df.drop('Class',axis=1).columns:\n    ax = fig.add_subplot(6,5,loc)\n    loc+=1\n    sns.distplot(df[i],ax=ax)","30b30cf2":"# Distribution plot of Time variable\nplt.figure(figsize=(12,6))\nsns.distplot(df['Time']\/60\/60)\nprint('Seems like less trasaction during night time')\nprint('Any how, variable does not look very important, will drop it.')\nprint('-'*70)","e8c13970":"df = df.drop('Time',axis=1)","963c0828":"# Lets rescale the amount variable, as other are\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ndf['Amount'] = scaler.fit_transform(np.array(df['Amount']).reshape(-1,1))","add7ad5f":"df.head()","4c3f6dfc":"# Correlation Matrix\nprint('No major correlation between any variable, few seem to  have with amount and class')\nprint('-'*70)\nplt.figure(figsize=(6,6))\nsns.heatmap(df.corr())","ce57abfb":"# Correlation Matrix for Amount and Class\nprint(\"No Major Correlation here also\")\nprint('-'*40)\ndf.corr()[['Amount','Class']]","e17821bf":"# Under Sampling Data for Model building\nX1 = df[df['Class'] == 1]\nX2 = df[df['Class'] == 0].sample(492)\nX = pd.concat([X1,X2],axis=0)\nX = X.sample(frac=1).reset_index(drop=True)\ny = X['Class']\nX = X.drop('Class',axis=1)","eede7ffa":"y.value_counts().plot.bar()","282d4dd8":"# Import Important Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC","26f94a03":"# Lets Seperate Training And Testing Data for our sample dataset\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.4)\nprint('Training Data Size : ',X_train.shape)\nprint('Testing Data Size : ',X_test.shape)\nprint('Training Data Class Size : ',y_train.shape)\nprint('Testing Data Class Size : ',y_test.shape)","be7417ed":"def modeller(X,y,model):\n    model.fit(X,y)\n    return model","51589dd8":"def scorer(X,y,model):\n    pred = model.predict(X)\n    acc = round(accuracy_score(y,pred),4)\n    recc = round(recall_score(y,pred),4)\n    return (acc,recc)","21314afd":"# Lets build models for Logistic Regression, Neural Network and Support Vector Machine Models\nlr = modeller(X_train,y_train,LogisticRegression(solver='lbfgs',))\nmlp = modeller(X_train,y_train,MLPClassifier())\nsvm =  modeller(X_train,y_train,SVC(gamma='auto',probability=True))","0a8a14bb":"print('Model Name \\t\\t| Accuracy \\t| Recall Acuracy')\nprint('-'*50)\nacc,recc = scorer(X_test,y_test,lr)\nprint('Logistic Regression \\t| {} \\t| {}'.format(acc,recc))\nacc,recc = scorer(X_test,y_test,mlp)\nprint('Neural Network \\t\\t| {} \\t| {}'.format(acc,recc))\nacc,recc = scorer(X_test,y_test,svm)\nprint('Support Vector Machine \\t| {} \\t| {}'.format(acc,recc))\nprint('-'*50)","3c299358":"def roc_scorer(X,y,model):\n    preds = model.predict_proba(X)[:,1]\n    fpr,tpr,_ = roc_curve(y,preds)\n    roc_auc = round(auc(fpr,tpr),6)\n    return (fpr,tpr,roc_auc)","ee515ee0":"fig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111)\nfpr,tpr,roc_auc = roc_scorer(X_test,y_test,lr)\nax.plot(fpr,tpr,label='Logistic Regression (Area under the curve) : {}'.format(roc_auc))\nfpr,tpr,roc_auc = roc_scorer(X_test,y_test,mlp)\nax.plot(fpr,tpr,label='Neural Network (Area under the curve) : {}'.format(roc_auc))\nfpr,tpr,roc_auc = roc_scorer(X_test,y_test,svm)\nax.plot(fpr,tpr,label='Support Vector Machine (Area under the curve) : {}'.format(roc_auc))\nplt.legend(loc=4)","1d0f2921":"# Lets use the Neural Network Model on our full data set","61341c99":"# Setup full data for predictions\ndf_y = df['Class']\ndf_X  = df.drop('Class',axis=1)\ndf_X.shape","bc4cd3ea":"# Lets use the model we fit on our balanced data set to predict full data\nmlp = modeller(X_train,y_train,MLPClassifier())\nprint('Model Name \\t\\t| Accuracy \\t| Recall Acuracy')\nprint('-'*50)\nacc,recc = scorer(df_X,df_y,mlp)\nprint('Neural Network \\t\\t| {} \\t| {}'.format(acc,recc))\nprint('-'*50)","d4e1b12d":"# Let get the ROC curve on the the full data set\nfig = plt.figure(figsize=(8,6))\nax = fig.add_subplot(111)\nfpr,tpr,roc_auc = roc_scorer(df_X,df_y,mlp)\nax.plot(fpr,tpr,label='Neural Network (Area under the curve) : {}'.format(roc_auc))\nplt.legend(loc=4)","9e52ed32":"# Confusion Matrix\nfig = plt.figure(figsize=(6,6))\nsns.heatmap(confusion_matrix(df_y,mlp.predict(df_X)),annot=True,cmap='Blues')\nplt.xlabel('Predict Class')\nplt.ylabel('True Class')","9a600fc5":"Under-representation of a class in one or more important predictor variables. Suppose, to address the question of gender discrimination, we have survey data on salaries within a particular field, e.g., computer software. It is known women are under-represented considerably in a random sample of software engineers, which would be important when adjusting for other variables such as years employed and current level of seniority. Suppose only 20% of software engineers are women, i.e., males are 4 times as frequent as females. If we were designing a survey to gather data, we would survey 4 times as many females as males, so that in the final sample, both genders will be represented equally.","d68514a7":"We can see Logistic gives best accuracy  and  recall score,\nbut as the score is close, lets look at ROC curve for both","0c07f2a5":"### Hi there!! Through this notebook i am trying to evaluate and model Credit Card Fraud Data Set","19f90e5d":"We can see that  even though Neural Network is not the best model when using recall or accuracy score to decide, it stands out when using Area Under the Curve","a1a1610e":"We can see that the model is predicting around 477 frauds correctly, out of total 492 frauds.\nThis is a good prediction, given that we have just used simple neural network and not used any Deep Learning models","68dfad28":"# Under Sampling","12cfe68b":"I am working on adding a deep learning section into the notebook in future versions.","b0862880":"# Introduction","f8909ee0":"The datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.","024dd086":"## Exploratry Data Analysis","f08f1c4c":"# Machine Learning Models","8faf913a":"We are taking 492 available samples of 'Class 1' and joining them with random 492 'Class 0' rows to create a balanced data set for modelling our classifier.","e6d86461":"We are getting 0.99 value for AUC, which is pretty good.","932fe7a0":"We will analyze and understand the dataset, then apply needed tranformations to make it suitable for modelling.\nThen we would undersample the dataset to get a balanced sample data, where class 1 and class 0 targets are in equal ratio.\n\nThen we will use a few machine learning algorithms to find the best fit for our model.\nAfter which we will use the model generated to predict the enitre data set and see how is the  accuracy score and recall score.\n\nWe will also use ROC curve and Area under the  curve to decide which classifier to use.","169f77af":"# Data Wrangling","1f37ec6d":"All the features are mean centered except 'Time' lets look at it more closely","59c258e2":"Please Upvote if you find this notebook useful and fun to read, this would motivate me to publish  more good work!!","76b7426d":"# Credit Card Fraud Detection","eaffb421":"Do comment your feedback, it would be much appreciated, and do not forget to upvote if you find this intresting.\nHappy Analytics to you !!!","1168557e":"Please feel free to leave any comments or suggestions.."}}