{"cell_type":{"3de7f78b":"code","18c82f17":"code","1dbb3fbc":"code","e6ffaae9":"code","7371b1b3":"code","be5da9c9":"code","155f5aba":"code","bc52e4c2":"code","fb320108":"markdown","f3d1ab0c":"markdown","2bd4f8c5":"markdown","9cc464d1":"markdown","394b06fb":"markdown","8b708a77":"markdown","f4dc0eff":"markdown","17856317":"markdown"},"source":{"3de7f78b":"import numpy\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F","18c82f17":"class DiceLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice = (2.*intersection + smooth)\/(inputs.sum() + targets.sum() + smooth)  \n        \n        return 1 - dice","1dbb3fbc":"class DiceBCELoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(DiceBCELoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        intersection = (inputs * targets).sum()                            \n        dice_loss = 1 - (2.*intersection + smooth)\/(inputs.sum() + targets.sum() + smooth)  \n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        Dice_BCE = BCE + dice_loss\n        \n        return Dice_BCE","e6ffaae9":"class IoULoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(IoULoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #intersection is equivalent to True Positive count\n        #union is the mutually inclusive area of all labels & predictions \n        intersection = (inputs * targets).sum()\n        total = (inputs + targets).sum()\n        union = total - intersection \n        \n        IoU = (intersection + smooth)\/(union + smooth)\n                \n        return 1 - IoU","7371b1b3":"ALPHA = 0.8\nGAMMA = 2\n\nclass FocalLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(FocalLoss, self).__init__()\n\n    def forward(self, inputs, targets, alpha=ALPHA, gamma=GAMMA, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #first compute binary cross-entropy \n        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n        BCE_EXP = torch.exp(-BCE)\n        focal_loss = alpha * (1-BCE_EXP)**gamma * BCE\n                       \n        return focal_loss","be5da9c9":"ALPHA = 0.5\nBETA = 0.5\n\nclass TverskyLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(TverskyLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #True Positives, False Positives & False Negatives\n        TP = (inputs * targets).sum()    \n        FP = ((1-targets) * inputs).sum()\n        FN = (targets * (1-inputs)).sum()\n       \n        Tversky = (TP + smooth) \/ (TP + alpha*FP + beta*FN + smooth)  \n        \n        return 1 - Tversky","155f5aba":"ALPHA = 0.5\nBETA = 0.5\nGAMMA = 1\n\nclass FocalTverskyLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(FocalTverskyLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, gamma=GAMMA):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #True Positives, False Positives & False Negatives\n        TP = (inputs * targets).sum()    \n        FP = ((1-targets) * inputs).sum()\n        FN = (targets * (1-inputs)).sum()\n        \n        Tversky = (TP + smooth) \/ (TP + alpha*FP + beta*FN + smooth)  \n        FocalTversky = (1 - Tversky)**gamma\n                       \n        return FocalTversky","bc52e4c2":"ALPHA = 0.5 # < 0.5 penalises FP more, > 0.5 penalises FN more\nCE_RATIO = 0.5 #weighted contribution of modified CE loss compared to Dice loss\n\nclass ComboLoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(ComboLoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA):\n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #True Positives, False Positives & False Negatives\n        intersection = (inputs * targets).sum()    \n        dice = (2. * intersection + smooth) \/ (inputs.sum() + targets.sum() + smooth)\n        \n        inputs = torch.clamp(inputs, e, 1.0 - e)       \n        out = - (ALPHA * ((targets * torch.log(inputs)) + ((1 - ALPHA) * (1.0 - targets) * torch.log(1.0 - inputs))))\n        weighted_ce = out.mean(-1)\n        combo = (CE_RATIO * weighted_ce) - ((1 - CE_RATIO) * dice)\n        \n        return combo","fb320108":"# BCE-Dice Loss\n---\nThis loss combines Dice loss with the standard binary cross-entropy (BCE) loss that is generally the default for segmentation models. Combining the two methods allows for some diversity in the loss, while benefitting from the stability of BCE. The equation for multi-class BCE by itself will be familiar to anyone who has studied logistic regression:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/80f87a71d3a616a0939f5360cec24d702d2593a2)","f3d1ab0c":"# Jaccard\/Intersection over Union (IoU) Loss\n---\nThe IoU metric, or Jaccard Index, is similar to the Dice metric and is calculated as the ratio between the overlap of the positive instances between two sets, and their mutual combined values:\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/eaef5aa86949f49e7dc6b9c8c3dd8b233332c9e7)\n\nLike the Dice metric, it is a common means of evaluating the performance of pixel segmentation models.","2bd4f8c5":"# Focal Tversky Loss\n---\n\nA variant on the Tversky loss that also includes the gamma modifier from Focal Loss.","9cc464d1":"# Combo Loss\n---\nThis loss was introduced by Taghanaki et al in their paper \"Combo loss: Handling input and output imbalance in multi-organ segmentation\", retrievable here: https:\/\/arxiv.org\/abs\/1805.02798. Combo loss is a combination of Dice Loss and a modified Cross-Entropy function that, like Tversky loss, has additional constants which penalise either false positives or false negatives more respectively.\n\nSince my GPU quota has run out this week, as of V16 these functions have not been tested, so please leave any debugging notes in the comments section below.","394b06fb":"\u0417\u0434\u0435\u0441\u044c \u0441\u043e\u0431\u0440\u0430\u043d\u044b \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0440\u0435\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u0439 \u043b\u043e\u0441\u0441\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u043e\u0433\u0443\u0442 \u0431\u044b\u0442\u044c \u043f\u043e\u043b\u0435\u0437\u043d\u044b \u0434\u043b\u044f \u0434\u0430\u043d\u043d\u043e\u0439 \u0437\u0430\u0434\u0430\u0447\u0438.\n\n(\u041f\u043e \u043c\u043e\u0442\u0438\u0432\u0430\u043c https:\/\/www.kaggle.com\/bigironsphere\/loss-function-library-keras-pytorch)","8b708a77":"# Focal Loss\n---\nFocal Loss was introduced by *Lin et al* of Facebook AI Research in 2017 as a means of combatting extremely imbalanced datasets where positive cases were relatively rare. Their paper \"Focal Loss for Dense Object Detection\" is retrievable here: https:\/\/arxiv.org\/abs\/1708.02002. In practice, the researchers used an alpha-modified version of the function so I have included it in this implementation.","f4dc0eff":"# Dice Loss\n---\n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/a80a97215e1afc0b222e604af1b2099dc9363d3b)","17856317":"# Tversky Loss\n---\nThis loss was introduced in \"Tversky loss function for image segmentationusing 3D fully convolutional deep networks\", retrievable here: https:\/\/arxiv.org\/abs\/1706.05721. It was designed to optimise segmentation on imbalanced medical datasets by utilising constants that can adjust how harshly different types of error are penalised in the loss function. From the paper:\n\n>... in the case of \u03b1=\u03b2=0.5 the Tversky index simplifies to be the same as the Dice coefficient, which is also equal to the F1 score.  With \u03b1=\u03b2=1, Equation 2 produces Tanimoto coefficient, and setting \u03b1+\u03b2=1 produces the set of F\u03b2 scores. Larger \u03b2s weigh recall higher than precision (by placing more emphasis on false negatives).\n\nTo summarise, this loss function is weighted by the constants 'alpha' and 'beta' that penalise false positives and false negatives respectively to a higher degree in the loss function as their value is increased. The beta constant in particular has applications in situations where models can obtain misleadingly positive performance via highly conservative prediction. You may want to experiment with different values to find the optimum. With alpha==beta==0.5, this loss becomes equivalent to Dice Loss."}}