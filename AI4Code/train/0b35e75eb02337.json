{"cell_type":{"325ff3c3":"code","538b016b":"code","f24bc81d":"code","7661a63e":"code","2e31d6e9":"code","c0f005f8":"code","edd8421a":"code","25118e5d":"code","16f92bec":"code","01cc8a63":"code","7497d6f8":"code","d76dab87":"code","6869543d":"code","4de7e131":"code","99240ad4":"code","86b098a6":"code","f5fbad07":"code","6fabb0b3":"code","a2f0e0e3":"code","38ed731f":"code","ad4e9709":"code","c937a28d":"code","06054dcc":"code","7ae12275":"code","2ee1786f":"code","59907f20":"code","7a4adebc":"markdown","bd8474b7":"markdown","20e43f48":"markdown","71d4ef3c":"markdown","48c3d093":"markdown","66c7d00f":"markdown","25e7a076":"markdown","74625a02":"markdown","f611f2ab":"markdown","6fa88f23":"markdown","488fdd6f":"markdown","c392b027":"markdown","d78ac7a1":"markdown","af0cb63a":"markdown","167e9de6":"markdown","79d16522":"markdown","a2691752":"markdown","0182fecb":"markdown","7d641777":"markdown","a5b8a8af":"markdown","66a949e7":"markdown","e57249cb":"markdown","8c7c12ad":"markdown","88636bff":"markdown","c93564c2":"markdown","64567e60":"markdown","3b7e54b6":"markdown"},"source":{"325ff3c3":"import pandas as pd\nimport math\nimport os\n\ndata = pd.read_csv(\"..\/input\/santander-customer-transaction-prediction\/train.csv\")\nprint(data.shape)\ndata.head()","538b016b":"y = data.target.values\nx_data = data.drop(['target','ID_code'], axis=1)","f24bc81d":"from sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler()  \nscaler.fit(x_data)  \nx_data = scaler.transform(x_data)\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_data, y, test_size=0.1, random_state=42)","7661a63e":"y_train= y_train.reshape((y_train.shape[0], 1))\ny_test= y_test.reshape((y_test.shape[0] , 1))\n\nx_train=x_train.T\ny_train=y_train.T\n\nprint(x_train.shape)\nprint(y_train.shape)","2e31d6e9":"import numpy as np\nimport matplotlib.pyplot as plt","c0f005f8":"def initialize_parameters(layer_dims):\n    parameters = {}\n    L = len(layer_dims)           \n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n    return parameters  ","edd8421a":"def linear_forward(A, W, b):\n    Z = np.dot(W,A)+b\n    cache = (A, W, b)\n    return Z, cache","25118e5d":"def sigmoid(z):\n    A = 1\/(1 + np.exp(-z))\n    activation_cache = A.copy()\n    return A, activation_cache","16f92bec":"def relu(z):\n    A = z*(z > 0)\n    activation_cache = z\n    return A, activation_cache","01cc8a63":"def linear_activation_forward(A_prev, W, b, activation):\n    if activation == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n    elif activation == \"relu\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n    cache = (linear_cache, activation_cache)\n    return A, cache","7497d6f8":"def forward_propagation(minibatch_X, parameters):\n    caches = []\n    A = minibatch_X\n    L = len(parameters) \/\/ 2                  \n    for l in range(1, L):\n        A_prev = A \n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n        caches.append(cache)\n    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n    caches.append(cache)\n    return AL, caches","d76dab87":"def compute_cost(AL, Y,parameters,lambd ):\n    m = Y.shape[1]\n    cost = (-1\/m)*np.sum(np.multiply(Y,np.log(AL))+np.multiply((1-Y),np.log(1-AL)))\n    cost = np.squeeze(cost)  \n    \n    L = len(parameters) \/\/ 2 \n    regularization = 0;\n    \n    for l in range(L):\n        regularization +=  np.sum(np.square(parameters[\"W\" + str(l + 1)]))\n        \n    L2_regularization_cost = lambd \/ (2 * m) * regularization\n    cost = cost + L2_regularization_cost\n    return cost","6869543d":"def random_mini_batches(X, Y, mini_batch_size=64, seed=0):\n    np.random.seed(seed)  \n    m = X.shape[1] \n    mini_batches = []\n\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1, m))\n    \n    num_complete_minibatches = math.floor(m \/ mini_batch_size) \n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n        mini_batch_Y = shuffled_Y[:, k * mini_batch_size:(k + 1) * mini_batch_size]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[:, (k + 1) * mini_batch_size:]\n        mini_batch_Y = shuffled_Y[:, (k + 1) * mini_batch_size:]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n\n    return mini_batches","4de7e131":"def initialize_velocity(parameters):\n    L = len(parameters) \/\/ 2  \n    v = {}\n    for l in range(L):\n        v[\"dW\" + str(l + 1)] = np.zeros(parameters['W' + str(l + 1)].shape)\n        v[\"db\" + str(l + 1)] = np.zeros(parameters['b' + str(l + 1)].shape)\n    return v","99240ad4":"def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n    L = len(parameters) \/\/ 2  \n    for l in range(L):\n        v[\"dW\" + str(l + 1)] = beta * v[\"dW\" + str(l + 1)] + (1 - beta) * grads['dW' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta * v[\"db\" + str(l + 1)] + (1 - beta) * grads['db' + str(l + 1)]\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v[\"dW\" + str(l + 1)]\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v[\"db\" + str(l + 1)]\n    return parameters, v","86b098a6":"def initialize_adam(parameters):\n    L = len(parameters) \/\/ 2 \n    v = {}\n    s = {}\n    for l in range(L):\n        v[\"dW\" + str(l + 1)] = np.zeros(parameters['W' + str(l + 1)].shape)\n        v[\"db\" + str(l + 1)] = np.zeros(parameters['b' + str(l + 1)].shape)\n        s[\"dW\" + str(l + 1)] = np.zeros(parameters['W' + str(l + 1)].shape)\n        s[\"db\" + str(l + 1)] = np.zeros(parameters['b' + str(l + 1)].shape)\n    return v, s","f5fbad07":"def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate=0.01,beta1=0.9, beta2=0.999, epsilon=1e-8):\n    L = len(parameters) \/\/ 2  \n    v_corrected = {}  \n    s_corrected = {}  \n\n    for l in range(L):\n        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n\n        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] \/ (1 - np.power(beta1, t))\n        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] \/ (1 - np.power(beta1, t))\n\n        s[\"dW\" + str(l + 1)] = beta1 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n        s[\"db\" + str(l + 1)] = beta1 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n\n        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] \/ (1 - np.power(beta2, t))\n        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] \/ (1 - np.power(beta2, t))\n\n        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * (\n        v_corrected[\"dW\" + str(l + 1)] \/ (np.sqrt(s_corrected[\"dW\" + str(l + 1)]) + epsilon))\n        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * (\n        v_corrected[\"db\" + str(l + 1)] \/ (np.sqrt(s_corrected[\"db\" + str(l + 1)]) + epsilon))\n    return parameters, v, s","6fabb0b3":"def update_parameters_with_gd(parameters, grads, learning_rate):\n    L = len(parameters) \/\/ 2 \n    for l in range(L):\n        parameters[\"W\" + str(l+1)] = parameters[\"W\"+str(l+1)]-learning_rate*grads[\"dW\"+str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\"+str(l+1)]-learning_rate*grads[\"db\"+str(l+1)]\n    return parameters","a2f0e0e3":"def linear_backward(dZ, cache,lambd):\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    dW = (1\/m)*np.dot(dZ,A_prev.T) + lambd \/ m * W\n    db = (1\/m)*np.sum(dZ, axis=1, keepdims = True)\n    dA_prev = np.dot(W.T,dZ)\n    return dA_prev, dW, db","38ed731f":"def sigmoid_backward(dA, activation_cache):\n    return dA*(activation_cache*(1-activation_cache))","ad4e9709":"def relu_backward(dA, activation_cache):\n    return dA*(activation_cache > 0)","c937a28d":"    \ndef linear_activation_backward(dA, cache, activation, lambd):\n    linear_cache, activation_cache = cache\n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache,lambd)        \n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache,lambd)    \n    return dA_prev, dW, db","06054dcc":"def backward_propagation(minibatch_X, minibatch_Y, caches, lambd):\n    grads = {}\n    L = len(caches) \n    m = minibatch_X.shape[1]\n    minibatch_Y = minibatch_Y.reshape(1,minibatch_X.shape[1])\n    dAL = - (np.divide(minibatch_Y, minibatch_X) - np.divide(1 - minibatch_Y, 1 - minibatch_X))\n    current_cache = caches[L-1]\n    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\", lambd)\n    for l in reversed(range(L-1)):\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\", lambd)\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n    return grads","7ae12275":"def model(X, Y, layers_dims, optimizer, learning_rate=0.0007, mini_batch_size=64, beta=0.9,beta1=0.9, beta2=0.999, epsilon=1e-8,lambd = 0.7,  num_epochs=10000, print_cost=True):\n    L = len(layers_dims)  \n    costs = []  \n    t = 0  \n    seed = 10  \n    parameters = initialize_parameters(layers_dims)\n\n    if optimizer == \"gd\":\n        pass  \n    elif optimizer == \"momentum\":\n        v = initialize_velocity(parameters)\n    elif optimizer == \"adam\":\n        v, s = initialize_adam(parameters)\n\n    for i in range(num_epochs):\n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n        for minibatch in minibatches:\n            (minibatch_X, minibatch_Y) = minibatch\n            AL, caches = forward_propagation(minibatch_X, parameters)\n            cost = compute_cost(AL, minibatch_Y,parameters,lambd )\n            grads = backward_propagation(AL, minibatch_Y, caches, lambd)\n            if optimizer == \"gd\":\n                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n            elif optimizer == \"momentum\":\n                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n            elif optimizer == \"adam\":\n                t = t + 1  \n                parameters, v, s = update_parameters_with_adam(parameters, grads, v, s,t, learning_rate, beta1, beta2, epsilon)\n        if print_cost and i % 1 == 0:\n            print(\"Cost after epoch %i: %f\" % (i, cost))\n        if print_cost and i % 1 == 0:\n            costs.append(cost)\n\n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return parameters","2ee1786f":"def predict(X, y, parameters): \n    m = X.shape[1]\n    p = np.zeros((1,m), dtype = np.int)\n    AL, caches = forward_propagation(X, parameters)\n    for i in range(0, AL.shape[1]):\n        if AL[0,i] > 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n    print(str(np.mean((p[0,:] == y[0,:]))))\n    return p","59907f20":"layers_dims =np.array([200,1000,1000,1000,1000,1000,1])\nparameters = model(x_train, y_train, layers_dims, optimizer=\"adam\", learning_rate=0.001, mini_batch_size=10000, beta=0.9,beta1=0.9, beta2=0.999, epsilon=1e-8,lambd = 13.24, num_epochs=100, print_cost=True)\npred_train = predict(x_train, y_train, parameters)\nprint(pred_train)\npred_test = predict(x_test.T, y_test.T, parameters)\nprint(pred_test)","7a4adebc":"**Initialize the Weights and Biases for all layers**","bd8474b7":"**Apply the Linear activation code over the linear backward propagation**","20e43f48":"**Update parameters using momentum gradients**","71d4ef3c":"**Aggregate the Forward propagation all together**","48c3d093":"**Linear backpropagation calculation**","66c7d00f":"**Define sigmoid activation function for binary classification layer**","25e7a076":"**Run the code and define the layers size and number**","74625a02":"**Initialize adam optimizer parameters that is needed in adam gradients updating**","f611f2ab":"**Predection function to use the model and check the results **","6fa88f23":"**Do the feature scaling**\n\n**Then Split Features and Labels into training set and testing set**","488fdd6f":"**Define relu activation function for Hidden layers**","c392b027":"**Hereby in this Kernal I will Implement Deep Learning technique (Deep Neural Network) from scratch, the implementation is gonna start by data importing as next.\n    after importing data, i will display the shape of the training set and sample from data**","d78ac7a1":"**Update parameters using normal BGD gradients**","af0cb63a":"**Divide data into Features and Labels**","167e9de6":"**Update parameters using adam gradients**","79d16522":"**- Code have been inspired by Deep Learning specialaization coursera**","a2691752":"**Calculate sigmoid activation derivative function for binary classification layer backpropagation calculations**","0182fecb":"**Calculate the cross-entropy cost function**","7d641777":"**Import needed Library for implementation**","a5b8a8af":"**Reshape Features and Labels datasets to have the fixed shape and prevent (x, ) shape**","66a949e7":"**Calculate relu activation derivative function for hiddin layers backpropagation calculations**","e57249cb":"**Linear Forward propagation**","8c7c12ad":"**Apply the activation code over the linear forward propagation**","88636bff":"**Compute the Mini-batches by determined size**","c93564c2":"**Initialize Velocity parameters that is needed in momentum gradients updating**","64567e60":"**Put all backppropagation together**","3b7e54b6":"**All together model**"}}