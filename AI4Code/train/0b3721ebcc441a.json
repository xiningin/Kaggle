{"cell_type":{"9ee53325":"code","5c8b0bfb":"code","c7077b3b":"code","ae8d9f53":"code","c2ca6bb0":"code","5c4dcd51":"code","7ca01aee":"code","913b80dd":"code","a97fb45c":"code","1d8765af":"code","942ce34d":"code","2d472d19":"code","c9f3cbc8":"code","603751cb":"code","95ab940f":"code","d29d3f28":"code","da309bab":"code","80ee701e":"markdown","cfb2cde8":"markdown","9ef3d635":"markdown","814bfcfa":"markdown","72979b35":"markdown","bc161754":"markdown"},"source":{"9ee53325":"import numpy as np \nimport pandas as pd\nfrom tqdm import tqdm_notebook\nfrom sklearn.linear_model import Lasso, Ridge\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom numpy.random import seed\nseed(802)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","5c8b0bfb":"%%time\ntrain = pd.read_csv(\"..\/input\/train.csv\", \n                         dtype={\"acoustic_data\": np.int16, \n                                \"time_to_failure\": np.float32}).values","c7077b3b":"## the simplified one\ndef extract_features(z):\n    z_abs = np.abs(z)\n    return np.c_[z.mean(axis=1),\n                 z_abs.max(axis=1),\n                 (z*(z>=0)*(z<=10)).std(axis=1),\n                 np.transpose(np.quantile(z, q=[0.7, 0.95], axis=1)),\n                 np.transpose(np.quantile(z_abs, q=[0.3, 0.95], axis=1))]\n\n\ndef create_X(x, last_index=None, n_steps=150, step_length=1000):\n    if last_index == None:\n        last_index = len(x)\n       \n    assert last_index - n_steps * step_length >= 0\n\n    temp = x[(last_index - n_steps * step_length):last_index].reshape(n_steps, -1)\/3\n    \n    # Extracts features of sequences of full length 1000 and some fractions of it\n    return np.c_[extract_features(temp),\n                 extract_features(temp[:, -step_length \/\/ 3:]),\n                 extract_features(temp[:, -step_length \/\/ 10:])]","ae8d9f53":"%%time\nrows = 150_000\nn_features = create_X(train[:rows,0]).shape[1]\nn_targets = 5\nprint(\"The model RNN is based on {0} features and {1} targets.\\n\".format(n_features, n_targets))","c2ca6bb0":"batch_size = 32\neps = 0.5 # weights for the second target (time_after_failure)\nn_steps = 150\nstep_length = rows\/\/n_steps\n\nindex_earthquake_start = np.nonzero(np.diff(train[:,1]) > 0)[0] + 1\ncv_earthquake_index = index_earthquake_start[1]","5c4dcd51":"print(index_earthquake_start)","7ca01aee":"def generator(data, min_index=0, max_index=None, batch_size=32, n_steps=150, step_length=1000):\n    if max_index is None:\n        max_index = len(data) - 1\n     \n    while True:\n        # Pick indices of ending positions\n        seg_length = n_steps * step_length\n        index_range = range(min_index + seg_length, max_index, 20000)\n        rows = np.random.choice(index_range, size=batch_size, replace=False)\n#         for limit in index_earthquake_start: \n#             rows = rows[np.logical_not\\\n#                         (np.logical_and(rows>limit, rows<(limit+160000)))]\n         \n        # Initialize feature matrices and targets\n        samples = np.zeros((batch_size, n_steps, n_features))\n        \n        ## adding a sequence of targets or a single target\n        targets = np.zeros((batch_size, n_targets))\n        \n        for j, row in enumerate(rows):\n            samples[j] = create_X(data[:, 0], \n                                  last_index = row, \n                                  n_steps = n_steps,\n                                  step_length = step_length)\n            \n            if n_targets == 1:\n                ## single target\n                targets[j] = data[row - 1, 1]\n            elif n_targets == 2:\n                ## here the training needs to be chosen as the ones after the first earthquake\n                targets[j,0] = data[row - 1, 1]\n                ## time_after_failure \n                taf_idx = index_earthquake_start[np.sum(row > index_earthquake_start) - 1]\n                targets[j,1] = eps*(data[taf_idx, 1] - targets[j,0])\n            elif n_targets > 2:\n                ## multiple targets (preferably odd number)\n                for i in range(n_targets):\n                    # targets are all in one segments\n                    # targets[j,i] = data[row-1-i*(seg_length\/\/n_targets), 1]  \n                    # targets are spanning multiple neighboring segments\n                    targets[j,i] = data[row - 1 - i*(seg_length\/\/2), 1] \n\n        yield samples, targets","913b80dd":"# Initialize generators\ntrain_gen = generator(train, \n                      batch_size=batch_size,\n                      n_steps=n_steps, \n                      step_length=step_length) \n\n# train_gen = generator(train, \n#                       batch_size=batch_size, \n#                       min_index=cv_earthquake_index,\n#                       n_steps=n_steps, \n#                       step_length=step_length)\n\nvalid_gen = generator(train, \n                      batch_size=batch_size, \n                      max_index=cv_earthquake_index-1,\n                      n_steps=n_steps, \n                      step_length=step_length)\n\n# verify the generator\naux, aux2 = next(train_gen)\nprint(aux.shape, aux2.shape)","a97fb45c":"from keras.models import Sequential\nfrom keras.engine.topology import Layer\nfrom keras.layers import Dense, CuDNNGRU, Dropout, LSTM, CuDNNLSTM, Bidirectional, BatchNormalization\nfrom keras.optimizers import adam, RMSprop\nfrom keras import initializers, regularizers, constraints\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\n\nfrom tensorflow import set_random_seed\nset_random_seed(1127)","1d8765af":"# The LSTM architecture\nmodel = Sequential()\n\n''' \nLSTM based RNN (GPU)\n'''\n# # First RNN layer\n# model.add(CuDNNLSTM(units=96, return_sequences=True, input_shape=(None,n_features)))\n# model.add(Dropout(0.2))\n\n# # Second LSTM layer\n# model.add(CuDNNLSTM(units=48, return_sequences=True))\n# model.add(Dropout(0.2))\n\n# # Third LSTM layer\n# model.add(CuDNNLSTM(units=48, return_sequences=True))\n# model.add(Dropout(0.2))\n\n# # Fourth LSTM layer\n# model.add(CuDNNLSTM(units=48))\n\n# model.add(Dense(units=n_targets))\n\n\n''' \nGRU based RNN (GPU)\n'''\n# First RNN layer\nmodel.add(CuDNNGRU(units=50, return_sequences=True, input_shape=(None,n_features)))\nmodel.add(Dropout(0.2))\n\n# Second LSTM layer\nmodel.add(CuDNNGRU(units=50, return_sequences=True))\nmodel.add(Dropout(0.2))\n\n# Third LSTM layer\nmodel.add(CuDNNGRU(units=50, return_sequences=True))\nmodel.add(Dropout(0.2))\n\n# Fourth LSTM layer\nmodel.add(CuDNNGRU(units=50))\n# model.add(Dropout(0.2))\n\n\n# The output layer\nmodel.add(Dense(units=n_targets))\n\n\nmodel.summary()","942ce34d":"# Compile and fit model\n\ncb = [ModelCheckpoint(\"model.hdf5\", monitor='val_mean_absolute_error',\n                      save_best_only=True, period=3)]\n\nmodel.compile(optimizer = 'rmsprop',\n              loss = 'logcosh',\n              metrics = ['mae'])\n\n# model.compile(optimizer = 'rmsprop',\n#               loss = 'mae')\n\n\nhistory = model.fit_generator(train_gen,\n                              steps_per_epoch=1000,\n                              epochs=50,\n                              verbose=2,\n                              callbacks=cb,\n                              validation_data=valid_gen,\n                              validation_steps=200\n                             )","2d472d19":"# Visualize accuracies\n\nloss = history.history['mean_absolute_error']\nval_loss = history.history['val_mean_absolute_error']\n\n# loss = history.history['loss']\n# val_loss = history.history['val_loss']\nepochs = np.asarray(history.epoch) + 1\n    \nplt.plot(epochs, loss, 'bo', label = \"Training MAE\")\nplt.plot(epochs, val_loss, 'b*', label = \"Validation MAE\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.legend();","c9f3cbc8":"rows = 150000\nnum_segments = int(np.floor(train.shape[0] \/ rows))\n\ny_tr = np.zeros(num_segments)\ny_tr_pred = np.zeros((num_segments,n_targets))\n\nfor i in tqdm_notebook(range(num_segments)):\n    x = train[i*rows : i*rows+rows, 0]\n    y_tr[i] = train[i*rows+rows-1, 1]\n    y_tr_pred[i] = model.predict(np.expand_dims(create_X(x), 0))[0]","603751cb":"print(\"The training MAE is {:.7}.\".format(np.abs(y_tr_pred[:,0] - y_tr).mean()))","95ab940f":"if n_targets>2:\n    time = np.arange(n_targets).reshape(-1,1)\n    y_tr_extrapolated = np.zeros(num_segments)\n\n    for i in tqdm_notebook(range(num_segments)):\n        clf = Ridge(alpha=0.1)\n        clf.fit(time, y_tr_pred[i])\n        if clf.coef_>0:\n            y_tr_extrapolated[i]  = clf.predict(time[:3]).mean()      \n        else:\n            y_tr_extrapolated[i] = y_tr_pred[i,:3].mean()\n    print(\"The training MAE for extrapolation is {:.7}.\"\\\n          .format(np.abs(y_tr_extrapolated - y_tr).mean()))\n    \n    plt.figure(figsize=(18, 6))\n    plt.plot(y_tr, color='g', label='time_to_failure', linewidth = 2)\n    plt.plot(y_tr_extrapolated, color='b', label='RNN extrapolated prediction')\n    plt.legend(loc='best');\n    plt.title('RNN prediction vs ttf');\nelse:\n    plt.figure(figsize=(18, 6))\n    plt.plot(y_tr, color='g', label='time_to_failure', linewidth = 2)\n    plt.plot(y_tr_pred, color='b', label='RNN predictions')\n    plt.legend(loc='best');\n    plt.title('RNN prediction vs ttf');","d29d3f28":"# Load submission file\nsubmission = pd.read_csv('..\/input\/sample_submission.csv', index_col='seg_id', dtype={\"time_to_failure\": np.float32})\nx = None\ny_test_pred = np.zeros((len(submission),n_targets))\n\n\n# Load each test data, create the feature matrix, get numeric prediction\nfor i, seg_id in enumerate(tqdm_notebook(submission.index)):\n  #  print(i)\n    seg = pd.read_csv('..\/input\/test\/' + seg_id + '.csv')\n    x = seg['acoustic_data'].values[:]\n    y_test_pred[i] = model.predict(np.expand_dims(create_X(x), 0))[0]\n    if n_targets > 2:\n        clf = Ridge(alpha=0.1)\n        clf.fit(time, y_test_pred[i].reshape(-1,1))\n        if clf.coef_>0:\n            y_pred_extrapolated = clf.predict(time[:n_targets-2]).mean()      \n        else:\n            y_pred_extrapolated = y_test_pred[i,:n_targets-2].mean()\n        \n        submission.time_to_failure[i] = y_pred_extrapolated\n    else:\n        submission.time_to_failure[i] = y_test_pred[i]\n\n# Save\nsubmission.to_csv('submission_lstm_extrapolated.csv')","da309bab":"pd.DataFrame(y_tr_pred).to_csv('y_tr_pred.csv',index=False)\npd.DataFrame(y_test_pred).to_csv('y_test_pred.csv',index=False)","80ee701e":"# RNN Model","cfb2cde8":"# Data generator\nThe generator endlessly selects `batch_size` ending positions of sub-time series. For each ending position, the `time_to_failure` serves as target, while the features are created by the function `create_X`.","9ef3d635":"# Predictions","814bfcfa":"# Summary\n\n* Adding multiple target values spanning multiple segments.\n\n# Reference\n* [RNN starter for huge time series](https:\/\/www.kaggle.com\/mayer79\/rnn-starter-for-huge-time-series) \n* [RNN starter notebook](https:\/\/www.kaggle.com\/devilears\/rnn-starter-kernel-with-notebook)\n* [Intro to RNN with LSTM and GRU](https:\/\/www.kaggle.com\/thebrownviking20\/intro-to-recurrent-neural-networks-lstm-gru)\n* [Wavelet denoising](https:\/\/www.kaggle.com\/tarunpaparaju\/lanl-earthquake-prediction-signal-denoising)","72979b35":"### Some global variables","bc161754":"# Feature generation\n\nFor a given ending position \"last_index\", we split the last 150000 values \nof \"x\" into 150 pieces of length 1000 each. So n_steps * step_length should equal 150000.\nFrom each piece, a set features are extracted. This results in a feature matrix of dimension (150 time steps x features).  "}}