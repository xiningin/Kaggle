{"cell_type":{"77ee2e16":"code","ca615af9":"code","54ca8192":"code","b4d4c04f":"code","0a315f81":"code","7403c2c0":"code","a7bc0592":"code","2f5ee210":"code","a68bb5b3":"code","8e8c38d5":"code","c45a9166":"code","45653c9f":"code","8dbd47d7":"code","4ee5d2fe":"code","cff83a60":"code","3bc6553f":"code","0574fc6c":"code","0f7ab03b":"code","eb7c6459":"markdown","611f48a4":"markdown","f732a8a1":"markdown","33cb7518":"markdown","c13e4d30":"markdown","6a629b80":"markdown","913b1c34":"markdown","e8437acc":"markdown","b3bb4ed6":"markdown","a2f68cf7":"markdown","44a82d4f":"markdown","888f3a70":"markdown","f0c8281c":"markdown","dc67e677":"markdown","32c986cd":"markdown"},"source":{"77ee2e16":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn.preprocessing as pp\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, Birch\nfrom sklearn.metrics import silhouette_score\nimport seaborn as sns\nimport scipy.cluster.hierarchy as shc\nimport nbformat\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ca615af9":"#import data\nraw_data = pd.read_csv(\"\/kaggle\/input\/ccdata\/CC GENERAL.csv\")","54ca8192":"#explore and prep data\nprint(raw_data.shape) # rows and columns\nraw_data.head() # first 5 records","b4d4c04f":"#fill null\/na values with mean values for minimum payments\nraw_data['MINIMUM_PAYMENTS'].fillna(value=raw_data['MINIMUM_PAYMENTS'].mean(), inplace = True)\n\n#fill null\/na values with mean values for credit limit\nraw_data['CREDIT_LIMIT'].fillna(value=raw_data['CREDIT_LIMIT'].mean(), inplace = True)\n\n#count null values in columns \n#fill null values with mean values for minimum \nraw_data.isnull().sum().sort_values(ascending=False)\n\n# drop cust_id from raw data\nraw_data = raw_data.drop('CUST_ID', axis = 1) ","0a315f81":"# describe data\nraw_data.describe()","7403c2c0":"#Low Variance Filter\nvar = raw_data.var() # Variance \ncols = raw_data.columns #columns \nvariable1 = [ ] \nfor i in range(0,len(var)):\n    if var[i]>=10:   #setting the threshold as 10%\n       variable1.append(cols[i+1])\n    \nprint(variable1)\n\nfiltered_data = raw_data[['BALANCE_FREQUENCY', 'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE', 'PURCHASES_FREQUENCY', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS', 'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT']].copy()\n\nfiltered_data.info()","a7bc0592":"figure = plt.figure(figsize=(16, 9))\nraw_PURCHASES = figure.add_subplot(2,4,1) \nlog_PURCHASES = figure.add_subplot(2,4,2) #silhouette bar chart\nraw_BALANCE = figure.add_subplot(2,4,3) #silhouette bar chart\nlog_BALANCE = figure.add_subplot(2,4,4) #silhouette bar chart\nraw_CREDIT_LIMIT = figure.add_subplot(2,4,5) #silhouette bar chart\nlog_CREDIT_LIMIT = figure.add_subplot(2,4,6) #silhouette bar chart\nraw_MINIMUM_PAYMENTS = figure.add_subplot(2,4,7) #silhouette bar chart\nlog_MINIMUM_PAYMENTS = figure.add_subplot(2,4,8) #silhouette bar chart\n\nsns.distplot(raw_data['PURCHASES'], axlabel='Purchases Raw Data', ax=raw_PURCHASES)\n\npurchases_log=np.log(1 + raw_data['PURCHASES'])\nsns.distplot(purchases_log, axlabel='Purchases Log Data', ax=log_PURCHASES)\n\nsns.distplot(raw_data['BALANCE'], axlabel='Balance Raw Data', ax=raw_BALANCE)\n\npurchases_log=np.log(1 + raw_data['BALANCE'])\nsns.distplot(purchases_log, axlabel='Balance Log Data', ax=log_BALANCE)\n\nsns.distplot(raw_data['CREDIT_LIMIT'], axlabel='Credit Limit Raw Data', ax=raw_CREDIT_LIMIT)\n\npurchases_log=np.log(1 + raw_data['CREDIT_LIMIT'])\nsns.distplot(purchases_log, axlabel='Credit Limit Log Data', ax=log_CREDIT_LIMIT)\n\nsns.distplot(raw_data['MINIMUM_PAYMENTS'], axlabel='Minimum Payments Raw Data', ax=raw_MINIMUM_PAYMENTS)\n\npurchases_log=np.log(1 + raw_data['MINIMUM_PAYMENTS'])\nsns.distplot(purchases_log, axlabel='Minimum Payments Log Data', ax=log_MINIMUM_PAYMENTS)\n\nfigure.tight_layout()","2f5ee210":"# Log-transformation\n\ncols =  ['BALANCE',\n         'PURCHASES',\n         'ONEOFF_PURCHASES',\n         'INSTALLMENTS_PURCHASES',\n         'CASH_ADVANCE',\n         'CASH_ADVANCE_TRX',\n         'PURCHASES_TRX',\n         'CREDIT_LIMIT',\n         'PAYMENTS',\n         'MINIMUM_PAYMENTS',\n        ]\n\n# Note: Adding 1 for each value to avoid inf values\nraw_data[cols] = np.log(1 + raw_data[cols])\n\nraw_data.head()","a68bb5b3":"# Standardize data\nscaler = pp.StandardScaler() \nscaled_df = scaler.fit_transform(raw_data) \n  \n# Normalizing the Data \nnormalized_df = pp.normalize(scaled_df) \n  \n# Converting the numpy array into a pandas DataFrame \nnormalized_df = pd.DataFrame(normalized_df) \n\nnormalized_df.describe()","8e8c38d5":"figure = plt.figure(figsize=(16, 9))\nelbow = figure.add_subplot(1,2,1) #elbow chart\nkmean_sil = figure.add_subplot(1,2,2) #silhouette bar chart\n\nn_clusters=10\ncost=[]\nfor i in range(1,n_clusters):\n    kmean= KMeans(i)\n    kmean.fit(normalized_df)\n    cost.append(kmean.inertia_) \n    elbow.set_ylabel('Sum of Squared Errors', fontsize = 15)\n    elbow.set_xlabel('Number of Clusters', fontsize = 15)\n    elbow.set_title('K-MEANS Clustering SSE: Elbow Chart', fontsize = 15)\n    elbow.plot(cost, 'bx-')\n\n\nsilhouette_scores = [] \n\nfor n_cluster in range(2, 8):\n    silhouette_scores.append( \n        silhouette_score(normalized_df, KMeans(n_clusters = n_cluster).fit_predict(normalized_df))) \n    \n# Plotting a bar graph to compare the results \nk = [2, 3, 4, 5, 6,7] \nkmean_sil.bar(k, silhouette_scores) \nkmean_sil.set_title('K-MEANS: Number of Clusters vs. Silhouette Score', fontsize = 15)\nkmean_sil.set_xlabel('Number of Clusters', fontsize = 15) \nkmean_sil.set_ylabel('Silhouette Score', fontsize = 15) ","c45a9166":"kmean= KMeans(3)\nkmean.fit(normalized_df)\nlabels=kmean.labels_\nclusters=pd.concat([raw_data, pd.DataFrame({'cluster':labels})], axis=1)","45653c9f":"datamart_melt = pd.melt(clusters.reset_index(),\nid_vars=['cluster'],\nvalue_vars=['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n            'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', \n            'CASH_ADVANCE', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n            'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', \n            'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS', \n            'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE'],\nvar_name='ATTRIBUTES',\nvalue_name='VALUES')\n\nplt.figure(figsize=(24, 9))\nchart = sns.lineplot(x=\"ATTRIBUTES\", y=\"VALUES\", hue='cluster', data=datamart_melt)\nplt.title('Snake Plot of Standardized Attributes')\nplt.xticks(rotation=75)\nplt.legend(loc='upper left', fontsize='large')\n\nfigure.tight_layout()","8dbd47d7":"cluster_avg = clusters.groupby(['cluster']).mean()\npopulation_avg = clusters.mean()\npopulation_avg = population_avg.drop('cluster')\nrelative_imp = cluster_avg\/population_avg\nrelative_imp.sort_values(by=['cluster'],inplace=True)\nrelative_imp.round(2).unstack()\n\nplt.figure(figsize=(20, 8))\nplt.title('Relative Importance of Attributes')\nsns.heatmap(data=relative_imp, annot=True, fmt='.2f', cmap='RdYlGn', yticklabels='auto')\nplt.ylim(0,3)\nplt.xlabel('Features')\nplt.ylabel('Clusters')\nfigure.tight_layout()\nplt.savefig('kmeans_heat.png')\nplt.show()","4ee5d2fe":"for c in clusters:\n    grid= sns.FacetGrid(clusters, col='cluster')\n    grid.map(plt.hist, c)    \n    plt.show()","cff83a60":"figure = plt.figure(figsize=(21, 7))\nkmean_sil = figure.add_subplot(1,3,1) #kmean silhouette bar chart\nbirch_sil = figure.add_subplot(1,3,2) #birch chart\nagg_sil = figure.add_subplot(1,3,3) #AgglomerativeClustering chart\n\nkmean_silhouette_scores = [] \nbirch_silhouette_scores = [] \nagg_silhouette_scores = [] \n\nfor n_cluster in range(2, 8):\n    kmean_silhouette_scores.append( \n        silhouette_score(normalized_df, KMeans(n_clusters = n_cluster).fit_predict(normalized_df))) \n    \n    birch_silhouette_scores.append( \n        silhouette_score(normalized_df, Birch(n_clusters = n_cluster).fit_predict(normalized_df))) \n        \n    agg_silhouette_scores.append( \n        silhouette_score(normalized_df, AgglomerativeClustering(n_clusters = n_cluster).fit_predict(normalized_df))) \n    \n# Plotting a bar graph to compare the results \nk = [2, 3, 4, 5, 6,7] \nkmean_sil.bar(k, kmean_silhouette_scores) \nkmean_sil.set_title('K-MEANS: Number of Clusters vs. Silhouette Score', fontsize = 10)\nkmean_sil.set_xlabel('Number of Clusters', fontsize = 20) \nkmean_sil.set_ylabel('Silhouette Score', fontsize = 20) \n\nbirch_sil.bar(k, birch_silhouette_scores) \nbirch_sil.set_title('BIRCH: Number of Clusters vs. Silhouette Score', fontsize = 10)\nbirch_sil.set_xlabel('Number of Clusters', fontsize = 20) \nbirch_sil.set_ylabel('Silhouette Score', fontsize = 20) \n  \nagg_sil.bar(k, agg_silhouette_scores) \nagg_sil.set_title('Agglomerative: Number of Clusters vs. Silhouette Score', fontsize = 10)\nagg_sil.set_xlabel('Number of Clusters', fontsize = 20) \nagg_sil.set_ylabel('Silhouette Score', fontsize = 20)\n\n\nfigure.tight_layout()","3bc6553f":"birch_cluster= Birch(n_clusters=3)\nbirch_cluster.fit(normalized_df)\nlabels=birch_cluster.labels_\n\nbirch_clusters=pd.concat([raw_data, pd.DataFrame({'cluster':labels})], axis=1)\nbirch_clusters.head()","0574fc6c":"# Prep data for snake plot\ndatamart_melt = pd.melt(birch_clusters.reset_index(),\nid_vars=['cluster'],\nvalue_vars=['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n            'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', \n            'CASH_ADVANCE', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n            'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', \n            'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS', \n            'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE'],\nvar_name='ATTRIBUTES',\nvalue_name='VALUES')\n\n#snakeplot\nplt.figure(figsize=(24, 9))\nchart = sns.lineplot(x=\"ATTRIBUTES\", y=\"VALUES\", hue='cluster', data=datamart_melt)\nplt.title('BIRCH: Snake Plot of Standardized Attributes')\nplt.xticks(rotation=75)\nplt.legend(loc='upper left', fontsize='large')\n\nfigure.tight_layout()\n\n## Relative Importance Heatmap\nplt.figure(figsize=(20, 8))\nplt.title('BIRCH: Relative Importance of Attributes')\nsns.heatmap(data=relative_imp, annot=True, fmt='.2f', cmap='RdYlGn', yticklabels='auto')\nplt.ylim(0,3)\nplt.xlabel('Features')\nplt.ylabel('Clusters')\nfigure.tight_layout()\nplt.savefig('kmeans_heat.png')\nplt.show()","0f7ab03b":"# model fit with 3 clusters\nagg_cluster= AgglomerativeClustering(n_clusters=3)\nagg_cluster.fit(normalized_df)\nagg_clusters=pd.concat([raw_data, pd.DataFrame({'cluster':labels})], axis=1)\n\n# data prep for snake plot\ndatamart_melt = pd.melt(agg_clusters.reset_index(),\nid_vars=['cluster'],\nvalue_vars=['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n            'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', \n            'CASH_ADVANCE', 'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n            'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', \n            'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS', \n            'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE'],\nvar_name='ATTRIBUTES',\nvalue_name='VALUES')\n\n# Snakeplot\nplt.figure(figsize=(24, 9))\nchart = sns.lineplot(x=\"ATTRIBUTES\", y=\"VALUES\", hue='cluster', data=datamart_melt)\nplt.title('Agglomerative: Snake Plot of Standardized Attributes')\nplt.xticks(rotation=75)\nplt.legend(loc='upper left', fontsize='large')\n\nfigure.tight_layout()\n\n# relative importance heatmap\nplt.figure(figsize=(20, 8))\nplt.title('Agglomerative: Relative Importance of Attributes')\nsns.heatmap(data=relative_imp, annot=True, fmt='.2f', cmap='RdYlGn', yticklabels='auto')\nplt.ylim(0,3)\nplt.xlabel('Features')\nplt.ylabel('Clusters')\nfigure.tight_layout()\nplt.savefig('Agglomerative_heat.png')\nplt.show()","eb7c6459":"## Compared to K-Means: BIRCH & Agglomerative Clustering\n\nAs part of my analysis, I also compared the k-means approach and its handling of the data with BIRCH and Agglomerative Clustering approaches. \n\nBIRCH (balanced iterative reducing and clustering using hierarchies) is an unsupervised data mining algorithm used to perform hierarchical clustering over particularly large data-sets.[1] An advantage of BIRCH is its ability to incrementally and dynamically cluster incoming, multi-dimensional metric data points in an attempt to produce the best quality clustering for a given set of resources (memory and time constraints). In most cases, BIRCH only requires a single scan of the database. [9]\n\nAgglomerative clustering is another form of hierarchical clustering. This approach is a form of \u201cbottom up\u201d clustering \u201c\u2026each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.[10] \n\nAll of the approaches seem to have a clear preference for 2 clusters. Based on the analysis of the K-Means results, I could see those two segments representing the extremes based (in a general sense, not the actual features) on the total spend in association with purchase frequency,  cash advances and the number and the amount of money spent on one-off purchases.\n \nWhere the results get a little more interesting is that the K-Means and BIRCH have a strong preference for several more clusters, 6 and 7 respectfully. If I were looking for more considerable variation and possibly looking for niche customers segments that are not as obvious, pursuing the analysis of 6 to 7 clusters might provide that insight.\n\nPer the snake plots it is very easy to see that all three approaches had a similar outcome and treatment of the data. The clusters are not mapped one to one but the visual is pretty telling.","611f48a4":"# Clustering Approaches\n\nCluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data mining, and a common technique for statistical data analysis, used in many fields, including machine learning, pattern recognition, image analysis, information retrieval, bioinformatics, data compression, and computer graphics[5].\n\nFor this data set and this analysis, we are attempting to cluster credit card data to identify customer segments based on their credit card transaction history. \n \nThe first approach in attempting to cluster the data is the K-Means clustering approach complete with an interpretation of the data clusters. Following that analysis is a general comparison of how the K Means, Birch, and Agglomerative clustering approaches differ in their treatment of the data.","f732a8a1":"Many of the features have values that are either 1 or 0. However the features that deal with dollar figures vary quite a bit. I chose to log transform these values to reduce the scale into a normal distribution. It also helps with scaling and grouping the data when analyzing the clusters. Having the effect of grouping into a discernable range.\n\nThe final step in preparing the data was transforming the data with the sklearn.preprocessing.StandardScaler and sklearn.preprocessing.Normalizing. ","33cb7518":"## K-Means Interpretation\n\nBased on the data I would classify the three customer segments as follows:\n\nCluster 0: Uses their credit card frequently for daily purchases and whatever catches their eye. Their credit card is used for daily spending with the accumulation of a balance they start paying on quickly but may be working on paying off that debt in installments. They carry an average to above average credit limit. They don\u2019t use cash advances. \n\nCluster 1: Is the reluctant credit card user with the preference for cash advances when necessary. Maybe using cash to get out of a jam as they only transact very infrequently with only one-off purchases via cash advance. They will then primarily only pay the minimum payments towards paying down their debt. \n\nCluster 3: Everyday credit card user for daily items. This segment of users their credit much less often than cluster 0 and does not use cash advances. Their credit limit leans more average than high and they lead the pack in minimum payments. ","c13e4d30":"# Introduction\nThis document contains a cursory look at clustering techniques. In this document, I attempt to use various clustering techniques to identify customer \u201cclusters\u201d in consumer credit card data while observing how each technique handles the data set and how the results may be similar or dissimilar. This document provides a walk-thru of the data treatment and methods used to discern the number of clusters. The purpose of this analysis is to identify different customer personas in the credit card data.\n\nIn this document, I walk through an analysis of the data with a K-Means clustering approach complete with an interpretation of the data clusters. Following that analysis is a general comparison of how the K Means, Birch, and Agglomerative clustering approaches differ in their treatment of the data.\n\nThe data for this document is a public dataset of consumer credit card data published on Kaggle [1]. The dataset is accompanied by several user-submitted kernels that provide some starting points and insight for this analysis [2]. For this document, I have noted the most useful kernels in the references section of this document. My work differs in that it combines several approaches not contained in a single kernel. My treatment of the data and analysis of the results also varies significantly from any single kernel published on Kaggle. My analysis does not align with other approaches. My steps to prepare the data, identify the number of clusters and interpretations of the results will, in some cases, differ significantly. I also provide details concerning my data preparation steps not shared in the public kernels. Additionally, my treatment of the data provides a direct comparison of the output of the various clustering approaches.","6a629b80":"# Data Preperation\n\nColumns from this point will be referenced as features. The data contained 314 null values that were imputed with mean values from their respective columns. MIMIMUM_PAYMENTS had 313 null values. Null values imputed with 864.21. CREDIT_LIMIT had 1 null value and the null value was imputed with 4494.45.\n\nFor this analysis I chose to use all of the features minus the CUST_ID feature. The CUST_ID is specific to an individual and does not help with this analysis. However, the CUST_ID could provide a connection to more demographic information that could help create better clusters and better customer segmentation.\n\nI did explore trying to reduce the number of features by removing all features with low variance and features with a high level of correlation. The approach did reduce 7 additional features, but the reduction of the features made interpreting the clusters less intuitive in the end; or at least harder to express in terms of customer segments.","913b1c34":"The relative importance of each feature is calculated as the mean of the clusters divided by the mean of the population mean minus 1. This calculation gives you a number relative to the cluster mean. The further the values is from 0 the more significant that feature is in relation to the total population [5]","e8437acc":"### References\n\n1. https:\/\/www.kaggle.com\/arjunbhasin2013\/ccdata \n2. https:\/\/www.kaggle.com\/arjunbhasin2013\/ccdata\/kernels \n3. https:\/\/www.edupristine.com\/blog\/k-means-algorithm \n4. https:\/\/en.wikipedia.org\/wiki\/BIRCH \n5. https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html \n6. https:\/\/inseaddataanalytics.github.io\/INSEADAnalytics\/CourseSessions\/Sessions45\/ClusterAnalysisReading.html\n7. https:\/\/en.wikipedia.org\/wiki\/Hierarchical_clustering","b3bb4ed6":"The elbow chart appears to recommend somewhere between 3 and 5 potential clusters. The silhouette score for 2 clusters is the highest with the rest of the scores from 3 to 7 clusters being very close to equal. \n\nIn the effort to pick the ideal number of clusters, we can use the mathematical approaches outlined in the charts. Additionally, in making this decision having expertise with data and the domain it represents is very beneficial in the analysis process. In this case, I would like to see some variety without trying to spread the analysis too thin. I am interested in more general groupings than I am in finding niche groups in the fictional customer segment. For this analysis and the rest of this effort, I did my work with 3 clusters.\n\nThe number of features associated with this data makes seeing the data very difficult. After trying several methods, I landed on snake plots and heatmaps as the most useful visuals to compare the clusters holistically.","a2f68cf7":"## K-Means Clustering\nk-means clustering is a method of vector quantization, originally from signal processing, that is popular for cluster analysis in data mining. k-means clustering aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.[6]\n\nTo identify a general estimate of the number of clusters to consider for the k-means approach, I created an elbow chart using the Elbow method. The Elbow method is a heuristic method of interpretation and validation of consistency within cluster analysis designed to help finding the appropriate number of clusters in a dataset. It is often ambiguous and not very reliable, and hence other approaches for determining the number of clusters such as the Silhouette method are preferable. [7]\n\nI then evaluated the silhouette scores for a range of cluster options based on the elbow chart; figure 1. The silhouette measures how similar a point is in the cluster compared to other clusters in a range of -1 to 1. [8]","44a82d4f":"## BIRCH Clustering","888f3a70":"# Abstract\nClustering is an effective technique used to find logical groupings in data. For this paper I will explore at length the k-means clustering approach in an effort to identify customer segments in credit card data. I will then use the BIRCH & Agglomerative clustering approaches to compare their treatment of the data with the k-means treatment of the data. I will then close with condensed version of some lessons learned while putting this paper together.","f0c8281c":"# Data Preperation for K-Means Clustering\n\nFor K-Means clustering, the algorithm works best with variables with normalized data with the same variance and standard deviations. As an initial check, I investigated the skewness of a subset of my variables. I found that most (if not all) of the features not normalized between 0 and 1 are skewed.\n\nBased on my analysis of skewness, I then log-transformed the data. I then evaluated the mean and standard deviations of my data. The data showed that I still needed to center my data. To center my data, I used klearn.preprocessing.StandardScaler on my data. This process has the effect of subtracting the mean values from each observation in each feature[3].\n\nThe final step was to normalize my data. Normalizing my data had the impact of keeping my means and std the uniform across the data set but scales my data to have unit norm [4].\n\nI did not explicitly address outliers in my data set.\n\nI also chose to re-use this dataset as prepared for the other clustering techniques applied to this data for consistency. I also assume that since these approaches are all measured clustering approaches that the data preparation should transfer. In a real-world application, my assumption would need to be validated.","dc67e677":"# Data\nThe consumer credit card data consists of 8950 rows with 18 columns. ","32c986cd":"## Agglomerative Clustering"}}