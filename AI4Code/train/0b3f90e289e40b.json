{"cell_type":{"1222ff87":"code","c685a192":"code","6d45007a":"code","686630e5":"code","3cf69b48":"code","15dde67c":"code","53f001ee":"code","b52347b8":"code","8ac77618":"code","3471a64f":"code","250cf06d":"code","acfeef81":"code","b7435aee":"code","6695777b":"code","fb15ef70":"code","acd0e3ce":"code","f98230f0":"code","8d50718d":"code","c07434a9":"code","c17365c1":"code","d81afd00":"code","32d152ad":"code","9e55770e":"code","76d30d7a":"code","5cf999a8":"code","b96705ca":"code","83cc3e24":"code","525ea119":"code","f6b0c2ae":"code","cfcf1ddd":"code","8cda91ad":"code","e666f812":"code","4a2505a0":"code","672494a7":"code","039f1b5d":"code","d6a8f496":"code","46d6f38f":"code","dc0951e1":"code","62a75a46":"code","d81a0591":"code","296ced99":"code","fc07470f":"code","87b6b65e":"code","dc227753":"code","eaec25fe":"code","8e1b9b4d":"code","72931856":"code","95697a45":"code","b7a99b7a":"code","39f78f40":"code","f0802a57":"code","4b15e1c2":"code","acbc29a4":"code","592fa073":"code","f4be98d4":"code","08fa43c4":"code","a758ef03":"code","4d0219df":"code","519bd7fc":"code","a5819fc5":"code","4b476abb":"code","31f90f27":"code","26388cfc":"code","7bf2ab3e":"code","34226124":"code","11165187":"code","12159ae8":"code","a4a4d30d":"code","eb079224":"code","a02d4742":"code","a0998402":"code","4d9231ce":"code","9cfd3962":"code","d52f8007":"code","5aca1a16":"code","31e35fea":"code","dcdc529c":"code","848605fc":"code","50164d6b":"code","4a73efcf":"code","b64d0c9f":"code","6bff54df":"code","99928301":"code","e2e94e24":"code","e74a928a":"code","64586c1b":"code","258ec8b1":"code","7da75179":"code","6e15a041":"code","bd372947":"code","ee37bc1e":"markdown","34a26513":"markdown","432e8fcc":"markdown","879036c4":"markdown","b84c52df":"markdown","5116b687":"markdown","43e27a82":"markdown","a88ea610":"markdown","f4e69858":"markdown","bc8a6dbf":"markdown","bf117cc0":"markdown","cb19102c":"markdown","ecec80ec":"markdown","ec6ee109":"markdown","64da37c3":"markdown","ea105d7c":"markdown","28577e01":"markdown","ee284a11":"markdown","1744a813":"markdown","c5c2171e":"markdown","55f10d30":"markdown","ffcbaebe":"markdown","37bdc701":"markdown","03c86ccb":"markdown","983637f4":"markdown","d6967b6c":"markdown","9c304396":"markdown","7e5aee0e":"markdown","bcfc3380":"markdown","2669884a":"markdown","b1f7727c":"markdown","fb45c0a9":"markdown","f5436e1c":"markdown","c9b38c28":"markdown","8f101285":"markdown","84dcaca7":"markdown","2542440d":"markdown","045edc8d":"markdown","912ed3b1":"markdown","3f717052":"markdown","7803d101":"markdown","cf2bfdec":"markdown","ee56754b":"markdown","c5430b0a":"markdown","f83c44d2":"markdown","d26f5197":"markdown","7051e633":"markdown","b9d22a79":"markdown","1414cafb":"markdown","0cb67f44":"markdown","1a94bc92":"markdown","203ad02c":"markdown","5521c39b":"markdown","260dd56f":"markdown","4898b67a":"markdown","e1738716":"markdown","3c7ba46c":"markdown","386a79d4":"markdown","f05be067":"markdown","c0c3d1ab":"markdown","22ffeb65":"markdown","68a549c2":"markdown","dd2b7fdd":"markdown","a72d7a96":"markdown","9af35200":"markdown","4f02894c":"markdown","33c516f1":"markdown","60a54494":"markdown","12622b95":"markdown","71b0c151":"markdown","6d3c486d":"markdown","744906e3":"markdown","8270e763":"markdown","89fc229f":"markdown","278b7983":"markdown","502356ad":"markdown","7671d1a7":"markdown","e32333f5":"markdown","08c71f35":"markdown","536521c9":"markdown","11802674":"markdown","d6f91789":"markdown","746b13b0":"markdown","2d1594e9":"markdown","8800f004":"markdown","b907fc9e":"markdown","c352c0fb":"markdown","5c3578bb":"markdown","307966ee":"markdown","17778b29":"markdown","67b7a368":"markdown","7f8fa622":"markdown","8f17e4a4":"markdown","7e9ba4a8":"markdown","22f2a46a":"markdown","b1dac02e":"markdown","e8af964f":"markdown","ee151731":"markdown","91b98193":"markdown","4862a871":"markdown","9034c4bf":"markdown","de2974ff":"markdown","9cc151ad":"markdown","99d5b3b4":"markdown","51a8f5b1":"markdown","3272b6e4":"markdown"},"source":{"1222ff87":"\"\"\" Data Analysis Packages\"\"\"\nimport numpy as np\nimport pandas as pd\nimport math\nimport scipy.stats as stats\nfrom patsy import dmatrices\nimport warnings\n\n\"\"\"Data Visualisation Packages\"\"\"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.tools import make_subplots\nfrom plotly.offline import iplot, init_notebook_mode\nimport plotly.express as px\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom plotly.graph_objs import *\n\n\"\"\"Pre-Processing Packages\"\"\"\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\n\"\"\"Modeling and Regression Packages\"\"\"\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\n\"\"\"Model Evaluation\"\"\"\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","c685a192":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6d45007a":"#Looking at the Values and the State of the dataset\nwarnings.filterwarnings('ignore')\ndf = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf.head()\n\ntest_df = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","686630e5":"print(df.isnull().sum())","3cf69b48":"f, (ax_box, ax_hist) = plt.subplots(2, sharex=True)\nmean=df['Age'].mean()\nmedian=df['Age'].median()\n#mode=df['Age'].mode().get_values()[0]\n\nsns.boxplot(df[\"Age\"], ax=ax_box)\nax_box.axvline(mean, color='r', linestyle='--')\nax_box.axvline(median, color='g', linestyle='-')\n#ax_box.axvline(mode, color='b', linestyle='-')\n\nsns.distplot(df[\"Age\"], ax=ax_hist)\nax_hist.axvline(mean, color='r', linestyle='--')\nax_hist.axvline(median, color='g', linestyle='-')\n#ax_hist.axvline(mode, color='b', linestyle='-')\n\nplt.legend({'Mean':mean,'Median':median})\n\nax_box.set(xlabel='')\nplt.show()","15dde67c":"print(\"The mean age is \" + str(mean))\nprint(\"The median age is \" + str(median))","53f001ee":"\"\"\"Filling the null rows with the mean value\"\"\"\ndf['Age'] = df['Age'].fillna(float(mean)) \n\ntest_df['Age'] = test_df['Age'].fillna(float(mean)) ","b52347b8":"\"\"\"Checking that all the null value rows are filled\"\"\"\ndf['Age'].isnull().sum()","8ac77618":"df['Embarked'].value_counts()","3471a64f":"#For the Nan values, we assigned them to the highest volume port which is S. Since there's a higher probability of the entry from that port.\ndef embarked_null(x):\n    if isinstance(x, str) == False:\n        return 'S'\n    else:\n        return x\ndf['Embarked'] = df['Embarked'].apply(embarked_null)\n\ntest_df['Embarked'] = test_df['Embarked'].apply(embarked_null)","250cf06d":"#Listing all the columns\ndf.columns","acfeef81":"def survival_status(x):\n    x = int(x)\n    if x == 0:\n        return \"Did not survive\"\n    else:\n        return \"Survive\"\ndf[\"Survived\"] = df[\"Survived\"].apply(survival_status)","b7435aee":"df_Survived_Count = df['Survived'].value_counts().rename_axis('Survived').reset_index(name='Count')\ndf_Survived_Count\n\n#Matplotlib Style\n# plt.bar(df_Survived_Count['Survived'], df_Survived_Count['Count'])\n# plt.title('Frequency counter for the Survivability of Passengers')\n# plt.xlabel('Survivable Status')\n# plt.ylabel('Frequency')\n\n#Plotly Style\nfig = px.bar(df_Survived_Count, x = 'Survived', y = 'Count')\nfig.update_layout(width=800,height=300)\nfig.show()","6695777b":"sns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"Pclass\", data=df).set_title('Frequency Bar Chart for PClass(Ticket Class)')","fb15ef70":"sns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"Sex\", data=df).set_title('Frequency Bar Chart for Sex')","acd0e3ce":"print(\"The number of unique ticket values: \" + str(len(list(df['Ticket'].unique()))))\nprint(\"The total number of tickets were: \" + str(df['Ticket'].count()))","f98230f0":"sns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"Embarked\", data=df).set_title('Frequency Bar Chart for the port of embarkation')","8d50718d":"#Plotting Histogram\ndef plotHistogram(variable):\n    \"\"\"Plots histogram and density plot of a variable.\"\"\"\n    \n    # Create subplot object.\n    fig = make_subplots(\n        rows=2,\n        cols=1,\n        print_grid=False,\n    subplot_titles=(f\"Distribution of {variable.name} with Histogram\", f\"Distribution of {variable.name} with Density Plot\"))\n    \n    # This is a count histogram\n    fig.add_trace(\n        go.Histogram(\n            x = variable,\n            hoverinfo=\"x+y\",\n            marker = dict(color = \"chocolate\")\n        ),\n    row=1,col=1)\n    \n    # This is a density histogram\n    fig.add_trace(\n        go.Histogram(\n            x = variable,\n            hoverinfo=\"x+y\",\n            histnorm = \"density\",\n            marker = dict(color = \"darkred\")\n        ),\n    row=2,col=1)\n    \n    # Update layout\n    fig.layout.update(\n        height=400, \n        width=800,\n        hovermode=\"closest\",\n        showlegend=False,\n        paper_bgcolor=\"rgb(243, 243, 243)\",\n        plot_bgcolor=\"rgb(243, 243, 243)\"\n        )\n    \n    # Update axes\n    fig.layout.yaxis1.update(title=\"<b>Abs Frequency<\/b>\")\n    fig.layout.yaxis2.update(title=\"<b>Density(%)<\/b>\")\n    fig.layout.xaxis2.update(title=f\"<b>{variable.name}<\/b>\")\n    return fig.show()","c07434a9":"df_Age_Count = df['Age'].value_counts().rename_axis('Age').reset_index(name='Count')\ndf_Age_Count\n\n#Matplotlib Style\n# plt.bar(df_Age_Count['Age'], df_Age_Count['Count'])\n# plt.title('Frequency counter for the Survivability by Age')\n# plt.xlabel('Age')\n# plt.ylabel('Frequency')\n\n#Plotly Style\nfig = px.histogram(df_Age_Count, x=\"Age\", y='Count',nbins=50)\nfig.update_layout(width=800,height=300)\nfig.show()","c17365c1":"df_SibSp_Count = df['SibSp'].value_counts().rename_axis('SibSp').reset_index(name='Count')\ndf_SibSp_Count\n#Matplotlib Style\nplt.bar(df_SibSp_Count['SibSp'], df_SibSp_Count['Count'])\nplt.title('Frequency counter for the Survivability by SibSp')\nplt.xlabel('# of siblings\/spouse on board')\nplt.ylabel('Frequency')\n#Plotly Style - Unable to render pdf export therefore using matplotlib\n#plotHistogram(df.Age)\n#plotHistogram(df.SibSp)","d81afd00":"df_Parch_Count = df['Parch'].value_counts().rename_axis('Parch').reset_index(name='Count')\ndf_Parch_Count\n#Matplotlib Style\nplt.bar(df_Parch_Count['Parch'], df_Parch_Count['Count'])\nplt.title('Frequency counter for the Survivability by Parch')\nplt.xlabel('# Parents\/Children on board')\nplt.ylabel('Frequency')\n#Plotly Style - Unable to render pdf export therefore using matplotlib\n#plotHistogram(df.Parch)","32d152ad":"df_Fare_Count = df['Fare'].value_counts().rename_axis('Fare').reset_index(name='Count')\n#Matplotlib Style\n# plt.bar(df_Fare_Count['Fare'], df_Fare_Count['Count'], color = 'c0')\n# plt.title('Frequency counter for the Survivability by Fare')\n# plt.xlabel('Fare')\n# plt.ylabel('Frequency')\n\n#Plotly Style\nfig = px.histogram(df_Fare_Count, x=\"Fare\", y='Count',nbins=100)\nfig.update_layout(width=800,height=300)\nfig.show()","9e55770e":"#Take only the class of the cabin\ndef cabin_class(x):\n    x1 = str(x)\n    x2 = x1[0] \n    x3 = x2.upper()\n    return x3\n    \ndf[\"Cabin\"] = df[\"Cabin\"].apply(cabin_class)\n\ntest_df[\"Cabin\"] = test_df[\"Cabin\"].apply(cabin_class)","76d30d7a":"sns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"Cabin\", data=df).set_title('Frequency Bar Chart for the port of Cabin')","5cf999a8":"df[[\"Name\"]]","b96705ca":"#Performing strip function\ndf['SirName'] = df['Name']\ndf.SirName = df.SirName.str.split(\".\")\ndef get_surname_1(x):\n    return x[0]\ndf.SirName = df.SirName.apply(get_surname_1)\ndf.SirName = df.SirName.str.split(\", \")\ndef get_surname_2(x):\n    return x[1]\ndf.SirName = df.SirName.apply(get_surname_2)\n\n\ntest_df['SirName'] = test_df['Name']\ntest_df.SirName = test_df.SirName.str.split(\".\")\ndef get_surname_1(x):\n    return x[0]\ntest_df.SirName = test_df.SirName.apply(get_surname_1)\ntest_df.SirName = test_df.SirName.str.split(\", \")\ndef get_surname_2(x):\n    return x[1]\ntest_df.SirName = test_df.SirName.apply(get_surname_2)","83cc3e24":"df['SirName'].value_counts()","525ea119":"#Transforming the less frqeuent values\n\"\"\"Create a bucket Officer and put Dr, Rev, Col, Major, Capt titles into it.\"\"\"\ndf['SirName'].replace(to_replace = [\"Dr\", \"Rev\", \"Col\", \"Major\", \"Capt\"], value = \"Officer\", inplace = True,regex=True)\n\n\"\"\"Put Dona, Jonkheer, Countess, Sir, Lady, Don in bucket Aristocrat.\"\"\"\ndf['SirName'].replace(to_replace = [\"Dona\", \"Jonkheer\", \"Countess\", \"Sir\", \"Lady\", \"Don\"], value = \"Aristocrat\", inplace = True,regex=True)\n\n\"\"\"Finally Replace Mlle and Ms with Miss. And Mme with Mrs.\"\"\"\ndf['SirName'].replace({\"Mlle\":\"Miss\", \"Ms\":\"Miss\", \"Mme\":\"Mrs\"}, inplace = True,regex=True)\n\n\"\"\"Replace the Aristocrat with Aristocrat\"\"\"\ndf['SirName'].replace({\"the Aristocrat\":\"Aristocrat\"}, inplace = True,regex=True)\n\ndf['SirName'].value_counts()\n\n\n\n\"\"\"Create a bucket Officer and put Dr, Rev, Col, Major, Capt titles into it.\"\"\"\ntest_df['SirName'].replace(to_replace = [\"Dr\", \"Rev\", \"Col\", \"Major\", \"Capt\"], value = \"Officer\", inplace = True,regex=True)\n\n\"\"\"Put Dona, Jonkheer, Countess, Sir, Lady, Don in bucket Aristocrat.\"\"\"\ntest_df['SirName'].replace(to_replace = [\"Dona\", \"Jonkheer\", \"Countess\", \"Sir\", \"Lady\", \"Don\"], value = \"Aristocrat\", inplace = True,regex=True)\n\n\"\"\"Finally Replace Mlle and Ms with Miss. And Mme with Mrs.\"\"\"\ntest_df['SirName'].replace({\"Mlle\":\"Miss\", \"Ms\":\"Miss\", \"Mme\":\"Mrs\"}, inplace = True,regex=True)\n\n\"\"\"Replace the Aristocrat with Aristocrat\"\"\"\ntest_df['SirName'].replace({\"the Aristocrat\":\"Aristocrat\"}, inplace = True,regex=True)\n\ntest_df['SirName'].value_counts()","f6b0c2ae":"#Function that converts the catergorical variables into ordinal values\ndef social_status_number(x):\n    if x == 'Mr' or x == 'Miss' or x == 'Mrs':\n        return 1\n    elif x == 'Master':\n        return 2\n    elif x == 'Officer':\n        return 3\n    else:\n        return 4\ndf['Social Status Number'] = df['SirName'].apply(social_status_number)\n\ntest_df['Social Status Number'] = test_df['SirName'].apply(social_status_number)","cfcf1ddd":"#We write a function that takes the first index of the string. If it's a number, then it would return a string 'Number', else it would return the first index of the string \ndef ticket_algo(x):\n    x1 = x.split()\n    x2 = x1[0][0]\n    \n    if x2.isdigit() == True:\n        return 'Number'\n    else:\n        return x2\n\ndf['Ticket'] = df['Ticket'].apply(ticket_algo)\n\ntest_df['Ticket'] = test_df['Ticket'].apply(ticket_algo)","8cda91ad":"df['Ticket'].head()","e666f812":"print(df['Ticket'].unique())\nprint('The new number of unique ticket values are ' + str(len(df['Ticket'].unique())))","4a2505a0":"sns.set(style=\"darkgrid\")\nax = sns.countplot(x=\"Ticket\", data=df).set_title('Frequency Bar Chart for the Ticket')","672494a7":"\"\"\"Merge SibSp and Parch to create a variable Family_size.\"\"\"\ndf[\"familySize\"] = df.SibSp + df.Parch + 1  # Adding 1 for single person\n\ntest_df[\"familySize\"] = test_df.SibSp + df.Parch + 1  # Adding 1 for single person\nprint(\"Categories in Family_size:\")\ndisplay(df.familySize.value_counts())","039f1b5d":"print(\"Categories in Family_size:\")\ndisplay(test_df.familySize.value_counts())","d6a8f496":"\"\"\"Create buckets of single, small, medium, and large and then put respective values into them.\"\"\"\ndf.familySize.replace(to_replace = [1], value = \"single\", inplace = True)\ndf.familySize.replace(to_replace = [2,3], value = \"small\", inplace = True)\ndf.familySize.replace(to_replace = [4,5], value = \"medium\", inplace = True)\ndf.familySize.replace(to_replace = [6, 7, 8, 11], value = \"large\", inplace = True)\n\ntest_df.familySize.replace(to_replace = [1], value = \"single\", inplace = True)\ntest_df.familySize.replace(to_replace = [2,3], value = \"small\", inplace = True)\ntest_df.familySize.replace(to_replace = [4,5], value = \"medium\", inplace = True)\ntest_df.familySize.replace(to_replace = [6, 7, 8, 10, 13], value = \"large\", inplace = True)","46d6f38f":"#Matplotlib Style\n# plt.figure(figsize=(16, 2))\n# plt.title('Box plot for Age distribution')\n# plt.xlabel('Age')\n# ax = sns.boxplot(x=df['Age'])\n#Plotly Style - Unable to render pdf export therefore using matplotlib\nfig = go.Figure()\nfig.add_trace(go.Box(x=df['Age']))\nfig.update_layout(height=300, title_text='Box Plot for Age')\nfig.show()","dc0951e1":"#Matplotlib Style\n# plt.figure(figsize=(16, 2))\n# plt.title('Box plot for Fare distribution')\n# plt.xlabel('Fare')\n# ax = sns.boxplot(x=df['Fare'])\n#Plotly Style - Unable to render pdf export therefore using matplotlib\nfig = go.Figure()\nfig.add_trace(go.Box(x=df['Fare']))\nfig.update_layout(height=300, title_text='Box Plot for Fare')\nfig.show()","62a75a46":"#Removing the outliers which has value of 200 dollars and above.\ndf = df[df['Fare']< 265]","d81a0591":"#Remove unecessary columns\ndf1 = df.drop(['Name', 'SirName'], axis=1)\ndf1.head()\n\ntest_df = test_df.drop(['Name', 'SirName'], axis=1)\ntest_df.head()","296ced99":"%%html\n<img src=\"ANOVA Test.png\", width = 500>","fc07470f":"numVariable = df1['Fare']\ncatVariable = df1['Survived']\n#Seperating into the 2 different population dataset\ngroupNumVariableByCatVariable0 = numVariable[catVariable == 'Did not survive']\ngroupNumVariableByCatVariable1 = numVariable[catVariable == 'Survive']\n\nfValue, pValue = stats.f_oneway(groupNumVariableByCatVariable1, groupNumVariableByCatVariable0)\nprint('The F value is: ' + str(round(fValue,5)))\nprint('------------------------')\nprint('The p value is: ' + str(pValue))","87b6b65e":"# Graphical Visualisation\ndf_groupedby_fare = df1[['Fare','Survived']]\ndf_groupedby_fare = df_groupedby_fare.groupby(['Survived']).mean()\ndf_groupedby_fare.reset_index(inplace = True)\n\n#Matplotlib Style\n# plt.bar(df_groupedby_fare['Survived'], df_groupedby_fare['Fare'])\n# plt.title('Frequency counter for the Survivability by Fare')\n# plt.xlabel('Survived')\n# plt.ylabel('Fare')\n\n#Plotly Style\nfig = px.bar(df_groupedby_fare, x='Survived', y='Fare', height=400)\nfig.show()","dc227753":"numVariable = df1['Age']\ncatVariable = df1['Survived']\n#Seperating into the 2 different population dataset\ngroupNumVariableByCatVariable0 = numVariable[catVariable == 'Did not survive']\ngroupNumVariableByCatVariable1 = numVariable[catVariable == 'Survive']\n\nfValue, pValue = stats.f_oneway(groupNumVariableByCatVariable1, groupNumVariableByCatVariable0)\nprint('The F value is: ' + str(round(fValue,5)))\nprint('------------------------')\nprint('The p value is: ' + str(pValue))","eaec25fe":"#Graphical Visualisation\ndf_groupedby_age = df1[['Age','Survived']]\ndf_groupedby_age = df_groupedby_age.groupby(['Survived']).mean()\ndf_groupedby_age.reset_index(inplace = True)\n\n#Matplotlib Style\n# plt.bar(df_groupedby_age['Survived'], df_groupedby_age['Age'])\n# plt.title('Frequency counter for the Survivability by Age')\n# plt.xlabel('Survived')\n# plt.ylabel('Age')\n\n#Plotly Style\nfig = px.bar(df_groupedby_age, x='Survived', y='Age', height=400)\nfig.show()","8e1b9b4d":"%%html\n<img src=\"Chi-Squared Test.png\", width = 500>","72931856":"#We first need to compute the frequency of each unique value of \"Sex\" and \"Survived\" showing up so we use the crosstab function in pandas\nX = df1['Sex']\ny = df1['Survived']\n\narray_by_survived = pd.crosstab(index = X, columns = y)\n\nchi2_stat, p_val, dof, ex = stats.chi2_contingency(array_by_survived)\nprint(\"===Chi2 Stat===\")\nprint(chi2_stat)\nprint('-------------------------')\nprint(\"===Degrees of Freedom===\")\nprint(dof)\nprint('-------------------------')\nprint(\"===P-Value===\")\nprint(p_val)\nprint('-------------------------')\nprint(\"===Contingency Table===\")\nprint(ex)","95697a45":"#Graphical Visualisation\nX = array_by_survived.index\nfig = go.Figure(data=[\n    go.Bar(name='Survived', x = X, y=array_by_survived['Survive']),\n    go.Bar(name='Did not Survived', x = X, y=array_by_survived['Did not survive'])\n])\nfig.update_layout(barmode='stack')\nfig.show()","b7a99b7a":"#We first need to compute the frequency of each unique value of \"Pclass\" and \"Survived\" showing up so we use the crosstab function in pandas\nX = df1['Pclass']\ny = df1['Survived']\n\narray_by_survived = pd.crosstab(index = X, columns = y)\n\nchi2_stat, p_val, dof, ex = stats.chi2_contingency(array_by_survived)\nprint(\"===Chi2 Stat===\")\nprint(chi2_stat)\nprint('-------------------------')\nprint(\"===Degrees of Freedom===\")\nprint(dof)\nprint('-------------------------')\nprint(\"===P-Value===\")\nprint(p_val)\nprint('-------------------------')\nprint(\"===Contingency Table===\")\nprint(ex)","39f78f40":"#Bonferroni Method\ncatVariable = df1['Pclass']\ndef calculateBonferroniAdjusted(catVariable, targetCatVariable=df1['Survived']):\n    catEncoded = pd.get_dummies(catVariable)\n    for column in catEncoded.columns:\n        catGroupedByCatTarget = pd.crosstab(index = catEncoded[column], columns = targetCatVariable)\n        testResult = stats.chi2_contingency(catGroupedByCatTarget)\n        print(f\"Bonferroni-adjusted pvalue between {catVariable.name}({column}) and {targetCatVariable.name}:\")\n        print(f\"{testResult}\\n\")\n        \ncalculateBonferroniAdjusted(catVariable)","f0802a57":"#We first need to compute the frequency of each unique value of \"Embarked\" and \"Survived\" showing up so we use the crosstab function in pandas\nX = df1['Embarked']\ny = df1['Survived']\n\narray_by_survived = pd.crosstab(index = X, columns = y)\n\nchi2_stat, p_val, dof, ex = stats.chi2_contingency(array_by_survived)\nprint(\"===Chi2 Stat===\")\nprint(chi2_stat)\nprint('-------------------------')\nprint(\"===Degrees of Freedom===\")\nprint(dof)\nprint('-------------------------')\nprint(\"===P-Value===\")\nprint(p_val)\nprint('-------------------------')\nprint(\"===Contingency Table===\")\nprint(ex)","4b15e1c2":"#Bonferroni Method\ncatVariable = df1['Embarked']\ndef calculateBonferroniAdjusted(catVariable, targetCatVariable=df1['Survived']):\n    catEncoded = pd.get_dummies(catVariable)\n    for column in catEncoded.columns:\n        catGroupedByCatTarget = pd.crosstab(index = catEncoded[column], columns = targetCatVariable)\n        testResult = stats.chi2_contingency(catGroupedByCatTarget)\n        print(f\"Bonferroni-adjusted pvalue between {catVariable.name}({column}) and {targetCatVariable.name}:\")\n        print(f\"{testResult}\\n\")\n        \ncalculateBonferroniAdjusted(catVariable)","acbc29a4":"#We first need to compute the frequency of each unique value of \"familySize\" and \"Survived\" showing up so we use the crosstab function in pandas\nX = df1['familySize']\ny = df1['Survived']\n\narray_by_survived = pd.crosstab(index = X, columns = y)\n\nchi2_stat, p_val, dof, ex = stats.chi2_contingency(array_by_survived)\nprint(\"===Chi2 Stat===\")\nprint(chi2_stat)\nprint('-------------------------')\nprint(\"===Degrees of Freedom===\")\nprint(dof)\nprint('-------------------------')\nprint(\"===P-Value===\")\nprint(p_val)\nprint('-------------------------')\nprint(\"===Contingency Table===\")\nprint(ex)","592fa073":"#Bonferroni Method\ncatVariable = df1['familySize']\ndef calculateBonferroniAdjusted(catVariable, targetCatVariable=df1['Survived']):\n    catEncoded = pd.get_dummies(catVariable)\n    for column in catEncoded.columns:\n        catGroupedByCatTarget = pd.crosstab(index = catEncoded[column], columns = targetCatVariable)\n        testResult = stats.chi2_contingency(catGroupedByCatTarget)\n        print(f\"Bonferroni-adjusted pvalue between {catVariable.name}({column}) and {targetCatVariable.name}:\")\n        print(f\"{testResult}\\n\")\n        \ncalculateBonferroniAdjusted(catVariable)","f4be98d4":"#We first need to compute the frequency of each unique value of \"Social Status Number\" and \"Survived\" showing up so we use the crosstab function in pandas\nX = df1['Social Status Number']\ny = df1['Survived']\n\narray_by_survived = pd.crosstab(index = X, columns = y)\n\nchi2_stat, p_val, dof, ex = stats.chi2_contingency(array_by_survived)\nprint(\"===Chi2 Stat===\")\nprint(chi2_stat)\nprint('-------------------------')\nprint(\"===Degrees of Freedom===\")\nprint(dof)\nprint('-------------------------')\nprint(\"===P-Value===\")\nprint(p_val)\nprint('-------------------------')\nprint(\"===Contingency Table===\")\nprint(ex)","08fa43c4":"#We first need to compute the frequency of each unique value of \"Ticket\" and \"Survived\" showing up so we use the crosstab function in pandas\nX = df1['Ticket']\ny = df1['Survived']\n\narray_by_survived = pd.crosstab(index = X, columns = y)\n\nchi2_stat, p_val, dof, ex = stats.chi2_contingency(array_by_survived)\nprint(\"===Chi2 Stat===\")\nprint(chi2_stat)\nprint('-------------------------')\nprint(\"===Degrees of Freedom===\")\nprint(dof)\nprint('-------------------------')\nprint(\"===P-Value===\")\nprint(p_val)\nprint('-------------------------')\nprint(\"===Contingency Table===\")\nprint(ex)","a758ef03":"df1.columns","4d0219df":"def multiVariableBarPlot(catVar1, catVar2, catVar3, targetVaraible = df1['Survived']):\n    #Perform crosstab to create dataframe used for plotting\n    catGroupedByCatTarget = pd.crosstab(index = [catVar1, catVar2, catVar3], columns = targetVaraible, normalize='index')*100\n    #Setting the dimensions for the plot\n    fig,ax = plt.subplots(1,1,figsize = (18,3))\n    fontSize = 15\n    #\n    catGroupedByCatTarget.rename({0:\"%Died\", 1:\"%Survived\"}, axis = 1, inplace = True)\n    catGroupedByCatTarget.plot.bar(color = [\"red\", \"green\"],ax=ax)\n    ax.set_xlabel(f\"{catVar1.name},{catVar2.name},{catVar3.name}\", fontsize = fontSize)\n    ax.set_ylabel(\"Relative Frequency(%)\", fontsize = fontSize)\n    ax.tick_params(axis=\"x\", labelsize=fontSize)\n    ax.tick_params(axis=\"x\", labelsize=fontSize)\n    plt.legend(loc = \"best\")\n    return plt.show()","519bd7fc":"catVar1 = df1['Pclass']\ncatVar2 = df1['Sex']\ncatVar3 = df1['Embarked']\nmultiVariableBarPlot(catVar1, catVar2, catVar3, targetVaraible = df1['Survived'])","a5819fc5":"catVar1 = df1['Pclass']\ncatVar2 = df1['Sex']\ncatVar3 = df1['Cabin']\nmultiVariableBarPlot(catVar1, catVar2, catVar3, targetVaraible = df1['Survived'])","4b476abb":"catVar1 = df1['Pclass']\ncatVar2 = df1['Sex']\ncatVar3 = df1['SibSp']\nmultiVariableBarPlot(catVar1, catVar2, catVar3, targetVaraible = df1['Survived'])","31f90f27":"catVar1 = df1['Pclass']\ncatVar2 = df1['Sex']\ncatVar3 = df1['Parch']\nmultiVariableBarPlot(catVar1, catVar2, catVar3, targetVaraible = df1['Survived'])","26388cfc":"catVar1 = df1['Pclass']\ncatVar2 = df1['Sex']\ncatVar3 = df1['Social Status Number']\nmultiVariableBarPlot(catVar1, catVar2, catVar3, targetVaraible = df1['Survived'])","7bf2ab3e":"#Create bin categories for Age.\nageGroups = [\"infant\",\"child\",\"teenager\",\"youngAdult\",\"adult\",\"elderly\"]\n\n#Create range for each bin categories of Age.\ngroupRanges = [0,5,12,18,35,60,81]\n\n#Create and view categorized Age with original Age.\ndf1[\"Age Binned\"] = pd.cut(df1.Age, groupRanges, labels = ageGroups)\n\ntest_df[\"Age Binned\"] = pd.cut(test_df.Age, groupRanges, labels = ageGroups)","34226124":"df1['Fare'].max()","11165187":"test_df['Fare'].max()","12159ae8":"#Create bin categories for Fare.\nfareGroups = [\"low\",\"medium\",\"high\"]\n\n#Create range for each bin categories of Fare.\nfareGroupRanges = [-1, 33, 100, 264]\n\n#Create and view categorized Fare with original Fare.\ndf1[\"Fare Binned\"] = pd.cut(df1.Fare, fareGroupRanges, labels = fareGroups)\n\n\n#Create bin categories for Fare.\nfareGroups = [\"low\",\"medium\",\"high\"]\n\n#Create range for each bin categories of Fare.\nfareGroupRanges = [-1, 33, 100, 514]\n\ntest_df[\"Fare Binned\"] = pd.cut(test_df.Fare, fareGroupRanges, labels = fareGroups)","a4a4d30d":"test_df[\"Fare Binned\"] = test_df[\"Fare Binned\"].fillna(\"high\")","eb079224":"df1.head()","a02d4742":"#Creating a dataframe object where I drop columns that I'll not be using subsequently\ndf2 = df1.drop(columns=['Age','Fare','Ticket','SibSp','Parch'])\ndf2.head(3)\n\ntest_df = test_df.drop(columns=['Age','Fare','Ticket','SibSp','Parch'])","a0998402":"#Family Size\nfamilySize_dict = {'single': 1,\n                  'small' : 2,\n                  'medium' : 3,\n                  'large' : 4}\ndf2['familySize Ordinal'] = df.familySize.map(familySize_dict)\n\ntest_df['familySize Ordinal'] = test_df.familySize.map(familySize_dict)","4d9231ce":"#Age\nAge_dict = {'infant': 1,\n                  'child' : 2,\n                  'teenager' : 3,\n                  'youngAdult' : 4,\n                  'adult' : 5,\n                  'elderly' : 6}\ndf2['Age Ordinal'] = df2['Age Binned'].map(Age_dict)\n\ntest_df['Age Ordinal'] = test_df['Age Binned'].map(Age_dict)","9cfd3962":"Fare_dict = {'low': 1,\n            'medium' : 2,\n            'high' : 3}\ndf2['Fare Ordinal'] = df2['Fare Binned'].map(Fare_dict)\n\ntest_df['Fare Ordinal'] = test_df['Fare Binned'].map(Fare_dict)","d52f8007":"df2 = df2.drop(columns=['familySize','Age Binned','Fare Binned'])\ndf2.head(2)\n\ntest_df = test_df.drop(columns=['familySize','Age Binned','Fare Binned'])","5aca1a16":"#Performing Oridnal Encoding on the Survived feature\nle = preprocessing.LabelEncoder()\ndf2['Survived'] = le.fit_transform(df2['Survived'])","31e35fea":"df2.head(1)","dcdc529c":"#Performing One-hot enconding(Dummy Variable) from Ordinal encoding\nenc_df = df2[['PassengerId','Sex','Cabin','Embarked']]\nenc_df = pd.get_dummies(enc_df)\nenc_df.head()\ndf3 = pd.merge(df2, enc_df, on='PassengerId', how='inner')\n\nenc_df = test_df[['PassengerId','Sex','Cabin','Embarked']]\nenc_df = pd.get_dummies(enc_df)\ntest_df = pd.merge(test_df, enc_df, on='PassengerId', how='inner')","848605fc":"df3 = df3.drop(columns=['PassengerId','Sex','Cabin','Embarked'])\n\ntest_df = test_df.drop(columns=['PassengerId','Sex','Cabin','Embarked'])","50164d6b":"df3['Age Ordinal'] = df3['Age Ordinal'].astype('int64')\ndf3['Fare Ordinal'] = df3['Fare Ordinal'].astype('int64')\n\ntest_df['Age Ordinal'] = test_df['Age Ordinal'].astype('int64')\ntest_df['Fare Ordinal'] = test_df['Fare Ordinal'].astype('int64')","4a73efcf":"#Summary of the data types of the different features\ndf3.dtypes","b64d0c9f":"#Looking at the dimension of the dataset\nxTrain = df3.drop(columns = [\"Survived\"], axis = 1)\nyTrain = df3[['Survived']]\n\nprint(f\"Input Matrix Dimension: {xTrain.shape}\")\nprint(f\"Output Vector Dimension: {yTrain.shape}\")","6bff54df":"#Looking at the names of all the different columns that we would train the dataset with\nxTrain.columns","99928301":"seed = 43\n\"\"\"Now initialize all the classifiers object.\"\"\"\n\"\"\"#1.Logistic Regression\"\"\"\nlr = LogisticRegression()\n\n\"\"\"#2.Support Vector Machines\"\"\"\nsvc = SVC(gamma = \"auto\")\n\n\"\"\"#3.Random Forest Classifier\"\"\"\nrf = RandomForestClassifier(random_state = seed, n_estimators = 100)\n\n\"\"\"#4.KNN\"\"\"\nknn = KNeighborsClassifier()\n\n\"\"\"#5.Gaussian Naive Bayes\"\"\"\ngnb = GaussianNB()\n\n\"\"\"#6.Decision Tree Classifier\"\"\"\ndt = DecisionTreeClassifier(random_state = seed)\n\n\"\"\"#7.Gradient Boosting Classifier\"\"\"\ngbc = GradientBoostingClassifier(random_state = seed)\n\n\"\"\"#8.Adaboost Classifier\"\"\"\nabc = AdaBoostClassifier(random_state = seed)\n\n\"\"\"#9.ExtraTrees Classifier\"\"\"\netc = ExtraTreesClassifier(random_state = seed)\n\n\"\"\"#10.Extreme Gradient Boosting\"\"\"\nxgbc = XGBClassifier()\n\n\"\"\"List of all the models with their indices.\"\"\"\nmodelNames = [\"LR\", \"SVC\", \"RF\", \"KNN\", \"GNB\", \"DT\", \"GBC\", \"ABC\", \"ETC\", \"XGBC\"]\nmodels = [lr, svc, rf, knn, gnb, dt, gbc, abc, etc, xgbc]","e2e94e24":"def calculateTrainAccuracy(model):\n    \"\"\"Returns training accuracy of a model.\"\"\"\n    model.fit(xTrain, yTrain)\n    trainAccuracy = model.score(xTrain, yTrain)\n    trainAccuracy = round(trainAccuracy*100, 2)\n    return trainAccuracy\n\n# Calculate train accuracy of all the models and store them in a dataframe\nmodelScores = list(map(calculateTrainAccuracy, models))\ntrainAccuracy = pd.DataFrame(modelScores, columns = [\"trainAccuracy\"], index=modelNames)\ntrainAccuracySorted = trainAccuracy.sort_values(by=\"trainAccuracy\", ascending=False)\nprint(\"~~~~~~~ Training Accuracy of the Classifiers ~~~~~~~~~~~\")\nprint(trainAccuracySorted)","e74a928a":"def calculateXValScore(model):\n    \"\"\"Returns models' cross validation scores.\"\"\"\n    \n    xValScore = cross_val_score(model, xTrain, yTrain, cv = 10, scoring=\"accuracy\").mean()\n    xValScore = round(xValScore*100, 2)\n    return xValScore\n\n# Calculate cross validation scores of all the models and store them in a dataframe\nmodelScores = list(map(calculateXValScore, models))\nxValScores = pd.DataFrame(modelScores, columns = [\"xValScore\"], index=modelNames)\nxValScoresSorted = xValScores.sort_values(by=\"xValScore\", ascending=False)\n\ndisplay(xValScoresSorted)","64586c1b":"import pickle\nmodelNames_list = [\"LR\", \"SVC\", \"RF\", \"KNN\", \"GNB\", \"DT\", \"GBC\", \"ABC\", \"ETC\", \"XGBC\"]\nmodels_list = [lr, svc, rf, knn, gnb, dt, gbc, abc, etc, xgbc]\n\nfor model, modelName in zip(models_list, modelNames_list):\n    model_filename = modelName + \".sav\"\n    pickle.dump(model, open(model_filename, \"wb\"))","258ec8b1":"#Checking for collinearity\npearsoncorr = xTrain.corr(method='pearson')\n#Styling\nplt.figure(figsize=(30, 10))\nsns.heatmap(pearsoncorr, \n            xticklabels=pearsoncorr.columns,\n            yticklabels=pearsoncorr.columns,\n            cmap='RdBu_r',\n            annot=True,\n            linewidth=0.3)","7da75179":"test_df[\"Cabin_T\"] = 0","6e15a041":"yPred = gbc.predict(test_df)\nyPred_df = pd.DataFrame({'yPred': yPred})","bd372947":"yPred_df.to_csv(\"yPred.csv\")","ee37bc1e":"### (b) Nominal Encoding","34a26513":"<a id=\"point_3_1\"><\/a> \n## 3.1 Overview of the train dataset\nWe look at the values, datatype and the state of the data.","432e8fcc":"### (c) Combining the Ordinal and Nominal Encoding","879036c4":"**Findings:** Pclass 3 and Male sex are the deciding factor.","b84c52df":"**Findings:** We calculate the p-value to be very small(less than 0.01 level of significance), therefore we can reject the null hypothesis. We can conclude that the mean of the two columns are different and their interaction is statistically significant.","5116b687":"<a id=\"point_4_2_2\"><\/a> \n### 4.2.2 Sibsp (# of siblings\/spouse on board)\nBar plot of the frequency of the number of siblings\/spouse on board.","43e27a82":"**Findings:** There is a large proportion of \"ordinary citizens\" and there were a few high ranking members of society such as the Master, Officer and Aristocrat.","a88ea610":"<a id=\"point_9_1_1\"><\/a>\n## 9.11 Age","f4e69858":"<a id=\"point_1\"><\/a> \n# 1. Data Table\nThe breakdown of the data available and its characteristics:\n\n- PassengerId is a unique identifying number assigned to each passenger.\n- Survived is a flag that indicates if a passenger survived or died ( i.e., 0 = No, 1 = Yes).\n- Pclass is the passenger class (i.e., 1 = 1st class, 2 = 2nd class, 3 = 3rd class).\n- Name is the name of the passenger.\n- Sex indicates the gender of the passenger (i.e., Male or female).\n- Age indicates the age of the passenger.\n- Sibsp is the number of siblings\/spouses aboard.\n- Parch is the number of parents\/children aboard.\n- Ticket indicates the ticket number issued to the passenger.\n- Fare indicates the amount of money spent on their ticket.\n- Cabin indicates the cabin category occupied by the passenger.\n- Embarked indicates the port where the passenger embarked from (i.e., C = Cherbourg, Q = Queenstown, S = Southampton).","bc8a6dbf":"<a id=\"point_8_1\"><\/a>\n## 8.1 (Pclass, Sex, Embarked) vs Survived","bf117cc0":"**Conclusion:** From the Chi-Squared test,we determine that the Embakred has an association with Survival rate. And conducting the Bonferroni method, we determine that the Embarked(C) and Embarked(S) has the greatest association, while Embarked(Q) has not statistical significance).","cb19102c":"**Conclusion:** Therefore, through the outier detection section, we were able scope out the extent of the outliers. For the Fare, we have remove a single outlier which may result in a large skew in our results if we were to perform normalisation later on and affecting the accuracy of our regression model.","ecec80ec":"<a id=\"point_4_1_3\"><\/a> \n### 4.13 Sex\nBar plot of the frequency of the Sex.","ec6ee109":"**Findings**: We note that there's a much larger proportion of 3rd class ticket sold, while there's around half the amount of tickets sold for the 1st and 2nd class tickets which is similar to the seat allocation in modern airlines.","64da37c3":"**Findings:** Pclass 3 and Male sex are the deciding factor.","ea105d7c":"<a id=\"point_4_1_2\"><\/a> \n### 4.12 Pclass(Ticket Class)\nBar plot of the frequency of the Ticket Class.","28577e01":"**Findings:** Looks like all the tree based models have highest train accuracy followed Random Forest, Decision Tree and Extra Tree Classifier. But train accuracy of a model is not enough to tell if a model can be able to generalize the unseen data or not. Because training data is something our model has been trained with, i.e., data our model has already seen it. We all know that, the purpose of building a machine learning model is to generalize the unseen data, i.e., data our model has not yet seen. Hence we can't use training accuracy for our model evaluation rather we must know how our model will perform on the data our model is yet to see.","ee284a11":"**Findings:** Pclass 3 and Male sex are the deciding factor.","1744a813":"**Conclusion:** The graphical representation further supports the ANOVA analysis that there's a correlation between the survivability and fare. And we can observe from the bar plots that the average fare is higher for passengers who survivied.","c5c2171e":"With respect to the K-Fold Cross validation method, we can see that the top 3 models with the highest score is Gradient Boosting Classifier then Support Vector Machines and finally Extra Tree Classifer. I would be choosing the Logistic Regression model as my regression model of choice due to the relative high scoring and my personal familarity with the mathematics behind the algorithm which would allow me to implement the model more accurately without syntax or semantic(logic) error.\n\nI would be using the statsmodel package to do my final evaluation with the summary output which would provide me with statistical values which would aid in my conclusion.","55f10d30":"<a id=\"point_3_2_1\"><\/a> \n## 3.21 Null Value in Age Column\nAssigning values for the age.","ffcbaebe":"<a id=\"point_4_2_1\"><\/a> \n### 4.2.1 Age\nBar plot of the frequency of the Age.","37bdc701":"<a id=\"point_10_1\"><\/a>\n## 10.1 Training Model\nWe would train 10 different classifiers for this binary classification problem.","03c86ccb":"We realised that the Salutation always follows a puntuation mark(.) behind it, and therefore, we can use it to get the more easily extract the salutation","983637f4":"<a id=\"point_4\"><\/a> \n# 4. Univariate Analysis\nIn this next section, we'll utilised univariate analysis, which separately explores the distribution of each variable in a data set. It looks at the range of values, as well as the central tendency of the values. \n<br>Univariate data analysis does not look at relationships between various variables (like bivariate and multivariate analysis) rather it summarises each variable on its own. \n<br>Methods to perform univariate analysis will depend on whether the variable is categorical or numerical. \n<br> (1) For numerical variable, we would explore its shape of distribution (distribution can either be symmetric or skewed) using **histogram and density plots**. \n<br> (2) For categorical variables, we would use **bar plots** to visualize the absolute and proportional frequency distribution.","d6967b6c":"<a id=\"point_7\"><\/a>\n## 7. Bivariate Analysis\nThe next step after we have perform single variable analysis is to compare 2 variables(Bivariate Analysis). We first standardise the dataset where the values is centered around 0 and with a standard deviation of 1, while avoiding normalisation which we identified earlier which may result in the skewing of the values.\n<br> Our approach is to use statistical methods to create quantitative results compared against survivability, which is further substantiated by graphical visualisations.\n<br> There are 2 Statistical methods used:\n<br> (1) ANOVA: We used ANOVA to compare the mean of 2 groups of value for **numerical values.**\n<br> (2) Chi-Squared Test: We use Chi-Sqaured Test to compare the mean of 2 groups of value for **catergorical values.**\n<br> The row in question would be the column that we are testing against the Survivability target variable(which is **catergorical value**)","9c304396":"<a id=\"point_7_2_6\"><\/a>\n## 7.26 Chi-Squared Test & Bonferroni Method for Ticket","7e5aee0e":"<a id=\"point_7_2_4\"><\/a>\n## 7.24 Chi-Squared Test & Bonferroni Method for familySize\nSibsp is the number of siblings\/spouses aboard.","bcfc3380":"<a id=\"point_4_2_3\"><\/a> \n### 4.2.3 Parch(# Parents\/Children on board)\nBar plot of the frequency of the # Parents\/Children on board.","2669884a":"<a id=\"point_10\"><\/a>\n## 10. Model Building\nThe Model buiding invovled splitting the dataset into its consituents which are the predictor variables(X variables), and its depedent variable(Y variables). After which, we would fit the predictor variables into various algorithms and evaluates which model fits the data more completely.","b1f7727c":"<a id=\"point_9_3\"><\/a>\n## 9.3 Encoding Catergorical Variables\nTo utilised the regression for our prediction model, we would need to convert the catergorical variables into numerical values for mathematical manipulation. There are several various to encode the catergorical variables into numerical values, for this project I'll be usig two types of encoding.\n<br> The first is ordinal encoding where I'll be assigning numerical values from 0 upwards in integer value, where the numbers are assigned based off the order of the catergorical variable\n<br> The second is nominal encoding, more specificially using the method called one-hot(dummary variable) encoding where there's no particular order to the different values. This is where there is no relation or order between classes therefore we do not want the algorithm to consider them under the same order.","fb45c0a9":"**Findings:** We realised that the p value is much smaller than the level of significance of 0.01, thefreo we can reject the null hypothesis and use the assert the claim that they are do not occur independent of one another. However, since there are 3 columns for the 3 Pclass, we would need to conduct Bonferroni method to determine which pair is the claim applicable for.","f5436e1c":"<a id=\"point_5_1\"><\/a>\n# 5.1 Cabin\nWith the large number of different unique valeus of the cabin id, we wanted to investigate if there's a general class of cabin that we can derive. We realised that for the cabin id, the id is split into the first alphabet and subsequent numbers. We postualate that the first alphabet is the class of the cabin and the number represents the room number.","c9b38c28":"**Findings:** We realised that the median age and the mean age is quite close with the mean age being 29.7 while the median age is 28. In addition, from observing the mean and median value and the graphical visualization, we can conclude that the Age column is right skewed. Therefore, we'll fill the age column with the mean since it will help us to correct the right skew in the Age column.","8f101285":"<a id=\"point_5\"><\/a>\n# 5. Feature Engineering\nIn this section, we would either modify or create new features from the exsisting features which are otherwise hard to analyse in their raw forms that we saw in Univariate Analysis section. This will allows us to extract information from the dataset which may appear eitherwise convoluted with data that may be 'messy' and have no pattern in its raw form. \n<br>There are various methods that we employed for the different features in this section, and they are explicitly stated below.\n* Features that we would be focusing on: Cabin, Name, Parch and Ticket","84dcaca7":"**Findings:** Since all of the expected frequencies are greater than 5, the chi2 test results can be trusted. We can reject the null hypothesis as the p-value is less than 0.05(infact p value is less than 0.01). Thus, the results indicate that there is a statistically significant relationship between the variables familySize and Survived.","2542440d":"<a id=\"point_8\"><\/a>\n## 8. Multi-Variate Analysis\nIn multivariate analysis, we try to find the relationship among more than two variables. Number of predictor variable in bivariate analysis was one. On the contrary, number of predictor variables for multivariate analysis are more than one. More specifically, we will try to associate more than one predictor variable with the response variable. We will just visualize the impact of different predictor variables (3 variables) at a time on variable Survived. We'll be using bar plots to represent the different permutations of the predictor variables.\n\nThere'll a large number of permutation of the combination of the multivariate analysis(9C3 = 84 ways to be specific), therefore I'll be performing specific analysis on the factors that I would like to more imperatively want to have a clearer picture of.","045edc8d":"<a id=\"point_8_4\"><\/a>\n## 8.4 (Pclass, Sex, Parch) vs Survived","912ed3b1":"<a id=\"point_7_2_5\"><\/a>\n## 7.25 Chi-Squared Test & Bonferroni Method for Social Status Number","3f717052":"<a id=\"point_11\"><\/a>\n# 11. Final Words & Acknowledgement\n\nThank you for viewing this project. If there's any constructive criticisms or feedback, I would greatly appreciate you sharing with me as I'm in this jounrney to learn new things and I can be contacted at james_gan@mymail.sutd.edu.sg\n\nThis project would also not be possible without the community in Kaggle whom I referred to for questions and gain much inspirations and knowledge from viewing other similar projects to create this current notebook above.","7803d101":"**Findings:** Sex seems to be the deciding factor once again.","cf2bfdec":"** Findings:** The fare seem to have a large outlier who fare cost over 500 dollars, and the outliers exist on the upper limit. Looking at the graph, we can see that the data for the fare is right skewed. The large deviation from the upper limit would result in the data being skewed and affecitng the accuracy of the data, therefore, I've decided to remove the points from 265 dollars and above.","ee56754b":"<a id=\"point_5_4\"><\/a>\n## 5.4 SibSp and Parch\nThe 2 variables are highly correlated to one another where SibSp: No. of siblings and Parch: No. of relatives, therefore we perform a function to create a new variable called 'familySize' that merge these two features.","c5430b0a":"Therefore, with the highest accuracy score, Linear Regression is the model of choice to predict the survivability of passengers on board the Titanic. Through statistical and visual analysis of the predictor variables through bar plots, ANOVA test, Chi-Squared test and Bonforreoni Method we have been narrowed down to include only the Class, Sex, Cabin Number,Port of Embarkation,Social Status, Family Size, Age and Fare paid. The statiscal summary shows the R^2 value of 0.3611 (which is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model). ","f83c44d2":"**Findings**: There were significantly more people who did not survive the accident, which aligns with the project background where it's stated that there's not enough lifeboats for all the passengers and there's a high death toll. Next, we'll like to observe if there's any varations the different ticket classes onboard the Titanic.","d26f5197":"<a id=\"point_6_1\"><\/a>\n## 6.1 Age","7051e633":"<a id=\"point_3\"><\/a> \n# 3. Data Exploration\nThe goal of the data exploration phase is to **understand the dataset** that has been provided to us with details further substantiated in the data table in section 1. This allows us to have a brief overview of the condition of the dataset and to pre-empt myself to think about possible missing values and 'dirty' data.\n<br>Therefore, we can do elementary data cleaning such as removing null values and filling in missing values.","b9d22a79":"**Conclusion:** The graphical representation further supports the ANOVA analysis that there's no correlation between the survivability and fare. And the average age does not have a strong relationship with survivability.","1414cafb":"<a id=\"point_10_3\"><\/a>\n## 10.3 Conclusion","0cb67f44":"**Findings**: There are many unique catergorical values for the ticket id therefore, we would need to perform more pre-processing.","1a94bc92":"<a id=\"point_4_1\"><\/a> \n## 4.1 Catergorical Variables","203ad02c":"**Conclusion:** Therefore, through our feature engineering section, we were able to manipulate the features to create more actionable values in the columns that we can subsequently use for our analysis and regression.","5521c39b":"<a id=\"point_4_1_5\"><\/a> \n### 4.15 Cabin\n**Findings:** From the Data Exploation phase at 3.1, we discovered that there's a large number of null values, therefore we would require to do more data manipulation later on.","260dd56f":"<a id=\"point_7_1\"><\/a>\n## 7.1 ANOVA Test for Numerical Variables\nThis subsection, we perform ANOVA test for the columns that are numerical(Continuous). The ANOVA test that we would be performing for this sub-section 8.2 is the one way ANOVA where we'll be focusing on comparing the the mean value for two population set(Survived and Not-Survived).\n\nThe ANOVA(ANalysis Of VAriance) test lets us check whether a numeric response variable varies according to the levels (or class) of a categorical variable. When I simply refer to 'ANOVA', for this exmaple I mean the 'one way' ANOVA which is a test for exploring the impact of one single factor on three or more groups (but two groups would also do, as we explain below).\nThe one-way ANOVA tests whether the mean of some numeric variable differs across the levels of one categorical variable. It essentially answers the question: do any of the group means differ from one another? The null hypothesis is all of the group means are equal. And the alternate hypothesis is any of the group means differ from one another.\n\n<br> H0(Null Hypothesis): mean of Fare(Survived) == mean of Fare(Did not Survived)\n<br> H1(Alternative Hypothesis): mean of Fare(Survived) != mean of Fare(Did not Survived)\n\nWe can reject the null hypothesis when the p-value calculated below is smaller than my defined level of significance of 0.01 which means that that the mean of one group differ from the other.","4898b67a":"<a id=\"point_7_1_1\"><\/a>\n## 7.11 ANOVA Test for Fare","e1738716":"**Findings**: The age distributions shows that the majority of the passengers on board were middle age with more children than elderly people, where the working adults class had the greatest disposable income to justify the cost of the ticket.","3c7ba46c":"<a id=\"point_7_2\"><\/a>\n## 7.2 Chi-Squared Test for Discrete Variables\nThis subsection, we perform Chi-Squared test for the columns that are Catergorical(Discrete).\n\n<br> The Chi-square test of independence tests if there is a significant relationship between two categorical variables.The data is usually displayed in a cross-tabulation format with each row representing a category for one variable and each column representing a category for another variable. Chi-square test of independence is an omnibus test.That is it tests the data as a whole. This means that one will not be able to tell which levels (categories) of the variables are responsible for the relationship.\n\n\n<br>Null Hypothesis, H0: pij == pi*pj\n<br>Alternative Hypothesis, Ha: p1 != p2\n<br>Where p1 = Survivability\n<br>& p2 = Predictor variable investigated\n\n<br> Interpretation: The null hypothesis suggest that the two variables are indepedent and their relationship is statiscally not significiant. This happens when the p-value that we caculate below is smaller than my set level of significance of 0.01.","386a79d4":"**Conclusion:** From the Chi-Squared test,we determine that the familySize has an association with Survival rate. And conducting the Bonferroni method, we determine that the familySize(large) and familySize(single) has the greatest association, while familySize(samll) and familySize(medium) has low statistical significance).","f05be067":"**Conclusion:** From the Chi-Squared test,we determine that the Pclass has an association with Survival rate. And conducting the Bonferroni method, we determine that the Pclass(1) and Pclass(3) has the greatest association.","c0c3d1ab":"<a id=\"point_2\"><\/a> \n# 2. Python Packages\nWe import various packages within the Python Language to aid in the code creation process.","22ffeb65":"### Context of Project\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nTherefore, the goal of the project is to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","68a549c2":"**Conclusion:** This is further supported by graphical visualization where it can be seen that the survival rate of the different sex is almost 200%. Where there were more female survival than male survival.","dd2b7fdd":"<a id=\"point_7_1_2\"><\/a>\n## 7.12 ANOVA Test for Age","a72d7a96":"<a id=\"point_4_1_4\"><\/a> \n### 4.14 Tickets","9af35200":"**Findings:** The expected frequency value is greater than 5, therefore we can conclude that this chi-squared test is reliable. The p value is very small and less than the 0.01 level of significance therefore we can reject the null hypthesis where survivability and sex are not independent and there's statistical significance in their relationship.","4f02894c":"**Findings:** Since all of the expected frequencies are greater than 5, the chi2 test results can be trusted. We can reject the null hypothesis as the p-value is less than 0.05(infact p value is less than 0.01). Thus, the results indicate that there is a statistically significant relationship between the variables Embarked and Survived.","33c516f1":"## 4.2 Numerical Variables\nTo explore the numerical variables, we utilised data visualisation with Density Plot and Histogram to see the total count and frequency of each individual variable. The custom code for the creation of a new function: \"plotHistogram\", to create density and barplots used for the numerical variables later on","60a54494":"**Findings**: Most of the passengers travelling on the ship was mainly individuals travellers, with some travelling with someone else, most likely their spouse and very few families.","12622b95":"# Methodology\nTo improve readability, I have broken down the notebook into these sub sections\n* [1. Data Table](#point_1)\n* [2. Python Packages](#point_2)\n* [3. Data Exploration](#point_3)\n    * [3.1 Overview of the train dataset](#point_3_1) \n    * [3.2 Handling Null values](#point_3_2) \n         * [3.21 Null Value in Age Column](#point_3_2_1)\n         * [3.22 Null Value in Embarked Column](#point_3_2_2)     \n* [4. Univariate Analysis](#point_4)\n    * [4.1 Catergorical Variables](#point_4_1)\n        * [4.11 Survived](#point_4_1_1)\n        * [4.12 Pclass](#point_4_1_2)\n        * [4.13 Sex](#point_4_1_3)\n        * [4.14 Tickets](#point_4_1_4)\n        * [4.15 Cabin](#point_4_1_5)\n        * [4.16 Embarked](#point_4_1_6)\n    * [4.2 Numerical Variables](#point_4_2)\n        * [4.21 Age](#point_4_2_1)\n        * [4.22 SibSp](#point_4_2_2)\n        * [4.23 Parch](#point_4_2_3)\n        * [4.24 Fare](#point_4_2_4)   \n* [5. Feature Engineering](#point_5)\n    * [5.1 Cabin](#point_5_1)\n    * [5.2 Name](#point_5_2)\n    * [5.3 Ticket](#point_5_3)\n    * [5.4 SibSp and Parch](#point_5_4)\n* [6. Outlier Detection](#point_6)\n    * [6.1 Age](#point_6_1)\n    * [6.2 Fare](#point_6_2)\n* [7. Bivariate Analysis](#point_7)\n    * [7.1 ANOVA Test for Numerical Variables](#point_7_1)\n        * [7.11 ANOVA Test for Fare](#point_7_1_1)\n        * [7.12 ANOVA Test for Age](#point_7_1_2)\n    * [7.2 Chi-Squared Test & Bonferroni Method for Discrete Variables](#point_7_2)\n        * [7.21 Chi-Squared Test for Sex](#point_7_2_1)\n        * [7.22 Chi-Squared Test & Bonferroni Method for Pclass](#point_7_2_2)\n        * [7.23 Chi-Squared Test & Bonferroni Method for Embarked](#point_7_2_3)\n        * [7.24 Chi-Squared Test & Bonferroni Method for familySize](#point_7_2_4)\n        * [7.25 Chi-Squared Test & Bonferroni Method for Social Status Number](#point_7_2_5)\n        * [7.26 Chi-Squared Test & Bonferroni Method for Ticket](#point_7_2_6)\n* [8. Multi-variate Analysis](#point_8)\n    * [8.1 (Pclass, Sex, Embarked) vs Survived](#point_8_1)\n    * [8.2 (Pclass, Sex, Cabin) vs Survived](#point_8_2)\n    * [8.3 (Pclass, Sex, SibSp) vs Survived](#point_8_3)\n    * [8.4 (Pclass, Sex, Parch) vs Survived](#point_8_4)\n    * [8.5 (Pclass, Sex, Social Status Number) vs Survived](#point_8_5)\n* [9. Data Transformation](#point_9)\n    * [9.1 Binning Continuous Variables](#point_9_1)\n        * [9.11 Age](#point_9_1_1)\n        * [9.12 Fare](#point_9_1_2)\n    * [9.2 Dropping Features](#point_9_2)\n    * [9.3 Encoding Catergorial Variables](#point_9_3)\n* [10. Model Building](#point_10)\n    * [10.1 Training Model](#point_10_1)\n    * [10.2 Cross Validation of Model - Using K-Fold Cross Validation](#point_10_2)\n    * [10.3 Conclusion](#point_10_3)\n* [11. Final Words and Acknowledgement](#point_11)","71b0c151":"<a id=\"point_3_2_2\"><\/a> \n## 3.22 Null Value in Embarked Column\nThe nature of the embarked column is catergorical vairable and we observe that there are only 3 unique values for the Port of Embarkation, therefore we would use a different approach to handle the null value as comapred to the Age column. In this case, we observe that the Port S has the highest number of passengers boarding, therefore there's a higher probability that the missing data passenger embarked from Port S and we'll fill it in.","6d3c486d":"**Findings:** We set the NaN values to be N and we realised that other than the NaN values, the largest number of cabins is Cabin B & C","744906e3":"<a id=\"point_8_3\"><\/a>\n## 8.3 (Pclass, Sex, SibSp) vs Survived","8270e763":"**Conclusion:** Therefore in this data explaratory phase, we have identified the columns that have missing value and filled up with values we deemed are the most appropriate by observing the column in its entirety. For now, this elementary step into the project will allow us to have a cleaner dataset to work with and we'll perform various task along the notebook to aid in our analysis.","89fc229f":"**Findings:** We calculate the p-value and it's larger than 0.01 level of significance, therefore we do not reject the null hypothesis. We can conclude that the mean of the two columns are same and their interaction is statistically insignificant.","278b7983":"<a id=\"point_6_2\"><\/a>\n## 6.2 Fare","502356ad":"**Formulas:**\n<br> Expected Frequency(eij) = (ni*nj)\/n\n<br> Test Statistic Value: \u03a3\u03a3 ((nij-eij)^2)\/eij","7671d1a7":"**Conclusion:** The bivariate analysis brought much clarity to me on the factors that had an impact on the survivability of the passengers through statistical significance. However for the Ticket and Social Status Number feature, I'm unable to conduct Chi-Squared test due to the expected frequencies not reaching a threshold where I can be confident of the statistical significance.","e32333f5":"<a id=\"point_9\"><\/a>\n## 9. Data Transformation\nIn this section, we will categorize our continuous variables. After that, redundant and useless features will be dropped. And finally categorical variables will be encoded into numerical variables to feed our machine learning models.\n<br> The purpose of categorizing the the continuous variable is to prevent overfitting of the regression model as some features such as Age can be classified into catergories without a loss of valueable information.","08c71f35":"<a id=\"point_7_2_1\"><\/a>\n## 7.21 Chi-Squared Test for Sex","536521c9":"# Goal of Project:\nThe goal of this project is to utilized the dataset provided from Kaggle website to analyze the possible factors that affecting surivability and creating a prediction model for survivability of passengers using variables from the dataset for the Kaggle Titanic Competition\n<br> <br> In addition, my personal goal for this projet is to use this project as an application for  my university module on Statistics, to make use of the statiscal tools to create inference, analysis through hypothesis testing and finally creating regression models. The Statistical methods I applied are: ANOVA, Chi-Squared Test, Bonferroni Method, Regression Modeling(Logistics & Linear Regression), Ordinal & Nominal Encoding, and Correlation Coefficient.\n<br> <br>  The link to my university Statistics module information is attatched here: [SUTD Statistics Module](https:\/\/esd.sutd.edu.sg\/courses\/40004-statistics\/)","11802674":"<a id=\"point_4_1_1\"><\/a> \n### 4.11 Survived\nBar plot of the frequency of the survival status.","d6f91789":"**Findings:** Since all the expected frequencies aren't greater than 5, chi-square test result can't be trusted.","746b13b0":"<a id=\"point_9_1_2\"><\/a>\n## 9.12 Fare","2d1594e9":"<a id=\"point_4_1_6\"><\/a> \n### 4.16 Embarked\nBar plot of the frequency of the port of embarkation.","8800f004":"**Findings:** We realised that there's large proportion of data that occurs outside of the upper and lower limit of the box plot. The data of the age is tended towards left skewed as seen from the graph above. However, I have decided to keep all the values inside as it falls within the reasonable region from age 0.42 to 80, which seems like the reasonable life expectancy during that period. \n","b907fc9e":"### Visualizing the mean and median\nPlotting box-whiskers plot and histogram plot to see the 'skewness' of the dataset. This will help us to determine if the data is skewed and guide us in choosing more appropriate values to fill into the null value entry for the Age column.","c352c0fb":"<a id=\"point_9_1\"><\/a>\n## 9.1 Binning Continuous Variables\nWe group the continuous variables into certain pre-determined catergory.\n\nNote: Binning continuous variables prevents overfitting which is a common problem for tree based models like decision trees and random forest etc.","5c3578bb":"<a id=\"point_5_2\"><\/a>\n# 5.2 Name\nFor the name column, we realised that there are rows that have different salutations for the name which may allow us to get not only the sex but the social status of the individual. Therefore, we performed various split function to get the person salutation.","307966ee":"<a id=\"point_4_2_4\"><\/a> \n### 4.2.4 Fare\nBar plot of the frequency of the Fare.","17778b29":"<a id=\"point_8_5\"><\/a>\n## 8.5 (Pclass, Sex, Social Status Number) vs Survived","67b7a368":"**Findings:** There are 8 unique ticket values which are 'A' 'P' 'S' 'Number' 'C' 'W' 'F' 'L'.","7f8fa622":"**Findings:** We realised that there are 3 columns in the dataset that contain null values, these 3 rows are age, cabin and embarked.","8f17e4a4":"**Findings**: Majority of the ticket prices were at the lower end while there were fewer higher priced tickets","7e9ba4a8":"<a id=\"point_6\"><\/a>\n# 6. Outlier Detection\nWe intend to look at the numerical variables(more specfically age and fare) to determine the spread of data with the box and wiskers diagram to better ascertain the outliers. We do not create the box and wiskers diagram for Parch and SibSp due to the small spread of the data.[](http:\/\/)","22f2a46a":"**Findings:** Since all the expected frequencies aren't greater than 5, chi-square test result can't be trusted.","b1dac02e":"<a id=\"point_9_2\"><\/a>\n## 9.2 Dropping Features","e8af964f":"<a id=\"point_8_2\"><\/a>\n## 8.2 (Pclass, Sex, Cabin) vs Survived","ee151731":"**Findings**: There were more male compared to female passengers onboard the ship.","91b98193":"### Converting into ordinal values from catergorical variables\nWe create 3 levels to encode the catergorical variables into ordinal values.\n<br>1 is the for ordinary citizens with salutations Mr, Miss and Mrs. 2 is for higher level which is Master. 3 for Officer. 4 for Aristocrat.","4862a871":"<a id=\"point_5_3\"><\/a>\n## 5.3 Ticket\nWe realised that there are many unique values for the column in Ticket and therefore we would want to generalise the values to get fewer distinct results. I decided to take the remove all the other values in the string other than the first value to create a catergory based off this first value.","9034c4bf":"<a id=\"point_7_2_3\"><\/a>\n## 7.23 Chi-Squared Test & Bonferroni Method for Embarked","de2974ff":"**Findings:** Across the x-axis, it seems like the sex had the biggest contribution to the survivability.","9cc151ad":"**Findings**: We visually oberseved that most poeple boarded at a Port S compared to the other 2 ports.","99d5b3b4":"### (a) Ordinal Encoding","51a8f5b1":"<a id=\"point_10_2\"><\/a>\n## 10.2 Cross Validation of Model - Using K-Fold Cross Validation\nLearning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test. Note that the word \u201cexperiment\u201d is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally.","3272b6e4":"<a id=\"point_7_2_2\"><\/a>\n## 7.22 Chi-Squared Test & Bonferroni Method for Pclass"}}