{"cell_type":{"f2deb513":"code","5a9aa240":"code","f1e44eee":"code","4fd7a725":"code","295f8e21":"code","762bb4e0":"code","c8bc8d00":"code","c1a96b22":"code","2d3c91f0":"code","78e855fd":"code","9462ac6c":"code","2ba416ef":"code","238c380f":"code","2da4bb0b":"code","65f97e39":"code","71176dd3":"code","a3d6bf08":"code","68e6b2eb":"code","e5bd0ae4":"code","500a444d":"code","228dbf8e":"code","452f74d5":"code","385c82ca":"code","c4a22571":"code","0b4a78fb":"code","c29b27de":"code","54b7a2f7":"code","1fdb6be9":"code","ffc5214c":"code","fe60be42":"code","b5453654":"code","6feb2f7a":"code","9ab09138":"code","885e25c0":"markdown","154289b4":"markdown","97c5a05a":"markdown","00e9b3a7":"markdown","a8f2fe6b":"markdown","e8e463e2":"markdown","bac32d82":"markdown","2678db63":"markdown","25d54601":"markdown","0972567a":"markdown","f2e070e3":"markdown","52108950":"markdown","10b5a7d4":"markdown","f890967b":"markdown"},"source":{"f2deb513":"import numpy as np\nimport matplotlib.pyplot as plt \nfrom PIL import Image\nfrom sklearn.mixture import GaussianMixture\nimport os","5a9aa240":"!ls","f1e44eee":"# Read image and convert to a numpy array #\nim = Image.open('..\/input\/haiti-image\/Haiti_Image.tif')\nimArray = np.array(im)\nnrows, ncolumns, nbands = np.shape(imArray)\n\n# Reshape image array to number of obs. by number of features # \nX = np.reshape(imArray, [nrows*ncolumns, nbands])","4fd7a725":"# Create and train GMM model # \nmodel = GaussianMixture(n_components=4, tol=0.1)\nmodel.fit(X)\nyhat = model.predict(X)","295f8e21":"# Reshape into an image # \nimSubset_hat = np.reshape(yhat, [nrows, ncolumns])","762bb4e0":"# Plot 2 approaches to segmentation # \nplt.figure()\nplt.suptitle('Unsupervised EM Classification BIC = '+\"{:e}\".format(model.bic(X)))\nplt.subplot(121)\nplt.imshow(imArray)\nplt.title('Color Image')\nplt.subplot(122)\nplt.imshow(imSubset_hat)\nplt.title('GMM EM Classification Result')\nplt.show()","c8bc8d00":"# Create and train GMM model Kmeans # \nmodel_2 = GaussianMixture(n_components=4, tol=0.1, covariance_type='spherical')\nmodel_2.fit(X)\nyhat2 = model_2.predict(X)\n\nimSubset_hat2 = np.reshape(yhat2, [nrows, ncolumns])\n\nplt.figure()\nplt.suptitle('Unsupervised EM Classification BIC = '+\"{:e}\".format(model_2.bic(X)))\nplt.subplot(121)\nplt.imshow(imArray)\nplt.title('Color Image')\nplt.subplot(122)\nplt.imshow(imSubset_hat2)\nplt.title('GMM EM Classification Result')\nplt.show()","c1a96b22":"prob_hat = model.predict_proba(X)\nprob_hat2 = model_2.predict_proba(X)","2d3c91f0":"prob_hat = model.predict_proba(X)\nprob_hat2 = model_2.predict_proba(X)\nmax0, max1, max2, max3 = prob_hat.max(axis=0)\n\nclass0 = prob_hat[(prob_hat[:,0] == max0)]\nclass1 = prob_hat[(prob_hat[:,1] == max1)]\nclass2 = prob_hat[(prob_hat[:,2] == max2)]\nclass3 = prob_hat[(prob_hat[:,3] == max3)]","78e855fd":"plt.figure()\nplt.suptitle('Unsupervised EM Classification, BIC = '+\"{:}\".format(model.bic(X)))\nplt.subplot(121)\nplt.imshow(imArray)\nplt.title('Color Image')\nplt.subplot(122)\nplt.imshow(class0)\nplt.title('GMM EM Classification Result')","9462ac6c":"plt.figure()\nplt.suptitle('Unsupervised EM Classification, BIC = '+\"{:}\".format(model.bic(X)))\nplt.subplot(121)\nplt.imshow(imArray)\nplt.title('Color Image')\nplt.subplot(122)\nplt.imshow(class1)\nplt.title('GMM EM Classification Result')","2ba416ef":"plt.figure()\nplt.suptitle('Unsupervised EM Classification, BIC = '+\"{:}\".format(model.bic(X)))\nplt.subplot(121)\nplt.imshow(imArray)\nplt.title('Color Image')\nplt.subplot(122)\nplt.imshow(class2)\nplt.title('GMM EM Classification Result')","238c380f":"plt.figure()\nplt.suptitle('Unsupervised EM Classification, BIC = '+\"{:}\".format(model.bic(X)))\nplt.subplot(121)\nplt.imshow(imArray)\nplt.title('Color Image')\nplt.subplot(122)\nplt.imshow(class3)\nplt.title('GMM EM Classification Result')","2da4bb0b":"%matplotlib inline\nimport theano\nimport pymc3 as pm\nimport sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nsns.set_style('white')\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons","65f97e39":"X, Y = make_moons(noise=0.2, random_state=0, n_samples=1000)\nX = scale(X)\nX = X.astype(float)\nY = Y.astype(float)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5)","71176dd3":"fig, ax = plt.subplots(figsize=(12, 8))\nax.scatter(X[Y==0, 0], X[Y==0, 1], label='Class 0')\nax.scatter(X[Y==1, 0], X[Y==1, 1], color='r', label='Class 1')\nsns.despine(); ax.legend()\nax.set(xlabel='X', ylabel='Y', title='Toy binary classification data set');","a3d6bf08":"# just to see the dimensions o four data\nprint(X_train.shape)\nprint(Y_train.shape)","68e6b2eb":"def construct_nn(ann_input, ann_output):\n    n_hidden = 5 # Number of neurons in each hidden layer\n    \n    # Initialize random weights between each layer\n    init_1 = np.random.randn(X.shape[1], n_hidden).astype(float)\n    init_2 = np.random.randn(n_hidden, n_hidden).astype(float)\n    init_out = np.random.randn(n_hidden).astype(float)\n        \n    with pm.Model() as neural_network:\n        # Weights from input to hidden layer\n        weights_in_1 = pm.Normal('w_in_1', 0, sd=1, \n                                 shape=(X.shape[1], n_hidden), \n                                 testval=init_1)\n        \n        # Weights from 1st to 2nd layer\n        weights_1_2 = pm.Normal('w_1_2', 0, sd=1, \n                                shape=(n_hidden, n_hidden), \n                                testval=init_2)\n        \n        # Weights from hidden layer to output\n        weights_2_out = pm.Normal('w_2_out', 0, sd=1, \n                                  shape=(n_hidden,), \n                                  testval=init_out)\n        \n        # Build neural-network using tanh activation function\n        act_1 = pm.math.tanh(pm.math.dot(ann_input, \n                                         weights_in_1))\n        act_2 = pm.math.tanh(pm.math.dot(act_1, \n                                         weights_1_2))\n        act_out = pm.math.sigmoid(pm.math.dot(act_2, \n                                              weights_2_out))\n        \n        # Binary classification -> Bernoulli likelihood\n        out = pm.Bernoulli('out', \n                           act_out,\n                           observed=ann_output,\n                           total_size=Y_train.shape[0] # IMPORTANT for minibatches\n                          )\n    return neural_network\n\n# Trick: Turn inputs and outputs into shared variables. \n# It's still the same thing, but we can later change the values of the shared variable \n# (to switch in the test-data later) and pymc3 will just use the new data. \n# Kind-of like a pointer we can redirect.\n# For more info, see: http:\/\/deeplearning.net\/software\/theano\/library\/compile\/shared.html\nann_input = theano.shared(X_train)\nann_output = theano.shared(Y_train)\nneural_network = construct_nn(ann_input, ann_output)","e5bd0ae4":"from pymc3.theanof import set_tt_rng, MRG_RandomStreams\nset_tt_rng(MRG_RandomStreams(42))","500a444d":"%%time\n\nwith neural_network:\n    inference = pm.ADVI()\n    approx = pm.fit(n=50000, method=inference)","228dbf8e":"trace = approx.sample(draws=5000)","452f74d5":"plt.plot(-inference.hist)\nplt.ylabel('ELBO')\nplt.xlabel('iteration');","385c82ca":"# Replace arrays our NN references with the test data\nann_input.set_value(X_test)\nann_output.set_value(Y_test)\n\nwith neural_network:\n    ppc = pm.sample_posterior_predictive(trace, samples=500, progressbar=False)\n\n# Use probability of > 0.5 to assume prediction of class 1\npred = ppc['out'].mean(axis=0) > 0.5","c4a22571":"fig, ax = plt.subplots()\nax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\nax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\nsns.despine()\nax.set(title='Predicted labels in testing set', xlabel='X', ylabel='Y');","0b4a78fb":"print('Accuracy = {}%'.format((Y_test == pred).mean() * 100))","c29b27de":"grid = pm.floatX(np.mgrid[-3:3:100j,-3:3:100j])\ngrid_2d = grid.reshape(2, -1).T\ndummy_out = np.ones(grid.shape[1], dtype=np.int8)","54b7a2f7":"ann_input.set_value(grid_2d)\nann_output.set_value(dummy_out)\n\nwith neural_network:\n    ppc = pm.sample_posterior_predictive(trace, samples=500, progressbar=True)","1fdb6be9":"cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\nfig, ax = plt.subplots(figsize=(14, 8))\ncontour = ax.contourf(grid[0], grid[1], ppc['out'].mean(axis=0).reshape(100, 100), cmap=cmap)\nax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\nax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\ncbar = plt.colorbar(contour, ax=ax)\n_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X', ylabel='Y');\ncbar.ax.set_ylabel('Posterior predictive mean probability of class label = 0');","ffc5214c":"cmap = sns.cubehelix_palette(light=1, as_cmap=True)\nfig, ax = plt.subplots(figsize=(14, 8))\ncontour = ax.contourf(grid[0], grid[1], ppc['out'].std(axis=0).reshape(100, 100), cmap=cmap)\nax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\nax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\ncbar = plt.colorbar(contour, ax=ax)\n_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X', ylabel='Y');\ncbar.ax.set_ylabel('Uncertainty (posterior predictive standard deviation)');","fe60be42":"import numpy as np\nnp.max( ppc['out'].std(axis=0))","b5453654":"minibatch_x = pm.Minibatch(X_train, batch_size=32)\nminibatch_y = pm.Minibatch(Y_train, batch_size=32)\n\nneural_network_minibatch = construct_nn(minibatch_x, minibatch_y)\nwith neural_network_minibatch:\n    inference = pm.ADVI()\n    approx = pm.fit(40000, method=inference)","6feb2f7a":"plt.plot(-inference.hist)\nplt.ylabel('ELBO')\nplt.xlabel('iteration');","9ab09138":"trace_VI = approx.sample(draws=10000)\npm.traceplot(trace_VI);","885e25c0":"Let's look at our predictions:","154289b4":"Looking at the standard deviation of the posterior predictive, we see that the neural network did a good job of splitting classes along the \"dividing line\" of each class, in a sort of double parabola sort of shape. The uncertainly increases, and the darker colors fan out towards +\/- one standard deviation.","97c5a05a":"### Model specification\n\nA neural network is quite simple. The basic unit is a [perceptron](https:\/\/en.wikipedia.org\/wiki\/Perceptron) which is nothing more than [logistic regression](http:\/\/pymc-devs.github.io\/pymc3\/notebooks\/posterior_predictive.html#Prediction). We use many of these in parallel and then stack them up to get hidden layers. Here we will use 2 hidden layers with 5 neurons each which is sufficient for such a simple problem.","00e9b3a7":"### 2. ","a8f2fe6b":"Looking at the weights of this neural network, it appears that there are at lot of overlapping distributions around -2, 0, and 2 in the second plot, and -1, 0, and 1 in the first. The third plot shows so overlapping around zero. Maybe there are too many neurons in this network, and some are not helpful in classification.","e8e463e2":"### Probability surface","bac32d82":"# Variational Inference: Bayesian Neural Networks\n\n**This was adapted from an excellent notebook here:**\n(c) 2016-2018 by Thomas Wiecki, updated by Maxim Kochurov\nOriginal blog post: https:\/\/twiecki.github.io\/blog\/2016\/06\/01\/bayesian-deep-learning\/","2678db63":"The two approaches seem to deliver very different results. Both models' BIC are similar, however the classification results seem to be negative images of one another. The only similarity I see is in the top left corner of the image.","25d54601":"### 1. ","0972567a":"$P(image class | data point) = \n(P(data point being from the given image class) P(image class)) \/\n(P(data point))$","f2e070e3":"Now that we trained our model, lets predict on the hold-out set using a posterior predictive check (PPC). ","52108950":"## Lets look at what the classifier has learned\n\nFor this, we evaluate the class probability predictions on a grid over the whole input space.","10b5a7d4":"The government would want to focus on a certain class, and separate probabilistic views if they want to per say, focus on paving streets, where streets may be classfied as a certain value from the image. They look largely unpaved. The government can uses these difference classes to gain insights on paving roads, the amount of housing, or the amount of vegetation.","f890967b":"## Joe Amoroso\n## ja6mw\n## DS 6040 Homework 4"}}