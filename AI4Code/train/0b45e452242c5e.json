{"cell_type":{"c3c6902b":"code","b9c12571":"code","b067f270":"code","4aacdc28":"code","b2452b8c":"code","2a74fb33":"code","c4bceeb1":"code","80a41d9a":"code","ca8a1887":"code","204ee732":"code","564b63a3":"code","b2f3c4cd":"code","e815ce0b":"code","f3ac3e00":"code","43d19b0e":"code","66d53b4b":"code","3f11622e":"code","29cf8b99":"code","2e9f68c0":"code","03e65c25":"code","4db09ebf":"code","07c9730c":"code","aa035b5f":"code","0c6e9b01":"code","fc6569d4":"code","75197fe4":"code","eacc6972":"code","6e536f5a":"code","2dac8367":"code","99eadd3e":"code","95533821":"code","81d01bb2":"code","85e05754":"code","0ea819ab":"code","3b0bb760":"code","8146b960":"code","7450fe84":"code","a41407b3":"code","e68ca76b":"code","ac275193":"code","424630ff":"code","e10bcc8d":"code","c06e1bb8":"code","ca31984f":"code","2e8a0e14":"code","89690b73":"code","a458efcf":"code","0607cb64":"code","863b56b4":"code","b17dbac5":"code","f40e0685":"code","6271b7e0":"code","bfaf13df":"code","9dc3a399":"code","8d2159f4":"code","283cd9fe":"code","bf6cab2a":"code","2addadee":"code","4ef7fe7c":"code","9c674118":"code","bb8f8c8e":"code","05d4f8ac":"code","a588de7d":"code","0d2e3e24":"code","c4cb8b1c":"code","0eb4f632":"code","f44ba887":"code","213bda11":"code","33e4a767":"code","c374bb6e":"code","e228a83e":"code","64b71948":"code","35e3a488":"code","f5d5bfd9":"code","fcb3116e":"code","dfe9535f":"code","388f997b":"markdown","906113a2":"markdown","cb2ecb0e":"markdown","70f96431":"markdown","df580758":"markdown","4fe89690":"markdown","8fc04e2e":"markdown","5c4bf501":"markdown","394d3931":"markdown","2bf42a4a":"markdown","6523b98d":"markdown","f8b486f6":"markdown","73a6dfbe":"markdown","72a42cb1":"markdown","6377551e":"markdown","1d3268ff":"markdown","b2585966":"markdown","bc90676b":"markdown","8ffb1c7b":"markdown","a4e1b22a":"markdown","34023372":"markdown","cacb3ed4":"markdown","8b5eb735":"markdown","0f089922":"markdown","bb279bf5":"markdown","42d4a4cd":"markdown","2e0f006b":"markdown"},"source":{"c3c6902b":"#import modules\n\nimport numpy as np #scientific computing library\nimport pandas as pd  #data analysis and manipulation library\n\nimport matplotlib.pyplot as plt #library for creating static, animated, and interactive visualizations \nimport seaborn as sns #data visualization library based on matplotlib\nsns.set_context(\"notebook\", font_scale=1.2, rc={\"lines.linewidth\": 2.5})\n# sns.set(style=\"white\", context=\"talk\") #set to a specific seaborn plot style\n\nimport plotly.offline as py # to create interactive, publication-quality graphs\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff","b9c12571":"df = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\ndf.head(4)","b067f270":"print(\"ATTRIBUTES OVERVIEW\"+\"\\n\"+\"-\"*20+\"\\n\")\nprint(df.info())\nprint(\"\\n\"+\"HOW MANY UNIQUE VALUES PER ATTRIBUTE?\"+\"\\n\"+\"-\"*40+\"\\n\")\nprint(df.nunique())","4aacdc28":"#Check all attributes for cells with whitespaces\ndef check_all_columns_for_whitespaces(the_df):\n    for c in the_df.columns:\n        for i in the_df[str(c)]:\n            if str(i) == \" \":\n                print(str(c)+' has a cell with a whitespace')\n            else:\n                pass\n\ncheck_all_columns_for_whitespaces(df)","b2452b8c":"#Replace whitespaces in TotalCharges with NAN (not a number: numeric data type to represent any value that is undefined)\ndf[\"TotalCharges\"] = df[\"TotalCharges\"].replace(\" \",np.nan)\n\n#Remove NAN instances from the dataframe\ndf = df[df[\"TotalCharges\"].notnull()]\n#Reset the index\ndf = df.reset_index()[df.columns]\n\n#TotalCharges is of dtype object, while it contains continuous numerical values, therefor change to float type\ndf['TotalCharges'] = df['TotalCharges'].astype(float)\n\n#ID is useless, also no relationship between numeric part of ID and target value.\ndf = df.drop('customerID',axis=1)\n\n#Replace \nyn_map = { 0:'No',1:'Yes' }\ndf['SeniorCitizen'] = df['SeniorCitizen'].map(yn_map)","2a74fb33":"print(df['InternetService'].unique())\nprint(\"Before: \"+str(df['MultipleLines'].unique())) #No phone service = No\n\n#Replace \"No phone service\" with \"No\"\ndf['MultipleLines'] = df['MultipleLines'].replace('No phone service','No')\nprint(\"After: \"+str(df['MultipleLines'].unique()))","c4bceeb1":"#All the following attributed have a 3th label \"No internet service\"\n#Replace it with equivalent \"No\"\n\nprint(\"Before:\"+\"\\n\")\nfor c in df[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']]:\n    print(df[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']][str(c)].unique())\n    \nto_replace_columns = ['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']\n\nfor i in to_replace_columns:\n    df[i] = df[i].replace('No internet service','No')\n\nprint(\"\\n\"+\"After:\"+\"\\n\")\nfor c in df[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']]:\n    print(df[['OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport','StreamingTV','StreamingMovies']][str(c)].unique())","80a41d9a":"print(df['Contract'].unique())\nprint(df['PaymentMethod'].unique())","ca8a1887":"#df for churn and non churn customers\nchurn     = df[df[\"Churn\"] == \"Yes\"]\nnot_churn = df[df[\"Churn\"] == \"No\"]\n\n#Separating catagorical and numerical columns\ntarget_col = [\"Churn\"]\ncat_cols   = df.nunique()[df.nunique() < 17].keys().tolist()\ncat_cols   = [x for x in cat_cols if x not in target_col] # remove target column\nnum_cols   = [x for x in df.columns if x not in cat_cols + target_col]","204ee732":" for c in num_cols:\n    sns.distplot(df[c])\n    plt.grid()\n    plt.show()","564b63a3":"sns.pairplot(df[['tenure','MonthlyCharges','TotalCharges','Churn']], hue = 'Churn',plot_kws = {'alpha': 0.45})\nplt.show()","b2f3c4cd":"#After EDA the continuous numerical values can be binned\n#Data binning: a way to group numbers of more or less continuous values into a smaller number of \"bins\"\n\n#discrete numerical values\n\n#Bin tenure even further\ndf['tenure_bin_round'] = np.array(np.floor(np.array(df['tenure']) \/ 4.))\nprint(\"Reducing from {} bins to {} bins\".format(str(df['tenure'].nunique()),str(df['tenure_bin_round'].nunique())))\n\n#continuous numerical values\n\n#Bin the monthly charges.\ndf['MonthlyCharges_bin_round'] = np.array(np.floor(np.array(df['MonthlyCharges']) \/ 10.))\nprint(\"Reducing from {} unique values to {} bins\".format(str(df['MonthlyCharges'].nunique()),str(df['MonthlyCharges_bin_round'].nunique())))\n\n#Bin the total charges.\ndf['TotalCharges_bin_round'] = np.array(np.floor(np.array(df['TotalCharges']) \/ 1000.))\nprint(\"Reducing from {} unique values to {} bins\".format(str(df['TotalCharges'].nunique()),str(df['TotalCharges_bin_round'].nunique())))\n","e815ce0b":"#drop original variables after binning\ndf = df.drop(['tenure','MonthlyCharges','TotalCharges'],axis=1)","f3ac3e00":"print(\"People churning {0:,.2f}%\".format(100*(len(churn)\/len(df))))\nprint(\"People not churning {0:,.2f}%\".format(100*(len(not_churn)\/len(df))))","43d19b0e":"sns.countplot(df['Churn'])\nplt.show()","66d53b4b":"for c in cat_cols:\n    plt.figure(figsize=(15,4))\n    sns.countplot(df[c])\n    plt.grid()\n    plt.show()","3f11622e":"for c in cat_cols:\n    plt.figure(figsize=(15,4))\n    sns.countplot(x=c, hue=\"Churn\", data=df)\n    plt.grid()\n    plt.show()","29cf8b99":"#Change dtype of binned variables to int\nprint(type(df['TotalCharges_bin_round'].iloc[2]))\n\nconvert_to_int = ['tenure_bin_round','MonthlyCharges_bin_round','TotalCharges_bin_round']\n\nfor c in convert_to_int:\n    df[c] = df[c].astype(int)\n    \nprint(type(df['TotalCharges_bin_round'].iloc[2]))","2e9f68c0":"print(df.shape)\ndf.head(4)","03e65c25":"#Attributes\nX = df.drop('Churn',axis=1)\nprint(X.shape)\n\n#Target\ny = df['Churn']#)np.array(.reshape(-1,1)\nprint(y.shape)","4db09ebf":"print(X.shape)\nprint(y.shape)","07c9730c":"X.columns","aa035b5f":"#Encode categorical variables\nfrom sklearn.preprocessing import OrdinalEncoder\nohe = OrdinalEncoder()\nX = pd.DataFrame(ohe.fit_transform(X),columns=['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService',\n       'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup',\n       'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies',\n       'Contract', 'PaperlessBilling', 'PaymentMethod', 'tenure_bin_round',\n       'MonthlyCharges_bin_round', 'TotalCharges_bin_round'])   \nX = X.astype(int) # convert from float to int\nprint(X.shape)\nprint(type(X))\n\n#Encode target labels with value between 0 and n_classes-1.\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = pd.DataFrame(le.fit_transform(y))\nprint(y.shape)\nprint(type(y))","0c6e9b01":"X.nunique()>2","fc6569d4":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import make_column_transformer\ncolumn_trans = make_column_transformer((OneHotEncoder(),['InternetService','Contract','PaymentMethod',\n                                                         'tenure_bin_round','MonthlyCharges_bin_round',\n                                                         'TotalCharges_bin_round']),remainder='passthrough')\nprint(column_trans.fit_transform(X).shape)\ncolumn_trans.fit_transform(X)","75197fe4":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline","eacc6972":"logreg=LogisticRegression(solver='lbfgs',max_iter=7600)\npipe = make_pipeline(column_trans,logreg)","6e536f5a":"y = y[0]","2dac8367":"from sklearn.model_selection import cross_val_score\nprint(\"Accuracy: {0:,.3f}%\".format(cross_val_score(pipe,X, y,cv=10,scoring='accuracy').mean()))\nprint(\"recall: {0:,.3f}%\".format(cross_val_score(pipe,X, y,cv=10,scoring='recall').mean()))\nprint(\"precision: {0:,.3f}%\".format(cross_val_score(pipe,X, y,cv=10,scoring='precision').mean()))\nprint(\"f1: {0:,.3f}%\".format((cross_val_score(pipe,X, y,cv=10,scoring='f1').mean())))","99eadd3e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix","95533821":"logmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)\nlog_predictions = logmodel.predict(X_test)\nprint(classification_report(y_test,log_predictions))\nplot_confusion_matrix(logmodel, X_test, y_test)\nplt.show()\nplot_confusion_matrix(logmodel, X_test, y_test, normalize='true')\nplt.show()","81d01bb2":"# first 10 prediction responses\nprint(logmodel.predict(X_test)[0:10]) \nprint('\\n')\n# first 10 predicted probabilities of class membership\nprint(logmodel.predict_proba(X_test)[0:10]) \nprint('\\n')\n# first 10 predicted probabilities of class 1\nprint(logmodel.predict_proba(X_test)[0:10,1])\n\n#place results in variable\ny_logistic_pred_prob = np.array(logmodel.predict_proba(X_test)[:,1]).reshape(-1,1)","85e05754":"plt.rcParams['font.size'] = 14\nplt.hist(logmodel.predict_proba(X_test)[:,1], bins=8)\nplt.xlim(0,1)\nplt.title('Histogram of predicted probabilities')\nplt.xlabel('Predicted probabilities of churn')\nplt.ylabel('Frequency')\nplt.show()","0ea819ab":"from sklearn.preprocessing import binarize\ny_logistic_pred_class = binarize(y_logistic_pred_prob,threshold=0.4)[:]","3b0bb760":"y_logistic_pred_prob[0:10][:,0]","8146b960":"y_logistic_pred_class[0:10][:,0]","7450fe84":"confusion_matrix(y_test,log_predictions)","a41407b3":"confusion_matrix(y_test,y_logistic_pred_class)","e68ca76b":"print(\"Increase in sensitivity (recall) from {:0.2f} to {:0.2f}\".format((320\/(320+290)),416\/(416+194)))","ac275193":"print(\"Decrease in specificity from {:0.2f} to {:0.2f}\".format((1529\/(1529+182)),1412\/(1412+299)))","424630ff":"from sklearn.svm import SVC\nsvc_model = SVC(kernel='rbf',random_state=4,probability=True)\nsvc_model.fit(X_train, y_train)\nSVM_predictions = svc_model.predict(X_test)\nprint(classification_report(y_test,SVM_predictions))\nplot_confusion_matrix(svc_model, X_test, y_test)\nplt.show()\nplot_confusion_matrix(svc_model, X_test, y_test, normalize='true')\nplt.show()","e10bcc8d":"from sklearn import tree","c06e1bb8":"tree_clf = tree.DecisionTreeClassifier(criterion='entropy')\ntree_clf = tree_clf.fit(X_train, y_train)\ntree_predictions = tree_clf.predict(X_test)","ca31984f":"print(classification_report(y_test,tree_predictions))\nplot_confusion_matrix(tree_clf, X_test, y_test)\nplt.show()\nplot_confusion_matrix(tree_clf, X_test, y_test, normalize='true')\nplt.show()","2e8a0e14":"import graphviz \ndot_data = tree.export_graphviz(tree_clf, out_file=None) \ngraph = graphviz.Source(dot_data) \ngraph.render(\"beautiful_tree\")","89690b73":"#GaussianNB implements the Gaussian Naive Bayes algorithm for classification. \n\nfrom sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb_predictions = gnb.fit(X_train, y_train).predict(X_test)\nprint(\"Number of mislabeled points out of a total {} points : {}\".format(X_test.shape[0], (y_test != gnb_predictions).sum()))","a458efcf":"print(classification_report(y_test,gnb_predictions))\nprint('\\n')\nplot_confusion_matrix(gnb, X_test, y_test)\nprint('\\n')\nplot_confusion_matrix(gnb, X_test, y_test, normalize='true')","0607cb64":"# store the predicted probabilities for class 1\ny_logistic_pred_prob = np.array(logmodel.predict_proba(X_test)[:,1]).reshape(-1,1)\ny_svm_pred_prob = np.array(svc_model.predict_proba(X_test)[:,1]).reshape(-1,1)\ny_tree_prob = np.array(tree_clf.predict_proba(X_test)[:,1]).reshape(-1,1)\ny_gnb_prob = np.array(gnb.predict_proba(X_test)[:,1]).reshape(-1,1)","863b56b4":"from sklearn.metrics import roc_curve, auc\n\n#logistic tpr and fpr\nlogistic_fpr, logistic_tpr, threshold_log = roc_curve(y_test, y_logistic_pred_prob)\nauc_logistic = auc(logistic_fpr, logistic_tpr)\n\n#SVM tpr and fpr\nsvm_fpr, svm_tpr, threshold_svm = roc_curve(y_test, y_svm_pred_prob)\nauc_svm = auc(svm_fpr, svm_tpr)\n\n#Tree tpr and fpr\ntree_fpr, tree_tpr, threshold_tree = roc_curve(y_test, y_tree_prob,drop_intermediate=False)\nauc_tree = auc(tree_fpr, tree_tpr)\n\n#GNB tpr and fpr\ngnb_fpr, gnb_tpr, threshold_gnb = roc_curve(y_test, y_gnb_prob)\nauc_gnb = auc(gnb_fpr, gnb_tpr)\n\n\n#plot\nplt.figure(figsize=(7,7), dpi=100)\nplt.plot(svm_fpr, svm_tpr, linestyle='-', label='SVM (auc = %0.3f)' % auc_svm)\nplt.plot(logistic_fpr, logistic_tpr, marker='.', label='Logistic (auc = %0.3f)' % auc_logistic)\nplt.plot(tree_fpr, tree_tpr, marker='.', label='Tree (auc = %0.3f)' % auc_tree)\nplt.plot(gnb_fpr, gnb_tpr, marker='.', label='GNB (auc = %0.3f)' % auc_gnb)\n\nplt.xlabel('FPR = 1-specificity')\nplt.ylabel('TPR = sensitivity')\nplt.title('ROC')\nplt.grid(True)\nplt.legend()\nplt.show()","b17dbac5":"# calculate cross-validated AUC\nfrom sklearn.model_selection import cross_val_score\n\n#Logistic regression\nprint(cross_val_score(logmodel, X, y, cv=10, scoring='roc_auc').mean())\n#SVM\nprint(cross_val_score(svc_model, X, y, cv=10, scoring='roc_auc').mean())\n#Decision tree\nprint(cross_val_score(tree_clf, X, y, cv=10, scoring='roc_auc').mean())\n#GNB\nprint(cross_val_score(gnb, X, y, cv=10, scoring='roc_auc').mean())","f40e0685":"# calculate AUC with metrics.roc_auc_score\nfrom sklearn import metrics\nprint(metrics.roc_auc_score(y_test, y_logistic_pred_prob))\nprint(metrics.roc_auc_score(y_test, y_svm_pred_prob))\nprint(metrics.roc_auc_score(y_test, y_tree_prob))\nprint(metrics.roc_auc_score(y_test, y_gnb_prob))","6271b7e0":"roc_list = []\nacc = []\nprec = []\nrec = []\n\nfor spc in np.linspace(0.1,0.9,num=50):\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=spc, random_state=42)\n    logmodel = LogisticRegression()\n    logmodel.fit(X_train,y_train)\n    log_predictions = logmodel.predict(X_test)\n    roc_list.append(cross_val_score(logmodel, X_test, y_test, cv=10, scoring='roc_auc').mean())   \n    acc.append((confusion_matrix(y_test, log_predictions)[0][0]+\n               confusion_matrix(y_test, log_predictions)[1][1])\/(\n               confusion_matrix(y_test, log_predictions)[0][0]+\n              confusion_matrix(y_test, log_predictions)[0][1]+\n              confusion_matrix(y_test, log_predictions)[1][0]+\n              confusion_matrix(y_test, log_predictions)[1][1]))\n    prec.append((confusion_matrix(y_test, log_predictions)[1][1])\/(               \n              confusion_matrix(y_test, log_predictions)[0][1]+              \n              confusion_matrix(y_test, log_predictions)[1][1]))\n    rec.append((confusion_matrix(y_test, log_predictions)[1][1])\/(               \n              confusion_matrix(y_test, log_predictions)[1][1]+              \n              confusion_matrix(y_test, log_predictions)[1][0]))","bfaf13df":"plt.figure(figsize=(5,5), dpi=100)\nplt.plot(np.linspace(0.1,0.9,num=50), roc_list)\nplt.xlim(0,1)\nplt.xlabel('test_size')\nplt.ylabel('AUC')\nplt.title('AUC vs. test_size')\nplt.grid(True)\nplt.legend()\nplt.show()","9dc3a399":"max(roc_list)","8d2159f4":"plt.figure(figsize=(5,5), dpi=100)\nplt.plot(np.linspace(0.1,0.9,num=50), acc)\nplt.xlim(0,1)\nplt.xlabel('test_size')\nplt.ylabel('Accuracy')\nplt.title('Accuracy vs. test_size')\nplt.grid(True)\nplt.legend()\nplt.show()","283cd9fe":"plt.figure(figsize=(5,5), dpi=100)\nplt.plot(np.linspace(0.1,0.9,num=50), prec)\nplt.xlim(0,1)\nplt.xlabel('test_size')\nplt.ylabel('Precision')\nplt.title('Precision vs. test_size')\nplt.grid(True)\nplt.legend()\nplt.show()","bf6cab2a":"plt.figure(figsize=(5,5), dpi=100)\nplt.plot(np.linspace(0.1,0.9,num=50), rec)\nplt.xlim(0,1)\nplt.xlabel('test_size')\nplt.ylabel('Recall')\nplt.title('Recall vs. test_size')\nplt.grid(True)\nplt.legend()\nplt.show()","2addadee":"print(X.shape)\nprint(X.columns)","4ef7fe7c":"df_joined = X\ndf_joined['Churn'] = y\ndf_copy = df_joined","9c674118":"plt.figure(figsize=(18,10))\nsns.heatmap(df_joined.corr(),annot=True, fmt=\".2\")","bb8f8c8e":"correlations = pd.DataFrame(df_joined.corr()['Churn'])\ncorrelations = correlations.drop('Churn',axis=0)\ncorrelations.sort_values(by=['Churn'],ascending=False)","05d4f8ac":"correlations.sort_values(by=['Churn'],ascending=False).plot(kind='bar',figsize=(10,5))\nplt.grid()\nplt.show()","a588de7d":"features_to_drop = correlations[abs(correlations['Churn'])<0].reset_index()['index'].to_list()\nprint(features_to_drop)","0d2e3e24":"df_joined = df_copy\ndf_joined = df_joined.drop(features_to_drop,axis=1)","c4cb8b1c":"X_features_reduced = df_joined.drop('Churn',axis=1)","0eb4f632":"X_train, X_test, y_train, y_test = train_test_split(X_features_reduced, y, test_size=0.35, random_state=42)\n\nlogmodel_features_reduces = LogisticRegression()\nlogmodel_features_reduces.fit(X_train,y_train)\nlog_predictions_features_reduces = logmodel_features_reduces.predict(X_test)\n\nsvc_model_features_reduces = SVC(kernel='rbf',random_state=4,probability=True)\nsvc_model_features_reduces.fit(X_train, y_train)\nSVM_predictions_features_reduces = svc_model_features_reduces.predict(X_test)\n\ntree_clf_features_reduces = tree.DecisionTreeClassifier(criterion='entropy')\ntree_clf_features_reduces = tree_clf.fit(X_train, y_train)\ntree_predictions_features_reduces = tree_clf_features_reduces.predict(X_test)\n\ngnb_features_reduces = GaussianNB()\ngnb_predictions_features_reduces = gnb_features_reduces.fit(X_train, y_train).predict(X_test)\n\n# store the predicted probabilities for class 1\ny_logistic_pred_prob_features_reduces = np.array(logmodel_features_reduces.predict_proba(X_test)[:,1]).reshape(-1,1)\ny_svm_pred_prob_features_reduces = np.array(svc_model_features_reduces.predict_proba(X_test)[:,1]).reshape(-1,1)\ny_tree_prob_features_reduces = np.array(tree_clf_features_reduces.predict_proba(X_test)[:,1]).reshape(-1,1)\ny_gnb_prob_features_reduces = np.array(gnb_features_reduces.predict_proba(X_test)[:,1]).reshape(-1,1)\n\n#logistic tpr and fpr\nlogistic_fpr, logistic_tpr, threshold_log = roc_curve(y_test, y_logistic_pred_prob_features_reduces)\nauc_logistic = auc(logistic_fpr, logistic_tpr)\n\n#SVM tpr and fpr\nsvm_fpr, svm_tpr, threshold_svm = roc_curve(y_test, y_svm_pred_prob_features_reduces)\nauc_svm = auc(svm_fpr, svm_tpr)\n\n#Tree tpr and fpr\ntree_fpr, tree_tpr, threshold_tree = roc_curve(y_test, y_tree_prob_features_reduces,drop_intermediate=False)\nauc_tree = auc(tree_fpr, tree_tpr)\n\n#GNB tpr and fpr\ngnb_fpr, gnb_tpr, threshold_gnb = roc_curve(y_test, y_gnb_prob_features_reduces)\nauc_gnb = auc(gnb_fpr, gnb_tpr)\n\n\n#plot\nplt.figure(figsize=(8,8), dpi=100)\nplt.plot(svm_fpr, svm_tpr, linestyle='-', label='SVM (auc = %0.3f)' % auc_svm)\nplt.plot(logistic_fpr, logistic_tpr, marker='.', label='Logistic (auc = %0.3f)' % auc_logistic)\nplt.plot(tree_fpr, tree_tpr, marker='.', label='Tree (auc = %0.3f)' % auc_tree)\nplt.plot(gnb_fpr, gnb_tpr, marker='.', label='GNB (auc = %0.3f)' % auc_gnb)\n\nplt.xlabel('FPR = 1-specificity')\nplt.ylabel('TPR = sensitivity')\nplt.title('ROC with {} features'.format(len(X_test.columns)))\nplt.grid(True)\nplt.legend()\nplt.show()\n\nprint('-'*30)\nprint('log_predictions_features_reduces')\nprint(classification_report(y_test,log_predictions_features_reduces))\nprint('-'*30)\nprint('SVM_predictions_features_reduces')\nprint(classification_report(y_test,SVM_predictions_features_reduces))\nprint('-'*30)\nprint('tree_predictions_features_reduces')\nprint(classification_report(y_test,tree_predictions_features_reduces))\nprint('-'*30)\nprint('gnb_predictions_features_reduces')\nprint(classification_report(y_test,gnb_predictions_features_reduces))","f44ba887":"X_test","213bda11":"X_test = X_test.reset_index()[X_test.columns]\ny_test = pd.DataFrame(y_test)\ny_test = y_test.reset_index()[y_test.columns]\nX_test.shape","33e4a767":"print(np.array(X_test.iloc[400].to_list()).reshape(1,-1))\nprint(y_test.iloc[400].to_list())","c374bb6e":"random_person = np.array(X_test.iloc[np.random.randint(0, 703 + 1)].to_list()).reshape(1,-1)\n\nprint(logmodel.predict(random_person))\nprint(logmodel.predict_proba(random_person))","e228a83e":"def will_i_churn_or_not(gender,\n                     SeniorCitizen,\n                     Partner,\n                     Dependents,\n                     PhoneService,\n                     MultipleLines,\n                     InternetService,\n                     OnlineSecurity,\n                     OnlineBackup,\n                     DeviceProtection,\n                     TechSupport,\n                     StreamingTV,\n                     StreamingMovies,\n                     Contract,\n                     PaperlessBilling,\n                     PaymentMethod,\n                     tenure_bin_round,\n                     MonthlyCharges_bin_round,\n                     TotalCharges_bin_round):\n    \n    my_df = pd.DataFrame(data=[gender,\n                     SeniorCitizen,\n                     Partner,\n                     Dependents,\n                     PhoneService,\n                     MultipleLines,\n                     InternetService,\n                     OnlineSecurity,\n                     OnlineBackup,\n                     DeviceProtection,\n                     TechSupport,\n                     StreamingTV,\n                     StreamingMovies,\n                     Contract,\n                     PaperlessBilling,\n                     PaymentMethod,\n                     tenure_bin_round,\n                     MonthlyCharges_bin_round,\n                     TotalCharges_bin_round]).transpose()\n    \n    the_probability = logmodel.predict_proba(my_df)\n    \n    the_prediction = logmodel.predict(my_df)\n    \n    if the_prediction == 0:\n        print('The probability you will not churn is {:.3f}%'.format(the_probability[0][0]))\n    else:\n        print('The probability you will churn is {:.3f}%'.format(the_probability[0][1]))","64b71948":"random_person = np.array(X_test.iloc[np.random.randint(0, 703 + 1)].to_list()).reshape(1,-1)","35e3a488":"tupled_person = tuple(random_person[0])","f5d5bfd9":"will_i_churn_or_not(*tupled_person)","fcb3116e":"churn_probability = pd.DataFrame(logmodel.predict_proba(X_test)[:,1], columns=['Churn_probability']) \n#for all the predictions (rows), give me the probabilities of churning (1)","dfe9535f":"churn_probability.sort_values(by='Churn_probability',ascending=False).head(20)","388f997b":"## <a id='2'>2. Data understanding<\/a>\n\nEach row represents a customer, each column contains customer\u2019s attributes described on the column Metadata.\n\nThe dataset includes information about:\n\n* Customers who left within the last month \u2013 the column is called Churn\n* Services that each customer has signed up for \u2013 phone, multiple lines, internet, online security, online backup, device protection, tech support,and streaming TV and movies\n* Customer account information \u2013 how long they\u2019ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n* Demographic info about customers \u2013 gender, age range, and if they have partners and dependents","906113a2":"### Categorical variables","cb2ecb0e":"# Telco customer churn\n\n## <a id='1'>1. Business understanding<\/a>\n\nSince the cell phone market is now saturated, the huge growth in the wireless market has tapered off. Therefor as a telcom businnes attracting new customers is much more expensive than retaining existing ones. Therefor a large part of the marketing budget should go into preventing churn. The goal of this notebook is to decide which customers should be offered a special retention deal. \n\n<b>Interesting fact:<\/b> the earliest adopters of data mining were telcom businesses to maintain customer retention. <br>(Provost F., Fawcett T)\n\n\n <img src=\"https:\/\/images.unsplash.com\/photo-1533664488202-6af66d26c44a?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1000&q=80\" width=\"400\" height=\"60\" style=\"float:left\"> ","70f96431":"## <a id='8'> 8. Ranking top 20 customers most likely to churn<\/a>","df580758":"### <a id='2.1'>2.1 Data manipulation<\/a>","4fe89690":"### <a id='6.5'>6.5. ROC and AUC<\/a>\n\n\u201cA receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. It is created by plotting the fraction of true positives out of the positives (TPR = true positive rate) vs. the fraction of false positives out of the negatives (FPR = false positive rate), at various threshold settings. TPR is also known as sensitivity, and FPR is one minus the specificity or true negative rate.\u201d\n\nROC curves typically feature true positive rate on the Y axis, and false positive rate on the X axis. This means that the top left corner of the plot is the \u201cideal\u201d point - a false positive rate of zero, and a true positive rate of one. This is not very realistic, but it does mean that a larger area under the curve (AUC) is usually better.","8fc04e2e":"### <a id='6.3'>6.3. Decision tree<\/a>\n\nDecision Trees (DTs) are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.","5c4bf501":"- For all four performances 0.2 < test_size < 0.4 seems the best choise.\n- Once above 0.4 less and less data will be used to train the model and overfitting is likely to occur.\n- I decided to go for 0.35.","394d3931":"- Removing features with abs(correlation) <0.05 did not improve the performance for any of the 4 models.\n- I also experimenten with a for loop to try all the different correlation tresholds, ranging from all the features to only 1 feature to train the model. The best performance was using all the features.","2bf42a4a":"## <a id='4'>4. Model<\/a>","6523b98d":"### <a id='6.4'>6.4. Naive Bayes<\/a>\n\nNaive Bayes methods are a set of supervised learning algorithms based on applying Bayes\u2019 theorem with the \u201cnaive\u201d assumption of conditional independence between every pair of features given the value of the class variable.","f8b486f6":"### <a id='6.1'>6.1. Logistic classifier<\/a>","73a6dfbe":"### <a id='6.7'>6.7. Feature selection impact on the performance of all 4 models<\/a>","72a42cb1":"### <a id='6.6'>6.6. Effect of changing the test size on model performance (logistic regression)<\/a>","6377551e":"## <a id='5'>5. Evaluation<\/a>\n\nUse cross-validation (CV) for evaluation of the model.\n\nCV is a resampling procedure used to evaluate machine learning models on a limited data sample. The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. \n\nCross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n\n<div class=\"img-with-text\">\n  <img src=\"https:\/\/www.justintodata.com\/wp-content\/uploads\/2020\/06\/image-8.png\" alt=\"image\" style=\"width:45%\"  align=\"left\">","1d3268ff":"* Senior citizen: in the US they consider someone from 60 years old onwards to be a senior citizen.\n* Dependents: having to provide support for family members.\n* Tenure: the period of time a person holds a position.\n* Multiple lines: a multi-line phone system condenses multiple lines into a single device which means that more than one person will be able to make or receive calls at the same time.","b2585966":"## <a id='6'>6. Performance comparison of different models<\/a>\n\n- Modeling performance from now one will be without pipeline and without \"make_column_transformer\" of features with >2 labels\n- So the following features have more than 2 labels:\n        -['InternetService',\n        'Contract',\n        'PaymentMethod',\n        'tenure_bin_round',\n        'MonthlyCharges_bin_round',\n        'TotalCharges_bin_round']","bc90676b":"Impact of removing features with abs(cor)<0.1","8ffb1c7b":"## <a id='7'>7. Create function to predict churn probability via API<\/a>","a4e1b22a":"### <a id='6.2'>6.2. SVM classifier<\/a>","34023372":"If we would lower the threshold from 0.5 to 0.4, more people would be predicted to churn. (sensitivity of the classifier)","cacb3ed4":"### <a id='2.2'>2.2 Exploratory data analysis (EDA)<\/a>","8b5eb735":"## Overview \n\n- <a href='#1'>1. Business understanding<\/a>\n- <a href='#2'>2. Data understanding<\/a>\n    - <a href='#2.1'>2.1. Data manipulation<\/a>\n    - <a href='#2.2'>2.2. Exploratory data analysis (EDA)<\/a>\n- <a href='#3'>3. Data preparation<\/a>\n- <a href='#4'>4. Model<\/a>\n- <a href='#5'>5. Evaluation<\/a>\n- <a href='#6'>6. Performance comparison of different models<\/a>\n    - <a href='#6.1'>6.1. Logistic classifier<\/a>\n        - <a href='#6.1.1'>6.1.1 Impact of changing the threshold<\/a>\n    - <a href='#6.2'>6.2. SVM classifier<\/a>\n    - <a href='#6.3'>6.3. Decision tree<\/a>    \n    - <a href='#6.4'>6.4. Naive Bayes (GNB)<\/a>    \n    - <a href='#6.5'>6.5. ROC and AUC<\/a>    \n    - <a href='#6.6'>6.6. Effect of changing the test size on logistic regression model performance<\/a>    \n    - <a href='#6.7'>6.7. Feature selection impact on model performance (logistic regression)<\/a>    \n- <a href='#7'>7. Create function to predict churn probability via API<\/a>\n- <a href='#8'>8. Ranking top 20 customers most likely to churn <\/a>","0f089922":"### Numerical variables","bb279bf5":"## <a id='3'>3. Data preparation<\/a>","42d4a4cd":"* Customers with low tenure are more likely to churn.\n* Customers with high monthly charges are more likely to churn.\n","2e0f006b":"### <a id='6.1.1'>6.1.1. Impact of changing the threshold<\/a>\n\n- Threshold of 0.5 (probability) used by default for binary classification\n- Changing the threshold changes the sensitivity and specificity (they have an inverse relationship) of the model.\n- First put time and effort in making a good model, changing the threshold is something you can do at the end.\n- Depending on usecase increase or decrease threshold."}}