{"cell_type":{"f7069b12":"code","fec1d092":"code","e6475556":"code","1c11f3c2":"code","56d30bcd":"code","2e61147c":"code","8ec146d4":"code","1fd0ee12":"code","50796131":"code","9b5a663f":"code","c5f094be":"code","d2193d75":"code","8570c323":"code","208c3c9b":"code","c1b61981":"code","9a987aa4":"code","cf5118b7":"code","0718a173":"code","b47638e3":"code","51e04deb":"code","8a6f7c6a":"code","c36638bd":"code","8ccaae63":"code","f14311b4":"code","dd4f28c8":"code","91b63bcd":"code","63fabb35":"code","c1d035f6":"markdown","6f7a07e8":"markdown","f9f64cf3":"markdown","20534082":"markdown"},"source":{"f7069b12":"!nvidia-smi","fec1d092":"!git clone https:\/\/github.com\/huggingface\/transformers.git\n!pip install -U .\/transformers\n!pip install git+https:\/\/github.com\/huggingface\/nlp.git","e6475556":"import torch\nimport nlp\nfrom transformers import LongformerTokenizerFast","1c11f3c2":"tokenizer = LongformerTokenizerFast.from_pretrained('allenai\/longformer-base-4096')","56d30bcd":"import torch\nfrom transformers import LongformerTokenizer, LongformerForQuestionAnswering\n\ntokenizer = LongformerTokenizer.from_pretrained(\"valhalla\/longformer-base-4096-finetuned-squadv1\")\nmodel = LongformerForQuestionAnswering.from_pretrained(\"valhalla\/longformer-base-4096-finetuned-squadv1\")\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# # print (\"device \",device)\n# model = model.to(device)\n\ntext = \"Huggingface has democratized NLP. Huge thanks to Huggingface for this.\"\nquestion = \"What has Huggingface done ?\"\nencoding = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\ninput_ids = encoding[\"input_ids\"]\n\n# default is local attention everywhere\n# the forward method will automatically set global attention on question tokens\nattention_mask = encoding[\"attention_mask\"]\n\nstart_scores, end_scores = model(input_ids, attention_mask=attention_mask)\nall_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n\nanswer_tokens = all_tokens[torch.argmax(start_scores) :torch.argmax(end_scores)+1]\nanswer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n# output => democratized NLP","2e61147c":"import gc\ngc.collect()","8ec146d4":"print(answer)","1fd0ee12":"text = '''Essay Topic: Learning at Home during Lockdown:\nMy Parents and My Teachers\n\nEver since the lockdown started, I feel lonely at home.\nI do have a brother but soon realized that talking to a person\nor doing the same thing consistently can get monotonous.\nSometimes, I even feel that it would be better to go to school,\nwhich a month-back I could not have thought of in a million\nyears.At my house, both my parents are doctors. Not that they do\nnot have holidays, they do! Somehow, the holidays do not\nseem enough.My parents are treating COVID-19 patients and often discuss their healthcare. \nAt times, I \u001fnd their conversations scary and mom calms me down by saying this will end soon.\nYet, I am hardly convinced with her explanations. In the little time that I get to talk to \nmy friends, we discuss the current situation due to pandemic and its advantages, especially \non the environment, as us human beings are in lockdown.\nA few days ago, when my father and I were sitting in the balcony at night I looked up in the\nsky and saw a lot more stars than I usually get to see. Even my mom told me that Yamuna\nriver is getting cleaner amidst the lockdown.\nI also feel that my friends have their parents at home, spending quality time with them and\nall having fun times, together. While they have fun, my parents are at the hospital treating\npatients and, of course, this is something that makes me very proud. Still, it is not the same\nas having them at home.\nHowever, the advantage of not having parents at home is that I do not have to do any work\nuntil they are back. A few weeks ago, I panicked thinking that I would not get to celebrate my\nbirthday on its due date, just as it was not celebrated the previous three consecutive years on\nthe birthday day, since my parents were busy treating patients of either typhoid, pneumonia\nor dengue. A sigh of relief, this year it does not matter that much as long as my family and I\nare safe.\nI am also anxious about school; I hope that they do not take away our summer holidays to\nmake up for the missed school days. I always enjoyed attending Bharatanatyam dance\nclasses but now, due to the lockdown, we have these classes on Zoom, which I can only\nimagine, must be hard for the teacher as she tries to make it look e\u001eortless. These classes,\non the other hand, do us some good, as we do not get to copy someone if we need to.\nOn weekdays the school gives us work, which I sometimes \u001fnd overwhelming, but it is\nmore work on their side, so that is impressive. Another thing I like is the kind of e\u001eort the\nteachers are making to teach us by newer methods like making videos of concepts and\neven dance steps, so hats o\u001e to them for that!\nOn days when we do have homework, my parents when home check it, which is good\nbecause after the tiring day at work they still spend time with us.\nOut of the many things I have learned during the lockdown, one main thing is that my parents\nkeep reminding through their example that we should keep hope and stay positive. \n'''","50796131":"len(text.split())","9b5a663f":"def longformer(text,question):\n    encoding = tokenizer.encode_plus(question, text, return_tensors=\"pt\")\n    input_ids = encoding[\"input_ids\"]\n\n    # default is local attention everywhere\n    # the forward method will automatically set global attention on question tokens\n    attention_mask = encoding[\"attention_mask\"]\n\n    start_scores, end_scores = model(input_ids, attention_mask=attention_mask)\n    all_tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n\n    answer_tokens = all_tokens[torch.argmax(start_scores) :torch.argmax(end_scores)+1]\n    answer = tokenizer.decode(tokenizer.convert_tokens_to_ids(answer_tokens))\n    return answer","c5f094be":"question = \"What is my parents profession?\"","d2193d75":"longformer(text,question)","8570c323":"question = \"What is one main thing I have learned during lockdown?\"","208c3c9b":"longformer(text,question)","c1b61981":"text = ''' The primary reasons for the American revolution were \n1. The Stamp Act 2. The Townshend Acts 3. The Boston Massacre 4. The Boston Tea Party \n5. The Coercive Acts 6. Lexington and Concord 7. British attacks on coastal towns'''\n","9a987aa4":"question = \"What were the reasons  for American revolution ?\"","cf5118b7":"longformer(text,question)","0718a173":"text = '''I spotted it in a junk shop in Bridport, a roll-top desk.\nThe man said it was early nineteenth century, and oak.\nI had wanted one, but they were far too expensive. This\none was in a bad condition, the roll-top in several pieces,\none leg clumsily mended, scorch marks all down one\nside. It was going for very little money. I thought I could\nrestore it. It would be a risk, a challenge, but I had to\nhave it. I paid the man and brought it back to my\nworkroom at the back of the garage. I began work on it\non Christmas Eve.\nI removed the roll-top completely and pulled out the\ndrawers. The veneer had lifted almost everywhere \u2014 it looked like water damage to me. Both fire and water had\nclearly taken their toll on this desk. The last drawer was\nstuck fast. I tried all I could to ease it out gently. In the\nend I used brute force. I struck it sharply with the side of\nmy fist and the drawer flew open to reveal a shallow space\nunderneath, a secret drawer. There was something in\nthere. I reached in and took out a small black tin box.\nSello-taped to the top of it was a piece of lined notepaper,\nand written on it in shaky\nhandwriting: \u201cJim\u2019s\nlast letter, received\nJanuary 25, 1915.\nTo be buried with\nme when the\ntime comes.\u201d I\nknew as I did\nit that it was\nwrong of me to\nopen the box,\nbut curiosity\ngot the better of\nmy scruples. It\nusually does.\nInside the box there was an envelope. The address\nread: \u201cMrs Jim Macpherson, 12 Copper Beeches, Bridport,\nDorset.\u201d I took out the letter and unfolded it. It was written\nin pencil and dated at the top \u2014 \u201cDecember 26, 1914\n\u201d.Dearest Connie,\nI write to you in a much happier frame of mind because\nsomething wonderful has just happened that I must tell you about at once. \nWe were all standing to in our trenches\nyesterday morning, Christmas morning. It was crisp and\nquiet all about, as beautiful a morning as I\u2019ve ever seen, as\ncold and frosty as a Christmas morning should be.\nI should like to be able to tell you that we began it.\nBut the truth, I\u2019m ashamed to say, is that Fritz began it.\nFirst someone saw a white flag waving from the trenches\nopposite. Then they were calling out to us from across\nno man\u2019s land, \u201cHappy Christmas, Tommy! Happy\nChristmas!\u201d When we had got over the surprise, some of\nus shouted back, \u201cSame to you, Fritz! Same to you!\u201d I\nthought that would be that. We all did. But then suddenly\none of them was up there in his grey greatcoat and waving\na white flag. \u201cDon\u2019t shoot, lads!\u201d someone shouted. And\nno one did. Then there was another Fritz up on the\nparapet, and another. \u201cKeep your heads down,\u201d I told the\nmen, \u201cit\u2019s a trick.\u201d But it wasn\u2019t.\nOne of the Germans was waving a bottle above his\nhead. \u201cIt is Christmas Day, Tommy. We have schnapps.\nWe have sausage. We meet you? Yes?\u201d By this time there\nwere dozens of them walking towards us across no man\u2019s\nland and not a rifle between them. Little Private Morris\nwas the first up. \u201cCome on, boys. What are we waiting\nfor?\u201d And then there was no stopping them. I was the\nofficer. I should have stopped them there and then, I\nsuppose, but the truth is that it never even occurred to\nme I should. All along their line and ours I could see\nmen walking slowly towards one another, grey coats,\nkhaki coats meeting in the middle. And I was one of\nthem. I was part of this. In the middle of the war we\nwere making peace.You cannot imagine, dearest Connie, my feelings as\nI looked into the eyes of the Fritz officer, who approached\nme, hand outstretched. \u201cHans Wolf,\u201d he said, gripping\nmy hand warmly and holding it. \u201cI am from Dusseldorf.\nI play the cello in the orchestra. Happy Christmas.\u201d\u201cCaptain Jim Macpherson,\u201d I replied. \u201cAnd a Happy\nChristmas to you too. I\u2019m a school teacher from Dorset,\nin the west of England.\u201d\n\u201cAh, Dorset,\u201d he smiled. \u201cI know this place. I know it\nvery well.\u201d We shared my rum ration and his excellent\nsausage. And we talked, Connie, how we talked. He spoke\nalmost perfect English. But it turned out that he had\nnever set foot in Dorset, never even been to England.\nHe had learned all he knew of England from school,\nand from reading books in English. His favourite writer\nwas Thomas Hardy, his favourite book Far from the\nMadding Crowd. So out there in no man\u2019s land we talked\nof Bathsheba and Gabriel Oak and Sergeant Troy and\nDorset. He had a wife and one son, born just six months\nago. As I looked about me there were huddles of khaki\nand grey everywhere, all over no man\u2019s land, smoking,\nlaughing, talking, drinking, eating. Hans Wolf and I\nshared what was left of your wonderful Christmas cake,\nConnie. He thought the marzipan was the best he had\never tasted. I agreed. We agreed about everything, and\nhe was my enemy. There never was a Christmas party\nlike it, Connie.\nThen someone, I don\u2019t know who, brought out a\nfootball. Greatcoats were dumped in piles to make\ngoalposts, and the next thing we knew it was Tommy\nagainst Fritz out in the middle of no man\u2019s land. Hans\nWolf and I looked on and cheered, clapping our hands\nand stamping our feet, to keep out the cold as much as\nanything. There was a moment when I noticed our\nbreaths mingling in the air between us. He saw it too\nand smiled. \u201cJim Macpherson,\u201d he said after a while,\n\u201cI think this is how we should resolve this war. A football\nmatch. No one dies in a football match. No children are\norphaned. No wives become widows.\u201d\n\u201cI\u2019d prefer cricket,\u201d I told him. \u201cThen we Tommies\ncould be sure of winning, probably.\u201d We laughed at\nthat, and together we watched the game. Sad to say,Connie, Fritz won, two goals to one. But as Hans Wolf\ngenerously said, our goal was wider than theirs, so it\nwasn\u2019t quite fair.\nThe time came, and all too soon, when the game was\nfinished, the schnapps and the rum and the sausage\nhad long since run out, and we knew it was all over.\nI wished Hans well and told him I hoped he would see\nhis family again soon, that the fighting would end and\nwe could all go home.\n\u201cI think that is what every soldier wants, on both\nsides,\u201d Hans Wolf said. \u201cTake care, Jim Macpherson.\nI shall never forget this moment, nor you.\u201d He saluted\nand walked away from me slowly, unwillingly, I felt.\nHe turned to wave just once and then became one of\nthe hundreds of grey-coated men drifting back towards\ntheir trenches.\nThat night, back in our dugouts, we heard them\nsinging a carol, and singing it quite beautifully. It was\nStille Nacht, Silent Night. Our boys gave them a rousing\nchorus of While Shepherds Watched. We exchanged\ncarols for a while and then we all fell silent. We had had\nour time of peace and goodwill, a time I will treasure as\nlong as I live.Dearest Connie, by Christmas time next year, this\nwar will be nothing but a distant and terrible memory.\nI know from all that happened today how much both\narmies long for peace. We shall be together again soon,\nI\u2019m sure of it.\n'''","b47638e3":"len(text.split())","51e04deb":"question = \"Who had written the letter?\"","8a6f7c6a":"longformer(text,question)","c36638bd":"question = \"Why was the letter written \u2014 what was the wonderful thing that had happened?\"","8ccaae63":"longformer(text,question)","f14311b4":"question = \"What jobs did Hans Wolf and Jim Macpherson have when they were not soldiers?\"","dd4f28c8":"len(longformer(text,question).split())","91b63bcd":"context = text + '''I folded the letter again and slipped it carefully back\ninto its envelope. I kept awake all night. By morning I\nknew what I had to do. I drove into Bridport, just a few\nmiles away. I asked a boy walking his dog where Copper\nBeeches was. House number 12 turned out to be nothing\nbut a burned-out shell, the roof gaping, the windows\nboarded-up. I knocked at the house next door and asked\nif anyone knew the whereabouts of a Mrs Macpherson.\nOh yes, said the old man in his slippers, he knew her\nwell. A lovely old lady, he told me, a bit muddle-headed,\nbut at her age she was entitled to be, wasn\u2019t she? A\nhundred and one years old. She had been in the house\nwhen it caught fire. No one really knew how the fire had\nstarted, but it could well have been candles. She used\ncandles rather than electricity, because she always\nthought electricity was too expensive. The fireman had\ngot her out just in time. She was in a nursing home\nnow, he told me, Burlington House, on the Dorchester\nroad, on the other side of town.I found Burlington House Nursing Home easily enough.\nThere were paper chains up in the hallway and a lighted\nChristmas tree stood in the corner with a lopsided angel\non top. I said I was a friend come to visit Mrs Macpherson\nto bring her a Christmas present. I could see through\ninto the dining room where everyone was wearing a paper\nhat and singing. The matron had a hat on too and\nseemed happy enough to see me. She even offered me a\nmince pie. She walked me along the corridor.\n\u201cMrs Macpherson is not in with the others,\u201d she told\nme. \u201cShe\u2019s rather confused today so we thought it best\nif she had a good rest. She has no family you know, no\none visits. So I\u2019m sure she\u2019ll be only too pleased to see\nyou.\u201d She took me into a conservatory with wicker chairs\nand potted plants all around and left me.\nThe old lady was sitting in a wheelchair, her hands\nfolded in her lap. She had silver white hair pinned into a\nwispy bun. She was gazing out at\nthe garden. \u201cHello,\u201d I said. She\nturned and looked up at me\nvacantly. \u201cHappy Christmas,\nConnie,\u201d I went on. \u201cI found\nthis. I think it\u2019s yours.\u201d As I was\nspeaking her eyes never left my\nface. I opened the tin box and\ngave it to her. That was the\nmoment her eyes lit up with\nrecognition and her face\nbecame suffused with a sudden\nglow of happiness. I explained\nabout the desk, about how I\nhad found it, but I don't think\nshe was listening. For a while she said nothing, but stroked the letter tenderly with her\nfingertips.\nSuddenly she reached out and took my hand. Her\neyes were filled with tears. \u201cYou told me you\u2019d come home\nby Christmas, dearest,\u201d she said. \u201cAnd here you are,\nthe best Christmas present in the world. Come closer,\nJim dear, sit down.\u201d\nI sat down beside her, and she kissed my cheek. \u201cI\nread your letter so often Jim, every day. I wanted to\nhear your voice in my head. It always made me feel you\nwere with me. And now you are. Now you\u2019re back you\ncan read it to me yourself. Would you do that for me,\nJim dear? I just want to hear your voice again. I\u2019d love\nthat so much. And then perhaps we\u2019ll have some tea.\nI\u2019ve made you a nice Christmas cake, marzipan all\naround. I know how much you love marzipan.\u201d '''","63fabb35":"question = \"Who did Connie Macpherson think her visitor was?\"","c1d035f6":"Here we are using the awesome new nlp library to load and process the dataset.\nAlso we will use Transformers's fast tokenizers alignement methods to get position of answer spans  ","6f7a07e8":"## Load and process data","f9f64cf3":"The Longformer model was presented in [Longformer: The Long-Document Transformer](https:\/\/arxiv.org\/abs\/2004.05150) by Iz Beltagy, Matthew E. Peters, Arman Cohan. As the paper explains it\n\n> `Longformer` is a BERT-like model for long documents.\n\n\nTraining longformer for QA is similar to how you train BERT for QA. But there few things to keep in mind when using longformer for QA task.\n\nLongformer uses sliding-window local attention which scales linearly with sequence length. This is what allows longformer to handle longer sequences. For more details on how the sliding window attention works, please refer to the paper. Along with local attention longformer also allows you to use global attention for certain tokens. For QA task, all question tokens should have global attention.\n\nThe attention is configured using the `attention_mask` paramter of the `forward` method of `LongformerForQuestionAnswering`. Mask values are selected in [0, 1, 2]: 0 for no attention (padding tokens), 1 for local attention (a sliding window attention), 2 for global attention (tokens that attend to all other tokens, and all other tokens attend to them).\n\nAs stated above all question tokens should be given gloabl attention. The `LongformerForQuestionAnswering` model handles this automatically for you. To allow it to do that\n1. The input sequence must have three sep tokens, i.e the sequence should be encoded like this `<s> question<\/s><\/s> context<\/s>`. If you encode the question and answer as a input pair, then the tokenizer already takes care of that, you shouldn't worry about it.\n2. input_ids should always be a batch of examples.","20534082":"# Longformer for Question Answering"}}