{"cell_type":{"0e04c10a":"code","b6cdec58":"code","b693e6dd":"code","b1c4622c":"code","0e59cb7c":"code","9e306a74":"code","71185a81":"code","3d1a526e":"code","f7677e09":"code","d253684e":"code","962cb47a":"code","8d33041c":"code","f92c4e42":"code","94de2c9f":"code","fdbaeacf":"code","a9953686":"code","130677d3":"code","0aa54b25":"code","d4773e1e":"code","8fcddb39":"code","4bfa0b62":"code","24b82918":"code","1e7cb0a9":"code","fb759022":"code","44f4b030":"code","b1a4bd60":"code","7c116102":"code","2b3d87b4":"code","d7e92c3b":"markdown","8f394af8":"markdown","9851890e":"markdown","86206346":"markdown","67c644fb":"markdown","4fa408d3":"markdown","49f2f2df":"markdown","94dce84d":"markdown","72e2ad5e":"markdown","39dd128a":"markdown","fca4aede":"markdown","98a27b40":"markdown","fcb66d60":"markdown","25c46c31":"markdown","fc7b2287":"markdown","749dfc74":"markdown","57a5523c":"markdown","d0563f6a":"markdown","4217a0c8":"markdown","39a354bd":"markdown","8796130d":"markdown","436f2add":"markdown","b91be7d1":"markdown","60462e07":"markdown","8930bd55":"markdown","091cc1b8":"markdown","bb3b53fa":"markdown","d4a9ad88":"markdown"},"source":{"0e04c10a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_score,StratifiedKFold,GridSearchCV\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier, StackingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b6cdec58":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","b693e6dd":"train.head()","b1c4622c":"print(\"Rows and columns in the training set are:-\",train.shape[0],\",\",train.shape[1])\nprint(\"Rows and columns in the test set are:-\",test.shape[0],\",\",test.shape[1])","0e59cb7c":"train.info()","9e306a74":"train =train.astype({\"Pclass\":\"category\"})\ntest =test.astype({\"Pclass\":\"category\"})","71185a81":"plt.pie(train[\"Sex\"].value_counts(),explode=[0,.1],startangle=90,labels=train[\"Sex\"].value_counts().index,shadow=True,autopct='%1.1f%%')","3d1a526e":"f, ax = plt.subplots(figsize=(8,10))\nplot = sns.countplot(x=\"Sex\",hue=\"Survived\",data=train)\nbars = ax.patches\nhalf = int(len(ax.patches)\/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\nfor left,right in zip(left_bars,right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    height_t = height_l + height_r\n    ax.text(left.get_x()+left.get_width()\/2,height_l+5,'{0:.0%}'.format(height_l\/height_t),ha=\"center\")\n    ax.text(right.get_x()+right.get_width()\/2,height_r+5,'{0:.0%}'.format(height_r\/height_t),ha=\"center\")\n    ax.text(left.get_x()+left.get_width()\/2,height_l\/2,'{0}'.format(height_l),ha=\"center\")\n    ax.text(right.get_x()+right.get_width()\/2,height_r\/2,'{0}'.format(height_r),ha=\"center\")\nax.set_title(\"Survival by Gender\")\nax.set_ylabel(\"Count of Passengers\")\nax.set_xlabel(\"Gender\")\nax.legend(title='Survived', loc='upper right', labels=['No', 'Yes'])","f7677e09":"train[\"Age_bin\"] = pd.cut(train.Age,[0,18,30,40,50,60,100])\nf, ax = plt.subplots(figsize=(8,8))\nplot = sns.countplot(x=\"Age_bin\",hue=\"Survived\",data=train)\nbars = ax.patches\nhalf = int(len(ax.patches)\/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\nfor left,right in zip(left_bars,right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    height_t = height_l + height_r\n    ax.text(left.get_x()+left.get_width()\/2,height_l+2,'{0:.0%}'.format(height_l\/height_t),ha=\"center\")\n    ax.text(right.get_x()+right.get_width()\/2,height_r+2,'{0:.0%}'.format(height_r\/height_t),ha=\"center\")\n    ax.text(left.get_x()+left.get_width()\/2,height_l\/2,'{0}'.format(height_l),ha=\"center\")\n    ax.text(right.get_x()+right.get_width()\/2,height_r\/2,'{0}'.format(height_r),ha=\"center\")\nax.set_title(\"Survival by Age\")\nax.set_ylabel(\"Count of Passengers\")\nax.set_xlabel(\"Age Group\")\nax.legend(title='Survived', loc='upper right', labels=['No', 'Yes'])","d253684e":"f, ax = plt.subplots(figsize=(8,8))\nplot = sns.countplot(x=\"Pclass\",hue=\"Survived\",data=train)\nbars = ax.patches\nhalf = int(len(ax.patches)\/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\nfor left,right in zip(left_bars,right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    height_t = height_l + height_r\n    ax.text(left.get_x()+left.get_width()\/2,height_l+2,'{0:.0%}'.format(height_l\/height_t),ha=\"center\")\n    ax.text(right.get_x()+right.get_width()\/2,height_r+2,'{0:.0%}'.format(height_r\/height_t),ha=\"center\")\n    ax.text(left.get_x()+left.get_width()\/2,height_l\/2,'{0}'.format(height_l),ha=\"center\")\n    ax.text(right.get_x()+right.get_width()\/2,height_r\/2,'{0}'.format(height_r),ha=\"center\")\nax.set_title(\"Survival by Class\")\nax.set_ylabel(\"Count of Passengers\")\nax.set_xlabel(\"Class\")\nax.legend(title='Survived', loc='upper right', labels=['No', 'Yes'])","962cb47a":"f, ax = plt.subplots(figsize=(8,8))\nplot = sns.countplot(x=\"Embarked\",hue=\"Survived\",data=train)\nbars = ax.patches\nhalf = int(len(ax.patches)\/2)\nleft_bars = bars[:half]\nright_bars = bars[half:]\nfor left,right in zip(left_bars,right_bars):\n    height_l = left.get_height()\n    height_r = right.get_height()\n    height_t = height_l + height_r\n    ax.text(left.get_x()+left.get_width()\/2,height_l+2,'{0:.0%}'.format(height_l\/height_t),ha=\"center\")\n    ax.text(right.get_x()+right.get_width()\/2,height_r+2,'{0:.0%}'.format(height_r\/height_t),ha=\"center\")\n    ax.text(left.get_x()+left.get_width()\/2,height_l\/2,'{0}'.format(height_l),ha=\"center\")\n    ax.text(right.get_x()+right.get_width()\/2,height_r\/2,'{0}'.format(height_r),ha=\"center\")\nax.set_title(\"Survival by Port of Embarkation\")\nax.set_ylabel(\"Count of Passengers\")\nax.set_xlabel(\"Port\")\nax.legend(title='Survived', loc='upper right', labels=['No', 'Yes'])","8d33041c":"pd.crosstab(train.Pclass,train.Embarked).apply(lambda c: c\/c.sum(), axis=0).style.format(\"{:.2%}\")","f92c4e42":"y=train[\"Survived\"].values\ntest_index = len(y)-1\ndata = pd.concat([train,test],sort=False).reset_index(drop=True)\ndata.info()","94de2c9f":"data[\"Title\"]= data[\"Name\"].str.split(\", \",expand=True)[1].str.split(\".\",expand=True)[0]\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\ndata.replace({'Title': mapping}, inplace=True)","fdbaeacf":"data[\"Fam_size\"] = data[\"SibSp\"]+data[\"Parch\"]+1\n\n\ndef mapping(title,tick_size):\n    for (ind,val) in zip(age_impute,age_impute.values()):\n        if((ind[0]==title) & (ind[1]==tick_size)):\n            return val\n        \n\nage_impute=data.groupby([\"Title\",\"Fam_size\"])[\"Age\"].median().astype(\"float64\").to_dict()","a9953686":"data.loc[data[\"Age\"].isnull(),\"Age\"]= data.loc[data[\"Age\"].isnull(),[\"Title\",\"Fam_size\"]].apply(lambda x: mapping(x[0],x[1]),axis=1)","130677d3":"data[\"Age_bin\"] = pd.qcut(data[\"Age\"],10,duplicates=\"drop\")\nlabel = LabelEncoder()\ndata['Age_bin'] = label.fit_transform(data['Age_bin'].astype(str))","0aa54b25":"data[\"Fare\"].fillna(data[~data[\"Fare\"].isnull()].groupby(\"Pclass\")[\"Fare\"].mean()[3],inplace=True)\ndata['Last_Name'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\n\n\ndata['Family_Survival'] = 0.5\n\nfor grp, grp_data in data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n    if (len(grp_data) != 1):\n        for ind, row in grp_data.iterrows():\n            vmax = grp_data.drop(ind)['Survived'].max()\n            vmin = grp_data.drop(ind)['Survived'].min()\n            passid = row['PassengerId']\n            if (vmax == 1.0):\n                data.loc[data['PassengerId'] == passid, 'Family_Survival'] = 1\n            elif (vmin==0.0):\n                data.loc[data['PassengerId'] == passid, 'Family_Survival'] = 0\n                \nfor grp, grp_data in data.groupby('Ticket'):\n    if (len(grp_data) != 1):\n        for ind, row in grp_data.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                vmax = grp_data.drop(ind)['Survived'].max()\n                vmin = grp_data.drop(ind)['Survived'].min()\n                passid = row['PassengerId']\n                if (vmax == 1.0):\n                    data.loc[data['PassengerId'] == passid, 'Family_Survival'] = 1\n                elif (vmin==0.0):\n                    data.loc[data['PassengerId'] == passid, 'Family_Survival'] = 0                \n","d4773e1e":"data[\"Embarked\"].fillna(data[\"Embarked\"].mode()[0],inplace=True)\ndata[\"Fare\"]=data[\"Fare\"]\/data[\"Fam_size\"]\ndata[\"Fare\"].fillna(data[~data[\"Fare\"].isnull()].groupby(\"Pclass\")[\"Fare\"].mean()[3],inplace=True)","8fcddb39":"data[\"Fare_bin\"] = pd.qcut(data[\"Fare\"],13)\nfrom sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\ndata['Fare_bin'] = label.fit_transform(data['Fare_bin'].astype(str))","4bfa0b62":"data.drop([\"Name\",\"SibSp\",\"Parch\",\"Ticket\",\"Age\",\"Fare\",\"PassengerId\",\"Last_Name\",\"Survived\",\"Title\",\"Cabin\"],inplace=True,axis=1)","24b82918":"data.isnull().sum()\/data.shape[0]*100","1e7cb0a9":"data = pd.get_dummies(data,drop_first=True)","fb759022":"train_1 = data.iloc[:test_index+1,:]\ntest_1 = data.iloc[test_index+1:,:]","44f4b030":"\nX_train,X_test,y_train,y_test = train_test_split(train_1,y,test_size=0.2, random_state=42)","b1a4bd60":"sk_fold = StratifiedKFold(10,shuffle=True, random_state=42)\nsc =StandardScaler()\nX_train= sc.fit_transform(X_train)\nX_train_1= sc.transform(train_1.values)\nX_test= sc.transform(X_test)\nX_submit= sc.transform(test_1.values)\nlog_reg = LogisticRegression()\nran_for  = RandomForestClassifier()\nada_boost = AdaBoostClassifier()\ngrad_boost = GradientBoostingClassifier(n_estimators=100)\nhist_grad_boost = HistGradientBoostingClassifier()\nknn = KNeighborsClassifier()\ntree= DecisionTreeClassifier()\nsvc = SVC()\nxgb = XGBClassifier()\nclf = [(\"Logistic Regression\",log_reg,{\"penalty\":['l2'],\"C\":[100, 10, 1.0, 0.1, 0.01]}),\\\n       (\"Support Vector\",svc,{\"kernel\": [\"rbf\"],\"gamma\":[0.1, 1, 10, 100],\"C\":[0.1, 1, 10, 100, 1000]}),\\\n       (\"Decision Tree\", tree, {}),\\\n       (\"Random Forest\",ran_for,{\"n_estimators\":[100],\"random_state\":[42],\"min_samples_leaf\":[5,10,20,40,50],\"bootstrap\":[False]}),\\\n       (\"Adapative Boost\",ada_boost,{\"n_estimators\":[100],\"learning_rate\":[.6,.8,1]}),\\\n       (\"Gradient Boost\",grad_boost,{}),\\\n       (\"Histogram GB\",hist_grad_boost,{\"loss\":[\"binary_crossentropy\"],\"min_samples_leaf\":[5,10,20,40,50],\"l2_regularization\":[0,.1,1]}),\\\n       (\"XGBoost\",xgb,{\"n_estimators\":[200],\"max_depth\":[3,4,5],\"learning_rate\":[.01,.1,.2],\"subsample\":[.8],\"colsample_bytree\":[1],\"gamma\":[0,1,5],\"lambda\":[.01,.1,1]}),\\\n      (\"K Nearest\",knn,{\"n_neighbors\":[3,5,8],\"leaf_size\":[25,30,35]})]\nstack_list=[]\ntrain_scores = pd.DataFrame(columns=[\"Name\",\"Train Score\",\"Test Score\"])\ni=0\nfor name,clf1,param_grid in clf:\n    clf = GridSearchCV(clf1,param_grid=param_grid,scoring=\"accuracy\",cv=sk_fold,return_train_score=True)\n    clf.fit(X_train,y_train.reshape(-1,1))\n    y_pred = clf.best_estimator_.predict(X_test)\n    cm = confusion_matrix(y_test,y_pred)\n    #train_scores.loc[i]= [name,cross_val_score(clf,X_train,y_train,cv=sk_fold,scoring=\"accuracy\").mean(),(cm[0,0]+cm[1,1,])\/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    train_scores.loc[i]= [name,clf.best_score_,(cm[0,0]+cm[1,1,])\/(cm[0,0]+cm[0,1]+cm[1,0]+cm[1,1])]\n    stack_list.append(clf.best_estimator_)\n    i=i+1\n    \nest = [(\"dec_tree\",stack_list[2]),(\"ran_for\",stack_list[3]),(\"ada_boost\",stack_list[4]),(\"grad_boost\",stack_list[5]),(\"hist_grad_boost\",stack_list[6]),(\"svc\",stack_list[1]),(\"lr\",stack_list[0]),(\"knn\",stack_list[8])]\nsc = StackingClassifier(estimators=est,final_estimator = stack_list[2],cv=sk_fold,passthrough=False)\nsc.fit(X_train,y_train)\ny_pred = sc.predict(X_test)\ncm1 = confusion_matrix(y_test,y_pred)\ny_pred_train = sc.predict(X_train)\ncm2 = confusion_matrix(y_train,y_pred_train)\ntrain_scores.append(pd.Series([\"Stacking\",(cm2[0,0]+cm2[1,1,])\/(cm2[0,0]+cm2[0,1]+cm2[1,0]+cm2[1,1]),(cm1[0,0]+cm1[1,1,])\/(cm1[0,0]+cm1[0,1]+cm1[1,0]+cm1[1,1])],index=train_scores.columns),ignore_index=True)","7c116102":"stack_list[7].fit(X_train_1,y)\ny_submit = stack_list[7].predict(X_submit)\nsubmit = pd.DataFrame({\n        \"PassengerId\": test.PassengerId,\n        \"Survived\": y_submit\n    })","2b3d87b4":"submit.PassengerId = submit.PassengerId.astype(int)\nsubmit.Survived = submit.Survived.astype(int)\nsubmit.to_csv(\"titanic_submit.csv\", index=False)\n    ","d7e92c3b":"We can bin the Fares also in to 13 equal bins. We then encode it to preserve the orinal nature of Fare.","8f394af8":"Finally let's drop all the features that we wouldnt't be using for our modelling.","9851890e":"Exporting the data to submit.","86206346":"Onehot encoding the categorical variables","67c644fb":"Family Size is a useful feature which can give us the size of the group travelling together.\nWe create a dictionary to map the Age basis the mean for a particular Title and the Family Size.\n\nWe create the family size feature from the Sibling\/Spouse and Parent\/Child features.","4fa408d3":"If we plot the survival rate according to age groups the higest survival rate is for children(0-18 Age group). We can use these age bin further for our analysis as part of feature transformation.","49f2f2df":"## Titanic : Survival prediction\n\nRMS Titanic was a British passenger ship, which tragically sunk on 15th April 1912 after colliding with an iceberg during it's maiden journey from Southampton to New York. It was one of the deadliest peacetime marine disasters of modern history.\n\nIt was the largest passenger ship at it's time and was considered \"unsinkable\". There was a shortage of lifeboats  onboard, one of the main reasons which caused the deaths of 1502 out of 2224 passenger and crew. We are provided a dataset with the particulars of the passengers onboard along with the flag if they survived or not.\n\nOur aim is to study this dataset and find out if it was more than chance that determined if a passenger survived or not. If there is a pattern we need to build a predictive model for the chance of survival of a passenger with a particular set of features(i.e. age, gender, economic status etc).\n\n### Edit- For some interesting hyperparameter tuning techniques, please check out the notebook in the link below:-\nhttps:\/\/www.kaggle.com\/ankur123xyz\/advanced-hyperparameter-tuning-techniques\n\n\nLet's move to the dataset and start by importing the requisite packages.","94dce84d":"We will have a look at the datatypes for all the features.\nWhat we get is a mix of integers, floats and object data types. We will do some type conversions later.","72e2ad5e":"We impute the Age category basis the dicionary that we have created.","39dd128a":"Based on the sample data we saw earlier and the datatypes visibile here, we can go ahead and convert Pclass to categorical type.","fca4aede":"Since Cabin has ~78% missing values in both training and test set, we can go ahead and drop this feature. \n\nBefore we start operations on the dataset we should combine the train and test datasets as we would be using the running the test data through the model trained on the training data.","98a27b40":"The Age feature will have to be imputed wherever missing, since we could see some co-relation between survival rates and Age earlier.\n\nWe can extract the salutation from the name and use that to get an Age to impute basis a measure of central tendency.\n\nWe map the less frequent salutations to the frequent onces.","fcb66d60":"The passengers in the 2nd and 3rd classes were less fortunate than the ones from 1st class. 63% of the 1st class passengers survived while only 47% and 24% of the passengers from 2nd and 3rd class survived respectively.","25c46c31":"We move on to checking the number of rows and columns in the train and test set.\n<br>There would be an extra feature in the training set that would be the \"Survived\" flag.","fc7b2287":"We would import the train and test set into different datasets.","749dfc74":"We split the dataframe to get our original passenger data from the training set and the test set.","57a5523c":"We should check for gender having any effect on the survival rate of the passengers.\n\nTurns out the survival rate for females was much higher at 74% whereas for males it was only 19%. Possible reason could be preference given while boarding the lifeboats.","d0563f6a":"Since we have 2 passenger for whom the Embarked location is missing, we impute it using the mode strategy.\nFor the passenger whose Fare is missing we impute it with the mean fare of the 3rd Class passengers since the passenger had a 3rd Class ticket.\n\nAlso we divide the fare by the family size since the Fare is for the entire ticket(which may have multiple passengers) and not only for that particular passenger.","4217a0c8":"We further split the training set in to a train and test set to validate our model.","39a354bd":"Coming to the modeling part. We first scale the data using standard scaler.\nWe use grid search with stratified kfold validation for 9 algorithms.\nWe get the scores from the cross validation for all these models and run a prediction on the test data from our train_test_split.\nFor stacking we get the accuracy based on fitting the train and test set.","8796130d":"We bin the Ages for the passengers for the whole dataset and then encode it since there is a ordinal relation between the Age groups.","436f2add":"If we look at the survival rates as per the port of embarkation, the highest rate is for those who embarked at Cherbourg. But if we dig deeper in to the class split according to port of embarkation, we see that ~50% of the passengers who boarded at Cherbourg were 1st class passengers, whereas for Queenstown and Southampton it was 2.6% and 19.7%. We have already seen earlier that 1st class passengers had a considerably higher chance of survival.","b91be7d1":"Getting a glimpse of the training dataset using the head function.","60462e07":"We can make use of the last names since survival rate of a family is interlinked. The logic for the below is based on the logic from below kernel.Thanks to S.Xu\nhttps:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever\n\nAlso we have imputed Fare for the family as of now grouped by class.","8930bd55":"We need to check for missing values in our dataset. Seems we are all good to go as we don't have null values.","091cc1b8":"Got a score of 0.80861 on submission. Cheers ! ","bb3b53fa":"We fit and predict on the best XGB model that we derived based on the scores.","d4a9ad88":"Now let us start with some visualtizations of the data to study it better.\n\n64.8% of the passengers were male whereas 35.2% were female."}}