{"cell_type":{"4e44b495":"code","20614a66":"code","089dee65":"code","3fcb7857":"code","1c78f75f":"code","2687494d":"code","8a5114fd":"code","6a1db26a":"code","37683af1":"code","5c0f3b03":"code","946491ec":"code","0b625558":"markdown","051c55cd":"markdown","03f36981":"markdown","b9e7dabf":"markdown","f7dfff85":"markdown","76328fbc":"markdown","5aa03c31":"markdown","3ac67eb3":"markdown"},"source":{"4e44b495":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","20614a66":"Raw_data = datasets.load_boston()\n\n#Take the data, feature_cols, target(price)\ntarget = Raw_data.target\ntrain_data = Raw_data.data\nfeature_cols = Raw_data.feature_names \n\n#Create the df \ndf = pd.DataFrame(train_data, columns = feature_cols)\ndf.head()","089dee65":"df.info()","3fcb7857":"df.describe().transpose()","1c78f75f":"#See the correlation between feature and target\ndf['Target'] = target\nplt.figure(figsize=(10, 10))\nsns.heatmap(df.corr(), annot=True ,center=0)","2687494d":"x_train, x_test, y_train, y_test = train_test_split(df, target, train_size = 0.8, random_state= 4)\n\nfrom sklearn.preprocessing import StandardScaler\nx_train = StandardScaler().fit_transform(x_train)\n\nx_test = StandardScaler().fit_transform(x_test)","8a5114fd":"from numpy.ma.core import shape\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import mean_squared_error\ndef model_fit(x_train, x_test, y_train, y_test):\n  \n  from sklearn.linear_model import LinearRegression\n  LR = LinearRegression().fit(x_train, y_train)\n\n  print('The score of LinearRegression:', LR.score(x_test, y_test))\n  print('The MSE of LinearRegression:', mean_squared_error(y_test, LR.predict(x_test)))\n  print('The MSLE of LinearRegression:', mean_squared_log_error(y_test, LR.predict(x_test)))\n  print('\\n')\n\n  from sklearn.ensemble import RandomForestRegressor\n  RFR = RandomForestRegressor().fit(x_train, y_train)\n  print('The score of RandomForestRegressor:', RFR.score(x_test, y_test))\n  print('The MSE of RandomForestRegressor:', mean_squared_error(y_test, RFR.predict(x_test)))\n  print('The MSLE of RandomForestRegressor:', mean_squared_log_error(y_test, RFR.predict(x_test)))\n  print('\\n')\n  \n  from xgboost import XGBRegressor\n  XG = XGBRegressor().fit(x_train, y_train)\n  print('The score of XGBRegressor:', XG.score(x_test, y_test))\n  print('The MSE of XGBRegressor:', mean_squared_error(y_test, XG.predict(x_test)))\n  print('The MSLE of XGBRegressor:', mean_squared_log_error(y_test, XG.predict(x_test)))\n  print('\\n')\n\n  from sklearn.svm import SVR\n  svr = SVR(kernel = 'rbf').fit(x_train, y_train)\n  print('The score of SVR:', svr.score(x_test, y_test))\n  print('The MSE of SVR:', mean_squared_error(y_test, svr.predict(x_test)))\n  print('The MSLE of SVR:', mean_squared_log_error(y_test, svr.predict(x_test)))\n  print('\\n')\n  \n  from sklearn.ensemble import GradientBoostingRegressor \n  GBC = GradientBoostingRegressor().fit(x_train, y_train)\n  print('The score of GradientBoostingRegressor:', GBC.score(x_test, y_test))\n  print('The MSE of GradientBoostingRegressor:', mean_squared_error(y_test, GBC.predict(x_test)))\n  print('The MSLE of GradientBoostingRegressor:', mean_squared_log_error(y_test, GBC.predict(x_test)))\n  print('\\n')\n  \n  DL = keras.Sequential([layers.Dense(64, activation = 'relu', input_shape = ((x_train.shape[1],))),\n                            layers.Dense(32, activation = 'relu'),\n                            layers.Dense(1)\n                            ])\n  optimizer = tf.keras.optimizers.RMSprop(0.001)\n  DL.compile(loss= 'mse', optimizer=optimizer, metrics = ['mse', 'mae', 'msle'])\n  history = DL.fit(x = x_train, y = y_train, epochs = 100) \n  return LR, RFR, XG, svr, GBC, DL","6a1db26a":"Model = model_fit(x_train, x_test, y_train, y_test)","37683af1":"ML_model = ['LinearRegression', 'RandomForestRegressor', 'XGBRegressor', 'SVR', 'GradientBoostingRegressor', 'DeepLearning']\nfor i in range(6):\n  plt.scatter(y_test, Model[i].predict(x_test))\n  plt.plot([x for x in range(55)], [x for x in range(55)], color = 'r')\n  plt.xlabel(\"Prices\")\n  plt.ylabel(\"Predicted prices\")\n  plt.title(ML_model[i])\n  plt.show()\n  plt.clf()","5c0f3b03":"MSE_num = []\nfor i in range(6):\n  MSE = mean_squared_error(y_test, Model[i].predict(x_test))\n  MSE_num.append(MSE)\nplt.figure(figsize = (8, 5))\nplt.title('MSE')\nsns.barplot(MSE_num, ML_model)","946491ec":"ML_model = ['LinearRegression', 'RandomForestRegressor', 'XGBRegressor', 'SVR', 'GradientBoostingRegressor', 'DeepLearning']\nMSLE_num = []\nfor i in range(6):\n  MSE = mean_squared_log_error(y_test, Model[i].predict(x_test))\n  MSLE_num.append(MSE)\nplt.figure(figsize = (8, 5))\nplt.title('MSLE')\nsns.barplot(MSLE_num, ML_model)","0b625558":"> # 2.Data exploration","051c55cd":"> # 1.Read the data","03f36981":"> # 4.ML DL Model","b9e7dabf":"> # 3.EDA","f7dfff85":"# 6.MSE. MSLE comparsion with ML DL models","76328fbc":"> **Split the df and target as train and test**","5aa03c31":"> # 5. The comparsion with prediction and reality price","3ac67eb3":"# ML:\n# 1. LinearRegression\n# 2. RandomForestRegressor\n# 3. XGBRegressor\n# 4. SVR\n# 5. GradientBoostingRegressor\n# 6. Deep Learning"}}