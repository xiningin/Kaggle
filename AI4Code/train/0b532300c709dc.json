{"cell_type":{"221055ae":"code","6f1ae4f1":"code","8d36c410":"code","e769a8e3":"code","1e1daacb":"code","6a1e6cc6":"code","c5c062e0":"code","08aaa7ef":"code","f30c9ac4":"code","a5750994":"code","d3d63734":"code","a174b240":"code","bdde63f7":"code","976bf5b4":"code","da5cdd5f":"code","6a07847f":"code","eb13689f":"code","d9503d2f":"code","99e85dc1":"code","c67019c2":"code","4dc454a7":"code","e21944f0":"code","2098c4a2":"code","7a63dfd4":"code","99a4b750":"code","a538decf":"code","2b0a84c1":"code","10fc401f":"code","02e05384":"code","9962eb45":"code","b4c71426":"code","c1b66516":"code","68500fac":"code","e19ace06":"code","f67e86f3":"code","58306a71":"code","d467dd6f":"code","7f647975":"code","5c278c7c":"code","daaa45d8":"code","32c33b42":"code","64807b3c":"code","3d206632":"code","40d84af2":"code","040d4cf1":"code","db2b4f21":"code","fbf4ce50":"code","3f5f36ed":"code","fac71353":"code","dc1ed520":"code","f72c32e0":"code","81998cd3":"code","941e483a":"code","40d6c304":"code","95329eac":"code","f152dbb4":"code","2c0f1fce":"code","77690976":"markdown","ef0f0387":"markdown","45fe3304":"markdown","4e125aa3":"markdown","52e57042":"markdown","5e844a33":"markdown","5379abc5":"markdown","cd2f60c0":"markdown","4904713a":"markdown","c0522bb4":"markdown","a65298d8":"markdown","515f7463":"markdown","dca1c442":"markdown","b4756fc2":"markdown","1ab16d4e":"markdown","251edd05":"markdown","53ca9b30":"markdown","a5965d12":"markdown","cdd68c98":"markdown","c7ea7394":"markdown","d31b0aee":"markdown","ff35b065":"markdown","ce0721bd":"markdown","b525e2ff":"markdown","f0e8ab20":"markdown","1b9fd583":"markdown","43746094":"markdown","97558b6b":"markdown","31cf59a6":"markdown","6f9c06e3":"markdown","4bfb3e48":"markdown"},"source":{"221055ae":"import pandas as pd\n\ndf = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndf.head()","6f1ae4f1":"df.shape","8d36c410":"df.info()","e769a8e3":"df.isnull().sum()","1e1daacb":"df.isna().sum()","6a1e6cc6":"import matplotlib.pyplot as plt\nimport seaborn as sns","c5c062e0":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'fixed acidity', data = df)","08aaa7ef":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'volatile acidity', data = df)","f30c9ac4":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'citric acid', data = df)","a5750994":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'residual sugar', data = df)","d3d63734":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'chlorides', data = df)","a174b240":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'free sulfur dioxide', data = df)","bdde63f7":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'total sulfur dioxide', data = df)","976bf5b4":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'density', data = df)","da5cdd5f":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'pH', data = df)","6a07847f":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'sulphates', data = df)","eb13689f":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'alcohol', data = df)","d9503d2f":"# We will Predict Binary Classification, so we will convert quality into 2 categories(bad, good)\nbins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = group_names)","99e85dc1":"df.head()","c67019c2":"# Label Encoder for Quality\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nle = LabelEncoder()","4dc454a7":"# 0 for bad and 1 for good\ndf['quality'] = le.fit_transform(df['quality'])\ndf.head()","e21944f0":"# Count Quality Value\nsns.countplot(df['quality'])","2098c4a2":"df['quality'].value_counts()","7a63dfd4":"# Input Data into X and y\nX = df.drop('quality', axis = 1)\ny = df['quality']","99a4b750":"from sklearn.model_selection import train_test_split\nimport numpy as np","a538decf":"# 80% : 20%\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","2b0a84c1":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score","10fc401f":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(random_state = 42)\nrfc.fit(X_train,y_train)\n\nrfc_acc = accuracy_score(y_test, rfc.predict(X_test))\nrfc_cv = cross_val_score(rfc,X_train,y_train,cv = 10, scoring='accuracy').mean()\n\nprint('Random Forest Classifier accuracy: {:.5f}'.format(rfc_acc))\nprint('Random Forest Classifier Cross Val Score : {:.5f}'.format(rfc_cv))","02e05384":"from sklearn.ensemble import BaggingClassifier\n\nbgc = BaggingClassifier()\nbgc.fit(X_train,y_train)\n\nbgc_acc = accuracy_score(y_test, bgc.predict(X_test))\nbgc_cv = cross_val_score(bgc,X_train,y_train,cv = 10, scoring='accuracy').mean()\n\nprint('BaggingClassifier accuracy: {:.5f}'.format(bgc_acc))\nprint('BaggingClassifier Cross Val Score : {:.5f}'.format(bgc_cv))","9962eb45":"from lightgbm import LGBMClassifier\n\nlgb = LGBMClassifier()\nlgb.fit(X_train,y_train)\n\nlgb_acc = accuracy_score(y_test, lgb.predict(X_test))\nlgb_cv = cross_val_score(lgb,X_train,y_train,cv = 10, scoring='accuracy').mean()\n\nprint('LightGBM accuracy: {:.5f}'.format(lgb_acc))\nprint('LGBM Cross Val Score : {:.5f}'.format(lgb_cv))","b4c71426":"from sklearn.ensemble import ExtraTreesClassifier\n\netc = ExtraTreesClassifier()\netc.fit(X_train,y_train)\n\netc_acc = accuracy_score(y_test, etc.predict(X_test))\netc_cv = cross_val_score(etc,X_train,y_train,cv = 10, scoring='accuracy').mean()\n\nprint('ExtraTreeClassifier accuracy: {:.5f}'.format(etc_acc))\nprint('ExtraTreeClassifier Cross Val Score : {:.5f}'.format(etc_cv))","c1b66516":"from xgboost import XGBClassifier\n\nxgb=XGBClassifier()\nxgb.fit(X_train, y_train)\n\nxgb_acc = accuracy_score(y_test, xgb.predict(X_test))\nxgb_cv = cross_val_score(xgb,X_train,y_train,cv = 10, scoring='accuracy').mean()\n\nprint('XGB accuracy: {:.5f}'.format(xgb_acc))\nprint('XGB Cross Val Score : {:.5f}'.format(xgb_cv))","68500fac":"print('---------------------------------------------------------')\nprint('Random Forest Classifier accuracy: {:.5f}'.format(rfc_acc))\nprint('Random Forest Classifier Cross Val Score : {:.5f}'.format(rfc_cv))\nprint('---------------------------------------------------------')\nprint('BaggingClassifier accuracy: {:.5f}'.format(bgc_acc))\nprint('BaggingClassifier Cross Val Score : {:.5f}'.format(bgc_cv))\nprint('---------------------------------------------------------')\nprint('LightGBM accuracy: {:.5f}'.format(lgb_acc))\nprint('LGBM Cross Val Score : {:.5f}'.format(lgb_cv))\nprint('---------------------------------------------------------')\nprint('ExtraTreeClassifier accuracy: {:.5f}'.format(etc_acc))\nprint('ExtraTreeClassifier Cross Val Score : {:.5f}'.format(etc_cv))\nprint('---------------------------------------------------------')\nprint('XGB accuracy: {:.5f}'.format(xgb_acc))\nprint('XGB Cross Val Score : {:.5f}'.format(xgb_cv))","e19ace06":"from sklearn.model_selection import RandomizedSearchCV","f67e86f3":"#Randomized Search CV\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\nmin_samples_split = [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10]","58306a71":"# Params for Tuning\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}\n\nprint(random_grid)","d467dd6f":"rf = RandomForestClassifier()\nrf=RandomizedSearchCV(estimator = rf, param_distributions = random_grid,scoring='accuracy', n_iter = 10, cv = 10, verbose=2, random_state=42, n_jobs = 1)","7f647975":"# Train Model\nrf.fit(X_train,y_train)","5c278c7c":"# Best Parameter\nrf.best_params_","daaa45d8":"# Best Score in RandomForest Classifier\nrfbs = rf.best_score_\nprint(\"Random Forest Classifier Best Score : \", rfbs)","32c33b42":"# For the Parameters we using RFC Params for Tuning\n\nlgb = LGBMClassifier()\n# Implement Tuning kedalam Model\nlgb=RandomizedSearchCV(estimator = lgb, param_distributions = random_grid,scoring='accuracy', n_iter = 10, cv = 10, verbose=2, random_state=42, n_jobs = 1)","64807b3c":"# Train Model\nlgb.fit(X_train,y_train)","3d206632":"lgb.best_params_","40d84af2":"# Best Score in LGB Classifier\nlgbbs = lgb.best_score_\nprint(\"LightGBM Classifier Best Score : \", lgbbs)","040d4cf1":"# For the Parameters we using RFC Params for Tuning\n\netc = ExtraTreesClassifier()\netc=RandomizedSearchCV(estimator = etc, param_distributions = random_grid,scoring='accuracy', n_iter = 10, cv = 10, verbose=2, random_state=42, n_jobs = 1)","db2b4f21":"etc.fit(X_train, y_train)","fbf4ce50":"etc.best_params_","3f5f36ed":"# Best Score in LGB Classifier\netcbs = etc.best_score_\nprint(\"Extra Tree Classifier Best Score : \", etcbs)","fac71353":"from scipy.stats import uniform, randint\nxgb=XGBClassifier()","dc1ed520":"# Dictionary HyperParameter Tuning for XGB Classifier\nparams = {\n    \"gamma\": uniform(0, 0.5),\n    \"learning_rate\": uniform(0.03, 0.3), # default 0.1 \n    \"max_depth\": randint(2, 6), # default 3\n    \"n_estimators\": randint(100, 150), # default 100\n    \"subsample\": uniform(0.6, 0.4)\n}","f72c32e0":"xgb = RandomizedSearchCV(estimator = xgb, param_distributions = params,scoring='accuracy', n_iter = 10, cv = 10, verbose=2, random_state=42, n_jobs = 1)","81998cd3":"xgb.fit(X_train,y_train)","941e483a":"xgb.best_params_","40d6c304":"# Best Score in XGB Classifier\nxgbbs = xgb.best_score_\nprint(\"XGBoost Best Score : \", xgbbs)","95329eac":"print('---------------------------------------------------------')\nprint('---------------------------------------------------------')\nprint('---------------------------------------------------------')\nprint('Random Forest Classifier accuracy                               : {:.5f}'.format(rfc_acc))\nprint('Random Forest Classifier Cross Val Score                        : {:.5f}'.format(rfc_cv))\nprint(\"Random Forest Classifier After Hyperparameter Tuning Best Score : {:.5f}\".format(rfbs))\nprint('---------------------------------------------------------')\nprint('---------------------------------------------------------')\nprint('---------------------------------------------------------')\nprint('LightGBM Classifier accuracy                                    : {:.5f}'.format(lgb_acc))\nprint('LightGBM Classifier Cross Val Score                             : {:.5f}'.format(lgb_cv))\nprint(\"LightGBM Classifier After Hyperparameter Tuning Best Score      : {:.5f}\".format(lgbbs))\nprint('---------------------------------------------------------')\nprint('---------------------------------------------------------')\nprint('---------------------------------------------------------')\nprint('Extra Tree Classifier accuracy                                  : {:.5f}'.format(etc_acc))\nprint('Extra Tree Classifier Cross Val Score                           : {:.5f}'.format(etc_cv))\nprint(\"Extra Tree Classifier After Hyperparameter Tuning Best Score    : {:.5f}\".format(etcbs))\nprint('---------------------------------------------------------')\nprint('---------------------------------------------------------')\nprint('---------------------------------------------------------')\nprint('XGB Classifier accuracy                                         : {:.5f}'.format(xgb_acc))\nprint('XGB Classifier Cross Val Score                                  : {:.5f}'.format(xgb_cv))\nprint(\"XGB Classifier After Hyperparameter Tuning Best Score           : {:.5f}\".format(xgbbs))","f152dbb4":"results = {\n    'Model': ['Random Forest', 'LightGBM', 'Extra Tree', 'XGB'],\n    'Accuracy': [rfc_acc, lgb_acc, etc_acc, xgb_acc],\n    'Cross Val Score': [rfc_cv, lgb_cv, etc_cv, xgb_cv],\n    'After Tuning Score': [rfbs, lgbbs, etcbs, xgbbs]\n}\n\nresult_model = pd.DataFrame(results)","2c0f1fce":"result_model","77690976":"Import CrossValScore and Accuracy Score","ef0f0387":"![](https:\/\/vitalflux.com\/wp-content\/uploads\/2020\/09\/Screenshot-2020-09-08-at-4.17.30-PM.png)","45fe3304":"## XGB Classifier","4e125aa3":"## Bagging Classifier","52e57042":"=> 90% acc will be Hyperparameter Tuning","5e844a33":"Read Datasets","5379abc5":"## Random Search CV","cd2f60c0":"Check Na Values","4904713a":"## Random Forest Clasifier","c0522bb4":"# Hyperparameter Tuning","a65298d8":"# Data Preprocessing","515f7463":"![](https:\/\/media.springernature.com\/original\/springer-static\/image\/chp%3A10.1007%2F978-3-030-26142-9_11\/MediaObjects\/482374_1_En_11_Fig1_HTML.png)","dca1c442":"## The model we will use","b4756fc2":"## LGBM Classifier","1ab16d4e":"# Observe the Data","251edd05":"## Extra Tree Classifier","53ca9b30":"Check Null Values","a5965d12":"## LGBM Classifier","cdd68c98":"# EDA","c7ea7394":"## Extra Tree Classifier","d31b0aee":"# Conclusion","ff35b065":"Split Data","ce0721bd":"# Modelling","b525e2ff":"## Random Forest Classifier","f0e8ab20":"Label Encoder","1b9fd583":"## Conclusion","43746094":"## XGB CLassifier","97558b6b":"![](https:\/\/miro.medium.com\/max\/1170\/1*VY3lEFysaQ0nnV_zkxyU-w.png)","31cf59a6":"![](https:\/\/www.researchgate.net\/publication\/343566690\/figure\/fig2\/AS:932532258369537@1599344424444\/Comparison-between-XGBoost-level-wise-horizontal-tree-growth-and-LightGBM-vertical.png)","6f9c06e3":"Import Library","4bfb3e48":"![](https:\/\/images.akira.ai\/glossary\/lightgbm-boosting%20framework-akira-ai.png)"}}