{"cell_type":{"45b90c52":"code","71c9b52a":"code","a11ab360":"code","032da778":"code","794a9cb8":"code","1c4131c1":"code","647b7c2a":"code","bc0d2973":"code","a3c74665":"code","8281b30c":"code","9981580b":"code","9d5771e1":"code","dc4cab12":"code","902526db":"code","0dcdcc9f":"code","9565a1de":"code","bef37baa":"code","8853e769":"code","cc5a9a4c":"code","8ea0c495":"code","76b58d71":"code","256f6070":"code","d7187ed3":"code","dc20a601":"code","16b5435b":"code","62b0e436":"code","a6434f9a":"code","e06a1f2f":"code","af52574d":"code","97e25d42":"code","7f8b984b":"code","d72ca455":"code","b3079aef":"code","b511f3af":"code","f3517db4":"code","033ab713":"markdown","6c39f9f0":"markdown","c4b347ce":"markdown","abb29725":"markdown","c94ccde0":"markdown","e836d63c":"markdown","a8d563ff":"markdown","9a667439":"markdown","47b63b49":"markdown","982f8e4f":"markdown","e6802065":"markdown","e7604bca":"markdown","637a1701":"markdown","53c6006d":"markdown","f440fc8d":"markdown","b23a713e":"markdown","4c0d2c6b":"markdown","f9e8786a":"markdown","d941eef9":"markdown","865c571e":"markdown"},"source":{"45b90c52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","71c9b52a":"data = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\ndata.head()","a11ab360":"data = data.drop('Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',1)\ndata = data.drop('Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2', 1)\ndata = data.drop('CLIENTNUM', 1)","032da778":"import seaborn as sns\nimport plotly.express as plx\nimport matplotlib.pyplot as plt","794a9cb8":"cat_features = data.select_dtypes(['object','category']).columns.to_list()\n\nfor cat in cat_features:\n    fig = plx.pie(data[cat].value_counts(False), values=cat, names = data[cat].value_counts(False).index,title = cat,template='ggplot2')\n    fig.show()","1c4131c1":"fig = plx.box(data, color=\"Attrition_Flag\", y=\"Total_Relationship_Count\",title='Number of products held by customer', points=\"all\")\nfig.show()","647b7c2a":"fig = plx.box(data, color=\"Attrition_Flag\", y=\"Months_Inactive_12_mon\", points=\"all\",title='Number of months with no transactions in the last year')\nfig.show()","bc0d2973":"fig = plx.histogram(data, x=\"Credit_Limit\", color=\"Attrition_Flag\",title='Credit limit on the credit card')\nfig.show()","a3c74665":"fig = plx.box(data, color=\"Attrition_Flag\", y=\"Total_Revolving_Bal\", points=\"all\",title='Total revolving balance on the credit card')\nfig.show()","8281b30c":"fig = plx.histogram(data, x=\"Total_Trans_Ct\", color=\"Attrition_Flag\",title='Number of transactions made in the last year')\nfig.show()","9981580b":"fig = fig = plx.box(data, color=\"Attrition_Flag\", y=\"Total_Ct_Chng_Q4_Q1\", points=\"all\",title='Change in transaction number over the last year (Q4 over Q1)')\nfig.show()","9d5771e1":"def display_correlation_matrix(data):\n    \"\"\" Displays a correlation matrix for a dataset \"\"\"\n    corr = data.corr()\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n    f, ax = plt.subplots(figsize=(50, 50))\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n       square=True, annot=True,linewidths=.5, cbar_kws={\"shrink\": .5})\n    \ndisplay_correlation_matrix(data)","dc4cab12":"data_copy = data\n\ndata.info()","902526db":"data_cat = data[cat_features]\ndata_cat","0dcdcc9f":"a_map = {'Attrited Customer':1, 'Existing Customer':0}\ndata['Attrition_Flag'] = data['Attrition_Flag'].map(a_map)","9565a1de":"g_map = {'M':0, 'F':1}\ndata['Gender'] = data['Gender'].map(g_map)","bef37baa":"from sklearn.preprocessing import OrdinalEncoder\n\nmarital = OrdinalEncoder(categories=[['Unknown','Single','Married','Divorced']])\ndata['Marital_Status'] = marital.fit_transform(data_cat[['Marital_Status']])","8853e769":"edu = OrdinalEncoder(categories=[['Unknown', 'Uneducated', 'High School', 'College','Graduate',\n                                  'Post-Graduate','Doctorate']])\ndata['Education_Level'] = edu.fit_transform(data_cat[['Education_Level']])","cc5a9a4c":"income = OrdinalEncoder(categories=[['Unknown','Less than $40K','$40K - $60K','$60K - $80K',\n                                     '$80K - $120K','$120K +']])\ndata['Income_Category'] = income.fit_transform(data_cat[['Income_Category']])","8ea0c495":"card = OrdinalEncoder(categories=[['Blue', 'Silver','Gold','Platinum']])\ndata['Card_Category'] = card.fit_transform(data_cat[['Card_Category']])","76b58d71":"data[cat_features].head()","256f6070":"cont = data.select_dtypes(['float64','int64','category']).columns.to_list()\ndata_cont = data[cont]\ndata_cont.head()","d7187ed3":"from sklearn.preprocessing import StandardScaler\n\ncredit = StandardScaler()\ndata['Credit_Limit'] = credit.fit_transform(data_cont[['Credit_Limit']])","dc20a601":"revolv_bal = StandardScaler()\ndata['Total_Revolving_Bal'] = revolv_bal.fit_transform(data_cont[['Total_Revolving_Bal']])","16b5435b":"avg_open = StandardScaler()\ndata['Avg_Open_To_Buy'] = avg_open.fit_transform(data_cont[['Avg_Open_To_Buy']])","62b0e436":"trans = StandardScaler()\ndata['Total_Trans_Amt'] = trans.fit_transform(data_cont[['Total_Trans_Amt']])","a6434f9a":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OrdinalEncoder\n\ndef preprocessCustomersPipeline(data):\n    \"\"\"Prepares the original customers data table in a form ready for the Deep Learning model, using\n    all the transformations and edits we used above, and returns a ready dataset\"\"\"\n\n    data = data.drop('Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',1)\n    data = data.drop('Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2', 1)\n    data = data.drop('CLIENTNUM', 1)\n    \n    #Categorical Features\n    a_map = {'Attrited Customer':1, 'Existing Customer':0}\n    data['Attrition_Flag'] = data['Attrition_Flag'].map(a_map)\n    g_map = {'M':0, 'F':1}\n    data['Gender'] = data['Gender'].map(g_map)\n    \n    marital = OrdinalEncoder(categories=[['Unknown','Single','Married','Divorced']])\n    data['Marital_Status'] = marital.fit_transform(data[['Marital_Status']])\n\n    edu = OrdinalEncoder(categories=[['Unknown', 'Uneducated', 'High School', 'College','Graduate',\n                                      'Post-Graduate','Doctorate']])\n    data['Education_Level'] = edu.fit_transform(data[['Education_Level']])\n\n    income = OrdinalEncoder(categories=[['Unknown','Less than $40K','$40K - $60K','$60K - $80K',\n                                         '$80K - $120K','$120K +']])\n    data['Income_Category'] = income.fit_transform(data[['Income_Category']])\n\n    card = OrdinalEncoder(categories=[['Blue', 'Silver','Gold','Platinum']])\n    data['Card_Category'] = card.fit_transform(data[['Card_Category']])\n    \n    #Numerical Features\n    credit = StandardScaler()\n    data['Credit_Limit'] = credit.fit_transform(data[['Credit_Limit']])\n    revolv_bal = StandardScaler()\n    data['Total_Revolving_Bal'] = revolv_bal.fit_transform(data[['Total_Revolving_Bal']])\n    avg_open = StandardScaler()\n    data['Avg_Open_To_Buy'] = avg_open.fit_transform(data[['Avg_Open_To_Buy']])\n    trans = StandardScaler()\n    data['Total_Trans_Amt'] = trans.fit_transform(data[['Total_Trans_Amt']])\n    \n    nums = ['Credit_Limit','Total_Revolving_Bal','Avg_Open_To_Buy','Total_Trans_Amt', 'Total_Trans_Ct']\n    for n in nums:\n        s = StandardScaler()\n        data[n] = s.fit_transform(data[[n]])\n    return data","e06a1f2f":"data_raw = pd.read_csv('\/kaggle\/input\/credit-card-customers\/BankChurners.csv')\ndata_raw_shuffled = data_raw.sample(frac=1)\n\ntrain = data_raw_shuffled.iloc[2025:,:]\ntest = data_raw_shuffled.iloc[:2025,:]\n\ntrain_p = preprocessCustomersPipeline(train)\ntest_p = preprocessCustomersPipeline(test)","af52574d":"X_train = train_p.iloc[:,1:]\ny_train = train_p.iloc[:,0]\nX_test = test_p.iloc[:,1:]\ny_test = test_p.iloc[:,0]","97e25d42":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nimport xgboost\nfrom sklearn.metrics import accuracy_score\n\nrf = RandomForestClassifier()\nknn = KNeighborsClassifier(n_neighbors=5)\nsvc = SVC()\nxgb = xgboost.XGBClassifier(use_label_encoder=False)\n\nml_models = [rf,knn,svc, xgb]\nfor ml in ml_models:\n    ml.fit(X_train, y_train)\n    y_pred = ml.predict(X_test)\n    print(accuracy_score(y_pred,y_test))","7f8b984b":"import tensorflow as tf\nfrom tensorflow import keras\n\nmodel = keras.models.Sequential([\n    keras.layers.Dense(19,input_shape=X_train.shape[1:]),\n    keras.layers.BatchNormalization(),\n    keras.layers.Dense(7, activation=\"elu\", kernel_initializer=\"LecunNormal\"),\n    keras.layers.Dropout(0.3),\n    keras.layers.Dense(2, activation=\"relu\",  kernel_initializer=\"he_normal\")\n])","d72ca455":"model.compile(loss=\"sparse_categorical_crossentropy\", \n              optimizer=keras.optimizers.Nadam(learning_rate=0.001),metrics=[\"accuracy\"])\n\nhistory = model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))","b3079aef":"import matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator\n\ndef plot_learning_curves(loss, val_loss):\n    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.-\", label=\"Training loss\")\n    plt.plot(np.arange(len(val_loss)) + 1, val_loss, \"r.-\", label=\"Validation loss\")\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n    plt.axis([1, 50, 0.25, 2])\n    plt.legend(fontsize=14)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.grid(True)\n\nplot_learning_curves(history.history[\"loss\"], history.history[\"val_loss\"])\nplt.show()","b511f3af":"from sklearn.base import BaseEstimator, ClassifierMixin\n\n#Translate the Tensorflow model into a Scikit-Learn model, so it can be used with the ensemble voting classifier\nclass NN(BaseEstimator, ClassifierMixin):\n    def __init__(self, demo_param='demo'):\n         self.demo_param = demo_param\n\n    def fit(self, X, y):\n        return self\n\n    def predict(self, X):\n        return model.predict(X)\n    \n    def predict_proba (self, X):\n        return model.predict(X)","f3517db4":"from sklearn.ensemble import VotingClassifier\n\nrf = RandomForestClassifier()\nknn = KNeighborsClassifier(n_neighbors=5)\nsvc = SVC(probability=True)\nxgb = xgboost.XGBClassifier(use_label_encoder=False)\nnn = NN()\n\nvoting_clf = VotingClassifier(estimators=[('rf',rf), ('knn', knn), ('svc',svc),('xgb',xgb), ('nn', nn)], voting=\"soft\", weights=[1,0.2,0.2,1,0.6])\nvoting_clf.fit(X_train, y_train)\npred = voting_clf.predict(X_test)\nprint(accuracy_score(pred, y_test))","033ab713":"The customer gender is almost even, 30% college graduates with half being either Highschool graduates, unkown, or uneducated. The remaining 40% are either current college students,or grad students.\n\nAlmost half are married, 38% single, and the remaining 12 are divorced or unkown. 35% of customers make less than $40k per year which is near the poverty threshold. The rest are more evenly spaced out. 93% of customers choose the cheapest card option (likely the lowest interest rate) with a tiny portion choosing the more expensive cards","6c39f9f0":"### Feature Description\n\nCustomer_Age: Demographic variable - Customer's Age in Years\n\nGender: Demographic variable - M=Male, F=Female\n\nDependent_count: Demographic variable - Number of dependents\n\nEducation_Level: Demographic variable - Educational Qualification of the account holder (example: high school, college graduate, etc.)\n\nMarital_Status: Demographic variable - Married, Single, Divorced, Unknown\n\nIncome_Category: Demographic variable - Annual Income Category of the account holder (<  40K, 40K - 60K,  60K\u2212 80K,  80K\u2212 120K, > $120K, Unknown) <--To be One Hot-encoded\n\nCard_Category: Product Variable - Type of Card (Blue, Silver, Gold, Platinum)\n\nMonths_on_book: Period of relationship with bank\n\nTotal_Relationship_Count: Total no. of products held by the customer\n\nMonths_Inactive_12_mon: No. of months inactive in the last 12 months\n\nContacts_Count_12_mon: No. of Contacts in the last 12 months\n\nCredit_Limit: Credit Limit on the Credit Card\n\nTotal_Revolving_Bal: Total Revolving Balance on the Credit Card\n\nAvg_Open_To_Buy: Open to Buy Credit Line (Average of last 12 months)\n\nTotal_Amt_Chng_Q4_Q1: Change in Transaction Amount (Q4 over Q1)\n\nTotal_Trans_Amt: Total Transaction Amount (Last 12 months)\n\nTotal_Trans_Ct: Total Transaction Count (Last 12 months)\n\nTotal_Ct_Chng_Q4_Q1: Change in Transaction Count (Q4 over Q1)\n\nAvg_Utilization_Ratio: Average Card Utilization Ratio\n\n\n### Target\nAttrition_Flag: Internal event (customer activity) variable - if the account is closed then 1(Attrited Customer) else 0(Existing Customer)","c4b347ce":"## Converting Categorical Features","abb29725":"# ML Model Shortlisting\nTesting out a couple Machine Learning models before we use the Deep Neural Net","c94ccde0":"# Exploring the Data\n\nThe report for the sales team begins here","e836d63c":"### Getting the Data","a8d563ff":"The Backend Engineer at the bank gives us the data through their MySQL database in an easy to use CSV with all missing features replaced by an \"unkown\" string. However, he tried to train a Naive Bayes classifier and accidently left in 2 prediction columns in the data. No Worries. We'll also remove the Clientnumber as this isn't important ","9a667439":"# Conclusion:\n* The highest accuracy achieved was 97.3% which went way above our manager's expectations. However, we have raised the bar, and in further problems, he will be expecting more\n* Total Transaction change,revolving balance,and Number of contacts within the past year are most correlated with a churning customer\n\nThank you so much for reading through to the end, I hope you found this story helpful.\n\n-Alex","47b63b49":"Churned customers have a much smaller revolving balance which, because they don't fully pay off their credit card balance, may signify that they have less disposable income than staying customers that know they can pay off their revolving balance","982f8e4f":"# Avengers Assemble\n\nEnsembling our ML models together to get the highest possible accuracy. It's interesting how the Random Forest and XGBoost Classifiers got a 10% higher accuracy than the shallow neural network, for we did not need to tweak any of its hyperparameters. Perhaps Ockham's Razor is clearly shown here?","e6802065":"The following features are the most correlated (> 0.75%)\n* The months of being a customer with the bank(months on the book) and the Age are positive \n* Credit Limit and Average Open To Buy Credit Line are also positive\n\nThe following are moderately correlated (30-75%)\n* Total Transaction count and Total Relationship count are negative\n* Credit Limit and Average Utilization Ratio are negative\n* Total Revolving balance and Average Utilization Ratio are positive\n* Average Open To Buy and Average Utilization Ration are negative\n","e7604bca":"Churned customers have a lower credit limit, so perhaps increase the credit limit for these","637a1701":"# Data Preprocessing","53c6006d":"Churned customers are likely to hold less credit cards than existing customers which is shown by a lower median . Is there a deal you provide that favors customers with multiple credit cards? (Like customers with spouses, families, or buisnesses that need additional cards).","f440fc8d":"Churned customers will have a lower amount of transactions, which makes sense as they're less involved with this company and will have a smaller transaction change over time as displayed bellow","b23a713e":"Churned customers tend to have slightly more inactive months, but the distribution is more concentrated from the 1-4 months inactive (though this may be from the small sample size)","4c0d2c6b":"### **A lower Total Transaction change,revolving balance, and higher Number of contacts within the past year are most correlated with a churning customer**","f9e8786a":"## Scaling Continous Features","d941eef9":"# The Neural Network\n\nSince this is a standard binary classification problem, we'll use a shallow feedforward neural network with 19 input neurons and 2 output neurons","865c571e":"# Bank Churner Prediction\n\n## The Problem\n\n### Background:\nA manager at a local bank is disturbed with more and more customers leaving their credit card services. They need a way of predicting which customers are most likely to stop using their credit card products (**Customer Churn**) in order to proactively check in on the customer to provide them better services in order to convince them to change their minds. You are given a dataset of 10,000 customers with 18 features per customer. Roughly 16% of the current customer base have churned so far, so it will be difficult to predict the ones who will.\n\nAs you analyze the data, before you create the model, the sales team also needs you to determine the most influential factors that can lead to a customer's decision of leaving the business. The head of the sales department is expecting a report that helps them visualize where the differences lie between churning and non-churning customers. \n\n### Objectives:\n* Identify which customers are most likely to be churned so the bank manager knows who to provide a better service to\n    * The Top Priority is to identify churning customers,as if we predict non-churning customers as churned, it won't harm our business, but predicting churning customers as Non-churning will. False negatives won't hurt us, but False Positives do\n    * This task is binary classification\n* A clean and easy to understand visual report that helps the sales team better visualize what makes a client churn or not churn\n\n**Performance of the model will be mesured with accuracy and the rate of False Positives. The manager is looking for at least a 90% F1 Score accuracy** \n* Precision and Recall Curves as well as the Confusion Matrix will also be used\n\n### Issues:\n* since only 16% of the customers are churned, a data upsampling method (like SMOTE) is needed to match them with the regular customer size to give our model a better chance of catching small details that would be missed had we not upsampled it"}}