{"cell_type":{"86a346cd":"code","08f50131":"code","4298c744":"code","36714b02":"code","12bff1cb":"code","ad75c794":"code","7648a6e3":"code","b98a0072":"code","2cf12d23":"code","9e261c98":"code","8c58ba9d":"code","c6127680":"code","d2fe2250":"code","39201a62":"markdown","47f71323":"markdown","9c9bc33a":"markdown","d5e18b11":"markdown","2920c586":"markdown","d2a71f31":"markdown","1cd2cedd":"markdown","6b39196b":"markdown"},"source":{"86a346cd":"import sys, os, re, csv, codecs, numpy as np, pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport matplotlib as mpl\nmpl.rcParams['figure.figsize'] = (12, 12)\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation,SpatialDropout1D,GRU\n\nfrom tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D,Conv1D,GlobalAveragePooling1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import initializers, regularizers, constraints, optimizers, layers","08f50131":"EMBEDDING_FILE=f'..\/input\/glove6b50d\/glove.6B.50d.txt'\nTRAIN_DATA_FILE=f'..\/input\/researchtopictags\/train.csv'\nTEST_DATA_FILE=f'..\/input\/researchtopictags\/test.csv'","4298c744":"embed_size = 50 # how big is each word vector\nmax_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a comment to use","36714b02":"train_data = pd.read_csv(TRAIN_DATA_FILE)\ntest_data = pd.read_csv(TEST_DATA_FILE)\n\ntrain_data['combined_text'] = train_data['TITLE'] + \"<join>\" + train_data['ABSTRACT']\ntest_data['combined_text'] = test_data['TITLE'] + \"<join>\" + test_data['ABSTRACT']\n\nlist_sentences_train = train_data[\"combined_text\"].fillna(\"_na_\").values\nlist_classes = ['Computer Science','Physics','Mathematics', 'Statistics','Quantitative Biology','Quantitative Finance']\ny = train_data[list_classes].values\nlist_sentences_test = test_data[\"combined_text\"].fillna(\"_na_\").values","12bff1cb":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_te = pad_sequences(list_tokenized_test, maxlen=maxlen)","ad75c794":"def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))","7648a6e3":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nemb_mean,emb_std","b98a0072":"word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","2cf12d23":"def plot_graphs(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_'+string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_'+string])\n    plt.show()","9e261c98":"model1 = tf.keras.Sequential([Input(shape=(maxlen,)),\n                             Embedding(max_features, embed_size, weights=[embedding_matrix]),\n                             Bidirectional(LSTM(50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n                             Bidirectional(LSTM(50, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n                             GlobalMaxPool1D(),\n                             Dense(50, activation=\"relu\"),\n                             Dropout(0.2),\n                             Dense(6, activation=\"sigmoid\")\n                            ])\noptim = tf.keras.optimizers.Adam(learning_rate=1e-3)\nmodel1.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])","8c58ba9d":"history1 = model1.fit(X_t, y, batch_size=32, epochs=10, validation_split=0.3)\nplot_graphs(history1,'loss')","c6127680":"#model2 = tf.keras.Sequential([Input(shape=(maxlen,)),\n#                             Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False),\n#                             SpatialDropout1D(0.2),\n#                             Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1)),\n#                             Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\"),\n#                             GlobalAveragePooling1D(),\n#                             Dense(50, activation=\"relu\"),\n#                             Dropout(0.2),\n#                             Dense(6, activation=\"sigmoid\")\n#                            ])\n#model2.compile(loss='binary_crossentropy', optimizer=optim, metrics=['accuracy'])","d2fe2250":"#history1 = model2.fit(X_t, y, batch_size=32, epochs=4, validation_split=0.3)","39201a62":"Read the glove word vectors (space delimited strings) into a dictionary from word->vector.","47f71323":"We include the GloVe word vectors in our input files. To include these in your kernel, simple click 'input files' at the top of the notebook, and search 'glove' in the 'datasets' section.","9c9bc33a":"Simple bidirectional LSTM with two fully connected layers. We add some dropout to the LSTM since even 2 epochs is enough to overfit.","d5e18b11":"Use these vectors to create our embedding matrix, with random initialization for words that aren't in GloVe. We'll use the same mean and stdev of embeddings the GloVe has when generating the random init.","2920c586":"Now we're ready to fit out model! Use `validation_split` when not submitting.","d2a71f31":"Set some basic config parameters:","1cd2cedd":"Standard keras preprocessing, to turn each comment into a list of word indexes of equal length (with truncation or padding as needed).","6b39196b":"Read in our data and replace missing values:"}}