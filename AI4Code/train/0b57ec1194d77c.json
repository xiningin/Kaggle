{"cell_type":{"1b450d1d":"code","37a336e9":"code","8aa5e6fd":"code","b923e373":"code","065809d2":"code","7347cfb5":"code","c44aa8f5":"code","92c312a7":"code","81b945d8":"code","6cd6b530":"code","b886519a":"code","3d1d6424":"code","509f891a":"code","3c16cd20":"code","c44b03f6":"code","a885d7f3":"code","f58431e2":"code","7b78e94b":"code","9f4d985e":"code","d801327a":"code","04368955":"code","4c483f5b":"code","b332e3a4":"code","03ce526c":"markdown","46c81a51":"markdown","b9d776ac":"markdown","392aa7e7":"markdown","094eea33":"markdown","a1330527":"markdown","6999b4a1":"markdown","dd63bea4":"markdown","880520ac":"markdown","e374a5c8":"markdown","32474d4c":"markdown","2c8f67ee":"markdown","d2fea9a0":"markdown","c64ecab6":"markdown","5d55fd78":"markdown","804465a0":"markdown","4f854a03":"markdown","b34b505e":"markdown","180eedec":"markdown"},"source":{"1b450d1d":"import io\nimport openpyxl\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","37a336e9":"auto_train_ds = pd.read_csv(\"..\/input\/customer-segmentation\/Train.csv\", sep=\",\")\nauto_test_ds = pd.read_csv(\"..\/input\/customer-segmentation\/Test.csv\", sep=\",\")\nauto_ds = pd.concat([auto_train_ds, auto_test_ds])\n\nauto_ds","8aa5e6fd":"#Checking a dataset sample\n\npd.set_option(\"display.max_rows\", 100)\npd.set_option(\"display.max_columns\", 100)\npd.options.display.float_format=\"{:,.2f}\".format\nauto_ds.sample(n=10, random_state=0)","b923e373":"#Checking dataset info by feature\n\nauto_ds.info(verbose=True, null_counts=True)","065809d2":"#Checking the existence of zeros in rows\n\n(auto_ds==0).sum(axis=0).to_excel(\"zeros_per_feature.xlsx\")\n(auto_ds==0).sum(axis=0)","7347cfb5":"#Checking the existence of duplicated rows\n\nauto_ds.duplicated().sum()","c44aa8f5":"#Checking data balancing (for classification)\n\nauto_balancing = pd.DataFrame()\nauto_balancing[\"Count\"] = auto_ds[\"Segmentation\"].value_counts()\nauto_balancing[\"Count%\"] = auto_ds[\"Segmentation\"].value_counts()\/auto_ds.shape[0]*100\n\nauto_balancing","92c312a7":"#Checking basic statistical data by feature\n\nauto_ds.describe(include=\"all\")","81b945d8":"#1\nauto_ds.drop_duplicates(inplace=True)\n\n#2\nauto_ds[\"Work_Experience_to_Age_Ratio\"] = auto_ds[\"Work_Experience\"] \/ auto_ds[\"Age\"]\n\n#3\nauto_ds[\"Ever_Married\"].fillna(\"No\", inplace=True)\nauto_ds[\"Graduated\"].fillna(\"No\", inplace=True)\nauto_ds.dropna(subset=[\"Profession\"], inplace=True)\nauto_ds[\"Family_Size\"].fillna(auto_ds[\"Family_Size\"].mean(), inplace=True)\nauto_ds.dropna(subset=[\"Var_1\"], inplace=True)\nauto_ds[\"Work_Experience\"].replace(0, np.nan, inplace=True)\nauto_ds[\"Work_Experience\"].fillna(auto_ds[\"Work_Experience\"].sum() \/ auto_ds[\"Age\"].sum() * auto_ds[\"Age\"], inplace=True)\nauto_ds[\"Work_Experience_to_Age_Ratio\"].replace(0, np.nan, inplace=True)\nauto_ds[\"Work_Experience_to_Age_Ratio\"].fillna(auto_ds[\"Work_Experience\"] \/ auto_ds[\"Age\"], inplace=True)\n\n#4\nauto_ds.drop([\"ID\"], axis=1, inplace=True)\n\n#5\nauto_ds = pd.concat([auto_ds, pd.get_dummies(auto_ds[\"Gender\"], prefix=\"Gender\")], axis=1)\nauto_ds = pd.concat([auto_ds, pd.get_dummies(auto_ds[\"Ever_Married\"], prefix=\"Ever_Married\")], axis=1)\nauto_ds = pd.concat([auto_ds, pd.get_dummies(auto_ds[\"Graduated\"], prefix=\"Graduated\")], axis=1)\nauto_ds = pd.concat([auto_ds, pd.get_dummies(auto_ds[\"Profession\"], prefix=\"Profession\")], axis=1)\nauto_ds[\"Spending_Score_Level\"] = auto_ds[\"Spending_Score\"].apply(lambda x: [\"Low\", \"Average\", \"High\"].index(x))+1 #Label encoding\nauto_ds = pd.concat([auto_ds, pd.get_dummies(auto_ds[\"Var_1\"], prefix=\"Var_1\")], axis=1)\n#target variable for the ML model (label encoding)\nauto_ds[\"Segmentation_Level\"] = auto_ds[\"Segmentation\"].apply(lambda x: [\"A\", \"B\", \"C\", \"D\"].index(x))+1 #Label encoding\n#target variable for the DL model (one-hot encoding)\nauto_ds = pd.concat([auto_ds, pd.get_dummies(auto_ds[\"Segmentation\"], prefix=\"Segmentation\")], axis=1)\nSegmentation_encoding_dl = np.asarray(auto_ds[[\"Segmentation_A\", \"Segmentation_B\", \"Segmentation_C\", \"Segmentation_D\"]]) #creating for the DL model the response variable through the concatenation of the created dummy columns, forming an array\n\n#6\nauto_ds[\"Age_Range\"] = np.where(auto_ds.Age>=60, \"60+\", np.where(auto_ds.Age>=50, \"50-60\", np.where(auto_ds.Age>=40, \"40-50\", np.where(auto_ds.Age>=30, \"30-40\", np.where(auto_ds.Age>=18, \"18-30\", \"18-\")))))\nauto_ds[\"Work_Experience_Range\"] = np.where(auto_ds.Work_Experience>=10, \"10+\", np.where(auto_ds.Work_Experience>=5, \"5-10\", \"0-5\"))\nauto_ds[\"Family_Size_Range\"] = np.where(auto_ds.Family_Size>=6, \"6+\", np.where(auto_ds.Family_Size>=3, \"3-6\", \"0-3\"))\nauto_ds[\"Work_Experience_to_Age_Ratio_Range\"] = np.where(auto_ds.Work_Experience_to_Age_Ratio>=0.5, \"0.5+\", np.where(auto_ds.Work_Experience_to_Age_Ratio>=0.4, \"0.4-0.5\", np.where(auto_ds.Work_Experience_to_Age_Ratio>=0.3, \"0.3-0.4\", np.where(auto_ds.Work_Experience_to_Age_Ratio>=0.2, \"0.2-0.3\", np.where(auto_ds.Work_Experience_to_Age_Ratio>=0.1, \"0.1-0.2\", \"0+\")))))\n\nauto_ds.to_excel(\"auto_ds_clean.xlsx\")","6cd6b530":"#Plotting Categorical Variables\n\nfig, ax = plt.subplots(1, 2)\nauto_ds[\"Segmentation\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nauto_ds[\"Segmentation\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Segmentation Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nauto_ds[\"Gender\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nauto_ds[\"Gender\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Gender Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nauto_ds[\"Ever_Married\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nauto_ds[\"Ever_Married\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Marriage Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nauto_ds[\"Graduated\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nauto_ds[\"Graduated\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Graduation Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nauto_ds[\"Profession\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nauto_ds[\"Profession\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Profession Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nauto_ds[\"Spending_Score\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nauto_ds[\"Spending_Score\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Spending Score Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\nfig, ax = plt.subplots(1, 2)\nauto_ds[\"Var_1\"].value_counts().plot.bar(color=\"purple\", ax=ax[0])\nauto_ds[\"Var_1\"].value_counts().plot.pie(autopct='%1.1f%%',shadow=True,textprops={\"fontsize\": 10},ax=ax[1])\nfig.suptitle(\"Var 1 Frequency\", fontsize=15)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\n\n#Plotting Numerical Variables\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Age Distribution\", fontsize=15)\nsns.distplot(auto_ds[\"Age\"], ax=ax[0])\nsns.boxplot(auto_ds[\"Age\"], ax=ax[1])\nsns.violinplot(auto_ds[\"Age\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Work Experience Distribution\", fontsize=15)\nsns.distplot(auto_ds[\"Work_Experience\"], ax=ax[0])\nsns.boxplot(auto_ds[\"Work_Experience\"], ax=ax[1])\nsns.violinplot(auto_ds[\"Work_Experience\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Family Size Distribution\", fontsize=15)\nsns.distplot(auto_ds[\"Family_Size\"], ax=ax[0])\nsns.boxplot(auto_ds[\"Family_Size\"], ax=ax[1])\nsns.violinplot(auto_ds[\"Family_Size\"], ax=ax[2])\n\nfig, ax = plt.subplots(1,3)\nfig.suptitle(\"Work Experience to Age Ratio Distribution\", fontsize=15)\nsns.distplot(auto_ds[\"Work_Experience_to_Age_Ratio\"], ax=ax[0])\nsns.boxplot(auto_ds[\"Work_Experience_to_Age_Ratio\"], ax=ax[1])\nsns.violinplot(auto_ds[\"Work_Experience_to_Age_Ratio\"], ax=ax[2])","b886519a":"#Alternatively using Profile Report to see variables statistics and correlations\n\n# from pandas_profiling import ProfileReport\n# profile = ProfileReport(auto_ds, title=\"Automobile Customer Segmentation Classification\")\n# profile.to_file(output_file=\"Automobile_Customer_Segmentation_Classification.html\")","3d1d6424":"#Plotting Bar Charts, also considering all numerical to categorical variables created at the step before\n\nfig, axarr = plt.subplots(4, 3, figsize=(30, 18))\nsns.countplot(x=\"Gender\", hue = \"Segmentation\", data = auto_ds, ax=axarr[0][0])\nsns.countplot(x=\"Ever_Married\", hue = \"Segmentation\", data = auto_ds, ax=axarr[0][1])\nsns.countplot(x=\"Graduated\", hue = \"Segmentation\", data = auto_ds, ax=axarr[0][2])\nsns.countplot(x=\"Profession\", hue = \"Segmentation\", data = auto_ds, ax=axarr[1][0])\nsns.countplot(x=\"Spending_Score\", hue = \"Segmentation\", data = auto_ds, ax=axarr[1][1])\nsns.countplot(x=\"Var_1\", hue = \"Segmentation\", data = auto_ds, ax=axarr[1][2])\nsns.countplot(x=\"Age_Range\", hue = \"Segmentation\", data = auto_ds, ax=axarr[2][0])\nsns.countplot(x=\"Work_Experience_Range\", hue = \"Segmentation\", data = auto_ds, ax=axarr[2][1])\nsns.countplot(x=\"Family_Size_Range\", hue = \"Segmentation\", data = auto_ds, ax=axarr[2][2])\nsns.countplot(x=\"Work_Experience_to_Age_Ratio_Range\", hue = \"Segmentation\", data = auto_ds, ax=axarr[3][0])\n\n#Deleting original categorical columns\n\nauto_ds.drop([\"Segmentation\", \"Gender\", \"Ever_Married\", \"Graduated\", \"Profession\", \"Spending_Score\", \"Var_1\", \"Age_Range\",\n              \"Work_Experience_Range\", \"Family_Size_Range\", \"Work_Experience_to_Age_Ratio_Range\"], axis=1, inplace=True)\n\n#Plotting a Heatmap\n\nfig, ax = plt.subplots(1, figsize=(25,25))\nsns.heatmap(auto_ds.corr(), annot=True, fmt=\",.2f\")\nplt.title(\"Heatmap Correlation\", fontsize=20)\nplt.tick_params(labelsize=12)\nplt.xticks(rotation=90)\nplt.yticks(rotation=45)\n\n#Plotting a Pairplot\n\n# sns.pairplot(auto_ds)","509f891a":"#Plotting a Feature Importance\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom matplotlib import pyplot\n#Defining Xs and y\nX = auto_ds.drop([\"Segmentation_Level\"], axis=1)\ny = auto_ds[\"Segmentation_Level\"]\n#Defining the model\nmodel = RandomForestClassifier().fit(X, y)\n#Getting importance\nimportance = model.feature_importances_\n#Summarizing feature importance\nfor i,v in enumerate(importance):\n    print(\"Feature:{0:}, Score:{1:,.4f}\".format(X.columns[i], v))\n#Plotting feature importance\npd.Series(model.feature_importances_[::-1], index=X.columns[::-1]).plot(kind=\"barh\", figsize=(25,25))","3c16cd20":"#Defining Xs and y\n\nX = auto_ds[[\"Age\", \"Family_Size\", \"Profession_Healthcare\", \"Profession_Entertainment\", \"Profession_Artist\", \"Ever_Married_Yes\",\n             \"Graduated_No\", \"Spending_Score_Level\"]]\ny = auto_ds[\"Segmentation_Level\"]\ny_dl = Segmentation_encoding_dl #for the DL model\n\n#Scaling all features\n\nfrom sklearn.preprocessing import MinMaxScaler\nsc_X = MinMaxScaler()\nX_scaled = sc_X.fit_transform(X)\nX_scaled = pd.DataFrame(X_scaled)\n\n#Setting train\/test split\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, random_state=0)\nX_train_dl, X_test_dl, y_train_dl, y_test_dl = train_test_split(X_scaled, y_dl, random_state=0) #for the DL model","c44b03f6":"#Creating a Logistic Regression model and checking its Metrics\n\nfrom sklearn import linear_model\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n\n#Trying different polynomial degrees\ndegrees = [1, 2, 3]\nprint(\"Testing degrees:\")\nfor a in degrees:\n    poly = PolynomialFeatures(degree=a)\n    X_train_degree = poly.fit_transform(X_train)\n    X_test_degree = poly.fit_transform(X_test)\n    model_lr = linear_model.LogisticRegression(max_iter=1000000000).fit(X_train_degree, y_train.values.ravel())\n    y_preds_train = model_lr.predict(X_train_degree)\n    y_preds_test = model_lr.predict(X_test_degree)\n    accuracy_train = accuracy_score(y_train, y_preds_train)\n    accuracy_test = accuracy_score(y_test, y_preds_test)\n    precision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\n    precision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\n    recall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\n    recall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\n    f1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\n    f1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\n    print(\"Train: Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n    print(\"Test : Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\")\n\n#Choosing the best polynomial degree\nchosen_degree = 2\npoly = PolynomialFeatures(degree=chosen_degree)\n\n#Working on X_train & X_test in the polynomial chosen degree\nX_train_degree = poly.fit_transform(X_train)\nX_test_degree = poly.fit_transform(X_test)\n\n#Fitting to the model\nmodel_lr = linear_model.LogisticRegression(max_iter=1000000000).fit(X_train_degree, y_train.values.ravel())\nprint(f\"Linear Regression Intercept: {model_lr.intercept_}\")\nprint(f\"Linear Regression Coefficients: {model_lr.coef_}, \\n\")\n\n#Getting the predictions & Metrics\ny_preds_train = model_lr.predict(X_train_degree)\ny_preds_test = model_lr.predict(X_test_degree)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\nprint(\"Chosen degree:\")\nprint(\"Train: Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_degree, accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_degree, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\\nConfusion matrix:\")\nconfusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\nprint(f\"{confusion_matrix}, \\n\")\nsns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\nX_degree = poly.fit_transform(X_scaled)\ny_preds_all = model_lr.predict(X_degree)\nauto_ds[\"Segmentation_Level_predicted\"] = y_preds_all\nauto_ds.to_excel(\"model_lr.xlsx\")","a885d7f3":"# #Creating a Logistic Regression CV model and checking its Metrics (for this exercise we\u00b4re skipping this algorithm due to the execution time since we\u00b4d need a more powerful machine)\n\n# #Trying different polynomial degrees\n# degrees = [1, 2]\n# print(\"Testing degrees:\")\n# for a in degrees:\n#     poly = PolynomialFeatures(degree=a)\n#     X_train_degree = poly.fit_transform(X_train)\n#     X_test_degree = poly.fit_transform(X_test)\n#     model_lr_cv = linear_model.LogisticRegressionCV(max_iter=1000000000).fit(X_train_degree, y_train)\n#     y_preds_train = model_lr_cv.predict(X_train_degree)\n#     y_preds_test = model_lr_cv.predict(X_test_degree)\n#     accuracy_train = accuracy_score(y_train, y_preds_train)\n#     accuracy_test = accuracy_score(y_test, y_preds_test)\n#     precision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\n#     precision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\n#     recall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\n#     recall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\n#     f1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\n#     f1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\n#     print(\"Train: Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n#     print(\"Test : Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\")\n\n# #Choosing the best polynomial degree\n# chosen_degree = 2\n# poly = PolynomialFeatures(degree=chosen_degree)\n\n# #Working on X_train & X_test in the polynomial chosen degree\n# X_train_degree = poly.fit_transform(X_train)\n# X_test_degree = poly.fit_transform(X_test)\n\n# #Fitting to the model\n# model_lr_cv = linear_model.LogisticRegressionCV(1000000000).fit(X_train_degree, y_train)\n# print(f\"Linear Regression Intercept: {model_lr_cv.intercept_}\")\n# print(f\"Linear Regression Coefficients: {model_lr_cv.coef_}, \\n\")\n\n# #Getting the predictions & Metrics\n# y_preds_train = model_lr_cv.predict(X_train_degree)\n# y_preds_test = model_lr_cv.predict(X_test_degree)\n# accuracy_train = accuracy_score(y_train, y_preds_train)\n# accuracy_test = accuracy_score(y_test, y_preds_test)\n# precision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\n# precision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\n# recall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\n# recall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\n# f1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\n# f1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\n# print(\"Chosen degree:\")\n# print(\"Train: Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_degree, accuracy_train, precision_train, recall_train, f1_train))\n# print(\"Test : Degree:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_degree, accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\\nConfusion matrix:\")\n# confusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\n# print(f\"{confusion_matrix}, \\n\")\n# sns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n# #Visualizing y_pred in the dataset\n# X_degree = poly.fit_transform(X)\n# y_preds_all = model_lr_cv.predict(X_scaled)\n# auto_ds[\"Segmentation_Level_predicted\"] = y_preds_all\n# auto_ds.to_excel(\"model_lr_cv.xlsx\")","f58431e2":"#Creating a SVM model and checking its Metrics\n\nfrom sklearn import svm\n\n#Fitting to the model\nmodel_svm = svm.SVC().fit(X_train, y_train)\n\n#Getting the predictions & Metrics\ny_preds_train = model_svm.predict(X_train)\ny_preds_test = model_svm.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\nprint(\"Train: Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\\nConfusion matrix:\")\nconfusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\nprint(f\"{confusion_matrix}, \\n\")\nsns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_svm.predict(X_scaled)\nauto_ds[\"Segmentation_Level_predicted\"] = y_preds_all\nauto_ds.to_excel(\"model_svm.xlsx\")","7b78e94b":"#Creating a Naive Bayes model and checking its Metrics\n\nfrom sklearn import naive_bayes\n\n#Fitting to the model\nmodel_nb = naive_bayes.MultinomialNB().fit(X_train, y_train)\n\n#Getting the predictions & Metrics\ny_preds_train = model_nb.predict(X_train)\ny_preds_test = model_nb.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\nprint(\"Train: Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\\nConfusion matrix:\")\nconfusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\nprint(f\"{confusion_matrix}, \\n\")\nsns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_nb.predict(X_scaled)\nauto_ds[\"Segmentation_Level_predicted\"] = y_preds_all\nauto_ds.to_excel(\"model_nb.xlsx\")","9f4d985e":"#Creating a KNN model and checking its Metrics\n\nfrom sklearn import neighbors\n\n#Trying different neighbors\nn_neighbors = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\nprint(\"Testing neighbors:\")\nfor a in n_neighbors:\n    model_knn = neighbors.KNeighborsClassifier(n_neighbors=a).fit(X_train, y_train)\n    y_preds_train = model_knn.predict(X_train)\n    y_preds_test = model_knn.predict(X_test)\n    accuracy_train = accuracy_score(y_train, y_preds_train)\n    accuracy_test = accuracy_score(y_test, y_preds_test)\n    precision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\n    precision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\n    recall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\n    recall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\n    f1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\n    f1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\n    print(\"Train: Neighbors:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n    print(\"Test : Neighbors:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\")\n\n#Choosing the best neighbor\nchosen_neighbor = 10\nmodel_knn = neighbors.KNeighborsClassifier(n_neighbors=chosen_neighbor).fit(X_train, y_train)\ny_preds_train = model_knn.predict(X_train)\ny_preds_test = model_knn.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\nprint(\"Chosen neighbors:\")\nprint(\"Train: Neighbors:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_neighbor, accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Neighbors:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_neighbor, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\\nConfusion matrix:\")\nconfusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\nprint(f\"{confusion_matrix}, \\n\")\nsns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_knn.predict(X_scaled)\nauto_ds[\"Segmentation_Level_predicted\"] = y_preds_all\nauto_ds.to_excel(\"model_knn.xlsx\")","d801327a":"#Creating a Random Forest model and checking its Metrics\n\nfrom sklearn import ensemble\n\n#Trying different depths\ndepths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nprint(\"Testing depths:\")\nfor a in depths:\n    model_rf = ensemble.RandomForestClassifier(max_depth=a, random_state=0).fit(X_train, y_train)\n    y_preds_train = model_rf.predict(X_train)\n    y_preds_test = model_rf.predict(X_test)\n    accuracy_train = accuracy_score(y_train, y_preds_train)\n    accuracy_test = accuracy_score(y_test, y_preds_test)\n    precision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\n    precision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\n    recall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\n    recall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\n    f1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\n    f1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\n    print(\"Train: Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n    print(\"Test : Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\")\n\n#Choosing the best depth\nchosen_depth = 5\nmodel_rf = ensemble.RandomForestClassifier(max_depth=chosen_depth, random_state=0).fit(X_train, y_train)\ny_preds_train = model_rf.predict(X_train)\ny_preds_test = model_rf.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\nprint(\"Chosen depth:\")\nprint(\"Train: Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_depth, accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_depth, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\\nConfusion matrix:\")\nconfusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\nprint(f\"{confusion_matrix}, \\n\")\nsns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_rf.predict(X_scaled)\nauto_ds[\"Segmentation_Level_predicted\"] = y_preds_all\nauto_ds.to_excel(\"model_rf.xlsx\")","04368955":"#Creating a XGBoost model and checking its Metrics\n\nfrom xgboost import XGBClassifier\n\n#Trying different depths\ndepths = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nprint(\"Testing depths:\")\nfor a in depths:\n    model_xgbc = XGBClassifier(max_depth=a, objective=\"multi:softmax\", num_class=4, random_state=0).fit(X_train, y_train)\n    y_preds_train = model_xgbc.predict(X_train)\n    y_preds_test = model_xgbc.predict(X_test)\n    accuracy_train = accuracy_score(y_train, y_preds_train)\n    accuracy_test = accuracy_score(y_test, y_preds_test)\n    precision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\n    precision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\n    recall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\n    recall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\n    f1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\n    f1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\n    print(\"Train: Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_train, precision_train, recall_train, f1_train))\n    print(\"Test : Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(a, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\")\n\n#Choosing the best depth\nchosen_depth = 2\nmodel_xgbc = XGBClassifier(max_depth=chosen_depth, objective=\"multi:softmax\", num_class=4, random_state=0).fit(X_train, y_train)\ny_preds_train = model_xgbc.predict(X_train)\ny_preds_test = model_xgbc.predict(X_test)\naccuracy_train = accuracy_score(y_train, y_preds_train)\naccuracy_test = accuracy_score(y_test, y_preds_test)\nprecision_train = precision_score(y_train, y_preds_train, average=\"weighted\")\nprecision_test = precision_score(y_test, y_preds_test, average=\"weighted\")\nrecall_train = recall_score(y_train, y_preds_train, average=\"weighted\")\nrecall_test = recall_score(y_test, y_preds_test, average=\"weighted\")\nf1_train = f1_score(y_train, y_preds_train, average=\"weighted\")\nf1_test = f1_score(y_test, y_preds_test, average=\"weighted\")\nprint(\"Chosen depth:\")\nprint(\"Train: Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_depth, accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Depth:{0:,.0f}, Accuracy:{1:,.3f}, Precision:{2:,.3f}, Recall:{3:,.3f}, F1:{4:,.3f}\".format(chosen_depth, accuracy_test, precision_test, recall_test, f1_test))\nprint(\"\\nConfusion matrix:\")\nconfusion_matrix = pd.crosstab(y_test, y_preds_test, rownames=[\"Actual\"], colnames=[\"Predicted\"])\nprint(f\"{confusion_matrix}, \\n\")\nsns.heatmap(confusion_matrix, annot=True, fmt='0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_xgbc.predict(X_scaled)\nauto_ds[\"Segmentation_Level_predicted\"] = y_preds_all\nauto_ds.to_excel(\"model_xgbc.xlsx\")","4c483f5b":"#Creating a Deep Learning model and checking its Metrics\n\nfrom keras import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping\npd.options.mode.chained_assignment = None\n\n#Creating a model\nmodel_dl = Sequential()\n\n#Input and First Hidden Layer\nmodel_dl.add(Dense(units=256, activation=\"relu\", input_dim=X_train.shape[1]))\n\n#Output Layer\nmodel_dl.add(Dense(units=4, activation=\"softmax\"))\n\n#Compiling the neural network\nmodel_dl.compile(optimizer=\"Adam\", loss=\"CategoricalCrossentropy\", metrics=[\"CategoricalAccuracy\"])\n\n#Fitting to the model\nmodel_dl.fit(X_train_dl, y_train_dl, epochs=100)\n\n#Getting the predictions & Metrics\n#Training set\ny_preds_train = model_dl.predict(X_train_dl)\ny_preds_train = pd.DataFrame(y_preds_train)\n#Creating y_preds_all columns with the probabilities in the original dataset\nauto_ds_train = pd.DataFrame()\nauto_ds_train[\"Segmentation_A_predicted_train\"] = y_preds_train[0]\nauto_ds_train[\"Segmentation_B_predicted_train\"] = y_preds_train[1]\nauto_ds_train[\"Segmentation_C_predicted_train\"] = y_preds_train[2]\nauto_ds_train[\"Segmentation_D_predicted_train\"] = y_preds_train[3]\n#Finding the highest probability per row\nmax_prob_per_row_train_ds = auto_ds_train[[\"Segmentation_A_predicted_train\", \"Segmentation_B_predicted_train\", \"Segmentation_C_predicted_train\", \"Segmentation_D_predicted_train\"]].idxmax(axis=1)\nmax_prob_per_row_train_ds = pd.DataFrame(max_prob_per_row_train_ds)\nmax_prob_per_row_train_ds = max_prob_per_row_train_ds.reset_index(drop=True)\nauto_ds_train[\"Segmentation_predicted_train\"] = max_prob_per_row_train_ds\n#Setting ones and zeros according to the probabilities\nfor i in range(0, len(max_prob_per_row_train_ds)):\n    if  max_prob_per_row_train_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_A_predicted_train\":\n        auto_ds_train[\"Segmentation_A_predicted_train\"][i] = 1\n        auto_ds_train[\"Segmentation_B_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_C_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_D_predicted_train\"][i] = 0    \n    elif max_prob_per_row_train_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_B_predicted_train\":\n        auto_ds_train[\"Segmentation_A_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_B_predicted_train\"][i] = 1\n        auto_ds_train[\"Segmentation_C_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_D_predicted_train\"][i] = 0\n    elif max_prob_per_row_train_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_C_predicted_train\":\n        auto_ds_train[\"Segmentation_A_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_B_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_C_predicted_train\"][i] = 1\n        auto_ds_train[\"Segmentation_D_predicted_train\"][i] = 0\n    elif max_prob_per_row_train_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_D_predicted_train\":\n        auto_ds_train[\"Segmentation_A_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_B_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_C_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_D_predicted_train\"][i] = 1\n    else:\n        auto_ds_train[\"Segmentation_A_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_B_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_C_predicted_train\"][i] = 0\n        auto_ds_train[\"Segmentation_D_predicted_train\"][i] = 0\n#Creating for the DL model the response variable through the concatenation of the created dummy columns, forming an array\ny_preds_train_encoded = np.asarray(auto_ds_train[[\"Segmentation_A_predicted_train\", \"Segmentation_B_predicted_train\", \"Segmentation_C_predicted_train\", \"Segmentation_D_predicted_train\"]])\n\n#Test set\ny_preds_test = model_dl.predict(X_test_dl)\ny_preds_test = pd.DataFrame(y_preds_test)\n#Creating y_preds_all columns with the probabilities in the original dataset\nauto_ds_test = pd.DataFrame()\nauto_ds_test[\"Segmentation_A_predicted_test\"] = y_preds_test[0]\nauto_ds_test[\"Segmentation_B_predicted_test\"] = y_preds_test[1]\nauto_ds_test[\"Segmentation_C_predicted_test\"] = y_preds_test[2]\nauto_ds_test[\"Segmentation_D_predicted_test\"] = y_preds_test[3]\n#Finding the highest probability per row\nmax_prob_per_row_test_ds = auto_ds_test[[\"Segmentation_A_predicted_test\", \"Segmentation_B_predicted_test\", \"Segmentation_C_predicted_test\", \"Segmentation_D_predicted_test\"]].idxmax(axis=1)\nmax_prob_per_row_test_ds = pd.DataFrame(max_prob_per_row_test_ds)\nmax_prob_per_row_test_ds = max_prob_per_row_test_ds.reset_index(drop=True)\nauto_ds_test[\"Segmentation_predicted_test\"] = max_prob_per_row_test_ds\n#Setting ones and zeros according to the probabilities\nfor i in range(0, len(max_prob_per_row_test_ds)):\n    if  max_prob_per_row_test_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_A_predicted_test\":\n        auto_ds_test[\"Segmentation_A_predicted_test\"][i] = 1\n        auto_ds_test[\"Segmentation_B_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_C_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_D_predicted_test\"][i] = 0    \n    elif max_prob_per_row_test_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_B_predicted_test\":\n        auto_ds_test[\"Segmentation_A_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_B_predicted_test\"][i] = 1\n        auto_ds_test[\"Segmentation_C_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_D_predicted_test\"][i] = 0\n    elif max_prob_per_row_test_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_C_predicted_test\":\n        auto_ds_test[\"Segmentation_A_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_B_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_C_predicted_test\"][i] = 1\n        auto_ds_test[\"Segmentation_D_predicted_test\"][i] = 0\n    elif max_prob_per_row_test_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_D_predicted_test\":\n        auto_ds_test[\"Segmentation_A_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_B_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_C_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_D_predicted_test\"][i] = 1\n    else:\n        auto_ds_test[\"Segmentation_A_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_B_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_C_predicted_test\"][i] = 0\n        auto_ds_test[\"Segmentation_D_predicted_test\"][i] = 0\n#Creating for the DL model the response variable through the concatenation of the created dummy columns, forming an array\ny_preds_test_encoded = np.asarray(auto_ds_test[[\"Segmentation_A_predicted_test\", \"Segmentation_B_predicted_test\", \"Segmentation_C_predicted_test\", \"Segmentation_D_predicted_test\"]])\n\naccuracy_train = accuracy_score(y_train_dl, y_preds_train_encoded)\naccuracy_test = accuracy_score(y_test_dl, y_preds_test_encoded)\nprecision_train = precision_score(y_train_dl, y_preds_train_encoded, average=\"weighted\")\nprecision_test = precision_score(y_test_dl, y_preds_test_encoded, average=\"weighted\")\nrecall_train = recall_score(y_train_dl, y_preds_train_encoded, average=\"weighted\")\nrecall_test = recall_score(y_test_dl, y_preds_test_encoded, average=\"weighted\")\nf1_train = f1_score(y_train_dl, y_preds_train_encoded, average=\"weighted\")\nf1_test = f1_score(y_test_dl, y_preds_test_encoded, average=\"weighted\")\nprint(\"Train: Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_train, precision_train, recall_train, f1_train))\nprint(\"Test : Accuracy:{0:,.3f}, Precision:{1:,.3f}, Recall:{2:,.3f}, F1:{3:,.3f}\".format(accuracy_test, precision_test, recall_test, f1_test))\n# print(\"\\nConfusion matrix:\")\n# from sklearn.metrics import confusion_matrix\n# confusion_matrix = confusion_matrix(y_test_dl, y_preds_test_encoded)\n# print(f\"{confusion_matrix}, \\n\")\n# sns.heatmap(confusion_matrix, annot=True, fmt='.0f')\n\n#Visualizing y_pred in the dataset\ny_preds_all = model_dl.predict(X_scaled)\ny_preds_all = pd.DataFrame(y_preds_all)\n#Creating y_preds_all columns with the probabilities in the original dataset\nauto_ds[\"Segmentation_A_predicted\"] = y_preds_all[0]\nauto_ds[\"Segmentation_B_predicted\"] = y_preds_all[1]\nauto_ds[\"Segmentation_C_predicted\"] = y_preds_all[2]\nauto_ds[\"Segmentation_D_predicted\"] = y_preds_all[3]\n#Finding the highest probability per row\nmax_prob_per_row_all_ds = auto_ds[[\"Segmentation_A_predicted\", \"Segmentation_B_predicted\", \"Segmentation_C_predicted\", \"Segmentation_D_predicted\"]].idxmax(axis=1)\nmax_prob_per_row_all_ds = pd.DataFrame(max_prob_per_row_all_ds)\nmax_prob_per_row_all_ds = max_prob_per_row_all_ds.reset_index(drop=True)\nauto_ds[\"Segmentation_predicted\"] = max_prob_per_row_all_ds\n#Setting ones and zeros according to the probabilities\nfor i in range(0, len(max_prob_per_row_all_ds)):\n    if  max_prob_per_row_all_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_A_predicted\":\n        auto_ds[\"Segmentation_A_predicted\"][i] = 1\n        auto_ds[\"Segmentation_B_predicted\"][i] = 0\n        auto_ds[\"Segmentation_C_predicted\"][i] = 0\n        auto_ds[\"Segmentation_D_predicted\"][i] = 0    \n    elif max_prob_per_row_all_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_B_predicted\":\n        auto_ds[\"Segmentation_A_predicted\"][i] = 0\n        auto_ds[\"Segmentation_B_predicted\"][i] = 1\n        auto_ds[\"Segmentation_C_predicted\"][i] = 0\n        auto_ds[\"Segmentation_D_predicted\"][i] = 0\n    elif max_prob_per_row_all_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_C_predicted\":\n        auto_ds[\"Segmentation_A_predicted\"][i] = 0\n        auto_ds[\"Segmentation_B_predicted\"][i] = 0\n        auto_ds[\"Segmentation_C_predicted\"][i] = 1\n        auto_ds[\"Segmentation_D_predicted\"][i] = 0\n    elif max_prob_per_row_all_ds.iloc[[i], [0]].T.squeeze() == \"Segmentation_D_predicted\":\n        auto_ds[\"Segmentation_A_predicted\"][i] = 0\n        auto_ds[\"Segmentation_B_predicted\"][i] = 0\n        auto_ds[\"Segmentation_C_predicted\"][i] = 0\n        auto_ds[\"Segmentation_D_predicted\"][i] = 1\n    else:\n        auto_ds[\"Segmentation_A_predicted\"][i] = 0\n        auto_ds[\"Segmentation_B_predicted\"][i] = 0\n        auto_ds[\"Segmentation_C_predicted\"][i] = 0\n        auto_ds[\"Segmentation_D_predicted\"][i] = 0\nauto_ds.to_excel(\"model_dl.xlsx\")","b332e3a4":"#Entering Xs\n\n# age_input = float(input(\"Enter the client\u00b4s age: \"))\n# fam_input = int(input(\"Enter the client\u00b4s family size: \"))\n# prof_hc_input = str(input(\"Is the client\u00b4s profession Healthcare (Yes\/No)? \"))\n# if prof_hc_input == \"Yes\":\n#     prof_hc_input = 1\n# else:\n#     prof_hc_input = 0\n# prof_e_input = str(input(\"Is the client\u00b4s profession Entertainment (Yes\/No)? \"))\n# if prof_e_input == \"Yes\":\n#     prof_e_input = 1\n# else:\n#     prof_e_input = 0\n# prof_a_input = str(input(\"Is the client\u00b4s profession Artist (Yes\/No)? \"))\n# if prof_a_input == \"Yes\":\n#     prof_a_input = 1\n# else:\n#     prof_a_input = 0\n# married_input = str(input(\"Has the client ever married (Yes\/No)? \"))\n# if married_input == \"Yes\":\n#     married_input = 1\n# else:\n#     married_input = 0\n# grad_n_input = str(input(\"Is the client graduated (Yes\/No)? \"))\n# if grad_n_input == \"Yes\":\n#     grad_n_input = 0\n# else:\n#     grad_n_input = 1\n# spend_input = int(input(\"Enter the client\u00b4s score level: \"))\n\n#Defining Xs\n# X_mod_dep = pd.DataFrame({\"Age\":[age_input], \"Family_Size\":[fam_input], \"Profession_Healthcare\":[prof_hc_input], \n#                           \"Profession_Entertainment\":[prof_e_input], \"Profession_Artist\":[prof_a_input], \n#                           \"Ever_Married_Yes\":[married_input], \"Graduated_No\":[grad_n_input], \"Spending_Score_Level\":[spend_input]})\n\n#Choosing an specific client for testing:\nX_mod_dep = pd.DataFrame({\"Age\":[79], \"Family_Size\":[1], \"Profession_Healthcare\":[0], \n                          \"Profession_Entertainment\":[0], \"Profession_Artist\":[1], \n                          \"Ever_Married_Yes\":[1], \"Graduated_No\":[0], \"Spending_Score_Level\":[3]})\n\n#Appending X_mod_dep to original X dataframe, so we can scale it all together next\n\nX_with_X_mode_dep = X.append(X_mod_dep)\nX_with_X_mode_dep = X_with_X_mode_dep.reset_index(drop=True)\n\n#Scaling all features\n\nfrom sklearn.preprocessing import MinMaxScaler\nsc_X = MinMaxScaler()\nX_scaled = sc_X.fit_transform(X_with_X_mode_dep)\nX_scaled = pd.DataFrame(X_scaled)\n\n#Recovering X_mod_dep row in dataframe after scaling\n\nX_mod_dep = X_scaled.tail(1)\n\n#Predicting results\n\nprediction = model_xgbc.predict(X_mod_dep)\nif prediction == 1:\n    prediction_answer = \"A\"\nif prediction == 2:\n    prediction_answer = \"B\"\nif prediction == 3:\n    prediction_answer = \"C\"\nif prediction == 4:\n    prediction_answer = \"D\"\n\nprint(\"\")\nprint(f\"This client\u00b4s predicted Segmentation is: {prediction_answer}.\")","03ce526c":"# 5. Data Cleaning\n\n    We\u00b4ll perform the following:\n\n    1. Remove duplicated rows (38 in total)\n\n\n    2. Create a calculated column (Work_Experience_to_Age_Ratio) that could potentialy be a relevant feature to the model (to be tested later)\n\n\n    3. Treat missing values:\n        *   3.1 Ever_Married: consider missing values as No\n        *   3.2 Graduated: consider missing values as No\n        *   3.3 Profession: delete (it\u00b4s not representative the amount of blanks)\n        *   3.4 Family_Size: mean\n        *   3.5 Var_1: delete (it\u00b4s not representative the amount of blanks)\n        *   3.6 Work_Experience: estimate based on Age\n         \n    \n    4. Remove column ID as it\u00b4s not important to the model\n           \n        \n    5. Convert categorical variables (Gender, Ever_Married, Graduated, Profession, Spending_Score, Var_1, Segmentation) to dummies\n    \n    \n    6. Convert all numerical variables (Age, Work_Experience, Family_Size, Work_Experience_to_Age_Ratio) to categorical ranges (to be used in next step when analyzing correlations)\n    \n\n    * No outliers found\n    * The entire dataset will be taken","46c81a51":"# 9.7 XGBoost","b9d776ac":"# 9.6 Random Forest","392aa7e7":"# 4. Data Preliminary Exploration","094eea33":"# 9.4 Naive Bayes","a1330527":"# 8. Data Modelling","6999b4a1":"# 9.5 KNN","dd63bea4":"# 9.3 SVM","880520ac":"# 6.\tData Exploration","e374a5c8":"# 9. Machine Learning Algorithms Implementation & Assessment","32474d4c":"# 11. Conclusions\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\nIn this project we went through all the process from defining the business objective, collecting data, exploring features and distributions, treating data, understanding correlations, selecting relevant features, data modelling and presenting 7 different algorithms with metrics to select the best to predict the Customer Segmentation, what will help the business adopt the best marketing strategies to each of them and bring more market share and revenue to the company. The chosen model was XGBoost since it\u00b4s the most accurate, although it has limitations and doesn\u2019t present a high accuracy. We could reach a more robust model having more data about customers, it\u00b4s something to explore and go deeper in the organization with the business team and the data engineer in order to explore if more relevant features are available.","2c8f67ee":"# 1. Introduction: Business Goal & Problem Definition\n\nThe goal of this project is to study and predict the right group of new customers for an automotive company, so the company can adopt the specific proven marketing strategy to each of them and be more succesful in the business. For that we\u00b4ll use the Customer Segmentation Classification Dataset available in Kaggle, containing 10,695 observations, each with the following attributes:\n\nIF YOU LIKE IT OR IF IT HELPS YOU SOMEHOW, COULD YOU PLEASE UPVOTE? THANK YOU VERY MUCH!!!\n\n* ID\tUnique ID\n* Gender\tGender of the customer\n* Ever_Married\tMarital status of the customer\n* Age\tAge of the customer\n* Graduated\tIs the customer a graduate?\n* Profession\tProfession of the customer\n* Work_Experience\tWork Experience in years\n* Spending_Score\tSpending score of the customer\n* Family_Size\tNumber of family members for the customer (including the customer)\n* Var_1\tAnonymised Category for the customer\n* Segmentation\t(target) Customer Segment of the customer","d2fea9a0":"# 10. Model Deployment","c64ecab6":"# 9.8 Deep Learning","5d55fd78":"# 9.1 Logistic Regression","804465a0":"# 2. Importing Basic Libraries","4f854a03":"# 3. Data Collection","b34b505e":"# 9.2 Logistic Regression CV","180eedec":"# 7. Correlations Analysis & Features Selection"}}