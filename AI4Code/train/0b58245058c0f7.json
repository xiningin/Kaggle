{"cell_type":{"36ee92db":"code","c3f1adea":"code","eb5158a2":"code","f6db4e4b":"code","6b37f072":"code","55a6632c":"code","dc4f33de":"code","c46f3d0a":"code","40ee65a5":"code","61a6c3d9":"code","25fa3d62":"code","9b345360":"code","7a40f0ae":"code","8e33a48d":"code","9d22d66e":"code","92dbb43c":"code","7e3e7248":"code","38e5b66a":"code","9400416b":"code","3b226f4b":"code","66578aa2":"code","68589e8c":"code","484e7e44":"code","df14e4bf":"code","c9e99d34":"code","4f692cd4":"code","dfba4c61":"code","255bbe0c":"code","16d56e78":"code","da017b9f":"code","f7c16430":"code","3856cea8":"code","5874bfa6":"code","b3255c62":"code","e3c85746":"code","98068fdc":"code","f181daed":"markdown","5f9bcfae":"markdown","c64cdfc4":"markdown","cc321337":"markdown","2fdec7d7":"markdown","6f1ed3cc":"markdown","3f600099":"markdown","d39969aa":"markdown","e38e72d0":"markdown","cf9fc688":"markdown","080611c0":"markdown","caafdd75":"markdown","38cac64a":"markdown"},"source":{"36ee92db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c3f1adea":"import matplotlib.pyplot as plt\nfrom gensim.models import KeyedVectors\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nimport time\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\n\nfrom keras.layers import Dense, LSTM, Flatten, Embedding\nfrom keras.utils import to_categorical\nfrom keras.backend import clear_session","eb5158a2":"# load Google's pre-trained word2vec embeddings\nfilename = \"\/kaggle\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin\"\n\nstart = time.time()\ngoogle_embeddings = KeyedVectors.load_word2vec_format(filename, binary=True)\n\nprint(\"Load time (seconds): \", (time.time() - start))","f6db4e4b":"# load Stanford's pre-trained GloVe embeddings\nglove_file = \"\/kaggle\/input\/glove-embeddings\/glove.6B.300d.txt\"\nglove_word2vec_file = \"glove.6B.100d.txt.word2vec\"\n\nglove2word2vec(glove_file, glove_word2vec_file)","6b37f072":"# glove embeddings\nstart = time.time()\n\nglove_embeddings = KeyedVectors.load_word2vec_format(glove_word2vec_file, binary=False)\n\nprint(\"Load time (seconds): \", (time.time() - start))","55a6632c":"# load data\ndata = pd.read_csv(\"\/kaggle\/input\/enron-email-classification-using-machine-learning\/preprocessed.csv\")\n\n# view first 5 rows of the dataframe 'data'\ndata.head()","dc4f33de":"# shape of the data\ndata.shape","c46f3d0a":"# count number of instances in each folder\ndata['X-Folder'].unique()","40ee65a5":"def label_encoder(data):\n    class_le = LabelEncoder()\n    # apply label encoder on the 'X-Folder' column\n    y = class_le.fit_transform(data['X-Folder'])\n    return y","61a6c3d9":"y = label_encoder(data)\ninput_data = data['text']","25fa3d62":"# split the data into training and test set\nX_train, X_test, y_train, y_test = train_test_split(input_data, y, test_size=0.1)","9b345360":"# one-hot encode the output labels\nY_train = to_categorical(y_train, 20)\nY_test = to_categorical(y_test, 20)","7a40f0ae":"# prepare tokenizer\nt = Tokenizer()\n\n# fit the tokenizer on the docs\nt.fit_on_texts(input_data)\nvocab_size = len(t.word_index) + 1\n\n# integer encode the documents\nX_train_encoded_docs = t.texts_to_sequences(X_train)\nX_test_encoded_docs = t.texts_to_sequences(X_test)","8e33a48d":"# pad documents to a max length of 150 words\nmax_length = 150\nX_train_padded_docs = pad_sequences(X_train_encoded_docs, maxlen=max_length, padding='post')\nX_test_padded_docs = pad_sequences(X_test_encoded_docs, maxlen=max_length, padding='post')\n\nprint(X_train_padded_docs[0])","9d22d66e":"# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab_size, 300))\n\nfor word, i in t.word_index.items():\n    \n    try:\n        embedding_vector = google_embeddings[word]\n    \n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector \n    except:\n        pass","92dbb43c":"# define the model\nmodel = Sequential()\ne = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False)\nmodel.add(e)\nmodel.add(LSTM(100, dropout=0.2))\nmodel.add(Flatten())\nmodel.add(Dense(20, activation='softmax'))\n\n\n# compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# summarize the model\nmodel.summary()","7e3e7248":"clear_session()","38e5b66a":"# train the model\nclear_session()\nhistory = model.fit(X_train_padded_docs, Y_train, epochs=60, verbose=1, validation_split=0.1)","9400416b":"# evaluate the model on the test set\naccr = model.evaluate(X_test_padded_docs, Y_test)\nprint(\"Test Set: \\n Loss: {:0.3f}\\n Accuracy: {:0.3f}\".format(accr[0], accr[1]))","3b226f4b":"# plot the loss\nplt.title(\"Word2Vec Loss\")\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='validation')\nplt.legend()\nplt.show()","66578aa2":"# plot the accuracy\nplt.title(\"Word2Vec Accuracy\")\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='validation')\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel(\"Accuracy\")\nplt.show()","68589e8c":"# create a weight matrix for words in training docs\nembedding_matrix = np.zeros((vocab_size, 300))\n\nfor word, i in t.word_index.items():\n    \n    try:\n        embedding_vector = glove_embeddings[word]\n    \n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector \n    except:\n        pass","484e7e44":"# define the model\nmodel2 = Sequential()\ne = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable=False)\nmodel2.add(e)\nmodel2.add(LSTM(100, dropout=0.2))\nmodel2.add(Flatten())\nmodel2.add(Dense(20, activation='softmax'))\n\n\n# compile the model\nmodel2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# summarize the model\nmodel2.summary()","df14e4bf":"# fit the model\nhistory2 = model2.fit(X_train_padded_docs, Y_train, epochs=60, verbose=1, validation_split=0.1)","c9e99d34":"# evaluate the model2 on the test set\naccr2 = model2.evaluate(X_test_padded_docs, Y_test)\nprint(\"Test Set: \\n Loss: {:0.3f}\\n Accuracy: {:0.3f}\".format(accr2[0], accr2[1]))","4f692cd4":"# plot the loss\nplt.title(\"Glove Word2Vec Loss\")\nplt.plot(history2.history['loss'], label='train')\nplt.plot(history2.history['val_loss'], label='validation')\nplt.legend()\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"loss\")\nplt.show()","dfba4c61":"# plot the accuracy\nplt.title(\"GloVe Word2Vec Accuracy\")\nplt.plot(history2.history['accuracy'], label='train')\nplt.plot(history2.history['val_accuracy'], label='validation')\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel(\"Accuracy\")\nplt.show()","255bbe0c":"# define the model\nmodel3 = Sequential()\nmodel3.add(Embedding(vocab_size, 300, input_length=max_length))\nmodel3.add(LSTM(100, dropout=0.4))\nmodel3.add(Flatten())\nmodel3.add(Dense(20, activation='softmax'))\n\n# compile the model\nmodel3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# summarize the model\nmodel3.summary()","16d56e78":"# train the model\nhistory3 = model3.fit(X_train_padded_docs, Y_train, validation_split=0.1, epochs=20, verbose=1)","da017b9f":"# save learned word embeddings\nown_embeddings = model3.get_layer('embedding_2').get_weights()[0]\n\ncustom_w2v = {}\n\nfor word, index in t.word_index.items():\n    custom_w2v[word] = own_embeddings[index]\n    \nimport pickle    \n# save to file\nwith open(\"own_embeddings.pkl\", \"wb\") as handle:\n    pickle.dump(custom_w2v, handle, protocol=pickle.HIGHEST_PROTOCOL)","f7c16430":"# load own_embeddings\n#with open(\"own_embeddings.pkl\", \"rb\") as handle:\n #   own_embeddings = pickle.load(handle)","3856cea8":"# evaluate the model\naccr3 = model3.evaluate(X_test_padded_docs, Y_test)\n\nprint(\"Test Set:\\n  Loss: {:0.3f}\\n   Accuracy: {:0.3f}\".format(accr3[0], accr3[1]))","5874bfa6":"# plot the loss\nplt.title(\"Loss\")\nplt.plot(history3.history['loss'], label='train')\nplt.plot(history3.history['val_loss'], label='validation')\nplt.legend()\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"loss\")\nplt.show()","b3255c62":"# plot the accuracy\nplt.title(\"Accuracy\")\nplt.plot(history3.history['accuracy'], label='train')\nplt.plot(history3.history['val_accuracy'], label='validation')\nplt.legend()\nplt.xlabel('Epochs')\nplt.ylabel(\"Accuracy\")\nplt.show()","e3c85746":"# create a dataframe to store results\nres_data = {\n    \"Technique\": ['Word2Vec', 'GloVe', 'Training data Embeddings'],\n    \"test accuracy\": [accr[1], accr2[1], accr3[1]]\n}\nresult = pd.DataFrame(res_data)\nresult","98068fdc":"data.shape","f181daed":"#### 2.1 Google's Word Embeddings","5f9bcfae":"## 8. Results","c64cdfc4":"- **Word2Vec:** Google's pre-trained word embeddings Google news data (about 100 billion words).\n- **GloVe:** Stanford's pre-trained word embeddings trained on Wikipedia data with 6 billion tokens and a 400,000 word vocabulary.\n-  **Training data Embeddings:** Embeddings learned from training data\n- LSTM with one hidden layer and 100 units has been trained to classify folders based on these word embeddings.","cc321337":"## 5. Using Google's Pre-trained Word Embeddings","2fdec7d7":"#### 4.1 Encode class labels","6f1ed3cc":"## 6. Using GloVe Word Embeddings","3f600099":"## Enron Email Classification using Word Embeddings and LSTM\n\nyou can find pre-processing of 'preprocessed.csv' for machine learning classifiers at:\n\n[https:\/\/www.kaggle.com\/ankur561999\/enron-email-classification-using-machine-learning](https:\/\/www.kaggle.com\/ankur561999\/enron-email-classification-using-machine-learning)\n\nand data cleaning of enron email dataset at:\n\n[https:\/\/www.kaggle.com\/ankur561999\/data-cleaning-enron-email-dataset](https:\/\/www.kaggle.com\/ankur561999\/data-cleaning-enron-email-dataset)","d39969aa":"## 3. Load Data","e38e72d0":"## 7. Word Embeddings using Training Data","cf9fc688":"## 1. Import necessary libraries","080611c0":"## 2. Load Pre-trained Embeddings","caafdd75":"## 4. Prepare Data","38cac64a":"#### 2.2 Stanford's Word Embeddings"}}