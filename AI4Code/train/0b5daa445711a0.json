{"cell_type":{"212e6e28":"code","53aa3eaf":"code","9a38865c":"code","9e4bb2b6":"code","fc34f580":"code","065c4a8f":"code","156aba08":"code","766989ae":"code","ba5281e4":"code","36b88bd2":"code","e3cc0a60":"code","ef31a9b8":"code","1208e023":"code","be072db6":"markdown","be7ec451":"markdown","162559d0":"markdown","f2d4900c":"markdown","4b060625":"markdown","09e92911":"markdown","f869cac0":"markdown","003bfe2d":"markdown","76e2df2c":"markdown"},"source":{"212e6e28":"import numpy as np\nimport pandas as pd\nimport pickle\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel\nfrom sklearn.cluster import MiniBatchKMeans","53aa3eaf":"# Read the articles\nX = pd.read_csv(\"..\/input\/all-the-news\/articles1.csv\")\nX = X[pd.isna(X['title'])==False]\nX = X[pd.isna(X['content'])==False]","9a38865c":"# Preview the data\nX.head(3)","9e4bb2b6":"# data information\nX.info()","fc34f580":"# tfidf calculation\ntext_content = X['content']\nvector = TfidfVectorizer(max_df=0.3,         # drop words that occur in more than X percent of documents\n                             #min_df=8,      # only use words that appear at least X times\n                             stop_words='english', # remove stop words\n                             lowercase=True, # Convert everything to lower case \n                             use_idf=True,   # Use idf\n                             norm=u'l2',     # Normalization\n                             smooth_idf=True # Prevents divide-by-zero errors\n                            )\ntfidf = vector.fit_transform(text_content)","065c4a8f":"# Save objects on filesystem\n#pickle.dump(X, open('output\/X', 'wb')) \n#pickle.dump(vector, open('output\/vector', 'wb')) \n#pickle.dump(tfidf, open('output\/tfidf', 'wb')) ","156aba08":"# load objects on filesystem\n#X = pickle.load(open('X', 'rb'))\n#vector = pickle.load(open('vector', 'rb'))\n#tfidf = pickle.load(open('tfidf', 'rb'))","766989ae":"# Request function : search the top_n articles from a request ( request = string)\ndef search(tfidf_matrix,model,request, top_n = 5):\n    request_transform = model.transform([request])\n    similarity = np.dot(request_transform,np.transpose(tfidf_matrix))\n    x = np.array(similarity.toarray()[0])\n    indices=np.argsort(x)[-5:][::-1]\n    return indices\n\n# Find similar : get the top_n articles similar to an article \ndef find_similar(tfidf_matrix, index, top_n = 5):\n    cosine_similarities = linear_kernel(tfidf_matrix[index:index+1], tfidf_matrix).flatten()\n    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n    return [index for index in related_docs_indices][0:top_n]    \n\n# Print the result\ndef print_result(request_content,indices,X):\n    print('\\nsearch : ' + request_content)\n    print('\\nBest Results :')\n    for i in indices:\n        print('id = {0:5d} - title = {1}'.format(i,X['title'].loc[i]))","ba5281e4":"request = 'peillon macron fillon marche'\n#request = text_content[0]\n\nresult = search(tfidf,vector, request, top_n = 5)\nprint_result(request,result,X)","36b88bd2":"index = 13084\nresult = find_similar(tfidf, index, top_n = 5)\nprint_result('13084 - title = Accusations of Anti-Semitism Taint French Presidential Race',result,X)","e3cc0a60":"# Clustering  Kmeans\nk = 20\nkmeans = MiniBatchKMeans(n_clusters = k)\nkmeans.fit(tfidf)\ncenters = kmeans.cluster_centers_.argsort()[:,::-1]\nterms = vector.get_feature_names()\n","ef31a9b8":"# centers of the clusters\nfor i in range(0,k):\n    word_list=[]\n    print(\"cluster%d:\"% i)\n    for j in centers[i,:10]:\n        word_list.append(terms[j])\n    print(word_list) ","1208e023":"# predict\nrequest_transform = vector.transform([\"Accurate predictions of Earth\u2019s warming require computers that are too expensive for one country or institution\"])\nkmeans.predict(request_transform)","be072db6":"## Search the top_n articles from a request","be7ec451":"## Search functions","162559d0":"## Find similar articles to an article ","f2d4900c":"#### Exploration main purposes : \n\n> * in a corpus of texts, search texts based on key words\n> * pick one text among the best result\n> * give the most similar texts to this one  \n\n## The data : articles from US Media\n\nArticles are from the New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News, Talking Points Memo, Buzzfeed News, National Review, New York Post, the Guardian, NPR, Reuters, Vox, and the Washington Post. \n\nThe data falls between the years of 2016 and July 2017.\n\n![nytimes.jpg](attachment:nytimes.jpg)","4b060625":"# What are the texts similar to a given text in a corpus of texts","09e92911":"## Clustering the corpus of texts with kmeans","f869cac0":"## Assessing word relevancy via term frequency-inverse document frequency\n\nThe tf-idf is the product of the term frequency and the inverse document frequency:\n\n$$\\text{tf-idf}(t,d)=\\text{tf (t,d)}\\times \\text{idf}(t,d)$$\n\n* tf(t, d) is the term frequency of the word t in the document d\n* The inverse document frequency *idf(t, d)* can be calculated as:\n\n$$\\text{idf}(t,d) = \\text{log}\\frac{n_d}{1+\\text{df}(d, t)},$$\n\n* $n_d$ is the total number of documents\n* *df(d, t)* is the number of documents *d* that contain the term *t*. \n\n(adding 1 to the denominator prevents divide-by-zero errors)\n\nScikit-learn implements the `TfidfVectorizer`, that takes 1d array of texts as input and transforms them into tf-idfs:","003bfe2d":"## Save \/ Load the model (if needed)","76e2df2c":"## Bag of words representing the center of each cluster"}}