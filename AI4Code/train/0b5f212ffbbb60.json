{"cell_type":{"8c9fb79d":"code","d40d4006":"code","2c3059d7":"code","fc592903":"code","9f9092a5":"markdown"},"source":{"8c9fb79d":"import numpy as np\n\n# elf pulling at random \nclass basic_elf():\n    def __init__(self):\n        self.mu   = None\n        self.win  = None\n        self.loss = None\n        self.id   = None\n        self.tot_reward = 0 \n    def set_pars(self,obs,conf):\n        self.n_bandits = conf.banditCount \n        self.win  = np.ones(self.n_bandits)\n        self.loss = np.ones(self.n_bandits)\n        self.id   = obs.agentIndex\n    def pull(self,obs,config):\n        '''random elf'''\n        return int(np.random.choice(config.banditCount))\n\n# elf employing epsilon greedy algorithm\nclass epsilon_elf(basic_elf):\n    def __init__(self,epsilon):\n        super(epsilon_elf,self).__init__()\n        self.epsilon = epsilon\n    def pull(self,obs,conf):\n        '''epsilon elf'''\n        if obs.step == 0:\n            self.set_pars(obs,conf)\n        else:\n            r = obs.reward - self.tot_reward\n            self.tot_reward = obs.reward\n            \n            self.win[obs['lastActions'][self.id]]  += r\n            self.loss[obs['lastActions'][self.id]] += 1-r\n            \n        mu = self.win\/(self.win+self.loss);\n        w=(self.epsilon\/self.n_bandits)*np.ones(self.n_bandits)\n        w[np.argmax(mu)] = 1-self.epsilon+self.epsilon\/self.n_bandits\n            \n        return int(np.random.choice(self.n_bandits,1,p=list(w)))    \n    \n\n    \n# elf employing Boltzmann algorithm\nclass boltzmann_elf(basic_elf):\n    def __init__(self,invT):\n        super(boltzmann_elf,self).__init__()\n        self.invT = invT\n    def pull(self,obs,conf):\n        '''boltzmann elf'''\n        if obs.step == 0:\n            self.set_pars(obs,conf)\n        else:\n            r = obs.reward - self.tot_reward\n            self.tot_reward = obs.reward\n            \n            self.win[obs['lastActions'][self.id]]  += r\n            self.loss[obs['lastActions'][self.id]] += 1-r\n            \n        mu = self.win\/(self.win+self.loss);w=np.exp(self.invT*mu)\n            \n        return int(np.random.choice(self.n_bandits,1,p=list(w\/w.sum())))\n    \n# elf employing pursuit algorithm    \nclass pursuit_elf(basic_elf):\n    def __init__(self,beta):\n        super(pursuit_elf,self).__init__()\n        self.beta = beta\n    def pull(self,obs,conf):\n        '''pursuit elf'''\n        if obs.step == 0:\n            self.set_pars(obs,conf)\n            self.pi = np.ones(self.n_bandits)\/self.n_bandits\n        else:\n            r = obs.reward - self.tot_reward\n            self.tot_reward = obs.reward\n            \n            self.win[obs['lastActions'][self.id]]  += r\n            self.loss[obs['lastActions'][self.id]] += 1-r\n            \n            mu = self.win\/(self.win+self.loss);mu_max = int(np.argmax(mu));\n            self.pi=self.pi- self.beta*self.pi; self.pi[mu_max]+=self.beta\n        \n        \n        return int(np.random.choice(np.arange(self.n_bandits), 1, p=list(self.pi) ))\n\n\n    \n# elf employing reinforcement comparison algorithm\nclass reinforcement_elf(basic_elf):\n    def __init__(self,alpha,beta):\n        super(reinforcement_elf,self).__init__()\n        self.alpha = alpha\n        self.beta  = beta\n        self.rbar  = 0.5\n    def pull(self,obs,conf):\n        '''reinforcement elf'''\n        if obs.step == 0:\n            self.set_pars(obs,conf)\n            self.pi = (1\/self.n_bandits)*np.ones(self.n_bandits)\n        else:\n            r = obs.reward - self.tot_reward\n            self.tot_reward = obs.reward\n            \n            self.pi[obs['lastActions'][self.id]] +=self.beta*(r-self.rbar)      \n            self.rbar += self.alpha*(r - self.rbar)\n            \n        w=np.exp(self.pi)\n            \n        return int(np.random.choice(self.n_bandits,1,p=list(w\/w.sum())))                   \n\n# elf employing UCB algorithm\nclass UCB_elf(basic_elf):\n    def __init__(self):\n        super(UCB_elf,self).__init__()\n    def pull(self,obs,conf):\n        '''boltzmann elf'''\n        if obs.step == 0:\n            self.set_pars(obs,conf)\n        else:\n            r = obs.reward - self.tot_reward\n            self.tot_reward = obs.reward\n            \n            self.win[obs['lastActions'][self.id]]  += r\n            self.loss[obs['lastActions'][self.id]] += 1-r\n            \n        pulls = self.win+self.loss           \n        mu = self.win\/(self.win+self.loss);\n        \n        return int(np.argmax( mu + np.sqrt(2*np.log(1+obs.step)\/pulls) ))\n                   \n                   ","d40d4006":"! pip install kaggle-environments --upgrade -q\nfrom kaggle_environments import evaluate, make, utils\n\nenv = make(\"mab\", debug=True)","2c3059d7":"import matplotlib.pyplot as plt\n\ndef baseline(basic,name,agents,names,reps):\n    \n    if len(agents)!=len(names):\n        print('Number of agents do not correspond to number of names.')\n    \n    means = np.zeros([len(names),2])\n    mins = np.zeros([len(names),2])\n    maxes = np.zeros([len(names),2])\n    \n    for i, agenti in enumerate(agents):\n        results = np.zeros([reps,2])\n        for j in range(reps):\n            \n            env.reset()\n            #print('run agent')\n            env.run([basic, agenti])\n            #print('save agent')\n            json = env.toJSON()\n            rewards = json['rewards']\n            results[j,0] = rewards[0]\n            results[j,1] = rewards[1] \n        means[i] = np.mean(results,axis=0)\n        mins[i] = np.min(results,axis=0)\n        maxes[i] = np.max(results,axis = 0)\n    \n    plus = maxes -means;minus = means - mins\n\n    bars0 = np.vstack((minus.T[0],plus.T[0]))\n    bars1 = np.vstack((minus.T[1],plus.T[1]))\n \n    plt.errorbar(np.arange(len(names)),means.T[0],bars0,marker = 'o',ls = '',label = 'Random')\n    plt.errorbar(np.arange(len(names)),means.T[1],bars1,marker = 'o',ls = '',label = 'Algorithm')\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n    plt.xticks(np.arange(len(names)),names,rotation = 'vertical')\n    plt.title(f'Random selection vs each algorithm over {reps} runs')\n    plt.ylabel('Min\/Mean\/Max Reward')\n    \n    plt.show()\n\n        \n    return mins,maxes, means","fc592903":"random = basic_elf()\nepsilon001 = epsilon_elf(0.01)\nepsilon01 = epsilon_elf(0.1)\nepsilon02 = epsilon_elf(0.2)\nboltzmann01 = boltzmann_elf(.1)\nboltzmann1 = boltzmann_elf(1)\nboltzmann2 = boltzmann_elf(2)\nboltzmann10 = boltzmann_elf(10) \npursuit001 = pursuit_elf(0.01)\npursuit01 = pursuit_elf(0.1)\npursuit02 = pursuit_elf(0.2)\nreinforcement001001 = reinforcement_elf(0.01,0.01)\nreinforcement01001 = reinforcement_elf(0.1,0.01)\nreinforcement00101 = reinforcement_elf(0.01,0.1)\nreinforcement0101 = reinforcement_elf(0.1,0.1)\nreinforcement0201 = reinforcement_elf(0.2,0.1)\nreinforcement0102 = reinforcement_elf(0.1,0.2)\nreinforcement0202 = reinforcement_elf(0.2,0.2)\nUCB = UCB_elf()\nagents = [\n          epsilon001.pull,epsilon01.pull, epsilon02.pull, \n          boltzmann01.pull,boltzmann1.pull,boltzmann2.pull,boltzmann10.pull,  \n          pursuit001.pull,pursuit01.pull,pursuit02.pull,\n          reinforcement001001.pull,\n          UCB.pull]\nnames = [\n         'greedy, (e = 0.01)','greedy, (e = 0.1)','greedy, (e = 0.2)',\n         'boltzmann, (invT = 0.1)','boltzmann, (invT = 1)','boltzmann, (invT = 2)','boltzmann, (invT = 10)', \n         'pursuit, (beta = 0.01)','pursuit, (beta = 0.1)','pursuit, (beta = 0.2)',\n         'reinforcement, (a=b=0.001)',\n         'UCB']\n\n\nmins,maxes,means = baseline(random.pull,'random',agents,names,20)","9f9092a5":"**A script evaluating several standard multi-arm bandit algorithms.**\n\nI'm completely new to the multi-armed bandit problem, so I coded up a bunch of the standard algorithms and compared each to a random algorithm. "}}