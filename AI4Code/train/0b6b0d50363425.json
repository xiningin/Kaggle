{"cell_type":{"0b8c4f47":"code","d55fba14":"code","5c550de7":"code","9f4add47":"code","2d49afe8":"code","7a2c8d17":"code","548335b1":"code","83af055e":"code","eacde6ee":"code","3be9e61c":"code","6ec50a24":"code","21c17b2e":"code","7c128bd1":"code","1720840f":"code","4ea0f1ef":"code","d4b11baa":"code","e1b48845":"code","8362458e":"code","ea0ac157":"code","a301a5e6":"code","c21b31ac":"markdown","5ebe6e08":"markdown","44e4ab7f":"markdown","bf9c1509":"markdown","5f1ae742":"markdown","89f74974":"markdown","052423ff":"markdown","e511b016":"markdown","51c1381b":"markdown","3a72611f":"markdown"},"source":{"0b8c4f47":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d55fba14":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","5c550de7":"import seaborn as sns\nimport os\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport re\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier","9f4add47":"titanic = train.copy()\ntitanic.head()","2d49afe8":"plt.figure(figsize=(4, 5)) #Counts for survived and non-survived passengers\nsns.countplot(x='Survived', data=titanic)\nplt.show()","7a2c8d17":"cat_cols = ['Pclass', 'Sex', 'Embarked']#Counts for survived and non-survived passengers distinguished by Pclass, sex, and embark location\n\nfig, ax = plt.subplots(1, 3, figsize=(20, 4))\nfor ind, val in enumerate(cat_cols):\n    sns.countplot(x=val, hue='Survived', data=titanic, ax=ax[ind])\n    ax[ind].legend(['Died', 'Survived'])","548335b1":"titanic[titanic['Survived'] == 1].Age.plot.kde() and titanic[titanic['Survived'] == 0].Age.plot.kde()","83af055e":"titanic[titanic['Survived'] == 1].Fare.plot.kde() and titanic[titanic['Survived'] == 0].Fare.plot.kde()","eacde6ee":"cat_cols = ['Parch', 'SibSp']#Counts for survived and non-survived passengers distinguished by the number of parents and siblings on board\n\nfig, ax = plt.subplots(1, 2, figsize=(20, 4))\nfor ind, val in enumerate(cat_cols):\n    sns.countplot(x=val, hue='Survived', data=titanic, ax=ax[ind])\n    ax[ind].legend(['Died', 'Survived'])","3be9e61c":"titanic.describe() #Overview of the statistics","6ec50a24":"test.describe()","21c17b2e":"class StdScalerByGroup(BaseEstimator, TransformerMixin):\n    \n    def __init__(self):\n        pass\n\n    def fit(self, X, y=None):\n        df = pd.DataFrame(X)\n        \n        self.grps_ = [df.groupby(df.columns[0]).mean().to_dict('index'), df.groupby(df.columns[0]).std().to_dict('index')]\n\n        return self\n\n    def transform(self, X, y=None):\n        try:\n            getattr(self, \"grps_\")\n        except AttributeError:\n            raise RuntimeError(\"You must fit the transformer before tranforming the data!\")\n        \n\n        df = pd.DataFrame(X)\n\n        for i in df.columns[1:]:\n        \tmean = df[df.columns[0]].map(self.grps_[0]).map(lambda x: x[i])\n        \tstd = df[df.columns[0]].map(self.grps_[1]).map(lambda x: x[i])\n        \tdf[i] = (df[i] - mean) \/ std\n        \n        return df.drop(df.columns[0], axis = 1)","7c128bd1":"def final(dataset):\n    #Drop unnecessary columns\n    titanic = dataset.copy()\n    titanic = titanic.drop(['PassengerId', 'Ticket', 'Cabin'], axis = 1)\n    \n    #Impute missingness. Median for numerical and mode for categorical\n    titanic['Age'] = titanic[['Age']].fillna(titanic.Age.median())\n    \n    freq = titanic.Embarked.mode().iloc[0]\n    titanic['Embarked'] = titanic[['Embarked']].fillna(freq)\n    \n    titanic['Fare'] = titanic[['Fare']].fillna(titanic.Fare.median())\n    \n    #Feature engineering\n    \n    #name\n    #Regex to filter the name of the passengers to only their title. We believe that the name of a passenger has no meaning but the title of he\/she does.\n    l = []\n    for i in titanic.Name.values:\n        l.append(re.findall('[A-Z]{1}[a-z]+\\.', i)[0])\n    titanic.Name = l\n    \n    #We also see there are many rare titles in the names. We want to combine those uncommon ones so then it would be easier to do the one-hot encoding.\n    titanic['Name'] = titanic['Name'].replace(['Lady.', 'Countess.','Capt.', 'Col.', 'Don.', 'Dr.', 'Major.', 'Rev.', 'Sir.', 'Jonkheer.', 'Dona.'], 'Rare')\n\n    titanic['Name'] = titanic['Name'].replace('Mlle.', 'Miss.')\n    titanic['Name'] = titanic['Name'].replace('Ms.', 'Miss.')\n    titanic['Name'] = titanic['Name'].replace('Mme.', 'Mrs.')\n    \n    title_mapping = {\"Mr.\": 1, \"Miss.\": 2, \"Mrs.\": 3, \"Master.\": 4, \"Rare.\": 5}\n    titanic['Name'] = titanic['Name'].map(title_mapping)\n    titanic['Name'] = titanic['Name'].fillna(0)\n    \n\n    \n    #family\n    #We want to combine the siblings and parents together in a new column, called \"Family\".\n    titanic['Family'] = titanic['SibSp'] + titanic['Parch']\n    \n    \n    #sex\n    #Converting categorical to numerical\n    def sex(n):\n        if n == 'male':\n            return 1\n        else:\n            return 0\n    titanic['Sex'] = titanic.Sex.apply(sex)\n    \n    #embarked\n    places = titanic.Embarked.unique()\n    pl = titanic['Embarked'].apply(lambda x: pd.Series(x == places, index=places, dtype=float))\n    titanic = pd.concat([titanic, pl], axis = 1)\n    \n    #Drop unnecessary columns again\n    titanic = titanic.drop([ 'Embarked'], axis = 1)\n\n    #age\n    #Z-values for Age column\n    g = titanic[['Pclass', 'Age']]\n    std = StdScalerByGroup().fit(g)\n    titanic['Age'] = std.transform(g)\n    \n    return titanic","1720840f":"copy = final(titanic)\n\n\nX = copy.drop('Survived', axis = 1)\ny = copy.Survived\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 1)\nml = Pipeline([('r', RandomForestClassifier(n_estimators=500,max_depth=6,min_samples_leaf=2,max_features='sqrt'))])\nml.fit(X_train, y_train)","4ea0f1ef":"ml.predict(X_test)","d4b11baa":"ml.score(X_test, y_test)","e1b48845":"preds = ml.predict(final(test))","8362458e":"ml.score(X,y)","ea0ac157":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": preds\n    })","a301a5e6":"submission.to_csv('submission.csv', index=False)","c21b31ac":"Before we start the actual machine learning process, we import all the packages that we will need in the later process. We also load the training and testing dataset given by Kaggle.","5ebe6e08":"The describe() function does not check the missingness for the categorical columns. So columns like PassengerId, Ticket, and Embarked will be checked using different methods. Before we check the missingness for those columns, we decided to drop the PassengerId and Ticket columns because we believe those columns does not have any meaningful values for our machine learning. So we only check the missingness of Embarked column.","44e4ab7f":"Machine Learning process:\nRandom forest classifier.","bf9c1509":"Distribution of people's age who survived vs didn't survived","5f1ae742":"Hi, I am Wilson Xie. This is my first Kaggle project with my teammate Eric Wang.","89f74974":"There are missing values for the Age column in the training dataset, and there are missing values for the Age and Fare columns in the testing dataset. So we are going to impute missing values for those columns later on.","052423ff":"We created a class called StdScalerByGroup because we will use this class when we want to standardize Age column later on. Basically, this function groups the Pclass status by its mean and standard deviation, then the Z-value will be calculated based on the Age.","e511b016":"Distribution of people's fare who survived vs didn't survived","51c1381b":"Distributions","3a72611f":"From the visualizations above, we could conclude that there are more passengers died in the Titanic training dataset. The upper Pclass passenger would also have a higher chance of surviving than the other lower Pclass. We asssume that the upper class passengers have the priority to leave the Titanic first before drowning. In addition, male passengers have fewer survivors than females in the training dataset. We assume that the male passengers are protecting and rescuing the female passengers, thus females have more survivors."}}