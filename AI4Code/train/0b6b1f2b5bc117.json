{"cell_type":{"7102ad38":"code","cd32518f":"code","36c77b53":"code","d4a0b2ea":"code","b6f03073":"code","d7327889":"code","a19a8915":"code","7d762892":"code","419f0148":"code","60d3e33b":"code","192e47ab":"code","a16cdc6b":"code","3ab7babe":"code","64ae83a8":"code","a03bbff9":"code","172c0729":"code","4eb13802":"code","5d073245":"code","37995d72":"code","47120d0b":"code","b167e246":"code","8bbfbcba":"code","c545f00e":"code","5f26de54":"code","86abdf91":"code","982dea57":"code","fc3aaec2":"code","b2b51498":"code","bbd1ae07":"code","cccff8cd":"code","6a056841":"code","b5bb33d3":"code","a4779941":"code","9cf8f964":"code","45e7a8bc":"code","14190b1c":"code","edc0ea5b":"code","39e57f29":"code","cb15dbe9":"code","619fcbb8":"code","c4b913fa":"code","b036f1de":"code","3db38703":"code","864b94b9":"code","a2786dbf":"code","a90e17ff":"code","61a9cfe0":"code","1ffe6ebb":"code","a147db81":"code","a854ad4b":"code","7f536ab6":"code","6e844405":"code","27de3b47":"code","75f44f38":"code","3b22d05b":"code","ae1c333c":"code","dcad4eb3":"code","bbbb3cc4":"code","75aa294b":"code","856ac0f6":"code","0176d27c":"code","25682c6e":"code","eadc6eab":"code","1805c736":"markdown","c1906803":"markdown","900bcf98":"markdown","1fda70e8":"markdown","d6b795c0":"markdown","e2edb8a8":"markdown","4d6621cf":"markdown","a01ed4ee":"markdown","89a32f54":"markdown","10f86348":"markdown","9981a2c4":"markdown","eec57ed2":"markdown","a2cc34e5":"markdown","14a7f899":"markdown","0d77300c":"markdown","748230c0":"markdown","53f9d09f":"markdown","acaf0b1b":"markdown","d9d08f7a":"markdown","d77b19e4":"markdown","c56c2c2b":"markdown","7b28840a":"markdown","e1e1fa62":"markdown","b204da5d":"markdown","77595042":"markdown","c356df7c":"markdown","c1d6c9e8":"markdown","cfd09d86":"markdown"},"source":{"7102ad38":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n#Visualization\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap \n#Libraries for ML\nfrom sklearn.preprocessing import StandardScaler #Standardizasyon i\u00e7in\nfrom sklearn.model_selection import train_test_split, GridSearchCV #GridSearchCV: KNN ile ilgili en iyi parametreleri belirlemek\nfrom sklearn.metrics import accuracy_score ,confusion_matrix #Sonu\u00e7 de\u011ferlendirme\nfrom sklearn.neighbors import KNeighborsClassifier,LocalOutlierFactor #Trainin algoritmas\u0131 ve NCA ve Outlier de\u011ferler i\u00e7in\nfrom sklearn.decomposition import PCA #PCA i\u00e7in\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport itertools\nplt.style.use('fivethirtyeight')\nimport seaborn as sns\n# import warnings\nimport warnings\n# filter warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","cd32518f":"#read data\ndata = pd.read_csv(\"..\/input\/diabetes.csv\")\ndata.head()","36c77b53":"#Split Data as M&B\np = data[data.Outcome == 1]\nn = data[data.Outcome == 0]","d4a0b2ea":"sns.countplot(x='Outcome',data=data)\nplt.title(\"Count 0 & 1\")\nplt.show()","b6f03073":"print('Data Shape',data.shape)\nprint(data.info()) \ndescribe = data.describe()\ndescribe.T","d7327889":"corr_data = data.corr() \nsns.clustermap(corr_data,annot= True,fmt = '.2f')\nplt.title('Correlation Between Features')\nplt.show();","a19a8915":"#Box p. \u00f6ncesi bir melted i\u015flemi gerekitor.\ndata_melted = pd.melt(data,id_vars='Outcome',\n                      var_name='Features',\n                      value_name='Value')\n\nplt.figure()\nsns.boxplot(x='Features',y='Value',hue='Outcome',data=data_melted) #Featureslar target'a g\u00f6re ayr\u0131ld\u0131.\nplt.xticks(rotation=75) #Feature isimleri 90 derece dik g\u00f6r\u00fclecek.\nplt.show()\n","7d762892":"#General Analysis\n\ndata1 = data[data[\"Outcome\"]==1]\ncolumns = data.columns[:8]\nplt.subplots(figsize=(18,18))\nlength =len(columns)\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot((length\/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    plt.ylabel(\"Count\")\n    data1[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","419f0148":"#General Analysis\n\ndata1 = data[data[\"Outcome\"]==0]\ncolumns = data.columns[:8]\nplt.subplots(figsize=(18,18))\nlength =len(columns)\nfor i,j in itertools.zip_longest(columns,range(length)):\n    plt.subplot((length\/2),3,j+1)\n    plt.subplots_adjust(wspace=0.2,hspace=0.5)\n    plt.ylabel(\"Count\")\n    data1[i].hist(bins=20,edgecolor='black')\n    plt.title(i)\nplt.show()","60d3e33b":"\n\nplt.scatter(p.Pregnancies,p.Glucose,color = \"brown\",label=\"Diabet Positive\",alpha=0.4)\nplt.scatter(n.Pregnancies,n.Glucose,color = \"Orange\",label=\"Diabet Negative\",alpha=0.2)\nplt.xlabel(\"Pregnancies\")\nplt.ylabel(\"Glucose\")\nplt.legend()\nplt.show()\n\n#We appear that it is clear segregation.","192e47ab":"#Visualization, Scatter Plot\n\nplt.scatter(p.Age,p.Pregnancies,color = \"lime\",label=\"Diabet Positive\",alpha=0.4)\nplt.scatter(n.Age,n.Pregnancies,color = \"black\",label=\"Diabet Negative\",alpha=0.2)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Pregnancies\")\nplt.legend()\nplt.show()\n\n#We appear that it is clear segregation.","a16cdc6b":"#Visualization, Scatter Plot\n\nplt.scatter(p.Glucose,p.Insulin,color = \"lime\",label=\"Diabet Positive\",alpha=0.4)\nplt.scatter(n.Glucose,n.Insulin,color = \"black\",label=\"Diabet Negative\",alpha=0.1)\nplt.xlabel(\"Glucose\")\nplt.ylabel(\"Insulin\")\nplt.legend()\nplt.show()\n\n#We appear that it is clear segregation.","3ab7babe":"x = data.drop(['Outcome'],axis=1) \ny = data.Outcome\ncolumns = x.columns.tolist() # Featurelar\u0131n isimlerini bir listede toplad\u0131k.\n\nclf = LocalOutlierFactor() #KNN de\u011feri gerekiyor. Tan\u0131ml\u0131 de\u011feri 20 ve bizde 20 kullanaca\u011f\u0131z. O nedenle bir\u015fey yazmayaca\u011f\u0131z.\ny_pred = clf.fit_predict(x) #LOF uygulay\u0131o negetif outlier f. al\u0131yoruz.\n\nx_score = clf.negative_outlier_factor_\noutlier_score = pd.DataFrame()\noutlier_score['score'] = x_score\n\nthreshold_outliers = -1.5\nfiltre = outlier_score['score'] < threshold_outliers\noutlier_index = outlier_score[filtre].index.tolist() #outlier de\u011ferlerine threshold uygulanm\u0131\u015f olanlar\u0131 bir listeye att\u0131k\n\nplt.figure()\nplt.scatter(x.iloc[outlier_index,0], x.iloc[outlier_index,1],color = 'blue',s=50,label='outliers')\nplt.scatter(x.iloc[:,0]\n            ,x.iloc[:,1],color='k',s=3,label='data_point') #s : boyut\n\nradius = (x_score.max() - x_score ) \/ (x_score.max() - x_score.min() ) #De\u011ferleri normalize ederek bias\u0131 \u00f6nledik\noutlier_score['radius '] = radius\n\nplt.scatter(x.iloc[:,0], x.iloc[:,1], s=1000*radius, edgecolors='r',facecolor='none',label='Outlier skores')\nplt.legend()\nplt.show();","64ae83a8":"x = x.drop(outlier_index) #outliers remove\ny = y.drop(outlier_index).values #outliers remove","a03bbff9":"test_size = 0.2\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=test_size,random_state=42)","172c0729":"scaler = StandardScaler()\nx_train = scaler.fit_transform(x_train) #scaler isimli bir de\u011fi\u015fkene Standartscaler metodunu atay\u0131p sonra bu de\u011fi\u015fken arac\u0131l\u0131\u011f\u0131yla xtraindeki verileri standardize ettik\n\nx_test = scaler.transform(x_test)\n\nx_train_df = pd.DataFrame(x_train,columns=columns)\nx_train_df_describe = x_train_df.describe()\nx_train_df['target'] = y_train","4eb13802":"data_melted = pd.melt(x_train_df,id_vars='target',\n                      var_name='Features',\n                      value_name='Value')\n\nplt.figure()\nsns.boxplot(x='Features',y='Value',hue='target',data=data_melted) #Featureslar target'a g\u00f6re ayr\u0131ld\u0131.\nplt.xticks(rotation=75) #Feature isimleri 90 derece dik g\u00f6r\u00fclecek.\nplt.show()","5d073245":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()","37995d72":"#K-fold CV\nfrom sklearn.model_selection import cross_val_score\naccuraccies = cross_val_score(estimator = LR, X= x_train, y=y_train, cv=10)\nprint(\"Average Accuracies: \",np.mean(accuraccies))\nprint(\"Standart Deviation Accuracies: \",np.std(accuraccies))","47120d0b":"LR.fit(x_train,y_train)\nprint(\"Test Accuracy {}\".format(LR.score(x_test,y_test))) \n\nLRscore = LR.score(x_test,y_test)","b167e246":"#Confusion Matrix\n\nyprediciton1= LR.predict(x_test)\nytrue = y_test\n\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(ytrue,yprediciton1)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()","8bbfbcba":"knn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(x_train, y_train) #Calculation (In the supervise learning this section = training)\ny_predict = knn.predict(x_test) #Test Section\ncm = confusion_matrix(y_test, y_predict) #Plooting\nacc = accuracy_score(y_test, y_predict) #Accuracy Score\nscore = knn.score(x_test, y_test) #acc ile buras\u0131n\u0131n sonucu ayn\u0131 \u00e7\u0131kacak. Do\u011frulama ama\u00e7l\u0131 yap\u0131l\u0131yor.\n\nprint(\"Score:\",score)\nprint(\"CM:\",cm)\nprint(\"Basic KNN Acc:\",acc)","c545f00e":"def KNN_best_parameters(x_train,x_test,y_train,y_test):\n    \n    k_range = list(range(1,51)) #En uygun k de\u011feri buluma\n    weight_options = ['uniform','distance'] #En uygun weighti buluma\n    #manhattan_distance = 1\n    #euclidean_distance = 2\n    distance_options = [1,2] #En uygun distance type buluma\n    print()\n    param_grid = dict(n_neighbors=k_range,weights=weight_options,p=distance_options) #Aranacak parametreleri bir s\u00f6zl\u00fckte toplad\u0131k.\n\n    knn =KNeighborsClassifier() #Parametrelerin denenece\u011fi knn olu\u015fturuldu.\n    grid = GridSearchCV(knn,param_grid,cv=10,scoring='accuracy') #Parametrelerin aranmas\u0131 i\u00e7in method\n    grid.fit(x_train, y_train) #fitting ile best parm. elde edildi\n    \n    print('Best training score: {} with parametres: {}'.format(grid.best_score_,grid.best_params_))\n    print()\n    \n    knn = KNeighborsClassifier(**grid.best_params_) #Test setinde deneme i\u015flemi i\u00e7in\n    knn.fit(x_train, y_train)\n    \n    y_predict_test = knn.predict(x_test)\n    y_predict_train = knn.predict(x_train)\n\n    cm_test = confusion_matrix(y_test,y_predict_test)\n    cm_train = confusion_matrix(y_train,y_predict_train)\n\n    acc_test = accuracy_score(y_test,y_predict_test)  \n    acc_train = accuracy_score(y_train,y_predict_train)\n\n    print('Test Score: {}, Train Score: {}'.format(acc_test,acc_train))\n    print()\n    print('CM Test:',cm_test)\n    print('CM Train:',cm_train)\n    \n    return grid","5f26de54":"grid = KNN_best_parameters(x_train,x_test,y_train,y_test)","86abdf91":"KNNscore = grid.best_score_","982dea57":"#%% PCA\n\nscale = StandardScaler()\nx_scaled = scaler.fit_transform(x) #x verisi b\u00f6l\u00fcnmeden tam bir \u015fekilde PCA i\u00e7in scale edildi. \n\npca = PCA(n_components=2) #2 componentli bir PCA olu\u015fturduk.\npca.fit(x_scaled)\nx_reduce_pca = pca.transform(x_scaled) #2feature'a yani boyuta d\u00fc\u015f\u00fcr\u00fclm\u00fc\u015f x \npca_data =pd.DataFrame(x_reduce_pca,columns=['p1','p2']) #reduce datadan incelemek i\u00e7in bir dataframe olu\u015fturuldu\npca_data['target'] = y #buna target eklendi. G\u00f6rselle\u015ftirmek i\u00e7in gerekli.\n\nsns.scatterplot(x='p1',y='p2',hue='target',data=pca_data) # targeta g\u00f6re renklendirilmi\u015f grafik\nplt.title('PCA: P1 Vs P2')","fc3aaec2":"x_train_pca, x_test_pca, y_train_pca, y_test_pca = train_test_split(x_reduce_pca,y,test_size=test_size,random_state=42)\n\ngrid_pca = KNN_best_parameters(x_train_pca, x_test_pca, y_train_pca, y_test_pca)\n#en iyi parametreleri elde etti\u011fimiz metodu PCA i\u00e7in \u00e7al\u0131\u015ft\u0131r\u0131yorum.","b2b51498":"cmap_light = ListedColormap(['orange',  'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\nh = .05 # step size in the mesh\nX = x_reduce_pca\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = grid_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"%i-Class classification (k = %i, weights = '%s')\"\n          % (len(np.unique(y)),grid_pca.best_estimator_.n_neighbors, grid_pca.best_estimator_.weights))","bbd1ae07":"#Confusion Matrix\n\nyprediciton2= knn.predict(x_test)\nytrue = y_test\n\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(ytrue,yprediciton2)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()","cccff8cd":"#SVM with Sklearn\n\nfrom sklearn.svm import SVC\n\nSVM = SVC(random_state=42)","6a056841":"#K-fold CV\nfrom sklearn.model_selection import cross_val_score\naccuraccies = cross_val_score(estimator = SVM, X= x_train, y=y_train, cv=5)\nprint(\"Average Accuracies: \",np.mean(accuraccies))\nprint(\"Standart Deviation Accuracies: \",np.std(accuraccies))","b5bb33d3":"SVM.fit(x_train,y_train)  #learning \n#SVM Test \nprint (\"SVM Accuracy:\", SVM.score(x_test,y_test))\n\nSVMscore = SVM.score(x_test,y_test)","a4779941":"#Confusion Matrix\n\nyprediciton3= SVM.predict(x_test)\nytrue = y_test\n\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(ytrue,yprediciton3)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()","9cf8f964":"#Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nNB = GaussianNB()","45e7a8bc":"#K-fold CV\nfrom sklearn.model_selection import cross_val_score\naccuraccies = cross_val_score(estimator = NB, X= x_train, y=y_train, cv=35)\nprint(\"Average Accuracies: \",np.mean(accuraccies))\nprint(\"Standart Deviation Accuracies: \",np.std(accuraccies))","14190b1c":"NB.fit(x_train,y_train) #learning\n#prediction\nprint(\"Accuracy of NB Score: \", NB.score(x_test,y_test))\n\nNBscore= NB.score(x_test,y_test)","edc0ea5b":"#Confusion Matrix\n\nyprediciton4= NB.predict(x_test)\nytrue = y_test\n\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(ytrue,yprediciton4)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()","39e57f29":"#Decision Tree Algorithm\n\nfrom sklearn.tree import DecisionTreeClassifier\nDTC = DecisionTreeClassifier()","cb15dbe9":"#K-fold CV\nfrom sklearn.model_selection import cross_val_score\naccuraccies = cross_val_score(estimator = DTC, X= x_train, y=y_train, cv=55)\nprint(\"Average Accuracies: \",np.mean(accuraccies))\nprint(\"Standart Deviation Accuracies: \",np.std(accuraccies))","619fcbb8":"DTC.fit(x_train,y_train) #learning\n#prediciton\nprint(\"Decision Tree Score: \",DTC.score(x_test,y_test))\nDTCscore = DTC.score(x_test,y_test)","c4b913fa":"#Confusion Matrix\n\nyprediciton5= DTC.predict(x_test)\nytrue = y_test\n\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(ytrue,yprediciton5)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()","b036f1de":"#Decision Tree Algorithm\n\nfrom sklearn.tree import DecisionTreeClassifier\nDTC = DecisionTreeClassifier()\nDTC.fit(x_train,y_train) #learning\n#prediciton\nprint(\"Decision Tree Score: \",DTC.score(x_test,y_test))\n","3db38703":"#Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\nRFC= RandomForestClassifier(n_estimators = 82, random_state=42) #n_estimator = DT","864b94b9":"#K-fold CV\nfrom sklearn.model_selection import cross_val_score\naccuraccies = cross_val_score(estimator = RFC, X= x_train, y=y_train, cv=10)\nprint(\"Average Accuracies: \",np.mean(accuraccies))\nprint(\"Standart Deviation Accuracies: \",np.std(accuraccies))","a2786dbf":"RFC.fit(x_train,y_train) # learning\nprint(\"Random Forest Score: \",RFC.score(x_test,y_test))\nRFCscore=RFC.score(x_test,y_test)","a90e17ff":"#Find Optimum K value\nscores = []\nfor each in range(80,100):\n    RFfind = RandomForestClassifier(n_estimators = each)\n    RFfind.fit(x_train,y_train)\n    scores.append(RFfind.score(x_test,y_test))\n    \nplt.figure(1, figsize=(10, 5))\nplt.plot(range(80,100),scores,color=\"black\",linewidth=2)\nplt.title(\"Optimum N Estimator Value\")\nplt.xlabel(\"N Estimators\")\nplt.ylabel(\"Score(Accuracy)\")\nplt.grid(True)\nplt.show()","61a9cfe0":"#Confusion Matrix\n\nyprediciton6= RFC.predict(x_test)\nytrue = y_test\n\nfrom sklearn.metrics import confusion_matrix\nCM = confusion_matrix(ytrue,yprediciton6)\n\n#CM visualization\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(CM,annot = True, linewidths=0.5,linecolor=\"red\",fmt=\".0f\",ax=ax)\nplt.xlabel(\"Prediction(Ypred)\")\nplt.ylabel(\"Ytrue\")\nplt.show()","1ffe6ebb":"#Import Library\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential \nfrom keras.layers import Dense","a147db81":"def buildclassifier():\n    classifier = Sequential() #initialize NN\n    classifier.add(Dense(units = 24, kernel_initializer = 'uniform',activation = 'relu', input_dim =x_train.shape[1]))\n    classifier.add(Dense(units = 24, kernel_initializer = 'uniform',activation = 'relu'))\n    classifier.add(Dense(units = 24, kernel_initializer = 'uniform',activation = 'relu'))\n    classifier.add(Dense(units = 24, kernel_initializer = 'uniform',activation = 'relu'))\n    classifier.add(Dense(units = 24, kernel_initializer = 'uniform',activation = 'relu'))\n    classifier.add(Dense(units = 24, kernel_initializer = 'uniform',activation = 'relu'))\n    classifier.add(Dense(units = 1, kernel_initializer = 'uniform',activation = 'sigmoid'))\n    classifier.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])\n    return classifier","a854ad4b":"classifier = KerasClassifier(build_fn = buildclassifier, epochs = 500)\naccuracies = cross_val_score(estimator = classifier, X = x_train, y= y_train, cv = 5)\nmean = accuracies.mean()\nvariance = accuracies.std()\nprint(\"Accuracy mean: \"+ str(mean))\nprint(\"Accuracy variance: \"+ str(variance))","7f536ab6":"ANNmean = accuracies.mean()","6e844405":"from sklearn.ensemble import GradientBoostingClassifier\ngbm_model = GradientBoostingClassifier().fit(x_train, y_train)\n\ny_pred = gbm_model.predict(x_test)\ntestscore_gbm=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","27de3b47":"### Model Tuning\n\ngbm_params = {\"learning_rate\" : [0.001, 0.01],\n             \"n_estimators\": [100,500],\n             \"max_depth\": [3,10],\n             \"min_samples_split\": [2,10]}\n\ngbm = GradientBoostingClassifier()\n\ngbm_cv = GridSearchCV(gbm, gbm_params, cv = 10, n_jobs = -1, verbose = 2)\ngbm_cv.fit(x_train, y_train)\n\nprint(\"The Best Parameters: \" + str(gbm_cv.best_params_))","75f44f38":"gbm = GradientBoostingClassifier(learning_rate = 0.01, \n                                 max_depth = 3,\n                                min_samples_split = 2,\n                                n_estimators = 100)\n\ngbm_tuned =  gbm.fit(x_train,y_train)\ny_pred = gbm_tuned.predict(x_test)\ncrosscore_gbm=accuracy_score(y_test, y_pred)\naccuracy_score(y_test, y_pred)","3b22d05b":"gbmscore = gbm.score(x_test,y_test)","ae1c333c":"#F1-Score For Logistic Regression\nfrom sklearn.metrics import f1_score\nLRf1 = f1_score(ytrue, yprediciton1, average='weighted') \nLRf1","dcad4eb3":"#K-NN\nKNNf1= f1_score(ytrue, yprediciton2, average='weighted') \nKNNf1","bbbb3cc4":"#SVM\nSVMf1=f1_score(ytrue, yprediciton3, average='weighted') \nSVMf1","75aa294b":"#naive bayes\nNBf1 = f1_score(ytrue, yprediciton4, average='weighted') \nNBf1","856ac0f6":"#Decision Tree\nDTf1=f1_score(ytrue, yprediciton5, average='weighted') \nDTf1","0176d27c":"#RandomForest\nRFf1=f1_score(ytrue, yprediciton6, average='weighted') \nRFf1","25682c6e":"\nscores=[LRscore,KNNscore,SVMscore,NBscore,DTCscore,RFCscore,mean,gbmscore]\nAlgorthmsName=[\"Logistic Regression\",\"K-NN\",\"SVM\",\"Naive Bayes\",\"Decision Tree\", \"Random Forest\",\"Artificial Neural Network\",\"Gradient Boosting Machine\"]\n\n#create traces\n\ntrace1 = go.Scatter(\n    x = AlgorthmsName,\n    y= scores,\n    name='Algortms Name',\n    marker =dict(color='rgba(0,255,0,0.5)',\n               line =dict(color='rgb(0,0,0)',width=2)),\n                text=AlgorthmsName\n)\ndata = [trace1]\n\nlayout = go.Layout(barmode = \"group\",\n                  xaxis= dict(title= 'ML Algorithms',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Prediction Scores',ticklen= 5,zeroline= False))\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","eadc6eab":"scoresf1=[LRf1,KNNf1,SVMf1,NBf1,DTf1,RFf1]\n#create traces\n\ntrace1 = go.Scatter(\n    x = AlgorthmsName,\n    y= scoresf1,\n    name='Algortms Name',\n    marker =dict(color='rgba(225,126,0,0.5)',\n               line =dict(color='rgb(0,0,0)',width=2)),\n                text=AlgorthmsName\n)\ndata = [trace1]\n\nlayout = go.Layout(barmode = \"group\", \n                  xaxis= dict(title= 'ML Algorithms',ticklen= 5,zeroline= False),\n              yaxis= dict(title= 'Prediction Scores(F1)',ticklen= 5,zeroline= False))\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","1805c736":"<a id =\"6\"><\/a><br>\n## Analysis of Non-Diabetic Cases","c1906803":"<a id =\"7\"><\/a><br>\n## Visualization of Features","900bcf98":"<a id =\"11\"><\/a><br>\n# Machine Learning Models","1fda70e8":"<a id =\"22\"><\/a><br>\n# Result Visialization","d6b795c0":"<a id =\"21\"><\/a><br>\n## F1 Score Calculation","e2edb8a8":"<a id =\"13\"><\/a><br>\n## K-nearest neighbors (KNN) & PCA Analysis","4d6621cf":"<a id =\"20\"><\/a><br>\n# Evaluation of Results","a01ed4ee":"<a id =\"3\"><\/a><br>\n# Exploratory Data Analysis","89a32f54":"<a id =\"1\"><\/a><br>\n# Read and Examination Dataset","10f86348":"# Conclusion\n\n1. Thank you for investigation my kernel.\n2. I was comparison ML Classification Algorithms with the Pima Indians Diabetes Database.\n3. I found the best result with Random Forest and SVM.\n4. I expect your opinion and criticism.\n\n# If you like this kernel, Please Upvote :) Thanks\n\n<img src=\"https:\/\/media1.giphy.com\/media\/l0ExvuzJGJNZJZ47S\/giphy.gif?cid=790b76115cc05331372f4d64593e8962\" width=\"500px\">\n\n","9981a2c4":"## Introduction \n* Diabetes is a chronic (long-lasting) health condition that affects how your body turns food into energy. \n* Most of the food you eat is broken down into sugar (also called glucose) and released into your bloodstream.\n\n### Goals\n* I am classified diabetic patients by using several ML models.\n\n#### Machine Learning Models\n\n* Logistic Regression\n* KNN\n* SVM\n* Navie Bayesian\n* Decision Tree\n* Random Forest \n* Artificial Neural Networks\n\n\n### Dataset Informations\n* **Name:** [Pima Indians Diabetes Database](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database)\n* **Feautres:** 9 clinical features for prediction \n","eec57ed2":"# Scatter Plot For Comparasion of ML Algorithms Prediciton Scores (F1)","a2cc34e5":"<a id =\"15\"><\/a><br>\n# Naive Bayes (NB)","14a7f899":"<a id =\"9\"><\/a><br>\n# Train Tespt Split & Standardization Processing","0d77300c":"<a id =\"2\"><\/a><br>\n### Basic Visualization","748230c0":"<a id =\"18\"><\/a><br>\n# Artificial Neural Network (ANN)","53f9d09f":"\n## Content\n\n1. [Read and Examination Dataset](#1)\n    * [Basic Visualization](#2)\n2. [Exploratory Data Analysis](#3)\n    * [Feautre Visualization with Box plot [Before Standardization]](#4)\n    * [Analysis of Diabetic Cases](#5)\n    * [Analysis of Non-Diabetic Cases](#6)\n    * [Visualization of Features](#7)\n    * [Outlier Detection](#8)\n3. [Train Test Split & Standardization Processing](#9)\n    * [Box Plot Visualization after the Standardization](#10)\n4. [Machine Learning Models](#11)\n    * [Logistic Regression (LR)](#12)\n    * [K-nearest neighbors (KNN) & PCA Analysis](#13)\n    * [Support Vector Machine (SVM)](#14)\n    * [Naive Bayesian (NB)](#15)\n    * [Decision Tree](#16)\n    * [Random Forest (RF)](#17)\n    * [Artificial Neural Networks (ANN)](#18)\n    * [Gradient Boosting Machines (GBM)](#19)\n5. [Evaluation of Results](#20)\n    * [F1 Score Calculation](#21)\n    * [Results Visualization](#22)\n","acaf0b1b":"<a id =\"4\"><\/a><br>\n## Feautre Visualization with Box plot [Before Standardization]","d9d08f7a":"### Drop Outliers","d77b19e4":"<a id =\"12\"><\/a><br>\n## Logistic Regression (LR)","c56c2c2b":"<a id =\"10\"><\/a><br>\n## Box Plot Visualization after the Standardization","7b28840a":"<a id =\"8\"><\/a><br>\n## Outlier Detection","e1e1fa62":"## Libraries","b204da5d":"<a id =\"16\"><\/a><br>\n## Decision Tree\n<img src=\"https:\/\/emerj.com\/wp-content\/uploads\/2018\/04\/3049155-poster-p-1-machine-learning-is-just-a-big-game-of-plinko.gif\" width=\"500px\">\n","77595042":"<a id =\"5\"><\/a><br>\n## Analysis of Diabetic Cases","c356df7c":"<a id =\"14\"><\/a><br>\n## Support Vector Machine (SVM)","c1d6c9e8":"<a id =\"17\"><\/a><br>\n# Random Forest","cfd09d86":"<a id =\"19\"><\/a><br>\n## Gradient Boosting Machine (GBM)"}}