{"cell_type":{"9f5a28a1":"code","0cbed416":"code","e5c48296":"code","1ec7e287":"code","52d9edf0":"code","5422d7a8":"code","024c3663":"code","2a6ddc00":"code","671f3a18":"code","955c83c9":"code","1796c36f":"code","5b547166":"code","d95bd67e":"code","d4bff744":"code","37c3bed3":"code","2f3168ea":"code","13ccd823":"code","30363571":"code","66df5f44":"code","97873e57":"code","0f049785":"code","97f5968c":"code","54c44eab":"code","011bf158":"markdown","277e0919":"markdown","f4f3fde0":"markdown","7f75c8bb":"markdown","a36d87d1":"markdown","e549ef5f":"markdown","d0fc3e77":"markdown","50f4be0d":"markdown","807c7a1f":"markdown","c1c0fa60":"markdown","ef642e0b":"markdown","35c79fd7":"markdown","6ed94903":"markdown","0ce79271":"markdown","3200959d":"markdown","8f3a8e51":"markdown","215e79b0":"markdown","af04f7e9":"markdown","7dc18255":"markdown","157f566c":"markdown","7279c354":"markdown","a80cbd76":"markdown","ec3e41d6":"markdown","92a1c57f":"markdown","77233b40":"markdown"},"source":{"9f5a28a1":"import pandas as pd\nimport numpy as np\nfrom itertools import product\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor","0cbed416":"for p in [np, pd, lgb]:\n    print (p.__name__, p.__version__)","e5c48296":"sales = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest_data = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_category = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\n\nsales.head()","1ec7e287":"def downcast_dtypes(df):\n    '''\n        Changes column types in the dataframe: \n                \n                `float64` type to `float32`\n                `int64`   type to `int32`\n    '''\n    \n    # Select columns to downcast\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n    \n    # Downcast\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols]   = df[int_cols].astype(np.int32)\n    \n    return df","52d9edf0":"def lag_feature(all_data, list_lags, index_cols, cols_to_rename):\n    shift_range = list_lags\n\n    for month_shift in tqdm_notebook(shift_range):\n        train_shift = all_data[index_cols + cols_to_rename].copy()\n    \n        train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift\n    \n        foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x\n        train_shift = train_shift.rename(columns=foo)\n\n        all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)\n\n    del train_shift\n    return all_data","5422d7a8":"Monthly_sales = sales.groupby([\"date_block_num\", \"shop_id\"])['item_cnt_day'].sum().reset_index(name = 'item_cnt_month')\n\n\nfig, axs = plt.subplots(10, 6)\n\nfor i in range(60):\n  shop_sale_per_month = Monthly_sales.loc[Monthly_sales['shop_id']==i]\n  axs[i\/\/6,i%6].tick_params(axis='both', which='both', bottom=False, top= False, labelbottom=False, right=False, left=False, labelleft=False)\n  axs[i\/\/6,i%6].plot(shop_sale_per_month['date_block_num'], shop_sale_per_month['item_cnt_month'])\n\n\ndel Monthly_sales, shop_sale_per_month","024c3663":"sales.loc[sales.shop_id == 0, 'shop_id'] = 57\ntest_data.loc[test_data.shop_id == 0, 'shop_id'] = 57\n\nsales.loc[sales.shop_id == 1, 'shop_id'] = 58\ntest_data.loc[test_data.shop_id == 1, 'shop_id'] = 58\n\nsales.loc[sales.shop_id == 10, 'shop_id'] = 11\ntest_data.loc[test_data.shop_id == 10, 'shop_id'] = 11\n\n\n# remove the oulier\nsales = sales[sales.item_cnt_day<1001]","2a6ddc00":"temp_df = pd.merge(test_data[['shop_id','item_id']],sales[['shop_id','item_id']], on=['shop_id','item_id'], how='left', indicator='Exist')\ntemp_var =  (temp_df['Exist']=='left_only').sum()\nprint('Number of unique shop-item combination in the test set that do not exist in the training set:',temp_var)","671f3a18":"Leakage_Percentage = ((test_data.shape[0]-temp_var)\/test_data.shape[0])*100\nprint('Percentage of shop-item combination in test data that are available in the training set:', Leakage_Percentage)","955c83c9":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops\/items combinations from that month\ngrid = [] \nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales[sales['date_block_num']==block_num]['shop_id'].unique()\n    cur_items = sales[sales['date_block_num']==block_num]['item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n#turn the grid into pandas dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n#get aggregated values for (shop_id, item_id, month)\ngb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':['sum']})\ngb.rename(columns = {'sum':'target'}, inplace = True) \n\n#fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n#join aggregated data to the grid\nall_data = pd.merge(grid,gb,how='left',on=index_cols).fillna(0)\n#sort the data\nall_data.sort_values(['date_block_num','shop_id','item_id'],inplace=True)\nall_data['target'] = all_data['target'].fillna(0).clip(0,20)\n\n# Don't use old data from year 2013\nall_data = all_data[all_data['date_block_num'] >= 12] \nall_data","1796c36f":"# Adding city_enc column\nshops['city'] = shops.shop_name.apply(lambda x: str.replace(x, '!', '')).apply(lambda x: x.split(' ')[0])\nshops['city_enc'] = LabelEncoder().fit_transform(shops['city'])\nshops_data = shops[['shop_id','city_enc']]\nall_data = pd.merge(all_data, shops_data, how='left', on=['shop_id'])\n\n# Adding item_category_id column\nall_data = pd.merge(all_data, items, how='left', on=['item_id'])\nall_data = all_data.drop('item_name',axis =1)\n\n# Adding basket_enc column\nitem_category['basket'] = item_category['item_category_name'].apply(lambda x: str(x).split(' ')[0])\nitem_category['basket_enc'] = LabelEncoder().fit_transform(item_category['basket'])\nitem_category = item_category[['item_category_id','basket_enc']]\nall_data = pd.merge(all_data, item_category, how='left', on=['item_category_id'])\nall_data","5b547166":"all_data = pd.concat([all_data, test_data], ignore_index=True, sort=False, keys=['date_block_num','shop_id','item_id', 'city_enc', 'item_category_id', 'basket_enc', 'target'])\nall_data = downcast_dtypes(all_data)\nall_data","d95bd67e":"# shop-month aggregates\ngb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':['sum']})\ngb.rename(columns = {'sum':'target_shop'}, inplace = True)\n\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)\n\n# item-month aggregates\ngb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':['sum']})\ngb.rename(columns = {'sum':'target_item'}, inplace = True)\ngb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]\nall_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)\n\nall_data = downcast_dtypes(all_data)\n\nall_data","d4bff744":"from tqdm import tqdm_notebook\n\nindex_cols = ['shop_id', 'item_id', 'date_block_num', 'item_category_id', 'basket_enc', 'city_enc']\ncols_to_rename = list(all_data.columns.difference(index_cols)) \nlist_lags = [1, 2, 3]\nall_data = lag_feature(all_data, list_lags, index_cols, cols_to_rename)\nall_data = downcast_dtypes(all_data)\n\nall_data","37c3bed3":"shift_range = list_lags\n\n# List of all lagged features\nfit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] \n# We will drop these at fitting stage\nto_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] \ndel sales, grid\nto_drop_cols","2f3168ea":"X_train = all_data[all_data.date_block_num < 33].drop(to_drop_cols, axis=1)\nY_train = all_data[all_data.date_block_num < 33]['target']\nX_valid = all_data[all_data.date_block_num == 33].drop(to_drop_cols, axis=1)\nY_valid = all_data[all_data.date_block_num == 33]['target']\nX_test = all_data[all_data.date_block_num == 34].drop(to_drop_cols, axis=1)","13ccd823":"X = X_train.append(X_valid)\nY = np.append(Y_train, Y_valid)","30363571":"lgb_params = {\n               'feature_fraction': 0.75,\n               'metric': 'rmse',\n               'nthread':1, \n               'min_data_in_leaf': 2**7, \n               'bagging_fraction': 0.75, \n               'learning_rate': 0.05, \n               'objective': 'mse', \n               'bagging_seed': 2**7, \n               'num_leaves': 2**7,\n               'bagging_freq':1,\n               'verbose':0 \n              }\nlgb = lgb.train(lgb_params, lgb.Dataset(X, label=Y), 100)\n\npred_lgb_val = lgb.predict(X_valid)\n\nprint('Train mse is %f' % mean_squared_error(Y_train, lgb.predict(X_train)))\nprint('Val mse is %f' % mean_squared_error(Y_valid, pred_lgb_val))","66df5f44":"rf = RandomForestRegressor(bootstrap=0.7, criterion='mse', max_depth=10,\n           max_features=6, max_leaf_nodes=None, min_impurity_decrease=0.0,\n           min_impurity_split=None, min_samples_leaf=1,\n           min_samples_split=2, min_weight_fraction_leaf=0.0,\n           n_estimators=10, n_jobs=4, oob_score=False, random_state=None,\n           verbose=0, warm_start=False)\nrf.fit(X,Y)\n\npred_rf_val = rf.predict(X_valid)\nprint('Train mse is %f' % mean_squared_error(Y_train, rf.predict(X_train)))\nprint('Val mse is %f' % mean_squared_error(Y_valid, pred_rf_val))","97873e57":"plt.scatter(pred_rf_val, pred_lgb_val)","0f049785":"X_val_level2 = np.c_[pred_rf_val, pred_lgb_val]\n\nlr = LinearRegression()\nlr.fit(X_val_level2, Y_valid)\npred_lr_val =  lr.predict(X_val_level2)\nprint('Test mse is %f' % mean_squared_error(Y_valid, pred_lr_val))","97f5968c":"lr = LinearRegression()\nlr.fit(X_val_level2, Y_valid)\npred_lr_val =  lr.predict(X_val_level2)\nprint('Test mse is %f' % mean_squared_error(Y_valid, pred_lr_val))","54c44eab":"all_data.to_csv('mycsvfile.csv',index=False)","011bf158":"I'm going to stack two models in order to improve the model.","277e0919":"This fuction is part of one of the assignments to reduce the size of data","f4f3fde0":"Finally, I'm going to add lagged data. Based on my analysis using trial and error, I found just lagged data for previous 3 months has highest impact","7f75c8bb":"Now, I'm going to add the test data","a36d87d1":"# Basic functions","e549ef5f":"# Final Project Coursera","d0fc3e77":"The following code is based in assignments and it will be used in order to create all posible combinations of shop-items and fill out the target column.","50f4be0d":"# Final Test","807c7a1f":"As we can see, about 52% of combinations already exists in the training set. I'm going to use them if any leaked is found.","c1c0fa60":"# Mean Encoding","ef642e0b":"Here, I'm going to aggregate data","35c79fd7":"In general, sales have seasonal behaviour as expected. However, some shops show abnormal behaviour which turned out to be duplication issue and fixed as follows. Also, we removed the outliers from data.","6ed94903":"# Data Leakage","0ce79271":"Finding the number of unique shop-item combinations that only exist in test data","3200959d":"# Stacking","8f3a8e51":"I have ploted sales of shops per month, in order to see the general behavior","215e79b0":"The first step is to load all the required libraries and load raw data files into memory.","af04f7e9":"I'm going to add the following features:\n* City code from the shops csv\n* Category ID\n* The text data in item category gives some extra info that can be used such as the basket of commodities that an item belongs to","7dc18255":"# Training","157f566c":"Here is how data looks like. ","7279c354":"I'm going to train two LGBM and random forest model and later I'll stack them","a80cbd76":"**Validating packages versions**.\n\nI'm going to validate de version of the installed packages","ec3e41d6":"It is required to add lagged data as new features","92a1c57f":"# EDA","77233b40":"# Feature Engineering"}}