{"cell_type":{"f89c0346":"code","b113ab2b":"code","2f5ccead":"code","65b01b90":"code","e05491ae":"code","a7e866a0":"code","543fe49a":"code","d12877ba":"code","4ff34462":"code","c8bd8462":"code","0040c9b7":"code","93f57f12":"code","e9f055ab":"code","f9beb5ab":"code","1e067ee2":"code","71cd4bec":"code","eec0cbca":"code","80d5c4da":"code","3b0dfe58":"code","11dd1123":"code","4861f2b2":"code","3fa4abfc":"code","a240002d":"code","24c2b9cf":"code","3d5941bf":"code","7f398bb3":"code","5f68e971":"code","5cb0e5ce":"code","3c571086":"code","3695e4dd":"code","e799f95b":"code","c8f912a3":"code","051dbfb8":"code","cac859bb":"code","191c5bb7":"markdown","1b0fa940":"markdown","b5897896":"markdown","10008068":"markdown","ab2551ef":"markdown","bf40b27c":"markdown"},"source":{"f89c0346":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling as pp\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b113ab2b":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","2f5ccead":"train.head()","65b01b90":"combined = pd.concat([train, test], ignore_index=True)\ncombined.head()","e05491ae":"target = train['Survived']\n","a7e866a0":"print('Sum of missing values')\nprint(combined.isnull().sum())\nprint('-------------------------------')\n\nprint('Percentage of missing values')\nprint(combined.isnull().mean())","543fe49a":"# drop this column becaues they are not useful and cabin features has a lot of missing values\ncombined = combined.drop(['Survived', 'Cabin', 'Ticket', 'PassengerId'], axis=1)","d12877ba":"combined['Age'].describe()","4ff34462":"#Age cleaning\n\n# replace null values with mean of age\ncombined['Age'] = combined['Age'].replace(np.NaN, np.mean(combined['Age']))\n\n# create a bin for the age group\nbins = (-1, 0, 5, 12, 18, 40, 60, 120)\nagegroup = ['Unknown', 'Baby', 'Child', 'Teenager', 'Youth', 'Adult', 'Elder']\n\n# create new feature agegroup\ncombined['Agegroup'] = pd.cut(combined['Age'], bins, labels=agegroup)\n\n# drop age column\ncombined = combined.drop(combined[['Age']], axis=1)\n\n","c8bd8462":"combined.head()","0040c9b7":"# Handling the fare column\n#fill missing value\ncombined['Fare'] = combined['Fare'].replace(np.NaN, np.mean(combined['Fare']))","93f57f12":"# Handling Names\n#create new feature by extracting the titles from the names idea gotten from\n#https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n\ncombined['Title'] = combined.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n# After extraction group titles\ncombined['Title'] = combined['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don',\n                                                'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', \n                                                'Dona'], 'Rare')\ncombined['Title'] = combined['Title'].replace('Mlle', 'Miss')\ncombined['Title'] = combined['Title'].replace('Ms', 'Miss')\ncombined['Title'] = combined['Title'].replace('Mme', 'Mrs')\n\n#drop name column\ncombined = combined.drop(combined[['Name']], axis=1)","e9f055ab":"combined['FamilySize'] = combined['SibSp'] + combined['Parch'] + 1","f9beb5ab":"# embarked column\n#fill missing value with the most frequent \ncombined['Embarked'] = combined['Embarked'].fillna('S')","1e067ee2":"combined = combined.drop(combined[['Parch', 'SibSp']], axis=1)","71cd4bec":"combined.head()","eec0cbca":"# get the nominal and ordinal columns that needs to be label encoded or one hot encoded\n\nnominal = combined[['Embarked', 'Sex', 'Title']]\nordinal = combined[['Agegroup']]","80d5c4da":"\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n# One hot encode the norminal columns\n\nfor col in nominal:\n    dummy = pd.get_dummies(nominal, drop_first=True)\n    combineddf = pd.concat([combined, dummy], axis=1)\n    \n    \n#label encode the ordinal features\ncombineddf['Agegroup'] = le.fit_transform(combineddf['Agegroup'])\ncombineddf['Title'] = le.fit_transform(combineddf['Title'])\n    \ncombineddf = combineddf.drop(nominal, axis=1)","3b0dfe58":"combineddf.head()","11dd1123":"#Split the data into train and test\n\ntraindf = combineddf.loc[:890,:]\ntestdf = combineddf.loc[891:, :]","4861f2b2":"print(traindf.shape)\nprint(testdf.shape)","3fa4abfc":"#use train_test_split to creata train and validation data\n\nX_train, X_test, y_train, y_test = train_test_split(traindf, target, test_size= 0.2, random_state=42)","a240002d":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100)\nrf.fit(X_train, y_train)","24c2b9cf":"pred = rf.predict(X_test)","3d5941bf":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, pred))","7f398bb3":"rf1_pred = rf.predict(testdf)\nsub = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived':rf1_pred})\n\nsub = sub.to_csv('rf1.csv', index=False)","5f68e971":"clf = RandomForestClassifier()\n\nparameters = {'n_estimators': [100, 200, 300],\n              'max_features': ['sqrt', 'auto', 'log2', None],\n              'criterion': ['entropy', 'gini'],\n              'bootstrap': [True, False],\n              'max_depth': [2, 4,5, 6, 7],\n              'min_samples_leaf': [2, 3, 4,5,6,7]}\n\n\ncv = GridSearchCV(clf, param_grid=parameters, cv=5, n_jobs=-1)\n\ncv.fit(X_train, y_train)","5cb0e5ce":"best_params = cv.best_params_","3c571086":"rfr = RandomForestClassifier(max_depth=best_params[\"max_depth\"], \n                            n_estimators=best_params[\"n_estimators\"],\n                            bootstrap=best_params[\"bootstrap\"],\n                            min_samples_leaf=best_params[\"min_samples_leaf\"],\n                            max_features=best_params['max_features'],\n                            criterion = best_params['criterion'],\n                        )\n\nrfr.fit(X_train, y_train)","3695e4dd":"rfr_pred = rfr.predict(X_test)\nprint(classification_report(y_test, rfr_pred))","e799f95b":"sub = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived':rf1_pred})\n\nsub = sub.to_csv('rf.csv', index=False)","c8f912a3":"from imblearn.over_sampling import SMOTE","051dbfb8":"sm = SMOTE(random_state=42)\nX_train_res, y_train_res = sm.fit_resample(X_train, y_train.ravel())\n\nrfs = RandomForestClassifier()\nrfs.fit(X_train, y_train)\n\npred = rfs.predict(X_test)\n\nprint(classification_report(y_test, pred))","cac859bb":"rfs_pred = rfs.predict(testdf)\nsub = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived':rfs_pred})\n\nsub = sub.to_csv('rfs.csv', index=False)","191c5bb7":"# Model","1b0fa940":"# Data Preparation","b5897896":"Some hyper parameter tuning","10008068":"Checking the shape of the train and test data frame","ab2551ef":"Read the data","bf40b27c":"Here i used SMOTE  to handle imbalanced distribution of data in the target variable"}}