{"cell_type":{"cae68443":"code","cf69140d":"code","810e3c13":"code","80895dc0":"code","709ff7b1":"code","b9ae8b26":"code","6e4783f1":"code","12d7b4df":"code","5e605ed9":"code","a34ff9f7":"code","9b6b4a94":"code","fd18eaf2":"code","1e8ec9fc":"code","0d6d84c0":"markdown","cb009280":"markdown","c45ed116":"markdown","882226e4":"markdown"},"source":{"cae68443":"import pandas as pd","cf69140d":"df = pd.DataFrame({'text': ['I like kaggle very much',\n                            'I do not like kaggle',\n                            'I do really love machine learning']})\ndf","810e3c13":"from sklearn.feature_extraction.text import CountVectorizer\n\n\nvectorizer = CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\nbag = vectorizer.fit_transform(df['text'])\nbag.toarray()","80895dc0":"print(vectorizer.vocabulary_)","709ff7b1":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n\nvectorizer = CountVectorizer(token_pattern=u'(?u)\\\\b\\\\w+\\\\b')\ntransformer = TfidfTransformer()\n\ntf = vectorizer.fit_transform(df['text'])\ntfidf = transformer.fit_transform(tf)\nprint(tfidf.toarray())","b9ae8b26":"print(vectorizer.vocabulary_)","6e4783f1":"from gensim.models import word2vec\n\n\nsentences = [d.split() for d in df['text']]\nmodel = word2vec.Word2Vec(sentences, size=10, min_count=1, window=2, seed=7)","12d7b4df":"model.wv['like']","5e605ed9":"model.wv.most_similar('like')","a34ff9f7":"df['text'][0].split()","9b6b4a94":"import numpy as np\n\n\nwordvec = np.array([model.wv[word] for word in df['text'][0].split()])\nwordvec","fd18eaf2":"np.mean(wordvec, axis=0)","1e8ec9fc":"np.max(wordvec, axis=0)","0d6d84c0":"# Word2vec","cb009280":"# TF-IDF","c45ed116":"# Bag of Words","882226e4":"This notebook is a sample code with Japanese comments.\n\n# 3.3 Titanic\u306e\u5148\u3078\u884c\u304f\u2462\uff01\u3000\u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u89e6\u308c\u3066\u307f\u3088\u3046"}}