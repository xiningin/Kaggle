{"cell_type":{"4ba32bcd":"code","a393eea2":"code","e4ac7290":"code","b1ab76f6":"code","3a9f23d7":"code","faa72af1":"code","4ab54319":"code","c276c726":"code","921800b9":"code","4ff3d6d8":"code","02b5c374":"code","e13ca5f7":"code","7d0f9a5a":"code","5c328692":"code","471c2c67":"code","03d2cb06":"code","54588e55":"code","bae95178":"code","16967702":"code","add33f20":"code","3f5df390":"code","14ba9993":"code","3b5fb435":"code","5c09b23f":"code","f0f12adc":"code","3931efec":"code","bb191cc5":"code","93c011dd":"markdown","251e09d0":"markdown","e0f25827":"markdown","d8a70531":"markdown","f73dfe27":"markdown","0cf1bb38":"markdown","2b6f023c":"markdown","f999add4":"markdown","2056e6b0":"markdown","f56b1874":"markdown","3a2b2a77":"markdown","9fb9a888":"markdown","eccca7ab":"markdown","b041eb3e":"markdown","2bd8886f":"markdown","7306057d":"markdown","63f4226c":"markdown","b6f0603c":"markdown","9a30108a":"markdown","df44f51e":"markdown","7d21a293":"markdown","3c51713a":"markdown","1e96655d":"markdown","c7474ca3":"markdown","28ecacd6":"markdown","b58aa9ef":"markdown","1f1485ef":"markdown"},"source":{"4ba32bcd":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom plotnine import *\nfrom matplotlib import gridspec\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn import metrics\nfrom sklearn.metrics import pairwise_distances\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom sklearn.mixture import GaussianMixture\nimport matplotlib.cm as cm\nfrom sklearn.cluster import DBSCAN","a393eea2":"energy_df=pd.read_csv(r'..\/input\/eergy-efficiency-dataset\/ENB2012_data.csv')\nenergy_df.head()","e4ac7290":"energy_df.columns=[\"relative_compactness\",\"surface_area\",\"wall_area\",\"roof_area\",\"overall_height\",\"orientaion\",\n                   \"glazing_area\",\"glazing_area_dist\",\"heating_load\",\"cooling_load\"]","b1ab76f6":"energy_df.describe()","3a9f23d7":"energy_df.loc[energy_df[\"glazing_area\"]==0].describe()","faa72af1":"energy_df.loc[energy_df[\"glazing_area_dist\"]==0].describe()","4ab54319":"energy_df.hist(figsize=(15,15))\nplt.show()","c276c726":"energy_df[\"log_heating_load\"]=np.log(energy_df[\"heating_load\"])\nenergy_df[\"log_heating_load\"].hist(bins=6)\nplt.show()","921800b9":"sns.pairplot(energy_df)\nplt.show()","4ff3d6d8":"corr = energy_df.corr()\nmask = np.zeros_like(corr, dtype=bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(12, 10))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, mask=mask,cmap=cmap, vmax=.9, center=0, square=True, linewidths=.5, annot=True,cbar_kws={\"shrink\": .5})\nplt.show()","02b5c374":"energy_df_f=energy_df.copy()\nenergy_df_f.drop([\"heating_load\",\"cooling_load\"],axis=1,inplace=True)\n#energy_df_f.drop([\"log_heating_load\",\"cooling_load\"],axis=1,inplace=True)","e13ca5f7":"energy_df_f.columns","7d0f9a5a":"SH_score=[]\nCH_score=[]\nprint(\"Silhouette analysis based on number of clusters:\")\nfig, ax = plt.subplots(3, 2, figsize=(15,8))\nfor i in [2,3,4,5,6,7]:\n    clusterer = KMeans(n_clusters=i, init='k-means++',n_init=10, max_iter=100,random_state=48)\n    Kmean_label = clusterer.fit_predict(energy_df_f)\n    CH_temp=metrics.calinski_harabasz_score(energy_df_f, Kmean_label)\n    CH_score.append(CH_temp)\n    \n    q, mod = divmod(i, 2)\n    visualizer = SilhouetteVisualizer(clusterer, colors='yellowbrick', ax=ax[q-1][mod])\n    visualizer.fit(energy_df_f)\n    SH_score.append(visualizer.silhouette_score_)\n","5c328692":"cluster_score=list(zip(SH_score,CH_score))\ncluster_score=pd.DataFrame(cluster_score)\ncluster_score.columns=[\"Silhouette_score\",\"CH_score\"]\ncluster_score.index=cluster_score.index+2","471c2c67":"fig,ax_cs=plt.subplots(1,2,figsize=(10,5))\nsns.lineplot(data=cluster_score.iloc[:,0],ax=ax_cs[0])\nsns.lineplot(data=cluster_score.iloc[:,1],ax=ax_cs[1])\nplt.show()","03d2cb06":"clusterer_best = KMeans(n_clusters=3, init='k-means++',n_init=10, max_iter=100,random_state=48)\nKmean_label_best = clusterer_best.fit_predict(energy_df_f)\nenergy_df_f[\"Cluster label\"]=Kmean_label_best","54588e55":"fig,ax=plt.subplots(3,3,figsize=(15,8),sharex=True)\nfor i,col in enumerate(energy_df_f.columns[:-1]):\n    q, mod = divmod(i, 3)\n    sns.scatterplot(data=energy_df_f,y=col,x=energy_df_f.columns[-1],ax=ax[q][mod])","bae95178":"energy_df_f.loc[:,['relative_compactness', 'surface_area', 'wall_area', 'Cluster label']].groupby(\"Cluster label\").\\\nagg([np.mean,np.median,np.std])","16967702":"energy_df_f.loc[:,[ 'roof_area','overall_height', 'log_heating_load','Cluster label']].\\\ngroupby(\"Cluster label\").agg([np.mean,np.median,np.std])","add33f20":"energy_df_em_f=energy_df_f.iloc[:,:-1].copy()","3f5df390":"SH_EM_score=[]\nCH_EM_score=[]\n#print(\"Silhouette analysis based on number of clusters:\")\n#fig, ax = plt.subplots(3, 2, figsize=(15,8))\nfor i in [2,3,4,5,6,7]:\n    em_clusterer = GaussianMixture(n_components=i,max_iter=100,n_init=10,init_params='kmeans',random_state=48)\n    em_label = em_clusterer.fit_predict(energy_df_em_f)\n    CH_temp=metrics.calinski_harabasz_score(energy_df_em_f, em_label)\n    CH_EM_score.append(CH_temp)\n    SH_temp=metrics.silhouette_score(energy_df_em_f, em_label)\n    SH_EM_score.append(SH_temp)\n    ","14ba9993":"cluster_score_em=list(zip(SH_EM_score,CH_EM_score))\ncluster_score_em=pd.DataFrame(cluster_score_em)\ncluster_score_em.columns=[\"Silhouette_score\",\"CH_score\"]\ncluster_score_em.index=cluster_score_em.index+2","3b5fb435":"fig,ax_cs=plt.subplots(1,2,figsize=(10,5))\nsns.lineplot(data=cluster_score_em.iloc[:,0],ax=ax_cs[0])\nsns.lineplot(data=cluster_score_em.iloc[:,1],ax=ax_cs[1])\nplt.show()","5c09b23f":"clusterer_em_best = GaussianMixture(n_components=3,max_iter=100,n_init=10,init_params='kmeans',random_state=48)\nem_label_best = clusterer_em_best.fit_predict(energy_df_em_f)\nem_silhouette_values = metrics.silhouette_samples(energy_df_em_f.iloc[:,:-1], em_label_best)\nem_silhouette_avg = metrics.silhouette_score(energy_df_em_f.iloc[:,:-1], em_label_best)\ny_lower = 0\nfig, ax1 = plt.subplots(1, 1)\nfor i in np.unique(em_label_best):\n    # Aggregate the silhouette scores for samples belonging to\n    # cluster i, and sort them\n    ith_cluster_silhouette_values = \\\n        em_silhouette_values[em_label_best == i]\n\n    ith_cluster_silhouette_values.sort()\n\n    size_cluster_i = ith_cluster_silhouette_values.shape[0]\n    y_upper = y_lower + size_cluster_i\n\n    color = cm.nipy_spectral(float(i) \/ (max(np.unique(em_label_best))+1))\n    ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                      0, ith_cluster_silhouette_values,\n                      facecolor=color, edgecolor=color, alpha=0.7)\n\n    # Label the silhouette plots with their cluster numbers at the middle\n    ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n    # Compute the new y_lower for next plot\n    y_lower = y_upper + 10  # 10 for the 0 samples\n\nax1.set_title(\"The silhouette plot for the various clusters.\")\nax1.set_xlabel(\"The silhouette coefficient values\")\n#ax1.set_ylabel(\"Cluster label\")\n\n# The vertical line for average silhouette score of all the values\nax1.axvline(x=em_silhouette_avg, color=\"red\", linestyle=\"--\")\n\n#ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\nax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\nplt.show()","f0f12adc":"energy_df_em_f[\"Cluster label\"]=em_label_best\nfig,ax=plt.subplots(3,3,figsize=(15,8),sharex=True)\nfor i,col in enumerate(energy_df_em_f.columns[:-1]):\n    q, mod = divmod(i, 3)\n    sns.scatterplot(data=energy_df_em_f,y=col,x=energy_df_em_f.columns[-1],ax=ax[q][mod])","3931efec":"energy_df_em_f.loc[:,['relative_compactness', 'surface_area', 'wall_area', 'Cluster label']].groupby(\"Cluster label\").\\\nagg([np.mean,np.median,np.std])","bb191cc5":"energy_df_em_f.loc[:,[ 'roof_area','overall_height', 'log_heating_load','Cluster label']].\\\ngroupby(\"Cluster label\").agg([np.mean,np.median,np.std])","93c011dd":"Looking at heating load, it seems to be heavily skewed to the right. Therefore, log transformation will be done on heating load to make it more normalised. ","251e09d0":"## Visualising Data Based on 3 Clusters","e0f25827":"Based on the figures above, the most optimal number of cluster is 3 as it has the smallest values for Silhouette score and CH score. This indicates that the 3 clusters are able to separate the datapoints clearly while retaining the large within cluster variability for each cluster.  ","d8a70531":"relative compactness is highly correlated to surface area, roof area and overall height. \n\nheating load is highly correlated to cooling load which suggested that only 1 of them can be used as dependent factor.  ","f73dfe27":"This notebook will use energy efficiency dataset to explore 2 clustering techniques: K-means and Expectation Maximisation. \n\nK-means clustering is a clustering method that randomly select a few points as centroids and cluster the data points according to the distance of the datapoints from the centroids. The datapoints that are closer to one particular centroid will be assigned to the cluster where the centroid is located. The centroid for each cluster usually will be the mean values of the cluster. \nhttps:\/\/en.wikipedia.org\/wiki\/K-means_clustering\n\nExpectation maximisation clustering makes use of Guassian mixture models to estimate the latent variables in the data and maximise the parameters of the models using the data. From there, the datapoints can be grouped into multiple clusters according to the estimated distributions from the mixture models.  \nhttps:\/\/en.wikipedia.org\/wiki\/Expectation%E2%80%93maximization_algorithm\n\nThe data source is from https:\/\/archive.ics.uci.edu\/ml\/datasets\/Energy+efficiency","0cf1bb38":"This section will understand the data using histograms, scatter plots and correlation matrix. Data cleansing and transformation will be done if they are necessary. ","2b6f023c":"This section will use K-Means Clustering to do clustering on the data. To determine the optimal number of clusters, CH and Silhouette scores are used. ","f999add4":"# Data Exploration & Transformation","2056e6b0":"Cluster 2 has buildings that have large value for relative compactness and small value for surface area but same mean value for wall area compared to cluster 1. Cluster 0 has buildings that have large wall areas compared to cluster 1 and cluster 2.","f56b1874":"# Searching for Optimal Number of Clusters Using Calinski-Harabasz (CH) and Silhouette Scores","3a2b2a77":"# Data Loading ","9fb9a888":"Looking at the cluster size for each cluster, cluster 0 is the smallest followed by cluster 2 and cluster 3.","eccca7ab":"This section will load the data into the notebook to look at the data structure. ","b041eb3e":"# Library Loading","2bd8886f":"Cluster 1 has the highest relative compactess and lowest surface area compared to other clusters while cluster 2 has the lowest wall area compared to other clusters in terms of mean and median. ","7306057d":"This section will use EM Clustering to do clustering on the data. To determine the optimal number of clusters, the same metric scores in the previous section are used.\n\nHowever, the Silhouette visualiser from Yellowbrick library does not support EM clustering. So, only the number of clusters with the smallest Silhouette score will be visualised.","63f4226c":"# Using Raw Data for K-Means Clustering","b6f0603c":"Based on the graphs above, they both showed that 3 clusters are the most optimal as 3 clusters have the smallest Silhouette score and CH score indicating that they contain high variability within cluster for each cluster with clear separation among clusters.","9a30108a":"Compared to K-Means clustering, other than orientation, glazing area and glazing area distribution, cluster 1 is the cluster with lowest values while cluster 0 and cluster 2 have higher values when comparing to cluster 1.","df44f51e":"## Searching for Optimal Number of Clusters Using Calinski-Harabasz (CH) and Silhouette Scores","7d21a293":"After log transformation, the distribution looks better but shows a bimodal distribution as two peaks are formed. ","3c51713a":"Cluster 1 has buildings with larger roof area but lower in terms of height and heating load (log) respectively. Cluster 0 has buildings with larger heating load (log) compared to cluster 1 and cluster 2.\n\nIn K-Means clustering, cluster 1 contains buildings that are tall, high relative compactness and high heating load but smaller surface area and roof area while in EM clustering, cluster 1 contains buildings that are low in terms of height and heating load but large in terms of roof area and surface area. \n\nAs a conclusion, the mean values for relative compactness, surface area, roof area, overall height and wall area in each cluster are different  indicating that these features might influence the heating load required by the building to maintain the warm indoor environment. ","1e96655d":"Based on the visualisation above, there are clear differences between clusters in terms of relative compactness, surface area, wall area, roof area, overall height and log_heating_load. ","c7474ca3":"# Using Raw Data for Expectation Maximisation (EM) Clustering","28ecacd6":"## Visualising Data Based on 3 Clusters","b58aa9ef":"Cluster 1 has the lowest roof area, highest overall height and log heating load compared to other clusters. ","1f1485ef":"Looking at the figures above, 3 clusters to 5 clusters seems to be optimal number of clusters as each cluster exceed the average Silhouette score. Cluster 2 for 3 clusters seem to be the biggest among the 3 clusters while the size of the cluster for 4 to 5 clusters seems to be quite uniform except the last cluster. "}}