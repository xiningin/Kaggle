{"cell_type":{"aa6acaf2":"code","9eb69c88":"code","1d43748c":"code","e3df6275":"code","bcea6e6e":"code","5085d563":"code","ccdd9190":"code","01ad0b1d":"code","1beaf05d":"code","06ae0c51":"code","e6be8e96":"code","d2975d58":"code","2a66f6ad":"code","127ffcf1":"code","86f0cb43":"markdown","4795efed":"markdown","b0d02711":"markdown","1cebd7a1":"markdown","49b62449":"markdown","72d9e951":"markdown","4f34e3ca":"markdown","606baa12":"markdown","327800a1":"markdown","f0646b5b":"markdown","3d099c9a":"markdown","021114d9":"markdown","3f7cd2b3":"markdown","501741a6":"markdown","c29cff4b":"markdown","e6c2d03d":"markdown","86fe127a":"markdown","991e020f":"markdown"},"source":{"aa6acaf2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import manifold #needed for multidimensional scaling (MDS) and t-SNE\nfrom sklearn import cluster #needed for k-Means clustering\nfrom sklearn import preprocessing #needed for scaling attributes to the nterval [0,1]","9eb69c88":"df = pd.read_csv('..\/input\/iris_nolabels.csv')\ndata = np.array(df.values, dtype=float)\nprint('(number of examples, number of attributes): ', data.shape)","1d43748c":"min_max_scaler = preprocessing.MinMaxScaler()\ndata = min_max_scaler.fit_transform(data)","e3df6275":"colors = np.array(['orange', 'blue', 'lime', 'blue', 'khaki', 'pink', 'green', 'purple'])\n\n# points - a 2D array of (x,y) coordinates of data points\n# labels - an array of numeric labels in the interval [0..k-1], one for each point\n# centers - a 2D array of (x, y) coordinates of cluster centers\n# title - title of the plot\n\ndef clustering_scatterplot(points, labels, centers, title):\n    # plot the examples, i.e. the data points\n    \n    n_clusters = np.unique(labels).size\n    for i in range(n_clusters):\n        h = plt.scatter(points[labels==i,0],\n                        points[labels==i,1], \n                        c=colors[i%colors.size],\n                        label = 'cluster '+str(i))\n\n    # plot the centers of the clusters\n    if centers is not None:\n        plt.scatter(centers[:,0], centers[:,1], c='r', marker='*', s=500)\n\n    _ = plt.title(title)\n    _ = plt.legend()\n    _ = plt.xlabel('x')\n    _ = plt.ylabel('y')","bcea6e6e":"k = 3\nclustered_data_sklearn = cluster.KMeans(n_clusters=k, n_init=10, max_iter=300).fit(data)","5085d563":"# append the cluster centers to the dataset\ndata_and_centers = np.r_[data,clustered_data_sklearn.cluster_centers_]","ccdd9190":"# project both th data and the k-Means cluster centers to a 2D space\nXYcoordinates = manifold.MDS(n_components=2).fit_transform(data_and_centers)\nprint(\"transformation complete\")","01ad0b1d":"# plot the transformed examples and the centers\n# use the cluster assignment to colour the examples\nclustering_scatterplot(points=XYcoordinates[:-k,:], \n                       labels=clustered_data_sklearn.labels_, \n                       centers=XYcoordinates[-k:,:], \n                       title='MDS')","1beaf05d":"# project both th data and the k-Means cluster centers to a 2D space\nXYcoordinates = manifold.TSNE(n_components=2).fit_transform(data_and_centers)\nprint(\"transformation complete\")","06ae0c51":"# plot the transformed examples and the centers\n# use the cluster assignment to colour the examples\n# plot the transformed examples and the centers\n# use the cluster assignment to colour the examples\nclustering_scatterplot(points=XYcoordinates[:-k,:], \n                       labels=clustered_data_sklearn.labels_,\n                       centers=XYcoordinates[-k:,:], \n                       title='TSNE')","e6be8e96":"df['cluster'] = pd.Series(clustered_data_sklearn.labels_, index=df.index)","d2975d58":"df.head()","2a66f6ad":"df.tail()","127ffcf1":"df.groupby('cluster').mean()","86f0cb43":"# A. Preparation","4795efed":"This notebook demonstrates:\n- the k-Means clustering algorithm\n- the use of manifold-learning techniques to project a clustered dataset into a 2D space and visualise it as a scatterplot\n\nIt is assumed that the input dataset contains ___only numerical___ attributes. To use this code with a dataset that contains categorical attributes, they first need to be _somehow_ transformed to numerical. How categorical attributes can be represented as numerical may depend on the nature of the dataset.\n\nWhen you go through this notebook for the first time you may __skip section C__ and go back to it at the end.","b0d02711":"## Add the cluster labels as an extra column in the original dataframe","1cebd7a1":"## Conclusion\n\nBy looking at the table above we can argue that:\n* One of the clusters are large iris flowers with the largest petals out of the three types.\n* Another cluster are iris flowers with small petals (notably smaller than the other two types) but with somehow wide sepals, the widest sepals of all three types. This is also the cluster that is clearly differentiated from the other two clusters in the scatter plots.\n* The third cluster are flowers that are slightly smaller than the flowers in the cluster with the largest flowers with a most notable difference in the width of the petals.\n\n**Note: If the code is executed again, k-Means may produce either different clusters or the same clusters but in a different order.**","49b62449":"# D. Cluster Analysis","72d9e951":"## Scale the Data\nHere we scale the values in each column to the interval [0,1]. See https:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html for alternative scaling methods. In yout work on this excersise experiment with other scaling techniques to see if they lead to a more insightful clustering.","4f34e3ca":"# TUTORIAL 6: CLUSTERING AND MANIFOLD LEARNING\nby [Nikola S. Nikolov](http:\/\/bdarg.org\/niknikolov)\n\n-----","606baa12":"# C. Manifold-Learning For Visualisation of a Clustering","327800a1":"Number of clusters $k$.","f0646b5b":"## Load the Dataset","3d099c9a":"## Apply multi-dimensional scaling (MDS) to project both the data and the k-Means cluster centers to a 2D space","021114d9":"## Compute the mean value of each attribute for each cluster","3f7cd2b3":"## Import Python Modules","501741a6":"## Scatterplot Function \n(to be used below for visualising a clustering)\n\nSkip this section when you read the notebook for the first time.","c29cff4b":"# Further Work\n\n1. Plot the sum of squared distances from the data points to the centers of the k-Means clusters for various values of k. Use the Elbow method to pick the best value of k. Attempt also another method for determining the best value of k. If these methods suggest a different value of k (from the one you used in Task 3), then compute a new k-Means clustering.\n\n2. Apply another clustering algorithm (from the ones available in scikit-learn) for the same dataset and an appropriate manifold-learning technique to visualise it. Observe the differences between the k-Means clustering and the clustering found by the second algorithm you have tried by visualising the clusterings with an appropriate manifold-learning technique.\n\nTo do this refer to the following articles:\n\n* Methods for finding the best k for k-Means:\nhttps:\/\/stackoverflow.com\/questions\/19197715\/scikit-learn-k-means-elbow-criterion\n\n* Scikit-learn Clustering Algorithms:\nhttps:\/\/scikit-learn.org\/stable\/modules\/clustering.html\n\n* Scikit-learn Feature Scalers:\nhttps:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html\n\n* Scikit-learn Manifold Learning Methods:\nhttps:\/\/scikit-learn.org\/stable\/modules\/manifold.html","e6c2d03d":"See https:\/\/scikit-learn.org\/stable\/modules\/clustering.html for alternative clustering methods.","86fe127a":"## Apply t-SNE to project both the data and the k-Means cluster centers to a 2D space","991e020f":"# B. K-Means Clustering"}}