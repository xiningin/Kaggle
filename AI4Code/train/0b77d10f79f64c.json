{"cell_type":{"d116b7eb":"code","7e9931a1":"code","1e6f531b":"code","d5a22b40":"code","6c6740a1":"code","c9b44ed7":"code","4178f2db":"code","00e25711":"code","599a0921":"code","d3ae0a2b":"code","ffa1395b":"markdown","a7febdac":"markdown","70f43b7b":"markdown","63ca1024":"markdown","1436a58f":"markdown","f5e5ef61":"markdown","ac426d37":"markdown","ce22eed2":"markdown"},"source":{"d116b7eb":"!pip install --upgrade langdetect -q # install language detection\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport json # read json files\n\nimport glob #find pathnames\nimport random# for sampling\nrandom.seed(4)\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.cluster import KMeans# Import k-means to perform clusters analysis\nfrom sklearn.metrics import silhouette_score #To chose the number of topics\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport nltk #natural language processing\nfrom nltk.corpus import stopwords\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\n                        \nfrom gensim import corpora #Create Corprea\nfrom gensim.models.ldamodel import LdaModel #LDA model\n\nimport pyLDAvis.gensim #Display Topics\n\n        \nstop_words =  set(stopwords.words('english'))\nstop_words.update(['et', 'al',\"addition\", \"respectively\", \"found\", \"although\",'present',\n                  'identified','Thu','Finally','either','suggesting','include',\"well\", \n                   \"associated\", \"method\", \"result\",'used','doi','display',\n                  'https','copyright', 'holder','org','author','available','made','peer',\n                  'reviewed','without','permission','license','rights','reserverd','Furthermore'\n                  'using','preprint','allowed','following','may','thus','funder','International',\n                 'granted','compared','will','one','two','use','different','likely','Discussion',\n                 'medRexiv','Introduction','Moreover','known','funder','6','7','8','paywall' ,\n                       'downarrow','textstyle','1','2','3','4','5','9','10','0','include','number','work','begin','fig','show'])\n","7e9931a1":"#The files while they have some structure are not completely uniform so pandas.read_json() will not work in this case. For example one article could have one author and another five\n#the Schema\n\n#with open(\"\/kaggle\/input\/CORD-19-research-challenge\/json_schema.txt\",'r') as f:\n#    file = f.read()\n#    print(file)\n","1e6f531b":"class Kaggle_Covid_19:\n    '''Created to easy the ETL\/EDA Kaggle Covid_19 dataset from:\n    https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge'''\n        \n    def _article_paper_id_title(self,article):\n        '''Create a list of a paper's ID and title''' \n\n        metadata = [article[\"paper_id\"],article[\"metadata\"][\"title\"]]\n        return  metadata\n    \n    \n    def _article_authors(self,article):\n        '''Create a list of a paper's ID, authors and source'''\n        authors = []\n\n        for idx in range(len(article[\"metadata\"][\"authors\"])):\n            author = [article[\"paper_id\"],article[\"metadata\"][\"authors\"][idx][\"first\"], article[\"metadata\"][\"authors\"][idx][\"last\"]]\n            authors.append(author)\n        return authors\n\n    \n    def _article_text(self,article):\n        '''Create a list of a paper's ID, and abstracts and body text'''\n        import numpy as np\n        text = article[\"metadata\"][\"title\"]\n        abstract = ''\n    \n        try:\n            for idx in range(len(article[\"abstract\"])):\n                abstract = abstract + '\\n\\n' + article[\"abstract\"][idx][\"text\"]\n            abstract = abstract.strip()\n        except: abstract = ''\n        \n        for idx in range(len(article[\"body_text\"])):\n            text = text + '\\n\\n' + article[\"body_text\"][idx][\"section\"] + '\\n\\n' + article[\"body_text\"][idx][\"text\"]\n        text = text.strip()\n    \n        article_text = [article[\"paper_id\"],text,abstract]\n        return article_text    \n    \n    \n    \n    def _check_lang(self,text,seed=0):\n        '''Check the language of text'''\n        \n        from langdetect import detect, DetectorFactory\n        DetectorFactory.seed = seed\n        \n        try:\n            return detect(text)\n    \n        except: return None\n    \n    \n    def _tokenize(self,text):\n    \n        '''Cleans Text Data by:\n        1.Remove stopwords\n        2.Remove punctuation\n        3.Normalize text\n        4.Lemmatize text'''\n    \n        from nltk.stem.wordnet import WordNetLemmatizer\n        import re\n        from nltk.tokenize import word_tokenize\n    \n        lemmatizer = WordNetLemmatizer()\n        \n        # normalize case and remove punctuation\n        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n    \n        # tokenize text\n        tokens = word_tokenize(text)\n    \n        # lemmatize(noun) and remove stop words\n        lem = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n        \n        # lemmatize(verb) and remove stop words\n        lem = [lemmatizer.lemmatize(word,pos = 'v') for word in lem ]\n        \n        # lemmatize(adjective) and remove stop words\n        tokens  = [lemmatizer.lemmatize(word,pos = 'a') for word in lem ]\n\n        return tokens\n\n\n    def _load_files(self):\n    \n        '''(Kaggle) Takes in filepath and returns three dataframe with:\n        1.[\"Paper_Id\",'Text','Abstract']\n        2.[\"Paper_Id\",\"Title\"]\n        3.[\"Paper_Id\",\"First_Name\",\"Last_Name\"]'''\n    \n\n    \n        filepaths = glob.glob(self.filepath,recursive = True) #getting file paths\n    \n        articles = random.sample(filepaths,self.num_of_articles) #taking a sample as the corpus is to large (for Kaggle)\n    \n    \n        #initialize lists\n        titles_list = []\n        authors_list = []\n        text = []\n    \n        #Filling lists\n        for article in articles:\n            article = json.load(open(article, 'rb'))\n            titles_list.append(self._article_paper_id_title(article))\n            authors_list.extend([*self._article_authors(article)])              \n            text.append(self._article_text(article)) \n    \n        #Transform lists into DataFrames\n            \n        self.authors = pd.DataFrame(authors_list,columns = [\"Paper_Id\",\"First_Name\",\"Last_Name\"])\n        self.titles  = pd.DataFrame(titles_list,columns = [\"Paper_Id\",\"Title\"])\n        self.texts   = pd.DataFrame(text,columns = [\"Paper_Id\",'Text','Abstract'])\n        \n        self._to_clean = pd.concat([self.texts[\"Paper_Id\"],self.texts['Text'] + self.texts['Abstract']],axis = 1 )\n        self._to_clean.columns =[\"Paper_Id\",'Full_Text']\n\n\n\n\n\n    def _clean_text(self):\n        \n        '''Produces a list of cleaned text data (load_files should be run first)'''\n        import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n        #Combine all text data into one column\n  \n        lang = []\n        \n        for text in self._to_clean['Full_Text']:\n            if self._check_lang(text) != None:\n                lang.append(self._check_lang(text))\n            else:lang.append(None)\n    \n        self._to_clean['Language'] = pd.Series(lang)\n        \n        #Remove articles that are not in english\n        self._to_clean = self._to_clean[self._to_clean['Language'] == 'en']\n\n        #Remove the Language column \n        self._to_clean = self._to_clean.drop('Language',axis = 1)\n        #clean text data\n        _clean_text = self._to_clean['Full_Text'].apply(self._tokenize) \n        self.clean_text = pd.concat([self._to_clean['Paper_Id'],_clean_text],axis = 1)\n\n        #Remove unneeded dataframes \n        del self._to_clean\n        del _clean_text\n\n    \n    def _tfidf(self):\n        '''Creates TFIDF matrix'''\n        # initialize tf-idf vectorizer \n        vectorizer = TfidfVectorizer()\n        # compute tf-idf values\n        self.tfidf_matrix = vectorizer.fit_transform([' '.join(doc) for doc in self.clean_text['Full_Text']])\n        \n        \n    def _cosine_sim(self):\n        '''Creates a dataframe of the cosine similarities of all articles'''\n        self._tfidf()\n        sims = cosine_similarity(self.tfidf_matrix, self.tfidf_matrix)#get similarity scores\n        self.similarity_ = pd.DataFrame(list(sims)) # Create similarity dataframe\n\n        #Change columns and index of the dataframe\n        self.similarity_.columns = self.clean_text['Paper_Id']\n        self.similarity_.index = self.clean_text['Paper_Id']\n\n\n        \n    def __init__(self, filepath,num_of_articles=1000):\n        self.filepath = filepath\n        self.num_of_articles = num_of_articles\n        self._load_files()\n        self._clean_text()\n        self._cosine_sim()\n        \n        \n\n    \n    def keyword_Search(self,keywords):\n        '''Searches data set for articles containing keywords.\n    \n        articles: Article dataframe from load_files function\n           \n        clean_text : The output of the clean_data fuction.\n        \n        keywords: A list of words to search for in the articles'''\n    \n   \n        keywords = self._tokenize(' '.join(keywords))\n    \n        papers = []\n\n        for word in keywords:\n            for idx,row in self.clean_text.iterrows():\n                try:\n                    if word in row['Full_Text']:\n                         papers.append(row['Paper_Id'])\n                except: TypeError\n        \n        return papers\n    \n    \n    def similarity(self,paper_id_,score = 0):\n        '''Acepsts \n            paper_id: ID of the Article\n            sim: similarity dataframe\n            score: minimum similarity score [0,1]\n        \n            Returns a pd.Series with of similarity scores with a minimum value score\n        '''\n        similar_articles = self.similarity_.loc[self.similarity_[paper_id_]>score,paper_id_]\n        return similar_articles.drop(paper_id_,axis = 0)\n","d5a22b40":"file_path = \"\/kaggle\/input\/\" + \"**\/*.json\" \n\ncovid = Kaggle_Covid_19(file_path,num_of_articles=10000)","6c6740a1":"#init number of clusters\nclus_num = 6\nsil_score = []\n\nSum_of_squared_distances = []\n \nfor k in range(2,clus_num):\n    kmeans = KMeans(n_clusters = k).fit(covid.tfidf_matrix)\n    labels = kmeans.labels_\n    sil_score.append(silhouette_score(covid.tfidf_matrix, labels, metric = 'euclidean'))\n    \nplt.plot(range(2,clus_num), sil_score, 'bx-')\nplt.xlabel('clusters')\nplt.ylabel('silhouette score')\nplt.title('silhouette Method For Optimal k')\nplt.show()\n\n#the highest point denotes the number of clusters","c9b44ed7":"_np = np.asarray(sil_score)  #convert to np.array\n\ntopic_num = _np.argmax() + 2 #set number of topics\n\n#Create Corpus and Dictionary \ndictionary = corpora.Dictionary(covid.clean_text['Full_Text'])\ncorpus = [dictionary.doc2bow(text) for text in covid.clean_text['Full_Text']]\n\n#Create LDA model\nldamodel = LdaModel(corpus, num_topics=topic_num,id2word=dictionary, passes=30)\n\n#Display Topics \nlda_display = pyLDAvis.gensim.prepare(ldamodel, corpus,dictionary, sort_topics=False)\npyLDAvis.display(lda_display)","4178f2db":"def topic_keywords(ldamodel,topic_num):\n    '''Extract topics from LDA model'''\n    keywords = {}\n    for i in range(topic_num):\n        keywords[i] = ', '.join(x[0] for x in ldamodel.show_topic(i))\n    \n    return keywords","00e25711":"#Extract the main topic for each document in the corpus\nmain_topic_prop = [sorted(article, key=lambda x: (x[1]), reverse=True)\\\n                   for i, article in enumerate(ldamodel[corpus])]\n\n#Dictionary of topics in LDA model\ntopics = topic_keywords(ldamodel,topic_num)\n\n#Create a Data Frame with the Main Topic and %score \ntopic_Dataframe = pd.DataFrame([(doc[0][0],doc[0][1],topics[doc[0][0]]) for doc in main_topic_prop],\\\n                             columns = ['Dominant_Topic', '% Score', 'Topic_Keywords'])\n\ntopic_Dataframe  = pd.concat([covid.clean_text['Paper_Id'].reset_index(drop = True),topic_Dataframe],axis = 1)#Concate Paper ID for articles\ntopic_Dataframe ","599a0921":"covid.keyword_Search(keywords = ['cough'])[:5]","d3ae0a2b":"covid.similarity(paper_id_ = '119a8706e074b4cf6f024cf2f77607835cbd06a6',score = 0.3)","ffa1395b":"# Loading Files","a7febdac":"# Import Packages","70f43b7b":"The following class Kaggle_Covid_19 contains the code for ETL of the data. there is also a method for the keyword search. (A class was creater beacuse the code was getting to repetative)","63ca1024":"# Topic modeling","1436a58f":"# Cosine Similarity","f5e5ef61":"This notebook takes a a quick look at Covid-19 article dataset on Kaggle https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge  \nIt focuses on different ways of sifting through the articles.  \nThe noteboook is separated as such \n\n1. Loading and Cleaning\n2. Keyword Search\n3. Topic Modeling\n4. Cosine Similarity","ac426d37":"The number of topics chosen for the LDA is usually done through trail and error. I prefer to use KMeans as a starting point ","ce22eed2":"# Keyword search"}}