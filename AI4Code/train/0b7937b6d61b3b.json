{"cell_type":{"3c3e21e1":"code","ec21a284":"code","5b3e7b0b":"code","324aa43e":"code","8833b51a":"code","fe2812ca":"code","d0ded420":"code","c7fbffb5":"code","23cd93e6":"code","3ca02443":"code","66345890":"code","0df6d3e2":"code","49372e6e":"code","ec61aeed":"code","cd9a9a59":"code","b0bb91e0":"code","3d4a79ef":"code","28ee7c67":"code","82b0dd46":"code","98eca5a4":"markdown","eeb59eba":"markdown","1ffd27c1":"markdown","f293fa34":"markdown","cb7d2311":"markdown","9a9763c0":"markdown","19634555":"markdown","afd09b9b":"markdown","8db8d383":"markdown","dd40d934":"markdown","4ed53ba3":"markdown"},"source":{"3c3e21e1":"!pwd\n!ls","ec21a284":"cat \/proc\/cpuinfo| grep \"processor\"| wc -l","5b3e7b0b":"!nvidia-smi","324aa43e":"!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n!pip install --upgrade kornia\n!pip install allennlp==1.1.0.rc4","8833b51a":"import torch\nfrom torch import nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.optim import lr_scheduler\nfrom PIL import Image\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport time\nimport os\nimport copy\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\nimport cv2\nfrom tqdm import tqdm\nprint(torch.__version__)","fe2812ca":"#efn\n!pip install efficientnet_pytorch\nfrom efficientnet_pytorch import EfficientNet","d0ded420":"TRAIN_DATASET_PATH = '..\/input\/digixcv\/train_data'\nIMG_SIZE = (512, 512)\nBATCH_SIZE = 8\nNUM_WORKERS = 2\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","c7fbffb5":"class ImageDataset(torch.utils.data.Dataset):\n    def __init__(self, image_fns, label_dict, data_transforms):\n        self.image_fns = image_fns\n        self.label_dict= label_dict\n        self.transforms = data_transforms\n    \n    def __getitem__(self, index):\n#         label = self.label_dict[image_fns[index].split('\/')[-2]]\n        label = self.label_dict[self.image_fns[index].split('\/')[-2]]\n        image = Image.open(self.image_fns[index]).convert(\"RGB\")\n        image = self.transforms(image)\n        \n        return image, label#, image_fns[index]\n    \n    def __len__(self):\n        return len(self.image_fns)\n        ","23cd93e6":"image_fns = glob(os.path.join(TRAIN_DATASET_PATH, '*', '*.*'))\nlabel_names = [s.split('\/')[-2] for s in image_fns]\nNUM_IMAGES = len(label_names)\nprint('NUM_IMAGES:', NUM_IMAGES)\n\nunique_labels = list(set(label_names))\nunique_labels.sort()\nid_labels = {_id:name for name, _id in enumerate(unique_labels)}\n\nNUM_CLASSES = len(unique_labels)\nprint(\"NUM_CLASSES:\", NUM_CLASSES)","3ca02443":"_mean, _std = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n\n# train_transform = transforms.Compose(\n#     [transforms.RandomRotation((-15, 15)),\n#      transforms.Resize(IMG_SIZE[0]),\n#      transforms.RandomCrop(IMG_SIZE[0]),\n#      transforms.ColorJitter(brightness=0.1, contrast=0.1,saturation=0.1),\n#      transforms.ToTensor(),\n#      transforms.Normalize(_mean, _std),])\n\nimport random\nclass SquareRandomCrop:\n    def __call__(self, pil_im):\n        w, h = pil_im.size\n        if w == h:\n            return pil_im\n        elif w > h:\n            top, bottom = 0, h\n            left = random.randint(0, w-h)\n            right = left + h\n            return pil_im.crop((left, top, right, bottom))\n        else:\n            left, right = 0, w\n            top = random.randint(0, h-w)\n            bottom = top + w\n            return pil_im.crop((left, top, right, bottom))\n\nrots = [transforms.RandomRotation((90, 90)), transforms.RandomRotation((180, 180)), transforms.RandomRotation((270, 270))]\nrots_trans = transforms.RandomChoice(rots)\ntrain_transform = transforms.Compose(\n    [transforms.RandomApply([rots_trans], p=0.2),\n     transforms.RandomRotation((-15, 15)),\n     transforms.Resize(IMG_SIZE[0]),\n     transforms.RandomChoice([SquareRandomCrop(), transforms.CenterCrop(IMG_SIZE[0])]),\n     transforms.ColorJitter(brightness=0.1, contrast=0.1,saturation=0.1),\n     transforms.ToTensor(),\n     transforms.Normalize(_mean, _std),])\n\nval_transform = transforms.Compose(\n    [transforms.Resize(IMG_SIZE[0]),\n     transforms.CenterCrop(IMG_SIZE[0]),\n     transforms.ToTensor(),\n     transforms.Normalize(_mean, _std),])","66345890":"# torch.hub.list('pytorch\/vision:v0.6.0')","0df6d3e2":"# keetar's way of train-val split: 1 sample per class which has >=4 samples\nlabel_dict = {i:[] for i in range(NUM_CLASSES)}\nfor fn in image_fns:\n    label = id_labels[fn.split('\/')[-2]]\n    label_dict[label].append(fn)\ntrain_fns, val_fns = [], []\nfor label, label_fns in label_dict.items():\n    if len(label_fns)>=4:\n        val_fns.append(label_fns[0])\n        train_fns += label_fns[1:]\n    else:\n        train_fns += label_fns\nlen(train_fns), len(val_fns)","49372e6e":"# train_fns, val_fns = train_test_split(image_fns, test_size=0.1, shuffle=True, random_state=42)\n\ntrain_dataset = ImageDataset(train_fns, id_labels, train_transform)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                                          shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\nval_dataset = ImageDataset(val_fns, id_labels, val_transform)\nval_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE,\n                                          shuffle=True, num_workers=NUM_WORKERS, pin_memory=True)\ndatalaoders_dict = {'train':train_loader, 'val':val_loader}\n\n# from pyretri.models.backbone.backbone_impl.reid_baseline import ft_net_own\n# path_file = '\/data\/nextcloud\/dbc2017\/files\/jupyter\/model\/resnet50-19c8e357.pth'\n# model = ft_net_own(progress=True)\n\n# model = torch.hub.load('pytorch\/vision:v0.6.0', 'resnet50', pretrained=True)\n# num_ftrs = model.fc.in_features\n# model.fc = nn.Linear(num_ftrs, NUM_CLASSES)\n\n# model = torch.hub.load('pytorch\/vision:v0.6.0', 'densenet201', pretrained=True)\n# num_ftrs = model.classifier.in_features\n# model.classifier = nn.Linear(num_ftrs, NUM_CLASSES)","ec61aeed":"FEATURE_DIM = 512\narch = EfficientNet.from_pretrained('efficientnet-b7', num_classes=FEATURE_DIM)\narch_nohead = EfficientNet.from_pretrained('efficientnet-b7', include_top=False)\n# model.load_state_dict(torch.load('models\/efn0_512_best.pth'))\n\n#refernce https:\/\/github.com\/MuggleWang\/CosFace_pytorch\/blob\/master\/layer.py\ndef cosine_sim(x1, x2, dim=1, eps=1e-8):\n    # print(x1.shape, x2.shape)\n    ip = torch.mm(x1, x2.t())\n    w1 = torch.norm(x1, 2, dim)\n    w2 = torch.norm(x2, 2, dim)\n    return ip \/ torch.ger(w1,w2).clamp(min=eps)\n\nclass MarginCosineProduct(nn.Module):\n    r\"\"\"Implement of large margin cosine distance: :\n    Args:\n        in_features: size of each input sample\n        out_features: size of each output sample\n        s: norm of input feature\n        m: margin\n    \"\"\"\n\n    def __init__(self, in_features, out_features, s=30.0, m=0.35):\n        super(MarginCosineProduct, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.s = s\n        self.m = m\n        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))\n        nn.init.xavier_uniform_(self.weight)\n        #stdv = 1. \/ math.sqrt(self.weight.size(1))\n        #self.weight.data.uniform_(-stdv, stdv)\n\n    def forward(self, input, label):\n        cosine = cosine_sim(input, self.weight)\n        # cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n        # --------------------------- convert label to one-hot ---------------------------\n        # https:\/\/discuss.pytorch.org\/t\/convert-int-into-one-hot-format\/507\n        one_hot = torch.zeros_like(cosine)\n        one_hot.scatter_(1, label.view(-1, 1), 1.0)\n        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n        output = self.s * (cosine - one_hot * self.m)\n\n        return output\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(' \\\n               + 'in_features=' + str(self.in_features) \\\n               + ', out_features=' + str(self.out_features) \\\n               + ', s=' + str(self.s) \\\n               + ', m=' + str(self.m) + ')'\n\nclass Net(nn.Module):\n    def __init__(self, arch=None, in_features=None, out_features=None, s=30.0, m=0.35):\n        super(Net, self).__init__()\n        self.arch = arch\n        self.cosface = MarginCosineProduct(in_features, out_features, s, m)\n        \n    def forward(self, x, label):\n        x = self.arch(x)\n        x = self.cosface(x, label)\n        return x\n\nclass NDNet(nn.Module):\n    def __init__(self, arch=None, in_features=None, out_features=None, s=30.0, m=0.35):\n        super(NDNet, self).__init__()\n        self.arch = arch\n        self.cosface = MarginCosineProduct(in_features, out_features, s, m)\n        \n    def forward(self, x, label):\n        x = self.arch(x)\n        x = x.flatten(start_dim=1)\n        x = self.cosface(x, label)\n        return x\n\n# model = Net(arch=arch, in_features=FEATURE_DIM, out_features=NUM_CLASSES)\n\n\nmodel=NDNet(arch=arch_nohead, in_features=2560, out_features=NUM_CLASSES, s=30.0, m=0.35)\n\n\nprint(model.eval())","cd9a9a59":"!pip install kornia\nimport kornia","b0bb91e0":"i = random.randint(1, len(train_dataset))\nfig, axs = plt.subplots(3, 3, figsize=(16, 8))\nfor row in range(3):\n    for col in range(3):\n        ax = axs[row, col]\n        ax.set_axis_off()\n        image, label = train_dataset[i]\n        image = kornia.enhance.denormalize(image, mean=torch.Tensor(_mean), std=torch.Tensor(_std))\n        # print(image)\n        numpy_image = image.permute(1,2,0).cpu().numpy()\n        ax.imshow(numpy_image)","3d4a79ef":"def train_model(model, dataloaders, criterion, optimizer, scheduler, num_epochs, name):\n    \n    ROOT_PATH = '.\/'\n\n    if not os.path.isdir(ROOT_PATH):\n        os.mkdir(ROOT_PATH)\n\n    log_path = os.path.join(ROOT_PATH, f'[LOG]{name}.txt')\n    def log(message):\n        print(message)\n        with open(log_path, 'a+') as logger:\n            logger.write(f'{message}\\n')\n\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    best_loss = 1e5\n    \n    scaler = torch.cuda.amp.GradScaler()\n    \n    for epoch in range(num_epochs):\n        lr = optimizer.param_groups[0]['lr']\n        log('Epoch {}\/{}, lr={}'.format(epoch, num_epochs - 1, lr))\n\n        # Each epoch has a training and validation phaseSS\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    with torch.cuda.amp.autocast():\n#                         outputs = model(inputs)\n                        outputs = model(inputs, labels)\n                        _, preds = torch.max(outputs, 1)\n                        loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n#                         loss.backward()\n#                         optimizer.step()\n                        scaler.scale(loss).backward() \n                        scaler.step(optimizer)\n                        scaler.update()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train':\n                scheduler.step(running_loss)\n\n            epoch_loss = running_loss \/ len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() \/ len(dataloaders[phase].dataset)\n\n\n            log('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_loss < best_loss:\n                best_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n                torch.save(best_model_wts, os.path.join(ROOT_PATH, f'{name}_best.pth'))\n\n        # save every ckpt\n        ckpt = {\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'scheduler_state_dict': scheduler.state_dict(),\n        }\n        torch.save(ckpt, os.path.join(ROOT_PATH, f'{name}_ckpt@epoch{epoch}.pth'))\n        print()\n\n    time_elapsed = time.time() - since\n    log('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    log('Best val Acc: {:4f}'.format(best_acc))\n    log('Best val loss: {:4f}'.format(best_loss))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","28ee7c67":"# keetar's way: Weighted cross entropy : proportional to 1\/log(class cnt)\nlabel_img_counts = {label: len(label_fns) for label, label_fns in label_dict.items()}\n","82b0dd46":"model = model.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\ncriterion = torch.nn.CrossEntropyLoss()\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=0, verbose=True)\nmodel_net = train_model(model, datalaoders_dict, criterion, optimizer, scheduler, num_epochs=3, name='efn7_512_cosface_nodense')","98eca5a4":"# weighted cross-entropy","eeb59eba":"# get labels","1ffd27c1":"# define transforms","f293fa34":"# train","cb7d2311":"# hyp","9a9763c0":"# define trainer","19634555":"# init data","afd09b9b":"# install&import","8db8d383":"# visualize","dd40d934":"# init model","4ed53ba3":"# define dataset class"}}