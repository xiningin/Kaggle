{"cell_type":{"6c06432c":"code","9dc7e8c3":"code","6c2ebfde":"code","6717556d":"code","89a363e8":"code","472f0af8":"code","dfe575a3":"code","b9483bb6":"code","a191fa85":"code","cd698199":"code","ed9a48b9":"code","ecf10a9c":"code","ba886b11":"code","fc8ebb93":"code","6353e2e9":"code","b9f14766":"code","ff9cb221":"code","0eb38916":"code","71b507d0":"code","587ebc6a":"markdown","79a3c2a2":"markdown","85ae933c":"markdown","d53ae7a6":"markdown","72d4e20a":"markdown","fd9c7147":"markdown","fb263c14":"markdown","68536652":"markdown","76c3587e":"markdown","54b6c04c":"markdown","622e491a":"markdown","915d4d88":"markdown","9440c879":"markdown","d6de0e1b":"markdown","50bf3d9f":"markdown","4b5ade73":"markdown","7f601af0":"markdown"},"source":{"6c06432c":"import pandas as pd\nimport numpy as np\nimport re\n\nfrom sklearn.metrics import matthews_corrcoef\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBRFClassifier\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","9dc7e8c3":"train_df = pd.read_csv(\"..\/input\/open-shopee-code-league-marketing-analytics\/train.csv\").set_index('row_id')\ntest_df = pd.read_csv(\"..\/input\/open-shopee-code-league-marketing-analytics\/test.csv\").set_index('row_id')\nuser_df = pd.read_csv(\"..\/input\/open-shopee-code-league-marketing-analytics\/users.csv\")","6c2ebfde":"train_df.head()","6717556d":"user_df.head()","89a363e8":"train_df = train_df.merge(user_df, left_on = 'user_id', right_on = 'user_id')\ntest_df = test_df.merge(user_df, left_on = 'user_id', right_on = 'user_id')","472f0af8":"train_df.head()","dfe575a3":"def add_datepart(df, fldname, drop=True):\n    fld = df[fldname]\n    if not np.issubdtype(fld.dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    for n in ('Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear',\n            'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start'):\n        df[targ_pre+n] = getattr(fld.dt,n.lower())\n    if drop: df.drop(fldname, axis=1, inplace=True)\n    \nadd_datepart(train_df, 'grass_date')\nadd_datepart(test_df, 'grass_date')","b9483bb6":"train_df.head()","a191fa85":"def remove_datecols(df):\n    fld = ['grass_Year','grass_Is_quarter_end','grass_Is_quarter_start','grass_Is_year_start','grass_Is_year_end']\n    df.drop(fld, axis=1, inplace=True)\n    \nremove_datecols(train_df)\nremove_datecols(test_df)","cd698199":"train_df.head()","ed9a48b9":"#Remove userid because they are not required for prediction\ntrain_df.drop('user_id', axis = 1, inplace = True)\ntest_df.drop('user_id', axis = 1, inplace = True)","ecf10a9c":"train_df = pd.concat([train_df.drop('country_code', axis=1), pd.get_dummies(train_df['country_code'])], axis=1)\ntrain_df = pd.concat([train_df.drop('domain', axis=1), pd.get_dummies(train_df['domain'])], axis=1)\ntest_df = pd.concat([test_df.drop('country_code', axis=1), pd.get_dummies(test_df['country_code'])], axis=1)\ntest_df = pd.concat([test_df.drop('domain', axis=1), pd.get_dummies(test_df['domain'])], axis=1)","ba886b11":"#Replace never open, never checkout and never login to nan\ntrain_df.replace([\"Never open\", \"Never login\", \"Never checkout\"], np.nan, inplace = True)\ntest_df.replace([\"Never open\", \"Never login\", \"Never checkout\"], np.nan, inplace = True)\ntrain_df[['last_open_day','last_login_day','last_checkout_day']] = train_df[['last_open_day','last_login_day','last_checkout_day']].apply(pd.to_numeric)\ntest_df[['last_open_day','last_login_day','last_checkout_day']] = test_df[['last_open_day','last_login_day','last_checkout_day']].apply(pd.to_numeric)\n","fc8ebb93":"#Fill missing values\ntrain_df.fillna({\"last_open_day\":train_df['last_open_day'].max(),\n                 \"last_login_day\":train_df['last_login_day'].max(),\n                 \"last_checkout_day\":train_df['last_checkout_day'].max(),\n                 \"attr_1\": 2,\n                 \"attr_2\": 2,\n                 \"attr_3\": 2,\n                 \"age\": train_df['age'].median()}, inplace = True)\n\ntest_df.fillna({\"last_open_day\":test_df['last_open_day'].max(),\n                 \"last_login_day\":test_df['last_login_day'].max(),\n                 \"last_checkout_day\":test_df['last_checkout_day'].max(),\n                 \"attr_1\": 2,\n                 \"attr_2\": 2,\n                 \"attr_3\": 2,\n                 \"age\": test_df['age'].median()}, inplace = True)","6353e2e9":"def get_xy(df, target_col='open_flag', **kwargs):\n    feature_cols = [ col for col in df.columns if col != target_col ]\n    if target_col in df:\n        X = df[feature_cols]\n        y = df[target_col]\n        X_train, X_valid, y_train, y_valid = train_test_split(X, y)\n        return X_train, X_valid, y_train, y_valid\n    else:\n        X_test = df[feature_cols]\n        return X_test\n    \nX_train, X_valid, y_train, y_valid = get_xy(train_df, test_size=0.2, random_state=123)\nX_test = get_xy(test_df)","b9f14766":"model = RandomForestClassifier(max_depth = 200, n_estimators = 300, n_jobs = -1, bootstrap = True, random_state = 123)\nrf = model.fit(X_train, y_train)","ff9cb221":"y_pred = rf.predict(X_valid)\nmatthews_corrcoef(y_valid,y_pred)","0eb38916":"# Make predictions on test set with RF\ny_pred = model.predict(X_test)\nsubmission_df = pd.DataFrame({'row_id':np.arange(len(test_df)),\n                              'open_flag':y_pred})\nsubmission_df.to_csv('submission.csv', index = False)","71b507d0":"def imp_df(column_names, importances):\n  df = pd.DataFrame({'variable': column_names,\n                     'variable_importance': importances}) \\\n         .sort_values('variable_importance', ascending = False) \\\n         .reset_index(drop = True)\n  return df\nbase_imp = imp_df(X_train.columns, rf.feature_importances_)\n\nplt.figure(figsize = (15,8))\nfig = sns.barplot(x = 'variable_importance', y = 'variable', data = base_imp, orient = 'h', color = 'royalblue')\nplt.title(\"Variable Importance using RFC\", fontsize = 20)\nplt.xlabel('Variable Importance', fontsize = 16)\nplt.ylabel('')\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 14)\nplt.show()","587ebc6a":"# Adding Additional Features by Adding Date Parts\n\nWhen we look at the ``grass_date`` feature in the dataset, it may not appear to be a great predictive feature on its own. However, if we dissect the dates and separate it based on the day of week, day of month, etc.. Some of these additional features might serve as a great predictor.\n\nFor example, some might open an e-mail relating to e-commerce during the weekends in their own personal e-mails because that is when they are most likely to be free.\n\nSo I will make use of fast.ai's ``add_datepart`` function to help with the addition of these features.","79a3c2a2":"# Welcome to my notebook!\nThis competition is a classic machine learning classification problem. And like with most data science competition, **dealing with noise and data pre-processing is a major part of the process.** I will walk you through my entire thought process in dealing with noise present within the data. It should be easy enough for beginners to understand but don't hope to achieve a high score with this! :)\n\n","85ae933c":"Now we see that every row has been added 5 additional features that could potentially help with the robustness of the prediction.","d53ae7a6":"# Read and inspect the csv files","72d4e20a":"# Predicting the Test Set and Submitting to Kaggle","fd9c7147":"# Evaluating the Model","fb263c14":"# One-Hot Encoding E-mail Domain and Country Code\n\nI do this because these are categorical features and should not have any cardinality within the random forest algorithm.","68536652":"# Importing libraries","76c3587e":"# Processing Never Open\/Login\/Checkout and Other Missing Values","54b6c04c":"# [Optional] Inspecting Feature Importance","622e491a":"# Modelling using Random Forest Classifier","915d4d88":"# Conclusion\n\nI am clearly still a beginner in fine-tuning the random forest classifier algorithm. It's been awhile since I messed around with machine learning algorithms, but I'm still happy I managed to tackle this competition!\n\nThere is still much to learn including using more sophisticated algorithms such as LightGBM and XGBoost as well as hyperparameter tuning using GridSearchCV.\n\nThere are still a lot of things you can do in data preparation such as, and not limited to:\n\n***1. Dealing with outliers (negative age and age hitting 118)***\n\n***2. Removing certain features that appear redundant with either Recursive Feature Elimination or simply deciding from the feature importance chart***\n\n***3. More feature engineering, especially with the top 3 features.***\n\nI hope you find this notebook useful especially in the data pre-processing part for your participation in this competition! :)","9440c879":"# Data Anomalies\n\nWith the help of [@soappp9527's EDA work](http:\/\/www.kaggle.com\/soappp9527\/marketing-analytics-eda-by-r-data-table-ggplot2), and inspecting them within the csv in the excel program, here are the basic anomalies that requires processing.\n\n**1. Never open\/login\/checkout in 3 of the columns**\n\n**2. Missing values in attr_1,2 and 3**\n\n**3. Missing values in age**","d6de0e1b":"# Split Training Dataset for Modelling\n\nCredits to [Nathaniel Ng's notebook](https:\/\/www.kaggle.com\/nathaniel\/marketing-analytics-baseline-0-41795) for the following chunk of code, saved me the trouble from typing it out on my own!","50bf3d9f":"Now that we have separated the date into multiple features, it's time to remove features that are unnecessary for obvious reasons.","4b5ade73":"# Merging Users Dataframe with Train and Test Dataframes\n\nEvery feature is important to me unless otherwise told by the random forest algorithm's feature importance. Since we are given the user's information, why not make use of the data? Especially when you think about it, age might probably play a factor in opening e-mails. So let's do a left join on both the train and test dataframes to add additional features for prediction.","7f601af0":"# Removing User ID"}}