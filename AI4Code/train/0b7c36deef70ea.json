{"cell_type":{"250071bf":"code","14abed50":"code","483609bb":"code","9a1e5e8d":"code","b6ac248e":"code","7f75f7f9":"code","a2176dc9":"code","c288dc81":"code","6837ea16":"code","d89b71f1":"code","9b24641e":"code","7ae80b8b":"code","26f5493f":"code","ab87573c":"code","43189784":"code","40eb1c0b":"code","54552a64":"code","c63db568":"code","f7909491":"code","7ccfd767":"code","c96df186":"code","aba79418":"code","406157c9":"markdown","20f10a2c":"markdown","8351e4c3":"markdown","40dc4911":"markdown","280232bd":"markdown","232afb83":"markdown","08385217":"markdown","494fe0b4":"markdown","7a11bcec":"markdown","b064a581":"markdown","654a9622":"markdown","6a63149f":"markdown","be66c10b":"markdown"},"source":{"250071bf":"# Install Ecco. This assumes you have pytorch installed.\n!pip install ecco\nimport warnings\nwarnings.filterwarnings('ignore')","14abed50":"import ecco\n\n# Load pre-trained language model. Setting 'activations' to True tells Ecco to capture neuron activations.\n# distillgpt is a distilled GPT2 model. You can also try 'gpt2' \nlm = ecco.from_pretrained('distilgpt2', activations=True)","483609bb":"text = \"It was a big\"\n\n# Generate one token\noutput_1 = lm.generate(text, generate=1, do_sample=False)","9a1e5e8d":"text = \"It was a matter of\"\n\n# Generate one token\noutput_1 = lm.generate(text, generate=1, do_sample=False)","b6ac248e":"# Show the top 10 candidate output tokens for position #5. \n# Layer 5 is the last layer in the model.\noutput_1.layer_predictions(position=5, layer=5, topk=10)","7f75f7f9":"# What are the token IDs of the two words?\nlm.tokenizer(\" principle principal\")","a2176dc9":"# Compare the rankings of \"Principle\" and \"Principal\" across layers\noutput_1.rankings_watch(watch=[7989, 10033], position=5)","c288dc81":"text = \"Heathrow airport is located in\"\noutput_2 = lm.generate(text, generate=5, do_sample=False)","6837ea16":"text = \" Heathrow airport is located in the city of\"\noutput_2 = lm.generate(text, generate=1, do_sample=False)","d89b71f1":"# What other tokens were possible to output in place of \"London\"?\noutput_2.layer_predictions(position=9, layer=5, topk=30)","9b24641e":"# Now that the model has selcted the tokens \"London . \\n\"\n# How did each layer rank these tokens during processing?\noutput_2.rankings()","7ae80b8b":"text= \"The cities of the Australia are:\\n1. Sydney\\n2. Adelaide\\n3. Melbourne\\n4.\"\n\noutput_3 = lm.generate(text, generate=20, do_sample=True)","26f5493f":"text= \"The countries of the European Union are:\\n1. Austria\\n2. Belgium\\n3. Bulgaria\\n4.\"\n\noutput_3 = lm.generate(text, generate=20, do_sample=True)","ab87573c":"output_3.rankings()","43189784":"output_3.saliency()","40eb1c0b":"output_3.saliency(style=\"detailed\")","54552a64":"import ecco\nlm = ecco.from_pretrained('distilgpt2', activations=True)","c63db568":"text = ''' Now I ask you: what can be expected of man since he is a being endowed with strange qualities? Shower upon him every earthly blessing, drown him in a sea of happiness, so that nothing but bubbles of bliss can be seen on the surface; give him economic prosperity, such that he should have nothing else to do but sleep, eat cakes and busy himself with the continuation of his species, and even then out of sheer ingratitude, sheer spite, man would play you some nasty trick. He would even risk his cakes and would deliberately desire the most fatal rubbish, the most uneconomical absurdity, simply to introduce into all this positive good sense his fatal fantastic element. It is just his fantastic dreams, his vulgar folly that he will desire to retain, simply in order to prove to himself--as though that were so necessary-- that men still are men and not the keys of a piano, which the laws of nature threaten to control so completely that soon one will be able to desire nothing but by the calendar. And that is not all: even if man really were nothing but a piano-key, even if this were proved to him by natural science and mathematics, even then he would not become reasonable, but would purposely do something perverse out of simple ingratitude, simply to gain his point. And if he does not find means he will contrive destruction and chaos, will contrive sufferings of all sorts, only to gain his point! He will launch a curse upon the world, and as only man can curse (it is his privilege, the primary distinction between him and other animals), may be by his curse alone he will attain his object--that is, convince himself that he is a man and not a piano-key!\n'''\noutput = lm.generate(text, generate=1, do_sample=True)","f7909491":"# Factorize activations in all the layers\nnmf_1 = output.run_nmf(n_components=10) ","7ccfd767":"nmf_1.explore()","c96df186":"# Factorize the activations of only the first layer\nnmf_2 = output.run_nmf(n_components=10, from_layer=0, to_layer=1)","aba79418":"nmf_2.explore()","406157c9":"Ecco is a python library for explaining Natural Language Processing models using interactive visualizations.\n\nIt provides multiple interfaces to aid the explanation and intuition of Transformer-based language models. Read: Interfaces for Explaining Transformer Language Models.\n\nEcco runs inside Jupyter notebooks. It is built on top of pytorch and transformers.\n\nThe library is currently an alpha release of a research project. Not production ready. ","20f10a2c":"### Probing the model's world knowledge \n\nWhat happens if we present the following input sentence to the model:\n\n`The countries of the European Union are:\\n1. Austria\\n2. Belgium\\n3. Bulgaria\\n4. ___________`\n\nNamely, we have these questions:\n* Q. Will the model continue the numbering correctly?\n* Q. Will it succeed in following the formatting?\n* Q. Will it succeed in naming countries? European ones?\n* Q. Will the model \"notice\" the alphabetical order of the list? Will it follow it?\n","8351e4c3":"The `rankings()` visualization view shows us at which layers the model resolved the output token for each position.","40dc4911":"### Detailed saliency view\nWe can see a more detailed view of the saliency values using the detailed view:","280232bd":"This visualization is based on the great visual treatment by nostalgebraist in [Interpreting GPT: the logit lens](https:\/\/www.lesswrong.com\/posts\/AcKRB8wDpdaN6v6ru\/interpreting-gpt-the-logit-lens).","232afb83":"## Exploring World Knowledge And Layer Analysis\nDoes the model \"know\" where Heathrow airport is located? To probe the model, we let's try the input sentence: \n\n`Heathrow airport is located in ____`","08385217":"![68747470733a2f2f61722e706567672e696f2f696d672f6563636f2d6c6f676f2d772d3830302e706e67.png](attachment:68747470733a2f2f61722e706567672e696f2f696d672f6563636f2d6c6f676f2d772d3830302e706e67.png)","494fe0b4":"## Intro to Ecco -- Making Language Models More Transparent\nThis notebook is an intro to [Ecco](https:\/\/www.eccox.io) and is the companion to the video: [Take A Look Inside Language Models With Ecco | PyData Khobar](https:\/\/www.youtube.com\/watch?v=rHrItfNeuh0). \n ","7a11bcec":"This view shows the top 10 candidate tokens, their probability, and their rankings.\n\n### Comparing two token candidates for a single position","b064a581":"# Neuron Factors","654a9622":"## Overview\n\nComplete the sentence:\n`It was a matter of ____`","6a63149f":"While the output is not incorrect, it doesn't really anwer the question we are after. Let's slightly change the input sentence:","be66c10b":"The `saliency()` visualization shows which tokens contributed the most towards generating each output token (using the gradient X Input method):"}}