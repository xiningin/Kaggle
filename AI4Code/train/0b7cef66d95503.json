{"cell_type":{"610b5bf9":"code","ecc534b5":"code","dc21612a":"code","591e8fc3":"code","770321d9":"code","a96750f5":"code","0040c406":"code","bf8ceff6":"code","207f38ba":"code","cf560392":"code","6988150c":"code","b8e821c3":"code","d4642b56":"code","1a7ce252":"code","610a7f20":"code","74d3821e":"markdown","625a667f":"markdown","23c452dc":"markdown","ed2a97e8":"markdown","9b2f5d34":"markdown"},"source":{"610b5bf9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ecc534b5":"train_data = pd.read_csv('\/kaggle\/input\/cat-in-the-dat-ii\/train.csv', index_col='id')","dc21612a":"train_data.head()","591e8fc3":"train_data.info()","770321d9":"for col in train_data.columns:\n    print(col, train_data[col].nunique())","a96750f5":"categorical_columns = [col for col in train_data.columns if train_data[col].dtype == object]\ncolumns_with_high_cardinality = [col for col in categorical_columns if train_data[col].nunique() > 30]\n\ntrain_data.drop(columns_with_high_cardinality, axis=1, inplace=True)","0040c406":"train_data","bf8ceff6":"from sklearn.model_selection import train_test_split\n\ny = train_data['target']\ntrain_data.drop(['target'], axis=1, inplace=True)\ntrain_X, valid_X, train_y, valid_y = train_test_split(train_data, y, train_size=0.8, test_size=0.2, random_state=0)","207f38ba":"for col in train_X.columns:\n    mode = train_X[col].mode()[0]\n    train_X[col].fillna(mode, inplace=True)\n    valid_X[col].fillna(mode, inplace=True)","cf560392":"train_X","6988150c":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\ncolumns_oh_encode = [col for col in train_X.columns if train_X[col].dtype == object]\n\noh_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n\ncolumn_transformer = ColumnTransformer(transformers=[\n    ('onehot', oh_encoder, columns_oh_encode)\n])","b8e821c3":"from xgboost import XGBClassifier\nfrom sklearn.pipeline import Pipeline\n\nmodel = XGBClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n\npipeline = Pipeline(steps=[\n    ('transformer', column_transformer),\n    ('model', model)\n])\n\npipeline.fit(train_X, train_y)\nprediction = pipeline.predict_proba(valid_X)","d4642b56":"from sklearn.metrics import roc_auc_score\n\nscore = roc_auc_score(valid_y, prediction[:,1])\nprint(score)","1a7ce252":"test_data = pd.read_csv('\/kaggle\/input\/cat-in-the-dat-ii\/test.csv', index_col='id')\n\ntest_data.drop(columns_with_high_cardinality, axis=1, inplace=True)\n\n#\nfor col in train_data.columns:\n    mode = train_data[col].mode()[0]\n    train_data[col].fillna(mode, inplace=True)\n    test_data[col].fillna(mode, inplace=True)\n\n#\npipeline.fit(train_data, y)\nprediction_test = pipeline.predict_proba(test_data)","610a7f20":"output = pd.DataFrame({\n    'id': test_data.index,\n    'target': prediction_test[:,1]\n})\noutput.to_csv('submission.csv', index=False)","74d3821e":"Drop columns with high cardinality","625a667f":"XGBClassifier","23c452dc":"Impute missing values","ed2a97e8":"OneHot-encode columns with low cardinality","9b2f5d34":"Predict the test data."}}