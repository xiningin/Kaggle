{"cell_type":{"ba34050a":"code","8973665c":"code","c40df0c4":"code","0a1d9db8":"code","636d2304":"code","c04a24e9":"code","20e3e005":"code","e6de88d9":"code","5a6b0808":"code","e748bcd7":"code","91c1aec4":"code","0ceb18fa":"code","2dd6539a":"code","7a8439d6":"markdown","88c9dbe5":"markdown","08ead741":"markdown","f11c6604":"markdown","efedd70b":"markdown","f5e04db6":"markdown","ee389ba2":"markdown","ffadb7ed":"markdown","1f0b784d":"markdown","a4aeeeb2":"markdown","8dd4c73e":"markdown","3fd2fc62":"markdown","54b04e87":"markdown","b793bbd0":"markdown","7ea743b8":"markdown","81480c33":"markdown","21bee865":"markdown","d783f0e5":"markdown","b9c73661":"markdown","d7f8ccb0":"markdown","edc08603":"markdown","0f44726c":"markdown","6cbc0159":"markdown","86e66671":"markdown","84974487":"markdown","e37c7f64":"markdown","fb39b0dd":"markdown","358c0e87":"markdown","cadf75cf":"markdown","5b1f1efd":"markdown","fdc06e7c":"markdown","095d4f92":"markdown","65533135":"markdown","1d361876":"markdown","19279d74":"markdown","10023ba7":"markdown"},"source":{"ba34050a":"import numpy as np\n\ndata = np.genfromtxt('..\/input\/mystery.dat', delimiter = ',')\n\n","8973665c":"#Create a function to get the output\ndef predicted_values(X, w):\n    # X will be n x (d+1)\n    # w will be (d+1) x 1\n    predictions = np.matmul(X,w) # n x 1\n    return(predictions)","c40df0c4":"#rho computation\ndef rho_compute(y,X,w,j):\n    #y is the response variable\n    #X is the predictor variables matrix\n    #w is the weight vector\n    #j is the feature selector\n    X_k = np.delete(X,j,1) #Remove the j variable i.e. j column\n    w_k = np.delete(w,j) #Remove the weight j\n    predict_k = predicted_values(X_k, w_k)\n    residual = y - predict_k\n    rho_j = np.sum(X[:,j]*residual)\n    return(rho_j)","0a1d9db8":"#z computation for unnormalised features\ndef z_compute(X):\n    z_vector = np.sum(X*X, axis = 0) #Compute sum for each column\n    return(z_vector)","636d2304":"def coordinate_descent(y,X,w,alpha,z,tolerance):\n    max_step = 100.\n    iteration = 0\n    while(max_step > tolerance):\n        iteration += 1\n        #print(\"Iteration (start) : \",iteration)\n        old_weights = np.copy(w)\n        #print(\"\\nOld Weights\\n\",old_weights)\n        for j in range(len(w)): #Take the number of features ie columns\n            rho_j = rho_compute(y,X,w,j)\n            if j == 0: #Intercept is not included with the alpha regularisation\n                w[j] = rho_j\/z[j]\n            elif rho_j < -alpha*len(y):\n                w[j] = (rho_j + (alpha*len(y)))\/z[j]\n            elif rho_j > -alpha*len(y) and rho_j < alpha*len(y):\n                w[j] = 0.\n            elif rho_j > alpha*len(y):\n                w[j] = (rho_j - (alpha*len(y)))\/z[j]\n            else:\n                w[j] = np.NaN\n        #print(\"\\nNew Weights\\n\",w)\n        step_sizes = abs(old_weights - w)\n        #print(\"\\nStep sizes\\n\",step_sizes)\n        max_step = step_sizes.max()\n        #print(\"\\nMax step:\",max_step)\n        \n        \n    return(w, iteration, max_step)","c04a24e9":"#Initialise the data\n\n#101 rows for both input (x) and output (y)\nx = data[:,0:100] # 100 predictors - columns 0 to 99\ny = data[:,100] # 1 response variable - column 100\n\n#Obtain feature matrix by adding column of 1s to input matrix x\nX = np.column_stack((np.ones((x.shape[0],1)),x)) #101 columns\n\n#Initialise weight\/parameter vector, w, to be a zero vector\nw = np.zeros(X.shape[1], dtype = float)\n\n#Pre-compute the z_j term\nz = z_compute(X)\n\n#Set the alpha and tolerance level\nalpha = 0.1\ntolerance = 0.0001\n\n#Obtain the following from the coordinate descent:\n#1. Optimum weight parameter\n#2. Number of iterations\n#3. Maximum step size at the last iteration\n\nw_opt, iterations, max_step = coordinate_descent(y,X,w,alpha,z,tolerance)","20e3e005":"#Print out the optimised weights\nnp.set_printoptions(precision = 3, suppress = True)\nprint(\"Intercept is:\",w_opt[0])\nprint(\"\\nCoefficients are:\\n\",w_opt[1:101])\nprint(\"\\nNumber of iterations is:\",iterations)","e6de88d9":"#Sort to see which are the most important features\nvalues = np.sort(abs(w_opt[1:101]))[::-1] \nindex = np.argsort(abs(w_opt[1:101]))[::-1] + 1 #Add 1 to not show zero-index\n\nnp.set_printoptions(precision = 3, suppress = True)\nprint(np.column_stack((index,values)))","5a6b0808":"#Compare with sklearn's Lasso\nfrom sklearn import linear_model\nmystery = linear_model.Lasso(alpha = 0.1, tol = 0.0001) #alpha is just the regulariser term\nmystery.fit(x,y)\n\nprint(\"sklearn Lasso intercept :\",mystery.intercept_)\nprint(\"\\nsklearn Lasso coefficients :\\n\",mystery.coef_)\nprint(\"\\nsklearn Lasso number of iterations :\",mystery.n_iter_)","e748bcd7":"values = np.sort(abs(mystery.coef_))[::-1]\nindex = np.argsort(abs(mystery.coef_))[::-1] + 1\n\nnp.set_printoptions(precision = 3, suppress = True)\nprint(np.column_stack((index,values)))","91c1aec4":"#Comparison within the same cell\n\nprint(\"sklearn Lasso intercept :\",mystery.intercept_)\nprint(\"\\nsklearn Lasso coefficients :\\n\",mystery.coef_)\nprint(\"\\nsklearn Lasso number of iterations :\",mystery.n_iter_)\nprint(\"\\n------------------------------------------------------------------------\\n\")\nprint(\"Intercept is:\",w_opt[0])\nprint(\"\\nCoefficients are:\\n\",w_opt[1:101])\nprint(\"\\nNumber of iterations is:\",iterations)","0ceb18fa":"w_opt[1:101]","2dd6539a":"print(\"Values are compared to sklearn's Lasso results:\")\nnp.set_printoptions(precision = 10, suppress = False)\nerror = np.zeros(len(w_opt[1:101]), dtype = float)\nfor j in range(len(w_opt[1:101])):\n    if(w_opt[1:101][j] == 0 and mystery.coef_[j] == 0):\n        error[j] = 0.\n    else:\n        error[j] = abs(((w_opt[1:101][j] - mystery.coef_[j])\/mystery.coef_[j])*100)\n\nprint(\"Maximum difference in coefficients (%):\",np.max(error))\nprint(\"Average difference in coefficients (%):\",np.mean(error))\n\nintercept_diff = abs(((w_opt[0] - mystery.intercept_)\/mystery.intercept_)*100)\nprint(\"Difference in intercept (%):\",intercept_diff)\n\n","7a8439d6":"Therefore, the final partial derivative equation is: \n\n$$ \\frac{\ud835\udf15L(w)}{\ud835\udf15w_j} = \\frac{-1}{N}\u03c1_j + \\frac{1}{N}\\tilde{w}_jz_j$$","88c9dbe5":"To minimise the loss function, the partial derivative of the loss function is set to 0.\n\nHence, the optimum solution for the MSE part of the loss functions is:\n\n$$ w_j = \\frac{\u03c1_j}{z_j}$$","08ead741":"No derivative exists at $|w_j| = 0$!! Or rather, there is no gradient for the sharp point of that graph. \n\nFrom here, we have to look into sub-gradients of $|w_j|$:\n![subgradient](https:\/\/i.imgur.com\/Xljou2e.png)\n\nThe sub-gradients of $|w_j|$ are annotated by V which is between -1 and 1.\n\nTo better understand subgradients, refer to [this video at time 4:11](https:\/\/www.coursera.org\/lecture\/ml-regression\/deriving-the-lasso-coordinate-descent-update-6OLyn)","f11c6604":"Hence, the partial derivations of the regulariser term in the loss function are :\n\n$$ \n\u03b1\ud835\udf15_{w_j}\\left(\\sum_{j=1}^d |w_j|\\right) = \u03b1\\frac{\ud835\udf15|w_j|}{\ud835\udf15w_j} = \n    \\begin{cases}\n        -\u03b1 & \\text{when $w_j$ < 0}\\\\\n        [-\u03b1,\u03b1] & \\text{when $w_j$ = 0}\\\\\n        \u03b1 & \\text{when $w_j$ > 0}\\\\\n    \\end{cases}\n$$\n","efedd70b":"## 3. Deriving the coordinate descent used in lasso regression","f5e04db6":"## 6. Conclusion","ee389ba2":"#### Solving the partial derivation of the total Lasso regression loss function","ffadb7ed":"## 5. numpy coding to perform LASSO regression on mystery.dat","1f0b784d":"Attempt to emulate sklearn.linear_model.Lasso() function using numpy only is rather successful. Using sklearn.linear_model.Lasso(), the coding in numpy managed to replicate with results with very minimal error. \n","a4aeeeb2":"#### Deriving the partial derivative of the regulariser term in the loss function","8dd4c73e":"## 2. Refresher on Lasso Regression","3fd2fc62":"The lasso regression loss function is from the edX course is defined as","54b04e87":"Ignoring the regulariser term \u03b1, let the focus be on the mean square error (MSE) of the loss function\n\n$$ L(w) = \\frac{1}{2N}\\left(\\sum_{i=1}^n (y^{(i)} - \\tilde{w} \\cdot \\tilde{x}^{(i)})^2\\right) $$","b793bbd0":"Coordinate descent is an optimisation algorithm which goal is to minimise some function. The diagram below best describes what coordinate descent does:\n\n![coordinate descent](https:\/\/i.imgur.com\/2x4JoZI.png)\nSource: Machine Learning Course from University of Washington on Coursera","7ea743b8":"**The goal is to minimise this loss function by adjusting the weights i.e. the vector w.**","81480c33":"Partially differentiating the above equation with $w_j$ yields:\n\n$$ \\frac{\ud835\udf15L(w)}{\ud835\udf15w_j} = \\frac{1}{2N}\\left(\\sum_{i=1}^n 2(y^{(i)} - \\tilde{w} \\cdot \\tilde{x}^{(i)})(-x_j)\\right) $$\n\nTo apply coordinate descent, the $\\tilde{w} \\cdot \\tilde{x}^{(i)}$ is rewritten: \n\n$$ \\tilde{w} \\cdot \\tilde{x}^{(i)} = \\left(\\sum_{k\u2260j}^n \\tilde{w}_k \\cdot \\tilde{x}^{(i)}_k\\right) + \\tilde{w}_j\\tilde{x}^{(i)}_j $$\n\nExpanding\/Summarising the partial differential equation:\n\n$$ \\frac{\ud835\udf15L(w)}{\ud835\udf15w_j} = \\frac{1}{N}\\left(\\sum_{i=1}^n (-\\tilde{x}^{(i)}_j)(y^{(i)} - \\left(\\sum_{k\u2260j}^n \\tilde{w}_k \\cdot \\tilde{x}^{(i)}_k\\right)) \\right) + \\frac{1}{N}\\tilde{w}_j\\left(\\sum_{i=1}^n \\tilde{x}^{(i)}_j \\right)^2    $$","21bee865":"Objective: To emulate sklearn.linear_model.Lasso() using numpy","d783f0e5":"The partial derivative of the rewritten loss function in section 2 is:\n\n$$\n\\frac{\ud835\udf15L(w)}{\ud835\udf15w_j} = \\frac{-1}{N}\u03c1_j + \\frac{1}{N}\\tilde{w}_jz_j + \n    \\begin{cases}\n        -\u03b1 & \\text{when $w_j$ < 0}\\\\\n        [-\u03b1,\u03b1] & \\text{when $w_j$ = 0}\\\\\n        \u03b1 & \\text{when $w_j$ > 0}\\\\\n    \\end{cases}\n$$","b9c73661":"# Lasso Regression with Coordinate Descent using Numpy","d7f8ccb0":"To partially differentiate the regularise term in the loss function, let's first look at the graph of $|w_j|$\n\n![w_j modulus](https:\/\/i.imgur.com\/h4xN1iD.png)","edc08603":"$$\\tilde{w} = [w_0, w_1, w_2, ..., w_d]$$\n$$\\tilde{x} = [1, x_1, x_2, ..., x_d]$$","0f44726c":"When assessing the convergence, it is up to the user to set how small of the step size it wants the coordinate descent to take to be considered as a convergence.\n\nNote from the illustration above that the coordinate descent takes smaller and smaller steps to reach the minimum value of the cost function.\n\nWe can measure the maximum step size taken during each iteration of the descent, and when the maximum step size is lower than a tolerance level (set by the user), we say that the coordinate descent has converged. ","6cbc0159":"From scikit-learn's description in Section 1.1.3 [here](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html), the lasso regression loss function is defined as ","86e66671":"$$ \\frac{1}{2N}||y - Xw||^2_2 + \u03b1||w||_1 $$","84974487":"Note that $w_0$ is not part of the regulariser term, hence it is updated using the MSE partial derivative term, therefore:\n\n$$ w_0 = \\frac{\u03c1_0}{z_0}$$","e37c7f64":"The equation above can be re-written as follows to aid understanding\n\n$$ L(w) = \\frac{1}{2N}\\left(\\sum_{i=1}^n (y^{(i)} - \\tilde{w} \\cdot \\tilde{x}^{(i)})^2\\right) + \u03b1\\left(\\sum_{j=1}^d |w_j|\\right)$$","fb39b0dd":"#### Deriving the partial derivative of the MSE term in the loss function\n","358c0e87":"The data that will be used in this notebook will be the mystery.dat file from the UCSanDiego ML course on edX. Use the [link here](https:\/\/courses.edx.org\/courses\/course-v1:UCSanDiegoX+DSE220x+1T2019\/courseware\/bcbbed9cb9dc4157ba9e7e608dddb0ac\/0290999d30f74caab3c9e5fb955d29fc\/1?activate_block_id=block-v1%3AUCSanDiegoX%2BDSE220x%2B1T2019%2Btype%40vertical%2Bblock%4015741288db67451991e67be793b369d0) to navigate to the file and download it.\n\nBefore you start running codes on this notebook, place the mystery.dat file in the same directory.","cadf75cf":"\\begin{equation*}\nL(w,b) = \\left(\\sum_{i=1}^n (y^{(i)} - (w \\cdot x^{(i)} + b))^2\\right) + \u03bb||w||_1\n\\end{equation*}\n\n$$||w||_1  = |w_1| + |w_2| + |w_3| + ... $$","5b1f1efd":"Let us define the terms $\u03c1_j$ and $z_j$:\n\n$$ \u03c1_j = \\left(\\sum_{i=1}^n (\\tilde{x}^{(i)}_j)(y^{(i)} - \\left(\\sum_{k\u2260j}^n \\tilde{w}_k \\cdot \\tilde{x}^{(i)}_k\\right)) \\right)$$\n\n$$ z_j = \\left(\\sum_{i=1}^n \\tilde{x}^{(i)}_j \\right)^2$$\n\n","fdc06e7c":"1. Pre-compute:\n$$ z_j = \\left(\\sum_{i=1}^n \\tilde{x}^{(i)}_j \\right)^2$$\n\n2. Initialise the $\\tilde{w}$ to be a zero vector\n\n3. Initialise the maximum step parameter and tolerance level\n> Can just set the maximum step to any large value at the beginning\n\n4. While the algorithm has not converged\n> For j = 0,1,...,D\n>> Compute:\n$$ \u03c1_j = \\left(\\sum_{i=1}^n (\\tilde{x}^{(i)}_j)(y^{(i)} - \\left(\\sum_{k\u2260j}^n \\tilde{w}_k \\cdot \\tilde{x}^{(i)}_k\\right)) \\right)$$\n>> If j = 0\n$$ w_j = \\frac{\u03c1_j}{z_j}$$\n>> Else\n$$ w_j = \n    \\begin{cases}\n        \\frac{\u03c1_j + N\u03b1}{z_j} & \\text{if $\u03c1_j$ < -N\u03b1}\\\\\n        0 & \\text{if -N\u03b1 \u2264 $\u03c1_j$ \u2264 N\u03b1 }\\\\\n        \\frac{\u03c1_j - N\u03b1}{z_j} & \\text{if $\u03c1_j$ > N\u03b1}\\\\\n    \\end{cases}\n$$","095d4f92":"Note that $w_{0}$ will be the intercept which is b in the loss function from the edX course. Besides that, \u03b1 is the equivalent to \u03bb.\n\nThe difference between the edX's loss function and scikit's is the appearance of 1\/2N in front of the first summation term. N represents the number of observations\/samples i.e. number of rows in the dataset.\n\n**This loss function is optimised using coordinate descent**","65533135":"From the illustration above, the contours represent the value of the loss function where the value at the center is the lowest (imagine a convex function). Coordinate descent works by choosing one feature to optimise while keeping the other features the same.\n\n**Coordinate descent**:\n\nInitialise w = 0\n>while not converged \n>>pick a coordinate j, update j by minimising the loss function by altering the value of j","1d361876":"## 4. The coordinate descent algorithm used here","19279d74":"When setting the partial derivative to 0, there are 3 cases to consider:\n\n**Case 1 :** $w_j$ < 0\n$$\n\\frac{-1}{N}\u03c1_j + \\frac{1}{N}\\tilde{w}_jz_j - \u03b1 = 0\n$$\n\n$$\nw_j = \\frac{\u03c1_j + N\u03b1}{z_j}\n$$\n\nFor $w_j$ < 0\n$$\n\u03c1_j < -N\u03b1\n$$\n\n**Case 2 :** $w_j$ = 0\n$$\n\\frac{-1}{N}\u03c1_j + \\frac{1}{N}\\tilde{w}_jz_j + [-\u03b1,\u03b1] = 0\n$$\n\n$$\nw_j = [\\frac{\u03c1_j - N\u03b1}{z_j},\\frac{\u03c1_j + N\u03b1}{z_j}]\n$$\n\nFor $w_j$ = 0, need $[\\frac{\u03c1_j - N\u03b1}{z_j},\\frac{\u03c1_j + N\u03b1}{z_j}]$ to contain 0, hence\n$$\n-N\u03b1 \u2264 \u03c1_j \u2264 N\u03b1 \n$$\n\n**Case 3 :** $w_j$ > 0\n$$\n\\frac{-1}{N}\u03c1_j + \\frac{1}{N}\\tilde{w}_jz_j + \u03b1 = 0\n$$\n\n$$\nw_j = \\frac{\u03c1_j - N\u03b1}{z_j}\n$$\n\nFor $w_j$ > 0\n$$\n\u03c1_j > N\u03b1\n$$","10023ba7":"## 1. Set up notebook and load data set"}}