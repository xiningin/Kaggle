{"cell_type":{"87dec57e":"code","b5360d08":"code","bcacc8d8":"code","0a10bd44":"code","650ba75e":"code","f9ffb63d":"code","e6cff1ee":"code","0f2b9145":"code","564af6a2":"code","4adbf2a0":"code","195a093a":"code","99e87559":"code","d1fcc864":"code","9ab1f454":"code","3e17e326":"code","5256961e":"code","f7d1e345":"code","11f8bd03":"code","76120b68":"code","70d412aa":"code","ed030149":"code","08dc6168":"code","eac44f41":"code","83a82058":"code","84fe8e13":"code","af0aac69":"code","c3b5e5ae":"code","ff8f3a75":"code","9d5e3ba0":"code","aacabdb9":"code","cae17d3b":"code","a0bb91f9":"code","ee251c77":"code","988fcf21":"code","6c0fbab9":"code","79bffb7e":"code","36243de1":"code","cf215019":"code","acdcb369":"code","f994c210":"code","7e9eceb2":"code","d379b463":"code","55559f7c":"code","7bbf00d4":"code","7d0ce6ca":"code","aa476c01":"code","b545035c":"code","cdca347f":"code","731fff3d":"code","b317926f":"code","df0108f9":"code","c7cb06f7":"code","6143001b":"code","07de138a":"code","3cb5dff2":"code","c75e4444":"code","17d150bb":"code","d6fcc42d":"code","f2d8333d":"code","06c64006":"code","c593dcd5":"code","c8ee21de":"markdown","2b93a21b":"markdown","a9e9d9c6":"markdown","7fcd3b0b":"markdown","ff470608":"markdown","8f5695f6":"markdown","0f8a16aa":"markdown","2767492a":"markdown","62071e9c":"markdown","f0de3486":"markdown","95aa5f79":"markdown","4b0276d9":"markdown","d5e637bd":"markdown","42633bd5":"markdown","e71a749f":"markdown","f1e727b9":"markdown","d68c8ee9":"markdown","574aaa44":"markdown"},"source":{"87dec57e":"# Data manipulation and plotting modules\nimport numpy as np\nimport pandas as pd","b5360d08":"# Data pre-processing\n# z = (x-mean)\/stdev\nfrom sklearn.preprocessing import RobustScaler","bcacc8d8":"# Working with imbalanced data\n# http:\/\/contrib.scikit-learn.org\/imbalanced-learn\/stable\/generated\/imblearn.over_sampling.SMOTE.html\n# Check imblearn version number as:\n#   import imblearn;  imblearn.__version__\n# Install as:  conda install -c conda-forge imbalanced-learn\nfrom imblearn.over_sampling import SMOTE, ADASYN\n\n# Also using NearMiss for undersampling\nfrom imblearn.under_sampling import NearMiss","0a10bd44":"from sklearn.model_selection import StratifiedKFold","650ba75e":"# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier","f9ffb63d":"# libraries for ROC graphs & metrics\nimport scikitplot as skplt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import average_precision_score\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score, auc, roc_curve, roc_auc_score, accuracy_score, classification_report, precision_recall_curve\nfrom sklearn.model_selection import cross_val_score","e6cff1ee":"# Modules for graph plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","0f2b9145":"# Importing GridSearchCV\nfrom sklearn.model_selection import GridSearchCV, train_test_split","564af6a2":"# Importing RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","4adbf2a0":"# Importing some supporting modules\nimport time\nimport random\nimport os\nfrom scipy.stats import uniform\nimport warnings\nwarnings.filterwarnings(\"ignore\")","195a093a":"print(os.listdir(\"..\/input\/\"))\ndata = pd.read_csv(\"..\/input\/creditcard.csv\")","99e87559":"# Explore Data\ndata.info()","d1fcc864":"# Data Contents\ndata.head(10)","9ab1f454":"data.describe()","3e17e326":"# Class is the target for this problem, \n# 1 indicates fraudulent transaction \n# and 0 as normal transaction\n# lets check if data is balanced\n\nprint(data.Class.value_counts())","5256961e":"# Check if any column in dataset is all zeroes\n# We need to remove these columns\n# Sum each row, and check in which case sum is 0\n# axis = 1 ==> Across columns\n\nx = np.sum(data, axis = 1)\nv = x.index[x == 0]     # Get index of the row which meets a condition\n\nif len(v) > 0:\n    print(\"Column Index: \", v)\nelse:\n    print(\"Good - No Column in Dataset has all rows zeroes.\")","f7d1e345":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = data['Amount'].values\ntime_val = data['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])","11f8bd03":"# Since most of the data has already been scaled we should scale the columns that are left to scale (Amount and Time)\n# RobustScaler is less prone to outliers.\n\nrob_scaler = RobustScaler()\n\ndata['scaled_amount'] = rob_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\ndata['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n\ndata.drop(['Time','Amount'], axis=1, inplace=True)","76120b68":"# Lets check the class values where 1 indicates frauds and 0 shows normal transactions.\nprint('No Frauds: Label 0 - {0} % of the dataset.'.format(round(data['Class'].value_counts()[0]\/len(data) * 100,2)))\nprint('Frauds   : Label 1 - {0} % of the dataset.'.format(round(data['Class'].value_counts()[1]\/len(data) * 100,2)))","70d412aa":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot(x = 'Class', data = data, palette= colors)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)\nplt.show()","ed030149":"# Separate data set into Original_X and Original_y \nOriginal_X = data.drop('Class', axis=1)\nOriginal_y = data['Class']\n\nprint(\"Original X Shape: \", Original_X.shape)\nprint(\"\\nOriginal  Y Shape: \", Original_y.shape)\nprint(\"\\nOriginal X Columns: \\n\", Original_X.columns)\nprint(\"\\n\\n\")\nOriginal_X.head()","08dc6168":"def printvalues(section, X_train, X_test, y_train, y_test, X):\n    print(section, \"_X Train Shape: \", X_train.shape)\n    print(section, \"_y Train Shape: \", y_train.shape)\n    print(section, \"_X Test Shape: \", X_test.shape)\n    print(section, \"_y Test Shape: \", y_test.shape)\n    print(\"\\n\\n\", section, \" data columns: \\n\")\n    for i in range(0, len(X.columns)):\n        print(i, \" \", X.columns[i])\n        \ndef printscore(model, y_test, y_pred):\n    # Best Score and best parameters\n    print(\"Best score Train:\", model.best_score_ * 100)\n    print(\"Best parameter set :\", model.best_params_)\n\n    # Check Accuracy\n    print(\"Accuracy: \", accuracy_score(y_test, y_pred) * 100)\n    print(\"Precision: \", precision_score(y_test, y_pred))\n    print(\"Recall: \", recall_score(y_test, y_pred))\n    print(\"F1: \", f1_score(y_test, y_pred))\n    print(\"ROC_AUC: \", roc_auc_score(y_test, y_pred))","eac44f41":"# Modelling on the given data without using under \/ over sampling first.\n\nOriginal_X_train, Original_X_test, Original_y_train, Original_y_test = train_test_split(Original_X, Original_y, test_size=0.25, random_state=42)\n\nprintvalues(\"Original\", Original_X_train, Original_X_test, Original_y_train, Original_y_test, Original_X)","83a82058":"# Since our classes are highly skewed we should make them equivalent in order to have a normal distribution of the classes.\n\n# Lets shuffle the data before creating the subsamples\n\ndata = data.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_data = data.loc[data['Class'] == 1]\nnon_fraud_data = data.loc[data['Class'] == 0][:492]\n\nnormal_distributed_data = pd.concat([fraud_data, non_fraud_data])\n\n# Shuffle dataframe rows\nnew_data = normal_distributed_data.sample(frac=1, random_state=42)\nprint(\"\\nNew Data Shape\", new_data.shape,\"\\n\\n\")\nnew_data.head()","84fe8e13":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot(x = 'Class', data = new_data, palette= colors)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)\nplt.show()","af0aac69":"# Undersampling before cross validating (prone to overfit)\nnew_X = new_data.drop('Class', axis=1)\nnew_y = new_data['Class']\n\n# This is explicitly used for undersampling.\nUnder_X_train, Under_X_test, Under_y_train, Under_y_test = train_test_split(new_X, new_y, test_size=0.25, random_state=42)\n\nprintvalues(\"Under\", Under_X_train, Under_X_test, Under_y_train, Under_y_test, new_X)","c3b5e5ae":"nr = NearMiss()\nNearMiss_X, NearMiss_y = nr.fit_sample(Original_X, Original_y)\n\nNearMiss_X_train, NearMiss_X_test, NearMiss_y_train, NearMiss_y_test = train_test_split(NearMiss_X, NearMiss_y, test_size=0.25, random_state=42)\n\n#let us check the amount of records in each category\nprint(\"NearMiss_X_train Shape: \", NearMiss_X_train.shape)\nprint(\"NearMiss_y_train Shape: \", NearMiss_y_train.shape)","ff8f3a75":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot(x = NearMiss_y_train, palette= colors)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)\nplt.show()","9d5e3ba0":"sm = SMOTE(sampling_strategy='minority')\nSMOTE_X, SMOTE_y = sm.fit_sample(Original_X, Original_y)\n\nSMOTE_X_train, SMOTE_X_test, SMOTE_y_train, SMOTE_y_test = train_test_split(SMOTE_X, SMOTE_y, test_size=0.25, random_state=42)\n\n#let us check the amount of records in each category\nprint(\"SMOTE_X_train Shape: \", SMOTE_X_train.shape)\nprint(\"SMOTE_y_train Shape: \", SMOTE_y_train.shape)\nprint(\"SMOTE_Classes: \", np.bincount(SMOTE_y_train))","aacabdb9":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot(x = SMOTE_y_train, palette= colors)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)\nplt.show()","cae17d3b":"ad = ADASYN(sampling_strategy='minority')\nADASYN_X, ADASYN_y = ad.fit_sample(Original_X, Original_y)\n\nADASYN_X_train, ADASYN_X_test, ADASYN_y_train, ADASYN_y_test = train_test_split(ADASYN_X, ADASYN_y, test_size=0.25, random_state=42)\n\n#let us check the amount of records in each category\nprint(\"ADASYN_X_train Shape: \", ADASYN_X_train.shape)\nprint(\"ADASYN_y_train Shape: \", ADASYN_y_train.shape)\nprint(\"ADASYN_Classes: \", np.bincount(ADASYN_y_train))","a0bb91f9":"colors = [\"#0101DF\", \"#DF0101\"]\n\nsns.countplot(x = ADASYN_y_train, palette= colors)\nplt.title('Class Distributions \\n (0: No Fraud || 1: Fraud)', fontsize=14)\nplt.show()","ee251c77":"def cGridSearchCV(X_train, y_train):\n    \n    parameters = {'learning_rate': [1], 'n_estimators': [100], 'max_depth': [1]}\n    workers = 6\n    \n    clf = GridSearchCV(XGBClassifier(silent = False, n_jobs= workers),\n                       parameters,\n                       n_jobs = workers,\n                       cv = 3,\n                       verbose = 1,\n                       scoring = ['accuracy', 'roc_auc'],\n                       refit = 'roc_auc'\n                       )\n    clf.fit(X_train, y_train)\n    return clf","988fcf21":"##################### Randomized Search #################\n\ndef cRandomizedSearchCV(X_train, y_train):\n\n    parameters = {'learning_rate': [1], 'n_estimators': [10], 'max_depth': [1]}\n    workers = 6\n\n    clf = RandomizedSearchCV(XGBClassifier(silent = False, n_jobs=workers),\n                            param_distributions=parameters,\n                            scoring= ['roc_auc', 'accuracy'],\n                            n_iter=15,          # Max combination of parameter to try. Default = 10\n                            verbose = 1,\n                            refit = 'roc_auc',\n                            n_jobs = workers,       # Use parallel cpu threads\n                            cv = 3               # No of folds, so n_iter * cv combinations\n                           )\n    clf.fit(X_train, y_train)\n    return clf","6c0fbab9":"def MiscClassifiers(X_train, y_train):\n    # Let's implement simple classifiers\n\n    classifiers = {\n        \"LogisiticRegression\": LogisticRegression(n_jobs = 6),\n        \"KNearest\": KNeighborsClassifier(n_jobs = 6),\n        \"DecisionTreeClassifier\": DecisionTreeClassifier(max_depth = 2)\n    }\n\n    # 100% accuracy score, these could be due to overfitting.\n    # we shall use under and over sampling in next few sections to fix this.\n\n    for key, classifier in classifiers.items():\n        classifier.fit(X_train, y_train)\n        training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n        print(\"Classifiers: \", classifier.__class__.__name__, \"has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","79bffb7e":"MiscClassifiers(Original_X_train, Original_y_train)","36243de1":"GridSearch_result_without_sampling = cGridSearchCV(Original_X_train, Original_y_train)","cf215019":"# Best Score and best parameters\nGridSearch_y_pred_without_sampling = GridSearch_result_without_sampling.predict(Original_X_test)\n\nprintscore(GridSearch_result_without_sampling, Original_y_test, GridSearch_y_pred_without_sampling)","acdcb369":"RandomizedSearch_result_without_sampling = cRandomizedSearchCV(Original_X_train, Original_y_train)","f994c210":"# Best Score and best parameters\nRandomizedSearch_y_pred_without_sampling = RandomizedSearch_result_without_sampling.predict(Original_X_test)\n\nprintscore(RandomizedSearch_result_without_sampling, Original_y_test, RandomizedSearch_y_pred_without_sampling)","7e9eceb2":"MiscClassifiers(Under_X_train, Under_y_train)","d379b463":"GridSearch_result_under_sampling = cGridSearchCV(Under_X_train, Under_y_train)","55559f7c":"# Best Score and best parameters\nGridSearch_y_pred_under_sampling = GridSearch_result_under_sampling.predict(Under_X_test)\n\nprintscore(GridSearch_result_under_sampling, Under_y_test, GridSearch_y_pred_under_sampling)","7bbf00d4":"RandomizedSearch_result_under_sampling = cRandomizedSearchCV(Under_X_train, Under_y_train)","7d0ce6ca":"# Best Score and best parameters\nRandomizedSearch_y_pred_under_sampling = RandomizedSearch_result_under_sampling.predict(Under_X_test)\n\nprintscore(RandomizedSearch_result_under_sampling, Under_y_test, RandomizedSearch_y_pred_under_sampling)","aa476c01":"MiscClassifiers(NearMiss_X_train, NearMiss_y_train)","b545035c":"GridSearch_result_NearMiss = cGridSearchCV(NearMiss_X_train, NearMiss_y_train)\n\n# Best Score and best parameters\nGridSearch_y_pred_NearMiss = GridSearch_result_NearMiss.predict(NearMiss_X_test)\n\nprintscore(GridSearch_result_NearMiss, NearMiss_y_test, GridSearch_y_pred_NearMiss)","cdca347f":"GridSearch_result_NearMiss = cGridSearchCV(NearMiss_X_train, NearMiss_y_train)\n\n# Best Score and best parameters\nGridSearch_y_pred_NearMiss = GridSearch_result_NearMiss.predict(NearMiss_X_test)\n\nprintscore(GridSearch_result_NearMiss, NearMiss_y_test, GridSearch_y_pred_NearMiss)","731fff3d":"RandomizedSearch_result_NearMiss = cRandomizedSearchCV(NearMiss_X_train, NearMiss_y_train)\ntime.sleep(2)\n\n# Best Score and best parameters\nRandomizedSearch_y_pred_NearMiss = RandomizedSearch_result_NearMiss.predict(NearMiss_X_test)\n\nprintscore(RandomizedSearch_result_NearMiss, NearMiss_y_test, RandomizedSearch_y_pred_NearMiss)","b317926f":"MiscClassifiers(SMOTE_X_train, SMOTE_y_train)","df0108f9":"sss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\n# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# Implementing SMOTE Technique \n# Cross Validating parameters\n\nrand_log_reg = \"\"\nbest_est = \"\"\nprediction = \"\"\n\n\nparameters = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10], 'learning_rate': [1], 'n_estimators': [10], 'max_depth': [1]}\n\nrand_log_reg = RandomizedSearchCV(XGBClassifier(silent = False, n_jobs=6),\n                            param_distributions=parameters,\n                            scoring= ['roc_auc', 'accuracy'],\n                            n_iter=15,          # Max combination of parameter to try. Default = 10\n                            verbose = 1,\n                            refit = 'roc_auc',\n                            cv = 3,\n                            n_jobs = 6, \n                            )\n\nfor train, test in sss.split(SMOTE_X, SMOTE_y):\n    rand_log_reg.fit(SMOTE_X[train], SMOTE_y[train])    \n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(SMOTE_X[test])\n    \n    accuracy_lst.append(best_est.score(SMOTE_X[test], SMOTE_y[test]))\n    precision_lst.append(precision_score(SMOTE_y[test], prediction))\n    recall_lst.append(recall_score(SMOTE_y[test], prediction))\n    f1_lst.append(f1_score(SMOTE_y[test], prediction))\n    auc_lst.append(roc_auc_score(SMOTE_y[test], prediction))\n    \nprint('---' * 10)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('---' * 10)","c7cb06f7":"GridSearch_result_SMOTE = cGridSearchCV(SMOTE_X_train, SMOTE_y_train)\n\n# Best Score and best parameters\nGridSearch_y_pred_SMOTE = GridSearch_result_SMOTE.predict(SMOTE_X_test)\n\nprintscore(GridSearch_result_SMOTE, SMOTE_y_test, GridSearch_y_pred_SMOTE)","6143001b":"RandomizedSearch_result_SMOTE = cRandomizedSearchCV(SMOTE_X_train, SMOTE_y_train)\n\n# Best Score and best parameters\nRandomizedSearch_y_pred_SMOTE = RandomizedSearch_result_SMOTE.predict(SMOTE_X_test)\n\nprintscore(RandomizedSearch_result_SMOTE, SMOTE_y_test, RandomizedSearch_y_pred_SMOTE)","07de138a":"MiscClassifiers(ADASYN_X_train, ADASYN_y_train)","3cb5dff2":"# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# Implementing ADASYN Technique \n# Cross Validating parameters\n\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10], 'learning_rate': [1], 'n_estimators': [10], 'max_depth': [1]}\n\nADASYN_rand_log_reg = RandomizedSearchCV(XGBClassifier(silent = False, n_jobs=6),\n                            param_distributions=parameters,\n                            scoring= ['roc_auc', 'accuracy'],\n                            n_iter=15,\n                            verbose = 1,\n                            refit = 'roc_auc',\n                            cv = 3,\n                            n_jobs = 6, \n                            )\n\nfor train, test in sss.split(ADASYN_X_train, ADASYN_y_train):\n    ADASYN_rand_log_reg.fit(ADASYN_X_train[train], ADASYN_y_train[train])    \n    best_est = ADASYN_rand_log_reg.best_estimator_\n    ADASYN_prediction = best_est.predict(ADASYN_X_train[test])\n    \n    accuracy_lst.append(best_est.score(ADASYN_X_train[test], ADASYN_y_train[test]))\n    precision_lst.append(precision_score(ADASYN_y_train[test], ADASYN_prediction))\n    recall_lst.append(recall_score(ADASYN_y_train[test], ADASYN_prediction))\n    f1_lst.append(f1_score(ADASYN_y_train[test], ADASYN_prediction))\n    auc_lst.append(roc_auc_score(ADASYN_y_train[test], ADASYN_prediction))\n    \nprint('---' * 10)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('---' * 10)","c75e4444":"GridSearch_result_ADASYN = cGridSearchCV(ADASYN_X_train, ADASYN_y_train)\n\n# Best Score and best parameters\nGridSearch_y_pred_ADASYN = GridSearch_result_ADASYN.predict(ADASYN_X_test)\n\nprintscore(GridSearch_result_ADASYN, ADASYN_y_test, GridSearch_y_pred_ADASYN)","17d150bb":"RandomizedSearch_result_ADASYN = cRandomizedSearchCV(ADASYN_X_train, ADASYN_y_train)\n\n# Best Score and best parameters\nRandomizedSearch_y_pred_ADASYN = RandomizedSearch_result_ADASYN.predict(ADASYN_X_test)\n\nprintscore(RandomizedSearch_result_ADASYN, ADASYN_y_test, RandomizedSearch_y_pred_ADASYN)","d6fcc42d":"def conf_matrix(axis, y_test, y_pred, title):\n    ##### Confusion matrix\n    cm = confusion_matrix(y_test,y_pred)\n    axis.set_title(title)\n    sns.heatmap(cm, annot=True, fmt='g', annot_kws={\"size\": 14}, ax = axis)","f2d8333d":"fig, axis = plt.subplots(5,2,figsize=(10,10))\n\nconf_matrix(axis[0,0], Original_y_test, GridSearch_y_pred_without_sampling, \"GridSearcCV\\nWithout Sampling\")\nconf_matrix(axis[0,1], Original_y_test, RandomizedSearch_y_pred_without_sampling, \"RandomizedSearcCV\\nWithout Sampling\")\n\nconf_matrix(axis[1,0], Under_y_test, GridSearch_y_pred_under_sampling, \"GridSearcCV\\nUnder Sampling\")\nconf_matrix(axis[1,1], Under_y_test, RandomizedSearch_y_pred_under_sampling, \"RandomizedSearcCV\\nUnder Sampling\")\n\nconf_matrix(axis[2,0], NearMiss_y_test, GridSearch_y_pred_NearMiss, \"GridSearcCV\\nNearMiss Sampling\")\nconf_matrix(axis[2,1], NearMiss_y_test, RandomizedSearch_y_pred_NearMiss, \"RandomizedSearcCV\\nNearMiss Sampling\")\n\nconf_matrix(axis[3,0], SMOTE_y_test, GridSearch_y_pred_SMOTE, \"GridSearcCV\\nSMOTE Sampling\")\nconf_matrix(axis[3,1], SMOTE_y_test, RandomizedSearch_y_pred_SMOTE, \"RandomizedSearcCV\\nSMOTE Sampling\")\n\nconf_matrix(axis[4,0], ADASYN_y_test, GridSearch_y_pred_ADASYN, \"GridSearcCV\\nADASYN Sampling\")\nconf_matrix(axis[4,1], ADASYN_y_test, RandomizedSearch_y_pred_ADASYN, \"RandomizedSearcCV\\nADASYN Sampling\")\n\nfig.tight_layout()","06c64006":"def ROC_Curve_Graph(axis, model, X_test, y_test, title):\n    # probbaility of occurrence of each class\n    y_pred_prob = model.predict_proba(X_test)\n\n    #print(\"y_pred_prob shape : \", y_pred_prob.shape)\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[: , 0], pos_label = 0)\n\n    # Plot the ROC curve\n    fig = plt.figure(figsize=(10,10))          # Create window frame\n    ax = axis   # Create axes\n    ax.plot(fpr, tpr)           # Plot on the axes\n    # Also connect diagonals\n    ax.plot([0, 1], [0, 1], ls=\"--\")   # Dashed diagonal line\n    # Labels etc\n    ax.set_xlabel('False Positive Rate')  # Final plot decorations\n    ax.set_ylabel('True Positive Rate')\n    ax.set_title(title)\n\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.0])","c593dcd5":"fig, axis = plt.subplots(5,2,figsize=(10,10))\n\nROC_Curve_Graph(axis[0, 0], GridSearch_result_without_sampling, Original_X_test, Original_y_test, \"GridSearcCV\\nWithout Sampling\")\nROC_Curve_Graph(axis[0, 1], RandomizedSearch_result_without_sampling, Original_X_test, Original_y_test, \"RandomizedSearcCV\\nWithout Sampling\")\n\nROC_Curve_Graph(axis[1, 0], GridSearch_result_under_sampling, Under_X_test, Under_y_test, \"GridSearcCV\\nUnder Sampling\")\nROC_Curve_Graph(axis[1, 1], RandomizedSearch_result_under_sampling, Under_X_test, Under_y_test, \"RandomizedSearcCV\\nUnder Sampling\")\n\nROC_Curve_Graph(axis[2, 0], GridSearch_result_NearMiss, NearMiss_X_test, NearMiss_y_test, \"GridSearcCV\\nNearMiss Sampling\")\nROC_Curve_Graph(axis[2, 1], RandomizedSearch_result_NearMiss, NearMiss_X_test, NearMiss_y_test, \"RandomizedSearcCV\\nNearMiss Sampling\")\n\nROC_Curve_Graph(axis[3, 0], GridSearch_result_SMOTE, SMOTE_X_test, SMOTE_y_test, \"GridSearcCV\\nSMOTE Sampling\")\nROC_Curve_Graph(axis[3, 1], RandomizedSearch_result_SMOTE, SMOTE_X_test, SMOTE_y_test, \"RandomizedSearcCV\\nSMOTE Sampling\")\n\nROC_Curve_Graph(axis[4, 0], GridSearch_result_ADASYN, ADASYN_X_test, ADASYN_y_test, \"GridSearcCV\\nADASYN Sampling\")\nROC_Curve_Graph(axis[4, 1], RandomizedSearch_result_ADASYN, ADASYN_X_test, ADASYN_y_test, \"RandomizedSearcCV\\nADASYN Sampling\")\n    \nfig.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","c8ee21de":"#### Multiple Classifiers","2b93a21b":"### Under Sampling - NearMiss","a9e9d9c6":"### Over Sampling based on ADASYN\n\n#### ADASYN\n\nADAptive SYNthetic (ADASYN) is based on the idea of adaptively generating minority data samples according to their distributions using K nearest neighbor. The algorithm adaptively updates the distribution and there are no assumptions made for the underlying distribution of the data.  The algorithm uses Euclidean distance for KNN Algorithm. The key difference between ADASYN and SMOTE is that the former uses a density distribution, as a criterion to automatically decide the number of synthetic samples that must be generated for each minority sample by adaptively changing the weights of the different minority samples to compensate for the skewed distributions. The latter generates the same number of synthetic samples for each original minority sample.\n\nReference: <a href=\"http:\/\/sci2s.ugr.es\/keel\/pdf\/algorithm\/congreso\/2008-He-ieee.pdf\">Article<\/a>\n","7fcd3b0b":"### Code Templates for reuse in next below sections","ff470608":"####### Dropping Time and Amount Columns as they are not required.\ndata.drop(columns = ['Time', 'Amount'], inplace = True)\nprint(data.shape)","8f5695f6":"### Preparing data for over sampling based on SMOTE and ADASYN","0f8a16aa":"### Under Sampling based on NearMiss","2767492a":"# Credit Card Fraud Detection\n\n#### Context\n\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\n#### Data Contents\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. ","62071e9c":"### GridSearchCV - Under Sampling","f0de3486":"### RandomizedSearchCV - Under Sampling","95aa5f79":"### Preparing data for under sampling","4b0276d9":"\n### Ending this Kernel here","d5e637bd":"### Plotting ROC Curve","42633bd5":"### Plotting Confusion Matrix","e71a749f":"#### GridSearchCV - Under Sampling - NearMiss","f1e727b9":"### Over Sampling based on SMOTE\n\n#### SMOTE\n\nSynthetic Minority Over sampling Technique (SMOTE) algorithm applies KNN approach where it selects K nearest neighbors, joins them and creates the synthetic samples in the space. The algorithm takes the feature vectors and its nearest neighbors, computes the distance between these vectors. The difference is multiplied by random number between (0, 1) and it is added back to feature. SMOTE algorithm is a pioneer algorithm and many other algorithms are derived from SMOTE.\n\nReference: <a href=\"https:\/\/www.jair.org\/media\/953\/live-953-2037-jair.pdf\">Article<\/a>\n","d68c8ee9":"#### Multiple Classifiers","574aaa44":"### Modelling with Random Under-Sampling"}}