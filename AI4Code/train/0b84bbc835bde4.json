{"cell_type":{"905e84b8":"code","79cd1df3":"code","5333ed19":"code","f28c4d3b":"code","53a05a0e":"code","cb3e2d9d":"code","40b20f95":"code","e084d262":"code","86e66ee5":"code","f4354591":"code","5e942727":"code","839abd27":"code","94d77b6a":"code","717f6c21":"code","eed05c9f":"code","c53521a6":"code","692db25a":"code","3110f190":"code","112da1af":"code","c5f876ce":"code","ec23a34c":"code","95cdfa49":"code","44dfea78":"code","c7ea737d":"code","ce02f8ec":"code","23937cf5":"code","8baee88e":"code","b1ee6608":"code","7ff7da17":"code","8399c319":"markdown","4fb98388":"markdown","91341fd0":"markdown"},"source":{"905e84b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","79cd1df3":"from PIL import Image\nfrom sklearn.model_selection import train_test_split\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nimport torch as pt \nimport tensorflow as tf\nfrom torch.utils.data import Dataset, DataLoader , Subset, ConcatDataset, ChainDataset\nfrom torch.nn import functional as F\nfrom torchvision.datasets import MNIST, FashionMNIST","5333ed19":"## If this give the HTTP error just run it again after 15-30 seconds. There's some server issue in the MNIST\/FMNIST directory.\n## This takes around 4-5 mins to download the data\n\n(MNIST_train_data, MNIST_train_labels), (MNIST_test_data, MNIST_test_labels) = tf.keras.datasets.mnist.load_data(path=\"mnist.npz\")\n(Fashion_MNIST_train_data,Fashion_MNIST_train_labels),(Fashion_MNIST_test_data,Fashion_MNIST_test_labels) = tf.keras.datasets.fashion_mnist.load_data()\n    \nFashion_MNIST_all_data = Fashion_MNIST_test_data #np.concatenate([Fashion_MNIST_train_data,Fashion_MNIST_test_data]).astype(np.float32)\nFashion_MNIST_all_labels = Fashion_MNIST_test_labels #np.concatenate([Fashion_MNIST_train_labels,Fashion_MNIST_test_labels]) \n\nMNIST_train_data = MNIST_train_data\/255\nMNIST_test_data = MNIST_test_data\/255\nFashion_MNIST_all_data = Fashion_MNIST_all_data\/255 ","f28c4d3b":"import tensorflow as tf\n\n## Tensorflow model\n\n\nclass Model():\n    def __init__(self, L2_WEIGHT_DECAY = 1e-6):\n        self.inputlayer = tf.keras.layers.Input(shape=(28,28,1),name='initial_input')\n        self.conv_1 = tf.keras.layers.Conv2D(4,5)\n        self.pool_1 = tf.keras.layers.MaxPool2D(2)\n        self.pool_2 = tf.keras.layers.MaxPool2D(2)\n        self.conv_2 = tf.keras.layers.Conv2D(12,5)\n        self.flatten = tf.keras.layers.Flatten()\n        self.dense_1 = tf.keras.layers.Dense(120,activation = 'relu')\n        self.dense_2 = tf.keras.layers.Dense(64,activation = 'relu')\n        self.dense_3 = tf.keras.layers.Dense(10,activation = 'softmax')\n        \n    def get_train_model(self):\n        \n        inputs = self.inputlayer\n        x = self.conv_1(inputs)\n        x = self.pool_1(x)\n        x = self.conv_2(x)\n        x = self.pool_2(x)\n        x = self.flatten(x)\n        x = self.dense_1(x)\n        x = self.dense_2(x)\n        x = self.dense_3(x)\n        \n        return tf.keras.Model(inputs,x)\n    \n    def get_model_with_penulitmate_layer(self):\n        \n        inputs = self.inputlayer\n        x = self.conv_1(inputs)\n        x = self.pool_1(x)\n        x = self.conv_2(x)\n        x = self.pool_2(x)\n        x = self.flatten(x)\n        x = self.dense_1(x)\n        x = self.dense_2(x)\n        y = self.dense_3(x)\n        \n        return tf.keras.Model(inputs,[y,x])","53a05a0e":"class eulid_dist(tf.keras.layers.Layer):\n    def __init__(self, output_classes, weight_decay):\n        super(eulid_dist, self).__init__()\n        self.output_classes = output_classes\n        self.weight_decay = weight_decay\n        \n    def build(self, input_shape):\n        self.kernel = self.add_weight(\"kernel\",\n                                  shape=[int(input_shape[-1]),\n                                         self.output_classes],\n                            regularizer=tf.keras.regularizers.l2(self.weight_decay))\n        \n    def call(self, x):\n        return -tf.norm(tf.expand_dims(x,axis=-1)-self.kernel, ord='euclidean',axis=-2)","cb3e2d9d":"class cosine(tf.keras.layers.Layer):\n    def __init__(self, output_classes, weight_decay):\n        super(cosine, self).__init__()\n        self.output_classes = output_classes\n        self.weight_decay = weight_decay\n        \n    def build(self, input_shape):\n        self.kernel = self.add_weight(\"kernel\",\n                                  shape=[int(input_shape[-1]),\n                                         self.output_classes],\n                            regularizer=tf.keras.regularizers.l2(self.weight_decay))\n        \n    def call(self, x):\n        kernel_norm = tf.norm(self.kernel, ord='euclidean',axis=0)\n        input_norm = tf.norm(x, ord='euclidean',axis=-1,keepdims=True)\n        \n        output = (x@self.kernel)\/(kernel_norm*input_norm+1e-9)\n        return output ","40b20f95":"import tensorflow as tf\n\n## Tensorflow model\n\n\nclass GOdin_Model():\n    def __init__(self, L2_WEIGHT_DECAY = 1e-4, similarity = 'I'):\n        self.inputlayer = tf.keras.layers.Input(shape=(28,28,1),name='initial_input')\n        self.conv_1 = tf.keras.layers.Conv2D(4,5,\n                                            kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n                                            bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY))\n        self.conv_2 = tf.keras.layers.Conv2D(12,5,\n                                            kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n                                            bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY))\n        \n        self.pool_1 = tf.keras.layers.MaxPool2D(2)\n        self.pool_2 = tf.keras.layers.MaxPool2D(2)\n        self.flatten = tf.keras.layers.Flatten()\n        \n        self.dense_1 = tf.keras.layers.Dense(128,activation = 'relu',\n                                            kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n                                            bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY))\n        self.dense_2 = tf.keras.layers.Dense(64,activation = 'relu',\n                                            kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n                                            bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY))\n    \n        self.norm_0 = tf.keras.layers.LayerNormalization()\n        self.norm = tf.keras.layers.LayerNormalization()\n        \n        self.g_func = tf.keras.layers.Dense(1,\n                                            kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n                                            bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY))\n        \n        self.g_norm = tf.keras.layers.BatchNormalization()\n        self.g_activation = tf.keras.layers.Activation('sigmoid')\n        \n        if similarity == \"I\":\n            self.h_func = tf.keras.layers.Dense(10,\n                            kernel_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY),\n                            bias_regularizer=tf.keras.regularizers.l2(L2_WEIGHT_DECAY))\n        \n        elif similarity == \"E\":\n            self.h_func = eulid_dist(10, weight_decay=L2_WEIGHT_DECAY)\n            \n        elif similarity == 'C':\n            self.h_func = cosine(10, weight_decay=L2_WEIGHT_DECAY)\n            \n        else:\n            assert False,\"Incorrect similarity Measure\"\n            \n        self.div = tf.keras.layers.Lambda(lambda x:tf.math.divide(x[0]+1e-9, x[1]+1e-9))\n        self.pred = tf.keras.layers.Softmax(axis=-1)\n                    \n    def get_train_model(self, return_train_model=True):\n        \n        inputs = self.inputlayer\n        x = self.conv_1(inputs)\n        x = self.pool_1(x)\n        x = self.conv_2(x)\n        x = self.pool_2(x)\n        x = self.flatten(x)\n        \n        #x = self.norm_0(x)\n        x = self.dense_1(x)\n        #x = self.norm(x)\n        x = self.dense_2(x)\n        \n        h = self.h_func(x)\n        \n        g = self.g_func(x)\n        g = self.g_norm(g)\n        g = self.g_activation(g)\n\n        pred = self.div([h,g])\n        pred = self.pred(pred)\n        \n        if return_train_model:\n            return tf.keras.Model(inputs,pred)\n        else:\n            return tf.keras.Model(inputs,[pred,x,g,h])","e084d262":"tf.keras.backend.clear_session()\nnet = GOdin_Model(similarity=\"I\")\ntraining_net = net.get_train_model()\nprediction_net = net.get_train_model(return_train_model=False)\ntraining_net.summary()","86e66ee5":"training_net.compile(optimizer=tf.keras.optimizers.Adam(),\n                     loss=tf.keras.losses.SparseCategoricalCrossentropy())","f4354591":"hist = training_net.fit(x=MNIST_train_data,y=MNIST_train_labels,\n                 validation_data = (MNIST_test_data,MNIST_test_labels),\n                 epochs=20,batch_size=64,\n                 verbose=2,\n                 shuffle=True)","5e942727":"def create_pertubation(input_image,model):\n    image = tf.cast(input_image, tf.float32)\n    with tf.GradientTape() as tape:\n        tape.watch(image)\n        prediction = model(image)\n        prediction = tf.math.reduce_max(prediction,axis=-1)\n\n    # Get the gradients of the prediction w.r.t to the input image.\n    gradient = tape.gradient(prediction, image)\n    # Get the sign of the gradients to create the perturbation\n    signed_grad = tf.sign(gradient)\n    return signed_grad","839abd27":"perturbed_inputs = []\nepsi = 0.08\nfor ii in [MNIST_train_data,MNIST_test_data,Fashion_MNIST_all_data]:\n    grads = create_pertubation(ii,training_net)\n    ii = ii - epsi*(grads.numpy())\n    ii[ii<0]=0\n    ii[ii>1]=1\n    perturbed_inputs.append(ii)","94d77b6a":"predictions = prediction_net.predict(np.concatenate([MNIST_train_data,MNIST_test_data,Fashion_MNIST_all_data]))","717f6c21":"predictions_perturbed = prediction_net.predict(np.concatenate(perturbed_inputs))","eed05c9f":"predictions[2][:60000].mean(),predictions[2][60000:70000].mean(),predictions[2][70000:].mean(),predictions[2].max(),predictions[2].min()","c53521a6":"predictions_perturbed[2][:60000].mean(),predictions_perturbed[2][60000:70000].mean(),predictions_perturbed[2][70000:].mean(),predictions_perturbed[2].max(),predictions_perturbed[2].min()","692db25a":"tf.keras.backend.clear_session()","3110f190":"from numba import cuda\ncuda.select_device(0)\ncuda.close()","112da1af":"from cuml.manifold import TSNE","c5f876ce":"%%time\nlast_layer_values = predictions[1]\nlast_layer_values_perturbed = predictions_perturbed[1]\nX_embedded = TSNE(n_components=2, perplexity=15).fit_transform(last_layer_values)\nX_embedded_perturbed = TSNE(n_components=2, perplexity=15).fit_transform(last_layer_values_perturbed)","ec23a34c":"labels = np.concatenate([MNIST_train_labels,10+MNIST_test_labels,-np.ones_like(Fashion_MNIST_all_labels,dtype=np.int8)])\nsource = np.concatenate([np.zeros_like(MNIST_train_labels),np.ones_like(MNIST_test_labels),-np.ones_like(Fashion_MNIST_all_labels,dtype=np.int8)])\nentropies = -np.sum(predictions[0]*np.log(predictions[0]+1e-9),axis=1)\ng_s = predictions[2]\nh_s = predictions[3]\n\ndf = pd.DataFrame(np.concatenate([X_embedded,labels[...,np.newaxis],entropies[...,np.newaxis],source[...,np.newaxis],g_s,np.max(h_s,axis=1,keepdims=True)],axis=1),columns=['first','second','labels','entropies','source','g_s','h_s'])\n\ndf_perturbed = pd.DataFrame(np.concatenate([X_embedded_perturbed,labels[...,np.newaxis],entropies[...,np.newaxis],source[...,np.newaxis],g_s,np.max(h_s,axis=1,keepdims=True)],axis=1),columns=['first','second','labels','entropies','source','g_s','h_s'])\n\n\nmap_labels = {-1:'Fashion_Mnist'}\nmap_labels.update({ii:f'Trained_{ii}' for ii in range(10)})\nmap_labels.update({ii:f'Untrained_{ii-10}' for ii in range(10,20)})\n\ndf['label_names'] = df.labels.astype(int).map(map_labels)\ndf_perturbed['label_names'] = df_perturbed.labels.astype(int).map(map_labels)","95cdfa49":"colours = [\"#000000\"]\ncolours_10 = [\"#E53935\",\n            \"#F4511E\",\n            \"#283593\", \n            \"#03A9F4\", \n            \"#00ACC1\",\n            \"#689F38\", \n            \"#FBC02D\",\n            \"#FB8C00\",\n            \"#7B1FA2\",\n            \"#757575\",]\n\ncolours_20 = [\"#F5B7B1\",\n                \"#EDBB99\", \n                \"#7FB3D5\",\n                \"#D6EAF8\",\n                \"#80DEEA\", \n                \"#C5E1A5\",\n                \"#FFF59D\",\n                \"#FAD7A0\",\n                \"#D2B4DE\",\n                \"#E5E7E9\",]\n\n\nhue_order = [\"Fashion_Mnist\"]\nhue_order_10 =  [f'Trained_{ii}' for ii in range(10)]\nhue_order_20 = [f'Untrained_{ii-10}' for ii in range(10,20)]","44dfea78":"df = df_perturbed","c7ea737d":"sns.set(rc={'axes.facecolor':'#F9F9F9'})\n\ns = 15\nalpha = 0.8\n\nfig,ax = plt.subplots(1,1, figsize=(18,18))\nsns.scatterplot(x='first',y='second',data=df[df['source']==1], \n                hue='label_names',hue_order=hue_order_20, palette=colours_20,\n                legend='full', s=s, alpha = alpha, ax=ax);\nsns.scatterplot(x='first',y='second',data=df[df['source']==0], \n                hue='label_names',hue_order=hue_order_10, palette=colours_10,\n                legend='full', s=s, ax=ax);\nsns.scatterplot(x='first',y='second',data=df[df['source']==-1], \n                hue='label_names',hue_order=hue_order, palette=colours,\n                legend='full', s=s, alpha = alpha, ax=ax);\n\nax.set_xlabel(\"First\")\nax.set_ylabel(\"Second\")\nax.set_yticklabels([])\nax.set_xticklabels([]);","ce02f8ec":"sns.set(rc={'axes.facecolor':'#D7D7D7'})\n\ncmap = 'Reds'\n\nnorm = plt.Normalize(df['entropies'].min(), df['entropies'].max())\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\n\ns = 8\nalpha = 0.8\npercentage = 1\nfig,ax = plt.subplots(1,1, figsize=(20,16))\ndf.sort_values('entropies',ascending=True, inplace=True)\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==0], \n                hue='entropies', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha, edgecolor=None);\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==-1], \n               hue='entropies', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha, edgecolor=None);\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==1], \n               hue='entropies', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha, edgecolor=None);\n\n\n\nax.set_xlabel(\"First\")\nax.set_ylabel(\"Second\")\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.figure.colorbar(sm,label='Entropy');","23937cf5":"sns.set(rc={'axes.facecolor':'#BCD7FF'})\n\ncmap = 'Reds'\n\nnorm = plt.Normalize(df['h_s'].min(), df['g_s'].max())\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\n\ns = 10\nalpha = 1\npercentage = 1\nfig,ax = plt.subplots(1,1, figsize=(20,16))\n\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==0], \n                hue='h_s', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha);\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==-1], \n               hue='h_s', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha);\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==1], \n               hue='h_s', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha);\n\n\n\nax.set_xlabel(\"First\")\nax.set_ylabel(\"Second\")\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.figure.colorbar(sm,label='H_value');","8baee88e":"sns.set(rc={'axes.facecolor':'#BCD7FF'})\n\ncmap = 'Reds'\n\nnorm = plt.Normalize(df['g_s'].min(), df['g_s'].max())\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\n\ns = 10\nalpha = 1\npercentage = 1\nfig,ax = plt.subplots(1,1, figsize=(16,12))\ndf.sort_values('g_s',ascending=True, inplace=True)\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==0], \n                hue='g_s', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha);\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==-1], \n               hue='g_s', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha);\nsns.scatterplot(x='first',y='second',data=df.iloc[-int(percentage*df.shape[0]):].loc[df['source']==1], \n               hue='g_s', palette=cmap, s=s, ax=ax, legend=None, alpha = alpha);\n\n\n\nax.set_xlabel(\"First\")\nax.set_ylabel(\"Second\")\nax.set_yticklabels([])\nax.set_xticklabels([])\nax.figure.colorbar(sm,label='G_value');","b1ee6608":"fig,ax=plt.subplots(1,1,figsize=(8,8))\nsns.histplot(data=df,x='g_s',hue='source', kde=True,ax=ax);","7ff7da17":"fig,ax=plt.subplots(1,1,figsize=(8,8))\nsns.histplot(data=df_perturbed,x='g_s',hue='source', kde=True,ax=ax);","8399c319":"## Visualization","4fb98388":"## T-SNE","91341fd0":"## Model"}}