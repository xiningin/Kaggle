{"cell_type":{"4d96cad5":"code","830ca06d":"code","5a36140a":"code","0f190135":"code","e6d03eb1":"code","7bcb9eea":"code","b7e947fd":"code","da9b6c61":"code","ff49963b":"code","1f224b96":"code","92e38bfd":"code","f4923ca8":"code","9a877130":"code","bc729a95":"code","ab374041":"code","a82cc267":"code","36ce64cc":"code","ad36bfd9":"code","d4f2e40a":"markdown","0e6ef4a5":"markdown","34e4cae3":"markdown","c8e87ef1":"markdown","c3fe4a96":"markdown","0c46a307":"markdown","80474fcf":"markdown","7c70685d":"markdown","9f7e7891":"markdown","021db959":"markdown","4c576431":"markdown","3ce9090d":"markdown","537c6c4b":"markdown","a9bd7969":"markdown","ecd0075a":"markdown","aaad8a85":"markdown","21e0381d":"markdown"},"source":{"4d96cad5":"## Kaggle config\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","830ca06d":"# visualisation packeges\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme()\n\n# ensemble learing and model evaluation packages\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import roc_curve,roc_auc_score, auc, confusion_matrix, classification_report, plot_confusion_matrix\n# from sklearn.cross_validation import StratifiedKFold\n\n# neural network models\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.backend import clear_session\nimport random\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n#     tf.random.set_seed(seed)\nseed_everything(2021)","5a36140a":"# Load the data \"Credit Card Fraud \" data set\ndata = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\ndata.head()","0f190135":"sns.jointplot(data=data[data['Class']==1],\n               x= \"Time\",\n               y= \"V1\",\n               kind='hex',\n                edgecolor='#C1C1C1',               \n                color='#CB002B',height = 10,\n               joint_kws = {\"gridsize\":50} )\nsns.jointplot(data=data[data['Class']==0],\n           x= \"Time\",\n           y= \"V1\",\n           kind='hex',\n            edgecolor='#C1C1C1',\n            color='#009637',height = 10,\n           joint_kws = {\"gridsize\":50} )\n\n","e6d03eb1":"\nfor i in data.iloc[:,1:-1].columns:\n    sns.jointplot(data=data, x= \"Time\", y= i, hue = \"Class\",height = 10 )","7bcb9eea":"\n# the marority of the feature is not significantly related, but we should scale income.\nsns.heatmap(data.corr())\n","b7e947fd":"\n# Simple feature cleaning\nscaler =  RobustScaler()\n\n# +0.01 for calculatioscaler.fit_transform(np.log10(data['Amount']+0.01).values.reshape(-1,1))n of log10 \nscaled_amount = scaler.fit_transform(np.log10(data['Amount']+0.01).values.reshape(-1,1))\ndata.insert(0, 'scaled_amount', scaled_amount)\n\ndata.drop(['Time','Amount'], axis=1, inplace=True)\n\n","da9b6c61":"X_train, X_test, y_train, y_test =train_test_split(data.iloc[:,:-1],\n                                                   data.iloc[:,-1],\n                                                   test_size=0.2,\n                                                   stratify =data.iloc[:,-1])\ndata_fraud = X_train[y_train == 1]\ndata_norm  = X_train[y_train == 0]\n\n\n\nprint(f'fraud data shape = {data_fraud.shape}')\nprint(f'normal data shape = {data_norm.shape}')\n\nprint('#' * 40)\n\nprint(f'data validation  set shape = {X_test.shape} for final model evaluation ')","ff49963b":"def create_model():\n    model = Sequential()\n    model.add(Dense(units = 16 , activation = 'tanh' , input_dim = 29))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    model.add(Dense(units = 8 , activation = 'tanh'))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    model.add(Dense(units = 4 , activation = 'tanh'))\n    model.add(BatchNormalization())\n    model.add(Dense(units = 2 , activation = 'tanh'))\n    model.add(Dense(units = 1 , activation = 'sigmoid'))\n    model.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n#     print(model.summary())\n    return model\n# create_model()","1f224b96":"\ndef generate_model(fraud, norm , y):\n    size = fraud.shape[0]\n    # there will be same amount of data of both class in this subset \n    norm_sample = norm.sample(size)\n    \n    sample_data = pd.concat([fraud, norm_sample])\n    \n    X_train = sample_data\n    y_train = y[sample_data.index]\n    \n    model = create_model()\n    model.fit(X_train, y_train, \n                   validation_split = 0.2,\n                   batch_size = 8,\n                   epochs = 50,\n                   shuffle=True,\n                   verbose=0)\n    # return trained model\n    return model ","92e38bfd":"# clear backend keras session\nclear_session()\n\n# use 500 VNNs to ensemble learning \nmodels = [generate_model(data_fraud, data_norm, y_train) for i in range(50)]","f4923ca8":"predicted_y = np.array([model.predict(X_test) for model in models]).mean(axis = 0).reshape(-1)","9a877130":"\nprint(f'true      y shape  = {y_test.shape}')\nprint(f'predicted y shape  = {predicted_y.shape}')","bc729a95":"fpr, tpr, threshold = roc_curve(y_test, predicted_y)\n# print(fpr, tpr, threshold)\n\nauc1 = auc(fpr, tpr)\n## Plot the result\nplt.figure(figsize = (10,5))\nplt.title(\"Performance on test dataset\")\nplt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% auc1)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","ab374041":"def create_baseline_model():\n    model = Sequential()\n    model.add(Dense(units = 29 , activation = 'tanh' , input_dim = 29))\n#     model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    model.add(Dense(units = 29 , activation = 'tanh'))\n#     model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    model.add(Dense(units = 16 , activation = 'tanh'))\n#     model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    model.add(Dense(units = 16 , activation = 'tanh'))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    model.add(Dense(units = 8 , activation = 'tanh'))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    model.add(Dense(units = 4 , activation = 'tanh'))\n    model.add(BatchNormalization())\n    model.add(Dense(units = 2 , activation = 'tanh'))\n    model.add(Dense(units = 1 , activation = 'sigmoid'))\n    model.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\n    print(model.summary())\n    return model","a82cc267":"base = KerasClassifier(build_fn = create_baseline_model,\n                       nb_epoch = 50, \n                       batch_size = 100,\n                       verbose = 2)","36ce64cc":"base.fit(X_train,y_train)","ad36bfd9":"predicted_y = base.predict(X_test) \n\nfpr, tpr, threshold = roc_curve(y_test, predicted_y)\nprint(fpr, tpr, threshold)\n\nauc1 = auc(fpr, tpr)\n## Plot the result\n\n# plt.plot(fpr, tpr, color = 'b', label = 'AUC = %0.3f' % auc1)\n# plt.legend(loc = 'lower right')\n# plt.plot([0, 1], [0, 1],'r--')\n# plt.xlim([0, 1])\n# plt.ylim([0, 1])\n# plt.ylabel('True Positive Rate')\n# plt.xlabel('False Positive Rate')\n# plt.show()   \n# fpr, tpr, threshold = roc_curve(y_test, predicted_y)\n# # print(fpr, tpr, threshold)\n\nauc1 = auc(fpr, tpr)\n## Plot the result\nplt.figure(figsize = (10,5))\nplt.title(\"Performance on test dataset\")\nplt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% auc1)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","d4f2e40a":"#### 4.1.4 STEP 3 train the models \n\nI use experimental measure this time, but combining the keras.wrappers.scikit_learn.kerasclassifier and sklearn.VotingClassifier should be a better practice","0e6ef4a5":"### 4.1 Model Building\n#### 4.1.1 Stratify sampling of the trainin data, making sure that every sub-model is trained by balanced data","34e4cae3":"\n## 3. Methodology\n   ### 3.1 New approach -- VNN ensemble \n   \n   Introduction of Random Forest : Use multiple disicion tree sub-models  that trained by \n       1. Random feature selection.\n       2. Random sample of training set.\n       3. Different hyperparams.\n       \n       \n   And the democratic voting of of those disicion trees will finally determine the output of the \"Forest\" .\n   \n   \n ### 3.2 \u3010 4 Steps \u3011to build VNN version random forest:\n \n   \n   #### 3.2.1 STEP 1\n   Build a template VNN model : whose trainable parameters amount should be small so that the model can be suitable for ralatively small amount of training set.\n   \n   #### 3.2.2 STEP 2 \n   \n   Build a class which inherit keras.wrappers.scikit_learn.kerasclassifier, overwrite \"fit\" function, add \"random sample inputs and features of the input\" process before actually fitting.\n   \n   #### 3.2.3 STEP 3 \n   \n   Train the those classifier.\n   \n   #### 3.2.4 STEP 4\n   \n   Ensemble those models into a single aggregated one, use soft voting to decide final output of the classifier model.  \n   \n   \n   \n#### 3.3  Rationale\n   \n   \n   The reason why I want to try this is that I think it would be definitely cool if we can random sample the training set and features to build multiple small and weaker NN models, and then bagging-aggregate them to a ensembled one. Neural Network models are good at extracting complex and abstract relation between input and output through the non-linear activation function and multi-layered processes, therefore, I think it's doable to try NN models on what desision tree models do.   \n   \n   In case of imbalanced dataset like this (Credit Card Fraud), it's obvious that traditinoal NN model will be distorted to predict all sample \"False\" and still achieve 99%+ accuracy, ***But What if we ensemble sub-models that is all trained by balenced dataset? *** \n\n   I think that balanced sub-dataset will urge the model to extract the subtle information that really tell the fraud transactions from the normal ones, and we con mitigate the drawback of loss of information, caused by dramatically degree of down-sampling (from 200k+ to 400 in this data set), by repeating this model and ensemble them. This can be deemed as another way of up-sampling.  \n   \n   \n   \n","c8e87ef1":"    Todo :\n    Define a new kind of **Keras Classifier** that inherit the original one, but overwrite the fit function to fit **only the random sample of the original data**. Therefore, the new Classifier class could be compatible with **Sklearn.ensemble.VotingClassifier** ","c3fe4a96":"\n> Hi Kagglers, my name is Ben Chen. Since I'm is still a data rookie, the content below might contain unintentional misleading parts, and any feedback from you will be helpful and highly appreciated. Thank you in advance for viewing my work :D.\n\n## Table of Contents\n\n\n1. Abstract\n\n2. Introductory\n    * Dataset background\n    * Purpose\n    * EDAs\n    \n3. Methodology\n    * New approach -- VNN ensemble\n    * Retionale\n    \n4. Results\n   \n    * Model building\n    * ROC AUC comparison \n    \n5. Discussion\n    * Compatibility with keras.wrappers.scikit_learn.KerasClassifier  \n    * Reflection of the mathematical meaning\n    \n6. Conclusion\n    \n7. Reference \n    * dataset \n    * packages\n    * IBM data science\n\n\n\n","0c46a307":"### EDA => time and other feature jointplot \n\n\"Time\" feature serise data need further feature engineering because its distribution is messy, and the imformation it contain might be better extracted by some sort of poisson regression or related methods. For this notebook, I choose to drop this out for simplicity.","80474fcf":"#### 4.1.2 STEP 1 Template model create function","7c70685d":"\n\n## 5 Discussion \n\n### 5.1 Compatibility with keras.wrappers.scikit_learn.KerasClassifier\n\nIf this model is made Compatibe with keras.wrappers.scikit_learn.KerasClassifier, then with VotingClassifier API of Sklean it can fully access all other sklearn tools that require estimator as input. I will later imitate the sklearn random forest git code and create an keras version.\n\n### 5.2 Reflection of the mathematical meaning of this model\n\nI've considered that if the aggregated bagging model will simply become a transformation of a single larger deep learning network, and that if you can actually get a mathematically indentical outcome from the mechanisms of dense layer (act as soft voting), batch training (act like multiple sub-models), Dropout layer (act like random feature selection). However, i don't have the answer yet. \n\nBut here's a couple of my ideas : \nAs far as i know, the weight initialization in the neural network model is, if not apply WI technique, random. And this  increases the independency of each model and subsequently, lower the overall model variance. \n\nUnder completely independent models, we have ==>  \n\n![](https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/7bc72a6e608a8d94197a504e8b4ad80e6ee17d72)\n\n\nAnd divided by n shows that aggregate n completely independant models lower the varience by n.\n\nBeside, the bagging of the sample, to some extent, increase the independancy and lower the variance as well.\n\n\n\n\n","9f7e7891":"## 4. Result","021db959":"## 2. Introduction\n\n\n\n   * Dataset understanding:\n  \n   For the sake of personal privacy concern, this dataset is comprised of processed features, something like the PCA components, which make us harder to conduct processing with understanding of the nature of the data. There is almost no additional scaling and data selection task needed.\n   \n   The \"Amount\" and \"Time\" columns are raw data, which we will later do some simple preprocessing.   \n      \n   * Purpose\n   With good quality of card fraud prediction model, we can detect highly risky transaction before the damage done ( with high precision rate models) and, in the mean time, not disturb normal transaction card holders( with high recall rate models), thus, this dataset really  \n   \n   This notebook is tryiny to explore a new kind of classifier model, which i think is really cool, that can potentially provide high prediction performance and create value for the society.\n   \n>    But most importantly, if I can have any people on kaggle provide any feedback to me, that would be my greatest pleasure.","4c576431":"### EDA \n\n\"Time\" feature serise data need further feature engineering because its distribution is messy, and the imformation it contain might be better extracted by some sort of poisson regression or related methods. For this notebook, I choose to drop this out for simplicity.","3ce9090d":"\n* Time serise data is little complex to process, therefore, I choose to drop that feature for simplicity\n* Use Log10 to transform \"Amount\" column, and put it into  \"RobustScaler\"(less sensitive to outlier)\n\nnote that 0.01 that added to the data is for computational purpose, because log10( 0 ) is nagative infinity","537c6c4b":"#### 4.1.5 Model Done !! ( Rough version, but anyway :D )\n\n![woww](https:\/\/unsplash.com\/photos\/Aka2x2D4Ph0\/download?force=true&w=1920)","a9bd7969":"#### 4.1.3 STEP 2 (rough version) : \nI generated a single model and train it with balenced data ( all fraud data and same amount of samples from normal data), skipped the random feature selection and skipped the class inherience section as well.","ecd0075a":"## 1.  Abstract\n    This notebook is experimental, trying to apply the the core concept of random forest classifier, which ensemble weak sub-models to achieve good bais-variance trade-off, on neurual network models (Vanilla Neurual Network). \n    \n    I find that ensembling learning of balence dataset can be deem as an alternative of up-sampling when encountering imbalenced data. \n    \n    Compared with merely Vanilla Neutral Networks have 0.89 AUC score, the ensembled version hit 0.967.\n\n","aaad8a85":"### 4.2 ROC AUC comparison ( between straight VNN and VNN version Random Forest)","21e0381d":"![Forest Way](https:\/\/images.unsplash.com\/photo-1425913397330-cf8af2ff40a1?ixlib=rb-1.2.1&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1267&q=80)"}}