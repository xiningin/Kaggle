{"cell_type":{"0dfb5c21":"code","9a9eb56b":"code","7637ddb7":"code","d9ae228f":"code","31965afd":"code","3a62426d":"code","fb69a378":"code","44efd01c":"code","df65666e":"code","4c580b03":"code","a652e707":"code","fe25f9ec":"code","6930437b":"code","69d811a9":"code","34cd5dac":"code","cba12b91":"code","17e47d24":"code","a971a637":"code","53cbab37":"code","7d24e9f1":"code","e7a5e7bf":"code","438b8703":"code","9ba3afb9":"code","6244f6c5":"code","2ed1f5a4":"code","f9aa8625":"code","b88f433d":"code","ada08c57":"code","6eef3640":"code","bd3c4c8b":"code","832d1199":"code","1524f4ab":"code","5f72e4f1":"code","883f7152":"code","5b625ffc":"code","b4f4e7b9":"code","1ed8b564":"code","86ceac9a":"code","19dadd06":"code","e6740dff":"code","54805e70":"code","59d84cb6":"code","dc8e7626":"code","f036f2e5":"code","da36020f":"code","da895a4e":"code","c17598f7":"code","062eb15d":"code","6330c966":"code","8a8b6451":"code","553561a1":"code","4c263039":"code","0b836314":"code","f8073a0e":"code","4abf8fe6":"code","49e783bc":"code","d4512ab2":"code","7d93b099":"code","2347c3a1":"code","2369e2ae":"code","c17ac680":"code","1a6da020":"code","3abeb6fb":"code","4a917407":"code","ab135152":"code","d234a31c":"code","c0ad51a0":"code","70898613":"code","7a56d38e":"code","fb2c4808":"markdown","866e6d4b":"markdown","92f1234f":"markdown","415d94d2":"markdown","b21f83dd":"markdown","0d7fea63":"markdown","a23f2710":"markdown","7625e131":"markdown","c9bb14e8":"markdown","b8d5ee0d":"markdown","5e0e931d":"markdown","78468e29":"markdown","0e28c084":"markdown","1e819367":"markdown","0c873db4":"markdown","4077100d":"markdown","dfd28dea":"markdown","bc28e69b":"markdown","625482c6":"markdown","40cd4e42":"markdown","dda839e3":"markdown","67156bc4":"markdown","b55701ac":"markdown","5d111038":"markdown","e613569b":"markdown","fb3687b6":"markdown","6820d3d9":"markdown","8ae26b49":"markdown","fcff38f0":"markdown"},"source":{"0dfb5c21":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import Counter","9a9eb56b":"train= pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest =pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nss =pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","7637ddb7":"train.head()","d9ae228f":"test.head()","31965afd":"train.describe()","3a62426d":"sns.catplot(x='Pclass', data=train,kind='count', hue='Sex',palette='rocket')","fb69a378":"sns.catplot(x = 'Sex',data=train, kind='count',palette='rocket')","44efd01c":"sns.barplot(x='Pclass', y='Survived', data=train,palette='rocket')","df65666e":"sns.catplot(x ='Survived',data=train, kind='count',hue='Sex',palette='rocket')","4c580b03":"sns.catplot(x='Pclass', data=train,kind='count', hue='Sex',palette='rocket')","a652e707":"#Age\nsns.boxplot(x='Survived',y='Age',data=train,palette='winter')","fe25f9ec":"sns.displot(train['Age'].dropna(),kde=False,color='darkred',bins=40)","6930437b":"#Survival and dead distribution with respect to age\nplt.figure(figsize=(20, 30))\nsns.countplot(y = \"Age\",hue=\"Survived\", data=train)","69d811a9":"FacetGrid = sns.FacetGrid(train, row='Embarked', size=5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","34cd5dac":"print(train[\"SibSp\"].value_counts())","cba12b91":"ax = sns.countplot(x = \"SibSp\",hue=\"Survived\", data=train,palette='rocket')","17e47d24":"sns.heatmap(train.corr(method=\"spearman\"));","a971a637":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap=\"viridis\")","53cbab37":"total = train.isnull().sum().sort_values(ascending=False)\ntotal","7d24e9f1":"train.head()","e7a5e7bf":"train.drop(['Name','Ticket','Cabin','Fare'],axis=1,inplace=True)\ntest.drop(['Name','Ticket','Cabin','Fare'],axis=1,inplace=True)","438b8703":"train","9ba3afb9":"sns.boxplot(x='Pclass',y='Age',data=train)","6244f6c5":"sns.boxplot(x='Pclass',y='Age',data=test)","2ed1f5a4":"def impute_age_train(cols):\n    Age=cols[0]\n    Pclass=cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37\n        elif Pclass == 2:\n            return 29\n        else :\n            return 24\n    else:\n        return Age","f9aa8625":"def impute_age_test(cols):\n    Age=cols[0]\n    Pclass=cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 42\n        elif Pclass == 2:\n            return 27\n        else :\n            return 25\n    else:\n        return Age","b88f433d":"train['Age']=train[['Age','Pclass']].apply(impute_age_train,axis=1)\ntest['Age']=test[['Age','Pclass']].apply(impute_age_test,axis=1)","ada08c57":"train['Embarked'].describe()","6eef3640":"common='S'\ntrain['Embarked'] = train['Embarked'].fillna(common)","bd3c4c8b":"train.isnull().sum()\n","832d1199":"Embarked = pd.get_dummies(train['Embarked'],drop_first=True)\nEmbarked_test = pd.get_dummies(test['Embarked'],drop_first=True)\nEmbarked.head()","1524f4ab":"#Converting values\ntrain['Sex']=train['Sex'].map({'male':0,'female':1})\ntest['Sex']=test['Sex'].map({'male':0,'female':1})\ntrain.drop(['Embarked'],axis=1,inplace=True)\ntest.drop(['Embarked'],axis=1,inplace=True)","5f72e4f1":"train","883f7152":"test","5b625ffc":"train = pd.concat([train,Embarked],axis=1)\ntest = pd.concat([test,Embarked_test],axis=1)\ntrain.head()","b4f4e7b9":"train.loc[train['Age'] <= 16, 'Age'] = 0\ntrain.loc[(train['Age'] > 16) & (train['Age'] <= 32), 'Age'] = 1\ntrain.loc[(train['Age'] > 32) & (train['Age'] <= 48), 'Age'] = 2\ntrain.loc[(train['Age'] > 48) & (train['Age'] <= 64), 'Age'] = 3\ntrain.loc[ train['Age'] > 64, 'Age'] = 4","1ed8b564":"test.loc[test['Age'] <= 16, 'Age'] = 0\ntest.loc[(test['Age'] > 16) & (test['Age'] <= 32), 'Age'] = 1\ntest.loc[(test['Age'] > 32) & (test['Age'] <= 48), 'Age'] = 2\ntest.loc[(test['Age'] > 48) & (test['Age'] <= 64), 'Age'] = 3\ntest.loc[ test['Age'] > 64, 'Age'] = 4","86ceac9a":"train= train.drop(\"PassengerId\", axis = 1)","19dadd06":"test.head()","e6740dff":"from sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom catboost import CatBoostClassifier\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.model_selection import train_test_split\n","54805e70":"X_train = train.drop(\"Survived\", axis=1)\nY_train = train[\"Survived\"]\nX_test = test.drop(\"PassengerId\", axis = 1)\n","59d84cb6":"KNN=KNeighborsClassifier()\nNAIVE=GaussianNB()\nSVM=SVC()\nDT=DecisionTreeClassifier()\nLR = LogisticRegression()\nRF = RandomForestClassifier()\nEnsemble = VotingClassifier( estimators= [('KNN',KNN),('NB',NAIVE),('SVM',SVM),('DT',DT),('LR',LR),('RF',RF)], voting = 'hard')","dc8e7626":"Ensemble.fit(X_train,Y_train)","f036f2e5":"from sklearn import metrics\nY_pred_rand = (Ensemble.predict(X_train) > 0.5).astype(int)\nprint('Precision : ', np.round(metrics.precision_score(Y_train, Y_pred_rand)*100,2))\nprint('Accuracy : ', np.round(metrics.accuracy_score(Y_train, Y_pred_rand)*100,2))\nprint('Recall : ', np.round(metrics.recall_score(Y_train, Y_pred_rand)*100,2))\nprint('F1 score : ', np.round(metrics.f1_score(Y_train, Y_pred_rand)*100,2))\nprint('AUC : ', np.round(metrics.roc_auc_score(Y_train, Y_pred_rand)*100,2))","da36020f":"catboost = CatBoostClassifier(silent=True)\ncatboost.fit(X_train, Y_train)\nY_pred = catboost.predict(X_test)\nacc_catboost = round(catboost.score(X_train, Y_train) * 100, 2);","da895a4e":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","c17598f7":"svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","062eb15d":"knn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","6330c966":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","8a8b6451":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","553561a1":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","4c263039":"random_forest = RandomForestClassifier(n_estimators = 100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","0b836314":"\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","f8073a0e":"sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","4abf8fe6":"models = pd.DataFrame({'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n                                 'Random Forest', 'Naive Bayes', 'Perceptron', 'Stochastic Gradient Decent', \n                                 'Linear SVC', 'Decision Tree', 'CatBoost'],\n                       'Score': [acc_svc, acc_knn, acc_log, acc_random_forest, acc_gaussian, acc_perceptron,\n                                 acc_sgd, acc_linear_svc, acc_decision_tree, acc_catboost]})\n\nmodels.sort_values(by = 'Score', ascending = False, ignore_index = True)","49e783bc":"classifiers = []\nclassifiers.append(LogisticRegression())\nclassifiers.append(SVC())\nclassifiers.append(KNeighborsClassifier(n_neighbors = 5))\nclassifiers.append(GaussianNB())\nclassifiers.append(Perceptron())\nclassifiers.append(LinearSVC())\nclassifiers.append(SGDClassifier())\nclassifiers.append(DecisionTreeClassifier())\nclassifiers.append(RandomForestClassifier())\nclassifiers.append(CatBoostClassifier(silent=True))\n\nlen(classifiers)","d4512ab2":"from sklearn.model_selection import cross_val_score","7d93b099":"cv_results = []\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(classifier, X_train, Y_train, scoring = 'accuracy',verbose=False))\n    ","2347c3a1":"cv_mean = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_mean.append(cv_result.mean())\n    cv_std.append(cv_result.std())","2369e2ae":"cv_res = pd.DataFrame({'Cross Validation Mean': cv_mean, 'Cross Validation Std': cv_std, 'Algorithm': ['Logistic Regression', 'Support Vector Machines', 'KNN', 'Gausian Naive Bayes', 'Perceptron', 'Linear SVC', 'Stochastic Gradient Descent', 'Decision Tree', 'Random Forest', 'CatBoost']})\ncv_res.sort_values(by = 'Cross Validation Mean', ascending = False, ignore_index = True)","c17ac680":"sns.barplot('Cross Validation Mean', 'Algorithm', data = cv_res, order = cv_res.sort_values(by = 'Cross Validation Mean', ascending = False)['Algorithm'], palette = 'Set3', **{'xerr': cv_std})\nplt.ylabel('Algorithm')\nplt.title('Cross Validation Scores')","1a6da020":"param_grid = {'C': [0.1, 1, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf']}  \n  \ngrid = GridSearchCV(SVC(), param_grid, refit = True, verbose = 0) \n\ngrid.fit(X_train, Y_train)","3abeb6fb":"print(\"Best parameters: \", grid.best_params_) \nprint(\"Best estimator: \", grid.best_estimator_)","4a917407":"svc = SVC(C = 100, gamma = 0.01, kernel = 'rbf')\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","ab135152":"cross_val_score(svc, X_train, Y_train, scoring = 'accuracy', cv = 10).mean()","d234a31c":"ss.head()","c0ad51a0":"submit = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': Y_pred})\nsubmit.head()","70898613":"submit.shape","7a56d38e":"submit.to_csv(\"final_submission.csv\",index = False)","fb2c4808":"Women on port Q and S has higher probability of surviving,Same cannot be said for the women on port C.The probability of men surviving is lower on port Q and S and it is higher in C.","866e6d4b":"Filling up mean values for null.","92f1234f":"In class 3 males are more than 2x of women.","415d94d2":"<h1><center>Titanic EDA<\/center><\/h1>","b21f83dd":"# What's in the notebook?\n**1)EDA**\n\n**2)Detection of missing data and imputation.**\n\n**3)Feature enginnering.**\n\n**4)Coverting categorical features.**\n\n**5)Modelling.**\n\n**6)Ensemble.**\n\n**7)K-fold cv.**\n\n**8)Hyperparameter tuning.**","0d7fea63":"# Please upvote if you like this notebook.","a23f2710":"<h1><center>MODELLING<\/center><\/h1>","7625e131":"The number of male passengers are almost 2x of female passengers appox.","c9bb14e8":"Above,we can see that the age of passengers ranges between 0.4 to 80 years.And 38% of the people survived \nwhose records are mentioned in this training set.","b8d5ee0d":"Despite the ship being male dominated.The survival rate of women is higher.As we can see that large number of women survived despite being minorities.","5e0e931d":"In my case i got support vector machine with more accuracy.","78468e29":"<h1><center>Handling Missing values and imputing it<\/center><\/h1>","0e28c084":"Spearman corr of training data","1e819367":"The average age of people seems to almost same for survived and dead.however we can observe some outliers","0c873db4":"Here,we can see that person in class 1 has the higher chances of survival.Where as person in class 3 has lower chance.","4077100d":"# What's new?\n**1. Removed executon logs to increase the readability.**\n\n# Note:\n**This is a work in progress.Therefore,your feedback is valuable for me.**","dfd28dea":"Age distrubution in titanic traning dataset","bc28e69b":"# Hyperparameter tuning","625482c6":"From the above plot we can see that people with 0 and 1 Sibsp(Sibling or spouse) has greater chance of survival.Maybe because of sibsp with 0 or 1 were present in greater number.","40cd4e42":"# k-fold Cross-validation","dda839e3":"# Catogorical features","67156bc4":"These are the following missing values.","b55701ac":"**Relation ship between PCLASS and AGE**","5d111038":"**Svc has higher cv score**","e613569b":"# Types of features in titanic\n\n**Categorical**: is a collection of information that is divided into groups.\nEg. Embarked (C = Cherbourg; Q = Queenstown; S = Southampton)\n\n**Ordinal**: They are similar to categorical features but they have an order.\nEg. Pclass (1, 2, 3)\n\n**Binary**: A categrorical feature which has only 2 types of categories.Which is often represted as 0 and 1.\nEg: Sex (Male\/Female)\n\n**Continuous**: They can take up any value between the minimum and maximum values in a column.\nEg. Age, Fare\n\n**Count**: They represent the count of a variable.\nEg. SibSp, Parch","fb3687b6":"S is common in Embarked.We will fill null values with it.","6820d3d9":"Class 3 males are more than 2x of women.","8ae26b49":"# Ensemble technique","fcff38f0":"All the yellow lines that we see are missing values.We can see that majority of values are missing in cabin and age."}}