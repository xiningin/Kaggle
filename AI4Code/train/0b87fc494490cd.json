{"cell_type":{"68d68689":"code","7b281a66":"code","68b522e7":"code","7cc883db":"code","afa9ed10":"code","fc30a8ea":"markdown","36f4547d":"markdown","07b8c00e":"markdown","18419ec9":"markdown","d90f00f3":"markdown"},"source":{"68d68689":"import numpy as np\nimport numpy.random as np_rnd\nimport scipy as sp\nfrom sklearn.base import BaseEstimator\nfrom sklearn.tree import DecisionTreeRegressor\nfrom scipy.stats import norm as dist\n\ndef default_tree_learner(depth=3):\n    return DecisionTreeRegressor(\n        criterion='friedman_mse',\n        min_samples_split=2,\n        min_samples_leaf=1,\n        min_weight_fraction_leaf=0.0,\n        max_depth=depth,\n        splitter='best')\n\nclass MLE:\n    def __init__(self, seed=123):\n        pass\n\n    def loss(self, forecast, Y):\n        return forecast.nll(Y.squeeze()).mean()\n\n    def grad(self, forecast, Y, natural=True):\n        fisher = forecast.fisher_info()\n        grad = forecast.D_nll(Y)\n        if natural:\n            grad = np.linalg.solve(fisher, grad)\n        return grad\n\n\nclass CRPS:\n    def __init__(self, K=32):\n        self.K = K\n\n    def loss(self, forecast, Y):\n        return forecast.crps(Y.squeeze()).mean()\n\n    def grad(self, forecast, Y, natural=True):\n        metric = forecast.crps_metric()\n        grad = forecast.D_crps(Y)\n        if natural:\n            grad = np.linalg.solve(metric, grad)\n        return grad\n\nEPS = 1e-8\nclass Normal(object):\n    n_params = 2\n\n    def __init__(self, params, temp_scale = 1.0):\n        self.loc = params[0]\n        self.scale = np.exp(params[1] \/ temp_scale) + 1e-8\n        self.var = self.scale #** 2  + 1e-8\n        self.shp = self.loc.shape\n\n        self.dist = dist(loc=self.loc, scale=self.scale)\n\n    def __getattr__(self, name):\n        if name in dir(self.dist):\n            return getattr(self.dist, name)\n        return None\n\n    def nll(self, Y):\n        return -self.dist.logpdf(Y)\n\n    def D_nll(self, Y_):\n        Y = Y_.squeeze()\n        D = np.zeros((self.var.shape[0], 2))\n        D[:, 0] = (self.loc - Y) \/ self.var\n        D[:, 1] = 1 - ((self.loc - Y) ** 2) \/ self.var\n        return D\n\n    def crps(self, Y):\n        Z = (Y - self.loc) \/ (self.scale + EPS)\n        return (self.scale * (Z * (2 * sp.stats.norm.cdf(Z) - 1) + \\\n                2 * sp.stats.norm.pdf(Z) - 1 \/ np.sqrt(np.pi)))\n\n    def D_crps(self, Y_):\n        Y = Y_.squeeze()\n        Z = (Y - self.loc) \/ (self.scale + EPS)\n        D = np.zeros((self.var.shape[0], 2))\n        D[:, 0] = -(2 * sp.stats.norm.cdf(Z) - 1)\n        D[:, 1] = self.crps(Y) + (Y - self.loc) * D[:, 0]\n        return D\n\n    def crps_metric(self):\n        I = np.c_[2 * np.ones_like(self.var), np.zeros_like(self.var),\n                  np.zeros_like(self.var), self.var]\n        I = I.reshape((self.var.shape[0], 2, 2))\n        I = 1\/(2*np.sqrt(np.pi)) * I\n        return I #+ 1e-4 * np.eye(2)\n\n    def fisher_info(self):\n        FI = np.zeros((self.var.shape[0], 2, 2))\n        FI[:, 0, 0] = 1\/self.var + 1e-5\n        FI[:, 1, 1] = 2\n        return FI\n\n    def fisher_info_cens(self, T):\n        nabla = np.array([self.pdf(T),\n                          (T - self.loc) \/ self.scale * self.pdf(T)])\n        return np.outer(nabla, nabla) \/ (self.cdf(T) * (1 - self.cdf(T))) + 1e-2 * np.eye(2)\n\n    def fit(Y):\n        m, s = sp.stats.norm.fit(Y)\n        return np.array([m, np.log(s)])\n        #return np.array([m, np.log(1e-5)])\n\n\nclass NGBoost(BaseEstimator):\n\n    def __init__(self, Dist=Normal, Score=MLE(),\n                 Base=default_tree_learner, natural_gradient=True,\n                 n_estimators=500, learning_rate=0.01, minibatch_frac=1.0,\n                 verbose=True, verbose_eval=100, tol=1e-4):\n        self.Dist = Dist\n        self.Score = Score\n        self.Base = Base\n        self.natural_gradient = natural_gradient\n        self.n_estimators = n_estimators\n        self.learning_rate = learning_rate\n        self.minibatch_frac = minibatch_frac\n        self.verbose = verbose\n        self.verbose_eval = verbose_eval\n        self.init_params = None\n        self.base_models = []\n        self.scalings = []\n        self.tol = tol\n\n    def pred_param(self, X, max_iter=None):\n        m, n = X.shape\n        params = np.ones((m, self.Dist.n_params)) * self.init_params\n        for i, (models, s) in enumerate(zip(self.base_models, self.scalings)):\n            if max_iter and i == max_iter:\n                break\n            resids = np.array([model.predict(X) for model in models]).T\n            params -= self.learning_rate * resids * s\n        return params\n\n    def sample(self, X, Y, params):\n        if self.minibatch_frac == 1.0:\n            return np.arange(len(Y)), X, Y, params\n        sample_size = int(self.minibatch_frac * len(Y))\n        idxs = np_rnd.choice(np.arange(len(Y)), sample_size, replace=False)\n        return idxs, X[idxs,:], Y[idxs], params[idxs, :]\n\n    def fit_base(self, X, grads):\n        models = [self.Base().fit(X, g) for g in grads.T]\n        fitted = np.array([m.predict(X) for m in models]).T\n        self.base_models.append(models)\n        return fitted\n\n    def line_search(self, resids, start, Y, scale_init=1):\n        S = self.Score\n        D_init = self.Dist(start.T)\n        loss_init = S.loss(D_init, Y)\n        scale = scale_init\n        while True:\n            scaled_resids = resids * scale\n            D = self.Dist((start - scaled_resids).T)\n            loss = S.loss(D, Y)\n            norm = np.mean(np.linalg.norm(scaled_resids, axis=1))\n            if not np.isnan(loss) and (loss < loss_init or norm < self.tol) and\\\n               np.linalg.norm(scaled_resids, axis=1).mean() < 5.0:\n                break\n            scale = scale * 0.5\n        self.scalings.append(scale)\n        return scale\n\n    def fit(self, X, Y, X_val = None, Y_val = None, train_loss_monitor = None, val_loss_monitor = None):\n\n        loss_list = []\n        val_loss_list = []\n        self.fit_init_params_to_marginal(Y)\n\n        params = self.pred_param(X)\n        if X_val is not None and Y_val is not None:\n            val_params = self.pred_param(X_val)\n\n        S = self.Score\n\n        if not train_loss_monitor:\n            train_loss_monitor = S.loss\n\n        if not val_loss_monitor:\n            val_loss_monitor = S.loss\n\n        for itr in range(self.n_estimators):\n            _, X_batch, Y_batch, P_batch = self.sample(X, Y, params)\n\n            D = self.Dist(P_batch.T)\n\n            loss_list += [train_loss_monitor(D, Y_batch)]\n            loss = loss_list[-1]\n            grads = S.grad(D, Y_batch, natural=self.natural_gradient)\n\n            proj_grad = self.fit_base(X_batch, grads)\n            scale = self.line_search(proj_grad, P_batch, Y_batch)\n\n            params -= self.learning_rate * scale * np.array([m.predict(X) for m in self.base_models[-1]]).T\n\n            val_loss = 0\n            if X_val is not None and Y_val is not None:\n                val_params -= self.learning_rate * scale * np.array([m.predict(X_val) for m in self.base_models[-1]]).T\n                val_loss = val_loss_monitor(self.Dist(val_params.T), Y_val)\n                val_loss_list += [val_loss]\n                if len(val_loss_list) > 10 and np.mean(np.array(val_loss_list[-5:])) > \\\n                   np.mean(np.array(val_loss_list[-10:-5])):\n                    if self.verbose:\n                        print(f\"== Quitting at iteration \/ VAL {itr} (val_loss={val_loss:.4f})\")\n                    break\n\n            if self.verbose and int(self.verbose_eval) > 0 and itr % int(self.verbose_eval) == 0:\n                grad_norm = np.linalg.norm(grads, axis=1).mean() * scale\n                print(f\"[iter {itr}] loss={loss:.4f} val_loss={val_loss:.4f} scale={scale:.4f} \"\n                      f\"norm={grad_norm:.4f}\")\n\n            if np.linalg.norm(proj_grad, axis=1).mean() < self.tol:\n                if self.verbose:\n                    print(f\"== Quitting at iteration \/ GRAD {itr}\")\n                break\n\n        return self\n\n    def fit_init_params_to_marginal(self, Y, iters=1000):\n        try:\n            E = Y['Event']\n            T = Y['Time'].reshape((-1, 1))[E == 1]\n        except:\n            T = Y\n        self.init_params = self.Dist.fit(T)\n        return\n\n\n    def pred_dist(self, X, max_iter=None):\n        params = np.asarray(self.pred_param(X, max_iter))\n        dist = self.Dist(params.T)\n        return dist\n\n    def predict(self, X):\n        dist = self.pred_dist(X)\n        return list(dist.loc.flatten())\n","7b281a66":"import pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom dateutil.parser import parse\n\ndef clock_to_sec(s):\n    min_str, sec_str, msec_str = s.split(':')\n    return 60 * int(min_str) + int(sec_str)\n\ndef featurize(rows):\n    featnames = []\n    features = []\n    for _, row in rows.iterrows():\n        if row['NflId'] == row['NflIdRusher']:\n            rusher = row['NflId']\n            team = row['Team']\n            \n            featnames += ['Home']\n            features += [int(team == 'home')]\n            \n            featnames += ['Clock']\n            features += [clock_to_sec(row['GameClock'])]\n            \n            featnames += ['Quarter']\n            features += [int(row['Quarter'])]\n            \n            if team == 'home':\n                featnames += ['FriendScore']\n                features += [int(row['HomeScoreBeforePlay'])]\n                featnames += ['EnemyScore']\n                features += [int(row['VisitorScoreBeforePlay'])]\n            else:\n                featnames += ['FriendScore']\n                features += [int(row['VisitorScoreBeforePlay'])]\n                featnames += ['EnemyScore']\n                features += [int(row['HomeScoreBeforePlay'])]\n                \n            featnames += ['RusherHeight']\n            ft, inch = row['PlayerHeight'].split('-')\n            features += [int(ft) * 12 + int(inch)]\n            \n            featnames += ['RusherWeight']\n            features += [int(row['PlayerWeight'])]\n            \n            featnames += ['RusherAge']\n            features += [parse(row['TimeHandoff']).year - parse(row['PlayerBirthDate']).year]\n            \n            featnames += ['Week']\n            features += [int(row['Week'])]\n            \n            featnames += ['Temperature']\n            try:\n                features += [int(row['Temperature'])]\n            except:\n                features += [60]\n            \n            featnames += ['Humidity']\n            try:\n                features += [int(row['Humidity'])]\n            except:\n                features += [55]\n        \n            # Orient the game rightwards always, and center on the ball\/rusher\n            right = row['PlayDirection'] == 'right'\n            if right:\n                ball_X0, ball_Y0, ball_S, ball_A, ball_D = row['X'], row['Y'], row['S'], row['A'], np.radians(row['Dir'])\n            else:\n                ball_X0, ball_Y0, ball_S, ball_A, ball_D = 120 - row['X'], 53.3 - row['Y'], row['S'], row['A'], np.radians((row['Dir'] + 180) % 360)\n            break\n            \n            featnames += ['RusherSpeed']\n            features += [ball_S]\n    \n            featnames += ['RusherDir']\n            features += [ball_D]\n            \n            featnames += ['RusherAccel']\n            features += [row['A']]\n    \n    friends, enemies = np.zeros((10, 5)), np.zeros((11, 5))\n    fidx, eidx = 0, 0\n    for _, row in rows.iterrows():\n        if row['NflId'] == rusher:\n            continue\n        if right:\n            vals = [row['X'], row['Y'], row['S'], row['A'], np.radians(row['Dir'])]\n        else:\n            vals = [120 - row['X'], 53.3 - row['Y'], row['S'], row['A'], np.radians((row['Dir'] + 180) % 360)]\n        if team == row['Team']:\n            friends[fidx, :] = vals\n            fidx += 1\n        else:\n            enemies[eidx, :] = vals\n            eidx += 1\n\n    assert(fidx == 10)\n    assert(eidx == 11)\n\n    friends_rel_X0 = friends[:, 0] - ball_X0\n    friends_rel_Y0 = friends[:, 1] - ball_Y0\n    friends_R0 = (friends_rel_X0 ** 2 + friends_rel_Y0 ** 2) ** .5\n    friends_Theta0 = np.arctan2(friends_rel_Y0, friends_rel_X0)\n\n    enemies_rel_X0 = enemies[:, 0] - ball_X0\n    enemies_rel_Y0 = enemies[:, 1] - ball_Y0\n    enemies_R0 = (enemies_rel_X0 ** 2 + enemies_rel_Y0 ** 2) ** .5\n    enemies_Theta0 = np.arctan2(enemies_rel_Y0, enemies_rel_X0)\n\n    featnames += ['Ball_X0', 'Ball_Y0']\n    features += [ball_X0, ball_Y0]\n\n    Rs = [0, 3, 10, 30, 120]\n    for r in range(len(Rs)-1):\n        rl, rh = Rs[r], Rs[r+1]\n        for t in range(8):\n            tl, th = t * np.pi\/8, (t+1) * np.pi\/8\n            key = 'map0_r%dq%d' % (rh, t)\n            featnames.append('F' + key)\n            features.append(np.sum(\n                (rl <= friends_R0) & (friends_R0 < rh)\n                & (tl <= friends_Theta0) & (friends_Theta0 < th)))\n            featnames.append('E' + key)\n            features.append(np.sum(\n                (rl <= enemies_R0) & (enemies_R0 < rh)\n                & (tl <= enemies_Theta0) & (enemies_Theta0 < th)))\n\n    return featnames, features\n\ndef fparse(fname):\n    df = pd.read_csv(fname, low_memory=False)\n\n    Y = df[['PlayId', 'Yards']].groupby('PlayId').mean().values if 'Yards' in df.columns else None\n\n\n    X = []\n    for p in tqdm(range(int(df.shape[0] \/ 22))):\n        rows = df.iloc[p * 22: p*22 + 22]\n        names, features = featurize(rows)\n        X.append(features)\n\n\n    #X = df.groupby('PlayId').apply(grouper)\n\n    return np.array(X), Y","68b522e7":"X, Y = fparse('\/kaggle\/input\/nfl-big-data-bowl-2020\/train.csv')","7cc883db":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nnp.random.seed(123)\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2)\nngb = NGBoost(Dist=Normal, Score=CRPS(), verbose=True, learning_rate=0.01, n_estimators=5000)\nngb.fit(X_train, Y_train, X_val=X_val, Y_val=Y_val)\n\nY_dists = ngb.pred_dist(X_val)\n\n# test Root Mean Squared Error\ntest_RMSE = np.sqrt(mean_squared_error(Y_dists.mean(), Y_val))\nprint('Val RMSE', test_RMSE)\n\n# test CRPS\ntest_CRPS = Y_dists.crps(Y_val.flatten()).mean()\nprint('Val CRPS', test_CRPS)","afa9ed10":"from kaggle.competitions import nflrush\n\nenv = nflrush.make_env()\niter_test = env.iter_test()\n\nQ = list(range(-99, 100))\nfor (batch, sample) in tqdm(iter_test):\n    _, X_feats = featurize(batch)\n    X_test = np.array([X_feats])\n    Y_pred = ngb.pred_dist(X_test)\n    sample.iloc[0] = Y_pred.cdf(Q)\n    env.predict(sample)\n\nenv.write_submission_file()","fc30a8ea":"Include a standalone version of NGBoost derived from git:\/\/github.com\/stanfordmlgroup\/ngboost.git","36f4547d":"Feature extractor:","07b8c00e":"Load design matrix:","18419ec9":"Train model:","d90f00f3":"Make predictions:"}}