{"cell_type":{"89059dda":"code","7bd63b01":"code","f2868cff":"code","9224e0d1":"code","fd4dd620":"code","ec16eefe":"code","0bb56f51":"code","924c8a44":"code","8cb138fe":"code","a3f0bb17":"code","46014f84":"code","e242b01b":"code","31f98588":"code","30c410ca":"code","0aee66d3":"code","3686f305":"code","1bb67c6f":"code","c1cfb8f0":"code","1fb63200":"code","b10b926d":"code","62148ee2":"code","f091623a":"code","b04bb9c3":"code","3067710f":"code","d9f6bc4a":"code","3114da5a":"code","def6c9cd":"code","b3f7171d":"code","84ef2992":"code","af024b94":"code","b5b6ad15":"code","098e5a7a":"code","079b7e6f":"code","d46c4711":"code","0bb5ada0":"code","c0380a33":"markdown","e93bf858":"markdown"},"source":{"89059dda":"import os,sys\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Model\nfrom keras.layers import Input,LSTM,GRU,Dense,Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical","7bd63b01":"# config variables\nBATCH_SIZE = 64\nEPOCHS = 100\nLATENT_DIM = 256\nNUM_SAMPLES = 10000\nMAX_SEQUENCE_LENGTH = 100\nMAX_NUM_WORDS = 20000\nEMBEDDING_DIM = 100","f2868cff":"input_texts = [] # sentence in original language\ntarget_texts = [] # sentence in target language\ntarget_texts_inputs = [] # sentence in target language offset by 1","9224e0d1":"n = 0\nfor line in open('\/kaggle\/input\/hin.txt'):\n    if n!=10:\n        print(line)\n        n+=1","fd4dd620":"# load in the data\nt = 0\nfor line in open('\/kaggle\/input\/hin.txt'):\n  t+=1\n  if t>NUM_SAMPLES:\n    break\n  # input and target are seperated by '\\t'\n  if '\\t' not in line:\n    continue\n  input_text, translation = line.split('\\t')\n  target_text = translation + ' <eos>'\n  target_text_input = '<sos> ' + translation\n  input_texts.append(input_text)\n  target_texts.append(target_text)\n  target_texts_inputs.append(target_text_input)\nprint('num samples:',len(input_texts))","ec16eefe":"tokenizer_inputs = Tokenizer(num_words=MAX_NUM_WORDS)\ntokenizer_inputs.fit_on_texts(input_texts)\ninput_sequences = tokenizer_inputs.texts_to_sequences(input_texts)","0bb56f51":"# word to index mapping for input language\nword2idx_inputs = tokenizer_inputs.word_index\nprint('Found %s unique input tokens.'%len(word2idx_inputs))","924c8a44":"# max length input seq\nmax_len_input = max(len(s) for s in input_sequences)","8cb138fe":"# tokenize the outputs\ntokenizer_outputs = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\ntokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs)\ntarget_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\ntarget_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)","a3f0bb17":"# word to index mapping for output language\nword2idx_outputs = tokenizer_outputs.word_index\nprint('Found %s unique output tokens.'%len(word2idx_outputs))","46014f84":"num_words_output = len(word2idx_outputs)+1","e242b01b":"# max length output seq\nmax_len_target = max(len(s) for s in target_sequences)","31f98588":"# padding the sequences\nencoder_inputs = pad_sequences(input_sequences, maxlen=max_len_input)\nprint('Encoder data shape:',encoder_inputs.shape)\n\ndecoder_inputs = pad_sequences(target_sequences_inputs, maxlen=max_len_target, padding='post')\nprint('Decoder data shape:',decoder_inputs.shape)\n\ndecoder_targets = pad_sequences(target_sequences, maxlen=max_len_target, padding='post')","30c410ca":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip","0aee66d3":"!unzip glove.6B.zip","3686f305":"# loading pre-trained word vectors\nword2vec = {}\nwith open('glove.6B.%sd.txt'%EMBEDDING_DIM) as f:\n  for line in f:\n    values = line.split()\n    word = values[0]\n    vec = np.asarray(values[1:],dtype='float32')\n    word2vec[word] = vec\nprint('Found %s word vectors.'%len(word2vec))","1bb67c6f":"# prepare embedding matrix\nnum_words = min(MAX_NUM_WORDS, len(word2idx_inputs)+1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word2idx_inputs.items():\n  if i < MAX_NUM_WORDS:\n    embedding_vector = word2vec.get(word)\n    if embedding_vector is not None:\n      embedding_matrix[i] = embedding_vector","c1cfb8f0":"# creating embedding layer\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=max_len_input,\n                            trainable=False)","1fb63200":"decoder_targets_one_hot = np.zeros((len(input_texts),\n                                   max_len_target,\n                                   num_words_output),dtype='float32')\nfor i,d in enumerate(decoder_targets):\n  for t,word in enumerate(d):\n    decoder_targets_one_hot[i, t, word] = 1","b10b926d":"# build the model\nencoder_inputs_placeholder = Input(shape=(max_len_input,))\nx = embedding_layer(encoder_inputs_placeholder)\nencoder = LSTM(LATENT_DIM, return_state=True, dropout=0.5)\nencoder_outputs, h, c = encoder(x)","62148ee2":"encoder_states = [h,c] # keeping state to pass into decoder","f091623a":"decoder_inputs_placeholder = Input(shape=(max_len_target,))\ndecoder_embedding = Embedding(num_words_output, LATENT_DIM)\ndecoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)","b04bb9c3":"decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True,\n                    dropout=0.5)\ndecoder_outputs,_,_ = decoder_lstm(decoder_inputs_x,\n                                   initial_state=encoder_states)","3067710f":"decoder_dense = Dense(num_words_output,activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)","d9f6bc4a":"model = Model([encoder_inputs_placeholder,decoder_inputs_placeholder],\n              decoder_outputs)\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","3114da5a":"r = model.fit([encoder_inputs, decoder_inputs],decoder_targets_one_hot,\n              batch_size=BATCH_SIZE,\n              epochs=70,\n              validation_split=0.2)","def6c9cd":"# plot the results\nplt.plot(r.history['loss'],label='loss')\nplt.plot(r.history['val_loss'],label='val_loss')\nplt.legend()\nplt.show()","b3f7171d":"encoder_model = Model(encoder_inputs_placeholder, encoder_states)\n\ndecoder_state_input_h = Input(shape=(LATENT_DIM,))\ndecoder_state_input_c = Input(shape=(LATENT_DIM,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]","84ef2992":"decoder_inputs_single = Input(shape=(1,))\ndecoder_inputs_single_x = decoder_embedding(decoder_inputs_single)","af024b94":"decoder_outputs, h, c = decoder_lstm(decoder_inputs_single_x,\n                                     initial_state=decoder_states_inputs)","b5b6ad15":"decoder_states = [h, c]\ndecoder_outputs = decoder_dense(decoder_outputs)","098e5a7a":"decoder_model = Model([decoder_inputs_single]+decoder_states_inputs,\n                      [decoder_outputs]+decoder_states)","079b7e6f":"# mapping indices back to words\nidx2word_eng = {v:k for k,v in word2idx_inputs.items()}\nidx2word_trans = {v:k for k,v in word2idx_outputs.items()}","d46c4711":"def decode_sequence(input_seq):\n  states_value = encoder_model.predict(input_seq)\n  # generating empty target seq of len 1\n  target_seq = np.zeros((1,1))\n  target_seq[0,0] = word2idx_outputs['<sos>']\n  \n  eos = word2idx_outputs['<eos>']\n  output_sentence = []\n  for _ in range(max_len_target):\n    output_tokens,h,c = decoder_model.predict([target_seq]+states_value)\n    \n    idx = np.argmax(output_tokens[0,0,:])\n    if eos == idx:\n      break\n    \n    word = ''\n    if idx>0:\n      word = idx2word_trans[idx]\n      output_sentence.append(word)\n    \n    target_seq[0,0] = idx\n    states_value = [h,c]\n    \n  return ' '.join(output_sentence) ","0bb5ada0":"i = np.random.choice(len(input_texts))\ninput_seq = encoder_inputs[i:i+1]\ntranslation = decode_sequence(input_seq)\nprint('Input:',input_texts[i])\nprint('Translation:',translation)","c0380a33":"### Neural Machine Translation using seq2seq","e93bf858":"please upvote it if you like it"}}