{"cell_type":{"6571c89f":"code","1c3438c6":"code","f6f0bc9b":"code","e1578aa3":"code","3a669c06":"code","8c5e6990":"code","0a35306f":"code","57f68798":"code","3af15dcd":"code","816b4103":"code","d1657583":"code","876534a9":"code","6c713128":"code","1777671b":"code","39b74779":"code","036d5b6e":"code","3db2aced":"code","6b0a0ccc":"code","6c0a599a":"code","44c7c6e3":"code","e5cf30a3":"code","aaf66705":"code","045a019d":"code","c59cbc98":"code","a45bf2e4":"code","87243fc5":"code","1ed21291":"code","3a785f53":"code","e5c3dc80":"code","cacb1e8a":"code","072c1331":"code","0def7026":"code","5f40d71c":"code","d1949cb6":"code","b5974740":"markdown","223722c1":"markdown","e3669b42":"markdown","3ddd4dcd":"markdown","52e7653a":"markdown","50c8d07e":"markdown","838d79c4":"markdown","0dae8319":"markdown","0c59f222":"markdown","b0911c5a":"markdown","30d6f4c3":"markdown","7a510ae7":"markdown","c34da511":"markdown","f8056b29":"markdown","1a0537ef":"markdown","87544cbe":"markdown","afda9b88":"markdown","553a2e6c":"markdown","d0bd1e84":"markdown","0a2333ac":"markdown","157e4e0a":"markdown","ca583ccb":"markdown","e40875e3":"markdown","186c2240":"markdown","03b39b7b":"markdown","98f7084f":"markdown","0cd545dd":"markdown"},"source":{"6571c89f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c3438c6":"from tensorflow import keras as ks\nfrom matplotlib import pyplot as plt\nimport plotly.graph_objs as go\nimport numpy as np\nimport time\nimport datetime\nimport pandas as pd\nfrom sklearn import model_selection # model assesment and model selection strategies\nfrom sklearn import metrics # model evaluation metrics\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\nfrom sklearn.preprocessing import LabelEncoder,MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.metrics import confusion_matrix","f6f0bc9b":"mnist = ks.datasets.mnist\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()","e1578aa3":"x_train.shape,y_train.shape","3a669c06":"x_train[0].shape","8c5e6990":"print('Train: X=%s, y=%s' % (x_train.shape, y_train.shape))\nprint('Test: X=%s, y=%s' % (x_test.shape, y_test.shape))\n\nfor i in range(9):\n    plt.subplot(330 + 1 + i)\n    plt.imshow(x_train[i], cmap=plt.get_cmap('gray'))\n\nplt.subplots_adjust(hspace = 0.5)\nplt.show()","0a35306f":"for i in range(0, 10):\n    img = x_train[i]\n    label = y_train[i]\n    imgplot = plt.imshow(img, cmap='gray')\n    print('This is a {}'.format(label))\n    plt.show()","57f68798":"dataset=[]\nfor i in x_train:\n    dataset.append(np.reshape(i,(784)).tolist())","3af15dcd":"len(dataset[0])","816b4103":"df_train=pd.DataFrame(dataset)","d1657583":"df_train","876534a9":"dataset=[]\nfor i in x_test:\n    dataset.append(np.reshape(i,(784)).tolist())","6c713128":"df_test=pd.DataFrame(dataset)","1777671b":"df_test","39b74779":"X_train=df_train\nX_test=df_test","036d5b6e":"len (X_train), len(y_train)","3db2aced":"len(X_test), len(y_test)","6b0a0ccc":"dt = DecisionTreeClassifier(max_depth=4,random_state=42)","6c0a599a":"dt.fit(X_train,y_train)","44c7c6e3":"score_train=dt.score(X_train, y_train)\nscore_test=dt.score(X_test, y_test)\nprint('Resultados para: Train: {} - Test: {}'.format(score_train,score_test))","e5cf30a3":"RANDOM_STATE=42\nlista_x=[]\nlista_strain=[]\nlista_stest=[]\nfor x in range(4,20):\n    dt = DecisionTreeClassifier(max_depth=x,random_state=RANDOM_STATE)\n    dt.fit(X_train,y_train)\n    score_train=dt.score(X_train, y_train)\n    score_test=dt.score(X_test, y_test)\n    print('Resultados para max_depth: {} Train: {} - Test: {}'.format(x,score_train,score_test))\n    lista_x.append(x)\n    lista_strain.append(score_train)\n    lista_stest.append(score_test)\nscore_df=pd.DataFrame(index=lista_x, data={'Score_train':lista_strain, 'Score_test':lista_stest})","aaf66705":"# Vamos a pintar los resultados obtenidos para ver con que valor nos quedamos:\nfig = go.Figure()\nfig.update_layout(title_text='Relaci\u00f3n de Score con max_depth:')\nfig.add_trace(go.Scatter(x=score_df.index,y=score_df['Score_train'], mode='lines+markers', name='Score_train'))\nfig.add_trace(go.Scatter(x=score_df.index,y=score_df['Score_test'], mode='lines+markers', name='Score_test'))\nfig.show()","045a019d":"RANDOM_STATE=42\nlista_x=[]\nlista_strain=[]\nlista_stest=[]\nfor i in range(10,100,20):\n    xg_reg = xgb.XGBClassifier(colsample_bytree = 0.3, learning_rate = 0.05, alpha = 10, n_estimators = i, \n                                max_depth=8,random_state=RANDOM_STATE)\n    xg_reg.fit(X_train,y_train)\n    score_train=xg_reg.score(X_train, y_train)\n    score_test=xg_reg.score(X_test, y_test)\n    print('n_stimators: {}. Train: {} - Test: {}'.format(i,score_train,score_test))\n    lista_x.append(i)\n    lista_strain.append(score_train)\n    lista_stest.append(score_test)\nscore_df=pd.DataFrame(index=lista_x, data={'Score_train':lista_strain, 'Score_test':lista_stest})\n","c59cbc98":"# Vamos a pintar los resultados obtenidos para ver con que valor nos quedamos:\nfig = go.Figure()\nfig.update_layout(title_text='Relaci\u00f3n de Score conn_stimators:')\nfig.add_trace(go.Scatter(x=score_df.index,y=score_df['Score_train'], mode='lines+markers', name='Score_train'))\nfig.add_trace(go.Scatter(x=score_df.index,y=score_df['Score_test'], mode='lines+markers', name='Score_test'))\nfig.show()","a45bf2e4":" xg_reg = xgb.XGBClassifier(colsample_bytree = 0.3, learning_rate = 0.05, alpha = 10, n_estimators = 50, \n                                max_depth=8,random_state=RANDOM_STATE)\n","87243fc5":"xg_reg.fit(X_train,y_train)","1ed21291":"predictions = xg_reg.predict(X_test)","3a785f53":"y_test_pred = pd.DataFrame(xg_reg.predict(X_test), index=pd.DataFrame(y_test).index, columns=['numeroPrediccion'])","e5c3dc80":"results_df = pd.DataFrame(y_test).join(y_test_pred)","cacb1e8a":"results_df.columns=['numero','numeroPrediccion']","072c1331":"results_df['Success']=(results_df['numero']==results_df['numeroPrediccion']).astype(int)","0def7026":"results_df","5f40d71c":"results_df['Success'].hist()","d1949cb6":"results_df['Success'].value_counts()","b5974740":"Pintamos unas pocas para hacernos una idea:","223722c1":"Pintamos los exitos:","e3669b42":"Vamos a montar el dataset con las predicciones:","3ddd4dcd":"Vemos el tama\u00f1o de uan de ellas:","52e7653a":"# importamos las librer\u00edas necesarias:","50c8d07e":"Calculamos los exitos en las predicciones:","838d79c4":"Los renombramos a X_train, X_test","0dae8319":"Nos quedamos con max_depth igual 8","0c59f222":"Vemos que con solo 4 niveles d eprofundidad este logra un accuracy de casi un 60%","b0911c5a":"# Entrenamos y predecimos","30d6f4c3":"1. # Metemos ensamble con XGBoost","7a510ae7":"Hacemos los mismo, pero esta vez con un codigo hecho por mi:","c34da511":"Solo ha fallado unos 490 de 10000. Esta bastante bien.","f8056b29":"# Cargamos el Dataset de Mnist:","1a0537ef":"Vamos a tunear un poco el hiperparametro max_depth a ver en cuanto se lo dejamos:\n","87544cbe":"Nos quedamos con 50 stimators","afda9b88":"pintamos los resultados anteriores para verlo mejor:","553a2e6c":"Vemos la estructura de la primera imagen","d0bd1e84":"Hacemos lo mismo con el Dataset de test:","0a2333ac":"Vamos a \"aplanar\" las imagenes pasandolas a un vector. Como la imagen tiene 28x28 pixeles, necesitaremos un vector de 784 pixeles. Hacemos una lista con estas imagenes \"aplanadas\" ","157e4e0a":"Instanciamos un arbol de decision del tipo rclasificador para entrenarlo con el dataset que hemos generado","ca583ccb":"A este Dataset le llamamos df_train","e40875e3":"# \"Aplanamos\" las imagenes","186c2240":"# Entrenamos el modelo","03b39b7b":"Vemos la estructura de estos datasets","98f7084f":"Pintamos para ver con cuantos stimators nos quedamos\n","0cd545dd":"Vemos que tiene buena pinta. Es lo que queriamos."}}