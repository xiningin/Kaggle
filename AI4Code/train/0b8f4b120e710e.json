{"cell_type":{"4f49d2cc":"code","a16e0d3d":"code","d6335c7c":"code","39a92345":"code","e32291c7":"code","8c25154c":"code","da95afb3":"code","134114f9":"code","93930216":"code","18912c0d":"code","341942a7":"code","9519c8c1":"code","0f50824c":"code","6187bfb7":"code","6bfeecce":"code","01f519f3":"code","784a773d":"code","6a4363dd":"code","8f6be12d":"code","228c24d5":"code","11c336aa":"code","91ff6ed3":"code","87d5ebf9":"code","096c180a":"code","933b768c":"code","88ca45ca":"code","5eaaec52":"code","d2b2fdd5":"code","8772b48e":"code","4fdb832c":"code","281a6fe6":"code","2f8ba513":"code","a142059b":"code","c1ce5e47":"code","99e4541d":"markdown","9b7de345":"markdown","586e434a":"markdown","289efa2f":"markdown","bd7923eb":"markdown","d7e07792":"markdown","73cba6fe":"markdown","a59f111f":"markdown","02e67273":"markdown","6e192354":"markdown","5c160cd5":"markdown","5dca2e12":"markdown","49bfbe40":"markdown"},"source":{"4f49d2cc":"# import the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import VarianceThreshold","a16e0d3d":"# import the data\ntrain = pd.read_csv('\/kaggle\/input\/santander-customer-satisfaction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/santander-customer-satisfaction\/test.csv')\n\n# check the shape\ntrain.shape, test.shape","d6335c7c":"train.head()\n# test.head()","39a92345":"# Dividing the dataframe into dependent and independent variable\nX=train.drop(['ID','TARGET'], axis=1)\ny=train['TARGET']\ntest=test.drop(['ID'], axis=1)\n\n# check the shape again\nX.shape, y.shape, test.shape","e32291c7":"# Now we will seperate the data into training and testing dataset\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=99)\n\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","8c25154c":"# Constant and Quasi Constant\nfilter=VarianceThreshold(0.01)\n# this will be used to remove the data which has low variance of 1% or below","da95afb3":"X_train=filter.fit_transform(X_train)\nX_test=filter.transform(X_test)\ntest=filter.transform(test)\n# now check the shape again\nX_train.shape, X_test.shape, test.shape","134114f9":"X_train_T=X_train.T\nX_test_T=X_test.T\ntest_T=test.T","93930216":"# converting the transposed value into a dataframe\nX_train_T=pd.DataFrame(X_train_T)\nX_test_T=pd.DataFrame(X_test_T)\ntest_T=pd.DataFrame(test_T)","18912c0d":"X_train_T\nX_test_T\ntest_T","341942a7":"X_train_T.duplicated().sum()","9519c8c1":"test_T.duplicated().sum()","0f50824c":"duplicated=X_train_T.duplicated()\nduplicated","6187bfb7":"# perform inversion\nfeatures_to_keep=[not index for index in duplicated]\nfeatures_to_keep","6bfeecce":"X_train=X_train_T[features_to_keep].T\nprint(X_train.shape)\n\nX_test=X_test_T[features_to_keep].T\nprint(X_test.shape)\n\ntest=test_T[features_to_keep].T\nprint(test.shape)","01f519f3":"X_train","784a773d":"# scaler\nscaler=StandardScaler()\nX_train=scaler.fit_transform(X_train)\nX_test=scaler.transform(X_test)\ntest=scaler.transform(test)\n\nX_train","6a4363dd":"# check the shape again\nX_train.shape,X_test.shape,test.shape,y_train,y_test,","8f6be12d":"# reshaping\nX_train=X_train.reshape(X_train.shape[0],X_train.shape[1],1)\nX_test=X_test.reshape(X_test.shape[0],X_test.shape[1],1)\ntest=test.reshape(test.shape[0],test.shape[1],1)\n# check the shape again\nX_train.shape, X_test.shape, test.shape","228c24d5":"# series to numpy\ny_train=y_train.to_numpy()\ny_test=y_test.to_numpy()","11c336aa":"# import libraries\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Flatten,Conv1D,MaxPool1D,BatchNormalization,Dropout\nfrom tensorflow.keras.optimizers import Adam","91ff6ed3":"# model\nmodel=Sequential()\n# layers\nmodel.add(Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(252,1)))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(pool_size=2))\nmodel.add(Dropout(0.3))\n\nmodel.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(pool_size=2))\nmodel.add(Dropout(0.5))\n\nmodel.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(pool_size=2))\nmodel.add(Dropout(0.5))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))","87d5ebf9":"# compiling the model\nmodel.compile(optimizer=Adam(learning_rate=0.00005), loss='binary_crossentropy', metrics=['accuracy'])","096c180a":"%%time\n# fittting the model\nhistory=model.fit(X_train,y_train, epochs=10,batch_size=128 ,validation_data=(X_test,y_test))","933b768c":"# plotting the data\npd.DataFrame(history.history).plot(figsize=(10,8))\nplt.grid(True)\nplt.show()","88ca45ca":"# prediction\ny_pred=model.predict_classes(test)\ny_pred","5eaaec52":"# plot confusion matrix\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix","d2b2fdd5":"plot_confusion_matrix?","8772b48e":"mat=confusion_matrix(y_test, y_pred[:15204,])\nplot_confusion_matrix(conf_mat=mat, figsize=(7,7))","4fdb832c":"y_pred.ndim","281a6fe6":"y_pred=np.ravel(y_pred)\ny_pred","2f8ba513":"results = pd.Series(y_pred,name=\"TARGET\")\nresults","a142059b":"submission = pd.concat([pd.Series(range(1,75819),name = \"ID\"),results],axis = 1)\nsubmission","c1ce5e47":"submission.to_csv(\"submission.csv\",index=False)","99e4541d":"Our training data contains 'Target' Column extra ,which is our dependent variable and testing data contains only the independent variables.\n\nAlso the Column ID is of no relevance so will drop it.","9b7de345":"Here all the values which are True are the duplicate ones,\nWe will perform inversion on the values by changing True to False, and False to True.","586e434a":"As the data in y_pred is 2 dimensional, we will convert the same into 1 dim","289efa2f":"Now our data is completely preprocessed , we will Build a model for it","bd7923eb":"we will convert the shape acceptable to our neural network and y_test and y_train into numpy format","d7e07792":"Now we will check how many features are duplicate","73cba6fe":"We can see that earlier we had 369 features, now our features are 268.\n\nNow we will remove the duplicate features as well. We will remove the duplicates by transposing the data , converting the columns into rows and rows into columns","a59f111f":"There is variance in the dataset so we will scale the data","02e67273":"This preprocessing will help into getting a better accuracy for the model.","6e192354":"Now we will remove the duplicates and keep only the unique features and also we will transpose the dataframe again into its original shape","5c160cd5":"We will perform Feature Selection Methods hers.\n\nRemoving Constant, Quasi Constant and Duplicate Features","5dca2e12":"In this file we will perform CNN on the Santander File.\n\nFeature Selection , inversing and transposing the file to achieve a better accuracy","49bfbe40":"Store the duplicates into a variable"}}