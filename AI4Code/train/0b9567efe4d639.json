{"cell_type":{"32d41e11":"code","1e3ffb04":"code","d4bbdf2f":"code","951bd918":"code","49f91ec2":"code","0f5eacb0":"code","23d78732":"code","fe2407e6":"code","b3bb6a91":"code","b0ab36d6":"code","26b67caf":"code","2280d4c0":"code","d6f9a7e8":"code","8bc89a20":"code","28675ef9":"code","d39d4b9e":"code","915538ad":"code","221796f0":"markdown","70d73e81":"markdown","4cd38396":"markdown","c14b9298":"markdown","776cc711":"markdown","b1046971":"markdown","28bb6da5":"markdown","6a678097":"markdown","7f07dba3":"markdown","22d54c29":"markdown","8ae02e16":"markdown","8b88890b":"markdown","aee7a965":"markdown","810eb490":"markdown","2344a318":"markdown","b7e2473f":"markdown","7abedc2d":"markdown","ae90bf06":"markdown","37f03a63":"markdown","a6a4e78b":"markdown","e3b95338":"markdown","ccd6fd70":"markdown"},"source":{"32d41e11":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom random import randint\n%matplotlib inline \nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split\ndef split(df,label):\n    X_tr, X_te, Y_tr, Y_te = train_test_split(df, label, test_size=0.25, random_state=42)\n    return X_tr, X_te, Y_tr, Y_te\n\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold, cross_val_score\n\nclassifiers = ['LinearSVM', 'RadialSVM', \n               'Logistic',  'RandomForest', \n               'AdaBoost',  'DecisionTree', \n               'KNeighbors','GradientBoosting']\n\nmodels = [svm.SVC(kernel='linear'),\n          svm.SVC(kernel='rbf'),\n          LogisticRegression(max_iter = 1000),\n          RandomForestClassifier(n_estimators=200, random_state=0),\n          AdaBoostClassifier(random_state = 0),\n          DecisionTreeClassifier(random_state=0),\n          KNeighborsClassifier(),\n          GradientBoostingClassifier(random_state=0)]\n\n\ndef acc_score(df,label):\n    Score = pd.DataFrame({\"Classifier\":classifiers})\n    j = 0\n    acc = []\n    X_train,X_test,Y_train,Y_test = split(df,label)\n    for i in models:\n        model = i\n        model.fit(X_train,Y_train)\n        predictions = model.predict(X_test)\n        acc.append(accuracy_score(Y_test,predictions))\n        j = j+1     \n    Score[\"Accuracy\"] = acc\n    Score.sort_values(by=\"Accuracy\", ascending=False,inplace = True)\n    Score.reset_index(drop=True, inplace=True)\n    return Score\n\ndef plot(score,x,y,c = \"b\"):\n    gen = [1,2,3,4,5]\n    plt.figure(figsize=(6,4))\n    ax = sns.pointplot(x=gen, y=score,color = c )\n    ax.set(xlabel=\"Generation\", ylabel=\"Accuracy\")\n    ax.set(ylim=(x,y))","1e3ffb04":"def initilization_of_population(size,n_feat):\n    population = []\n    for i in range(size):\n        chromosome = np.ones(n_feat,dtype=np.bool)     \n        chromosome[:int(0.3*n_feat)]=False             \n        np.random.shuffle(chromosome)\n        population.append(chromosome)\n    return population\n\n\ndef fitness_score(population):\n    scores = []\n    for chromosome in population:\n        logmodel.fit(X_train.iloc[:,chromosome],Y_train)         \n        predictions = logmodel.predict(X_test.iloc[:,chromosome])\n        scores.append(accuracy_score(Y_test,predictions))\n    scores, population = np.array(scores), np.array(population) \n    inds = np.argsort(scores)                                    \n    return list(scores[inds][::-1]), list(population[inds,:][::-1]) \n\n\ndef selection(pop_after_fit,n_parents):\n    population_nextgen = []\n    for i in range(n_parents):\n        population_nextgen.append(pop_after_fit[i])\n    return population_nextgen\n\n\ndef crossover(pop_after_sel):\n    pop_nextgen = pop_after_sel\n    for i in range(0,len(pop_after_sel),2):\n        new_par = []\n        child_1 , child_2 = pop_nextgen[i] , pop_nextgen[i+1]\n        new_par = np.concatenate((child_1[:len(child_1)\/\/2],child_2[len(child_1)\/\/2:]))\n        pop_nextgen.append(new_par)\n    return pop_nextgen\n\n\ndef mutation(pop_after_cross,mutation_rate,n_feat):   \n    mutation_range = int(mutation_rate*n_feat)\n    pop_next_gen = []\n    for n in range(0,len(pop_after_cross)):\n        chromo = pop_after_cross[n]\n        rand_posi = [] \n        for i in range(0,mutation_range):\n            pos = randint(0,n_feat-1)\n            rand_posi.append(pos)\n        for j in rand_posi:\n            chromo[j] = not chromo[j]  \n        pop_next_gen.append(chromo)\n    return pop_next_gen\n\ndef generations(df,label,size,n_feat,n_parents,mutation_rate,n_gen,X_train,\n                                   X_test, Y_train, Y_test):\n    best_chromo= []\n    best_score= []\n    population_nextgen=initilization_of_population(size,n_feat)\n    for i in range(n_gen):\n        scores, pop_after_fit = fitness_score(population_nextgen)\n        print('Best score in generation',i+1,':',scores[:1])  #2\n        pop_after_sel = selection(pop_after_fit,n_parents)\n        pop_after_cross = crossover(pop_after_sel)\n        population_nextgen = mutation(pop_after_cross,mutation_rate,n_feat)\n        best_chromo.append(pop_after_fit[0])\n        best_score.append(scores[0])\n    return best_chromo,best_score","d4bbdf2f":"data_bc = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")\nlabel_bc = data_bc[\"diagnosis\"]\nlabel_bc = np.where(label_bc == 'M',1,0)\ndata_bc.drop([\"id\",\"diagnosis\",\"Unnamed: 32\"],axis = 1,inplace = True)\n\nprint(\"Breast Cancer dataset:\\n\",data_bc.shape[0],\"Records\\n\",data_bc.shape[1],\"Features\")","951bd918":"display(data_bc.head())\nprint(\"All the features in this dataset have continuous values\")","49f91ec2":"score1 = acc_score(data_bc,label_bc)\nscore1","0f5eacb0":"logmodel = RandomForestClassifier(n_estimators=200, random_state=0)\nX_train,X_test, Y_train, Y_test = split(data_bc,label_bc)\nchromo_df_bc,score_bc=generations(data_bc,label_bc,size=80,n_feat=data_bc.shape[1],n_parents=64,mutation_rate=0.20,n_gen=5,\n                         X_train = X_train,X_test = X_test,Y_train = Y_train,Y_test = Y_test)","23d78732":"plot(score_bc,0.9,1.0,c = \"gold\")","fe2407e6":"data_pd = pd.read_csv(\"..\/input\/parkinson-disease-detection\/Parkinsson disease.csv\")\nlabel_pd = data_pd[\"status\"]\ndata_pd.drop([\"status\",\"name\"],axis = 1,inplace = True)\n\nprint(\"Parkinson's disease dataset:\\n\",data_pd.shape[0],\"Records\\n\",data_pd.shape[1],\"Features\")","b3bb6a91":"display(data_pd.head())\nprint(\"All the features in this dataset have continuous values\")","b0ab36d6":"score3 = acc_score(data_pd,label_pd)\nscore3","26b67caf":"logmodel = DecisionTreeClassifier(random_state=0)\nX_train,X_test, Y_train, Y_test = split(data_pd,label_pd)\nchromo_df_pd,score_pd=generations(data_pd,label_pd,size=80,n_feat=data_pd.shape[1],n_parents=64,mutation_rate=0.20,n_gen=5,\n                         X_train = X_train,X_test = X_test,Y_train = Y_train,Y_test = Y_test)","2280d4c0":"plot(score_pd,0.9,1.0,c = \"orange\")","d6f9a7e8":"data_pcos = pd.read_csv(\"..\/input\/pcos-dataset\/PCOS_data.csv\")\nlabel_pcos = data_pcos[\"PCOS (Y\/N)\"]\ndata_pcos.drop([\"Sl. No\",\"Patient File No.\",\"PCOS (Y\/N)\",\"Unnamed: 44\",\"II    beta-HCG(mIU\/mL)\",\"AMH(ng\/mL)\"],axis = 1,inplace = True)\ndata_pcos[\"Marraige Status (Yrs)\"].fillna(data_pcos['Marraige Status (Yrs)'].describe().loc[['50%']][0], inplace = True) \ndata_pcos[\"Fast food (Y\/N)\"].fillna(1, inplace = True) \n\nprint(\"PCOS dataset:\\n\",data_pcos.shape[0],\"Records\\n\",data_pcos.shape[1],\"Features\")","8bc89a20":"display(data_pcos.head())\nprint(\"The features in this dataset have both discrete and continuous values\")","28675ef9":"score4 = acc_score(data_pcos,label_pcos)\nscore4","d39d4b9e":"logmodel = RandomForestClassifier(n_estimators=200, random_state=0)\nX_train,X_test, Y_train, Y_test = split(data_pcos,label_pcos)\nchromo_df_pcos,score_pcos=generations(data_pcos,label_pcos,size=80,n_feat=data_pcos.shape[1],n_parents=64,mutation_rate=0.20,n_gen=5,\n                         X_train = X_train,X_test = X_test,Y_train = Y_train,Y_test = Y_test)","915538ad":"plot(score_pcos,0.9,1.0,c = \"limegreen\")","221796f0":"### 1. Looking at dataset","70d73e81":"### 2. Checking Accuracy","4cd38396":"_______\n## Note:\n#### The \"chromo_df\" variable returns a list of np.array where we can see which features were selected in the Genetic algorithm (False represents the dropped features).\n[array([ True,  True,  True, False,  True,  True, False, False, False,                  \n        False,  True,  True,  True, False,  True, False, False,  True,                  \n        False,  True, False,  True,  True,  True,  True,  True,  True,                  \n        False, False, False]),               \n        .                \n        .                   \n        .                  \n        .          \n        ]   \n________","c14b9298":"### Importing the required libraries","776cc711":"#### We can see an improvement of 3-4%","b1046971":"#### Choosing the best classifier for further calculations","28bb6da5":"_____\n# Parkinson's disease\n_____","6a678097":"____\n# PCOS\n____","7f07dba3":"#### We can see an improvement of 1-2%","22d54c29":"### 3. Visualization","8ae02e16":"## Implementation of Genetic Algorithm for Feature Selection\n________\n#### First, we run a function to initialize a random population.\n#### The randomized population is now run through the fitness function, which returns the best parents (highest accuracy).\n#### Selection from these best parents will occur depending on the n-parent parameter.\n#### After doing the same, it will be put through the crossover and mutation functions respectively.\n#### Cross over is created by combining genes from the two fittest parents by randomly picking a part of the first parent and a part of the second parent.\n#### The mutation is achieved by randomly flipping selected bits for the crossover child.\n#### A new generation is created by selecting the fittest parents from the previous generation and applying cross-over and mutation.\n#### This process is repeated for n number of generations.\n______","8b88890b":"____\n### Function Description\n#### 1. split():\nSplits the dataset into training and test set.\n#### 2. acc_score():\nReturns accuracy for all the classifiers.\n#### 3. plot():\nFor plotting the results.\n_____\n### Function Description for Genetic Algorithm\n#### 1. initilization_of_population():\nTo initialize a random population.\n#### 2. fitness_score():\nReturns the best parents along with their score.\n#### 3. selection():\nSelection of the best parents.\n#### 4. crossover():\nPicks half of the first parent and half of the second parent.\n#### 5. mutation():\nRandomly flips selected bits from the crossover child.\n#### 6. generations():\nExecutes all the above functions for the specified number of generations\n____\n### The following 3 datasets are used:\n\n1. Breast Cancer\n2. Parkinson's Disease\n3. PCOS\n_____\n### Plan of action:\n\n* Looking at dataset (includes a little preprocessing)\n* Checking Accuracy (comparing accuracies with the new dataset)\n* Visualization (Plotting the graphs)\n____","aee7a965":"### 3. Visualization","810eb490":"____\n# Breast Cancer\n____","2344a318":"#### We can see an improvement of 5-7%","b7e2473f":"### 2. Checking Accuracy","7abedc2d":"## Genetic Algorithm\n_________\n#### The Genetic Algorithm(GA) is an evolutionary algorithm(EA) inspired by Charles Darwin\u2019s theory of natural selection which espouses Survival of the fittest. As per the natural selection theory, the fittest individuals are selected to produce offsprings. The fittest parents' characteristics are then passed on to their offsprings using cross-over and mutation to ensure better chances of survival. Genetic algorithms are randomized search algorithms that generate high-quality optimization solutions by imitating the biologically inspired natural selection process such as selection, cross-over, and mutation.\n\n### Terminology for Genetic Algorithm\n![](https:\/\/miro.medium.com\/max\/695\/1*vIrsxg12DSltpdWoO561yA.png)\n#### **Population** contains a set of possible solutions for the stochastic search process to begin. GA will iterate over multiple generations till it finds an acceptable and optimized solution. First-generation is randomly generated.\n#### **Chromosome** represents one candidate solution present in the generation or population. A chromosome is also referred to as a Genotype. A chromosome is composed of Genes that contain the value for the optimal variables.\n#### **Phenotype** is the decoded parameter list for the genotype that is processed by the Genetic Algorithm. Mapping is applied to the genotype to convert to a phenotype.\n#### The **Fitness function** or the objective function evaluates the individual solution or phenotypes for every generation to identify the fittest members.\n__________\n### Different Genetic Operators\n#### **Selection** is the process of selecting the fittest solution from a population, and then the fittest solutions act as parents of the next generation of solutions. This allows the next generation to inherit the strong features naturally. Selection can be performed using Roulette Wheel Selection or **Ranked Selection** based on the fitness value.\n\n#### **Cross-over** or recombination happens when genes from the two fittest parents are randomly exchanged to form a new genotype or solution. Cross over can be a One-point cross over or Multi-Point Cross over based on the parent's segments of genes exchanged.\n![image.png](attachment:e240e0f3-60da-44b4-81f7-16bb1e506ff5.png)\n#### Here **One-point Cross-over** is used.\n#### After a new population is created through selection and crossover, it is randomly modified through **mutation**. A **mutation** is a process to modify a genotype using a random process to promote diversity in the population to find better and optimized solutions.\n![](https:\/\/miro.medium.com\/max\/385\/1*bk6zF_rpgGi8IcPIY6fCWg.png)\n______\n### Usage of Genetic Algorithm in Artificial Intelligence\n#### A Genetic Algorithm is used for Search and Optimization using an iterative process to arrive at the best solution out of multiple solutions.\n#### 1. A Genetic Algorithm can find an appropriate set of hyperparameters and their values for a deep learning model to increase its performance in Deep Learning.\n#### 2. A Genetic Algorithm can also be used to determine the best amount of features to include in a machine learning model for predicting the target variable.\n____\n\n### Working of Genetic Algorithm\n![](https:\/\/miro.medium.com\/max\/598\/1*TZ840m0DvghL80GodVGLeQ.png)\n____","ae90bf06":"### 1. Looking at dataset","37f03a63":"### 3. Visualization","a6a4e78b":"#### From looking at these results we can see a greater improvement in accuracy as compared to using methods such as Threshold Variance, Pearson Correlation, and F-score for feature selection.\n#### Link to these methods:\n##### [Variance Threshold](https:\/\/www.kaggle.com\/tanmayunhale\/feature-selection-variance-threshold)\n##### [Pearson Correlation](https:\/\/www.kaggle.com\/tanmayunhale\/feature-selection-pearson-correlation)\n##### [F-score](https:\/\/www.kaggle.com\/tanmayunhale\/feature-selection-f-score)\n#### Reference Paper : [Genetic Algorithm Optimization Algorithm](https:\/\/pub.towardsai.net\/genetic-algorithm-optimization-algorithm-f22234015113)\n","e3b95338":"### 2. Checking Accuracy","ccd6fd70":"### 1. Looking at dataset"}}