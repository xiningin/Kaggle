{"cell_type":{"8dd057ed":"code","f6f35559":"code","81c6fc97":"code","b9f026b5":"code","f2ae5148":"code","ccc00a1f":"code","6a70f6c8":"code","2be8da27":"code","738f36f9":"code","13f6df5d":"code","ecb778e4":"code","86b54f0c":"code","704770bf":"code","455fd424":"code","ace09ad9":"code","c6a2afe3":"code","edffc4a9":"code","81c992f4":"code","ba2ed58e":"code","3dd3d6b1":"code","02ca5a28":"code","066e2b54":"code","59fd74b2":"code","8d995285":"markdown","2b813609":"markdown","6b744558":"markdown","d80c75ef":"markdown","df04c978":"markdown","dd3b8d45":"markdown","fe975b31":"markdown","2460c653":"markdown","9cad4ab3":"markdown","f145ea9e":"markdown","933f4a6a":"markdown","50f6f9e3":"markdown","092e26b6":"markdown","b0678f95":"markdown","21b2f381":"markdown","d5653424":"markdown","512c7bd2":"markdown","c8b96d37":"markdown","486ab60e":"markdown","3dc05b03":"markdown","124caa91":"markdown","b4a61638":"markdown"},"source":{"8dd057ed":"# Import torch\nimport torch\n\n# Create some tensors\nx = torch.empty(5, 3)\nprint(x, '\\n')\n\ny = torch.ones(5, 3, dtype=torch.long)\nprint(y, '\\n')\n\nz = torch.tensor([[0, 1, 2], [3, 4, 5]])\nprint(z)","f6f35559":"# Imports\nimport torch\nimport torch.nn as nn                               # to access build-in functions to build the NN\nimport torch.nn.functional as F                     # to access activation functions\nfrom torchvision import datasets, transforms        # to access the MNIST dataset\nimport torch.optim as optim                         # to build out optimizer\n\nimport numpy as np\nimport matplotlib.pyplot as plt # for plotting\n%matplotlib inline\nimport seaborn as sns\nimport sklearn.metrics","81c6fc97":"# Load in the data from torchvision datasets \n# train=True to access training images and train=False to access test images\n# We also transform to Tensors the images\nmnist_train = datasets.MNIST('data', train = True, download = True, transform=transforms.ToTensor())\nmnist_test = datasets.MNIST('data', train = False, download = True, transform=transforms.ToTensor())","b9f026b5":"# How the object looks:\nprint('Structure of train data:', mnist_train, '\\n')\nprint('Structure of test data:', mnist_test, '\\n')\nprint('Image on index 0 shape:', list(mnist_train)[0][0].shape)\nprint('Image on index 0 label:', list(mnist_train)[0][1])","f2ae5148":"# Check a sample of the images\n# We need to import again, because our train and test data are Tensors already\nsample = datasets.MNIST('data', train=True, download=True)\n\nplt.figure(figsize = (16, 3))\nfor k, (image, label) in enumerate(sample):\n    if k >= 16:\n        break\n    plt.subplot(2, 8, k+1)\n    plt.imshow(image)","ccc00a1f":"# Creating the Network:\nclass MNISTClassifier(nn.Module):                           # nn.Module is a subclass from which we inherit\n    def __init__(self):                                     # Here you define the structure\n        super(MNISTClassifier, self).__init__()             \n        self.layers = nn.Sequential(nn.Linear(28*28, 50),   # Create first layer: from 784 neurons to 50\n                                    nn.ReLU(),              # Call activation function\n                                    nn.Linear(50, 20),      # Second layer: from 50 neurons to 20\n                                    nn.ReLU(),              # Call Activation function\n                                    nn.Linear(20, 10))      # Last layer: from 20 neurons to 10\n        # 10 because we have 10 categories of numbers from which we need to pick 1\n        # If we would have wanted to classify images labeled \"dog\", \"cat\", \"crocodile\",\n           # the final layer would have had 3 neurons.\n        \n    def forward(self, image, prints=False):                 # Function where you take the image though the FNN\n        if prints: print('Image shape:', image.shape)\n        image = image.view(-1, 28*28)                       # Flatten image: from [1, 28, 28] to [784]\n        if prints: print('Image reshaped:', image.shape)\n        out = self.layers(image)                            # Create Log Probabilities\n        if prints: print('Out shape:', out.shape)\n        \n        return out","6a70f6c8":"torch.manual_seed(1) # set the random seed\nnp.random.seed(1) #set random seed in numpy\n\n# Selecting 1 image with its label\nimage_example, label_example = mnist_train[0]\nprint('Image shape:', image_example.shape)\nprint('Label:', label_example, '\\n')\n\n# Creating an instance of the model\nmodel_example = MNISTClassifier()\nprint(model_example, '\\n')\n\n# Creating the log probabilities\nout = model_example(image_example, prints=True)\nprint('out:', out, '\\n')\n\n# Choose maximum probability and then select only the label (not the prob number)\nprediction = out.max(dim=1)[1]\nprint('prediction:', prediction)","2be8da27":"# Creating LOSS and Optimizer instances\n\n# Loss is the function that calculates how far is the prediction from the true value\ncriterion = nn.CrossEntropyLoss()\nprint('Criterion:', criterion, '\\n')\n\n# Using this loss the Optimizer computes the gradients of each neuron and updates the weights\noptimizer = optim.SGD(model_example.parameters(), lr=0.005, momentum=0.9)\nprint('Optimizer:', optimizer)","738f36f9":"# Let's also look at how many parameters (weights and biases) are updating during 1 single backpropagation\n# Parameter Understanding\nfor i in range(6):\n    print(i+1, ':', list(model_example.parameters())[i].shape)","13f6df5d":"torch.manual_seed(1) # set the random seed\nnp.random.seed(1) #set random seed in numpy\n\nprint('Log Probabilities:', out)\nprint('Actual value:', torch.tensor(label_example).reshape(-1))\n\n# Clear gradients - always needs to be called before backpropagation\noptimizer.zero_grad()\n# Compute loss\nloss = criterion(out, torch.tensor(label_example).reshape(-1))\nprint('Loss:', loss)\n# Compute Gradients\nloss.backward()\n# Update weights\noptimizer.step()\n\n# After this 1 iteration the weights have updated once","ecb778e4":"# Create trainloaders for train and test data\n# We put shuffle=True so the images shuffle after every epoch\ntrain_loader = torch.utils.data.DataLoader(mnist_train, batch_size=60, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(mnist_test, batch_size=10, shuffle=True)\n\n# Inspect Trainloader\nprint('trainloader object:', train_loader, '\\n')\n\n# Select First Batch\nimgs, labels = next(iter(train_loader))\n\nprint('Object shape:', imgs.shape)      # [60, 1, 28, 28]: 60 images of size [1, 28, 28]\nprint('Label values:', labels)          # actual labels for the 60 images\nprint('Total Inages:', labels.shape)    # 60 labels in total","86b54f0c":"torch.manual_seed(1) # set the random seed\nnp.random.seed(1) #set random seed in numpy\n\nn = 0\nfor k, (images, labels) in enumerate(train_loader):\n    # Stop after 3 iterations\n    if k >= 3:\n        break\n    \n    print('========== Batch', k, ':')\n    # Prediction:\n    out = model_example(images)\n    print('out shape:', out.shape)\n    \n    # Update weights (or parameters):\n    loss = criterion(out, labels)\n    print('loss:', loss)\n    \n    print('Optimizing...')\n    # Computes the gradient of current tensor\n    loss.backward()\n    # Performs a single optimization step.\n    optimizer.step()\n    # Clears the gradients of all optimized\n    optimizer.zero_grad()\n    print('Done.')    \n    \n    if k<2: print('\\n')","704770bf":"# Instantiate 2 variables for total cases and correct cases\ncorrect_cases = 0\ntotal_cases = 0\n\n# Sets the module in evaluation mode (VERY IMPORTANT)\nmodel_example.eval()\n\nfor k, (images, labels) in enumerate(train_loader):\n    # Just show first 3 batches accuracy\n    if k >= 3: break\n    \n    print('==========', k, ':')\n    out = model_example(images)\n    print('Out:', out.shape)\n    \n    # Choose maximum probability and then select only the label (not the prob number)\n    prediction = out.max(dim = 1)[1]\n    print('Prediction:', prediction.shape)\n    \n    # Number of correct cases - we first see how many are correct in the batch\n            # then we sum, then convert to integer (not tensor)\n    correct_cases += (prediction == labels).sum().item()\n    print('Correct:', correct_cases)\n    \n    # Total cases\n    total_cases += images.shape[0]\n    print('Total:', total_cases)\n    \n    \n    if k < 2: print('\\n')\n        \n\nprint('Average Accuracy after 3 iterations:', correct_cases\/total_cases)","455fd424":"def get_accuracy(model, data, batchSize = 20):\n    '''Iterates through data and returnes average accuracy per batch.'''\n    # Sets the model in evaluation mode\n    model.eval()\n    \n    # Creates the dataloader\n    data_loader = torch.utils.data.DataLoader(data, batch_size=batchSize)\n    \n    correct_cases = 0\n    total_cases = 0\n    \n    for (images, labels) in iter(data_loader):\n        # Is formed by 20 images (by default) with 10 probabilities each\n        out = model(images)\n        # Choose maximum probability and then select only the label (not the prob number)\n        prediction = out.max(dim = 1)[1]\n        # First check how many are correct in the batch, then we sum then convert to integer (not tensor)\n        correct_cases += (prediction == labels).sum().item()\n        # Total cases\n        total_cases += images.shape[0]\n    \n    return correct_cases \/ total_cases","ace09ad9":"def train_network(model, train_data, test_data, batchSize=20, num_epochs=1, learning_rate=0.01, weight_decay=0,\n                 show_plot = True, show_acc = True):\n    \n    '''Trains the model and computes the average accuracy for train and test data.\n    If enabled, it also shows the loss and accuracy over the iterations.'''\n    \n    print('Get data ready...')\n    # Create dataloader for training dataset - so we can train on multiple batches\n    # Shuffle after every epoch\n    train_loader = torch.utils.data.DataLoader(dataset=train_data, batch_size=batchSize, shuffle=True)\n    \n    # Create criterion and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=weight_decay)\n    \n    # Losses & Iterations: to keep all losses during training (for plotting)\n    losses = []\n    iterations = []\n    # Train and test accuracies: to keep their values also (for plotting)\n    train_acc = []\n    test_acc = []\n    \n    print('Training started...')\n    iteration = 0\n    # Train the data multiple times\n    for epoch in range(num_epochs):\n        \n        for images, labels in iter(train_loader):\n            # Set model in training mode:\n            model.train()\n            \n            # Create log probabilities\n            out = model(images)\n            # Clears the gradients from previous iteration\n            optimizer.zero_grad()\n            # Computes loss: how far is the prediction from the actual?\n            loss = criterion(out, labels)\n            # Computes gradients for neurons\n            loss.backward()\n            # Updates the weights\n            optimizer.step()\n            \n            # Save information after this iteration\n            iterations.append(iteration)\n            iteration += 1\n            losses.append(loss)\n            # Compute accuracy after this epoch and save\n            train_acc.append(get_accuracy(model, train_data))\n            test_acc.append(get_accuracy(model, test_data))\n            \n    \n    # Show Accuracies\n    # Show the last accuracy registered\n    if show_acc:\n        print(\"Final Training Accuracy: {}\".format(train_acc[-1]))\n        print(\"Final Testing Accuracy: {}\".format(test_acc[-1]))\n    \n    # Create plots\n    if show_plot:\n        plt.figure(figsize=(10,4))\n        plt.subplot(1,2,1)\n        plt.title(\"Loss Curve\")\n        plt.plot(iterations[::20], losses[::20], label=\"Train\", linewidth=4, color='#008C76FF')\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Loss\")\n\n        plt.subplot(1,2,2)\n        plt.title(\"Accuracy Curve\")\n        plt.plot(iterations[::20], train_acc[::20], label=\"Train\", linewidth=4, color='#9ED9CCFF')\n        plt.plot(iterations[::20], test_acc[::20], label=\"Test\", linewidth=4, color='#FAA094FF')\n        plt.xlabel(\"Iterations\")\n        plt.ylabel(\"Accuracy\")\n        plt.legend(loc='best')\n        plt.show()","c6a2afe3":"# Select images\nmnist_data = datasets.MNIST('data', train = True, download=True, transform=transforms.ToTensor())\nmnist_data = list(mnist_data)\n\n# Training and Testing selection\nmnist_train = mnist_data[:500]     # 500 training images\nmnist_test = mnist_data[500:1000]  # 500 test images\n\n# Create Model Instance\nmodel1 = MNISTClassifier()\n\n# Train...\ntrain_network(model1, mnist_train, mnist_test, num_epochs=200)","edffc4a9":"# Predefined Function that shows 20 images\ndef show20(data, title='Default'):\n    plt.figure(figsize=(10,2))\n    for n, (image, label) in enumerate(data):\n        if n >= 20:\n            break\n        plt.subplot(2, 10, n+1)\n        plt.imshow(image)\n        plt.suptitle(title, fontsize=15);\n        \n\n# Create original and rotated set\noriginal_images = datasets.MNIST('data', train=True, download=True)\nrotated_images = datasets.MNIST('data', train=True, download=True, \n                                transform=transforms.RandomRotation(25, fill=(0,)))\n\n#Show images\nshow20(original_images, 'Original')\nshow20(rotated_images, 'Rotated')","81c992f4":"# Creating a personalized transform\n# First Rotates, then transforms to tensor, then normalizes the images\nmytransform = transforms.Compose([transforms.RandomRotation(25, fill=(0,)),\n                                  transforms.ToTensor(),\n                                 transforms.Normalize([0.5], [0.5])])\n\n# Import the MNIST data aplying the transformations\nmnist_data_aug = datasets.MNIST('data', train = True, download=True, transform=mytransform)\nmnist_data_aug = list(mnist_data_aug)\n\n# We select first 500 images as our training\nmnist_train_aug = mnist_data_aug[:500]\n\n# ------ Training the model ------\n# Create Model Instance\nmodel_aug = MNISTClassifier()\n\n# Train...\ntrain_network(model_aug, mnist_train_aug, mnist_test, num_epochs=200)","ba2ed58e":"# Create Model Instance\nmodel2 = MNISTClassifier()\n\n# Train...\ntrain_network(model2, mnist_train, mnist_test, num_epochs=200, learning_rate=0.001, weight_decay=0.0005)","3dd3d6b1":"class MNISTClassifier_improved(nn.Module):\n    def __init__(self, layer1_size=50, layer2_size=20, dropout=0.4):       # Structure of the FNN \n        super(MNISTClassifier_improved, self).__init__()\n        \n        self.layers = nn.Sequential(nn.Dropout(p = dropout),               # Dropout for first layer\n                                    nn.Linear(28*28, layer1_size),         # From 784 neurons to layer1_size\n                                    nn.ReLU(),                             # Activation Function\n                                    nn.Dropout(p = dropout),               # Dropout for second layer\n                                    nn.Linear(layer1_size, layer2_size),   # From layer1_size neurons to layer2_size\n                                    nn.ReLU(),                             # Activation Function\n                                    nn.Dropout(p = dropout),               # Dropout for last layer\n                                    nn.Linear(layer2_size, 10))            # Output layer\n        \n    def forward(self, image):                # Taking the image through the NN\n        image = image.view(-1, 28*28)        # Flatten the matrix to a vector\n        out = self.layers(image)             # Log Probabilities output\n        \n        return out","02ca5a28":"# Training on the newly network:\n# Create Model Instance\nmodel_improved = MNISTClassifier_improved(layer1_size=80, layer2_size=50, dropout=0.5)\nprint(model_improved)\n\n# Train...\ntrain_network(model_improved, mnist_train, mnist_test, num_epochs=200)","066e2b54":"def get_confusion_matrix(model, test_data):\n    # First we make sure we disable Gradient Computing\n    torch.no_grad()\n    \n    # Model in Evaluation Mode\n    model.eval()\n    \n    preds, actuals = [], []\n\n    for image, label in mnist_test:\n        # Add 1 more dimension for batching\n        image = image.unsqueeze(0)\n        out = model_improved(image)\n\n        prediction = torch.max(out, dim=1)[1].item()\n        preds.append(prediction)\n        actuals.append(label)\n    \n    return sklearn.metrics.confusion_matrix(preds, actuals)","59fd74b2":"plt.figure(figsize=(16, 5))\nsns.heatmap(get_confusion_matrix(model_improved, mnist_test), cmap='icefire', annot=True, linewidths=0.1)\nplt.title('Confusion Matrix', fontsize=15);","8d995285":"The prediction is wrong! Keep in mind the model is NOT trained yet, so of course the prediction is not accurate.\n\n## 5.3 Backpropagation\n\nSo, the purpose is to UPDATE the weights and biases in the neural network so it *learns* to recognize the digits and accurately classify them. This is done during backpropagation, when the model literally goes back and updates the parameters (weights) a little bit. Before going any further, I highly recommend watching the following video which explains the concept of Backpropagation.\n\n<div class=\"alert alert-block alert-info\">\n<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/ba\/3B1B_Logo.png' width='50' align='left'><\/img>\n<p><a href='https:\/\/www.youtube.com\/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3'>What is Backpropagation really doing?<\/a><\/p>\n<p>Cheers again to 3Blue1Brown for his amazing structured videos.<\/p>\n<\/div>\n\n### 5.3.1 Loss and Optimizer Functions:\n\nThese 2 are like brother and sister: work hand in hand during the neural network training. They change by the case, but their main purpose is the same:\n\n**Loss Function (`criterion`): given an output and an actual, it computes the difference between them**\n* Regressive loss functions:\n    * MAE: `torch.nn.L1Loss()`\n    * MSE: `torch.nn.MSELoss()` etc.\n* Classification loss functions:\n    * Cross Entropy Loss: `torch.nn.CrossEntropyLoss()`\n    * Binary Cross Entropy Loss: `torch.nn.BCELoss()` etc.\n* Embedding Loss functions (whether 2 inputs are similar or not):\n    * Hinge Loss: `torch.nn.HingeEmbeddingLoss()`\n    * Cosine Loss: `torch.nn.CosineEmbeddingLoss()` etc.\n\n**Optimizer Function (`torch.optim`): updates the weights and biases to REDUCE the loss**\n* Examples:\n    * Stochastic Gradient Descent: `SGD()`\n    * Adam: `Adam()`\n    * Adagrad: `Adagrad()`\n    \nDifferent neural networks and purposes can require different loss and optimizer functions. Click [here](https:\/\/pytorch.org\/docs\/stable\/nn.html) to check all of them.","2b813609":"### 6.4.2 Predefined Training Function\n\n<img src='https:\/\/i.imgur.com\/S1miUl0.png' width=600>","6b744558":"## 6.2 Accuracy of the Classifier\nDuring Training, we would usually want to check for the accuracy of the model, to see how good or how bad is performing.\n\n<div class=\"alert alert-block alert-warning\"> \n<strong>Note<\/strong>: During <strong>training<\/strong>, it is highly important to set the model into training mode by calling <code>your_model.train()<\/code>. This enables gradients training, the Dropout() function etc. When you <strong>evaluate<\/strong> the model call <code>your_model.eval()<\/code>. This disables the gradients, Dropout() function etc and sets the model in evaluation mode.\n<\/div>","d80c75ef":"### 5.3.3 Do 1 backpropagation: Compute the LOSS and OPTIMIZE for our images_example:","df04c978":"That was a lot. Please, bear with me.\n\n## 5.1 Activation Functions\nAn activation function is a fancy way of saying that we are making the output of each neuron *nonlinear*, because we WANT to learn non-linear relationships between the input and the output.\n\nThere are maaany types of activation functions, but some of them are:\n\n### Rectifier Linear Unit (ReLu)\nThe function is linear when the activation is above zero, and is equal to zero otherwise.\n<img src=\"https:\/\/miro.medium.com\/max\/1026\/0*g9ypL5M3k-f7EW85.png\" width=\"350\">\n\n### Sigmoid\nThe sigmoid function has a tilted \"S\" shape, and its output is always between 0 and 1. They are *interpreted as probabilities* (probability of input to be digit 1, probability of input to be digit 2 etc.).\n<img src=\"https:\/\/miro.medium.com\/max\/4000\/1*JHWL_71qml0kP_Imyx4zBg.png\" width=\"350\">\n\n### Tanh\nA variation of the Sigmoid, but it outputs values between -1 and 1.\n<img src=\"https:\/\/mathworld.wolfram.com\/images\/interactive\/TanhReal.gif\" width=\"300\">\n\nThese Activation Functions squish the neuron's output between the 2 values, preventing big numbers becoming much bigger and small numbers becoming much smaller.\n\n## 5.2 Making a Forward Pass\n\nA forward pass is when you take the images one by one (or batch by batch, we'll come back to this) and we put them through the neural network, which outputs for each a log probability (10 in out case).\n\nLet's look at 1 example:\n\n<img src='https:\/\/i.imgur.com\/ywMFtDz.png' width='600'>","dd3b8d45":"# 8. Overfitting\n\nAs any other Machine Learning Model, Neural Nets can suffer from overfitting. Overfitting is when a neural network model learns about the quirks of the training data, rather than information that is generalizable to the task at hand.\n\n## 8.1 Data Augmentation\nWhy try to collect more data when you can create some on your own? *Data Augmentation* generates more data points from our existing data set by:\n* Flipping each image horizontally or vertically (won't work for digit recognition, but might for other tasks)\n* Shifting each pixel a little to the left or right\n* Rotating the images a little\n* Adding noise to the image\n\n<img src='https:\/\/www.kdnuggets.com\/wp-content\/uploads\/cats-data-augmentation.jpg' width='400'>\n\nFor our example we'll rotate the images randomly up to 35 degrees.","fe975b31":"## 8.3 Dropout() and Layer Optimization\n\n### 8.3.1 Dropout() Function\nThis technique builds *many* models and then averages their prediction at test time (this is why it is very important to call `model.eval()` when we want to evaluate).\n\nFor each model we **dropout** (drop out, zero out, remove etc.) a portion of neurons from each training iteration. Hence, in different iterations of training, we will drop out a different set of neurons.\n\nThis way we prevent the weight from being overly dependent on eachother: for example for one weight to be unnecessarily large to compensate for another unnecessarily large weight with the opposite sign. In other words, weights are encouraged to be *strong and independent*.\n<img src='https:\/\/miro.medium.com\/max\/1200\/1*iWQzxhVlvadk6VAJjsgXgg.png' width=400>\n\n\n### 8.3.2 Layer Optimization\nOur `MNISTClassifier()` had until now 3 layers with a fixed number on neurons in each layer. We can change that by making it changable during training, so eventually we can apply `Grid Search` and find the best combination possible.\n\n### 8.3.3 Changing the Structure of our MNISTClassifier()\nNow let's change our Neural Net a bit:\n* `nn.Dropout(p=0.4)`: each neuron has 40% chance of being dropped\n* `layer1_size`: size of the first hidden layer\n* `layer2_size`: size of the second hidden layer","2460c653":"# Other \"How I taught myself Deep Learning\" Notebooks\n* [How I taught myself Deep Learning: ConvNet (CNNs)](https:\/\/www.kaggle.com\/andradaolteanu\/how-i-taught-myself-deep-learning-convnet-cnns)\n* [How I taught myself Deep Learning: Recurrent NNs](https:\/\/www.kaggle.com\/andradaolteanu\/how-i-taught-myself-deep-learning-recurrent-nns)\n\nIf you have any questions, please do not hesitate to ask. This notebook is made to bring more clear understanding of concepts and coding, so this would also help me add, modify and improve it. \n\n<div class=\"alert alert-block alert-warning\"> \n<p>If you liked this, upvote!<\/p>\n<p>Cheers!<\/p>\n<\/div>","9cad4ab3":"Until now we:\n1. Created a Vanilla FNN\n2. Took 1 image through the network and create prediction\n3. Look at the prediction vs actual and computed the loss\n4. Using the loss we updated the weights and biases\n\nThis is called training. The next chapters will be dedicated to training the network and improving it.\n\n# 6. Training the Neural Network\nOur purpose now that we have the structure in place and the data is to make the Vanilla FNN perform well.\n\n## 6.1 Batches\nWith an artificial neural network, we may want to use more than one image at one time. That way, we can compute the *average* loss across a **mini-batch** of **multiple** images, and take a step to optimize the **average** loss. The average loss across multiple training inputs is going to be less \"noisy\" than the loss for a single input, and is less likely to provide \"bad information\" because of a \"bad\" input.\n\nBatches can have different sizes:\n* one extreme is `batch_size` = 1: meaning that we compute the loss and update after EACH image (so we have 60,000 batches of size 1)\n* a `batch_size` = 60: means that, for 60,000 training images, we'll have 1000 batches of size 60\n* the other extreme is `batch_size` = 60,000: when we input ALL images and do 1 backpropagation (we have 1 batch of size 60,000 images)\n\nThe actual batch size that we choose depends on many things. We want our batch size to be large enough to not be too \"noisy\", but not so large as to make each iteration too expensive to run.\n\n<img src='https:\/\/i.imgur.com\/M6ZkRXa.png' width='400'>\n\nIn the above example, instead of having 70 noisy losses we'll have just 7 averaged losses.","f145ea9e":"## 8.2 Weight Decay and Learning Rate\n\n**Weight Decay**: The idea of weight decay is to *penalize large weights*. Large weights mean that the prediction relies heavily on the content of one or multiple pixels. So, we penalize them by adding and extra term to the `criterion` function.\n\n**Learning Rate**: This one is probably not new. In FNNs we train using gradient descent to update the weights. The learning rate *controls* [how much to change the model in response to the estimated error each time the model weights are updated](https:\/\/machinelearningmastery.com\/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks\/). If we choose a lr too large we might overshoot the local minima, while using a lr too small we might wait longer for the model to train, as the steps are tinier.\n\n<img src='https:\/\/srdas.github.io\/DLBook\/DL_images\/TNN2.png' width='400'>","933f4a6a":"# 3. What are Neural Networks?\n\n## 3.1 Youtube Videos that will save you time:\n\nThere are 2 **super informative** videos on Youtube from *3Blue1Brown* that explain very well what neural networks are and what is an FNN: Feed Foward Neural Network. I **highly recommend** taking ~20 minutes and  watching them before going any further (it is always better to visualise than rather read to understand abstract concepts). \n\n<div class=\"alert alert-block alert-info\">\n<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/ba\/3B1B_Logo.png' width='50' align='left'><\/img>\n<p><a href='https:\/\/www.youtube.com\/watch?v=aircAruvnKk&t=1007s'>What are Neural Networks?<\/a><\/p>\n<p><a href='https:\/\/www.youtube.com\/watch?v=IHZwWFHWa-w'>How do Neural Networks learn?<\/a><\/p>\n<\/div>\n\n## 3.2 The Perceptron:\n\nA **Perceptron** is a single layer neural network, while a **Multi Layer Perceptron** is called a Neural Network. \n\nI **highly suggest** reading [this blog post](https:\/\/towardsdatascience.com\/what-the-hell-is-perceptron-626217814f53) for some very good explanations.\n\n<img src = 'https:\/\/i.imgur.com\/IHgw2au.png' width='400'>\n\n## 3.3 How does a plain Vanilla Neural Network look?\nPlain vanilla Neural Networks (or Feed Forward Neural Networks, FNNs for the lazy people \ud83d\ude0e) have the most simple architecture in the Neural Networks realm, but their basics will help you understand much more complicated dinosaurs moving on.\n\n> Pro Tip: Use [this site](http:\/\/alexlenail.me\/NN-SVG\/index.html) to construct your own FNN (it can be dreadful to draw lines by yourself).\n\n<img src='https:\/\/i.imgur.com\/wqHC1T4.png' width='600'>\n\nWe have 1 example of an FNN: our network is already trained to receive an B&W image and create the `output` 1 if the image is a cat and 0 otherwise. Therefore, the FNN is a Classifier. Let's break down the steps:\n1. Input: An image of a cat with `height` = 10 pixels, `length` = 20 pixels and `channels` = 1 (because is B&W; for RGB images, the number of channels is 3).\n2. The image (composed by numbers as you saw in the above videos) is therefore a matrix of shape `[1, 10, 20]`.\n3. Layer1: Because the input in the FNN must be **linear**, the matrix is **vectorized** by reshaping it into a vector of size `[200]`.\n4. Layer2: the first layer is multiplied by the weights and formes the second layer, composed by `[300]` neurons\n5. Layer3: The last hidden layer is composed of `[400]` neurons\n6. Output: It is composed of 1 single neuron, which **fires** the value 1 if the image is a cat or 0 otherwise.\n\n## 3.4 Why so deep? Deep vs Shallow Networks\n\nFirst of all, the FNN isn't really doing anything special that a simple ML can't achieve. The beauty in Deep Learning is actually the amount of data and computations that can be handled much better than in an usual ML algorithm.\n\nIn Feed Foward Neural Nets, the hidden layers gradually \/increase\/decrease in hidden size (number of neurons) so more and more **details** of the input (images, text etc.) can be grasped.\n\n<img src='https:\/\/i.imgur.com\/D8QhLWM.png' width=350>\n\nIt is known that Deep Neural Nets (thin and tall) are **better** than Shallow ones (fat and short). This happens because the deep ones can learn more and more abstract representations the *deeper* you go. Also, the number of parameters is smaller so the training is faster.\n\n... Let's start programming.\n\n# 4. The Data - MNIST\n\nWe'll be working in MNIST Dataset, which is usually the go-to dataset when starting in Neural Networks. Nevertheless, you can apply the following principles on any datasets (images, text, tabular data, audio data), as **all data can be represented in numbers**. Cuz this is just it: numbers.","50f6f9e3":"# 1. Introduction\n\nThis notebook is just me being frustrated on **deep learning** and trying to understand in \"baby steps\" what is going on here. For somebody that starts in this area with no background whatsoever it can be very confusing, especially because I seem to be unable to find code with many explanations and comments.\n\nSo, if you are frustrated just like I was when I started this stuff I hope the following guidelines will help you. I am by no means a teacher, but in this notebook I will:\n1. Share articles\/videos I watched that TRULY helped\n2. Explain code along the way to the best of my ability\n\n<div class=\"alert alert-block alert-warning\"> \n<strong>Note<\/strong>: Deep learning coding is VERY different in structure than the usual <em>sklearn<\/em> for machine learning. In addition, it usually works with <em>images<\/em> and <em>text<\/em>, while <em>ML<\/em> usually works with <em>tabular<\/em> data. So please, be patient with yourself and if you don't understand something right away, continue reading\/ coding and it will all make sense in the end.\n<\/div>\n\n# 2. Before we start\n\n> This is my first notebook in the \"series\": **How I taught myself Deep Learning**.\n1. **[How I taught myself Deep Learning: ConvNet (CNNs)](https:\/\/www.kaggle.com\/andradaolteanu\/how-i-taught-myself-deep-learning-convnet-cnns)**\n        * Why ConvNets\n        * Convolutions Explained\n        * Computing Activation Maps\n        * Kernels, Padding, Stride\n        * AlexNet\n        * MNIST Classification using Convolutions\n2. **[How I taught myself Deep Learning: Recurrent NNs](https:\/\/www.kaggle.com\/andradaolteanu\/how-i-taught-myself-deep-learning-recurrent-nns)**\n        * 1 Layer RNNs\n        * Multiple Neurons RNN\n        * Vanilla RNN for MNIST Classification\n        * Multilayer RNNs\n        * Tanh Activation Function\n        * Multilayer RNN for MNIST\n        * LSTMs and Vanishing Gradient Problem\n        * Bidirectional LSTMs\n        * LSTM for MNIST Classification      \n\n<img src='https:\/\/miro.medium.com\/max\/1200\/1*4br4WmxNo0jkcsY796jGDQ.jpeg' width=200>\n`Pytorch`: This is the library we will be using; it is allegedly much easier than `Keras` and it's starting to make a breakthrough nowadays. Pytorch has a *different structure* than the normal *machine learning* code in *sklearn*. So be aware!\n\n`Tensors`: Instead of working with tabular data or `numpy` arrays, we'll be working with tensors. [A tensor is a container which can house data in N dimensions](https:\/\/www.kdnuggets.com\/2018\/05\/wtf-tensor.html).\n\n<img src='https:\/\/hadrienj.github.io\/assets\/images\/2.1\/scalar-vector-matrix-tensor.png' width='300'>\n\nSo, to not complicate ourselves, `tensors` are *very* similar with `numpy arrays`, but they offer much better GPU support, so they are faster. Let's take a look:","092e26b6":"# 5. Vanilla FNN Neural Network\n\nWhen working with neural networks you actually NEED to define a class for yourself. For example, when you're building a `RandomForestClassifier()`, sklearn already has that made for you. So, you just *call* the object from the library and afterwards just fine tune the hyperparameters.\n\nFor Neural Networks is different: they can be so volatile, depending on the structure of your input (eg. an image of shape `[3, 500, 250]`), number of `hidden layers`, number of `neurons` in each hidden layer, weather or not you want to call the `Dropout()` functions etc. You can also build multiple neural networks and then combine them in another one (for example in a Sequence2Sequence RNN).\n\nSo, here we will create our own neural net:\n\n>Note: you can ignore the `prints`, I usually put them to understand what the network does when you click *run*\n\n> Note2: `super()` function is there because the `MNISTClassifier` class inherits attributes from it's parent class `nn.Module`. By calling this function we make this possible. Removing it would lead to an *error*.","b0678f95":"#### Improved Model Structure:\n<img src='https:\/\/i.imgur.com\/m22zEqN.png' width='600'>","21b2f381":"# 9. Bonuses:\n## 9.1 Confusion Matrix","d5653424":"# References:\n* [Create your own FNN](http:\/\/alexlenail.me\/NN-SVG\/index.html)\n* [WTF is a Tensor?](https:\/\/www.kdnuggets.com\/2018\/05\/wtf-tensor.html)\n* [What the hell is a Perceptron?](https:\/\/towardsdatascience.com\/what-the-hell-is-perceptron-626217814f53)\n* 3Blue1Brown videos:\n    * [But what is a Neural Network?](https:\/\/www.youtube.com\/watch?v=aircAruvnKk&t=1007s)\n    * [What is Backpropagation really doing?](https:\/\/www.youtube.com\/watch?v=Ilg3gGewQ5U&list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&index=3)\n    * [Gradient Descent, how Neural Networks learn](https:\/\/www.youtube.com\/watch?v=IHZwWFHWa-w)\n* [All `torch.` functions (including loss & optimizer functions)](https:\/\/pytorch.org\/docs\/stable\/nn.html)\n* [Impact of Learning Rate in NNs](https:\/\/machinelearningmastery.com\/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks\/)","512c7bd2":"### 8.1.1 Training on Augmented Data:\n\n* `transforms.Normalize()`: means to scale the input features of a neural network, so that all features are scaled similarly. For images is not really necessary, as they all have the same structure, but I threw it here just for reference.","c8b96d37":"![](https:\/\/i.imgur.com\/qQP7mg3.png)","486ab60e":"### 5.3.2 MNISTClassifier trainable parameters:\n* 1 : torch.Size([50, 784]) - 50 weights (or parameters) for each 28x28 neurons (28x28x50 weights in total)\n* 2 : torch.Size([50]) - 50 biases\n* 3 : torch.Size([20, 50]) - 20 weights (or parameters) for each 50 neurons (50x20 weights in total)\n* 4 : torch.Size([20]) - 20 biases\n* 5 : torch.Size([10, 20]) - 10 weights (or parameters) for each 20 neurons (10x20 weights in total)\n* 6 : torch.Size([10]) - value of the final neurons (the log probabilities)","3dc05b03":"### 6.1.1 Training the Example Network on a batch instead of image by image:","124caa91":"# 7. Model Evaluation\n\nNow that we have our functions ready, we can start training on the ENTIRE dataset.\n\nBut first, to make the training faster, we will:\n* select 500 training images and 500 testing images\n* `batch_size` will be by default set to 20 images\/batch\n* we'll iterate through the data 200 times (`num_epochs`=200)","b4a61638":"## 6.3 Iterations vs Epochs:\n\n**Iterations**: number of iterations is the number of times we *update* the weights (parameters) of the FNN. For example, above we did 3 iterations.\n\n**Epoch**: number of times *all* training data was used once to update the parameters. This is used because, in general, we would want to train the network for longer. Until now in this notebook we haven't completed yet a full epoch.\n\n## 6.4 Predefined Functions: Accuracy and Training Loop\nNow let's create some functions, so our trainin process will become easier:\n\n### 6.4.1 Predefined Accuracy Function:"}}