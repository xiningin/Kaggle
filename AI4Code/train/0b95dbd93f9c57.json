{"cell_type":{"eea7b302":"code","6adefaeb":"code","1e9c746c":"code","9a4d6dc7":"code","7d24537e":"code","dab85270":"code","5c6b55e0":"code","12f068ee":"code","72809941":"code","776d7657":"code","ec918a1b":"code","caae6a8d":"code","20d294d2":"code","68fd5409":"code","bcc748c6":"code","0a24270a":"code","faa2ab22":"code","bb920baf":"code","142f23c4":"code","2eb34aa0":"code","5f1d7a1f":"code","d7e785d2":"code","eafe474a":"code","1db34f6f":"code","91bf7390":"code","463d3f90":"code","cf82cdfb":"markdown","9f9cfd94":"markdown","c1d6f218":"markdown","d927e046":"markdown","60b2e4f7":"markdown","72100ee4":"markdown","8abf92ad":"markdown","75e1d75d":"markdown","92dbeefd":"markdown","7c78cd64":"markdown","b7e42075":"markdown","ffa9d297":"markdown","5ca077d7":"markdown","a6e2a84d":"markdown","d71ef5e6":"markdown","77f04778":"markdown","e2a0137e":"markdown","386484aa":"markdown","f2dfed32":"markdown","7165f709":"markdown","3db5a113":"markdown"},"source":{"eea7b302":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import linear_model\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import tree\nfrom sklearn import svm\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom lightgbm import LGBMClassifier\nimport plotly.express as px\nimport urllib\n\ndef AlgoFun (X_train, Y_train, X_test, Y_test, Algo, Result):\n    \n    if (Algo == 'LOG'):\n        log_reg=LogisticRegression(C=1000,max_iter=50000)\n        log_reg.fit(X_train, Y_train)\n        Y_pred = log_reg.predict(X_test)\n        \n    elif (Algo == 'LIN'):\n        model = linear_model.LinearRegression()\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n        \n    elif (Algo == 'KNN'):\n        KNN=KNeighborsClassifier(n_neighbors=20)\n        KNN.fit(X_train, Y_train)\n        Y_pred=KNN.predict(X_test)\n\n    elif (Algo == 'RFC'):\n        Clf =  RandomForestClassifier(n_estimators = 500, n_jobs = -1)\n        Clf.fit(X_train, Y_train)\n        Y_pred=Clf.predict(X_test) \n        compare1 = pd.DataFrame()\n        compare1[0] = Clf.feature_importances_\n        compare1[1] = X_test.columns\n        print('Feature importance: ')\n        print(compare1.sort_values(by=0,ascending= False))\n        \n    elif (Algo == 'NN'):\n        NN = MLPClassifier(solver='sgd', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1, max_iter=1000)\n        NN.fit(X_train, Y_train)\n        Y_pred = NN.predict(X_test)\n        \n    elif (Algo == 'DTR'):\n        DTR = tree.DecisionTreeClassifier()\n        DTR.fit(X_train, Y_train)\n        Y_pred=DTR.predict(X_test)\n        \n    elif (Algo == 'GSCV'):\n        estimator = RandomForestRegressor(random_state = 42,criterion='mse')\n        para_grids = {\n                    \"n_estimators\" : [10,50,100],\n                    \"max_features\" : [\"auto\", \"log2\", \"sqrt\"],\n                    'max_depth' : [4,5,6,7,8,9,15],\n                    \"bootstrap\"    : [True, False]\n                }\n        Grid = GridSearchCV(estimator, para_grids,cv= 5)\n        Grid.fit(X_train, Y_train)\n        best_param = Grid.best_estimator_\n        print(best_param)\n        Y_pred = best_param.predict(X_test)\n\n        \n    elif (Algo == 'RFR'):\n        # Using the best model from Grid Serach CV\n        model = RandomForestRegressor(bootstrap=False, max_depth=15, max_features='log2',n_estimators=50, random_state=42)\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n      \n    elif (Algo == 'GBR'):   \n        GBR = GradientBoostingRegressor(n_estimators=100, max_depth=4)\n        GBR.fit(X_train, Y_train)\n        Y_pred = GBR.predict(X_test)\n        \n    elif (Algo == 'GNB'):   \n        GNB = GaussianNB()\n        GNB.fit(X_train, Y_train)\n        Y_pred = GNB.predict(X_test)\n      \n    elif (Algo == 'ADA'):\n        model = AdaBoostRegressor(random_state=0, n_estimators=100)\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n        \n    elif (Algo == 'XGB'):\n        model = XGBRegressor()\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)\n        \n    elif (Algo == 'LGB'):\n        model = LGBMClassifier(objective='multiclass', random_state=5)\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)        \n   \n    elif (Algo == 'CAT'):\n        Cat = CatBoostClassifier(silent = True)\n        details = Cat.fit(X_train, Y_train)\n        Y_pred = Cat.predict(X_test)\n        \n    elif (Algo == 'SVM'):\n        model = svm.SVC(kernel='linear') # Linear Kernel\n        model.fit(X_train, Y_train)\n        Y_pred = model.predict(X_test)       \n            \n    else:\n        print(\"Wrong Algo\")\n        \n    ActVPred = pd.DataFrame({'Actual': Y_test, 'Predicted': Y_pred})\n    print(ActVPred)\n\n    #Checking the accuracy\n    print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, Y_pred))\n    print('Mean Squared Error:', metrics.mean_squared_error(Y_test, Y_pred))\n    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, Y_pred)))\n\n    if (Result == 'Binary'):\n        Count_row = []\n        Visual_rep = []\n\n        index = 0\n        for i, row in ActVPred.iterrows():\n            if (row['Predicted'] < 0.5):\n                Visual_rep.append(0)\n            else:\n                Visual_rep.append(1)\n\n            if (row['Actual'] < 1):\n                if (row['Predicted'] < 0.5):\n                    Count_row.append(1)\n                else:\n                    Count_row.append(0)\n            else:\n                if (row['Predicted'] >= 0.5):\n                    Count_row.append(1)\n                else:\n                    Count_row.append(0)\n            index = index + 1\n\n        print('--------------------------------------------------------------------------')\n        print(Algo)\n        print('Model accruracy scores: {:.3f}'.format(Count_row.count(1)\/index))\n\n\n        ax = plt.subplots(figsize=(10, 10))\n        ax = sns.heatmap(confusion_matrix(Visual_rep,Y_test),annot=True,cmap='coolwarm',fmt='d')\n        ax.set_title('Prediction vs Original Data (Confusion Matrix)',fontsize=18)\n        ax.set_xticklabels(['Actual 0','Actual 1'],fontsize=18)\n        ax.set_yticklabels(['Predicted 0','Predicted 1'],fontsize=18)\n\n        filename = 'ConfusionMatrix.jpg'\n        plt.savefig(Algo + \" \" + filename)\n        plt.show()\n    \n    elif (Result == 'Analog'):\n        fig, ax = plt.subplots()\n        minimum = min (Y_test.min(), Y_pred.min())\n        maximum = max (Y_test.max(), Y_pred.max())\n        ax.scatter(Y_test, Y_pred)\n        ax.plot([minimum, maximum], [minimum, maximum], 'k--', lw=4)\n        ax.set_xlabel('Measured')\n        ax.set_ylabel('Predicted')\n        filename = 'Result Plot.jpg'\n        plt.savefig(Algo + \" \" + filename)\n        plt.show()\n    \n    else:\n        print(\"Wrong parameter\")\n\n    return (ActVPred)","6adefaeb":"Data =  pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')\n\nData.describe()","1e9c746c":"Data.head()","9a4d6dc7":"Data.isnull().sum()","7d24537e":"Data.isna().sum()","dab85270":"Data1 = Data\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.violinplot(x='neighbourhood_group', y='price', data=Data1, hue='neighbourhood_group')","5c6b55e0":"Data1 = Data[Data['price']<300]\nsns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.violinplot(x='neighbourhood_group', y='price', data=Data1, hue='neighbourhood_group')","12f068ee":"Data1 = Data[Data['price']<300]\nscat = px.scatter(Data1, x = 'neighbourhood_group', y = 'price', color = 'neighbourhood_group',marginal_y = 'box')\n\nscat.show()","72809941":"plt.figure(figsize=(15,13))\n\n#loading the png NYC image found on Google and saving to my local folder along with the project\ni=urllib.request.urlopen('https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/ec\/Neighbourhoods_New_York_City_Map.PNG')\n\nnyc_img=plt.imread(i)\n#scaling the image based on the latitude and longitude max and mins for proper output\nplt.imshow(nyc_img,zorder=0,extent=[-74.258, -73.7, 40.49,40.92])\nax=plt.gca()\n\n#using scatterplot again\nData1.plot(kind='scatter', x='longitude', y='latitude', c='price', ax=ax, \n           cmap=plt.get_cmap('jet'), colorbar=True, alpha=0.4, zorder=5)\n\nplt.legend()\nplt.show()","776d7657":"Data1 = Data[Data['price']<300]\nscat = px.scatter(Data1, x = 'room_type', y = 'price', color = 'room_type',marginal_y = 'box')\n\nscat.show()","ec918a1b":"Data1 = Data[Data['price']<300]\nimport plotly.express as px\n\n\nfig = px.scatter(Data1, x=\"room_type\", y=\"price\", animation_frame=\"neighbourhood_group\", color= \"price\", size=\"price\", hover_name=\"neighbourhood\",width=1200, height=700)\n\nfig.show()","caae6a8d":"Data['room_type'] = Data['room_type'].astype(\"category\").cat.codes\nData['neighbourhood'] = Data['neighbourhood'].astype(\"category\").cat.codes\nData['neighbourhood_group']= Data['neighbourhood_group'].astype(\"category\").cat.codes\nData.info()","20d294d2":"Data['price_log'] = np.log(Data.price+1)\n\nplt.figure(figsize=(12,10))\nsns.distplot(Data['price_log'])\nplt.title(\"Log-Price Distribution Plot\",size=15, weight='bold')","68fd5409":"stats.probplot(Data['price_log'], plot=plt)\nplt.show()","bcc748c6":"Data1 = Data.drop(columns=['name','id' ,'host_id','host_name', 'last_review','price'])\nData1.fillna(Data1.mean(), inplace=True)\nData1.isnull().sum()","0a24270a":"plt.figure(figsize=(15,12))\npalette = sns.diverging_palette(20, 220, n=256)\ncorr=Data1.corr(method='pearson')\nsns.heatmap(corr, annot=True, fmt=\".2f\", cmap=palette, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5}).set(ylim=(11, 0))\nplt.title(\"Correlation Matrix\",size=15, weight='bold')","faa2ab22":"hpi_df = Data1[['latitude','longitude','minimum_nights','number_of_reviews','reviews_per_month','price_log']]\nsns.pairplot(hpi_df)","bb920baf":"nyc_model_x, nyc_model_y = Data1.iloc[:,:-1], Data1.iloc[:,-1]\n\n\nf, axes = plt.subplots(5, 2, figsize=(15, 20))\nsns.residplot(nyc_model_x.iloc[:,0],nyc_model_y, lowess=True, ax=axes[0, 0], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,1],nyc_model_y, lowess=True, ax=axes[0, 1],\n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,2],nyc_model_y, lowess=True, ax=axes[1, 0], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,3],nyc_model_y, lowess=True, ax=axes[1, 1], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,4],nyc_model_y, lowess=True, ax=axes[2, 0], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,5],nyc_model_y, lowess=True, ax=axes[2, 1], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,6],nyc_model_y, lowess=True, ax=axes[3, 0], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,7],nyc_model_y, lowess=True, ax=axes[3, 1], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,8],nyc_model_y, lowess=True, ax=axes[4, 0], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nsns.residplot(nyc_model_x.iloc[:,9],nyc_model_y, lowess=True, ax=axes[4, 1], \n                          scatter_kws={'alpha': 0.5}, \n                          line_kws={'color': 'red', 'lw': 1, 'alpha': 0.8})\nplt.setp(axes, yticks=[])\nplt.tight_layout()","142f23c4":"#Eigen vector of a correlation matrix.\nmulticollinearity, V=np.linalg.eig(corr)\nmulticollinearity","2eb34aa0":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nnyc_model_x = scaler.fit_transform(nyc_model_x)","5f1d7a1f":"X_train, X_test, Y_train, Y_test  = train_test_split(nyc_model_x, nyc_model_y, test_size=0.3,random_state=42)","d7e785d2":"pd.DataFrame(X_train).to_csv('X_train.csv')\npd.DataFrame(X_test).to_csv('X_test.csv')\n\npd.DataFrame(Y_train).to_csv('Y_train.csv')\npd.DataFrame(Y_test).to_csv('Y_test.csv')","eafe474a":"X_train = pd.read_csv('.\/X_train.csv')\nX_test = pd.read_csv('.\/X_test.csv')\nY_train = pd.read_csv('.\/Y_train.csv').to_numpy()[:,1]\nY_test = pd.read_csv('.\/Y_test.csv').to_numpy()[:,1]","1db34f6f":"ActVPred = AlgoFun (X_train, Y_train, X_test, Y_test, 'XGB', 'Analog')","91bf7390":"ActVPred = AlgoFun (X_train, Y_train, X_test, Y_test, 'RFR', 'Analog')","463d3f90":"Y_test1 = ActVPred['Actual'] = np.exp(ActVPred['Actual'])-1\nY_pred1 = ActVPred['Predicted'] = np.exp(ActVPred['Predicted'])-1\n\nfig, ax = plt.subplots()\nminimum = min (Y_test1.min(), Y_pred1.min())\nmaximum = max (Y_test1.max(), Y_pred1.max())\nax.scatter(Y_test1, Y_pred1)\nax.plot([minimum, maximum], [minimum, maximum], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nfilename = 'Plot.jpg'\nplt.savefig(\"Final\" + filename)\nplt.show()\n\nActVPred.to_csv('Final Submission.csv')","cf82cdfb":"# 16. Final Conclusion","9f9cfd94":"# 7. Checking for correlation among factors and key contributors to price\n\nKey contributers to price is room_type and longitude. \nProperties centrally located have higher price variance. Properties located up North and down South are mostly cheaper\n\nNext set of equally important pair is number of reviews and reviews per month. \nSo how famous the listing is also effects price. Looks like mid priced (4-6 in log price) hotels tend to have larger reviews\n\nLast but not the leat is minimum_nights. Listings with more nights tend to be mid priced (4-6 in log price) hotels.","c1d6f218":"# Airbnb Rate Prediction","d927e046":"# 9. Multicollinearity\nMulticollinearity will help to measure the relationship between explanatory variables in multiple regression. \nIf there is multicollinearity, these highly related input variables should be eliminated from the model.\n\nNot one of the eigenvalues of the correlation matrix is close to zero. It means that there is no multicollinearity in the data.","60b2e4f7":"In below graph, the good fit indicates that normality is a reasonable approximation.","72100ee4":"# 6. Removing all unnecesaary data","8abf92ad":"# Function AlgoFun\n\nThe below function takes in a parameter called Algo. You need to pass any of the below values depending on which algorithm to run\n\n**Algo Values:**\n\n* LOG : Logistic Regression\n* LIN : Linear Regression\n* KNN : K Nearest Neighbor\n* RFC : Random Forest Classifier\n* NN : Neural Network\n* DTR : Decision Tree\n* GSCV: Grid Search CV\n* RFR : Random Forest Regression\n* GBR : Gradient Boosting Regression\n* GNB : Gaussian Naive Bayes\n* ADA : Ada Boost\n* XGB : XG Boost\n* LGB : Light GBM\n* CAT : Cat Boost\n* SVM : Support Vector Machine\n\n**Result Values**\n\n* Binary : If predictions are 0 or 1\n* Analog : If predictions is a number","75e1d75d":"# 12. Load Cleaned Data","92dbeefd":"# 5. Removing the price skeweness by using log","7c78cd64":"# 4. Data cleanup","b7e42075":"# 10. Normalizing the data \n\nStandard Scaler technique will be used to normalize the data set. Thus, each feature has 0 mean and 1 standard deviation.","ffa9d297":"# 13. XG Boost","5ca077d7":"# 3. Room Type analysis\nThe analysis of different types of apartments indicates:\n1. Entire home\/apt are genrally more expensive\n2. Brooklyn and Manhattan being expensive areas have more to offer in shared rooms\n3. Queens, Staten Island and Bronx being cheapers areas shared rooms are lesser on offer and have more options in private rooms and Entire home\/apt","a6e2a84d":"# 8. Residual Plots\nResidual Plot is strong method to detect outliers, non-linear data and detecting data for regression models. \nThe below charts show the residual plots for each feature with the price.\n\nAn ideal Residual Plot, the red line would be horizontal. Based on the below charts, most features are non-linear. \nOn the other hand, there are not many outliers in each feature. \nThis result led to underfitting. Underfitting can occur when input features do not have a strong relationship to target variables or over-regularized. \nFor avoiding underfitting new data features can be added or regularization weight could be reduced.\n\nIn this kernel, since the input feature data could not be increased, Regularized Linear Models will be used \nfor regularization and polynomial transformation will be made to avoid underfitting.","d71ef5e6":"![](http:\/\/w6ig3zr1jm-flywheel.netdna-ssl.com\/wp-content\/uploads\/2020\/03\/bigstock-Hand-In-Blue-Rubber-Glove-Hold-347247313-1-scaled.jpg)","77f04778":"# 15. Random Forest Regression","e2a0137e":"# 11. Save cleaned Data","386484aa":"# 2. Price Analysis\nThe analysis of the price indicates:\n1. Prices are generally between 25 to 300 USD\n2. Clearly Queens, Staten Island and Bronx have similar medians group with similar prices\n3. Brooklyn, Queens and Manhattan have more options with larger prices range on an average","f2dfed32":"# 16. References\n1. https:\/\/www.kaggle.com\/duygut\/airbnb-nyc-price-prediction\n2. https:\/\/www.kaggle.com\/dgomonov\/data-exploration-on-nyc-airbnb","7165f709":"![](http:\/\/i.pinimg.com\/564x\/04\/69\/68\/046968095bf6798a3133678ad36bd7d1.jpg)","3db5a113":"# 1. Checking for empty cells"}}