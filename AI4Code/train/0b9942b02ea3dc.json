{"cell_type":{"b40b5006":"code","738e7f5e":"code","cde3b45c":"code","01e60b8b":"code","96339342":"code","013a0080":"code","352879c8":"code","38d7d3eb":"code","7b5ead18":"code","71bc6fdd":"code","e6e31ad1":"code","c7b27827":"code","68ed62eb":"code","de03ed62":"code","739fed09":"code","25422c12":"code","db445575":"code","e6d483f1":"code","2472aae1":"code","e77197b9":"code","8e3f235e":"code","dae83385":"markdown","1a3600b6":"markdown","5fe7d3b8":"markdown","9899be71":"markdown","71f31064":"markdown","819c4583":"markdown","6b104840":"markdown","bbf2edf6":"markdown","d8aac55c":"markdown"},"source":{"b40b5006":"import random\nimport math\nimport time\nimport os\nimport glob\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import transforms","738e7f5e":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndevice","cde3b45c":"INPUT_PATH = \"..\/input\/cat-dataset\/CAT_00\"\nimg_path_list = glob.glob(os.path.join(INPUT_PATH,\"*.jpg\"))\nprint(len(img_path_list))","01e60b8b":"plt.figure(figsize=(20,15),tight_layout = True)\nfor i in range(30):\n    plt.subplot(6,5,i+1)\n    img = Image.open(img_path_list[i])\n    plt.imshow(img)\n    plt.axis(\"OFF\")\n    plt.title(img.size)","96339342":"def seed_everything():\n    torch.manual_seed(1234)\n    np.random.seed(1234)\n    random.seed(1234)\nseed_everything() ","013a0080":"class Generator(nn.Module):\n    def __init__(self, z_dim=20, image_size = 64):\n        super(Generator, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.ConvTranspose2d(z_dim, # \u6f5c\u5728\u6b21\u5143 in_channle\n                               image_size*8, #  out_channel \n                               kernel_size = 4,\n                               stride = 1),\n            nn.BatchNorm2d(image_size*8),\n            nn.ReLU(inplace=True))\n\n        self.layer2 = nn.Sequential(\n            nn.ConvTranspose2d(image_size*8, # \u6f5c\u5728\u6b21\u5143 in_channle\n                               image_size*4, #  out_channel \n                               kernel_size = 4,\n                               stride = 2,\n                               padding = 1),\n            nn.BatchNorm2d(image_size*4),\n            nn.ReLU(inplace=True))\n\n        self.layer3 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 4, \n                               image_size * 2,\n                               kernel_size=4, \n                               stride=2, \n                               padding=1),\n            nn.BatchNorm2d(image_size * 2),\n            nn.ReLU(inplace=True))\n\n        self.layer4 = nn.Sequential(\n            nn.ConvTranspose2d(image_size * 2, \n                               image_size,\n                               kernel_size=4, \n                               stride=2, \n                               padding=1),\n            nn.BatchNorm2d(image_size),\n            nn.ReLU(inplace=True))\n\n        self.last = nn.Sequential(\n            nn.ConvTranspose2d(image_size, \n                               3, #\u30ab\u30e9\u30fc\u753b\u50cf\u306a\u306e\u3067\u3053\u3053\u306f3\u306b\u3057\u3066\u304a\u304f\n                               kernel_size=4,\n                               stride=2, \n                               padding=1),\n            nn.Tanh())\n\n    def forward(self, z):\n        '''\n        \u30d0\u30b0\u3060\u3057\u306e\u305f\u3081\u306bprint\u3067shape\u53d6\u3063\u3066\u304a\u304f\n        '''\n        #print(f\"z_shape :{z.shape}\")\n        out = self.layer1(z)\n        #print(f\"layer1_out_shape : {out.shape}\")\n        out = self.layer2(out)\n        #print(f\"layer2_out_shape : {out.shape}\")\n        out = self.layer3(out)\n        #print(f\"layer3_out_shape : {out.shape}\")\n        out = self.layer4(out)\n        #print(f\"layer4_out_shape : {out.shape}\")\n        out = self.last(out)\n        #print(f\"layer5_out_shape : {out.shape}\")\n        return out","352879c8":"# \u52d5\u4f5c\u78ba\u8a8d\nG = Generator(z_dim = 20, image_size = 64)\n# \u4e71\u6570\ninput_z = torch.randn(1,20)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1,1)\n# output\nfake_images = G(input_z)\nprint(f\"fake_img_shape:{fake_images.shape}\")\n\n#\nimg_transformed = fake_images[0][0].detach().numpy() # https:\/\/teratail.com\/questions\/234693 detach\u306b\u3064\u3044\u3066\nplt.imshow(img_transformed)\nplt.show()","38d7d3eb":"class Discriminator(nn.Module):\n    def __init__(self,image_size = 64):\n        super(Discriminator, self).__init__()\n        \n        self.layer1 = nn.Sequential(\n            nn.Conv2d(3, # in_channel\u306f\u30ab\u30e9\u30fc\u306a\u306e\u30673\n                      image_size,\n                      kernel_size = 4,\n                      stride = 2,\n                      padding = 1\n                     ),\n            nn.LeakyReLU(0.1, inplace = True))\n        \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(image_size, \n                      image_size*2,\n                      kernel_size = 4,\n                      stride = 2,\n                      padding = 1\n                     ),\n            nn.LeakyReLU(0.1, inplace = True))\n        \n        self.layer3 = nn.Sequential(\n            nn.Conv2d(image_size*2, \n                      image_size*4,\n                      kernel_size = 4,\n                      stride = 2,\n                      padding = 1\n                     ),\n            nn.LeakyReLU(0.1, inplace = True))\n\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(image_size*4, \n                      image_size*8,\n                      kernel_size = 4,\n                      stride = 2,\n                      padding = 1\n                     ),\n            nn.LeakyReLU(0.1, inplace = True))\n        self.last = nn.Conv2d(image_size*8, 1, kernel_size=4, stride=1)\n        \n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.last(out)\n        return out","7b5ead18":"# \u52d5\u4f5c\u78ba\u8a8d\nD = Discriminator(image_size=64)\n\n# \u507d\u753b\u50cf\u3092\u751f\u6210\ninput_z = torch.randn(1, 20)\ninput_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\nfake_images = G(input_z)\n\n# \u507d\u753b\u50cf\u3092D\u306b\u5165\u529b\nd_out = D(fake_images)\n\n# \u51fa\u529bd_out\u306bSigmoid\u3092\u304b\u3051\u30660\u304b\u30891\u306b\u5909\u63db\nprint(nn.Sigmoid()(d_out))","71bc6fdd":"def make_img_path_list(use_dir_num):\n    '''\n    \u753b\u50cf\u683c\u7d0d\u30d5\u30a9\u30eb\u30c0\u306f\u306fCAT_00~CAT06\u307e\u3067\u3042\u308b\u3002\u3069\u3053\u307e\u3067\u4f7f\u3046\u304b\u6307\u5b9a\u3057\u3066\u3001\u305d\u3053\u307e\u3067\u306e\u753b\u50cfpath\u306e\u30ea\u30b9\u30c8\u3092\u8fd4\u3059\n    '''\n    train_img_list = []\n    for i in range(use_dir_num):\n        use_dir = f\"..\/input\/cat-dataset\/CAT_0{i}\"\n        paths = glob.glob(os.path.join(use_dir,\"*.jpg\"))\n        train_img_list+=paths\n        print(\"num_img\",len(train_img_list))\n    return train_img_list","e6e31ad1":"train_img_list = make_img_path_list(7)","c7b27827":"# transform\nIMG_MEAN = [0.5, 0.5, 0.5]\nIMG_STD = [0.5, 0.5, 0.5]\nsize = (64,64)\n\ndata_transform = transforms.Compose([\n    transforms.Resize(size),\n    transforms.ToTensor(), \n    transforms.Normalize(mean=IMG_MEAN, std=IMG_STD),\n])","68ed62eb":"# transform\u52d5\u4f5c\u78ba\u8a8d\nfor i in range(5):\n    img = Image.open(img_path_list[i])\n    x = data_transform(img)\n    plt.title(x.shape)\n    plt.imshow(x.permute(1,2,0))\n    plt.axis(\"off\")\n    plt.show()","de03ed62":"# dataset class\nclass GAN_Dataset(data.Dataset):\n    def __init__(self, file_list, transform):\n        self.file_list = file_list\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.file_list)\n    \n    def __getitem__(self, index):\n        img = Image.open(self.file_list[index])\n        img = self.transform(img)\n        return img","739fed09":"# DataLoader\u306e\u4f5c\u6210\u3068\u52d5\u4f5c\u78ba\u8a8d\ntrain_dataset = GAN_Dataset(\n    file_list=train_img_list, \n    transform=data_transform)\n\n# DataLoader\u3092\u4f5c\u6210\nbatch_size = 64\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# \u52d5\u4f5c\u306e\u78ba\u8a8d\nbatch_iterator = iter(train_dataloader)  # \u30a4\u30c6\u30ec\u30fc\u30bf\u306b\u5909\u63db\nimges = next(batch_iterator)  # 1\u756a\u76ee\u306e\u8981\u7d20\u3092\u53d6\u308a\u51fa\u3059\nprint(imges.size())  # torch.Size([64, 1, 64, 64])","25422c12":"# \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u521d\u671f\u5316\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        # Conv2d\u3068ConvTranspose2d\u306e\u521d\u671f\u5316\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n    elif classname.find('BatchNorm') != -1:\n        # BatchNorm2d\u306e\u521d\u671f\u5316\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","db445575":"# \u30e2\u30c7\u30eb\u3092\u5b66\u7fd2\u3055\u305b\u308b\u95a2\u6570\u3092\u4f5c\u6210\ndef train_model(G, D, dataloader, num_epochs,device):\n    # \u6700\u9069\u5316\u624b\u6cd5\u306e\u8a2d\u5b9a\n    g_lr, d_lr = 0.0005, 0.0005\n    beta1, beta2 = 0.1, 0.9\n    g_optimizer = torch.optim.Adam(G.parameters(), g_lr, [beta1, beta2])\n    d_optimizer = torch.optim.Adam(D.parameters(), d_lr, [beta1, beta2])\n\n    # \u8aa4\u5dee\u95a2\u6570\u3092\u5b9a\u7fa9\n    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n\n    # \u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u30cf\u30fc\u30c9\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\n    z_dim = 20\n    mini_batch_size = 64\n\n    # \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092GPU\u3078\n    G.to(device)\n    D.to(device)\n\n    G.train()  # \u30e2\u30c7\u30eb\u3092\u8a13\u7df4\u30e2\u30fc\u30c9\u306b\n    D.train()  # \u30e2\u30c7\u30eb\u3092\u8a13\u7df4\u30e2\u30fc\u30c9\u306b\n\n    # \u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u304c\u3042\u308b\u7a0b\u5ea6\u56fa\u5b9a\u3067\u3042\u308c\u3070\u3001\u9ad8\u901f\u5316\u3055\u305b\u308b\n    torch.backends.cudnn.benchmark = True\n\n    # \u753b\u50cf\u306e\u679a\u6570\n    num_train_imgs = len(dataloader.dataset)\n    batch_size = dataloader.batch_size\n\n    # \u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u30ab\u30a6\u30f3\u30bf\u3092\u30bb\u30c3\u30c8\n    iteration = 1\n    logs = []\n\n    # epoch\u306e\u30eb\u30fc\u30d7\n    for epoch in range(num_epochs):\n\n        # \u958b\u59cb\u6642\u523b\u3092\u4fdd\u5b58\n        t_epoch_start = time.time()\n        epoch_g_loss = 0.0  # epoch\u306e\u640d\u5931\u548c\n        epoch_d_loss = 0.0  # epoch\u306e\u640d\u5931\u548c\n\n        print('-------------')\n        print('Epoch {}\/{}'.format(epoch, num_epochs))\n        print('-------------')\n        print('\uff08train\uff09')\n\n        # \u30c7\u30fc\u30bf\u30ed\u30fc\u30c0\u30fc\u304b\u3089minibatch\u305a\u3064\u53d6\u308a\u51fa\u3059\u30eb\u30fc\u30d7\n        for imges in dataloader:\n\n            # --------------------\n            # 1. Discriminator\u306e\u5b66\u7fd2\n            # --------------------\n            # \u30df\u30cb\u30d0\u30c3\u30c1\u304c\u30b5\u30a4\u30ba\u304c1\u3060\u3068\u3001\u30d0\u30c3\u30c1\u30ce\u30fc\u30de\u30e9\u30a4\u30bc\u30fc\u30b7\u30e7\u30f3\u3067\u30a8\u30e9\u30fc\u306b\u306a\u308b\u306e\u3067\u3055\u3051\u308b\n            if imges.size()[0] == 1:\n                continue\n\n            # GPU\u304c\u4f7f\u3048\u308b\u306a\u3089GPU\u306b\u30c7\u30fc\u30bf\u3092\u9001\u308b\n            imges = imges.to(device)\n\n            # \u6b63\u89e3\u30e9\u30d9\u30eb\u3068\u507d\u30e9\u30d9\u30eb\u3092\u4f5c\u6210\n            # epoch\u306e\u6700\u5f8c\u306e\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u306f\u30df\u30cb\u30d0\u30c3\u30c1\u306e\u6570\u304c\u5c11\u306a\u304f\u306a\u308b\n            mini_batch_size = imges.size()[0]\n            label_real = torch.full((mini_batch_size,), 1).to(device)\n            label_fake = torch.full((mini_batch_size,), 0).to(device)\n\n            # \u771f\u306e\u753b\u50cf\u3092\u5224\u5b9a\n            d_out_real = D(imges)\n\n            # \u507d\u306e\u753b\u50cf\u3092\u751f\u6210\u3057\u3066\u5224\u5b9a\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images)\n\n            # \u8aa4\u5dee\u3092\u8a08\u7b97\n            label_real = label_real.type_as(d_out_real.view(-1))\n            label_fake = label_fake.type_as(d_out_fake.view(-1))\n            \n            d_loss_real = criterion(d_out_real.view(-1), label_real)\n            d_loss_fake = criterion(d_out_fake.view(-1), label_fake)\n            d_loss = d_loss_real + d_loss_fake\n\n            # \u30d0\u30c3\u30af\u30d7\u30ed\u30d1\u30b2\u30fc\u30b7\u30e7\u30f3\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n\n            d_loss.backward()\n            d_optimizer.step()\n\n            # --------------------\n            # 2. Generator\u306e\u5b66\u7fd2\n            # --------------------\n            # \u507d\u306e\u753b\u50cf\u3092\u751f\u6210\u3057\u3066\u5224\u5b9a\n            input_z = torch.randn(mini_batch_size, z_dim).to(device)\n            input_z = input_z.view(input_z.size(0), input_z.size(1), 1, 1)\n            fake_images = G(input_z)\n            d_out_fake = D(fake_images)\n\n            # \u8aa4\u5dee\u3092\u8a08\u7b97\n            g_loss = criterion(d_out_fake.view(-1), label_real)\n\n            # \u30d0\u30c3\u30af\u30d7\u30ed\u30d1\u30b2\u30fc\u30b7\u30e7\u30f3\n            g_optimizer.zero_grad()\n            d_optimizer.zero_grad()\n            g_loss.backward()\n            g_optimizer.step()\n\n            # --------------------\n            # 3. \u8a18\u9332\n            # --------------------\n            epoch_d_loss += d_loss.item()\n            epoch_g_loss += g_loss.item()\n            iteration += 1\n\n        # epoch\u306ephase\u3054\u3068\u306eloss\u3068\u6b63\u89e3\u7387\n        t_epoch_finish = time.time()\n        print('-------------')\n        print('epoch {} || Epoch_D_Loss:{:.4f} ||Epoch_G_Loss:{:.4f}'.format(\n            epoch, epoch_d_loss\/batch_size, epoch_g_loss\/batch_size))\n        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n        t_epoch_start = time.time()\n\n    return G, D\n","e6d483f1":"## \u5b66\u7fd2\u30fb\u691c\u8a3c\u3092\u5b9f\u884c\u3059\u308b\n# \u521d\u671f\u5316\u306e\u5b9f\u65bd\nG = Generator(z_dim = 20, image_size = 64)\nD = Discriminator(image_size=64)\n\nG.apply(weights_init)\nD.apply(weights_init)\nprint(\"\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u521d\u671f\u5316\u5b8c\u4e86\")\n\nnum_epochs = 50\nG_update, D_update = train_model(\n    G, D, dataloader=train_dataloader, num_epochs=num_epochs, device = device)","2472aae1":"# \u751f\u6210\u753b\u50cf\u3068\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u53ef\u8996\u5316\u3059\u308b\n# \u672c\u30bb\u30eb\u306f\u826f\u3044\u611f\u3058\u306e\u753b\u50cf\u304c\u751f\u6210\u3055\u308c\u308b\u307e\u3067\u3001\u4f55\u5ea6\u3082\u5b9f\u884c\u3057\u76f4\u3057\u3066\u3044\u307e\u3059\u3002\n\n# \u5165\u529b\u306e\u4e71\u6570\u751f\u6210\nbatch_size = 40\nz_dim = 20\nfixed_z = torch.randn(batch_size, z_dim)\nfixed_z = fixed_z.view(fixed_z.size(0), fixed_z.size(1), 1, 1)\n\n# \u753b\u50cf\u751f\u6210\nfake_images = G_update(fixed_z.to(device))\n\n# \u51fa\u529b\nfig = plt.figure(figsize=(16, 10),tight_layout = True)\nfor i in range(0, 40):\n    plt.subplot(5, 8, i+1)\n    plt.imshow(fake_images[i].cpu().detach().permute(1,2,0))\n    plt.axis(\"off\")","e77197b9":"# \u751f\u6210\u753b\u50cf\u3068\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u53ef\u8996\u5316\u3059\u308b\n# \u672c\u30bb\u30eb\u306f\u826f\u3044\u611f\u3058\u306e\u753b\u50cf\u304c\u751f\u6210\u3055\u308c\u308b\u307e\u3067\u3001\u4f55\u5ea6\u3082\u5b9f\u884c\u3057\u76f4\u3057\u3066\u3044\u307e\u3059\u3002\n\n# \u5165\u529b\u306e\u4e71\u6570\u751f\u6210\nbatch_size = 40\nz_dim = 20\nfixed_z = torch.randn(batch_size, z_dim)\nfixed_z = fixed_z.view(fixed_z.size(0), fixed_z.size(1), 1, 1)\n\n# \u753b\u50cf\u751f\u6210\nfake_images = G_update(fixed_z.to(device))\n\n# \u51fa\u529b\nfig = plt.figure(figsize=(16, 10),tight_layout = True)\nfor i in range(0, 40):\n    plt.subplot(5, 8, i+1)\n    plt.imshow(fake_images[i].cpu().detach().permute(1,2,0))\n    plt.axis(\"off\")","8e3f235e":"# \u751f\u6210\u753b\u50cf\u3068\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u53ef\u8996\u5316\u3059\u308b\n# \u672c\u30bb\u30eb\u306f\u826f\u3044\u611f\u3058\u306e\u753b\u50cf\u304c\u751f\u6210\u3055\u308c\u308b\u307e\u3067\u3001\u4f55\u5ea6\u3082\u5b9f\u884c\u3057\u76f4\u3057\u3066\u3044\u307e\u3059\u3002\n\n# \u5165\u529b\u306e\u4e71\u6570\u751f\u6210\nbatch_size = 40\nz_dim = 20\nfixed_z = torch.randn(batch_size, z_dim)\nfixed_z = fixed_z.view(fixed_z.size(0), fixed_z.size(1), 1, 1)\n\n# \u753b\u50cf\u751f\u6210\nfake_images = G_update(fixed_z.to(device))\n\n# \u51fa\u529b\nfig = plt.figure(figsize=(16, 10),tight_layout = True)\nfor i in range(0, 40):\n    plt.subplot(5, 8, i+1)\n    plt.imshow(fake_images[i].cpu().detach().permute(1,2,0))\n    plt.axis(\"off\")","dae83385":"# dataloader\u4f5c\u6210","1a3600b6":"# GCGAN\u5b9f\u88c5\n- SAGAN\u3082\u5b9f\u88c5\u3057\u305f\u3044\u3051\u3069\u3001Self-attention\u306e\u5b9f\u88c5\u3060\u308b\u305d\u3046\u306a\u306e\u3067\u3001\u4e00\u65e6GCGAN\n- pytorch\u672c\u3000\u3000p246~\u57fa\u672c\u305d\u306e\u307e\u307e\u3082\u3089\u3046\n- \u305f\u3060\u3057\u30ab\u30e9\u30fc\u753b\u50cf\u306a\u306e\u3067\u6c17\u3092\u3064\u3051\u308b","5fe7d3b8":"## \u5b66\u7fd2","9899be71":"- https:\/\/www.kaggle.com\/crawford\/cat-dataset","71f31064":"- \u753b\u7d20\u8352\u3044\u3057\u3001\u3084\u3070\u305d\u3046\u306a\u6c17\u914d\u304c\u3059\u308b...\u30d0\u30b1\u30e2\u30ce\u304c\u3067\u304d\u305d\u3046...","819c4583":"- b:ch:w:h = 1:3:64:64\u306a\u306e\u3067ok","6b104840":"## Generator","bbf2edf6":"# Discriminator","d8aac55c":"- \u753b\u50cf\u30b5\u30a4\u30ba\u304c\u30d0\u30e9\u30d0\u30e9\u306a\u306e\u3067\u3001crop\u3059\u308b\u5fc5\u8981\u3042\u308a\u3002\n    - 64:64\u3067resize\u3068center-crop\u3057\u3088\u3046\u304b\u306a"}}