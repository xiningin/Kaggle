{"cell_type":{"f8519b30":"code","bed586f8":"code","bfeddd14":"code","c66fc21e":"code","c337df83":"code","389bb435":"code","c06e05bf":"code","21f25179":"code","88773774":"code","b3d7640e":"code","84451c34":"code","6d8dcb58":"code","c38fa6a1":"code","7ccaf90f":"code","418f6707":"code","cfdfba5a":"code","7ca59bf0":"code","e952da55":"code","01cc6aca":"code","83825ba3":"code","8049ba2b":"code","2bfd4632":"code","e5682c2f":"code","c5ff49c3":"code","fac20e8d":"code","70c7c869":"code","57b48f80":"code","2aee6a74":"code","aefcc2be":"code","ae5135c0":"code","b5391b62":"code","e6fde489":"code","c98517af":"code","71341836":"code","4170c921":"code","73fb4084":"code","c87c4cf6":"code","7bbabd47":"code","a8110287":"code","8376744f":"code","7ba0a7b1":"code","998f9af6":"code","02b660a3":"code","37025851":"code","fd183eb7":"code","9346f161":"code","9ecdd747":"code","4051e148":"code","76ec2380":"code","759dc58a":"code","96819fcc":"code","ceecbbd0":"code","ce3b0c6d":"markdown","86d8eda2":"markdown","5b515c2e":"markdown","efcac3fe":"markdown","44eff85d":"markdown","16aa4f6a":"markdown","022a2ef5":"markdown","10430b93":"markdown","689402d0":"markdown","a91ffb49":"markdown","4fc633f2":"markdown","31624dcb":"markdown","8cb67db0":"markdown","b27d49a6":"markdown","abc9a71e":"markdown","649503b0":"markdown","00d9cf24":"markdown","50b3cc79":"markdown","ddc80526":"markdown","b107bc7b":"markdown","5d7454c0":"markdown","3d812ed0":"markdown","7f29e728":"markdown","75462f88":"markdown","07d54899":"markdown"},"source":{"f8519b30":"import os\nprint(os.listdir(\"..\/input\"))\n\nimport warnings\nwarnings.filterwarnings('ignore')","bed586f8":"import bq_helper\nfrom bq_helper import BigQueryHelper\n# https:\/\/www.kaggle.com\/sohier\/introduction-to-the-bq-helper-package\nstackOverflow = bq_helper.BigQueryHelper(active_project=\"bigquery-public-data\",\n                                   dataset_name=\"stackoverflow\")","bfeddd14":"stack_db = BigQueryHelper(\"bigquery-public-data\", \"stackoverflow\")\nstack_db.list_tables()","c66fc21e":"stack_db.head(\"posts_questions\", num_rows=5)","c337df83":"stack_db.table_schema(\"posts_questions\")","389bb435":"query1 = \"\"\"\n         SELECT\n             title,\n             body as question,\n             tags as labels, \n             view_count as views\n         FROM\n             `bigquery-public-data.stackoverflow.posts_questions`\n         WHERE \n             (tags LIKE '%python%' OR\n             tags LIKE '%java%' OR \n             tags LIKE '%sql%' OR \n             tags LIKE '%|r|%' OR\n             tags LIKE 'r|%') AND\n             LENGTH(body) < 1000\n         LIMIT\n             7500;\n         \"\"\"\n\nquestions_df = stackOverflow.query_to_pandas(query1)","c06e05bf":"#import necessary packages for analysis \nimport nltk\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')","21f25179":"questions_df['labels'] = questions_df['labels'].str.split('|')","88773774":"def return_tags(labels):\n    langauges = [lang for lang in labels if lang in ['python','java','sql','r','javascript']] \n    return langauges","b3d7640e":"questions_df['labels'] = questions_df['labels'].apply(return_tags)","84451c34":"# Find rows that contain only a single tag\nprocessed_df = questions_df[(questions_df['labels'].apply(len) > 0) & (questions_df['labels'].apply(len) < 2)]  ","6d8dcb58":"#verify that only questions with a single language tag are included\nprocessed_df.head()","c38fa6a1":"processed_df.info()","7ccaf90f":"# In order to properly work with these labels, we must convert them from lists into strings.\ndef lst_to_str(lst):\n    unpacked = ''.join(lst)\n    return unpacked\n\nprocessed_df.loc[:,'labels'] = processed_df.loc[:,'labels'].apply(lst_to_str)\n\n#processed_df.describe()\n#processed_df.groupby('labels').describe()","418f6707":"grid = sns.FacetGrid(processed_df[processed_df['views'] < 4000], col = 'labels', height = 5, aspect = 0.6)\ngrid.map(plt.hist, 'views', bins = 75)\naxes = grid.axes\naxes[0,1].set_xlim([0,4000])\n\nplt.tight_layout()","cfdfba5a":"processed_df.loc[:,'length_of_question'] = processed_df.loc[:,'question'].apply(len)","7ca59bf0":"# verify the procedure worked.\nprocessed_df.head(2)","e952da55":"# Plot the length of each question based on what language they were written in\ngrid = sns.FacetGrid(processed_df, hue = 'labels', height = 5, aspect = 2)\ngrid.map(plt.hist, 'length_of_question', bins = 50, alpha = 0.5)\naxes = grid.axes\n\naxes[0,0].set_title('Distribution of question lengths for each programming language')\naxes[0,0].set_xlim([0,1000])\naxes[0,0].legend()","01cc6aca":"# An example of a question posted on stack overflow. Notice the html syntax and code delimiters.\nprocessed_df.head()","83825ba3":"import lxml.html\n\ndef find_code(html_str):\n    final_list = []\n\n    dom = lxml.html.fromstring(html_str)\n    codes = dom.xpath('\/\/code')\n\n    for code in codes:\n        if code.text is None: \n            final_list.append('')\n        else:\n            final_list.append(code.text)\n        \n        \n    final_list = ' '.join(final_list)\n    return final_list ","8049ba2b":"processed_df.loc[:,'code'] = processed_df.loc[:,'question'].apply(find_code)","2bfd4632":"def count_colons(txt):\n    return txt.count(':')\n\ndef count_semicolons(txt):\n    return txt.count(';')\n\ndef count_slashes(txt):\n    return txt.count('\/')\n                                      \ndef count_cbrackets(txt):\n    return txt.count('{') + txt.count('}')\n\ndef count_sbrackets(txt):\n    return txt.count('[') + txt.count(']')\n\ndef count_quotes(txt):\n    return txt.count('\"') + txt.count(\"'\")\n\ndef count_arithmetic(txt):\n    return txt.count('<') + txt.count('>') + txt.count('-') + txt.count('+') \n\ndef count_period(txt):\n    return txt.count('.')","e5682c2f":"processed_df.loc[:,'colon count']     = processed_df.loc[:,'code'].apply(count_colons)\nprocessed_df.loc[:,'semicolon count'] = processed_df.loc[:,'code'].apply(count_semicolons)\nprocessed_df.loc[:,'slash count']     = processed_df.loc[:,'code'].apply(count_slashes)\nprocessed_df.loc[:,'cbracket count']  = processed_df.loc[:,'code'].apply(count_cbrackets)\nprocessed_df.loc[:,'sbracket count']  = processed_df.loc[:,'code'].apply(count_sbrackets)\nprocessed_df.loc[:,'quote count']     = processed_df.loc[:,'code'].apply(count_quotes)\nprocessed_df.loc[:,'operator count']  = processed_df.loc[:,'code'].apply(count_arithmetic)\nprocessed_df.loc[:,'period count']    = processed_df.loc[:,'code'].apply(count_period)","c5ff49c3":"# Verify the functions worked.\nprocessed_df.head()","fac20e8d":"fig, axis = plt.subplots(figsize=(25,30), nrows = 5)\n\npython_features = processed_df[processed_df['labels'] == 'python'].loc[:,'colon count':]\njs_features = processed_df[processed_df['labels'] == 'javascript'].loc[:,'colon count':]\njava_features = processed_df[processed_df['labels'] == 'java'].loc[:,'colon count':]\nsql_features = processed_df[processed_df['labels'] == 'sql'].loc[:,'colon count':]\nr_features = processed_df[processed_df['labels'] == 'r'].loc[:,'colon count':]\n\nsql_features.head()\n\nsns.heatmap(python_features, cmap = 'viridis', ax = axis[0])\naxis[0].set_title('Python features heatmap')\n\nsns.heatmap(js_features, cmap = 'inferno', ax = axis[1])\naxis[1].set_title('Javascript features heatmap')\n\nsns.heatmap(java_features, cmap = 'viridis', ax = axis[2])\naxis[2].set_title('Java features heatmap')\n\nsns.heatmap(sql_features, cmap = 'inferno', ax = axis[3])\naxis[3].set_title('SQL features heatmap')\n\nsns.heatmap(r_features, cmap = 'viridis', ax = axis[4])\naxis[4].set_title('R features heatmap')\n","70c7c869":"total_syntax_features = processed_df.groupby('labels').sum(axis=1).loc[:,'colon count':]\n\nfig, aggregate_axis = plt.subplots(figsize=(15,6))\nsns.heatmap(total_syntax_features, cmap = 'plasma', ax = aggregate_axis)\naggregate_axis.set_title('Syntactic Features of Programming Languages')","57b48f80":"from sklearn.preprocessing import MinMaxScaler\n\nmin_max_scaler = MinMaxScaler()\ntotal_syntax_features_scaled = pd.DataFrame(min_max_scaler.fit_transform(total_syntax_features.T), columns = total_syntax_features.index, index = total_syntax_features.columns)\ntotal_syntax_features_scaled.head(10)\n\nfig, axis_scaled = plt.subplots(figsize=(15,6))\nsns.heatmap(total_syntax_features_scaled.T, cmap = 'plasma', ax = axis_scaled)\naxis_scaled.set_title('Normalized Code Features')","2aee6a74":"from bs4 import BeautifulSoup\n\ndef find_text(html_str):\n    full_text = ''\n\n    parsedContent = BeautifulSoup(html_str, 'html.parser')\n\n    text = parsedContent.findAll('p')\n    \n    for paragraph in text:\n        full_text = full_text + paragraph.getText()\n        \n    return full_text    ","aefcc2be":"import re \nimport string\nfrom nltk.corpus import stopwords \n\nstop_words = stopwords.words()\ntranslation_table = dict.fromkeys(map(ord, string.punctuation), None)\n\ndef remove_punc_and_stopwords(full_text):\n    cleaned_text = full_text.translate(translation_table)\n    word_lst = re.findall('[a-zA-Z]+', cleaned_text)\n    return \" \".join(word_lst)","ae5135c0":"def clean_html_text(text):\n    final_text = find_text(text)\n    bag_of_words = remove_punc_and_stopwords(final_text)\n    return bag_of_words","b5391b62":"processed_df.head(2)","e6fde489":"# Remove digits and special characters from title column. Note that it does not require an html parser.\nprocessed_df.loc[:,'title'] = processed_df.loc[:,'title'].apply(remove_punc_and_stopwords)","c98517af":"# Extract the text of the question column, and remove digits and special characters.\nprocessed_df.loc[:,'question'] = processed_df.loc[:,'question'].apply(clean_html_text)","71341836":"# Observe that there are no longer any punctuation or digits in the questions.\nprocessed_df.head(10)","4170c921":"from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\ncv_title = CountVectorizer().fit(processed_df['title'])\nvectorized_title = cv_title.transform(processed_df['title'])\nvectorized_title_df = pd.DataFrame(vectorized_title.toarray(), columns = cv_title.get_feature_names())\n\n# implement tfidf\ntfidf_title = TfidfTransformer().fit(vectorized_title)\nvectorized_tfidf_title = tfidf_title.transform(vectorized_title)\nvectorized_tfidf_title_df = pd.DataFrame(vectorized_tfidf_title.toarray(), columns = cv_title.get_feature_names())","73fb4084":"cv_question = CountVectorizer().fit(processed_df['question'])\nvectorized_question = cv_question.transform(processed_df['question'])\nvectorized_question_df = pd.DataFrame(vectorized_question.toarray(), columns = cv_question.get_feature_names())\n\n# implement tfidf\ntfidf_question = TfidfTransformer().fit(vectorized_question)\nvectorized_tfidf_question = tfidf_question.transform(vectorized_question)\nvectorized_tfidf_question_df = pd.DataFrame(vectorized_tfidf_question.toarray(), columns = cv_question.get_feature_names())","c87c4cf6":"# combine dataframes \ncv_bow = pd.concat([vectorized_title_df, vectorized_question_df], axis = 1)\ncv_tfidf_bow = pd.concat([vectorized_tfidf_title_df, vectorized_tfidf_question_df], axis = 1)","7bbabd47":"# import multinomial NB classifier and fit to dataset\nfrom sklearn.naive_bayes import MultinomialNB\n\nnb_cv_words_only = MultinomialNB().fit(cv_bow, processed_df['labels'])\nnb_cv_tfidf_words_only = MultinomialNB().fit(cv_tfidf_bow, processed_df['labels'])","a8110287":"# make predictions\ncv_bow_predictions = nb_cv_words_only.predict(cv_bow)\ncv_tfidf_bow_predictions = nb_cv_tfidf_words_only.predict(cv_tfidf_bow)","8376744f":"# display classification report + confusion matrix\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint('Multinomial Naive-Bayes Classification Report: ')\nprint(classification_report(processed_df['labels'], cv_bow_predictions))\nprint('\\n')\nprint('Multinomial Naive-Bayes with TFIDF Classification Report: ')\nprint(classification_report(processed_df['labels'], cv_tfidf_bow_predictions))\n\nfig, axes = plt.subplots(nrows=2,figsize=(10,10))\n\nsns.heatmap(confusion_matrix(processed_df['labels'], cv_bow_predictions), ax = axes[0], annot=True, cmap='magma', fmt = 'g',\n            xticklabels=['java', 'javascript','python','r','sql'], yticklabels=['java', 'javascript','python','r','sql'])\naxes[0].set_title('Confusion Matrix of Naive-Bayes Classification')\naxes[0].set_xlabel('Predicted Label')\naxes[0].set_ylabel('True Label')\n\nsns.heatmap(confusion_matrix(processed_df['labels'], cv_tfidf_bow_predictions), ax = axes[1], annot=True, cmap='magma', fmt = 'g',\n            xticklabels=['java', 'javascript','python','r','sql'], yticklabels=['java', 'javascript','python','r','sql'])\naxes[1].set_title('Confusion Matrix of Naive-Bayes Classification with TFIDF')\naxes[1].set_xlabel('Predicted Label')\naxes[1].set_ylabel('True Label')\n\nplt.tight_layout()","7ba0a7b1":"# Repeat process for bag of words including code features\ncode_features = processed_df.loc[:,'colon count':]\n\ncv_words_and_code = pd.concat([cv_bow.reset_index(drop = True), code_features.reset_index(drop = True)], axis = 1)\ncv_words_and_code_tfidf = TfidfTransformer().fit_transform(cv_words_and_code)\n\nnb_cv_words_and_code = MultinomialNB().fit(cv_words_and_code, processed_df['labels'])\nnb_cv_words_and_code_tfidf = MultinomialNB().fit(cv_words_and_code_tfidf, processed_df['labels'])\n\nnb_complete_predictions = nb_cv_words_and_code.predict(cv_words_and_code)\nnb_tfidf_complete_predictions = nb_cv_words_and_code.predict(cv_words_and_code_tfidf)","998f9af6":"print('Multinomial Naive-Bayes with code features: ')\nprint(classification_report(processed_df['labels'], nb_complete_predictions))\n\nprint('\\n')\n\nprint('Multinomial Naive-Bayes with TFIDF and code features: ')\nprint(classification_report(processed_df['labels'], nb_tfidf_complete_predictions))\n\nfig, axes = plt.subplots(nrows=2,figsize=(10,10))\nsns.heatmap(confusion_matrix(processed_df['labels'], nb_complete_predictions), ax = axes[0], annot=True, cmap='magma', fmt = 'g',\n            xticklabels=['java', 'javascript','python','r','sql'], yticklabels=['java', 'javascript','python','r','sql'])\naxes[0].set_title('Confusion Matrix of Naive-Bayes Classification with code features')\naxes[0].set_xlabel('Predicted Label')\naxes[0].set_ylabel('True Label')\nsns.heatmap(confusion_matrix(processed_df['labels'], nb_tfidf_complete_predictions), ax = axes[1], annot=True, cmap='magma', fmt = 'g',\n            xticklabels=['java', 'javascript','python','r','sql'], yticklabels=['java', 'javascript','python','r','sql'])\naxes[1].set_title('Confusion Matrix of Naive-Bayes Classification with TFIDF and code features')\naxes[1].set_xlabel('Predicted Label')\naxes[1].set_ylabel('True Label')","02b660a3":"from sklearn.preprocessing import StandardScaler","37025851":"views_and_length = processed_df.loc[:,['views','length_of_question']]\ncode_features = processed_df.loc[:,'colon count':]\n\n# Scale the views column\nss_view_length = StandardScaler()\nscaled_views_and_length = ss_view_length.fit_transform(views_and_length.astype(float)) \nscaled_views_and_length_df = pd.DataFrame(scaled_views_and_length, columns = views_and_length.columns)\n\n# Scale the code features\nss_code = StandardScaler()\nscaled_code_features = ss_code.fit_transform(code_features.astype(float))\nscaled_code_features_df = pd.DataFrame(scaled_code_features, columns = code_features.columns)\n\n# Concatenate the results into complete preprocessed dataframe.\nfinal_scaled_vectorized_df = pd.concat([vectorized_title_df, vectorized_question_df, scaled_views_and_length_df, scaled_code_features_df], axis = 1)\nfinal_scaled_vectorized_df.head(2)","fd183eb7":"final_scaled_vectorized_df.info()","9346f161":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\n\nclassifier = Sequential()\nclassifier.add(Dense(units = 10000,\n                     input_shape = (30040,),\n                     kernel_initializer = 'glorot_uniform',\n                     activation = 'relu'\n                    )\n              )  \n#classifier.add(Dropout(0.35))\nclassifier.add(Dense(units = 1150,\n                     kernel_initializer = 'glorot_uniform',\n                     activation = 'relu'\n                    )\n              )\n#classifier.add(Dropout(0.25))\nclassifier.add(Dense(units = 130,\n                     kernel_initializer = 'glorot_uniform',\n                     activation = 'relu'\n                    )\n              )\n#classifier.add(Dropout(0.25))\nclassifier.add(Dense(units = 50,\n                     kernel_initializer = 'glorot_uniform',\n                     activation = 'relu'\n                    )\n              )\nclassifier.add(Dense(units = 5,\n                     kernel_initializer = 'glorot_uniform',\n                     activation = 'softmax'\n                    )\n              )\n\nclassifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])","9ecdd747":"from sklearn.model_selection import train_test_split\n\nX = final_scaled_vectorized_df\ny = pd.get_dummies(processed_df['labels'])\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","4051e148":"#training the data. A batch size and number of epochs were chosen to remain within the memory constraints.\nclassifier.fit(X_train, y_train, batch_size = 42, epochs = 75)","76ec2380":"# store the predictions. \npredictions = classifier.predict(X_test)\npredictions","759dc58a":"# Retrieve labels by taking the max value in each row and convert it to a 1, effectively 'labeling' the resutls.\nfrom sklearn.preprocessing import LabelBinarizer\n\nlabels = np.argmax(predictions, axis = 1)\nlb = LabelBinarizer()\nlabeled_predictions = lb.fit_transform(labels)","96819fcc":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test.values, labeled_predictions))","ceecbbd0":"from sklearn.metrics import confusion_matrix\n\nprint('Confusion matrix for Java')\nprint(confusion_matrix(y_test.loc[:,'sql'], labeled_predictions[:,0]))\nprint('\\n')\n\nprint('Confusion matrix for Javascript')\nprint(confusion_matrix(y_test.loc[:,'javascript'], labeled_predictions[:,1]))\nprint('\\n')\n\nprint('Confusion matrix for Python')\nprint(confusion_matrix(y_test.loc[:,'python'], labeled_predictions[:,2]))\nprint('\\n')\n\nprint('Confusion matrix for R')\nprint(confusion_matrix(y_test.loc[:,'r'], labeled_predictions[:,3]))\nprint('\\n')\n\nprint('Confusion matrix for SQL')\nprint(confusion_matrix(y_test.loc[:,'sql'], labeled_predictions[:,4])) \n","ce3b0c6d":"Here we are better able to visualize which features are the most prominent amongst their respective languages. ","86d8eda2":"Now to implement the neural network, we need to combine all of the desired features. This includes the bag of words, the code features, the `'length_of_question'` column, and the `'views'` column.\nClearly, the values of `'length_of_question'`, and `'views'` will far outnumber the ones and zeros of the countvectorized columns, as well as the code syntax totals. This can heavily skew the result of our neural network. In order to avoid this we will use sci-kit learn's StandardScaler to scale the values within an appropriate range. I chose to scale each section seperately within their \"category' because I do not know which features have the most impact in determining what language the question pertains to. Perhaps, the language is more heavily dicatacted by the code_features than the content? As such, keep them scaled within their respective categories. ","5b515c2e":"It must be noted that this heatmap is skewed based on how many of each type of question was retrieved by the SQL query. Nonetheless, some observations can still be made: \n* Amongst the special characters in both python and Java, periods occur quite often. This makes sense considering that both languages are object-oriented, where their methods are denonted by periods. \n* The quote count for javascript is extremely high. This is due to the fact that javascript is used in conjunction with HTML, in which text denoted by quotes occur more frequently. \n\nIn order to remove the effects of the uneven retrieval of data from the SQL query, we will use sci-kit `preprocessing` library.","efcac3fe":"From these heatmaps, a couple of observations can be found:\n* The square bracket count, quote count, operator count and period count occur the most frequently in python.\n* For Javascript, quotes and operators clearly appear the most frequenctly.\n* For Java, the occurrences of each special character is fairly even. \n* In SQL, these special characters rarely occur aside from arithmetic operatos. This is a largely due to SQL using white space and new lines to denote complete statements. \n* For R, quote counts, operator counts, and period counts occur most often. \n\nMany of these observations make sense, with knowledge of each language's syntactic difference. \nIn order to get a complete overview, a heatmap displaying the total of of each special character grouped by their language is plotted. ","44eff85d":"# Data Visualization\n\nNow we will begin exploring the data to determine what features to extract and train our neural network on. The first features that come to mind are the number of views each question received as well as the length of each question. For example, perhaps due to the complexity of a language over another, the questions pertaining to that langauge may be longer in length. Additionally, perhaps due to popularity of a certain langauge over another, questions related to one langauge may have more views over another. These features were the first to come to mind due to already being numeric within the dataframe.  ","16aa4f6a":"It appears that most languages are distrbuted evenly across number of views. There were some outliers amongst the languages however, I removed them for easier visualization. \n\nNow we will analyze the length of each question. This will be done by first applying the `len` function to the `question` column and placing the results into a new column labeled `length_of_question`.","022a2ef5":"# Retrieving Data from database using SQL","10430b93":"In order to actually implement the html parser onto the question column of our dataframe, we must define a function that retrieves the code. ","689402d0":"Next, we find the programming language tags and limit the questions to only those involving a single programming langauge. This is done for simplicity. \n","a91ffb49":"For this project, we will be retrieving the title, body, tags, and views from the posts_questions table. Shown below is the query and command used to retrieve this information. The query ensures the retrieved data is tagged as either a python, java, javascript, sql, or R based question. Additionally, the length of the body was limited to only 1000 characters in length, and only 7500 results were retreived. These constraints were placed in order to prevent memory overflow. ","4fc633f2":"# Data Preprocessing","31624dcb":"In order to properly train our data, we must split our data into a training set and a test set. Again, we use sci-kit learn's `train-test-split` function. We choose `X` to be our preprocessed data and our `y` to be the labels. When setting y, we must remember to dummy encode the labels because the labels are categorical. ","8cb67db0":"We will now implement the `CountVectorizer` and covert the text into a \"bag-of-words\". Additionally, we will use tf-idf to scale the weights according to how relevant each word is to the dataset. We will store the results into new dataframes to be concatenated with the rest of our neural network inputs later.\n\n**Edit: The Multinomial naive-Bayes was experimented with and without using term frequency - inverse document frequency scaling. This process was repeated for the bag of words concatenated with the extracted code features.*","b27d49a6":"# Conclusion\nUsing some feature engineering and natural language processing techniques, I was able to train a neural network to classify questions from Stack Overflow based solely on their titles and content with a precision, recall, and f1-score of approximately 0.93. These results were achieved using the Multinomial Naive Bayes Classifier in conjunction with the extracted features from the code blocks of the text. In the future, I would like to develop an way to gather information from syntactic features that aren't described by characters. These features include spacing and stylistic tendencies users in each language tend towards. Additionally, this project shows how powerful the naive Bayes model is, despite the assumption that features are independent within classes. It is possible this classifier worked so well because the conditional probabilities between words are so small, they are almost negligible. ","abc9a71e":"Clearly, no obvious trends are visible. The heigth of the bars indicate how many questions had a length within that range, and are affected by how many questions are about each language. From the plot, the questions are fairly similar in length throughout. \n \n ## Extracting Features from Code\nNow that we know that didn't work we have to find different features to train our neural network on. \nSome background: In typical stack overflow questions, questions are asked by first summarizing the problem, and then including code blocks describing what the user has tried. \nWe know that each language has syntactic difference, for example, in R, values are assigned using a less than sign along with a dash('<-'), whereas in python, values are assigned using the equals sign('='). What if we honed in on these differences? Additional differences that come to mind are the number of brackets used, the number of arithmetic operators, and even punctuation such as periods or slashes. \n\nOkay we have a plan of action, how do we actually implement this? Well first we have to parse the question strings for the code blocks. These questions are written in html and as such we can use a html parser to retrieve those code blocks. ","649503b0":"Now that we've extracted features from the `code` column, we can begin processing the actual contents of the questions and their respective titles. In order to do this we will use an html parser to extract the text from those columns, and then use a `CountVectorizer` to convert those words into a sparse matrix to serve as inputs to our neural network. This is also known as converting text into a \"bag-of-words\" and is a common natural language processing technique. \n\nTo do this, we will parse the question for any text. For this project, we will only be using english text. We will also exclude numbers and stopwords. ","00d9cf24":"# Building the Deep Learning Network\nWe will not begin constructing our neural network. We will do this useing Keras, which is a framework for Google's TensorFlow library. Notice that from the `.info()` method above, we observe that our input dataframe has 30,183 entries. This will play a role in determining the number of neurons each hidden layer contains as well as how many hidden layers we have. I chose the number of neurons to gradually decrease until the expected five outputs(one for each classification). Additionally, for the hidden layers the activation function was chosen to be `'relu'` and the output activation function was chosen to be `'softmax'`, which is standard for single label classification problems such as this.   ","50b3cc79":"# Evaluation\nIn order to evaluate the results of the network, we will use the sci-kit learn's `classification_report` and `confusion matrix`. The classification report will give the precision, recall, and f1-score of the results of an neural network, while the confusion matrix will indicate how many rows were properly classified.","ddc80526":"Now that we've applied our function and retrieved our code, we need to extract the features of that code. We will now define functions that retrieve these features and apply them to our newly created `code` column. ","b107bc7b":"**note, I attempted to use k-fold cross validation, however due to memory constraints, I was unable to.*\n\n**(05\/28\/19)This kernel was updated to include dropout layers to reduce the possibility of overfitting, however, better results were achieved without the dropout layers.*\n","5d7454c0":"Now that we have verified that the functions worked as intended, we will use heatmaps to visualize the results, and potentially determine if there are any noticeable difference in each languages' syntax. ","3d812ed0":"# Project Overview\nThis project aims to classify questions retrieved from the Stack Overflow database based on the language they are related to. In order to accomplish this, the data will first be retrieved using an SQL query. Following, natural langauge processing(NLP) techniques and feature engineering will be used in conjunction with a deep learning neural network in an attempt to classify the questions. This will be my first kernel here on Kaggle and I'm really excited to share what I've accomplished. \n\n**I wished to include more techniques such as k-fold cross validation and use more data, however due to the 13 GB memory limit, I was unable to. However, I just want to thank Kaggle for allowing me to use a GPU compatible with TensorFlow as my own laptop runs on an AMD GPU.*  ","7f29e728":"First, we must split the tags. From the header, we can observe that the tags are seperated by the character '|' and are split accordingly.","75462f88":"## Extracting Features from Content","07d54899":"First, we will explore the distribution of the number of view each question got based on what language they were related to. "}}