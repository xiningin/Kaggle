{"cell_type":{"f4bee795":"code","1f408fc6":"code","51d8404d":"code","a0bbe563":"code","94ae5951":"code","2409f6fa":"code","8ef9fcc4":"code","517ed600":"code","898c67df":"code","25406829":"code","3e10d320":"code","3d5ca267":"code","6cc606c2":"code","c60df007":"markdown","6dc08e90":"markdown","816face8":"markdown","70da3cf0":"markdown","2adc4e18":"markdown","626c6726":"markdown","611b4cd1":"markdown","43c347ed":"markdown","26f0591e":"markdown","295bc955":"markdown","a959b465":"markdown","e3595429":"markdown","8331fdb2":"markdown","23c9606e":"markdown","206a264c":"markdown"},"source":{"f4bee795":"import numpy as np\nimport pandas as pd\nimport os\n\n# printing path to data file\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print('DATA PATH:  {}'.format(os.path.join(dirname, filename)))\n\n# reading data as dataframe, printing shape, printing features list and type,\n# printing number of NaNs per feature, printing first 3 rows\ndata = pd.read_csv('\/kaggle\/input\/austin-weather\/austin_weather.csv', parse_dates=['Date'])\nprint('DATA SHAPE: {}'.format(data.shape))\nprint('------------------------------------------------------------------')\nprint('FEATURES LIST AND TYPES:\\n{}'.format(data.dtypes))\nprint('------------------------------------------------------------------')\nprint('NUMBER OF NaNs PER FEATURE:\\n{}'.format(data.isnull().sum(axis=0)))\ndata.head(3)","1f408fc6":"# replacing ' ' with 'Clear'\ndata['Events'] = data['Events'].replace(' ', 'Clear')\n\n# adding feature 'DayOfYear'\ndays_of_year = [date.dayofyear for date in data['Date']]\ndata['DayOfYear'] = days_of_year\n\ndata[['Date', 'DayOfYear', 'Events']].head(3)","51d8404d":"from sklearn.impute import SimpleImputer\n\n# printing '-' and 'T' count before removing them\nnumber_of_nans = data.stack().value_counts()['-']\nnumber_of_ts = data.stack().value_counts()['T']\nprint('BEFORE imputing and replacing, in the dataset there are {} \"-\" and {} \"Ts\"'.format(number_of_nans,number_of_ts))\n\n# replacing '-' with NaN to later use SimpleImputer\ndata = data.replace('-', float('NaN'))\ndata['PrecipitationSumInches'] = data['PrecipitationSumInches'].replace('T', 0.0005)\n\n# list of features to be checked for NaNs, imputing\nto_be_imputed = list(set(data.columns)-set(['Date', 'Events']))\n\nimp = SimpleImputer(missing_values=float('NaN'), strategy='mean')\ndata[to_be_imputed] = imp.fit_transform(data[to_be_imputed])\n\n# printing '-' and 'T' count after removing them\nif '-' not in data.stack().value_counts(): number_of_nans = 0\nif 'T' not in data.stack().value_counts(): number_of_ts = 0\nprint('AFTER imputing and replacing, in the dataset there are {} \"-\" and {} \"Ts\"'.format(number_of_nans,number_of_ts))","a0bbe563":"from sklearn.preprocessing import MinMaxScaler\n\n# not all features need to be scaled: 'Date' and 'Events' don't need scaling. \n# 'Date' is just there for intepretability and will not be used for classification\n# in its place 'DayOfYear' will be used. \n# 'Events' represents the class. \n# Therefore i prepare the list of feature names that need scaling to scale just those\n# i keep the original data untouched to later make a comparison\nscaled_data = data.copy()\nto_be_scaled = list(set(data.columns)-set(['Date', 'Events']))\nscaler = MinMaxScaler()\nscaler.fit(data[to_be_scaled])\nscaled_data[to_be_scaled] = scaler.transform(data[to_be_scaled])","94ae5951":"from sklearn.preprocessing import LabelEncoder\n\n# encoding\npreprocessed_data = scaled_data\nencoder = LabelEncoder()\nencoder.fit(scaled_data['Events'])\npreprocessed_data['Events'] = encoder.transform(scaled_data['Events'])\n\n# printing\nclasses_occurrences = preprocessed_data['Events'].value_counts().to_frame()\nclasses_occurrences['Class'] = encoder.inverse_transform(classes_occurrences.index)\nclasses_occurrences = classes_occurrences.sort_index(axis=0)\nprint(classes_occurrences)","2409f6fa":"# relabeling\npreprocessed_data['Events'] = preprocessed_data['Events'].replace([1], 0)\npreprocessed_data['Events'] = preprocessed_data['Events'].replace([2,5,6], 1)\npreprocessed_data['Events'] = preprocessed_data['Events'].replace([3,4,7,8], 2)\n\n# printing classes and number of occurrences\nclasses_occurrences = preprocessed_data['Events'].value_counts().to_frame()\nclasses_occurrences['Class'] = ['Clear', 'Rain', 'Thunderstorm']\nclasses_occurrences = classes_occurrences.sort_index(axis=0)\nprint(classes_occurrences)","8ef9fcc4":"import matplotlib.pyplot as plt\nfrom matplotlib import cm\n%matplotlib inline\nplt.style.use('seaborn-darkgrid')\n\nx_axis_original = data['HumidityHighPercent']\ny_axis_original = data['PrecipitationSumInches']\n\nx_axis_scaled = preprocessed_data['HumidityHighPercent']\ny_axis_scaled = preprocessed_data['PrecipitationSumInches']\n\nrainbow = cm.get_cmap('rainbow', 3)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20, 8))\naxes[0].scatter(x_axis_original, y_axis_original, c=preprocessed_data['Events'], cmap=rainbow)\nright_plot = axes[1].scatter(x_axis_scaled, y_axis_scaled, c=preprocessed_data['Events'], cmap=rainbow)\ncbar = fig.colorbar(right_plot, ticks=[0, 1, 2])\ncbar.ax.set_yticklabels(classes_occurrences['Class'])\nplt.tight_layout()","517ed600":"from sklearn.decomposition import PCA\n\nfeatures = list(set(preprocessed_data.columns)-set(['Date', 'Events']))\n\n# the set of first 10 features extracted with PCA\npca = PCA(n_components=len(features))\npca.fit(preprocessed_data[features])\nX_PCA = pd.DataFrame(data=pca.transform(preprocessed_data[features]))\n\ny = preprocessed_data['Events']\n\nprint('X_PCA shape: {}'.format(X_PCA.shape))","898c67df":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# function i'll use later to asses the model's performances\ndef evaluate(y_test, y_pred, avg):\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average=avg)\n    recall = recall_score(y_test, y_pred, average=avg)\n    f1 = f1_score(y_test, y_pred, average=avg)\n\n    return (accuracy, precision, recall, f1)","25406829":"from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\n# for storing scores\naccuracies = []\nprecisions = [[],[],[]]\nrecalls = [[],[],[]]\nf1s = [[],[],[]]\n# different ways of calculating the average\navgs = ['macro', 'micro', 'weighted']\n\n# each iteration increases by one the number of principal components used for the training\nfor i in range(2, 21):\n    X_train, X_test, y_train, y_test = train_test_split(X_PCA[X_PCA.columns[:i]], y, test_size=0.3, random_state=0) # 70 - 30 split\n    clf = SVC(gamma='auto', kernel='linear')\n    clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test)\n    \n    # saving scores for later plotting\n    for j in range(len(avgs)):\n        acc, prec, rec, f1 = evaluate(y_test, predictions, avgs[j])\n        precisions[j].append(prec)\n        recalls[j].append(rec)\n        f1s[j].append(f1)   \n        if j==0:\n            accuracies.append(acc)\n\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 8))\nlines = []\nfor i in range(3):\n    l1=axes[i].plot(range(2,21), accuracies, color='red')[0]\n    l2=axes[i].plot(range(2,21), precisions[i], color='blue')[0]\n    l3=axes[i].plot(range(2,21), recalls[i], color='green')[0]\n    l4=axes[i].plot(range(2,21), f1s[i], color='orange')[0]\n    lines = [l1, l2, l3, l4]\n    axes[i].set_title('Scores calculated with {} average'.format(avgs[i]))\n    axes[i].set(xlabel='Number of PCA components')\nfig.legend(lines, ['Accuracy', 'Precision', 'Recall', 'F1'], loc='center right', fontsize=16)\nplt.tight_layout()","3e10d320":"X_train, X_test, y_train, y_test = train_test_split(X_PCA[X_PCA.columns[:8]], y, test_size=0.3, random_state=0) # 70 - 30 split\nclf = SVC(gamma='auto', kernel='linear')\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\nconfusion_matrix = pd.crosstab(predictions, y_test, rownames=['Actual'], colnames=['Predictions'])\n\nimport seaborn as sn\n\nsn.set(font_scale=1.4)\nsn.heatmap(confusion_matrix, annot=True,annot_kws={\"size\": 12}, fmt=\"d\", cmap=\"YlGnBu\")\nplt.show()","3d5ca267":"from sklearn.model_selection import GridSearchCV, KFold\nfrom time import gmtime, strftime\n\n# converting to numpy array to do THIS later\nX = X_PCA[X_PCA.columns[:15]].to_numpy()\n\n# sets of parameters to be tested\nkernels = ['rbf', 'poly', 'linear', 'sigmoid']\ncs = [2**i for i in range(1, 5)]\ndegrees = [i for i in range(2, 5)]\ngammas = ['auto', 'scale']\ncoef0s = [2**i for i in range(1, 5)]\nparameters = {'kernel': kernels, 'C': cs, 'degree': degrees, 'gamma':gammas, 'coef0': coef0s}\nscores = []\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=0)\n\ni = 0\nprint('Starting at: {}'.format(strftime(\"%H:%M:%S\", gmtime())))\nprint('')\nfor train_index, test_index in folds.split(X):\n    print('Working on {}-th fold... '.format(i))\n    i+=1\n    \n    # THIS can't be (this easily) done with a DataFrame\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    svc = SVC()\n    clf = GridSearchCV(svc, parameters, cv=5, scoring='accuracy')\n    clf.fit(X_train, y_train)\n    predictions = clf.predict(X_test)\n\n    scores.append({'best_score': clf.best_score_, 'best_param': clf.best_params_})\nprint('')\nprint('Ending at: {}'.format(strftime(\"%H:%M:%S\", gmtime())))\nprint('')\n\nmax_score = 0\nbest_params = {}\nfor score in scores:\n    acc = score['best_score']\n    params = score['best_param']\n    if(acc > max_score):\n        max_score = acc\n        best_params = params\n    print('Best_Score {}'.format(acc))\n    print('Parameters {}'.format(params))","6cc606c2":"X_train, X_test, y_train, y_test = train_test_split(X_PCA[X_PCA.columns[:15]], y, test_size=0.2, random_state=0)\n\nclf = SVC(kernel=best_params['kernel'], C=best_params['C'], degree=best_params['degree'], gamma=best_params['gamma'], coef0=best_params['coef0'])\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\n\nconfusion_matrix = pd.crosstab(predictions, y_test, rownames=['Actual'], colnames=['Predictions'])\n\nsn.set(font_scale=1.4)\nsn.heatmap(confusion_matrix, annot=True,annot_kws={\"size\": 12}, fmt=\"d\", cmap=\"YlGnBu\")\nplt.show()\n\n# different ways of calculating the average\navgs = ['macro', 'micro', 'weighted']\n\naccuracies = []\nprecisions = []\nrecalls = []\nf1s = []\n# saving scores for later plotting\nfor j in range(len(avgs)):\n    acc, prec, rec, f1 = evaluate(y_test, predictions, avgs[j])\n    precisions.append(prec)\n    recalls.append(rec)\n    f1s.append(f1)   \n    accuracies.append(acc)\n\nscores = {'Average': avgs, 'Accuracy': accuracies, 'Precision': precisions, 'Recall': recalls, 'F1': f1s}\nscores_df = pd.DataFrame(data=scores)\nscores_df.head(3)","c60df007":"As we can see, the model isn't very good at classifying rain: it gets right 23 examples, but classifies 7 true rains as thunderstorms and 8 as clear weather.","6dc08e90":"# SVC - Support Vector Classifier, testing different numbers of principal components\nScikit-learn offers an SVC implementation that supports multiclass problems which are handled using the one vs one technique.\nIn this section, i test the behaviour of SVC with an increasing number of principal components.\n","816face8":"# Preprocessing: Encoding Labels\nTo encode the target feature 'Events' i'm going to use LabelEncoder, an encoder specifically built to encode target variables. The encoder encodes each value to a value in [0, n_classes-1]","70da3cf0":"# Original data vs scaled data\nFor curiosity's sake (and as an excuse for plotting stuff) i'll plot the same scatter plot with the original and scaled features. The features i chose are: Humidity and Precipitation. Being both measured with positive values, min-max scaler scales the values in the range [0, 1]. The color of the data points drawn is based on the class they are example of.","2adc4e18":"# Reading and preparing the data\n","626c6726":"Just what i wanted. Now i'll replace '-' with NaN, 'T' with 0.0005 and use SimpleImputer to fill the missing values (NaNs).","611b4cd1":"Here we can see how using different ways of calculating the average (for multiclass problems) affects the scores. The scores used are:\n\n* Accuracy: $$ \\frac{Correct\\ Predictions}{Total\\ Predictions} $$\n* Precision: $$ \\frac{True\\ Positive}{True\\ Positive\\ +\\ False\\ Positive} $$\n* Recall: $$ \\frac{True\\ Positive}{True\\ Positive\\ +\\ False\\ Negative} $$\n* F1: $$ 2\\ \u22c5\\ \\frac{Precision\\ \u22c5\\ Recall}{Precision\\ +\\ Recall} $$\n\nAccuracy is the simplest score but can be misleading when the number of examples of different classes are disproportionate.\n\nPrecision should be maximized when it's important to minimize false positives\n\nRecall should be maximized when it's important to minimize false positives\n\nF1 is an instance of F Measure where \u03b2 = 1. Given the definition of F measure:\n\n$$ (1+\u03b2^2)\\ \u22c5\\ \\frac{Precision\\ \u22c5\\ Recall}{(\u03b2^2\\ \u22c5\\ \\ Precision)\\ +\\ Recall} $$\n\nWhen \u03b2 > 1 the F measure weighs recall higher, when \u03b2 < 1 the F measure weighs precision higher.\n\nTo get a better idea of what's going on, i'll print the **confusion matrix** of SVC on 8 principal components.","43c347ed":"# Outline\nI make this kernel to get some experience with the general ML pipeline:\n* Analyze and clean the data\n* Preprocess the data\n* Do feature selection\/extraction\n* Do model selection and evaluation","26f0591e":"I'll now train a model using the most promesing parameters","295bc955":"We can notice that some classes are under represented with just a few examples.\nFor semplicity sake, i'll simplify the problem by adjusting the labeling by the following rule:\n\nif that day it didn't rain, then label = 0\n\nelse if that day there was a thunderstorm, then label = 2\n\nelse label = 1","a959b465":"Much better results","e3595429":"# Model Selection: KFold nested cross validation\nFrom the plots above 15 seems a reasonable number of components to keep.\n\nIn this section i use KFold and GridSearchCV to make model selection.\n\nKFold takes the data and returns the indeces neccessary to divide it in K folds. K-1 folds will be used as training set and 1 as test set. Iteratively each fold will be used once as test set.\n\nGridSearchCV uses an estimator with the given grid of parameters.","8331fdb2":"# Preprocessing: Scaling\nSince this is meteorological data, i'm not expecting huge differences in values (considering a single feature), that is, i'm not expecting outlying values. However different features differ quite a bit in their values range. On top of that, many estimators expect the data to be standardized. For these reasons i choose to use the Min-Max scaler which follows the formula:\n\n$$X_{sc} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n\n\nEssentially what it does is shrink the range of values to [0, 1] or [-1, 1] if there are negative values","23c9606e":"# Feature Extraction\/Selection: PCA\nNext i'm going to do some feature extraction\/selection. I'll then try classification. \nIn a nutshell PCA finds a new set of axes that maximizes the variance in the data. The axes are ordered such that the variance in axis_i is greater than the variance in axis_i+1.\nBy taking the first N axes then, we will have the N axes that better represent the data.\nIn this case, i only have 20 features, and all of them are quite relevant. Using PCA may be overkill.\nHowever to experiment with PCA, i'll keep 20 components, and then train the classification algorithm iteratively with increasing number of components, to see the effects on the performances.","206a264c":"3 important things to notice:\n* There are no NaNs\n* There are many features of type object, however, these are mostly numerical with the exception of feature 'Events'\n* 'Events' uses a string to represent the weather for the day. The string ' ' (space) represents clear weather.\n\nFirst i'm going to replace ' ' with 'Clear' to make the meaning of the value more *clear* (eheh) \n\nSecondly i'm going to add a feature: 'DayOfYear' which is the day of the year, meaning Jan 1 is day 1, Dec 31 is day 365 (assuming the year is not a leap year). This will make the calculations easier and will preserve the information the date brings to the dataset (since weather is seasonal) Note: this could be considered feature extraction.\n\nThirdly, even though there are no NaNs, the dataset documentation says that **in place of missing values there is the string '-'**. So i'm going to replace '-' with NaN and use an imputer to impute the missing values. Furthermore, **the feature 'PrecipitationSumInches' uses the value 'T' to represent trace amounts** of precipitation. To solve this problem i can either assign a value lower than the lowest value (e.g. 0.0005 since the lowest value is 0.01) or assign 0. I'll choose the first option since it seems the most correct."}}