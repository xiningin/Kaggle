{"cell_type":{"b5571e8e":"code","fdd4af64":"code","d83e9c59":"code","839bcc8a":"code","df05b436":"code","a54c97a7":"code","95729248":"code","4dfbf86f":"code","26e731b2":"code","14adcd15":"code","d810b57c":"code","feb61721":"code","ab7ef7fa":"code","d9693f60":"code","950e14d4":"code","fc6972a2":"code","b54c4f22":"code","4d90b93e":"code","c635ec97":"code","47e5f0b9":"code","442dbb0a":"code","88d4bdeb":"code","408b23bb":"code","158badd1":"code","692780a9":"markdown","63d2efd0":"markdown","987e9a7c":"markdown"},"source":{"b5571e8e":"# params we will probably want to do some hyperparameter optimization later\nBASE_MODEL= 'InceptionV3' # ['VGG16', 'RESNET52', 'InceptionV3', 'Xception', 'DenseNet169', 'DenseNet121', 'InceptionResNet']\nIMG_SIZE = (512, 512) # [(512, 512), (256, 256), (128, 128)]\nBATCH_SIZE = 12 # [1, 8, 16, 24]\nDROPOUT = 0.25 # [0, 0.25, 0.5]\nLEARN_RATE = 1e-4 # [1e-4, 1e-3, 4e-3]\nCROP_SIZE = 20\nTEST_SIZE = 0.25\nMAX_STEPS = 500\nDOWNSCALE_OUTPUT = 8\nEPOCHS = 15 # [100, 100]\nUSE_AUGMENTATION = False # [True, False]","fdd4af64":"%matplotlib inline\nimport os, sys\nimport h5py, numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport doctest\nimport copy\nimport functools\nfrom keras import models, layers\nfrom IPython.display import Image\nfrom keras.utils.vis_utils import model_to_dot\ndef show_model(in_model):\n    f = model_to_dot(in_model, show_shapes=True, rankdir='UD')\n    return Image(f.create_png())\ndef autotest(func):\n    globs = copy.copy(globals())\n    globs.update({func.__name__: func})\n    doctest.run_docstring_examples(\n        func, globs, verbose=True, name=func.__name__)\n    return func\nseg_dir = '..\/input\/'","d83e9c59":"train_h5 = h5py.File(os.path.join(seg_dir, 'train_segs.h5'), 'r')\nfor k in train_h5.keys():\n    print(k, train_h5[k].shape, train_h5[k].chunks, train_h5[k].dtype)","839bcc8a":"from keras.preprocessing.image import ImageDataGenerator\nif BASE_MODEL=='VGG16':\n    from keras.applications.vgg16 import VGG16 as PTModel, preprocess_input\nelif BASE_MODEL=='RESNET52':\n    from keras.applications.resnet50 import ResNet50 as PTModel, preprocess_input\nelif BASE_MODEL=='InceptionV3':\n    from keras.applications.inception_v3 import InceptionV3 as PTModel, preprocess_input\nelif BASE_MODEL=='InceptionResNet':\n    from keras.applications.inception_resnet_v2 import InceptionResNetV2 as PTModel, preprocess_input\nelif BASE_MODEL=='Xception':\n    from keras.applications.xception import Xception as PTModel, preprocess_input\nelif BASE_MODEL=='DenseNet169': \n    from keras.applications.densenet import DenseNet169 as PTModel, preprocess_input\nelif BASE_MODEL=='DenseNet121':\n    from keras.applications.densenet import DenseNet121 as PTModel, preprocess_input\nelse:\n    raise ValueError('Unknown model: {}'.format(BASE_MODEL))","df05b436":"base_pretrained_model = PTModel(input_shape =  IMG_SIZE+(3,), \n                                include_top = False, \n                                weights = 'imagenet')\nbase_pretrained_model.trainable=False\nbase_pretrained_model.summary()","a54c97a7":"from collections import defaultdict, OrderedDict\nfrom keras.models import Model\nlayer_size_dict = defaultdict(list)\ninputs = []\nfor lay_idx, c_layer in enumerate(base_pretrained_model.layers):\n    if not c_layer.__class__.__name__ == 'InputLayer':\n        layer_size_dict[c_layer.get_output_shape_at(0)[1:3]] += [c_layer]\n    else:\n        inputs += [c_layer]\n# freeze dict\nlayer_size_dict = OrderedDict(layer_size_dict.items())\nfor k,v in layer_size_dict.items():\n    print(k, [w.__class__.__name__ for w in v])","95729248":"from collections import defaultdict\nfrom keras import layers\n@autotest\ndef resize_layer(in_layer, old_size, new_size, use_channels=False):\n    \"\"\"\n    Zeropad or crops layer to make it a specific size (useful for segmentation and combining models)\n    >>> f = layers.Input((10, 11, 12))\n    >>> resize_layer(f, (10, 11, 12), (20, 21))._keras_shape[1:]\n    (20, 21, 12)\n    >>> resize_layer(layers.Input((50, 49, 12)), None, (32, 33))._keras_shape[1:3]\n    (32, 33)\n    >>> resize_layer(layers.Input((50, 5, 12)), None, (32, 33))._keras_shape[1:3]\n    (32, 33)\n    \"\"\"\n    if old_size is None:\n        old_size = in_layer._keras_shape[1:]\n    assert not use_channels, \"Not implemented\"\n    x_old, x_new = old_size[0], new_size[0]\n    y_old, y_new = old_size[1], new_size[1]\n    \n    pad_args = defaultdict(lambda : None)\n    if x_old<x_new:\n        # zero-padding\n        pad_args['x_pad'] = (x_new-x_old)\n    elif x_old>x_new:\n        # cropping\n        pad_args['x_crop'] = (x_old-x_new)\n    if y_old<y_new:\n        # zero-padding\n        pad_args['y_pad'] = (y_new-y_old)\n    elif y_old>y_new:\n        # cropping\n        pad_args['y_crop'] = (y_old-y_new)\n    sf_pad_args = defaultdict(lambda : (0, 0))\n    use_crop = False\n    use_pad = False\n    for k, v in pad_args.items():\n        sf_pad_args[k] = (v\/\/2, v-v\/\/2)\n        if '_crop' in k: \n            use_crop=True\n        if '_pad' in k:\n            use_pad=True\n    out_layer = in_layer\n    if use_crop:\n        out_layer = layers.Cropping2D((sf_pad_args['x_crop'], sf_pad_args['y_crop']))(out_layer)\n    if use_pad:\n        out_layer = layers.ZeroPadding2D((sf_pad_args['x_pad'], sf_pad_args['y_pad']))(out_layer)\n    return out_layer","4dfbf86f":"def _wrap_layer(cur_layer):\n    channel_count = cur_layer._keras_shape[-1]\n    cur_layer = layers.Conv2D(channel_count\/\/2, kernel_size=(3,3), \n                       padding = 'same', \n                              use_bias=False,\n                       activation = 'linear')(cur_layer)\n    cur_layer = layers.BatchNormalization()(cur_layer) # gotta keep an eye on that internal covariant shift\n    cur_layer = layers.LeakyReLU(0.1)(cur_layer)\n    return cur_layer\ndef unet_from_encoder(encoder_model, out_shape, depth=2):\n    out_x, out_y, out_dim = out_shape\n    last_layer = None\n    layer_size_dict = defaultdict(list)\n    inputs = []\n    for lay_idx, c_layer in enumerate(encoder_model.layers):\n        if not c_layer.__class__.__name__ == 'InputLayer':\n            layer_size_dict[c_layer.get_output_shape_at(0)[1:3]] += [c_layer]\n        else:\n            inputs += [c_layer]\n    # freeze dict\n    layer_size_dict = OrderedDict(layer_size_dict.items())\n    for i in reversed(range(depth+1)):\n        if last_layer is not None:\n            last_layer = layers.UpSampling2D((2, 2))(last_layer)\n        cur_x = out_x\/\/2**i\n        cur_y = out_y\/\/2**i\n        print(cur_x, cur_y)\n        best_match_layers = sorted([(np.abs(x_dim-cur_x)+np.abs(y_dim-cur_y), v)\n         for (x_dim, y_dim), v in layer_size_dict.items()], key=lambda x: x[0])[0][-1]\n        last_best_match = best_match_layers[-1]\n        print(last_best_match.output_shape)\n        c_layer = last_best_match.get_output_at(0)\n        pad_layer = resize_layer(c_layer, \n                                 last_best_match.output_shape[1:], \n                                 (cur_x, cur_y))\n        if last_layer is None:\n            last_layer = pad_layer\n        else:\n            last_layer = layers.concatenate([last_layer, pad_layer])\n        last_layer = _wrap_layer(last_layer)\n        \n    final_output = layers.Conv2D(out_dim, kernel_size=(1,1), padding = 'same', activation = 'sigmoid')(last_layer)\n    final_output = layers.Cropping2D((CROP_SIZE\/\/DOWNSCALE_OUTPUT, CROP_SIZE\/\/DOWNSCALE_OUTPUT))(final_output)\n    final_output = layers.ZeroPadding2D((CROP_SIZE\/\/DOWNSCALE_OUTPUT, CROP_SIZE\/\/DOWNSCALE_OUTPUT))(final_output)\n    return models.Model(inputs=[encoder_model.get_input_at(0)],\n                outputs=[final_output])","26e731b2":"unet_model = unet_from_encoder(base_pretrained_model, \n                               (512\/\/DOWNSCALE_OUTPUT, \n                                512\/\/DOWNSCALE_OUTPUT, 1),\n                              depth=2)\nshow_model(unet_model)","14adcd15":"img_gen_args = dict(samplewise_center=False, \n                              samplewise_std_normalization=False, \n                              horizontal_flip = True, \n                              vertical_flip = False, \n                              height_shift_range = 0.05, \n                              width_shift_range = 0.02, \n                              rotation_range = 3, \n                              shear_range = 0.01,\n                              fill_mode = 'nearest',\n                              zoom_range = 0.05,\n                               preprocessing_function=preprocess_input)\nimg_gen = ImageDataGenerator(**img_gen_args)","d810b57c":"last_train_index = int(train_h5['image'].shape[0]*(1-TEST_SIZE))\ndef make_gen(start_idx, last_idx, batch_size, seed=None):\n    if seed is None:\n        seed = np.random.choice(range(9999))\n    flow_args = dict(**img_gen_args)\n    img_gen = ImageDataGenerator(**img_gen_args)\n    mask_args = flow_args.copy()\n    mask_args.pop('preprocessing_function')\n    mask_gen = ImageDataGenerator(**mask_args)\n    while True:\n        c_index = np.random.choice(range(start_idx, last_idx-batch_size))\n        image_val = train_h5['image'][c_index:(c_index+batch_size)]\n        image_val = np.concatenate([image_val, image_val, image_val], -1)\n        mask_val = train_h5['mask'][c_index:(c_index+batch_size)][:, ::DOWNSCALE_OUTPUT, ::DOWNSCALE_OUTPUT]\n        if USE_AUGMENTATION:\n            for x, y in zip(img_gen.flow(image_val, seed = seed), \n                            mask_gen.flow(mask_val, seed = seed)):\n                yield x, y\n        else:\n            yield image_val, mask_val \ntrain_gen = make_gen(0, last_train_index, BATCH_SIZE)\nvalid_gen = make_gen(last_train_index, train_h5['image'].shape[0],  BATCH_SIZE)","feb61721":"a, b = next(train_gen)\nprint(a.shape, b.shape)","ab7ef7fa":"from skimage.util.montage import montage2d\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\nax1.imshow(montage2d(a[:, :, :, 0]), cmap='bone')\nax2.imshow(montage2d(b[:, :, :, 0]))","d9693f60":"import keras.backend as K\nfrom keras.optimizers import Adam\nfrom keras.losses import binary_crossentropy\nimport tensorflow as tf\ndef get_iou_vector(A, B):\n    batch_size = A.shape[0]\n    metric = 0.0\n    for batch in range(batch_size):\n        t, p = A[batch], B[batch]\n        true = np.sum(t)\n        pred = np.sum(p)\n        # deal with empty mask first\n        if true == 0:\n            metric += (pred == 0)\n            continue \n        # non empty mask case.  Union is never empty \n        # hence it is safe to divide by its number of pixels\n        intersection = np.sum(t * p)\n        union = true + pred - intersection\n        iou = intersection \/ union\n        \n        # iou metrric is a stepwise approximation of the real iou over 0.5\n        iou = np.floor(max(0, (iou - 0.45)*20)) \/ 10\n        \n        metric += iou \n    # teake the average over all images in batch\n    metric \/= batch_size\n    return metric\n\ndef iou_score(label, pred):\n    # Tensorflow version\n    return tf.py_func(get_iou_vector, [label, pred > 0.5], tf.float64)\ndef dice_coef(y_true, y_pred, smooth=1):\n    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n    return K.mean( (2. * intersection + smooth) \/ (union + smooth), axis=0)\ndef dice_p_bce(in_gt, in_pred):\n    return 0.0*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\ndef true_positive_rate(y_true, y_pred):\n    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))\/K.sum(y_true)\nfor i_lay in unet_model.layers:\n    if i_lay in base_pretrained_model.layers:\n        i_lay.trainable=False\n\nunet_model.compile(optimizer=Adam(1e-3, decay = 1e-6), \n                   loss=dice_p_bce, \n                   metrics=[dice_coef, \n                            iou_score,\n                            'binary_accuracy', \n                            true_positive_rate,])\nloss_history = []\nunet_model.summary()","950e14d4":"from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\nweight_path=\"{}_weights.best.hdf5\".format('vgg_unet')\n\ncheckpoint = ModelCheckpoint(weight_path, monitor='val_loss', verbose=1, \n                             save_best_only=True, mode='min', save_weights_only = True)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=10, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.0001)\nearly = EarlyStopping(monitor=\"val_loss\", \n                      mode=\"min\", \n                      patience=5) # probably needs to be more patient, but kaggle time is limited\ncallbacks_list = [checkpoint, early, reduceLROnPlat]","fc6972a2":"step_count = min(last_train_index\/\/BATCH_SIZE, MAX_STEPS)\nloss_history += [unet_model.fit_generator(train_gen, \n                                         steps_per_epoch=step_count, \n                                         epochs=EPOCHS, \n                                         validation_data=valid_gen,\n                                          validation_steps=TEST_SIZE\/(1-TEST_SIZE)*step_count,\n                                         callbacks=callbacks_list\n                                       )]","b54c4f22":"def show_loss(loss_history):\n    epich = np.cumsum(np.concatenate(\n        [np.linspace(0.5, 1, len(mh.epoch)) for mh in loss_history]))\n    fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(22, 10))\n    _ = ax1.plot(epich,\n                 np.concatenate([mh.history['loss'] for mh in loss_history]),\n                 'b-',\n                 epich, np.concatenate(\n            [mh.history['val_loss'] for mh in loss_history]), 'r-')\n    ax1.legend(['Training', 'Validation'])\n    ax1.set_title('Loss')\n\n    _ = ax2.plot(epich, np.concatenate(\n        [mh.history['true_positive_rate'] for mh in loss_history]), 'b-',\n                     epich, np.concatenate(\n            [mh.history['val_true_positive_rate'] for mh in loss_history]),\n                     'r-')\n    ax2.legend(['Training', 'Validation'])\n    ax2.set_title('True Positive Rate\\n(Positive Accuracy)')\n    \n    _ = ax3.plot(epich, np.concatenate(\n        [mh.history['binary_accuracy'] for mh in loss_history]), 'b-',\n                     epich, np.concatenate(\n            [mh.history['val_binary_accuracy'] for mh in loss_history]),\n                     'r-')\n    ax3.legend(['Training', 'Validation'])\n    ax3.set_title('Binary Accuracy (%)')\n    \n    _ = ax4.plot(epich, np.concatenate(\n        [mh.history['dice_coef'] for mh in loss_history]), 'b-',\n                     epich, np.concatenate(\n            [mh.history['val_dice_coef'] for mh in loss_history]),\n                     'r-')\n    ax4.legend(['Training', 'Validation'])\n    ax4.set_title('DICE')\n\nshow_loss(loss_history)","4d90b93e":"unet_model.load_weights(weight_path, by_name=True)","c635ec97":"unlocked=0\nfor i_lay in unet_model.layers:\n    if i_lay in base_pretrained_model.layers:\n        i_lay.trainable=True\n        unlocked+=1\nprint('released {}\/{} layers'.format(unlocked, len(unet_model.layers)))\nunet_model.compile(optimizer=Adam(1e-4, decay=1e-6), \n                   loss=dice_p_bce, \n                   metrics=[dice_coef, \n                            iou_score,\n                            'binary_accuracy', \n                            true_positive_rate])\nunet_model.summary()","47e5f0b9":"loss_history += [unet_model.fit_generator(train_gen, \n                                         steps_per_epoch=step_count, \n                                         epochs=EPOCHS, \n                                         validation_data=valid_gen,\n                                          validation_steps=TEST_SIZE\/(1-TEST_SIZE)*step_count,\n                                         callbacks=callbacks_list\n                                       )]","442dbb0a":"show_loss(loss_history)","88d4bdeb":"unet_model.load_weights(weight_path)\nunet_model.save('full_model.h5')","408b23bb":"sample_X = train_h5['image'][last_train_index:]\nsample_X = np.concatenate([sample_X, sample_X, sample_X], -1)\nout_scores = unet_model.evaluate(sample_X,\n                              train_h5['mask'][last_train_index:][:,::DOWNSCALE_OUTPUT, ::DOWNSCALE_OUTPUT],\n                              batch_size=BATCH_SIZE\n                             )","158badd1":"for k, v in zip(unet_model.metrics_names, out_scores):\n    print('{}: {:.1%}'.format(k, v))","692780a9":"# Build Input Generators","63d2efd0":"# Overview\n- Using a pretrained model to segment\nHere is an example kernel where we use a pretrained VGG16 model as the encoder portion of a U-Net and thus can benefit from the features already created in the model and only focus on learning the specific decoding features. The strategy was used with LinkNet by one of the top placers in the competition. I wanted to see how well it worked in particular comparing it to standard or non-pretrained approaches, the code is setup now for VGG16 but can be easily adapted to other problems","987e9a7c":"# Release Encoder\nSee if we can finetune the encoder after training the decoder"}}