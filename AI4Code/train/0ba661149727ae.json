{"cell_type":{"e8e6b15b":"code","5d5f2f0f":"code","1d58d8eb":"code","3d030751":"code","2511814e":"code","312bd0f9":"code","08a5dad6":"code","e9066b8e":"code","319eea29":"code","aebdfa38":"code","70484279":"code","7a7fb32d":"code","dc58364f":"code","03203f8f":"code","07f9e737":"code","c12f52c7":"code","cbead37c":"code","906fe0ba":"code","4d71bb6c":"code","418d3e3a":"code","c4116cdf":"code","369b130d":"code","21a7e907":"code","7c905f97":"code","baf883a9":"code","899d6f87":"code","4815223f":"code","da5c2c5c":"code","d048db0b":"code","bed8e516":"code","d48932b4":"code","2f3aa4c8":"code","ea1ee5f8":"markdown","0ea29f05":"markdown","67cd4642":"markdown","e7159bd0":"markdown","a2e2caf9":"markdown","b543f5a1":"markdown","2ad28691":"markdown","d93afad9":"markdown","fdeed1ea":"markdown","d0beba42":"markdown","12dcd71d":"markdown","d57b7d9a":"markdown","6df4adc4":"markdown","5f85fb1d":"markdown","9a96178f":"markdown","55b2f511":"markdown","acd37c5a":"markdown","621c48e7":"markdown","9f34df38":"markdown","10229cfd":"markdown"},"source":{"e8e6b15b":"# libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nnp.random.seed(32)\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.manifold import TSNE\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import LSTM, Conv1D, MaxPooling1D, Dropout\nfrom keras.utils.np_utils import to_categorical\n\n\n%matplotlib inline","5d5f2f0f":"df = pd.read_csv(\"..\/input\/GrammarandProductReviews.csv\")","1d58d8eb":"df.head()","3d030751":"plt.hist(df['reviews.rating'])","2511814e":"df['target'] = df['reviews.rating']<4","312bd0f9":"plt.hist(df['target'])","08a5dad6":"train_text, test_text, train_y, test_y = train_test_split(df['reviews.text'],df['target'],test_size = 0.2)","e9066b8e":"train_text.shape","319eea29":"MAX_NB_WORDS = 20000\n\n# get the raw text data\ntexts_train = train_text.astype(str)\ntexts_test = test_text.astype(str)\n\n# finally, vectorize the text samples into a 2D integer tensor\ntokenizer = Tokenizer(nb_words=MAX_NB_WORDS, char_level=False)\ntokenizer.fit_on_texts(texts_train)\nsequences = tokenizer.texts_to_sequences(texts_train)\nsequences_test = tokenizer.texts_to_sequences(texts_test)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","aebdfa38":"sequences[0]","70484279":"type(tokenizer.word_index), len(tokenizer.word_index)","7a7fb32d":"index_to_word = dict((i, w) for w, i in tokenizer.word_index.items())","dc58364f":"\" \".join([index_to_word[i] for i in sequences[0]])","03203f8f":"seq_lens = [len(s) for s in sequences]\nprint(\"average length: %0.1f\" % np.mean(seq_lens))\nprint(\"max length: %d\" % max(seq_lens))","07f9e737":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.hist(seq_lens, bins=50);","c12f52c7":"plt.hist([l for l in seq_lens if l < 200], bins=50);","cbead37c":"MAX_SEQUENCE_LENGTH = 150\n\n# pad sequences with 0s\nx_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', x_train.shape)\nprint('Shape of data test tensor:', x_test.shape)","906fe0ba":"y_train = train_y\ny_test = test_y\n\ny_train = to_categorical(np.asarray(y_train))\nprint('Shape of label tensor:', y_train.shape)","4d71bb6c":"from keras.layers import Dense, Input, Flatten\nfrom keras.layers import GlobalAveragePooling1D, Embedding\nfrom keras.models import Model\n\nEMBEDDING_DIM = 50\nN_CLASSES = 2\n\n# input: a sequence of MAX_SEQUENCE_LENGTH integers\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n\nembedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=True)\nembedded_sequences = embedding_layer(sequence_input)\n\naverage = GlobalAveragePooling1D()(embedded_sequences)\npredictions = Dense(N_CLASSES, activation='softmax')(average)\n\nmodel = Model(sequence_input, predictions)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam', metrics=['acc'])","418d3e3a":"model.fit(x_train, y_train, validation_split=0.1,\n          nb_epoch=10, batch_size=128)","c4116cdf":"output_test = model.predict(x_test)\nprint(\"test auc:\", roc_auc_score(y_test,output_test[:,1]))","369b130d":"# input: a sequence of MAX_SEQUENCE_LENGTH integers\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\n\nx = LSTM(128, dropout=0.2, recurrent_dropout=0.2)(embedded_sequences)\npredictions = Dense(2, activation='softmax')(x)\n\n\nmodel = Model(sequence_input, predictions)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])","21a7e907":"model.fit(x_train, y_train, validation_split=0.1,\n          nb_epoch=2, batch_size=128)","7c905f97":"output_test = model.predict(x_test)\nprint(\"test auc:\", roc_auc_score(y_test,output_test[:,1]))","baf883a9":"# input: a sequence of MAX_SEQUENCE_LENGTH integers\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedded_sequences = embedding_layer(sequence_input)\n\n# 1D convolution with 64 output channels\nx = Conv1D(64, 5)(embedded_sequences)\n# MaxPool divides the length of the sequence by 5\nx = MaxPooling1D(5)(x)\nx = Dropout(0.2)(x)\nx = Conv1D(64, 5)(x)\nx = MaxPooling1D(5)(x)\n# LSTM layer with a hidden size of 64\nx = Dropout(0.2)(x)\nx = LSTM(64)(x)\npredictions = Dense(2, activation='softmax')(x)\n\nmodel = Model(sequence_input, predictions)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])","899d6f87":"model.fit(x_train, y_train, validation_split=0.1,\n          nb_epoch=5, batch_size=128)","4815223f":"output_test = model.predict(x_test)\nprint(\"test auc:\", roc_auc_score(y_test,output_test[:,1]))","da5c2c5c":"from keras import backend as K\nget_emb_layer_output = K.function([model.layers[0].input],\n                                  [model.layers[2].input])\nembedding_output = get_emb_layer_output([x_test[:3000]])[0]","d048db0b":"emb_shape = embedding_output.shape\nto_plot_embedding = embedding_output.reshape(emb_shape[0],emb_shape[1]*emb_shape[2])\ny = y_test[:3000]","bed8e516":"sentence_emb_tsne = TSNE(perplexity=30).fit_transform(to_plot_embedding)","d48932b4":"print(sentence_emb_tsne.shape)\nprint(y.shape)","2f3aa4c8":"plt.figure()\nplt.scatter(sentence_emb_tsne[np.where(y == 0), 0],\n                   sentence_emb_tsne[np.where(y == 0), 1],\n                   marker='x', color='g',\n                   linewidth='1', alpha=0.8, label='Happy')\nplt.scatter(sentence_emb_tsne[np.where(y == 1), 0],\n                   sentence_emb_tsne[np.where(y == 1), 1],\n                   marker='v', color='r',\n                   linewidth='1', alpha=0.8, label='Unhappy')\n\nplt.xlabel('Dim 1')\nplt.ylabel('Dim 2')\nplt.title('T-SNE')\nplt.legend(loc='best')\nplt.savefig('1.png')\nplt.show()  ","ea1ee5f8":"Vector space model is well known in information retrieval where each document is represented as a vector. The vector components represent weights or importance of each word in the document. The similarity between two documents is computed using the cosine similarity measure.\n\n![](https:\/\/iksinc.files.wordpress.com\/2015\/04\/screen-shot-2015-04-12-at-10-58-21-pm.png?w=768&h=740)\n\nimage & explanation taken from : https:\/\/iksinc.online\/tag\/continuous-bag-of-words-cbow\/","0ea29f05":"The model seems to overfit. Indeed the train error is really low whereas the test error is really higher. It seems that we have a variance porblem. Adding regularization like drop-out may help to stabilize the performance\n\nEdit : we have added drop-out. It is still not enough. We need to work on regularization technics to stabilize our performance.\n\nEdit2 : With less epochs, we manage to reduce the variance.","67cd4642":"to visualize our results we will use tsne","e7159bd0":"### Preprocessing text for the (supervised) CBOW model\n\nWe will implement a simple classification model in Keras. Raw text requires (sometimes a lot of) preprocessing.\n\nThe following cells uses Keras to preprocess text:\n- using a tokenizer. You may use different tokenizers (from scikit-learn, NLTK, custom Python function etc.). This converts the texts into sequences of indices representing the `20000` most frequent words\n- sequences have different lengths, so we pad them (add 0s at the end until the sequence is of length `1000`)\n- we convert the output classes as 1-hot encodings","a2e2caf9":"Due to the distribution of the ratings, we will consider that a customer is pleased by the product if the rating is higher than 3. Thus we will consider that a customer doesn't make a good review when the rating is equal or lower to 3.","b543f5a1":"## __Text Mining on Reviews__\n\n![](http:\/\/)![](http:\/\/thepayoffprinciple.com\/wp-content\/uploads\/2018\/06\/NLP-Practitioner.jpg)","2ad28691":"We are going to use our precedent model for our embedding. Then we will pass our 100 first reviews in the embedding and plot them with the label.","d93afad9":"### Visualize the outputs of our own Embeddings","fdeed1ea":"## A more complex model : CNN - LSTM","d0beba42":"## A simple supervised CBOW model in Keras","12dcd71d":"![](http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-chain.png)\n\nimage taken from : http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/","d57b7d9a":"[](http:\/\/)This notebook is __a lot inspired__ by : \n- https:\/\/github.com\/m2dsupsdlclass\/lectures-labs\n- [](http:\/\/)https:\/\/github.com\/m2dsupsdlclass\/lectures-labs\/blob\/master\/labs\/06_deep_nlp\/NLP_word_vectors_classification_rendered.ipynb\n\nIt is part of an amazing github created by Olivier Grisel and Charles Ollion for their courses at Master Data Science from Polytechnique\n\nThe goal of this notebook is to learn to use Neural Networks for text classification. The main goal is for you to understand how we can apply deep learning on raw text and what are the techniques behin it\n\nIn this notebook, we will:\n- Train a shallow model with learning embeddings\n\nHowever keep in mind:\n- Deep Learning can be better on text classification that simpler ML techniques, but only on very large datasets and well designed\/tuned models.\n- Many open source projects are really powerfull annd can be re-used: [word2vec](https:\/\/github.com\/dav\/word2vec) and [gensim's word2vec](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html)   (self-supervised learning only), [fastText](https:\/\/github.com\/facebookresearch\/fastText) (both supervised and self-supervised learning), [Vowpal Wabbit](https:\/\/github.com\/JohnLangford\/vowpal_wabbit\/wiki) (supervised learning).\n- Plain shallow sparse TF-IDF bigrams features without any embedding and Logistic Regression or Multinomial Naive Bayes is often competitive in small to medium datasets.","6df4adc4":"We will only consider the  text of the reviews and the ratings.\n\nWe are going to make an approximation in order to predict from the text the satisfaction level of the customer.","5f85fb1d":"We can definitly see a trand in our representation between negative and postive sentences.","9a96178f":"The tokenizer object stores a mapping (vocabulary) from word strings to token ids that can be inverted to reconstruct the original message (without formatting):","55b2f511":"## A complex model : LSTM","acd37c5a":"\nLet's zoom on the distribution of regular sized posts. The vast majority of the posts have less than 200 symbols:","621c48e7":"\nLet's have a closer look at the tokenized sequences:","9f34df38":"## The dataset","10229cfd":"We can see that we have a lot of \"happy\" customer due to our target distribution"}}