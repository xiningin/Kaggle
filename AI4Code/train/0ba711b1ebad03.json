{"cell_type":{"42881e5a":"code","0a2a8a3c":"code","ac3d3c54":"code","8ffd058d":"code","fdae9c17":"code","9ff7a823":"code","067e6dce":"code","a317a9b7":"code","b768f1f8":"code","4724b29f":"code","28dfb4d7":"code","726a34a2":"code","377adda1":"code","b73206d6":"code","44e024ac":"code","d7602717":"code","84d42f55":"code","3e8fbea2":"code","2362d0f3":"code","18662485":"code","f642682e":"markdown","17c574be":"markdown","a3eee4d6":"markdown","b552e719":"markdown","c77957cc":"markdown","de507cbf":"markdown","eb922ed8":"markdown","9d2a87a7":"markdown","6e525b9d":"markdown","746612cc":"markdown","571bf6e1":"markdown","a2e093c6":"markdown","00a05219":"markdown","c7e3827b":"markdown","4d93f07c":"markdown","a0e0e38d":"markdown","d2d99db9":"markdown","0aed5b9c":"markdown"},"source":{"42881e5a":"import numpy as np \nimport pandas as pd\nimport json","0a2a8a3c":"import tqdm \n# for some fancy loading bars","ac3d3c54":"dir = \"..\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\"","8ffd058d":"with open(dir) as f:\n    for i in f:\n        print(i)\n        break","fdae9c17":"with open(dir) as f:\n    \n    for i in f:\n        json_loaded = json.loads(i)\n        \n        for key in json_loaded:\n            print(key,\" | \",  json_loaded[key])\n            \n        break\n        ","9ff7a823":"headlines = []\nlabels = []\n\nwith open(dir) as f:\n    for i in f:\n        json_loaded = json.loads(i)\n        headlines.append(json_loaded['headline'])\n        labels.append(json_loaded['is_sarcastic'])\n\nprint(f\"Number of sentences\/headlines: {len(headlines)}\")\nprint(\"A random entry and its label:\\n\")\nprint(headlines[42])\nprint(labels[42])\n# is_sarcastic = 1 for a sarcastic comment and 0 for a non sarcastic one\n","067e6dce":"import re\nimport nltk","a317a9b7":"# Example\ns = \"My github page is https:\/\/github.com\/The-Bread please follow me \"\ntext = re.sub(r'(https?:\\\/\\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', '', s, flags=re.MULTILINE)\ntext","b768f1f8":"# example\ns = \"Buy buys . Likes. seems . Doesn't, does, easy,\"\n\ns = s.lower()\ntoken = nltk.word_tokenize(s)\nsentence = [nltk.stem.SnowballStemmer('english').stem(word) for word in token]\n\nprint(\" \".join(sentence))","4724b29f":"def preprocess(sentence):\n    sentence = re.sub(r'(https?:\\\/\\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', '', sentence, flags=re.MULTILINE)\n    sentence = sentence.lower()\n    token = nltk.word_tokenize(sentence)\n    sentence = [nltk.stem.SnowballStemmer('english').stem(word) for word in token]\n    return \" \".join(sentence)","28dfb4d7":"example = headlines[52]\nprint(\"Original:\")\nprint(example)\nprint(\"\\nPreprocessed: \")\nprint(preprocess(example))","726a34a2":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","377adda1":"S = [\"Hello\", \"How are you today\", \"Where are you\"]\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(S)\nprint(f\"Actual Sentence: {S[2]}\\n\\n\")\n\nS = tokenizer.texts_to_sequences(S)\nprint(f\"Tokenized Sequence: {S[2]}\\n\\n\")\n\nprint(tokenizer.word_index)\n","b73206d6":"padded = pad_sequences(S, maxlen=3, padding='post', truncating='post')\nprint(f\"Padded sequences:\\n{padded}\")","44e024ac":"def tokenize_data(X):\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(X)\n    X = tokenizer.texts_to_sequences(X)\n    X = pad_sequences(X, maxlen=15, padding='post', truncating='post')\n    X = np.array(X) # convert to numpy array \n    return X, tokenizer","d7602717":"stemmed = [preprocess(s) for s in tqdm.tqdm(headlines, desc='Stemming the Headlines')]\nprint(\"Stemming complete\")\n\nprint(\"\\nTokenizing the headlines\")\nX, tokenizer = tokenize_data(stemmed)\nprint(\"Headlines Tokenized\")\nprint(f\"\\nInput Shape: {X.shape}\")\n\nY = np.array(labels)\nprint(f\"Ouput Shape: {Y.shape}\")\n\n# Reshape Y to make it 2D like the inputs\nY = np.reshape(Y, (-1, 1))\nprint(f\"Final Output Shape: {Y.shape}\")","84d42f55":"print(\"Number of words tokenized: \", len(tokenizer.word_index))\nnum_words = len(tokenizer.word_index)","3e8fbea2":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, LSTM, Bidirectional, GlobalAveragePooling1D, Conv1D, Embedding","2362d0f3":"model = Sequential([\n    Embedding(num_words+1, output_dim=10, input_length=15),\n    LSTM(15, dropout=0.1, return_sequences=True),\n    Conv1D(15, 10, activation='relu'),\n    GlobalAveragePooling1D(),\n    \n    Dense(32,activation='relu'),\n    Dense(1, activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics = ['acc'])\nmodel.summary()","18662485":"model.fit(X, Y, epochs=3)","f642682e":"Our goal is to make a model which will input a sentence, and output whether its sarcastic or not.\n##### For working on text based models , a special type of networks are used called RNN(Recurrent Neural Networks). \nMore info about RNNs can be found [here](https:\/\/towardsdatascience.com\/understanding-rnn-and-lstm-f7cdf6dfc14e). Alternatively, you can enroll in [this course](https:\/\/www.coursera.org\/learn\/natural-language-processing-tensorflow\/) by DeepLearning.ai on coursera (Recommended).\n##### We will be using LSTM version of RNNs with Conv1D layers to make a simple classifier","17c574be":"#### Now to apply these methods to the headlines we got from the dataset","a3eee4d6":"Next step is to convert the sentences into a sequence of numbers corresponding to each word.\nThe numbers are given according to the frequency of that particular word ","b552e719":"# Clean the acquired data\nNow its time to clean the sentences. \nThis usually consists of 4 parts:\n1. Remove unnecessary details like links and punctuations \n2. Convert each word to its base form\n3. Convert the words to numerical tokens \n4. Pad these tokenized sequences to a fixed length ","c77957cc":"If you want to build a highly accurate model, you would use the pretrained vectors for these words. Some examples are GloVe which offers vectors for a lot of commonly used words. They have also provided us with vectors with different Embedding Dimensions (50d, 100d, 200d, etc) ","de507cbf":"# Reading the json file","eb922ed8":"### 2. Replacing words with their base forms\nSnowballStemmer and tokenize from nltk helps to reduce similar words from different tenses to the same form","9d2a87a7":"Json files are stored in the form of collection of dictionary objects as strings:","6e525b9d":"### 1. Removing the links from the text\nBy using some Regex we can filter out the website link from the text with just one line ","746612cc":"### The Embedding layer:\nThe Embedding layer is responsible for making sense out of the numbers assigned to the words. This layer maps each token as an N-dimensional vector. Similar vectors point near the same location on some dimension and might point away from each other on higher dimensions. ","571bf6e1":"Now we can store the relevant information like headline and is_sarcastic as seperate entities","a2e093c6":"# Making the model","00a05219":"By using the json library, we can convert this string into a python dictionary","c7e3827b":"### Pad these sequences. \n###### We can see that sentences can have varying length which might be problematic when passing through a neural network which requires a fixed input size.\n###### Hence we fill the shorter sentences with 0 (since that is not reserved for any other word in the tokenizer index) and we truncate the longer sentences. We can either do these from the left side(pre) or the right side(post)","4d93f07c":"# What next?\n### We overlooked the dataset exploration. Check for dataset imbalance and try to fix it using oversampling or undersampling \n### Divide the data into training and test data or perform cross validation\n\n### Test out different models and compare their validation accuracy and loss ","a0e0e38d":"### Tokenize the sentence","d2d99db9":"Combine both the steps to a single function","0aed5b9c":"For example : words like cat, dog and pug might be located closely but on higher dimensions, cat might be farther from dog than the word pug (since it is a type of dog)."}}