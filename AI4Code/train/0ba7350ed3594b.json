{"cell_type":{"9e86b91e":"code","c3feaded":"code","af29882d":"code","1534d0d4":"code","71e8f710":"code","e6ab4415":"code","3cc76045":"code","c9fd3c31":"code","870213cc":"code","8fa6e047":"code","3eea9e81":"code","2795cd2d":"code","9476a1b4":"code","97bda0e4":"code","cebd9964":"code","670dc55d":"code","9a6d38e2":"code","cac6b4ff":"code","c5560ad4":"code","08838d5e":"code","9cd3fa7e":"code","6dfe1227":"code","ba0da76d":"code","147056d4":"code","1edc62a7":"code","87d4fc7a":"code","b520bbd1":"code","5d5c68f9":"code","5880c667":"code","51662d43":"code","6c79fc1c":"code","16b11663":"code","868c2eb3":"code","8be68dc1":"code","aee19ea1":"code","7c4fc6ff":"markdown","47847e67":"markdown","2a431e8a":"markdown","01491d7e":"markdown","f13d2e8b":"markdown","d82c8a81":"markdown","6b35d4a0":"markdown","aa714bf9":"markdown","de4afc8f":"markdown","f07a7f86":"markdown","124e08cf":"markdown","3c1d769d":"markdown"},"source":{"9e86b91e":"import os\nprint(os.listdir('..\/input\/taiwanfood\/Train\/Train\/'))","c3feaded":"dataPath = '..\/input\/taiwanfood\/Train\/Train\/'\ntestPath = '..\/input\/taiwnafodd\/Test\/Test\/'","af29882d":"import tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.applications.mobilenet import preprocess_input, decode_predictions\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\nfrom sklearn.metrics import classification_report, confusion_matrix","1534d0d4":"# load EfficientNetB7 model with imagenet parameteres\n#base_model = efn.EfficientNetB7(input_shape=input_shape, weights='imagenet', include_top=False)","71e8f710":"import numpy as np\nimport cv2\nimport glob\nimport random\n\nfrom IPython.display import Image\nimport matplotlib.pyplot as plt","e6ab4415":"TargetSize = (224, 224)\ndef prepare_image(filepath):\n    img = cv2.imread(filepath)\n    img_resized = cv2.resize(img, TargetSize, interpolation=cv2.INTER_CUBIC)\n    img_result  = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n    return img_result","3cc76045":"#dirList = glob.glob(dataPath+'*') # list of all directories in dataPath\n#dirList.sort() # sorted in alphabetical order\n#print(dirList)\ndirList = os.listdir(dataPath)\ndirList.sort()\nprint(dirList)","c9fd3c31":"print(len(dirList))","870213cc":"#Y_data = []\n#for i in range(len(dirList)):\n#    fileList = glob.glob(dirList[i]+'\/*.jpg')\n#    [Y_data.append(i) for file in fileList]\n#print(Y_data)\nY_data = []\nfor i in range(len(dirList)):\n    fileList = glob.glob(dataPath+dirList[i]+'\/*.jpg')\n    [Y_data.append(i) for file in fileList]\nprint(Y_data)","8fa6e047":"X_data = []\nfor i in range(len(dirList)):\n    fileList = glob.glob(dataPath+dirList[i]+'\/*.jpg')\n    [X_data.append(prepare_image(file)) for file in fileList]\nX_data = np.asarray(X_data)\nprint(X_data.shape)","3eea9e81":"## random shuffle\nfrom sklearn.utils import shuffle\nX_data, Y_data = shuffle(X_data, Y_data, random_state=0)","2795cd2d":"print(Y_data)","9476a1b4":"# randomly select a picture to show\ntestNum = random.randint(0,len(X_data)-1)\nprint(testNum)\nplt.imshow(X_data[testNum])","97bda0e4":"# define number of classes & labels\nnum_classes = 34 \n#labels = [dir.replace(dataPath, \"\") for dir in dirList]\nlabels = dirList\nprint(labels)","cebd9964":"# counting number of pictures of each class\nequilibre = []\n[equilibre.append(Y_data.count(i)) for i in range(len(dirList))]\nprint(equilibre)","670dc55d":"# plot the circle of value counts in dataset\nplt.figure(figsize=(5,5))\nmy_circle=plt.Circle( (0,0), 0.5, color='white')\nplt.pie(equilibre, labels=labels, colors=['red','green'],autopct='%1.1f%%')\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.show()","9a6d38e2":"# Data Normalisation\nX_train = X_data \/ 255.0\nprint(X_train.shape)","cac6b4ff":"# One-hot encoding\nY_train = to_categorical(Y_data)\nprint(Y_train.shape)","c5560ad4":"# use MobieNet V2 as base model\nbase_model=MobileNetV2(input_shape=(224,224,3),weights='imagenet',include_top=False) \n\n# add Fully-Connected Layers to Model\nx=base_model.output\nx=GlobalAveragePooling2D()(x)\nx=Dense(1024,activation='relu')(x) # FC layer 1\nx=Dense(64,activation='relu')(x)   # FC layer 2\npreds=Dense(num_classes,activation='softmax')(x) #final layer with softmax activation\n\nmodel=Model(inputs=base_model.input,outputs=preds)","08838d5e":"# Transfer Learning setting\nbase_model.trainable = False","9cd3fa7e":"# Compile Model\nmodel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","6dfe1227":"# Train Moel (target is loss <0.01)\nbatch_size = 10\nnum_epochs = 100\nhistory = model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs)","ba0da76d":"batch_size = 5\nnum_epochs = 500\nry = model.fit(X_train, Y_train, batch_size=batch_size, epochs=num_epochs)","147056d4":"# Save Model\nmodels.save_model(model,'taiwanfood.h5')","1edc62a7":"!tflite_convert --keras_model_file=taiwanfood.h5 --output_file=taiwanfood.tflite","87d4fc7a":"!tflite_convert \\\n--keras_model_file=taiwanfood.h5 \\\n--output_file=taiwanfood_quant.tflite \\\n--inference_type=QUANTIZED_UINT8 \\\n--default_ranges_min=0 \\\n--default_ranges_max=255 \\\n--mean_values=128 \\\n--std_dev_values=128 \\\n--allow_custom_ops\n","b520bbd1":"!ls *.tflite","5d5c68f9":"# select one bird picture to test\ntestNum = random.randint(0,19)\ni=random.randint(0,33)\nimageFile = dataPath+dirList[i]+'\/'+str(testNum)+'.jpg'\nplt.imshow(prepare_image(imageFile))","5880c667":"# test model\ntestData = prepare_image(imageFile).reshape(1,224,224,3)\ntestData = testData \/ 255.0\npredictions = model.predict(testData)\nmaxindex = int(np.argmax(predictions))\nprint(predictions[0][maxindex],labels[maxindex])","51662d43":"# select one bird picture to test\ntestNum = random.randint(0,19)\ni=random.randint(0,33)\nimageFile = dataPath+dirList[i]+'\/'+str(testNum)+'.jpg'\nplt.imshow(prepare_image(imageFile))","6c79fc1c":"# test model\ntestData = prepare_image(imageFile).reshape(1,224,224,3)\ntestData = testData \/ 255.0\npredictions = model.predict(testData)\nmaxindex = int(np.argmax(predictions))\nprint(predictions[0][maxindex],labels[maxindex])","16b11663":"Y_pred = model.predict(X_train)\ny_pred = np.argmax(Y_pred,axis=1)\n#y_label= [labels[k] for k in y_pred]\ncm = confusion_matrix(Y_data, y_pred)\nprint(cm)","868c2eb3":"import itertools\ndef plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n        \n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","8be68dc1":"plot_confusion_matrix(cm, \n                      normalize=False,\n                      target_names = labels,\n                      title=\"Confusion Matrix, not Normalized\")","aee19ea1":"print(classification_report(Y_data, y_pred, target_names=labels))","7c4fc6ff":"# Image Classification\n## MobileNetV2 transfer learning","47847e67":"* Accuracy = TP+TN \/ TP+FP+FN+TN\n* Precision = TP \/ TP+FP\n* Recall = TP \/ TP+FN","2a431e8a":"## Build Model\n### Load MobileNetV2 model & add FC-layers","01491d7e":"### Data Normalization","f13d2e8b":"### Shuffle Data","d82c8a81":"## Train Model","6b35d4a0":"### check the entire training dataset","aa714bf9":"## Save Model","de4afc8f":"## Prepare Data","f07a7f86":"## Plot Confusion Matrix","124e08cf":"## Weighted Average Recall\n\n![WAR.png](attachment:WAR.png)\n* TP : True Positive\n* FP : False Positive\n* TN : True Negative\n* FN : False Negative\n\n\n","3c1d769d":"## Test Model"}}