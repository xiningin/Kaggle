{"cell_type":{"c0cce8b0":"code","b7409cb9":"code","8c0c0cee":"code","d37b2b7b":"code","bf0d1845":"code","1b968311":"code","e29875d8":"code","b562dca9":"markdown","e76e5c8c":"markdown","c23ca1c0":"markdown","4529c52c":"markdown","31226282":"markdown","7518db40":"markdown","643f4f59":"markdown","75b445ce":"markdown"},"source":{"c0cce8b0":"#importing necessary libararies\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns \npd.set_option('display.max_columns',500)\npd.set_option('display.max_rows',500)\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor","b7409cb9":"import optuna\ndf = pd.read_csv(\"..\/input\/30days-folds\/train_folds.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")","8c0c0cee":"imp_col = [c for c in df.columns if c not in (\"id\", \"target\", \"kfold\")]\ncategorical_col = [col for col in imp_col if 'cat' in col]\nnumerical_col = [col for col in imp_col if col.startswith(\"cont\")]\ndf_test = df_test[imp_col]\nfor folds in range(5):\n    X_train=df[df.kfold!=folds].reset_index()\n    X_valid=df[df.kfold==folds].reset_index()\n    y_train=X_train.target\n    y_valid=X_valid.target\n    X_train=X_train[imp_col]\n    X_valid=X_valid[imp_col]\n    X_test=df_test.copy()\n\n    ohe=OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n    X_train_ohe=ohe.fit_transform(X_train[categorical_col])\n    X_valid_ohe=ohe.transform(X_valid[categorical_col])\n    X_test_ohe=ohe.transform(X_test[categorical_col])\n\n    X_train_ohe=pd.DataFrame(X_train_ohe,columns=[f\"ohe_{i}\"for i in range(X_train_ohe.shape[1])]) \n    X_valid_ohe=pd.DataFrame(X_valid_ohe,columns=[f\"ohe_{i}\"for i in range(X_valid_ohe.shape[1])])\n    X_test_ohe=pd.DataFrame(X_test_ohe,columns=[f\"ohe_{i}\"for i in range(X_test_ohe.shape[1])]) \n    X_train=pd.concat([X_train,X_train_ohe],axis=1)\n    X_valid=pd.concat([X_valid,X_valid_ohe], axis=1)\n    X_test = pd.concat([X_test,X_test_ohe], axis=1)\n\n    X_train = X_train.drop(categorical_col, axis=1)\n    X_valid = X_valid.drop(categorical_col, axis=1)\n    X_test = X_test.drop(categorical_col, axis=1)\n    \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_valid = scaler.transform(X_valid)\n    X_test = scaler.transform(X_test)","d37b2b7b":"def run(trial):\n    fold=0\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n   \n    xgb_model = XGBRegressor(random_state=42,\n                tree_method=\"gpu_hist\",\n                gpu_id=1,\n                predictor=\"gpu_predictor\",\n                n_estimators=7000,\n                learning_rate=learning_rate,\n                reg_lambda=reg_lambda,\n                reg_alpha=reg_alpha,\n                subsample=subsample,\n                colsample_bytree=colsample_bytree,\n                max_depth=max_depth,)\n    xgb_model.fit(X_train, y_train, early_stopping_rounds=100, eval_set=[(X_valid, y_valid)], verbose=1000)\n    preds_valid = xgb_model.predict(X_valid) \n    rmse = mean_squared_error(y_valid, preds_valid, squared=False)\n    return rmse  ","bf0d1845":"study = optuna.create_study(direction='minimize')\nstudy.optimize(run, n_trials=100)","1b968311":"study.best_params","e29875d8":"print(\"Number of finished trials: {}\".format(len(study.trials)))\n\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"Value: {}\".format(trial.value))\n\nprint(\"Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","b562dca9":"****Step-3 Feature Engineering.****\n\nI have tried various techniques like OrdinalEncoder, One Hot Encoding ,Normalisation etc. \n\nThe best came out to be the OneHotEncoder on categorical variables and after that StandardScaler upon the whole dataset after one hot encoding.","e76e5c8c":"****Step-2 Importing the datasets.****\n\n****I have used datasets from 30 days of code and from 30days-folds by Abhishek Thakur.****","c23ca1c0":"# *Feature Engineering + Hyperparameter tuning*","4529c52c":"***To check the Best parameters.***","31226282":"**In the next notebook, I will share the optimized model.**\n\n**If you found my work helpful, please upvote!**\nThank You!","7518db40":"****Step-1 Importing Libraries.****","643f4f59":"****Step-4 Hyperparameter Tuning.****\n\nUsing Optuna:\n\nSteps-\n\n* Define objective function to be optimized.Here run.\n* Suggest hyperparameter values using trial object. \n* Create a study object and invoke the optimize method over 100 trials.","75b445ce":"****In this notebook, I have shared the techniques like feature engineering, hyperparameter tuning using Optuna.****"}}