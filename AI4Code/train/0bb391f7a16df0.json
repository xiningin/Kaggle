{"cell_type":{"89504495":"code","94296d8d":"code","61560d48":"code","85ec0275":"code","93460c8b":"code","0f857c12":"code","ae9a7d01":"code","a1d82c83":"code","a40cb186":"code","4b7b76dd":"code","4905fe15":"code","3a6ba23c":"code","c533628c":"code","c65514b0":"code","00cdd1c0":"code","1d852aac":"code","1248bece":"code","5de4983c":"code","209fafb4":"code","b0457064":"code","15b0ba2e":"code","c1d4da7d":"code","a4734051":"code","b9bc0c34":"code","948ab129":"code","ba5bc8d1":"code","b50c6b8b":"code","84230c21":"code","47769b56":"code","ad722f68":"code","4478eb54":"code","9a62d1fa":"code","86c84475":"code","aa85a545":"code","56215a00":"code","24b61fda":"code","dd473643":"code","cb5fb6bb":"code","1685e9a5":"code","ea088f17":"code","de2e6221":"code","013e16ae":"code","1671e9d2":"code","06734678":"code","3776f88e":"code","1ed3a9b8":"code","ff63d431":"code","a407682f":"code","30f045fe":"code","486e7fdb":"code","334fc5dd":"code","fd5fbb37":"code","96f8c392":"code","8313e4a4":"code","aa043d0b":"code","ec7f48d5":"markdown","a5dcfc8e":"markdown","ea585a6b":"markdown","ad8f8233":"markdown","b6f0f834":"markdown","a28cd4a9":"markdown","8c83e1d7":"markdown","ba690455":"markdown","129e9160":"markdown","481d640e":"markdown","0a5d7775":"markdown","4497e06c":"markdown","7eb743ce":"markdown","d2797538":"markdown","763ac35f":"markdown","e3b6c7cb":"markdown","19b292ce":"markdown","a00d5cca":"markdown","348f8154":"markdown","617cfe40":"markdown","0e3d07d6":"markdown","c0a41df6":"markdown","5cd8278b":"markdown","adbef742":"markdown","73fa20e4":"markdown","6c7c7bbc":"markdown","2e08da7e":"markdown","a08709ba":"markdown","c03ac98e":"markdown","44f8c928":"markdown","245f52fe":"markdown","e69ee514":"markdown","af77b461":"markdown","cb7bbc00":"markdown","317fa890":"markdown","b84ff643":"markdown","073e5c69":"markdown","735fbed0":"markdown","c4e5f015":"markdown","2a7d8265":"markdown","77b7ec3d":"markdown","82d9d7a2":"markdown","2865e24d":"markdown","3c8656c1":"markdown","26d6c997":"markdown","8733e65a":"markdown","ea2b6e33":"markdown","39d09e65":"markdown"},"source":{"89504495":"import pandas as pd\ndf_hscore = pd.read_csv('\/kaggle\/input\/happiness-report-dataset\/happiness_score_dataset.csv')\ndf_hscore.head() #Printing first 5 rows","94296d8d":"#Checking dimesion of dataset\n\ndf_hscore.shape","61560d48":"#Copying dataframe to new df\ndf = df_hscore.copy()\n\n#Checking datatypes\ndf.info()","85ec0275":"#Displaying null values using heatmap\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.heatmap(df.isnull())","93460c8b":"#Checking unique values in categorical features\n\nfeatures = df.columns\nfor x in features:\n    if df[x].dtypes == object:\n        print(f\"%-30s: %5d\"%(x,len(df[x].unique())))","0f857c12":"#Encoding feature Region'\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nencoded_Region = le.fit_transform(df['Region'])\nfor i,x in zip(range(len(le.classes_)),le.classes_):\n    print(\"%4d: %s\"%(i,x))","ae9a7d01":"#Replacing value of feature Region with encoded_Region\n\ndf['Region'] = encoded_Region","a1d82c83":"#Dropping feature Country from dataset as it can not be encoded.\n\ndf_new=df.drop(columns=['Country'])\ndf_new.columns","a40cb186":"#Statistical Summary of Dataset\n\ndf.describe()","4b7b76dd":"#Checking categorical feature Region with countplot\nx_labels = [\"%s: %d\"%(le.classes_[i],i) for i in range(len(le.classes_))]\nsns.countplot(df_new['Region'])\nplt.xlabel('Region')\nplt.ylabel('Country (count)')\nplt.xticks(range(len(le.classes_)),x_labels,rotation=45,horizontalalignment=\"right\",fontsize=12)\nplt.show()\n","4905fe15":"#Checking Data Distribution with the help of distplot.\nrows = 3\ncols = 4\nfig, axes = plt.subplots(rows,cols,figsize=(rows*cols*1.5,rows*cols))\nplt.subplots_adjust(hspace=0.5)\nk=0\nfor i in range(rows):\n    for j in range(cols):\n        sns.distplot(df_new[df_new.columns[k]],ax=axes[i][j])\n        axes[i][j].set_title(f\"Distribution Plot: {df_new.columns[k]}\")\n        k += 1\n        if k == len(df_new.columns):\n            break\n\nplt.show()","3a6ba23c":"#Checking Data with boxplot\nrows = 3\ncols = 4\nfig, axes = plt.subplots(rows,cols,figsize=(rows*cols*1.5,rows*cols))\nplt.subplots_adjust(hspace=0.5)\nk=0\nfor i in range(rows):\n    for j in range(cols):\n        sns.boxplot(df_new[df_new.columns[k]],ax=axes[i][j])\n        axes[i][j].set_title(f\"Box Plot: {df_new.columns[k]}\")\n        k += 1\n        if k == len(df_new.columns):\n            break\n\nplt.show()","c533628c":"#Checking Relationship between Region and Happiness Score\ninput_x=\"Region\" \ntarget=\"Happiness Score\"\nsns.scatterplot(x=input_x, y=target, data=df_new)\nplt.xticks(range(len(le.classes_)),x_labels,rotation=45,horizontalalignment='right')\nplt.show()","c65514b0":"#Checking Relationship between Happiness Rank and Happiness Score\ninput_x=\"Happiness Rank\" \ntarget=\"Happiness Score\"\nsns.scatterplot(x=input_x, y=target, data=df_new)\nplt.show()\n","00cdd1c0":"#Checking Relationship between Standard Error and Happiness Score\ninput_x=\"Standard Error\" \ntarget=\"Happiness Score\"\nsns.scatterplot(x=input_x, y=target, data=df_new)\nplt.show()","1d852aac":"#Checking Relationship between Economy (GDP per Capita) and Happiness Score\ninput_x=\"Economy (GDP per Capita)\" \ntarget=\"Happiness Score\"\nsns.scatterplot(x=input_x, y=target, data=df_new)\nplt.show()","1248bece":"#Checking Relationship between Family and Happiness Score\ninput_x=\"Family\" \ntarget=\"Happiness Score\"\nsns.scatterplot(x=input_x, y=target, data=df_new)\nplt.show()","5de4983c":"#Checking Relationship between Health (Life Expectancy) and Happiness Score\ninput_x=\"Health (Life Expectancy)\" \ntarget=\"Happiness Score\"\nsns.scatterplot(x=input_x, y=target, data=df_new)\nplt.show()","209fafb4":"#Checking Relationship between Freedom and Happiness Score\ninput_x=\"Freedom\" \ntarget=\"Happiness Score\"\nsns.scatterplot(x=input_x, y=target, data=df_new)\nplt.show()","b0457064":"#Checking Relationship between Trust (Governmnet Corruption) and Happiness Score\ninput_x=\"Trust (Government Corruption)\" \ntarget=\"Happiness Score\"\nsns.scatterplot(x=input_x, y=target, data=df_new)\nplt.show()","15b0ba2e":"#Checking Relationship between Generosity and Happiness Score\ninput_x=\"Generosity\" \ntarget=\"Happiness Score\"\nsns.scatterplot(x=input_x, y=target, data=df_new)\nplt.show()\n","c1d4da7d":"#Checking Relationship between Dystopia Residual and Happiness Score\ninput_x=\"Dystopia Residual\" \ntarget=\"Happiness Score\"\nsns.scatterplot(x=input_x, y=target, data=df_new)\nplt.show()","a4734051":"#Checking Correlation of Features\ndf_corr = df_new.corr()\n\n#Showing Correlation with the help of heatmap\nplt.figure(figsize=(8,6))\nsns.heatmap(df_corr, annot=True, fmt='.2f')\nplt.show()","b9bc0c34":"#Showing Correlation of Features with Happiness Score using bar plot\ndf_corr['Happiness Score'].sort_values(ascending=False).drop(['Happiness Score']).plot.bar()\nplt.plot([-1,10],[0,0],color='r')\nplt.ylabel('Correlation Value')\nplt.title('Correlation: Features vs Happiness Score')\nplt.show()","948ab129":"#Checking skewness\n\ndf_new.skew()","ba5bc8d1":"#Checking outlier with zscore\nfrom scipy.stats import zscore\nimport numpy as np\n\nz = np.abs(zscore(df_new))\nnp.where(z>3) #Printing location of outliers","b50c6b8b":"#Removing outliers\ndf_wo = df_new[(z<=3).all(axis=1)]\nprint(f\"Original Shape: {df_new.shape}\")\nprint(f\"New Shape: {df_wo.shape}\")\nprint(f\"% Loss: {(len(df_new)-len(df_wo))*100\/len(df_new)}%\")","84230c21":"#Seperating Input and Output variables.\nX = df_wo.drop(columns=['Happiness Score'])\nY = df_wo['Happiness Score']\nprint(X.shape)\nprint(Y.shape)","47769b56":"#Treating skewness\nfrom sklearn.preprocessing import power_transform\nX_t = power_transform(X)","ad722f68":"#Scaling Data for model using StandardScaler\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nscaled_x = sc.fit_transform(X_t)","4478eb54":"from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n\n#Defining function for best random state\ndef get_best_rstate(r,model,x,y,test_size=0.25):\n    best_rState = 0\n    best_r2Score = 0\n    for i in range(r):\n        x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=test_size,random_state=i)\n        model.fit(x_train,y_train)\n        predict_y = model.predict(x_test)\n        temp_r2Score = r2_score(y_test,predict_y)\n        if temp_r2Score>best_r2Score:\n            best_r2Score = temp_r2Score\n            best_rState = i\n            \n    return best_rState, best_r2Score\n\n#Defining function for best cv\ndef get_best_cv(model,parameters,x_train,y_train):\n    best_cv = 0\n    best_cvScore = 0\n    for i in range(2,20):\n        gscv = GridSearchCV(model,parameters)\n        gscv.fit(x_train,y_train)\n        temp_cvScore = cross_val_score(gscv.best_estimator_,x_train,y_train,cv=i).mean()\n        if temp_cvScore>best_cvScore:\n            best_cvScore = temp_cvScore\n            best_cv = i\n            \n    return best_cv, best_cvScore","9a62d1fa":"from sklearn.linear_model import LinearRegression\n#Finding best rand_state for train test split\nbest_rState, best_r2Score = get_best_rstate(400,LinearRegression(),scaled_x,Y)\nprint(f\"Best random_state: {best_rState} with best r2_score: {best_r2Score}\")","86c84475":"#Splitting train test with random_state = 373\nlr_x_train,lr_x_test,lr_y_train,lr_y_test = train_test_split(scaled_x,Y,test_size=0.25,random_state=373)","aa85a545":"#Hypertuning parameters\nparameters = {\n    \"fit_intercept\": [True,False],\n    \"normalize\": [True,False],\n}\n\n#Finding best CV\nbest_cv, lr_best_cvScore = get_best_cv(LinearRegression(),parameters,lr_x_train,lr_y_train)\nprint(f\"Best CV: {best_cv} with best cross_val_score: {lr_best_cvScore}\")","56215a00":"#Building Final Model with hpypertuned parameters and cv = 10\nlr_gscv = GridSearchCV(LinearRegression(),parameters,cv=10)\nlr_gscv.fit(lr_x_train,lr_y_train)\nprint(lr_gscv.best_params_)","24b61fda":"#Building Final Model with hpypertuned parameters and cv = 10\nlr_gscv = GridSearchCV(LinearRegression(),parameters,cv=10)\nlr_gscv.fit(lr_x_train,lr_y_train)\nprint(lr_gscv.best_params_)","dd473643":"#Checking Final Performance of Model\nlr_predict_y = lr_gscv.best_estimator_.predict(lr_x_test)\n\nlr_r2Score = r2_score(lr_y_test,lr_predict_y)\nlr_mse = mean_squared_error(lr_y_test,lr_predict_y)\nlr_mae = mean_absolute_error(lr_y_test,lr_predict_y)\n\nprint(f\"R2_SCORE: {round(lr_r2Score*100,2)}%\\t MSE: {lr_mse}\\t RMSE: {np.sqrt(lr_mse)}\\t MAE: {lr_mae}\")","cb5fb6bb":"from sklearn.linear_model import Lasso\n#Finding the best random state for train test split\nbest_rState, best_r2Score = get_best_rstate(400,Lasso(),scaled_x,Y)\n\nprint(f\"Best random_state: {best_rState} with best r2_score: {best_r2Score}\")","1685e9a5":"#Splitting train test data with random_state = 321\nla_x_train,la_x_test,la_y_train,la_y_test = train_test_split(scaled_x,Y,test_size=0.25,random_state=321)","ea088f17":"import warnings\nwarnings.simplefilter('ignore')\n#Hypertuning parameters\nparameters = {\n    \"alpha\": [0.0001,0.001,0.01,0.1,1.0],\n    \"fit_intercept\": [True,False],\n    \"normalize\": [True,False],\n    \"tol\": [1e-6,1e-5,1e-4],\n    \"selection\": ['cyclic','random'],\n}\n#Finding best CV\nbest_cv,la_best_cvScore = get_best_cv(Lasso(),parameters,la_x_train,la_y_train)\nprint(f\"Best CV: {best_cv} with best cross_val_score: {la_best_cvScore}\")","de2e6221":"#Building final model with hypertuned parameters with cv=12\nla_gscv = GridSearchCV(Lasso(),parameters,cv=12)\nla_gscv.fit(la_x_train,la_y_train)\n\nprint(la_gscv.best_params_)","013e16ae":"#Checking Final Performanace of the model\nla_predict_y = la_gscv.predict(la_x_test)\n\nla_r2Score = r2_score(la_y_test,la_predict_y)\nla_mse = mean_squared_error(la_y_test,la_predict_y)\nla_mae = mean_absolute_error(la_y_test,la_predict_y)\n\nprint(f\"R2_SCORE: {round(la_r2Score*100,2)}%\\t MSE: {la_mse}\\t RMSE: {np.sqrt(la_mse)}\\t MAE: {la_mae}\")","1671e9d2":"from sklearn.linear_model import Ridge\n#Finding the best random state for train test split\nbest_rState, best_r2Score = get_best_rstate(400,Ridge(),scaled_x,Y)\n\nprint(f\"Best random_state: {best_rState} with best r2_score: {best_r2Score}\")","06734678":"#Splitting train test data with random_state = 373\nri_x_train,ri_x_test,ri_y_train,ri_y_test = train_test_split(scaled_x,Y,test_size=0.25,random_state=373)","3776f88e":"#Hypertuning parameters\nparameters = {\n    \"alpha\": [0.0001,0.001,0.01,0.1,1.0],\n    \"fit_intercept\": [True,False],\n    \"normalize\": [True,False],\n    \"tol\": [1e-6,1e-5,1e-4],\n    \"solver\": ['auto','svd','cholesky','lsqr','sparse_cg','sag','saga'],\n}\n#Finding best CV\nbest_cv,ri_best_cvScore = get_best_cv(Ridge(),parameters,ri_x_train,ri_y_train)\nprint(f\"Best CV: {best_cv} with best cross_val_score: {ri_best_cvScore}\")","1ed3a9b8":"#Building final model with hypertuned parameters with cv=10\nri_gscv = GridSearchCV(Ridge(),parameters,cv=10)\nri_gscv.fit(ri_x_train,ri_y_train)\n\nprint(ri_gscv.best_params_)","ff63d431":"#Checking Final Performanace of the model\nri_predict_y = ri_gscv.predict(ri_x_test)\n\nri_r2Score = r2_score(ri_y_test,ri_predict_y)\nri_mse = mean_squared_error(ri_y_test,ri_predict_y)\nri_mae = mean_absolute_error(ri_y_test,ri_predict_y)\n\nprint(f\"R2_SCORE: {round(ri_r2Score*100,2)}%\\t MSE: {ri_mse}\\t RMSE: {np.sqrt(ri_mse)}\\t MAE: {ri_mae}\")","a407682f":"from sklearn.linear_model import SGDRegressor\n#Finding the best random state for train test split\nbest_rState, best_r2Score = get_best_rstate(400,SGDRegressor(),scaled_x,Y)\n\nprint(f\"Best random_state: {best_rState} with best r2_score: {best_r2Score}\")","30f045fe":"#Splitting train test data with random_state = 266\nsg_x_train,sg_x_test,sg_y_train,sg_y_test = train_test_split(scaled_x,Y,test_size=0.25,random_state=266)","486e7fdb":"#Hypertuning parameters\nparameters = {\n    \"loss\": ['huber','squared_loss','epsilon_insensitive','squared_epsilon_insensitive'],\n    \"penalty\": ['l2','l1','elasticnet'],\n    \"alpha\": [0.00001,0.0001,0.001,0.01],\n    \"fit_intercept\": [True,False],\n    \"shuffle\": [True,False],\n    \"tol\": [1e-6,1e-5,1e-4],\n    \"learning_rate\": ['constant','optimal','invscaling','adaptive'],\n}\n#Finding best CV\nbest_cv,sg_best_cvScore = get_best_cv(SGDRegressor(),parameters,sg_x_train,sg_y_train)\nprint(f\"Best CV: {best_cv} with best cross_val_score: {sg_best_cvScore}\")","334fc5dd":"#Building final model with hypertuned parameters with cv=5\nsg_gscv = GridSearchCV(SGDRegressor(),parameters,cv=5)\nsg_gscv.fit(sg_x_train,sg_y_train)\n\nprint(sg_gscv.best_params_)","fd5fbb37":"#Checking Final Performanace of the model\nsg_predict_y = sg_gscv.predict(sg_x_test)\n\nsg_r2Score = r2_score(sg_y_test,sg_predict_y)\nsg_mse = mean_squared_error(sg_y_test,sg_predict_y)\nsg_mae = mean_absolute_error(sg_y_test,sg_predict_y)\n\nprint(f\"R2_SCORE: {round(sg_r2Score*100,2)}%\\t MSE: {sg_mse}\\t RMSE: {np.sqrt(sg_mse)}\\t MAE: {sg_mae}\")","96f8c392":"#Comparing models\n\nmodel_name = ['LinearRegression','Lasso','Ridge','SGDRegressor']\nr2Score = [lr_r2Score,la_r2Score,ri_r2Score,sg_r2Score]\nmse = [lr_mse,la_mse,ri_mse,sg_mse]\nmae = [lr_mae,la_mae,ri_mae,sg_mae]\ncvs = [lr_best_cvScore,la_best_cvScore,ri_best_cvScore,sg_best_cvScore]\n\nmodels = pd.DataFrame({\n    \"name\":model_name,\n    \"r2_score\":r2Score,\n    \"mean_squared_error\":mse,\n    \"mean_absolute_error\":mae,\n    \"cross_val_score\":cvs\n})\nmodels[\"r2_score - cross_val_score\"]=models[\"r2_score\"]-models[\"cross_val_score\"]\nmodels","8313e4a4":"import joblib\nfilename = \"world_happiness_project.pkl\"\njoblib.dump(la_gscv.best_estimator_,open(filename,'wb'))","aa043d0b":"conc = pd.DataFrame({\"Original\":np.array(la_y_test),\"Predicted\":np.array(la_predict_y)})\nconc","ec7f48d5":"Remarks: -Feature Standar Error, Family, Turst (Government Corruption), Generosity and Dystopia Residual shows presence of outliers.","a5dcfc8e":"Remarks: No null values found in dataset.","ea585a6b":"Remarks: -Most number of participating country in happiness score is from Region 8. -Least number of participating country in happiness score is from Region 0 and 5.","ad8f8233":"# Dataset Preparation","b6f0f834":"# Saving Model or Serialization","a28cd4a9":"Remarks: r2_score is almost same in all models. In case of Lasso model, difference between r2_score and corss_val_score is very less as compared to other models, therefore, proceeding with Lasso Model","8c83e1d7":"Remarks: -Data skewness is present in: -Standard Error -Family -Trust (Government Corruption) -Generosity and needs to be treated accordingly.","ba690455":"# Skewness","129e9160":"# Exploratory Data Analysis (EDA)","481d640e":"Remarks: -With the increase in Health (Life Expectancy), Happiness Score also increases.","0a5d7775":"Remarks: -Data is not distributed normally in any feature. -Data is highly left skewed in Economy (GDP per Capita), Family, Health (Life Expectancy) and Freedom. -Data is highly right skewed in Trust (Government Corruption","4497e06c":"Remarks: -Economy (GDP per Capita), Family, Health (Life Expectancy), Freedom, Dystopia Residual are highly correlated. -Happiness Rank is negatively high correlated.","7eb743ce":"# 3. Ridge","d2797538":"# 4. SGDRegressor","763ac35f":"Happiness Rank: Rank of any country in a particular year.\n\nCountry: Name of the country.\n\nStandard Error: The standard error of the happiness score.\n\nHappiness Score: Happiness score as the sum of all numerical columns in the datasets.\n\nEconomy (GDP per Capita): The extent to which GDP contributes to the calculation of the Happiness score\n\nTrust: A quantification of the people\u2019s perceived trust in their governments.\n\nHealth (Life Expectancy): The extent to which Life expectancy contributed to the calculation of the Happiness Score.\n\nGenerosity: Numerical value estimated based on the perception of Generosity experienced by poll takers in their country.\n\nFamily Support: Metric estimating satisfaction of people with their friends and family.\n\nFreedom: Perception of freedom quantified.\n\nDystopia: Hypothetically the saddest country in the world.\n","e3b6c7cb":"# Finding Best Model","19b292ce":"Remarks: -Happiness Rank is inversely proportional to Happiness Score, i.e., as Happiness Rank increases, Happiness Score decreases.","a00d5cca":"Remarks: -With the increase in Trust (Governmnet Corruption), Happiness Score decreases. -Most of the Happpiness Score lies between 0.0 to 0.2 of Trust (Government Corruption).","348f8154":"Remarks: -Region 0 has >7 Happiness Score. -Region 1, mostly have >4.5 and <6 Happiness Score. -Region 2 is between 4.9 to 6.4 Happiness Score. -Region 3 is mostly between 5.6 to 7.2 Happiness Score. -Region 4 is mostly between 4.5 to 6.7 Happiness Score. -Region 5 has >=7 Happiness Score. -Region 6 has >3.5 and <7 Happiness Score. -Region 7 has >3.5 and <5.5 Happiness Score. -Region 8 is mostly between 3.4 to 5.4 Happiness Score. -Region 9 is mostly between 6 to 7.5 Happiness Score.\n","617cfe40":"Remarks: -Economy (GDP per Capita) is directly proportional to Happiness Score, i.e., as Economy (GDP per Capita) increases, Happiness Score increases.","0e3d07d6":"# Univariate Analysis","c0a41df6":"# Outlier Removal","5cd8278b":"Remarks: -Since data loss is between 5 to 6 percent, proceeding with dataset without outliers.","adbef742":"# 2. Lasso","73fa20e4":"# Bivariate Analysis","6c7c7bbc":"# Best Model Selection","2e08da7e":"Remarks: Feature Country and Region are of object type therefore Label Encoding is required accordingly. No null values are present in dataset.","a08709ba":"# Multi-Variate Analysis","c03ac98e":"Remarks: -Density of Happiness Score decreases as the Generosity increases.","44f8c928":"# Content","245f52fe":"# Features Explanation :","e69ee514":"Remarks: -Happiness Score is dense when Standard Error is between 0.03 to 0.06","af77b461":"Remarks: Lasso Model is performing with 99.48% of Accuracy.","cb7bbc00":"# 1. LinearRegression","317fa890":"# Statistical Summary","b84ff643":"# Label Encoding","073e5c69":"Remarks: -count for all features are same therefore no null values are present. -mean is almost same as median, so lesser or no skewness is present. -large difference between 75th percentile and max is there in Hppiness Rank, so outlier may be present.","735fbed0":"Remarks: -With the increase in Dystopia Residual, Happiness Score increases","c4e5f015":"Remarks: -Happiness Rank is negatively high correlated to Happiness Score. -Positively high correlated to Happiness Score are: -Economy (GDP per Capita) -Family -Health (Life Expectancy) -Freedom -Dystopia Residual -Trust (Government Corruption) -Moderately correlated features are: -Generosity -Standard Error -Region","2a7d8265":"# Context","77b7ec3d":"# Loading Dataset","82d9d7a2":"Remarks: Country has 158 unique values which is equal to the total records, therefore, label encoding for it will of no use. Region has 10 unique values, therefore, label encoding is required for it","2865e24d":"The World Happiness Report is a landmark survey of the state of global happiness. The first report was published in 2012, the second in 2013, the third in 2015, and the fourth in the 2016 Update. The World Happiness 2017, which ranks 155 countries by their happiness levels, was released at the United Nations at an event celebrating International Day of Happiness on March 20th. The report continues to gain global recognition as governments, organizations and civil society increasingly use happiness indicators to inform their policy-making decisions. Leading experts across fields \u2013 economics, psychology, survey analysis, national statistics, health, public policy and more \u2013 describe how measurements of well-being can be used effectively to assess the progress of nations. The reports review the state of happiness in the world today and show how the new science of happiness explains personal and national variations in happiness.","3c8656c1":"# Conclusion","26d6c997":"Remarks: -With increase in Freedom, Happiness Score also increases. -Most of the Happiness Score lies between 0.3 to 0.65 of Freedom.","8733e65a":"# Preparing Data for Model","ea2b6e33":"Remarks: -Happiness Score is increasing as the Family increases.","39d09e65":"The happiness scores and rankings use data from the Gallup World Poll. The scores are based on answers to the main life evaluation question asked in the poll. This question, known as the Cantril ladder, asks respondents to think of a ladder with the best possible life for them being a 10 and the worst possible life being a 0 and to rate their own current lives on that scale. The scores are from nationally representative samples for the years 2013-2016 and use the Gallup weights to make the estimates representative. The columns following the happiness score estimate the extent to which each of six factors \u2013 economic production, social support, life expectancy, freedom, absence of corruption, and generosity \u2013 contribute to making life evaluations higher in each country than they are in Dystopia, a hypothetical country that has values equal to the world\u2019s lowest national averages for each of the six factors. They have no impact on the total score reported for each country, but they do explain why some countries rank higher than others."}}