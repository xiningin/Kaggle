{"cell_type":{"afc428c5":"code","58b57112":"code","fd0dd0a2":"code","f8c45420":"code","917bcf44":"code","81fb9675":"code","ed3861ae":"code","dddefd3b":"code","3e20a099":"code","f8609402":"code","58dbcff2":"code","46c3c343":"code","2618b90c":"code","58133f34":"code","b6a0ee2f":"code","d464fbdc":"code","7f9a987d":"code","24851917":"code","67ab006f":"code","048f3d3d":"code","494a1e3e":"code","5930809b":"code","bbecb58b":"code","5b2b15f3":"code","ab8a5388":"code","f45cdcba":"code","2919773d":"code","ee646403":"markdown","ac8bc3ac":"markdown","d362aca8":"markdown","ae4761bd":"markdown","65ebb2e9":"markdown","1481dfdf":"markdown","0756492a":"markdown","bb37644d":"markdown","096511d5":"markdown","3382cdf4":"markdown","eb2c3b78":"markdown","a13e1a04":"markdown"},"source":{"afc428c5":"\n\nimport os\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport datetime\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix,classification_report\nimport matplotlib.pyplot as plt\nfrom pprint import pprint\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import cohen_kappa_score\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold,GroupShuffleSplit,StratifiedShuffleSplit\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn import model_selection\nfrom sklearn.cross_decomposition import PLSRegression, PLSSVD\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import scale \nfrom sklearn.utils import class_weight\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport hashlib\nfrom ml_metrics import quadratic_weighted_kappa\nfrom catboost import CatBoostClassifier,Pool,CatBoostRegressor\nimport shap\nimport statistics\nfrom functools import partial\nimport scipy as sp\nfrom lightgbm import LGBMRegressor\nimport lightgbm as lgb\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nimport pickle\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.utils import shuffle","58b57112":"df_train=pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv')","fd0dd0a2":"spec=pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/specs.csv')","f8c45420":"df_test=pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv')","917bcf44":"    #spec=None\n    spec['info']=spec['info'].str.upper()\n    spec['hashed_info']=spec['info'].transform(hash)\n    spec_unique=pd.DataFrame(spec[['hashed_info']].drop_duplicates())\n    spec_unique['deduped_event_id']=np.arange(len(spec_unique))\n    spec=pd.merge(spec,spec_unique,on='hashed_info',how='left')\n    z=dict(zip(spec.event_id,spec.deduped_event_id))\n    df_train['event_id']=df_train['event_id'].map(z)\n    df_test['event_id']=df_test['event_id'].map(z)\n        #df_train=df_train[df_train['event_id'].isin(df_test['event_id'])]\n    df_train=df_train[df_train['event_id']!=137]  # this particular event id only has 2 records in train and none in test....\n    df_event_id_train=pd.pivot_table(df_train.loc[:,['installation_id','game_session','event_id']],aggfunc=len,columns=['event_id'],index=['installation_id','game_session']).add_prefix('event_id_').rename_axis(None,axis=1).reset_index()\n    df_event_id_test=pd.pivot_table(df_test.loc[:,['installation_id','game_session','event_id']],aggfunc=len,columns=['event_id'],index=['installation_id','game_session']).add_prefix('event_id_').rename_axis(None,axis=1).reset_index()\n    df_event_id_train=df_event_id_train.fillna(0)\n    df_event_id_train=df_event_id_train.fillna(0)\n    df_event_id_test=df_event_id_test.fillna(0)","81fb9675":"def create_features(df):\n    df['timestamp']=pd.to_datetime(df['timestamp'])\n    df['Incorrect_Game_Attempt']=np.where((df['event_data'].str.contains('\"correct\":false')&(df['type']=='Game')),1,0)\n    df['Correct_Game_Attempt']=np.where((df['event_data'].str.contains('\"correct\":true')&(df['type']=='Game')),1,0)\n    df['Is_Weekend']=np.where(((df['timestamp'].dt.day_name()=='Sunday')|(df['timestamp'].dt.day_name()=='Saturday')),1,0)\n    df['Phase_Of_Day']=np.where(df['timestamp'].dt.hour.isin(range(6,12)),'Morning',np.where(df['timestamp'].dt.hour.isin(range(13,19)),'Evening','Night'))\n    #train_Assessed_ids=set(df.loc[df['type']=='Assessment']\n    #.loc[((df['title'] != \"Bird Measurer (Assessment)\")&(df['event_code']==4100))|((df['title'] == \"Bird Measurer (Assessment)\")&(df['event_code']==4110))]\n    #.loc[:,'installation_id'].unique())\n    df_world=pd.pivot_table(df.loc[df['world']!='NONE',['installation_id','game_session','world']].drop_duplicates(),index=['installation_id','game_session'],columns=['world'],aggfunc=len).add_prefix('rolling_').rename_axis(None, axis=1).reset_index()\n    \n    df_type_world=pd.merge(df_world,pd.pivot_table(df.loc[:,['installation_id','game_session','type']].drop_duplicates(),index=['installation_id','game_session'],columns=['type'],fill_value=0,aggfunc=len).rename_axis(None, axis=1).reset_index(),on=['installation_id','game_session'],how='right')\n    df_type_world_title=pd.merge(df_type_world,pd.pivot_table(df.loc[:,['installation_id','game_session','title']].drop_duplicates(),index=['installation_id','game_session'],columns=['title'],fill_value=0,aggfunc=len).add_prefix('rolling_').rename_axis(None, axis=1).reset_index(),on=['installation_id','game_session'],how='right')\n\n    df_activity_weekend=pd.merge(df_type_world_title,pd.DataFrame(pd.pivot_table(df.loc[:,['installation_id','game_session','Is_Weekend']].drop_duplicates(),index=['installation_id','game_session'],columns=['Is_Weekend'],fill_value=0,aggfunc=len)).add_prefix('Weekend_').rename_axis(None, axis=1).reset_index(),on=['installation_id','game_session'],how='right')\n    df_activity_weekend_phase_of_day=pd.merge(pd.DataFrame(pd.pivot_table(df.loc[:,['installation_id','game_session','Phase_Of_Day']].drop_duplicates(),index=['installation_id','game_session'],columns=['Phase_Of_Day'],fill_value=0,aggfunc=len)).rename_axis(None, axis=1).reset_index(),df_activity_weekend,on=['installation_id','game_session'],how='left')\n    df_train_Assessed=df.copy()\n    df_train_Assessed['Incorrect_Attempt']=np.where((df['event_data'].str.contains('\"correct\":false'))&(((df['title'] != \"Bird Measurer (Assessment)\")&(df['event_code']==4100))|((df['title'] == \"Bird Measurer (Assessment)\")&(df['event_code']==4110))),1,0)\n    df_train_Assessed['Correct_Attempt']=np.where((df['event_data'].str.contains('\"correct\":true'))&(((df['title'] != \"Bird Measurer (Assessment)\")&(df['event_code']==4100))|((df['title'] == \"Bird Measurer (Assessment)\")&(df['event_code']==4110))),1,0)\n    df_train_acc=df_train_Assessed[df_train_Assessed['title'].isin(['Bird Measurer (Assessment)','Mushroom Sorter (Assessment)','Cauldron Filler (Assessment)','Chest Sorter (Assessment)','Cart Balancer (Assessment)'])].groupby(['installation_id','title','game_session'])['Incorrect_Attempt','Correct_Attempt'].sum().rename_axis(None, axis=1).reset_index()\n    df_train_acc['Total_Attempts']=df_train_acc.apply(lambda x: x['Incorrect_Attempt'] + x['Correct_Attempt'], axis=1)\n    #df_train_acc=df_train_acc[df_train_acc['Total_Attempts']!=0]\n    #\n    #df_train_acc['accuracy']=[x['Correct_Attempt']\/ x['Total_Attempts'] for x in df_train_acc if x['Total_Attempts']>0]\n    df_train_acc['accuracy']=np.where(df_train_acc['Total_Attempts']>0,df_train_acc['Correct_Attempt']\/ df_train_acc['Total_Attempts'],0)\n    df_train_acc['accuracy_group']=np.where(df_train_acc['accuracy']==1,3,np.where(df_train_acc['accuracy']==.5,2,np.where(df_train_acc['accuracy']==0,0,1)))\n    df_game_attempt=df.groupby(['installation_id','game_session'])['Incorrect_Game_Attempt','Correct_Game_Attempt'].sum().rename_axis(None, axis=1).reset_index()\n\n    df_event_codes=pd.pivot_table(df_train_Assessed.loc[:,['installation_id','game_session','event_code']],index=['installation_id','game_session'],columns=['event_code'],fill_value=0,aggfunc=len).add_prefix('event_code_').rename_axis(None, axis=1).reset_index()\n    df_final=pd.merge(pd.merge(df_train_acc,df_activity_weekend_phase_of_day,on=['installation_id','game_session'],how='right'),df_event_codes,on=['installation_id','game_session'],how='right')\n    df_gametime=df.groupby(['installation_id','game_session'])['game_time','timestamp','event_count'].max().reset_index()\n    df_final=pd.merge(df_final,df_gametime,on=['installation_id','game_session'],how='left')\n    df_final=df_final.fillna(value=0)\n    #df_final_title=pd.pivot_table(df_final.loc[df_final['title']!=0,['installation_id','game_session','title','Incorrect_Attempt','Correct_Attempt']].drop_duplicates(),index=['installation_id','game_session'],columns=['title'],values=['Incorrect_Attempt','Correct_Attempt'],aggfunc=sum)\n    #df_final_title=pd.pivot_table(df_final.loc[df_final['title']!=0,['installation_id','game_session','title','Incorrect_Attempt','Correct_Attempt']].drop_duplicates(),index=['installation_id','game_session'],columns=['title'],values=['Incorrect_Attempt','Correct_Attempt'],aggfunc=sum)\n    #df_final_title.columns=['_'.join(col) for col in df_final_title.columns.values]\n    #df_final_title=df_final_title.reset_index()\n    #df_final=pd.merge(df_final.drop(['Incorrect_Attempt','Correct_Attempt','accuracy'],axis=1),df_final_title,on=['installation_id','game_session'],how='left')\n    df_final=pd.merge(df_final,df.loc[df['world']!=\"NONE\",['installation_id','game_session','world']].drop_duplicates(),on=['installation_id','game_session'],how='left')\n    df_final=pd.merge(df_final,df_game_attempt,on=['installation_id','game_session'],how='left')\n    df_final=df_final.fillna(value=0)\n    return(df_final)","ed3861ae":"def rolling_exponential_average(df):\n    col=list(df.select_dtypes(include=['float64','int64']).columns)\n    #print(df.head())\n    df=df.sort_values(['installation_id','timestamp'])\n    df_rolling_avg=df.groupby(['installation_id'])[col].apply(lambda x:x.ewm(alpha=0.1,min_periods=1).mean())\n    df_rolling_avg=df_rolling_avg.rename(columns={'accuracy_group':'rolling_accuracy_group','CRYSTALCAVES':'rolling_CRYSTALCAVES','MAGMAPEAK':'rolling_MAGMAPEAK', 'TREETOPCITY':'rolling_TREETOPCITY'})\n    #print(df_rolling_avg.index)\n    #df_rolling_avg.index.names=['installation_id', 'level_1']\n    df_rolling_avg.index.names=['level_1']\n    return(df_rolling_avg)","dddefd3b":"def create_final_dataset(df,Is_Test=0):\n    f=create_features(df)\n    df_assessed=set(df.loc[df['type']=='Assessment']\n                    .loc[((df['title'] != \"Bird Measurer (Assessment)\")&(df['event_code']==4100))|\n                         ((df['title'] == \"Bird Measurer (Assessment)\")&(df['event_code']==4110))]\n                    .loc[:,'game_session'].unique())\n    #if (Is_Test==0):\n    #    f=pd.merge(f,df_event_id_train,right_index=True,left_index=True)\n    #else:\n    #    f=pd.merge(f,df_event_id_test,right_index=True,left_index=True)\n    if (Is_Test==0):\n        f=pd.merge(f,df_event_id_train,on=['installation_id','game_session'],how='left')\n    else:\n        f=pd.merge(f,df_event_id_test,on=['installation_id','game_session'],how='left')\n    \n    df_rolling_avg=rolling_exponential_average(f)\n    #print(df_rolling_avg.columns)\n    df_rolling_avg=df_rolling_avg.fillna(0)\n    f.index.names=['level_1']\n    \n    t=pd.merge(f[['installation_id','game_session','timestamp','world','title']],df_rolling_avg,left_index=True,right_index=True,how='left')\n    #print(t.loc[t['game_session'].isin(df_assessed),'title'].unique())\n    t=t.reset_index(drop=True).sort_values(['installation_id','timestamp'])\n    tm=pd.merge(t[['installation_id','game_session','title','world']],t.groupby(['installation_id']).shift(1).drop(['game_session','title','world'],axis=1),left_index=True,right_index=True,how='left')\n    tm=tm.dropna()\n    tm=tm[tm['game_session'].isin(df_assessed)]\n    #print(tm.loc[tm['game_session'].isin(df_assessed),'title'].unique())\n    #print(tm.columns)\n    if (Is_Test==0):\n        tm['last_attempt_time']=tm.groupby(['installation_id']).shift(1)['timestamp']\n        tm['last_world']=tm.groupby(['installation_id']).shift(1)['world']\n        tm['last_title']=tm.groupby(['installation_id']).shift(1)['title']\n        tm['time_since_last_attempt']=np.where(tm['last_attempt_time'].isna(),0,(tm['timestamp']-tm['last_attempt_time']).dt.seconds)\n    else:\n        df_latest_Attempt=t.sort_values(['installation_id','timestamp']).groupby(['installation_id'],as_index=False).last()\n        #df_latest_Attempt=t.groupby(['installation_id','timestamp'],as_index=False).max()\n        #print(df_latest_Attempt.shape)\n        #print(tm[['timestamp']].head())\n        tm=tm.sort_values(['installation_id','timestamp','title','world']).groupby(['installation_id'],as_index=False).last().rename(columns={'timestamp':'last_attempt_time','world':'last_world','title':'last_title'}).drop_duplicates()\n        \n        df_latest_Attempt=pd.merge(tm[['installation_id','last_attempt_time','last_world','last_title']],df_latest_Attempt,on=['installation_id'],how='right')\n        #print(df_latest_Attempt.head())\n        df_latest_Attempt['time_since_last_attempt']=np.where(df_latest_Attempt['last_attempt_time'].isna(),0,(pd.to_datetime(df_latest_Attempt['timestamp'].dt.tz_localize(None))-pd.to_datetime(df_latest_Attempt['last_attempt_time'])).dt.seconds)\n        tm=df_latest_Attempt.copy()\n        #print(tm.shape)\n    #tm1=pd.merge(tm,tm.groupby(['installation_id'])['timestamp'].shift(1),left_index=True,right_index=True,how='left')\n    tm1=pd.merge(tm,f[['accuracy_group','game_session','installation_id']],on=['game_session','installation_id'],how='left')\n    tm1=tm1.fillna(0)\n    return(tm1)","3e20a099":"f_train=create_final_dataset(df_train,0)\n\n\nf_test=create_final_dataset(df_test,1)\n\nf_train = f_train.reindex(sorted(f_train.columns), axis=1)\nf_test = f_test.reindex(sorted(f_test.columns), axis=1)","f8609402":"f_train_1=shuffle(f_train)","58dbcff2":"f_train_1.reset_index(inplace=True, drop=True) ","46c3c343":"class OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n    \n    def _kappa_loss(self, coef, X, y):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n        return -cohen_kappa_score(y, preds, weights = 'quadratic')\n    \n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X = X, y = y)\n        initial_coef = [0.5, 1.5, 2.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method = 'nelder-mead')\n    \n    def predict(self, X, coef):\n        preds = pd.cut(X, [-np.inf] + list(np.sort(coef)) + [np.inf], labels = [0, 1, 2, 3])\n        return preds\n    \n    def coefficients(self):\n        return self.coef_['x']","2618b90c":"def model_classification(train_X,train_y,val_X,val_y,cat_feat):\n    \n    CatBoost=CatBoostClassifier(\n        learning_rate=0.2,\n        max_depth=6,\n        bootstrap_type='Bernoulli',\n        l2_leaf_reg= 0.2,\n        iterations=400,\n        subsample=.7,\n        loss_function='MultiClass',\n        eval_metric=\"WKappa\",\n        random_seed=4,cat_features=cat_feat,class_weights=[1,2,2,.5],\n        classes_count=4)\n    clf = CatBoost.fit(train_X,train_y,eval_set=(val_X,val_y), verbose_eval=False,use_best_model=True,plot=True,early_stopping_rounds=100)\n    return(clf)\n\n    ","58133f34":"def model_regressor(train_X,train_y,val_X,val_y,fit):\n    lgb_model=LGBMRegressor(objective='regression',\n                         max_depth=13,num_leaves=6,\n                          n_estimators=1200,\n                         bagging_fraction=0.8,\n                         random_state=42,verbose=0,reg_alpha=.3,reg_lambda=.3,callbacks=[lgb.reset_parameter(learning_rate=[0.2] * 200 + [0.15] * 250+[.1]*250+[.05]*250+[.01]*250)])\n    if fit==0:\n        reg=lgb_model.fit(train_X,train_y,eval_set=(val_X,val_y), verbose=False)\n    else:\n        reg=lgb_model.fit(train_X,train_y,verbose=False)\n    \n    \n\n        \n    return(reg)\n","b6a0ee2f":"def fit(X_train,y_train,classifier):\n    train_X = X_train\n    train_y=y_train\n    \n    train_X=train_X.drop('installation_id',axis=1)\n    \n    \n    if(classifier==1):\n            clf=model_classification(train_X,train_y,0,0,cat_feat)\n            pred_val=clf.predict(val_X)\n            print(classification_report(val_y,pred_val))\n        #print(evals_result)\n            score=cohen_kappa_score(pred_val,val_y,weights='quadratic')\n            scores.append(score)\n            print('choen_kappa_score :',score)\n            coeff=0\n    else:\n            clf=model_regressor(train_X,train_y,0,0,1)\n            optR = OptimizedRounder()\n            optR.fit(clf.predict(train_X), train_y)\n            coeff = optR.coefficients()         \n            \n            \n    return(clf,coeff)  ","d464fbdc":"\ndef cv(X_train,y_train,classifier=1,n_splits=5):\n    scores=[]\n\n    #kf = StratifiedShuffleSplit(n_splits=n_splits, random_state=42,)\n    Gf=GroupKFold(n_splits=n_splits)\n    #y_pre=np.zeros((len(validation_set),4),dtype=float)\n    #final_test=xgb.DMatrix(validation_set)\n    \n    i=0\n    \n    \n    for train_index, val_index in Gf.split(X_train, y_train,X_train['installation_id']): \n        #print(len(train_index),len(val_index))\n        train_X = X_train.iloc[train_index]\n        train_X=train_X.drop('installation_id',axis=1)\n        val=X_train.iloc[val_index].join(y_train[val_index])\n        #val_X = X_train.iloc[val_index]\n        #val_X=val_X.drop('installation_id',axis=1)\n        train_y = y_train[train_index]\n        \n        #print(train_y.columns)\n        #val_y = y_train[val_index]\n        val=val.groupby('installation_id',as_index=False).apply(lambda x :x.iloc[np.random.randint(0,len(x))])\n        val_X=val.drop(['accuracy_group','installation_id'],axis=1)\n        val_y=val['accuracy_group']\n        #class_weights=class_weight.compute_class_weight('balanced',np.unique(train_y),train_y)\n        #print(class_weights)\n        #print(np.unique(w_array))   \n        #xgb_train = xgb.DMatrix(train_X, train_y,weight=None)\n        #xgb_eval = xgb.DMatrix(val_X, val_y)\n        if(classifier==1):\n            clf=model_classification(train_X,train_y,val_X,val_y,0)\n            pred_val=clf.predict(val_X)\n            print(classification_report(val_y,pred_val))\n        #print(evals_result)\n            score=cohen_kappa_score(pred_val,val_y,weights='quadratic')\n            scores.append(score)\n            print('choen_kappa_score :',score)\n        else:\n            clf=model_regressor(train_X,train_y,val_X,val_y,0)\n            print(clf.predict(train_X))\n            optR = OptimizedRounder()\n            optR.fit(clf.predict(train_X), train_y)\n            coeff = optR.coefficients()\n            print(coeff)\n            \n            val_pred=clf.predict(val_X)\n            pred_val=[regression_class(val_pred_i,coeff) for val_pred_i in val_pred]\n\n            print(classification_report(val_y,pred_val))\n            score=cohen_kappa_score(pred_val,val_y,weights='quadratic')\n            scores.append(score)\n            print('choen_kappa_score :',score)           \n    print(statistics.mean(scores),statistics.stdev(scores))      \n    return(clf)\n            \n\n        \n\n   ","7f9a987d":"def regression_class(pred,coeff):\n    if (pred<=coeff[0]):\n        class_=0\n    elif (coeff[0]<pred<=coeff[1]):\n        class_=1\n    elif (coeff[1]<pred<=coeff[2]):\n        class_=2\n    else:\n        class_=3\n    return(class_)","24851917":"f_train_1['accuracy_group']=f_train_1['accuracy_group'].astype('int')","67ab006f":"X_train=f_train_1.drop(['accuracy_group','game_session','last_attempt_time','timestamp'],axis=1)\ny_train=f_train_1['accuracy_group']\nX_test=f_test.drop(['accuracy_group','game_session','last_attempt_time','timestamp'],axis=1)\ny_test=f_test['accuracy_group']","048f3d3d":"c=['last_world','last_title','world','title']\ncat_feat=[X_train.columns.get_loc(col) for col in c]\nX_train=X_train.join(pd.get_dummies(X_train[c])).drop(c,axis=1)","494a1e3e":"X_test=X_test.join(pd.get_dummies(X_test[c])).drop(c,axis=1)","5930809b":"reg_cv=cv(X_train,y_train,0,5)","bbecb58b":"reg_model,coeff=fit(X_train,y_train,0)","5b2b15f3":"f_test['accuracy_group']=[regression_class(pred_i,coeff) for pred_i in reg_model.predict(X_test.drop('installation_id',axis=1))] ","ab8a5388":"Submission_file=f_test[['installation_id','accuracy_group']]","f45cdcba":"Submission_file['accuracy_group'].value_counts()\n","2919773d":"Submission_file.to_csv('submission.csv',index=False)","ee646403":"A session has multiple event_id and we can't create 166 dummy variables just for this column, so its better to reduce them into few... reducing them to 30 variables covers ~78% variability","ac8bc3ac":"If you carefully look at Spec file, although there are 387 unique event_ids but unique info are still 167...so i thought that I can merge event_ids for same info in order to reduce cardinality of event_id in raw data.... ","d362aca8":"**Best is yet to come**\n* This is a WIP kernel, haven't fully evolved in feature space and model space\n* Will be adding more variables and removing  lot of garbage variables in future\n* for now, have tried LGB regressor as initial attempt for modeling with some Bayesian hyper parameter tuning, exploring other option too\n* Need your suggestion\/criticism\/upvotes for improvement","ae4761bd":"With some hyper parameter tuning , I have used following. Average weighted kappa for validation set is ~.51","65ebb2e9":"Training and Cross validation ia tricky one here and a lot of experimentation can be done. While splitting train\/validation, it is done keeping installation_id as group which means all assessed sessions of a installation_id ar ein either of these two sets.\n\nFurther more, since the actual test set has got only one randomly selected assessed session, the same is done with our validation set also, so that it mimics the actual test set and our CV are somewhere near LB scores.\n\nAs of now, I am treating this problem as a classification one. Yes....I am not considering the ordinal relation among output classes. In future, I will be adding regressor to do the same","1481dfdf":"After looking at the data, one thing you must have realised without thinking twice that this data can't be given to any model in present form...It needs a overhaul makeup\n\nQuestion comes do you want to have features at user level or at game session level???\n\nThis is quite subjective, putting features at user level makes one thing difficult to determine....that is Accuracy_Group(our dependent variable) because it is defined for game session\n\nOk...having said that what is wrong if we prepare our data at game session level...nothing much..except one thing...the samples won't be independent of each other as same user has multiple sessions. This can be handled in train\/val set split\n\n","0756492a":"Now every session must have accumulated data for respective user till that time...I have tried using **exponential moving average** for past sessions, this will put more weight on recent sessions and fade up the older ones\n\nThis will give for *ith* game session of particular user, get the mean of accuracy, type of worlds, correct attempts etc from begining till that session","bb37644d":"**Problem Statement**","096511d5":"Now we should create features at installation_id+game_session level. Features that I am adding thru following function are:\n* Weekend\n* Phase of day(Day, Evening or night)\n* Count of various world in respective session\n* Count of various game type in respective session\n* Count of various world in respective session\n* Count of Correct\/Incorrect attempts in game\n* Total correct, Incorrect and Total attempts in respective session\n* Accuracy and Accuracy group in respective session\n","3382cdf4":"Ok....seems we have created a lot of variables by now(more than 80 I believe). These variables are created for all sessions(irrespective of whether an attempt was made or not in that session)....for training pupose those sessions where in no attempt was made doesn't help much becuase accuracy group is NA for such sessions(not 0, it means user tried but coudn't pass)\n\nOne thing needs to be understood, the ultimate goal is prediction for test set and for latest assessed session in tets set we don't have much info(expect fot time,title and world).\nSo its better we train our model keeping history in mind and to exclude such features from current session because test doesn't have it. for example, for given assessed session in test we don't know all event id, event codes, world , type etc in full, so its betters we keep the rolling average variables till previous session for every session where an attempt has been given\nFor current session, the features that I have used are:\n* Title\n* World\n* Time since last attempt\nRest all other features belong to previous session... I hope it's not confusing\n","eb2c3b78":"**Modeling the Problem**","a13e1a04":"The problem statement provided in competition page is very ambigious and can give hard time determining what we are supposed to predict.\nFirst point of confusion starts with Train and Test files being same in column structure... then test file also has data with correct or incorrect assessement submissions... If it already has it then what we are supposed to predict???\n\nTo answer this, we need to look at Test file more closely(with problem statement echoing in background \"*In the test set, we have truncated the history after the start event of a single assessment, chosen randomly, for which you must predict the number of attempts.*\")\n\nOk...so if a kid has in total took 30 sessions in this game playing games, clips, assessements and so on...out of which he has actually attempted assessements in 10 sessions, then they have randomly selected any of these assessed attempts and has truncated complete data after that attempt(including full details of last assessed sessions). Goal is to predict that in that last session(chosen at randomly) how many attempts that kid has taken to pass the assessement...\nNow its pretty straightforward...Isn't it???"}}