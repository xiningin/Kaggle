{"cell_type":{"1fe054f0":"code","b2719fee":"code","3863dda2":"code","97df816e":"code","61045bbb":"code","540295d3":"code","22d37e5d":"code","01bcd1f5":"code","a0abae33":"code","7ad3585b":"code","1e166143":"code","4a0ca65f":"code","cd291567":"code","326b79ab":"code","aa7e6847":"code","54145c4e":"code","85ce5ae2":"code","f444ebdd":"code","534d652d":"code","504f929a":"code","5baa0798":"code","951c8a84":"code","1496e191":"code","b23c4691":"markdown","0a3cf96c":"markdown","b94c9790":"markdown","791f20d0":"markdown","8f45597c":"markdown","c1df3e8a":"markdown","903e2927":"markdown","19684355":"markdown","6b2e0827":"markdown","5717c31d":"markdown","cab6f98e":"markdown","e53833ea":"markdown","6510cbb5":"markdown","e595fdb4":"markdown","04b1b643":"markdown","381d8164":"markdown","a27d4609":"markdown","c84e1672":"markdown","1645574a":"markdown","932ef436":"markdown","6a66bfbf":"markdown"},"source":{"1fe054f0":"#Simple Data processing\nimport numpy as np #linear algebra\nimport pandas as pd # data processing, .csv load\nimport os\n\n#Feature Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\n#Data Visualization\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.ticker import NullFormatter\nimport matplotlib.ticker as ticker\nimport itertools #For Confusion Matrix\n%matplotlib inline\nimport seaborn as sns\n\n#Feature Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\n\n# Scaling\nfrom sklearn import preprocessing #For data normalization\n\n# Model Selection\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV # For parameterization and splitting data\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics # For Accuracy\n\n#Classification Algorithms\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import RidgeClassifier","b2719fee":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3863dda2":"cancer=pd.read_csv('\/kaggle\/input\/breast-cancer-prediction-dataset\/Breast_cancer_data.csv')\ncancer.head()","97df816e":"cancer.info()","61045bbb":"cancer.describe()","540295d3":"cancer.columns.unique","22d37e5d":"x, ax=plt.subplots(figsize=(12,12))\nsns.heatmap(cancer.corr(),annot=True, linewidths=0.5, ax=ax)\nplt.show()","01bcd1f5":"cancer_corr = cancer.corr() \ncorr_target = abs(cancer_corr[\"diagnosis\"])\nrelevant_features = corr_target[corr_target>0.5]\nrelevant_features","a0abae33":"X_feat=cancer[['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area',\n       'mean_smoothness']]\ny_feat=cancer['diagnosis']\nX_feat","7ad3585b":"#Feature Selection\nbestfeatures = SelectKBest(score_func=f_classif, k=5)\nfit = bestfeatures.fit(X_feat,y_feat)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X_feat.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Assembly','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(12,'Score'))  #print 10 best features","1e166143":"train_accuracy= []\naccuracy_list = []\nalgorithm = []\n\nX_train,X_test,y_train,y_test = train_test_split(cancer[['mean_perimeter','mean_radius','mean_area']]\n                                                 ,cancer['diagnosis'],test_size=0.2, random_state=0)\nprint(\"X_train shape :\",X_train.shape)\nprint(\"Y_train shape :\",y_train.shape)\nprint(\"X_test shape :\",X_test.shape)\nprint(\"Y_test shape :\",y_test.shape)","4a0ca65f":"scaler_ss=preprocessing.StandardScaler()\nX_train_scaled=scaler_ss.fit_transform(X_train)\nX_test_scaled=scaler_ss.transform(X_test)","cd291567":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion Matrix',\n                          cmap=plt.cm.BuGn):\n\n    print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","326b79ab":"Log_Reg=LogisticRegression(C=1, class_weight='balanced', dual=False,\n                   fit_intercept=True, intercept_scaling=1, l1_ratio=None,\n                   max_iter=1000, multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=0, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\nLog_Reg.fit(X_train_scaled, y_train)\ny_reg=Log_Reg.predict(X_test_scaled)\nprint(\"Train Accuracy {0:.3f}\".format(Log_Reg.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_reg)))\ncm = metrics.confusion_matrix(y_test, y_reg)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Benign', 'Malignant'],\n                          title='Logistic Regression')\naccuracy_list.append(metrics.accuracy_score(y_test, y_reg)*100)\ntrain_accuracy.append(Log_Reg.score(X_train_scaled, y_train))\nalgorithm.append('Logistic Regression')","aa7e6847":"SVC_param={'kernel':['sigmoid','rbf','poly'],'C':[1],'decision_function_shape':['ovr'],'random_state':[0]}\nSVC_pol=SVC()\nSVC_parm=GridSearchCV(SVC_pol, SVC_param, cv=5)\nSVC_parm.fit(X_train_scaled, y_train)\ny_pol=SVC_parm.predict(X_test_scaled)\nprint(\"The best parameters are \",SVC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(SVC_parm.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_pol)))\ncm = metrics.confusion_matrix(y_test, y_pol)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Benign', 'Malignant'],\n                          title='SVM')\ntrain_accuracy.append(SVC_parm.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_pol)*100)\nalgorithm.append('SVM')","54145c4e":"error = []\n# Calculating error for K values between 1 and 40\nfor i in range(1, 40):\n    K_NN =KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=i, p=2,\n                     weights='distance')\n    K_NN.fit(X_train_scaled, y_train)\n    pred_i = K_NN.predict(X_test_scaled)\n    error.append(np.mean(pred_i != y_test))","85ce5ae2":"plt.figure(figsize=(12, 6))\nplt.plot(range(1, 40), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')","f444ebdd":"K_NN =KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=15, p=2,\n                     weights='distance')\nK_NN.fit(X_train_scaled, y_train)\ny_KNN=K_NN.predict(X_test_scaled)\nprint(\"Train Accuracy {0:.3f}\".format(K_NN.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_KNN)))\ncm = metrics.confusion_matrix(y_test, y_KNN)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Benign', 'Malignant'],\n                          title='KNN')\ntrain_accuracy.append(K_NN.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_KNN)*100)\nalgorithm.append('KNN')","534d652d":"RFC_param={'max_depth':[1,2,3,4,5],'n_estimators':[10,25,50,100,150],'random_state':[None],'criterion':['entropy','gini'],'max_features':[0.5]}\nRFC=RandomForestClassifier()\nRFC_parm=GridSearchCV(RFC, RFC_param, cv=5)\nRFC_parm.fit(X_train_scaled, y_train)\ny_RFC=RFC_parm.predict(X_test_scaled)\nprint(\"The best parameters are \",RFC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(RFC_parm.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_RFC)))\ncm = metrics.confusion_matrix(y_test, y_RFC)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Benign', 'Malignant'],\n                          title='RFC')\ntrain_accuracy.append(RFC_parm.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_RFC)*100)\nalgorithm.append('Random Forest')","504f929a":"GBC_parma={'loss':['deviance','exponential'],'n_estimators':[10,25,50,100,150],'learning_rate':[0.1,0.25, 0.5, 0.75],\n          'criterion':['friedman_mse'], 'max_features':[None],'max_depth':[1,2,3,4,5,10],'random_state':[None]}\nGBC = GradientBoostingClassifier()\nGBC_parm=GridSearchCV(GBC, GBC_parma, cv=5)\nGBC_parm.fit(X_train_scaled, y_train)\ny_GBC=GBC_parm.predict(X_test_scaled)\nprint(\"The best parameters are \",GBC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(GBC_parm.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_GBC)))\ncm = metrics.confusion_matrix(y_test, y_GBC)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Benign', 'Malignant'],\n                          title='GBC')\ntrain_accuracy.append(GBC_parm.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_GBC)*100)\nalgorithm.append('GBC')","5baa0798":"RC_parma={'solver':['svd','lsqr','cholesky'],'alpha':[0,0.5,0.75,1,1.5,2],'normalize':[True,False]}\nRC=RidgeClassifier()\nRC_parm=GridSearchCV(RC, RC_parma, cv=5)\nRC_parm.fit(X_train_scaled, y_train)\ny_RC=RC_parm.predict(X_test_scaled)\nprint(\"The best parameters are \",RC_parm.best_params_)\nprint(\"Train Accuracy {0:.3f}\".format(RC_parm.score(X_train_scaled, y_train)))\nprint('Test Accuracy' \"{0:.3f}\".format(metrics.accuracy_score(y_test, y_RC)))\ncm = metrics.confusion_matrix(y_test, y_RC)\nnp.set_printoptions(precision=2)\nplt.figure()\nplot_confusion_matrix(cm, classes=['Alive', 'Death'],\n                          title='Ridge Classifier')\ntrain_accuracy.append(RC_parm.score(X_train_scaled, y_train))\naccuracy_list.append(metrics.accuracy_score(y_test, y_RC)*100)\nalgorithm.append('Ridge Classifier')","951c8a84":"#Train Accuracy\nf,ax = plt.subplots(figsize = (10,5))\nsns.barplot(x=train_accuracy,y=algorithm,palette = sns.dark_palette(\"blue\",len(accuracy_list)))\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Algorithm\")\nplt.title('Algorithm Train Accuracy')\nplt.show()","1496e191":"#Testing Accuracy\nf,ax = plt.subplots(figsize = (10,5))\nsns.barplot(x=accuracy_list,y=algorithm,palette = sns.dark_palette(\"blue\",len(accuracy_list)))\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Algorithm\")\nplt.title('Algorithm Test Accuracy')\nplt.show()","b23c4691":"# <u> Feature Selection for Higher Breast Cancer Prediction Accuracy<\/u>\n<u> By: Christopher Smith https:\/\/github.com\/CWSmith022\/Learning.git<\/u>\n\nThe .csv file was obtained from: https:\/\/www.kaggle.com\/merishnasuwal\/breast-cancer-prediction-dataset\n\n\nCancer is a disease that is caused by rapid cellular proliferation. The differences in treatment or severity can matter greatly depending on benign or malignancy. Here, an approach using tools in the sci-kit learn library will be used for prediction of tumor type. The process starts by feature selection with either the KBestFunction by $Chi^{2}$ score or a correlation matrix, then the data is preprocessed to be used for several supervised Machine Learning Algorithms.\n\nThe Algorithms used are:\n\n- Logistic Regression\n- Support Vector Machines\n- K-Nearest Neighbors\n- Random Forest\n- Gradient Boosting\n- Ridge Classifier\n\n## <u> Logistic Regression <\/u> \nA model that is used statistically for binary dependent variables based on the probability of an event occuring. This can be further extended for several variables in a classification setting for multi-class prediction.\n    \n## <u> Support Vector Machines (SVM) <\/u>\nCommonly used for classification tasks, SVM's function by a Kernel which draws points on a hyperplane and uses a set of vectors to separate data points. This separation of data points creates a decision boundary for where a new data point can be predicted for a specific class label. \n\n## <u> K-Nearest Neighbors <\/u>\nSimply, an algorithm that clusters the data and by a measure of distance to the 'k' nearest points votes for a specific class prediction.\n\n## <u> Random Forest <\/u> \nAn ensemble method that estimates several weak decision trees and combines the mean to create an uncorrelated forest at the end. The uncorrelated forest should be able to predict more accurately than an individual tree.\n\n## <u> Gradient Boosting <\/u>\nSimilar to Random Forest, Gradient Boosting builds trees one at a time then ensembles them as each one is built.\n\n## <u> Ridge Classifier <\/u>\nNormalizes data then treats problem as a multi-output regression task.","0a3cf96c":"<a id=\"5\"><\/a>\n# Splitting The Data","b94c9790":"<a id=\"3\"><\/a>\n## Exploring Data","791f20d0":"<a id=\"1\"><\/a>\n## Importing Libraries","8f45597c":"<a id=\"0\"><\/a>\n## <b> Feature Meaning <b>\n- diagnosis: The diagnosis of breast tissues (1 = malignant, 0 = benign) where malignant denotes that the disease is harmful\n- mean_radius: mean of distances from center to points on the perimeter\n- mean_texture: standard deviation of gray-scale values\n- mean_perimeter: mean size of the core tumor\n- mean_area: mean area of the core tumor\n- mean_smoothness: mean of local variation in radius lengths","c1df3e8a":"<a id=\"7.2\"><\/a>\n## Support Vector Machine\nBy using GRIDSearchCV the best kernel will be decided for the model.","903e2927":"<a id=\"4\"><\/a>\n## Feature Selection","19684355":"<a id=\"1\"><\/a>\n## Importing Data","6b2e0827":"## Confusion Matrix","5717c31d":"<a id=\"6\"><\/a>\n# Feature Scaling (Normalization)\nTo remove outlier bias the formula $z=(x-u)\/s$ is used first on the training set then applied to the testing set","cab6f98e":"<a id=\"7.5\"><\/a>\n## Gradient Boosting Classifier\nBy using GRIDSearchCV the best parameters will be decided for the model.","e53833ea":"<a id=\"7\"><\/a>\n# <b> Machine Learning <b>\n<b> 1=malignant, 0=benign <b>\n<a id=\"7.1\"><\/a>\n## <b> Logistic Regression <b>","6510cbb5":"<a id=\"9\"><\/a>\n# Evaluation of Accuracy","e595fdb4":"<a id=\"7.4\"><\/a>\n## Random Forest\nBy using GRIDSearchCV the best parameters will be decided for the model.","04b1b643":"We see that for both the correlation matrix and KBest Feature Selector that the mean_perimeter, mean_radius and mean_area are the most relevant features towards the diagnosis of begnign or melignant cancer.","381d8164":"## Table of Contents\n[0.Feature Meaning](#0) <br\/>\n[1.Importing Libraries](#1) <br\/>\n[2.Importing Data](#2) <br\/>\n[3.Exploring Data](#3) <br\/>\n[4.Feature Selection](#3) <br\/>\n[5.Splitting the Data](#4) <br\/>\n[6.Feature Scaling (Normalization)](#5) <br\/>\n[7.Machine Learning](#6) <br\/>\n    [7.1.Logistic Regression](#7.1) <br\/>\n    [7.2.Support Vector Machine](#7.2) <br\/>\n    [7.3.K-Nearest Neighbor](#7.3) <br\/>\n    [7.4.Random Forest](#7.4) <br\/>\n    [7.5.Gradient Boosting](#7.5) <br\/>\n    [7.6.Ridge Classifier](#7.6) <br\/>\n[8.Evaluation of Acuracy](#9) <br\/>\n[9.Discussion](#10) <br\/>","a27d4609":"<a id=\"10\"><\/a>\n# Discussion\n- Using the KBest approach with $Chi^{2}$ score and correlation matrices can be comparable for feature selection for this data set.\n- However, both should be compared as when higher numbers of features are introduced there may be less overlap between both methods\n- GridSearchCV allows for quick paramter tuning for models to find a best fit approach for every alogirthm used.\n- All the models used here performed ~90% testing accuracy\n- If this notebook is helpful please provide an upvote and comment!","c84e1672":"Looking at the data it appears that there is no missing values and that they are all numeric.","1645574a":"Looking at the error helps decide the best K-Value given the parameters. The lower the error at K the better accuracy there will be.","932ef436":"<a id=\"7.3\"><\/a>\n## K-Nearest Neighbor\nFirst we need to select the best value of K for the highest accuracy in the model.","6a66bfbf":"<a id=\"7.6\"><\/a>\n## Ridge Classifier\nBy using GRIDSearchCV the best parameters will be decided for the model."}}