{"cell_type":{"7ac630b8":"code","2b7edda5":"code","d6d6e7dd":"code","7154b680":"code","d7ed1a73":"code","31d28697":"code","fec6806e":"code","899ffd4c":"code","7ad5c37c":"code","87bc69b3":"code","97602810":"code","20ba780d":"code","5314173e":"code","53a265b8":"code","349bd40b":"code","c91f68d9":"code","ecb44d53":"code","c7e5a4c7":"code","2504fc76":"code","e9f2bff1":"code","da339f37":"code","c7df2f71":"code","7de442bc":"code","4223e0c9":"code","7c6912b1":"code","3c9c1415":"code","cbf0e4c2":"code","43cc55f1":"code","7179dd5e":"code","b86eb0f0":"code","119a99ad":"code","c2363307":"code","c05a16ff":"code","4c9445f1":"code","90743961":"code","e2e28aff":"code","542139c0":"code","913430b7":"code","bc9c0997":"code","bfa976f7":"code","9626679f":"code","a6ff493c":"code","ceac0c40":"code","db30d8c4":"code","5c8621bb":"code","f87e344f":"code","4fc335b2":"code","7a7f8972":"code","09723e40":"code","db96b06c":"code","8389890f":"code","71d8d224":"code","33788b3d":"code","83bab00a":"code","3187ea57":"code","e4b4ab30":"code","23c0cb25":"code","6d415cb5":"code","a0f575b6":"code","39fc5ba4":"code","076c422e":"code","7f986452":"code","ca518cae":"code","b9cb5391":"code","7b6da231":"code","535470ed":"code","fabc0119":"code","188b4e89":"code","314aaec9":"code","7bf618e4":"markdown","fb1898de":"markdown","6990b417":"markdown","8934e4ae":"markdown","3fbc210d":"markdown","7b0498f0":"markdown","3e105914":"markdown","ad28ce86":"markdown","51491976":"markdown","180a5537":"markdown","a4dd30fc":"markdown","31052779":"markdown","cfe722a4":"markdown","a8e657e4":"markdown","29ec987b":"markdown","9a99db12":"markdown","ccfcf7ce":"markdown","b991ad3e":"markdown","2b076ac7":"markdown","98a9ea0c":"markdown","c12eb224":"markdown","954be437":"markdown","bf0bc87f":"markdown","b6e148a9":"markdown","9cd5a2e1":"markdown","075924ed":"markdown","7e36ed99":"markdown","6edd2ace":"markdown","36454b2d":"markdown","b01ceaaf":"markdown","4577f44c":"markdown","528a75f8":"markdown","35897b4f":"markdown","f82b1e38":"markdown","96e5932d":"markdown","add40ed8":"markdown","6119f80d":"markdown","c1cfe97e":"markdown","b21c5137":"markdown","f6da8d7d":"markdown","ca5e43d2":"markdown","5a080fdb":"markdown","ca969a98":"markdown","f7142bd8":"markdown","caeb318e":"markdown","2c0f1ea8":"markdown","88a579aa":"markdown","364da53a":"markdown","be03ad2c":"markdown","f09f431c":"markdown","d6a207b3":"markdown","93aa2896":"markdown","9a41e3a1":"markdown","20d151c2":"markdown","1b168be4":"markdown","4b74f2de":"markdown","627085c9":"markdown","6265f0fb":"markdown","12546f3d":"markdown","b277a4a4":"markdown","3eb99a90":"markdown"},"source":{"7ac630b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2b7edda5":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","d6d6e7dd":"data=pd.read_excel('\/kaggle\/input\/Admission.xlsx')","7154b680":"data.head()","d7ed1a73":"data.info()","31d28697":"print('Categorical Columns are ','\\n',list(data.select_dtypes(include='object')),'\\n')\nprint('Numerical Columns are ','\\n',list(data.select_dtypes(exclude='object')))","fec6806e":"data.isnull().mean()*100","899ffd4c":"data.nunique()","7ad5c37c":"data.describe(include='object').T","87bc69b3":"data.describe().T","97602810":"data.isnull().mean()*100","20ba780d":"data.plot(kind='box',layout=(3,4),subplots=1,figsize=(10,8))\nplt.show()","5314173e":"sns.distplot(data['Salary'])\nprint('Skewness of Y- Variable is ',data['Salary'].skew())","53a265b8":"data['Entrance_Test'].value_counts()","349bd40b":"data['Entrance_Test'].isnull().sum()","c91f68d9":"data['Entrance_Test']=data['Entrance_Test'].fillna('No-Exam')","ecb44d53":"plt.figure(figsize=(10,6))\nsns.heatmap(data.corr(),cmap='magma',annot=True)\nplt.show()","c7e5a4c7":"data.corr()['Salary'] # To get exact values of correlation by X-Variables on Y-Variable.","2504fc76":"for i in data.columns:\n    sns.scatterplot(data[i],data['Salary'],color='black')\n    plt.show()","e9f2bff1":"from sklearn.model_selection import train_test_split #Importing the library","da339f37":"x=data.drop('Salary',axis=1)\ny=data['Salary']","c7df2f71":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=43)","7de442bc":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","4223e0c9":"data[data['Placement']!='Placed']['Salary'].value_counts() # Verifying is there any mismatch between Placement and Salary.","7c6912b1":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()","3c9c1415":"data.select_dtypes(include='object').head()","cbf0e4c2":"from scipy.stats import zscore #or we can use Standard Scalar, since the degrees of freedom is zero in this case we can use either of them.","43cc55f1":"x=pd.concat([x.select_dtypes(include='object').apply(le.fit_transform),(x.select_dtypes(exclude='object').apply(zscore))],axis=1)\nx.head()","7179dd5e":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=43) #Splitting the data","b86eb0f0":"from sklearn.linear_model import LinearRegression # Importing the package for Linear Regression\nlr=LinearRegression()","119a99ad":"lr.fit(x_train,y_train)","c2363307":"y_pred=lr.predict(x_test)","c05a16ff":"from sklearn.metrics import r2_score,mean_squared_error,mean_absolute_error","4c9445f1":"print('R2-Score ',r2_score(y_test,y_pred))\nprint('RMSE  ',np.sqrt(mean_squared_error(y_test,y_pred)))","90743961":"x_test['Predicted Salary']=y_pred # Adding the predicted value to the dataframe (Test Data Set)","e2e28aff":"x_test.head()","542139c0":"from sklearn.feature_selection import RFE","913430b7":"#no of features\nnof_list=np.arange(1,19)            \nhigh_score=0\n#Variable to store the optimum features\nnof=0          \nscore_list =[]\nfor n in range(len(nof_list)):\n    X_train, X_test, y_train, y_test = train_test_split(x,y, test_size = 0.3, random_state = 43)\n    model = LinearRegression()\n    rfe = RFE(model,nof_list[n])\n    X_train_rfe = rfe.fit_transform(X_train,y_train)\n    X_test_rfe = rfe.transform(X_test)\n    model.fit(X_train_rfe,y_train)\n    score = model.score(X_test_rfe,y_test)\n    score_list.append(score)\n    if(score>high_score):\n        high_score = score\n        nof = nof_list[n]\nprint(\"Optimum number of features: %d\" %nof)\nprint(\"Score with %d features: %f\" % (nof, high_score))\nrfe = RFE(model,10)\nrfe.fit_transform(X_train,y_train)\ny_pred = rfe.predict(X_test)\n\nr2_score(y_test,y_pred)\npd.DataFrame(rfe.support_,index=x.columns)","bc9c0997":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif[\"VIF Values\"]=[variance_inflation_factor(x.values, col) \n                   for col in range(0, x.shape[1])]\nvif.index=x.columns\nvif.T","bfa976f7":"from statsmodels.stats.api import het_goldfeldquandt\nhet_goldfeldquandt(y_train, x_train)","9626679f":"import statsmodels.api as sm","a6ff493c":"xc=sm.add_constant(x_train)","ceac0c40":"ols=sm.OLS(y_train,xc).fit()","db30d8c4":"ols.summary()","5c8621bb":"from statsmodels.stats.stattools import durbin_watson\ndurbin_watson(ols.resid)","f87e344f":"import statsmodels.tsa.api as smt\n\nacf = smt.graphics.plot_acf(ols.resid, lags=40 , alpha=0.05)\nacf.show()","4fc335b2":"from statsmodels.stats.diagnostic import linear_rainbow","7a7f8972":"linear_rainbow(ols,frac=0.5)","09723e40":"x=data.drop('Salary',axis=1)\ny=data['Salary']\nx=pd.concat([x.select_dtypes(include='object').apply(le.fit_transform),(x.select_dtypes(exclude='object').apply(zscore))],axis=1)","db96b06c":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=43) #Re-splitting the data since \n                                        #there was an addition of prediction column to the table for the previous question.","8389890f":"from sklearn.tree import DecisionTreeRegressor\ndt=DecisionTreeRegressor()","71d8d224":"dt.fit(x_train,y_train)","33788b3d":"y_pred=dt.predict(x_test)","83bab00a":"r2_score(y_test,y_pred)","3187ea57":"from sklearn.ensemble import RandomForestRegressor\nrf=RandomForestRegressor()","e4b4ab30":"rf.fit(x_train,y_train)","23c0cb25":"y_pred=rf.predict(x_test)","6d415cb5":"r2_score(y_test,y_pred)","a0f575b6":"from sklearn.model_selection import RandomizedSearchCV","39fc5ba4":"rf=RandomForestRegressor()","076c422e":"parameters={'n_estimators':range(1,50),'min_samples_split':range(2,10),'min_samples_leaf':range(2,10)}","7f986452":"rand=RandomizedSearchCV(estimator=rf,param_distributions=parameters,scoring='r2',cv=5,return_train_score=True)","ca518cae":"rand.fit(x_train,y_train)","b9cb5391":"y_pred=rand.predict(x_test)","7b6da231":"r2_score(y_test,y_pred)","535470ed":"pd.DataFrame(rand.cv_results_).sort_values(by='param_n_estimators').set_index('param_n_estimators')['mean_test_score'].plot.line()\npd.DataFrame(rand.cv_results_).sort_values(by='param_n_estimators').set_index('param_n_estimators')['mean_train_score'].plot.line()","fabc0119":"rf.fit(x_train,y_train)\npd.DataFrame(rf.feature_importances_,index=x.columns).sort_values(by=0,ascending=False)","188b4e89":"pd.DataFrame(rf.feature_importances_,index=x.columns).sort_values(by=0,ascending=False).head()","314aaec9":"pd.DataFrame(rf.feature_importances_,index=x.columns).sort_values(by=0,ascending=False).head().plot.bar()","7bf618e4":"#### 2. Check VIF for Multi-Collinearity ","fb1898de":"#### From these values we can say that the r2-score in Decision Tree is much lower and R2- Score in Random Forest is pretty high AS COMPARED TO decision tree. This is a clear indication of over ftting in decision tree model construction and lowering of variance by taking the average of different trees helps to reduce over fitting in ensemble modeling. ","6990b417":"#### Risk in interpretation is because the data is a small portion of the data, the data should be statistically verified with the help of statistical value in order to get full-confidence on the data prediction. Time was a huge constrain for the prediction and the analysis for the dataset. Over all- It can be considered for a rough estimation for the salary.","8934e4ae":"* I got a R-square value of '0.7412', which can be further improved using different methods.","3fbc210d":"#### Base models were often had a lot of assumptions about the data distribution, in this problem the assumptions are not completely fulfilled. Thus I moved forward for a different higher model for better prediction.","7b0498f0":"* Splitting of the data is of the specific propotion.","3e105914":"#### 5. Check the linearity of the data","ad28ce86":"### Random Forest Model : Regression","51491976":"#### 1. Applying RFE to get necessary feature list","180a5537":"* Info gives a rough idea about the data types of the data which is provided. The data provided can be cleaned data or uncleaned data. In this case most of the data types are actually matching with the variables. In some cases the numerical digits can be written as string variable.","a4dd30fc":"#### 4. Check for Auto-Correlation and Other analysis using OLS Method.","31052779":"#### Check for normality of the Y-Variable.","cfe722a4":"* We can proceed further with Hyper Parametric Tuning for Better Accuracy Expectations.","a8e657e4":"#### The scatter plot gives a good insight about the dependancy of salary over different other variables in the data frame. They are not actually linear thus we cannot do a descriptive statistics for the studies but the inferential statistics will be useful for the prediction of salary by using all the relevant variables provided.","29ec987b":"* Since most of the values are lying in nearest to each other values, interpretation of the result may end up in wromg conclusion. Thus I proceed without dropping the variables.","9a99db12":"#### To check for the association between variables, we can consider scatter plotting for the visualisation.","ccfcf7ce":"* Auto corrrelation - As Durbin-Watson is 2.135  no serial correlation (Since the value is near to 2)\n* Jarque-Bera (JB):  Value is considerably high so it is not-normaly distributed ","b991ad3e":"* From the visualisation we can say the highest correlation is with Percent_SSC with correlation of 0.22.","2b076ac7":"* There are variables which are categorical, to fit in the models we should convert the data into numerical values.","98a9ea0c":"* We have 67 values which are missing in entrance test variable, from the data distribution we can say that the most occuring value(Mode) is MAT. But I assume that the students with null values in the entrance test is the students who did not take any entrance test for getting admission for MBA, So I treat these values are the students who did not take any examination for MBA admission.","c12eb224":"### 3. Check for defects in the data. Perform necessary actions to \u2018fix\u2019 these defects ","954be437":"* This visualisation will help for understanding the outliers. From the graph it is clear that a few variables has outliers but since the respective parameter like exerience or salary is a variable which can have outliers in this range. Experience has a outlier of 3, which is acceptable. So, I would proceed without any outlier treatment.","bf0bc87f":"* So, in a business problem, the data interpretation can be based on these 5 initial highly important parameters.","b6e148a9":"> Key Points :\n    >> * I used random forest for my final model presentation, even though decision tree gives a good result, the variance will be high and the complexity will be high for a small dataset like this. <br\/>\n    >> * I used Randomized Search CV  due to computational limitation over Grid Search CV, even though Grid Search CV gives better result by giving equal weightage to all the data point. <br\/>","9cd5a2e1":"* Special Note : In case the in question if it is mandatory to use a outlier treatment in that case we can cap the variable outliers using lambda function. In this case outlier treatment will drop the variance in the data, which will end up resulting in a less accurate model.","075924ed":"#### Null Value Treatment : ","7e36ed99":"As mentioned in the previous question we have 17.13% null values in Entrance Test variable and the other values are not having any null values.","6edd2ace":"### 1. Read the dataset (tab, csv, xls, txt, inbuilt dataset)","36454b2d":"- We have null values in Entrance Test column.","b01ceaaf":"* From these unique value numbers we can say, the Sl.No is a continues variable since it matches with the shape of the data set. Gender has two unique values. We have 7 types of course degrees and 8 type od entrance tests, 3 types of specialization and so on. To know further we can proceed with each of those unique values.","4577f44c":"* We hace 17.13% of missing values in entrance test variable with respect to its total number of values. The rest of the variables do not have any missing values in primary verification.","528a75f8":"* Since we have to local maximas for the y-variable and the skewness is 0.24, we can say that the variable doesn't follow normal distribution. For statistical verification we can check shapiro test for more accurate result.","35897b4f":"* This is the statistical understanding about the numerical values in the dataset provided (Without any cleaning)","f82b1e38":"#### We cannot say whether the R-Value is good enough or not since the value is not standardised with respect to the number of variables. We have to consider the adjusted value which we can further obtain from OLS method.","96e5932d":"# END","add40ed8":"## Q8.Summarize as follows:","6119f80d":"#### Since the compuational power is less, I try with Randomised CV Search.","c1cfe97e":"### Q5.Split dataset into train and test (70:30) ","b21c5137":"#### Since the R2-Value is not good enough we have to consider other methods to improve it.","f6da8d7d":">Points to be noted :\n    >> * The assumption tests for the base model fails in this problem.<br\/>\n    >> * In this case we have to go for a slightly advanced model calculation. <br\/>\n    >> * I would like to proceed further with one tree model and one ensemble modeling. (Decision Tree and Random Forest (To avoid over fitting caused by decision tree) <br\/>\n    >> * The accuracy of base line model is not upto the expectation, thus proceed for slightly complex model and then tune hyper parameters accordingly","ca5e43d2":"* To check for the relationship we can consider heatmap of correlation matrix.","5a080fdb":"* These are the variables which has to be converted.","ca969a98":"### Q6. Fit a base model. Please write your key observations ","f7142bd8":"* Variables are in different scales thus we should scale them for acqiring better insights out of the model prediction.","caeb318e":"#### 3. Check for Hetroscedacity. ","2c0f1ea8":"### 4. Summarize relationships among variables.","88a579aa":"* From RFE, We can say that the optimum number of features we can use is 7, if that is the case, we can get a higher accuracy score.","364da53a":"#### To check the nature and charectristics of the splitting values :","be03ad2c":"* Lower models, the bias was higher due to low variance, higher model optimisation helped for better treatment of this bias-variance trade off in the model.","f09f431c":"### Decision Tree Model : Regression","d6a207b3":"* P-Value is less than alpha(0.05-Assumed) thus we reject null hypothesis, thus the value follow uniform variance in this case.","93aa2896":"* We have no null values in the numerical variables. On a vague note from the data structure we can see that there are no major outliers in the data. But we have to check further with the box plot or any other visualisation method.","9a41e3a1":"* In this linearity check(Rainbow-Test), the P-Value is approx. zero which is less than alpha value thus we reject null hypothesis.","20d151c2":"#### Hypothesis Assumption\n* H0 : The data is homoscedastic\n* H1 : The data is hetroscedastic","1b168be4":"### Qn.7 How do you improve the accuracy of the model? Write clearly the changes that you will make before re-fitting the model. Fit the final model. ","4b74f2de":"#### We have 8 variables which are categorical and the rest are taken as numerical values. The data structure is having 391 values among which in the case of entrance test variable we have 324 entries and the rest are null values.","627085c9":"* From these values and the visaulisation we can say Percent_Degree and Percent_HSC are some variables which shows very less correlation with the y-variable. So, we can drop them. Since the size of the dataset is critical in this stage and the confirmation of no-relation is not yet proved. I assume that they can have some impacts on the dependent variable and keeping them intact for the time being.","6265f0fb":"### 2. Summarize important observations from the data set ","12546f3d":"* This information gives an idea about the distributions on each categorical variables.","b277a4a4":"* The data seems valid, there is no recorded data about salary of a student who didn't get placed.","3eb99a90":"* In the final call, it is clearly seen that the most efficient and useful parameters for the determination of salary will be of as follow :"}}