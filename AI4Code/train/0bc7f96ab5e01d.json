{"cell_type":{"73bf7758":"code","f5c0df79":"code","8e1ffc50":"code","05a40677":"code","018307a2":"code","74536375":"code","eece44ab":"code","063cbd76":"code","7d22a999":"code","bc562989":"code","5270dc47":"code","de7ed326":"code","f9e3a5f9":"code","6b4912b9":"code","e34b6364":"code","3ef6c1d5":"code","ba5a9efe":"code","fea516b7":"code","5ae3f71d":"code","d09ac59a":"code","213caafd":"code","d98525e8":"code","e82fcdd9":"code","87e415ac":"code","04dd14d9":"code","9de941c6":"code","9ea758f9":"code","93b67546":"code","0b0057f5":"code","3c8354e5":"code","71496923":"code","69c7a006":"code","94a92bfe":"code","c1314ab9":"code","ca5155e9":"markdown","68d9033b":"markdown","c5b2f015":"markdown","ea59cec5":"markdown","d2e25708":"markdown","994cafd0":"markdown","41c536cc":"markdown","fbedcec8":"markdown","772f8593":"markdown","ddc71b44":"markdown","1435ee1a":"markdown","729ee622":"markdown","843ac15b":"markdown","8d2f6380":"markdown","28558733":"markdown","c26f6bf7":"markdown","8f981a06":"markdown","afbaea2f":"markdown","c982c5a6":"markdown","3caadc28":"markdown","0fb7fff0":"markdown","b051ee6f":"markdown","bf3d1b1b":"markdown","e7ae5b47":"markdown","8d8b0fa5":"markdown","5ca26d73":"markdown","5f01cb6a":"markdown","240d5021":"markdown","905fd1a1":"markdown","92408ff2":"markdown","b9480a26":"markdown","682015d9":"markdown","3c89e688":"markdown","6a600678":"markdown","4fea4999":"markdown"},"source":{"73bf7758":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport string\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\npyo.init_notebook_mode()\nfrom sklearn.decomposition import TruncatedSVD,PCA\nfrom sklearn.feature_extraction.text import CountVectorizer\nnltk.download('vader_lexicon')\nfrom sklearn.cluster import KMeans\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\nfrom wordcloud import WordCloud,STOPWORDS\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.graphics.tsaplots import plot_acf\nfrom statsmodels.graphics.tsaplots import plot_pacf\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom nltk.util import ngrams\nfrom nltk import word_tokenize\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport random\nplt.rc('figure',figsize=(17,13))\n","f5c0df79":"f_data = pd.read_csv('\/kaggle\/input\/pfizer-vaccine-tweets\/vaccination_tweets.csv')\nf_data.head(3)","8e1ffc50":"#f_data.text =f_data.text.str.lower()\n\n#Remove twitter handlers\nf_data.text = f_data.text.apply(lambda x:re.sub('@[^\\s]+','',x))\n\n#remove hashtags\nf_data.text = f_data.text.apply(lambda x:re.sub(r'\\B#\\S+','',x))\n\n\n# Remove URLS\nf_data.text = f_data.text.apply(lambda x:re.sub(r\"http\\S+\", \"\", x))\n\n# Remove all the special characters\nf_data.text = f_data.text.apply(lambda x:' '.join(re.findall(r'\\w+', x)))\n\n#remove all single characters\nf_data.text = f_data.text.apply(lambda x:re.sub(r'\\s+[a-zA-Z]\\s+', '', x))\n\n# Substituting multiple spaces with single space\nf_data.text = f_data.text.apply(lambda x:re.sub(r'\\s+', ' ', x, flags=re.I))\n","05a40677":"sid = SIA()\nf_data['sentiments']           = f_data['text'].apply(lambda x: sid.polarity_scores(' '.join(re.findall(r'\\w+',x.lower()))))\nf_data['Positive Sentiment']   = f_data['sentiments'].apply(lambda x: x['pos']+1*(10**-6)) \nf_data['Neutral Sentiment']    = f_data['sentiments'].apply(lambda x: x['neu']+1*(10**-6))\nf_data['Negative Sentiment']   = f_data['sentiments'].apply(lambda x: x['neg']+1*(10**-6))\n\nf_data.drop(columns=['sentiments'],inplace=True)","018307a2":"#Number of Words\nf_data['Number_Of_Words'] = f_data.text.apply(lambda x:len(x.split(' ')))\n#Average Word Length\nf_data['Mean_Word_Length'] = f_data.text.apply(lambda x:np.round(np.mean([len(w) for w in x.split(' ')]),2) )","74536375":"plt.subplot(2,1,1)\nplt.title('Distriubtion Of Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\nsns.kdeplot(f_data['Negative Sentiment'],bw=0.1)\nsns.kdeplot(f_data['Positive Sentiment'],bw=0.1)\nsns.kdeplot(f_data['Neutral Sentiment'],bw=0.1)\nplt.subplot(2,1,2)\nplt.title('CDF Of Sentiments Across Our Tweets',fontsize=19,fontweight='bold')\nsns.kdeplot(f_data['Negative Sentiment'],bw=0.1,cumulative=True)\nsns.kdeplot(f_data['Positive Sentiment'],bw=0.1,cumulative=True)\nsns.kdeplot(f_data['Neutral Sentiment'],bw=0.1,cumulative=True)\nplt.xlabel('Sentiment Value',fontsize=19)\nplt.show()","eece44ab":"#Sorting And Feature Engineering\nf_data = f_data.sort_values(by='date')\nft_data=f_data.copy()\nft_data['date'] = pd.to_datetime(f_data['date']).dt.date\n\nft_data['year']         = pd.DatetimeIndex(ft_data['date']).year\nft_data['month']        = pd.DatetimeIndex(ft_data['date']).month\nft_data['day']          = pd.DatetimeIndex(ft_data['date']).day\nft_data['day_of_year']  = pd.DatetimeIndex(ft_data['date']).dayofyear\nft_data['quarter']      = pd.DatetimeIndex(ft_data['date']).quarter\nft_data['season']       = ft_data.month%12 \/\/ 3 + 1","063cbd76":"f_data=f_data.reset_index().drop(columns=['index'])\npartitions = []\npartitions.append(f_data.loc[44:np.round(len(f_data)\/3,0)-1,:])\npartitions.append(f_data.loc[np.round(len(f_data)\/3,0):2*int(len(f_data)\/3)-1,:])\npartitions.append(f_data.loc[2*np.round(len(f_data)\/3,0):3*int(len(f_data)\/3)-1,:])\n\n\n\nneg_part_means =[]\nneg_part_std   =[]\npos_part_means =[]\npos_part_std   =[]\nfor part in partitions:\n    neg_part_means.append(part['Negative Sentiment'].mean())\n    neg_part_std.append(part['Negative Sentiment'].std())\n    pos_part_means.append(part['Positive Sentiment'].mean())\n    pos_part_std.append(part['Positive Sentiment'].std())\n    \nres_df = pd.DataFrame({'Positive Sentiment Mean':pos_part_means,'Negative Sentiment Mean':neg_part_means,'Positive Sentiment SD':pos_part_std,'Negative Sentiment SD':neg_part_std},\n                     index = [f'Partition_{i}' for i in range(1,4)])\n\n\ndef highlight_greater(x):\n    temp = x.copy()\n    temp = temp.round(0).astype(int)\n    m1 = (temp['Partition_1_Mean'] == temp['Partition_2_Mean'])\n    m2 = (temp['Partition_1_SD'] == temp['Partition_2_SD'])\n    m3 = (temp['Partition_1_Mean'] < temp['Partition_2_Mean']+3) & (temp['Partition_1_Mean'] > temp['Partition_2_Mean']-3)\n    m4 = (temp['Partition_1_SD'] < temp['Partition_2_SD']+3) & (temp['Partition_1_SD'] > temp['Partition_2_SD']-3)\n\n    df1 = pd.DataFrame('background-color: ', index=x.index, columns=x.columns)\n    #rewrite values by boolean masks\n    df1['Partition_1_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(~m1, 'background-color: {}'.format('salmon'),        df1['Partition_2_Mean'])\n    df1['Partition_1_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(m3, 'background-color: {}'.format('gold'),           df1['Partition_2_Mean'])\n    df1['Partition_1_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_Mean'])\n    df1['Partition_2_Mean'] = np.where(m1, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_Mean'])\n\n    df1['Partition_1_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(~m2, 'background-color: {}'.format('salmon'),        df1['Partition_2_SD'])\n    df1['Partition_1_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(m4, 'background-color: {}'.format('gold'),           df1['Partition_2_SD'])\n    df1['Partition_1_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_1_SD'])\n    df1['Partition_2_SD'] = np.where(m2, 'background-color: {}'.format('mediumseagreen'), df1['Partition_2_SD'])\n\n    return df1\n\n\n\n#res_df.style.apply(highlight_greater,axis=None)\nres_df = res_df.T\nres_df = pd.DataFrame(res_df.values,columns=res_df.columns,index=['Positive Sentiment','Negative Sentiment','Positive Sentiment','Negative Sentiment'])\nres_df = pd.concat([res_df.iloc[:2,:],res_df.iloc[2:,:]],axis=1)\nres_df.columns = ['Partition_1_Mean','Partition_2_Mean','Partition_3_Mean','Partition_1_SD','Partition_2_SD','Partition_3_SD']\nres_df.style.apply(highlight_greater,axis=None)","7d22a999":"fig = make_subplots(rows=3, cols=2)\n\nfor idx,prt in enumerate(partitions):\n    fig.add_trace(\n    go.Scatter(x=prt['date'], y=prt['Positive Sentiment'],name=f'Positive Part {idx+1}'),\n    row=idx+1, col=1)\n    fig.add_trace(\n    go.Scatter(x=prt['date'], y=prt['Negative Sentiment'],name=f'Negative Part {idx+1}'),\n    row=idx+1, col=2)\n\nfig.update_layout(height=600, width=900, title_text=\"Distibution Of Daily Sentiments Over Our Time Line For Each Partition\")\nfig.show()","bc562989":"fig = make_subplots(rows=4, cols=2, subplot_titles=('Observed Pos', 'Observed Neg', 'Trend Pos','Trend Neg','Seasonal Pos','Seasonal Neg','Residual Pos','Residual Neg'))\nb_date_mean = ft_data.groupby(by='date').mean().reset_index()\n\nlbl = ['Positive','Negative']\n\nfor idx,column in enumerate(['Positive Sentiment','Negative Sentiment']):\n    res = seasonal_decompose(b_date_mean[column], period=5, model='additive', extrapolate_trend='freq')\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.observed)), y=res.observed,name='{} Observed'.format(lbl[idx])),\n    row=1, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.trend)), y=res.trend,name='{} Trend'.format(lbl[idx])),\n    row=2, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.seasonal)), y=res.seasonal,name='{} Seasonal'.format(lbl[idx])),\n    row=3, col=idx+1)\n    \n    fig.add_trace(\n    go.Scatter(x=np.arange(0,len(res.resid)), y=res.resid,name='{} Residual'.format(lbl[idx])),\n    row=4, col=idx+1)\n            \nfig.update_layout(height=600, width=900, title_text=\"Decomposition Of Our Sentiments into Trend,Level,Seasonality and Residuals\")\nfig.show()","5270dc47":"f, ax = plt.subplots(nrows=2, ncols=1, figsize=(16, 10))\n\nax[0].set_title('Positive Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(b_date_mean['Positive Sentiment'],ax=ax[0],lw=3)\nax[1].set_title('Negative Autocorrelation Analysis ',fontsize=18,fontweight='bold')\nautocorrelation_plot(b_date_mean['Negative Sentiment'],ax=ax[1],color='tab:red',lw=3)\nplt.show()","de7ed326":"f, ax = plt.subplots(nrows=2, ncols=2, figsize=(16, 10))\nax[0,0].set_ylim(-1.1,1.1)\nax[1,0].set_ylim(-1.1,1.1)\nax[0,1].set_ylim(-1.1,1.1)\nax[1,1].set_ylim(-1.1,1.1)\n\nplot_acf(b_date_mean['Negative Sentiment'],lags=20, ax=ax[0,0],title='Autocorrelation Negative')\nplot_pacf(b_date_mean['Negative Sentiment'],lags=20, ax=ax[1,0],title='Partial Autocorrelation Negative')\nplot_acf(b_date_mean['Positive Sentiment'],lags=20, ax=ax[0,1],color='tab:red',title='Autocorrelation Positive')\nplot_pacf(b_date_mean['Positive Sentiment'],lags=20, ax=ax[1,1],color='tab:red',title='Partial Autocorrelation Positive')\nplt.show()","f9e3a5f9":"b_date_mean = ft_data.groupby(by='date').mean().reset_index()\nb_date_std = ft_data.groupby(by='date').std().reset_index()\n\n\nfig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Daily Average Positive Sentiment',  'Daily Average Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['date'], y=b_date_mean['Positive Sentiment'],name='Positive Sentiment Mean'),\n    row=1, col=1\n)\n\n    \n#positive mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['date'].values[0], y0=b_date_mean['Positive Sentiment'].mean(), x1=b_date_mean['date'].values[-1], y1=b_date_mean['Positive Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean'\n)\n\nfig.add_annotation(x=b_date_mean['date'].values[3], y=b_date_mean['Positive Sentiment'].mean(),\n            text=r\"$\\mu : {:.2f}$\".format(b_date_mean['Positive Sentiment'].mean()),\n            showarrow=True,\n            arrowhead=3,\n            yshift=10)\n\n\n\nfig.add_trace(\n    go.Scatter(x=b_date_mean['date'], y=b_date_mean['Negative Sentiment'],name='Negative Sentiment Mean'),\n    row=2, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_mean['date'].values[0], y0=b_date_mean['Negative Sentiment'].mean(), x1=b_date_mean['date'].values[-1], y1=b_date_mean['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\nfig.add_annotation(x=b_date_mean['date'].values[3], y=b_date_mean['Negative Sentiment'].mean(),\n            text=r\"$\\mu : {:.2f}$\".format(b_date_mean['Negative Sentiment'].mean()),\n            showarrow=True,\n            arrowhead=3,\n            yshift=10,\n            xref='x2', \n            yref='y2')\n\n\n\nfig.add_annotation(x=b_date_mean['date'].values[5], y=b_date_mean['Negative Sentiment'].mean()+0.01,\n            text=r\"Start Of Decline\",\n            showarrow=True,\n            arrowhead=6,\n            yshift=10,\n            xref='x2', \n            yref='y2')\n\nfig.add_annotation(x=b_date_mean['date'].values[15], y=.024,\n            text=r\"Start Of Incline\",\n            showarrow=True,\n            arrowhead=6,\n            yshift=10,\n            xref='x2', \n            yref='y2')\n\nfig['layout']['xaxis2']['title'] = 'Date'\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Average Change With Time\")\nfig.show()\n","6b4912b9":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Daily Deviation in Positive Sentiment',  'Daily Deviation in Negative Sentiment'))\n\nfig.add_trace(\n    go.Scatter(x=b_date_std['date'], y=b_date_std['Positive Sentiment'],name='Positive Sentiment SD'),\n    row=1, col=1\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_std['date'].values[0], y0=b_date_std['Negative Sentiment'].mean(), x1=b_date_std['date'].values[-1], y1=b_date_std['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x2', \n        yref='y2'\n)\n\n#negative mean\nfig.add_shape(type=\"line\",\n    x0=b_date_std['date'].values[0], y0=b_date_std['Positive Sentiment'].mean(), x1=b_date_std['date'].values[-1], y1=b_date_std['Positive Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n        xref='x1', \n        yref='y1'\n)\n\n\nfig.add_trace(\n    go.Scatter(x=b_date_std['date'], y=b_date_std['Negative Sentiment'],name='Negative Sentiment SD'),\n    row=2, col=1\n)\n\nfig['layout']['xaxis2']['title'] = 'Date'\nfig.update_layout(height=700, width=900, title_text=\"Sentiment Deviation Change With Time\")\nfig.show()\n","e34b6364":"import datetime\nb_date_count = ft_data.groupby(by='date').count().reset_index()\nb_date_count = b_date_count.rename(columns={'id':'Tweets Per Day'})\nfig = ex.line(b_date_count,x='date',y='Tweets Per Day')\n\n# fig.add_annotation(x=b_date_mean['date'].values[15], y=.024,\n#             text=r\"Start Of Incline\",\n#             showarrow=True,\n#             arrowhead=6,\n#             yshift=10)\n\n\nfig.add_shape(type=\"line\",\n    x0=b_date_count['date'].values[0], y0=b_date_count['Negative Sentiment'].mean(), x1=b_date_count['date'].values[-1], y1=b_date_count['Negative Sentiment'].mean(),\n    line=dict(\n        color=\"Red\",\n        width=2,\n        dash=\"dashdot\",\n    ),\n        name='Mean',\n)\n\nfig.update_traces(mode=\"markers+lines\")\nfig.update_layout(hovermode=\"x unified\")\n\n\n###annots\nb_date_count.date = pd.to_datetime(b_date_count.date)\nb_date_count_dt = b_date_count.set_index('date')\nfig.add_annotation(x=datetime.datetime(2021,2,19), y=b_date_count_dt.loc[pd.Timestamp('2021-02-19'),'year'],\n            text=r\"Israeli study finds Pfizer vaccine 85% effective after first shot\",\n            showarrow=True,\n            arrowhead=3,\n            yshift=5,bordercolor=\"#c7c7c7\")\n\nfig.add_annotation(x=datetime.datetime(2021,1,29), y=b_date_count_dt.loc[pd.Timestamp('2021-01-29'),'year'],\n            text=r\"vaccine found to be effective against variant discovered in U.K.\",\n            showarrow=True,\n            arrowhead=3,\n            yshift=5,ay=-160,bordercolor=\"#c7c7c7\")\nfig.add_annotation(x=datetime.datetime(2021,1,8), y=b_date_count_dt.loc[pd.Timestamp('2021-01-8'),'year'],\n            text=r\"Commission proposes to purchase up to 300 million additional doses of BioNTech-Pfizer vaccine\",\n            showarrow=True,\n            arrowhead=3,\n            yshift=5,ay=-30,bordercolor=\"#c7c7c7\")\n\nfig.add_annotation(x=datetime.datetime(2021,1,20), y=b_date_count_dt.loc[pd.Timestamp('2021-01-20'),'year'],\n            text=r\"The presidency of Joe Biden began\",\n            showarrow=True,\n            arrowhead=3,\n            yshift=3,ay=120,bordercolor=\"#c7c7c7\")\n\nfig.update_layout(title='<b>Daily Tweet Count<b>',width=1200)\nfig.show()","3ef6c1d5":"import pymc3 as pm\n\nwith pm.Model():\n    alpha = 1\/(b_date_count['Tweets Per Day'].mean())\n    LAMBDA1  = pm.Exponential('lambda1',alpha)\n    LAMBDA2  = pm.Exponential('lambda2',alpha)\n\n    tau = pm.DiscreteUniform('tau',lower=0,upper=len(b_date_count)-1)\n    \n    idx = np.arange(len(b_date_count))\n    LAMBDA_ = pm.math.switch(tau>idx,LAMBDA1,LAMBDA2)\n    \n    obs = pm.Poisson('obs',LAMBDA_,observed=b_date_count['Tweets Per Day'])\n    \n    step = pm.Metropolis()\n    trace = pm.sample(10000,step=step)","ba5a9efe":"#histogram of the samples:\nlambda_1_samples = trace['lambda1'][10000:]\nlambda_2_samples = trace['lambda2'][10000:]\ntau_samples = trace['tau'][10000:]\nn_count_data = len(b_date_count)\ncount_data = b_date_count['Tweets Per Day']\nax = plt.subplot(311)\nax.set_autoscaley_on(False)\n\nplt.hist(lambda_1_samples, histtype='stepfilled', bins=30, alpha=0.85,\n         label=\"posterior of $\\lambda_1$\", color=\"#A60628\", density=True)\nplt.legend(loc=\"upper left\")\nplt.title(r\"\"\"Posterior distributions of the variables\n    $\\lambda_1,\\;\\lambda_2,\\;\\tau$\"\"\")\nplt.xlabel(\"$\\lambda_1$ value\")\n\nax = plt.subplot(312)\nax.set_autoscaley_on(False)\nplt.hist(lambda_2_samples, histtype='stepfilled', bins=30, alpha=0.85,\n         label=\"posterior of $\\lambda_2$\", color=\"#7A68A6\", density=True)\nplt.legend(loc=\"upper left\")\nplt.xlabel(\"$\\lambda_2$ value\")\n\nplt.subplot(313)\nw = 1.0 \/ tau_samples.shape[0] * np.ones_like(tau_samples)\nplt.hist(tau_samples, bins=n_count_data, alpha=1,\n         label=r\"posterior of $\\tau$\",\n         color=\"#467821\", weights=w, rwidth=2.)\n\nplt.legend(loc=\"upper left\")\nplt.xlabel(r\"$\\tau$ (in days)\")\nplt.ylabel(\"probability\");","fea516b7":"plt.subplot(2,1,1)\nplt.title('Selecting A Cut-Off For Most Positive\/Negative Tweets',fontsize=19,fontweight='bold')\n\nax0 = sns.kdeplot(f_data['Negative Sentiment'],bw=0.1)\n\nkde_x, kde_y = ax0.lines[0].get_data()\nax0.fill_between(kde_x, kde_y, where=(kde_x>0.25) , \n                interpolate=True, color='tab:blue',alpha=0.6)\n\nplt.annotate('Cut-Off For Most Negative Tweets', xy=(0.25, 0.5), xytext=(0.4, 2),\n            arrowprops=dict(facecolor='red', shrink=0.05),fontsize=16,fontweight='bold')\n\nax0.axvline(f_data['Negative Sentiment'].mean(), color='r', linestyle='--')\nax0.axvline(f_data['Negative Sentiment'].median(), color='tab:orange', linestyle='-')\nplt.legend({'PDF':f_data['Negative Sentiment'],r'Mean: {:.2f}'.format(f_data['Negative Sentiment'].mean()):f_data['Negative Sentiment'].mean(),\n            r'Median: {:.2f}'.format(f_data['Negative Sentiment'].median()):f_data['Negative Sentiment'].median()})\n\nplt.subplot(2,1,2)\n\nax1 = sns.kdeplot(f_data['Positive Sentiment'],bw=0.1,color='green')\n\nplt.annotate('Cut-Off For Most Positive Tweets', xy=(0.4, 0.43), xytext=(0.4, 2),\n            arrowprops=dict(facecolor='red', shrink=0.05),fontsize=16,fontweight='bold')\nkde_x, kde_y = ax1.lines[0].get_data()\nax1.fill_between(kde_x, kde_y, where=(kde_x>0.4) , \n                interpolate=True, color='tab:green',alpha=0.6)\nax1.set_xlabel('Sentiment Strength',fontsize=18)\n\n\nax1.axvline(f_data['Positive Sentiment'].mean(), color='r', linestyle='--')\nax1.axvline(f_data['Positive Sentiment'].median(), color='tab:orange', linestyle='-')\nplt.legend({'PDF':f_data['Positive Sentiment'],r'Mean: {:.2f}'.format(f_data['Positive Sentiment'].mean()):f_data['Positive Sentiment'].mean(),\n            r'Median: {:.2f}'.format(f_data['Positive Sentiment'].median()):f_data['Positive Sentiment'].median()})\n\nplt.show()","5ae3f71d":"Most_Positive = f_data[f_data['Positive Sentiment'].between(0.4,1)]\nMost_Negative = f_data[f_data['Negative Sentiment'].between(0.25,1)]","d09ac59a":"Most_Positive_text = ' '.join(Most_Positive.text)\nMost_Negative_text = ' '.join(Most_Negative.text)\n\n\npwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(Most_Positive_text)\nnwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(Most_Negative_text)\n\nplt.subplot(1,2,1)\nplt.title('Common Words Among Most Positive Tweets',fontsize=16,fontweight='bold')\nplt.imshow(pwc)\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.title('Common Words Among Most Negative Tweets',fontsize=16,fontweight='bold')\nplt.imshow(nwc)\nplt.axis('off')\n\nplt.show()","213caafd":"l_t = Most_Positive_text\n\nw1_dict = dict()\nfor word in l_t.split():\n    w= word.strip()\n    if w in STOPWORDS:\n        continue\n    else:\n        w1_dict[w] = w1_dict.get(w,0)+1\nw1_dict = {k: v for k, v in sorted(w1_dict.items(), key=lambda item: item[1],reverse=True)}\n\nl_t = Most_Negative_text\nw2_dict = dict()\nfor word in l_t.split():\n    w= word.strip()\n    if w in STOPWORDS:\n        continue\n    else:\n        w2_dict[w] = w2_dict.get(w,0)+1\nw2_dict = {k: v for k, v in sorted(w2_dict.items(), key=lambda item: item[1],reverse=True)}\n\ntop_10_pos = list(w1_dict.keys())[:10]\ntop_10_neg = list(w2_dict.keys())[:10]\n\nplt.subplot(1,2,1)\nw_c = WordCloud(width=600,height=400,collocations = False,colormap='nipy_spectral',background_color='white').generate(' '.join(top_10_pos))\nplt.title('Top 10 Words In Most Positive Tweets',fontsize=19,fontweight='bold')\nplt.imshow(w_c)\nplt.axis('off')\nplt.subplot(1,2,2)\nw_c = WordCloud(width=600,height=400,collocations = False,colormap='nipy_spectral',background_color='white').generate(' '.join(top_10_neg))\nplt.title('Top 10 Words In Most Negative Tweets',fontsize=19,fontweight='bold')\nplt.imshow(w_c)\nplt.axis('off')\nplt.show()","d98525e8":"\ntoken=nltk.word_tokenize(' '.join(f_data.text))\npos_bigram=ngrams(token,2)\npos_bigram_dict = dict()\npos_trigram =ngrams(token,3)\npos_trigram = [k for k in pos_trigram if k[0] in top_10_pos]\n\ntoken=nltk.word_tokenize(' '.join(f_data.text))\nneg_bigram=ngrams(token,2)\nneg_bigram_dict = dict()\nneg_trigram =ngrams(token,3)\nneg_trigram = [k for k in neg_trigram if k[0] in top_10_neg]\n\nfor i in neg_bigram:\n    neg_bigram_dict[i] = neg_bigram_dict.get(i,0)+1\nfor i in pos_bigram:\n    pos_bigram_dict[i] = pos_bigram_dict.get(i,0)+1\n        \npos_trigram_dict = dict()\nneg_trigram_dict = dict()\n\nfor i in pos_trigram:\n    pos_trigram_dict[i] = pos_trigram_dict.get(i,0)+1\n    \nfor i in neg_trigram:\n    neg_trigram_dict[i] = neg_trigram_dict.get(i,0)+1\n \n\npos_trigram_df = pd.DataFrame(random.sample(list(pos_trigram_dict.keys()),k=15),columns=['One Of Top 10 Words','Second Word','Third Word'])\n\ndef get_prob(sir):\n    key = (sir['One Of Top 10 Words'],sir['Second Word'],sir['Third Word'])\n    w3 = pos_trigram_dict[key]\n    w2 = pos_bigram_dict[(sir['One Of Top 10 Words'],sir['Second Word'])]\n    return w3\/w2\n\npos_trigram_df['Probabilty Of Sentence'] = pos_trigram_df.apply(get_prob,axis=1)\n\npos_trigram_df.style.background_gradient(subset='Probabilty Of Sentence',cmap='vlag')","e82fcdd9":"neg_trigram_df = pd.DataFrame(random.sample(list(neg_trigram_dict.keys()),k=15),columns=['One Of Top 10 Words','Second Word','Third Word'])\n\ndef get_prob(sir):\n    key = (sir['One Of Top 10 Words'],sir['Second Word'],sir['Third Word'])\n    w3 = neg_trigram_dict[key]\n    w2 = neg_bigram_dict[(sir['One Of Top 10 Words'],sir['Second Word'])]\n    return w3\/w2\n\nneg_trigram_df['Probabilty Of Sentence'] = neg_trigram_df.apply(get_prob,axis=1)\n\nneg_trigram_df.style.background_gradient(subset='Probabilty Of Sentence',cmap='vlag')","87e415ac":"Most_Positive_ht = ' '.join(Most_Positive[Most_Positive.hashtags.notna()].hashtags)\nMost_Negative_ht = ' '.join(Most_Negative[Most_Negative.hashtags.notna()].hashtags)\n\nMost_Positive_ht = re.sub(r'\\W', ' ', Most_Positive_ht)\nMost_Negative_ht = re.sub(r'\\W', ' ', Most_Negative_ht)\nMost_Positive_ht = re.sub(r'\\s+', ' ', Most_Positive_ht, flags=re.I)\nMost_Negative_ht = re.sub(r'\\s+', ' ', Most_Negative_ht, flags=re.I)\n\npwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(Most_Positive_ht)\nnwc = WordCloud(width=600,height=400,collocations = False,background_color='white').generate(Most_Negative_ht)\n\nplt.subplot(1,2,1)\nplt.title('Common Hashtags Among Most Positive Tweets',fontsize=16,fontweight='bold')\nplt.imshow(pwc)\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.title('Common Hashtags Among Most Negative Tweets',fontsize=16,fontweight='bold')\nplt.imshow(nwc)\nplt.axis('off')\n\nplt.show()","04dd14d9":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Top 10 Most Positive Locations Contributes',  'Top 10 Most Negative Locations Contributes'))\n\nfig.add_trace(\n    go.Bar(x=Most_Positive.user_location.value_counts()[:10].index, y=Most_Positive.user_location.value_counts()[:10].values,name='Number Of Tweets'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Bar(x=Most_Negative.user_location.value_counts()[:10].index, y=Most_Negative.user_location.value_counts()[:10].values,name='Number Of Tweets'),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Locations That Contribute The Most To Our Cut-Offs\")\nfig.show()\n","9de941c6":"fig = make_subplots(rows=2, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =f_data[['user_followers','user_friends','user_favourites','user_verified','Positive Sentiment','Neutral Sentiment','Negative Sentiment']].corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =f_data[['user_followers','user_friends','user_favourites','user_verified','Positive Sentiment','Neutral Sentiment','Negative Sentiment']].corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Locations That Contribute The Most To Our Cut-Offs\")\nfig.show()","9ea758f9":"def user_popularity(sir):\n    teta = 0\n    if sir['user_verified'] == 'True':\n        teta=1\n    else:\n        teta=0.5\n    rho = np.sqrt(sir['user_followers']*0.65+sir['user_friends']*0.25+sir['user_favourites']*0.10)*teta\n    return np.round(rho)\n\nf_data['user_popularity'] = f_data.apply(user_popularity,axis=1)","93b67546":"user_df = f_data.groupby(by='user_name').mean().reset_index()\nuser_df[['user_name','user_popularity','Positive Sentiment','Negative Sentiment']].sample(5)","0b0057f5":"KM = KMeans(5,random_state=42)\nC_DF = user_df[['user_popularity']].copy()\nC_DF.user_popularity = (C_DF.user_popularity-C_DF.user_popularity.mean())\/C_DF.user_popularity.std()\n#C_DF.Mean_Word_Length = (C_DF.Mean_Word_Length-C_DF.Mean_Word_Length.mean())\/C_DF.Mean_Word_Length.std()\n\nKM.fit(C_DF)\np_df = pd.DataFrame({'X':user_df['user_popularity'],'Y':user_df['Mean_Word_Length'],'Z':user_df['Number_Of_Words'],'C':KM.labels_})\nex.scatter_3d(p_df,x='X',y='Y',z='Z',color='C',title='Clustering Users By Popularity')","3c8354e5":"\n\n\nuser_df[\"Popularity\"] = KM.labels_\nrg = list(user_df[\"Popularity\"].value_counts().index)\n\nuser_df[\"Popularity\"]  = user_df[\"Popularity\"].replace({rg[0]:'Less Than Average',rg[1]:'Average',rg[2]:'Popular',rg[3]:'Very Popular',rg[4]:'Superstar'})\n\npop_df = user_df.groupby(by='Popularity').mean().reset_index()\n\n\nfig = make_subplots(rows=2, cols=2,shared_xaxes=True,subplot_titles=('Average Positive Sentiment For Each Category','Proportion Of Different Statuses',  'Average Negative Sentiment For Each Category'),\n                   specs=[[{\"type\": \"bar\"},{'type':'pie','rowspan':2}],\n                           [{\"type\": \"bar\"},None]\n                         ])\n\nfig.add_trace(\n    go.Bar(x=pop_df.Popularity, y=pop_df['Positive Sentiment'],name='Positive Sentiment'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Bar(x=pop_df.Popularity, y=pop_df['Negative Sentiment'],name='Negative Sentiment'),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Pie(values=user_df.Popularity.value_counts().values, labels=user_df.Popularity.value_counts().index,name='Proportion Of Different Statuses',hole=0.3,\n          marker_colors=ex.colors.diverging.BrBG),\n    row=1, col=2\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Different Popularity Categories Average Sentiment Strength\")\nfig.update_yaxes(title_text=\"Mean Sentiment\")\nfig.show()\n","71496923":"b_date_mean = f_data.copy()\nb_date_mean['user_created'] = pd.to_datetime(b_date_mean['user_created']).dt.normalize().dt.year\nb_date_mean = b_date_mean.groupby(by='user_created').mean().reset_index()\n\n\nex.line(b_date_mean,x='user_created',y='user_popularity')\n\nfig = go.Figure()\n\nfor column in ['user_popularity','user_followers','user_friends','user_favourites','Positive Sentiment','Negative Sentiment']:\n    fig.add_trace(\n        go.Scatter(\n            x = b_date_mean.user_created,\n            y = b_date_mean[column],\n            name = column,\n            mode='lines'\n        )\n    )\n    \n\nbtns = []\nfor x,col in enumerate(['user_popularity','user_followers','user_friends','user_favourites','Positive Sentiment','Negative Sentiment']):\n    bol = [False]*12\n    bol[x]=True\n    d = dict(label = col,\n                  method = 'update',\n                  args = [{'visible':bol},\n                          {'title': 'Distribution of [' +col+'] Over The Years',\n                           'showlegend':True}])\n    btns.append(d)\n    \n    \nfig.update_layout(title='User Creation Year Effect On Different Aspects',\n    updatemenus=[go.layout.Updatemenu(\n        active=0,\n        showactive=True,\n        buttons=btns\n        )\n    ])\n\nfig.update_xaxes(title_text='User Creation Year')\nfig.update_yaxes(title_text='Yearly Mean Value')\n\nfig.show()","69c7a006":"user_df=user_df.sort_values(by='user_popularity',ascending=False)\nfig = make_subplots(\n    rows=3, cols=1,\n    shared_xaxes=True,\n    vertical_spacing=0.03,\n    specs=[[{\"type\": \"table\"}],\n           [{\"type\": \"scatter\"}],\n           [{\"type\": \"scatter\"}]]\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=user_df[\"user_popularity\"],\n        y=user_df[\"Positive Sentiment\"],\n        mode=\"markers\",\n        name=\"Positive Sentiment\"\n    ),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=user_df[\"user_popularity\"],\n        y=user_df[\"Negative Sentiment\"],\n        mode=\"markers\",\n        name=\"Negative Sentiment\"\n    ),\n    row=3, col=1\n)\n\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['<b>user_name<b>',\"<b>Popularity<b>\",'<b>Negative Sentiment<b>','<b>Positive Sentiment<b>'],\n            font=dict(size=19,family=\"Lato\"),\n            align=\"center\"\n        ),\n        cells=dict(\n            values=[user_df[k].tolist() for k in ['user_name',\"user_popularity\",'Negative Sentiment','Positive Sentiment']],\n            align = \"center\", font=dict(size=12)),\n    ),\n    row=1, col=1\n)\n\nfigg = ex.scatter(user_df, x='user_popularity', y='Positive Sentiment', trendline='ols')\ntrendline = figg.data[1]\nfig.add_trace(trendline)\nfigg = ex.scatter(user_df, x='user_popularity', y='Negative Sentiment', trendline='ols', color_discrete_sequence=['red'])\ntrendline = figg.data[1]\nfig.add_trace(trendline,3,1)\n\nfig.update_layout(\n    height=800,\n    showlegend=False,\n    title_text=\"Positive And Negative Sentiment Strength Relationship With Popularity\",\n)\nfig.update_yaxes(title_text=\"Sentiment Strength\")\nfig.show()","94a92bfe":"NUMBER_OF_COMPONENTS = 450\n\nCVZ = CountVectorizer()\nSVD = TruncatedSVD(NUMBER_OF_COMPONENTS)\n\ntext_data = f_data.text.copy()\ntext_data = text_data.apply(lambda x: ' '.join([word for word in x.split() if word not in STOPWORDS and len(word) > 1]).strip())\n\nstemmer= PorterStemmer()\nlemmatizer=WordNetLemmatizer()\n\ntext_data = text_data.apply(lambda x: ' '.join([stemmer.stem(word) for word in word_tokenize(x)]))\ntext_data = text_data.apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in word_tokenize(x)]))\n\nC_vector = CVZ.fit_transform(text_data)\n\n\npc_matrix = SVD.fit_transform(C_vector)\n\nevr = SVD.explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\", \n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(     title='{:.2f}% of the Tweet Text Variance Can Be Explained Using {} Words'.format(np.sum(evr)*100,NUMBER_OF_COMPONENTS))\nfig.show()","c1314ab9":"best_fearures = [[CVZ.get_feature_names()[i],SVD.components_[0][i]] for i in SVD.components_[0].argsort()[::-1]]\nworddf = pd.DataFrame(np.array(best_fearures[:450])[:,0]).rename(columns={0:'Word'})\nworddf['Explained Variance'] =  np.round(evr*100,2)\nworddf['Explained Variance'] =worddf['Explained Variance'].apply(lambda x:str(x)+'%')\napp = []\nfor word in worddf.Word:\n    total_count = 0\n    for tweet in text_data:\n        if tweet.find(word)!= -1:\n            total_count+=1\n    app.append(total_count)\nworddf['Appeared_On_X_Tweets'] = app\nworddf\n\nfig = go.Figure()\nfig.add_trace(\n    go.Table(\n        header=dict(\n            values=['<b>Word<b>',\"<b>Explaines X% of Variance<b>\",'<b>Appeared On X Tweets<b>'],\n            font=dict(size=19,family=\"Lato\"),\n            align=\"center\"\n        ),\n        cells=dict(\n            values=[worddf[k].tolist() for k in ['Word',\"Explained Variance\",'Appeared_On_X_Tweets']],\n            align = \"center\")\n    ),\n    \n)\n\nfig.show()","ca5155e9":"<a id=\"4.5\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Text Decomposition Analysis<\/h1>\n","68d9033b":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:180%;text-align:center;border-radius: 15px 50px;\">Overview on User Analysis<\/h1>\n","c5b2f015":"<p style=\"text-align: center;\"><strong><span style='font-family: \"Times New Roman\", Times, serif; font-size: 26px;'>Possible Explanation For The Incline On The 27th<\/span><\/strong><\/p>","ea59cec5":"<p style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; text-align: center;\"><span data-preserver-spaces=\"true\" style='color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>VADER sentimental analysis relies on a dictionary that maps lexical features to emotion intensities known as sentiment scores. The sentiment score of a text can be obtained by summing up each word&apos;s intensity in the text.<\/span><\/p>\n<p style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 24px;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">For example,- Words like<\/span><em style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">&nbsp;&apos;love,&apos; &apos;enjoy,&apos; &apos;happy,&apos; &apos;like&apos;&nbsp;<\/span><\/em><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">all convey a positive sentiment. Also, VADER is intelligent enough to understand these words&apos; basic context, such as&nbsp;<\/span><em style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">&quot;did not love&quot;<\/span><\/em><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">&nbsp;as a negative statement. It also understands the emphasis of capitalization and punctuation, such as&nbsp;<\/span><em style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">&quot;ENJOY.&quot;<\/span><\/em><\/span><\/span><\/p>\n<p style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; text-align: center;\"><em style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\"><span data-preserver-spaces=\"true\" style='color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; font-family: \"Times New Roman\", Times, serif; font-size: 24px;'><\/span><\/em><\/p>\n<p><br><\/p>\n<p style=\"text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br><\/span><\/p>\n<p style=\"text-align: center;\"><a href=\"http:\/\/%3Cp%3E%3Ca%20href=%22https%3A\/\/towardsdatascience.com\/sentimental-analysis-using-vader-a3415fef7664#%3A~%3Atext=VADER%20(%20Valence%20Aware%20Dictionary%20for,intensity%20(strength)%20of%20emotion.&text=VADER%20sentimental%20analysis%20relies%20on,intensities%20known%20as%20sentiment%20scores.%22%3Ehttps%3A\/\/towardsdatascience.com\/sentimental-analysis-using-vader-a3415fef7664#%3A~%3Atext=VADER%20(%20Valence%20Aware%20Dictionary%20for,intensity%20(strength)%20of%20emotion.&text=VADER%20sentimental%20analysis%20relies%20on,intensities%20known%20as%20sentiment%20scores.%3C\/a%3E%3C\/p%3E%20%3Cp%3E%3Cbr%3E%3C\/p%3E\"><span style=\"font-family: 'Times New Roman', Times, serif;\">Source<\/span><\/a><\/p>","d2e25708":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>From the ACR and PACR plots above, it is clear that there is no significant correlation between sentiments lagged by x units.<\/span><\/p>\n<p><br><\/p>","994cafd0":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We see that the tweets' sentiments do not meet stationarity requirements as to non-constant mean and variance.\nIn the above code cell, we have tested our hypothesis on 3 partitions of our data.\nMight it indicate we have some trend in our data?<\/span><\/p>\n<p><br><\/p>","41c536cc":"<a id=\"4.1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:180%;text-align:center;border-radius: 15px 50px;\">Time Based Analysis<\/h1>\n","fbedcec8":"<a id=\"5\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Conclusions<\/h1>\n\n\n","772f8593":"<a id=\"4.4\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:180%;text-align:center;border-radius: 15px 50px;\">User Analysis<\/h1>\n","ddc71b44":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The dataset was grouped by the user names, and the average value of each feature was calculated.\n    Next, we will divide our dataset into 5 categories of popularity, where 1 is a user who has a very low popularity score compared to others, and 5 is prevalent and influential.<\/span><\/p>\n<p><br><\/p>","1435ee1a":"<a id=\"3.1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Feature Engineering<\/h1>\n","729ee622":"<a id=\"4.3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:180%;text-align:center;border-radius: 15px 50px;\">Correlation Analysis<\/h1>\n","843ac15b":"\n<a id=\"4.2\"><\/a>\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:180%;text-align:center;border-radius: 15px 50px;\">Extreme Sentiment Analysis<\/h1>\n","8d2f6380":"<a id=\"3\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Vader Sentiment Analysis<\/h1>\n","28558733":"\n<p style=\"text-align: center;\"><strong><span style='font-family: \"Times New Roman\", Times, serif; font-size: 28px;'>The total variance contribution of different words that make up our reduced dimension.<\/span><\/strong><\/p>","c26f6bf7":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>London and Miami are the leading locations when looking at the tweets we took from our positive cut-off in contrast to Moronto City, which stands out in the most negative cut-off.<\/span><\/p>\n<p><br><\/p>","8f981a06":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Unfortunately, we see no significant correlation between the tweet sentiments and any other numeric features given in our dataset, especially those that describe users.<\/span><\/p>","afbaea2f":"<a id=\"4\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h1>\n","c982c5a6":"<a id=\"2\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Text Preprocessing<\/h1>\n","3caadc28":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Above is a trigram of 15 sentences that start with one of the top 10 negative tweet words and the probability that the sentence will appear in random 'extremly' negative tweet.<\/span><\/p>\n<p><br><\/p>","0fb7fff0":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We observe that the most popular Twitter users tend to be more extreme towards positivity and negativity in their tweets.<\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>And as to the regular, not popular users, they tend to be more positive on average in their tweet sentiment.<\/span><\/p>\n<p><br><\/p>","b051ee6f":"<a id=\"4.1.1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:180%;text-align:center;border-radius: 15px 50px;\">Probabilistic Inference<\/h1>\n","bf3d1b1b":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The daily standard deviation of the positive and negative sentiments becomes less stable after the 27, which further increases the question&apos;s interest, what happened on the 27?<\/span><\/p>\n<p><br><\/p>","e7ae5b47":"<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:350%;text-align:center;border-radius: 15px 50px;\">Table Of Content<\/h1>\n\n\n\n* [1. Libraires And Utilities](#1)\n\n* [2 Text Preprocessing](#2)\n\n* [3 Vader Sentiment Analysis](#3)\n    * [3.1 Feature Engineering](#3.1)\n\n\n* [4. Exploratory Data Analysis](#4)\n    * [4.1 Time Based Analysis](#4.1)\n        * [4.1.1 Probabilistic Inference](#4.1.1)\n    * [4.2 Extreme Sentiment Analysis](#4.2)\n    * [4.3 Sentiment Correlation Analysis](#4.3)\n    * [4.4 User Analysis](#4.4)\n    * [4.5 Text Decomposition Analysis](#4.5)\n\n* [5. Conclusions](#5)\n","8d8b0fa5":"<a id=\"1\"><\/a>\n\n<h1 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Libraires And Utilities<\/h1>\n","5ca26d73":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>This section of our analysis will define a new metric representing a user&apos;s popularity; we will define a formula that will output a score of popularity based on the user follower, friend, and favorite counts.<\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The new metric will allow us to analyze the different users and the contribution to the average sentiments by different groups of user popularity.<\/span><\/p>\n<p><br><\/p>\n\n\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>We will define user popularity using the following formula; $$ \\Theta = \\begin{Bmatrix}1\\textrm{ if user is verified}\\\\\\textrm{else }0.5\\end{Bmatrix} $$ $$\\textrm{Popularity:}\\rho  = \\sqrt{0.65*\\textrm{User Followers}+0.25*\\textrm{User Friends}+0.10*\\textrm{User Favourites}}* \\Theta $$<\/span><\/p>\n<p><br><\/p>","5f01cb6a":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>One observation that interests me the most is that from the 20 of December to the 27 of December, there is a decline in the strength of the average negative sentiment; what happened in that week that induced this decline?<\/span><\/p>\n<p><br><\/p>","240d5021":"<p style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; text-align: center;\"><span data-preserver-spaces=\"true\" style='color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Based on the sentiment analysis done, we learned that;<\/span><\/p>\n<ul style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">The dominant sentiment of Pfizer vaccine-related tweets is neutral.<\/span><\/span><\/span><\/li>\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">The tweets tend to be more positive than negative on average.<\/span><\/span><\/span><\/li>\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">The sentiment data is not stationary and does not correlate over lagged time periods, thus meaning we cannot forecast how people will feel about the vaccine with the current data.<\/span><\/span><\/span><\/li>\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">There was a trend of decreasing negative sentiment strength from the 13th of December to the 27th, which was when the EU started vaccination.<\/span><\/span><\/span><\/li>\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">The positive sentiments&apos; deviation is more stable than the negative sentiment, which is much less stable. In other words, the strength of the positivity in people&apos;s tweets tends to be more or less the same, unlike the negative strength.<\/span><\/span><\/span><\/li>\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">There are recurring words both in the extremely negative and extremely positive tweets. We got a general idea of the sentences formed in those groups by creating a trigram table with different sentences&apos; probabilities.<\/span><\/span><\/span><\/li>\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">Some locations tend to be more negative\/positive than others on average.<\/span><\/span><\/span><\/li>\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">There is no significant correlation between the sentiment and other numeric features provided in the dataset.<\/span><\/span><\/span><\/li>\n    <li style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; list-style-type: disc; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span data-preserver-spaces=\"true\" style=\"color: rgb(14, 16, 26); background: transparent; margin-top:0pt; margin-bottom:0pt;\">On average, the most popular users tend to be the most extreme as to the sentiment strength in their tweets, both positive and negative.<\/span><\/span><\/span><\/li>\n<\/ul>\n<p style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br><\/span><\/span><\/p>\n<p style=\"color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; text-align: center;\"><span data-preserver-spaces=\"true\" style='color: rgb(14, 16, 26); background: transparent; margin-top: 0pt; margin-bottom: 0pt; font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>The sentiment strength of a tweet offers us a domain from which we can learn how the population is reacting to the vaccine; such insight can allow the different government to channel their advertisements towards more negative groups that usually refuse to believe in the integrity of the vaccine and observing the change over time similarly as we saw between the 13th and 27th of December.<\/span><\/p>","905fd1a1":"<p style=\"text-align: center;\"><strong><span style='font-family: \"Times New Roman\", Times, serif; font-size: 26px;'>What Happened On The 17th December<\/span><\/strong><\/p>\n<a href=\"https:\/\/ibb.co\/rxZWpbz\"><img src=\"https:\/\/i.ibb.co\/Pc5SWTP\/Screenshot-2021-01-09-202708.png\" alt=\"Screenshot-2021-01-09-202708\" border=\"0\"><\/a>","92408ff2":"<a href=\"https:\/\/ibb.co\/PmzJrvX\"><img src=\"https:\/\/i.ibb.co\/YRTJNFn\/Screenshot-2021-01-08-120838.png\" alt=\"Screenshot-2021-01-08-120838\" border=\"0\"><\/a>","b9480a26":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'> What have we gained? Immediately, we can see the uncertainty in our estimates: the wider the distribution, the less certain our posterior belief should be. We can also see what the plausible values for the parameters are: $\\lambda_1$ is around 99 and $\\lambda_2$ is around 43-44. The posterior distributions of the two $\\lambda$s are clearly distinct, indicating that it is indeed likely that there was a change in the user's text-message behaviour with strong confidence at day 50 i.e 31 of January which is visually the first suspect.<\/span><\/p>\n<p><br><\/p>","682015d9":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>You can observe that the distributions of the sentiments follow a normal distribution; the negative and positive sentiments are very similar, proposing that there may be no significant differences in the strength of our data&apos;s positive and negative sentiments.<\/span><\/p>\n<p style=\"text-align: center;\"><span style=\"font-size: 24px;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><br><\/span><\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>It is also clear that the dominant sentiment is neutral; oddly, most of the tweets do not resemble more positive or negative sentiment rather neutral.<\/span><\/p>","3c89e688":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Denoting day $i$'s  count by $C_i$,\n    $$ C_i \\sim \\text{Poisson}(\\lambda)  $$ We are not sure what the value of the $\\lambda$ parameter really is, however. Looking at the count plot above, it appears that the rate might become lower late in the observation period (with every day new data is being added), which is equivalent to saying that $\\lambda$ increases at some point during the observations. Let's assume that on some day during the observation period (call it $\\tau$), the parameter $\\lambda$ suddenly drops to a lower value. So we really have two $\\lambda$ parameters: one for the period before $\\tau$, and one for the rest of the observation period. we define our switchpoint as:\n$$\n\\lambda = \n\\begin{cases}\n\\lambda_1  & \\text{if } t \\lt \\tau \\cr\n\\lambda_2 & \\text{if } t \\ge \\tau\n\\end{cases}\n$$ If, in reality, no sudden change occurred and indeed $\\lambda_1 = \\lambda_2$, then the $\\lambda$s posterior distributions should look about equal. \\begin{align}\n&\\lambda_1 \\sim \\text{Exp}( \\alpha ) \\\\\\\n&\\lambda_2 \\sim \\text{Exp}( \\alpha )\n\\end{align}  What about $\\tau$? Because of the noisiness of the data, it's difficult to pick out a priori when $\\tau$ might have occurred. Instead, we can assign a *uniform prior belief* to every possible day. This is equivalent to saying\n\\begin{align}\n& \\tau \\sim \\text{DiscreteUniform(1,NumberOfDays) }\\\\\\\\\n& \\Rightarrow P( \\tau = k ) = \\frac{1}{NumberOfDays}\n\\end{align}<\/span><\/p>\n","6a600678":"<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>In the above code cell, we preprocess the text feature of our dataset, which contains the tweet&apos;s body.<\/span><\/p>\n<p style=\"text-align: center;\"><span style=\"font-family: 'Times New Roman', Times, serif;\"><span style=\"font-size: 24px;\">Our goal is to perform the sentiment analysis on clean text data to avoid noise and misreadings!<\/span><\/span><\/p>\n<p style=\"text-align: center;\"><span style='font-size: 24px; font-family: \"Times New Roman\", Times, serif;'>Remembering the good all saying, garbage in garbage out.<\/span><\/p>","4fea4999":"<p style=\"text-align: center;\"><span style='font-family: \"Times New Roman\", Times, serif; font-size: 24px;'>Above is a trigram of 15 sentences that start with one of the top 10 positive tweet words and the probability that the sentence will appear in random 'extremly' negative tweet.<\/span><\/p>\n<p><br><\/p>"}}