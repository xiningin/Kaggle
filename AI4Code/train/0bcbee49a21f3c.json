{"cell_type":{"e84ada5c":"code","3c885e49":"code","0e865604":"code","fa568f9e":"code","cfb68d6b":"code","90122b23":"code","b84901b1":"code","dba4dad6":"code","93ee15c1":"code","a57aedb0":"code","b904e5b9":"code","994b1a24":"code","26d145fa":"code","641d2824":"code","2eab2f28":"code","f06b9d38":"code","057ea07f":"code","84b2e716":"code","5f3309b6":"code","b4bdce3e":"code","0d022a52":"code","c6bd2ccb":"code","532ff5b8":"code","f4a25d3f":"code","6f609295":"code","430e2e61":"code","e5ac79b0":"code","d542a63a":"code","f64ce4f7":"code","261faebd":"code","ca2fb9de":"code","287b8388":"code","55174489":"code","0374e746":"code","57ccce20":"code","1372849b":"code","8bc1b312":"code","7a0e3a50":"code","91773706":"code","c210ef71":"code","d5620007":"code","e46e906b":"code","9bd31d8b":"code","bbe2d1b4":"code","f8f49d24":"code","41e2becb":"code","c9749312":"code","855d80e7":"code","fc6eedb9":"code","e3e3e60b":"code","c95584d2":"code","3731988e":"code","269efab2":"code","03d56544":"code","15544fcb":"code","8e5a6a02":"code","81321190":"code","f2dfc7f7":"code","3e905e3c":"code","f0a873c1":"code","41b267a0":"code","5ad9fb9d":"code","4d2c0d7f":"code","a8a19dc8":"code","d2c6e24b":"code","591fd199":"code","d405d432":"code","bc7f3fd5":"code","f159c859":"code","53903c56":"code","b5afb896":"code","d302902b":"code","eeaa9e67":"code","a4395f3b":"code","f6dd64d0":"code","59f42158":"code","ca0f04f4":"code","d9b8cb47":"code","eb491fe9":"markdown","d77c24de":"markdown","0b648494":"markdown","2aaee67d":"markdown","27e3c5f2":"markdown","9087f6e8":"markdown","a5d1df7b":"markdown"},"source":{"e84ada5c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n","3c885e49":"## read train and test data\n\ntrain=pd.read_csv(\"..\/input\/identifying-the-sentiments-nlp\/train_2kmZucJ.csv\")\ntrain.head()","0e865604":"train.shape","fa568f9e":"test=pd.read_csv(\"..\/input\/identifying-the-sentiments-nlp\/test_oJQbWVk.csv\")\ntest.head()","cfb68d6b":"train_cpy=train.copy()\ntest_cpy=test.copy()","90122b23":"train.label.value_counts()","b84901b1":"# Clean the tweets    \n\n\nwords_remove = [\"ax\",\"i\",\"you\",\"edu\",\"s\",\"t\",\"m\",\"subject\",\"can\",\"lines\",\"re\",\"what\", \"there\",\"all\",\"we\",\n                \"one\",\"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\"but\",\"is\",\"in\",\"a\",\"not\",\"with\",\"as\",\n                \"was\",\"if\",\"they\",\"are\",\"this\",\"and\",\"it\",\"have\",\"has\",\"from\",\"at\",\"my\",\"be\",\"by\",\"not\",\"that\",\n                \"to\",\"from\",\"com\",\"org\",\"so\",\"said\",\"from\",\"what\",\"told\",\"over\",\"more\",\"other\",\n                \"have\",\"last\",\"with\",\"this\",\"that\",\"such\",\"when\",\"been\",\"says\",\"will\",\"also\",\"where\",\"why\",\n                \"would\",\"today\", \"in\", \"on\", \"you\", \"r\", \"d\", \"u\", \"hw\",\"wat\", \"oly\", \"s\", \"b\", \"ht\", \n                \"rt\", \"p\",\"the\",\"th\", \"n\", \"was\"]\n\n\ndef cleantext(df, words_to_remove = words_remove): \n    \n    df['cleaned_tweet'] = df['tweet'].replace(r'\\'|\\\"|\\,|\\.|\\?|\\+|\\-|\\\/|\\=|\\(|\\)|\\n|\"', '', regex=True)\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(\"  \", \" \")\n\n    ### dont change the original tweet\n    # remove emoticons form the tweets\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'<ed>','', regex = True)\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'\\B<U+.*>|<U+.*>\\B|<U+.*>','', regex = True)\n    \n    # convert tweets to lowercase\n    df['cleaned_tweet'] = df['cleaned_tweet'].str.lower()\n    \n    #remove user mentions\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'^(@\\w+)',\"\", regex=True)\n    \n    #remove_symbols\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'[^a-zA-Z0-9]', \" \", regex=True)\n\n    #remove punctuations \n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'[[]!\"#$%\\'()\\*+,-.\/:;<=>?^_`{|}]+',\"\", regex = True)\n\n    #remove_URL(x):\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'https.*$', \"\", regex = True)\n\n    #remove 'amp' in the text\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'amp',\"\", regex = True)\n    \n    #remove words of length 1 or 2 \n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'\\b[a-zA-Z]{1,2}\\b','', regex=True)\n\n    #remove extra spaces in the tweet\n    df['cleaned_tweet'] = df['cleaned_tweet'].replace(r'^\\s+|\\s+$',\" \", regex=True)\n     \n    \n    #remove stopwords and words_to_remove\n    stop_words = set(stopwords.words('english'))\n    mystopwords = [stop_words, \"via\", words_to_remove]\n    \n    df['fully_cleaned_tweet'] = df['cleaned_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in mystopwords]))\n    \n\n    return df\n\n#get the processed tweets\ntrain = cleantext(train,words_remove)\ntest = cleantext(test,words_remove)","dba4dad6":"train.head()","93ee15c1":"train['tokenized_tweet']=train['fully_cleaned_tweet'].apply(word_tokenize)\ntest['tokenized_tweet']=test['fully_cleaned_tweet'].apply(word_tokenize)","a57aedb0":"train.head()","b904e5b9":"test.head()","994b1a24":"# if word has a digit, remove that word\ntrain['tokenized_tweet']=train['tokenized_tweet'].apply(lambda x: [y for y in x if not any(c.isdigit() for c in y)])","26d145fa":"# if word has a digit, remove that word\ntest['tokenized_tweet']=test['tokenized_tweet'].apply(lambda x: [y for y in x if not any(c.isdigit() for c in y)])","641d2824":"train.head()","2eab2f28":"stemmer = PorterStemmer()\n\ntrain['tokenized_tweet'] = train['tokenized_tweet'].apply(lambda x: [stemmer.stem(i) for i in x])","f06b9d38":"test['tokenized_tweet'] = test['tokenized_tweet'].apply(lambda x: [stemmer.stem(i) for i in x])","057ea07f":"train.head()","84b2e716":"from sklearn.feature_extraction.text import CountVectorizer","5f3309b6":"bow_vec=CountVectorizer(max_df=0.7, min_df=2, max_features=1000, stop_words='english')","b4bdce3e":"train['tokenized_tweet']=[\" \".join(tokenized_tweet) for tokenized_tweet in train['tokenized_tweet'].values]\ntest['tokenized_tweet']=[\" \".join(tokenized_tweet) for tokenized_tweet in test['tokenized_tweet'].values]","0d022a52":"train.head()","c6bd2ccb":"train['count']=train['tokenized_tweet'].str.split().str.len()\ntest['count']=test['tokenized_tweet'].str.split().str.len()","532ff5b8":"train['count']=train['count'].astype('category')\ntest['count']=test['count'].astype('category')","f4a25d3f":"y=train['label']","6f609295":"train.drop(['id','tweet','cleaned_tweet','fully_cleaned_tweet','label'],axis=1,inplace=True)","430e2e61":"test.drop(['id','tweet','cleaned_tweet','fully_cleaned_tweet'],axis=1,inplace=True)","e5ac79b0":"train_bow=bow_vec.fit_transform(train['tokenized_tweet'])\ntest_bow=bow_vec.transform(test['tokenized_tweet'])\n","d542a63a":"from sklearn.feature_extraction.text import TfidfVectorizer","f64ce4f7":"tfidf_vec=TfidfVectorizer(max_df=0.7, min_df=2,max_features=1000, stop_words='english')","261faebd":"train_tfidf=tfidf_vec.fit_transform(train['tokenized_tweet'])\ntest_tfidf=tfidf_vec.transform(test['tokenized_tweet'])","ca2fb9de":"### Understanding the common words","287b8388":"\nall_words = ' '.join([text for text in train['tokenized_tweet']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","55174489":"normal_words =' '.join([text for text in train['tokenized_tweet'][y == 0]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","0374e746":"negative_words =' '.join([text for text in train['tokenized_tweet'][y == 1]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","57ccce20":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\n\nX_train, X_test, y_train, y_test = train_test_split(train_bow, y, test_size=0.3, random_state=42)\n\nlr=LogisticRegression(C=0.5)\n\nlr.fit(X_train,y_train)","1372849b":"y_pred=lr.predict(X_test)","8bc1b312":"f1_score(y_test,y_pred)","7a0e3a50":"y_test_pred=lr.predict(test_bow)","91773706":"submission = pd.concat([test_cpy['id'],pd.DataFrame(y_test_pred, columns=['label'])],1)","c210ef71":"submission.to_csv('sub_lreg_bow.csv', index=False) # writing data to a CSV file","d5620007":"X_train, X_test, y_train, y_test = train_test_split(train_tfidf, y, test_size=0.3, random_state=42)","e46e906b":"lr.fit(X_train,y_train)","9bd31d8b":"y_pred=lr.predict(X_test)","bbe2d1b4":"f1_score(y_test,y_pred)","f8f49d24":"y_test_pred=lr.predict(test_tfidf)","41e2becb":"submission = pd.concat([test_cpy['id'],pd.DataFrame(y_test_pred, columns=['label'])],1)","c9749312":"submission.to_csv('sub_lreg_tfidf.csv', index=False) # writing data to a CSV file","855d80e7":"import lightgbm as lgb\nlgbm=lgb.LGBMClassifier(reg_alpha=0.05)","fc6eedb9":"lgbm.fit(X_train,y_train)","e3e3e60b":"y_pred=lgbm.predict(X_test)","c95584d2":"f1_score(y_test,y_pred)","3731988e":"y_test_pred=lgbm.predict(test_tfidf)","269efab2":"submission = pd.concat([test_cpy['id'],pd.DataFrame(y_test_pred, columns=['label'])],1)","03d56544":"submission.to_csv('sub_lgbm_tfidf.csv', index=False) # writing data to a CSV file","15544fcb":"import catboost as cb\ncb=cb.CatBoostClassifier()","8e5a6a02":"cb.fit(X_train,y_train)","81321190":"y_pred=cb.predict(X_test)","f2dfc7f7":"f1_score(y_test,y_pred)","3e905e3c":"y_test_pred=cb.predict(test_tfidf)","f0a873c1":"submission = pd.concat([test_cpy['id'],pd.DataFrame(y_test_pred, columns=['label'])],1)","41b267a0":"submission.to_csv('sub_cb_tfidf.csv', index=False) # writing data to a CSV file","5ad9fb9d":"import xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier","4d2c0d7f":"xgb=XGBClassifier()","a8a19dc8":"params={'colsample_bytree': [0.7], 'learning_rate': [0.1], 'max_depth': [7], 'min_child_weight': [4],\n        'n_estimators': [100],'objective': ['reg:logistic'], 'subsample': [0.6], 'verbosity': [1]}","d2c6e24b":"from sklearn.model_selection import GridSearchCV\nxgb1 = GridSearchCV(xgb,\n                    params,\n                    cv = 3,\n                    n_jobs = 5,\n                    verbose=True)\n","591fd199":"xgb1.fit(X_train,y_train)","d405d432":"y_pred=xgb1.predict(X_test)","bc7f3fd5":"f1_score(y_test,y_pred)","f159c859":"y_test_pred=xgb1.predict(test_tfidf)","53903c56":"submission = pd.concat([test_cpy['id'],pd.DataFrame(y_test_pred, columns=['label'])],1)","b5afb896":"submission.to_csv('sub_xgb_tfidf.csv', index=False) # writing data to a CSV file","d302902b":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier()","eeaa9e67":"dt.fit(X_train,y_train)","a4395f3b":"y_pred=dt.predict(X_test)","f6dd64d0":"f1_score(y_test,y_pred)","59f42158":"y_test_pred=dt.predict(test_tfidf)","ca0f04f4":"submission = pd.concat([test_cpy['id'],pd.DataFrame(y_test_pred, columns=['label'])],1)","d9b8cb47":"submission.to_csv('sub_dt_tfidf.csv', index=False) # writing data to a CSV file","eb491fe9":"### Vectorize the tweet","d77c24de":"## Building Model","0b648494":"### Building Light GBM Model","2aaee67d":"### Building CATBoost model","27e3c5f2":"### LGBM tfidf has performed well and have selected it as my final model","9087f6e8":"### XGBOOST","a5d1df7b":"### Using TFIDF features"}}