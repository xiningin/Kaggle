{"cell_type":{"69e91c8b":"code","6d8dd945":"code","316a28ac":"code","193105ba":"code","19b7911e":"markdown","cc925c49":"markdown","b746cba0":"markdown","f8bc6090":"markdown","c47f5947":"markdown","1238ab22":"markdown","eaff723f":"markdown"},"source":{"69e91c8b":"!pip install ..\/input\/shopee-external-models\/Keras_Applications-1.0.8-py3-none-any.whl\n!pip install ..\/input\/shopee-external-models\/efficientnet-1.1.0-py3-none-any.whl\nimport numpy as np\nimport pandas as pd\nimport gc\nimport matplotlib.pyplot as plt\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport tensorflow as tf\nimport efficientnet.tfkeras as efn\nfrom tqdm.notebook import tqdm\nimport math\nfrom shutil import copyfile\ncopyfile(src = \"..\/input\/bert-baseline\/tokenization.py\", dst = \"..\/working\/tokenization.py\")\nimport tokenization\nimport tensorflow_hub as hub","6d8dd945":"# For tf.dataset\nAUTO = tf.data.experimental.AUTOTUNE\n\n\n#use BERT or TFIDF, set true for bert, false for tfidf\nUSE_BERT = False\n# Configuration\nBATCH_SIZE = 8\nIMAGE_SIZE = [512, 512]\n# Seed\nSEED = 42\n# Verbosity\nVERBOSE = 1\n# Number of classes of each fold\nN_CLASSES = [8811, 8811, 8811, 8812, 8811]\n\n# Flag to get cv score\nGET_CV = True\n# Flag to check ram allocations (debug)\nCHECK_SUB = False\n\ndf = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n# If we are comitting, replace train set for test set and dont get cv\nif len(df) > 3:\n    GET_CV = False\ndel df\n","316a28ac":"\n# Function to get our f1 score\ndef f1_score(y_true, y_pred):\n    y_true = y_true.apply(lambda x: set(x.split()))\n    y_pred = y_pred.apply(lambda x: set(x.split()))\n    intersection = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    len_y_pred = y_pred.apply(lambda x: len(x)).values\n    len_y_true = y_true.apply(lambda x: len(x)).values\n    f1 = 2 * intersection \/ (len_y_pred + len_y_true)\n    return f1\n\n# Function to read out dataset\ndef read_dataset():\n    if GET_CV:\n        # Get train data from preprocess dataset (here we have our target ready)\n        df = pd.read_csv('..\/input\/shopee-tf-records-1\/train_folds.csv')\n        if CHECK_SUB:\n            df = pd.concat([df, df], axis = 0)\n            df.reset_index(drop = True, inplace = True)\n        image_paths = '..\/input\/shopee-product-matching\/train_images\/' + df['image']\n    else:\n        df = pd.read_csv('..\/input\/shopee-product-matching\/test.csv')\n        image_paths = '..\/input\/shopee-product-matching\/test_images\/' + df['image']\n        \n    return df, image_paths\n\n# Function to decode our images\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels = 3)\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    return image\n\n# Function to read our test image and return image\ndef read_image(image):\n    image = tf.io.read_file(image)\n    image = decode_image(image)\n    return image\n\n# Function to get our dataset that read images\ndef get_dataset(image):\n    dataset = tf.data.Dataset.from_tensor_slices(image)\n    dataset = dataset.map(read_image, num_parallel_calls = AUTO)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset\n\n# Return tokens, masks and segments from a text array or series\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","193105ba":"\n# Arcmarginproduct class keras layer\nclass ArcMarginProduct(tf.keras.layers.Layer):\n    '''\n    Implements large margin arc distance.\n\n    Reference:\n        https:\/\/arxiv.org\/pdf\/1801.07698.pdf\n        https:\/\/github.com\/lyakaap\/Landmark2019-1st-and-3rd-Place-Solution\/\n            blob\/master\/src\/modeling\/metric_learning.py\n    '''\n    def __init__(self, n_classes, s=30, m=0.50, easy_margin=False,\n                 ls_eps=0.0, **kwargs):\n\n        super(ArcMarginProduct, self).__init__(**kwargs)\n\n        self.n_classes = n_classes\n        self.s = s\n        self.m = m\n        self.ls_eps = ls_eps\n        self.easy_margin = easy_margin\n        self.cos_m = tf.math.cos(m)\n        self.sin_m = tf.math.sin(m)\n        self.th = tf.math.cos(math.pi - m)\n        self.mm = tf.math.sin(math.pi - m) * m\n\n    def get_config(self):\n\n        config = super().get_config().copy()\n        config.update({\n            'n_classes': self.n_classes,\n            's': self.s,\n            'm': self.m,\n            'ls_eps': self.ls_eps,\n            'easy_margin': self.easy_margin,\n        })\n        return config\n\n    def build(self, input_shape):\n        super(ArcMarginProduct, self).build(input_shape[0])\n\n        self.W = self.add_weight(\n            name='W',\n            shape=(int(input_shape[0][-1]), self.n_classes),\n            initializer='glorot_uniform',\n            dtype='float32',\n            trainable=True,\n            regularizer=None)\n\n    def call(self, inputs):\n        X, y = inputs\n        y = tf.cast(y, dtype=tf.int32)\n        cosine = tf.matmul(\n            tf.math.l2_normalize(X, axis=1),\n            tf.math.l2_normalize(self.W, axis=0)\n        )\n        sine = tf.math.sqrt(1.0 - tf.math.pow(cosine, 2))\n        phi = cosine * self.cos_m - sine * self.sin_m\n        if self.easy_margin:\n            phi = tf.where(cosine > 0, phi, cosine)\n        else:\n            phi = tf.where(cosine > self.th, phi, cosine - self.mm)\n        one_hot = tf.cast(\n            tf.one_hot(y, depth=self.n_classes),\n            dtype=cosine.dtype\n        )\n        if self.ls_eps > 0:\n            one_hot = (1 - self.ls_eps) * one_hot + self.ls_eps \/ self.n_classes\n\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output\n\n# Function to get the embeddings of our images with the fine-tuned model\ndef get_image_embeddings(image_paths, fold = 4):\n    embeds = []\n    \n    if fold == 4:\n        margin = ArcMarginProduct(\n                n_classes = N_CLASSES[0], \n                s = 30, \n                m = 0.7, \n                name='head\/arc_margin', \n                dtype='float32'\n                )\n    elif fold == 3:\n        margin = ArcMarginProduct(\n                n_classes = N_CLASSES[1], \n                s = 30, \n                m = 0.7, \n                name='head\/arc_margin', \n                dtype='float32'\n                )\n    elif fold == 2:\n        margin = ArcMarginProduct(\n                n_classes = N_CLASSES[2], \n                s = 30, \n                m = 0.7, \n                name='head\/arc_margin', \n                dtype='float32'\n                )\n    elif fold == 1:\n        margin = ArcMarginProduct(\n                n_classes = N_CLASSES[3], \n                s = 30, \n                m = 0.7, \n                name='head\/arc_margin', \n                dtype='float32'\n                )\n    elif fold == 0:\n        margin = ArcMarginProduct(\n                n_classes = N_CLASSES[4], \n                s = 30, \n                m = 0.7, \n                name='head\/arc_margin', \n                dtype='float32'\n                )\n\n    inp = tf.keras.layers.Input(shape = (*IMAGE_SIZE, 3), name = 'inp1')\n    label = tf.keras.layers.Input(shape = (), name = 'inp2')\n    x = efn.EfficientNetB1(weights = None, include_top = False)(inp)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = margin([x, label])\n        \n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n\n    model = tf.keras.models.Model(inputs = [inp, label], outputs = [output])\n    if fold == 4:\n        model.load_weights('..\/input\/shopeetrainednewmodels\/EfficientNetB1_512_42_4.h5')\n    elif fold == 3:\n        model.load_weights('..\/input\/shopeetrainednewmodels\/EfficientNetB1_512_42_3.h5')\n    elif fold == 2:\n        model.load_weights('..\/input\/shopeetrainednewmodels\/EfficientNetB1_512_42_2.h5')\n    elif fold == 1:\n        model.load_weights('..\/input\/shopeetrainednewmodels\/EfficientNetB1_512_42_1.h5')\n    elif fold == 0:\n        model.load_weights('..\/input\/shopeetrainednewmodels\/EfficientNetB1_512_42_0.h5')\n    model = tf.keras.models.Model(inputs = model.input[0], outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) \/ chunk))\n    for j in iterator:\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        image_dataset = get_dataset(image_paths[a:b])\n        image_embeddings = model.predict(image_dataset)\n        embeds.append(image_embeddings)\n    del model\n    image_embeddings = np.concatenate(embeds)\n    print(f'Our image embeddings shape is {image_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return image_embeddings\n\n\n# Function to get our text title embeddings using a pre-trained bert model\ndef get_text_embeddings_bert(df, max_len = 70):\n    embeds = []\n    module_url = \"..\/input\/shopee-external-models\/bert_en_uncased_L-24_H-1024_A-16_1\"\n    bert_layer = hub.KerasLayer(module_url, trainable = True)\n    vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n    do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n    tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n    text = bert_encode(df['title'].values, tokenizer, max_len = max_len)\n    \n    margin = ArcMarginProduct(\n            n_classes = 11014, \n            s = 30, \n            m = 0.5, \n            name='head\/arc_margin', \n            dtype='float32'\n            )\n    \n    input_word_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    label = tf.keras.layers.Input(shape = (), name = 'label')\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    x = margin([clf_output, label])\n    output = tf.keras.layers.Softmax(dtype='float32')(x)\n    model = tf.keras.models.Model(inputs = [input_word_ids, input_mask, segment_ids, label], outputs = [output])\n    \n    model.load_weights('..\/input\/bert-baseline\/Bert_123.h5')\n    model = tf.keras.models.Model(inputs = model.input[0:3], outputs = model.layers[-4].output)\n    chunk = 5000\n    iterator = np.arange(np.ceil(len(df) \/ chunk))\n    for j in iterator:\n        a = int(j * chunk)\n        b = int((j + 1) * chunk)\n        text_chunk = ((text[0][a:b], text[1][a:b], text[2][a:b]))\n        text_embeddings = model.predict(text_chunk, batch_size = BATCH_SIZE)\n        embeds.append(text_embeddings)\n    del model\n    text_embeddings = np.concatenate(embeds)\n    print(f'Our text embeddings shape is {text_embeddings.shape}')\n    del embeds\n    gc.collect()\n    return text_embeddings\n\n# Function to get our text title embeddings\ndef get_text_embeddings(df, max_features = 15500):\n    model = TfidfVectorizer(stop_words = 'english', binary = True, max_features = max_features)\n    text_embeddings = model.fit_transform(df['title'])\n    print(f'Our title text embedding shape is {text_embeddings.shape}')\n    del model\n    return text_embeddings\n\n# Function to get 50 nearest neighbors of each image and text and apply thresholds find in the training phase that optimize f1 cv score\ndef get_neighbors(df, image_embeddings, text_embeddings, KNN = 50):\n    # Get distances and indices from image and text embeddings\n    neighbors_model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine').fit(image_embeddings)\n    image_distances, image_indices = neighbors_model.kneighbors(image_embeddings)\n    neighbors_model = NearestNeighbors(n_neighbors = KNN, metric = 'cosine').fit(text_embeddings)\n    text_distances, text_indices = neighbors_model.kneighbors(text_embeddings)\n  \n    # Iterate through different thresholds to maximize cv, run this in interactive mode, then replace else clause with a solid threshold\n    if GET_CV:\n        predictions = []\n        for k in range(df.shape[0]):\n            # This are the original thresholds that gives 0.8035 cv (optimize with a for loop)\n            idx_image = np.where(image_distances[k,] < 0.46)[0]\n            ids_image = image_indices[k,idx_image]\n            if USE_BERT:\n                idx_text = np.where(text_distances[k,] < 0.50)[0]\n            else:\n                idx_text = np.where(text_distances[k,] < 0.30)[0]\n            ids_text = text_indices[k,idx_text]\n            # Get the union of boths ids\n            ids = list(set(list(ids_image) + list(ids_text)))\n            posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n            predictions.append(posting_ids)\n        #find prediction score\n        df['pred_matches'] = predictions\n        df['f1'] = f1_score(df['matches'], df['pred_matches'])\n        score = df['f1'].mean()\n        print(f'Our f1 score is {score}')\n    else:\n        predictions = []\n        for k in range(df.shape[0]):\n            # Reduce the thresholds because we are predicting more observations\n            idx_image = np.where(image_distances[k,] < 0.35)[0]\n            ids_image = image_indices[k,idx_image]\n            if USE_BERT:\n                idx_text = np.where(text_distances[k,] < 0.30)[0]\n            else:\n                idx_text = np.where(text_distances[k,] < 0.19)[0]\n            ids_text = text_indices[k,idx_text]\n            # Get the union of boths ids\n            ids = list(set(list(ids_image) + list(ids_text)))\n            posting_ids = ' '.join(df['posting_id'].iloc[ids].values)\n            predictions.append(posting_ids)\n        \n    del neighbors_model, image_distances, image_indices, text_distances, text_indices\n    gc.collect()\n    return df, predictions\n\n# Read data and image paths\ndf, image_paths = read_dataset()\n\n# Get image embeddings\nimage_embeddings_4 = get_image_embeddings(image_paths, fold = 4)\nimage_embeddings_3 = get_image_embeddings(image_paths, fold = 3)\nimage_embeddings_2 = get_image_embeddings(image_paths, fold = 2)\nimage_embeddings_1 = get_image_embeddings(image_paths, fold = 1)\nimage_embeddings_0 = get_image_embeddings(image_paths, fold = 0)\ngc.collect()\n\n# Average 5 folds embeddings\nimage_embeddings = np.average([image_embeddings_4, image_embeddings_3, image_embeddings_2, image_embeddings_1, image_embeddings_0], axis = 0)\ndel image_embeddings_4, image_embeddings_3, image_embeddings_2, image_embeddings_1, image_embeddings_0\n\n# Get text embeddings (for test set increase the amount of features for optimal score (it should have more unique words))\nif USE_BERT:\n    if GET_CV:\n        text_embeddings = get_text_embeddings_bert(df)\n    else:\n        text_embeddings = get_text_embeddings_bert(df)\nelse:\n    if GET_CV:\n        text_embeddings = get_text_embeddings(df, max_features = 15500)\n    else:\n        text_embeddings = get_text_embeddings(df, max_features = 21500)\n\n# Get neighbors\ndf, predictions = get_neighbors(df, image_embeddings, text_embeddings, KNN = 50)\ndf['matches'] = predictions\ndf[['posting_id', 'matches']].to_csv('submission.csv', index = False)","19b7911e":"## Helper Functions","cc925c49":"## Main Script","b746cba0":"## This version uses EffNetB1 + TFIDF, to use BERT change USE_BERT variable to 1","f8bc6090":"## Imports","c47f5947":"# Introduction and Thoughts\n\nHello Everyone,          \nThis notebook is a combination of the GroupKFolds, a nice CV strategy and BERT baseline model proposed by ragner. The image models are trained using 5 different tfrecords and the embeddings are averaged just like previous notebook. Basically this is a continuation and modification of [this great notebook](https:\/\/www.kaggle.com\/ragnar123\/shopee-inference-efficientnetb1-tfidfvectorizer). For bert I have used fine tuned model from [this notebook](https:\/\/www.kaggle.com\/ragnar123\/bert-baseline). The CV for this notebook is high (0.92) but it performs not as expected in leaderboard. My guess is that it's due to overfitting or I was not able to find a proper threshold. I am still searching for a good threshold which gives good CV as well as LB. \nI have also fine tuned threshold for TFIDF which now gives a better LB values then previous notebook.\nThis notebook can be used as by changing `USE_BERT` variable:\n* EfficientNetB1 + TFIDF Vectorizer                       \n* EfficientNetB1 + Bert Tokenizer     \n\n\n### If you find this notebook useful please check Ragner's original notebook too. \n\n**References:**      \n@ragner123's great notebook which describes an effective GroupKFolds CV for images:           \nhttps:\/\/www.kaggle.com\/ragnar123\/shopee-inference-efficientnetb1-tfidfvectorizer\/              \nRagner's two other awesome notebooks which showcase BERT baseline model that I used here:            \nhttps:\/\/www.kaggle.com\/ragnar123\/bert-baseline                                 \nhttps:\/\/www.kaggle.com\/ragnar123\/unsupervised-baseline-arcface                 ","1238ab22":"## If you find this notebook useful, don't forget to give an upvote :)","eaff723f":"## Global Variables"}}