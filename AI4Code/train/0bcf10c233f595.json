{"cell_type":{"b375064d":"code","1ad47c48":"code","21b5ffe2":"code","b8e52268":"code","e9173057":"code","070dbf36":"code","dc74ea41":"code","9f534797":"code","3bb121ad":"code","2b926924":"code","27294345":"code","7853d640":"code","1ba41490":"code","731fcf2f":"code","cf60a5dd":"code","bf7491b7":"code","16b60101":"code","5ffff4b0":"markdown","eeb7c5bd":"markdown","5dfcb4d9":"markdown","ce8fdf59":"markdown","58fdcb9f":"markdown","f5478196":"markdown","b4935b08":"markdown","8a41e3cb":"markdown","a8917bd9":"markdown","edc64688":"markdown","b4585dee":"markdown"},"source":{"b375064d":"from IPython.display import YouTubeVideo\n\nYouTubeVideo('nRBnh4qbPHI',width=800, height=450)","1ad47c48":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom keras.models import Model\nfrom keras.layers import Input, LSTM, Dense, GRU\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","21b5ffe2":"data_dir = '..\/input\/frenchenglish\/'\nos.listdir(data_dir)","b8e52268":"path = '..\/input\/frenchenglish\/fra.txt'\nbatch_size = 128\nepochs=100\nlatent_dim=256\nnum_samples=10000","e9173057":"# Vectorize the data.\ninput_texts = []\ntarget_texts = []\ninput_characters = set()\ntarget_characters = set()\n\n\nwith open(path, 'r', encoding='utf-8') as f:\n    lines = f.read().split('\\n')\n    \n    \nfor line in lines[: min(num_samples, len(lines) - 1)]:\n    input_text, target_text, _ = line.split('\\t')\n    # We use \"tab\" as the \"start sequence\" character\n    # for the targets, and \"\\n\" as \"end sequence\" character.\n    target_text = '\\t' + target_text + '\\n'\n    input_texts.append(input_text)\n    target_texts.append(target_text)\n    \n    \n    for char in input_text:\n        if char not in input_characters:\n            input_characters.add(char)\n            \n            \n    for char in target_text:\n        if char not in target_characters:\n            target_characters.add(char)","070dbf36":"input_characters = sorted(list(input_characters))\ntarget_characters = sorted(list(target_characters))\n\nnum_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\n\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])","dc74ea41":"print('Number of samples:', len(input_texts))\n\nprint('Number of unique input tokens:', num_encoder_tokens)\n\nprint('Number of unique output tokens:', num_decoder_tokens)\n\nprint('Max sequence length for inputs:', max_encoder_seq_length)\n\nprint('Max sequence length for outputs:', max_decoder_seq_length)","9f534797":"input_token_index = dict(\n    [(char, i) for i, char in enumerate(input_characters)])\ntarget_token_index = dict(\n    [(char, i) for i, char in enumerate(target_characters)])\n\nencoder_input_data = np.zeros(\n    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n    dtype='float32')\ndecoder_input_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n    dtype='float32')\ndecoder_target_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n    dtype='float32')","3bb121ad":"for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n    for t, char in enumerate(input_text):\n        encoder_input_data[i, t, input_token_index[char]] = 1.\n    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n    for t, char in enumerate(target_text):\n        # decoder_target_data is ahead of decoder_input_data by one timestep\n        decoder_input_data[i, t, target_token_index[char]] = 1.\n        if t > 0:\n            '''\n            decoder_target_data will be ahead by one timestep\n            and will not include the start character.\n            \n            '''\n            \n            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n    decoder_target_data[i, t:, target_token_index[' ']] = 1.","2b926924":"#Encoder\nencoder_inputs = Input(shape=(None, num_encoder_tokens))\nencoder = LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]","27294345":"#Decoder\ndecoder_inputs = Input(shape=(None, num_decoder_tokens))\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the \n# return states in the training model, but we will use them in inference.\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n                                     initial_state=encoder_states)\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n","7853d640":"# Run training\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_split=0.2)","1ba41490":"model.save('s2s.h5')","731fcf2f":"encoder_model = Model(encoder_inputs, encoder_states)\n\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_outputs, state_h, state_c = decoder_lstm(\n    decoder_inputs, initial_state=decoder_states_inputs)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states)","cf60a5dd":"# Reverse-lookup token index to decode sequences back to\n# something readable.\nreverse_input_char_index = dict(\n    (i, char) for char, i in input_token_index.items())\nreverse_target_char_index = dict(\n    (i, char) for char, i in target_token_index.items())","bf7491b7":"def decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0, target_token_index['\\t']] = 1.\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == '\\n' or\n           len(decoded_sentence) > max_decoder_seq_length):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_decoder_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence\n","16b60101":"for seq_index in range(100):\n    # Take one sequence (part of the training set)\n    # for trying out decoding.\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print('-')\n    print('Input sentence:', input_texts[seq_index])\n    print('Decoded sentence:', decoded_sentence)","5ffff4b0":"# What is Machine Translation?\n**Machine translation is the task of automatically converting source text in one language to text in another language.**\n\n***In a machine translation task, the input already consists of a sequence of symbols in some language, and the computer program must convert this into a sequence of symbols in another language.***\n\n\n**Given a sequence of text in a source language, there is no one single best translation of that text to another language. This is because of the natural ambiguity and flexibility of human language. This makes the challenge of automatic machine translation difficult, perhaps one of the most difficult in artificial intelligence:**\n\n***The fact is that accurate translation requires background knowledge in order to resolve ambiguity and establish the content of the sentence.***\n\n**Classical machine translation methods often involve rules for converting text in the source language to the target language. The rules are often developed by linguists and may operate at the lexical, syntactic, or semantic level. This focus on rules gives the name to this area of study: Rule-based Machine Translation, or RBMT.**\n\n***RBMT is characterized with the explicit use and manual creation of linguistically informed rules and representations.***\n\n**The key limitations of the classical machine translation approaches are both the expertise required to develop the rules, and the vast number of rules and exceptions required.**\n\n**For more details refer [here](https:\/\/machinelearningmastery.com\/introduction-neural-machine-translation\/)**","eeb7c5bd":"# Loading Dataset","5dfcb4d9":"# Introduction\n\n![](https:\/\/miro.medium.com\/max\/550\/1*BbF4o_uKCRKerXpZiJBlpg.png)","ce8fdf59":"# Model Developement\n\n**We start by defining Encoder and Decoder, Encoder will recieve words in english while Decoder will play the role of translator to the words**","58fdcb9f":"# Pr\u00e9diction","f5478196":"# Loading Libraries","b4935b08":"**To get started watch this video of Siraj Raval on Machine Translation!!**\n\n**bonne chance**","8a41e3cb":"***The process of Neural Machine Translation was widely infulenced from the innovations done by Britishers during World War 2 in breaking the Enigma code. Later this process was adopted by US agencies to keep a track of Russian innovations through translation of their published papers***","a8917bd9":"# Inference stage\n**Now we begin with Inference mode or sampling mode, The process can be summarized below**\n1. **Encode input and retrieve initial decoder state**\n2. **Run one step of decoder with this initial state and a \"start of sequence\" token as target. Output will be the next target token**\n3. **Repeat with the current target token and current states**","edc64688":"**For Implementation simplicity I'll be refereing to [keras](https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/lstm_seq2seq.py) documentation for this**\n\n**Happy Learning**","b4585dee":"# Save Model"}}