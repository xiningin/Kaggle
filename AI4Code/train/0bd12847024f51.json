{"cell_type":{"12807b9e":"code","ca0d3914":"code","b9a9131e":"code","75a4480e":"code","f224be42":"code","34a4efec":"code","cf4139b0":"code","308bde1a":"code","3bc15e17":"code","59a6069a":"code","f4759e8c":"code","115cfeb3":"code","0862775b":"code","5a683714":"code","c2b31f74":"code","eaf221ee":"code","f68840a9":"code","6b2bdf78":"code","24277199":"code","8ad3867c":"code","292142d7":"code","f94dc93e":"code","b4846d37":"code","7cc81441":"code","30e3db0a":"code","8fd31de5":"code","8a3423a4":"code","1eed445b":"code","e4bfbd65":"code","9595f914":"code","8aee900d":"code","9cbd4f83":"code","77d665c9":"code","92d275cf":"code","ec1c1705":"code","2f533a1e":"code","ba93915f":"code","91cd7027":"code","2aa0a50b":"code","62e69496":"code","9750eec7":"code","29aa10d3":"code","6f8826ca":"code","94aed258":"code","6ea5f39d":"code","b3d5900c":"code","dcd0698a":"code","9b650f87":"code","026a16df":"code","8e9fded7":"markdown","c3d5ce49":"markdown","7fe2a0f6":"markdown","4eacfdad":"markdown","072de061":"markdown","f157f7d9":"markdown","830d3e52":"markdown","851a2084":"markdown","af8563f2":"markdown","bc065220":"markdown","1ea2861d":"markdown","1855d87c":"markdown","c559b939":"markdown","cbc412ed":"markdown","5aef13bc":"markdown","e90a7270":"markdown"},"source":{"12807b9e":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport math\nimport re   \nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom matplotlib import rcParams\nfrom wordcloud import WordCloud\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ca0d3914":"pd.set_option('display.max_columns', None) \npd.set_option('display.max_rows', None)  \npd.set_option('display.max_colwidth', -1) ","b9a9131e":"#importing data and storing it in pandas DataFrame\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")","75a4480e":"#checking head and tail of train data\ntrain.head()","f224be42":"train.shape","34a4efec":"train.info()","cf4139b0":"train.isnull().sum()","308bde1a":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')\nplt.show()","3bc15e17":"print(train.target.value_counts())\nplt.figure(figsize=(8,5))\nsns.countplot(train.target)\nplt.show()","59a6069a":"#creating a new column which will consist of the length of text in each row. \ntrain['len_text'] = np.NaN\nfor i in range(0,len(train['text'])):\n    train['len_text'][i]=(len(train['text'][i]))\ntrain.len_text = train.len_text.astype(int)","f4759e8c":"#creating subplots to see distribution of length of tweet\nsns.set_style(\"darkgrid\")\nf, (ax1, ax2) = plt.subplots(figsize=(12,6),nrows=1, ncols=2,tight_layout=True)\nsns.distplot(train[train['target']==1][\"len_text\"],bins=30,ax=ax1)\nsns.distplot(train[train['target']==0][\"len_text\"],bins=30,ax=ax2)\nax1.set_title('\\n Distribution of length of tweet labelled Disaster\\n')\nax2.set_title('\\nDistribution of length of tweet labelled No Disaster\\n ')\nax1.set_ylabel('Frequency')","115cfeb3":"# word cloud for words related to Disaster \ntext=\" \".join(post for post in train[train['target']==1].text)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(11,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words related to Disaster \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","0862775b":"# word cloud for words related to No Disaster \ntext=\" \".join(post for post in train[train['target']==0].text)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words related to No Disaster \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()\n","5a683714":"# Import Tokenizer\nfrom nltk.tokenize import RegexpTokenizer\n#Instantiate Tokenizer\ntokenizer = RegexpTokenizer(r'\\w+') ","c2b31f74":"#changing the contents of selftext to lowercase\ntrain.loc[:,'text'] = train.text.apply(lambda x : str.lower(x))\n\n#removing hyper link, latin characters and digits\ntrain['text']=train['text'].str.replace('http.*.*', '',regex = True)\ntrain['text']=train['text'].str.replace('\u00fb.*.*', '',regex = True)\ntrain['text']=train['text'].str.replace(r'\\d+','',regex= True)","eaf221ee":"# \"Run\" Tokenizer\ntrain['tokens'] = train['text'].map(tokenizer.tokenize)","f68840a9":"train.head()","6b2bdf78":"#assigning stopwords to a variable\nstop = stopwords.words(\"english\")","24277199":"# adding this stop word to list of stopwords as it appears on frequently occuring word\nitem=['amp']\n\nstop.extend(item)","8ad3867c":"#removing stopwords from tokens\ntrain['tokens']=train['tokens'].apply(lambda x: [item for item in x if item not in stop])","292142d7":"lemmatizer = WordNetLemmatizer()","f94dc93e":"lemmatize_words=[]\nfor i in range (len(train['tokens'])):\n    word=''\n    for j in range(len(train['tokens'][i])):\n        lemm_word=lemmatizer.lemmatize(train['tokens'][i][j])#lemmatize\n        \n        word=word + ' '+lemm_word # joining tokens into sentence    \n    lemmatize_words.append(word) # storing in list","b4846d37":"#creating a new column to store the result\ntrain['lemmatized']=lemmatize_words\n\n#displaying first 5 rows of dataframe\ntrain.head()","7cc81441":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LogisticRegression","30e3db0a":"#defining X and y for the model\nX = train['lemmatized']\ny = train['target']","8fd31de5":"# Spliting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)","8a3423a4":"# pipeline will consist of two stages:\n# 1.Instantiating countVectorizer\n# 2.Instantiating logistic regression model\n\npipe = Pipeline([\n    ('cvec', TfidfVectorizer()),  \n    ('lr', LogisticRegression()) \n])","1eed445b":"tuned_params = {\n    'cvec__max_features': [2500, 3000, 3500],\n    'cvec__min_df': [2,3],\n    'cvec__max_df': [.9, .95],\n    'cvec__ngram_range': [(1,1), (1,2)]\n}\ngs = GridSearchCV(pipe, param_grid=tuned_params, cv=3) # Evaluating model on unseen data\n\nmodel_lr=gs.fit(X_train, y_train) # Fitting model\n\n# This is the average of all cv folds for a single \n#combination of the parameters specified in the tuned_params\nprint(gs.best_score_) \n\n#displaying the best values of parameters\ngs.best_params_","e4bfbd65":"gs.score(X_train, y_train)","9595f914":"gs.score(X_test, y_test)","8aee900d":"# Generating predictions!\npredictions_lr = model_lr.predict(X_test)","9cbd4f83":"# Importing the confusion matrix function\nfrom sklearn.metrics import confusion_matrix\n\n# Generating confusion matrix\nconfusion_matrix(y_test, predictions_lr)","77d665c9":"#interpreting confusion matrix\ntn, fp, fn, tp = confusion_matrix(y_test, predictions_lr).ravel()\n\n#values with coreesponding labels\nprint(\"True Negatives: %s\" % tn)\nprint(\"False Positives: %s\" % fp)\nprint(\"False Negatives: %s\" % fn)\nprint(\"True Positives: %s\" % tp)","92d275cf":"# word cloud for Frequntly occuring words related to Disaster\ntext=\" \".join(post for post in train[train['target']==1].lemmatized)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words related to Disaster \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()\n","ec1c1705":"# word cloud for Frequntly occuring words related to No Disaster\ntext=\" \".join(post for post in train[train['target']==0].lemmatized)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words related to No Disaster \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","2f533a1e":"#reading the test data\ntest=pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest.head()","ba93915f":"#creating a new column- length \n# this gives the length of the post\ntest['length'] = np.NaN\nfor i in range(0,len(test['text'])):\n    test['length'][i]=(len(test['text'][i]))\ntest.length = test.length.astype(int)","91cd7027":"# word cloud for Frequntly occuring words in test dataframe\ntext=\" \".join(post for post in test.text)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words in test dataframe \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","2aa0a50b":"# Instantiate Tokenizer\ntokenizer = RegexpTokenizer(r'\\w+')\n\n#changing the contents of selftext to lowercase\ntest.loc[:,'text'] = test.text.apply(lambda x : str.lower(x))\n\n#removing hyper link and latin characters\ntest['text']=test['text'].str.replace('http.*.*', '',regex = True)\ntest['text']=test['text'].str.replace('\u00fb.*.*', '',regex = True)\ntest['text']=test['text'].str.replace(r'\\d+','',regex= True)\n\n# \"Run\" Tokenizer\ntest['tokens'] = test['text'].map(tokenizer.tokenize)","62e69496":"test.head()","9750eec7":"#removing stopwords from tokens\ntest['tokens']=test['tokens'].apply(lambda x: [item for item in x if item not in stop])","29aa10d3":"lemmatize_words=[]\nfor i in range (len(test['tokens'])):\n    word=''\n    for j in range(len(test['tokens'][i])):\n        lemm_word=lemmatizer.lemmatize(test['tokens'][i][j])#lemmatize\n        \n        word=word + ' '+lemm_word # joining tokens into sentence    \n    lemmatize_words.append(word) # store in list","6f8826ca":"#creating a new column to store the result\ntest['lemmatized']=lemmatize_words\n\n#displaying first 5 rows of dataframe\ntest.head()","94aed258":"# word cloud for Frequntly occuring words in test dataframe after lemmatizing\ntext=\" \".join(post for post in test.lemmatized)\nwordcloud = WordCloud(max_font_size=90, max_words=50, background_color=\"white\", colormap=\"inferno\").generate(text)\nplt.figure(figsize=(10,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.title('\\nFrequntly occuring words in test dataframe \\n\\n',fontsize=18)\nplt.axis(\"off\")\nplt.show()","6ea5f39d":"predictions_kaggle = model_lr.predict(test['lemmatized'])","b3d5900c":"# Creating an empty data frame\nsubmission_kaggle = pd.DataFrame()","dcd0698a":"# Assigning values to the data frame-submission_kaggle\nsubmission_kaggle['Id'] = test.id\nsubmission_kaggle['target'] = predictions_kaggle","9b650f87":"# Head of submission_kaggle\nsubmission_kaggle.head()","026a16df":"# saving data as  final_kaggle.csv\nsubmission_kaggle.loc[ :].to_csv('final_kaggle.csv',index=False)","8e9fded7":"## Text Cleaning","c3d5ce49":"From above two graps we can see that there are many null values in the location attribute so we will drop it for our prediction model.","7fe2a0f6":"Now checking for balance of our  train data","4eacfdad":"### Logistic Regression","072de061":"### Lemmatizing","f157f7d9":"### Training Data","830d3e52":"### Removing StopWords","851a2084":"Thank You!","af8563f2":"Checking for null values in our dataset","bc065220":"## Modeling","1ea2861d":"Hi Kagglers, welcome to the NLP with Disaster Tweets competition. This is my first attempt to any NLP competition. Do comment if there are any queries and please upvote, if you find it useful!","1855d87c":"From above we can conclude that our data is not imbalance.","c559b939":"## Importing Libraries","cbc412ed":"# Test Data","5aef13bc":"### Tokenizing","e90a7270":"## Creating .csv file"}}