{"cell_type":{"d97152f3":"code","cf2446d7":"code","889c0d57":"code","572ac7ad":"code","aaba481a":"code","1bb375fa":"code","131eacd9":"code","7ae24094":"code","125651e2":"code","db7c354a":"code","fa3801d6":"code","f9f580a2":"code","fd1f00c1":"markdown","6a7a1f6d":"markdown","be2940eb":"markdown","3e076263":"markdown","6d9cdfb4":"markdown","31983952":"markdown"},"source":{"d97152f3":"import numpy as np\nimport pandas as pd\n\nfrom itertools import combinations_with_replacement\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.datasets import make_regression","cf2446d7":"class Regression:\n    def __init__(self, learning_rate, iteration, regularization):\n        \"\"\"\n        :param learning_rate: A samll value needed for gradient decent, default value id 0.1.\n        :param iteration: Number of training iteration, default value is 10,000.\n        \"\"\"\n        self.m = None\n        self.n = None\n        self.w = None\n        self.b = None\n        self.regularization = regularization # will be the l1\/l2 regularization class according to the regression model.\n        self.lr = learning_rate\n        self.it = iteration\n\n    def cost_function(self, y, y_pred):\n        \"\"\"\n        :param y: Original target value.\n        :param y_pred: predicted target value.\n        \"\"\"\n        return (1 \/ (2*self.m)) * np.sum(np.square(y_pred - y)) + self.regularization(self.w)\n    \n    def hypothesis(self, weights, bias, X):\n        \"\"\"\n        :param weights: parameter value weight.\n        :param X: Training samples.\n        \"\"\"\n        return np.dot(X, weights) #+ bias\n\n    def train(self, X, y):\n        \"\"\"\n        :param X: training data feature values ---> N Dimentional vector.\n        :param y: training data target value -----> 1 Dimentional array.\n        \"\"\"\n        # Insert constant ones for bias weights.\n        X = np.insert(X, 0, 1, axis=1)\n\n        # Target value should be in the shape of (n, 1) not (n, ).\n        # So, this will check that and change the shape to (n, 1), if not.\n        try:\n            y.shape[1]\n        except IndexError as e:\n            # we need to change it to the 1 D array, not a list.\n            print(\"ERROR: Target array should be a one dimentional array not a list\"\n                  \"----> here the target value not in the shape of (n,1). \\nShape ({shape_y_0},1) and {shape_y} not match\"\n                  .format(shape_y_0 = y.shape[0] , shape_y = y.shape))\n            return \n        \n        # m is the number of training samples.\n        self.m = X.shape[0]\n        # n is the number of features.\n        self.n = X.shape[1]\n\n        # Set the initial weight.\n        self.w = np.zeros((self.n , 1))\n\n        # bias.\n        self.b = 0\n\n        for it in range(1, self.it+1):\n            # 1. Find the predicted value through the hypothesis.\n            # 2. Find the Cost function value.\n            # 3. Find the derivation of weights.\n            # 4. Apply Gradient Decent.\n            y_pred = self.hypothesis(self.w, self.b, X)\n            #print(\"iteration\",it)\n            #print(\"y predict value\",y_pred)\n            cost = self.cost_function(y, y_pred)\n            #print(\"Cost function\",cost)\n            # fin the derivative.\n            dw = (1\/self.m) * np.dot(X.T, (y_pred - y)) + self.regularization.derivation(self.w)\n            #print(\"weights derivation\",dw)\n            #db = -(2 \/ self.m) * np.sum((y_pred - y))\n\n            # change the weight parameter.\n            self.w = self.w - self.lr * dw\n            #print(\"updated weights\",self.w)\n            #self.b = self.b - self.lr * db\n\n\n            if it % 10 == 0:\n                print(\"The Cost function for the iteration {}----->{} :)\".format(it, cost))\n    def predict(self, test_X):\n        \"\"\"\n        :param test_X: feature values to predict.\n        \"\"\"\n        # Insert constant ones for bias weights.\n        test_X = np.insert(test_X, 0, 1, axis=1)\n\n        y_pred = self.hypothesis(self.w, self.b, test_X)\n        return y_pred","889c0d57":"# Define the traning data.\nX, y = make_regression(n_samples=50000, n_features=8)\n\n# Chnage the shape of the target to 1 dimentional array.\ny = y[:, np.newaxis]\n\nprint(\"=\"*100)\nprint(\"Number of training data samples-----> {}\".format(X.shape[0]))\nprint(\"Number of training features --------> {}\".format(X.shape[1]))\nprint(\"Shape of the target value ----------> {}\".format(y.shape))","572ac7ad":"# display the data.\ndata = pd.DataFrame(X)\ndata.head()\n","aaba481a":"# display the data.\ndata_y = pd.DataFrame(y)\ndata_y.head()","1bb375fa":"def PolynomialFeature(X, degree):\n    \"\"\"\n    It is type of feature engineering ---> adding some more features based on the exisiting features \n    by squaring or cubing.\n    :param X: data need to be converted.\n    :param degree: int- The degree of the polynomial that the features X will be transformed to.\n    \"\"\"\n    n_samples, n_features = X.shape\n\n    # get the index combinations.\n    combination = [combinations_with_replacement(range(n_features), i) for i in range(0, degree + 1)]\n    combination_index = [index for obj in combination for index in obj]\n\n    # generate a empty array with new shape.\n    new_n_features = len(combination_index)\n    X_new = np.empty((n_samples, new_n_features))\n\n    for i, com_index  in enumerate(combination_index):\n        X_new[:, i] = np.prod(X[:, com_index], axis=1)\n    \n    return X_new\n\n# Used for Polynomial Ridge regression.\nclass l2_regularization:\n    \"\"\"Regularization used for Ridge Regression\"\"\"\n    def __init__(self, lamda):\n        self.lamda = lamda\n\n    def __call__(self, weights):\n        \"This will be retuned when we call this class.\"\n        return self.lamda * np.sum(np.square(weights))\n    \n    def derivation(self, weights):\n        \"Derivation of the regulariozation function.\"\n        return self.lamda * 2 * (weights)\n","131eacd9":"class PolynamialRegression(Regression):\n    \"\"\"\n    Polynomail Regression is also a type of non-linear regression with no regularization. \n    Before fitting the linear regression, the dependant variable is tranformed to some polynomail degree.\n    This is basincally transforming linear data to have some nonliniarity.\n    \"\"\"\n    def __init__(self, learning_rate, iteration, degree):\n        \"\"\"\n        :param learning_rate: [range from 0 to infinity] the stpe distance used while doing gradiant decent.\n        :param iteration: int - Number of iteration to do.\n        :param degree: int - The degree of the polynomial that the feature transformed to.\n        \"\"\"\n        self.degree = degree\n        # No regularization here. So, making the regularization methods to return 0.\n        self.regularization = lambda x: 0\n        self.regularization.derivation = lambda x: 0\n        super().__init__(learning_rate, iteration, self.regularization)\n    \n    def train(self, X, y):\n        \"\"\"\n        :param X: training data feature values ---> N Dimentional vector.\n        :param y: training data target value -----> 1 Dimentional array.\n        \"\"\"\n        # change the data to \n        X_poly = PolynomialFeature(X, degree=self.degree)\n        return super().train(X_poly, y)\n    \n    def predict(self, test_X):\n        \"\"\"\n        :param test_X: feature values to predict.\n        \"\"\"\n        test_X_poly = PolynomialFeature(test_X, degree=self.degree)\n        return super().predict(test_X_poly)","7ae24094":"#define the parameters\nparam = {\n    \"degree\" : 2,\n    \"learning_rate\" : 0.1,\n    \"iteration\" : 100,\n}\nprint(\"=\"*100)\npolynomial_reg = PolynamialRegression(**param)\n\n# Train the model.\npolynomial_reg.train(X, y) \n\n# Predict the values.\ny_pred = polynomial_reg.predict(X)\n\n#Root mean square error.\nscore = r2_score(y, y_pred)\nprint(\"The r2_score of the trained model\", score)","125651e2":"from sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\n# data is already defined, going to use the same data for comparision.\nprint(\"=\"*100)\nprint(\"Number of training data samples-----> {}\".format(X.shape[0]))\nprint(\"Number of training features --------> {}\".format(X.shape[1]))\n","db7c354a":"linear_reg_sklearn = LinearRegression()\n\npoly = PolynomialFeatures(degree = 2)\nX_new = poly.fit_transform(X)\nlinear_reg_sklearn.fit(X, y)\n\n# predict the value\ny_pred_sklearn = linear_reg_sklearn.predict(X)\nscore = r2_score(y, y_pred_sklearn)\nprint(\"=\"*100)\nprint(\"R2 score of the model is {}\".format(score))","fa3801d6":"class PolynamialRidgeRegression(Regression):\n    \"\"\"\n    Polynomail Ridge Regression is basically polynomial regression with l2 regularization.\n    \"\"\"\n    def __init__(self, learning_rate, iteration, degree, lamda):\n        \"\"\"\n        :param learning_rate: [range from 0 to infinity] the stpe distance used while doing gradiant decent.\n        :param iteration: int - Number of iteration to do.\n        :param degree: int - The degree of the polynomial that the feature transformed to.\n        \"\"\"\n        self.degree = degree\n        # No regularization here. So, making the regularization methods to return 0.\n        self.regularization = l2_regularization(lamda)\n        super().__init__(learning_rate, iteration, self.regularization)\n    \n    def train(self, X, y):\n        \"\"\"\n        :param X: training data feature values ---> N Dimentional vector.\n        :param y: training data target value -----> 1 Dimentional array.\n        \"\"\"\n        # change the data to \n        X_poly = PolynomialFeature(X, degree=self.degree)\n        return super().train(X_poly, y)\n    \n    def predict(self, test_X):\n        \"\"\"\n        :param test_X: feature values to predict.\n        \"\"\"\n        test_X_poly = PolynomialFeature(test_X, degree=self.degree)\n        return super().predict(test_X_poly)","f9f580a2":"#define the parameters\nparam = {\n    \"lamda\": 0.1,\n    \"degree\" : 2,\n    \"learning_rate\" : 0.1,\n    \"iteration\" : 100,\n}\nprint(\"=\"*100)\npolynomial_reg = PolynamialRidgeRegression(**param)\n\n# Train the model.\npolynomial_reg.train(X, y) \n\n# Predict the values.\ny_pred = polynomial_reg.predict(X)\n\n#Root mean square error.\nscore = r2_score(y, y_pred)\nprint(\"The r2_score of the trained model\", score)","fd1f00c1":"# Polynomial Regression using scikit-learn for comparision","6a7a1f6d":"# Polynomial Ridge Regression from scratch","be2940eb":"# Polynomial Regression from Scratch","3e076263":"# Common Regression class","6d9cdfb4":"# Data Creation","31983952":"# Supervised Machine Learning models scratch series....\nyou can also check....\n\n- 1) Linear Regression         ---> https:\/\/www.kaggle.com\/ninjaac\/linear-regression-from-scratch\n- 2) Lasso Regression          ---> https:\/\/www.kaggle.com\/ninjaac\/lasso-and-ridge-regression-from-scratch \n- 3) Ridge Regression          ---> https:\/\/www.kaggle.com\/ninjaac\/lasso-and-ridge-regression-from-scratch\n- 4) ElasticNet Regression     ---> https:\/\/www.kaggle.com\/ninjaac\/elasticnet-regression-from-scratch \n- 5) Polynomail Regression     ---> https:\/\/www.kaggle.com\/ninjaac\/polynomial-and-polynomialridge-regression-scratch (Same Notebook you are looking now)\n- 5) PolynomailRidge Regression---> https:\/\/www.kaggle.com\/ninjaac\/polynomial-and-polynomialridge-regression-scratch (Same Notebook you are looking now)\n- 6) KNN Classifier            ---> https:\/\/www.kaggle.com\/ninjaac\/knnclassifier-from-scratch "}}