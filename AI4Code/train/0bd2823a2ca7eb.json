{"cell_type":{"9c08f50d":"code","ef9e64a6":"code","45cd3385":"code","aead4246":"code","7c15f267":"code","e0f79e6f":"code","cac2e2b8":"code","609009e7":"code","71cec891":"code","17a84cf7":"code","68c64173":"code","55283e74":"markdown","350460ed":"markdown","84c13012":"markdown","18b1fbe3":"markdown","2072c9d1":"markdown","05950c1d":"markdown","a4e264d5":"markdown","3cd90b6f":"markdown","877e4c22":"markdown","8c424d0c":"markdown"},"source":{"9c08f50d":"import os \nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential,Model\nfrom keras.layers import Dense,Flatten,Dropout,BatchNormalization,Conv2D,MaxPool2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom  skimage.transform import resize\nfrom keras.utils import to_categorical\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import RMSprop, Adam, SGD\nimport pickle","ef9e64a6":"training_dir=\"..\/input\/asl-and-some-words\/ASL\"\ncontent=sorted(os.listdir(training_dir))\nprint(content)\nlen(content)","45cd3385":"data_generator = ImageDataGenerator(\n    samplewise_center=True, \n    samplewise_std_normalization=True,\n    brightness_range=[0.8, 1.0],\n    zoom_range=[1.0, 1.2],\n    validation_split=0.1\n)\n\ntrain_generator = data_generator.flow_from_directory(training_dir, target_size=(200,200), shuffle=True, seed=13,\n                                                     class_mode='categorical', batch_size=64, subset=\"training\")\n\nvalidation_generator = data_generator.flow_from_directory(training_dir, target_size=(200, 200), shuffle=True, seed=13,\n                                                     class_mode='categorical', batch_size=64, subset=\"validation\")","aead4246":"WEIGHTS_FILE = '.\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\ninception_v3_model = keras.applications.inception_v3.InceptionV3(\n    input_shape = (200, 200, 3), \n    include_top = False, \n    weights = 'imagenet'\n)\n\ninception_v3_model.summary()","7c15f267":"inception_output_layer = inception_v3_model.get_layer('mixed7')\nprint('Inception model output shape:', inception_output_layer.output_shape)\n\ninception_output = inception_v3_model.output","e0f79e6f":"from tensorflow.keras import layers\nx = layers.GlobalAveragePooling2D()(inception_output)\nx = layers.Dense(1024, activation='relu')(x)                  \nx = layers.Dense(51, activation='softmax')(x)           \n\nmodel = Model(inception_v3_model.input, x) \n\nmodel.compile(\n    optimizer=SGD(lr=0.0001, momentum=0.9),\n    loss='categorical_crossentropy',\n    metrics=['acc']\n)\nfor layer in model.layers[:249]:\n    layer.trainable = False\nfor layer in model.layers[249:]:\n    layer.trainable = True","cac2e2b8":"LOSS_THRESHOLD = 0.2\nACCURACY_THRESHOLD = 0.95\n\nclass ModelCallback(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs={}):\n    if logs.get('val_loss') <= LOSS_THRESHOLD and logs.get('val_acc') >= ACCURACY_THRESHOLD:\n      print(\"\\nReached\", ACCURACY_THRESHOLD * 100, \"accuracy, Stopping!\")\n      self.model.stop_training = True\n\ncallback = ModelCallback()\n","609009e7":"history = model.fit_generator(\n    train_generator,\n    validation_data=validation_generator,\n    steps_per_epoch=200,\n    validation_steps=50,\n    epochs=50,\n    callbacks=[callback]\n)","71cec891":"model.save('transferlearning.h5')","17a84cf7":"plt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validationloss')\nplt.title('Training and validation loss')\nplt.legend(loc=0)\nplt.figure()","68c64173":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\n\n\nplt.show()","55283e74":"American Sign Language, typically referred to as ASL, is a visual language characterized\nby the formation of manual signs. ASL utilizes a unique form of syntax to communicate\nthoughts, feelings, and ideas. Due to discrepancies in census data, the number of American\ncitizens living within Deaf culture and communicating with ASL is widely disputed. However,\nmany sources claim that ASL is the third most widely used language in the United States,\nfollowing English and Spanish.\n\nThe question is, How to reduce the communication gap between the normal people who aren't familiar with ASL and the large community of people having hearing or speech impairments. So, here comes the role of people like us having superpower of technology. Here's a small attempt of me using that superpower to create an AI based system which can recognize as well as translate few static signs in ASL using computer vision.","350460ed":"# Overview of Data","84c13012":"# Dependencies","18b1fbe3":"![iv3.png](attachment:iv3.png)","2072c9d1":"# American Sign Language Recognition","05950c1d":"# Evaluation Metrics","a4e264d5":"![ASL.png](attachment:ASL.png)","3cd90b6f":"# Workflow of InceptionV3 Model","877e4c22":"# Image Data Augmentation","8c424d0c":"# Transfer Learning based Model Training"}}