{"cell_type":{"c206b585":"code","2c0e403a":"code","c02c9fb2":"code","b9b2e7dc":"code","07080fcb":"code","f680ad46":"code","2ae8231f":"code","99964fe5":"code","5c785dbb":"code","daae121b":"code","d00cf58d":"code","2b70d650":"code","39d8ff38":"code","7ccfa7ad":"code","2a98fe58":"code","667f2c06":"code","bc8709b4":"code","bdf3cde7":"code","7bd5e9a8":"code","b121bdc1":"code","e00b7060":"code","71620026":"code","22550889":"code","bf1ac215":"code","82252730":"markdown","2a4a4099":"markdown","e053fd91":"markdown","db11ca1b":"markdown","c08c7413":"markdown","5d176d81":"markdown","876410de":"markdown","d808598a":"markdown","bae8122e":"markdown","24b4a45b":"markdown","e639ea35":"markdown","42544db3":"markdown","ce5b664e":"markdown","88fd289b":"markdown","b354785c":"markdown","d005c341":"markdown","6a162bdf":"markdown"},"source":{"c206b585":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns #visualization\nimport matplotlib.pyplot as plt #visualization\n%matplotlib inline\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.offline as py\nimport plotly.express as px\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2c0e403a":"df = pd.read_csv('..\/input\/epitope-prediction\/input_sars.csv', encoding='ISO-8859-2')\ndf.head()","c02c9fb2":"df1 = pd.read_csv('..\/input\/epitope-prediction\/input_covid.csv', encoding='ISO-8859-2')\ndf1.head()","b9b2e7dc":"df2 = pd.read_csv('..\/input\/epitope-prediction\/input_bcell.csv', encoding='ISO-8859-2')\ndf2.head()","07080fcb":"#Code by Akash Sahoo https:\/\/www.kaggle.com\/akashdotcom\/covid-19-sars-b-cell-predictions-using-nn-eda\/notebook\n\n#training data created\nframes = [df2, df]\nbcell_sars_df = pd.concat(frames, axis=0, ignore_index=True)\nbcell_sars_df.head() ","f680ad46":"#checking for null values\nbcell_sars_df.isna().sum()","2ae8231f":"#Code by Akash Sahoo https:\/\/www.kaggle.com\/akashdotcom\/covid-19-sars-b-cell-predictions-using-nn-eda\/notebook\n#shuffling the dataset\nbcell_sars_df = bcell_sars_df.sample(frac=1).reset_index(drop = True)","99964fe5":"#checking the target variable countplot\nsns.set_style('darkgrid')\nsns.countplot(bcell_sars_df['target'],linewidth=3,palette=\"Set2\",edgecolor='black')","5c785dbb":"# Distribution of different type of amount\nfig , ax = plt.subplots(1,3,figsize = (12,5))\n\nstart_position = df.start_position.values\nend_position = df.end_position.values\ntarget = df.target.values\n\nsns.distplot(start_position , ax = ax[0] , color = 'blue').set_title('B Cell Start Position' , fontsize = 14)\nsns.distplot(end_position , ax = ax[1] , color = 'cyan').set_title('B Cell End Position' , fontsize = 14)\nsns.distplot(target , ax = ax[2] , color = 'purple').set_title('B Cell Target' , fontsize = 14)\n\n\nplt.show()","daae121b":"#Code by Akash Sahoo https:\/\/www.kaggle.com\/akashdotcom\/covid-19-sars-b-cell-predictions-using-nn-eda\/notebook\n\nidx_train = bcell_sars_df['target'].astype(\"bool\").values\nfig, axes = plt.subplots(2, 3,figsize=(16,8))\nsns.set_style('darkgrid')\naxes = [x for a in axes for x in a]\nfor i,name in enumerate([\"kolaskar_tongaonkar\", \"chou_fasman\", \"hydrophobicity\", \"stability\", \"parker\", \"emini\"]):\n    value = bcell_sars_df[name]\n    sns.distplot(value[~idx_train],ax = axes[i], color='red')\n    sns.distplot(value[idx_train],ax = axes[i], color = 'blue')\n    axes[i].set_xlabel(name,fontsize=12)\n    fig.legend(labels = [\"target 0\",\"target 1\"],loc=\"right\",fontsize=12)","d00cf58d":"# Plot of Chou Fasman Method\nplt.style.use(\"classic\")\nsns.distplot(df['chou_fasman'], color='blue')\nplt.title(f\"Chou Fasman [\\u03BC : {df['chou_fasman'].mean():.2f} status | \\u03C3 : {df['chou_fasman'].std():.2f} status]\")\nplt.xlabel(\"Chou Fasman Method\")\nplt.ylabel(\"Count\")\nplt.show()","2b70d650":"# Plot of Emini Method\nplt.style.use(\"classic\")\nsns.distplot(df['emini'], color='red')\nplt.title(f\"Emini [\\u03BC : {df['emini'].mean():.2f} status | \\u03C3 : {df['emini'].std():.2f} status]\")\nplt.xlabel(\"Emini Method\")\nplt.ylabel(\"Count\")\nplt.show()","39d8ff38":"# Plot of Kolaskar Tongaonkar Method\nplt.style.use(\"classic\")\nsns.distplot(df['kolaskar_tongaonkar'], color='green')\nplt.title(f\"kolaskar_tongaonkar [\\u03BC : {df['kolaskar_tongaonkar'].mean():.2f} status | \\u03C3 : {df['kolaskar_tongaonkar'].std():.2f} status]\")\nplt.xlabel(\"Kolaskar Tongaonkar Method\")\nplt.ylabel(\"Count\")\nplt.show()","7ccfa7ad":"# Plot of Parker Method\nplt.style.use(\"classic\")\nsns.distplot(df['parker'], color='purple')\nplt.title(f\"Parker [\\u03BC : {df['parker'].mean():.2f} status | \\u03C3 : {df['parker'].std():.2f} status]\")\nplt.xlabel(\"Parker Method\")\nplt.ylabel(\"Count\")\nplt.show()","2a98fe58":"##https:\/\/www.kaggle.com\/ilyabiro\/visual-analysis-or-how-to-find-out-who-escaped-1st\/notebook\nfig, ax = plt.subplots(figsize=(10, 8))\nax.set_title('Methods distribution')\ng = sns.kdeplot(df['chou_fasman'].loc[df['emini'] == 1], \n                shade= True, ax=ax, label='emini').set_xlabel('chou_fasman')\ng = sns.kdeplot(df['chou_fasman'].loc[df['emini'] == 0], \n                shade=True, ax=ax, label='emini')\nax.grid()","667f2c06":"#https:\/\/www.kaggle.com\/ilyabiro\/visual-analysis-or-how-to-find-out-who-escaped-1st\/notebook\nfig, axarr = plt.subplots(1, 2, figsize=(14,8))\naxarr[0].set_title('Kolaskar Tongaonkar Method Distribution')\nf = sns.distplot(df['kolaskar_tongaonkar'], color='g', bins=15, ax=axarr[0])\naxarr[1].set_title('Methods ')\ng = sns.kdeplot(df['kolaskar_tongaonkar'].loc[df['target'] == 1], \n                shade= True, ax=axarr[1], label='kolaskar_tongaonkar').set_xlabel('Parker & Kolaskar Tongaonkar Methods')\ng = sns.kdeplot(df['parker'].loc[df['target'] == 0], \n                shade=True, ax=axarr[1], label='parker')","bc8709b4":"import matplotlib.style as style\n\ntrain_heat=df[df[\"target\"].notnull()]\ntrain_heat=train_heat.drop([\"target\"],axis=1)\nstyle.use('ggplot')\nsns.set_style('whitegrid')\nplt.subplots(figsize = (20,8))\n## Plotting heatmap. \n\n# Generate a mask for the upper triangle (taken from seaborn example gallery)\nmask = np.zeros_like(train_heat.corr(), dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n\nsns.heatmap(train_heat.corr(), \n            cmap=sns.diverging_palette(150, 275, l=60, n=7), \n            mask = mask, \n            annot=True, \n            center = 0, \n           );\n## Give title. \nplt.title(\"Heatmap of all the Features\", fontsize = 30);","bdf3cde7":"fig = px.bar(df, \n             x='chou_fasman', y='isoelectric_point', color_discrete_sequence=['crimson'],\n             title='Chou and Fasman Scale & Isoelectric Point', text='peptide_seq')\n\nfig.show()","7bd5e9a8":"fig = px.bar(df, \n             x='hydrophobicity', y='parker', color_discrete_sequence=['darkgreen'],\n             title='Parker Method Hydrophilic Scale', text='peptide_seq')\nfig.show()","b121bdc1":"fig = px.bar(df, \n             x='stability', y='emini', color_discrete_sequence=['purple'],\n             title='Emini Method & Stability', text='peptide_seq')\nfig.show()","e00b7060":"fig = px.bar(df, \n             x='kolaskar_tongaonkar', y='aromaticity', color_discrete_sequence=['#2B3A67'],\n             title='Kolaskar and Tongaonkar Antigenicity Scale', text='end_position')\nfig.show()","71620026":"fig = px.histogram(df1, x=\"parker\", color=\"hydrophobicity\", title='Parker Method Hydrophilic Scale')\nfig.update_layout(hovermode='x')\nfig.show()","22550889":"fig = px.violin(df1, y=\"kolaskar_tongaonkar\", color=\"isoelectric_point\", title='Kolaskar and Tongaonkar Antigenicity Scal')\nfig.show()","bf1ac215":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Mar\u00edlia Prata, @mpwolke Was here' )","82252730":"![](https:\/\/journals.plos.org\/plosone\/article\/figure\/image?size=inline&id=info:doi\/10.1371\/journal.pone.0078605.t004)https:\/\/journals.plos.org\/plosone\/article?id=10.1371\/journal.pone.0078605","2a4a4099":"#Chou and Fasman beta turn prediction\n\nReference: Chou PY, Fasman GD. Prediction of the secondary structure of proteins from their amino acid sequence. Adv Enzymol Relat Areas Mol Biol. 1978;47:45-148.\n\nDescription: The rationale for predicting turns to predict antibody epitopes is based on the paper by Pellequer et al, Immunology Letters. Instead of implementing the turn scale of that paper which has some non-standard properties, the authors decided to use the Chou and Fasman scale which is commonly used to predict beta turns .http:\/\/tools.iedb.org\/bcell\/help\/\n\nSecondary structural prediction of proteins from their amino acid sequence, by Peter Y. Chou and Gerald D. Fasman\n\nPredicted secondary structures of proteins (\u03b1-helix, \u03b2-pleated sheet and \u03b2-turns) give insight into the understanding of protein folding and biological activity.https:\/\/www.sciencedirect.com\/science\/article\/abs\/pii\/0968000477904406","e053fd91":"#Parker Hydrophilicity Prediction\n\nReference: Parker JM, Guo D, Hodges RS. New hydrophilicity scale derived from high-performance liquid chromatography peptide retention data: correlation of predicted surface residues with antigenicity and X-ray-derived accessible sites. Biochemistry. 1986 Sep 23; 25(19):5425-32.\n\nDescription: In this method, hydrophilic scale based on peptide retention times during high-performance liquid chromatography (HPLC) on a reversed-phase column was constructed.\n\nA window of seven residues was used for analyzing epitope region. The corresponding value of the scale was introduced for each of the seven residues and the arithmetical mean of the seven residue value was assigned to the fourth, (i+3), residue in the segment.http:\/\/tools.iedb.org\/bcell\/help\/\n\nSince hydrophilicity parameters have been used extensively in algorithms to predict which amino acids residues are antigenic, the authors have compared the profiles generated by their new set of hydrophilic HPLC (high-performance liquid chromatography) parameters on the same scale as nine other sets of parameters.","db11ca1b":"#Hydrophobic strip-of-helix algorithm for selection of T cell-presented peptides\n\nAuthors: C J Stille , L J Thomas, V E Reyes, R E Humphreys - PMID: 2825000 DOI: 10.1016\/0161-5890(87)90068-x\n\nIn extension of the hypothesis that an amphipathic alpha helix of Ii (Phe146-Val164) bound to the foreign antigen-presenting site (desetope) of class II MHC molecules through hydrophobic amino acid residues (Phe146, Leu150, Leu153, Met157, Ile160, Val164) which were present in an axial strip along one side of the Ii helix, the authors developed an algorithm to search for T cell-presented peptides showing a similar hydrophobic strip-of-helix. \n\nSuch peptides might bind to the class II MHC molecule site which was complementary to the Ii hydrophobic strip-of-helix. The strip-of-helix hydrophobicity index was the mean hydrophobicity (from Kyte-Doolittle values) of sets of amino acids in axial strips down sides of helices for 3-6 turns, at positions, n, n + 4, N + 7, n + 11, n + 14, and n + 18.\n\nPeptides correlating well with T cell responsiveness had: (1) 12-19 amino acids (3-5 cycles or 4-6 turns of an alpha helix), (2) a strip with highly hydrophobic residues, (3) adjacent, moderately hydrophilic strips, and (4) no prolines.\n\nThe degree of hydrophilicity of the remainder of a putative antigenic helix above a threshold value did not count in this index. That is, the magnitude of amphipathicity was not judged to be the principal selecting factor for T cell-presented peptides.\n\nThis simple algorithm to quantitate strip-of-helix hydrophobicity in a putative amphipathic alpha helix, allowing otherwise generally hydrophilic residues, predicted 10 of 12 T cell-presented peptides in seven well-studied proteins. The derivation and application of this algorithm were analyzed.https:\/\/pubmed.ncbi.nlm.nih.gov\/2825000\/","c08c7413":"#Spoiler alert: No missing Values.","5d176d81":"#Kolaskar and Tongaonkar antigenicity scale\n\nReference: Kolaskar AS, Tongaonkar PC. A semi-empirical method for prediction of antigenic determinants on protein antigens. FEBS Lett. 1990 Dec 10;276(1-2):172-4.\n\nDescription: A semi-empirical method which makes use of physicochemical properties of amino acid residues and their frequencies of occurrence in experimentally known segmental epitopes was developed to predict antigenic determinants on proteins. \n\nApplication of this method to a large number of proteins has shown by the authors that the method can predict antigenic determinants with about 75% accuracy which is better than most of the known methods.http:\/\/tools.iedb.org\/bcell\/help\/","876410de":"#The PentaFOLD 3.0 Algorithm for the selection of Stable Elements of Secondary Structure to be included in Vaccine Peptides\n\nAuthor(s): Vladislav Victorovich Khrustalev - Journal Name: Protein & Peptide Letters -\nDOI : 10.2174\/0929866527666201110123851\n\nAims: The aim of this study was to create a new version of the PentaFOLD algorithm and to test its performance experimentally in several proteins and peptides.\n\nBackground: Synthetic vaccines can cause production of neutralizing antibodies only in case if short peptides form the same secondary structure as fragments of full-length proteins. The PentaFOLD 3.0 algorithm was designed to check stability of alpha helices, beta strands, and random coils using several propensity scales obtained during analysis of 1730 3D structures of proteins.\n\nObjective: The algorithm has been tested in the three peptides known to keep the secondary structure of the corresponding fragments of full-length proteins: the NY25 peptide from the Influenza H1N1 hemagglutinin, the SF23 peptide from the diphtheria toxin, the NQ21 peptide from the HIV1 gp120; as well as in the CC36 peptide from the human major prion protein.\n\nMethod: Affine chromatography for antibodies against peptides accompanied by circular dichroism and fluorescence spectroscopy were used to check the predictions of the algorithm.\n\nResult: Immunological experiments showed that all abovementioned peptides are more or less immunogenic in rabbits. The fact that antibodies against the NY25, the SF23, and the NQ21 form stable complexes with corresponding full-length proteins has been confirmed by affine chromatography. The surface of SARS CoV-2 spike receptor-binding domain interacting with hACE2 has been shown to be unstable according to the results of the PentaFOLD 3.0.\n\nConclusion: The PentaFOLD 3.0 algorithm (http:\/\/chemres.bsmu.by\/PentaFOLD30.htm) can be used with the aim to design vaccine peptides with stable secondary structure elements.\nhttps:\/\/www.eurekaselect.com\/node\/187782\/article\/the-pentafold-30-algorithm-for-the-selection-of-stable-elements-of-secondary-structure-to-be-included-in-vaccine-peptides","d808598a":"#Antibody Epitope Prediction - SOURCE http:\/\/tools.iedb.org\/bcell\/help\/\n\nI. Methods for predicting continuous antibody epitope from protein sequences\n\nGeneral basis: Parameters such as hydrophilicity, flexibility, accessibility, turns, exposed surface, polarity and antigenic propensity of polypeptides chains have been correlated with the location of continuous epitopes. This has led to a search for empirical rules that would allow the position of continuous epitopes to be predicted from certain features of the protein sequence.\n\nGeneral method: When computing the score for a given residue i, the amino acids in an interval of the chosen length, centered around residue i, are considered. In other words, for a window size n, the i - (n-1)\/2 neighboring residues on each side of residue i were used to compute the score for residue i. Unless specified, the score for residue i is the average of the scale values for these amino acids (see table 1 for specific method implementation details). In general, a window size of 5 to 7 is appropriate for finding regions that may potentially be antigenic.\n\nMethods that were included in the files: Chou and Fasman ;; Emini;; Kolaskar and Tongaonkar;; Parker.\n\nMethods that were not in this Dataset files: Karplus and Schulz ;; Bepipred-1.0 Linear Epitope Prediction;; BepiPred-2.0: Sequential B-Cell Epitope Predictor.","bae8122e":"#Emini surface accessibility scale\n\nReference: Emini EA, Hughes JV, Perlow DS, Boger J. Induction of hepatitis A virus-neutralizing antibody by a virus-specific synthetic peptide. J Virol. 1985 Sep;55(3):836-9.\n\nDescription: The calculation was based on surface accessibility scale on a product instead of an addition within the window. The accessibility profile was obtained using the formulae Sn = (n+4+i ) (0.37)-6 where Sn is the surface probability, dn is the fractional surface probability value, and i vary from 1 to 6.\n\nA hexapeptide sequence with Sn greater than 1.0 indicates an increased probability for being found on the surface.http:\/\/tools.iedb.org\/bcell\/help\/","24b4a45b":"#The Three files: Covid, B Cell and Sars.","e639ea35":"#B cell epitopes\n\nB cell epitopes are most commonly discontinuous (also called conformational or assembled), consisting of segments of multiple chains brought together by the folding of the protein (antigen). Only about 10% of all epitopes recognized by antibodies are thought to be continuous (also called linear or sequential).\n\nCurrent state-of-the-art epitope prediction generally uses machine learning approaches. Larsen et al. trained a HIDDEN MARKOV MODEL (HMM) on epitopes in conjunction with random amino acid sequences. The antigenicity of amino acids was then derived from the ratios of their emission probabilities by the epitope HMM and the random HMM. The area under curve (AUC) of receiver operating characteristic (ROC) of such a classifier in combination with a hydrophobicity scale was 0.671. \n\nS\u00f6llner and Mayer used DECISION TREE and NEAREST NEIGHBOR MACHINE LEARNING algorithms. They utilized over 1,000 attributes related to relative positions of amino acids in the sequences and over 250 propensity scales. The post test probability (a measure similar to accuracy) of the best classifier was 69.31%. In the studies cited below, the classifiers used have been trained and tested on B cell epitopes from the Bcipep database as positive examples and random peptides as negative examples (except for Rubinstein et al., who apparently used epitope and non-epitope parts of the same antigens as positive and negative examples). \n\nSaha and Raghava  employed a RECURRENT NEURAL NETWORK. The accuracy of the trained network was 65.93%.\n\nChen et al. used the SUPPORT VECTOR MACHINE LEARNING algorithm. Their attributes were frequencies of amino acid pairs and five propensity scores. The resulting accuracy was 73.71%.\n\nEl-Manzalawy et al., used SUPPORT VECTOR machines in combination with a subsequence kernel. Their attributes were all subsequences of peptides up to a certain length, including subsequences with gaps. The AUC of the subsequence kernel classification was 0.812 and the accuracy 73.37% (original data set). They reimplemented Chen et al.\u2019s method, which yielded an AUC of 0.717 and an accuracy of 65.65%. The difference compared to the accuracy reported by Chen et al. is probably due to differences in the exact composition of the data set. Results reported by El-Manzalawy et al. confirm the superiority of the subsequence kernel classification.\n\nRubinstein et al. used the NAIVE BAYES MACHINE LEARNING algorithm. Their attributes were the frequencies of amino acids, the structural properties of proteins and a number of propensity scores. With all these attributes, the AUC was 0.55 and the accuracy was 70.6%. If only the best attributes were selected, the AUC and the accuracy increased to 0.59 and 80.4%, respectively. This large increase (in accuracy) may be due to the fact that apparently the whole data set was used in attribute selection instead of only the training set.https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","42544db3":"#Epitope Predictions Indicate the Presence of Two Distinct Types of Epitope-Antibody-Reactivities Determined by Epitope Profiling of Intravenous Immunoglobulins\n\nAuthors: Mitja Lu\u0161trek,  ,  Peter Lorenz,  Michael Kreutzer,  Zilliang Qian,  ,  Felix Steinbeck,  Di Wu,  ,  Nadine Born,  Bjoern Ziems,  Michael Hecker,  Miri Blank,  Yehuda Shoenfeld,  Zhiwei Cao,  Michael O. Glocker,  Yixue Li,  Georg Fuellen,  and Hans-J\u00fcrgen Thiesen.\n\nPLoS One. 2013; 8(11): e78605.\nPublished online 2013 Nov 11. doi: 10.1371\/journal.pone.0078605\n\nEpitope-antibody-reactivities (EAR) of intravenous immunoglobulins (IVIGs) determined for 75,534 peptides by microarray analysis demonstrate that roughly 9% of peptides derived from 870 different human protein sequences react with antibodies present in IVIG.\n\nComputational prediction of linear B cell epitopes was conducted using machine learning with an ensemble of classifiers in combination with position weight matrix (PWM) analysis. Machine learning slightly outperformed PWM with area under the curve (AUC) of 0.884 vs. 0.849.\nhttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","ce5b664e":"#Computationally-driven Identification of Antibody Epitopes\n\nAuthors: Casey K Hua, Albert T Gacerez, Charles L Sentman, Margaret E Ackerman, Yoonjoo Choi, Chris Bailey-Kellogg -  \nDartmouth College, United States; Korea Advanced Institute of Science and Technology (KAIST), Republic of Korea.\nhttps:\/\/doi.org\/10.7554\/eLife.29023.001\n\nUnderstanding where antibodies recognize antigens can help define mechanisms of action and provide insights into progression of immune responses. The authors investigated the extent to which information about binding specificity implicitly encoded in amino acid sequence can be leveraged to identify antibody epitopes.\n\nIn computationally-driven epitope localization, possible antibody\u2013antigen binding modes are modeled, and targeted panels of antigen variants are designed to experimentally test these hypotheses. Prospective application of this approach to two antibodies enabled epitope localization using five or fewer variants per antibody, or alternatively, a six-variant panel for both simultaneously.\n\nRetrospective analysis of a variety of antibodies and antigens demonstrated an almost 90% success rate with an average of three antigen variants, further supporting the observation that the combination of computational modeling and protein design can reveal key determinants of antibody\u2013antigen binding and enable efficient studies of collections of antibodies identified from polyclonal samples or engineered libraries.\nhttps:\/\/elifesciences.org\/articles\/29023","88fd289b":"#Isoelectric Point Estimation, Amino Acid Sequence and Algorithms, By Amanda Maxwell\n\nThe isoelectric point, or pI,represents a point of balance for a molecule, where the external surface charge is a net zero. This factor governs electrophoretic mobility in proteins and also plays a role in identifying peptides from mass spectral proteomics data.\n\npI depends on a number of factors, including amino acid sequence, post-translational modifications (PTMs) and presence of side chain\u2014all of which can alter surface charge and behavior depending on the pH of the environment.\n\nVarious methods for predicting pI in denatured proteins exist, and most base this calculation on amino acid sequence with reference to pKa values recorded for ionizable constituents. Although these predictive methods exist, their performance can be variable and may skew ensuing results.https:\/\/www.thermofisher.com\/blog\/proteomics\/isoelectric-point-estimation-amino-acid-sequence-and-algorithms\/","b354785c":"#Overview of computationally-driven epitope identification by EpiScope.\n\n(A) Ab\u2013Ag docking models are generated using computational docking methods. In the example, the green structure is the Ag human IL-18 (PDB ID: 2VXT:A), while the cartoons represent possible poses of the Ab (limited here to three for clarity). Full details including docking models and designs for this example are provided in a PyMol session. (B) Ag variants containing a pre-defined numbers of mutations (here triple mutations, colored triangles) are generated for each docking model. (C) Variants are clustered with respect to spatial locations in the Ag, and a set of variants predicted to disrupt all of the docking models is selected. (D) Ag mutagenesis and Ag-Ab binding experiments are performed to identify which mutations result in loss of Ab recognition. (E) Examination of the disruptive variant(s) enables localization of the Ab epitope in terms of both mutated positions (pink balls) and consistent docking models, here with the model (light pink cartoon) quite similar to the actual crystal structure (dark pink cartoon).\n\nhttps:\/\/doi.org\/10.7554\/eLife.29023.002\n\n![](https:\/\/iiif.elifesciences.org\/lax:29023%2Felife-29023-fig1-v2.tif\/full\/617,\/0\/default.webp)https:\/\/elifesciences.org\/articles\/29023","d005c341":"#The unexpectedly high classification cross-validation accuracy on the unclassifiable peptides.\n\nThe 1st degree classifiable and unclassifiable peptides are exactly the opposite of each other with respect to the attributes most useful for classification. For example, if a peptide is classifiable, it is likely to bind if it has a HIGH AROMACITY; if it is unclassifiable, it is likely to bind if it has a LOW Aromaticity.\n\nBecause of this opposite behavior, a classifier that correctly classifies one of the two groups must fail on the other. The classifier that was trained on the whole training set correctly classified the larger group (classifiable) and failed on the smaller one (unclassifiable). However, when the classifier that was trained on the 1st degree unclassifiable peptides was used, it no longer faced the contradiction between the groups and thus performed well.\nhttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC3823795\/","6a162bdf":"#The snippet above was made for Titanic Competition, therefore the KDE was suppose to be used when you have sort of Yes\/No, Survived\/Not Survived. Otherwise, the Kde will sink. Just like mine above.  "}}