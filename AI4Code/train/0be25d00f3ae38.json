{"cell_type":{"37919d6f":"code","e56a7d16":"code","8910c0b9":"code","63dda7d0":"code","4e967a8a":"code","1ad6ebc5":"code","f389cfd5":"code","2ae1bd09":"code","e11de6bf":"code","8678f89d":"code","0d3339d9":"code","bbf8a7e5":"code","9486ba08":"code","c4074202":"code","43624ac1":"markdown","9f44b956":"markdown","0a46604c":"markdown"},"source":{"37919d6f":"import os, time\nfrom PIL import Image, ImageOps\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nfrom torchvision import transforms\nimport torch.optim as optim\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","e56a7d16":"batch_size = 32\nlr = 0.0002\nL1_lambda = 100\nbeta1 = 0.5\nbeta2 = 0.999\ninverse_order = True\nimg_size = 256\n\ntrain_epoch = 150\n\nsave_root = '.\/results'\nif not os.path.isdir(save_root):\n    os.mkdir(save_root)","8910c0b9":"class generator(nn.Module):\n    # initializers\n    def __init__(self, d=64):\n        super(generator, self).__init__()\n        # Unet encoder\n        self.conv1 = nn.Conv2d(3, d, 4, 2, 1)\n        self.conv2 = nn.Conv2d(d, d * 2, 4, 2, 1)\n        self.conv2_bn = nn.BatchNorm2d(d * 2)\n        self.conv3 = nn.Conv2d(d * 2, d * 4, 4, 2, 1)\n        self.conv3_bn = nn.BatchNorm2d(d * 4)\n        self.conv4 = nn.Conv2d(d * 4, d * 8, 4, 2, 1)\n        self.conv4_bn = nn.BatchNorm2d(d * 8)\n        self.conv5 = nn.Conv2d(d * 8, d * 8, 4, 2, 1)\n        self.conv5_bn = nn.BatchNorm2d(d * 8)\n        self.conv6 = nn.Conv2d(d * 8, d * 8, 4, 2, 1)\n        self.conv6_bn = nn.BatchNorm2d(d * 8)\n        self.conv7 = nn.Conv2d(d * 8, d * 8, 4, 2, 1)\n        self.conv7_bn = nn.BatchNorm2d(d * 8)\n        self.conv8 = nn.Conv2d(d * 8, d * 8, 4, 2, 1)\n        # self.conv8_bn = nn.BatchNorm2d(d * 8)\n\n        # Unet decoder\n        self.deconv1 = nn.ConvTranspose2d(d * 8, d * 8, 4, 2, 1)\n        self.deconv1_bn = nn.BatchNorm2d(d * 8)\n        self.deconv2 = nn.ConvTranspose2d(d * 8 * 2, d * 8, 4, 2, 1)\n        self.deconv2_bn = nn.BatchNorm2d(d * 8)\n        self.deconv3 = nn.ConvTranspose2d(d * 8 * 2, d * 8, 4, 2, 1)\n        self.deconv3_bn = nn.BatchNorm2d(d * 8)\n        self.deconv4 = nn.ConvTranspose2d(d * 8 * 2, d * 8, 4, 2, 1)\n        self.deconv4_bn = nn.BatchNorm2d(d * 8)\n        self.deconv5 = nn.ConvTranspose2d(d * 8 * 2, d * 4, 4, 2, 1)\n        self.deconv5_bn = nn.BatchNorm2d(d * 4)\n        self.deconv6 = nn.ConvTranspose2d(d * 4 * 2, d * 2, 4, 2, 1)\n        self.deconv6_bn = nn.BatchNorm2d(d * 2)\n        self.deconv7 = nn.ConvTranspose2d(d * 2 * 2, d, 4, 2, 1)\n        self.deconv7_bn = nn.BatchNorm2d(d)\n        self.deconv8 = nn.ConvTranspose2d(d * 2, 3, 4, 2, 1)\n        self.tanh = nn.Tanh()\n\n    # weight_init\n    def weight_init(self, mean, std):\n        for m in self._modules:\n            normal_init(self._modules[m], mean, std)\n\n    # forward method\n    def forward(self, input):\n        e1 = self.conv1(input)\n        e2 = self.conv2_bn(self.conv2(F.leaky_relu(e1, 0.2)))\n        e3 = self.conv3_bn(self.conv3(F.leaky_relu(e2, 0.2)))\n        e4 = self.conv4_bn(self.conv4(F.leaky_relu(e3, 0.2)))\n        e5 = self.conv5_bn(self.conv5(F.leaky_relu(e4, 0.2)))\n        e6 = self.conv6_bn(self.conv6(F.leaky_relu(e5, 0.2)))\n        e7 = self.conv7_bn(self.conv7(F.leaky_relu(e6, 0.2)))\n        e8 = self.conv8(F.leaky_relu(e7, 0.2))\n        # e8 = self.conv8_bn(self.conv8(F.leaky_relu(e7, 0.2)))\n        d1 = F.dropout(self.deconv1_bn(self.deconv1(F.relu(e8))), 0.5, training=True)\n        d1 = torch.cat([d1, e7], 1)\n        d2 = F.dropout(self.deconv2_bn(self.deconv2(F.relu(d1))), 0.5, training=True)\n        d2 = torch.cat([d2, e6], 1)\n        d3 = F.dropout(self.deconv3_bn(self.deconv3(F.relu(d2))), 0.5, training=True)\n        d3 = torch.cat([d3, e5], 1)\n        d4 = self.deconv4_bn(self.deconv4(F.relu(d3)))\n        # d4 = F.dropout(self.deconv4_bn(self.deconv4(F.relu(d3))), 0.5)\n        d4 = torch.cat([d4, e4], 1)\n        d5 = self.deconv5_bn(self.deconv5(F.relu(d4)))\n        d5 = torch.cat([d5, e3], 1)\n        d6 = self.deconv6_bn(self.deconv6(F.relu(d5)))\n        d6 = torch.cat([d6, e2], 1)\n        d7 = self.deconv7_bn(self.deconv7(F.relu(d6)))\n        d7 = torch.cat([d7, e1], 1)\n        d8 = self.deconv8(F.relu(d7))\n        o = self.tanh(d8)\n\n        return o\n\nclass discriminator(nn.Module):\n    # initializers\n    def __init__(self, d=64):\n        super(discriminator, self).__init__()\n        self.conv1 = nn.Conv2d(6, d, 4, 2, 1)\n        self.conv2 = nn.Conv2d(d, d * 2, 4, 2, 1)\n        self.conv2_bn = nn.BatchNorm2d(d * 2)\n        self.conv3 = nn.Conv2d(d * 2, d * 4, 4, 2, 1)\n        self.conv3_bn = nn.BatchNorm2d(d * 4)\n        self.conv4 = nn.Conv2d(d * 4, d * 8, 4, 1, 1)\n        self.conv4_bn = nn.BatchNorm2d(d * 8)\n        self.conv5 = nn.Conv2d(d * 8, 1, 4, 1, 1)\n        self.sigmoid = nn.Sigmoid()\n\n    # weight_init\n    def weight_init(self, mean, std):\n        for m in self._modules:\n            normal_init(self._modules[m], mean, std)\n\n    # forward method\n    def forward(self, input, label):\n        x = torch.cat([input, label], 1)\n        x = F.leaky_relu(self.conv1(x), 0.2)\n        x = F.leaky_relu(self.conv2_bn(self.conv2(x)), 0.2)\n        x = F.leaky_relu(self.conv3_bn(self.conv3(x)), 0.2)\n        x = F.leaky_relu(self.conv4_bn(self.conv4(x)), 0.2)\n        x = self.sigmoid(self.conv5(x))\n\n        return x\n\ndef normal_init(m, mean, std):\n    if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n        m.weight.data.normal_(mean, std)\n        m.bias.data.zero_()","63dda7d0":"train_img_nms = os.listdir('\/kaggle\/input\/cityscapes-image-pairs\/cityscapes_data\/cityscapes_data\/train')\nval_img_nms = os.listdir('\/kaggle\/input\/cityscapes-image-pairs\/cityscapes_data\/cityscapes_data\/val')\n\n# tmp_img = Image.open('..\/input\/cityscapes-image-pairs\/cityscapes_data\/train\/1.jpg')\n# img = tmp_img.crop((0, 0, 256, 256))\n# seg = tmp_img.crop((256, 0, 512, 256))\n# tmp_img","4e967a8a":"data_root_dir = '\/kaggle\/input\/cityscapes-image-pairs\/cityscapes_data\/cityscapes_data\/'\nclass CityscapesDataset(Dataset):\n    def __init__(self, img_nms, data_root_dir, split, transforms=None, augm=True):\n        super().__init__()\n        self.img_nms = img_nms\n        self.data_root_dir = data_root_dir\n        self.transforms = transforms\n        self.split = split\n        self.augm = augm\n        \n    def __len__(self):\n        return len(self.img_nms)\n        \n    def __getitem__(self, idx):\n        tmp_img = Image.open(os.path.join(self.data_root_dir, self.split, self.img_nms[idx]))\n        img = tmp_img.crop((0, 0, 256, 256))\n        seg = tmp_img.crop((256, 0, 512, 256))\n        if self.augm:\n            img, seg = self._random_crop(img, seg)\n            img, seg = self._random_fliph(img, seg)\n        if self.transforms:\n            img = self.transforms(img)\n            seg = self.transforms(seg)\n        return seg, img\n    \n    def _random_crop(self, img, seg, crop_size=(224, 224)): # inputs are PIL images\n        crop = np.random.rand() > 0.1\n        if crop:\n            w, h = img.size\n            new_h, new_w = crop_size\n\n            top = np.random.randint(0, h - new_h)\n            left = np.random.randint(0, w - new_w)\n\n            img = img.crop((left, top, left + new_w, top + new_h))\n            seg = seg.crop((left, top, left + new_w, top + new_h))\n            return img, seg\n        else:\n            return img, seg\n    \n    def _random_fliph(self, img, seg): # inputs are PIL images\n        flip = np.random.rand() > 0.5\n        if flip:\n            return ImageOps.mirror(img), ImageOps.mirror(seg)\n        else:\n            return img, seg","1ad6ebc5":"transform = transforms.Compose(\n    [\n        transforms.Resize((img_size, img_size)),\n        transforms.CenterCrop(img_size),\n        transforms.ToTensor(),\n    ]\n)","f389cfd5":"def get_concat_h(im1, im2):\n    dst = Image.new('RGB', (im1.width + im2.width, im1.height))\n    dst.paste(im1, (0, 0))\n    dst.paste(im2, (im1.width, 0))\n    return dst","2ae1bd09":"# val img\nval_dataset = CityscapesDataset(['1.jpg'], data_root_dir, 'val', transform, augm=False)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size)\nfixed_x_, fixed_y_ = iter(val_dataloader).next()\nto_pil = transforms.ToPILImage()\nseg_pil = to_pil(fixed_x_.to('cpu')[0])\nimg_pil = to_pil(fixed_y_.to('cpu')[0])\ndisplay(get_concat_h(img_pil,seg_pil))\nfixed_x_, fixed_y_ = Variable(fixed_x_.to(device)), Variable(fixed_y_.to(device))","e11de6bf":"train_dataset = CityscapesDataset(train_img_nms, data_root_dir, 'train', transform)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n\n# network\nG = generator()\nD = discriminator()\nG.weight_init(mean=0.0, std=0.02)\nD.weight_init(mean=0.0, std=0.02)\nG = G.to(device)\nD = D.to(device)\n\n# loss\nBCE_loss = nn.BCELoss().to(device)\nL1_loss = nn.L1Loss().to(device)\n\n# Adam optimizer\nG_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(beta1, beta2))\nD_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(beta1, beta2))\n\ntrain_hist = {}\ntrain_hist['D_losses'] = []\ntrain_hist['G_losses'] = []\ntrain_hist['D_result'] = []\ntrain_hist['per_epoch_ptimes'] = []\ntrain_hist['total_ptime'] = []","8678f89d":"print('training start!')\nstart_time = time.time()\nfor epoch in range(train_epoch):\n    D_losses = []\n    G_losses = []\n    epoch_start_time = time.time()\n    num_iter = 0\n    G.train()\n    D.train()\n    for x_, y_ in train_dataloader:\n        # train discriminator D\n        D.zero_grad()\n\n        x_, y_ = Variable(x_.to(device)), Variable(y_.to(device))\n\n        D_result = D(x_, y_).squeeze()\n        D_real_loss = BCE_loss(D_result, Variable(torch.ones(D_result.size()).to(device)))\n\n        G_result = G(x_)\n        D_result = D(x_, G_result).squeeze()\n        D_fake_loss = BCE_loss(D_result, Variable(torch.zeros(D_result.size()).to(device)))\n\n        D_train_loss = (D_real_loss + D_fake_loss) * 0.5\n        D_train_loss.backward()\n        D_optimizer.step()\n\n        train_hist['D_losses'].append(D_train_loss.item())\n\n        D_losses.append(D_train_loss.item())\n\n        # train generator G\n        G.zero_grad()\n\n        G_result = G(x_)\n        D_result = D(x_, G_result).squeeze()\n        train_hist['D_result'].append(D_result.mean().item())\n\n        G_train_loss = BCE_loss(D_result, Variable(torch.ones(D_result.size()).to(device))) + L1_lambda * L1_loss(G_result, y_)\n        G_train_loss.backward()\n        G_optimizer.step()\n\n        train_hist['G_losses'].append(G_train_loss.item())\n\n        G_losses.append(G_train_loss.item())\n\n        num_iter += 1\n\n    epoch_end_time = time.time()\n    per_epoch_ptime = epoch_end_time - epoch_start_time\n\n    print('[%d\/%d] - ptime: %.2f, loss_d: %.3f, loss_g: %.3f' % ((epoch + 1), train_epoch, per_epoch_ptime, torch.mean(torch.FloatTensor(D_losses)),\n                                                              torch.mean(torch.FloatTensor(G_losses))))\n    if epoch % 10 == 0:\n        G.eval()\n        G_fixed_result = G(fixed_x_)\n        results_img = get_concat_h(to_pil(fixed_y_.to('cpu')[0]), to_pil(G_fixed_result.to('cpu')[0]))\n        results_img = get_concat_h(results_img, to_pil(fixed_x_.to('cpu')[0]))\n#         display(results_img)\n        fixed_p = os.path.join(save_root, str(epoch + 1).zfill(4) + '.png')\n        results_img.save(fixed_p)\n    train_hist['per_epoch_ptimes'].append(per_epoch_ptime)\n\nend_time = time.time()\ntotal_ptime = end_time - start_time\ntrain_hist['total_ptime'].append(total_ptime)\nprint('minute', total_ptime \/ 60)","0d3339d9":"if not os.path.isdir('.\/model'):\n    os.mkdir('.\/model')\nG_model_path = f'.\/model\/G_{epoch}.pth'\ntorch.save(G.state_dict(), G_model_path)\nD_model_path = f'.\/model\/D_{epoch}.pth'\ntorch.save(D.state_dict(), D_model_path)","bbf8a7e5":"import matplotlib.pyplot as plt\nplt.plot(list(range(len(train_hist['D_losses']))), train_hist['D_losses'], label=\"D_losses\")\nplt.legend()","9486ba08":"plt.plot(list(range(len(train_hist['D_losses']))), train_hist['G_losses'], label=\"G_losses\")\nplt.legend()","c4074202":"plt.plot(list(range(len(train_hist['D_losses']))), train_hist['D_result'], label=\"D_resultb\")\nplt.legend()","43624ac1":"transform = transforms.Compose([\n    transforms.Resize((img_size, img_size)),\n        transforms.ToTensor(),\n])\n","9f44b956":"https:\/\/github.com\/znxlwm\/pytorch-pix2pix","0a46604c":"# https:\/\/datumstudio.jp\/blog\/matplotlib-2%E8%BB%B8%E3%82%B0%E3%83%A9%E3%83%95%E3%81%AE%E6%9B%B8%E3%81%8D%E6%96%B9\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nfig, ax1 = plt.subplots()\n \n# ax1\u3068ax2\u3092\u95a2\u9023\u3055\u305b\u308b\nax2 = ax1.twinx()\n \n# \u305d\u308c\u305e\u308c\u306eaxes\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u306elines\u5c5e\u6027\u306bLine2D\u30aa\u30d6\u30b8\u30a7\u30af\u30c8\u3092\u8ffd\u52a0\nax1.plot(list(range(len(train_hist['G_losses']))), train_hist['D_losses'],\n         color=cm.Set1.colors[0], label=\"D_losses\")\nax2.plot(list(range(len(train_hist['G_losses']))), train_hist['G_losses'],\n         color=cm.Set1.colors[1], label=\"G_losses\")\n \n# \u51e1\u4f8b\n# \u30b0\u30e9\u30d5\u306e\u672c\u4f53\u8a2d\u5b9a\u6642\u306b\u3001\u30e9\u30d9\u30eb\u3092\u624b\u52d5\u3067\u8a2d\u5b9a\u3059\u308b\u5fc5\u8981\u304c\u3042\u308b\u306e\u306f\u3001barplot\u306e\u307f\u3002plot\u306f\u81ea\u52d5\u3067\u8a2d\u5b9a\u3055\u308c\u308b\uff1e\nhandler1, label1 = ax1.get_legend_handles_labels()\nhandler2, label2 = ax2.get_legend_handles_labels()\n# \u51e1\u4f8b\u3092\u307e\u3068\u3081\u3066\u51fa\u529b\u3059\u308b\nax1.legend(handler1 + handler2, label1 + label2, loc=2, borderaxespad=0.)\n \n# pageview_max = 3 * max(frame[\"pageview\"])\n# register_max = 1.2 * max(frame[\"register\"])\n \n# ax1.set_ylim([0, pageview_max])\n# ax2.set_ylim([0, register_max])"}}