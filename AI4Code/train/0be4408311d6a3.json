{"cell_type":{"ca46457e":"code","90d85f26":"code","48a746eb":"code","84ea5f3d":"code","9a0b7132":"code","319fb109":"code","e7dda005":"code","e7d3b30e":"code","561b40f7":"code","339ee943":"code","6f51dcd1":"code","249abe3a":"code","f8639e87":"code","e0a1c6a4":"code","45755eba":"code","81d44df8":"markdown"},"source":{"ca46457e":"import keras\nfrom gensim.models import KeyedVectors\n!wget -P \/root\/input\/ -c \"https:\/\/s3.amazonaws.com\/dl4j-distribution\/GoogleNews-vectors-negative300.bin.gz\"\nEMBEDDING_FILE = '\/root\/input\/GoogleNews-vectors-negative300.bin.gz' # from above\nword2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)","90d85f26":"result = word2vec.most_similar_cosmul(positive=['actor','woman'], negative=['man'] )\nresult[0]","48a746eb":"from keras.datasets import imdb\nfrom keras.preprocessing import sequence\n\nmax_features = 10000  # number of words to consider as features\nmaxlen = 500  # cut texts after this number of words (among top max_features most common words)\nbatch_size = 32\n\nprint('Loading data...')\n(input_train, y_train), (input_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(input_train), 'train sequences')\nprint(len(input_test), 'test sequences')\n\nprint('Pad sequences (samples x time)')\ninput_train = sequence.pad_sequences(input_train, maxlen=maxlen)\ninput_test = sequence.pad_sequences(input_test, maxlen=maxlen)\nprint('input_train shape:', input_train.shape)\nprint('input_test shape:', input_test.shape)","84ea5f3d":"from keras.layers import Dense, Bidirectional, SimpleRNN, Embedding, Dropout, LSTM\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nes=EarlyStopping(patience=4,min_delta=0.001, mode='max', monitor='val_acc')\nimport matplotlib.pyplot as plt\ndef overfit(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(len(acc))\n\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n\n    plt.figure()\n\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\n    plt.show()","9a0b7132":"\nmetrics=[]\nmodel = Sequential()\nmodel.add(Embedding(10000, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n                    epochs=10,\n                    batch_size=128,\n                    validation_split=0.2,callbacks=[es])\n\n","319fb109":"\n\noverfit(history)\nm=model.evaluate(input_test,y_test)\nprint(m)\nmetrics.append({\"single RNN\": m})","e7dda005":"from keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(LSTM(32))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n                    epochs=10,\n                    batch_size=128,\n                    validation_split=0.2,callbacks=[es])\noverfit(history)\nm=model.evaluate(input_test,y_test)\nprint(m)\nmetrics.append({\"small LSTM\": m})","e7d3b30e":"from keras.layers import Dense, Bidirectional, SimpleRNN, Embedding, Dropout\nmodel = Sequential()\nmodel.add(Embedding(10000, 32))\nmodel.add(Bidirectional(LSTM(64, return_sequences=True)))\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nmodel.summary()\n\nhistory = model.fit(input_train, y_train,\n                    epochs=20,\n                    batch_size=128,\n                    validation_split=0.2,callbacks=[es])\n\noverfit(history)\nm=model.evaluate(input_test,y_test)\nprint(m)\nmetrics.append({\"Bi LSTM\": m})\n","561b40f7":"model = Sequential()\nmodel.add(Embedding(10000, 32))\nmodel.add(LSTM(128, return_sequences=True))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['acc'])\nhistory = model.fit(input_train, y_train,\n                    epochs=10,\n                    batch_size=128,\n                    validation_split=0.2,callbacks=[es])\noverfit(history)\nm=model.evaluate(input_test,y_test)\nprint(m)\nmetrics.append({\"large LSTM\": m})","339ee943":"plt.title(\"Models Accuracy\")\nplt.bar([list(i)[0] for i in metrics], [list(i.values())[0][1] for i in metrics])","6f51dcd1":"import tensorflow\nmodel = Sequential()\nmodel.add(Embedding(10000, 32))\nmodel.add(LSTM(64,dropout=0.2))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.00001),\n              loss='binary_crossentropy',\n              metrics=['acc'])\nmodel.summary()\n\nhistory = model.fit(input_train, y_train,\n                    epochs=80,\n                    batch_size=128,\n                    validation_split=0.2,callbacks=[es])\n\noverfit(history)\nm=model.evaluate(input_test,y_test)","249abe3a":"import tensorflow\nes=EarlyStopping(patience=3,min_delta=0.001, mode='max', monitor='val_acc')\nmodel = Sequential()\n\nmodel.add(Embedding(10000,32))\n\nmodel.add(Bidirectional(LSTM(64,dropout=0.2, return_sequences=False)))\n#model.add(Bidirectional(LSTM(32,dropout=0.2, return_sequences=True)))\nmodel.add(Dense(32))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=tensorflow.keras.optimizers.Adam(learning_rate=0.0001),\n              loss='binary_crossentropy',\n              metrics=['acc'])\nmodel.summary()\n\nhistory = model.fit(input_train, y_train,\n                    epochs=40,\n                    batch_size=128,\n                    validation_split=0.2,callbacks=[es])\n\noverfit(history)\nm=model.evaluate(input_test,y_test)","f8639e87":"import numpy as np\nfrom gensim.test.utils import common_texts\nfrom gensim.models import Word2Vec\ntest_sentences = [\n  \"That movie was absolutely awful\",\n  \"The acting was a bit lacking\",\n  \"The film was creative and surprising\",\n  \"Absolutely fantastic!\",\n  \"This movie is not worth the money\",\n  \"The only positive thing with this movie is the music\"\n]\n\ntext=[]\nfor i in test_sentences:\n    phrase=[]\n    for u in i.split(\" \"):\n        phrase.append(u)\n    text.append(phrase)\nb=tensorflow.keras.datasets.imdb.get_word_index(path=\"imdb_word_index.json\")\n    \n","e0a1c6a4":"vecs=[]\n\nfor i in text:\n    #arr=np.zeros((10))\n    arr=[]\n    for u in range(len(i)):\n        try:\n            mm=i[u].lower()\n            \n            #arr[u]=b[mm]\n            arr.append(b[mm])\n        except:\n            #arr[u]=b['the']\n            arr.append(1)\n    #vecs.append(arr.astype(\"int\"))\n    vecs.append(arr)\n\nvecs=tensorflow.keras.preprocessing.sequence.pad_sequences(vecs, maxlen=10,value=0, padding=\"post\")\nprint(vecs)","45755eba":"preds=model.predict_classes(vecs)\n\nfor i in range(len(test_sentences)):\n    \n    print(test_sentences[i] + \" : \"+str(preds[i]))","81d44df8":"# Part 2"}}