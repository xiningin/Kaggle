{"cell_type":{"e14e8c22":"code","18dde172":"code","c9280d9c":"code","a6816261":"code","f19038aa":"code","4dae091c":"code","b5c4849d":"code","f0131339":"code","856ad60d":"code","cd3a6476":"code","8cc5240e":"code","d20b49f4":"code","bb072821":"code","a0874dd3":"code","270b1e09":"code","892ddca2":"code","c48e42aa":"code","25e0719d":"code","c3cd4abc":"code","3fdcf59e":"code","ab8bb6d9":"code","d1513d5d":"code","22cd86ce":"code","8e09890a":"code","92d9fc66":"code","0e66b8fa":"code","2d1946bf":"code","56d92cb3":"code","a4768a66":"code","78415390":"code","5b4d695f":"code","c4b9f42d":"code","5d392be2":"code","b57191d9":"code","ff28f7d6":"code","8dcd1fef":"code","67d90e87":"code","3ab4f0fb":"code","4f6b57ea":"code","5a399c5e":"code","cdce3402":"code","819e7927":"code","39d48606":"code","5c5d1f5d":"code","662a5394":"code","e3d58cf9":"code","8bf866a2":"code","9530dbe6":"code","fb6f3f64":"code","5abde8c2":"code","bd7de681":"code","3a03ee2e":"code","7b136bae":"code","c969a92d":"code","f8769286":"code","1f5ef0ce":"code","65e6c4ec":"code","c97b8b42":"code","7d5774b4":"code","c878e6ff":"code","35132036":"code","243a2143":"code","8f79bac6":"code","7a26e720":"markdown","d0bc728c":"markdown","39262d85":"markdown","36e4f2ac":"markdown","d0175cc6":"markdown","f54c8003":"markdown","461164d3":"markdown","d74cad4f":"markdown","24bed2de":"markdown","70ba5341":"markdown","8f5e6f51":"markdown","2914f405":"markdown","0e6156e2":"markdown","7422253d":"markdown","35a22048":"markdown","1c317146":"markdown","a50c679f":"markdown","e98a9378":"markdown","4bfcf2b1":"markdown","943cd17a":"markdown","07a66a79":"markdown","234c9a95":"markdown","10b206ea":"markdown","dc1710cd":"markdown","3dee4548":"markdown","04ebc64d":"markdown","0018ff94":"markdown","108d3687":"markdown","a2b7167c":"markdown","d35eb0b2":"markdown","a0a30e3d":"markdown","5c27a468":"markdown","70bebd9f":"markdown","f13407cd":"markdown","3966b118":"markdown","921256d4":"markdown","e0592c8c":"markdown","5e78d925":"markdown","8744af13":"markdown","7aca6ca8":"markdown","050af404":"markdown","943f808c":"markdown","37caae03":"markdown","782b029a":"markdown","3305c075":"markdown","0c7acf06":"markdown","0ae3fe20":"markdown","f325d038":"markdown","fa70156c":"markdown"},"source":{"e14e8c22":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n","18dde172":"df = pd.read_csv('..\/input\/bank-marketing-analysis\/bank-additional-full.csv', header=0,sep=\";\")\ndf.head(10)\ndf.shape","c9280d9c":"df.head(10)","a6816261":"df.info()","f19038aa":"df.isna().any()","4dae091c":"for column in df.columns:\n    print(column,df[column].nunique())","b5c4849d":"cat_features = [col for col, dtype in df.dtypes.items() if dtype == 'object']","f0131339":"plt.figure(figsize=(25, 80), facecolor='white')\nplotnumber =1\nfor cat_feature in cat_features:\n    ax = plt.subplot(12,3,plotnumber)\n    sns.countplot(y=cat_feature, data=df, palette='pastel')\n    plt.xlabel(cat_feature)\n    plt.title(cat_feature)\n    plotnumber+=1\nplt.show()","856ad60d":"plt.figure(figsize=(25, 80), facecolor='white')\nplotnumber =1\nfor cat_feature in cat_features:\n    ax = plt.subplot(12,3,plotnumber)\n    sns.countplot(y=cat_feature, hue='y', palette='pastel', edgecolor='.6', data=df)\n    plt.xlabel(cat_feature)\n    plt.title(cat_feature)\n    plotnumber+=1\nplt.show()","cd3a6476":"num_features = [col for col, dtype in df.dtypes.items() if dtype == 'int64' or dtype == 'float64']","8cc5240e":"plt.figure(figsize=(20,60), facecolor='white')\nplotnumber =1\nfor num_feature in num_features:\n    ax = plt.subplot(12,3,plotnumber)\n    sns.kdeplot(df[num_feature], bw=1.5)\n    plt.xlabel(num_feature)\n    plotnumber+=1\nplt.show()","d20b49f4":"plt.figure(figsize=(20,60))\nplotnumber =1\nfor num_feature in num_features:\n    ax = plt.subplot(12,3,plotnumber)\n    sns.boxplot(data = df, x = num_feature, palette='pastel')\n    plt.xlabel(num_feature)\n    plotnumber+=1\nplt.show()","bb072821":"plt.figure(figsize = (6, 4))\nsns.countplot(data = df, x = 'y')\nplt.tight_layout()","a0874dd3":"df['y'].groupby(df['y']).count()","270b1e09":"df.groupby('y').mean()","892ddca2":"# Categorical boolean mask\ncategorical_feature_mask = df.dtypes==object\n# filter categorical columns using mask\ncategorical_cols = df.columns[categorical_feature_mask].tolist()","c48e42aa":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","25e0719d":"# apply le on categorical feature columns\ndf[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\ndf[categorical_cols].head(10)","c3cd4abc":"df.head()","3fdcf59e":"plt.figure(figsize=(20, 20))\ncorr = df.corr()\nsns.heatmap(corr, fmt='.2f',annot=True)","ab8bb6d9":"df1=df.copy()","d1513d5d":"df1.groupby(['y','pdays']).size()","22cd86ce":"df1.drop(['pdays'],axis=1, inplace=True)","8e09890a":"df1.groupby(['y','campaign'],sort=True)['campaign'].count()","92d9fc66":"df2 = df1[df1['campaign'] < 37]","0e66b8fa":"X = df2.iloc[:, :-1]\nX.head()","2d1946bf":"y = df2.iloc[:, -1]\ny","56d92cb3":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=80, max_features='auto')\nrf.fit(X, y)\nprint('Training done using Random Forest')\n\nranking = np.argsort(-rf.feature_importances_)\nf, ax = plt.subplots(figsize=(11, 11))\nsns.barplot(x=rf.feature_importances_[ranking], y=X.columns.values[ranking], orient='h')\nax.set_xlabel(\"feature importance\")\nplt.tight_layout()\nplt.show()","a4768a66":"X_main = X.iloc[:,ranking[1:11]]\ny_main = y","78415390":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE()\nX_sm, y_sm = smote.fit_sample(X_main, y_main)\n","5b4d695f":"plt.figure(figsize = (20, 5))\nplt.subplot(1, 2, 1)\nsns.countplot(x = y_main, palette='pastel')\nplt.title('Reparition before SMOTE')\nplt.subplot(1, 2, 2)\nsns.countplot(x = y_sm, palette='pastel')\nplt.title('Reparition after SMOTE')\nplt.show()","c4b9f42d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.2, random_state=0)","5d392be2":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import f1_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier","b57191d9":"def k_fold_fit_and_evaluate(X, y, model, scoring_method, n_splits=5):\n    # define evaluation procedure\n    cv = KFold(n_splits=5, random_state=42, shuffle=True)\n    # evaluate model\n    scores = cross_validate(model, X_train, y_train, scoring=scoring_method, cv=cv, n_jobs=-1)\n    return scores[\"test_score\"]\n\nscoring_method_f1 = make_scorer(lambda true_target, prediction: f1_score(true_target, prediction, average=\"weighted\"))","ff28f7d6":"random_state = 42\nmodels = {\n    \"GaussianNB\": GaussianNB(),\n    \"DummyClassifier\": DummyClassifier(strategy=\"most_frequent\"),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier(max_depth=32, min_samples_leaf=1, random_state=random_state),\n    \"KNeighborsClassifier\": KNeighborsClassifier(n_neighbors=1, weights=\"uniform\"),   \n    \"LogisticRegression\": LogisticRegression(C=8, random_state=random_state),\n    \"GradientBoostingClassifier\": GradientBoostingClassifier(loss = 'deviance', n_estimators = 20),\n    \"XGBClassifier\": XGBClassifier(objective='binary:logistic', learning_rate=0.1, max_depth=22, n_estimators=300)\n}\n\n\ndict_f1 = {}\nfor name, model in models.items():\n    metrics_f1 = k_fold_fit_and_evaluate(X_train, y_train, model, scoring_method_f1, n_splits=5) \n    dict_f1[name] = np.mean(metrics_f1)\n\nval = []\nfor i in dict_f1.values():\n    val.append(i)\n\nkeys = []\nfor i in dict_f1.keys():\n    keys.append(i)\n\nplt.figure(figsize=(13,5))\nplt.barh(keys, val)\nfor index, value in enumerate(val):\n    plt.text(value, index, str(round(value,3)))\nplt.title(\"mean F1\")\n","8dcd1fef":"from sklearn.model_selection import GridSearchCV\n\nrandom_state = 42\nn_splits = 5\nscoring_method = make_scorer(lambda prediction, true_target: f1_score(true_target, prediction, average=\"weighted\"))\n\nmodel_parameters = {\n    \"GaussianNB\": {\n    \n    },\n    \"DummyClassifier\": {\n        'strategy':['stratified','most_frequent','prior','uniform']\n    },\n    \"DecisionTreeClassifier\": {\n        'max_depth': [20, 22, 28, 32, 37, 38, 42, 45, 50, 70],\n        'min_samples_leaf':[1, 2, 3, 4, 5]\n    },\n    \"KNeighborsClassifier\": {\n        'n_neighbors':[1, 2, 3, 4], \n        'weights':[\"uniform\", \"distance\"]\n    },\n    \"LogisticRegression\": {\n        'C':[7, 8, 10, 15, 30, 40, 50, 70],\n        'max_iter':[1000]\n        \n    },\n    \"GradientBoostingClassifier\": {\n        'loss': [\"deviance\", \"exponential\"],\n        'n_estimators': [1, 2, 10, 20]\n    }\n}\n\nfor model_name, parameters in model_parameters.items():\n    model = models[model_name]\n    \n    cv = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n    grid_search = GridSearchCV(model, parameters, cv=cv, n_jobs=-1, verbose=False, scoring=scoring_method).fit(X_train, y_train)\n\n    best_score = grid_search.best_score_\n    best_params = grid_search.best_params_\n    \n    print(model_name)\n    print(\"- best_score =\", best_score)\n    print(\"best paramters:\")\n    for k,v in best_params.items():\n        print(\"-\", k, v)","67d90e87":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nimport sklearn.metrics as metrics\n\ny_true = y_test","3ab4f0fb":"clf_gbc = GradientBoostingClassifier(loss = 'deviance', n_estimators = 20, random_state=42)\n\nclf_gbc.fit(X_train, y_train)\ny_predicted_gbc = clf_gbc.predict(X_test)\nprint(classification_report(y_true, y_predicted_gbc, zero_division = 0))","4f6b57ea":"cm = confusion_matrix(y_test, y_predicted_gbc)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","5a399c5e":"probs = clf_gbc.predict_proba(X_test)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_true, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","cdce3402":"clf_dt = DecisionTreeClassifier(max_depth=32, min_samples_leaf=1, random_state=0)\nclf_dt.fit(X_train, y_train)\n\ny_predicted_dt = clf_dt.predict(X_test)\nprint(classification_report(y_true, y_predicted_dt, zero_division = 0))","819e7927":"cm = confusion_matrix(y_test, y_predicted_dt)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","39d48606":"probs = clf_dt.predict_proba(X_test)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_true, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","5c5d1f5d":"clf_knn=KNeighborsClassifier(n_neighbors=1, weights=\"uniform\")\nclf_knn.fit(X_train, y_train)\n\ny_predicted_knn = clf_knn.predict(X_test)\n\nprint(classification_report(y_true, y_predicted_knn, zero_division = 0))","662a5394":"cm = confusion_matrix(y_test, y_predicted_knn)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","e3d58cf9":"probs = clf_knn.predict_proba(X_test)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_true, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","8bf866a2":"from sklearn.preprocessing import MinMaxScaler\n\nnorm = MinMaxScaler().fit(X_train)\nX_train_norm = norm.transform(X_train)","9530dbe6":"norm = MinMaxScaler().fit(X_test)\nX_test_main_norm = norm.transform(X_test)","fb6f3f64":"from sklearn import preprocessing\n\nstd_scale = preprocessing.StandardScaler().fit(X_train)\nX_train_std = std_scale.transform(X_train)","5abde8c2":"std_scale = preprocessing.StandardScaler().fit(X_test)\nX_test_main_std = std_scale.transform(X_test)","bd7de681":"clf_knn=KNeighborsClassifier(n_neighbors=1, weights=\"uniform\")\nclf_knn.fit(X_train_norm, y_train)\n\ny_predicted_knn = clf_knn.predict(X_test_main_norm)\n\nprint(classification_report(y_true, y_predicted_knn, zero_division = 0))","3a03ee2e":"cm = confusion_matrix(y_test, y_predicted_knn)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","7b136bae":"probs = clf_knn.predict_proba(X_test_main_norm)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_true, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","c969a92d":"clf_knn=KNeighborsClassifier(n_neighbors=1, weights=\"uniform\")\nclf_knn.fit(X_train_std, y_train)\n\ny_predicted_knn = clf_knn.predict(X_test_main_std)\n\nprint(classification_report(y_true, y_predicted_knn, zero_division = 0))","f8769286":"cm = confusion_matrix(y_test,y_predicted_knn)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","1f5ef0ce":"probs = clf_knn.predict_proba(X_test_main_std)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_true, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","65e6c4ec":"clf_xgb=XGBClassifier(objective='binary:logistic',learning_rate=0.1,max_depth=22,n_estimators=300)\nclf_xgb.fit(X_train, y_train)\n\ny_predicted_xgb = clf_xgb.predict(X_test)\n\nprint(classification_report(y_true, y_predicted_xgb, zero_division = 0))","c97b8b42":"cm = confusion_matrix(y_test,y_predicted_xgb)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","7d5774b4":"probs = clf_xgb.predict_proba(X_test)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_true, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","c878e6ff":"from catboost import CatBoostClassifier","35132036":"params = {'loss_function':'Logloss', # objective function\n          'eval_metric':'AUC', # metric\n          'verbose': 1000,\n         }\n\nclf_cat = CatBoostClassifier(**params)\nclf_cat.fit(X_train, y_train)\n\ny_predicted_cat = clf_cat.predict(X_test)\n\nprint(classification_report(y_true, y_predicted_cat, zero_division = 0))","243a2143":"cm = confusion_matrix(y_test,y_predicted_cat)\nsns.heatmap(cm, annot=True,fmt='g')\nplt.xlabel('Predicted')\nplt.ylabel('True Value')\nplt.show()","8f79bac6":"probs = clf_cat.predict_proba(X_test)\npreds = probs[:,1]\n\nfpr, tpr, threshold = metrics.roc_curve(y_true, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","7a26e720":"Important note: duration highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.","d0bc728c":"#### Decision Tree","39262d85":"#### X and y preparation","36e4f2ac":"#### Find Outliers in numerical features","d0175cc6":"## Data Overview","f54c8003":"Model with the best score is <em>XGBoost Classifier<\/em>  \nAchieved roc-auc: 0.97","461164d3":"#### Find Features with One Value","d74cad4f":"Oversampling Using SMOTE Methode","24bed2de":"#### Handling Imbalanced Dataset","70ba5341":"*Standardization*","8f5e6f51":"## Exploratory Data Analysis","2914f405":"Removing outliers in feature 'campaign'","0e6156e2":"#### Categorical Feature Encoding","7422253d":"*Normalization*","35a22048":"## Data Preparation","1c317146":"Age, duration, compaign, pdays, previous and cons.conf.idx have some outliers","a50c679f":"No missing value found","e98a9378":"## Predictive Model Preparation","4bfcf2b1":"#### Visualizing the categorical features","943cd17a":"#### Remove Outliers","07a66a79":"Correlation between features","234c9a95":"#### Visualizing the numerical features","10b206ea":"From this correlation matrix we can see, that duration,pdays,emp.var.rate,euribor3m and nr.employed are more correlated to target columns.","dc1710cd":"#### Gradient Boosting","3dee4548":"Dropping pdays-column as it has 999 value (means client was not previously contacted) for around 90%+ ","04ebc64d":"Find out the relationship between categorical variable and target value","0018ff94":"#### Task\n\nThe data is related with direct marketing campaigns (phone calls) of a Portuguese banking institution. The classification goal is to predict if the client will subscribe a term deposit (variable y).\n\n#### Data description\nDataset `bank-additional-full.csv` has 41188 examples and 20 inputs, ordered by date (from May 2008 to November 2010), very close to the data analyzed in [Moro et al., 2014].","108d3687":"#### Check if the Data set is balanced or not based on target values","a2b7167c":"#### Grid-Search","d35eb0b2":"#### Average F1-Score For Different Models","a0a30e3d":"## Conclusion","5c27a468":"#### KNeighbors With Standardization","70bebd9f":"Given dataset seems to be highly imbalanced.","f13407cd":"No feature with only one value","3966b118":"#### Find Missing Values (NaN)","921256d4":"# Bank Marketing","e0592c8c":"<li>Customers who work as admin, technician and blue-collar are more inclined towards a term deposit;<\/li>\n<li>Married customers have high interest on deposit;<\/li>\n<li>Customers with university_degree are more inclined towards a term deposit;<\/li>\n<li>Customers who don't have credit in default are more inclined towards a term deposit;<\/li>\n<li>During the summer seasons (May to August) customers show high interest to deposit;<\/li>\n<li>Customers who has personal loan seems to be less interested on deposit;<\/li>\n<li>Customers who were contacted via 'cellular' are more inclined towards a term deposit.<\/li>","5e78d925":"Boxplot on numerical features to find outliers","8744af13":"#### Cross-Validation","7aca6ca8":"## Predictive Model Application","050af404":"#### XG Boost ","943f808c":"#### Feature importance","37caae03":"#### CatBoost","782b029a":"Removing outliers in feature 'pdays'","3305c075":"#### KNeighbors With Normalization","0c7acf06":"#### KNeighbors","0ae3fe20":"Check count based on categorical features","f325d038":"Assuming campaign count greater than 37 as outliers","fa70156c":"#### Spliting Data On Traing And Test"}}