{"cell_type":{"94c49987":"code","e0827732":"code","95256688":"code","75a91666":"code","8752cea3":"code","29b2c351":"code","249cab74":"code","0105a7c4":"code","01c2f3eb":"code","ec45e8dd":"code","5104d68d":"code","fb9dcf69":"code","958fe903":"code","cdfddc2b":"code","f57d8df6":"code","124333ab":"code","04986c1d":"code","cda9ff0a":"code","1dfbaa04":"code","0ecf9c86":"code","697f2692":"code","d0ac8b70":"code","3cf5898e":"code","7078f68f":"code","37316b5f":"code","44458d88":"code","f61224ed":"code","58de6f1b":"code","a61346f6":"code","22babe2a":"code","105eda37":"code","8142458a":"code","3418d8cc":"code","d86150d4":"code","e0f447e0":"code","ff0d2e91":"code","b054c26e":"code","28f7af31":"code","24feb0b5":"code","b8233ab2":"code","a4c3d2f4":"code","a39302b1":"code","12307b7e":"code","1ce7f93e":"code","ab41b1c4":"code","b103b39b":"code","d776d28d":"code","a77ca0f0":"code","092e3992":"code","4819e9f8":"code","31e0ad4e":"code","e4d183f9":"code","f62b34c2":"code","6b4602d9":"code","f7432ac9":"code","74f44080":"code","5f10d23b":"code","6bb1b07f":"code","a63ff948":"code","93d8263e":"code","4b881586":"code","794f5966":"code","25302cc0":"code","b0528254":"code","7e8706e5":"code","74ce0c39":"code","40248494":"code","99c71a8f":"code","5d2e4a2b":"code","358c9395":"code","1a902439":"code","8ec2ccad":"code","f93a4a1a":"code","5f70300f":"code","23c93996":"code","0014af73":"code","ecf0f4a5":"code","b8272e6f":"code","43f54d16":"code","40d2ef4e":"code","648a8dd4":"code","7cce83b8":"code","eab804c1":"code","d6f23371":"code","1b7d6bdb":"code","64558c77":"code","e52f40e5":"code","66845852":"code","0587fe2d":"code","f4ea5332":"code","5fd43cf7":"code","415fcde1":"code","088f7398":"code","d6ef290b":"code","d1aa8db7":"code","aeaa9f90":"code","add95ccb":"code","819549e0":"code","392ed933":"code","650ab6c3":"code","bf126b6c":"code","ff330684":"code","a725afb6":"code","35a8589b":"code","582ced1b":"code","ef27c140":"code","d0ee7e94":"code","b0fd0205":"code","586fdf9f":"code","6395f97d":"code","645767e1":"code","a2a10c71":"code","174b1230":"code","52e54416":"code","1971a2eb":"code","1aa960b2":"code","f4c0343e":"code","d2cd7c1d":"markdown","a9acf77c":"markdown","549acf57":"markdown","b00c9724":"markdown","f2dbaa1e":"markdown","333833a8":"markdown","517d8f10":"markdown","93a67a29":"markdown","61fe3ef7":"markdown","215c09bb":"markdown","1216e6fc":"markdown","0cd4508b":"markdown","d40b0b14":"markdown","5c612a86":"markdown","3ec291c6":"markdown","e587874d":"markdown","b6989683":"markdown","4f042570":"markdown","e9352b88":"markdown","f8425751":"markdown","54f858a7":"markdown","7cfd6d93":"markdown","e8f0c4ba":"markdown","3f7f3dcf":"markdown","a0baa370":"markdown","50aeb395":"markdown","f937026a":"markdown","4a09a6ac":"markdown","9c1aa0f1":"markdown","6dc8b801":"markdown","12f41062":"markdown","2a11c7dc":"markdown","ec54cc36":"markdown","fd1d53fe":"markdown","44172071":"markdown","d6eb36a2":"markdown","ff5ef402":"markdown","bca8853c":"markdown","f69cecc9":"markdown","878d062f":"markdown","ef69fc10":"markdown","5e5263f0":"markdown","e5373cbc":"markdown","8925aa21":"markdown","b0f1840c":"markdown","58987a7e":"markdown","be8eb58b":"markdown","7c14728e":"markdown","a7c00c04":"markdown","fc511edc":"markdown","d923ee7a":"markdown","574b083d":"markdown","73462249":"markdown","b4a4f04b":"markdown","0ed941ef":"markdown","0d192bc0":"markdown","4f3faa47":"markdown","36631982":"markdown","ce7a2dd3":"markdown","1c548534":"markdown","b994ea18":"markdown","16fad618":"markdown","18e961e5":"markdown","ccd6e063":"markdown","4664ae59":"markdown","6ffd7c74":"markdown","deab2a6e":"markdown","4778b3bc":"markdown","bfaa39d2":"markdown","237ebb2e":"markdown","5cdf77f9":"markdown","27cdd78f":"markdown","ed83eb36":"markdown","51481953":"markdown","3d325d2e":"markdown","e8ff9789":"markdown","e433299a":"markdown","7e93ba6e":"markdown","03d34cfc":"markdown","43da4100":"markdown","8c20b6c0":"markdown","0f4d4305":"markdown","4caf233e":"markdown","e749262e":"markdown","f3584b46":"markdown","dd7de20c":"markdown","56e8353c":"markdown","f508e872":"markdown","29ea553b":"markdown","fbcb2d9a":"markdown","9b9e5d5e":"markdown","6617bc2a":"markdown","b9e07c3b":"markdown","24d4f543":"markdown","28339293":"markdown","bbcf38aa":"markdown","a51be2ae":"markdown","1c3d5515":"markdown","456eaac3":"markdown","b0a604f6":"markdown","22596a51":"markdown","b61145d8":"markdown","74046684":"markdown","7be6fcba":"markdown","b507ac54":"markdown","6c720e2a":"markdown","3aacf808":"markdown","36a539a9":"markdown","3d3a1c45":"markdown","c294b1b0":"markdown","bb741055":"markdown","b13d2a8b":"markdown","b1bfdb1e":"markdown","b0bdd3f8":"markdown","9d07a073":"markdown","7b5dc753":"markdown","8ec81683":"markdown","ca434498":"markdown","878be64d":"markdown","e2a0a181":"markdown","ef6c9761":"markdown","d99704ab":"markdown","92fb4002":"markdown","d7ed78f3":"markdown","6ba3a96a":"markdown","45cd4a8f":"markdown","66904be2":"markdown","67db1a0f":"markdown","c19a05dc":"markdown","646c8546":"markdown","d04696e8":"markdown","65aa8703":"markdown","2751d06c":"markdown","596e3b63":"markdown","bfa32aaa":"markdown","d8997179":"markdown","d6fd3b3a":"markdown","332c7076":"markdown","471e5275":"markdown","7ec2d1a4":"markdown","a3d2667d":"markdown","124f5dcd":"markdown","65524529":"markdown","5394b7c6":"markdown","556f8a29":"markdown","66df86dc":"markdown","49edac7a":"markdown","3d2f4657":"markdown","fc91f6b6":"markdown","64702c2d":"markdown","b4b578f4":"markdown","d1447026":"markdown","8b3d3700":"markdown","6f46be19":"markdown","339658b4":"markdown","91e82bc2":"markdown","dad172d9":"markdown","ee984ca5":"markdown","2a174b48":"markdown","c33fc7bc":"markdown","bbb678fc":"markdown","de390b10":"markdown","ec68bf43":"markdown","877be42a":"markdown","f99295c6":"markdown","50932e4b":"markdown","2ddbf3c8":"markdown","3a496dbb":"markdown","9f82ea19":"markdown","12640c38":"markdown","1ebadfa6":"markdown","5b873889":"markdown","5c20afea":"markdown","44094ee8":"markdown","0f3b6feb":"markdown","6a7724a7":"markdown","78920920":"markdown","64f50dd9":"markdown","afb6f58e":"markdown","5701b427":"markdown","14aa7880":"markdown","342adaa6":"markdown","528722e9":"markdown","a75cd2ef":"markdown","88261912":"markdown","6735cc2d":"markdown","3cb88afc":"markdown","395b9113":"markdown","606dbeb6":"markdown","241b4405":"markdown","cc198289":"markdown","2f3b7a1c":"markdown","659e9f8a":"markdown","7de77605":"markdown","f00276d3":"markdown","5e37bc0e":"markdown","80877eec":"markdown","b9fe941e":"markdown","c32d9ad4":"markdown","d12c66bc":"markdown","d32ff4e9":"markdown","9c452ad7":"markdown","8383777c":"markdown","2ab4664b":"markdown","1ec9a7b5":"markdown","74b45660":"markdown","b121ffd1":"markdown","4aec6dc3":"markdown","8bd71358":"markdown","9bc9139a":"markdown","3a06ef39":"markdown","60689dc0":"markdown","5b366ed0":"markdown","c2fdaded":"markdown","e8efbd99":"markdown","58d1c0cc":"markdown","69acbc7d":"markdown","34d27659":"markdown","48e8d5a2":"markdown","035c6609":"markdown","957d3a00":"markdown","a0028fc8":"markdown","57a85706":"markdown","171307b3":"markdown","94b48f1e":"markdown","9e8865a5":"markdown","20401469":"markdown","9a1c76cc":"markdown","b712cc11":"markdown","d27085b3":"markdown","b0fb97f1":"markdown","34c74392":"markdown","f049cb06":"markdown","1a6aae77":"markdown","02f55591":"markdown","0e5ee52b":"markdown","7fc7d60d":"markdown","8cd57e77":"markdown"},"source":{"94c49987":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.io as pio\nimport plotly\nimport plotly.graph_objs as go\nimport sklearn\nimport re\nfrom plotly import tools\nfrom joblib import dump, load\nfrom sklearn import preprocessing\nfrom IPython.display import Image\nfrom scipy.stats import chi2_contingency\nfrom plotly.subplots import make_subplots\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\nimport plotly.express as px\n\npio.renderers.default = \"svg\"\n\n\n%matplotlib inline","e0827732":"df = pd.read_csv('covtype.csv')\ndf.head()","95256688":"df.info()","75a91666":"class_names=('1. Spruce\/Fir',\n             '2. Lodgepole Pine',\n             '3. Ponderosa Pine',\n             '4. Cottonwood\/Willow',\n             '5. Aspen',\n             '6. Douglas-fir',\n             '7. Krummholz')","8752cea3":"label_encoded_WA = (df.iloc[:,10:14].values == 1).nonzero()[1]\nlabel_encoded_ST = (df.iloc[:,14:54].values == 1).nonzero()[1]\ncolumns = [x for x in range(10,14)]\nle_df = df.drop(columns=df.columns[columns])\ncolumns = [x for x in range(14,54)]\nle_df.drop(columns=df.columns[columns], inplace=True)\nle_df.insert(len(le_df.columns)-1, 'Wilderness_Area', label_encoded_WA)\nle_df.insert(len(le_df.columns)-1, 'Soil_Type', label_encoded_ST)\n\n# Save to csv\nle_df.to_csv('le_covtype.csv')","29b2c351":"# Read previous pre-computed label encoded dataset\nle_df = pd.read_csv('le_covtype.csv', index_col=0)\nle_df.head()","249cab74":"def histogram_feature(feature, title, barmode='group', image=False):\n    classes_sorted = list(le_df.Cover_Type.unique())\n    classes_sorted.sort()\n#     categories = le_df['Wilderness_Area'].unique()\n    traces = []\n    for c in classes_sorted:\n        traces.append(go.Histogram(\n            histfunc=\"count\",\n    #         x=categories,\n            x=le_df[le_df.Cover_Type == c][feature],\n            name=class_names[c-1],\n            marker_color = plotly.colors.DEFAULT_PLOTLY_COLORS[c-1]\n        ))\n    layout = go.Layout(\n        title=title,\n        barmode=barmode\n    )\n    fig = go.Figure(traces, layout)\n    if (image):\n        pio.write_image(fig, 'images\/' + title + '.png', width=1000, height=600)\n    else:\n        pio.show(fig)","0105a7c4":"def percentage_barchart_feature(feature, title, features_name=None, image=False):\n    classes_sorted = list(le_df.Cover_Type.unique())\n    classes_sorted.sort()\n    traces = []\n    features_range = list(le_df[feature].unique())\n    features_range.sort()\n    features_name = features_range if (features_name == None) else features_name\n    if (len(features_range) != len(features_name)):\n        raise ValueError(\"features_names len must correspond to the number \\\n                         of unique values of this feature\")\n    totals_wilderness = [len(le_df[le_df[feature] == i]) for i in features_range]\n    y = []\n    for c in classes_sorted:\n        y.append([100 * len(le_df[(le_df[feature] == i) & (le_df['Cover_Type'] == c)]) \/ \n                totals_wilderness[i] for i in range(4)])\n\n    fig = go.Figure(data=[\n        go.Bar(name=class_names[c-1], x=features_name, y=y[c-1],\n               marker_color=plotly.colors.DEFAULT_PLOTLY_COLORS[c-1]) for c in classes_sorted\n    ])\n    # Change the bar mode\n    fig.update_layout(title_text=title, barmode='stack')\n    if (image):\n        pio.write_image(fig, 'images\/' + title + '.png', width=1000, height=600)\n    else:\n        pio.show(fig)","01c2f3eb":"def boxplot_feature(feature, title, image=False):\n    classes_sorted = list(le_df.Cover_Type.unique())\n    classes_sorted.sort()\n    traces = []\n    i = 0\n    for c in classes_sorted:\n        rgb_str = re.findall(\"\\d+\", plotly.colors.DEFAULT_PLOTLY_COLORS[i])\n        rgb = tuple(int(x) for x in rgb_str)\n        i += 1\n        traces.append(go.Box(\n            y=le_df[le_df.Cover_Type == c][feature],\n            name=class_names[c-1],\n            marker_color = '#%02x%02x%02x' % rgb\n#             marker_color=[plotly.colors.DEFAULT_PLOTLY_COLORS[c]]\n        ))\n    layout = go.Layout(\n        title=title\n    )\n    fig = go.Figure(traces, layout)\n    if (image):\n        pio.write_image(fig, 'images\/' + title + '.png', width=1000, height=600)\n    else:\n        pio.show(fig)","ec45e8dd":"display(df['Cover_Type'].value_counts())\nhistogram_feature('Cover_Type', 'Cover Type')","5104d68d":"(le_df.Wilderness_Area+1).value_counts()","fb9dcf69":"df.isna().values.any()","958fe903":"plt.figure(figsize=(10,10))\nplt.title(\"Heatmap\")\n_ = sns.heatmap(data=le_df.iloc[:, :10].corr(), annot=True)","cdfddc2b":"# plt.figure(figsize=(30,30))\nplt.title(\"Pairplot\")\nscatt_df = pd.concat([df.iloc[:, :10], df.iloc[:, 54]], axis=1)\nscatt_df.rename(columns={'Horizontal_Distance_To_Hydrology': 'hor_dist_hyd',\n                         'Vertical_Distance_To_Hydrology': 'ver_dist_hyd',\n                         'Horizontal_Distance_To_Roadways': 'hor_dist_road',\n                         'Horizontal_Distance_To_Fire_Points': 'hor_dist_fire'\n                        }, inplace=True)\nsns.set(style=\"ticks\")\n_ = sns.pairplot(scatt_df, vars=scatt_df.columns[slice(0,10,1)], markers='.',\n                 hue='Cover_Type', plot_kws=dict(edgecolor=\"none\"))","f57d8df6":"print('Hillshade_9am zero values: {}'.format(len(le_df[le_df['Hillshade_9am'] == 0])))\nprint('Hillshade_Noon zero values: {}'.format(len(le_df[le_df['Hillshade_Noon'] == 0])))\nprint('Hillshade_3pm zero values: {}'.format(len(le_df[le_df['Hillshade_3pm'] == 0])))","124333ab":"ax = sns.scatterplot(x=\"Slope\", y=\"Hillshade_3pm\", markers='.',\n                     hue=\"Cover_Type\", palette=sns.color_palette()[:7],\n                     data=df).set_title(\"A closer look to Slope-Hillshade_3pm joint distribution\")","04986c1d":"missing_test = le_df[le_df['Hillshade_3pm'] == 0]\nmissing_train = le_df[le_df['Hillshade_3pm'] != 0]\nmissing_y_tr = missing_train['Hillshade_3pm']\nmissing_y_test = missing_test['Hillshade_3pm']\nmissing_train.drop(columns=['Hillshade_3pm'])\nmissing_test.drop(columns=['Hillshade_3pm'])\nmissing_train.drop(columns=['Cover_Type'])\nmissing_test.drop(columns=['Cover_Type'])\nmissing_X_tr = missing_train.values\nmissing_X_test = missing_test.values\nrfInputer = RandomForestClassifier(n_estimators=30, n_jobs=8, random_state=0) \nrfInputer.fit(missing_X_tr, missing_y_tr)\nmissing_pred = rfInputer.predict(missing_X_test)\ndel rfInputer","cda9ff0a":"le_df.loc[le_df.Hillshade_3pm == 0, 'Hillshade_3pm'] = missing_pred\ndf.loc[df.Hillshade_3pm == 0, 'Hillshade_3pm'] = missing_pred","1dfbaa04":"ax = sns.scatterplot(x=\"Slope\", y=\"Hillshade_3pm\", markers='.',\n                     hue=\"Cover_Type\", palette=sns.color_palette()[:7],\n                     data=df).set_title(\"Slope-Hillshade_3pm joint distribution after predicting zero values\")","0ecf9c86":"title = 'Elevation boxplot'\nboxplot_feature('Elevation', title, image=False)","697f2692":"title = 'Aspect boxplot'\nboxplot_feature('Aspect', title, image=False)","d0ac8b70":"title = 'Slope boxplot'\nboxplot_feature('Slope', title, image=False)","3cf5898e":"title = 'Horizontal_Distance_To_Hydrology boxplot'\nboxplot_feature('Horizontal_Distance_To_Hydrology', title, image=False)","7078f68f":"title = 'Vertical_Distance_To_Hydrology boxplot'\nboxplot_feature('Vertical_Distance_To_Hydrology', title, image=False)","37316b5f":"title = 'Horizontal_Distance_To_Roadways boxplot'\nboxplot_feature('Horizontal_Distance_To_Roadways', title, image=False)","44458d88":"title = 'Horizontal_Distance_To_Fire_Points'\nboxplot_feature('Horizontal_Distance_To_Fire_Points', title, image=False)","f61224ed":"title = 'Hillshade_9am'\nboxplot_feature('Hillshade_9am', title, image=False)","58de6f1b":"title = 'Hillshade_Noon'\nboxplot_feature('Hillshade_Noon', title, image=False)","a61346f6":"title = 'Hillshade_3pm'\nboxplot_feature('Hillshade_3pm', title, image=False)","22babe2a":"check_WA = np.sum(df.values[:, 10:14], axis=1)\ncheck_WA_unique = np.unique(check_WA)\nprint(f\"number of active Wilderness_Area's categories for each row {check_WA_unique}\")\ncheck_ST = np.sum(df.values[:, 14:54], axis=1)\ncheck_ST_unique = np.unique(check_ST)\nprint(f\"number of active Soil Type's categories for each row {check_ST_unique}\")","105eda37":"title = 'Wilderness Area Distribution'\nfeatures_name = ['Rawah', 'Neota', 'Comanche Peak', 'Cache la Poudre']\npercentage_barchart_feature('Wilderness_Area', title, features_name, image=False)","8142458a":"soil_types = [le_df[le_df['Soil_Type'] == i] for i in range(40)]\nspecs = [[{\"type\": \"domain\"} for i in range(5)] for i in range(8)]\ncolors = plotly.colors.DEFAULT_PLOTLY_COLORS\nlabels = class_names \nsubplot_titles = ['        ST' + str(i+1) for i in range(40)]\nfig = make_subplots(rows=8, cols=5, specs=specs, subplot_titles=subplot_titles)\ni=0\nfor row in range(8):\n    for col in range(5):\n        values = []\n        for c in range(1,8):\n            mask = soil_types[(row) * 5 + (col)]['Cover_Type'] == c\n            value = len(soil_types[(row) * 5 +(col)][mask.values])\n            value = None if (value == 0) else value\n            values.append(value)\n        i += 1\n        sub_fig = go.Pie(labels=labels, values=values)\n        fig.add_trace(sub_fig, row=row+1, col=col+1)\n        fig.update_traces(marker=dict(colors=colors,\n                                      line=dict(color='#000000',\n                                                width=1)))\nfig.update_layout(height=1000, width=1000, title_text=\"Soil Type's label percentage\")\n#dynamic rendering\n# fig.show(render='notebook')\n\n# static rendering\npio.show(fig, height=1000, width=1000)","3418d8cc":"contingency = pd.crosstab(le_df['Cover_Type'],\n                          le_df['Wilderness_Area'])\ndisplay(contingency)\n# dof are (rows-1)(columns-1)\nc, p, dof, expected = chi2_contingency(contingency)\nprint('chi2: {}, p-value: {}, dof:{}'.format(c, p, dof))\n# display(pd.DataFrame(expected))","d86150d4":"contingency = pd.crosstab(le_df['Cover_Type'],\n                          le_df['Soil_Type'])  \ndisplay(contingency)\n# dof are (rows-1)(columns-1)\nc, p, dof, expected = chi2_contingency(contingency)\nprint('chi2: {}, p-value: {}, dof:{}'.format(c, p, dof))\n# display(pd.DataFrame(expected))","e0f447e0":"contingency = pd.crosstab(le_df['Wilderness_Area'], \n                            le_df['Soil_Type'])  \ndisplay(contingency)\n# dof are (rows-1)(columns-1)\nc, p, dof, expected = chi2_contingency(contingency)\nprint('chi2: {}, p-value: {}, dof:{}'.format(c, p, dof))\n# display(pd.DataFrame(expected))","ff0d2e91":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import TransformerMixin, BaseEstimator\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom imblearn.pipeline import Pipeline as imbPipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.decomposition import PCA as sklearnPCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import tree\nfrom pydotplus import graph_from_dot_data\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom xgboost import XGBClassifier\nfrom sklearn import svm\nfrom sklearn import linear_model\nfrom sklearn.kernel_approximation import Nystroem\nimport graphviz\nimport xgboost as xgb\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.kernel_approximation import Nystroem, RBFSampler","b054c26e":"#label encoded split\ny_le = le_df['Cover_Type'].copy()\ny_le -= 1\nX_le = le_df.drop(columns=['Cover_Type'])\nX_le= le_df.iloc[:, :-1]\nX_train_le, X_test_le, y_train_le, y_test_le = train_test_split(X_le, y_le, test_size=.2, random_state=0)\n\n# dummy encoded split\ndf = df.drop(columns=['Wilderness_Area1', 'Soil_Type1'])\ny_he = df['Cover_Type'].copy()\ny_he -= 1\nX_he = df.iloc[:, :-1]\nX_train_he, X_test_he, y_train_he, y_test_he = train_test_split(X_he, y_he, test_size=.2, random_state=0)","28f7af31":"categorical_cols = [cname for cname in X_he.columns \n                    if X_he[cname].nunique() < 3]\nnumerical_cols = [cname for cname in X_he.columns\n                 if X_he[cname].nunique() >= 3]\n\nX_he[numerical_cols] = X_he[numerical_cols].apply(pd.to_numeric, downcast='float')","24feb0b5":"def conf_matrix(y_true, y_pred):\n    data = confusion_matrix(y_true, y_pred)\n    # broadcast norm all over confusion matrix\n    data = data \/ np.sum(data, axis=1)[:, np.newaxis]\n    df_cm = pd.DataFrame(data, columns=np.unique(y_true)+1, index = np.unique(y_true)+1)\n    df_cm.index.name = 'Actual'\n    df_cm.columns.name = 'Predicted'\n    plt.figure(figsize = (7,4))\n    sns.set(font_scale=1.2)#for label size\n    ax = sns.heatmap(df_cm, cmap=\"Blues\", annot=True, annot_kws={\"size\": 12},\n               cbar=False)# font size\n    \n    _ = ax.set_title('Normalized Confusion Matrix')\n    precision_scores = precision_score(y_true, y_pred, average=None)\n    recall_scores = recall_score(y_true, y_pred, average=None)\n    print(\"precision scores: {}\".format(np.around(precision_scores, decimals=3)))\n    print(\"recall scores: {}\".format(np.around(recall_scores, decimals=3)))","b8233ab2":"def stratified_K_cross(X, y, classifier):\n    cv_train_score = []\n    cv_test_score = []\n    skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\n    for train_idx, test_idx in skf.split(X, y):\n        X_train, X_test = X.iloc[train_idx, :], X.iloc[test_idx, :]\n        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n        classifier.fit(X_train, y_train)\n        score = classifier.score(X_train, y_train)\n        cv_train_score.append(score)\n        score = classifier.score(X_test, y_test)\n        cv_test_score.append(score)\n    return cv_test_score, cv_train_score \n\ndef print_cv_scores(scores):\n    print('K-cross validation scores:')\n    print('accuracies: {}'.format(scores['test_acc']))\n    print('f1_scores: {}'.format(scores['test_f1']))\n    print('mean accuracy: {0:.3f} (+\/-{1:.3f})'.format(np.mean(scores['test_acc']),\n                                             np.std(scores['test_acc'])))\n    print('mean f1_score: {0:.3f} (+\/-{1:.3f})'.format(np.mean(scores['test_f1']),\n                                             np.std(scores['test_f1'])))","a4c3d2f4":"skf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\nscoring = {'acc' : 'accuracy',\n           'f1' : 'f1_macro'}","a39302b1":"# @ignore_warnings(category=ConvergenceWarning)\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\ndef plot_learning_curve(estimator, X, y, title=\"\", cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5),\n                        ):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train\/test splits.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n    \"\"\"\n    \n    fig = go.Figure() \n    \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    leg=True\n        \n    p1 = go.Scatter(x=train_sizes, y=test_scores_mean + test_scores_std,\n                    mode='lines',\n                    line=dict(color=\"green\", width=1),\n                    showlegend=False,  )\n    fig.add_trace(p1)\n    \n    p2 = go.Scatter(x=train_sizes, y=test_scores_mean - test_scores_std,\n                    mode='lines',\n                    line=dict(color=\"green\", width=1),\n                    showlegend=False, fill='tonexty')\n    fig.add_trace(p2)\n    \n    p3 = go.Scatter(x=train_sizes, y=train_scores_mean + train_scores_std,\n                    mode='lines',\n                    line=dict(color=\"red\", width=1),\n                    showlegend=False)\n    fig.add_trace(p3)\n    \n    p4 = go.Scatter(x=train_sizes, y=train_scores_mean - train_scores_std,\n                    mode='lines',\n                    line=dict(color=\"red\", width=1),\n                    showlegend=False, fill='tonexty')\n    fig.add_trace(p4)\n    \n    p5 = go.Scatter(x=train_sizes, y=train_scores_mean, \n                    marker=dict(color='red'),\n                    name=\"Training score\", showlegend=leg)\n    fig.add_trace(p5)\n    \n    p6 = go.Scatter(x=train_sizes, y=test_scores_mean, \n                    marker=dict(color='green'),\n                    name=\"Cross-validation score\", showlegend=leg)\n    fig.add_trace(p6)\n    fig.update_layout(title=title,\n                      xaxis_title=\"Training size\",\n                      yaxis_title=\"Accuracy score\"\n                     )\n    pio.show(fig)\n  ","12307b7e":"def print_best_n_classifiers(grid_search, n):\n\n    print(\"Grid scores on development set:\")\n    indexes = np.argsort(grid_search.cv_results_['mean_test_f1_macro'])[::-1][:n]\n    means_accuracy = grid_search.cv_results_['mean_test_accuracy'][indexes]\n    stds_accuracy = grid_search.cv_results_['std_test_accuracy'][indexes]\n    means_f1_macro = grid_search.cv_results_['mean_test_f1_macro'][indexes]\n    stds_f1_macro = grid_search.cv_results_['std_test_f1_macro'][indexes]\n    params = [grid_search.cv_results_['params'][i] for i in indexes]\n\n    for mean_a, std_a, mean_f, std_f, param in zip(means_accuracy, stds_accuracy,\n                                                    means_f1_macro, stds_f1_macro, params):\n        print(\"accuracy %0.3f (+\/-%0.03f)\"\n              % (mean_a, std_a * 2))\n        print(\"f1 %f (+\/-%0.03f) for %r\"\n              % (mean_f, std_f * 2, param))\n","1ce7f93e":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', SimpleImputer(), categorical_cols)\n    ]\n)","ab41b1c4":"class Debug(BaseEstimator, TransformerMixin):\n    \n    def transform(self, X):\n        display(pd.DataFrame(X).head())\n        print(X.shape)\n        return X\n    \n    def fit(self, X, y=None, **fit_params):\n        return self","b103b39b":"logistic_model = LogisticRegression(random_state=0, C=1, penalty='l2', solver='saga', tol=1e-1,\n                                    multi_class='multinomial', max_iter=1000, class_weight='balanced')\nclf_lr = Pipeline(steps=[('preprocessor', preprocessor),\n#                                       ('dbg', Debug()),\n                                      ('classifier', logistic_model)])\n\nscores = cross_validate(clf_lr, X_train_he, y_train_he, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=8)\nprint_cv_scores(scores)","d776d28d":"clf_lr.fit(X_train_he, y_train_he)\nprint('10 most discriminative feature per class')\ncoeff_df = pd.DataFrame()\nfor i in range(1, 7+1):\n    train_df = X_train_he\n    idxs = np.argsort(np.abs(logistic_model.coef_[i-1]))[::-1]\n    coeff_i = pd.DataFrame(train_df.columns[idxs])\n    coeff_i.columns = ['Feature_class' + str(i)]\n    coeff_i[\"Coefficient_class\" + str(i)] = pd.Series(logistic_model.coef_[i-1][idxs])\n    coeff_i[\"Coefficient_class\" + str(i)] = round(coeff_i[\"Coefficient_class\" + str(i)], 4)\n#     print(\"class {}\".format(class_names[i-1]))\n#     coeff_i = coeff_i.reindex(coeff_i[\"Coefficient\"].abs().sort_values(ascending=False).index)\n    coeff_df = pd.concat([coeff_df, coeff_i[:10]], axis=1)\ndisplay(coeff_df.transpose())","a77ca0f0":"logistic_model = LogisticRegression(random_state=0, C=1, penalty='l2', solver='saga', tol=1e-1,\n                                    multi_class='multinomial', max_iter=1000, class_weight='balanced')\npreprocessor.fit(X_train_he, y_train_he)\nX = preprocessor.transform(X_train_he)\nselector = RFE(logistic_model, 40, step=1)\n_ = selector.fit(X, y_train_he)\nprint('Selected feature:')\nprint(X_train_he.columns[selector.support_].values)","092e3992":"rX = selector.transform(X)\n_ = logistic_model.fit(rX, y_train_he)","4819e9f8":"scores = cross_validate(logistic_model, rX, y_train_he, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=8)\n\nprint_cv_scores(scores)","31e0ad4e":"poly = PolynomialFeatures(2, interaction_only=False)\npoly_preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', \n         Pipeline([\n             ('polynomial', poly),\n             ('scaler', StandardScaler())\n         ]),\n         numerical_cols),\n        ('cat', SimpleImputer(), categorical_cols)\n    ]\n)","e4d183f9":"logistic_model_pf = LogisticRegression(random_state=0, C=1, penalty='l2', solver='saga', tol=1e-1,\n                                    multi_class='multinomial', class_weight='balanced')\n\nclf_lr_pf = Pipeline([(\"polynomial_features\", poly_preprocessor),\n#                      (\"debug\", Debug()),\n                     (\"logistic_regression\", logistic_model_pf)\n                    ])\nscores = cross_validate(clf_lr_pf, X_train_he, y_train_he, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=1)\nprint_cv_scores(scores)","f62b34c2":"logistic_model_rus = LogisticRegression(random_state=0, C=1000, penalty='l2', solver='saga', tol=1e-1,\n                                    multi_class='multinomial', max_iter=1000)\n\nclf_lr_rus = imbPipeline([\n    (\"polynomial_features\", poly_preprocessor),\n    ('near_miss', RandomUnderSampler(random_state=42)),\n    ('classifier', logistic_model_rus)])\n\nscores = cross_validate(clf_lr_rus, X_train_he, y_train_he, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=8)\nprint_cv_scores(scores)","6b4602d9":"logistic_regression_sm = LogisticRegression(random_state=0, C=1, penalty='l2', solver='saga', tol=1e-1,\n                                    multi_class='multinomial')\nclf_lr_sm = imbPipeline([(\"polynomial_features\", poly_preprocessor),\n                     (\"smote\", SMOTE(random_state=42, k_neighbors=10, n_jobs=8)),\n                     (\"logistic_regression\", logistic_regression_sm)\n                    ])\nscores = cross_validate(clf_lr_sm, X_train_he, y_train_he, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=1)\nprint_cv_scores(scores)","f7432ac9":"plot_learning_curve(clf_lr_sm,  X_train_he, y_train_he,title='SMOTE 2-degree polynomial Logistic Regression', cv=skf, n_jobs=1)","74f44080":"# clf_lr_sm.fit(X_train_he, y_train_he)\ny_pred = clf_lr_sm.predict(X_test_he)\ntest_f1 = f1_score(y_test_he, y_pred, average='macro')\ntest_acc = accuracy_score(y_test_he, y_pred)\nconf_matrix(y_test_he, y_pred)\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_le_new, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test_le_new, y_pred)))","5f10d23b":"knn = KNeighborsClassifier(n_neighbors=6, p=2, weights='distance', n_jobs=8)\nscores = cross_validate(knn, X_train_he, y_train_he, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=1)\nprint_cv_scores(scores)","6bb1b07f":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', SimpleImputer(), categorical_cols)\n    ]\n)","a63ff948":"X_std = (preprocessor.fit_transform(X_he))\nmean_vec = np.mean(X_std, axis=0)\ncov_mat = (X_std - mean_vec).T.dot((X_std - mean_vec)) \/ (X_std.shape[0]-1)\ncov_mat = np.cov(X_std.T)\neig_vals, eig_vecs = np.linalg.eig(cov_mat)\n\nfor ev in eig_vecs:\n    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort()\neig_pairs.reverse()","93d8263e":"tot = sum(eig_vals)\nvar_exp = [(i \/ tot)*100 for i in sorted(eig_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\n\ntrace1 = dict(\n    type='bar',\n    x=['PC %s' %i for i in range(1,20)],\n    y=var_exp,\n    name='Individual'\n)\n\ntrace2 = dict(\n    type='scatter',\n    x=['PC %s' %i for i in range(1,20)], \n    y=cum_var_exp,\n    name='Cumulative'\n)\n\ndata = [trace1, trace2]\n\nlayout=dict(\n    title='Explained variance by different principal components',\n    yaxis=dict(\n        title='Explained variance in percent'\n    ),\n    annotations=list([\n        dict(\n            x=1.16,\n            y=1.05,\n            xref='paper',\n            yref='paper',\n            text='Explained Variance',\n            showarrow=False,\n        )\n    ])\n)\n\nfig = dict(data=data, layout=layout)\npio.show(fig, filename='selecting-principal-components', renderer='notebook')","4b881586":"sklearn_pca = sklearnPCA(n_components=3)\nX_pca = sklearn_pca.fit_transform(X_std)\ny = y_he\ndata_pca = pd.DataFrame(X_pca, columns = ['PC1', 'PC2','PC3'])\ndata_pca['Label'] = y_he + 1","794f5966":"sns.lmplot(x=\"PC1\", y=\"PC2\",\n           data=data_pca,\n           fit_reg=False,\n           markers='.',\n           hue='Label', # color by cluster\n           legend=True,\n           scatter_kws={\"s\": 10}\n          )\npcaplot = plt.gca()\n_ = pcaplot.set_title('Visualizing first two Principal Components')","25302cc0":"def scatter_plot(plot, data, x_label, y_label, z_label, class_label, c, m, label):\n#     print(data['Label'] == class_label)\n    data_pca[data_pca['Label'] == class_label]\n    x = data[ data['Label'] == class_label ]['PC1'] #groupby Name column x_label\n#     display(x)\n    y = data[ data['Label'] == class_label ]['PC3']\n    z = data[ data['Label'] == class_label ]['PC2']\n    # s: size point; alpha: transparent 0, opaque 1; label:legend\n    plot.scatter(x,y,z,color=c, edgecolors='face', s=5, alpha=0.9, marker=m,label=label)\n    plot.set_xlabel(x_label)\n    plot.set_ylabel(y_label)\n    plot.set_zlabel(z_label)\n    return\n\nplot = plt.figure(figsize=(9,9)).gca(projection='3d')\nplt.title('Visualizing first three Principal Components')\n# scatter_plot\ncolors = ['b', 'o', 'g', 'r', 'v', 'b']\nscatter_plot(plot, data_pca, 'PC1','PC2','PC3', 1,'royalblue','.','0')\nscatter_plot(plot, data_pca, 'PC1','PC2','PC3', 2,'orange','.','0')\nscatter_plot(plot, data_pca, 'PC1','PC2','PC3', 3,'g','.','0')\nscatter_plot(plot, data_pca, 'PC1','PC2','PC3', 4,'red','.','0')\nscatter_plot(plot, data_pca, 'PC1','PC2','PC3', 5,'mediumpurple','.','0')\nscatter_plot(plot, data_pca, 'PC1','PC2','PC3', 6,'brown','.','0')\nscatter_plot(plot, data_pca, 'PC1','PC2','PC3', 7,'hotpink','.','0')\n","b0528254":"def plot_k_cross_val(X, y, title=\"\", cv=None, n_jobs=1):\n    \n    fig = go.Figure() \n    ks = [i for i in range (1,11)]\n    train_scores = np.array([]).reshape(0,5)\n    test_scores = np.array([]).reshape(0,5)\n    for k in ks:\n        sklearn_pca = sklearnPCA(n_components=7)\n        knn_cv= KNeighborsClassifier(n_neighbors=k, p=2, weights='distance', n_jobs=8)\n        knn_pca_pipeline = make_pipeline(sklearn_pca ,knn_cv)\n        scores = cross_validate(knn_pca_pipeline, X, y, cv=cv,\n                                scoring=scoring, return_train_score=True,\n                                n_jobs=1)\n        train_scores = np.concatenate((train_scores, scores['train_f1'].reshape(1, 5)), axis=0)\n        test_scores = np.concatenate((test_scores, scores['test_f1'].reshape(1, 5)), axis=0)\n        \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n#     if(colnum==1):\n#         leg=True\n#     else:\n#         leg=False\n    leg=True\n        \n    p1 = go.Scatter(x=ks, y=test_scores_mean + test_scores_std,\n                    mode='lines',\n                    line=dict(color=\"green\", width=1),\n                    showlegend=False,  )\n    fig.add_trace(p1)\n    \n    p2 = go.Scatter(x=ks, y=test_scores_mean - test_scores_std,\n                    mode='lines',\n                    line=dict(color=\"green\", width=1),\n                    showlegend=False, fill='tonexty')\n    fig.add_trace(p2)\n    \n    p3 = go.Scatter(x=ks, y=train_scores_mean + train_scores_std,\n                    mode='lines',\n                    line=dict(color=\"red\", width=1),\n                    showlegend=False)\n    fig.add_trace(p3)\n    \n    p4 = go.Scatter(x=ks, y=train_scores_mean - train_scores_std,\n                    mode='lines',\n                    line=dict(color=\"red\", width=1),\n                    showlegend=False, fill='tonexty')\n    fig.add_trace(p4)\n    \n    p5 = go.Scatter(x=ks, y=train_scores_mean, \n                    marker=dict(color='red'),\n                    name=\"Training\", showlegend=leg)\n    fig.add_trace(p5)\n    \n    p6 = go.Scatter(x=ks, y=test_scores_mean, \n                    marker=dict(color='green'),\n                    name=\"validation\", showlegend=leg)\n    fig.add_trace(p6)\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"k neighbors\",\n        yaxis_title=\"Macro F1-score\",\n    )\n    pio.show(fig)\n  \nplot_k_cross_val(X_train_he, y_train_he, cv=skf)","7e8706e5":"sklearn_pca = sklearnPCA(n_components=7)\nknn_pca = KNeighborsClassifier(n_jobs=8)\nknn_pca_pipeline = make_pipeline(sklearn_pca ,knn_pca)\nparam_grid = {\n    'kneighborsclassifier__n_neighbors': [1, 4, 6, 8, 10],\n    'kneighborsclassifier__weights' : ['uniform', 'distance'],\n    'kneighborsclassifier__p' : [1, 2]\n}","74ce0c39":"grid_search = GridSearchCV(estimator=knn_pca_pipeline,\n                           param_grid=param_grid,\n                           scoring=['accuracy', 'f1_macro'],\n                           refit='f1_macro',\n                           cv=skf,\n                           n_jobs=8)","40248494":"grid_search.fit(X_train_he, y_train_he)","99c71a8f":"print_best_n_classifiers(grid_search, 5)","5d2e4a2b":"sklearn_pca = sklearnPCA(n_components=7)\nknn_pca = KNeighborsClassifier(n_jobs=8, n_neighbors=4, p=1, weights='distance')\nknn_pca_pipeline = make_pipeline(sklearn_pca ,knn_pca)","358c9395":"plot_learning_curve(grid_search.best_estimator_,X_train_he, y_train_he, title=\"KNN Classifier\", cv=skf, n_jobs=8)","1a902439":"grid_search.best_estimator.fit(X_train_he, y_train_he)\ny_pred = knn_pca_pipeline.predict(X_test_he)\ntest_f1 = f1_score(y_test_he, y_pred, average='macro')\ntest_acc = accuracy_score(y_test_he, y_pred)\nconf_matrix(y_test_he, y_pred)\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_le_new, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test_le_new, y_pred)))","8ec2ccad":"little_tree = tree.DecisionTreeClassifier(max_depth=2)\nlittle_tree.fit(X_train_le, y_train_le)\nclass_names = ['Spruce\/Fir', 'Lodgepole Pine', 'Ponderosa Pine',\n               'Cottonwood\/Willow', 'Aspen', 'Douglas-fir', \n               'Krummholz']\ndot_data = tree.export_graphviz(little_tree, out_file=None,\n                               feature_names=le_df.columns[:12],\n                               class_names=class_names,\n                               filled=True, rounded=True)\n# graph\npydot_graph = graph_from_dot_data(dot_data)\npydot_graph.write_png('images\/original_tree.png')\n# pydot_graph.set_size('\"5,5!\"')\n# pydot_graph.write_png('images\/resized_tree.png')\nImage('images\/original_tree.png')","f93a4a1a":"sm_tree = SMOTE(k_neighbors=10, n_jobs=8, random_state=42)\ndt_pipeline = imbPipeline(steps=[('smote', sm_tree),\n                               ('tree', DecisionTreeClassifier(random_state=0))])\nscores = cross_validate(dt_pipeline, X_train_le, y_train_le, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=8)\nprint_cv_scores(scores)","5f70300f":"dt_pipeline.fit(X_train_le, y_train_le)\nfeature_importance = dict(zip(le_df.columns, dt_pipeline.steps[1][1].feature_importances_))\nsorted_feature_importance = sorted(feature_importance.items(),\n                                  key = lambda kv: kv[1], reverse=True)\n# convert list of tuple in a dictionary\nsorted_feature_importance = {k:v for k,v in sorted_feature_importance}\n# display(sorted_feature_importance)\nlabels = list(sorted_feature_importance.keys())\nvalues = list(sorted_feature_importance.values())\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values)])\nfig.update_layout(title=\"Feature importance pie plot\",\n                  autosize=False,\n                  width=800, height=450)\npio.show(fig, renderer='notebook')","23c93996":"def plot_depth_cross_val(X, y, title=\"\", cv=None, n_jobs=8):\n    \n    fig = go.Figure() \n    ds = [i for i in range (1,41, 5)]\n    train_scores = np.array([]).reshape(0,5)\n    test_scores = np.array([]).reshape(0,5)\n    for d in ds:\n        dt_pipeline = DecisionTreeClassifier(max_depth=d,random_state=0)\n        scores = cross_validate(dt_pipeline, X, y, cv=cv,\n                                scoring=scoring, return_train_score=True,\n                                n_jobs=n_jobs)\n        train_scores = np.concatenate((train_scores, scores['train_f1'].reshape(1, 5)), axis=0)\n        test_scores = np.concatenate((test_scores, scores['test_f1'].reshape(1, 5)), axis=0)\n        \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    \n    leg=True\n        \n    p1 = go.Scatter(x=ds, y=test_scores_mean + test_scores_std,\n                    mode='lines',\n                    line=dict(color=\"green\", width=1),\n                    showlegend=False,  )\n    fig.add_trace(p1)\n    \n    p2 = go.Scatter(x=ds, y=test_scores_mean - test_scores_std,\n                    mode='lines',\n                    line=dict(color=\"green\", width=1),\n                    showlegend=False, fill='tonexty')\n    fig.add_trace(p2)\n    \n    p3 = go.Scatter(x=ds, y=train_scores_mean + train_scores_std,\n                    mode='lines',\n                    line=dict(color=\"red\", width=1),\n                    showlegend=False)\n    fig.add_trace(p3)\n    \n    p4 = go.Scatter(x=ds, y=train_scores_mean - train_scores_std,\n                    mode='lines',\n                    line=dict(color=\"red\", width=1),\n                    showlegend=False, fill='tonexty')\n    fig.add_trace(p4)\n    \n    p5 = go.Scatter(x=ds, y=train_scores_mean, \n                    marker=dict(color='red'),\n                    name=\"Training\", showlegend=leg)\n    fig.add_trace(p5)\n    \n    p6 = go.Scatter(x=ds, y=test_scores_mean, \n                    marker=dict(color='green'),\n                    name=\"validation\", showlegend=leg)\n    fig.add_trace(p6)\n    fig.update_layout(\n        title=title,\n        xaxis_title=\"depth size\",\n        yaxis_title=\"Macro F1-score\",\n    )\n    pio.show(fig, renderer='notebook')\n  \nplot_depth_cross_val(X_train_le, y_train_le, cv=skf)","0014af73":"decision_tree_param = {\n    'tree__criterion': ['entropy'],\n    \"tree__min_samples_split\": [3],\n    \"tree__max_depth\": [20, 24, 28, 32],\n    'tree__splitter' : ['best'],\n    'tree__max_features' : [None],\n    'smote__k_neighbors' : (5, 10)\n}\ndt_pipeline = imbPipeline(steps=[('smote', SMOTE(n_jobs=1, random_state=42)),\n                                 ('tree', DecisionTreeClassifier(random_state=0))])\n\ngrid_search_dt = GridSearchCV(estimator=dt_pipeline,\n                           param_grid=decision_tree_param,\n                           scoring=['accuracy', 'f1_macro'],\n                           refit='f1_macro',\n                           cv=3,\n                           n_jobs=8)\n_ = grid_search_dt.fit(X_train_le, y_train_le)","ecf0f4a5":"print_best_n_classifiers(grid_search_dt, 5)","b8272e6f":"plot_learning_curve(grid_search_dt.best_estimator_,  X_train_le, y_train_le, title='Decision Tree Classifier', cv=skf, n_jobs=8)","43f54d16":"y_pred = grid_search_dt.best_estimator_.predict(X_test_le)\ntest_f1 = f1_score(y_test_le, y_pred, average='macro')\ntest_acc = accuracy_score(y_test_le, y_pred)\nconf_matrix(y_test_le, y_pred)\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_le_new, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test_le_new, y_pred)))","40d2ef4e":"bagging = BaggingClassifier(n_jobs=8)\nscores = cross_validate(bagging, X_train_le, y_train_le, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=1)\nprint_cv_scores(scores)","648a8dd4":"dt = DecisionTreeClassifier(criterion='entropy', max_depth=32,\n                           max_features=None, min_samples_split=3,\n                           splitter='best') \nbagging_prev_dt = BaggingClassifier(base_estimator=dt, n_estimators=100, bootstrap=True,\n                                    n_jobs=8, random_state=0)\nscores = cross_validate(bagging_prev_dt, X_train_le, y_train_le, cv=skf,\n                        scoring=scoring, return_train_score=True,\n                        n_jobs=1)\nprint_cv_scores(scores)","7cce83b8":"bagging_prev_dt = BaggingClassifier(base_estimator=dt, n_estimators=100, bootstrap=True,\n                                    n_jobs=8, random_state=0)\nplot_learning_curve(bagging_prev_dt,  X_train_le, y_train_le, title='Bagging Classifier', cv=skf, n_jobs=1)","eab804c1":"bagging_prev_dt = BaggingClassifier(base_estimator=dt, n_estimators=100, bootstrap=True,\n                                    n_jobs=8, random_state=0)\n\nbagging_prev_dt.fit(X_train_le, y_train_le)\ny_pred = bagging_prev_dt.predict(X_test_le)\ntest_f1 = f1_score(y_test_le, y_pred, average='macro')\ntest_acc = accuracy_score(y_test_le, y_pred)\nconf_matrix(y_test_le, y_pred)\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_le_new, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test_le_new, y_pred)))","d6f23371":"random_forest = RandomForestClassifier(n_estimators=100, random_state=0, class_weight='balanced', n_jobs=8)\nscores = cross_validate(random_forest, X_train_le, y_train_le, cv=skf, scoring=scoring,\n                        return_train_score=True,n_jobs=1)\nprint_cv_scores(scores)","1b7d6bdb":"random_forest = RandomForestClassifier(n_estimators=100, random_state=0, class_weight='balanced', n_jobs=8)\nrandom_forest.fit(X_train_le, y_train_le)\nfeature_importance = random_forest.feature_importances_\nfeature_importance = {X_train_le.columns[i] : feature_importance [i] for i in range(len(X_train_le.columns))}\nsorted_feature_importance = sorted(feature_importance.items(),\n                                  key = lambda kv: kv[1], reverse=True)\n# convert list of tuple in a dictionary\nsorted_feature_importance = {k:v for k,v in sorted_feature_importance}\n# display(sorted_feature_importance)\nlabels = list(sorted_feature_importance.keys())\nvalues = list(sorted_feature_importance.values())\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values)])\nfig.update_layout(title=\"Feature importance pie plot\",\n                  autosize=False,\n                  width=800, height=450)\npio.show(fig, render='notebook')","64558c77":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start=50, stop=500, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# class_weight for handling imbalanced dataset\nclass_weight = [None, 'balanced'] \n# # Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap,\n               'class_weight' : class_weight\n              }\n# random_grid","e52f40e5":"rf = RandomForestClassifier(n_jobs=8, random_state=0)\nrf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n                               n_iter=20, scoring=['accuracy', 'f1_macro'],\n                               cv=skf, refit='f1_macro', random_state=42,\n                               n_jobs=1\n                              )\nrf_random.fit(X_train_le, y_train_le)\nprint_best_n_classifiers(rf_random, 5)","66845852":"plot_learning_curve(rf_random.best_estimator_,  X_train_le, y_train_le, \"Random forest\", cv=skf, n_jobs=1)","0587fe2d":"rf = RandomForestClassifier(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_features='auto',\n                            max_depth=40, class_weight=None, bootstrap=False, n_jobs=8, random_state=0)\nrf.fit(X_train_le, y_train_le)\ny_pred = rf.predict(X_test_le)\ntest_f1 = f1_score(y_test_le, y_pred, average='macro')\ntest_acc = accuracy_score(y_test_le, y_pred)\nconf_matrix(y_test_le, y_pred)\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_le_new, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test_le_new, y_pred)))","f4ea5332":"class_weights = compute_class_weight('balanced', np.unique(y_train_le), y_train_le)\nweight_array = [class_weights[y_train_le.iloc[i]-1] for i in range(len(y_train_le.values))]","5fd43cf7":"# baseline sensata\nbst = XGBClassifier(max_depth=10, n_estimators=100, random_state=0, objective='multi:softmax', n_jobs=8)\nscores = cross_validate(bst, X_train_le, y_train_le, cv=skf, scoring=scoring,\n                        return_train_score=True, fit_params={'sample_weight': weight_array},\n                        n_jobs=1)\nprint_cv_scores(scores)","415fcde1":"parameters = {\n        'silent': [False],\n        'max_depth': [6, 10, 15, 20],\n        'learning_rate': [0.001, 0.01, 0.1, 0.2, 0,3],\n        'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'colsample_bytree': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'colsample_bylevel': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n        'min_child_weight': [0.5, 1.0, 3.0, 5.0, 7.0, 10.0],\n        'gamma': [0, 0.25, 0.5, 1.0],\n        'reg_lambda': [0.1, 1.0, 5.0, 10.0, 50.0, 100.0],\n        'n_estimators': [100]\n}\nxgb_clf = XGBClassifier(objective='multi:softmax', nthread=8)\nskf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\nrandom_search = RandomizedSearchCV(xgb_clf, param_distributions=parameters,\n                                   scoring=['accuracy', 'f1_macro'], n_jobs=1,\n                                   n_iter=20, cv=skf, verbose=3,\n                                   refit='f1_macro', random_state=42)\n_ = random_search.fit(X_train_le, y_train_le)","088f7398":"print_best_n_classifiers(random_search, 5)","d6ef290b":"bestXGBC = XGBClassifier(subsample= 0.6, silent= False, reg_lambda= 1.0,\n                        n_estimators= 100, min_child_weight= 1.0, max_depth= 24,\n                        learning_rate= 0.1, gamma= 0.25, colsample_bytree= 0.8,\n                        colsample_bylevel= 0.8, n_jobs=8)\nscores = cross_validate(bestXGBC, X_train_he, y_train_he, cv=skf, scoring=scoring,\n                        return_train_score=True, n_jobs=8)\nprint_cv_scores(scores)","d1aa8db7":"bestXGBC = XGBClassifier(subsample= 0.6, silent= False, reg_lambda= 1.0,\n                        n_estimators= 50, min_child_weight= 1.0, max_depth= 24,\n                        learning_rate= 0.1, gamma= 0.25, colsample_bytree= 0.8,\n                        colsample_bylevel= 0.8, n_jobs=8)\nplot_learning_curve(bestXGBC, X_train_le, y_train_le, \"Gradient Boosting\", cv=skf, n_jobs=1)","aeaa9f90":"bestXGBC.fit(X_train_le, y_train_le)\ny_pred = bestXGBC.predict(X_test_le)\nprint(conf_matrix(y_test_le, y_pred))\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_le, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test, y_pred)))","add95ccb":"mean_hill = (le_df['Hillshade_9am'] + le_df['Hillshade_Noon'] + le_df['Hillshade_3pm']) \/ 3\ndistance_hydro = ((le_df['Vertical_Distance_To_Hydrology']) ** 2 + le_df['Horizontal_Distance_To_Hydrology']) **(1\/2)\nsum_dist_road_fire = le_df['Horizontal_Distance_To_Roadways'] + le_df['Horizontal_Distance_To_Fire_Points']\ndiff_dist_road_fire = abs(le_df['Horizontal_Distance_To_Roadways'] - le_df['Horizontal_Distance_To_Fire_Points'])\nsum_dist_road_hydro = le_df['Horizontal_Distance_To_Roadways'] + le_df['Horizontal_Distance_To_Hydrology']\ndiff_dist_road_hydro = abs(le_df['Horizontal_Distance_To_Roadways'] - le_df['Horizontal_Distance_To_Hydrology'])\nsum_dist_fire_hydro = le_df['Horizontal_Distance_To_Fire_Points'] + le_df['Horizontal_Distance_To_Hydrology']\ndiff_dist_fire_hydro = abs(le_df['Horizontal_Distance_To_Fire_Points'] - le_df['Horizontal_Distance_To_Hydrology'])","819549e0":"le_df['mean_hill'] = mean_hill\nle_df['distance_hydro'] = distance_hydro\nle_df['sum_dist_road_fire'] = sum_dist_road_fire\nle_df['diff_dist_road_fire'] = diff_dist_road_fire\nle_df['sum_dist_road_hydro'] = sum_dist_road_hydro\nle_df['diff_dist_road_hydro'] = diff_dist_fire_hydro\nle_df['sum_dist_fire_hydro'] = sum_dist_fire_hydro\nle_df['diff_dist_fire_hydro'] = sum_dist_road_fire\nle_df.head(5)","392ed933":"#label encoded split\ny_le = le_df['Cover_Type'].copy()\ny_le -= 1\nX_le = le_df.drop(columns=['Cover_Type'])\nX_train_le_new, X_test_le_new, y_train_le_new, y_test_le_new = train_test_split(X_le, y_le, test_size=.2, random_state=0)\n                                           ","650ab6c3":"bestXGBC = XGBClassifier(subsample= 0.6, silent= False, reg_lambda= 1.0,\n                        n_estimators= 100, min_child_weight= 1.0, max_depth= 24,\n                        learning_rate= 0.1, gamma= 0.25, colsample_bytree= 0.8,\n                        colsample_bylevel= 0.8, n_jobs=8)\n_ = bestXGBC.fit(X_train_le_new, y_train_le_new)","bf126b6c":"feature_importance = bestXGBC.feature_importances_\nfeature_importance = {X_train_le_new.columns[i] : feature_importance [i] for i in range(len(X_train_le_new.columns))}\nsorted_feature_importance = sorted(feature_importance.items(),\n                                  key = lambda kv: kv[1], reverse=True)\n# convert list of tuple in a dictionary\nsorted_feature_importance = {k:v for k,v in sorted_feature_importance}\n# display(sorted_feature_importance)\nlabels = list(sorted_feature_importance.keys())\nvalues = list(sorted_feature_importance.values())\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values)])\nfig.update_layout(title=\"Feature importance pie plot\",\n                  autosize=False,\n                  width=800, height=450)\nfig.show()","ff330684":"y_pred = bestXGBC.predict(X_test_le_new)\nconf_matrix(y_test_le_new, y_pred)\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_le_new, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test_le_new, y_pred)))","a725afb6":"preprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_cols),\n        ('cat', SimpleImputer(), categorical_cols)\n    ]\n)","35a8589b":"svc = svm.LinearSVC(loss='squared_hinge', dual=False, penalty='l2', C=10000.0,\n                    multi_class='ovr', class_weight='balanced', max_iter=1000, \n                    random_state=0)\nsvc_clf = Pipeline(steps=[('preprocessing', preprocessor),\n                          ('svc', svc)])\nscores = cross_validate(svc_clf, X_train_he, y_train_he, cv=skf, scoring=scoring,\n                        return_train_score=True, n_jobs=8)\nprint_cv_scores(scores)","582ced1b":"from matplotlib.colors import ListedColormap\nfrom sklearn.kernel_approximation import (RBFSampler,\n                                          Nystroem)\nfrom sklearn.linear_model import SGDClassifier\n\ndef plot_decision_regions(X, y, classifier, test_idx=None, resolution=0.02):\n\n    # setup marker generator and color map\n    markers = ('s', 'x', 'o', '^', 'v', 'P', 'D')\n    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan', 'yellow', 'purple')\n    cmap = ListedColormap(colors[:])\n\n    # plot the decision surface\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                           np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n\n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1],\n                    alpha=0.8, c=colors[idx],\n                    marker=markers[idx], label=cl)\n\n    # highlight test samples\n    if test_idx:\n        # plot all samples\n        if not versiontuple(np.__version__) >= versiontuple('1.9.0'):\n            X_test, y_test = X[list(test_idx), :], y[list(test_idx)]\n            warnings.warn('Please update to NumPy 1.9.0 or newer')\n        else:\n            X_test, y_test = X[test_idx, :], y[test_idx]\n\n        plt.scatter(X_test[:, 0],\n                    X_test[:, 1],\n                    c='',\n                    alpha=1.0,\n                    linewidths=1,\n                    marker='o',\n                    s=55, label='test set')\n        \n\npca = sklearnPCA(n_components=2)\nX_train_he_pca = pca.fit_transform(preprocessor.fit_transform(X_train_he.iloc[:1000, :]))\n\nmodels = []\nmodels.append(Pipeline([(\"svm\", SGDClassifier(n_jobs=8, random_state=0))]))\nmodels.append(Pipeline([(\"feature_map\", Nystroem(kernel='polynomial', degree=3, random_state=0)),\n                     (\"svm\", SGDClassifier(n_jobs=8, random_state=0))]))\nmodels.append(Pipeline([(\"feature_map\", Nystroem(kernel='rbf', random_state=0, gamma=0.5)),\n                     (\"svm\", SGDClassifier(n_jobs=8, random_state=0))]))\nmodels.append(Pipeline([(\"feature_map\", RBFSampler(random_state=0, gamma=0.5)),\n                     (\"svm\", SGDClassifier(n_jobs=8, random_state=0))])) \nnsamples=1000\nplt.tight_layout()\nplt.figure(figsize=(18, 14))\ntitles = ['Linear SVC', 'Nystroem polynomial(degree=3) approximation',\n          'Nystroem rbf approximation gamma=0.5', 'Fourier rbf approximation gamma=0.5']\nfor i in range(4):\n    models[i].fit(X_train_he_pca, y_train_he.iloc[:nsamples])\n    plt.subplot(2, 2, i + 1)\n    plot_decision_regions(X_train_he_pca, y_train_he[:1000], models[i])\n    plt.title(titles[i])","ef27c140":"param = {\n    'sgdclassifier__alpha': [1000, 10, 0.1, 0.001, 0.00001, 0.0000001],\n    'sgdclassifier__loss': ['hinge', 'squared_hinge'],\n    'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n    'sgdclassifier__class_weight': [None, 'balanced']\n    \n}\n\n\nskf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\nsklearn_pca = sklearnPCA(n_components=30)\nsgd_classifier = SGDClassifier(max_iter=1000, average=True, n_jobs=8)\nsvm_clf = make_pipeline(preprocessor, sklearn_pca, sgd_classifier)\nrandom_search = RandomizedSearchCV(svm_clf, param_distributions=param,\n                                   scoring=['accuracy', 'f1_macro'], n_jobs=1,\n                                   n_iter=20, cv=skf, refit='f1_macro', random_state=42)\n\nrandom_search.fit(X_train_he, y_train_he)\nprint_best_n_classifiers(random_search, 5)","d0ee7e94":"param = {\n    'nystroem__gamma': [1000, 10, 0.1, 0.001, 0.00001, 0.0000001],\n    'nystroem__degree': [2, 3],\n    'nystroem__n_components' : [100],\n    'sgdclassifier__alpha': [1000, 10, 0.1, 0.001, 0.00001, 0.0000001],\n    'sgdclassifier__loss': ['hinge', 'squared_hinge'],\n    'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n    'sgdclassifier__class_weight': [None, 'balanced']\n    \n}\n\n\nskf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\nsklearn_pca = sklearnPCA(n_components=30)\nfeature_map_nystroem = Nystroem(kernel='polynomial', random_state=0)\nsgd_classifier = SGDClassifier(max_iter=1000, average=True, random_state=0, n_jobs=8)\nsvm_clf = make_pipeline(preprocessor, sklearn_pca, feature_map_nystroem,\n                        sgd_classifier)\nrandom_search = RandomizedSearchCV(svm_clf, param_distributions=param,\n                                   scoring=['accuracy', 'f1_macro'], n_jobs=1,\n                                   n_iter=20, cv=skf, verbose=3,\n                                   refit='f1_macro', random_state=42)\n\nrandom_search.fit(X_train_he, y_train_he)\nprint_best_n_classifiers(random_search, 5)","b0fd0205":"param = {\n    'nystroem__gamma': [1000, 10, 0.1, 0.001, 0.00001, 0.0000001],\n    'nystroem__n_components' : [100],\n    'sgdclassifier__alpha': [1000, 10, 0.1, 0.001, 0.00001, 0.0000001],\n    'sgdclassifier__loss': ['hinge', 'squared_hinge'],\n    'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n    'sgdclassifier__class_weight': [None, 'balanced']\n    \n}\n\n\nskf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\nsklearn_pca = sklearnPCA(n_components=30)\nfeature_map_nystroem = Nystroem(kernel='rbf', random_state=0)\nsgd_classifier = SGDClassifier(max_iter=1000, average=True, random_state=0, n_jobs=8)\nsvm_clf = make_pipeline(preprocessor, sklearn_pca, feature_map_nystroem,\n                        sgd_classifier)\nrandom_search = RandomizedSearchCV(svm_clf, param_distributions=param,\n                                   scoring=['accuracy', 'f1_macro'], n_jobs=1,\n                                   n_iter=20, cv=skf,\n                                   refit='f1_macro', random_state=42)\n\nrandom_search.fit(X_train_he, y_train_he)","586fdf9f":"print_best_n_classifiers(random_search, 5)","6395f97d":"from sklearn.kernel_approximation import RBFSampler\nparam = {\n    'rbfsampler__gamma': [1000, 10, 0.1, 0.001, 0.00001, 0.0000001],\n    'rbfsampler__n_components' : [100],\n    'sgdclassifier__alpha': [1000, 10, 0.1, 0.001, 0.00001, 0.0000001],\n    'sgdclassifier__loss': ['hinge', 'squared_hinge'],\n    'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet'],\n    'sgdclassifier__class_weight': [None, 'balanced']\n    \n}\n\n\nskf = StratifiedKFold(n_splits=5, random_state=0, shuffle=True)\nsklearn_pca = sklearnPCA(n_components=30)\nrbf_sampler = RBFSampler(random_state=0)\nsgd_classifier = SGDClassifier(max_iter=1000, average=True, random_state=0, n_jobs=8)\nsvm_clf = make_pipeline(preprocessor, sklearn_pca, rbf_sampler,\n                        sgd_classifier)\nrandom_search = RandomizedSearchCV(svm_clf, param_distributions=param,\n                                   scoring=['accuracy', 'f1_macro'], n_jobs=1,\n                                   n_iter=20, cv=skf, verbose=3,\n                                   refit='f1_macro', random_state=42)\n\n# random_search.fit(X_train_he, y_train_he)\nprint_best_n_classifiers(random_search, 5)","645767e1":"sklearn_pca = sklearnPCA(n_components=30)\nrbf_sampler = RBFSampler(random_state=0, n_components=100, gamma=0.001)\nsgd_classifier = SGDClassifier(penalty='l1', loss='squared_hinge', alpha=1e-07, max_iter=1000, average=True, random_state=0, n_jobs=8)\nsvm_clf = make_pipeline(StandardScaler(), sklearn_pca, rbf_sampler,\n                        sgd_classifier)\n\nplot_learning_curve(svm_clf,  X_train_he.values, y_train_he.values, title='SVM Classifier', cv=skf, n_jobs=1)","a2a10c71":"def dict_rus(y):\n    ratios = {0:15000, 1:15000, 2:15000, 6:15000}\n    ratios[3] = len(y[y == 3]) \n    ratios[4] = len(y[y == 4]) \n    ratios[5] = len(y[y == 5]) \n    return ratios","174b1230":"warnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nprint('Undersampled class label distribution:')\nrus = RandomUnderSampler(sampling_strategy=dict_rus, random_state=42)\nsvm_clf = svm.SVC(C=1000, gamma=0.1)\nsklearn_pca = sklearnPCA(n_components=30)\nscaler = StandardScaler()\nrus_x = scaler.fit_transform(X_train_he)\nrus_x = sklearn_pca.fit_transform(rus_x)\nrus_x, rus_y = rus.fit_resample(rus_x, y_train_he)\n\n_ = plt.hist(rus_y+1, bins=7, alpha=0.7, ec='k')","52e54416":"scores = {}\nscores['test_f1'] = np.array([])\nscores['test_acc'] = np.array([])\nfor tr_idxs, val_idxs in skf.split(X_train_he, y_train_he):\n    X_train, y_train = X_train_he.values[tr_idxs], y_train_he.values[tr_idxs]\n    X_val, y_val = X_train_he.values[val_idxs], y_train_he.values[val_idxs]\n    \n    rus = RandomUnderSampler(random_state=42)\n    svm_clf = svm.SVC(C=1000, gamma=0.1)\n    sklearn_pca = sklearnPCA(n_components=30)\n    scaler = StandardScaler()\n    rus_x = scaler.fit_transform(X_train)\n    rus_x = sklearn_pca.fit_transform(rus_x)\n    rus_x, rus_y = rus.fit_resample(rus_x, y_train)\n    svm_clf.fit(rus_x, rus_y)\n    rus_test = scaler.transform(X_val)\n    rus_test = sklearn_pca.transform(rus_test)\n    y_pred = svm_clf.predict(rus_test)\n    svm_clf.fit(rus_x, rus_y)\n    f1 = f1_score(y_val, y_pred, average='macro')\n    acc = accuracy_score(y_val, y_pred)\n    scores['test_f1'] = np.append(scores['test_f1'], f1) \n    scores['test_acc'] = np.append(scores['test_acc'], acc)\n    \nprint_cv_scores(scores)","1971a2eb":"rus = RandomUnderSampler(sampling_strategy=dict_rus, random_state=42)\npreprocessor = StandardScaler()\nsvm_clf = svm.SVC(C=1000, gamma=0.1)\nsklearn_pca = sklearnPCA(n_components=30)\nrus_x = preprocessor.fit_transform(X_train_he)\nrus_x = sklearn_pca.fit_transform(rus_x)\nrus_x, rus_y = rus.fit_resample(rus_x, y_train_he)\n\nsvm_clf.fit(rus_x, rus_y)\n\nrus_test = preprocessor.transform(X_test_he)\nrus_test = sklearn_pca.transform(rus_test)\ny_pred = svm_clf.predict(rus_test)\nprint(conf_matrix(y_test_he, y_pred))\nprint('f1-score: {0:.3f}'.format(f1_score(y_test_he, y_pred, average='macro')))\nprint('accuracy: {0:.3f}'.format(accuracy_score(y_test_he, y_pred)))","1aa960b2":"acc = [0.620, 0.970, 0.945, 0.970, 0.969, 0.972, 0.977, 0.826]\nf1 =  [0.541, 0.944, 0.915, 0.946, 0.945, 0.946, 0.955, 0.799]\nmodel = [\"SMOTE polynomial LR\", \"pca KNN\",\n         \"SMOTE Decision Tree\", \"Bagging\", \"Random Forest\",\n         \"XGBoost\", \"XGBoost FE\", \"RUS SVM\"\n        ]\ndata = pd.DataFrame(data = {'classifier': model, 'accuracy': acc, 'f1-score':f1})","f4c0343e":"fig1 = px.bar(data, x='classifier' , y='f1-score', color='f1-score',\n             labels={'f1-score':'f1-score'}, height=400)\nfig2 = px.bar(data, x='classifier' , y='accuracy', color='accuracy',\n             labels={'accuracy':'accuracy'}, height=400)\n\ntrace1 = fig1['data'][0]\ntrace2 = fig2['data'][0]\nfig = make_subplots(rows=2, cols=1)\nfig.add_trace(trace1, row=1, col=1)\nfig.add_trace(trace2, row=2, col=1)\nfig.update_layout(height=700)\nfig.update_yaxes(title_text=\"f1-score\", row=1, col=1)\nfig.update_yaxes(title_text=\"accuracy\", row=2, col=1)\npio.show(fig, renderer='notebook')","d2cd7c1d":"### Dataset integrity check <a class=\"tocSkip\"\/>","a9acf77c":"Although various approaches were tried, it still difficult to effectively optimize a good classifier due to the long training times.\n<br>Substantial improvement in metrics via SVM approximation can be achieved by moving closer to the exact SVM classifier, increasing the rank of kernel transformation matrix and the maximum number of iteration for SGD Classifier. However, this direction would be hugely expensive in terms of computational time.","549acf57":"The *scatterplot* is another valuable tool to inspect the existing relationship between features. It consists of a graph of two continuous variables in which the explanatory variable is plotted on the x-axis and the dependent one on the y-axis.\nNext, will explore all combination of two quantitative variables searching for patterns in data.","b00c9724":"As remarked before, the located tree species depend on given Wilderness_Area. In this plot, we can observe how different percentages of the cover type vary according to the wilderness area.<br>\nWilderness_Area may have a reduced predicting power in some classes like Lodgepole Pine or Spruce\/Fir Cover type, which are spread all across different areas. At the same time may be very discriminative for Cottonwood\/Willow, which can be found only on Cache la Poudre areas.","f2dbaa1e":"By adding predictors linear combinations, we effectively help our model in determining sharper decision boundaries. Feature engineering brought an increase in every single class f1-score.","333833a8":"# Model Selection","517d8f10":"#### Learning curve <a class=\"tocSkip\"\/>","93a67a29":"We will employ the bootstrap technique for resampling the original dataset into B training datasets and train a different tree on top of each one.\nSince we are interested in obtaining a single prediction for $\\mathscr{x_0}$, we then aggregate by voting as a sort of average each prediction $\\hat{f}^{*b}(\\mathscr{x_0})$, i.e. the resulting prediction will be the class predicted by the most of different bagging trees. \nThis method actually provides a variance error reduction in predictions.","61fe3ef7":"#### Learning curve <a class=\"tocSkip\"\/>","215c09bb":"Despite the model may overfit with the smaller training set, considering the more conspicuous gap with training accuracy, it benefits from adding more data like a sort of regularization. \n<br>This generally happens because bigger datasets make model harder to overfit, thus encouraging better generalization.","1216e6fc":"80K represents a reasonable size for fitting an exact SVM, therefore in the following experiment, we fit an RBF SVM classifier without approximations on top of about 80K samples from an undersampled dataset. We will, therefore, random undersample data by taking 15k training instances except for minority classes: 3, 4 and 6 for which we will consider all the available samples.","0cd4508b":"Through a random search, we discover a combination of parameters by means of which we can achieve a good validation f1-score. Model capacity results particularly sensitive to `n_estimators` and `max_depth`, hence by fixing the number of trees to 100 (to have reasonable training time) and slightly increase `max_depth` until symptoms of overfitting occur, we can improve the model further.","d40b0b14":"Cover_Type: forest cover type designation, its possible values are between 1 and 7, mapped in the following way:\n1. Spruce\/Fir\n1. Lodgepole Pine\n1. Ponderosa Pine\n1. Cottonwood\/Willow\n1. Aspen\n1. Douglas-fir\n1. Krummholz","5c612a86":"Categorical and numerical columns are identified in the dataset for further application of different model-dependent preprocessing methods.","3ec291c6":"The coefficients of the fitted model provide us some information on which are the most predictive features.","e587874d":"# Conclusions","b6989683":"Hyperparameters:\n1. `n_estimators`: number of trees, actually is not a critical parameter in this setup, because an high value would not lead to overfitting\n2. `bootstrap`: whether to sample with replacement or not from original dataset\n\n<p>\nAfter several attempts of different meaningful hyperparameters, it turns out that ensembling trees with params taken from the previous section best classifier outperform both baseline and the other models tried.\n<\/p>","4f042570":"## Support Vector Machines","e9352b88":"#### Horizontal_Distance_To_Fire_Points <a class=\"tocSkip\"\/>","f8425751":"#### Test set evaluation <a class=\"tocSkip\"\/>","54f858a7":"We can also notice from both pairplot and hotmap:\n\n1. a weak positive correlation between Elevation and Horizontal_Distance_To_Hydrology\n2. a weak positive correlation between Elevation and Horizontal_Distance_To_Roadways\n3. a week positive correlation between Horizontal_Distance_To_Roadways and Horizontal_Distance_To_Firepoints\n4. a moderate positive correlation between Horizontal_Distance_To_Hydrology and Vertical_Distance_To_Hydrology\n<br>\n\n<p>Highly elevated lands may be more distant from water, reasonably also from roads. As regards the last two interactions, they seem to be less intuitive.\n<\/p>","7cfd6d93":"#### Polynomial Classifier<a class=\"tocSkip\"\/>","e8f0c4ba":"Let's examine if a dependency exists even between the pair of features Soil_Type and Wilderness_Area","3f7f3dcf":"To check the integrity of categorical data, next, we sum for each row all the one-hot encoded columns expecting for each qualitative predictor a value exactly equal to 1(we sum several 0 values and at most a 1 value).","a0baa370":"#### SGDC with RBF kernel approximation learning curve <a class=\"tocSkip\"\/>","50aeb395":"#### Testing the best model <a class=\"tocSkip\"\/>","f937026a":"The best model according to the estimation of validation metrics is composed of *SMOTE* with $k=10$ neighbours for tackling class imbalance and *degree-2 polynomial logistic regression* ","4a09a6ac":"#### Hyperparamter tuning <a class=\"tocSkip\"\/>","9c1aa0f1":"Almost all classifiers except SVM and XGBoost have allowed us to obtain relatively good results since baseline models, whereas the previously mentioned ones make hyperparameter optimization definitely harder. Moreover, we have seen that tree models and KNN benefits a lot from *complex non-linear dependencies*, whereas linear classifiers (logistic regression and SVC) cannot. Next, we summarize the overall results obtained using the chosen metrics in the following bar chart:","6dc8b801":"*Stratification* seeks to ensure that each fold is representative of the full dataset, also reducing variance between each fold evaluation, which generally leads to a better estimate of test evaluation.<br>\nIn particular stratified method involves randomly dividing the training set into $k$ groups, preserving the percentage of samples for each class: one of these folds is kept as a validation set, whereas the others are kept for training. This procedure is repeated $k$ times, alternating each time the validation phase on a different fold and then computing the final estimate by averaging the $k$ evaluations.\n\\begin{equation}\nCV_{(n)} = \\frac{1}{n} \\sum\\limits_{i=1}^{n} {F_1}_i \n\\end{equation}\n<p>The choice of $k = 5$ is motivated by empirical evidence that this choice yield test error estimates that suffer neither from excessively high bias nor from very high variance.","12f41062":"*Hotmap* provides an intuitive representation of Pearson's correlation coefficient between each pair of numerical features.","2a11c7dc":"*Feature importance* is computed as the (normalized) total reduction of the criterion brought by that feature. It is also known as the Gini importance.\n<br>This pie plot, with its easy interpretability, shows us for every feature, how much it leads to a *purity* improvement in child nodes after splitting its range of values.","ec54cc36":"# Introduction","fd1d53fe":"Actually, our SGDC with squared hinge loss and RBF low-rank approximation kernel since the beginning of the curve(37K training size) almost does not benefit from a bigger training set.\nWe may exploit such insight for further experiments by undersampling data.","44172071":"For the *Central Limit Theorem*, given a set of *n* independent observation $Z_1, ..., \nZ_n$, each with variance $\\sigma^2$, the variance of the mean $\\overline{Z}$ of the observations is given by $\\sigma^2\/n$.\n<br>Similarly, by averaging prediction from different classifiers, we expect a *reduction of variance*.\n<br>In the *bootstrap method*, we emulate the process of obtaining a new independent sample set from the population by sampling with replacement from the original dataset. This technique is usually applied in the calculation of estimator properties such as sample mean, e.g. estimating standard error or confidence interval.","d6eb36a2":"Logistic regression models the probability that an instance belongs to a particular category.\nIn logistic model logistic function is used.\n\\begin{equation}\np( X ) = \\frac{ e^{{\\beta_0} + {\\beta_1 X}} } { 1 + e^{{\\beta_0} + {\\beta_1 X}} }\n\\end{equation}\n\nAfter some manipulation, we can obtain a linear model but on a non-linear scale of probability.\n\n\\begin{equation}\n\\log\\left(\\frac{p( X )}{1-p(x)}\\right) = \\beta_0 + \\beta_1 X\n\\end{equation}\n\n\nTo fit this model maximum likelihood method is used. The intuition is that we search for best parameters $\\beta_0$, $\\beta_1$ that maximize the probability of obtaining the given dataset. \n\n\\begin{equation}\n\\hat{\\beta}_0, \\hat{\\beta}_1 = \\arg \\max\\limits_{\\beta_0 \\in B_0, \\beta_1 \\in B_1} \\mathcal{L} (\\beta_0, \\beta_1 | x_1, ..., x_n)\n\\end{equation}\n\nAlthough we have done some transformation to the original logistic function, logarithm does not alter the solution and makes log-likelihood faster to compute.\n\nIn this work we should classify instances into more than two classes, then two possible approaches are allowed:\n- $B$  one class vs the rest classifiers, with $B = |\\{C\\}|$\n- multinomial logistic regression\n\\begin{equation}\nsoftmax(x)_i = \\frac{e^{x_i}}{\\sum_{j}^{B}e^{x_j}},    i = 1, ... B\n\\end{equation}\n\nWe go for *multinomial logistic regression* as it usually leads to better results in shorter training time.","ff5ef402":"## Random Forest","bca8853c":"#### The Proportion of Variance Explained <a class=\"tocSkip\"\/>","f69cecc9":"Boosting learning curve has similar trend to the previous two ensemble tree-based method.","878d062f":"Random forest, like other tree-based methods, behaves well from adding more data.","ef69fc10":"In this notebook, we will not make any assumption on the outliers by leaving them as they are in the dataset.","5e5263f0":"We perform hyperparameters selection by fitting and evaluating several models, characterized by specific parameters, through stratified K-cross validation.\nIn grid search model are build by picking every single parameter from a grid of candidates exploring all the possible combination in an exhaustive search.\n<br>\nRecall that validation metrics involved in the evaluation are the previously mentioned: f1-score and accuracy.","e5373cbc":"In the next plots, we pick a subset of first 1000 instances from the training set and we train an SVC on four different kernel mapping approximations on top of the first two principal components.\n<br>In such a way, we can visualize and get an idea about linear, *polynomial* and *RBF* (both Nystroem and Fourier approximation) kernel approximations decision boundaries.","8925aa21":"A decision tree model stratifies the data space in different regions as a result of various *recursive binary splits*, which compose a series of decision rules.\n<br>In an effort to grow the optimum tree, it is computationally infeasible to evaluate every combination of possible splits, as a consequence, they are performed in a top-down greedy fashion.\nWhen building a tree two possible measure are used to evaluate *node purity* and so the goodness of a future split: \n1. *Gini index*:\n    \\begin{equation}\n    G = \\sum\\limits_{k=1}^{K} \\hat{p}_{mk} (1 - \\hat{p}_{mk})\n    \\end{equation}\n    <br>where $\\hat{p}_{mk}$ represents the portion of training observation in the *m*-th region that are from the *k*-th class\n2. *Cross entropy*:\n    \\begin{equation}\n    D = - \\sum\\limits_{k=1}^{K} \\hat{p}_{mk} \\log{\\hat{p}_{mk}}\n    \\end{equation}\n<br>In performing a split for a given node, we need to \n    determine which feature split allows the best improvement in terms of purity for such node.<br>\n    Tree models are therefore built by selecting the current best possible split *greedily*, at each step.\n<\/p>","b0f1840c":"We start from training using a baseline model with balanced class weights and default parameters, 100 10-depth boosted trees.","58987a7e":"In this section, we followed two possible approaches to deal with datasets proportions, kernel approximations with SGDC and exact SVMs trained on undersampled data. \nThe experiments with undersampled data, as we notice, lead us to better results, though with an inter-class f1-score standard deviation of 0.80. Further improvement could be made by using more training data or employing more complex undersampling methods in order to lose as little information as possible from the original data.","be8eb58b":"Let's visualize how $K$ affects our classification metric, f1-score.","7c14728e":"Considering the overall f1-score and confusion matrix result, here we got slightly better results, although we suffer from less sensitivity for class 4 and 5.","a7c00c04":"- It would be worth it to try different strategies in handling outliers, rather than simply leave them as they are.\n- Throughout the model selection, the most challenging task was dealing with unbalanced data. Finer tuning of parameters seems the only way to obtain a lower standard deviation in f1-score macro among all classes and consequently a more unbiased classifier. <br>In this direction could be reasonable a better tuning of class weights or resampling ratios by means of cross-validation. Another possible countermeasure could be choosing a metric that penalizes classifiers with higher inter-class f1-score standard deviation.<br>For this reason, our best classifier performs worst than average in detecting class 4 and 5 instances as we can see from XGBoost test confusion matrix and although several attempts were tried in order to help the model to improve its sensitivity, these have led to higher sensitivity for these classes at the expenses of precision, hence lowering their overall f1-scores.\n- Further new variables could be added on the XGBoost model like soil characteristics as the ones in soil descriptions in EDA (rubbly, very stony, extremely stony, etc.), categorical interactions or even less meaningful interaction in a brute-force approach. Best predictors can be further selected by examining feature importance and removing the less meaningful ones in an attempt to keep the dataset with a manageable number of columns. ","fc511edc":"### Further improvement with feature engineering","d923ee7a":"Definitely, quadratic terms provide a slight improvement to the model. ","574b083d":"### Testing the best model <a class=\"tocSkip\"\/>","73462249":"#### Soil_Type  Cover_Type Chi2 test <a class=\"tocSkip\"\/>","b4a4f04b":"#### Kernels <a class=\"tocSkip\"\/>","0ed941ef":"#### Linear Classifier <a class=\"tocSkip\">","0d192bc0":"#### Testing best parameters <a class=\"tocSkip\"\/>","4f3faa47":"### Features distribution","36631982":"#### PCA as Exploratory Data Analysis <a class=\"tocSkip\"\/>","ce7a2dd3":"After inputting the predicted values of Hillshade_3pm, data seems to follow the overall joint distribution better.","1c548534":"Data point in this projection are rather overlapped that it is not even possible to see points from all the classes. Despite this ,it may be interesting to notice that all classes have a sort of U shape once projected on first two PCs.","b994ea18":"For logistic regression, we will use the *dummy encoded* version of the dataset after that some preprocessing is performed, especially for the numerical feature, which needs to be standardized. Standardization is mandatory for *saga* solver because it is susceptible to features scale.","16fad618":"### Normalized Confusion Matrix","18e961e5":"### Baseline Model <a class=\"tocSkip\"\/>","ccd6e063":"#### Testing best parameters <a class=\"tocSkip\"\/>","4664ae59":"The main problem of *kernel methods* is its high computational cost connected with kernel matrices. The cost is at least quadratic in the number of training data points, but most kernel methods include computation of matrix inversion or eigenvalue decomposition and the cost becomes cubic in the number of training data.<br>\nKernel approximation, using a subset of data as a basis, allow us to better scale on large datasets and to approximate feature mappings of certain kernels as a result of a non-linear transformation of the input.\n<br>*Nystroem Method* consists in approximating the kernel matrix $\\hat{K}\\in\\mathbb{R}^{nxn}$, that represents data in the kernel method, with $\\tilde{K}\\in\\mathbb{R}^{nxq}$ of rank $q$.<br>\n\nIn order to compute that kernel, we need first to choose $q$ via `n_components` parameter and then we are going to train an SVC on top of the newly obtained features.","6ffd7c74":"Decision tree classifier is somewhat balanced in terms of recall between different classes, but again a not negligible portion of class 4 was misclassified as class 3. Overall macro f1-score is also affected by lower than average precisions in classes 4, 5 and 6.","deab2a6e":"Rawah and Comanche Peak areas (WA 1 and WA 3) are the most common in the whole dataset due to their assortment of tree species. These areas typically have lodgepole pine (class 2) as primary species, followed by spruce\/fir(class 1) and aspen (class 5).\nDataset is consequently *extremely imbalanced* towards the first two classes.","4778b3bc":"Correlation analysis is a method to evaluate first-order interaction between numerical features.<br>In particular, *Pearson's correlation* is a measure of the linear relationship strength between two numerical variables.\n<br>\n<br>\n\\begin{equation}\n    \\rho_{X,Y} =  \\frac{\\text{cov}(X,Y)}{\\sigma_X\\sigma_Y} = \\frac{\\text{E}[(X - \\mu_X)(Y-\\mu_Y)]}{\\sigma_X\\sigma_Y} \\end{equation}\n<br>\nwhere $\\mu_X$ and $\\mu_Y$ are the expected values $\\sigma_X$ $\\sigma_Y$ standard deviations.\nThe correlation coefficient can assume values that range from -1 to 1, where the extremes respectively represent a strong or a negative relationship, while 0 implies the absence.\n<br>In particular, a resulting positive coefficient means that an increase in one variable corresponds to an increase in the second one, proportionally to the strength.\nContrarily, a negative coefficient will result from variables that move in the opposite direction, therefore as x increase, y decreases, and vice versa.\nNotice that even a strong correlation cannot prove causation between two variables.\n","bfaa39d2":"### Baseline model <a class=\"tocSkip\"\/>","237ebb2e":"### SVC","5cdf77f9":"#### Kernel Approximation <a class=\"tocSkip\"\/>","27cdd78f":"*Evaluation metrics* are crucial in expressing and comparing model performances. Metrics also provide, during the validation phase, valuable feedback for *hyperparameters optimization*.\nIn choosing the appropriate metric, the class *imbalance* should be seriously considered.\n<br>Accuracy metric is a typical case in which a high score may be misleading if reached with a poor model biased towards predicting majority classes, which in our dataset amount to 85% of all instances.\n<br>Consequently, in such cases, *f1-score* can be a fair metric that behaves impartially with imbalanced data and can be easily adaptable to a multiclass classification problem.\n\\begin{equation}\n\\end{equation}\n\n\\begin{equation}\nF_1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}\n\\qquad\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\end{equation}\n\nwhere $precision = \\frac{TP}{TP + FP}$ and $recall = \\frac{TP}{TP + FN}$","ed83eb36":"### Hyperparameters tuning","51481953":"In this section, our baseline model will be a decision tree classifier trained on top of SMOTE oversampled training data, which in general provides better performance than class weights on original imbalanced data.","3d325d2e":"### First a little preprocessing <a class=\"tocSkip\"\/>","e8ff9789":"## Numerical Features ","e433299a":"### Polynomial logistic regression","7e93ba6e":"Data standardization plays a decisive role with SVC and SVM classifiers since they are very sensible from feature scaling. If trained with unstandardized data, hyperplane orientation would depend more on feature range of data than on its distribution.\nPreprocessing aside, our baseline model will be a linear SVC with `squared_hinge` loss, `C = 10^5` with `l2` penalty and `class_weights='balanced'`.","03d34cfc":"#### Wilderness_Area Cover_Type Chi2 Test <a class=\"tocSkip\"\/>","43da4100":"#### Scatterplot matrix <a class=\"tocSkip\"\/>","8c20b6c0":"KNN classifier performs overall well on test data, except for class 4, which has a lower recall, caused by many misclassifications of class 4 instances as class 3.","0f4d4305":"From these plots, we can notice some dependencies between:\n1. hillshades and Aspect \n1. hillshades and Slope\n1. between hillshades themselves<br>\n\nHillshade, in particular, determines surface lighting during the different phases of the day, hence it allows to read land morphology and Aspect and Slope are features about morphology, that's might be a possible reason behind the interaction of these features.","4caf233e":"### SVM","e749262e":"### Stratified k-fold cross-validation","f3584b46":"#### Aspect <a class=\"tocSkip\"\/>\nAspect is the orientation of the slope, measured clockwise in degrees from 0 to 360, where 0 is north-facing, 90 is east-facing, 180 is south facing and 270 is west-facing.","dd7de20c":"Nystroem polynomial kernel hyperparameters:\n1. `gamma`: kernel coefficient\n1. `n_components`: number of points used to construct the mapping in this approximation instead that all the training set.\n1. `degree`: degree of polynomial kernel\n","56e8353c":"In both cases, we can assume $\\text{p-value} < 0.01 $ which indicates substantial evidence against $H_0$, consequently, we reject it.","f508e872":"# Exploratory Data Analysis","29ea553b":"#### Testing best parameters <a class=\"tocSkip\"\/>","fbcb2d9a":"In particular, coefficients found through maximum likelihood fitting might help us understanding how much each feature affects the class logit. The idea is that positive coefficients for a given class and feature indicate that the class is more likely to happen with a predictor increase, while negative coefficients indicate the opposite.","9b9e5d5e":"## Model Evaluation","6617bc2a":"F1-score is the harmonic mean of precision and recall, where an F1 score reaches its best value at 1, whenever perfect precision and recall are achieved, and worst at 0.\nAlthough this metric is well defined for binary classification, it can also be used in a multiclass setup with little adaptations.\n<br>Several options among *macro*, *micro*, and *weighted* average are possible in our context. Unfortunately, the last two easily lead, like accuracy metrics, to provide higher scores to classifiers biased toward predicting majority classes, almost ignoring the remaining ones.\n<br>The *macro* average tends less to be biased toward the most populated classes because it is calculated by only averaging the F1-score calculated for each class, thus assigning to each cover type the same weight regardless of its number of observations.\nWe still mention accuracy, secondly, in models comparison for its easy interpretability.","b9e07c3b":"### Feature selection with RFE","24d4f543":"### Baseline model <a class=\"tocSkip\"\/>","28339293":"From the top 5 models, we may conclude that neither bootstrap nor class weights won't help too much in this setup.\n<br>Secondly, it must be mentioned that like bagging also random forest never overfits with the increasing of `n_estimators`, nevertheless a great number of trees impacts considerably training time.\n<br>In this case, 150 or more trees are enough.","bbcf38aa":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\">Introduction<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Features-explanation\" data-toc-modified-id=\"Features-explanation-1.1\">Features explanation<\/a><\/span><\/li><li><span><a href=\"#Reversing-one-hot-encoding-to-label-encoding\" data-toc-modified-id=\"Reversing-one-hot-encoding-to-label-encoding-1.2\">Reversing one-hot encoding to label encoding<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-2\">Exploratory Data Analysis<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Cover-Type-distribution\" data-toc-modified-id=\"Cover-Type-distribution-2.1\">Cover Type distribution<\/a><\/span><\/li><li><span><a href=\"#Numerical-Features\" data-toc-modified-id=\"Numerical-Features-2.2\">Numerical Features<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Correlation-Analysis\" data-toc-modified-id=\"Correlation-Analysis-2.2.1\">Correlation Analysis<\/a><\/span><\/li><li><span><a href=\"#Inputting-Hillshade_3pm-with-Random-Forest-Classifier\" data-toc-modified-id=\"Inputting-Hillshade_3pm-with-Random-Forest-Classifier-2.2.2\">Inputting Hillshade_3pm with Random Forest Classifier<\/a><\/span><\/li><li><span><a href=\"#Features-distribution\" data-toc-modified-id=\"Features-distribution-2.2.3\">Features distribution<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Categorical-Features\" data-toc-modified-id=\"Categorical-Features-2.3\">Categorical Features<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Features-distribution\" data-toc-modified-id=\"Features-distribution-2.3.1\">Features distribution<\/a><\/span><\/li><li><span><a href=\"#Chi-square-test-for-categorical-data\" data-toc-modified-id=\"Chi-square-test-for-categorical-data-2.3.2\">Chi-square test for categorical data<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Model-Selection\" data-toc-modified-id=\"Model-Selection-3\">Model Selection<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Model-Evaluation\" data-toc-modified-id=\"Model-Evaluation-3.1\">Model Evaluation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#F1-score\" data-toc-modified-id=\"F1-score-3.1.1\">F1-score<\/a><\/span><\/li><li><span><a href=\"#Normalized-Confusion-Matrix\" data-toc-modified-id=\"Normalized-Confusion-Matrix-3.1.2\">Normalized Confusion Matrix<\/a><\/span><\/li><li><span><a href=\"#Stratified-k-fold-cross-validation\" data-toc-modified-id=\"Stratified-k-fold-cross-validation-3.1.3\">Stratified k-fold cross-validation<\/a><\/span><\/li><li><span><a href=\"#Learning-curve\" data-toc-modified-id=\"Learning-curve-3.1.4\">Learning curve<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Logistic-Regression\" data-toc-modified-id=\"Logistic-Regression-3.2\">Logistic Regression<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Feature-selection-with-RFE\" data-toc-modified-id=\"Feature-selection-with-RFE-3.2.1\">Feature selection with RFE<\/a><\/span><\/li><li><span><a href=\"#Polynomial-logistic-regression\" data-toc-modified-id=\"Polynomial-logistic-regression-3.2.2\">Polynomial logistic regression<\/a><\/span><\/li><li><span><a href=\"#Resampling\" data-toc-modified-id=\"Resampling-3.2.3\">Resampling<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#KNN\" data-toc-modified-id=\"KNN-3.3\">KNN<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Dimensionality-reduction-with-PCA\" data-toc-modified-id=\"Dimensionality-reduction-with-PCA-3.3.1\">Dimensionality reduction with PCA<\/a><\/span><\/li><li><span><a href=\"#Hyperparameters-tuning\" data-toc-modified-id=\"Hyperparameters-tuning-3.3.2\">Hyperparameters tuning<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Decision-Tree-Classifier\" data-toc-modified-id=\"Decision-Tree-Classifier-3.4\">Decision Tree Classifier<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Feature-importance\" data-toc-modified-id=\"Feature-importance-3.4.1\">Feature importance<\/a><\/span><\/li><li><span><a href=\"#Hyperparameters-tuning\" data-toc-modified-id=\"Hyperparameters-tuning-3.4.2\">Hyperparameters tuning<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Bagging\" data-toc-modified-id=\"Bagging-3.5\">Bagging<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameters-tuning\" data-toc-modified-id=\"Hyperparameters-tuning-3.5.1\">Hyperparameters tuning<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Random-Forest\" data-toc-modified-id=\"Random-Forest-3.6\">Random Forest<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Feature-importance\" data-toc-modified-id=\"Feature-importance-3.6.1\">Feature importance<\/a><\/span><\/li><li><span><a href=\"#Hyperparameters-tuning\" data-toc-modified-id=\"Hyperparameters-tuning-3.6.2\">Hyperparameters tuning<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Boosting\" data-toc-modified-id=\"Boosting-3.7\">Boosting<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Hyperparameters-tuning\" data-toc-modified-id=\"Hyperparameters-tuning-3.7.1\">Hyperparameters tuning<\/a><\/span><\/li><li><span><a href=\"#Further-improvement-with-feature-engineering\" data-toc-modified-id=\"Further-improvement-with-feature-engineering-3.7.2\">Further improvement with feature engineering<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Support-Vector-Machines\" data-toc-modified-id=\"Support-Vector-Machines-3.8\">Support Vector Machines<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#SVC\" data-toc-modified-id=\"SVC-3.8.1\">SVC<\/a><\/span><\/li><li><span><a href=\"#SVM\" data-toc-modified-id=\"SVM-3.8.2\">SVM<\/a><\/span><\/li><li><span><a href=\"#Minibatch-Gradient-Descent-optimizer\" data-toc-modified-id=\"Minibatch-Gradient-Descent-optimizer-3.8.3\">Minibatch Gradient Descent optimizer<\/a><\/span><\/li><li><span><a href=\"#Hyperparameters-tuning\" data-toc-modified-id=\"Hyperparameters-tuning-3.8.4\">Hyperparameters tuning<\/a><\/span><\/li><li><span><a href=\"#Exact-SVM-on-undersampled-data\" data-toc-modified-id=\"Exact-SVM-on-undersampled-data-3.8.5\">Exact SVM on undersampled data<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#Conclusions\" data-toc-modified-id=\"Conclusions-4\">Conclusions<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Models-comparison\" data-toc-modified-id=\"Models-comparison-4.1\">Models comparison<\/a><\/span><\/li><li><span><a href=\"#Further-improvement\" data-toc-modified-id=\"Further-improvement-4.2\">Further improvement<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#References\" data-toc-modified-id=\"References-5\">References<\/a><\/span><\/li><\/ul><\/div>","a51be2ae":"### Chi-square test for categorical data","1c3d5515":"By looking at the previous two charts, we could claim that it is very likely to be a dependency between each categorical features, Wilderness_Area or Soil_Type, and the class label Cover_Type.\n<br>Our assumption may be verified by testing *the null hypothesis of independence* $H_0$, i.e. no relationship exists between them, whereas the alternative hypothesis is that the variables are related.\nThe chi-square test involves computing the chi-square statistic and then comparing the obtained value with the critical one in $\\chi^2$ distribution with $DF = (R - 1)(C - 1)$ and a chosen $\\alpha$ ($\\alpha = 0.05$ provides us sufficient evidence in rejecting the hypothesis).","456eaac3":"Now considering dataset dimensions, *Principal Component Analysis* will be beneficial to mitigate the curse of dimensionality and to improve model performance in computing prediction. This because KNN during the predictions phase will compute euclidean or l1 distance among $\\mathscr{x}_0$ and all the points in the training set, hence reducing the number of features will result in lower distances and fewer dimensions involved in computations.","b0a604f6":"By adding third component in this plot we can explain about 58% of variance in data.","22596a51":"$H_0$ can be rejected in this case, too.\nThus, an *evident dependency* exists not only between each of the two categorical features and class labels Cover_Type, but also between them.","b61145d8":"### Simple model baseline  <a class=\"tocSkip\"\/>","74046684":"##### Outliers","7be6fcba":"<img src=\"images\/1_6UFpLFl59O9e3e38ffTXJQ.png\" alt=\"Drawing\" style=\"width: 400px;\"\/>\n\nSource: https:\/\/www.researchgate.net\/publication\/287601878\/figure\/fig1\/AS:316826589384744@1452548753581\/The-schematic-of-NRSBoundary-SMOTE-algorithm.png","b507ac54":"Since $K=4$ leads to flexible boundaries, the model trained on a subset of data easily tends to *overfit data*.\n<br>As we can see, by adding more training observation, the gap between training and validation scores shrinks more and more as a result of variance error reduction.\n<br>Unfortunately, we are not so far from the score obtained with the baseline model, though PCA makes model predictions faster.","6c720e2a":"We can recognize an elbow shape in correspondence of the 8th component with a cumulative PVE of 87.58%","3aacf808":"The model can't overfit even with increasing of `max_depth`. It might be due to the large dataset proportions and `min_samples_split=3` setting.","36a539a9":"#### Elevation <a class=\"tocSkip\"\/>","3d3a1c45":"Let's see how last obtained classifier behaves with the increase of training data","c294b1b0":"In this dataset, distance features are recorded in meters, including Vertical_Distance_To_Hydrology, which also presents *negative values* for specific area measurements situated under the nearest water surface.","bb741055":"Similarly to bagging models in random forest classifier, we build B trees on bootstrapped data. Still, this model usually provides a significant improvement by trying *to decorrelate* trees predictions by choosing a random subset of possible features when evaluating a potential split, usually $m\\approx{\\sqrt{p}}$ with $p$ total number of predictor (12 in our case).<br>\nThrough this method is possible to make a robust classifier even in the cases where a strong predictor is present in dataset (e.g Elevation for some classes).<br>\nBy choosing local best predictors we are trying to exploit different patterns, avoiding always splitting data by the same most predictive variables.","b13d2a8b":"In training a decision tree, we got a variety of hyperparameters to optimize:\n1. `criterion`:  evaluation split function, e.g. Gini or cross-entropy\n1. `min_samples_split`: minimum number of samples required to split a node\n1. `max_depth`: the maximum depth which model is allowed to reach\n1. `splitter`: strategy used to split feature interval of values, 'best' and 'random' splits are possible options. In the first case at each step best split is chosen among features, evaluating for each one the most purity improving branch. Otherwise, with the second option, a random split is performed for each feature, then the best among them is chosen.\n1. `max_features`: number of features to take into account in performing a split, e.g. all n_features, sqrt(n_features), etc.\n\n<br>We will also try two different `k_neighbors` values for SMOTE oversampling.\nIn addition, we will search over two different `k_neighbors` values for SMOTE oversampling.\nWith this dataset, cross-entropy has proven to work slightly better than Gini index.\nThe same goes for `min_samples_split = 3`, \n`splitter = best` and `max_features = n_features`.\nBy fixing these parameters, we can save a lot of computational time, consequently grid searching only over `max_depth`.\n<br>On the next plot, we will see in detail how the model behaves as trees depth upper bound changes.","b1bfdb1e":"## KNN","b0bdd3f8":"## Decision Tree Classifier","9d07a073":"Polynomial logistic regression will also evaluate polynomial numerical features dependencies. Here we will fit the classifier on degree-2 polynomial, taking into account squares terms and products between the variables.","7b5dc753":"#### Learning curve <a class=\"tocSkip\"\/>","8ec81683":"EDA makes it possible to visualize with the naked eye data distribution and interactions both among features themselves and among features and the label. We can achieve it through the visual representation of descriptive statistics such as maximum and minimum, median, quartiles for quantitative predictors. In contrast, histograms and pie charts are better suited for qualitative ones.","ca434498":"By mean of Bootstrap aggregation, we have an overall improvement f1-score, also for class 5 wherein we can notice, in the normalized confusion matrix, that despite its recall is decreased, precision raised from 0.839 to 0.942.","878be64d":"#### Learning curve <a class=\"tocSkip\"\/>","e2a0a181":"\"Forest Cover Type\" dataset contains tree observations from four areas of the Roosevelt National Forest in Colorado. All measurements are cartographic variables (no remote sensing) from 30 meter x 30 meter sections of forest and amount to over half a million.","ef6c9761":"Elevation feature proves to be by far the most discriminative predictor because, as seen before in EDA, each class feature distribution profoundly differs from the others. \n<br>The same goes for Horizontal-Distance_To_Fire_Points and Horizontal_Distance_To_Roadways, though in less magnitude.\n<br>Finally, Soil_Type discriminating power, strictly depends on individual soil value.","d99704ab":"#### Soil_Type Wilderness_Area Chi2 test <a class=\"tocSkip\"\/>","92fb4002":"### Learning curve","d7ed78f3":"### Baseline model <a class=\"tocSkip\"\/>","6ba3a96a":"## Further improvement","45cd4a8f":"The graph curves show that with $K < 4$ we have a higher variance-error contribution because of the very flexible class boundaries. On the other side, for $K > 4$, we risk to overgeneralize data and increase the bias error.","66904be2":"## Boosting","67db1a0f":"#### Wilderness Area <a class=\"tocSkip\"\/>","c19a05dc":"##### Kernel decision boundaries visualization <a class=\"tocSkip\"\/>","646c8546":"Hillshade_9am and Hillshade_Noon zeros cardinalities are negligible; as a result, we will only handle Hillshade_3pm supposed missing values, by predicting all the Hillshade_3pm zero values. \nActual zero values appeared indistinguishable from the others, thus, they will be overwritten together with missing values. \nBesides, if a nearby value is predicted by the classifier rather than 0, it should not be such a big deal for Cover_Type prediction.\nRandom Forest Classifier details will be covered later in a later section.","d04696e8":"Similarly to the previous classifier, the models have a large number of parameters, then we are forced to random search in order to explore a significant portion of the hyperparameter space in a feasible amount of time. ","65aa8703":"#### Hotmap <a class=\"tocSkip\"\/>","2751d06c":"### Hyperparameters tuning ","596e3b63":"# [Forest Cover Type Multi-class Classification](https:\/\/www.kaggle.com\/uciml\/forest-cover-type-dataset)<a class=\"tocSkip\"\/>","bfa32aaa":"PCA tool may also be used to visualize up to 3 PC dimensions by plotting data after projection into the new subspace.","d8997179":"## Bagging","d6fd3b3a":"It is the simplest, but also the fastest method which undersamples the majority classes by randomly picking without replacement.\n<br>Next, we will train and validate the model from the previous section on a random undersampled version of the dataset.","332c7076":"This plot shows that Spruce\/Fir, Lodgepole Pine and Krummholz are located in the most distant areas from roadways and have a more generous range of values in their distribution compared to classes 3, 4, 5 and 6.","471e5275":"<br>In order to tune $C$ and $\\gamma$ hyperparameters faster we manaully search them on svm by means of cross-validation on top of the undersampled balanced set (2178 samples for each class).\n<br>Consequently, in test phase we train a svm classifier with chosen parameters on about 80k training set and evaluate it on the test split.","7ec2d1a4":"A *confusion matrix* is used to asses overall model performances evaluating model inferences class by class, by showing how many instances have been correctly classified or misclassified with respect to the actual class.\nThen for a fixed row $i$, we can evaluate on each column $j$ how many observations were assigned to $j$-th class, i.e. the $_{ij}$ indexed cell represents the number of actual class $i$ occurrences classified as $j$.\nWe now introduce a much intuitive variation for imbalanced data normalizing by row. Ratios, as opposed to absolute numbers, are immediately comparable also for classes with different cardinality.\nInterestingly it turns out that *normalized confusion matrix* main diagonal values correspond to recall for each class.","a3d2667d":"We have a fixed set of values for all forest species because aspect(azimuth) measures are expressed in degree. Despite this, we can identify different medians and interquartile ranges for each class.","124f5dcd":"The chi-square statistic is computed on the squared difference between observed and expected values in each cell of the contingency table.","65524529":"## Models comparison","5394b7c6":"## Categorical Features","556f8a29":"Exploring the vast space of possible parameters by a grid search would be too expensive in computational time. A good compromise could be a *random search* of 20 iterations, in which we just build 20 different models by random choosing from a preselected subset of parameters.","66df86dc":"### Resampling","49edac7a":"First of all, we reverse one-hot encoding of categorical variables into label encoding to obtain a database better suited for tree-based models fitting and qualitative features plotting.","3d2f4657":"### Learning curve <a class=\"tocSkip\"\/>","fc91f6b6":"### F1-score","64702c2d":"### Dimensionality reduction with PCA","b4b578f4":"Maximal Margin Classifier is a binary classifier based on a *separating hyperplane* that divides p-dimensional space in two, one for each class.\n\\begin{equation}\n\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p = 0  \n\\end{equation}\n<br>\nSo in the training phase \nwe will calculate the best orientation for the hyperplane, while \nin test phase given a new observation $\\mathscr{x_0}$ we just assign it to first or second class depending on which side of the hyperplane is located.\n\\begin{equation}\n\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p > 0 \\\\ or \n\\\\  \n\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p < 0  \n\\end{equation}\n\nIn numerous cases, a perfect separating hyperplane does not exist, hence the need to introduce a *soft margin* in SVC, which is a generalization of Maximal Margin Classifier. In Support Vector Classifier an observation is allowed to be located on the incorrect side of the margin or hyperplane, thus providing a certain degree of tolerance for misclassification of some instances. The mathematical formulation is: \n<br><br>\n\\begin{equation}\n\\begin{aligned}\n& \\min\\limits {\\beta, \\beta_0} \\frac{1}{2}\\lVert{\\beta}\\rVert^2 + C \\sum\\limits_{i=1}^N \\xi_i\\\\\n& \\text{subject to}\\enspace \\xi_i > 0, y_i(x_i^T \\beta + \\beta_0) > 1 - \\xi_i, \\forall i\n\\end{aligned}\n\\end{equation}\n\n\n<br>\nwhere $\\xi_1, ..., \\xi_n$ are the *slack variables* that \nmodel the possibility for an observation to be on the other side \nof margin or hyperplane, in fact we have $\\xi_i = 0$ if i-th instance of data is correctly classified while $\\xi_i > 0$ \nif misclassified. In this way, $C$ plays a critical role as a regularization term in deciding how much data points could be misclassified to ensure a bigger margin. In particular, $C=\\infty$ correspond to *Maximal Margin Classifier* problem, while lower $C$ will introduce more bias error in some cases allowing better generalization from training data.\n$C$ parameter will be tuned empirically from data with cross-validation.\n<br>Multiclass SVM or multiclass SVC are an extension from the basic one in which two possible approaches could be used:\n1. *one-versus-one classifier*: $\\binom{K}{2}$ SVMs are trained, one for each pair of classes and final classification is performed by assign the most frequent predicted class to new test observation $\\mathscr{x_0}$\n1. *one-versus-rest classifier*: K classifiers are fitted and each of these will separate k-th class from the K-1 rest. The final classification is done by choosing the class result the most distant from the margin.\n\nThe second approach is chosen in this notebook because of the large amount of time required for training.","d1447026":"Kernel approximation method is not enough to allow the model to scale efficiently. \n<br>With Stochastic Gradient Descent estimator instead, we follow a different optimization strategy: in particular, while minimizing the objective function, we update SVC weights parameter based on gradients of a small subset of the training set called batch, rather than on the whole dataset as previously done in SVC. This process is iteratively repeated until it either converge or reaches max iterations stop condition.","8b3d3700":"### Baseline model <a class=\"tocSkip\"\/>","6f46be19":"### Hyperparameters tuning","339658b4":"PCA is a dimensionality reduction method. Its key idea is to find a *lower-dimension representation* that explains as much as possible the variability in data.\n<br>Each dimension, also called components, found by PCA is a linear combination of starting *p* features and it is orthogonal with respect to the other m found.\nTherefore each principal components has the form:\n\\begin{equation}\nZ_m = \\phi_{1m}X_1 + \\phi_{2m}X_2 + ... + \\phi_{pm}X_p\n\\end{equation}\n\nThe loadings constitute the loading vector $\\phi_m = (\\phi_{1m} \\phi_{2m} ... \\phi_{pm})^T$ which define a direction where the data vary the most.\n<br> They are nomalized: &nbsp;&nbsp;&nbsp;&nbsp;$\\sum\\limits_{j=1}^p \\phi_{jm}^2 = 1$.\n\n\n##### PCA transformation steps: <a class=\"tocSkip\"\/>\n1. Standardize the data X\n2. Obtain the eigenvectors and eigenvalues either:\n    1. by factorizing the covariance matrix: $X_{std}^TX_{std} = W\\hat{\\Sigma}^2W^T$\n    1. by performing Singular Value Decomposition on $X_{std} = U\\Sigma{}W^T$\n3. Sort eigenvalues in descending order and choose the first k, with k equal to the selected number of components \n4. Construct the projection matrix $W_k$ by selecting from $W$ the eigenvectors corresponding to the chosen $k$ greater eigenvalues.\n5. Transform the original dataset $X_{std}$ via multiplying it by $W_k$ to obtain a k-dimensional feature space","91e82bc2":"### Exact SVM on undersampled data","dad172d9":"#### Feature Importance <a class=\"tocSkip\"\/>\n<br>Some of the newly created features have reached considerable feature importance and are better suited for splitting data. They are:\n- 'diff_dist_fire_hydro'\n- 'sum_dist_road_fire'\n- 'sum_dist_road_hydro' \n- 'diff_fist_road_fire',\n- 'sum_dist_fire_hydro'\n","ee984ca5":"Interestingly we have better accuracy mostly with a smaller dataset as a result of reducing variance error.\nOverall we have a similar trend to the decision tree learning curve, though employing ensembling, we achieve an overall better accuracy score.","2a174b48":"Then we just fit the previous best performing Xgboost model, and test it again.","c33fc7bc":"We can enlarge feature space by also considering *interactions between features*, thus introducing linear combinations between them to the original dataset. This is particularly effective for tree-based models, which can only perform split by considering one feature at time and cannot extract such dependencies.\nIn particular, we will provide to the classifier the following features: \n- mean_hill : mean of hillshades\n- distance_hydro : euclidean distance to hydrology\n- sum_dist_road_fire : sum between roadways and fire points distances\n- diff_dist_road_fire : absolute distance between roadways and fire points distances\n- sum_dist_road_hydro : sum between roadways and hydrology distances \n- diff_dist_road_hydro : absolute distance between roadways and hydrology distances \n- sum_dist_fire_hydro : sum between fire points and hydrology distances\n- diff_dist_fire_hydro : absolute distance between fire points and hydrology distances","bbb678fc":"Interestingly Horizontal_Distance_To_Fire_Points and Horizontal_Distance_To_Roadways have a similar range of values for every class.","de390b10":"Boosting uses a separate ensemble technique, in which trees are aggregated sequentially. Each tree is now fitted to *the residual* originated from the model at the previous step, rather than to class label $Y$. The model will benefit from this in areas where the residual is bigger.\n<br>In each training iteration, a new tree is grown and added to the model multiplied by a shrinkage parameter $\\lambda$ that controls the learning rate, then the model is updated and the new residual is computed.\n<br>The number of trees needed is strictly dependent on data and $\\lambda$.\nTherefore the resulting model will be:\n\\begin{equation}\n\\sum\\limits_{b=1}^{B} \\lambda \\hat{f}^b(\\mathscr{x})\n\\end{equation}\n<br>\nwhere $B$ is the total number of trees composing the model and $\\hat{f}^b(\\mathscr{x})$ the contribution of the b-th tree to the model.","ec68bf43":"In this section we optimize through random search SGDC without kernels and later we look for improvement via  kernel approximation with the purpose of selecting the best hyperparameters for this model.","877be42a":"### Hyperparameters tuning ","f99295c6":"Hyperparameters to optimize:\n1. `subsample`: subsample: the fraction of the training samples(randomly selected) that will be used to train each tree\n1. `reg_lambda`: L2 regularization term on weights\n1. `n_estimators`: number of trees which constitute the model, an high value could lead to overfitting\n1. `min_child_weight`: regularization term. If the tree partition step results in a leaf node with the sum of instance weights less than this parameter, then the building process will give up further partitioning\n1. `max_depth`: the maximum depth which each tree in the model can reach\n1. `learning_rate`: the shrinkage parameter $\\lambda$\n1. `gamma`: minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is the more conservative the algorithm will be\n1. `colsample_bytree`: the fraction of features (randomly selected) that will be used to train each tree\n1. `colsample_bylevel`: subsample at each level of depth, the features are selected from the ones chosen for the current tree","50932e4b":"#### RBF Classifier <a class=\"tocSkip\"\/>","2ddbf3c8":"## Reversing one-hot encoding to label encoding","3a496dbb":"In this section, we briefly present the information regarding the meaning of each feature and its admissible values.","9f82ea19":"In some cases, we could have a data distribution in which linear boundaries cannot effectively separate classes. \n<br>In these scenarios, it is still possible to train a model with extra variables used to describe *non-linear dependencies*.<br>\nActually, with the additional variables we are still finding a linear decision boundary, but in a *higher-dimensional space* that projected on the starting one will lead to a non-linear hypersurface.","12640c38":"#### Nystroem RBF random search <a class=\"tocSkip\"\/>","1ebadfa6":"The models which achieve the highest f1-score on test split are all the ensembling models and KNN; XGboost was slightly better than the rest, nevertheless making explicit feature interaction allowed us to reach higher f1-score and accuracy in predicting the forest cover.","5b873889":"Eyeballing hillshades scatterplots, we can notice an *unusual number of zero values* such that many of them could be missing values.","5c20afea":"It is clear from the results that adding more regularization to the model does not improve our score metric. Probably we are in the presence of a high bias error, thus our classifier is missing relevant relationships between features and target outputs. So next, we will try a polynomial regression model with a higher learning capacity to overcome such a problem.","44094ee8":"In finding good performing classification models we will reproduce the following workflow:\n- dataset splitting in *training*, and *test* set\n\nThen for each statistical model, we perform:\n- *hyperparameter optimization* (either manual, grid-search or random-search optimization) via *stratified 5-fold-cross validation* on the training set.\n- train model on whole training set\n- test model on unseen test data.\n\nLastly, we will:\n- compare results on testing data\n","0f3b6feb":"#### Distance to Hydrology <a class=\"tocSkip\"\/>","6a7724a7":"In this setup, Elevation is still the most predictive feature, even though it has lost 5.3%. Soil_Type and many other features, on the contrary, have gained some further importance. This happens because we are not always considering Elevation while splitting data, but only when it falls in the $m$ chosen predictors.","78920920":"*Data resampling methods* are an alternative approach to handle class imbalance by rebalancing them. \nThere are two different main approaches: *undersampling* and *oversampling*.\n\n<img src=\"images\/resampling.png\" alt=\"Drawing\" style=\"width: 800px;\"\/>\nSource: https:\/\/www.kaggle.com\/rafjaa\/resampling-strategies-for-imbalanced-datasets\n\n*Undersampling* means resampling data by discarding instances in the dataset from majority classes. It may lead to the loss of relevant information for our task.\nOn the opposite *oversampling* techniques generates new synthetic data from minority classes rebalancing the dataset.","64f50dd9":"The dataset comprehends 54 cartographic variables plus the class label, 10 of which are quantitative features while the remaining 44 correspond to 2 qualitative variables one-hot encoded.\nIn this work our goal is to classify the *cover type* based on predictor variables of each observation (30 x 30 meter cell).\n<br>Since we have 7 possible Cover Type in given areas, we are approaching a *multiclass classification problem*","afb6f58e":"SMOTE technique involves determining, for each sample of the minority classes, the k-nearest point, measuring each vector distance between every pair of sample-neighbors and multiplying it by a random number between 0 and 1 obtaining a new sample as a result. $k$ is a hyperparameter that we found works pretty well if $k=10$.\n<br>At this point, we try to tackle class imbalance with SMOTE oversampling rather than class weights, still fitting a logistic regression model on degree-2 polynomial features.","5701b427":"### Baseline model <a class=\"tocSkip\"\/>","14aa7880":"#### Learning Curve <a class=\"tocSkip\"\/>","342adaa6":"#### Soil Type <a class=\"tocSkip\"\/>","528722e9":"### Hyperparameters tuning ","a75cd2ef":"### Minibatch Gradient Descent optimizer","88261912":"# References ","6735cc2d":"This plot highlights an underfitting problem. We have a high error, indeed, in both training and validation.\nAs a result, adding more data would not benefit the model, which suffers from a high bias error.","3cb88afc":"### Hyperparameters tuning","395b9113":"Hyperparameters\n1. `n_estimators` : number of trees\n1. `min_samples_split` : the minimum number of samples required in splitting a node\n1. `min_samples_leaf` : in evaluating a new possible split, it guarantees a minimum number of samples in a node in order to be considered a leaf\n1. `max_features` : number of features to take into account in performing a split, `auto` corresponds to `sqrt(n_features)`\n1. `max_depth` : the maximum depth which each tree in the model can reach\n1. `class_weight` : with `balanced` we try to tackle class imbalance by applying high weight penalties to misclassification of more infrequent classes. \n1. `bootstrap` : whether to sample with replacement or not from original the dataset","606dbeb6":"The remaining hyperparameters to tune are:\n1. `n_neighbors` : represent the $K$ nearest neighbors evaluated during prediction\n2. `weights` : if `uniform` all neighbors have equal weight, while `distance` gives different importance to each voted based on the inverse of distance, so closer points became more important\n3. `p`: `1` for using l1 distance and `2` for l2 distance","241b4405":"\\begin{equation}\n\\chi^2 = \\sum\\limits_{i=1, j=1}^{i=R, j=C} \\frac{( O_{ij} - E_{ij} )^2}{ E_{ij}}\n\\end{equation}","cc198289":"#### Slope <a class=\"tocSkip\"\/>\nThe slope is the steepness or the degree of incline of a surface.","2f3b7a1c":"We will consider *PVE* in deciding how many components to keep to fit the model on top of them, in replacement to our original features. Since we are using PCA for a classification problem, a common way of choosing $k$ is by determining it in an end-to-end fashion via cross-validation.\n<br>A valid alternative is to analyze cumulative PVE and keep principal components until an *elbow shape* occurs, usually at about 85% of the total variance.","659e9f8a":"## Cover Type distribution","7de77605":"## Logistic Regression","f00276d3":"Nystroem and RBFSampler radial basis function kernel approximation hyperparameters:\n1. `gamma`: kernel coefficient\n1. `n_components`: number of points used to construct the mapping in this approximation instead that all the training set.\n","5e37bc0e":"As a baseline, we will use an ensemble model of 10 decision trees(without any max length constraint), each one trained on top of a bootstrapped dataset, i.e. sampled with replacement from the original one.","80877eec":"**Cover Type number of observation**","b9fe941e":"This plot shows that Elevation is an excellent predictor because each of these tree species has a separate elevation interquartile range.\nSome classes have even non-overlapping minimum-maximum range values like:\n- Krummholz (class 7) and Ponderosa Pine (class 3) \n- Krummholz (class 7) and Douglas-fir (class 6)\n- Cottonwood\/Willow (class 4) and Aspen (class 5)\n- Cottonwood\/Willow (class 4) and Spruce\/Fir (class 1)","c32d9ad4":"#### Hillshades <a class=\"tocSkip\"\/>\nHillshading is a technique used to visualize terrain as shaded relief, illuminating it with a hypothetical light source.\nAs a result, the orientation of the surface to the light source determines the illumination value for that area.\nIn our case, we have three hillshade values depending on the position of the sun, the light source, during different times of the day, in particular, 9 am, 12 am, and 3 pm.","d12c66bc":"1. Boslaugh, Sarah: Statistics in a Nutshell. O'Reilly (2012)\n1. James, G., Witten, D., Hastie, T., Tibshirani, R.: An Introduction to Statistical Learning. Springer (2013)\n1. https:\/\/www.e-education.psu.edu\/geog480\/node\/490\n1. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html\n1. https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.over_sampling.SMOTE.html\n1. https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/generated\/imblearn.under_sampling.RandomUnderSampler.html\n1. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\n1. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html\n1. http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingClassifier.html\n1. http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n1. https:\/\/medium.com\/@srnghn\/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3\n1. https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\n1. http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html\n1. http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html\n1. https:\/\/scikit-learn.org\/stable\/modules\/kernel_approximation.html","d32ff4e9":"*Recursive Feature Elimination* is a technique aimed at decreasing the number of features to build simpler models. At each step, a logistic regression model is fitted, then all but the least predictive coefficients are kept, while this one is pruned. This process is repeated until the target number of features is reached.<br>\nWe found that reducing the number of features to 40 could be a good compromise in terms of model complexity and f1-score with respect to the baseline model.","9c452ad7":"#### Distance to Roadways <a class=\"tocSkip\"\/>","8383777c":"We start with a baseline model with `penalty='l2'`, `C=1` and `class weights='balanced'`.\n1. the reason behind the choice of saga solver is the higher speed in fitting coefficients for large datasets and the availability of multiclass logistic regression implementation.\n2. C, the inverse of regularization strength(l2 in our case) will not make any significant difference because, as we will see, the model does not have enough capacity to overfit data. \n3. balanced class weights will contrast class imbalance by applying a weight penalty for misclassification inversely proportional to class frequencies in data.","2ab4664b":"SGDC hyperparameters\n1. `alpha`: It is the constant weight which multiplies the regularization term. It has a behavior similar to C\n2. `loss`: *hinge* is regular SVC loss while 'squared_hinge' (or L2-SVM) penalizes violated margin more strongly, so quadratically instead of linearly\n3. `penalty`: penalty parameter stands for regularization type, so *l1*, *l2* or *elasticnet*, which corresponds to using both terms\n4. `average`: if true regular SGD is done. When optimization is finished, the averaged weights among the step are used instead of the last found.\n5. `class_weight`: with 'balanced' we try to tackle class \nimbalance by applying high weight penalties to misclassification of more infrequent classes.","1ec9a7b5":"Similarly to the previous predictor, slope measure is expressed in degree, although we now have a narrow range of values between 0 and 66 degrees.","74b45660":"We start the hyperparameter optimization form a baseline KNN classifier with $K=6$.\n<br>Notice that we decide empirically to not preprocess data because no matter which scaler we use, it worsens the classifier performance rather than helping the model not to be biased towards features with a higher magnitude.<br>","b121ffd1":"KNN Classifier is a method that attempts to estimate the conditional distribution of label $Y$ based on $X$ features. Provided $K$ and a new observation $\\mathscr{x_0}$ the model search for a neighborhood $\\mathcal{N_0}$ \nof $K$ nearest observations in the training data that are closest to $\\mathscr{x_0}$. \n\\begin{equation}\nPr(Y = j|X = \\mathscr{x_0}) = \\frac{1}{K} \\sum\\limits_{i \\in \\mathcal{N_0}} I(y_i = j)\n\\end{equation}\n\nWith the growth of $K$, we move from *flexible boundaries* to almost linear ones. Low $K$s will result in high-variance low-bias classifiers, whereas for big $K$ it is the opposite. A reliable method to establish a good $K$ is empirically choosing it from data with cross-validation.","4aec6dc3":"### Feature importance","8bd71358":"The division is the same for both different encoded datasets (one-hot encoded and label encoded) motivated by the need to obtain comparable test metrics on the same test data, which in this case amount to 20% of the dataset.\n<br>Before starting to fit models on one-hot encoded data, we perform a little adjustment by *dummy encoding* qualitative features,  removing a column from both Wilderness_Area and Soil_Type. The necessity of uncorrelated features motivates this operation.","9bc9139a":"The learning curve purpose is to graphically show how the error or similarly the accuracy in our case behave with the increase of training data. It helps in determining whether the model could benefit from adding with more data as a sort of regularization for models with high variance errors.\nWe will see that often the gap between validation and training curve will become smaller as a consequence of adding more training data.\n<br>Please note that we always expect higher accuracy on training data with respect to validation data, for the reason that we indirectly seek to maximize it.\nDespite this, if we have a wider gap between the two curves when training on a smaller subset of data, this probably means that the model has enough learning capacity to find patterns in the training data that are due to random chance (noise). This phenomenon is called *overfitting*.","3a06ef39":"### Inputting Hillshade_3pm with Random Forest Classifier","60689dc0":"Let's take a brief overview of the class label's data distribution.","5b366ed0":"Before starting let's check that there are no nan value predictors.","c2fdaded":"#### How k affects the classifier performances? <a class=\"tocSkip\"\/>","e8efbd99":"We obtained worse results compared to balanced class weights. This because by naively undersampling to minority class cardinality, we are discarding too much data, and consequently, we risk to miss some valuable patterns from training data. ","58d1c0cc":"Wilderness areas number of observation: ","69acbc7d":"Our models have captured many dependencies between features and labels, thus we probably have a low intrinsic irreducible error in forest data gathered, considering that we have reached about 0.95 f1-score and 0.98 accuracy.\nMoreover, dataset size helps us to avoid high variance errors arising from overfitting except for models with a lot of learning capacity, such as ensemble learning models.","34d27659":"Let's examine which are the top five performing classifiers.","48e8d5a2":"### Correlation Analysis","035c6609":"Random Forest Classifier with default parameters is composed of 100 random trees grown with no depth constraint on bootstrapped data.","957d3a00":"### Features distribution","a0028fc8":"### Feature importance","57a85706":"After validating different models, it turns out that *seven principal components* allow us to obtain almost the same accuracy of the original model with 52 features, though requiring less computational time during predictions.\nPlus, PCA on *scaled feature* will always lead to worse results compared to unscaled ones. For this reason, exceptionally in this pipeline, we will apply dimensionality reduction directly on *raw features*, then we will fit a KNN classifier on top of the first 7 PCs.\nThis means that features with a wider range of values as elevation and different distance features play a decisive role in determining the PCs and consequently the class labels.","171307b3":"#### RBF Sampler random search <a class=\"tocSkip\"\/>","94b48f1e":"#### Testing the best model <a class=\"tocSkip\"\/>","9e8865a5":"## Features explanation","20401469":"These pie charts show for each Soil Type the corresponding distribution of forest cover type. In other words, we have different percentages of the composition of cover types, which differ according to the soil.\nThese charts show that almost all soils include more than two possible cover type, which indicates this feature may be beneficial for classification, nevertheless in combination with other predictors.","9a1c76cc":"Top five classifiers performances in the validation phase:","b712cc11":"<img src=\"images\/stochastic gradient descent.png\"\/>\nSource: https:\/\/www.i2tutorials.com\/deep-learning-interview-questions-and-answers\/explain-brief-about-mini-batch-gradient-descent\/","d27085b3":"Trying to resolve the problems derived from unbalanced data with SMOTE, we fall in the opposite problem: oversampled minority classes suffer from low precision and high recall. Further improvement can be performed by tuning the oversampling ratio between the majority and the resampled minority classes.","b0fb97f1":"#### Random undersamping <a class=\"tocSkip\"\/>","34c74392":"The same goes for Soil_Type feature which is encoded as 40 one-hot encoded binary columns (0 = absence or 1 = presence) and each of these represents soil type designation.\n<br>All the possible options are:\n\n1. Cathedral family - Rock outcrop complex, extremely stony\n1. Vanet - Ratake families complex, very stony\n1. Haploborolis - Rock outcrop complex, rubbly\n1. Ratake family - Rock outcrop complex, rubbly\n1. Vanet family - Rock outcrop complex complex, rubbly\n1. Vanet - Wetmore families - Rock outcrop complex, stony\n1. Gothic family\n1. Supervisor - Limber families complex\n1. Troutville family, very stony\n1. Bullwark - Catamount families - Rock outcrop complex, rubbly\n1. Bullwark - Catamount families - Rock land complex, rubbly. \n1. Legault family - Rock land complex, stony\n1. Catamount family - Rock land - Bullwark family complex, rubbly\n1. Pachic Argiborolis - Aquolis complex\n1. unspecified in the USFS Soil and ELU Survey\n1. Cryaquolis - Cryoborolis complex\n1. Gateview family - Cryaquolis complex\n1. Rogert family, very stony\n1. Typic Cryaquolis - Borohemists complex\n1. Typic Cryaquepts - Typic Cryaquolls complex\n1. Typic Cryaquolls - Leighcan family, till substratum complex\n1. Leighcan family, till substratum, extremely bouldery\n1. Leighcan family, till substratum - Typic Cryaquolls complex\n1. Leighcan family, extremely stony\n1. Leighcan family, warm, extremely stony\n1. Granile - Catamount families complex, very stony\n1. Leighcan family, warm - Rock outcrop complex, extremely stony\n1. Leighcan family - Rock outcrop complex, extremely stony\n1. Como - Legault families complex, extremely stony\n1. Como family - Rock land - Legault family complex, extremely stony\n1. Leighcan - Catamount families complex, extremely stony\n1. Catamount family - Rock outcrop - Leighcan family complex, extremely stony\n1. Leighcan - Catamount families - Rock outcrop complex, extremely stony\n1. Cryorthents - Rock land complex, extremely stony\n1. Cryumbrepts - Rock outcrop - Cryaquepts complex\n1. Bross family - Rock land - Cryumbrepts complex, extremely stony\n1. Rock outcrop - Cryumbrepts - Cryorthents complex, extremely stony\n1. Leighcan - Moran families - Cryaquolls complex, extremely stony\n1. Moran family - Cryorthents - Leighcan family complex, extremely stony\n1. Moran family - Cryorthents - Rock land complex, extremely stony","f049cb06":"#### Testing best parameters <a class=\"tocSkip\"\/>","1a6aae77":"Wilderness_Area feature is one-hot encoded to 4 binary columns (0 = absence or 1 = presence), each of these corresponds to a wilderness area designation.\n<br>Areas are mapped to value in the following way:\n1. Rawah Wilderness Area\n2. Neota Wilderness Area\n3. Comanche Peak Wilderness Area\n4. Cache la Poudre Wilderness Area","02f55591":"- Elevation: Elevation in meters.\n- Aspect: Aspect in degrees azimuth.\n- Slope: Slope in degrees.\n- Horizontal_Distance_To_Hydrology: Horizontal distance in meters to nearest surface water features.\n- Vertical_Distance_To_Hydrology: Vertical distance in meters to nearest surface water features.\n- Horizontal_Distance_To_Roadways: Horizontal distance in meters to the nearest roadway.\n- Hillshade_9am: hillshade index at 9am, summer solstice. Value out of 255.\n- Hillshade_Noon: hillshade index at noon, summer solstice. Value out of 255.\n- Hillshade_3pm: shade index at 3pm, summer solstice. Value out of 255.\n- Horizontal_Distance_To_Fire_Point*: horizontal distance in meters to nearest wildfire ignition points.\n- Wilderness_Area#: wilderness area designation.\n- Soil_Type#: soil type designation.","0e5ee52b":"#### Grid search <a class=\"tocSkip\"\/>","7fc7d60d":"It can be shown that SVC can be represented as\n\\begin{equation}\n    f(x) = \\beta_0 + \\sum\\limits_{i=1}^n \\alpha_i \\langle x,x_i\\rangle\n\\end{equation}\nwhere $\\alpha_i$ and $\\beta_0$ are parameters that need to be estimated by means of $\\binom{n}{2}$ inner products between training observations.\n<br>It is shown that is possible to replace inner products with *kernel* K, which is a measure of the similarity between two observations and it turns out that $\\alpha_i \\neq 0$ in the solution only for support vectors, hence we will obtain:\n\\begin{equation}\n    f(x) = \\beta_0 + \\sum\\limits_{i \\in S}^n \\alpha_i K(x,x_i)\n\\end{equation}\n\n","8cd57e77":"Random Forest overall performs slightly worse than bagging method in  terms of accuracy and averaged f1-score in both validation and test phase. \n<br>It is very likely that this is due to random found hyperparameters. Searching deeply for better hyperparameters would be very time-consuming, especially for models with the same or  higher number of trees."}}