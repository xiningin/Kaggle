{"cell_type":{"03a46502":"code","4e2b4646":"code","96dab199":"code","ce24beff":"code","43d43e41":"code","c53a8636":"code","e0d74c8a":"code","dbfa79f9":"code","e40ddcb9":"code","8f09d14e":"code","a259f918":"code","59e198b7":"code","dfabe24b":"code","d9f2cf2a":"code","146be33a":"code","782ac0cd":"code","fb66a774":"code","9c75320a":"code","61d9af97":"code","0689bfa1":"code","b03f65cf":"code","304438ef":"code","badd45ae":"code","44d9fe0f":"code","b9f4df5a":"code","1714c337":"code","a956d531":"code","998be52f":"code","4f1f8545":"code","d7b0b7dc":"code","0771043e":"code","10813348":"code","13842a36":"code","dad62e9d":"code","fe9ec1b6":"code","8251fa23":"code","66209632":"code","66410248":"code","741471c8":"code","2b50c2dc":"code","712ef744":"code","70aa2b96":"code","0f01e267":"code","14760c41":"code","6d9891f9":"code","e1b49889":"code","15e22c2f":"code","ae4d2008":"code","6d773c91":"code","015b0262":"code","997f27ab":"code","efe19968":"code","99b8ee00":"code","807d2444":"code","9dff6d01":"code","c1688fc4":"code","f793e966":"code","2ea795af":"code","03d2f04e":"code","abb4b2ef":"code","2e116c2b":"code","fd73d166":"code","26929eb1":"markdown","f6af001a":"markdown","5be8d327":"markdown","efe2d927":"markdown","0c4de7d5":"markdown","389a323d":"markdown","03564512":"markdown","ac25ae6d":"markdown","96ef1bfe":"markdown","b93edd53":"markdown","b3efc500":"markdown","dbce78bd":"markdown","54d4491f":"markdown","866a655b":"markdown","4ba2e793":"markdown","9c0107d1":"markdown","01dddd9d":"markdown","19870c31":"markdown","2ef3d04d":"markdown","e7bc19ad":"markdown","13c2f2b8":"markdown","1ecfee80":"markdown","5ad56a29":"markdown","0d69d09f":"markdown","93902f23":"markdown","adcae737":"markdown","525e160e":"markdown"},"source":{"03a46502":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e2b4646":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","96dab199":"asteroid = pd.read_csv(\"..\/input\/d\/shrushtijoshi\/asteroid-impacts\/orbits - orbits.csv\")","ce24beff":"asteroid.head()","43d43e41":"asteroid.describe()","c53a8636":"asteroid.info()","e0d74c8a":"asteroid.columns","dbfa79f9":"sns.heatmap(asteroid.isnull(),cbar=False,yticklabels=False,cmap = 'viridis')","e40ddcb9":"asteroid.replace(np.nan, inplace = True)\nasteroid=asteroid.dropna()\nasteroid","8f09d14e":"plt.figure(figsize=(12,12))\nsns.heatmap(asteroid.corr(),cmap=\"Blues\",annot=True)","a259f918":"from sklearn.preprocessing import StandardScaler","59e198b7":"variables= ['Epoch (TDB)', 'Orbit Axis (AU)', 'Orbit Eccentricity',\n       'Orbit Inclination (deg)', 'Perihelion Argument (deg)',\n       'Node Longitude (deg)', 'Mean Anomoly (deg)',\n       'Perihelion Distance (AU)', 'Aphelion Distance (AU)',\n       'Orbital Period (yr)', 'Minimum Orbit Intersection Distance (AU)',\n       'Orbital Reference', 'Asteroid Magnitude']\nx = asteroid.loc[:, variables].values","dfabe24b":"y = asteroid.loc[:,['Hazardous']].values","d9f2cf2a":"x = StandardScaler().fit_transform(x)\nx = pd.DataFrame(x)\nx.head()\n","146be33a":"from sklearn.decomposition import PCA\npca=PCA()","782ac0cd":"pca = PCA(n_components=4)","fb66a774":"x.shape","9c75320a":"x","61d9af97":"scaledwpca=pca.fit_transform(x)","0689bfa1":"scaledwpca.shape","b03f65cf":"scaledwpca=pd.DataFrame(scaledwpca)\nscaledwpca.head()","304438ef":"PC_values=np.arange(pca.n_components_) + 3\nplt.figure(figsize=(10,10))\nplt.plot(PC_values,pca.explained_variance_ratio_,'ro-',linewidth=2)\nplt.title(\"Scree Plot\")\nplt.xlabel(\"Principal Components\")\nplt.ylabel(\"Proportion of Explained Variance\")\nplt.show()","badd45ae":"print(\"Explained Variance\", pca.explained_variance_)\nprint(\"Proportion of Explained Variance\", pca.explained_variance_ratio_)","44d9fe0f":"import numpy as np\nprint('Cumulative Proportion of Explained Variance',np.cumsum(pca.explained_variance_ratio_))","b9f4df5a":"scaledwpca['Hazardous'] = y\nscaledwpca.columns=['PC1','PC2','PC3','PC4','Hazardous']\nscaledwpca.head()","1714c337":"pcak = pd.DataFrame(scaledwpca)\npcak.head()","a956d531":"per_var = np.round(pca.explained_variance_ratio_* 100, decimals=1)\nlabels = ['PC' + str(x) for x in range(1, len(per_var)+1)]\n \nplt.bar(x=range(1,len(per_var)+1), height=per_var, tick_label=labels)\nplt.ylabel('Percentage of Explained Variance')\nplt.xlabel('Principal Component')\nplt.title('Scree Plot')\nplt.show()","998be52f":"asteroid.info()","4f1f8545":"from sklearn.cluster import KMeans","d7b0b7dc":"wss=[]\nk = range(1,13)\nfor i in k:\n    kmeans_pca=KMeans(n_clusters = i)\n    kmeans_pca.fit(pcak)\n    wss.append(kmeans_pca.inertia_)","0771043e":"plt.figure(figsize=(10,10))\nplt.plot(k,wss,'bo-')\nplt.xlabel(\"Number of Clusters\")\nplt.ylabel(\"WSS\")\nplt.title(\"KMeans with PCA CLUSTERS\")\nplt.show()","10813348":"kmeans_pca.cluster_centers_","13842a36":"k_means = KMeans(n_clusters = 4, random_state = 42)\nk_means.fit(pcak[['PC1', 'PC2', 'PC3', 'PC4']])","dad62e9d":"pcak['KMeans_labels'] = k_means.labels_\n\n#Plotting resulting clusters\nplt.figure(figsize = (10, 10))\nplt.scatter(pcak['PC1'], pcak['PC3'], c = pcak['KMeans_labels'])\nplt.title('K-Means Clustering', fontsize = 22)\nplt.xlabel('Feature 1', fontsize = 14)\nplt.ylabel('Feature 2', fontsize = 14)","fe9ec1b6":"pcak['KMeans_labels'] = k_means.labels_\n\nplt.figure(figsize = (10, 10))\nplt.scatter(pcak['PC3'], pcak['PC4'], c = pcak['KMeans_labels'])\nplt.title('K-Means Clustering', fontsize = 22)\nplt.xlabel('Feature 1', fontsize = 14)\nplt.ylabel('Feature 2', fontsize = 14)","8251fa23":"x = asteroid.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12,13]].values\ny = asteroid.iloc[:,[15]].values","66209632":"x","66410248":"y","741471c8":"y","2b50c2dc":"from sklearn.model_selection import train_test_split\nX_train,X_test, Y_train, Y_test = train_test_split(x,y,train_size=0.7,test_size=0.3,random_state=100)","712ef744":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()","70aa2b96":"X_train=sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)","0f01e267":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(X_train,Y_train)","14760c41":"Y_pred = classifier.predict(X_test)","6d9891f9":"from sklearn.metrics import confusion_matrix","e1b49889":"cm = confusion_matrix(Y_test,Y_pred)\ncm","15e22c2f":"from sklearn.metrics import accuracy_score","ae4d2008":"accuracy  = accuracy_score(Y_test,Y_pred)\naccuracy","6d773c91":"print(\"Accuracy for LogReg model is\",accuracy*100)","015b0262":"import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","997f27ab":"asteroid.head()","efe19968":"features = ['Epoch (TDB)', 'Orbit Axis (AU)', 'Orbit Eccentricity',\n       'Orbit Inclination (deg)', 'Perihelion Argument (deg)',\n       'Node Longitude (deg)', 'Mean Anomoly (deg)',\n       'Perihelion Distance (AU)', 'Aphelion Distance (AU)',\n       'Orbital Period (yr)', 'Minimum Orbit Intersection Distance (AU)',\n       'Orbital Reference', 'Asteroid Magnitude']\nX = asteroid[features]\ny = asteroid['Classification']","99b8ee00":"X","807d2444":"y","9dff6d01":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n","c1688fc4":"dec = DecisionTreeClassifier()\n\ndec = dec.fit(X_train,y_train)","f793e966":"y_pred = dec.predict(X_test)","2ea795af":"from sklearn import tree","03d2f04e":"fig = plt.figure(figsize=(25,20))\nDecisionTree= tree.plot_tree(dec,feature_names=features,filled=True)","abb4b2ef":"print(tree.export_text(dec))","2e116c2b":"print(\"Accuracy of decision tree is:\",metrics.accuracy_score(y_test, y_pred)*100)","fd73d166":"print(\"Accuracy of Logistic Regression Model is :\",accuracy*100)\nprint(\"Accuracy of Decision Tree Model is:\",metrics.accuracy_score(y_test, y_pred)*100)","26929eb1":"Understanding the decision tree in a text format","f6af001a":"Elbow Method graph","5be8d327":"Importing Libraries","efe2d927":"Size of dataset after fitting it to PCA","0c4de7d5":"Correlation Map\n","389a323d":"![image.png](attachment:f4dd2c23-9e2d-44e5-97d7-578dc4c33d26.png)","03564512":"Often used in high dimensionality data sets, where which variable calls the shots is difficult to estimate. PCA says hold my beer and does it for us. In other words, PCA calculates which feature is the driving feature in the dataset and explains the most variance. This is not to be confused with thinking that PCA finds the HIGHEST Variance, no. Dimensions thus decided are plotted orthogonally where the pc component with most variance is the main linear line. PCA tries to minimize loss in variance when the full power of variable(s) are not being taken fully into consideration in calculation of a model for a target variable. Here, Target Variable =  Hazardous","ac25ae6d":"Checking for null values","96ef1bfe":"Graph shows the elbow point at 4 clusters after which it starts declining steadily","b93edd53":"Proportion of explained variance is individual variances dvided by summation of all variances in the components.","b3efc500":"# Principal Component Analysis","dbce78bd":"Specifying number of pca components","54d4491f":"Here, as can be seen, there is a lot of loss in variance, due to an ill fitting line. PCA does operate on the understanding of Linear regression. However, should not be confused because it is an unsupervised algorithm.","866a655b":"Putting the pc components in a dataset for K Means","4ba2e793":"# Asteroid Impacts","9c0107d1":"![image.png](attachment:4f26b232-70ad-49b8-8c7b-2e2a4749324d.png)","01dddd9d":"**K MEANS CLUSTERING**","19870c31":"# Logistic Regression","2ef3d04d":"*****Scaling*****","e7bc19ad":"Size of dataset orginally","13c2f2b8":"Here, as can be seen, there is a lesser loss in variance, due to an better fitting line. The second line is perpendicular (orthogonality) to the first and is indicative of the second PC Component. ","1ecfee80":"Target Variable = Hazardous","5ad56a29":"****Dataset was taken from NASA JPL's 'Possible Asteroid Impact' dataset uploaded on Kaggle. I have modified the original dataset where the Hazardous values are separated from the object classification names for better implementation of multiple algorithms.\nWe will perform Principal Component Analysis and TRY to use the values thus derived into a K Means clustering visualization on the \"Hazardous\" target variable, followed by Logistic Regression(also Hazardous) and Decision Tree on the \"Classification\" target variable. Comments and suggestions are appreciated. Here's a link to the original dataset \nhttps:\/\/www.kaggle.com\/nasa\/asteroid-impacts ****\n","0d69d09f":"# Decision Tree","93902f23":"#   **PCA, K Means, Logistic Regression and DecisionTree**","adcae737":"***PCA***","525e160e":"Reading the Data"}}