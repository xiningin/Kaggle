{"cell_type":{"e1ed1e5f":"code","7b1d105d":"code","ed2ce6d3":"code","0b43d868":"code","6718d41e":"code","2f8d0576":"code","bdb8899b":"markdown","7d67b149":"markdown","8bc8bf1b":"markdown","1f0a39ae":"markdown","47424909":"markdown","deb88544":"markdown","6631b2b6":"markdown","2646780d":"markdown"},"source":{"e1ed1e5f":"import pandas as pd\nimport numpy as np\nimport os\n\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\n\nos.listdir('..\/input\/')\ntrain = pd.read_csv('..\/input\/fashion-mnist_train.csv')\ntest = pd.read_csv('..\/input\/fashion-mnist_test.csv')\n\n","7b1d105d":"ds = train.append(test)\n\nX = ds.iloc[:,1:]\ny = ds.iloc[:,0]\n\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state = True)","ed2ce6d3":"rf = RandomForestClassifier(n_estimators = 100,max_features = 5)\n\nrf.fit(X_train,y_train)\n\nprint('Training score: {:.3f}'.format(rf.score(X_train,y_train)))\nprint('Test score : {:.3f}'.format(rf.score(X_test,y_test)))","0b43d868":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\n\nrf = RandomForestClassifier().fit(X_train,y_train)\n\n\nparam_values = {'n_estimators' : [10,50,100], 'max_features' : [5,10,20]}\ngs = GridSearchCV(rf,param_grid = param_values, cv = 5,scoring = 'accuracy')\n\ngs.fit(X_test,y_test)\n","6718d41e":"print(gs.best_score_,gs.best_params_)","2f8d0576":"gb = GradientBoostingClassifier(n_estimators = 100,max_features = 5,learning_rate = 0.1)\n\ngb.fit(X_train,y_train)\nprint('Training score: {:.3f}'.format(gb.score(X_train,y_train)))\nprint('Test score: {:.3f}'.format(gb.score(X_test,y_test)))","bdb8899b":"### 2.1 Model tuning\n\nIn the above model, I selected n_estimators = 100 (how many individual trees the model is fitting), and max_features = 5 (how many parameters are considered at each split). We can tune these parameters to see if we can improve the model's fit. I am going to use grid search to do this. \n\n*Note: to improve fit I could increase my parameters further to build more trees and consider more features, but this causes the model to run slowly, and I am using this notebook more as a tutorial than as a real attempt to get the best score possible.*\n","7d67b149":"# Image classification using Random Forests and Gradient Boosting \n","8bc8bf1b":"I am going to compare using Random Forests and Gradient Boosting Machines to classify fashion images. \n\n## 1. Data Prep\nFirstly, I will load in the libraries and datasets. There are two datasets to load in; training and test. These files both contain a column of class labels - what the image is of - and then the rest of the columns each represent a pixel of the image. The pixel columns will be our features, and the class label is our target to predict. ","1f0a39ae":"## 2. Random Forests\n\nI am going to start by fitting a random forest. The random forest model computes many decision trees, taking an average over all of the trees for predictions. In a classification problem like the one we are solving, each tree computes a probability the observation belongs to a given class, and it is these probabilities we average, ultimately classifying to the class with the highest average from this process. \n\nRandom forests are a type of ensemble model, which means computing many models and combining them to make predictions. The performance of an ensemble model is likely to outperform a single model; here, each of the trees is probably over-fitting to different parts of the data, so by fitting many trees and taking an average we can get better overall predictive performance. \n\nHowever, an element of randomness is needed to decorrelate the trees. There are two ways this randomness is incorporated:\n\n* Each tree is fitted to a bootstrap sample of the original data, which is the same size as the original dataset. Essentially this means each tree is fitted to a new dataset created by sampling with replacement from the original dataset until the new dataset is equal in number of observations to the original dataset. Note that due to sampling with replacement, some observations from the original data will not appear in the bootstrap sample, and some will be repeated.\n\n\n* At each split in each tree, only a subset of features can be considered. We set this value in our model, and then each split considers only this many features at each split, selected at random.\n\n### 2.1 Model fitting\n\nWe can create a simple random forest model and see how well it performs on the test and training datasets.****","47424909":"I am going to merge the two training and test files together and create my own training and test splits.","deb88544":"## 3. Gradient Boosting Machines\n\nGradient Boosting Machines are another type of ensemble model that uses decision trees. However, this time, instead of fitting many trees to bootstrap samples and taking an average, each tree is fit sequentially, trying to correct for the errors of the previous tree. The parameters involved are the same as in the random forest model, but with the addition of the learning rate; this controls how fast the sequential trees try to correct for the error of the previous trees. \n\n### 3.1 Model fitting\n\nWe can fit a gradient boosting machine to our data, specifying our parameters.","6631b2b6":"### 3.2 Model tuning\n\nAs above, with random forests, we could run Grid search for Gradient boosting, but include learning_rate as a parameter to tune.","2646780d":"We can see our model does best with 100 trees (n_estimators) and allowing 20 features to be considered at each split. "}}