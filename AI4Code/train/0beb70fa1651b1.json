{"cell_type":{"5167dfbd":"code","58755255":"code","802236e3":"code","05cc257a":"code","164fc45c":"code","b2aa4603":"code","32af5071":"code","5e3b530b":"code","b3263254":"code","8964c7af":"code","795c6fe5":"code","926a05c3":"code","5b144f4c":"code","31e23a9a":"code","36786da1":"code","071a75b1":"code","31d45395":"code","66d9daac":"code","34a1b96d":"code","18e44263":"code","504f44a5":"code","158f320b":"code","dbebc09f":"code","6b853563":"code","e4b6a407":"code","ba6d818f":"code","e10672eb":"code","8ee27956":"code","4b88ae2c":"code","94a9a6b4":"code","b2061825":"code","a1c8f83f":"code","37c73383":"code","354295f5":"code","c176483f":"code","10ddf428":"code","188e1628":"code","05ada1e6":"code","7d2473dd":"code","229edd04":"code","a6841cb5":"code","62f2156a":"code","c6541498":"code","3c4e289f":"code","814e5902":"code","edcf5f8c":"code","5a24aa44":"code","59283632":"code","5f795f02":"markdown","f84c18dc":"markdown","1b81a666":"markdown","e190b12d":"markdown","16a2b8ac":"markdown","1fcc63be":"markdown","0c81016e":"markdown","6f4fe99a":"markdown","af935c0f":"markdown","30fcbeed":"markdown","8ed93a31":"markdown","5f0e7859":"markdown","a70af4da":"markdown","cf7a3963":"markdown","94be8cd5":"markdown","f1d6e6dd":"markdown","18622fc9":"markdown","1ac3df7c":"markdown","718752b5":"markdown","62659034":"markdown","a5ebfc93":"markdown","5a0a6a0a":"markdown","1bc4e485":"markdown","34f660a8":"markdown","d124acf0":"markdown","17d686f9":"markdown","3f9586ee":"markdown","874de1fd":"markdown"},"source":{"5167dfbd":"%pip install contractions\n\nimport numpy as np\nimport pandas as pd\nimport contractions\nimport nltk\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nfrom tensorflow.keras.models import save_model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, Flatten, MaxPooling1D, Conv1D\nfrom collections import Counter\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords, wordnet\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score\nfrom IPython.display import display","58755255":"data = pd.read_csv(\"..\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv\")\ndisplay(data.head())\ndisplay(data.info())\nprint(data[\"tweet\"][22])","802236e3":"data.drop(\"id\", axis=1, inplace=True)\ndisplay(data.head())\ndisplay(data.info())","05cc257a":"def expand_contractions(df_series):\n    \"\"\" Expands contractions from text in pandas series.\n        (Eg: can't --> cannot)\n        \n    Args:\n        df_series (pd.Series): Pandas series containing text data.\n    \n    Returns:\n        df_series (pd.Series): Pandas series containing text data after \n                               expanding contractions.\n    \"\"\"\n    \n    for i in range(len(df_series)):\n        df_series[i] = contractions.fix(df_series[i])\n    \n    return df_series\n\n\n\ndef get_pos(token):\n    \"\"\" Returns \"part of speech\" of the token which is understandable \n        by WordNetLemmatizer.\n        \n    Args:\n        token (str): Single token whose POS to be identified.\n    \n    Returns:\n        (str): POS tag of the token in a format understandable by WordNetLemmatizer.\n    \"\"\"\n    \n    pos_tag = nltk.pos_tag(token)[0][1][0].upper()\n    pos_tag_dict = {\"J\": wordnet.ADJ,\n                    \"N\": wordnet.NOUN,\n                    \"V\": wordnet.VERB,\n                    \"R\": wordnet.ADV}\n    \n    # Returns wordnet.NOUN as default if it can't find exact POS \n    return pos_tag_dict.get(pos_tag, wordnet.NOUN)\n\n\n\ndef lemmatize_series(df_series, remove_stopwords=False):\n    \"\"\" Lemmatizes text data in pandas series and removes stopwords.\n        \n    Args:\n        df_series (pd.Series): Pandas series containing text data.\n        remove_stopwords (bool): Removes stopwords from the text if True. \n                                 Defaults to False.\n                                 \n    Returns:\n        df_series (pd.Series): Pandas series containing lemmatized text data \n                               without stopwords if specified.\n    \"\"\"\n    \n    if remove_stopwords:\n        stop_words = set(stopwords.words(\"english\"))\n        lm = WordNetLemmatizer()\n        for i in range(len(df_series)):\n            df_series[i] = ' '.join([lm.lemmatize(word, get_pos(word)) \n                                     for word in df_series[i].split() \n                                      if not word.lower() in stop_words])\n    else:\n        lm = WordNetLemmatizer()\n        for i in range(len(df_series)):\n            df_series[i] = ' '.join([lm.lemmatize(word, get_pos(word)) \n                                     for word in df_series[i].split()])\n    \n    return df_series","164fc45c":"def preprocess_tweets(df_series, remove_stopwords=True):\n    \"\"\" Removes account tags (@user) and all non-alphanumeric characters except whitespace.\n        \n    Args:\n        df_series (pd.series): Pandas series object containing tweets.\n        remove_stopwords (bool): Removes stopwords from tweets if True. Defaults to True. \n        \n    Returns:\n        df_series (pd.series): Pandas series object containing preprocessed tweets. \n    \"\"\"\n    \n    # Expand contractions (Eg: can't --> cannot)\n    df_series = expand_contractions(df_series)\n    \n    # Removes '@user' tags\n    df_series = df_series.str.replace(\"@user\", \"\", regex=False)\n    \n    # Removes '&amp' tags\n    df_series = df_series.str.replace(\"&amp\", \" \")\n    \n    # Removes non alphanumeric characters\n    df_series = df_series.str.replace(\"[^a-zA-Z0-9 ]\", \" \")\n    \n    # Lemmatize tweets\n    df_series = lemmatize_series(df_series, remove_stopwords=remove_stopwords)\n    \n    return df_series","b2aa4603":"data[\"tweet\"] = preprocess_tweets(data[\"tweet\"])\ndisplay(data.head())\nprint(data[\"tweet\"][22])","32af5071":"labels = {0: \"Not Offensive\", 1: \"offensive\"}\nprint(labels)","5e3b530b":"X_train, X_test, y_train, y_test = train_test_split(data[\"tweet\"], data[\"label\"],\n                                                    stratify=data[\"label\"],\n                                                    train_size=0.8,\n                                                    random_state=22)","b3263254":"# displays shape of training and validation data\nprint(X_train.shape)\nprint(X_test.shape)","8964c7af":"def plot_confusion_matrix(conf_matrix):\n    \n    \"\"\" Plots confusion matrix.\n    \n    Args:\n        conf_matrix (list): Array containing values as in the format returned\n                            by 'sklearn.metrics.confusion_matrix'\n                            \n    Returns:\n        Nothing\n    \n    \"\"\"\n    \n    fig, ax = plt.subplots(figsize=(7.5, 7.5))\n    ax.matshow(conf_matrix, cmap=plt.cm.Blues, alpha=0.3)\n    for i in range(conf_matrix.shape[0]):\n        for j in range(conf_matrix.shape[1]):\n            ax.text(x=j, y=i, s=conf_matrix[i, j], va='center', ha='center', size='xx-large')\n    \n    plt.xlabel('Predictions', fontsize=18)\n    plt.ylabel('Actuals', fontsize=18)\n    plt.title('Confusion Matrix', fontsize=18)\n    plt.show()\n    \n    ","795c6fe5":"class CountVectorSentimentModel:\n    \n    \"\"\" \n    This class can be used to classify text data.\n    \n    Uses dot product to find similarity between given text and training\n    data. Finds top similar training data's labels and returns most common\n    labels from the top 7 labels as predicted label.\n    \n    NOTE: This statistical method uses CountVectorizer to vectorize \n    text, which returns word count vectors.\n    \n    ...\n    \n    Attributes\n    ----------\n    vect (CountVectorizer): CountVectorizer object fit on training data\n                            to vectorize text.\n        \n    train_count_vectors (list): Array of word count vectors of training data.\n    \n    y_train (list): Array of labels of each observation in training data.\n    \n    \n    Methods\n    -------\n    fit: Vectorizes text data and stores them for generating predictions.\n        \n    predict: Takes array of text data as argument to classify them.\n    \n    most_common: Finds and returns most frequent element from a list.\n    \n    get_dot_product: Converts text into word count vectors and multiply (dot product) \n                     it with vector stored by .fit() method.\n    \n    \"\"\"\n    \n    \n    def __init__(self):\n        \n        \"\"\" Class constructor.\n        \n        Args:\n            Nothing\n            \n        Returns:\n            Nothing\n        \n        \"\"\"\n        \n        self.vect = CountVectorizer()    # Initializes vectorizer\n        \n    \n    def fit(self, X_train, y_train): \n        \n        \"\"\" Vectorizes text data and stores them for generating predictions.\n        \n        Args:\n            X_train (list): Array containing text data to be vectorized and stored.\n            y_train (list): Array of labels.\n        \n        Returns:\n            Nothing\n            \n        \"\"\"\n        \n        # Fit vectorizer on training data and vectorizes text\n        self.train_count_vectors = self.vect.fit_transform(X_train).toarray()\n        \n        # Stores labels\n        self.y_train = y_train.values\n    \n    \n    def predict(self, X_test):\n        \n        \"\"\" Takes array of text data as argument to classify them.\n        \n        Args:\n            X_test (list): Array of text data to classify.\n            \n        Returns:\n            labels (list): Array of predicted labels.\n        \n        \"\"\"\n        \n        labels = []\n        for text in X_test:\n            # Uses dot product to find similarity between given text and training\n            # data. Finds top similar training data's labels and returns most common\n            # labels from the top 7 labels as predicted label.\n            \n            dot_vector = self.get_dot_product(text)\n            indices = np.argsort(dot_vector)[::-1][:7]  \n            labels.append(self.most_common(self.y_train[indices]))\n            \n        return labels\n    \n    \n    def most_common(self, lst):\n        \n        \"\"\" Finds and returns most frequent element from a list.\n        \n        Args:\n            lst (list): Array from which most frequent element to be found.\n            \n        Returns:\n            (int): Most frequent element in the array. \n        \n        \"\"\"\n        \n        data = Counter(lst)\n        return data.most_common(1)[0][0]\n        \n    \n    def get_dot_product(self, text):\n        \n        \"\"\" Converts text into word count vectors and multiply (dot product) it with\n            vector stored by .fit() method.\n            \n        Args:\n            text (str): String to be vectorized and multiplied (dot product) with \n                        vector stored by .fit() method.\n                        \n        Returns:\n            dot_products (list): Array of values obtained by dot product.\n            \n        \"\"\"\n        \n        count_vector = self.vect.transform([text]).toarray()[0]\n        dot_products = count_vector.T@self.train_count_vectors.T\n        return dot_products","926a05c3":"# Training\ncvsm = CountVectorSentimentModel()\ncvsm.fit(X_train, y_train)","5b144f4c":"%%timeit\ncvsm.predict([\"thought factory bbc neutrality on right wing fascism politics media blm brexit trump leaders\"])","31e23a9a":"class CountVectorSentimentModelOptimized:\n    \n    \"\"\" \n    This class can be used to classify text data.\n    \n    Optimised version of CountVectorSentimentModel class.\n    Uses same concept in a different, efficient way.\n    \n    Stores two vectors when calling .fit() method. One is non offensive \n    vector which is a single word count vector of all words in non offensive tweets.\n    Another one is offensive vector which is a single word count vector of all words\n    in offensive tweets. When calling .predict() method, Calculates dot product between\n    given new vector and these two vectors. Category with highest dot product value is \n    returned as predicted label. This eliminates the loops, vectorizing operations\n    and large matrices which were in old CountVectorSentimentModel which \n    accounts for improved performance.  \n    \n    NOTE: This statistical method uses CountVectorizer to vectorize \n    text, which returns word count vectors.\n    \n    ...\n    \n    Attributes\n    ----------\n    vect (CountVectorizer): CountVectorizer object fit on whole training data\n                            to vectorize text.\n        \n    non_off_vector (list): Word count vector of non offensive data.\n    \n    off_vector (list): Word count vector of offensive data.\n    \n    \n    Methods\n    -------\n    fit: Vectorizes text data and stores them for generating predictions.\n        \n    predict: Takes array of word count vectors as argument to classify them.\n    \n    vectorize: Vectorizes text into word count vectors with the same vectorizer \n               object fit on training data.\n    \n    \"\"\"\n    \n    \n    def __init__(self):\n        \n        \"\"\" Class constructor.\n        \n        Args:\n            Nothing\n            \n        Returns:\n            Nothing\n            \n        \"\"\"\n        \n        self.vect = CountVectorizer()    # Initializes vectorizer\n    \n    \n    def fit(self, train_df, X_col, y_col):\n        \n        \"\"\" Vectorizes all training data into single word count vector\n            for each category and stores them to generate predictons.\n            \n        Args:\n            train_df (pd.DataFrame): Pandas DataFrame object holding training data.\n            X_col (str): Name of the column containing text data.\n            y_col (str): Name of the column containing labels.\n        \n        Returns:\n            Nothing\n            \n        \"\"\"\n        \n        self.vect.fit(train_df[X_col])    # Fitting vectorizer to training data\n        \n        # Subsetting data by each category\n        non_off_tweets = train_df[train_df[y_col]==0][X_col].values\n        off_tweets = train_df[train_df[y_col]==1][X_col].values\n        \n        # Concatinating all non offensive tweets into single string \n        whole_non_off_tweets = \"\"\n        for tweet in non_off_tweets:\n            whole_non_off_tweets += tweet\n            \n        # Concatinating all offensive tweets into single string\n        whole_off_tweets = \"\"\n        for tweet in off_tweets:\n            whole_off_tweets += tweet\n        \n        # Vectorizing and storing each vector \n        self.non_off_vector = self.vect.transform([whole_non_off_tweets]).toarray()[0]\n        self.off_vector = self.vect.transform([whole_off_tweets]).toarray()[0]\n        \n    \n    def predict(self, X_test_vectors):\n        \n        \"\"\" Takes word count vectors as argument and returns predicted labels. \n        \n        Args:\n            X_test_vectors (list): Word count vectors.\n            \n        Returns:\n            y_pred (list): Array of predicted labels.  \n        \n        \"\"\"\n        \n        # Calculates dot product between X_test_vectors and non_off_vector\n        # stored by .fit() method and X_test_vectors and off_vector stored by\n        # .fit() method. category which has the highest dot product value\n        # is the predicted label.\n          \n        non_off_weights = self.non_off_vector.T@X_test_vectors.T\n        off_weights = self.off_vector.T@X_test_vectors.T\n        \n        y_pred = []\n        for i in range(len(non_off_weights)):\n            y_pred.append(np.argmax([non_off_weights[i], off_weights[i]]))\n        \n        return y_pred\n\n    \n    def vectorize(self, X_test):\n        \n        \"\"\" Used to vectorize text into word count vectors with the same\n            vectorizer object fit on training data.\n            \n        Args:\n            X_test (list): Array containing text data.\n            \n        Returns:\n            (list): Array of word count vectors.\n        \n        \"\"\"\n        return self.vect.transform(X_test).toarray()\n    ","36786da1":"# Converting data into compatible format\ntrain_df = pd.DataFrame({\"tweets\": X_train, \"labels\": y_train})\ntrain_df.reset_index(drop=True, inplace=True)\ndisplay(train_df.head())","071a75b1":"# Training\ncvsm_opt = CountVectorSentimentModelOptimized()\ncvsm_opt.fit(train_df, X_col=\"tweets\", y_col=\"labels\")","31d45395":"# Vectorizing sample text\ntrial_vect = cvsm_opt.vectorize([\"thought factory bbc neutrality on right wing fascism politics media blm brexit trump leaders\"])","66d9daac":"%%timeit\ncvsm_opt.predict(trial_vect)","34a1b96d":"# Gets predictions\nX_test_vect = cvsm_opt.vectorize(X_test)\ny_pred_cvsm = cvsm_opt.predict(X_test_vect)","18e44263":"# Plots confusion matrix and displays classification report\nconf_matrix_cvsm = confusion_matrix(y_test, y_pred_cvsm)\nplot_confusion_matrix(conf_matrix_cvsm)\nprint(classification_report(y_test, y_pred_cvsm, target_names=[\"Not offensive\", \"offensive\"]))","504f44a5":"# Dictionary to store F1 Scores of different models\nf1_dict = {}","158f320b":"# Updates model score to f1_dict\nf1_dict[\"cvsm\"] = f1_score(y_test, y_pred_cvsm)","dbebc09f":"class TfidfSentimentModel(CountVectorSentimentModelOptimized):\n    \n    \"\"\" \n    This class can be used to classify text data.\n    \n    Inherits all attributes and methods from CountVectorSentimentModelOptimized\n    class except that the vectorizer used is different.\n    \n    Tf-Idf vectors are more useful than word count vectors in this context since\n    the weights for common words are close to 0 in Tf-Idf vectors. So, We will have\n    higher value for unique words present in different classes.\n    \n    NOTE: This statistical method uses TfidfVectorizer to vectorize \n    text, which returns Tf-Idf vectors.\n    \n    ...\n    \n    Attributes\n    ----------\n    vect (CountVectorizer): CountVectorizer object fit on whole training data\n                            to vectorize text.\n        \n    non_off_vector (list): Word count vector of non offensive data.\n    \n    off_vector (list): Word count vector of offensive data.\n    \n    \n    Methods\n    -------\n    fit: Vectorizes text data and stores them for generating predictions.\n        \n    predict: Takes array of word count vectors as argument to classify them.\n    \n    vectorize: Vectorizes text into word count vectors with the same vectorizer \n               object fit on training data.\n    \n    \"\"\"\n    \n    def __init__(self):\n        self.vect = TfidfVectorizer()    # Initializes vectorizer\n        \n        ","6b853563":"# Training\ntvsm = TfidfSentimentModel()\ntvsm.fit(train_df, X_col=\"tweets\", y_col=\"labels\")","e4b6a407":"# Vectorizing sample text\ntrial_vect = tvsm.vectorize([\"thought factory bbc neutrality on right wing fascism politics media blm brexit trump leaders\"])","ba6d818f":"%%timeit\ntvsm.predict(trial_vect)","e10672eb":"# Gets predictions\nX_test_vect = tvsm.vectorize(X_test)\ny_pred_tfidf = tvsm.predict(X_test_vect)","8ee27956":"# Plots confusion matrix and displays classification report\nconf_matrix_tfidf = confusion_matrix(y_test, y_pred_tfidf)\nplot_confusion_matrix(conf_matrix_tfidf)\nprint(classification_report(y_test, y_pred_tfidf, target_names=[\"Not offensive\", \"offensive\"]))\n\n# Updates model score to f1_dict\nf1_dict[\"tvsm\"] = f1_score(y_test, y_pred_tfidf)","4b88ae2c":"def plot_history(history):\n    \n    \"\"\" Plots training history of models.\n    \n    Args:\n        history (History): Tensorflow History object containing training \n                           history of the model.\n                           \n    Returns:\n        Nothing\n        \n    \"\"\"\n    \n    history_dict = history.history\n    train_loss = history_dict['loss']    # Training loss over epochs\n    val_loss = history_dict['val_loss']    # Validation loss over epochs\n    epochs = range(1, len(history_dict['loss'])+1)\n    plt.plot(epochs, train_loss,'b', label='Training error')\n    plt.plot(epochs, val_loss,'b', color=\"orange\", label='Validation error')\n    plt.title('Training and Validation error')\n    plt.xlabel('Epochs')\n    plt.ylabel('Error')\n    plt.legend()\n    plt.show()\n    \n    ","94a9a6b4":"# Defining parameters\nvocab_size = 30000\nembedding_dim = 18\nmax_len = 50\ntrunc_type = \"post\"\npadding_type = \"post\"\noov_token = \"<OOV>\"","b2061825":"# Fitting tokenizer\ntokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)\ntokenizer.fit_on_texts(X_train)","a1c8f83f":"# Converting texts to sequences and padding them to make\n# them compatible with embedding layers\n\ntraining_seq = tokenizer.texts_to_sequences(X_train)\ntraining_padded = pad_sequences(training_seq,\n                                truncating = trunc_type,\n                                padding = padding_type,\n                                maxlen = max_len)\n\ntesting_seq = tokenizer.texts_to_sequences(X_test)\ntesting_padded = pad_sequences(testing_seq,\n                               truncating = trunc_type,\n                               padding = padding_type,\n                               maxlen = max_len)","37c73383":"# Defining model architecture and compiling\n\nembedding_model1 = Sequential([\n            Embedding(vocab_size, embedding_dim, input_length=max_len),\n            Flatten(),\n            Dense(64, activation=\"relu\"),\n            Dense(32, activation=\"relu\"),\n            Dense(16, activation=\"relu\"),\n            Dense(8, activation=\"relu\"),\n            Dense(4, activation=\"relu\"),\n            Dense(1, activation=\"sigmoid\")\n        ])\n\nembedding_model1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","354295f5":"# Displays model summary\ndisplay(embedding_model1.summary())","c176483f":"# Training\nhistory1 = embedding_model1.fit(training_padded, \n                              y_train.values, \n                              validation_data=(testing_padded, y_test.values),\n                              epochs=5,\n                              batch_size=32)","10ddf428":"# Plots loss over epochs\nplot_history(history1)","188e1628":"# Gets predictions\ny_pred_embedding_model1 = embedding_model1.predict(testing_padded).flatten()\ny_pred_embedding_model1 = [1 if x>=0.5 else 0 for x in y_pred_embedding_model1]","05ada1e6":"# Plots confusion matrix and displays classification report\nconf_matrix_embedding_model1 = confusion_matrix(y_test, y_pred_embedding_model1)\nplot_confusion_matrix(conf_matrix_embedding_model1)\nprint(classification_report(y_test, y_pred_embedding_model1, \n                            target_names=[\"Not offensive\", \"offensive\"]))\n\n# Updates model score to f1_dict\nf1_dict[\"embedding_model\"] = f1_score(y_test, y_pred_embedding_model1)","7d2473dd":"# Defining model architecture and compiling\n\nembedding_model2 = Sequential([\n            Embedding(vocab_size, embedding_dim, input_length=max_len),\n            Conv1D(16, 4, activation=\"relu\"),\n            MaxPooling1D(2),\n            Conv1D(32, 4, activation=\"relu\"),\n            Flatten(),\n            Dense(64, activation=\"relu\"),\n            Dense(32, activation=\"relu\"),\n            Dense(16, activation=\"relu\"),\n            Dense(8, activation=\"relu\"),\n            Dense(4, activation=\"relu\"),\n            Dense(1, activation=\"sigmoid\")\n        ])\n\nembedding_model2.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","229edd04":"# Displays model summary\nembedding_model2.summary()","a6841cb5":"# Training\nhistory2 = embedding_model2.fit(training_padded, \n                              y_train.values, \n                              validation_data=(testing_padded, y_test.values),\n                              epochs=5,\n                              batch_size=32)","62f2156a":"# Plots loss over epochs\nplot_history(history2)","c6541498":"# Gets predictions\ny_pred_embedding_model2 = embedding_model2.predict(testing_padded).flatten()","3c4e289f":"y_pred_embedding_model2 = [1 if x>=0.5 else 0 for x in y_pred_embedding_model2]","814e5902":"# Plots confusion matrix and displays classification report\nconf_matrix_embedding_model2 = confusion_matrix(y_test, y_pred_embedding_model2)\nplot_confusion_matrix(conf_matrix_embedding_model2)\nprint(classification_report(y_test, y_pred_embedding_model2, target_names=[\"Not offensive\", \"offensive\"]))\n\n# Updates model score to f1_dict\nf1_dict[\"embedding_model_conv1D\"] = f1_score(y_test, y_pred_embedding_model2)","edcf5f8c":"# Displays F1 Score of all models\nprint(f1_dict)","5a24aa44":"# Displays comparison of models based on F1 Score\nx = list(f1_dict.keys())\ny = list(f1_dict.values())\nax = sns.barplot(y, x)\nax.set_xlabel(\"F1 Score\")\nplt.show()","59283632":"# Saving trained version of optimized count vector sentiment model\ncvsm_file = open(\"cvsm.obj\", 'wb')\npickle.dump(cvsm_opt, cvsm_file)\n\n# Saving trained version of optimized Tf-Idf vector sentiment model\ntvsm_file = open(\"tvsm.obj\", 'wb')\npickle.dump(tvsm, tvsm_file)\n\n# Saving word embedding model\nsave_model(embedding_model1, \"embedding\")\n\n# Saving word embedding model\nsave_model(embedding_model2, \"embedding_conv1d\")","5f795f02":"![image.png](attachment:c6cfe2e9-03a3-47e9-9e6e-d3e5e5426476.png)","f84c18dc":"> **TASK : To classify tweets as Offensive(1) \/ Non-offensive(0)**\n\n> **EDA Notebook Link :** https:\/\/www.kaggle.com\/balamurugan1603\/twitter-sentiment-analysis-eda","1b81a666":"# **Metric :**\n\nF1 Score is being used to compare the models due to class imbalance.\n\nTrue Positives (TP) - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes.\n\nTrue Negatives (TN) - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.\n\nFalse Positives (FP) \u2013 When actual class is no and predicted class is yes.\n\nFalse Negatives (FN) \u2013 When actual class is yes but predicted class in no.\n\nPrecision = TP\/TP+FP\n\nRecall = TP\/TP+FN\n\nF1 Score = 2(Recall Precision) \/ (Recall + Precision)\nF1 is usually more useful than accuracy, especially if for an uneven class distribution.","e190b12d":"# **Twitter Sentiment Analysis**","16a2b8ac":"![image.png](attachment:b786973a-c3f7-4676-8f41-1b971a039030.png)","1fcc63be":"*Time taken for single prediction :*","0c81016e":"# **Motivation :**\nHate speech is an unfortunately common occurrence on the Internet. Often social media sites like Facebook and Twitter face the problem of identifying and censoring problematic posts while weighing the right to freedom of speech. The importance of detecting and moderating hate speech is evident from the strong connection between hate speech and actual hate crimes. Early identification of users promoting hate speech could enable outreach programs that attempt to prevent an escalation from speech to action. Sites such as Twitter and Facebook have been seeking to actively combat hate speech. In spite of these reasons, NLP research on hate speech has been very limited, primarily due to the lack of a general definition of hate speech, an analysis of its demographic influences, and an investigation of the most effective features.\n","6f4fe99a":"By : <a href=\"https:\/\/www.linkedin.com\/in\/bala-murugan-62073b212\/\">Balamurugan P<\/a>","af935c0f":"*Time taken for single prediction :*","30fcbeed":"* *Hmm...Thats pretty quicker.*","8ed93a31":"**Model with Embedding, 1-dimensional Convolutions (Conv1D) and Dense layers :**","5f0e7859":"**Using Term frequency - Inverse document frequency vectors :**\n\n\nIn Tf-Idf vectors, The weights for frequent words are close to 0. So, We will have higher value for unique words present in the corpus.","a70af4da":"# **Text classification using word embeddings and deep learning :**","cf7a3963":"# **Text Classification models using dot product similarity :**","94be8cd5":"* *This method takes very longer time and infeasible to use for classification.*","f1d6e6dd":"# **Splitting data :**","18622fc9":"*Embedding along with conv1D layers performs slightly better than other methods. The performance of less complex model which uses Tf-Idf and dot product that I have created is also not bad.*","1ac3df7c":"# **Dependencies :**","718752b5":"# **Data :**\n\n**train.csv** - Labelled dataset of 31,962 tweets. Csv file with each line storing a tweet id, its label and the tweet.\n\n**test.csv** - The test data file contains only tweet ids and the tweet text with each tweet in a new line.","62659034":"*Time taken for single prediction :*","a5ebfc93":"**Using word count vectors :**\n\nWord count vector is the vector containing count of the words for words present in a document and 0 for words not present in the document but present in the whole corpus. ","5a0a6a0a":"**Model with Embedding layer and normal dense layers :**","1bc4e485":"# **Preprocessing :**","34f660a8":"# **Save the models :**","d124acf0":"# **Function to plot confusion matrix :**","17d686f9":"*In the above plot each point represents a word and each cluster represents a category. This Image is attached just for better understanding of word embeddings.*","3f9586ee":"**Let's optimize it**","874de1fd":"Word embeddings are layers used in neural networks which projects words into higher dimensional vectors. Unlike other text vectorization techniques, word embedding layers trains as like other neural network layers. The words with similar meaning are clustered together at the end of training. Embedding layers understands the meaning of the words from the labels."}}