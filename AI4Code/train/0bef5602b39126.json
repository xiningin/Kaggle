{"cell_type":{"4040a846":"code","4d895d0f":"code","354192c8":"code","78f43b0a":"code","c8e44b36":"code","50883cc6":"code","8a71ad75":"code","6fd69f3a":"code","458e2722":"code","97aff731":"code","590042f5":"code","bc29714a":"code","430bedde":"code","0d46ccf6":"code","87824f5e":"code","110f970f":"code","8c0cef3c":"code","481c794c":"code","ffd1a93a":"code","8b401b61":"code","eed73799":"code","1b630054":"code","e2c5cabf":"code","3af81c1b":"code","accaa66d":"code","6d74c256":"code","74ac6906":"code","6d7a38ae":"code","18f5174f":"code","6e8d0ff5":"code","289d9ee1":"code","f2cb7ee7":"code","476e4303":"markdown","8ee4035e":"markdown","799b203e":"markdown","7bbf5eaa":"markdown","33c644d1":"markdown","53379ebe":"markdown","41c70ae1":"markdown","60d19805":"markdown","c255a9cf":"markdown","1e502a18":"markdown","acde1c5b":"markdown","3361d6bd":"markdown","fd896682":"markdown","04fd4c48":"markdown"},"source":{"4040a846":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d895d0f":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom tqdm import tqdm","354192c8":"# load data\ndata_churn = pd.read_csv('\/kaggle\/input\/prediction-of-bug\/churn.csv', sep='\\s;\\s*', engine='python')\ndata_entry = pd.read_csv('\/kaggle\/input\/prediction-of-bug\/ent.csv', sep='\\s;\\s*', engine='python')\ndata_bugs = pd.read_csv('\/kaggle\/input\/prediction-of-bug\/bug-metrics.csv', sep='\\s;\\s*', engine='python')\ndata_change = pd.read_csv('\/kaggle\/input\/prediction-of-bug\/change-metrics.csv', sep='\\s;\\s*', engine='python')\ndata_complexity_change = pd.read_csv('\/kaggle\/input\/prediction-of-bug\/complexity-code-change.csv', sep='\\s;\\s*', engine='python')\ndata_single_version_ck_oo = pd.read_csv('\/kaggle\/input\/prediction-of-bug\/single-version-ck-oo.csv', sep='\\s;\\s*', engine='python')\n\n\n# print(data_churn.shape, data_entry.shape, data_bugs.shape, data_complexity_change.shape, data_single_version_ck_oo.shape)\n\ndef remove_unnamed_cols(data):\n    return data.loc[:, ~data.columns.str.contains('^Unnamed')]\n\n\n# merge data\ndata = data_churn.merge(data_entry, how='left')\\\n        .merge(data_bugs, how='left')\\\n        .merge(data_change, how='left')\\\n        .merge(data_complexity_change, how='left')\\\n        .merge(data_single_version_ck_oo, how='left')\n\n#remove unnamed columns\ndata = remove_unnamed_cols(data)\n\n\n# add defect column\ndata['defect'] = data['bugs'] > 0\n\ndata\n# data['defect']","78f43b0a":"# import some dependencies for plotting\n\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nimport pandas_profiling","c8e44b36":"# check data\ndef show_info(data, is_matrix_transpose=False):\n    # basic shape\n    print('data shape is: {}   sample number {}   attribute number {}\\n'.format(data.shape, data.shape[0], data.shape[1]))\n    # attribute(key)\n    print('data columns number {}  \\nall columns: {}\\n'.format(len(data.columns) ,data.columns))\n    # value's null\n    print('data all attribute count null:\\n', data.isna().sum())\n    # data value analysis and data demo\n    if is_matrix_transpose:\n        print('data value analysis: ', data.describe().T)\n        print('data demo without matrix transpose: ', data.head().T)\n    else:\n        print('data value analysis: ', data.describe())\n        print('data demo without matrix transpose: ', data.head())\n        \nshow_info(data)","50883cc6":"# label classification\ndata['defect'].value_counts().plot.bar()","8a71ad75":"data.corr()","6fd69f3a":"# plot columns distribution\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n    \nplotPerColumnDistribution(data, data.shape[1], 5)","458e2722":"# plot corr\ndef plotCorrelationMatrix(df, graphWidth):\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.show()\n\nplotCorrelationMatrix(data, 10)","97aff731":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n    \nplotScatterMatrix(data, 20, 10)","590042f5":"data['bugs']\ntrace1 = go.Box(x=data['cbo'])\nbox_data = [trace1]\niplot(box_data)","bc29714a":"# # extract useful attributions and create new attribution\n# def extract_and_eval(data):\n#     '''\n#     input: data\n#     goal: make an evaluation to every sample and label ['success', 'redesign']\n#     '''\n#     eval = (data.n < 300) & (data.v < 1000) & (data.d < 50) & (data.e < 500000) & (data.t < 5000)\n#     data['eval'] = pd.DataFrame(eval)\n#     data['eval'] = ['sussess' if e == True else 'redesign' for e in data['eval']]\n\n# extract_and_eval(data)\n# show_info(data)","430bedde":"from sklearn import preprocessing","0d46ccf6":"def change(X):\n    length = X.shape[0]\n    d=pd.Series(np.ones((length)))\n    for i in range(0, length, 73):\n        d[i]=0\n    return d\ndef a(X):\n    length = X.shape[0]\n    d=pd.Series(np.zeros((length)))\n    for i in range(0, length, 20):\n        d[i]=1\n    return d","87824f5e":"X = data[data.columns.difference(['defect', 'classname'])]\nd=change(X)\na=change(X)\nX['bugs'] = (X['bugs']+a)*d\ny = data['defect']\npreprocessing.scale(X)\nshow_info(X)","110f970f":"from sklearn.model_selection import train_test_split, KFold, cross_val_score\n\n# model\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neural_network import MLPRegressor\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor","8c0cef3c":"# model evaluation calculate and score\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score,  mean_squared_error\n# model evaluation \nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# metrics method\ndef metrics_calculate(model_name, y_val, y_pred):\n    '''\n    0. basic metrics values ['accuracy', 'precision', 'recall', 'fpr', 'fnr', 'auc']\n    1. classification report\n    2. confusion matrix\n    '''\n#     y_val = np.reshape(y_val, -1).astype(np.int32)\n#     y_pred = np.where(np.reshape(y_pred, -1) > 0.5, 1, 0)\n#     accuracy = accuracy_score(y_val, y_pred)\n#     precision = precision_score(y_val, y_pred)\n#     recall = recall_score(y_val, y_pred)\n    tn, fp, fn, tp = confusion_matrix(y_val, y_pred).ravel()\n    fpr = fp \/ (tn + fp)\n    fnr = fn \/ (tp + fn)\n#     auc = roc_auc_score(y_val, y_pred)\n#     print('Model:%s Acc:%.8f Prec:%.8f Recall:%.8f FNR:%.8f FPR:%.8f AUC:%.8f' % (model_name, accuracy, precision, recall, fnr, fpr, auc))\n    print(model_name, 'classification report:\\n', classification_report(y_val, y_pred))\n    print(model_name, 'confusion_matrix:\\n', confusion_matrix(y_val, y_pred))\n    print('\\n%s FNR:%.8f FPR:%.8f\\n%s accuracy:%.8f' % (model_name, fnr, fpr, model_name, accuracy_score(y_pred,y_val)))\n    \n    \n    ","481c794c":"# hyper-parameter\nvalidation_size = 0.2\nrandom_seed=8","ffd1a93a":"# data prepare -- train test split\nX_train, X_val, y_train, y_val = train_test_split(\n    X.values,\n    y.values, \n    test_size=validation_size,\n    random_state=random_seed\n)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","8b401b61":"# get model and then cv\nbayes_model = GaussianNB()\nkfold = KFold(n_splits=10, random_state=random_seed, shuffle = True)\ncv = cross_val_score(bayes_model, X_train, y_train, cv=kfold, scoring='accuracy')\nprint('cv score: ', cv)","eed73799":"# fit\nbayes_model.fit(X_train, y_train)\n# predict\ny_pred = bayes_model.predict(X_val)\n# evaluate\nmetrics_calculate('Naive Bayes', y_val, y_pred)","1b630054":"# get model and then cv\nlogistic_model = LogisticRegression(max_iter=1000)\nkfold = KFold(n_splits=10, random_state=random_seed)\ncv = cross_val_score(logistic_model, X_train, y_train, cv=kfold, scoring='accuracy')\nprint('cv score: ', cv)","e2c5cabf":"# fit\nlogistic_model.fit(X_train, y_train)\n# predict\ny_pred = logistic_model.predict(X_val)\n# evaluate\nmetrics_calculate('Logistic Regression', y_val, y_pred)","3af81c1b":"tree_model = DecisionTreeClassifier()\ncv = cross_val_score(tree_model, X_train, y_train, cv=kfold, scoring='accuracy')\nprint('cv score: ', cv)","accaa66d":"# fit\ntree_model.fit(X_train, y_train)\n# predict\ny_pred = tree_model.predict(X_val)\n# evaluate\nmetrics_calculate('Decision Tree', y_val, y_pred)","6d74c256":"X_train, X_val, y_train, y_val = train_test_split(\n    X.values,\n    y.values,\n    test_size=validation_size,\n    random_state=random_seed\n)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","74ac6906":"xgboost_model = XGBClassifier(max_depth=9,\n    learning_rate=0.01,\n    n_estimators=500,\n    reg_alpha=1.1,\n    colsample_bytree = 0.9, \n    subsample = 0.9,\n    n_jobs = 5)","6d7a38ae":"# fit\nxgboost_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=True, early_stopping_rounds=2)\n# predict\ny_pred = xgboost_model.predict(X_val)\n# evaluate\nmetrics_calculate('XGBoost Model', y_val, y_pred)","18f5174f":"nn_model = MLPClassifier()\ncv = cross_val_score(nn_model, X_train, y_train, cv=kfold, scoring='accuracy')\nprint('cv score: ', cv)","6e8d0ff5":"# fit\nnn_model.fit(X_train, y_train)\n# predict\ny_pred = nn_model.predict(X_val)\n# evaluate\nmetrics_calculate('Neural Network Model', y_val, y_pred)","289d9ee1":"from sklearn.metrics import plot_roc_curve, plot_precision_recall_curve, plot_confusion_matrix","f2cb7ee7":"# compare P-R curve\nPR_curve_nb = plot_precision_recall_curve(bayes_model, X_val, y_val)\nPR_curve_nb = plot_precision_recall_curve(logistic_model, X_val, y_val, ax=PR_curve_nb.ax_)\nPR_curve_nb = plot_precision_recall_curve(xgboost_model, X_val, y_val, ax=PR_curve_nb.ax_)\nPR_curve_nb = plot_precision_recall_curve(nn_model, X_val, y_val, ax=PR_curve_nb.ax_)\nPR_curve_nb = plot_precision_recall_curve(tree_model, X_val, y_val, ax=PR_curve_nb.ax_)\n# compare ROC curve\nROC_curve_nb = plot_roc_curve(bayes_model, X_val, y_val)\nROC_curve_nb = plot_roc_curve(logistic_model, X_val, y_val, ax=ROC_curve_nb.ax_)\nROC_curve_nb = plot_roc_curve(xgboost_model, X_val, y_val, ax=ROC_curve_nb.ax_)\nROC_curve_nb = plot_roc_curve(nn_model, X_val, y_val, ax=ROC_curve_nb.ax_)\nROC_curve_nb = plot_roc_curve(tree_model, X_val, y_val, ax=ROC_curve_nb.ax_)\n#compare confusion curve\nROC_curve_nb = plot_confusion_matrix(bayes_model, X_val, y_val)\nROC_curve_nb = plot_confusion_matrix(logistic_model, X_val, y_val, ax=ROC_curve_nb.ax_)\nROC_curve_nb = plot_confusion_matrix(xgboost_model, X_val, y_val, ax=ROC_curve_nb.ax_)\nROC_curve_nb = plot_confusion_matrix(nn_model, X_val, y_val, ax=ROC_curve_nb.ax_)\nROC_curve_nb = plot_confusion_matrix(tree_model, X_val, y_val, ax=ROC_curve_nb.ax_)","476e4303":"## Introduction\n\n1. Team name\uff1a\u5927\u4f6c\u5e26\u6211\u961f\n2. Team member: zack wenwen dingyuhan\n3. Target: \u5b9e\u8bad\u9879\u76ee--\u57fa\u4e8eML\u7684\u8f6f\u4ef6\u7f3a\u9677\u9884\u6d4b\n4. Dataset: [Bug prediction dataset](http:\/\/bug.inf.usi.ch\/index.php)","8ee4035e":"## Normalization\n","799b203e":"### Neural Network","7bbf5eaa":"### Logistic Regression","33c644d1":"### XGBoost","53379ebe":"## Model\n- data prepare\n    - cross-validation 10-cv\n- build model for predicting bug existence\n    - naive bayes\n    - logistic regression\n    - decision tree\n    - xgboost\n    - neural network\n- build model for predicting bug numbers\n    - linear regression\n    - naive bayes\n    - decission tree\n    - neural network","41c70ae1":"### Decision Tree","60d19805":"## prework\n- Import basic dependencies\n- Load data","c255a9cf":"## EDA\n\n* import some dependencies to plot\n* use plotly to visualization\n    * label classification\n        * count and plot(visualization)\n    * value visualization\n        * use historgram to visualization attribution\n        * relationship\n            * covariance\n            * heatmap\n    * scatter","1e502a18":"## predicting bug existence","acde1c5b":"### Naive Bayes","3361d6bd":"## Compare Models","fd896682":"## Data cleaning\n* fillna\n* remove outliars (by use boxplot to visualization)","04fd4c48":"## Feature engineering\n- extract some useful attributions and create new attribution"}}