{"cell_type":{"52ecbdae":"code","a3363758":"code","53dd525d":"code","041dd523":"code","fec09b44":"code","3977a790":"code","27525743":"code","66b69d18":"code","5d45af97":"code","2fc683d5":"code","fb9fd741":"code","472c67b9":"code","dd3f3290":"code","61d8d05c":"code","302c3789":"code","afe50215":"code","b3c70e3a":"code","ac2fd7c4":"code","df14230a":"code","c7b09789":"code","41956d37":"code","273bce9e":"code","96c4cae0":"code","451149bd":"code","fb21af4d":"code","6498ba51":"code","75080e15":"code","9ddd5a6d":"code","d90a9064":"code","31c0a041":"markdown"},"source":{"52ecbdae":"import numpy as np\nimport pandas as pd\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport cv2\nfrom tqdm import tqdm\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split","a3363758":"directory = '..\/input\/pins-face-recognition\/105_classes_pins_dataset'","53dd525d":"Name=[]\nfor file in os.listdir(directory):\n    Name+=[file[5:]]\nprint(Name)\nprint(len(Name))","041dd523":"N=list(range(len(Name)))\nnormal_mapping=dict(zip(Name,N)) \nreverse_mapping=dict(zip(N,Name)) ","fec09b44":"!pip install mtcnn\nfrom mtcnn.mtcnn import MTCNN","3977a790":"detector = MTCNN()","27525743":"def mtcnn_detector(image): \n    face_location = detector.detect_faces(image)\n    for face in zip(face_location): \n        x_coordinate,y_coordinate,width,height=face[0]['box']\n        image=image[(y_coordinate):(y_coordinate+height),(x_coordinate):(x_coordinate+width)]\n    return image","66b69d18":"path0='..\/input\/pins-face-recognition\/105_classes_pins_dataset\/pins_Adriana Lima\/Adriana Lima106_8.jpg'\nimage=cv2.imread(path0)\nprint(image.shape)\nplt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))","5d45af97":"image2=mtcnn_detector(image) \nprint(image2.shape)\nplt.imshow(cv2.cvtColor(image2, cv2.COLOR_BGR2RGB))","2fc683d5":"image3=cv2.resize(image2,(60,60))\nprint(image3.shape)\nplt.imshow(cv2.cvtColor(image3, cv2.COLOR_BGR2RGB))","fb9fd741":"dataset=[]\ndatacount=[]\ncount=0\nfor file in tqdm(os.listdir(directory)):\n    path=os.path.join(directory,file)\n    for im in os.listdir(path):\n        image=cv2.imread(os.path.join(path,im))\n        image2=mtcnn_detector(image)\n        if image2.shape[0]>10 and image2.shape[1]>10:\n            image3=cv2.resize(image2,(60,60))\n            dataset+=[image3]\n            datacount+=[count]\n    count+=1","472c67b9":"n=len(dataset)\nprint(n)\nD=list(range(n))\nrandom.seed(2021)\nrandom.shuffle(D)","dd3f3290":"trainY=np.array(datacount)[D[0:(n\/\/4)*3]]\ntestY=np.array(datacount)[D[(n\/\/4)*3:]]\ntrainX=np.array(dataset)[D[0:(n\/\/4)*3]]\ntestX=np.array(dataset)[D[(n\/\/4)*3:]]","61d8d05c":"print(trainX.shape)\nprint(testX.shape)\nprint(trainY.shape)\nprint(testY.shape)","302c3789":"labels1=to_categorical(trainY)\ntrainY2=np.array(labels1)","afe50215":"tlabels1=to_categorical(testY)\ntestY2=np.array(tlabels1)","b3c70e3a":"trainx,testx,trainy,testy=train_test_split(trainX,trainY2,test_size=0.2,random_state=44)","ac2fd7c4":"print(trainx.shape)\nprint(testx.shape)\nprint(trainy.shape)\nprint(testy.shape)","df14230a":"datagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rotation_range=20,zoom_range=0.2,\n                        width_shift_range=0.2,height_shift_range=0.2,shear_range=0.1,fill_mode=\"nearest\")","c7b09789":"pretrained_model3 = tf.keras.applications.DenseNet201(input_shape=(60,60,3),include_top=False,weights='imagenet',pooling='avg')\npretrained_model3.trainable = False","41956d37":"inputs3 = pretrained_model3.input\nx3 = tf.keras.layers.Dense(105, activation='relu')(pretrained_model3.output)\noutputs3 = tf.keras.layers.Dense(len(Name), activation='softmax')(x3)\nmodel = tf.keras.Model(inputs=inputs3, outputs=outputs3)","273bce9e":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","96c4cae0":"his=model.fit(datagen.flow(trainx,trainy,batch_size=32),validation_data=(testx,testy),epochs=300)","451149bd":"y_pred=model.predict(testX)\npred=np.argmax(y_pred,axis=1)\nground=testY\nprint(classification_report(ground,pred))","fb21af4d":"get_acc = his.history['accuracy']\nvalue_acc = his.history['val_accuracy']\nget_loss = his.history['loss']\nvalidation_loss = his.history['val_loss']\n\nepochs = range(len(get_acc))\nplt.plot(epochs, get_acc, 'r', label='Accuracy of Training data')\nplt.plot(epochs, value_acc, 'b', label='Accuracy of Validation data')\nplt.title('Training vs validation accuracy')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","6498ba51":"epochs = range(len(get_loss))\nplt.plot(epochs, get_loss, 'r', label='Loss of Training data')\nplt.plot(epochs, validation_loss, 'b', label='Loss of Validation data')\nplt.title('Training vs validation loss')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","75080e15":"pred2=model.predict(testX)\n\nPRED=[]\nfor item in pred2:\n    value2=np.argmax(item)      \n    PRED+=[value2]","9ddd5a6d":"ANS=testY","d90a9064":"accuracy=accuracy_score(ANS,PRED)\nprint(accuracy)","31c0a041":"# Face Detect and Extract"}}