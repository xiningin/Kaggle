{"cell_type":{"7f7427a6":"code","1fcd0a86":"code","bd41d99b":"code","be95d1b4":"code","db46c526":"code","61d754ed":"code","40c2ce38":"code","38953f0c":"code","0c7f3618":"code","a972a4c4":"code","ddc1bf34":"code","e782bfb5":"code","865e9f52":"code","ddd143ac":"code","a92c46ad":"code","043a5207":"code","b0d6b83e":"code","17c16727":"code","8d3eb938":"code","3576fc72":"code","9d1e2725":"code","26c1acbd":"code","0bfb0232":"code","3677fe44":"code","b5a8fb6a":"code","057e2044":"code","919af954":"code","c8dd3cf0":"code","2d9f726b":"code","50ed593d":"code","5b56b11b":"code","167d658f":"code","e6f62d4d":"code","3637331b":"code","1baf7074":"code","ecfbcd82":"code","9d3548df":"code","1be390df":"code","fc6e11b8":"code","51ec4ca2":"code","126e81cd":"code","49b5d555":"code","770b9f6e":"code","00803e6a":"code","83731e98":"code","4e1125d4":"code","b64c2bdc":"code","98d26454":"code","06d1b0dc":"code","ebf38916":"code","c52cb05f":"code","d57d635d":"markdown","860d6313":"markdown","58f2fd92":"markdown","ed5da991":"markdown","2662c63f":"markdown","3c645c5f":"markdown","309667da":"markdown","8b1cd0d2":"markdown","1e0309d2":"markdown","6cf5723b":"markdown","d217debb":"markdown","cc95fcd9":"markdown","bdff78f4":"markdown","8fb01938":"markdown","98fca3e0":"markdown","9b94dd54":"markdown","680cc4e0":"markdown","044e892c":"markdown","d41d1420":"markdown","a007914c":"markdown","a1c5150c":"markdown","80b790d2":"markdown","d6c2db49":"markdown","5c95e697":"markdown","4634d5c6":"markdown","c4236f09":"markdown","5b614b52":"markdown","e22fdbee":"markdown","dec81e5a":"markdown","21177e36":"markdown","341cf218":"markdown","b5f38712":"markdown","ed44fa57":"markdown","40f20fc8":"markdown","8f40da94":"markdown","329fa34b":"markdown","2b8c05a0":"markdown","3bd0972a":"markdown","f553bb5f":"markdown"},"source":{"7f7427a6":"#essentials\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport pickle\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n#tools\/metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.model_selection import train_test_split\nimport shap\n\n#modeling\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n#pandas show all columns\npd.set_option('display.max_columns', None)\n\n#%load_ext autoreload\n#%autoreload 2\n#%run .\/__init__","1fcd0a86":"master_table = pd.read_csv('\/kaggle\/input\/nba-mvp-candidates-1980-2022\/master_table.csv')","bd41d99b":"master_table.head()","be95d1b4":"master_table.info()","db46c526":"#correlations of each features\ncorr_matrix = master_table.corr()\n\n#plot heat map\nmask = np.zeros_like(corr_matrix)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(15, 15))\n    ax = sns.heatmap(corr_matrix, mask=mask, vmax=.3, square=True,cmap=\"RdYlGn\")\n","61d754ed":"def calculate_ml_scores(df):\n    X = df.copy()\n    y = X[\"Share\"]\n\n    X.drop('Share', axis=1, inplace=True)\n\n    # Label encoding for categoricals\n    for colname in X.select_dtypes(\"object\"):\n        X[colname], _ = X[colname].factorize()\n\n    # All discrete features should now have integer dtypes (double-check this before using MI!)\n    discrete_features = X.dtypes == int\n    \n    mi_scores = mutual_info_regression(X, y)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return X, y, mi_scores","40c2ce38":"#drop columns for mutual information\nto_drop_mi = ['Rank','Player','Age','year','Tm','team','First','Pts Won','Pts Max','WS','WS\/48']\nmaster_table_mi = master_table.copy()\nmaster_table_mi.drop(to_drop_mi, axis=1, inplace=True)","38953f0c":"X, y, mi_scores = calculate_ml_scores(df=master_table_mi)","0c7f3618":"def plot_mi_scores(scores, figsize):\n    scores = scores.sort_values(ascending=True)\n    width = np.arange(len(scores))\n    ticks = list(scores.index)\n    \n    fig, ax = plt.subplots(figsize=figsize)\n    ax.barh(width, scores)\n    \n    for index, value in enumerate(scores):\n        plt.text(value +0.005 , index, str(round(value,2)))\n    \n    plt.yticks(width, ticks)    \n    plt.title(\"Mutual Information Scores\")\n\nplot_mi_scores(mi_scores, figsize=(14,11))","a972a4c4":"def add_win_lose_col(df):\n    rank_lst = []\n    for i in list(df['Rank']):\n        if i == '1':\n            rank_lst.append('won')\n        else:\n            rank_lst.append('lost')\n    master_table_rank = df.copy()\n    master_table_rank['Win\/Lose'] = rank_lst\n    return master_table_rank","ddc1bf34":"def show_feature_vs_share(feature, df):\n    fig = px.scatter(data_frame = df,\n               x=feature,\n               y='Share',\n               color='Win\/Lose',\n               color_discrete_sequence=['blue','gray'], \n               hover_data={\n                   'Win\/Lose': False,\n                   'Player': True, \n                   'year': True,\n                   'seed': True,\n                   'W\/L%': True, \n                   'W': True\n                   \n               })\n    fig.update_layout(height=500,\n                     title = f\"{feature} vs. MVP share\")\n    fig.show()","e782bfb5":"features = ['win_shares', \n            'player_efficiency_rating',\n            'value_over_replacement_player',\n            'box_plus_minus',\n            'offensive_box_plus_minus',\n            'usage_percentage',\n            'seed',\n            'W',\n            'W\/L%',\n            'PTS']\n\nmaster_table_rank = add_win_lose_col(df=master_table)\n\nfor feature in features:\n    show_feature_vs_share(feature=feature, df=master_table_rank)","865e9f52":"#drop columns \n\nto_drop = [\n    'Rank',\n    'Player',\n    'Age',\n    'year',\n    'Tm',\n    'team',\n    'First',\n    'Pts Won',\n    'Pts Max',\n    'WS\/48',\n    'WS',\n    'MP',\n    'G',\n    'W', \n    'FG%',\n    '3P%',\n    'STL', \n    'BLK',\n    'three_point_attempt_rate',\n    'total_rebound_percentage',\n    'offensive_rebound_percentage',\n    'block_percentage',\n    'defensive_rebound_percentage',\n    'steal_percentage',\n    'turnover_percentage',\n    'assist_percentage',\n    'AST',\n    'TRB',\n    'FT%',\n    'win_shares', \n    'box_plus_minus', \n    'defensive_box_plus_minus',\n    'offensive_win_shares', \n    'defensive_win_shares', \n    'true_shooting_percentage' \n]","ddd143ac":"#run another Mutual Information Score analysis\nmaster_table_mi2 = master_table.copy()\nmaster_table_mi2.drop(to_drop, axis=1, inplace=True)\nX, y, mi_scores2 = calculate_ml_scores(df=master_table_mi2)\nplot_mi_scores(mi_scores2, figsize=(14,4))","a92c46ad":"def train_test_split_by_year(year, df, scaling=False):\n    #test year = selected year, train year = other years outside of selected year\n    train_df = df[df['year'] != year]\n    test_df = df[df['year'] == year]\n    \n    train_df2 = train_df.copy()\n    test_df2 = test_df.copy()\n    \n    train_df2.drop(to_drop, axis=1, inplace=True)\n    test_df2.drop(to_drop, axis=1, inplace=True)\n    \n    if scaling == True:\n        sc_X = StandardScaler()\n        sc_y = StandardScaler()\n        train_df2 = sc_X.fit_transform(train_df2)\n        test_df2 = sc_y.fit_transform(test_df2)\n    \n    X_train = train_df2.copy()\n    y_train = X_train[\"Share\"]\n    \n    X_test = test_df2.copy()\n    y_test = X_test[\"Share\"]\n\n    X_train.drop('Share', axis=1, inplace=True)\n    cols = X_train.columns\n    X_test.drop('Share', axis=1, inplace=True)\n\n    X_train = np.array(X_train)\n    y_train = np.array(y_train)\n    X_test = np.array(X_test)\n    y_test = np.array(y_test)\n    \n    return X_train, y_train, X_test, y_test, cols","043a5207":"def run_model(regressor, X_train, y_train, X_test, y_test, df, year):\n    model = regressor\n    model.fit(X_train, y_train) \n    predictions = model.predict(X_test)\n    mae = mean_absolute_error(predictions, y_test)\n    r2 = r2_score(y_test, predictions)\n    \n    mvp_race = df[df['year'] == year]\n    mvp_race['predicted_share'] = predictions\n    mvp_race = mvp_race.sort_values([\"Share\", \"predicted_share\"], ascending = (False, False))\n                                    \n    actual_winner = mvp_race[mvp_race['Share'] == mvp_race['Share'].max()]['Player']\n    predicted_winner = mvp_race[mvp_race['predicted_share'] == mvp_race['predicted_share'].max()]['Player']\n    return model, mae, r2, predicted_winner.iloc[0], actual_winner.iloc[0], mvp_race","b0d6b83e":"years = [year for year in range(1980, 2022)]\n\ndef run_model_average(df, regressor, scaling=False, print_metrics=False):\n    mae_lst = []\n    r2_lst = []\n    predicted_lst = []\n    actual_lst = []\n    label_lst =[]\n    model_lst = []\n    for year in tqdm(years):\n        X_train, y_train, X_test, y_test, cols = train_test_split_by_year(year=year, df=df, scaling=False)\n        model, mae, r2, predicted_winner, actual_winner, mvp_race = run_model(regressor,\n                                                             X_train,\n                                                              y_train,\n                                                              X_test,\n                                                              y_test,\n                                                              df=df,\n                                                              year=year,\n                                                            )\n        if predicted_winner == actual_winner:\n            label = 'correct'\n        else:\n            label = 'incorrect'\n        mae_lst.append(mae)\n        r2_lst.append(r2)\n        predicted_lst.append(predicted_winner)\n        actual_lst.append(actual_winner)\n        label_lst.append(label)\n        model_lst.append(model)\n    d = {\n    'year': years,\n    'MAE': mae_lst,\n    'R squared': r2_lst,\n    'Predicted MVP': predicted_lst,\n    'Actual MVP': actual_lst,\n    'Label': label_lst\n    }\n\n    summary = pd.DataFrame(d)\n    correct_count = summary['Label'].value_counts().iloc[0]\n    incorrect_count = summary['Label'].value_counts().iloc[1]\n    accuracy = correct_count \/ (correct_count + incorrect_count)\n    avg_mae = summary['MAE'].mean()\n    avg_r2  = summary['R squared'].mean()\n    \n    if print_metrics == True:\n        print(f\"Average MAE: {avg_mae}\")\n        print(f\"Average R squared: {avg_r2}\")\n        print(f\"Prediction accuracy: {accuracy}\")\n    return avg_mae, avg_r2, accuracy, summary, model_lst, cols","17c16727":"lr_avg_mae, lr_avg_r2, lr_accuracy, lr_summary, lr_models, cols = run_model_average(df=master_table,\n                  regressor = LinearRegression(),\n                 scaling=True,\n                print_metrics=True)","8d3eb938":"#display feature importance for tree algorithms (RF, XGB, LGBM\ndef avg_feature_importance(models, cols):\n    lst = []\n    for model in models:\n        feature_importance = list(model.feature_importances_)\n        lst.append(feature_importance)\n        \n    df = pd.DataFrame(lst, columns=cols)\n    mean_features = df.mean()\n    \n    #df2 = pd.DataFrame([cols ,mean_features], columns=['Feature', 'Feature Importance'])\n    df2 = pd.DataFrame([cols ,mean_features]).T\n    df2 = df2.rename(columns={0:'Feature', 1:'Score'}).sort_values(by='Score', ascending=False)\n    \n    #plt.rcParams[\"figure.figsize\"] = (7,4)\n    plt.title('Feature Importance Score')\n    sns.barplot(x='Score',\n                y= 'Feature',\n               data=df2,\n                  )\n    plt.show()","3576fc72":"rf_avg_mae, rf_avg_r2, rf_accuracy, rf_summary, rf_models, rf_cols = run_model_average(df=master_table,\n                  regressor=RandomForestRegressor(n_estimators = 23, \n                                                  random_state = 0, \n                                                  max_depth=7, \n                                                  min_samples_leaf=1,\n                                                  min_samples_split=2),\n                print_metrics=True)","9d1e2725":"avg_feature_importance(models=rf_models, cols=rf_cols)","26c1acbd":"#BEST MODEL\n# 16 5, 0.2745\n\nxgb_avg_mae, xgb_avg_r2, xgb_accuracy, xgb_summary, xgb_models, xgb_cols = run_model_average(df=master_table,\n                  regressor = XGBRegressor(n_estimators=16, max_depth=5, learning_rate = 0.2745, subsample=1, colsample_bytree=1),\n                 scaling=False, print_metrics=True)","0bfb0232":"xgb_summary","3677fe44":"avg_feature_importance(models=xgb_models, cols=xgb_cols)","b5a8fb6a":"lgbm_avg_mae, lgbm_avg_r2, lgbm_accuracy, lgbm_summary, lgbm_models, lgbm_cols = run_model_average(df=master_table,\n                  regressor = LGBMRegressor(n_estimators=23,\n                                            max_depth=4,\n                                            learning_rate=0.15,\n                                            num_leaves=28,\n                                            boosting_type='goss',\n                                            random_state = 0,\n                                           ),\n                 scaling=False, print_metrics=True)\n","057e2044":"avg_feature_importance(models=lgbm_models, cols=lgbm_cols)","919af954":"d = {\n    'Model': ['Linear Regression', 'Random Forest Regressor', 'XGBoost Regressor', 'LGBM Regressor'],\n    'average MAE': [lr_avg_mae,rf_avg_mae, xgb_avg_mae, lgbm_avg_mae],\n    'average R squared': [lr_avg_r2,rf_avg_r2, xgb_avg_r2, lgbm_avg_r2],\n    'accuracy': [lr_accuracy,rf_accuracy, xgb_accuracy, lgbm_accuracy],\n}\nmodel_summary_df = pd.DataFrame(d)\nmodel_summary_df.style.highlight_max(subset = ['average R squared', 'accuracy'],\n                       color = 'lightgreen', axis = 0)","c8dd3cf0":"#all models in models list have same parameters\nbest_xgb_model = xgb_models[0]\nbest_rf_model = rf_models[0]\nbest_lgbm_model = lgbm_models[0]","2d9f726b":"def validate_year(year):\n    X_train, y_train, X_test, y_test, cols = train_test_split_by_year(year, df=master_table, scaling=False)\n    model, mae, r2, predicted_winner, actual_winner, mvp_race = run_model(best_xgb_model,\n                                              X_train, y_train, X_test, y_test, df=master_table, year=year)\n    # shift column 'Name' to first position\n    nineth_column = mvp_race.pop('predicted_share')\n    # first_column) function\n    mvp_race.insert(8, 'predicted_share', nineth_column)\n    mvp_race = mvp_race.reset_index(drop=True)\n\n    X_test_df = pd.DataFrame(columns=cols, data = X_test)\n    \n    print(f'Predicted: {predicted_winner}')\n    print(f'Actual: {actual_winner}')\n    avg_feature_importance(models=[model], cols=cols)\n    \n    return model, X_test_df, mvp_race #mvp_race.style.highlight_max(subset = ['Share', 'predicted_share'], color = 'lightgreen', axis = 0)","50ed593d":"def visualize_shap_values(mvp_race, model):\n    top_candidates = list(mvp_race.head(3)['Player'])\n\n    for idx, player in enumerate(top_candidates):\n        data_for_prediction = mvp_race[mvp_race['Player'] == player]\n        data_for_prediction = data_for_prediction[list(xgb_cols)]\n        data_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n        rank = idx + 1\n        print(f\"Rank: {rank}: {player}\")\n\n        # Create object that can calculate shap values\n        explainer = shap.TreeExplainer(model)\n        # Calculate Shap values\n        shap_values = explainer.shap_values(data_for_prediction_array)\n        shap.initjs()\n        display(shap.force_plot(explainer.expected_value, shap_values, data_for_prediction))","5b56b11b":"model, X_test_df, mvp_race = validate_year(year=2011)","167d658f":"visualize_shap_values(mvp_race, model)","e6f62d4d":"#data to be forecasted: 2022 mvp candidates from NBA's MVP ladder\ndata_2022 = pd.read_csv('\/kaggle\/input\/nba-mvp-candidates-1980-2022\/data_2022.csv')\ndata_2022_cleaned = data_2022.copy()\ndata_2022_cleaned = data_2022_cleaned[list(xgb_cols)]","3637331b":"def prep_train_test(df):\n    #train; using hitorical data from 1980 - 2021\n    forecast_X_train_df = df.copy()\n    forecast_X_train_df.drop(to_drop, axis=1, inplace=True)\n    forecast_y_train_df = forecast_X_train_df['Share']\n    forecast_X_train_df.drop(['Share'], axis=1, inplace=True)\n    \n    #data to be forecasted: 2022 mvp candidates from NBA's MVP ladder\n    forecast_X_test_df = data_2022_cleaned\n    \n    print(f'Training dataset columns: \\n{list(forecast_X_train_df.columns)} \\n')\n    print(f'Forecasting dataset columns: \\n{list(forecast_X_test_df.columns)}')\n    return forecast_X_train_df, forecast_y_train_df, forecast_X_test_df","1baf7074":"def fit_forecast_model(regressor):\n    model = regressor\n    model.fit(forecast_X_train_df, forecast_y_train_df) \n    predictions = model.predict(forecast_X_test_df)\n    \n    mvp_race_forecast = data_2022.copy()\n    mvp_race_forecast['Share Prediction'] = predictions\n    mvp_race_forecast = mvp_race_forecast.sort_values([\"Share Prediction\"], ascending = (False))\n    \n    mvp_race_forecast_sub = mvp_race_forecast[[\n                                                'Player',\n                                                'Share Prediction',\n                                                'PTS',\n                                               'value_over_replacement_player',\n                                               'seed',\n                                                'W\/L%',\n                                               'player_efficiency_rating',\n                                               'win_shares_per_48_minutes',\n                                                'offensive_box_plus_minus',\n                                                'usage_percentage',\n                                            'free_throw_attempt_rate'\n                                              ]].reset_index(drop=True)\n    mvp_race_forecast_sub.head()\n    avg_feature_importance(models=[model], cols=data_2022_cleaned.columns)\n    return model, mvp_race_forecast_sub","ecfbcd82":"def show_highlighted_df(df):\n    return df.style.highlight_max(subset = ['value_over_replacement_player',\n                                                    'player_efficiency_rating',\n                                                    'W\/L%',\n                                                    'win_shares_per_48_minutes',\n                                                    'usage_percentage',\n                                                    'free_throw_attempt_rate',\n                                                    'offensive_box_plus_minus',\n                                                    'PTS',\n                                                    'Share Prediction'], color = 'lightgreen', axis = 0)","9d3548df":"#train on historical data, predict on 2022 data\nforecast_X_train_df, forecast_y_train_df, forecast_X_test_df = prep_train_test(df=master_table)","1be390df":"best_xgb_model","fc6e11b8":"#best xgb model\nxgb_model, xgb_mvp_race_forecast = fit_forecast_model(regressor = XGBRegressor(\n                                            n_estimators=16,\n                                            max_depth=5,\n                                            learning_rate=0.2745))\n\n# 16 5, 0.2745\n\n","51ec4ca2":"show_highlighted_df(df=xgb_mvp_race_forecast)","126e81cd":"visualize_shap_values(model= xgb_model, mvp_race=xgb_mvp_race_forecast)","49b5d555":"best_rf_model","770b9f6e":"#best random forest model\nrf_model, rf_mvp_race_forecast = fit_forecast_model(regressor = RandomForestRegressor(n_estimators = 23, \n                                                  random_state = 0, \n                                                  max_depth=7, \n                                                  min_samples_leaf=1,\n                                                  min_samples_split=2)\n                  )","00803e6a":"show_highlighted_df(df=rf_mvp_race_forecast)","83731e98":"visualize_shap_values(model = rf_model, mvp_race= rf_mvp_race_forecast)","4e1125d4":"best_lgbm_model","b64c2bdc":"#best LightGBM model\nlgbm_model, lgbm_mvp_race_forecast = fit_forecast_model(\n                                                regressor = LGBMRegressor(\n                                                                    n_estimators=23,\n                                                                    max_depth=4,\n                                                                    learning_rate=0.15,\n                                                                    num_leaves=28,\n                                                                    boosting_type='goss',\n                                                                    random_state = 0,\n                                           ))","98d26454":"show_highlighted_df(df=lgbm_mvp_race_forecast)","06d1b0dc":"visualize_shap_values(model = lgbm_model, mvp_race= lgbm_mvp_race_forecast)","ebf38916":"forecast_tables = [xgb_mvp_race_forecast, lgbm_mvp_race_forecast, rf_mvp_race_forecast]\nmodel_names = ['XGBoost', 'LightGBM', 'Random Forest']\n\ntables = []\nfor name, forecast_table in zip(model_names, forecast_tables):\n    player_sub = forecast_table.head(5)[['Player']]\n    player_sub['Rank'] = ['1st Place','2nd Place','3rd Place','4th Place','5th Place']\n    player_sub2 = player_sub.T.reset_index(drop=True)\n    player_sub2.columns = player_sub2.iloc[1]\n    player_sub2.drop(player_sub2.tail(1).index,inplace=True)\n\n    share_sub = forecast_table.head(5)[['Share Prediction']]\n    share_sub['Rank'] = ['1st Place Share','2nd Place Share','3rd Place Share', '4th Place Share', '5th Place Share']\n    \n    share_sub2 = share_sub.T.reset_index(drop=True)\n    share_sub2.columns = share_sub2.iloc[1]\n    share_sub2.drop(share_sub2.tail(1).index,inplace=True)\n\n    merged_df = pd.concat([player_sub2, share_sub2], axis=1).sort_index(axis=1)\n    merged_df['Model'] = name\n    tables.append(merged_df)\n    \nfinal_summary_table = pd.concat(tables)\n# shift column 'Name' to first position\nfirst_column = final_summary_table.pop('Model')\n# first_column) function\nfinal_summary_table.insert(0, 'Model', first_column)","c52cb05f":"final_summary_table","d57d635d":"Best Models: ","860d6313":"In order to evaluate a model for all seasons (where there is available relevant data), a \u201cshuffling\u201d inspired approach can be used. For example, for each finished season (from 1980 to 2021), the model is to be tested for data of one particular season and trained on data of other seasons that were not selected. This would repeat until data for all individual seasons have been tested and predicted to calculate overall accuracy.\n\nSince the actual MVP share for 1982 is known, it can be used to compare against the predicted MVP shares. Generally, MAE (Mean Absolute Error) and R\u00b2 (coefficient of determination) are some popular metrics to evaluate regression models. MAE is a measure of error between paired observations. R\u00b2 is the statistical measure of how well the data are fitted to a model, or how much variance\/uncertainty the model is able to explain. The formula for both metrics are shown below:\n\nEach iteration would take a single year as the test data and the other years that were not selected as the training data. Then each year\u2019s candidate who received the highest predicted MVP shares would be compared to the actual MVP (candidate who received the highest actual MVP shares). A label (correct\/incorrect) would be created for each year as shown below\n\nUsing these labels, an accuracy metric can be created.\n\n> **Accuracy** = (correct labels) \/ (correct labels + incorrect labels)\n\nThere are 42 total seasons from 1980 to 2021. If, for example, the model predicts MVP shares for players of all seasons and 39 of the MVP\u2019s are correctly identified with the highest predicted shares in their respective year, The accuracy of the model would be (39 \/ 42) = 92.8%.\nThe objective for this project is set. The primary goal is to build models that produce highest accuracy (most correct labels) and the secondary goal is to validate that these models have minimized average MAE (mean MAE for all years) and maximized R\u00b2 (mean R\u00b2 for all years).\n","58f2fd92":"### Model Helper Functions","ed5da991":"### Model 1: XGBoost","2662c63f":"# Objective \nIn this project, the objective is to build a NBA MVP prediction model and forecast the upcoming MVP for the current (2022) season. see this [Medium post](http:\/\/towardsdatascience.com\/predicting-the-next-nba-mvp-using-machine-learning-62615bfcff7) and this [Github Repository](http:\/\/https:\/\/github.com\/DavidYoo912\/nba_mvp_project) for full details\n\n## Data\nA combination of pandas HTML table scraping function and basketball reference scraping tool were utilized to pull raw data.\n* master_table.csv: contains statistics of historical MVP candidates\n* data_2022.csv: contains statistics of current season's MVP candidates\n\n","3c645c5f":"# NBA MVP Prediction","309667da":"Mutual information is a function measuring association between feature and target, which may seem similar to correlation which detects linear relationships. However, this metric is able to detect any kind of relationship (and not just linear).\n\nThe disclaimer on mutual information is that the features that score high do not always impact the model performance. The usefulness of mutual information can vary depending on type of model.","8b1cd0d2":"# Modeling <a class=\"anchor\" id=\"modeling\"><\/a>","1e0309d2":"# Exploratory Data Analysis <a class=\"anchor\" id=\"eda\"><\/a>\n","6cf5723b":"# Forecasting 2022 MVP <a class=\"anchor\" id=\"forecasting\"><\/a>","d217debb":"train, predict, calculate MAE & R squared, show actual vs. predicted in a dataframe","cc95fcd9":"#### Random Forest Regressor","bdff78f4":"### Mutual Information","8fb01938":"### Visualize significant features vs. MVP Shares","98fca3e0":"NOTE: **VORP** (value_over_replacement_player) metric for 2022 candidates has been adjusted as a projection considering the games left in the season. ","9b94dd54":"Some general features (x axis) were plotted against the MVP share (y axis). The number of team wins and team seeding are normally some crucial factors that determine the MVP discussion. Also, an advanced statistic called VORP (Value over Replacement Player: used to measure a player\u2019s overall contribution to the team) was plotted.\n\nScatter points represent each candidates from 1980\u20132021 with the actual MVP winners highlighted in blue\n\nAnother interesting (but maybe obvious) feature was the usage rate. The usage rate is an estimate of the percentage of the team plays that a player has been involved in.\n\nAt this point these variables may seem to have somewhat linear relationship with the MVP share metric. It could be valid to consider them as model features as the experiments are conducted.","680cc4e0":"#### XGBoost","044e892c":"#### MVP Prediction Summary","d41d1420":"<img src= \"https:\/\/miro.medium.com\/max\/1400\/1*RqhDhnB5dk3ftCCwA8pULA.jpeg\" alt =\"Titanic\" style='width: 600px;'>","a007914c":"# Preprocessing <a class=\"anchor\" id=\"preprocessing\"><\/a>","a1c5150c":"## Table of Contents\n* [Import data\/modules](#import-data)\n* [Exploratory Data Analysis](#eda)\n* [Preprocessing](#preprocessing)\n* [Modeling](#modeling)\n* [Forecasting 2022 MVP](#forecasting)","80b790d2":"### Train\/Test Split","d6c2db49":"#### LightGBM","5c95e697":"#### Linear Regression","4634d5c6":"### Validate specific year ","c4236f09":"### Models\n* Linear Regression\n* Random Forest Regressor\n* XGBoost Regressor\n* LightGBM Regressor\n\n(see parameter_tuning.ipynb for parameter tuning scripts)","5b614b52":"The following function can be used to check details on a specific year's MVP race along with its predictions from the model \n* adjust year to check out the year of the MVP race interested\n* see 'Share' for actual share from the specific year's MVP race\n* see 'predicted_share' for model's predicted share","e22fdbee":"#### Model Summaries","dec81e5a":"#### Model 3 LightGBM","21177e36":"test on selected year, train on all other years that weren't selected","341cf218":"Many features that contained repetitive information were removed. For example, the feature \u201cwin share\u201d is just a counting stat version of the \u201cwin shares per 48 minutes\u201d which is the rate of combined offensive and defensive contribution that a player makes to his team. One reason for selecting rate metrics over counting metrics is because the models will be used to forecast\/predict the MVP of the current 2022 season. Because this season is still ongoing, the counting stats are still incomplete for the year. That would not be much of an issue with the rate based metrics.\n\nOne exception to this is the value over replace player (VORP) metric. The feature just proved to be a powerful indicator during model building. However, because this is a counting statistic, the MVP candidates of the 2022 season will not have the complete values. For the sake of this project, this can be adjusted to be somewhat of a projection. This will be mentioned in detail in the forecasting section.","b5f38712":"find average metrics and overall accuracy","ed44fa57":"**NOTE** this is as of 1\/14\/2022\n\nInterestingly, all three models predicted Giannis Antetokoumpo to be the 2022 MVP. However, the sequence and the appearance of the MVP candidates vary between each model.","40f20fc8":"# Import data\/modules <a class=\"anchor\" id=\"import-data\"><\/a>\n","8f40da94":"### Read Data","329fa34b":"For the experiment, there was a back and forth iteration of feature section and modeling. Various combination of features are selected and are utilized as input for models. This process was repeated until reasonable models were built.\n\nThe tool of mutual information along with human intuition and domain knowledge were used to experiment with combination of features. After many iterations of trial and error, final features were the following:\n","2b8c05a0":"### Correlation Matrix","3bd0972a":"drop unnecessary or redundant features","f553bb5f":"### Model 2: Random Forest"}}