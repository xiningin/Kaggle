{"cell_type":{"f3c1827e":"code","22e0e9c6":"code","1c912986":"code","4217ce53":"code","ebed001a":"code","3a8c968e":"code","7fe6b977":"code","11f85d27":"code","391a129a":"code","d6fa34a7":"code","74f56bf6":"code","2e3d6647":"code","fa041f5a":"code","b206caa6":"code","70bc02ae":"code","ff9d0f54":"code","e8216d95":"code","a6e6b697":"code","e8acaf09":"code","ff96bd5b":"code","9c99ab45":"code","16bc3f66":"code","d71ec58f":"code","e84a7ead":"code","f64faf5b":"code","75278863":"code","038b9211":"code","6de9cc49":"code","e836d488":"code","ab2b33f9":"code","af688cba":"code","4796c964":"code","f4fd2664":"code","8f741514":"code","9781464a":"code","e8d6fb7d":"code","b27338d5":"code","04c904f2":"markdown","0a1a6f59":"markdown","aa60429f":"markdown","48e9db8b":"markdown","4bffe6c3":"markdown","2ee8238f":"markdown","ebc51b49":"markdown","e6c0e8e4":"markdown","73eaef70":"markdown","520ccbe3":"markdown","96255927":"markdown","5371bebd":"markdown","650a7e22":"markdown","ed00274c":"markdown","e986c331":"markdown"},"source":{"f3c1827e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","22e0e9c6":"pip install lifetimes --upgrade\n","1c912986":"\n# visualization\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# sns.set_style('whitegrid')\ncolor = sns.color_palette()\n\n\n%matplotlib inline\n\n\nimport lifetimes\n\n#Let's make this notebook reproducible \nnp.random.seed(42)\n\nimport random\nrandom.seed(42)\n\nimport warnings\nwarnings.filterwarnings('ignore')","4217ce53":"#Reading ecommerce data\ndata_file = '..\/input\/customer_segmentation\/customer_segmentation.csv'\necommerce_data =pd.read_csv(data_file , engine=\"python\" )","ebed001a":"ecommerce_data.head()","3a8c968e":"ecommerce_data.info()","7fe6b977":"#change the column names\n\necommerce_data.rename(index=str, columns={'InvoiceNo': 'invoice_num',\n                              'StockCode' : 'stock_code',\n                              'Description' : 'description',\n                              'Quantity' : 'quantity',\n                              'InvoiceDate' : 'invoice_date',\n                              'UnitPrice' : 'unit_price',\n                              'CustomerID' : 'cust_id',\n                              'Country' : 'country'}, inplace=True)","11f85d27":"ecommerce_data.info()","391a129a":"# check missing values for each column \necommerce_data.isnull().sum().sort_values(ascending=False)","d6fa34a7":"# check out the rows with missing values\necommerce_data[ecommerce_data.isnull().any(axis=1)].head()","74f56bf6":"# change the invoice_date format - String to Timestamp format\necommerce_data['invoice_date'] = pd.to_datetime(ecommerce_data.invoice_date, format='%m\/%d\/%Y %H:%M')\necommerce_data.head()","2e3d6647":"# ecommerce_new without missing values\necommerce_new = ecommerce_data.dropna(axis=0)","fa041f5a":"# check missing values for each column \necommerce_new.isnull().sum().sort_values(ascending=False)","b206caa6":"# change columns type - String to Int type \necommerce_new['cust_id'] = ecommerce_new['cust_id'].astype('int64')\necommerce_new.head()","70bc02ae":"## Remove Quantity with negative values\necommerce_new = ecommerce_new[ecommerce_new.quantity > 0]","ff9d0f54":"ecommerce_new.describe().round(2)","e8216d95":"# amount_spent = quantity ** unit_price\necommerce_new['amount_spent'] = ecommerce_new['quantity'] * ecommerce_new['unit_price']","a6e6b697":"# rearrange all the columns for easy reference\necommerce_new = ecommerce_new[['invoice_num','invoice_date','stock_code','description','quantity','unit_price','amount_spent','cust_id','country']]\necommerce_new.head()","e8acaf09":"ecommerce_new.insert(loc=2, column='year_month', value=ecommerce_new['invoice_date'].map(lambda x: 100*x.year + x.month))\necommerce_new.insert(loc=3, column='month', value=ecommerce_new.invoice_date.dt.month)\n# +1 to make Monday=1.....until Sunday=7\necommerce_new.insert(loc=4, column='day', value=(ecommerce_new.invoice_date.dt.dayofweek)+1)\necommerce_new.insert(loc=5, column='hour', value=ecommerce_new.invoice_date.dt.hour)\necommerce_new.tail()\n","ff96bd5b":"ecommerce_new.columns","9c99ab45":"elog = ecommerce_new[['cust_id','invoice_date']]\ndisplay(elog.sample(5))","16bc3f66":"elog.invoice_date.describe()","d71ec58f":"ecommerce_new.tail()","e84a7ead":"%%time\nfrom lifetimes.utils import calibration_and_holdout_data\n\ncalibration_period_ends = '2011-9-09 12:50:00'\n\n\nsummary_cal_holdout = calibration_and_holdout_data(elog, \n                                                   customer_id_col = 'cust_id', \n                                                   datetime_col = 'invoice_date', \n                                                   freq = 'D', #days\n                                        calibration_period_end=calibration_period_ends,\n                                        observation_period_end='2011-12-09 12:50:00' )","f64faf5b":"print (summary_cal_holdout.head())","75278863":"%%time \n\nfrom lifetimes import ModifiedBetaGeoFitter\n\nmbgnbd = ModifiedBetaGeoFitter(penalizer_coef=0.01)\nmbgnbd.fit(summary_cal_holdout['frequency_cal'], \n        summary_cal_holdout['recency_cal'], \n        summary_cal_holdout['T_cal'],\n       verbose=True)","038b9211":"print(mbgnbd)\n","6de9cc49":"t = 90 # days to predict in the future \nsummary_cal_holdout['predicted_purchases'] = mbgnbd.conditional_expected_number_of_purchases_up_to_time(t, \n                                                                                      summary_cal_holdout['frequency_cal'], \n                                                                                      summary_cal_holdout['recency_cal'], \n                                                                                      summary_cal_holdout['T_cal'])\n\nsummary_cal_holdout['p_alive'] = mbgnbd.conditional_probability_alive(summary_cal_holdout['frequency_cal'], \n                                                                         summary_cal_holdout['recency_cal'], \n                                                                         summary_cal_holdout['T_cal'])\nsummary_cal_holdout['p_alive'] = np.round(summary_cal_holdout['p_alive'] \/ summary_cal_holdout['p_alive'].max(), 2)\n\n#summary_cal_holdout['clv'] = gg.customer_lifetime_value(\n#    mbgnbd, #the model to use to predict the number of future transactions\n#    summary_cal_holdout['frequency_cal'],\n#    summary_cal_holdout['recency_cal'],\n#    summary_cal_holdout['T_cal'],\n#    summary_cal_holdout['monetary_value_cal'],\n#    time=3, # months\n#    discount_rate=0 #0.0025 # = 0.03\/12 monthly discount rate ~ 3% annually\n#)\n#summary_cal_holdout['clv'] += (-1*summary_c","e836d488":"display(summary_cal_holdout.sample(2).T)","ab2b33f9":"%%time \n\nfrom lifetimes.plotting import plot_period_transactions\nax = plot_period_transactions(mbgnbd, max_frequency=7)\nax.set_yscale('log')\nsns.despine();","af688cba":"%%time \n\nfrom lifetimes.plotting import plot_calibration_purchases_vs_holdout_purchases\n\nplot_calibration_purchases_vs_holdout_purchases(mbgnbd, summary_cal_holdout)\nsns.despine();","4796c964":"from lifetimes.plotting import plot_history_alive\nfrom datetime import date\nfrom pylab import figure, text, scatter, show\n\nindividual = summary_cal_holdout.iloc[400]\n\nid = individual.name\nt = 365*50\n\ntoday = date.today()\ntwo_year_ago = today.replace(year=today.year - 2)\none_year_from_now = today.replace(year=today.year + 1)\n\nsp_trans = elog.loc[elog['cust_id'] == id]\n\nfrom lifetimes.utils import calculate_alive_path\n\nt = (today - sp_trans.invoice_date.min().date()).days\np_alive_today = pd.DataFrame(calculate_alive_path(mbgnbd, sp_trans, 'invoice_date', t, freq='D'))[0].tail(1).values\np_alive_today = np.round(p_alive_today[0], 2)\nprint('Probability that customer is alive today is', p_alive_today)\n\nt = (one_year_from_now - sp_trans.invoice_date.min().date()).days\nax = plot_history_alive(mbgnbd, t, sp_trans, 'invoice_date', start_date=two_year_ago) #, start_date='2016-01-01'\nax.vlines(x=today, ymin=0, ymax=1.05, colors='#4C4C4C')\nax.hlines(y=0.8, xmin=two_year_ago, xmax=one_year_from_now, colors='#4C4C4C')\n\nax.set_xlim(two_year_ago, one_year_from_now) # sp_trans.ORDER_DATE.min()\nax.set_ylim(0, 1.05)\n\nplt.xticks(rotation=-90)\ntext(0.75, 0.1, p_alive_today, ha='center', va='center', transform=ax.transAxes)\n\nsns.despine()","f4fd2664":"elog.columns = ['cust_id', 'invoice_date']","8f741514":"%%time\n# Get expected and actual repeated cumulative transactions.\n\nfrom lifetimes.utils import expected_cumulative_transactions\n\nt = (elog.invoice_date.max() - elog.invoice_date.min()).days\ndf = expected_cumulative_transactions(mbgnbd, elog, 'invoice_date', 'cust_id', t)","9781464a":"df.tail()\n","e8d6fb7d":"%%time\n# Calibration period = 2016-09-04 to 2017-09-30\nfrom datetime import datetime\n\ncal = datetime.strptime('2018-06-30', '%Y-%m-%d')\n\nfrom lifetimes.plotting import plot_cumulative_transactions\nt = (elog.invoice_date.max() - elog.invoice_date.min()).days\nt_cal = (cal - elog.invoice_date.min()).days\nplot_cumulative_transactions(mbgnbd, elog, 'invoice_date', 'cust_id', t, t_cal, freq='D')\nsns.despine()","b27338d5":"%%time \n\nfrom lifetimes.plotting import plot_incremental_transactions\nplot_incremental_transactions(mbgnbd, elog, 'invoice_date', 'cust_id', t, t_cal, freq='D')\nsns.despine()","04c904f2":"### Predicted Transactions with Time\n","0a1a6f59":"# **Data Cleaning**","aa60429f":"## Creating RFM Matrix based on transaction log\n### Spliting calibration and holdout period","48e9db8b":"#### >  Date range of orders","4bffe6c3":"### Estimating customer lifetime value using the Gamma-Gamma model\nThe Gamma-Gamma model and the independence assumption:\n\nModel assumes that there is no relationship between the monetary value and the purchase frequency. In practice we need to check whether the Pearson correlation between the two vectors is close to 0 in order to use this model.","2ee8238f":"## Model evaluation\nAccessing model fit","ebc51b49":"## Predictions for each customer\n","e6c0e8e4":"## Feature set\n","73eaef70":"Predict the conditional, expected average lifetime value of our customers.\nModel performance will increase if it is trained on all the data and not a sample as is the case here...\n\nCheers..","520ccbe3":"## Remove rows with missing values\n","96255927":"## Formate data","5371bebd":"## Training model - MBG\/NBD\nModel assumptions:\n\n* While active, the number of transactions made by a customer follows a Poisson process with transaction rate  \u03bb .\n* Heterogeneity in  \u03bb  across customers follows a Gamma distribution with shape parameter  r  and scale parameter  \u03b1 .\n* At time zero and right after each purchase the customer becomes inactive with a constant probability  p .\n* Heterogeneity in  p  across customers follows a Gamma distribution with parameter  a  and  b .\n* The transaction rate  \u03bb  and the dropout probability  p  vary independently across customers.","650a7e22":"## Check missing values for each column\n","ed00274c":"## Add the column - amount_spent\n","e986c331":"### Customer Probability History\n"}}