{"cell_type":{"5e455028":"code","57d83abb":"code","a91cc130":"code","74a9bb22":"code","9029c46e":"code","1c1acab4":"code","c4e56cba":"code","ab28ba96":"code","d9fcf779":"code","10dae328":"code","8d1676cd":"code","d96c3339":"code","06650fc2":"code","2822109a":"code","1128eaf0":"code","d318dba1":"code","c4137144":"code","217fc542":"code","a674db1f":"code","820322d2":"code","dc7196e6":"code","303c25f8":"code","8f53904a":"code","7dfa071b":"code","f3173929":"code","2acabb14":"code","0c112496":"code","c81daea7":"code","2b6df473":"code","8a150017":"code","feb44a9d":"code","4d6ef24e":"code","0fe55a54":"code","dc828dae":"code","88eb9e96":"code","1a72de38":"code","b94f5382":"code","0354901f":"code","70408e8c":"code","82274ede":"code","b2e200dd":"code","db36a0c5":"code","a7f0936e":"code","2f16176b":"code","a4df5d1d":"code","769f6ac5":"code","33f908bb":"code","a23d2598":"code","30b4150d":"code","4a4080d8":"code","801bcc1c":"code","5d395d0b":"code","59df7902":"code","b5877145":"code","dc4f9f86":"code","f82385f0":"code","d1ba652f":"code","f0abffe5":"code","09ae904c":"code","a03c26f5":"code","683817b4":"code","350e1445":"code","cc50299d":"code","1c993db1":"code","bb434447":"code","0fda91f3":"code","431801f1":"code","fadb140e":"code","d93b8a72":"code","ef22bc6c":"markdown","88fcd5dd":"markdown","5147f9a0":"markdown","aa75e2ee":"markdown","ca8f7c87":"markdown","70473443":"markdown","72abf59e":"markdown","738fd01e":"markdown","f2a8dcfd":"markdown"},"source":{"5e455028":"#importing necessary libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import graph_objs as go\nimport plotly.figure_factory as ff\nimport re\nimport string\nfrom collections import Counter\nimport plotly.express as px\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n","57d83abb":"# Importing and Loading the data into data frame\ntrain_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nsample_sub = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","a91cc130":"train_df","74a9bb22":"test_df","9029c46e":"##checking shape of dataframe\nprint(train_df.shape)\nprint(test_df.shape)","1c1acab4":"train_df.info()","c4e56cba":"train_df.describe()","ab28ba96":"#Identifying Missing Values in Column\ntrain_df.isnull().sum()","d9fcf779":"#dropping missing value\ntrain_df.dropna(inplace=True)","10dae328":"#Identifying Missing Values in Column\ntrain_df.isnull().sum()","8d1676cd":"#Identifying Missing Values in Column\ntest_df.isnull().sum()","d96c3339":"#Count of texts in each category of sentiments\ntemp = train_df.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Greens')","06650fc2":"plt.figure(figsize=(10,5))\nsns.countplot(x='sentiment',data=train_df)","2822109a":"#funnel chart for visualization\nfig = go.Figure(go.Funnelarea(\n    text =temp.sentiment,\n    values = temp.text,\n    title = {\"position\": \"top center\", \"text\": \"Funnel-Chart of Sentiment Distribution\"}\n    ))\nfig.show()","1128eaf0":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","d318dba1":"jaccard_out=[]\n\nfor ind,row in train_df.iterrows():\n    str1 = row.text\n    str2 = row.selected_text\n\n    jaccard_score = jaccard(str1,str2)\n    jaccard_out.append([str1,str2,jaccard_score])\n","c4137144":"# merging the above jaccard_score with orginal dataset\njaccard = pd.DataFrame(jaccard_out,columns=[\"text\",\"selected_text\",\"jaccard_score\"])\ntrain_df = train_df.merge(jaccard,how='outer')\ntrain_df","217fc542":"#creating a column for Difference In Number Of words of Selected_text and Text\n\n\n#Number Of words in whole text\ntrain_df['Count_text'] = train_df['text'].apply(lambda x:len(str(x).split()))\n\n#Number Of words in Selected Text\ntrain_df['Count_ST'] = train_df['selected_text'].apply(lambda x:len(str(x).split()))\n","a674db1f":"#Difference in Number of words text and Selected Text\ntrain_df['diff_count_words'] = train_df['Count_text'] - train_df['Count_ST'] \ntrain_df","820322d2":"hist_data = [train_df['Count_ST'],train_df['Count_text']]\n\ngroup_labels = ['Selected_Text', 'Text']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels,show_curve=False)\nfig.update_layout(title_text='Distribution of Number Of words in Selected_Text & Text')\nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=600,\n  \n)\nfig.show()","dc7196e6":"#Distribution of Number Of words in Selected_Text & Text using Kernel Distribution\n#the tweets having number of words greater than 25 are very less and thus the number of words distribution plot is right skewed\nplt.figure(figsize=(10,5))\np1=sns.kdeplot(train_df['Count_ST'], shade=True, color=\"r\").set_title('Kernel Distribution of Number Of words')\np1=sns.kdeplot(train_df['Count_text'], shade=True, color=\"b\")","303c25f8":"# This plot will show the comparision of the negative(red) and postive(blue) setiments based on Difference between Number Of words of \"text\" and \"selected text\"\n#jaccard_score is 1 if there is no difference\nplt.figure(figsize=(12,6))\np1=sns.kdeplot(train_df[train_df['sentiment']=='positive']['diff_count_words'], shade=True, color=\"b\").set_title('Kernel Distribution of Difference in Number Of words')\np2=sns.kdeplot(train_df[train_df['sentiment']=='negative']['diff_count_words'], shade=True, color=\"r\")\nplt.legend(labels=['diff_count_words_positive','diff_count_words_negative'])\n#p3=sns.kdeplot(train_df[train_df['sentiment']=='neutral']['diff_count_words'], shade=True, color=\"g\") #RuntimeError: Selected KDE bandwidth is 0. Cannot estimate density.","8f53904a":"#neutral KDE bandwidth is 0. Cannot estimate density.\n#therefore plotting using distplot\nplt.figure(figsize=(10,5))\nsns.distplot(train_df[train_df['sentiment']=='neutral']['diff_count_words'],kde=False)","7dfa071b":"#performing the above operation for jaccard_score\n#jaccard_score is more for more number of matching words\nplt.figure(figsize=(10,5))\np1=sns.kdeplot(train_df[train_df['sentiment']=='positive']['jaccard_score'], shade=True, color=\"b\").set_title('KDE of Jaccard Scores across different Sentiments')\np2=sns.kdeplot(train_df[train_df['sentiment']=='negative']['jaccard_score'], shade=True, color=\"r\")\nplt.legend(labels=['positive','negative'])","f3173929":"#for neutral\nplt.figure(figsize=(10,5))\nsns.distplot(train_df[train_df['sentiment']=='neutral']['jaccard_score'],kde=False)","2acabb14":"# find those clusters\ndf = train_df[train_df['Count_text']<=2]\ndf\n","0c112496":"#jaccord_score mean for each sentiment \ndf.groupby('sentiment').mean()['jaccard_score']","c81daea7":"#having a view of positive sentiments\ndf[df['sentiment']=='positive']","2b6df473":"#cleaning the corpus\n#Make text lowercase, remove text in square brackets,remove links,remove punctuation and remove words containing numbers.\ndef clean_text(text):\n   \n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","8a150017":"train_df['text'] = train_df['text'].apply(lambda x:clean_text(x))\ntrain_df['selected_text'] = train_df['selected_text'].apply(lambda x:clean_text(x))","feb44a9d":"train_df.head()","4d6ef24e":"#Most Common words in our Target-Selected Text\ntrain_df['temp_list'] = train_df['selected_text'].apply(lambda x:str(x).split())\ntop = Counter([item for sublist in train_df['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","0fe55a54":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","dc828dae":"#While we cleaned our dataset we didnt remove the stop words and hence we can see the most coomon word is 'to' .\ndef remove_stopword(x):\n    return [y for y in x if y not in stopwords.words('english')]\ntrain_df['temp_list'] = train_df['temp_list'].apply(lambda x:remove_stopword(x))","88eb9e96":"top = Counter([item for sublist in train_df['temp_list'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","1a72de38":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","b94f5382":"#most common word in text\ntrain_df['temp_list1'] = train_df['text'].apply(lambda x:str(x).split()) #List of words in every row for text\ntrain_df['temp_list1'] = train_df['temp_list1'].apply(lambda x:remove_stopword(x)) #Removing Stopwords","0354901f":"top = Counter([item for sublist in train_df['temp_list1'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp = temp.iloc[1:,:] # removed first common word  I'm  and took data from second row\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","70408e8c":"#SO we can see the Most common words in Selected text and Text are almost the same,which was obvious\nfig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","82274ede":"#most common word sentiment wise\nPositive_sent = train_df[train_df['sentiment']=='positive']\nNegative_sent = train_df[train_df['sentiment']=='negative']\nNeutral_sent = train_df[train_df['sentiment']=='neutral']","b2e200dd":"#MosT common positive words\ntop = Counter([item for sublist in Positive_sent['temp_list'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","db36a0c5":"fig = px.bar(temp_positive, x=\"count\", y=\"Common_words\", title='Most Commmon Positive Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","a7f0936e":"#MosT common negative words\ntop = Counter([item for sublist in Negative_sent['temp_list'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(20))\ntemp_negative = temp_negative.iloc[1:,:]\ntemp_negative.columns = ['Common_words','count']\ntemp_negative.style.background_gradient(cmap='Reds')","2f16176b":"fig = px.treemap(temp_negative, path=['Common_words'], values='count',title='Tree Of Most Common Negative Words')\nfig.show()","a4df5d1d":"#MosT common Neutral words\ntop = Counter([item for sublist in Neutral_sent['temp_list'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(20))\ntemp_neutral = temp_neutral.loc[1:,:]\ntemp_neutral.columns = ['Common_words','count']\ntemp_neutral.style.background_gradient(cmap='Reds')","769f6ac5":"fig = px.bar(temp_neutral, x=\"count\", y=\"Common_words\", title='Most Commmon Neutral Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","33f908bb":"fig = px.treemap(temp_neutral, path=['Common_words'], values='count',title='Tree Of Most Common Neutral Words')\nfig.show()","a23d2598":"#We can see words like get,go,dont,got,u,cant,lol,like are common in all three segments . \n#That's interesting because words like dont and cant are more of negative nature and words like lol are more of positive nature.\n#Does this mean our data is incorrectly labelled , we will have more insights on this after N-gram analysis\n#It will be interesting to see the word unique to different sentiments\n\n","30b4150d":"#unique word in each segment\nraw_text = [word for word_list in train_df['temp_list1'] for word in word_list]","4a4080d8":"def words_unique(sentiment,numwords,raw_words):\n    '''\n    Input:\n        segment - Segment category (ex. 'Neutral');\n        numwords - how many specific words do you want to see in the final result; \n        raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n\n    '''\n    allother = []\n    for item in train_df[train_df.sentiment != sentiment]['temp_list1']:\n        for word in item:\n            allother .append(word)\n    allother  = list(set(allother ))\n    \n    specificnonly = [x for x in raw_text if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in train_df[train_df.sentiment == sentiment]['temp_list1']:\n        for word in item:\n            mycounter[word] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n    \n    return Unique_words","801bcc1c":"#positive tweet\nUnique_Positive= words_unique('positive', 20, raw_text)\nprint(\"The top 20 unique words in Positive Tweets are:\")\nUnique_Positive.style.background_gradient(cmap='Greens')","5d395d0b":"fig = px.treemap(Unique_Positive, path=['words'], values='count',title='Tree Of Unique Positive Words')\nfig.show()","59df7902":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Positive['count'], labels=Unique_Positive.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Positive Words')\nplt.show()","b5877145":"Unique_Negative= words_unique('negative', 10, raw_text)\nprint(\"The top 10 unique words in Negative Tweets are:\")\nUnique_Negative.style.background_gradient(cmap='Reds')","dc4f9f86":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.rcParams['text.color'] = 'black'\nplt.pie(Unique_Negative['count'], labels=Unique_Negative.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Negative Words')\nplt.show()","f82385f0":"Unique_Neutral= words_unique('neutral', 10, raw_text)\nprint(\"The top 10 unique words in Neutral Tweets are:\")\nUnique_Neutral.style.background_gradient(cmap='Oranges')","d1ba652f":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Neutral['count'], labels=Unique_Neutral.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Neutral Words')\nplt.show()","f0abffe5":"df_train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\ndf_submission = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","09ae904c":"df_train['Num_words_text'] = df_train['text'].apply(lambda x:len(str(x).split())) #Number Of words in main Text in train set","a03c26f5":"df_train = df_train[df_train['Num_words_text']>=3]","683817b4":"def save_model(output_dir, nlp, new_model_name):\n    ''' This Function Saves model to \n    given output directory'''\n    \n    output_dir = f'..\/working\/{output_dir}'\n    if output_dir is not None:        \n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\", output_dir)","350e1445":"# pass model = nlp if you want to train on top of existing model \n\ndef train(train_data, output_dir, n_iter=20, model=None):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    \"\"\n    if model is not None:\n        nlp = spacy.load(output_dir)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n    \n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    # add labels\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # sizes = compounding(1.0, 4.0, 1.001)\n        # batch up the examples using spaCy's minibatch\n        if model is None:\n            nlp.begin_training()\n        else:\n            nlp.resume_training()\n\n\n        for itn in tqdm(range(n_iter)):\n            random.shuffle(train_data)\n            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts,  # batch of texts\n                            annotations,  # batch of annotations\n                            drop=0.5,   # dropout - make it harder to memorise data\n                            losses=losses, \n                            )\n            print(\"Losses\", losses)\n    save_model(output_dir, nlp, 'st_ner')","cc50299d":"def get_model_out_path(sentiment):\n    '''\n    Returns Model output path\n    '''\n    model_out_path = None\n    if sentiment == 'positive':\n        model_out_path = 'models\/model_pos'\n    elif sentiment == 'negative':\n        model_out_path = 'models\/model_neg'\n    return model_out_path","1c993db1":"def get_training_data(sentiment):\n    '''\n    Returns Trainong data in the format needed to train spacy NER\n    '''\n    train_data = []\n    for index, row in df_train.iterrows():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return train_data","bb434447":"#training model for positive and negative tewwts\nsentiment = 'positive'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n# For DEmo Purposes I have taken 3 iterations you can train the model as you want\ntrain(train_data, model_path, n_iter=3, model=None)","0fda91f3":"sentiment = 'negative'\n\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_out_path(sentiment)\n\ntrain(train_data, model_path, n_iter=3, model=None)","431801f1":"def predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text","fadb140e":"selected_texts = []\nMODELS_BASE_PATH = '..\/input\/models\/models\/'\n#kaggle\/input\/models\/models\/\n\nif MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", MODELS_BASE_PATH)\n    model_pos = spacy.load(MODELS_BASE_PATH + 'model_pos')\n    model_neg = spacy.load(MODELS_BASE_PATH + 'model_neg')\n        \n    for index, row in df_test.iterrows():\n        text = row.text\n        output_str = \"\"\n        if row.sentiment == 'neutral' or len(text.split()) <= 2:\n            selected_texts.append(text)\n        elif row.sentiment == 'positive':\n            selected_texts.append(predict_entities(text, model_pos))\n        else:\n            selected_texts.append(predict_entities(text, model_neg))\n        \ndf_test['selected_text'] = selected_texts","d93b8a72":"df_submission['selected_text'] = df_test['selected_text']\ndf_submission.to_csv(\"submission.csv\", index=False)\ndisplay(df_submission.head(10))","ef22bc6c":"#### the no of jaccord_score for neutral tweets are high for matching text","88fcd5dd":"#### This shows that for all 2 or less than 2 letter text,avg score for neutral are 97.7 and 78.8 and 76.5 for negative and positive respectively","5147f9a0":"### Data Cleaning","aa75e2ee":"#### from the above graph we can see a bump near the jaccord_score = 1 . This shows that for a cluster of negative and positive tweets the text and selected text are same .we need to find those clusters then we can predict text for selected texts for those tweets irrespective of sentiments.\n#### to find those cluster one approach would be to check tweets which have number of words lesss than or equal to 2 in text. This is because in such tweets text might be completely used in selected text","ca8f7c87":"## EDA: Visualising the Data","70473443":"#### The train data conatins 4 columns whereas the test data contains 3 columns . The different column here is \"selected_text\" of train data which determine the sentiments of tweet. The objective in this competition is to construct a model that can do the same - look at the labeled sentiment column for a given tweet in the test data and extract what word or phrase best supports it.\n\n#### The metric in this competition is the word-level Jaccard Similarity Scores. For this we need to perform Jaccard similarity for strings(between text and Selected_text)\n","72abf59e":"## Modelling using NER","738fd01e":"## Importing and Understanding Data","f2a8dcfd":"#### this graph shows that those text which has difference zero are very high. from above graph text and selected text are mostly the same for neutral tweets"}}