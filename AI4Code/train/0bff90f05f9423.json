{"cell_type":{"dbba64f4":"code","30ae1bfd":"code","5b4b36ac":"code","5dafa754":"code","c96c65fb":"code","2e92a46f":"code","39271388":"code","13e44cdc":"code","ff5ea93f":"code","54a4c10f":"code","12fb2688":"code","af544965":"code","544e153b":"code","602fc0a5":"code","44d7c5d7":"code","476ffac6":"code","fd1b9885":"code","632480ed":"code","85470bd4":"code","47326a04":"code","14e58b51":"markdown","6b1bbd3f":"markdown","d4d26970":"markdown","f793d52c":"markdown","6eb7b147":"markdown","bcf5e811":"markdown","649f25f8":"markdown","55d547cc":"markdown","3ad1a8dd":"markdown","2005f84c":"markdown","133e4a89":"markdown","55d3c109":"markdown","cac85f62":"markdown","bced7814":"markdown","6768c61e":"markdown"},"source":{"dbba64f4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","30ae1bfd":"df = pd.read_csv(\"..\/input\/youtube-new\/USvideos.csv\")\ndf.head()","5b4b36ac":"# Checking Null Values\ndf.isnull().sum()","5dafa754":"df.describe()","c96c65fb":"df.columns","2e92a46f":"df1 = df.drop(['video_id', 'trending_date', 'title', 'channel_title', 'category_id',\n       'publish_time', 'tags', 'thumbnail_link', 'comments_disabled', 'ratings_disabled',\n       'video_error_or_removed', 'description'], axis=1) # Deleting column\ndf1.head()","39271388":"corr = df1.corr()\nsns.heatmap(corr, annot=True)","13e44cdc":"from sklearn.model_selection import train_test_split","ff5ea93f":"x = df1[['views', 'dislikes', 'comment_count']]\ny = df1['likes']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=3)","54a4c10f":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import metrics","12fb2688":"lm = LinearRegression()\nlm.fit(x_train, y_train)      # Fitting model with x_train and y_train\nlm_pred = lm.predict(x_test)  # Predicting the results\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, lm_pred, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, lm_pred))\nprint(\"Accuracy :\",lm.score(x_test, y_test))","af544965":"labels = {'True Labels': y_test, 'Predicted Labels': lm_pred}\ndf_lm = pd.DataFrame(data = labels)\nsns.lmplot(x='True Labels', y= 'Predicted Labels', data = df_lm)","544e153b":"from xgboost import XGBRegressor","602fc0a5":"xgb = XGBRegressor()\nxgb.fit(x_train, y_train)       # Fitting model with x_train and y_train\nxgb_pred = xgb.predict(x_test)  # Predicting the results\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, xgb_pred, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, xgb_pred))\nprint(\"Accuracy :\",xgb.score(x_test, y_test))","44d7c5d7":"xgb = XGBRegressor(n_estimators=5000, learning_rate=0.001)\nxgb.fit(x_train, y_train)       # Fitting model with x_train and y_train\nxgb_pred = xgb.predict(x_test)  # Predicting the results\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, xgb_pred, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, xgb_pred))\nprint(\"Accuracy :\",xgb.score(x_test, y_test))","476ffac6":"labels_xgb = {'True Labels': y_test, 'Predicted Labels': xgb_pred}\ndf_xgb = pd.DataFrame(data = labels_xgb)\nsns.lmplot(x='True Labels', y= 'Predicted Labels', data = df_xgb)","fd1b9885":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics","632480ed":"rf = RandomForestRegressor()\nrf.fit(x_train, y_train)       # Fitting model with x_train and y_train\nrf_pred = rf.predict(x_test)   # Predicting the results\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, rf_pred, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, rf_pred))\nprint(\"Accuracy :\",rf.score(x_test, y_test))","85470bd4":"nEstimator = [140,160,180,200,220] \ndepth = [10,15,20,25,30]\n\nRF = RandomForestRegressor()\nhyperParam = [{'n_estimators':nEstimator,'max_depth': depth}]\n\ngsv = GridSearchCV(RF,hyperParam,cv=5,verbose=1,scoring='r2',n_jobs=-1)\ngsv.fit(x_train, y_train)\n\nprint(\"Best HyperParameter: \",gsv.best_params_)\nscores = gsv.cv_results_['mean_test_score'].reshape(len(nEstimator),len(depth))\nmaxDepth=gsv.best_params_['max_depth']\nnEstimators=gsv.best_params_['n_estimators']\n\nmodel = RandomForestRegressor(n_estimators = nEstimators,max_depth=maxDepth)\nmodel.fit(x_train, y_train)        # Fitting model with x_train and y_train\n\n# Predicting the results:\nrf_pred_tune = model.predict(x_test)\nprint('RMSE:', np.sqrt(mean_squared_error(y_test, rf_pred_tune, squared=False)))\nprint('r2 score: %.2f' % r2_score(y_test, rf_pred_tune))\nprint(\"Accuracy :\",model.score(x_test, y_test))","47326a04":"labels_rf = {'True Labels': y_test, 'Predicted Labels': rf_pred_tune}\ndf_rf = pd.DataFrame(data = labels_rf)\nsns.lmplot(x='True Labels', y= 'Predicted Labels', data = df_rf)","14e58b51":"We have used Random Forest Regressor for \nthis dataset. Two models of Random Forest are \nintroduced here, first is simple random forest \nregressor and second is hyperparameter tuned \nrandom forest with n_estimators = \n[140,160,180,200,220] and max_depth = \n[10,15,20,25,30].\n1. Simple Random Forest:\n* RMSE: 221.89\n* R2 Score: 0.95\n* Accuracy: 95.23 %\n2. Hyperparameter tuned model:\n* Best max_depth: 30\n* Best n_estimators: 140\n* RMSE: 221.76\n* R2 Score: 0.95\n* Accuracy: 95.24 %\n\nRandom Forest Regressor provided a very good \nresult and best among all 3 ML models we used.\nIt has the lowest RMSE value and R2 is very \nclose to 1.0 with very high accuracy.\nShows the lmplot for the XGB model, \nit fits the regressor model perfectly and \nprovides a very good positive result close to a \n45-degree line.","6b1bbd3f":"# **Conclusion:**\n\nThis dataset is YouTube trending video for \nUSVideos, where we predicted \u2018likes\u2019. This \ndataset had few variables that were useless to \nuse like ID\u2019s, links, description etc. So, we \nremoved those variables and continued our \nRegression models with important variables. \nWe tested 3 machine learning models for this \ndataset, 1st was Linear regression which \nprovided good accuracy of 89.14%, RMSE was \n272.56 and R2 was very close to 1.0, i.e., 0.89. \n2nd we used XGB Regressor, for this we used \n2 models, 1st was a simple model and we got \ngood results and 2nd model was tuned XGB \nRegressor with n_estimators=500 and \nlearning_rate = 0.001, XGB Tuned Regressor \nmodel gave slightly better results, i.e., 94.70 \naccuracies, R2 0.95 and RMSE 227.84. 3rd we \nused Random Forest Regressor, in this we used \n2 models, which was Simple Random Forest \nand Hyperparameter tuned model, we received \nvery good results for these models, like RMSE \n223.13, R2 0.95 and Accuracy 95.12 %, \nhowever after using hyperparameter model, we \ngot slightly better and best results among all the \nmodels that we used, we got RMSE: 221.66, \nR2: 0.95 and Accuracy 95.25 % when \nmax_depth was 30 and n_estimators was 180. \nConclusively, Hyperparameter Tuned Random \nForest was best in predicting target variable \n\u2018likes\u2019.","d4d26970":"# Data Wrangling","f793d52c":"# Correlation:\n\nWe can demonstrate that all variables \nare in good correlation with the target variable. \nViews have 0.85 correlation with likes, dislikes \nhave 0.45 correlation with likes and comment \ncount has 0.80 correlation with target variable \nlikes.\nWe can conclude that none of the independent \nvariables is highly correlated among \nthemselves.\n","6eb7b147":"# 1. Multiple Linear Regression:","bcf5e811":"### Hyperparameter Tuning Random Forest","649f25f8":"# Splitting data into train and test\n\nWe have split this data into 75% and 25% for \ntrain and test sets respectively using sklearn.model_selection train_test_split. We \ncreated x_train, x_test, y_train and y_test. The \nRandom state for train and test is 3.","55d547cc":"We used 2 XGB Regressor models for this \ndataset. The First will be a very simple XGB \nRegressor model and the second will be tuned \nXGB Regressor by changing \nparameters.\n1. Simple XGB Regressor:\n* RMSE: 229.42\n* R2 Score: 0.95\n* Accuracy: 94.55 % \n2. Tuned XGB Regressor:\n(n_estimators=5000, learning_rate = \n0.001)\n* RMSE: 227.84\n* R2 Score: 0.95\n* Accuracy: 94.70 % \n\nXGB Regressor provided a very good result \nwhen compared to linear regression. The \nhyperparameter tuned XGB Regressor model \ngave 0.15 % more accuracy than the Simple \nXGB Regressor. Also, it can be observed that \nfor both the models R2 value is very close to 1.0 \nwhich is very good for a model. Also, the \nRMSE value for Tuned XGB Regressor is low \nthan the Simple XGB Regressor.\nShows the lmplot for the XGB model, \nit fits the regressor model perfectly and \nprovides a very good positive result close to a \n45-degree line.","3ad1a8dd":"We have used Multiple Linear Regression for \nthe YouTube USVideos dataset. The linear \nRegression model provided a good result. The \nR2 score value is close to 1.0 and accuracy is \nalso above average.\n* RMSE: 272.56\n* R2 Score: 0.89\n* Accuracy: 89.14 %\n\nShows the lmplot, which fits the \nregression model with conditional parameters,\nand it can be observed that a straight line can be \ndrawn.","2005f84c":"# 3. Random Forest","133e4a89":"# 2. XGBoost Regressor","55d3c109":"### Tuning XGBoost","cac85f62":"### Simple XGBoost","bced7814":"### Simple Random Forest","6768c61e":"First, Null values need to be checked as it is \nvery important to predict likes. We only have \n570 Null values for the Description variable. \nThere are so many irrelevant variables such as \ndates, links, ID, and description. We are \nremoving all variables because we need \nnumerical values for regression models.\nDropped 'video_id', 'trending_date', 'title',\n'channel_title', 'category_id', 'publish_time',\n'tags', 'thumbnail_link', 'comments_disabled', \n'ratings_disabled', 'video_error_or_removed', \n'description'. We will only work with most \nimportant variables: \u2018likes\u2019, \u2018views\u2019, \u2018dislikes\u2019 \nand \u2018comment_count\u2019."}}