{"cell_type":{"d4f3a13a":"code","220a37b7":"code","0da18ca9":"code","fc1987c8":"code","5cc061f7":"code","f4b76a2d":"code","c5049ebe":"code","d78cbfa6":"code","46f5c0bd":"code","4e0057e6":"code","434d34bc":"code","3931d301":"code","9418e574":"code","47888ed8":"code","d747d0fd":"code","f4c8807b":"code","f14d08f6":"code","b97bdac9":"code","a897be17":"code","2b8c0dc5":"code","93e84658":"code","b195cb1c":"code","b2ab786f":"code","abb24ce9":"code","7773ef4a":"code","3d9e2aaf":"code","da36e96b":"code","2bc9b696":"code","1bd73262":"code","5a959516":"code","d47efc98":"code","5a840148":"code","cd3002ac":"code","64c6444a":"code","215cc55c":"code","e059f3f5":"code","e2013bbb":"code","7e7ac265":"code","f05bbf4f":"code","6c35c3e6":"code","b4a459ef":"code","f5455b84":"code","719a0734":"code","c98d3ef5":"code","fd956c5e":"code","05245c41":"code","fe737dee":"code","f7a2626b":"code","4dece044":"code","21c47ba1":"markdown","b0b7fc87":"markdown","9ba9a500":"markdown","ed6f65fe":"markdown","759fcd2c":"markdown","6d61605a":"markdown","a39113a2":"markdown","fc941bfc":"markdown","213d8f59":"markdown","af912778":"markdown","bc1ad4a5":"markdown","f79447c5":"markdown","86cf7d9b":"markdown","a9d6948e":"markdown","b06f8283":"markdown","f1618302":"markdown","803df38a":"markdown","419a6bd8":"markdown","adceac86":"markdown","776be1cf":"markdown","498192d4":"markdown","8a66e87e":"markdown","64e48262":"markdown","29406cd4":"markdown","e2dfc3ba":"markdown","976fc445":"markdown","ecef66ba":"markdown","235415f1":"markdown","1db42df0":"markdown","ade1423f":"markdown","291563bc":"markdown","fd74ea52":"markdown","04c14d3d":"markdown","dfa8c95f":"markdown","9585f277":"markdown","ee9b7ef0":"markdown","107297c4":"markdown","09b86e84":"markdown"},"source":{"d4f3a13a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\nimport re\n\n# Use tqdm to show progress of an pandas function we use\ntqdm.pandas()\n\nfrom gensim.models import KeyedVectors as kv\nfrom gensim.scripts.glove2word2vec import glove2word2vec\n\nembedding_path_dict= {'googlenews':{\n                            'path':'..\/input\/embeddings\/GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin',\n                            'format':'word2vec',\n                            'binary': True\n                      },\n                      'glove':{\n                            'path':'..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt',\n                            'format': 'glove',\n                            'binary': ''\n                      },\n                      'glove_word2vec':{\n                            'path':'..\/input\/glove.840B.300d.txt.word2vec',\n                            'format': 'word2vec',\n                            'binary': False\n                      },\n                      'wiki':{\n                            'path': '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec',\n                            'format': 'word2vec',\n                            'binary': False\n                      },\n                      'paragram':{\n                            'path': '..\/input\/embeddings\/paragram_300_sl999\/paragram_300_sl999.txt',\n                            'format': '',\n                            'binary': False\n                      }\n                    }\n","220a37b7":"train=pd.read_csv(\"..\/input\/train.csv\")\ntest= pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape:\", test.shape)","0da18ca9":"train.head()","fc1987c8":"train = train.loc[train.question_text.str.len()>100]","5cc061f7":"len(train.loc[train['target']==0])","f4b76a2d":"num_pos= len(train.loc[train['target']==1])\nprint(num_pos)","c5049ebe":"len(train['target'])","d78cbfa6":"# Get word embeddings\ndef get_embeddings(embedding_path_dict, emb_name):\n    \"\"\"\n    :params embedding_path_dict: a dictionary containing the path, binary flag, and format of the desired embedding,\n            emb_name: the name of the embedding to retrieve\n    :return embedding index: a dictionary containing the embeddings\"\"\"\n    \n    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n    \n    embeddings_index = {}\n    if (emb_name == 'googlenews'):\n        emb_path = embedding_path_dict[emb_name]['path']\n        bin_flag = embedding_path_dict[emb_name]['binary']\n        embeddings_index = kv.load_word2vec_format(emb_path, binary=bin_flag).vectors\n    elif (emb_name in ['glove', 'wiki']):\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_path_dict[emb_name]['path']) if len(o)>100)    \n    elif (emb_name == 'paragram'):\n        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embedding_path_dict[emb_name]['path'], encoding=\"utf8\", errors='ignore'))\n    return embeddings_index\n\n#Convert GLoVe format into word2vec format\ndef glove_to_word2vec(embedding_path_dict, emb_name='glove', output_emb='glove_word2vec'):\n    \"\"\"\n    Convert the GLOVE embedding format to a word2vec format\n    :params embedding_path_dict: a dictionary containing the path, binary flag, and format of the desired embedding,\n            glove_path: the name of the GLOVE embedding\n            output_file_path: the name of the converted embedding in embedding_path_dict. \n    :return output from the glove2word2vec script\n    \"\"\"\n    glove_input_file = embedding_path_dict[emb_name]['path']\n    word2vec_output_file = embedding_path_dict[output_emb]['path']                \n    return glove2word2vec(glove_input_file, word2vec_output_file)\n","46f5c0bd":"# Get stats of a given embeddings index\ndef get_emb_stats(embeddings_index):\n\n    # Put all embeddings in a numpy matrix\n    all_embs= np.stack(embeddings_index.values())\n\n    # Get embedding stats\n    emb_mean = all_embs.mean()\n    emb_std = all_embs.std()\n    \n    num_embs = all_embs.shape[0]\n    \n    emb_size = all_embs.shape[1]\n    \n    return emb_mean,emb_std, num_embs, emb_size ","4e0057e6":"# Converts sentences into lists of tokens\n# We use this function to allow more control over what constitutes a word\n# It also allows us to explore ways to cover more the pre-defined word embeddings.\n\ndef tokenize(sentences, restrict_to_len=-1):\n    \"\"\"\n    :params sentence_list: list of strings\n    :returns tok_sentences: list of list of tokens\n    \"\"\"\n    \n    if restrict_to_len>0:\n        tok_sentences = [re.findall(r\"[\\w]+[']*[\\w]+|[\\w]+|[.,!?;]\", x ) \\\n                         for x in sentences if len(x)>restrict_to_len]\n    else:\n       tok_sentences = [re.findall(r\"[\\w]+[']*[\\w]+|[\\w]+|[.,!?;]\", x ) \\\n                         for x in sentences] \n    return tok_sentences\n\n#Build the vocabulary given a list of sentence words\ndef get_vocab(sentences, verbose= True):\n    \"\"\"\n    :param sentences: a list of list of words\n    :return: a dictionary of words and their frequency \n    \"\"\"\n    vocab={}\n    for sentence in tqdm(sentences, disable = (not verbose)):\n        for word in sentence:\n            try:\n                vocab[word] +=1\n            except KeyError:\n                vocab[word] = 1\n    return vocab\n\ndef repl(m):\n    return '#' * len(m.group())\n\n#Convert numerals to a # sign\ndef convert_num_to_pound(sentences):\n    return sentences.progress_apply(lambda x: re.sub(\"[1-9][\\d]+\", repl, x)).values\n","434d34bc":"\n#find words in common between a given embedding and our vocabulary\ndef compare_vocab_and_embeddings(vocab, embeddings_index):\n    \"\"\"\n    :params vocab: our corpus vocabulary (a dictionary of word frquencies)\n            embeddings_index: a genim object containing loaded embeddings.\n    :returns in_common: words in common,\n             in_common_freq: total frequency in the corpus vocabulary of \n                             all words in common\n             oov: out of vocabulary words\n             oov_frequency: total frequency in vocab of oov words\n    \"\"\"\n    in_common={}\n    oov=[]\n    in_common=[]\n    in_common_freq = 0\n    oov_freq = 0\n    \n    # Compose the vocabulary given the sentence tokens\n    vocab = get_vocab(sentences)\n\n    for word in tqdm(vocab):\n        if word in embeddings_index:\n            in_common.append(word)\n            in_common_freq += vocab[word]\n        else: \n            oov.append(word)\n            oov_freq += vocab[word]\n    \n    print('Found embeddings for {:.2%} of vocab'.format(len(in_common) \/ len(vocab)))\n    print('Found embeddings for  {:.2%} of all text'.format(in_common_freq \/ (in_common_freq + oov_freq)))\n\n    return sorted(in_common)[::-1], sorted(oov)[::-1], in_common_freq, oov_freq, vocab\n\n# print the list of out-of-vocabulary words sorted by their frequency in teh training text\ndef show_oov_words(oov, vocab,  num_to_show=15):\n    # Sort oov words by their frequency in the text\n    sorted_oov= sorted(oov, key =lambda x: vocab[x], reverse=True )\n\n    # Show oov words and their frequencies\n    if (len(sorted_oov)>0):\n        print(\"oov words:\")\n        for word in sorted_oov[:num_to_show]:\n            print(\"%s\\t%s\"%(word, vocab[word]))\n    else:\n        print(\"No words were out of vocabulary.\")\n        \n    return len(sorted_oov);\n","3931d301":"embedding_name = 'glove'\nembeddings_index= get_embeddings(embedding_path_dict, embedding_name)\nimport gc; gc.collect()","9418e574":"# Get embedding stats\nemb_mean,emb_std, num_embs, emb_size = get_emb_stats(embeddings_index)\nprint(\"mean: %5.5f\\nstd: %5.5f\\nnumber of embeddings: %d\\nembedding vector size:%d\" \\\n      %(emb_mean,emb_std, num_embs, emb_size))","47888ed8":"question_text = train[\"question_text\"]\n\n# Get a list of token for each question text\n# restrict_to_len is approximately the mean sentence length+ 0.5std\nsentences = tokenize(question_text)\n","d747d0fd":"# Does our tokenization method produce a good match with \n# the words in the selected embedding type?\n\n# Get words in common and out of vocabulary words\nin_common, oov, in_common_freq, oov_freq, vocab = compare_vocab_and_embeddings(sentences, embeddings_index)\n\n# Print a sorted list of the oov words\nshow_oov_words(oov, vocab)\n","f4c8807b":"contr_dict={\"I\\'m\": \"I am\",\n            \"won\\'t\": \"will not\",\n            \"\\'s\" : \"\", \n            \"\\'ll\":\"will\",\n            \"\\'ve\":\"have\",\n            \"n\\'t\":\"not\",\n            \"\\'re\": \"are\",\n            \"\\'d\": \"would\",\n            \"y'all\": \"all of you\"}\n\ndef replace_contractions(sentences, contr_dict=contr_dict):\n    res_sentences=[]\n    for sent in sentences:\n        for contr in contr_dict:\n            sent = sent.replace(contr, \" \"+contr_dict[contr])\n        res_sentences.append(sent)\n    return res_sentences","f14d08f6":"# start by replacing contractions\nsentences = replace_contractions(question_text)\n\n# Get a list of token for each question text\n# restrict_to_len is approximately the mean sentence length+ 0.5std\nsentences = tokenize(sentences)","b97bdac9":"# Does our tokenization method produce a good match with \n# the words in the selected embedding type?\n\n# Get words in common and out of vocabulary words\nin_common, oov, in_common_freq, oov_freq, vocab = compare_vocab_and_embeddings(sentences, embeddings_index)\n\n# Print a sorted list of the oov words\nshow_oov_words(oov, vocab)","a897be17":"print(\"Is 'Quora' in the wiki embeddings index?\",'Quora' in embeddings_index)\nprint(\"Is 'quora' in the wiki embeddings index?\",'quora' in embeddings_index)","2b8c0dc5":"w_quoran_contr_dict={\"I\\'m\": \"I am\",\n                    \"won\\'t\": \"will not\",\n                    \"\\'s\" : \"\", \n                    \"\\'ll\":\"will\",\n                    \"\\'ve\":\"have\",\n                    \"n\\'t\":\"not\",\n                    \"\\'re\": \"are\",\n                    \"\\'d\": \"would\",\n                    \"y'all\": \"all of you\",\n                    \"Quoran\": \"Quora contributor\",\n                    \"quoran\": \"quora contributor\"\n                    }","93e84658":"# replace contractions using a contr dict containing replacement for Quoran\nsentences = replace_contractions(question_text, contr_dict = w_quoran_contr_dict)\n\n# Get a list of token for each question text\n# restrict_to_len is approximately the mean sentence length+ 0.5std\nsentences = tokenize(sentences)","b195cb1c":"# Does our tokenization method produce a good match with \n# the words in the selected embedding type?\n\n# Get words in common and out of vocabulary words\nin_common, oov, in_common_freq, oov_freq, vocab = compare_vocab_and_embeddings(sentences, embeddings_index)\n\n# Print a sorted list of the oov words\nshow_oov_words(oov, vocab)","b2ab786f":"print(\"0 in embedding index?\", ('0' in embeddings_index))\nprint(\"Other digits?\", ('1' in embeddings_index) and ('2' in embeddings_index))","abb24ce9":"import re\n\ndef convert_height(sentences):\n    res_sentences = []\n    for sent in sentences:\n        res_sent = re.sub( \"(\\d+)\\'(\\d+)\", \"\\1 foot \\2\", sent)\n        res_sentences.append(res_sent)\n    return res_sentences","7773ef4a":"# start by converting heights such as 5'4 to longer format 5 foot 4\nsentences = convert_height(question_text)\n\n# replace contractions\nsentences = replace_contractions(sentences)\n\n# Get a list of token for each question text\n# restrict_to_len is approximately the mean sentence length+ 0.5std\nsentences = tokenize(sentences)","3d9e2aaf":"# Does our tokenization method produce a good match with \n# the words in the selected embedding type?\n\n# Get words in common and out of vocabulary words\nin_common, oov, in_common_freq, oov_freq, vocab = compare_vocab_and_embeddings(sentences, embeddings_index)\n\n# Print a sorted list of the oov words\nshow_oov_words(oov, vocab)","da36e96b":"embedding_name = 'paragram'\nembeddings_index= get_embeddings(embedding_path_dict, embedding_name)\nimport gc; gc.collect()","2bc9b696":"# Get embedding stats\nemb_mean,emb_std, num_embs, emb_size = get_emb_stats(embeddings_index)\nprint(\"mean: %5.5f\\nstd: %5.5f\\nnumber of embeddings: %d\\nembedding vector size:%d\" \\\n      %(emb_mean,emb_std, num_embs, emb_size))","1bd73262":"question_text = train[\"question_text\"]\n\n# Get a list of token for each question text\n# restrict_to_len is approximately the mean sentence length+ 0.5std\nsentences = tokenize(question_text)\n","5a959516":"# Does our tokenization method produce a good match with \n# the words in the selected embedding type?\n\n# Get words in common and out of vocabulary words\nin_common, oov, in_common_freq, oov_freq, vocab = compare_vocab_and_embeddings(sentences, embeddings_index)\n\n# Print a sorted list of the oov words\nshow_oov_words(oov, vocab)","d47efc98":"def convert_to_lower(sentences):\n    res_sentences = []\n    for sent in sentences:\n        lower_sent = sent.lower()\n        res_sentences.append(lower_sent)\n    return res_sentences","5a840148":"# convert capitals to lowercase\nsentences = convert_to_lower(question_text)\n\n# Get a list of token for each question text\n# restrict_to_len is approximately the mean sentence length+ 0.5std\nsentences = tokenize(sentences)","cd3002ac":"# Does our tokenization method produce a good match with \n# the words in the selected embedding type?\n\n# Get words in common and out of vocabulary words\nin_common, oov, in_common_freq, oov_freq, vocab = compare_vocab_and_embeddings(sentences, embeddings_index)\n\n# Print a sorted list of the oov words\nshow_oov_words(oov, vocab)","64c6444a":"# start by converting capitals to lowercase\nsentences = convert_to_lower(question_text)\n\n# replace contractions\nsentences = replace_contractions(sentences)\n\n# Get a list of token for each question text\n# restrict_to_len is approximately the mean sentence length+ 0.5std\nsentences = tokenize(sentences)","215cc55c":"# Does our tokenization method produce a good match with \n# the words in the selected embedding type?\n\n# Get words in common and out of vocabulary words\nin_common, oov, in_common_freq, oov_freq, vocab = compare_vocab_and_embeddings(sentences, embeddings_index)\n\n# Print a sorted list of the oov words\nshow_oov_words(oov, vocab)","e059f3f5":"# start by replacing heights such as 5'4 to a longer format (5 foot 4)\nsentences = convert_height(question_text)\n\n# convert capitals to lowercase\nsentences = convert_to_lower(sentences)\n\n# replace contractions\nsentences = replace_contractions(sentences)\n\n# Get a list of token for each question text\n# restrict_to_len is approximately the mean sentence length+ 0.5std\nsentences = tokenize(sentences)","e2013bbb":"# Does our tokenization method produce a good match with \n# the words in the selected embedding type?\n\n# Get words in common and out of vocabulary words\nin_common, oov, in_common_freq, oov_freq, vocab = compare_vocab_and_embeddings(sentences, embeddings_index)\n\n# Print a sorted list of the oov words\nshow_oov_words(oov, vocab)","7e7ac265":"embedding_name = 'wiki'\nembeddings_index= get_embeddings(embedding_path_dict, embedding_name)\nimport gc; gc.collect()","f05bbf4f":"# Get embedding stats\nemb_mean,emb_std, num_embs, emb_size = get_emb_stats(embeddings_index)\nprint(\"mean: %5.5f\\nstd: %5.5f\\nnumber of embeddings: %d\\nembedding vector size:%d\" \\\n      %(emb_mean,emb_std, num_embs, emb_size))","6c35c3e6":"question_text = train[\"question_text\"]\n\n# Get a list of token for each question text\n# restrict_to_len is approximately the mean sentence length+ 0.5std\nsentences = tokenize(question_text)\n","b4a459ef":"# Does our tokenization method produce a good match with \n# the words in the selected embedding type?\n\n# Get words in common and out of vocabulary words\nin_common, oov, in_common_freq, oov_freq, vocab = compare_vocab_and_embeddings(sentences, embeddings_index)\n\n# Print a sorted list of the oov words\nshow_oov_words(oov, vocab)","f5455b84":"# start by replacing contractions\nsentences = replace_contractions(question_text)\n\n# Get a list of token for each question text\n# restrict_to_len is approximately the mean sentence length+ 0.5std\nsentences = tokenize(sentences)","719a0734":"# Does our tokenization method produce a good match with \n# the words in the selected embedding type?\n\n# Get words in common and out of vocabulary words\nin_common, oov, in_common_freq, oov_freq, vocab = compare_vocab_and_embeddings(sentences, embeddings_index)\n\n# Print a sorted list of the oov words\nshow_oov_words(oov, vocab)","c98d3ef5":"print(\"Is 'Quora' in the wiki embeddings index?\",'Quora' in embeddings_index)\nprint(\"Is 'quora' in the wiki embeddings index?\",'quora' in embeddings_index)","fd956c5e":"# start by replacing contractions using the contractions dict containing replacements for Quoran\nsentences = replace_contractions(question_text, contr_dict = w_quoran_contr_dict)\n\n# Get a list of token for each question text\n# restrict_to_len is approximately the mean sentence length+ 0.5std\nsentences = tokenize(sentences)","05245c41":"# Does our tokenization method produce a good match with \n# the words in the selected embedding type?\n\n# Get words in common and out of vocabulary words\nin_common, oov, in_common_freq, oov_freq, vocab = compare_vocab_and_embeddings(sentences, embeddings_index)\n\n# Print a sorted list of the oov words\nshow_oov_words(oov, vocab)","fe737dee":"print(\"0 in embedding index?\", ('0' in embeddings_index))\nprint(\"Other digits?\", ('1' in embeddings_index) and ('2' in embeddings_index))","f7a2626b":"# start by converting height to longer form\nsentences = convert_height(question_text)\n\n# replace contractions\nsentences = replace_contractions(sentences,  contr_dict = w_quoran_contr_dict)\n\n# Get a list of token for each question text\n# restrict_to_len is approximately the mean sentence length+ 0.5std\nsentences = tokenize(sentences)","4dece044":"# Does our tokenization method produce a good match with \n# the words in the selected embedding type?\n\n# Get words in common and out of vocabulary words\nin_common, oov, in_common_freq, oov_freq, vocab = compare_vocab_and_embeddings(sentences, embeddings_index)\n\n# Print a sorted list of the oov words\nshow_oov_words(oov, vocab)","21c47ba1":"### Tokenize Training Text","b0b7fc87":"Very slight improvement. But overall, we managed to improve our coverage from 83.54% to 86.43%. For The GloVe embedding the main issue affecting our results was contractions. Replacing height with a longer format helped very slightly.","9ba9a500":"## Get Training and Test Data","ed6f65fe":"### Functions: Compare Training and Embedding Vocabulary","759fcd2c":"### Choose Embedding","6d61605a":"Better! Quorans seems to be a repeatedly missed word in this embedding as well... ","a39113a2":"# Introduction\nThe Quora Insincere Question Classification competition allows us to use the four embeddings:  glove.840B.300d (GloVe), paragram_300_sl999 (paragram),  wiki-news-300d-1M (wiki) and GoogleNews-vectors-negative300 (GoogleNews). In a kernel titled: [\"How to: Preprocessing when Using Embeddings\"](https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings), the author raises the issue of tokenization and its effect on how much of the training vocabulary is covered by words in an embedding. The author uses Google news embeddings to illustrate this point. ** In this kernel I expand on this point by exploring the effect of tokenization assumptions on the other three embeddings: GloVe, Paragram, and Wiki News.** \n\nOur base tokenization method simply defines words as sequences of letters, sequences of letters with an apostrophe somewhere in the sequence, or a puctuation mark. To reduce the amount of preprocessing, nothing was removed during the initial tokenization. More preprocessing is added gradually to improve coverage of the training vocabulary.","fc941bfc":"### Tokenize Training Text","213d8f59":"Contractions seem to be the main issue here..","af912778":"* [https:\/\/www.kaggle.com\/sudalairajkumar\/a-look-at-different-embeddings](http:\/\/https:\/\/www.kaggle.com\/sudalairajkumar\/a-look-at-different-embeddings)\n* [https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings](http:\/\/https:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-when-using-embeddings)","bc1ad4a5":"No change. Probably because it is only a single word whose frequency is small compared to the size of the vocabulary.\n\nwhat about heights? There are some tokens that mention height such as 5'2 and 6'4.. can we convert those to a longer, more compatible, format? \n\n(Note: Different height values  show up frequently further down the list. To see it in your own notebook use show_oov_words(oov, vocab, num_to_show=100). I am only showing a small list of oov words here to make the notebook more readable) ","f79447c5":"We can replace Quorans with Quora contributors...","86cf7d9b":"### Choose Embedding","a9d6948e":"Lets convert heights to a longer format...","b06f8283":"First, does the embeddings index contain digits? (Google News replaces numbers > 9 with # signs)","f1618302":"### Tokenize Training Text","803df38a":"# Exploring Embeddings\n\nWe are now ready to explore each embedding and tokenization techniques that maximize its coverage of the training vocabulary","419a6bd8":"We can replace Quorans with Quora contributors...","adceac86":"### Acknowledgments","776be1cf":"Much better... Quorans is the top word missed now...I wonder if Quora or quora is in the embeddings_index vocabulary..","498192d4":"There are also frequent mentions of height..I wonder how that will affect the results...","8a66e87e":"Fixed that but with no effect on overall result since the qord frequency is small compared to the total vocabulary size. \n\nLets now look at the effect of switching to heights.. First, does this embedding contain numbers as tokens?","64e48262":"### Choose Embedding","29406cd4":"In this notebook We looked at three different embeddings: glove.840B.300d (GloVe), paragram_300_sl999 (paragram), and wiki-news-300d-1M (wiki) and how to best tokenize our Quora training text so that we maximize the percentage of words represented by the embeddings index.  In general, capetalization, contractions, and, to a lesser extent, height measurements, had the most impact on how much of the training vocabulary was covered by an embedding.\n\nOur base tokenization method simply defined words as sequences of letters, sequences of letters with an apostrophe somewhere in the sequence, or a puctuation mark. To reduce the amount of preprocessing nothing was removed in this method. More preprocessing was done based on our observations of which words were missed by the embedding. Coverage of an embedding was improved by about 3 percentage point for the GloVe and Wiki embeddings. The most significant improvement was for the paragram embedding which improved  36 percentage points from 50.03% to 85.97%.\n\nAwaerness of the intricacies of each embedding should help improve the accuracy of our Quora Insincere Questions Classification networks. ","e2dfc3ba":"### Functions: Embedding-Related Functions","976fc445":"good coverage but the top missing words all have contractions. We deal with those next...","ecef66ba":"## GloVe","235415f1":"## Paragram","1db42df0":"Interesting! Very few vocabulary words are in common with the paragrams vocabulary. The top missing ones all have capital letters so let us convert to lower case","ade1423f":"Slightly better. \n\nWe managed to improve the vocabulary coverage from a mere 50.03% to 85.97% by replacing capitals with lower case letters and expanding contractions. Replacing heights with a longer form improved results very slightly.","291563bc":"## Wiki","fd74ea52":"## Conclusion","04c14d3d":"### Functions:  Tokenize Training Sentences","dfa8c95f":"# Setting Up","9585f277":"Much better!  Lets deal with the contractions now...","ee9b7ef0":"## Functions\nHere I define functions that will be used repeatedly in this notebook. More functions will be added as we learn about the embeddings","107297c4":"Slightly better!\n\nOverall coverage for the Wiki embeddings improved from 79.36% to 82.09%. The main issue in this embedding was dealing with contractions. As with the other two embeddings, replacing height with a longer format had a minor effect on the overall result.","09b86e84":"Good...So let us replace all heights of the form \\d\\'\\d such as 5'4 with the \"5 foot 4\".."}}