{"cell_type":{"c98e68c0":"code","76b3d35c":"code","4ca857b2":"code","c219b835":"code","1198196d":"code","239751a5":"code","76fde8f5":"code","678f28ab":"code","56ce3346":"code","5b3d472b":"code","96615108":"code","7aa1fab5":"code","36d09aa0":"code","df4d7d58":"code","dde44c8a":"code","ca23e788":"code","cab65855":"code","dc3a2d16":"code","a91ebcdc":"code","89970ea7":"code","7c1eaad7":"code","9e61e7f0":"code","c2c351f1":"code","1ec29ddc":"code","542a28ac":"code","df2f8357":"code","de01f93a":"code","b23354c2":"code","4bf9ed78":"code","0d77222a":"code","6a95e820":"code","666f56cd":"code","5d83c2f8":"code","b2f9f48b":"code","d2c700ad":"code","f506aa26":"code","793463fd":"code","6a44f64c":"code","a43f6768":"code","a8f95678":"markdown","2be3df9f":"markdown","16678afb":"markdown","27fc88f7":"markdown","13ea1162":"markdown","42e3621f":"markdown","80b24f3d":"markdown","8103f626":"markdown","555cabfd":"markdown","3202b599":"markdown","165f3d86":"markdown","99429f32":"markdown","09d10a99":"markdown","a517522d":"markdown","0264fd00":"markdown","967aa089":"markdown","a11249ef":"markdown","0b216c7a":"markdown","3b3a7b54":"markdown","59ca623d":"markdown","24db18b9":"markdown","52e3571a":"markdown","9198ee1b":"markdown","dfabe2d9":"markdown","5553ca2f":"markdown","94b4dfc5":"markdown","138a4b64":"markdown","c8bfe32e":"markdown","c14d5b7f":"markdown","610561fd":"markdown","194dd334":"markdown","f560c93c":"markdown","aec7e595":"markdown","f1622226":"markdown","938f7629":"markdown","cd47cceb":"markdown","d5ef09be":"markdown","4e1b97a8":"markdown","52f0e410":"markdown","1de8dcb9":"markdown","b106941a":"markdown","720a3893":"markdown"},"source":{"c98e68c0":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n \n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#model selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,roc_auc_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\n\n#preprocess.\nfrom keras.preprocessing.image import ImageDataGenerator\n\n#dl libraraies\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Dense , merge\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.utils import to_categorical\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.callbacks import ReduceLROnPlateau\n\n\nfrom keras.layers.merge import dot\nfrom keras.models import Model\n\n\n# specifically for deeplearning.\nfrom keras.layers import Dropout, Flatten,Activation,Input,Embedding\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nimport tensorflow as tf\nimport random as rn\nfrom IPython.display import SVG\n \n# specifically for manipulating zipped images and getting numpy arrays of pixel values of images.\nimport cv2                  \nimport numpy as np  \nfrom tqdm import tqdm\nimport os                   \nfrom random import shuffle  \nfrom zipfile import ZipFile\nfrom PIL import Image\n\n\n#TL pecific modules\nfrom keras.applications.vgg16 import VGG16","76b3d35c":"train=pd.read_csv(r'..\/input\/movielens100k\/ratings.csv')","4ca857b2":"df=train.copy()","c219b835":"df.head()","1198196d":"df['userId'].unique()","239751a5":"len(df['userId'].unique())","76fde8f5":"df['movieId'].unique()","678f28ab":"len(df['movieId'].unique())","56ce3346":"df['userId'].isnull().sum()","5b3d472b":"df['rating'].isnull().sum()","96615108":"df['movieId'].isnull().sum()","7aa1fab5":"df['rating'].min() # minimum rating","36d09aa0":"df['rating'].max() # maximum rating","df4d7d58":"df.userId = df.userId.astype('category').cat.codes.values\ndf.movieId = df.movieId.astype('category').cat.codes.values","dde44c8a":"df['userId'].value_counts(ascending=True)","ca23e788":"df['movieId'].unique()","cab65855":"# creating utility matrix.\nindex=list(df['userId'].unique())\ncolumns=list(df['movieId'].unique())\nindex=sorted(index)\ncolumns=sorted(columns)\n \nutil_df=pd.pivot_table(data=df,values='rating',index='userId',columns='movieId')\n# Nan implies that user has not rated the corressponding movie.","dc3a2d16":"util_df","a91ebcdc":"util_df.fillna(0)","89970ea7":"# x_train,x_test,y_train,y_test=train_test_split(df[['userId','movieId']],df[['rating']],test_size=0.20,random_state=42)\nusers = df.userId.unique()\nmovies = df.movieId.unique()\n\nuserid2idx = {o:i for i,o in enumerate(users)}\nmovieid2idx = {o:i for i,o in enumerate(movies)}","7c1eaad7":"df['userId'] = df['userId'].apply(lambda x: userid2idx[x])\ndf['movieId'] = df['movieId'].apply(lambda x: movieid2idx[x])\nsplit = np.random.rand(len(df)) < 0.8\ntrain = df[split]\nvalid = df[~split]\nprint(train.shape , valid.shape)","9e61e7f0":"n_movies=len(df['movieId'].unique())\nn_users=len(df['userId'].unique())\nn_latent_factors=64  # hyperparamter to deal with. ","c2c351f1":"user_input=Input(shape=(1,),name='user_input',dtype='int64')","1ec29ddc":"user_embedding=Embedding(n_users,n_latent_factors,name='user_embedding')(user_input)\n#user_embedding.shape","542a28ac":"user_vec =Flatten(name='FlattenUsers')(user_embedding)\n#user_vec.shape","df2f8357":"movie_input=Input(shape=(1,),name='movie_input',dtype='int64')\nmovie_embedding=Embedding(n_movies,n_latent_factors,name='movie_embedding')(movie_input)\nmovie_vec=Flatten(name='FlattenMovies')(movie_embedding)\n#movie_vec","de01f93a":"sim=dot([user_vec,movie_vec],name='Simalarity-Dot-Product',axes=1)\nmodel =keras.models.Model([user_input, movie_input],sim)\n# #model.summary()\n# # A summary of the model is shown below-->","b23354c2":"model.compile(optimizer=Adam(lr=1e-4),loss='mse')","4bf9ed78":"train.shape\nbatch_size=128\nepochs=50","0d77222a":"History = model.fit([train.userId,train.movieId],train.rating, batch_size=batch_size,\n                              epochs =epochs, validation_data = ([valid.userId,valid.movieId],valid.rating),\n                              verbose = 1)","6a95e820":"from pylab import rcParams\nrcParams['figure.figsize'] = 10, 5\nimport matplotlib.pyplot as plt\nplt.plot(History.history['loss'] , 'g')\nplt.plot(History.history['val_loss'] , 'b')\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.grid(True)\nplt.show()","666f56cd":"n_latent_factors=50\nn_movies=len(df['movieId'].unique())\nn_users=len(df['userId'].unique())","5d83c2f8":"user_input=Input(shape=(1,),name='user_input',dtype='int64')\nuser_embedding=Embedding(n_users,n_latent_factors,name='user_embedding')(user_input)\nuser_vec=Flatten(name='FlattenUsers')(user_embedding)\nuser_vec=Dropout(0.40)(user_vec)","b2f9f48b":"movie_input=Input(shape=(1,),name='movie_input',dtype='int64')\nmovie_embedding=Embedding(n_movies,n_latent_factors,name='movie_embedding')(movie_input)\nmovie_vec=Flatten(name='FlattenMovies')(movie_embedding)\nmovie_vec=Dropout(0.40)(movie_vec)","d2c700ad":"sim=dot([user_vec,movie_vec],name='Simalarity-Dot-Product',axes=1)","f506aa26":"nn_inp=Dense(96,activation='relu')(sim)\nnn_inp=Dropout(0.4)(nn_inp)\n# nn_inp=BatchNormalization()(nn_inp)\nnn_inp=Dense(1,activation='relu')(nn_inp)\nnn_model =keras.models.Model([user_input, movie_input],nn_inp)\nnn_model.summary()\n","793463fd":"nn_model.compile(optimizer=Adam(lr=1e-3),loss='mse')","6a44f64c":"batch_size=128\nepochs=20","a43f6768":"History = nn_model.fit([train.userId,train.movieId],train.rating, batch_size=batch_size,\n                              epochs =epochs, validation_data = ([valid.userId,valid.movieId],valid.rating),\n                              verbose = 1)","a8f95678":"#### Now let us focus on the other main thing!!! Using a NN to matrix factorization.\n\n1) Note that this way is not much different from the previous approach.\n\n2) The main difference is that we have used Fully Connected layers as well as the Dropout layers and the BatchNormalization layers.\n\n3) The number of units and the number of layers etc.. are the hyperparametrs here as in a traditional neural network.\n\n","2be3df9f":"<a id=\"content5\"><\/a>\n## 5 ) Using a Neural Network","16678afb":"####  Similary playing with no of latent factors,  other parameters in the model architecture can give to even better results!!!!!","27fc88f7":"## 3.2 ) Compiling the Model","13ea1162":"## 2.2 ) Creating the Utility Matrix","42e3621f":"Note that the metrics used is 'Mean squared Error'. Our aim is to minimize the mse on the training set ie over the values which the user has rated (100004 ratings).","80b24f3d":"[ **1 ) Exploratory the Data**](#content1)","8103f626":"I am using the movie-lens-100K data. Note that this file contains the ratings given by our set of users to different movies. In all it contains total 100K ratings; to be exact 1000004.","555cabfd":"<a id=\"content1\"><\/a>\n## 1 ) Exploring the Data","3202b599":"## [ Please star\/upvote in case you like it. ]","165f3d86":"This confirms that none of the columns has any NULL or Nan value.","99429f32":"#### Notice the summary of the model and also the architecture of the model which u can tune of course.","09d10a99":"[ **5 ) Using a Neural Network**](#content5)","a517522d":"## 5.2 ) Specifying the Model architecture","0264fd00":"[ **2 ) Preparing the Data**](#content2)","967aa089":"## 1.1 ) Importing Various Modules","a11249ef":"Similarly we have 9066 unique movies, Also note that as provided each user has voted for atleast 20 movies. We will see that the utility matrix thus created thus is quite sparse.","0b216c7a":"<a id=\"content3\"><\/a>\n## 3 ) Matrix Factorization","3b3a7b54":"#### BREAKING IT DOWN--\n\n1) First we need to create embeddings for both the user as well as the item or movie. For this I have used the Embedding layer from keras.\n\n2) Specify the input expected to be embedded (Both in user and item embedding). The use a Embedding layer which expects the no of latent factors in the resulting embedding and also the no of users or items.\n\n3) Then we take the 'Dot-Product' of both the embeddings using the 'merge' layer. Note that 'dot-product' is just a measure of simalrity and we can use any other mode like 'mulitply' or 'cosine simalarity' or 'concatenate' etc...\n\n4) Lastly we make a Keras model from the specified details.\n","59ca623d":"#### Here comes the main part!!!      \n\n1) Now we move on to the crux of the notebook ie Matrix Factorization. In matrix facorization, we basically break a matrix into usually 2 smaller matrices each with smaller dimensions. these matrices are oftem called 'Embeddings'.  We can have variants of Matrix Factorizartion-> 'Low Rank MF' , 'Non-Negaive MF' (NMF) and so on..  \n\n2) Here I  have used the so called 'Low Rank Matrix Factorization'.  I have created  embeddings for both user as well as the item; movie in our case. The number of dimensions or the so called 'Latent Factors' in the embeddings is a hyperparameter to deal with in this implementation of Collaborative Filtering.                                                  ","24db18b9":"## 5.3 ) Compiling the Model","52e3571a":"## Collaborative Filtering Based Recommender Systems using Low Rank Matrix Factorization(User & Movie Embeddings) & Neural Network in Keras.","9198ee1b":"<a id=\"content2\"><\/a>\n## 2 ) Preparing the data","dfabe2d9":"## THE END!!!","5553ca2f":"#### Note that the validation loss is close to 0.84 which is quite decent. Also note that it has decrreased from 1.26 in the case of normal Matrix Factorization to this value here.","94b4dfc5":"<a id=\"content4\"><\/a>\n## 4 ) Evaluating the Model Performance","138a4b64":"## CONTENTS::->","c8bfe32e":"[ **4 ) Evaluating the Model Performance**](#content4)","c14d5b7f":"## 2.1 ) Encoding the columns","610561fd":"## 5. 4) Fitting on Training set & Validating on Validation Set.","194dd334":"#### BREAKING IT DOWN--\n\n1) This is the utility matrix; for each of the 671 users arranged rowwise; each column shows the rating of the movie given by a particular user.\n\n2) Note that majority of the matrix is filled with 'Nan' which shows that majority of the movies are unrated by many users.\n\n3) For each movie-user pair if the entry is NOT 'Nan' the vaue indicates the rating given by user to that corressponding movie. \n\n4) For now I am gonna fill the 'Nan' value with value '0'. But note that this just is just indicative, a 0 implies NO RATING and doesn't mean that user has rated 0 to that movie. It doesn't at all represent any rating.","f560c93c":"## 2.3 ) Creating Training and Validation Sets.","aec7e595":"#### Note that for 671 users and 9066 movies we can have a maximum of 671*9066 = 6083286 ratings. But note that we have only 100004 ratings with us. Hence the utility matrix has only about 1.6 % of the total values. Thus it can be concluded that it is quite sparse. This limits the use of some algorithms. Hence we will create embeddings for them later.","f1622226":"## [ Please star\/upvote if u liked it. ]","938f7629":"## 1.3 ) Exploring the dataset","cd47cceb":"## 1.2 ) Reading the CSV file","d5ef09be":"## 3.3 ) Fitting on Training set & Validating on Validation Set.","4e1b97a8":"## 3.1 ) Creating the Embeddings ,Merging and Making the Model from Embeddings","52f0e410":"## 5.1 ) Creating the Embeddings","1de8dcb9":"####  Note that I have used 50 latent factors as that seems to give reasonable performance. Furhter tuning and careful optimization can give even better results.","b106941a":"[ **3 ) Matrix Factorization**](#content3)","720a3893":"Note that in total we have 671 unique users whose userid range from 1->671."}}