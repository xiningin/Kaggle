{"cell_type":{"02d7e368":"code","6d335649":"code","9015de8d":"code","b39a2cf8":"code","41add10f":"code","3582e347":"code","b4d5e2a3":"code","31600801":"code","433d8161":"code","ec33c252":"code","e840785b":"code","5a7fa1db":"code","60dfc2ec":"code","8bfebc22":"code","d30aae01":"code","ee15e2ab":"code","bb921476":"code","c6aee1c1":"code","643ae7ad":"code","69e53bee":"code","8bc0cd67":"code","90d7e88d":"code","a36de9c9":"code","8ab2e14b":"code","cfe1904e":"code","4d11f28a":"code","3ac049f5":"code","ecc5c101":"code","2a9b76e2":"code","613e87b2":"code","a796b829":"code","7a79205b":"code","69004ef4":"markdown","25dde8d5":"markdown","96695713":"markdown","6e3a31f4":"markdown","8207be5a":"markdown","6418f2da":"markdown","f9611153":"markdown"},"source":{"02d7e368":"import re\nimport nltk\nimport os\nimport time\nimport spacy\n\n\nfrom nltk.corpus import words as english_words, stopwords\nfrom nltk.stem import PorterStemmer # suffixes (\u201cing\u201d, \u201cly\u201d, \u201ces\u201d, \u201cs\u201d etc) from a word\nfrom nltk.stem import WordNetLemmatizer # Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.metrics.association import BigramAssocMeasures, TrigramAssocMeasures\nfrom nltk.collocations import BigramCollocationFinder, TrigramCollocationFinder \nfrom collections import Counter \nfrom wordcloud import WordCloud # Make a word cloud with a single word that\u2019s repeated\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","6d335649":"pip install textstat","9015de8d":"files = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        with open((os.path.join(dirname, filename))) as w: files.append(w.read())","b39a2cf8":"names=[ x.replace('.txt','') for x in filenames]","41add10f":"months=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\ncity, date, month=[],[],[]\nfor n in names:\n    index=-1\n    for m in months:\n        index=n.find(m)\n        if index!=-1:\n            month.append(m)\n            break\n    city.append(n[:index])\n    date.append(n[index+3:])","3582e347":"# parsing date\nday,year=[],[]\nday=[x.split(\"_\")[0] for x in date]\nyear=[x.split(\"_\")[1] for x in date]\ndate=[x+y+z for x,y,z in zip(day, month, year)]\nnew_date=pd.to_datetime(date)","b4d5e2a3":"cities=[re.sub('([A-Z])',r' \\1', c ) for c in city]","31600801":"data=pd.DataFrame(data={'City':cities,'Date':new_date, 'Speech':files})","433d8161":"data.head()","ec33c252":"def speech_to_words (speech, remove_stopwords=False):\n    cln_speech=re.sub('[^a-zA-Z]',\" \", speech) # remove non-letters \n    words=cln_speech.lower().split()       # convert to small letters and split to words \n    if remove_stopwords:                       # remove stop words\n        stop=stopwords.words('english')\n        stop.extend(['from', 'subject', 're', 'edu', 'use', 'going', 'said', 'you','and'])\n        stop=(stop)\n        words=[w for w in words if not w in stop]\n    stemmer=nltk.PorterStemmer()               # remove suffixes (\u201cing\u201d, \u201cly\u201d, \u201ces\u201d, \u201cs\u201d etc)\n    clean_words=[stemmer.stem(w) for w in words]\n    lemmatizer= WordNetLemmatizer()            # procedure of obtaining the root form of the word\n    simple_words=[lemmatizer.lemmatize(w, pos='a') for w in clean_words]\n    return simple_words","e840785b":"def speech_to_words2 (speech, remove_stopwords=False): # without stemmer &  lematizer\n    cln_speech=re.sub('[^a-zA-Z]',\" \", speech) # remove non-letters \n    words=cln_speech.lower().split()       # convert to small letters and split to words \n    if remove_stopwords:                       # remove stop words\n        stop=stopwords.words('english')\n        stop.extend(['from', 'subject', 're', 'edu', 'use', 'going', 'said', 'you','and', 'wow'])\n        stop=(stop)\n        words=[w for w in words if not w in stop]\n    return words","5a7fa1db":"tokenizer=nltk.data.load('tokenizers\/punkt\/english.pickle')","60dfc2ec":"def speech_to_sentences (speech, tokenizer, remove_stopwords=False):\n    raw_sentences=tokenizer.tokenize(speech)  # Use the NLTK tokenizer to split the paragraph into sentences\n    sentences=[]\n    for sentence in raw_sentences:\n        if len(sentence)>0:\n            sentences.append(speech_to_words2(sentence, remove_stopwords))\n    return sentences","8bfebc22":"speech_words=data['Speech'].apply(lambda speech:speech_to_words (speech, remove_stopwords=True))\ndata['words_in_speech']=data['Speech'].apply(lambda x: len(x.split(\" \")))\n\nspeech_sentences=data['Speech'].apply(lambda speech: speech_to_sentences (speech,tokenizer, remove_stopwords=True))\nfor i in speech_sentences.index: # clear from empty lists\n    speech_sentences[i]=[x for x in speech_sentences[i] if x!=[]]\ndata['sentences_in_speech']=speech_sentences.apply(lambda x: len(x))","d30aae01":"print(f'In all speeches are: {data.words_in_speech.sum()} words and {data.sentences_in_speech.sum()} sentences.')\n","ee15e2ab":"sentence_list=[] # how many sentences in all speeches\nfor sentence in speech_sentences:\n    sentence_list = sentence_list + sentence\n\nall_words=[]     # how many words in all speeches\nfor word in speech_words:\n    all_words=all_words+word","bb921476":"word_freq = Counter(all_words) # lets try to count the top occuring terms in speeches","c6aee1c1":"Biden=word_freq['biden']\nprint(f'Donald Trump mentioned Biden {Biden} times in his speeches.')","643ae7ad":"# lets try to vizualize the top occuring terms\nwords = [word[0] for word in word_freq.most_common(20)]   # top 20 most common words\ncount = [word[1] for word in word_freq.most_common(20)]  #how many time they use in speech\n\nplt.figure(figsize=(15, 12))\nplt.bar(words, count)\nplt.xticks(words, rotation=90)\nplt.title('Most common words', fontsize=15)\nplt.show()","69e53bee":"# Plotting wordCloud with most common words\nwc = WordCloud(max_words = 1000, width = 1600, height = 800, background_color ='white',repeat=True)\nwc.fit_words(word_freq)\n\nplt.figure(figsize = (15, 15), facecolor = None)\nplt.axis(\"off\")\nplt.imshow(wc, interpolation='bilinear')\nplt.title('Most common words', fontsize=15)\nplt.show()","8bc0cd67":"text=\" \".join(files)\ncl_text=speech_to_words2(text, remove_stopwords=True)\n\nbigram_measures = BigramAssocMeasures()\nbi_finder = BigramCollocationFinder.from_words(cl_text)\nbi_phrases=bi_finder.nbest(bigram_measures.raw_freq, 10)\nbi_phrases=list((x+\" \"+y) for x,y in bi_phrases)\nprint(f'The most frequent 2 words phrases in Donald Trump rallies are: \\n\\n{bi_phrases} \\n\\n')\n\ntrigram_measures = TrigramAssocMeasures()\ntri_finder = TrigramCollocationFinder.from_words(cl_text)\ntri_phrases=tri_finder.nbest(trigram_measures.raw_freq, 10)\ntri_phrases=list((x+\" \"+ y + \" \"+ z) for x, y, z, in  tri_phrases)\nprint(f'The most frequent 3 words phrases in Donald Trump rallies are: \\n\\n{tri_phrases} ')","90d7e88d":"import textstat\n\n# Returns the Flesch Reading Ease Score\ndata['reading_score']=data['Speech'].apply(lambda x: textstat.flesch_reading_ease(x))\nprint(f'Mean Flesch Reading Easy Score is : {data.reading_score.mean()}')","a36de9c9":"plt.figure(figsize=(16,8))\nnames = data['City'].values\ndif=[\"Very Confusing\", \"Difficult\", \"Fairly Difficult\", \"Standard\",\"Fairly Easy\", \"Easy\", \"Very Easy\"]\ngrade= [0, 30,50, 60, 70, 80, 90]\nbarWidth = 0.9\nplt.bar(names, data.reading_score,width=barWidth)\nplt.xticks(names, rotation=90)\nplt.yticks(grade, dif)\nplt.legend(fontsize=15) \nplt.show()","8ab2e14b":"s_analyzer=SentimentIntensityAnalyzer()\nsentiments=data['Speech'].apply(lambda x: s_analyzer.polarity_scores(x))\ndata['negative_sentiment']=sentiments.apply(lambda x : x['neg'])\ndata['neutral_sentiment']=sentiments.apply(lambda x : x['neu'])\ndata['positive_sentiment']=sentiments.apply(lambda x : x['pos'])","cfe1904e":"plt.figure(figsize=(16,8))\nnames = data['City'].values\nbarWidth = 0.9\n\n# Create negative Bars\nplt.bar( names, data.negative_sentiment, color='#f23838', \\\n        edgecolor='white', width=barWidth, label='Negative')\n# Create neutral Bars\nplt.bar( names,data.neutral_sentiment, bottom=data.negative_sentiment, color='#5676f5', \n        edgecolor='white', width=barWidth, label='Neutral')\n# Create positive Bars\nplt.bar( names,data.positive_sentiment, \\\n        bottom=[i+j for i,j in zip(data.negative_sentiment, data.neutral_sentiment)], \\\n        color='#53fc6d', edgecolor='white', width=barWidth, label='Positive')\n \nplt.xticks(names, rotation=90)\nplt.xlabel(\"City\", fontsize=15)\nplt.ylabel(\"Sentiment Intensity\", fontsize=15)\nplt.legend(fontsize=15) \nplt.show()\n\n","4d11f28a":"# find and create list of all negative words in speeches\nnegative_words=[]\nfor word in cl_text:\n    if s_analyzer.polarity_scores(word)['compound']<0:\n        negative_words.append(word)\n        \nneg_words=Counter(negative_words)","3ac049f5":"wc1 = WordCloud(max_words = 200, width = 1600, height = 800, background_color ='black',repeat=True)\nwc1.fit_words(neg_words)\n\nplt.figure(figsize = (15, 10), facecolor = None)\nplt.axis(\"off\")\nplt.imshow(wc1, interpolation='bilinear')\nplt.title('Most common negative words', fontsize=15)\nplt.show()","ecc5c101":"# find and create list of all positive words in speeches\npositive_words=[]\nfor word in cl_text:\n    if s_analyzer.polarity_scores(word)['compound']>0:\n        positive_words.append(word)\n        \npos_words=Counter(positive_words)","2a9b76e2":"wc2 = WordCloud(max_words = 200, width = 1600, height = 800, background_color ='white',repeat=True)\nwc2.fit_words(pos_words)\n\nplt.figure(figsize = (15, 10), facecolor = None)\nplt.axis(\"off\")\nplt.imshow(wc2, interpolation='bilinear')\nplt.title('Most common positive words', fontsize=15)\nplt.show()","613e87b2":"# spaCy uses a statistical model to classify a broad range of entities, \n# including persons, organisations, dates\n\nnlp = spacy.load('en')\ndoc = nlp(data['Speech'][0])\nspacy.displacy.render(doc, style='ent',jupyter=True)","a796b829":"doc = nlp(data['Speech'][0]) # first speech sentence segmentation\nsentence_l = [s for s in doc.sents]\n\n# spaCy features a fast and accurate syntactic dependency parser, \n# and has a rich API for navigating the tree.\nspacy.displacy.render(sentence_l[3], style='dep',jupyter=True,options = {'compact':60})\npos_list = [(token, token.pos_) for token in sentence_l[3]]","7a79205b":"class SpacyMagic(object):\n    _spacys = {}\n\n    @classmethod\n    def get(cls, lang):\n        if lang not in cls._spacys:\n            import spacy\n            cls._spacys[lang] = spacy.load(lang, disable=['tagger', 'ner','pos'])\n        return cls._spacys[lang]\n    \ndef run_spacy(text):\n    nlp = SpacyMagic.get('en')\n    doc = nlp(text)\n    return doc\n\ndef clean_text(inp):\n    spacy_text = run_spacy(inp)\n    out_str= ' '.join ([token.lemma_ for token in spacy_text if token.is_stop != True and token.is_punct != True\\\n                        and token.is_alpha ==True])\n    return out_str\n\nfor i in data.index:\n    data.loc[i,'spacy_text'] = clean_text(data.loc[i,'Speech'])","69004ef4":"### After Sentiment analysis we also can see that emotionally more positive speech was in Texas","25dde8d5":"### POS tagging and Dependency Parsing","96695713":"### In general, Trump's speech is quite simple and understandable, because it is written for a wide range of people. But the analysis shows that in Texas the readability was more difficult compared to other cities.","6e3a31f4":"## Calculating the readability index","8207be5a":"# Sentiment analysis of Donald Trump","6418f2da":"# Most frequent words","f9611153":"# spaCy\n\n### Named Entity Recognition"}}