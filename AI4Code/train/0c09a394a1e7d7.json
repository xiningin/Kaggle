{"cell_type":{"47172e8f":"code","ad38e9bc":"code","ad5db89e":"code","ed959289":"code","f32006ea":"code","b10677e4":"markdown","1ab2b805":"markdown","50b3018a":"markdown"},"source":{"47172e8f":"import gensim\nfrom gensim.models import Word2Vec\n\nfrom sklearn.decomposition import PCA\n\nimport pandas as pd\n\nimport plotly.express as px","ad38e9bc":"from plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)","ad5db89e":"model = Word2Vec.load(\"\/kaggle\/input\/wallstreetbets-named-entity-recognition\/word_vecs_processed.model\")\nvocab = list(model.wv.key_to_index)\nX = model.wv[vocab]\n","ed959289":"pca = PCA(n_components =2)\nX_pca = pca.fit_transform(X)\ndf = pd.DataFrame(X_pca, index = vocab, columns = ['x','y'])\ndf['vocab'] = df.index\ndf = df.head(1000)","f32006ea":"fig = px.scatter(df, x = 'x', y = 'y', text = df['vocab'])\n\nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=900)","b10677e4":"# What are we looking at?","1ab2b805":"# First lets do our imports!","50b3018a":"What this notebook is doing is taking our word embeddings, aka word vectors, which are 350 dimensional arrays representing each word in our corpus that has been used more than 10 times. \n\nThe word2vec algorithm is basically an unsupervised machine learning algorithm that groups words \"near\" each other based on their usage in the text. The closer the words appear together, the more often they are used together in context.\n\nSome noteable groupings you can see: chunked links, represnted by words like \"https, www, jpg\" etc. This cluster is off the the side and removed from the general conversation.\n\nYou can see an options cluster by rotating the graph, words like \"expiration, expiry, strike, premium, contracts, puts, calls, eod\" all appear together."}}