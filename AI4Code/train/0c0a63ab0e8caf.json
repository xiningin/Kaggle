{"cell_type":{"8161d363":"code","8ad34369":"code","0406ba4e":"code","5f753a90":"code","43628513":"code","5a8edb09":"code","6c741183":"code","a7e99c6b":"code","54d9d15a":"code","b150be61":"code","3395bb44":"code","eb181240":"code","5cdb57d1":"code","9928bc52":"code","bba9c205":"code","d1830182":"code","df8717d0":"code","6851f3d4":"code","d86ca9e2":"code","5338164a":"code","3ca64bc8":"code","e2d6f47f":"code","3831ba09":"code","85a69c66":"code","6bf22cad":"code","8097cc42":"code","4ccc31d9":"code","c95651f8":"code","9b10ada3":"code","32d80070":"code","9aa3e0f4":"markdown","0d4ec4f4":"markdown","4c36e758":"markdown","8aae2e8c":"markdown","b4e80f75":"markdown","8186e216":"markdown","e6286edd":"markdown","d7caf93c":"markdown","e91ef7c1":"markdown","ad71d3f1":"markdown","a6b4cd81":"markdown","a9ccaab6":"markdown","a8e1f7c7":"markdown","3ec4a58e":"markdown","ae06c2cb":"markdown","fcb62319":"markdown","40083231":"markdown","69cda928":"markdown","4953371a":"markdown","b1ca0c9b":"markdown","6c10a5c7":"markdown","79a82be3":"markdown","e0e83b58":"markdown","501dcda3":"markdown","77848c23":"markdown","26e2abdd":"markdown","e26b4de7":"markdown"},"source":{"8161d363":"!pip install torchsummary","8ad34369":"from IPython.core.interactiveshell import InteractiveShell\nimport seaborn as sns\n# PyTorch\nimport torchvision\nfrom torchvision import transforms, datasets, models\nimport torch\nfrom torch import optim, cuda\nfrom torch.utils.data import DataLoader, sampler\nimport torch.nn as nn\n\nimport warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Data science tools\nimport numpy as np\nimport pandas as pd\nimport os\n\n# Image manipulations\nfrom PIL import Image\n# Useful for examining network\nfrom torchsummary import summary\n# Timing utility\nfrom timeit import default_timer as timer\n\n# Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['font.size'] = 14\n\n# Printing out all outputs\nInteractiveShell.ast_node_interactivity = 'all'\n\nimport os\nprint(os.listdir(\"..\/input\"))","0406ba4e":"from subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\/chest_xray\/chest_xray\"]).decode(\"utf8\"))","5f753a90":"datadir = '..\/input\/chest_xray\/chest_xray'\ntraindir = datadir + '\/train\/'\nvaliddir = datadir + '\/val\/'\ntestdir = datadir + '\/test\/'\n\nsave_file_name = 'vgg16-chest-4.pt'\ncheckpoint_path = 'vgg16-chest-4.pth'\n\n# Change to fit hardware\nbatch_size = 128\n\n# Whether to train on a gpu\ntrain_on_gpu = cuda.is_available()\nprint(f'Train on gpu: {train_on_gpu}')\n\n# Number of gpus\nif train_on_gpu:\n    gpu_count = cuda.device_count()\n    print(f'{gpu_count} gpus detected.')\n    if gpu_count > 1:\n        multi_gpu = True\n    else:\n        multi_gpu = False","43628513":"# Empty lists\ncategories = []\nimg_categories = []\nn_train = []\nn_valid = []\nn_test = []\nhs = []\nws = []\n\n# Iterate through each category\nfor d in os.listdir(traindir):\n    if not d.startswith('.'):\n        categories.append(d)\n\n        # Number of each image\n        train_imgs = os.listdir(traindir + d)\n        valid_imgs = os.listdir(validdir + d)\n        test_imgs = os.listdir(testdir + d)\n        n_train.append(len(train_imgs))\n        n_valid.append(len(valid_imgs))\n        n_test.append(len(test_imgs))\n\n        # Find stats for train images\n        for i in train_imgs:\n            if not i.startswith('.'):\n                img_categories.append(d)\n                img = Image.open(traindir + d + '\/' + i)\n                img_array = np.array(img)\n                # Shape\n                hs.append(img_array.shape[0])\n                ws.append(img_array.shape[1])\n\n# Dataframe of categories\ncat_df = pd.DataFrame({'category': categories,\n                       'n_train': n_train,\n                       'n_valid': n_valid, 'n_test': n_test}).\\\n    sort_values('category')\n\n# Dataframe of training images\nimage_df = pd.DataFrame({\n    'category': img_categories,\n    'height': hs,\n    'width': ws\n})\n\n#cat_df.sort_values('n_train', ascending=False, inplace=True)\ncat_df.set_index('category')['n_train'].plot.bar(\n    color='c', figsize=(20, 6))\nplt.xticks(rotation=0)\nplt.ylabel('Count')\nplt.title('Training Images by Category')","5a8edb09":"img_dsc = image_df.groupby('category').mean()\nplt.figure(figsize=(10, 6))\nsns.kdeplot(\n    img_dsc['height'], label='Average Height')\nsns.kdeplot(\n    img_dsc['width'], label='Average Width')\nplt.xlabel('Pixels')\nplt.ylabel('Density')\nplt.title('Average Size Distribution')","6c741183":"def imshow(image):\n    \"\"\"Display image\"\"\"\n    plt.figure(figsize=(6, 6))\n    plt.imshow(image)\n    plt.axis('off')\n    plt.show()\n\n\n","a7e99c6b":"x = Image.open(traindir + 'NORMAL\/IM-0128-0001.jpeg')\nnp.array(x).shape\nimshow(x)","54d9d15a":"x = Image.open(traindir + 'PNEUMONIA\/person1001_bacteria_2932.jpeg')\nnp.array(x).shape\nimshow(x)","b150be61":"image_transforms = {\n    # Train uses data augmentation\n    'train':\n    transforms.Compose([\n        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n        transforms.RandomRotation(degrees=15),\n        transforms.ColorJitter(),\n        transforms.RandomHorizontalFlip(),\n        transforms.CenterCrop(size=224),  # Image net standards\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])  # Imagenet standards\n    ]),\n    # Validation does not use augmentation\n    'val':\n    transforms.Compose([\n        transforms.Resize(size=256),\n        transforms.CenterCrop(size=224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    # Test does not use augmentation\n    'test':\n    transforms.Compose([\n        transforms.Resize(size=256),\n        transforms.CenterCrop(size=224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}","3395bb44":"data = {\n    'train':\n    datasets.ImageFolder(root=traindir, transform=image_transforms['train']),\n    'val':\n    datasets.ImageFolder(root=validdir, transform=image_transforms['val']),\n    'test':\n    datasets.ImageFolder(root=testdir, transform=image_transforms['test'])\n}\n\n# Dataloader iterators\ndataloaders = {\n    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True),\n    'val': DataLoader(data['val'], batch_size=batch_size, shuffle=True),\n    'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True)\n}","eb181240":"trainiter = iter(dataloaders['train'])\nfeatures, labels = next(trainiter)\nfeatures.shape, labels.shape","5cdb57d1":"n_classes = len(cat_df)\nprint(f'There are {n_classes} different classes.')\n\nlen(data['train'].classes)","9928bc52":" model = models.vgg16(pretrained=True)","bba9c205":"for param in model.parameters():\n    param.requires_grad = False","d1830182":"n_inputs = model.classifier[6].in_features\n\n# Add on classifier\nmodel.classifier[6] = nn.Sequential(\n    nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.4),\n    nn.Linear(256, n_classes), nn.LogSoftmax(dim=1))\n\nmodel.classifier","df8717d0":"total_params = sum(p.numel() for p in model.parameters())\nprint(f'{total_params:,} total parameters.')\ntotal_trainable_params = sum(\n    p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'{total_trainable_params:,} training parameters.')","6851f3d4":"if train_on_gpu:\n    model = model.to('cuda')\n\nif multi_gpu:\n    model = nn.DataParallel(model)","d86ca9e2":"def get_pretrained_model(model_name):\n    \"\"\"Retrieve a pre-trained model from torchvision\n\n    Params\n    -------\n        model_name (str): name of the model (currently only accepts vgg16 and resnet50)\n\n    Return\n    --------\n        model (PyTorch model): cnn\n\n    \"\"\"\n\n    if model_name == 'vgg16':\n        model = models.vgg16(pretrained=True)\n\n        # Freeze early layers\n        for param in model.parameters():\n            param.requires_grad = False\n        n_inputs = model.classifier[6].in_features\n\n        # Add on classifier\n        model.classifier[6] = nn.Sequential(\n            nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(256, n_classes), nn.LogSoftmax(dim=1))\n\n    elif model_name == 'resnet50':\n        model = models.resnet50(pretrained=True)\n\n        for param in model.parameters():\n            param.requires_grad = False\n\n        n_inputs = model.fc.in_features\n        model.fc = nn.Sequential(\n            nn.Linear(n_inputs, 256), nn.ReLU(), nn.Dropout(0.2),\n            nn.Linear(256, n_classes), nn.LogSoftmax(dim=1))\n\n    # Move to gpu and parallelize\n    if train_on_gpu:\n        model = model.to('cuda')\n\n    if multi_gpu:\n        model = nn.DataParallel(model)\n\n    return model","5338164a":"model = get_pretrained_model('vgg16')\nif multi_gpu:\n    summary(\n        model.module,\n        input_size=(3, 224, 224),\n        batch_size=batch_size,\n        device='cuda')\nelse:\n    summary(\n        model, input_size=(3, 224, 224), batch_size=batch_size, device='cuda')","3ca64bc8":"model.class_to_idx = data['train'].class_to_idx\nmodel.idx_to_class = {\n    idx: class_\n    for class_, idx in model.class_to_idx.items()\n}\n\nlist(model.idx_to_class.items())","e2d6f47f":"criterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters())\n\nfor p in optimizer.param_groups[0]['params']:\n    if p.requires_grad:\n        print(p.shape)","3831ba09":"\ndef train(model,\n          criterion,\n          optimizer,\n          train_loader,\n          valid_loader,\n          save_file_name,\n          max_epochs_stop=3,\n          n_epochs=20,\n          print_every=2):\n    \"\"\"Train a PyTorch Model\n\n    Params\n    --------\n        model (PyTorch model): cnn to train\n        criterion (PyTorch loss): objective to minimize\n        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n        train_loader (PyTorch dataloader): training dataloader to iterate through\n        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n        save_file_name (str ending in '.pt'): file path to save the model state dict\n        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n        n_epochs (int): maximum number of training epochs\n        print_every (int): frequency of epochs to print training stats\n\n    Returns\n    --------\n        model (PyTorch model): trained cnn with best weights\n        history (DataFrame): history of train and validation loss and accuracy\n    \"\"\"\n\n    # Early stopping intialization\n    epochs_no_improve = 0\n    valid_loss_min = np.Inf\n\n    valid_max_acc = 0\n    history = []\n\n    # Number of epochs already trained (if using loaded in model weights)\n    try:\n        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n    except:\n        model.epochs = 0\n        print(f'Starting Training from Scratch.\\n')\n\n    overall_start = timer()\n\n    # Main loop\n    for epoch in range(n_epochs):\n\n        # keep track of training and validation loss each epoch\n        train_loss = 0.0\n        valid_loss = 0.0\n\n        train_acc = 0\n        valid_acc = 0\n\n        # Set to training\n        model.train()\n        start = timer()\n\n        # Training loop\n        for ii, (data, target) in enumerate(train_loader):\n            # Tensors to gpu\n            if train_on_gpu:\n                data, target = data.cuda(), target.cuda()\n\n            # Clear gradients\n            optimizer.zero_grad()\n            # Predicted outputs are log probabilities\n            output = model(data)\n\n            # Loss and backpropagation of gradients\n            loss = criterion(output, target)\n            loss.backward()\n\n            # Update the parameters\n            optimizer.step()\n\n            # Track train loss by multiplying average loss by number of examples in batch\n            train_loss += loss.item() * data.size(0)\n\n            # Calculate accuracy by finding max log probability\n            _, pred = torch.max(output, dim=1)\n            correct_tensor = pred.eq(target.data.view_as(pred))\n            correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n            # calculate test accuracy for each object class\n            '''for i in range(batch_size):       \n                label = target.data[i]\n                class_correct[label] += correct[i].item()\n                class_total[label] += 1'''\n                \n            # Need to convert correct tensor from int to float to average\n            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n            # Multiply average accuracy times the number of examples in batch\n            train_acc += accuracy.item() * data.size(0)\n            \n            # Track training progress\n            print(\n                f'Epoch: {epoch}\\t{100 * (ii + 1) \/ len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.',\n                end='\\r')\n\n        # After training loops ends, start validation\n        else:\n            model.epochs += 1\n\n            # Don't need to keep track of gradients\n            with torch.no_grad():\n                # Set to evaluation mode\n                model.eval()\n\n                # Validation loop\n                for data, target in valid_loader:\n                    # Tensors to gpu\n                    if train_on_gpu:\n                        data, target = data.cuda(), target.cuda()\n\n                    # Forward pass\n                    output = model(data)\n\n                    # Validation loss\n                    loss = criterion(output, target)\n                    # Multiply average loss times the number of examples in batch\n                    valid_loss += loss.item() * data.size(0)\n\n                    # Calculate validation accuracy\n                    _, pred = torch.max(output, dim=1)\n                    correct_tensor = pred.eq(target.data.view_as(pred))\n                    accuracy = torch.mean(\n                        correct_tensor.type(torch.FloatTensor))\n                    # Multiply average accuracy times the number of examples\n                    valid_acc += accuracy.item() * data.size(0)\n\n                # Calculate average losses\n                train_loss = train_loss \/ len(train_loader.dataset)\n                valid_loss = valid_loss \/ len(valid_loader.dataset)\n\n                # Calculate average accuracy\n                train_acc = train_acc \/ len(train_loader.dataset)\n                valid_acc = valid_acc \/ len(valid_loader.dataset)\n\n                history.append([train_loss, valid_loss, train_acc, valid_acc])\n\n                # Print training and validation results\n                if (epoch + 1) % print_every == 0:\n                    print(\n                        f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n                    )\n                    print(\n                        f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n                    )\n\n                # Save the model if validation loss decreases\n                if valid_loss < valid_loss_min:\n                    # Save model\n                    torch.save(model.state_dict(), save_file_name)\n                    # Track improvement\n                    epochs_no_improve = 0\n                    valid_loss_min = valid_loss\n                    valid_best_acc = valid_acc\n                    best_epoch = epoch\n\n                # Otherwise increment count of epochs with no improvement\n                else:\n                    epochs_no_improve += 1\n                    # Trigger early stopping\n                    if epochs_no_improve >= max_epochs_stop:\n                        print(\n                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n                        )\n                        total_time = timer() - overall_start\n                        print(\n                            f'{total_time:.2f} total seconds elapsed. {total_time \/ (epoch+1):.2f} seconds per epoch.'\n                        )\n\n                        # Load the best state dict\n                        model.load_state_dict(torch.load(save_file_name))\n                        # Attach the optimizer\n                        model.optimizer = optimizer\n\n                        # Format history\n                        history = pd.DataFrame(\n                            history,\n                            columns=[\n                                'train_loss', 'valid_loss', 'train_acc',\n                                'valid_acc'\n                            ])\n                        return model, history\n\n    # Attach the optimizer\n    model.optimizer = optimizer\n    # Record overall time and print out stats\n    total_time = timer() - overall_start\n    print(\n        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n    )\n    print(\n        f'{total_time:.2f} total seconds elapsed. {total_time \/ (epoch):.2f} seconds per epoch.'\n    )\n    # Format history\n    history = pd.DataFrame(\n        history,\n        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n    return model, history","85a69c66":"model, history = train(\n    model,\n    criterion,\n    optimizer,\n    dataloaders['train'],\n    dataloaders['val'],\n    save_file_name=save_file_name,\n    max_epochs_stop=5,\n    n_epochs=10,\n    print_every=2)","6bf22cad":"plt.figure(figsize=(8, 6))\nfor c in ['train_loss', 'valid_loss']:\n    plt.plot(\n        history[c], label=c)\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Average Negative Log Likelihood')\nplt.title('Training and Validation Losses')","8097cc42":"plt.figure(figsize=(8, 6))\nfor c in ['train_acc', 'valid_acc']:\n    plt.plot(\n        100 * history[c], label=c)\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Average Accuracy')\nplt.title('Training and Validation Accuracy')","4ccc31d9":"def save_checkpoint(model, path):\n    \"\"\"Save a PyTorch model checkpoint\n\n    Params\n    --------\n        model (PyTorch model): model to save\n        path (str): location to save model. Must start with `model_name-` and end in '.pth'\n\n    Returns\n    --------\n        None, save the `model` to `path`\n\n    \"\"\"\n\n    model_name = path.split('-')[0]\n    assert (model_name in ['vgg16', 'resnet50'\n                           ]), \"Path must have the correct model name\"\n\n    # Basic details\n    checkpoint = {\n        'class_to_idx': model.class_to_idx,\n        'idx_to_class': model.idx_to_class,\n        'epochs': model.epochs,\n    }\n\n    # Extract the final classifier and the state dictionary\n    if model_name == 'vgg16':\n        # Check to see if model was parallelized\n        if multi_gpu:\n            checkpoint['classifier'] = model.module.classifier\n            checkpoint['state_dict'] = model.module.state_dict()\n        else:\n            checkpoint['classifier'] = model.classifier\n            checkpoint['state_dict'] = model.state_dict()\n\n    elif model_name == 'resnet50':\n        if multi_gpu:\n            checkpoint['fc'] = model.module.fc\n            checkpoint['state_dict'] = model.module.state_dict()\n        else:\n            checkpoint['fc'] = model.fc\n            checkpoint['state_dict'] = model.state_dict()\n\n    # Add the optimizer\n    checkpoint['optimizer'] = model.optimizer\n    checkpoint['optimizer_state_dict'] = model.optimizer.state_dict()\n\n    # Save the data to the path\n    torch.save(checkpoint, path)","c95651f8":"save_checkpoint(model, path=checkpoint_path)","9b10ada3":"def load_checkpoint(path):\n    \"\"\"Load a PyTorch model checkpoint\n\n    Params\n    --------\n        path (str): saved model checkpoint. Must start with `model_name-` and end in '.pth'\n\n    Returns\n    --------\n        None, save the `model` to `path`\n\n    \"\"\"\n\n    # Get the model name\n    model_name = path.split('-')[0]\n    assert (model_name in ['vgg16', 'resnet50'\n                           ]), \"Path must have the correct model name\"\n\n    # Load in checkpoint\n    checkpoint = torch.load(path)\n\n    if model_name == 'vgg16':\n        model = models.vgg16(pretrained=True)\n        # Make sure to set parameters as not trainable\n        for param in model.parameters():\n            param.requires_grad = False\n        model.classifier = checkpoint['classifier']\n\n    elif model_name == 'resnet50':\n        model = models.resnet50(pretrained=True)\n        # Make sure to set parameters as not trainable\n        for param in model.parameters():\n            param.requires_grad = False\n        model.fc = checkpoint['fc']\n\n    # Load in the state dict\n    model.load_state_dict(checkpoint['state_dict'])\n\n    total_params = sum(p.numel() for p in model.parameters())\n    print(f'{total_params:,} total parameters.')\n    total_trainable_params = sum(\n        p.numel() for p in model.parameters() if p.requires_grad)\n    print(f'{total_trainable_params:,} total gradient parameters.')\n\n    # Move to gpu\n    if multi_gpu:\n        model = nn.DataParallel(model)\n\n    if train_on_gpu:\n        model = model.to('cuda')\n\n    # Model basics\n    model.class_to_idx = checkpoint['class_to_idx']\n    model.idx_to_class = checkpoint['idx_to_class']\n    model.epochs = checkpoint['epochs']\n\n    # Optimizer\n    optimizer = checkpoint['optimizer']\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n    return model, optimizer\nmodel, optimizer = load_checkpoint(path=checkpoint_path)\n\nif multi_gpu:\n    summary(model.module, input_size=(3, 224, 224), batch_size=batch_size)\nelse:\n    summary(model, input_size=(3, 224, 224), batch_size=batch_size)","32d80070":"test_loader=dataloaders['test']\ntest_loss = 0.0\nclass_correct = list(0. for i in range(2))\nclass_total = list(0. for i in range(2))\nclasses = [0,1]\nmodel.eval()\ni=1\n# iterate over test data\nlen(test_loader)\nfor data, target in test_loader:\n    i=i+1\n    if len(target)!=batch_size:\n        continue\n        \n    # move tensors to GPU if CUDA is available\n    if train_on_gpu:\n        data, target = data.cuda(), target.cuda()\n    # forward pass: compute predicted outputs by passing inputs to the model\n    output = model(data)\n    # calculate the batch loss\n    loss = criterion(output, target)\n    # update test loss \n    test_loss += loss.item()*data.size(0)\n    # convert output probabilities to predicted class\n    _, pred = torch.max(output, 1)    \n    # compare predictions to true label\n    correct_tensor = pred.eq(target.data.view_as(pred))\n    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n    # calculate test accuracy for each object class\n#     print(target)\n    \n    for i in range(batch_size):       \n        label = target.data[i]\n        class_correct[label] += correct[i].item()\n        class_total[label] += 1\n\n# average test loss\ntest_loss = test_loss\/len(test_loader.dataset)\nprint('Test Loss: {:.6f}\\n'.format(test_loss))\n\nfor i in range(2):\n    if class_total[i] > 0:\n        print('Test Accuracy of %5s: %2d%% (%2d\/%2d)' % (\n            classes[i], 100 * class_correct[i] \/ class_total[i],\n            np.sum(class_correct[i]), np.sum(class_total[i])))\n    else:\n        print('Test Accuracy of %5s: N\/A (no training examples)' % (classes[i]))\n\nprint('\\nTest Accuracy (Overall): %2d%% (%2d\/%2d)' % (\n    100. * np.sum(class_correct) \/ np.sum(class_total),\n    np.sum(class_correct), np.sum(class_total)))","9aa3e0f4":"**Contents**\n\n- <a href='#1'>1. Import the necessary libraries<\/a>\n\n- <a href='#2'>2. Read the dataset<\/a>\n\n- <a href='#3'>3. Exploration of the data<\/a>\n\n- <a href='#4'>4. Image Preprocessing<\/a>\n   \n- <a href='#5'>5. Data Iterators<\/a>\n\n- <a href='#6'>6. Using Pre-Trained Models for Image Classification<\/a>\n\n- <a href='#7'>7. Training the model <\/a>\n\n- <a href='#7'>7. Saving and loading the model <\/a>\n\n- <a href='#7'>7. Making Predictions <\/a>\n ","0d4ec4f4":"t = image_transforms['train']\nplt.figure(figsize=(24, 24))\n\nfor i in range(16):\n    ax = plt.subplot(4, 4, i + 1)\n    _ = imshow_tensor(t(ex_img), ax=ax)\n\nplt.tight_layout()","4c36e758":"<a id='4'>4. Image Preprocessing<\/a>","8aae2e8c":"<a id='6'>6. Using Pre-Trained Models for Image Classifications<\/a>\n\nPyTorch has many pretrained models we can use. All of these models have been trained on Imagenet which consists of millions of images across 1000 categories. What we want to do with pretrained models is freeze the early layers, and replace the classification module with our own.\n\n**Approach**\n\n* Load in pre-trained weights from a network trained on a large dataset\n* Freeze all the weights in the lower (convolutional) layers\n* Layers to freeze can be adjusted depending on similarity of task to large training dataset\n* Replace the classifier (fully connected) part of the network with a custom classifier\n* Number of outputs must be set equal to the number of classes\n* Train only the custom classifier (fully connected) layers for the task\n* Optimizer model classifier for smaller dataset","b4e80f75":"Load the pre-trained model","8186e216":"I'll illustrate the process by using one model, vgg16.\n\nFirst off, load in the model with pretrained weights.","e6286edd":"Add on Custom Classifier\nWe'll train a classifier consisting of the following layers\n\n* Fully connected with ReLU activation (n_inputs, 256)\n* Dropout with 40% chance of dropping\n* Fully connected with log softmax output (256, n_classes)","d7caf93c":"**Moving to GPU**","e91ef7c1":"In this notebook, we'll see how to use PyTorch to train a classifier to identify the presence of Pneumonia by looking at chest X Ray images. Through this project, we'll get familiar with the basics of transfer learning, PyTorch and convolutional neural networks.","ad71d3f1":"- <a href='#7'>7. Saving and loading the model <\/a>\n\nThe train function saves the best model state_dict() which are the weights of the model. To save more information about the model, we use the below function.","a6b4cd81":"Mapping of Classes to Indexes","a9ccaab6":"def imshow_tensor(image, ax=None, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n\n    if ax is None:\n        fig, ax = plt.subplots()\n\n    # Set the color channel as the third dimension\n    image = image.numpy().transpose((1, 2, 0))\n\n    # Reverse the preprocessing steps\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    image = std * image + mean\n\n    # Clip the image pixel values\n    image = np.clip(image, 0, 1)\n\n    ax.imshow(image)\n    plt.axis('off')\n\n    return ax, image","a8e1f7c7":"Let's see some images","3ec4a58e":"**Training Loss and Optimizer:** The loss is the negative log likelihood and the optimizer is the Adam optimizer.","ae06c2cb":"<a id='1'>1. Import the necessary libraries<\/a>","fcb62319":"To prepare the images for our network, we have to resize them to 224 x 224 and normalize each color channel by subtracting a mean value and dividing by a standard deviation. We will also augment our training data in this stage. These operations are done using image transforms, which prepare our data for a neural network.\n\n**Data Augmentation**\n\nTo get more data, we just need to make minor alterations to our existing dataset. Minor changes such as flips or translations or rotations. Our neural network would think these are distinct images anyway. A convolutional neural network that can robustly classify objects even if its placed in different orientations is said to have the property called invariance. More specifically, a CNN can be invariant to translation, viewpoint, size or illumination (Or a combination of the above). This means for training, we randomly resize and crop the images and also flip them horizontally. A different random transformation is applied each epoch (while training), so the network effectively sees many different versions of the same image. All of the data is also converted to Torch Tensors before normalization. The validation and testing data is not augmented but is only resized and normalized. The normalization values are standardized for Imagenet.","40083231":"![Aug](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*dJNlEc7yf93K4pjRJL55PA.png)","69cda928":"<a id='3'>3. Exploration of the data <\/a>","4953371a":"Let's see how our model has performed.","b1ca0c9b":"<a id='7'>7. Making Predictions<\/a>\n\nNow we will test our model and evaluate Model over all classes.\n","6c10a5c7":"<a id='2'>2. Read the dataset<\/a>\n\nAs we can see, the dataset has been divided into train, validation and test folders.","79a82be3":"\nex_img = Image.open(traindir+ 'PNEUMONIA\/person1010_bacteria_2941.jpeg')\nimshow_tensor(ex_img)","e0e83b58":"# Hi! Welcome to my kernel.","501dcda3":"<a id='7'>7. Training the model<\/a>\n\nFor training, we iterate through the train DataLoader, each time passing one batch through the model. One complete pass through the training data is known as an epoch, and we train for a set number of epochs or until early stopping kicks in (more below). After each batch, we calculate the loss (with criterion(output, targets)) and then calculate the gradients of the loss with respect to the model parameters with loss.backward(). This uses autodifferentiation and backpropagation to calculate the gradients.\n\nAfter calculating the gradients, we call optimizer.step() to update the model parameters with the gradients. This is done on every training batch so we are implementing stochastic gradient descent (or rather a version of it with momentum known as Adam). For each batch, we also compute the accuracy for monitoring and after the training loop has completed, we start the validation loop. This will be used to carry out early stopping.","77848c23":"**Loading the model**","26e2abdd":"<a id='5'>5. Data Iterators<\/a>\n\nTo avoid loading all of the data into memory at once, we use training DataLoaders. First, we create a dataset object from the image folders, and then we pass these to a DataLoader.","e26b4de7":"Freeze the early layers"}}