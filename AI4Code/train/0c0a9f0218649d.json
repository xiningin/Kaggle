{"cell_type":{"4f498859":"code","e2419006":"code","1200dd67":"code","0f522cd1":"code","ff244867":"code","0b28878e":"code","b8e1c135":"code","8e6a366b":"code","f5f07eab":"code","04ddcfc2":"code","5297d2a0":"code","b36db64c":"code","5eef3de6":"code","0d4ff954":"code","4209c5cf":"code","0dfaa6a0":"code","d36e512b":"code","e6976f62":"code","2610a16f":"code","729b7a87":"code","fc1d6800":"code","77649b8a":"code","a23de516":"code","b64e7a55":"code","c7bd46df":"code","36be7c6e":"code","7e0e3851":"code","dca7bfe2":"code","73ca69f9":"code","fbbd3cb4":"code","bbb5514b":"code","2e2d8b69":"code","054c4998":"code","e721e572":"code","dd18d4aa":"code","9550750b":"code","aa98d563":"code","4617f1df":"code","1ab88432":"code","f3c67d80":"code","38fdf5fd":"code","1008dda3":"markdown","e55080e3":"markdown","43980652":"markdown","4498f4b1":"markdown","20d32bef":"markdown","09903cd1":"markdown","6d6c2226":"markdown","cb026e66":"markdown","8d2ad1f8":"markdown","e4baf665":"markdown","e5b154d1":"markdown","84d6d7d5":"markdown","2fef29fc":"markdown","a89a5b82":"markdown","ea5922e9":"markdown","b0c26d44":"markdown"},"source":{"4f498859":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as grid_spec\n\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score, recall_score, roc_auc_score, precision_score, f1_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","e2419006":"df_train= pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ndf_test= pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\ndf_subm= pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")","1200dd67":"#Custom Color Palette \ud83c\udfa8\ncustom_colors = [\"#CC5803\",\"#E2711D\",\"#FF9505\",\"#FFB627\",\"#FFC971\"]\ncustomPalette = sns.set_palette(sns.color_palette(custom_colors))\nsns.palplot(sns.color_palette(custom_colors),size=1.2)\nplt.tick_params(axis='both', labelsize=0, length = 0)","0f522cd1":"df_train_row_count, df_train_column_count=df_train.shape\nprint('Total number of rows (Train):', df_train_row_count)\nprint('Total number of columns (Train):', df_train_column_count)","ff244867":"df_test_row_count, df_test_column_count=df_test.shape\nprint('Total number of rows (Test):', df_test_row_count)\nprint('Total number of columns (Test):', df_test_column_count)","0b28878e":"df_train.dtypes","b8e1c135":"df_train.head()","8e6a366b":"df_train.describe().T","f5f07eab":"df_test.describe().T","04ddcfc2":"display(df_train.info())\ndisplay(df_test.info())","5297d2a0":"df_train.isna().sum()","b36db64c":"df_test.isna().sum()","5eef3de6":"background_color = 'white'\nmissing = pd.DataFrame(columns=['% Train Missing values'],data=df_train.isnull().sum()\/len(df_train))\nmissing.drop(['claim'],inplace=True)\nmissing_tst = pd.DataFrame(columns=['% Test Missing values'],data=df_test.isnull().sum()\/len(df_test))\nfig = plt.figure(figsize=(10, 30),facecolor=background_color)\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.5, hspace=0.5)\nax0 = fig.add_subplot(gs[0, 0])\nax1 = fig.add_subplot(gs[0, 1])\nfor s in [\"right\", \"top\",\"bottom\",\"left\"]:\n    ax0.spines[s].set_visible(False)\n    ax1.spines[s].set_visible(False)\nsns.heatmap(missing,cbar=False,annot=True,fmt=\".2%\", linewidths=2,cmap='YlOrRd',vmax=1, ax=ax0)\nsns.heatmap(missing_tst,cbar=False,annot=True,fmt=\".2%\", linewidths=2,cmap='copper_r',vmax=1, ax=ax1)\nax1.set_yticklabels([])\nplt.show()","0d4ff954":"print (\"Unique values are:\\n\",df_train.nunique())","4209c5cf":"df_train.claim.value_counts()","0dfaa6a0":"x = df_train['claim'].value_counts()\n\nfig,ax=plt.subplots(figsize=(7,4))\nax.barh([1],x.values[1],height=0.7,color='#CC5803',alpha=0.7)\nplt.text(-35000,1, '1', {'font': 'Trebuchet MS','weight':'bold','Size': '16','style':'normal', 'color':'black'},alpha = 0.7)\nplt.text(500000,1, '49.84%', {'font': 'Trebuchet MS','weight':'bold','Size': '16','style':'normal', 'color':'black'},alpha = 0.7)\n\nax.barh([0],x.values[0],height=0.7,color='#FFB627',alpha=0.7)\nplt.text(-35000,0,'0',{'font': 'Trebuchet MS','weight':'bold','Size': '16','style':'normal', 'color':'black'}, alpha = 0.7)\nplt.text(500000,0, '50.15%',{'font': 'Trebuchet MS','weight':'bold','Size': '16','style':'normal', 'color':'black'}, alpha = 0.7)\n\n\nplt.text(-50,1.77, 'How claim is distributed? - 0 vs 1',{'font': 'Trebuchet MS','weight':'bold','Size': '20','style':'normal', 'color':'#FF9505'}, alpha = 0.9)\nplt.text(600000,1.65, '0 ', {'font': 'Trebuchet MS','weight':'bold','Size': '16','style':'normal', 'color':'#FFB627'},alpha = 0.8)\nplt.text(615000,1.65, '|', {'font': 'Trebuchet MS','weight':'bold','Size': '16','style':'normal', 'color':'black'}, alpha = 0.9)\nplt.text(625000,1.65, '1',  {'font': 'Trebuchet MS','weight':'bold','Size': '16','style':'normal', 'color':'#CC5803'},alpha = 0.7)\nplt.text(-50,1.5, 'Nearly equal distribution')\n\nax.axes.get_xaxis().set_visible(False)\nax.axes.get_yaxis().set_visible(False)\nax.spines['bottom'].set_visible(False)\nax.spines['left'].set_visible(True)\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)","d36e512b":"plt.figure(figsize=(11,11))\ncorr=df_train.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, cmap=custom_colors, robust=True, center=0,square=True, linewidths=.6)\nplt.title('Correlation')\nplt.show()","e6976f62":"correlations_data = df_train.corr()['claim'].sort_values()\nprint(correlations_data.head(20),'\\n')\nprint(correlations_data.tail(20),'\\n')","2610a16f":"corr_feat = df_train.corr()\nplt.figure(figsize=(24,8))\ncorr_feat[\"claim\"][:-1].plot(kind=\"bar\",grid=True,color='#FF9505')\nplt.title(\"Features correlation\")","729b7a87":"df_train.drop(columns = 'id', inplace = True)\ndf_test.drop(columns = 'id', inplace = True)","fc1d6800":"df = pd.concat([df_train.drop([\"claim\"], axis=1)])\ndf = df_train.columns[0:118]\nplt.subplots(figsize=(20,160))\nlength = len(df)\nfor i, j in zip(df, range(length)):\n    fig = plt.subplot((length\/2), 3, j+1)\n    plt.subplots_adjust(wspace=.25, hspace=.6)\n    plt.yticks([])\n    sns.histplot(x=df_train[i],alpha=0.5,color='#FFC971')\n    sns.histplot(x=df_test[i],alpha=0.5,color='#370617')\n    fig.legend(labels=('Train','Test'))","77649b8a":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\ndf_train_n = pd.DataFrame(imputer.fit_transform(df_train),columns=df_train.columns,index=df_train.index)\ndf_test_n = pd.DataFrame(imputer.fit_transform(df_test),columns=df_test.columns,index=df_test.index)","a23de516":"# define dataset\nX = df_train_n.drop(['claim'], axis=1)\ny = df_train_n['claim']","b64e7a55":"from sklearn.model_selection import train_test_split\n\n# creating dataset split for prediction\nX_train, X_test , y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=42) # 80-20 split\n\n# Checking split \nprint('X_train:', X_train.shape)\nprint('y_train:', y_train.shape)\nprint('X_test:', X_test.shape)\nprint('y_test:', y_test.shape)","c7bd46df":"from sklearn.preprocessing import MinMaxScaler\nms = MinMaxScaler()\nms.fit(X_train)\nX_train = ms.transform(X_train)\nX_test = ms.transform(X_test)","36be7c6e":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nmodel1 = XGBClassifier(random_state=42,n_estimators= 500,learning_rate=0.05,eval_metric=\"auc\",\n                      max_depth=8,booster='gbtree',verbosity=0,tree_method = 'gpu_hist',task_type=\"GPU\")\nmodel1.fit(X,y)","7e0e3851":"import shap\nexplainer = shap.Explainer(model1)\nshap_values = explainer(X)\nshap.plots.beeswarm(shap_values,max_display=20)","dca7bfe2":"predicted1 = model1.predict(X)","73ca69f9":"m1_cm = confusion_matrix(y, predicted1)\nm1_acc_score = accuracy_score(y, predicted1)\nprint(\"Confusion Matrix\")\nprint(m1_cm)\nsns.heatmap(m1_cm, annot=True,cmap=custom_colors)\nprint(\"\\n\")\nprint(\"Accuracy of XGBoost:\",round(m1_acc_score*100,2),'\\n')\nprint(classification_report(y,predicted1))","fbbd3cb4":"import lightgbm as lgbm  \nmodel2 =  lgbm.LGBMClassifier(objective= 'binary',learning_rate = 0.05, max_depth = 3, \n                        device = 'gpu',n_estimators=500)\nmodel2.fit(X_train,y_train)","bbb5514b":"predicted2 = model2.predict(X)","2e2d8b69":"m2_cm = confusion_matrix(y, predicted2)\nm2_acc_score = accuracy_score(y, predicted2)\nprint(\"Confusion Matrix\")\nprint(m2_cm)\nsns.heatmap(m2_cm, annot=True,cmap=custom_colors)\nprint(\"\\n\")\nprint(\"Accuracy of LGBM:\",round(m2_acc_score*100,2),'\\n')\nprint(classification_report(y,predicted2))","054c4998":"from catboost import CatBoostClassifier\nmodel3 = CatBoostClassifier(random_state=42,max_depth = 3, iterations = 5000,learning_rate=0.005,\n                           early_stopping_rounds=50,task_type=\"GPU\")\nmodel3.fit(X,y, verbose=1)","e721e572":"predicted3 = model3.predict(X)","dd18d4aa":"m3_cm = confusion_matrix(y, predicted3)\nm3_acc_score = accuracy_score(y, predicted3)\nprint(\"Confusion Matrix\")\nprint(m3_cm)\nsns.heatmap(m3_cm, annot=True,cmap=custom_colors)\nprint(\"\\n\")\nprint(\"Accuracy of CatBoost:\",round(m3_acc_score*100,2),'\\n')\nprint(classification_report(y,predicted3))","9550750b":"model_eval = pd.DataFrame({'Model': ['XGBoost','LGBM','CatBoost'], 'Accuracy': [m1_acc_score,\n                    m2_acc_score,m3_acc_score]})\nmodel_eval = model_eval.set_index('Model').sort_values(by='Accuracy',ascending=False)\nfig = plt.figure(figsize=(12, 4))\ngs = fig.add_gridspec(1, 2)\ngs.update(wspace=0.8, hspace=0.8)\nax0 = fig.add_subplot(gs[0, 0])\nsns.heatmap(model_eval,cmap=custom_colors, annot=True,fmt=\".1%\", linewidths=4,cbar=False,ax=ax0)\nplt.show()","aa98d563":"from mlxtend.classifier import StackingCVClassifier\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits=10,random_state=42)\nclr_models = (model1,model2,model3)\nmodel_stack = StackingCVClassifier(classifiers=clr_models, meta_classifier=model1, \n                            use_features_in_secondary=True,shuffle=False,cv=kfold,random_state=42)\nmodel_stack.fit(X, y)","4617f1df":"predicted_st = model_stack.predict(X)","1ab88432":"y_pred_stack = model_stack.predict(df_test)","f3c67d80":"df_subm['claim'] = y_pred_stack\ndf_subm","38fdf5fd":"df_subm.to_csv('submission_stack.csv', index=False)","1008dda3":"<h3 style=\"font-family: Trebuchet MS;background-color:#ffc501;color:brown;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;letter-spacing: 2px;\"><strong><centre>StackingCVClassifier \u23f3<\/centre><\/strong><\/h3>","e55080e3":"Reference: https:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingCVClassifier\/","43980652":"<p style=\"font-family: Trebuchet MS; line-height: 2; font-size: 16px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #CC5803\">We can see that the number of people that not claim and claim are almost the same of 480,404 and 477,515, respectively (i.e.around 50%)<\/p>\n  ","4498f4b1":"<h3 style=\"font-family: Trebuchet MS;background-color:#ffc501;color:brown;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;letter-spacing: 2px;\"><strong><centre>XGBoost \u23f3<\/centre><\/strong><\/h3>","20d32bef":"<h3 style=\"font-family: Trebuchet MS;background-color:#ffc501;color:brown;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;letter-spacing: 2px;\"><strong><centre>Checking for missing values \u270f\ufe0f <\/centre><\/strong><\/h3>","09903cd1":"<p style=\"font-family: Trebuchet MS; line-height: 2; font-size: 16px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #CC5803\">For the training dataset, every feature has nearly 15,000 missing values totalling 1,820,782 in all and for testing dataset, every feature has almost 7,800 missing values totalling 936,218 in all<\/p><\/p>\n  ","6d6c2226":"<h3 style=\"font-family: Trebuchet MS;background-color:#ffc501;color:brown;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;letter-spacing: 2px;\"><strong><centre>Importing Libraries & Packages \ud83d\udcda <\/centre><\/strong><\/h3>","cb026e66":"<p style=\"font-family: Trebuchet MS; line-height: 2; font-size: 16px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #CC5803\">All the 118 features are  weakly correlated.<\/p>\n  ","8d2ad1f8":"<p style=\"font-family: Trebuchet MS; line-height: 2; font-size: 16px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #CC5803\">Except the target column which is of int64 datatype, all the other features are of float64 datatype.<\/p>","e4baf665":"<h3 style=\"font-family: Trebuchet MS;background-color:#ffc501;color:brown;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;letter-spacing: 2px;\"><strong><centre>CatBoost \u23f3<\/centre><\/strong><\/h3>","e5b154d1":"<h3 style=\"font-family: Trebuchet MS;background-color:#ffc501;color:brown;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;letter-spacing: 2px;\"><strong><centre>Importing & Reading the dataset \ud83d\udcdd <\/centre><\/strong><\/h3>","84d6d7d5":"<h3 style=\"font-family: Trebuchet MS;background-color:#ffc501;color:brown;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;letter-spacing: 2px;\"><strong><centre>LGBM \u23f3<\/centre><\/strong><\/h3>","2fef29fc":"<p style=\"font-family: Trebuchet MS; line-height: 2; font-size: 24px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #b20710\">\ud83c\udf41 TPS SEPTEMBER 2021 \ud83c\udf42<\/p>","a89a5b82":"<h3 style=\"font-family: Trebuchet MS;background-color:#ffc501;color:brown;text-align: center;padding-top: 5px;padding-bottom: 5px;border-radius: 15px 50px;letter-spacing: 2px;\"><strong><centre>Work in progress<\/centre><\/strong><\/h3>","ea5922e9":"![TPS Sep 2021.jpg](attachment:9a093260-a6b4-4c11-80a7-8088db793ade.jpg)","b0c26d44":"<p style = \"font-family: Trebuchet MS; font-size: 16px; color: rgba(0,0,0,.7)\"> Observations on this TPS: <li>1. 'claim' column is the target variable <\/li> <li>2. Train dataset has 957,919 rows and 120 columns<\/li><li>3. Test dataset has 493,474 rows and 119 columns<\/li><li>4. In train dataset, every feature has nearly 15,000 missing values totalling 1,820,782 in all<\/li><li>5. In test dataset, every feature has almost 7,800 missing values totalling 936,218 in all<\/li><\/p>"}}