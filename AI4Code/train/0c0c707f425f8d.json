{"cell_type":{"84356dbf":"code","b90d17f7":"code","2d32d58f":"code","6626b529":"code","a0ae043f":"code","733ec4ed":"code","8ed75f96":"code","30d9bb86":"code","7a9df62f":"code","10911e98":"code","c34036a5":"code","2a4fb609":"code","f30d0269":"code","e0d5cdd9":"code","067187a4":"code","6b39c473":"code","6e87ca9c":"code","003aecef":"code","66203db8":"code","9a3763a3":"code","bdebc4d1":"code","38539924":"code","56b9baf3":"code","a7a09046":"code","7401d657":"code","d660168d":"code","40286397":"code","d12312cb":"code","b681ffcd":"code","a1917bf3":"code","2fd5741c":"code","f6db6757":"code","11bb67d2":"code","4d4e648c":"code","b8fb73c2":"code","21d9743b":"code","262d036c":"code","08df6ab3":"code","fb04b398":"code","65b896fa":"markdown","4ea840e3":"markdown","8d714cbd":"markdown","25168033":"markdown","5e3d3f0e":"markdown","707f5108":"markdown","9a2b04dd":"markdown","4fcb3b49":"markdown","75599250":"markdown","4d43e83f":"markdown","200e3962":"markdown","0688f7f0":"markdown","2f5a3bd5":"markdown","2823dc5a":"markdown","18dd9525":"markdown","c37984fa":"markdown","4dc5773d":"markdown","380e278d":"markdown","ea27db73":"markdown","a04a2ec1":"markdown","9fdd7211":"markdown","d95f7874":"markdown","41430611":"markdown","dddb946d":"markdown","047c05c9":"markdown","66625018":"markdown","184cfadd":"markdown","3dfdc114":"markdown","275d71cd":"markdown","0745d469":"markdown","1219a47e":"markdown","7dde4c87":"markdown","25a1d6a4":"markdown","347ff195":"markdown","7623f244":"markdown","5b024dce":"markdown","006cb284":"markdown","ed16454f":"markdown","0d0895f2":"markdown","78574315":"markdown","42262145":"markdown","92f0e34b":"markdown","20ae6d08":"markdown","22179164":"markdown"},"source":{"84356dbf":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b90d17f7":"import re\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS \n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report","2d32d58f":"fake = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv')\nfake.shape","6626b529":"fake.head(3)","a0ae043f":"true = pd.read_csv('\/kaggle\/input\/fake-and-real-news-dataset\/True.csv')\ntrue.shape","733ec4ed":"true.head(3)","8ed75f96":"true['label'] = 0\nfake['label'] = 1","30d9bb86":"df = true.append(fake.reset_index(drop=True))\nprint(df.shape)\ndf = df.reset_index(drop=True)\ndf","7a9df62f":"df.info()","10911e98":"df[df.text == \" \"]","c34036a5":"df.drop(df[df.text == \" \"].index.tolist(), inplace=True)","2a4fb609":"df.drop('label', axis=1)[df.duplicated(keep=False)]","f30d0269":"df.drop(df[df.duplicated()].index.tolist(), inplace=True)\ndf.shape","e0d5cdd9":"df[df.duplicated(['title'])]","067187a4":"df.drop(df[df.duplicated(['title'])].index.tolist(), inplace=True)\ndf.shape","6b39c473":"df[df.duplicated(['text'], keep=False)]","6e87ca9c":"df.drop(df[df.duplicated(['text'])].index.tolist(),inplace=True)","003aecef":"df.shape","66203db8":"df.groupby(\"label\")['title'].count().plot.bar()","9a3763a3":"df.sort_values('date').date.reset_index(drop=True).iloc[[5,10,20,50,100,200,38264,38268]]","bdebc4d1":"import plotly.graph_objects as go\n\nfig = go.Figure(layout=go.Layout(title='Day wise article count',yaxis_title='No. of news',xaxis_title='Dates'))\n\nfig.add_trace(go.Scatter(y=df[df.label==0].groupby(['date']).label.count(),\n                    mode='lines',\n                    name='true'))\nfig.add_trace(go.Scatter(x=df[df.label==1].groupby(['date']).label.count().index,y=df[df.label==1].groupby(['date']).label.count(),\n                    mode='lines',\n                    name='fake'))\n\nfig.show()","38539924":"df.drop('date', axis=1,inplace=True)","56b9baf3":"df[df.label==1].subject.value_counts()","a7a09046":"df[df.label==0].subject.value_counts()","7401d657":"df.drop('subject', axis=1,inplace=True)","d660168d":"df['text'] = df['title'] + ' ' + df['text']\ndel df['title']\n\ndf.head(2)","40286397":"\nnltk_stopwords = stopwords.words('english')\nwordcloud_stopwords = STOPWORDS\n\nnltk_stopwords.extend(wordcloud_stopwords)\n\nstopwords = set(nltk_stopwords)\nprint(stopwords)\n","d12312cb":"\ndef clean(text):\n    \n    text = re.sub(\"http\\S+\", '', str(text))\n    \n    # Contractions ref: https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert\n    \n    text = re.sub(r\"he's\", \"he is\", str(text))\n    text = re.sub(r\"there's\", \"there is\", str(text))\n    text = re.sub(r\"We're\", \"We are\", str(text))\n    text = re.sub(r\"That's\", \"That is\", str(text))\n    text = re.sub(r\"won't\", \"will not\", str(text))\n    text = re.sub(r\"they're\", \"they are\", str(text))\n    text = re.sub(r\"Can't\", \"Cannot\", str(text))\n    text = re.sub(r\"wasn't\", \"was not\", str(text))\n    text = re.sub(r\"aren't\", \"are not\", str(text))\n    text = re.sub(r\"isn't\", \"is not\", str(text))\n    text = re.sub(r\"What's\", \"What is\", str(text))\n    text = re.sub(r\"haven't\", \"have not\", str(text))\n    text = re.sub(r\"hasn't\", \"has not\", str(text))\n    text = re.sub(r\"There's\", \"There is\", str(text))\n    text = re.sub(r\"He's\", \"He is\", str(text))\n    text = re.sub(r\"It's\", \"It is\", str(text))\n    text = re.sub(r\"You're\", \"You are\", str(text))\n    text = re.sub(r\"I'M\", \"I am\", str(text))\n    text = re.sub(r\"shouldn't\", \"should not\", str(text))\n    text = re.sub(r\"wouldn't\", \"would not\", str(text))\n    text = re.sub(r\"i'm\", \"I am\", str(text))\n    text = re.sub(r\"I'm\", \"I am\", str(text))\n    text = re.sub(r\"Isn't\", \"is not\", str(text))\n    text = re.sub(r\"Here's\", \"Here is\", str(text))\n    text = re.sub(r\"you've\", \"you have\", str(text))\n    text = re.sub(r\"we're\", \"we are\", str(text))\n    text = re.sub(r\"what's\", \"what is\", str(text))\n    text = re.sub(r\"couldn't\", \"could not\", str(text))\n    text = re.sub(r\"we've\", \"we have\", str(text))\n    text = re.sub(r\"who's\", \"who is\", str(text))\n    text = re.sub(r\"y'all\", \"you all\", str(text))\n    text = re.sub(r\"would've\", \"would have\", str(text))\n    text = re.sub(r\"it'll\", \"it will\", str(text))\n    text = re.sub(r\"we'll\", \"we will\", str(text))\n    text = re.sub(r\"We've\", \"We have\", str(text))\n    text = re.sub(r\"he'll\", \"he will\", str(text))\n    text = re.sub(r\"Y'all\", \"You all\", str(text))\n    text = re.sub(r\"Weren't\", \"Were not\", str(text))\n    text = re.sub(r\"Didn't\", \"Did not\", str(text))\n    text = re.sub(r\"they'll\", \"they will\", str(text))\n    text = re.sub(r\"they'd\", \"they would\", str(text))\n    text = re.sub(r\"DON'T\", \"DO NOT\", str(text))\n    text = re.sub(r\"they've\", \"they have\", str(text))\n    text = re.sub(r\"i'd\", \"I would\", str(text))\n    text = re.sub(r\"should've\", \"should have\", str(text))\n    text = re.sub(r\"where's\", \"where is\", str(text))\n    text = re.sub(r\"we'd\", \"we would\", str(text))\n    text = re.sub(r\"i'll\", \"I will\", str(text))\n    text = re.sub(r\"weren't\", \"were not\", str(text))\n    text = re.sub(r\"They're\", \"They are\", str(text))\n    text = re.sub(r\"let's\", \"let us\", str(text))\n    text = re.sub(r\"it's\", \"it is\", str(text))\n    text = re.sub(r\"can't\", \"cannot\", str(text))\n    text = re.sub(r\"don't\", \"do not\", str(text))\n    text = re.sub(r\"you're\", \"you are\", str(text))\n    text = re.sub(r\"i've\", \"I have\", str(text))\n    text = re.sub(r\"that's\", \"that is\", str(text))\n    text = re.sub(r\"i'll\", \"I will\", str(text))\n    text = re.sub(r\"doesn't\", \"does not\", str(text))\n    text = re.sub(r\"i'd\", \"I would\", str(text))\n    text = re.sub(r\"didn't\", \"did not\", str(text))\n    text = re.sub(r\"ain't\", \"am not\", str(text))\n    text = re.sub(r\"you'll\", \"you will\", str(text))\n    text = re.sub(r\"I've\", \"I have\", str(text))\n    text = re.sub(r\"Don't\", \"do not\", str(text))\n    text = re.sub(r\"I'll\", \"I will\", str(text))\n    text = re.sub(r\"I'd\", \"I would\", str(text))\n    text = re.sub(r\"Let's\", \"Let us\", str(text))\n    text = re.sub(r\"you'd\", \"You would\", str(text))\n    text = re.sub(r\"It's\", \"It is\", str(text))\n    text = re.sub(r\"Ain't\", \"am not\", str(text))\n    text = re.sub(r\"Haven't\", \"Have not\", str(text))\n    text = re.sub(r\"Could've\", \"Could have\", str(text))\n    text = re.sub(r\"youve\", \"you have\", str(text))\n    \n    # Others\n    text = re.sub(\"U.S.\", \"United States\", str(text))\n    text = re.sub(\"Dec\", \"December\", str(text))\n    text = re.sub(\"Jan.\",\"January\", str(text))\n    \n    # Punctuations & special characters\n    text = re.sub(\"[^A-Za-z0-9]+\",\" \", str(text))\n    \n    # Stop word removal\n    text = \" \".join(str(i).lower() for i in text.split() if i.lower() not in stopwords)\n\n    return text\n    ","b681ffcd":"\ndf['text'] = df['text'].map(lambda x: clean(x))\n","a1917bf3":"df.text.iloc[:3]","2fd5741c":"X_train, X_test, y_train, y_test = train_test_split(df.drop('label',axis=1), df.label, test_size=0.2, random_state=42)\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","f6db6757":"vectorizer = TfidfVectorizer(min_df=0.01,ngram_range=(1,3))\nvectorizer.fit(X_train.text)\n\nX_tr = vectorizer.transform(X_train.text)\nX_te = vectorizer.transform(X_test.text)\n\nprint(X_tr.shape, X_te.shape)","11bb67d2":"clf = SGDClassifier(loss='log')\n\ngs = GridSearchCV(\n    estimator = clf,\n    param_grid = {'alpha':np.logspace(-10,5,16)},\n    cv = 5,\n    return_train_score = True,\n    scoring = 'accuracy'\n    )\n\ngs.fit(X_tr,y_train)\n\nresults = pd.DataFrame(gs.cv_results_)\n\nresults = results.sort_values(['param_alpha'])\ntrain_auc = results['mean_train_score']\ncv_auc = results['mean_test_score']\nalpha = pd.Series([ math.log(i) for i in np.array(results['param_alpha']) ]) \n\nplt.plot(alpha, train_auc, label='Train AUC')\nplt.plot(alpha, cv_auc, label='CV AUC')\nplt.scatter(alpha, train_auc)\nplt.scatter(alpha, cv_auc)\nplt.legend()\nplt.xlabel('log(alpha): hyperparameter')\nplt.ylabel('Accuracy')\nplt.title('Hyperparameter vs Accuracy Plot')\nplt.grid()\nplt.show()\n\nprint(gs.best_params_)","4d4e648c":"clf = SGDClassifier(loss='log',alpha=1e-06, random_state=42).fit(X_tr,y_train)\n\nprint('Training score : %f' % clf.score(X_tr,y_train))\nprint('Test score : %f' % clf.score(X_te,y_test))\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, clf.predict_proba(X_tr)[:,1])\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, clf.predict_proba(X_te)[:,1])\n\nplt.plot(train_fpr, train_tpr, label=\"train AUC =\"+str(auc(train_fpr, train_tpr)))\nplt.plot(test_fpr, test_tpr, label=\"test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.title(\"AUC Plot\")\nplt.grid()\nplt.show()","b8fb73c2":"print(classification_report(y_train.values, clf.predict(X_tr)))\nconfusion_matrix(y_train, clf.predict(X_tr))","21d9743b":"print(classification_report(y_test.values, clf.predict(X_te)))\n\ncm = pd.DataFrame(confusion_matrix(y_test,clf.predict(X_te)) , index = ['Fake','Not Fake'] , columns = ['Fake','Not Fake'])\nsns.heatmap(cm,cmap= 'Blues', annot = True, fmt='', xticklabels = ['Fake','Not Fake'], yticklabels = ['Fake','Not Fake'])\nplt.xlabel(\"Actual\")\nplt.ylabel(\"Predicted\")\nplt.title('Confusion matrix on test data')\nplt.show()","262d036c":"coef = [abs(i) for i in clf.coef_.ravel()]\nfeature_names = vectorizer.get_feature_names()\nfeature_imp = dict(zip(feature_names,coef))\nfeature_imp = {k: v for k, v in sorted(feature_imp.items(), key=lambda item: item[1], reverse=True)}\n\ntop_50_features = {k: feature_imp[k] for k in list(feature_imp)[0:50]}\n\nfig, ax = plt.subplots(figsize=(6,10))\n\npeople = top_50_features.keys()\ny_pos = np.arange(len(people))\nimportance = top_50_features.values()\n\nax.barh(y_pos, importance,align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(people)\nax.invert_yaxis()\nax.set_xlabel('Importance')\nax.set_ylabel('Features')\nax.set_title('Top 50 Features')\n\nplt.show()","08df6ab3":"feature_imp = dict(zip(feature_names,coef))\nfeature_imp = {k: v for k, v in sorted(feature_imp.items(), key=lambda item: item[1], reverse=False)}\n\nbottom_50_features = {k: feature_imp[k] for k in list(feature_imp)[0:50]}\n\nfig, ax = plt.subplots(figsize=(6,10))\n\npeople = bottom_50_features.keys()\ny_pos = np.arange(len(people))\nimportance = bottom_50_features.values()\n\nax.barh(y_pos, importance,align='center')\nax.set_yticks(y_pos)\nax.set_yticklabels(people)\nax.invert_yaxis()\nax.set_xlabel('Importance')\nax.set_ylabel('Features')\nax.set_title('Least 50 important features')\n\nplt.show()","fb04b398":"# import pickle\n\n# with open('vectorizer', 'wb') as f:\n#     pickle.dump(vectorizer, f)\n    \n# with open('model', 'wb') as f:\n#     pickle.dump(clf, f)","65b896fa":"<b>Checking for duplicates<\/b>","4ea840e3":"<hr>","8d714cbd":"<hr>","25168033":"<img src=\"https:\/\/media-assets-02.thedrum.com\/cache\/images\/thedrum-prod\/s3-news-tmp-140656-fake_news--default--1280.jpg\" \/>","5e3d3f0e":"<hr>","707f5108":"Now we check for duplicates by just title","9a2b04dd":"From above table it implies that we do not have any null fields, but after some observation it was observed that in text features we have 445 samples that contain just a single space. To apply classical machine learning algorithms we will have to get rid of these samples but for deep learning we can choose to delete them because we will later on merge title feature with text to tokenization and padding will take care of shorter text.\n\nFor now we remove these empty samples.","4fcb3b49":"<b>Now we dump the vectorizer and classifier, so that they can used during real-time predictions<\/b>","75599250":"<hr>","4d43e83f":"#### Hyperparameter tuning Logistic Regression","200e3962":"<b>Assigning target labels and merging into one dataframe<\/b>","0688f7f0":"<b>Now we clean the title feature<\/b>\n\n1. We remove urls (if any)\n2. Perform decontractions\n3. Non-acronize few popular words\n4. Remove punctuations and all special characters\n5. Remove stopwords","2f5a3bd5":"We experimented with various hyperparameters with tdidf vectorizer like n-grams, min-df etc and found the chosen parameters as reasonable.\n\nWe also tried few other classifiers like SGD with log loss and SVM, but there was no significant improvement in the incorrectly labelled points.\n\nWhile performing text pre-processing we found that there were certain words like 'Reuters' and 'official', that were only present in negative i.e. real news data samples. Having these words in the corpus has significantally contributed in the performance of the model.\n\nSince much simpler and easy to interpret models are giving good results, therefore we do not apply deep learning to this problem.","2823dc5a":"#### Training on best parameters","18dd9525":"<b>What is fake news ?<\/b><br>\nFake news, also known as junk news, pseudo-news, alternative facts or hoax news, is a form of news consisting of deliberate disinformation or hoaxes spread via traditional news media (print and broadcast) or online social media. Digital news has brought back and increased the usage of fake news, or yellow journalism. The news is then often reverberated as misinformation in social media but occasionally finds its way to the mainstream media as well. [source](https:\/\/en.wikipedia.org\/wiki\/Fake_news)\n\n* It creates panic\n* Damages reputation of public and private entities\n* Misleads people, to benefit fradusters\n* Motivated by personal vendatta, some people backs such generators with support\n\n<br>\n\n<b>PROBLEM <\/b>: How to distinguish between a real news and a fake news?\n\n<br>\n\n<b>SOLUTION <\/b>: A human can read the news and cross-check with the original source or internet to find some supporting ancillary news. Depending on the results, the news can be credited as real or bogus.\n\nOR\n\nWe can show an algorithm huge number of fake and real news articles so that it learns to differenciate between them automatically, and then it will give a probability score or percentage of confidence as an output for a given news article, that it is real or fake.","c37984fa":"#### Final Test Scores","4dc5773d":"We get 5960 samples that are completely same with other sample titles. We also remove them before prcedding further.","380e278d":"<b>Checking class distributions<\/b>","ea27db73":"<b>Dates distribution<\/b>","a04a2ec1":"**Bottom 50 n-grams**","9fdd7211":"### Splitting into train, test and CV","d95f7874":"Checking for duplicates just by text we observe that there are samples that have same text but different titles, and there are 15 such samples that have their duplicate pair-end in text, subject and date, but not in title. We also remove them.","41430611":"<hr>","dddb946d":"<b>DEFINING ML PROBLEM<\/b>\n* We should be able to correctly classify all the fake news as fake, hence recall is important.\n* We also want to penalize incorrecly classified data points, so we will use log-loss as loss function.\n* We do not have strict latency concerns.","047c05c9":"### Subject Feature\n\nWe have different classes in subject feature for both target classes. We can bin the positive (i.e. fake) class subject unique values to match negative (i.e. true) subject unique values.\n\nOR\n\nWe can merge the subject into the text.\n\nWe can also bin and merge but then there will not be any information left to distinguish, which also holds true if we bin one class values to match other class unique values. After experimenting with some simple models we chose to remove them.","66625018":"<hr>","184cfadd":"### Using sklearn TfidfVectorizer","3dfdc114":"<br>","275d71cd":"<b>Importing Libraries<\/b>","0745d469":"<hr>","1219a47e":"<b>We create stopwords for cleaning the text<\/b>\n\nWe chose to combine nltk and wordcloud stopwords despite more than 90% elements as same, because decontractions became easy with this approach.","7dde4c87":"**Top 50 n-grams**","25a1d6a4":"Now we are left with 38,269 samples.","347ff195":"<br>","7623f244":"<hr>","5b024dce":"<b>Different types of dates<\/b>","006cb284":"<b>We merge title and text features into single feature text<\/b>","ed16454f":"<hr>","0d0895f2":"### Other vectorizers that can be tried are:\n\n1. Glove vectors\n2. Avg W2V\n2. Tfidf weighted W2V","78574315":"We have 405 samples that match with each other in terms of title, text, date and subject, so we remove them.","42262145":"<hr>","92f0e34b":"### Date Feature\n\nDateformats we have are DD-MM-YY and MM DD, YYYY. We can clean them and keep a uniform format across the feature but that will not be of much use. Because we do not want our classifier to distinguish a news item on the basis of the date that article was created hence we remove them from our dataset.\n\nWe also see that for fake news, dates are distributed across a three year period and for true news all dates are between ~Jan 2016 to ~Jan 2018.","20ae6d08":"<b>Loading Datasets<\/b>","22179164":"### Observations"}}