{"cell_type":{"ec3dd40c":"code","f3559e58":"code","c17e75f2":"code","1b8c53f6":"code","c9a84de0":"code","b3333b0d":"code","73385806":"code","c2447182":"code","c2bde4bd":"code","69f2bcfd":"code","504a35f0":"code","42702fbd":"code","333784fa":"code","c5b77c5a":"code","136117ae":"code","e2851445":"code","456da3b9":"code","117aa8d6":"code","5a64f5a5":"code","8a125ba1":"code","bb7d7048":"code","80852da6":"code","d638f833":"code","b2d42936":"markdown","d32fb204":"markdown","0cae64fb":"markdown","41eac0f2":"markdown","2e69bccb":"markdown","e0d46c4a":"markdown","144b7f11":"markdown","6f6d12b6":"markdown","2d5b6dba":"markdown","27e77b9e":"markdown","b898e945":"markdown","80bff046":"markdown","33604ae4":"markdown","0628d1c2":"markdown","dfc289c4":"markdown","a77349ec":"markdown","fa100e18":"markdown"},"source":{"ec3dd40c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/recommodel-dataset\/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f3559e58":"# Import packages\nfrom surprise import Reader, Dataset, SVD\nfrom surprise.model_selection.validation import cross_validate\n!pip install openpyxl\nimport openpyxl\nimport random\nimport matplotlib.pyplot as plt","c17e75f2":"UserMaster = pd.read_excel(\"\/kaggle\/input\/recommodel-dataset\/5_6165515447073506141.xlsx\")\nprint(UserMaster.shape)\nUserMaster","1b8c53f6":"UserMaster.describe(include='all')","c9a84de0":"###################### Cleaning data #####################################\nUserData = UserMaster[['user_id','name_and_surname','agespinner','addspinner','education','income','profession']]\nUserData = UserData.rename({'user_id': 'UserID', 'name_and_surname': 'Name', 'agespinner':'Age','addspinner':'User-Location','profession':'Profession',\n                           'education':'Education','income':'Income'}, axis=1)\nUserData","b3333b0d":"########### Import Post data information #####################\n\nPostMaster = pd.read_excel(\"\/kaggle\/input\/recomm-model-dataset2\/5_6174511346269291424.xlsx\")\nPostMaster = PostMaster.rename({'donation_type':'Donation Type','post_id':\"PostID\"},axis=1)\n\nimport random\nrandom.seed(1000)\n#given_list = ['Tom', 'nick', 'krish', 'jack']\ndef addrows(x):\n    res= []\n    for i in range(1,37):\n        r = random.sample(x,1)\n        res.append(r)\n    return(res)\na = np.array(addrows(list(UserData['UserID']))).reshape(-1)\nNeedyID_df = {'NeedID': a}\nNeedyID_df = pd.DataFrame(NeedyID_df)\nPostMaster = pd.concat([NeedyID_df.reset_index(drop=True), PostMaster], axis=1)\nPostMaster =PostMaster[['Donation Type','NeedID','PostID']]\nPostMaster","73385806":"random.seed(1000)\n#given_list = ['Tom', 'nick', 'krish', 'jack']\ndef addrows(x):\n    res= []\n    for i in range(1,10000):\n        r = random.sample(x,1)\n        res.append(r)\n    return(res)\n\n\n# Separate User IDS\nfrom sklearn import model_selection\nUserIds = UserData[\"UserID\"].unique()\nrandom.shuffle(UserIds)\nUser_IDS, Sim_User = model_selection.train_test_split(UserIds, train_size=0.9, test_size=0.1)\n\n\na = np.array(addrows(list(User_IDS))).reshape(-1)\nb = np.array(addrows(list(PostMaster[\"Donation Type\"].unique()))).reshape(-1)\nc = np.array(addrows(list(User_IDS))).reshape(-1)\n#d = np.array(addrows(list(Userdata[\"User-Location\"].unique()))).reshape(-1)\ne = np.array(addrows(list(['a1','a2','a3','a4']))).reshape(-1)\nNeedydata = {'HelperID':a,\n            'Donation Type':b,\n            'NeedID':c,\n            'PostID':e\n            }\n\nNeedydata = pd.DataFrame(Needydata)\nNeedydata","c2447182":"############## Removing User ID which are same in Helper and Need ID ################\nNeedydata[\"Same ID\"] = Needydata['HelperID']==Needydata['NeedID']\nprint(Needydata.shape)\nNeedydata = Needydata[Needydata['Same ID'] != True]\nprint(Needydata.shape)\nprint(Needydata['Same ID'].unique())\nNeedydata = Needydata.drop(columns='Same ID', axis=1)\n\n############## Data Prep for Model ################\nNeed_request = pd.DataFrame(Needydata)\nModelDatarequest = PostMaster.merge(UserData, left_on='NeedID',right_on='UserID', how='left')\nModelDatareq = ModelDatarequest.drop(['PostID',\"UserID\"],axis=1)\nModelDatareq","c2bde4bd":"Sim_User = UserData.copy()\nSim_User = Sim_User[~Sim_User['UserID'].isin(Needydata.HelperID)].reset_index(drop=True)\nSim_User['Age']=Sim_User['Age'].astype(str)\nSim_User['Income']=Sim_User['Income'].astype(str)\ndef combined_features(row):\n    return row['Name']+\" \"+row['Age']+\" \"+row['Education']+\" \"+row['Profession']+\" \"+row['Income']+\" \"+row['User-Location']\nSim_User[\"combined_features\"] = Sim_User.apply(combined_features, axis =1)\n\n\nNo_History = pd.DataFrame(UserData[~UserData.UserID.isin(Needydata.HelperID)].UserID)\nNo_History = pd.DataFrame(No_History[~No_History.UserID.isin(Needydata.NeedID)].UserID).reset_index(drop=True)\nNo_History['Similar'] = 0\n#No_History\n\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\ncv = CountVectorizer()\ncount_matrix = cv.fit_transform(Sim_User[\"combined_features\"])\n#print(\"Count Matrix:\", count_matrix.toarray())\n\ncosine_sim = cosine_similarity(count_matrix)\nmiss_data = pd.DataFrame()\n\ndef get_sim(x):\n    PostID=x\n    def get_index_from_title(title):\n        return Sim_User[Sim_User.UserID == title][\"UserID\"].index[0]\n    User_index = get_index_from_title(PostID)\n    #User_index\n\n    similar = list(enumerate(cosine_sim[User_index]))\n    cos_similar = sorted(similar, key=lambda x:x[1], reverse=True)\n\n    def get_title_from_index(PostID):\n        return Sim_User[Sim_User.index == PostID][\"UserID\"].values[0]\n    i=0\n    re= []\n    for recom in cos_similar:\n        #print(get_title_from_index(recom[0]))\n        a = get_title_from_index(recom[0])\n        re.append(a)\n        i=i+1\n        if i>1:\n            break\n    print(a)\n    No_History.iloc[o,1]=a\n    \nfor o in range(0,len(No_History)):\n    get_sim(No_History.iloc[o,0])\nNo_History","69f2bcfd":"ModelData = UserData[~UserData.UserID.isin(No_History)].reset_index(drop=True)\nModelData = Needydata.merge(UserData, left_on='NeedID',right_on='UserID', how='left')\n#ModelData['Donation Type'] = ModelData['Donation Type'].astype(str)\n#ModelData['PostID'] = ModelData['PostID'].astype(str)\n#ModelData['User-Location_Needy'] = ModelData['User-Location_Needy'].astype(str)\n#ModelData['Age'] = ModelData['Age'].astype(str)\n#ModelData['Name'] = ModelData['Name'].astype(str)\n#ModelData['Gender'] = ModelData['Gender'].astype(str)\n#ModelData['PostID'] = range(1, len(ModelData) + 1)\n#ModelData['Income'] = ModelData['Income'].astype('str')\nModelData = ModelData.drop(['PostID','UserID'],axis=1)\nModelData.head()","504a35f0":"import seaborn as sns\n\nfig_dims = (10, 6)\nfig, ax =plt.subplots(figsize=fig_dims)\nsns.countplot(x=\"Donation Type\", data=ModelData)","42702fbd":"fig_dims = (10, 6)\nfig, ax =plt.subplots(figsize=fig_dims)\nsns.countplot(x=\"Age\", data=ModelData)","333784fa":"fig_dims = (10, 6)\nfig, ax =plt.subplots(figsize=fig_dims)\nsns.countplot(x=\"User-Location\", data=ModelData)","c5b77c5a":"fig_dims = (10, 6)\nfig, ax =plt.subplots(figsize=fig_dims)\nsns.countplot(x=\"Profession\", data=ModelData)","136117ae":"a=ModelData[ModelData[\"HelperID\"]=='JtCrCSYllgXu5uhmPuoCRLPfI6A2']\n\nfig_dims = (20, 6)\nfig, ax =plt.subplots(1,2,figsize=fig_dims)\n# grouped barplot\nsns.countplot(x=\"User-Location\",  hue=\"Donation Type\", data=a, ax=ax[0])\nsns.countplot(x=\"Donation Type\", data=a,  ax=ax[1])","e2851445":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport sklearn\n\nX = ModelData.loc[:, ModelData.columns != 'HelperID'].values\ny = ModelData.loc[:, ModelData.columns == 'HelperID'].values\nz= ModelData.append(ModelDatareq)\nzz = z.loc[:, z.columns != 'HelperID'].values\n\n################# Label Encoding #################\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nfor i in range(0,8):\n    le.fit(zz[:,i])\n    X[:,i] = le.transform(X[:,i])\n\n################# Split train and test   using 80-20 rule #################\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\n################# Data Normalisation #################\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\ny_pred  =  classifier.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix,accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test,y_pred)\n","456da3b9":"ModelDatareq","117aa8d6":"recomData = ModelDatareq.iloc[:].values\nzz =  z.loc[:, z.columns != 'HelperID'].values\nfor i in range(0,8):\n    le.fit(zz[:,i])\n    recomData[:,i] = le.transform(recomData[:,i])\nrecomData = sc.transform(recomData)\nrecomData\n\nDonor_recomm = pd.DataFrame()\nfor i in range(0,36):\n    data = recomData[[i]]\n    Recommendation = {'Donor_Rec' : classifier.classes_, 'Strength' : classifier.predict_proba(data).reshape(-1)}\n    Recommendation = pd.DataFrame(Recommendation)\n    Recommendation[\"Needy ID\"] = ModelDatarequest.NeedID[i]\n    Recommendation[\"PostID\"] = ModelDatarequest.PostID[i]\n    Donor_recomm = Donor_recomm.append(Recommendation.sort_values(\"Strength\",ascending=False).head(10))\nDonor_recomm.head(15)","5a64f5a5":"# Saving the File\nDonor_recomm.to_csv('Donor_recomm_by_post.csv')","8a125ba1":"# Data Preparation\n\nModelData = Needydata.merge(UserData, left_on='HelperID',right_on='UserID', how='left')\n#ModelData['Donation Type'] = ModelData['Donation Type'].astype(str)\n#ModelData['PostID'] = ModelData['PostID'].astype(str)\n#ModelData['User-Location_Needy'] = ModelData['User-Location_Needy'].astype(str)\n#ModelData['Age'] = ModelData['Age'].astype(str)\n#ModelData['Name'] = ModelData['Name'].astype(str)\n#ModelData['Gender'] = ModelData['Gender'].astype(str)\n#ModelData['PostID'] = range(1, len(ModelData) + 1)\n#ModelData['Income'] = ModelData['Income'].astype('str')\nModelData = ModelData.drop(['PostID','UserID'],axis=1)\nModelData.head()\nHelp_request = pd.DataFrame(Needydata)\nModelDatareq = Help_request.merge(UserData, left_on='NeedID',right_on='UserID', how='left')\nModelDatareq = ModelDatareq.drop(['PostID',\"UserID\"],axis=1)\nModelDatareq","bb7d7048":"X = ModelData.loc[:, ModelData.columns != 'NeedID'].values\ny = ModelData.loc[:, ModelData.columns == 'NeedID'].values\n\n\nz= ModelData.append(ModelDatareq)\nzz = z.loc[:, z.columns != 'NeedID'].values\n\n################# Label Encoding #################\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nfor i in range(0,8):\n    le.fit(zz[:,i])\n    X[:,i] = le.transform(X[:,i])\n\n################# Split train and test   using 80-20 rule #################\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n################# Data Normalisation #################\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)\n\ny_pred  =  classifier.predict(X_test)\n\nfrom sklearn.metrics import confusion_matrix,accuracy_score\ncm = confusion_matrix(y_test, y_pred)\nac = accuracy_score(y_test,y_pred)\n","80852da6":"recomData = ModelDatareq.loc[:, ModelDatareq.columns != 'NeedID'].values\nzz =  z.loc[:, z.columns != 'NeedID'].values\nfor i in range(0,8):\n    le.fit(zz[:,i])\n    recomData[:,i] = le.transform(recomData[:,i])\nrecomData = sc.transform(recomData)\nrecomData\nNeedy_recomm = pd.DataFrame()\nfor i in range(0,36):\n    data = recomData[[i]]\n    Recommendation = {'Donor_ID' : classifier.classes_, 'Strength' : classifier.predict_proba(data).reshape(-1)}\n    Recommendation = pd.DataFrame(Recommendation)\n    Recommendation[\"Needy ID\"] = ModelDatarequest.NeedID[i]\n    Recommendation[\"PostID\"] = ModelDatarequest.PostID[i]\n    Needy_recomm = Needy_recomm.append(Recommendation.sort_values(\"Strength\",ascending=False).head(10))\n\nNeedy_recomm.sort_values('Donor_ID').head(10)","d638f833":"Needy_recomm.sort_values('Donor_ID').to_csv('needy_recomm_to_Helper.csv')","b2d42936":"#### Model Training","d32fb204":"# Load Data\n### 1. User Master Data includes all User related information\n### 2. Post Master Data has the information based on Posts in the app.","0cae64fb":"In the Above chart we have pulled the Helper_ID(Donor) 'JtCrCSYllgXu5uhmPuoCRLPfI6A2' information and see that this Donor have mostly Donated in User-Location of \"Goregoan\" and also we can see that this donor mostly Donates Cloth's.","41eac0f2":"## EDA Analysis","2e69bccb":"### Model Preparation\n\n##### We are using a classical Naive Baye's Model to predict the likelihood of Users\/Donor recommendation for each Post","e0d46c4a":"We have found 4 such userID which does not have histroy and we have mapped the nearest User which has history and are similar to them.","144b7f11":"### Generating Recommendations for Each User(Donor_ID)\n##### In the below table You will find 10 observation which is an example of 2 User(Donor), where we can see 7 post (PostID) recommendation for the 1st user(Donor_ID)","6f6d12b6":"### Generating Recommendations for Each Post\n##### In the below table You will find 20 observation which is an example of 2 Post(PostID) each have 10 (Donor_rec) recommendations","2d5b6dba":"In the above data Summary we can see there are total of 37 Unique Users where there is information about each User based on their education, income, profession, etc","27e77b9e":"The Donation type in the above chart reflect that the Donation are equally distributed in the hostorical data where it is not skewed to any particular donation","b898e945":"As we can see in the above figure that the most donation is coming from the Users who are from Educational background follwer by Volunteer and Health","80bff046":"#### Generating random data to create history for all the Users.","33604ae4":"In the above bar plot we see that the User who have made the Donations are mostly belong to age group of 20-30","0628d1c2":"The graoh above indicates that the Most Donation are made from \"Goregoan\" Location","dfc289c4":"### Model Training","a77349ec":"# Post Recommendation\n\n#### In this process we are recommending Post by Needy to each User (Donors), where we have use the same Naive Baye's algorithm to find the probability of each post recommendation to a particular Donor.","fa100e18":"# Similar User Identification by using Cosine Similarity algorithm\n\n#### Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis.\n#### Here we are using this information to find Users which are similar to each other, because in some user might not have historical data and which will lead to no prediction\/recommendation for that User. So using this method we can bring the information of the closest User and and use its history to make prediction for the new User user which has no historical information"}}