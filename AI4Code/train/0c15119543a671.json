{"cell_type":{"bf0b958a":"code","0dcd482c":"code","e7a32f04":"code","6c027b98":"code","2f06a44e":"code","39a4cbd9":"code","b876a58b":"code","21e72d9e":"code","5a77bb75":"code","e49e3241":"code","3007e258":"code","11873290":"code","8ff3277d":"code","bad3b20d":"code","14a7ca29":"code","9cfd2024":"code","4fa21b6e":"code","323cf899":"code","ea76b766":"code","90349ca2":"code","82350bc1":"code","679b6fd3":"code","38de3637":"code","04961d4e":"code","c2270b88":"code","ba053173":"code","8754317e":"code","c99c61fc":"markdown","0d9b9425":"markdown","a85c5e76":"markdown","4f99181b":"markdown","a03d2905":"markdown","c369c0a0":"markdown","90cdc2fd":"markdown"},"source":{"bf0b958a":"import pandas as pd\nimport numpy as np\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nimport gc\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\n\nfrom hyperopt import hp, tpe, Trials, STATUS_OK\nfrom hyperopt.fmin import fmin\nfrom hyperopt.pyll.stochastic import sample\n\nfrom IPython.display import HTML\n\nfrom plotly.subplots import make_subplots\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.tools as tls\nimport cufflinks as cf\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport matplotlib.ticker as ticker\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%config InlineBackend.figure_format = 'retina'\n\npd.options.plotting.backend = 'plotly'\npd.options.display.max_columns = 100\n\nSEED = 42\nnp.random.seed(SEED)\ncf.go_offline()","0dcd482c":"#GLOBAL HYPEROPT PARAMETERS\nNUM_EVALS = 1000 #number of hyperopt evaluation rounds\nN_FOLDS = 5 #number of cross-validation folds on data in each evaluation round\n\n#LIGHTGBM PARAMETERS\nLGBM_MAX_LEAVES = 2**11 #maximum number of leaves per tree for LightGBM\nLGBM_MAX_DEPTH = 25 #maximum tree depth for LightGBM\nEVAL_METRIC_LGBM_REG = 'mae' #LightGBM regression metric. Note that 'rmse' is more commonly used \nEVAL_METRIC_LGBM_CLASS = 'auc'#LightGBM classification metric\n\n#XGBOOST PARAMETERS\nXGB_MAX_LEAVES = 2**12 #maximum number of leaves when using histogram splitting\nXGB_MAX_DEPTH = 25 #maximum tree depth for XGBoost\n# EVAL_METRIC_XGB_REG = 'mae' #XGBoost regression metric\nEVAL_METRIC_XGB_REG = 'rmse' #XGBoost regression metric\nEVAL_METRIC_XGB_CLASS = 'auc' #XGBoost classification metric\n\n#CATBOOST PARAMETERS\nCB_MAX_DEPTH = 8 #maximum tree depth in CatBoost\n# OBJECTIVE_CB_REG = 'MAE' #CatBoost regression metric\nOBJECTIVE_CB_REG = 'RMSE' #CatBoost regression metric\nOBJECTIVE_CB_CLASS = 'Logloss' #CatBoost classification metric\n\n#OPTIONAL OUTPUT\nBEST_SCORE = 0","e7a32f04":"def quick_hyperopt(data, labels, package='lgbm', num_evals=NUM_EVALS, diagnostic=False):\n    \n    #==========\n    #LightGBM\n    #==========\n    \n    if package=='lgbm':\n        \n        print('Running {} rounds of LightGBM parameter optimisation:'.format(num_evals))\n        #clear space\n        gc.collect()\n        \n        integer_params = ['max_depth',\n                         'num_leaves',\n                          'max_bin',\n                         'min_data_in_leaf',\n                         'min_data_in_bin']\n        \n        def objective(space_params):\n            \n            #cast integer params from float to int\n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n            \n            #extract nested conditional parameters\n            if space_params['boosting']['boosting'] == 'goss':\n                top_rate = space_params['boosting'].get('top_rate')\n                other_rate = space_params['boosting'].get('other_rate')\n                #0 <= top_rate + other_rate <= 1\n                top_rate = max(top_rate, 0)\n                top_rate = min(top_rate, 0.5)\n                other_rate = max(other_rate, 0)\n                other_rate = min(other_rate, 0.5)\n                space_params['top_rate'] = top_rate\n                space_params['other_rate'] = other_rate\n            \n            subsample = space_params['boosting'].get('subsample', 1.0)\n            space_params['boosting'] = space_params['boosting']['boosting']\n            space_params['subsample'] = subsample\n            \n            #for classification, set stratified=True and metrics=EVAL_METRIC_LGBM_CLASS\n            cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=False,\n                                early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_REG, seed=42)\n            \n            best_loss = cv_results['l1-mean'][-1] #'l2-mean' for rmse\n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = 1 - cv_results['auc-mean'][-1]\n            #if necessary, replace 'auc-mean' with '[your-preferred-metric]-mean'\n            return{'loss':best_loss, 'status': STATUS_OK }\n        \n        train = lgb.Dataset(data, labels)\n                \n        #integer and string parameters, used with hp.choice()\n        boosting_list = [{'boosting': 'gbdt',\n                          'subsample': hp.uniform('subsample', 0.5, 1)},\n                         {'boosting': 'goss',\n                          'subsample': 1.0,\n                         'top_rate': hp.uniform('top_rate', 0, 0.5),\n                         'other_rate': hp.uniform('other_rate', 0, 0.5)}] #if including 'dart', make sure to set 'n_estimators'\n        metric_list = ['MAE', 'RMSE'] \n        #for classification comment out the line above and uncomment the line below\n        #metric_list = ['auc'] #modify as required for other classification metrics\n        objective_list_reg = ['huber', 'gamma', 'fair', 'tweedie']\n        objective_list_class = ['binary', 'cross_entropy']\n        #for classification set objective_list = objective_list_class\n        objective_list = objective_list_reg\n\n        space ={'boosting' : hp.choice('boosting', boosting_list),\n                'num_leaves' : hp.quniform('num_leaves', 2, LGBM_MAX_LEAVES, 1),\n                'max_depth': hp.quniform('max_depth', 2, LGBM_MAX_DEPTH, 1),\n                'max_bin': hp.quniform('max_bin', 32, 255, 1),\n                'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 256, 1),\n                'min_data_in_bin': hp.quniform('min_data_in_bin', 1, 256, 1),\n                'min_gain_to_split' : hp.quniform('min_gain_to_split', 0.1, 5, 0.01),\n                'lambda_l1' : hp.uniform('lambda_l1', 0, 5),\n                'lambda_l2' : hp.uniform('lambda_l2', 0, 5),\n                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n                'metric' : hp.choice('metric', metric_list),\n                'objective' : hp.choice('objective', objective_list),\n                'feature_fraction' : hp.quniform('feature_fraction', 0.5, 1, 0.01),\n                'bagging_fraction' : hp.quniform('bagging_fraction', 0.5, 1, 0.01)\n            }\n        \n        #optional: activate GPU for LightGBM\n        #follow compilation steps here:\n        #https:\/\/www.kaggle.com\/vinhnguyen\/gpu-acceleration-for-lightgbm\/\n        #then uncomment lines below:\n        #space['device'] = 'gpu'\n        #space['gpu_platform_id'] = 0,\n        #space['gpu_device_id'] =  0\n\n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n                \n        #fmin() will return the index of values chosen from the lists\/arrays in 'space'\n        #to obtain actual values, index values are used to subset the original lists\/arrays\n        best['boosting'] = boosting_list[best['boosting']]['boosting']#nested dict, index twice\n        best['metric'] = metric_list[best['metric']]\n        best['objective'] = objective_list[best['objective']]\n                \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n        \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    #==========\n    #XGBoost\n    #==========\n    \n    if package=='xgb':\n        \n        print('Running {} rounds of XGBoost parameter optimisation:'.format(num_evals))\n        #clear space\n        gc.collect()\n        \n        integer_params = ['max_depth']\n        \n        def objective(space_params):\n            \n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n                \n            #extract multiple nested tree_method conditional parameters\n            #libera te tutemet ex inferis\n            if space_params['tree_method']['tree_method'] == 'hist':\n                max_bin = space_params['tree_method'].get('max_bin')\n                space_params['max_bin'] = int(max_bin)\n                if space_params['tree_method']['grow_policy']['grow_policy']['grow_policy'] == 'depthwise':\n                    grow_policy = space_params['tree_method'].get('grow_policy').get('grow_policy').get('grow_policy')\n                    space_params['grow_policy'] = grow_policy\n                    space_params['tree_method'] = 'hist'\n                else:\n                    max_leaves = space_params['tree_method']['grow_policy']['grow_policy'].get('max_leaves')\n                    space_params['grow_policy'] = 'lossguide'\n                    space_params['max_leaves'] = int(max_leaves)\n                    space_params['tree_method'] = 'hist'\n            else:\n                space_params['tree_method'] = space_params['tree_method'].get('tree_method')\n                \n            #for classification replace EVAL_METRIC_XGB_REG with EVAL_METRIC_XGB_CLASS\n            cv_results = xgb.cv(space_params, train, nfold=N_FOLDS, metrics=[EVAL_METRIC_XGB_REG],\n                             early_stopping_rounds=100, stratified=False, seed=42)\n            \n            best_loss = cv_results['test-rmse-mean'].iloc[-1] #or 'test-mae-mean' if using RMSE\n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = 1 - cv_results['test-auc-mean'].iloc[-1]\n            #if necessary, replace 'test-auc-mean' with 'test-[your-preferred-metric]-mean'\n            return{'loss':best_loss, 'status': STATUS_OK }\n        \n        train = xgb.DMatrix(data, labels)\n        \n        #integer and string parameters, used with hp.choice()\n        boosting_list = ['gbtree', 'gblinear'] #if including 'dart', make sure to set 'n_estimators'\n        metric_list = ['MAE', 'RMSE'] \n        #for classification comment out the line above and uncomment the line below\n        #metric_list = ['auc']\n        #modify as required for other classification metrics classification\n        \n        tree_method = [{'tree_method' : 'exact'},\n               {'tree_method' : 'approx'},\n               {'tree_method' : 'hist',\n                'max_bin': hp.quniform('max_bin', 2**3, 2**7, 1),\n                'grow_policy' : {'grow_policy': {'grow_policy':'depthwise'},\n                                'grow_policy' : {'grow_policy':'lossguide',\n                                                  'max_leaves': hp.quniform('max_leaves', 32, XGB_MAX_LEAVES, 1)}}}]\n        \n        #if using GPU, replace 'exact' with 'gpu_exact' and 'hist' with\n        #'gpu_hist' in the nested dictionary above\n        \n        objective_list_reg = ['reg:linear', 'reg:gamma', 'reg:tweedie']\n        objective_list_class = ['reg:logistic', 'binary:logistic']\n        #for classification change line below to 'objective_list = objective_list_class'\n        objective_list = objective_list_reg\n        \n        space ={'boosting' : hp.choice('boosting', boosting_list),\n                'tree_method' : hp.choice('tree_method', tree_method),\n                'max_depth': hp.quniform('max_depth', 2, XGB_MAX_DEPTH, 1),\n                'reg_alpha' : hp.uniform('reg_alpha', 0, 5),\n                'reg_lambda' : hp.uniform('reg_lambda', 0, 5),\n                'min_child_weight' : hp.uniform('min_child_weight', 0, 5),\n                'gamma' : hp.uniform('gamma', 0, 5),\n                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n                'eval_metric' : hp.choice('eval_metric', metric_list),\n                'objective' : hp.choice('objective', objective_list),\n                'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1, 0.01),\n                'colsample_bynode' : hp.quniform('colsample_bynode', 0.1, 1, 0.01),\n                'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),\n                'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n                'nthread' : -1\n            }\n        \n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n        \n        best['tree_method'] = tree_method[best['tree_method']]['tree_method']\n        best['boosting'] = boosting_list[best['boosting']]\n        best['eval_metric'] = metric_list[best['eval_metric']]\n        best['objective'] = objective_list[best['objective']]\n        \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n        if 'max_leaves' in best:\n            best['max_leaves'] = int(best['max_leaves'])\n        if 'max_bin' in best:\n            best['max_bin'] = int(best['max_bin'])\n        \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        \n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    #==========\n    #CatBoost\n    #==========\n    \n    if package=='cb':\n        \n        print('Running {} rounds of CatBoost parameter optimisation:'.format(num_evals))\n        \n        #clear memory \n        gc.collect()\n            \n        integer_params = ['depth',\n                          #'one_hot_max_size', #for categorical data\n                          'min_data_in_leaf',\n                          'max_bin']\n        \n        def objective(space_params):\n                        \n            #cast integer params from float to int\n            for param in integer_params:\n                space_params[param] = int(space_params[param])\n                \n            #extract nested conditional parameters\n            if space_params['bootstrap_type']['bootstrap_type'] == 'Bayesian':\n                bagging_temp = space_params['bootstrap_type'].get('bagging_temperature')\n                space_params['bagging_temperature'] = bagging_temp\n                \n            if space_params['grow_policy']['grow_policy'] == 'LossGuide':\n                max_leaves = space_params['grow_policy'].get('max_leaves')\n                space_params['max_leaves'] = int(max_leaves)\n                \n            space_params['bootstrap_type'] = space_params['bootstrap_type']['bootstrap_type']\n            space_params['grow_policy'] = space_params['grow_policy']['grow_policy']\n                           \n            #random_strength cannot be < 0\n            space_params['random_strength'] = max(space_params['random_strength'], 0)\n            #fold_len_multiplier cannot be < 1\n            space_params['fold_len_multiplier'] = max(space_params['fold_len_multiplier'], 1)\n                       \n            #for classification set stratified=True\n            cv_results = cb.cv(train, space_params, fold_count=N_FOLDS, \n                             early_stopping_rounds=25, stratified=False, partition_random_seed=42)\n           \n            best_loss = cv_results['test-RMSE-mean'].iloc[-1] #'test-MAE-mean' for MAE\n            #for classification, comment out the line above and uncomment the line below:\n            #best_loss = cv_results['test-Logloss-mean'].iloc[-1]\n            #if necessary, replace 'test-Logloss-mean' with 'test-[your-preferred-metric]-mean'\n            \n            return{'loss':best_loss, 'status': STATUS_OK}\n        \n        train = cb.Pool(data, labels.astype('float32'))\n        \n        #integer and string parameters, used with hp.choice()\n        bootstrap_type = [{'bootstrap_type':'Poisson'}, \n                           {'bootstrap_type':'Bayesian',\n                            'bagging_temperature' : hp.loguniform('bagging_temperature', np.log(1), np.log(50))},\n                          {'bootstrap_type':'Bernoulli'}] \n        LEB = ['No', 'AnyImprovement', 'Armijo'] #remove 'Armijo' if not using GPU\n        #score_function = ['Correlation', 'L2', 'NewtonCorrelation', 'NewtonL2']\n        grow_policy = [{'grow_policy':'SymmetricTree'},\n                       {'grow_policy':'Depthwise'},\n                       {'grow_policy':'Lossguide',\n                        'max_leaves': hp.quniform('max_leaves', 2, 32, 1)}]\n        eval_metric_list_reg = ['MAE', 'RMSE', 'Poisson']\n        eval_metric_list_class = ['Logloss', 'AUC', 'F1']\n        #for classification change line below to 'eval_metric_list = eval_metric_list_class'\n        eval_metric_list = eval_metric_list_reg\n                \n        space ={'depth': hp.quniform('depth', 2, CB_MAX_DEPTH, 1),\n                'max_bin' : hp.quniform('max_bin', 1, 32, 1), #if using CPU just set this to 254\n                'l2_leaf_reg' : hp.uniform('l2_leaf_reg', 0, 5),\n                'min_data_in_leaf' : hp.quniform('min_data_in_leaf', 1, 50, 1),\n                'random_strength' : hp.loguniform('random_strength', np.log(0.005), np.log(5)),\n                #'one_hot_max_size' : hp.quniform('one_hot_max_size', 2, 16, 1), #uncomment if using categorical features\n                'bootstrap_type' : hp.choice('bootstrap_type', bootstrap_type),\n                'learning_rate' : hp.uniform('learning_rate', 0.05, 0.25),\n                'eval_metric' : hp.choice('eval_metric', eval_metric_list),\n                'objective' : OBJECTIVE_CB_REG,\n                #'score_function' : hp.choice('score_function', score_function), #crashes kernel - reason unknown\n                'leaf_estimation_backtracking' : hp.choice('leaf_estimation_backtracking', LEB),\n                'grow_policy': hp.choice('grow_policy', grow_policy),\n                #'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),# CPU only\n                'fold_len_multiplier' : hp.loguniform('fold_len_multiplier', np.log(1.01), np.log(2.5)),\n                'od_type' : 'Iter',\n                'od_wait' : 25,\n                'task_type' : 'GPU',\n                'verbose' : 0\n            }\n        \n        #optional: run CatBoost without GPU\n        #uncomment line below\n        #space['task_type'] = 'CPU'\n            \n        trials = Trials()\n        best = fmin(fn=objective,\n                    space=space,\n                    algo=tpe.suggest,\n                    max_evals=num_evals, \n                    trials=trials)\n        \n        #unpack nested dicts first\n        best['bootstrap_type'] = bootstrap_type[best['bootstrap_type']]['bootstrap_type']\n        best['grow_policy'] = grow_policy[best['grow_policy']]['grow_policy']\n        best['eval_metric'] = eval_metric_list[best['eval_metric']]\n        \n        #best['score_function'] = score_function[best['score_function']] \n        #best['leaf_estimation_method'] = LEM[best['leaf_estimation_method']] #CPU only\n        best['leaf_estimation_backtracking'] = LEB[best['leaf_estimation_backtracking']]        \n        \n        #cast floats of integer params to int\n        for param in integer_params:\n            best[param] = int(best[param])\n        if 'max_leaves' in best:\n            best['max_leaves'] = int(best['max_leaves'])\n        \n        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n        \n        if diagnostic:\n            return(best, trials)\n        else:\n            return(best)\n    \n    else:\n        print('Package not recognised. Please use \"lgbm\" for LightGBM, \"xgb\" for XGBoost or \"cb\" for CatBoost.') ","6c027b98":"def reduce_mem_usage(df: pd.DataFrame,\n                     verbose: bool = True) -> pd.DataFrame:\n    \n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if (c_min > np.iinfo(np.int8).min\n                        and c_max < np.iinfo(np.int8).max):\n                    df[col] = df[col].astype(np.int8)\n                elif (c_min > np.iinfo(np.int16).min\n                      and c_max < np.iinfo(np.int16).max):\n                    df[col] = df[col].astype(np.int16)\n                elif (c_min > np.iinfo(np.int32).min\n                      and c_max < np.iinfo(np.int32).max):\n                    df[col] = df[col].astype(np.int32)\n                elif (c_min > np.iinfo(np.int64).min\n                      and c_max < np.iinfo(np.int64).max):\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if (c_min > np.finfo(np.float16).min\n                        and c_max < np.finfo(np.float16).max):\n                    df[col] = df[col].astype(np.float16)\n                elif (c_min > reduce_mem_usagenp.finfo(np.float32).min\n                      and c_max < np.finfo(np.float32).max):\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    reduction = (start_mem - end_mem) \/ start_mem\n\n    msg = f'Mem. usage decreased to {end_mem:5.2f} MB ({reduction * 100:.1f} % reduction)'\n    if verbose:\n        print(msg)\n\n    return df","2f06a44e":"def feature_importances(df, model, model_name, max_num_features=10):\n    feature_importances = pd.DataFrame(columns = ['feature', 'importance'])\n    feature_importances['feature'] = df.columns\n    feature_importances['importance'] = model.feature_importances_\n    feature_importances.sort_values(by='importance', ascending=False, inplace=True)\n    feature_importances = feature_importances[:max_num_features]\n    plt.figure(figsize=(12, 6));\n    sns.barplot(x=\"importance\", y=\"feature\", data=feature_importances);\n    plt.title(model_name+' features importance:');","39a4cbd9":"train = pd.read_csv(\"\/kaggle\/input\/mytracker-mlhomework1-fall20\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/mytracker-mlhomework1-fall20\/test.csv\")","b876a58b":"# \u043d\u0430\u043a\u0438\u0434\u044b\u0432\u0430\u0435\u043c \u043c\u0435\u0434\u0438\u0430\u043d\u0443 (\u0438\u043b\u0438 \u0447\u0442\u043e-\u0442\u043e \u0434\u0440\u0443\u0433\u043e\u0435?) \u0432\u043c\u0435\u0441\u0442\u043e \u043f\u0443\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\n# train['Holiday'] =  train['Holiday'].fillna(np.median(train['Holiday'].median())).astype(\"int64\")\n\ntrain['Holiday'] =  train['Holiday'].fillna(0)","21e72d9e":"for dataset in [train, test]:\n    dataset['Date'] = pd.to_datetime(dataset['Date'])","5a77bb75":"# \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u043f\u043e \u0434\u0430\u0442\u0435\ntrain.sort_values(by=['Date'],inplace=True,ascending=True)\ntest.sort_values(by=['Date'],inplace=True,ascending=True)","e49e3241":"def is_weekend(row):\n    if row['DayOfWeek'] > 4:\n        return 1\n    else:\n        return 0","3007e258":"df_tmp=train.copy()","11873290":"# df_tmp[\"saleYear\"] = df_tmp['Date'].dt.year\ndf_tmp[\"saleMonth\"] = df_tmp['Date'].dt.month\ndf_tmp[\"saleDay\"] = df_tmp['Date'].dt.day\n# df_tmp[\"saleDayOfWeek\"] = df_tmp['Date'].dt.dayofweek\ndf_tmp[\"saleDayOfYear\"] = df_tmp['Date'].dt.dayofyear\ndf_tmp[\"saleQuarter\"] = df_tmp[\"Date\"].dt.quarter\ndf_tmp[\"saleWeekNum\"] = df_tmp['Date'].dt.week\n\ndf_tmp[\"saleIsWeekEnd\"] = df_tmp.apply(lambda row: is_weekend(row), axis=1)\ndf_tmp[\"saleIsMonthEnd\"] = df_tmp['Date'].dt.is_month_end.astype(np.int8)\n# df_tmp[\"saleIsMonthStart\"] = df_tmp['Date'].dt.is_month_start.astype(np.int8)\n# df_tmp[\"saleIsYearEnd\"] = df_tmp['Date'].dt.is_year_end.astype(np.int8)\n# df_tmp[\"saleIsYearStart\"] = df_tmp['Date'].dt.is_year_start.astype(np.int8)\n# df_tmp[\"saleIsQuarterEnd\"] = df_tmp['Date'].dt.is_quarter_end.astype(np.int8)\n# df_tmp[\"saleIsQuarterStart\"] = df_tmp['Date'].dt.is_quarter_start.astype(np.int8)\n\ndf_tmp['summer'] = (df_tmp['saleMonth'].isin(['6', '7', '8'])).astype(np.int8)\ndf_tmp['autumn'] = (df_tmp['saleMonth'].isin(['9', '10', '11'])).astype(np.int8)\ndf_tmp['winter'] = (df_tmp['saleMonth'].isin(['12', '1', '2'])).astype(np.int8)\ndf_tmp['spring'] = (df_tmp['saleMonth'].isin(['3', '4', '5'])).astype(np.int8)\n\n# # TODO \u043f\u0440\u043e\u0442\u0435\u0441\u0442\u0438\u0442\u044c \u043f\u0440\u043e\u0434\u0430\u0436\u0438 \u043f\u043e \u0441\u0435\u0437\u043e\u043d\u0430\u043c\n# df_tmp['saleSummer'] = df_tmp['summer'].astype('str') + '-->' + df_tmp['Sales'].astype('str')\n# df_tmp['saleAutumn'] = df_tmp['autumn'].astype('str') + '-->' + df_tmp['Sales'].astype('str')\n# df_tmp['saleWinter'] = df_tmp['winter'].astype('str') + '-->' + df_tmp['Sales'].astype('str')\n# df_tmp['saleSpring'] = df_tmp['spring'].astype('str') + '-->' + df_tmp['Sales'].astype('str')\n# df_tmp = df_tmp.drop(columns=['summer', 'autumn', 'winter', 'spring'])","8ff3277d":"df_tmp = reduce_mem_usage(df_tmp)\n# \ngc.collect()","bad3b20d":"def RMSLE(y_true, y_pred, *args, **kwargs):\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))","14a7ca29":"y = df_tmp[\"Sales\"]\n\ndel df_tmp[\"Sales\"]\ngc.collect()","9cfd2024":"df_tmp = df_tmp.drop([\n    \"Date\",\n    'Customers',\n    'Open',\n    'Promo'\n], axis=1)\ngc.collect()","4fa21b6e":"X = df_tmp.copy()","323cf899":"X_train, X_validation, y_train, y_validation = train_test_split(\n    X, y, train_size=0.7, random_state=SEED\n)","ea76b766":"# #obtain optimised parameter dictionary\n# lgbm_params = quick_hyperopt(X_train, y_train, 'lgbm', 2500)","90349ca2":"# #open with: lgbm_params = np.load('..\/input\/YOUR_FILEPATH\/lgbm_params.npy').item()\n# np.save('lgbm_params.npy', lgbm_params)","82350bc1":"# xgb_params = quick_hyperopt(X_train, y_train, 'xgb', 2000)","679b6fd3":"# np.save('xgb_params.npy', xgb_params)","38de3637":"cb_params = quick_hyperopt(X_train, y_train, 'cb', 200)","04961d4e":"np.save('cb_params.npy', cb_params)","c2270b88":"cb_params","ba053173":"# example_params, example_trials = quick_hyperopt(\n#     X_train, y_train, 'lgbm', 5, diagnostic=True\n# )","8754317e":"# example_trials.trials[3]","c99c61fc":"# Prediction","0d9b9425":"![](https:\/\/siliconangle.com\/wp-content\/blogs.dir\/1\/files\/2017\/07\/Yandex-CatBoost.png)","a85c5e76":"# Diagnostic Mode","4f99181b":"![](https:\/\/i.imgur.com\/Jqw0FGz.jpg)","a03d2905":"![](https:\/\/raw.githubusercontent.com\/dmlc\/dmlc.github.io\/master\/img\/logo-m\/xgboost.png)","c369c0a0":"The parameter dictionary can then be saved as a kernel output. Remember to use .item() when loading it in a new kernel.","90cdc2fd":"![image.png](attachment:image.png)"}}