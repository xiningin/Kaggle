{"cell_type":{"a4859725":"code","7fb5ba07":"code","01a46bbe":"code","0dfd8cf7":"code","2468cd09":"code","aaf982d1":"code","2e8efde8":"code","b81123b8":"code","648323bd":"code","4780c1b8":"code","9363ba98":"code","a87fb183":"code","57a81874":"code","1dd66318":"code","34c1db06":"code","1bf8b1c4":"code","b1f3296b":"code","bb65de80":"markdown","7b709682":"markdown","bcf8923b":"markdown","5b546679":"markdown","9336b4cf":"markdown","042e7dc7":"markdown","7f12a0f4":"markdown"},"source":{"a4859725":"import numpy as np\nimport pandas as pd\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)","7fb5ba07":"data = pd.read_csv('\/kaggle\/input\/train-set-metadata-for-dfdc\/metadata', low_memory=False)","01a46bbe":"(data['pxl.hash'] == data['pxl.hash.orig']).value_counts()","0dfd8cf7":"data['md5'].value_counts().value_counts().head()","2468cd09":"data['wav.hash'].value_counts().value_counts().head()","aaf982d1":"data['pxl.hash'].value_counts().value_counts().head()","2e8efde8":"data.head()","b81123b8":"data.shape","648323bd":"data.label.value_counts()","4780c1b8":"data.split.value_counts()","9363ba98":"set(data.original) - set(data.filename)","a87fb183":"set(data.loc[data.original == 'NAN', 'filename']) - set(data.original)","57a81874":"data.loc[data.original != 'NAN', 'original'].value_counts().hist(bins=40)","1dd66318":"data.loc[data.original != 'NAN', 'original'].value_counts().value_counts().head()","34c1db06":"for col in data.columns:\n    print(pd.crosstab(data[col],data['label']))","1bf8b1c4":"pd.crosstab(data['video.@display_aspect_ratio'],data['label'])","b1f3296b":"pd.crosstab([data['video.@display_aspect_ratio'], data['audio.@codec_time_base']],data['label'])","bb65de80":"### This notebook explores what we have in the DFDC full train set metadata and json files.","7b709682":"# Hashes","bcf8923b":"The dataset for this notebook  is at\nhttps:\/\/www.kaggle.com\/zaharch\/train-set-metadata-for-dfdc\n\nThe train data for this competition is big, almost 500Gb, so I hope it can be useful to have all the json files and the metadata in one dataframe.\n\nThe dataset includes, for each video file\n1. Info from the json files: **filename**, **folder**, **label**, **original**\n2. **split**: train (118346 videos), public validation test (400 videos) or train sample (400 videos). 119146 videos in total. Note that the public validation and the train sample are subsets of the full train, so it is enough to mark them in this dataframe.\n3. Full file **md5** column\n4. Hash on audio file sequence **wav.hash** and on subset of pixels **pxl.hash**\n5. The rest are metadata fields from the files, obtained with ffprobe. Note that I removed many columns, which didn't give new information.","5b546679":"There are duplicated for both **md5**, **pxl.hash** and **wav.hash**. Duplicates for **wav.hash** are OK, but duplicates for **md5** mean that there are identical files in the dataset.","9336b4cf":"This is how the data looks like","042e7dc7":"Fakes always have at least some pixel-level changes. That means that all audio fakes are also video fakes.","7f12a0f4":"# Other fields"}}