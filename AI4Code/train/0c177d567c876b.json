{"cell_type":{"15897a6d":"code","b509b2d7":"code","d6bf27cc":"code","e0fb4da3":"code","ba900851":"code","ee0c49f7":"code","7b27b523":"code","1f2f5c81":"code","75dbbfba":"code","2e94d0c2":"code","1491242f":"code","71a005e9":"markdown","32e374d8":"markdown","9134bad4":"markdown","0c71b877":"markdown","afb60291":"markdown"},"source":{"15897a6d":"import numpy as np \nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold","b509b2d7":"df = pd.read_csv(\"..\/input\/train.csv\")","d6bf27cc":"def preprocess(df):\n    \"\"\"\n    Main feature engineering function.\n    \"\"\"\n    def mk_categoricals(df, prefixes=None, subsets=None):\n        \"\"\"\n        Converts one-hot-encoded categorical to true categorical.\n        prefixes: list of prefixes of one-hot-encoded categorical variables\n                  e.g. for variables\n                      abastaguadentro, =1 if water provision inside the dwelling\n                      abastaguafuera, =1 if water provision outside the dwelling\n                      abastaguano, =1 if no water provision\n                  we provide prefix \"abastagua\"\n        subsets: dictionary {name_of_feature: [columns], ...}\n                 e.g. for variables\n                     public, \"=1 electricity from CNFL,  ICE,  ESPH\/JASEC\"\n                     planpri, =1 electricity from private plant\n                     noelec, =1 no electricity in the dwelling\n                     coopele, =1 electricity from cooperative\n                 we provide {\"electricity\": ['public', 'planpri', 'noelec', 'coopele']}\n        \"\"\"\n        def mk_category(dummies):\n            assert (dummies.sum(axis=1) <= 1).all()\n            nans = dummies.sum(axis=1) != 1\n            if nans.any():\n                dummies = dummies.assign(_na=nans.astype(int))\n            return dummies.idxmax(axis=1).astype('category')\n\n        categoricals = pd.DataFrame()\n\n        if prefixes:\n            for prefix in prefixes:\n                columns = df.columns[df.columns.str.startswith(prefix)]\n                categoricals[prefix] = mk_category(df[columns])\n        if subsets:\n            for feature_name, subset in subsets.items():\n                categoricals[feature_name] = mk_category(df[subset])\n\n        return categoricals\n    groupper = df.groupby('idhogar')\n    interactions = (pd.DataFrame(dict(\n                    head_escolari=df.parentesco1 * df.escolari,\n                    head_female=df.parentesco1 * df.female,\n                    head_partner_escolari=df.parentesco2 * df.escolari))\n                    .groupby(df.idhogar)\n                    .max())\n    # basic interaction features\n    \n    my_features = (groupper.mean()[['escolari', 'age', 'hogar_nin', \n                                    'hogar_total', 'epared3', 'epared1',\n                                    'etecho3', 'etecho1', 'eviv3', 'eviv1',\n                                    'male',\n                                    'r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', \n                                    'r4m3',\n                                    'r4t1', 'r4t2', 'r4t3', 'v2a1', 'rooms', \n                                    'bedrooms',\n                                    'meaneduc', \n                                    'SQBdependency', 'rez_esc', 'refrig', \n                                    'tamviv', 'overcrowding']]\n                   .join(groupper.std()[['escolari', 'age']], \n                         rsuffix='_std')\n                   .join(groupper[['escolari', 'age']].min(), rsuffix=\"_min\")\n                   .join(groupper[['escolari', 'age']].max(), rsuffix=\"_max\")\n                   .join(groupper[['dis']].sum(), rsuffix=\"_sum\")\n                   # partially based on\n                   # https:\/\/www.kaggle.com\/taindow\/predicting-poverty-levels-with-r\n                   .assign(child_rate=lambda x: x.hogar_nin \/ x.hogar_total,\n                           wrf=lambda x: x.epared3 - x.epared1 +\n                                         x.etecho3 - x.etecho1 +\n                                         x.eviv3 - x.eviv1,\n                           # wrf is an integral feature that measure\n                           # quality of the house\n                           escolari_range=lambda x: x.escolari_max - x.escolari_min,\n                           age_range=lambda x: x.age_max - x.age_min,\n                           rent_per_individual=lambda x: x.v2a1 \/ x.r4t3,\n                           rent_per_child=lambda x: x.v2a1 \/ x.r4t1,\n                           rent_per_over65=lambda x: x.v2a1 \/ x.r4t3,\n                           rent_per_room=lambda x: x.v2a1 \/ x.rooms,\n                           rent_per_bedroom=lambda x: x.v2a1 \/ x.bedrooms,\n                           rooms_per_individual=lambda x: x.rooms \/ x.r4t3,\n                           rooms_per_child=lambda x: x.rooms \/ x.r4t1,\n                           bedrooms_per_individual=lambda x: x.bedrooms \/ x.r4t3,\n                           bedrooms_per_child=lambda x: x.bedrooms \/ x.r4t1,\n                           years_schooling_per_individual=lambda x: x.escolari \/ x.r4t3,\n                           years_schooling_per_adult=lambda x: x.escolari \/ (x.r4t3 - x.r4t1),\n                           years_schooling_per_child=lambda x: x.escolari \/ x.r4t3\n                          )\n                   .drop(['hogar_nin', 'hogar_total', 'epared3', 'epared1',\n                                   'etecho3', 'etecho1', 'eviv3', 'eviv1'], \n                         axis=1)\n                   .join(interactions)\n                   .join(groupper[['computer', 'television', \n                                   'qmobilephone', 'v18q1']]\n                         .mean().sum(axis=1).rename('technics'))\n                   # we provide integral technical level as a new feature \n                   .assign(technics_per_individual=lambda x: x.technics \/ x.r4t3,\n                           technics_per_child=lambda x: x.technics \/ x.r4t1)\n                   .join(mk_categoricals(groupper.mean(), \n                                prefixes=['lugar', 'area', 'tipovivi', \n                                          'energcocinar', \n                                          'sanitario', 'pared', 'piso',\n                                          'abastagua'],\n                                subsets={'electricity': ['public', \n                                                         'planpri', \n                                                         'noelec', \n                                                         'coopele']}))\n                  )\n    return my_features","e0fb4da3":"X = preprocess(df)\ny = df.groupby('idhogar').Target.mean().astype(int)\n# for some households, different Target values are given\n# this is probably mistake\n# we will try to fix it by using mean()","ba900851":"clf = lgb.LGBMClassifier(class_weight='balanced', boosting_type='dart',\n                         drop_rate=0.9, min_data_in_leaf=100, \n                         max_bin=255,\n                         n_estimators=500,\n                         bagging_fraction=0.01,\n                         min_sum_hessian_in_leaf=1,\n                         importance_type='gain',\n                         learning_rate=0.1, \n                         max_depth=-1, \n                         num_leaves=31)","ee0c49f7":"df_test = pd.read_csv(\"..\/input\/test.csv\").set_index('Id')\nX_test = preprocess(df_test)","7b27b523":"kf = StratifiedKFold(n_splits=5, shuffle=True)\n# partially based on https:\/\/www.kaggle.com\/c0conuts\/xgb-k-folds-fastai-pca\npredicts = []\nfor train_index, test_index in kf.split(X, y):\n    print(\"###\")\n    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    clf.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n            early_stopping_rounds=20)\n    predicts.append(clf.predict(X_test))","1f2f5c81":"predict_by_hh = pd.DataFrame(np.array(predicts).mean(axis=0).round().astype(int),\n                             columns=['Target'],\n                             index=X_test.index)","75dbbfba":"predict = df_test.join(predict_by_hh, on='idhogar')[['Target']].astype(int)","2e94d0c2":"predict.to_csv(\"output.csv\")","1491242f":"sorted(zip(clf.feature_importances_, X.columns), reverse=True)","71a005e9":"## Preprocessing \/ feature engineering\nThe input data are per-individual, but predictions should be finally made per-household. (Technically, the expected output is per-individual as well, but only predictions for head of househould are taken into account. So, it's per-household.)  Due to this,  we will work per-household during model fitting \/ predicting.\n\nSome features (like `rooms`, `bedrooms`) are in fact per-household, others are per-individuals. We transform per-individual features to per-households by groupping and aggregating. The particular aggregation function depends on the feature (`mean`, `std`, `max`, `min`, etc.). Sometimes, we apply different aggregation function to the same features: in particular, `mean` and `std` of `escolari` and `age` are found to be very useful.\n\nWe also have lots of categorical variables in the dataset. They are already one-hot-encoded. However, we will use LightGBM that is capable of using \"true\" categorical variables. So we reencode them as categoricals, see function `mk_categoricals` below.\n\nWe also add several new features that are based on interactions between given features, partially based on [this kernel](https:\/\/www.kaggle.com\/taindow\/predicting-poverty-levels-with-r) by [@taindow](https:\/\/www.kaggle.com\/taindow).\n\n","32e374d8":"## Feature importance","9134bad4":"## Learn and predict with early stopping\nI found the idea in [this kernel](https:\/\/www.kaggle.com\/c0conuts\/xgb-k-folds-fastai-pca]) by [Nicolas Maignan](https:\/\/www.kaggle.com\/c0conuts) and it increased performance drastically. We use `StratifiedKFold` to split our dataset into 5 folds, select one fold as validation set and train model with early stopping using the rest 4 folds as a training set. Then we use this model to predict outcomes for test set and record the predictions. Repeat 5 times, so every fold is validation set one time. Then we calculate average of predictions (here we use the fact that Target is in fact ordered and can be treated as numeric, so averaging is possible and looks reasonable).","0c71b877":"## Summary\nWe train LightGBM DART model with early stopping via 5-fold cross-validation for [Costa Rican Household Poverty Level Prediction](https:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction).\n\nInteresting observations:\n\n- standard deviation of years of schooling and age per household are important features.\n- early stopping and averaging of predictions over models trained during 5-fold cross-valudation improves performance drastically.","afb60291":"## Model\n\nWe will use LightGMB with sklearn interface. Some parameters added to avoid overfitting, but I'm actually not sure they are useful (i.e. `drop_rate`, `baggin_fraction` and so on).  `class_weight='balanced'` seem to be neccessary as our data are very imbalanced."}}