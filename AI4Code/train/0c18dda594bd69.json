{"cell_type":{"1ab3ba28":"code","7b1190cb":"code","c15f8492":"code","ea92c391":"code","85a724c1":"code","f388d086":"code","62be3444":"code","d3c0d1a5":"code","bfb0cad9":"code","5109e162":"code","6868ac7e":"code","984981cf":"code","0f3699d3":"code","128ebc2e":"code","02b76ada":"code","778e4782":"code","f9b456ab":"code","68153a8b":"code","da0e1917":"code","4b3c4c12":"code","38acf131":"code","80937879":"code","a3a133b7":"code","4f51f18f":"code","d7f0307d":"code","395ba2ec":"code","58b32086":"code","15e72b66":"code","617663de":"code","dbcaa4cb":"code","5282d374":"code","56da4b93":"code","00832305":"code","3d0b4b28":"code","544ffb41":"code","627d7928":"code","a1cf65ff":"code","22aa18db":"code","8a9c0cf6":"code","1467e88b":"code","aa9b0f89":"code","ccad0931":"code","27b14a1a":"code","3bd4ac0c":"code","c613bd82":"code","d296e416":"code","9f3b2f0d":"code","44510d24":"code","0bdccab1":"code","64841f10":"code","13793c04":"code","1f930766":"code","2895be3f":"code","cc505f32":"code","5911e859":"code","1146d81e":"code","72ce2f39":"code","f146ded0":"code","bf341b58":"code","b946a8ee":"code","b74d3a2b":"code","8a3d2c8e":"code","7ef35de2":"code","2d57fa32":"code","6a513508":"code","84b04dae":"code","8892d8e2":"code","0d8ee5c6":"code","6cae87dd":"code","b9f2031c":"code","a46dcb9a":"code","520420e8":"code","16ad0a69":"code","a7320bbc":"code","21dfb698":"code","042df468":"code","ee380230":"code","5704be19":"code","1bf7440b":"code","a019e63d":"code","291a20b9":"code","f807c46f":"markdown","b4371eb2":"markdown","4d6daa06":"markdown","2ae708c7":"markdown","9d752e60":"markdown","d89a09a5":"markdown","fc9bf005":"markdown","e63ab1dc":"markdown","13655c06":"markdown","710a4c26":"markdown","5e541487":"markdown","16da7d19":"markdown","d5edcf48":"markdown","06736918":"markdown","647b23be":"markdown","a076fb15":"markdown","983cfc62":"markdown","5559ef2a":"markdown","6b1a582a":"markdown","5b3a2180":"markdown","7c414442":"markdown","ede3bffb":"markdown","05b6218f":"markdown","fd55ab7a":"markdown","992b8abe":"markdown","fec04b36":"markdown","ca4efa93":"markdown","424075cc":"markdown","06fef737":"markdown","d7b71a48":"markdown","5ed2573a":"markdown","4ea16c6b":"markdown","dd0dd77e":"markdown","6a8ec142":"markdown","7fc089e4":"markdown","a6392ff0":"markdown","4e935390":"markdown","e55f9413":"markdown","eae694b2":"markdown","c3206195":"markdown","404ab2e5":"markdown","ec123728":"markdown","146933e5":"markdown","d2311546":"markdown","87ce0dad":"markdown","b4e7989e":"markdown","2446323d":"markdown","9a5ff5b6":"markdown","95d1a3dc":"markdown","a28c31e2":"markdown","00b985a9":"markdown","e6cd2701":"markdown","e294aa1e":"markdown","0364b382":"markdown","6ff031f2":"markdown","a17d677f":"markdown","21888e78":"markdown","a80860fe":"markdown","d8834798":"markdown","e8ccafe8":"markdown","fa59dcf2":"markdown","8525b8ca":"markdown","170d7140":"markdown","d8f767b0":"markdown"},"source":{"1ab3ba28":"#Import the libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\nsns.set(style='white', context='notebook', palette='deep')","7b1190cb":"# load the databse\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nIDtest = test[\"PassengerId\"]","c15f8492":"# see the training data\ntrain.head()","ea92c391":"# See the testing data\ntest.head()","85a724c1":"# import required libraries\nfrom collections import Counter\n\n# Outlier detection \ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","f388d086":"#Outliers in train\ntrain.loc[Outliers_to_drop]","62be3444":"# Drop outliers\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","d3c0d1a5":"train_len = len(train)\ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","bfb0cad9":"# Fill empty and NaNs values with NaN\ndataset = dataset.fillna(np.nan)\n\n# Check for Null values\ndataset.isnull().sum()","5109e162":"train.info()","6868ac7e":"train.isnull().sum()","984981cf":"train.head()","0f3699d3":"# Lets see the data types of Train data\ntrain.dtypes","128ebc2e":"# Lets take a look into the Summarie and statistics\ntrain.describe()","02b76ada":"# Correlation matrix between numerical values (SibSp Parch Age and Fare values) and Survived \nsns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, \n                fmt = \".2f\", cmap = \"coolwarm\");","778e4782":"# Explore SibSp feature vs Survived\nm = sns.catplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\", height = 7)\nm.despine(left=True)\nm = m.set_ylabels(\"survival probability\")","f9b456ab":"m = sns.catplot(x='Parch', y='Survived', data=train, kind='bar', height=7)\nm.despine(left=True)\nm = m.set_ylabels(\"survival probability\")","68153a8b":"# Explore Age vs Survived\nm = sns.FacetGrid(train, col='Survived')\nm = m.map(sns.distplot, \"Age\")","da0e1917":"# Explore Age distibution \ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 0) & (train[\"Age\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train[\"Age\"][(train[\"Survived\"] == 1) & (train[\"Age\"].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","4b3c4c12":"dataset.Fare.isnull().sum()","38acf131":"#Fill Fare missing values with the median value\ndataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())","80937879":"# Explore Fare distribution \n# Flexibly plot a univariate distribution of observations.\nm = sns.distplot(dataset[\"Fare\"], color=\"r\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\nm = m.legend(loc=\"best\")","a3a133b7":"# Apply log to Fare to reduce skewness distribution\ndataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","4f51f18f":"m = sns.distplot(dataset[\"Fare\"], color=\"r\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\nm = m.legend(loc=\"best\")","d7f0307d":"g = sns.barplot(x=\"Sex\",y=\"Survived\",data=train)\ng = g.set_ylabel(\"Survival Probability\")","395ba2ec":"# See the two groups data ratio\ntrain[[\"Sex\",\"Survived\"]].groupby('Sex').mean()","58b32086":"# Explore Pclass vs Survived\ng = sns.catplot(x=\"Pclass\",y=\"Survived\",data=train, kind=\"bar\", height = 6, palette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","15e72b66":"# Let's explore Pclass vs Survived by Sex\ng = sns.catplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train,\n                   height=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","617663de":"# we have seen some missing values in embarkation lets see\ndataset.Embarked.isnull().sum()","dbcaa4cb":"# Lets explore the embarked to fill the missing value\ndataset.Embarked.value_counts()","5282d374":"# S have the most number of Embarkation so lets fill the missing values with this\ndataset.Embarked.fillna(\"S\", inplace=True)","56da4b93":"dataset.Embarked.isnull().sum()","00832305":"# Explore Embarked vs Survived \ng = sns.catplot(x=\"Embarked\", y=\"Survived\",  data=train,\n                   height=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","3d0b4b28":"# Explore Pclass vs Embarked \ng = sns.catplot(\"Pclass\", col=\"Embarked\",  data=train,\n                   height=6, kind=\"count\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Count\")","544ffb41":"dataset.isnull().sum()","627d7928":"# Explore Age vs Sex, Parch , Pclass and SibSP\ng = sns.catplot(y=\"Age\", x=\"Sex\", data=dataset, kind=\"box\")\ng = sns.catplot(y=\"Age\", x=\"Sex\", hue=\"Pclass\", data=dataset, kind=\"box\")\ng = sns.catplot(y=\"Age\", x=\"Parch\", data=dataset, kind=\"box\")\ng = sns.catplot(y=\"Age\", x=\"SibSp\", data=dataset, kind=\"box\")","a1cf65ff":"# First lets change the sex into categorical numbers\ndataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})","22aa18db":"g = sns.heatmap(dataset[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),cmap=\"BrBG\",annot=True)","8a9c0cf6":"# Filling missing value of Age \n\n## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = dataset[\"Age\"].median()\n    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        dataset['Age'].iloc[i] = age_pred\n    else :\n        dataset['Age'].iloc[i] = age_med","1467e88b":"dataset.Age.isnull().sum()","aa9b0f89":"g = sns.catplot(x=\"Survived\", y = \"Age\",data = train, kind=\"box\")\ng = sns.catplot(x=\"Survived\", y = \"Age\",data = train, kind=\"violin\")","ccad0931":"# Create a family size descriptor from SibSp and Parch\ndataset[\"Fsize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1","27b14a1a":"g = sns.catplot(x=\"Fsize\",y=\"Survived\",data = dataset, kind='point')\ng = g.set_ylabels(\"Survival Probability\")","3bd4ac0c":"# Create new feature of family size\ndataset['Single'] = dataset['Fsize'].map(lambda s: 1 if s == 1 else 0)\ndataset['SmallF'] = dataset['Fsize'].map(lambda s: 1 if  s == 2  else 0)\ndataset['MedF'] = dataset['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndataset['LargeF'] = dataset['Fsize'].map(lambda s: 1 if s >= 5 else 0)","c613bd82":"dataset.head()","d296e416":"# Let's again analyse family size with survival rate\ng = sns.catplot(x=\"Single\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.catplot(x=\"SmallF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.catplot(x=\"MedF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")\ng = sns.catplot(x=\"LargeF\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"Survival Probability\")","9f3b2f0d":"dataset.shape","44510d24":"dataset['Cabin'].head()","0bdccab1":"dataset[\"Cabin\"].describe()","64841f10":"dataset[\"Cabin\"].isnull().sum()","13793c04":"dataset[\"Cabin\"][dataset[\"Cabin\"].notnull()].head()","1f930766":"# Replace the Cabin number by the type of cabin 'X' if not\ndataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])","2895be3f":"# Show the counts of observations in each categorical bin using bars.\ng = sns.countplot(dataset[\"Cabin\"],order=['A','B','C','D','E','F','G','T','X'])","cc505f32":"g = sns.catplot(y=\"Survived\",x=\"Cabin\",data=dataset,kind=\"bar\",order=['A','B','C','D','E','F','G','T','X'])\ng = g.set_ylabels(\"Survival Probability\")","5911e859":"# Create Dummy variables\ndataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")","1146d81e":"dataset['Ticket'].head()","72ce2f39":"## Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \n\nTicket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket\ndataset[\"Ticket\"].head()","f146ded0":"dataset.Ticket.value_counts()","bf341b58":"dataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")","b946a8ee":"dataset.head()","b74d3a2b":"# Create categorical values for Pclass\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")","8a3d2c8e":"# Drop useless variables \ndataset.drop(labels = [\"PassengerId\", \"Name\", \"Embarked\"], axis = 1, inplace = True)","7ef35de2":"dataset.head()","2d57fa32":"dataset.shape","6a513508":"## Separate train dataset and test dataset\n\ntrain = dataset[:train_len]\ntest = dataset[train_len:]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)","84b04dae":"train.head()","8892d8e2":"test.head()","0d8ee5c6":"## Separate train features and label \n\ntrain[\"Survived\"] = train[\"Survived\"].astype(int)\n\nY_train = train[\"Survived\"]\n\nX_train = train.drop(labels = [\"Survived\"],axis = 1)","6cae87dd":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)","b9f2031c":"# Modeling step Test differents algorithms \nrandom_state = 7\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\n\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","a46dcb9a":"### META MODELING  WITH ADABOOST, RF, EXTRATREES and GRADIENTBOOSTING\n\n# Adaboost\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(X_train,Y_train)\n\nada_best = gsadaDTC.best_estimator_","520420e8":"gsadaDTC.best_score_","16ad0a69":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X_train,Y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","a7320bbc":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,Y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","21dfb698":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,Y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_","042df468":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train,Y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","ee380230":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,Y_train,cv=kfold)","5704be19":"nrows = ncols = 2\nfig, axes = plt.subplots(nrows = nrows, ncols = ncols, sharex=\"all\", figsize=(15,15))\n\nnames_classifiers = [(\"AdaBoosting\", ada_best),(\"ExtraTrees\",ExtC_best),(\"RandomForest\",RFC_best),(\"GradientBoosting\",GBC_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x = classifier.feature_importances_[indices][:40] , orient='h',ax=axes[row][col])\n        g.set_xlabel(\"Relative importance\",fontsize=12)\n        g.set_ylabel(\"Features\",fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name + \" feature importance\")\n        nclassifier += 1","1bf7440b":"test_Survived_RFC = pd.Series(RFC_best.predict(test), name=\"RFC\")\ntest_Survived_ExtC = pd.Series(ExtC_best.predict(test), name=\"ExtC\")\ntest_Survived_SVMC = pd.Series(SVMC_best.predict(test), name=\"SVC\")\ntest_Survived_AdaC = pd.Series(ada_best.predict(test), name=\"Ada\")\ntest_Survived_GBC = pd.Series(GBC_best.predict(test), name=\"GBC\")\n\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_RFC,test_Survived_ExtC,test_Survived_AdaC,test_Survived_GBC, test_Survived_SVMC],axis=1)\n\n\ng= sns.heatmap(ensemble_results.corr(),annot=True)","a019e63d":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(X_train, Y_train)","291a20b9":"test_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n\nresults = pd.concat([IDtest,test_Survived],axis=1)\n\nresults.to_csv(\"ensemble_python_voting.csv\",index=False)","f807c46f":"I plot the feature importance for the 4 tree based classifiers (Adaboost, ExtraTrees, RandomForest and GradientBoosting).\n\nWe note that the four classifiers have different top features according to the relative importance. It means that their predictions are not based on the same features. Nevertheless, they share some common important features for the classification , for example 'Fare', 'Pc-3', 'Age' and 'Sex'.\n\nWe can say that:\n\n- Pc_1, Pc_2, Pc_3 and Fare refer to the general social standing of passengers.\n\n- Sex  refer to the gender.\n\n- Age  refer to the age of passengers.\n\n- Fsize, LargeF, MedF, Single refer to the size of the passenger family.\n\n**According to the feature importance of this 4 classifiers, the prediction of the survival seems to be more associated with the Age, the Sex, the family size of the passengers more than the location in the boat.***","b4371eb2":"The correlation map confirms the factorplots observations except for Parch. Age is not correlated with Sex, but is negatively correlated with Pclass, Parch and SibSp.\n\nIn the plot of Age in function of Parch, Age is growing with the number of parents \/ children. But the general correlation is negative.\n\nSo, i decided to use SibSP, Parch and Pclass in order to impute the missing ages.\n\nThe strategy is to fill Age with the median age of similar rows according to Pclass, Parch and SibSp.","4d6daa06":"We can see that people with large family size have less survival rate.\nAdditionally let's create 4 categories according to family size.","2ae708c7":"## 4. Filling missing values","9d752e60":"#### Parch","d89a09a5":"We can see that Age have 256 missing values. So let's explore it a bit to find the best way to fill the missing values","fc9bf005":"10 outliers are detected 3 of them have very high tickets price and 7 of them have very high SibSp values.","e63ab1dc":"#### 6.3.1 Predict and Submit results","13655c06":"The person having 0-2 siblings\/spouse have good chance of survival and person have 3-5 have very less chance pf survival","710a4c26":"Females have a high rate of Survival","5e541487":"No difference between median value of age in survived and not survived subpopulation.\n\nBut in the violin plot of survived passengers, we still notice that very young passengers have higher survival rate.","16da7d19":"It looks like that passengers aged between 20-35 survived the most and passenger aged between 55-70 have very less survival rate.","d5edcf48":"As we can see that females of class 1 and 2 tickets have higher rate of survival than females of 3rd class ticket.\nWe can also see that ticket class is an important factor for the survival rate for male and female both.","06736918":"### 5.1 Family Size\nWe can imagine that large families will have more difficulties to evacuate, looking for theirs sisters\/brothers\/parents during the evacuation. So, i choosed to create a \"Fize\" (family size) feature which is the sum of SibSp , Parch and 1 (including the passenger).","647b23be":"## 3. Feature analysis","a076fb15":"#### 6.1.1 Cross Validation Models\nI compared 10 popular classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure.\n\n- SVC\n- Decision Tree\n- AdaBoost\n- Random Forest\n- Extra Trees\n- Gradient Boosting\n- Multiple layer perceprton (neural network)\n- KNN\n- Logistic regression\n- Linear Discriminant Analysis","983cfc62":"#### 6.1.3 Plot learning curves\n> Learning curves are a good way to see the overfitting effect on the training set and the effect of the training size on the accuracy.","5559ef2a":"#### Sex","6b1a582a":"Here we can see that having 1-3 parents\/children have higher probability of survival.\nThis is also a good feature.","5b3a2180":"### 5.2 Cabin","7c414442":"1st class is higher in Cherbourg (C) and the 3rd class is the most frequent for passenger coming from Southampton (S) and Queenstown (Q).\nAt this point, we can say that the first class has an higher survival rate. My hypothesis is that first class passengers were prioritised during the evacuation due to their influence.","ede3bffb":"I decided to choose the SVC, AdaBoost, RandomForest , ExtraTrees and the GradientBoosting classifiers for the ensemble modeling.","05b6218f":"Factorplots of family size categories show that Small and Medium families have more chance to survive than single passenger and large families.","fd55ab7a":"In the heatmap we can see that fare feature have a very good coorelation with survival.","992b8abe":"#### 6.2.1 Combining models\nI choosed a voting classifier to combine the predictions coming from the 5 classifiers.\n\nI preferred to pass the argument \"soft\" to the voting parameter to take into account the probability of each vote.","fec04b36":"#### Fare","ca4efa93":"### 2.3 Join Train and Test Set\nJoin train and test datasets in order to obtain the same number of features during categorical conversion.","424075cc":"The first letter of the cabin indicates the Desk, i choosed to keep this information only, since it indicates the probable location of the passenger in the Titanic.","06fef737":"As we can see, Fare distribution is very skewed. This can lead to overweigth very high values in the model, even if it is scaled.\n\nIn this case, it is better to transform it with the log function to reduce this skew.","d7b71a48":"## 1. Introduction\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.\n\n### Evaluation\nThe historical data has been split into two groups, a 'training set' and a 'test set'. For the training set, we provide the outcome ( 'ground truth' ) for each passenger. You will use this set to build your model to generate predictions for the test set.\n\nFor each passenger in the test set, you must predict whether or not they survived the sinking ( 0 for deceased, 1 for survived ). Your score is the percentage of passengers you correctly predict.\n\nThe Kaggle leaderboard has a public and private component. 50% of your predictions for the test set have been randomly assigned to the public leaderboard ( the same 50% for all users ). Your score on this public portion is what will appear on the leaderboard. At the end of the contest, we will reveal your score on the private 50% of the data, which will determine the final winner. This method prevents users from 'overfitting' to the leaderboard.","5ed2573a":"### 3.1 Numerical values","4ea16c6b":"Age and Cabin features have most missing values.\n\nSurvived missing values correspond to the join testing dataset (Survived column doesn't exist in test set and has been replace by NaN values when concatenating the train and test set)","dd0dd77e":"### 6.1 Simple Modeling","6a8ec142":"#### Pclass","7fc089e4":"### 5.3 Tickets","a6392ff0":"Because of the low number of passenger that have a cabin, survival probabilities have an important standard deviation and we can't distinguish between survival probability of passengers in the different desks.\n\nBut we can see that passengers with a cabin have generally more chance to survive than passengers without (X).\n\nIt is particularly true for cabin B, C, D, E and F.","4e935390":"It seems that passenger coming from Cherbourg (C) have more chance to survive.\n\nMy hypothesis is that the proportion of first class passengers is higher for those who came from Cherbourg than Queenstown (Q), Southampton (S).\n\nLet's see the Pclass distribution vs Embarked","e55f9413":"Age distribution seems to be the same in Male and Female subpopulations, so Sex is not informative to predict Age.\n\nHowever, 1st class passengers are older than 2nd class passengers who are also older than 3rd class passengers.\n\nMoreover, the more a passenger has parents\/children the older he is and the more a passenger has siblings\/spouses the younger he is.","eae694b2":"#### 6.1.2 Hyperparameter tunning for best models\n> I performed a grid search optimization for AdaBoost, ExtraTrees , RandomForest, GradientBoosting and SVC classifiers.\n   I set the \"n_jobs\" parameter to 4 since i have 4 cpu . The computation time is clearly reduced.\n  ","c3206195":"The train set only have 170 Age an 680 Cabing missing values","404ab2e5":"### 2.4 Null and missing values","ec123728":"### 2.2 Outliers Detection","146933e5":"### 6.3 Prediction","d2311546":"The prediction seems to be quite similar for the 5 classifiers except when Adaboost is compared to the others classifiers.\n\nThe 5 classifiers give more or less the same prediction but there is some differences. Theses differences between the 5 classifier predictions are sufficient to consider an ensembling vote.","87ce0dad":"skewness is reduced","b4e7989e":"#### Embarked(Port of Embarkation)","2446323d":"When we superimpose the two densities , we cleary see a peak correponsing (between 0 and 5) to babies and very young childrens with survival rate and a peak corresponsing (between 25-30) to young adults with no survival rate.","9a5ff5b6":"It shows clearly that Female have more chance to survive than Male.\nSo Sex, will play an important role in the prediction of the survival.","95d1a3dc":"### 6.2 Ensemble modeling","a28c31e2":"### 3.2 Categorical values","00b985a9":"one missing value let's fill it with the median.","e6cd2701":"The person having a 1st class ticket have high probability of survival as compared to people with 3rd class tickets.","e294aa1e":"## 6. Modeling","0364b382":"> It could mean that tickets sharing the same prefixes could be booked for cabins placed together. It could therefore lead to the actual placement of the cabins within the ship.\n\n> Tickets with same prefixes may have a similar class and survival.\n\n> So i decided to replace the Ticket feature column by the ticket prefixe. Which may be more informative.","6ff031f2":"## 5. Feature Engineering","a17d677f":"#### Sibsp","21888e78":"## 2. Load and check data","a80860fe":"# Predicting Titanic Survivors\n1 Introduction\n\n2 Load and check data\n- 2.1 load data\n- 2.2 Outlier detection\n- 2.3 Join train and test set\n- 2.4 Null and missing values\n\n3 Feature analysis\n- 3.1 Numerical values\n- 3.2 Categorical values\n\n4 Filling missing Values\n- 4.1 Age\n\n5 Feature engineering\n- 5.1 Family Size\n- 5.2 Cabin\n- 5.3 Ticket\n\n6 Modeling\n- 6.1 Simple modeling\n - 6.1.1 Cross validate models\n - 6.1.2 Hyperparamater tunning for best models\n - 6.1.3 Plot learning curves\n - 6.1.4 Feature importance of the tree based classifiers\n- 6.2 Ensemble modeling\n - 6.2.1 Combining models\n- 6.3 Prediction\n - 6.3.1 Predict and Submit results","d8834798":"### 2.1 Load Data","e8ccafe8":"#### Age","fa59dcf2":"Since outliers can have a dramatic effect on the prediction (espacially for regression problems), i choosed to manage them.\n\n The Tukey method (Tukey JW., 1977) is used to detect ouliers which defines an interquartile range comprised between the 1st and 3rd quartile of the distribution values (IQR). An outlier is a row that have a feature value outside the (IQR +- an outlier step).\n\nDetected the outliers from the numerical values features (Age, SibSp, Sarch and Fare). Then, considered outliers as rows that have at least two outlied numerical values.","8525b8ca":"### 4.1 Age","170d7140":"#### 6.1.4 Feature importance of tree based classifiers\nIn order to see the most informative features for the prediction of passengers survival, i displayed the feature importance for the 4 tree based classifiers.","d8f767b0":"GradientBoosting and Adaboost classifiers tend to overfit the training set. According to the growing cross-validation curves GradientBoosting and Adaboost could perform better with more training examples.\n\nSVC and ExtraTrees classifiers seem to better generalize the prediction since the training and cross-validation curves are close together."}}