{"cell_type":{"49daa64d":"code","a3a53b50":"code","8649bf3f":"code","01c02f54":"code","4444616c":"code","d72da15f":"code","4aee875b":"code","3935d884":"code","e32dc888":"code","890323b8":"code","dfb992bb":"code","2976999e":"code","9ffac20e":"code","8bf45bbf":"code","20fc61fe":"code","33ac6925":"code","7bd98b01":"code","d13a50e3":"code","d43a6745":"markdown"},"source":{"49daa64d":"import pandas as pd\nimport numpy as np\nimport lightgbm as lgbm\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.cluster import KMeans\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\n","a3a53b50":"train = pd.read_csv(\"..\/input\/turkiye-is-bankas-machine-learning-challenge-3\/train.csv\")\ntest = pd.read_csv(\"..\/input\/turkiye-is-bankas-machine-learning-challenge-3\/test.csv\")\nmonth_data = pd.read_csv(\"..\/input\/turkiye-is-bankas-machine-learning-challenge-3\/monthly_expenditures.csv\")\ntrain.shape,test.shape,month_data.shape","8649bf3f":"train.target.mean()","01c02f54":"#Filling missing values\ncat_cols = [\"egitim\",\"is_durumu\",\"meslek_grubu\"]\ntrain[cat_cols] = train[cat_cols].fillna(\"missing\")\ntest[cat_cols] = test[cat_cols].fillna(\"missing\")","4444616c":"month_data.head()","d72da15f":"# Aggregating monthly transactions data for further merging.\nagg_month = month_data.groupby([\"musteri\",\"sektor\"])[[\"islem_adedi\",\"aylik_toplam_tutar\"]].agg([\"sum\"])\nagg_month = agg_month.unstack()\nagg_month.columns = [col[2]+\"_\"+col[0]+\"_\"+col[1] for col in agg_month.columns]\nagg_month.head()\n\n# Fill nas\nfor col in agg_month.columns:\n    agg_month.loc[agg_month[col].isnull(),col] = 0\n\nprint(agg_month.shape)\nagg_month = agg_month.reset_index()\nagg_month.head()","4aee875b":"test[\"target\"] = 2\nall_data = pd.concat([train,test],axis=0)\nall_data.shape","3935d884":"# Label encoding categorical features\nfrom sklearn import preprocessing\nfor col in cat_cols:\n    print(col)\n    le = preprocessing.LabelEncoder()\n    all_data[col] = le.fit_transform(all_data[col].values)\n","e32dc888":"# Merging aggregated dataset with the original features.\nall_data = all_data.merge(agg_month,\"left\",\"musteri\")\nall_data.head()","890323b8":"# Splitting back to train and test\ntest = all_data.iloc[len(train):]\ntrain = all_data.iloc[:len(train)]\ntrain.shape,test.shape","dfb992bb":"# Selecting which features to use during modeling.\nfeatures = [col for col in train.columns if col not in [\"musteri\",\"target\",\"tarih\"]]\nlen(features)","2976999e":"y = train[\"target\"]\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\noof = np.zeros(len(train))\nscore_list = []\nfold = 1\ntest_preds = []\n\n\nfor train_index, test_index in kf.split(train):\n    X_train, X_val = train.iloc[train_index], train.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    print(X_train.shape,X_val.shape)\n\n    \n    y_pred_list = []\n    for seed in [None]: # Add more values to this list if you want to have a multiple seed average.\n        dtrain = lgbm.Dataset(X_train[features], y_train)\n        dvalid = lgbm.Dataset(X_val[features], y_val)\n        print(seed)\n        params = {\"objective\": \"binary\",\n              \"metric\": \"auc\",\n              \"verbosity\": -1,\n              \"boosting_type\": \"gbdt\",\n              \"feature_fraction\":0.5,\n              \"num_leaves\": 15,\n                  \"max_depth\":6,\n              \"lambda_l1\":0,\n              \"lambda_l2\":4,\n              \"learning_rate\":0.01,\n              'min_child_samples': 75,\n              \"bagging_fraction\":0.75,\n              \"bagging_freq\":1,\n              #\"max_bin\":75\n             }\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                        dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=700,\n                        categorical_feature = [\"egitim\",\"is_durumu\"]\n                    )\n\n    \n        y_pred_list.append(model.predict(X_val[features]))\n        print(roc_auc_score(y_val,   np.mean(y_pred_list,axis=0)))\n        test_preds.append(model.predict(test[features]))\n        \n    \n    oof[test_index] = np.mean(y_pred_list,axis=0)    \n    score = roc_auc_score(y_val,oof[test_index])\n\n    print(f\"AUC Fold-{fold} : {score}\")\n    score_list.append(score)\n    fold+=1\n\nnp.mean(score_list)","9ffac20e":"# Full oof score.\nroc_auc_score(y,oof)","8bf45bbf":"# Fold scores.\nscore_list","20fc61fe":"# Feature importance\nlgbm.plot_importance(model,figsize=(10,10))","33ac6925":"test[\"target\"] = np.mean(test_preds,axis=0)\ntest[[\"musteri\",\"target\"]].head()","7bd98b01":"# Preds' distribution\ntest.target.hist(bins=100)","d13a50e3":"test[[\"musteri\",\"target\"]].to_csv(\"submission.csv\",index=False)","d43a6745":"# Quick Change:\n# I've added \"categorical_feature = [\"egitim\",\"is_durumu\"]\" to lgbm.train, Both CV and LB improved."}}