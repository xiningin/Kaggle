{"cell_type":{"a456b7b2":"code","f4f3bf89":"code","98fb0afb":"code","5d33dad0":"code","74849f3e":"code","79bd3db6":"code","aab5ce08":"code","e6c8302f":"code","164d6558":"code","444de285":"code","7466d802":"code","598d7bb9":"code","7d728b2d":"code","366a8571":"code","ff5e3c63":"code","dbf89b99":"code","165c1b07":"code","87aa5c05":"code","d2f0f02f":"code","021902b6":"code","57380b6a":"code","a96cad22":"code","37b59a74":"code","a3865d06":"code","19ef1e9c":"code","7e5e0972":"code","540b288a":"code","ddfb9876":"code","fdf487a3":"code","f4312d52":"code","bf8c0c42":"code","ad77fa68":"code","e6710f8f":"code","cfffdb14":"code","38da196c":"code","808095e9":"code","8965e55e":"code","08bb3fa5":"markdown","228be06c":"markdown","92e65b3a":"markdown","ce324bf2":"markdown","640e21a7":"markdown","45d49f29":"markdown","c2b22c30":"markdown","6527bd92":"markdown","94d31b17":"markdown","90428182":"markdown","656fff68":"markdown","091c0610":"markdown","a95603c1":"markdown","763ad808":"markdown"},"source":{"a456b7b2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Configs\npd.options.display.float_format = '{:,.4f}'.format\nsns.set(style=\"whitegrid\")\nplt.style.use('seaborn')\nseed = 42\nnp.random.seed(seed)","f4f3bf89":"file_path = '\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv'\ndf = pd.read_csv(file_path, encoding='latin-1')\ndf = df[['v1', 'v2']]\ndf.columns = ['label', 'message']\n\nprint(\"DataSet = {} rows and {} columns\".format(df.shape[0], df.shape[1]))\ndf.head()","98fb0afb":"def eda_categ_feat_desc_plot(series_categorical, title = \"\"):\n    series_name = series_categorical.name\n    val_counts = series_categorical.value_counts()\n    val_counts.name = 'quantity'\n    val_percentage = series_categorical.value_counts(normalize=True)\n    val_percentage.name = \"percentage\"\n    val_concat = pd.concat([val_counts, val_percentage], axis = 1)\n    val_concat.reset_index(level=0, inplace=True)\n    val_concat = val_concat.rename( columns = {'index': series_name} )\n    \n    fig, ax = plt.subplots(figsize = (12,4), ncols=2, nrows=1) # figsize = (width, height)\n    if(title != \"\"):\n        fig.suptitle(title, fontsize=18)\n        fig.subplots_adjust(top=0.8)\n\n    s = sns.barplot(x=series_name, y='quantity', data=val_concat, ax=ax[0])\n    for index, row in val_concat.iterrows():\n        s.text(row.name, row['quantity'], row['quantity'], color='black', ha=\"center\")\n\n    s2 = val_concat.plot.pie(y='percentage', autopct=lambda value: '{:.2f}%'.format(value),\n                             labels=val_concat[series_name].tolist(), legend=None, ax=ax[1],\n                             title=\"Percentage Plot\")\n\n    ax[1].set_ylabel('')\n    ax[0].set_title('Quantity Plot')\n\n    plt.show()","5d33dad0":"def plot_nn_loss_acc(history):\n    fig, (axis1, axis2) = plt.subplots(nrows=1, ncols=2, figsize=(16,6))\n    # summarize history for accuracy\n    axis1.plot(history.history['accuracy'], label='Train', linewidth=3)\n    axis1.plot(history.history['val_accuracy'], label='Validation', linewidth=3)\n    axis1.set_title('Model accuracy', fontsize=16)\n    axis1.set_ylabel('accuracy')\n    axis1.set_xlabel('epoch')\n    axis1.legend(loc='upper left')\n    # summarize history for loss\n    axis2.plot(history.history['loss'], label='Train', linewidth=3)\n    axis2.plot(history.history['val_loss'], label='Validation', linewidth=3)\n    axis2.set_title('Model loss', fontsize=16)\n    axis2.set_ylabel('loss')\n    axis2.set_xlabel('epoch')\n    axis2.legend(loc='upper right')\n    plt.show()","74849f3e":"def plot_words_distribution(mydf, target_column, title='Words distribution', x_axis='Words in column'):\n    # adaptade of https:\/\/www.kaggle.com\/alexcherniuk\/imdb-review-word2vec-bilstm-99-acc\n    # def statistics\n    len_name = target_column +'_len'\n    mydf[len_name] = np.array(list(map(len, mydf[target_column])))\n    sw = mydf[len_name]\n    median = sw.median()\n    mean   = sw.mean()\n    mode   = sw.mode()[0]\n    # figure\n    fig, ax = plt.subplots()\n    sns.distplot(mydf[len_name], bins=mydf[len_name].max(),\n                hist_kws={\"alpha\": 0.9, \"color\": \"blue\"}, ax=ax,\n                kde_kws={\"color\": \"black\", 'linewidth': 3})\n    ax.set_xlim(left=0, right=np.percentile(mydf[len_name], 95)) # Dont get outiliers\n    ax.set_xlabel(x_axis)\n    ymax = 0.025\n    plt.ylim(0, ymax)\n    # plot vertical lines for statistics\n    ax.plot([mode, mode], [0, ymax], '--', label=f'mode = {mode:.2f}', linewidth=4)\n    ax.plot([mean, mean], [0, ymax], '--', label=f'mean = {mean:.2f}', linewidth=4)\n    ax.plot([median, median], [0, ymax], '--', label=f'median = {median:.2f}', linewidth=4)\n    ax.set_title(title, fontsize=20)\n    plt.legend()\n    plt.show()","79bd3db6":"import time\n\ndef time_spent(time0):\n    t = time.time() - time0\n    t_int = int(t) \/\/ 60\n    t_min = t % 60\n    if(t_int != 0):\n        return '{}min {:.3f}s'.format(t_int, t_min)\n    else:\n        return '{:.3f}s'.format(t_min)","aab5ce08":"from sklearn.metrics import confusion_matrix, classification_report\n\nthis_labels = ['HAM','SPAM']\n\ndef class_report(y_real, y_my_preds, name=\"\", labels=this_labels):\n    if(name != ''):\n        print(name,\"\\n\")\n    print(confusion_matrix(y_real, y_my_preds), '\\n')\n    print(classification_report(y_real, y_my_preds, target_names=labels))","e6c8302f":"df['message_lenght'] = np.array(list(map(len, df['message'])))\n\nmin_len = df['message_lenght'].min()\nmax_len = df['message_lenght'].max()\n\nprint('min len:', min_len)\nprint(df[df.message_lenght == min_len].message.iloc[0], '\\n')\n\nprint('max len:', max_len)\nprint(df[df.message_lenght == max_len].message.iloc[0])\n\ndf = df.drop(['message_lenght'], axis=1)","164d6558":"eda_categ_feat_desc_plot(df.label, 'Unbalanced DataSet of SPAM')","444de285":"plot_words_distribution(df, 'message', title='Words distribution all data')","7466d802":"plot_words_distribution( df.query('label == \"ham\"'), 'message', title='Words distribution to ham')","598d7bb9":"plot_words_distribution( df.query('label == \"spam\"'), 'message', title='Words distribution to spam')","7d728b2d":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\",\n\"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\",\n\"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\",\n\"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\",\n\"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\",\n\"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\",\n\"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\",\n\"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n\"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n\"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n\"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\",\n\"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\",\n\"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\",\n\"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\",\n\"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\",\n\"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\",\n\"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n\"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n\nmispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora',\n'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are',\n'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I',\n'theBest': 'the best', 'howdoes': 'how does', 'Etherium': 'Ethereum', 'narcissit': 'narcissist',\n'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend',\n'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n'demonitization': 'demonetization', 'demonetisation': 'demonetization', 'pok\u00e9mon': 'pokemon'}","366a8571":"def clean_contractions(text, mapping):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text\n\ndef correct_spelling(x, dic):\n    for word in dic.keys():\n        x = x.replace(word, dic[word])\n    return x","ff5e3c63":"from nltk.corpus import stopwords\nimport re\n\n# Stop Words in Python. Is better search in a 'set' structure\nstops = set(stopwords.words(\"english\"))  \n\ndef clean_text( text ):\n    # 1. Remove non-letters        \n    text = re.sub(\"[^a-zA-Z]\", \" \", text) \n    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n    # 2. Convert to lower case, split into individual words\n    text = text.lower().split()                                             \n    # 3. Remove stop words\n    meaningful_words = [w for w in text if not w in stops]   \n    # 4. Join the words back into one string separated by space.\n    return( \" \".join( meaningful_words )) ","dbf89b99":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\ndef lematizer(text):\n    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n    text = \" \".join(text)\n    return text","165c1b07":"clean_column = 'clean_message'\n\ndf[clean_column] = df['message'].apply(lambda x: clean_contractions(x, contraction_mapping))\n\ndf[clean_column] = df[clean_column].apply(lambda x: correct_spelling(x, mispell_dict))\n\ndf[clean_column] = df[clean_column].apply(clean_text)\n\ndf[clean_column] = df[clean_column].apply(lematizer)\n","87aa5c05":"from sklearn.model_selection import train_test_split\n\nX = df[clean_column].values\ny = df['label'].replace({'ham': 0, 'spam': 1})\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","d2f0f02f":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense , Input , LSTM , Embedding, Dropout , Activation, GRU, Flatten\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\nfrom keras.layers import Convolution1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers","021902b6":"# Tokenizer\nmaxlen = 130\nmax_features = 6000\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(np.concatenate((x_train, x_test), axis=0))","57380b6a":"# Convert x_train\nlist_tokenized_train = tokenizer.texts_to_sequences(x_train) # convert string to numbers, \nX_t = pad_sequences(list_tokenized_train, maxlen=maxlen) # create a array of 130 spaces and put all words in end\n\n## Convert x_test\nX_tt = tokenizer.texts_to_sequences(x_test)\nX_tt = pad_sequences(X_tt, maxlen=maxlen)","a96cad22":"model = Sequential()\nmodel.add(Embedding(max_features, 128))\nmodel.add(Bidirectional(LSTM(32, return_sequences = True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(20, activation=\"relu\"))\nmodel.add(Dropout(0.05))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nbatch_size = 100\nepochs = 5\nhistory = model.fit(X_t, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, validation_data=(X_tt, y_test))","37b59a74":"y_pred = model.predict_classes(X_tt)\n\nclass_report(y_test, y_pred, \"Keras Neural Net LSTM\")","a3865d06":"plot_nn_loss_acc(history)","19ef1e9c":"from sklearn.feature_extraction.text import CountVectorizer\n\n# instantiate the vectorizer\nvect = CountVectorizer()\nvect.fit(np.concatenate((x_train, x_test), axis=0))\n\nX_train_dtm = vect.transform(x_train)\nX_test_dtm = vect.transform(x_test)\nx_full_dtm = vect.transform( np.concatenate((x_train, x_test), axis=0) )\n\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit( x_full_dtm )\nX_train_dtm_tfft = tfidf_transformer.transform(X_train_dtm)\nX_test_dtm_tfft  = tfidf_transformer.transform(X_test_dtm)","7e5e0972":"from sklearn.model_selection import GridSearchCV\n\ndef optimize_random_forest(mx_train, my_train, my_hyper_params, hyper_to_search, hyper_search_name, cv=4, scoring='accuracy'):\n    \"\"\"search best param to unic one hyper param\n    @mx_train, @my_train = x_train, y_train of dataset\n    @my_hyper_params: dict with actuals best_params: start like: {}\n      => will be accumulated and modified with each optimization iteration\n      => example stater: best_hyper_params = {'random_state': 42, 'n_jobs': -1}\n    @hyper_to_search: dict with key @hyper_search_name and list of values to gridSearch:\n    @hyper_search_name: name of hyperparam\n    \"\"\"\n    if(hyper_search_name in my_hyper_params.keys()):\n        del my_hyper_params[hyper_search_name]\n    if(hyper_search_name not in hyper_to_search.keys()):\n        raise Exception('\"hyper_to_search\" dont have {} in dict'.format(hyper_search_name))\n        \n    t0 = time.time()\n        \n    rf = RandomForestClassifier(**my_hyper_params)\n    \n    grid_search = GridSearchCV(estimator = rf, param_grid = hyper_to_search, \n      scoring = scoring, n_jobs = -1, cv = cv)\n    grid_search.fit(mx_train, my_train)\n    \n    print('took', time_spent(t0))\n    \n    data_frame_results = pd.DataFrame(\n        data={'mean_fit_time': grid_search.cv_results_['mean_fit_time'],\n        'mean_test_score_'+scoring: grid_search.cv_results_['mean_test_score'],\n        'ranking': grid_search.cv_results_['rank_test_score']\n         },\n        index=grid_search.cv_results_['params']).sort_values(by='ranking')\n    \n    print('The Best HyperParam to \"{}\" is {} with {} in {}'.format(\n        hyper_search_name, grid_search.best_params_[hyper_search_name], grid_search.best_score_, scoring))\n    \n    my_hyper_params[hyper_search_name] = grid_search.best_params_[hyper_search_name]\n    \n    \"\"\"\n    @@my_hyper_params: my_hyper_params appends best param find to @hyper_search_name\n    @@data_frame_results: dataframe with statistics of gridsearch: time, score and ranking\n    @@grid_search: grid serach object if it's necessary\n    \"\"\"\n    return my_hyper_params, data_frame_results, grid_search","540b288a":"best_hyper_params = {'random_state': 42, 'n_jobs': -1} # Stater Hyper Params","ddfb9876":"search_hyper = {'min_samples_split': [2, 5, 10]}\n\nbest_hyper_params, results, last_grid_search = optimize_random_forest(\n    X_train_dtm, y_train, best_hyper_params, search_hyper, 'min_samples_split')","fdf487a3":"search_hyper = {'n_estimators': [50, 100, 200, 400, 800, 1600, 2000] }\n\nbest_hyper_params, results, last_grid_search = optimize_random_forest(\n    X_train_dtm, y_train, best_hyper_params, search_hyper, 'n_estimators')","f4312d52":"search_hyper = { 'max_depth': [100, 110, 115, 200, 400, None] }\n\nbest_hyper_params, results, last_grid_search = optimize_random_forest(\n    X_train_dtm, y_train, best_hyper_params, search_hyper, 'max_depth')\n\nresults","bf8c0c42":"search_hyper = { 'max_features': ['auto', 'log2', None] }\n\nbest_hyper_params, results, last_grid_search = optimize_random_forest(\n    X_train_dtm, y_train, best_hyper_params, search_hyper, 'max_features')\n\nresults","ad77fa68":"search_hyper = { 'criterion' :['gini', 'entropy'] }\n\nbest_hyper_params, results, last_grid_search = optimize_random_forest(\n    X_train_dtm, y_train, best_hyper_params, search_hyper, 'criterion')","e6710f8f":"search_hyper = { 'min_samples_split' :[ 8, 9, 10, 11, 12] }\n\nbest_hyper_params, results, last_grid_search = optimize_random_forest(\n    X_train_dtm, y_train, best_hyper_params, search_hyper, 'min_samples_split')","cfffdb14":"search_hyper = { 'max_leaf_nodes' :[ 2, 100, 200, 300, 400, None] }\n\nbest_hyper_params, results, last_grid_search = optimize_random_forest(\n    X_train_dtm, y_train, best_hyper_params, search_hyper, 'max_leaf_nodes')","38da196c":"best_hyper_params","808095e9":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier() \nforest = forest.fit(X_train_dtm, y_train)\ny_pred = forest.predict(X_test_dtm)\n\nclass_report(y_test, y_pred, 'Random Forest Normal')","8965e55e":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(**best_hyper_params) \nforest = forest.fit(X_train_dtm, y_train)\ny_pred = forest.predict(X_test_dtm)\n\nclass_report(y_test, y_pred, 'Random Forest HyperTuned')","08bb3fa5":"### Def functions <a id='index05'><\/a>","228be06c":"## Split in train and test <a id='index07'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n","92e65b3a":"## Evaluate Model <a id='index09'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","ce324bf2":"https:\/\/blog.barracuda.com\/2013\/10\/03\/ham-v-spam-whats-the-difference\/\n\n","640e21a7":"## Snippets <a id='index02'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","45d49f29":"## Table Of Contents (TOC) <a id=\"top\"><\/a>\n\n+ [Import Libs and DataSet](#index01) \n+ [Snippets](#index02)\n+ [EDA](#index03)\n+ [Text Cleaning](#index04)\n  - [Def functions](#index05)\n  - [Execute pre-processing](#index06)\n+ [Split in train and Test](#index07)\n+ [Develop Model](#index08)\n+ [Evaluate Model](#index09)\n+ [Conclusion](#index10)\n\n\n## Import Libs and DataSet <a id='index01'><\/a>","c2b22c30":"### Execute Text Pre-Procesing <a id='index06'><\/a>","6527bd92":"## Problem Description\n\nClassify SPAM or HAM\n\nMeaning of HAM\n\n> The term \u2018ham\u2019 was originally coined by SpamBayes sometime around 2001and is currently defined and understood to be \u201cE-mail that is generally desired and isn't considered spam.\u201d - [link](https:\/\/blog.barracuda.com\/2013\/10\/03\/ham-v-spam-whats-the-difference\/)","94d31b17":"## Text Cleaning <a id='index04'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","90428182":"## Conclusion <a id='index10'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n**References**\n\nhttps:\/\/www.kaggle.com\/nilanml\/imdb-review-deep-model-94-89-accuracy\n\nThis is an initial kernel, in the future it will be filled with the theoretical part and more details about NLP, how a kernel to start understand NLP.","656fff68":"## Develop Model <a id='index08'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","091c0610":"## Hyper Tuning a RandomForest","a95603c1":"## EDA <a id='index03'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","763ad808":"<h1 align=\"center\"> SPAM Detector: Text Classify <\/h1>\n\n<img src=\"https:\/\/raw.githubusercontent.com\/deepankarkotnala\/Email-Spam-Ham-Classifier-NLP\/master\/images\/email_spam_ham.png\" width=\"50%\" \/>\n\nCreated: 2020-09-15\n\nLast updated: 2020-09-15\n\nKaggle Kernel made by \ud83d\ude80 <a href=\"https:\/\/www.kaggle.com\/rafanthx13\"> Rafael Morais de Assis<\/a>\n\nThis is an initial kernel, in the future it will be filled with the theoretical part and more details about NLP"}}