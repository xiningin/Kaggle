{"cell_type":{"c844de45":"code","12d0ae6a":"code","e7d58f84":"code","9691310b":"code","b83ec3ef":"code","4bafe4ef":"code","fd30135c":"code","6723212b":"code","a3d814b3":"code","e8c4d87c":"markdown","083e85bd":"markdown","a0472288":"markdown","5954bf02":"markdown","d6f413cf":"markdown"},"source":{"c844de45":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","12d0ae6a":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error","e7d58f84":"train_raw = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/train.csv\", index_col=0)\ntest_raw = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\", index_col=0)\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")","9691310b":"train_X = train_raw.drop(\"loss\", axis=1)\ntrain_y = train_raw[\"loss\"] #.astype(int)\nX_test = test_raw.copy()","b83ec3ef":"splits = 10\nmodel_preds = 0\noof_preds = np.zeros(train_X.shape[0])\nskf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=0)\nevals_results = {}\n\nfor i, (train_idx, valid_idx) in enumerate(skf.split(train_X, train_y)):\n    X_train, y_train = train_X.iloc[train_idx], train_y.iloc[train_idx]\n    X_valid, y_valid = train_X.iloc[valid_idx], train_y.iloc[valid_idx]\n    \n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n    \n    params = {\n        \"task\": \"train\",\n        \"boosting_type\": \"gbdt\",\n        \"objective\": \"regression\",\n        \"metric\": {\"rmse\"},\n        \"eta\": 0.05,\n        \"max_depth\": 40,\n        \"num_leaves\": 10,\n        \"min_child_samples\": 20,\n        \"feature_fraction\": 0.5,\n        \"bagging_fraction\": 0.5,\n        \"bagging_freq\": 5,\n        \"verbose\": -1\n    }\n    \n    model = lgb.train(\n        params,\n        lgb_train,\n        num_boost_round=500,\n        valid_names=[\"train\", \"valid\"],\n        valid_sets=[lgb_train, lgb_valid],\n        evals_result=evals_results,\n        early_stopping_rounds=50,\n        verbose_eval=0\n    )\n    \n    model_preds += (model.predict(X_test)) \/ splits\n    oof_preds[valid_idx] = model.predict(X_valid)\n    print(\"Fold {} RMSE : {}\".format(i, np.sqrt(mean_squared_error(y_valid, oof_preds[valid_idx]))))\nprint(\"Total RMSE : {}\".format(np.sqrt(mean_squared_error(train_y, oof_preds))))","4bafe4ef":"plt.figure(figsize=(8, 5))\nplt.title(\"RMSE\")\nplt.plot(evals_results[\"train\"][\"rmse\"], label=\"train\")\nplt.plot(evals_results[\"valid\"][\"rmse\"], label=\"valid\")\nplt.xlabel(\"num round\")\nplt.ylabel(\"loss\")\nplt.legend()","fd30135c":"importance = model.feature_importance(importance_type=\"gain\")\nfeature = train_X.columns\nimportance_df = pd.DataFrame({\"feature\":feature, \"importance\":importance})\nimportance_df = importance_df.sort_values(\"importance\")\nimportance_df.set_index(\"feature\").iloc[-10:].plot(kind=\"barh\", figsize=(10, 8),title=\"Top 10 feature importances\", fontsize=10, colormap=\"summer\")","6723212b":"fig, ax = plt.subplots(figsize=(16, 4), ncols=2, nrows=1)\nfig.suptitle(\"Loss prediction distribution\", fontsize=15)\nfig.subplots_adjust(top=0.8)\nax[0].hist(model_preds, bins=20, color=\"g\", edgecolor=\"k\")\nax[0].set_title(\"Test prediction\")\nax[0].set_xlabel(\"Loss\")\nax[1].hist(oof_preds, bins=20, color=\"y\", edgecolor=\"k\")\nax[1].set_title(\"Valid prediction\")\nax[1].set_xlabel(\"Loss\")","a3d814b3":"submission[\"loss\"] = model_preds\nsubmission.to_csv(\"submission.csv\", index=False)","e8c4d87c":"<font size=\"6\">Simple LightGBM & KFold without feature engineering<\/font>","083e85bd":"<font size=\"5\">Feature importance<\/font>","a0472288":"<font size=\"5\">Submission<\/font>","5954bf02":"<font size=\"5\">Make model and prediction<\/font>","d6f413cf":"<font size=\"5\">Distribution of predicitions<\/font>"}}