{"cell_type":{"9e3e193f":"code","ea7ffa9e":"code","b1b2adf2":"code","7e546773":"code","b6030df3":"code","623b8196":"code","25f14efa":"code","e5a3755a":"code","d9e312e1":"code","71d74446":"code","454a1d41":"code","6d9a0de6":"code","fe341747":"code","59b8b6df":"code","d3fa12eb":"code","c70c18b5":"code","f8539059":"code","b5a7e65e":"code","b8b57a80":"code","f2a6bfce":"code","91da37b9":"code","c033e645":"code","5a3cd567":"code","5b09d1d2":"code","878fc18c":"code","7017d5db":"code","322e039b":"code","8c22ecdd":"code","efa35684":"code","7ddbedbb":"code","fd99d006":"code","59503bf9":"code","9d7c7a1a":"code","0a02a73c":"code","5af32287":"code","5b83acb8":"code","fc6529d7":"code","4cb1ac5c":"code","05f564d8":"code","6539c70a":"code","7f6b6612":"code","037e096e":"code","826508dd":"code","d821db43":"code","c09d1cd9":"code","6daa58a2":"code","236a7fae":"code","702d8803":"code","6d23c2a9":"code","6140863d":"code","d9b14b2a":"code","03c13654":"code","13200f9c":"code","6611dd3b":"code","2d49758f":"code","4c0f9e67":"code","de572f9b":"code","d6689af6":"code","913b2edb":"markdown","b3219f66":"markdown","45b923bf":"markdown","f4a5c6d8":"markdown","03bf0b9e":"markdown","e1bd042a":"markdown","72cf7f79":"markdown","0983b4e4":"markdown","fba80148":"markdown","b6a8142b":"markdown","e68f569d":"markdown","49d386da":"markdown","68800e3a":"markdown","5ca93820":"markdown","31f25605":"markdown","4c254c4d":"markdown","bd1ac821":"markdown","49671eba":"markdown","6923ab57":"markdown","9f85d628":"markdown","dcf737a2":"markdown","0ce94e9c":"markdown","adecd515":"markdown","6f076e30":"markdown","12c7cef0":"markdown","95c60207":"markdown"},"source":{"9e3e193f":"from IPython.display import Image\nimport os\n#!ls ..\/input\/\nImage(\"..\/input\/boostingimage\/Boosting_Main.PNG\")","ea7ffa9e":"# Loading Libraries\nimport pandas as pd # for data analysis\nimport numpy as np # for scientific calculation\nimport seaborn as sns # for statistical plotting\nimport datetime # for working with date fields\nimport matplotlib.pyplot as plt # for plotting\n%matplotlib inline\nimport math # for mathematical calculation","b1b2adf2":"#Reading Loan Payment given Data Set.\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/loan-payment\/Loan_payments_data.csv'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7e546773":"# Reading Loan Payment .csv file.\nloanpayment = pd.read_csv(\"..\/input\/loan-payment\/Loan_payments_data.csv\")                             # Reading data using simple Pandas","b6030df3":"# Describe method is used to view some basic statistical details like percentile, mean, std etc. of a data frame of numeric values.\nloanpayment.describe()","623b8196":"#Checking shape of data\nloanpayment.shape","25f14efa":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n# Verifying top 3 sample records of data.\nloanpayment.head(3)\n# Checking Null Values : We can see there are No Null Values \nloanpayment.isnull().sum()\n# Checking the data information.\n# Observation: No missing values\nloanpayment.info()","e5a3755a":"loanpayment['loan_status'].value_counts()","d9e312e1":"loanpayment['education'].value_counts()","71d74446":"loanpayment['Gender'].value_counts()","454a1d41":"loanpayment['terms'].value_counts()","6d9a0de6":"loanpayment.groupby(by=['Gender','education','loan_status'])['loan_status'].count()","fe341747":"print(np.min(loanpayment.age))\nprint(np.max(loanpayment.age))","59b8b6df":"loanpayment['age_bins'] = pd.cut(x=loanpayment['age'], bins=[18, 20, 30, 40, 50, 60])","d3fa12eb":"plt.rcParams['figure.figsize'] = (20.0, 10.0)\nplt.rcParams['font.family'] = \"serif\"\nfig, ax =plt.subplots(3,2)\nsns.countplot(loanpayment['Gender'], ax=ax[0,0])\nsns.countplot(loanpayment['education'], ax=ax[0,1])\nsns.countplot(loanpayment['loan_status'], ax=ax[1,0])\nsns.countplot(loanpayment['Principal'], ax=ax[1,1])\nsns.countplot(loanpayment['terms'], ax=ax[2,0])\nsns.countplot(loanpayment['age_bins'], ax=ax[2,1])\nfig.show();","c70c18b5":"import plotly.express as px\n\nfig = px.histogram(loanpayment, x=\"terms\", y=\"Principal\", color = 'Gender',\n                   marginal=\"violin\", # or violin, rug,\n                   color_discrete_sequence=['indianred','lightblue'],\n                   )\n\nfig.update_layout(\n    title=\"Gender wise segregation of loan terms and principal\",\n    xaxis_title=\"Loan Term\",\n    yaxis_title=\"Principal Count\/Gender Segregation\",\n)\nfig.update_yaxes(tickangle=-30, tickfont=dict(size=7.5))\n\nfig.show()","f8539059":"fig = px.scatter_3d(loanpayment,z=\"age\",x=\"Principal\",y=\"terms\",\n    color = 'Gender', size_max = 18,\n    color_discrete_sequence=['indianred','lightblue']\n    )\n\nfig.show()","b5a7e65e":"loanpayment.head(1)","b8b57a80":"for dataset in [loanpayment]:\n    dataset.loc[dataset['age'] <= 20,'age']=0\n    dataset.loc[(dataset['age']>20) & (dataset['age']<=25),'age']=1\n    dataset.loc[(dataset['age']>25) & (dataset['age']<=30),'age']=2\n    dataset.loc[(dataset['age']>30) & (dataset['age']<=35),'age']=3\n    dataset.loc[(dataset['age']>35) & (dataset['age']<=40),'age']=4\n    dataset.loc[(dataset['age']>40) & (dataset['age']<=45),'age']=5\n    dataset.loc[(dataset['age']>45) & (dataset['age']<=50),'age']=6\n    dataset.loc[(dataset['age']>50) & (dataset['age']<=55),'age']=7","f2a6bfce":"loanpayment.head(1)","91da37b9":"# Import label encoder \nfrom sklearn import preprocessing \n  \n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \nloanpayment_fe=loanpayment.copy()\n# Encode labels in column 'education'. \nloanpayment_fe['education_label']= label_encoder.fit_transform(loanpayment_fe['education']) \n#loanpayment_fe['education_label'].unique() \n# Encode labels in column 'loan_status'. \nloanpayment_fe['loan_status_label']= label_encoder.fit_transform(loanpayment_fe['loan_status']) \n#loanpayment_fe['loan_status_label'].unique() \n# Encode labels in column 'Gender'. \nloanpayment_fe['gender_label']= label_encoder.fit_transform(loanpayment_fe['Gender']) \n#loanpayment_fe['gender_label'].unique() ","c033e645":"loanpayment_fe.head(1)","5a3cd567":"loanpayment_fe=loanpayment_fe.drop(['Loan_ID','loan_status','education','Gender','age_bins'],axis=1)\nloanpayment_fe.head(1)","5b09d1d2":"loanpayment_fe['effective_date']= pd.to_datetime(loanpayment_fe['effective_date']) \nloanpayment_fe['due_date']= pd.to_datetime(loanpayment_fe['due_date']) \nloanpayment_fe['paid_off_time']= pd.to_datetime(loanpayment_fe['paid_off_time']) \nloanpayment_fe.info()","878fc18c":"loanpayment_fe['actual_tenure_days'] = loanpayment_fe['due_date'] - loanpayment_fe['effective_date']\nloanpayment_fe['actual_tenure_days']=loanpayment_fe['actual_tenure_days']\/np.timedelta64(1,'D')","7017d5db":"print(np.min(loanpayment_fe['actual_tenure_days']))\nprint(np.max(loanpayment_fe['actual_tenure_days']))","322e039b":"loanpayment_fe['paidoff_tenure_days'] = loanpayment_fe['paid_off_time'] - loanpayment_fe['effective_date']\nloanpayment_fe['paidoff_tenure_days']=loanpayment_fe['paidoff_tenure_days']\/np.timedelta64(1,'D')","8c22ecdd":"print(np.min(loanpayment_fe['paidoff_tenure_days']))\nprint(np.max(loanpayment_fe['paidoff_tenure_days']))","efa35684":"loanpayment_fe.isnull().sum()","7ddbedbb":"loanpayment_fe.describe().T","fd99d006":"loanpayment_fe=loanpayment_fe.drop(['past_due_days'],axis=1)","59503bf9":"null_data = loanpayment_fe[loanpayment_fe.isnull().any(axis=1)]\nnull_data.head(2)","9d7c7a1a":"notnull_data = loanpayment_fe[loanpayment_fe.notnull().any(axis=1)]\nnotnull_data.head(2)","0a02a73c":"loanpayment_fe['paidoff_tenure_days'] = loanpayment_fe.groupby(['Principal','terms','age','education_label','gender_label'])['paidoff_tenure_days'].transform(lambda x:x.fillna(x.mean()))\nloanpayment_fe['paidoff_tenure_days'] = loanpayment_fe['paidoff_tenure_days'].fillna(0)","5af32287":"\nloanpayment_fe.isnull().sum()","5b83acb8":"loanpayment_fe=loanpayment_fe.drop(['effective_date','due_date','paid_off_time'],axis=1)\nloanpayment_fe.head(1)","fc6529d7":"loanpayment_fe['paidoff_tenure_days']=loanpayment_fe['paidoff_tenure_days'].round().astype(int)","4cb1ac5c":"loanpayment_fe['paidoff_tenure_days'].value_counts().head(2)","05f564d8":"loanpayment_fe.info()","6539c70a":"loanpayment_fe.head(2)","7f6b6612":"loanpayment_fe['loan_status_label'].value_counts().plot.bar();","037e096e":"X2=loanpayment_fe.drop(['loan_status_label'],axis=1)\nX1=preprocessing.scale(X2)\nX=pd.DataFrame(X1)\ny=loanpayment_fe['loan_status_label']","826508dd":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,shuffle=True,test_size=0.25,stratify=y)","d821db43":"print(len(X_train[X_train==0]), len(X_train[X_train==1]), len(X_train[X_train==2]))\nprint(len(X_test[X_test==0]), len(X_test[X_test==1]), len(X_test[X_test==2]))\nprint(len(y_train[y_train==0]), len(y_train[y_train==1]), len(y_train[y_train==2]))\nprint(len(y_test[y_test==0]), len(y_test[y_test==1]), len(y_test[y_test==2]))","c09d1cd9":"#!pip install imblearn","6daa58a2":"from imblearn.over_sampling import SMOTE\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\noversampler = SMOTE(random_state=0)\nX_train ,y_train = oversampler.fit_sample(X_train, y_train)","236a7fae":"print(len(X_train[X_train==0]), len(X_train[X_train==1]), len(X_train[X_train==2]))\nprint(len(X_test[X_test==0]), len(X_test[X_test==1]), len(X_test[X_test==2]))\nprint(len(y_train[y_train==0]), len(y_train[y_train==1]), len(y_train[y_train==2]))\nprint(len(y_test[y_test==0]), len(y_test[y_test==1]), len(y_test[y_test==2]))","702d8803":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","6d23c2a9":"from IPython.display import Image\nimport os\n#!ls ..\/input\/\nImage(\"..\/input\/boostingimage\/Boosting_DataFlow.PNG\")","6140863d":"# Import Models\nimport xgboost\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\n\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import VotingClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nlearning_rate = 1\nseed=123","d9b14b2a":"\nmodel = AdaBoostClassifier(random_state=seed)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\ntest_scores = accuracy_score(y_test, predictions)  \nprint(\"Training Accuracy score (training): {0:.3f}\".format(model.score(X_train, y_train)));\nprint(\"Test Accuracy Score: %0.2f (+\/- %0.2f)\" % (test_scores.mean(),test_scores.std()));\n\nprint(\"Test Confusion Matrix:\")\nprint(confusion_matrix(y_test, predictions));\n\nprint(\"Test Classification Report\")\nprint(classification_report(y_test, predictions));","03c13654":"kfold = model_selection.KFold(n_splits=5, random_state=seed,shuffle=True)\n\nbdt_real = AdaBoostClassifier(DecisionTreeClassifier(),n_estimators=600,learning_rate=1)\n\nbdt_discrete = AdaBoostClassifier(DecisionTreeClassifier(),n_estimators=600,learning_rate=1.5,algorithm=\"SAMME\")\n\n# For real samples.\nbdt_real.fit(X_train, y_train)\nresults = model_selection.cross_val_score(bdt_real, X_train, y_train, cv=kfold,n_jobs=1)\npredicted = model_selection.cross_val_predict(bdt_real,X_test,y_test,cv=kfold) \ntest_scores = accuracy_score(y_test, predicted)    \nprint(\"Real Samples Train Accuracy: %0.2f (+\/- %0.2f)\" % (results.mean(), results.std()));\nprint(\"Real Samples Test Accuracy: %0.2f (+\/- %0.2f)\" % (test_scores.mean(),test_scores.std()));\nprint(\"Real Samples Test classification_report \\n\", classification_report(y_test, predicted));\n\n# For Discrete samples\nbdt_discrete.fit(X_train, y_train)\nresults = model_selection.cross_val_score(bdt_discrete, X_train, y_train, cv=kfold,n_jobs=1)\npredicted = model_selection.cross_val_predict(bdt_discrete,X_test,y_test,cv=kfold) \ntest_scores = accuracy_score(y_test, predicted)    \nprint(\"Discrete Samples Train Accuracy: %0.2f (+\/- %0.2f)\" % (results.mean(), results.std()));\nprint(\"Discrete Accuracy: %0.2f (+\/- %0.2f)\" % (test_scores.mean(),test_scores.std()));\nprint(\"Discrete classification_report \\n\", classification_report(y_test, predicted));\n\n\nreal_test_errors = []\ndiscrete_test_errors = []\n\nfor real_test_predict, discrete_train_predict in zip(bdt_real.staged_predict(X_test), bdt_discrete.staged_predict(X_test)):\n    real_test_errors.append(1. - accuracy_score(real_test_predict, y_test))\n    discrete_test_errors.append(1. - accuracy_score(discrete_train_predict, y_test))\n\nn_trees_discrete = len(bdt_discrete)\nn_trees_real = len(bdt_real)\n\n# Boosting might terminate early, but the following arrays are always\n# n_estimators long. We crop them to the actual number of trees here:\ndiscrete_estimator_errors = bdt_discrete.estimator_errors_[:n_trees_discrete]\nreal_estimator_errors = bdt_real.estimator_errors_[:n_trees_real]\ndiscrete_estimator_weights = bdt_discrete.estimator_weights_[:n_trees_discrete]\n\nplt.figure(figsize=(15, 5))\n\nplt.subplot(131)\nplt.plot(range(1, n_trees_discrete + 1), discrete_test_errors, c='black', label='SAMME')\nplt.plot(range(1, n_trees_real + 1),real_test_errors, c='black',linestyle='dashed', label='SAMME.R')\nplt.legend()\nplt.ylim(0.18, 0.62)\nplt.ylabel('Test Error')\nplt.xlabel('Number of Trees')\n\nplt.subplot(132);\nplt.plot(range(1, n_trees_discrete + 1), discrete_estimator_errors,\"b\", label='SAMME', alpha=.5)\nplt.plot(range(1, n_trees_real + 1), real_estimator_errors,\"r\", label='SAMME.R', alpha=.5)\nplt.legend()\nplt.ylabel('Error')\nplt.xlabel('Number of Trees')\nplt.ylim((.2,max(real_estimator_errors.max(),discrete_estimator_errors.max()) * 1.2))\nplt.xlim((-20, len(bdt_discrete) + 20))\n\nplt.subplot(133);\nplt.plot(range(1, n_trees_discrete + 1), discrete_estimator_weights,\"b\", label='SAMME')\nplt.legend()\nplt.ylabel('Weight')\nplt.xlabel('Number of Trees')\nplt.ylim((0, discrete_estimator_weights.max() * 1.2))\nplt.xlim((-20, n_trees_discrete + 20))\n\n# prevent overlapping y-axis labels\nplt.subplots_adjust(wspace=0.25)\nplt.show();","13200f9c":"kfold = model_selection.KFold(n_splits=5, random_state=seed,shuffle=True)\n\nlr_list = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\n\nfor learning_rate in lr_list:\n    gb_clf = GradientBoostingClassifier(n_estimators=600, learning_rate=learning_rate,random_state=seed) #max_features=2, max_depth=2\n    gb_clf.fit(X_train, y_train)\n    print(\"Learning rate: \", learning_rate);\n    print(\"Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)));\n    print(\"Accuracy score (testing): {0:.3f}\".format(gb_clf.score(X_test, y_test)));","6611dd3b":"gb_clf2 = GradientBoostingClassifier(n_estimators=600, learning_rate=0.1, random_state=seed)\ngb_clf2.fit(X_train, y_train)\npredictions = gb_clf2.predict(X_test)\ntest_scores = accuracy_score(y_test, predictions)  \nprint(\"Learning rate: \", learning_rate);\nprint(\"Training Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)));\nprint(\"Test Accuracy Score: %0.2f (+\/- %0.2f)\" % (test_scores.mean(),test_scores.std()));\n\nprint(\"Test Confusion Matrix:\");\nprint(confusion_matrix(y_test, predictions));\n\nprint(\"Test Classification Report\");\nprint(classification_report(y_test, predictions));","2d49758f":"\nmodel=xgb.XGBClassifier(n_estimators=600, learning_rate=learning_rate,random_state=seed,gamma=0.2,colsample_bytree=0.75)\nmodel.fit(X_train, y_train)\npredictions = model.predict(X_test)\ntest_scores = accuracy_score(y_test, predictions)  \nprint(\"Learning rate: \", learning_rate);\nprint(\"Training Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)));\nprint(\"Test Accuracy Score: %0.2f (+\/- %0.2f)\" % (test_scores.mean(),test_scores.std()));\n\nprint(\"Test Confusion Matrix:\");\nprint(confusion_matrix(y_test, predictions));\n\nprint(\"Test Classification Report\");\nprint(classification_report(y_test, predictions));      \n      ","4c0f9e67":"model=CatBoostClassifier(iterations=50,random_seed=seed,learning_rate=learning_rate,custom_loss=['AUC', 'Accuracy']);\nmodel.fit(X_train,y_train,eval_set=(X_test, y_test));\npredictions = model.predict(X_test);\ntest_scores = accuracy_score(y_test, predictions);\nprint(\"Learning rate: \", learning_rate);\nprint(\"Training Accuracy score (training): {0:.3f}\".format(gb_clf.score(X_train, y_train)));\nprint(\"Test Accuracy Score: %0.2f (+\/- %0.2f)\" % (test_scores.mean(),test_scores.std()));\n\nprint(\"Test Confusion Matrix:\");\nprint(confusion_matrix(y_test, predictions));\n\nprint(\"Test Classification Report\");\nprint(classification_report(y_test, predictions));","de572f9b":"#Converting the dataset in proper LGB format\nd_train=lgb.Dataset(X_train, label=y_train)\n\nfrom sklearn.metrics import precision_score\n# Parameters setting\nparams={}\nparams['learning_rate']=1\nparams['boosting_type']='gbdt' \nparams['objective']='multiclass' \nparams['metric']='multi_logloss' \nparams['n_estimators']=600\nparams['num_class']=3 \n\n#training the model on 100 epocs\nclf=lgb.train(params,d_train,100)  \npredictions=clf.predict(X_test)\n\n#argmax() method for preditions\npredictions = [np.argmax(line) for line in predictions]\n\n#using precision score for error metrics\nprint(predictions);\nprint(precision_score(predictions,y_test,average=None).mean());\n","d6689af6":"ada_boost = AdaBoostClassifier()\ngrad_boost = GradientBoostingClassifier()\nxgb_boost = XGBClassifier()\n\nada_boost_real = AdaBoostClassifier(DecisionTreeClassifier(),n_estimators=600,learning_rate=learning_rate,random_state=seed)\nada_boost_discrete = AdaBoostClassifier(DecisionTreeClassifier(),n_estimators=600,learning_rate=learning_rate,random_state=seed,algorithm=\"SAMME\")\ngrad_boost = GradientBoostingClassifier(n_estimators=600, learning_rate=learning_rate,random_state=seed)\nxgb_boost=xgb.XGBClassifier(n_estimators=600, learning_rate=0.05,random_state=seed,gamma=0.2,colsample_bytree=0.75)\n\n\neclf = EnsembleVoteClassifier(clfs=[ada_boost_real,ada_boost_discrete, grad_boost, xgb_boost], voting='hard')\nlabels = ['Ada Boost Real','Ada Boost Discrete', 'Grad Boost', 'XG Boost','Ensemble']\n\nfor clf, label in zip([ada_boost_real,ada_boost_discrete, grad_boost, xgb_boost,eclf], labels):\n    scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy');\n    print(\"Hard Voting Mean: {0:.3f}, std: (+\/-) {1:.3f} [{2}]\".format(scores.mean(), scores.std(), label));\n    \n","913b2edb":"XGBoost: XGBoost (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as \u2018regularized boosting\u2018 technique.\n\nLet us see how XGBoost is comparatively better than other techniques:\n\n    1. Regularization:\n        Standard GBM implementation has no regularisation like XGBoost.\n        Thus XGBoost also helps to reduce overfitting.\n    2. Parallel Processing:\n        XGBoost implements parallel processing and is faster than GBM .\n        XGBoost also supports implementation on Hadoop.\n    3. High Flexibility:\n        XGBoost allows users to define custom optimization objectives and evaluation criteria adding a whole new dimension to the model.\n    4. Handling Missing Values:\n        XGBoost has an in-built routine to handle missing values.\n    5. Tree Pruning:\n        XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits beyond which there is no positive gain.\n    6. Built-in Cross-Validation:\n        XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.\n\n","b3219f66":"AdaBoost: Adaptive boosting or AdaBoost is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.\n\nBelow are the steps for performing the AdaBoost algorithm:\n\n    1. Initially, all observations in the dataset are given equal weights.\n    2. A model is built on a subset of data.\n    3. Using this model, predictions are made on the whole dataset.\n    4. Errors are calculated by comparing the predictions and actual values.\n    5. While creating the next model, higher weights are given to the data points which were predicted incorrectly.\n    6. Weights can be determined using the error value. For instance, higher the error more is the weight assigned to the observation.\n    7. This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached.","45b923bf":"* [Table of Contents](#Tableofcontents)\n<a id=\"DataVisualizations\"><\/a>\n\n# Data Visualizations","f4a5c6d8":"Boosting is a general ensemble method that creates a strong classifier from a number of weak classifiers.\n\nThis is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.\n\n1. A subset is created from the original dataset.\n2. Initially, all data points are given equal weights.\n3. A base model is created on this subset.\n4. This model is used to make predictions on the whole dataset.\n5. Errors are calculated using the actual values and predicted values.\n6. The observations which are incorrectly predicted, are given higher weights.\n (Here, the three misclassified blue-plus points will be given higher weights)\n7. Another model is created and predictions are made on the dataset.\n (This model tries to correct the errors from the previous model)\n8. Similarly, multiple models are created, each correcting the errors of the previous model.\n9. The final model (strong learner) is the weighted mean of all the models (weak learners).\n10. Thus, the boosting algorithm combines a number of weak learners to form a strong learner.\n\nThe individual models would not perform well on the entire dataset, but they work well for some part of the dataset. Thus, each model actually boosts the performance of the ensemble.","03bf0b9e":"* [Table of Contents](#Tableofcontents)\n<a id=\"Voting\"><\/a>\n\n# Voting Classifier","e1bd042a":"* [Table of Contents](#Tableofcontents)\n<a id=\"SMOTE\"><\/a>\n# SMOTE to Balance Data","72cf7f79":"#### Taken DataFlow from this site.\n#### link: https:\/\/mksaad.wordpress.com\/2019\/12\/21\/bagging-and-boosting\/","0983b4e4":"* [Table of Contents](#Tableofcontents)\n<a id=\"References\"><\/a>\n# References:\n1. https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-for-ensemble-models\/\n2. https:\/\/machinelearningmastery.com\/ensemble-machine-learning-algorithms-python-scikit-learn\/\n3. https:\/\/medium.com\/@rrfd\/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de\n4. https:\/\/machinelearningmastery.com\/develop-first-xgboost-model-python-scikit-learn\/\n5. https:\/\/programtalk.com\/python-examples\/mlxtend.classifier.EnsembleVoteClassifier\/\n6. http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/EnsembleVoteClassifier\/\n7. http:\/\/rasbt.github.io\/mlxtend\/api_modules\/mlxtend.classifier\/EnsembleVoteClassifier\/\n8. https:\/\/machinelearningmastery.com\/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost\/\n9. https:\/\/stackabuse.com\/gradient-boosting-classifiers-in-python-with-scikit-learn\/\n10. https:\/\/machinelearningmastery.com\/gradient-boosting-machine-ensemble-in-python\/\n11. https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/\n12. https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_adaboost_multiclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-multiclass-py\n13. https:\/\/towardsdatascience.com\/a-beginners-guide-to-xgboost-87f5d4c30ed7\n14. https:\/\/medium.com\/@nitin9809\/lightgbm-binary-classification-multi-class-classification-regression-using-python-4f22032b36a2\n15. https:\/\/towardsdatascience.com\/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205\n16. https:\/\/mksaad.wordpress.com\/2019\/12\/21\/bagging-and-boosting\/ \n\n\n","fba80148":"CatBoost: CatBoost can automatically deal with categorical variables and does not require extensive data preprocessing like other machine learning algorithms.\nHandling categorical variables is a tedious process, especially when you have a large number of such variables. When your categorical variables have too many labels (i.e. they are highly cardinal), performing one-hot-encoding on them exponentially increases the dimensionality and it becomes really difficult to work with the dataset.\n\n","b6a8142b":"Voting:EnsembleVoteClassifier is a meta-classifier for combining similar or conceptually different machine learning classifiers for classification via majority or plurality voting.The EnsembleVoteClassifier implements \"hard\" and \"soft\" voting. In hard voting, we predict the final class label as the class label that has been predicted most frequently by the classification models. In soft voting, we predict the class labels by averaging the class-probabilities.","e68f569d":"* [Table of Contents](#Tableofcontents)\n<a id=\"EnsembleIntroduction\"><\/a>\n# Ensemble Introduction\n\n## What is Ensemble Learning\n**Wikipedia Defination:** Ensemble learning is a type of machine learning that studies algorithms and architectures that build collections, or ensembles, of statistical classifiers that are more accurate than a single classifier.\n                                                            (or)\nIn statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.\n\nLet\u2019s understand the concept of ensemble learning with an example. Suppose you are a movie director and you have created a short movie on a very important and interesting topic. Now, you want to take preliminary feedback (ratings) on the movie before making it public. What are the possible ways by which you can do that?\n\nA: You may ask one of your friends to rate the movie for you.\n\nNow it\u2019s entirely possible that the person you have chosen loves you very much and doesn\u2019t want to break your heart by providing a 1-star rating to the horrible work you have created.\n\nB: Another way could be by asking 5 colleagues of yours to rate the movie.\n\nThis should provide a better idea of the movie. This method may provide honest ratings for your movie. But a problem still exists. These 5 people may not be \u201cSubject Matter Experts\u201d on the topic of your movie. Sure, they might understand the cinematography, the shots, or the audio, but at the same time may not be the best judges of dark humour.\n\nC: How about asking 50 people to rate the movie?\n\nSome of which can be your friends, some of them can be your colleagues and some may even be total strangers.\n\nThe responses, in this case, would be more generalized and diversified since now you have people with different sets of skills. And as it turns out \u2013 this is a better approach to get honest ratings than the previous cases we saw.\n\nWith these examples, you can infer that a diverse group of people are likely to make better decisions as compared to individuals. Similar is true for a diverse set of models in comparison to single models. This diversification in Machine Learning is achieved by a technique called Ensemble Learning.\n\nThere are three main terms describing the ensemble (combination) of various models into one more effective model:\n1. Bagging used to decrease the model\u2019s variance.\n2. Boosting used to decrease the model\u2019s bias. \n3. Stacking usedto increase the predictive force of the classifier.","49d386da":"* [Table of Contents](#Tableofcontents)\n<a id=\"Conclusion\"><\/a>\n\n# Conclusion:\n\nTo understand how ensemble techniques works, I've taken Loan Payment dataset for this multiclass classification problem and applied Stacking, Blending, Bagging and Boosting ensemble techniques to verify how train and test accuracy works. Concluded that XGBoost model works best for this problem statement. \n\nNote: I've done this experiment by dividing it into 3 parts and covered all models. Please visit all 3 articles to understand better. Happy Learning!!!\n\n1.    Part 1: Stacking & Blending: https:\/\/www.kaggle.com\/rameshbabugonegandla\/ensemble-tec-stacking-blending-loanpayment-project\n2.    Part 2: Bagging: https:\/\/www.kaggle.com\/rameshbabugonegandla\/ensemble-tec-bagging-loanpayment-project\n3.    Part 3: Boosting: Current notebook\/kernel\n","68800e3a":"* [Table of Contents](#Tableofcontents)\n<a id=\"LightGBM\"><\/a>\n\n# Light GBM Classifier","5ca93820":"* [Table of Contents](#Tableofcontents)\n<a id=\"XGBoost\"><\/a>\n\n# XGBoost Classifier","31f25605":"<a id=\"Tableofcontents\"><\/a>\n# Table of Contents\n\n* [Introduction](#Introduction)\n* [EDA](#EDA)\n* [Data Visualizations](#DataVisualizations)\n* [Feature Engineering](#FeatureEngineering)\n* [Train and Test Split](#TrainandTestSplit)\n* [SMOTE to Balance Data](#SMOTE)\n* [Ensemble Introduction](#EnsembleIntroduction)\n* [Boosting Introduction](#Boosting)\n* [AdaBoost Classifier](#AdaBoost)\n* [Gradient Boost Classifier](#GradientBoost)\n* [XGBoost Classifier](#XGBoost)\n* [CatBoost Classifier](#CatBoost)\n* [LightGBM Classifier](#LightGBM)\n* [Voting Classifier](#VotingClassifier)\n* [References](#References)\n* [Conclusion](#Conclusion)","4c254c4d":"LightGBM: LightGBM is a gradient boosting framework that uses tree-based algorithms and follows leaf-wise approach while other algorithms work in a level-wise approach pattern. The images below will help you understand the difference in a better way.\n\nBefore discussing how Light GBM works, let\u2019s first understand why we need this algorithm when we have so many others (like the ones we have seen above). Light GBM beats all the other algorithms when the dataset is extremely large. Compared to the other algorithms, Light GBM takes lesser time to run on a huge dataset. Leaf-wise growth may cause over-fitting on smaller datasets but that can be avoided by using the \u2018max_depth\u2019 parameter for learning. \n\n","bd1ac821":"* [Table of Contents](#Tableofcontents)\n<a id=\"TrainandTestSplit\"><\/a>\n# Train and Test Split","49671eba":"* [Table of Contents](#Tableofcontents)\n<a id=\"Introduction\"><\/a>\n# Introduction\n### Domain : Banking\n\n### Dataset: Loan Payment data\n\n### Attributes:\n\n1. Loan_id : A unique loan number assigned to each loan customers\n2. Loan_status: Whether a loan is paid off, in collection, new customer yet to payoff, or paid off after the collection efforts\n3. Principal: Basic principal loan amount at the origination terms, could be weekly (7 days), biweekly, and monthly payoff schedule\n4. effective_date: When the loan got originated and took effects\n5. due_date: Since it\u2019s one-time payoff schedule, each loan has one single due date\n6. paidoff_time: The actual time a customer pays off the loan\n7. pastdue_days: How many days a loan has been past due age, education, Gender: A customer\u2019s basic demographic information","6923ab57":"* [Table of Contents](#Tableofcontents)\n<a id=\"AdaBoost\"><\/a>\n\n# AdaBoost Classifier","9f85d628":"### Main intension of developing this project is to understand the Ensemble Technique Boosting concepts. Demonstrated Boosting techniques for individual models, individual models with hyper parameter tuning, mulitple models and used Voting Classifer to understand the accuracy for the multi classification problem with Loan Payment Dataset.\n\nNote: Up to Train-Test Split used part 1 article which has published in my previous notebook.\n\n1. Part 1: Stacking & Blending: https:\/\/www.kaggle.com\/rameshbabugonegandla\/ensemble-tec-stacking-blending-loanpayment-project\n2. Part 2: Bagging: https:\/\/www.kaggle.com\/rameshbabugonegandla\/ensemble-tec-bagging-loanpayment-project\n3. Part 3: Boosting: Current notebook\/kernel\n\nHappy Learning!!!","dcf737a2":"* [Table of Contents](#Tableofcontents)\n<a id=\"CatBoost\"><\/a>\n\n# CatBoost Classifier","0ce94e9c":"* [Table of Contents](#Tableofcontents)\n<a id=\"Boosting\"><\/a>\n# Boosting Introduction","adecd515":"* [Table of Contents](#Tableofcontents)\n<a id=\"EDA\"><\/a>\n\n# EDA","6f076e30":"Gradient Boost: Gradient Boosting or GBM is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner. Regression trees used as a base learner, each subsequent tree in series is built on the errors calculated by the previous tree.","12c7cef0":"* [Table of Contents](#Tableofcontents)\n<a id=\"GradientBoost\"><\/a>\n\n# Gradient Boosting Classifier","95c60207":"* [Table of Contents](#Tableofcontents)\n<a id=\"FeatureEngineering\"><\/a>\n# Feature Engineering"}}