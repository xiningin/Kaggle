{"cell_type":{"1abcd8d6":"code","b996f13d":"code","b0a1832d":"code","5f11b61f":"code","74db20aa":"code","4e7e9eb5":"code","89bbdf9d":"code","b44d6343":"code","bdf0e48f":"code","1202807c":"code","9a79d2dc":"code","e3fb12ea":"code","1d7d9032":"markdown"},"source":{"1abcd8d6":"#Getting the dataset (the data is in JSON format)\n!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/sarcasm.json \\\n    -O \/tmp\/sarcasm.json","b996f13d":"#importing the necessary libraries\n#We need the json library to convert the dataset which is in JSON format\nimport json\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","b0a1832d":"#The dataset consists of URL(article link), headlines and the label(whether it is sarcastic or not) in JSON format\n#so we are going to load them and get the headlines and labels and add them into the lists   \n\nwith open(\"\/tmp\/sarcasm.json\", 'r') as f:\n    datastore = json.load(f)\n\nsentences = []\nlabels = []\n\nfor itm in datastore:\n  sentences.append(itm['headline'])\n  labels.append(itm['is_sarcastic'])\n\n#Lets see the first 10 headlines\nprint(sentences[:10])\nprint(labels[:10])","5f11b61f":"#Now lets divide the data into training data and validation data\ntrain_sentences = sentences[:20000]\ntest_sentences = sentences[20000:]\ntrain_labels = labels[:20000]\ntest_labels = labels[20000:]","74db20aa":"#Preparing(tokenizing, converting into sequence, padding) the training data \ntokenizer = Tokenizer(num_words=10000, oov_token='<OOV>')\ntokenizer.fit_on_texts(train_sentences)\nword_index = tokenizer.word_index\nprint(word_index)","4e7e9eb5":"train_seq = tokenizer.texts_to_sequences(train_sentences)\ntrain_padded_seq = pad_sequences(sequences=train_seq, maxlen = 100, padding='post', truncating='post')\nprint(train_padded_seq)","89bbdf9d":"#Preparing the test data\ntest_seq = tokenizer.texts_to_sequences(test_sentences)\ntest_padded_seq = pad_sequences(sequences=test_seq, maxlen = 100, padding='post', truncating='post')\nprint(test_padded_seq.shape)","b44d6343":"import numpy as np\n#Converting the data into numpy arrays\ntrain_padded_seq = np.array(train_padded_seq)\ntrain_labels = np.array(train_labels)\ntest_padded_seq = np.array(test_padded_seq)\ntest_labels = np.array(test_labels)","bdf0e48f":"#Defining the model\n#Embedding layer converts the 2D tensor into 3D temsor for feeding into the dense layer.\n#It has 3 imp parameters: 1. input_dim (the size of the vocabulary) 2. output_dim (the no of neurons in the dense layer or in the output) 3. input_length (length of the sequence)\n\nmodel = tf.keras.Sequential([\n                             tf.keras.layers.Embedding(10000, 16, input_length =100 ),\n                             tf.keras.layers.GlobalAveragePooling1D(),\n                             tf.keras.layers.Dense(24,activation='relu'),\n                             tf.keras.layers.Dense(1,activation='sigmoid')\n])","1202807c":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics = ['accuracy'])\nhistory = model.fit(train_padded_seq, train_labels, epochs = 30, validation_data=(test_padded_seq, test_labels), verbose = 2)","9a79d2dc":"import matplotlib.pyplot as plt\n\ndef plot(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric])\n  plt.xlabel('epochs')\n  plt.ylabel(metric)\n  plt.legend([metric, 'val_'+metric])\n  plt.show()\n\nplot(history, 'accuracy')\nplot(history, 'loss')","e3fb12ea":"#Testing the model using some sample data...\nsentence = ['Scientists to kill ducks to see why they are dying', 'Safety meeting ends in accident', 'Farms protest continues over low MSP']\nsequence = tokenizer.texts_to_sequences(sentence)\npadded_sequence = pad_sequences(sequences=sequence, maxlen = 100, padding='post', truncating='post')\nresult = model.predict(padded_sequence)\nprint(result)","1d7d9032":"**UPVOTE IF YOU LIKED THE KERNEL**"}}