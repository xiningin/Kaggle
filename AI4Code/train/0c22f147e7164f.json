{"cell_type":{"7722bdc5":"code","dd2000be":"code","8f1b718c":"code","5e5d3a70":"code","0843a4e3":"code","32a39635":"code","c9088ccd":"code","d2bbe868":"code","8eb16fc3":"code","1911e1f2":"code","c8449e43":"code","c25424a0":"code","5368f4f1":"code","a2132a5a":"code","716c770e":"code","a17f33e9":"code","41791b87":"code","6abf053d":"code","325b5f7c":"code","4ba55c2f":"markdown","7115aa3b":"markdown","05ea7ce6":"markdown","bccb810e":"markdown","d05cf2ad":"markdown","0fa0802f":"markdown","52a42f4f":"markdown","1d1f7897":"markdown","a8edacfa":"markdown","66467b34":"markdown"},"source":{"7722bdc5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nfrom numpy import newaxis, r_, c_, mat\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","dd2000be":"print('Loading data ...\\n');\n\n# Load Data\ndata = pd.read_csv('..\/input\/ex1data2.txt', header = None) #read from dataset\ndata.head(5)","8f1b718c":"X = data.iloc[:, 0:2]\ny = data.iloc[:, 2]\nm = len(y)","5e5d3a70":"print('size of house, no. of bedrooms,  price')\nfor i in range(10):\n     print('{:8.0f}{:10.0f}{:20.0f}'.format(X.iloc[i,0], X.iloc[i,1], y[i]))","0843a4e3":"def featureNormalize(X):\n    \"\"\"\n    Normalizes the features in x.\n\n    Parameters\n    ----------\n    X : ndarray, shape (n_samples, n_features) = Features to be normalized.\n\n    Returns\n    -------\n    X_norm : ndarray, shape (n_samples, n_features)\n        A normalized version of X where the mean value of each feature is 0 and the standard deviation is 1.\n    mu : ndarray, shape (n_features,)\n        The mean value.\n    sigma : ndarray, shape (n_features,)\n        The standard deviation.\n    \"\"\"\n    X_norm = X.copy()\n    mu = np.zeros(X.shape[1])\n    sigma = np.zeros(X.shape[1])\n    \n    \n    mu = np.mean(X, axis=0)\n    sigma = np.std(X, axis=0)\n    X_norm = (X - mu) \/ sigma\n    return X_norm, mu, sigma","32a39635":"X_norm, mu, sigma = featureNormalize(X)\nprint('Computed mean:', mu)\nprint('Computed standard deviation:', sigma)\n\n","c9088ccd":"# Add intercept term to X\nX = np.hstack((np.ones((m, 1)), X_norm))","d2bbe868":"# Choose some alpha value\nalpha = 0.15\nnum_iters = 400\n\n# Init theta and run gradient descent\ntheta = np.zeros(3)","8eb16fc3":"def h(theta,X): #Linear hypothesis function\n    return np.dot(X,theta)","1911e1f2":"def computeCostMulti(X,y,theta): #Cost function\n    \"\"\"\n    theta is an n- dimensional vector of initial theta guess\n    X is matrix with n- columns and m- rows\n    y is a matrix with m- rows and 1 column\n    \"\"\"\n    #note to self: *.shape is (rows, columns)\n    J = float((1.\/(2*m)) * np.dot((h(theta,X)-y).T,(h(theta,X)-y)))\n    return J","c8449e43":"print ('Calculate cost with Initial Theta value: ',computeCostMulti(X,y,theta))","c25424a0":"def gradientDescentMulti(X, y, theta, alpha, iterations):\n    \"\"\"\n    Parameters\n    X = ndarray, shape (n_samples, n_features)(m*n) = Training Data\n    y = ndarray, shape (n_samples) = Labels\n    theta = ndarray , shape(n_features) = n. Theta is an Initial linear regression parameter\n    alpha = float, it's a learning rate\n    \n    \"\"\"\n    m = len(y)\n    J_history = np.zeros(iterations)\n    for i in range(num_iters):\n        theta -= alpha \/ m * ((h(theta,X) - y).T.dot(X))\n        J_history[i] = computeCostMulti(X, y, theta)\n\n    return theta, J_history","5368f4f1":"# perform linear regression on the data set\ntheta, J_history = gradientDescentMulti(X, y, theta, alpha, num_iters)\n# get the cost (error) of the model\nprint ('Further testing and calculate theta after performing gradient decent:',computeCostMulti(X,y,theta))","a2132a5a":"print('theta:', theta.ravel())\n\nplt.plot(J_history)\nplt.xlabel('Iterations')\nplt.ylabel('Cost_J')\nplt.show()","716c770e":"normalize_data = (np.array([1650,3])-mu)\/sigma\nnormalize_data = np.hstack((np.ones(1), normalize_data))\nprice = normalize_data.dot(theta)\nprint('Predicted price for a 1650 sq-ft, 3 br house:', price)","a17f33e9":"data = pd.read_csv('..\/input\/ex1data2.txt', header = None) #read from dataset\nX = data.iloc[:, 0:2]\ny = data.iloc[:, 2]\nm = len(y)\n\n# Add intercept term to X\nX = np.hstack((np.ones((m, 1)), X))","41791b87":"def normalEqn(X,y):\n    pinv_X = np.linalg.pinv(np.dot(X.T,X))\n    X_y = np.dot(X.T,y)\n    theta = np.dot(pinv_X,X_y)\n    return np.linalg.inv(X.T.dot( X )).dot( X.T ).dot( y )","6abf053d":"theta3 = normalEqn(X,y)\nprint (theta3)","325b5f7c":"price = np.array([(np.ones(1)), 1650, 3]).dot(theta3)\nprint ('Predicted price for a 1650 sq-ft, 3 br house (using normal equations):', price)","4ba55c2f":"**Linear regression with multiple variables**\n\nIn this part, we will implement linear regression with multiple variables to predict the prices of houses. \nSuppose you are selling your house and you want to know what a good market price would be. One way to do this is to first collect information on recent houses sold and make a model of housing prices.\n\nThe file ex1data2.txt contains a training set of housing prices in Portland, Oregon. The first column is the size of the house (in square feet), the second column is the number of bedrooms, and the third column is the price of the house.","7115aa3b":"**3.1 Feature Normalization**\n\nBy looking at the values, note that house sizes are about 1000 times the number of bedrooms. When features differ by orders of magnitude, first performing feature scaling can make gradient descent converge much more quickly.\n\nOur task here is to: \n* Subtract the mean value of each feature from the dataset. After subtracting the mean, \n* additionally scale (divide) the feature values by their respective \u201cstandard deviations.\u201d\n\nThe standard deviation is a way of measuring how much variation there is in the range of values of a particular feature (most data points will lie within \u00b12 standard deviations of the mean); this is an alternative to taking the range of values (max-min).","05ea7ce6":"**Plot convergence Graph**","bccb810e":"In Previous Kernel we have performed Linear Regression with one variable. These exercises are from coursera machine learning course. Please keep Prof. Andrew Ng's each video and lecture note side by side for better understanding about this kernel. ","d05cf2ad":"We are now just one step back to find the predicted price for 1650 sq. ft, 3 bedroom house.\nFor that we need to take normalize data.\n\n**Prediction**","0fa0802f":"If you like my efforts, please upvote. I value your suggesstions too. \nThanks for stoping by. Stay Tuned for Logistic Regression (coursera ML exercise)","52a42f4f":"After performing gradientDecent, our cost value is reduced from 65591548106.45744 to 2043280050.602828. \n\nSo far so good, looks we will find the best fit theta values. lets varify with convergence plot.","1d1f7897":"**3.2 Gradient Descent**\n\nPreviously, we implemented gradient descent on a univariate regression problem. \nThe only difference now is that there is one more feature in the matrix X. The hypothesis function and the batch gradient descent update rule remain unchanged.\n\nfor equations, use lecture notes or video for reference.\n\nIn the multivariate case, the cost function can also be written in the forllowing vectorized form.\n\n$$ J(\\theta) = \\frac{1}{2m}(X\\theta - \\vec{y})^T(X\\theta - \\vec{y}) $$\n\nwhere\n$$ X = \\begin{pmatrix}          - (x^{(1)})^T - \\\\         - (x^{(2)})^T - \\\\           \\vdots \\\\         - (x^{(m)})^T - \\\\ \\\\      \\end{pmatrix} \\qquad \\mathbf{y} = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)} \\\\\\end{bmatrix}$$\n","a8edacfa":"**Predicted price for a 1650 sq-ft, 3 br house (using normal equations)**","66467b34":"**3.3 Normal Equations**\n\nIn the lecture videos, you learned that the closed-form solution to linear regression is $$ \\theta = \\left( X^T X\\right)^{-1} X^T\\vec{y}$$\nUsing normal equation formula does not require any feature scaling, and you will get an exact solution in one calculation: there is no \u201cloop until convergence\u201d like in gradient descent\n\n"}}