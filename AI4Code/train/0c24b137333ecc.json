{"cell_type":{"8709bede":"code","15b07a38":"code","4a3d5081":"code","b64bb717":"code","de5dd003":"code","9ad34917":"code","1e61b604":"code","4bc3c933":"code","1b528a67":"code","961fbdbd":"code","19bec0b9":"code","1ca9b002":"code","7b4d2973":"code","2089f122":"code","b29869e8":"code","3d68d85e":"code","04398254":"code","9e467b1c":"code","90d7c048":"markdown","dd98c870":"markdown","cd13eb3f":"markdown","633778e1":"markdown","639ee228":"markdown","48517030":"markdown","7624c05f":"markdown","eacab03e":"markdown","ea32a460":"markdown","df3d174f":"markdown","36e035fe":"markdown","3ae3e4ee":"markdown","208cfe31":"markdown","44ab602a":"markdown","037ee065":"markdown"},"source":{"8709bede":"import warnings\nwarnings.filterwarnings('ignore')\nfrom __future__ import absolute_import, division, print_function\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom PIL import Image\nimport os\nfrom keras.utils import to_categorical\nfrom keras import backend as K\nfrom keras import layers\nfrom keras.preprocessing.image import save_img\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.models import Sequential,Input,Model\nfrom keras.layers import Dense,Flatten,Dropout,Concatenate,GlobalAveragePooling2D,Lambda,ZeroPadding2D\nfrom keras.layers import SeparableConv2D,BatchNormalization,MaxPooling2D,Conv2D\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.optimizers import Adam,SGD\nfrom keras.utils.vis_utils import plot_model\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping,ReduceLROnPlateau\nprint(os.listdir(\"..\/input\/cell_images\/cell_images\"))","15b07a38":"infected = os.listdir('..\/input\/cell_images\/cell_images\/Parasitized\/') \nuninfected = os.listdir('..\/input\/cell_images\/cell_images\/Uninfected\/')","4a3d5081":"data = []\nlabels = []\n\nfor i in infected:\n    try:\n    \n        image = cv2.imread(\"..\/input\/cell_images\/cell_images\/Parasitized\/\"+i)\n        image_array = Image.fromarray(image , 'RGB')\n        resize_img = image_array.resize((64 , 64))\n        data.append(np.array(resize_img))\n        label = to_categorical(1, num_classes=2)\n        labels.append(label)\n        \n    except AttributeError:\n        print('')\n    \nfor u in uninfected:\n    try:\n        \n        image = cv2.imread(\"..\/input\/cell_images\/cell_images\/Uninfected\/\"+u)\n        image_array = Image.fromarray(image , 'RGB')\n        resize_img = image_array.resize((64 , 64))\n        data.append(np.array(resize_img))\n        label = to_categorical(0, num_classes=2)\n        labels.append(label)\n        \n    except AttributeError:\n        print('')","b64bb717":"data = np.array(data)\nlabels = np.array(labels)\n\nnp.save('Data' , data)\nnp.save('Labels' , labels)","de5dd003":"print('Cells : {} | labels : {}'.format(data.shape , labels.shape))","9ad34917":"plt.figure(1, figsize = (15 , 7))\nplt.subplot(1 , 2 , 1)\nplt.imshow(data[0])\nplt.title('Infected Cell')\nplt.xticks([]) , plt.yticks([])\n\nplt.subplot(1 , 2 , 2)\nplt.imshow(data[15000])\nplt.title('Uninfected Cell')\nplt.xticks([]) , plt.yticks([])\n\nplt.show()","1e61b604":"n = np.arange(data.shape[0])\nnp.random.shuffle(n)\ndata = data[n]\nlabels = labels[n]","4bc3c933":"data = data.astype(np.float32)\nlabels = labels.astype(np.int32)\ndata = data\/255","1b528a67":"from sklearn.model_selection import train_test_split\n\ntrain_x , eval_x , train_y , eval_y = train_test_split(data , labels , \n                                            test_size = 0.2 ,\n                                            random_state = 111)\n\n# eval_x , test_x , eval_y , test_y = train_test_split(x , y , \n#                                                     test_size = 0.2 , \n#                                                     random_state = 111)","961fbdbd":"print('train data shape {} ,eval data shape {} '.format(train_x.shape, eval_x.shape))","19bec0b9":"train_aug = ImageDataGenerator(\n    rescale=1.\/255,\n    shear_range=0.2,  \n    zoom_range=0.2,        \n    horizontal_flip=True,\n    vertical_flip=True)  \n\nval_aug= ImageDataGenerator(\n    rescale=1.\/255)\n\ntrain_gen = train_aug.flow(\n    train_x,\n    train_y,\n    batch_size=16)\n\nval_gen = val_aug.flow(\n    eval_x,\n    eval_y,\n    batch_size=16)\n","1ca9b002":"def show_final_history(history):\n    fig, ax = plt.subplots(1, 2, figsize=(15,5))\n    ax[0].set_title('loss')\n    ax[0].plot(history.epoch, history.history[\"loss\"], label=\"Train loss\")\n    ax[0].plot(history.epoch, history.history[\"val_loss\"], label=\"Validation loss\")\n    ax[1].set_title('acc')\n    ax[1].plot(history.epoch, history.history[\"acc\"], label=\"Train acc\")\n    ax[1].plot(history.epoch, history.history[\"val_acc\"], label=\"Validation acc\")\n    ax[0].legend()\n    ax[1].legend()","7b4d2973":"def ConvBlock(model, layers, filters,name):\n    for i in range(layers):\n        model.add(SeparableConv2D(filters, (3, 3), activation='relu',name=name))\n        model.add(BatchNormalization())\n        model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n    \ndef FCN():\n    model = Sequential()\n    model.add(Lambda(lambda x: x, input_shape=(64, 64, 3)))\n    ConvBlock(model, 1, 64,'block_1')\n    ConvBlock(model, 1, 128,'block_2')\n    ConvBlock(model, 1, 256,'block_3')\n    ConvBlock(model, 1, 512,'block_4')\n    model.add(Flatten())\n    model.add(Dense(1024,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(2,activation='sigmoid'))\n    return model\n\nmodel = FCN()\nmodel.summary()\n\n# SVG(model_to_dot(model).create(prog='dot', format='svg'))\n# plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","2089f122":"#-------Callbacks-------------#\nbest_model_weights = '.\/base.model'\ncheckpoint = ModelCheckpoint(\n    best_model_weights,\n    monitor='val_loss',\n    verbose=1,\n    save_best_only=True,\n    mode='min',\n    save_weights_only=False,\n    period=1\n)\nearlystop = EarlyStopping(\n    monitor='val_loss',\n    min_delta=0.001,\n    patience=7,\n    verbose=2,\n    mode='min'\n)\n\nreduce = ReduceLROnPlateau(\n    monitor='val_loss',\n    factor=0.5,\n    patience=40,\n    verbose=1, \n    mode='auto',\n    cooldown=1 \n)\n\ncallbacks = [checkpoint,earlystop,reduce]","b29869e8":"opt = SGD(lr=1e-4,momentum=0.99)\nopt1 = Adam(lr=2e-4)\n\nmodel.compile(\n    loss='binary_crossentropy',\n    optimizer=opt,\n    metrics=['accuracy']\n)\n    \nhistory = model.fit_generator(\n    train_gen, \n    steps_per_epoch  = 5000, \n    validation_data  = val_gen,\n    validation_steps = 2000,\n    epochs = 10, \n    verbose = 1,\n    callbacks=callbacks\n)","3d68d85e":"show_final_history(history)\nmodel.load_weights(best_model_weights)\nmodel_score = model.evaluate_generator(val_gen,steps=50)\nprint(\"Model Test Loss:\",model_score[0])\nprint(\"Model Test Accuracy:\",model_score[1])\nmodel.save('malaria.h5')","04398254":"preds = model.predict(eval_x, batch_size=16)\npreds = np.argmax(preds, axis=-1)\n\n# Original labels\norig_test_labels = np.argmax(eval_y, axis=-1)\n\nprint(orig_test_labels.shape)\nprint(preds.shape)\n\nprint(np.unique(orig_test_labels))\nprint(np.unique(preds))","9e467b1c":"\nfrom sklearn.metrics import confusion_matrix\ncm=confusion_matrix(orig_test_labels , preds)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True,cmap=plt.cm.Blues)\nplt.xticks(range(2), ['Normal', 'Infected'], fontsize=16)\nplt.yticks(range(2), ['Normal', 'Infected'], fontsize=16)\nplt.show()\n","90d7c048":"**TRAIN,TEST,SPLIT**","dd98c870":"\nThe images from infected folder are read one by one.\n\nThe images are resized to (64x64) and stored to data list.\n\nThe images in infected folder are assigned the label=1 and stored in labels list.\n\nThe process is similar for uninfected folder but here the label=0. \n\n","cd13eb3f":"*LAYERS:-*\n\nDepthwise SeparableConv is a good replacement for Conv layer. It introduces lesser number of parameters as compared to normal convolution and as different filters are applied to each channel, it captures more information.\n\nBatchNormalization and Dropout are used to prevent overfitting.\n\nInput shape is taken as 64x64 as also shown in the preprocessing.","633778e1":"****This is the KERAS CNN implementation for the MALARIA CELL IMAGES DATASET with 95% accuracy********\n\nANY FEEDBACK IN THE COMMENTS WILL BE HIGHLY APPRECIATED.","639ee228":"**PLOTS FOR INFECTED AND UNINFECTED CELL:-**","48517030":"Data augmentation is a powerful technique which helps in almost every case for improving the robustness of a model. But augmentation can be much more helpful where the dataset is imbalanced. You can generate different samples of undersampled class in order to try to balance the overall distribution.\n\nHere we can implement both horizontal and vertical flips because that will not alter the result.A horizontally or vertically flipped infected cell still remains an infected cell.","7624c05f":"**MODEL**","eacab03e":"As you can see the train and test are highly imbalanced after our data preprocessing.\n\nSo, data augmentation will help us in getting results for our test set.","ea32a460":"Note that the monitor for the callbacks is val_loss.","df3d174f":"**DATA AUGMENTATION**","36e035fe":"**PREDICTIONS**","3ae3e4ee":"**CALLBACKS**","208cfe31":"**DATA PREPROCESSING:-**","44ab602a":"**Function for plotting loss,accuracy**","037ee065":"Breakdown of this notebook:\n\n1. Loading the dataset: Load the data and import the libraries.\n1. Data Preprocessing: \n     * Reading the images,labels stored in 2 folders(Parasitized,Uninfected).\n     * Plotting the Uninfected and Parasitized images with their respective labels.\n     * Normalizing the image data.\n     * Train,test split\n1. Data Augmentation: Augment the train and validation data using ImageDataGenerator\n1. Creating and Training the Model: Create a cnn model in KERAS.\n1. Evaluation: Display the plots from the training history.\n1. Submission: Run predictions with model.predict, and create confusion matrix."}}