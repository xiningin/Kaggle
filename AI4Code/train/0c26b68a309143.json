{"cell_type":{"fbbc29f5":"code","8a82fa8f":"code","6bd4083a":"code","39bca6a3":"code","16ec9f0c":"code","16297dc3":"code","c4b4c34b":"code","fd2520bc":"code","f909b205":"code","4b2bb357":"code","23f26d1f":"code","3fa0afdc":"code","ecb482d7":"code","b253dd09":"code","8cd516c6":"code","26dcd963":"code","18c6c9cc":"code","bb0a4593":"code","38e5815a":"code","c1ea9972":"code","2a01c1b2":"code","d76a359c":"code","81aab798":"code","e0c9df78":"code","61743cb9":"code","4e5f6f25":"code","9da222cd":"code","2b7addee":"code","f4cc4325":"markdown","fbe87b62":"markdown","f0cbe9f7":"markdown","721b0862":"markdown","582a43dd":"markdown","3c01b458":"markdown","70d18b0d":"markdown","72a3adba":"markdown"},"source":{"fbbc29f5":"!pip install transformers","8a82fa8f":"import random\nimport numpy as np\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n                              TensorDataset)\nfrom sklearn.metrics import accuracy_score, f1_score\n\nfrom transformers import *","6bd4083a":"# Load dataframes\ntrain_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsample = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","39bca6a3":"# We'll use 80% of the training data set to \n# actually train the model, and use the rest as validation set\nmsk = np.random.rand(len(train_df)) < 0.8\ntrain = train_df[msk]\nvalid = train_df[~msk]","16ec9f0c":"# InputExample is one unit of data, with an id, text, and label\nclass InputExample(object):\n    def __init__(self, id, text, label=None):\n        self.id = id\n        self.text = text\n        self.label = label\n\n# InputData is one unit of features that we'll actually feed into BERT,\n# with an ID, input_ids, input_mask, segment_ids, and label (target value)\nclass InputData(object):\n    def __init__(self,\n                 example_id,\n                 choices_features,\n                 label\n\n                 ):\n        self.example_id = example_id\n        _, input_ids, input_mask, segment_ids = choices_features[0]\n        self.choices_features = {\n            'input_ids': input_ids,\n            'input_mask': input_mask,\n            'segment_ids': segment_ids\n        }\n        self.label = label\n\n# Generate examples from the dataframe\ndef read_examples(df, is_training):\n    if not is_training:\n        df['target'] = np.zeros(len(df), dtype=np.int64)\n    examples = []\n    for val in df[['id', 'text', 'target']].values:\n        examples.append(InputExample(id=val[0], text=val[1], label=val[2]))\n    return examples, df\n\ndef select_field(features, field):\n    return [\n        feature.choices_features[field] for feature in features\n    ]","16297dc3":"# Generate the input_ids, input_mask, and segment_ids as described above\n# Note that, for now, we're considering every tweet to be only one sentence.\ndef convert_examples_to_features(examples, tokenizer, max_seq_length,\n                                 is_training):\n    features = []\n    for example_index, example in enumerate(examples):\n\n        text = tokenizer.tokenize(example.text)\n        MAX_TEXT_LEN = max_seq_length - 2 \n        text = text[:MAX_TEXT_LEN]\n\n        choices_features = []\n\n        tokens = [\"[CLS]\"] + text + [\"[SEP]\"]  \n        segment_ids = [0] * (len(text) + 2) \n        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n        input_mask = [1] * len(input_ids)\n\n        padding_length = max_seq_length - len(input_ids)\n        input_ids += ([0] * padding_length)\n        input_mask += ([0] * padding_length)\n        segment_ids += ([0] * padding_length)\n        choices_features.append((tokens, input_ids, input_mask, segment_ids))\n\n        label = example.label\n\n        features.append(\n            InputData(\n                example_id=example.id,\n                choices_features=choices_features,\n                label=label\n            )\n        )\n    return features","c4b4c34b":"# The BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","fd2520bc":"# Hyperparameters\nmax_seq_length = 512  \nlearning_rate = 1e-5  \nnum_epochs = 2  \nbatch_size = 8 ","f909b205":"# Training\ntrain_examples, train_df = read_examples(train, is_training=True)\nlabels = train_df['target'].astype(int).values\ntrain_features = convert_examples_to_features(\n    train_examples, tokenizer, max_seq_length, True)\ntrain_input_ids = torch.tensor(select_field(train_features, 'input_ids'))\ntrain_input_mask = torch.tensor(select_field(train_features, 'input_mask'))\ntrain_segment_ids = torch.tensor(select_field(train_features, 'segment_ids'))\ntrain_label = torch.tensor([f.label for f in train_features])\n\n# Validation\nvalid_examples, valid_df = read_examples(valid, is_training=True)\nlabels = valid_df['target'].astype(int).values\nvalid_features = convert_examples_to_features(\n    valid_examples, tokenizer, max_seq_length, True)\nvalid_input_ids = torch.tensor(select_field(valid_features, 'input_ids'))\nvalid_input_mask = torch.tensor(select_field(valid_features, 'input_mask'))\nvalid_segment_ids = torch.tensor(select_field(valid_features, 'segment_ids'))\nvalid_label = torch.tensor([f.label for f in valid_features])\n\n# Test\ntest_examples, test_df = read_examples(test, is_training=False)\ntest_features = convert_examples_to_features(\n    test_examples, tokenizer, max_seq_length, True)\ntest_input_ids = torch.tensor(select_field(test_features, 'input_ids'), dtype=torch.long)\ntest_input_mask = torch.tensor(select_field(test_features, 'input_mask'), dtype=torch.long)\ntest_segment_ids = torch.tensor(select_field(test_features, 'segment_ids'), dtype=torch.long)\n\n# Torch datasets\ntrain = torch.utils.data.TensorDataset(train_input_ids, train_input_mask, train_segment_ids, train_label)\nvalid = torch.utils.data.TensorDataset(valid_input_ids, valid_input_mask, valid_segment_ids, valid_label)\ntest = torch.utils.data.TensorDataset(test_input_ids, test_input_mask, test_segment_ids)\n\n# Torch dataloaders\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n","4b2bb357":"class NeuralNet(nn.Module):\n    def __init__(self, hidden_size = 768, num_classes = 2):\n        super(NeuralNet, self).__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased',  \n                                        output_hidden_states=True,\n                                        output_attentions=True)\n\n        for param in self.bert.parameters():\n            param.requires_grad = True\n        \n        self.drop_out = nn.Dropout() # dropout layer to prevent overfitting\n        self.fc = nn.Linear(hidden_size, num_classes) # fully connected layer\n        \n    def forward(self, input_ids, input_mask, segment_ids):\n        \n        last_hidden_state, pooler_output, all_hidden_states, all_attentions = self.bert(input_ids, \n                                                                                        token_type_ids = segment_ids,\n                                                                                        attention_mask = input_mask)\n        last_hidden_state = last_hidden_state[:, 0,:]                                                       \n        \n        # Linear layer expects a tensor of size [batch size, input size]\n        out = self.drop_out(last_hidden_state) \n        out = self.fc(out) \n        return F.log_softmax(out)","23f26d1f":"model = NeuralNet()\nmodel.cuda() # Send the model to the GPU","3fa0afdc":"# Training the model\nmodel.train()\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\ntotal_step = len(train_loader)\nfor epoch in range(num_epochs):\n    train_loss = 0.\n    \n    for i, batch in enumerate(train_loader):\n        batch = tuple(t.cuda() for t in batch)\n        x_ids, x_mask, x_sids, y_truth = batch\n        y_pred = model(x_ids, x_mask, x_sids)\n        loss = loss_fn(y_pred, y_truth)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item() \/ len(train_loader)\n\n        total = len(y_truth)\n        _, predicted = torch.max(y_pred.data, 1)\n        correct = (predicted == y_truth).sum().item()\n\n        # Uncomment to get status updates during training\n        '''\n        if (i + 1) % 50 == 0:\n            print('Epoch [{}\/{}], Step [{}\/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n                          (correct \/ total) * 100))\n        '''     ","ecb482d7":"# Test the model on the validation set\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for i, batch in enumerate(valid_loader):\n        batch = tuple(t.cuda() for t in batch)\n        #batch = tuple(t for t in batch)\n        x_ids, x_mask, x_sids, y_truth = batch\n        y_pred = model(x_ids, x_mask, x_sids)\n        _, predicted = torch.max(y_pred.data, 1)\n        total += len(labels)\n        correct += (predicted == y_truth).sum().item()\n\n    print('Test Accuracy of the model on the validation set: {} %'.format((correct \/ total) * 100))\n","b253dd09":"test_preds = []\nwith torch.no_grad():\n        for i, batch in enumerate(test_loader):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids = batch\n            y_pred = model(x_ids, x_mask, x_sids).detach()\n            test_preds[i * batch_size:(i + 1) * batch_size] = F.softmax(y_pred, dim=1).cpu().numpy()  \n      ","8cd516c6":"sample['target'] = np.argmax(test_preds, axis = 1)\nsample.to_csv('submission.csv', index = False)","26dcd963":"from sklearn.metrics import confusion_matrix\n\n# Load dataframes\ntrain_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsample = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')\n\n# We'll use 80% of the training data set to \n# actually train the model, and use the rest as validation set\nmsk = np.random.rand(len(train_df)) < 0.8\ntrain = train_df[msk]\nvalid = train_df[~msk]","18c6c9cc":"batch_size = 1\n\n# Training\ntrain_examples, train_df = read_examples(train, is_training=True)\nlabels = train_df['target'].astype(int).values\ntrain_features = convert_examples_to_features(\n    train_examples, tokenizer, max_seq_length, True)\ntrain_input_ids = torch.tensor(select_field(train_features, 'input_ids'))\ntrain_input_mask = torch.tensor(select_field(train_features, 'input_mask'))\ntrain_segment_ids = torch.tensor(select_field(train_features, 'segment_ids'))\ntrain_label = torch.tensor([f.label for f in train_features])\n\n# Validation\nvalid_examples, valid_df = read_examples(valid, is_training=True)\nlabels = valid_df['target'].astype(int).values\nvalid_features = convert_examples_to_features(\n    valid_examples, tokenizer, max_seq_length, True)\nvalid_input_ids = torch.tensor(select_field(valid_features, 'input_ids'))\nvalid_input_mask = torch.tensor(select_field(valid_features, 'input_mask'))\nvalid_segment_ids = torch.tensor(select_field(valid_features, 'segment_ids'))\nvalid_label = torch.tensor([f.label for f in valid_features])\n\n# Test\ntest_examples, test_df = read_examples(test, is_training=False)\ntest_features = convert_examples_to_features(\n    test_examples, tokenizer, max_seq_length, True)\ntest_input_ids = torch.tensor(select_field(test_features, 'input_ids'), dtype=torch.long)\ntest_input_mask = torch.tensor(select_field(test_features, 'input_mask'), dtype=torch.long)\ntest_segment_ids = torch.tensor(select_field(test_features, 'segment_ids'), dtype=torch.long)\n\n# Torch datasets\ntrain = torch.utils.data.TensorDataset(train_input_ids, train_input_mask, train_segment_ids, train_label)\nvalid = torch.utils.data.TensorDataset(valid_input_ids, valid_input_mask, valid_segment_ids, valid_label)\ntest = torch.utils.data.TensorDataset(test_input_ids, test_input_mask, test_segment_ids)\n\n# Torch dataloaders\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\nvalid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)","bb0a4593":"Bert = BertModel.from_pretrained('bert-base-uncased',  \n                                    output_hidden_states=True,\n                                    output_attentions=True)\nBert.cuda()\n\n# These will be used to generate training and testing dataframes\n# to train the logistic regression model on\ntotal_step = len(train_loader)\ntrain_rows = []\n\nwith torch.no_grad():\n        for i, batch in enumerate(train_loader):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids, y_truth = batch\n            last_hidden_state, _, _, _ = Bert(x_ids, \n                                                token_type_ids = x_sids,\n                                              attention_mask = x_mask)\n            train_rows.append(np.array(last_hidden_state[:, 0, :].cpu()))\n            \n            # Uncomment to get status updates\n            '''\n            if (i + 1) % 50 == 0:\n                 print('Step [{}\/{}]'.format( i + 1, total_step))\n            '''          ","38e5815a":"total_step = len(valid_loader)\nvalid_rows = []\n\nwith torch.no_grad():\n        for i, batch in enumerate(valid_loader):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids, y_truth = batch\n            last_hidden_state, _, _, _ = Bert(x_ids, \n                                                token_type_ids = x_sids,\n                                              attention_mask = x_mask)\n            valid_rows.append(np.array(last_hidden_state[:, 0, :].cpu()))\n            \n            # Uncomment to get status updates\n            '''\n            if (i + 1) % 50 == 0:\n                print('Step [{}\/{}]'.format( i + 1, total_step))  \n            '''","c1ea9972":"total_step = len(test_loader)\ntest_rows = []\n\nwith torch.no_grad():\n        for i, batch in enumerate(test_loader):\n            batch = tuple(t.cuda() for t in batch)\n            x_ids, x_mask, x_sids = batch\n            last_hidden_state, _, _, _ = Bert(x_ids, \n                                                token_type_ids = x_sids,\n                                              attention_mask = x_mask)\n            test_rows.append(np.array(last_hidden_state[:, 0, :].cpu()))\n            \n            # Uncomment to get status updates\n            '''\n            if (i + 1) % 50 == 0:\n                print('Step [{}\/{}]'.format( i + 1, total_step))  \n            '''","2a01c1b2":"# Train dataframe\ntrain_df2 = pd.DataFrame(np.concatenate(train_rows))\ntrain_df2['target'] = np.array(train_label)\n\n# Validation dataframe\nvalid_df2 = pd.DataFrame(np.concatenate(valid_rows))\nvalid_df2['target'] = np.array(valid_label)\n\n# Test dataframe\ntest_df2 = pd.DataFrame(np.concatenate(test_rows))","d76a359c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nlogreg = LogisticRegression()","81aab798":"X = train_df2.drop('target', axis = 1)\ny = train_df2['target']","e0c9df78":"X_train, X_test, y_train, y_test = train_test_split(X, y)","61743cb9":"logreg.fit(X_train, y_train)","4e5f6f25":"X_val = valid_df2.drop('target', axis = 1)\ny_val = valid_df2['target']\ny_val_pred = logreg.predict(X_val)\nconfusion_matrix(y_val, y_val_pred)","9da222cd":"# Generate the submission\ny_sub = logreg.predict(test_df2)","2b7addee":"sample['target'] = y_sub\n#sample.to_csv('submission.csv', index = False)","f4cc4325":"BERT expects us to do some work up front before using the model, and we construct these datasets so that they have everything BERT needs to function.\n\nThere are three main things BERT needs: \n\n1. input_ids: A tokenized vector of the sentence (a token for each word, and special tokens at the beginning and end of each sentence).  [CLS] tokens are used to indicate the beginning of a sentence, and [SEP] tokens are used to indicate the end of a sentence.\n\n2. segment_ids: A vector that identifies which sentence each token belongs to (BERT can take up to two sentences at a time)\n\n3. input_mask: A vector which represents where out tokens are (as opposed to padding)\n\nEvery vector needs to be of the same length, so we add a padding of 0's as needed, until a maximum length of 512.\n\nFor example, if the input text was \"There was a fire today\", BERT would expect the following inputs:<br \/>\ninput_ids: [[CLS], 'There', 'was', 'a', 'fire', 'today', [SEP], 0, ...., 0]<br \/>\nsegment_ids: [0, 0, 0, 0, 0, 0, 0,... 0] (note that these are all 0's, since there's only one sentence)<br \/>\ninput_mask: [1, 1, 1, 1, 1, 1, 1, 0, ...., 0]\n\nOne thing to note is that in this code, we act as if all tweets were a single sentence.  This code could be improved by utilizing segment_ids to mark different sentences.","fbe87b62":"<font size = \"5\">\nStep 3: Implement Models\n<\/font>","f0cbe9f7":"<font size = \"5\">The Big Picture: <\/font>\n<br>\nBefore we can apply a classification algorithm to text, we need to generate features that we can use in the algorithm.  BERT generates those features for us.\n<blank>\n<font size = \"5\">\nWhat is BERT? \n    <\/font>\n<br>\nBERT stands for Bidirectional Encoder Representations from Transformers.  It's a neural-network based technique for natural language process pretraining.\n\nBasically, Google built a neural network that takes in sentences from text and generates *contextualized sentence embeddings*: vector representations of sentences that capture the sentences' semantic meaning.\n\nThey did this by pretraining on BookCorpus, a dataset consisting of 11,038 unpublished books from 16 different genres and 2,500 million words from text passages of English Wikipedia, with the task of Next Sentence Prediction (NSP) and word masking (Maked LM or MLM).  In short, BERT knows how to read things and encode them meaningfully.\n\nThis is awesome because can use these sentence embeddings as features, either as one layer of a neural network, or as an input to another ML algorithm","721b0862":"<font size = \"5\">Model 1: A simple neural network<\/font> \n<br>\n(score 0.834351)\n<br>\n<br>\nThe first method we'll use is a very simple neural network, composed of the output of the output from BERT, a single dropout layer, and one fully connected layer.\n\nThe important thing here is what we actually use from BERT: the last hidden state (last_hidden_state).  BERT is a transformer, composed of 12 encoding layers.  The last_hidden_state is the output of the very last of those encoding layers.\n\nlast_hidden_state is a three dimensional tensor of size [batch_size, tokens (up to 512), hidden layer size (768)].  BERT is set up so that the sentence embedding resides at the location of the [CLS] token, and thus, we use last_hidden_state[:, 0, :] for classification tasks.","582a43dd":"<font size = \"5\"> Step 2: Import data and build datasets<\/font>\n<br>\n<br>\nWe need to build torch datasets (and then data loaders) that we can pass into BERT.\n\nCredit goes to Bibek for most of the code in his section, which was borrowed from his <a href = \"https:\/\/www.kaggle.com\/bibek777\/bert-baseline\">BERT Baseline<\/a> notebook.","3c01b458":"<font size = \"5\">Step 1: Install transformers and import libraries<\/font>\n<br>\n<br>\nBert can be implemented using the transformer library from hugging face.  It will need to be installed","70d18b0d":"<font size = \"5\">\nModel 2: Logistic Regression\n<\/font> \n<br>\n(Score: 0.54498) \n<br> \n<br>\nThe second method we'll use is to use the BERT output as features for training a logistic regression model.\n<br>\n<br>\nThe output from BERT could be used as an input to any classification algorithm.  We're just using logistic regression as an example.\n<br>\n<br>","72a3adba":"<font size = \"3\">\nIn this notebook, we're going to fine-tune Google's BERT model in two *very* simple ways.<\/font>\n<br>\n<br>\nFirst, we'll build a simple neural using PyTorch. Then, we'll use logistic regression. \n<br>\n<br>\nHopefully this notebook shows you how to implement BERT in many different ways, so that it can be a reference for use in other competitions.\n\n\n    "}}