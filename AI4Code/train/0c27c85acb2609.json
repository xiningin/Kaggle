{"cell_type":{"558479af":"code","db5537cd":"code","1a434e3b":"code","cccb3b2c":"code","3cbca169":"code","de34ace5":"code","fe65b6dd":"code","8c10707f":"code","6f7de27f":"code","d8422de5":"code","1f62ec9b":"code","ad442a24":"code","5eadfe03":"code","80b23f4a":"code","17e5d96a":"code","15112e12":"code","f84216ca":"code","2efc884e":"code","5b9649d1":"code","8b1e94d6":"code","563a5092":"code","3c6bd386":"markdown","a732f915":"markdown","0613df48":"markdown","943ea8d5":"markdown","8a42f360":"markdown","916a41e0":"markdown","5347bfe2":"markdown","7a381b75":"markdown","ac0cf0ef":"markdown","bdacd25b":"markdown","ff769f27":"markdown","304df6fa":"markdown","0f29ade8":"markdown","1e2d9124":"markdown","4ba02ae7":"markdown"},"source":{"558479af":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport torch\nimport transformers\nimport tokenizers\nfrom tqdm import tqdm\nimport pickle\nimport time\nfrom sklearn.base import BaseEstimator\nfrom sklearn.model_selection import ParameterGrid\nimport json\n\ntf.random.set_seed(1234)\n\nclass SentenceSelector(BaseEstimator):\n    def __init__(self, maxlen=96, bert_path=\"\/kaggle\/input\/tf-roberta\", epochs=3, batch_size=32, learning_rate=3e-5,\n                 optimizer=\"adam\", validation_split=0.2, ignore_neutrals=True, verbose=False):\n        self.verbose = verbose\n        self.maxlen = maxlen\n        self.bert_path = bert_path\n        self.bert_vocab = self.bert_path + \"\/vocab-roberta-base.json\"\n        self.bert_merges = self.bert_path + \"\/merges-roberta-base.txt\"\n        self.bert_config = self.bert_path + \"\/config-roberta-base.json\"\n        self.bert_pretrained = self.bert_path + \"\/pretrained-roberta-base.h5\"\n        self.tokenizer = tokenizers.ByteLevelBPETokenizer(vocab_file=self.bert_vocab,merges_file=self.bert_merges,\n                                                           lowercase=True, add_prefix_space=True)\n        self.model = None\n        self.epochs = epochs\n        self.batch_size = batch_size \n        self.learning_rate = learning_rate\n        self.optimizer = optimizer\n        self.validation_split = validation_split\n        self.ignore_neutrals = ignore_neutrals\n        self.sentiment_col = \"sentiment\"\n        self.text_col = \"text\"\n        self.sentiment_dict = {\"positive\": self.tokenizer.encode(\"positive\").ids[0],\n                                \"negative\": self.tokenizer.encode(\"negative\").ids[0],\n                                \"neutral\": self.tokenizer.encode(\"neutral\").ids[0]}\n        \n    def fit(self, X, y):\n        \n        # Gambis master for neutral texts\n        if self.ignore_neutrals == True:\n            X_temp = X.to_dict(orient=\"records\")\n            X_new = []\n            y_new = []\n            for i in range(len(X_temp)):\n                if X_temp[i][self.sentiment_col] != \"neutral\":\n                    X_new.append(X_temp[i])\n                    y_new.append(y[i])\n            X = pd.DataFrame(X_new)\n            y = pd.Series(y_new)\n            \n        if isinstance(X, pd.DataFrame):           \n            X = X.values\n        if isinstance(y, pd.DataFrame) or isinstance(y, pd.Series):\n            y = y.values      \n            \n        input_ids, attention_mask, token_type_ids, start_tokens, end_tokens = self._preprocessing_train(X, y)\n\n        K.clear_session()\n        model = self._build_model()\n\n        model.fit(x=[input_ids, attention_mask, token_type_ids], y=[start_tokens, end_tokens],\n                  epochs=self.epochs, batch_size=self.batch_size, verbose=1, validation_split=self.validation_split)\n\n        self.model = model\n\n    def _build_model(self):\n        ids = tf.keras.layers.Input((self.maxlen,), dtype=tf.int32)\n        att = tf.keras.layers.Input((self.maxlen,), dtype=tf.int32)\n        tok = tf.keras.layers.Input((self.maxlen,), dtype=tf.int32)\n        \n        if self.optimizer.lower() == \"rmsprop\":\n            optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate)\n        else:\n            optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n\n        config = transformers.RobertaConfig.from_pretrained(self.bert_config)\n        bert_model = transformers.TFRobertaModel.from_pretrained(self.bert_pretrained, config=config)\n        x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n\n        x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n        x1 = tf.keras.layers.Conv1D(128, 2,padding='same')(x1)\n        x1 = tf.keras.layers.LeakyReLU()(x1)\n        x1 = tf.keras.layers.Conv1D(64, 2,padding='same')(x1)\n        x1 = tf.keras.layers.Dense(1)(x1)\n        x1 = tf.keras.layers.Flatten()(x1)\n        x1 = tf.keras.layers.Activation('softmax')(x1)\n\n        x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n        x2 = tf.keras.layers.Conv1D(128, 2, padding='same')(x2)\n        x2 = tf.keras.layers.LeakyReLU()(x2)\n        x2 = tf.keras.layers.Conv1D(64, 2, padding='same')(x2)\n        x2 = tf.keras.layers.Dense(1)(x2)\n        x2 = tf.keras.layers.Flatten()(x2)\n        x2 = tf.keras.layers.Activation('softmax')(x2)\n\n        model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n        model.compile(loss=\"binary_crossentropy\", optimizer=optimizer)\n        return model\n        \n    def _preprocessing_train(self, X, y):            \n        input_ids = np.ones((X.shape[0], self.maxlen), dtype=\"int32\")\n        attention_mask = np.zeros((X.shape[0], self.maxlen), dtype=\"int32\")\n        token_type_ids = np.zeros((X.shape[0], self.maxlen), dtype=\"int32\")\n        start_tokens = np.zeros((X.shape[0], self.maxlen), dtype=\"int32\")\n        end_tokens = np.zeros((X.shape[0], self.maxlen), dtype=\"int32\")\n        \n        for k in range(X.shape[0]):\n            text1 = \" \" + \" \".join(X[k][0].split())\n            text2 = \" \".join(y[k].split())\n            idx = text1.find(text2)\n            chars = np.zeros((len(text1)))\n            chars[idx:idx + len(text2)] = 1\n            if text1[idx-1] == ' ':\n                chars[idx-1] = 1 \n\n            enc = self.tokenizer.encode(text1) \n\n            offsets = []; idx = 0\n            for t in enc.ids:\n                w = self.tokenizer.decode([t])\n                offsets.append((idx,idx+len(w)))\n                idx += len(w)\n\n            toks = []\n            for i,(a,b) in enumerate(offsets):\n                sm = np.sum(chars[a:b])\n                if sm>0: toks.append(i) \n\n            s_tok = self.sentiment_dict[X[k][1]]\n            input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n            attention_mask[k,:len(enc.ids)+5] = 1\n\n            if len(toks)>0:\n                start_tokens[k,toks[0]+1] = 1\n                end_tokens[k,toks[-1]+1] = 1\n\n        return input_ids, attention_mask, token_type_ids, start_tokens, end_tokens\n\n    def _preprocessing_test(self, X):            \n        input_ids = np.ones((X.shape[0], self.maxlen),dtype=\"int32\")\n        attention_mask = np.zeros((X.shape[0], self.maxlen),dtype=\"int32\")\n        token_type_ids = np.zeros((X.shape[0], self.maxlen),dtype=\"int32\")\n        \n        for k in range(X.shape[0]):\n            text1 = \" \"+\" \".join(X[k][0].split())\n            enc = self.tokenizer.encode(text1)                \n            s_tok = self.sentiment_dict[X[k][1]]\n            input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n            attention_mask[k,:len(enc.ids)+5] = 1\n            \n        return input_ids, attention_mask, token_type_ids\n\n    def score(self, X_test, y_test):\n        if self.model == None:\n            raise AttributeError('The model is not trained yet.')\n            \n        # Gambis master for neutral text\n        if self.ignore_neutrals == True:\n            preds = self._predict_ignore_neutrals(X_test)\n        else:\n            preds = self.predict(X_test)\n            \n        scores = []\n        \n        for pred, true in zip(preds, y_test):\n            score = self.jaccard(pred, true)\n            scores.append(score)\n            \n        return np.mean(scores)\n    \n    def jaccard(self, str1, str2):\n        \"The same score function of the competition with only an added of try except to avoid error when we have empty strings.\"\n        a = set(str1.lower().split()) \n        b = set(str2.lower().split())\n        c = a.intersection(b)\n\n        try:\n            jaccard_score = float(len(c)) \/ (len(a) + len(b) - len(c))\n        except Exception as e:\n            print(\"An error occurred when compute jaccard score:\", e)\n            jaccard_score = float(0)\n        finally:\n            return jaccard_score\n    \n    def predict(self, X):\n        if self.model == None:\n            raise AttributeError('The model is not trained yet.') \n            \n        # Gambis master for neutral texts\n        if self.ignore_neutrals == True:\n            return self._predict_ignore_neutrals(X)\n        \n        if isinstance(X, pd.DataFrame):\n            X = X.values\n            \n        input_ids, attention_mask, token_type_ids = self._preprocessing_test(X)\n        \n        preds = self.model.predict([input_ids, attention_mask, token_type_ids])\n        \n        selected_text = []\n        for k in range(X.shape[0]):\n            start = np.argmax(preds[0][k,])\n            end = np.argmax(preds[1][k,])\n            \n            if start > end: \n                text = X[k][0]\n            else:\n                text = \" \"+\" \".join(X[k][0].split())\n                enc = self.tokenizer.encode(text)\n                text = self.tokenizer.decode(enc.ids[start-1:end])\n                \n            selected_text.append(text)\n\n        return selected_text\n\n    def _predict_ignore_neutrals(self, X):\n        if self.model == None:\n            raise AttributeError('The model is not trained yet.')\n        \n        X.reset_index(drop=True, inplace=True)\n        \n        selected_text = []\n        for each in range(len(X)):\n            if X[self.sentiment_col][each] == \"neutral\":\n                selected_text.append(X[self.text_col][each])\n            else:\n                X_temp = X.loc[each]\n                X_temp = X_temp[[self.text_col, self.sentiment_col]]\n                X_temp = np.array([X_temp.values])\n                input_ids, attention_mask, token_type_ids = self._preprocessing_test(X_temp)\n                preds = self.model.predict([input_ids, attention_mask, token_type_ids])\n                for k in range(X_temp.shape[0]):\n                    start = np.argmax(preds[0][k,])\n                    end = np.argmax(preds[1][k,])\n\n                    if start > end: \n                        text = X_temp[k][0]\n                    else:\n                        text = \" \"+\" \".join(X_temp[k][0].split())\n                        enc = self.tokenizer.encode(text)\n                        text = self.tokenizer.decode(enc.ids[start-1:end])\n\n                    selected_text.append(text)\n\n        return selected_text\n    \n    def save_model(self, filepath):\n        if self.model == None:\n            raise AttributeError('The model is not trained yet.')     \n        tf.keras.models.save_model(self.model, filepath)\n    \n    def load_model(self, filepath, force=False):\n        if self.model != None and force == False:\n            raise AttributeError('You already have a loaded model. If you want to reload inform the parameter force=True')\n        self.model = tf.keras.models.load_model(filepath)\n        \n###########################################\n\ndef GridSearch(param_grid, X_train, y_train, X_test, y_test, train_size=None, test_size=None, log_scores=False, verbose=False):\n    if isinstance(train_size, float):\n        x1, x2, y1, y2 = train_test_split(X_train, y_train, train_size=train_size, random_state=42, stratify=X_train[\"sentiment\"])\n        if verbose:\n            print(\"train dataset reduced from {} to {} rows\".format(len(X_train), len(x1)))\n        X_train = x1.copy()\n        y_train = y1.copy()\n    if isinstance(test_size, float):\n        x1, x2, y1, y2 = train_test_split(X_test, y_test, test_size=test_size, random_state=42, stratify=X_test[\"sentiment\"])\n        if verbose:\n            print(\"test dataset reduced from {} to {} rows\".format(len(X_test), len(x2)))\n        X_test = x2.copy()\n        y_test = y2.copy()\n\n    param_grid = list(ParameterGrid(param_grid))\n    if verbose:\n        print(\"The param_grid will result in {} fits.\".format(len(param_grid)))\n        \n    scores = []\n    best_score = 0\n    best_param = None\n    \n    exec_counter = 0\n    for params in param_grid:\n        exec_counter += 1\n        if verbose:\n            print(\"{}\/{}\".format(exec_counter, len(param_grid)))\n        model = SentenceSelector()\n        model.set_params(**params)\n        model.fit(X_train, y_train)\n        score = model.score(X_test, y_test)\n        scores.append({\"score\": score, \"params\": model.get_params()})\n\n    for each in scores:\n        if each[\"score\"] > best_score:\n            best_score = each[\"score\"]\n            best_param = each[\"params\"]\n\n    if log_scores:\n        with open('gridsearch_scores_log.json', 'w') as outfile:\n            json.dump(scores, outfile)\n        \n    result = {\"score\": best_score, \"params\": best_param}\n    if verbose:\n        print(result)\n    return result\n\ndef input_files(data_map, tpu=False):\n    if tpu:\n        kaggle = KaggleDatasets()\n        for dataset in data_map.keys():\n            GCS_DS_PATH = KaggleDatasets().get_gcs_path(dataset)\n            if isinstance(data_map.get(dataset), str):\n                data_map[dataset] = GCS_DS_PATH\n            if isinstance(data_map.get(dataset), dict):\n                for file in data_map[dataset].keys():\n                    data_map[dataset][file] = GCS_DS_PATH + \"\/\" + data_map[dataset][file]\n    else:\n        for dataset in data_map.keys():\n            if isinstance(data_map.get(dataset), str):\n                data_map[dataset] = \"\/kaggle\/input\/{}\".format(data_map[dataset])\n            if isinstance(data_map.get(dataset), dict):\n                for file in data_map[dataset].keys():\n                    data_map[dataset][file] = \"\/kaggle\/input\/{}\/{}\".format(dataset, data_map[dataset][file])\n    return data_map","db5537cd":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    tpu_on = True\n    !pip install gcsfs\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n    tpu_on = False\n\nif tpu_on:\n    from kaggle_datasets import KaggleDatasets\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.","1a434e3b":"data_map = {\n    \"tweet-sentiment-extraction\": {\n        \"train\": \"train.csv\",\n        \"test\": \"test.csv\",\n        \"submission\": \"sample_submission.csv\"\n    },\n    \"tf-roberta\": \"tf-roberta\/\"\n}\n    \nfiles = input_files(data_map, tpu=tpu_on)\nfiles","cccb3b2c":"df_train = pd.read_csv(files[\"tweet-sentiment-extraction\"][\"train\"])\ndf_train.fillna('', inplace=True)\nprint(df_train.shape)\ndf_train.head()","3cbca169":"X = df_train.drop(columns=[\"textID\", \"selected_text\"])\ny = df_train[\"selected_text\"].values","de34ace5":"print(\"The biggest sentence has {} characteres.\".format(max([len(each) for each in X[\"text\"]])))","fe65b6dd":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42, stratify=X[\"sentiment\"])\n\nprint(\"train:\", len(X_train))\nprint(\"test:\", len(X_test))","8c10707f":"parameters = {\n    \"maxlen\": 141,\n    \"epochs\": 3,\n    \"batch_size\": 32,\n    \"learning_rate\": 3e-5,\n    \"validation_split\": 0.2,\n    \"optimizer\": \"adam\",\n    \"ignore_neutrals\": False,\n    \"bert_path\": files[\"tf-roberta\"],\n    \"model_filepath\": \"model.sav\",\n    \"GridSearch\": False,\n    \"ParameterGrid\": {\n        \"maxlen\": [96, 141],\n        \"epochs\": [3, 4, 5],\n        \"batch_size\": [16, 32],\n        \"learning_rate\": [2e-5, 3e-5, 5e-5],\n        \"ignore_neutrals\": [True, False],\n        \"optimizer\": [\"adam\"],\n        \"verbose\": [True],\n        \"validation_split\": [0.2],\n        \"bert_path\": [files[\"tf-roberta\"]]\n    }\n}","6f7de27f":"if parameters[\"GridSearch\"]:\n    gridsearch = GridSearch(parameters[\"ParameterGrid\"], X_train, y_train, X_test, y_test, train_size=0.3, test_size=0.1, log_scores=True, verbose=True)\n    model = SentenceSelector()\n    model.set_params(**gridsearch[\"params\"])\nelse:\n    model = SentenceSelector(maxlen=parameters[\"maxlen\"], epochs=parameters[\"epochs\"], batch_size=parameters[\"batch_size\"],\n                         ignore_neutrals=parameters[\"ignore_neutrals\"], learning_rate=parameters[\"learning_rate\"],\n                         optimizer=parameters[\"optimizer\"], validation_split=parameters[\"validation_split\"])\n\nmodel.fit(X_train, y_train)\nmodel.save_model(parameters[\"model_filepath\"])","d8422de5":"model.get_params()","1f62ec9b":"train_val = X_train.copy()\ntrain_val[\"true_selected_text\"] = y_train\ntrain_val = train_val.to_dict(orient=\"records\")\nlst = []\n\nwith tqdm(total=len(train_val)) as pbar:\n    for each in train_val:\n        each[\"selected_text\"] = model.predict(pd.DataFrame([each]))[0]\n        each[\"score\"] = model.jaccard(each[\"true_selected_text\"], each[\"selected_text\"])\n        lst.append(each)\n        pbar.update(1)\n    \ntrain_val = pd.DataFrame(lst)\nscore = train_val[\"score\"].mean()\n\nprint(\"\\n Score (training set): {}\".format(score))","ad442a24":"train_val.groupby(\"sentiment\").mean().plot(kind=\"bar\", rot=1);\n\ntrain_val.groupby(\"sentiment\").mean()","5eadfe03":"test_val = X_test.copy()\ntest_val[\"true_selected_text\"] = y_test\ntest_val = test_val.to_dict(orient=\"records\")\nlst = []\n\nwith tqdm(total=len(test_val)) as pbar:\n    for each in test_val:\n        each[\"selected_text\"] = model.predict(pd.DataFrame([each]))[0]\n        each[\"score\"] = model.jaccard(each[\"true_selected_text\"], each[\"selected_text\"])\n        lst.append(each)\n        pbar.update(1)\n    \ntest_val = pd.DataFrame(lst)\nscore = test_val[\"score\"].mean()\n\nprint(\"Score (testing set): {}\".format(score))","80b23f4a":"# Error analysis - Sentiment\ntest_val.groupby(\"sentiment\").mean().plot(kind=\"bar\", rot=1);\n\ntest_val.groupby(\"sentiment\").mean()","17e5d96a":"test_val.sort_values(by=\"score\", ascending=True).head(20)","15112e12":"test_val.sort_values(by=\"score\", ascending=False).head(20)","f84216ca":"df_test = pd.read_csv(files[\"tweet-sentiment-extraction\"][\"test\"])\ndf_test.fillna('', inplace=True)\nprint(df_test.shape)\ndf_test.head()","2efc884e":"df_test[\"selected_text\"] = model.predict(df_test[[\"text\", \"sentiment\"]])","5b9649d1":"df_sub = pd.read_csv(files[\"tweet-sentiment-extraction\"][\"submission\"])\ndf_sub.fillna('', inplace=True)\nprint(df_sub.shape)\ndf_sub.head()","8b1e94d6":"df_sub = df_sub.merge(df_test, on=\"textID\")\ndf_sub.drop(columns=[\"selected_text_x\", \"text\", \"sentiment\"], errors=\"ignore\", inplace=True)\ndf_sub.columns = [\"textID\", \"selected_text\"]\ndf_sub","563a5092":"df_sub.to_csv(\"submission.csv\", index=False)","3c6bd386":"## 2.2 Split in X and y","a732f915":"## 2.1 Load train dataset","0613df48":"## 2.3 Split in train and test","943ea8d5":"### 4.2.2 Error analysis (testing set) - Top 20 worst scores","8a42f360":"### 4.2.1 Error analysis (testing set) - Sentiment","916a41e0":"## 4.1 Validation - training set\n\nThis is a good sanity check if the model is working and has high enough capacity to fit the training data.","5347bfe2":"### 4.1.1 Error analysis (training set) - Sentiment","7a381b75":"### 4.2.3 Error analysis (testing set) - Top 20 best scores","ac0cf0ef":"# 4. Validation\n\nWe'll make the validation in two steps. First in the **training set** and then, in the **testing set**.","bdacd25b":"# Tweet Sentiment Extraction\n\nThis notebook was developed for the competition: [Tweet Sentiment Extraction](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction)\n\n## What this model will try to predict?\n\nIt's attempting to predict the word or phrase from the tweet that exemplifies the provided sentiment. The word or phrase should include all characters within that span (i.e. including commas, spaces, etc.). See an example below:\n\n```\n{\n    \"text\": \"Sooo SAD I will miss you here in San Diego!!!\",\n    \"selected_text\": \"Sooo SAD\",\n    \"sentiment\": \"negative\"\n}\n```\n\nThe objective it's find `selected_text` as **\"Soo SAD\"** in `text` **\"Sooo SAD I will miss you here in San Diego!!!\"**\n\n### How it's will be evaluate?\n\nThe metric in this competition is the [word-level Jaccard score](https:\/\/en.wikipedia.org\/wiki\/Jaccard_index). A good description of Jaccard similarity for strings is [here](https:\/\/towardsdatascience.com\/overview-of-text-similarity-metrics-3397c4601f50).\n\nA Python implementation based on the links above, and matched with the output of the C# implementation on the back end, is provided below.\n\n```\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n```\n\n---\n\n## Proposed solution\n\nWe'll apply RoBERTa model (by [ai.facebook](https:\/\/ai.facebook.com\/)) with Tensor Flow to solve this problem. It's one of *state of the art* for natural language processing today.\n\n[RoBERTa: A Robustly Optimized BERT Pretraining Approach](https:\/\/arxiv.org\/abs\/1907.11692) is based on Google's BERT model released in 2018.\n\nYou can see below the scores os RoBERTa compared with others ensembles:\n\n![image.png](attachment:image.png)\n\n---\n\n## Architecture\n\n![](https:\/\/trello-attachments.s3.amazonaws.com\/5d93d97e715aba7c97fde8f7\/5ea0ee2bfc245456c9f29a81\/5a070a62be29775d1b7f5dfdfbef4059\/tweet_sentiment_extraction.png)\n\n---\n\n## References\n\n- [TensorFlow roBERTa - \\[0.705\\] | Kaggle](https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705\/)\n- [transformers 2.8.0 documentation](https:\/\/huggingface.co\/transformers\/)\n","ff769f27":"# 2. Load and prepare dataset","304df6fa":"# 1. Modeling","0f29ade8":"## 4.2 Validation - testing set\n\n","1e2d9124":"# 3. Execution","4ba02ae7":"# 5. Load submission files"}}