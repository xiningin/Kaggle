{"cell_type":{"2f8d8916":"code","9e803a2b":"code","de1aa797":"code","60466d16":"code","2dc11796":"code","4b7d2815":"code","e0085814":"code","741ad4a5":"code","ac3759ad":"code","3e6a5884":"code","dc107011":"code","c487b85c":"code","16e6a755":"code","00687598":"code","1f35da2e":"code","c7240bc0":"code","48ee1dff":"code","7ed4f49e":"code","c2efe3d5":"code","26d43593":"code","df9a5ab1":"code","f450deb7":"code","6ac5e5a4":"code","d1fee97d":"code","ab09977f":"code","991563aa":"code","d096bcee":"code","9eef30fc":"code","9e940376":"code","64ce1762":"code","3db45443":"code","817f4900":"code","d6ee94c9":"code","a1260482":"code","32230173":"code","520012fc":"code","6658bcdc":"code","e8fabdd6":"code","8e9a27c6":"code","eafc6133":"markdown","81b42c66":"markdown","b4b1f8ab":"markdown","680b0210":"markdown","752420f5":"markdown","e23a00c1":"markdown","b4401b1f":"markdown","72442847":"markdown","8e5045a1":"markdown","fdcebc67":"markdown","c2a01e93":"markdown","4f180dc7":"markdown","3b877d23":"markdown","2c124795":"markdown","1aedd399":"markdown","efadba8a":"markdown","2b779950":"markdown","a7744927":"markdown","10fcdb1e":"markdown","6704956e":"markdown","fc6feef9":"markdown","61419c41":"markdown","0251ab9a":"markdown","5d4eea8c":"markdown","47e1dcf7":"markdown","3924b66d":"markdown"},"source":{"2f8d8916":"# Loading libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nimport xgboost as xgb\nimport statsmodels.api as sm\nfrom scipy.stats import chi2_contingency\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.formula.api import ols\n\n\n# Import datasets\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","9e803a2b":"train = pd.read_csv('..\/input\/learn-together\/train.csv')\ntest = pd.read_csv('..\/input\/learn-together\/test.csv')\n","de1aa797":"# Dimension of train dataset\ntrain.shape","60466d16":"# Dimension of test dataset\ntest.shape","2dc11796":"# Target variable\ny = train.Cover_Type\ntrain.drop(['Cover_Type'], axis=1, inplace=True)","4b7d2815":"# Whole dataframe\ndf = pd.concat([train,test], axis=0)\nX_full = df","e0085814":"# dimensions of dataset\nprint(X_full.shape)\n# list types for each attribute\nX_full.dtypes\n# take a peek at the first rows of the data\nX_full.head(5)\n# summarize attribute distributions for data frame\nprint(X_full.describe().T)\nprint(X_full.info())\ndef rstr(X_full): return X_full.shape, X_full.apply(lambda x: [x.unique()])\nprint(rstr(X_full))\n","741ad4a5":"# Look at the level of each feature\nfor column in X_full.columns:\n    print(column, X_full[column].nunique())","ac3759ad":"# numerical features\nX_full['Elevation'] = X_full['Elevation'].astype(float)\nX_full['Aspect'] = X_full['Aspect'].astype(float)\nX_full['Slope'] = X_full['Slope'].astype(float)\nX_full['Horizontal_Distance_To_Hydrology'] = X_full['Horizontal_Distance_To_Hydrology'].astype(float)\nX_full['Vertical_Distance_To_Hydrology'] = X_full['Vertical_Distance_To_Hydrology'].astype(float)\nX_full['Horizontal_Distance_To_Roadways'] = X_full['Horizontal_Distance_To_Roadways'].astype(float)\nX_full['Hillshade_9am'] = X_full['Hillshade_9am'].astype(float)\nX_full['Hillshade_Noon'] = X_full['Hillshade_Noon'].astype(float)\nX_full['Hillshade_3pm'] = X_full['Hillshade_3pm'].astype(float)\nX_full['Horizontal_Distance_To_Fire_Points'] = X_full['Horizontal_Distance_To_Fire_Points'].astype(float)\n","3e6a5884":"# check missing values both to numeric features and categorical features \nfeat_missing = []\n\nfor f in X_full.columns:\n    missings = X_full[f].isnull().sum()\n    if missings > 0:\n        feat_missing.append(f)\n        missings_perc = missings\/X_full.shape[0]\n        \n        # printing summary of missing values\n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n\n# how many variables do present missing values?\nprint()\nprint('In total, there are {} variables with missing values'.format(len(feat_missing)))","dc107011":"# summarize the class distribution\ny = y.astype(object) \ncount = pd.crosstab(index = y, columns=\"count\")\npercentage = pd.crosstab(index = y, columns=\"frequency\")\/pd.crosstab(index = y, columns=\"frequency\").sum()\npd.concat([count, percentage], axis=1)","c487b85c":"ax = sns.countplot(x=y, data=X_full).set_title(\"Target Variable Distribution\")","16e6a755":"# categorical features\ncategorical_cols = [cname for cname in X_full.columns if\n                    X_full[cname].dtype in ['int64']]\ncat = X_full[categorical_cols]\ncat.columns","00687598":"cat_train = cat.iloc[0:15119,:]\ncat_test = cat.iloc[15120:565892,:]","1f35da2e":"def rstr(cat_train): return cat_train.shape, cat_train.apply(lambda x: [x.unique()])\nprint(rstr(cat_train))\ndef rstr(cat_test): return cat_test.shape, cat_test.apply(lambda x: [x.unique()])\nprint(rstr(cat_test))","c7240bc0":"# Drop features not helpful \ncat = cat.drop(['Id', 'Soil_Type15', 'Soil_Type7'], axis=1)","48ee1dff":"# Visualizations\nsns.set( rc = {'figure.figsize': (5, 5)})\nfcat = ['Soil_Type1','Soil_Type2','Soil_Type3','Soil_Type4','Soil_Type5','Soil_Type6','Soil_Type8','Soil_Type9',\n       'Soil_Type10','Soil_Type11','Soil_Type12','Soil_Type13','Soil_Type14','Soil_Type16','Soil_Type17','Soil_Type18',\n        'Soil_Type19','Soil_Type20', 'Soil_Type21','Soil_Type22','Soil_Type23','Soil_Type24','Soil_Type25','Soil_Type26',\n        'Soil_Type27','Soil_Type28','Soil_Type29','Soil_Type30','Soil_Type31','Soil_Type32','Soil_Type33','Soil_Type34',\n        'Soil_Type35','Soil_Type36','Soil_Type37','Soil_Type38','Soil_Type39','Soil_Type40','Wilderness_Area1',\n       'Wilderness_Area2','Wilderness_Area3','Wilderness_Area4']\n\nfor col in fcat:\n    plt.figure()\n    sns.countplot(x=cat[col], data=cat, palette=\"Set3\")\n    plt.show()","7ed4f49e":"# Chi-Squared test as Feature Selection\ncat2 = pd.concat([y,cat_train], axis=1)\nclass ChiSquare:\n    def __init__(self, dataframe):\n        self.df = dataframe\n        self.p = None #P-Value\n        self.chi2 = None #Chi-2 Test Statistic\n        self.dof = None\n        \n        self.dfObserved = None\n        self.dfExpected = None\n        \n    def _print_chisquare_result(self, colX, alpha):\n        result = \"\"\n        if self.p<alpha:\n            result=\"{0} is IMPORTANT for Prediction\".format(colX)\n        else:\n            result=\"{0} is NOT an important predictor. (Discard {0} from the model)\".format(colX)\n\n        print(result)\n        \n    def TestIndependence(self,colX,colY, alpha=0.05):\n        X = self.df[colX].astype(str)\n        Y = self.df[colY].astype(str)\n        \n        self.dfObserved = pd.crosstab(Y,X) \n        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)\n        self.p = p\n        self.chi2 = chi2\n        self.dof = dof \n        \n        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n        \n        self._print_chisquare_result(colX,alpha)\n\n\n# Initialize Chi-Squared Test\ncT = ChiSquare(cat2)\n\n# Feature Selection\ntestColumns = ['Soil_Type1','Soil_Type2','Soil_Type3','Soil_Type4','Soil_Type5','Soil_Type6','Soil_Type8','Soil_Type9',\n       'Soil_Type10','Soil_Type11','Soil_Type12','Soil_Type13','Soil_Type14','Soil_Type16','Soil_Type17','Soil_Type18',\n        'Soil_Type19','Soil_Type20', 'Soil_Type21','Soil_Type22','Soil_Type23','Soil_Type24','Soil_Type25','Soil_Type26',\n        'Soil_Type27','Soil_Type28','Soil_Type29','Soil_Type30','Soil_Type31','Soil_Type32','Soil_Type33','Soil_Type34',\n        'Soil_Type35','Soil_Type36','Soil_Type37','Soil_Type38','Soil_Type39','Soil_Type40','Wilderness_Area1',\n       'Wilderness_Area2','Wilderness_Area3','Wilderness_Area4']\n\nfor var in testColumns:\n    cT.TestIndependence(colX=var,colY='Cover_Type') ","c2efe3d5":"# Drop feature not helpful by Feature Selection\ncat = cat.drop(['Soil_Type8', 'Soil_Type25'], axis=1)\ncat.shape","26d43593":"# Numerical features\nnumerical_cols = [cname for cname in X_full.columns if\n                 X_full[cname].dtype in ['float']]\nnum = X_full[numerical_cols]\nnum.columns","df9a5ab1":"def rstr(num): return num.shape, num.apply(lambda x: [x.nunique()])\nprint(rstr(num))","f450deb7":"# Visualizations\nsns.set( rc = {'figure.figsize': (5, 5)})\nfnum = ['Aspect','Elevation', 'Hillshade_3pm','Hillshade_9am', 'Hillshade_Noon', 'Horizontal_Distance_To_Fire_Points', \n        'Horizontal_Distance_To_Hydrology','Horizontal_Distance_To_Roadways', 'Slope', 'Vertical_Distance_To_Hydrology']\n\nfor col in fnum:\n    plt.figure()\n    x=num[col]\n    sns.distplot(x, bins=10)\n    plt.xticks(rotation=45)\n    plt.show()      ","6ac5e5a4":"for col in fnum:\n    plt.figure()\n    x=num[col]\n    sns.boxplot(x,palette=\"Set1\",linewidth=1)\n    plt.xticks(rotation=45)\n    plt.show()  ","d1fee97d":"# Anova Test as Feature Selection\nnum_train = num.iloc[0:15119,:]\nnum_test = num.iloc[15120:565892,:]\nnum2 = pd.concat([y,num_train], axis=1)\nnum2['Cover_Type'] = num2['Cover_Type'].astype(int)","ab09977f":"results = ols('Cover_Type ~ Aspect+Elevation+Hillshade_3pm+Hillshade_9am+Hillshade_Noon+Horizontal_Distance_To_Fire_Points+Horizontal_Distance_To_Hydrology+Horizontal_Distance_To_Roadways+Slope+Vertical_Distance_To_Hydrology', data=num2).fit()\naov_table = sm.stats.anova_lm(results, typ=2)\naov_table","991563aa":"# Outliers\n\n# Elevation\n#before\nplt.figure()\nx=num['Elevation']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n#corrections\ni = 'Elevation'\nq75, q25 = np.percentile(num.Elevation.dropna(), [75 ,25])\nq95, q05 = np.percentile(num.Elevation.dropna(), [95 ,5])\niqr = q75 - q25 \nmin = q25 - (iqr*1.5)\nmax = q75 + (iqr*1.5) \nnum[i].loc[num[i] < min] = q05\nnum[i].loc[num[i] > max] = q95\n\n#after\nplt.figure()\nx=num['Elevation']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n# Hillshade_3pm\n#before\nplt.figure()\nx=num['Hillshade_3pm']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show() \n\n#corrections\ni = 'Hillshade_3pm'\nq75, q25 = np.percentile(num.Hillshade_3pm.dropna(), [75 ,25])\nq95, q05 = np.percentile(num.Hillshade_3pm.dropna(), [95 ,5])\niqr = q75 - q25 \nmin = q25 - (iqr*1.5)\nmax = q75 + (iqr*1.5) \nnum[i].loc[num[i] < min] = q05\nnum[i].loc[num[i] > max] = q95\n\n#after\nplt.figure()\nx=num['Hillshade_3pm']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show() \n\n\n# Hillshade_9am\n#before\nplt.figure()\nx=num['Hillshade_9am']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n#corrections\ni = 'Hillshade_9am'\nq75, q25 = np.percentile(num.Hillshade_9am.dropna(), [75 ,25])\nq95, q05 = np.percentile(num.Hillshade_9am.dropna(), [95 ,5])\niqr = q75 - q25 \nmin = q25 - (iqr*1.5)\nmax = q75 + (iqr*1.5) \nnum[i].loc[num[i] < min] = q05\nnum[i].loc[num[i] > max] = q95\n\n#after\nplt.figure()\nx=num['Hillshade_9am']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n\n# Hillshade_Noon\n#before\nplt.figure()\nx=num['Hillshade_Noon']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n#corrections\ni = 'Hillshade_Noon'\nq75, q25 = np.percentile(num.Hillshade_Noon.dropna(), [75 ,25])\nq95, q05 = np.percentile(num.Hillshade_Noon.dropna(), [95 ,5])\niqr = q75 - q25 \nmin = q25 - (iqr*1.5)\nmax = q75 + (iqr*1.5) \nnum[i].loc[num[i] < min] = q05\nnum[i].loc[num[i] > max] = q95\n\n#after\nplt.figure()\nx=num['Hillshade_Noon']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n\n# Horizontal_Distance_To_Fire_Points\n#before\nplt.figure()\nx=num['Horizontal_Distance_To_Fire_Points']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n#corrections\ni = 'Horizontal_Distance_To_Fire_Points'\nq75, q25 = np.percentile(num.Horizontal_Distance_To_Fire_Points.dropna(), [75 ,25])\nq95, q05 = np.percentile(num.Horizontal_Distance_To_Fire_Points.dropna(), [95 ,5])\niqr = q75 - q25 \nmin = q25 - (iqr*1.5)\nmax = q75 + (iqr*1.5) \nnum[i].loc[num[i] < min] = q05\nnum[i].loc[num[i] > max] = q95\n\n#after\nplt.figure()\nx=num['Horizontal_Distance_To_Fire_Points']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n# Horizontal_Distance_To_Hydrology\n#before\nplt.figure()\nx=num['Horizontal_Distance_To_Hydrology']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n#corrections\ni = 'Horizontal_Distance_To_Hydrology'\nq75, q25 = np.percentile(num.Horizontal_Distance_To_Hydrology.dropna(), [75 ,25])\nq95, q05 = np.percentile(num.Horizontal_Distance_To_Hydrology.dropna(), [95 ,5])\niqr = q75 - q25 \nmin = q25 - (iqr*1.5)\nmax = q75 + (iqr*1.5) \nnum[i].loc[num[i] < min] = q05\nnum[i].loc[num[i] > max] = q95\n\n#after\nplt.figure()\nx=num['Horizontal_Distance_To_Hydrology']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n\n# Horizontal_Distance_To_Roadways\n#before\nplt.figure()\nx=num['Horizontal_Distance_To_Roadways']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n#corrections\ni = 'Horizontal_Distance_To_Roadways'\nq75, q25 = np.percentile(num.Horizontal_Distance_To_Roadways.dropna(), [75 ,25])\nq95, q05 = np.percentile(num.Horizontal_Distance_To_Roadways.dropna(), [95 ,5])\niqr = q75 - q25 \nmin = q25 - (iqr*1.5)\nmax = q75 + (iqr*1.5) \nnum[i].loc[num[i] < min] = q05\nnum[i].loc[num[i] > max] = q95\n\n#after\nplt.figure()\nx=num['Horizontal_Distance_To_Roadways']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n\n# Slope\n#before\nplt.figure()\nx=num['Slope']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n#corrections\ni = 'Slope'\nq75, q25 = np.percentile(num.Slope.dropna(), [75 ,25])\nq95, q05 = np.percentile(num.Slope.dropna(), [95 ,5])\niqr = q75 - q25 \nmin = q25 - (iqr*1.5)\nmax = q75 + (iqr*1.5) \nnum[i].loc[num[i] < min] = q05\nnum[i].loc[num[i] > max] = q95\n\n#after\nplt.figure()\nx=num['Slope']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n\n# Vertical_Distance_To_Hydrology\n#before\nplt.figure()\nx=num['Vertical_Distance_To_Hydrology']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n\n#corrections\ni = 'Vertical_Distance_To_Hydrology'\nq75, q25 = np.percentile(num.Vertical_Distance_To_Hydrology.dropna(), [75 ,25])\nq95, q05 = np.percentile(num.Vertical_Distance_To_Hydrology.dropna(), [95 ,5])\niqr = q75 - q25 \nmin = q25 - (iqr*1.5)\nmax = q75 + (iqr*1.5) \nnum[i].loc[num[i] < min] = q05\nnum[i].loc[num[i] > max] = q95\n\n#after\nplt.figure()\nx=num['Vertical_Distance_To_Hydrology']\nsns.boxplot(x,palette=\"Set2\",linewidth=1)\nplt.xticks(rotation=45)\nplt.show()\n","d096bcee":"num['X1X2'] = num['Aspect']*num['Elevation']\nnum['X1X3'] = num['Aspect']*num['Hillshade_3pm']\nnum['X1X4'] = num['Aspect']*num['Hillshade_9am']\nnum['X1X5'] = num['Aspect']*num['Hillshade_Noon']\nnum['X1X6'] = num['Aspect']*num['Horizontal_Distance_To_Fire_Points']\nnum['X1X7'] = num['Aspect']*num['Horizontal_Distance_To_Hydrology']\nnum['X1X8'] = num['Aspect']*num['Horizontal_Distance_To_Roadways']\nnum['X1X9'] = num['Aspect']*num['Slope']\nnum['X1X10'] = num['Aspect']*num['Vertical_Distance_To_Hydrology']\nnum['X2X3'] = num['Elevation']*num['Hillshade_3pm']\nnum['X2X4'] = num['Elevation']*num['Hillshade_9am']\nnum['X2X5'] = num['Elevation']*num['Hillshade_Noon']\nnum['X2X6'] = num['Elevation']*num['Horizontal_Distance_To_Fire_Points']\nnum['X2X7'] = num['Elevation']*num['Horizontal_Distance_To_Hydrology']\nnum['X2X8'] = num['Elevation']*num['Horizontal_Distance_To_Roadways']\nnum['X2X9'] = num['Elevation']*num['Slope']\nnum['X2X10'] = num['Elevation']*num['Vertical_Distance_To_Hydrology']\nnum['X3X4'] = num['Hillshade_3pm']*num['Hillshade_9am']\nnum['X3X5'] = num['Hillshade_3pm']*num['Hillshade_Noon']\nnum['X3X6'] = num['Hillshade_3pm']*num['Horizontal_Distance_To_Fire_Points']\nnum['X3X7'] = num['Hillshade_3pm']*num['Horizontal_Distance_To_Hydrology']\nnum['X3X8'] = num['Hillshade_3pm']*num['Horizontal_Distance_To_Roadways']\nnum['X3X9'] = num['Hillshade_3pm']*num['Slope']\nnum['X3X10'] = num['Hillshade_3pm']*num['Vertical_Distance_To_Hydrology']\nnum['X4X5'] = num['Hillshade_9am']*num['Hillshade_Noon']\nnum['X4X6'] = num['Hillshade_9am']*num['Horizontal_Distance_To_Fire_Points']\nnum['X4X7'] = num['Hillshade_9am']*num['Horizontal_Distance_To_Hydrology']\nnum['X4X8'] = num['Hillshade_9am']*num['Horizontal_Distance_To_Roadways']\nnum['X4X9'] = num['Hillshade_9am']*num['Slope']\nnum['X4X10'] = num['Hillshade_9am']*num['Vertical_Distance_To_Hydrology']\nnum['X5X6'] = num['Hillshade_Noon']*num['Horizontal_Distance_To_Fire_Points']\nnum['X5X7'] = num['Hillshade_Noon']*num['Horizontal_Distance_To_Hydrology']\nnum['X5X8'] = num['Hillshade_Noon']*num['Horizontal_Distance_To_Roadways']\nnum['X5X9'] = num['Hillshade_Noon']*num['Slope']\nnum['X5X10'] = num['Hillshade_Noon']*num['Vertical_Distance_To_Hydrology']\nnum['X6X7'] = num['Horizontal_Distance_To_Fire_Points']*num['Horizontal_Distance_To_Hydrology']\nnum['X6X8'] = num['Horizontal_Distance_To_Fire_Points']*num['Horizontal_Distance_To_Roadways']\nnum['X6X9'] = num['Horizontal_Distance_To_Fire_Points']*num['Slope']\nnum['X6X10'] = num['Horizontal_Distance_To_Fire_Points']*num['Vertical_Distance_To_Hydrology']\nnum['X7X8'] = num['Horizontal_Distance_To_Hydrology']*num['Horizontal_Distance_To_Roadways']\nnum['X7X9'] = num['Horizontal_Distance_To_Hydrology']*num['Slope']\nnum['X7X10'] = num['Horizontal_Distance_To_Hydrology']*num['Vertical_Distance_To_Hydrology']\nnum['X8X9'] = num['Horizontal_Distance_To_Roadways']*num['Slope']\nnum['X8X10'] = num['Horizontal_Distance_To_Roadways']*num['Vertical_Distance_To_Hydrology']\nnum['X9X10'] = num['Slope']*num['Vertical_Distance_To_Hydrology']\n","9eef30fc":"X_all = pd.concat([cat, num], axis=1)\nX_all.shape","9e940376":"# Create correlation matrix\ncorr_matrix = X_all.corr().abs()\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n# Find index of feature columns with correlation greater than 0.75\nto_drop = [column for column in upper.columns if any(upper[column] > 0.75)]\nto_drop","64ce1762":"# Drop features \nX_all.drop(X_all[to_drop], axis=1, inplace=True)\nX_all.shape\n","3db45443":"y = y.astype('int')\ntrain_ = X_all.iloc[0:15120,:]\ntest_ = X_all.iloc[15121:581012,:]\n# Break off train and validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(train_, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n","817f4900":"# Test options and evaluation metric\n\n# Spot Check Algorithms\nmodels = []\nmodels.append(('LR', LogisticRegression(random_state=0)))\nmodels.append(('BAG', BaggingClassifier(random_state=0)))\nmodels.append(('RF', RandomForestClassifier(random_state=0)))\nmodels.append(('ADA', AdaBoostClassifier(random_state=0)))\nmodels.append(('GBM', GradientBoostingClassifier(random_state=0)))\nmodels.append(('XGB', XGBClassifier(random_state=0)))\nresults_t = []\nresults_v = []\nnames = []\nscore = []\nfor name, model in models:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid,cv=5)\n    my_model.fit(X_train, y_train)\n    predictions_t = my_model.predict(X_train) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_train, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v) \n    results_t.append(accuracy_train)\n    results_v.append(accuracy_valid)\n    names.append(name)\n    f_dict = {\n        'model': name,\n        'accuracy_train': accuracy_train,\n        'accuracy_valid': accuracy_valid,\n    }\n    score.append(f_dict)\n    \nscore = pd.DataFrame(score, columns = ['model','accuracy_train', 'accuracy_valid'])","d6ee94c9":"print(score)","a1260482":"# Spot Check Algorithms with standardized dataset\npipelines = []\npipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR', LogisticRegression(random_state=0))])))\npipelines.append(('ScaledBAG', Pipeline([('Scaler', StandardScaler()),('BAG', BaggingClassifier(random_state=0))])))\npipelines.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestClassifier(random_state=0))])))\npipelines.append(('ScaledADA', Pipeline([('Scaler', StandardScaler()),('ADA', AdaBoostClassifier(random_state=0))])))\npipelines.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingClassifier(random_state=0))])))\npipelines.append(('ScaledXGB', Pipeline([('Scaler', StandardScaler()),('XGB', XGBClassifier(random_state=0))])))\npipelines.append(('ScaledNN', Pipeline([('Scaler', StandardScaler()),('NN', MLPClassifier(random_state=0))])))\nresults_t = []\nresults_v = []\nnames = []\nscore_sd = []\nfor name, model in pipelines:\n    param_grid = {}\n    my_model = GridSearchCV(model,param_grid,cv=5)\n    my_model.fit(X_train, y_train)\n    predictions_t = my_model.predict(X_train) \n    predictions_v = my_model.predict(X_valid)\n    accuracy_train = accuracy_score(y_train, predictions_t) \n    accuracy_valid = accuracy_score(y_valid, predictions_v) \n    results_t.append(accuracy_train)\n    results_v.append(accuracy_valid)\n    names.append(name)\n    f_dict = {\n        'model': name,\n        'accuracy_train': accuracy_train,\n        'accuracy_valid': accuracy_valid,\n    }\n    score_sd.append(f_dict)\n    \nscore_sd = pd.DataFrame(score_sd, columns = ['model','accuracy_train', 'accuracy_valid'])","32230173":"print(score_sd)","520012fc":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)\n\n(pd.Series(model.feature_importances_, index=X_train.columns)\n   .nlargest(4)\n   .plot(kind='barh')).set_title(\"Random Forest\") ","6658bcdc":"model = AdaBoostClassifier()\nmodel.fit(X_train, y_train)\n\n(pd.Series(model.feature_importances_, index=X_train.columns)\n   .nlargest(4)\n   .plot(kind='barh')).set_title(\"AdaBoost\") ","e8fabdd6":"model = GradientBoostingClassifier()\nmodel.fit(X_train, y_train)\n\n(pd.Series(model.feature_importances_, index=X_train.columns)\n   .nlargest(4)\n   .plot(kind='barh')).set_title(\"Gradient Boosting\") ","8e9a27c6":"model = XGBClassifier()\nmodel.fit(X_train, y_train)\n\n(pd.Series(model.feature_importances_, index=X_train.columns)\n   .nlargest(4)\n   .plot(kind='barh')).set_title(\"XGBoost\") ","eafc6133":"# <a id='11'>11. Merge Numerical Features and Categorical Features in a new Dataframe dropping Correlated Features<\/a>","81b42c66":"- <a href='#1'>1. Prepare Workspace<\/a>\n- <a href='#2'>2. Loading Datasets<\/a>\n- <a href='#3'>3. Summarize Data<\/a>\n- <a href='#4'>4. Formatting Features<\/a>\n- <a href='#5'>5. Handling Missing Values<\/a>\n- <a href='#6'>6. Target Variable Analysis<\/a>\n- <a href='#7'>7. Categorical Features Analysis<\/a>\n- <a href='#8'>8. Numerical Features Analysis<\/a>\n- <a href='#9'>9. Handling Outliers<\/a>\n- <a href='#10'>10. Feature Engineering on Numerical Features<\/a>\n- <a href='#11'>11. Merge Numerical Features and Categorical Features in a new Dataframe dropping Correlated Features<\/a>\n- <a href='#12'>12. Split Dataset into Train, Validation and Test Datasets<\/a>\n- <a href='#13'>13. Baseline Model Selection<\/a>\n- <a href='#14'>14. Features Importance<\/a>\n\n","b4b1f8ab":"# Learn With Other Kaggle Users\n### Classify forest types based on information about the area\n\nIn this competition the goal is to predict a multilabel classification variable, 'Cover Type', made by seven label types:\n\n1 - Spruce\/Fir\n\n2 - Lodgepole Pine\n\n3 - Ponderosa Pine\n\n4 - Cottonwood\/Willow\n\n5 - Aspen\n\n6 - Douglas-fir\n\n7 - Krummholz\n\nThe training set is made up by 15.120 observations with predictor variables (55 features) and the target variable. The goal is to predict the 'Cover Type' target variable for every row in the test set (565.892 observations).\nThe evaluation of goodness of fit is based on Accuracy.\n","680b0210":"# <a id='13'>13. Baseline Model Selection<\/a>","752420f5":"Grouped numerical features into a new subset and done a graphical analysis by distribution and boxplot.\nFeature Selection has done by one-way Anova Test: no one numerical feature has been removed.\nThere are 10 numerical features.\n\n","e23a00c1":"Datasets are loaded and merged in a unique one dataset to work in an agile manner.","b4401b1f":"# <a id='5'>5. Handling Missing Values<\/a>","72442847":"# <a id='6'>6. Target Variable Analysis<\/a>","8e5045a1":"After working on subset, a new dataframe has been created deleting also correlated features.\nAt the end from 55 predictor variables the merged dataset has 57 features.  ","fdcebc67":"Grouped all categorical features (integer) into a new subset and done a graphical analysis.\nFrom the train set and test set analysis there are 2 features with different levels from each one, so is better to remove them also with 'Id' feature that is not considered helpful. \nUsed a Chi-Square Test as a Feature Selection, in this way other 2 features are been removed.\nFrom 45 initial categorical features are been left 40 categorical features.\nNeither dummy nor one-hot encoding has been done, is not necessary, features are in a binary values. \n","c2a01e93":"# <a id='1'>1. Prepare Workspace<\/a>","4f180dc7":"# <a id='10'>10. Feature Engineering on Numerical Features<\/a>","3b877d23":"# <a id='7'>7. Categorical Features Analysis<\/a>","2c124795":"# <a id='8'>8. Numerical Features Analysis<\/a>","1aedd399":"# <a id='2'>2. Loading Datasets<\/a>","efadba8a":"Numerical features are converted into 'float' format, instead categorical features are left into 'integer' format, because they have all binary values.","2b779950":"# <a id='4'>4. Formatting Features<\/a>","a7744927":"Both quantitative and qualitative analysis of the target variable.\nGood news, the dependent variable is equally distributed on all levels.","10fcdb1e":"# <a id='3'>3. Summarize Data<\/a>","6704956e":"# <a id='9'>9. Handling Outliers<\/a>","fc6feef9":"# <a id='12'>12. Split Dataset into Train, Validation and Test Datasets<\/a>","61419c41":"From 10 numerical features 8 of them have outliers. They are been removed with the IQR rule.","0251ab9a":"# <a id='14'>14. Features Importance<\/a>","5d4eea8c":"![](https:\/\/www.viaggi-lowcost.info\/wp-content\/uploads\/2013\/03\/foresta-nera.jpg)","47e1dcf7":"Feature engineering has been applied on numerical features looking to build non-linear features.","3924b66d":"The whole dataset is splitted into both categorical and numerical features to do a better analysis."}}