{"cell_type":{"d9b3c61b":"code","252a0ebb":"code","7a816cc2":"code","67bc2c88":"code","37835e61":"code","2d3fbe31":"code","7f1b9c15":"code","2a9ad8fe":"code","a6122d60":"code","05e28383":"code","ee14b06d":"code","8d08ffa6":"markdown","e62cadb0":"markdown","5ed08b3e":"markdown","78cfe965":"markdown","ff2f1350":"markdown","ad913643":"markdown","a3f2baf5":"markdown","36b44fda":"markdown","69bd4727":"markdown","bde53ee4":"markdown","b36df7b3":"markdown"},"source":{"d9b3c61b":"import pandas as pd\ntrain = pd.read_csv('..\/input\/train_batch.csv')\ntest = pd.read_csv('..\/input\/test_batch.csv')\nprint('train shape:', train.shape)\nprint('test shape:', test.shape)\n\ntrain.head()","252a0ebb":"import numpy as np\nimport tensorflow as tf\nfrom sklearn.utils import shuffle\nimport pandas as pd\nfrom math import ceil\nimport logging\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n\n# L\u1ea5y s\u1ed1 l\u01b0\u1ee3ng classes\ndef get_num_classes(data_path):\n    return len(pd.read_csv(data_path, header = None, usecols = [0])[0].unique())","7a816cc2":"def create_dataset(data_path, alphabet=\"\"\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"\/\\\\|_@#$%^&*~`+-=<>()[]{}\"\"\",\n                   max_length=1014, batch_size=128, is_training=True):\n    label_converter = lambda x: int(x) - 1\n    data = pd.read_csv(data_path, header = None, converters = {0: label_converter})\n    num_iter = ceil(data.shape[0] \/ batch_size)\n    if is_training:\n        data = shuffle(data, random_state = 42)\n    num_columns = data.shape[1]\n\n    for idx in range(2, num_columns):\n        # collect whole data comment into column 1\n        data[1] += data[idx]\n    # drop another column of data except 0, 1 and convert data into numpy\n    data = data.drop([idx for idx in range(2, num_columns)], axis = 1).values\n    alphabet = list(alphabet)\n    # create the indentity matrice\n    identity_mat = np.identity(len(alphabet))\n    \n    def generator():\n        for row in data:\n            label, text = row\n            text = np.array([identity_mat[alphabet.index(i)] for i in list(str(text)) if i in alphabet], dtype = np.float32)\n            if len(text) > max_length:\n                text = text[:max_length]\n            elif 0 < len(text) < max_length:\n                text = np.concatenate((text, np.zeros((max_length-len(text), len(alphabet)), dtype = np.float32)))\n            elif len(text) == 0:\n                text = np.zeros((max_length, len(alphabet)), dtype = np.float32)\n            # yield the embedding matrix at characters level\n            # height is number of basic characters set (68)\n            # width is maximum_length_characters in sentence (300)\n            yield text.T, label\n    return tf.data.Dataset.from_generator(generator, \\\n                                          (tf.float32, tf.int32), \\\n                                          ((len(alphabet), max_length), (None))).batch(batch_size), num_iter","67bc2c88":"alphabet = \"\"\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"\/\\\\|_@#$%^&*~`+-=<>()[]{}\"\"\"\ntrain_path = '..\/input\/train_batch.csv'\ntest_path = '..\/input\/test_batch.csv'\ntest_interval = 1\nmax_length = 1014\nfeature = 'small'\nbatch_size = 128\nnum_epochs = 2\nlr = 1e-2\noptimizer = 'sgd'\ndropout = 0.5\nlog_path = 'tensorboard_khanh\/char_level_cnn'\nsaved_path = 'trained_models_khanh'\nes_min_delta = 0.\nes_patience = 3\nallow_soft_placement = True\nlog_device_placement = False","37835e61":"class Char_level_cnn():\n    def __init__(self, batch_size = 128, num_classes = 14, feature = 'small',\n                 kernel_size = [7, 7, 3, 3, 3, 3], padding = 'VALID'):\n        # with python version 2\n        # supper(char_level_cnn, self).__init__()\n        self.batch_size = batch_size\n        self.num_classes = num_classes\n        if feature == 'small':\n            self.num_filters = 256\n            self.stddev_initialization = 0.05\n            self.num_fully_connected_features = 1024\n        else:\n            self.num_filters = 1024\n            self.stddev_initialization = 0.02\n            self.num_fully_connected_features = 2048\n        self.kernel_size = kernel_size\n        self.padding = padding\n        \n    def forward(self, input, keep_prob):\n        # expand dims in the deepest inside of tensor to transform shape from 3 to 4. \n        # For example from shape (3, 3, 4) to (3, 3, 4, 1)\n        output = tf.expand_dims(input, -1)\n        logging.info('input shape: {}'.format(output.get_shape()))\n        output = self._create_conv(output, \n                                   [output.get_shape().as_list()[1], self.kernel_size[0], 1, self.num_filters],\n                                   'conv1', 3)\n        logging.info('output shape 1: {}'.format(output.get_shape()))\n        # shape below are the shape of weight matrix, which have heights equal 1 and lengths equal kernel size. \n        # This mean that convolution product only impose on the length of each sentence regardless of characters dimension.\n        output = self._create_conv(output, [1, self.kernel_size[1], self.num_filters, self.num_filters], 'conv2', 3)\n        logging.info('output shape 2: {}'.format(output.get_shape()))\n        output = self._create_conv(output, [1, self.kernel_size[2], self.num_filters, self.num_filters], 'conv3')\n        logging.info('output shape 3: {}'.format(output.get_shape()))\n        output = self._create_conv(output, [1, self.kernel_size[3], self.num_filters, self.num_filters], 'conv4')\n        logging.info('output shape 4: {}'.format(output.get_shape()))\n        output = self._create_conv(output, [1, self.kernel_size[4], self.num_filters, self.num_filters], 'conv5')\n        logging.info('output shape 5: {}'.format(output.get_shape()))\n        output = self._create_conv(output, [1, self.kernel_size[5], self.num_filters, self.num_filters], 'conv6', 3)\n        logging.info('output shape 6: {}'.format(output.get_shape()))\n        new_feature_size = int(self.num_filters*((input.get_shape().as_list()[2] - 96)\/27))\n        flatten = tf.reshape(output,[-1, new_feature_size])\n        \n        output = self._create_fc(flatten, [new_feature_size, self.num_fully_connected_features], 'fc1', keep_prob)\n        logging.info('output shape 7: {}'.format(output.get_shape()))\n        output = self._create_fc(output, [self.num_fully_connected_features, self.num_fully_connected_features], 'fc2',\n                                 keep_prob)\n        logging.info('output shape 8: {}'.format(output.get_shape()))\n        output = self._create_fc(output, [self.num_fully_connected_features, self.num_classes], 'fc3')\n        logging.info('output shape 9: {}'.format(output.get_shape()))\n        return output\n    \n    def _create_conv(self, input, shape, name_scope, pool_size = None):\n        '''\n        shape: shape of weight\n        name_scope: name of layer\n        '''\n        with tf.name_scope(name_scope):\n            weight = self._initialize_weight(shape, self.stddev_initialization)\n            bias = self._initialize_bias([shape[-1]])\n            # shape NHWC\n            # input shape: [batch, in_height, in_width, in_channel]\n            # filter shape: [f_height, f_width, in_channel, out_channel]\n            # stride shape: [1, stride, stride, 1]\n            # output: [batch, out_height, out_width, out_channel]\n            conv = tf.nn.conv2d(input=input, filter=weight, strides=[1, 1, 1, 1], padding=self.padding, name='conv')\n            # tf.nn.bias_add will plus bias into whole element of ouput conv2d.\n            activation = tf.nn.relu(tf.nn.bias_add(conv, bias), name=\"relu\")\n            if pool_size:\n                return tf.nn.max_pool(value=activation, ksize=[1, 1, pool_size, 1], strides=[1, 1, pool_size, 1],\n                                      padding=self.padding, name='maxpool')\n            else:\n                return activation\n        \n    def _create_fc(self, input, shape, name_scope, keep_prob = None):\n        with tf.name_scope(name_scope):\n            weight = self._initialize_weight(shape, self.stddev_initialization)\n            bias = self._initialize_bias([shape[-1]])\n            dense = tf.nn.bias_add(tf.matmul(input, weight), bias, name=\"dense\")\n            if keep_prob is not None:\n                return tf.nn.dropout(dense, keep_prob, name=\"dropout\")\n            else:\n                return dense\n            \n    def _initialize_weight(self, shape, stddev):\n        return tf.Variable(tf.truncated_normal(shape = shape, stddev = stddev, dtype = tf.float32, name = 'weight'))\n    \n    def _initialize_bias(self, shape):\n        return tf.Variable(tf.constant(0, shape = shape, dtype = tf.float32, name = 'bias'))\n    \n    def loss(self, logits, labels):\n        return tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n    \n    def accuracy(self, logits, labels):\n        return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(labels, tf.int64)), dtype = tf.float32))\n    \n    def confusion_matrix(self, logits, labels):\n        return tf.confusion_matrix(tf.cast(labels, tf.int64), tf.argmax(logits, 1), num_classes = self.num_classes)","2d3fbe31":"from math import pow\nimport shutil\nimport os\nimport numpy as np\nimport logging\nimport sys\nfrom functools import reduce\n\n# Create logger\nlogger = logging.getLogger()\nlogger.setLevel(logging.DEBUG)\n# Create STDERR handler\nhandler = logging.StreamHandler(sys.stderr)\n# Create formatter and add it to the handler\nformatter = logging.Formatter('%(asctime)s : %(name)s: %(levelname)s : %(message)s')\nhandler.setFormatter(formatter)\n# Set STDERR handler as the only handler \nlogger.handlers = [handler]\n\ndef classifier(model, optimizer):\n    with tf.Graph().as_default():\n        batch_size = 128\n        session_conf = tf.ConfigProto(\n            allow_soft_placement = allow_soft_placement, \n            log_device_placement = log_device_placement\n        )\n\n        session_conf.gpu_options.allow_growth = True\n        training_set, num_training_iters = create_dataset(train_path, alphabet, max_length, \n                                                         batch_size, True)\n        test_set, num_test_iters = create_dataset(test_path, alphabet, \n                                                  max_length, batch_size, False)\n        train_iterator = training_set.make_initializable_iterator()\n        test_iterator = test_set.make_initializable_iterator()\n\n        handle = tf.placeholder(tf.string, shape = [])\n        keep_prob = tf.placeholder(tf.float32, name = 'dropout_prob')\n\n        iterator = tf.data.Iterator.from_string_handle(handle, training_set.output_types, training_set.output_shapes)\n        texts, labels = iterator.get_next()\n\n        logits = model.forward(texts, keep_prob)\n        loss = model.loss(logits, labels)\n        # record loss into summary\n        loss_summary = tf.summary.scalar('loss', loss)\n        # record accuracy into summary\n        accuracy = model.accuracy(logits, labels)\n        accuracy_summary = tf.summary.scalar('accuracy', accuracy)\n        # unstack to convert tensor rank R into rank R-1 depend on order is declared in axis\n        batch_size = tf.unstack(tf.shape(texts))[0]\n        confusion = model.confusion_matrix(logits, labels)\n        global_step = tf.Variable(0, name='global_step', trainable = False)\n\n        if optimizer == 'sgd':\n            values = [lr]\n            boundaries = []\n            for i in range(1, 10):\n                # decrease learning rate twice time in each boundary steps which show off in tf.train.piecewise_constant()\n                values.append(lr\/pow(2, i))\n                # num_training_iters = number of records in dataset\/batch_size\n                boundaries.append(3 * num_training_iters * i)\n            # create constant learning rate at boundaries and values.\n            learning_rate = tf.train.piecewise_constant(global_step, boundaries, values)\n            # add momentum for learning_rate\n            optimizer = tf.train.MomentumOptimizer(learning_rate, momentum = 0.9)\n        else:\n            optimizer = tf.train.AdamOptimizer(lr)\n\n        train_op = optimizer.minimize(loss, global_step = global_step)\n        merged = tf.summary.merge([loss_summary, accuracy_summary])\n        # init whole global variable\n        init = tf.global_variables_initializer()\n        # save training process\n        saver = tf.train.Saver()\n        if os.path.isdir(log_path):\n            shutil.rmtree(log_path)\n        os.makedirs(log_path)\n        if os.path.isdir(saved_path):\n            shutil.rmtree(saved_path)\n        os.makedirs(saved_path)\n        output_file = open(saved_path + os.sep + \"logs.txt\", \"w\")\n        output_file.write(\"Model's parameters: {}\".format('FLAG values dict'))\n        best_loss = 1e5\n        best_epoch = 0\n\n        # create session\n        with tf.Session(config = session_conf) as sess:\n            # write into tensorboard\n            train_writer = tf.summary.FileWriter(log_path + os.sep + 'train', sess.graph)\n            test_writer = tf.summary.FileWriter(log_path + os.sep + 'test')\n            sess.run(init)\n            for epoch in range(num_epochs):\n                sess.run(train_iterator.initializer)\n                sess.run(test_iterator.initializer)\n                train_handle = sess.run(train_iterator.string_handle())\n                test_handle = sess.run(test_iterator.string_handle())\n                train_iter = 0\n                while True:\n                    try:\n                        _, tr_loss, tr_accuracy, summary, step = sess.run(\n                            [train_op, loss, accuracy, merged, global_step],\n                            feed_dict={handle: train_handle, keep_prob: dropout})\n                        print(\"Epoch: {}\/{}, Iteration: {}\/{}, Loss: {}, Accuracy: {}\".format(\n                                epoch + 1,\n                                num_epochs,\n                                train_iter + 1,\n                                num_training_iters,\n                                tr_loss, tr_accuracy))\n                        train_writer.add_summary(summary, step)\n                        train_iter += 1\n                    except (tf.errors.OutOfRangeError, StopIteration):\n                        break\n\n                if epoch % test_interval == 0:\n                    loss_ls = []\n                    loss_summary = tf.Summary()\n                    accuracy_ls = []\n                    accuracy_summary = tf.Summary()\n                    confusion_matrix = np.zeros([num_classes, num_classes], np.int32)\n                    num_samples = 0\n                    while True:\n                        try:\n                            test_loss, test_accuracy, test_confusion, samples = sess.run(\n                                [loss, accuracy, confusion, batch_size],\n                                feed_dict={handle: test_handle, keep_prob: 1.0})\n                            # test_loss are mean loss of each batch_size\n                            loss_ls.append(test_loss * samples)\n                            accuracy_ls.append(test_accuracy * samples)\n                            confusion_matrix += test_confusion\n                            num_samples += samples\n                        except (tf.errors.OutOfRangeError, StopIteration):\n                            break\n\n                    mean_test_loss = sum(loss_ls) \/ num_samples\n                    # add mean_test_loss into loss in summary log\n                    loss_summary.value.add(tag='loss', simple_value=mean_test_loss)\n                    test_writer.add_summary(loss_summary, epoch)\n                    mean_test_accuracy = sum(accuracy_ls) \/ num_samples\n                    # add mean_test_accuracy into accuracy in summary log\n                    accuracy_summary.value.add(tag='accuracy', simple_value=mean_test_accuracy)\n                    test_writer.add_summary(accuracy_summary, epoch)\n\n                    # write into ouput_file logs\n                    output_file.write(\n                            \"Epoch: {}\/{} \\nTest loss: {} Test accuracy: {} \\nTest confusion matrix: \\n{}\\n\\n\".format(\n                                epoch + 1, num_epochs,\n                                mean_test_loss,\n                                mean_test_accuracy,\n                                confusion_matrix))\n\n                    print(\"Epoch: {}\/{}, Final loss: {}, Final accuracy: {}\".format(epoch + 1, num_epochs,\n                                                                                    mean_test_loss,\n                                                                                    mean_test_accuracy))\n\n                    # save model at best epoch in case it gain best loss\n                    if mean_test_loss + es_min_delta < best_loss:\n                        best_loss = mean_test_loss\n                        best_epoch = epoch\n                        saver.save(sess, saved_path + os.sep + \"char_level_cnn\")\n\n                    # stop training model when number of epoch exceed the best_epoch\n                    if epoch - best_epoch > es_patience > 0:\n                        print(\"Stop training at epoch {}. The lowest loss achieved is {}\".format(epoch, best_loss))\n                        break\n\n            output_file.close()","7f1b9c15":"num_classes = get_num_classes('..\/input\/test_batch.csv')\nmodel = Char_level_cnn(batch_size = batch_size, num_classes = num_classes, feature = feature)\nclassifier(model, optimizer = optimizer)","2a9ad8fe":"class char_level_cnn_est():\n    def __init__(self, num_classes = 14, feature = 'small',\n                 kernel_size = [7, 7, 3, 3, 3, 3], padding = 'valid', keep_prob = [0.5, 0.5]):\n            self.input = input\n            self.num_classes = num_classes\n            self.feature = feature\n            self.kernel_size = kernel_size\n            self.padding = padding\n            if self.feature == 'small':\n                self.num_filters = 256\n                self.num_fully_connected_features = 1024\n            else:\n                self.num_filters = 1024\n                self.num_fully_connected_features = 2048\n            self.keep_prob = keep_prob\n            \n                \n    def forward(self, input, keep_prob):\n        input_layer = tf.expand_dims(input, -1)\n        logging.info('input shape {}'.format(input_layer.get_shape()))\n        \n        conv1 = tf.layers.conv2d(\n            inputs = input_layer,\n            filters = 32, \n            kernel_size = [input_layer.get_shape().as_list()[1], self.kernel_size[0]],\n            strides = [1, 1],\n            padding = 'valid',\n            activation = tf.nn.relu,\n            name = 'conv1'\n        )\n        logging.info('conv1 shape {}'.format(conv1.get_shape()))\n        \n        max_pool1 = tf.layers.max_pooling2d(\n            inputs = conv1,\n            pool_size = [1, 3],\n            strides = [1, 3],\n            name = 'max_pool1'\n        )\n        logging.info('maxpool1 shape {}'.format(max_pool1.get_shape()))\n        \n        conv2 = tf.layers.conv2d(\n            inputs = max_pool1,\n            filters = self.num_filters,\n            kernel_size = [1, self.kernel_size[1]],\n            strides = [1, 1],\n            padding = 'valid',\n            activation = tf.nn.relu,\n            name = 'conv2'\n        )\n        logging.info('conv2 shape {}'.format(conv2.get_shape()))\n        \n        max_pool2 = tf.layers.max_pooling2d(\n            inputs = conv2,\n            pool_size = [1, 3],\n            strides = [1, 3],\n            name = 'max_pool2'\n        )\n        logging.info('maxpool2 shape {}'.format(max_pool2.get_shape()))\n        \n        conv3 = tf.layers.conv2d(\n            inputs = max_pool2,\n            filters = self.num_filters,\n            kernel_size = [1, self.kernel_size[2]],\n            strides = [1, 1],\n            padding = 'valid',\n            activation = tf.nn.relu,\n            name = 'conv3'\n        )\n        logging.info('conv3 shape {}'.format(conv3.get_shape()))\n        \n        conv4 = tf.layers.conv2d(\n            inputs = conv3,\n            filters = self.num_filters,\n            kernel_size = [1, self.kernel_size[3]],\n            strides = [1, 1],\n            padding = 'valid',\n            activation = tf.nn.relu,\n            name = 'conv4'\n        )\n        logging.info('conv4 shape {}'.format(conv4.get_shape()))\n        \n        conv5 = tf.layers.conv2d(\n            inputs = conv4,\n            filters = self.num_filters,\n            kernel_size = [1, self.kernel_size[4]],\n            strides = [1, 1],\n            padding = 'valid',\n            activation = tf.nn.relu,\n            name = 'conv5'\n        )\n        logging.info('conv5 shape {}'.format(conv5.get_shape()))\n        \n        conv6 = tf.layers.conv2d(\n            inputs = conv5,\n            filters = self.num_filters,\n            kernel_size = [1, self.kernel_size[5]],\n            strides = [1, 1],\n            padding = 'valid',\n            activation = tf.nn.relu,\n            name = 'conv6'\n        )\n        logging.info('conv6 shape {}'.format(conv6.get_shape()))\n        \n        max_pool3 = tf.layers.max_pooling2d(\n            inputs = conv6,\n            pool_size = [1, 3],\n            strides = [1, 3],\n            name = 'max_pool3'\n        )\n        logging.info('maxpool3 shape {}'.format(max_pool3.get_shape()))\n        \n        shape = max_pool3.get_shape().as_list()\n        # Calculate the shape size when flat tensor into vector\n        new_size = reduce(lambda x, y: x*y, shape[1:]) \n        flat = tf.reshape(max_pool3, [-1, new_size])\n        logging.info('flat shape {}'.format(flat.get_shape()))\n        \n        fc1 = tf.layers.dense(\n            inputs = flat,\n            units = self.num_fully_connected_features,\n            name = 'fc1'\n        )\n        \n        logging.info('fc1 shape {}'.format(fc1.get_shape()))\n        if keep_prob is not None:\n            fc1 = tf.layers.dropout(\n                inputs = fc1, \n                rate = self.keep_prob[0],\n                name = 'drop1'\n            )\n        \n        fc2 = tf.layers.dense(\n            inputs = fc1,\n            units = self.num_fully_connected_features,\n            name = 'fc2'\n        )\n        logging.info('fc2 shape {}'.format(fc2.get_shape()))\n        \n        if keep_prob is not None:\n            fc2 = tf.layers.dropout(\n                inputs = fc2, \n                rate = self.keep_prob[1],\n                name = 'drop2'\n            )\n        \n        logits = tf.layers.dense(\n            inputs = fc2,\n            units = self.num_classes,\n            name = 'ouput'\n        )\n        logging.info('logits shape {}'.format(logits.get_shape()))\n        \n        return logits\n    \n    def loss(self, logits, labels):\n        return tf.losses.sparse_softmax_cross_entropy(logits = logits, labels = labels)\n    \n    def accuracy(self, logits, labels):\n        return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(labels, tf.int64)), dtype = tf.float32))\n    \n    def confusion_matrix(self, logits, labels):\n        return tf.confusion_matrix(tf.cast(labels, tf.int64), tf.argmax(logits, 1), num_classes = self.num_classes)","a6122d60":"num_classes = get_num_classes('..\/input\/test_batch.csv')\nmodel2 = char_level_cnn_est(num_classes = num_classes, feature = feature)\nclassifier(model2, optimizer = optimizer)","05e28383":"from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Reshape\n\nclass char_level_cnn_keras():\n    def __init__(self, num_classes = 14, feature = 'small',\n                 kernel_size = [7, 7, 3, 3, 3, 3], padding = 'valid', keep_prob = [0.5, 0.5]):\n            self.input = input\n            self.num_classes = num_classes\n            self.feature = feature\n            self.kernel_size = kernel_size\n            self.padding = padding\n            if self.feature == 'small':\n                self.num_filters = 256\n                self.num_fully_connected_features = 1024\n            else:\n                self.num_filters = 1024\n                self.num_fully_connected_features = 2048\n            self.keep_prob = keep_prob\n            \n                \n    def forward(self, input, keep_prob):\n        input_layer = tf.expand_dims(input, -1)\n        logging.info('input shape {}'.format(input_layer.get_shape()))\n        \n        conv1 = Conv2D(\n            filters = 32, \n            kernel_size = [input_layer.get_shape().as_list()[1], self.kernel_size[0]],\n            strides = [1, 1],\n            padding = 'valid',\n            activation = 'relu',\n            name = 'conv1'\n        )(input_layer)\n        \n        logging.info('conv1 shape {}'.format(conv1.get_shape()))\n        \n        max_pool1 = MaxPooling2D(\n            pool_size = [1, 3],\n            strides = [1, 3],\n            name = 'max_pool1'\n        )(conv1)\n        logging.info('maxpool1 shape {}'.format(max_pool1.get_shape()))\n        \n        conv2 = Conv2D(\n            filters = self.num_filters,\n            kernel_size = [1, self.kernel_size[1]],\n            strides = [1, 1],\n            padding = 'valid',\n            activation = tf.nn.relu,\n            name = 'conv2'\n        )(max_pool1)\n        logging.info('conv2 shape {}'.format(conv2.get_shape()))\n        \n        max_pool2 = MaxPooling2D(\n            pool_size = [1, 3],\n            strides = [1, 3],\n            name = 'max_pool2'\n        )(conv2)\n        logging.info('maxpool2 shape {}'.format(max_pool2.get_shape()))\n        \n        conv3 = Conv2D(\n            filters = self.num_filters,\n            kernel_size = [1, self.kernel_size[2]],\n            strides = [1, 1],\n            padding = 'valid',\n            activation = tf.nn.relu,\n            name = 'conv3'\n        )(max_pool2)\n        logging.info('conv3 shape {}'.format(conv3.get_shape()))\n        \n        conv4 = Conv2D(\n            filters = self.num_filters,\n            kernel_size = [1, self.kernel_size[3]],\n            strides = [1, 1],\n            padding = 'valid',\n            activation = tf.nn.relu,\n            name = 'conv4'\n        )(conv3)\n        logging.info('conv4 shape {}'.format(conv4.get_shape()))\n        \n        conv5 = Conv2D(\n            filters = self.num_filters,\n            kernel_size = [1, self.kernel_size[4]],\n            strides = [1, 1],\n            padding = 'valid',\n            activation = tf.nn.relu,\n            name = 'conv5'\n        )(conv4)\n        logging.info('conv5 shape {}'.format(conv5.get_shape()))\n        \n        conv6 = Conv2D(\n            filters = self.num_filters,\n            kernel_size = [1, self.kernel_size[5]],\n            strides = [1, 1],\n            padding = 'valid',\n            activation = tf.nn.relu,\n            name = 'conv6'\n        )(conv5)\n        logging.info('conv6 shape {}'.format(conv6.get_shape()))\n        \n        max_pool3 = MaxPooling2D(\n            pool_size = [1, 3],\n            strides = [1, 3],\n            name = 'max_pool3'\n        )(conv6)\n        logging.info('maxpool3 shape {}'.format(max_pool3.get_shape()))\n        \n        shape = max_pool3.get_shape().as_list()\n        # Calculate the shape size when flat tensor into vector\n        new_size = reduce(lambda x, y: x*y, shape[1:]) \n        flat = Reshape([new_size])(max_pool3)\n        logging.info('flat shape {}'.format(flat.get_shape()))\n        \n        fc1 = Dense(\n            units = self.num_fully_connected_features,\n            name = 'fc1'\n        )(flat)\n        \n        logging.info('fc1 shape {}'.format(fc1.get_shape()))\n        if keep_prob is not None:\n            fc1 = Dropout(\n                rate = self.keep_prob[0],\n                name = 'drop1'\n            )(fc1)\n        \n        fc2 = Dense(\n            units = self.num_fully_connected_features,\n            name = 'fc2'\n        )(fc1)\n        logging.info('fc2 shape {}'.format(fc2.get_shape()))\n        \n        if keep_prob is not None:\n            fc2 = Dropout(\n                rate = self.keep_prob[1],\n                name = 'drop2'\n            )(fc2)\n        \n        logits = Dense(\n            units = self.num_classes,\n            name = 'ouput'\n        )(fc2)\n        logging.info('logits shape {}'.format(logits.get_shape()))\n        \n        return logits\n    \n    def loss(self, logits, labels):\n        return tf.losses.sparse_softmax_cross_entropy(logits = logits, labels = labels)\n    \n    def accuracy(self, logits, labels):\n        return tf.reduce_mean(tf.cast(tf.equal(tf.argmax(logits, 1), tf.cast(labels, tf.int64)), dtype = tf.float32))\n    \n    def confusion_matrix(self, logits, labels):\n        return tf.confusion_matrix(tf.cast(labels, tf.int64), tf.argmax(logits, 1), num_classes = self.num_classes)","ee14b06d":"num_classes = get_num_classes('..\/input\/test_batch.csv')\nmodel3 = char_level_cnn_keras(num_classes = num_classes, feature = feature)\nclassifier(model3, optimizer = optimizer)","8d08ffa6":"Hu\u1ea5n luy\u1ec7n model","e62cadb0":"X\u00e2y d\u1ef1ng h\u00e0m `classifier()` hu\u1ea5n luy\u1ec7n m\u00f4 h\u00ecnh.","5ed08b3e":"H\u00e0m `create_dataset()` s\u1ebd tr\u1ea3 v\u1ec1 m\u1ed9t generator gi\u00fap kh\u1edfi t\u1ea1o c\u00e1c iterator c\u1ee7a dataset c\u00f3 s\u1ed1 quan s\u00e1t theo batch_size \u0111\u01b0\u1ee3c feed v\u00e0o model trong m\u1ed7i b\u01b0\u1edbc training.\n\nKhai b\u00e1o c\u00e1c bi\u1ebfn ch\u00ednh s\u1eed d\u1ee5ng cho to\u00e0n b\u1ed9 3 model.","78cfe965":"# 1. Gi\u1edbi thi\u1ec7u\n\nPh\u00e2n lo\u1ea1i v\u0103n b\u1ea3n theo ch\u1ee7 \u0111\u1ec1 l\u00e0 m\u1ed9t trong nh\u1eefng b\u00e0i to\u00e1n quen thu\u1ed9c trong x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean. C\u00e1c ph\u01b0\u01a1ng ph\u00e1p s\u1eed d\u1ee5ng trong ph\u00e2n lo\u1ea1i v\u0103n b\u1ea3n c\u0169ng kh\u00e1 \u0111a d\u1ea1ng. Ch\u00fang ta c\u00f3 th\u1ec3 tham kh\u1ea3o m\u1ed9t s\u1ed1 b\u00e0i vi\u1ebft:\n\n1. [Ph\u00e2n lo\u1ea1i v\u0103n b\u1ea3n s\u1eed d\u1ee5ng word-embedding k\u1ebft h\u1ee3p v\u1edbi m\u00f4 h\u00ecnh LTSM - T\u00e1c gi\u1ea3 Kh\u00e1nh Ph\u1ea1m \u0110\u00ecnh](https:\/\/www.kaggle.com\/phamdinhkhanh\/simple-lstm-for-text-classification-spam-email)\n2. [Ph\u00e2n lo\u1ea1i v\u0103n b\u1ea3n s\u1eed d\u1ee5ng word-embedding v\u00e0 linear classifier - T\u00e1c gi\u1ea3 To\u00e0n Ph\u1ea1m](https:\/\/viblo.asia\/p\/phan-loai-van-ban-tieng-viet-tu-dong-phan-1-yMnKM3bal7P?fbclid=IwAR1XSo1J92rliPasQHEjjKIIP4K8jyYEpmuq8rA9cj5dv14QzYE_qwLl4tA)\n\nNh\u1eefng m\u00f4 h\u00ecnh n\u00e0y \u0111\u1ec1u c\u00f3 m\u1ed9t \u0111i\u1ec3m chung l\u00e0 th\u1ef1c hi\u1ec7n l\u01b0\u1ee3ng h\u00f3a v\u0103n b\u1ea3n tr\u00ean c\u01a1 s\u1edf t\u1eeb v\u1ef1ng \u0111\u1ec3 t\u1ea1o th\u00e0nh c\u00e1c embedding vector \u0111\u01b0a v\u00e0o model. G\u1ea7n \u0111\u00e2y, d\u1ef1a tr\u00ean source code \u0111\u01b0\u1ee3c chia s\u1ebb b\u1edfi t\u00e1c gi\u1ea3 Vi\u1ec7t Nguy\u1ec5n v\u1ec1 \u1ee9ng d\u1ee5ng c\u1ee7a [ph\u00e2n lo\u1ea1i v\u0103n b\u1ea3n d\u1ef1a tr\u00ean character level](https:\/\/www.facebook.com\/groups\/1601966719912937\/permalink\/1961794677263471\/) k\u1ebft h\u1ee3p v\u1edbi ph\u01b0\u01a1ng ph\u00e1p t\u1eeb b\u00e0i b\u00e1o g\u1ed1c [Character-level Convolutional Networks for Text Classification](https:\/\/arxiv.org\/pdf\/1509.01626.pdf?fbclid=IwAR0swq-SqULXfBCRSpMV1jdLZ5-GAQ0aHAaTFR0ljWbFeY948i2uaDW6Ojc) c\u1ee7a nh\u00f3m t\u00e1c gi\u1ea3 Yann LeCun, Junbo Zhao, Xiang Zhang \u0111\u01b0\u1ee3c vi\u1ebft t\u1eeb April 2016, m\u00ecnh \u0111\u00e3 n\u1eafm b\u1eaft th\u00eam \u0111\u01b0\u1ee3c m\u1ed9t h\u01b0\u1edbng ti\u1ebfp c\u1eadn m\u1edbi cho b\u00e0i to\u00e1n ph\u00e2n lo\u1ea1i v\u0103n b\u1ea3n. Ph\u01b0\u01a1ng ph\u00e1p \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong b\u00e0i b\u00e1o c\u00f3 2 \u0111i\u1ec3m \u0111\u1eb7c bi\u1ec7t kh\u00e1c v\u1edbi c\u00e1c model ph\u00e2n lo\u1ea1i v\u0103n b\u1ea3n th\u00f4ng th\u01b0\u1eddng \u0111\u00f3 l\u00e0: T\u00e1c gi\u1ea3 d\u1ef1a tr\u00ean character-level t\u1ee9c l\u00e0 c\u00e1c k\u00ed t\u1ef1 ch\u1eef c\u00e1i, ch\u1eef s\u1ed1 v\u00e0 d\u1ea5u c\u00e2u thay v\u00ec c\u00e1c t\u1eeb v\u1ef1ng; T\u00e1c gi\u1ea3 s\u1eed d\u1ee5ng ki\u1ebfn tr\u00fac Convolutional Neural Networks thay v\u00ec Recurrent Neural Networks nh\u01b0 th\u00f4ng th\u01b0\u1eddng. R\u00f5 r\u00e0ng \u0111\u1ed1i v\u1edbi ph\u01b0\u01a1ng ph\u00e1p s\u1eed d\u1ee5ng character-level ch\u00fang ta s\u1ebd ti\u1ebft ki\u1ec7m so v\u1edbi word-embedding \u0111\u00f3 l\u00e0 kh\u00f4ng ph\u1ea3i x\u00e2y d\u1ef1ng m\u1ed9t b\u1ed9 t\u1eeb \u0111i\u1ec3n. H\u01a1n n\u1eefa s\u1ed1 l\u01b0\u1ee3ng character-level ch\u1ec9 bao g\u1ed3m 68 k\u00ed t\u1ef1 v\u00e0 nh\u1ecf h\u01a1n r\u1ea5t nhi\u1ec1u so v\u1edbi m\u1ed9t b\u1ed9 t\u1eeb \u0111i\u1ec3n c\u00f3 th\u1ec3 l\u00ean t\u1edbi h\u00e0ng ch\u1ee5c ng\u00e0n t\u1eeb v\u1ef1ng. C\u1ea3 hai ph\u01b0\u01a1ng ph\u00e1p \u0111\u1ec1u c\u00f3 th\u1ec3 l\u01b0\u1ee3ng h\u00f3a m\u1ed9t \u0111o\u1ea1n v\u0103n th\u00e0nh nh\u1eefng ma tr\u1eadn s\u1ed1 nh\u01b0ng character-level s\u1ebd c\u00f3 s\u1ed1 chi\u1ec1u th\u1ea5p h\u01a1n so v\u1edbi word-level. Trong qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u00ean \u0111\u00e2y s\u1ebd l\u00e0 m\u1ed9t l\u1ee3i th\u1ebf \u0111\u00e1ng k\u1ec3 c\u1ee7a character-level. B\u00ean c\u1ea1nh \u0111\u00f3 Convolutional Neural Networks th\u00f4ng th\u01b0\u1eddng \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong x\u1eed l\u00fd \u1ea3nh d\u1ef1a tr\u00ean n\u1ec1n t\u1ea3ng l\u00e0 c\u00e1c ph\u00e9p t\u00edch ch\u1eadp m\u00e0 \u00edt khi \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong x\u1eed l\u00fd ng\u00f4n ng\u1eef t\u1ef1 nhi\u00ean. Tuy nhi\u00ean m\u1ea1ng Convolution Neural Networks d\u1ef1a tr\u00ean t\u00ednh ch\u1ea5t chia s\u1ebb tham s\u1ed1 v\u00e0 k\u1ebft n\u1ed1i \u0111\u1ecba ph\u01b0\u01a1ng t\u1edbi c\u00e1c v\u00f9ng \u1ea3nh \u0111\u1ec3 t\u00ecm ra c\u00e1c \u0111\u1eb7c tr\u01b0ng ch\u00ednh c\u1ee7a d\u1eef li\u1ec7u nh\u1eb1m ph\u00e2n lo\u1ea1i ch\u00fang. \u0110\u1ed1i v\u1edbi v\u0103n b\u1ea3n, ta c\u0169ng ho\u00e0n to\u00e0n c\u00f3 th\u1ec3 d\u1ef1a v\u00e0o nh\u1eefng \u00fd ngh\u0129a th\u1ec3 hi\u1ec7n qua nh\u1eefng t\u1eeb ng\u1eef ho\u1eb7c c\u00e2u v\u0103n \u0111\u1ec3 tr\u00edch l\u1ecdc \u0111\u1eb7c tr\u01b0ng. Ch\u1eb3ng h\u1ea1n \u0111\u1ed1i v\u1edbi ch\u1ee7 \u0111\u1ec1 ph\u00e2n lo\u1ea1i b\u00ecnh lu\u1eadn ti\u00eau c\u1ef1c, t\u00edch c\u1ef1c c\u0103n c\u1ee9 v\u00e0o c\u00e1c nh\u00f3m t\u1eeb li\u00ean quan \u0111\u1ebfn c\u1ea3m x\u00fac nh\u01b0: `nh\u00e0m, ch\u00e1n, nh\u1ea3m, phim qu\u00e1 nh\u1ea1t` ho\u1eb7c `hay, tuy\u1ec7t v\u1eddi, tuy\u1ec7t c\u00fa m\u00e8o` \u0111\u1ec3 ph\u00e2n lo\u1ea1i. R\u00f5 r\u00e0ng, \u00fd t\u01b0\u1edfng k\u1ebft chia s\u1ebb tham s\u1ed1 v\u00e0 k\u1ebft n\u1ed1i \u0111\u1ecba ph\u01b0\u01a1ng c\u0169ng r\u1ea5t ph\u00f9 h\u1ee3p \u0111\u1ed1i v\u1edbi d\u1eef li\u1ec7u v\u0103n b\u1ea3n, kh\u00f4ng ch\u1ec9 ri\u00eang h\u00ecnh \u1ea3nh. \u0110\u1ec3 bi\u1ebft hi\u1ec7u qu\u1ea3 c\u1ee7a m\u1ea1ng Convolution Neural Networks nh\u01b0 th\u1ebf n\u00e0o trong ph\u00e2n lo\u1ea1i v\u0103n b\u1ea3n, h\u00e3y c\u00f9ng th\u1ef1c h\u00e0nh qua b\u00e0i vi\u1ebft n\u00e0y.\n\nC\u00e1c n\u1ed9i dung \u0111\u01b0\u1ee3c gi\u1edbi thi\u1ec7u:\n\n1. C\u00e1c b\u01b0\u1edbc x\u1eed l\u00fd ch\u00ednh \u0111\u1ec3 x\u00e2y d\u1ef1ng m\u1ea1ng convolutional networks \u1edf b\u00e0i b\u00e1o g\u1ed1c.\n2. Th\u1ef1c h\u00e0nh project tr\u00ean n\u1ec1n t\u1ea3ng tensorflow v\u1edbi 3 ph\u01b0\u01a1ng ph\u00e1p ch\u00ednh: \n* low level: X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh b\u1eb1ng c\u00e1c ph\u00e9p bi\u1ebfn \u0111\u1ed5i ma tr\u1eadn \u1edf t\u1ea7ng th\u1ea5p nh\u1ea5t.\n* tensorflow: X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh s\u1eed d\u1ee5ng c\u00e1c layers c\u1ee7a tensorflow.\n* keras: X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh tr\u00ean n\u1ec1n t\u1ea3ng keras.\n\nT\u1ea5t c\u1ea3 c\u00e1c codes \u0111\u1ec1u \u0111\u01b0\u1ee3c ph\u00e1t tri\u1ec3n d\u1ef1a tr\u00ean y t\u01b0\u1edfng ch\u00ednh t\u1eeb source code c\u1ee7a t\u00e1c gi\u1ea3 Vi\u1ec7t Nguy\u1ec5n. M\u1ed9t l\u1ea7n n\u1eefa, xin c\u1ea3m \u01a1n anh \u0111\u00e3 chia s\u1ebb c\u00e1c nghi\u00ean c\u1ee9u c\u1ee7a m\u00ecnh.\n\n# 2. T\u1ed5ng qu\u00e1t ki\u1ebfn tr\u00fac m\u1ea1ng\n\nM\u00f4 h\u00ecnh s\u1ebd s\u1eed d\u1ee5ng b\u1ed9 k\u00ed t\u1ef1 c\u01a1 s\u1edf c\u00f3 \u0111\u1ed9 d\u00e0i 68: `\"\"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"\/\\\\|_@#$%^&*~`+-=<>()[]{}\"\"` \u0111\u1ec3 m\u00e3 h\u00f3a c\u00e1c \u0111o\u1ea1n v\u0103n. M\u1ed7i m\u1ed9t k\u00ed t\u1ef1 s\u1ebd \u0111\u01b0\u1ee3c \u0111\u1eb7c tr\u01b0ng b\u1edfi 1 vector \u0111\u01a1n v\u1ecb 68 chi\u1ec1u, gi\u00e1 tr\u1ecb 1 s\u1ebd xu\u1ea5t hi\u1ec7n \u1edf v\u1ecb tr\u00ed t\u01b0\u01a1ng \u1ee9ng c\u1ee7a k\u00ed t\u1ef1 trong b\u1ed9 k\u00ed t\u1ef1 c\u01a1 s\u1edf, c\u00e1c ph\u1ea7n t\u1eed \u1edf c\u00e1c v\u1ecb tr\u00ed c\u00f2n l\u1ea1i b\u1eb1ng 0. Ch\u1eb3ng h\u1ea1n a n\u1eb1m \u1edf v\u1ecb tr\u00ed th\u1ee9 3, do \u0111\u00f3 k\u00ed t\u1ef1 a s\u1ebd \u0111\u01b0\u1ee3c bi\u1ec3u di\u1ec5n b\u1edfi vector `[0, 0, 1, 0, 0,...,0]`. Khi \u0111\u00f3 m\u1ed9t v\u0103n b\u1ea3n s\u1ebd bi\u1ec3u di\u1ec5n b\u1edfi m\u1ed9t ma tr\u1eadn m\u00e0 m\u1ed7i c\u1ed9t c\u1ee7a ma tr\u1eadn \u0111\u00f3 \u0111\u1ea1i di\u1ec7n cho m\u1ed9t k\u00ed t\u1ef1 c\u1ea5u th\u00e0nh n\u00ean v\u0103n b\u1ea3n. \u0110\u1ec3 th\u1ed1ng nh\u1ea5t \u0111\u1ed9 d\u00e0i cho c\u00e1c v\u0103n b\u1ea3n kh\u00e1c nhau, ch\u00fang ta s\u1ebd l\u1ea5y ra 1014 k\u00ed t\u1ef1 \u0111\u1ea7u ti\u00ean xu\u1ea5t hi\u1ec7n trong v\u0103n b\u1ea3n th\u1ecfa m\u00e3n n\u1eb1m trong b\u1ed9 k\u00ed t\u1ef1 c\u01a1 s\u1edf. Nh\u01b0 v\u1eady m\u1ed7i m\u1ed9t v\u0103n b\u1ea3n s\u1ebd l\u00e0 m\u1ed9t ma tr\u1eadn c\u00f3 k\u00edch th\u01b0\u1edbc 68x1014.\nTr\u01b0\u1eddng h\u1ee3p v\u0103n b\u1ea3n c\u00f3 \u0111\u1ed9 d\u00e0i k\u00e9m 1014 k\u00ed t\u1ef1, c\u00e1c vector 0 s\u1ebd \u0111\u01b0\u1ee3c t\u1ef1 \u0111\u1ed9ng th\u00eam v\u00e0o cu\u1ed1i c\u00f9ng \u0111\u1ec3 \u0111\u1ea3m b\u1ea3o \u0111\u1ed9 d\u00e0i lu\u00f4n l\u00e0 1014.\n\nS\u01a1 \u0111\u1ed3 x\u1eed l\u00fd c\u1ee7a m\u1ea1ng t\u00edch ch\u1eadp trong b\u00e0i vi\u1ebft g\u1ed1c nh\u01b0 sau:\n\n![CNN Character - level](https:\/\/imgur.com\/4X8o4Yi.png)\n\n","ff2f1350":"## 3.2. Tensorflow layers","ad913643":"Hu\u1ea5n luy\u1ec7n model","a3f2baf5":"## 3.1. Tensorflow low level\n\nX\u00e2y d\u1ef1ng class `Char_level_cnn()` c\u00f3 c\u00e1c ch\u1ee9ng n\u0103ng ch\u00ednh:\n\n1. H\u00e0m `forward()`: Kh\u1edfi t\u1ea1o ki\u1ebfn tr\u00fac m\u1ea1ng neural.\n2. H\u00e0m `_create_conv()`: Kh\u1edfi t\u1ea1o c\u00e1c layers convolution.\n3. H\u00e0m `_create_fc()`: Kh\u1edfi t\u1ea1o c\u00e1c layers fully connected.\n4. H\u00e0m `loss()`: T\u00ednh to\u00e1n m\u1ea5t m\u00e1t.\n5. H\u00e0m `accuracy()`: T\u00ednh Accuracy.\n6. H\u00e0m `confusion_matrix()`: Th\u1ed1ng k\u00ea k\u1ebft qu\u1ea3 d\u1ef1 b\u00e1o theo c\u00e1c c\u1eb7p kh\u1ea3 n\u0103ng c\u1ee7a `(Predict, Ground Truth)` d\u01b0\u1edbi d\u1ea1ng ma tr\u1eadn confusion.\n","36b44fda":"Vi\u1ebft c\u00e1c h\u00e0m t\u1ed5ng qu\u00e1t s\u1eed d\u1ee5ng trong c\u1ea3 3 model","69bd4727":"## 3.3. Keras","bde53ee4":"Hu\u1ea5n luy\u1ec7n model.","b36df7b3":"Qui tr\u00ecnh c\u1ee7a s\u01a1 \u0111\u1ed3 tr\u00ean gi\u1ed1ng v\u1edbi h\u1ea7u h\u1ebft c\u00e1c s\u01a1 \u0111\u1ed3 x\u00e2y d\u1ef1ng m\u1ed9t m\u1ea1ng CNN. \u0110\u1ea7u ti\u00ean s\u1ebd l\u00e0 c\u00e1c layers convolutions v\u00e0 layer maxpooling, ti\u1ebfp theo ma tr\u1eadn k\u1ebft qu\u1ea3 t\u1eeb c\u00e1c layers tr\u01b0\u1edbc s\u1ebd \u0111\u01b0\u1ee3c flatten \u0111\u1ec3 k\u1ebft n\u1ed1i \u0111\u1ebfn c\u00e1c layers fully-connected, layer cu\u1ed1i c\u00f9ng s\u1ebd c\u00f3 s\u1ed1 units b\u1eb1ng v\u1edbi s\u1ed1 classes c\u1ee7a b\u00e0i to\u00e1n.\nC\u1ee5 th\u1ec3 v\u1ec1 c\u00e1c b\u01b0\u1edbc c\u1ee7a s\u01a1 \u0111\u1ed3 s\u1ebd nh\u01b0 sau:\n\n1. M\u00e3 h\u00f3a c\u00e1c v\u0103n b\u1ea3n th\u00e0nh m\u1ed9t ma tr\u1eadn embedding theo character-level c\u00f3 k\u00edch th\u01b0\u1edbc l\u00e0 68x1024.\n2. Th\u1ef1c hi\u1ec7n t\u00edch ch\u1eadp tr\u00ean ma tr\u1eadn \u0111\u1ea7u v\u1edbi m\u1ed9t b\u1ed9 l\u1ecdc c\u00f3 k\u00edch th\u01b0\u1edbc l\u00e0 68xkernel_size (kernel_size \u0111\u01b0\u1ee3c qui \u0111\u1ecbnh \u1edf c\u1ed9t kernel b\u1ea3ng b\u00ean d\u01b0\u1edbi, v\u1edbi layer \u0111\u1ea7u ti\u00ean kernel_size = 7). T\u00edch ch\u1eadp n\u00e0y s\u1ebd c\u00f3 t\u00e1c d\u1ee5ng t\u1ed5ng h\u1ee3p c\u00e1c \u0111\u1eb7c tr\u01b0ng tr\u00ean to\u00e0n b\u1ed9 v\u0103n b\u1ea3n trong ph\u1ea1m vi 7 k\u00ed t\u1ef1. S\u1ed1 l\u01b0\u1ee3ng c\u1ee7a b\u1ed9 l\u1ecdc s\u1ebd b\u1eb1ng height c\u1ee7a layer ti\u1ebfp theo. Trong \u0111\u1ed3 th\u1ecb n\u1ebfu ta qui \u0111\u1ecbnh s\u1ed1 l\u01b0\u1ee3ng b\u1ed9 l\u1ecdc l\u00e0 Feature th\u00ec output s\u1ebd l\u00e0 m\u1ed9t ma tr\u1eadn k\u00edch th\u01b0\u1edbc Feature x Length. Length \u0111\u01b0\u1ee3c bi\u1ebfn \u0111\u1ed5i theo c\u00f4ng th\u1ee9c t\u00ednh k\u00edch th\u01b0\u1edbc trong m\u1ea1ng t\u00edch ch\u1eadp $L' = \\frac{L-F}{S}+1$ trong \u0111\u00f3 $L$ l\u00e0 length c\u1ee7a layers li\u1ec1n tr\u01b0\u1edbc, $F$ l\u00e0 kernel_size, $S$ l\u00e0 b\u01b0\u1edbc di chuy\u1ec3n b\u1ed9 l\u1ecdc. \n2. Th\u1ef1c hi\u1ec7n li\u00ean ti\u1ebfp c\u00e1c layers convolutions k\u1ebft h\u1ee3p xen k\u1ebd v\u1edbi maxpooling \u0111\u1ec3 tr\u00edch l\u1ecdc \u0111\u1eb7c tr\u01b0ng v\u00e0 gi\u1ea3m chi\u1ec1u d\u1eef li\u1ec7u. \n![Table Convolution Layers](https:\/\/i.imgur.com\/fRdXEYz.png)\nTrong b\u00e0i vi\u1ebft t\u00e1c gi\u1ea3 th\u1ef1c hi\u1ec7n 2 phi\u00ean b\u1ea3n \u0111\u1ed1i v\u1edbi s\u1ed1 l\u01b0\u1ee3ng features l\u1edbn v\u00e0 s\u1ed1 l\u01b0\u1ee3ng features nh\u1ecf. Do kh\u00f4ng thay \u0111\u1ed5i s\u1ed1 l\u01b0\u1ee3ng features qua c\u00e1c layers n\u00ean ta th\u1ea5y trong s\u01a1 \u0111\u1ed3 height c\u1ee7a c\u00e1c ma tr\u1eadn kh\u00f4ng thay \u0111\u1ed5i. Chi\u1ec1u thay \u0111\u1ed5i ch\u00ednh l\u00e0 Length v\u00e0 \u0111\u00e2y ch\u00ednh l\u00e0 chi\u1ec1u ch\u00ednh th\u1ef1c hi\u1ec7n t\u00edch ch\u1eadp. \u0110\u1ed1i v\u1edbi chi\u1ec1u Feature, t\u00e1c gi\u1ea3 lu\u00f4n thi\u1ebft l\u1eadp stride v\u00e0 kernel_size = 1 n\u00ean c\u00f3 th\u1ec3 coi l\u00e0 kh\u00f4ng th\u1ef1c hi\u1ec7n t\u00edch ch\u1eadp theo chi\u1ec1u n\u00e0y.\nTrong b\u1ea3ng tr\u00ean \u0111\u1ed1i v\u1edbi nh\u1eefng layer c\u00f3 Pool = 3, m\u1ed9t max_pooling layer s\u1ebd \u0111\u01b0\u1ee3c th\u1ef1c hi\u1ec7n theo Length ngay sau conv layer tr\u01b0\u1edbc \u0111\u00f3. V\u1edbi c\u00e1c layer Pool = N\/A kh\u00f4ng th\u1ef1c hi\u1ec7n max_pooling.\n3. Cu\u1ed1i c\u00f9ng l\u00e0 c\u00e1c layers fully connected c\u00f3 s\u1ed1 units theo 2 version features l\u1edbn v\u00e0 features nh\u1ecf nh\u01b0 b\u1ea3ng d\u01b0\u1edbi:\n\n![Table Fully Connected Layers](https:\/\/imgur.com\/kgKl5GW.png)\n\n\u1ede layer th\u1ee9 9 t\u00f9y theo s\u1ed1 l\u01b0\u1ee3ng classes c\u1ee7a b\u00e0i to\u00e1n m\u00e0 ta s\u1ebd khai b\u00e1o t\u01b0\u01a1ng \u1ee9ng.\n\n# 3. X\u00e2y d\u1ef1ng m\u00f4 h\u00ecnh\n\nB\u00ean d\u01b0\u1edbi ta s\u1ebd ti\u1ebfn h\u00e0nh x\u00e2y d\u1ef1ng model theo 3 version: `low level, tensorflow, keras`. Do qu\u00e1 tr\u00ecnh hu\u1ea5n luy\u1ec7n t\u1ed1n nhi\u1ec1u th\u1eddi gian v\u00e0 h\u1ea1n ch\u1ebf c\u1ea5u h\u00ecnh m\u00e1y n\u00ean m\u00ecnh ch\u1ec9 th\u1ef1c hi\u1ec7n hu\u1ea5n luy\u1ec7n tr\u00ean m\u1ed9t m\u1eabu d\u1eef li\u1ec7u demo nh\u1ecf \u0111\u01b0\u1ee3c tr\u00edch t\u1eeb b\u1ed9 d\u1eef li\u1ec7u [amazon_review_polarity](https:\/\/drive.google.com\/open?id=0Bz8a_Dbh9QhbaW12WVVZS2drcnM).\n\nKh\u1ea3o s\u00e1t d\u1eef li\u1ec7u."}}