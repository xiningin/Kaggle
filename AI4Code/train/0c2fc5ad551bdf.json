{"cell_type":{"9122d617":"code","c8a8ec4f":"code","e2774a99":"code","b4ced4a7":"code","f0088f3d":"code","163d329c":"code","ab2a0237":"code","5acfd8ab":"code","aeb02330":"code","7aa4bec2":"code","b1f976e5":"code","950a57cb":"markdown","437f0e67":"markdown","2dad68e7":"markdown","dd5b0a2e":"markdown","bec8b41d":"markdown","dcc7569c":"markdown","f0ff6378":"markdown","3cb6c0fa":"markdown"},"source":{"9122d617":"#Importing the required modules\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\ndf = pd.read_csv('..\/input\/Iris.csv')","c8a8ec4f":"df.tail()       #Will return last 5 records of the dataframe","e2774a99":"df.info()","b4ced4a7":"df.describe()","f0088f3d":"sns.pairplot( df.drop(['Id'],axis=1) ,hue='Species')\nplt.legend(prop={'size': 24})","163d329c":"x=np.array(df.drop(['Id','Species'],axis=1))   \ny=df['Species']       #No need as K-mean is an unsupervised learning and we'll try to identify the classification through our model ","ab2a0237":"#Finding the optimum number of clusters for k-means classification using \"Elbow method\"\nfrom sklearn.cluster import KMeans\nwcss = []   #within cluster sum of squares\n\nfor i in range(1, 10):\n    kmeans = KMeans(n_clusters = i)\n    kmeans.fit(x)\n    wcss.append(kmeans.inertia_)\n    \n#Graph to find the suitable k value \nplt.plot(range(1, 10), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')   \nplt.show()","5acfd8ab":"#Training our model\nkmeans = KMeans(n_clusters = 3, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\nkmeans.fit(x)\ny_kmeans=kmeans.predict(x)\n#y_kmeans = kmeans.fit_predict(x)       #perfoms both fit and predict\nprint(len(y_kmeans))\nprint(y_kmeans)","aeb02330":"#Plot to compare our actual classification with model classification\n\nfig, axs = plt.subplots(ncols=2,figsize=(30,8))\naxs[0].scatter(x[y_kmeans == 0, 3], x[y_kmeans == 0, 1], s = 100, c = 'yellow', label = 'Iris-versicolor')\naxs[0].scatter(x[y_kmeans == 1, 3], x[y_kmeans == 1, 1], s = 100, c = 'red', label = 'Iris-setosa')\naxs[0].scatter(x[y_kmeans == 2, 3], x[y_kmeans == 2, 1], s = 100, c = 'orange', label = 'Iris-virginica')\naxs[0].scatter(kmeans.cluster_centers_[:, 3], kmeans.cluster_centers_[:,1], s = 300, c = 'black', label = 'Centroids')\naxs[0].set_title(\"Predictions Classification\")\naxs[0].legend(prop={'size': 14})\n\naxs[1].scatter(df[df['Species']=='Iris-versicolor']['PetalWidthCm'], df[df['Species']=='Iris-versicolor']['SepalWidthCm'], s = 100, c = 'yellow', label = 'Iris-versicolor')\naxs[1].scatter(df[df['Species']=='Iris-setosa']['PetalWidthCm'], df[df['Species']=='Iris-setosa']['SepalWidthCm'], s = 100, c = 'red', label = 'Iris-setosa')\naxs[1].scatter(df[df['Species']=='Iris-virginica']['PetalWidthCm'], df[df['Species']=='Iris-virginica']['SepalWidthCm'], s = 100, c = 'orange', label = 'Iris-virginica')\naxs[1].set_title(\"Actual Classification\")\naxs[1].legend(prop={'size': 14})","7aa4bec2":"df['Actual value']=df['Species']\ndf['Actual value'].replace(['Iris-versicolor','Iris-setosa','Iris-virginica'],[0,1,2],inplace=True)\ndf['Model prediction']=list(y_kmeans)\ndf.head()","b1f976e5":"uniq=df['Actual value'].nunique()\nactual_li=[0]*uniq\nmodel_li=[0]*uniq\ntrue_li=[0]*uniq\ntrue_count= 0\nactual_count = 0\nfor a,p in zip(df['Actual value'],df['Model prediction']):\n    actual_li[a]=actual_li[a]+1\n    model_li[p]=model_li[p]+1\n    true_count=true_count+1\n    if a==p:\n        true_li[a]=true_li[a]+1\n        actual_count=actual_count+1\n        \naccuracy=(actual_count*100)\/true_count\n        \nprint(actual_li)        #Actual classification\nprint(model_li)       #Model classification\nprint(true_li)          #Accurate classification\nprint(true_count)\nprint(actual_count)\nprint(accuracy)\n","950a57cb":"\n# Please upvote if you liked it. \n","437f0e67":"Iris dataset is the most commonly used dataset for Supervised Machine Learning Algorithm. In this kernel we will look it in an unsupervised manner through K-means algorithm.","2dad68e7":"# <a id='desc'> K-means Baseline<\/a>","dd5b0a2e":"From above pairplot we see that one of the class is clearly separable to other two class for all the features","bec8b41d":"<a id='desc'> Elbow Method<\/a>\n\n\nSince in unsupervised learning we are not given with number of target class, thus we use Elbow Method to get an intuition of the number of different class present in our model.\n\nElbow method is based on Sum of Square method. For every different k value we calculate the squared distance of the centroid with the cluster point and the count after which there is no significant decrease  we select that value as our ideal k value\n\n","dcc7569c":"Thus our prediction is 100% accurate for **Setosa** and for other two class seems almost same as that of actual values but with some conflicts. \n\nNow let's see the result mathematically. I have tried to keep the code general for this so that it can be directly used by anyone in other unsupervised algorithm or in general\n","f0ff6378":"From this we see that our prediction for the class ** \u201cSetosa\u201d**  were 100% accurate , followed by ** \"Versicolor\"**  and least for **\"Virginica\"**.\n\nThis is evident from the fact that in the graph **Setosa **  on being top left corner was easily separable from other two class and there were no conflict unlike for **Versicolor** and **Virginica**\n\n**Our model has  good accuracy of 89%**","3cb6c0fa":"After k=3 we see that there is not much significant decrease in the squared value. Thus we take k=3 for our model.  \n\nAlso in the graph we than we observe that the decrease from k=2 to k=3 is not much significant as it is from k=1 to k=2 , this indicate that there might not be clear separation boundary for cluster 2 and cluster 3 which is also evident from the above pair plot graph.\n"}}