{"cell_type":{"92e6f589":"code","66505093":"code","9afd6744":"code","b37c6dbe":"code","7b27a2f5":"code","6423fe71":"code","59fcbf41":"code","4ceb120a":"code","e771f303":"code","3324d5f9":"code","d4f57c72":"code","2a07584c":"code","c80f5ec4":"code","d3184920":"code","668868f8":"code","4e19c831":"code","f7e53f2f":"code","a1e762d3":"code","54df1a87":"code","b3bcbf9f":"code","9c861717":"code","81b3e79e":"markdown","77a611b1":"markdown"},"source":{"92e6f589":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\ndata_paths=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        data_paths.append(os.path.join(dirname, filename))\nprint(data_paths[0],data_paths[1])\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","66505093":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom os import listdir\nfrom pickle import dump\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\nimport string\ndef extract_features(directory):\n\t# load the model\n\tmodel = VGG16()\n\t# re-structure the model\n\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n\t# summarize\n\tprint(model.summary())\n\t# extract features from each photo\n\tfeatures = dict()\n\tfor name in listdir(directory):\n\t\t# load an image from file\n\t\tfilename = directory + '\/' + name\n\t\timage = load_img(filename, target_size=(224, 224))\n\t\t# convert the image pixels to a numpy array\n\t\timage = img_to_array(image)\n\t\t# reshape data for the model\n\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n\t\t# prepare the image for the VGG model\n\t\timage = preprocess_input(image)\n\t\t# get features\n\t\tfeature = model.predict(image, verbose=0)\n\t\t# get image id\n\t\timage_id = name.split('.')[0]\n\t\t# store feature\n\t\tfeatures[image_id] = feature\n\t\t#print('>%s' % name)\n\treturn features\n\n\n\n \ndef embedds(wordtoidx,vocab_size):\n        embeddings_index = {} \n        f = open('..\/input\/glove6b300dtxt\/glove.6B.300d.txt', encoding=\"utf-8\")\n        for line in f:\n            values = line.split()\n            word = values[0]\n            coefs = np.asarray(values[1:], dtype='float32')\n            embeddings_index[word] = coefs\n        embedding_dim = 300\n        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n        for word, i in wordtoidx.items():\n                embedding_vector = embeddings_index.get(word)\n                if embedding_vector is not None:\n                    embedding_matrix[i] = embedding_vector\n        return  embedding_matrix  \n","9afd6744":"# extract descriptions for images\ndef load_descriptions(doc):\n\tmapping = dict()\n\t# process lines\n\tfor line in doc.split('\\n'):\n\t\t# split line by white space\n\t\ttokens = line.split()\n\t\tif len(line) < 2:\n\t\t\tcontinue\n\t\t# take the first token as the image id, the rest as the description\n\t\timage_id, image_desc = tokens[0], tokens[1:]\n\t\t# remove filename from image id\n\t\timage_id = image_id.split('.')[0]\n\t\t# convert description tokens back to string\n\t\timage_desc = ' '.join(image_desc)\n\t\t# create the list if needed\n\t\tif image_id not in mapping:\n\t\t\tmapping[image_id] = list()\n\t\t# store description\n\t\tmapping[image_id].append(image_desc)\n\treturn mapping\n \n# load doc into memory\ndef load_doc(filename):\n\t# open the file as read only\n\tfile = open(filename, 'r')\n\t# read all text\n\ttext = file.read()\n\t# close the file\n\tfile.close()\n\treturn text\ndef load_set(filename):\n\tdoc = load_doc(filename)\n\tdataset = list()\n\t# process line by line\n\tfor line in doc.split('\\n')[1:]:\n\t\t# skip empty lines\n\t\tif len(line) < 1:\n\t\t\tcontinue\n\t\tif line=='image,caption':\n\t\t\tcontinue\n\t\t# get the image identifier\n\t\tidentifier = line.split('.')[0]\n\t\tdataset.append(identifier)\n\treturn set(dataset)\n\n# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n\t# load document\n\tdoc = load_doc(filename)\n\tdescriptions = dict()\n\tfor line in doc.split('\\n'):\n\t\t# split line by white space\n\t\ttokens = line.split()\n\t\t# split id from description\n\t\timage_id, image_desc = tokens[0], tokens[1:]\n\t\t# skip images not in the set\n\t\tif image_id in dataset:\n\t\t\t# create list\n\t\t\tif image_id not in descriptions:\n\t\t\t\tdescriptions[image_id] = list()\n\t\t\t# wrap description in tokens\n\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n\t\t\t# store\n\t\t\tdescriptions[image_id].append(desc)\n\treturn descriptions","b37c6dbe":"\ndef clean_descriptions(descriptions):\n\t# prepare translation table for removing punctuation\n\ttable = str.maketrans('', '', string.punctuation)\n\tfor key, desc_list in descriptions.items():\n\t\tfor i in range(len(desc_list)):\n\t\t\tdesc = desc_list[i]\n\t\t\t# tokenize\n\t\t\tdesc = desc.split()\n\t\t\t# convert to lower case\n\t\t\tdesc = [word.lower() for word in desc]\n\t\t\t# remove punctuation from each token\n\t\t\tdesc = [w.translate(table) for w in desc]\n\t\t\t# remove hanging 's' and 'a'\n\t\t\tdesc = [word for word in desc if len(word)>1]\n\t\t\t# remove tokens with numbers in them\n\t\t\tdesc = [word for word in desc if word.isalpha()]\n\t\t\t# store as string\n\t\t\tdesc_list[i] =  ' '.join(desc)\n\n# convert the loaded descriptions into a vocabulary of words\ndef to_vocabulary(descriptions):\n\t# build a list of all description strings\n\tall_desc = set()\n\tfor key in descriptions.keys():\n\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n\treturn all_desc\n\n# save descriptions to file, one per line\ndef save_descriptions(descriptions, filename):\n\tlines = list()\n\tfor key, desc_list in descriptions.items():\n\t\tfor desc in desc_list:\n\t\t\tlines.append(key + ' ' + desc)\n\tdata = '\\n'.join(lines)\n\tfile = open(filename, 'w')\n\tfile.write(data)\n\tfile.close()\n    # load photo features\ndef load_photo_features(filename, dataset):\n\t# load all features\n\tall_features = load(open(filename, 'rb'))\n\t# filter features\n\tfeatures = {k: all_features[k] for k in dataset}\n\treturn features\n ","7b27a2f5":"# extract features from all images\ndirectory = '..\/input\/flickr8kimagescaptions\/flickr8k\/images'\nfeatures = extract_features(directory)\nprint('Extracted Features: %d' % len(features))\n# save to file\ndump(features, open('features.pkl', 'wb'))\nfilename = '..\/input\/flickr8kimagescaptions\/flickr8k\/captions.txt'\n# load descriptions\ndoc = load_doc(filename)\n# parse descriptions\ndescriptions = load_descriptions(doc)\nprint('Loaded: %d ' % len(descriptions))\n# clean descriptions\nclean_descriptions(descriptions)\n# summarize vocabulary\nvocabulary = to_vocabulary(descriptions)\nprint('Vocabulary Size: %d' % len(vocabulary))\n# save to file\nsave_descriptions(descriptions, 'descriptions.txt')    ","6423fe71":"\nfrom pickle import load\n# load training dataset (6K)\nfilename = '..\/input\/flickr8kimagescaptions\/flickr8k\/captions.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\ntest=set(list(train)[7091:])\ntrain=set(list(train)[:7091])\n# descriptions\ntest_descriptions = load_clean_descriptions('descriptions.txt', test)\nprint('Descriptions: test=%d' % len(test_descriptions))\n# photo features\ntest_features = load_photo_features('features.pkl', test)\n#print('Photos: test=%d' % len(test_features))\n\n# descriptions\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# photo features\ntrain_features = load_photo_features('features.pkl', train)\n#print('Photos: train=%d' % len(train_features))\n\n","59fcbf41":"# convert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n\tall_desc = list()\n\tfor key in descriptions.keys():\n\t\t[all_desc.append(d) for d in descriptions[key]]\n\treturn all_desc\n \n# fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n\tlines = to_lines(descriptions)\n\ttokenizer = Tokenizer()\n\ttokenizer.fit_on_texts(lines)\n\treturn tokenizer\n \n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)","4ceb120a":"dump(tokenizer, open('tokenizer.pkl', 'wb'))","e771f303":"from keras.utils import to_categorical\n\n# calculate the length of the description with the most words\ndef max_length(descriptions):\n\tlines = to_lines(descriptions)\n\treturn max(len(d.split()) for d in lines)\n# create sequences of images, input sequences and output words for an image\ndef create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n\tX1, X2, y = list(), list(), list()\n\t# walk through each description for the image\n\tfor desc in desc_list:\n\t\t# encode the sequence\n\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n\t\t# split one sequence into multiple X,y pairs\n\t\tfor i in range(1, len(seq)):\n\t\t\t# split into input and output pair\n\t\t\tin_seq, out_seq = seq[:i], seq[i]\n\t\t\t# pad input sequence\n\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n\t\t\t# encode output sequence\n\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n\t\t\t# store\n\t\t\tX1.append(photo)\n\t\t\tX2.append(in_seq)\n\t\t\ty.append(out_seq)\n\treturn np.array(X1), np.array(X2), np.array(y)","3324d5f9":"max_length=max_length(descriptions)\nwordtoidx=tokenizer.word_index\n","d4f57c72":"print(max_length)\n","2a07584c":"embedding_matrix=embedds(wordtoidx ,vocab_size)","c80f5ec4":"\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.layers  import Input,Dense,Flatten,LSTM, GRU, Bidirectional, Embedding,add,Dropout,Conv2D\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ndef caption_model(vocab_size,max_length):\n            embds=Embedding(vocab_size,300,mask_zero=True,weights=[embedding_matrix])\n            image_inputs=Input(shape=(4096,))\n            reg0=Dropout(0.5)(image_inputs)\n            x=Dense(300,'relu')(reg0)\n            text_input=Input(shape=(max_length,))\n            emb=embds(text_input)\n            reg1=Dropout(0.5)(emb)\n            seq0=GRU(300)(reg1,x)\n            seq1=add([x,seq0])\n            decoder0=Dense(300,'relu')(seq1)\n            decoder1=Dense(300,'relu')(decoder0)\n            decoder2=Dense(300,'relu')(decoder1)\n            decoder3=Dense(vocab_size,'softmax')(decoder2)\n            model=Model(inputs=[image_inputs,text_input],outputs=decoder3)\n            print(model.summary())\n            model.compile( optimizer='adam', loss='categorical_crossentropy',metrics='acc')\n            return model","d3184920":"model=caption_model(vocab_size,max_length)\n","668868f8":"# data generator, intended to be used in a call to model.fit_generator()\ndef data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n\t# loop for ever over images\n\twhile 1:\n\t\tfor key, desc_list in descriptions.items():\n\t\t\t# retrieve the photo feature\n\t\t\tphoto = photos[key][0]\n\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n\t\t\tyield [in_img, in_seq], out_word","4e19c831":"\n# test the data generator\ngenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\ninputs, outputs = next(generator)\nprint(inputs[0].shape)\nprint(inputs[1].shape)\nprint(outputs.shape)","f7e53f2f":"# train the model, run epochs manually and save after each epoch\nepochs = 20\nsteps = len(train_descriptions)\nfor i in range(epochs):\n\t# create the data generator\n\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n\t# fit for one epoch\n\tmodel.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=2)\n\t# save model\n\tmodel.save('model_' + str(i) + '.h5')","a1e762d3":"# map an integer to a word\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n             if index == integer:\n                     return word\n    return None\n \n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n    # seed the generation process\n    in_text = 'startseq'\n     # iterate over the whole length of the sequence\n    for i in range(max_length):\n        # integer encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        # pad input\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        # predict next word\n        yhat = model.predict([photo,sequence], verbose=1)\n        # convert probability to integer\n        yhat = np.argmax(yhat)\n        # map integer to word\n        word = word_for_id(yhat, tokenizer)\n        # stop if we cannot map the word\n        if word is None:\n            break\n             # append as input for generating the next word\n        in_text += ' ' + word\n        # stop if we predict the end of the sequence\n        if word == 'endseq':\n            break\n    return in_text","54df1a87":"from nltk.translate.bleu_score import corpus_bleu\n# evaluate the skill of the model\ndef evaluate_model(model, descriptions, photos, tokenizer, max_length):\n    actual, predicted = list(), list()\n    # step over the whole set\n    for key, desc_list in descriptions.items():\n        # generate description\n        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n        # store actual and predicted\n        references = [d.split() for d in desc_list]\n        actual.append(references)\n        predicted.append(yhat.split())\n    # calculate BLEU score\n    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n","b3bcbf9f":"evaluate_model(model, test_descriptions, test_features, tokenizer, 31)","9c861717":"generate_desc(model, tokenizer, pic, 31)","81b3e79e":"## data preperation","77a611b1":"## Feature extraction  &  Defining a model"}}