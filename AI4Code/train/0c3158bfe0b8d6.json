{"cell_type":{"b3095483":"code","07f9a9c2":"code","b50d9318":"code","5fb45ce0":"code","0b95c819":"code","d0fce683":"code","f6b833dc":"code","c4b158ed":"code","b954eb78":"code","f7fe4420":"code","bcf47542":"code","4ad8e746":"code","34a93792":"markdown","e4f9a4c4":"markdown","cd5a49dc":"markdown"},"source":{"b3095483":"import multiprocessing as mp\nfrom pathlib import Path\nfrom typing import Any, Callable, List, Tuple\n\nimport matplotlib.pyplot as plt\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nimport torch\nfrom torch.distributions.bernoulli import Bernoulli\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import io, transforms\n# from torchvision.utils import Image, ImageDraw\nfrom torchvision.transforms.functional import to_pil_image\nfrom tqdm.auto import tqdm\n# import wandb\n# wandb.login()\n\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\n%matplotlib inline\nprint(torch.__version__, pl.__version__)","07f9a9c2":"# Image parameters\nTRAIN_FILES = \"\/kaggle\/input\/coco-2017-dataset\/coco2017\/train2017\/\"\nIMAGE_SIZE = 256\nPATCH_SIZE = 16\nZERO_PCT = 0.1\nPATCHES_PER_ROW = (IMAGE_SIZE \/\/ PATCH_SIZE)\nNUM_PATCHES = PATCHES_PER_ROW ** 2\nRGB_CHANNELS = 3\nNUM_PIXELS = PATCH_SIZE ** 2 * RGB_CHANNELS\n\n# Training parameters\nBATCH_SIZE = 64\nEPOCHS = 2\nLR = 2e-3\n\n# Transformer parameters\nN_HEADS = 8\nN_LAYERS = 6","b50d9318":"class ImageData(Dataset):\n    def __init__(self, files: List[str]):\n        self.files = files\n        self.resize = transforms.Resize((IMAGE_SIZE, IMAGE_SIZE))\n        \n    def __len__(self):\n        return len(self.files)\n    \n    def __getitem__(self, i):\n        img = self.resize(io.read_image(self.files[i]))\n        if img.shape[0] == 1:\n            return torch.cat([img]*3)\n        else:\n            return img\n\nclass CollateFn:\n    def __init__(self):\n        self.m = Bernoulli(1 - ZERO_PCT)\n\n    def __call__(\n        self, batch: List[torch.FloatTensor]\n    ) -> Tuple[torch.FloatTensor, torch.FloatTensor]:\n        patches = torch.stack(batch)\\\n                    .unfold(2, PATCH_SIZE, PATCH_SIZE)\\\n                    .unfold(3, PATCH_SIZE, PATCH_SIZE)\n\n        num_images = len(patches)\n        patches = patches.reshape(\n            num_images,\n            RGB_CHANNELS, \n            NUM_PATCHES, \n            PATCH_SIZE, \n            PATCH_SIZE\n        )\n        patches.transpose_(1, 2)\n        \n        y = patches.reshape(num_images, NUM_PATCHES, -1) \/ 255.0 - 0.5\n        mask = self.m.sample((1, NUM_PATCHES, 1))\n        x = y * mask\n\n        return x, y\n\nfiles = [str(file) for file in Path(TRAIN_FILES).glob(\"*.jpg\")]\nimage_data = ImageData(files)\ncollate_fn = CollateFn()\n\nimage_dl = DataLoader(\n    image_data, \n    BATCH_SIZE*2, \n    shuffle=True, \n    drop_last=True, \n    num_workers=4,\n    pin_memory=True,\n    collate_fn=collate_fn,\n)","5fb45ce0":"x, y = next(iter(image_dl))\nx.shape, y.shape","0b95c819":"class Model(nn.Module):\n    def __init__(self, d_model, n_head, n_layers):\n        super().__init__()\n        # transformer\n        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=n_head)\n        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n\n        # positional embedding\n        w_pos = torch.randn(NUM_PATCHES, d_model \/\/ 2) \/ d_model ** 0.5\n        self.pos_embed = nn.Parameter(w_pos)\n\n        # pixel projection\n        self.linear = nn.Linear(NUM_PIXELS, NUM_PIXELS \/\/ 2)\n\n    def forward(self, x):\n        pixel_proj = self.linear(x)\n        position = torch.stack([self.pos_embed]*len(x))\n        x = torch.cat([pixel_proj, position], dim=-1)\n\n        return self.encoder(x)","d0fce683":"class LightningModel(pl.LightningModule):\n    def __init__(\n        self,\n        model: nn.Module,\n        loss_fn: Callable,\n        lr: float,\n    ):\n        super().__init__()\n        self.model = model\n        self.lr = lr\n        self.loss_fn = loss_fn\n\n    def common_step(\n        self,\n        batch: Tuple[torch.FloatTensor, torch.FloatTensor],\n    ) -> torch.FloatTensor:\n        x, y = batch\n        out = self.model(x)\n        loss = self.loss_fn(out, y)\n        return loss\n\n    def training_step(\n        self, batch: Tuple[torch.FloatTensor, torch.FloatTensor], *args: List[Any]\n    ) -> torch.Tensor:\n        loss = self.common_step(batch)\n        self.log(name=\"Training MAE\", value=loss, on_step=True, on_epoch=True)\n        return loss\n\n#     def validation_step(\n#         self, batch: Tuple[torch.FloatTensor, torch.FloatTensor], *args: List[Any]\n#     ) -> torch.Tensor:\n#         loss = self.common_step(batch)\n#         self.log(name=\"Validation MAE\", value=loss, on_step=True, on_epoch=True)\n#         return loss\n\n    def configure_optimizers(self) -> torch.optim.Optimizer:\n        return torch.optim.Adam(self.model.parameters(), lr=self.lr)\n","f6b833dc":"model = Model(NUM_PIXELS, N_HEADS, N_LAYERS)\nlightning_model = LightningModel(model, F.l1_loss, LR)\n\ntrainer = pl.Trainer(\n    max_epochs=EPOCHS,\n    gpus=torch.cuda.device_count(),\n    gradient_clip_val=1.0,\n#     logger=WandbLogger(\"ViT_inpaint2\", \"\/kaggle\/working\/logs\/\"),\n    precision=16\n)\ntrainer.fit(lightning_model, image_dl)","c4b158ed":"def reconstruct_image(img):\n    patches = img.reshape(PATCHES_PER_ROW, PATCHES_PER_ROW, RGB_CHANNELS, PATCH_SIZE, PATCH_SIZE)\n    rows = 0\n    reconstruct = torch.cat(\n                        [torch.cat([patches[rows, cols, ...] for cols in range(PATCHES_PER_ROW)], dim=2) \n                             for rows in range(PATCHES_PER_ROW)], \n                        dim=1\n    )\n\n    return reconstruct + 0.5\n    \n# to_pil_image(reconstruct_image(y1[0]))","b954eb78":"model = model.eval().to(device)\nwith torch.no_grad():\n    out = model(x.to(device))","f7fe4420":"i = 2\nx_reconstruct = reconstruct_image(x[i])\ny_reconstruct = reconstruct_image(out[i])","bcf47542":"to_pil_image(x_reconstruct)","4ad8e746":"to_pil_image(y_reconstruct)","34a93792":"## Model","e4f9a4c4":"## Image reconstruction","cd5a49dc":"## Data"}}