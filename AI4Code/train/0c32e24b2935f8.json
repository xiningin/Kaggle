{"cell_type":{"da6a53c5":"code","47e30c9b":"code","5c49cbcd":"code","3575cbf8":"code","3d97dc21":"code","1504b138":"code","317df03e":"code","916bbf2c":"code","c46175a2":"code","fef7befe":"code","e7430bf8":"code","bf602996":"code","1651638d":"code","9f1b3770":"code","5926fc2a":"code","76aa01b0":"code","92e0aef8":"code","839ec5b6":"code","50f1ccf8":"code","58fd8263":"code","c96ac934":"code","4afa3f1c":"code","702d27d6":"code","d7a4d4ec":"code","e25fd941":"code","a07eb921":"code","22ee6789":"code","89483fc1":"code","d027d8d5":"code","02b3e446":"code","9635204c":"code","4be619b8":"code","36db7a5b":"code","4e3a7d33":"code","42066a53":"code","c0eea5ea":"code","6b221b46":"code","ecd51118":"code","cb5897d4":"code","2041c399":"code","d6f0e355":"code","d8696716":"code","66e444e4":"code","31afb530":"code","1dec1f31":"markdown","b9c16c8d":"markdown","56b4918d":"markdown","c47cfc11":"markdown","af9ea472":"markdown","b6611675":"markdown","70c1676e":"markdown","d7958ea8":"markdown"},"source":{"da6a53c5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandas.api.types import is_numeric_dtype\nfrom sklearn.preprocessing import LabelEncoder\n\npd.set_option('display.max_rows', 999)\npd.set_option('display.max_columns', 999)\n\n%load_ext autoreload\n%autoreload 2\n\nINPUT = '\/kaggle\/input\/m5-forecasting-accuracy\/'","47e30c9b":"import pandas as pd\nimport numpy as np\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\n# from: https:\/\/www.kaggle.com\/ajinomoto132\/reduce-mem\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\"\n    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.\n    \"\"\"\n\n    start_mem = df.memory_usage().sum() \/ 1024 ** 2\n    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n\n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            continue\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == \"int\":\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype(\"category\")\n\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","5c49cbcd":"class M5Split2:\n    \n    def __init__(self, n_splits, group_id, date_col, train_size, gap_size, val_size, step):\n        self.n_splits = n_splits + 1\n        self.group_id = group_id\n        self.date_col = date_col\n        self.train_size = train_size\n        self.gap_size = gap_size\n        self.val_size = val_size\n        self.step = step\n        \n        \n    def split(self, df):\n        df = df.sort_values(by=[self.date_col])\n        indexes = []\n        group_indexes = np.array(df.groupby(self.group_id, observed=True).apply(lambda x: x.index))\n        for split in range(self.n_splits, 0, -1):\n            val_idx = []\n            gap_idx = []\n            train_idx = []\n                \n            for idx_arr in group_indexes:\n                \n                if self.train_size + self.gap_size + self.val_size + self.step*split > len(idx_arr):\n                    print(f'Max Split reached')\n                    break\n                    \n                val_idx += list(idx_arr[-(self.val_size + self.step*(split-1)):len(idx_arr) - 1 - self.step*(split-1)])\n                gap_idx += list(idx_arr[-(self.gap_size + self.val_size + self.step*(split-1)):-(self.val_size + self.step*(split-1))])\n                train_idx += list(idx_arr[-(self.train_size + self.gap_size + self.val_size + self.step*(self.n_splits)):-(self.val_size + self.gap_size + self.step*(split-1))])\n                \n            yield train_idx, gap_idx, val_idx\n","3575cbf8":"# https:\/\/www.kaggle.com\/ragnar123\/very-fst-model\ndef simple_fe_extra(data):\n    \n    # rolling demand features\n    data['lag_t56'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(56))\n    data['lag_t57'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(57))\n    data['lag_t58'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(58))\n    data['rolling_mean_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(56).rolling(7).mean())\n    data['rolling_std_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(56).rolling(7).std())\n    data['rolling_mean_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(56).rolling(30).mean())\n    data['rolling_mean_t90'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(56).rolling(90).mean())\n    data['rolling_mean_t180'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(56).rolling(180).mean())\n    data['rolling_std_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(56).rolling(30).std())\n    data['rolling_skew_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(56).rolling(30).skew())\n    data['rolling_kurt_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(56).rolling(30).kurt())\n    \n    \n    # price features\n    data['lag_price_t1'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n    data['price_change_t1'] = (data['lag_price_t1'] - data['sell_price']) \/ (data['lag_price_t1'])\n    data['rolling_price_max_t365'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n    data['price_change_t365'] = (data['rolling_price_max_t365'] - data['sell_price']) \/ (data['rolling_price_max_t365'])\n    data['rolling_price_std_t7'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n    data['rolling_price_std_t30'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n    data.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n    \n    # time features\n    data['date'] = pd.to_datetime(data['date'])\n    data['week'] = data['date'].dt.week\n    data['day'] = data['date'].dt.day\n    data['dayofweek'] = data['date'].dt.dayofweek\n    \n    \n    return data\n","3d97dc21":"# ------------------------- CALENDAR ------------------------ #\ndf_calendar = pd.read_csv(filepath_or_buffer=f'{INPUT}calendar.csv')\ndf_calendar = reduce_mem_usage(df_calendar)\nprint(df_calendar.shape)\ndf_calendar['date'] = pd.to_datetime(df_calendar['date'])\ndf_calendar['weekday'] = df_calendar['weekday'].astype(str)\ndf_calendar['d'] = df_calendar['d'].astype(str)\ndf_calendar['event_name_1'] = df_calendar['event_name_1'].astype(str)\ndf_calendar['event_type_1'] = df_calendar['event_type_1'].astype(str)\ndf_calendar['event_name_2'] = df_calendar['event_name_2'].astype(str)\ndf_calendar['event_type_2'] = df_calendar['event_type_2'].astype(str)\n\ndf_calendar['event_tomorrow_1'] = df_calendar['event_name_1'].shift(-1)\ndf_calendar['event_tomorrow_2'] = df_calendar['event_name_2'].shift(-1)\ndf_calendar['event_type_tomorrow_1'] = df_calendar['event_type_1'].shift(-1)\ndf_calendar['event_type_tomorrow_2'] = df_calendar['event_type_2'].shift(-1)\n\ndf_calendar = df_calendar.fillna(value='nan')","1504b138":"# event_name_1 and event_name_2 should be fitted together since both are essentailly the same\nle1 = LabelEncoder()\nle1.fit(pd.concat(objs=[df_calendar['event_name_1'], df_calendar['event_name_2']], axis=0))\ndf_calendar['event_name_1'] = le1.transform(df_calendar['event_name_1'])\ndf_calendar['event_name_2'] = le1.transform(df_calendar['event_name_2'])\n\n# event_type_1 and event_type_2 should be fitted together since both are essentailly the same\nle2 = LabelEncoder()\nle2.fit(pd.concat(objs=[df_calendar['event_type_1'], df_calendar['event_type_2']], axis=0))\ndf_calendar['event_type_1'] = le2.transform(df_calendar['event_type_1'])\ndf_calendar['event_type_2'] = le2.transform(df_calendar['event_type_2'])\n\nle3 = LabelEncoder()\nle3.fit(pd.concat(objs=[df_calendar['event_tomorrow_1'], df_calendar['event_tomorrow_2']], axis=0))\ndf_calendar['event_tomorrow_1'] = le3.transform(df_calendar['event_tomorrow_1'])\ndf_calendar['event_tomorrow_2'] = le3.transform(df_calendar['event_tomorrow_2'])\n\nle4 = LabelEncoder()\nle4.fit(pd.concat(objs=[df_calendar['event_type_tomorrow_1'], df_calendar['event_type_tomorrow_2']], axis=0))\ndf_calendar['event_type_tomorrow_1'] = le4.transform(df_calendar['event_type_tomorrow_1'])\ndf_calendar['event_type_tomorrow_2'] = le4.transform(df_calendar['event_type_tomorrow_2'])\n\n\ndf_calendar = reduce_mem_usage(df_calendar)","317df03e":"df_calendar.tail()","916bbf2c":"# ------------------------- SELLING PRICES ------------------------ #\ndf_selling_prices = pd.read_csv(f'{INPUT}sell_prices.csv')\nprint(df_selling_prices.shape)\ndf_selling_prices['store_id'] = df_selling_prices['store_id'].astype(str)\ndf_selling_prices['item_id'] = df_selling_prices['item_id'].astype(str)\ndf_selling_prices['sell_price_cents'] = df_selling_prices['sell_price'] - df_selling_prices['sell_price'].astype(int) \ndf_selling_prices['sell_price_perceived'] = df_selling_prices['sell_price'].astype(int)\ndf_selling_prices = reduce_mem_usage(df_selling_prices)","c46175a2":"# Generate which columns to be loaded\n\ndef generate_cols(factor=0.5):\n    cols =['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n    start = int(1914*(1-factor))\n    days = ['d_' + str(i) for i in range(start, 1914, 1)]\n    cols += days\n    return cols\n\ncolumns = generate_cols(factor=0.2)","fef7befe":"# ------------------------- SALES TRAIN VALIDATION ---------------------- #\ndf_sales_train_val = pd.read_csv(f'{INPUT}sales_train_validation.csv', usecols=columns)\ndf_sales_train_val['id'] = df_sales_train_val['id'].astype(str)\ndf_sales_train_val['item_id'] = df_sales_train_val['item_id'].astype(str)\ndf_sales_train_val['store_id'] = df_sales_train_val['store_id'].astype(str)\ndf_sales_train_val['dept_id'] = df_sales_train_val['dept_id'].astype(str)\ndf_sales_train_val['cat_id'] = df_sales_train_val['cat_id'].astype(str)\ndf_sales_train_val['state_id'] = df_sales_train_val['state_id'].astype(str)\n\nprint(f'Shape before melting: {df_sales_train_val.shape}')\nprint(f\"Unique products before melting: {df_sales_train_val['id'].nunique()}\")\n\ndf_sales_train_val = pd.melt(df_sales_train_val, id_vars=['id', 'item_id', 'store_id', 'dept_id', 'cat_id', 'state_id'], var_name='d',\n                              value_name='demand')\n\nprint(f'\\nShape after melting: {df_sales_train_val.shape}')\nprint(f\"Unique products after melting: {df_sales_train_val['id'].nunique()}\")\n\n# 0 means validation set\ndf_sales_train_val['set'] = 0\ndf_sales_train_val['strategy'] = 'train'\ndf_sales_train_val['id'] = df_sales_train_val['item_id'].str.cat(df_sales_train_val['store_id'], sep='_')\n\n\nprint(f'\\nShape after making id from item_id and store_id: {df_sales_train_val.shape}')\nprint(f\"Unique products after id from item_id and store_id: {df_sales_train_val['id'].nunique()}\")","e7430bf8":"df_sales_train_val = df_sales_train_val.drop_duplicates()\nprint(f'Shape after making id from item_id and store_id: {df_sales_train_val.shape}')\nprint(f\"Unique products after id from item_id and store_id: {df_sales_train_val['id'].nunique()}\")","bf602996":"df_sales_train_val = reduce_mem_usage(df_sales_train_val)","1651638d":"# --------------------- ADDING CALENDAR AND SELLING PRICES to SALES TRAIN VAL ---------------------- #\ndf_sales_train_val = df_sales_train_val.merge(df_calendar, on = ['d'], how='left')\ndf_sales_train_val = df_sales_train_val.merge(df_selling_prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], how='left')","9f1b3770":"print(f'Shape after merging: {df_sales_train_val.shape}')","5926fc2a":"df_sales_train_val = df_sales_train_val.reset_index(drop=True)","76aa01b0":"le = []\nfor num, col in enumerate(['id', 'item_id', 'store_id', 'dept_id', 'cat_id', 'state_id']):\n    le_id = LabelEncoder()\n    le.append(le_id)\n    le[num].fit(df_sales_train_val[col])\n    df_sales_train_val[f'{col}_encoded'] = le[num].transform(df_sales_train_val[col])","92e0aef8":"# -------------------------- SAMPLE SUBMISSION -------------------------- #\ndf_sample_submission = pd.read_csv(f'{INPUT}sample_submission.csv')\ndf_sample_submission = reduce_mem_usage(df_sample_submission)\ndf_sample_submission['id'] = df_sample_submission['id'].astype(str)","839ec5b6":"df_sample_submission['state_id'] = df_sample_submission['id'].str[-15:-13]\ndf_sample_submission['store_id'] = df_sample_submission['id'].str[-15:-11]\ndf_sample_submission['item_id'] = df_sample_submission['id'].str[0:-16]\ndf_sample_submission['dept_id'] = df_sample_submission['id'].str[0:-20]\ndf_sample_submission['cat_id'] = df_sample_submission['id'].str[0:-22]\ndf_sample_submission.tail()","50f1ccf8":"# -------------------------- SALES TEST -------------------------- #\ndf_sales_test = pd.melt(df_sample_submission, id_vars=['id', 'item_id', 'store_id', 'dept_id', 'cat_id', 'state_id'], var_name='d', value_name='demand')\n\ndf_sales_test['d2'] = df_sales_test['d'].str[1:]\ndf_sales_test['d2'] = df_sales_test['d2'].astype(int)\ndf_sales_test.loc[df_sales_test['id'].str[-10:]=='evaluation', 'd2'] = df_sales_test.loc[df_sales_test['id'].str[-10:]=='evaluation', 'd2'] + 28\ndf_sales_test['d2'] = df_sales_test['d2'] + 1913\ndf_sales_test['d2'] = df_sales_test['d2'].astype(str)\ndf_sales_test['d2'] = 'd_' + df_sales_test['d2']\n\n# 1 indicates test set\ndf_sales_test['set'] = 1\ndf_sales_test.loc[df_sales_test['id'].str[-10:]=='validation', 'strategy'] = 'validation'\ndf_sales_test.loc[df_sales_test['id'].str[-10:]=='evaluation', 'strategy'] = 'evaluation'\ndf_sales_test['original_id'] = df_sales_test['id']\ndf_sales_test['id'] = df_sales_test['id'].str[0:-11]\n\ndf_sales_test = df_sales_test.drop(columns=['d'])\ndf_sales_test = df_sales_test.rename(columns={'d2': 'd'})","58fd8263":"# # --------------------- ADDING CALENDAR AND SELLING PRICES to SALES TRAIN VAL ---------------------- #\ndf_sales_test = df_sales_test.merge(df_calendar, on = ['d'], how='left')\ndf_sales_test = df_sales_test.merge(df_selling_prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], how='left')","c96ac934":"# ---------------------- Making {id}_encoded features ------------------------ #\nfor num, col in enumerate(['id', 'item_id', 'store_id', 'dept_id', 'cat_id', 'state_id']):\n    df_sales_test[f'{col}_encoded'] = le[num].transform(df_sales_test[col])","4afa3f1c":"# -------------------------- TRAIN VAL TEST -------------------------- #\ndf_sales_test = df_sales_test.drop(columns=['original_id'])\ndf_train_val_test = pd.concat(objs=[df_sales_train_val, df_sales_test], axis=0, ignore_index=True)\ndf_train_val_test = reduce_mem_usage(df_train_val_test)","702d27d6":"df_train_val_test['d_number'] = df_train_val_test['d'].str[2:].astype(int)","d7a4d4ec":"df_train_val_test = df_train_val_test.sort_values(by=['id', 'd_number'])","e25fd941":"df_train_val_test = df_train_val_test.reset_index(drop=True)","a07eb921":"df_train_val_test = simple_fe_extra(df_train_val_test)","22ee6789":"for col in df_train_val_test.columns:\n    if is_numeric_dtype(df_train_val_test[col]):\n        df_train_val_test[col] = df_train_val_test[col].round(decimals=3)","89483fc1":"df_train_val_test['is_weekend'] = 0\ndf_train_val_test.loc[df_train_val_test['weekday'].isin(['Saturday', 'Sunday']), 'is_weekend'] = 1 ","d027d8d5":"# Making a feature 'prob_high_demand' which is a float which is max on day, day before and day after the event_name_1 \n# and which decreases as we move away from event date on both in past and future. Think of it as a bell curve over date\n# peaking at event date\n\nindexes = df_train_val_test.groupby(by=['event_name_1']).groups\ndel indexes[30]                # 30 representes 'nan' event\nhigh_demand_indexes = []\nmid_demand_indexes = []\nmid_low_demand_indexes = []\nfor key, val in indexes.items():\n    for value in indexes[key]:\n        high_demand_indexes.append(value)\n        high_demand_indexes.append(value + 1)\n        mid_demand_indexes.append(value + 2)\n        mid_low_demand_indexes.append(value + 3)\n        high_demand_indexes.append(value - 1)\n        mid_demand_indexes.append(value - 2)\n        mid_low_demand_indexes.append(value - 3)\n        \nhigh_demand_indexes = list(set(high_demand_indexes))\nmid_demand_indexes = list(set(mid_demand_indexes))\nmid_low_demand_indexes = list(set(mid_low_demand_indexes))\n\ndf_train_val_test['prob_high_demand'] = 0\nhigh_demand_indexes = list(set(df_train_val_test.index).intersection(set(high_demand_indexes)))\nmid_demand_indexes = list(set(df_train_val_test.index).intersection(set(mid_demand_indexes)))\nmid_low_demand_indexes = list(set(df_train_val_test.index).intersection(set(mid_low_demand_indexes)))\n\ndf_train_val_test.loc[mid_low_demand_indexes, 'prob_high_demand'] = 0.3\ndf_train_val_test.loc[mid_demand_indexes, 'prob_high_demand'] = 0.6\ndf_train_val_test.loc[high_demand_indexes, 'prob_high_demand'] = 0.9","02b3e446":"df_train_val_test['date'] = pd.to_datetime(df_train_val_test['date'])","9635204c":"df_train_val_test['is_weekend'] = df_train_val_test['is_weekend'].astype('category')","4be619b8":"import lightgbm\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error as mse\nimport time","36db7a5b":"cols = list(df_train_val_test.columns)","4e3a7d33":"cols","42066a53":"# X_train = df_train_val_test.loc[df_train_val_test['set']==0, cols]\n# y_train = X_train['demand']\n# X_train = X_train.drop(columns=['d_number', 'set', 'demand', 'strategy', 'week', 'day', 'dayofweek',\n#                                 'wday','month']) # drop 'set' bcoz it can cause data leak ","c0eea5ea":"# scores = []\n# m5split2 = m5s.M5Split2(n_splits=0, group_id='id', date_col='date', train_size=365, gap_size=0, val_size=28, step=28)\n# fold = 0\n# estimators = []\n# for trn_idx, _, val_idx in m5split2.split(X_train):\n#     fold_s_time = time.time()\n#     s_time = time.time()\n#     X_trn = X_train.loc[trn_idx, :]\n#     y_trn = y_train[trn_idx]\n#     X_val = X_train.loc[val_idx, :]\n#     y_val = y_train[val_idx]\n    \n#     e_time = time.time()\n#     print(f'Time taken for splitting of fold{fold}: {e_time-s_time} seconds')\n\n#     s_time = time.time()\n#     lgbmr = lightgbm.LGBMRegressor(objective = \"regression\",\n#                                    boosting_type = 'gbdt',\n#                                    n_estimators = 1000,\n#                                    random_state = 20,\n#                                    learning_rate = 0.1,\n#                                    bagging_fraction = 0.75,\n#                                    bagging_freq = 10, \n#                                    colsample_bytree = 0.75)\n#     lgbmr.fit(X_trn.drop(columns=['date', 'd']), y_trn)\n#     estimators.append(lgbmr)\n#     y_predict = np.around(lgbmr.predict(X_val.drop(columns=['date', 'd'])))\n#     e_time = time.time()\n#     print(f'Time taken for learning of fold{fold}: {e_time-s_time} seconds')\n    \n#     s_time = time.time()\n#     rmse_score = np.sqrt(mse(y_val, y_predict))\n#     print(f'CV RMSE score of fold{fold} (train_len: {len(trn_idx)}, val_len: {len(val_idx)}): {rmse_score}')\n#     scores.append(rmse_score)\n#     fold_e_time = time.time()\n#     print(f'Total time taken for fold{fold}: {fold_e_time-fold_s_time} seconds\\n')\n#     fold += 1\n\n# print(f'Mean score: {sum(scores)\/len(scores)}, Stdev: {np.std(scores)}')","6b221b46":"# # Since the splitter above will not train model on last 28 days I manually trained on data which includes the last 28 days as well\n\n# X_trn = X_train.loc[X_train['date']>='2015-04-24 00:00:00']\n# y_trn = y_train[X_trn.index]\n# lgbmr = lightgbm.LGBMRegressor(objective = \"regression\",\n#                                    boosting_type = 'gbdt',\n#                                    n_estimators = 1000,\n#                                    random_state = 20,\n#                                    learning_rate = 0.1,\n#                                    bagging_fraction = 0.75,\n#                                    bagging_freq = 10, \n#                                    colsample_bytree = 0.75)\n# lgbmr.fit(X_trn.drop(columns=['date', 'd']), y_trn)\n# estimators.append(lgbmr)","ecd51118":"# X_test = df_train_val_test.loc[df_train_val_test['set']==1, cols]\n# X_test = X_test.drop(columns=['d_number', 'set', 'demand', 'strategy', 'week', 'day', 'dayofweek', \n#                               'wday', 'month'])\n# predictions = pd.DataFrame(columns=[estimator.__class__.__name__ + str(estimators.index(estimator)) for estimator in estimators])\n\n# for i, estimator in enumerate(estimators):\n#     y_predict = estimator.predict(X_test.drop(columns=['date', 'd']))\n#     predictions[predictions.columns[i]] = pd.Series(y_predict)\n\n# cols = list(predictions.columns)\n# predictions['final'] = np.mean(predictions.loc[:, cols], axis=1)","cb5897d4":"# predictions.max()","2041c399":"# predictions.min()","d6f0e355":"# prediction_col = 'LGBMRegressor1'\n# predictions.loc[predictions[prediction_col] < 0.0, prediction_col] = 0.0 \n\n# predictions = predictions.set_index(X_test.index)\n\n# X_test['pred'] = predictions['LGBMRegressor1']\n\n\n# df_train_val_test.loc[X_test.index, 'demand'] = X_test['pred']\n# df_submit = df_train_val_test.loc[X_test.index, :]\n# df_submit['original_id'] = df_submit['id'].str.cat(df_submit['strategy'], sep='_')\n\n# df_submit.to_csv(f'{INPUT}submit.csv', index=False)\n\n# df_submit.head()","d8696716":"# df_submit = pd.read_csv(f'{INPUT}submit.csv')\n# df_submit = reduce_mem_usage(df_submit)","66e444e4":"# df_submit['d2'] = df_submit['d'].str[2:]\n# df_submit['d2'] = df_submit['d2'].astype(int)\n# df_submit.loc[df_submit['strategy']=='validation', 'd2'] = df_submit.loc[df_submit['strategy']=='validation', 'd2'] - 1913\n# df_submit.loc[df_submit['strategy']=='evaluation', 'd2'] = df_submit.loc[df_submit['strategy']=='evaluation', 'd2'] - 1941\n# df_submit['d2'] = df_submit['d2'].astype(str)\n# df_submit['d2'] = 'F' + df_submit['d2']\n# df_submit.loc[df_submit['strategy']=='evaluation', 'demand'] = 0.0\n# df_submission = df_submit.loc[:, ['original_id', 'demand', 'd2']].pivot(index='original_id', columns='d2', values='demand')","31afb530":"# cols_order = ['F' + str(i) for i in range(1,29)]\n# df_submission = df_submission[cols_order]\n\n# df_sample_submission = pd.read_csv(f'{INPUT}sample_submission.csv')\n# df_sample_submission = df_sample_submission.set_index(keys='id')\n# df_submission = df_submission.reindex(df_sample_submission.index)\n# df_submission = df_submission.reset_index()\n# df_submission = df_submission.rename(columns={'original_id': 'id'})\n# df_submission.to_csv(f'{INPUT}submission.csv', index=False)\n# df_submission","1dec1f31":"## Modelling","b9c16c8d":"## Cross validation","56b4918d":"## Preprocessing","c47cfc11":"## Inspiration\n\n* https:\/\/www.kaggle.com\/ajinomoto132\/reduce-mem\n* https:\/\/www.kaggle.com\/zmnako\/lgbm-update-0-85632\n* https:\/\/www.kaggle.com\/ratan123\/m5-forecasting-lightgbm-with-timeseries-splits\n* https:\/\/www.kaggle.com\/ragnar123\/very-fst-model","af9ea472":"## Import Statements","b6611675":"## Ensembling and Prediction","70c1676e":"## Helper functions","d7958ea8":"# M5-Accuracy"}}