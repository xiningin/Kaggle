{"cell_type":{"a20ec6aa":"code","3126c6a0":"code","c60ca04c":"code","1113e80d":"code","0e8dd189":"code","c48ea81a":"code","f934566a":"code","b27399af":"code","e9348395":"code","37b80fda":"code","b3ec0afc":"code","71bae5ea":"code","f5de724f":"code","6938b56f":"code","771856c7":"code","3b1cc403":"code","a3892572":"code","a6c7975c":"code","f25bb2cb":"code","9b32490a":"code","1c701676":"code","83670e13":"code","2b58bf98":"code","f4c7c472":"code","69036c8d":"code","fd2ed6f4":"code","b6f0accc":"code","f37f2366":"code","3be1bdbd":"code","793342eb":"code","6a0a22f0":"code","abd5e666":"code","dc331d38":"code","3e804644":"code","abe42617":"code","383dd70f":"markdown","9ea17c4d":"markdown","329898de":"markdown","3ed02358":"markdown","616edc0c":"markdown","bbe5b4f0":"markdown","774e02b8":"markdown","70ebc5c8":"markdown","eabcd1a3":"markdown","7ebe63e7":"markdown","0c4e64c0":"markdown","4e251a91":"markdown","2ed29be3":"markdown","4153582a":"markdown","3389ed38":"markdown","741d26a8":"markdown","e8a926c6":"markdown"},"source":{"a20ec6aa":"# Import the required modules\nimport datetime\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\nmatplotlib.rcParams['font.size'] = 8\nfrom skimage import img_as_float\nfrom skimage import exposure\nimport plotly.graph_objects as go\n\nimport os\nimport glob\nimport random\nfrom skimage import io # To preprocess the images\nfrom distutils.file_util import copy_file\nimport seaborn as sns\nimport cv2\nimport keras\nfrom keras.models import load_model\nfrom keras import backend as K\nimport tensorflow as tf\n\nfrom skimage.transform import rescale\nfrom keras_preprocessing.image import ImageDataGenerator\n\nimport warnings\nwarnings.simplefilter('ignore')","3126c6a0":"# Set dataset path\n\nDATASET_PATH = '..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset'\n\n# There are two classes of images that we will deal with\ncls = ['COVID', 'Normal']","c60ca04c":"covid_path = os.path.join(DATASET_PATH, cls[0], '*')\ncovid = glob.glob(covid_path)\ncovid = io.imread(covid[0])\n\nnormal_path = os.path.join(DATASET_PATH, cls[1], '*')\nnormal = glob.glob(normal_path)\nnormal = io.imread(normal[0])\n\nf, axes = plt.subplots(1, 2, sharey=True)\nf.set_figwidth(10)\n\naxes[0].imshow(covid, cmap='gray')\naxes[1].imshow(normal, cmap='gray')","1113e80d":"print(f'Image shape for COVID dataset is: {covid.shape}')\nprint(f'Image shape for Normal dataset is: {normal.shape}')","0e8dd189":"print(f'Number of COVID Images: {len(os.listdir(covid_path[:-2]))} \\\n\\nNumber of Non-COVID Images: {len(os.listdir(normal_path[:-2]))}')","c48ea81a":"# Histogram Equalization\n\ndef plot_img_and_hist(image, axes, bins=256):\n    \"\"\"Plot an image along with its histogram and cumulative histogram.\n\n    \"\"\"\n    image = img_as_float(image)\n    ax_img, ax_hist = axes\n    ax_cdf = ax_hist.twinx()\n\n    # Display image\n    ax_img.imshow(image, cmap=plt.cm.gray)\n    ax_img.set_axis_off()\n\n    # Display histogram\n    ax_hist.hist(image.ravel(), bins=bins, histtype='step', color='black')\n    ax_hist.ticklabel_format(axis='y', style='scientific', scilimits=(0, 0))\n    ax_hist.set_xlabel('Pixel intensity')\n    ax_hist.set_xlim(0, 1)\n    ax_hist.set_yticks([])\n\n    # Display cumulative distribution\n    img_cdf, bins = exposure.cumulative_distribution(image, bins)\n    ax_cdf.plot(bins, img_cdf, 'r')\n    ax_cdf.set_yticks([])\n\n    return ax_img, ax_hist, ax_cdf","f934566a":"# Load a normal image\nimg = normal\n\n# Contrast stretching\np2, p98 = np.percentile(img, (2, 98))\nimg_rescale = exposure.rescale_intensity(img, in_range=(p2, p98))\n\n# Equalization\nimg_eq = exposure.equalize_hist(img)\n\n# Adaptive Equalization\nimg_adapteq = exposure.equalize_adapthist(img, clip_limit=0.03)\n\n# Display results\nfig = plt.figure(figsize=(12, 8))\naxes = np.zeros((2, 4), dtype=np.object)\naxes[0, 0] = fig.add_subplot(2, 4, 1)\nfor i in range(1, 4):\n    axes[0, i] = fig.add_subplot(2, 4, 1+i, sharex=axes[0,0], sharey=axes[0,0])\nfor i in range(0, 4):\n    axes[1, i] = fig.add_subplot(2, 4, 5+i)\n\nax_img, ax_hist, ax_cdf = plot_img_and_hist(img, axes[:, 0])\nax_img.set_title('Low contrast image')\n\ny_min, y_max = ax_hist.get_ylim()\nax_hist.set_ylabel('Number of pixels')\nax_hist.set_yticks(np.linspace(0, y_max, 5))\n\nax_img, ax_hist, ax_cdf = plot_img_and_hist(img_rescale, axes[:, 1])\nax_img.set_title('Contrast stretching')\n\nax_img, ax_hist, ax_cdf = plot_img_and_hist(img_eq, axes[:, 2])\nax_img.set_title('Histogram equalization')\n\nax_img, ax_hist, ax_cdf = plot_img_and_hist(img_adapteq, axes[:, 3])\nax_img.set_title('Adaptive equalization')\n\nax_cdf.set_ylabel('Fraction of total intensity')\nax_cdf.set_yticks(np.linspace(0, 1, 5))\n\n# prevent overlap of y-axis labels\nfig.tight_layout()\nplt.show()","b27399af":"# Load a covid image\nimg = covid\n\n# Contrast stretching\np2, p98 = np.percentile(img, (2, 98))\nimg_rescale = exposure.rescale_intensity(img, in_range=(p2, p98))\n\n# Equalization\nimg_eq = exposure.equalize_hist(img)\n\n# Adaptive Equalization\nimg_adapteq = exposure.equalize_adapthist(img, clip_limit=0.03)\n\n# Display results\nfig = plt.figure(figsize=(12, 8))\naxes = np.zeros((2, 4), dtype=np.object)\naxes[0, 0] = fig.add_subplot(2, 4, 1)\nfor i in range(1, 4):\n    axes[0, i] = fig.add_subplot(2, 4, 1+i, sharex=axes[0,0], sharey=axes[0,0])\nfor i in range(0, 4):\n    axes[1, i] = fig.add_subplot(2, 4, 5+i)\n\nax_img, ax_hist, ax_cdf = plot_img_and_hist(img, axes[:, 0])\nax_img.set_title('Low contrast image')\n\ny_min, y_max = ax_hist.get_ylim()\nax_hist.set_ylabel('Number of pixels')\nax_hist.set_yticks(np.linspace(0, y_max, 5))\n\nax_img, ax_hist, ax_cdf = plot_img_and_hist(img_rescale, axes[:, 1])\nax_img.set_title('Contrast stretching')\n\nax_img, ax_hist, ax_cdf = plot_img_and_hist(img_eq, axes[:, 2])\nax_img.set_title('Histogram equalization')\n\nax_img, ax_hist, ax_cdf = plot_img_and_hist(img_adapteq, axes[:, 3])\nax_img.set_title('Adaptive equalization')\n\nax_cdf.set_ylabel('Fraction of total intensity')\nax_cdf.set_yticks(np.linspace(0, 1, 5))\n\n# prevent overlap of y-axis labels\nfig.tight_layout()\nplt.show()","e9348395":"# Create the list of paths to the images\n\n# Lists for access paths\nlistCovidPaths = []\nlistNormalPaths = []\n\n# Get covid images files paths\nfor root, directories, files in os.walk(covid_path[:-2]):\n    for name in files:\n        listCovidPaths.append(os.path.join(root, name))\n        \n# Get normal images files paths\nfor root, directories, files in os.walk(normal_path[:-2]):\n    for name in files:\n        listNormalPaths.append(os.path.join(root, name))\n\n# Shuffle lists for random train \/ test\n\nrandom.shuffle(listCovidPaths)\nrandom.shuffle(listNormalPaths)","37b80fda":"# Create new folders for image training\n\n# main folder\n!mkdir .\/Data\/\n\n# Train data folders\n!mkdir .\/Data\/Train\/\n!mkdir .\/Data\/Train\/Covid\/\n!mkdir .\/Data\/Train\/Normal\/\n\n# Test data folders\n!mkdir .\/Data\/Test\/\n!mkdir .\/Data\/Test\/Covid\/\n!mkdir .\/Data\/Test\/Normal\/\n\n# Paths to covid images folders\npathCovidTrain = '.\/Data\/Train\/Covid\/'\npathCovidTest = '.\/Data\/Test\/Covid\/'\n\n# Paths to normal images folders\npathNormalTrain = '.\/Data\/Train\/Normal\/'\npathNormalTest = '.\/Data\/Test\/Normal\/'","b3ec0afc":"# Move files to new folders in the 80:20 ratio\n\nlen_covid = len(os.listdir(covid_path[:-2]))\nlen_normal = len(os.listdir(normal_path[:-2]))\ncovid_80 = round(len(os.listdir(covid_path[:-2])) * 0.8)     # 80% of the COVID data\nnormal_80 = round(len(os.listdir(normal_path[:-2])) * 0.8)   # 80% of the Normal data\n\n# Move covid images files to new folders\nfor i in range(len_covid):\n    if i < covid_80:\n        copy_file(listCovidPaths[i], pathCovidTrain)\n    else : \n        copy_file(listCovidPaths[i], pathCovidTest)\n\n# Move normal images files to new folders\nfor i in range(len_normal):\n    if i < normal_80:\n        copy_file(listNormalPaths[i], pathNormalTrain)\n    else:\n        copy_file(listNormalPaths[i], pathNormalTest)","71bae5ea":"# Definition of data generators\n\n# for train data\ntrainGenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale = 1.\/255,\n    rotation_range = 20,\n    zoom_range = 0.2,\n    shear_range = 0.2,\n    featurewise_center = True,\n    featurewise_std_normalization = True,\n    width_shift_range = 0,\n    height_shift_range = 0,\n    vertical_flip = False,\n    fill_mode = 'nearest'\n)\n\n# for test data\ntestGenerator = tf.keras.preprocessing.image.ImageDataGenerator(\n    rescale = 1.\/255,    \n)","f5de724f":"# Build data generators\n\n# Build for train data\npathTrainDir = '.\/Data\/Train\/'\n\ntrainGeneratorBuild = trainGenerator.flow_from_directory(\n    pathTrainDir,\n    target_size = (299, 299),\n    class_mode = 'binary',\n    batch_size = 16,\n    shuffle = True \n)\n\n# Build for test data\npathTestDir = '.\/Data\/Test\/'\n\ntestGeneratorBuild = testGenerator.flow_from_directory(\n    pathTestDir,\n    target_size = (299, 299),\n    class_mode = 'binary',\n    batch_size = 16,\n    shuffle = True\n)","6938b56f":"# COVID and Normal dataset directory\nBASIS_DIR = '..\/input\/covid19-radiography-database\/COVID-19_Radiography_Dataset'\nCLASSES = [\"COVID\",  \"Normal\"]","771856c7":"#Image augmentation process:\ntrain_datagen = ImageDataGenerator(\n    rescale = 1.\/255,\n    rotation_range = 20,\n    horizontal_flip = True,\n    vertical_flip = False,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    fill_mode = 'nearest',\n    \n    #split dataset to training(80%) and validation(20%):\n    validation_split = 0.2\n)","3b1cc403":"# Training dataset and Validation dataset:\ntrain_data = train_datagen.flow_from_directory(\n    directory=BASIS_DIR,\n    target_size=(299, 299),\n    batch_size=32,\n    shuffle=True,\n    class_mode='binary',\n    subset='training',\n    classes=CLASSES\n    )\nval_data = train_datagen.flow_from_directory(\n    directory=BASIS_DIR,\n    target_size=(299, 299),\n    batch_size=32,\n    shuffle=True,\n    class_mode='binary',\n    subset='validation',\n    classes=CLASSES\n    )","a3892572":"#Using sequential model:\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(299, 299, 3)),\n    tf.keras.layers.MaxPooling2D(2, 2),\n\n    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n\n    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    \n    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n\n    \n    tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.summary()","a6c7975c":"# Define our custom loss function\n\ndef focal_loss(y_true, y_pred):\n    gamma = 2.0\n    alpha = 0.25\n    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n\n\n# Compile our model\nadam = tf.keras.optimizers.Adam(lr=0.0001)\nmodel.compile(loss=[focal_loss], metrics=[\"accuracy\"], optimizer=adam)","f25bb2cb":"# Training process:\nstart = datetime.datetime.now()\n\nnumber_epochs = 25\n\nhistory = model.fit(train_data,\n                    epochs = number_epochs, \n                    validation_data = val_data,\n                    verbose = 1)\n\nend = datetime.datetime.now()\nprint(f'Total Training Time: {end - start}')\n\n# verbose = 0 => silent,\n#         = 1 => progress bar,\n#         = 2 => one line per epoch","9b32490a":"model.save('my_model_1.h5')","1c701676":"# Plot training accuracy and validation accuracy\n\nplt.plot(history.history['accuracy'], 'r', label='Accuracy Training')\nplt.plot(history.history['val_accuracy'], 'b', label='Accuracy Validation')\nplt.title('Accuracy Training and Validation')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(loc=0)\nplt.show()","83670e13":"# Plotting training loss and validation loss\nplt.plot(history.history['loss'], 'r', label='Loss Training')\nplt.plot(history.history['val_loss'], 'b', label='Loss Validation')\nplt.title('Loss Training and Validation')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(loc=0)\nplt.show()","2b58bf98":"# Define the Keras model\n# Use InceptionResNetV2 Keras model\n\nengine = tf.keras.applications.InceptionResNetV2(\n    # Freezing the weights of the top layer in the InceptionResNetV2 pre-traiined model\n    include_top = False,\n\n    # Use Imagenet weights\n    weights = 'imagenet',\n\n    # Define input shape to 224x224x3\n    input_shape = (224, 224, 3),\n\n    # Set classifier activation to sigmoid\n    classifier_activation = 'sigmoid'\n)","f4c7c472":"# Define the Keras model outputs\n\nx = tf.keras.layers.GlobalAveragePooling2D(name = 'avg_pool')(engine.output)\nout = tf.keras.layers.Dense(1, activation = 'sigmoid', name = 'dense_output')(x)\n\n\n# Build the Keras model\n\nmodel = tf.keras.models.Model(inputs = engine.input, outputs = out)","69036c8d":"# Compile the model\n\nmodel.compile(\n    # Set optimizer to Adam(0.001)\n    optimizer = tf.keras.optimizers.Adam(0.001),\n\n    # Set loss to binary crossentropy\n    loss = 'binary_crossentropy',\n\n    # Set metrics to accuracy\n    metrics = ['accuracy']\n)","fd2ed6f4":"# Fit Keras model\nstart = datetime.datetime.now()\n\nhistory = model.fit_generator(\n    # Use train generator\n    trainGeneratorBuild,\n\n    # Set epochs to 12\n    epochs = 12,\n\n    # Set steps per epochs to 300\n    steps_per_epoch = 300,\n    \n    # Set verbose to 1\n    verbose = 1\n)\n\nend = datetime.datetime.now()\nprint(f'Total Training Time: {end - start}')","b6f0accc":"model.save('my_model_2.h5')","f37f2366":"# Create a graph representing the loss\n\n# Get loss data\nlossG = history.history['loss']\naccuracyG = history.history['accuracy']\nepochs = [i for i in range(len(lossG))]\n\n# Create graph\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(\n        x = epochs,\n        y = lossG,\n        name = 'Loss',\n        marker = dict(\n            color = 'rgba(250,50,50,1)'        \n        )\n    )\n)\n\nfig.add_trace(\n    go.Scatter(\n        x = epochs,\n        y = accuracyG,\n        name = 'Accuracy',\n        marker = dict(\n            color = 'rgba(50,250,50,1)'        \n        )\n    )\n)\n\nfig.update_layout(\n    title = 'Model loss',\n    template = 'plotly_white'\n)\n\nfig.update_xaxes(\n    title_text='Epochs'\n)\nfig.update_yaxes(\n    title_text='Loss \/ Accuracy values'\n)\n\nfig.show()","3be1bdbd":"# Checke the accuracy of the Keras model on the test data\n\ntestLoss, testAccuracy = model.evaluate(\n    # Use test generator\n    testGeneratorBuild,\n    \n    # Set verbose to 1\n    verbose = 1\n)\n\n# Print results\nprint('Accuracy of model : ' + str(round(testAccuracy,4)*100) + ' %')\nprint('Loss of model : ' + str(round(testLoss,4)))","793342eb":"# Define our custom loss function\n\ndef focal_loss(y_true, y_pred):\n    gamma = 2.0\n    alpha = 0.25\n    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n","6a0a22f0":"# Compile our model\n\nadam = tf.keras.optimizers.Adam(lr=0.0001)\nmodel.compile(loss=[focal_loss], metrics=[\"accuracy\"], optimizer=adam)","abd5e666":"# Fit Keras model\nstart = datetime.datetime.now()\n\nhistory = model.fit_generator(\n    # Use train generator\n    trainGeneratorBuild,\n\n    # Set epochs to 20\n    epochs = 20,\n\n    # Set steps per epochs to 300\n    steps_per_epoch = 300,       # batch_size\n\n    # Set verbose to 1\n    verbose = 1\n)\n\nend = datetime.datetime.now()\nprint(f'Total Training Time: {end - start}')","dc331d38":"model.save('my_model_3.h5')","3e804644":"# Create a graph representing the loss\n\n# Get loss data\nlossG = history.history['loss']\naccuracyG = history.history['accuracy']\nepochs = [i for i in range(len(lossG))]\n\n# Create graph\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(\n        x = epochs,\n        y = lossG,\n        name = 'Loss',\n        marker = dict(\n            color = 'rgba(250,50,50,1)'        \n        )\n    )\n)\n\nfig.add_trace(\n    go.Scatter(\n        x = epochs,\n        y = accuracyG,\n        name = 'Accuracy',\n        marker = dict(\n            color = 'rgba(50,250,50,1)'        \n        )\n    )\n)\n\nfig.update_layout(\n    title = 'Model loss',\n    template = 'plotly_white'\n)\n\nfig.update_xaxes(\n    title_text='Epochs'\n)\nfig.update_yaxes(\n    title_text='Loss \/ Accuracy values'\n)\n\nfig.show()","abe42617":"# Check the accuracy of the Keras model on the test data\n\ntestLoss, testAccuracy = model.evaluate(\n    # Use test generator\n    testGeneratorBuild,\n    \n    # Set verbose to 1\n    verbose = 1\n)\n\n# Print results\nprint('Accuracy of model : ' + str(round(testAccuracy,4)*100) + ' %')\nprint('Loss of model : ' + str(round(testLoss,4)))","383dd70f":"# Dataset Info:\n# COVID-19 RADIOGRAPHY\nA team of researchers from Qatar University, Doha, Qatar, and the University of Dhaka, Bangladesh along with their collaborators from Pakistan and Malaysia in collaboration with medical doctors have created a database of chest X-ray images for COVID-19 positive cases along with Normal and Viral Pneumonia images.\n\n<br>\n<hr>\n<br>\n\n### Citation:\n- M.E.H. Chowdhury, T. Rahman, A. Khandakar, R. Mazhar, M.A. Kadir, Z.B. Mahbub, K.R. Islam, M.S. Khan, A. Iqbal, N. Al-Emadi, M.B.I. Reaz, M. T. Islam, \u201cCan AI help in screening Viral and COVID-19 pneumonia?\u201d IEEE Access, Vol. 8, 2020, pp. 132665 - 132676. [Paper link](https:\/\/ieeexplore.ieee.org\/document\/9144185)\n\n- Rahman, T., Khandakar, A., Qiblawey, Y., Tahir, A., Kiranyaz, S., Kashem, S.B.A., Islam, M.T., Maadeed, S.A., Zughaier, S.M., Khan, M.S. and Chowdhury, M.E., 2020. Exploring the Effect of Image Enhancement Techniques on COVID-19 Detection using Chest X-ray Images. [Paper Link](https:\/\/doi.org\/10.1016\/j.compbiomed.2021.104319)\n\n<br>\n<hr>\n<br>\n\n### Acknowledgments:\nThanks to the Italian Society of Medical and Interventional Radiology (SIRM) for publicly providing the COVID-19 Chest X-Ray dataset, Valencia Region Image Bank (BIMCV) padchest dataset and would like to thank J. P. Cohen for taking the initiative to gather images from articles and online resources. Finally to the Chest X-Ray Images (pneumonia) database in Kaggle and Radiological Society of North America (RSNA) Kaggle database for making a wonderful X-ray database for normal, lung opacity, viral, and bacterial pneumonia images. Also, a big thanks to our collaborators!\n\n<br>\n<hr>\n<br>\n\n### Dataset Link:\n[COVID-19 Radiography Database](https:\/\/www.kaggle.com\/tawsifurrahman\/covid19-radiography-database)","9ea17c4d":"### (ii) Loss: Focal Loss\nBecause our dataset is unbalanced, we can utilise this strategy to balance the weighting of our training instances.\nInstead of giving all training examples the same weight, focus loss gives the well-classified instances a lower weight. As a result, more training emphasis is placed on data that is difficult to classify!\n\nIf we have a data imbalance in practise, our majority class will soon become well-classified because we have considerably more data for it. As a result, we may use the focus loss to give those minority class instances more relative weight during training, ensuring that we obtain high accuracy on our minority class as well.","329898de":"## Train Model from Scratch","3ed02358":"# 4. Train-Test Split","616edc0c":"# 5. Model Traning","bbe5b4f0":"# 7. Plot Loss","774e02b8":"# 2. Exploratory Data Analysis","70ebc5c8":"> <strong> This is an Imbalanced Dataset!! <\/strong>\n>\n> To deal with this issue, we have applied ***focal loss*** later in the section.","eabcd1a3":"## Train Model using Trasfer Learning","7ebe63e7":"# Motivation:\nThousands of individuals have died as a result of the coronavirus epidemic, which has afflicted millions of people around the world. Any technical device that allows for rapid and accurate COVID-19 infection detection can be extremely beneficial to healthcare providers.\n\nX-ray imaging is an easily accessible method used to diagnose COVID-19 patients, according to Chowdhury et al. (2020). Despite the fact that regular Chest X-Ray (XCR) scans can help with early detection of suspected cases, the images of various viral pneumonia patients are identical. As a result, radiologists have a hard time distinguishing COVID-19 from other viral pneumonia patients.\n\nThe goal of this database and current study is to see how useful artificial intelligence (AI) can be in detecting COVID-19 from chest X-ray pictures quickly and accurately.","0c4e64c0":"# 3. Data Augmentation","4e251a91":"# Contents:\n<blockquote>\n1. Reading the Dataset & Selecting 2 classes <br>\n2. EDA <br>\n3. Data Augmentation <br>\n4. Train-Test Split <br>\n5. Model Training <br>\n6. Fine Tuning the Model <br>\n7. Plotting Losses\n<\/blockquote>","2ed29be3":"There are a total of 21165 samples, which are classified into four categories:\n1. Covid-19\n2. Lung Opacity\n3. Normal\n4. Viral Pneumonia\n\nThe photos are all in the Portable Network Graphics (PNG) file type and are 299x299 pixels in size. The database presently contains 3,616 COVID-19 positive cases, 10,192 Normal, 6,012 Lung Opacity (Non-COVID lung infection), and 1,345 Viral Pneumonia pictures, according to the most recent update.\n\nWe will only train our model on two of these four classes, namely the 'Normal' and 'COVID' classes.","4153582a":"## Preprocessing Images","3389ed38":"# 6. Fine Tuning the Model","741d26a8":"\n### (i) Loss: binary_crossentropy","e8a926c6":"# 1. Reading the Dataset"}}