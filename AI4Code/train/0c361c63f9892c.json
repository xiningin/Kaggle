{"cell_type":{"36303763":"code","e52696f4":"code","2e590a9f":"code","2438f9a4":"code","96ab4aad":"code","575f4d6c":"code","1d9a4dbe":"code","58e858ab":"code","1588292f":"code","51be6748":"code","00897d94":"code","95844804":"code","0c94ae97":"code","63cdb2d1":"code","17d7d946":"code","1a11bd54":"code","270f2d22":"code","44f6b39d":"code","d4d6fd8a":"code","820fb957":"code","1860d139":"code","7371e088":"code","cbc2d536":"code","3e3ba438":"code","8054df9f":"code","f49b9bc9":"code","24dd7898":"code","e1081bbf":"code","a848066a":"code","b4e2ccda":"code","6c723e33":"code","b4aca911":"code","6395b785":"code","b79be175":"code","bafe13a8":"code","5356177d":"code","f7ad7748":"code","a215ba03":"code","cda0604a":"code","85e5bc93":"code","77a8ed34":"code","b0c1fb18":"code","62da28ac":"code","6cb566dd":"code","4c8b9416":"code","911e3f9d":"code","16ef9781":"markdown","43323117":"markdown","ad554cae":"markdown","52a2e60a":"markdown","6040df0b":"markdown","eb8ef8d7":"markdown","125b36c9":"markdown","7eb41f07":"markdown","949eb449":"markdown","f1cf5011":"markdown","35ae287c":"markdown","e6c80a7c":"markdown","2a1d5299":"markdown","b454266b":"markdown","77cc510b":"markdown","b613715b":"markdown","6bc75173":"markdown","55e7409a":"markdown","55bb6999":"markdown","8e71c190":"markdown","cf65536c":"markdown","4a43a4ce":"markdown","3a909dcc":"markdown","c38d5534":"markdown","5e049363":"markdown","b98192e3":"markdown","6d9c8391":"markdown"},"source":{"36303763":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport sklearn as sk\n\n# for fast visualization\n%matplotlib inline\n\n#  retina display \n%config InlineBackend.figure_format = 'retina'\n\n# for minus\nmpl.rc('axes', unicode_minus=False)","e52696f4":"# train data\ntrain = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\n# test data\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\n# sample solution\nss = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","2e590a9f":"# let's look into train data\nprint(train.shape)\ntrain.describe()","2438f9a4":"# let's look into train data\nprint(test.shape)\ntest.describe()","96ab4aad":"# train data\nprint(\"=====Train Data - null values in Columns=====\", end = '\\n\\n')\n# Column\nprint(\"Counts of null values in a Column\")\nprint( train.isnull().sum(), end = '\\n\\n')\n\nprint(\"Percentage of null values in a Column\")\nprint( round( 100*train.isnull().sum()\/len(train), 2), end='\\n\\n' )\n\nprint(\"Average Percentage of null values in a Column\")\nprint( round(np.array(100*train.isnull().sum()\/len(train)).mean(),2))","575f4d6c":"# train data\nprint(\"=====Train Data - null values in Rows=====\", end = '\\n\\n')\n# Row\nprint(\"Counts of null values in a Row\")\nprint( train.isnull().sum(axis =1), end = '\\n\\n')\nprint()\n\nprint(\"Percentage of Counts of null values in a Row\")\nprint( round( 100*train.isnull().sum(axis =1)\/(train.shape[1]), 2))","1d9a4dbe":"# test data\nprint(\"=====Test Data - null valuesin Columns=====\", end = '\\n\\n')\n# Column \nprint(\"Counts of null values in a Column\")\nprint(test.isnull().sum(), end = '\\n\\n')\n\nprint(\"Percentage of Counts of null values in a Column\")\nprint(round(100*test.isnull().sum()\/(test.shape[0]), 2), end='\\n\\n')\n\nprint(\"Average Percentage of null values in a Column\")\nprint(round(np.array(100*test.isnull().sum()\/(test.shape[0])).mean(),2))","58e858ab":"# test data\nprint(\"=====Test Data - null values in Rows=====\", end = '\\n\\n')\n# Row \nprint(\"Counts of null values in a Row\")\nprint( test.isnull().sum(axis =1), end = '\\n\\n')\n\nprint(\"Percentage of Counts of null values in a Row\")\nprint(round( 100*test.isnull().sum(axis =1)\/(train.shape[1]),2))","1588292f":"# null values in column\ntrain.isnull().sum(axis = 0)","51be6748":"# null values in rows\ntrain.isnull().sum(axis = 1)","00897d94":"train['miss_cnt'] = train.isnull().sum(axis =1)\ntrain.miss_cnt","95844804":"test['miss_cnt'] = test.isnull().sum(axis =1)\ntest.miss_cnt","0c94ae97":"# miss_train\n\nmiss_train = train.loc[train.isnull().sum(axis =1) != 0, :]\nprint(miss_train.shape)\nmiss_train.head(3)","63cdb2d1":"# no_miss_train\n\nno_miss_train = train.loc[train.isnull().sum(axis=1) == 0, :]\nprint(no_miss_train.shape)\nno_miss_train.head(3)","17d7d946":"miss_train.shape[0] + no_miss_train.shape[0] == train.shape[0]","1a11bd54":"fig = plt.figure(figsize = (15, 71)) \ncols = train.columns.tolist()[1:119]\nfor num in cols:\n    plt.subplot(24, 5, cols.index(num) +1 )\n    sns.set_style(\"white\") \n    plt.title(num, size = 12)\n    a = sns.histplot(data = train, x =num, color = '#f9ba32', linewidth = 1.3, kde=True)\n    sns.histplot(data = test, x=num, color = '#426e86', linewidth = 1.3,kde=True)\n    plt.ylabel('')\n    plt.xlabel('') \n    \n    plt.xticks([])\n    plt.yticks([])\n    for lc in ['right', 'left', 'top']:\n        a.spines[lc].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n\nfig.tight_layout(h_pad = 3)\n\nplt.figtext(0.335, 1.02, 'Distribution of features', color = '#2f3131', size = 25) # moonspace \ud3f0\ud2b8 \uc88b\uc544\ud558\uc9c0\ub9cc, \ucf54\ub7a9\uc5d0 \uc5c6\uc73c\ubbc0\ub85c \uc0dd\ub7b5. \nplt.figtext(0.3, 1.01, 'train', color = '#f9ba32', size = 19)\nplt.figtext(0.66, 1.01, 'test', color = '#426e86', size = 19)\n\nplt.show()","270f2d22":"fig = plt.figure(figsize = (15, 71)) \ncols = train.columns.tolist()[1:119]\nfor num in cols:\n    plt.subplot(24, 5, cols.index(num) +1 )\n    sns.set_style(\"white\") \n    plt.title(num, size = 12)\n\n    a = sns.histplot(data= train.loc[train.claim == 0], x = num, color = '#f9ba32', linewidth = 1.3, kde=True)\n    sns.histplot(data= train.loc[train.claim == 1], x = num, color = '#426e86', linewidth = 1.3, kde=True)\n    plt.ylabel('')\n    plt.xlabel('') \n    plt.xticks([])\n    plt.yticks([])\n    for lc in ['right', 'left', 'top']:\n        a.spines[lc].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n\nfig.tight_layout(h_pad = 3)\n\nplt.figtext(0.335, 1.02, 'Distribution of features Whether train.claim == 0 or 1', color = '#2f3131', size = 25) \nplt.figtext(0.3, 1.01, 'No claim', color = '#f9ba32', size = 19) \nplt.figtext(0.66, 1.01, 'Claimed', color = '#426e86', size = 19)\n\nplt.show()","44f6b39d":"fig = plt.figure(figsize = (15, 71)) \ncols = train.columns.tolist()[1:119]\nfor num in cols:\n    plt.subplot(24, 5, cols.index(num) +1 )\n    sns.set_style(\"white\") # \uc774\uc05c white\n    plt.title(num, size = 12)\n\n    a = sns.histplot(data= miss_train.loc[miss_train.claim == 0], x = num, color = '#f9ba32', linewidth = 1.3, kde=True)\n    sns.histplot(data= miss_train.loc[miss_train.claim == 1], x = num, color = '#426e86', linewidth = 1.3, kde=True)\n    plt.ylabel('')\n    plt.xlabel('') \n    plt.xticks([])\n    plt.yticks([])\n    for lc in ['right', 'left', 'top']:\n        a.spines[lc].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n\nfig.tight_layout(h_pad = 3)\n\nplt.figtext(0.335, 1.02, 'miss_train - Distribution of features Whether claim == 0 or 1', color = '#2f3131', size = 25) \nplt.figtext(0.3, 1.01, 'No claim', color = '#f9ba32', size = 19) \nplt.figtext(0.66, 1.01, 'Claimed', color = '#426e86', size = 19)\n\nplt.show()","d4d6fd8a":"fig = plt.figure(figsize = (15, 71)) \ncols = train.columns.tolist()[1:119]\nfor num in cols:\n    plt.subplot(24, 5, cols.index(num) +1 )\n    sns.set_style(\"white\") \n    plt.title(num, size = 12)\n\n    a = sns.histplot(data= no_miss_train.loc[no_miss_train.claim == 0], x = num, color = '#f9ba32', linewidth = 1.3, kde=True)\n    sns.histplot(data= no_miss_train.loc[no_miss_train.claim == 1], x = num, color = '#426e86', linewidth = 1.3, kde=True)\n    plt.ylabel('')\n    plt.xlabel('') \n    plt.xticks([])\n    plt.yticks([])\n    for lc in ['right', 'left', 'top']:\n        a.spines[lc].set_visible(False)\n        a.spines['bottom'].set_linewidth(1.2)\n\nfig.tight_layout(h_pad = 3)\n\nplt.figtext(0.335, 1.02, 'No miss_train - Distribution of features Whether claim == 0 or 1', color = '#2f3131', size = 25) \nplt.figtext(0.3, 1.01, 'No claim', color = '#f9ba32', size = 19) \nplt.figtext(0.66, 1.01, 'Claimed', color = '#426e86', size = 19)\n\nplt.show()","820fb957":"# target value visualizing\n# referenced: https:\/\/wikidocs.net\/92114\n\nplt.figure(figsize = (9, 9))\nplt.title('Target Variable Counts', fontsize = 29)\nplt.pie(train.claim.value_counts(), labels=['No Claim', 'Claim'], textprops = {'fontsize':20},\n                                    shadow =True, startangle = 120, counterclock = False, \n                                    explode=[0.10, 0], colors =['#ff9999', '#ffc000'])\nplt.show()","1860d139":"# miss_train - miss_cnt\nplt.figure(figsize=(15, 9))\nplt.title(\"miss_train - hue: claim - miss_cnt\", fontsize= 16)\nsns.countplot(data=miss_train, x='miss_cnt', hue='claim')","7371e088":"# no_miss_train - miss_cnt\nplt.figure(figsize=(15, 9))\nplt.title(\"no_miss_train - hue: claim - miss_cnt\", fontsize= 16)\nsns.countplot(data=no_miss_train, x='miss_cnt', hue='claim')","cbc2d536":"miss_cnt_amount_claim =miss_train.groupby(miss_train.miss_cnt)['claim'].count()\nmiss_cnt_amount_claim ","3e3ba438":"miss_cnt_claim = pd.pivot_table(data=miss_train, index='miss_cnt', values='claim', aggfunc=np.sum)\nmiss_cnt_claim","8054df9f":"miss_cnt_ratio = pd.concat([miss_cnt_claim, miss_cnt_amount_claim], axis = 1 )\nmiss_cnt_ratio.columns = ['claim_counts', 'total_amount']\nmiss_cnt_ratio['percantage'] = miss_cnt_ratio.claim_counts \/ miss_cnt_ratio.total_amount\nmiss_cnt_ratio","f49b9bc9":"def mc_cat(val):\n    if val == 0:\n        return 0\n    elif val == 1 or val == 14:\n        return 1\n    else:\n        return 2","24dd7898":"train['mc_cat'] = train['miss_cnt'].apply(mc_cat)\ntest['mc_cat'] = test['miss_cnt'].apply(mc_cat)","e1081bbf":"train.head()","a848066a":"test.head()","b4e2ccda":"train[['miss_cnt', 'mc_cat']]","6c723e33":"test[['miss_cnt', 'mc_cat']]","b4aca911":"def get_outlier(df, column, weight=1.6):\n    quantile_25 = df[column].quantile(0.25)\n    quantile_75 = df[column].quantile(0.75)\n\n    IQR = quantile_75 - quantile_25\n    IQR_weight = IQR*weight\n  \n    lowest = quantile_25 - IQR_weight\n    highest = quantile_75 + IQR_weight\n  \n    outlier_idx = df[column][ (df[column] < lowest) | (df[column] > highest) ].index\n\n    return outlier_idx","6395b785":"train['out_cnt'] =0\nfor col in train.columns[1:119]:\n    outlier_idx = get_outlier(train, col)\n    train.loc[outlier_idx, 'out_cnt'] += 1","b79be175":"train['out_cnt']","bafe13a8":"test['out_cnt'] = 0\nfor col in test.columns[1:119]:\n    outlier_idx = get_outlier(test, col)\n    test.loc[outlier_idx, 'out_cnt'] += 1","5356177d":"test['out_cnt']","f7ad7748":"# Visualizing train data\nplt.figure(figsize=(12,8))\nsns.countplot(data=train, x='out_cnt', hue='claim')","a215ba03":"# Visualizing test data\nplt.figure(figsize=(12,8))\nsns.countplot(data=test, x='out_cnt')","cda0604a":"print(test.loc[test.out_cnt >= 9].shape[0])","85e5bc93":"X_num = train.iloc[:, 1:119]\ntest_num = test.iloc[:, 1:119]","77a8ed34":"X_num.describe()","b0c1fb18":"test_num.describe()","62da28ac":"# Source: https:\/\/www.kaggle.com\/seoltommy\/gmo-submission).\n\n# Data preprocessing functions\ndef get_stats_per_row(data):\n    features = [x for x in data.columns.values if x[0]==\"f\"] \n    \n    data['max_row'] = data[features].max(axis=1)\n    data['min_row'] = data[features].min(axis=1)\n    data['std'] = data[features].std(axis=1)\n    # If the difference between the mean and the median was large, the median function was used.\n    # If the difference is small the quantile function was used.\n    # Quantile function has better than mean function.\n    median_set = ['f9', 'f12', 'f26', 'f27', 'f28', 'f32', 'f33', 'f35', 'f62', 'f74', 'f82', 'f86', 'f98', 'f108', 'f116']\n    for col_name in features:\n        if col_name in median_set:\n            data[col_name].fillna(data[col_name].median(), inplace=True)\n        else:\n            data[col_name].fillna(data[col_name].quantile(0.75), inplace=True)\n            \n    # Multiply feature is the feature that multiply all values in a row\n    data['multiply'] = 1\n    for feature in features:\n        data['multiply'] = data[feature] * data['multiply']\n    \n    return data\n","6cb566dd":"X_num = get_stats_per_row(X_num)\ntest_num = get_stats_per_row(test_num)","4c8b9416":"X_num.describe()","911e3f9d":"test_num.describe()","16ef9781":"### Difference in scale\nStill same distribution, but difference in scale is more clearer!\n\n\n\n\n\n\n","43323117":"### RobustScaler instead of dropping outliers\nWe concluded that it is better to using RobustScaler instead of dropping outliers. It could be dangerous to drop outliers(row datas) becuase amount of outliers for drop is large. ","ad554cae":"### Overview on null values","52a2e60a":"### New column 'out_cnt'\n- 'out_cnt' for counting features in which one row could be an outlier data per row. Created this column in both train data and test data.\n- used 'for' in python for counting features from f1 to f118 and plus 1 whenever a row data becomes an outlier in each feature. ","6040df0b":"#### Visualized about Counts of Outliers in rows\nUsed Seaborn's countplot for visualizing values of 'out_cnt'.","eb8ef8d7":"### Load Dataset\n- One of kaggle Notbook's benefits is easy for loading datas as you know. ","125b36c9":"### Strong relation between the number of null values and claim. \nwe can transform this into categorical feature. You can check that a row data with 2 ~ 13 null values has  70 ~ 80% probability to claim and a row data with 1 or 14 null values has 50% probability to claim. My team made new categorical feature, 'mc_cat', whose categorical values are 0, 1, 2 on train data and test data by following rules. \n\n- 2 ~ 13 null values -> 2\n- 1, 14 null values -> 1\n- no null values -> 0 \n\n","7eb41f07":"## Lastly, Target column 'claim'","949eb449":"## Visulalize distribution between Train data 'claim' is 0 and 'claim' is 1. \nWe feel needs to check distribution of Train data when train's column 'claim'(target column) is 0 and Train data when train's column 'claim' is 1, after looking above hisplots above. We just did this in the same way.","f1cf5011":"## Visulalize Train & Test\n Checked distribution of each feature in train data and test data. We also used histplot of seaborn's with kde option.    \n We discriminate between both's distributions by giving two different line colors.","35ae287c":"## How about miss_train?\nVisulalized distribution between 'miss_train' data's column 'claim' is 0 and 'claim' is 1. ","e6c80a7c":"## How to treat null values and 'miss_cnt'?\n\nVisualize null values with seaborn's countplot in miss_train data and no_miss_train data first.","2a1d5299":"## And no_miss_train?\nVisulalized distribution between 'no_miss_train' data's column 'claim' is 0 and 'claim' is 1. ","b454266b":"### Import","77cc510b":"### Relation between number of null values and 'claim' columns.","b613715b":"# SMTM's TPS, Sep-2021 EDA\n\n\"SHOW ME THE MEDAL\", My Team. We scored 0.81785 with ideas shared in the 'Code' and 'Discussion' (including Team GMO). In this note, We share only EDA note and will share ideas of preprocessing and models. We studied some of EDA notebooks below.\n\nTPS September 2021 EDA :  https:\/\/www.kaggle.com\/dwin183287\/tps-september-2021-eda      \nCATBOOST BRONZE :  https:\/\/www.kaggle.com\/lonnieqin\/catboost-tabular-playground-prediction-sep-2021       \nTPS Soft Voting (XGB\/CB\/LGBM) :  https:\/\/www.kaggle.com\/dmitryuarov\/tps-soft-voting-xgb-cb-lgbm#Basic-informationn        \nTeam GMO's EDA :   https:\/\/www.kaggle.com\/seoltommy\/gmo-submission\n\nMy team used Google Colab, but writing this note for sharing. Reading this note, please consider for my writing skills in English. For these several years, I have been apart from English. By the way, I will start this EDA note, and hope this would be helpful. ","6bc75173":"## IQR\nIt is important how to treat outliers. Therefore, we extracted row-index of outliers in each feature and then counted the number of features in which one row could be an outlier data. And visualized this.","55e7409a":"### Seperate numeric columns and categorical columns","55bb6999":"### Let's Look into 'null values'\nOne of EDA nots we looked into says 'null values in each row' is related to 'claim' feature. You can see this insight in Discussion. Therefore, my team decided to look into null values further and further. ","8e71c190":"### Difference in scale\nStill same distribution, difference in scale got clearer. However, scale in 'Claimed' is more larger while above's is more smaller. This can be helpful if we can input this insight into ML. \n\n\n\n\n\n\n\n\n\n\n\n","cf65536c":"## miss_train, no_miss_train\nWe made 'miss_train' pandas DataFrame, which contains rows that have more than one null value in a row and made 'no_miss_train' pandas DataFrame, which contains rows that have more no null value in a row. ","4a43a4ce":"## column 'miss_cnt'\nFor further study, we also made new column, 'miss_cnt', which shows counts of null values in each row. ","3a909dcc":"### We should do same thing on test data.","c38d5534":"#### Checked that there is no data which we have missed. ","5e049363":"## Almost same distribution\nEvery feature's distributions between them are almost same and even scale is almost same, too.\n\n\n\n","b98192e3":"### GMO's EDA note.\nThis is why I seperated numerical columns and categorical columns. I found two things interesting and important from Team GMO's EDA note (https:\/\/www.kaggle.com\/seoltommy\/gmo-submission).  \n\n- How to impute missing data(NaN)    \n They looked into every column closely, and they imputed missing values with aporopriate value to each feature.\n\n- Column 'multiply'     \n I had no idea about this before checked their note. They said this feature was helpful in ML predicts. \n\n\n\n#### Check these below!\n \n\n","6d9c8391":"## Same Distribution shape and different Scale\nYou can see that distribution shape is the same between train and test, just both seem different in scale."}}