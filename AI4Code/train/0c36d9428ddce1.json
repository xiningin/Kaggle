{"cell_type":{"f3f595b3":"code","ac3e4acb":"code","6b36870e":"code","6d14979a":"code","389e6b5d":"code","960522da":"code","0f908f72":"code","d3a1dad9":"code","b44ae26f":"code","cc8fe97d":"code","dfa1499a":"code","2d22f066":"code","a4face36":"code","eb088ddd":"code","81e0b1b5":"code","99871496":"code","c57f0d84":"code","0db2e038":"code","4973aacc":"code","d752150c":"code","ee45f33b":"code","0f0d344e":"markdown","71f236d2":"markdown","977b94f6":"markdown","7bb0ff2d":"markdown","b087b0a9":"markdown","c6fb6ee3":"markdown","45d83ccc":"markdown","156811d3":"markdown","37388ad4":"markdown"},"source":{"f3f595b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","ac3e4acb":"data = pd.read_csv('..\/input\/voice.csv')","6b36870e":"data.info()","6d14979a":"data.label.value_counts()","389e6b5d":"# Convert label feature: female = 0 male = 1\ndata['label'] = [1 if i=='male' else 0 for i in data.label]\ndata.label.value_counts()","960522da":"# data selection\nx_data = data.drop(['label'], axis=1) # it is a matrix excluding label feature\ny = data.label.values # it is a vector wich contains only label feature","0f908f72":"x_data.head()","d3a1dad9":"y","b44ae26f":"# normalization of x_data and obtaining x\nx = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nx.head()","cc8fe97d":"# train test split (we split our data into 2 parts: train and test. Test part is 20% of all data)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42) # 0.2=20%","dfa1499a":"# take transpose of all these partial data\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T ","2d22f066":"# initialize w: weight and b: bias\ndimension = 20\ndef initialize(dimension):\n    w = np.full((dimension,1), 0.01)\n    b = 0.0\n    return w,b","a4face36":"# sigmoid function\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head\n\n# check sigmoid function\nsigmoid(0)","eb088ddd":"def cost(y_head, y_train):\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost_value = np.sum(loss)\/x_train.shape[1] # for scaling\n    return cost_value","81e0b1b5":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train)+b\n    y_head = sigmoid(z)\n    cost_value = cost(y_head, y_train)\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train, ((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = (np.sum(y_head-y_train))\/x_train.shape[1]\n    \n    return cost_value, derivative_weight, derivative_bias","99871496":"def logistic_regression(x_train, x_test, y_train, y_test, learning_rate, num_iteration):\n    w,b = initialize(dimension)\n    cost_list = []\n    index = []\n    for i in range(num_iteration):\n        cost_value, derivative_weight, derivative_bias = forward_backward_propagation(w,b,x_train,y_train)\n        \n        # updating weight and bias\n        w = w-learning_rate*derivative_weight\n        b = b-learning_rate*derivative_bias\n\n        if i % 10 == 0:\n            index.append(i)\n            cost_list.append(cost_value)\n            print('cost after iteration {}: {}'.format(i,cost_value))\n    \n    # in for loop above, we have obtained final values of parameters(weight and bias): machine has learnt them \n           \n    z_final = np.dot(w.T,x_test)+b\n    z_final_sigmoid = sigmoid(z_final) #z_final value after sigmoid function\n    \n    # prediction\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z_final_sigmoid is bigger than 0.5, our prediction is sign 1 (y_head_=1)\n    # if z_final_sigmoid is smaller than 0.5, our prediction is sign 0 (y_head_=0)\n    for i in range(z_final_sigmoid.shape[1]):\n        if z_final_sigmoid[0,i]<= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n            \n    # print test errors\n    print('test accuracy: {} %'.format(100-np.mean(np.abs(y_prediction-y_test))*100))\n    \n    # plot iteration vs cost function\n    plt.figure(figsize=(15,10))\n    plt.plot(index, cost_list)\n    plt.xticks(index, rotation='vertical')\n    plt.xlabel('number of iteration', fontsize=14)\n    plt.ylabel('cost', fontsize=14)\n    plt.show()          ","c57f0d84":"# run the program\n# Firstly, learning_rate and num_iteration are chosen randomly. Then it is tuned accordingly\nlogistic_regression(x_train, x_test, y_train, y_test, learning_rate=1.5, num_iteration=200)","0db2e038":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train.T,y_train.T)","4973aacc":"# prediction of test data\nlog_reg.predict(x_test.T)","d752150c":"# actual values\ny_test","ee45f33b":"print('test_accuracy: {}'.format(log_reg.score(x_train.T,y_train.T)))","0f0d344e":"**Derivatives of cost function wrt w and b**\n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}x(  y_head - y)^T$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (y_head-y)$$","71f236d2":"**Updating Weight and Bias**\n\n* alpha = learning rate\n* J: cost function\n* w: weight\n* b: bias\n    <a href=\"http:\/\/imgbb.com\/\"><img src=\"http:\/\/image.ibb.co\/hYTTJH\/8.jpg\" alt=\"8\" border=\"0\"><\/a>\n*  Using similar way, we update bias","977b94f6":"<a id=\"1\"><\/a> <br>\n# Logistic Regression\n* When we have binary classification( 0 and 1 outputs) we can use logistic regression","7bb0ff2d":"# LOGISTIC REGRESSION USING SKLEAR","b087b0a9":"At first glance, we see that 21th value of predicted data is 1, however it is 0 in actual data (y_test). There can be also other wrong predictions","c6fb6ee3":"  * Example of a cost function vs weight\n   <a href=\"http:\/\/imgbb.com\/\"><img src=\"http:\/\/image.ibb.co\/dAaYJH\/7.jpg\" alt=\"7\" border=\"0\"><\/a>","45d83ccc":"* For sigmoid function please visit\nhttps:\/\/en.wikipedia.org\/wiki\/Sigmoid_function","156811d3":"* Mathematical expression of log loss(error) function is: \n    <a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/image.ibb.co\/eC0JCK\/duzeltme.jpg\" alt=\"duzeltme\" border=\"0\"><\/a>","37388ad4":"* Computation graph of logistic regression\n<a href=\"http:\/\/ibb.co\/c574qx\"><img src=\"http:\/\/preview.ibb.co\/cxP63H\/5.jpg\" alt=\"5\" border=\"0\"><\/a>\n    * Parameters to be found are weights and bias\n    * Initial values of weight and bias parameters can be chosen arbitrarily\n    * For every iteration, we are going to calculate loss function\n    * Sum of the loss function will be our cost function\n    * We are going to update weight and bias parameters using derivative of cost function and a learning rate\n    * Learning rate is a hyperparameter that is chosen randomly and tuned afterward.\n    * After many iteratios, the cost wil be minimized and we will obtain final weight and bias parameters to be used (our machine will learn them)\n    * Using these final weight and bias parameters we are going to predict a given test data"}}