{"cell_type":{"4915601c":"code","70743396":"code","bae2caf7":"code","e416133a":"code","97b6c30d":"code","b2eaa79d":"code","8a2e6b7c":"code","4e56e978":"code","e110df8e":"code","ef6202d5":"markdown"},"source":{"4915601c":"# importing libs\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport string","70743396":"# Load file\nwith open('\/kaggle\/input\/word2vec-data\/text8\/text8') as f:\n    raw_txt = f.read()\n\n# Replace puntuations with corresponding english word\ndef relace_puntuations(text):\n    text = text.lower()\n    text = text.replace('.', ' PERIOD ')\n    text = text.replace(',', ' COMMA ')\n    text = text.replace('\"', ' QUOTATION_MARK ')\n    text = text.replace(';', ' SEMICOLON ')\n    text = text.replace('!', ' EXCLAMATION_MARK ')\n    text = text.replace('?', ' QUESTION_MARK ')\n    text = text.replace('(', ' LEFT_PAREN ')\n    text = text.replace(')', ' RIGHT_PAREN ')\n    text = text.replace('--', ' HYPHENS ')\n    text = text.replace('?', ' QUESTION_MARK ')\n    text = text.replace(':', ' COLON ')\n    return text\n\nraw_txt = relace_puntuations(raw_txt)\n\n# split the text into words\nwords = word_tokenize(raw_txt)\n\n# remove remaining punctuations\nwords = [ w for w in words if w not in string.punctuation ]\n\n# Remove stopwords ( or frequent words like the, of, a etc ) from words data (Or we could use Milkov Subsampling)\nfrom nltk.corpus import stopwords\nstopword_eng = set(stopwords.words('english'))\nwords = [w for w in words if w not in stopword_eng]\n\n# Remove less frequent words like \n# from collections import Counter\n# import random\n# word_freq = Counter(words)\n# less_freq_words = [ w for w, c in word_freq.items() if c <= 3]\n# words = [w for w in words if w not in less_freq_words]\n\n# create lookup dictionary for all words\nint2word = dict(enumerate(set(words)))\nword2int = {w:i for i, w in int2word.items()}","bae2caf7":"def cosine_similarity(embedding, valid_size=16, valid_window=100, device='cpu'):\n    # sim = (a . b) \/ |a||b|\n    \n    embed_vectors = embedding.weight\n    magnitudes = embed_vectors.pow(2).sum(dim=1).sqrt().unsqueeze(0)\n    valid_examples = np.array(random.sample(range(valid_window), valid_size\/\/2))\n    valid_examples = np.append(valid_examples,\n                               random.sample(range(1000,1000+valid_window), valid_size\/\/2))\n    valid_examples = torch.LongTensor(valid_examples).to(device)    \n    valid_vectors = embedding(valid_examples)\n    similarities = torch.mm(valid_vectors, embed_vectors.t())\/magnitudes\n        \n    return valid_examples, similarities","e416133a":"ecoded_words = [ word2int[w] for w in words]\n# get context word for center words\ndef get_surrounding_words(words, ctx_word_inx, window=5):\n    size = np.random.randint(1,window+1)\n    start_inx = ctx_word_inx - size if (ctx_word_inx - size) > 0 else 0\n    end_inx = start_inx + size\n    s_words = words[start_inx:ctx_word_inx] + words[ctx_word_inx+1:end_inx+1]\n    return s_words\n\ndef get_batch(words, batch_size=50, window=5):\n    n_words = len(words)\n    total_batch = n_words\/batch_size \n    words = words[:int(total_batch*batch_size)]\n    for idx in range(0, n_words, batch_size):\n        x, y = [], []\n        batch_words = words[idx:idx+batch_size]\n        for i in range(len(batch_words)):\n            s_words = get_surrounding_words(batch_words, i, window)\n            c_word = batch_words[i]\n            x.extend([c_word]*len(s_words))\n            y.extend(s_words)\n        yield x, y","97b6c30d":"class SkipGram(nn.Module):\n    def __init__(self, vocabulary_size, embedding_size):\n        super().__init__()\n        self.embedding = nn.Embedding(vocabulary_size, embedding_size)\n        self.fc = nn.Linear(embedding_size, vocabulary_size)\n        self.logsoft = nn.LogSoftmax(dim=1)\n        \n    def forward(self, ctx_word):\n        score = self.embedding(ctx_word)\n        prob = self.fc(score)\n        out = self.logsoft(prob)\n        return out","b2eaa79d":"def train(net, words, vocabulary_size, embedding_size,epoch=5 , batch_size=10 ,window_size=5, lr=0.0001):\n    net.train()\n    criterion = nn.NLLLoss()\n    optim = torch.optim.Adam(net.parameters(), lr=lr)\n    for e in range(epoch):        \n        steps = 0\n        for x, y in get_batch(words,batch_size,window_size):\n            steps +=1\n            inp = torch.LongTensor(x)\n            label = torch.LongTensor(y)        \n            out_prop = net(inp)        \n            loss = criterion(out_prop, label)\n            net.zero_grad()\n            loss.backward()\n            optim.step()\n            if steps % 500 == 0:\n                valid_examples, valid_similarities = cosine_similarity(net.embedding)\n                _, closest_idxs = valid_similarities.topk(6)           \n                for ii, valid_idx in enumerate(valid_examples):\n                    closest_words = [int2word[idx.item()] for idx in closest_idxs[ii]][1:]\n                    print(int2word[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n                print(\"...\")","8a2e6b7c":"n_vocab = len(int2word)\nn_embeddings = 100\nmodel = SkipGram(n_vocab, n_embeddings)\nprint(model)","4e56e978":"train(model, ecoded_words, n_vocab, n_embeddings, epoch=5)","e110df8e":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\nfrom sklearn.manifold import TSNE\nembeddings = model.embedding.weight.to('cpu').data.numpy()\nviz_words = 100\ntsne = TSNE()\nembed_tsne = tsne.fit_transform(embeddings[:viz_words, :])\nfig, ax = plt.subplots(figsize=(16, 16))\nfor idx in range(viz_words):\n    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n    plt.annotate(int2word[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)","ef6202d5":"## Skip-gram Word2Vec\nIn this notebook we will implement the Word2Vec algorithm using the skip-gram architecture using pytorch.\n### Readings\nI suggest reading these either beforehand or while you're working on this material.\n* A really good [conceptual overview](http:\/\/mccormickml.com\/2016\/04\/19\/word2vec-tutorial-the-skip-gram-model) of Word2Vec from Chris McCormick\n* [First Word2Vec paper](https:\/\/arxiv.org\/pdf\/1301.3781.pdf) from Mikolov et al.\n* [Neural Information Processing Systems, paper with improvements for Word2Vec](http:\/\/papers.nips.cc\/paper\/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) also from Mikolov et al.\n\n<img src=\"https:\/\/i.imgur.com\/Lk9trQu.png\" width=500 \/>\n"}}