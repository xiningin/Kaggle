{"cell_type":{"4bb875fc":"code","7e6c3fa5":"code","55164ddb":"code","6ba2a4a7":"code","7a13efe1":"code","7acafff0":"code","723cab36":"code","d37383da":"code","06935983":"code","8f5c99dc":"code","bbd2a27c":"code","de7ef929":"code","c03cf891":"code","e21b42fb":"code","9801964f":"code","da1b53c7":"code","c5c68824":"code","0bd3c844":"code","16faa089":"code","9596f9ee":"code","d932b889":"code","1c4e26a8":"code","8b47f858":"code","64025ca4":"code","e69bd56f":"code","89ce8bbb":"code","31d62153":"code","d0635fc7":"code","0f359512":"markdown","31857668":"markdown","92709c78":"markdown","9eacd4ba":"markdown","fd673ecb":"markdown","c8616234":"markdown","97b28fbb":"markdown","c15fab51":"markdown","298cae24":"markdown","b22e8010":"markdown","6e27b1c4":"markdown","af90cc92":"markdown","af8e3627":"markdown"},"source":{"4bb875fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# To create temporary directory, type in following in the Console\n# os.chdir(\"\/kaggle\/\")\n# !mkdir temp\n# os.listdir()\n\n# Any results we write to the current directory are saved as output in '\/kaggle\/working\/' directory\nprint()\nprint(os.listdir('..'))\nprint(os.listdir('\/kaggle\/input'))\nprint(os.listdir('\/kaggle\/working\/'))\n# print(os.listdir('\/kaggle\/temp\/'))\n","7e6c3fa5":"# Importing needed libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport h5py\nimport cv2\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\nfrom keras.utils import plot_model\nfrom keras.callbacks import LearningRateScheduler, ModelCheckpoint\n\nfrom sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\nfrom timeit import default_timer as timer\n\n# Check point\nprint('Libraries are imported successfully')\n","55164ddb":"# Building model for RGB datasets\n# RGB --> {128C5-P2-D30} --> {256C5-P2-D30} --> {512C5-P2-D30} --> {1024C3-P2-D30} --> 2048-D30 --> 43\n\n# Initializing model to be as linear stack of layers\nmodel = Sequential()\n\n# Adding first convolutional-pooling pair\nmodel.add(Conv2D(128, kernel_size=5, padding='same', activation='relu', input_shape=(48, 48, 3),\n                 kernel_initializer='random_normal',\n                 bias_initializer='zeros'))\nmodel.add(MaxPool2D())\nmodel.add(Dropout(0.3))\n\n# Adding second convolutional-pooling pair\nmodel.add(Conv2D(256, kernel_size=5, padding='same', activation='relu',\n                 kernel_initializer='random_normal',\n                 bias_initializer='zeros'))\nmodel.add(MaxPool2D())\nmodel.add(Dropout(0.3))\n\n# Adding third convolutional-pooling pair\nmodel.add(Conv2D(512, kernel_size=5, padding='same', activation='relu',\n                 kernel_initializer='random_normal',\n                 bias_initializer='zeros'))\nmodel.add(MaxPool2D())\nmodel.add(Dropout(0.3))\n\n# Adding fourth convolutional-pooling pair\nmodel.add(Conv2D(1024, kernel_size=3, padding='same', activation='relu',\n                 kernel_initializer='random_normal',\n                 bias_initializer='zeros'))\nmodel.add(MaxPool2D())\nmodel.add(Dropout(0.3))\n\n# Adding fully connected layers\nmodel.add(Flatten())\nmodel.add(Dense(2048, activation='relu',\n                kernel_initializer='random_normal',\n                bias_initializer='zeros'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(43, activation='softmax'))\n\n# Compiling created model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Saving model for RGB datasets\nmodel.save('model_ts_rgb_original.h5')\n\n\n\n# Building model for GRAY datasets\n# GRAY --> {128C5-P2-D30} --> {256C5-P2-D30} --> {512C5-P2-D30} --> {1024C3-P2-D30} --> 2048-D30 --> 43\n\n# Initializing model to be as linear stack of layers\nmodel = Sequential()\n\n# Adding first convolutional-pooling pair\nmodel.add(Conv2D(128, kernel_size=5, padding='same', activation='relu', input_shape=(48, 48, 1),\n                 kernel_initializer='random_normal',\n                 bias_initializer='zeros'))\nmodel.add(MaxPool2D())\nmodel.add(Dropout(0.3))\n\n# Adding second convolutional-pooling pair\nmodel.add(Conv2D(256, kernel_size=5, padding='same', activation='relu',\n                 kernel_initializer='random_normal',\n                 bias_initializer='zeros'))\nmodel.add(MaxPool2D())\nmodel.add(Dropout(0.3))\n\n# Adding third convolutional-pooling pair\nmodel.add(Conv2D(512, kernel_size=5, padding='same', activation='relu',\n                 kernel_initializer='random_normal',\n                 bias_initializer='zeros'))\nmodel.add(MaxPool2D())\nmodel.add(Dropout(0.3))\n\n# Adding fourth convolutional-pooling pair\nmodel.add(Conv2D(1024, kernel_size=3, padding='same', activation='relu',\n                 kernel_initializer='random_normal',\n                 bias_initializer='zeros'))\nmodel.add(MaxPool2D())\nmodel.add(Dropout(0.3))\n\n# Adding fully connected layers\nmodel.add(Flatten())\nmodel.add(Dense(2048, activation='relu',\n                kernel_initializer='random_normal',\n                bias_initializer='zeros'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(43, activation='softmax'))\n\n# Compiling created model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Saving 1st model for GRAY datasets\nmodel.save('model_ts_gray_original.h5')\n\n\n# Check point\nprint('2 models are saved successfully')\n","6ba2a4a7":"# Loading model\nmodel_rgb = load_model('\/kaggle\/working\/model_ts_rgb_original.h5')\nmodel_gray = load_model('\/kaggle\/working\/model_ts_gray_original.h5')\n\n# Check point\nprint('2 models are loaded successfully')\n","7a13efe1":"# Plotting model's layers in form of flowchart\nplot_model(model_rgb,\n           to_file='model_ts_rgb_original.png',\n           show_shapes=True,\n           show_layer_names=False,\n           rankdir='TB',\n           dpi=500)\n\nplot_model(model_gray,\n           to_file='model_ts_gray_original.png',\n           show_shapes=True,\n           show_layer_names=False,\n           rankdir='TB',\n           dpi=500)\n","7acafff0":"# Showing model's summary in form of table\nmodel_rgb.summary()\nprint()\nmodel_gray.summary()\n","723cab36":"# Showing dropout rate\nprint(model_rgb.layers[2].rate)\nprint(model_gray.layers[2].rate)\n\n# Showing strides for the 1st layer (convolutional)\nprint(model_rgb.layers[0].strides)\nprint(model_gray.layers[0].strides)\n\n# Showing strides for the 2nd layer (max pooling)\nprint(model_rgb.layers[1].strides)\nprint(model_gray.layers[1].strides)\n","d37383da":"# Defining lists to collect models in\nmodel_rgb = []\nmodel_gray = []\n\n\n# Loading models and appending them into lists\nfor i in range(2):\n    model_rgb.append(load_model('\/kaggle\/working\/model_ts_rgb_original.h5'))\n    \n    model_gray.append(load_model('\/kaggle\/working\/model_ts_gray_original.h5'))\n\n\n# Check point\nprint('4 separate models are successfully loaded')\n","06935983":"# Showing models' input shapes\nprint(model_rgb[0].layers[0].input_shape)\nprint()\nprint(model_gray[0].layers[0].input_shape)\n","8f5c99dc":"# Defining number of epochs\nepochs = 20\n\n\n# Defining schedule to update learning rate\nlearning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + 50), verbose=1)\n\n\n# Check point\nprint('Number of epochs and schedule for learning rate are set successfully')\n","bbd2a27c":"# Preparing list with datasets' names\ndatasets = ['dataset_ts_rgb_255_mean_original.hdf5',\n            'dataset_ts_rgb_255_mean_std_original.hdf5',\n            'dataset_ts_gray_255_mean_original.hdf5',\n            'dataset_ts_gray_255_mean_std_original.hdf5']\n\n\n# Defining list to collect results in\nh = []\n\n\n# Training model with all Traffic Signs datasets in a loop\nfor i in range(4):\n    # Opening saved Traffic Signs dataset from HDF5 binary file\n    # Initiating File object\n    # Opening file in reading mode by 'r'\n    with h5py.File('\/kaggle\/input\/pre-processing-of-traffic-signs-dataset\/' + datasets[i], 'r') as f:\n        # Extracting saved arrays for training by appropriate keys\n        # Saving them into new variables\n        x_train = f['x_train']  # HDF5 dataset\n        y_train = f['y_train']  # HDF5 dataset\n        # Converting them into Numpy arrays\n        x_train = np.array(x_train)  # Numpy arrays\n        y_train = np.array(y_train)  # Numpy arrays\n\n        # Extracting saved arrays for validation by appropriate keys\n        # Saving them into new variables\n        x_validation = f['x_validation']  # HDF5 dataset\n        y_validation = f['y_validation']  # HDF5 dataset\n        # Converting them into Numpy arrays\n        x_validation = np.array(x_validation)  # Numpy arrays\n        y_validation = np.array(y_validation)  # Numpy arrays\n    \n    \n    # Check point\n    print('Following dataset is successfully opened:        ', datasets[i])\n    \n    \n    # Preparing classes to be passed into the model\n    # Transforming them from vectors to binary matrices\n    # It is needed to set relationship between classes to be understood by the algorithm\n    # Such format is commonly used in training and predicting\n    y_train = to_categorical(y_train, num_classes = 43)\n    y_validation = to_categorical(y_validation, num_classes = 43)\n    \n    \n    # Check point\n    print('Binary matrices are successfully created:        ', datasets[i])\n \n\n    # Preparing filepath to save best weights\n    best_weights_filepath = 'w' + datasets[i][7:-5] + '.h5'\n       \n    # Defining schedule to save best weights\n    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n                                   save_weights_only=True,                                   \n                                   monitor='val_accuracy',\n                                   mode='max',\n                                   save_best_only=True,\n                                   period=1,\n                                   verbose=1)\n    \n    \n    # Check point\n    print('Schedule to save best weights is created:        ', datasets[i])\n\n    \n    # Checking if RGB dataset is opened\n    if i <= 1:\n        # Training RGB model with current dataset\n        temp = model_rgb[i].fit(x_train, y_train,\n                                batch_size=50,\n                                epochs=epochs,\n                                validation_data=(x_validation, y_validation),\n                                callbacks=[learning_rate, best_weights],\n                                verbose=1)\n\n        \n        # Adding results of model for current RGB dataset in the list\n        h.append(temp)\n        \n        \n        # Check points\n        print('Model for RGB is successfully trained on:        ', datasets[i])\n        print('Trained weights for RGB are saved successfully:  ', 'w' + datasets[i][7:-5] + '.h5')\n        print()\n    \n    # Checking if GRAY dataset is opened\n    elif i >= 2:\n        # Training GRAY model with current dataset\n        temp = model_gray[i-2].fit(x_train, y_train,\n                                   batch_size=50,\n                                   epochs=epochs,\n                                   validation_data=(x_validation, y_validation),\n                                   callbacks=[learning_rate, best_weights],\n                                   verbose=1)\n\n        \n        # Adding results of 1st model for current GRAY dataset in the list\n        h.append(temp)\n        \n        \n        # Check points\n        print('Model for GRAY is successfully trained on:       ', datasets[i])\n        print('Trained weights for GRAY are saved successfully: ', 'w' + datasets[i][7:-5] + '.h5')\n        print()\n","de7ef929":"# Resulted accuracies of all pre-processed Traffic Signs datasets\nfor i in range(4):\n    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n                                                    max(h[i].history['val_accuracy']),\n                                                    datasets[i][8:-5]))\n","c03cf891":"# Showing other parameters that history holds\nprint(h[0].params)\n","e21b42fb":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (12.0, 6.0)\nplt.rcParams['font.family'] = 'Times New Roman'\n\n\n# Plotting accuracies of all Traffic Signs datasets for 1st model\nplt.plot(h[0].history['val_accuracy'], '-o')\nplt.plot(h[1].history['val_accuracy'], '-o')\nplt.plot(h[2].history['val_accuracy'], '-o')\nplt.plot(h[3].history['val_accuracy'], '-o')\n\n\n# Showing legend\nplt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n            'gray_255_mean', 'gray_255_mean_std'],\n           loc='lower right',\n           fontsize='xx-large')\n\n\n# Giving name to axes\nplt.xlabel('Epoch', fontsize=16)\nplt.ylabel('Accuracy', fontsize=16)\n\n\n# Setting limit along Y axis\nplt.ylim(0.97, 0.9992)\n\n\n# Giving name to the plot\nplt.title('Validation accuracies', fontsize=16)\n\n\n# Saving plot\nplt.savefig('validation_model_ts_dataset_original.png', dpi=500)\n\n\n# Showing the plot\nplt.show()\n","9801964f":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (12.0, 6.0)\nplt.rcParams['font.family'] = 'Times New Roman'\n\n\n# Plotting training and validation losses of all Traffic Signs datasets for 1st model\nplt.plot(h[0].history['loss'], '-ob')\nplt.plot(h[1].history['loss'], '-og')\nplt.plot(h[2].history['loss'], '-or')\nplt.plot(h[3].history['loss'], '-oc')\n\nplt.plot(h[0].history['val_loss'], '-ob')\nplt.plot(h[1].history['val_loss'], '-og')\nplt.plot(h[2].history['val_loss'], '-or')\nplt.plot(h[3].history['val_loss'], '-oc')\n\n\n# Showing legend\nplt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n            'gray_255_mean', 'gray_255_mean_std'],\n           loc='center right',\n           fontsize='large')\n\n\n# Giving name to axes\nplt.xlabel('Epoch', fontsize=16)\nplt.ylabel('Loss', fontsize=16)\n\n\n# Giving name to the plot\nplt.title('Losses', fontsize=16)\n\n\n# Saving plot\nplt.savefig('losses_model_ts_dataset_original.png', dpi=500)\n\n\n# Showing the plot\nplt.show()\n","da1b53c7":"# Defining lists to collect models in\nmodel_rgb = []\nmodel_gray = []\n\n\n# Loading 1st model for Traffic Signs dataset\nfor i in range(2):\n    model_rgb.append(load_model('\/kaggle\/working\/model_ts_rgb_original.h5'))\n    model_gray.append(load_model('\/kaggle\/working\/model_ts_gray_original.h5'))\n\n\n# Check point\nprint('4 separate models are successfully loaded')\n","c5c68824":"# Showing models' input shapes\nprint(model_rgb[0].layers[0].input_shape)\nprint()\nprint(model_gray[0].layers[0].input_shape)\n","0bd3c844":"# Preparing list with weights' names\nweights = ['w_ts_rgb_255_mean_original.h5',\n           'w_ts_rgb_255_mean_std_original.h5',\n           'w_ts_gray_255_mean_original.h5',\n           'w_ts_gray_255_mean_std_original.h5']\n\n\n# Loading best weights for 1st model\nfor i in range(4):    \n    # Checking if it is RGB model\n    if i <= 1:\n        # loading and assigning best weights\n        model_rgb[i].load_weights('\/kaggle\/working\/' + weights[i])\n        \n        \n        # Check point\n        print('Best weights for RGB model are loaded and assigned  : ', weights[i])\n    \n    # Checking if it is GRAY model\n    elif i >= 2:\n        # loading and assigning best weights\n        model_gray[i-2].load_weights('\/kaggle\/working\/' + weights[i])\n        \n        \n        # Check point\n        print('Best weights for GRAY model are loaded and assigned : ', weights[i])\n","16faa089":"# Preparing list with datasets' names\ndatasets = ['dataset_ts_rgb_255_mean_original.hdf5',\n            'dataset_ts_rgb_255_mean_std_original.hdf5',\n            'dataset_ts_gray_255_mean_original.hdf5',\n            'dataset_ts_gray_255_mean_std_original.hdf5']\n\n\n# Defining variable to identify the best model\naccuracy_best = 0\n\n\n# Testing 1st model with all Traffic Signs datasets in a loop\nfor i in range(4):    \n    # Opening saved Traffic Signs dataset from HDF5 binary file\n    # Initiating File object\n    # Opening file in reading mode by 'r'\n    with h5py.File('\/kaggle\/input\/pre-processing-of-traffic-signs-dataset\/' + datasets[i], 'r') as f:\n        # Extracting saved arrays for testing by appropriate keys\n        # Saving them into new variables\n        x_test = f['x_test']  # HDF5 dataset\n        y_test = f['y_test']  # HDF5 dataset\n        # Converting them into Numpy arrays\n        x_test = np.array(x_test)  # Numpy arrays\n        y_test = np.array(y_test)  # Numpy arrays\n    \n    \n    # Check point\n    print('Dataset is opened :', datasets[i])\n    \n    \n    # Check point\n    # Showing shapes of loaded arrays\n    if i == 0:\n        print('x_test shape      :', x_test.shape)\n        print('y_test shape      :', y_test.shape)\n    \n    \n    # Checking if RGB dataset is opened\n    if i <= 1:\n        # Testing RGB model with current dataset\n        temp = model_rgb[i].predict(x_test)\n        \n        \n        # Check point\n        # Showing prediction shape and scores\n        if i == 0:\n            print('prediction shape  :', temp.shape)  # (3111, 43)\n            print('prediction scores :', temp[0, 0:5])  # 5 score numbers\n      \n    \n        # Getting indexes of maximum values along specified axis\n        temp = np.argmax(temp, axis=1)\n        \n        \n        # Check point\n        # Showing prediction shape after convertion\n        # Showing predicted and correct indexes of classes\n        if i == 0:\n            print('prediction shape  :', temp.shape)  # (3111,)\n            print('predicted indexes :', temp[0:10])\n            print('correct indexes   :', y_test[:10])\n        \n        \n        # Calculating accuracy\n        # We compare predicted class with correct class for all input images\n        # By saying 'temp == y_test' we create Numpy array with True and False values\n        # By function 'np.mean' we calculate mean value:\n        # all_True \/ (all_True + all_False)\n        accuracy = np.mean(temp == y_test)\n        \n        \n        # Check point\n        # Showing True and False matrix\n        if i == 0:\n            print('T and F matrix    :', (temp == y_test)[0:10])\n        \n        \n        # Check point\n        # Showing calculated accuracy\n        print('Testing accuracy  : {0:.5f}'.format(accuracy))\n        print()\n    \n    # Checking if GRAY dataset is opened\n    elif i >= 2:\n        # Testing GRAY model with current dataset\n        temp = model_gray[i-2].predict(x_test)\n        \n        \n        # Getting indexes of maximum values along specified axis\n        temp = np.argmax(temp, axis=1)\n        \n        \n        # Calculating accuracy\n        # We compare predicted class with correct class for all input images\n        # By saying 'temp == y_test' we create Numpy array with True and False values\n        # By function 'np.mean' we calculate mean value:\n        # all_True \/ (all_True + all_False)\n        accuracy = np.mean(temp == y_test)\n        \n        \n        # Check point\n        # Showing calculated accuracy\n        print('Testing accuracy  : {0:.5f}'.format(accuracy))\n        print()\n    \n    \n    # Identifying the best model\n    # Saving predicted indexes of the best model\n    if accuracy > accuracy_best:\n        # Updating value of the best accuracy\n        accuracy_best = accuracy\n        \n        # Saving predicted indexes of the best model into array\n        # Updating array with predicted indexes of the best model\n        y_predicted_best = temp\n    ","9596f9ee":"# Showing the main classification metrics of the best model\nprint(classification_report(y_test, y_predicted_best))\n","d932b889":"# Confusion matrix is a two dimensional matrix that visualizes the performance,\n# and makes it easy to see confusion between classes,\n# by providing a picture of interrelation\n\n# Each row represents a number of actual class  \n# Each column represents a number of predicted class  \n\n\n# Computing confusion matrix to evaluate accuracy of classification\nc_m = confusion_matrix(y_test, y_predicted_best)\n\n# Showing confusion matrix in form of Numpy array\nprint(c_m)\n","1c4e26a8":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n\n# Setting default size of the plot\n# Setting default fontsize used in the plot\nplt.rcParams['figure.figsize'] = (14.0, 14.0)\nplt.rcParams['font.size'] = 12\n\n\n# Implementing visualization of confusion matrix\ndisplay_c_m = ConfusionMatrixDisplay(c_m)\n\n\n# Plotting confusion matrix\n# Setting colour map to be used\ndisplay_c_m.plot(cmap='PuRd')\n# Other possible options for colour map are:\n# 'OrRd', 'autumn_r', 'Blues', 'cool', 'Greens', 'Greys', 'copper_r'\n\n\n# Setting fontsize for xticks and yticks\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\n\n\n# Setting fontsize for xlabels and ylabels\nplt.xlabel('Predicted label', fontsize=18)\nplt.ylabel('True label', fontsize=18)\n\n\n# Giving name to the plot\nplt.title('Confusion Matrix: Traffic Signs Dataset', fontsize=18)\n\n\n# Saving plot\nplt.savefig('confusion_matrix_ts_model.png', transparent=True, dpi=500)\n\n\n# Showing the plot\nplt.show()\n","8b47f858":"# Opening saved Mean Image for RGB Traffic Signs dataset\n# Initiating File object\n# Opening file in reading mode by 'r'\n# (!) On Windows, it might need to change\n# this: + '\/' +\n# to this: + '\\' +\n# or to this: + '\\\\' +\nwith h5py.File('\/kaggle\/input\/pre-processing-of-traffic-signs-dataset' + '\/' + \n               'mean_rgb_dataset_ts_original.hdf5', 'r') as f:\n    # Extracting saved array for Mean Image\n    # Saving it into new variable\n    mean_rgb = f['mean']  # HDF5 dataset\n    # Converting it into Numpy array\n    mean_rgb = np.array(mean_rgb)  # Numpy arrays\n\n\n# Opening saved Standard Deviation for RGB Traffic Signs dataset\n# Initiating File object\n# Opening file in reading mode by 'r'\n# (!) On Windows, it might need to change\n# this: + '\/' +\n# to this: + '\\' +\n# or to this: + '\\\\' +\nwith h5py.File('\/kaggle\/input\/pre-processing-of-traffic-signs-dataset' + '\/' + \n               'std_rgb_dataset_ts_original.hdf5', 'r') as f:\n    # Extracting saved array for Standard Deviation\n    # Saving it into new variable\n    std_rgb = f['std']  # HDF5 dataset\n    # Converting it into Numpy array\n    std_rgb = np.array(std_rgb)  # Numpy arrays\n\n\n# Opening saved Mean Image for GRAY Traffic Signs dataset\n# Initiating File object\n# Opening file in reading mode by 'r'\n# (!) On Windows, it might need to change\n# this: + '\/' +\n# to this: + '\\' +\n# or to this: + '\\\\' +\nwith h5py.File('\/kaggle\/input\/pre-processing-of-traffic-signs-dataset' + '\/' + \n               'mean_gray_dataset_ts_original.hdf5', 'r') as f:\n    # Extracting saved array for Mean Image\n    # Saving it into new variable\n    mean_gray = f['mean']  # HDF5 dataset\n    # Converting it into Numpy array\n    mean_gray = np.array(mean_gray)  # Numpy arrays\n\n\n# Opening saved Standard Deviation for GRAY Traffic Signs dataset\n# Initiating File object\n# Opening file in reading mode by 'r'\n# (!) On Windows, it might need to change\n# this: + '\/' +\n# to this: + '\\' +\n# or to this: + '\\\\' +\nwith h5py.File('\/kaggle\/input\/pre-processing-of-traffic-signs-dataset' + '\/' + \n               'std_gray_dataset_ts_original.hdf5', 'r') as f:\n    # Extracting saved array for Standard Deviation\n    # Saving it into new variable\n    std_gray = f['std']  # HDF5 dataset\n    # Converting it into Numpy array\n    std_gray = np.array(std_gray)  # Numpy arrays\n\n\n# Check points\n# Showing shapes of loaded Numpy arrays\nprint('RGB Mean Image          :', mean_rgb.shape)\nprint('RGB Standard Deviation  :', std_rgb.shape)\nprint('GRAY Mean Image         :', mean_gray.shape)\nprint('GRAY Standard Deviation :', std_gray.shape)\n","64025ca4":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (2.5, 2.5)\n\n\n\n# Reading image by OpenCV library\n# In this way image is opened already as Numpy array\n# (!) OpenCV by default reads images in BGR order of channels\n# (!) On Windows, it might need to change\n# this: + '\/' +\n# to this: + '\\' +\n# or to this: + '\\\\' +\nimage_ts_bgr = cv2.imread('\/kaggle\/input\/images-for-testing' + '\/' + 'ts_to_test.jpg')\n\n# Swapping channels from BGR to RGB by OpenCV function\nimage_ts_rgb = cv2.cvtColor(image_ts_bgr, cv2.COLOR_BGR2RGB)\n\n# Resizing image to 32 by 32 pixels size\nimage_ts_rgb = cv2.resize(image_ts_rgb,\n                              (48, 48),\n                              interpolation=cv2.INTER_CUBIC)\n\n# Check point\n# Showing loaded and resized image\nplt.imshow(image_ts_rgb)\nplt.show()\n\n\n\n# Implementing normalization by dividing image's pixels on 255.0\nimage_ts_rgb_255 = image_ts_rgb \/ 255.0\n\n# Implementing normalization by subtracting Mean Image\nimage_ts_rgb_255_mean = image_ts_rgb_255 - mean_rgb\n\n# Implementing preprocessing by dividing on Standard Deviation\nimage_ts_rgb_255_mean_std = image_ts_rgb_255_mean \/ std_rgb\n\n# Check points\n# Showing shape of Numpy array with RGB image\n# Showing some pixels' values\nprint('Shape of RGB image         :', image_ts_rgb.shape)\nprint('Pixels of RGB image        :', image_ts_rgb[:5, 0, 0])\nprint('RGB \/255.0                 :', image_ts_rgb_255[:5, 0, 0])\nprint('RGB \/255.0 => mean         :', image_ts_rgb_255_mean[:5, 0, 0])\nprint('RGB \/255.0 => mean => std  :', image_ts_rgb_255_mean_std[:5, 0, 0])\nprint()\n\n\n\n# Converting image to GRAY by OpenCV function\nimage_ts_gray = cv2.cvtColor(image_ts_rgb, cv2.COLOR_RGB2GRAY)\n\n# Extending dimension from (height, width) to (height, width, one channel)\nimage_ts_gray = image_ts_gray[:, :, np.newaxis]\n\n# Check point\n# Showing converted into GRAY image\nplt.imshow(image_ts_gray, cmap=plt.get_cmap('gray'))\nplt.show()\n\n\n\n# Implementing normalization by dividing image's pixels on 255.0\nimage_ts_gray_255 = image_ts_gray \/ 255.0\n\n# Implementing normalization by subtracting Mean Image\nimage_ts_gray_255_mean = image_ts_gray_255 - mean_gray\n\n# Implementing preprocessing by dividing on Standard Deviation\nimage_ts_gray_255_mean_std = image_ts_gray_255_mean \/ std_gray\n\n# Check points\n# Showing shape of Numpy array with GRAY image\n# Showing some pixels' values\nprint('Shape of GRAY image        :', image_ts_gray.shape)\nprint('Pixels of GRAY image       :', image_ts_gray[:5, 0, 0])\nprint('GRAY \/255.0                :', image_ts_gray_255[:5, 0, 0])\nprint('GRAY \/255.0 => mean        :', image_ts_gray_255_mean[:5, 0, 0])\nprint('GRAY \/255.0 => mean => std :', image_ts_gray_255_mean_std[:5, 0, 0])\n","e69bd56f":"# Extending dimension from (height, width, channels) to (1, height, width, channels)\nimage_ts_rgb_255_mean = image_ts_rgb_255_mean[np.newaxis, :, :, :]\nimage_ts_rgb_255_mean_std = image_ts_rgb_255_mean_std[np.newaxis, :, :, :]\n\nimage_ts_gray_255_mean = image_ts_gray_255_mean[np.newaxis, :, :, :]\nimage_ts_gray_255_mean_std = image_ts_gray_255_mean_std[np.newaxis, :, :, :]\n\n# Check points\n# Showing shapes of extended Numpy arrays\nprint('RGB \/255.0 => mean         :', image_ts_rgb_255_mean.shape)\nprint('RGB \/255.0 => mean => std  :', image_ts_rgb_255_mean_std.shape)\nprint()\nprint('GRAY \/255.0 => mean        :', image_ts_gray_255_mean.shape)\nprint('GRAY \/255.0 => mean => std :', image_ts_gray_255_mean_std.shape)\n","89ce8bbb":"# Defining function to plot bar chart with scores values\ndef bar_chart(scores, bar_title, show_xticks=True, labels=None):\n    # Arranging X axis\n    x_positions = np.arange(scores.size)\n\n    # Creating bar chart\n    barlist = plt.bar(x_positions, scores, align='center', alpha=0.6)\n\n    # Highlighting the highest bar\n    barlist[np.argmax(scores)].set_color('red')\n\n    # Giving labels to bars along X axis\n    if show_xticks:\n        plt.xticks(x_positions, labels, rotation=20, fontsize=15)\n\n    # Giving name to axes\n    plt.xlabel('Class', fontsize=20)\n    plt.ylabel('Value', fontsize=20)\n\n    # Giving name to bar chart\n    plt.title('Classification: ' + bar_title, fontsize=20)\n\n    # Showing bar chart\n    plt.show()\n\n\n# Check point\nprint('Function to plot Bar Chart is successfully defined')\n","31d62153":"# Preparing labels for Traffic Signs dataset\n# Getting Pandas dataFrame with labels\n# (!) On Windows, it might need to change\n# this: + '\/' +\n# to this: + '\\' +\n# or to this: + '\\\\' +\nlabels_ts = pd.read_csv('\/kaggle\/input\/traffic-signs-preprocessed' + '\/' + 'label_names.csv', sep=',')\n\n\n# Check point\n# Showing first 5 elements of the dataFrame\nprint(labels_ts.head())\nprint()\n\n\n# Showing class's name of the 1st element\nprint(labels_ts.loc[0, 'SignName'])\nprint()\n\n\n# Converting into Numpy array\nlabels_ts = np.array(labels_ts.loc[:, 'SignName']).flatten()\n\n\n# Check points\n# Showing size of Numpy array\n# Showing all elements of Numpy array\nprint('Total number of labels:', labels_ts.size)\nprint()\nprint(labels_ts)\n","d0635fc7":"# Magic function that renders the figure in a jupyter notebook\n# instead of displaying a figure object\n%matplotlib inline\n\n# Setting default size of the plot\nplt.rcParams['figure.figsize'] = (12, 7)\n\n\n\n# Testing RGB model trained on dataset: dataset_ts_rgb_255_mean.hdf5\n# Input image is preprocessed in the same way\n# Measuring classification time\nstart = timer()\nscores = model_rgb[0].predict(image_ts_rgb_255_mean)\nend = timer()\n\n# Scores are given as 43 numbers of predictions for each class\n# Getting only one class with maximum value\nprediction = np.argmax(scores)\n\n# Check points\n# Showing scores shape and values\n# Printing class index, label and time\nprint()\nprint('Scores shape        :', scores.shape)\nprint('Scores values       :', scores[0, 10:15])\nprint('Scores sum          :', scores[0].sum())\nprint('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\nprint('Class index         :', prediction)\nprint('Label               :', labels_ts[prediction])\nprint('Time                : {0:.5f}'.format(end - start))\n\n# Plotting bar chart with scores values\nbar_chart(scores[0],\n          bar_title='1st RGB model, ts_rgb_255_mean',\n          show_xticks=False)\n\n\n\n# Testing RGB model trained on dataset: dataset_ts_rgb_255_mean_std.hdf5\n# Input image is preprocessed in the same way\n# Measuring classification time\nstart = timer()\nscores = model_rgb[1].predict(image_ts_rgb_255_mean_std)\nend = timer()\n\n# Scores are given as 43 numbers of predictions for each class\n# Getting only one class with maximum value\nprediction = np.argmax(scores)\n\n# Check points\n# Showing scores shape and values\n# Printing class index, label and time\nprint()\nprint('Scores shape        :', scores.shape)\nprint('Scores values       :', scores[0, 10:15])\nprint('Scores sum          :', scores[0].sum())\nprint('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\nprint('Class index         :', prediction)\nprint('Label               :', labels_ts[prediction])\nprint('Time                : {0:.5f}'.format(end - start))\n\n# Plotting bar chart with scores values\nbar_chart(scores[0],\n          bar_title='1st RGB model, ts_rgb_255_mean_std',\n          show_xticks=False)\n\n\n\n# Testing GRAY model trained on dataset: dataset_ts_gray_255_mean.hdf5\n# Input image is preprocessed in the same way\n# Measuring classification time\nstart = timer()\nscores = model_gray[0].predict(image_ts_gray_255_mean)\nend = timer()\n\n# Scores are given as 43 numbers of predictions for each class\n# Getting only one class with maximum value\nprediction = np.argmax(scores)\n\n# Check points\n# Showing scores shape and values\n# Printing class index, label and time\nprint()\nprint('Scores shape        :', scores.shape)\nprint('Scores values       :', scores[0, 10:15])\nprint('Scores sum          :', scores[0].sum())\nprint('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\nprint('Class index         :', prediction)\nprint('Label               :', labels_ts[prediction])\nprint('Time                : {0:.5f}'.format(end - start))\n\n# Plotting bar chart with scores values\nbar_chart(scores[0],\n          bar_title='1st GRAY model, ts_gray_255_mean',\n          show_xticks=False)\n\n\n\n# Testing GRAY model trained on dataset: dataset_ts_gray_255_mean_std.hdf5\n# Input image is preprocessed in the same way\n# Measuring classification time\nstart = timer()\nscores = model_gray[1].predict(image_ts_gray_255_mean_std)\nend = timer()\n\n# Scores are given as 43 numbers of predictions for each class\n# Getting only one class with maximum value\nprediction = np.argmax(scores)\n\n# Check points\n# Showing scores shape and values\n# Printing class index, label and time\nprint()\nprint('Scores shape        :', scores.shape)\nprint('Scores values       :', scores[0, 10:15])\nprint('Scores sum          :', scores[0].sum())\nprint('Score of prediction : {0:.5f}'.format(scores[0][prediction]))\nprint('Class index         :', prediction)\nprint('Label               :', labels_ts[prediction])\nprint('Time                : {0:.5f}'.format(end - start))\n\n# Plotting bar chart with scores values\nbar_chart(scores[0],\n          bar_title='1st GRAY model, ts_gray_255_mean_std',\n          show_xticks=False)\n","0f359512":"# \u2714\ufe0f Confusion matrix","31857668":"# \u2728 Classification report","92709c78":"# \ud83d\udce5 Importing needed libraries","9eacd4ba":"# \ud83d\uddbc\ufe0f Testing on one image","fd673ecb":"# \u27b0 Designing and Saving Deep CNN model","c8616234":"* **Training** deep CNN on **\"original\"** version of TS for classification\n* **Dataset** with **\"original\"**, **\"light\"** & **\"hard\"** versions:\n[https:\/\/www.kaggle.com\/valentynsichkar\/traffic-signs-1-million-images-for-classification](https:\/\/www.kaggle.com\/valentynsichkar\/traffic-signs-1-million-images-for-classification)\n* **Notebook** with pre-processed **\"original\"** data: [https:\/\/www.kaggle.com\/valentynsichkar\/pre-processing-of-traffic-signs-dataset\/](https:\/\/www.kaggle.com\/valentynsichkar\/pre-processing-of-traffic-signs-dataset\/)","97b28fbb":"**Design**, **Train** & **Test** deep CNN for Image Classification.\n\n**Join** the course & enjoy new opportunities to get deep learning skills:\n\n\n[https:\/\/www.udemy.com\/course\/convolutional-neural-networks-for-image-classification\/](https:\/\/www.udemy.com\/course\/convolutional-neural-networks-for-image-classification\/?referralCode=12EE0D74A405BF4DDE9B)\n\n\n![](https:\/\/github.com\/sichkar-valentyn\/1-million-images-for-Traffic-Signs-Classification-tasks\/blob\/main\/images\/slideshow_classification.gif?raw=true)","c15fab51":"# \ud83d\udcc2 Defining separate 4 models for training","298cae24":"# \ud83d\udce5 Loading and Verifying model","b22e8010":"# \u26d4\ufe0f Deep CNN for Traffic signs Classification","6e27b1c4":"# \ud83e\uddee Calculating testing accuracies","af90cc92":"# \ud83c\udf93 Related course for classification tasks","af8e3627":"# \u27bf Training separately defined models"}}