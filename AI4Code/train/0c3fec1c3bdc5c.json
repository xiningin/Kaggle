{"cell_type":{"5cb9c1fc":"code","a8637c93":"code","e6d73c01":"code","f62560bd":"code","f6caece1":"code","97ee301a":"code","c6d0e726":"code","048aff93":"code","07f5fa14":"code","44ef46cc":"code","87ecad24":"code","2a749dc6":"code","10c3ecd0":"code","d24198e4":"code","fb996989":"code","0ac9eece":"code","fd8c1ba4":"code","a4ba3c27":"code","fe740c20":"code","16061294":"markdown","77a6d7bf":"markdown","5fdb8521":"markdown","05e9aefc":"markdown","5b4c94db":"markdown","296a4845":"markdown","2e255ea2":"markdown","30f1e1d9":"markdown","41934f2d":"markdown","cf18f1f2":"markdown","61d1f80b":"markdown","99ca7182":"markdown","4e84ca0b":"markdown","c04af9b8":"markdown","103cc978":"markdown","30d83b7f":"markdown"},"source":{"5cb9c1fc":"#importing dataset and libraries\nimport numpy as np \nimport pandas as pd \n\ndf = pd.read_csv(\"\/kaggle\/input\/star-dataset\/6 class csv.csv\")","a8637c93":"df.shape","e6d73c01":"df.head()","f62560bd":"#checking for missing values\ndf.isnull().sum()","f6caece1":"#different star types and Spectral Classes\ndf['Star type'].value_counts() , df['Spectral Class'].value_counts()","97ee301a":"import matplotlib.pyplot as plt\nimport seaborn as sns","c6d0e726":"#checking correlation between variables for PCA\nsns.heatmap(data = df.corr(), annot = True)","048aff93":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\n\ndf['Star_color'] = labelencoder.fit_transform(df['Star color'])\ndf['Spectral_Class'] = labelencoder.fit_transform(df['Spectral Class'])","07f5fa14":"features = df.drop(['Star type','Star color','Spectral Class'], axis = 1)\nlabels = df['Star type']","44ef46cc":"#scaling our training model\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nscaled_train_features = scaler.fit_transform(features)","87ecad24":"from sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(scaled_train_features)\nexp_variance = pca.explained_variance_ratio_","2a749dc6":"fig, ax = plt.subplots()\nax.bar(range(pca.n_components_), exp_variance)\nax.set_xlabel('Principal Component number')","10c3ecd0":"cum_exp_variance = np.cumsum(exp_variance)\n\nfig, ax = plt.subplots()\nax.plot(cum_exp_variance)\nax.axhline(y=0.85, linestyle=':')","d24198e4":"n_component = 2\n\npca = PCA(n_component, random_state=10)\npca.fit(scaled_train_features)\npca_projection = pca.transform(scaled_train_features)","fb996989":"from sklearn.model_selection import train_test_split\ntrain_features, test_features, train_labels, test_labels = train_test_split(pca_projection, labels, random_state=10)","0ac9eece":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state=10)\ndt.fit(train_features, train_labels)\npred_labels_tree = dt.predict(test_features)","fd8c1ba4":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(random_state=10)\nlogreg.fit(train_features, train_labels)\npred_labels_logit = logreg.predict(test_features)","a4ba3c27":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(random_state=10)\nclf.fit(train_features, train_labels)","fe740c20":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nkf = KFold(n_splits=10)\n\ntree = DecisionTreeClassifier()\nlogreg = LogisticRegression()\nclf = RandomForestClassifier()\n\ntree_score = cross_val_score(tree, pca_projection, labels, cv=kf)\nlogit_score = cross_val_score(logreg, pca_projection, labels, cv=kf)\nrt_score = cross_val_score(clf,pca_projection, labels, cv=kf)\n\n# Mean of all the score arrays\nprint(\"Decision Tree:\", np.mean(tree_score),\"Logistic Regression:\", np.mean(logit_score),\"Random Forest:\",np.mean(rt_score))","16061294":"Lets take a look at our dataset:","77a6d7bf":"# Model Building","5fdb8521":"# Introduction","05e9aefc":"**Random Forest Classifier**","5b4c94db":"We can assume n_components equal to 2 as about 85% of the variance can be explained, hence we perform PCA with number of components equal to 2.","296a4845":"**Logistic Regression**","2e255ea2":"Thanks for reading! Any Suggestions and improvements are welcomed.","30f1e1d9":"**Spliting Dataset**","41934f2d":"# Understanding the Data","cf18f1f2":"The features we have here are a few properties of a star:\n\n* Absolute Temperature (in K) \nTemperature of the Star in Kelvin\n* Relative Luminosity (L\/Lo)\nRelative Luminosity is the ratio of brightness (Luminosity) of the star with the brightness of the Sun.\n* Relative Radius (R\/Ro)\nRelative Radius is again the ratio of the radius of a star compared to the radius of the Sun.\n* Absolute Magnitude (Mv)\nAbsolute magnitude (M) is a measure of the luminosity of a celestial object, on an inverse logarithmic astronomical magnitude scale.\n* Star Color (white,Red,Blue,Yellow,yellow-orange etc)\nSelf explainatory. It is the colour of Star as it appears in the sky.\n* Spectral Class (O,B,A,F,G,K,,M)\nIt is basically a class in which a star falls based on most of the above features.\n![HRmetrics.jpg](attachment:HRmetrics.jpg)\n* Star Type (Red Dwarf, Brown Dwarf, White Dwarf, Main Sequence , SuperGiants, HyperGiants)\nType of star. We'll be trying to predict this value using the features","61d1f80b":"# Validation","99ca7182":"We'll be using KFold Cross validation to validate and compare all 3 of our models","4e84ca0b":"Today, our goal is to apply machine learning methods in Python to classify stars into types using multiple features.","c04af9b8":"**Decision Tree Classifier**","103cc978":"# PCA and Normalizing ","30d83b7f":"# Conclusion"}}