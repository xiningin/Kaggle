{"cell_type":{"f153e90d":"code","61c02f29":"code","84581eb2":"code","ad609df2":"code","71c82f26":"code","45988e55":"code","1ff464a0":"code","0bea4c76":"code","1be384fb":"code","d4a5e4e2":"code","6476cf5f":"code","90e2f641":"code","017f92c9":"code","b8bce1a8":"code","00c0833c":"code","f3319397":"code","f78a9c8b":"code","ac4cf62e":"code","ad7d6abb":"code","3071c1be":"code","87fcf822":"code","ec29cfa9":"code","a347ec4c":"code","7342a833":"code","1dcfb726":"code","005a8fe2":"code","de088d83":"code","88cc35af":"code","626037e0":"code","1d5d88a0":"code","738a811f":"code","90cf5ecb":"code","3748094b":"code","bb9f741b":"code","2a9b777c":"code","661f705e":"code","b4de4556":"markdown","31b09469":"markdown","4ad5d92b":"markdown","31dda819":"markdown","9fc71794":"markdown","d88f78a4":"markdown","7148aa39":"markdown","05438b0d":"markdown","317b9fb9":"markdown"},"source":{"f153e90d":"import pandas as pd, matplotlib.pyplot as plt, seaborn as sns, numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","61c02f29":"data= pd.read_csv('..\/input\/vehicle-dataset-from-cardekho\/car data.csv')","84581eb2":"data.info()","ad609df2":"data.isnull().sum()","71c82f26":"data.describe()","45988e55":"cat_col = [x for x in data.columns if data[x].dtype=='O']\nnum_col = [x for x in data.columns if x not in cat_col]","1ff464a0":"cat_col","0bea4c76":"cat_col.remove('Car_Name')","1be384fb":"for x in cat_col:\n    print(data[x].unique())","d4a5e4e2":"num_col","6476cf5f":"data= pd.get_dummies(data, columns=cat_col, drop_first= True)","90e2f641":"data.head()","017f92c9":"data['time'] = 2021- data['Year']","b8bce1a8":"data.drop(['Year', 'Car_Name'], axis=1, inplace= True)","00c0833c":"data.head()","f3319397":"num_col.remove('Year')","f78a9c8b":"num_col.append('time')","ac4cf62e":"num_col","ad7d6abb":"for i in num_col:\n    sns.distplot(data[i])\n    plt.title(i)\n    plt.xlabel(i)\n    plt.ylabel='count'\n    plt.show()","3071c1be":"data.head()","87fcf822":"x= data.iloc[:,1:]\ny= data.iloc[:,0]","ec29cfa9":"x.head()","a347ec4c":"y.head()","7342a833":"from sklearn.ensemble import ExtraTreesRegressor\nec= ExtraTreesRegressor()\nec.fit(x,y)","1dcfb726":"fig= plt.subplots(figsize=(10,6))\nimp= pd.Series(ec.feature_importances_, index=x.columns)\nsns.barplot(imp, imp.index)","005a8fe2":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,random_state=0)","de088d83":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor","88cc35af":"algos= {\n        'Decision Tree Regressor' : {'model' : DecisionTreeRegressor(),\n                                    'para' : {'criterion': ['mse', 'mae', 'friedman_mse'],\n                                             'splitter': ['best', 'random'],'min_samples_split':[2,5,10,7]}\n                                    },\n        'Random Forest Regressor' : {'model' : RandomForestRegressor(),\n                                     'para' :{'criterion' : ['mse', 'mae'],'max_depth' : [1,2,5,7],\n                                              'n_estimators':[10,25,50,100,200,250,400,500],\n                                              'max_features': ['auto', 'sqrt', 'log2']}\n                                    }\n        }","626037e0":"result=[]\nfrom sklearn.model_selection import RandomizedSearchCV\nfor algo, param in algos.items():\n    rs=  RandomizedSearchCV(param['model'], param['para'], n_iter= 5, cv=3, scoring='neg_mean_squared_error')\n    rs.fit(x_train,y_train)\n    ypred= rs.predict(x_test)\n    result.append(pd.Series({'model': algo,\n                            'score': rs.best_score_, \n                            'parameter': rs.best_params_}))","1d5d88a0":"pd.options.display.max_colwidth=100\nresults = pd.concat(result, axis= 1).T.set_index('model')","738a811f":"results","90cf5ecb":"rf2= RandomForestRegressor(n_estimators= 500, max_features= 'auto', max_depth= 7, criterion= 'mse')\nrf2.fit(x_train, y_train)\nypred2= rf2.predict(x_test)","3748094b":"sns.distplot(y_test-ypred2)","bb9f741b":"plt.scatter(y_test, ypred2)","2a9b777c":"from sklearn import metrics\n\nprint('MAE:', metrics.mean_absolute_error(y_test, ypred2))\nprint('MSE:', metrics.mean_squared_error(y_test, ypred2))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, ypred2)))","661f705e":"import pickle\npickle.dump(rf2,open('model.pkl','wb'))","b4de4556":"#### Separating Categorical and Numerical Features","31b09469":"#### Using RandomSearchCV instead of GridSearchCV  because it is much faster","4ad5d92b":"The numerical data is POSITIVELY SKEWED. Generally, we should normalise it but, since we'll be using RANDOM FOREST which doesn't \n\nrequire normalisation\/ scaling ,so, we'll skip it.","31dda819":"# CONCLUSION\n\nAs we can see from the above graphs, the model is performing well as there is a somewhat linear graph between ypred nd ytest.","9fc71794":"No empty value.","d88f78a4":"### FINDING THE MOST IMPORTANT FEATURES","7148aa39":"##### Removing the \"Year \" and \"Car Name\" and keeping the age of the car","05438b0d":"## EXPLORATORY DATA ANALYSIS","317b9fb9":"####  Saving as a Pickle file (helps in deployment)"}}