{"cell_type":{"9c83ae40":"code","dd44202a":"code","d3afe18f":"code","39609d20":"code","e37e158b":"code","11ad669e":"code","7d4619fb":"code","5bd49b6b":"code","30c35811":"code","29f07f6d":"code","6c218565":"code","7527163a":"code","d2a5d517":"code","ec681f11":"code","d735fefb":"code","5c70d2ff":"code","9e2f10ca":"code","6fcfd694":"code","3616a0e6":"code","cad08def":"code","cf7b2b6f":"code","3f5de8e4":"code","45e9f650":"code","5f23c507":"code","a9721bd0":"code","f729590f":"markdown","1ad1e2d5":"markdown","b29488b9":"markdown","6a9958db":"markdown","099fad02":"markdown","ec39f1bd":"markdown","8a02c14e":"markdown","7f592a63":"markdown","d29b1abb":"markdown","9532b4d4":"markdown","c5d7cd05":"markdown","88a91016":"markdown","04e2eaf8":"markdown","6b07cc23":"markdown","ececf707":"markdown","3582522a":"markdown","f217b528":"markdown","a8c970a1":"markdown","010d38e5":"markdown","b0a003a4":"markdown","490c5ea4":"markdown","154542bf":"markdown","c761bd2a":"markdown","9684e5a8":"markdown"},"source":{"9c83ae40":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \n\nimport tensorflow as tf ","dd44202a":"print(tf.__version__)","d3afe18f":"W_true = 2\nb_true = 0.5","39609d20":"x = np.linspace(0, 3, 130)\n\ny = W_true * x + b_true + np.random.randn(*x.shape) * 0.5","e37e158b":"plt.figure(figsize=(10, 8))\n\nplt.scatter(x, y)\n\nplt.xlabel('x') \nplt.ylabel('y') \n\nplt.title(\"Training Data\") \nplt.show() ","11ad669e":"class LinearModel:\n\n    # In the init method, we initialize the weight and bias. \n    # Both of these are trainable variables\n    def __init__(self):\n        self.weight = tf.Variable(np.random.randn(), name = \"W\")\n        self.bias = tf.Variable(np.random.randn(), name = \"b\") \n        \n    # As the class is callable, we define the call method, which will be invoked in the \n    # forward pass through this simple linear model\n    def __call__(self, x):\n        # Self.weight * x, that is the input + self.bias\n        return self.weight * x + self.bias","7d4619fb":"def loss(y, y_pred):\n    return tf.reduce_mean(tf.square(y - y_pred))","5bd49b6b":"def train(linear_model, x, y, lr = 0.01):\n    \n    with tf.GradientTape() as tape:\n        \n        y_pred = linear_model(x)\n        \n        current_loss = loss(y, y_pred)\n\n    d_weight, d_bias = tape.gradient(current_loss, \n                                     [linear_model.weight, linear_model.bias])\n    \n    linear_model.weight.assign_sub(lr * d_weight)\n    linear_model.bias.assign_sub(lr * d_bias)","30c35811":"linear_model = LinearModel()\n\nweights, biases = [], []\n\n# Training loop originally set to 10, 50 then 100\nepochs = 100\n\nlr = 0.15","29f07f6d":"for epoch_count in range(epochs):\n    \n    # append the current weight and bias of our model to the weights and biases array\n    weights.append(linear_model.weight.numpy()) \n    biases.append(linear_model.bias.numpy())\n    \n    # Calculate the real_loss by invoking the loss function on the actual \n    # y value and the predicted value from the model\n    real_loss = loss(y, linear_model(x))\n    \n    # invoke the train function to calculate gradients and update our\n    # weights and bias values\n    train(linear_model, x, y, lr = 0.12)\n    \n    # Print the epock count and loss value after every epoch\n    print(f\"Epoch count {epoch_count}: Loss value: {real_loss.numpy()}\")","6c218565":"plt.figure(figsize=(10, 8))\n\nplt.plot(range(epochs), weights, 'r', range(epochs), biases, 'b')\nplt.plot([W_true] * epochs, 'r--', [b_true] * epochs, 'b--')\n\nplt.legend(['W', 'b', 'true W', 'true b'])\nplt.show()","7527163a":"linear_model.weight.numpy(), linear_model.bias.numpy()","d2a5d517":"rmse = loss(y, linear_model(x))\n\nrmse.numpy()","ec681f11":"plt.figure(figsize=(10, 8))\n\nplt.plot(x, y, 'ro', label = 'Original data') \nplt.plot(x, linear_model(x), label = 'Fitted line') \n\nplt.title('Linear Regression') \n\nplt.legend() \nplt.show() ","d735fefb":"from tensorflow import keras\nfrom tensorflow.keras import layers","5c70d2ff":"x.shape, y.shape","9e2f10ca":"x = pd.DataFrame(x, columns = ['x'])     \ny = pd.DataFrame(y, columns = ['y'])     ","6fcfd694":"x.head()","3616a0e6":"y.head()","cad08def":"x.shape, y.shape","cf7b2b6f":"model = keras.Sequential([layers.Dense(1, input_shape = (1,), activation = 'linear')])\n\noptimizer = tf.keras.optimizers.SGD(learning_rate = 0.001)\n\nmodel.compile(loss = 'mse', metrics = ['mse'], optimizer = optimizer)","3f5de8e4":"model.fit(x, y, epochs = 100)","45e9f650":"y_pred = model.predict(x)","5f23c507":"plt.figure(figsize=(10, 8))\n\nplt.scatter(x, y, c = 'blue', label = 'Original data')\nplt.plot(x, y_pred, color = 'r', label = 'Fitted line')\n\nplt.title('Linear Regression') \nplt.legend() \nplt.show()","a9721bd0":"#End of Code","f729590f":"The final value for weight and bias for our simple linear model is 2.11 and .39. Your values may be slightly different from mine due to the random nature of your values. \n\nThe true values that we used for these parameters to generate our artificial dataset initially was Weight = 2\nBias   = 0.5","1ad1e2d5":"forward pass here applies a simple linear transformation to our input x\n\nwe'll define a function named loss to calculate the loss of our model\n\ny refers to the actual y value\ny_pred is the predictive value from the model\n\nWe calculate the square of the difference between y and y_pred and then use the tf.reduce_mean\n\nThis is the mean\u2011square\u2011error loss of our linear regression model","b29488b9":"Defining the model\n\nWe manually instantiate the weights and biases. \n\nWhich are the trainable parameters that we're going to find during the training process of our model. \n\nThe simple class is called LinearModel.","6a9958db":"Artifical dataset to perform simple linear regression\n\nactual weight = 2\nactual bias   = 0.5","099fad02":"We instantiate the linear model\n\nSet up 2 arrays to track the weights and biases across epochs\n\nRun for a total of 10, 50 then 100 epochs with a learning rate of 0.15","ec39f1bd":"Training function","8a02c14e":"Matplotlib plot generated will give us an idea of how our model parameters converge to their final values during the training process. \n\nOn the x axis, we plotted the number of epochs of training that we've done\nOn the y axis we plotted the weight and bias values. \n\nThe dotted lines represent the true values of W and b, which we have used to artificially generate our data. \n\nThe solid lines represent values for the weight and bias of our linear model during the different epochs of training. \n\nYou can see initially that the weight and bias values differ very much from the true values, but as we run epochs of training, they converge to the true values","7f592a63":"We use np.linspace to generate a few different values of x between 0 and 3.\n\nWe compute corresponding values of y by using W_true on b_true\nWe add an additional random element, which is generated for each y value using np.random.randn\n\nSo that our x and y values don't exactly fit the formula Wx + B.","d29b1abb":"We use model.predict to get predicted values from our model","9532b4d4":"### Using model.fit()","c5d7cd05":"### Note:\n    First give epochs = 10, show the graph, then epochs = 50 and epochs = 100 show the graph each time","88a91016":"Clearly a linear relationship exists between x and y. \n\nx is the cause of the explanatory variable for our simple regression model. \n\ny is the effect or the target of our regression model","04e2eaf8":"We use keras.Sequential and set up exactly one layer within my neural network. \n\nThe single layer here is a dense layer, which I instantiate using layers.Dense. \n\nThe input_shape refers to the number of attributes or features in our input. \n\nThis is simple regression. We have just one x value, so input_shape is 1.\n\nactivation is equal to linear = no activation function\n\nWe use SGD - stochastic gradient descent optimizer for calculating gradients and updating our weights and biases\n\nlearning_rate = 0.001\n\nmodel.compile method on a sequential model configures the learning parameters of the model. \n\nWe want to compute the mse loss function. The metrics that we are tracking is mse, and the optimizer that we use is our SGD optimizer","6b07cc23":"We set up a for loop to start our training. \nFor epoch_count and range(epochs), for every epoch","ececf707":"The final value of the mean squared error for our model is a 0.235","3582522a":"input argument = linear_model x & y values\nlr = learning rate\ncurrent loss = passing in y actual and y predicted to the loss function\n\nAfter we've computed the loss, we use tape.gradient to calculate the gradient of the current loss with respect to the trainable parameters of our model, which is the linear_model.weight and the linear_model.bias\n\nd_weight is the gradient of the loss with respect to the weights of our model\nd_bias is the gradient of the loss with respect to the bias parameters\n\nWe use the assign_sub operation and the learning rate, to subtract these gradients from the weights and biases of the linear model\n\nThe train method is invoked for every epoch of training. \n\nWe make one forward pass in each epoch, calculate gradients, and use the gradient to update the weights and biases of our model","f217b528":"We do this after lesson 3: Keras Sequential API","a8c970a1":"When we visualize the original dataset as a scatter plot, and our linear model in the form of a fitted line on the scatter plot. \n\nThe fitted line is quite close to the original data. \n\nTraining for a longer period of time generally tends to improve our machine learning model somewhat","010d38e5":"To train the model, we call the high\u2011level fit API. \n\nWe pass in the training features in our x data frame and the corresponding target values in our y data frame, and we'll train for 100 epochs","b0a003a4":"When building out models using TensorFlow, you won't be handcrafting your weights and biases like we did in the above code\n\nInstead, we'll use the built\u2011in layers that Keras has to offer\n\nWe continue without artificially generated data","490c5ea4":"Instead of handcrafting our model, let's set up a sequential model using Keras layers\n\nA sequential model is where the output of one layer feeds in as an input to the next layer","154542bf":"We've generated a total of 130 points. These make up the 130 records for training our model","c761bd2a":"We set up these x and y values in the form of a pandas data frame","9684e5a8":"Defining the loss function"}}