{"cell_type":{"4d86dbb2":"code","e96a2efe":"code","3820e8fd":"code","629da8e5":"code","fd0c3c1b":"code","47cd908a":"code","fbe5f239":"code","1020fc2a":"code","267a7327":"code","ca7abf2d":"code","e09e38ab":"code","6b813b77":"code","cc1bf889":"code","50ed94c3":"code","86bd61db":"code","a51516f3":"code","ac3c4b53":"code","59c674f7":"code","0388e867":"code","9b139723":"code","b3f44e47":"code","72f4454e":"markdown","1b96be9c":"markdown","1e827863":"markdown","2bd41a24":"markdown","4db3e355":"markdown","1b2ac412":"markdown","b3336872":"markdown","7c2ab8a5":"markdown","24d63296":"markdown","69fd841b":"markdown","c7f8f355":"markdown"},"source":{"4d86dbb2":"#Installing ibraries\n!pip install regressors","e96a2efe":"import os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom regressors import stats\nfrom sklearn import linear_model as lm\nimport statsmodels.formula.api as sm\nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nprint(os.listdir(\"..\/input\"))","3820e8fd":"#Data Preprocessing\ngp=pd.read_csv(\"..\/input\/googleplaystore.csv\")\n#print(gp.head()) #understanding what information is in the data\n\ngoogle=pd.read_csv(\"..\/input\/googleplaystore.csv\") #to be used later when creating scatterplots, need general data\ngoogle = google[[\"Size\", \"Installs\", \"Reviews\", \"Price\", \"Rating\"]] # Have to list out specific columns, because if I don't when I convert to float and then remove null values, the entire dataframe is emptry\ngoogle = google[google.Size != 'Varies with device'] #removing all rows where there is this string\ngoogle = google[~google.Size.str.contains(\"k\")] #removing all rows where there is a k\ngoogle['Installs'] = google['Installs'].str.replace(',', '') #Removed commas\ngoogle['Installs'] = google['Installs'].str.replace('+', '') #Removed + signs\ngoogle['Size'] = google['Size'].str.replace('M', '') #Removed M's\ngoogle['Size'] = google['Size'].str.replace('k', '') #Removed k's \ngoogle['Price'] = google['Price'].str.replace('$', '') #Removed $ signs\n\nfor x in google:\n    google[x] = pd.to_numeric(google[x], errors='coerce') #converted everything to float\nprint(google.dtypes) #checking to make sure it all converted correctly\nprint(google.head())\ngoogle = google.dropna() #removed rows will null values\nprint(google.isnull().values.any()) #data does have null values\n\n#Creating and cleaning Dataframe for linear modeling\ngp = gp[[\"Size\", \"Installs\"]]\ngp = gp[gp.Size != 'Varies with device'] #removing all rows where there is this string\ngp = gp[~gp.Size.str.contains(\"k\")] #removing all rows where there is a k\ngp['Installs'] = gp['Installs'].str.replace(',', '') #Removed commas\ngp['Installs'] = gp['Installs'].str.replace('+', '') #Removed + signs\ngp['Size'] = gp['Size'].str.replace('M', '') #Removed M signs\ngp['Size'] = gp['Size'].str.replace('k', '') #Removed K signs\n\nfor x in gp:\n    gp[x] = pd.to_numeric(gp[x], errors='coerce') #converted everything to float\n\nprint(gp.dtypes) #checking to make sure it all converted correctly\n\n#print(gp) #making sure that all the data looks good\n#print(gp.count()) #Making sure that we have enought points of data\n#print(gp.isnull().values.any()) #data does have null values\ngp = gp.dropna() #removed rows will null values\nprint(gp.isnull().values.any()) #data does have null values\nprint(gp.count()) #Making sure that we have enought points of data","629da8e5":"print(google.head())","fd0c3c1b":"#Linear Regression Interaction - Size and Number of Reviews\ninter = sm.ols(formula=\"Installs ~ Size*Reviews\",data=google).fit()\nprint(inter.summary())","47cd908a":"#Linear Regression Interaction - Size and Price\ninter = sm.ols(formula=\"Installs ~ Size*Price\",data=google).fit()\nprint(inter.summary())","fbe5f239":"#Linear Regression Interaction - Size and Rating\ninter = sm.ols(formula=\"Installs ~ Size*Rating\",data=google).fit()\nprint(inter.summary())","1020fc2a":"#Linear Regression Interaction - \ninter = sm.ols(formula=\"Installs ~ Rating*Price\",data=google).fit()\nprint(inter.summary())","267a7327":"#Linear Regression Interaction - \ninter = sm.ols(formula=\"Installs ~ Reviews*Rating\",data=google).fit()\nprint(inter.summary())","ca7abf2d":"#Linear Regression Interaction - Number of Reviews, Ratings, and App Size\ninter = sm.ols(formula=\"Installs ~ Reviews*Rating*Size\",data=google).fit()\nprint(inter.summary())","e09e38ab":"#Linear Regression Interaction - \ninter = sm.ols(formula=\"Installs ~ Reviews*Rating*Size*Price\",data=google).fit()\nprint(inter.summary())","6b813b77":"#Simple Linear Regression of Number of App Reviews\n#Adjusted R-Squared & P-vlaue genarated using Statsmodels\nres = sm.ols(formula=\"Installs ~ Reviews\",data=google).fit()\nprint(res.summary())","cc1bf889":"#Polynomial Transformation of Number of App Reviews\n#Adjusted R-Squared & P-vlaue genarated for cubic polynomial transformation\nres = sm.ols(formula=\"Installs ~ Reviews + I(Reviews*Reviews)+ I(Reviews*Reviews*Reviews)\",data=google).fit()\nprint(res.summary())","50ed94c3":"#Logarithmic Transformation of Number of App Reviews\nres = sm.ols(formula = \"Installs ~ np.log(Reviews)\",data=google).fit()\nprint(res.summary())","86bd61db":"#Simple Linear Regression of App Size\n#Adjusted R-Squared & P-vlaue genarated using Statsmodels\nres = sm.ols(formula=\"Installs ~ Size\",data=google).fit()\nprint(res.summary())","a51516f3":"#Polynomial Transformation of App Size\n#Adjusted R-Squared & P-vlaue genarated for cubic polynomial transformation\nres = sm.ols(formula=\"Installs ~ Size + I(Size*Size)+ I(Size*Size*Size)\",data=google).fit()\nprint(res.summary())","ac3c4b53":"#Logarithmic Transformation of App Size\nres = sm.ols(formula = \"Installs ~ np.log(Size)\",data=google).fit()\nprint(res.summary())","59c674f7":"from mlxtend.feature_selection import SequentialFeatureSelector as sfs","0388e867":"DF=pd.read_csv(\"..\/input\/googleplaystore.csv\") #to be used later when creating scatterplots, need general data\n\nFS = DF\nFS['TypeFree'] = FS['Type'].map({'Free': 1, 'Paid': 0})\nFS = FS[[\"Size\", \"Reviews\", \"Installs\",\"Price\", \"Rating\", \"TypeFree\", \"Category\", \"Genres\", \"Content Rating\"]] # Have to list out specific columns, because if I don't when I convert to float and then remove null values, the entire dataframe is emptry\nFS = FS.rename(columns={\"Content Rating\":\"Content_Rating\"})\nFS = FS[FS.Size != 'Varies with device'] #removing all rows where there is this string\nFS = FS[~FS.Size.str.contains(\"k\")] #removing all rows where there is a k\nFS['Installs'] = FS['Installs'].str.replace(',', '') #Removed commas\nFS['Installs'] = FS['Installs'].str.replace('+', '') #Removed + signs\nFS['Size'] = FS['Size'].str.replace('M', '') #Removed M's\nFS['Size'] = FS['Size'].str.replace('k', '') #Removed k's \nFS['Price'] = FS['Price'].str.replace('$', '') #Removed $ signs\nFS = FS.dropna()\n\nfor x in FS:\n    FS[x] = pd.to_numeric(FS[x], errors='coerce')\n    \nFS['Category'] = pd.get_dummies(DF['Category'])\nFS['Genres'] = pd.get_dummies(DF['Genres'])\nFS['Content_Rating'] = pd.get_dummies(DF['Content Rating'])\n\noutputDF = FS[\"Installs\"].copy()\n\ninputDF = FS[[\"Size\",\"Reviews\",\"Price\",\"Rating\",\"TypeFree\",\"Category\",\"Genres\",\"Content_Rating\"]].copy()\n\ninter = sm.ols(formula=\"Installs ~ Reviews*Rating*Size*Category*Content_Rating\",data=FS).fit()\nprint(inter.summary())\n\n# from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n\n# forwardModel = sfs(LinearRegression(),k_features=5,forward=True,verbose=2,cv=5,n_jobs=-1,scoring='r2')\n# forwardModel.fit(inputDF,outputDF)\n\n# print (forwardModel.k_feature_idx_)\n\n# print(forwardModel.k_feature_names_)","9b139723":"DF=pd.read_csv(\"..\/input\/googleplaystore.csv\") #to be used later when creating scatterplots, need general data\n\nFS = DF\nFS['TypeFree'] = FS['Type'].map({'Free': 1, 'Paid': 0})\nFS = FS[[\"Size\", \"Reviews\", \"Installs\",\"Price\", \"Rating\", \"TypeFree\", \"Category\", \"Genres\", \"Content Rating\"]] # Have to list out specific columns, because if I don't when I convert to float and then remove null values, the entire dataframe is emptry\nFS = FS.rename(columns={\"Content Rating\":\"Content_Rating\"})\nFS = FS[FS.Size != 'Varies with device'] #removing all rows where there is this string\nFS = FS[~FS.Size.str.contains(\"k\")] #removing all rows where there is a k\nFS['Installs'] = FS['Installs'].str.replace(',', '') #Removed commas\nFS['Installs'] = FS['Installs'].str.replace('+', '') #Removed + signs\nFS['Size'] = FS['Size'].str.replace('M', '') #Removed M's\nFS['Size'] = FS['Size'].str.replace('k', '') #Removed k's \nFS['Price'] = FS['Price'].str.replace('$', '') #Removed $ signs\nFS = FS.dropna()\n\nfor x in FS:\n    FS[x] = pd.to_numeric(FS[x], errors='coerce')\n    \nFS['Category'] = pd.get_dummies(DF['Category'])\nFS['Genres'] = pd.get_dummies(DF['Genres'])\nFS['Content_Rating'] = pd.get_dummies(DF['Content Rating'])\n\noutputDF = FS[\"Installs\"].copy()\n\ninputDF = FS[[\"Size\",\"Reviews\",\"Price\",\"Rating\",\"TypeFree\",\"Category\",\"Genres\",\"Content_Rating\"]].copy()\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\n\nbackwardModel = sfs(LinearRegression(),k_features=5,forward=False,verbose=2,cv=5,n_jobs=-1,scoring='r2')\nbackwardModel.fit(inputDF,outputDF)\n\nprint (backwardModel.k_feature_idx_)\n\nprint(backwardModel.k_feature_names_)\n","b3f44e47":"from sklearn.model_selection import KFold, cross_val_score,cross_val_predict, LeaveOneOut\nfrom sklearn.linear_model import LinearRegression \n\nDF=pd.read_csv(\"..\/input\/googleplaystore.csv\") #to be used later when creating scatterplots, need general data\n\nFS = DF\nFS['TypeFree'] = FS['Type'].map({'Free': 1, 'Paid': 0})\nFS = FS[[\"Size\", \"Reviews\", \"Installs\",\"Price\", \"Rating\", \"TypeFree\", \"Category\", \"Genres\", \"Content Rating\"]] # Have to list out specific columns, because if I don't when I convert to float and then remove null values, the entire dataframe is emptry\nFS = FS.rename(columns={\"Content Rating\":\"Content_Rating\"})\nFS = FS[FS.Size != 'Varies with device'] #removing all rows where there is this string\nFS = FS[~FS.Size.str.contains(\"k\")] #removing all rows where there is a k\nFS['Installs'] = FS['Installs'].str.replace(',', '') #Removed commas\nFS['Installs'] = FS['Installs'].str.replace('+', '') #Removed + signs\nFS['Size'] = FS['Size'].str.replace('M', '') #Removed M's\nFS['Size'] = FS['Size'].str.replace('k', '') #Removed k's \nFS['Price'] = FS['Price'].str.replace('$', '') #Removed $ signs\nFS = FS.dropna()\n\nfor x in FS:\n    FS[x] = pd.to_numeric(FS[x], errors='coerce')\n    \nFS['Category'] = pd.get_dummies(DF['Category'])\nFS['Genres'] = pd.get_dummies(DF['Genres'])\nFS['Content_Rating'] = pd.get_dummies(DF['Content Rating'])\n\n\n\n#Backward Selection\n#From the backward selection we noticed that Size, Reviews, Ratings, Category and Content_rating have an effect on the installs.\n#rmse using Leave one out\noutputDF = FS[\"Installs\"].copy()\n\ninputDF = FS[[\"Size\",\"Reviews\",\"Rating\",\"Category\",\"Content_Rating\"]].copy()\n\nmodel = LinearRegression()\nloocv = LeaveOneOut()\n\nrmse = np.sqrt(-cross_val_score(model, inputDF, outputDF, scoring=\"neg_mean_squared_error\", cv = loocv))\nprint(rmse.mean())\n\n#rmse using Kfold - 5 folds\nkf = KFold(5, shuffle=True, random_state=42).get_n_splits(inputDF)\nrmse = np.sqrt(-cross_val_score(model, inputDF, outputDF, scoring=\"neg_mean_squared_error\", cv = kf))\nprint(rmse.mean())\n\n#rmse using Kfold - 10 folds\nkf = KFold(10, shuffle=True, random_state=42).get_n_splits(inputDF)\nrmse = np.sqrt(-cross_val_score(model, inputDF, outputDF, scoring=\"neg_mean_squared_error\", cv = kf))\nprint(rmse.mean())","72f4454e":"**Part E - Cross Validation**\n* Do the following for each of the two final models (from Part C and Part D).\n* Do a Leave-One-Out Cross Validation and determine the MSE\n* Do a K-fold cross-validation and determine the MSE (try with both 5 and 10 folds)","1b96be9c":"**Part A - Interactions between variables**\n* Explore at least 5 interactions - at least one should be a three-way interaction. \n* Determine and identify your discoveries for each interaction. \n* Is that interaction significant? \n* Is a combination of interactions in the model significant?","1e827863":"We also performed cross validation using Leave-one-out validation and k-fold validation. Even though the rmse values are high, we noticed that they are low when we pick up other transformations.\n","2bd41a24":"As part of this exercise, we looked at the polynomial transformations (upto 3rd of cubic) between the installs and reviews . We noticed that this transformation has yielded a R squared value of 0.46 and the p value is 0. This is a significant transformation. However when we look at the logarithmic transformation  it yielded less R squared value. This concludes that logarithmic transformation is not that significant.\n","4db3e355":"**Part F - Final Model**\n* Based on all your findings, propose your final model. \n* Was the model what you would have expected based on your original understanding of data? Were there any surprise findings?\n* Based on this new model, would you make any changes to your Validation strategy proposed in your previous project deliverable?","1b2ac412":"As part of this exercise, we looked at interactions between App Size, Number of installs, App Reviews, Price and App Rating to predict the number of installs for a given app. Out of these interactions we noticed that there is a significant interaction between App size, reviews and Ratings. We noticed that the R-squared value between these three predictors is close to .589 and the p value is low. We added price to these interactions but it did not help the model significantly. So after looking at various interaction combinations, we noticed that size, reviews and ratings play a significant role in predicting the number of installs.","b3336872":"**Part C - Forward Model Selection**\n* Start with an intercept-only model. \n* Choose at least 8 predictors - some of them can be numerical transformations or interactions that you found were significant in Parts A and B. \n* Write a single python script that does forward selection automatically in loops and identifies the best possible model.","7c2ab8a5":"Our final model is an interaction model between the variables Size, Reviews, Rating, Category and Content rating. The r-squared value of this interaction is 0.565. The formula for the model will be:\n\n                 Installs ~ Reviews*Rating*Size*Category*Content_Rating\n\nWhen we first looked at the data, we found out that reviews, ratings , price and size might play a significant role in determining the number of installs. However we noticed that price does not play a significant role instead the category and content rating plays a significant role.This tells us that if the app has good content, rating and reviews users are willing to pay the price for the application.\n\nOur validation strategy started by looking at the interaction between installs and size. During this stage we found out that size has a significant outcome on the number of installs. However we could expand our validation strategy to couple of other variables such as ratings and reviews.\n","24d63296":"**Part B - Numerical Transformations**\n* Explore Polynomial (up to 3rd order or Cubic) or Logarithmic transformations of at least two variables. \n* Determine if these transformations are significant. ","69fd841b":"**Part D - Backward Model Selection**\n* Start with the same (at least) 8 predictors and build a full model. \n* Write a single python script that does backward selection automatically in loops and arrives at the best possible model.","c7f8f355":"For this exercise, we choose 8 variables to validate our model. The variables are Size, Reviews, Rating, TypeFree, Category, Genres and Content rating. For forward and backward selection to work, we had to transform few of these variables using hot encoding technique. After the transformation and running both forward and backward model selection, we got Size, Reviews, Rating, Category and Content rating as the most significant variables with a high score. This is in line when we did the interaction between variables and noticed that Size, Reviews and Ratings play a significant role for the predictor variable installs.\n"}}