{"cell_type":{"ce05362f":"code","36ec176e":"code","555b91a9":"code","7bc8a8fb":"code","1a77feff":"code","55cbdd47":"code","abc693d7":"code","52d423eb":"code","f4efa015":"code","3c63d51e":"code","a6ac21fb":"code","bb628857":"code","cb89a9ad":"code","bfb61cea":"code","0d6b3691":"code","dcce64cc":"markdown"},"source":{"ce05362f":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import (brier_score_loss, precision_score, recall_score,\n                             f1_score, roc_auc_score)\nfrom sklearn.calibration import CalibratedClassifierCV, calibration_curve","36ec176e":"submission = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\")\n\nlabels = train.pop('target')\ntrain_id = train.pop(\"id\")\ntest_id = test.pop(\"id\")","555b91a9":"print(train.shape)\nprint(test.shape)","7bc8a8fb":"labels = labels.values","1a77feff":"train.head(5)","55cbdd47":"#Thanks to https:\/\/www.kaggle.com\/martin1234567890\/logistic-regression\ndata = pd.concat([train, test])\ndata[\"ord_5a\"] = data[\"ord_5\"].str[0]\ndata[\"ord_5b\"] = data[\"ord_5\"].str[1]\ndata.drop([\"bin_0\", \"ord_5\"], axis=1, inplace=True)","abc693d7":"# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = data.columns.values.tolist()\nfor col in features:\n    if data[col].dtype in numerics: continue\n    categorical_columns.append(col)\n    \n# Encoding categorical features\nfor col in categorical_columns:\n    if col in data.columns:\n        le = LabelEncoder()\n        le.fit(list(data[col].astype(str).values))\n        data[col] = le.transform(list(data[col].astype(str).values))","52d423eb":"train = data.iloc[:train.shape[0], :]\ntest = data.iloc[train.shape[0]:, :]","f4efa015":"print(train.shape)\nprint(test.shape)","3c63d51e":"train.info()","a6ac21fb":"train = train.fillna(0)\nX_train, X_test, y_train, y_test = train_test_split(train, labels, test_size=0.99,\n                                                    random_state=42)","bb628857":"def model_pred(clf, X_test):\n    if hasattr(clf, \"predict_proba\"):\n        prob_pos = clf.predict_proba(X_test)[:, 1]\n    else:  # use decision function\n        prob_pos = clf.decision_function(X_test)\n        prob_pos = \\\n        (prob_pos - prob_pos.min()) \/ (prob_pos.max() - prob_pos.min())\n    return prob_pos","cb89a9ad":"#Thanks to https:\/\/scikit-learn.org\/stable\/modules\/calibration.html\ndef plot_calibration_curve(est, name, fig_index):\n    \"\"\"Plot calibration curve for est w\/o and with calibration. \"\"\"\n    # Calibrated with isotonic calibration\n    isotonic = CalibratedClassifierCV(est, cv=2, method='isotonic')\n\n    # Calibrated with sigmoid calibration\n    sigmoid = CalibratedClassifierCV(est, cv=2, method='sigmoid')\n\n    # Logistic regression with no calibration as baseline\n    lr = LogisticRegression(C=1., solver='lbfgs')\n\n    fig = plt.figure(fig_index, figsize=(10, 10))\n    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n    ax2 = plt.subplot2grid((3, 1), (2, 0))\n\n    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n    for clf, name in [(lr, 'Logistic'),\n                      (est, name),\n                      (isotonic, name + ' + Isotonic'),\n                      (sigmoid, name + ' + Sigmoid')]:\n        clf.fit(X_train, y_train)\n        y_pred = clf.predict(X_test)\n        prob_pos = model_pred(clf, X_test) \n        clf_score = brier_score_loss(y_test, prob_pos, pos_label=1)\n        print(\"%s:\" % name)\n        print(\"\\tBrier: %1.3f\" % (clf_score))\n        print(\"\\tPrecision: %1.3f\" % precision_score(y_test, y_pred))\n        print(\"\\tRecall: %1.3f\" % recall_score(y_test, y_pred))\n        print(\"\\tF1: %1.3f\\n\" % f1_score(y_test, y_pred))\n\n        fraction_of_positives, mean_predicted_value = \\\n            calibration_curve(y_test, prob_pos, n_bins=10)\n\n        ax1.plot(mean_predicted_value, fraction_of_positives, \"s-\",\n                 label=\"%s (%1.3f)\" % (name, clf_score))\n\n        ax2.hist(prob_pos, range=(0, 1), bins=10, label=name,\n                 histtype=\"step\", lw=2)\n        if name == \"Naive Bayes + Sigmoid\":\n            test_pred = model_pred(clf, test)\n            submission[\"id\"] = test_id\n            submission[\"target\"] = clf.predict_proba(test)[:, 1]\n            submission.to_csv(\"submission.csv\", index=False)\n\n    ax1.set_ylabel(\"Fraction of positives\")\n    ax1.set_ylim([-0.05, 1.05])\n    ax1.legend(loc=\"lower right\")\n    ax1.set_title('Calibration plots  (reliability curve)')\n\n    ax2.set_xlabel(\"Mean predicted value\")\n    ax2.set_ylabel(\"Count\")\n    ax2.legend(loc=\"upper center\", ncol=2)\n\n    plt.tight_layout()","bfb61cea":"# Plot calibration curve for Gaussian Naive Bayes\nplot_calibration_curve(GaussianNB(), \"Naive Bayes\", 1)","0d6b3691":"# Plot calibration curve for Linear SVC\nplot_calibration_curve(LinearSVC(max_iter=10000), \"SVC\", 2)\n\nplt.show()","dcce64cc":"This kernel basin on the materials from:\n* https:\/\/scikit-learn.org\/stable\/modules\/calibration.html\n* https:\/\/www.kaggle.com\/martin1234567890\/logistic-regression"}}