{"cell_type":{"bb3f038a":"code","d79d9f6c":"code","02b760f3":"code","097a7b5f":"code","b98ee95e":"code","6ef98796":"code","6281a592":"code","e8582cf8":"code","f353ec24":"code","990f895e":"code","c3f3c669":"code","efc9d391":"code","8a28151b":"code","036cd9f9":"code","10dca995":"code","c0960535":"code","eae883ab":"code","f31f5430":"code","6a40257f":"code","fdfc9aa3":"code","02ee98d8":"code","9447e742":"code","f33c966e":"code","9f0b83ef":"code","3e3ec328":"code","ed3d71a1":"code","db9d7ca2":"code","42f17b09":"code","b40a8bb7":"code","8b637076":"code","9a81a085":"code","d69ed36b":"code","a76f5871":"code","cee57567":"code","a277b449":"code","7d9239ff":"code","6dee352c":"code","e69bac60":"code","04acf198":"code","553298d5":"code","cc533f06":"markdown","bb320372":"markdown","f97e0362":"markdown","2e1b9a9c":"markdown","6474a630":"markdown","24c84040":"markdown","6b665832":"markdown","a88a7565":"markdown","d024a88c":"markdown","e2854f0c":"markdown","8bc5f1f3":"markdown","7117cd1e":"markdown","e37e09a4":"markdown","a80c45f1":"markdown","df606f8a":"markdown","230d00ff":"markdown","1b5a0dc0":"markdown","32bd8967":"markdown","d951e99b":"markdown","2fcefa7c":"markdown","f34167c8":"markdown","00f5f3c9":"markdown","2357ac49":"markdown","55479586":"markdown","83e7881b":"markdown","ef452448":"markdown","983830de":"markdown","058eafcd":"markdown","1ad5ce6c":"markdown","175389d0":"markdown","48835c14":"markdown","1e8f02ac":"markdown","c938ee7f":"markdown","2cbf2830":"markdown","89ad6e93":"markdown","0844b9d1":"markdown","eeb998a9":"markdown","7f3d5d9b":"markdown","30fd0b4f":"markdown","10f569c1":"markdown","6df59942":"markdown","8f8d253f":"markdown","648bf11c":"markdown","39041f94":"markdown","ad9e8b6c":"markdown"},"source":{"bb3f038a":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport random\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# for Machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import StackingClassifier\n\nimport pickle\n\n# To ignore unwanted warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# for styling\nplt.style.use('seaborn-whitegrid')","d79d9f6c":"# Load dataset into the memory\n\ndata = pd.read_csv('\/kaggle\/input\/patient-treatment-classification\/data-ori.csv')\ndata.head(3)","02b760f3":"# Label encoding\n# (1=in care patient), (0=out care patient)\ndata['SOURCE'] = data.SOURCE.replace({\"in\":1, 'out':0})\n\n\n# get all the features\nfeatures = [feat for feat in data.columns if feat !='SOURCE']\n\nX = data[features] # feature set\ny = data['SOURCE'] # target\n\n# Splitting data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n\n# train and test datasets dimensions\nX_train.shape, X_test.shape","097a7b5f":"print(f\"The dataset contains {data.shape[0]} rows and {data.shape[1]} columns\")\n\nnum_features = [feat for feat in features if data[feat].dtype != object]\ncat_features = [feat for feat in features if data[feat].dtype == object]\n\nprint(f\"Total number of features : {len(features)}\")\nprint(f\"Number of numerical features : {len(num_features)}\")\nprint(f\"Number of categorical features : {len(cat_features)}\\n\")","b98ee95e":"# Show the percentage of missing values in each feature\n\ndata.isna().mean().to_frame(name='Missing %')","6ef98796":"# To count number unique values in each feature\n\ndata.nunique().to_frame(name='# of unique values')","6281a592":"# Target distribution\n\nplt.figure(figsize=(7,4))\nfig = sns.histplot(x='SOURCE', data=data)","e8582cf8":"# Check no. of data points for each class\n\ndata.SOURCE.value_counts().to_frame(name='Count')","f353ec24":"# Describe statistics for each numerical features\n\ndata[num_features].describe()","990f895e":"# Boxplot for each numerical feature\n\nfig, axes = plt.subplots(9, 1, figsize=(8, 25))\nfor i, c in enumerate(num_features):\n    f = data[[c]].boxplot(ax=axes[i], vert=False)","c3f3c669":"# Kernel Density Estimation plot for each numerical feature\n\nfig, axes = plt.subplots(9, 1, figsize=(8, 25))\nfor i, c in enumerate(num_features):\n    f = data[[c]].plot(kind='kde',ax=axes[i])","efc9d391":"# Bar and count plot for Categorical feature\nfig, axes = plt.subplots(1, 2, figsize=(20, 6))\ndf = data['SEX'].value_counts()\nbarplot = df.plot(kind='pie', ax=axes[0], title='SEX', autopct=\"%.2f\", fontsize=14, ylabel='')\ncountplot = sns.countplot(x='SEX', data=data, ax=axes[1])","8a28151b":"# KDEplot for each numerical feature w.r.t target\n\nfig, axes = plt.subplots(5,2, figsize=(14,22))\naxes = [ax for axes_row in axes for ax in axes_row]\nfor i,c in enumerate(data[num_features]):\n    plot = sns.kdeplot(data=data, x=c, hue='SOURCE', multiple='fill', ax=axes[i])","036cd9f9":"# Relationship between target and mean of each numerical features\n\nfig, axes = plt.subplots(5,2, figsize=(14,24))\naxes = [ax for axes_row in axes for ax in axes_row]\nfor i,c in enumerate(data[num_features]):\n    df = data.groupby(\"SOURCE\")[c].mean()\n    plot = df.plot(kind='bar', title=c, ax=axes[i], ylabel=f'Mean {c}', color=('orange','skyblue'))","10dca995":"# Pearson Correlation of features w.r.t each other\n\ncorr_matt = data[num_features].corr(method='pearson')\nplt.figure(figsize=(8,8))\ncorr = sns.heatmap(corr_matt, annot=True, cmap='Greens', cbar=False)","c0960535":"# Pearson's Correlation of features w.r.t target\n\ncorr_matt = data.corr(method='pearson')[['SOURCE']].sort_values(by='SOURCE',ascending=False)\nplt.figure(figsize=(3,5))\ncorr = sns.heatmap(corr_matt, annot=True, cmap='Greens', cbar=False)","eae883ab":"# Mean of target==1 w.r.t SEX\n\ndf = data.groupby('SEX')['SOURCE'].mean().to_frame().reset_index()\nplot = df.plot(kind='bar', x='SEX', y='SOURCE', color=('orange', 'skyblue'))","f31f5430":"# Replace labels of SEX with binary numbers\n\nX_train.SEX.replace({'F':0, 'M':1}, inplace=True)\nX_test.SEX.replace({'F':0, 'M':1}, inplace=True)","6a40257f":"# MinMaxScaler will scale the features to a range of [0, 1]\n\nscaler = MinMaxScaler(feature_range=(0, 1))\n\nX_train[num_features] = scaler.fit_transform(X_train[num_features]) #fit and transform the train set\nX_test[num_features] = scaler.transform(X_test[num_features]) #transform the test test","fdfc9aa3":"# preprocessed data\n\nX_train.head(3)","02ee98d8":"# Remove least correlated features [MCH, MCHC, MCV]\n\nX_train.drop(['MCH', 'MCHC','MCV'], axis=1, inplace=True)\nX_test.drop(['MCH', 'MCHC','MCV'], axis=1, inplace=True)\n\n# final train set\nX_train.head(3)","9447e742":"tree = DecisionTreeClassifier(random_state=1)\ntree.fit(X_train, y_train)\n\nprint(\"Train accuracy : \", accuracy_score(y_train, tree.predict(X_train)))\nprint(\"Test accuracy : \", accuracy_score(y_test, tree.predict(X_test)))","f33c966e":"# Hyperparameters\ndistribution = {'max_depth': [4, 6, 8, 10, 12, 14, 16],\n                'criterion': ['gini', 'entropy'],\n                'min_samples_split': [2, 10, 20, 30, 40],\n                'max_features': [0.2, 0.4, 0.6, 0.8, 1],\n                'max_leaf_nodes': [8, 16, 32, 64, 128,256],\n                'class_weight': [{0: 1, 1: 2}, {0: 1, 1: 3}, {0: 1, 1: 4}, {0: 1, 1: 5}]\n               }\n\n# Random search for best hyperparameters\nsearch = RandomizedSearchCV(DecisionTreeClassifier(random_state=1),\n                         distribution,\n                         scoring='accuracy',\n                         cv=3,\n                         verbose=1,\n                         random_state=1,\n                         n_iter=30)\n\nsearch.fit(X_train, y_train)\n\n# Best parameters for DT classifier\nsearch.best_params_","9f0b83ef":"# Retrain with best model\n\nbest_tree = search.best_estimator_\n\nbest_tree.fit(X_train, y_train)\nprint(\" Best train accuracy : \", accuracy_score(y_train, best_tree.predict(X_train)))\nprint(\" Best test accuracy : \", accuracy_score(y_test, best_tree.predict(X_test)))","3e3ec328":"print(classification_report(y_test, best_tree.predict(X_test)))","ed3d71a1":"logreg = LogisticRegression()\n\nlogreg.fit(X_train, y_train)\n\nprint(\"Train accuracy : \", accuracy_score(y_train, logreg.predict(X_train)))\nprint(\"Test accuracy : \", accuracy_score(y_test, logreg.predict(X_test)))","db9d7ca2":"# Hyperparameters\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n              'penalty':['l1', 'l2', 'elasticnet', 'none'],\n              'fit_intercept':[True, False],\n              'max_iter':[100, 200, 300],\n              'class_weight': [{0: 1, 1: 1}, {0: 1, 1: 2}, {0: 1, 1: 4}, {0: 1, 1: 5}]\n             }\n\n# Random search for best hyperparameters\nsearch = RandomizedSearchCV(LogisticRegression(random_state=1),\n                         param_grid,\n                         scoring='accuracy',\n                         cv=3,\n                         verbose=1,\n                         random_state=1,\n                         n_iter=30)\n\nsearch.fit(X_train, y_train)\n\n# Best parameters for Logistic regression classifier\nsearch.best_params_","42f17b09":"# Retrain with best model\n\nbest_logreg = search.best_estimator_\n\nbest_logreg.fit(X_train, y_train)\nprint(\"Best train accuracy : \", accuracy_score(y_train, best_logreg.predict(X_train)))\nprint(\"Best test accuracy : \", accuracy_score(y_test, best_logreg.predict(X_test)))","b40a8bb7":"print(classification_report(y_test, best_logreg.predict(X_test)))","8b637076":"svc = SVC(random_state=1)\n\nsvc.fit(X_train, y_train)\n\nprint(\"Train accuracy : \", accuracy_score(y_train, svc.predict(X_train)))\nprint(\"Test accuracy : \", accuracy_score(y_test, svc.predict(X_test)))","9a81a085":"# Hyperparameters\nparam_grid = {'C': [0.1, 1, 10, 100, 1000], \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n              'kernel': ['linear', 'rbf', 'poly'],\n              'degree':[0, 1, 2, 3, 4, 5, 6]\n             } \n\n# Random search for best hyperparameters\nsearch = RandomizedSearchCV(SVC(random_state=1),\n                         param_grid,\n                         scoring='accuracy',\n                         cv=3,\n                         verbose=1,\n                         random_state=1,\n                         n_iter=30)\n\nsearch.fit(X_train, y_train)\n\n# Best parameters for Support vector classifier\nsearch.best_params_","d69ed36b":"# Retrain with best model\n\nbest_svc = search.best_estimator_\n\nbest_svc.fit(X_train, y_train)\nprint(\"Best train accuracy : \", accuracy_score(y_train, best_svc.predict(X_train)))\nprint(\"Best test accuracy : \", accuracy_score(y_test, best_svc.predict(X_test)))","a76f5871":"print(classification_report(y_test, best_svc.predict(X_test)))","cee57567":"forest = RandomForestClassifier(random_state=1)\n\nforest.fit(X_train, y_train)\n\nprint(\"Train accuracy : \", accuracy_score(y_train, forest.predict(X_train)))\nprint(\"Test accuracy : \", accuracy_score(y_test, forest.predict(X_test)))","a277b449":"# Hyperparameters\nparams_grid = {'bootstrap': [True, False],\n             'max_depth': [2, 5, 10, 20, None],\n             'max_features': ['auto', 'sqrt'],\n             'min_samples_leaf': [1, 2, 4],\n             'min_samples_split': [2, 5, 10],\n             'n_estimators': [50, 100, 150, 200]}\n\n# Random search for best hyperparameters\nsearch = RandomizedSearchCV(RandomForestClassifier(random_state=1),\n                         params_grid,\n                         scoring='accuracy',\n                         cv=3,\n                         verbose=1,\n                         random_state=1,\n                         n_iter=20)\n\nsearch.fit(X_train, y_train)\n\n# Best parameters for Random forest classifier\nsearch.best_params_","7d9239ff":"# Retrain with best model\n\nbest_forest = search.best_estimator_\n\nbest_forest.fit(X_train, y_train)\nprint(\"Best train accuracy : \", accuracy_score(y_train, best_forest.predict(X_train)))\nprint(\"Best test accuracy : \", accuracy_score(y_test, best_forest.predict(X_test)))","6dee352c":"print(classification_report(y_test, best_forest.predict(X_test)))","e69bac60":"stack = StackingClassifier(estimators=[('best tree classifier', best_tree),\n                                       ('best logreg', best_logreg),\n                                       ('best svc', best_svc),\n                                       ('best forest classifier', best_forest)],\n                           \n                           final_estimator=LogisticRegression(),\n                           passthrough=True)\n\nstack.fit(X_train, y_train)\n\nprint(\"Train accuracy : \", accuracy_score(y_train, stack.predict(X_train)))\nprint(\"Test accuracy : \", accuracy_score(y_test, stack.predict(X_test)))","04acf198":"print(classification_report(y_test, stack.predict(X_test)))","553298d5":"# final features\nfeatures = ['HAEMATOCRIT', 'HAEMOGLOBINS', 'ERYTHROCYTE', 'LEUCOCYTE','THROMBOCYTE', 'AGE', 'SEX']\nnum_features = ['HAEMATOCRIT', 'HAEMOGLOBINS', 'ERYTHROCYTE', 'LEUCOCYTE','THROMBOCYTE', 'AGE']\n\nX = data[features] # feature set\ny = data['SOURCE'] # target\n\n# Splitting data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n\n# Label encoding\nX_train.SEX.replace({'M':1, 'F':0}, inplace=True)\nX_test.SEX.replace({'M':1, 'F':0}, inplace=True)\n\n# Feature scaling\nscaler = MinMaxScaler(feature_range=(0, 1))\n\nX_train[num_features] = scaler.fit_transform(X_train[num_features])\nX_test[num_features] = scaler.transform(X_test[num_features])\n\n# Train the final model again!\nmodel = StackingClassifier(estimators=[('best tree classifier', best_tree),\n                                       ('best logreg', best_logreg),\n                                       ('best svc', best_svc),\n                                       ('best forest classifier', best_forest)],\n                           \n                           final_estimator=LogisticRegression(),\n                           passthrough=True)\n\nmodel.fit(X_train, y_train)\nprint(\"Final model trained Successfully!\")\n\n# Pickle scaler object\nwith open(\"scaler.pkl\", 'wb') as file:\n    pickle.dump(scaler, file)\n    \n# Pickle model object\nwith open(\"model.pkl\", 'wb') as file:\n    pickle.dump(model, file)\n\nprint(\"Pickled and Saved Successfully!\")","cc533f06":"### Dataset Overview","bb320372":"# 7. Feature Engineering","f97e0362":"#### Bar plot","2e1b9a9c":"### 6.1.1 Target \nname : SOURCE<br>\nThere are two classes in the target\n- 0 : Outcare patient\n- 1 : Incare patient","6474a630":"# 9. Machine Learning","24c84040":"Observation : \n- data points belongs to class 0 : 1992\n- data points belongs to class 1 : 1317\n- Not an imbalanced dataset","6b665832":"### Label encoding","a88a7565":"# 2. About Data\nThe dataset contains Electronic Health Record predictions collected from a private Hospital in Indonesia. It contains the patient's laboratory test results used to determine the next patient's treatment whether <b>in care<\/b> or <b>out care.<\/b>\n\n### Attribute information\n- HAEMATOCRIT : Patient laboratory test result of haematocrit\n\n- HAEMOGLOBINS : Patient laboratory test result of haemoglobins\n\n- ERYTHROCYTE : Patient laboratory test result of erythrocyte\n\n- LEUCOCYTE : Patient laboratory test result of leucocyte\n\n- THROMBOCYTE : Patient laboratory test result of thrombocyte\n\n- MCH : Patient laboratory test result of MCH\n\n- MCHC : Patient laboratory test result of MCHC\n\n- MCV : Patient laboratory test result of MCV\n\n- AGE : Patient age\n\n- SEX : Patient gender\n\n- SOURCE : Target ( Binary :in\/out )\n\n#### Acknowledgements\nThis dataset was originally downloaded from Mendeley Data.\nSadikin, Mujiono (2020), \u201cEHR Dataset for Patient Treatment Classification\u201d, Mendeley Data, V1, doi: 10.17632\/7kv3rctx7m.1<br>\n","d024a88c":"Observations:\n- LEUCOCYTE and AGE are postively correlated with target\n- THROMBOCYTE, ERYTHROCYTE, HAEMOGLOBINS and HAEMATOCRIT are negatively correlated with target\n- No notable correlation found between MCHC, MCH, MCV, and target","e2854f0c":"### Random forest classifer","8bc5f1f3":"#### Box plot","7117cd1e":"# 1. Context\nIn hospitals, medical treatments and surgeries can be categorized into inpatient and outpatient procedures.<br> For patients, it is important to understand the difference between these two types of care, because they impact the length of a patient\u2019s stay in a medical facility and the cost of a procedure.\n\n<b>Inpatient Care (Incare Patient) and Outpatient Care (Outcare Patient)<\/b>\n\nThe difference between an inpatient and outpatient care is how long a patient must remain in the facility where they have the procedure done.\n\nInpatient care requires overnight hospitalization. Patients must stay at the medical facility where their procedure was done (which is usually a hospital) for at least one night. During this time, they remain under the supervision of a nurse or doctor.\n\nPatients receiving outpatient care do not need to spend a night in a hospital. They are free to leave the hospital once the procedure is over. In some exceptional cases, they need to wait while anesthesia wears off or to make sure there are not any complications. As long as there are not any serious complications, patients do not have to spend the night being supervised.<br>\n<i>[source of information: pbmhealth]","e37e09a4":"### Unique values","a80c45f1":"## Problem Statement\nIn today\u2019s world of automation, the skills and knowledge of a person could be utilized at the best places possible by automating tasks wherever possible. As a part of the hospital automation system, one can build a system that would predict and estimate whether the patient should be categorized as an incare patient or an outcare patient with the help of several data points about the patients, their conditions and lab tests.","df606f8a":"# 4. Import modules","230d00ff":"### 6.1.3 Categorical feature(s)\nname : SEX<br>\nlabels : F, M","1b5a0dc0":"##### Density plot","32bd8967":"### 6.2.2 Categorical feature(s)","d951e99b":"#### Correlation - matrix","2fcefa7c":"### Stacking classifier\nCombine all the best models","f34167c8":"### --------------------------- THE END ----------------------------","00f5f3c9":"#### Hyperparameter tuning for Logistic regression","2357ac49":"### Support Vector Classifier","55479586":"Observation : Luckily, we didn't find any missing values.","83e7881b":"## Save the final pipeline","ef452448":"Let's divide the data into train and test sets.<br>\nTrain : 80% of data<br>\nTest : 20% of data","983830de":"### Decision Tree Classifier","058eafcd":"### 6.1.2 Numerical features\nnames :\n- HAEMATOCRIT\n- HAEMOGLOBINS\n- ERYTHROCYTE\n- LEUCOCYTE\n- THROMBOCYTE\n- MCH\n- MCHC\n- MCV\n- AGE","1ad5ce6c":"## 6.1 Univariate analysis \nExploring each feature individually\n","175389d0":"<h1 style=\"font-size:30px\">Patient Treatment Classification<\/h1>","48835c14":"#### Hyperparameter tuning - Random forest classifier","1e8f02ac":"#### KDE plot","c938ee7f":"# 6. Exploratory data analysis","2cbf2830":"# 3. Objective:\n- Build a machine learning model to predict if the patient should be classified as <b>in care<\/b> or <b>out care<\/b> based on the patient's laboratory test result.\n\n\n## Machine Learning Solution\n- It is a Supervised Machine Learning Problem\n- Binary Classification task (0 or 1)\n- Performance metrics\n    - Precision\n    - Recall\n    - Accuracy","89ad6e93":"Observation:\n- Male Incare patients are more in number than Female Incare patients","0844b9d1":"### Missing values","eeb998a9":"Observations:\n- On average, The quantity of HAEMOTOCRIT, HAEMOGLOBINS, ERYTHROCYTE, and THROMBOCYTE features are lesser in In care patients.\n- MCH, MCHV, MCV are less informative to differentiate classes\n- The mean age of Incare patients is greater than the Outcare patients.","7f3d5d9b":"## 6.2 Bivariate analysis\n\n### 6.2.1 Numerical features","30fd0b4f":"#### Hyperparameter tuning of Decision tree classifier","10f569c1":"### Feature Scaling\n\nTransform all the numerical features into a range [0, 1]","6df59942":"### Logistic regression","8f8d253f":"# 8. Feature selection\nIn exploratory data analysis, we found out that the features MCH, MCHC and MCV are not very useful in predicting the target. Let's remove those features.","648bf11c":"# 5. Load and split data","39041f94":"Observations:\n- Few outliers are present in some numerical features\n- Most of the features follows the Gaussian distribution (approx.)","ad9e8b6c":"#### Hyperparameter tuning - Support vector classifier"}}