{"cell_type":{"dddf525c":"code","6e8cc050":"code","916e8bca":"code","19ec73f4":"code","bef0efb7":"code","26ae547a":"code","5d447a86":"code","106291c6":"code","263d4683":"code","94e7fe64":"code","4f197ae7":"code","62526458":"code","8d56fa2f":"code","1c464c32":"code","78b953d7":"code","ce4f087b":"code","d26fe233":"code","884e60d0":"code","b2787341":"code","b477e4e4":"code","f8c3ee23":"code","65d50c52":"code","db422c7d":"code","a044c80e":"code","2b3ba68a":"code","4ebf689b":"code","9666ce34":"code","df1bab7b":"code","db0cd0d7":"code","b911482b":"code","815a207c":"code","12930561":"code","fdaec87b":"code","f63955f0":"code","aa991180":"code","cceed71b":"code","47ac16b7":"code","01d63344":"code","fda62c0d":"code","7ee6a463":"code","e6b6ca29":"code","3f64ce21":"code","15d5a1b2":"markdown","104e6247":"markdown","b8383bf4":"markdown","be020d07":"markdown","616d3b06":"markdown","5b314a55":"markdown","7e826aee":"markdown","46814e60":"markdown","2f49c3f2":"markdown","9645e57b":"markdown","d27376d8":"markdown","637ca317":"markdown","1b7e54dd":"markdown","58201055":"markdown","2c1d177a":"markdown","290a3864":"markdown","cf403c78":"markdown","0e93eb52":"markdown","c2a35530":"markdown","77489901":"markdown","fb901a64":"markdown","d72f2d46":"markdown","c8ec9e50":"markdown","ebe9450c":"markdown","ce260183":"markdown"},"source":{"dddf525c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport datetime\nsns.set_style(\"dark\")\n\n\nfrom sklearn.linear_model import LinearRegression\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, learning_curve, RandomizedSearchCV, KFold, cross_validate, train_test_split, StratifiedShuffleSplit\nfrom mlxtend.regressor import StackingCVRegressor\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge, LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.pipeline import make_pipeline\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6e8cc050":"df = pd.read_csv('..\/input\/used-car-dataset-ford-and-mercedes\/bmw.csv')\n\ntrain, test = train_test_split(df, test_size=0.3, random_state=2020)\n\nprint('train shape: {}'.format(train.shape))\nprint('test shape: {}'.format(test.shape))\n\ntrain.sample(n=5)","916e8bca":"train['model'].unique()","19ec73f4":"# remove space before model name\ntrain['model'] = train['model'].apply(lambda x: x.strip())\ntrain['model'].unique()","bef0efb7":"train.info()","26ae547a":"train.apply(lambda x: x.nunique(), axis=0)","5d447a86":"fig, ax = plt.subplots(figsize=(12, 10))\ncmap = sns.diverging_palette(10, 240, n=9)\n\n# Plot heat map\ntrain_corr = train.corr()\nsns.heatmap(train_corr, annot=True, fmt=\".2f\", \n           linewidths=2, cmap=cmap, vmin=-1, vmax=1, \n           cbar_kws={\"shrink\": .9}, square=True);","106291c6":"sns.pairplot(train);","263d4683":"def plot_(df, col, y='price'):\n            \n    fig, ax = plt.subplots(1,2,figsize=(20,5))\n    ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90)\n    ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90) \n    fig.suptitle(str(col) + ' vs price')\n    \n\n    if df[col].dtypes == 'object':\n        sns.boxplot(data=df, x=col, y= y, ax=ax[0])\n        sns.countplot(df[col], ax=ax[1])\n        \n        \n    elif len(df[col].unique()) <= 10:\n        sns.swarmplot(data=df, x=col, y= y, ax=ax[0])\n        sns.countplot(df[col], ax=ax[1])\n        \n    else:\n#         sns.scatterplot(data=df, x=col, y= y, ax=ax[0])\n        sns.regplot(data=df, x=col, y= y, order=2, ax=ax[0])\n        sns.distplot(df[col], kde=False,fit=norm, ax=ax[1])\n\nprint('SET FUNCTION COMPLETED!')","94e7fe64":"# Get numerical columns\nnum_col = [col for col in train.columns if train[col].dtype != 'object']\nprint('Numerical Columns\\n'+ str(num_col))\n\n# Get categorical columns\ncat_col = [col for col in train.columns if train[col].dtype == 'object']\nprint('\\nCategorical Columns\\n'+ str(cat_col))","4f197ae7":"for col in num_col:\n    plot_(train, col)","62526458":"for col in cat_col:\n    plot_(train, col)","8d56fa2f":"train = train.drop(train.price[train.price >= 100000].index)","1c464c32":"pd.crosstab(train.model,train.fuelType, margins= True).style.background_gradient(cmap='Blues')","78b953d7":"train[(train.mpg >= 400)]","ce4f087b":"# Create age columns from year\nnow = datetime.datetime.now().year\ntrain['age'] = now - train['year']\ntest['age'] = now - test['year']\n\n# Change type\ntrain['year'] = train['year'].apply(str)\ntest['year'] = test['year'].apply(str)\n\n# Drop column year\ndata = [train,test]\nfor df in data:\n    df.drop(columns = ['year'], inplace=True)","d26fe233":"fig, ax = plt.subplots(1,2,figsize=(20,8))\nsns.boxplot(x=\"engineSize\", y=\"price\", data=train, ax=ax[0])\nsns.countplot(x=\"engineSize\", data=train, ax=ax[1])\nfig.suptitle('Before Grouping Engine Size');","884e60d0":"# Grouping engine size. \ndata = [train,test]\nfor df in data:\n    df['engineSize'].replace([0.0, 0.6, 1.0], 1.5, inplace=True)\n    df['engineSize'].replace([1.9, 2.2, 2.5], 2.0, inplace=True)\n    df['engineSize'].replace([2.8, 3.2, 3.5], 3.0, inplace=True)\n    df['engineSize'].replace([4.0, 4.4, 5.0, 6.6], 4.4, inplace=True)","b2787341":"fig, ax = plt.subplots(1,2,figsize=(20,8))\nsns.boxplot(x=\"engineSize\", y=\"price\", data=train, ax=ax[0])\nsns.countplot(x=\"engineSize\", data=train, ax=ax[1])\nfig.suptitle('After Grouping Engine Size');","b477e4e4":"plt.figure(figsize=(20,10))\n\nsns.scatterplot(data=train, x=\"age\", hue='engineSize', y=\"price\", palette=\"deep\")","f8c3ee23":"plt.figure(figsize=(20,10))\n\nsns.scatterplot(data=train, x=\"mileage\", hue='engineSize', y=\"price\",size=\"age\", palette=\"deep\");","65d50c52":"# Feature Extraction\ndata = [train,test]\nfor df in data:\n    df['eng_age'] = df.engineSize \/ df.age\n    df['mpg_eng'] = df.mpg \/ df.engineSize\n    df['mpg_age'] = df.mpg \/ df.age\n\ntrain","db422c7d":"fig, ax = plt.subplots(figsize=(12, 10))\ncmap = sns.diverging_palette(10, 240, n=9)\n\n# Plot heat map\ntrain_corr = train.corr()\nsns.heatmap(train_corr, annot=True, fmt=\".2f\", \n           linewidths=2, cmap=cmap, vmin=-1, vmax=1, \n           cbar_kws={\"shrink\": .9}, square=True);","a044c80e":"def plot(df, col):\n    plt.figure(figsize=(10, 5))\n    plt.hist(df[col], bins='auto')\n    plt.show()\n    print(col)\n    print(\"value between {:,} and {:,}\".format(df[col].min(),df[col].max()))","2b3ba68a":"plot(train, 'price')","4ebf689b":"# Log-Transform \ntrain.price = np.log1p(train.price)","9666ce34":"X_train = train.drop(columns = 'price')\ny_train = train.price\n\nX_test = test.drop(columns = 'price')\ny_test = test.price","df1bab7b":"# Get numerical columns\nnum_col = [col for col in X_train.columns if X_train[col].dtype != 'object']\nprint('Numerical Columns\\n'+ str(num_col))\n\n# Get categorical columns\ncat_col = [col for col in X_train.columns if X_train[col].dtype == 'object']\nprint('\\nCategorical Columns\\n'+ str(cat_col))","db0cd0d7":"# Pipeline\n\n# Numerical preprocess missing value\nnumerical_transformer = make_pipeline(RobustScaler())\n\n# Categorical preprocess missing value\ncategorical_transformer = make_pipeline(OneHotEncoder(handle_unknown='ignore',sparse=False))\n\n# Preprocess Numerical and Categorical variable\npreprocess = ColumnTransformer(transformers=[\n        ('num', numerical_transformer, num_col),\n        ('cat', categorical_transformer, cat_col)])","b911482b":"preprocess.fit_transform(X_train)\n\nenc_cat_col = preprocess.named_transformers_['cat']['onehotencoder'].get_feature_names()\nlabels = np.concatenate([num_col, enc_cat_col])\nX_train_ = pd.DataFrame(preprocess.fit_transform(X_train), columns=labels)\n\npreprocess.transform(X_test)\n\nenc_cat_col = preprocess.named_transformers_['cat']['onehotencoder'].get_feature_names()\nlabels = np.concatenate([num_col, enc_cat_col])\nX_test_ = pd.DataFrame(preprocess.transform(X_test), columns=labels)\n\nX_train_","815a207c":"kf =KFold(n_splits=5, shuffle=True, random_state=21)\n\n\nprint('='*30 +'BASE MODEL' + '='*30)\nlar = LinearRegression()\nlar.fit(X_train_, y_train)\nprint('slope \uff1a {:0.2f}'.format(lar.coef_[0]))\nprint('intercept : {:0.2f}'.format(lar.intercept_))\nprint('\\n')\n\nRMSE = -1*cross_val_score(lar, X_train_, y_train, cv=kf, scoring='neg_root_mean_squared_error')\n\nprint(f'RMSE: {RMSE.mean():.5f} \u00b1{RMSE.std():.3f}' )","12930561":"models = [('LinearRegression', LinearRegression()),\n          (\"Ridge\", Ridge()),\n          (\"Lasso\", Lasso()),\n          (\"ElasticNet\", ElasticNet()),\n          ('RandomForestRegressor', RandomForestRegressor()),\n          ('GradientBoostingRegressor', GradientBoostingRegressor()),\n          (\"XGBoost\", XGBRegressor()),\n          (\"LightGBM\", LGBMRegressor()),\n          (\"CatBoost\", CatBoostRegressor(verbose=False))]\n\nprint('='*30 +'RMSE TRAIN MODEL' + '='*30)\n\nmean_rmse = []\nsd_rmse = []\nmodel_name = []\nfor name, model in models:\n    model.fit(X_train_, y_train)\n    RMSE = -1*cross_val_score(model, X_train_, y_train, cv=kf, scoring='neg_root_mean_squared_error')\n    print(f'{name:<25} {RMSE.mean():.5f} \u00b1{RMSE.std():.3f}')\n    \n    mean_rmse.append(RMSE.mean())\n    sd_rmse.append(RMSE.std())\n    model_name.append(name)\n\nprint('AVERAGE SCORE RMSE: {:.5f}'.format(np.mean(mean_rmse)))\n    \nbase = pd.DataFrame({'model':model_name, 'mean_rmse':mean_rmse, 'sd_rmse':sd_rmse})\nsns.barplot(x=\"mean_rmse\", y=\"model\", data=base, orient = 'h',**{'xerr': sd_rmse})\nplt.title('Baseline: Cross Validation Scores')\nplt.axvline(x = np.mean(mean_rmse), color = 'firebrick', linestyle = '--');","fdaec87b":"# Created an object model_CBR\ncbr = CatBoostRegressor(loss_function = \"RMSE\", eval_metric = \"RMSE\", verbose = False)\n\n# Defined the parameters of the model which we want to pass to through GridSearchCV \nparameters = {'max_depth' : [2, 5, 10],\n              'learning_rate' : [0.001, 0.01, 0.1],\n              'n_estimators' : [100, 200],\n              'min_child_samples' : [2, 5, 10]\n             }\n\nRgrid_search = RandomizedSearchCV(cbr, parameters, cv=kf, scoring = 'neg_mean_absolute_error', verbose = False, n_jobs = -1, return_train_score = True)\n\nRgrid_search.fit(X_train_, y_train)","f63955f0":"Rgrid_search.best_estimator_","aa991180":"RMSE = -1*cross_val_score(Rgrid_search.best_estimator_, X_train_, y_train, cv=kf, scoring='neg_root_mean_squared_error')\n\nprint('SCORE RMSE: {:.5f} \u00b1{:.3f}'.format(RMSE.mean(), RMSE.std()))","cceed71b":"feature_imp = pd.DataFrame(Rgrid_search.best_estimator_.feature_importances_, \n                           X_train_.columns,columns = ['Feature_Importances']).sort_values(by='Feature_Importances', ascending = False)\nfeature_imp","47ac16b7":"fig, ax = plt.subplots(figsize = (15, 10))\nax = sns.barplot(x=\"Feature_Importances\", y=feature_imp.index, data=feature_imp)","01d63344":"prediction = Rgrid_search.best_estimator_.predict(X_test_) \n\ny_pred = np.expm1(prediction)\n\npd.Series(y_pred, name='price_predict')","fda62c0d":"X_test_","7ee6a463":"compare = pd.concat([y_test.reset_index(drop=True),pd.Series(y_pred, name='price_predict'), X_test.reset_index(drop=True)], axis='columns')\ncompare.sample(10)","e6b6ca29":"fig, ax = plt.subplots(figsize = (10, 10))\n\nax.scatter(y_test, y_pred, edgecolors=(0, 0, 0))\nax.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=4)\nax.set_xlabel('Measured')\nax.set_ylabel('Predicted')\nax.axvline(x = 50000, color = 'firebrick', alpha = 0.1, linestyle = '--')\nax.axhline(y = 37000, color = 'firebrick', alpha = 0.1, linestyle = '--');\n\nplt.show()","3f64ce21":"index_out = []\nindex_out.extend(compare.loc[(compare.price >=50000) & (compare.price_predict <= 37000)].index.to_list())\nindex_out.extend(compare.loc[(compare.price >=40000) & (compare.price_predict <= 20000)].index.to_list())\nindex_out.extend(compare.loc[(compare.price >=80000) & (compare.price_predict <= 50000)].index.to_list())\n\ncompare.iloc[index_out]","15d5a1b2":"* The `engineSize\/age` of car is the higest importance and follow by the `mileage` and `mpg\/engineSize`.\n* The larger `engineSize` the price is higher, the older cars (higher `age`) the price are cheaper.\n* The higher `mileage`, `mpg` price will drop.","104e6247":"The `i8` and `M4` model that have a high error so we have to deliberate when we use this model to predict price of car. ","b8383bf4":"# 2. Exporatory Data Analysis","be020d07":"There some points that much error.","616d3b06":"CatBoostRegressor has lowest RMSE so we  will tune hyperparameter to improve CatBoostRegressor performance by using GridSearchCV","5b314a55":"Great! There are no missing value.","7e826aee":"# Feature Engineering","46814e60":"* Manual has lower price than other transmission.\n* Most of fuel type are Diesel and Petrol respectively and Diesel has lower price than others.","2f49c3f2":"* `price` will drop when smaller `engineSize` and a higher `mileage`.","9645e57b":"# Model","d27376d8":"## Ourlier","637ca317":"For mpg above 400 that is only for i3 model.","1b7e54dd":"# Introduction\nWhen you buy a new car mostly the price was determined by the manufacturer, on the other hand when buying a used car, it's usually hard to know what a fair price is. There are a variety of features of a car like mileage, transmission, type of fuel, fuel consumption, year of manufacturing, and so on these factors influence the price. In this project, we use Machine Learning to evaluate the price of a used car with a supervised learning method. These imformaiton can help the customer to make a good decision to buy used car in fair price.\n\nIn this project we are using the dataset on BMW used car. The features available\nin this dataset are price, model, year, transmission, mileage, tax, mpg, enginesize  and type of fuel.","58201055":"Some outlier that might reduce model performance so we will drop some outlier later.","2c1d177a":"## Numerical features","290a3864":"High correlation feature are `year` `mileage` (correlation > 0.5)\n* if new car price will higher than old car that make sense. (positive correlation)\n* mileage is high mean that car might wear and tear so the price will drop. (negative correlation)\n\nmiles per gallon (mpg) and tax seem not significant correlate to price.","cf403c78":"We will divide data into 5 folds i.e. K=5. Now we have 5 sets of data to train and test our model. So the model will get trained and tested 5 times, but for every iteration we will use one fold as test data and rest all as training data. Note that for every iteration, data in training and test fold changes which adds to the effectiveness of this method.\n\nK Fold cross validation helps to generalize the machine learning model and reduce overfitting","0e93eb52":"## Category features","c2a35530":"<img src=\"https:\/\/www.bmw-lebanon.com\/content\/dam\/bmw\/marketMIDEAST\/common\/local-content\/X%20Range%20Campaign\/Xrange-home.jpg\">","77489901":"Most of `engineSize` are 2.0, 3.0 and 1.5 respectively so we will group `engineSize`. ","fb901a64":"# 1. Data Preparation","d72f2d46":"## 1.1 Load Data","c8ec9e50":"<img src=\"https:\/\/www.bmw.co.th\/content\/dam\/bmw\/common\/all-models\/i-series\/i3\/2017\/design\/BMW-i-series-i3-design-exterior-design-update-01.jpg\/_jcr_content\/renditions\/cq5dam.resized.img.585.low.time1535122344052.jpg\">","ebe9450c":"* The higher `age` of car the `price` will go down.\n* At the same `age` if the larger `engineSize` the price will higher. ","ce260183":"### Check correlation"}}