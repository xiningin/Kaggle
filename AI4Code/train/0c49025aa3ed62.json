{"cell_type":{"6085ad85":"code","8d1cb391":"code","240f78c3":"code","8c4e9db4":"code","4d711647":"code","5c7b0ccd":"code","584386ad":"code","f10b45f8":"code","49a06c73":"code","f9ccb81c":"markdown","6dc9e6a1":"markdown","ea31642d":"markdown","20063e1b":"markdown","65c3fee1":"markdown"},"source":{"6085ad85":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8d1cb391":"# import lightgbm as lgb\nimport optuna.integration.lightgbm as lgb\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nimport optuna","240f78c3":"df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv')","8c4e9db4":"def exec_lgb_model(X_train, X_test, y_train, y_test, params):\n    model = lgb_model(X_train, y_train, params)\n    y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n    return log_loss(y_test, y_pred), model, params\ndef lgb_optuna_model(X_train, X_test, y_train, y_test, params):\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n    model = lgb.train(params,\n                      lgb_train,\n                      valid_sets=lgb_eval,\n                      num_boost_round=200,\n                      early_stopping_rounds=20)\n    return model","4d711647":"def preprocess_X(df, include_id = False):\n    X = df[['feature_'+str(x) for x in range(1, 75)]]\n    if include_id:\n        X['id'] = df['id']\n    return X\n\ndef preprocess_data(df, drop_tgt = False):\n    df[['_','tgt']] = df['target'].str.split('_', expand=True)\n    if drop_tgt:\n        df=df.drop(['tgt','_'], axis=1)\n    X = preprocess_X(df)\n    y = df['tgt'].astype(int)-1\n    X\n    return X, y","5c7b0ccd":"X, y = preprocess_data(df)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 114514)","584386ad":"lgbm_params = {'objective': 'multiclass',\n 'metric': 'multi_logloss',\n 'num_class': 9,\n 'feature_pre_filter': False,\n 'lambda_l1': 9.970496731080852,\n 'lambda_l2': 0.33605433486440883,\n 'num_leaves': 9,\n 'feature_fraction': 0.4,\n 'bagging_fraction': 0.8180268235748993,\n 'bagging_freq': 5,\n 'min_child_samples': 20,\n 'num_iterations': 100,\n 'early_stopping_round': 10}\nmodel = lgb_optuna_model(X_train, X_test, y_train, y_test, lgbm_params)","f10b45f8":"X_val = preprocess_X(df_test)\ny_pred = model.predict(X_val, num_iteration=model.best_iteration)\nprediction = pd.DataFrame(y_pred, columns=['Class_'+str(x) for x in range(1, 10)])\nprediction['id'] = df_test['id']\nprediction","49a06c73":"prediction.to_csv('\/kaggle\/working\/submission.csv', index=False)","f9ccb81c":"# importing modules","6dc9e6a1":"# Defining preprocessing as a funciton","ea31642d":"# Defining model as a function\n\nThis makes much easier to tune hyperparameters or to run a stacking \/ blending later on","20063e1b":"# Simple LightGBM + Optuna integration\n\nOptuna has a LightGBM integration for quick parameter tuning.\n\nThis notebook provides you a good starting point using a simple LGBM module and a baseline workspace around the classifier.","65c3fee1":"# Reading Data"}}