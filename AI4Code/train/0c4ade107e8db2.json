{"cell_type":{"bf104ad8":"code","703839b6":"code","b1b41da8":"code","d0e27d62":"code","f71e8028":"code","c5f6698b":"code","64ad404b":"code","cf498b13":"code","c8bd6122":"code","f708720c":"code","ecddea8b":"code","6cbe2fd5":"code","c991f56f":"code","9461c064":"code","57c50b96":"code","bfad412e":"code","0ec56e53":"code","9a38bdc7":"code","571b11da":"markdown","8621c9ed":"markdown","b8f35f64":"markdown","6f215016":"markdown","44356a2d":"markdown","3c488e59":"markdown","fd7d81a1":"markdown","5640dc9d":"markdown","a31432d7":"markdown","a058b371":"markdown","cd4feff4":"markdown"},"source":{"bf104ad8":"import pandas as pd\nimport numpy as np\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom gensim.models import Word2Vec\nimport multiprocessing \nimport keras\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nimport nltk\n!pip install tweet-preprocessor\nfrom nltk.corpus import stopwords\nimport preprocessor as p\nfrom nltk.tokenize import sent_tokenize as sent\nfrom nltk.tokenize import word_tokenize as word\n#nltk.download('stopwords')\nfrom nltk.stem import PorterStemmer \nimport re\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n%matplotlib inline","703839b6":"df = pd.read_excel('\/kaggle\/input\/personality-data\/mbti9k_comments250.xlsx',nrows=50)\nprint(len(df))","b1b41da8":"REPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\nSTOPWORDS = set(stopwords.words('english'))\nFLOAT=re.compile('[-+]?\\d*\\.\\d+|\\d+')\nps = PorterStemmer() \ndef clean_text(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = FLOAT.sub('', text)\n    text = text.lower() # lowercase text\n    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n    \n    text = text.replace('x', '')\n#    text = re.sub(r'\\W+', '', text)\n    text = ' '.join(ps.stem(word) for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n    return text","d0e27d62":"raw_text=df['comment']\n#raw_text=data['body']\ncleaned_text=[]\nword_text=[]\nf_text=[]\n#print(datetime.now(pytz.timezone('Asia\/Calcutta')).strftime('%Y-%m-%d %H:%M:%S'))\nfor i in range(0,len(raw_text)):\n    text1=p.clean(raw_text[i])\n    sent_text=sent(text1)\n    for sen in sent_text:\n        text=clean_text(sen) \n        text=word(text) #Tokenize into words\n        f_text.append(text)  #Preparing input for word to vector model\n    #if i==2:\n    #print(raw_text[i])\n        #print('\\n\\n\\n')\n        #print(f_text)\n        #break\nprint(len(f_text))","f71e8028":"raw_label1=df['type']\nraw_label=[y.lower() for y in raw_label1]\nlabel=[]\nfor x in raw_label:\n    if x=='intj':\n     label.append(1)\n    else:\n     label.append(0)\n        \n    \nprint(len(label) )   ","c5f6698b":"EMB_DIM=300\nw2v=Word2Vec(f_text,size=EMB_DIM,window=5,min_count=5,negative=15,iter=10,sg=0,workers=multiprocessing.cpu_count())\nmodels=w2v.wv","64ad404b":"models.save_word2vec_format('model_sg.bin')\nmodels.save_word2vec_format('model_sg.txt', binary=False)\nresult=models.similar_by_word('friend')\nprint(result)","cf498b13":"data_val=[]\nfor i in range(0,len(raw_text)):\n    text1=p.clean(raw_text[i])\n    text=clean_text(text1) \n    data_val.append(text)\n    ","c8bd6122":"# The maximum number of words to be used. (most frequent)\nMAX_NB_WORDS = 50000\n# Max number of words in each complaint.\nMAX_SEQUENCE_LENGTH = 400\n# This is fixed.\nEMBEDDING_DIM = 100\ntokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-.\/:;<=>?@[\\]^_`{|}~', lower=True)\ntokenizer.fit_on_texts(data_val)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n","f708720c":"X = tokenizer.texts_to_sequences(data_val)\nX = sequence.pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', X.shape)\nprint(X)","ecddea8b":"Y=label","6cbe2fd5":"X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)","c991f56f":"max_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","9461c064":"def RNN():\n    inputs = Input(name='inputs',shape=[MAX_SEQUENCE_LENGTH])\n    layer = Embedding(MAX_NB_WORDS,EMBEDDING_DIM,input_length=MAX_SEQUENCE_LENGTH)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","57c50b96":"model = RNN()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","bfad412e":"model.fit(X_train,Y_train,batch_size=128,epochs=10,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","0ec56e53":"accr = model.evaluate(X_test,Y_test)","9a38bdc7":"print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","571b11da":"Prepared data for word embedding","8621c9ed":"Call the function and compile the model.","b8f35f64":"### RNN\nDefine the RNN structure.","6f215016":"### Process the data\n* Tokenize the data and convert the text to sequences.\n* Add padding to ensure that all the sequences have the same shape.\n* There are many ways of taking the *max_len* and here an arbitrary length of 150 is chosen.","44356a2d":"# Import the necessary libraries","3c488e59":"### Load the data into Pandas dataframe","fd7d81a1":"The model performs well on the validation set and this configuration is chosen as the final model.","5640dc9d":"Process the test set data.","a31432d7":"Evaluate the model on the test set.","a058b371":"Split into training and test data.","cd4feff4":"Fit on the training data."}}