{"cell_type":{"0407bee1":"code","572b61f7":"code","98d768f5":"code","cd616ac7":"code","81e5a803":"code","26145310":"code","970d64c1":"code","6b5eba94":"code","a97098be":"code","3ace2523":"code","c64cd5e9":"code","76af643d":"code","df1cad5b":"code","b796ca42":"code","cd01e486":"code","a649cdac":"code","83399f64":"code","7c5ca4e3":"code","eb2b0ff7":"code","23cf8402":"code","fa673442":"code","a9be14f4":"code","f089af28":"code","5d343fc2":"code","d3a75374":"code","cb2485b7":"code","60fca46b":"code","262ad4a0":"code","644ca323":"code","10e94f95":"code","ccf92557":"code","0d964383":"code","673a91d0":"code","40e024e0":"code","92497c22":"code","edf57c6c":"code","1eb7726a":"code","247f5f15":"code","2d03c275":"code","fd631fe3":"code","0ab15416":"code","3242b103":"code","6eaddfa5":"code","4e40aaff":"code","d6e97b14":"code","a9fe02bb":"code","b85c55c9":"code","08932d95":"markdown","1939c3ec":"markdown","9a331ef1":"markdown","3d241513":"markdown","87b3ae3f":"markdown","2400c555":"markdown","fa11d7d7":"markdown","18d36ccb":"markdown","4212928e":"markdown","16e9221e":"markdown","9715f219":"markdown","8050cbd6":"markdown","13fe92db":"markdown","bbbdf70a":"markdown","d64f2ad3":"markdown","52f24e24":"markdown","8508e737":"markdown","41876a22":"markdown","85f17e2b":"markdown","d2b45a65":"markdown","defeb807":"markdown","fc8a7332":"markdown","2dc82450":"markdown","ce3c0947":"markdown","c1222afd":"markdown","a9e061a2":"markdown","7a98e031":"markdown","7995baa7":"markdown","988c73ad":"markdown","2411d902":"markdown","a3130dd4":"markdown","ff9bf537":"markdown","73a66732":"markdown","d140546e":"markdown","734df4e8":"markdown","5f4e9e7a":"markdown","cd9fe039":"markdown","138b9f71":"markdown"},"source":{"0407bee1":"from sklearn.metrics import mean_absolute_error\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.base import TransformerMixin\nfrom sklearn.base import RegressorMixin\nfrom sklearn.base import clone\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\n\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import RobustScaler\n\nfrom xgboost import XGBRegressor\nfrom xgboost import train\nfrom xgboost import DMatrix\nfrom xgboost import cv\n\nfrom lightgbm import LGBMRegressor\n\nfrom scipy.stats import norm\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n\npd.set_option('display.max_rows', 500)","572b61f7":"# Read the data\nX = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col='Id')\nX_submit = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col='Id')\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice              \nX.drop(['SalePrice'], axis=1, inplace=True)","98d768f5":"sns.distplot(y)","cd616ac7":"y.skew()","81e5a803":"y_log = np.log(y)","26145310":"sns.distplot(y_log)\nprint(\"Skewness: %f\" % y_log.skew())","970d64c1":"features = pd.concat([X, X_submit]).reset_index(drop=True)\nfeatures.shape","6b5eba94":"all_data_na = (features.isnull().sum() \/ len(features)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","a97098be":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","3ace2523":"features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")","c64cd5e9":"features[\"MiscFeature\"] = features[\"MiscFeature\"].fillna(\"None\")","76af643d":"features[\"Alley\"] = features[\"Alley\"].fillna(\"None\")","df1cad5b":"features[\"Fence\"] = features[\"Fence\"].fillna(\"None\")","b796ca42":"features[\"FireplaceQu\"] = features[\"FireplaceQu\"].fillna(\"None\")","cd01e486":"nans_cols = []\n\nfor k in features:\n    if features[k].isnull().sum() > 0:\n        nans_cols.append((k, features[k].isnull().sum()))\nnans_cols = sorted(nans_cols, key = lambda t: t[1], reverse=True)        ","a649cdac":"nans_cols","83399f64":"garage_X = features[features['GarageYrBlt'].isnull()][['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'GarageYrBlt', 'GarageArea']]\ngarage_X[garage_X.notnull()]","7c5ca4e3":"features['has_garage'] = features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)","eb2b0ff7":"garage_X = features[features['BsmtCond'].isnull()][['BsmtCond', 'BsmtExposure', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1', 'TotalBsmtSF']]\ngarage_X[garage_X.notnull()]","23cf8402":"features['MasVnrArea'] = features['MasVnrArea'].fillna(0)\nfeatures['MasVnrType'] = features['MasVnrType'].fillna('None')\nfeatures['MSZoning'] = features['MSZoning'].fillna(features['MSZoning'].value_counts().idxmax())\nfeatures['Utilities'] = features['Utilities'].fillna(features['Utilities'].value_counts().idxmax())\nfeatures['BsmtFullBath'] = features['BsmtFullBath'].fillna(features['BsmtFullBath'].value_counts().idxmax())\nfeatures['Functional'] = features['Functional'].fillna(features['Functional'].value_counts().idxmax())\nfeatures['Exterior1st'] = features['Exterior1st'].fillna(features['Functional'].value_counts().idxmax())\nfeatures['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].value_counts().idxmax())\nfeatures['BsmtFinSF1'] = features['BsmtFinSF1'].fillna(features['BsmtFinSF1'].value_counts().idxmax())\nfeatures['BsmtFinSF2'] = features['BsmtFinSF2'].fillna(0)\nfeatures['BsmtUnfSF'] = features['BsmtUnfSF'].fillna(0)\nfeatures['BsmtHalfBath'] = features['BsmtHalfBath'].fillna(features['BsmtHalfBath'].value_counts().idxmax())\nfeatures['TotalBsmtSF'] = features['TotalBsmtSF'].fillna(0)\nfeatures['Electrical'] = features['Electrical'].fillna(features['Electrical'].value_counts().idxmax())\nfeatures['KitchenQual'] = features['KitchenQual'].fillna(features['KitchenQual'].value_counts().idxmax())\nfeatures['SaleType'] = features['SaleType'].fillna(features['SaleType'].value_counts().idxmax())\n\n# GarageType etc : data description says NA for garage features is \"no garage\"\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    features[col] = features[col].fillna(0)\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    features[col] = features[col].fillna('None')\n\n# BsmtQual etc : data description says NA for basement features is \"no basement\"\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    features[col] = features[col].fillna('None')","fa673442":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nfeatures[\"LotFrontage\"] = features.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","a9be14f4":"#Check remaining missing values if any \nfeatures_na = (features.isnull().sum() \/ len(features)) * 100\nfeatures_na = features_na.drop(features_na[features_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :features_na})\nmissing_data.head()","f089af28":"# Some numerical features are actually really categories\nfeatures = features.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n                       \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n                                   7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}\n                      })","5d343fc2":"features.shape","d3a75374":"final_features = pd.get_dummies(features).reset_index(drop=True)\nfinal_features.shape","cb2485b7":"X = final_features.iloc[:len(y), :]\nX_sub = final_features.iloc[len(y):, :]\nX.shape, y.shape, X_sub.shape","60fca46b":"X_train, X_test, y_train, y_test = train_test_split(X, y_log, test_size=0.3, random_state=42)","262ad4a0":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model, X, y):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n    rmse= np.sqrt(-cross_val_score(model, X.values, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","644ca323":"xgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","10e94f95":"xgb.fit(X_train, y_train)","ccf92557":"print(\"Accuracy score: {0}\".format(xgb.score(X_test, y_test)))\n\npredictions_regr = np.exp(xgb.predict(X_test))\nprint(\"Mean Absolute Error: {0}\".format(mean_absolute_error(np.exp(y_test), predictions_regr)))","0d964383":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","673a91d0":"lasso.fit(X_train, y_train)","40e024e0":"print(\"Accuracy score: {0}\".format(lasso.score(X_test, y_test)))\n\npredictions_regr = np.exp(lasso.predict(X_test))\nprint(\"Mean Absolute Error: {0}\".format(mean_absolute_error(np.exp(y_test), predictions_regr)))","92497c22":"lgb = LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","edf57c6c":"lgb.fit(X_train, y_train)","1eb7726a":"print(\"Accuracy score: {0}\".format(lgb.score(X_test, y_test)))\n\npredictions_regr = np.exp(lgb.predict(X_test))\nprint(\"Mean Absolute Error: {0}\".format(mean_absolute_error(np.exp(y_test), predictions_regr)))","247f5f15":"gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","2d03c275":"gboost.fit(X_train, y_train)","fd631fe3":"print(\"Accuracy score: {0}\".format(gboost.score(X_test, y_test)))\n\npredictions_regr = np.exp(gboost.predict(X_test))\nprint(\"Mean Absolute Error: {0}\".format(mean_absolute_error(np.exp(y_test), predictions_regr)))","0ab15416":"score = rmsle_cv(lasso, X_train, y_train)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3242b103":"score = rmsle_cv(xgb, X_train, y_train)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","6eaddfa5":"score = rmsle_cv(lgb, X_train, y_train)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","4e40aaff":"score = rmsle_cv(gboost, X_train, y_train)\nprint(\"Gradient Boosting score score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","d6e97b14":"gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","a9fe02bb":"gboost.fit(X, y_log)","b85c55c9":"submission = np.exp(gboost.predict(X_sub))\nsub = pd.DataFrame({'Id': X_submit.index,'SalePrice': submission})\n\nsub.to_csv('iowa_houses_submission_gboost.csv', sep=',', index=False)","08932d95":"* GradientBoostingRegressor- 2.5.1","1939c3ec":"# Importing necessary libraries","9a331ef1":"* PoolQC : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.","3d241513":"* Fence : data description says NA means \"no fence\"","87b3ae3f":"* XGBRegressor- 2.2.1","2400c555":"> # Submission - 3\n> **Remaking the best model**","fa11d7d7":"***Create the models***","18d36ccb":"Training the model- 3.2","4212928e":"Training the model- 2.3.2","16e9221e":"* LGBMRegressor- 2.4.1","9715f219":"* LotFrontage : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.","8050cbd6":"Basic evaluation- 2.2.3","13fe92db":"Basic evaluation- 2.5.3","bbbdf70a":"Training the model- 2.5.2","d64f2ad3":"**Handling skeweness - 1.2.1**\n\nTLDR; Skewness isn't good and we have to somehow fix it.\n\nWe can see that the distribution is distorted or skewed and altough real life distributions are usually skewed if there are too much skewness, in the data, then many statistical model don\u2019t work but why? \n\nSo in skewed data, the tail region may act as an outlier for the statistical model and we know that outliers (nodes that are far from the majority of the data) adversely affect the model\u2019s performance especially regression-based models.\n\nMore practically-\nIf the skewness parameter is less than -1 or greater than 1, the distribution is highly skewed and you should fix it.\n\nAll about skewness - \n* https:\/\/www.kaggle.com\/getting-started\/110134\n* https:\/\/towardsdatascience.com\/skewed-data-a-problem-to-your-statistical-model-9a6b5bb74e37","52f24e24":"Basic evaluation- 2.3.3","8508e737":"> # Handling data - 1\n> **Reading data- 1.1**","41876a22":"* Alley : data description says NA means \"no alley access\"","85f17e2b":"So as we can see the Gradient Boosting model done the best job... according to that we'll decide to pick that model!\nbut before we make a submission we should train our model on ALL the data that is provided to us in order to make the best submission we can.","d2b45a65":"**Dealing with missing data - 1.2.2**\n","defeb807":"We fix our skewness (putting data into normal distribution) with the log function","fc8a7332":"> # Base Modeling - 2","2dc82450":"* Applying cross-validation on all the algorithms - 2.6","ce3c0947":"As we can see the columns that are NaN's for all of the garage features are nans at the same rows (for most of them) which probably means that there are just no garages for these houses...","c1222afd":"* LASSO Regression- 2.3.1\n\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline","a9e061a2":"> Splitting the data back into the original train\/sub form- 1.3","7a98e031":"* FireplaceQu : data description says NA means \"no fireplace\"","7995baa7":"Splitting train data into train\/test- 1.3.1","988c73ad":"Same as the garage we can see the numbers for the basement add up just the same so we'll have a quick look at the basment data","2411d902":"* MiscFeature : data description says NA means \"no misc feature\"","a3130dd4":"Creating the submission file- 3.3","ff9bf537":"Basic evaluation- 2.4.3","73a66732":"> **Plotting data and manipulating it - 1.2**","d140546e":"Training the model- 2.4.2","734df4e8":"* Create a corss validation strategy- 2.1.1","5f4e9e7a":"Encoding some features features- 1.2.3","cd9fe039":"So in the basement case we have some data missing in places where there's actually a basement but the vast majority of the NaN are really due to no basement in the house...","138b9f71":"Training the model- 2.2.2"}}