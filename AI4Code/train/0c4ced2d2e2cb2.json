{"cell_type":{"286dd275":"code","0c1e8c8b":"code","9488274f":"code","59471339":"code","80935294":"code","de379e86":"code","df2cbf3f":"code","5ac2160e":"code","4553f06b":"code","ce8d138b":"code","deddc2d0":"code","148382d0":"code","dfb14320":"code","15da7ec2":"code","e5b3f59c":"code","8b1a7814":"code","f0af5449":"code","157b8281":"code","a4464c02":"code","689a7f6f":"code","7a844cb5":"code","2ffbb104":"code","c5666a9e":"code","0fd0a713":"code","7c7fbdf1":"code","6b93ea00":"code","c8aec4eb":"code","c3342a6e":"code","118ed61e":"code","24af6b53":"code","937e07e5":"code","7d5c932c":"code","10a81fb3":"code","0135eb64":"code","8caf0667":"code","c3a71a0a":"code","aa91f59b":"code","d35fd5b0":"code","4142f917":"code","4233614b":"code","709e0f37":"code","91fcc23d":"code","41e57441":"code","c0ed9a3b":"code","62df98c6":"code","4645507a":"code","b5a860bb":"code","ce4af133":"code","a3588d81":"code","3d73ce08":"code","7083cfdd":"code","8d2aa214":"code","07d5c5d8":"code","cf6b2e86":"code","c5168041":"code","19048eda":"code","6d5614d0":"code","1832853b":"code","58f0da09":"code","360afa2d":"code","d65444fc":"code","fd3848cf":"code","42d70e2c":"code","f527b5dc":"code","c5aecb92":"code","9db2cb57":"code","b2f45350":"code","2ae6ebc7":"code","8d486b03":"code","fa4a4ce0":"code","2d825506":"code","500e3f42":"code","4629a999":"code","dc3d3811":"code","769e8dcb":"code","276d2e71":"code","df9fce40":"code","8bc2f03b":"code","2dc8a3cc":"code","fc5b56d7":"code","e91b9641":"code","22e839d6":"code","b3bc438e":"code","742293e7":"code","5efa7cdc":"code","8ff5f5e5":"code","14f66deb":"code","24fb437e":"code","339a4bfa":"code","0f99a705":"code","44d6668d":"code","79e18cc6":"code","d91d61a2":"code","a2e8e364":"code","3831e2c6":"code","e508c0e1":"code","31981cff":"code","568d0169":"code","514811fc":"code","0cbfd086":"code","2cf77be1":"code","dbd0af5e":"code","a65de074":"code","33b4037e":"code","714041bc":"code","02496597":"markdown","80b31ffe":"markdown","2f31c1af":"markdown","e2a283c5":"markdown","844e72d5":"markdown","a237dfa6":"markdown","31d06c28":"markdown","c05ec536":"markdown","f81856fc":"markdown","342440f5":"markdown","d97f89c8":"markdown","8d33dea8":"markdown","6b4f3248":"markdown","24c223e5":"markdown","4c8b42ee":"markdown","59ea6716":"markdown","b308a146":"markdown","4e2595f1":"markdown","4775fd91":"markdown"},"source":{"286dd275":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport math\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport glob\nfrom datetime import datetime\nimport random\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\nOUTPUT_DIR = '.\/'","0c1e8c8b":"train = pd.read_csv('..\/input\/shigglecup-2nd\/train.csv')\nprint(train.shape)\ntrain.head()","9488274f":"team_df = pd.read_csv('..\/input\/shigglecup-2nd\/team_id.csv')\nprint(team_df.shape)\nteam_df.head()","59471339":"poke_df = pd.read_csv('..\/input\/shigglecup-2nd\/pokemon.csv')\nprint(poke_df.shape)\npoke_df.head()","80935294":"type_df = pd.read_csv('..\/input\/shigglecup-2nd\/typetable.csv')\nprint(type_df.shape)\ntype_df.head()","de379e86":"test = pd.read_csv('..\/input\/shigglecup-2nd\/test.csv')\nprint(test.shape)\ntest.head()","df2cbf3f":"sub_df = pd.read_csv('..\/input\/shigglecup-2nd\/sample_submission.csv')\nprint(sub_df.shape)\nsub_df.head()","5ac2160e":"# test\u306b\u306a\u3044\u30ec\u30b3\u30fc\u30c9\u306e\u524a\u9664\n# no_test_team_id = list(set(list(train['first'].unique()) + list(train['second'].unique())) - set(list(test['first'].unique()) + list(test['second'].unique())))\n# no_test_team_id","4553f06b":"# print(train.shape)\n# train = train[~((train['first'].isin(no_test_team_id))|(train['second'].isin(no_test_team_id)))]\n# print(train.shape)","ce8d138b":"# type\u6570\n# poke_df['type_count'] = np.where(poke_df['Type_2'].isnull(), 1, 2)\n# poke_df.head(50)","deddc2d0":"# types\npoke_df['types'] = poke_df['Type_1'] + '_' + poke_df['Type_2'].astype(str)\npoke_df.head(50)","148382d0":"# status\nstats = ['Attack', 'Defense', 'HP', 'Sp_Atk', 'Sp_Def', 'Speed']\nstats_atk_spd = ['Attack', 'Speed']\nstats_spatk_spd = ['Sp_Atk', 'Speed']\nstats_def_hp = ['Defense', 'HP']\nstats_spdef_hp = ['Sp_Def', 'HP']\n# stats_physical = ['Attack', 'Defense']\n# stats_special = ['Sp_Atk', 'Sp_Def']","dfb14320":"# \u6b63\u898f\u5316\nfor col in stats:\n    poke_df[col] = poke_df[col] \/ poke_df[col].max()","15da7ec2":"poke_df['stats_sum'] = poke_df[stats].sum(axis=1)\npoke_df['stats_def+hp'] = poke_df['Defense'] + poke_df['HP']\npoke_df['stats_def+speed'] = poke_df['Defense'] + poke_df['Speed']\npoke_df['stats_spdef+hp'] = poke_df['Sp_Def'] + poke_df['HP']\npoke_df['stats_spdef+speed'] = poke_df['Sp_Def'] + poke_df['Speed']\npoke_df['stats_speed+hp'] = poke_df['Speed'] + poke_df['HP']\npoke_df[\"Attack__Sp_Atk_Diff\"] = poke_df[\"Attack\"] - poke_df[\"Sp_Atk\"]\npoke_df[\"Defense__Sp_Def_Diff\"] = poke_df[\"Defense\"] - poke_df[\"Sp_Def\"]\npoke_df['atk_type'] = np.where(poke_df['Attack']>=poke_df['Sp_Atk'], 0, 1)\npoke_df['def_type'] = np.where(poke_df['Defense']>=poke_df['Sp_Def'], 0, 1)\npoke_df[\"atk_spatk+speed\"] = np.where(poke_df['atk_type']==0, poke_df[\"Attack\"]+poke_df[\"Speed\"], poke_df['Sp_Atk']+poke_df['Speed'])\n# poke_df[\"def_spdef+hp\"] = np.where(poke_df['def_type']==0, poke_df[\"Defense\"]+poke_df[\"HP\"], poke_df['Sp_Def']+poke_df['HP'])\npoke_df['stats_type'] = (poke_df['Attack'] + poke_df['Sp_Atk'] + poke_df['Speed']) - (poke_df['Defense'] + poke_df['Sp_Def'] + poke_df['HP'])","e5b3f59c":"poke_df = poke_df.drop(columns='Name')","8b1a7814":"poke_df['Legendary'] = poke_df['Legendary'].astype(int)","f0af5449":"poke_df.head()","157b8281":"type_df['type_attack_mean'] = type_df.iloc[:, 1:19].mean(axis=1)\ntype_df['type_defence_mean'] = type_df.iloc[:, 1:19].mean(axis=0).reset_index(drop=True)\ntype_df","a4464c02":"def effective_type(row):\n    \n    effective_list = row[row>1].index.tolist()\n    return effective_list\n\ndef not_effective_type(row):\n    \n    not_effective_list = row[row<1].index.tolist()\n    return not_effective_list","689a7f6f":"effective = type_df.iloc[:, 1:19].apply(effective_type, axis=1)\nnot_effective = type_df.iloc[:, 1:19].apply(not_effective_type, axis=1)\n\ntype_df[\"effective\"] = effective\ntype_df[\"not_effective\"] = not_effective\ndisplay(type_df.head())","7a844cb5":"print(team_df.shape)\nfor i in range(6):\n    team_df = pd.merge(team_df, poke_df.add_suffix(f'_{i+1}'), on=f'pokemon_id_{i+1}', how='left')\n    print(team_df.shape)","2ffbb104":"team_df.head()","c5666a9e":"print(team_df.shape)\nfor i in range(6):\n    _tmp1 = pd.merge(team_df[[f'Type_1_{i+1}']], type_df.add_suffix(f'_atk_{i+1}'), left_on=f'Type_1_{i+1}', right_on=f'atck_atk_{i+1}', how='left').drop(columns=f'atck_atk_{i+1}')\n    _tmp2 = pd.merge(team_df[[f'Type_2_{i+1}']], type_df.add_suffix(f'_atk_{i+1}'), left_on=f'Type_2_{i+1}', right_on=f'atck_atk_{i+1}', how='left').drop(columns=f'atck_atk_{i+1}').fillna(0)\n    # \u653b\u6483\u5074\u306f\u76f8\u6027\u3044\u3044\u653b\u6483\u3057\u304b\u4f7f\u308f\u306a\u3044\u60f3\u5b9a\n    _tmp1.iloc[:, 1:19] = np.where(_tmp1.iloc[:, 1:19] >= _tmp2.iloc[:, 1:19], _tmp1.iloc[:, 1:19], _tmp2.iloc[:, 1:19])\n    # \u5e73\u5747\u5024\u306f\u5e73\u5747\u3092\u53d6\u308b\n    _tmp1.iloc[:, 19:21] = (_tmp1.iloc[:, 19:21] + _tmp2.iloc[:, 19:21]) \/ 2\n    _tmp1[f'count_effective_atk_{i+1}'] = 0\n    _tmp1[f'count_not_effective_atk_{i+1}'] = 0\n    for j, r in _tmp1.iterrows():\n        effective_list =  []\n        not_effective_list =  []\n        effective_list.extend(r[f\"effective_atk_{i+1}\"])\n        not_effective_list.extend(r[f\"not_effective_atk_{i+1}\"])\n        if _tmp2.loc[j,f\"effective_atk_{i+1}\"]!=0:\n            effective_list.extend(_tmp2.loc[j, f\"effective_atk_{i+1}\"])\n        if _tmp2.loc[j,f\"not_effective_atk_{i+1}\"]!=0:\n            not_effective_list.extend(_tmp2.loc[j, f\"not_effective_atk_{i+1}\"])\n        effective_list = list(set(effective_list))\n        not_effective_list = list(set(not_effective_list))\n        _tmp1.at[j, f\"effective_atk_{i+1}\"] = effective_list\n        _tmp1.at[j, f\"not_effective_atk_{i+1}\"] = not_effective_list\n        _tmp1.at[j, f\"count_effective_atk_{i+1}\"] = len(effective_list)\n        _tmp1.at[j, f\"count_not_effective_atk_{i+1}\"] = len(not_effective_list)\n    team_df = pd.concat([team_df, _tmp1.iloc[:, 1:]], axis=1)\n    print(team_df.shape)","0fd0a713":"print(team_df.shape)\nfor i in range(6):\n    _tmp1 = pd.merge(team_df[[f'Type_1_{i+1}']], type_df.set_index('atck').T.add_suffix(f'_def_{i+1}'), left_on=f'Type_1_{i+1}', right_index=True, how='left')\n    _tmp2 = pd.merge(team_df[[f'Type_2_{i+1}']], type_df.set_index('atck').T.add_suffix(f'_def_{i+1}'), left_on=f'Type_2_{i+1}', right_index=True, how='left').fillna(0)\n    _idx = _tmp2[_tmp2[f'Type_2_{i+1}']==0].index\n    _tmp3 = _tmp1.loc[_idx].iloc[:, 1:]\n    _idx = _tmp2[_tmp2[f'Type_2_{i+1}']!=0].index\n    _tmp4 = (_tmp1.loc[_idx].iloc[:, 1:] + _tmp2.loc[_idx].iloc[:, 1:]) \/ 2\n    _tmp = pd.concat([_tmp3, _tmp4]).sort_index()\n    _tmp = _tmp.astype('float')\n    team_df = pd.concat([team_df, _tmp], axis=1)\n    print(team_df.shape)","7c7fbdf1":"team_df['Normal_def_1'].unique()","6b93ea00":"types = type_df['atck'].unique()","c8aec4eb":"# \u30c1\u30fc\u30e0\u5225\u306etype\u3054\u3068\u306b\u5bfe\u3059\u308b\u653b\u6483\u306e\u5f37\u3055\nprint(team_df.shape)\nfor t in types:\n    _tmp = team_df[[col for col in team_df.columns if f'{t}_atk' in col]].agg(['mean', 'min', 'max'], axis=1).add_prefix(f'{t}_atk_')\n    team_df = pd.concat([team_df, _tmp], axis=1)\n    print(team_df.shape)","c3342a6e":"# \u30c1\u30fc\u30e0\u5225\u306etype\u3054\u3068\u306b\u5bfe\u3059\u308b\u5b88\u5099\u306e\u5f37\u3055\nprint(team_df.shape)\nfor t in types:\n    _tmp = team_df[[col for col in team_df.columns if f'{t}_def' in col]].agg(['mean', 'min', 'max'], axis=1).add_prefix(f'{t}_def_')\n    team_df = pd.concat([team_df, _tmp], axis=1)\n    print(team_df.shape)","118ed61e":"# team\u5225\u306eeffective\u30bf\u30a4\u30d7\u6570\u3068not effective\u30bf\u30a4\u30d7\u6570\nprint(team_df.shape)\nteam_df['effective_atk_team'] = team_df[['effective_atk_1', 'effective_atk_2', 'effective_atk_3',\n                                         'effective_atk_4', 'effective_atk_5', 'effective_atk_6']].sum(axis=1)\nteam_df['effective_atk_team'] = team_df['effective_atk_team'].apply(lambda x:list(set(x)))\nteam_df['count_effective_atk_team'] = team_df['effective_atk_team'].apply(lambda x:len(x))\nteam_df['not_effective_atk_team'] = team_df[['not_effective_atk_1', 'not_effective_atk_2', 'not_effective_atk_3',\n                                             'not_effective_atk_4', 'not_effective_atk_5', 'not_effective_atk_6']].sum(axis=1)\nteam_df['not_effective_atk_team'] = team_df['not_effective_atk_team'].apply(lambda x:list(set(x)))\nteam_df['count_not_effective_atk_team'] = team_df['not_effective_atk_team'].apply(lambda x:len(x))\nteam_df['eff-noeff_team'] = team_df['count_effective_atk_team'] - team_df['count_not_effective_atk_team']\nprint(team_df.shape)","24af6b53":"# \u7269\u7406\u30bf\u30a4\u30d7\u304b\u7279\u6b8a\u30bf\u30a4\u30d7\u304b\nprint(team_df.shape)\nteam_df[\"total_attack_type\"] = team_df[[col for col in team_df.columns if 'atk_type' in col]].sum(axis=1)\nteam_df[\"total_def_type\"] = team_df[[col for col in team_df.columns if 'def_type' in col]].sum(axis=1)\nprint(team_df.shape)","937e07e5":"# \u30c1\u30fc\u30e0\u5225\u306estats\u5408\u8a08\u306e\u7d71\u8a08\u60c5\u5831\nprint(team_df.shape)\nfor s in ['stats_sum', 'atk_spatk+speed', 'stats_def+hp', 'stats_spdef+hp', 'stats_def+speed',\n          'stats_spdef+speed', 'stats_speed+hp', 'Attack__Sp_Atk_Diff', 'Defense__Sp_Def_Diff']:\n    _tmp = team_df[[col for col in team_df.columns if s in col]].agg(['mean', 'min', 'max'], axis=1).add_prefix(f'{s}_')\n    team_df = pd.concat([team_df, _tmp], axis=1)\n    print(team_df.shape)","7d5c932c":"# \u30c1\u30fc\u30e0\u5225\u306estats\u3054\u3068\u306e\u7d71\u8a08\u60c5\u5831\nprint(team_df.shape)\nfor s in stats:\n    _tmp = team_df[[col for col in team_df.columns if s in col]].agg(['mean', 'min', 'max'], axis=1).add_prefix(f'{s}_')\n    team_df = pd.concat([team_df, _tmp], axis=1)\n    print(team_df.shape)","10a81fb3":"# \u30c1\u30fc\u30e0\u5225\u306estats\u5408\u8a08\u306e\u7d71\u8a08\u60c5\u5831\nprint(team_df.shape)\nfor s in ['atk_type', 'def_type']:\n    _tmp = team_df[[col for col in team_df.columns if s in col]].agg(['sum'], axis=1).add_prefix(f'{s}_')\n    team_df = pd.concat([team_df, _tmp], axis=1)\n    print(team_df.shape)","0135eb64":"# \u30c1\u30fc\u30e0\u5225\u306e\u7279\u6027\u3002\u653b\u6483\u7684\u304b\u9632\u5fa1\u7684\u304b\nprint(team_df.shape)\nfor s in ['stats_type']:\n    _tmp = team_df[[col for col in team_df.columns if s in col]].agg(['mean'], axis=1).add_prefix(f'{s}_')\n    team_df = pd.concat([team_df, _tmp], axis=1)\n    print(team_df.shape)","8caf0667":"def count_unique_type(s):\n    return len(s[~s.isnull()].unique())","c3a71a0a":"# type\u306e\u30e6\u30cb\u30fc\u30af\u6570\u3001\u30bf\u30a4\u30d7\nprint(team_df.shape)\nteam_df['total_type_count'] = team_df[[col for col in team_df.columns if 'Type' in col]].T.apply(lambda x: count_unique_type(x))\n# team_df['type_unique'] = team_df[[col for col in team_df.columns if 'Type' in col]].T.apply(lambda x: x.unique())\nprint(team_df.shape)","aa91f59b":"# \u4f1d\u8aac\u306e\u6570\nteam_df['legend_count'] = team_df[[col for col in team_df.columns if 'Legendary' in col]].sum(axis=1)","d35fd5b0":"# \u30bf\u30a4\u30d7\u5225\u306estats\u5e73\u5747\nout = pd.DataFrame()\nout2 = pd.DataFrame()\nfor i in range(1,7):\n    _tmp = team_df.groupby(f'types_{i}')[f'Speed_{i}', f'HP_{i}', f'stats_sum_{i}',\n                                         f'atk_spatk+speed_{i}', f'stats_def+hp_{i}', f'stats_spdef+hp_{i}'].mean()\n    out = pd.concat([out, _tmp])\nfor s in ['Speed', 'HP', 'stats_sum', 'atk_spatk+speed', 'stats_def+hp', 'stats_spdef+hp']:\n    out[s] = out[[col for col in out.columns if s in col]].mean(axis=1)\n    _tmp = out.groupby(out.index)[s].mean()\n    out2 = pd.concat([out2, _tmp], axis=1)\nout2 = out2.add_prefix('types_').add_suffix('_mean')","4142f917":"print(team_df.shape)\nfor i in range(1,7):\n    team_df = pd.merge(team_df, out2.add_suffix(f'_{i}'), left_on=f'types_{i}', right_index=True, how='left')\nprint(team_df.shape)","4233614b":"print(train.shape)\ntrain = pd.merge(train, team_df.add_prefix('first_'), left_on='first', right_on='first_team_id', how='left').drop(columns='first_team_id')\nprint(train.shape)\ntrain = pd.merge(train, team_df.add_prefix('second_'), left_on='second', right_on='second_team_id', how='left').drop(columns='second_team_id')\nprint(train.shape)","709e0f37":"train.head()","91fcc23d":"print(test.shape)\ntest = pd.merge(test, team_df.add_prefix('first_'), left_on='first', right_on='first_team_id', how='left').drop(columns='first_team_id')\nprint(test.shape)\ntest = pd.merge(test, team_df.add_prefix('second_'), left_on='second', right_on='second_team_id', how='left').drop(columns='second_team_id')\nprint(test.shape)","41e57441":"test.head()","c0ed9a3b":"def calc_eff_type(s, col, n):\n    diff_list = []\n    for i in range(1,7):\n        f_in = 0\n        s_in = 0\n        for j in [1,2]:\n            if s[f'second_Type_{j}_{i}'] in s[f'first_{col}_{n}']:\n                f_in += 1\n            if s[f'first_Type_{j}_{n}'] in s[f'second_{col}_{i}']:\n                s_in += 1\n        diff_list.append(f_in - s_in)\n    return diff_list","62df98c6":"# \u30dd\u30b1\u30e2\u30f3\u3054\u3068\u3001\u76f8\u624b\u3078\u306e\u653b\u6483\u306e\u52b9\u304d\u3084\u3059\u3055\u306e\u5dee\nprint(train.shape)\nfor col in ['effective_atk']:\n    for n in range(1,7):\n        train['diff_list'] = train.apply(calc_eff_type, args=(col, n), axis=1)\n        train[f'first-second_{col}_adv_{n}'] = train['diff_list'].apply(lambda x: sum([i > 0 for i in x]))\n        train[f'first-second_{col}_mean_{n}'] = train['diff_list'].apply(lambda x: np.mean(x))\n        train = train.drop(columns='diff_list')\nprint(train.shape)","4645507a":"# \u30dd\u30b1\u30e2\u30f3\u3054\u3068\u3001\u76f8\u624b\u3078\u306e\u653b\u6483\u306e\u52b9\u304d\u306b\u304f\u3055\u306e\u5dee\nprint(train.shape)\nfor col in ['not_effective_atk']:\n    for n in range(1,7):\n        train['diff_list'] = train.apply(calc_eff_type, args=(col, n), axis=1)\n        train[f'first-second_{col}_adv_{n}'] = train['diff_list'].apply(lambda x: sum([i < 0 for i in x]))\n        train[f'first-second_{col}_mean_{n}'] = train['diff_list'].apply(lambda x: np.mean(x))\n        train = train.drop(columns='diff_list')\nprint(train.shape)","b5a860bb":"print(train.shape)\ntrain['first-second_effective_atk_adv_sum'] = train[[col for col in train.columns if 'first-second_effective_atk_adv_' in col]].sum(axis=1)\ntrain['first-second_effective_atk_mean_mean'] = train[[col for col in train.columns if 'first-second_effective_atk_mean_' in col]].mean(axis=1)\ntrain['first-second_not_effective_atk_adv_sum'] = train[[col for col in train.columns if 'first-second_not_effective_atk_adv_' in col]].sum(axis=1)\ntrain['first-second_not_effective_atk_mean_mean'] = train[[col for col in train.columns if 'first-second_not_effective_atk_mean_' in col]].mean(axis=1)\nprint(train.shape)","ce4af133":"def calc_stats(s, stats, n):\n    diff_list = []\n    for i in range(1,7):\n        diff_list.append(s[f'first_{stats}_{n}'] - s[f'second_{stats}_{i}'])\n    return diff_list","a3588d81":"# \u30dd\u30b1\u30e2\u30f3\u306e\u5bfe\u6226\u7d44\u5408\u305b\u3054\u3068\u306estats\u306e\u5dee\nprint(train.shape)\nfor col in ['Speed', 'HP', 'stats_sum', 'atk_spatk+speed', 'stats_def+hp', 'stats_spdef+hp','stats_def+speed',\n            'stats_spdef+speed', 'stats_speed+hp']:\n    for n in range(1,7):\n        train['diff_list'] = train.apply(calc_stats, args=(col, n), axis=1)\n        train[f'first-second_{col}_adv_{n}'] = train['diff_list'].apply(lambda x: sum([i > 0 for i in x]))\n        train[f'first-second_{col}_mean_{n}'] = train['diff_list'].apply(lambda x: np.mean(x))\n        train = train.drop(columns='diff_list')\nprint(train.shape)","3d73ce08":"print(train.shape)\ntrain['first-second_Speed_adv_sum'] = train[[col for col in train.columns if 'first-second_Speed_adv_' in col]].sum(axis=1)\ntrain['first-second_Speed_mean_mean'] = train[[col for col in train.columns if 'first-second_Speed_mean_' in col]].mean(axis=1)\ntrain['first-second_HP_adv_sum'] = train[[col for col in train.columns if 'first-second_HP_adv_' in col]].sum(axis=1)\ntrain['first-second_HP_mean_mean'] = train[[col for col in train.columns if 'first-second_HP_mean_' in col]].mean(axis=1)\ntrain['first-second_stats_sum_adv_sum'] = train[[col for col in train.columns if 'first-second_stats_sum_adv_' in col]].sum(axis=1)\ntrain['first-second_stats_sum_mean_mean'] = train[[col for col in train.columns if 'first-second_stats_sum_mean_' in col]].mean(axis=1)\ntrain['first-second_atk_spatk+speed_adv_sum'] = train[[col for col in train.columns if 'first-second_atk_spatk+speed_adv_' in col]].sum(axis=1)\ntrain['first-second_atk_spatk+speed_mean_mean'] = train[[col for col in train.columns if 'first-second_atk_spatk+speed_mean_' in col]].mean(axis=1)\ntrain['first-second_stats_def+hp_adv_sum'] = train[[col for col in train.columns if 'first-second_stats_def+hp_adv_' in col]].sum(axis=1)\ntrain['first-second_stats_def+hp_mean_mean'] = train[[col for col in train.columns if 'first-second_stats_def+hp_mean_' in col]].mean(axis=1)\ntrain['first-second_stats_spdef+hp_adv_sum'] = train[[col for col in train.columns if 'first-second_stats_spdef+hp_adv_' in col]].sum(axis=1)\ntrain['first-second_stats_spdef+hp_mean_mean'] = train[[col for col in train.columns if 'first-second_stats_spdef+hp_mean_' in col]].mean(axis=1)\ntrain['first-second_stats_def+speed_adv_sum'] = train[[col for col in train.columns if 'first-second_stats_def+speed_adv_' in col]].sum(axis=1)\ntrain['first-second_stats_def+speed_mean_mean'] = train[[col for col in train.columns if 'first-second_stats_def+speed_mean_' in col]].mean(axis=1)\ntrain['first-second_stats_spdef+speed_adv_sum'] = train[[col for col in train.columns if 'first-second_stats_spdef+speed_adv_' in col]].sum(axis=1)\ntrain['first-second_stats_spdef+speed_mean_mean'] = train[[col for col in train.columns if 'first-second_stats_spdef+speed_mean_' in col]].mean(axis=1)\ntrain['first-second_stats_speed+hp_adv_sum'] = train[[col for col in train.columns if 'first-second_stats_speed+hp_adv_' in col]].sum(axis=1)\ntrain['first-second_stats_speed+hp_mean_mean'] = train[[col for col in train.columns if 'first-second_stats_speed+hp_mean_' in col]].mean(axis=1)\nprint(train.shape)","7083cfdd":"# \u30dd\u30b1\u30e2\u30f3\u306e\u5bfe\u6226\u7d44\u5408\u305b\u3054\u3068\u306estats\u306e\u5dee\nprint(test.shape)\nfor col in ['Speed', 'HP', 'stats_sum', 'atk_spatk+speed', 'stats_def+hp', 'stats_spdef+hp','stats_def+speed',\n            'stats_spdef+speed', 'stats_speed+hp']:\n    for n in range(1,7):\n        test['diff_list'] = test.apply(calc_stats, args=(col, n), axis=1)\n        test[f'first-second_{col}_adv_{n}'] = test['diff_list'].apply(lambda x: sum([i > 0 for i in x]))\n        test[f'first-second_{col}_mean_{n}'] = test['diff_list'].apply(lambda x: np.mean(x))\n        test = test.drop(columns='diff_list')\nprint(test.shape)","8d2aa214":"print(test.shape)\ntest['first-second_Speed_adv_sum'] = test[[col for col in test.columns if 'first-second_Speed_adv_' in col]].sum(axis=1)\ntest['first-second_Speed_mean_mean'] = test[[col for col in test.columns if 'first-second_Speed_mean_' in col]].mean(axis=1)\ntest['first-second_HP_adv_sum'] = test[[col for col in test.columns if 'first-second_HP_adv_' in col]].sum(axis=1)\ntest['first-second_HP_mean_mean'] = test[[col for col in test.columns if 'first-second_HP_mean_' in col]].mean(axis=1)\ntest['first-second_stats_sum_adv_sum'] = test[[col for col in test.columns if 'first-second_stats_sum_adv_' in col]].sum(axis=1)\ntest['first-second_stats_sum_mean_mean'] = test[[col for col in test.columns if 'first-second_stats_sum_mean_' in col]].mean(axis=1)\ntest['first-second_atk_spatk+speed_adv_sum'] = test[[col for col in test.columns if 'first-second_atk_spatk+speed_adv_' in col]].sum(axis=1)\ntest['first-second_atk_spatk+speed_mean_mean'] = test[[col for col in test.columns if 'first-second_atk_spatk+speed_mean_' in col]].mean(axis=1)\ntest['first-second_stats_def+hp_adv_sum'] = test[[col for col in test.columns if 'first-second_stats_def+hp_adv_' in col]].sum(axis=1)\ntest['first-second_stats_def+hp_mean_mean'] = test[[col for col in test.columns if 'first-second_stats_def+hp_mean_' in col]].mean(axis=1)\ntest['first-second_stats_spdef+hp_adv_sum'] = test[[col for col in test.columns if 'first-second_stats_spdef+hp_adv_' in col]].sum(axis=1)\ntest['first-second_stats_spdef+hp_mean_mean'] = test[[col for col in test.columns if 'first-second_stats_spdef+hp_mean_' in col]].mean(axis=1)\ntest['first-second_stats_def+speed_adv_sum'] = test[[col for col in test.columns if 'first-second_stats_def+speed_adv_' in col]].sum(axis=1)\ntest['first-second_stats_def+speed_mean_mean'] = test[[col for col in test.columns if 'first-second_stats_def+speed_mean_' in col]].mean(axis=1)\ntest['first-second_stats_spdef+speed_adv_sum'] = test[[col for col in test.columns if 'first-second_stats_spdef+speed_adv_' in col]].sum(axis=1)\ntest['first-second_stats_spdef+speed_mean_mean'] = test[[col for col in test.columns if 'first-second_stats_spdef+speed_mean_' in col]].mean(axis=1)\ntest['first-second_stats_speed+hp_adv_sum'] = test[[col for col in test.columns if 'first-second_stats_speed+hp_adv_' in col]].sum(axis=1)\ntest['first-second_stats_speed+hp_mean_mean'] = test[[col for col in test.columns if 'first-second_stats_speed+hp_mean_' in col]].mean(axis=1)\nprint(test.shape)","07d5c5d8":"def calc_damage(s, n):\n    damage_list = []\n    for i in range(1,7):\n        if (s[f'first_atk_type_{n}']==0)&(s[f'second_atk_type_{i}']==0):\n            damage_list.append((s[f'first_atk_spatk+speed_{n}'] \/ s[f'second_Defense_{i}']) - (s[f'second_atk_spatk+speed_{n}'] \/ s[f'first_Defense_{i}']))\n        elif (s[f'first_atk_type_{n}']==1)&(s[f'second_atk_type_{i}']==0):\n            damage_list.append((s[f'first_atk_spatk+speed_{n}'] \/ s[f'second_Sp_Def_{i}']) - (s[f'second_atk_spatk+speed_{n}'] \/ s[f'first_Defense_{i}']))\n        elif (s[f'first_atk_type_{n}']==0)&(s[f'second_atk_type_{i}']==1):\n            damage_list.append((s[f'first_atk_spatk+speed_{n}'] \/ s[f'second_Defense_{i}']) - (s[f'second_atk_spatk+speed_{n}'] \/ s[f'first_Sp_Def_{i}']))\n        elif (s[f'first_atk_type_{n}']==1)&(s[f'second_atk_type_{i}']==1):\n            damage_list.append((s[f'first_atk_spatk+speed_{n}'] \/ s[f'second_Sp_Def_{i}']) - (s[f'second_atk_spatk+speed_{n}'] \/ s[f'first_Sp_Def_{i}']))\n    return damage_list","cf6b2e86":"# \u30dd\u30b1\u30e2\u30f3\u306e\u5bfe\u6226\u7d44\u5408\u305b\u3054\u3068\u306e\u4e0e\u3048\u308b\u60f3\u5b9a\u30c0\u30e1\u30fc\u30b8\nprint(train.shape)\nfor n in range(1,7):\n    train['damage_list'] = train.apply(calc_damage, args=(n, ), axis=1)\n    train[f'first-second_damage_adv_{n}'] = train['damage_list'].apply(lambda x: sum([i > 0 for i in x]))\n    train[f'first-second_damage_mean_{n}'] = train['damage_list'].apply(lambda x: np.mean(x))\n    train = train.drop(columns='damage_list')\nprint(train.shape)","c5168041":"print(train.shape)\ntrain['first-second_damage_adv_sum'] = train[[col for col in train.columns if 'first-second_damage_adv_' in col]].sum(axis=1)\ntrain['first-second_damage_mean_mean'] = train[[col for col in train.columns if 'first-second_damage_mean_' in col]].mean(axis=1)\nprint(train.shape)","19048eda":"# \u30dd\u30b1\u30e2\u30f3\u306e\u5bfe\u6226\u7d44\u5408\u305b\u3054\u3068\u306e\u4e0e\u3048\u308b\u60f3\u5b9a\u30c0\u30e1\u30fc\u30b8\nprint(test.shape)\nfor n in range(1,7):\n    test['damage_list'] = test.apply(calc_damage, args=(n, ), axis=1)\n    test[f'first-second_damage_adv_{n}'] = test['damage_list'].apply(lambda x: sum([i > 0 for i in x]))\n    test[f'first-second_damage_mean_{n}'] = test['damage_list'].apply(lambda x: np.mean(x))\n    test = test.drop(columns='damage_list')\nprint(test.shape)","6d5614d0":"print(test.shape)\ntest['first-second_damage_adv_sum'] = test[[col for col in test.columns if 'first-second_damage_adv_' in col]].sum(axis=1)\ntest['first-second_damage_mean_mean'] = test[[col for col in test.columns if 'first-second_damage_mean_' in col]].mean(axis=1)\nprint(test.shape)","1832853b":"def calc_damage_hp(s, n):\n    damage_list = []\n    for i in range(1,7):\n        if (s[f'first_atk_type_{n}']==0)&(s[f'second_atk_type_{i}']==0):\n            damage_list.append((s[f'second_HP_{i}'] \/ s[f'first_atk_spatk+speed_{n}'] \/ s[f'second_Defense_{i}']) - (s[f'first_HP_{n}'] \/ s[f'second_atk_spatk+speed_{i}'] \/ s[f'first_Defense_{n}']))\n        elif (s[f'first_atk_type_{n}']==1)&(s[f'second_atk_type_{i}']==0):\n            damage_list.append((s[f'second_HP_{i}'] \/ s[f'first_atk_spatk+speed_{n}'] \/ s[f'second_Sp_Def_{i}']) - (s[f'first_HP_{n}'] \/ s[f'second_atk_spatk+speed_{i}'] \/ s[f'first_Defense_{n}']))\n        elif (s[f'first_atk_type_{n}']==0)&(s[f'second_atk_type_{i}']==1):\n            damage_list.append((s[f'second_HP_{i}'] \/ s[f'first_atk_spatk+speed_{n}'] \/ s[f'second_Defense_{i}']) - (s[f'first_HP_{n}'] \/ s[f'second_atk_spatk+speed_{i}'] \/ s[f'first_Sp_Def_{n}']))\n        elif (s[f'first_atk_type_{n}']==1)&(s[f'second_atk_type_{i}']==1):\n            damage_list.append((s[f'second_HP_{i}'] \/ s[f'first_atk_spatk+speed_{n}'] \/ s[f'second_Sp_Def_{i}']) - (s[f'first_HP_{n}'] \/ s[f'second_atk_spatk+speed_{i}'] \/ s[f'first_Sp_Def_{n}']))\n    return damage_list","58f0da09":"# \u30dd\u30b1\u30e2\u30f3\u306e\u5bfe\u6226\u7d44\u5408\u305b\u3054\u3068\u306e\u4e0e\u3048\u308b\u60f3\u5b9a\u30c0\u30e1\u30fc\u30b8\u306e\u76f8\u624b\u306eHP\u8003\u616e\nprint(train.shape)\nfor n in range(1,7):\n    train['damage_list'] = train.apply(calc_damage_hp, args=(n, ), axis=1)\n    train[f'first-second_hp_damage_adv_{n}'] = train['damage_list'].apply(lambda x: sum([i < 0 for i in x]))\n    train[f'first-second_hp_damage_mean_{n}'] = train['damage_list'].apply(lambda x: np.mean(x))\n    train = train.drop(columns='damage_list')\nprint(train.shape)","360afa2d":"print(train.shape)\ntrain['first-second_hp_damage_adv_sum'] = train[[col for col in train.columns if 'first-second_hp_damage_adv_' in col]].sum(axis=1)\ntrain['first-second_hp_damage_mean_mean'] = train[[col for col in train.columns if 'first-second_hp_damage_mean_' in col]].mean(axis=1)\nprint(train.shape)","d65444fc":"# \u30dd\u30b1\u30e2\u30f3\u306e\u5bfe\u6226\u7d44\u5408\u305b\u3054\u3068\u306e\u4e0e\u3048\u308b\u60f3\u5b9a\u30c0\u30e1\u30fc\u30b8\u306e\u76f8\u624b\u306eHP\u8003\u616e\nprint(test.shape)\nfor n in range(1,7):\n    test['damage_list'] = test.apply(calc_damage_hp, args=(n, ), axis=1)\n    test[f'first-second_hp_damage_adv_{n}'] = test['damage_list'].apply(lambda x: sum([i < 0 for i in x]))\n    test[f'first-second_hp_damage_mean_{n}'] = test['damage_list'].apply(lambda x: np.mean(x))\n    test = test.drop(columns='damage_list')\nprint(test.shape)","fd3848cf":"print(test.shape)\ntest['first-second_hp_damage_adv_sum'] = test[[col for col in test.columns if 'first-second_hp_damage_adv_' in col]].sum(axis=1)\ntest['first-second_hp_damage_mean_mean'] = test[[col for col in test.columns if 'first-second_hp_damage_mean_' in col]].mean(axis=1)\nprint(test.shape)","42d70e2c":"# fitst\u3068second\u306e\u30c1\u30fc\u30e0\u5225\u306estats\u306e\u4f59\n# \u629c\u3044\u3066\u3082\u3044\u3044\u304b\u3082\nprint(train.shape)\nfor col in ['Speed', 'HP', 'stats_sum', 'atk_spatk+speed', 'stats_def+hp', 'stats_spdef+hp']:\n    train[f'first\/second_{col}_mean'] = train[f'first_{col}_mean'] \/ train[f'second_{col}_mean']\n    train[f'first\/second_{col}_min'] = train[f'first_{col}_min'] \/ train[f'second_{col}_min']\n    train[f'first\/second_{col}_max'] = train[f'first_{col}_max'] \/ train[f'second_{col}_max']\nprint(train.shape)","f527b5dc":"# fitst\u3068second\u306e\u30c1\u30fc\u30e0\u5225\u306estats\u306e\u4f59\n# \u629c\u3044\u3066\u3082\u3044\u3044\u304b\u3082\nprint(test.shape)\nfor col in ['Speed', 'HP', 'stats_sum', 'atk_spatk+speed', 'stats_def+hp', 'stats_spdef+hp']:\n    test[f'first\/second_{col}_mean'] = test[f'first_{col}_mean'] \/ test[f'second_{col}_mean']\n    test[f'first\/second_{col}_min'] = test[f'first_{col}_min'] \/ test[f'second_{col}_min']\n    test[f'first\/second_{col}_max'] = test[f'first_{col}_max'] \/ test[f'second_{col}_max']\nprint(test.shape)","c5aecb92":"# \u30dd\u30b1\u30e2\u30f3\u3054\u3068\u306e\u30bf\u30a4\u30d7\u76f8\u6027\u306e\u5dee\ndef calc_type_comp(s, n):\n    diff_list = []\n    for i in range(1,7):\n        if (type(s[f'first_Type_2_{n}'])==float)&(type(s[f'second_Type_2_{i}'])==float):\n            first_atk = s[f'first_{s[f\"second_Type_1_{i}\"]}_atk_{n}']\n            second_atk = s[f'second_{s[f\"first_Type_1_{n}\"]}_atk_{i}']\n        elif (type(s[f'first_Type_2_{n}'])!=float)&(type(s[f'second_Type_2_{i}'])==float):\n            first_atk = s[f'first_{s[f\"second_Type_1_{i}\"]}_atk_{n}']\n            second_atk = s[f'second_{s[f\"first_Type_1_{n}\"]}_atk_{i}'] * s[f'second_{s[f\"first_Type_2_{n}\"]}_atk_{i}']\n        elif (type(s[f'first_Type_2_{n}'])==float)&(type(s[f'second_Type_2_{i}'])!=float):\n            first_atk = s[f'first_{s[f\"second_Type_1_{i}\"]}_atk_{n}'] * s[f'first_{s[f\"second_Type_2_{i}\"]}_atk_{n}']\n            second_atk = s[f'second_{s[f\"first_Type_1_{n}\"]}_atk_{i}']\n        elif (type(s[f'first_Type_2_{n}'])!=float)&(type(s[f'second_Type_2_{i}'])!=float):\n            first_atk = s[f'first_{s[f\"second_Type_1_{i}\"]}_atk_{n}'] * s[f'first_{s[f\"second_Type_2_{i}\"]}_atk_{n}']\n            second_atk = s[f'second_{s[f\"first_Type_1_{n}\"]}_atk_{i}'] * s[f'second_{s[f\"first_Type_2_{n}\"]}_atk_{i}']\n        diff_list.append(first_atk - second_atk)\n    return diff_list","9db2cb57":"# \u30dd\u30b1\u30e2\u30f3\u3054\u3068\u306e\u30bf\u30a4\u30d7\u76f8\u6027\u306e\u5dee\nprint(train.shape)\nfor n in range(1,7):\n    train['diff_list'] = train.apply(calc_type_comp, args=(n,), axis=1)\n    train[f'first-second_typecomp_adv_{n}'] = train['diff_list'].apply(lambda x: sum([i > 0 for i in x]))\n    train[f'first-second_typecomp_mean_{n}'] = train['diff_list'].apply(lambda x: np.mean(x))\n    train = train.drop(columns='diff_list')\nprint(train.shape)","b2f45350":"print(train.shape)\ntrain['first-second_typecomp_adv_sum'] = train[[col for col in train.columns if 'first-second_typecomp_adv_' in col]].sum(axis=1)\ntrain['first-second_typecomp_mean_mean'] = train[[col for col in train.columns if 'first-second_typecomp_mean_' in col]].mean(axis=1)\nprint(train.shape)","2ae6ebc7":"# \u30dd\u30b1\u30e2\u30f3\u3054\u3068\u306e\u30bf\u30a4\u30d7\u76f8\u6027\u306e\u5dee\nprint(test.shape)\nfor n in range(1,7):\n    test['diff_list'] = test.apply(calc_type_comp, args=(n,), axis=1)\n    test[f'first-second_typecomp_adv_{n}'] = test['diff_list'].apply(lambda x: sum([i > 0 for i in x]))\n    test[f'first-second_typecomp_mean_{n}'] = test['diff_list'].apply(lambda x: np.mean(x))\n    test = test.drop(columns='diff_list')\nprint(test.shape)","8d486b03":"print(test.shape)\ntest['first-second_typecomp_adv_sum'] = test[[col for col in test.columns if 'first-second_typecomp_adv_' in col]].sum(axis=1)\ntest['first-second_typecomp_mean_mean'] = test[[col for col in test.columns if 'first-second_typecomp_mean_' in col]].mean(axis=1)\nprint(test.shape)","fa4a4ce0":"def calc_damage_hp_type(s, n):\n    damage_list = []\n    for i in range(1,7):\n        if (type(s[f'first_Type_2_{n}'])==float)&(type(s[f'second_Type_2_{i}'])==float):\n            first_atk = s[f'first_{s[f\"second_Type_1_{i}\"]}_atk_{n}']\n            second_atk = s[f'second_{s[f\"first_Type_1_{n}\"]}_atk_{i}']\n        elif (type(s[f'first_Type_2_{n}'])!=float)&(type(s[f'second_Type_2_{i}'])==float):\n            first_atk = s[f'first_{s[f\"second_Type_1_{i}\"]}_atk_{n}']\n            second_atk = s[f'second_{s[f\"first_Type_1_{n}\"]}_atk_{i}'] * s[f'second_{s[f\"first_Type_2_{n}\"]}_atk_{i}']\n        elif (type(s[f'first_Type_2_{n}'])==float)&(type(s[f'second_Type_2_{i}'])!=float):\n            first_atk = s[f'first_{s[f\"second_Type_1_{i}\"]}_atk_{n}'] * s[f'first_{s[f\"second_Type_2_{i}\"]}_atk_{n}']\n            second_atk = s[f'second_{s[f\"first_Type_1_{n}\"]}_atk_{i}']\n        elif (type(s[f'first_Type_2_{n}'])!=float)&(type(s[f'second_Type_2_{i}'])!=float):\n            first_atk = s[f'first_{s[f\"second_Type_1_{i}\"]}_atk_{n}'] * s[f'first_{s[f\"second_Type_2_{i}\"]}_atk_{n}']\n            second_atk = s[f'second_{s[f\"first_Type_1_{n}\"]}_atk_{i}'] * s[f'second_{s[f\"first_Type_2_{n}\"]}_atk_{i}']\n        if (s[f'first_atk_type_{n}']==0)&(s[f'second_atk_type_{i}']==0):\n            damage_list.append((s[f'second_HP_{i}'] \/ s[f'first_atk_spatk+speed_{n}'] \/ s[f'second_Defense_{i}'] * first_atk) - (s[f'first_HP_{n}'] \/ s[f'second_atk_spatk+speed_{i}'] \/ s[f'first_Defense_{n}'] * second_atk))\n        elif (s[f'first_atk_type_{n}']==1)&(s[f'second_atk_type_{i}']==0):\n            damage_list.append((s[f'second_HP_{i}'] \/ s[f'first_atk_spatk+speed_{n}'] \/ s[f'second_Sp_Def_{i}'] * first_atk) - (s[f'first_HP_{n}'] \/ s[f'second_atk_spatk+speed_{i}'] \/ s[f'first_Defense_{n}'] * second_atk))\n        elif (s[f'first_atk_type_{n}']==0)&(s[f'second_atk_type_{i}']==1):\n            damage_list.append((s[f'second_HP_{i}'] \/ s[f'first_atk_spatk+speed_{n}'] \/ s[f'second_Defense_{i}'] * first_atk) - (s[f'first_HP_{n}'] \/ s[f'second_atk_spatk+speed_{i}'] \/ s[f'first_Sp_Def_{n}'] * second_atk))\n        elif (s[f'first_atk_type_{n}']==1)&(s[f'second_atk_type_{i}']==1):\n            damage_list.append((s[f'second_HP_{i}'] \/ s[f'first_atk_spatk+speed_{n}'] \/ s[f'second_Sp_Def_{i}'] * first_atk) - (s[f'first_HP_{n}'] \/ s[f'second_atk_spatk+speed_{i}'] \/ s[f'first_Sp_Def_{n}'] * second_atk))\n    return damage_list","2d825506":"# \u30dd\u30b1\u30e2\u30f3\u3054\u3068\u306e\u30bf\u30a4\u30d7\u76f8\u6027\u8003\u616e\u3057\u305f\u30c0\u30e1\u30fc\u30b8\nprint(train.shape)\nfor n in range(1,7):\n    train['damage_list'] = train.apply(calc_damage_hp_type, args=(n, ), axis=1)\n    train[f'first-second_hp_damage_type_adv_{n}'] = train['damage_list'].apply(lambda x: sum([i < 0 for i in x]))\n    train[f'first-second_hp_damage_type_mean_{n}'] = train['damage_list'].apply(lambda x: np.mean(x))\n    train = train.drop(columns='damage_list')\nprint(train.shape)","500e3f42":"print(train.shape)\ntrain['first-second_hp_damage_type_adv_sum'] = train[[col for col in train.columns if 'first-second_hp_damage_type_adv_' in col]].sum(axis=1)\ntrain['first-second_hp_damage_type_mean_mean'] = train[[col for col in train.columns if 'first-second_hp_damage_type_mean_' in col]].mean(axis=1)\nprint(train.shape)","4629a999":"# \u30dd\u30b1\u30e2\u30f3\u3054\u3068\u306e\u30bf\u30a4\u30d7\u76f8\u6027\u8003\u616e\u3057\u305f\u30c0\u30e1\u30fc\u30b8\nprint(test.shape)\nfor n in range(1,7):\n    test['damage_list'] = test.apply(calc_damage_hp_type, args=(n, ), axis=1)\n    test[f'first-second_hp_damage_type_adv_{n}'] = test['damage_list'].apply(lambda x: sum([i < 0 for i in x]))\n    test[f'first-second_hp_damage_type_mean_{n}'] = test['damage_list'].apply(lambda x: np.mean(x))\n    test = test.drop(columns='damage_list')\nprint(test.shape)","dc3d3811":"print(test.shape)\ntest['first-second_hp_damage_type_adv_sum'] = test[[col for col in test.columns if 'first-second_hp_damage_type_adv_' in col]].sum(axis=1)\ntest['first-second_hp_damage_type_mean_mean'] = test[[col for col in test.columns if 'first-second_hp_damage_type_mean_' in col]].mean(axis=1)\nprint(test.shape)","769e8dcb":"# first\u3068second\u306e\u30e6\u30cb\u30fc\u30af\u30bf\u30a4\u30d7\u6570\u306e\u5dee\nprint(train.shape)\ntrain['first-second_total_type_count'] = train['first_total_type_count'] - train['second_total_type_count']\n# train['first\/second_total_type_count'] = train['first_total_type_count'] \/ train['second_total_type_count']\nprint(train.shape)","276d2e71":"# first\u3068second\u306e\u30e6\u30cb\u30fc\u30af\u30bf\u30a4\u30d7\u6570\u306e\u5dee\nprint(test.shape)\ntest['first-second_total_type_count'] = test['first_total_type_count'] - test['second_total_type_count']\n# test['first\/second_total_type_count'] = test['first_total_type_count'] \/ test['second_total_type_count']\nprint(test.shape)","df9fce40":"cat_cols = ['Type_1', 'Type_2']","8bc2f03b":"train['flag'] = 1\ntest['flag'] = 0\nwhole_df = pd.concat([train , test], axis=0)\nprint(whole_df.shape)","2dc8a3cc":"cat_col_list = []\nfor col in cat_cols:\n    cat_col_list += [n for n in whole_df.columns if col in n]\ncat_col_list += ['first_types_1', 'first_types_2','first_types_3', 'first_types_4', 'first_types_5', 'first_types_6',\n                 'second_types_1', 'second_types_2','second_types_3', 'second_types_4', 'second_types_5', 'second_types_6',]\ncat_col_list","fc5b56d7":"# name_dict = dict(zip(poke_df['Name'].unique(),list(range(len(poke_df['Name'].unique())))))\ntype_dict =  dict(zip(type_df['atck'].unique(),list(range(len(type_df['atck'].unique())))))\ntypes_dict =  dict(zip(poke_df['types'].unique(),list(range(len(poke_df['types'].unique())))))","e91b9641":"# label Encoding\nprint(whole_df.shape)\nfor col in cat_col_list:\n    if 'Type' in col:\n        whole_df[f'LabelEncoder_{col}'] = whole_df[col].map(type_dict).fillna(99)\n    elif 'types' in col:\n        whole_df[f'LabelEncoder_{col}'] = whole_df[col].map(types_dict)\nprint(whole_df.shape)","22e839d6":"# Count Encoding","b3bc438e":"train = whole_df[whole_df['flag']==1]\ntest = whole_df[whole_df['flag']==0]\nprint(train.shape, test.shape)","742293e7":"train.columns","5efa7cdc":"print(train.shape, test.shape)\nfor t in types:\n    train.drop(columns=[col for col in train.columns if t in col])\n    test.drop(columns=[col for col in test.columns if t in col])\nprint(train.shape, test.shape)","8ff5f5e5":"from sklearn import preprocessing, model_selection\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\nfrom sklearn.metrics import mean_squared_error, roc_auc_score, roc_curve\nimport pickle\n\nimport torch\nimport torch.nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport optuna.integration.lightgbm as opt_lgb","14f66deb":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    def __init__(self):\n        self.target_col='target'\n        self.seed=42\n        self.n_fold=6\n        self.trn_fold=[0,1,2,3,4,5] # [0,1,2,3,4,5,6]\n        self.train=True\n#         self.EPOCHS=300 # tabnet\n#         self.PATIENCE=20 # tabnet\n#         self.BATCH_SIZE=32 # tabnet\n#         self.V_BATCH=32 # tabnet\n#         self.WORKERS=2 # tabnet\n#         self.LOSS='rmse' # tabnet\nCONFIG = CFG()","24fb437e":"from sklearn.model_selection._split import _BaseKFold, _RepeatedSplits\nfrom sklearn.utils.validation import check_random_state, column_or_1d\nfrom sklearn.utils.multiclass import type_of_target\nfrom collections import defaultdict\n\nclass StratifiedGroupKFold(_BaseKFold):\n\n    \"\"\"\n    Parameters\n    ----------\n    n_splits : int, default=5\n        Number of folds. Must be at least 2.\n    shuffle : bool, default=False\n        Whether to shuffle each class's samples before splitting into batches.\n        Note that the samples within each split will not be shuffled.\n        This implementation can only shuffle groups that have approximately the\n        same y distribution, no global shuffle will be performed.\n    random_state : int or RandomState instance, default=None\n        When `shuffle` is True, `random_state` affects the ordering of the\n        indices, which controls the randomness of each fold for each class.\n        Otherwise, leave `random_state` as `None`.\n        Pass an int for reproducible output across multiple function calls.\n        See :term:`Glossary <random_state>`.\n\n    \"\"\"\n\n    def __init__(self, n_splits=5, shuffle=False, random_state=None):\n        super().__init__(n_splits=n_splits, shuffle=shuffle,\n                         random_state=random_state)\n\n    def _iter_test_indices(self, X, y, groups):\n\n        rng = check_random_state(self.random_state)\n        y = np.asarray(y)\n        type_of_target_y = type_of_target(y)\n        allowed_target_types = ('binary', 'multiclass')\n        if type_of_target_y not in allowed_target_types:\n            raise ValueError(\n                'Supported target types are: {}. Got {!r} instead.'.format(\n                    allowed_target_types, type_of_target_y))\n\n        y = column_or_1d(y)\n        _, y_inv, y_cnt = np.unique(y, return_inverse=True, return_counts=True)\n        if np.all(self.n_splits > y_cnt):\n            raise ValueError(\"n_splits=%d cannot be greater than the\"\n                             \" number of members in each class.\"\n                             % (self.n_splits))\n        n_smallest_class = np.min(y_cnt)\n        if self.n_splits > n_smallest_class:\n            warnings.warn((\"The least populated class in y has only %d\"\n                           \" members, which is less than n_splits=%d.\"\n                           % (n_smallest_class, self.n_splits)), UserWarning)\n        n_classes = len(y_cnt)\n        \n        \n        _, groups_inv, groups_cnt = np.unique(\n            groups, return_inverse=True, return_counts=True)\n        y_counts_per_group = np.zeros((len(groups_cnt), n_classes))\n        for class_idx, group_idx in zip(y_inv, groups_inv):\n            y_counts_per_group[group_idx, class_idx] += 1\n\n        y_counts_per_fold = np.zeros((self.n_splits, n_classes))\n        groups_per_fold = defaultdict(set)\n\n        if self.shuffle:\n            rng.shuffle(y_counts_per_group)\n\n        # Stable sort to keep shuffled order for groups with the same\n        # class distribution variance\n        sorted_groups_idx = np.argsort(-np.std(y_counts_per_group, axis=1),\n                                       kind='mergesort')\n\n        for group_idx in sorted_groups_idx:\n            group_y_counts = y_counts_per_group[group_idx]\n            best_fold = self._find_best_fold(\n                y_counts_per_fold=y_counts_per_fold, y_cnt=y_cnt,\n                group_y_counts=group_y_counts)\n            y_counts_per_fold[best_fold] += group_y_counts\n            groups_per_fold[best_fold].add(group_idx)\n\n        for i in range(self.n_splits):\n            test_indices = [idx for idx, group_idx in enumerate(groups_inv)\n                            if group_idx in groups_per_fold[i]]\n            yield test_indices\n\n    def _find_best_fold(\n            self, y_counts_per_fold, y_cnt, group_y_counts):\n        best_fold = None\n        min_eval = np.inf\n        min_samples_in_fold = np.inf\n        for i in range(self.n_splits):\n            y_counts_per_fold[i] += group_y_counts\n            # Summarise the distribution over classes in each proposed fold\n            std_per_class = np.std(\n                y_counts_per_fold \/ y_cnt.reshape(1, -1),\n                axis=0)\n            y_counts_per_fold[i] -= group_y_counts\n            fold_eval = np.mean(std_per_class)\n            samples_in_fold = np.sum(y_counts_per_fold[i])\n            is_current_fold_better = (\n                fold_eval < min_eval or\n                np.isclose(fold_eval, min_eval)\n                and samples_in_fold < min_samples_in_fold\n            )\n            if is_current_fold_better:\n                min_eval = fold_eval\n                min_samples_in_fold = samples_in_fold\n                best_fold = i\n        return best_fold\n","339a4bfa":"Fold = StratifiedGroupKFold(n_splits=CONFIG.n_fold)\nfor n, (train_index, val_index) in enumerate(Fold.split(train, train['target'], train['first'])):\n    train.loc[val_index, 'fold'] = int(n)\ntrain['fold'] = train['fold'].astype(int)\ndisplay(train.groupby(['fold', 'first']).size())\ndisplay(train.groupby(['fold', 'target']).size())","0f99a705":"# Fold = GroupKFold(n_splits=CONFIG.n_fold)\n# for n, (train_index, val_index) in enumerate(Fold.split(train, train['target'], train['first'])):\n#     train.loc[val_index, 'fold'] = int(n)\n# train['fold'] = train['fold'].astype(int)\n# train.groupby(['fold', 'first']).size()","44d6668d":"# ====================================================\n# Utils\n# ====================================================\ndef init_logger(log_file=OUTPUT_DIR+'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=CONFIG.seed)","79e18cc6":"# ====================================================\n# Helper Functions\n# ====================================================\ndef visualize_importance(models, feat_train_df):\n    \"\"\"lightGBM \u306e model \u914d\u5217\u306e feature importance \u3092 plot \u3059\u308b\n    CV\u3054\u3068\u306e\u30d6\u30ec\u3092 boxen plot \u3068\u3057\u3066\u8868\u73fe\u3057\u307e\u3059.\n\n    args:\n        models:\n            List of lightGBM models\n        feat_train_df:\n            \u5b66\u7fd2\u6642\u306b\u4f7f\u3063\u305f DataFrame\n    \"\"\"\n    feature_importance_df = pd.DataFrame()\n    for i, model in enumerate(models):\n        _df = pd.DataFrame()\n        _df['feature_importance'] = model.feature_importance(importance_type='gain')\n        _df['column'] = feat_train_df.columns\n        _df['fold'] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, _df], axis=0, ignore_index=True)\n\n    order = feature_importance_df.groupby('column')\\\n        .sum()[['feature_importance']]\\\n        .sort_values('feature_importance', ascending=False).index[:50]\n\n    fig, ax = plt.subplots(figsize=(max(6, len(order) * .4), 7))\n    sns.boxenplot(data=feature_importance_df, x='column', y='feature_importance', order=order, ax=ax, palette='viridis')\n    ax.tick_params(axis='x', rotation=90)\n    ax.grid()\n    fig.tight_layout()\n    return fig, ax","d91d61a2":"params_lgbm = {\n        'task': 'train',\n        'boosting_type': 'gbdt',\n        'learning_rate': 0.05,\n        'objective': 'binary',\n        'metric': 'binary_logloss',\n        'max_depth': -1,\n        'n_jobs': -1,\n        'feature_fraction': 0.7,\n        'bagging_fraction': 0.7,\n        'lambda_l2': 1,\n        'verbosity': 0, # -1, 0\n        'random_state': CONFIG.seed,\n        'force_col_wise':True\n        #'bagging_freq': 5\n}","a2e8e364":"# first, second\u3092\u9664\u5916\ncats = ['first_Generation_1', 'first_Generation_2', 'first_Generation_3',\n        'first_Generation_4', 'first_Generation_5', 'first_Generation_6',\n        'second_Generation_1', 'second_Generation_2', 'second_Generation_3',\n        'second_Generation_4', 'second_Generation_5', 'second_Generation_6',\n        'LabelEncoder_first_Type_1_1', 'LabelEncoder_first_Type_1_2', 'LabelEncoder_first_Type_1_3',\n        'LabelEncoder_first_Type_1_4', 'LabelEncoder_first_Type_1_5', 'LabelEncoder_first_Type_1_6',\n        'LabelEncoder_first_Type_2_1', 'LabelEncoder_first_Type_2_2', 'LabelEncoder_first_Type_2_3', \n        'LabelEncoder_first_Type_2_4', 'LabelEncoder_first_Type_2_5', 'LabelEncoder_first_Type_2_6',\n        'LabelEncoder_second_Type_1_1', 'LabelEncoder_second_Type_1_2', 'LabelEncoder_second_Type_1_3',\n        'LabelEncoder_second_Type_1_4', 'LabelEncoder_second_Type_1_5', 'LabelEncoder_second_Type_1_6',\n        'LabelEncoder_second_Type_2_1', 'LabelEncoder_second_Type_2_2', 'LabelEncoder_second_Type_2_3', \n        'LabelEncoder_second_Type_2_4', 'LabelEncoder_second_Type_2_5', 'LabelEncoder_second_Type_2_6',\n        'first_Legendary_1', 'first_Legendary_2', 'first_Legendary_3',\n        'first_Legendary_4', 'first_Legendary_5', 'first_Legendary_6',\n        'second_Legendary_1', 'second_Legendary_2', 'second_Legendary_3',\n        'second_Legendary_4', 'second_Legendary_5', 'second_Legendary_6',\n        ]\ndel_col = ['first', 'second',\n           'first_pokemon_id_1', 'first_pokemon_id_2', 'first_pokemon_id_3', \n           'first_pokemon_id_4', 'first_pokemon_id_5', 'first_pokemon_id_6', \n           'second_pokemon_id_1', 'second_pokemon_id_2', 'second_pokemon_id_3', \n           'second_pokemon_id_4', 'second_pokemon_id_5', 'second_pokemon_id_6', \n           'first_Type_1_1', 'first_Type_1_2', 'first_Type_1_3', 'first_Type_1_4', 'first_Type_1_5', 'first_Type_1_6',\n           'first_Type_2_1', 'first_Type_2_2', 'first_Type_2_3', 'first_Type_2_4', 'first_Type_2_5', 'first_Type_2_6',\n           'second_Type_1_1', 'second_Type_1_2', 'second_Type_1_3', 'second_Type_1_4', 'second_Type_1_5', 'second_Type_1_6',\n           'second_Type_2_1', 'second_Type_2_2', 'second_Type_2_3', 'second_Type_2_4', 'second_Type_2_5', 'second_Type_2_6',\n           'first_types_1', 'first_types_2', 'first_types_3', 'first_types_4', 'first_types_5', 'first_types_6', \n           'second_types_1', 'second_types_2', 'second_types_3', 'second_types_4', 'second_types_5', 'second_types_6',\n           'first_effective_atk_1', 'first_effective_atk_2', 'first_effective_atk_3',\n           'first_effective_atk_4', 'first_effective_atk_5', 'first_effective_atk_6', 'first_effective_atk_team',\n           'first_not_effective_atk_1', 'first_not_effective_atk_2', 'first_not_effective_atk_3',\n           'first_not_effective_atk_4', 'first_not_effective_atk_5', 'first_not_effective_atk_6', 'first_not_effective_atk_team',\n           'second_effective_atk_1', 'second_effective_atk_2', 'second_effective_atk_3',\n           'second_effective_atk_4', 'second_effective_atk_5', 'second_effective_atk_6', 'second_effective_atk_team',\n           'second_not_effective_atk_1', 'second_not_effective_atk_2', 'second_not_effective_atk_3',\n           'second_not_effective_atk_4', 'second_not_effective_atk_5', 'second_not_effective_atk_6', 'second_not_effective_atk_team',\n           'LabelEncoder_first_types_1', 'LabelEncoder_first_types_2', 'LabelEncoder_first_types_3',\n           'LabelEncoder_first_types_4', 'LabelEncoder_first_types_5', 'LabelEncoder_first_types_6',\n           'LabelEncoder_second_types_1', 'LabelEncoder_second_types_2', 'LabelEncoder_second_types_3',\n           'LabelEncoder_second_types_4', 'LabelEncoder_second_types_5', 'LabelEncoder_second_types_6',\n           'fold', 'pred_lgb', 'target']\nfeature_col = [col for col in train.columns if col not in del_col]\nprint('We consider {} features'.format(len(feature_col)))\n\ntrain['pred_lgb'] = 0","3831e2c6":"train[cats] = train[cats].astype('int')\ntrain[cats] = train[cats].astype('category')\ntrain[cats].dtypes","e508c0e1":"test[cats] = test[cats].astype('int')\ntest[cats] = test[cats].astype('category')\ntest[cats].dtypes","31981cff":"train[cats]","568d0169":"# \u6b20\u640d\u5024\nprint(train[feature_col].isnull().sum()[train[feature_col].isnull().sum()!=0])\nprint(test[feature_col].isnull().sum()[test[feature_col].isnull().sum()!=0])","514811fc":"from sklearn.metrics import roc_auc_score\n# ====================================================\n# Train loop\n# ====================================================\ndef train_loop(folds, fold):\n    LOGGER.info(f\"========== fold: {fold+1} training ==========\")\n    print(datetime.now())\n    \n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n    \n    X_train = folds[feature_col].loc[trn_idx].reset_index(drop=True)\n    y_train = folds[CONFIG.target_col].loc[trn_idx].reset_index(drop=True).values\n    X_val = folds[feature_col].loc[val_idx].reset_index(drop=True)\n    y_val = folds[CONFIG.target_col].loc[val_idx].reset_index(drop=True).values\n    print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n    ##############################################################\n    # LGB\n    ##############################################################\n    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cats)\n    val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=cats)\n#     lgb_params = {\n#     \"objective\": \"binary\", \n#     \"metric\": \"binary_logloss\", \n#     \"boosting_type\": \"gbdt\",\n#     'learning_rate': 0.1,\n#     'verbosity': 0,\n#     'random_state': CONFIG.seed,\n#     'num_leaves': CONFIG.num_leaves,\n#     'force_col_wise':True\n#     }\n    opt_params = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"boosting_type\": \"gbdt\",\n        'learning_rate': 0.1,\n        'random_state' : 42,\n        'force_col_wise' : True,\n        \"verbosity\": -1,\n        'random_state': CONFIG.seed,\n    }\n    model_lgb = opt_lgb.train(opt_params,\n                              train_data,\n                              time_budget = 600,\n                              valid_sets = val_data,\n                              verbose_eval = False,\n                              verbosity = -1,\n                              num_boost_round = 3000,\n                              show_progress_bar = False,\n                              early_stopping_rounds = 50\n                             )\n#     model_lgb = lgb.train(params_lgbm, \n#                           train_data,\n#                           5000, \n#                           valid_sets=val_data, \n#                           verbose_eval= 1000,\n#                           early_stopping_rounds=500\n#                          )\n    best_params = model_lgb.params\n    print(\"Best params:\", best_params)\n    preds = model_lgb.predict(X_val, num_iteration=model_lgb.best_iteration)\n    train.loc[val_idx, 'pred_lgb'] = preds\n    score_lgb = roc_auc_score(y_val, preds)\n    print('Fold {} {}: {}'.format(fold+1, 'lgb', score_lgb))\n    LOGGER.info('Fold {} {}: {}'.format(fold+1, 'lgb', score_lgb))\n    # save model\n    pickle.dump(model_lgb, open(f'lgb_model_{fold+1}.pkl', 'wb'))\n        \n    return model_lgb, score_lgb","0cbfd086":"scores_folds = {}\nscores_folds['lgb'] = []\nmodels = []\nfor fold in range(CONFIG.n_fold):\n    if fold in CONFIG.trn_fold:\n        model_lgb, score_lgb = train_loop(train, fold)\n        models.append(model_lgb)\n        scores_folds['lgb'].append(score_lgb)\n\nlgb_oof_df = train[[CONFIG.target_col, 'pred_lgb']].copy()\n# CV result\nLOGGER.info(f\"========== CV ==========\")\nprint('=========== LGB ===========')\nscore = roc_auc_score(lgb_oof_df[CONFIG.target_col].values, lgb_oof_df['pred_lgb'].values)\nprint('AUC {}: {} - Folds: {}'.format('lgb', score, scores_folds['lgb']))\nLOGGER.info('AUC {}: {} - Folds: {}'.format('lgb', score, scores_folds['lgb']))\n\n# save result\nlgb_oof_df.to_csv(OUTPUT_DIR +'lgb_oof_df.csv', index=False)\n\n# feature importance\nfig, ax = visualize_importance(models, train[feature_col])","2cf77be1":"# OOF\u3067ROC Curve\u3092\u78ba\u8a8d\nfpr, tpr, thres = roc_curve(train['target'], lgb_oof_df['pred_lgb'])\nplt.figure(figsize = (5, 5)) #\u5358\u4e00\u30b0\u30e9\u30d5\u306e\u5834\u5408\u306e\u30b5\u30a4\u30ba\u6bd4\u306e\u4e0e\u3048\u65b9\nplt.plot(fpr, fpr, linestyle='dashed')\nplt.plot(fpr, tpr, marker='o')\nplt.xlabel('False Positive Rete', fontsize = 13)\nplt.ylabel('True Positive Rete', fontsize = 13)\nplt.grid()\nplt.show()","dbd0af5e":"MODEL_PATH = '.\/'\ndel test[CONFIG.target_col]","a65de074":"# ====================================================\n# inference lgb\n# ====================================================\ntest['target_lgb'] = 0\nmodels = [ MODEL_PATH + f'lgb_model_{fold+1}.pkl' for fold in CONFIG.trn_fold]\nfor path in models:\n    model = pickle.load(open(path, 'rb'))\n    print(test[feature_col].shape)\n    test['target_lgb'] += model.predict(test[feature_col])\n\ntest['target_lgb'] = test['target_lgb']\/CONFIG.n_fold","33b4037e":"# oof, pred\u306e\u4e88\u6e2c\u5206\u5e03\u306e\u78ba\u8a8d\nplt.figure(figsize=(16, 5))\nsns.distplot(lgb_oof_df['pred_lgb'], label='oof')\nsns.distplot(test['target_lgb'], label='pred')\nplt.legend()\nplt.grid()\nplt.show()","714041bc":"test[['target_lgb']].rename(columns={'target_lgb':'target'}).reset_index().to_csv(OUTPUT_DIR + 'submission_lgb.csv', index=False)\ndisplay(test[['first','second', 'target_lgb']])","02496597":"# Pokemon","80b31ffe":"# Data Loading","2f31c1af":"# Test Merge","e2a283c5":"# Preprocess","844e72d5":"# Training","a237dfa6":"# CV Split","31d06c28":"# Train Merge","c05ec536":"# Inference","f81856fc":"- 013\u304b\u3089","342440f5":"# Merge","d97f89c8":"# Add Feature 2","8d33dea8":"# type\u7279\u5fb4\u91cf\u524a\u9664","6b4f3248":"# Target Encoding","24c223e5":"# Utils","4c8b42ee":"# Type","59ea6716":"# Helper Functions","b308a146":"# Categorical Encoder","4e2595f1":"# CONFIG","4775fd91":"# Add Feature"}}