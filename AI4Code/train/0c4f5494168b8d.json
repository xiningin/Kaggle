{"cell_type":{"107bb936":"code","51ca2878":"code","4ca6dabc":"code","9f8d88b3":"code","c4e8a2de":"code","3f79de01":"code","cee9a990":"code","7f8f24d5":"code","079a2c9a":"code","f1835def":"code","d2156738":"code","d14e62fe":"markdown","d31ba16e":"markdown","64c2e217":"markdown","573f9cce":"markdown","6dd2b9d4":"markdown","e9aafb4f":"markdown","aa22cdde":"markdown","bcad3569":"markdown"},"source":{"107bb936":"# torchsummaryX is a pretty neat library that allows one to get a summary of the PyTorch modelin a similar way to\n# the summary provided by Keras\n!pip install torchsummaryX","51ca2878":"import torch\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\nfrom torchsummaryX import summary\n\nfrom pylab import rcParams\n\nrcParams['figure.figsize'] = 10, 8","4ca6dabc":"x = np.arange(100)\ny = x + np.random.randn(len(x)) * 10","9f8d88b3":"plt.plot(x, y, 'o')","c4e8a2de":"model = torch.nn.Sequential(\n    torch.nn.Linear(1, 1),\n)\n\n# Make sure the data is represented as tensors\nxt = torch.tensor(x.reshape(-1, 1), dtype=torch.float)\nyt = torch.tensor(y.reshape(-1, 1), dtype=torch.float)","3f79de01":"summary(model, xt)","cee9a990":"[x for x in model[0].parameters()]","7f8f24d5":"# Let's run this for a few epochs\nfor i in range(20000):\n    # Run the data through our model\n    y_pred = model(xt)\n    \n    # Using MSE as the loss function, no big deal here\n    loss = (y_pred - yt).pow(2.0).sum()\n    \n    # According to PyTorch's documentation, the gradient values are accumulated in the leaves of the\n    # graph that represents the model. This means that between each interaction, before running the backward\n    # pass, I need to zero the gradients\n    model.zero_grad()\n    \n    # This does the backward pass, which basically computes the gradients for all parameters that can be learned\n    # in our model (the ones that I mentioned above as flagged with requires_grad=True) \n    loss.backward()\n    \n    if i % 1000 == 0:\n        print(i, loss.item())\n    \n    # Can improve here by using PyTorch's optimizer, instead of updating the parameters myself(??)\n    with torch.no_grad():\n        for p in model.parameters():\n            p -= 0.0000001 * p.grad    # update the parameters using a learning rate to scale down the changes","079a2c9a":"y_pred = model(xt)","f1835def":"plt.plot(xt.detach().numpy(), y_pred.detach().numpy(), label='Regression')\nplt.plot(x, y, 'or', label='Data Points')\nplt.plot(x, x, 'k', label='Original Line')\nplt.legend()\nplt.show()","d2156738":"[x for x in model.parameters()]","d14e62fe":"Let's take a look at the model:","d31ba16e":"Now, when I check the parameters of the model, I can see two of them: the $1 \\times 1$ matrix $A$ and the bias vector $b$ (with only one element). Another important thing to notice is how PyTorch flagged both parameters with *requires_grad=True*. According to PyTorch's documentation, \"Internally, the parameters of each Module are stored in Tensors with requires_grad=True\". This means that all parameters that can be learned will have a gradient computed during the backward pass.","64c2e217":"Next step is to create some artificial data.","573f9cce":"## Linear Regression with PyTorch","6dd2b9d4":"This is a notebook I created as a personal note for myself while trying to learn PyTorch. Although perhaps one of the main goals of PyTorch is to provide a neural networks\/deep learning framework, it is possible (although not necessarily recommended) to do some simpler tasks with it (which can be, at least for me, quite instructive).\n\nIn this notebook, I am trying to use a sequential model and a linear layer to create a linear regression model. There are probably other better notebooks around doing the same thing, but doing this myself was a good first exercise. I decided to share in case it is useful to others. Also, if you have any comments, I'd like to hear them.","e9aafb4f":"Now, I can compute the prediction and plot things to see how well the model learned","aa22cdde":"Now for the model. According to the PyTorch documentation, a linear layer applies a linear transformation to the incoming data according to the formula:\n\n\\begin{equation}\ny=xA^T+b\n\\end{equation}\n\nThis is a very simple linear transformation, I just have one input dimension and one output dimension. In other words, the matrix $A$ is actually just a scalar. As such, I create the linear layer with dimensions $1 \\times 1$.","bcad3569":"And finally, how the parameters look like after optimization?"}}