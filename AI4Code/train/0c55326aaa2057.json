{"cell_type":{"99b9a40d":"code","2784bca7":"code","e1552d0e":"code","7c67adb1":"code","09749af2":"code","177d3477":"code","79849c1b":"code","eb0d9812":"code","e340a3d8":"code","984d7043":"code","aa53406c":"code","84ed9e98":"code","946180eb":"code","ff852ee8":"code","209e9a91":"code","1938b75c":"code","2ba0d322":"code","bf220265":"code","f110f3bf":"code","125bdc7f":"code","7ab1b88f":"code","775b8b06":"code","a999bf9f":"code","335bf111":"code","3be1c5b6":"code","d454c80d":"code","fc6f70ba":"code","c35320e9":"code","c0ffa731":"code","b7ed7d28":"code","9d126dbf":"code","c2f8910c":"code","f9625cdb":"code","028e1829":"code","cbb947a1":"code","21bec38f":"code","34bf3ffd":"code","e6b5d1a7":"code","4867421c":"code","8ec52d8c":"code","81c69889":"code","bbc035c7":"code","734e54b5":"markdown","258ca9f9":"markdown","3ae945d3":"markdown","42717ccf":"markdown","34c8eb5d":"markdown","cc7a3cba":"markdown","f53429e0":"markdown","691de82f":"markdown"},"source":{"99b9a40d":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\nfrom wordcloud import WordCloud,STOPWORDS\nimport nltk\nfrom nltk.stem import PorterStemmer,WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nimport gensim\nfrom gensim.utils import simple_preprocess\nfrom gensim.parsing.preprocessing import STOPWORDS\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import one_hot, Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten,Embedding,Input,LSTM,Conv1D,MaxPool1D,Bidirectional,Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.utils import to_categorical","2784bca7":"#importing dataset\ndf = pd.read_csv(\"\/kaggle\/input\/stock-sentiment\/stock_sentiment.csv\")","e1552d0e":"df.head(8)","7c67adb1":"#checking datatypes\ndf.info()","09749af2":"#checking for null values\ndf.isnull().sum()","177d3477":"#countplot showing type of sentiment\nsns.countplot(df.Sentiment)\nplt.show()","79849c1b":"#counting total null values in Sentiment column of data\ndf.Sentiment.nunique()","eb0d9812":"import string\nstring.punctuation","e340a3d8":"# function to clean punctuations from data\ndef remove_punc(message):\n    punc_removed = [word    for word in message if word not in string.punctuation]\n    punc_join = ''.join(punc_removed)\n    return punc_join","984d7043":"# applying above function to the text\ndf[\"Cleaned text\"] = df.Text.apply(remove_punc)","aa53406c":"df.head()\n","84ed9e98":"nltk.download('stopwords')\nstopwords.words('english')","946180eb":"# cleaning data which contains stop words\nstop_words = stopwords.words(\"english\")\nstop_words.extend([\"from\",\"subject\",\"re\",\"edu\",\"use\",\"will\",\"day\",\"user\",\"today\",\"stock\",\"week\",\"year\",\"aap\",\"https\",\"com\"])","ff852ee8":"#function to process text data\ndef preprocess(text):\n    result = []\n    for token in gensim.utils.simple_preprocess(text):\n        if token not in stop_words and len(token)>=3:\n            result.append(token)\n    \n    return result","209e9a91":"df[\"Cleaned text\"] = df[\"Cleaned text\"].apply(preprocess)","1938b75c":"df[\"Cleaned text joined\"] = df[\"Cleaned text\"].apply(lambda x: \" \".join(x))","2ba0d322":"df.head()","bf220265":"#positive sentiment wordcloud\nplt.figure(figsize=(20,20))\nwc = WordCloud(max_words=1000, width=1600, height=800).generate(\" \".join(df[df[\"Sentiment\"]==1][\"Cleaned text joined\"]))\nplt.imshow(wc)\nplt.show()","f110f3bf":"# negative sentiment wordcloud\nplt.figure(figsize=(20,20))\nwc2  = WordCloud(max_words=1000,width=1600, height=800).generate(\" \".join(df[df[\"Sentiment\"]==0][\"Cleaned text joined\"]))\nplt.imshow(wc2)\nplt.show()","125bdc7f":"nltk.download('punkt')","7ab1b88f":"maxlen= -1\n\nfor doc in df[\"Cleaned text joined\"]:\n    tokens = nltk.word_tokenize(doc)\n    if maxlen< len(tokens):\n        maxlen = len(tokens)\n        \nprint(\"Max number of words in any Doc:\",maxlen)","775b8b06":"tweets_len = [len(nltk.word_tokenize(x))       for x in df[\"Cleaned text joined\"]]\ntweets_len[:7]","a999bf9f":"plt.figure(figsize=(12,8))\nplt.hist(x=tweets_len, bins=50)\nplt.show()","335bf111":"sns.countplot(df[\"Sentiment\"], data=df)\nplt.show()","3be1c5b6":"list_of_words = []\n\nfor i in df[\"Cleaned text\"]:\n    for j in i:\n        list_of_words.append(j)","d454c80d":"total_words = len(list(set(list_of_words)))\ntotal_words","fc6f70ba":"x = df[\"Cleaned text\"]\ny = df[\"Sentiment\"]\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x,y, test_size=0.1, random_state=24)","c35320e9":"print(X_train.shape)\nprint(X_test.shape)","c0ffa731":"#tokenizing text data\ntokenizer = Tokenizer(num_words = total_words)\ntokenizer.fit_on_texts(X_train)\n\n#training sequence\ntrain_sequences = tokenizer.texts_to_sequences(X_train)\n\n#testing sequences\ntest_sequences = tokenizer.texts_to_sequences(X_test)","b7ed7d28":"# Adding padding to training and testing\npadded_train = pad_sequences(train_sequences, maxlen=29)\npadded_test = pad_sequences(test_sequences, maxlen=29)\n","9d126dbf":"for i, doc in enumerate(padded_train[:3]):\n    print(\"Padded encoding for the document:\",i+1,\"is \",doc)","c2f8910c":"y_train_cat = to_categorical(y_train, 2)\ny_test_cat = to_categorical(y_test, 2)","f9625cdb":"print(y_train_cat.shape)\nprint(y_test_cat.shape)","028e1829":"model = Sequential()\nmodel.add(Embedding(total_words, output_dim= 512))\n\nmodel.add(LSTM(256))\n\nmodel.add(Dense(128, activation='relu'))\n\nmodel.add(Dropout(0.3))\nmodel.add(Dense(2,activation=\"softmax\"))\n\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics = ['acc'])\nmodel.summary()","cbb947a1":"model.fit(padded_train, y_train_cat, batch_size=32, validation_split=0.2, epochs=3)","21bec38f":"y_pred = model.predict(padded_test)","34bf3ffd":"y_pred","e6b5d1a7":"prediction = []\n\nfor i in y_pred:\n    prediction.append(np.argmax(i))","4867421c":"original = []\n\nfor i in y_test_cat:\n    original.append(np.argmax(i))","8ec52d8c":"## Accuracy","81c69889":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nreport = classification_report(original,prediction)\nprint(report)\n\nprint(\"Accuracy of LSTM model: 75%\")","bbc035c7":"plt.figure(figsize=(14,8))\ncm = confusion_matrix(original,prediction)\nsns.heatmap(data = cm, annot=True)\nplt.show()","734e54b5":"## Data Visualization","258ca9f9":"## Accuracy","3ae945d3":"# Data Cleaning","42717ccf":"## Data Modelling","34c8eb5d":"* Author: Purvit Vashishtha\n* Working Environment: Kaggle Notebook\n* Submitted on: 20 Feb 5:05:45 pm","cc7a3cba":"## LSTM Model","f53429e0":"# Sentiment Analysis on Stocks Data","691de82f":"## Data Exploration"}}