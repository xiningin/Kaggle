{"cell_type":{"f4d22150":"code","d7b9cb0b":"code","4007ff88":"code","c14e91d9":"code","3518e276":"code","b1a83464":"code","59bdfa6d":"code","6bfefade":"code","2ce3e8f4":"code","9c7d2a10":"code","edd31211":"code","76a526f6":"code","cde1433e":"code","887a9b58":"code","920e01cf":"code","001d5924":"code","f716691b":"code","31c5fd9e":"code","f079409a":"code","eac376c6":"code","748fabd8":"code","09647c83":"code","6fb68277":"code","17c95336":"code","ce73ce3c":"code","8ab992e8":"code","28850eec":"code","4ee978a4":"code","f1750761":"code","488c7856":"code","319aa8c0":"code","091f6a3b":"code","a7ab9be0":"code","13cbaf83":"code","9d088269":"code","04fe7095":"code","5ba12558":"code","204eb1fe":"code","7fb3c4cb":"code","f9ff6619":"code","80b2e028":"code","b92c165d":"code","ae0e5655":"code","6f214a04":"code","4cfc52aa":"code","19d169e2":"code","93b070e8":"code","e37ad523":"code","a1291e34":"code","f010078b":"code","e1d8ca81":"code","18767ae8":"code","0ef0c800":"code","3e64c37a":"code","0d96254f":"code","b9c19ca3":"code","988e1cd5":"code","6f051977":"code","4dba2b6b":"code","b9fb4dc3":"code","ec4253be":"markdown","3d4943ae":"markdown","e654d6ea":"markdown","cfb1333a":"markdown","3e639cc3":"markdown","15cf6356":"markdown","e45a9d37":"markdown","3806b113":"markdown","99424c96":"markdown","6e2ba973":"markdown","ece4b8d5":"markdown","19a8aa95":"markdown","fc4768a5":"markdown","c33cc2e3":"markdown","43fa0a2d":"markdown","b736f641":"markdown","25e0723b":"markdown","87a5defe":"markdown","b1fd6a7a":"markdown","4030dad7":"markdown","42e799ad":"markdown","e01950a3":"markdown","6eee299b":"markdown","48c6a177":"markdown","92fa7d50":"markdown","f3e57ef6":"markdown","f72859b9":"markdown","450b7195":"markdown","f0f81055":"markdown","cee93970":"markdown","b11b1b2a":"markdown","f66d0f72":"markdown","de20a6f1":"markdown","7ca0b791":"markdown","174b60d7":"markdown","474c1320":"markdown","e30fe454":"markdown","d88946e9":"markdown","2cd56bf8":"markdown","62175a89":"markdown","88cb69bb":"markdown","859aa1b7":"markdown","49ee4e1f":"markdown"},"source":{"f4d22150":"# Basic libraries\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.simplefilter('ignore')\n\n# Directry check\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d7b9cb0b":"# Statistics library\nfrom scipy.stats import norm\nfrom scipy import stats\nimport scipy\n\n# Data preprocessing\nimport datetime\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Visualization\nfrom matplotlib import pyplot as plt\nplt.style.use('fivethirtyeight')\nimport seaborn as sns\n\n# Machine learning\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\n\n# Validataion\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score","4007ff88":"# data description\nf = open(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/data_description.txt\")\ndata = f.read()\nf.close()\n\nprint(data)","c14e91d9":"# data loading\nsample = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ndf_train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","3518e276":"# Sample data check\nsample.head()","b1a83464":"# train data\ndf_train.head()","59bdfa6d":"# test data\ndf_test.head()","6bfefade":"# data size\nprint(\"train data size:{}\".format(df_train.shape))\nprint(\"test data size:{}\".format(df_test.shape))","2ce3e8f4":"# data info\nprint(\"*\"*50)\nprint(\"train data information\")\nprint(df_train.info())","9c7d2a10":"# summery\ndf_train.describe()","edd31211":"# Distribution\nfig, ax = plt.subplots(1,2,figsize=(20,6))\nsns.distplot(df_train[\"SalePrice\"], fit=norm, ax=ax[0])\nax[0].set_xlabel(\"SalePrice\")\nax[0].set_ylabel(\"count(normalized)\")\nax[0].set_title(\"SalePrice distribution\")\n\n# Probability  plot\nstats.probplot(df_train['SalePrice'], plot=ax[1])","76a526f6":"# Distribution\nlog_SelePrice = np.log10(df_train[\"SalePrice\"])\nfig, ax = plt.subplots(1,2,figsize=(20,6))\nsns.distplot(log_SelePrice, fit=norm, ax=ax[0])\nax[0].set_xlabel(\"SalePrice\")\nax[0].set_ylabel(\"count(normalized)\")\nax[0].set_title(\"log10(SalePrice distribution)\")\n\n# Probability  plot\nstats.probplot(log_SelePrice, plot=ax[1])","cde1433e":"# dtype object\ndtype = pd.DataFrame({\"columns\":df_train.dtypes.index,\n                     \"dtype\":df_train.dtypes})\ndtype[\"dtype\"] = [str(i) for i in dtype[\"dtype\"]]\ndtype","887a9b58":"# columns\nnum_columns = dtype.query('dtype==\"int64\" | dtype==\"float64\"')[\"columns\"].values[1:-1]\nobj_columns = dtype.query('dtype==\"object\"')[\"columns\"].values\n\nprint(\"numerical_values_count:{}\".format(len(num_columns)))\nprint(\"object_values_count:{}\".format(len(obj_columns)))","920e01cf":"# correlation\nmatrix = df_train[num_columns].fillna(0) # tempolary fill na =0\n\nplt.figure(figsize=(16,16))\nhm = sns.heatmap(matrix.corr(), vmax=1, vmin=-1, cmap=\"bwr\", square=True)","001d5924":"# skewness\nskew = []\nfor i in range(df_train[num_columns].shape[1]):\n    median = df_train[num_columns].iloc[:,i].median()\n    s = scipy.stats.skew(df_train[num_columns].iloc[:,i].fillna(median))\n    skew.append(s)\n\n# kurtosis\nkurt = []\nfor i in range(df_train[num_columns].shape[1]):\n    median = df_train[num_columns].iloc[:,i].median()\n    s = scipy.stats.kurtosis(df_train[num_columns].iloc[:,i].fillna(median))\n    kurt.append(s)\n\n\nser_kur = pd.DataFrame(skew, index=df_train[num_columns].columns, columns=[\"skew\"])\nser_kur[\"kurt\"] = kurt","f716691b":"# skewness\nplt.figure(figsize=(20,6))\nplt.bar(ser_kur.index, ser_kur[\"skew\"], color=\"skyblue\")\nplt.xlabel(\"variables\")\nplt.xticks(rotation=90)\nplt.ylabel(\"skewness\")","31c5fd9e":"# kurtsis\nplt.figure(figsize=(20,6))\nplt.bar(ser_kur.index, ser_kur[\"kurt\"], color=\"pink\")\nplt.xlabel(\"variables\")\nplt.xticks(rotation=90)\nplt.ylabel(\"kurtosis\")","f079409a":"fig, ax = plt.subplots(9,4, figsize=(25,50))\nfor i in range(0,36):\n    if i < 4:\n        sns.distplot(df_train[num_columns].iloc[:,i], ax=ax[0,i], kde=False)\n    elif i >= 4 and i < 8:\n        sns.distplot(df_train[num_columns].iloc[:,i], ax=ax[1,i-4], kde=False)\n    elif i >= 8 and i < 12:\n        sns.distplot(df_train[num_columns].iloc[:,i], ax=ax[2,i-8], kde=False)\n    elif i >= 12 and i < 16:\n        sns.distplot(df_train[num_columns].iloc[:,i], ax=ax[3,i-12], kde=False)\n    elif i >= 16 and i < 20:\n        sns.distplot(df_train[num_columns].iloc[:,i], ax=ax[4,i-16], kde=False)\n    elif i >= 20 and i < 24:\n        sns.distplot(df_train[num_columns].iloc[:,i], ax=ax[5,i-20], kde=False)\n    elif i >= 24 and i < 28:\n        sns.distplot(df_train[num_columns].iloc[:,i], ax=ax[6,i-24], kde=False)\n    elif i >= 28 and i < 32:\n        sns.distplot(df_train[num_columns].iloc[:,i], ax=ax[7,i-28], kde=False)\n    else:\n        sns.distplot(df_train[num_columns].iloc[:,i], ax=ax[8,i-32], kde=False)","eac376c6":"df_train[obj_columns].head() ","748fabd8":"fig, ax = plt.subplots(9,5, figsize=(25,50))\nfor i in range(0,43):\n    if i < 5:\n        sns.countplot(df_train[obj_columns].iloc[:,i], ax=ax[0,i])\n    elif i >= 5 and i < 10:\n        sns.countplot(df_train[obj_columns].iloc[:,i], ax=ax[1,i-5])\n    elif i >= 10 and i < 15:\n        sns.countplot(df_train[obj_columns].iloc[:,i], ax=ax[2,i-10])\n    elif i >= 15 and i < 20:\n        sns.countplot(df_train[obj_columns].iloc[:,i], ax=ax[3,i-15])\n    elif i >= 20 and i < 25:\n        sns.countplot(df_train[obj_columns].iloc[:,i], ax=ax[4,i-20])\n    elif i >= 25 and i <30:\n        sns.countplot(df_train[obj_columns].iloc[:,i], ax=ax[5,i-25])\n    elif i >= 30 and i <35:\n        sns.countplot(df_train[obj_columns].iloc[:,i], ax=ax[6,i-30])\n    elif i >= 35 and i <40:\n        sns.countplot(df_train[obj_columns].iloc[:,i], ax=ax[7,i-35])\n    else:\n        sns.countplot(df_train[obj_columns].iloc[:,i], ax=ax[8,i-40])","09647c83":"fig, ax = plt.subplots(9,5, figsize=(25,50))\nfor i in range(0,43):\n    if i < 5:\n        sns.boxplot(df_train[obj_columns].iloc[:,i], df_train[\"SalePrice\"], ax=ax[0,i])\n    elif i >= 5 and i < 10:\n        sns.boxplot(df_train[obj_columns].iloc[:,i], df_train[\"SalePrice\"], ax=ax[1,i-5])\n    elif i >= 10 and i < 15:\n        sns.boxplot(df_train[obj_columns].iloc[:,i], df_train[\"SalePrice\"], ax=ax[2,i-10])\n    elif i >= 15 and i < 20:\n        sns.boxplot(df_train[obj_columns].iloc[:,i], df_train[\"SalePrice\"], ax=ax[3,i-15])\n    elif i >= 20 and i < 25:\n        sns.boxplot(df_train[obj_columns].iloc[:,i], df_train[\"SalePrice\"], ax=ax[4,i-20])\n    elif i >= 25 and i <30:\n        sns.boxplot(df_train[obj_columns].iloc[:,i], df_train[\"SalePrice\"], ax=ax[5,i-25])\n    elif i >= 30 and i <35:\n        sns.boxplot(df_train[obj_columns].iloc[:,i], df_train[\"SalePrice\"], ax=ax[6,i-30])\n    elif i >= 35 and i <40:\n        sns.boxplot(df_train[obj_columns].iloc[:,i], df_train[\"SalePrice\"], ax=ax[7,i-35])\n    else:\n        sns.boxplot(df_train[obj_columns].iloc[:,i], df_train[\"SalePrice\"], ax=ax[8,i-40])","6fb68277":"null_df = pd.DataFrame({\"colname\":df_train.isnull().sum().index,\n                       \"null_count\":df_train.isnull().sum()})\nnull_df[\"null_ratio\"] = null_df[\"null_count\"]\/df_train.shape[0]\nnull_df.sort_index(by=\"null_ratio\", ascending=False).head(20)","17c95336":"# Copy the dataframe\ntrain_df = df_train.copy()","ce73ce3c":"# drop columns\ndrop_columns = [\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\"]\ntrain_df.drop(drop_columns, axis=1, inplace=True)\n\n# test data\ndf_test.drop(drop_columns, axis=1, inplace=True)","8ab992e8":"# dtype object\ndtype = pd.DataFrame({\"columns\":train_df.dtypes.index,\n                     \"dtype\":train_df.dtypes})\ndtype[\"dtype\"] = [str(i) for i in dtype[\"dtype\"]]\n\n# columns\nnum_columns = dtype.query('dtype==\"int64\" | dtype==\"float64\"')[\"columns\"].values[1:-1]\nobj_columns = dtype.query('dtype==\"object\"')[\"columns\"].values\n\nprint(\"numerical_values_count:{}\".format(len(num_columns)))\nprint(\"object_values_count:{}\".format(len(obj_columns)))","28850eec":"# skewness > 1 columns\nposskew_col = ser_kur.query(\"skew>1\").index\n\n# quantile 99% value dataframe\nquant_99 = pd.DataFrame({})\n\n# Create list\ncolnames = []\nquant = []\n# Roop\nfor i in range(train_df[posskew_col].shape[1]):\n    c = train_df[posskew_col].columns[i]\n    q = train_df[posskew_col].iloc[:,i].quantile(0.999)\n    colnames.append(c)\n    quant.append(q)\n    \nquant_99[\"column\"]=colnames\nquant_99[\"quantile_99\"]=quant\n\nquant_99","4ee978a4":"# drop outliers\nfor i in range(0, quant_99.shape[0]):\n    train_df = train_df[ train_df[ quant_99[\"column\"][i]] < quant_99[\"quantile_99\"][i] ]\nprint(\"train_df shape:{}\".format(train_df.shape))","f1750761":"fig, ax = plt.subplots(9,4, figsize=(25,50))\nfor i in range(0,36):\n    if i < 4:\n        sns.distplot(train_df[num_columns].iloc[:,i], ax=ax[0,i], kde=False)\n    elif i >= 4 and i < 8:\n        sns.distplot(train_df[num_columns].iloc[:,i], ax=ax[1,i-4], kde=False)\n    elif i >= 8 and i < 12:\n        sns.distplot(train_df[num_columns].iloc[:,i], ax=ax[2,i-8], kde=False)\n    elif i >= 12 and i < 16:\n        sns.distplot(train_df[num_columns].iloc[:,i], ax=ax[3,i-12], kde=False)\n    elif i >= 16 and i < 20:\n        sns.distplot(train_df[num_columns].iloc[:,i], ax=ax[4,i-16], kde=False)\n    elif i >= 20 and i < 24:\n        sns.distplot(train_df[num_columns].iloc[:,i], ax=ax[5,i-20], kde=False)\n    elif i >= 24 and i < 28:\n        sns.distplot(train_df[num_columns].iloc[:,i], ax=ax[6,i-24], kde=False)\n    elif i >= 28 and i < 32:\n        sns.distplot(train_df[num_columns].iloc[:,i], ax=ax[7,i-28], kde=False)\n    else:\n        sns.distplot(train_df[num_columns].iloc[:,i], ax=ax[8,i-32], kde=False)","488c7856":"train_df.drop([\"BsmtFinSF2\", \"LowQualFinSF\", \"BsmtHalfBath\", \"KitchenAbvGr\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"MiscVal\"], axis=1, inplace=True)\n\n# test data\ndf_test.drop([\"BsmtFinSF2\", \"LowQualFinSF\", \"BsmtHalfBath\", \"KitchenAbvGr\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"MiscVal\"], axis=1, inplace=True)","319aa8c0":"train_df.drop([\"1stFlrSF\", \"GarageCars\"], axis=1, inplace=True)\n\n# test data\ndf_test.drop([\"1stFlrSF\", \"GarageCars\"], axis=1, inplace=True)","091f6a3b":"train_df[\"LotArea\"] = np.log10(train_df[\"LotArea\"]+1)\ntrain_df[\"TotalBsmtSF\"] = np.log10(train_df[\"TotalBsmtSF\"]+1)\ntrain_df[\"GrLivArea\"] = np.log10(train_df[\"GrLivArea\"]+1)\n\n# test data\ndf_test[\"LotArea\"] = np.log10(df_test[\"LotArea\"]+1)\ndf_test[\"TotalBsmtSF\"] = np.log10(df_test[\"TotalBsmtSF\"]+1)\ndf_test[\"GrLivArea\"] = np.log10(df_test[\"GrLivArea\"]+1)","a7ab9be0":"# dtype object\ndtype = pd.DataFrame({\"columns\":train_df.dtypes.index,\n                     \"dtype\":train_df.dtypes})\ndtype[\"dtype\"] = [str(i) for i in dtype[\"dtype\"]]\n\n# columns\nnum_columns = dtype.query('dtype==\"int64\" | dtype==\"float64\"')[\"columns\"].values[1:-1]\nobj_columns = dtype.query('dtype==\"object\"')[\"columns\"].values\n\nprint(\"numerical_values_count:{}\".format(len(num_columns)))\nprint(\"object_values_count:{}\".format(len(obj_columns)))","13cbaf83":"pd.DataFrame(obj_columns)","9d088269":"# Drop the decided parameters.\ndrop_col = [\"Street\", \"LandContour\", \"Utilities\", \"LandSlope\", \"Condition1\", \"Condition2\", \"RoofMatl\", \"BsmtFinType2\", \"Heating\", \n            \"CentralAir\", \"Electrical\", \"Functional\",\"GarageQual\", \"GarageCond\", \"PavedDrive\"]\ntrain_df.drop(drop_col, axis=1, inplace=True)\n\n# test data\ndf_test.drop(drop_col, axis=1, inplace=True)\n\n# checking the df\ntrain_df.head()","04fe7095":"# dtype object\ndtype = pd.DataFrame({\"columns\":train_df.dtypes.index,\n                     \"dtype\":train_df.dtypes})\ndtype[\"dtype\"] = [str(i) for i in dtype[\"dtype\"]]\n\n# columns\nnum_columns = dtype.query('dtype==\"int64\" | dtype==\"float64\"')[\"columns\"].values[1:-1]\nobj_columns = dtype.query('dtype==\"object\"')[\"columns\"].values\n\nprint(\"numerical_values_count:{}\".format(len(num_columns)))\nprint(\"object_values_count:{}\".format(len(obj_columns)))","5ba12558":"# MSZoning\nmapping = {\"RL\":1, \"RM\":2, \"FV\":3, \"RH\":4, \"C(all)\":5}\ntrain_df[obj_columns[0]] = train_df[obj_columns[0]].map(mapping)\ndf_test[obj_columns[0]] = df_test[obj_columns[0]].map(mapping)\n\n# LotShape\nmapping = {\"Reg\":1, \"IR1\":2, \"IR2\":3, \"IR3\":4}\ntrain_df[obj_columns[1]] = train_df[obj_columns[1]].map(mapping)\ndf_test[obj_columns[1]] = df_test[obj_columns[1]].map(mapping)\n\n# LotConfig\nmapping = {\"Inside\":1, \"Corner\":2, \"CulDSac\":3, \"FR2\":4, \"FR3\":5}\ntrain_df[obj_columns[2]] = train_df[obj_columns[2]].map(mapping)\ndf_test[obj_columns[2]] = df_test[obj_columns[2]].map(mapping)\n\n# Neighborhood\nmapping = {'NAmes':1, 'CollgCr':2, 'OldTown':3, 'Edwards':4, 'Somerst':5, 'Gilbert':6, 'NridgHt':7, 'Sawyer':8, 'NWAmes':9, 'SawyerW':10, 'BrkSide':11, 'Crawfor':12,\n       'Mitchel':13, 'NoRidge':14, 'Timber':15, 'IDOTRR':16, 'ClearCr':17, 'StoneBr':18, 'SWISU':19, 'MeadowV':20, 'Blmngtn':21, 'BrDale':22, 'Veenker':23, 'NPkVill':24, 'Blueste':25}\ntrain_df[obj_columns[3]] = train_df[obj_columns[3]].map(mapping)\ndf_test[obj_columns[3]] = df_test[obj_columns[3]].map(mapping)\n        \n# BldgType\nmapping = {'1Fam':1, 'TwnhsE':2, 'Duplex':3, 'Twnhs':4, '2fmCon':5}\ntrain_df[obj_columns[4]] = train_df[obj_columns[4]].map(mapping)\ndf_test[obj_columns[4]] = df_test[obj_columns[4]].map(mapping)\n\n# HouseStyle\nmapping = {'1Story':1, '2Story':2, '1.5Fin':3, 'SLvl':4, 'SFoyer':5, '1.5Unf':6, '2.5Unf':7, '2.5Fin':8}\ntrain_df[obj_columns[5]] = train_df[obj_columns[5]].map(mapping)\ndf_test[obj_columns[5]] = df_test[obj_columns[5]].map(mapping)\n        \n# RoofStyle\nmapping = {'Gable':1, 'Hip':2, 'Flat':3, 'Gambrel':4, 'Mansard':5, 'Shed':6}\ntrain_df[obj_columns[6]] = train_df[obj_columns[6]].map(mapping)\ndf_test[obj_columns[6]] = df_test[obj_columns[6]].map(mapping)\n        \n# Exterior1st\nmapping = {'VinylSd':1, 'HdBoard':2, 'MetalSd':3, 'Wd Sdng':4, 'Plywood':5, 'CemntBd':6, 'BrkFace':7, 'WdShing':8, 'Stucco':9, 'AsbShng':10, 'BrkComm':11, 'Stone':12,\n           'ImStucc':13, 'AsphShn':14, 'CBlock':15}\ntrain_df[obj_columns[7]] = train_df[obj_columns[7]].map(mapping)\ndf_test[obj_columns[7]] = df_test[obj_columns[7]].map(mapping) \n        \n# Exterior2nd\nmapping = {'VinylSd':1, 'MetalSd':2, 'HdBoard':3, 'Wd Sdng':4, 'Plywood':5, 'CmentBd':6, 'Wd Shng':7, 'Stucco':8, 'BrkFace':9, 'AsbShng':10, 'ImStucc':11, 'Brk Cmn':12,\n           'Stone':13, 'AsphShn':14, 'Other':15, 'CBlock':16} \ntrain_df[obj_columns[8]] = train_df[obj_columns[8]].map(mapping)\ndf_test[obj_columns[8]] = df_test[obj_columns[8]].map(mapping) \n        \n# MasVnrType\nmapping = {'None':1, 'BrkFace':2, 'Stone':3, 'BrkCmn':4}\ntrain_df[obj_columns[9]] = train_df[obj_columns[9]].map(mapping)\ndf_test[obj_columns[9]] = df_test[obj_columns[9]].map(mapping)         ","204eb1fe":"# ExterQual\nmapping = {'TA':1, 'Gd':2, 'Ex':3, 'Fa':4}\ntrain_df[obj_columns[10]] = train_df[obj_columns[10]].map(mapping)\ndf_test[obj_columns[10]] = df_test[obj_columns[10]].map(mapping)\n\n# ExterCond\nmapping = {'TA':1, 'Gd':2, 'Fa':3, 'Ex':4, 'Po':5}\ntrain_df[obj_columns[11]] = train_df[obj_columns[11]].map(mapping)\ndf_test[obj_columns[11]] = df_test[obj_columns[11]].map(mapping)\n\n# Foundation\nmapping = {'PConc':1, 'CBlock':2, 'BrkTil':3, 'Slab':4, 'Stone':5, 'Wood':6}\ntrain_df[obj_columns[12]] = train_df[obj_columns[12]].map(mapping)\ndf_test[obj_columns[12]] = df_test[obj_columns[12]].map(mapping)\n\n# BsmtQual\nmapping = {'TA':1, 'Gd':2, 'Ex':3, 'Fa':4}\ntrain_df[obj_columns[13]] = train_df[obj_columns[13]].map(mapping)\ndf_test[obj_columns[13]] = df_test[obj_columns[13]].map(mapping)\n\n# BsmtCond\nmapping = {'TA':1, 'Gd':2, 'Fa':3, 'Po':4}\ntrain_df[obj_columns[14]] = train_df[obj_columns[14]].map(mapping)\ndf_test[obj_columns[14]] = df_test[obj_columns[14]].map(mapping)\n\n# BsmtExposure\nmapping = {'No':1, 'Av':2, 'Gd':3, 'Mn':4}\ntrain_df[obj_columns[15]] = train_df[obj_columns[15]].map(mapping)\ndf_test[obj_columns[15]] = df_test[obj_columns[15]].map(mapping)\n\n# BsmtFinType1\nmapping = {'Unf':1, 'GLQ':2, 'ALQ':3, 'BLQ':4, 'Rec':5, 'LwQ':6}\ntrain_df[obj_columns[16]] = train_df[obj_columns[16]].map(mapping)\ndf_test[obj_columns[16]] = df_test[obj_columns[16]].map(mapping)\n\n# HeatingQC\nmapping = {'Ex':1, 'TA':2, 'Gd':3, 'Fa':4, 'Po':5}\ntrain_df[obj_columns[17]] = train_df[obj_columns[17]].map(mapping)\ndf_test[obj_columns[17]] = df_test[obj_columns[17]].map(mapping)\n\n# KitchenQual\nmapping = {'TA':1, 'Gd':2, 'Ex':3, 'Fa':4}\ntrain_df[obj_columns[18]] = train_df[obj_columns[18]].map(mapping)\ndf_test[obj_columns[18]] = df_test[obj_columns[18]].map(mapping)\n\n# GarageType\nmapping = {'Attchd':1, 'Detchd':2, 'BuiltIn':3, 'Basment':4, 'CarPort':5, '2Types':6}\ntrain_df[obj_columns[19]] = train_df[obj_columns[19]].map(mapping)\ndf_test[obj_columns[19]] = df_test[obj_columns[19]].map(mapping)","7fb3c4cb":"# GarageFinish\nmapping = {'Unf':1, 'RFn':2, 'Fin':3}\ntrain_df[obj_columns[20]] = train_df[obj_columns[20]].map(mapping)\ndf_test[obj_columns[20]] = df_test[obj_columns[20]].map(mapping)\n\n# SaleType\nmapping = {'WD':1, 'New':2, 'COD':3, 'ConLD':4, 'ConLw':5, 'ConLI':6, 'CWD':7, 'Oth':8, 'Con':9}\ntrain_df[obj_columns[21]] = train_df[obj_columns[21]].map(mapping)\ndf_test[obj_columns[21]] = df_test[obj_columns[21]].map(mapping)\n\n# SaleCondition\nmapping = {'Normal':1, 'Partial':2, 'Abnorml':3, 'Family':4, 'Alloca':5, 'AdjLand':6}\ntrain_df[obj_columns[22]] = train_df[obj_columns[22]].map(mapping)\ndf_test[obj_columns[22]] = df_test[obj_columns[22]].map(mapping)","f9ff6619":"# fill columns name\nfill_null_columns = [\"MSZoning\", \"BsmtQual\", \"BsmtCond\", \"BsmtQual\", \"BsmtExposure\", \"BsmtFinType1\", \"GarageType\", \"GarageYrBlt\", \"GarageFinish\", \"LotFrontage\",\n                     \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtFullBath\", \"KitchenQual\", \n                     \"GarageArea\", \"SaleType\"]\n\n# fill by median\nfor i in range(len(fill_null_columns)):\n    median = train_df[fill_null_columns[i]].median()\n    train_df[fill_null_columns[i]].fillna(median, inplace=True)\n    # test data\n    df_test[fill_null_columns[i]].fillna(median, inplace=True)","80b2e028":"# quantile 75% sales price\nquat_price_75 = train_df[\"SalePrice\"].quantile(0.75)\n\ndef price_flg(x):\n    if x[\"SalePrice\"] >= quat_price_75:\n        res = 1\n    else:\n        res = 0\n    return res\n\ntrain_df[\"Price_flg\"] = train_df.apply(price_flg, axis=1)","b92c165d":"# data set\nX = train_df.iloc[:,1:-2]\ny = train_df[\"Price_flg\"]","ae0e5655":"# Train test data split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)","6f214a04":"# Random forest classifier\n# Create instance\nforest = RandomForestClassifier(n_estimators=10, random_state=10)\n\n# Gridsearch\nparam_range = [5,10,15,20]\nleaf = [60,70, 80]\ncriterion = [\"entropy\", \"gini\", \"error\"]\nparam_grid = [{\"n_estimators\":param_range, \"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n\ngs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"accuracy\", cv=10, n_jobs=1)\n\n# Fitting\ngs = gs.fit(X_train, y_train)\n\nprint(gs.best_score_)\nprint(gs.best_params_)","4cfc52aa":"# Fitting for forest instance\nforest = RandomForestClassifier(n_estimators=15, random_state=10,\n                               criterion='gini', max_depth=15,\n                               max_leaf_nodes=60)\nforest.fit(X_train, y_train)\n\n# Importance \nimportance = forest.feature_importances_\n\n# index\nindices = np.argsort(importance)[::-1]\n\nfor f in range(X_train.shape[1]):\n    print(\"%2d) %-*s %f\" %(f+1, 30, X.columns[indices[f]], importance[indices[f]]))","19d169e2":"# Visualization\nplt.figure(figsize=(15,6), facecolor=\"lavender\")\n\nplt.title('Feature Importances')\nplt.bar(range(X_train.shape[1]), importance[indices], color='blue', align='center')\nplt.xticks(range(X_train.shape[1]), X.columns[indices], rotation=90)\nplt.xlim([-1, X_train.shape[1]])\nplt.tight_layout()","93b070e8":"# Selected parameters\nselect_columns = select_columns = [\"OverallQual\", \"GrLivArea\", \"GarageArea\", \"TotalBsmtSF\", \"YearBuilt\", \"ExterQual\", \"2ndFlrSF\", \"LotArea\", \"YearRemodAdd\", \"KitchenQual\", \"Foundation\",\n          \"GarageYrBlt\", \"Fireplaces\", \"BsmtUnfSF\", \"BsmtFinSF1\", \"MasVnrArea\", \"OpenPorchSF\", \"TotRmsAbvGrd\", \"BsmtQual\", \"LotFrontage\", \"WoodDeckSF\",\n          \"Neighborhood\", \"MoSold\", \"BedroomAbvGr\"]","e37ad523":"# data set\n# target price change to log form price distribution analysis\nX = train_df[select_columns]\ny = np.log10(train_df[\"SalePrice\"])","a1291e34":"# Train test data split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)","f010078b":"# create instance\nforest = RandomForestRegressor(n_estimators=100, random_state=10)\n\nparams = {\"max_depth\":[3, 5, 10], \"n_estimators\":[10, 15, 20, 25]}\n\n# Fitting\ncv_f = GridSearchCV(forest, params, cv = 10, n_jobs =10)\n\ncv_f.fit(X_train, y_train)","e1d8ca81":"# prediction\ny_train_pred_f = cv_f.predict(X_train)\ny_test_pred_f = cv_f.predict(X_test)","18767ae8":"print(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_f)))\nprint(\"MSE test;{}\".format(mean_squared_error(y_test, y_test_pred_f)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_f)))\nprint(\"R2 score test:{}\".format(r2_score(y_test, y_test_pred_f)))","0ef0c800":"# Create instance\nxgbr = xgb.XGBRegressor()\n\nparams = {'max_depth': [3, 5, 10], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 5, 10, 100], \n          'subsample': [0.8, 0.85, 0.9, 0.95], 'colsample_bytree': [0.5, 1.0]}\n\n# Fitting\ncv_x = GridSearchCV(xgbr, params, cv = 10, n_jobs =1)\ncv_x.fit(X_train, y_train)","3e64c37a":"# prediction\ny_train_pred_x = cv_x.predict(X_train)\ny_test_pred_x = cv_x.predict(X_test)","0d96254f":"print(\"MSE train:{}\".format(mean_squared_error(y_train, y_train_pred_x)))\nprint(\"MSE test;{}\".format(mean_squared_error(y_test, y_test_pred_x)))\n\nprint(\"R2 score train:{}\".format(r2_score(y_train, y_train_pred_x)))\nprint(\"R2 score test:{}\".format(r2_score(y_test, y_test_pred_x)))","b9c19ca3":"plt.figure(figsize=(10,6))\nplt.scatter(y_test_pred_f, y_test_pred_f - y_test, c=\"b\", marker='o', alpha=0.5, label=\"RandomForest\")\nplt.scatter(y_test_pred_x, y_test_pred_x - y_test, c=\"r\", marker='o', alpha=0.5, label=\"XGB\")\nplt.xlabel('Predicted values')\nplt.ylabel('Residuals')\nplt.legend(loc = 'upper left')\nplt.hlines(y = 0, xmin = 4.6, xmax = 5.7, lw = 2, color = 'black')\n#plt.xlim([-10, 50])","988e1cd5":"y_Test_pred_x = cv_x.predict(df_test[select_columns])\ny_Test_pred_f = cv_f.predict(df_test[select_columns])\ny_Test_pred = (y_Test_pred_x + y_Test_pred_f)\/2\n# Return from logarithmic value\nsubmit_y_Test = 10**y_Test_pred","6f051977":"# submit dataframe\nsubmit = pd.DataFrame({\"Id\":df_test[\"Id\"], \"SalePrice\":submit_y_Test})","4dba2b6b":"submit[\"SalePrice\"].describe()","b9fb4dc3":"submit.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","ec4253be":"I try to predict House Prices with random forest regression and XGB regression.","3d4943ae":"- 5 parameters are about over 50% null data. Perhaps it is known that the variable itself has important information, but it is not appropriate as a feature quantity representing the entire data in model construction.","e654d6ea":"For investigation of important features, I try to use Random forest classifier's importance.<br>\nFirst, for classifier, sales price separate two category, high and low with quantile 75%.","cfb1333a":"## Prediction by Random forest regression","3e639cc3":"### Last, fill null value about<br>\ntrain_data null columns : \"MSZoning\", \"BsmtQual\", \"BsmtCond\", \"BsmtQual\", \"BsmtExposure\", \"BsmtFinType1\", \"GarageType\", \"GarageYrBlt\", \"GarageFinish\" by median values.\ntest_data null columns : \"LotFrontage\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtFullBath\", \"KitchenQual\", \"GarageArea\", \"SaleType\"","15cf6356":"### Calculate the distribution skewness and kurtosis for each variable","e45a9d37":"Remained column names","3806b113":"## Null data","99424c96":"### Numerical values","6e2ba973":"## Data Cleaning and preprocessing","ece4b8d5":"- From the results of the bar graph and box plot, 15 parameters were decided to delete the following parameters, which have extremely small variation and are predicted to have a low relationship with prices.\n\nDropped parameters<br>\n\"Street\", \"LandContour\", \"Utilities\", \"LandSlope\", \"Condition1\", \"Condition2\", \"RoofMatl\", \"BsmtFinType2\", \"Heating\", \"CentralAir\", Electrical\", \"Functional, \"GarageQual\", \"GarageCond\", \"PavedDrive\",","19a8aa95":"- The skewness represents the bias of the distribution. All skew is positive values, so their values are biased left side.<br>\nNext check the distributions.","fc4768a5":"## Submit","c33cc2e3":"## Data loading and Checking","43fa0a2d":"- There is a slight variation in XGB. Also for training data, XGB has a smaller difference in mse and can be said to be a more robust model.\nHowever, it is not clear until looking at the result whether this result is optimal from the no-free lunch theorem.<br>\nThis time, I decided to use the average of both results, hoping to be more robust.","b736f641":"- Also, for improving distribution, I decide to change 4 parameters to log. : \"LotArea\", \"TotalBsmtSF\", \"GrLivArea\"","25e0723b":"Next, I checked the box plot by House price.","87a5defe":"### Numerical value preprocessing","b1fd6a7a":"## Investigation of important features by Random forest classifier","4030dad7":"## Libraries","42e799ad":"Data have many categorical values, the count is 43.<br>\nI checked each count plot. ","e01950a3":"- Each categorical variable has a different distribution of home prices and may be related. However, this alone cannot tell what is important.","6eee299b":"### Direction\n- Null values \nOver 50% null variables : Dropped<br>\nOther null variables : Fill with each median value, because many samples are biased distribution, after following preprocessing.<br>\n- Numerical values\n1st, outliers is dropped for skewness >1 parameters, by larger value over quantile 0.999%.<br>\n2nd, after check distribution, if necessary, logarithmize to make the distribution uniform.\n- Categorcal values\nChange to numerical flag.\n\n### And after, they are trained by random forest. And I check the important parameter from result.","48c6a177":"## Validation","92fa7d50":"- We can see the correlation, Some of strong correlation is \u3010\"TotalBsmtSF\" & \"1stFlrSF\"\u3011, \u3010\"GarageCars\" & \"GarageArea\"\u3011.","f3e57ef6":"### Categorical values","f72859b9":"- Almost variables are listed with low importance. <br>\nThis time, Since there is a risk of over-fitting if there are many variables, this time we decided to decrease parameters to 24 parameters from 48 parameters based on this result.","450b7195":"## Explanatry variables","f0f81055":"- We can see that some variables have a large bias and several outliers. These may affect the model performance during machine learning and should be handled with care.","cee93970":"### Categorical value preprocessing","b11b1b2a":"### Target value : SalePrice","f66d0f72":"- Distribution is near to normalized. This distribution is statistically easier to handle","de20a6f1":"## EDA","7ca0b791":"Fill null values by median","174b60d7":"- Some are random or uniform, some are fairly biased. (Please forgive the overlapping of labels)","474c1320":"## XGB Regressor is better.","e30fe454":"### Null valuer preprocessing","d88946e9":"## Agenda\n- Data loading and Data checking\n- EDA\n- Data Cleaning and preprocessing\n- Investigation of important features by Random forest classifier\n- Prediction by Random forest regression and XGB\n- Validation\n- Submit","2cd56bf8":"# House Prices, EDA and RF\/XGB ensemble prediction","62175a89":"## Prediction by XGB regressor","88cb69bb":"- Distributions are improved, but some parameters are still strong imbalanced.<br>\nSo, I decided to leave out such a parameter, following parameters.<br>\n\"BsmtFinSF2\", \"LowQualFinSF\", \"BsmtHalfBath\", \"KitchenAbvGr\", \"EnclosedPorch\", \"3SsnPorch\", \"ScreenPorch\", \"PoolArea\", \"MiscVal\"\n\n- Aggregate parameters with high multicollinearity.\n\"TotalBsmtSF\" and \"1stFlrSF\" \u21d2 \"TotalBsmtSF\"<br>\n\"GarageCars\" and \"GarageArea\" \u21d2 \"AgarageArea\"","859aa1b7":"- In the case of such a biased distribution, it may be highly likely[](http:\/\/) that it follows a lognormal distribution. Then take the logarithm and re-plot.","49ee4e1f":"### Residual plot"}}