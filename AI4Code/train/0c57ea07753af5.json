{"cell_type":{"7fb53f1f":"code","f9da3874":"code","48d1c942":"code","aa34aa33":"code","03fb0061":"code","2d2effb8":"code","593bcbca":"code","33935ce4":"code","083d8665":"code","bf575dc0":"code","79b6f1d9":"code","455adb07":"code","a53fe0dc":"code","268ee0c3":"code","551b1491":"code","13add95f":"code","fea3df2b":"markdown"},"source":{"7fb53f1f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nimport skimage.io\nimport keras.backend as K\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout,BatchNormalization ,Activation\nfrom tensorflow.keras.models import Model, Sequential\nfrom keras.applications.nasnet import NASNetLarge\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam","f9da3874":"train_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   validation_split = 0.2,\n                                  \n        rotation_range=5,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.2,\n        #zoom_range=0.2,\n        horizontal_flip=True,\n        vertical_flip=True,\n        fill_mode='nearest')\n\nvalid_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                  validation_split = 0.2)\n\ntest_datagen  = ImageDataGenerator(rescale = 1.\/255\n                                  )","48d1c942":"train_dataset  = train_datagen.flow_from_directory(directory = '..\/input\/fer2013\/train',\n                                                   target_size = (48,48),\n                                                   class_mode = 'categorical',\n                                                   subset = 'training',\n                                                   batch_size = 64)","aa34aa33":"valid_dataset = valid_datagen.flow_from_directory(directory = '..\/input\/fer2013\/train',\n                                                  target_size = (48,48),\n                                                  class_mode = 'categorical',\n                                                  subset = 'validation',\n                                                  batch_size = 64)","03fb0061":"test_dataset = test_datagen.flow_from_directory(directory = '..\/input\/fer2013\/test',\n                                                  target_size = (48,48),\n                                                  class_mode = 'categorical',\n                                                  batch_size = 64)","2d2effb8":"base_model = tf.keras.applications.VGG16(input_shape=(48,48,3),include_top=False,weights=\"imagenet\")","593bcbca":"# Freezing Layers\n\nfor layer in base_model.layers[:-4]:\n    layer.trainable=False","33935ce4":"# Building Model\n\nmodel=Sequential()\nmodel.add(base_model)\nmodel.add(Dropout(0.5))\nmodel.add(Flatten())\nmodel.add(BatchNormalization())\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32,kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Activation('relu'))\nmodel.add(Dense(7,activation='softmax'))","083d8665":"# Model Summary\n\nmodel.summary()","bf575dc0":"from tensorflow.keras.utils import plot_model\nfrom IPython.display import Image\nplot_model(model, to_file='convnet.png', show_shapes=True,show_layer_names=True)\nImage(filename='convnet.png') ","79b6f1d9":"def f1_score(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K.epsilon())\n    return f1_val","455adb07":"METRICS = [\n      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n      tf.keras.metrics.Precision(name='precision'),\n      tf.keras.metrics.Recall(name='recall'),  \n      tf.keras.metrics.AUC(name='auc'),\n        f1_score,\n]","a53fe0dc":"lrd = ReduceLROnPlateau(monitor = 'val_loss',patience = 20,verbose = 1,factor = 0.50, min_lr = 1e-10)\n\nmcp = ModelCheckpoint('model.h5')\n\nes = EarlyStopping(verbose=1, patience=20)","268ee0c3":"model.compile(optimizer='Adam', loss='categorical_crossentropy',metrics=METRICS)","551b1491":"history=model.fit(train_dataset,validation_data=valid_dataset,epochs = 5,verbose = 1,callbacks=[lrd,mcp,es])","13add95f":"#%% PLOTTING RESULTS (Train vs Validation FOLDER 1)\n\ndef Train_Val_Plot(acc,val_acc,loss,val_loss,auc,val_auc,precision,val_precision,f1,val_f1):\n    \n    fig, (ax1, ax2,ax3,ax4,ax5) = plt.subplots(1,5, figsize= (20,5))\n    fig.suptitle(\" MODEL'S METRICS VISUALIZATION \")\n\n    ax1.plot(range(1, len(acc) + 1), acc)\n    ax1.plot(range(1, len(val_acc) + 1), val_acc)\n    ax1.set_title('History of Accuracy')\n    ax1.set_xlabel('Epochs')\n    ax1.set_ylabel('Accuracy')\n    ax1.legend(['training', 'validation'])\n\n\n    ax2.plot(range(1, len(loss) + 1), loss)\n    ax2.plot(range(1, len(val_loss) + 1), val_loss)\n    ax2.set_title('History of Loss')\n    ax2.set_xlabel('Epochs')\n    ax2.set_ylabel('Loss')\n    ax2.legend(['training', 'validation'])\n    \n    ax3.plot(range(1, len(auc) + 1), auc)\n    ax3.plot(range(1, len(val_auc) + 1), val_auc)\n    ax3.set_title('History of AUC')\n    ax3.set_xlabel('Epochs')\n    ax3.set_ylabel('AUC')\n    ax3.legend(['training', 'validation'])\n    \n    ax4.plot(range(1, len(precision) + 1), precision)\n    ax4.plot(range(1, len(val_precision) + 1), val_precision)\n    ax4.set_title('History of Precision')\n    ax4.set_xlabel('Epochs')\n    ax4.set_ylabel('Precision')\n    ax4.legend(['training', 'validation'])\n    \n    ax5.plot(range(1, len(f1) + 1), f1)\n    ax5.plot(range(1, len(val_f1) + 1), val_f1)\n    ax5.set_title('History of F1-score')\n    ax5.set_xlabel('Epochs')\n    ax5.set_ylabel('F1 score')\n    ax5.legend(['training', 'validation'])\n\n\n    plt.show()\n    \n\nTrain_Val_Plot(history.history['accuracy'],history.history['val_accuracy'],\n               history.history['loss'],history.history['val_loss'],\n               history.history['auc'],history.history['val_auc'],\n               history.history['precision'],history.history['val_precision'],\n               history.history['f1_score'],history.history['val_f1_score']\n              )","fea3df2b":"# **Dataset & Description**\n\n\n\n[![image.png](attachment:image.png)](http:\/\/)\n\n\nThe data consists of 48x48 pixel grayscale images of faces. The faces have been automatically registered so that the face is more or less centred and occupies about the same amount of space in each image.\n\nThe task is to categorize each face based on the emotion shown in the facial expression into one of seven categories (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral). The training set consists of 28,709 examples and the public test set consists of 3,589 examples.\n\n\n\n\n\n\n\n\n[Dataset link ](https:\/\/www.kaggle.com\/msambare\/fer2013)"}}