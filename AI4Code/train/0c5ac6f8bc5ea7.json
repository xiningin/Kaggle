{"cell_type":{"adba851f":"code","c0938081":"code","08c08b50":"code","adb41563":"code","b22f6292":"code","9031e1b0":"code","82a71eeb":"code","d9559e24":"code","8f1b927c":"code","cd2ea561":"code","beaed9e2":"code","7d4eaff0":"code","b5ee2447":"code","509fed60":"code","9068d6f6":"code","a900d7bb":"code","9f8237d4":"code","89686105":"code","32e59028":"code","ac301eca":"code","30cee4a9":"code","2aa4e7d0":"code","212ff3d1":"code","0d81bb0c":"code","5d37126a":"code","73ea9efd":"code","64e346a1":"code","1ec7f83e":"code","35856edd":"code","07ce8885":"code","19760389":"code","9b5e6d1e":"code","797f2af9":"code","15e65cf0":"code","69ea8d25":"code","c64366a7":"code","db1aa9fa":"markdown","7ed63a55":"markdown","8e799589":"markdown","8631d4fa":"markdown","4c8702ea":"markdown","bb2ab93d":"markdown","f34a7164":"markdown","14160f5f":"markdown","62602510":"markdown","02a12a24":"markdown","fe34c202":"markdown","47415b65":"markdown","e4e4bf2c":"markdown","5eb1eef0":"markdown","a21000da":"markdown","c7de5bcb":"markdown","a05d5ca9":"markdown","4844c292":"markdown","2238988c":"markdown","49633464":"markdown","61173574":"markdown","c52f3ed3":"markdown"},"source":{"adba851f":"import pandas as pd\nimport numpy as np\nimport unicodedata","c0938081":"data = pd.read_csv(\"..\/input\/jeopardy.csv\", dtype = {\"round\": np.int16, \"value\": np.int16})\ndata.head()","08c08b50":"data.daily_double.describe()","adb41563":"data.daily_double.loc[data.daily_double == \"no\"] = False\ndata.daily_double.loc[data.daily_double == \"yes\"] = True\ndata.head()","b22f6292":"data[\"answer\"] = data[\"answer\"].str.upper()\ndata[\"question\"] = data[\"question\"].str.upper()\ndata.head()","9031e1b0":"#for example\ndata[\"answer\"].iloc[11]","82a71eeb":"data[\"answer\"] = data[\"answer\"].str.replace(\"\\\\\\\\\", \"\")\ndata[\"question\"] = data[\"question\"].str.replace(\"\\\\\\\\\", \"\")\ndata[\"category\"] = data[\"category\"].str.replace(\"\\\\\\\\\", \"\")\ndata[\"answer\"].iloc[11]","d9559e24":"#notice the question\ndata.iloc[16:17]","8f1b927c":"def strip_accents(text):\n    try:\n        text = unicode(text, 'utf-8')\n    except NameError: # unicode is a default on python 3 \n        pass\n    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n    return str(text)\n\ndata[\"category\"] = data[\"category\"].apply(strip_accents)\ndata[\"answer\"] = data[\"answer\"].apply(strip_accents)\ndata[\"question\"] = data[\"question\"].apply(strip_accents)\n\n#and now\ndata.iloc[16:17]","cd2ea561":"#notice\ndata.iloc[129490:129497, ]","beaed9e2":"data[\"value\"].iloc[:129493] = 2*data[\"value\"].iloc[:129493]\ndata.iloc[129490:129497, ]","7d4eaff0":"ACCEPTABLE_VALUES = [200, 400, 600, 800, 1000, 1200, 1600, 2000]\n\ndata.loc[(data[\"round\"] != 3) & ((~data[\"value\"].isin(ACCEPTABLE_VALUES))|data[\"daily_double\"])]","b5ee2447":"data.loc[349562:349571, ]","509fed60":"indexes_with_problems = [index for index in range(0, len(data)) if data[\"round\"].iloc[index] != 3 \\\n                                                                and ((data[\"value\"].iloc[index] not in ACCEPTABLE_VALUES)\n                                                                     or data[\"daily_double\"].iloc[index])]\nlen(indexes_with_problems)\nassert(len(indexes_with_problems) == 17025)","9068d6f6":"ROUND_ONE_VALUES = set([200, 400, 600,  800,  1000])\nROUND_TWO_VALUES = set([400, 800, 1200, 1600, 2000])\n\nindex = 0\nwhile(index < 349630):        #the last problem is at 349625\n    current_category = data[\"category\"].iloc[index]\n    indexes_in_category = [index]\n    index += 1\n    while(data[\"category\"].iloc[index] == current_category):\n        indexes_in_category.append(index)\n        index += 1\n    for i in indexes_in_category:\n        if i in indexes_with_problems:\n            indexes_in_category.remove(i)\n            possible_values = ROUND_ONE_VALUES.copy() if data[\"round\"].iloc[i] == 1 else ROUND_TWO_VALUES.copy()\n            for j in indexes_in_category:\n                if data[\"value\"].iloc[j] in possible_values:\n                    possible_values.remove(data[\"value\"].iloc[j])\n                else:\n                    print(\"Problem removing value from line\", j, end= \"\\n\")\n            data[\"value\"].iloc[i] = max(possible_values)\n            break","a900d7bb":"data.iloc[4257:4263]","9f8237d4":"data.iloc[77347:77352]","89686105":"data[\"value\"].iloc[77347] = 400","32e59028":"data.iloc[79592:79597]","ac301eca":"data[\"value\"].iloc[79592] = 400","30cee4a9":"data.tail(6)","2aa4e7d0":"index = 0\nwhile(index < 349636):        #since we see above there is no problem with the last round\n    while data[\"round\"].iloc[index] == 3: #some Final Jeopardy questions appear successively without any round content\n        index += 1\n    current_category = data[\"category\"].iloc[index]\n    indexes_in_category = [index]\n    index += 1\n    while(data[\"category\"].iloc[index] == current_category):\n        indexes_in_category.append(index)\n        index += 1\n    for i in indexes_in_category:\n        possible_values = ROUND_ONE_VALUES.copy() if data[\"round\"].iloc[i] == 1 else ROUND_TWO_VALUES.copy()\n        for j in indexes_in_category:\n            if data[\"value\"].iloc[j] in possible_values:\n                possible_values.remove(data[\"value\"].iloc[j])\n            else:\n                print(\"Problem removing value from line \", j, end= \"\\n\")\n                break","212ff3d1":"data.iloc[44064:44067]","0d81bb0c":"data[\"value\"].iloc[44065] = 400\ndata.iloc[44064:44067]","5d37126a":"data.to_csv(\"jeopardy_clean.csv\", index = False)","73ea9efd":"data = pd.read_csv(\"jeopardy_clean.csv\")","64e346a1":"del data[\"daily_double\"] #1\n\ndata[\"air_date\"] = pd.to_datetime(data[\"air_date\"], format = \"%m\/%d\/%Y\") #so that ids are in order by date\n\ndata[\"category_id\"] = data.groupby([\"air_date\", \"category\"]).ngroup() + 1 #2\n\ndata.sort_values(by=[\"category_id\", \"value\"], ascending=[False, False], inplace = True) #3 and #6\n\ndel data[\"value\"] #4\ndel data[\"air_date\"] #5\n\nbad_strings = [\"HEARD HERE\", \"SEEN HERE\", \"SHOWN HERE\", \"PICTURED HERE\"]\nfor string in bad_strings:\n    data = data[~data[\"answer\"].str.contains(string)] #7\n\nnormal_questions = data.loc[data[\"round\"] != 3] #8\nfinal_questions = data.loc[data[\"round\"] == 3]\n\ndel normal_questions[\"round\"] #9\ndel final_questions[\"round\"]\n\nnormal_questions.reset_index(inplace = True, drop = True)\nfinal_questions.reset_index(inplace = True, drop = True)\n\nnormal_questions.head(8)","1ec7f83e":"final_questions.head(5)","35856edd":"normal_questions.to_csv(\"jeopardy_normal.csv\", index = False)\nfinal_questions.to_csv(\"jeopardy_final.csv\", index = False)","07ce8885":"normals = pd.read_csv(\"jeopardy_normal.csv\")\nfinals = pd.read_csv(\"jeopardy_final.csv\")","19760389":"num_categories = len(normals[\"category_id\"].unique())\nnum_finals = len(finals)\nnum_clues = len(normals)\nprint(\"There are {:,} clues across {:,} categories and {:,} Final Jeopardy questions\"\\\n          .format(num_clues,        num_categories,     num_finals), end=\".\")","9b5e6d1e":"finals[\"agg\"] = finals[\"category\"] + \"|\" + finals[\"answer\"] + \"|\" + finals[\"question\"]\ndel finals[\"category\"]\ndel finals[\"answer\"]\ndel finals[\"question\"]\nfinals.head()","797f2af9":"finals.to_csv(\"jeopardy_final_agg.csv\", index = False)","15e65cf0":"normals[\"agg\"] = normals[\"category\"] + \"|\" + normals[\"answer\"] + \"|\" + normals[\"question\"]\ndel normals[\"category\"]\ndel normals[\"answer\"]\ndel normals[\"question\"]\nnormals.head()","69ea8d25":"normals[\"freq\"] = normals.groupby(\"category_id\")[\"category_id\"].transform(\"count\")\nnormals[\"category_id\"].loc[normals[\"freq\"] < 5] = 0\ndel normals[\"freq\"]\nnormals.head()","c64366a7":"size = len(normals)\nfor quarter in range(1, 5):\n    chunk = normals.iloc[int((quarter-1)*size\/4) : int(quarter*size\/4), ]\n    chunk.to_csv(\"jeopardy_normal_agg\" + str(5-quarter) + \".csv\", index = False)","db1aa9fa":"# Simple, Interactive, and Dynamic Jeopardy [Web App](http:\/\/www.jacobtbigham.com\/jeopardy)\nI recently finished reading Madeleine Albright's engaging *Fascism: A Warning*, and upon completing it two thoughts crossed my mind. First, I was thankful to have access to Alrbight's insights and experiences with world leaders; her unique pedigree and history offers perspective I could not dream of. Second, I was mildly alarmed that for this book\u2014more than most others I have read\u2014I knew I would forget the vast majority of information in the book. Keeping track of Orban, Erdogan, Chavez, and the names of cities, places, and other dictators is just too difficult for me to handle. I could, of course, have kept diligent notes while reading, but I figured that even then I might not be able to recall names and ideas when needed.\n\nLong story much shorter: I want to develop a simple Jeopardy game into which I can feed a keyword and then receive Jeopardy questions related to that keyword. Fortunately, the Jeopardy [Archive](http:\/\/www.j-archive.com) stores hundreds of thousands of past Jeopardy questions, which others have dutifully scraped from the JSON files in the archive.\n\nBecause my personal [website](http:\/\/www.jacobtbigham.com) is hosted on SquareSpace (don't @ me), databasing these questions was a little difficult, since I was forced to use Javascript (and I can't keep Promises, apparently) and could not use SQL. Fortunately, I learned a lot more from coding this project than I bargained for.\n\nHere's an example of gameplay:\n\n![Game](https:\/\/i.imgur.com\/xm4DLKq.gif)\n\nA couple of features (and lack of features) worth noting:\n- Users input a single keyword or key phrase, to which the app then queries the database for matches.\n- Those keywords can match exactly or as substrings within other words (e.g., dog: dog, or erdogan).\n- I did not employ any ML algorithms to extract key words, phrases, or topics from questions and answers.\n- Instead, I give users the option to either view clues that *only* match the keyword **or** to get clues that matched the keyword *and* the other clues in the same categories as those clues. In this way, I let the Jeopardy writers do my ML for me.\n- If there are not enough clues that match a given keyword, then the keyword is rejected.\n- I used the Levenshtein distance between users' answers and correct answers to determine whether answers were correct.\n- I separated Final Jeopardy and regular Jeopardy questions so that users can get the full Jeopardy experience.\n- The first round contains one Daily Double question, and the second round contains two Daily Double questions.\n- Users can restart the game with a new keyword at any time.\n- Users can share their score to social media sites, along with the keyword for their custom game.\n- Users can alternatively opt to play a random Jeopardy game, with clues from random and unrelated categories.\n\nMy overall approach was the following:\n1. Construct a database to hold regular Jeopardy and Final Jeopardy questions\n2. Build a user interface with HTML and Javascript to interact with the database and present questions\n3. Enjoy!\n\nFeedback, suggestions, and corrections are greatly appreciated!","7ed63a55":"First iterations of my implementation of the game allowed for the board to have fewer than 25 clues. For example, in the VENICE category in the above output, on the show in which that category was played, only three of the clues were revealed. I figured it would be better (and more inclusive) to give those categories a chance, but it just looks gross, especially when there are only 1 or 2 clues in a category. So, I'm going to assign any clues that belong to an incomplete category a category_id of 0, which I can then exclude from database searches when desired. Notably, this still allows clues from incomplete categories to appear in games in which the user does not opt to keep categories intact.","8e799589":"First, let's load the data, which is available [here](https:\/\/drive.google.com\/drive\/folders\/1fxY181PdiA1KoJRG23ZVLC2Y5CIRqQWx), and take care of some of the easy wrangling. (I converted from .tsv to .csv format on my computer so that it would open automatically in Excel, wherein I removed the comments and notes sections.)","8631d4fa":"Fantastic! Now, because the show double point values on November 26, 2001, we need to adjust clue values prior to that date to match. That the data are arranged chronologically makes this simple:","4c8702ea":"Let's first verify that the daily_double only has \"yes\" and \"no\" values, and then let's change the values to True and False:","bb2ab93d":"Miraculous! By the way, notice that Final Jeopardy questions are indicated by a round value of 3, and their corresponding value is 0. Now, also not obvious is that not every point value is correct or possible:","f34a7164":"Interesting problems were at fault here! For element 4257, which was a Final Jeopardy question, its category\u2014EUROPE\u2014was the same as the first category for the next show, so the code saw them all as one category. There's no need for an edit here, but I show the cells below to verify. The other two problematic values occurred for episodes that contained \"bonus\" rounds, where contestants could give either one or two of the two possible answers. I'll edit these manually since there are only two of these rounds:","14160f5f":"Now, just as a final sanity check, let's make sure that every category has *unique* point values and adjust any that don't:","62602510":"Beautiful! I also want to remove any special\/foreign\/accented characters (since otherwise a search would not be able to locate them!), so let's do that here using [this](https:\/\/stackoverflow.com\/a\/44433664) function:","02a12a24":"So that I need not run this processing code every time I want to use the cleaned data, let's go ahead and save these DataFrames in new .csv files called `jeopardy_normal.csv` and `jeopardy_final.csv`.","fe34c202":"Great! Now, just to be consistent with the capitalization of the categories (and the style of the actual game show answers), let's convert our answers and questions to uppercase:","47415b65":"## The Web Application\nYou can view the work-in-progress game [here](http:\/\/www.jacobtbigham.com\/jeopardy) and most of the source code [here](https:\/\/github.com\/jacobtbigham\/jeopardy\/blob\/master\/website_source_code.html).\n\nOne of the toughest parts of the implementation was scaling. Because the text to display constantly changes as the game proceeds (and cannot be known ahead of time), the size of text and other display elements has to continually be changed throughout the duration of a game. Indeed, you can find a great number of methods in the Javascript code that are devoted specifically to resizing.\n\nAnother difficult component of the game is dealing with answer validation. Because the question and answer data rely on user input from J-archive, there's no guarantee that all answers will be spelled correctly or formatted uniformly (e.g., if an answer is \"seventeen,\" can we be sure that the database text will be the word \"seventeen,\" or will it be just the numerical \"17\"?). To deal with some deviations in spellings\u2014both in player input and database spelling\u2014I've allowed a tolerance based on edit distance and answer length. I've also remove some common stop-words from answers (like \"the,\" \"a,\" etc.) and accepted answers if only the last name of an individual is input.\n\nThere remains much to improve and develop:\n- Build a more robust system for verifying players' answer inputs\n- Implement a cookie system that tracks how many times users have played so that different portions of the database are searched (and questions are less likely to repeat)\n- Improve cross-browser compatibility and display uniformity\n- Implement a high-score system, both for random questions and specific keywords","e4e4bf2c":"This is simply a typo in the J-Archive data, and I will fix it:","5eb1eef0":"## Wrangling","a21000da":"## Preparation for Database Querying\nThe fact that I have separate columns for category, answer, and question is not especially memory-intensive. Namely, I have to separate the values *somehow*, and it's no costlier to store a comma than it is to store any other character. However, when I run database queries, it *is* costlier and requires unnecessarily more lines of code to run three queries (on category, answer, and question) and then merge the results than it is to just run one query for matches on the aggregate of those three values. So, I'm going to aggregate the data into just one block of text, where the category, question, and answer are separated by a `|` symbol.","c7de5bcb":"Stellar! Furthermore, though it's not immediately obvious here, many of the catepgories, answers, and questions contain quotation marks that are formatted grotesquely. Let's get those in order:","a05d5ca9":"That does it for the preparatory steps.","4844c292":"As if by providence, we notice here another problem as well: not all the clues are here, but not for the reason we might first suppose (that the clue was never revealed). If we look at the Jeopardy archive [page](http:\/\/www.j-archive.com\/showgame.php?game_id=6388) for 7-25-2019, the reasons for these problems become clear..\n\nThe 4000-point clues, for example, are Daily Doubles! And the missing clue in the \"GATES\" category is a clue that contained an image\u2014and those clues are omitted from the dataset.\n\nI'm going to impute the Daily Double values by assigning them whatever value is missing based on the other clues for each category. Notably, sometimes there are missing clues in categories with Daily Doubles. In such cases, I will assign to the Daily Double question the highest of missing values, since, in the past, Daily Double questions tended lower on the board.\n\nIt's mildly more costly but easier to implement this by running over the entire dataset, not just the problem-values we identified before. This approach makes is easier to identify category chunks. I could have used a swifter Pandas aggregate and sort approach, but the iterative approach below works and isn't so slow that it's worth scrapping.","2238988c":"Because there are limitations on file size for uploading to the database on Back4App, I'm going to split this set into 4 chunks. Whether the chunks split cleanly between categories is not relevant, since they'll be reaggregated in the database. New imports stack atop old imports, so chunk 1 will be the bottom quarter, up to chunk 4 the top quarter.\n\nNotably, I could always just use a sort on the category_ids, but I'm not super sure how efficient the database I'm using is!","49633464":"Finally, let's see how many total clues, categories, and Final Jeopardy questions we have in this set:","61173574":"## App-Specific Wrangling\nThinking ahead about implementation, I'd like to make a few little fixes. \n\nFirst, since I really don't care about the exact air date of each question, but I *do* care about the relative air dates (I'd like to prioritize newer questions), **I'm going to remove the air_date column**. The relative order of air dates is contained in the DataFrame index. Indeed, since database queries will access questions from top to bottom, **I'm going to reverse the vertical order of the entries**\u2014with the caveat (CC) introduced below.\n\nSecond, I really don't care whether a clue was a Daily Double question, for two reasons: one, since I'm going to be pulling from questions across many categories anyways, it might be disruptive and unnecessarily costly to preserve this data; and two, and more importantly, the game has changed such that the location and relative difficulty of Daily Double questions seems random. So, **I'm going to drop the daily-double column**.\n\nThird, I don't so much care what round in which a question appeared. I do, however, care whether it was a Final Jeopardy question, since the syntax of those questions (and their difficulty) tends to differ from those of the normal rounds. I could add a separate column to track this\u2014and remove the round column\u2014but what **I'm going to instead do is create a separate DataFrame with just Final Jeopardy questions**. My rationale for this is speed: I'll only want to search for Final Jeopardy questions *when it's time for Final Jeopardy*, so there's no sense in keeping an unneeded extra column (round) or in keeping unneeded extra clues (the Final Jeopardy clues) in the same data structure as the normal clues. **I'll remove the round column after that separation**.\n\nFourth, there are some clues that originally had audio and video accompaniments that are not replicated here. Often these clues contain the phrase \"heard here\" or \"seen here.\" **I will remove clues that contain these phrases.**\n\nFinally, I also don't *really* care how much each question is worth; I only care about the relative order, which is contained in the index. So, **I'm going to drop the value column**. I do care, however, whether questions were in the same category group. Because category names are not unique, **I'm going to give each category from each game a distinct category_id**. (CC) Because I'll be flipping the vertical order of the clues, **I'll first flip the vertical order of the clues in each category**, such that after overall flipping, the harder (higher-valued) questions appear lower in the database.\n\nSo, let's go ahead and make these final changes:\n1. Remove the daily_double column\n2. Give each category a unique category_id\n3. Flip the vertical order of questions in each category\n4. Remove the value column\n5. Remove the air_date column\n6. Flip the overall vertical order of clues\n7. Remove and clues with \"heard here\" or \"seen here\" in the answer\n8. Separate regular and Final Jeopardy questions\n9. Remove the round column","c52f3ed3":"All set! There are some further changes I will make to the data for reasons idiosyncratic to my use case, but I'm exporting this set as `jeopardy_clean.csv` for anyone else to use. I think many would find the clean-ups beneficial. (Plus, I'll likely need to come back to this as a starting point for any changes, and I don't want to rerun the above code every time!)"}}