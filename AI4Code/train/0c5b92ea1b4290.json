{"cell_type":{"11afccf9":"code","b57bc369":"code","c860c44d":"code","c05d7bbe":"code","75a9f8fe":"code","1c92e49e":"code","24966e8c":"code","b77a6676":"code","b7794c3d":"code","5e9a0c16":"code","7e00dbba":"code","b4e39e9a":"code","36ebbb96":"code","e99a1f59":"code","6ec461f9":"code","259fac40":"code","e5fc043a":"code","ad5ebfc9":"code","6ff1655b":"code","6db6545c":"code","3ed454b7":"code","464a8ed5":"code","541f2480":"code","b8a9030b":"markdown","f18d75e6":"markdown","6e23e9d9":"markdown","b636c463":"markdown","3b5aac2a":"markdown","98004287":"markdown","4551026a":"markdown","65722abd":"markdown","8cb16bf7":"markdown"},"source":{"11afccf9":"import matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom keras import backend as K\nimport os\nfrom collections import defaultdict\nfrom shutil import copy\nfrom shutil import copytree, rmtree\nimport PIL\nimport glob\nfrom keras.preprocessing.image import img_to_array, array_to_img\nfrom keras import layers\nfrom keras.models import Sequential, Model\nfrom keras.preprocessing.image import ImageDataGenerator\nimport os\nimport shutil\nfrom keras.losses import binary_crossentropy\nimport imageio\n!pip install -q git+https:\/\/github.com\/tensorflow\/docs\nimport tensorflow_docs.vis.embed as embed","b57bc369":" tf.compat.v1.disable_eager_execution()","c860c44d":"food_list = ['Apple Granny Smith','Corn','Onion White','Limes']\nsrc_train = '\/kaggle\/input\/fruit-recognition\/train\/train'\ndest_train = 'train_mini'\n\n","c05d7bbe":"# \u0412\u0441\u043f\u043e\u043c\u043e\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u044f \u043c\u0438\u043d\u0438-\u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u0432\ndef dataset_mini(food_list, src, dest):\n  if os.path.exists(dest):\n    rmtree(dest) # \u0443\u0434\u0430\u043b\u044f\u0435\u043c \u043f\u0430\u043f\u043a\u0443 \u043c\u0438\u043d\u0438-\u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 (\u0435\u0441\u043b\u0438 \u0442\u0430\u043a\u0430\u044f \u0435\u0441\u0442\u044c), \u0434\u043b\u044f \u0442\u043e\u0433\u043e, \u0447\u0442\u043e\u0431\u044b \u0432 \u043f\u0430\u043f\u043a\u0435 \u0431\u044b\u043b\u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u043a\u043b\u0430\u0441\u0441\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043c\u044b \u0443\u043a\u0430\u0436\u0435\u043c\n  os.makedirs(dest)\n  for food_item in food_list :\n    print(\"Copying images into\",food_item)\n    copytree(os.path.join(src,food_item), os.path.join(dest,food_item))\n      ","75a9f8fe":"print(\"\u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u0441 \u0432\u044b\u0431\u0440\u0430\u043d\u043d\u044b\u043c\u0438 \u043a\u043b\u0430\u0441\u0441\u0430\u043c\u0438\")\ndataset_mini(food_list, src_train, dest_train)","1c92e49e":"print(\"\u0412\u0441\u0435\u0433\u043e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0432 \u043c\u0438\u043d\u0438 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0435\")\n\n!find train_mini -type d -or -type f -printf '.' | wc -c","24966e8c":"img_width, img_height = 112, 112\nIMG_SHAPE = (img_width, img_height, 3)\nlatent_dim = 2","b77a6676":"datagen = ImageDataGenerator(rescale=1.\/ 255)","b7794c3d":"train_gen = datagen.flow_from_directory('train_mini\/',  class_mode='input', \n                                        target_size=(img_width, img_height), batch_size=32, color_mode = 'rgb')","5e9a0c16":"def compute_latent(x):\n    mu, sigma = x\n    batch = K.shape(mu)[0]\n    dim = K.int_shape(mu)[1]\n    eps = K.random_normal(shape=(batch,dim))\n    return mu + K.exp(sigma\/2)*eps","7e00dbba":"def kl_reconstruction_loss(true, pred):\n    # Reconstruction loss (binary crossentropy)\n    reconstruction_loss = binary_crossentropy(K.flatten(true), K.flatten(pred)) * img_width * img_height\n\n    # KL divergence loss\n    kl_loss = 1 + sigma - K.square(mu) - K.exp(sigma)\n    kl_loss = K.sum(kl_loss, axis=-1)\n    kl_loss *= -0.5\n    # Total loss = 50% rec + 50% KL divergence loss\n    return K.mean(reconstruction_loss + kl_loss)","b4e39e9a":"\ni = layers.Input(shape=IMG_SHAPE)\ncx = layers.Conv2D(filters=128, kernel_size=5, strides=2, padding='same', activation='relu')(i)\ncx = layers.BatchNormalization()(cx)\ncx = layers.Conv2D(filters=256, kernel_size=5, strides=2, padding='same', activation='relu')(cx)\ncx = layers.BatchNormalization()(cx)\ncx = layers.Conv2D(filters=512, kernel_size=5, strides=2, padding='same', activation='relu')(cx)\ncx = layers.BatchNormalization()(cx)\ncx = layers.Conv2D(filters=1024, kernel_size=5, strides=2, padding='same', activation='relu')(cx)\ncx = layers.BatchNormalization()(cx)\nx  = layers.Flatten()(cx)\nx  = layers.Dense(20, activation='relu')(x)\nx  = layers.BatchNormalization()(x)\n\nmu = layers.Dense(latent_dim)(x)\nsigma = layers.Dense(latent_dim)(x)\n\nlatent_space = layers.Lambda(compute_latent, output_shape=(latent_dim,))([mu, sigma])\n\nconv_shape = K.int_shape(cx)\n\nd_i   = layers.Input(shape=(latent_dim, ))\nx     = layers.Dense(conv_shape[1] * conv_shape[2] * conv_shape[3], activation='relu')(d_i)\nx     = layers.BatchNormalization()(x)\nx     = layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\ncx    = layers.Conv2DTranspose(filters=1024, kernel_size=5, strides=2, padding='same', activation='relu')(x)\ncx    = layers.BatchNormalization()(cx)\ncx    = layers.Conv2DTranspose(filters=512, kernel_size=5, strides=2, padding='same', activation='relu')(cx)\ncx    = layers.BatchNormalization()(cx)\ncx    = layers.Conv2DTranspose(filters=256, kernel_size=5, strides=2, padding='same', activation='relu')(cx)\ncx    = layers.BatchNormalization()(cx)\ncx    = layers.Conv2DTranspose(filters=128, kernel_size=5, strides=2, padding='same', activation='relu')(cx)\ncx    = layers.BatchNormalization()(cx)\no     = layers.Conv2DTranspose(filters=3, kernel_size=3, activation='sigmoid', padding='same')(cx)\n\nencoder = Model(i, latent_space)\ndecoder = Model(d_i, o)\nvae_outputs = decoder(encoder(i))\nvae = Model(i,vae_outputs)","36ebbb96":"encoder.summary()","e99a1f59":"decoder.summary()","6ec461f9":"vae.compile(optimizer='adam', loss=kl_reconstruction_loss)\nhistory = vae.fit(train_gen, epochs=40)","259fac40":"generator_model = decoder\nx_values = np.linspace(-5, 5, 30)\ny_values = np.linspace(-5, 5, 30)\nfigure = np.zeros((112 * 30, 112 * 30,3))\nfor ix, x in enumerate(x_values):\n    for iy, y in enumerate(y_values):\n        latent_point = np.array([[x, y]])\n        generated_image = generator_model.predict(latent_point)[0]\n        figure[ix*112:(ix+1)*112, iy*112:(iy+1)*112] = generated_image\nplt.figure(figsize=(15, 15))\nplt.imshow(figure, extent=[5,-5,5,-5])\nplt.show()","e5fc043a":"def plot_label_clusters(encoder, data, labels):\n    # display a 2D plot of the digit classes in the latent space\n    z_mean = encoder.predict(data)\n    plt.figure(figsize=(12, 10))\n    plt.scatter(z_mean[:, 0], z_mean[:, 1], c = labels.classes)\n    plt.colorbar()\n    plt.xlabel(\"z[0]\")\n    plt.ylabel(\"z[1]\")\n    plt.show()","ad5ebfc9":"test_images = glob.glob('train_mini\/**\/*.jpg', recursive= True)\nX_train = np.zeros((len(test_images), img_width, img_height, 3))\nfor i in range(len(test_images)):\n  image = tf.keras.preprocessing.image.load_img(test_images[i], target_size= (img_width, img_height))\n  X_train[i] = img_to_array(image) \/ 255","6ff1655b":"plot_label_clusters(encoder, X_train, train_gen)","6db6545c":"!mkdir images_for_gif","3ed454b7":"!cd images_for_gif\nfor x in np.arange(0,5,0.1):\n    latent_point = np.array([[x, -1]])\n    image = generator_model.predict(latent_point)[0]\n    plt.imshow(image)\n    plt.savefig('images_for_gif\/' +'image' + str(x) + '.png')","464a8ed5":"anim_file = 'cvae.gif'\n\nwith imageio.get_writer(anim_file, mode='I') as writer:\n  filenames = glob.glob('images_for_gif\/image*.png')\n  filenames = sorted(filenames)\n  for filename in filenames:\n    image = imageio.imread(filename)\n    writer.append_data(image)\n  image = imageio.imread(filename)\n  writer.append_data(image)","541f2480":"embed.embed_file(anim_file)","b8a9030b":"\u0411\u0435\u0437 \u044d\u0442\u043e\u0439 \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0438, \u043c\u043e\u0434\u0435\u043b\u044c \u043e\u0442\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442\u0441\u044f \u043e\u0431\u0443\u0447\u0430\u0442\u044c\u0441\u044f (","f18d75e6":"\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u0431\u044b\u043b \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 https:\/\/www.kaggle.com\/sshikamaru\/fruit-recognition. \u042f \u0432\u044b\u0431\u0440\u0430\u043b \u0434\u0430\u0442\u0430\u0441\u0435\u0442, \u0441\u043e\u0434\u0435\u0440\u0436\u0430\u0449\u0438\u0439 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0433\u043e \u0440\u0430\u0437\u0440\u0435\u0448\u0435\u043d\u0438\u044f, \u0432\u0441\u0435\u0433\u043e 100\u0445100 \u043f\u0438\u043a\u0441\u0435\u043b\u0435\u0439, \u0442\u0430\u043a \u043a\u0430\u043a VAE encoder  \u043f\u043b\u043e\u0445\u043e \u043f\u043e\u0434\u0445\u043e\u0434\u0438\u0442 \u0434\u043b\u044f \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439 \u0432\u044b\u0441\u043e\u043a\u043e\u0433\u043e \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430, \u043b\u0443\u0447\u0448\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c GAN \u043c\u043e\u0434\u0435\u043b\u0438","6e23e9d9":"\u041d\u0438\u0436\u0435 \u043e\u043f\u0438\u0441\u0430\u043d\u044b \u0444\u0443\u043d\u043a\u0446\u0438\u0438, \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0435 \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b VAE, \u0432 \u0447\u0430\u0441\u0442\u043d\u043e\u0441\u0442\u0438 \u0441\u0432\u043e\u044f loss \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438 - \u0441\u043e\u0432\u043e\u043a\u0443\u043f\u043d\u043e\u0441\u0442\u044c Reconstruction-loss \u0438 KL-loss. \u0410 compute_latent \u044d\u0442\u043e \u043d\u0435\u043a\u0438\u0439 Reparameterization trick, \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u044e\u0449\u0438\u0439 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0433\u0440\u0430\u0434\u0438\u0435\u043d\u0442\u043d\u044b\u0439 \u0441\u043f\u0443\u0441\u043a \u0434\u043b\u044f \u043d\u0430\u0448\u0435\u0439 loss \u0444\u0443\u043d\u043a\u0446\u0438\u0438.","b636c463":"\u042f \u0440\u0435\u0448\u0438\u043b \u0441\u043e\u0437\u0434\u0430\u0442\u044c \u043c\u0438\u043d\u0438-\u0432\u044b\u0431\u043e\u0440\u043a\u0443 \u0438\u0437 4 \u043a\u043b\u0430\u0441\u0441\u043e\u0432, \u0438\u043c\u0435\u043d\u043d\u043e \u044d\u0442\u0438 \u043a\u043b\u0430\u0441\u0441\u044b \u0432\u044b\u0431\u0440\u0430\u043b \u0434\u043b\u044f \u0431\u043e\u043b\u044c\u0448\u0435\u0439 \u043d\u0430\u0433\u043b\u044f\u0434\u043d\u043e\u0441\u0442\u0438 (\u0443 \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u043b\u0438\u0431\u043e \u0446\u0432\u0435\u0442, \u043b\u0438\u0431\u043e \u0444\u043e\u0440\u043c\u0430 \u0434\u0440\u0443\u0433\u0430\u044f)","3b5aac2a":"\u041d\u0438\u0436\u0435 \u0432\u0438\u0437\u0443\u0430\u043b\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u044b \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u044b \u043a\u043b\u0430\u0441\u0441\u043e\u0432 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0438\u0437 latent space","98004287":"\u0422\u0430\u043a\u0443\u044e \u0441\u043b\u043e\u0436\u043d\u0443\u044e \u0441\u0442\u0440\u0443\u043a\u0442\u0443\u0440\u0443 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e\u0434\u0441\u043c\u043e\u0442\u0440\u0435\u043b \u0432 \u0434\u0430\u043d\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u0435 - https:\/\/arxiv.org\/pdf\/1511.06434.pdf.\n(\u0412 \u0440\u0430\u0431\u043e\u0442\u0435 \u0443\u043a\u0430\u0437\u0430\u043d\u0430 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0430, \u043d\u0435 \u0441\u0430\u043c \u043a\u043e\u0434) \u0414\u0430, \u043e\u043d\u0430 \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u0433\u0440\u043e\u043c\u043e\u0437\u0434\u043a\u043e\u0439, \u043d\u043e \u044d\u0442\u0430 \u0446\u0435\u043d\u0430 \u0437\u0430 \u0431\u043e\u043b\u0435\u0435 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u0443\u044e \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u044e \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439.","4551026a":"\u0410 \u0442\u0443\u0442 \u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0437\u0438\u043b \u0441\u0435\u0442\u043a\u0443 \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0445 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432 \u0438\u0437 \u0434\u0435\u043a\u043e\u0434\u0435\u0440\u0430, \u043f\u0440\u0438 \u043f\u0435\u0440\u0435\u0434\u0430\u0447\u0435 \u0435\u043c\u0443 \u0442\u043e\u0447\u0435\u043a (x,y)","65722abd":"\u042f \u0432\u044b\u0431\u0440\u0430\u043b VAE Autoencoder, \u0442\u0430\u043a \u043a\u0430\u043a \u043e\u043d \u0441 \u043b\u0435\u0433\u043a\u043e\u0441\u0442\u044c\u044e \u043f\u043e\u0437\u0432\u043e\u043b\u044f\u0435\u0442 \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043d\u043e\u0432\u044b\u0435 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f, \u0432\u0441\u0435\u0433\u043e \u043b\u0438\u0448\u044c \u043f\u0435\u0440\u0435\u0434\u0430\u0432 \u0435\u043c\u0443 \u043e\u0434\u043d\u0443 \u0435\u0434\u0438\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u0443\u044e \u0442\u043e\u0447\u043a\u0443 (x,y).","8cb16bf7":"\u0410 \u0442\u0443\u0442  \u0441\u043e\u0437\u0434\u0430\u044e\u0442\u0441\u044f \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f \u0434\u043b\u044f \u0433\u0438\u0444\u043a\u0438, \u0433\u0438\u0444\u043a\u0430 \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043f\u0435\u0440\u0435\u0445\u043e\u0434 \u0438\u0437 \u043e\u0434\u043d\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u0432 \u0434\u0440\u0443\u0433\u043e\u0439"}}