{"cell_type":{"79a9cdf2":"code","26ee5093":"code","57506a6f":"code","805a3d8c":"code","a1489aa5":"code","00236a93":"code","399be440":"code","e44efd50":"code","78bc5cd0":"code","8e047eb3":"code","44261261":"code","5a26e39a":"code","836cdfa5":"code","15eb1b37":"code","96165625":"code","0bdf5468":"code","c690c1a8":"code","28e0c512":"code","594c57f3":"code","9e84a86d":"code","ab1db709":"code","69b4bdb1":"code","6efad624":"code","e9520e49":"code","09683c58":"code","9081e6eb":"code","518d8710":"code","28d1ea16":"code","d2904711":"code","6869a55a":"code","71ca0586":"code","f936edc8":"code","459389be":"code","962e8c53":"code","62f4a911":"code","30cf3a20":"code","fda61b35":"code","304dfc74":"code","11917a0c":"code","607e3202":"code","7d0c9ba0":"code","6e0af24e":"code","c33ae5e9":"code","ab996acc":"code","6bafdf22":"code","d73056b0":"code","f8380a43":"markdown","480d4207":"markdown","6d93e707":"markdown","a5fe145e":"markdown","584437b3":"markdown","90d4a315":"markdown","25fae4a5":"markdown","41b47c26":"markdown","f999c17c":"markdown","a998568f":"markdown","3c834d26":"markdown","32cf22d0":"markdown","23164213":"markdown","bc5d9a5e":"markdown","366f3b0b":"markdown","4ffca739":"markdown","ab62fabf":"markdown","2dd69fe1":"markdown","db1a2f88":"markdown","84bd30d7":"markdown","7cfc8cef":"markdown","f33dd9ad":"markdown","6e806d64":"markdown","20afb197":"markdown","56e56197":"markdown","4b0b1286":"markdown"},"source":{"79a9cdf2":"import numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import classification_report, f1_score, precision_score, recall_score\nfrom scipy.stats import probplot, mannwhitneyu, boxcox, shapiro\n\n\n%matplotlib inline","26ee5093":"def plot_qq(data,feature):\n    \"\"\"\n    QQ-plot\n    \"\"\"\n    sns.set(rc={\"figure.figsize\": (10, 6)})\n    probplot(data[feature].sample(1000), plot=plt)\n    plt.title(f'Q-Q plot {feature}')\n    plt.show()\n    \n    \ndef plot_kde_qq_interval(data, feature):\n    \"\"\"\n    Makes 3 plots:\n    1.Builds density graph for feature in the context of target variable.\n    2.QQ-plot\n    3.Confidence intervals in context of target variable\n    \n    \"\"\"\n    \n    sns.set(rc={\"figure.figsize\": (18, 12)})\n    \n    plt.subplot(221)\n    sns.kdeplot(data[data['choose'] == 1][feature],label='\u0420\u0435\u043f\u0435\u0442\u0438\u0442\u043e\u0440 \u043f\u043e\u0434\u0445\u043e\u0434\u0438\u0442', color='green')\n    sns.kdeplot(data[data['choose'] == 0][feature],label='\u0420\u0435\u043f\u0435\u0442\u0438\u0442\u043e\u0440 \u043d\u0435 \u043f\u043e\u0434\u0445\u043e\u0434\u0438\u0442', color='red')\n    plt.xlabel(feature)\n    plt.ylabel('Distribution density')\n    plt.title(f'Distribution {feature} in context of target variable')\n    \n    plt.subplot(222)\n    probplot(data[feature].sample(1000), dist='norm', plot=plt)\n    plt.title(f'Q-Q plot {feature}')\n    \n \n    plt.show()\n    \n    \n\"\"\"\n                Tests\n\"\"\"    \ndef shapiro_samples(data, feature, samples_num, bxc = False , lmbda = None):\n    \"\"\"\n    Checking distribution for normality by the Shapiro-Wilk criterion and reverse Box-Cox conversion\n    \n    \"\"\"\n    shapiro_l = []\n    boxcox_l = []\n    for i in range(samples_num):\n        spl = data[feature].sample(1000)\n        c,p = shapiro(spl)\n        shapiro_l.append(p)\n        \n        if bxc:\n            x, y = boxcox(spl, lmbda=lmbda)\n            c, p = shapiro(x)\n            boxcox_l.append(p)\n    \n    if bxc:\n        print(f'Shapiro average P-value with reverse transformation: {sum(boxcox_l)\/len(boxcox_l)}')\n        print(f'Shapiro average P-value without reverse transformation: {sum(shapiro_l)\/len(shapiro_l)}')\n        print('Threshold \u03b1 = 0,05')\n    if not bxc:\n        print(f'Shapiro average P-value: {sum(shapiro_l)\/len(shapiro_l)}')\n        print('Threshold \u03b1 = 0,05')\n        \n        \ndef manna_uitny(data, feature, samples_num):\n    \"\"\"\n    Mann\u2013Whitney average P-value\n    \"\"\"\n\n    m_list = []\n    for i in range(samples_num):\n        spl_0 = data[data['choose']==0][feature].sample(1000)\n        spl_1 = data[data['choose']==1][feature].sample(1000)\n        \n        c,p = mannwhitneyu(spl_0, spl_1)\n        m_list.append(p)\n        \n    print(f'Mann\u2013Whitney average P-value: {sum(m_list)\/len(m_list)}')\n    print('Threshold \u03b1 = 0,05')\n    \n    \ndef show_proba_calibration_plots(y_predicted_probs, y_true_labels):\n    \"\"\"\n    Makes 3 plots:\n        1. F1 score, precision, recall\n        2. Histogram of probability distribution for each class\n        3. Scoring table\n    \"\"\"\n    preds_with_true_labels = np.array(list(zip(y_predicted_probs, y_true_labels)))\n\n    thresholds = []\n    precisions = []\n    recalls = []\n    f1_scores = []\n\n    for threshold in np.linspace(0.1, 0.9, 9):\n        thresholds.append(threshold)\n        precisions.append(precision_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n        recalls.append(recall_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n        f1_scores.append(f1_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n\n    scores_table = pd.DataFrame({'f1':f1_scores,\n                                 'precision':precisions,\n                                 'recall':recalls,\n                                 'probability':thresholds}).sort_values('f1', ascending=False).round(3)\n  \n    figure = plt.figure(figsize = (15, 5))\n\n    plt1 = figure.add_subplot(121)\n    plt1.plot(thresholds, precisions, label='Precision', linewidth=4)\n    plt1.plot(thresholds, recalls, label='Recall', linewidth=4)\n    plt1.plot(thresholds, f1_scores, label='F1', linewidth=4)\n    plt1.set_ylabel('Scores')\n    plt1.set_xlabel('Probability threshold')\n    plt1.set_title('Probabilities threshold calibration')\n    plt1.legend(bbox_to_anchor=(0.25, 0.25))   \n    plt1.table(cellText = scores_table.values,\n               colLabels = scores_table.columns, \n               colLoc = 'center', cellLoc = 'center', loc = 'bottom', bbox = [0, -1.3, 1, 1])\n\n    plt2 = figure.add_subplot(122)\n    plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 0][:, 0], \n              label='Another class', color='royalblue', alpha=1)\n    plt2.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 1][:, 0], \n              label='Main class', color='darkcyan', alpha=0.8)\n    plt2.set_ylabel('Number of examples')\n    plt2.set_xlabel('Probabilities')\n    plt2.set_title('Probability histogram')\n    plt2.legend(bbox_to_anchor=(1, 1))\n\n    plt.show()\n","57506a6f":"train = pd.read_csv('\/kaggle\/input\/choose-tutors\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/choose-tutors\/test.csv')","805a3d8c":"train['choose'].value_counts()","a1489aa5":"train.info(), test_df.info()","00236a93":"train.describe().T","399be440":"train.hist(figsize =(16,14),bins = 15, grid = True)\nplt.show()","e44efd50":"test_df.hist(figsize =(16,14),bins = 15, grid = True)\nplt.show()","78bc5cd0":"plot_kde_qq_interval(train, 'age')","8e047eb3":"shapiro_samples(train, 'age', 30)","44261261":"shapiro_samples(train, 'age', 30, bxc = True)","5a26e39a":"train['age'], _ = boxcox(train['age'])\ntest_df['age'], _ = boxcox(test_df['age'])","836cdfa5":"plot_qq(train,'age')","15eb1b37":"shapiro_samples(train, 'age', 30)","96165625":"manna_uitny(train, 'age', 30)","0bdf5468":"plot_kde_qq_interval(train, 'lesson_price')","c690c1a8":"shapiro_samples(train, 'lesson_price', 30, bxc = True)","28e0c512":"manna_uitny(train, 'lesson_price', 30)","594c57f3":"plot_kde_qq_interval(train, 'mean_exam_points')","9e84a86d":"shapiro_samples(train, 'mean_exam_points', 30, bxc = True)","ab1db709":"manna_uitny(train, 'mean_exam_points', 30)","69b4bdb1":"sns.boxplot(train['qualification'],train['mean_exam_points'])","6efad624":"train['age_group']=pd.cut(train['age'],[0,40,46,51,100],labels=[1,2,3,4]).astype('float')\ntrain['price_group']=pd.cut(train['lesson_price'],[0,1300,1550,2150,10000],labels=[1,2,3,4]).astype('float')\ntrain['qualification_height']=pd.cut(train['qualification'],[-1,3,5],labels=[0,1]).astype('float')","e9520e49":"test_df['age_group']=pd.cut(test_df['age'],[0,40,46,51,100],labels=[1,2,3,4]).astype('float')\ntest_df['price_group']=pd.cut(test_df['lesson_price'],[0,1300,1550,2150,10000],labels=[1,2,3,4]).astype('float')\ntest_df['qualification_height']=pd.cut(test_df['qualification'],[-1,3,5],labels=[0,1]).astype('float')","09683c58":"# Analogous to SMOTE balancing\ndef balance_df_by_target(df, target_name, balancing_type='oversampling'):\n    target_counts = df[target_name].value_counts()\n\n    major_class_name = target_counts.argmax()\n    minor_class_name = target_counts.argmin()\n    \n    if balancing_type == 'oversampling':\n        \n        disbalance_coeff = int(target_counts[major_class_name] \/ target_counts[minor_class_name]) - 1\n\n        for i in range(disbalance_coeff):\n            sample = df[df[target_name] == minor_class_name].sample(target_counts[minor_class_name])\n            df = df.append(sample, ignore_index=True)\n\n        return df.sample(frac=1) \n    \n    if balancing_type == 'undersampling':\n        \n        major_class_sample = df[df[target_name] == major_class_name].sample(target_counts[minor_class_name])\n        minor_class_sample = df[df[target_name] == minor_class_name]\n        \n        df = pd.concat([major_class_sample, minor_class_sample], ignore_index=True)\n        \n        return df.sample(frac=1) ","9081e6eb":"target = 'choose'\ny = train[target]\nX = train.drop(columns=['Id', 'choose'])\ntest_X = test_df.drop(columns='Id')","518d8710":"df_for_balancing = pd.concat([X, y], axis=1)\ndf_balanced = balance_df_by_target(df_for_balancing, target, balancing_type='oversampling')\n    \ndf_balanced[target].value_counts()","28d1ea16":"\"\"\"df_for_balancing = pd.concat([X, y], axis=1)\ndf_balanced = balance_df_by_target(df_for_balancing, target, balancing_type='undersampling')\n    \ndf_balanced[target].value_counts()\"\"\"","d2904711":"X = df_balanced.drop(columns=target)\ny = df_balanced[target]","6869a55a":"def Standard(data):\n    means = np.mean(data, axis=0)\n    stds = np.std(data, axis=0)\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            data[i][j] = (data[i][j] - means[j])\/stds[j]\n    return data","71ca0586":"X = X.values\ny = y.values\ntest_X = test_X.values","f936edc8":"Standard(X)\nStandard(test_X)","459389be":"class Logistic_regression:\n    def __init__(self, n_iterations=1000, eta=0.05):\n        self.n_iterations = 1000\n        self.eta = eta\n\n    @staticmethod\n    def log_grad(w, c, X, target):\n        m = X.shape[0]\n        y = (2 * target - 1)\n        score = np.dot(X, w.T).flatten()\n        Z = -y \/ (m * (1 + np.exp(y * score)))\n        grad = Z[np.newaxis, :].dot(X)\n        return grad, np.sum(Z)\n\n    @classmethod\n    def optimize(cls, w, c, X, y, n_iterations, eta):\n        for i in range(n_iterations):\n            grad_w, grad_c = cls.log_grad(w, c, X, y)\n            w = w - eta * grad_w\n            c = c - eta * grad_c\n        return w, c\n\n    def fit(self, X, y):\n        self.X = X\n        self.y_true = y\n        w0 = np.zeros((1, X.shape[1]))\n        c0 = 0\n        self.w, self.c = self.optimize(w0, c0, X, y, self.n_iterations, self.eta)\n\n    def predict_proba(self, X):\n        score = X.dot(self.w.T).flatten() + self.c\n        self.score = score\n        return 1 \/ (1 + np.exp(-score))\n\n    def predict(self, X, thr=0.52): # Why threshold was set to this value? Below is a graph that explains this choice\n        proba = self.predict_proba(X)\n        y_predicted = np.zeros(proba.shape, dtype=bool)\n        y_predicted[proba > thr] = 1\n        y_predicted[proba <= thr] = 0\n        self.y_predicted = y_predicted\n        return y_predicted\n    \n    def confusion_matrix(self,X,y):\n        y_pred = self.predict(X)\n        predicted = np.where(y_pred == True, 1, 0)\n        TP=np.sum(np.logical_and(predicted==1, y==1))\n        FP=np.sum(np.logical_and(predicted==1, y==0))\n        FN=np.sum(np.logical_and(predicted==0, y==1))\n        TN=np.sum(np.logical_and(predicted==0, y==0))\n        return np.array([[TP,FP],[FN, TN]])\n         \n        \n    def make_auc_pr(self, X, Y):\n        \n        prob_x = self.predict_proba(X)\n        targets = Y\n        order = np.argsort(prob_x.flatten())\n        targets = targets[order]\n        num_pos=np.sum(targets)\n        num_neg=len(targets)-num_pos\n\n        TPR=np.concatenate([[0], np.cumsum(1-targets)\/(num_neg)])\n        FPR=np.concatenate([[0], np.cumsum(targets)\/(num_pos)])\n        \n        plt.figure(figsize = (16, 7))\n        plt.subplot(1,2,1)\n        AUC_ROC = np.trapz(TPR, x = FPR, dx=0.1)\n        plt.title('ROC curve')\n        plt.ylim(0, 1.05)\n        plt.xlabel('FPR')\n        plt.ylabel('TPR')\n        plt.grid()\n        plt.legend(' ', title=f'AUC-ROC={AUC_ROC:.3f}')\n        plt.plot(FPR, TPR)\n        plt.subplot(1,2,2)\n        precision=np.cumsum(1-targets)\/(np.cumsum(1-targets)+np.cumsum(targets))\n        recall = TPR\n        AUC_PR = np.sum(precision*(TPR[1:]-TPR[:-1]))\n        plt.title('PR curve')\n        plt.ylim(0, 1.05)\n        plt.xlabel('recall')\n        plt.ylabel('presision')\n        plt.grid()\n        plt.legend(' ', title=f'AUC-PR={AUC_PR:.3f}')\n        plt.plot(recall[1:], precision)","962e8c53":"def log_loss(pred, y):\n    return -np.sum(y*np.log(pred)+(1-y)*np.log(1-pred))\/len(y)","62f4a911":"def get_classification_report(y_train_true, y_train_pred, y_test_true, y_test_pred):\n    \"\"\"\n    Outputs F1_score\/precision\/recall in accordance with model operation on training and test datasets + CONFUSION MATRIX\n    \"\"\"\n    print('TRAIN\\n\\n' + classification_report(y_train_true, y_train_pred))\n    print('TEST\\n\\n' + classification_report(y_test_true, y_test_pred))\n    print('CONFUSION MATRIX\\n')\n    print(pd.crosstab(y_test_true, y_test_pred))","30cf3a20":"def tts(X, y, test_size, random_state):\n    np.random.seed(random_state)\n    \n    train_test_cut = int(len(X) * (1 - test_size))\n    \n    shuffle_index = np.random.permutation(X.shape[0])\n    X_shuffled, y_shuffled = X[shuffle_index], y[shuffle_index]\n    \n    X_train, X_test, y_train, y_test = \\\n    X_shuffled[:train_test_cut], \\\n    X_shuffled[train_test_cut:], \\\n    y_shuffled[:train_test_cut], \\\n    y_shuffled[train_test_cut:]\n    \n    return X_train, X_test, y_train, y_test","fda61b35":"X_train, X_test, y_train, y_test = tts(X, y, test_size=0.3, random_state=21)","304dfc74":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","11917a0c":"model = Logistic_regression()\nmodel.fit(X_train, y_train)\n\ny_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\ny_test_pred_probs = model.predict_proba(X_test)\n\nget_classification_report(y_train, y_train_pred, y_test, y_test_pred)","607e3202":"model.make_auc_pr(X_train, y_train)","7d0c9ba0":"model.make_auc_pr(X_test, y_test)","6e0af24e":"pred_proba = model.predict_proba(X_test)\nshow_proba_calibration_plots(pred_proba, y_test)","c33ae5e9":"model.fit(X,y)","ab996acc":"y_pred = model.predict_proba(test_X)","6bafdf22":"submis = pd.concat([test_df['Id'], pd.Series(y_pred)], axis = 1)\nsubmis = submis.rename(columns = {0 : 'choose'})","d73056b0":"submis.to_csv('LogLoss_predict.csv', index=None)","f8380a43":"P-value \u0441\u0438\u043b\u044c\u043d\u043e \u043c\u0435\u043d\u044c\u0448\u0435 alpha, \u0447\u0442\u043e \u0433\u043e\u0432\u043e\u0440\u0438\u0442 \u043e \u0431\u043e\u043b\u044c\u0448\u043e\u0439 \u0440\u0430\u0437\u043d\u0438\u0446\u0435 \u0432 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0438 \u0434\u0432\u0443\u0445 \u0432\u044b\u0431\u043e\u0440\u043e\u043a. \u041c\u043e\u0436\u043d\u043e \u0437\u0430\u043a\u043b\u044e\u0447\u0438\u0442\u044c, \u0447\u0442\u043e \u0434\u0430\u043d\u043d\u044b\u0439 \u043f\u0440\u0438\u0437\u043d\u0430\u043a \u0434\u043e\u0432\u043e\u043b\u044c\u043d\u043e \u043f\u043e\u043b\u0435\u0437\u0435\u043d.\n\nP-value is much smaller than alpha, which indicates a large difference in distribution of two samples(for our alpha).","480d4207":"### Lesson_price ","6d93e707":"### Mean_exam_points \t","a5fe145e":"Let's train model on entire data set and predict probabilities(problem requires predicting probabilities, not class)","584437b3":"### Classification report","90d4a315":"\u0413\u0440\u0430\u0444\u0438\u043a \u043d\u0430\u0433\u043b\u044f\u0434\u043d\u043e \u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0438\u0440\u0443\u0435\u0442 \u0440\u0430\u0437\u043d\u0438\u0446\u0443 \u043c\u0435\u0436\u0434\u0443 \u0440\u0435\u043f\u0435\u0442\u0438\u0442\u043e\u0440\u0430\u043c\u0438, \u043e \u0447\u0435\u043c \u0432 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u043c \u0441\u0432\u0438\u0434\u0435\u0442\u0435\u043b\u044c\u0441\u0442\u0432\u0443\u0435\u0442 \u043a\u0440\u0438\u0442\u0435\u0440\u0438\u0439 \u041c\u0430\u043d\u043d\u0430-\u0423\u0438\u0442\u043d\u0438. \u0420\u0430\u0437\u043d\u0438\u0446\u0430 \u043c\u0435\u0436\u0434\u0443 \u0434\u0432\u0443\u043c\u044f \u0432\u044b\u0431\u043e\u0440\u043a\u0430\u043c\u0438 \u043e\u0447\u0435\u0432\u0438\u0434\u043d\u0430.\n\nGraph demonstrates the difference between tutors and the Mann-Whitney test confirms this.","25fae4a5":"\u0418\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e\u0435 \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u0435.\u0423 \u043a\u0432\u0430\u043b\u0438\u0444\u0438\u043a\u0430\u0446\u0438\u0439 (3,4) \u0443\u0441\u043f\u0435\u0432\u0430\u0435\u043c\u043e\u0441\u0442\u044c \u0432\u044b\u0448\u0435, \u0447\u0435\u043c \u0443 (1,2). \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c 4 \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u0438 \u0443\u0447\u0435\u043d\u0438\u043a\u043e\u0432 \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u043a\u0432\u0430\u0440\u0442\u0438\u043b\u0435\u0439. \u0422\u0430\u043a \u0436\u0435 \u0432\u044b\u0432\u0435\u0434\u0435\u043c \u0433\u0440\u0443\u043f\u043f\u044b \u043f\u043e *'age'* \u0438 *'price'*.\n\nQualifications (3,4) have higher academic performance than those (1,2).Let's create 4 groups of students depending on quartiles. Also make groups by *'age'* and *'price'*.","41b47c26":"### \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \/ Let's look at distribution","f999c17c":"Let's look at Q-Q plots","a998568f":"##### Undersampling","3c834d26":"#### Train","32cf22d0":"#### Test","23164213":"### \u0421\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u0438\u0437\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \/ Standardization","bc5d9a5e":"Big imbalance in target variable","366f3b0b":"##### Ovarsampling","4ffca739":"\u0412 \u0434\u0430\u043d\u043d\u043e\u043c \u0440\u0435\u0449\u0435\u043d\u0438\u0438 \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c\u0441\u044f \u043b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0439 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0435\u0439.\n\nIn this solution i use standard logistic regression.","ab62fabf":"### Log_Loss error","2dd69fe1":"\u041f\u0440\u043e\u0432\u0435\u0440\u0438\u043c \u043d\u0430 \"\u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c\"(\u0413\u0430\u0443\u0441\u0441\u043e\u0432\u043e) \u043f\u043e \u0442\u0435\u0441\u0442\u0443 \u0428\u0430\u043f\u0438\u0440\u043e-\u0423\u0438\u043b\u043a\u0430 \u0441 \u0434\u043e\u0432\u0435\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c\u044e 95%\n\nCheck for \"normality\" (Gaussian) by the Shapiro-Wilk test","db1a2f88":"We did not get a normal distribution, so let's use nonparametric criteria","84bd30d7":"### \u0421\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u043d\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \/ Let's generate some new features","7cfc8cef":"p > alpha - distributions are not very different \/ \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043d\u0435 \u0441\u0438\u043b\u044c\u043d\u043e \u0440\u0430\u0437\u043b\u0438\u0447\u0430\u044e\u0442\u0441\u044f","f33dd9ad":"### \u0411\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u043a\u0430 \/ Balancing","6e806d64":"### \u0423\u0434\u0435\u043b\u0438\u043c \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u043e\u0435 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435 \u0447\u0438\u0441\u043b\u043e\u0432\u044b\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0430\u043c \/ Let's make special attention to numerical features\n### Age","20afb197":"\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0435 \u0434\u043e\u0442\u044f\u0433\u0438\u0432\u0430\u0435\u0442 \u0434\u043e \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e, \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0411\u043e\u043a\u0441\u0430-\u041a\u043e\u043a\u0441\u0430 \n\nLet's try the Box-Cox transformation","56e56197":"### Train_test_split","4b0b1286":"## Log_Loss model\n"}}