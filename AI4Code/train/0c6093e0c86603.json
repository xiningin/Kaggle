{"cell_type":{"b8b230c0":"code","c1d6f77b":"code","ded30414":"code","3bd8ec3f":"code","b8b25dd4":"code","ca7247ed":"code","6072146d":"code","cbd6297f":"code","958b17b4":"code","a6d1225c":"code","685b843a":"code","070e1d68":"code","5ed0db21":"code","759049da":"code","499df80d":"code","e6b4d1aa":"code","cb087a5d":"code","77eeb7e4":"code","7d2b9719":"code","5b2acf8f":"code","2c950f6d":"code","380c3e49":"code","f4c0c7d8":"code","d4f4be05":"code","be85af25":"code","fae0f745":"code","4f66e797":"code","2ddb0c08":"code","724a05de":"code","43db9b39":"code","4761c51d":"code","d9575ce9":"code","b018358c":"code","d1de135e":"code","bc94e4e5":"code","56020e94":"code","efaa9ca5":"code","d044a71e":"code","1aacc471":"code","654f3c9e":"code","a7765523":"code","a241cba0":"code","d4754596":"markdown","edda3f02":"markdown","5584d1bc":"markdown","3b3cae05":"markdown","f67d9b4a":"markdown","b3a4e28a":"markdown","5ebf58bd":"markdown","1b5fb668":"markdown","dfcb9991":"markdown","3d692735":"markdown","a9fbe2ae":"markdown","a8ba599c":"markdown","5b7402fa":"markdown","292aee62":"markdown","7b738e04":"markdown","928979e7":"markdown","0b97d674":"markdown","7457556b":"markdown","31855eed":"markdown","f1301fd4":"markdown","cc4f9641":"markdown","8018064a":"markdown","1b53455c":"markdown","bc404f5d":"markdown","6f9ed4ca":"markdown","9174c13a":"markdown","b54774cc":"markdown","0eab2122":"markdown","edf83c14":"markdown","19274e9d":"markdown","502dbcca":"markdown","5ed76cf6":"markdown","7d4a2d87":"markdown","765b7105":"markdown","5c31b470":"markdown","b7d5cb80":"markdown","58860317":"markdown","2dbf4a6b":"markdown","6b4b8890":"markdown","bf8a91cc":"markdown","dbe2b818":"markdown","6098d34c":"markdown","980b1851":"markdown","6da7d9a2":"markdown","c89522a6":"markdown","a008789e":"markdown","64d1b65e":"markdown","f24d1b4e":"markdown","c083fb4b":"markdown","262ed7a7":"markdown","7909c6fc":"markdown","acdbe9ff":"markdown","8014c902":"markdown"},"source":{"b8b230c0":"!cp -r ..\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset \/kaggle\/working\n!pip install timm\n!pip install -q imutils","c1d6f77b":"CFG = {\n    'fold_num': 3,\n    'seed': 125,\n    'img_size': 224, \n    'train_bs': 128,\n    'valid_bs': 128,\n    'weight_decay':1e-6,\n    'model_arch': 'gluon_resnet50_v1b', \n    'epochs': 10,\n    'lr': 0.0001,\n    'num_workers': 4,\n    'accum_iter': 1,\n    'fold_num': 3,\n    'accelerator': 'GPU', \n    'num_classes': 9,\n    'grad_clip': 0.001\n}","ded30414":"if CFG['accelerator']=='TPU':\n    !curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n    !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev\n    import torch\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.debug.metrics as met\n    import torch_xla.distributed.parallel_loader as pl\n    import torch_xla.distributed.xla_multiprocessing as xmp\n    import torch_xla.utils.utils as xu\n    import os\n    os.environ[\"XLA_USE_BF16\"] = \"1\"\n    os.environ[\"XLA_TENSOR_ALLOCATOR_MAXSIZE\"] = \"100000000\"\n    device = xm.xla_device()\n    \nif CFG['accelerator']=='GPU':\n    import torch\n    device = torch.device('cuda:0')\n    print(torch.cuda.is_available())","3bd8ec3f":"from glob import glob\nimport joblib\nimport sklearn\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom sklearn import metrics\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom skimage import io\n\nfrom datetime import datetime\n#import torchvision\n#from torchvision import transforms\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\n#from torchvision.utils import make_grid \nfrom torch import nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data\nfrom torchvision import datasets\nfrom torchvision import transforms\nimport os\nfrom os.path import join\nfrom os import listdir, rmdir\nfrom shutil import move\nimport random\nfrom operator import itemgetter\nimport copy\nimport time\nimport timm\n\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nimport pydicom\nfrom scipy.ndimage.interpolation import zoom\nfrom scipy.cluster.vq import kmeans,whiten\nfrom scipy.stats import zscore\nfrom operator import itemgetter\n\nimport optuna\nfrom optuna.trial import TrialState\nimport shutil\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport cv2\nimport imutils\nimport gc\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b8b25dd4":"input_path = '.\/Fish_Dataset'","ca7247ed":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(CFG['seed'])","6072146d":"for i in os.listdir(input_path):\n    if os.path.isdir(input_path+'\/'+i)==False:\n        os.remove(input_path+'\/'+i)\n    else:\n        for x in os.listdir(input_path+'\/'+i): \n            if x.split()[-1]=='GT':\n                shutil.rmtree(input_path+'\/'+i+'\/'+x)","cbd6297f":"for folders in os.listdir(input_path):\n    root = input_path+'\/'+folders+'\/'\n    for filename in listdir(join(root, folders)):\n        move(join(root, folders, filename), join(root, filename))\n    rmdir(join(root, folders))","958b17b4":"data = {'image_id': [], 'labels': []}\n\nfor folders in os.listdir(input_path):\n    for files in os.listdir(input_path+'\/'+folders):\n        data['image_id'].append (input_path+'\/'+folders+'\/'+files)\n        data['labels'].append(folders)\n        \nall_data = pd.DataFrame.from_dict(data)\nall_data","a6d1225c":"train, test = train_test_split(all_data, test_size=0.3, random_state=CFG['seed'], shuffle=True)","685b843a":"fig = go.Figure(\n    data=[ go.Bar(x=train['labels'].value_counts().index, \n            y=train['labels'].value_counts().values,\n            text=train['labels'].value_counts().values,\n            textposition='auto',name='Count',\n           marker_color='indianred')],\n    layout_title_text=\"Class distribution\"\n)\nfig.show()","070e1d68":"for idx in tqdm(train.index):\n    img_name = train.loc[idx,'image_id']\n    #reading the image and converting BGR color space to RGB\n    img = cv2.cvtColor(cv2.imread(img_name), cv2.COLOR_BGR2RGB)\n    \n    #normalize the image in the range [0,1]\n    norm_image = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n    \n    width,height,depth = img.shape\n    \n    #adding new column to the tabel with width height and aspect ratio for every image\n    train.loc[idx,'Width'] = width\n    train.loc[idx,'Height'] = height\n    train.loc[idx,'Aspect Ratio'] = width\/height\n    \n    #calculate mean and standart deviation for each image\n    train.loc[idx,'Mean'] = img.mean()\n    train.loc[idx,'SD'] = img.std()\n    \n    #calculate mean and standart deviation for each normalized image\n    train.loc[idx,'Normalized_Mean'] = norm_image.mean()\n    train.loc[idx,'Normalized_SD'] = norm_image.std()\ntrain","5ed0db21":"fig =  make_subplots(rows=2,cols=1,subplot_titles=['Original Image', 'Normalized Image'])\ncolors = ['rgb({}, {}, {})'.format(random.randint(0,255), random.randint(0,255), random.randint(0,255)) for iter in range(len(train['labels'].unique()))]\nfor idx,class_name in enumerate(train['labels'].unique()):\n    #scatter plot between mean and variance of the images for every disease\n    fig.add_trace(go.Scatter(x=train[train['labels'] == class_name]['Mean'],\n                             y=train[train['labels'] == class_name]['SD'],\n                            mode = 'markers',name=class_name, marker_color=colors[idx]),1,1)\n    \n    #scatter plot between mean and variance of the normalized images for every disease\n    fig.add_trace(go.Scatter(x=train[train['labels'] == class_name]['Normalized_Mean'],\n                             y=train[train['labels'] == class_name]['Normalized_SD'],\n                            mode = 'markers',name=class_name, marker_color=colors[idx], showlegend=False),2,1)\n#x-axis and y axis title\nfig.update_xaxes(title_text=\"Mean\", row=1, col=1)\nfig.update_yaxes(title_text=\"Standard Deviation\", row=1, col=1)\n\nfig.update_xaxes(title_text=\"Mean\", row=2, col=1)\nfig.update_yaxes(title_text=\"Standard Deviation\", row=2, col=1)\nfig.show()","759049da":"fig = make_subplots(rows=2,cols=2,\n                    subplot_titles=['Mean','Standard Deviation','Normalized Mean','Normalized Standard Deviation'],\n                    shared_xaxes=True)\ncolors = ['rgb({}, {}, {})'.format(random.randint(0,255), random.randint(0,255), random.randint(0,255)) for iter in range(len(train['labels'].unique()))]\nfor idx,class_name in enumerate(train['labels'].unique()):\n    fig.add_trace(go.Box(y=train[train['labels'] == class_name]['Mean'],\n                        name=class_name,showlegend=False,\n                        marker_color=colors[idx]),1,1)\n    fig.add_trace(go.Box(y=train[train['labels'] == class_name]['Normalized_Mean'],\n                        name=class_name,showlegend=False,\n                        marker_color=colors[idx]),2,1)\n    fig.add_trace(go.Box(y=train[train['labels'] == class_name]['SD'],\n                        name=class_name,showlegend=False,\n                        marker_color=colors[idx]),1,2)\n    fig.add_trace(go.Box(y=train[train['labels'] == class_name]['Normalized_SD'],\n                        name=class_name,showlegend=False,\n                        marker_color=colors[idx]),2,2)\nfig.update_layout(title='Outlier Detection - Box Plot')\nfig.show()","499df80d":"def calculate_fences(array):\n    upper_fence = array.describe()['75%']+1.5*(array.describe()['75%']-array.describe()['25%'])\n    lower_fence = array.describe()['25%']-1.5*(array.describe()['75%']-array.describe()['25%'])\n    return lower_fence, upper_fence","e6b4d1aa":"outliers = set()\n\nfor species in train['labels'].unique():\n    images  = train[train['labels'] ==  species]\n    \n    data_for_fences_mean = train[train['labels']==species].loc[:, 'Normalized_Mean']\n    data_for_fences_sd = train[train['labels']==species].loc[:, 'Normalized_SD']\n    \n    outliers_mean = images[images['Normalized_Mean'].between(calculate_fences(data_for_fences_mean)[0],calculate_fences(data_for_fences_mean)[1],inclusive=True)]\n    outliers_mean = images[~images['image_id'].isin(outliers_mean['image_id'])]\n    \n    outliers_st = images[images['Normalized_SD'].between(calculate_fences(data_for_fences_sd)[0],calculate_fences(data_for_fences_sd)[1],inclusive=True)]\n    outliers_st = images[~images['image_id'].isin(outliers_st['image_id'])]\n    \n    delete_mean = set(outliers_mean['image_id'].astype(str).values)\n    delete_st= outliers_st['image_id'].astype(str).values\n    outliers = outliers.union(delete_mean)\n    outliers = outliers.union(delete_st)\n    \nprint('Number of Outliers:', len(outliers))","cb087a5d":"imgs = list(outliers)\n\ngridimg = []        \nfor idx,img_name in enumerate(np.random.choice(imgs,25,replace=False)):\n    np_image = mpimg.imread(img_name)\n    gridimg.append(np_image)\n\nfig = plt.figure(figsize=(25, 25))\ngrid = ImageGrid(fig, 111,  \n                 nrows_ncols=(5, 5),  \n                 axes_pad=0.1,  \n                 label_mode=\"1\")\n\nfor ax, im in zip(grid, gridimg):\n    # Iterating over the grid returns the Axes.\n    ax.imshow(im)\n    ax.axis('off')\n\nplt.show()","77eeb7e4":"for images in outliers:\n    os.remove(images)\n    train = train.drop(train[train['image_id']==images].index)","7d2b9719":"train","5b2acf8f":"# create an dict to store all k_means_colr values for all the images with flatten structure\nk_means_cluster_colors = {i: [] for i in os.listdir(input_path)}\nimages = {i: [] for i in os.listdir(input_path)}\n\ndef k_means_cluster(classes):\n    df = train[train['labels'] == classes]\n    for idx in df.index:\n        img_name = train.loc[idx,'image_id']\n        img = cv2.cvtColor(cv2.imread(img_name), cv2.COLOR_BGR2RGB)\n        img = imutils.resize(img,height=150)\n        \n        #normalize the given image\n        norm_img = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n        images[class_name].append(img_name) \n  \n        #k-means chstering with cluster size of 5\n        cluster_centers, distortion = kmeans(norm_img.reshape((-1,3)),5)\n\n        #standard deviation for each color band\n        std = np.expand_dims(img.reshape((-1,3)).std(axis=0),1) \n        \n        if cluster_centers.shape[0]==5:\n            k_means_cluster_colors[class_name].append((np.matmul(cluster_centers,std).T).astype(int)[0])\n        else:\n            k_means_cluster_colors[classes]=[]\n            return k_means_cluster(classes)\n\nfor class_name in tqdm(train['labels'].unique()):\n    k_means_cluster(class_name)","2c950f6d":"k_means_outliers = {i: [] for i in os.listdir(input_path)}\n\nfor idx,class_name in enumerate(k_means_cluster_colors):\n    x = np.sum(k_means_cluster_colors[class_name],axis=1)\n    z_score = zscore(x)\n    k_means_outliers[class_name].append(list(np.where((z_score>3))[0]) + list(np.where((z_score<-3))[0]))","380c3e49":"rows = 3\ncols = 3\nfig =  make_subplots(rows=rows,cols=cols,subplot_titles=list(k_means_cluster_colors.keys()))\n#fig =  make_subplots(rows=3,cols=3,subplot_titles=['Original Image', 'Normalized Image'])\ncolors = ['rgb({}, {}, {})'.format(random.randint(0,255), random.randint(0,255), random.randint(0,255)) for iter in range(len(train['labels'].unique()))]\nclasses = 0\n\nfor row in range(rows):\n    for col in range(cols):\n        x = np.sum(k_means_cluster_colors[list(k_means_cluster_colors.keys())[classes]],axis=1)\n        z_score = zscore(x)\n        #fig=go.Figure()\n        fig.add_trace(go.Histogram(x=z_score, marker_color=colors[classes]), row+1, col+1)\n        classes+=1\n    \nfig.update_layout(height=600, width=800, title_text=\"Z-Score\")\nfig.show()","f4c0c7d8":"imgs = []\nfor class_name in k_means_outliers:\n    for idx in k_means_outliers[class_name][0]:\n        imgs.append(images[class_name][idx])\n        \ngridimg = []        \nfor idx,img_name in enumerate(np.random.choice(imgs,8,replace=False)):\n    np_image = mpimg.imread(img_name)\n    gridimg.append(np_image)\n\nfig = plt.figure(figsize=(25, 25))\ngrid = ImageGrid(fig, 111,  # similar to subplot(111)\n                 nrows_ncols=(2, 4),  # creates 2x2 grid of axes\n                 axes_pad=0.1,  # pad between axes in inch.\n                 label_mode=\"1\")\n\nfor ax, im in zip(grid, gridimg):\n    # Iterating over the grid returns the Axes.\n    ax.imshow(im)\n    ax.axis('off')\n\nplt.show()","d4f4be05":"for images in imgs:\n    os.remove(images)\n    train = train.drop(train[train['image_id']==images].index)","be85af25":"train","fae0f745":"def init_logger(log_file='.\/'+'train.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nLOGGER = init_logger()","4f66e797":"def get_img(path):\n    im_bgr = cv2.imread(path)\n    im_rgb = im_bgr[:, :, ::-1]\n    return im_rgb","2ddb0c08":"class CustomDataset(Dataset):\n    def __init__(self, df, data_root, \n                 transforms=None, \n                 output_label=True, \n                 one_hot_label=False):\n        \n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms = transforms\n        self.data_root = data_root\n        \n        self.output_label = output_label\n        self.one_hot_label = one_hot_label\n        \n        if output_label == True:\n            self.labels = self.df['labels'].values\n            #print(self.labels)\n            \n            if one_hot_label is True:\n                self.labels = np.eye(self.df['labels'].max()+1)[self.labels]\n                #print(self.labels)\n            \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        \n        # get labels\n        if self.output_label:\n            target = self.labels[index]\n          \n        img  = get_img(\"{}\".format(self.df.loc[index]['image_id']))\n        if self.transforms:\n            img = self.transforms(image=img)['image']\n        \n        if self.output_label == True:\n            return img, target\n        else:\n            return img","724a05de":"from albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)\nfrom albumentations.pytorch import ToTensorV2\n\ndef get_transforms(data):\n    \n    if data == 'train':\n        return Compose([\n            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            CoarseDropout(p=0.5),\n            Cutout(p=0.5),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n\n    elif data == 'val':\n        return Compose([\n            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n            Resize(CFG['img_size'], CFG['img_size']),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)","43db9b39":"# Function for plotting samples\ndef plot_samples(samples):  \n    fig, ax = plt.subplots(nrows=5, ncols=5, figsize=(15,12))\n    i = 0\n    for row in range(5):\n         for col in range(5):\n                img = mpimg.imread(samples[i][0][0])\n                ax[row][col].imshow(img)\n                ax[row][col].axis('off')\n                ax[row][col].set_title(samples[i][1], fontsize=15)\n                i+=1\n  \nexample = all_data\n\nrand_samples = [] \nfor _ in range(25): \n#     rand_samples.append([random.sample([os.path.join(input_path+'\/'+str(classes), str(filename)) for filename in os.listdir(input_path)], 1), classes]) \n    sample = random.sample(list(train['image_id']), 1)\n    rand_samples.append([sample, train[train['image_id']==sample[0]]['labels'].values[0]])\nrand_samples[0]\nplot_samples(rand_samples)\nplt.suptitle('Samples', fontsize=30)\nplt.show()","4761c51d":"le = LabelEncoder()\nsets = {'train':train, 'test':test}\nfor x in ['train', 'test']:\n    sets[x]['labels'] = le.fit_transform(sets[x].labels.values)","d9575ce9":"def prepare_dataloader(df, trn_idx, val_idx, data_root='.\/Fish_Dataset'):\n\n    train_ = df.reset_index().loc[trn_idx,:].reset_index(drop=True)\n    valid_ = df.reset_index().loc[val_idx,:].reset_index(drop=True)\n        \n    train_ds = CustomDataset(train_, data_root, transforms=get_transforms(data='train'), output_label=True, one_hot_label=False)\n    valid_ds = CustomDataset(valid_, data_root, transforms=get_transforms(data='val'), output_label=True)\n    \n    if CFG['accelerator']=='TPU':\n        \n        train_sampler = torch.utils.data.distributed.DistributedSampler(\n            train_ds,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=True\n    )\n\n        valid_sampler = torch.utils.data.distributed.DistributedSampler(\n            valid_ds,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=True\n    )\n    \n        train_loader = torch.utils.data.DataLoader(\n            train_ds,\n            batch_size=CFG['train_bs'],\n            pin_memory=True,\n            drop_last=False,\n            sampler=train_sampler,\n            num_workers=CFG['num_workers'],\n        #sampler=BalanceClassSampler(labels=train_['label'].values, mode=\"downsampling\")\n    )\n        val_loader = torch.utils.data.DataLoader(\n            valid_ds, \n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            sampler=valid_sampler,\n            pin_memory=True,\n    )\n        \n    if CFG['accelerator']=='GPU':\n        \n        train_loader = torch.utils.data.DataLoader(\n            train_ds,\n            batch_size=CFG['train_bs'],\n            pin_memory=True,\n            drop_last=False,\n            shuffle=True,        \n            num_workers=CFG['num_workers']\n    )\n        val_loader = torch.utils.data.DataLoader(\n            valid_ds, \n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=True\n    )\n    \n    \n    return train_loader, val_loader, len(train_ds), len(valid_ds)","b018358c":"from torch import autograd\n\ndef train_one_epoch(loader, epoch, model, optimizer, criterion, gradient_clipping=False):\n   \n   LOGGER.info('--------------------------------------------------------------')\n   LOGGER.info('Epoch: {}\/{}'.format(epoch+1, CFG['epochs']))\n    \n   model.train()\n\n   running_loss = 0.\n   running_corrects = 0.\n    \n   pbar = tqdm(enumerate(loader), total=len(loader))\n   for step, (imgs, targets) in pbar:\n        \n      imgs, targets = imgs.to(device).float(), targets.to(device).long()\n      if CFG['accelerator']=='GPU':\n        with autocast():\n            output = model(imgs)\n            loss = criterion(output, targets)\n            loss = loss \/ CFG['accum_iter']\n        scaler.scale(loss).backward()\n            \n            \n      if CFG['accelerator']=='TPU':\n        output = model(imgs)\n        gc.collect()\n        loss = criterion(output, targets)\n        loss = loss \/ CFG['accum_iter']\n        loss.backward()\n      \n      if gradient_clipping:\n        timm.utils.agc.adaptive_clip_grad(model.parameters(), clip_factor=CFG['grad_clip'], eps=1e-3, norm_type=2.0)\n        \n      _, pred = output.max(dim=1)\n      running_corrects += torch.sum(pred == targets.data)\n      running_loss += loss.item()*imgs.size(0)\n    \n      if loss.item()==float(\"NaN\"):\n            xm.master_print('0'*50)\n            break\n            \n      description = f'train | epoch {epoch+1} | current loss: {loss.item()*imgs.size(0):.4f}'\n      pbar.set_description(description)\n    \n      if ((step + 1) % CFG['accum_iter'] == 0) or ((step + 1) == len(loader)):\n            \n            if CFG['accelerator']=='GPU':\n                scaler.step(optimizer)\n                scaler.update()\n                \n            if CFG['accelerator']=='TPU':\n                xm.optimizer_step(optimizer)\n                \n            optimizer.zero_grad()\n   return running_corrects, running_loss","d1de135e":"def valid_one_epoch(loader,epoch, model, device, criterion):\n    model.eval()\n    \n    running_loss = 0.\n    running_corrects = 0.\n    \n    pbar = tqdm(enumerate(loader), total=len(loader))\n    for step, (imgs, targets) in pbar:\n       imgs, targets = imgs.to(device).float(), targets.to(device).long()\n       with torch.no_grad():\n             output = model(imgs)\n             gc.collect()\n             _, pred = output.max(dim=1)\n             loss = criterion(output, targets)\n       running_corrects += torch.sum(pred == targets.data)\n       running_loss += loss.item()*imgs.size(0)\n       gc.collect()\n    \n       description = f'val | epoch {epoch+1} | current loss: {loss.item()*imgs.size(0):.4f}'\n       pbar.set_description(description)\n                \n    return running_corrects, running_loss","bc94e4e5":"def fit(seed, epochs, model, freeze, device, fold, train_loader, val_loader, criterion, gradient_clipping=False):\n  \n  if CFG['accelerator']=='TPU':\n          LOGGER.info('Creating a model {}...'.format(seed))\n          #device = xm.xla_device()\n          WRAPPED_MODEL = xmp.MpModelWrapper(model)\n          model = WRAPPED_MODEL.to(device)\n          #model.to(device)\n          if freeze==True:\n            if seed==1:\n              optimizer = torch.optim.Adam(model.head.parameters(), lr=CFG['lr']* xm.xrt_world_size(), betas=(0.9, 0.999), eps=1e-08, weight_decay=CFG['weight_decay'])\n            if seed==2 or seed==3:\n              optimizer = torch.optim.Adam(model.fc.parameters(), lr=CFG['lr']* xm.xrt_world_size(), betas=(0.9, 0.999), eps=1e-08, weight_decay=CFG['weight_decay'])\n            if seed==4 or seed==0:\n              optimizer = torch.optim.Adam(model.classifier.parameters(), lr=CFG['lr']* xm.xrt_world_size(), betas=(0.9, 0.999), eps=1e-08, weight_decay=CFG['weight_decay'])\n          if freeze==False:\n            optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr']* xm.xrt_world_size(), betas=(0.9, 0.999), eps=1e-08, weight_decay=CFG['weight_decay'])\n        \n  if CFG['accelerator'] == 'GPU':\n    LOGGER.info('Creating a model...')\n    model.to(device)\n    if freeze==True:\n        if seed==1:\n            optimizer = torch.optim.Adam(model.head.parameters(), lr=CFG['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=CFG['weight_decay'])\n        if seed==2 or seed==3:\n            optimizer = torch.optim.Adam(model.fc.parameters(), lr=CFG['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=CFG['weight_decay'])\n        if seed==4 or seed==0:\n            optimizer = torch.optim.Adam(model.classifier.parameters(), lr=CFG['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=CFG['weight_decay'])           \n    if freeze==False:\n       optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], betas=(0.9, 0.999), eps=1e-08, weight_decay=CFG['weight_decay'])\n    \n  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.1)\n  since = time.time()\n  best_model = copy.deepcopy(model.state_dict())\n  best_acc = 0.0\n  \n  if CFG['accelerator']=='TPU':\n     if freeze==True:\n                                         \n        for fase in ['CLASSIFIER TRAINING', 'ALL NET TRAINING']:\n                                         \n            if fase=='ALL NET TRAINING':             \n               model.load_state_dict(best_model)\n               for param in model.parameters():\n                   param.requires_grad=True\n               optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr']* xm.xrt_world_size()\/10, betas=(0.9, 0.999), eps=1e-08, weight_decay=CFG['weight_decay'])  \n               scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.1)     \n                                         \n            for epoch in range(epochs):\n                                         \n                #train\n                para_loader = pl.ParallelLoader(train_loader, [device])\n                running_corrects, running_loss = train_one_epoch(para_loader.per_device_loader(device), epoch, model, optimizer, criterion, gradient_clipping)\n                epoch_loss = running_loss \/ train_len\n                epoch_acc = running_corrects\/ train_len\n                all_accuracies['fold_{}'.format(fold+1)][0].append(epoch_acc) \n                all_losses['fold_{}'.format(fold+1)][0].append(epoch_loss) \n                LOGGER.info('{} - loss:{}, accuracy:{}'.format('train', epoch_loss, epoch_acc))\n                del para_loader\n                gc.collect()\n                \n                #val\n                para_loader = pl.ParallelLoader(val_loader, [device])\n                running_corrects, running_loss = valid_one_epoch(para_loader.per_device_loader(device), epoch, model, device, criterion)\n                epoch_loss = running_loss \/ val_len\n                epoch_acc = running_corrects\/ val_len\n                all_accuracies['fold_{}'.format(fold+1)][1].append(epoch_acc) \n                all_losses['fold_{}'.format(fold+1)][1].append(epoch_loss)\n                LOGGER.info('{} - loss:{}, accuracy:{}'.format('val', epoch_loss, epoch_acc))\n                del para_loader\n                gc.collect()\n                   \n                #epoch end\n                LOGGER.info('Time: {}m {}s'.format((time.time()- since)\/\/60, (time.time()- since)%60))\n                LOGGER.info('=='*31)\n                if epoch_acc >= best_acc:\n                  best_acc = epoch_acc\n                  best_model = copy.deepcopy(model.state_dict())\n                scheduler.step()\n      \n            time_elapsed = time.time() - since\n            LOGGER.info('{} TIME {}m {}s'.format(fase, time_elapsed\/\/60, time_elapsed%60))\n            LOGGER.info('=='*31)\n                                         \n     if freeze==False:\n        for epoch in range(epochs):\n                                         \n                #train\n                para_loader = pl.ParallelLoader(train_loader, [device])\n                running_corrects, running_loss = train_one_epoch(para_loader.per_device_loader(device), epoch, model, optimizer, criterion, gradient_clipping)\n                epoch_loss = running_loss \/ train_len\n                epoch_acc = running_corrects\/ train_len\n                all_accuracies['fold_{}'.format(fold+1)][0].append(epoch_acc) \n                all_losses['fold_{}'.format(fold+1)][0].append(epoch_loss) \n                LOGGER.info('{} - loss:{}, accuracy:{}'.format('train', epoch_loss, epoch_acc))\n                del para_loader\n                gc.collect()\n                \n                \n                #val\n                para_loader = pl.ParallelLoader(val_loader, [device])\n                running_corrects, running_loss = valid_one_epoch(para_loader.per_device_loader(device), epoch, model, device, criterion)\n                epoch_loss = running_loss \/ val_len\n                epoch_acc = running_corrects\/ val_len\n                all_accuracies['fold_{}'.format(fold+1)][1].append(epoch_acc) \n                all_losses['fold_{}'.format(fold+1)][1].append(epoch_loss) \n                LOGGER.info('{} - loss:{}, accuracy:{}'.format('val', epoch_loss, epoch_acc))\n                del para_loader\n                gc.collect()\n    \n                #epoch end\n                LOGGER.info('Time: {}m {}s'.format((time.time()- since)\/\/60, (time.time()- since)%60))\n                LOGGER.info('=='*31)\n                if epoch_acc >= best_acc:\n                  best_acc = epoch_acc\n                  best_model = copy.deepcopy(model.state_dict())\n                scheduler.step()\n      \n        time_elapsed = time.time() - since\n        LOGGER.info('FOLD_{} TRAINING TIME {}m {}s'.format(fold+1, time_elapsed\/\/60, time_elapsed%60))\n        LOGGER.info('=='*31) \n                                         \n                                         \n  if CFG['accelerator']=='GPU':\n     if freeze==True:\n                                         \n        for fase in ['CLASSIFIER TRAINING', 'ALL NET TRAINING']:\n                                         \n            if fase=='ALL NET TRAINING':             \n               model.load_state_dict(best_model)\n               for param in model.parameters():\n                   param.requires_grad=True\n               optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr']\/10, betas=(0.9, 0.999), eps=1e-08, weight_decay=CFG['weight_decay'])  \n               scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 4, gamma=0.1)     \n                                         \n            for epoch in range(CFG['epochs']):\n                                         \n                #train                           \n                running_corrects, running_loss = train_one_epoch(train_loader, epoch, model, optimizer, criterion, gradient_clipping)\n                epoch_loss = running_loss \/ train_len\n                epoch_acc = running_corrects\/ train_len\n                all_accuracies['fold_{}'.format(fold+1)][0].append(epoch_acc) \n                all_losses['fold_{}'.format(fold+1)][0].append(epoch_loss) \n                LOGGER.info('{} - loss:{}, accuracy:{}'.format('train', epoch_loss, epoch_acc))\n                \n                #val\n                running_corrects, running_loss = valid_one_epoch(val_loader, epoch, model, device, criterion)\n                epoch_loss = running_loss \/ val_len\n                epoch_acc = running_corrects\/ val_len\n                all_accuracies['fold_{}'.format(fold+1)][1].append(epoch_acc) \n                all_losses['fold_{}'.format(fold+1)][1].append(epoch_loss) \n                LOGGER.info('{} - loss:{}, accuracy:{}'.format('val', epoch_loss, epoch_acc))\n                \n                #epoch end\n                LOGGER.info('Time: {}m {}s'.format((time.time()- since)\/\/60, (time.time()- since)%60))\n                LOGGER.info('=='*31)\n                if epoch_acc >= best_acc:\n                  best_acc = epoch_acc\n                  best_model = copy.deepcopy(model.state_dict())\n                scheduler.step()\n      \n            time_elapsed = time.time() - since\n            LOGGER.info('{} TIME {}m {}s'.format(fase, time_elapsed\/\/60, time_elapsed%60))\n            LOGGER.info('=='*31)\n                                         \n     if freeze == False :\n        for epoch in range(epochs):\n                \n                #train                           \n                running_corrects, running_loss = train_one_epoch(train_loader, epoch, model, optimizer, criterion, gradient_clipping)\n                epoch_loss = running_loss \/ train_len\n                epoch_acc = running_corrects\/ train_len\n                all_accuracies['fold_{}'.format(fold+1)][0].append(epoch_acc) \n                all_losses['fold_{}'.format(fold+1)][0].append(epoch_loss) \n                LOGGER.info('{} - loss:{}, accuracy:{}'.format('train', epoch_loss, epoch_acc))\n                \n                #val                             \n                running_corrects, running_loss = valid_one_epoch(val_loader, epoch, model, device, criterion)\n                epoch_loss = running_loss \/ val_len\n                epoch_acc = running_corrects\/ val_len\n                all_accuracies['fold_{}'.format(fold+1)][1].append(epoch_acc) \n                all_losses['fold_{}'.format(fold+1)][1].append(epoch_loss) \n                LOGGER.info('{} - loss:{}, accuracy:{}'.format('val', epoch_loss, epoch_acc))\n                \n                #epoch end\n                LOGGER.info('Time: {}m {}s'.format((time.time()- since)\/\/60, (time.time()- since)%60))\n                LOGGER.info('=='*31)\n                if epoch_acc >= best_acc:\n                  best_acc = epoch_acc\n                  best_model = copy.deepcopy(model.state_dict())\n                scheduler.step()\n      \n        time_elapsed = time.time() - since\n        LOGGER.info('FOLD_{} TRAINING TIME {}m {}s'.format(fold+1, time_elapsed\/\/60, time_elapsed%60))\n        LOGGER.info('=='*31) \n                                         \n  if CFG['accelerator']=='TPU':\n     xm.save(best_model,'{}_fold_{}.pth'.format(CFG['model_arch'], fold+1))\n  if CFG['accelerator']=='GPU':\n     torch.save(best_model,'{}_fold_{}.pth'.format(CFG['model_arch'], fold+1))\n  \n  LOGGER.info('Prediction Saved! \\n')","56020e94":"CFG","efaa9ca5":"if __name__ == '__main__':\n    \n    folds = StratifiedKFold(n_splits=CFG['fold_num'], shuffle=True, random_state=CFG['seed']).split(np.arange(train.shape[0]), train.labels.values)\n    \n    fold_best_acc = []\n    \n    all_losses = {f'fold_{x+1}':[[],[]] for x in range(CFG['fold_num'])}\n    all_accuracies = {f'fold_{x+1}':[[],[]] for x in range(CFG['fold_num'])}\n    \n    for fold, (trn_idx, val_idx) in enumerate(folds):\n        \n        #we'll train fold 0 first\n        if fold > 0:\n            break \n            \n        LOGGER.info('\\n\\n')\n        LOGGER.info(f'========== fold: {fold+1} training ==========')\n        LOGGER.info('Accelerator: {}'.format(CFG['accelerator']))\n        LOGGER.info(f'Device: {device}')\n            \n        train_loader, val_loader, train_len, val_len = prepare_dataloader(train, trn_idx, val_idx, data_root='.\/Fish_Dataset')\n        model = timm.create_model(CFG['model_arch'], pretrained=True, num_classes=CFG['num_classes'])\n        if CFG['accelerator']=='GPU':\n            model = nn.DataParallel(model).to(device)\n        scaler = GradScaler()\n        criterion = nn.CrossEntropyLoss().to(device)\n        #criterion = TaylorCrossEntropyLossV1().to(device)\n        fit(seed=1, epochs = CFG['epochs'], model=model, freeze=False, device=device, fold=fold, train_loader=train_loader, val_loader=val_loader, criterion=criterion, gradient_clipping=False)\n\n        LOGGER.info(f'========== fold: {fold+1} result ==========')\n        LOGGER.info('Score: {}'.format(max(all_accuracies['fold_{}'.format(fold+1)][1])))\n        fold_best_acc.append(max(all_accuracies['fold_{}'.format(fold+1)][1]))   \n        \n        del model, train_loader, val_loader#, scaler\n        torch.cuda.empty_cache()\n    # CV result\n    LOGGER.info(f\"========== CV ==========\")\n    LOGGER.info(f'Score: {max(fold_best_acc)}')","d044a71e":"model = timm.create_model('gluon_resnet50_v1b', pretrained=False, num_classes=CFG['num_classes'])\nif CFG['accelerator']=='GPU':\n    model = nn.DataParallel(model).to(device)\nstate = torch.load('.\/gluon_resnet50_v1b_fold_1.pth')\nmodel.load_state_dict(state);\nmodel.to(device);","1aacc471":"test","654f3c9e":"#test.reset_index()\ntest_ds= CustomDataset(test, '.\/Fish_Dataset', transforms=get_transforms(data='val'), output_label=True)\ntest_loader = torch.utils.data.DataLoader(\n            test_ds, \n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=True)\ntest_len = len(test_ds)","a7765523":"def test_func(loader, model, test_len):\n        with torch.no_grad():\n            model.eval()\n            running_corrects = 0.\n            pbar = tqdm(enumerate(loader), total=len(loader))\n            for step, (imgs, targets) in pbar:\n                imgs, targets = imgs.to(device).float(), targets.to(device).long()\n                output = model(imgs)\n                _, pred = output.max(dim=1)\n                running_corrects += torch.sum(pred == targets.data)\n                \n            acc = running_corrects \/ test_len    \n            return acc","a241cba0":"accuracy = test_func(test_loader, model, test_len).item()\naccuracy","d4754596":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>For convenience, I transferred the image data to the pandas table<\/strong><\/p>","edda3f02":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>So, we have reached the most important part. The function is very simple, although it looks scary. It does two things: it checks the configurator which accelerator we are using, and the second thing is that it checks the condition for whether the upper layers need to be frozen. I also added several optimizers that depend on the seed, because when the layers are frozen, the classifier is called differently for some networks (by default, freezing means the freezing of the classifier, if you need to freeze the network in your own way, you will have to rewrite the code a little). To be honest, this function made my life very much easier and now I don't have to constantly rewrite the code for TPU, which speeds up my experiments with the network<\/strong><\/p>","5584d1bc":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>This function gets the indexes for the pandas table and forms datasets from the corresponding objects. Then it wraps datasets in dataloaders depending on which accelerator you choose. It also gives out the length of datasets, that is, the number of objects in it in order to then calculate the accuracy and loss<\/strong><\/p>","3b3cae05":"# **1.2 Outliers Detection using K-Means Clustering**","f67d9b4a":"# **1. Exploratory Data Analysis (EDA)**","b3a4e28a":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>First, we need to transfer data from the input folder to the working folder, because in the first case we will not be able to change the input data, but can only read. The first line is responsible for data movement. This line usually takes a long time to execute, so don't be alarmed<\/strong><\/p>","5ebf58bd":"![Boxplot-with-outliers-The-upper-and-lower-fences-represent-values-more-and-less-than (1).png](attachment:265cef10-9213-476a-96a9-e0dbbf810e9b.png)","1b5fb668":"# **0. Importing Libraries**","dfcb9991":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Since the creator of the dataset did not provide test data, we will have to do it ourselves<\/strong><\/p>","3d692735":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>For augmentation, I use my favorite library Albumentations. Also, the augmentations are divided into two parts, since we need to change the training part a lot, and the validation and test parts only slightly<\/strong><\/p>","a9fbe2ae":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>After another deletion, let's check how the table has changed<\/strong><\/p>","a8ba599c":"![Pagrus-pagrus-Red-porgy-Common-Sea-Bream-Seabream-atlantic-ocean-scuba-diving-canary-islands-Mediterranean-Sea.jpg](attachment:3bcddc01-f572-4017-8e12-0ba0705f1719.jpg)","5b7402fa":"**To find out more:** https:\/\/www.kaggle.com\/ramjib\/cassava-leaf-disease-eda-and-outliers","292aee62":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>We looked at the outliers, now we can delete them from the folder and from the table<\/strong><\/p>","7b738e04":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>This function will help us save the training process in a separate file, which we can download after training in the working folder<\/strong><\/p>","928979e7":"**$$Zscore=\\frac{x-Mean}{Standart Deviation}$$**","0b97d674":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>We start and get the accuracy at the output<\/strong><\/p>","7457556b":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Displaying outliers in the data<\/strong><\/p>","31855eed":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Function for validation one epoch. I also want to draw your attention to gc.collect(). The point is that during training, memory problems can occur and everything will stop, so use this method to force the system to try to free the maximum amount of available memory<\/strong><\/p>","f1301fd4":"<p style = \"color : #28AFEA; font-size : 30px; font-family : 'Comic Sans MS'; text-align : center;\"><strong>Boxplot Anatomy<\/strong><\/p>","cc4f9641":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Before that we saved the model and now we are downloading the best one<\/strong><\/p>","8018064a":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>After we deleted the folder, it would be great to move all photos from the folder with the classification data to the original class folder<\/strong><\/p>","1b53455c":"# **2. Data preparation**","bc404f5d":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Here I have encoded the target values of the objects into numbers<\/strong><\/p>","6f9ed4ca":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Now let's delete them<\/strong><\/p>","9174c13a":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>To carry out experiments and so that your results do not differ from mine, we will make our code more deterministic<\/strong><\/p>","b54774cc":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>This is what fish with different classes look like<\/strong><\/p>","0eab2122":"![39536outlierdetection.png](attachment:3f516b0b-c9db-4efa-bda1-5ff17f42bf4e.png)","edf83c14":"# **4. Test**","19274e9d":"# **3.  Training**","502dbcca":"<p style = \"color : #28AFEA; font-size : 40px; font-family : 'Comic Sans MS'; text-align : center;\"><strong>Fish Recognizing<\/strong><\/p>","5ed76cf6":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Outlier processing is an important part of preparing data for feeding into a neural network. Outliers can be a serious problem because the neural network does not know what is outlier and what is not, and learns from all the data. Learning from outliers is not a good idea for generalizing the sample, because outliers have anamous properties and those regularities that a neural network learns from outliers simply will not apply to the data as a whole<\/strong><\/p>","7d4a2d87":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>We load the data into the dataset and wrap it in the dataloader<\/strong><\/p>","765b7105":"![1-s2.0-S095070512030647X-gr3.jpg](attachment:f794561e-32b6-4983-9777-43d47a2d07cc.jpg)","5c31b470":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>In K-Means clustering outliers are found by distance based approach and cluster based approach<\/strong><\/p>","b7d5cb80":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Since we will process outliers, or rather find and delete them, we need criteria by which we will determine these outliers<\/strong><\/p>","58860317":"![0_R9u16eEcsZHpjH4O_.png](attachment:638dc1cf-f81b-49b9-b9ba-ef15cf414be9.png)","2dbf4a6b":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>You need to check if the number of rows in the table has changed<\/strong><\/p>","6b4b8890":"***Function for training one epoch. In GPU format, I wrote a few lines for mixed precision training, with autocast(), scaler.scale(loss).backward(), scaler.step(optimizer), scaler.update(). In short, mixed precision is the use of both 16-bit and 32-bit floating point types in the model during training, so that it runs faster and uses less memory. Without this, my GPU code will not start and will give an error that the memory has run out***\n\n**To find out more:** https:\/\/pytorch.org\/blog\/accelerating-training-on-nvidia-gpus-with-pytorch-automatic-mixed-precision\/\n\n***I also wrote a few lines that are responsible for the accumulation of the gradient, this is what you could see in the configurator. The fact is that not always crumpling allows us to use a large batch size, for example, we wanted to use a batch size of 32, but memory does not allow us to set a size greater than 8, which will have a bad effect on accuracy. What to do? We can count and accumulate gradients in 4 batches, but we update them not after every batch, but after every four. It turns out 8 * 4, which we originally wanted. Of course, this will not be completely equivalent to a batch with a size of 32 due to the batch norm, but in any case, at least a little solves the problem***\n\n**To find out more:** https:\/\/kozodoi.me\/python\/deep%20learning\/pytorch\/tutorial\/2021\/02\/19\/gradient-accumulation.html","bf8a91cc":"<p style = \"color : #28AFEA; font-size : 30px; font-family : 'Comic Sans MS'; text-align : center;\"><strong>Thanks for reading to the end. I hope this was helpful. I posted this work because I want to show something new and get feedback if you changed something and it turned out better. I am always glad to hear recommendations for changing the code. Please support if this was helpful. Thanks again for your attention<\/strong><\/p>","dbe2b818":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Based on Boxplot, we will calculate the first outliers. As you already saw from the explanation above, anything that is not between upper_fence and lower_fence will be considered an outlier<\/strong><\/p>","6098d34c":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Let's see what the test data looks like<\/strong><\/p>","980b1851":"<p style = \"color : #28AFEA; font-size : 30px; font-family : 'Comic Sans MS'; text-align : center;\"><strong>Hey! In this project we are trying to recognize 9 fish species. I hope it will be useful to someone and if so, then I would like to ask you for a upvote and feedback, because this is the only way I can understand what was useful and what is better not to write anymore. Thanks and enjoy reading)<\/strong><\/p>","6da7d9a2":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>These are the configurations of the project. In general, I think there is nothing to explain here other than accum_iter and accelerator. I will talk about the first thing a little later, but the third thing needs to be explained right away. As you know, networks should not be trained on the CPU unless you are immortal. Currently, we can choose TPU and GPU. The first option is faster, although I will train the networks with the GPU on this project. Also, the code for training on TPU and GPU differs in many places, so I wrote the code (which you are reading now) that would change the accelerator with just one change. In the accelerator graph, you just need to write \"TPU\" or \"GPU\" and, of course, change the accelerator on the right in the panel, and the code will do the rest for yo<\/strong><\/p>","c89522a6":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Assign the path to the working folder to the variable<\/strong><\/p>","a008789e":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Function for predicting a test sample<\/strong><\/p>","64d1b65e":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Everything is very simple here. We just split the data into folds and train (cross-validation). But since my task is simply to show how everything works, we will be validated on one fold. The weights from this model were ported from https:\/\/cv.gluon.ai\/model_zoo\/classification.html<\/strong><\/p>","f24d1b4e":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Own dataset for loading data, nothing out of the ordinary<\/strong><\/p>","c083fb4b":"# **1.3 Z-Score**","262ed7a7":"# **1.1 Outlier Detection using Mean and Standard Deviation**","7909c6fc":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>The fact is that the class folder contains two more folders. The first folder is for classification and the second for segmentation. So in this project we are engaged in classification, then we only need it, therefore I decided to delete the folder with the data for segmentation<\/strong><\/p>","acdbe9ff":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>As expected, the distribution of classes is the same, because the author of the dataset wrote about this<\/strong><\/p>","8014c902":"<p style = \"color : #28AFEA; font-size : 15px; font-family : 'Comic Sans MS';\"><strong>Let's see what the outlier looks like<\/strong><\/p>"}}