{"cell_type":{"6e77c82f":"code","0d22a839":"code","72c1db3c":"code","3db8ca2b":"code","d2112bd2":"code","b6d2efb9":"code","17231d87":"code","0ea602ef":"code","1958b3b3":"code","0fe088cc":"code","8603c23c":"code","edba6822":"code","58f8b873":"code","9304b63a":"code","17301638":"code","ac6e4476":"code","21e8d43c":"code","c0c3eeb1":"code","28f2db44":"code","78a5039e":"code","88288b16":"code","15bd73a3":"code","346bd8bf":"code","8f2ae9b0":"code","40487666":"code","f823bba4":"code","161f9e39":"code","774a07b9":"code","bea785d3":"code","21b59fc8":"code","1e83641e":"code","c8a07325":"code","933f5a3c":"code","bf684749":"code","181d40c3":"code","e9b4bc37":"code","a4969de6":"code","00b53db4":"code","ce7fe4d2":"code","1e939ff3":"code","70a48321":"code","d1b0c826":"code","ccd69947":"code","77fa9782":"code","35ad6411":"code","7b345748":"code","21b1997b":"code","5de74d8a":"code","9d4debce":"code","a86c4272":"code","b25358bc":"code","feba8f2e":"code","7388c694":"code","8fc28177":"code","5f1d3f33":"code","08abe35f":"code","cfbcaa37":"markdown","95788e58":"markdown","144d8534":"markdown","b19d1d1f":"markdown","34024294":"markdown","c6ebcf54":"markdown","003be063":"markdown","750136c1":"markdown","a0f94f0c":"markdown","80769322":"markdown","ee4462cc":"markdown","5e23bca5":"markdown","2eda6343":"markdown"},"source":{"6e77c82f":"import pandas as pd\nimport numpy as np","0d22a839":"# Read the data set\n\ndata = pd.read_table('....input\/SMSSpamCollection', header=None, names=['Class', 'sms'])\ndata.head(10)","72c1db3c":"len(data)","3db8ca2b":"# Get the count, unique and frequency of the data\n\ndata.describe()","d2112bd2":"# Now groupby the class column & describe it\n\ndata.groupby('Class').describe()","b6d2efb9":"# Check the length of the each SMS\n\ndata['length']=data['sms'].apply(len)\ndata.head(10)","17231d87":"# Convert the class into numerical value\n\ndata['labels'] = data.Class.map({'ham':0, 'spam':1})\ndata.head(10)","0ea602ef":"# Now drop the class column\n\ndata = data.drop('Class', axis = 1)\ndata.head(10)","1958b3b3":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","0fe088cc":"data['length'].plot(bins=50,kind='hist')","8603c23c":"data.length.describe()","edba6822":"# A message with 910 words\n\ndata[data['length']==910]['sms'].iloc[0]","58f8b873":"# Example using to create a function which will be use in later part of the code\n\nimport string\nmess = 'sample message!...'\nnopunc=[char for char in mess if char not in string.punctuation]\nnopunc=''.join(nopunc)\nprint(nopunc)","9304b63a":"# Now import the stopwords\n\nfrom nltk.corpus import stopwords\nstopwords.words('english')","17301638":"nopunc.split()","ac6e4476":"# Check whether the word nopunc.split() is present in stopwords.words('english') or not\n\nclean_mess=[word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\nclean_mess","21e8d43c":"# Now let's put both of these together in a function to apply it to our DataFrame later on:\n\ndef text_process(mess):\n    nopunc =[char for char in mess if char not in string.punctuation]\n    nopunc=''.join(nopunc)\n    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]","c0c3eeb1":"# Tokenization can be done but by not removing puctuation\n\nfrom nltk.tokenize import sent_tokenize, word_tokenize\ndata['sms'].apply(word_tokenize).head(10)","28f2db44":"data['sms'].apply(sent_tokenize).head(10)","78a5039e":"# By removing puctuation\n\ndata['sms'].head(10).apply(text_process)","88288b16":"#create an object of class PorterStemmer\n\nfrom nltk.stem import PorterStemmer\nporter = PorterStemmer()","15bd73a3":"data['sms'] = data['sms'].str.lower()","346bd8bf":"print ([porter.stem(word) for word in data['sms']])","8f2ae9b0":"# Convert to X & y\n\nX = data.sms\ny = data.labels","40487666":"print(X.shape)","f823bba4":"print(y.shape)","161f9e39":"from sklearn.model_selection  import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)","774a07b9":"X_train.head()","bea785d3":"y_train.head()","21b59fc8":"# Vectorizing the sentence & removing the stopwords\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n#vect = CountVectorizer(stop_words='english')\n\n# or we can use the fuction text_process in place of stop_words = 'english'\nvect = CountVectorizer(analyzer=text_process)","1e83641e":"vect.fit(X_train)","c8a07325":"# Printing the vocabulary\n\nvect.vocabulary_","933f5a3c":"# vocab size\nlen(vect.vocabulary_.keys())","bf684749":"# transforming the train and test datasets\n\nX_train_transformed = vect.transform(X_train)\nX_test_transformed = vect.transform(X_test)","181d40c3":"# note that the type is transformed (sparse) matrix\n\nprint(type(X_train_transformed))\nprint(X_train_transformed)","e9b4bc37":"# training the NB model and making predictions\nfrom sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\n\n# fit\nmnb.fit(X_train_transformed,y_train)\n\n# predict class\ny_pred_class = mnb.predict(X_test_transformed)\n\n# predict probabilities\ny_pred_proba = mnb.predict_proba(X_test_transformed)","a4969de6":"# note that alpha=1 is used by default for smoothing\nmnb","00b53db4":"# printing the overall accuracy\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test, y_pred_class)","ce7fe4d2":"# confusion matrix\nmetrics.confusion_matrix(y_test, y_pred_class)\n# help(metrics.confusion_matrix)","1e939ff3":"from sklearn.metrics import classification_report,confusion_matrix\nprint(classification_report(y_test, y_pred_class))\nprint(confusion_matrix(y_test, y_pred_class))","70a48321":"confusion = metrics.confusion_matrix(y_test, y_pred_class)\nprint(confusion)\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\nTP = confusion[1, 1]","d1b0c826":"sensitivity = TP \/ float(FN + TP)\nprint(\"sensitivity\",sensitivity)","ccd69947":"specificity = TN \/ float(TN + FP)\nprint(\"specificity\",specificity)","77fa9782":"precision = TP \/ float(TP + FP)\nprint(\"precision\",precision)\nprint(metrics.precision_score(y_test, y_pred_class))","35ad6411":"print(\"precision\",precision)\nprint(\"PRECISION SCORE :\",metrics.precision_score(y_test, y_pred_class))\nprint(\"RECALL SCORE :\", metrics.recall_score(y_test, y_pred_class))\nprint(\"F1 SCORE :\",metrics.f1_score(y_test, y_pred_class))","7b345748":"y_pred_class","21b1997b":"y_pred_proba","5de74d8a":"# creating an ROC curve\nfrom sklearn.metrics import confusion_matrix as sk_confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred_proba[:,1])\nroc_auc = auc(false_positive_rate, true_positive_rate)","9d4debce":"# area under the curve\nprint (roc_auc)","a86c4272":"# matrix of thresholds, tpr, fpr\npd.DataFrame({'Threshold': thresholds, 'TPR': true_positive_rate, 'FPR':false_positive_rate})","b25358bc":"# plotting the ROC curve\n%matplotlib inline  \nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.title('ROC')\nplt.plot(false_positive_rate, true_positive_rate)","feba8f2e":"print(len(X_train),len(X_test),len(y_train),len(y_test))","7388c694":"from sklearn.pipeline import Pipeline\npipeline = Pipeline([\n   ( 'bow',CountVectorizer(analyzer=text_process)),\n    ('classifier',MultinomialNB()),\n])","8fc28177":"pipeline.fit(X_train,y_train)","5f1d3f33":"predictions = pipeline.predict(X_test)","08abe35f":"print(classification_report(predictions,y_test))","cfbcaa37":"## Split the data into train & test","95788e58":"### Continuing Normalization - Stemming","144d8534":"### Or ","b19d1d1f":"### Creating a Data Pipeline","34024294":"### Model Evaluation","c6ebcf54":"## Import the libraries","003be063":"## Exploratory Data Analysis","750136c1":"## ROC Curve","a0f94f0c":"### Building and Evaluating the Model","80769322":"### Text Pre - Processing - Tokenization","ee4462cc":"## Visualize the Data","5e23bca5":"### Vectorization","2eda6343":"## Text Pre - Processing"}}