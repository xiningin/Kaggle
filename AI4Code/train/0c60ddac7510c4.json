{"cell_type":{"c86307ab":"code","2130a1b2":"code","96d32e6e":"code","fb11c95c":"code","fe5102b6":"code","19ad1849":"code","596d46d6":"code","66f9ab0f":"code","cc0c69f7":"code","9a03d14d":"code","38b4dfe9":"code","66a81e83":"code","bc7b43c2":"code","55573a6c":"code","e6385f70":"code","db6f74e7":"code","0199da93":"code","ac4f3779":"code","3e775d61":"code","1782645a":"code","c7ed7f18":"code","1746cfc4":"code","77ac1cb0":"markdown","f30edacd":"markdown","3c2b7c96":"markdown","66f917af":"markdown","d14a6fb9":"markdown","cb1d2a93":"markdown","5fa15c0a":"markdown","6804ed71":"markdown","497e9caf":"markdown","3f789d56":"markdown","7b346ae9":"markdown","e62685ab":"markdown","4d377752":"markdown","1e09e53f":"markdown","95baf2f8":"markdown","c4dc6823":"markdown","94c4c23a":"markdown","ac60997b":"markdown"},"source":{"c86307ab":"import numpy as np # linear algebra\nimport pandas as pd ","2130a1b2":"!pip3 install pycaret","96d32e6e":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","fb11c95c":"from pycaret import classification\nclassification_setup = classification.setup(data=train,target='Survived', ignore_features = ['Ticket', 'Name', 'PassengerId'], silent = True, session_id=42)","fe5102b6":"classification.compare_models()","19ad1849":"from pycaret.classification import *\nmodels()\ncompare_models(whitelist = models(type='ensemble').index.tolist())","596d46d6":"lgb_classifier = classification.create_model('lightgbm')","66f9ab0f":"params = {'learning_rate':[0.15,0.1,0.05,0.01,0.005,0.001],\n          'n_estimators':[100,250,500,750,1000,1250,1500,1750],\n          'max_depth': np.random.randint(1, (len(train.columns)*.85),20),\n          'max_features': np.random.randint(1, len(train.columns),20),\n          'min_samples_split':[2,4,6,8,10,20,40,60,100], \n          'min_samples_leaf':[1,3,5,7,9],\n          'criterion': [\"gini\", \"entropy\"]}\n\ntune_lgb = classification.tune_model(lgb_classifier, custom_grid = params)","cc0c69f7":"# Tune the model\nparams = {'alpha':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]}\ntune_ridge = classification.tune_model(create_model('ridge'), custom_grid = params, n_iter=50, fold=50)","9a03d14d":"# ensemble boosting\nbagging = classification.ensemble_model(tune_lgb, method= 'Bagging')","38b4dfe9":"from pycaret.classification import blend_models\n# blending all models\nblend_all = blend_models(method='hard')","66a81e83":"# create individual models for stacking\nridge_cls = classification.create_model('ridge')\nextre_tr = classification.create_model('et')\nlgb = classification.create_model('lightgbm')\ncat_cls = classification.create_model('catboost')\nlg_cls = classification.create_model('lr')\n","bc7b43c2":"from pycaret.classification import stack_models\n# stacking models\nstacker = stack_models(estimator_list = [ridge_cls, extre_tr, lgb, cat_cls, lg_cls],method='hard')","55573a6c":"interpret_model(tune_lgb)","e6385f70":"from pycaret.classification import *\n# plotting a model\nplot_model(tune_lgb,plot = 'pr')","db6f74e7":"# plotting a model\nplot_model(tune_lgb,plot = 'confusion_matrix')","0199da93":"# Validation Curve\nplot_model(tune_lgb, plot = 'vc')`","ac4f3779":"# AUC Curve\nplot_model(tune_lgb, plot = 'auc')","3e775d61":"# error Curve\nplot_model(tune_lgb, plot = 'error')","1782645a":"y_pred = predict_model(tune_lgb, data=test)","c7ed7f18":"y_pred","1746cfc4":"submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred['Label']\n    })\nsubmission.to_csv(\"submission.csv\", index=False)","77ac1cb0":"Just a functional execution call, and it will compare all the classification models with few seconds and display the sorted score grid.\n\nNote: It seems that the Ridge classifier gives higher accuracy than the rest classifier.","f30edacd":"**Create Model**\n\n* Let\u2019s create an individual model that displays different evaluation matric using 10 k-fold with mean and std.\n* create_model function takes just the one parameter \u2013 the model abbreviation as a string.","3c2b7c96":"**Hyperparameter Tuning**\n\n* Depending on the model evaluation metric(s) we are interested in pycaret helps us to straightaway zoom in on the top-performing model which we can further tune using the hyper-parameters.\n* tune_model() function tune the hyperparameters of a model and it takes one parameter model abbreviation string (same as we used for creating model)","66f917af":"This returns you pandas dataframe with all ready-to-use models available in the library.","d14a6fb9":"## Import Libararies","cb1d2a93":"![Co-learning Lounge](https:\/\/s3.ap-south-1.amazonaws.com\/townscript-production\/images\/2545d2c7-a6e8-486e-97e6-737c42cef670.jpg)\nThanks to the Co-learning Lounge for pushing to create learning content on [PyCaret](https:\/\/pycaret.org\/) with Kaggle playground problem.\n\nYou can find most updated and comprehensive learning material in their community.\n\nJoin and follow the [Co-learning Lounge](https:\/\/linktr.ee\/colearninglounge) for more.","5fa15c0a":"**Ensemble Model**","6804ed71":"**Blend Models**\n\nCombining different machine learning models and use a majority vote or the average predicted probabilities in case of classification to predict the final outcome.","497e9caf":"**Data Dictionary**:\n* survival - Survival (0 = No; 1 = Yes)\n* class - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n* name - Name\n* sex - Sex\n* age - Age\n* sibsp - Number of Siblings\/Spouses Aboard\n* parch - Number of Parents\/Children Aboard\n* ticket - Ticket Number\n* fare - Passenger Fare\n* cabin - Cabin\n* embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)","3f789d56":"**Prediction**","7b346ae9":"* Above score grid, shows the result of the model at each iteration and provide mean and std of it.","e62685ab":"So, now the necessary preprocessing is done, let\u2019s create a classification model. ","4d377752":"In PyCaret, we can create bagging, boosting, blending, and stacking ensemble models with just one line of code.","1e09e53f":"**Setup**\n\n* Setup() performs inferences about the data and creates the transformation pipeline to prepare the data for modeling and deployment. \n* Initializing setup() function performs some basic preprocessing tasks like ignoring the IDs and Date Columns, imputing the missing values, encoding the categorical variables, and splitting the dataset into the train-test split, data imbalance, feature selection, binning, etc. for the rest of the modeling steps. When you run the setup function, it will first confirm the data types, and then if you press enter, it will create an environment for data preprocessing.\n* It takes 2 mendatory parameter Dataframe and name of the target column\n","95baf2f8":"Quickly let us get into the installation and build a perfect model.","c4dc6823":"**Plot Model**\nPycaret also evaluate your model performance as easy as you build the model with different plots","94c4c23a":"**Stack Models**\n\nStacking is an ensembling method that uses meta-learning. The idea behind stacking is to build a meta-model that generates the final prediction using the prediction of multiple base estimators.","ac60997b":"**Compare Models**\n\n* Compare_models function train all the models which are available in library using stratified cross validation, this function will return score grid of all model across k-fold(default=10).\n* Scoring matrics used are Accuracy, AUC, Recall, Precision, F1, Kappa and MCC. Mean and standard deviation of the scores across the folds are also returned.\n* You can blacklist(omit certain models from the comparison) and whiltelist(un only certain models for the comparison) the model, passig model ID\u2019s as a list of strings\neg. whitelist = compare_models(whitelist = ['dt','rf','xgboost'])\nblacklist = compare_models(blacklist = ['catboost', 'svm'])\n* Best model return as per the sort parameter(default=Accuracy) passed.\n* Also we can select N top models passing n_select(default=1) parameter"}}