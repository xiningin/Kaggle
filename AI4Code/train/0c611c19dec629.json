{"cell_type":{"34abf6b3":"code","fb94eb30":"code","98d43042":"code","1d905d6e":"code","e8090179":"code","fe54340c":"code","279d2159":"code","e8dca211":"code","fd0f5013":"code","44ff4c41":"code","2d5ac79b":"code","5d0ae55f":"code","55fa8bff":"code","c2d0afe3":"code","b4ed7573":"code","c0ddfa0b":"code","65132f8b":"code","a85dbe8e":"code","3d0d4d62":"code","d88fe177":"code","a5f5aada":"code","8129739f":"markdown","a46c9b14":"markdown","f6db61b1":"markdown","9e44249f":"markdown","354b42a4":"markdown","5c383386":"markdown","0cab63ea":"markdown","fd69200f":"markdown","7ac9cb1a":"markdown"},"source":{"34abf6b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fb94eb30":"%%bash\ncd ..\/input\/tabular-playground-series-jan-2021\/train.csv\nls","98d43042":"import pandas as pd\ndata_train = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/train.csv')\ndata_test = pd.read_csv('..\/input\/tabular-playground-series-jan-2021\/test.csv')","1d905d6e":"data_test.describe()","e8090179":"data_train.info()","fe54340c":"import seaborn as sns\nimport matplotlib.pyplot as  plt\n\nfig,ax = plt.subplots(14,1,figsize = (15,50))\ncols = data_train.iloc[:,1:].columns\nfor col in range(0,len(cols) - 1):\n        plt.sca(ax[col]) \n        ax[col]  =  sns.distplot(data_train[cols[col]])\n        ax[col].set_xlabel(cols[col])\nplt.show()\n","279d2159":"fig,ax = plt.subplots(figsize = (15,15))\nsns.heatmap(data_train.iloc[:,1:].corr(),annot = True,ax = ax)","e8dca211":"fig,ax = plt.subplots(figsize = (25,15))\nsns.boxplot(ax = ax, x=\"variable\", y=\"value\", data=pd.melt(data_train.iloc[:,1:-1]))","fd0f5013":"from sklearn.decomposition import PCA\n\npca = PCA(7)\nout = pca.fit_transform(data_train.iloc[:,1:-1])\n\n","44ff4c41":"pd.DataFrame(out)","2d5ac79b":"fig,ax = plt.subplots(figsize = (15,15))\nsns.heatmap(pd.DataFrame(out).corr(),annot = True,ax = ax)","5d0ae55f":"def train_test_split(data,train_split = 0.1):\n    train_mask = data_train.loc[:,['id']].applymap(lambda x : abs(hash(str(x))) % 10000  < 10000 * train_split)\n    test_mask = data_train.loc[:,['id']].applymap(lambda x : abs(hash(str(x))) % 10000  >= 10000 * train_split)\n    return (data.iloc[[x[0] for x in train_mask.values],1:15] , data.iloc[[x[0] for x in test_mask.values],1:15] , data.iloc[[x[0] for x in train_mask.values],-1] , data.iloc[[x[0] for x in test_mask.values],-1])","55fa8bff":"x_train, x_test,y_train,y_test = train_test_split(data_train,0.7)","c2d0afe3":"import tensorflow as tf\n\ndef generate_feature_column(x):\n    features = []\n    for i in x.columns:\n        features.append(tf.feature_column.numeric_column(i))\n    features.append(tf.feature_column.embedding_column(tf.feature_column.crossed_column(['cont6', 'cont9','cont10','cont11','cont12','cont13'], hash_bucket_size=3000),dimension = 100))\n    return features\n    ","b4ed7573":"generate_feature_column(x_train)","c0ddfa0b":"def generate_train_data(x,y,albation = False,batch = 100,epochs = 10):\n    if(not albation):\n        return tf.data.Dataset.from_tensor_slices((dict(x), y.values)).shuffle(buffer_size=batch).repeat(count=epochs).batch(batch)\n    else:\n        return tf.data.Dataset.from_tensor_slices((dict(x.iloc[0:1000,:]),y[0:1000].values)).shuffle(buffer_size=batch).repeat(count=epochs).batch(batch)","65132f8b":"def generate_test_data(x,batch = 100):\n        return tf.data.Dataset.from_tensor_slices(dict(x)).batch(batch)","a85dbe8e":"next(iter(generate_train_data(x_train,y_train)))","3d0d4d62":"batch = 100\nepochs = 5\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1,\n                              patience=1, min_lr=0.00001)\nmodel = tf.keras.Sequential([\n    tf.keras.layers.DenseFeatures(generate_feature_column(x_train)),\n    tf.keras.layers.Dense(1)\n])\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001), loss = 'mse', metrics=[tf.keras.metrics.RootMeanSquaredError()])\nmodel.fit(generate_train_data(x_train,y_train,batch = batch,epochs = epochs) ,validation_data=generate_train_data(x_test,y_test),epochs=epochs,verbose = 1,workers=-1,batch_size = batch)","d88fe177":"pred = model.predict(generate_test_data(data_test))","a5f5aada":"pd.DataFrame({'id':data_test.iloc[:,0].values,'target': np.reshape(pred,(200000,))}).to_csv('.\/result.csv',index=False)","8129739f":"We will do some feature engineering to further improve the quality  of the data","a46c9b14":"Our data generator is working fine , lets start building our model","f6db61b1":"We will use PCA to reduce dimentionality","9e44249f":"# Exploratory Data Analysis\n\nLets explore the spread of the dataset and also try to find  if there are any missing values","354b42a4":"# Albation experiment\n\nWe will split the training , test and eval data , and then start training our neural network","5c383386":"Now we will define our pre-processing pipeline based on tensorflow. This will help us make preprocessing part of the model itself.Also since we are using neural networks , we will use regularisation as well , so that it will work as dimentioality reducer.","0cab63ea":"The plots give us a small insight into the nature of the data . We can see that the dataset is quite intact and has no missing values. However there are some values which show a certain degree of corellation . Lets explore this a bit more.","fd69200f":"The above heat map shows us that some  the data is corellated. Let us also study the relative rannge of the data","7ac9cb1a":"Now we will define the functions for dataset generation . We will first use a small dataset for albation experiments , then we will feed the full data"}}