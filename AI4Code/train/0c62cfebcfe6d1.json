{"cell_type":{"3fbf5470":"code","628b1ab5":"code","f159a995":"code","57ee3a0e":"code","a5d215a4":"code","badc2c3e":"code","c64bd860":"code","bdbd0bb0":"code","99002037":"code","4bbaa1c7":"code","f63a989e":"code","2464a569":"code","f80c058d":"code","d5c831f0":"code","520536f7":"code","7b99f746":"code","336006d1":"code","e12fce67":"code","8e2b9702":"code","4ea1a4ab":"code","d9f63069":"code","226a299f":"code","37026566":"code","5222a62d":"code","d05a6333":"code","b1aee47d":"code","64020cfb":"code","9572789b":"code","f570539a":"code","d37398d2":"code","9757a696":"code","a1781edf":"code","3784e94a":"code","eb112d7a":"code","a821e075":"code","77c279e3":"code","571e59ff":"code","bd32346b":"code","64adea0c":"code","c4441fe3":"code","f30401fb":"code","d5cbaddc":"code","1e510f34":"code","7cf279ed":"code","da9f53bc":"code","53a1dba0":"code","7a8c6ec3":"code","9d1b1d1d":"code","6d8b148f":"code","6bfbefdd":"code","a1463fa7":"code","471e1a4e":"code","d46a02ba":"code","baa5fba6":"code","6ef16913":"code","6b5f38f6":"code","95b515d4":"code","799bac9d":"code","c308f4d3":"code","9c0c3f91":"code","41450b98":"code","c9f9b860":"code","41c201a0":"code","2e1a8c73":"code","a0ca66ce":"code","75679f8a":"code","b1e9b734":"code","80f8719d":"code","134b5b81":"code","d33d9b30":"code","53341476":"code","fc11c579":"code","5687c6b5":"code","bd6bdb5e":"code","ddbfb398":"code","0c66c01c":"code","f85a3923":"code","b1dfbbbe":"code","f983dff7":"code","9d502e48":"code","f158723b":"code","d18a4fb6":"code","8442df3d":"code","e925a362":"markdown","4ea20930":"markdown","cb25358b":"markdown","99450905":"markdown","9301a695":"markdown","fd94e4b3":"markdown","6c3a2140":"markdown","7ecece54":"markdown","f298da84":"markdown","8fe61b83":"markdown","27bafd9e":"markdown","98ec27e4":"markdown","46461c9f":"markdown","d65a7332":"markdown","0598d842":"markdown","50e6bb5a":"markdown","696372b5":"markdown","9f798dc5":"markdown","5eb84980":"markdown","06ff6ca5":"markdown","ff3a66c9":"markdown","696497e8":"markdown","6193dc4e":"markdown","eb58b0ef":"markdown","8611693c":"markdown","cd53fff1":"markdown","97e4bbff":"markdown","af905546":"markdown","d61220fd":"markdown","800e1631":"markdown","dc111c97":"markdown","8e3eff79":"markdown","60dd57f0":"markdown","6b4d469c":"markdown","54389273":"markdown","30b94668":"markdown","d1b36314":"markdown","841ba9ef":"markdown","ba174b2d":"markdown","31f8f4ea":"markdown","37cc0ec7":"markdown","9d0baf8f":"markdown","10948cb1":"markdown","69a0f7b9":"markdown","56af647a":"markdown","d2aa0d32":"markdown","99dfd542":"markdown","ace59009":"markdown","59d5f9d2":"markdown","f475c3d3":"markdown","a1a4d293":"markdown","bfb1b238":"markdown","55b3297e":"markdown","e20242bb":"markdown","3c84b6a9":"markdown","35f558f7":"markdown","1c0a96d4":"markdown"},"source":{"3fbf5470":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\npd.set_option('display.max_columns', 500)\n\nimport warnings\nwarnings.filterwarnings('ignore')","628b1ab5":"bike = pd.read_csv(\"\/kaggle\/input\/bike-sharing-datasetcsv\/bike_sharing_dataset.csv\")\nbike.head()","f159a995":"bike.describe()","57ee3a0e":"# Checking the shape of the data\nbike.shape","a5d215a4":"bike.info()","badc2c3e":"bike.isnull().sum()","c64bd860":"# Renaming columns for clear understanding\n\nbike.rename(columns = {'yr':'year','mnth':'month','hum':'humidity','cnt':'count'}, inplace = True) \nbike.head()","bdbd0bb0":"# Converting column season : season (1:spring, 2:summer, 3:fall, 4:winter)\nbike[['season']] = bike[['season']].apply(lambda x: x.map({1:'Spring', 2:'Summer', 3:'Fall', 4:'Winter'}))","99002037":"bike.season\n\n# So we can see that the values have been converted.","4bbaa1c7":"# Similarly we'll do the same for Month, Weathersit and Weekday\n\nbike[['month']] = bike[['month']].apply(lambda x: x.map({1:'Jan',2:'Feb',3:'Mar',4:'Apr',5:'May',6:'June',7:'July',8:'Aug',9:'Sep',10:'Oct',11:'Nov',12:'Dec'}))\n\nbike[['weathersit']] = bike[['weathersit']].apply(lambda x: x.map({1: 'Clear',2:'Mist + Cloudy',3:'Light Snow',4:'Snow + Fog'}))\n\nbike[['weekday']] = bike[['weekday']].apply(lambda x: x.map({0:'Sun',1:'Mon',2:'Tue',3:'Wed',4:'Thu',5:'Fri',6:'Sat'}))","f63a989e":"bike.head()\n\n# Now we can observe that all the data is good to go for dummy variable creation","2464a569":"# We can check the number of unique values is a column\n# If the number of unique values <=40: Categorical column\n# If the number of unique values in a column >= 50: Continuous column\n\nbike.nunique().sort_values()","f80c058d":"sns.pairplot(bike, vars=['temp','humidity','casual','windspeed','registered','atemp','count'])\nplt.show()","d5c831f0":"plt.figure(figsize=(20, 12))\n\nplt.subplot(3,3,1)\nsns.boxplot(x = 'year', y = 'count', data = bike)\n\nplt.subplot(3,3,2)\nsns.boxplot(x = 'holiday', y = 'count', data = bike)\n\nplt.subplot(3,3,3)\nsns.boxplot(x = 'workingday', y = 'count', data = bike)\n\nplt.subplot(3,3,4)\nsns.boxplot(x = 'month', y = 'count', data = bike)\n\nplt.subplot(3,3,5)\nsns.boxplot(x = 'weathersit', y = 'count', data = bike)\n\nplt.subplot(3,3,6)\nsns.boxplot(x = 'season', y = 'count', data = bike)\n\nplt.subplot(3,3,7)\nsns.boxplot(x = 'weekday', y = 'count', data = bike)\n\nplt.show()","520536f7":"# Relation between month, year and target variable count.\n\nplt.figure(figsize=(10,5))\nsns.barplot(x = 'month', y = 'count', hue = 'year', data = bike, palette = 'Paired')\nplt.show()","7b99f746":"# Heatmap to see correlation between variables\n\nplt.figure(figsize=(25, 12))\nsns.heatmap(bike.corr(), cmap='RdYlGn', annot = True)\nplt.title(\"Correlation between Variables\")\nplt.show()","336006d1":"# Removing uneccessary columns like date, casual, registered, atemp and instant as they're not required\n\nbike.drop(columns = [\"instant\", \"dteday\", \"casual\", \"registered\", \"atemp\"], inplace=True)","e12fce67":"# Creating dummy variables for month, season, weathersit, weekday and drop the first column since it will be all 0.\n\nmonths = pd.get_dummies(bike.month,drop_first=True)\n\nweekdays = pd.get_dummies(bike.weekday,drop_first=True)\n\nweather_sit = pd.get_dummies(bike.weathersit,drop_first=True)\n\nseasons = pd.get_dummies(bike.season,drop_first=True)","8e2b9702":"# Concatinate all the new dataframes to the original Bike dataframe\n\nbike = pd.concat([months, weekdays, weather_sit, seasons, bike], axis=1)\nbike.head()","4ea1a4ab":"# Now drop season, month, weekday, weathersit as we have created the dummies for it\n\nbike.drop(['season','month','weekday','weathersit'], axis = 1, inplace = True)\nbike.head()","d9f63069":"bike.shape","226a299f":"# Heatmap to see correlation between variables\n\nplt.figure(figsize=(25, 20))\n\nsns.heatmap(bike.corr(), cmap='RdYlGn', annot = True)\nplt.show()","37026566":"from sklearn.model_selection import train_test_split","5222a62d":"# Making a Train\/Test split of 70:30\n\nbike_train, bike_test = train_test_split(bike, train_size = 0.7, random_state = 100)","d05a6333":"# Shape after split\n\nprint(bike_train.shape)\nprint(bike_test.shape)","b1aee47d":"from sklearn.preprocessing import MinMaxScaler","64020cfb":"# Instantiating the object\nscaler = MinMaxScaler()\n\n# Create a list of continuous variables\nc_vars=['temp','humidity','windspeed','count']\n\n# Fit on data by using fit_transform()\n\nbike_train[c_vars] = scaler.fit_transform(bike_train[c_vars])\nbike_train.head()","9572789b":"# Checking numeric variables(min and max) after scaling\n\nbike_train.describe()","f570539a":"plt.figure(figsize=(25, 20))\nsns.heatmap(bike_train.corr(),cmap='YlOrRd',annot = True)\n\nplt.show()","d37398d2":"# Divide the data into X and y\n\ny_train = bike_train.pop('count')\nX_train = bike_train","9757a696":"y_train.head()","a1781edf":"X_train.head()","3784e94a":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","eb112d7a":"lm = LinearRegression()\nlm.fit(X_train, y_train)\n\n# RFE needs 2 things:\n# 1. The model\n# 2. No. of variables\/features we want to choose\n\nrfe = RFE(lm, 15)\nrfe = rfe.fit(X_train, y_train)","a821e075":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))\n\n# RFE has:\n# 1. Support, which tells whether this partiular feature was selected or not\n# 2. Ranking, which is 1 for all the features that are selected, in this case for all the 10 values and for other features it\n#             is ranked in terms of importance in the model.","77c279e3":"col = X_train.columns[rfe.support_]\ncol\n\n# So, we'll take all those top 10 columns for which RFE support is True\/1.","571e59ff":"X_train.columns[~rfe.support_]","bd32346b":"# Creating X_test dataframe with RFE selected variables\n\nX_train_rfe = X_train[col]","64adea0c":"import statsmodels.api as sm","c4441fe3":"# Adding a constant variable \n\nX_train_rfe = sm.add_constant(X_train_rfe)","f30401fb":"lm = sm.OLS(y_train,X_train_rfe).fit()   # Running the linear model","d5cbaddc":"# Let's see the summary of our linear model\n\nprint(lm.summary())","1e510f34":"# Drop the constant term\nX_train_rfe = X_train_rfe.drop(['const'], axis=1)","7cf279ed":"from statsmodels.stats.outliers_influence import variance_inflation_factor","da9f53bc":"# Calculate the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","53a1dba0":"# Drop humidity\n\nX_train_new1 = X_train_rfe.drop([\"humidity\"], axis = 1)","7a8c6ec3":"# Re-building the model without Humidity\n\nX_train_lm1 = sm.add_constant(X_train_new1)\n\nlm1 = sm.OLS(y_train,X_train_lm1).fit()\n\nprint(lm1.summary())","9d1b1d1d":"# Dropping the constant\n\nX_train_lm1 = X_train_lm1.drop(['const'], axis=1)","6d8b148f":"# Calculate the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new1\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","6bfbefdd":"# Drop Nov\n\nX_train_new2 = X_train_lm1.drop([\"Nov\"], axis = 1)","a1463fa7":"# Build a model\n\nX_train_lm2 = sm.add_constant(X_train_new2)\n\nlm2 = sm.OLS(y_train,X_train_lm2).fit()\n\nprint(lm2.summary())","471e1a4e":"# Drop the constant\n\nX_train_lm2 = X_train_lm2.drop(['const'],axis=1)","d46a02ba":"# Calculate the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new2\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","baa5fba6":"# Drop the Dec column\n\nX_train_new3 = X_train_lm2.drop(['Dec'],axis=1)","6ef16913":"# Re-building the model without Dec\n\nX_train_lm3 = sm.add_constant(X_train_new3)\n\nlm3 = sm.OLS(y_train,X_train_lm3).fit()\n\nprint(lm3.summary())","6b5f38f6":"# Drop constant \n\nX_train_lm3 = X_train_lm3.drop(['const'],axis=1)","95b515d4":"# Calculate the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new3\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","799bac9d":"# Drop Jan\n\nX_train_new4 = X_train_lm3.drop(['Jan'], axis=1)","c308f4d3":"# Build a model\n\nX_train_lm4 = sm.add_constant(X_train_new4)\n\nlm4 = sm.OLS(y_train,X_train_lm4).fit()\n\nprint(lm4.summary())","9c0c3f91":"# Drop constant\n\nX_train_lm4 = X_train_lm4.drop(['const'], axis=1)","41450b98":"# Calculate the VIFs for the new model\n\nvif = pd.DataFrame()\nX =X_train_new4\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c9f9b860":"# Drop Windspeed\n\nX_train_new5 = X_train_lm4.drop(['windspeed'], axis=1)","41c201a0":"# Building a model\n\nX_train_lm5 = sm.add_constant(X_train_new5)\n\nlm5 = sm.OLS(y_train,X_train_lm5).fit()\n\nprint(lm5.summary())","2e1a8c73":"# Drop the constant\n\nX_train_lm5 = X_train_lm5.drop(['const'],axis=1)","a0ca66ce":"# Calculate the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new5\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","75679f8a":"# Drop Summer\n\nX_train_new6 = X_train_lm5.drop(['Summer'], axis=1)","b1e9b734":"# Building a model\n\nX_train_lm6 = sm.add_constant(X_train_new6)\n\nlm6 = sm.OLS(y_train,X_train_lm6).fit()\n\nprint(lm6.summary())","80f8719d":"# Drop the constant\n\nX_train_lm6 = X_train_lm6.drop(['const'],axis=1)","134b5b81":"# Calculate the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_new6\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","d33d9b30":"X_train_lm6","53341476":"X_train_lm6.shape","fc11c579":"# y_train predicted\nX_train_lm6 = sm.add_constant(X_train_lm6)\ny_train_pred = lm6.predict(X_train_lm6)","5687c6b5":"fig = plt.figure()\n\nplt.figure(figsize=(14,7))\nsns.distplot((y_train - y_train_pred), bins = 20)\n\nplt.title('Error Terms', fontsize = 20)\nplt.xlabel('Errors', fontsize = 18)\n\nplt.show()","bd6bdb5e":"# Create a list of numeric variables\n\nnum_vars=['temp','humidity','windspeed','count']\n\n# Fit on data by using transform(), we need not to fit the test data.\n\nbike_test[num_vars] = scaler.transform(bike_test[num_vars])\nbike_test.head()","ddbfb398":"# Dividing into X_test and y_test\n\ny_test = bike_test.pop('count')\nX_test = bike_test\nX_test.describe()","0c66c01c":"# Columns\n\nX_train_new5.columns","f85a3923":"# Creating X_test_new dataframe by dropping variables from X_test\n\nX_test_new = X_test[X_train_new5.columns]\n\n# Adding a constant variable \n\nX_test_new = sm.add_constant(X_test_new)\nX_test_new.head()","b1dfbbbe":"y_pred = lm5.predict(X_test_new)","f983dff7":"from sklearn.metrics import r2_score","9d502e48":"r2_score(y_test, y_pred)","f158723b":"Adj_r2 = 1 - (1 - 0.8092660) * (11 - 1) \/ (11 - 1 - 1)\nprint(Adj_r2)","d18a4fb6":"# Plotting y_test and y_pred to understand the spread.\n\nplt.figure(figsize=(15,8))\nplt.scatter(y_test, y_pred, color='blue')\nplt.title('y_test vs y_pred', fontsize=20)\nplt.xlabel('y_test', fontsize=18)\nplt.ylabel('y_pred', fontsize=16)\nplt.show()","8442df3d":"plt.figure(figsize=(15,8))\n\nsns.regplot(x = y_test, y = y_pred, ci = 68, fit_reg = True, scatter_kws = {\"color\": \"blue\"}, line_kws = {\"color\": \"red\"})\n\nplt.title('y_test vs y_pred', fontsize=20) \nplt.xlabel('y_test', fontsize=18)\nplt.ylabel('y_pred', fontsize=16)\n\nplt.show()","e925a362":"### Step 1.3: Converting unnecessary numerical columns to String values","4ea20930":"## Problem Statement\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\n- Which variables are significant in predicting the demand for shared bikes.\n- How well those variables describe the bike demands\n\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. \n\n\n## Business Goal:\nYou are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market. ","cb25358b":"1. We can see that temperature variable is having the highest coefficient 0.4695, which means if the temperature increases by one unit the number of bike rentals increases by 0.4695.\n2. Similary, we can observe coefficients of other variables in the equation for best fitted line which are positively impacting the bike rentals.\n3. We also observe that there are some variables with negative coefficients, \n    - A negative coefficient denotes that as the independent variable will increase, the dependent will decrease.\n    - July, Light Snow , Mist + Cloudy, Spring and Holiday variables have negative coefficient. \n4. The coefficient value signifies how much the mean of the dependent variable changes given a one-unit shift in the independent variable while holding other variables in the model constant.","99450905":"<h2 style=\"color: Teal\">Step 8: Dividing into X and y sets<\/h2>","9301a695":"**Inference from the above heatmap:**\n1. Year and Temp are correlated to Count (target variable).\n2. Months of June, August and September are also a little bit related as counts are a bit high (>= 0.2).\n3. We can also see that Spring, and months of January and February are negatively correlated to Count.","fd94e4b3":"> Dropping \"Jan\" seems to be a better idea due to it's low VIF and high p-value.","6c3a2140":"### Step 3.4: Plotting a Heatmap to find correlation","7ecece54":"<h1 style=\"color:brown\">BoomBikes Case Study<\/h1>","f298da84":"Dividing into X and Y sets for the model building","8fe61b83":"> We can observe that there are no null values in the data set","27bafd9e":"<h2 style=\"color: Teal\">Step 14: Business Related Goals for BoomBikes<\/h2>","98ec27e4":"**Inference from the above heatmap:**\n1. There is a negative correlation between Spring and count, which is what we saw earlier.\n2. Year and Temp are highly correlated to count.\n3. We can also observe that Summer season and June to October months are in good correlation with the 'count' variable and seem to have good influence on our target variable count.","46461c9f":"<h2 style=\"color: Teal\">Step 6: Rescaling the Features<\/h2>\n\n> There are 2 common ways of rescaling, Standardization or Normalization so that units of coefficients are all on the same scale.\n\n**2 Ways to rescale:**\n1. Min-Max scaling (Normalisation): Between 0 and 1 : \n> ### Xnorm = $\\frac {x - minValue} {maxValue - minValue}$\n\n\n2. Standardisation :mean-0, sigma-1 : \n> ### Xstd = $\\frac {x - \\mu} {\\sigma}$\n\n###### For our analysis, we'll use min-max scaling as it'll deal with the outliers as well and we don't have to worry about them later.","d65a7332":"> \"Windspeed\" can be dropped due to it's high p-value and negative correlation.","0598d842":"<h2 style=\"color: Teal\">Step 7: Checking the Correlation between Normalized features<\/h2>","50e6bb5a":"<h2 style=\"color: Teal\">Step 5: Splitting the Data into Training and Test Sets<\/h2>","696372b5":"Running RFE with the output number of the variable equal to 15, since preferrably we should select 50% of features for RFE and we have 29 features, so we can select 15 features","9f798dc5":"**Now we can observe that we have our model**\n\nThe VIFs and p-values both are within an acceptable range. So we go ahead and make our predictions using this model only.","5eb84980":"### So the equation of our best fitted line from the above linear regression model would be:\n\n$ count = 0.0654\\times Sep + 0.0534\\times Winter + 0.2332\\times year + 0.4695\\times temp - 0.0690\\times July - 0.2993\\times (LightSnow) - 0.0781\\times(Mist + Cloudy) - 0.1122\\times Spring - 0.1006\\times holiday $","06ff6ca5":"**Evaluate R-square for test:**\n\n$ R^2 = 1-\\frac{RSS}{TSS} $","ff3a66c9":"### Step 3.3: Understanding the year variable","696497e8":"**So we can observe from the above plot, that:**\n1. year, temp, atemp, casual and registered have high correlation with the target variable, however atemp, casual and registered are of no use and only year variable is what matters for the analysis.\n2. From the above EDA we are sure that we can create a Linear Regression model for further analysis and predictions","6193dc4e":"<h2 style=\"color: Teal\">Step 3: Visualising the Data - Exploratory Data Analysis<\/h2>","eb58b0ef":"Difference in $R^2$ between train and test is \n\n$R_{Train}^2 - R_{Test}^2$ \n\n=> 82.2 - 80.9 = 1.3%","8611693c":"**We can observe that the Error Terms are normally distributed.**","cd53fff1":"<h2 style=\"color: Teal\">Step 4: Removing uneccessary columns and creating dummy variable<\/h2>","97e4bbff":"<h2 style=\"color: Teal\">Step 11: Predictions<\/h2>\n\nApplying scaling on the test set","af905546":"<h2 style=\"color: Teal\">Step 9: Building a Linear Model<\/h2>","d61220fd":"**The plots above shows the relationship between categorical variables and count variable:**\n1. Bike Rentals are more during the Fall season and than in summer which also explains as to why rentals are higher in September and October.\n2. Bike Rentals are more in the year 2019 compared to 2018.\n3. Bike Rentals are more in clear weather.\n4. Bike Rentals are more on Thursday, Friday and Sunday.","800e1631":"### Step 4.1: Checking the correlation between variables again","dc111c97":"Plotting the Regression Line","8e3eff79":"Now we'll check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), we'll plot a histogram.","60dd57f0":"> After dropping 'Humidity, we can see that VIF of temp came down to 5.17 from 17.79.\n\n> 'November' variable can also be dropped as its insignificant by looking at high p-value and quite low VIF.","6b4d469c":"<h2 style=\"color: Teal\">Step 10: Residual Analysis on the train data<\/h2>","54389273":"<h2 style=\"color: Teal\">Step 12: Model Evaluation<\/h2>","30b94668":"We'll be using a combination of feature selections, i.e. Automated selection (Recursive Feature Elimination) and Manual selection (VIF, p-value, etc)","d1b36314":"<h2 style=\"color: Teal\">Step 13: Interpretation<\/h2>","841ba9ef":"Difference in adjusted $R^2$ between train and test is \n\nAdj $R_{Train}^2$ - Adj $R_{Test}^2$ \n\n=> 81.9 - 78.8 = 3.1%","ba174b2d":"> The company can focus more on Temperature and provide offers or improve quality of service.\n\n> We can see demand for bikes was more in 2019 than 2018, hence we can observe that as the time moves forward BoomBikes was gaining popularity however, due to Corona Pandemic the business went down, but after proper advertisement and schemes they can re-gain popularity.\n\n> We can focus more on winter season, and the month(s) of September as they have good influence on bike rentals.\n\n> We can see month of July, weather conditions like light snow, Mist and cloudy, spring season and holidays have negative coefficients and negatively correlated to bike rentals. So we can give some offers there to increase the demand.\n\n> Obviously features like weather conditions and temperatures are not in our control, but we can install good grip tyres for the snow and maybe fog lights for proper visibility and indication to other moving vehicles, so that people feel safe while driving. \n\n> BoomBikes can also reduce price initially and then increase gradually as the demand increases for the features where there is negative correlations.\n\n> Holidays might be having negative correlations due to people going out to other places and offering people influencing offers or reducing prices during that time and then gradually increasing can attract customers.","31f8f4ea":"**Analysis of the above plots:**\n1. We can see that there are some independent variables positively correlated to the 'count' variable.\n2. We can also observe that bike rentals are more correlated to temp and atemp.\n3. Registered and count is also highly related which is quite obvious.\n4. We can also see that when humidity is 40+, registerations have been made. ","37cc0ec7":"**Henceforth, we can say that lm6, i.e. Linear Model 6 is our final model.**","9d0baf8f":"### Step 1.2: Data Sanity Check","10948cb1":"> The target variable should be demand of Bikes. Hence from the above data and provided description of the data, we can derive that the target variable is the column cnt.","69a0f7b9":"#### Checking VIF\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating VIF is:\n\n$ VIF_i = \\frac{1}{1 - {R_i}^2} $","56af647a":"All the continuous variables\/features are now mapped between 0 and 1 and are normalised.","d2aa0d32":"**Evaluate Adjusted R-square for test:**\n\n$ Adjusted R^2 = \\frac{1-(1-R^2)(n-1)}{(n-p-1)} $\n\nn = sample size, p = number of independent variables","99dfd542":"### Step 1.1: Understanding the data","ace59009":"> 'December' variable can be dropped as its insignificant by looking at very high p-value.","59d5f9d2":"> 'humidity' variable can be dropped as its insignificant by looking at very high VIF.","f475c3d3":"### Step 3.1: Continuous Variables","a1a4d293":"> \"Summer\" can also be dropped due to high p-value.","bfb1b238":"Making predictions using our model","55b3297e":"**The difference of $R^2$ and Adj $R^2$ between train and test is less that 5%, hence we can say that this is the BEST MODEL.**","e20242bb":"### Step 3.2: Categorical Variables","3c84b6a9":"**Analysis of the above model:**\n> The p-value for each term tests the null hypothesis that the coefficient is equal to zero (no effect). A low p-value (< 0.05) indicates that you can reject the null hypothesis.\n\n> VIF > 10 means we have high multicollinearity. However, in this case, with values less than 5, we are in good shape and can proceed with our regression.\n\n> R-squared measures the strength of the relationship between our model and the dependent variable on a convenient 0 \u2013 100% scale and we have the R-square value of 0.822 or 82.2%\n\n> The adjusted R-squared adjusts for the number of terms in the model and we got it around 0.819 or 81.9% (~82%)","35f558f7":"<h2 style=\"color: Teal\">Step 1: Loading and Understanding the Data<\/h2>","1c0a96d4":"### Step 9.1: Building a model using statsmodel"}}