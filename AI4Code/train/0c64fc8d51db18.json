{"cell_type":{"1ebbd525":"code","38f2f224":"code","ef190ac1":"code","c2db6d5d":"code","f61d7fcf":"code","5c4eefc1":"code","58a5fc8a":"code","ecf35e23":"code","9e5e3c2a":"code","dcfcf641":"code","d9947a7b":"code","bb08c4d1":"code","78c387e4":"code","d3a17ca2":"code","3407604b":"code","dd722f54":"code","6212d29f":"code","183d5b32":"code","257bb43a":"code","8de53af4":"code","cee30f61":"code","234f1951":"code","27bc31e1":"code","2e356aa7":"code","b39ace8c":"code","f5f68f98":"code","093d071f":"code","07d8bed4":"code","d2955a41":"code","3db4f30a":"code","80c56527":"code","e68d427d":"code","782acad6":"code","722c53a1":"code","5f88da39":"code","f72f1182":"code","0d930703":"code","a9b95f34":"code","1624b215":"code","ad9819e9":"markdown","b6f6d4de":"markdown","ca626060":"markdown","67ff2a12":"markdown","862e9793":"markdown","7cccd5c1":"markdown","0bb0c938":"markdown","f2057c1b":"markdown","8be59240":"markdown","b8a6de84":"markdown","ac5fd49f":"markdown","06667e14":"markdown","b07f7753":"markdown","13676484":"markdown","0e0b191a":"markdown","e758366a":"markdown","db3eec7b":"markdown","4667930f":"markdown"},"source":{"1ebbd525":"import pandas as pd\nimport numpy as np\nimport warnings","38f2f224":"# importing the dataset:\n\ndf = pd.read_csv('..\/input\/median-household-income-maryland-2018\/Combined.csv')","ef190ac1":"df.head()","c2db6d5d":"# checking count of missing values in the dataset:\n\ndf.isna().sum()","f61d7fcf":"# checking data types:\n\ndf.dtypes","5c4eefc1":"# checking dimensions:\n\ndf.shape","58a5fc8a":"df['geo']","ecf35e23":"dff = df.copy()\n\ndff['Proportion of Unemployed'] = dff['Unemployment'] \/ dff ['Labor Force']\ndff = dff.drop(columns=dff.columns[3])\ndff.head()","9e5e3c2a":"# Finding out IQR:\n\nQ1 = dff.quantile(0.25)\nQ3 = dff.quantile(0.75)\nIQR = Q3 - Q1\nprint(IQR)","dcfcf641":"# Removing outliers:\n\ndff1 = dff[~((dff < (Q1 - 1.5 * IQR)) |(dff > (Q3 + 1.5 * IQR))).any(axis=1)]\ndff1.shape","d9947a7b":"dff2 = pd.get_dummies(dff1['geo'], drop_first=True)\nfinal = pd.concat([dff1, dff2], axis=1)\nfinal.head()","bb08c4d1":"final = final.drop(columns=final.columns[0])","78c387e4":"# changing columns name:\n\nfinal.columns = ['Total Population, 2018', 'Labor Force',\n       'Median Household Income ($ Dollars)',\n       'Median Sale Price of a Home ($ Dollars)',\n       'Local Personal Income Tax Rate (%)',\n       'Bachelors Degree Attainment (%)',\n       'Average Travel Time to Work (Minutes)', 'Proportion of Unemployed',\n       'Anne Arundel County', 'Baltimore City', 'Calvert County',\n       'Caroline County', 'Carroll County', 'Cecil County', 'Charles County',\n       'Dorchester County', 'Frederick County', 'Garrett County',\n       'Harford County', 'Kent County', 'Queen Annes County',\n       'Somerset County', 'St. Marys County', 'Talbot County',\n       'Washington County', 'Wicomico County']","d3a17ca2":"#replacing string value in personal income tax rate:\n\nfinal['Local Personal Income Tax Rate (%)'] = final['Local Personal Income Tax Rate (%)'].replace(['3.05%'],3.05)\nfinal.head()","3407604b":"#changing datatypes:\n\nfinal['Personal Income Tax Rate'] = final['Local Personal Income Tax Rate (%)'].astype(float)\nfinal = final.drop(['Local Personal Income Tax Rate (%)'], axis = 1) \nfinal.head()","dd722f54":"import pandas.util.testing as tm\nimport seaborn as sns\n\nsns.pairplot(dff)","6212d29f":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndff_corr = dff.corr()\nmatrix = np.triu(dff.corr())\nplt.figure(figsize = (16,5))\nax = sns.heatmap(dff_corr, annot=True, mask = matrix) #notation: \"annot\" not \"annote\"\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)","183d5b32":"import seaborn as sns\nlst = ['Labor Force', 'Median Household Income ($ Dollars)', 'Personal Income Tax Rate',\n       'Bachelors Degree Attainment (%)',\n       'Average Travel Time to Work (Minutes)']\nfor i in lst:\n    sns.lmplot(x = i, y = 'Proportion of Unemployed' , data = final)","257bb43a":"#Separating the regressors and the regressands:\n\nY = final['Proportion of Unemployed']\nX = final.iloc[:, :24]","8de53af4":"# Checking for multi-collinearity:\n\nX.corr()","cee30f61":"final = final.drop(['Total Population, 2018', 'Median Sale Price of a Home ($ Dollars)'], axis=1)","234f1951":"#Train-Test split:\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\n\n\nprint('How many samples do we have in our test and train datasets?')\nprint('X_train: ', np.shape(x_train))\nprint('y_train: ', np.shape(y_train))\nprint('X_test: ', np.shape(x_test))\nprint('y_test: ', np.shape(y_test))\n","27bc31e1":"#Fitting the model:\n\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\n\nreg.fit(x_train, y_train)","2e356aa7":"# Checking model performance:\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nprint('MSE of test-set: ', mean_squared_error(y_test, reg.predict(x_test)))\nprint('R2 score of test-set: ', r2_score(y_test, reg.predict(x_test)))","b39ace8c":"from sklearn.linear_model import Ridge, RidgeCV\n\nridgecv = RidgeCV(alphas = [1,0.1,0.01,0.001,0.0001], scoring = 'neg_mean_squared_error', normalize = True)\nridgecv.fit(x_train, y_train) \nridgecv.alpha_","f5f68f98":"ridge4 = Ridge(alpha = ridgecv.alpha_, normalize = True)\nridge4.fit(x_train, y_train)\nprint('MSE: ', mean_squared_error(y_test, ridge4.predict(x_test)))\nprint('R2 score: ', r2_score(y_test, ridge4.predict(x_test)))\n","093d071f":"ridge4.fit(X, Y)\npd.Series(ridge4.coef_, index = X.columns)","07d8bed4":"from sklearn.linear_model import Lasso, LassoCV\nfrom sklearn.metrics import mean_squared_error, r2_score\n\nlassocv = LassoCV(alphas = None, cv = 10, max_iter = 100000, normalize = True)\nlassocv.fit(x_train, y_train)","d2955a41":"lasso = Lasso(max_iter = 10000, normalize = True)\n\nlasso.set_params(alpha=lassocv.alpha_)\nlasso.fit(x_train, y_train)\nprint('MSE: ', mean_squared_error(y_test, lasso.predict(x_test)))\nprint('R2 score: ', r2_score(y_test, lasso.predict(x_test)))","3db4f30a":"lasso.fit(X, Y)\npd.Series(lasso.coef_, index = X.columns)","80c56527":"from yellowbrick.regressor import PredictionError, ResidualsPlot\n","e68d427d":"#Prediction Error:\n\nvisualizer = PredictionError(reg)\nvisualizer.fit(x_train, y_train)  \nvisualizer.score(x_test, y_test)  \nvisualizer.poof()","782acad6":"#Residual Plot:\n\nvisualizer = ResidualsPlot(reg)\nvisualizer.fit(x_train, y_train)  \nvisualizer.score(x_test, y_test)  \nvisualizer.poof()","722c53a1":"ridge = Ridge()","5f88da39":"#Prediction Error:\n\nvisualizer = PredictionError(ridge)\nvisualizer.fit(x_train, y_train)  \nvisualizer.score(x_test, y_test)  \nvisualizer.poof()","f72f1182":"#Residual Plot:\n\nvisualizer = ResidualsPlot(ridge)\nvisualizer.fit(x_train, y_train)  \nvisualizer.score(x_test, y_test)  \nvisualizer.poof()","0d930703":"lasso = Lasso()","a9b95f34":"#Prediction Error:\n\nvisualizer = PredictionError(lasso)\nvisualizer.fit(x_train, y_train)  \nvisualizer.score(x_test, y_test)  \nvisualizer.poof()","1624b215":"#Residual Plot:\n\nvisualizer = ResidualsPlot(lasso)\nvisualizer.fit(x_train, y_train)  \nvisualizer.score(x_test, y_test)  \nvisualizer.poof()","ad9819e9":"From the correlation plot, we notice two things:\n\n    1) Total Population and Labor Force has a very high correlation [0.9],\n    2) Median Sale price of House and Median Household Income has a very high correlation [~0.9]\n    \n    \nSo, we drop Total Population and Median Sale price of House.","b6f6d4de":"# Linear Regression Model:","ca626060":"## Lasso Regression:","67ff2a12":"## Visualising the predictions:","862e9793":"## Linear Regression:","7cccd5c1":"It seems that almost all of our features have a linear association with proportion of unemployment. Thus, we will fit a **Linear Regression** algorithm.","0bb0c938":"# Data visualisation:","f2057c1b":"## Removing outliers:","8be59240":"Our linear regression model returned a very good fit for the train data (~ 1), but our test data accuracy has been very poor (). This indicates that our algorithm has **overfit** the data and it needs to be **regularized**. We appeal to both Ridge and Lasso regularization techniques and then conclude which technique yields in lower **variance**.","b8a6de84":"## Ridge Regression:","ac5fd49f":"# Conclusion:\n    \n1) Given the data on economy of Maryland, we tried to predict the proportion of unemployed people in the economy, where:\n          <br> <font color=blue>proportion of unemployed people = Unemployment $ \/ $ Labor Force<\/font>,\n          \n2) For the county feature, we used one hot encoding and created separate features,\n\n3) We then tried to visualise the relationship between proportion of unemployed people and the remaining continuous features. Our inference was that almost all of them had a linear one-to-one relationship.\n\n4) We then fit a Linear Regression algorithm and checked for its **Multi-collinearity** assumption.\n\n5) Although, our model returned a very good Train accuracy (_low bias_), there was very poor Test accuracy (_high variance_).\n\n6) Thus, we used _regularisation_ techniques on our linear model and concluded that **_Lasso Regression_** results in the best fit, in terms of variance and bias.","06667e14":"## Ridge Regression:","b07f7753":"## Linear Regression Model:","13676484":"## One hot encoding:","0e0b191a":"## Checking for linear relationships among variables:","e758366a":"# Feature engineering:","db3eec7b":"## Fitting the model:","4667930f":"## Lasso Regression:"}}