{"cell_type":{"e86a0a58":"code","e3b79b23":"code","81a4d798":"code","670ddc38":"code","60cb62e0":"code","3269f071":"code","baaec09d":"code","0d9e37d0":"code","9ea8014b":"code","74477027":"code","015e702d":"code","addba858":"code","19b79f34":"code","359a1dd6":"code","211b1b04":"code","f5038db6":"code","479cb49a":"code","6f7faac3":"code","9376ddf8":"code","ece6af30":"code","68d85215":"code","bd34e6de":"code","f614243f":"code","e680a01b":"code","5526719c":"code","8844a3dd":"code","823d8eba":"code","02e33dca":"code","05442815":"code","a616cfef":"code","80ca9adf":"code","5eba4185":"code","b1766293":"code","746ff3ce":"code","6db251f6":"code","ba2eb9b0":"code","60628828":"code","039ce87d":"code","0c71efad":"code","5586cb3b":"code","4dd30a67":"code","b1be0e84":"code","ba63ec42":"code","08f43439":"code","a518a1f8":"code","abcd4bf3":"code","d4195465":"code","75a80748":"code","586036f1":"code","0f47dada":"code","9954b07d":"code","6251e03d":"code","c0ca2d35":"code","5d4d9925":"code","5f4ca552":"code","d7bd509f":"code","ff35386e":"code","ac42eacb":"code","4d2c0d55":"code","d43196e6":"markdown","5e734c92":"markdown","8e23b5e3":"markdown","d937e156":"markdown","b371f6db":"markdown","7a50b603":"markdown","a5f62d27":"markdown","ac1d1071":"markdown","144fadc6":"markdown","6b36b718":"markdown","3b351f6c":"markdown","52c0aca5":"markdown","3b187ddd":"markdown","0ea47ebe":"markdown","4b5cd80b":"markdown","fb46b029":"markdown","f789b03a":"markdown","8154edb2":"markdown","f10da80a":"markdown","045b60b1":"markdown","d45227fd":"markdown","f733206e":"markdown","1ae2b852":"markdown","d262b974":"markdown","f2bf3c04":"markdown","3e2aaa83":"markdown","f1972eae":"markdown","c90707ac":"markdown","28a9dcdc":"markdown","92c1f5cd":"markdown","aa6fe1ce":"markdown","dd31e6aa":"markdown","2df8c796":"markdown","a34a16f0":"markdown","d8b3fff9":"markdown","67236cfa":"markdown","1e7be42f":"markdown","7ac153a9":"markdown","05b5dfe3":"markdown","3b60504e":"markdown","241592d4":"markdown","eecd0063":"markdown","b7e45f89":"markdown","bd4be385":"markdown","6b6a7291":"markdown","165edfae":"markdown","b5f71646":"markdown","893cf868":"markdown","d99e571b":"markdown","0c512836":"markdown","15a56981":"markdown","52ef980f":"markdown","806a9869":"markdown","f7fcbe32":"markdown","0b209abd":"markdown","e94aab6d":"markdown","4f46fd4b":"markdown","fb888ea0":"markdown","aefb7567":"markdown","d736362e":"markdown","901cd154":"markdown","8d4545b6":"markdown","c6645fc4":"markdown","4a6d9623":"markdown","2e7629d1":"markdown","8a698318":"markdown","cbd1c344":"markdown","73fde44f":"markdown","4df672aa":"markdown"},"source":{"e86a0a58":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","e3b79b23":"import shutil\nsrc=\"\/kaggle\/input\/cricketer-download-pictures-v4\"\ndest = \"\/kaggle\/working\/data\/\"\ndestination = shutil.copytree(src, dest)  \ndestination","81a4d798":"import os\narr = os.listdir(destination+\"train\")","670ddc38":"from fastai.vision import *\nfrom fastai.metrics import error_rate\nimport warnings\nwarnings.filterwarnings(\"ignore\")","60cb62e0":"path=\"\/kaggle\/working\/data\/train\/\"\nfnames = get_image_files(path+\"Virat Kolhi\")\nfnames[:5]","3269f071":"classes_dir=['other-cricketers','AB de Villiers', 'Brian Lara', 'Rahul Dravid', 'Rohit Sharma', 'Sachin Tendulkar', 'Shane Warne', 'Virat Kolhi']\n\n\nfor cls in classes_dir:\n    print(cls)\n    verify_images(path  + cls, delete=True)","baaec09d":"src = (ImageList.from_folder(path=path)\n       .split_by_rand_pct(0.2)\n       .label_from_folder()\n       )","0d9e37d0":"tfms = get_transforms(do_flip=False)","9ea8014b":"bs = 32\n# bs = 16  uncomment this line if you run out of memory even after clicking Kernel->Restart","74477027":"data = (src.transform(tfms, size=224)\n        .databunch(bs=64).normalize(imagenet_stats))","015e702d":"data.show_batch(rows=3, figsize=(7,6))","addba858":"print(data.classes)\nlen(data.classes),data.c","19b79f34":"learn = cnn_learner(data, models.resnet34, metrics=error_rate)","359a1dd6":"##learn.model","211b1b04":"learn.fit_one_cycle(6)","f5038db6":"## learn.fit_one_cycle(2)","479cb49a":"learn.save('stage-1')","6f7faac3":"interp = ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()\n\nlen(data.valid_ds)==len(losses)==len(idxs)","9376ddf8":"interp.plot_top_losses(9, figsize=(15,11))","ece6af30":"interp.plot_confusion_matrix(figsize=(12,12), dpi=60)","68d85215":"interp.most_confused(min_val=2)","bd34e6de":"learn.unfreeze()","f614243f":"learn.lr_find()","e680a01b":"learn.recorder.plot()","5526719c":"learn.fit_one_cycle(1, max_lr=slice(3e-5,1.5e-4))","8844a3dd":"learn.save('stage-2')","823d8eba":"learn.load('stage-2');","02e33dca":"interp = ClassificationInterpretation.from_learner(learn)","05442815":"interp.plot_confusion_matrix(figsize=(12,12), dpi=60)","a616cfef":"interp.plot_top_losses(9, figsize=(15,11))","80ca9adf":"from fastai.widgets import *","5eba4185":"data = (src.transform(tfms, size=224)\n        .databunch(bs=64).normalize(imagenet_stats))\n","b1766293":"#db = (ImageList.from_folder(path)\n#                   .split_none()\n#                   .label_from_folder()\n#                   .transform(get_transforms(), size=224)\n#                   .databunch()\n#     )\n\n#losses, idxs = interp.top_losses()\n#top_loss_paths = data.valid_ds.x[idxs]","746ff3ce":"learn_cln = cnn_learner(data, models.resnet34, metrics=error_rate)\n\nlearn_cln.load('stage-2');","6db251f6":"ds, idxs = DatasetFormatter().from_toplosses(learn_cln)","ba2eb9b0":"ImageCleaner(ds, idxs, path)","60628828":"ds, idxs = DatasetFormatter().from_similars(learn_cln)","039ce87d":"ImageCleaner(ds, idxs, path, duplicates=True)","0c71efad":"learn_cln.export()","5586cb3b":"#learn_cln.save('learn_cln_stage-3')","4dd30a67":"src = (ImageList.from_csv(path, 'cleaned.csv', folder='.')\n       .split_by_rand_pct(0.2)\n       .label_from_df()\n       )","b1be0e84":"data = (src.transform(tfms, size=229)\n        .databunch(bs=64).normalize(imagenet_stats))","ba63ec42":"learn_3 = cnn_learner(data, models.resnet34, metrics=error_rate)\nlearn_3.load('stage-2');","08f43439":"learn_3.fit_one_cycle(4)","a518a1f8":"learn_3.export()","abcd4bf3":"learn_3.unfreeze()","d4195465":"learn_3.lr_find()","75a80748":"learn_3.recorder.plot()","586036f1":"\nlearn_3.fit_one_cycle(2, max_lr=slice(1e-5,1.1e-4))\n","0f47dada":"learn_3.save('stage-3')","9954b07d":"learn_3.export()","6251e03d":"interp = ClassificationInterpretation.from_learner(learn)\n\nlosses,idxs = interp.top_losses()\n\nlen(data.valid_ds)==len(losses)==len(idxs)\n\ninterp.plot_top_losses(9, figsize=(15,11))","c0ca2d35":"interp.plot_confusion_matrix(figsize=(12,12), dpi=60)","5d4d9925":"data.classes","5f4ca552":"# fastai.defaults.device = torch.device('cpu')","d7bd509f":"img = open_image(path + '\/Virat Kolhi\/Virat Kolhi face_239.jpg')\nimg","ff35386e":"classes = ['AB de Villiers',\n 'Brian Lara',\n 'Other Cricketer',\n 'Rahul Dravid',\n 'Rohit Sharma',\n 'Sachin Tendulkar',\n 'Shane Warne',\n 'Virat Kolhi']\n\n\n#tfms = get_transforms(do_flip=False)\n#data = (src.transform(tfms, size=224)\n        #.databunch(bs=64).normalize(imagenet_stats))\n\ndata2 = ImageDataBunch.single_from_classes(path, classes,tfms, size=224).normalize(imagenet_stats)\nlearn_img = cnn_learner(data2, models.resnet34)\nlearn_img.load('stage-3')","ac42eacb":"pred_class,pred_idx,outputs = learn.predict(img)\npred_class,pred_idx,outputs","4d2c0d55":"classes [pred_idx]","d43196e6":"A common way for computer vision datasets is to use just one folder with a whole bunch of subfolders in it. So the interesting bit then is how do we get the labels. \n\nIn machine learning, the labels refer to the thing we are trying to predict. If we just eyeball this, we could immediately see that the labels are actually part of the Folder Name. We need to somehow get a list of label bits for each file.","5e734c92":"The first thing you're going to need to know is what were the classes that you trained with. You need to know not just what are they but what were the order. So you will actually need to serialize that or just type them in, or in some way make sure you've got exactly the same classes that you trained with.","8e23b5e3":"<a id='Data_Bunch'><\/a>\n### 4. Apply concept of Data Bunch for Image Classification","d937e156":"As we are working with the Kaggle Kernal - we start by moving the Training data from input folder to the working folder. (We do not have write access for the input folder)","b371f6db":"<a id='Exp_model'><\/a>\n### 13. Use Model in Production","7a50b603":"After you have cleaned up your noisy images, you can then retrain your model and hopefully you'll find it's a little bit more accurate. \n\nOne thing you might be interested to discover when you do this is it actually doesn't matter most of the time very much. On the whole, these models are pretty good at dealing with moderate amounts of noisy data. The problem would occur is if your data was not randomly noisy but biased noisy. ","a5f62d27":"<a id='Interp_matrix'><\/a>\n### 8. Interpret our Model Performance using loss function, Classification Matrix","ac1d1071":"So we run FileDeleter passing in that sorted list of paths and what pops up is basically the same thing as plot_top_losses. In other words, these are the ones which is either wrong about or least confident about. \n\nSome of them shouldn't be in our dataset. So what I do is I click on the delete button, \n\nI'll keep going confirm until I get to a coupe of screen full of the things that all look okay and that suggests to me that I've got past the worst bits of the data. ","144fadc6":"If you don't have a GPU on your server, it will use the CPU automatically. If you have a GPU machine and you want to test using a CPU, you can just uncomment this line and that tells fastai that you want to use CPU by passing it back to PyTorch.","6b36b718":"For now, just know that to create a learner for a convolutional neural network, you just have to tell it two things: data: \n- What's your data (data bunch).  \n- What's your architecture. There are lots of different ways of constructing a convolutional neural network.\n\n","3b351f6c":"We can see that some images have multiple cricketers and have been misclassified.","52c0aca5":"I am a Virat Kolhi fan. So lets get an image of Virat and see if our classifier detects him","3b187ddd":"Data.show_batch can be used to show some of the contents in my data bunch, they all seem to have being zoomed and cropped in a reasonably nice way. So basically what it'll do is something called by default center cropping which means it'll grab the middle bit and it'll also resize it. ","0ea47ebe":"### In the code below we are going to classify images on the Data we downloaded from from Google Images using this [Kernal](https:\/\/www.kaggle.com\/anandpuntambekar\/cricketer-download-pictures-v4)\n\n### While doing so we shall understand in detail every line of code for creating the Image classifier","4b5cd80b":"To see what comes out, we could use this class for class interpretation. We are going to use this factory method from learner, so we pass in a learn object. Remember a learn object knows two things:\n- What's your data\n- What is your model. Now it's not just an architecture, it's actually a trained model\n\nThat's all the information we need to interpret that model.\n\n#### Loss Function:\n\nA loss function is something that tells you how good was your prediction. Specifically it means if you predicted one class with great confidence, but actually you were wrong, then that's going to have a high loss because you were very confident about the wrong answer. \n\nBy plotting the top losses, we are going to find out what were the things that we were the most wrong on, or the most confident about what we got wrong.","fb46b029":"- size: what size images do you want to work with.\n- bs: batch size (i.e. number of images processed at a time). Set to 16 if memory is not enough.\n- tfms: we'll talk about transforms later - basically what it'll do is something called by default center cropping which means it'll grab the middle bit and it'll also resize it.","f789b03a":"<a id='Imp_Perf'><\/a>\n### 9. Improve model performance with Differential Learning Rate","8154edb2":"While defining the DataBunch We also have to create the Validation Data set. Overfitting is where you do well on the training data , but dont have good performance outside the training data. The way to do that is using something called a validation set. \n\nA validation set is a set of images that your model does not get to look at. We shall print the error rate for validation set - a set of images that our model never got to see. When we created our data bunch, it automatically created a validation set for us. In this case we randomly choose 20% pictures for validation Data set using the split_by_rand_pct","f10da80a":"For the vast majority of things you in production on a CPU. GPU is good at doing lots of things at the same time, but unless  you're going to have 64 images to classify at the same time to put into a batch into a GPU you will not need it.\n\nIt's much easier if you just wrap one thing, throw it at a CPU to get it done, and comes back again. \n\nYes, it's going to take maybe 10 or 20 times longer so maybe it'll take 0.2 seconds rather than 0.01 seconds. ","045b60b1":"#### Architecture:\n\nArchitecture defines the various layers involved in the machine learning cycle and involves the major steps being carried out in the transformation of raw data into training data sets capable for enabling the decision making of a system.\n\nA particular kind of architecture called ResNet which works extremely well. For a while, at least, you really only need to choose between ResNet34 and ResNet50.\n\nWhen we are getting started pick a smaller one because it'll train faster. \n","d45227fd":"We save the model in order to use it later if required","f733206e":"Warning messages are typically issued in situations where it is useful to alert the user of some condition in a program, where that condition (normally) doesn\u2019t warrant raising an exception and terminating the program. For example, one might want to issue a warning when a program uses an obsolete module\n\nThe determination whether to issue a warning message is controlled by the warning filter,\n\nThe \"ignore\" input stands for never print matching warnings\n\nhttps:\/\/docs.python.org\/3\/library\/warnings.html","1ae2b852":"#### Size:\n\nThe is a shortcoming of current deep learning technology which is that a GPU has to apply the exact same instruction to a whole bunch of things at the same time in order to be fast. If the images are different shapes and sizes, you can't do that. So we have to make all of the images the same shape and size.\n\n\nA commom approach is to use images of size 224 by 224,  most models tend to use so it . If you just use size=224, you're probably going to get pretty good results most of the time.  ","d262b974":"We are going to work with the fastai V1 library which sits on top of Pytorch 1.0. The fastai library provides many useful functions that enable us to easily build neural networks and train our models.\n\nWe are going to work with the libraries asscoicated with Computer vision for fastai","f2bf3c04":"Lets discuss a few parameters we shall be using in order to train our model","3e2aaa83":"And as per usual, we created a data bunch, but this time, we're not going to create a data bunch from a folder full of images, we're going to create a special kind of data bunch which is one that's going to grab one single image at a time. So we're not actually passing it any data. The only reason we pass it a path is so that it knows where to load our model from. That's just the path that's the folder that the model is going to be in.\n\nWe need to pass this image through the same information that we trained with. So the same transforms, the same size, the same normalization.","f1972eae":"We can see that the classifier says its more than 93% probhability that its cattgory no 7 - which is","c90707ac":"#### Transfer Learning:\n\nThe first time we run the commant below it downloads the ResNet34 pre-trained weights. \n\nThis particular model has actually already been trained on looking at about one and a half million pictures of all kinds of different things, a thousand categories of things, using an image dataset called ImageNet. \n\nWe download those pre-trained weights so that we don't start with a new model that knows nothing about anything, \n\nWe actually start with a model that knows how to recognize a thousand categories of things in ImageNet. \n\nThis is because the initial Layers of the pretrained model are used to indtify patterns such as lines and curves. These weights can be reused in our case, saving us significant about of training time.","28a9dcdc":"<a id='Imp_Label'><\/a>\n### 10. Improve model performance with Cleaning Labels","92c1f5cd":"<a id='Data_Aug'><\/a>\n### 11. Apply Data Augmentation to improve performance further","aa6fe1ce":"This new widget this week called the FileDeleter. \n\nThere is not just plot_top-losses but there's also top_losses and top_losses returns two things: the losses of the things that were the worst and the indexes into the dataset of the things that were the worst. If you don't pass anything at all, it's going to actually return the entire dataset, but sorted so the first things will be the highest losses. \n\nEvery dataset in fastai has x and y and the x contains the things that are used to, in this case, get the images. So this is the image file names and the y's will be the labels. So if we grab the indexes and pass them into the dataset's x, this is going to give us the file names of the dataset ordered by which ones had the highest loss (i.e. which ones it was either confident and wrong about or not confident about). So we can pass that to this new widget that they've created.","dd31e6aa":"Lets have a look at the image names, get_image_files will grab an array of all of the image files as shown below.","2df8c796":"Let's get our hands dirty \n\nWe are going to do create our own classifier with own images. Images have been downloaded using the kernal : https:\/\/www.kaggle.com\/anandpuntambekar\/cricketer-download-pictures-v4","a34a16f0":"### Interpretation","d8b3fff9":"#### Learners\n\nLearner: A general concept for things that can learn to fit a model. From that, there are various subclasses to make things easier in particular, there is a convnet learner (something that will create a convolutional neural network for you).","67236cfa":"Lets us freeze the weights to see if we can increase the accuracy even further","1e7be42f":"We can improve it by using more layers and we will do this next week but by basically doing a ResNet50 instead of ResNet34.\n\nWhat you'll find is it's very likely if you try to do this, you will get an error and the error will be your GPU has ran out of memory.\n\n\nIt's very likely that if you try to run this, you'll get an out of memory error and that's because it's just trying to do too much - too many parameter updates for the amount of RAM you have. That's easily fixed - batch size. This basically says how many images do you train at one time. If you run out of memory, just make it smaller.\n\nIt's fine to use a smaller batch size. It might take a little bit longer. That's all.","7ac153a9":"Will launch a training using the 1cycle policy to help you train your model faster.\n\nThe number, 6, decides how many times do we show the dataset to the model so that it can learn from it. ( Is called Epoch Number)\n\nEach time it sees a picture, it's going to get a little bit better. But it's going to take time and it means it could overfit. \n\nIf it sees the same picture too many times, it will just learn to recognize that picture. We got an error rate of 8.8%.","05b5dfe3":"So 91% of the time, we correctly picked the exact right one of those 8 cricketers. So ResNet tends to work pretty well across a wide range of different kind of details around choices that you might make. \n\nWe basically found some coefficients and parameters that work pretty well and it took us a minute and 56 seconds. So if we want to start doing some more playing around and come back later, we probably should save those weights. You can just go learn.save and give it a name. ","3b60504e":"Learning rate finder, what you are looking for is the strongest downward slope that's kind of sticking around for quite a while","241592d4":"Its Game time, you've got yourmodel, you saved those weights, and how are you going to detect if its Virat or Dhoni ?","eecd0063":"<a id='Model_Img'><\/a>\n### 12. Apply Model on Single Image\n\n","b7e45f89":"We shall use export.pkl to deploy our Model in the next Blog","bd4be385":"We see that the error has dropped to 4.3 % on the validation set","6b6a7291":"As many of you may know % are special directives to Jupyter Notebook itself, they are not Python code. They are called \"magics.\"\n\nThese magic commands are intended to solve common problems in data analysis using Python. In fact, they control the behaviour of IPython itself.\n\nMagic commands act as convenient functions where Python syntax is not the most natural one.\n\n- If somebody changes underlying library code while I'm running this, please reload it automatically\n- If somebody asks to plot something, then please plot it here in this Jupyter Notebook\n\nWe starts with the following three lines; ","165edfae":"<a id='Work_Image_classfication'><\/a>\n### 2. Work with Data in a popular Fomat used for Image classfication(Train, Validation, Test)\n","b5f71646":"Before Proceeding forward lets talk about learning rate -\n\n#### Learning rate- \n\nThe learning rate basically says how quickly am I updating the parameters in my model parameters\n\nBecause we are trying to fine-tune things now, we can't use learning rate. So based on the learning rate finder, we will a rate well before it started getting worse. So I decided to pick 1e-6. \n\n#### Differential Learning Rate\nThere's no point training all the layers at the same rate, because we know that the later layers worked just fine before when we were training much more quickly. So what we can actually do is we can pass a range of learning rates to learn.fit_one_cycle. :\n\n","893cf868":"<a id='Clean_Images'><\/a>\n\n### 3. Clean the Inut Data Set - remove the images that aren't actually images at all","d99e571b":"In the code below we shall \n\n- Use same image source , but use size 229 , instead of 224.\n- Use Tranfer learning by loading the weights of learn_cln_stage-3 and fine tuning the same","0c512836":"The widget will not delete images directly from disk but it will create a new csv file cleaned.csv from where you can create a new ImageDataBunch with the corrected labels to continue training your model.","15a56981":"#### Data augmentation\n\nData augmentation is a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset. Image data augmentation is used to expand the training dataset in order to improve the performance and ability of the model to generalize.","52ef980f":"### Going forward we shall perform the following actions\n\n[1. Getting started and Data Set up](#Start_Data)\n\n\n[2. Work with Data in a popular Fomat used for Image classfication(Train, Validation, Test)](#Work_Image_classfication)\n\n\n[3. Clean the Inut Data Set - remove the images that aren't actually images at all](#Clean_Images)\n\n\n[4. Apply concept of Data Bunch for Image Classification](#Data_Bunch)\n\n\n[5. Apply concept of Normalization, Batch Size, Data Augmentation on Data Bunch](#Apply_Augmentation)\n\n\n[6. Apply concepts associated to Learners, Architecture, Transfer Learning](#Apply_Learning)\n\n\n[7. Fit Model on Training Data, Save Model](#Fit_Model)\n\n\n[8. Interpret our Model Performance using loss function, Classification Matrix](#Interp_matrix)\n\n\n[9. Improve model performance with Differential Learning Rate](#Imp_Perf)\n\n\n[10. Improve model performance with Cleaning Labels](#Imp_Label)\n\n\n[11. Apply Data Augmentation to improve performance further](#Data_Aug)\n\n\n[12. Apply Model on Single Image](#Model_Img)\n\n\n[13. Use Model in Production](#Exp_model)\n\n\n\n","806a9869":"\nIn order to create a model in fastai we will need to create a DataBunch object, in our case the src variable.\n\nIn fastai, everything we model with is going to be a DataBunch object. DataBunch object contains 2 or 3 datasets - \n- Training data\n- Validation data\n- and optionally test data.\n\nFor each of those, it contains your images and your labels, your texts and your labels, or your tabular data and your labels, or so forth. And that all sits there in this one place","f7fcbe32":"Lets Take a look at All folder Names in the Train Folder we just created","0b209abd":"- We see that the error rate has decreased from 8.8% to 3.9%","e94aab6d":"<a id='Fit_Model'><\/a>\n### 7. Fit Model on Training Data , Save Model","4f46fd4b":"Maybe Google image search didn't give you exactly the right images all the time.  We want to clean it up. So combining human expert with a computer learner is a really good idea. \n\nWe are goin to use the plot top losses ( images which were either the most wrong about or the least confident about) and decide which of those are noisy. \n\nIt's very unlikely that if there is a mislabeled data that it's going to be predicted correctly and with high confidence.","fb888ea0":"This will create a file named 'export.pkl' in the directory where we were working that contains everything we need to deploy our model (the model, the weights but also some metadata like the classes or the transforms\/normalization used).\n\nYou probably want to use CPU for inference, except at massive scale (and you almost certainly don't need to train in real-time). If you don't have a GPU that happens automatically. You can test your model on CPU like so:","aefb7567":"#### Data augmentation :\n\nIt is perhaps the most important regularization technique when training a model for Computer Vision: instead of feeding the model with the same pictures every time, we do small random transformations (a bit of rotation, zoom, translation, etc...) that don't change what's inside the image (to the human eye) but do change its pixel values. Models trained with data augmentation will then generalize better.\n\nget_transforms is a set of transforms with default values that work pretty well in a wide range of tasks, it's often easiest to use get_transforms. Depending on the nature of the images in your data, you may want to adjust a few arguments, the most important being:\n\ndo_flip: if True the image is randomly flipped (default behavior)\nflip_vert: limit the flips to horizontal flips (when False) or to horizontal and vertical flips as well as 90-degrees rotations (when True)","d736362e":"#### Normalization:\n\nNormalizing the images- means that the pixel values start out from 0 to 255. And some channels might tend to be really bright, some might tend to be really dull, some might vary a lot, and some might not very much at all. \n\nIt really helps train a deep learning model if each one of those red green and blue channels has a mean of zero and a standard deviation of one.\n\nIf your data is not normalized, it can be quite difficult for your model to train well. So if you have trouble training a model, one thing to check is that you've normalized it.\n\nGenerally in nearly all machine learning tasks, you have to make all of your data about the same \"size\" - they are specifically about the same mean and standard deviation. So there is a normalize function used to normalize our data bunch helps us do so.","901cd154":"Let's make our model better.\n\nWe can make it better by using fine-tuning. So far we fitted 4 epochs and it ran pretty quickly. The reason it ran pretty quickly is that there was a little trick we used. These convolutional networks, they have many layers. What we did was we added a few extra layers to the end and we only trained those.\n\nWe basically left most of the model exactly as it was. If we are trying to build a model at something that's similar to the original pre-trained model (in this case, similar to the ImageNet data), that works pretty well.\n\nBut what we really want to do is to go back and train the whole model. This is why we pretty much always use this two stage process. By default, when we call fit or fit_one_cycle on a ConvLearner, it'll just fine-tune these few extra layers added to the end and it will run very fast. It will basically never overfit but to really get it good, you have to call unfreeze. unfreeze is the thing that says please train the whole model. Then we can call fit_one_cycle again.","8d4545b6":"The next thing we needed to do is to remove the images that aren't actually images at all. There's always a few images in every batch that are corrupted . Usually the URL had an image but it doesn't anymore.\n\nWe use verify_images which will check all of the images in a path and will tell you if there's a problem. If you set delete=True, it will actually delete it for us in order to get a clean Data set","c6645fc4":"The other thing we want to do is to look at the labels. All of the possible label names are called your classes. With DataBunch, you can print out your data.classes.","4a6d9623":"<a id='Start_Data'><\/a>\n### 1. Getting started and Data Set up","2e7629d1":"Putting your model in production","8a698318":"You use this keyword in Python called slice and that can take a start value and a stop value and basically what this says is train the very first layers at a learning rate of 1e-6, and the very last layers at a rate of 1e-4, and distribute all the other layers across that (i.e. between those two values equally).\n\nA good rule of thumb is after you unfreeze, pass a max learning rate parameterin the slice, make the second part of that slice about 10 times smaller than your first stage. \n\nOur first stage defaulted to about 1e-3 so it's about 1e-4. And the first part of the slice should be a value from your learning rate finder which is well before things started getting worse. So you can see things are starting to get worse maybe about here:\n\n","cbd1c344":"<a id='Apply_Learning'><\/a>\n### 6. Apply concepts associated to Learners, Architecture, Transfer Learning","73fde44f":"<a id='Apply_Augmentation'><\/a>\n### 5. Apply concept of Normalization, Batch Size, Data Augmentation on Data Bunch","4df672aa":"#### Batch Size:\n\nIn order to avoid memory error( too many parameter updates for the amount of RAM ) - batch size needs to be fixed. \n\nBatch Size basically says how many images do you train at one time. If you run out of memory, just make it smaller."}}