{"cell_type":{"7757ef00":"code","b4de0fab":"code","2ee9c09c":"code","8a087262":"code","631dee0b":"code","cf9ba156":"code","7a01d9cd":"code","a759a7be":"code","c9fd2ed8":"code","3242ef52":"code","03c14cd7":"code","250b8682":"code","3c7da4d7":"code","2066dea6":"code","59afa2a1":"code","7f7bf416":"code","3f1d16ff":"code","a3665f9b":"markdown","38d7ea91":"markdown","9dad8a60":"markdown","e796e8c8":"markdown","65354a63":"markdown","66d10d79":"markdown","73a0cb12":"markdown","b720c6bf":"markdown","e301cd5f":"markdown","38dcbe07":"markdown","e195251d":"markdown"},"source":{"7757ef00":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b4de0fab":"train_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv',sep=',')\ntest_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv',sep=',')","2ee9c09c":"train_df.head()","8a087262":"test_df.head()","631dee0b":"train_data = np.array(train_df, dtype = 'float32')\ntest_data = np.array(test_df, dtype='float32')","cf9ba156":"x_train = train_data[:,1:]\/255 #Skip 1st column as it is a label data\ny_train = train_data[:,0] # 1st column is label\nx_test= test_data[:,:]\/255","7a01d9cd":"from sklearn.model_selection import train_test_split\nx_train,x_validate,y_train,y_validate = train_test_split(x_train,y_train,test_size = 0.2,random_state = 1)\nprint(\"x_train shape: \" + str(x_train.shape))\nprint(\"x_validate shape: \" + str(x_validate.shape))\nprint(\"x_test shape: \" + str(x_test.shape))\nprint(\"y_train shape: \" + str(y_train.shape))\nprint(\"y_validate shape: \" + str(y_validate.shape))","a759a7be":"height = width = 28\nx_train = x_train.reshape(x_train.shape[0],height,width,1)\nx_validate = x_validate.reshape(x_validate.shape[0],height,width,1)\nx_test = x_test.reshape(x_test.shape[0],height,width,1)\nprint(\"x_train shape: \" + str(x_train.shape))\nprint(\"x_validate shape: \" + str(x_validate.shape))\nprint(\"x_test shape: \" + str(x_test.shape))","c9fd2ed8":"from keras.preprocessing.image import ImageDataGenerator\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\ndatagen.fit(x_train)","3242ef52":"from keras.models import Sequential\nfrom keras.layers import Activation,Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3),kernel_initializer='glorot_uniform',input_shape=(height, width, 1),name='conv0'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2),name='max_pool0'))\nmodel.add(Dropout(0.25))\n          \nmodel.add(Conv2D(64, kernel_size=(3, 3), name='conv1'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2),name='max_pool1'))\nmodel.add(Dropout(0.25))\n          \nmodel.add(Conv2D(128, (3, 3), activation='relu', name='conv2'))\n\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu',name = 'fc'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(10, activation='softmax'))\nmodel.summary()","03c14cd7":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom keras.utils import plot_model\n\nplot_model(model,to_file='model.png',show_shapes = True, show_layer_names = True)\nimg = mpimg.imread('model.png')\nplt.figure(figsize =(40,40))\nplt.imshow(img)","250b8682":"model.compile(loss ='sparse_categorical_crossentropy', optimizer= 'Adam',metrics =['accuracy'])","3c7da4d7":"from keras.callbacks import ModelCheckpoint\ncheckpoint = ModelCheckpoint(filepath = 'cnn.hdf5', verbose = 1, save_best_only = True)","2066dea6":"history = model.fit_generator(datagen.flow(x_train, y_train, batch_size= 128), epochs = 50, verbose=1,callbacks = [checkpoint],validation_data=(x_validate,y_validate))","59afa2a1":"predicted_classes = model.predict_classes(x_test)\nprint(predicted_classes)","7f7bf416":"sample_submission = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv',sep = ',')\nsample_submission.head()","3f1d16ff":"submission  = pd.DataFrame({\n    \"ImageId\": range(1,x_test.shape[0] + 1),\n    \"Label\": predicted_classes\n})\nsubmission.to_csv(\"submission.csv\", index=False)\ndisplay(submission.head(3))\ndisplay(submission.tail(3))","a3665f9b":"Let's perform data augmentation for training data.<br>\nUsing this CNN model will see different set of images during each epoch training.<br>\nThis helps to generalize the data set which helps to improve test accuracy with a liite loss on training accuracy.<br>\n[Image Data Augmentation using Keras](https:\/\/machinelearningmastery.com\/image-augmentation-deep-learning-keras\/)<br>\n[Keras official documentation](https:\/\/keras.io\/preprocessing\/image\/)","38d7ea91":"Separate label data as y and image data as x and rescale it to (0,1) range from (0,255) range","9dad8a60":"Let's read the csv files using pandas read_csv which provides data in form of pandas dataframe.\nPandas offers lots of cool features to deal with csv data. [Official Documentation](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html)<br>\nLet's have a look at some features:\n1.  By default read_csv consider 1st column as header and title for the column names. But it can be customized by passing an array as name.<br>\ndata = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv',sep=',',name = ['name','Age','height','weight','Employee No'])\n\n2.  Rows can be skipped as follows and it also support callbacks.<br>\ndata = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv',sep=',',skiprows = 2) # From top it will skip 3 rows<br>\ndata = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv',sep=',',skipfooter = 2) # From bottom it will skip 3 rows<br>\ndata = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv',sep=',',nrows  = 5) # Only read 5 rows<br>\n\n3. Only read a subset of the columns using usecols.<br>\ndata = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv',sep=',',usecols = ['name','Employee No'])\n\n4. For handling very large csv files, it can be read in chunks by specifying chunksize. It returns TextFileReader object.<br>\ndata = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv',sep=',',chunksize  = 5)<br>\nfor chunk in data:<br>\n$\\;\\;\\;\\;\\;\\;$print(chunk)<br>\n\n5. Handle NaN values.<br>\nClassify additional strings as NaN.<br>\ndata = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv',sep=',',na_values = ['Not Applicable','TBD'])\n\nDelete the rows which contains NaN data using dropna()<br>\ndata.dropna(inplace = False)\n\nOr the better way is to replace NaN with a number depends on the data set.<br>\ndata['weight'].fillna(50, inplace=True)","e796e8c8":"Have a look at train and test data.","65354a63":"Split training data as 80% training set and 20% validation set using scikit learn's train_test_split method.<br>\nValidation set will not update weights and bias of the neural network.<br>\nIt is used to verify neural network performance before validating on test data.<br>","66d10d79":"Convert dataframes into numpy array so it can be feed to the convolution neural network which will be created using tensorflow and keras.","73a0cb12":"Save the model when the lowest loss is achieved during epochs training iterations.","b720c6bf":"Based on the trained model let's predict digits from the test data","e301cd5f":"Let's have a look at submission.csv file.","38dcbe07":"Reshape the x data in the format -> (number of examples, height, width, channel)<br>\nThese are not RGB images hence channel is 1.","e195251d":"Let's visualize the model"}}