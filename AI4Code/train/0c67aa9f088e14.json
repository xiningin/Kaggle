{"cell_type":{"ef147720":"code","36902025":"code","70a5cce3":"code","ca104532":"code","5b629a7e":"code","a5de7747":"code","3634e31f":"code","a9b8d2e4":"code","ea956737":"code","093437d1":"code","4e9995e2":"code","4a62e8f0":"code","aecce3c5":"markdown","b4043b81":"markdown","02a6187b":"markdown"},"source":{"ef147720":"!pip install pyspark","36902025":"from pyspark.ml.evaluation import RegressionEvaluator, MulticlassClassificationEvaluator\nfrom pyspark.ml.recommendation import ALS\nfrom pyspark.ml.tuning import ParamGridBuilder, CrossValidator\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as f\nfrom pyspark.sql.functions import avg, when, explode\nfrom pyspark.sql.functions import to_timestamp\nfrom pyspark.sql.functions import udf, regexp_extract\nfrom pyspark.sql.types import IntegerType,FloatType\nfrom pyspark.ml.linalg import DenseVector\nfrom pyspark.mllib import linalg as mllib_linalg\nfrom pyspark.ml import linalg as ml_linalg\nfrom pyspark.sql.functions import lower, col\n\n\n\nimport html\nfrom pyspark.ml.feature import StopWordsRemover, Tokenizer, HashingTF, IDF\nfrom pyspark.ml.feature import RegexTokenizer,  CountVectorizer, OneHotEncoder, StringIndexer, VectorAssembler,  Word2Vec\nfrom pyspark.ml.classification import LogisticRegression,NaiveBayes,DecisionTreeClassifier,RandomForestClassifier\nfrom pyspark.mllib.regression import LabeledPoint\nfrom pyspark.mllib.classification import LogisticRegressionWithLBFGS\nfrom pyspark.ml import Pipeline\nimport pandas as pd\nimport seaborn as sns\n\nfrom pyspark.sql.functions import *\n","70a5cce3":"from pyspark.conf import SparkConf\nspark=SparkSession.builder \\\n        .config(\"spark.executor.memory\", \"4g\") \\\n        .config(\"spark.driver.memory\", \"4g\") \\\n        .appName(\"hw4\") \\\n        .getOrCreate()","ca104532":"#spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n#schema_dll = \"polarity FLOAT, id LONG, date_time STRING, query STRING, user STRING, text, STRING\"\n#spark_reader = spark.read.schema(schema_dll)\n#simple_date_format =  \"EEE MMM dd HH:mm:ss zzz yyyy\"\ntrain = (\n    spark.read.csv(\n        path=\"..\/input\/it2034ch1502-nlp\/train.csv\",\n        header=True,\n        inferSchema = True,\n       \n    )\n    .cache()\n)\ntest = (\n    spark.read.csv(\n        path=\"..\/input\/it2034ch1502-nlp\/test.csv\",\n        header=True,\n        inferSchema = True,\n       \n    )\n    .cache()\n)","5b629a7e":"def clean_data_(df):\n  @udf\n  def html_unescape(s: str):\n    return html.unescape(s)\n  #w = df.withColumn(\"date_time\",to_timestamp(df.date_time,\"EEE MMM dd HH:mm:ss zzz yyyy\").cast(\"timestamp\"))\n  d= df.withColumn(\"text\",html_unescape(\"text\"))\n  user_regex = r\"(@\\w{1,15})\"\n  c = d.withColumn(\"text\",f.regexp_replace(f.col(\"text\"), user_regex, \"\")\n    )\n  hashtag_regex = r\"\\B\\#(\\w{1,})\"#(\\#[a-zA-Z]+\\b)(?!;)\"\n\n  c = c.withColumn(\"text\",f.regexp_replace(f.col(\"text\"), hashtag_regex, \"\")\n    )\n  #Removing url and email from all the tweets\n\n  url_regex=r\"((https?|file|ftp):\\\/{2,3})+([-\\w+&@#\/%=~|$?!:,.]*)|(www.)+([-\\w+&@#\/%=~|$?!:,.]*)\"\n  email_regex= r\"[\\w.-]+@[\\w.-]+\\.[a-zA-Z]{1,}\"\n  c=c.withColumn(\"text\",\n    f.regexp_replace(f.col(\"text\"), email_regex, \"\"))\n  c=c.withColumn(\"text\",    f.regexp_replace(f.col(\"text\"), url_regex, \"\")\n    \n    \n)\n\n  number = r\"[^a-zA-Z']\"\n  space = r\" +\"\n  clean_data = (c.withColumn(\"text\",f.regexp_replace(f.col(\"text\"), number, \" \"))\n              .withColumn(\"text\",f.regexp_replace(f.col(\"text\"), space, \" \"))\n              .withColumn(\"text\",f.trim(f.col(\"text\")))\n              .withColumn(\"text\", f.lower(f.col(\"text\")))\n              .filter(f.col(\"text\") != '')\n              )\n  \n  return clean_data\n","a5de7747":"(training,  testing) = clean_data_(train).randomSplit([0.98, 0.2], seed=2020)\n","3634e31f":"#----------------Define tokenizer with regextokenizer()------------------\nregex_tokenizer = RegexTokenizer(pattern='\\\\W')\\\n                  .setInputCol(\"text\")\\\n                  .setOutputCol(\"tokens\")\n\n#----------------Define stopwords with stopwordsremover()---------------------\nextra_stopwords = ['http','amp','rt','t','c','the']\nstopwords_remover = StopWordsRemover()\\\n                    .setInputCol('tokens')\\\n                    .setOutputCol('filtered_words')\\\n                    .setStopWords(extra_stopwords)\n                    \n\n#----------Define bags of words using countVectorizer()---------------------------\ncount_vectors = CountVectorizer(vocabSize=10000, minDF=5)\\\n               .setInputCol(\"filtered_words\")\\\n               .setOutputCol(\"features\")\n\n\n#-----------Using TF-IDF to vectorise features instead of countVectoriser-----------------\nhashingTf = HashingTF(numFeatures=10000)\\\n            .setInputCol(\"filtered_words\")\\\n            .setOutputCol(\"raw_features\")\n            \n#Use minDocFreq to remove sparse terms\nidf = IDF(minDocFreq=5)\\\n        .setInputCol(\"raw_features\")\\\n        .setOutputCol(\"features\")\n\n#---------------Define bag of words using Word2Vec---------------------------\nword2Vec = Word2Vec(vectorSize=1000, minCount=0)\\\n           .setInputCol(\"filtered_words\")\\\n           .setOutputCol(\"features\")\n\n#-----------Encode the Category variable into label using StringIndexer-----------\nlabel_string_idx = StringIndexer()\\\n                  .setInputCol(\"Category\")\\\n                  .setOutputCol(\"label\")\n\n#-----------Define classifier structure for logistic Regression--------------\nlr = LogisticRegression(maxIter=200, regParam=0.1, elasticNetParam=0, labelCol='polarity')\n\n#---------Define classifier structure for Naive Bayes----------\nnb = NaiveBayes(smoothing=1, labelCol='polarity')\n\n","a9b8d2e4":"def result(predictions):\n    AccuracyEval = MulticlassClassificationEvaluator(labelCol = \"polarity\", metricName=\"accuracy\")\n    PrecisionEval = MulticlassClassificationEvaluator(labelCol = \"polarity\", metricName=\"weightedPrecision\")\n    RecallEval = MulticlassClassificationEvaluator(labelCol = \"polarity\", metricName=\"weightedRecall\")\n    F1Eval = MulticlassClassificationEvaluator(labelCol = \"polarity\", metricName=\"f1\")\n \n    print(\"K\u1ebft qu\u1ea3 \u0111\u00e1nh gi\u00e1:\")\n    print(\"Accuracy = %g\" %(AccuracyEval.evaluate(predictions)))\n    print(\"Precision = %g\" %(PrecisionEval.evaluate(predictions)))\n    print(\"Recall = %g\" %(RecallEval.evaluate(predictions)))\n    print(\"F1 Score = %g\" %(F1Eval.evaluate(predictions)))","ea956737":"pipeline_cv_lr = Pipeline().setStages([regex_tokenizer,stopwords_remover,count_vectors, lr])\nmodel_cv_lr = pipeline_cv_lr.fit(training)\npredictions_cv_lr = model_cv_lr.transform(testing)\nresult(predictions_cv_lr)","093437d1":"def pridict(df):\n\n  def func(prob):\n      if prob > 0.4 and prob <0.54 :\n          return 2\n      if prob>0.54:\n          return 0\n      return 4\n  func_udf = f.udf(func, IntegerType())\n  udf1 = f.udf(lambda x: float(x[0]),FloatType())\n  test = df.withColumn(\"probability_1\",udf1('probability'))\n  final = test.withColumn('new_column',func_udf(test['probability_1']))  \n  return final                                     \n","4e9995e2":"test2 = clean_data_(test)\nresults =  model_cv_lr.transform(test2)\n#final_data = results.select('id', 'prediction').withColumnRenamed('prediction', 'polarity').withColumn(\"polarity\", f.col(\"polarity\").cast(IntegerType()))\n#final_data.toPandas().to_csv(\"solutionsv2.csv\",header=True,index=False)\n","4a62e8f0":"solution = pridict(results)\nsample_solution=solution.select(\"id\",\"new_column\")\nsample_solution= sample_solution.withColumnRenamed(\"new_column\",\"polarity\")\nsample_solution.toPandas().to_csv(\"solutionsv2.csv\",header=True,index=False)\n","aecce3c5":"# Test","b4043b81":"# Building model","02a6187b":"B\u00e0i t\u1eadp m\u00f4n h\u1ecdc: X\u1eed l\u00fd d\u1eef li\u1ec7u l\u1edbn Gi\u1ea3ng vi\u00ean h\u01b0\u1edbng d\u1eabn: TS. \u0110\u1ed7 Tr\u1ecdng H\u1ee3p\n\nTh\u00e0nh vi\u00ean:\n\nNguy\u1ec5n Th\u00e0nh An - CH2001021"}}