{"cell_type":{"d2080eb0":"code","89b3ee78":"code","708ab2c8":"code","b724ecb8":"code","232ca529":"code","2ac18fae":"code","0699b226":"code","b98ed316":"code","d79177bf":"code","51ef45bb":"markdown","eb2399ad":"markdown","09203e4f":"markdown","5e8ac830":"markdown","481018c0":"markdown","91c8838f":"markdown","54fb7640":"markdown","ea114643":"markdown"},"source":{"d2080eb0":"# !pip install fairseq","89b3ee78":"# import pandas as pd\n# df_summary = pd.read_csv(\"..\/input\/news-summary\/news_summary_more.csv\")\n# news_txt = df_summary['text']","708ab2c8":"# with open('\/kaggle\/working\/news_text.txt','w') as save_txt:\n#     for i in range(100):\n#         save_txt.write(news_txt[i].strip()+'\\n')","b724ecb8":"# import torch\n\n# bart = torch.hub.load('pytorch\/fairseq', 'bart.large.cnn')\n# bart.cuda()\n# bart.eval()\n# bart.half()\n# count = 1\n# bsz = 32","232ca529":"# with open('\/kaggle\/working\/news_text.txt') as source, open('\/kaggle\/working\/test.hypo', 'w') as fout:\n#     sline = source.readline().strip()\n#     slines = [sline]\n#     for sline in source:\n#         if count % bsz == 0:\n#             with torch.no_grad():\n#                 hypotheses_batch = bart.sample(slines, beam=4, lenpen=2.0, max_len_b=140, min_len=55, no_repeat_ngram_size=3)\n\n#             for hypothesis in hypotheses_batch:\n#                 fout.write(hypothesis + '\\n')\n#                 fout.flush()\n#             slines = []\n\n#         slines.append(sline.strip())\n#         count += 1","2ac18fae":"# with open('\/kaggle\/working\/news_text.txt', 'r') as fout:\n#     src=fout.readlines()","0699b226":"# with open('\/kaggle\/working\/test.hypo', 'r') as fout:\n#     s=fout.readlines()","b98ed316":"# i=12\n# print('Source text:'+'\\n'+src[i])\n","d79177bf":"# print('Summary text:'+'\\n'+s[i])\n","51ef45bb":"[BART](https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/bart) is sequence-to-sequence model trained with denoising as pretraining objective. We show that this pretraining objective is more generic and show that we can match RoBERTa Results on SQuAD and GLUE and gain state-of-the-art results on summarization (XSum, CNN dataset), long form generative question answering (ELI5) and dialog response genration (ConvAI2). See the associated paper for more details.","eb2399ad":"# Load model for evaluation","09203e4f":"# BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension","5e8ac830":"# Make evaluation dataset from .csv files","481018c0":"[Fairseq](https:\/\/github.com\/pytorch\/fairseq\/tree\/master\/examples\/bart) is a sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling and other text generation tasks.","91c8838f":"# Introduction","54fb7640":"# Save generated summary and store as *\"test.hypo\"* file","ea114643":"# Setup"}}