{"cell_type":{"7c3a3332":"code","1c8a8d7a":"code","a295fc9c":"code","01868237":"code","3f8d59f1":"code","3141519f":"code","a61bbebf":"code","4850cc9b":"markdown"},"source":{"7c3a3332":"from matplotlib import pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nimport random","1c8a8d7a":"class Perceptron(object):\n    \n    def __init__(self, l_rate=0.1, n_iter=100):\n        self.l_rate = l_rate\n        self.n_iter = n_iter\n        \n    def train(self, x, y):\n        \n        # First element of the weight array is threshold\n        # Weights and threshold initialized with zeros\n        \n        self.weight = np.zeros(x.shape[1]+1)\n        self.costs = []\n        \n        for i in range(self.n_iter):\n            cost = 0\n            for xi, di in zip(x, d):\n                y = self.predict(xi)\n                error = di - y\n                self.weight[1:] += self.l_rate * error * xi\n                cost += (error**2) \/ 2.0\n                \n            self.costs.append(cost)\n        \n        return self\n    \n    # Activation function, returns 1 or -1\n    def predict(self, x):\n        return np.where(np.dot(self.weight[1:], x) + self.weight[0] >= 0.0, 1, -1)","a295fc9c":"# Creating data sets\n\ndef createSet(classSize):\n    classA = np.random.choice(np.arange(0, maxX, factor), size=(classSize, 3))\n    classB = np.random.choice(np.arange(minX, 0, factor), size=(classSize, 3))\n    \n    return classA, classB\n\n# Variables to create data sets\nclassSize = 50\ntrainingSize = 40\nminX = -1\nmaxX = 1\nfactor = 0.1\n\nclassA, classB = createSet(classSize)\n\n# Training set including first 40 element of each class\ntrainingA = classA[:trainingSize]\ntrainingB = classB[:trainingSize]\n\n# All inputs to train network \nx = np.concatenate((trainingA, trainingB), axis=0)\n\n# d is the desired outputs for training sets\n# It includes as; Class A = 1, Class B = -1\nd = np.ones(trainingSize*2)\nd[trainingSize:] = -1\n\n# Testing inputs including last 10 element of each class\ntestA = classA[trainingSize:]\ntestB = classB[trainingSize:]\n\n# Plotting part of training data\nfig = plt.figure()\nax = Axes3D(fig)\n\nax.scatter(trainingA[:, 0], trainingA[:, 1], trainingA[:, 2], c='r', marker='o')\nax.scatter(trainingB[:, 0], trainingB[:, 1], trainingB[:, 2], c='b', marker='o')\n\nax.set_xlabel('X1')\nax.set_ylabel('X2')\nax.set_zlabel('X3')\n\nplt.show()","01868237":"l_rate = 0.01\nn_iter = 100\n\n# Initializing Percepton class object\nmodel = Perceptron(l_rate, n_iter)\nmodel.train(x, d)\nweights = model.weight\n\nprint(\"Threshold: \", weights[0])\nprint(\"Weights: \", weights[1:])\n\nplt.plot(range(1, len(model.costs) + 1), model.costs, marker='x')\nplt.xlabel('Iteration')\nplt.ylabel('Cost Function')\nplt.show()","3f8d59f1":"fig = plt.figure()\nax = Axes3D(fig)\n\nax.scatter(trainingA[:, 0], trainingA[:, 1], trainingA[:, 2], c='r', marker='o')\nax.scatter(trainingB[:, 0], trainingB[:, 1], trainingB[:, 2], c='b', marker='o')\n\n[x1, x2] = np.meshgrid(np.arange(minX, maxX, factor), np.arange(minX, maxX, factor))\nx3 = (weights[0] - (weights[1]*x1) - (weights[2]*x2)) \/ weights[3]\nax.plot_surface(x1, x2, x3, color='c', alpha=0.3)","3141519f":"testResultA = [model.predict(testA[i]).tolist() for i in range(len(testA))]\ntestResultB = [model.predict(testB[i]).tolist() for i in range(len(testB))]\n\nprint(\"Test Reults for 10 Class A Samples:\\n\", testResultA)\n\nprint(\"\\n\\nTest Reults for 10 Class B Samples:\\n\", testResultB)","a61bbebf":"fig = plt.figure()\nax = Axes3D(fig)\n\nax.scatter(testA[:, 0], testA[:, 1], testA[:, 2], c='r', marker='o')\nax.scatter(testB[:, 0], testB[:, 1], testB[:, 2], c='b', marker='o')\n\nweights = model.weight\n\n[x1, x2] = np.meshgrid(np.arange(minX, maxX, factor), np.arange(minX, maxX, factor))\nx3 = (weights[0] - (weights[1]*x1) - (weights[2]*x2)) \/ weights[3]\nax.plot_surface(x1, x2, x3, color='c', alpha=0.3)","4850cc9b":"# Single Layer Perceptron Model Implementation"}}