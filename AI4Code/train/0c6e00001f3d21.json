{"cell_type":{"b6e9fbec":"code","1436d67f":"code","06fda44c":"code","2e816109":"code","9f287952":"code","bfa14372":"code","2af37383":"code","c432e4e3":"code","81bb35ab":"code","9894c810":"code","362e960f":"code","394acf44":"markdown","e7b6bb5e":"markdown","4f4a6772":"markdown","96544991":"markdown","8662096f":"markdown","b8174f23":"markdown","bffb0bf3":"markdown","9d373f98":"markdown","2f3790a4":"markdown","4d30cac3":"markdown"},"source":{"b6e9fbec":"import sys\nimport keras\nimport cv2\nimport numpy\nimport matplotlib\nimport skimage","1436d67f":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.optimizers import Adam\nfrom skimage.metrics import structural_similarity as ssim\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport math\nimport os\n\n# python magic function, displays pyplot figures in the notebook instead of separate display window\n%matplotlib inline","06fda44c":"#define a function for peak signal-to-noise ratio (PSNR)\ndef psnr(target,ref):#target image and refernce image\n    \n    #assume RGB image and convert all integer values to float\n    target_data=target.astype(float)\n    ref_data=ref.astype(float)\n    \n    diff=ref_data-target_data\n    diff=diff.flatten('C')#need ot flatten so computations can be done\n    \n    rmse=math.sqrt(np.mean(diff**2.))#2. for float values\n    \n    return 20*math.log10(255.\/rmse)\n\n#define function for mean squared error(MSE)\ndef mse(target,ref):\n    # the MSE between the two images is the sum of the squared difference between the two images\n    err=np.sum((target.astype('float')-ref.astype('float'))**2)\n    err=err\/float(target.shape[0]*target.shape[1])#divided by total number of pixels\n    \n    return err\n\n# define function that combines all three image quality metrics\ndef compare_images(target,ref):\n    scores=[]\n    scores.append(psnr(target,ref))\n    scores.append(mse(target,ref))\n    scores.append(ssim(target,ref,multichannel=True))#multichannel so that it can handle 3Dor 3 channel images RGB\/BGR \n    \n    return scores","2e816109":"import os\nimport cv2\n\n# prepare degraded images by introducing quality distortions resizing\n\ndef prepare_images(path, factor):\n    \n    # loop through the files in the directory\n    for file in os.listdir(path):\n        \n        # open the file\n        img = cv2.imread(path + '\/' + file)\n        \n        # find old and new image dimensions\n        h, w, c = img.shape\n        new_height = int(h \/ factor)\n        new_width = int(w \/ factor)\n        \n        # resize the image - down\n        img = cv2.resize(img, (new_width, new_height), interpolation = cv2.INTER_LINEAR) #interploation are methods for resizing images;how do you go from image with 100px to 1000px \n        #bilinear interpolation\n        \n        # resize the image - up\n        img = cv2.resize(img, (w, h), interpolation = cv2.INTER_LINEAR)\n        \n        # save the image\n        print('Saving {}'.format(file))\n        cv2.imwrite('images\/{}'.format(file), img)","9f287952":"prepare_images('..\/input\/srcnn-data\/Image-Restoration-using-SRCNN-master\/source\/',2)\n#source folder has high resolution images that will be converted to low resoltion images to be used for SRCNN","bfa14372":"#test the generated umages using the image quality metrics\n\nfor file in os.listdir('..\/input\/srcnn-data\/Image-Restoration-using-SRCNN-master\/images\/'):\n    \n    #open target and reference images\n    target=cv2.imread('..\/input\/srcnn-data\/Image-Restoration-using-SRCNN-master\/images\/{}'.format(file))\n    ref = cv2.imread('..\/input\/srcnn-data\/Image-Restoration-using-SRCNN-master\/source\/{}'.format(file))\n    \n    #calculate scores\n    scores=compare_images(target,ref)\n    \n     # print all three scores with new line characters (\\n) \n    print('{}\\nPSNR: {}\\nMSE: {}\\nSSIM: {}\\n'.format(file, scores[0], scores[1], scores[2]))\n        ","2af37383":"# define the SRCNN model\ndef model():\n    \n    # define model type\n    SRCNN = Sequential()\n    \n    #add model layers;filters =no. of nodes in the layer\n    SRCNN.add(Conv2D(filters=128, kernel_size = (9, 9), kernel_initializer='glorot_uniform',\n                     activation='relu', padding='valid', use_bias=True, input_shape=(None, None, 1)))#only if in keras.json image_data_format is channels_last; else if channels_first then 1,None,None\n    SRCNN.add(Conv2D(filters=64, kernel_size = (3, 3), kernel_initializer='glorot_uniform',\n                     activation='relu', padding='same', use_bias=True))\n    SRCNN.add(Conv2D(filters=1, kernel_size = (5, 5), kernel_initializer='glorot_uniform',\n                     activation='linear', padding='valid', use_bias=True))\n    #input_shape takes image of any height and width as long it is one channel\n    #that is how the SRCNN handles input,it handles image slice inputs, it doesn't work at all 3 channels at once\n    #SRCNN was trained on the luminescence channel in the YCrCb color space \n    \n    # define optimizer\n    adam = Adam(lr=0.0003)\n    \n    # compile model\n    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n    \n    return SRCNN","c432e4e3":"# define necessary image processing functions\n\n#necessary cuz when we run images through SRCNN based on the kernel sizes and convulational layers, we are going to lose some of these outside pixels\n#the images are going to get smaller and that's why it is neccesary to have a divisible image size \ndef modcrop(img,scale):\n    #temp size\n    tmpsz=img.shape\n    sz=tmpsz[0:2]\n    \n    #ensures that dimension of our image are divisible by scale(doesn't leaves hanging remainders) by cropping the images size\n    #np.mod returns the remainder bewtween our sz and scale\n    sz=sz-np.mod(sz,scale)\n    \n    img=img[0:sz[0],1:sz[1]]\n    return img\n\n#crop offs the bordersize from all sides of the image\ndef shave(image,border):\n    img=image[border: -border,border:-border]\n    return img","81bb35ab":"#define main prediction function\n\ndef predict(image_path):\n    \n    #load the srcnn model with weights cuz deep learning neural n\/w take lot of time to train(have to feed in large amount of input data)\n    srcnn=model()\n    srcnn.load_weights('..\/input\/srcnn-data\/Image-Restoration-using-SRCNN-master\/3051crop_weight_200.h5')\n     \n    #load the degraded and reference images\n    #in opencv, images are loaded as BGR channels\n    path,file=os.path.split(image_path)\n    degraded=cv2.imread(image_path)\n    ref=cv2.imread('..\/input\/srcnn-data\/Image-Restoration-using-SRCNN-master\/source\/{}'.format(file))\n    \n    #preprocess the image with modcrop\n    ref=modcrop(ref,3)#when calculating our image quality metrics later we have the same size image to what we produce in SRCNN network\n    degraded=modcrop(degraded,3)\n    \n    #convert the image to YCrCb(3 channel image) - (srcnn trained on Y channel)\n    temp=cv2.cvtColor(degraded,cv2.COLOR_BGR2YCrCb)\n    #opencv does a very good job in converting from rgb to YCrCb and back\n    \n    #create image slice and normalize cuz SRCNN works on one dimensional input(or 3D inputs of depth 1 ,ie, inputs with one channel)\n    Y=numpy.zeros((1,temp.shape[0],temp.shape[1],1),dtype=float)\n    #create a numpy array the we fill with data,temp.shape[0]=width,[1]=height and last one means one channel(essentially like batch=1 cuz that's what going to get passed to the n\/w ')\n    #fill in the data; all values are normalized to between 0 and 1 as that's how srcnn was trained\n    Y[0,:,:,0]=temp[:,:,0].astype(float)\/255\n    #first 0 means 0th index(we are saying that batch size is 1); :,: means every point in these channels; last 0 means first channel,ie, all the pixels in first luminescence channel\n    #so we have our image slice, we have the Y channel, which is the first channel(index 0) out of the image that we converted to YCrCb color space\n    \n    #perform super-resolution with srcnn\n    pre=srcnn.predict(Y,batch_size=1)#that's why we had index 0  above cuz we are saying that batch size is 1\n    \n    #post-process output cuz pre is still normalized\n    pre*=255#multiplying every pixel by 255\n    pre[pre[:]>255]=255#any pixels >255 set it =255 to prevent any rounding errors due to multiplication\n    pre[pre[:]<0] =0# same reason as above\n    pre=pre.astype(np.uint8)#convert float back to int values\n    \n    #cuz this is only the luminescence channel in the pre ,SO\n    #copy Y channel back to image and convert to BGR\n    temp=shave(temp,6)#accd.to tutor it loses 3 pixels on each side so if we shave this with a border 6,we can crop it appropriately there, so it is the same size as our output\n    #if not agree with tutor, use print statements to see the specific dimensions\n    \n    # for the first channel(Y channel), copy in the output of our network\n    temp[:,:,0]=pre[0,:,:,0]\n    #So we are keeping the red difference and blue difference, channels 1 and 2, in this temp image which is in the YCrCb color space\n    #and in the first one we are copying in our ouput,our luminiscence channel\n    \n    #convert back to bgr\n    output=cv2.cvtColor(temp,cv2.COLOR_YCrCb2BGR)\n    \n    #emove borderfrom reference and degraded image, so that all our images(ref,degraded(low res.), and ouput(high res.)) are of the same size\n    ref = shave(ref.astype(np.uint8), 6)\n    degraded = shave(degraded.astype(np.uint8), 6)\n    \n    # image quality calculations\n    scores = []\n    scores.append(compare_images(degraded, ref))#degraded wrt ref\n    scores.append(compare_images(output, ref))#high res. output wrt ref\n    \n    # return images and scores\n    return ref, degraded, output, scores","9894c810":"ref,degraded,output,scores=predict('..\/input\/srcnn-data\/Image-Restoration-using-SRCNN-master\/images\/flowers.bmp')\n# print all scores for all images\nprint('Degraded Image: \\nPSNR: {}\\nMSE: {}\\nSSIM: {}\\n'.format(scores[0][0], scores[0][1], scores[0][2]))\nprint('Reconstructed Image: \\nPSNR: {}\\nMSE: {}\\nSSIM: {}\\n'.format(scores[1][0], scores[1][1], scores[1][2]))\n\n\n# display images as subplots\nfig, axs = plt.subplots(1, 3, figsize=(20, 8))#1 row,3 columns\naxs[0].imshow(cv2.cvtColor(ref, cv2.COLOR_BGR2RGB))#first subplot\n#imshow assumes RGB images but cv2 loads images as BGR else channel mixing will take place and we will get weird images\naxs[0].set_title('Original')\naxs[1].imshow(cv2.cvtColor(degraded, cv2.COLOR_BGR2RGB))#2nd subplot\naxs[1].set_title('Degraded')\naxs[2].imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))\naxs[2].set_title('SRCNN')\n\n# remove the x and y ticks\nfor ax in axs:\n    ax.set_xticks([])#leave them blank to remove ticks\n    ax.set_yticks([])","362e960f":"for file in os.listdir('..\/input\/srcnn-data\/Image-Restoration-using-SRCNN-master\/images'):\n    \n    # perform super-resolution\n    ref, degraded, output, scores = predict('..\/input\/srcnn-data\/Image-Restoration-using-SRCNN-master\/images\/{}'.format(file))\n    \n    # display images as subplots\n    fig, axs = plt.subplots(1, 3, figsize=(20, 8))\n    axs[0].imshow(cv2.cvtColor(ref, cv2.COLOR_BGR2RGB))\n    axs[0].set_title('Original')\n    axs[1].imshow(cv2.cvtColor(degraded, cv2.COLOR_BGR2RGB))\n    axs[1].set_title('Degraded')\n    axs[1].set(xlabel = 'PSNR: {}\\nMSE: {} \\nSSIM: {}'.format(scores[0][0], scores[0][1], scores[0][2]))\n    axs[2].imshow(cv2.cvtColor(output, cv2.COLOR_BGR2RGB))\n    axs[2].set_title('SRCNN')\n    axs[2].set(xlabel = 'PSNR: {} \\nMSE: {} \\nSSIM: {}'.format(scores[1][0], scores[1][1], scores[1][2]))\n\n    # remove the x and y ticks\n    for ax in axs:\n        ax.set_xticks([])\n        ax.set_yticks([])\n      \n#     print('Saving {}'.format(file))\n#     fig.savefig('output\/{}.png'.format(os.path.splitext(file)[0])) \n#     plt.close()","394acf44":"# 2. Preparing Images\n","e7b6bb5e":"SSIM: 0.0 to 1.0 shows similarity between images\nMSE: Higher it is lower the resolution of the image\nPSNR: want it to be as high as possible cuz we want noise tobe low and it is the ratio of signal to noise","4f4a6772":"using the same images that were used in the original SRCNN paper. We can download these images from http:\/\/mmlab.ie.cuhk.edu.hk\/projects\/SRCNN.html.","96544991":"# 5. Deploying the SRCNN\nNow that we have defined our model, we can use it for single-image super-resolution, **AFTER** preprocessing the images extensively before using them as inputs to the network. This processing will include cropping and color space conversions.\n\nAdditionally, to save us the time it takes to train a deep neural network, we will be loading pre-trained weights for the SRCNN. These weights can be found at the following GitHub page: https:\/\/github.com\/MarkPrecursor\/SRCNN-keras\n\nOnce we have tested our network, we can perform single-image super-resolution on all of our input images. Furthermore, after processing, we can calculate the PSNR, MSE, and SSIM on the images that we produce. We can save these images directly or create subplots to conveniently display the original, low resolution, and high resolution images side by side.","8662096f":"# Using The Super Resolution Convolutional Neural Network(Deep Neural N\/W) for Image Restoration\n","b8174f23":"All the image quality metrics got better; PSNR increased, MSE devreased and SSIM increased from degraded image to reconstructed image","bffb0bf3":"# 1. Image Quality Metrics\nThe structural similiarity (SSIM) index was imported directly from the scikit-image library; however, we will have to define our own functions for the PSNR and MSE.","9d373f98":"# 4. Building the SRCNN Model\n In Keras, it's as simple as adding layers one after the other. The achitecture and hyper parameters of the SRCNN network can be obtained from the publication referenced above.","2f3790a4":"opencv is incredibly fast as it is designed for reat time computer vision application\n\nnew image(degraded) are of same resolution as base images.When sizing down the image we store the original pixel info in smaller area so we lost that info when sizing up the image.","4d30cac3":"# 3. Testing Low Resolution Images\nTo ensure that our image quality metrics are being calculated correctly and that the images were effectively degraded, lets calculate the PSNR, MSE, and SSIM between our reference images and the degraded images that we just prepared."}}