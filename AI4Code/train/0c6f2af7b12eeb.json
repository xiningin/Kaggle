{"cell_type":{"b0f46954":"code","07d11cba":"code","028673c3":"code","3d653779":"code","8ab36c01":"code","7b272e10":"code","c1f37373":"code","a9dd21f8":"code","e660dc7c":"code","55ff946c":"code","04b37353":"code","54b4ef2a":"code","d01a3b9f":"code","13d54db4":"code","4edde0ef":"code","59dd1c94":"code","02ab184d":"code","dcfa06c3":"code","d5acad31":"code","8374f354":"code","6adc10a6":"code","cffd68c0":"code","42b87bf0":"code","9e39bb1a":"markdown","ed5e1e9a":"markdown","dfb196c2":"markdown","a03b13ac":"markdown","6dfb5d28":"markdown","49f69580":"markdown","e63a2b44":"markdown"},"source":{"b0f46954":"%config Completer.use_jedi = False","07d11cba":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow, imread\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\n\nimport scipy.stats as stats\n\nimport lightgbm as lgb\nimport warnings","028673c3":"R_SEED = 37","3d653779":"submit = True # for some testing","8ab36c01":"submission_ex = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\ntrain_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv')","7b272e10":"targets_df = train_df[['loss']].copy()\ntrain_df.drop(['id', 'loss'], axis=1, inplace=True) \ntest_df.drop(['id'], axis=1, inplace=True) ","c1f37373":"all_df = pd.concat([train_df, test_df])\n# 1-------------------vvv","a9dd21f8":"warnings.filterwarnings(\"ignore\")\n\nfig = plt.figure(figsize = (30,60))\nax = fig.gca()\nhist = all_df.hist(bins = 50, layout = (20,5), color='k', alpha=0.5,  ax = ax)","e660dc7c":"# warnings.filterwarnings(\"ignore\")\ndef plot_fea_hist(fea_name):\n    fig = plt.figure(figsize = (10, 10))\n    ax = fig.gca()\n    hist = all_df[fea_name].hist(bins=150, ax = ax)","55ff946c":"def plot_kmeans(data, labels, no_of_cl, fea_name, ax):\n    ax.hist(data, 100, density = True)\n    for cl in range(no_of_cl):\n        ax.hist(data[labels == cl], 1, density = True, alpha = 0.5)\n    ax.set_title(fea_name)\n#     plt.show()","04b37353":"# guesswork\nfor_kmeans = [('f2', 2), ('f3', 3), ('f7', 3), ('f11', 3), ('f12', 2), ('f14', 2), ('f18', 2), ('f19', 2), ('f20', 4), ('f24', 3), \n              ('f26', 2), ('f27', 2), ('f32', 3), ('f34', 2), ('f38', 2), ('f48', 2), ('f50', 3), ('f57', 3), ('f67', 3), ('f76', 3),\n             ('f80', 2), ('f86', 4), ('f93', 2), ('f94', 2)]\n\nfig, axes = plt.subplots(nrows = 6, ncols = 4, figsize=(20, 30))\ni = 1\nfor f, n_clusters in for_kmeans:\n#     print(str(i) + ' of ' + str(len(for_kmeans)))\n    \n    # KMeans\n    data = all_df[[f]].values\n    km = KMeans(n_clusters = n_clusters, n_init = 50)\n    km.fit(data)\n    k_clus = km.labels_\n    \n    # print(km.cluster_centers_)\n    # print(pd.value_counts(km.labels_))\n\n    ax = axes[(i-1) \/\/ 4, (i-1) % 4]\n    plot_kmeans(data, k_clus, n_clusters, f, ax)\n\n    i += 1\n    \n    # # one_h_clus = np.zeros((k_clus.size, k_clus.max()+1))\n    # # one_h_clus[np.arange(k_clus.size), k_clus] = 1\n    # # for i in range(n_clusters):\n    # #     all_df['clus_' + str(i)] = one_h_clus[:,i]\n\n#     all_df[f + '_clus'] = k_clus\n    _dist = km.transform(data)\n    _dict = {f + '_dist_from_' + str(i): _dist[:,i] for i in range(n_clusters)}\n    for k, v in _dict.items():\n        all_df[k] = v\n#     del all_df[f]\nplt.show()","54b4ef2a":"def plot_gmm(model, data, fea_name, ax):\n    weights = model.weights_\n    means = model.means_\n    covars = model.covariances_\n\n    n, bins, patches = ax.hist(data, 100, density = True, alpha = 0.2, color = 'k')\n    x = np.arange(np.min(data), np.max(data), (np.max(data) - np.min(data)) \/ 100)\n    for i in range(len(weights)):\n        ax.plot(x, weights[i] * stats.norm.pdf(x,means[i],np.sqrt(covars[i])[0]), alpha = 0.7, linewidth = 3)\n    ax.set_title(fea_name)\n#     plt.show()","d01a3b9f":"# guesswork\nfor_gmm = [('f2', 2), ('f4', 3), ('f6', 2), ('f7', 4), ('f8', 2), ('f9', 2), ('f11', 3), ('f12', 2), ('f14', 3), ('f15', 3),\n          ('f16', 5), ('f18', 3), ('f19', 3), ('f21', 2), ('f22', 3), ('f24', 3), ('f26', 2), ('f28', 3), ('f30', 3), ('f32', 3),\n          ('f34', 2), ('f35', 3), ('f37', 3), ('f38', 3), ('f39', 3), ('f41', 3), ('f52', 3), ('f64', 3), ('f67', 3), ('f68', 2),\n          ('f69', 2), ('f73', 3), ('f75', 3), ('f76', 3), ('f77', 2), ('f79', 2), ('f80', 2), ('f81', 3), ('f85', 3), ('f89', 3),\n          ('f91', 3), ('f93', 3), ('f94', 3)]\n\nfig, axes = plt.subplots(nrows = 11, ncols = 4, figsize=(20, 50))\n\ni = 1\nfor f, n_clusters in for_gmm:\n#     print(str(i) + ' of ' + str(len(for_gmm)))\n    \n    # GMM\n    data = all_df[[f]].values\n    \n    gm = GaussianMixture(n_components = n_clusters, n_init = 5)\n    gm.fit(data)\n    k_clus_1 = gm.predict(data)\n    k_clus_2 = gm.predict_proba(data)\n\n    ax = axes[(i-1) \/\/ 4, (i-1) % 4]\n    plot_gmm(gm, data, f + '_clus_gmm', ax)\n    i += 1\n    \n    all_df[f + '_clus_gmm'] = k_clus_1\n    for j in range(len(k_clus_2[0])):\n        all_df[f + '_clus_gmm_' + str(j)] = k_clus_2[:, j]\n\nplt.show()","13d54db4":"all_df.head()","4edde0ef":"# lightGBM will work same without this, but I try to transform data as much as I can before splitting them on trainset and testset\nall_df_normalized = StandardScaler().fit_transform(all_df)\nall_df = pd.DataFrame(all_df_normalized, columns=all_df.columns)","59dd1c94":"# KMeans on whole dataset\nn_clusters = 2 # ?\n\ndata = all_df.values\nkm = KMeans(n_clusters = n_clusters, n_init = 50)\nkm.fit(data)\nk_clus = km.labels_\nprint(pd.value_counts(km.labels_))\n\n_dist = km.transform(data)\n_dict = {'all_dist_from_' + str(i): _dist[:,i] for i in range(n_clusters)}\nfor k, v in _dict.items():\n    all_df[k] = v","02ab184d":"# 1-------------------^^^\ntrain_df, test_df = all_df.iloc[:train_df.shape[0],:].copy(), all_df.iloc[-test_df.shape[0]:,:].copy()","dcfa06c3":"if submit:\n    X = train_df.copy()\n    y = targets_df[['loss']].copy()\nelse:\n    np.random.seed(R_SEED)\n    msk = np.random.rand(len(train_df)) < 0.9\n    X = train_df[msk].copy()\n    my_X = train_df[~msk].copy()\n    y = targets_df[msk].copy()\n    my_y = targets_df[~msk].copy()","d5acad31":"# qq_df = pd.concat([X, y], axis=1, join='inner')\n\n# corr = qq_df.corr()\n\n# mask = np.zeros_like(corr)\n# mask[np.triu_indices_from(mask)] = True\n\n# fig = plt.figure(figsize = (30, 25))\n# sns.heatmap(corr, cmap=\"flare\", mask=mask)\n# plt.show()","8374f354":"def softmax(x):\n    e_x = np.exp(x - np.max(x))\n    return e_x \/ e_x.sum(axis = 0)","6adc10a6":"def plot_fea_imp(model, model_name):\n    print('Plotting feature importances...')\n    fea_imp = pd.DataFrame({'imp': model.feature_importances_, 'col': train_df.columns})\n    fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False])#.iloc[-10:]\n    fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 70), legend=None)\n    plt.title('%s - Feature Importance' % (model_name))\n    plt.ylabel('Features')\n    plt.xlabel('Importance')","cffd68c0":"params_loss = {\n                'n_estimators': 150000, # there is early_stopping\n                'learning_rate': 0.002,\n                'min_child_samples': 500,\n                'feature_fraction': 0.35,\n                'bagging_fraction': 0.8,\n                'bagging_freq': 1,\n                }\n\nlgbm_reg = lgb.LGBMRegressor(\n                            **params_loss, \n                            objective='rmse',\n                            metric='rmse',\n                            n_jobs=-1\n                            )","42b87bf0":"import time\nstart_time = time.time()\n\n_target = 'loss'\n\npred = []\nval_rmse = []\nall_rmse = 0\n\nprint(X.shape)\n\nk = 5\n\nkfolds = KFold(n_splits = k, shuffle = True, random_state = R_SEED) # \n_k = 1\nfor train_index, test_index in kfolds.split(X):\n    print('------------------------------------------------k: ', _k, 'of', k)\n    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n\n    lgbm_reg.fit(\n        X_train, \n        y_train,\n        eval_set = [(X_val, y_val)],\n        verbose = 500,\n        early_stopping_rounds = 3000,\n#             callbacks = [lgb.reset_parameter(learning_rate = [0.005] * 500 + [0.001] * 29500)]\n    )\n\n    f_pred = lgbm_reg.predict(X_val)\n    curr_rmse = mean_squared_error(y_val[_target].values, f_pred, squared = False)\n    print('curr_rmse: ', curr_rmse, 'fold: ',_k, 'of', k)\n    _k += 1\n    val_rmse.append(curr_rmse)\n\n    all_rmse += curr_rmse\n\n    _p = lgbm_reg.predict(my_X if not submit else test_df)\n    pred.append(_p)\n\n    if not submit:\n        test_rmse = mean_squared_error(my_y, _p, squared = False)\n        print('test_rmse: ', test_rmse)\n\n    print(\"Execution time: \", time.time() - start_time, \"secs\")\n    \n#     plot_fea_imp(lgbm_reg, 'LGBMRegressor')\n\nprint('all_rmse: ', all_rmse \/ len(pred))\n\n    \npred1 = np.sum(pred, axis = 0) \/ len(pred)\npred2 = np.transpose(np.matmul(np.transpose(pred), softmax(val_rmse)))\npred3 = np.transpose(np.matmul(np.transpose(pred), softmax(np.squeeze(np.full((1, len(pred)), 100)) - val_rmse)))\n\nif not submit:\n    end_rmse1 = mean_squared_error(my_y, pred1, squared = False)\n    print('end_rmse1: ', end_rmse1)\n    end_rmse2 = mean_squared_error(my_y, pred2, squared = False)\n    print('end_rmse2: ', end_rmse2)\n    end_rmse3 = mean_squared_error(my_y, pred3, squared = False)\n    print('end_rmse3: ', end_rmse3)\nelse:\n    submission_1 = submission_ex[['id']].copy()\n    submission_2 = submission_ex[['id']].copy()\n    submission_3 = submission_ex[['id']].copy()\n    submission_1[_target] = pred1\n    submission_2[_target] = pred2\n    submission_3[_target] = pred3\n    submission_1.to_csv('submission_1.csv', index=False)\n    submission_2.to_csv('submission_2_p.csv', index=False)\n    submission_3.to_csv('submission_3.csv', index=False)","9e39bb1a":"#### step by step\nI know that kmeans is not very good for this, but, its fast enough for just a try.","ed5e1e9a":"gmm features represent probability that value of original feature belongs to certain distribution","dfb196c2":"#### interesting\nprediction_2 is really weird, but on testing it gives best blend score (next cell)","a03b13ac":"I created some new features as distance from cluster centroids for specified original features.","6dfb5d28":"### Main idea\nIf we merge train data with test data and perform series of transformation on them, maybe we create additional bond between them. Just an idea, let's see what will happen.","49f69580":"#### GaussianMixture","e63a2b44":"#### KMeans"}}