{"cell_type":{"b1a7a799":"code","45342f7a":"code","90a3455a":"code","c55079aa":"code","0d60a485":"code","7f587fbe":"code","d40361a6":"code","d013d6c9":"code","69d409db":"code","c566d909":"code","12a7742e":"code","10ef3e99":"code","79e60839":"code","ce1812ad":"code","409435ef":"code","539796f0":"code","2b7613da":"code","8295b9f0":"code","e6222f2d":"code","33e8f95b":"code","a668f4c9":"code","62ebcac9":"code","b3489fe8":"code","8d745f14":"code","596a09ab":"code","21f7365e":"code","1c36df23":"code","1b3f39e8":"code","7a9b6676":"code","6956402d":"code","94118d3f":"code","2d73d7c2":"code","c42a1286":"code","5cf64bce":"code","da0b1b21":"code","0bfd3396":"code","0d5dbf7d":"code","021f101a":"code","787ed8ca":"code","3f95db4a":"code","7e582353":"code","7de4fc37":"code","b7d16503":"code","cbc631a8":"code","408c33eb":"code","e3beab5f":"code","21c03538":"code","4b693b6f":"markdown","048a803d":"markdown","307ba4c9":"markdown","8db7db2c":"markdown","fc1810d4":"markdown","09679191":"markdown","227ad85b":"markdown","d32c99a2":"markdown","c0f26602":"markdown","7958b532":"markdown","d210c294":"markdown","74a36cf8":"markdown","d7b29162":"markdown","1955f5cd":"markdown","a41dce2a":"markdown","ff6f82b8":"markdown","bc84cf46":"markdown","cb269bff":"markdown","6d8f4c48":"markdown","26b4d1e9":"markdown","353f2ff1":"markdown","19d05445":"markdown","640611f3":"markdown","26475ce8":"markdown","c5a41e86":"markdown","aa3425ef":"markdown","48ae7d3f":"markdown","3851b7c2":"markdown"},"source":{"b1a7a799":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n# from sklearn.naive_bayes import GaussianNB\n# from sklearn.svm import SVC\n\nfrom sklearn.metrics import accuracy_score","45342f7a":"pd.set_option('float_format', '{:.3f}'.format)","90a3455a":"df = pd.read_csv('..\/input\/income-classification\/income_evaluation.csv')\ndf.head()","c55079aa":"# removing redundant spaces from column names\ndf.columns = list(map(lambda a: a.lstrip(), df.columns))","0d60a485":" print('Check the data. Some values are missing.') if df.isnull().any().any() else print(\"There are no missing values in the data.\")","7f587fbe":"df['workclass'].value_counts()","d40361a6":"shape0 = df.shape[0]\nfor column in df.columns:\n    df[column].replace(' ?', np.NaN, inplace=True)\ndf = df.dropna().reset_index().drop(columns=['index'])\nshape1 = df.shape[0]\nprint(str(shape0 - shape1) + ' rows have been removed.')","d013d6c9":"df['workclass'].value_counts()","69d409db":"income = df.income.value_counts()\nincome","c566d909":"colors = ['#ADEFD1FF', '#00203FFF']\nexplode = [0, 0.1]\nplt.pie(income, labels=income.values, colors=colors, explode = explode, shadow=True)\nplt.title('Income distribution')\nplt.legend(labels=income.index)","12a7742e":"# changing string to binary values.\ndf['income'].replace([' <=50K',' >50K'],[1,0], inplace=True)","10ef3e99":"# checking types of variables\ndf.dtypes","79e60839":"# object type variables cannot be included in a correlation heatmap.\nstats = df.select_dtypes(['float', 'int64']).drop(columns=['income'])","ce1812ad":"fig = plt.figure(figsize=(14,14))\ncorr = stats.corr()\nsns.heatmap(corr,\n            vmin=-1, vmax=1, center=0, \n            cmap=sns.diverging_palette(h_neg=250, h_pos=15, as_cmap=True),\n            square=True, annot=True)","409435ef":"stats.describe().transpose()","539796f0":"# creating graphs\ncolors = list(sns.color_palette(\"Paired\"))\nfig, ax = plt.subplots(nrows=6, ncols=2, figsize=(12,28))\nfor i in range(6):\n    sns.kdeplot(stats.iloc[:, i], \n                shade = True, \n                color = colors[i*2+1], \n                ax=ax[i, 0]).set(ylabel = '', xlabel = stats.columns[i])\n    \n    sns.boxplot(data=stats.iloc[:, i],\n                color = colors[i*2], orient=\"h\",\n                ax=ax[i, 1]).set(xlabel = '', ylabel = '')\nplt.show()","2b7613da":"df_final = pd.get_dummies(df)\ndf_final.head()","8295b9f0":"X = df_final.drop(columns=['income'])\ny = df_final['income']","e6222f2d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","33e8f95b":"# creating scaler and new standardized train and test data frames.\nscaler = MinMaxScaler()\n\nX_train_numeral = scaler.fit_transform(X_train.select_dtypes(['float', 'int64']))\nX_train_s = pd.concat([pd.DataFrame(X_train_numeral, index=X_train.index, columns=X_train.columns[:6]), \n                       X_train.iloc[:, 6:]], axis=1)\n\nX_test_numeral = scaler.transform(X_test.select_dtypes(['float', 'int64']))\nX_test_s = pd.concat([pd.DataFrame(X_test_numeral, index=X_test.index, columns=X_test.columns[:6]),\n                      X_test.iloc[:, 6:]], axis=1)","a668f4c9":"X_train.head()","62ebcac9":"# function responsible for fitting and predicting algorithms given through lists.\ndef algoritm_score_list(show_processing=False, standardized=False):\n    scores_list = []\n\n    for algorithm in algorithms:\n        if show_processing:\n            print('processing ' + str(algorithm) + ' algorithm...')\n\n        if standardized:\n            X_tn = X_train_s\n            X_tt = X_test_s\n        else:\n            X_tn = X_train\n            X_tt = X_test\n            \n        A = algorithm.fit(X_tn, y_train)\n        y_predict = A.predict(X_tt)\n        accuracy = accuracy_score(y_test,y_predict)\n\n        scores_list.append([A, accuracy, standardized])\n        \n    print('all predictions finished')\n    return scores_list","b3489fe8":"algorithms = [DecisionTreeClassifier(),\n              LogisticRegression(solver='liblinear'), \n              KNeighborsClassifier()]","8d745f14":"# default algorithms\ndefault_alg = algoritm_score_list()","596a09ab":"# default algorithms, standardized data\ndefault_alg_s = algoritm_score_list(standardized=True)","21f7365e":"df_alg = pd.DataFrame(default_alg, columns=['algorithm', 'accuracy', 'standardized'])\ndf_alg = pd.concat([df_alg, \n                    pd.DataFrame(default_alg_s, columns=['algorithm', 'accuracy', 'standardized'])], \n                   axis=0, \n                   ignore_index=True)\ndf_alg.sort_values(by='accuracy', ascending=False)","1c36df23":"# max_depth in already created DT\ndepth = default_alg[0][0].get_depth()\n\nalgorithms = []\nfor i in range(1, depth+1):\n    algorithms.append(DecisionTreeClassifier(max_depth=i))\nscore_list = algoritm_score_list()","1b3f39e8":"scores = list(map(lambda a: a[1], score_list))\nfig = plt.figure(figsize=(7, 7))\nplt.grid(b=True)\nsns.lineplot(x=range(1, depth+1), y=scores)\nplt.plot(scores.index(max(scores)) + 1, max(scores), \"or\")","7a9b6676":"print('The best algorithm: DecisionTreeClassifier(max_depth={}), its accuracy: {}'.format(scores.index(max(scores)) + 1, round(max(scores), 3)))","6956402d":"# min_samples_leaf in already created DT\ndefault_alg[0][0].get_params()['min_samples_leaf']","94118d3f":"algorithms = []\nfor i in range(1, 202, 5):\n    algorithms.append(DecisionTreeClassifier(min_samples_leaf=i))\nscore_list = algoritm_score_list()","2d73d7c2":"scores = list(map(lambda a: a[1], score_list))\nfig = plt.figure(figsize=(7, 7))\nplt.grid(b=True)\nsns.lineplot(x=range(1, 202, 5), y=scores)\nplt.plot(scores.index(max(scores))*5 + 1, max(scores), \"or\")","c42a1286":"print('The best algorithm: DecisionTreeClassifier(min_samples_leaf={}), its accuracy: {}'.format(scores.index(max(scores))*5 + 1, round(max(scores), 3)))","5cf64bce":"algorithms = []\nfor i in range(4, 30, 3):\n    for j in range(45, 86, 5):\n         algorithms.append(DecisionTreeClassifier(max_depth=i, min_samples_leaf=j))\nscore_list = algoritm_score_list()   ","da0b1b21":"max_depth = list(map(lambda a: a[0].get_params()['max_depth'], score_list))\nmin_samples_leaf = list(map(lambda a: a[0].get_params()['min_samples_leaf'], score_list))\nscores = list(map(lambda a: a[1], score_list))\n\ndf = pd.DataFrame(np.array([max_depth, min_samples_leaf, scores]).T, \n                  columns=['max_depth', 'min_s_l', 'scores'])","0bfd3396":"fig = plt.figure(figsize=(7, 7))\nsns.scatterplot(x=df['max_depth'], y=df['min_s_l'], hue=df['scores'], s=300)\nplt.legend(loc='lower left')","0d5dbf7d":"best = scores.index(max(scores))\nm_d = max_depth[best]\nm_s_l = min_samples_leaf[best]\nprint('The best algorithm: DecisionTreeClassifier(max_depth={}, min_samples_leaf={}), its accuracy: {}'.format(m_d, m_s_l, round(max(scores), 3)))","021f101a":"algorithms = [DecisionTreeClassifier(criterion='gini'), DecisionTreeClassifier(criterion='entropy')]\nscore_list = algoritm_score_list()   \nprint('criterion: gini, score: {}.'.format(str(round(score_list[0][1], 3))))\nprint('criterion: entropy, score: {}.'.format(str(round(score_list[1][1], 3))))","787ed8ca":"algorithms = [DecisionTreeClassifier(criterion='gini', max_depth=9), \n              DecisionTreeClassifier(criterion='gini', min_samples_leaf=66), \n              DecisionTreeClassifier(criterion='gini', max_depth=16, min_samples_leaf=65)]\ndf_DT = pd.DataFrame(algoritm_score_list(), columns=['algorithm', 'accuracy', 'standardized'])\ndf_DT","3f95db4a":"algorithms = []\nfor p in ['l1', 'l2']:\n    algorithms.append(LogisticRegression(solver='liblinear', penalty=p))\n    \ndf_LR_penalty = pd.DataFrame(algoritm_score_list(standardized=True), columns=['algorithm', 'accuracy', 'standardized'])\ndf_LR_penalty","7e582353":"algorithms = []\nfor c in [100, 10, 1.0, 0.1, 0.01]:\n    algorithms.append(LogisticRegression(solver='liblinear', penalty='l1', C=c))\n\ndf_LR = pd.DataFrame(algoritm_score_list(standardized=True), columns=['algorithm', 'accuracy', 'standardized'])\ndf_LR","7de4fc37":"algorithms = [KNeighborsClassifier(weights='uniform'), \n              KNeighborsClassifier(weights='distance')]\ndf_KN_weights = pd.DataFrame(algoritm_score_list(standardized=True), columns=['algorithm', 'accuracy', 'standardized'])\ndf_KN_weights","b7d16503":"algorithms = []\nfor i in range(1, 50, 5):\n    algorithms.append(KNeighborsClassifier(n_neighbors=i))\n    \ndf_KN_n_neighbors = pd.DataFrame(algoritm_score_list(standardized=True), columns=['algorithm', 'accuracy', 'standardized'])","cbc631a8":"fig = plt.figure(figsize=(7, 7))\nplt.grid(b=True)\nsns.lineplot(x=range(1, 50, 5), y=df_KN_n_neighbors['accuracy'])\nplt.plot(df_KN_n_neighbors[df_KN_n_neighbors['accuracy'] == max(df_KN_n_neighbors['accuracy'])].index.values[0]*5+1,\n         max(df_KN_n_neighbors['accuracy']),\n         \"or\")","408c33eb":"algorithms = [DecisionTreeClassifier(), \n              DecisionTreeClassifier(criterion='gini', max_depth=16, min_samples_leaf=65),\n              LogisticRegression(solver='liblinear'), \n              LogisticRegression(C=10, penalty='l1', solver='liblinear'),\n              KNeighborsClassifier(), \n              KNeighborsClassifier(n_neighbors=46)]\n\nfinal_list = algoritm_score_list()","e3beab5f":"algorithms = [DecisionTreeClassifier(), \n              DecisionTreeClassifier(criterion='gini', max_depth=16, min_samples_leaf=65),\n              LogisticRegression(solver='liblinear'), \n              LogisticRegression(C=10, penalty='l1', solver='liblinear'),\n              KNeighborsClassifier(), \n              KNeighborsClassifier(n_neighbors=46)]\nfinal_list = final_list + algoritm_score_list(standardized=True)","21c03538":"final_df = pd.DataFrame(final_list, columns=['algorithm', 'accuracy', 'standardized'])\nfinal_df.head(12).sort_values(by='accuracy', ascending=False)","4b693b6f":"Scores in both LogisticRegressions are similar. While I have to choose one of them I decided to use 'l1'.","048a803d":"I will use default weights parameter.","307ba4c9":"As for accuracy there isn't a huge difference between these three DecisionTrees. All of them have accuracy similar to 0.85. However, for the final test of all algorithms I will use DecissionTree with max_depth and min_samples_leaf specified.","8db7db2c":"It took so much time but I really wanted to see results. For n_neighbors parameter equal to about 46 there is the biggest accuracy.","fc1810d4":"### 3.1.1 max_depth","09679191":"## 3.2 Tuning LogisticRegression\nAs for LogiticRegression I will try to fix penalty and C parameters.","227ad85b":"# Introduction\nHello there! Welcome to my new notebook. In this script I made some EDA graphs and used three classification algorithms to predict income. To improve accuracy I fixed the algorithm's parameters. \nThis is my comparison of DecisionTreeClassifier, LogisticRegression and KNeighborsClassifier. ","d32c99a2":"My criterion comparison showed that there is no huge difference between gini and entropy in this particular dataset. Consequently, I use the default criterion: gini.","c0f26602":"### 3.1.2 min_samples_leaf","7958b532":"The graph shows that default depth overfitted the algorithm. A proper max_depth that maximaize accuracy is around 9.","d210c294":"### 3.1.4 criterion","74a36cf8":"## 3.3 Tuning KNeighborsClassifier\nI will try to optimize two parameters in this algorithm: weights and n_neighbors.","d7b29162":"As it can be observed after standardization there is an improvement of accuracy in LogisticRegression and KNeighborsClassifier which is in accordance with the theory. As for DecisionTreeClassifier, unstandardized data doesn't influence accuracy. In the following examples standardized data will be used if there was a bigger accuracy after standardization. Consequently, I will use it in LogisticRegression and KNeighborsClassifier.","1955f5cd":"### 3.1.3 max_depth and min_samples_leaf","a41dce2a":"# 1. Exploratory Data Analysis","ff6f82b8":"# 3. Modeling","bc84cf46":"There are question marks in the columns instead of 'NaN' or 'None' values. Question marks signalize missing values, that is why they will be removed.","cb269bff":"The final LogisticRegression algorithm is LogisticRegression(C=10, penalty='l1', solver='liblinear').","6d8f4c48":"### 3.1.5 Algorithms comparison ","26b4d1e9":"The graph shows that default min_samples_leaf overfitted the algorithm. A proper min_samples_leaf that increases accuracy should be bigger than 25 and smaller than 170 (it really depends on train and test spliting).","353f2ff1":"# 4 Final comparison and summary","19d05445":"## 1.2 Creating correlation heatmap","640611f3":"There is barely correlation between all numeric variables.","26475ce8":"## 1.2 Creating density functions and boxplots","c5a41e86":"## 3.1 Tuning DecisionTree\nWhile this is one of my first times using DecisionTreeClassifier and I'm not sure how to fix all parameters I will try to maximaize accuracy by changing max_depth and min_samples_leaf parameters. I will also check the accuracy of two criterion: gini and entropy.","aa3425ef":"DecisionTree and LogisticRegression with fixed parameters have the highest accuracy. Moreover, DecisionTree doesn't need standardized numeric data to work properly. As for DecisionTree, adding parameters increased accuracy by 3,5% in case of LogisticRegression this increase was much smaller. KNeighborsClassifier accuracy is smaller than the rest of algorithms. ","48ae7d3f":"## 1.1 Adjusting income column to binary type","3851b7c2":"# 2. Standardization and preparing variables"}}