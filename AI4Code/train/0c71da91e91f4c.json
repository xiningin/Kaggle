{"cell_type":{"465dc978":"code","e37d91bd":"code","acbb11d2":"code","b488a792":"code","85d4b165":"code","cb0552a1":"code","d95c2868":"code","79bfad0b":"code","0712516e":"code","3db25806":"code","cb216da6":"code","b5580d7b":"code","9939d068":"code","680f38bf":"code","1aec30e9":"code","4e8673d8":"code","8e594b97":"code","2f3d318d":"code","0c1b2575":"code","440f2775":"code","673b2dff":"code","334c2aa0":"code","0108e4c8":"code","2ebcb69e":"code","a3a7cc14":"code","1e510f9e":"code","8aa2de67":"code","bb688a12":"code","3d828e5a":"code","10e072c1":"code","0f91b98d":"code","8bde1afe":"code","25830efa":"code","cd98fb0f":"code","4f617cb0":"code","8d067e00":"code","44420061":"code","5c51de51":"code","07fb687c":"code","0dc30a5e":"code","692e0310":"code","30bfc884":"code","b695ecba":"code","238af341":"code","190f8676":"code","49b311be":"code","ee717a1a":"code","badcfc3a":"code","b1f0ac3c":"code","a27417be":"code","d670fe80":"markdown","18439c7e":"markdown","47022bba":"markdown","3ea4f8c9":"markdown","3703a765":"markdown","5b4611f0":"markdown","4f455dc0":"markdown","6138f387":"markdown","9371f9fe":"markdown","14ede6be":"markdown","13b6f481":"markdown","c94eda56":"markdown","3a55fca3":"markdown","bebc845f":"markdown","d7b402cd":"markdown","9b53f322":"markdown","91274527":"markdown","e13683d4":"markdown","99cdb593":"markdown","13c2ad30":"markdown","fee1f122":"markdown","71346faf":"markdown","e6559595":"markdown","9762f00d":"markdown","3c9e9993":"markdown","51c6e165":"markdown","249ef6e3":"markdown","6d81073d":"markdown","011818d7":"markdown","31d6bd13":"markdown","7bd3137f":"markdown","a1380cf5":"markdown","c203027c":"markdown","d5fb19ad":"markdown","c02db0b5":"markdown"},"source":{"465dc978":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.metrics import roc_auc_score,precision_recall_curve,roc_curve\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import model_selection\n\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\n\npd.set_option('max_columns', 50)","e37d91bd":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","acbb11d2":"%%time\ndf = pd.read_csv('\/kaggle\/input\/credit-risk\/original.csv')","b488a792":"# quick look at the dataset\ndf.info()","85d4b165":"target = 'default'  \nnum_vars = list(df.columns.drop(['clientid', 'default']))\n#cat_vars = list(df.columns.drop([''])) # there's no categorical features within the dataset","cb0552a1":"# choosing any id\ndf.tail()","d95c2868":"descriptive_measures = round(df.drop(['clientid','default'],axis=1).describe(),2)\ndescriptive_measures","79bfad0b":"# Visualiza\u00e7\u00e3o da vari\u00e1vel bin\u00e1ria (target)\ndf.default.value_counts().plot(kind = 'bar', color = ['C0', 'C1'], figsize=(6, 3), rot=0, title='PLOT: DEFAULT')\nplt.xlabel(\"EVENT: |  0=Default  |  1=No Default\")\nplt.ylabel(\"ABSOLUTE FREQUENCY (count)\");","0712516e":"# Overview of  binary variable (target)\ninad = df.default.value_counts()[0]\nad = df.default.value_counts()[1]\nprint('Total NO DEFAULT(0):', ad, 'lines, representing',round((ad\/(inad+ad))*100,2),'% of the dataset')\nprint('Total DEFAULT(1):', inad, 'lines, representing',round((inad\/(inad+ad))*100,2),'% of the dataset')","3db25806":"# code for plotting multiple HISTOGRAM\nfig, axs = plt.subplots(3,1, figsize=(10, 5),constrained_layout=True)\naxs[0].hist(df['income'],bins=100)\naxs[1].hist(df['age'], bins=100)\naxs[2].hist(df['loan'], bins=100)\nplt.show();","cb216da6":"# code for plotting multiple SCATTER PLOTS\nfig, axs = plt.subplots(3,1, figsize=(10,10),constrained_layout=True)\n\naxs[0].scatter(x=df['income'], y=df['default'], marker='o', color='r')\naxs[1].scatter(x=df['age'], y=df['default'], marker='o', color='r')\naxs[2].scatter(x=df['loan'], y=df['default'], marker='o', color='r')\n\n\nplt.show();","b5580d7b":"# code for plotting multiple BOXPLOTS\nfig, axes = plt.subplots(1, 3, figsize=(20, 5),constrained_layout=True)\nfig.suptitle('BOXPLOTS DE I, II and III', ha = 'right')\nmy_pal=\"Blues\"\n\nsns.boxplot(ax=axes[0], data=df, x = target, y=\"income\", palette=my_pal)\nsns.boxplot(ax=axes[1], data=df, x = target, y=\"age\", palette=my_pal)\nsns.boxplot(ax=axes[2], data=df, x = target, y=\"loan\", palette=my_pal);","9939d068":"# Asymmetry x Kurtosis calculation \ndata_k_s = {'Asymmetry':  df[num_vars].skew(), 'Kurtosis': df[num_vars].kurtosis()}\ndf_k_s = pd.DataFrame (data_k_s, columns = ['Asymmetry','Kurtosis'])\ndf_k_s","680f38bf":"(df.isnull().sum()* 100 \/ len(df)).sort_values(ascending=False).head(10).reset_index()","1aec30e9":"df.dropna(inplace=True)","4e8673d8":"df.drop(columns=\"clientid\",inplace=True)\n\nfeatures = df.columns.values[0:30]\nunique_max_train = []\nfor feature in features:\n    values = df[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])\n\nnp.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(15))","8e594b97":"df['default'].value_counts()","2f3d318d":"train_X = df.drop([\"default\"], axis=1)\ntrain_y = np.log1p(df[\"default\"].values)","0c1b2575":"from sklearn import ensemble\nmodel = ensemble.ExtraTreesRegressor(n_estimators=200, max_depth=20, max_features=0.5, n_jobs=-1, random_state=0)\nmodel.fit(train_X, train_y)\n\n## plotando as import\u00e2ncias ##\nfeat_names = df.columns.values\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","440f2775":"def plot_new_feature_distribution(df1, df2, label1, label2, features, n):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,n,figsize=(18,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,n,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=11)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","673b2dff":"def plot_roc(y_test,prob):\n    fpr, tpr, thresholds = roc_curve(y_test, probs)\n    # plot no skill\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    # plot the roc curve for the model\n    plt.plot(fpr, tpr, marker='.')\n    plt.title(\"ROC curve\")\n    plt.xlabel('false positive rate')\n    plt.ylabel('true positive rate')\n    # show the plot\n    plt.show()","334c2aa0":"def logistic(X,y):\n    y_train=df['default'].astype('uint8')\n    X_train,X_test,y_train,y_test=train_test_split(df.drop('default',axis=1),y_train,test_size=.2,random_state=2020)\n    lr=LogisticRegression()\n    lr.fit(X_train,y_train)\n    prob=lr.predict_proba(X_test)\n    \n    roc=roc_auc_score(y_test, lr.predict_proba(X_test)[:,1])\n    print('roc ',roc)\n\n    return (prob[:,1],y_test)\ny_train=df['default'].astype('uint8')\nprobs,y_test=logistic(df.drop('default',axis=1),y_train)","0108e4c8":"plot_roc(y_test,probs)","2ebcb69e":"train_x, val_x, train_y, val_y=train_test_split(df.drop('default',axis=1),y_train,test_size=.2,random_state=2020)\n\nclf = LogisticRegression(solver=\"lbfgs\").fit(train_x, train_y)\npred_y = clf.predict(val_x)","a3a7cc14":"rocrl=roc_auc_score(val_y, pred_y)\nrocrl","1e510f9e":"plot_roc(val_y, pred_y)","8aa2de67":"%%time\nmodel = RandomForestClassifier(n_estimators=220).fit(train_x,train_y)\npredictionforest = model.predict(val_x)","bb688a12":"%%time\nrocrf=roc_auc_score(val_y, predictionforest)\nprint('roc ',rocrf)","3d828e5a":"plot_roc(val_y, predictionforest)","10e072c1":"xgb_cfl = xgb.XGBClassifier(n_jobs = -1, \n                            n_estimators = 220)\n\nxgb_cfl.fit(train_x, train_y)\ny_scorexgb = xgb_cfl.predict_proba(val_x)[:,1]","0f91b98d":"rocxgb=roc_auc_score(val_y, y_scorexgb)\nprint('roc ',rocxgb)","8bde1afe":"plot_roc(val_y, y_scorexgb)","25830efa":"%%time\n# Logistic regression \nlog_cfl = LogisticRegression()\n\nparam_grid = {\n            'penalty' : ['l1','l2'], \n            'class_weight' : ['balanced', None], \n            'C' : [0.1, 1, 10, 100]\n            }\n\nCV_log_cfl = GridSearchCV(estimator = log_cfl, param_grid = param_grid , scoring = 'roc_auc', verbose = 1, n_jobs = -1)\nCV_log_cfl.fit(train_x, train_y)\n\nbest_parameters = CV_log_cfl.best_params_\nprint(\"The best parameters: \", best_parameters)","cd98fb0f":"log_cfl = LogisticRegression(C = best_parameters['C'], \n                             penalty = best_parameters['penalty'], \n                             class_weight = best_parameters['class_weight'])\n\nlog_cfl.fit(train_x, train_y)\ny_scoreLR = log_cfl.decision_function(val_x)","4f617cb0":"rocLR=roc_auc_score(val_y, y_scoreLR)\nprint('roc ',rocLR)","8d067e00":"plot_roc(val_y, y_scoreLR)","44420061":"rf_cfl = RandomForestClassifier(n_estimators=120).fit(train_x, train_y)","5c51de51":"param_grid = {\n            'n_estimators': [50, 100, 200],\n            'max_features': [2, 3],\n            'min_samples_leaf': [1, 2, 4],\n            'min_samples_split': [2, 5, 10]\n            }\n\nCV_rnd_cfl = GridSearchCV(estimator = rf_cfl, param_grid = param_grid, scoring = 'roc_auc', verbose = 10, n_jobs = -1)\nCV_rnd_cfl.fit(train_x, train_y)\n\nbest_parameters = CV_rnd_cfl.best_params_\nprint(\"The best parameters: \", best_parameters)","07fb687c":"rf_cfl = RandomForestClassifier(n_estimators = best_parameters['n_estimators'], \n                                 max_features = best_parameters['max_features'],  \n                                 min_samples_leaf = best_parameters['min_samples_leaf'],  \n                                 min_samples_split = 5)\nrf_cfl.fit(train_x, train_y)\ny_score = rf_cfl.predict_proba(val_x)[:,1]","0dc30a5e":"rocRF=roc_auc_score(val_y, y_score)\nprint('roc ',rocRF)","692e0310":"plot_roc(val_y, y_score)","30bfc884":"xgb_cfl = xgb.XGBClassifier(n_jobs = -1)\nxgb_cfl.fit(train_x, train_y)\ny_pred = xgb_cfl.predict(val_x)\ny_score = xgb_cfl.predict_proba(val_x)[:,1]","b695ecba":"%%time\nparam_grid = {\n            'n_estimators': [50, 100, 200]\n              }\n\nCV_xgb_cfl = GridSearchCV(estimator = xgb_cfl, param_grid = param_grid, scoring ='roc_auc', verbose = 2)\nCV_xgb_cfl.fit(train_x, train_y)\n\nbest_parameters = CV_xgb_cfl.best_params_\nprint(\"The best parameters: \", best_parameters)","238af341":"%%time\nparam_grid = {\n            'n_estimators': [50, 100, 200]\n              }\n\nCV_xgb_cfl = GridSearchCV(estimator = xgb_cfl, param_grid = param_grid, scoring ='roc_auc', verbose = 2)\nCV_xgb_cfl.fit(train_x, train_y)\n\nbest_parameters = CV_xgb_cfl.best_params_\nprint(\"The best parameters: \", best_parameters)","190f8676":"xgb_cfl2 = xgb.XGBClassifier(n_jobs = -1,n_estimators = 120)\n\nxgb_cfl2.fit(train_x, train_y)\ny_score2 = xgb_cfl.predict_proba(val_x)[:,1]","49b311be":"rocxgb=roc_auc_score(val_y, y_score2)\nprint('roc ',rocxgb)","ee717a1a":"plot_roc(val_y, y_score2)","badcfc3a":"voting_cfl = VotingClassifier (\n        estimators = [('xgb', xgb_cfl2), ('lt', log_cfl), ('rf', rf_cfl)],\n                     voting='soft', weights = [0.27, 0.23, 0.5])\n    \nvoting_cfl.fit(train_x, train_y)\n\ny_pred = voting_cfl.predict(val_x)\ny_score = voting_cfl.predict_proba(val_x)[:,1]","b1f0ac3c":"rocV=roc_auc_score(val_y, y_score)\nprint('roc ',rocV)","a27417be":"plot_roc(val_y, y_score)","d670fe80":"##  \ud83d\udcac **What's the Overall EDA and Local EDA?**\n\nI've created these 2 techniques to be applied into the EDA: I call them as Local EDA and Overall EDA. But, before we diving into them, I trully recommend to you to have a data dictionary (if possible) and to take down any comments, questions or alert points during this journey. One last tip I'd like to share is to clear out any questions along with Business Areas (if possible), since they know the business and the every day process as a whole. \n\n\n**LOCAL EDA**\n\nUnderstanding of **DATA STORY**, literally exploring the database. How? Just take any 'id' and navigate throughout the database (or databases) and over this 'navegation', try to understand what's going on with this 'id' within the dataset (datasets). The local EDA only works if your data has 'id' column or any column that individualises each database line. Apart from data dictionary, another option is, if none of the previous ones work, to gather info from Internet, articles, books, etc, in order to get the business context and insights and use them over your data analysis (do not forget taking down questions to be answered by Data Areas or Business Areas). So, the main goals of the local EDA method are:\n* to have a more focused view of what is happening with such an 'id' across the database (or databases)\n* to have a better understanding of the **DATA STORY** (what the data tells us)\n* to write down any questions to be asked to the Business Area\n\n**OVERALL EDA**\n\nUnderstanding the whole data statistical distribution pattern (I personally recommend this great tool that helps out a lot: panda profiling. So, the main objectives behind the Overall EDA technique are:\n* to detect possible inconsistencies within the data;\n* to have a general comprehension of the descriptive measures and, therefore, the data statistical distribution\n* to have a general understanding of \u201cdirty\u201d data or \u201calert\u201d data (zeros, missing values, Outliers, etc.)\n* to write down any questions to be asked to the Business Area\n","18439c7e":"* \ud83d\udd0e = EDA\n* \ud83d\udd27 = Feature Engineering\n* \ud83d\udcdd = Feature Selection\n* \ud83e\uddea = Feature Transformation\n* \ud83e\uddee = Descriptive Measures or Descriptive Analysis\n* \u26cf = Data-Mining\n* \ud83e\uddf9\ud83d\uddd1 = Data Cleaning & Dumping\n* \ud83d\udcca = Data Visualization\n* \u2702 = Cut-offs\n* \ud83d\udcac = Comments or Explanations \n* \ud83d\udce5 = Loading dataset \n* \ud83d\udcbe = Saving dataset\n* \ud83e\udd47 = Best\n* \ud83d\udd10 = Code Session\n* \ud83d\udcda = Data Dictionary or Python Libraries\n* \u26a1 = Important stuff","47022bba":"### \ud83e\uddee Descriptive Measures","3ea4f8c9":"### Overall EDA","3703a765":"### Baseline","5b4611f0":"## Data Description\n\n### Objective\n\nThe purpose of this database is to provide information about a bank's customers so that machine learning models can be developed that can predict whether a particular customer will repay the loan or not.\n\nVariable Name Description Type\n\n* clientid = Client id Person\n* income   = Income of the client\n* age      = Age of client\n* loan     = Load value.\n* default  = target 0 good client, 1 not.","4f455dc0":"## Grid Search CV","6138f387":"#### *Logistic Regression*","9371f9fe":"# \ud83d\udd10 Code Session","14ede6be":"## Icon Description\n### What's its purpose? \ud83e\udd14\nIn order to help the kagglers and visitors to navegate through the notebook with something that highlights the topic and avoid getting lost during the reading.","13b6f481":"#### dependent variable (*target*)","c94eda56":"## \ud83d\udce5 Load dataset","3a55fca3":"#### independent variable (*features*)","bebc845f":"### Missing Values","d7b402cd":"-----------------------","9b53f322":"Well...after those brief explanations, it's time to put our hands on! \nLet's have fun a bit! \ud83d\ude09","91274527":"## \ud83d\udcda Imports","e13683d4":"### Data Duplicate Analysis","99cdb593":"#### *Random Forest Classifier*","13c2ad30":"### Local EDA\n","fee1f122":"## \ud83d\udd0e EDA (Exploratory Data Analysis) ","71346faf":"## Final","e6559595":"## \ud83d\udcac What's the EDA?\n\nI've literally copied the meaning of EDA from this website https:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section1\/eda11.htm 'cus it's a fabulous definition of it.\n\n\"...Exploratory Data Analysis (EDA) is an approach\/philosophy for data analysis that employs a variety of techniques (mostly graphical) to:\n\n1. maximize insight into a dataset;\n2. uncover underlying structure;\n3. extract important variables;\n4. detect outliers and anomalies;\n5. test underlying assumptions;\n6. develop parsimonious models; and\n7. determine optimal factor settings\n\n...\"\n","9762f00d":"## \ud83d\udcac **What's the Automated EDA?**\n\n**AUTOMATED EDA (AEDA)**\n#### Have you ever heard of it?\n\nBasically, the Automated Exploratory Data Analysis (AEDA) is a tool to explore the database and summarize the main characteristics of it often using visual method. My favourite one is Pandas Profiling.\n\nI've been readind so many articles on Internet to raise all types of Automated EDA (AEDA) to share here on Kaggle. From all thoses papers, one of them has caught my attention: it's an article written by Saurav Anand. This blessed soul has made a terrific curatorship about AEDA and, in my humble opinion, it\u2019s well worth having a look at it at any moment you are free. Here is the link (below): \n* https:\/\/medium.com\/datadriveninvestor\/10-python-automatic-eda-libraries-which-makes-data-scientist-life-easier-825d0a928570\n\nOther great sources:\n* https:\/\/towardsdatascience.com\/sweetviz-automated-eda-in-python-a97e4cabacde\n* https:\/\/medium.com\/analytics-vidhya\/automated-eda-for-classification-77c25b847e43\n* https:\/\/analyticsindiamag.com\/tips-for-automating-eda-using-pandas-profiling-sweetviz-and-autoviz-in-python\/#:~:text=Exploratory%20Data%20Analysis%20(EDA)%20is,data%20we%20are%20working%20on.&text=that%20the%20data%20set%20is,on%20by%20visualizing%20the%20dataset.\n","3c9e9993":"#### *Voting Classifier*","51c6e165":"### \ud83d\udcca Data Visualization","249ef6e3":"#### *XGBClassifier*","6d81073d":"> ## Variable types\n\nI prefere to use this approach of separating variable types as a list to make clear which one is numerical and which one is categorical. In my opinion, this quick split helps to see suitable data transformation and data plots for each case. Thus, it's up to the data scientist's convenience to use it or not.","011818d7":"#### *Logistic Regression*","31d6bd13":"###  <font color=blue>\ud83d\udcac Comments\n<font color=darkblue>\n<pre><\/pre> \n    \n**INCOME**\n    \n- The feature varies between 20014 and 69995.7 thousand, with an income average ~ 45331.6. Important aspects when looking at data:\n     - 1) **quartiles** (Q1, Q2 and Q3) inform us of **quantity** and the respective value of **variation** around **32796.46 to 57791.28**;\n        - Interpretation Q1 tells us that 25% of data has a income up to 32796.46 and 75% of data has an income above from it\n        - Interpretation Q2 tells us that 50% of data has a income up to 45789.12 and 75% of data has an income above from it\n        - Interpretation Q3 tells us that 75% of data has a income up to 57791.28 and 25% of data has an income above from it\n     - 2) The MINIMUM value of 20014 and MAXIMUM value of 69995.7 does not indicate **POSSIBLE INCONSISTENCY IN DATA** once there is nothing within data that really catches my eye\n     - 3) No missing values (0% of missing values)\n     - 4) **Tendency**: according to scatter plot (above),there's no standard to define such tendency. There are ids within good or bad payer at any income available\n     - 5) According to graphical visualization (scatter and boxplot), the feature 'INCOME' does not seem to be a candidate variable for predictor to explain the variability of y (Feature Selection), since there's no differences between the variabilities of each label, 1 and 0. Therefore, such visualizations have the sensation of lack of \"movement\" in both charts. The ideal situation is to TEST and get the outcomes. Perhaps if we combined with other variables, this feature becomes significant for the model \n<pre> <\/pre>\n    \n**AGE**\n    \n- The feature varies between -52 and 64, with an age average ~ 41 years old. Important aspects when looking at data:\n     - 1) **quartiles** (Q1, Q2 and Q3) tell us of **quantity** and the respective value of **variation** around **29 to 53**;\n        - Interpretation Q1 tells us that 25% of data has an age up to 29 y.o and 75% of data has an age above from it\n        - Interpretation Q2 tells us that 50% of data has an age up to 41 and 75% of data has an income above from it\n        - Interpretation Q3 tells us that 75% of data has an age up to 53 and 25% of data has an income above from it\n     - 2) The MINIMUM age of -52 and MAXIMUM age of 64 indicates **POSSIBLE INCONSISTENCY IN DATA** (who has a negative age, ins't it?). The ideal situation is to question Business Area and\/or Data Area to clear it up. \n     - 3) Missing values detected in data (0.1% of missing values). However, this low percent won't cause any risk or impact on modeling\n     - 4) **Tendency**: according to scatter plot (above), the older the person is, the more likely he\/she is to be considered as a good payer and vice-verse.\n     - 5) According to graphical visualization (scatter and boxplot), the feature 'AGE' does seem to be a candidate variable for predictor to explain the variability of y (Feature Selection), since there's differences between the variabilities of each label, 1 and 0. Therefore, such visualizations have the sensation of \"movement\" in both charts. The ideal situation is to TEST during the modeling.    \n<pre> <\/pre>\n    \n**LOAN**\n    \n- The feature varies between 1.38 and 13766.05, with a loan average ~ 4444.37. Important aspects when looking at data:\n     - 1) **quartiles** (Q1, Q2 and Q3) tell us of **quantity** and the respective value of **variation** around **1939.71 to 6432.41**;\n        - Interpretation Q1 tells us that 25% of data has a loan up to 1939.71 and 75% of data has an age above from it\n        - Interpretation Q2 tells us that 50% of data has a loan up to 3974.72 and 75% of data has an income above from it\n        - Interpretation Q3 tells us that 75% of data has a loan up to 6432.41 and 25% of data has an income above from it\n     - 2) The MINIMUM age of 1.38 and MAXIMUM age of 13766.05 does not indicate **POSSIBLE INCONSISTENCY IN DATA** once there is nothing within data that really catches my eye \n     - 3) No missing values (0% of missing values)\n     - 4) **Tendency**: according to scatter plot (above), any loan with values up to 2500 shows a trend towards good payers. On the other hand, values above 2500 do not make it possible to separate good and bad payers.\n     - 5) According to graphical visualization (scatter and boxplot), the feature 'AGE' does seem to be a candidate variable for predictor to explain the variability of y (Feature Selection), since there's differences between the variabilities of each label, 1 and 0. Therefore, such visualizations have the sensation of \"movement\" in both charts. The ideal situation is to TEST during the modeling.   \n","7bd3137f":"#### *Random Forest Classifier*","a1380cf5":"## Credit risk\n### Will the Customer pay the financing or not?\n![image.png](attachment:image.png)\n\n*Source*: www.smithhanley.com\/2018\/06\/07\/credit-risk-managers-big-data\/\n\n\nHello Kagglers, how's it going?\nIn this notebook we're gonna dive into the INCREDIBLE credit risk world and how to do an EDA for that matter. Before we get started, let's have a look at theory and, then, move on more aspects. Enjoy the reading! Stay tuned! \n\nBest Regards,\nCris & Rodrigo\n\n\nP.s: I've literraly copied all the info from Investopedia. For more information, please go to https:\/\/www.investopedia.com\/ \n\n#### DEFINITION\n...\"Credit risk is the possibility of a loss resulting from a borrower's failure to repay a loan or meet contractual obligations. Traditionally, it refers to the risk that a lender may not receive the owed principal and interest, which results in an interruption of cash flows and increased costs for collection. Excess cash flows may be written to provide additional cover for credit risk. When a lender faces heightened credit risk, it can be mitigated via a higher coupon rate, which provides for greater cash flows.\n\n\nAlthough it's impossible to know exactly who will default on obligations, properly assessing and managing credit risk can lessen the severity of a loss. Interest payments from the borrower or issuer of a debt obligation are a lender's or investor's reward for assuming credit risk...\"\n\n\n#### KEY TAKEAWAYS\n* Credit risk is the possibility of losing a lender takes on due to the possibility of a borrower not paying back a loan. \n* Consumer credit risk can be measured by the five Cs: credit history, capacity to repay, capital, the loan's conditions, and associated collateral.\n* Consumers posing higher credit risks usually end up paying higher interest rates on loans.\n\n\n\n#### UNDESTANDING CREDIT RISK\nWhen lenders offer mortgages, credit cards, or other types of loans, there is a risk that the borrower may not repay the loan. Similarly, if a company offers credit to a customer, there is a risk that the customer may not pay their invoices. Credit risk also describes the risk that a bond issuer may fail to make payment when requested or that an insurance company will be unable to pay a claim.\n\nCredit risks are calculated based on the borrower's overall ability to repay a loan according to its original terms. To assess credit risk on a consumer loan, lenders look at the five Cs: credit history, capacity to repay, capital, the loan's conditions, and associated collateral.1\n\nSome companies have established departments solely responsible for assessing the credit risks of their current and potential customers. Technology has afforded businesses the ability to quickly analyze data used to assess a customer's risk profile.\n\nIf an investor considers buying a bond, they will often review the credit rating of the bond. If it has a low rating (< BBB), the issuer has a relatively high risk of default. Conversely, if it has a stronger rating (BBB, A, AA, or AAA), the risk of default is progressively diminished.\n\nBond credit-rating agencies, such as Moody's Investors Services and Fitch Ratings, evaluate the credit risks of thousands of corporate bond issuers and municipalities on an ongoing basis.2\ufeff 3\ufeff For example, a risk-averse investor may opt to buy an AAA-rated municipal bond. In contrast, a risk-seeking investor may buy a bond with a lower rating in exchange for potentially higher returns.\n\n\n\n#### CREDIT RISK VS. INTEREST RATES\n\u26a1 **Important**: if there is a higher level of perceived credit risk, investors and lenders usually demand a higher rate of interest for their capital.\n\nCreditors may also choose to forgo the investment or loan. For example, because a mortgage applicant with a superior credit rating and steady income is likely to be perceived as a low credit risk, they will receive a low-interest rate on their mortgage. In contrast, if an applicant has a poor credit history, they may have to work with a subprime lender\u2014a mortgage lender that offers loans with relatively high-interest rates to high-risk borrowers\u2014to obtain financing. The best way for a high-risk borrower to acquire lower interest rates is to improve their credit score; those struggling to do so might want to consider working with one of the best credit repair companies.\n\nSimilarly, bond issuers with less-than-perfect ratings offer higher interest rates than bond issuers with perfect credit ratings. The issuers with lower credit ratings use high returns to entice investors to assume the risk associated with their offerings.\n\n\n\n*Source*: https:\/\/www.investopedia.com\/terms\/c\/creditrisk.asp\n","c203027c":"###  <font color=blue>\ud83d\udcac Comments\n<font color=darkblue>\n<pre><\/pre> \n\n* The 'PLOT:DEFAULT' chart above shows that the DEFAULT event is considered a **RARE EVENT**, representing **14.15%** of the total \n* \ud83d\udea9 **ATTENTION POINT**: **UNBALANCED** data (think about some type of treatment for a model such as: Oversampling or Undersampling)","d5fb19ad":"#### Value Counts Target","c02db0b5":"###  <font color=blue>\ud83d\udcac Comments\n<font color=darkblue>\n<pre><\/pre> \n\nI've randomly chosen two IDs to understand what happened with them through data. The client ID number **1996** is **48** years old and posses a sallary around **59221.04** dollars and a loan that represents **3.25%** of the income. On the other hand, the client ID **1998** is younger (**28** years of age) and has an income around **44311.45** and a loan that represents **12.46%**.\n\n**Data Story**: It's a database that shows us different client profiles and their financial health (how much they make and what is the debt percent they have)"}}