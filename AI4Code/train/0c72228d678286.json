{"cell_type":{"37570967":"code","db2816b3":"code","1c19f3ca":"code","3f8eb244":"code","7d0947df":"markdown"},"source":{"37570967":"\nimport os\nfrom pathlib import Path\nin_folder_path = Path('..\/input\/roberta-large-cv424')\nscripts_dir = Path(in_folder_path \/ 'scripts')","db2816b3":"\nos.chdir(scripts_dir)\nexec(Path(\"imports.py\").read_text())\nexec(Path(\"config.py\").read_text())\nexec(Path(\"dataset.py\").read_text())\nexec(Path(\"model.py\").read_text())\nos.chdir('\/kaggle\/working')","1c19f3ca":"\ntest_df = pd.read_csv(\"\/kaggle\/input\/commonlitreadabilityprize\/test.csv\")\n\ntokenizer = torch.load(in_folder_path \/ 'tokenizer.pt')\nmodels_folder_path = Path(in_folder_path \/ 'models')\nmodels_preds = []\nn_models = 5\n\nfor model_num in range(n_models):\n    print(f'Inference#{model_num+1}\/{n_models}')\n    test_ds = CLRPDataset(data=test_df, tokenizer=tokenizer, max_len=Config.max_len, is_test=True)\n    test_sampler = SequentialSampler(test_ds)\n    test_dataloader = DataLoader(test_ds, sampler = test_sampler, batch_size=Config.batch_size)\n    model = torch.load(models_folder_path \/ f'best_model_{model_num}.pt').to(Config.device)\n\n    all_preds = []\n    model.eval()\n\n    for step,batch in enumerate(test_dataloader):\n        sent_id, mask = batch['input_ids'].to(Config.device), batch['attention_mask'].to(Config.device)\n        with torch.no_grad():\n            preds = model(sent_id, mask)\n            all_preds += preds.flatten().cpu().tolist()\n    \n    models_preds.append(all_preds)","3f8eb244":"models_preds = np.array(models_preds)\nprint(models_preds.shape)\nprint(models_preds)\nall_preds = models_preds.mean(axis=0)\nprint(all_preds.shape)\nresult_df = pd.DataFrame(\n    {\n        'id': test_df.id,\n        'target': all_preds\n    })\n\n\nresult_df.to_csv('submission.csv', index=False)\nresult_df.head(10)","7d0947df":"# **Notebooks sequence;)**\n* Train-val split notebook [here](https:\/\/www.kaggle.com\/chamecall\/train-val-split).<br>\n* Pretrain roberta-base on mlm with the competition data notebook [here](https:\/\/www.kaggle.com\/chamecall\/clrp-pretrain).<br>\n* Finetune pretrained roberta-base on readability task notebook [here](https:\/\/www.kaggle.com\/chamecall\/clrp-finetune).<br>\n* Inference model notebook [*CURRENT ONE*].<br>"}}