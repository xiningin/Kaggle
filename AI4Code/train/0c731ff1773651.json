{"cell_type":{"0fa34fdb":"code","0c4c5394":"code","c25cfb61":"code","984070b9":"code","38cf367a":"code","15590569":"code","2a0214a5":"code","590c7874":"code","dd077eca":"code","2ec63af5":"code","3c4e41bd":"code","6faccfb9":"code","d41e4806":"code","6e9a7bd5":"code","106023f6":"code","9dd059bf":"code","2b81bd79":"code","c409745e":"code","f3b0c225":"code","d15b47d2":"code","60fe7179":"code","0a29cb31":"code","fa56d49d":"code","86992927":"code","4aa31fa9":"code","5f5a76b0":"code","fee862d3":"code","cf197ff9":"code","8be22b81":"code","b8f0b69e":"code","ba98b694":"code","702a0ae5":"code","b02df337":"code","4438977c":"code","180020a4":"code","b7dc840c":"code","58e107be":"code","8827a605":"code","ca6a8ed8":"code","42db1c0c":"code","eb8549f0":"code","05504918":"code","243cbea1":"code","bef7eec7":"code","caaa7f19":"code","841ba85f":"code","0d1763bc":"code","11f9dedd":"code","5db8f8e5":"code","329a7cac":"code","f8d3af1f":"code","e8fe53d8":"code","c4a57496":"code","19496859":"code","e467a082":"code","481ecdd0":"code","a18192bb":"code","7ee39498":"code","8719a3c8":"code","2e934792":"code","cb2fd369":"code","8297663e":"code","222d3e3d":"code","9f53699c":"code","c211c86d":"code","53c7f04f":"code","63a5d134":"code","4dec9012":"code","25678968":"code","c1c7b572":"code","c5aecc13":"code","c6966733":"code","7cae7936":"code","48033fc0":"code","cf48d623":"code","f2bc9c00":"code","6093bbc3":"code","7256220e":"code","7bf97eca":"code","234cf667":"code","66441286":"code","e7dffa68":"code","f88adf1b":"code","3234b2c5":"code","bc570979":"code","f3bc7e5d":"code","6126410d":"code","bdea4829":"code","b555ad08":"code","ca04b7da":"code","9b49d17c":"code","f20f3e21":"code","6d203ff1":"code","44a5ed04":"code","36ef2250":"code","2f990cfd":"code","f09c16cf":"code","59b6708a":"code","3a2e9615":"code","92a3d603":"code","55a92922":"code","01c5bcd5":"code","9369bf76":"code","8a2c1693":"code","db149237":"code","24b5bae3":"code","d65448dc":"code","8facc0f7":"code","561c981d":"code","1e2e7d28":"code","dc56f1e2":"code","8a644c84":"code","bd976e6d":"code","998e865e":"code","26e17647":"code","86b9b3e4":"code","0eef2bbe":"code","e56c1ffc":"code","d7d97879":"code","4a61ff23":"code","52afe200":"code","3e462474":"code","bf42a32b":"code","660dae07":"code","0b3f053c":"code","8c07a847":"code","71e99083":"code","6d60382e":"code","ad8b50b2":"code","c9bcc225":"code","f6f49380":"code","bbfa1e04":"code","5b533771":"code","6132f142":"code","8b21a609":"code","d53898f2":"code","20aa184d":"code","3c86e3c1":"code","19243730":"code","a6ef8625":"code","83602458":"markdown","fdeb121b":"markdown","a6cbe3f7":"markdown","d605b6fd":"markdown","49f856a4":"markdown","253d5e27":"markdown","799702ae":"markdown","ec289139":"markdown","db1ceb98":"markdown","7117f52f":"markdown","2d6a5d08":"markdown","4c93b370":"markdown","b144bba6":"markdown","78d4d8e5":"markdown","1255ac24":"markdown","f5f00318":"markdown","5f8216ce":"markdown","7cdf82c0":"markdown","d4175368":"markdown","28cdc8fe":"markdown","194f3797":"markdown","6a2d1d21":"markdown","9c620444":"markdown","b34d5b4f":"markdown","290dd241":"markdown","4a77a837":"markdown","6f93d18d":"markdown","1cfa8b93":"markdown","2c336d2b":"markdown","dea6ce3c":"markdown","0ee6450d":"markdown","13634746":"markdown","464bfc37":"markdown","16d13bcf":"markdown","848369d3":"markdown","89672bee":"markdown","bf1af0f2":"markdown","22f9237d":"markdown","87f2193f":"markdown","26750c5c":"markdown","f5531408":"markdown","fea5d935":"markdown","726f8f92":"markdown","a07482cf":"markdown","a9917670":"markdown","0f902c84":"markdown","fded4a52":"markdown","3a1ece3a":"markdown","3a087134":"markdown","2a9c852c":"markdown","da19eb44":"markdown","db08531c":"markdown","79c5cbd0":"markdown","e76f4d2e":"markdown"},"source":{"0fa34fdb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0c4c5394":"from fastai.tabular.all import Path","c25cfb61":"path = Path('..\/input\/bluebook-for-bulldozers')","984070b9":"df = pd.read_csv(path\/'TrainAndValid.csv', low_memory=False)","38cf367a":"df.columns","15590569":"df['ProductSize'].unique()","2a0214a5":"# we can tell Pandas for a suitable ordering like\n\nsizes = 'Large', 'Large \/ Medium', 'Medium', 'Small', 'Mini', 'Compact'","590c7874":"df['ProductSize'] = df['ProductSize'].astype('category')\nprint(df['ProductSize'])\ndf['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\nprint(df['ProductSize'])","dd077eca":"df['ProductSize']","2ec63af5":"dep_var = 'SalePrice'","3c4e41bd":"df[dep_var] = np.log(df[dep_var])","6faccfb9":"# my attempt at above algorithm\n# https:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/all-about-decision-tree-from-scratch-with-python-implementation\/\n\n# picking categorical and non categorical\n# https:\/\/stackoverflow.com\/questions\/35826912\/what-is-a-good-heuristic-to-detect-if-a-column-in-a-pandas-dataframe-is-categori\nlikely_cat = {}\nfor i in df.columns:\n    #print(i)\n    likely_cat[i] = 1.*df[i].nunique()\/df[i].count() < 0.05 \n    \n# also handled by cont_cat_split","d41e4806":"likely_cat\n\n# these are columns that mightbe categorical","6e9a7bd5":"len(df)","106023f6":"import math\n\n# 1 level of dcision tree split \n# with randomly chosen 1 category\n\n# looping through each column\nfor i in df.columns:\n    # looping through levels\n    #print(\"levels for \" , i)\n    #print(df[i].nunique())\n    \n    # splitting into groups\n    if(df[i].dtype.kind in 'biufc' and df[i].nunique() > 20):\n        # it means that the value is continous\n        print(\" FOR CONTINOUS \", i)\n        print(\" SPLIT AT \", df[i].mean())\n        split_mean = df[i].mean()\n        \n        # splitting dataframe based on df[i].mean()\n        df_left = df[df[i] < split_mean]\n        df_right = df[df[i] > split_mean]\n        \n        print(\" SALEPRICE FOR LEFT NODE \", df_left[dep_var].mean())\n        print(\" SALEPRICE FOR RIGHT NODE \", df_right[dep_var].mean())\n        print(\" ACTUAL SALEPRICE FOR NODE \", df[dep_var].mean())\n        \n        print(\"\\n\")\n        \n    else:\n        # it means that the value is categorical\n        print(\" FOR CATEGORICAL \", i)\n        # lets split at random unique column\n        print(\" SPLIT AT \", df[i].unique()[math.floor(len(df[i].unique())\/2)])\n        split_cat =  df[i].unique()[math.floor(len(df[i].unique())\/2)]\n        \n        df_cat = df[df[i] == split_cat]\n        print(\" SALEPRICE FOR SPLIT NODE \", df_cat[dep_var].mean())\n        print(\" ACTUAL SALEPRICE FOR NODE \", df[dep_var].mean())\n        \n        print(\"\\n\")\n    \n\n# we are not doing it recursively nor looping through all columns\n# but seems like it gives a better understanding about what decision tree does","9dd059bf":"from fastai.tabular.all import add_datepart","2b81bd79":"df = add_datepart(df, 'saledate')","c409745e":"# same for test\ndf_test = pd.read_csv(path\/'Test.csv', low_memory=False)\ndf_test = add_datepart(df_test, 'saledate')","f3b0c225":"[o for o in df.columns if o.startswith('sale')]","d15b47d2":"from fastai.tabular.all import Categorify, FillMissing","60fe7179":"# I guess we will use it often in tabular data\nprocs = [Categorify, FillMissing]","0a29cb31":"cond = (df.saleYear<2011) | (df.saleMonth<10)\ntrain_idx = np.where(cond)[0]\nvalid_idx = np.where(~cond)[0]\n\nsplits = (list(train_idx), list(valid_idx))","fa56d49d":"from fastai.tabular.all import cont_cat_split, TabularPandas","86992927":"cont, cat = cont_cat_split(df, 1, dep_var=dep_var)\nto = TabularPandas(df, procs,cat,cont, y_names=dep_var, splits=splits)","4aa31fa9":"cont","5f5a76b0":"len(to.train), len(to.valid)","fee862d3":"to.show(3)","cf197ff9":"to1 = TabularPandas(df, procs, ['state', 'ProductGroup', 'Drive_System', 'Enclosure' ], [], y_names=dep_var, splits=splits)","8be22b81":"to1.show(3)","b8f0b69e":"to.items.head(3)['ProductGroup']\n# the underlying items are all numeric","ba98b694":"to1.items[['state', 'ProductGroup', 'Drive_System', 'Enclosure']].head(3)","702a0ae5":"to.classes['ProductSize']","b02df337":"try:\n    to.items['ProductSize']\nexcept Exception as e:\n    print(e)","4438977c":"from fastai.tabular.all import save_pickle","180020a4":"save_pickle('to.pkl',to)","b7dc840c":"try:\n    to = (path\/'to.pkl').load()\nexcept Exception as e:\n    print(e)\n    print('Am I living a lie?')","58e107be":"from fastai.tabular.all import load_pickle","8827a605":"to = load_pickle('to.pkl')","ca6a8ed8":"xs, y = to.train.xs, to.train.y\nvalid_xs, valid_y = to.valid.xs, to.valid.y","42db1c0c":"from sklearn.tree import DecisionTreeRegressor","eb8549f0":"m = DecisionTreeRegressor(max_leaf_nodes=4)\nm.fit(xs, y)","05504918":"# visualising tree - https:\/\/mljar.com\/blog\/visualize-decision-tree\/\nfrom sklearn import tree","243cbea1":"text_representation = tree.export_text(m)\nprint(text_representation)","bef7eec7":"for i in to.__dir__():\n    print(i)","caaa7f19":"print(to.x_names)","841ba85f":"to.y_names","0d1763bc":"_ = tree.plot_tree(m,feature_names=to.x_names, class_names=to.y_names, filled=True)","11f9dedd":"!pip install dtreeviz","5db8f8e5":"from dtreeviz.trees import *","329a7cac":"samp_idx = np.random.permutation(len(y))[:500]\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var,\n        fontname='DejaVu Sans', scale=1.6, label_fontsize=10,\n        orientation='LR')","f8d3af1f":"xs.loc[xs['YearMade']<1900, 'YearMade'] = 1950\nvalid_xs.loc[valid_xs['YearMade']< 1900, 'Yearmade'] = 1950","e8fe53d8":"xs['YearMade'].unique()","c4a57496":"m = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs,y)\n\ndtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var, scale=1.6, label_fontsize=10, orientation='LR')","19496859":"# lets make a biggertree with no stoppingcriteria\n\nm = DecisionTreeRegressor()\nm.fit(xs, y)","e467a082":"# creating a funciton to checck the root mean squared error of our model\n\ndef r_mse(pred, y):\n    return round(math.sqrt(((pred-y)**2).mean()), 6)\n\ndef m_rmse(m, xs, y):\n    return r_mse(m.predict(xs), y)","481ecdd0":"m_rmse(m, xs,y)\n\n# perfect (over fit) on training data","a18192bb":"valid_xs = to.valid.xs\nvalid_y = to.valid.y","7ee39498":"m_rmse(m, valid_xs, valid_y)","8719a3c8":"# it might be bad\n\nm.get_n_leaves(), len(xs)\n\n# we got so many nodes , so ofcourse its overfitting\n# lets restrict nodes to 25","2e934792":"m = DecisionTreeRegressor(min_samples_leaf=25)\nm.fit(to.train.xs, to.train.y)\nm_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)","cb2fd369":"m.get_n_leaves()","8297663e":"m_rmse(m, valid_xs, valid_y)\n\n# better but more imporvements needed","222d3e3d":"from sklearn.ensemble import RandomForestRegressor","9f53699c":"def rf(xs, y, n_estimators=540, max_samples=200_000, max_features=0.5, min_samples_leaf=5, **kwargs):\n    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)","c211c86d":"m = rf(xs, y)","53c7f04f":"m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y)","63a5d134":"preds= np.stack([t.predict(valid_xs) for t in m.estimators_])\n\n# m.estimators are the different random forest models","4dec9012":"r_mse(preds.mean(0), valid_y)","25678968":"plt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)])","c1c7b572":"r_mse(m.oob_prediction_,y)","c5aecc13":"preds = np.stack([t.predict(valid_xs) for t in m.estimators_])\npreds.shape","c6966733":"# now we have a prediction for every tree and every auction\n# getting standard deviation of all trees\npreds_std = preds.std(0)","7cae7936":"preds_std[:5]\n\n#this makes moreof a diffenrece in production systme","48033fc0":"def rf_feat_importance(m,df):\n    return pd.DataFrame({'cols': df.columns,'imp': m.feature_importances_}).sort_values('imp', ascending=False)","cf48d623":"fi = rf_feat_importance(m, xs)\nfi[:10]","f2bc9c00":"#heres aplot for relative importances\n\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)\n\nplot_fi(fi[:30])","6093bbc3":"to_keep = fi[fi.imp>0.005].cols\nlen(to_keep)","7256220e":"xs_imp = xs[to_keep]\nvalid_xs_imp = valid_xs[to_keep]","7bf97eca":"m = rf(xs_imp, y)","234cf667":"m_rmse(m, xs_imp, y), m_rmse(m, valid_xs_imp,valid_y)","66441286":"len(xs.columns), len(xs_imp.columns)","e7dffa68":"plot_fi(rf_feat_importance(m, xs_imp))","f88adf1b":"import scipy\nfrom scipy.cluster import hierarchy as hc\n\n\n# https:\/\/www.kaggle.com\/saty101\/fastai-course-v4-utils","3234b2c5":"\ndef cluster_columns(df, figsize=(10,6), font_size=12):\n    corr = np.round(scipy.stats.spearmanr(df).correlation, 4)\n    corr_condensed = hc.distance.squareform(1-corr)\n    z = hc.linkage(corr_condensed, method='average')\n    fig = plt.figure(figsize=figsize)\n    hc.dendrogram(z, labels=df.columns, orientation='left', leaf_font_size=font_size)\n    plt.show()","bc570979":"cluster_columns(xs_imp)","f3bc7e5d":"def get_oob(df):\n    m = RandomForestRegressor(n_estimators=40, min_samples_leaf=15,\n                              max_samples=50000, max_features=0.5, n_jobs=-1, oob_score=True)\n    m.fit(df,y)\n    return m.oob_score_","6126410d":"get_oob(xs_imp)","bdea4829":"# lets remove each of our ptentially redundant variables one at a time\n{c:get_oob(xs_imp.drop(c, axis=1)) for c in (\n    'saleYear', 'saleElapsed', 'ProductGroupDesc','ProductGroup',\n    'fiModelDesc', 'fiBaseModel',\n    'Hydraulics_Flow','Grouser_Tracks', 'Coupler_System')}","b555ad08":"# removing multiple variables\n\nto_drop = ['saleYear', 'ProductGroupDesc', 'fiBaseModel', 'Grouser_Tracks']\nget_oob(xs_imp.drop(to_drop, axis=1))","ca04b7da":"xs_final = xs_imp.drop(to_drop, axis=1)\nvalid_xs_final = valid_xs_imp.drop(to_drop, axis=1)","9b49d17c":"path = Path(\".\/\")","f20f3e21":"save_pickle(path\/'xs_final.pkl', xs_final)\nsave_pickle(path\/'valid_xs_final.pkl', valid_xs_final)","6d203ff1":"xs_final = load_pickle(path\/'xs_final.pkl')\nvalid_xs_final = load_pickle(path\/'valid_xs_final.pkl')","44a5ed04":"m = rf(xs_final, y)\nm_rmse(m, xs_final, y), m_rmse(m, valid_xs_final, valid_y)","36ef2250":"p = valid_xs_final['ProductSize'].value_counts(sort=False).plot.barh()\nc = to.classes['ProductSize']\nplt.yticks(range(len(c)), c)","2f990cfd":"ax = valid_xs_final['YearMade'].hist()","f09c16cf":"from sklearn.inspection import plot_partial_dependence\n\nfig, ax = plt.subplots(figsize=(12,4))\n\nplot_partial_dependence(m, valid_xs_final,['YearMade', 'ProductSize'], grid_resolution=20, ax=ax)","59b6708a":"!pip install treeinterpreter","3a2e9615":"!pip install waterfallcharts","92a3d603":"import warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nfrom treeinterpreter import treeinterpreter\nfrom waterfall_chart import plot as waterfall","55a92922":"row = valid_xs_final.iloc[:5]","01c5bcd5":"prediction, bias, contributions = treeinterpreter.predict(m, row.values)","9369bf76":"prediction[0], bias[0], contributions[0].sum()","8a2c1693":"waterfall(valid_xs_final.columns, contributions[0], threshold=0.08, \n          rotation_value=45,formatting='{:,.3f}');","db149237":"np.random.seed(42)","24b5bae3":"from fastai.tabular.all import torch","d65448dc":"# lets take a simple task of makingpredictions from 40 datapoints\n# showing a noisy linear relationship\n\nx_lin = torch.linspace(0,20,steps=40)\ny_lin = x_lin + torch.randn_like(x_lin)\nplt.scatter(x_lin, y_lin)","8facc0f7":"# we need to turn our variable to a matrix with one column\n\nxs_lin = x_lin.unsqueeze(1)\nx_lin.shape, xs_lin.shape","561c981d":"x_lin[:, None].shape\n\n# more flexible is to slice anarray with None which adds one additional unit axis","1e2e7d28":"m_lin = RandomForestRegressor().fit(xs_lin[:30], y_lin[:30])\n\n# we will use the first 30 rows to train the model","dc56f1e2":"plt.scatter(x_lin, y_lin, 20)\nplt.scatter(x_lin, m_lin.predict(xs_lin), color='red', alpha=0.5)","8a644c84":"# it is eady through using a random forest\n\ndf_dom = pd.concat([xs_final, valid_xs_final])\nis_valid = np.array([0]* len(xs_final) + [1]*len(valid_xs_final))\n\nm = rf(df_dom, is_valid)\nrf_feat_importance(m, df_dom)[:6]","bd976e6d":"m = rf(xs_final, y)\nprint('orig', m_rmse(m, valid_xs_final, valid_y))\n\nfor c in ('SalesID', 'saleElapsed', 'MachineID'):\n    m = rf(xs_final.drop(c, axis=1), y)\n    print(c, m_rmse(m, valid_xs_final.drop(c,axis=1), valid_y))","998e865e":"time_vars = ['SalesID', 'MachineID']\nxs_final_time = xs_final.drop(time_vars, axis=1)\nvalid_xs_time = valid_xs_final.drop(time_vars, axis=1)\n\n","26e17647":"m = rf(xs_final_time, y)\nm_rmse(m, valid_xs_time, valid_y)","86b9b3e4":"#another way to remove data is simply removing old data\n\nxs['saleYear'].hist()","0eef2bbe":"filt = xs['saleYear']>2004\nxs_filt = xs_final_time[filt]\ny_filt = y[filt]","e56c1ffc":"m = rf(xs_filt, y_filt)\nm_rmse(m, xs_filt, y_filt), m_rmse(m, valid_xs_time, valid_y)","d7d97879":"df_nn = pd.read_csv('..\/input\/bluebook-for-bulldozers\/TrainAndValid.csv', low_memory=False)\ndf_nn['ProductSize'] = df_nn['ProductSize'].astype('category')\ndf_nn['ProductSize'].cat.set_categories(sizes, ordered=True, inplace=True)\ndf_nn[dep_var] = np.log(df_nn[dep_var])\ndf_nn = add_datepart(df_nn, 'saledate')","4a61ff23":"df_nn_final = df_nn[list(xs_final_time.columns) + [dep_var]]","52afe200":"cont_nn,cat_nn = cont_cat_split(df_nn_final, max_card=9000, dep_var=dep_var)","3e462474":"df_nn_final['saleElapsed'].head()","bf42a32b":"cont_nn.append('saleElapsed')\ncat_nn.remove('saleElapsed')","660dae07":"df_nn['saleElapsed'] = df_nn['saleElapsed'].astype(int)","0b3f053c":"#lookingat cardinality of all categorical variables\ndf_nn_final[cat_nn].nunique()","8c07a847":"xs_filt2 = xs_filt.drop('fiModelDescriptor', axis=1)\nvalid_xs_time2 = valid_xs_time.drop('fiModelDescriptor', axis=1)","71e99083":"m2 = rf(xs_filt2, y_filt)\nm_rmse(m2, xs_filt2, y_filt), m_rmse(m2, valid_xs_time2, valid_y)\n\n# theres minimal impact so we will remove it","6d60382e":"cat_nn.remove('fiModelDescriptor')","ad8b50b2":"from fastai.tabular.all import Normalize","c9bcc225":"procs_nn = [Categorify, FillMissing, Normalize]\nto_nn = TabularPandas(df_nn_final, procs_nn, cat_nn, cont_nn,\n                      splits=splits, y_names=dep_var)","f6f49380":"# since Tabular models dont require much GPU RAM we can use larger batch sizes\ndls = to_nn.dataloaders(1024)","bbfa1e04":"y = to_nn.train.y\ny.min(), y.max()\n\n# lets look at the range of our dependent model","5b533771":"from fastai.tabular.all import tabular_learner, F","6132f142":"learn  = tabular_learner(dls, y_range=(8,12), layers=[500,250], \n                         n_out=1, loss_func=F.mse_loss)","8b21a609":"learn.lr_find()","d53898f2":"learn.fit_one_cycle(5, 1e-3)","20aa184d":"preds, targs = learn.get_preds()\nr_mse(preds, targs)","3c86e3c1":"learn.save('nn1')","19243730":"#tabular_learner??","a6ef8625":"try:\n    rf_preds = m.predict(valid_xs_time)\n    ens_preds = (to_np(preds.squeeze()) + rf_preds) \/2\n    r_mse(ens_preds,valid_y)\nexcept Exception as e:\n    print(e)","83602458":"We can create our tabular pandas the same way we created before with one significanta ddition we now need to introduce normalisation ","fdeb121b":"1. categorical values are handles thorugh embeddings in neural network, if max_card is lower then fastai will treat the variable as categorical, embedding larger than 10,000 should only be used after youve tested wether there are better was to do so. grup the variable so we will use 9000 as out `max_card`","a6cbe3f7":"We also want ot makesure it is of numeric type","d605b6fd":"this helpsin answering : For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?\n\nThis is used to see if a particular column is predicted to be expensive wy exactly is it predicted that way.","49f856a4":"\nOne minor issue we have to be aware of is that our PyTorch model and our sklearn model create data of different types: PyTorch gives us a rank-2 tensor (i.e, a column matrix), whereas NumPy gives us a rank-1 array (a vector). `squeeze` removes any unit axes from a tensor, and `to_np` converts it into a NumPy array:","253d5e27":"### Using Ensembling","799702ae":"### Removing redundant features","ec289139":"### Data Leakage\n\nHappens when cause and effect is reversed. SOmething that happened after the fact is used to use as prediction identifier. Like being happy means winning. But if the data was collected after winning most people would be happy anyway. \n","db1ceb98":"This shows that there are three columns that differ significantly berween trianingand validation sets: saleElapsed, SalesID, machineID","7117f52f":"* We have dicussed two approaches to tabular modeling: decision tree ensembles and neural networks. We've also mentioned two different decision tree ensembles: random forests, and gradient boosting machines. Each is very effective, but each also has compromises:\n\nRandom forests are the easiest to train, because they are extremely resilient to hyperparameter choices and require very little preprocessing. They are very fast to train, and should not overfit if you have enough trees. But they can be a little less accurate, especially if extrapolation is required, such as predicting future time periods.\n\nGradient boosting machines in theory are just as fast to train as random forests, but in practice you will have to try lots of different hyperparameters. They can overfit, but they are often a little more accurate than random forests.\n\nNeural networks take the longest time to train, and require extra preprocessing, such as normalization; this normalization needs to be used at inference time as well. They can provide great results and extrapolate well, but only if you are careful with your hyperparameters and take care to avoid overfitting.\n\nWe suggest starting your analysis with a random forest. This will give you a strong baseline, and you can be confident that it's a reasonable starting point. You can then use that model for feature selection and partial dependence analysis, to get a better understanding of your data.\n\nFrom that foundation, you can try neural nets and GBMs, and if they give you significantly better results on your validation set in a reasonable amount of time, you can use them. If decision tree ensembles are working well for you, try adding the embeddings for the categorical variables to the data, and see if that helps your decision trees learn better.","2d6a5d08":"Now we see a problem , if look at the Year made split its more like a christmas tree at the end the middle area is fully empty. so we are going to replace the values by 1950 for all Yearmade less than 1900","4c93b370":"Ordinal columns ,refers to columns containing strings or similar but where the strings have a natural ordering, like productSize here.","b144bba6":"### Combining embeddings with orher methods\n\nif you first train a neural network with categorical embeddings, and then use those categorical embeddings instead of the raw categorical columns in the model. In every case, the models are dramatically improved by using the embeddings instead of the raw categories.\n\nThis is a really important result, because it shows that you can get much of the performance improvement of a neural network without actually having to use a neural network at inference time. You could just use an embedding, which is literally just an array lookup, along with a small decision tree ensemble.\n\nThese embeddings need not even be necessarily learned separately for each model or task in an organization. Instead, once a set of embeddings are learned for some column for some task, they could be stored in a central place, and reused across multiple models. In fact, we know from private communication with other practitioners at large companies that this is already happening in many places.","78d4d8e5":"### handling dates in dataset","1255ac24":"Lookslikewe can remove SalesID, MachinID","f5f00318":"TabularPandas can also help in splitting data into training and validation sets for us. You cannot randomly choose because its is a time series.\n\n`np.where` returns theindices of all `True` values\n\nHere we choose validation as more than 2011","5f8216ce":"### The data\n\nIts a good practice to give `low_memory = false` asit sis enabled by default,and can cause continuity changes later.","7cdf82c0":"In fastai a tabular model is simply a model that takes in columns of continuos or categorical data and predicts a category. `tabular_learner` is an object of class `TabularModel`","d4175368":"There are two variables pertaining to \"model\" of the equipment bith with very high cardinalities suggesting that they may be redundant. Lets try removing it.","28cdc8fe":"You'll see that like `collab_learner`, it first calls `get_emb_sz` to calculate appropriate embedding sizes (you can override these by using the `emb_szs` parameter, which is a dictionary containing any column names you want to set sizes for manually), and it sets a few other defaults. Other than that, it just creates the `TabularModel`, and passes that to `TabularLearner` (note that `TabularLearner` is identical to `Learner`, except for a customized `predict` method).","194f3797":"### Removing low importance Variables \n\nif `fi.imp` < 0.005\n","6a2d1d21":"In defining randomforestresgressor `n_estimators` we define the number if trees we want,`max_samples` defines how many rows to sampe for each ree `max_features` for columns `min_samples_leaf` parameter limits the tree. n_job=-1 tells sklearn to use all ourCPUs to build the trees in parallel","9c620444":"## Lets see if using neural network helps","b34d5b4f":"This is exactly the parameters that are used for predictions","290dd241":"lets use neural net and random forest togethor. fusion time!","4a77a837":"using the same columns for our neural network that we did for random forest","6f93d18d":"### Finding Out of domain data\n\nin validation set. \n\nWe use decision tree to see if the data is in the training set or validatio set. To see this in action lets combine our training and validation sets together create a dependent varible that represent which dataset each row comes form","1cfa8b93":"HereKaggle tells you what metric to use, we use root mean squared log error (RMSLE) between actual and predicted auction prices. ","2c336d2b":"### Tree variance for confidence\n\nconfidence can be found using the considering the standard deviation ","dea6ce3c":"### Random forests\n\nBagging predictors is a method for generating multiple versions of a predictors and using these to get an aggregated predictor.\n\nthe procedure for doing it \n\n1. randomly chosse asubset of your own data\n1. train a model using this subset\n1. Save that model then return to step 1 a few times\n1. This will give you a number of trained models, take the predictions then take the average of each of these models predictions\n\nBrieman expanded this concept to randomly choosing rows for each models training but also selected randomly from a subset of columns this is called random forest.","0ee6450d":"### Rudimentary deision tree from scratch\n\nThis sequence of questions is now a procedure for taking any data item, whether an item from the training set or a new one, and assigning that item to a group. Namely, after asking and answering the questions, we can say the item belongs to the same group as all the other training data items that yielded the same set of answers to the questions. But what good is this? The goal of our model is to predict values for items, not to assign them into groups from the training dataset. The value is that we can now assign a prediction value for each of these groups\u2014for regression, we take the target mean of the items in the group.\n\nLet's consider how we find the right questions to ask. Of course, we wouldn't want to have to create all these questions ourselves\u2014that's what computers are for! The basic steps to train a decision tree can be written down very easily:\n\n1. Loop through each column of the dataset in turn.\n1. For each column, loop through each possible level of that column in turn.\n1. Try splitting the data into two groups, based on whether they are greater than or less than that value (or if it is a categorical variable, based on whether they are equal to or not equal to that level of that categorical variable).\n1. Find the average sale price for each of those two groups, and see how close that is to the actual sale price of each of the items of equipment in that group. That is, treat this as a very simple \"model\" where our predictions are simply the average sale price of the item's group.\n1. After looping through all of the columns and all the possible levels for each, pick the split point that gave the best predictions using that simple model.\n1. We now have two different groups for our data, based on this selected split. Treat each of these as separate datasets, and find the best split for each by going back to step 1 for each group.\n1. Continue this process recursively, until you have reached some stopping criterion for each group\u2014for instance, stop splitting a group further when it has only 20 items in it.\n\nAlthough this is an easy enough algorithm to implement yourself (and it is a good exercise to do so), we can save some time by using the implementation built into sklearn.\n\nFirst, however, we need to do a little data preparation.","13634746":"### Partial Dependence\n\nUNderstanding the relationship between prdictors , goood idea is to check the count the vaues er category to see howcommon eachcategory is","464bfc37":"We need to handlethe missing data and strings. Since sklearn doesnt di t out of the box we will use TabularProc , Categorify and FillMissing. \n\nTabularProc is like a transforms:\n but it modifies in place\n it runs everything together at once\n \n `Categorify` replaces the column with a numerical categorical column\n \n `FillMissing` replaced the missing values with the median of the column and create a new column with boolean indicated value was filled.","16d13bcf":"### Model Interpretation\n\nThese arete things to be considered in every prediction model\n> How Confident are we in our predictions using a particular row of data\n\n> For prediction with a particular row of data what were the most importatn factors and how they influenced that prediction\n\n> which columns were strongest predictors\n\n> which columns are redundant\n\n> How do predictions vary","848369d3":"We use `tabular_learner` for creating the model. we need a big model though","89672bee":"### Categorical Variables\n\nCategorical values are actually one of the most important feature thatn any other sometimes, and could not be avoided, so we need to one hot encode the categorical variables. ","bf1af0f2":"### Feature importance\n\nWe can use sklearns `feature_importances_`","22f9237d":"### Decision Tree ensembles \nAfter each question the data at that part of the tree is split between a yes and a no branch","87f2193f":"The most important data is one that we want to predict. The dependent variable","26750c5c":"we do not want ot treat `saleElapsed` as categorical.","f5531408":"### Tree Interpreter \n\nThese can help you to identify which factors influence specific predictions","fea5d935":"### Boosting\n\nNote from chapter\n\nSo far our approach to ensembling has been to use bagging, which involves combining many models (each trained on a different data subset) together by averaging them. As we saw, when this is applied to decision trees, this is called a random forest.\n\nThere is another important approach to ensembling, called boosting, where we add models instead of averaging them. Here is how boosting works:\n\nTrain a small model that underfits your dataset.\nCalculate the predictions in the training set for this model.\nSubtract the predictions from the targets; these are called the \"residuals\" and represent the error for each point in the training set.\nGo back to step 1, but instead of using the original targets, use the residuals as the targets for the training.\nContinue doing this until you reach some stopping criterion, such as a maximum number of trees, or you observe your validation set error getting worse.\nUsing this approach, each new tree will be attempting to fit the error of all of the previous trees combined. Because we are continually creating new residuals, by subtracting the predictions of each new tree from the residuals from the previous tree, the residuals will get smaller and smaller.\n\nTo make predictions with an ensemble of boosted trees, we calculate the predictions from each tree, and then add them all together. There are many models following this basic approach, and many names for the same models. Gradient boosting machines (GBMs) and gradient boosted decision trees (GBDTs) are the terms you're most likely to come across, or you may see the names of specific libraries implementing these; at the time of writing, XGBoost is the most popular.\n\nNote that, unlike with random forests, with this approach there is nothing to stop us from overfitting. Using more trees in a random forest does not lead to overfitting, because each tree is independent of the others. But in a boosted ensemble, the more trees you have, the better the training error becomes, and eventually you will see overfitting on the validation set.\n\nWe are not going to go into detail on how to train a gradient boosted tree ensemble here, because the field is moving rapidly, and any guidance we give will almost certainly be outdated by the time you read this. As we write this, sklearn has just added a HistGradientBoostingRegressor class that provides excellent performance. There are many hyperparameters to tweak for this class, and for all gradient boosted tree methods we have seen. Unlike random forests, gradient boosted trees are extremely sensitive to the choices of these hyperparameters; in practice, most people use a loop that tries a range of different hyperparameters to find the ones that work best.\n","726f8f92":"### Using TabularPandas and TabularProc","a07482cf":"### Extrapolation problem in random forests","a9917670":"Lets try and explain this tree.\nThe top node represents the represents the initial model before any splits were done when all data is one group.  \nIt predicts the value to be average of whole dataset we see the prediction of 10.10 for logof sales price with mean squared of 0.48\nbest split found was coupler system.\n\nMoving down to left we see the best split is YearMade. The leaf nodes have no questions to be asked.","0f902c84":"The result of training on this subset","fded4a52":"Random forest isnt ery sensitive to the hyperparameter choices such as max_features. max_samples can often be left at its default","3a1ece3a":"Why is this happening ? its because a random forest can only predict in the domain, and it is as high as it can go. SO in a datalike inflation which rises beyond the training data random forests will not work.","3a087134":"This chart shows the pair of columns that are most similar. Unsurprisingle the fields liek saleYear and saleElapsed were merged early. Similarity is determined throughrank. Lets define a function that trains the random forst on a dataset and gives the oob scorewe use  a lower max_samples and min_smaples_leaf.","2a9c852c":"## Chapter 9\n\nFastbook walkthrough.","da19eb44":"Since date can be treated as ordinal value, however its format presents a unique challenge. \n\nIn fastai we convert the date into a multiple columns like holiday, day of the week, month etc. through `datepart function`","db08531c":"### Conclusion","79c5cbd0":"TabularPandas is the equivalent of fastai Datasets object. TabularPandas need to be told which columns are continous and which aren't. ","e76f4d2e":"### Out of bag error\n\nThe performance on our validation set is worse than on our training set. OOB is a way of measuring prediction error on a different subse tof the training data."}}