{"cell_type":{"7f555511":"code","a37cb50b":"code","c479f6df":"code","2dea2f52":"code","c962f500":"code","9ecd33c4":"code","552c486a":"code","5e419dff":"code","6404e7c5":"code","2f05674f":"code","784eb67b":"code","c4a756cb":"code","e8be0be7":"code","3c5d27a3":"code","b28bb43a":"code","4d87a46e":"code","f9e26a76":"code","1f917cf6":"code","14398db5":"code","793d4ff8":"code","d55a9de7":"code","e982b583":"code","e6464b2e":"code","4b716834":"code","a0a682b8":"code","0ffbde47":"code","b07cebed":"code","5e4804df":"code","e39b6b7a":"code","9c91caeb":"code","e0eb339e":"code","40a9b59b":"code","36e0e392":"code","5fccb98d":"code","fa97f0ee":"code","227c4fe6":"code","2422d615":"code","41f477bd":"code","2c70deb5":"code","39710341":"markdown","5de77ac8":"markdown","eca0adfd":"markdown","b8f5cce3":"markdown","e75c61ab":"markdown","781d4451":"markdown","fe0a5e05":"markdown","2d145c43":"markdown","21d8bef5":"markdown","1a7fcd19":"markdown","df6afe12":"markdown","1f44f3d9":"markdown","9151678b":"markdown"},"source":{"7f555511":"#from google.colab import files\n#uploaded = files.upload() \n#Please select file location","a37cb50b":"#import statements\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, learning_curve, KFold, StratifiedShuffleSplit\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\nimport random\nfrom sklearn.svm import SVC\nimport sklearn.metrics as sk\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n# Importing the Keras libraries and packages\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import recall_score,auc, roc_auc_score, roc_curve,confusion_matrix,classification_report","c479f6df":"#change the dataset location\ndf = pd.read_csv('\/kaggle\/input\/bank-marketing\/bank-additional-full.csv', sep = ';')\ndf.shape","2dea2f52":"#viewing data\ndf.head()","c962f500":"#data info\ndf.info()\n#No null values in the data","9ecd33c4":"#Removing non-relevant variables\ndf1=df.drop(columns=['day_of_week','month','contact','poutcome'],axis=1)\ndf1","552c486a":"#Replacing all the binary variables to 0 and 1\ndf1.y.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1.default.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1.housing.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1.loan.replace(('yes', 'no'), (1, 0), inplace=True)\ndf1","5e419dff":"#creating Dummies for categorical variables\ndf2 = pd.get_dummies(df1)\ndf2.head()","6404e7c5":"#Removing extra dummy variables & checking descriptive stats\ndf3=df2.drop(columns=['job_unknown','marital_divorced','education_unknown'],axis=1)\ndf3.describe().T","2f05674f":"#Correlation plot\nplt.figure(figsize=(14,8))\ndf3.corr()['y'].sort_values(ascending = False).plot(kind='bar')","784eb67b":"#Creating binary classification target variable\ndf_target=df3[['y']].values\ndf_features=df3.drop(columns=['y'],axis=1).values\nx1_train, x1_test, y1_train, y1_test = train_test_split(df_features, df_target, test_size = 0.3, random_state = 0)","c4a756cb":"sc = StandardScaler()\nx1_train = sc.fit_transform(x1_train)\nx1_test = sc.transform(x1_test)","e8be0be7":"# Making the Confusion Matrix\ndef confusionmat(y,y_hat):\n  from sklearn.metrics import confusion_matrix,accuracy_score\n  cm = confusion_matrix(y, y_hat)\n  accu=accuracy_score(y,y_hat)\n  print(cm,\"\\n\")\n  print(\"The accuracy is\",accu)","3c5d27a3":"#Accuracy and Loss Curves\ndef learningcurve(history):\n  # list all data in history\n  print(history.history.keys())\n  # summarize history for accuracy\n  plt.plot(history.history['accuracy'])\n  plt.plot(history.history['val_accuracy'])\n  plt.title('model accuracy')\n  plt.ylabel('accuracy')\n  plt.xlabel('epoch')\n  plt.legend(['train', 'test'], loc='upper left')\n  plt.show()\n  # summarize history for loss\n  plt.plot(history.history['loss'])\n  plt.plot(history.history['val_loss'])\n  plt.title('model loss')\n  plt.ylabel('loss')\n  plt.xlabel('epoch')\n  plt.legend(['train', 'test'], loc='upper left')\n  plt.show()","b28bb43a":"  # Applying k-Fold Cross Validation\n  from sklearn.model_selection import cross_val_score\n  def kfold(x1,y1):\n    return cross_val_score(estimator = classifier, X = x1, y = y1, cv = 10)","4d87a46e":"from sklearn.model_selection import learning_curve\ndef knn_learningcurve(c, df_features, df_target):\n  train_sizes, train_scores, test_scores = learning_curve(c, df_features, df_target,cv=10,n_jobs=-1)\n  train_scores_mean = np.mean(train_scores, axis=1)\n  train_scores_std = np.std(train_scores, axis=1)\n  test_scores_mean = np.mean(test_scores, axis=1)\n  test_scores_std = np.std(test_scores, axis=1)\n\n  plt.figure()\n  plt.title(\"KNNClassifier\")\n  plt.legend(loc=\"best\")\n  plt.xlabel(\"Training examples\")\n  plt.ylabel(\"Score\")\n\n  plt.grid()\n\n  plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                      train_scores_mean + train_scores_std, alpha=0.1,\n                      color=\"r\")\n  plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                      test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n  plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n              label=\"Training score\")\n  plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n              label=\"Cross-validation score\")\n\n  plt.legend(loc=\"best\")\n  # sizes the window for readability and displays the plot\n  # shows error from 0 to 1.1\n  plt.ylim(-.1,1.1)\n  plt.show","f9e26a76":"#Roc Curve\ndef roc_auc(yTest,y_pred):\n    sns.set()\n    fpr, tpr, thresholds = roc_curve(yTest, y_pred)\n    roc_auc = auc(fpr,tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\n    plt.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--')\n    plt.xlim([-0.1,1.0])\n    plt.ylim([-0.1,1.01])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","1f917cf6":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(16,activation=\"relu\"))\n\n# Adding the second hidden layer\nclassifier.add(Dense(32,activation=\"relu\"))\n\n# Adding the output layer\nclassifier.add(Dense(1,activation=\"sigmoid\"))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nepoch=[100,250,500]\n","14398db5":"for e in epoch:\n  # Fitting the ANN to the Training set\n  history=classifier.fit(x1_train, y1_train, batch_size = 10, epochs=e,validation_split=0.3)\n  # Predicting the Test set results\n  y_pred = classifier.predict_classes(x1_test)\n  pre_score = sk.average_precision_score(y1_test, y_pred)\n  classifier.summary()\n  test_results = classifier.evaluate(x1_test, y1_test)\n  print(\"For epoch = {0}, the model test accuracy is {1}.\".format(e,test_results[1]))\n  print(\"The model test average precision score is {}.\".format(pre_score))\n  confusionmat(y1_test,y_pred)\n  learningcurve(history)\n","793d4ff8":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(16,activation=\"relu\"))\n\n# Adding the output layer\nclassifier.add(Dense(1,activation=\"sigmoid\"))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nhistory=classifier.fit(x1_train, y1_train, batch_size = 10, epochs=100,validation_split=0.3)\n# Predicting the Test set results\ny_pred = classifier.predict_classes(x1_test)\npre_score = sk.average_precision_score(y1_test, y_pred)\nclassifier.summary()\ntest_results = classifier.evaluate(x1_test, y1_test)\nprint(\"For epoch = {0}, the model test accuracy is {1}.\".format(100,test_results[1]))\nprint(\"The model test average precision score is {}.\".format(pre_score))\nconfusionmat(y1_test,y_pred)\nlearningcurve(history)\n\n","d55a9de7":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(16,activation=\"relu\"))\n\n# Adding the second hidden layer\nclassifier.add(Dense(32,activation=\"relu\"))\n\n# Adding the output layer\nclassifier.add(Dense(1,activation=\"sigmoid\"))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nhistory=classifier.fit(x1_train, y1_train, batch_size = 10, epochs=100,validation_split=0.3)\n# Predicting the Test set results\ny_pred = classifier.predict_classes(x1_test)\npre_score = sk.average_precision_score(y1_test, y_pred)\nclassifier.summary()\ntest_results = classifier.evaluate(x1_test, y1_test)\nprint(\"For epoch = {0}, the model test accuracy is {1}.\".format(100,test_results[1]))\nprint(\"The model test average precision score is {}.\".format(pre_score))\nconfusionmat(y1_test,y_pred)\nlearningcurve(history)\n\n","e982b583":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(16,activation=\"relu\"))\n\n# Adding the second hidden layer\nclassifier.add(Dense(32,activation=\"relu\"))\n\n# Adding the third hidden layer\nclassifier.add(Dense(32,activation=\"relu\"))\n\n# Adding the output layer\nclassifier.add(Dense(1,activation=\"sigmoid\"))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nhistory=classifier.fit(x1_train, y1_train, batch_size = 10, epochs=100,validation_split=0.3)\n# Predicting the Test set results\ny_pred = classifier.predict_classes(x1_test)\npre_score = sk.average_precision_score(y1_test, y_pred)\nclassifier.summary()\ntest_results = classifier.evaluate(x1_test, y1_test)\nprint(\"For epoch = {0}, the model test accuracy is {1}.\".format(100,test_results[1]))\nprint(\"The model test average precision score is {}.\".format(pre_score))\nconfusionmat(y1_test,y_pred)\nlearningcurve(history)\n\n","e6464b2e":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(16,activation=\"relu\"))\n\n# Adding the second hidden layer\nclassifier.add(Dense(16,activation=\"relu\"))\n\n# Adding the output layer\nclassifier.add(Dense(1,activation=\"sigmoid\"))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nhistory=classifier.fit(x1_train, y1_train, batch_size = 10, epochs=100,validation_split=0.3)\n# Predicting the Test set results\ny_pred = classifier.predict_classes(x1_test)\npre_score = sk.average_precision_score(y1_test, y_pred)\nclassifier.summary()\ntest_results = classifier.evaluate(x1_test, y1_test)\nprint(\"For epoch = {0}, the model test accuracy is {1}.\".format(100,test_results[1]))\nprint(\"The model test average precision score is {}.\".format(pre_score))\nconfusionmat(y1_test,y_pred)\nlearningcurve(history)\n\n","4b716834":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(32,activation=\"relu\"))\n\n# Adding the second hidden layer\nclassifier.add(Dense(16,activation=\"relu\"))\n\n# Adding the output layer\nclassifier.add(Dense(1,activation=\"sigmoid\"))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nhistory=classifier.fit(x1_train, y1_train, batch_size = 10, epochs=100,validation_split=0.3)\n# Predicting the Test set results\ny_pred = classifier.predict_classes(x1_test)\npre_score = sk.average_precision_score(y1_test, y_pred)\nclassifier.summary()\ntest_results = classifier.evaluate(x1_test, y1_test)\nprint(\"For epoch = {0}, the model test accuracy is {1}.\".format(100,test_results[1]))\nprint(\"The model test average precision score is {}.\".format(pre_score))\nconfusionmat(y1_test,y_pred)\nlearningcurve(history)\n\n","a0a682b8":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(32,activation=\"relu\"))\n\n# Adding the second hidden layer\nclassifier.add(Dense(32,activation=\"relu\"))\n\n# Adding the output layer\nclassifier.add(Dense(1,activation=\"sigmoid\"))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nhistory=classifier.fit(x1_train, y1_train, batch_size = 10, epochs=100,validation_split=0.3)\n# Predicting the Test set results\ny_pred = classifier.predict_classes(x1_test)\npre_score = sk.average_precision_score(y1_test, y_pred)\nclassifier.summary()\ntest_results = classifier.evaluate(x1_test, y1_test)\nprint(\"For epoch = {0}, the model test accuracy is {1}.\".format(100,test_results[1]))\nprint(\"The model test average precision score is {}.\".format(pre_score))\nconfusionmat(y1_test,y_pred)\nlearningcurve(history)\n\n","0ffbde47":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(32,activation=\"tanh\"))\n\n# Adding the second hidden layer\nclassifier.add(Dense(16,activation=\"tanh\"))\n\n# Adding the output layer\nclassifier.add(Dense(1,activation=\"sigmoid\"))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nhistory=classifier.fit(x1_train, y1_train, batch_size = 10, epochs=100,validation_split=0.3)\n# Predicting the Test set results\ny_pred = classifier.predict_classes(x1_test)\npre_score = sk.average_precision_score(y1_test, y_pred)\nclassifier.summary()\ntest_results = classifier.evaluate(x1_test, y1_test)\nprint(\"For epoch = {0}, the model test accuracy is {1}.\".format(100,test_results[1]))\nprint(\"The model test average precision score is {}.\".format(pre_score))\nconfusionmat(y1_test,y_pred)\nlearningcurve(history)\n\n","b07cebed":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(32,activation=\"sigmoid\"))\n\n# Adding the second hidden layer\nclassifier.add(Dense(16,activation=\"sigmoid\"))\n\n# Adding the output layer\nclassifier.add(Dense(1,activation=\"sigmoid\"))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nhistory=classifier.fit(x1_train, y1_train, batch_size = 10, epochs=100,validation_split=0.3)\n# Predicting the Test set results\ny_pred = classifier.predict_classes(x1_test)\npre_score = sk.average_precision_score(y1_test, y_pred)\nclassifier.summary()\ntest_results = classifier.evaluate(x1_test, y1_test)\nprint(\"For epoch = {0}, the model test accuracy is {1}.\".format(100,test_results[1]))\nprint(\"The model test average precision score is {}.\".format(pre_score))\nconfusionmat(y1_test,y_pred)\nlearningcurve(history)\n\n","5e4804df":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(32,activation=\"softmax\"))\n\n# Adding the second hidden layer\nclassifier.add(Dense(16,activation=\"softmax\"))\n\n# Adding the output layer\nclassifier.add(Dense(1,activation=\"sigmoid\"))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nhistory=classifier.fit(x1_train, y1_train, batch_size = 10, epochs=100,validation_split=0.3)\n# Predicting the Test set results\ny_pred = classifier.predict_classes(x1_test)\npre_score = sk.average_precision_score(y1_test, y_pred)\nclassifier.summary()\ntest_results = classifier.evaluate(x1_test, y1_test)\nprint(\"For epoch = {0}, the model test accuracy is {1}.\".format(100,test_results[1]))\nprint(\"The model test average precision score is {}.\".format(pre_score))\nconfusionmat(y1_test,y_pred)\nlearningcurve(history)\n\n","e39b6b7a":"# Initialising the ANN\nclassifier = Sequential()\n\n# Adding the input layer and the first hidden layer\nclassifier.add(Dense(32,activation=\"softmax\"))\n\n# Adding the second hidden layer\nclassifier.add(Dense(16,activation=\"softmax\"))\n\n# Adding the output layer\nclassifier.add(Dense(1,activation=\"sigmoid\"))\n\n# Compiling the ANN\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nhistory=classifier.fit(x1_train, y1_train, batch_size = 10, epochs=100,validation_split=0.3)\n# Predicting the Test set results\ny_pred = classifier.predict_classes(x1_test)\npre_score = sk.average_precision_score(y1_test, y_pred)\nclassifier.summary()\ntest_results = classifier.evaluate(x1_test, y1_test)\nprint(\"For epoch = {0}, the model test accuracy is {1}.\".format(e,test_results[1]))\nprint(\"The model test average precision score is {}.\".format(pre_score))\nconfusionmat(y1_test,y_pred)\nlearningcurve(history)","9c91caeb":"from sklearn.neighbors import KNeighborsClassifier\nk_list=[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]\naccu_KNN=[]\nfor i in k_list:\n  # Fitting K-NN to the Training set\n  classifier = KNeighborsClassifier(n_neighbors = i)\n  history=classifier.fit(x1_train, y1_train)\n\n  # Predicting the Test set results\n  y_pred = classifier.predict(x1_test)\n\n  # 10 fold cross validation\n  accuracies = kfold(x1_train, y1_train)\n\n  accu_KNN+=[accuracies.mean()]\n  std=accuracies.std()\n  report=sk.classification_report(y1_test,y_pred)\n  confusionmat(y1_test, y_pred)\n\n  print(\"KNN with n= \",i)\n  print(\"The Classification report\\n\",report,end='\\n')\n  print(\"The Accuracy score with only 1 Training and Testing Data Set\\n\",sk.accuracy_score(y1_test,y_pred)*100,end='\\n')\n  #after using cross validation with 10 folds\n  print(\"The mean of the accuracy scores with using 10 fold-cross validation\\n\",accuracies.mean()*100,end='\\n')\n  print(\"The Standard Deviation of the accuracy scores with using 10 fold-cross validation\\n\",std*100,end='\\n')\n\n  knn_learningcurve(classifier, df_features, df_target)","e0eb339e":"plt.plot(k_list,accu_KNN)\nplt.xlabel(\"k number\")\nplt.ylabel(\"Accuracy\")\nplt.show()","40a9b59b":"# Fitting K-NN to the Training set\nclassifier = KNeighborsClassifier(n_neighbors = 9,metric=\"manhattan\")\nhistory=classifier.fit(x1_train, y1_train)\n# Predicting the Test set results\ny_pred = classifier.predict(x1_test)\n\n# 10 fold cross validation\naccuracies = kfold(x1_train, y1_train)\naccu_KNN=accuracies.mean()\nstd=accuracies.std()\nreport=sk.classification_report(y1_test,y_pred)\nconfusionmat(y1_test, y_pred)\n\nprint(\"KNN with n= \",9)\nprint(\"The Classification report\\n\",report,end='\\n')\nprint(\"The Accuracy score with only 1 Training and Testing Data Set\\n\",sk.accuracy_score(y1_test,y_pred)*100,end='\\n')\n\nknn_learningcurve(classifier, df_features, df_target)","36e0e392":"# Fitting K-NN to the Training set\nclassifier = KNeighborsClassifier(n_neighbors = 9,metric=\"chebyshev\")\nhistory=classifier.fit(x1_train, y1_train)\n# Predicting the Test set results\ny_pred = classifier.predict(x1_test)\n\n# 10 fold cross validation\naccuracies = kfold(x1_train, y1_train)\naccu_KNN=accuracies.mean()\nstd=accuracies.std()\nreport=sk.classification_report(y1_test,y_pred)\nconfusionmat(y1_test, y_pred)\n\nprint(\"KNN with n= \",9)\nprint(\"The Classification report\\n\",report,end='\\n')\nprint(\"The Accuracy score with only 1 Training and Testing Data Set\\n\",sk.accuracy_score(y1_test,y_pred)*100,end='\\n')\n#after using cross validation with 10 folds\nprint(\"The mean of the accuracy scores with using 10 fold-cross validation\\n\",accuracies.mean()*100,end='\\n')\nprint(\"The Standard Deviation of the accuracy scores with using 10 fold-cross validation\\n\",std*100,end='\\n')\n\nknn_learningcurve(classifier, df_features, df_target)","5fccb98d":"# Fitting K-NN to the Training set\nclassifier = KNeighborsClassifier(n_neighbors = 9,metric=\"euclidean\")\nhistory=classifier.fit(x1_train, y1_train)\n# Predicting the Test set results\ny_pred = classifier.predict(x1_test)\n\n# 10 fold cross validation\naccuracies = kfold(x1_train, y1_train)\naccu_KNN=accuracies.mean()\nstd=accuracies.std()\nreport=sk.classification_report(y1_test,y_pred)\nconfusionmat(y1_test, y_pred)\n\nprint(\"KNN with n= \",9)\nprint(\"The Classification report\\n\",report,end='\\n')\nprint(\"The Accuracy score with only 1 Training and Testing Data Set\\n\",sk.accuracy_score(y1_test,y_pred)*100,end='\\n')\n#after using cross validation with 10 folds\nprint(\"The mean of the accuracy scores with using 10 fold-cross validation\\n\",accuracies.mean()*100,end='\\n')\nprint(\"The Standard Deviation of the accuracy scores with using 10 fold-cross validation\\n\",std*100,end='\\n')\n\nknn_learningcurve(classifier, df_features, df_target)","fa97f0ee":"# Fitting K-NN to the Training set\nclassifier = KNeighborsClassifier(n_neighbors = 9,metric=\"manhattan\")\nhistory=classifier.fit(x1_train, y1_train)\n# Predicting the Test set results\ny_pred = classifier.predict(x1_test)\n\nreport=sk.classification_report(y1_test,y_pred)\nconfusionmat(y1_test, y_pred)\n\nprint(\"KNN with n= \",9)\nprint(\"The Classification report\\n\",report,end='\\n')\nprint(\"The Accuracy score with only 1 Training and Testing Data Set\\n\",sk.accuracy_score(y1_test,y_pred)*100,end='\\n')\nknn_learningcurve(classifier, df_features, df_target)","227c4fe6":"roc_auc(y1_test,y_pred)","2422d615":"distance_list=['euclidean','manhattan','chebyshev']\nAccuracy_test=[88.5137,88.5800,88.4031]\n \nd1=pd.DataFrame(list(zip(distance_list,Accuracy_test)),columns=['distance_list','Accuracy'])\nprint(d1)\n\n# this is for plotting purpose\nindex = np.arange(len(distance_list))\nplt.bar(distance_list, Accuracy_test,  width=0.3)\nplt.xlabel('Distance metric')\nplt.ylabel('Accuracy')\nplt.title('Comparison')\nplt.figure(figsize=(2,2))\nplt.show()","41f477bd":"# knn and nn\ndistance_list=['Neual Network','KNN']\nAccuracy_test=[88.9265,88.5800]\n \nd1=pd.DataFrame(list(zip(distance_list,Accuracy_test)),columns=['Algorithm','Accuracy'])\nprint(d1)\n\n# this is for plotting purpose\nindex = np.arange(len(distance_list))\nplt.bar(distance_list, Accuracy_test,  width=0.3)\nplt.xlabel('Algorithms')\nplt.ylabel('Accuracy')\nplt.title('Comparison')\nplt.figure(figsize=(1,1))\nplt.show()","2c70deb5":"# comparison between svm  knn nn decision trees ensemble\n\nAlgorithms=['SVM-rbf','Decision Trees','Boosting','Neural Networks','KNN-Manhattan']\nAccuracy_test=[88.78,85.29,89.52,88.93,88.58]\n \nd1=pd.DataFrame(list(zip(Algorithms,Accuracy_test)),columns=['Algorithms','Accuracy'])\nprint(d1)\n\n# this is for plotting purpose\nplt.bar(Algorithms, Accuracy_test, width=0.3)\nplt.xlabel('Algorithms')\nplt.tick_params(axis='x', rotation=60)\nplt.ylabel('Accuracy')\nplt.title('Comparison')\nplt.figure(figsize=(2,2))\nplt.show()","39710341":"#Artificial Neural Network","5de77ac8":"##Experimenting with Activation Function","eca0adfd":"#Final K-NN","b8f5cce3":"##Experimenting with Number of Layers","e75c61ab":"##Experimenting with Number of Nodes","781d4451":"#Comparisons","fe0a5e05":"#K-Nearest Neighbor","2d145c43":"#Define Functions","21d8bef5":"##Experiment with Number of Neighbors","1a7fcd19":"##Experiment with Distance Metric","df6afe12":"#**Data** **Preprocessing**","1f44f3d9":"##Experimenting with Number of Epoch","9151678b":"#Final ANN"}}