{"cell_type":{"8ef8645b":"code","8fcd1c16":"code","1a5fc149":"code","07d95a44":"code","38961041":"code","c7d22f19":"code","c3faa83a":"code","3113c291":"code","6acb3cc7":"code","fdf05a16":"code","a8c357c0":"code","fd902f96":"code","780ae9e8":"code","6308c8c4":"code","5a021ae4":"code","92e4e362":"code","45ee379f":"code","dbe33a4b":"code","6701341a":"code","b3cdb1dc":"code","6761f059":"code","8b442cd1":"code","8d6373bc":"code","63e60403":"code","ed6dee33":"code","572024f7":"code","7f0c6bfb":"code","24188a38":"code","1c9d216b":"code","1c201da2":"code","59c0fb3a":"markdown","9e359a93":"markdown","5b14e98e":"markdown","ca68cf43":"markdown","41723bc7":"markdown","1648387e":"markdown","893ab13a":"markdown","c867d09b":"markdown","cc8eeb0d":"markdown","e5b34caf":"markdown","7fd00f4d":"markdown","518176f0":"markdown","f80926c1":"markdown","952d05e8":"markdown","85cf8311":"markdown","668096a3":"markdown","56bef5e8":"markdown","24f4d0d6":"markdown"},"source":{"8ef8645b":"# General imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nfrom joblib import Parallel, delayed\n\n# Tensorflow\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n# Transformers\nfrom transformers import TFAutoModel, TFBertModel, AutoTokenizer","8fcd1c16":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","1a5fc149":"# Configuration parameters\nAUTO = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 192\nMODEL = 'dccuchile\/bert-base-spanish-wwm-uncased' # for BERT model replace by e.g. dccuchile\/bert-base-spanish-wwm-uncased   # jplu\/tf-xlm-roberta-large\nLANG = \"es\" # can be any of es, it, tr in this notebook\nCONSTANT_LR = 3e-6 # 3e-6 generally good. Set lower e.g. 1e-6 for more finetuning\nBALANCEMENT = [0.8, 0.2] # non-toxic vs. toxic\nBERT_MODEL = True # specify if the given model is a BERT model\nN_EPOCHS = 3 # 3-5 epochs are usually enough. Set higher e.g. 5 for more finetuning\nN_ITER_PER_EPOCH = 10\nPREDICT_START_ITER = 10 # start iteration to predict on test. best iterations found around +-20 (2 full epochs)\n\n# Upgrades\nSTAGE2 = True # resume training with checkpoint of best model\nREPEAT_PL = 6 # Upgrade: repeat PL with train (I repeated 6x on my last subs). Default=0 (no pseudolabels)","07d95a44":"def regular_encode(texts, max_len):\n    \"\"\"\n    Tokenizing the texts into their respective IDs using regular batch encoding\n    \n    Accepts: * texts: the text to be tokenize\n             * max_len: max length of text\n    \n    Returns: * array of tokenized IDs \n    \"\"\"\n    enc_di = tokenizer.batch_encode_plus(\n        texts, \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=max_len\n    )\n    \n    return np.array(enc_di['input_ids'])","38961041":"def parallel_encode(texts, max_len):\n    \"\"\"\n    Tokenizing the texts into their respective IDs using parallel processing\n    \n    Accepts: * texts: the text to be tokenized\n             * max_len: max length of text\n    \n    Returns: * array of tokenized IDs + the toxicity label  \n    \"\"\"\n    enc_di = tokenizer.encode_plus(\n        str(texts[0]),\n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=max_len\n    )\n    return np.array(enc_di['input_ids']), texts[1]","c7d22f19":"def build_model(transformer, max_len):\n    \"\"\"\n    Build the model by using transformer layer and simple CLS token\n    \n    Accepts: * transformer: transformer layer\n             * max_len: max length of text\n    \n    Returns: * model \n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(cls_token)\n    model = Model(inputs=input_word_ids, outputs=out)    \n    return model","c3faa83a":"# First load the real tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","3113c291":"train = pd.read_csv(f\"\/kaggle\/input\/jigsaw-train-multilingual-coments-google-api\/jigsaw-toxic-comment-train-google-{LANG}-cleaned.csv\")\nvalid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\ntest = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv\")","6acb3cc7":"print(len(train), len(valid), len(test))","fdf05a16":"# if REPEAT_PL:\n#     sub = pd.read_csv(\"..\/input\/multilingual-toxic-comments-training-data\/test9500.csv\") # use one of earlier subs\n#     print(sub.head())\n#     print(len(sub))\n#     sub[\"comment_text\"] = test[\"content\"]\n#     sub = sub.loc[test[\"lang\"]==LANG].reset_index(drop=True)\n#     sub_repeat = pd.concat([sub]*REPEAT_PL, ignore_index=True) # repeat PL multipe times for training\n#     print('\\n', sub_repeat.head())\n#     print(len(sub_repeat))\n#     same_cols = [\"comment_text\", \"toxic\"]\n#     train = pd.concat([train[same_cols], sub_repeat[same_cols]]).sample(frac=1).reset_index(drop=True)","a8c357c0":"# Get specific validation and test\nvalid = valid.loc[valid[\"lang\"]==LANG].reset_index(drop=True)\ntest = test.loc[test[\"lang\"]==LANG].reset_index(drop=True)","fd902f96":"%%time \n# Tokenize train with parallel processing\nrows = zip(train['comment_text'].values.tolist(), train.toxic.values.tolist())\nx_y_train = Parallel(n_jobs=4, backend='multiprocessing')(delayed(parallel_encode)(row, max_len=MAX_LEN) for row in tqdm(rows))","780ae9e8":"print(len(x_y_train))\ntry:\n    print(x_y_train.shape)\nexcept:\n    pass\nprint(x_y_train[0])","6308c8c4":"x_train = np.vstack(np.array(x_y_train)[:,0])\n\ny_train = np.array(x_y_train)[:,1]\ny_train = np.asarray(y_train).astype('float32').reshape((-1,1))\nprint(y_train.shape)","5a021ae4":"%%time\n# Tokenize valid regular processing\nx_valid = regular_encode(valid.comment_text.values, max_len=MAX_LEN)\n\ny_valid = valid.toxic.values\ny_valid = np.asarray(y_valid).astype('float32').reshape((-1,1)) ","92e4e362":"%%time\nx_test = regular_encode(test.content.values, max_len=MAX_LEN)","45ee379f":"# Train and valid dataset\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .shuffle(buffer_size=len(x_train), seed = 18)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)","dbe33a4b":"# Balance the train dataset by creating seperate negative and positive datasets. \n# Note: tf.squeeze remove the added dim to labels\n# Example taken from https:\/\/www.tensorflow.org\/guide\/data\n\nnegative_ds = (\n  train_dataset\n    .filter(lambda _, y: tf.squeeze(y)==0)\n    .repeat())\n\npositive_ds = (\n  train_dataset\n    .filter(lambda _, y: tf.squeeze(y)==1)\n    .repeat())\n\nbalanced_ds = tf.data.experimental.sample_from_datasets(\n    [negative_ds, positive_ds], BALANCEMENT).batch(BATCH_SIZE) # Around 80%\/20% to be expected for 0\/1 labels","6701341a":"# distribute the datset according to the strategy\ntrain_dist_ds = strategy.experimental_distribute_dataset(balanced_ds)\nvalid_dist_ds = strategy.experimental_distribute_dataset(valid_dataset)","b3cdb1dc":"# Instantiate metrics\nwith strategy.scope():\n    # Accuracy, AUC, loss train\n    train_accuracy = tf.keras.metrics.BinaryAccuracy()\n    train_auc = tf.keras.metrics.AUC()\n    train_loss = tf.keras.metrics.Sum()\n    \n    # Accuracy, AUC, loss valid\n    valid_accuracy = tf.keras.metrics.BinaryAccuracy()\n    valid_auc = tf.keras.metrics.AUC()\n    valid_loss = tf.keras.metrics.Sum()\n    \n    # TP, TN, FN, FP train\n    train_TP = tf.keras.metrics.TruePositives()\n    train_TN = tf.keras.metrics.TrueNegatives()\n    train_FP = tf.keras.metrics.FalsePositives()\n    train_FN = tf.keras.metrics.FalseNegatives()\n    \n    # TP, TN, FN, FP valid\n    valid_TP = tf.keras.metrics.TruePositives()\n    valid_TN = tf.keras.metrics.TrueNegatives()\n    valid_FP = tf.keras.metrics.FalsePositives()\n    valid_FN = tf.keras.metrics.FalseNegatives()\n    \n    # Loss function and optimizer\n    loss_fn = lambda a,b: tf.nn.compute_average_loss(tf.keras.losses.binary_crossentropy(a,b), global_batch_size=BATCH_SIZE)\n    optimizer = tf.keras.optimizers.Adam(learning_rate=CONSTANT_LR)","6761f059":"@tf.function\ndef train_step(tokens, labels):\n    with tf.GradientTape() as tape:\n        probabilities = model(tokens, training=True)\n        loss = loss_fn(labels, probabilities)\n    grads = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n    \n    # update metrics\n    train_accuracy.update_state(labels, probabilities)\n    train_auc.update_state(labels, probabilities)\n    train_loss.update_state(loss)\n    \n    train_TP.update_state(labels, probabilities)\n    train_TN.update_state(labels, probabilities)\n    train_FP.update_state(labels, probabilities)\n    train_FN.update_state(labels, probabilities)\n    \n@tf.function\ndef valid_step(tokens, labels):\n    probabilities = model(tokens, training=False)\n    loss = loss_fn(labels, probabilities)\n    \n    # update metrics\n    valid_accuracy.update_state(labels, probabilities)\n    valid_auc.update_state(labels, probabilities)\n    valid_loss.update_state(loss)\n    \n    valid_TP.update_state(labels, probabilities)\n    valid_TN.update_state(labels, probabilities)\n    valid_FP.update_state(labels, probabilities)\n    valid_FN.update_state(labels, probabilities)\n    ","8b442cd1":"%%time\nwith strategy.scope():\n    if BERT_MODEL:\n        transformer_layer = TFBertModel.from_pretrained(MODEL, from_pt=True)\n    else:\n        transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","8d6373bc":"VALIDATION_STEPS = x_valid.shape[0] \/\/ BATCH_SIZE\nSTEPS_PER_EPOCH = x_train.shape[0] \/\/ (BATCH_SIZE*N_ITER_PER_EPOCH)\nprint(\"Steps per epoch:\", STEPS_PER_EPOCH)\nEPOCHS = N_EPOCHS*N_ITER_PER_EPOCH\n\nbest_auc = 0\nepoch = 0\n\npreds_all = []\nfor step, (tokens, labels) in enumerate(train_dist_ds):\n    # run training step\n    strategy.experimental_run_v2(train_step, args=(tokens, labels))\n    print('=', end='', flush=True)\n    \n    # print metrics training\n    if ((step+1) \/\/ STEPS_PER_EPOCH) > epoch:\n        print(\"\\n Epoch:\", epoch)\n        print('|', end='', flush=True)\n        print(\"TP -  TN  -  FP  -  FN\")\n        print(train_TP.result().numpy(), train_TN.result().numpy(), train_FP.result().numpy(), train_FN.result().numpy())\n        print(\"train AUC: \",train_auc.result().numpy())\n        print(\"train loss: \", train_loss.result().numpy() \/ STEPS_PER_EPOCH)\n        \n        # validation run for es, it, tr and save model\n        for tokens, labels in valid_dist_ds:\n            strategy.experimental_run_v2(valid_step, args=(tokens, labels))\n            print('=', end='', flush=True)\n\n        # compute metrics\n        print(\"\\n\")\n        print(\"TP -  TN  -  FP  -  FN\")\n        print(valid_TP.result().numpy(), valid_TN.result().numpy(), valid_FP.result().numpy(), valid_FN.result().numpy())\n        print(\"val AUC: \", valid_auc.result().numpy())\n        print(\"val loss: \", valid_loss.result().numpy() \/ VALIDATION_STEPS)\n\n        # Save predictions and weights of model\n        if (valid_auc.result().numpy() > best_auc) & (epoch>=PREDICT_START_ITER):\n            best_auc = valid_auc.result().numpy()\n            print(\"Prediction on test set - snapshot\")\n            preds = model.predict(test_dataset, verbose = 1)\n            preds_all.append(preds)\n            model.save_weights('best_model.h5') # keep track of best model\n        # set up next epoch\n        epoch = (step+1) \/\/ STEPS_PER_EPOCH\n\n        train_auc.reset_states()\n        valid_auc.reset_states()\n\n        valid_loss.reset_states()\n        train_loss.reset_states()\n        \n        train_TP.reset_states()\n        train_TN.reset_states()\n        train_FP.reset_states()\n        train_FN.reset_states()\n        \n        valid_TP.reset_states()\n        valid_TN.reset_states()\n        valid_FP.reset_states()\n        valid_FN.reset_states()\n        \n        if epoch >= EPOCHS:\n            break","63e60403":"#Generate averages of predictions: last one, and average of snapshots\ntest[\"toxic_best\"] = preds_all[-1]\ntest[\"toxic_avg\"] = sum(preds_all)\/len(preds_all)","ed6dee33":"# Save the predictions\nMODEL_NAME = MODEL.replace(\"\/\", \"-\")\ntest.to_csv(f\"test-{LANG}-{MODEL_NAME}-2-1.csv\", index=False)","572024f7":"valid = pd.read_csv('\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/validation.csv')\nvalid = valid.loc[valid[\"lang\"]==LANG].reset_index(drop=True)\ntest = pd.read_csv(\"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/test.csv\")\n\nif REPEAT_PL:\n    sub = pd.read_csv(\"..\/input\/multilingual-toxic-comments-training-data\/test9500.csv\") # use one of earlier subs\n    print(sub.head())\n    print(len(sub))\n    sub[\"comment_text\"] = test[\"content\"]\n    sub = sub.loc[test[\"lang\"]==LANG].reset_index(drop=True)\n    sub_repeat = pd.concat([sub]*REPEAT_PL, ignore_index=True) # repeat PL multipe times for training\n    print('\\n', sub_repeat.head())\n    print(len(sub_repeat))\n    same_cols = [\"comment_text\", \"toxic\"]\n    valid = pd.concat([valid[same_cols], sub_repeat[same_cols]]).sample(frac=1).reset_index(drop=True)\n\ntest = test.loc[test[\"lang\"]==LANG].reset_index(drop=True)  \n\nx_valid = regular_encode(valid.comment_text.values, max_len=MAX_LEN)\ny_valid = valid.toxic.values\ny_valid = np.asarray(y_valid).astype('float32').reshape((-1,1)) \nx_test = regular_encode(test.content.values, max_len=MAX_LEN)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)","7f0c6bfb":"if STAGE2:\n    # the validation set becomes train_dataset\n    train_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices((x_valid, y_valid)) # replaced by x_valid, y_valid!\n        .shuffle(buffer_size=len(x_valid), seed = 18)\n        .prefetch(AUTO)\n        .batch(BATCH_SIZE)\n        .repeat()\n    )\n    \n    # distribute the datset according to the strategy\n    train_dist_ds = strategy.experimental_distribute_dataset(train_dataset)","24188a38":"if STAGE2:\n    model.load_weights(\"best_model.h5\") # best model from stage1","1c9d216b":"if STAGE2:\n    STEPS_PER_EPOCH = round(x_valid.shape[0] \/ (BATCH_SIZE*N_ITER_PER_EPOCH))\n    print(\"Steps per epoch:\", STEPS_PER_EPOCH)\n    EPOCHS = N_EPOCHS*N_ITER_PER_EPOCH\n    best_auc = 0\n    epoch = 0\n\n    preds_all = []\n    for step, (tokens, labels) in enumerate(train_dist_ds):\n        # run training step\n        strategy.experimental_run_v2(train_step, args=(tokens, labels))\n        print('=', end='', flush=True)\n\n        # print metrics training\n        if ((step+1) \/\/ STEPS_PER_EPOCH) > epoch:\n            print(\"\\n Epoch:\", epoch)\n            print('|', end='', flush=True)\n            print(\"TP -  TN  -  FP  -  FN\")\n            print(train_TP.result().numpy(), train_TN.result().numpy(), train_FP.result().numpy(), train_FN.result().numpy())\n            print(\"train AUC: \",train_auc.result().numpy())\n            print(\"train loss: \", train_loss.result().numpy() \/ STEPS_PER_EPOCH)\n\n            # Save predictions and weights of model\n            if epoch>=PREDICT_START_ITER:\n                print(\"Prediction on test set - snapshot\")\n                preds = model.predict(test_dataset, verbose = 1)\n                preds_all.append(preds)\n                \n            # set up next epoch\n            epoch = (step+1) \/\/ STEPS_PER_EPOCH\n            \n            train_auc.reset_states()\n            train_loss.reset_states()\n\n            train_TP.reset_states()\n            train_TN.reset_states()\n            train_FP.reset_states()\n            train_FN.reset_states()\n            \n            if epoch >= EPOCHS:\n                # save model if needed\n                model.save_weights('best_model_valid.h5') \n                break","1c201da2":"if STAGE2:\n    #Generate averages of snapshot\n    test[\"toxic_mean_snap_valid\"] = sum(preds_all)\/len(preds_all)\n    # Save the predictions\n    MODEL_NAME = MODEL.replace(\"\/\", \"-\")\n    test_pre = pd.read_csv(f\"test-{LANG}-{MODEL_NAME}-2-1.csv\")\n    assert sum(test_pre[\"content\"]==test[\"content\"]) == len(test)\n    test[\"toxic_best\"] = test_pre[\"toxic_best\"]\n    test[\"toxic_avg\"] = test_pre[\"toxic_avg\"]\n    \n    test.to_csv(f\"test-{LANG}-{MODEL_NAME}-2.csv\", index=False)","59c0fb3a":"## Build datasets objects","9e359a93":"## Helper Functions processing","5b14e98e":"# \ud83d\udcbe Save predictions","ca68cf43":"## Custom training loop","41723bc7":"## TPU and configs","1648387e":"# \ud83e\udd84 Overview\nThis is a improved version of [rafiko1's 1st place baseline](https:\/\/www.kaggle.com\/rafiko1\/1st-place-baseline-xlm-r-es-it-tr), current version=2.\n\nVersion=1 partially modified below compared to [rafiko1's 1st place baseline](https:\/\/www.kaggle.com\/rafiko1\/1st-place-baseline-xlm-r-es-it-tr):\n- MODEL = 'dccuchile\/bert-base-spanish-wwm-uncased'\n- BERT_MODEL = True\n- REPEAT_PL = 6\n\nVersion=1 use monolingual bert models with spanish, you can get below score when submitting version=1 output file \"test-es-dccuchile-bert-base-spanish-wwm-uncased.csv\":\n- public score: 0.9483, private 0.9466 for stage1 best prediction \n- public score: 0.9487, private 0.9469 for stage1 avg prediction \n- public score: 0.9485, private 0.9465 for stage2 avg prediction \n- BTW, the initial submission score is: public 0.95, private 0.9485\n\nSo, the pseudo labels training don't improve the initial submission due to the forgetting nature of Stage2, **I fix this problem in version=2**.\n\nVersion=2 (current version) transfer pseudo labels test data from stage1 to stage2 **compared to version=1**, this can avoid the forgetting nature of Stage2, you can get below score when submitting version=2 (current version) output file \"test-es-dccuchile-bert-base-spanish-wwm-uncased-2.csv\":\n- public score: 0.9491, private 0.9475 for stage1 best prediction\n- public score: 0.9494, private 0.9479 for stage1 avg prediction\n- public score: 0.9505, private 0.9492 for stage2 avg prediction\n- BTW, the initial submission score is: public 0.95, private 0.9485\n\nWe can find this change can make pseudo labels strategy works.","893ab13a":"# \ud83d\udcc9 Custom training loop","c867d09b":"# \ud83c\udfa8 Monolingual Bert Models\nI sorted out the following Monolingual Bert Models from the transformers library\n- [french](https:\/\/huggingface.co\/camembert\/camembert-large)\n- [russian](https:\/\/huggingface.co\/blinoff\/roberta-base-russian-v0)\n- [spanish](https:\/\/huggingface.co\/dccuchile\/bert-base-spanish-wwm-uncased)\n- [turkish](https:\/\/huggingface.co\/dbmdz\/bert-base-turkish-cased)\n- [italian](https:\/\/huggingface.co\/dbmdz\/bert-base-italian-xxl-uncased)\n- [portuguese](https:\/\/huggingface.co\/neuralmind\/bert-large-portuguese-cased)\n\nIf it helps you, please upvote me!","cc8eeb0d":"## Build datasets objects","e5b34caf":"# \u2795 Load model","7fd00f4d":"# \ud83d\udcf7 Load model","518176f0":"# \u2328\ufe0f Stage 2: resume training on validation data","f80926c1":"## Helper Functions TF custom training","952d05e8":"## Tokenize","85cf8311":"## Create fast tokenizer","668096a3":"# \ud83d\udcbe Save predictions","56bef5e8":"# \ud83d\udcda Imports","24f4d0d6":"## Load text data into memory"}}