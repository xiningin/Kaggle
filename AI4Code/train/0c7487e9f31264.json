{"cell_type":{"361d00f3":"code","6bfeb956":"code","830d0ad0":"code","8c9beae0":"code","a4d78efd":"code","e2055ceb":"code","805b24f8":"code","422bb040":"code","289537f0":"code","17deb03e":"code","65131a08":"code","2607a74a":"code","1b4f5efa":"code","d5f2e932":"code","5eadf674":"code","b18fceb8":"code","ed8c2747":"code","76d1a0e2":"code","064c6240":"code","25534e32":"markdown","937f2e4d":"markdown","80de7a0f":"markdown","49ffa4f2":"markdown","f043d3cd":"markdown","36790212":"markdown","c10d610b":"markdown","cbe63e8c":"markdown","bddea686":"markdown","9c55ff5a":"markdown","3ffb5b07":"markdown","4db98151":"markdown","70f718af":"markdown"},"source":{"361d00f3":"import numpy as np\nimport matplotlib.pyplot as plt\nimport h5py\n\n\ndef sigmoid(Z):\n    \"\"\"\n    Implements the sigmoid activation in numpy\n    \n    Arguments:\n    Z -- numpy array of any shape\n    \n    Returns:\n    A -- output of sigmoid(z), same shape as Z\n    cache -- returns Z as well, useful during backpropagation\n    \"\"\"\n    \n    A = 1\/(1+np.exp(-Z))\n    cache = Z\n    \n    return A, cache\n\ndef relu(Z):\n    \"\"\"\n    Implement the RELU function.\n\n    Arguments:\n    Z -- Output of the linear layer, of any shape\n\n    Returns:\n    A -- Post-activation parameter, of the same shape as Z\n    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    \n    A = np.maximum(0,Z)\n    \n    assert(A.shape == Z.shape)\n    \n    cache = Z \n    return A, cache\n\n\ndef relu_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single RELU unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    \n    Z = cache\n    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n    \n    # When z <= 0, you should set dz to 0 as well. \n    dZ[Z <= 0] = 0\n    \n    assert (dZ.shape == Z.shape)\n    \n    return dZ\n\ndef sigmoid_backward(dA, cache):\n    \"\"\"\n    Implement the backward propagation for a single SIGMOID unit.\n\n    Arguments:\n    dA -- post-activation gradient, of any shape\n    cache -- 'Z' where we store for computing backward propagation efficiently\n\n    Returns:\n    dZ -- Gradient of the cost with respect to Z\n    \"\"\"\n    \n    Z = cache\n    \n    s = 1\/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)\n    \n    assert (dZ.shape == Z.shape)\n    \n    return dZ\n\n\ndef load_data():\n    train_dataset = h5py.File('..\/input\/catsanddogs\/train_catvnoncat.h5', \"r\")\n    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n\n    test_dataset = h5py.File('..\/input\/catsanddogs\/test_catvnoncat.h5', \"r\")\n    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n\n    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n    \n    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n    \n    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes\n\n\ndef initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    parameters -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"\n    \n    np.random.seed(1)\n    \n    W1 = np.random.randn(n_h, n_x)*0.01\n    b1 = np.zeros((n_h, 1))\n    W2 = np.random.randn(n_y, n_h)*0.01\n    b2 = np.zeros((n_y, 1))\n    \n    assert(W1.shape == (n_h, n_x))\n    assert(b1.shape == (n_h, 1))\n    assert(W2.shape == (n_y, n_h))\n    assert(b2.shape == (n_y, 1))\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters     \n\n\ndef initialize_parameters_deep(layer_dims):\n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the dimensions of each layer in our network\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n                    bl -- bias vector of shape (layer_dims[l], 1)\n    \"\"\"\n    \n    np.random.seed(1)\n    parameters = {}\n    L = len(layer_dims)            # number of layers in the network\n\n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) \/ np.sqrt(layer_dims[l-1]) #*0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n        \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n\n        \n    return parameters\n\ndef linear_forward(A, W, b):\n    \"\"\"\n    Implement the linear part of a layer's forward propagation.\n\n    Arguments:\n    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n\n    Returns:\n    Z -- the input of the activation function, also called pre-activation parameter \n    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n    \"\"\"\n    \n    Z = W.dot(A) + b\n    \n    assert(Z.shape == (W.shape[0], A.shape[1]))\n    cache = (A, W, b)\n    \n    return Z, cache\n\ndef linear_activation_forward(A_prev, W, b, activation):\n    \"\"\"\n    Implement the forward propagation for the LINEAR->ACTIVATION layer\n\n    Arguments:\n    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n    b -- bias vector, numpy array of shape (size of the current layer, 1)\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n\n    Returns:\n    A -- the output of the activation function, also called the post-activation value \n    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n             stored for computing the backward pass efficiently\n    \"\"\"\n    \n    if activation == \"sigmoid\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n    \n    elif activation == \"relu\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n    \n    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n    cache = (linear_cache, activation_cache)\n\n    return A, cache\n\ndef L_model_forward(X, parameters):\n    \"\"\"\n    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n    \n    Arguments:\n    X -- data, numpy array of shape (input size, number of examples)\n    parameters -- output of initialize_parameters_deep()\n    \n    Returns:\n    AL -- last post-activation value\n    caches -- list of caches containing:\n                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n    \"\"\"\n\n    caches = []\n    A = X\n    L = len(parameters) \/\/ 2                  # number of layers in the neural network\n    \n    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    for l in range(1, L):\n        A_prev = A \n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n        caches.append(cache)\n    \n    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n    caches.append(cache)\n    \n    assert(AL.shape == (1,X.shape[1]))\n            \n    return AL, caches\n\ndef compute_cost(AL, Y):\n    \"\"\"\n    Implement the cost function defined by equation (7).\n\n    Arguments:\n    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n\n    Returns:\n    cost -- cross-entropy cost\n    \"\"\"\n    \n    m = Y.shape[1]\n\n    # Compute loss from aL and y.\n    cost = (1.\/m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n    \n    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n    assert(cost.shape == ())\n    \n    return cost\n\ndef linear_backward(dZ, cache):\n    \"\"\"\n    Implement the linear portion of backward propagation for a single layer (layer l)\n\n    Arguments:\n    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n\n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    dW = 1.\/m * np.dot(dZ,A_prev.T)\n    db = 1.\/m * np.sum(dZ, axis = 1, keepdims = True)\n    dA_prev = np.dot(W.T,dZ)\n    \n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    \n    return dA_prev, dW, db\n\ndef linear_activation_backward(dA, cache, activation):\n    \"\"\"\n    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n    \n    Arguments:\n    dA -- post-activation gradient for current layer l \n    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n    \n    Returns:\n    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n    \"\"\"\n    linear_cache, activation_cache = cache\n    \n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n        \n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n    \n    return dA_prev, dW, db\n\ndef L_model_backward(AL, Y, caches):\n    \"\"\"\n    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n    \n    Arguments:\n    AL -- probability vector, output of the forward propagation (L_model_forward())\n    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n    caches -- list of caches containing:\n                every cache of linear_activation_forward() with \"relu\" (there are (L-1) or them, indexes from 0 to L-2)\n                the cache of linear_activation_forward() with \"sigmoid\" (there is one, index L-1)\n    \n    Returns:\n    grads -- A dictionary with the gradients\n             grads[\"dA\" + str(l)] = ... \n             grads[\"dW\" + str(l)] = ...\n             grads[\"db\" + str(l)] = ... \n    \"\"\"\n    grads = {}\n    L = len(caches) # the number of layers\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n    \n    # Initializing the backpropagation\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    \n    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n    current_cache = caches[L-1]\n    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n    \n    for l in reversed(range(L-1)):\n        # lth layer: (RELU -> LINEAR) gradients.\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n\n    return grads\n\ndef update_parameters(parameters, grads, learning_rate):\n    \"\"\"\n    Update parameters using gradient descent\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients, output of L_model_backward\n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n                  parameters[\"W\" + str(l)] = ... \n                  parameters[\"b\" + str(l)] = ...\n    \"\"\"\n    \n    L = len(parameters) \/\/ 2 # number of layers in the neural network\n\n    # Update rule for each parameter. Use a for loop.\n    for l in range(L):\n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n        \n    return parameters\n\ndef predict(X, y, parameters):\n    \"\"\"\n    This function is used to predict the results of a  L-layer neural network.\n    \n    Arguments:\n    X -- data set of examples you would like to label\n    parameters -- parameters of the trained model\n    \n    Returns:\n    p -- predictions for the given dataset X\n    \"\"\"\n    \n    m = X.shape[1]\n    n = len(parameters) \/\/ 2 # number of layers in the neural network\n    p = np.zeros((1,m))\n    \n    # Forward propagation\n    probas, caches = L_model_forward(X, parameters)\n\n    \n    # convert probas to 0\/1 predictions\n    for i in range(0, probas.shape[1]):\n        if probas[0,i] > 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n    \n    #print results\n    #print (\"predictions: \" + str(p))\n    #print (\"true labels: \" + str(y))\n    print(\"Accuracy: \"  + str(np.sum((p == y)\/m)))\n        \n    return p\n\ndef print_mislabeled_images(classes, X, y, p):\n    \"\"\"\n    Plots images where predictions and truth were different.\n    X -- dataset\n    y -- true labels\n    p -- predictions\n    \"\"\"\n    a = p + y\n    mislabeled_indices = np.asarray(np.where(a == 1))\n    plt.rcParams['figure.figsize'] = (40.0, 40.0) # set default size of plots\n    num_images = len(mislabeled_indices[0])\n    for i in range(num_images):\n        index = mislabeled_indices[1][i]\n        \n        plt.subplot(2, num_images, i + 1)\n        plt.imshow(X[:,index].reshape(256,256,3), interpolation='nearest')\n        plt.axis('off')\n        plt.title(\"Prediction: \" + classes[int(p[0,index])] + \" \\n Class: \" + classes[y[0,index]])","6bfeb956":"import time\nimport numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\nimport scipy\nfrom PIL import Image\nfrom scipy import ndimage\nimport cv2\nimport datetime as dt\nimport matplotlib.pylab as plb\nimport numpy as np\nimport os\nimport pandas as pd\nfrom glob import glob\ntry:\n    import cPickle as pickle\nexcept ImportError:  # Python 3.x\n    import pickle\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\nplt.rcParams['image.interpolation'] = 'nearest'\nplt.rcParams['image.cmap'] = 'gray'\n\n%load_ext autoreload\n%autoreload 2\n\nnp.random.seed(1)","830d0ad0":"classes = ['NORMAL', 'PNEUMONIA']","8c9beae0":"def proc_images(split='test'):\n    \"\"\"\n    Saves compressed, resized images as HDF5 datsets\n    Returns\n        data.h5, where each dataset is an image or class label\n        e.g. X23,y23 = image and corresponding class label\n    \"\"\"\n    start = dt.datetime.now()\n    # ..\/input\/\n    PATH = os.path.abspath(os.path.join('..', 'input'))\n    print(PATH)\n    # ..\/input\/sample\/images\/\n    SOURCE_IMAGES = os.path.join('..\/input\/chest-xray-pneumonia')\n    print(SOURCE_IMAGES)\n    # ..\/input\/sample\/images\/*.png\n    images = glob(os.path.join(SOURCE_IMAGES,'*', split, '*', \"*.jpeg\"))\n    # Load labels\n#     labels = pd.read_csv('..\/input\/sample_labels.csv')\n    target = 'PNEUMONIA'\n    \n    # Size of data\n    NUM_IMAGES = len(images)\n    HEIGHT = 256\n    WIDTH = 256\n    CHANNELS = 3\n    SHAPE = (HEIGHT, WIDTH, CHANNELS)\n    \n    with h5py.File(split + '_data.h5', 'w') as hf: \n        for i,img in enumerate(images):            \n            # Images\n            image = cv2.imread(img)\n            image = cv2.resize(image, (WIDTH,HEIGHT), interpolation=cv2.INTER_CUBIC)\n            Xset = hf.create_dataset(\n                name='X'+str(i),\n                data=image,\n                shape=(HEIGHT, WIDTH, CHANNELS),\n                maxshape=(HEIGHT, WIDTH, CHANNELS),\n                compression=\"gzip\",\n                compression_opts=9)\n            \n            # Labels\n            base = os.path.basename(os.path.dirname(img))\n            class_num = 1 if base == target else 0\n            \n            yset = hf.create_dataset(\n                name='y'+str(i),\n                shape=(1,),\n                data=class_num,\n                maxshape=(None,),\n                compression=\"gzip\",\n                compression_opts=9)\n#             if base == target:\n#                 yset = 1\n#             else:\n#                 yset = 0\n            end=dt.datetime.now()\n            print(\"\\r\", i, \": \", (end-start).seconds, \"seconds\", end=\"\")","a4d78efd":"# proc_images('train')\n# proc_images('test')\n# proc_images('val')","e2055ceb":"def load_h5_datasets():\n    h5_source = '..\/input\/pneumoniavsnormalh5'\n    \n    print('loading train set')\n    train_dataset = h5py.File(h5_source + '\/train_data.h5', \"r\")\n    train_size = len(train_dataset.keys()) \/\/ 2\n    sampled_indices = np.random.choice(np.arange(train_size), size=train_size\/\/4, replace=False)\n    train_x_orig = np.array([train_dataset[\"X\" + str(i)] for i in sampled_indices]) # your train set features\n    train_y = np.array([train_dataset[\"y\" + str(i)] for i in sampled_indices]).T # your train set labels\n    train_dataset.close()\n    \n    print('loading test set')\n    test_dataset = h5py.File(h5_source + '\/test_data.h5', \"r\")\n    test_size = len(test_dataset.keys()) \/\/ 2\n    test_x_orig = np.array([test_dataset[\"X\" + str(i)] for i in range(test_size)]) # your train set features\n    test_y = np.array([test_dataset[\"y\" + str(i)] for i in range(test_size)]).T # your train set labels\n    test_dataset.close()\n    \n    print('loading val set')\n    val_dataset = h5py.File(h5_source + '\/val_data.h5', \"r\")\n    val_size = len(val_dataset.keys()) \/\/ 2 # it has both the keys for Xi and yi, so halving them returns the size of the dataset\n    val_x_orig = np.array([val_dataset[\"X\" + str(i)] for i in range(val_size)]) # your train set features\n    val_y = np.array([val_dataset[\"y\" + str(i)] for i in range(val_size)]).T # your train set labels\n    val_dataset.close()\n    \n    print(f\"train size: {train_size}\")\n    print(f\"test size: {test_size}\")\n    print(f\"val size: {val_size}\")\n    \n    return train_x_orig,train_y, test_x_orig,test_y, val_x_orig, val_y","805b24f8":"import gc\n\ndef load_and_process_dataset():\n    train_x_orig,train_y, test_x_orig,test_y, val_x_orig, val_y = load_h5_datasets()\n    \n    index = 1\n    plt.imshow(train_x_orig[index])\n    print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]] +  \" picture.\")\n    \n    # Explore your dataset \n    m_train = train_x_orig.shape[0]\n    num_px = train_x_orig.shape[1]\n    m_test = test_x_orig.shape[0]\n\n    print (\"Number of training examples: \" + str(m_train))\n    print (\"Number of testing examples: \" + str(m_test))\n    print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n    print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n    print (\"train_y shape: \" + str(train_y.shape))\n    print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n    print (\"test_y shape: \" + str(test_y.shape))\n    \n    train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T\n    del train_x_orig\n    gc.collect() # Free original images memory from RAM\n    \n    test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n    del test_x_orig\n    gc.collect() # Free original images memory from RAM\n    \n    val_x_flatten = val_x_orig.reshape(val_x_orig.shape[0], -1).T\n    del val_x_orig\n    gc.collect() # Free original images memory from RAM\n    \n    # Standardize data to have feature values between 0 and 1.\n    train_x = train_x_flatten\/255.\n    test_x = test_x_flatten\/255.\n    val_x = val_x_flatten\/255.\n    \n    return train_x,train_y, test_x,test_y, val_x, val_y\n    ","422bb040":"train_x,train_y, test_x,test_y, val_x, val_y = load_and_process_dataset()","289537f0":"print (\"train_x's shape: \" + str(train_x.shape))\nprint (f'train_y has {100 * np.sum(train_y[0,:] == 1) \/ train_y.shape[1]}% of pneumonia images')\n\nprint (\"test_x's shape: \" + str(test_x.shape))\nprint (f'test_y has {100 * np.sum(test_y[0,:] == 1) \/ test_y.shape[1]}% of pneumonia images')\n\nprint (\"val_x's shape: \" + str(val_x.shape))\nprint (f'val_y has {100 * np.sum(val_y[0,:] == 1) \/ val_y.shape[1]}% of pneumonia images')","17deb03e":"# GRADED FUNCTION: L_layer_model\n\ndef L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n    \"\"\"\n    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n    \n    Arguments:\n    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n    learning_rate -- learning rate of the gradient descent update rule\n    num_iterations -- number of iterations of the optimization loop\n    print_cost -- if True, it prints the cost every 100 steps\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n\n    np.random.seed(1)\n    costs = []                         # keep track of cost\n    \n    # Parameters initialization. (\u2248 1 line of code)\n    ### START CODE HERE ###\n    parameters = initialize_parameters_deep(layers_dims)\n    ### END CODE HERE ###\n    \n     # Loop (gradient descent)\n    for i in range(0, num_iterations):\n\n        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n        ### START CODE HERE ### (\u2248 1 line of code)\n        AL, caches = L_model_forward(X, parameters)\n        ### END CODE HERE ###\n        \n        # Compute cost.\n        ### START CODE HERE ### (\u2248 1 line of code)\n        cost = compute_cost(AL, Y)\n        ### END CODE HERE ###\n    \n        # Backward propagation.\n        ### START CODE HERE ### (\u2248 1 line of code)\n        grads = L_model_backward(AL, Y, caches)\n        ### END CODE HERE ###\n \n        # Update parameters.\n        ### START CODE HERE ### (\u2248 1 line of code)\n        parameters = update_parameters(parameters, grads, learning_rate)\n        ### END CODE HERE ###\n                \n        # Print the cost every 100 training example\n        if print_cost and i % 5 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        if print_cost and i % 5 == 0:\n            costs.append(cost)\n            \n    # plot the cost\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per tens)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters","65131a08":"def save_parameters(parameters):\n    with open('trained_parameters.p', 'wb') as fp:\n        pickle.dump(parameters, fp, protocol=pickle.HIGHEST_PROTOCOL)\n    print('Parameters saved to: trained_parameters.p')\n        \ndef load_parameters(filename = 'trained_parameters.p'):\n    with open(filename, 'rb') as fp:\n        parameters = pickle.load(fp)\n    return parameters","2607a74a":"### CONSTANTS ###\nHEIGHT = 256\nWIDTH = 256\nCHANNELS = 3\nSHAPE = (HEIGHT, WIDTH, CHANNELS)\n\nlayers_dims = [HEIGHT*WIDTH*CHANNELS, 64, 20, 5, 1] #  5-layer model","1b4f5efa":"# parameters = load_parameters('..\/input\/best-parameters\/trained_parameters.p')","d5f2e932":"parameters = L_layer_model(train_x, train_y, layers_dims, learning_rate=0.001, num_iterations = 500, print_cost = True)","5eadf674":"save_parameters(parameters)","b18fceb8":"predictions_train = predict(train_x, train_y, parameters)","ed8c2747":"pred_test = predict(test_x, test_y, parameters)","76d1a0e2":"pred_val = predict(val_x, val_y, parameters)","064c6240":"print_mislabeled_images(classes, val_x, val_y, pred_val)","25534e32":"#### Loading h5 file\n\nNow we will load the h5 file to use it as ndarrays. We noticed our training dataset is too large, so we will be using only half of it for training","937f2e4d":"# Deep Neural Network for Image Classification: Final Project\n\nWe will use use the functions we'd implemented in the previous assignments to build a deep network, and apply it to pneumonia vs normal classification.\n\nLet's get started!","80de7a0f":"##  6) Results Analysis\n\nFirst, let's take a look at some images the L-layer model labeled incorrectly. This will show a few mislabeled images. ","49ffa4f2":"#### Processing dataset\n\nWe then need to flatten our representation, and then turning every value into the range of 0 <= x <= 1\n","f043d3cd":"#### Turn into h5\n\nHere we will process our dataset in order to create an h5 file from our jpeg images. Even though the dataset is only black and white pictures, we encoded it as RGB because the conversion was throwing errors.","36790212":"## 5 - L-layer Neural Network\n\nWe are using the helper functions we have implemented previously to build an $L$-layer neural network with the following structure: *[LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID*. The functions you may need and their inputs are:\n```python\ndef initialize_parameters_deep(layers_dims):\n    ...\n    return parameters \ndef L_model_forward(X, parameters):\n    ...\n    return AL, caches\ndef compute_cost(AL, Y):\n    ...\n    return cost\ndef L_model_backward(AL, Y, caches):\n    ...\n    return grads\ndef update_parameters(parameters, grads, learning_rate):\n    ...\n    return parameters\n```","c10d610b":"## 1 - Packages","cbe63e8c":"#### Storing parameters\n\nWe want to save the trained parameters in a file in order to be able to use the trained model later.","bddea686":"### Preprocessing\n","9c55ff5a":"## 2 - Dataset\n\nChest X-ray images (anterior-posterior) were selected from retrospective cohorts of pediatric patients of one to five years old from Guangzhou Women and Children\u2019s Medical Center, Guangzhou. All chest X-ray imaging was performed as part of patients\u2019 routine clinical care.\n\nFor the analysis of chest x-ray images, all chest radiographs were initially screened for quality control by removing all low quality or unreadable scans. The diagnoses for the images were then graded by two expert physicians before being cleared for training the AI system. In order to account for any grading errors, the evaluation set was also checked by a third expert.\n\nLet's get more familiar with the dataset. Load the data by running the cell below.","3ffb5b07":"We will now train the model as a 5-layer neural network. \n\nRun the cell below to train your model. The cost should decrease on every iteration. It may take up to 1 hour to run 500 iterations. Check if the \"Cost after iteration 0\" matches the expected output below, if not click on the square (\u2b1b) on the upper bar of the notebook to stop the cell and try to find your error.","4db98151":"Let's first import all the packages that we will need during this project. \n- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n- [matplotlib](http:\/\/matplotlib.org) is a library to plot graphs in Python.\n- [h5py](http:\/\/www.h5py.org) is a common package to interact with a dataset that is stored on an H5 file.\n- [PIL](http:\/\/www.pythonware.com\/products\/pil\/) and [scipy](https:\/\/www.scipy.org\/) are used here to test your model with your own picture at the end.\n- dnn_app_utils provides the functions implemented in the \"Building your Deep Neural Network: Step by Step\" assignment to this notebook.\n- np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work.","70f718af":"### Accuracy validation\n\nAfter training with different parameters, we were making decisions based on the test set. However, we want to make a final evaluation using the validation set"}}