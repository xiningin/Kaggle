{"cell_type":{"fcc1cee9":"code","abb79cdd":"code","67c79058":"code","b31dc8a7":"code","6668a263":"code","d61b5a55":"code","91d23cfd":"code","1dbe6bc3":"code","39bc4ce8":"code","c3048e01":"code","6341de26":"code","bed29ffd":"code","3b24dfa4":"code","cef433a9":"code","d0870acf":"markdown","e5d5c0a5":"markdown","3e0ddab2":"markdown","b6fe32d1":"markdown","5a1654c7":"markdown","5492c224":"markdown","704f3c28":"markdown","9852f72a":"markdown","f758aa4b":"markdown","62043785":"markdown","220d5c19":"markdown","fdd121f0":"markdown","9511dbdb":"markdown","250ed112":"markdown"},"source":{"fcc1cee9":"#for data processing\nimport numpy as np \nimport pandas as pd\nfrom math import ceil\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n#for model creation\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n#for evaluation and improvment\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n\n#for plotting the data\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns","abb79cdd":"train_data = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nlabel = train_data[\"label\"]\ny = to_categorical(label, 10)\ntest_data = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\nfulldata = pd.concat([train_data,test_data])\ntrain_data","67c79058":"#check if there is any missing values\nfulldata.drop(\"label\",axis = 1).isna().sum().sum()","b31dc8a7":"def show_img(dataset,row,label):\n    if \"label\" in dataset.columns:\n        x = np.array(dataset.drop(\"label\",axis = 1).iloc[row]).reshape(28,28,1)\n    else:\n        x = np.array(dataset.iloc[row]).reshape(28,28,1)\n    plt.imshow(x, cmap='gray')\n    plt.title(label)\n    plt.show()\nshow_img(train_data,1,label[1])\nplt.show()","6668a263":"sns.countplot(label)\nlabel.value_counts()\/len(label)","d61b5a55":"model = keras.Sequential([\n    layers.Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\",padding='same',input_shape=(28, 28, 1)),\n    layers.MaxPool2D(pool_size=(2, 2)),\n    \n    layers.Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\",padding='same'),\n    layers.MaxPool2D(pool_size=(2, 2)),\n    \n    layers.Conv2D(filters=128, kernel_size=(3,3), activation=\"relu\",padding='same'),\n    \n    layers.Flatten(),\n    layers.Dense(units=1024, activation=\"relu\"),\n    layers.Dense(units=512, activation=\"relu\"),\n    layers.Dense(units=10, activation=\"softmax\"),\n])","91d23cfd":"#split to train and test\nX_train,X_test,y_train,y_test = train_test_split(train_data.drop(\"label\",axis = 1),y,train_size = 0.9,random_state = 100)\n#transofrm to relevent dimensions\nX_train = X_train.to_numpy().reshape(X_train.shape[0],28,28,1)\nX_test = X_test.to_numpy().reshape(X_test.shape[0],28,28,1)\n#create model\nmodel.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n#fit model\nhistory = model.fit(\n    X_train,y_train,\n    validation_data = (X_test,y_test),\n    batch_size = 128,\n    epochs=50,\n    verbose=1,\n)\n","1dbe6bc3":"#check model convergence\nhistory_frame = pd.DataFrame(history.history)\nhistory_frame.loc[:, ['accuracy', 'val_accuracy']].plot()\nhistory_frame.loc[:, ['loss', 'val_loss']].plot();\n","39bc4ce8":"y_pred = model.predict(X_test)\ny_pred = np.argmax(y_pred,axis = -1)\ny_test = np.argmax(y_test,axis = -1)\npd.DataFrame(confusion_matrix(y_test,y_pred),columns = [i for i in range(10)],index = [i for i in range(10)])","c3048e01":"aug_model = keras.Sequential([\n    layers.InputLayer(input_shape=[28, 28, 1]),\n    #CNN layer 1\n    layers.Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\",padding='same'),\n    layers.Conv2D(filters=32, kernel_size=(3,3), activation=\"relu\",padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPool2D(pool_size=(2, 2)),\n    layers.Dropout(0.2),\n    \n    #CNN layer 2\n    layers.Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\",padding='same'),\n    layers.Conv2D(filters=64, kernel_size=(3,3), activation=\"relu\",padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPool2D(pool_size=(2, 2)),\n    layers.Dropout(0.2),\n    \n    #CNN layer 3\n    layers.Conv2D(filters=128, kernel_size=(3,3), activation=\"relu\",padding='same'),\n    layers.BatchNormalization(),\n    layers.MaxPool2D(pool_size=(2, 2)),\n    layers.Dropout(0.2),\n    \n    layers.Flatten(),\n    layers.Dense(units=1024, activation=\"relu\"),\n    layers.BatchNormalization(),\n    layers.Dropout(0.8),\n    layers.Dense(units=512, activation=\"relu\"),\n    layers.BatchNormalization(),\n    layers.Dropout(0.4),\n    layers.Dense(units=10, activation=\"softmax\")\n])\n\nX_train,X_test,y_train,y_test = train_test_split(train_data.drop(\"label\",axis = 1),y,train_size = 0.9,random_state = 100)\n\nX_train = X_train.to_numpy().reshape(X_train.shape[0],28,28,1)\nX_test = X_test.to_numpy().reshape(X_test.shape[0],28,28,1)\n\nwidth_shift_val = 0.1\nheight_shift_val = 0.1\nshear_range_val=10\nzoom_range_val= 0.1\nrandom_rotation_val = 10\ndatagen = ImageDataGenerator(rotation_range = random_rotation_val,width_shift_range = width_shift_val,height_shift_range = height_shift_val,zoom_range=zoom_range_val,)\ndatagen.fit(X_train)\n\naug_model.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss='categorical_crossentropy',\n    metrics=['accuracy']\n)\n\nchk_point = ModelCheckpoint('.\/best_model.hsf5',\n               monitor = 'val_accuracy',\n               mode = 'max',\n               save_best_model = True)\n\nagu_history = aug_model.fit(\n    datagen.flow(X_train,y_train,batch_size = 64),\n    validation_data = (X_test,y_test),\n    batch_size = 64,\n    epochs=100,\n    verbose=1,\n    callbacks = [ReduceLROnPlateau(monitor=\"val_loss\", factor = 0.6, patience = 5,verbose = 1),chk_point]\n)","6341de26":"aug_history_frame = pd.DataFrame(agu_history.history)\naug_history_frame.loc[:, ['accuracy', 'val_accuracy']].plot()\naug_history_frame.loc[:, ['loss', 'val_loss']].plot();\n\ny_pred = aug_model.predict(X_test)\ny_pred = np.argmax(y_pred,axis = -1)\ny_test = np.argmax(y_test,axis = -1)\npd.DataFrame(confusion_matrix(y_test,y_pred),columns = [i for i in range(10)],index = [i for i in range(10)])","bed29ffd":"# check for number of mistakes\nX_train,X_test,y_train,y_test = train_test_split(train_data.drop(\"label\",axis = 1),y,train_size = 0.9,random_state = 100)\ny_test = np.argmax(y_test,axis = -1)\nmiss_clf = y_pred!=y_test\nmissed_count = sum(miss_clf)\nprint(sum(miss_clf))","3b24dfa4":"nrow = ceil(missed_count\/3)\nncols = 3\nfig, axs = plt.subplots(nrow,ncols,figsize = (30,30))\naxs = axs.flatten()\n\nind = 0\nfor row,i in enumerate(miss_clf):\n    if (i and ind < 30):\n        x = np.array(X_test.iloc[row]).reshape(28,28,1)\n        axs[ind].imshow(x, cmap='gray')\n        axs[ind].set_title(f\"real val: {y_test[row]}, predicted val: {y_pred[row]}\")\n        ind +=1\nplt.tight_layout()\nplt.show()","cef433a9":"sub_label_vec = tf.keras.models.load_model('.\/best_model.hsf5').predict(test_data.to_numpy().reshape(test_data.shape[0],28,28,1))\nsub_labels = np.argmax(sub_label_vec,axis = -1)\nsub = pd.DataFrame({\"ImageId\":range(1,28001),\"Label\":sub_labels})\nsub.to_csv(\"submmision.csv\",index =False)\nsub","d0870acf":"**In the following notebook I am going to use CNN with data augmentation to classify number images from the MNIST dataset.**\n\n**First let's import the libraries we are going to work with.**","e5d5c0a5":"# **Now we will create a more complex model using data augmentation**","3e0ddab2":"# **Thanks for viewing, feel free to leave a comment and upvote :)**","b6fe32d1":"**Our target is distributed uniformly**","5a1654c7":"**looks like most of the misclassified images are reasonable, for most of them even a human would not classify the right number, the 9 that was wrongly classified as 8 really looks like an 8 and vice verca, also the 4's and 9's are confusing**","5492c224":"**Let's see how our new model does**","704f3c28":"**Now let's load the data!**","9852f72a":"**Let's make a simple model which we will try to improve later**","f758aa4b":"**our model clearly overfits the data we'll adress that problem with dropout neurons later**\n\n**Let's see where our model makes mistakes using a confution matrix**","62043785":"**Let's take a closer look at where our model didn't do so well**","220d5c19":"\n\n**let's see how are target is distributed**","fdd121f0":"**Let's see how our data looks**","9511dbdb":"we have many misclassification but reasnoble one's, it's easy to get confused between 4 and 9, 0 and 6, 2 and 7","250ed112":"**No missing values! :)**"}}