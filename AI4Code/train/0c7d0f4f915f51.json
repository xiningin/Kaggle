{"cell_type":{"01100d04":"code","ee5b0702":"code","2b0fdbcd":"code","fa3ce7c1":"code","5a1a2263":"code","4e326436":"code","fb0a7311":"code","557118e8":"code","708da6a7":"code","948229fd":"code","188b041a":"code","f122f9dd":"code","5008e353":"code","461bfecc":"code","c457f3b3":"code","836fcd47":"code","ad185e3c":"code","82bedd6b":"code","c2f97880":"code","ee1eca78":"code","64067fa9":"code","5908c1e0":"code","8c52a6d4":"code","474a57ab":"code","6fcbae1d":"code","330955d5":"code","18316a8a":"code","95802853":"code","116eab1a":"code","574e09bb":"code","876c1ee7":"code","806e5149":"code","ef83471a":"code","f8977623":"code","1e163307":"code","2863a2fd":"code","a0c642be":"code","d4ddc63c":"code","2ba58bae":"code","22d850a2":"code","28fc5615":"markdown","540de4f0":"markdown","0bf4ec53":"markdown","7518c15b":"markdown"},"source":{"01100d04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re # search in texts as scrap\nfrom collections import Counter\nfrom fbprophet import Prophet\nimport matplotlib.pyplot as plt # graph\nfrom plotly.offline import init_notebook_mode, iplot # graph\nfrom plotly import graph_objs as go # graph\n\n# Initialize plotly\ninit_notebook_mode(connected=True)\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","ee5b0702":"# importing and seeing the kaggle's file\nk1=pd.read_csv('\/kaggle\/input\/web-traffic-time-series-forecasting\/key_1.csv')\nk1.head()","2b0fdbcd":"# importing and seeing the kaggle's file\nk2=pd.read_csv('\/kaggle\/input\/web-traffic-time-series-forecasting\/key_2.csv')\nk2.head()","fa3ce7c1":"# importing and seeing the kaggle's file\nt2=pd.read_csv('\/kaggle\/input\/web-traffic-time-series-forecasting\/train_2.csv')\nt2.head()","5a1a2263":"# importing and seeing the kaggle's file\nt1=pd.read_csv('\/kaggle\/input\/web-traffic-time-series-forecasting\/train_1.csv')\nt1.head()","4e326436":"# importing and seeing the kaggle's file\nss2=pd.read_csv('\/kaggle\/input\/web-traffic-time-series-forecasting\/sample_submission_2.csv')\nss2.head()","fb0a7311":"# importing and seeing the kaggle's file\nss1=pd.read_csv('\/kaggle\/input\/web-traffic-time-series-forecasting\/sample_submission_1.csv')\nss1.head()\n","557118e8":"# view shapes of files \n\nprint('shape t1')\nprint(t1.shape)\nprint('shape t2')\nprint(t2.shape)\nprint('shape k1')\nprint(k1.shape)\nprint('shape k2')\nprint(k2.shape)\nprint('shape ss1')\nprint(ss1.shape)\nprint('shape ss2')\nprint(ss2.shape)","708da6a7":"# Verify general behavior\n\n#Total sum per column: \nt1.loc['Total',:]= t1.mean(axis=0)","948229fd":"x = t1.tail(1)\nx = x.T\nx","188b041a":"x.drop('Page', axis=0, inplace=True)\nx.drop('lang', axis=0, inplace=True)","f122f9dd":"x","5008e353":"def plotly_df(df, title=''):\n    \"\"\"Visualize all the dataframe columns as line plots.\"\"\"\n    common_kw = dict(x=df.index, mode='lines')\n    data = [go.Scatter(y=df[c], name=c, **common_kw) for c in df.columns]\n    layout = dict(title=title)\n    fig = dict(data=data, layout=layout)\n    iplot(fig, show_link=False)","461bfecc":"plotly_df(x, 'Total behavior')","c457f3b3":"# TO use prophet is necessary 2 columns ds (with date ) and y with information\ndf = x.reset_index()\ndf.columns = ['ds', 'y']\ndf.tail(n=3)","836fcd47":"# ds MUST be date\ndf['ds'] = pd.to_datetime(df['ds']).dt.date","ad185e3c":"# to try, last month will be predict \n\nprediction_size = 30\ntrain_df = df[:-prediction_size]\ntrain_df.tail(n=3)","82bedd6b":"# It's time to predict with prophet \n\nm = Prophet()\nm.fit(train_df)\nfuture = m.make_future_dataframe(periods=prediction_size)\nforecast = m.predict(future)","c2f97880":"forecast.tail(n=3)","ee1eca78":"m.plot(forecast)","64067fa9":"m.plot_components(forecast)","5908c1e0":"# minimizing outliers effects\n\noutliers = []\n\nfor i in range(0,len(df)-1):\n    if (df['y'][i] > forecast['yhat_upper'][i]):\n        outliers.append({'info':'max', 'index':i, 'date':df['ds'][i] ,'val':df['y'][i], 'forecast val': forecast['yhat_upper'][i], 'factor': df['y'][i]\/forecast['yhat'][i] })\n    if (df['y'][i] < forecast['yhat_lower'][i]):\n        outliers.append({'info':'min', 'index':i, 'date':df['ds'][i] ,'val':df['y'][i], 'forecast val': forecast['yhat_lower'][i], 'factor': df['y'][i]\/forecast['yhat'][i]})\n        \noutliers = pd.DataFrame(outliers)","8c52a6d4":"df_new = df.copy()\ndf_new","474a57ab":"for i in range (0, len(outliers)-1 ):    \n    df_new['y'][outliers['index'][i]] = df_new['y'][outliers['index'][i]] \/ outliers['factor'][i]","6fcbae1d":"# Prophet AGAIN with NEW df\n\n# ds MUST be date\ndf_new['ds'] = pd.to_datetime(df_new['ds']).dt.date","330955d5":"# to try, last month will be predict \n\nprediction_size = 30\ntrain_df = df_new[:-prediction_size]\ntrain_df.tail(n=3)","18316a8a":"# It's time to predict with prophet \n\nm = Prophet()\nm.fit(train_df)\nfuture = m.make_future_dataframe(periods=prediction_size)\nforecast = m.predict(future)","95802853":"m.plot(forecast)","116eab1a":"# Function to find page language\n\ndef get_language(page):\n    res = re.search('[a-z][a-z].wikipedia.org',page) # search text like mask\n    if res:\n        return res[0][0:2] # 2 first letters\n    return 'na'\n\nt1['lang'] = t1.Page.map(get_language) # new collumn\n# see: map () relation between series = https:\/\/www.geeksforgeeks.org\/python-pandas-map\/\n","574e09bb":"plt.figure(figsize=(12, 6))\nplt.title(\"Number of sites by languages\", fontsize=\"18\")\nt1['lang'].value_counts().plot.bar(rot=0);\n","876c1ee7":"t1.values[0]","806e5149":"t1.T.index.values","ef83471a":"t1['mean'] = ","f8977623":"\n#str_ = '2NE1_zh.wikipedia.org_all-access_spider'\nstr_[0:str_.find('.wikipedia.org')-3]\n\ndef get_subject(page):\n    res = page[0:page.find('.wikipedia.org')-3]\n    if res:\n        return res\n    return 'na'","1e163307":"t1['subject'] = t1.Page.map(get_subject) # new collumn","2863a2fd":"t1.Page","a0c642be":"#Split datasets in languages\n\nlang_sets = {}\nlang_sets['en'] = t1[t1.lang=='en'].iloc[:,0:-1]\nlang_sets['ja'] = t1[t1.lang=='ja'].iloc[:,0:-1]\nlang_sets['de'] = t1[t1.lang=='de'].iloc[:,0:-1]\nlang_sets['na'] = t1[t1.lang=='na'].iloc[:,0:-1]\nlang_sets['fr'] = t1[t1.lang=='fr'].iloc[:,0:-1]\nlang_sets['zh'] = t1[t1.lang=='zh'].iloc[:,0:-1]\nlang_sets['ru'] = t1[t1.lang=='ru'].iloc[:,0:-1]\nlang_sets['es'] = t1[t1.lang=='es'].iloc[:,0:-1]\n","d4ddc63c":"# access means for languages\nsums = {}\nfor key in lang_sets:\n    sums[key] = lang_sets[key].iloc[:,1:].sum(axis=0) \/ lang_sets[key].shape[0]","2ba58bae":"days = [r for r in range(sums['en'].shape[0])]\n","22d850a2":"################################ TESTS \n\n\n\n#re.search('{}.wikipedia.org','2NE1_zh.wikipedia.org_all-access_spider')\n#[0][0:2]\n# t1.Page.map(get_language)\n# lang_sets\n#lang_sets['es']\n#len(lang_sets)\n#japan = pd.DataFrame()\n#japan['date_']= ja.index\n#japan['count_'] = list(sums['ja'])\n#japan\n#pd.DataFrame(sums['ja'])\n\n#pd.DataFrame(sums['ja']).index\n\n\n\n# lang_sets['en'].iloc[:,1:].sum(axis=0)\n#lang_sets['en'].shape[0]","28fc5615":"## Importing Kaggle's Files","540de4f0":"[](http:\/\/)","0bf4ec53":"## WORKING ","7518c15b":"## General Information\n\n## Data Description\n\nThe training dataset consists of approximately 145k time series. Each of these time series represent a number of daily views of a different Wikipedia article, starting from July, 1st, 2015 up until December 31st, 2016. The leaderboard during the training stage is based on traffic from January, 1st, 2017 up until March 1st, 2017.\n\nThe second stage will use training data up until September 1st, 2017. The final ranking of the competition will be based on predictions of daily views between September 13th, 2017 and November 13th, 2017 for each article in the dataset. You will submit your forecasts for these dates by September 12th.\n\nFor each time series, you are provided the name of the article as well as the type of traffic that this time series represent (all, mobile, desktop, spider). You may use this metadata and any other publicly available data to make predictions. Unfortunately, the data source for this dataset does not distinguish between traffic values of zero and missing values. A missing value may mean the traffic was zero or that the data is not available for that day.\n\nTo reduce the submission file size, each page and date combination has been given a shorter Id. The mapping between page names and the submission Id is given in the key files.\n\n## File descriptions\n\nFiles used for the first stage will end in '_1'. Files used for the second stage will end in '_2'. Both will have identical formats. The complete training data for the second stage will be made available prior to the second stage.\n\n**train_*.csv** - contains traffic data. This a csv file where each row corresponds to a particular article and each column correspond to a particular date. Some entries are missing data. The page names contain the Wikipedia project (e.g. en.wikipedia.org), type of access (e.g. desktop) and type of agent (e.g. spider). In other words, each article name has the following format: 'name_project_access_agent' (e.g. 'AKB48_zh.wikipedia.org_all-access_spider').\n\n**key_*.csv** - gives the mapping between the page names and the shortened Id column used for prediction\n\n**sample_submission_*.csv** - a submission file showing the correct format"}}