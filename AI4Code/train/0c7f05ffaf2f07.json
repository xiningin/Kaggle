{"cell_type":{"86ec9a67":"code","21ac8b3c":"code","4efd1a89":"code","7e4c54c0":"code","ebde936a":"code","181f17f7":"code","8bc9dc10":"code","66ec7d52":"code","4c856fde":"code","f542199c":"code","6254b1fa":"code","c5c3db9a":"code","6cf7d06b":"code","687021a9":"code","e7a5b666":"code","825e318f":"code","3e29f38e":"code","f91d161e":"code","bd4159cc":"code","282c78de":"code","06be2162":"code","92522ac2":"code","1e4fafd9":"code","11b05955":"code","8aaf4b1d":"code","596be5ec":"code","dd2f85f4":"code","b950c62b":"code","6e964488":"code","65255340":"code","bfc16b6d":"code","e0b0be79":"code","07b0764a":"code","8c639b51":"code","886e741c":"code","a7b10365":"code","368d787e":"code","54f97133":"code","3fef509c":"code","af7ca34a":"code","7ee9014f":"code","62d0c98f":"code","91a4fe86":"code","dba6f471":"code","68837e34":"code","c4d3a9d5":"code","f9ee08e8":"code","ad58633c":"code","b204d7ad":"code","5f5c5d86":"code","4722731d":"code","5210d755":"code","2685a949":"markdown","fc07d6fa":"markdown","2e7d8201":"markdown","7972c0c4":"markdown","eb1a2eae":"markdown","edd9bc17":"markdown","db6dce4f":"markdown","1a981366":"markdown","3ac1eb62":"markdown","a90d38fb":"markdown","d8f3a00b":"markdown","39060687":"markdown","0b861104":"markdown","80db444f":"markdown","849310a0":"markdown","36fcbb41":"markdown","c722dd3d":"markdown","d2567a09":"markdown","94661116":"markdown","bce21899":"markdown","2710f1fe":"markdown","2bd74531":"markdown","e01fcf07":"markdown","a5f155a3":"markdown","c36ce14f":"markdown","10d33588":"markdown","d6dc7390":"markdown","5d0d6a0a":"markdown","edea6857":"markdown","450ebd63":"markdown","2814f75f":"markdown","d2a202fd":"markdown","35021ff6":"markdown","785a1124":"markdown","96bb47b4":"markdown","a5957a78":"markdown","7d14417c":"markdown","5dc24fa6":"markdown","e126eda2":"markdown","3954cfa4":"markdown","edc1c03b":"markdown","20db05af":"markdown","ea50a4f7":"markdown","bac96d8d":"markdown","30c48707":"markdown","133fc1ab":"markdown"},"source":{"86ec9a67":"import os\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport random\nimport gc\n\nimport sys\nsys.path.append('..\/input\/readability-package')\nimport readability\nimport spacy\n\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk import pos_tag, pos_tag_sents\nimport string\nimport re\nimport math\nimport pickle\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nimport torch\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\n# Autocast\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim.swa_utils import AveragedModel\n\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, AdamW\n\nimport warnings\nwarnings.filterwarnings('ignore')","21ac8b3c":"!cp -r ..\/input\/spacy-readability\/spacy_readability-master\/* .\/\n!cp -r ..\/input\/syllapy\/syllapy-master\/* .\/\nimport spacy\nfrom spacy_readability import Readability\n\nnlp = spacy.load('en')\nnlp.add_pipe(Readability(), last = True)","4efd1a89":"def seed_everything(seed = 0):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n\nseed = 0\nseed_everything(seed)","7e4c54c0":"base_dir = '..\/input\/commonlitreadabilityprize'\ntrain_data = pd.read_csv(f'{base_dir}\/train.csv')\n# Benchmark text\nbenchmark = train_data[train_data['standard_error'] == 0.]","ebde936a":"base_dir = '..\/input\/commonlitreadabilityprize'\ndata = pd.read_csv(f'{base_dir}\/test.csv')\nss = pd.read_csv(f'{base_dir}\/sample_submission.csv')\ndata.head()","181f17f7":"def clean_text(text):\n    text = text.lower().strip()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n\ndef text_preprocessing(text):\n    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n    nopunc = clean_text(text)\n    tokenized_text = tokenizer.tokenize(nopunc)\n    combined_text = ' '.join(tokenized_text)\n    return combined_text\n\ndef readability_feat(text):\n    text = nlp(text)\n    \n    return np.array([text._.flesch_kincaid_grade_level,\n                     text._.flesch_kincaid_reading_ease,\n                     text._.dale_chall,\n                     text._.coleman_liau_index,\n                     text._.automated_readability_index,\n                     text._.forcast], dtype = np.float)\n\ndef sample_text(targets, num_output = 5):\n    mean, var = targets[0], targets[1]\n    if targets[1] != 0.:\n        sampled_target = torch.normal(mean, var, size = (num_output,))\n    else:\n        sampled_target = torch.tensor([0.] * num_output, dtype = torch.float)\n    return sampled_target\n\ndef convert_examples_to_features(text, tokenizer, max_len, is_test = False, return_tensor = False):\n    # Take from https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-fit\n    text = text.replace('\\n', '')\n    if return_tensor:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            return_tensors = 'pt',\n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = True\n        )\n    else:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = True\n        )\n    return tok\n\ndef form_dataset(token, external_features = None, target = None, bins = None):\n    if target is not None:\n        if bins is not None:\n            if external_features is not None:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'external_features': torch.tensor(external_features, dtype = torch.float),\n                    'target': target,\n                    'bins': bins,\n                }\n            else:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'target': target,\n                    'bins': bins,\n                }\n        else:\n            if external_features is not None:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'external_features': torch.tensor(external_features, dtype = torch.float),\n                    'target': target,\n                }\n            else:\n                return {\n                    'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                    'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                    'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                    'target': target,\n                }\n    else:\n        if external_features is not None:\n            return {\n                'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n                'external_features': torch.tensor(external_features, dtype = torch.float),\n            }\n        else:\n            return {\n                'input_ids': torch.tensor(token['input_ids'], dtype = torch.long),\n                'token_type_ids': torch.tensor(token['token_type_ids'], dtype = torch.long),\n                'attention_mask': torch.tensor(token['attention_mask'], dtype = torch.long),\n            }","8bc9dc10":"class Readability_Dataset(Dataset):\n    def __init__(self, documents, tokenizer, max_len = 300, mode = 'infer'):\n        self.documents = documents\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.mode = mode\n        \n    def __len__(self):\n        return len(self.documents)\n    \n    def __getitem__(self, idx):\n        sample = self.documents.iloc[idx]\n        document = sample['excerpt']\n        \n        # Tokenize\n        features = convert_examples_to_features(document, self.tokenizer, self.max_len)\n        \n        return form_dataset(features)","66ec7d52":"class AttentivePooling(nn.Module):\n    def __init__(self, input_dim = 768, attention_dim = 1024):\n        super(AttentivePooling, self).__init__()\n        # Attention pooler\n        self.word_weight = nn.Linear(input_dim, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n    def forward(self, x, mask = None):\n        '''\n        x : Batch_size x Seq_len x input_dim\n        mask: \n        '''\n        # Attention Pooling (over sequence for the first sequence)\n        u_i = torch.tanh(self.word_weight(x))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        \n        if mask is not None:\n            att = att * (1 - mask.unsqueeze(-1))\n            \n        att = att \/ torch.sum(att, dim = 1, keepdim = True)\n        \n        x = x * att\n        return x.sum(dim = 1)","4c856fde":"class Readability_Model_RoBERTa_base_v11(nn.Module):\n    def __init__(self, backbone, model_config, benchmark_token = None, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True):\n        super(Readability_Model_RoBERTa_base_v11, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size, num_cat)\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.output_cat)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        output = self.layer_norm(output_backbone.pooler_output)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits \/= len(self.dropouts)\n        cats \/= len(self.dropouts)\n\n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","f542199c":"class Readability_Model_RoBERTa_large_v15(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_RoBERTa_large_v15, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean\/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att \/ torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits \/= len(self.dropouts_output)\n        cats \/= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","6254b1fa":"class Readability_Model_RoBERTa_large_v16(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_RoBERTa_large_v16, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Weighted mean pooling (over hidden layers)\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        output_backbone = torch.sum(hidden_states * layer_weight, dim = 0)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att \/ torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits \/= len(self.dropouts_output)\n        cats \/= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","c5c3db9a":"class Readability_Model_XLNet_large_cased_v2(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_XLNet_large_cased_v2, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids.squeeze(),\n                                        token_type_ids = token_type_ids.squeeze(),\n                                        attention_mask = attention_mask.squeeze())\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean\/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att \/ torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits \/= len(self.dropouts_output)\n        cats \/= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","6cf7d06b":"class Readability_Model_XLNet_large_cased_v3(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_XLNet_large_cased_v3, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids.squeeze(),\n                                        token_type_ids = token_type_ids.squeeze(),\n                                        attention_mask = attention_mask.squeeze())\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean\/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att \/ torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits \/= len(self.dropouts_output)\n        cats \/= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","687021a9":"class Readability_Model_GPT2_medium_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_GPT2_medium_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean\/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att \/ torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits \/= len(self.dropouts_output)\n        cats \/= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","e7a5b666":"class Readability_Model_ALBERT_xlarge_v2_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_ALBERT_xlarge_v2_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean\/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att \/ torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits \/= len(self.dropouts_output)\n        cats \/= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","825e318f":"class Readability_Model_ELECTRA_large_discriminator_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_ELECTRA_large_discriminator_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    \n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean\/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att \/ torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits \/= len(self.dropouts_output)\n        cats \/= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","3e29f38e":"class Readability_Model_DeBERTa_large_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_DeBERTa_large_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids, token_type_ids = token_type_ids, attention_mask = attention_mask)\n        \n        # Extract output\n        hidden_states = output_backbone.hidden_states\n        \n        # Mean\/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att \/ torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits \/= len(self.dropouts_output)\n        cats \/= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","f91d161e":"class Readability_Model_Funnel_large_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_Funnel_large_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids.squeeze(),\n                                        token_type_ids = token_type_ids.squeeze(),\n                                        attention_mask = attention_mask.squeeze())\n        \n        # Extract output\n        output_backbone = output_backbone.last_hidden_state\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att \/ torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits \/= len(self.dropouts_output)\n        cats \/= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","bd4159cc":"class Readability_Model_BART_large_v1(nn.Module):\n    def __init__(self, backbone, model_config, is_sampled = False, num_external_features = 6, num_output = 2, \n                 num_cat = 7, attention_dim = 1024, multisample_dropout = True, benchmark_token = None):\n        super(Readability_Model_BART_large_v1, self).__init__()\n        self.model_config = model_config\n        self.is_sampled = is_sampled\n        self.benchmark_token = benchmark_token\n        self.backbone = AutoModel.from_pretrained(backbone, config = self.model_config)\n        self.layer_norm = nn.LayerNorm(self.model_config.hidden_size * 2)    # Concat of mean and max pooling\n        self.output = nn.Linear(self.model_config.hidden_size * 2, num_output)   #  + num_external_features\n        self.output_cat = nn.Linear(self.model_config.hidden_size * 2, num_cat)\n        \n        # Attention pooler\n        self.word_weight = nn.Linear(self.model_config.hidden_size * 2, attention_dim)\n        self.context_weight = nn.Linear(attention_dim, 1)\n        \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(self.model_config.num_hidden_layers * 2).view(-1, 1, 1, 1))\n        \n        # Dropout layers\n        if multisample_dropout:\n            self.dropouts_output = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n            self.dropouts_cat = nn.ModuleList([\n                nn.Dropout(0.5) for _ in range(5)\n            ])\n        else:\n            self.dropouts = nn.ModuleList([nn.Dropout(0.3)])\n        \n        # Initialize weights\n        self._init_weights(self.layer_norm)\n        self._init_weights(self.output)\n        self._init_weights(self.word_weight)\n        self._init_weights(self.context_weight)\n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.init_std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean = 0.0, std = self.model_config.init_std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def forward(self, input_ids, token_type_ids, attention_mask, external_features = None):\n        output_backbone = self.backbone(input_ids = input_ids.squeeze(),\n                                        attention_mask = attention_mask.squeeze())\n        \n        decoder_hidden_states = output_backbone.decoder_hidden_states\n        encoder_hidden_states = output_backbone.encoder_hidden_states\n\n        # Mean\/max pooling (over hidden layers), concatenate with pooler\n        decoder_hidden_states = tuple(decoder_hidden_states[-i-1] for i in range(self.model_config.num_hidden_layers))\n        encoder_hidden_states = tuple(encoder_hidden_states[-i-1] for i in range(self.model_config.num_hidden_layers))\n        hidden_states = torch.stack(decoder_hidden_states + encoder_hidden_states, dim = 0)\n        layer_weight = F.softmax(self.hidden_layer_weights, dim = 0)\n        out_mean = torch.sum(hidden_states * layer_weight, dim = 0)\n        out_max, _ = torch.max(hidden_states, dim = 0)\n        output_backbone = torch.cat((out_mean, out_max), dim = -1)\n        output_backbone = self.layer_norm(output_backbone)\n        \n        # Attention Pooling (over time)\n        u_i = torch.tanh(self.word_weight(output_backbone))\n        u_w = self.context_weight(u_i).squeeze(1)\n        val = u_w.max()\n        att = torch.exp(u_w - val)\n        att = att \/ torch.sum(att, dim = 1, keepdim = True)\n        \n        output = output_backbone * att\n        output = output.sum(dim = 1)\n        \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_output):\n            if i == 0:\n                logits = self.output(dropout(output))\n                cats = self.output_cat(self.dropouts_cat[i](output))\n            else:\n                logits += self.output(dropout(output))\n                cats += self.output_cat(self.dropouts_cat[i](output))\n        \n        logits \/= len(self.dropouts_output)\n        cats \/= len(self.dropouts_output)\n        \n        if self.benchmark_token is not None:\n            logits = logits[:-1] - logits[-1]\n\n            cats = cats[:-1]\n        \n        if self.is_sampled:\n            return logits, None, torch.argmax(F.softmax(cats, dim = -1), dim = -1)\n        else:\n            return logits[:,0], torch.exp(0.5 * logits[:,1]), torch.argmax(F.softmax(cats, dim = -1), dim = -1)","282c78de":"def infer(model, dataloader, device = 'cpu', use_tqdm = True, benchmark_token = None):\n    model.eval()\n    \n    if use_tqdm:\n        tbar = tqdm(dataloader)\n    else:\n        tbar = dataloader\n        \n    pred = []\n        \n    for item in tbar:\n        input_ids = item['input_ids'].to(device)\n        token_type_ids = item['token_type_ids'].to(device)\n        attention_mask = item['attention_mask'].to(device)\n        \n        if benchmark_token is not None:\n            benchmark_input_ids, benchmark_token_type_ids, benchmark_attention_mask = benchmark_token\n            input_ids = torch.cat((input_ids, benchmark_input_ids), dim = 0)\n            token_type_ids = torch.cat((token_type_ids, benchmark_token_type_ids), dim = 0)\n            attention_mask = torch.cat((attention_mask, benchmark_attention_mask), dim = 0)\n            \n        with torch.no_grad():\n            with autocast():\n                pred_mean, pred_std, pred_bins = model(input_ids = input_ids, \n                                                       attention_mask = attention_mask, \n                                                       token_type_ids = token_type_ids)\n        \n        pred.extend(pred_mean.cpu().detach().numpy())\n        \n    # Stack\n    pred = np.array(pred)\n    \n    return pred","06be2162":"class config():\n    # For inference\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    use_tqdm = True\n    model_names = ['roberta_large_v15_0', 'roberta_large_v15_3',\n                   'roberta_large_v16_1', \n                   'gpt2_medium_v1_0', \n                   'xlnet_large_cased_v2_0', 'xlnet_large_cased_v3_0', 'xlnet_large_cased_v3_1', \n                   'electra_large_discriminator_v1_0', 'electra_large_discriminator_v1_1',\n                   'deberta_large_v1_0', 'deberta_large_v1_1',\n                   'funnel_large_v1_0',\n                   'bart_large_v1_0']\n    # For dataloader\n    max_len = [250] * 15\n    batch_size = (8, 8, 8, \n                  8, 8,\n                  6, \n                  6, 6, 6,\n                  8, 8,\n                  4, 4,\n                  8,\n                  8)    # In the same order as the 'model_names' attribute\n    num_workers = 4\n    # For models\n    num_bins = (29, 1, 29, \n                29, 29, \n                29, \n                29, 1, 1, \n                1, 1,\n                29, 29,\n                1,\n                1)    # In the same order as the 'model_names' attribute\n    \ncfg = config()","92522ac2":"# Tokenizer and model configuration\ntokenizer_roberta_large = AutoTokenizer.from_pretrained('..\/input\/robertalarge', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_roberta_large = AutoConfig.from_pretrained('..\/input\/robertalarge', output_hidden_states = True)\n\ntokenizer_gpt2_medium = AutoTokenizer.from_pretrained('..\/input\/gpt2-medium', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\ntokenizer_gpt2_medium.add_special_tokens({'pad_token': '[PAD]'})\n\ntokenizer_xlnet_large_cased = AutoTokenizer.from_pretrained('..\/input\/xlnet-large-cased', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\ntokenizer_xlnet_large_cased.add_special_tokens({'pad_token': '[PAD]'})\n\ntokenizer_electra_large = AutoTokenizer.from_pretrained('..\/input\/electra-large-discriminator', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_electra_large = AutoConfig.from_pretrained('..\/input\/electra-large-discriminator', output_hidden_states = True)\n\ntokenizer_deberta_large = AutoTokenizer.from_pretrained('..\/input\/deberta-large', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_debert_large = AutoConfig.from_pretrained('..\/input\/deberta-large', output_hidden_states = True)\n\ntokenizer_funnel_large = AutoTokenizer.from_pretrained('..\/input\/funnel-transformer-large', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_funnel_large = AutoConfig.from_pretrained('..\/input\/funnel-transformer-large', output_hidden_states = True)\n\ntokenizer_bart_large = AutoTokenizer.from_pretrained('..\/input\/bart-large', local_files_only = True, checkpoint_file = 'pytorch_model.bin')\nmodel_config_bart_large = AutoConfig.from_pretrained('..\/input\/bart-large', output_hidden_states = True)\n\n# Dataloader\ninfer_dataset_roberta_large_v15_0 = Readability_Dataset(data, tokenizer_roberta_large, max_len = cfg.max_len[0], mode = 'infer')\ninfer_dataloader_roberta_large_v15_0 = DataLoader(infer_dataset_roberta_large_v15_0, batch_size = cfg.batch_size[0], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_roberta_large_v15_3 = Readability_Dataset(data, tokenizer_roberta_large, max_len = cfg.max_len[2], mode = 'infer')\ninfer_dataloader_roberta_large_v15_3 = DataLoader(infer_dataset_roberta_large_v15_3, batch_size = cfg.batch_size[2], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_roberta_large_v16_1 = Readability_Dataset(data, tokenizer_roberta_large, max_len = cfg.max_len[4], mode = 'infer')\ninfer_dataloader_roberta_large_v16_1 = DataLoader(infer_dataset_roberta_large_v16_1, batch_size = cfg.batch_size[4], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_gpt2_medium_v1_0 = Readability_Dataset(data, tokenizer_gpt2_medium, max_len = cfg.max_len[5], mode = 'infer')\ninfer_dataloader_gpt2_medium_v1_0 = DataLoader(infer_dataset_gpt2_medium_v1_0, batch_size = cfg.batch_size[5], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_xlnet_large_cased_v2_0 = Readability_Dataset(data, tokenizer_xlnet_large_cased, max_len = cfg.max_len[6], mode = 'infer')\ninfer_dataloader_xlnet_large_cased_v2_0 = DataLoader(infer_dataset_xlnet_large_cased_v2_0, batch_size = cfg.batch_size[6], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_xlnet_large_cased_v3_0 = Readability_Dataset(data, tokenizer_xlnet_large_cased, max_len = cfg.max_len[7], mode = 'infer')\ninfer_dataloader_xlnet_large_cased_v3_0 = DataLoader(infer_dataset_xlnet_large_cased_v3_0, batch_size = cfg.batch_size[7], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_xlnet_large_cased_v3_1 = Readability_Dataset(data, tokenizer_xlnet_large_cased, max_len = cfg.max_len[8], mode = 'infer')\ninfer_dataloader_xlnet_large_cased_v3_1 = DataLoader(infer_dataset_xlnet_large_cased_v3_1, batch_size = cfg.batch_size[8], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_electra_large_v1_0 = Readability_Dataset(data, tokenizer_electra_large, max_len = cfg.max_len[9], mode = 'infer')\ninfer_dataloader_electra_large_v1_0 = DataLoader(infer_dataset_electra_large_v1_0, batch_size = cfg.batch_size[9], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_electra_large_v1_1 = Readability_Dataset(data, tokenizer_electra_large, max_len = cfg.max_len[10], mode = 'infer')\ninfer_dataloader_electra_large_v1_1 = DataLoader(infer_dataset_electra_large_v1_1, batch_size = cfg.batch_size[10], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_deberta_large_v1_0 = Readability_Dataset(data, tokenizer_deberta_large, max_len = cfg.max_len[11], mode = 'infer')\ninfer_dataloader_deberta_large_v1_0 = DataLoader(infer_dataset_deberta_large_v1_0, batch_size = cfg.batch_size[11], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_deberta_large_v1_1 = Readability_Dataset(data, tokenizer_deberta_large, max_len = cfg.max_len[12], mode = 'infer')\ninfer_dataloader_deberta_large_v1_1 = DataLoader(infer_dataset_deberta_large_v1_1, batch_size = cfg.batch_size[12], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_funnel_large_v1_0 = Readability_Dataset(data, tokenizer_funnel_large, max_len = cfg.max_len[13], mode = 'infer')\ninfer_dataloader_funnel_large_v1_0 = DataLoader(infer_dataset_funnel_large_v1_0, batch_size = cfg.batch_size[13], num_workers = cfg.num_workers, shuffle = False)\n\ninfer_dataset_bart_large_v1_0 = Readability_Dataset(data, tokenizer_bart_large, max_len = cfg.max_len[14], mode = 'infer')\ninfer_dataloader_bart_large_v1_0 = DataLoader(infer_dataset_bart_large_v1_0, batch_size = cfg.batch_size[14], num_workers = cfg.num_workers, shuffle = False)\n\n# Prediction storage\nprediction_roberta_large_v15_0 = np.zeros(data.shape[0])\nprediction_roberta_large_v15_2 = np.zeros(data.shape[0])\nprediction_roberta_large_v15_3 = np.zeros(data.shape[0])\nprediction_roberta_large_v16_0 = np.zeros(data.shape[0])\nprediction_roberta_large_v16_1 = np.zeros(data.shape[0])\nprediction_gpt2_medium_v1_0 = np.zeros(data.shape[0])\nprediction_xlnet_large_cased_v2_0 = np.zeros(data.shape[0])\nprediction_xlnet_large_cased_v3_0 = np.zeros(data.shape[0])\nprediction_xlnet_large_cased_v3_1 = np.zeros(data.shape[0])\nprediction_electra_large_v1_0 = np.zeros(data.shape[0])\nprediction_electra_large_v1_1 = np.zeros(data.shape[0])\nprediction_deberta_large_v1_0 = np.zeros(data.shape[0])\nprediction_deberta_large_v1_1 = np.zeros(data.shape[0])\nprediction_funnel_large_v1_0 = np.zeros(data.shape[0])\nprediction_bart_large_v1_0 = np.zeros(data.shape[0])\n\n# Tokenize the benchmark text\nbenchmark_token_roberta_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_roberta_large, cfg.max_len[0], return_tensor = True)\nbenchmark_token_roberta_large = (benchmark_token_roberta_large['input_ids'].to(cfg.device), \n                                 benchmark_token_roberta_large['token_type_ids'].to(cfg.device), \n                                 benchmark_token_roberta_large['attention_mask'].to(cfg.device))\n\nbenchmark_token_gpt2_medium = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_gpt2_medium, cfg.max_len[5], return_tensor = True)\nbenchmark_token_gpt2_medium = (benchmark_token_gpt2_medium['input_ids'].to(cfg.device), \n                               benchmark_token_gpt2_medium['token_type_ids'].to(cfg.device), \n                               benchmark_token_gpt2_medium['attention_mask'].to(cfg.device))\n\nbenchmark_token_xlnet_large_cased = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_xlnet_large_cased, cfg.max_len[6], return_tensor = True)\nbenchmark_token_xlnet_large_cased = (benchmark_token_xlnet_large_cased['input_ids'].to(cfg.device), \n                                     benchmark_token_xlnet_large_cased['token_type_ids'].to(cfg.device), \n                                     benchmark_token_xlnet_large_cased['attention_mask'].to(cfg.device))\n\nbenchmark_token_electra_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_electra_large, cfg.max_len[9], return_tensor = True)\nbenchmark_token_electra_large = (benchmark_token_electra_large['input_ids'].to(cfg.device), \n                                 benchmark_token_electra_large['token_type_ids'].to(cfg.device), \n                                 benchmark_token_electra_large['attention_mask'].to(cfg.device))\n\nbenchmark_token_deberta_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_deberta_large, cfg.max_len[11], return_tensor = True)\nbenchmark_token_deberta_large = (benchmark_token_deberta_large['input_ids'].to(cfg.device), \n                                 benchmark_token_deberta_large['token_type_ids'].to(cfg.device), \n                                 benchmark_token_deberta_large['attention_mask'].to(cfg.device))\n\nbenchmark_token_funnel_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_funnel_large, cfg.max_len[13], return_tensor = True)\nbenchmark_token_funnel_large = (benchmark_token_funnel_large['input_ids'].to(cfg.device), \n                                benchmark_token_funnel_large['token_type_ids'].to(cfg.device), \n                                benchmark_token_funnel_large['attention_mask'].to(cfg.device))\n\nbenchmark_token_bart_large = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer_bart_large, cfg.max_len[14], return_tensor = True)\nbenchmark_token_bart_large = (benchmark_token_bart_large['input_ids'].to(cfg.device), \n                              benchmark_token_bart_large['token_type_ids'].to(cfg.device), \n                              benchmark_token_bart_large['attention_mask'].to(cfg.device))\n\nfor fold in range(5):\n    print('*' * 50)\n    print(f'Fold: {fold}')\n    \n    # Load pretrained models\n    model_name = 'roberta_large'\n    print(f'Inference model, {model_name} version 15-0...')\n    model_roberta_large_v15_0 = Readability_Model_RoBERTa_large_v15('..\/input\/robertalarge', model_config_roberta_large, num_cat = cfg.num_bins[0], \n                                                                    benchmark_token = benchmark_token_roberta_large).to(cfg.device)\n    model_root_path = '..\/input\/clrroberta-largepretrained-modelsv15\/model_best_roberta_large_v15_0'\n    ckp = torch.load(f'{model_root_path}\/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_roberta_large_v15_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_roberta_large_v15_0 += infer(model_roberta_large_v15_0, infer_dataloader_roberta_large_v15_0, device = cfg.device, \n                                            use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_roberta_large) \/ 5\n    del model_roberta_large_v15_0; gc.collect()\n    \n    model_name = 'roberta_large'\n    print(f'Inference model, {model_name} version 15-3...')\n    model_roberta_large_v15_3 = Readability_Model_RoBERTa_large_v15('..\/input\/robertalarge', model_config_roberta_large, num_cat = cfg.num_bins[2], \n                                                                    benchmark_token = benchmark_token_roberta_large).to(cfg.device)\n    model_root_path = '..\/input\/clrroberta-largev15\/model_best_roberta_large_v15_3'\n    ckp = torch.load(f'{model_root_path}\/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_roberta_large_v15_3.load_state_dict(ckp['model_state_dict'])    \n    prediction_roberta_large_v15_3 += infer(model_roberta_large_v15_3, infer_dataloader_roberta_large_v15_3, device = cfg.device, \n                                            use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_roberta_large) \/ 5\n    del model_roberta_large_v15_3; gc.collect()\n    \n    model_name = 'roberta_large'\n    print(f'Inference model, {model_name} version 16-1...')\n    model_roberta_large_v16_1 = Readability_Model_RoBERTa_large_v16('..\/input\/robertalarge', model_config_roberta_large, num_cat = cfg.num_bins[4], \n                                                                    benchmark_token = benchmark_token_roberta_large).to(cfg.device)\n    model_root_path = '..\/input\/clrroberta-largepretrained-modelsv16\/model_best_roberta_large_v16_1'\n    ckp = torch.load(f'{model_root_path}\/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_roberta_large_v16_1.load_state_dict(ckp['model_state_dict'])\n    prediction_roberta_large_v16_1 += infer(model_roberta_large_v16_1, infer_dataloader_roberta_large_v16_1, device = cfg.device, \n                                            use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_roberta_large) \/ 5\n    del model_roberta_large_v16_1; gc.collect()\n    \n    model_name = 'gpt2_medium'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_config_gpt2_medium = AutoConfig.from_pretrained('..\/input\/gpt2-medium', output_hidden_states = True)\n    model_gpt2_medium_v1_0 = Readability_Model_GPT2_medium_v1('..\/input\/gpt2-medium', model_config_gpt2_medium, num_cat = cfg.num_bins[5], \n                                                              benchmark_token = benchmark_token_gpt2_medium).to(cfg.device)\n    model_gpt2_medium_v1_0.backbone.resize_token_embeddings(len(tokenizer_gpt2_medium))\n    model_root_path = '..\/input\/clrgpt2-mediumpretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}\/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_gpt2_medium_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_gpt2_medium_v1_0 += infer(model_gpt2_medium_v1_0, infer_dataloader_gpt2_medium_v1_0, device = cfg.device, \n                                         use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_gpt2_medium) \/ 5\n    del model_gpt2_medium_v1_0; gc.collect()\n    \n    model_name = 'xlnet_large_cased'\n    print(f'Inference model, {model_name} version 2-0...')\n    model_config_xlnet_large_cased = AutoConfig.from_pretrained('..\/input\/xlnet-large-cased', output_hidden_states = True)\n    model_xlnet_large_cased_v2_0 = Readability_Model_XLNet_large_cased_v2('..\/input\/xlnet-large-cased', model_config_xlnet_large_cased, \n                                                                          num_cat = cfg.num_bins[6], benchmark_token = benchmark_token_xlnet_large_cased).to(cfg.device)\n    model_xlnet_large_cased_v2_0.backbone.resize_token_embeddings(len(tokenizer_xlnet_large_cased))\n    model_root_path = '..\/input\/clrxlnet-largepretrained-models\/v02'\n    ckp = torch.load(f'{model_root_path}\/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_xlnet_large_cased_v2_0.load_state_dict(ckp['model_state_dict'])\n    prediction_xlnet_large_cased_v2_0 += infer(model_xlnet_large_cased_v2_0, infer_dataloader_xlnet_large_cased_v2_0, device = cfg.device, \n                                               use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_xlnet_large_cased) \/ 5\n    del model_xlnet_large_cased_v2_0; gc.collect()\n    \n    model_name = 'xlnet_large_cased'\n    print(f'Inference model, {model_name} version 3-0...')\n    model_config_xlnet_large_cased = AutoConfig.from_pretrained('..\/input\/xlnet-large-cased', output_hidden_states = True)\n    model_xlnet_large_cased_v3_0 = Readability_Model_XLNet_large_cased_v3('..\/input\/xlnet-large-cased', model_config_xlnet_large_cased, \n                                                                          num_cat = cfg.num_bins[7], benchmark_token = benchmark_token_xlnet_large_cased).to(cfg.device)\n    model_xlnet_large_cased_v3_0.backbone.resize_token_embeddings(len(tokenizer_xlnet_large_cased))\n    model_root_path = '..\/input\/clrxlnet-largepretrained-models\/v03'\n    ckp = torch.load(f'{model_root_path}\/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_xlnet_large_cased_v3_0.load_state_dict(ckp['model_state_dict'])\n    prediction_xlnet_large_cased_v3_0 += infer(model_xlnet_large_cased_v3_0, infer_dataloader_xlnet_large_cased_v3_0, device = cfg.device, \n                                               use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_xlnet_large_cased) \/ 5\n    del model_xlnet_large_cased_v3_0; gc.collect()\n    \n    model_name = 'xlnet_large_cased'\n    print(f'Inference model, {model_name} version 3-1...')\n    model_config_xlnet_large_cased = AutoConfig.from_pretrained('..\/input\/xlnet-large-cased', output_hidden_states = True)\n    model_xlnet_large_cased_v3_1 = Readability_Model_XLNet_large_cased_v3('..\/input\/xlnet-large-cased', model_config_xlnet_large_cased, \n                                                                          num_cat = cfg.num_bins[8], benchmark_token = benchmark_token_xlnet_large_cased).to(cfg.device)\n    model_xlnet_large_cased_v3_1.backbone.resize_token_embeddings(len(tokenizer_xlnet_large_cased))\n    model_root_path = '..\/input\/clrxlnet-large-casedv1'\n    ckp = torch.load(f'{model_root_path}\/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_xlnet_large_cased_v3_1.load_state_dict(ckp['model_state_dict'])\n    prediction_xlnet_large_cased_v3_1 += infer(model_xlnet_large_cased_v3_1, infer_dataloader_xlnet_large_cased_v3_1, device = cfg.device, \n                                               use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_xlnet_large_cased) \/ 5\n    del model_xlnet_large_cased_v3_1; gc.collect()\n    \n    model_name = 'electra_large_discriminator'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_electra_large_v1_0 = Readability_Model_ELECTRA_large_discriminator_v1('..\/input\/electra-large-discriminator', model_config_electra_large, \n                                                                                num_cat = cfg.num_bins[9], benchmark_token = benchmark_token_electra_large).to(cfg.device)\n    model_root_path = '..\/input\/clrelectra-largepretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}\/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_electra_large_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_electra_large_v1_0 += infer(model_electra_large_v1_0, infer_dataloader_electra_large_v1_0, device = cfg.device, \n                                           use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_electra_large) \/ 5\n    del model_electra_large_v1_0; gc.collect()\n    \n    model_name = 'electra_large_discriminator'\n    print(f'Inference model, {model_name} version 1-1...')\n    model_electra_large_v1_1 = Readability_Model_ELECTRA_large_discriminator_v1('..\/input\/electra-large-discriminator', model_config_electra_large, \n                                                                                num_cat = cfg.num_bins[10], benchmark_token = benchmark_token_electra_large).to(cfg.device)\n    model_root_path = '..\/input\/clrelectra-largev1'\n    ckp = torch.load(f'{model_root_path}\/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_electra_large_v1_1.load_state_dict(ckp['model_state_dict'])    \n    prediction_electra_large_v1_1 += infer(model_electra_large_v1_1, infer_dataloader_electra_large_v1_1, device = cfg.device, \n                                           use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_electra_large) \/ 5\n    del model_electra_large_v1_1; gc.collect()\n    \n    model_name = 'deberta_large'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_deberta_large_v1_0 = Readability_Model_DeBERTa_large_v1('..\/input\/deberta-large', model_config_debert_large, \n                                                                  num_cat = cfg.num_bins[11], benchmark_token = benchmark_token_deberta_large).to(cfg.device)\n    model_root_path = '..\/input\/clrdeberta-largepretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}\/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_deberta_large_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_deberta_large_v1_0 += infer(model_deberta_large_v1_0, infer_dataloader_deberta_large_v1_0, device = cfg.device, \n                                           use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_deberta_large) \/ 5\n    del model_deberta_large_v1_0; gc.collect()\n    \n    model_name = 'deberta_large'\n    print(f'Inference model, {model_name} version 1-1...')\n    model_deberta_large_v1_1 = Readability_Model_DeBERTa_large_v1('..\/input\/deberta-large', model_config_debert_large, \n                                                                  num_cat = cfg.num_bins[12], benchmark_token = benchmark_token_deberta_large).to(cfg.device)\n    if fold == 3:\n        model_deberta_large_v1_1 = AveragedModel(model_deberta_large_v1_1)\n        \n    model_root_path = '..\/input\/clrdeberta-largev1'\n    ckp = torch.load(f'{model_root_path}\/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_deberta_large_v1_1.load_state_dict(ckp['model_state_dict'])    \n    prediction_deberta_large_v1_1 += infer(model_deberta_large_v1_1, infer_dataloader_deberta_large_v1_1, device = cfg.device, \n                                           use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_deberta_large) \/ 5\n    del model_deberta_large_v1_1; gc.collect()\n    \n    model_name = 'funnel_large'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_funnel_large_v1_0 = Readability_Model_Funnel_large_v1('..\/input\/funnel-transformer-large', model_config_funnel_large, num_cat = cfg.num_bins[7], \n                                                                benchmark_token = benchmark_token_funnel_large).to(cfg.device)\n    model_root_path = '..\/input\/clrfunnel-largepretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}\/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_funnel_large_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_funnel_large_v1_0 += infer(model_funnel_large_v1_0, infer_dataloader_funnel_large_v1_0, device = cfg.device, \n                                          use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_funnel_large) \/ 5\n    del model_funnel_large_v1_0; gc.collect()\n    \n    model_name = 'bart_large'\n    print(f'Inference model, {model_name} version 1-0...')\n    model_bart_large_v1_0 = Readability_Model_BART_large_v1('..\/input\/bart-large', model_config_bart_large, num_cat = cfg.num_bins[8], \n                                                            benchmark_token = benchmark_token_bart_large).to(cfg.device)\n    model_root_path = '..\/input\/clrbart-largepretrained-modelsv1'\n    ckp = torch.load(f'{model_root_path}\/model_best_fold_{fold}_{model_name}.bin', map_location = cfg.device)\n    model_bart_large_v1_0.load_state_dict(ckp['model_state_dict'])    \n    prediction_bart_large_v1_0 += infer(model_bart_large_v1_0, infer_dataloader_bart_large_v1_0, device = cfg.device, \n                                        use_tqdm = cfg.use_tqdm, benchmark_token = benchmark_token_bart_large) \/ 5\n    del model_bart_large_v1_0; gc.collect()","1e4fafd9":"pred_zoo = np.vstack([prediction_roberta_large_v15_0, prediction_roberta_large_v15_3, \n                      prediction_roberta_large_v16_1, \n                      prediction_gpt2_medium_v1_0, \n                      prediction_xlnet_large_cased_v2_0, prediction_xlnet_large_cased_v3_0, prediction_xlnet_large_cased_v3_1, \n                      prediction_electra_large_v1_0, prediction_electra_large_v1_1, \n                      prediction_deberta_large_v1_0, prediction_deberta_large_v1_1, \n                      prediction_funnel_large_v1_0, \n                      prediction_bart_large_v1_0]).T\n\nss['target'] = np.mean(pred_zoo, axis = 1)","11b05955":"ENV = 'kaggle'\nassert ENV in ['colab', 'kaggle']\n \nPHASE = 'inference'\nassert PHASE in ['eval_oof','inference']","8aaf4b1d":"if ENV=='colab':\n    from google.colab import drive\n    drive.mount('\/content\/drive')","596be5ec":"import os\nimport math\nimport random\nimport time\n \nimport numpy as np\nimport pandas as pd\n \nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n \nimport transformers\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\n \nfrom sklearn import datasets\nfrom sklearn import model_selection\n\nimport gc, json, pickle, shutil\ngc.enable()\n\nfrom tqdm.auto import tqdm\nfrom matplotlib import pyplot as plt","dd2f85f4":"def create_folds(data, num_splits, shuffle=False, random_state=None):\n    # we create a new column called kfold and fill it with -1\n    data[\"kfold\"] = -1\n    \n    # the next step is to randomize the rows of the data\n    data = data.sample(frac=1, random_state=random_state).reset_index(drop=True)\n\n    # calculate number of bins by Sturge's rule\n    # I take the floor of the value, you can also\n    # just round it\n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    \n    # bin targets\n    data.loc[:, \"bins\"] = pd.cut(\n        data[\"target\"], bins=num_bins, labels=False\n    )\n    \n    # initiate the kfold class from model_selection module\n    kf = model_selection.StratifiedKFold(n_splits=num_splits, shuffle=shuffle, random_state=random_state)\n    \n    # fill the new kfold column\n    # note that, instead of targets, we use bins!\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n    \n    # drop the bins column\n    data = data.drop(\"bins\", axis=1)\n\n    # return dataframe with folds\n    return data","b950c62b":"def set_random_seed(random_seed):\n    random.seed(random_seed)\n    np.random.seed(random_seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(random_seed)\n\n    torch.manual_seed(random_seed)\n    torch.cuda.manual_seed(random_seed)\n    torch.cuda.manual_seed_all(random_seed)\n\n    torch.backends.cudnn.deterministic = True","6e964488":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.excerpt.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.target.values, dtype=torch.float32)\n            self.bins = torch.tensor(df.bins.values, dtype=torch.long)\n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = not NO_TOKEN_TYPE\n        )\n\n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        out_dict = {'input_ids':input_ids, 'attention_mask':attention_mask}\n        \n        if not NO_TOKEN_TYPE:\n            out_dict['token_type_ids'] = torch.tensor(self.encoded['token_type_ids'][index])\n        \n        if sa_complex is not None:\n            if sa_complex == 'hdd':\n                with open(f'SelfAttComplex\/{str(index).zfill(4)}.pkl','rb') as f:\n                    out_dict['sa_complex'] = pickle.load(f)\n            else:\n                out_dict['sa_complex'] = sa_complex[index]\n\n        if not self.inference_only:\n            out_dict['target'] = self.target[index]\n            out_dict['bins'] = self.bins[index]\n\n        return out_dict","65255340":"def SelfAttention_Complexity(df: pd.DataFrame, output_device):\n    pre_dataset = LitDataset(df, inference_only=True)\n    pre_loader = DataLoader(pre_dataset, batch_size=BATCH_SIZE,\n                            drop_last=False, shuffle=False)\n    \n    if output_device == 'hdd':\n        os.makedirs('SelfAttComplex', exist_ok=True)\n\n    cfg_update = {\"output_attentions\":True, \"hidden_dropout_prob\": 0.0,\n                  \"layer_norm_eps\": 1e-7}\n    if PHASE=='train':\n        config = AutoConfig.from_pretrained(MODEL_NAME)\n        config.update(cfg_update)\n        backbone = AutoModel.from_pretrained(MODEL_NAME, config=config).to(DEVICE)\n    elif PHASE=='eval_oof' or PHASE=='inference':\n        config = AutoConfig.from_pretrained(LOAD_BACKBONE_DIR)\n        config.update(cfg_update)\n        backbone = AutoModel.from_pretrained(LOAD_BACKBONE_DIR, config=config).to(DEVICE)\n\n    backbone.resize_token_embeddings(len(tokenizer))\n\n    output_sa_complex = []\n    backbone.eval()\n    idx = 0\n    with torch.no_grad():\n        for batch_num, dsargs in enumerate(tqdm(pre_loader)):\n\n            kwargs = {}\n            kwargs['input_ids'] = dsargs['input_ids'].to(DEVICE)\n            if not NO_TOKEN_TYPE:\n                kwargs['token_type_ids'] = dsargs['token_type_ids'].to(DEVICE)\n            kwargs['attention_mask'] = dsargs['attention_mask'].to(DEVICE)\n\n            if 't5' in MODEL_NAME.lower() and HAS_DECODER:\n                # shift to right\n                kwargs['decoder_input_ids'] = torch.cat([tokenizer.pad_token_id * torch.ones(kwargs['input_ids'].size(0), 1).long().to(DEVICE),\n                                                        kwargs['input_ids'][:,:-1]], dim=1)\n            \n            # self attention\n            output_backbone = backbone(**kwargs)\n            self_att = torch.stack(output_backbone.attentions, dim=1) #[batch, layer, head, seq, seq]\n            seq_len = self_att.size(-1)\n            self_att = self_att.view(self_att.size(0), -1, seq_len, seq_len) #[batch, layer*head, seq, seq]\n            self_att *= kwargs['attention_mask'].unsqueeze(1).unsqueeze(-1)\n\n            # self attention complexity\n            distance_from_diag = (torch.arange(seq_len).view(1, -1) - torch.arange(seq_len).view(-1, 1)) \/ (seq_len - 1)\n            distance_from_diag = distance_from_diag.to(DEVICE)\n            sa_complex = []\n            temp = self_att * distance_from_diag.unsqueeze(0).unsqueeze(1).clip(min=0)\n            temp = temp.sum(dim=-1) #[batch, layer*head, seq]\n            sa_complex.append(temp)\n            temp = self_att * distance_from_diag.unsqueeze(0).unsqueeze(1).clip(max=0).abs()\n            temp = temp.sum(dim=-1) #[batch, layer*head, seq]\n            sa_complex.append(temp)\n            sa_complex = torch.cat(sa_complex, dim=1).transpose(-2,-1) #[batch, seq, layer*head*2]\n\n            if output_device == 'hdd':\n                for batch_item in sa_complex:\n                    with open(f'SelfAttComplex\/{str(idx).zfill(4)}.pkl','wb') as f:\n                        pickle.dump(batch_item, f)\n                    idx += 1\n            else:\n                output_sa_complex.append(sa_complex)\n    \n    if output_device == 'hdd':\n        return 'hdd'\n    else:\n        output_sa_complex = torch.cat(output_sa_complex, dim=0)\n        return output_sa_complex.to(output_device)","bfc16b6d":"class LitModel(nn.Module):\n    def __init__(self, benchmark_token=None, use_max_pooling=False, sa_complex_dim=0):\n        super().__init__()\n \n        self.benchmark_token = benchmark_token\n        self.use_max_pooling = use_max_pooling\n        self.sa_complex_dim = sa_complex_dim\n        \n        cfg_update = {\"output_hidden_states\":True, \"hidden_dropout_prob\": 0.0,\n                      \"layer_norm_eps\": 1e-7}\n        if PHASE=='train':\n            config = AutoConfig.from_pretrained(MODEL_NAME)\n            config.save_pretrained(f'{SAVE_DIR}\/backbone')\n            config.update(cfg_update)                       \n            self.backbone = AutoModel.from_pretrained(MODEL_NAME, config=config)\n            self.backbone.save_pretrained(f'{SAVE_DIR}\/backbone')\n        elif PHASE=='eval_oof' or PHASE=='inference':\n            config = AutoConfig.from_pretrained(LOAD_BACKBONE_DIR)\n            config.update(cfg_update)                       \n            self.backbone = AutoModel.from_pretrained(LOAD_BACKBONE_DIR, config=config)\n            \n        self.hidden_layer_weights = nn.Parameter(torch.zeros(NUM_HIDDEN_LAYERS).view(-1, 1, 1, 1))\n \n        # Dropout layers\n        self.dropouts_regr = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n        self.dropouts_clsi = nn.ModuleList([\n            nn.Dropout(0.5) for _ in range(5)\n        ])\n \n        if self.use_max_pooling:\n            num_pool = 2\n        else:\n            num_pool = 1\n        self.attention_layer_norm = nn.LayerNorm(HIDDEN_SIZE * num_pool + sa_complex_dim)\n        self.attention = nn.Sequential(            \n            nn.Linear(HIDDEN_SIZE * num_pool + sa_complex_dim, 512 * num_pool),            \n            nn.Tanh(),                       \n            nn.Linear(512 * num_pool, 1),\n            nn.Softmax(dim=1)\n            )        \n        self.head_regressor = nn.Linear(HIDDEN_SIZE * num_pool + sa_complex_dim, 1)\n        self.head_classifier = nn.Linear(HIDDEN_SIZE * num_pool + sa_complex_dim, NUM_BINS)                   \n \n    def forward(self, input_ids, token_type_ids, attention_mask, self_att_complex):\n\n        kwargs = {}\n        if self.benchmark_token is None:\n            kwargs['input_ids'] = input_ids\n            if not NO_TOKEN_TYPE:\n                kwargs['token_type_ids'] = token_type_ids\n            kwargs['attention_mask'] = attention_mask\n        else:\n            benchmark_input_ids, benchmark_token_type_ids, benchmark_attention_mask = self.benchmark_token\n            kwargs['input_ids'] = torch.cat((input_ids, benchmark_input_ids), dim = 0)\n            if not NO_TOKEN_TYPE:\n                kwargs['token_type_ids'] = torch.cat((token_type_ids, benchmark_token_type_ids), dim = 0)\n            kwargs['attention_mask'] = torch.cat((attention_mask, benchmark_attention_mask), dim = 0)\n\n        if 't5' in MODEL_NAME.lower() and HAS_DECODER:\n            # shift to right\n            kwargs['decoder_input_ids'] = torch.cat([tokenizer.pad_token_id * torch.ones(kwargs['input_ids'].size(0), 1).long().to(DEVICE),\n                                                     kwargs['input_ids'][:,:-1]], dim=1)\n        output_backbone = self.backbone(**kwargs)\n        \n        # Extract output\n        if HAS_DECODER:\n            hidden_states = output_backbone.encoder_hidden_states + output_backbone.decoder_hidden_states[1:]\n        else:\n            hidden_states = output_backbone.hidden_states\n \n        # Mean\/max pooling (over hidden layers), concatenate with pooler\n        hidden_states = torch.stack(tuple(hidden_states[-i-1] for i in range(len(hidden_states) - 1)), dim = 0)\n        layer_weight = nn.functional.softmax(self.hidden_layer_weights, dim = 0)\n        output_backbone = torch.sum(hidden_states * layer_weight, dim = 0)\n        if self.use_max_pooling:\n            out_max, _ = torch.max(hidden_states, dim = 0)\n            output_backbone = torch.cat((output_backbone, out_max), dim = -1)\n        if self.sa_complex_dim != 0:\n            self_att_complex = torch.cat((self_att_complex, benchmark_sa_complex), dim = 0)\n            output_backbone = torch.cat((output_backbone, self_att_complex), dim = -1)\n        \n        output_backbone = self.attention_layer_norm(output_backbone)\n \n        # Attention Pooling\n        weights = self.attention(output_backbone)\n        context_vector = torch.sum(weights * output_backbone, dim=1)        \n \n        # Multiple dropout\n        for i, dropout in enumerate(self.dropouts_regr):\n            if i == 0:\n                output_regr = self.head_regressor(dropout(context_vector))\n                output_clsi = self.head_classifier(self.dropouts_clsi[i](context_vector))\n            else:\n                output_regr += self.head_regressor(dropout(context_vector))\n                output_clsi += self.head_classifier(self.dropouts_clsi[i](context_vector))\n \n        output_regr \/= len(self.dropouts_regr)\n        output_clsi \/= len(self.dropouts_clsi)\n\n        if self.benchmark_token is not None:\n            output_regr = output_regr[:-1] - output_regr[-1]\n            output_clsi = output_clsi[:-1]\n\n        # Now we reduce the context vector to the prediction score.\n        return output_regr, nn.functional.softmax(output_clsi, dim=-1)","e0b0be79":"class QuadraticWeightedKappaLoss(nn.Module):\n    def __init__(self, num_cat, device = 'cpu'):\n        super(QuadraticWeightedKappaLoss, self).__init__()\n        self.num_cat = num_cat\n        cats = torch.arange(num_cat).to(device)\n        self.weights = (cats.view(-1,1) - cats.view(1,-1)).pow(2) \/ (num_cat - 1)**2\n        \n    def _confusion_matrix(self, pred_smax, true_cat):\n        confusion_matrix = torch.zeros((self.num_cat, self.num_cat)).to(pred_smax.device)\n        for t, p in zip(true_cat.view(-1), pred_smax):\n            confusion_matrix[t.long()] += p\n        return confusion_matrix\n        \n    def forward(self, pred_smax, true_cat):\n        # Confusion matrix\n        O = self._confusion_matrix(pred_smax, true_cat)\n        \n        # Count elements in each category\n        true_hist = torch.bincount(true_cat, minlength = self.num_cat)\n        pred_hist = pred_smax.sum(dim = 0)\n        \n        # Expected values\n        E = torch.outer(true_hist, pred_hist)\n        \n        # Normlization\n        O = O \/ torch.sum(O)\n        E = E \/ torch.sum(E)\n        \n        # Weighted Kappa\n        numerator = torch.sum(self.weights * O)\n        denominator = torch.sum(self.weights * E)\n        \n        return COEF_QWK * numerator \/ denominator\n    \nclass BradleyTerryLoss(nn.Module):\n    def __init__(self):\n        super(BradleyTerryLoss, self).__init__()\n\n    def forward(self, pred_mean, true_mean):\n        batch_size = len(pred_mean)\n        true_comparison = true_mean.view(-1,1) - true_mean.view(1,-1)\n        pred_comparison = pred_mean.view(-1,1) - pred_mean.view(1,-1)\n        \n        return COEF_BT * (torch.log(1 + torch.tril(torch.exp(-true_comparison * pred_comparison))).sum()\n                          \/ (batch_size * (batch_size - 1) \/ 2))","07b0764a":"def eval_mse(model, data_loader):\n    \"\"\"Evaluates the mean squared error of the |model| on |data_loader|\"\"\"\n    model.eval()            \n    mse_sum = 0\n\n    all_pred_r = []\n    with torch.no_grad():\n        for batch_num, dsargs in enumerate(data_loader):\n            input_ids = dsargs['input_ids'].to(DEVICE)\n            attention_mask = dsargs['attention_mask'].to(DEVICE)\n            target = dsargs['target'].to(DEVICE)\n            bins = dsargs['bins'].to(DEVICE)\n\n            token_type_ids = None\n            if not NO_TOKEN_TYPE:\n                token_type_ids = dsargs['token_type_ids'].to(DEVICE)\n\n            self_att_complex = None\n            if USE_SELF_ATT:\n                self_att_complex = dsargs['sa_complex'].to(DEVICE)\n\n            pred_r, _ = model(input_ids, token_type_ids, attention_mask, self_att_complex)                       \n\n            mse_sum += nn.MSELoss(reduction=\"sum\")(pred_r.flatten(), target).item()\n            all_pred_r.append(pred_r)\n\n    return mse_sum \/ len(data_loader.dataset), torch.cat(all_pred_r, dim=0).squeeze()","8c639b51":"def train(model, model_path, train_loader, val_loader,\n          optimizer, num_epochs, fold, scheduler=None):    \n    best_val_rmse = None\n    best_epoch = 0\n    step = 0\n    last_eval_step = 0\n    eval_period = EVAL_SCHEDULE[0][1]    \n\n    start = time.time()\n\n    history = {'step':[], 'epoch':[], 'batch_num':[], 'val_rmse':[],\n               'trn_rmse':[], 'trn_qwk':[], 'trn_bt':[]}\n    \n    for epoch in range(num_epochs):\n        val_rmse = None         \n\n        epoch_target, epoch_bins, epoch_pred_r, epoch_pred_c = (torch.tensor([]),)*4\n        epoch_bins = epoch_bins.long()\n    \n        for batch_num, dsargs in enumerate(train_loader):\n            input_ids = dsargs['input_ids'].to(DEVICE)\n            attention_mask = dsargs['attention_mask'].to(DEVICE)\n            target = dsargs['target'].to(DEVICE)\n            bins = dsargs['bins'].to(DEVICE)\n\n            token_type_ids = None\n            if not NO_TOKEN_TYPE:\n                token_type_ids = dsargs['token_type_ids'].to(DEVICE)\n\n            self_att_complex = None\n            if USE_SELF_ATT:\n                self_att_complex = dsargs['sa_complex'].to(DEVICE)\n\n            optimizer.zero_grad()\n            \n            model.train()\n\n            pred_r, pred_c = model(input_ids, token_type_ids, attention_mask, self_att_complex)\n                                                        \n            loss = (nn.MSELoss(reduction=\"mean\")(pred_r.flatten(), target)\n                    + QWKloss(pred_c, bins) + BTloss(pred_r.flatten(), target))\n                        \n            loss.backward()\n            \n            epoch_target = torch.cat([epoch_target.to(DEVICE), target.clone().detach()], dim=0)\n            epoch_bins = torch.cat([epoch_bins.to(DEVICE), bins.clone().detach()], dim=0)\n            epoch_pred_r = torch.cat([epoch_pred_r.to(DEVICE), pred_r.clone().detach()], dim=0)\n            epoch_pred_c = torch.cat([epoch_pred_c.to(DEVICE), pred_c.clone().detach()], dim=0)\n\n            optimizer.step()\n            if scheduler:\n                scheduler.step()\n            \n            if step >= last_eval_step + eval_period:\n                # Evaluate the model on val_loader.\n                elapsed_seconds = time.time() - start\n                num_steps = step - last_eval_step\n                print(f\"\\n{num_steps} steps took {elapsed_seconds:0.3} seconds\")\n                last_eval_step = step\n                \n                mse, _ = eval_mse(model, val_loader)\n                val_rmse = math.sqrt(mse)\n                trn_rmse = nn.MSELoss(reduction=\"mean\")(epoch_pred_r.flatten(), epoch_target).item()\n                trn_qwk  = QWKloss(epoch_pred_c, epoch_bins).item()\n                trn_bt  = BTloss(epoch_pred_r.flatten(), epoch_target).item()\n\n                print(f\"Epoch: {epoch} batch_num: {batch_num}\", \n                      f\"val_rmse: {val_rmse:0.4}\", f\"train_rmse: {trn_rmse:0.4}\",\n                      f\"train_qwk: {trn_qwk:0.4}\", f\"train_bt: {trn_bt:0.4}\")\n\n                for rmse, period in EVAL_SCHEDULE:\n                    if val_rmse >= rmse:\n                        eval_period = period\n                        break\n                percent = step \/ (num_epochs * len(train_loader))\n                if 0.5 <= percent and percent <= 0.8:\n                    eval_period = min([eval_period, 8])\n                \n                if not best_val_rmse or val_rmse < best_val_rmse:                    \n                    best_val_rmse = val_rmse\n                    best_epoch = epoch\n                    torch.save(model.state_dict(), model_path)\n                    print(f\"New best_val_rmse: {best_val_rmse:0.4}\")\n                else:       \n                    print(f\"Still best_val_rmse: {best_val_rmse:0.4}\",\n                          f\"(from epoch {best_epoch})\")\n\n                ''' history json dump '''\n                history['step'].append(step)\n                history['epoch'].append(epoch)\n                history['batch_num'].append(batch_num)\n                history['val_rmse'].append(val_rmse)\n                history['trn_rmse'].append(trn_rmse)\n                history['trn_qwk'].append(trn_qwk)\n                history['trn_bt'].append(trn_bt)\n                with open(f'{SAVE_DIR}\/{MODEL_VER}_fold{fold+1}_history.json', 'w') as f:\n                    json.dump(history, f, indent=4)\n                    \n                start = time.time()\n                                            \n            step += 1\n\n        del epoch_target, epoch_bins, epoch_pred_r, epoch_pred_c\n        \n        print('\\nHidden Layer Weights:')\n        print(model.hidden_layer_weights.squeeze())\n        print(nn.functional.softmax(model.hidden_layer_weights.squeeze(),dim=0))\n    \n    return best_val_rmse","886e741c":"def predict(model, data_loader):\n    \"\"\"Returns an np.array with predictions of the |model| on |data_loader|\"\"\"\n    model.eval()\n\n    result = np.zeros(len(data_loader.dataset))    \n    index = 0\n    \n    with torch.no_grad():\n        for batch_num, dsargs in enumerate(data_loader):\n            input_ids = dsargs['input_ids'].to(DEVICE)\n            attention_mask = dsargs['attention_mask'].to(DEVICE)\n\n            token_type_ids = None\n            if not NO_TOKEN_TYPE:\n                token_type_ids = dsargs['token_type_ids'].to(DEVICE)\n\n            self_att_complex = None\n            if USE_SELF_ATT:\n                self_att_complex = dsargs['sa_complex'].to(DEVICE)\n                        \n            pred_r, _ = model(input_ids, token_type_ids, attention_mask, self_att_complex)                        \n\n            result[index : index + pred_r.shape[0]] = pred_r.flatten().to(\"cpu\")\n            index += pred_r.shape[0]\n\n    return result","a7b10365":"def create_optimizer(model):\n    named_parameters = list(model.named_parameters())\n    \n    backbone_parameters = [(n, p) for n, p in named_parameters if n.startswith('backbone')]\n    attention_parameters = [(n, p) for n, p in named_parameters if n.startswith('attention')]\n    hidden_wts_parameters = [(n, p) for n, p in named_parameters if n.startswith ('hidden_layer_weights')]\n    head_parameters = [(n, p) for n, p in named_parameters if n.startswith('head')]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    hidden_wts_group = [params for (name, params) in hidden_wts_parameters]\n    head_group = [params for (name, params) in head_parameters]\n \n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": hidden_wts_group, 'weight_decay': 0.0, 'lr': HIDDEN_WTS_LR})\n    parameters.append({\"params\": head_group})\n \n    no_decay = ['bias', 'LayerNorm.weight', 'layer_norm']\n \n    if 'roberta' in MODEL_NAME.lower() or 'electra' in MODEL_NAME.lower():\n        layers = [getattr(model, 'backbone').embeddings] + list(getattr(model, 'backbone').encoder.layer)\n    elif 'gpt2' in MODEL_NAME.lower():\n        layers = [getattr(model, 'backbone').wte] + list(getattr(model, 'backbone').h)\n    elif 'xlnet' in MODEL_NAME.lower():\n        layers = [getattr(model, 'backbone').word_embedding] + list(getattr(model, 'backbone').layer)\n    elif 'bart' in MODEL_NAME.lower():\n        enc_layers = ([getattr(model, 'backbone').encoder.embed_positions] +\n                      list(getattr(model, 'backbone').encoder.layers) +\n                      [getattr(model, 'backbone').encoder.layernorm_embedding])\n        dec_layers = ([getattr(model, 'backbone').decoder.embed_positions] +\n                      list(getattr(model, 'backbone').decoder.layers) + \n                      [getattr(model, 'backbone').decoder.layernorm_embedding])\n        assert len(enc_layers)==len(dec_layers)\n        layers = [getattr(model, 'backbone').shared]\n        for e, d in zip(enc_layers, dec_layers):\n            layers += [e, d]\n    elif 't5' in MODEL_NAME.lower():\n        enc_layers = (list(getattr(model, 'backbone').encoder.block) +\n                      [getattr(model, 'backbone').encoder.final_layer_norm])\n        dec_layers = (list(getattr(model, 'backbone').decoder.block) + \n                      [getattr(model, 'backbone').decoder.final_layer_norm])\n        assert len(enc_layers)==len(dec_layers)\n        layers = [getattr(model, 'backbone').shared]\n        for e, d in zip(enc_layers, dec_layers):\n            layers += [e, d]\n    else:\n        raise RuntimeError('specify the parameters for backbone.')\n \n    layers.reverse()\n    layerwise_learning_rate_decay = LAYERWISE_LR_DECAY**(1.0\/len(layers))\n    lr = BACKBONE_LR\n    for i, layer in enumerate(layers):\n        lr *= layerwise_learning_rate_decay\n        parameters += [\n            {\n                'params': [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n                'weight_decay': 0.01,\n                'lr': lr,\n            },\n            {\n                'params': [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n                'weight_decay': 0.0,\n                'lr': lr,\n            },\n        ]\n \n    return AdamW(parameters)","368d787e":"def convert_examples_to_features(text, tokenizer, max_len, is_test = False, return_tensor = False):\n    # Take from https:\/\/www.kaggle.com\/rhtsingh\/commonlit-readability-prize-roberta-torch-fit\n    text = text.replace('\\n', '')\n    if return_tensor:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            return_tensors = 'pt',\n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = not NO_TOKEN_TYPE\n        )\n    else:\n        tok = tokenizer.encode_plus(\n            text, \n            max_length = max_len, \n            padding = 'max_length', \n            truncation = True,\n            return_attention_mask = True,\n            return_token_type_ids = not NO_TOKEN_TYPE\n        )\n    return tok","54f97133":"def Train_or_Validation():\n    list_val_rmse = []\n \n    oof = []\n    for fold in range(NUM_FOLDS):\n        print(f\"\\nFold {fold + 1}\/{NUM_FOLDS}\")\n            \n        set_random_seed(SEED + fold)\n        \n        train_dataset = LitDataset(train_df[train_df['kfold'] != fold])\n        val_dataset = LitDataset(train_df[train_df['kfold'] == fold])\n        val_df = train_df[train_df['kfold'] == fold].copy()\n            \n        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                                  drop_last=True, shuffle=True, num_workers=0)    \n        val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE,\n                                drop_last=False, shuffle=False, num_workers=0)    \n        \n        sa_complex_dim = 0\n        if USE_SELF_ATT:\n            sa_complex_dim = benchmark_sa_complex.size(-1)\n        \n        model = LitModel(benchmark_token = benchmark_token, use_max_pooling = USE_MAX_POOLING,\n                         sa_complex_dim = sa_complex_dim).to(DEVICE)\n        \n        # Update vocabulary size\n        model.backbone.resize_token_embeddings(len(tokenizer))\n \n        if PHASE=='train':\n            model_path = f\"{SAVE_DIR}\/model_{fold + 1}.bin\"\n            set_random_seed(SEED + fold)    \n \n            optimizer = create_optimizer(model)                        \n            scheduler = get_cosine_schedule_with_warmup(\n                optimizer,\n                num_training_steps = NUM_EPOCHS * len(train_loader) * 11\/\/10,\n                num_warmup_steps = 50)\n            \n            list_val_rmse.append(train(model, model_path, train_loader, val_loader, optimizer, \n                                       num_epochs=NUM_EPOCHS, fold=fold, scheduler=scheduler, ))\n        \n        elif PHASE=='eval_oof':\n            model_path = f\"{MODEL_DIR}\/model_{fold + 1}.bin\"\n            model.load_state_dict(torch.load(model_path))\n            model.to(DEVICE)\n            \n            mse, pred_r = eval_mse(model, val_loader)\n            val_df['pred'] = pred_r.to('cpu').detach().numpy().copy()\n            oof.append(val_df)\n            list_val_rmse.append(math.sqrt(mse))\n \n        del model\n        gc.collect()\n        \n        print(\"\\nPerformance estimates:\")\n        print(list_val_rmse)\n        print(\"Mean:\", np.array(list_val_rmse).mean())\n\n    if PHASE=='eval_oof':\n        oof = pd.concat(oof).set_index('id').sort_index()\n\n    return oof","3fef509c":"def Inference():\n    all_predictions = np.zeros((NUM_FOLDS, len(test_df)))\n\n    test_dataset = LitDataset(test_df, inference_only=True)\n    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n                             drop_last=False, shuffle=False, num_workers=0)\n\n    for fold in range(NUM_FOLDS):            \n\n        sa_complex_dim = 0\n        if USE_SELF_ATT:\n            sa_complex_dim = benchmark_sa_complex.size(-1)\n\n        model = LitModel(benchmark_token = benchmark_token, use_max_pooling = USE_MAX_POOLING,\n                         sa_complex_dim = sa_complex_dim).to(DEVICE)\n\n        # Update vocabulary size\n        model.backbone.resize_token_embeddings(len(tokenizer))\n\n        model_path = f\"{MODEL_DIR}\/model_{fold + 1}.bin\"\n        print(f\"\\nUsing {model_path}\")\n                            \n        model.load_state_dict(torch.load(model_path))    \n        \n        all_predictions[fold] = predict(model, test_loader)\n        \n        del model\n        gc.collect()\n\n    predictions = all_predictions.mean(axis=0)\n    output_df = submission_df.copy()\n    output_df.target = predictions\n    print(output_df)\n\n    return output_df","af7ca34a":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif ENV=='colab':\n    BASE_DIR = '\/content\/drive\/MyDrive\/Colab Notebooks\/CLR\/input'\n    TRAIN_DATA_DIR = BASE_DIR\nelif ENV=='kaggle':\n    BASE_DIR = '..\/input\/commonlitreadabilityprize'\n    TRAIN_DATA_DIR = '..\/input\/step-1-create-folds'\n\ntrain_df = pd.read_csv(f'{TRAIN_DATA_DIR}\/train_folds.csv')\nbenchmark = train_df[(train_df.target == 0) & (train_df.standard_error == 0)].copy()\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(f\"{BASE_DIR}\/test.csv\")\nsubmission_df = pd.read_csv(f\"{BASE_DIR}\/sample_submission.csv\")","7ee9014f":"SEED = 1000\nNUM_FOLDS = 5\nNUM_EPOCHS = 4\nBATCH_SIZE = 8\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.52, 32), (0.49, 16), (0.48, 8), (0.47, 4), (-1., 2)]\nMODEL_NAME = 'roberta-large'\nMODEL_VER = 'CLRP_LightBase_031_RoBERTaL'\n \nNUM_HIDDEN_LAYERS = 24\nHIDDEN_SIZE = 1024\nNUM_BINS = 29\nCOEF_QWK = 0.0 # coefficient of QWK loss\nCOEF_BT = 1.0 # coefficient of Bradley-Terry loss\n\nUSE_MAX_POOLING = True\nUSE_SELF_ATT = True\nNO_TOKEN_TYPE = False\nHAS_DECODER = False\n\nBACKBONE_LR = 2e-5\nHIDDEN_WTS_LR = 1e-2\nLAYERWISE_LR_DECAY = 0.1\n\nif ENV=='colab':\n    MODEL_DIR = f'\/content\/drive\/MyDrive\/Colab Notebooks\/CLR\/{MODEL_VER}'\n    SAVE_DIR = MODEL_DIR\n    LOAD_BACKBONE_DIR = f'{MODEL_DIR}\/backbone'\nelif ENV=='kaggle':\n    MODEL_DIR = '..\/input\/clrp-lightbase-031-robertal-dat'\n    SAVE_DIR = '.'\n    LOAD_BACKBONE_DIR = '..\/input\/robertalarge'\n\nQWKloss = QuadraticWeightedKappaLoss(num_cat=NUM_BINS, device=DEVICE)\nBTloss = BradleyTerryLoss()\ntrain_df['bins'] = pd.cut(train_df['target'], bins=NUM_BINS, labels=False)\n\n# Setup Tokenizer\nif PHASE=='train':\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained(f'{SAVE_DIR}\/backbone')\nelif PHASE=='eval_oof' or PHASE=='inference':\n    tokenizer = AutoTokenizer.from_pretrained(LOAD_BACKBONE_DIR)\nif 'gpt2' in MODEL_NAME.lower():\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Tokenize the benchmark text\nbenchmark_token = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer, MAX_LEN, return_tensor = True)\nif NO_TOKEN_TYPE:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), None, benchmark_token['attention_mask'].to(DEVICE))\nelse:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), benchmark_token['token_type_ids'].to(DEVICE), benchmark_token['attention_mask'].to(DEVICE))\n\n# Main\nif PHASE=='train' or PHASE=='eval_oof':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(train_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    oof_df_RoBERTaL031 = Train_or_Validation()\n\nif PHASE=='eval_oof':\n    oof_df_RoBERTaL031.to_csv(f'oof_{MODEL_VER}.csv')\n\nif PHASE=='inference':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(test_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    submission_df_RoBERTaL031 = Inference()\n    submission_df_RoBERTaL031.to_csv(f\"submission_{MODEL_VER}.csv\", index=False)\n\nif os.path.isdir('SelfAttComplex'):\n    shutil.rmtree('SelfAttComplex')","62d0c98f":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif ENV=='colab':\n    BASE_DIR = '\/content\/drive\/MyDrive\/Colab Notebooks\/CLR\/input'\n    TRAIN_DATA_DIR = BASE_DIR\nelif ENV=='kaggle':\n    BASE_DIR = '..\/input\/commonlitreadabilityprize'\n    TRAIN_DATA_DIR = '..\/input\/step-1-create-folds'\n\ntrain_df = pd.read_csv(f'{TRAIN_DATA_DIR}\/train_folds.csv')\nbenchmark = train_df[(train_df.target == 0) & (train_df.standard_error == 0)].copy()\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(f\"{BASE_DIR}\/test.csv\")\nsubmission_df = pd.read_csv(f\"{BASE_DIR}\/sample_submission.csv\")","91a4fe86":"SEED = 2319\nNUM_FOLDS = 5\nNUM_EPOCHS = 4\nBATCH_SIZE = 8\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.52, 32), (0.49, 16), (0.48, 8), (0.47, 4), (-1., 2)]\nMODEL_NAME = 'roberta-large'\nMODEL_VER = 'CLRP_LightBase_031b_RoBERTaL'\n \nNUM_HIDDEN_LAYERS = 24\nHIDDEN_SIZE = 1024\nNUM_BINS = 29\nCOEF_QWK = 0.0 # coefficient of QWK loss\nCOEF_BT = 1.0 # coefficient of Bradley-Terry loss\n\nUSE_MAX_POOLING = True\nUSE_SELF_ATT = True\nNO_TOKEN_TYPE = False\nHAS_DECODER = False\n\nBACKBONE_LR = 2e-5\nHIDDEN_WTS_LR = 1e-2\nLAYERWISE_LR_DECAY = 0.1\n\nif ENV=='colab':\n    MODEL_DIR = f'\/content\/drive\/MyDrive\/Colab Notebooks\/CLR\/{MODEL_VER}'\n    SAVE_DIR = MODEL_DIR\n    LOAD_BACKBONE_DIR = f'{MODEL_DIR}\/backbone'\nelif ENV=='kaggle':\n    MODEL_DIR = '..\/input\/clrp-lightbase-031b-robertal-dat'\n    SAVE_DIR = '.'\n    LOAD_BACKBONE_DIR = '..\/input\/robertalarge'\n\nQWKloss = QuadraticWeightedKappaLoss(num_cat=NUM_BINS, device=DEVICE)\nBTloss = BradleyTerryLoss()\ntrain_df['bins'] = pd.cut(train_df['target'], bins=NUM_BINS, labels=False)\n\n# Setup Tokenizer\nif PHASE=='train':\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained(f'{SAVE_DIR}\/backbone')\nelif PHASE=='eval_oof' or PHASE=='inference':\n    tokenizer = AutoTokenizer.from_pretrained(LOAD_BACKBONE_DIR)\nif 'gpt2' in MODEL_NAME.lower():\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Tokenize the benchmark text\nbenchmark_token = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer, MAX_LEN, return_tensor = True)\nif NO_TOKEN_TYPE:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), None, benchmark_token['attention_mask'].to(DEVICE))\nelse:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), benchmark_token['token_type_ids'].to(DEVICE), benchmark_token['attention_mask'].to(DEVICE))\n\n# Main\nif PHASE=='train' or PHASE=='eval_oof':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(train_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    oof_df_RoBERTaL031b = Train_or_Validation()\n\nif PHASE=='eval_oof':\n    oof_df_RoBERTaL031b.to_csv(f'oof_{MODEL_VER}.csv')\n\nif PHASE=='inference':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(test_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    submission_df_RoBERTaL031b = Inference()\n    submission_df_RoBERTaL031b.to_csv(f\"submission_{MODEL_VER}.csv\", index=False)\n\nif os.path.isdir('SelfAttComplex'):\n    shutil.rmtree('SelfAttComplex')","dba6f471":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif ENV=='colab':\n    BASE_DIR = '\/content\/drive\/MyDrive\/Colab Notebooks\/CLR\/input'\n    TRAIN_DATA_DIR = BASE_DIR\nelif ENV=='kaggle':\n    BASE_DIR = '..\/input\/commonlitreadabilityprize'\n    TRAIN_DATA_DIR = BASE_DIR\n\n# train_df = pd.read_csv(f'{TRAIN_DATA_DIR}\/train_folds.csv')\ntrain_df = pd.read_csv(f'{TRAIN_DATA_DIR}\/train.csv')\ntrain_df = create_folds(train_df, num_splits=5, shuffle=True, random_state=1605)\n\nbenchmark = train_df[(train_df.target == 0) & (train_df.standard_error == 0)].copy()\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(f\"{BASE_DIR}\/test.csv\")\nsubmission_df = pd.read_csv(f\"{BASE_DIR}\/sample_submission.csv\")","68837e34":"SEED = 1605\nNUM_FOLDS = 5\nNUM_EPOCHS = 4\nBATCH_SIZE = 8\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.52, 32), (0.49, 16), (0.48, 8), (0.47, 4), (-1., 2)]\nMODEL_NAME = 'roberta-large'\nMODEL_VER = 'CLRP_LightBase_kfoldv2_026a_RoBERTaL'\n \nNUM_HIDDEN_LAYERS = 24\nHIDDEN_SIZE = 1024\nNUM_BINS = 29\nCOEF_QWK = 0.0 # coefficient of QWK loss\nCOEF_BT = 1.0 # coefficient of Bradley-Terry loss\n\nUSE_MAX_POOLING = True\nUSE_SELF_ATT = False\nNO_TOKEN_TYPE = False\nHAS_DECODER = False\n\nBACKBONE_LR = 2e-5\nHIDDEN_WTS_LR = 1e-2\nLAYERWISE_LR_DECAY = 0.1\n\nif ENV=='colab':\n    MODEL_DIR = f'\/content\/drive\/MyDrive\/Colab Notebooks\/CLR\/{MODEL_VER}'\n    SAVE_DIR = MODEL_DIR\n    LOAD_BACKBONE_DIR = f'{MODEL_DIR}\/backbone'\nelif ENV=='kaggle':\n    MODEL_DIR = '..\/input\/clrp-lightbase-kfoldv2-026a-robertal-dat'\n    SAVE_DIR = '.'\n    LOAD_BACKBONE_DIR = '..\/input\/robertalarge'\n\nQWKloss = QuadraticWeightedKappaLoss(num_cat=NUM_BINS, device=DEVICE)\nBTloss = BradleyTerryLoss()\ntrain_df['bins'] = pd.cut(train_df['target'], bins=NUM_BINS, labels=False)\n\n# Setup Tokenizer\nif PHASE=='train':\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained(f'{SAVE_DIR}\/backbone')\nelif PHASE=='eval_oof' or PHASE=='inference':\n    tokenizer = AutoTokenizer.from_pretrained(LOAD_BACKBONE_DIR)\nif 'gpt2' in MODEL_NAME.lower():\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Tokenize the benchmark text\nbenchmark_token = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer, MAX_LEN, return_tensor = True)\nif NO_TOKEN_TYPE:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), None, benchmark_token['attention_mask'].to(DEVICE))\nelse:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), benchmark_token['token_type_ids'].to(DEVICE), benchmark_token['attention_mask'].to(DEVICE))\n\n# Main\nif PHASE=='train' or PHASE=='eval_oof':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(train_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    oof_df_kfv2_RoBERTaL026a = Train_or_Validation()\n\nif PHASE=='eval_oof':\n    oof_df_kfv2_RoBERTaL026a.to_csv(f'oof_{MODEL_VER}.csv')\n\nif PHASE=='inference':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(test_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    submission_df_kfv2_RoBERTaL026a = Inference()\n    submission_df_kfv2_RoBERTaL026a.to_csv(f\"submission_{MODEL_VER}.csv\", index=False)\n\nif os.path.isdir('SelfAttComplex'):\n    shutil.rmtree('SelfAttComplex')","c4d3a9d5":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif ENV=='colab':\n    BASE_DIR = '\/content\/drive\/MyDrive\/Colab Notebooks\/CLR\/input'\n    TRAIN_DATA_DIR = BASE_DIR\nelif ENV=='kaggle':\n    BASE_DIR = '..\/input\/commonlitreadabilityprize'\n    TRAIN_DATA_DIR = '..\/input\/step-1-create-folds'\n\ntrain_df = pd.read_csv(f'{TRAIN_DATA_DIR}\/train_folds.csv')\nbenchmark = train_df[(train_df.target == 0) & (train_df.standard_error == 0)].copy()\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(f\"{BASE_DIR}\/test.csv\")\nsubmission_df = pd.read_csv(f\"{BASE_DIR}\/sample_submission.csv\")","f9ee08e8":"SEED = 1605\nNUM_FOLDS = 5\nNUM_EPOCHS = 4\nBATCH_SIZE = 8\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.52, 32), (0.49, 16), (0.48, 8), (0.47, 4), (-1., 2)]\nMODEL_NAME = 'google\/electra-large-discriminator'\nMODEL_VER = 'CLRP_LightBase_004_ElectraL'\n \nNUM_HIDDEN_LAYERS = 24\nHIDDEN_SIZE = 1024\nNUM_BINS = 29\nCOEF_QWK = 0.0 # coefficient of QWK loss\nCOEF_BT = 1.0 # coefficient of Bradley-Terry loss\n\nUSE_MAX_POOLING = False\nUSE_SELF_ATT = False\nNO_TOKEN_TYPE = False\nHAS_DECODER = False\n\nBACKBONE_LR = 2e-5\nHIDDEN_WTS_LR = 1e-2\nLAYERWISE_LR_DECAY = 0.1\n\nif ENV=='colab':\n    MODEL_DIR = f'\/content\/drive\/MyDrive\/Colab Notebooks\/CLR\/{MODEL_VER}'\n    SAVE_DIR = MODEL_DIR\n    LOAD_BACKBONE_DIR = f'{MODEL_DIR}\/backbone'\nelif ENV=='kaggle':\n    MODEL_DIR = '..\/input\/clrp-lightbase-004-electral-dat'\n    SAVE_DIR = '.'\n    LOAD_BACKBONE_DIR = '..\/input\/electra-large-discriminator'\n\nQWKloss = QuadraticWeightedKappaLoss(num_cat=NUM_BINS, device=DEVICE)\nBTloss = BradleyTerryLoss()\ntrain_df['bins'] = pd.cut(train_df['target'], bins=NUM_BINS, labels=False)\n\n# Setup Tokenizer\nif PHASE=='train':\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained(f'{SAVE_DIR}\/backbone')\nelif PHASE=='eval_oof' or PHASE=='inference':\n    tokenizer = AutoTokenizer.from_pretrained(LOAD_BACKBONE_DIR)\nif 'gpt2' in MODEL_NAME.lower():\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Tokenize the benchmark text\nbenchmark_token = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer, MAX_LEN, return_tensor = True)\nif NO_TOKEN_TYPE:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), None, benchmark_token['attention_mask'].to(DEVICE))\nelse:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), benchmark_token['token_type_ids'].to(DEVICE), benchmark_token['attention_mask'].to(DEVICE))\n\n# Main\nif PHASE=='train' or PHASE=='eval_oof':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(train_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    oof_df_ElectraL004 = Train_or_Validation()\n\nif PHASE=='eval_oof':\n    oof_df_ElectraL004.to_csv(f'oof_{MODEL_VER}.csv')\n\nif PHASE=='inference':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(test_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    submission_df_ElectraL004 = Inference()\n    submission_df_ElectraL004.to_csv(f\"submission_{MODEL_VER}.csv\", index=False)\n\nif os.path.isdir('SelfAttComplex'):\n    shutil.rmtree('SelfAttComplex')","ad58633c":"DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nif ENV=='colab':\n    BASE_DIR = '\/content\/drive\/MyDrive\/Colab Notebooks\/CLR\/input'\n    TRAIN_DATA_DIR = BASE_DIR\nelif ENV=='kaggle':\n    BASE_DIR = '..\/input\/commonlitreadabilityprize'\n    TRAIN_DATA_DIR = BASE_DIR\n\n# train_df = pd.read_csv(f'{TRAIN_DATA_DIR}\/train_folds.csv')\ntrain_df = pd.read_csv(f'{TRAIN_DATA_DIR}\/train.csv')\ntrain_df = create_folds(train_df, num_splits=5, shuffle=True, random_state=321)\n\nbenchmark = train_df[(train_df.target == 0) & (train_df.standard_error == 0)].copy()\ntrain_df.drop(train_df[(train_df.target == 0) & (train_df.standard_error == 0)].index,\n              inplace=True)\ntrain_df.reset_index(drop=True, inplace=True)\n\ntest_df = pd.read_csv(f\"{BASE_DIR}\/test.csv\")\nsubmission_df = pd.read_csv(f\"{BASE_DIR}\/sample_submission.csv\")","b204d7ad":"SEED = 1000\nNUM_FOLDS = 5\nNUM_EPOCHS = 4\nBATCH_SIZE = 5\nMAX_LEN = 248\nEVAL_SCHEDULE = [(0.52, 32), (0.49, 16), (0.48, 8), (0.47, 4), (-1., 2)]\nMODEL_NAME = 'microsoft\/deberta-large'\nMODEL_VER = 'CLRP_LightBase_kfoldv3_014_DeBERTaL'\n \nNUM_HIDDEN_LAYERS = 24\nHIDDEN_SIZE = 1024\nNUM_BINS = 29\nCOEF_QWK = 0.0 # coefficient of QWK loss\nCOEF_BT = 1.0 # coefficient of Bradley-Terry loss\n\nUSE_MAX_POOLING = True\nUSE_SELF_ATT = False\nNO_TOKEN_TYPE = False\nHAS_DECODER = False\n\nBACKBONE_LR = 1e-5 # 2e-5\nHIDDEN_WTS_LR = 5e-3 # 1e-2\nLAYERWISE_LR_DECAY = 0.1\n\nif ENV=='colab':\n    MODEL_DIR = f'\/content\/drive\/MyDrive\/Colab Notebooks\/CLR\/{MODEL_VER}'\n    SAVE_DIR = MODEL_DIR\n    LOAD_BACKBONE_DIR = f'{MODEL_DIR}\/backbone'\nelif ENV=='kaggle':\n    MODEL_DIR = '..\/input\/clrp-lightbase-kfoldv3-014-debertal-dat'\n    SAVE_DIR = '.'\n    LOAD_BACKBONE_DIR = '..\/input\/deberta-large'\n\nQWKloss = QuadraticWeightedKappaLoss(num_cat=NUM_BINS, device=DEVICE)\nBTloss = BradleyTerryLoss()\ntrain_df['bins'] = pd.cut(train_df['target'], bins=NUM_BINS, labels=False)\n\n# Setup Tokenizer\nif PHASE=='train':\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained(f'{SAVE_DIR}\/backbone')\nelif PHASE=='eval_oof' or PHASE=='inference':\n    tokenizer = AutoTokenizer.from_pretrained(LOAD_BACKBONE_DIR)\nif 'gpt2' in MODEL_NAME.lower():\n    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n\n# Tokenize the benchmark text\nbenchmark_token = convert_examples_to_features(benchmark['excerpt'].iloc[0], tokenizer, MAX_LEN, return_tensor = True)\nif NO_TOKEN_TYPE:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), None, benchmark_token['attention_mask'].to(DEVICE))\nelse:\n    benchmark_token = (benchmark_token['input_ids'].to(DEVICE), benchmark_token['token_type_ids'].to(DEVICE), benchmark_token['attention_mask'].to(DEVICE))\n\n# Main\nif PHASE=='train' or PHASE=='eval_oof':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(train_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    oof_df_kfv3_DeBERTaL014 = Train_or_Validation()\n\nif PHASE=='eval_oof':\n    oof_df_kfv3_DeBERTaL014.to_csv(f'oof_{MODEL_VER}.csv')\n\nif PHASE=='inference':\n    sa_complex = None # Self-Attention Complexity in Pretrained Model\n    if USE_SELF_ATT:\n        sa_complex = SelfAttention_Complexity(test_df, 'cpu')\n        benchmark_sa_complex = SelfAttention_Complexity(benchmark, DEVICE)\n    submission_df_kfv3_DeBERTaL014 = Inference()\n    submission_df_kfv3_DeBERTaL014.to_csv(f\"submission_{MODEL_VER}.csv\", index=False)\n\nif os.path.isdir('SelfAttComplex'):\n    shutil.rmtree('SelfAttComplex')","5f5c5d86":"weight = [0.4057247639 \/ 3] * 3 + [0.2704413533, 0.3238339126]\n\nif PHASE=='eval_oof':\n    oof_df = oof_df_RoBERTaL031[['pred']].copy()\n    oof_df['pred'] = (weight[0] * oof_df_RoBERTaL031['pred'].values +\n                      weight[1] * oof_df_RoBERTaL031b['pred'].values +\n                      weight[2] * oof_df_kfv2_RoBERTaL026a['pred'].values +\n                      weight[3] * oof_df_ElectraL004['pred'].values+\n                      weight[4] * oof_df_kfv3_DeBERTaL014['pred'].values)\n    oof_df.to_csv(f'oof_ensemble.csv')\n\nif PHASE=='inference':\n    submission_df['target'] = (weight[0] * submission_df_RoBERTaL031['target'].values +\n                               weight[1] * submission_df_RoBERTaL031b['target'].values +\n                               weight[2] * submission_df_kfv2_RoBERTaL026a['target'].values +\n                               weight[3] * submission_df_ElectraL004['target'].values +\n                               weight[4] * submission_df_kfv3_DeBERTaL014['target'].values)","4722731d":"meta_weights = [0.42, 0.58]\nss['target'] = ss['target'].values * meta_weights[0] + submission_df['target'].values * meta_weights[1]","5210d755":"ss.to_csv('submission.csv', index = None)\nss","2685a949":"* Version 15","fc07d6fa":"* Version 16","2e7d8201":"### RoBERTa large","7972c0c4":"# Tri's part\n##################################################################################################################","eb1a2eae":"### RoBERTa-large Ver.31b","edd9bc17":"# Hiro's part\n##################################################################################################################","db6dce4f":"* Version 1","1a981366":"### kfold: Ver.3, DeBERTa-large Ver.14","3ac1eb62":"* Version 1","a90d38fb":"* Version 1","d8f3a00b":"# Model\nThe model is inspired by the one from [Maunish](http:\/\/https:\/\/www.kaggle.com\/maunish\/clrp-roberta-svm).","39060687":"### Utils class","0b861104":"# Configuration","80db444f":"* Version 1","849310a0":"* Version 1","36fcbb41":"# Version 5\n* Ensemble Hiro's 5 models and Tri's 13 models.\n\n## Model list\n\n#### Hiro's models\n* RoBERTa-large version 31;\n* RoBERTa-large version 31-b;\n* kfold: Ver.2, RoBERTa-large Ver.26a;\n* ELECTRA-large Ver.4;\n* kfold: Ver.3, DeBERTa-large Ver.14\n\n#### Tri's models\n* RoBERTa-large version 15-0, 15-3;\n* RoBERTa-large version 16-1;\n* XLNet-large-cased version 2-0, 3-0, 3-1;\n* GPT2-medium version 1-0;\n* ELECTRA-large-discriminator version 1-0, 1-1;\n* DeBERTa-large version 1-0, 1-1;\n* Funnel-large version 1-0;\n* BART-large version 1-0","c722dd3d":"# Averaging","d2567a09":"* Version 11","94661116":"# Meta ensemble\n##################################################################################################################","bce21899":"# Self Attention Complexity in Pretrained Model","2710f1fe":"### ELECTRA-large Ver.4","2bd74531":"* Version 3","e01fcf07":"### ELECTRA large discriminator","a5f155a3":"# Loss function","c36ce14f":"### RoBERTa base","10d33588":"# Main","d6dc7390":"### Funnel-transformer\/large","5d0d6a0a":"# Final submission\n##################################################################################################################","edea6857":"# Training, Validation","450ebd63":"### GPT2 medium","2814f75f":"# Import data","d2a202fd":"### XLNet large cased","35021ff6":"* Version 2","785a1124":"# Dataset","96bb47b4":"# Dataset","a5957a78":"### BART large","7d14417c":"# Inference function","5dc24fa6":"### ALBERT xlarge v2","e126eda2":"### DeBERTa large","3954cfa4":"### RoBERTa-large Ver.31","edc1c03b":"### kfold: Ver.2, RoBERTa-large Ver.26a","20db05af":"# Models","ea50a4f7":"# Ensemble","bac96d8d":"# Utilities","30c48707":"# Models","133fc1ab":"* Version 1"}}