{"cell_type":{"52e020d9":"code","f54161f0":"code","9401aefc":"code","28b28860":"code","fa7f3992":"code","0b2e904e":"code","162ad4bd":"code","cac70087":"code","c566418c":"code","1a36242b":"code","e18113ec":"code","1392d64d":"code","c33c974b":"code","69999c76":"code","57223fb8":"code","2694bd68":"code","7e909991":"code","93aa9b56":"code","ae92e6ce":"code","9c00931f":"code","d3ca40f0":"code","7e43b598":"code","97057699":"code","a0932c77":"code","5200919c":"code","b5570a37":"code","e0633c3c":"code","4d5c9fd2":"code","773d5f8f":"code","e510710b":"code","ac727b9f":"code","0e97ed33":"code","193911dd":"code","95b84cc8":"code","34b8d431":"code","4d639683":"code","19365d6e":"code","1ab27658":"code","4e7fe206":"code","99272d07":"code","abaf0dfc":"code","5ce6a45a":"code","234e0219":"code","1a2ef017":"code","6e7a4fbf":"code","943b934c":"markdown","602439b3":"markdown","bd493312":"markdown","bf8d161e":"markdown","4799e6bf":"markdown","8116479c":"markdown","812ec54d":"markdown","2cec085c":"markdown","0b934f37":"markdown","808e8b3d":"markdown","45167231":"markdown","43f0b677":"markdown","10d9be0c":"markdown","c1fbf5d7":"markdown","cd2fa5ef":"markdown","e7b4da09":"markdown","98dbec82":"markdown","28afa6cf":"markdown","0ad31148":"markdown","7bc73b26":"markdown","6db3db11":"markdown","b9967c72":"markdown","2c0c3237":"markdown","d304b17e":"markdown","a8787692":"markdown","ab5887f4":"markdown","3119ff9b":"markdown","e5c63251":"markdown","6e5cca50":"markdown","872813dc":"markdown","66c41979":"markdown","62b0c0b5":"markdown","c5abd420":"markdown","b040131b":"markdown","378736e1":"markdown","13bca659":"markdown","6769ae2a":"markdown","a4f40489":"markdown"},"source":{"52e020d9":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport warnings\nimport re\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import StratifiedShuffleSplit, GridSearchCV, KFold\nfrom sklearn.preprocessing import Normalizer, OneHotEncoder, StandardScaler, OrdinalEncoder\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.preprocessing import KBinsDiscretizer\n\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\nimport os\n\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.max_rows = 10\npd.options.display.max_columns = 20\n\nprint(\"numpy version: {}\".format(np.__version__))\nprint(\"pandas version: {}\".format(pd.__version__))\nprint(\"seaborn version: {}\\n\".format(sns.__version__))\n\nsns.set_style(\"whitegrid\")\nflatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\nsns.set_palette(flatui)","f54161f0":"# Read the CSV\ntrain_csv_path = '\/kaggle\/input\/titanic\/train.csv'\ntest_csv_path = '\/kaggle\/input\/titanic\/test.csv'\nsample_sub_path = '\/kaggle\/input\/titanic\/gender_submission.csv'\ntrain_data = pd.read_csv(train_csv_path)\ntest_data = pd.read_csv(test_csv_path)\nsample_sub = pd.read_csv(sample_sub_path)\n\n# Keep a clean copy of train\nclean_copy = train_data.copy()\n\ntarget_col='Survived'","9401aefc":"print('Train data shape: {}'.format(train_data.shape))\nprint('Test data shape: {}'.format(test_data.shape))","28b28860":"train_data.head()","fa7f3992":"train_data.info()","0b2e904e":"train_data.describe()","162ad4bd":"target_value_counts=train_data.Survived.value_counts()\nprint(target_value_counts)","cac70087":"sns.set_style('ticks')\nfig, ax = plt.subplots()\nfig.set_size_inches(11, 8)\nclrs = ['#e74c3c', '#2ecc71']\nfig = sns.barplot(x=target_value_counts.index,\n            y=target_value_counts, \n            capsize=.3, \n            palette=clrs)\nplt.xlabel('0=No or 1=Yes')\nplt.ylabel('Number of Passengers')\nplt.title('Distribution of Test Data')\nplt.show(fig)","c566418c":"plt.figure(figsize=(12,8))\n\nsns.distplot(train_data[train_data.Survived == 0]['Fare'], \n             bins=10, color='#e74c3c')\nsns.distplot(train_data[train_data.Survived == 1]['Fare'], \n             bins=10, color='#2ecc71')\nplt.title('Fares by Survival', fontsize=20)\nplt.xlabel('Fare')\nplt.ylabel('Survival')\nplt.show()","1a36242b":"fig, ax  = plt.subplots()\nfig.set_size_inches(16, 8)\nfig = sns.violinplot(x=\"Sex\",\n            y=\"Survived\",\n            data=train_data)\nplt.xlabel('Passenger Sex')\nplt.ylabel('Survived')\nplt.title('Survival by Sex')\nplt.show(fig)","e18113ec":"fig, ax  = plt.subplots()\nfig.set_size_inches(16, 8)\nfig = sns.violinplot(x=\"Pclass\",\n            y=\"Survived\",\n            data=train_data)\nplt.xlabel('Passenger Class')\nplt.title('Survival by Passenger Class')\nplt.show(fig)","1392d64d":"train_data['Title'] = train_data.Name.apply(lambda x: re.search(' ([A-Z][a-z]+)\\.', x).group(1))\n\ntitle_dict = {\n        'Capt': 'Officer',\n        'Col':  'Officer',\n        'Major': 'Officer',\n        'Dr': 'Professional',\n        'Rev': 'Professional',\n        'Jonkheer': 'Elite',\n        'Don': 'Elite',\n        'Sir' : 'Elite',\n        'the Countess':'Elite',\n        'Dona': 'Elite',\n        'Lady' : 'Elite',\n        'Mme': 'Elite',\n        \"Ms\": 'Mrs',\n        \"Mrs\" : 'Mrs',\n        \"Mlle\": 'Miss',\n        \"Miss\" : 'Miss',\n        \"Mr\" :  'Mr',\n        \"Master\" : 'Master'\n    }\n\ntrain_data['Title'] = train_data.Title.map(title_dict)\ntrain_data['Title'] = train_data.Title.fillna('Unknown')","c33c974b":"fig, ax = plt.subplots()\nfig.set_size_inches(11, 8)\nfig = sns.countplot(x='Title', \n            capsize=.3, \n            palette=flatui,\n            data=train_data)\nplt.xlabel('Passenger Title')\nplt.ylabel('Number of Passengers')\nplt.title('Title of Passengers')\nplt.show(fig)","69999c76":"fig, ax  = plt.subplots()\nfig.set_size_inches(16, 8)\nfig = sns.violinplot(x=\"Title\",\n            y=\"Survived\",\n            data=train_data)\nplt.xlabel('Passenger Title')\nplt.ylabel('Survived')\nplt.title('Survival of Titles')\nplt.show(fig)","57223fb8":"train_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1","2694bd68":"fig, ax = plt.subplots()\nfig.set_size_inches(11, 8)\nsns.countplot(x='FamilySize',\n            capsize=.3, \n            palette=flatui,\n            data=train_data)\nplt.xlabel('Family Size')\nplt.ylabel('Number of Passengers')\nplt.title('Family Sizes')\nplt.show(fig)","7e909991":"fig, ax  = plt.subplots()\nfig.set_size_inches(16, 8)\nfig = sns.violinplot(x=\"FamilySize\",\n            y=\"Survived\",\n            data=train_data)\nplt.xlabel('Family Size')\nplt.ylabel('Survived')\nplt.title('Survival of Family Size')\nplt.show(fig)","93aa9b56":"#Filling missing fares\ntrain_data.Fare = train_data.Fare.fillna(-0.1)\n\nfare_intervals = (-1, 0, 8, 15, 31, 600)\nfare_labels = ['None', 'Group1', 'Group2', 'Group3', 'Group4']\ntrain_data[\"FareCat\"] = pd.cut(train_data.Fare, fare_intervals, labels=fare_labels)","ae92e6ce":"fig, ax = plt.subplots()\nfig.set_size_inches(11, 8)\nsns.countplot(x='FareCat',\n            capsize=.3, \n            palette=flatui,\n            data=train_data)\nplt.xlabel('Fare Category')\nplt.title('Fare Catefory Distribution')\nplt.show(fig)","9c00931f":"fig, ax  = plt.subplots()\nfig.set_size_inches(16, 8)\nfig = sns.violinplot(x=\"FareCat\",\n            y=\"Survived\",\n            data=train_data)\nplt.xlabel('Fare Category')\nplt.title('Survival by Fare Category')\nplt.show(fig)","d3ca40f0":"# Impute missing ages by sex, class, and title.\ntrain_data.loc[train_data.Age.isnull(), 'Age'] = train_data.groupby(['Sex','Pclass','Title']).Age.transform('median')\nprint('Null count: {}'.format(train_data['Age'].isnull().sum()))","7e43b598":"sns.set_style('ticks')\nfig, ax  = plt.subplots()\nfig.set_size_inches(16, 8)\nfig = sns.distplot(train_data['Age'],\n            bins=20)\nplt.xlabel('Age')\nplt.title('Distribution of Age')\nplt.show(fig)","97057699":"plt.figure(figsize=(16,8))\ng = sns.FacetGrid(train_data, col='Survived',size=7)\ng = g.map(sns.distplot, \"Age\")\nplt.show()","a0932c77":"fig, ax  = plt.subplots()\nfig.set_size_inches(16, 8)\nfig = sns.violinplot(x=\"FamilySize\",\n            y=\"Survived\",\n            data=train_data)\nplt.xlabel('Family Size')\nplt.ylabel('Survived')\nplt.title('Survival of Family Size')\nplt.show(fig)","5200919c":"train_data['AgeGroup'] = None\ntrain_data.loc[((train_data['Sex'] == 'male') & (train_data['Age'] <= 15)), 'AgeGroup'] = 'Boy'\ntrain_data.loc[((train_data['Sex'] == 'female') & (train_data['Age'] <= 15)), 'AgeGroup'] = 'Girl'\ntrain_data.loc[((train_data['Sex'] == 'male') & (train_data['Age'] > 15)), 'AgeGroup'] = 'AdultMale'\ntrain_data.loc[((train_data['Sex'] == 'female') & (train_data['Age'] > 15)), 'AgeGroup'] = 'AdultFemale'","b5570a37":"fig, ax = plt.subplots()\nfig.set_size_inches(11, 8)\nsns.countplot(x='AgeGroup',\n            capsize=.3, \n            palette=flatui,\n            data=train_data)\nplt.xlabel('Age Group')\nplt.ylabel('Number of Passengers')\nplt.title('Distribution of Age Groups')\nplt.show(fig)","e0633c3c":"fig, ax  = plt.subplots()\nfig.set_size_inches(16, 8)\nfig = sns.violinplot(x=\"AgeGroup\",\n            y=\"Survived\",\n            data=train_data)\nplt.xlabel('Age Group')\nplt.ylabel('Survived')\nplt.title('Survival of Age Groups')\nplt.show(fig)","4d5c9fd2":"train_data['Deck'] = train_data['Cabin'].apply(lambda s: s[0] if pd.notnull(s) else 'M')\n\n# Passengers in T deck changed to A deck\nidx = train_data[train_data['Deck'] == 'T'].index\ntrain_data.loc[idx, 'Deck'] = 'A'\n\n# Group decks\ntrain_data['Deck'] = train_data['Deck'].replace(['A', 'B', 'C'], 'ABC')\ntrain_data['Deck'] = train_data['Deck'].replace(['D', 'E'], 'DE')\ntrain_data['Deck'] = train_data['Deck'].replace(['F', 'G'], 'FG')","773d5f8f":"fig, ax = plt.subplots()\nfig.set_size_inches(11, 8)\nsns.countplot(x='Deck',\n            capsize=.3, \n            palette=flatui,\n            data=train_data)\nplt.xlabel('Deck')\nplt.title('Distribution of Passengers by Deck')\nplt.show(fig)","e510710b":"fig, ax  = plt.subplots()\nfig.set_size_inches(16, 8)\nfig = sns.violinplot(x=\"Deck\",\n            y=\"Survived\",\n            data=train_data)\nplt.xlabel('Deck')\nplt.title('Survival by Deck')\nplt.show(fig)","ac727b9f":"# Revert to clean copy\ntrain_data = clean_copy.copy()","0e97ed33":"def get_title(name):\n    title_search = re.search(r'([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return ''","193911dd":"def replace_titles(title):\n    if title in ['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']:\n        return 'Rare'\n    elif title in ['Countess', 'Mme']:\n        return 'Mrs'\n    elif title in ['Mlle', 'Ms']:\n        return 'Miss'\n    elif title =='Dr':\n        if x['Sex']=='Male':\n            return 'Mr'\n        else:\n            return 'Mrs'\n    else:\n        return title","95b84cc8":"train_data['Parch'].value_counts()\ntest_data['Parch'].value_counts()","34b8d431":"def feature_eng(X):\n    # Rename Categorical Columns\n    \n    # Create title feature\n    X['Title'] = X['Name'].apply(get_title)\n    X['Title'] = X['Title'].fillna('Miss')\n    X['Title'] = X['Title'].apply(replace_titles)\n    \n    # Drop Name\n    X.drop('Name', axis=1, inplace=True)\n    \n    #Impute Age\n    X.loc[X.Age.isnull(), 'Age'] = X.groupby(['Sex','Pclass','Title']).Age.transform('median')\n    \n    # Convert Pclass\n    X['Pclass'] = X['Pclass'].apply(lambda x: 'first' if x==1 else 'second' if x==2 else 'third')\n    \n    # Create Age Bins\n    binner = KBinsDiscretizer(encode='ordinal')\n    binner.fit(X[['Age']])\n    X['AgeBins'] = binner.transform(X[['Age']])\n    \n    # Create family size feature\n    X['FamilySize'] = X['SibSp'] + X['Parch'] + 1\n    \n    # Family size mapping\n    family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', \n                  5: 'Large', 6: 'Large', 7: 'Large', 8: 'Large', 11: 'Large'}\n    X['GroupSize'] = X['FamilySize'].map(family_map)\n    \n    # With Family Feature\n    X['WithFamily'] = (X['FamilySize']>1)\n    X['WithFamily'] = X['WithFamily'].apply(lambda x: 'yes' if x==1 else 'no')\n    \n    # Impute Fares\n    X.loc[(X.Fare.isnull()), 'Fare'] = X.Fare.median()\n    X.loc[(X.Fare==0), 'Fare'] = X.Fare.median()\n    \n    # Create Fare Bins\n    binner.fit(X[['Fare']])\n    X['FareBins'] = binner.transform(X[['Fare']])\n    \n    # Create deck and room features\n    X[\"Deck\"] = X[\"Cabin\"].str.slice(0,1)\n    X[\"Deck\"] = X[\"Deck\"].fillna(\"N\")\n    idx = X[X['Deck'] == 'T'].index\n    X.loc[idx, 'Deck'] = 'A'\n    \n    X[\"Room\"] = X[\"Cabin\"].str.slice(1,5).str.extract(\"([0-9]+)\", expand=False)\n    X['Room'] = X['Room'].astype(str)\n    X[\"Room\"] = X[\"Room\"].fillna('None')\n    \n    # Drop Cabin\n    X.drop('Cabin', axis=1, inplace=True)\n    \n    # Impute Embarked\n    encoder=OrdinalEncoder()\n    X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)\n\n    # Add some log features\n    X['LogAge'] = np.log(X['Age'])\n    X['LogFare'] = np.log(X['Fare'])\n    X['LogFamilySize'] = np.log(X['Fare'])\n    \n    # Add scaled features\n    scaler = StandardScaler()\n    scaler.fit(X[['Age']])\n    X['AgeScaled'] = scaler.transform(X[['Age']])\n    \n    scaler.fit(X[['Fare']])\n    X['FareScaled'] = scaler.transform(X[['Fare']])\n    \n    scaler.fit(X[['FamilySize']])\n    X['FareScaled'] = scaler.transform(X[['Fare']])\n    \n    # Get Dummies\n    X = pd.get_dummies(X, prefix='dummy', drop_first=True)\n    X.drop('PassengerId', axis=1, inplace=True)\n    X.drop('Age', axis=1, inplace=True)\n    X.drop('Parch', axis=1, inplace=True)\n    X.drop('SibSp', axis=1, inplace=True)\n    X.drop('Fare', axis=1, inplace=True)\n    return X","4d639683":"# Test feature eng method\ntrain_data_proc = feature_eng(train_data)\ntrain_data_proc.head()","19365d6e":"# Constants\nSEED = 37\nTRAIN_SPLIT = .15\n\nscores = ['precision', 'recall']\n\ntrain_data = clean_copy.copy()\n\n# Split off some testing data\nstrat_split = StratifiedShuffleSplit(n_splits=1,\n                              test_size=TRAIN_SPLIT,\n                              random_state=SEED)\n\nfor train_index, val_index in strat_split.split(train_data,  train_data[['Sex', 'Pclass']]): \n    X_train = train_data.loc[train_index]\n    X_val = train_data.loc[val_index]\n    \nX_test = test_data.copy()    \n    \ny_train = X_train['Survived'].copy()\ny_val = X_val['Survived'].copy()\nX_train.drop('Survived', axis=1, inplace=True)\nX_val.drop('Survived', axis=1, inplace=True)\n    \nprint('X_train shape: {}'.format(X_train.shape))\nprint('X_val shape: {}'.format(X_val.shape))\n\nprint('{} training records'.format(len(X_train)))\nprint('{} training labels'.format(len(y_train)))\n  \nprint('{} validation records'.format(len(X_val)))\nprint('{} validation labels'.format(len(y_val)))\n\nX_train['dataset'] = 1\nX_val['dataset'] = 2\nX_test['dataset'] = 3\n\ncombined = pd.concat([X_train, X_val, X_test])\ncombined.reset_index(drop=True, inplace=True)\n\n# call feature engineering method\nX_eng = feature_eng(combined)\n\nprint('combined shape: {}'.format(X_eng.shape))\n\nX_train_eng = X_eng[X_eng['dataset'] == 1]\nX_val_eng = X_eng[X_eng['dataset'] == 2]\nX_test_eng = X_eng[X_eng['dataset'] == 3]\nX_train_eng.drop('dataset', axis=1, inplace=True)\nX_val_eng.drop('dataset', axis=1, inplace=True)\nX_test_eng.drop('dataset', axis=1, inplace=True)\nX_train_eng.head()","1ab27658":"rf_params = [{'n_estimators': [100, 150, 200]}]\n\nfor score in scores:\n    print('Tuning hyper-parameters for %s \\n' % score)\n    print ('Creating pipeline instance.')\n    train_pipeline = Pipeline([\n          ('classifier', GridSearchCV(RandomForestClassifier(random_state=SEED),\n                                    rf_params, \n                                    scoring='%s_macro' % score,\n                                    verbose=10,\n                                    n_jobs=-1,\n                                    cv=8))])\n    \n    print('Fitting the model.')\n    train_pipeline.fit(X_train_eng, y_train)\n\n    print('Tuning hyper-parameters for %s \\n' % score)\n    print('Best parameters set found on development set: \\n')\n    print(train_pipeline.named_steps['classifier'].best_params_, '\\n')\n    print(\"Grid scores on development set:\\n\")\n    means = train_pipeline.named_steps['classifier'].cv_results_['mean_test_score']\n    stds = train_pipeline.named_steps['classifier'].cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, train_pipeline.named_steps['classifier'].cv_results_['params']):\n        print(\"%0.3f (+\/-%0.03f) for %r\"\n            % (mean, std * 2, params))\n    print()\n    print('Detailed classification report:\\n')\n    print('The model is trained on the full development set.')\n    print('The scores are computed on the full evaluation set.\\n')\n    y_true, y_pred = y_val, train_pipeline.predict(X_val_eng)\n    print(classification_report(y_val, y_pred), '\\n')","4e7fe206":"cm = confusion_matrix(y_val, y_pred)\nfig = plt.figure(figsize = (10,7))\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nlabels = ['Dead', 'Alive']\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","99272d07":"lr_params = [{'C': [1, 3, 5]}]\n\nfor score in scores:\n    print('Tuning hyper-parameters for %s \\n' % score)\n    print ('Creating pipeline instance.')\n    train_pipeline = Pipeline([\n          ('classifier', GridSearchCV(LogisticRegression(random_state=SEED),\n                                    lr_params, \n                                    scoring='%s_macro' % score,\n                                    verbose=10,\n                                    n_jobs=-1,\n                                    cv=8))])\n    \n    print('Fitting the model.')\n    train_pipeline.fit(X_train_eng, y_train)\n\n    print('Tuning hyper-parameters for %s \\n' % score)\n    print('Best parameters set found on development set: \\n')\n    print(train_pipeline.named_steps['classifier'].best_params_, '\\n')\n    print(\"Grid scores on development set:\\n\")\n    means = train_pipeline.named_steps['classifier'].cv_results_['mean_test_score']\n    stds = train_pipeline.named_steps['classifier'].cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, train_pipeline.named_steps['classifier'].cv_results_['params']):\n        print(\"%0.3f (+\/-%0.03f) for %r\"\n            % (mean, std * 2, params))\n    print()\n    print('Detailed classification report:\\n')\n    print('The model is trained on the full development set.')\n    print('The scores are computed on the full evaluation set.\\n')\n    y_true, y_pred = y_val, train_pipeline.predict(X_val_eng)\n    print(classification_report(y_val, y_pred), '\\n')","abaf0dfc":"cm = confusion_matrix(y_val, y_pred)\nfig = plt.figure(figsize = (10,7))\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nlabels = ['Dead', 'Alive']\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","5ce6a45a":"svc_params = [{'C': [1,3,5]}]\n\nfor score in scores:\n    print('Tuning hyper-parameters for %s \\n' % score)\n    print ('Creating pipeline instance.')\n    train_pipeline = Pipeline([\n          ('classifier', GridSearchCV(SVC(random_state=SEED),\n                                    svc_params, \n                                    scoring='%s_macro' % score,\n                                    verbose=10,\n                                    n_jobs=-1,\n                                    cv=8))])\n    \n    print('Fitting the model.')\n    train_pipeline.fit(X_train_eng, y_train)\n\n    print('Tuning hyper-parameters for %s \\n' % score)\n    print('Best parameters set found on development set: \\n')\n    print(train_pipeline.named_steps['classifier'].best_params_, '\\n')\n    print(\"Grid scores on development set:\\n\")\n    means = train_pipeline.named_steps['classifier'].cv_results_['mean_test_score']\n    stds = train_pipeline.named_steps['classifier'].cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, train_pipeline.named_steps['classifier'].cv_results_['params']):\n        print(\"%0.3f (+\/-%0.03f) for %r\"\n            % (mean, std * 2, params))\n    print()\n    print('Detailed classification report:\\n')\n    print('The model is trained on the full development set.')\n    print('The scores are computed on the full evaluation set.\\n')\n    y_true, y_pred = y_val, train_pipeline.predict(X_val_eng)\n    print(classification_report(y_val, y_pred), '\\n')","234e0219":"cm = confusion_matrix(y_val, y_pred)\nfig = plt.figure(figsize = (10,7))\nax = fig.add_subplot(111)\ncax = ax.matshow(cm)\nplt.title('Confusion matrix')\nfig.colorbar(cax)\nlabels = ['Dead', 'Alive']\nax.set_xticklabels([''] + labels)\nax.set_yticklabels([''] + labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","1a2ef017":"train_data = pd.concat([X_train_eng, X_val_eng])\ny_train = pd.concat([y_train, y_val])\n\nvoting_clf = VotingClassifier(\n                             estimators=[\n                                 ('rf', RandomForestClassifier(random_state=SEED,\n                                                              n_estimators=150)),\n                                 ('lr', LogisticRegression(random_state=SEED,\n                                                          C=3)),\n                                 ('svc', SVC(random_state=SEED,\n                                                  C=3,\n                                                  probability=True))\n                             ],\n                             voting='soft')\nvoting_clf.fit(train_data, y_train)","6e7a4fbf":"# Prep test Data\nsample_sub.drop('Survived', axis=1, inplace=True)\n\n# Predict\ny_pred_test = voting_clf.predict(X_test_eng)\n\n# Save\nsample_sub['Survived'] = y_pred_test\nsample_sub.reset_index(drop=True, inplace=True)\nsample_sub.to_csv('\/kaggle\/working\/submission.csv')\nprint('Output saved.')","943b934c":"# Titanic Survival\n\nTitanic EDA, visualizations, with SkLearn Pipelines for feature engineering, training - evaluation, and inference.\n\n## Challenge Description\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n### Data Dictionary\n\n|**Variable**|**Definition**|**Values**|**Null Count**|\n|-|-|-|-|\n|PassengerId|Integer - unique id|892-1309|0|\n|Survived|Integer - target|0=No, 1=Yes|0|\n|Pclass|Integer - ticket class|1=1st, 2=2nd, 3=3rd|0|\n|Name|String - passenger name|418 unique values|0|\n|Sex|String - passenger sex|male 64%, female, 36%|0|\n|Age|Float - passenger age|0.17 - 76|177|\n|SibSp|Integer - # of siblings\/spouses aboard|0-8|0|\n|Parch|Integer - # of parents\/children aboard|0-9|0|\n|Ticket|String - ticket number|409 unique values|0|\n|Fare|Float - price of ticket|0 - 512|0|\n|Cabin|String - cabin number|89 unique values|687|\n|Embarked|Enum - port of embarkation|C=Cherbourg, Q=Queenstown, S=Southampton|2|\n\n\n**Train Record Count:** 891\n\n**Test Record Count:** 418\n\n**Variable Notes**\n\n**pclass:** A proxy for socio-economic status (SES)\n    * 1st = Upper\n    * 2nd = Middle\n    * 3rd = Lower\n\n**Age:** \n    * Age is fractional if less than 1\n    * If the age is estimated, is it in the form of xx.5\n\n**sibsp:** \nThe dataset defines family relations in this way:\n    * Sibling = brother, sister, stepbrother, stepsister\n    * Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n**parch:** \nThe dataset defines family relations in this way...\n    * Parent = mother, father\n    * Child = daughter, son, stepdaughter, stepson\n    * Some children travelled only with a nanny, therefore parch=0 for them.","602439b3":"### Feature Engineering Method","bd493312":"#### Age Density by Survival","bf8d161e":"## Build and Evaluate a Random Forest Classifier","4799e6bf":"### Random Forest Classifier Confusion Matrix","8116479c":"#### Create a Family Size Feature","812ec54d":"#### Bin The Ages","2cec085c":"#### Create Title Feature","0b934f37":"## Build and Evaluate an LinearSVC Classifier","808e8b3d":"#### Survival by Fare Category","45167231":"### Train-Validation Split","43f0b677":"## Feature Engineering\n### Title Feature","10d9be0c":"### Survival by Pclass","c1fbf5d7":"#### Survival By Family Size","cd2fa5ef":"### Family Size","e7b4da09":"### Survival by Passenger Sex ","98dbec82":"### Age Feature","28afa6cf":"## Build and Evaluate a Logistic Regression Classifier","0ad31148":"#### Distribution of Titles","7bc73b26":"#### Survival Rate for Age Groups","6db3db11":"### Logistic Regression Confusion Matrix","b9967c72":"### Cabin Feature\n#### Create Deck Feature","2c0c3237":"## Ensemble, Retrain and Predict","d304b17e":"## Load the Data","a8787692":"## Basic Train Set Exploration","ab5887f4":"### Fare Feature\n####\u00a0Create Fare Category","3119ff9b":"### Retrain on Entire Test Set","e5c63251":"#### Fare Category Distribution","6e5cca50":"#### Distribution of Family Size","872813dc":"### Imports and Environment Settings","66c41979":"#### Passenger Distribution by Deck","62b0c0b5":"####\u00a0Age Distributuion Density","c5abd420":"### LinearSVC Confusion Matrix","b040131b":"### Target Distribution","378736e1":"## Exploratory Data Analysis\n### Survival By Passenger Fare","13bca659":"#### Survival by Deck","6769ae2a":"## Preprocessing","a4f40489":"#### Survival by Title"}}