{"cell_type":{"63876569":"code","fbd21648":"code","752a5473":"code","56299003":"code","2734828b":"code","d1b843fa":"code","9c55b975":"code","212cdd2b":"code","0ce822d3":"code","2f9fde7b":"code","64e3aa64":"code","8f4f2451":"markdown","6a35a823":"markdown","8b0ca134":"markdown","18944b49":"markdown","58909430":"markdown","0bbc31f4":"markdown","f301d9c4":"markdown","2f98e2a7":"markdown","83aa9546":"markdown","f5fcdcf8":"markdown","41c1633b":"markdown","bce48254":"markdown","1e8a89bd":"markdown"},"source":{"63876569":"\n%matplotlib inline\nimport random\nimport os\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\n\ninput_dir = '..\/input\/movielens-preprocessing'\nmodel_dir = '..\/input\/movielens-spiffy-model'\nmodel_path = os.path.join(model_dir, 'movie_svd_model_32.h5')\n\nmodel = keras.models.load_model(model_path)\nemb_layer = model.get_layer('movie_embedding')\n(w,) = emb_layer.get_weights()\n\nmovies_path = os.path.join(input_dir, 'movie.csv')\nmovies_df = pd.read_csv(movies_path, index_col=0)","fbd21648":"threshold = 100\nmainstream_movies = movies_df[movies_df.n_ratings >= threshold].reset_index(drop=True)\nprint(\"Went from {} to {} movies after applying threshold\".format(\n    len(movies_df), len(mainstream_movies),\n))\nw_full = w\nw = w[mainstream_movies.movieId]\ndf = mainstream_movies","752a5473":"from sklearn.manifold import TSNE\n\n# The default of 1,000 iterations gives fine results, but I'm training for longer just to eke\n# out some marginal improvements. NB: This takes almost an hour!\ntsne = TSNE(random_state=1, n_iter=15000, metric=\"cosine\")\n\nembs = tsne.fit_transform(w)\n# Add to dataframe for convenience\ndf['x'] = embs[:, 0]\ndf['y'] = embs[:, 1]","56299003":"\n# Save a copy of our t-SNE mapping data for later use (we'll be loading this file in the exercise)\ndf.to_csv('movies_tsne.csv')","2734828b":"embs[:5]","d1b843fa":"FS = (10, 8)\nfig, ax = plt.subplots(figsize=FS)\n# Make points translucent so we can visually identify regions with a high density of overlapping points\nax.scatter(df.x, df.y, alpha=.1);","9c55b975":"\n# Some helper functions for plotting annotated t-SNE visualizations\n\n# TODO: adjust_text not available in kernels\ntry:\n    from adjustText import adjust_text\nexcept ImportError:\n    def adjust_text(*args, **kwargs):\n        pass\n\ndef adjust_text(*args, **kwargs):\n    pass\n\ndef plot_bg(bg_alpha=.01, figsize=(13, 9), emb_2d=None):\n    \"\"\"Create and return a plot of all our movie embeddings with very low opacity.\n    (Intended to be used as a basis for further - more prominent - plotting of a \n    subset of movies. Having the overall shape of the map space in the background is\n    useful for context.)\n    \"\"\"\n    if emb_2d is None:\n        emb_2d = embs\n    fig, ax = plt.subplots(figsize=figsize)\n    X = emb_2d[:, 0]\n    Y = emb_2d[:, 1]\n    ax.scatter(X, Y, alpha=bg_alpha)\n    return ax\n\ndef annotate_sample(n, n_ratings_thresh=0):\n    \"\"\"Plot our embeddings with a random sample of n movies annotated.\n    Only selects movies where the number of ratings is at least n_ratings_thresh.\n    \"\"\"\n    sample = mainstream_movies[mainstream_movies.n_ratings >= n_ratings_thresh].sample(\n        n, random_state=1)\n    plot_with_annotations(sample.index)\n\ndef plot_by_title_pattern(pattern, **kwargs):\n    \"\"\"Plot all movies whose titles match the given regex pattern.\n    \"\"\"\n    match = df[df.title.str.contains(pattern)]\n    return plot_with_annotations(match.index, **kwargs)\n\ndef add_annotations(ax, label_indices, emb_2d=None, **kwargs):\n    if emb_2d is None:\n        emb_2d = embs\n    X = emb_2d[label_indices, 0]\n    Y = emb_2d[label_indices, 1]\n    ax.scatter(X, Y, **kwargs)\n\ndef plot_with_annotations(label_indices, text=True, labels=None, alpha=1, **kwargs):\n    ax = plot_bg(**kwargs)\n    Xlabeled = embs[label_indices, 0]\n    Ylabeled = embs[label_indices, 1]\n    if labels is not None:\n        for x, y, label in zip(Xlabeled, Ylabeled, labels):\n            ax.scatter(x, y, alpha=alpha, label=label, marker='1',\n                       s=90,\n                      )\n        fig.legend()\n    else:\n        ax.scatter(Xlabeled, Ylabeled, alpha=alpha, color='green')\n    \n    if text:\n        # TODO: Add abbreviated title column\n        titles = mainstream_movies.loc[label_indices, 'title'].values\n        texts = []\n        for label, x, y in zip(titles, Xlabeled, Ylabeled):\n            t = ax.annotate(label, xy=(x, y))\n            texts.append(t)\n        adjust_text(texts, \n                    #expand_text=(1.01, 1.05),\n                    arrowprops=dict(arrowstyle='->', color='red'),\n                   )\n    return ax\n\nFS = (13, 9)\ndef plot_region(x0, x1, y0, y1, text=True):\n    \"\"\"Plot the region of the mapping space bounded by the given x and y limits.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=FS)\n    pts = df[\n        (df.x >= x0) & (df.x <= x1)\n        & (df.y >= y0) & (df.y <= y1)\n    ]\n    ax.scatter(pts.x, pts.y, alpha=.6)\n    ax.set_xlim(x0, x1)\n    ax.set_ylim(y0, y1)\n    if text:\n        texts = []\n        for label, x, y in zip(pts.title.values, pts.x.values, pts.y.values):\n            t = ax.annotate(label, xy=(x, y))\n            texts.append(t)\n        adjust_text(texts, expand_text=(1.01, 1.05))\n    return ax\n\ndef plot_region_around(title, margin=5, **kwargs):\n    \"\"\"Plot the region of the mapping space in the neighbourhood of the the movie with\n    the given title. The margin parameter controls the size of the neighbourhood around\n    the movie.\n    \"\"\"\n    xmargin = ymargin = margin\n    match = df[df.title == title]\n    assert len(match) == 1\n    row = match.iloc[0]\n    return plot_region(row.x-xmargin, row.x+xmargin, row.y-ymargin, row.y+ymargin, **kwargs)","212cdd2b":"# This and several other helper functions are defined in a code cell above. Hit the \"code\"\n# button above if you're curious about how they're implemented.\nplot_by_title_pattern('Harry Potter', figsize=(15, 9), bg_alpha=.05, text=False);","0ce822d3":"plot_region_around('Harry Potter and the Order of the Phoenix', 4);","2f9fde7b":"docs = df[ (df.genres == 'Documentary') ]\nplot_with_annotations(docs.index, text=False, alpha=.4, figsize=(15, 8));","64e3aa64":"\nimport itertools\nsample_rate = 1\ngenre_components = ['Comedy', 'Drama', 'Romance']\ngenre_combos = set()\nfor size in range(1, 4):\n    combo_strs = ['|'.join(genres) for genres in itertools.combinations(genre_components, size)]\n    genre_combos.update(combo_strs)\n\nax = plot_bg(figsize=(16, 10))\ndromcoms = df[df.genres.isin(genre_combos)]\nif sample_rate != 1:\n    dromcoms = dromcoms.sample(frac=sample_rate, random_state=1)\nfor i, genre in enumerate(genre_components):\n    m = dromcoms[dromcoms.genres.str.contains(genre)]\n    marker = str(i+1)\n    add_annotations(ax, m.index, label=genre, alpha=.5, marker=marker, s=150, linewidths=5)\nplt.legend();","8f4f2451":"Not only are the Harry Potter movies tightly clustered, they're arranged roughly in order of release!\n\n<!-- TODO. Lest you think we just got lucky this time, feel free to click the \"output\" button on any of the below cells to see some more examples of how our mapping places movies from other franchises. -->","6a35a823":"The plot above has a green dot for each of the 8 Harry Potter movies - but they're so close together, they're impossible to distinguish at this scale. That's a good sign!\n\nLet's zoom in to get a closer look.","8b0ca134":"This is an awesome example of structure at the largest scale. Dramas are mostly in the upper-right half, and comedies are mostly in the other half (with romances having a more spread-out, bursty distribution).","18944b49":"## Local vs. Global structure\n\nOne of the key features of t-SNE which makes it so good for visualization is that it's good at capturing clusters *at multiple scales*. We've seen that our mapping is successfully capturing small, tight, local structures. What about bigger structures encompassing more loosely related movies?\n\nWe've already seen a small example of this above: the closest neighbours to the Harry Potter movies are movies from the *Hunger Games* series - another set of movies based on a series of young adult fantasy books. Makes sense!\n\nWhat about less niche genres? Where do documentaries fall?","58909430":"Nice! It's not a tight cluster, but there's definitely a strong pattern here.\n\nAnd just to reiterate: we never actually showed the model genre as a feature. It can't read the titles to see that *Harry Potter and the Philosopher's Stone* and *Harry Potter and the Chamber of Secrets* belong to the same series. It managed to pick up these latent patterns and incorporate them into the embedding space just by seeing data points like \"user 5299 gave movie 806 a rating of 4.5\". Pretty impressive!\n\nHere's another, slightly more complicated genre experiment: visualizing all movies whose genres are a subset of `{Comedy, Drama, Romance}` (i.e. comedies, dramas, romances, dramedies, romantic dramas, romcoms, and... \"dromcoms\" I guess?)","0bbc31f4":"# Your turn!\n\nHead over to [the Exercises notebook](https:\/\/www.kaggle.com\/kernels\/fork\/1599029) to get some hands-on practice working with visualizing embeddings with t-SNE.\n### P.S...\n\nThis course is still in beta, so I'd love to get your feedback. If you have a moment to [fill out a super-short survey about this lesson](https:\/\/form.jotform.com\/82826376784270), I'd greatly appreciate it. You can also leave public feedback in the comments below, or on the [Learn Forum](https:\/\/www.kaggle.com\/learn-forum).\n","f301d9c4":"# Did it work? \n\nIt's hard to judge by the shape alone. A good sanity check is to identify some groups of movies that we strongly believe *should* be close together, and see whether they're close in the 2-d space.\n\nFor example, all the Harry Potter movies should be close together, right?","2f98e2a7":"The whole point of doing this dimensionality reduction was visualization, so let's use matplotlib to draw a scatter plot of our movies, using our new 2-d mapping.","83aa9546":"In the previous lesson, we looked at some examples of the movie embeddings we learned, measured the distances between pairs of movies, looked up movies most similar to certain movies, and had some fun composing the semantics of movies with vector math. These are great ways to debug an embedding model, or understand how it works. But it's also pretty time-consuming.\n\nIn this lesson you'll learn how to use the t-SNE algorithm to visualize embeddings. This is a nice cheap technique for understanding the nature of your embeddings.\n\n# t-SNE?\n\nVisualizing data in 1 or 2 dimensions is easy - but it's not clear how to visualize embeddings which are 8-dimensional or 32-dimensional. t-SNE is a **dimensionality reduction** algorithm which is often used for visualization. It learns a mapping from a set of high-dimensional vectors, to a space with a smaller number of dimensions (usually 2), which is hopefully a good representation of the high-dimensional space.\n\nWhat makes a mapping a \"good representation\"? Put simply, t-SNE tries to make sure that if high-dimensional vectors $\\mathbf{u}$ and $\\mathbf{v}$ are close together, then $map(\\mathbf{u})$ and $map(\\mathbf{v})$ are close together in the 2-d mapping space.\n\n# The code\n\nTo start, we'll load our pretrained embeddings, like before.","f5fcdcf8":"Here's a sample of the 2-d vectors we've mapped our movies to:","41c1633b":"# Further Reading\n\nWe achieved good results using the default out-of-the box parameters for our t-SNE model, but depending on the characteristics of your data, you may not be so lucky.\n\nt-SNE isn't a simple closed-form mathematical operation. You're training a model to minimize some non-convex loss function using stochastic gradient descent. It may take a while, and require a bit of fiddling. You may even see very different results between two t-SNE models trained with the same parameters (set a fixed `random_state` if you want reproducibility).\n\nThe links below have some information you may find helpful if you're getting unsatisfactory results when trying to train a t-SNE model - or if you're just interested in learning more the underlying mathematics and implementation.\n\n- If you're interested in more in-depth, mathematical detail on t-SNE, I highly recommend checking out [the original paper by Laurens van der Maaten and Geoff Hinton that introduced t-SNE to the world](http:\/\/www.jmlr.org\/papers\/volume9\/vandermaaten08a\/vandermaaten08a.pdf).\n- [How to Use t-SNE effectively](https:\/\/distill.pub\/2016\/misread-tsne\/) features some incredible live, interactive examples, allowing you to apply t-SNE to various synthetic datasets and watch the training happen in real-time, and see the effect of changing parameters like the perplexity.\n- The [sklearn TSNE docs](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html) have good information on the meaning of each parameter, and some tips for setting them.\n    - See also: [scikit-learn's t-SNE user guide](http:\/\/scikit-learn.org\/stable\/modules\/manifold.html#t-sne)\n- [t-SNE FAQ](https:\/\/lvdmaaten.github.io\/tsne\/#faq) written by Laurens van der Maaten","bce48254":"We'll be using [scikit-learn's t-SNE implementation](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.manifold.TSNE.html).\n\nI mentioned that t-SNE tries to preserve \"closeness\" between entities in the feature space. As we saw in the previous lessons, there are a lot of competing notions of distance. By default, t-SNE uses euclidean distance. But because cosine distance is known to work well on embeddings, we'll pass in `metric=\"cosine\"` when creating the model.","1e8a89bd":"As we saw in our earlier lesson, our dataset has lots of obscure movies with few ratings (sometimes as few as one). We know so little about these movies that their embeddings are as good as random. We can clarify our visualization by selecting only movies that meet some popularity threshold."}}