{"cell_type":{"54c014bd":"code","f217f32b":"code","b430e4f6":"code","8968b40b":"code","a4ca7c48":"code","42ffe40e":"code","31e461ca":"code","88ad0d5e":"code","757f96f9":"code","d595556c":"code","e22fc4d7":"code","012e2ac7":"code","54bf6277":"code","1e979825":"code","7e5a3c7a":"code","cc34a991":"markdown","48bb668e":"markdown","d1517521":"markdown","b800c20b":"markdown","39bd7f34":"markdown","4c592bc1":"markdown","223aecdf":"markdown","4bef6fca":"markdown"},"source":{"54c014bd":"!pip -q install googletrans","f217f32b":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport random\nfrom googletrans import Translator\nfrom dask import bag, diagnostics\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","b430e4f6":"SEED = 42\nos.environ['PYTHONHASHSEED']=str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)","8968b40b":"train = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\ntest = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\ntest[\"label\"] = -1","a4ca7c48":"df = pd.concat([train, test])\ndf.loc[df[\"label\"]!=-1, \"type\"] = \"train\"\ndf.loc[df[\"label\"]==-1, \"type\"] = \"test\"","42ffe40e":"plt.figure(figsize=(12, 8))\n_ = sns.countplot(x=\"language\", hue=\"type\", data=df)\n_ = plt.title(\"Language Distribution\")","31e461ca":"plt.figure(figsize=(6, 4))\n_ = sns.countplot(x=\"label\", data=train)\n_ = plt.title(\"Label Distribution\")","88ad0d5e":"del df\ngc.collect()","757f96f9":"def translate(words, dest):\n    dest_choices = ['zh-cn',\n                    'ar',\n                    'fr',\n                    'sw',\n                    'ur',\n                    'vi',\n                    'ru',\n                    'hi',\n                    'el',\n                    'th',\n                    'es',\n                    'de',\n                    'tr',\n                    'bg'\n                    ]\n    if not dest:\n        dest = np.random.choice(dest_choices)\n        \n    translator = Translator()\n    decoded = translator.translate(words, dest=dest).text\n    return decoded\n\n\n#TODO: use a dask dataframe instead of all this\ndef trans_parallel(df, dest):\n    premise_bag = bag.from_sequence(df.premise.tolist()).map(translate, dest)\n    hypo_bag =  bag.from_sequence(df.hypothesis.tolist()).map(translate, dest)\n    with diagnostics.ProgressBar():\n        premises = premise_bag.compute()\n        hypos = hypo_bag.compute()\n    df[['premise', 'hypothesis']] = list(zip(premises, hypos))\n    return df","d595556c":"eng = train.loc[train.lang_abv == \"en\"].copy().pipe(trans_parallel, dest=None)\nnon_eng =  train.loc[train.lang_abv != \"en\"].copy().pipe(trans_parallel, dest='en')\ntrain = train.append([eng, non_eng])\ntrain.to_csv(\"train_augmented.csv\", index=False)","e22fc4d7":"eng = test.loc[test.lang_abv == \"en\"].copy().pipe(trans_parallel, dest=None)\nnon_eng =  test.loc[test.lang_abv != \"en\"].copy().pipe(trans_parallel, dest='en')\ntest = test.append([eng, non_eng])\ntest.to_csv(\"test_augmented.csv\", index=False)","012e2ac7":"print(f\"Augmented Training Data: {train.shape}\")\nprint(f\"Augmented Testing Data: {test.shape})","54bf6277":"df = pd.concat([train, test])\ndf.loc[df[\"label\"]!=-1, \"type\"] = \"train\"\ndf.loc[df[\"label\"]==-1, \"type\"] = \"test\"","1e979825":"plt.figure(figsize=(12, 8))\n_ = sns.countplot(x=\"language\", hue=\"type\", data=df)\n_ = plt.title(\"Language Distribution\")","7e5a3c7a":"plt.figure(figsize=(6, 4))\n_ = sns.countplot(x=\"label\", data=train)\n_ = plt.title(\"Label Distribution\")","cc34a991":"Training Data Augmentation","48bb668e":"Testing Data Augmentation","d1517521":"## Basic Exploration","b800c20b":"* Majority of the training & testing samples are of English language and the rest are in minority.\n* Number of samples per language in training data appear to be similar\n* Langauges in training and test dataset appear in similar ratios","39bd7f34":"Since, english sentences were translated to another language, chosen randomly. Number of sentences belonging to each class is quite different from the original setup ","4c592bc1":"## Translation","223aecdf":"# Data Augmentation with Translation\n\nUsing translation to augment data and increasing the training datasize. Code taken from John Miller's [Augmenting Data with Translation](https:\/\/www.kaggle.com\/jpmiller\/augmenting-data-with-translations) kernel.\n\nXLM-Roberta performs better when provided with translations. Here, Google translate is used to achieve the following - \n* Translate non-english sentences to English.\n* Translate English sentences to a randomly chosen language","4bef6fca":"Label distribution is the same as in the original dataset as expected."}}