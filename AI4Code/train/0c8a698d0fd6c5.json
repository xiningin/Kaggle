{"cell_type":{"f904c286":"code","9a54542c":"code","ac4d94bc":"code","c9eab199":"code","01d5b38b":"code","5229437f":"code","381061f9":"code","7373535c":"code","8a7a6c70":"code","2636360c":"code","57759238":"code","0ef983ba":"code","eda85777":"code","58db2e4c":"code","f424352e":"code","ee8f94c8":"code","32314ebd":"code","f7e938ce":"code","ebbb7926":"code","4de01dae":"code","01774c39":"code","cc97c53d":"code","54843f15":"code","cea4019f":"code","ce067a30":"code","d9037757":"code","ec375876":"code","37dfce62":"code","80403447":"code","4b5a523b":"code","3c25a796":"code","0fe1d015":"code","85089776":"code","a2cea673":"code","a1078c31":"code","29506c4c":"code","51449db2":"code","150406f5":"code","c2068551":"code","8f69f8c6":"code","216c01e1":"code","8b6cf968":"code","6a3830d1":"code","73e00575":"code","af617f1c":"code","b3daa213":"code","c4c92967":"code","2844be2f":"code","57b8d698":"code","97f2e610":"code","49645687":"code","7bfd77b1":"code","31c5a966":"code","5d1c5964":"code","4da73ef9":"code","7e783425":"code","bfcb0b58":"code","7a90c1cb":"code","f94d1524":"code","6d0fdded":"code","64604174":"code","de176db9":"code","171f95f3":"code","147fa75f":"code","a06e292c":"code","39a6155b":"code","24aea39b":"code","621700bd":"code","92c51641":"code","b4a3d796":"code","ebb0551d":"code","f18c8ae6":"code","4115eddb":"code","0f69c5ff":"code","017cb721":"code","e4cbd9b8":"code","380e2fd4":"code","0eae19bf":"code","5a6074a3":"code","f0aaf199":"code","1edff584":"code","55884072":"code","649a31fb":"code","732dda84":"code","5d4d0f7b":"code","336e03ce":"code","294fde9f":"code","ccaa0c55":"code","b64a75ad":"code","4ffd165f":"code","61290a50":"code","a50df4a4":"markdown","ace9e2f3":"markdown","08f75e32":"markdown","66f8dd1b":"markdown","abe57e1a":"markdown","46e2530e":"markdown","2dba3d78":"markdown","9cad682e":"markdown","f747cb19":"markdown","c303c33d":"markdown","e58ae77a":"markdown","69109eb0":"markdown","5bf14639":"markdown","9fdaef68":"markdown","69851837":"markdown","c9b4b616":"markdown","d50dd856":"markdown","38f2e081":"markdown","12fa8267":"markdown","ac5981f6":"markdown","040aab16":"markdown","39e5c84b":"markdown","f3a8bb6a":"markdown","d96c4130":"markdown","67be2847":"markdown","5c8f91ed":"markdown","88f11496":"markdown","83c32e6c":"markdown","f4a8e7eb":"markdown","b7165c29":"markdown","5b48891b":"markdown","1704c0e0":"markdown","8564e127":"markdown","53ba436e":"markdown","fe5c1510":"markdown","7b4b20df":"markdown","1ab280a6":"markdown","bfce73c9":"markdown","bcec4cf4":"markdown","a283986a":"markdown","99675f61":"markdown","abab6e36":"markdown","dabc542e":"markdown","f47704c2":"markdown","4e2f2c3c":"markdown","473edf78":"markdown","5205d7b0":"markdown","cd50422e":"markdown","92092c89":"markdown","7000dd14":"markdown","1a6c5e3c":"markdown","ead5f964":"markdown","ae39c5a7":"markdown","15139197":"markdown","0c65bca9":"markdown","2f42e7cd":"markdown","c3cbf2a7":"markdown","a80daacc":"markdown","8d0edeb6":"markdown","ed41411b":"markdown","b5fb3a75":"markdown","08a3cab2":"markdown","04656589":"markdown","04f6b9c6":"markdown","d0cbdc10":"markdown","f4dd7d49":"markdown","e15ea466":"markdown","73b4efed":"markdown","b753a9ac":"markdown","b5256206":"markdown","43ef6ba3":"markdown","52a8ce23":"markdown","b72aaedb":"markdown","47658559":"markdown","79f2af5b":"markdown","b63efded":"markdown","a027ea56":"markdown","64a9d3d1":"markdown","6bf5d2f0":"markdown","cb8a3bf6":"markdown","3a6af7d9":"markdown","ab0745c8":"markdown","0b688385":"markdown","57532e38":"markdown"},"source":{"f904c286":"import pandas as pd       \nimport matplotlib as mat\nfrom matplotlib import rcParams\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn import metrics\nfrom sklearn.neural_network import MLPClassifier\nimport scipy\nfrom scipy.integrate import simpson\n#pip install plotly==5.5.0 \nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, QuantileTransformer\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, auc, ConfusionMatrixDisplay, confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import GaussianNB, ComplementNB\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.utils import class_weight\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\nfrom tabulate import tabulate\n#pip install shap\nimport shap\nimport warnings\n#warnings.filterwarnings('ignore')\n#pip install xgboost\nimport xgboost as xgb\nfrom functools import partial\n#pip install optuna\nimport optuna\npd.set_option('max_column', None)\nplt.style.use(\"ggplot\")\nsns.set_style('darkgrid')\npd.set_option('max_column', None)\nsns.set(font_scale=1)\nSEED=42\nNSPLITS = 5\nSHUFFLE =True","9a54542c":"def decile_table(y_true, y_prob, labels=True, round_decimal=3):\n    \"\"\"Generates the Decile Table from labels and probabilities\n    \n    The Decile Table is creared by first sorting the customers by their predicted \n    probabilities, in decreasing order from highest (closest to one) to \n    lowest (closest to zero). Splitting the customers into equally sized segments, \n    we create groups containing the same numbers of customers, for example, 10 decile \n    groups each containing 10% of the customer base.\n    \n    Args:\n        y_true (array-like, shape (n_samples)):\n            Ground truth (correct\/actual) target values.\n        y_prob (array-like, shape (n_samples, n_classes)):\n            Prediction probabilities for each class returned by a classifier\/algorithm.\n        labels (bool, optional): If True, prints a legend for the abbreviations of\n            decile table column names. Defaults to True.\n        round_decimal (int, optional): The decimal precision till which the result is \n            needed. Defaults to '3'.\n    Returns:\n        dt: The dataframe dt (decile-table) with the deciles and related information.\n    Example:\n        >>> import kds\n        >>> from sklearn.datasets import load_iris\n        >>> from sklearn.model_selection import train_test_split\n        >>> from sklearn import tree\n        >>> X, y = load_iris(return_X_y=True)\n        >>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,random_state=3)\n        >>> clf = tree.DecisionTreeClassifier(max_depth=1,random_state=3)\n        >>> clf = clf.fit(X_train, y_train)\n        >>> y_prob = clf.predict_proba(X_test)\n        >>> kds.metrics.decile_table(y_test, y_prob[:,1])\n    \"\"\"\n    y_true = np.array(y_true)\n    y_prob = np.array(y_prob)\n\n    df = pd.DataFrame()\n    df['y_true'] = y_true\n    df['y_prob'] = y_prob\n    # df['decile']=pd.qcut(df['y_prob'], 10, labels=list(np.arange(10,0,-1))) \n    # ValueError: Bin edges must be unique\n\n    df.sort_values('y_prob', ascending=False, inplace=True)\n    df['decile'] = np.linspace(1, 11, len(df), False, dtype=int)\n\n    # dt abbreviation for decile_table\n    dt = df.groupby('decile').apply(lambda x: pd.Series([\n        np.min(x['y_prob']),\n        np.max(x['y_prob']),\n        np.mean(x['y_prob']),\n        np.size(x['y_prob']),\n        np.sum(x['y_true']),\n        np.size(x['y_true'][x['y_true'] == 0]),\n    ],\n        index=([\"prob_min\", \"prob_max\", \"prob_avg\",\n                \"cnt_cust\", \"cnt_resp\", \"cnt_non_resp\"])\n    )).reset_index()\n\n    dt['prob_min']=dt['prob_min'].round(round_decimal)\n    dt['prob_max']=dt['prob_max'].round(round_decimal)\n    dt['prob_avg']=round(dt['prob_avg'],round_decimal)\n    # dt=dt.sort_values(by='decile',ascending=False).reset_index(drop=True)\n\n    tmp = df[['y_true']].sort_values('y_true', ascending=False)\n    tmp['decile'] = np.linspace(1, 11, len(tmp), False, dtype=int)\n\n    dt['cnt_resp_rndm'] = np.sum(df['y_true']) \/ 10\n    dt['cnt_resp_wiz'] = tmp.groupby('decile', as_index=False)['y_true'].sum()['y_true']\n\n    dt['resp_rate'] = round(dt['cnt_resp'] * 100 \/ dt['cnt_cust'], round_decimal)\n    dt['cum_cust'] = np.cumsum(dt['cnt_cust'])\n    dt['cum_resp'] = np.cumsum(dt['cnt_resp'])\n    dt['cum_resp_wiz'] = np.cumsum(dt['cnt_resp_wiz'])\n    dt['cum_non_resp'] = np.cumsum(dt['cnt_non_resp'])\n    dt['cum_cust_pct'] = round(dt['cum_cust'] * 100 \/ np.sum(dt['cnt_cust']), round_decimal)\n    dt['cum_resp_pct'] = round(dt['cum_resp'] * 100 \/ np.sum(dt['cnt_resp']), round_decimal)\n    dt['cum_resp_pct_wiz'] = round(dt['cum_resp_wiz'] * 100 \/ np.sum(dt['cnt_resp_wiz']), round_decimal)\n    dt['cum_non_resp_pct'] = round(\n        dt['cum_non_resp'] * 100 \/ np.sum(dt['cnt_non_resp']), round_decimal)\n    dt['KS'] = round(dt['cum_resp_pct'] - dt['cum_non_resp_pct'], round_decimal)\n    dt['lift'] = round(dt['cum_resp_pct'] \/ dt['cum_cust_pct'], round_decimal)\n    if labels is True:\n        print_labels()\n    return dt[['decile', 'cum_resp_pct', 'cum_resp_pct_wiz']]","ac4d94bc":"def area_ratio(pcg):\n    # Area of the model \n    area_model = auc(np.append(0, pcg.decile.values), np.append(0, pcg.cum_resp_pct.values))\n    # Area of the base model\n    area_base = auc(np.append(np.arange(0, 100, 10), 100), np.append(np.arange(0, 10, 1), 10))\n    # Area between model and base\n    diff_base_model = area_model - area_base\n    # Area of the wizard\n    area_wizard = auc(np.append(0, pcg.decile.values), np.append(0, pcg.cum_resp_pct_wiz.values))\n    # Area between wizard and base\n    diff_wizard_base = area_wizard - area_base\n    # area ratio\n    area_ratio = diff_base_model \/ diff_wizard_base\n    return area_ratio","c9eab199":"def plot_lift_chart(pcg, area_ratio, title='Cumulative Gain Plot',\n                         title_fontsize=14, text_fontsize=10, figsize=None):\n    \"\"\"Generates the cumulative Gain Plot from labels and probabilities\n    The cumulative gains chart is used to determine the effectiveness of a\n    binary classifier. A detailed explanation can be found at\n    http:\/\/www2.cs.uregina.ca\/~dbd\/cs831\/notes\/lift_chart\/lift_chart.html \n    The implementation here works only for binary classification.\n    \n    Args:\n        y_true (array-like, shape (n_samples)):\n            Ground truth (correct) target values.\n        y_prob (array-like, shape (n_samples, n_classes)):\n            Prediction probabilities for each class returned by a classifier.\n        title (string, optional): Title of the generated plot. Defaults to\n            \"Decile-wise Lift Plot\".\n        title_fontsize (string or int, optional): Matplotlib-style fontsizes.\n            Use e.g. \"small\", \"medium\", \"large\" or integer-values (8, 10, 12, etc.)\n            Defaults to 14.\n        text_fontsize (string or int, optional): Matplotlib-style fontsizes.\n            Use e.g. \"small\", \"medium\", \"large\" or integer-values (8, 10, 12, etc.)\n            Defaults to 10.\n        figsize (2-tuple, optional): Tuple denoting figure size of the plot\n            e.g. (6, 6). Defaults to ``None``.\n    Returns:\n        None\n    Example:\n        >>> import kds\n        >>> from sklearn.datasets import load_iris\n        >>> from sklearn.model_selection import train_test_split\n        >>> from sklearn import tree\n        >>> X, y = load_iris(return_X_y=True)\n        >>> X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,random_state=3)\n        >>> clf = tree.DecisionTreeClassifier(max_depth=1,random_state=3)\n        >>> clf = clf.fit(X_train, y_train)\n        >>> y_prob = clf.predict_proba(X_test)\n        >>> kds.metrics.plot_cumulative_gain(y_test, y_prob[:,1])\n    \"\"\"\n\n    # Cumulative Gains Plot\n    # plt.subplot(2, 2, 3)\n    plt.plot(np.append(0, pcg.decile.values), np.append(0, pcg.cum_resp_pct.values), marker='o', label='Model')\n    plt.plot(np.append(0, pcg.decile.values), np.append(0, pcg.cum_resp_pct_wiz.values), 'c--', label='Wizard')\n    # plt.plot(list(np.arange(1,11)), np.ones(10), 'k--',marker='o')\n    plt.plot([0, 10], [0, 100], 'k--', marker='o', label='Random')\n    plt.text(4, 30, f\"Area ratio: {area_ratio}\")\n    plt.title(title, fontsize=title_fontsize)\n    plt.xlabel('Deciles', fontsize=text_fontsize)\n    plt.ylabel('% Resonders', fontsize=text_fontsize)\n    plt.legend(borderpad=1)\n    plt.grid(True)","01d5b38b":"df= pd.read_excel(r'default of credit card clients.xls',header = 1)","5229437f":"df.drop(['ID'],axis=1, inplace=True)","381061f9":"df.rename(columns={'PAY_0' : 'PAY_1', 'default payment next month' : 'Default'}, inplace=True)","7373535c":"df.loc[:,'EDUCATION'] = df.loc[:,'EDUCATION'].replace(0,5)\ndf.loc[:,'EDUCATION'] = df.loc[:,'EDUCATION'].replace(6,5)\ndf.loc[:,'EDUCATION'] = df.loc[:,'EDUCATION'].replace(5,4)","8a7a6c70":"data = df.copy()","2636360c":"sns.set(font_scale=1)\ntotal_cnt = df['Default'].count()\nrcParams['figure.figsize'] = 10,6\nsns.set(font_scale= 2)\nsns.set_style(\"white\")\nax = sns.countplot(x=\"Default\" , data=df, palette = 'Blues_r')\nax.set_title('Defaulted Count')\nplt.legend(loc = 'upper right')\nfor p in ax.patches:\n    x, height , width = p.get_x() , p.get_height() , p.get_width()\n    ax.text(x + width \/ 2, height + 10, f'{height} \/ {height \/ total_cnt * 100:2.1f}%', va='center', ha='center', size=20)\n\nsns.despine()","57759238":"sns.set(font_scale=1)\ntotal_cnt = df['Default'].count()\nsns.set(font_scale = 2)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nplt.figure(figsize=(12,8))\ndf['Default'].value_counts().plot.pie(autopct = '%1.1f%%', \n                                               shadow = True,\n                                               colors = ['grey', 'green'])\nplt.legend(labels=['No Default', 'Default'])\nplt.show()","0ef983ba":"sns.set(font_scale=1)\nplt.figure(figsize=(12,8))\nax = sns.countplot(x='SEX' ,hue=\"Default\", data=df,  palette = 'Blues_r')\nplt.xlabel('Sex')\nplt.ylabel('Number of clients')\nplt.ylim(0,20000)\nplt.xticks([0,1], ['Male', 'Female'])\nfor p in ax.patches:\n    ax.annotate((p.get_height()), (p.get_x()+0.16, p.get_height()+1000))\n    #print(p.get_height())\n    #print(p.get_x()+0.16, p.get_height()+1000)\nplt.show()","eda85777":"sns.set(font_scale=1)\nplt.figure(figsize=(10,4))\nax = sns.barplot(x=\"SEX\", y=\"Default\", data=df,palette = 'Blues_r' ,ci=None)\nplt.ylabel(\"% of Default\", fontsize=12)\nplt.ylim(0,0.5)\nplt.xticks([0,1],['Male', 'Female'], fontsize = 12)\n\nfor p in ax.patches:\n    ax.annotate(\"%.2f\" %(p.get_height()), (p.get_x()+0.35, p.get_height()+0.03),fontsize=13)\nplt.show()","58db2e4c":"sns.set(font_scale=1)\nplt.figure(figsize=(12,4))\nax = sns.countplot(data=df , x='EDUCATION', hue=\"Default\",palette = 'flare' )\nplt.xlabel(\"Education\", fontsize= 12)\nplt.ylabel(\"# of Clients\", fontsize= 12)\nplt.ylim(0,12000)\nplt.xticks([0,1,2,3],['Grad school', 'University', 'High School','Others'])\nfor p in ax.patches:\n    ax.annotate((p.get_height()), (p.get_x()+0.11, p.get_height()+500))\n\nplt.show()","f424352e":"sns.set(font_scale=1)\nplt.figure(figsize=(12,4))\nax = sns.barplot(data=df , x='EDUCATION', y=\"Default\",palette = 'flare' , ci=None)\nplt.ylabel(\"% of Default\", fontsize= 12)\nplt.ylim(0,0.5)\nplt.xticks([0,1,2,3],['Grad school', 'University', 'High School','Others'],fontsize=11)\nfor p in ax.patches:\n    ax.annotate(\"%.2f\" %(p.get_height()), (p.get_x()+0.30, p.get_height()+0.03), fontsize=14)\n\nplt.show()","ee8f94c8":"plt.figure(figsize=(12,4))\nsns.set(font_scale=1)\nax = sns.barplot(x = \"MARRIAGE\", y = \"Default\", data = df, palette = 'rocket', ci = None)\n\nplt.ylabel(\"% of Default\", fontsize= 12)\nplt.ylim(0,0.5)\nplt.xticks([0,1,2,3],['Unknown', 'Married', 'Single', 'Divorce'], fontsize = 11)\n\nfor p in ax.patches:\n    ax.annotate(\"%.2f\" %(p.get_height()), (p.get_x()+0.30, p.get_height()+0.03),fontsize=13)\n\nplt.show()","32314ebd":"plt.figure(figsize=(12,8))\nsns.set(font_scale=1)\nax = sns.barplot(x = \"SEX\", y = \"Default\", hue = \"MARRIAGE\", data = df, palette = 'rocket', ci = None)\n\nplt.ylabel(\"% of Default\", fontsize= 12)\nplt.ylim(0,0.5)\nplt.xticks([0,1],['Male', 'Female'], fontsize = 12)\n\nfor p in ax.patches:\n    ax.annotate(\"%.2f\" %(p.get_height()), (p.get_x()+0.06, p.get_height()+0.03),fontsize=12)\n\nplt.show()","f7e938ce":"plt.figure(figsize=(12,8))\nsns.set(font_scale=1)\nax = sns.barplot(x = \"SEX\", y = \"Default\", hue = \"EDUCATION\", data = df, palette = 'rocket', ci = None)\n\nplt.ylabel(\"% of Default\", fontsize= 12)\nplt.ylim(0,0.5)\nplt.xticks([0,1],['Male', 'Female'], fontsize = 12)\n\nfor p in ax.patches:\n    ax.annotate(\"%.2f\" %(p.get_height()), (p.get_x()+0.035, p.get_height()+0.03),fontsize=12)\n\nplt.show()","ebbb7926":"sns.set(font_scale=1)\nplt.figure(figsize=(12,6))\nsns.distplot(df['AGE'])\nplt.ticklabel_format(style='plain', axis='x')\nplt.ylabel('')\nplt.show()","4de01dae":"sns.set(font_scale=1)\nplt.figure(figsize=(12,4))\nsns.kdeplot(df.loc[(df['Default'] == 0), 'AGE'], label='No Default', shade=True) # kernel density estiamtion \nsns.kdeplot(df.loc[(df['Default'] == 1), 'AGE'] ,label='Default', shade=True)\nplt.ticklabel_format(style='plain', axis='x') \nplt.ylabel('')\nplt.legend()\nplt.show()","01774c39":"df['AgeBin'] = pd.cut(df['AGE'], [20,25,30,35,40,50,60,80])   # bin values into discrete intervals\nprint(df['AgeBin'].value_counts())","cc97c53d":"plt.figure(figsize=(12,4))\nsns.set(font_scale=1)\ndf['AgeBin'] = df['AgeBin'].astype('str')\nAgeBin_order = ['(20, 25]', '(25, 30]', '(30, 35]', '(35, 40]', '(40, 50]', '(50, 60]', '(60, 80]']\n\nax = sns.countplot(data = df, x = 'AgeBin', hue=\"Default\", palette = 'rocket', order = AgeBin_order)\n\nplt.xlabel(\"Age Group\", fontsize= 12)\nplt.ylabel(\"# of Clients\", fontsize= 12)\nplt.ylim(0,8000)\n\nfor p in ax.patches:\n    ax.annotate((p.get_height()), (p.get_x()+0.075, p.get_height()+300))\n\nplt.show()","54843f15":"plt.figure(figsize=(12,4))\nsns.set(font_scale=1)\nax = sns.barplot(x = 'AgeBin', y=\"Default\", data=df , palette = 'rocket', ci = None, order = AgeBin_order)\n\nplt.xlabel(\"Age Group\", fontsize= 12)\nplt.ylabel(\"% of Default\", fontsize= 12)\nplt.ylim(0,0.5)\n\nfor p in ax.patches:\n    ax.annotate(\"%.2f\" %(p.get_height()), (p.get_x()+0.25, p.get_height()+0.03),fontsize=13)\n\nplt.show()","cea4019f":"plt.figure(figsize=(12,4))\nsns.set(font_scale=1)\nsns.distplot(df['LIMIT_BAL'])\nplt.ticklabel_format(style='plain', axis='x') #repressing scientific notation on x\nplt.ylabel('')\nplt.show()","ce067a30":"plt.figure(figsize=(12,4)) \nsns.set(font_scale=1)\n# kernel density estimation\nsns.kdeplot(df.loc[(df['Default'] == 0), 'LIMIT_BAL'], label = 'No default', shade=True)\nsns.kdeplot(df.loc[(df['Default'] == 1), 'LIMIT_BAL'], label = 'Default', shade = True)\nplt.ticklabel_format(style='plain', axis='x') \nplt.ylabel('')\nplt.legend()\nplt.show()","d9037757":"df['LimitBin'] = pd.cut(df['LIMIT_BAL'],[5000, 50000, 100000, 150000, 200000, 300000, 400000, 500000, 1100000])","ec375876":"plt.figure(figsize=(14,4))\nsns.set(font_scale=1)\ndf['LimitBin'] = df['LimitBin'].astype('str') # astype() : cast a pandas object to a specified dtype\n\nLimitBin_order = ['(5000, 50000]', '(50000, 100000]', '(100000, 150000]', '(150000, 200000]',\n                '(200000, 300000]', '(300000, 400000]', '(400000, 500000]', '(500000, 1100000]']\n\nax = sns.countplot(data=df, x='LimitBin' , hue=\"Default\", palette = 'rocket', order = LimitBin_order)\n\nplt.xlabel(\"Amount of Given Credit\", fontsize= 12)\nplt.ylabel(\"# of Clients\", fontsize= 12)\nplt.ylim(0,8000)\nax.tick_params(axis=\"x\", labelsize= 9.5)\n\nfor p in ax.patches:\n    ax.annotate((p.get_height()), (p.get_x()+0.075, p.get_height()+300))\n    \nplt.show()","37dfce62":"plt.figure(figsize=(14,4))\nsns.set(font_scale=1)\nax = sns.barplot(x = \"LimitBin\", y = \"Default\", data = df, palette = 'rocket', ci = None, order = LimitBin_order)\n\n \nplt.xlabel(\"Amount of Given Credit\", fontsize= 12)\nplt.ylabel(\"% of Default\", fontsize= 12)\nplt.ylim(0,0.5)\n\nfor p in ax.patches:\n    ax.annotate(\"%.2f\" %(p.get_height()), (p.get_x()+0.25, p.get_height()+0.03),fontsize=13)\n\nplt.show()","80403447":"plt.figure(figsize=(12,6))\nsns.set(font_scale=1)\nsns.boxplot(x=\"SEX\", y=\"LIMIT_BAL\",data=df, palette = 'rocket', showmeans=True, \n           meanprops={\"markerfacecolor\":\"red\", \"markeredgecolor\":\"black\",\"markersize\":\"10\"})\nplt.ticklabel_format(style='plain', axis='y')\nplt.xticks([0,1],['Male','female'], fontsize = 12)\nplt.show()","4b5a523b":"plt.figure(figsize=(14,6))\nsns.set(font_scale=1)\nsns.boxplot(x = \"EDUCATION\", y = \"LIMIT_BAL\", data = df, palette = 'rocket', showmeans=True, \n            meanprops={\"markerfacecolor\":\"red\",  \"markeredgecolor\":\"black\", \"markersize\":\"10\"})\n\nplt.ticklabel_format(style='plain', axis='y') #repressing scientific notation   \nplt.xticks([0,1,2,3,4],['Grad School','University','High School','Others','Unknown'], fontsize = 11)\n\nplt.show()","3c25a796":"plt.figure(figsize=(14,6))\nsns.set(font_scale=1)\nsns.boxplot(x = \"MARRIAGE\", y = \"LIMIT_BAL\", data = df, palette = 'rocket', showmeans=True, \n            meanprops={\"markerfacecolor\":\"red\",  \"markeredgecolor\":\"black\", \"markersize\":\"10\"})\n\nplt.ticklabel_format(style='plain', axis='y') #repressing scientific notation    \nplt.xticks([0,1,2,3],['Unknown', 'Married', 'Single', 'Divorce'], fontsize = 11)\n\nplt.show()","0fe1d015":"plt.figure(figsize=(14,6))\nsns.set(font_scale=1)\nsns.boxplot(x = \"AgeBin\", y = \"LIMIT_BAL\",data = df, palette = 'rocket', order = AgeBin_order, showmeans=True, \n            meanprops={\"markerfacecolor\":\"red\",  \"markeredgecolor\":\"black\", \"markersize\":\"10\"})\n\nplt.ticklabel_format(style='plain', axis='y') #repressing scientific notation    \nplt.xlabel(\"Age Group\", fontsize= 12)\n\nplt.show()","85089776":"sns.set(font_scale=1)\nplt.figure(figsize=(14,6))\nsns.boxplot(x=\"EDUCATION\" , y=\"LIMIT_BAL\", hue='SEX',data=df, palette = 'rocket', showmeans=True, \n            meanprops={\"markerfacecolor\":\"red\",  \"markeredgecolor\":\"black\", \"markersize\":\"10\"})\nplt.ticklabel_format(style='plain', axis='y') #repressing scientific notation   \nplt.xticks([0,1,2,3,4],['Grad School','University','High School','Others','Unknown'], fontsize = 11)\n\nplt.show()","a2cea673":"plt.figure(figsize=(14,6))\nsns.set(font_scale=1)\nsns.boxplot(x = \"MARRIAGE\", y = \"LIMIT_BAL\", hue = 'SEX', data = df, palette = 'rocket', showmeans=True, \n            meanprops={\"markerfacecolor\":\"red\",  \"markeredgecolor\":\"black\", \"markersize\":\"10\"})\n\nplt.ticklabel_format(style='plain', axis='y') #repressing scientific notation    \nplt.xticks([0,1,2,3],['Unknown', 'Married', 'Single', 'Divorce'], fontsize = 11)\n\nplt.show()","a1078c31":"sns.set(font_scale=1)\nplt.figure(figsize=(14,6))\n\nsns.boxplot(x = \"AgeBin\", y = \"LIMIT_BAL\", hue = 'SEX', data = df, palette = 'rocket', order = AgeBin_order, showmeans=True, \n            meanprops={\"markerfacecolor\":\"red\",  \"markeredgecolor\":\"black\", \"markersize\":\"10\"})\n\nplt.ticklabel_format(style='plain', axis='y') #repressing scientific notation    \nplt.xlabel(\"Age Group\", fontsize= 12)\n\nplt.show()","29506c4c":"pay_x_fts = ['PAY_1', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\nplt.figure(figsize=(15,12))\nsns.set(font_scale=1)\nfor i,col in enumerate(pay_x_fts):\n    plt.subplot(3,2,i+1)\n    ax = sns.countplot(df.loc[:,col], palette = 'rocket')\n    plt.ylim(0,20000)\n    plt.ylabel('')\n    plt.tight_layout()\n    \n    for p in ax.patches:\n        ax.annotate((p.get_height()) , (p.get_x() + 0.08 , p.get_height()+500), fontsize = 11)\nplt.show()","51449db2":"plt.figure(figsize=(15,12))\nsns.set(font_scale=1)\nfor i,col in enumerate(pay_x_fts):\n    plt.subplot(3,2,i + 1)\n    ax = sns.barplot(x=col, y=\"Default\", data=df, palette = 'rocket', ci = None)\n    plt.ylabel(\"% of Default\", fontsize= 12)\n    plt.ylim(0,1.2)\n    plt.tight_layout()\n    \n    for p in ax.patches:\n        ax.annotate(\"%.2f\" %(p.get_height()), (p.get_x()+0.09, p.get_height()+0.03),fontsize=13)\n\nplt.show()","150406f5":"sns.set(font_scale=1)\n\nbill_amtx_fts = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\nplt.figure(figsize=(15,12))\n\nfor i,col in enumerate(bill_amtx_fts):\n    plt.subplot(3,2,i+1)\n    sns.distplot(df.loc[:,col])\n    plt.ticklabel_format(style='plain', axis='x') #repressing scientific notation    \n    plt.ylabel('')\n    plt.tight_layout()\n\nplt.show()","c2068551":"plt.figure(figsize=(15,12))\nsns.set(font_scale=1)\nfor i,col in enumerate(bill_amtx_fts):\n    plt.subplot(3,2,i+1)\n    sns.kdeplot(df.loc[(df['Default'] == 0), col], label = 'No Default', shade=True)\n    sns.kdeplot(df.loc[(df['Default'] == 1), col], label = 'Default', shade = True)\n    plt.xlim(-50000,200000)\n    plt.ylabel('')\n    plt.legend()\n    plt.tight_layout()\nplt.show()","8f69f8c6":"df['BILL_AMT1_bin'] = df['BILL_AMT1'].copy()\ndf['BILL_AMT2_bin'] = df['BILL_AMT2'].copy()\ndf['BILL_AMT3_bin'] = df['BILL_AMT3'].copy()\ndf['BILL_AMT4_bin'] = df['BILL_AMT4'].copy()\ndf['BILL_AMT5_bin'] = df['BILL_AMT5'].copy()\ndf['BILL_AMT6_bin'] = df['BILL_AMT6'].copy()","216c01e1":"bill_amtx_bins = ['BILL_AMT1_bin', 'BILL_AMT2_bin', 'BILL_AMT3_bin', 'BILL_AMT4_bin', 'BILL_AMT5_bin', 'BILL_AMT6_bin']\n\nfor i,col in enumerate(bill_amtx_bins):\n    df[col] = pd.cut(df[bill_amtx_bins[i]],[-350000,-1,0,25000, 75000, 200000, 2000000] )","8b6cf968":"plt.figure(figsize=(15,12))\nsns.set(font_scale=1) \nfor i,col in enumerate(bill_amtx_bins):    \n    plt.subplot(3,2,i + 1)\n    ax = sns.countplot(data = df, x = col, hue=\"Default\", palette = 'rocket')\n    plt.ylim(0,13000)\n    plt.ylabel('')\n    plt.xticks([0,1,2,3,4,5],['0 <', '= 0', '0-25k', '25k-75k', '75k-200k', '>200k'], fontsize = 11)\n    plt.tight_layout()\n\n    for p in ax.patches:\n        ax.annotate((p.get_height()), (p.get_x()+0.04, p.get_height()+700), fontsize = 11)    \n        \nplt.show()","6a3830d1":"pay_amtx_fts = ['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\nplt.figure(figsize=(15,12))\nsns.set(font_scale=1) \n\n\nfor i,col in enumerate(pay_amtx_fts):\n    plt.subplot(3,2,i+1)\n    sns.distplot(df.loc[:,col])\n    plt.ticklabel_format(style='plain', axis='x') \n    plt.ylabel('')\n    plt.tight_layout()\n\nplt.show()","73e00575":"plt.figure(figsize=(15,12))\nsns.set(font_scale=1) \n\n\nfor i,col in enumerate(pay_amtx_fts):\n    plt.subplot(3,2,i + 1)   \n    sns.kdeplot(df.loc[(df.Default == 0), col ],label  = 'No Default', shade=True )\n    sns.kdeplot(df.loc[(df.Default == 1), col ], label  = 'Default', shade = True )\n    plt.xlim(0,100000)\n    plt.ylabel('')\n    plt.legend()\n    plt.tight_layout()\n\nplt.show()","af617f1c":"plt.subplots(figsize=(30,20))\nsns.heatmap(data.corr(), annot=True)\nplt.show()","b3daa213":"df=data.copy()\ndf","c4c92967":"dataframe= df.copy()","2844be2f":"bill_amt = ['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\npay_amt  = ['PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\nbill_statement_sum = dataframe[bill_amt].sum(axis=1)\nprint(f\"Number of observation with zero total bill amount {sum(bill_statement_sum == 0)}\")\nbill_statement_sum_index = bill_statement_sum.loc[bill_statement_sum > 0].index\ndataframe = dataframe.loc[bill_statement_sum_index]\ndataframe.shape","57b8d698":"dataframe = dataframe.drop_duplicates(subset=[col for col in dataframe.columns if col != 'Default'])# drop duplicated rows\nfeatures_col = [col for col in dataframe.columns if col not in ['Default']]\nfeatures = dataframe[features_col]\ntarget = dataframe['Default']\ndataframe.shape","97f2e610":"SEED=2020\nX, X_test, y, y_test = train_test_split(features,\n                                        target,\n                                        test_size=.2,\n                                        shuffle=True,\n                                        random_state=SEED,\n                                        stratify=target\n                                        )","49645687":"# proportion des modalit\u00e9s de 'default' entre train set et test set\nunique_train, count_train = np.unique(y, return_counts=True)\nprint(f\"y train value counts: \\n {np.asanyarray((unique_train, count_train\/y.shape[0])).T}\")\nprint('\\n')\nunique_test, count_test = np.unique(y_test, return_counts=True)\nprint(f\"y test value counts: \\n {np.asanyarray((unique_test, count_test\/y_test.shape[0])).T}\")","7bfd77b1":"scaler = MinMaxScaler() # StandardScaler MinMaxScaler\nscaler.fit(X)\nscaled_x = scaler.transform(X)\nscaled_test = scaler.transform(X_test)\nX.loc[:, features_col] = scaled_x\nX_test.loc[:, features_col] = scaled_test","31c5a966":"nnet_article_train = X.copy()\nnnet_article_test  = X_test.copy()\nscaler = StandardScaler()\nscaler.fit(nnet_article_train) # Fit scaler\n# Scale train data for logistic regression\nscaled_train = scaler.transform(nnet_article_train) \nnnet_article_train.loc[:, features_col] = scaled_train\n# Scale test data for logistic regression\nscaled_test = scaler.transform(nnet_article_test)\nnnet_article_test.loc[:, features_col] = scaled_test","5d1c5964":"# Classifier implementing the k-nearest neighbors vote.\nknn = KNeighborsClassifier(n_neighbors=10, #number of neighbors to use\n                           algorithm='kd_tree', # algo used to compute the nearest neighbors\n                           n_jobs=-1, # number of parallel jobs to run for neighbors search : -1 means using all processors.\n                           weights='uniform' # Weight function used in prediction : uniform means that all the points in each neighborhood are weighted equally.\n                          )\nknn.fit(X, y) # fit knn classifier \ntrain_pred_knn = knn.predict(X) # returns labels of the train data\ntrain_proba_knn = knn.predict_proba(X) # returns probability estimates for the train data \ntest_pred_knn = knn.predict(X_test) # returns labels of the test data\ntest_proba_knn = knn.predict_proba(X_test) # returns probability estimates for the test data ","4da73ef9":"%%time\n\nNSPLITS = 5 # number of folds\nSHUFFLE =True\n\noof_label_knn_cv = np.zeros((X.shape[0]))\noof_proba_knn_cv = np.zeros((X.shape[0], 2))\ntest_proba_knn_cv = np.zeros((X_test.shape[0], 2))\n\nerror_rate_valids_knn_cv = []\narea_ratio_valids_knn_cv = []\n\n\nskf = StratifiedKFold(n_splits=NSPLITS, shuffle=SHUFFLE, random_state=SEED)\n# Stratified K-Folds provides train and test indices to split the data in train and test folds.\n# the folds are made by preserving the percentage of samples for each class.\n\nknn_cv = KNeighborsClassifier(n_neighbors=10, algorithm='kd_tree', n_jobs=-1)\n\ncounter = 1\n\nfor trn_idx, vld_idx in skf.split(X, y):\n    \n    print(f\"CV {counter}\")\n    # split the train data into NSPLITS=5 folds\n    # x_valid and y_valid are considered as a holdout or test data set.\n    # y_train and y_valid are considered as a training data set.\n    x_train_knn_cv, x_valid_knn_cv  = X.iloc[trn_idx].values, X.iloc[vld_idx].values\n    y_train_knn_cv, y_valid_knn_cv  = y.iloc[trn_idx].values, y.iloc[vld_idx].values\n    \n    print(f\"Shape of valid data: {x_valid_knn_cv.shape}\")\n    knn_cv.fit(x_train_knn_cv, y_train_knn_cv)\n    \n    oof_pred_label_knn_cv = knn_cv.predict(x_valid_knn_cv) # predicted labels on the out_of_fold data : the data were not used during the training of the model.\n    oof_pred_proba_knn_cv = knn_cv.predict_proba(x_valid_knn_cv) # predicted labels on the out_of_fold data\n    oof_label_knn_cv[vld_idx] = oof_pred_label_knn_cv\n    oof_proba_knn_cv[vld_idx] = oof_pred_proba_knn_cv\n    \n    y_pred_test_knn_cv = knn_cv.predict_proba(X_test) # probability estimates for the test set \n    valid_pcg_knn_cv = decile_table(y_valid_knn_cv, oof_pred_proba_knn_cv[:, 1], labels=False)\n    score_valid_knn_cv = 1 - accuracy_score(y_valid_knn_cv, oof_pred_label_knn_cv) # compute the error score for the holdout or test data set.\n    area_ratio_valid_knn_cv = area_ratio(valid_pcg_knn_cv) # compute area_ratio score for the holdout or test data set.\n    print(f\"error rate fold {counter} score: {score_valid_knn_cv} | area ratio score: {area_ratio_valid_knn_cv}\")\n    error_rate_valids_knn_cv.append(score_valid_knn_cv) # appending the error scores of the holdouts to a list. \n    area_ratio_valids_knn_cv.append(area_ratio_valid_knn_cv) # appending the area_ratio scores of the holdouts to a list. \n    test_proba_knn_cv += y_pred_test_knn_cv \/ NSPLITS\n    counter += 1\n    print(\"\\n\")\n","7e783425":"# defining an objective function to be optimized :\ndef objective(trial):\n\n    param_grid = {\n        \"objective\": \"binary:logistic\", # the loss function : logistic regression for binary classification\n        \"eval_metric\":'error', # specify eval metric for xgboost\n        'tree_method':'gpu_hist', \n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7]), # colsample_bytree is the subsample ratio of columns when constructing each tree\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7]), # Subsample ratio of the training instances\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01, 0.02]),\n        'max_depth': trial.suggest_categorical('max_depth', [5,6,7,9,11]), # max depth of the decision trees \n        'min_child_weight': trial.suggest_categorical('min_child_weight', [10, 30, 60, 100, 200])}\n\n    skf = StratifiedKFold(n_splits=NSPLITS, shuffle=SHUFFLE, random_state=SEED)\n\n    xgb_score = []\n    counter = 1\n    for trn_idx, vld_idx in skf.split(X, y):\n\n        # train valid separation\n        print(f\"CV {counter}\/{NSPLITS}\")\n        print('\\n')\n        # DMatrix a data structure used by XGBoost, which is optimized both for memory efficiency and training speed.  \n        # We can construct a DMatrix from different data sources (numpy ,...)\n        d_train = xgb.DMatrix(X.iloc[trn_idx].values, y.iloc[trn_idx].values)\n        d_val = xgb.DMatrix(X.iloc[vld_idx].values, y.iloc[vld_idx].values)\n        \n        # Fit and train xgboost\n        model = xgb.train(param_grid, d_train, evals=[(d_val, \"val\")], num_boost_round=10000, verbose_eval=50,\n                          early_stopping_rounds=100)\n    \n        # Predictions and score on validation data\n        pred_val = model.predict(d_val)\n        predictions = [round(value) for value in pred_val]\n        score = 1 - accuracy_score(y.iloc[vld_idx], predictions)\n        print(f\"Fold {counter} Xgboost {score}\")\n        xgb_score.append(score)\n        counter += 1\n  \n    return np.mean(np.array(xgb_score))\n# Create a study object and invoke the optimize method over 100 trials :\n#study = optuna.create_study(direction='minimize')\n#study.optimize(objective, n_trials=100)\n#print('Number of finished trials:', len(study.trials))\n#print('Best trial:', study.best_trial.params)","bfcb0b58":"# extreme gradient boosting trees :\n# boosting : Ensemble learning technique that works by combining several weak learners into a model with strng accuracy\n# in gradient boosting each predictor tries to improve on its predecessor by reducing the errors.\n# at each iteration it fits a new predictor to the residual errors made by the previous predictor.Models are added sequentially until no further improvements can be made.","7a90c1cb":"# optuna hyperparameter tuning output :\nBest_trial= {'colsample_bytree': 0.7, 'subsample': 0.4, 'learning_rate': 0.01, 'max_depth': 6, 'min_child_weight': 100, \"objective\": \"binary:logistic\",\n        \"eval_metric\":'error'}\n# fiting XGBoost with **Best_trial :\nxgboost = xgb.XGBClassifier(**Best_trial)\nxgboost.fit(X, y, # fit xgboost classifier\n            eval_set=[(X_test,y_test)], # evaluation on the Test set\n              verbose=100,\n            early_stopping_rounds=200)  \ntrain_pred_xgb = xgboost.predict(X) # returns labels of the train data\ntrain_proba_xgb = xgboost.predict_proba(X) # returns probability estimates for the train data \ntest_pred_xgb = xgboost.predict(X_test) # returns labels of the test data\ntest_proba_xgb = xgboost.predict_proba(X_test) # returns probability estimates for the test data ","f94d1524":"%%time\n# fiting XGBoost with **Best_trial and cross-validation :\n\nNSPLITS = 5\nSHUFFLE =True\n\noof_label_xgb_cv = np.zeros((X.shape[0]))\noof_proba_xgb_cv = np.zeros((X.shape[0], 2))\ntest_proba_xgb_cv = np.zeros((X_test.shape[0], 2))\n\nerror_rate_valids_xgb_cv = []\narea_ratio_valids_xgb_cv = []\n\n\nskf = StratifiedKFold(n_splits=NSPLITS, shuffle=SHUFFLE, random_state=SEED)\n\n\ncounter = 1\n\nfor trn_idx, vld_idx in skf.split(X, y):\n    \n    # split the train data into NSPLITS=5 folds\n    # x_valid and y_valid are considered as a holdout or test data set.\n    # y_train and y_valid are considered as a training data set.\n    print(f\"CV {counter}\")\n    x_train_xgb_cv, x_valid_xgb_cv  = X.iloc[trn_idx].values, X.iloc[vld_idx].values\n    y_train_xgb_cv, y_valid_xgb_cv  = y.iloc[trn_idx].values, y.iloc[vld_idx].values\n    \n    \n    print(f\"Shape pf valid data: {x_valid_xgb_cv.shape}\")\n    # Fit and train xgboost\n    xgboost_cv = xgb.XGBClassifier(**Best_trial)\n    xgboost_cv.fit(x_train_xgb_cv, y_train_xgb_cv, \n                eval_set=[(x_valid_xgb_cv,y_valid_xgb_cv)], # evaluation on the validation set\n                verbose=100, early_stopping_rounds=200)\n    \n    oof_pred_label_xgb_cv = xgboost_cv.predict(x_valid_xgb_cv) # predicted labels on the out_of_fold data : the data were not used during the training of the model.\n    oof_pred_proba_xgb_cv = xgboost_cv.predict_proba(x_valid_xgb_cv) # predicted labels on the out_of_fold data\n    oof_label_xgb_cv[vld_idx] = oof_pred_label_xgb_cv\n    oof_proba_xgb_cv[vld_idx] = oof_pred_proba_xgb_cv\n    \n    y_pred_test_xgb_cv = xgboost_cv.predict_proba(X_test) # probability estimates for the test set \n    valid_pcg_xgb_cv = decile_table(y_valid_xgb_cv , oof_pred_proba_xgb_cv[:, 1], labels=False)\n    score_valid_xgb_cv = 1 - accuracy_score(y_valid_xgb_cv , oof_pred_label_xgb_cv) # compute the error score for the holdout or test data set.\n    area_ratio_valid_xgb_cv = area_ratio(valid_pcg_xgb_cv) # compute area_ratio score for the holdout or test data set.\n    print(f\"error rate fold {counter} score: {score_valid_xgb_cv} | area ratio score: {area_ratio_valid_xgb_cv}\")\n    error_rate_valids_xgb_cv.append(score_valid_xgb_cv) # appending the error scores of the holdouts to a list. \n    area_ratio_valids_xgb_cv.append(area_ratio_valid_xgb_cv) # appending the area_ratio scores of the holdouts to a list. \n    test_proba_xgb_cv += y_pred_test_xgb_cv \/ NSPLITS\n    counter += 1\n    print(\"\\n\")","6d0fdded":"def objective_logistic(trial):\n    solver = trial.suggest_categorical(\"solver\", [ 'liblinear',  'lbfgs', 'sag', 'saga'])\n    C = trial.suggest_float(\"C\", 0.0, 1.0)\n        # 'penalty' parameter isn't relevant for this solver,\n        # so we always specify 'l2' as the dummy value.\n    logistic = LogisticRegression(max_iter=200, solver=solver, C=C,random_state=SEED)\n\n    skf = StratifiedKFold(n_splits=NSPLITS, shuffle=SHUFFLE, random_state=SEED)\n\n    logistic_score = []\n    counter = 1\n    for trn_idx, vld_idx in skf.split(X, y):\n\n        # train valid separation\n        print(f\"CV {counter}\/{NSPLITS}\")\n        print('\\n')\n        x_train, x_valid  = X.iloc[trn_idx].values, X.iloc[vld_idx].values\n        y_train, y_valid  = y.iloc[trn_idx].values, y.iloc[vld_idx].values\n        \n        # Fit and train logistic\n        logistic.fit(x_train,y_train)\n    \n        # Predictions and score on validation data\n        pred_val = logistic.predict(x_valid)\n        \n        score = 1 - accuracy_score(y_valid, pred_val)\n        print(f\"Fold {counter} logistic {score}\")\n        logistic_score.append(score)\n        counter += 1\n  \n    return np.mean(np.array(logistic_score))\n# Create a study object and invoke the optimize method over 100 trials :\n#study = optuna.create_study(direction=\"minimize\")\n#study.optimize(objective_logistic, n_trials=100)\n#print(\"Number of finished trials: \", len(study.trials))\n#print(\"Best trial:\")\n#trial = study.best_trial\n#print(\"  Value: \", trial.value)\n#print(\"  Params: \")\n#for key, value in trial.params.items():\n #      print(\"    {}: {}\".format(key, value))","64604174":"Params = {'solver': 'saga' ,'C': 0.9960292949722489} \nLogistic = LogisticRegression(**Params,penalty='l2',max_iter=200)\nLogistic.fit(X, y)  \ntrain_pred_Logistic = Logistic.predict(X)\ntrain_proba_Logistic = Logistic.predict_proba(X)\ntest_pred_Logistic = Logistic.predict(X_test)\ntest_proba_Logistic = Logistic.predict_proba(X_test)","de176db9":"%%time\n\nNSPLITS = 5\nSHUFFLE =True\n\noof_label_Logistic_cv = np.zeros((X.shape[0]))\noof_proba_Logistic_cv = np.zeros((X.shape[0], 2))\ntest_proba_Logistic_cv = np.zeros((X_test.shape[0], 2))\n\nerror_rate_valids_Logistic_cv = []\narea_ratio_valids_Logistic_cv = []\n\n\nskf = StratifiedKFold(n_splits=NSPLITS, shuffle=SHUFFLE, random_state=SEED)\n\n\ncounter = 1\n\nfor trn_idx, vld_idx in skf.split(X, y):\n    \n    print(f\"CV {counter}\")\n    # split the train data into NSPLITS=5 folds\n    # x_valid and y_valid are considered as a holdout or test data set.\n    # y_train and y_valid are considered as a training data set.\n    x_train_Logistic_cv, x_valid_Logistic_cv  = X.iloc[trn_idx].values, X.iloc[vld_idx].values\n    y_train_Logistic_cv, y_valid_Logistic_cv  = y.iloc[trn_idx].values, y.iloc[vld_idx].values\n    \n    \n    print(f\"Shape pf valid data: {x_valid_Logistic_cv.shape}\")\n    # Fit and train LogisticRegression\n    Logistic_cv = LogisticRegression(**Params,penalty='l2',max_iter=200)\n\n    Logistic_cv.fit(x_train_Logistic_cv, y_train_Logistic_cv)\n    \n    oof_pred_label_Logistic_cv = Logistic_cv.predict(x_valid_Logistic_cv) # predicted labels on the out_of_fold data : the data were not used during the training of the model.\n    oof_pred_proba_Logistic_cv = Logistic_cv.predict_proba(x_valid_Logistic_cv) # predicted labels on the out_of_fold data\n    oof_label_Logistic_cv[vld_idx] = oof_pred_label_Logistic_cv\n    oof_proba_Logistic_cv[vld_idx] = oof_pred_proba_Logistic_cv\n    \n    y_pred_test_Logistic_cv = Logistic_cv.predict_proba(X_test) # probability estimates for the test set \n    valid_pcg_Logistic_cv = decile_table(y_valid_Logistic_cv , oof_pred_proba_Logistic_cv[:, 1], labels=False)\n    score_valid_Logistic_cv = 1 - accuracy_score(y_valid_Logistic_cv , oof_pred_label_Logistic_cv) # compute the error score for the holdout or test data set.\n    \n    area_ratio_valid_Logistic_cv = area_ratio(valid_pcg_Logistic_cv) # compute area_ratio score for the holdout or test data set.\n    print(f\"error rate fold {counter} score: {score_valid_Logistic_cv} | area ratio score: {area_ratio_valid_Logistic_cv}\")\n    error_rate_valids_Logistic_cv.append(score_valid_Logistic_cv)\n    area_ratio_valids_Logistic_cv.append(area_ratio_valid_Logistic_cv)\n    test_proba_Logistic_cv+= y_pred_test_Logistic_cv\/ NSPLITS\n    counter += 1\n    print(\"\\n\")\n","171f95f3":"def objective_random_forest(trial):\n\n    \n\n    params = {\n            'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n            'max_depth': trial.suggest_int('max_depth', 4, 50),\n            'min_samples_split': trial.suggest_int('min_samples_split', 1, 150),\n            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 60)\n        }\n\n    \n    rfc = RandomForestClassifier(**params,n_jobs=2,random_state=SEED)\n\n    skf = StratifiedKFold(n_splits=NSPLITS, shuffle=SHUFFLE, random_state=SEED)\n\n    rfc_score = []\n    counter = 1\n    for trn_idx, vld_idx in skf.split(X, y):\n\n        # train valid separation\n        print(f\"CV {counter}\/{NSPLITS}\")\n        print('\\n')\n        x_train, x_valid  = X.iloc[trn_idx].values, X.iloc[vld_idx].values\n        y_train, y_valid  = y.iloc[trn_idx].values, y.iloc[vld_idx].values\n        \n        # Fit and train RandomForestClassifier\n        rfc.fit(x_train,y_train)\n    \n        # Predictions and score on validation data\n        pred_val = rfc.predict(x_valid)\n        \n        score = 1 - accuracy_score(y_valid, pred_val)\n        print(f\"Fold {counter} RFC {score}\")\n        rfc_score.append(score)\n        counter += 1\n  \n    return np.mean(np.array(rfc_score))\n#study = optuna.create_study(direction=\"minimize\")\n#study.optimize(objective_random_forest, n_trials=100)\n#\n#print(\"Number of finished trials: \", len(study.trials))\n#print(\"Best trial:\")\n#trial = study.best_trial\n#print(\"  Value: \", trial.value)\n#print(\"  Params: \")\n#for key, value in trial.params.items():\n#    print(\"    {}: {}\".format(key, value))","147fa75f":"RF = RandomForestClassifier(max_depth=7,n_estimators=400,criterion='gini',verbose=0)\nRF.fit(X, y)  \ntrain_pred_RF = RF.predict(X)\ntrain_proba_RF = RF.predict_proba(X)\ntest_pred_RF = RF.predict(X_test)\ntest_proba_RF = RF.predict_proba(X_test)","a06e292c":"%%time\n\nNSPLITS = 5\nSHUFFLE =True\n\noof_label_RF_CV = np.zeros((X.shape[0]))\noof_proba_RF_CV = np.zeros((X.shape[0], 2))\ntest_proba_RF_CV = np.zeros((X_test.shape[0], 2))\n\nerror_rate_valids_RF_CV = []\narea_ratio_valids_RF_CV = []\n\n\nskf = StratifiedKFold(n_splits=NSPLITS, shuffle=SHUFFLE, random_state=SEED)\n\n\ncounter = 1\n\nfor trn_idx, vld_idx in skf.split(X, y):\n    \n    print(f\"CV {counter}\")\n    x_train_RF_CV, x_valid_RF_CV  = X.iloc[trn_idx].values, X.iloc[vld_idx].values\n    y_train_RF_CV, y_valid_RF_CV  = y.iloc[trn_idx].values, y.iloc[vld_idx].values\n    \n    \n    print(f\"Shape pf valid data: {x_valid_RF_CV.shape}\")\n    # Fit and train RandomForestClassifier\n    RF_CV = RandomForestClassifier(max_depth=7, # The maximum depth of the tree\n                                   n_estimators=400, # the number of trees the algorithm builds before taking the maximum voting or the averages of predictions\n                                   criterion='gini', # gini impurity\n                                   verbose=0)\n\n    RF_CV.fit(x_train_RF_CV, y_train_RF_CV)\n    \n    oof_pred_label_RF_CV = RF_CV.predict(x_valid_RF_CV) #the predicted label for the test folds\n    \n    oof_pred_proba_RF_CV = RF_CV.predict_proba(x_valid_RF_CV) \n    \n    oof_label_RF_CV[vld_idx] = oof_pred_label_RF_CV\n    oof_proba_RF_CV[vld_idx] = oof_pred_proba_RF_CV\n    \n    y_pred_test_RF_CV = RF_CV.predict_proba(X_test)\n    \n    valid_pcg_RF_CV = decile_table(y_valid_RF_CV , oof_pred_proba_RF_CV[:, 1], labels=False)\n    \n    score_valid_RF_CV = 1 - accuracy_score(y_valid_RF_CV , oof_pred_label_RF_CV) # score of each fold\n    \n    area_ratio_valid_RF_CV = area_ratio(valid_pcg_RF_CV)\n    \n    print(f\"error rate fold {counter} score: {score_valid_RF_CV} | area ratio score: {area_ratio_valid_RF_CV}\")\n    \n    error_rate_valids_RF_CV.append(score_valid_RF_CV) # append error rate folds\n    area_ratio_valids_RF_CV.append(area_ratio_valid_RF_CV)\n    test_proba_RF_CV += y_pred_test_RF_CV \/ NSPLITS # predictproba for all the folds ***\n    counter += 1\n    print(\"\\n\")","39a6155b":"LDA = LinearDiscriminantAnalysis(solver='svd')\nLDA.fit(X, y)  \ntrain_pred_LDA = LDA.predict(X)\ntrain_proba_LDA = LDA.predict_proba(X)\ntest_pred_LDA = LDA.predict(X_test)\ntest_proba_LDA = LDA.predict_proba(X_test)","24aea39b":"%%time\n\nNSPLITS = 5\nSHUFFLE =True\n\noof_label_LDA_CV = np.zeros((X.shape[0]))\noof_proba_LDA_CV = np.zeros((X.shape[0], 2))\ntest_proba_LDA_CV = np.zeros((X_test.shape[0], 2))\n\nerror_rate_valids_LDA_CV = []\narea_ratio_valids_LDA_CV = []\n\n\nskf = StratifiedKFold(n_splits=NSPLITS, shuffle=SHUFFLE, random_state=SEED)\n\n\ncounter = 1\n\nfor trn_idx, vld_idx in skf.split(X, y):\n    \n    print(f\"CV {counter}\")\n    x_train_LDA_CV, x_valid_LDA_CV  = X.iloc[trn_idx].values, X.iloc[vld_idx].values\n    y_train_LDA_CV, y_valid_LDA_CV  = y.iloc[trn_idx].values, y.iloc[vld_idx].values\n    \n    \n    print(f\"Shape pf valid data: {x_valid_LDA_CV.shape}\")\n    # Fit and train LinearDiscriminantAnalysis\n    LDA_CV = LinearDiscriminantAnalysis(solver='svd')\n\n    LDA_CV.fit(x_train_LDA_CV, y_train_LDA_CV)\n    \n    oof_pred_label_LDA_CV = LDA_CV.predict(x_valid_LDA_CV)\n    oof_pred_proba_LDA_CV = LDA_CV.predict_proba(x_valid_LDA_CV)\n    oof_label_LDA_CV[vld_idx] = oof_pred_label_LDA_CV\n    oof_proba_LDA_CV[vld_idx] = oof_pred_proba_LDA_CV\n    \n    y_pred_test_LDA_CV = LDA_CV.predict_proba(X_test)\n    valid_pcg_LDA_CV = decile_table(y_valid_LDA_CV , oof_pred_proba_LDA_CV[:, 1], labels=False)\n    score_valid_LDA_CV = 1 - accuracy_score(y_valid_LDA_CV , oof_pred_label_LDA_CV)\n    \n    area_ratio_valid_LDA_CV = area_ratio(valid_pcg_LDA_CV)\n    print(f\"error rate fold {counter} score: {score_valid_LDA_CV} | area ratio score: {area_ratio_valid_LDA_CV}\")\n    error_rate_valids_LDA_CV.append(score_valid_LDA_CV)\n    area_ratio_valids_LDA_CV.append(area_ratio_valid_LDA_CV)\n    test_proba_LDA_CV += y_pred_test_LDA_CV \/ NSPLITS\n    counter += 1\n    print(\"\\n\")","621700bd":"# Instantiation of the Logisti algorithm\nnnet_params = {\n    'hidden_layer_sizes':(250, 150,), # Hidden layer size\n    'activation':'tanh',\n    'alpha':1e-4, # Weight decay (L2 regularization)\n    'batch_size':2**7, # Batch size\n    'solver':'adam', # Optimizer\n    'learning_rate':'adaptive',\n    'max_iter':100, # Number of iteration\n    'verbose':True,\n    'early_stopping':True,\n    'random_state':2022\n}\nnnet = MLPClassifier(**nnet_params)\n# Train the model\nnnet.fit(nnet_article_train, y)\n# Predict proba & labels on the train data\ntrain_pred_nnet = nnet.predict(nnet_article_train)\ntrain_proba_nnet = nnet.predict_proba(nnet_article_train)","92c51641":"%%time\n\n# Vector placeholders for score computation over a fold\noof_label_nnet_CV = np.zeros((nnet_article_train.shape[0]))\noof_proba_nnet_CV = np.zeros((nnet_article_train.shape[0], 2))\ntest_proba_nnet_CV = np.zeros((nnet_article_test.shape[0], 2))\n\n# Placeholders for mean score computation over all folds\nerror_rate_valids_nnet_CV = []\narea_ratio_valids_nnet_CV = []\n\n# Instantiation of the Stratified kfold generator\nskf = StratifiedKFold(n_splits=NSPLITS, shuffle=SHUFFLE, random_state=SEED)\n\n# Instantiation of the KNN algorithm\nnnet_params = {\n    'hidden_layer_sizes':(250, 150,), # Hidden layer size\n    'activation':'tanh',\n    'alpha':1e-4, # Weight decay (L2 regularization)\n    'batch_size':2**7, # Batch size\n    'solver':'adam', # Optimizer\n    'learning_rate':'adaptive',\n    'max_iter':100, # Number of iteration\n    'verbose':True,\n    'early_stopping':True,\n    'random_state':2022\n}                        \n\ncounter = 1\n\n# Loop over the folds\nfor trn_idx, vld_idx in skf.split(nnet_article_train, y):\n    \n    print(f\"CV {counter}\")\n    # Separate train and validation data\n    x_train_nnet_CV, x_valid_nnet_CV = nnet_article_train.iloc[trn_idx].values, nnet_article_train.iloc[vld_idx].values\n    y_train_nnet_CV, y_valid_nnet_CV = y.iloc[trn_idx].values, y.iloc[vld_idx].values\n    \n    # train the model\n    nnet.fit(x_train_nnet_CV, y_train_nnet_CV)\n    \n    # Predict label and probability on validation data\n    oof_pred_label_nnet_CV = nnet.predict(x_valid_nnet_CV)\n    oof_pred_proba_nnet_CV = nnet.predict_proba(x_valid_nnet_CV)\n    oof_label_nnet_CV[vld_idx] = oof_pred_label_nnet_CV\n    oof_proba_nnet_CV[vld_idx] = oof_pred_proba_nnet_CV\n    \n    # Area ratio computation\n    valid_pcg_nnet_CV = decile_table(y_valid_nnet_CV, oof_pred_proba_nnet_CV[:, 1], labels=False)\n    area_ratio_valid_nnet_CV = area_ratio(valid_pcg_nnet_CV)\n    # Error rate computation\n    score_valid_nnet_CV = 1 - accuracy_score(y_valid_nnet_CV, oof_pred_label_nnet_CV)\n    print(f\"error rate fold {counter} score: {score_valid_nnet_CV} | area ratio score: {area_ratio_valid_nnet_CV}\")\n    error_rate_valids_nnet_CV.append(score_valid_nnet_CV)\n    area_ratio_valids_nnet_CV.append(area_ratio_valid_nnet_CV)\n    \n    # Prediction on the test set\n    y_pred_test_nnet_CV = nnet.predict_proba(nnet_article_test)\n    test_proba_nnet_CV += y_pred_test_nnet_CV \/ NSPLITS\n    counter += 1\n    print(\"\\n\")\n","b4a3d796":"DT = DecisionTreeClassifier(max_depth=7, # The maximum depth of the tree to prevent overfitting\n                                   criterion='gini', # gini impurity\n                                   random_state=SEED\n                                   )\n# Train the model\nDT.fit(X, y)\n# Predict proba & labels on the train data\ntrain_pred_DT = DT.predict(X)\ntrain_proba_DT = DT.predict_proba(X)\ntest_pred_DT = knn.predict(X_test)\ntest_proba_DT = knn.predict_proba(X_test)","ebb0551d":"%%time\n\nNSPLITS = 5\nSHUFFLE =True\n\noof_label_DT_cv = np.zeros((X.shape[0]))\noof_proba_DT_cv = np.zeros((X.shape[0], 2))\ntest_proba_DT_cv = np.zeros((X_test.shape[0], 2))\n\nerror_rate_valids_DT_cv = []\narea_ratio_valids_DT_cv = []\n\nskf = StratifiedKFold(n_splits=NSPLITS, shuffle=SHUFFLE, random_state=SEED)\n\ncounter = 1\n\nfor trn_idx, vld_idx in skf.split(X, y):\n    \n    print(f\"CV {counter}\")\n    x_train_DT_cv, x_valid_DT_cv  = X.iloc[trn_idx].values, X.iloc[vld_idx].values\n    y_train_DT_cv, y_valid_DT_cv  = y.iloc[trn_idx].values, y.iloc[vld_idx].values\n    \n    \n    print(f\"Shape pf valid data: {x_valid_DT_cv.shape}\")\n    # Fit and train LogisticRegression\n    DT_cv = DecisionTreeClassifier(max_depth=7, # The maximum depth of the tree to prevent overfitting\n                                   criterion='gini', # gini impurity\n                                   random_state=SEED\n                                   )\n\n    DT_cv.fit(x_train_DT_cv, y_train_DT_cv)\n    \n    oof_pred_label_DT_cv = DT_cv.predict(x_valid_DT_cv) #the predicted label for the test folds\n    oof_pred_proba_DT_cv = DT_cv.predict_proba(x_valid_DT_cv)\n    oof_label_DT_cv[vld_idx] = oof_pred_label_DT_cv\n    oof_proba_DT_cv[vld_idx] = oof_pred_proba_DT_cv\n    \n    y_pred_test_DT_cv = DT_cv.predict_proba(X_test)\n    valid_pcg_DT_cv = decile_table(y_valid_DT_cv , oof_pred_proba_DT_cv[:, 1], labels=False)\n    score_valid_DT_cv = 1 - accuracy_score(y_valid_DT_cv , oof_pred_label_DT_cv) # score of each fold\n    \n    area_ratio_valid_DT_cv = area_ratio(valid_pcg_DT_cv)\n    print(f\"error rate fold {counter} score: {score_valid_DT_cv} | area ratio score: {area_ratio_valid_DT_cv}\")\n    error_rate_valids_DT_cv.append(score_valid_DT_cv) # append error rate folds\n    area_ratio_valids_DT_cv.append(area_ratio_valid_DT_cv)\n    test_proba_DT_cv+= y_pred_test_DT_cv\/ NSPLITS # predictproba for all the folds ***\n    counter += 1\n    print(\"\\n\")\n","f18c8ae6":"NB_train = X.copy()\nNB_test  = X_test.copy()\nscaler = StandardScaler()\nscaler.fit(X) # Fit scaler\n# Scale train data for logistic regression\nscaled_train = scaler.transform(NB_train) \nNB_train.loc[:, features_col] = scaled_train\n# Scale test data for logistic regression\nscaled_test = scaler.transform(NB_test)\nNB_test.loc[:, features_col] = scaled_test","4115eddb":"NB = GaussianNB()\nNB.fit(NB_train,y)\ntrain_pred_NB = NB.predict(NB_train)\ntrain_proba_NB = NB.predict_proba(NB_train)\ntrain_pcg_NB = decile_table(y, train_proba_NB[:, 1], labels=False)\ntest_pred_NB = NB.predict(NB_test)\ntest_proba_NB = NB.predict_proba(NB_test)","0f69c5ff":"%%time\n\nNSPLITS = 5\nSHUFFLE =True\n\noof_label_NB_cv = np.zeros((X.shape[0]))\noof_proba_NB_cv = np.zeros((X.shape[0], 2))\ntest_proba_NB_cv = np.zeros((X_test.shape[0], 2))\n\nerror_rate_valids_NB_cv = []\narea_ratio_valids_NB_cv = []\n\nskf = StratifiedKFold(n_splits=NSPLITS, shuffle=SHUFFLE, random_state=SEED)\n\ncounter = 1\n\nfor trn_idx, vld_idx in skf.split(NB_train, y):\n    \n    print(f\"CV {counter}\")\n    x_train_NB_cv, x_valid_NB_cv  = NB_train.iloc[trn_idx].values,NB_train.iloc[vld_idx].values\n    y_train_NB_cv, y_valid_NB_cv  = y.iloc[trn_idx].values, y.iloc[vld_idx].values\n    \n    print(f\"Shape pf valid data: {x_valid_NB_cv.shape}\")\n    # Fit and train LogisticRegression\n    NB_cv = GaussianNB()\n\n    NB_cv.fit(x_train_NB_cv, y_train_NB_cv)\n    \n    oof_pred_label_NB_cv = NB_cv.predict(x_valid_NB_cv)\n    oof_pred_proba_NB_cv = NB_cv.predict_proba(x_valid_NB_cv)\n    oof_label_NB_cv[vld_idx] = oof_pred_label_NB_cv\n    oof_proba_NB_cv[vld_idx] = oof_pred_proba_NB_cv\n    \n    y_pred_test_NB_cv = NB_cv.predict_proba(NB_test)\n    valid_pcg_NB_cv = decile_table(y_valid_NB_cv , oof_pred_proba_NB_cv[:, 1], labels=False)\n    score_valid_NB_cv = 1 - accuracy_score(y_valid_NB_cv, oof_pred_label_NB_cv)\n    \n    area_ratio_valid_NB_cv = area_ratio(valid_pcg_NB_cv)\n    print(f\"error rate fold {counter} score: {score_valid_NB_cv} | area ratio score: {area_ratio_valid_NB_cv}\")\n    error_rate_valids_NB_cv.append(score_valid_NB_cv)\n    area_ratio_valids_NB_cv.append(area_ratio_valid_NB_cv)\n    test_proba_NB_cv+= y_pred_test_NB_cv\/ NSPLITS\n    counter += 1\n    print(\"\\n\")\n","017cb721":"train_pcg_knn = decile_table(y, train_proba_knn[:, 1], labels=False)# calling the decile_table function with 'y' : labels for train \n                                                                    # and 'train_proba[:, 1]' : column  that contains the probability estimated for default==1\ntest_pcg_knn = decile_table(y_test, test_proba_knn[:, 1], labels=False) # calling the decile_table function with y_test and probability estimates of default==1\nerror_rate_train_knn = 1 - accuracy_score(y, train_pred_knn)\narea_ratio_train_knn = area_ratio(train_pcg_knn) # calling area_ratio function\nerror_rate_test_knn = 1 - accuracy_score(y_test, test_pred_knn)\narea_ratio_test_knn = area_ratio(test_pcg_knn)\ny_pred_label_knn = test_proba_knn.argmax(1)\nprint(f\"KNN error rate on train set (training): {error_rate_train_knn} | area ratio on train set (training): {area_ratio_train_knn}\\n\")\nprint(f\"KNN error rate on test (valid) set: {error_rate_test_knn} | area ratio on test (valid) set: {area_ratio_test_knn} \\n\")\nplot_lift_chart(test_pcg_knn, area_ratio_test_knn, title='Lift Chart KNN on test set')\ncm = confusion_matrix(y_test, y_pred_label_knn, labels=knn.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()","e4cbd9b8":"y_pred_label_knn_cv = test_proba_knn_cv.argmax(1) #Returns the indices of the maximum values along an axis.\ntest_pcg_knn_cv = decile_table(y_test, test_proba_knn_cv[:,1], labels=False)\ntest_area_ratio_knn_cv = area_ratio(test_pcg_knn_cv)\nprint(f\"Error rate CV score: {np.mean(np.array(error_rate_valids_knn_cv))} | Area ratio score: {np.mean(np.array(area_ratio_valids_knn_cv))}\\n\")\nprint(f\"Error rate test (validation) score: {1 - accuracy_score(y_test, y_pred_label_knn_cv)} | Area ratio score: {test_area_ratio_knn_cv}\")\nplot_lift_chart(test_pcg_knn_cv, test_area_ratio_knn_cv, title='Lift Chart KNN with CV on test set')\ncm = confusion_matrix(y_test, y_pred_label_knn_cv, labels=knn_cv.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()\nprint(\"\\n\")","380e2fd4":"train_pcg_xgb = decile_table(y, train_proba_xgb[:, 1], labels=False)\ntest_pcg_xgb = decile_table(y_test, test_proba_xgb[:, 1], labels=False)\nerror_rate_train_xgb = 1 - accuracy_score(y, train_pred_xgb)\narea_ratio_train_xgb = area_ratio(train_pcg_xgb)\nerror_rate_test_xgb = 1 - accuracy_score(y_test, test_pred_xgb)\narea_ratio_test_xgb = area_ratio(test_pcg_xgb)\ny_pred_label_xgb = test_proba_xgb.argmax(1)\nprint(f\"XGB error rate on train set (training): {error_rate_train_xgb} | area ratio on train set (training): {area_ratio_train_xgb}\\n\")\nprint(f\"XGB error rate on test (valid) set: {error_rate_test_xgb} | area ratio on test (valid) set: {area_ratio_test_xgb} \\n\")\nplot_lift_chart(test_pcg_xgb, area_ratio_test_xgb, title='Lift Chart XGBoost on test set')\ncm = confusion_matrix(y_test, y_pred_label_xgb, labels=xgboost.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()","0eae19bf":"y_pred_label_xgb_cv = test_proba_xgb_cv.argmax(1) #Returns the indices of the maximum values along an axis.\ntest_pcg_xgb_cv = decile_table(y_test, test_proba_xgb_cv[:, 1], labels=False)\ntest_area_ratio_xgb_cv = area_ratio(test_pcg_xgb_cv)\nprint(f\"Error rate CV score: {np.mean(np.array(error_rate_valids_xgb_cv))} | Area ratio score: {np.mean(np.array(area_ratio_valids_xgb_cv))}\\n\")\nprint(f\"Error rate test (validation) score: {1 - accuracy_score(y_test, y_pred_label_xgb_cv)} | Area ratio score: {test_area_ratio_xgb_cv}\")\nplot_lift_chart(test_pcg_xgb_cv, test_area_ratio_xgb_cv, title='Lift Chart XGBoost with CV on test set')\nprint(\"\\n\")\ncm = confusion_matrix(y_test, y_pred_label_xgb_cv, labels=xgboost_cv.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()","5a6074a3":"train_pcg_Logistic = decile_table(y, train_proba_Logistic[:, 1], labels=False)\ntest_pcg_Logistic = decile_table(y_test, test_proba_Logistic[:, 1], labels=False)\nerror_rate_train_Logistic = 1 - accuracy_score(y, train_pred_Logistic)\narea_ratio_train_Logistic = area_ratio(train_pcg_Logistic)\nerror_rate_test_Logistic = 1 - accuracy_score(y_test, test_pred_Logistic)\narea_ratio_test_Logistic = area_ratio(test_pcg_Logistic)\ny_pred_label_Logistic = test_proba_Logistic.argmax(1)\nprint(f\"Logistic Regression error rate on train set (training): {error_rate_train_Logistic} | area ratio on train set (training): {area_ratio_train_Logistic}\\n\")\nprint(f\"Logistic Regression error rate on test (valid) set: {error_rate_test_Logistic} | area ratio on test (valid) set: {area_ratio_test_Logistic} \\n\")\nplot_lift_chart(test_pcg_Logistic, area_ratio_test_Logistic, title='Lift Chart Logistic Regression on test set')\ncm = confusion_matrix(y_test, y_pred_label_Logistic, labels=Logistic.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()","f0aaf199":"y_pred_label_Logistic_cv = test_proba_Logistic_cv.argmax(1)\ntest_pcg_Logistic_cv = decile_table(y_test, test_proba_Logistic_cv[:, 1], labels=False)\ntest_area_ratio_Logistic_cv = area_ratio(test_pcg_Logistic_cv)\nprint(f\"Error rate CV score: {np.mean(np.array(error_rate_valids_Logistic_cv))} | Area ratio score: {np.mean(np.array(area_ratio_valids_Logistic_cv))}\\n\")\nprint(f\"Error rate test (validation) score: {1 - accuracy_score(y_test, y_pred_label_Logistic_cv)} | Area ratio score: {test_area_ratio_Logistic_cv}\")\nplot_lift_chart(test_pcg_Logistic_cv, test_area_ratio_Logistic_cv, title='Lift Chart Logistic Regression with CV on test set')\ncm = confusion_matrix(y_test, y_pred_label_Logistic_cv, labels=Logistic_cv.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()\nprint(\"\\n\")","1edff584":"train_pcg_RF = decile_table(y, train_proba_RF[:, 1], labels=False)\ntest_pcg_RF = decile_table(y_test, test_proba_RF[:, 1], labels=False)\nerror_rate_train_RF = 1 - accuracy_score(y, train_pred_RF)\narea_ratio_train_RF = area_ratio(train_pcg_RF)\nerror_rate_test_RF = 1 - accuracy_score(y_test, test_pred_RF)\narea_ratio_test_RF = area_ratio(test_pcg_RF)\ny_pred_label_RF = test_proba_RF.argmax(1)\nprint(f\"Random Forest error rate on train set (training): {error_rate_train_RF} | area ratio on train set (training): {area_ratio_train_RF}\\n\")\nprint(f\"Random Forest error rate on test (valid) set: {error_rate_test_RF} | area ratio on test (valid) set: {area_ratio_test_RF} \\n\")\nplot_lift_chart(test_pcg_RF, area_ratio_test_RF, title='Lift Chart Random Forest on test set')\ncm = confusion_matrix(y_test, y_pred_label_RF, labels=RF.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()","55884072":"y_pred_label_RF_CV = test_proba_RF_CV.argmax(1)\ntest_pcg_RF_CV = decile_table(y_test, test_proba_RF_CV[:, 1], labels=False)\ntest_area_ratio_RF_CV = area_ratio(test_pcg_RF_CV)\nprint(f\"Error rate CV score: {np.mean(np.array(error_rate_valids_RF_CV))} | Area ratio score: {np.mean(np.array(area_ratio_valids_RF_CV))}\\n\")\nprint(f\"Error rate test (validation) score: {1 - accuracy_score(y_test, y_pred_label_RF_CV)} | Area ratio score: {test_area_ratio_RF_CV}\")\nplot_lift_chart(test_pcg_RF_CV, test_area_ratio_RF_CV, title='Lift Chart Random Forest with CV on test set')\ncm = confusion_matrix(y_test, y_pred_label_RF_CV, labels=RF_CV.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()\nprint(\"\\n\")\n","649a31fb":"train_pcg_LDA = decile_table(y, train_proba_LDA[:, 1], labels=False)\ntest_pcg_LDA = decile_table(y_test, test_proba_LDA[:, 1], labels=False)\nerror_rate_train_LDA = 1 - accuracy_score(y, train_pred_LDA)\narea_ratio_train_LDA = area_ratio(train_pcg_LDA)\nerror_rate_test_LDA = 1 - accuracy_score(y_test, test_pred_RF)\narea_ratio_test_LDA = area_ratio(test_pcg_LDA)\ny_pred_label_LDA = test_proba_LDA.argmax(1)\nprint(f\"Linear Discriminant Analysis error rate on train set (training): {error_rate_train_LDA} | area ratio on train set (training): {area_ratio_train_LDA}\\n\")\nprint(f\"Linear Discriminant Analysis error rate on test (valid) set: {error_rate_test_LDA} | area ratio on test (valid) set: {area_ratio_test_LDA} \\n\")\nplot_lift_chart(test_pcg_LDA, area_ratio_test_LDA, title='Lift Chart Linear Discriminant Analysis on test set')\ncm = confusion_matrix(y_test, y_pred_label_LDA, labels=LDA.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()","732dda84":"y_pred_label_LDA_CV = test_proba_LDA_CV.argmax(1)\ntest_pcg_LDA_CV = decile_table(y_test, test_proba_LDA_CV[:, 1], labels=False)\ntest_area_ratio_LDA_CV = area_ratio(test_pcg_LDA_CV)\nprint(f\"Error rate CV score: {np.mean(np.array(error_rate_valids_LDA_CV))} | Area ratio score: {np.mean(np.array(area_ratio_valids_LDA_CV))}\\n\")\nprint(f\"Error rate test (validation) score: {1 - accuracy_score(y_test, y_pred_label_LDA_CV)} | Area ratio score: {test_area_ratio_LDA_CV}\")\nplot_lift_chart(test_pcg_LDA_CV, test_area_ratio_LDA_CV, title='Lift Chart Linear Discriminant Analysis with CV on test set')\ncm = confusion_matrix(y_test, y_pred_label_LDA_CV, labels=LDA_CV.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()\nprint(\"\\n\")","5d4d0f7b":"# Area ratio & accuracy on the train data\ntrain_pcg_nnet = decile_table(y, train_proba_nnet[:, 1], labels=False)\narea_ratio_train_nnet = area_ratio(train_pcg_nnet)\nerror_rate_train_nnet = 1 - accuracy_score(y, train_pred_nnet)\n\n# Predict proba & labels on the test data\ntest_pred_nnet = nnet.predict(nnet_article_test)\ntest_proba_nnet = nnet.predict_proba(nnet_article_test)\n\n# Area ratio & accuracy on the test data\ntest_pcg_nnet = decile_table(y_test, test_proba_nnet[:, 1], labels=False)\nerror_rate_test_nnet = 1 - accuracy_score(y_test, test_pred_nnet)\narea_ratio_test_nnet = area_ratio(test_pcg_nnet)\n\nprint(f\"Neural Network error rate on train set (training): {error_rate_train_nnet} | Neural Network area ratio on train set (training): {area_ratio_train_nnet}\\n\")\nprint(f\"Neural Network error rate on test (valid) set: {error_rate_test_nnet} | Neural Network area ratio on test (valid) set: {area_ratio_test_nnet} \\n\")\n\n# Plot lift chart\nplot_lift_chart(test_pcg_nnet, area_ratio_test_nnet, title='Lift Chart Neural Network on test set')\n\n# Confusion matrix\ncm = confusion_matrix(y_test, test_pred_nnet, labels=nnet.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()","336e03ce":"# Label predictions on test data\ny_pred_label_nnet_CV = test_proba_nnet_CV.argmax(1)\n# Area ratio on test data\ntest_pcg_nnet_CV = decile_table(y_test, test_proba_nnet_CV[:,1], labels=False)\ntest_area_ratio_nnet_CV = area_ratio(test_pcg_nnet_CV)\nprint(f\"Neural Network error rate CV score: {np.mean(np.array(error_rate_valids_nnet_CV))} | KNN area ratio score: {np.mean(np.array(area_ratio_valids_nnet_CV))}\\n\")\nprint(f\"Neural Network Forest error rate test (validation) score: {1 - accuracy_score(y_test, y_pred_label_nnet_CV)} | KNN area ratio score: {test_area_ratio_nnet_CV}\")\n# Plot lift chart\nplot_lift_chart(test_pcg_nnet_CV, test_area_ratio_nnet_CV, title='Lift Chart Neural Network with CV on test set')\nprint(\"\\n\")\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred_label_nnet_CV, labels=nnet.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                              display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()","294fde9f":"train_pcg_DT = decile_table(y, train_proba_DT[:, 1], labels=False)\ntest_pcg_DT = decile_table(y_test, test_proba_DT[:, 1], labels=False)\nerror_rate_train_DT = 1 - accuracy_score(y, train_pred_DT)\narea_ratio_train_DT = area_ratio(train_pcg_DT)\nerror_rate_test_DT = 1 - accuracy_score(y_test, test_pred_DT)\narea_ratio_test_DT = area_ratio(test_pcg_DT)\ny_pred_label_DT = test_proba_DT.argmax(1)\nprint(f\"Desicion Tree error rate on train set (training): {error_rate_train_DT} | area ratio on train set (training): {area_ratio_train_DT}\\n\")\nprint(f\"Desicion Tree error rate on test (valid) set: {error_rate_test_DT} | area ratio on test (valid) set: {area_ratio_test_DT} \\n\")\nplot_lift_chart(test_pcg_DT, area_ratio_test_DT, title='Lift Chart Desicion Tree on test set')\ncm = confusion_matrix(y_test, y_pred_label_DT, labels=DT.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()","ccaa0c55":"y_pred_label_DT_cv = test_proba_DT_cv.argmax(1)\ntest_pcg_DT_cv = decile_table(y_test, test_proba_DT_cv[:, 1], labels=False)\ntest_area_ratio_DT_cv = area_ratio(test_pcg_DT_cv)\nprint(f\"Error rate CV score: {np.mean(np.array(error_rate_valids_DT_cv))} | Area ratio score: {np.mean(np.array(area_ratio_valids_DT_cv))}\\n\")\nprint(f\"Error rate test (validation) score: {1 - accuracy_score(y_test, y_pred_label_DT_cv)} | Area ratio score: {test_area_ratio_DT_cv}\")\nplot_lift_chart(test_pcg_DT_cv, test_area_ratio_DT_cv, title='Lift Chart Desicion Tree with CV on test set')\ncm = confusion_matrix(y_test, y_pred_label_DT_cv, labels=DT_cv.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()\nprint(\"\\n\")","b64a75ad":"train_pcg_NB = decile_table(y, train_proba_NB[:, 1], labels=False)\ntest_pcg_NB = decile_table(y_test, test_proba_NB[:, 1], labels=False)\nerror_rate_train_NB = 1 - accuracy_score(y, train_pred_NB)\narea_ratio_train_NB = area_ratio(train_pcg_NB)\nerror_rate_test_NB = 1 - accuracy_score(y_test, test_pred_NB)\narea_ratio_test_NB = area_ratio(test_pcg_NB)\ny_pred_label_NB = test_proba_NB.argmax(1)\nprint(f\"Naive Bayes error rate on train set (training): {error_rate_train_NB} | area ratio on train set (training): {area_ratio_train_NB}\\n\")\nprint(f\"Naive Bayes rate on test (valid) set: {error_rate_test_NB} | area ratio on test (valid) set: {area_ratio_test_NB} \\n\")\nplot_lift_chart(test_pcg_NB, area_ratio_test_NB, title='Lift Chart Naive Bayes on test set')\ncm = confusion_matrix(y_test, y_pred_label_NB, labels=NB.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()","4ffd165f":"y_pred_label_NB_cv = test_proba_NB_cv.argmax(1)\ntest_pcg_NB_cv = decile_table(y_test, test_proba_NB_cv[:, 1], labels=False)\ntest_area_ratio_NB_cv = area_ratio(test_pcg_NB_cv)\nprint(f\"Error rate CV score: {np.mean(np.array(error_rate_valids_NB_cv))} | Area ratio score: {np.mean(np.array(area_ratio_valids_NB_cv))}\\n\")\nprint(f\"Error rate test (validation) score: {1 - accuracy_score(y_test, y_pred_label_NB_cv)} | Area ratio score: {test_area_ratio_NB_cv}\")\nplot_lift_chart(test_pcg_NB_cv, test_area_ratio_NB_cv, title='Lift Chart Naive Bayes with CV on test set')\ncm = confusion_matrix(y_test, y_pred_label_NB_cv, labels=NB_cv.classes_)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=['no defaulter', 'Defaulter'])\ndisp.plot()\nprint(\"\\n\")","61290a50":"table={'Error rate : training : without cv':[error_rate_train_knn,error_rate_train_xgb,error_rate_train_Logistic,error_rate_train_RF,error_rate_train_LDA,error_rate_train_nnet,error_rate_train_DT,error_rate_train_NB],\n       'Area ratio : training : without cv':[area_ratio_train_knn,area_ratio_train_xgb,area_ratio_train_Logistic,area_ratio_train_RF,area_ratio_train_LDA,area_ratio_train_nnet,area_ratio_train_DT,area_ratio_train_NB],\n        'Error rate : validation : without cv':[error_rate_test_knn,error_rate_test_xgb,error_rate_test_Logistic,error_rate_test_RF,error_rate_test_LDA,error_rate_test_nnet,error_rate_test_DT,error_rate_test_NB],\n       'Area ratio : validation : without cv':[area_ratio_test_knn,area_ratio_test_xgb,area_ratio_test_Logistic,area_ratio_test_RF,area_ratio_test_LDA,area_ratio_test_nnet,area_ratio_test_DT,area_ratio_test_NB],\n        'Error rate : training : with cv':[np.mean(np.array(error_rate_valids_knn_cv)),np.mean(np.array(error_rate_valids_xgb_cv)),np.mean(np.array(error_rate_valids_Logistic_cv)),np.mean(np.array(error_rate_valids_RF_CV)),np.mean(np.array(error_rate_valids_LDA_CV)),np.mean(np.array(error_rate_valids_nnet_CV)),np.mean(np.array(error_rate_valids_DT_cv)),np.mean(np.array(error_rate_valids_NB_cv))],\n       'Area ratio : training : with cv':[np.mean(np.array(area_ratio_valids_knn_cv)),np.mean(np.array(area_ratio_valids_xgb_cv)),np.mean(np.array(area_ratio_valids_Logistic_cv)),np.mean(np.array(area_ratio_valids_RF_CV)),np.mean(np.array(area_ratio_valids_LDA_CV)),np.mean(np.array(area_ratio_valids_nnet_CV)),np.mean(np.array(area_ratio_valids_DT_cv)),np.mean(np.array(area_ratio_valids_NB_cv))],\n       'Error rate : validation : with cv':[1 - accuracy_score(y_test, y_pred_label_knn_cv),1 - accuracy_score(y_test, y_pred_label_xgb_cv),1 - accuracy_score(y_test, y_pred_label_Logistic_cv),1 - accuracy_score(y_test, y_pred_label_RF_CV),1 - accuracy_score(y_test, y_pred_label_LDA_CV),1 - accuracy_score(y_test, y_pred_label_nnet_CV),1 - accuracy_score(y_test, y_pred_label_DT_cv),1 - accuracy_score(y_test, y_pred_label_NB_cv)],\n       'Area ratio : validation : with cv':[test_area_ratio_knn_cv,test_area_ratio_xgb_cv,test_area_ratio_Logistic_cv,test_area_ratio_RF_CV,test_area_ratio_LDA_CV,test_area_ratio_nnet_CV,test_area_ratio_DT_cv,test_area_ratio_NB_cv]}\npd.DataFrame(table,index=[\"KNN\",\"XGBoost\",\"Linear Regression\",\"Random Forest\",\"Linear Discriminant Analysis\",\"Neural Network MLP\",\"Desicion Tree\",\"Naive Bayes\"])","a50df4a4":"## Business objectives:","ace9e2f3":"#### Sans CV","08f75e32":"## Logistic Regression avec Validation Crois\u00e9e","66f8dd1b":"## Neural Network MLP","abe57e1a":"## KNN sans Validation Crois\u00e9e","46e2530e":"## DecisionTree sans Validation Crois\u00e9e","2dba3d78":"#### Avec CV","9cad682e":"## Feature : Amount of Previous Payment (PAY_AMTX)  ","f747cb19":"## Feature : repayment status (PAY_X) ","c303c33d":"## Naive Bayes","e58ae77a":"###### observation : there is a significant rate of default (over 30%) from customers with 50k or less of credit limit\n###### => the higher the limit, the lower is the chance of defaulting","69109eb0":"#### Avec CV","5bf14639":"## Neural Network sans Validation Crois\u00e9e","9fdaef68":"## Neural Network avec Validation Crois\u00e9e","69851837":"###### amount of given credit","c9b4b616":"###### in every single comparison, the probability was higher for men","d50dd856":"#### Sans CV","38f2e081":"## KNN avec Validation Crois\u00e9e","12fa8267":"## Linear Discriminant Analysis sans Validation Crois\u00e9e","ac5981f6":"###### observation : most customers have 200k or less of credit limit. \n###### A higher concentration of customers in default on that range","040aab16":"###### the lowest chance of default are regestered around [30-35] , while the highest occur at ([20-25] and 60+)","39e5c84b":" #### VI. Deploiement (optionnel)","f3a8bb6a":"# II. Data understanding\n\n##### D\u00e9terminer pr\u00e9cis\u00e9ment les donn\u00e9es \u00e0 analyser, \u00e0 identifier la qualit\u00e9 des donn\u00e9es disponibles. ","d96c4130":"#### Sans CV","67be2847":"###### observation : those who have a negative bill statement have a lower chance of default than the rest.","5c8f91ed":"## XGBoost sans Validation Crois\u00e9e","88f11496":"# IV. Modeling \n##### Expliquer et justifier le choix, le param\u00e9trage et le test de diff\u00e9rents algorithmes ","83c32e6c":"Aiming at the problem that the credit card default data of a financial institution is unbalanced, which leads to unsatisfactory prediction results, we have tried to implement data-preprocessing tips to clean the used data.\nElse for the used model, we intend to use multiple classifiers: KNN, XGBoost, LR, Decision Tree, Random Forest, LDA, MLP, NB.\nAs for the data-mining success criteria: we are aiming to use the loss function and area-ratio metric.","f4a8e7eb":"#### Avec CV","b7165c29":"## Logistic Regression","5b48891b":"# I. Business understanding:\n##### Expliquer les \u00e9l\u00e9ments m\u00e9tiers et probl\u00e9matiques \u00e0 r\u00e9soudre","1704c0e0":"Throughout our project, we had 4 main phases to accomplish: Data exploration, Data preparation,\nModelling and Evaluation. Each phase has its own set of technologies and algorithms which we used\nto obtain the best results possible.\n\nData exploration: Descriptive overview of the data in use\nTechnologies: Python 3 Libraries: pandas, matplotlib\nDuration: 7 days \n\nData Preparation: Methods and functions to adjust the data for more compatibility with the modelling algorithms\nTechnologies: Python 3 Libraries: NumPy, seaborn\nDuration: 20 days\n\nModelling: Apply the \u201cCleaned data\u201d to the Modelling algorithms\nTechnologies: KNN, XGB, Logistic Regression, Random Forest, Linear Discriminant Analysis, \nNeural Network, Decision Tree\nDuration: 30 days \n \nEvaluation: Compare the results with the criteria  \nTechnologies : Lift chart, confusion matrix\nDuration : 3 days\n","8564e127":"## Hyperparameter tuning avec optuna pour Random Forest","53ba436e":"###### we'll be following the same approach used to analyze 'Age' and 'Limit_Bal' , showing destribution and density , than dividing data inbins to get a clear\n###### percentage of default for each group","fe5c1510":"### The business understanding phase relies on 4 main steps:","7b4b20df":"###### we saw earlier that the average given credit for women was slightly higher than for men. that still holds up for several combinations \n###### of categories, except among customers that(have a grad school diploma \/ are married \/ are 50+ years old)","1ab280a6":"## Random Forest","bfce73c9":"First of all, starting with resources, we should mention that this is an academic project, so the dataset is given by the Machine Learning commission. As for other resources we mention first human resources which are the 6 members of our group whose role is to achieve the work, and the instructors for guidance. Else we mention hardware and software resources, which for our case is Kaggle cloud. The fact that our implementation was tested and run-on cloud with a GPU Tesla P100 that uses a 16 GB RAM, and then it was all run-on our colleague laptop, that englobe an i7 9th generation CPU, with 16 GB of RAM.\n\nSecond, several requirements, assumptions, and constraints are meant to be respected, such as the schedule for completion an acceptable which was defined by the instructors. Constraints for dividing the work and reassembling the whole tasks.\n\nBesides, some risks and contingencies that may cause delays for delivery was predicted and overcome by having several copies for each one of us so that we never get stopped or blocked in a task to do. Yet, fortunately, we did not face that issue. \n\nThe glossary of business terms and data-mining terms is as below:\nCV\tCross-Validation\nDT\tDecision Tree\nKNN\tK-Nearest Neighbour\nLDA\tLinear Discriminant Analysis\nLR\tLinear Regression\nMLP\tMultilayer perceptron\nRF\tRandom Forest\nXGB\tXGBoost\n\nLast but not least for this phase, as it is an academic project, no costs or benefits are meant to be mentioned.","bcec4cf4":"## Naive Bayes avec Validation Crois\u00e9e","a283986a":"## Amount of Given Credit (LIMIT_BAL) + Demographic Features (SEX, EDUCATION , MARRIAGE)","99675f61":"###### Between the age 25 and 40 the chance of default is a little lower \n###### we could divide our dataset in bins and check the percentage of default in each age group ","abab6e36":"## Assess situation:","dabc542e":"## KNN","f47704c2":"#### Avec CV","4e2f2c3c":"## DecisionTree avec Validation Crois\u00e9e","473edf78":"###### observation :\n###### we could analyze the relationship the credit limit and the combination of two demographic features","5205d7b0":"###### seaborn boxplot : (categorical destribution plots) median, bottom quartile(25th percentile) , top quartile(25th percentile) (interquartile range IQR) and whiskers(1.5*IQR)\n###### showmeans=True : mean triangle","cd50422e":"## Feature : Marriage","92092c89":"## Determine data mining goals:","7000dd14":"## Random Forest sans Validation Crois\u00e9e","1a6c5e3c":"#### avec CV","ead5f964":"## Desicion Tree","ae39c5a7":"## Linear Discriminant Analysis avec Validation Crois\u00e9e","15139197":"# Imports","0c65bca9":"#### Sans CV","2f42e7cd":"## Feature : Age","c3cbf2a7":"Financial threats are displaying a trend about the credit risk of commercial banks as the incredible improvement in the financial industry has arisen. In this way, one of the biggest threats faces by commercial banks is the risk prediction of default credit clients. Things have gotten so bad that credit card companies have to set aside an additional reserve to cover their losses from credit card defaults.\n\nPeople\u2019s inability to pay for their credit card bills could be due to different circumstances. However, when it\u2019s deliberate, meaning that customers have no plans in paying the bank back, it would be considered as a fraud. Either way, this imposes a huge risk on the credit card companies and we need to find a way to flag them.\n\nTo address this issue, a Taiwan-based credit card issuer wants to better predict the likelihood of default for its customers, as well as identify the key drivers that determine this likelihood. This would inform the issuer\u2019s decisions on who to give a credit card to and what credit limit to provide. It would also help the issuer have a better understanding of their current and potential customers, which would inform their future strategy, including their planning of offering targeted credit products to their customers. We can predict potential default accounts based on certain attributes. The idea is that the earlier the potential default accounts are detected, the lower the losses we will embrace. On the other hand, we can be proactive by providing tips to customers to prevent default. This not only protects customers but also minimizes risks and potential losses.\n\nSo, the main objective is to predict whether the customer will default on their credit card payment next month.\n","a80daacc":"## XGBoost avec Validation Crois\u00e9e","8d0edeb6":"## Benchmarking","ed41411b":"#### Sans CV","b5fb3a75":"## avec CV","08a3cab2":"## Produce project plan:","04656589":"## Validation Projet Machine Learning","04f6b9c6":"#### Sans CV","d0cbdc10":"## Feature : Amount of Bill Statement (BILL_AMTX)","f4dd7d49":"# V. Evaluation\n###### V\u00e9rifier le(s) mod\u00e8le(s) ou les connaissances obtenues afin de s\u2019assurer qu\u2019ils r\u00e9pondent aux objectifs formul\u00e9s au d\u00e9but du processus.\n","e15ea466":"## XGBoost","73b4efed":"#### Sans CV","b753a9ac":"## Logistic Regression sans Validation Crois\u00e9e","b5256206":"## Hyperparameter tuning avec optuna pour Logistic Regression","43ef6ba3":"###### Most people fall on 'married' or 'single' category\n###### Unknown category present a lower probability of default","52a8ce23":"## Random Forest avec Validation Crois\u00e9e","b72aaedb":"###### Comparing male\/female \n###### considering a similar education level and status('married' or 'single')","47658559":"## Feature : Limit_Bal","79f2af5b":"# III. Data preparation \n\nOrganiser les donn\u00e9es pour la mod\u00e9lisation","b63efded":"## Linear Discriminant Analysis","a027ea56":"###### observation : Most customers are duly paying their credit card bills. And it's pretty clear that their likelihood of default are much lower than the rest.","64a9d3d1":"## Feature : Sex","6bf5d2f0":"#### Avec CV","cb8a3bf6":"## Feature : Education","3a6af7d9":"#### Avec CV","ab0745c8":"## Hyperparameter tuning avec optuna pour XGBoost","0b688385":"#### Sans CV","57532e38":"## Naive Bayes sans Validation Crois\u00e9e"}}